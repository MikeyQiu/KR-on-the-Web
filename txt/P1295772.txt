7
1
0
2
 
n
a
J
 
3
2
 
 
]
h
p
-
m
e
h
c
.
s
c
i
s
y
h
p
[
 
 
1
v
9
4
6
6
0
.
1
0
7
1
:
v
i
X
r
a

Constant Size Molecular Descriptors For Use
With Machine Learning

Christopher R. Collins,† Geoﬀrey J. Gordon,‡ O. Anatole von Lilienfeld,¶ and
David J. Yaron∗,†

†Department of Chemistry, Carnegie Mellon University, Pittsburgh, Pennsylvania 15213
United States
‡Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania
15213 United States
¶Department of Chemistry, Institute of Physical Chemistry and National Center for
Computational Design and Discovery of Novel Materials (MARVEL), University of Basel,
4056 Basel, Switzerland

Abstract

1 Introduction

A set of molecular descriptors whose length is
independent of molecular size is developed for
machine learning models that target thermo-
dynamic and electronic properties of molecules.
These features are evaluated by monitoring per-
formance of kernel ridge regression models on
well-studied data sets of small organic molecules.
The features include connectivity counts, which
require only the bonding pattern of the molecule,
and encoded distances, which summarize dis-
tances between both bonded and non-bonded
atoms and so require the full molecular geome-
try. In addition to having constant size, these
features summarize information regarding the
local environment of atoms and bonds, such
that models can take advantage of similarities
resulting from the presence of similar chemical
fragments across molecules. Combining these
two types of features leads to models whose per-
formance is comparable to or better than the
current state of the art. The features introduced
here have the advantage of leading to models
that may be trained on smaller molecules and
then used successfully on larger molecules.

Cheminformatics has a long tradition of using
the techniques of machine learning to predict
properties of molecules. A particular focus has
been properties that emerge from interactions
of the molecule with complex external systems,
such as arise in drug discovery. 1,2 More recently,
informatics has been successfully used to de-
velop models of the properties of the molecules
themselves. This includes thermodynamic prop-
erties, 3–12 electronic 13–26 properties and molecu-
lar structure. 27–30 Successful predictions of such
molecular properties provides an alternative to
quantum chemical computations, leading to use-
ful predictions at a small fraction of the cost of
quantum computations. 4,5 Because the target
properties involve only the molecule itself, as
opposed to interactions with complex external
systems, the descriptors that bring the most
explanatory power to the informatics tasks may
diﬀer from those developed for targets involving
external systems. This paper explores a number
of descriptors, or features, for predicting the
properties of individual molecules.

A key dimension along which informatics mod-
els of molecular properties diﬀer is the degree
to which results from chemical computations
are used as features for the prediction task. On

1

one end of this spectrum, the model corrects
predictions from standard computational chem-
istry methods, so that they better match more
expensive calculations 17,18,31 or experiments. 3,32
At the other end of the spectrum, the only infor-
mation used is what one might get from a line
drawing or SMILES 33 string. At the latter level,
there have been many examples of using simple
counts of atoms and bonds to make predictions
of molecular properties. 5,34 The low computa-
tional cost of such models makes them useful
for searching molecular databases. 35 Their re-
stricted applicability to potential energy minima
or known bonding patterns; however, reduces
their predictive power.

Between the above two extremes are models
that utilize the 3D molecular geometry to make
predictions. Models that use features based on
the Coulomb matrix, such as the recent Bag of
Bonds (BoB) feature vector, have been shown
to be highly eﬀective with this level of informa-
tion. 5,34 However, the length of these feature
vectors scales formally as the square of the num-
ber of atoms in the largest molecule of interest. 36
There have been attempts to rectify this issue,
but they have been unable to achieve the same
level of performance. 20

A primary goal of the work presented here is
to develop feature vectors whose length is inde-
pendent of molecular size and that subsequently
lead to models that generalize well from smaller
to larger systems. This should be possible if
we assume: a) an eﬀective chemical periodicity
where large systems consist of smaller fragments
which resemble exemplary chemistries present in
the data base used for training, and b) locality
implying additivity. For certain systems, these
assumptions can break down; in particular, the
latter one when strong electron delocalization
eﬀects are present as in metals or in the π-bonds
of conjugated polymers. To achieve our goal,
two classes of features are explored.

The ﬁrst class of features are based on con-
nectivity counts, which have been explored in
a number of diﬀerent ways. 8,37–40 Here, these
features include counts of atom types and bond
types, which we refer to as rank-1 and rank-2
features since they summarize patterns related
to one and two atoms, respectively. Higher-rank

features, that count occurrences of patterns re-
lated to three or more bonded atoms, are also
explored.

The second class of features are encoded dis-
tances. These summarize the pairwise distances
between atoms in a molecule through a feature
vector whose length is independent of molecu-
lar size. The feature vector is a smoothed his-
togram of distances between atoms, including
non-bonded atoms.

Both classes of features summarize informa-
tion regarding the local environment of atoms
and bonds in the molecule. The resulting fea-
ture vectors therefore contain information on the
fragments present in the molecule. In particular,
when two molecules possess the same molecular
fragment, the interactions amongst the atoms
in that fragment make similar contributions to
the feature vector. Use of these feature vectors
to predict similarity between two molecules, as
in the kernel methods below, therefore incorpo-
rates similarity resulting from the presence of
similar fragments. In addition, the feature vec-
tors include information regarding interactions
amongst the fragments. The features may there-
fore enable models to generalize from smaller to
larger molecules.

2 Related Work

The features studied here are somewhat related
to features developed for neural net models of po-
tentials. 10,41–43 In those models, the total energy
is written as a sum over atomic energies, with
the atomic energies being functions of the local
environment. 44 The environment is described us-
ing features based on two and three body corre-
lation functions around the atom. The encoded
distance features explored here may be viewed
as smoothed versions of two-body correlation
functions between each pair of elements. We
note that the latter lack uniqueness, a crucial
feature property for guaranteeing error conver-
gence with training set size. 20 In particular, they
will fail to distinguish homometric molecules.

Below, we use kernel-based learning methods
to explore the utility of the features introduced
here. Our studies therefore have some similari-

2

ties to the Regularized Entropy Match Smooth
Overlap of Atomic Positions (REMatch-SOAP)
kernel, which provides an alternative means for
predicting molecular properties using only 3D
molecular geometries. 23 REMatch-SOAP does
not introduce a molecular feature vector and,
instead, provides a means to directly compute
the similarity between two molecular structures,
as needed for kernel methods. The approach be-
gins by computing the similarity between each
of the atoms in the two molecules being com-
pared, where the similarity measure includes the
molecular environment. The molecular similar-
ity is then obtained from a regularized entropy
match of the resulting atomic similarities.

In contrast to REMatch-SOAP, the features
introduced here are not limited to kernel meth-
ods and, when used in kernel methods, allow
us to separately consider the inﬂuences of three
factors: the feature vector, the distance metric
used to connect features to the similarity be-
tween molecules, and the kernel function. Our
emphasis here is on the features themselves. We
use cross validation to select the kernel, includ-
ing the width parameter for an exponential ker-
nel function and the choice of a Laplacian kernel,
which uses an L1 distance metric, versus a Gaus-
sian kernel, which uses an L2 distance metric.
The performance of the resulting models on well
studied data sets is examined, with particular
attention paid to the ability of the method to
scale between diﬀerent size molecules.

3 Data Sets

This work compares performance of various
molecular features on data sets that have been
the subject of past work, demonstrating the
predictive power of machine learning models
to predict molecular properties. These data
sets utilize molecules selected from the following
two data sets, each of which contains only the
bonding pattern as summarized in a SMILES
string. The GDB13 data set contains 977 mil-
lion molecules made of C, H, O, N, S, and Cl,
with up to 13 heavy atoms, 45 and the GDB17
data set contains 166 billion molecules made
up of C, H, O, N, and F with up to 17 heavy

atoms. 46

The QM7 data set corresponds to a subset
of the GDB13 molecules, consisting of all 7,165
molecules that contain 7 or fewer heavy atoms
of elements C, N, O, or S for which optimized
structures and atomization energies were calcu-
lated with the Perdew-Burke-Ernzerhof hybrid
functional (PBE0). 4 Models for the atomization
energies in QM7 have been extensively stud-
ied, primarily using features derived from the
Coulomb matrix. 4–6,11,34 As such, this data set
provides a useful benchmark for testing models
and comparing to previous work.

The QM7b data set extends the QM7 data set
to molecules containing chlorine, and includes
a broader range of properties such as HOMO
and LUMO energies, excitation energies, and
polarizabilities calculated at various levels of
theory. 16 QM7b allows performance of machine
learning models to be explored on a wider va-
riety of properties. Past works have also used
QM7b to explore multi-target regression meth-
ods which, by simultaneously predicting various
properties, can potentially beneﬁt from relation-
ships between the target properties. 6,16,34

The QM9 data set 47 contains data for a
subset of the GDB17 molecules, consisting of
133,885 molecules that contain nine or fewer
heavy atoms. (The name QM9 is used here to
be consistent with that of QM7.) The QM9 data
set has been used in several studies 19–21,34 and
includes optimized structures and 18 diﬀerent
properties calculated using B3LYP/6-31g(2df,p).
Below, QM9 is used to explore the degree to
which models training on smaller molecules can
transfer to larger molecules. The large amount
of data in QM9 also allows more extensive stud-
ies on the degree to which inclusion of additional
data improves model performance.

4 Machine learning

A variety of machine learning methods have al-
ready been used to predict molecular properties,
including linear regression, linear ridge regres-
sion, support vector regression, kernel ridge re-
gression, regression trees, k-nearest neighbors,
and neural nets/deep learning. 5,10,16 For this

3

The average of these MAEs is then taken as a
measure of model performance.

Here, model selection is based on a loop over
k=5 splits. Within this loop, one split is ig-
nored and the remaining 80% of data is used
to evaluate each set of hyperparameters, via 5-
fold cross validation. This generates 5 ﬁgures
of merit for each set of hyperparameters. The
hyperparameters that have the best average per-
formance are then selected as the ﬁnal model.
The MAE for this model, reported below and
in the Supporting Information, is the average
from a ﬁnal 5-fold cross validation performed on
the entire data set, using the selected hyperpa-
rameters. (See Supporting Information for the
hyperparameters of each model.)

5 Features

The primary goal of this work is a compari-
son of models developed using diﬀerent types
of molecular features. Ideally, identical molecu-
lar geometries should lead to identical feature
vectors, implying that the features are invari-
ant with respect to molecular translations and
rotations, and to reordering of the atoms.

For models trained on smaller molecules to
transfer to larger molecules, a number of other
properties of the feature vectors are desirable.
One such property is that the length of the fea-
ture vector be independent of molecular size. For
the Coulomb matrix and BoB features, this prop-
erty does not hold, as the number of elements in
the feature vector scales quadratically with the
number of atoms in the largest molecule in the
data set. To support models of diﬀerently sized
molecules, such feature vectors are expanded
to a length that can accommodate the largest
molecules in the data set, with zeros added to
pad the extended regions for smaller molecules.
For the remaining features considered below,
the feature vectors have a length that depends
on the molecular diversity, e.g. atom and bond
types, but does not depend on molecular size.

study, we have elected to use two standard ma-
chine learning methods, 48 linear ridge regres-
sion (LRR) and kernel ridge regression (KRR)
with the Gaussian and Laplacian kernels. These
methods were chosen due to their relative sim-
plicity, compared for instance to deep learning,
and because past work has shown them to be
eﬀective on the data sets studied here. 5,34

The models involve very many parameters
which, during “model training”, are adjusted
to obtain predictions that agree with a given
set of examples. In addition, both LRR and
KRR involve hyperparameters that serve to de-
ﬁne further the model form. The wide range of
values for the hyperparameters leads to a wide
range of possible models. “Model selection” in-
volves searching through these to ﬁnd a model,
or equivalently a set of hyperparameters, that
leads to good performance. Here, we do an ex-
haustive search over a grid of hyperparameter
values using cross validation as described below.
In LRR, there is one hyperparameter, α, which
is the weight on the ridge term. When α is zero,
the model produces the same result as linear
regression. Finite values for α penalize larger re-
gression parameters and so using ﬁnite values for
α favors simpler models (those with smaller re-
gression parameters). For LRR, the following α
values were scanned over: {10−5, 10−3, . . . , 101}.
KRR retains the α parameter of LRR and adds
two more. The choice of kernel for KRR can
be considered as a discrete hyperparameter and
here the Gaussian (exp(−γ||x−y||2
2)) and Lapla-
cian (exp(−γ||x − y||1)) kernels are included in
the search. For each of these kernels, there is an
additional continuous hyperparameter, γ, which
sets the width of the kernel. The search over α
and γ is exhaustive on a square grid with values
{10−11, 10−9, . . . , 10−1}.

Model selection and testing use k-fold cross
validation (CV). In k-fold CV, the examples
in the data set are split into k equally sized
sets. One of these folds is then held out, and
the examples in the remaining folds are used to
train a model. The mean absolute error (MAE)
for predictions made on the examples in the
hold out set is then computed. This process
is repeated, with each one of the k folds being
the hold out set, leading to k diﬀerent MAEs.

4

5.1 Null model

A “null” model is used to provide a baseline
measure of the diﬃculty of the prediction task.
The null model always predicts the mean value
of the training data. The MAE of the null model
therefore reﬂects the spread of the data.

5.2 Coulomb Matrix and Bag of

Bonds

A class of
features that has led to well-
performing models of molecular properties
are those derived from the Coulomb ma-
trix, 4–6,16–19,21

Mij =

(cid:40)

,

i

0.5Z 2.4
ZiZj
|ri−rj | ,

if i = j
if i (cid:54)= j

(1)

where Zi and ri are the atomic number and
Cartesian position of the ith atom. This feature
is appealing in that it summarizes the molecular
structure in terms of Coulomb interactions be-
tween atoms. Alternative forms for the interac-
tion, other than Coulomb interactions between
bare nuclei, have been tried but were not found
to enhance model performance. 34

While the Coulomb matrix is invariant to
molecular rotations and translations, it is not
invariant to reordering of the atoms. A num-
ber of means have been explored to address the
dependence on atom order, including sorting
the matrix based on the magnitude of the norm
of the rows, using randomly permuted sets of
matrices, and using the eigenvalues of the ma-
trix. 5,6,16 (The results generated here do not sort
the Coulomb matrix, and simply arrange the
lower triangle of the symmetric matrix into a
vector.) A recent successful approach rearranges
the Coulomb matrix into a “Bag of Bonds”, BoB,
feature. 34 In BoB, oﬀ-diagonal elements of the
matrix are ﬁrst divided into bags, based on the
elements involved in the Coulomb interaction
(CH, CC, CO, etc). The values in each bag are
then sorted from high to low values. The maxi-
mum size of each bag, across the molecules in
the data set, is then determined and zeros are
added to each bag so that all molecules have
bags of the same length. The bags are then con-

5

catenated to make a single feature vector that
is invariant to reordering of the atoms. Because
the BoB model is just a reordering of Coulomb
matrix elements, the length of the vector still
grows quadratically with the size of the largest
molecule in the data set. BoB has also recently
been extended to include three-body and higher
terms, 49 leading to feature vectors who length
grows as the third or higher power of the number
of atoms in the largest molecule.

5.3 Connectivity Counts

This class of features summarizes the bonding
pattern of a molecule through features that
count occurrences of some pattern regarding the
bonding between atoms. Each feature may be
assigned a rank, based on the number of distinct
atoms that are examined when testing for the
presence of the pattern. Rank-1 features count
atom types, such as element or element plus
coordination number, leading to a vector whose
length is equal to the number of atom types.
Rank-2 features count bond types (single, aro-
matic, double, triple), leading to a vector whose
length is the number of bond types. Rank-3
features count triplets of bonded atoms, and so
on.

For rank 1, each element of the feature vector,
vT , holds the number of atoms in the molecule
that have type T ,

(cid:88)

vT =

δA(i),T

i

(2)

where the sum is over atoms, i, A(i) is a func-
tion that returns the atom type, and δ is the
Kronecker symbol. Two atom typing schemes
are explored here, leading to two diﬀerent rank-
1 feature vectors denoted as 1 and 1C. For
1, atom type is deﬁned by element only. For
1C, atom type is deﬁned by a combination of
element and coordination number. Coordina-
tion number is taken as the number of bonds in
which the atom participates.

For rank 2, each element of the feature vector,
vT , holds the number of bonds in the molecule

that have type T ,

vT =

δB(i,j),T

(3)

(cid:88)

i<j

where the sum is over atoms, and B(i, j) is a
function that returns the bond type between
atoms i and j, or null if a bond is not present.
Two bond typing schemes are explored here,
leading to two diﬀerent rank-2 feature vectors
denoted as 2 and 2B. For 2, bond type is deﬁned
by the two elements participating in the bond. A
bond is assumed to be present if the separation
between atoms is less than the cutoﬀs listed in
the Supporting Information. For 2B, bond type
is deﬁned by the two elements participating in
the bond and the bond order, with bond orders
assigned based on bond length as discussed in
the Supporting Information.

For rank 3, each element of the feature vec-
tor corresponds to a tuple of two bond types,
(T1, T2), with a value

v(T1,T2) =

δB(i,j),T1δB(j,k),T2

(4)

(cid:88)

i<j<k

where the sum is over atoms. The feature counts
only situations where atoms i and k are bonded
to a common atom, j. The summation is thus
over all triples of atoms to which a bond angle
would typically be ascribed. Higher-rank pat-
terns are generalizations of Eq. (4) to higher
order. For example, the rank 4 summation is

v(T1,T2,T3) =

δB(i,j),T1δB(j,k),T2δB(k,l),T3

(cid:88)

i<j<k<l

(5)
with the summation being over all sets of four
atoms to which a dihedral angle would typically
be ascribed.

5.4 Encoded distances

The connectivity count features of Section 5.3
summarize the bonding pattern of a molecule
but do not include information about the bond
lengths, bond angles, or dihedral angles. The
encoded features developed here allow informa-
tion on the 3D structure of the molecule to be
used while keeping the length of the feature

vector independent of the size of the molecules
included in the data set. We include only rank-2
encodings, which summarize information regard-
ing atom-atom distances. Extension to higher
ranks would lead to a substantial increase in the
length of the total feature vector and are not
explored here.

The encoded distance features may be viewed
as generalizations of the above rank-2 connectiv-
ity features. Because the bond types of Eq. (3)
are based on atom-atom distances, the rank-2
connectivity counts of Section 5.3 may be viewed
as encoding atom-atom distances in a coarse
grained manner. To introduce the notation we
will use for encoding, we rewrite Eq. (3) as

vT =

1(rjk ∈ (rlow

T , rhigh

T

])

(6)

(cid:88)

j<k

T

T

and rhigh

where rlow
are the lower and upper
limits of the distance ranges that deﬁne the bond
type, T , for the corresponding pair of elements,
and rjk is the distance between atoms j and k.
Eq. (6) may be viewed as creating a histogram
for each pair of elements. For feature vector 2,
the histogram has just one bin and reports the
number of bonds between those elements in the
molecule. For feature vector 2B, the number of
bins in the histogram is the number of allowed
bond orders between those elements.

A simple approach to adding more information
to the feature vectors is to add more bins to the
histogram. The information encoded can also
be expanded to include distances between non-
bonded atoms. To accomplish this, we create a
uniformly spaced grid,

di = rstart +

(rend − rstart)i
Ngrid − 1

for i = 0, 1, . . . , Ngrid − 1 .

(7)

For each pair of elements, we then get a set of
Ngrid features,

vi =

1(rjk ∈ (di, di+1]) .

(8)

(cid:88)

j<k

The feature vector of Eq. (8) has two disadvan-
tages. The ﬁrst is that a small change in rjk

6

can lead to a discontinuous change in the fea-
ture vector when rjk is near a grid point, di.
In addition, for values of Ngrid that are large
compared to the number of pairs of elements
in the molecules of interest, the feature vector
becomes sparse, making the learning prone to
overﬁtting.

To overcome these disadvantages of the
histogram-like features, we generalize Eq. (8)
by replacing 1(rjk ∈ (di, di+1]) of Eq. (8) with
an encoding function, f ,

vi =

f (rjk, di, β).

(9)

(cid:88)

j<k

f returns a value in the range [0, 1] and may
depend on a parameter, β, that is used to alter
the smoothness of the function. We consider
two general classes of encoding functions: prob-
ability density functions (PDF) and cumulative
distribution functions (CDF). For both of these,
we consider three types of functions: the normal
distribution, the logistic distribution, and the
spike distribution (Table 1).

Table 1: Functions used for the encoded features
of Eq. (9). rjk is the distance between atoms, di
are the grid points, β is a smoothing parameter,
x = β(rjk − di), and (cid:15) is the distance between
two grid points.

f (rjk, di, β) in Eq. (9)

Name Name2
2N P
2N C
2LP
2LC
2SP
2SC

Normal PDF exp(−x2)
Normal CDF 1 + erf(x)
Logistic PDF exp(−x)/(1 + exp(−x))2
Logistic CDF 1/(1 + exp(−x))
Spike PDF
Spike CDF

1(|rjk − di| < (cid:15)/2)
1(rjk > di)

For the PDF class of functions, the feature
vector may be viewed as adding noise to each
of the atom-atom distances, rjk, with noise dis-
tributed according to the distribution of the
encoding function. The CDF class of functions
integrate the corresponding PDFs, leading to
a feature vector that increases monotonically
with distance and has a slope that indicates the
presence of atom-atom separations near di.

Empirical results, discussed below, suggest
that model performance may be improved by

7

limiting the geodesic distance, G, between atoms
included when computing the feature,

vi =

f (rjk, di, β)1(Gjk ≤ Gmax)

(10)

(cid:88)

j<k

where Gjk is an integer specifying the number
of bonds along the shortest path between the
atom j and atom k. The last term limits the
summation to pairs of atoms that are separated
by up to Gmax bonds. Other works have taken
a similar approach by setting a cutoﬀ based on
the Euclidean distance between atoms. 10,11,41–43
In the current studies, model performance was
found to be somewhat better when using a cut-
oﬀ based on geodesic distance. Examples of
encoded features for benzene can be seen in
Figure 1.

−1

The parameters specifying the encoded dis-
tance features were tuned by evaluating perfor-
mance on the QM7 data set. The Supporting
Information discusses the choice of Ngrid and β
of Eq. (9). For the results reported below, β
was 20 ˚A
, and the grid consists of 100 points
between 0.2 ˚A and 6 ˚A. The choice of cutoﬀ dis-
tance, Gmax of Eq. (10), is based on the results
in Figure 2, which show how model performance
varies with Gmax for various encoded features.
Results for the Coulomb matrix and BoB fea-
tures are also shown, in which case Gmax is im-
plemented by setting to zero those values of the
Coulomb matrix that correspond to atoms sepa-
rated by a geodesic distance greater than Gmax.
For nearly all features, the best performance is
obtained with Gmax of 2 or 3, corresponding to
distances over which angles and dihedrals would
be assigned to the molecular structure. This
is true of all features, when LRR is used, and
nearly all features, when KRR is used. The ex-
ceptions are KRR with 2SP (results not shown
as the MAE is greater than 50 kcal/mol) and
KRR with the Coulomb matrix. Note, however,
that when the Coulomb matrix elements are
reordered to form BoB features vectors, the op-
timal Gmax is in the 2-3 range. The observation
that Gmax is in this range for a broad range of
feature types and for both KRR and LRR mod-
els suggests that the atomization energy has a
local character, whereby the energy of a given

Figure 1: Encoded bond feature vectors for benzene. Left panels are 2N P and right panels are
2N C. Lower panels limit the geodesic distance between atoms, Gmax of Eq. (10), to 3. Upper panels
include all distances.

8

bond is primarily a function of its local envi-
ronment. For the remainder of this work, Gmax
is set to 3 for the encoded features. To allow
comparison with past work, the BoB features
do not limit Gmax.

Because the KRR models outperform LRR by
more than 0.5 kcal/mol, we also discuss only
results from KRR. (Full results from both LRR
and KRR are tabulated in the Supporting Infor-
mation.)

6 Results

Throughout this section, we summarize results
by listing the features, with notation as in Sec-
tion 5, followed in parentheses by the MAE of
a KRR model developed as in Section 4. For
example, 2B (7.69 kcal/mol) indicates that a
KRR model using a feature vector 2B leads to
an MAE of 7.69 kcal/mol. The Supporting Infor-
mation contains an extensive list of results from
both LRR and KRR models based on diﬀerent
feature sets. Selected results are discussed here.
We ﬁrst consider models that use a single
type of feature from Section 5 to predict the
atomization energies of the QM7 data set. For
connectivity counts, the best performing single
features are 2B (7.69 kcal/mol) and 3B (6.36
kcal/mol). The use of the 3D molecular struc-
ture via encoded distance features lowers the
error substantially, with the best feature being
2N P (2.45 kcal/mol). The other encoding func-
tions perform nearly as well, except for 2SP (146
kcal/mol) which is essentially a histogram and so
gives a discontinuous and sparse feature vector.
This suggests that smooth encoding functions
lead to enhanced performance.

Model performance can be enhanced by con-
catenating features of diﬀerent ranks. The ef-
fects of adding features of increasing rank are
shown in Figure 3, with selected results shown
in Table 2. The upper branch in Figure 3
(solid lines) shows results when only connectivity
counts are included and all feature vectors begin
with 12B. These have the advantage of requir-
ing only a line drawing or SMILES string of the
molecule. Addition of higher-rank connectivity
counts improves performance up though rank

4, with addition of rank 5 leading to a small
degradation. Figure 3 also examines whether
inclusion of bond order improves performance,
e.g. whether 3B (red line) or 3 (black line) leads
to a lower MAE. For feature vectors including
only connectivity counts, inclusion of bond order
enhances performance at rank 3, but degrades
performance at higher ranks. The lowest MAE
for concatenated connectivity counts is 12B3B4
(3.40 kcal/mol).

The lower branch (dashed lines) of Figure 3
shows results from feature vectors that begin
with 12LC, and so include encoded distances
from 3D molecular structures. For feature vec-
tors that include encoded distances, inclusion
of rank 3 leads to substantial improvements in
model performance and inclusion of rank 5 de-
grades performance. Inclusion of rank 4 either
slightly improves or slightly degrades perfor-
mance, depending on both the type of rank-2
encoding employed and the property being pre-
dicted.
In contrast to the connectivity only
features, inclusion of bond order leads to better
performance at both rank 3 and rank 4. The
best performance on QM7 atomization energies
is 12N P 3B (1.19 kcal/mol). Inclusion of rank
4, as 12N P 3B4B (1.25 kcal/mol), slightly de-
grades performance. The performance is signiﬁ-
cantly better than the 2.40 kcal/mol from BoB,
which was the previous state-of-the-art for the
QM7 data set. 34

We next use the QM9 data set to explore
the extent to which models trained on smaller
molecules may be transferred to larger molecules.
KRR models, using the features of Table 2, were
trained to the 3,993 molecules of the QM9 data
set that have seven or fewer heavy atoms. Two-
fold cross validation was used to select hyperpa-
rameters and the average MAE of the two folds,
computed using the selected hyperparameters,
is listed in the column labeled CV (for cross val-
idation) in Table 2. The remaining columns list
the MAE as molecules from the QM9 data set
are added to the small molecules used to train
the model. Because the molecules are sorted
by size, the average molecular size increases as
molecules are added. A useful quantitative mea-
sure of this increase is provided by the MAE of
the null model, which reports on the spread of

9

Table 2: MAE of atomization energies for KRR models that use the listed feature vectors, for both
the QM7 and QM9 data sets. The QM9 model is trained to molecules with 7 or fewer heavy atoms
(3,993 molecules), yielding the cross validation error listed under CV. The remaining columns apply
the model to subsets of QM9 with increasing molecular size. The lowest MAE for each column is
shown in bold.

feature
Null
Coulomb matrix
BoB
1
2B
3B
12B
12B3
12B3B
12B34
12B34B
12B3B4
12B3B4B
2LC
2N P
12LC
12LC3
12LC3B
12LC34
12LC34B
12LC3B4
12LC3B4B
12N P
12N P 3B
12N P 3B4B

QM7
179.01
3.37
2.40
14.58
7.69
6.36
6.88
5.28
4.51
3.64
4.13
3.40
3.87
2.74
2.45
1.68
1.61
1.43
1.73
1.46
1.56
1.45
1.45
1.19
1.25

CV
189.45
3.83
2.43
15.46
7.52
6.84
6.37
4.19
4.25
3.68
3.81
3.56
3.69
2.10
1.65
1.57
1.44
1.28
1.38
1.34
1.24
1.31
1.14
1.05
1.16

First X thousand
molecules of QM9
20
290.57
43.78
9.68
18.20
10.42
8.26
8.75
6.53
6.34
5.70
6.10
5.43
5.92
3.26
2.65
2.42
2.32
2.03
2.21
1.89
1.98
1.83
2.03
1.63
1.62

50
322.72
66.56
22.21
21.65
16.67
15.39
11.11
10.03
8.36
9.18
9.90
8.64
9.53
5.83
5.18
4.20
4.13
3.72
4.36
3.77
3.93
3.66
3.84
3.37
3.44

133
425.12
106.09
29.99
20.99
15.73
15.73
11.17
9.99
8.59
9.89
10.51
9.42
10.25
5.76
4.71
4.09
3.95
3.60
4.21
3.52
3.77
3.41
3.51
3.05
3.30

10

Figure 2: MAE of LRR (left) and KRR (right) models of QM7 atomization energies using each of
the features listed in the legend, as a function of Gmax in Eq. (10).

the atomization energies. The MAE of the null
model for the entire QM9 data set is 2.4 times
that of the smaller molecules of QM7.

We ﬁrst consider models that use only con-
nectivity features. For simple models based on
features that count elements (feature 1) or bond
types (feature 2B) the MAE for QM9 is 1.4 and
2.0 times that of QM7, respectively. This is less
than the factor of 2.4 seen for the null model
(Table 2) and suggests that the average atom-
ization energy per element, or average strength
of a given bond type, does not change substan-
tially with molecular size. Inclusion of higher-
rank connectivity counts substantially improves
model performance, with 12B3B4 leading to
the best performance on both QM7 and QM9.
With 12B3B4, the MAE of QM9 is 2.8 times
that of QM7, only slightly larger than the 2.4
increase in the null model.

Not surprisingly, models that use the Coulomb
matrix and BoB features do not transfer well
to larger molecules. The MAEs of such models
increase by over a factor of ten as one moves
from small molecules to the entire QM9 data set.
In addition, the MAEs on the full QM9 data set

are larger than those from models based either
on atom counts, 1, or bond counts, 2B.

Good transfer to larger molecules is, on the
other hand, obtained for models based on en-
coded distance features. For models based on
the 2LC or 2N P feature alone, the MAE for
QM9 relative to QM7 increases by factors of 2.1
and 1.9, respectively, which is comparable to
or less than the 2.4 increase of the null model.
This advantage is retained upon inclusion of
connectivity features of diﬀerent rank. For both
12N P 3Band 12N P 3B4B, the MAE of QM9 is
2.6 times that of QM7, which is only slightly
larger than the 2.4 increase in the null model.
The degree to which various feature vectors
are useful for creating models of other target
properties is explored using the QM7b data set.
Table 3 lists the MAE from BoB models de-
veloped here, along with results reported from
use of the Coulomb matrix 16 and REMatch-
SOAP kernel. 23 These are compared to mod-
els developed using 12N P 3B, which gives the
best performance on QM7 atomization ener-
gies (Table 2), and 12N P 3B4B, which extends
this to rank 4 and gives comparable results on

11

Table 3: Comparison of models for the properties in the QM7b data set. CM and SOAP lists MAEs
reported for the Coulomb Matrix 16 and REMatch-SOAP. 23 BoB is from models developed here
using BoB features as in Section 4. 12N P 3Band 12N P 3B4B list MAEs from models developed
using feature vectors of the type developed in this work. QM lists the estimated reliability of the
quantum chemical calculations used to create the QM7b data set. 16

Property
E (PBE0) (eV)
3
α (PBE0) (˚A
)
3
α (SCS) (˚A
)
HOMO (GW) (eV)
HOMO (PBE0) (eV)
HOMO (ZINDO) (eV)
LUMO (GW) (eV)
LUMO (PBE0) (eV)
LUMO (ZINDO) (eV)
IP (ZINDO) (eV)
EA (ZINDO) (eV)
E∗
1st (ZINDO) (eV)
E∗
max (ZINDO) (eV)
Imax (ZINDO) (Arb.)
a 0.15 eV from PBE0, MAE of formation enthalpy for the G3/99 set. 50,51

Mean Absolute Errors
Null 12N P 3B 12N P 3B4B CM 16 BoBh SOAP 23
0.04
0.05
7.69
0.05
0.07
1.04
0.02
0.06
1.15
0.12
0.14
0.54
0.11
0.12
0.49
0.13
0.13
0.78
0.12
0.13
0.31
0.08
0.09
0.53
0.10
0.10
1.08
0.19
0.18
0.78
0.13
0.11
1.17
0.18
0.28
1.54
1.56
1.27
2.62
0.08
0.07
0.15

0.16
0.11
0.08
0.16
0.15
0.15
0.13
0.12
0.11
0.17
0.11
0.13
1.06
0.07

0.04
0.06
0.05
0.13
0.12
0.13
0.13
0.09
0.10
0.16
0.09
0.23
1.23
0.06

0.08
0.08
0.04
0.14
0.13
0.14
0.15
0.11
0.14
0.19
0.15
0.20
1.30
0.08

0.23 eV from PBE0, MAE of atomization energy for six small molecules. 52,53
0.09-0.22 eV from B3LYP, MAE of atomization energy from various studies. 54

QM
0.09-0.23a
0.04-0.27b
0.04-0.27b
–
2.08d
0.79e
–
1.30e
0.93e
0.20, 0.15c
0.16f, 0.11c
0.18f, 0.21g
–
–

b 0.05-0.27 ˚A
0.04-0.14 ˚A

3

3

from B3LYP, MAE from various studies. 54
from MP2, MAE from various studies. 54

c B3LYP, MAE from various studies. 54
d MAE from GW values. 16
e ZINDO, MAE for a set of 17 retinal analogues. 55
f PBE0, MAE for the G3/99 set. 50,51
g TD-DFT(PBE0), MAE for a set of 17 retinal analogues. 55
h Calculations done in this work for comparison.

12

Figure 3: Eﬀects of adding higher-rank features
to concatenated feature vectors for models of
QM7 atomization energies. Solid lines are fea-
tures beginning with 12B, requiring only atom
connectivity. Dotted lines begin with 12LC and
thus encode distances from 3D geometries. For
ranks 3 and higher, red lines show addition of
features that distinguish bond order (3B, 4B,
or 5B) versus simply bond existence (3, 4, or
5).

QM7b. For the remaining studies reported be-
low, which include larger molecules and larger
data sets, we use the longer 12N P 3B4B fea-
ture. The results show that the performance
with 12N P 3B and 12N P 3B4B is comparable to
BoB and REMatch-SOAP, all of which lead to
predictions whose MAEs are at least as low as
the typical deviation of the quantum methods
from experiment used to generate the QM7b
data set (QM in Table 3).

The learning rate of models based on various
feature vectors are shown in Figure 4. Simi-
lar learning rates are obtained across feature
vectors, with the decrease in MAE for mod-
els based on simple bond counting, 2B, being
similar to those of BoB, 2LC, and 12N P 3B4B.
Figure 5 provides additional insight by showing
the MAE for both the training and test data.
The point at which the train and test error con-

Figure 4: MAE of atomization energy predic-
tions, as a function of the number of examples
included from the QM9 data set. The error bars,
which are often smaller than the line width, show
the standard deviation from ﬁve experiments,
each of which sampled molecules at random from
the full data set. Similar learning rates are ob-
tained for all feature vectors. (Hyperparameters
are listed in the Supporting Information.)

verge indicates the limit beyond which inclusion
of additional data may no longer be expected
to improve model performance. The model
based on the concatenated 12N P 3B4B feature
approaches this convergence signiﬁcantly more
slowly than the single 2B or 2LC features, sug-
gesting that the increase in performance of
12N P 3B4B comes at the cost of a somewhat
slower learning rate. For BoB, the training error
is nearly zero, suggesting that inclusion of addi-
tional data has the potential to further reduce
error.

The performance and learning rate of models,
based on the 12N P 3B4B feature vector, that
target a range of properties in the QM9 data set
are shown in Table 4. Performance and learning
rate on all thermodynamic energies are roughly
equivalent, giving MAEs of 1.6 kcal/mol. The
heat capacity is also well predicted, with an
MAE of 0.10 cal/(mol K) that is 2% that of the
null model. Models of the HOMO and LUMO
energies, and the HOMO-LUMO gap, also lead
to reasonable performance, with MAEs below

13

Figure 5: Comparison of test and train errors as a function of number of examples included from
the QM9 data set. The error bars, which are often smaller than the line width, show the standard
deviation from ﬁve experiments, each of which sampled molecules at random from the full data set.

Table 4: MAEs for KRR models that use the 12N P 3B4B feature vector to target properties in the
QM9 data set. The null column indicates the spread of the data. The columns are for training on X
molecules and testing on the remainder. The uncertainties are the standard deviations from four
experiments that sampled the X training molecules at random from the entire data set.

Property
µ (Debye)
α (Bohr3)
HOMO (eV)
LUMO (eV)
Gap (eV)
(cid:104)R2(cid:105) (Bohr2)
zpve (kcal/mol)
U0 (kcal/mol)
U (kcal/mol)
H (kcal/mol)
G (kcal/mol)
Cv (cal/(mol K))

Null
1.19
6.30
0.44
1.05
1.08
202.52
16.63
188.47
190.18
191.55
173.30
4.95

500
0.92(1)
0.98(4)
0.25(0)
0.39(1)
0.44(0)
86(3)
0.29(1)
7.28(33)
7.31(33)
7.32(33)
7.08(31)
0.32(0)

1000
0.86(1)
0.77(2)
0.22(1)
0.32(2)
0.39(2)
71(2)
0.21(1)
4.67(10)
4.69(10)
4.69(10)
4.58(7)
0.24(0)

Sampled Molecules
4000
0.76(1)
0.60(4)
0.17(0)
0.22(0)
0.28(0)
50.40(54)
0.14(0)
2.75(5)
2.77(5)
2.77(5)
2.73(4)
0.16(0)

2000
0.83(1)
0.66(1)
0.19(0)
0.26(0)
0.32(0)
59.41(91)
0.17(0)
3.64(13)
3.63(11)
3.63(11)
3.57(11)
0.20(0)

8000
0.68(0)
0.49(2)
0.15(0)
0.18(0)
0.24(0)
44.94(35)
0.11(0)
2.08(5)
2.09(5)
2.09(5)
2.06(5)
0.12(0)

16000
0.63(0)
0.41(1)
0.12(0)
0.15(0)
0.19(0)
36.31(19)
0.09(0)
1.58(2)
1.59(2)
1.59(2)
1.56(2)
0.10(0)

14

0.2 eV.

The performance on dipole moment, µ, is rel-
atively poor, with the 12N P 3B4B model re-
ducing the MAE of the null model by only
about one half. This may be due to µ having a
strong dependence on the overall geometry of
the molecule. Signiﬁcantly better performance
is obtained for the polarizability, α, where MAE
of the 12N P 3B4B model is 6.5% that of the
null model. Although polarizability is a global
property, it is primarily dependent on the total
volume of the molecule and thus is likely well
modeled as a sum of fragment properties. The
dipole moment, on the other hand, depends on
the detailed global arrangement of the atoms.

7 Discussion

This paper introduces feature vectors for ma-
chine learning of molecular properties whose
length depends on the diversity of molecules in
the data set, e.g. the number of elements and
bond types, but is independent of the size of the
molecules. These features lead to models whose
performance on atomization energies, and other
properties tabulated in well studied data sets, is
comparable to or better than previous models
based on the Coulomb matrix or BoB features.
In contrast to these previous models, the fea-
tures introduced here allow models trained on
small molecules to be applied successfully to
larger molecules.

Although models based solely on connectivity
counts would be ideal, because they require only
the information present in a SMILES string,
such models do not perform well. This re-
ﬂects the fact that chemistry is more complex
than simple Lewis structures—even for organic
molecules. When information regarding the
3D geometry of the molecule is included via
encoded distance features, the model perfor-
mance improves substantially and this perfor-
mance enhancement is retained upon transfer
from smaller to larger molecules.

The current work focuses on KRR models, for
which the features are used to compute distances
between pairs of molecules. These distances are
then passed through a kernel for use in the re-

gression. The above results suggest that the
features developed here are useful for comput-
ing distances between molecules, when coupled
to the standard kernels employed in machine
learning. The development of kernels that bet-
ter describe molecular similarity is left to future
work. The degree to which the features intro-
duced here lead to improved performance in deep
learning and other ML models also remains to
be explored; although, we note that the models
with the best current performance on the data
sets studied here use KRR.

8 Acknowledgements

The authors thank Haichen Li for helpful dis-
cussions. The authors would also like to thank
Raghunathan Ramakrishnan for helpful discus-
sions on details regarding the BoB model and the
data sets studied here. This work was supported
by the National Science Foundation under grant
CHE-1027985.

9 Supporting Information

Available

Supporting information includes the following:
distances between atoms used to deﬁne the bond
types of Section 5.3; details regarding the im-
plementation of the encoded distance features
of Section 5.4; and a listing of the MAE and
hyperparameters from LRR and KRR models of
the QM7 data set, obtained using a wide variety
of diﬀerent feature vectors.

References

(1) Lill, M. A. Drug Discovery Today 2007,

12, 1013–1017.

(2) Lusci, A.; Pollastri, G.; Baldi, P. J. Chem.

Inf. Model. 2013, 53, 1563–1575.

(3) Wang, X.; Wong, L.; Hu, L.; Chan, C.;
Su, Z.; Chen, G. J. Phys. Chem. A 2004,
108, 8514–8525.

15

(4) Rupp, M.; Tkatchenko, A.; M¨uller, K.-R.;
von Lilienfeld, O. A. Phys. Rev. Lett. 2012,
108, 058301.

(5) Hansen, K.; Montavon, G.; Biegler, F.; Fa-
zli, S.; Rupp, M.; Scheﬄer, M.; von Lilien-
feld, O. A.; Tkatchenko, A.; M¨uller, K.-R.
J. Chem. Theory Comput. 2013, 9, 3404–
3419.

(6) Montavon, G.; Hansen, K.; Fazli, S.;
Ziehe, A.;
Rupp, M.; Biegler, F.;
Tkatchenko, A.;
Lilienfeld, A. V.;
M¨uller, K.-R. Learning Invariant Repre-
sentations of Molecules for Atomization
Energy Prediction. Advances in Neural
Information Processing Systems. 2012; pp
440–448.

(7) Bart´ok, A. P.; Kondor, R.; Cs´anyi, G. Phys.
Rev. B: Condens. Matter Mater. Phys.
2013, 87, 184115.

(8) Qu, X.; Latino, D. A.; Aires-de Sousa, J.

J. Cheminf. 2013, 5, 34.

(9) Meredig, B.; Agrawal, A.; Kirklin, S.;
Saal, J. E.; Doak, J.; Thompson, A.;
Zhang, K.; Choudhary, A.; Wolverton, C.
Phys. Rev. B: Condens. Matter Mater.
Phys. 2014, 89, 094104.

(10) Behler, J. Phys. Chem. Chem. Phys. 2011,

13, 17930–17955.

(11) Barker, J.; Bulin, J.; Hamaekers, J.; Math-

ias, S. 2016,

(12) Hirn, M.; Poilvert, N.; Mallat, S. arXiv

preprint arXiv:1502.02077 2015,

(13) Olivares-Amaya, R.; Amador-Bedolla, C.;
Hachmann,
S.;
S´anchez-Carrera, R. S.; Vogt, L.; Aspuru-
Guzik, A. Energy Environ. Sci. 2011, 4,
4849–4861.

Atahan-Evrenk,

J.;

(14) Hachmann,

J.; Olivares-Amaya, R.;
Jinich, A.; Appleton, A. L.; Blood-
Forsythe, M. A.; Seress, L. R.; Rom´an-
Salgado, C.; Trepte, K.; Atahan-Evrenk, S.;
Shrestha, S.; Mondal, R.;
Er, S.;

Sokolov, A.; Bao, Z.; Aspuru-Guzik, A.
Energy Environ. Sci. 2014, 7, 698–704.

(15) O’Boyle, N. M.; Campbell, C. M.; Hutchi-
son, G. R. J. Phys. Chem. C 2011, 115,
16200–16210.

(16) Montavon, G.; Rupp, M.; Gobre, V.;
Vazquez-Mayagoitia, A.; Hansen, K.;
Tkatchenko, A.; M¨uller, K.-R.; von Lilien-
feld, O. A. New J. Phys. 2013, 15, 095003.

(17) Ramakrishnan, R.; Dral, P. O.; Rupp, M.;
von Lilienfeld, O. A. J. Chem. Theory
Comput. 2015, 11, 2087–2096.

(18) Ramakrishnan, R.;

Hartmann, M.;
Tapavicza, E.; von Lilienfeld, O. A. J.
Chem. Phys. 2015, 143 .

(19) Ramakrishnan, R.; von Lilienfeld, O. A.
CHIMIA Internationaltional Journal for
Chemistry 2015, 69, 182–186.

(20) von Lilienfeld, O. A.; Ramakrishnan, R.;
Rupp, M.; Knoll, A. Int. J. Quantum
Chem. 2015, 115, 1084–1093.

(21) Rupp, M.; Ramakrishnan, R.; von Lilien-
feld, O. A. J. Phys. Chem. Lett. 2015, 6,
3309–3313.

(22) Lopez-Bezanilla, A.; von Lilienfeld, O. A.
Phys. Rev. B: Condens. Matter Mater.
Phys. 2014, 89, 235411.

(23) De, S.; Bart´ok, A. P.; Cs´anyi, G.; Ceri-
otti, M. Phys. Chem. Chem. Phys. 2016,
18, 13754–13769.

(24) Ghiringhelli,

J.;
L. M.;
Levchenko, S. V.; Draxl, C.; Schef-
ﬂer, M. Phys. Rev. Lett. 2015, 114,
105503.

Vybiral,

(25) Kusne, A. G.; Gao, T.; Mehta, A.; Ke, L.;
Nguyen, M. C.; Ho, K.-M.; Antropov, V.;
Wang, C.-Z.; Kramer, M. J.; Long, C.;
Takeuchi, I. Sci. Rep. 2014, 4 .

(26) Isayev, O.; Fourches, D.; Muratov, E. N.;
Oses, C.; Rasch, K.; Tropsha, A.; Cur-
tarolo, S. Chem. Mater. 2015, 27, 735–743.

16

(27) Sadowski, P.; Baldi, P. J. Chem. Inf. Model.

(41) Behler, J.; Parrinello, M. Phys. Rev. Lett.

2013, 53, 3127–3130.

2007, 98, 146401.

(28) Nagata, K.; Randall, A.; Baldi, P. Proteins:
Struct., Funct., Bioinf. 2012, 80, 142–153.

(42) Gastegger, M.; Marquetand, P. J. Chem.
Theory Comput. 2015, 11, 2187–2198.

(29) Carr, S.; Garnett, R.; Lo, C. BASC: Apply-
ing Bayesian Optimization to the Search
for Global Minima on Potential Energy
Surfaces. International Conference on Ma-
chine Learning. 2016; pp 898–907.

(30) Wong, E.; Kolter, J. Z. An SVD and
Derivative Kernel Approach to Learning
from Geometric Data. Association for
the Advancement of Artiﬁcial Intelligence.
2015; pp 1889–1895.

(31) Ediz, V.; Monda, A. C.; Brown, R. P.;
Yaron, D. J. J. Chem. Theory Comput.
2009, 5, 3175–3184.

(32) Sun, J.; Wu, J.; Song, T.; Hu, L.; Shan, K.;
Chen, G. J. Phys. Chem. A 2014, 118,
9120–9131.

(33) Weininger, D. J. Chem. Inf. Comput. Sci.

1988, 28, 31–36.

(34) Hansen, K.; Biegler, F.; Ramakrishnan, R.;
Pronobis, W.; von Lilienfeld, O. A.;
M¨uller, K.-R.; Tkatchenko, A. J. Phys.
Chem. Lett. 2015, 6, 2326–2331.

(35) Morgan, H. L. J. Chem. Doc. 1965, 5, 107–

113.

059801.

(36) Moussa, J. E. Phys. Rev. Lett. 2012, 109,

(37) Huan, T. D.; Mannodi-Kanakkithodi, A.;
Ramprasad, R. Phys. Rev. B: Condens.
Matter Mater. Phys. 2015, 92, 014106.

(38) R¨ucker, G.; R¨ucker, C. J. Chem. Inf. Com-

put. Sci. 1993, 33, 683–695.

put. Sci. 2010, 50, 742–754.

(40) Mah´e, P.; Ueda, N.; Akutsu, T.; Perret, J.-
L.; Vert, J.-P. J. Chem. Inf. Comput. Sci.
2005, 45, 939–951.

(43) Gastegger, M.; Kauﬀmann, C.; Behler, J.;
Marquetand, P. J. Chem. Phys. 2016, 144 .

(44) Seko, A.; Takahashi, A.; Tanaka, I. Phys.
Rev. B: Condens. Matter Mater. Phys.
2015, 92, 054113.

(45) Blum, L. C.; Reymond, J.-L. J. Am. Chem.

Soc. 2009, 131, 8732.

(46) Ruddigkeit, L.;

van Deursen, R.;
Blum, L. C.; Reymond, J.-L. J. Chem. Inf.
Comput. Sci. 2012, 52, 2864–2875.

(47) Ramakrishnan, R.; Dral, P. O.; Rupp, M.;

von Lilienfeld, O. A. Sci. Data 2014, 1 .

(48) Pedregosa, F.; Varoquaux, G.; Gram-
fort, A.; Michel, V.; Thirion, B.; Grisel, O.;
Blondel, M.; Prettenhofer, P.; Weiss, R.;
Dubourg, V.; Vanderplas, J.; Passos, A.;
Cournapeau, D.; Brucher, M.; Perrot, M.;
Duchesnay, E. Journal of Machine Learn-
ing Research 2011, 12, 2825–2830.

(49) Bing, H.; von Lilienfeld, O. A. arXiv

preprint arXiv:1502.02077 2016,

(50) Staroverov, V. N.; Scuseria, G. E.; Tao, J.;
Perdew, J. P. J. Chem. Phys. 2003, 119,
12129–12137.

(51) Curtiss, L. A.;
Raghavachari, K.
2005, 123, 124107.

Redfern, P. C.;
J. Chem. Phys.

(52) Zhao, Y.; Pu, J.; Lynch, B. J.; Truh-
lar, D. G. Phys. Chem. Chem. Phys. 2004,
6, 673–676.

(53) Lynch, B. J.; Truhlar, D. G. J. Phys. Chem.

(54) Koch, W.; Holthausen, M. C. A Chemist’s
Guide to Density Functional Theory; John
Wiley & Sons, 2015.

(39) Rogers, D.; Hahn, M. J. Chem. Inf. Com-

A 2003, 107, 8996–8999.

17

(55) L´opez, C. S.; Faza, O. N.; Est´evez, S. L.;
de Lera, A. R. J. Comput. Chem. 2006,
27, 116–123.

18

