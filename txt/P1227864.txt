6
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
3
3
6
8
0
.
4
0
6
1
:
v
i
X
r
a

Word Ordering Without Syntax

Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber
Harvard University
{schmaltz@fas,srush@seas,shieber@seas}.harvard.edu

Abstract

neural networks via, in part, the network’s ability to
predict word order in simple sentences. He notes,

Recent work on word ordering has argued that
syntactic structure is important, or even re-
quired, for effectively recovering the order of
a sentence. We ﬁnd that, in fact, an n-gram
language model with a simple heuristic gives
strong results on this task. Furthermore, we
show that a long short-term memory (LSTM)
language model is even more effective at re-
covering order, with our basic model outper-
forming a state-of-the-art syntactic model by
11.5 BLEU points. Additional data and larger
beams yield further gains, at the expense of
training and search time.

1 Introduction

recovering the origi-
We address the task of
nal word order of a shufﬂed sentence,
referred
to as bag generation (Brown et al., 1990), shake-
and-bake generation (Brew, 1992), or more re-
linearization, as standardized in a recent
cently,
line of
iso-
research as a method useful
lating the performance of text-to-text generation
models (Zhang and Clark, 2011; Liu et al., 2015;
Liu and Zhang, 2015; Zhang and Clark, 2015). The
predominant argument of the more recent works is
that jointly recovering explicit syntactic structure is
crucial for determining the correct word order of the
original sentence. As such, these methods either
generate or rely on given parse structure to repro-
duce the order.

for

Independently, Elman (1990) explored lineariza-
tion in his seminal work on recurrent neural net-
works. Elman judged the capacity of early recurrent

The order of words in sentences reﬂects a num-
ber of constraints. . . Syntactic structure, selective
restrictions, subcategorization, and discourse con-
siderations are among the many factors which
join together to ﬁx the order in which words oc-
cur. . . [T]here is an abstract structure which un-
derlies the surface strings and it is this structure
which provides a more insightful basis for under-
standing the constraints on word order. . . . It is,
therefore, an interesting question to ask whether a
network can learn any aspects of that underlying
abstract structure (Elman, 1990).

neural

recurrent

networks

Recently,

have
reemerged as a powerful tool for learning the latent
structure of language.
In particular, work on long
short-term memory (LSTM) networks for language
modeling has provided improvements in perplexity.
We revisit Elman’s question by applying LSTMs
to the word-ordering task, without any explicit syn-
tactic modeling. We ﬁnd that language models are
in general effective for linearization relative to ex-
isting syntactic approaches, with LSTMs in particu-
lar outperforming the state-of-the-art by 11.5 BLEU
points, with further gains observed when training
with additional text and decoding with larger beams.

2 Background: Linearization

The task of linearization is to recover the original or-
der of a shufﬂed sentence. We assume a vocabulary
V and are given a sequence of out-of-order phrases
x1, . . . , xN , with xn ∈ V + for 1 ≤ n ≤ N . Deﬁne
M as the total number of tokens (i.e., the sum of the
lengths of the phrases). We consider two varieties of

the task: (1) WORDS, where each xn consists of a
single word and M = N , and (2) WORDS+BNPS,
where base noun phrases (noun phrases not con-
taining inner noun phrases) are also provided and
M ≥ N . The second has become a standard for-
mulation in recent literature.

Given input x, we deﬁne the output set Y to be
all possible permutations over the N elements of x,
where ˆy ∈ Y is the permutation generating the true
order. We aim to ﬁnd ˆy, or a permutation close to
it. We produce a linearization by (approximately)
optimizing a learned scoring function f over the set
of permutations, y∗ = arg maxy∈Y f (x, y).

3 Related Work: Syntactic Linearization

Recent approaches to linearization have been based
on reconstructing the syntactic structure to produce
the word order. Let Z represent all projective de-
pendency parse trees over M words. The objec-
tive is to ﬁnd y∗, z∗ = arg maxy∈Y,z∈Z f (x, y, z)
where f is now over both the syntactic structure
and the linearization. The current state of the art
on the Penn Treebank (PTB) (Marcus et al., 1993),
without external data, of Liu et al. (2015) uses a
transition-based parser with beam search to con-
struct a sentence and a parse tree. The scoring func-
tion is a linear model f (x, y) = θ⊤Φ(x, y, z) and
is trained with an early update structured percep-
tron to match both a given order and syntactic tree.
The feature function Φ includes features on the syn-
tactic tree. This work improves upon past work
which used best-ﬁrst search over a similar objective
(Zhang and Clark, 2011).

In follow-up work, Liu and Zhang (2015) argue
that syntactic models yield improvements over pure
surface n-gram models for the WORDS+BNPS case.
This result holds particularly on longer sentences
and even when the syntactic trees used in training
are of low quality. The n-gram decoder of this work
utilizes a single beam, discarding the probabilities
of internal, non-boundary words in the BNPs when
comparing hypotheses. We revisit this comparison
between syntactic models and surface-level models,
utilizing a surface-level decoder with heuristic fu-
ture costs and an alternative approach for scoring
partial hypotheses for the WORDS+BNPS case.

The
gram models for the word ordering task.
work of de Gispert et al. (2014) demonstrates im-
provements over the earlier syntactic model of
Zhang et al. (2012) by applying an n-gram language
model over the space of word permutations re-
stricted to concatenations of phrases seen in a large
corpus. Horvat and Byrne (2014) models the search
for the highest probability permutation of words un-
der an n-gram model as a Travelling Salesman Prob-
lem; however, direct comparisons to existing works
are not provided.

4 LM-Based Linearization

In contrast to the recent syntax-based approaches,
we use an LM directly for word ordering. We
consider two types of language models:
an n-
gram model and a long short-term memory network
(Hochreiter and Schmidhuber, 1997). For the pur-
pose of this work, we deﬁne a common abstraction
for both models. Let h ∈ H be the current state of
the model, with h0 as the initial state. Upon seeing
a word wi ∈ V, the LM advances to a new state
hi = δ(wi, hi−1). At any time, the LM can be
queried to produce an estimate of the probability of
the next word q(wi, hi−1) ≈ p(wi | w1, . . . , wi−1).
For n-gram language models, H, δ, and q can natu-
rally be deﬁned respectively as the state space, tran-
sition model, and edge costs of a ﬁnite-state ma-
chine.

LSTMs are a type of recurrent neural network
(RNN) that are conducive to learning long-distance
dependencies through the use of an internal mem-
ory cell. Existing work with LSTMs has gener-
ated state-of-the-art results in language modeling
(Zaremba et al., 2014), along with a variety of other
NLP tasks.

In our notation we deﬁne H as the hidden states
and cell states of a multi-layer LSTM, δ as the
LSTM update function, and q as a ﬁnal afﬁne trans-
formation and softmax given as q(∗, hi−1; θq) =
softmax(W h(L)
i−1 + b) where h(L)
i−1 is the top hid-
den layer and θq = (W , b) are parameters. We di-
rect readers to the work of Graves (2013) for a full
description of the LSTM update.

Additional previous work has also explored n-

For both models, we simply deﬁne the scoring

Algorithm 1 LM beam-search word ordering
1: procedure ORDER(x1 . . . xN , K, g)
2: B0 ← h(hi, {1, . . . , N }, 0, h0)i
for m = 0, . . . , M − 1 do
3:
for k = 1, . . . , |Bm| do
4:
(y, R, s, h) ← B(k)
m
for i ∈ R do
(s′, h′
for word w in phrase xi do
s′ ← s′ + log q(w, h′
h′

) ← (s, h)

5:
6:
7:
8:
9:
10:

← δ(w, h′

)

)

11:
12:
13:

14:

return BM

j ← m + |xi|
Bj ← Bj + (y + xi, R − i, s′, h′
keep top-K of Bj by f (x, y) + g(R)

)

function as

N

X
n=1

f (x, y) =

log p(xy(n) | xy(1), . . . , xy(n−1))

where the phrase probabilities are calculated word-
by-word by our language model.

Searching over all permutations Y is
in-
tractable, so we instead follow past work on lin-
earization (Liu et al., 2015) and LSTM generation
(Sutskever et al., 2014) in adapting beam search for
our generation step. Our work differs from the beam
search approach for the WORDS+BNPS case of pre-
vious work in that we maintain multiple beams, as
in stack decoding for phrase-based machine trans-
lation (Koehn, 2010), allowing us to incorporate
the probabilities of internal, non-boundary words
in the BNPs. Additionally, for both WORDS and
WORDS+BNPS, we also include an estimate of fu-
ture cost in order to improve search accuracy.
Beam search maintains M + 1
beams,
B0, . . . , BM , each containing at most
the top-
K partial hypotheses of that
length. A partial
hypothesis is a 4-tuple (y, R, s, h), where y is a
partial ordering, R is the set of remaining indices to
be ordered, s is the score of the partial linearization
f (x, y), and h is the current LM state. Each step
consists of expanding all next possible phrases and
adding the next hypothesis to a later beam. The full
beam search is given in Algorithm 1.

As part of the beam search scoring function we
also include a future cost g, an estimate of the score

WORDS WORDS+BNPS

Model

ZGEN-64
ZGEN-64+POS

NGRAM-64 (NO g)
NGRAM-64
NGRAM-512
LSTM-64
LSTM-512

ZGEN-64+LM+GW+POS
LSTM-64+GW
LSTM-512+GW

30.9
–

32.0
37.0
38.6
40.5
42.7

–
41.1
44.5

49.4
50.8

51.3
54.3
55.6
60.9
63.2

52.4
63.1
65.8

Table 1: BLEU score comparison on the PTB test
set. Results from previous works (for ZGEN) are
those provided by the respective authors, except for
the WORDS task. The ﬁnal number in the model
identiﬁer is the beam size, +GW indicates additional
Gigaword data. Models marked with +POS are pro-
vided with a POS dictionary derived from the PTB
training set.

contribution of the remaining elements in R. To-
gether, f (x, y) + g(R) gives a noisy estimate of the
total score, which is used to determine the K best
elements in the beam. In our experiments we use a
very simple unigram future cost estimate, g(R) =
Pi∈R Pw∈xi log p(w).

5 Experiments

Setup Experiments are on PTB with sections 2-
21 as training, 22 as validation, and 23 as test1.
We utilize two UNK types, one for initial upper-
case tokens and one for all other low-frequency to-
kens; end sentence tokens; and start/end tokens,
which are treated as words, to mark BNPs for the
WORDS+BNPS task. We also use a special symbol
to replace tokens that contain at least one numeric
character. We otherwise train with punctuation and
the original case of each token, resulting in a vocab-
ulary containing around 16K types from around 1M
training tokens.

For experiments marked GW we augment

the
PTB with a subset of
the Annotated Giga-
word corpus (Napoles et al., 2012). We follow

1In

practice,

and
Liu and Zhang (2015) use section 0 instead of 22 for vali-
dation (author correspondence).

in Liu et al. (2015)

results

the

BNP

g

GW

1

10

64

128

256

512

•
•
•

•
•

•

•

•
•

•
•

•

•

LSTM

53.6
59.4
60.1
26.8
36.8
35.5

49.7
53.6
27.1
34.6

58.0
62.2
64.2
33.8
40.7
40.7

52.6
55.6
32.6
37.5

59.1
62.9
64.9
35.3
41.7
41.7

53.2
56.2
33.8
38.1

NGRAM

41.7
47.6
48.4
15.4
25.0
23.8

40.6
45.7
14.6
27.1

60.0
63.6
65.6
36.5
42.0
42.9

54.0
56.6
35.1
38.4

60.6
64.3
66.2
38.0
42.5
43.7

54.7
56.6
35.8
38.7

Table 2: BLEU results on the validation set for
the LSTM and NGRAM model with varying beam
sizes, future costs, additional data, and use of base
noun phrases.

Liu and Zhang (2015) and train on a sample of 900k
Agence France-Presse sentences combined with the
The GW models beneﬁt
full PTB training set.
from both additional data and a larger vocabulary of
around 25K types, which reduces unknowns in the
validation and test sets.

We compare the models of Liu et al. (2015)
(known as ZGEN), a 5-gram LM using Kneser-Ney
smoothing (NGRAM)2, and an LSTM. We experi-
ment on the WORDS and WORDS+BNPS tasks, and
we also experiment with including future costs (g),
the Gigaword data (GW), and varying beam size.
We retrain ZGEN using publicly available code3 to
replicate published results.
is
The LSTM model

in size and
similar
architecture
to the medium LSTM setup of
Zaremba et al. (2014)4. Our implementation uses
the Torch5 framework and is publicly available6.

We compare the performance of the models using
the BLEU metric (Papineni et al., 2002). In gener-
ation if there are multiple tokens of identical UNK
type, we randomly replace each with possible un-
used tokens in the original source before calculating
BLEU. For comparison purposes, we use the BLEU

2We use the KenLM Language Model Toolkit (https://

kheafield.com/code/kenlm/).

3https://github.com/SUTDNLP/ZGen
4We hypothesize that additional gains are possible via a

larger model and model averaging, ceteris paribus.

5http://torch.ch
6https://github.com/allenschmaltz/word_

ordering

LSTM-512
LSTM-64
ZGen-64
LSTM-1

ZGen-64
LSTM-64

5

10

15

20

25
Sentence length

30

35

40

)

%

(
U
E
L
B

90

70

50

30

s
n
e
k
o
T

12000

8000

4000

0
0.0

0.2

0.4
0.6
Distortion rate

0.8

1.0

Figure 1: Experiments on the PTB validation on the
WORDS+BNPS models: (a) Performance on the set
by length on sentences from length 2 to length 40.
(b) The distribution of token distortion, binned at 0.1
intervals.

script distributed with the publicly available ZGEN
code.

Results Our main results are shown in Table 1.
On the WORDS+BNPS task the NGRAM-64 model
scores nearly 5 points higher than the syntax-
based model ZGEN-64.
The LSTM-64 then
surpasses NGRAM-64 by more than 5 BLEU
Differences on the WORDS task are
points.
Incorporat-
smaller, but show a similar pattern.
ing Gigaword further increases the result another 2
points. Notably, the NGRAM model outperforms the
combined result of ZGEN-64+LM+GW+POS from
Liu and Zhang (2015), which uses a 4-gram model
trained on Gigaword. We believe this is because
the combined ZGEN model incorporates the n-gram
scores as discretized indicator features instead of us-
ing the probability directly.7 A beam of 512 yields a
further improvement at the cost of search time.

To further explore the impact of search accuracy,

7In work of Liu and Zhang (2015), with the given decoder,
N-grams only yielded a small further improvement over the syn-
tactic models when discretized versions of the LM probabilities
were incorporated as indicator features in the syntactic models.

WORDS WORDS+BNPS

els, for both high- and low- resource languages and
domains.

Model
ZGEN-64(z∗)
ZGEN-64

NGRAM-64
NGRAM-512
LSTM-64
LSTM-512

39.7
40.8

46.1
47.2
51.3
52.8

64.9
65.2

67.0
67.8
71.9
73.1

Table 3: Unlabeled attachment scores (UAS) on
the PTB validation set after parsing and aligning
the output. For ZGEN we also include a result us-
ing the tree z∗ produced directly by the system.
For WORDS+BNPS, internal BNP arcs are always
counted as correct.

Table 2 shows the results of various models with
beam widths ranging from 1 (greedy search) to 512,
and also with and without future costs g. We see that
for the better models there is a steady increase in ac-
curacy even with large beams, indicating that search
errors are made even with relatively large beams.

One proposed advantage of syntax in lineariza-
tion models is that it can better capture long-distance
relationships. Figure 1 shows results by sentence
length and distortion, which is deﬁned as the abso-
lute difference between a token’s index position in
y∗ and ˆy, normalized by M . The LSTM model ex-
hibits consistently better performance than existing
syntax models across sentence lengths and generates
fewer long-range distortions than the ZGEN model.
Finally, Table 3 compares the syntactic ﬂuency of
the output. As a lightweight test, we parse the output
with the Yara Parser (Rasooli and Tetreault, 2015)
and compare the unlabeled attachment scores (UAS)
to the trees produced by the syntactic system. We
ﬁrst align the gold head to each output token.
(In
cases where the alignment is not one-to-one, we ran-
domly sample among the possibilities.) The models
with no knowledge of syntax are able to recover a
higher proportion of gold arcs.

6 Conclusion

Strong surface-level language models recover word
order more accurately than the models trained with
explicit syntactic annotations appearing in a recent
series of papers. This has implications for the utility
of costly syntactic annotations in generation mod-

Acknowledgments

We thank Yue Zhang and Jiangming Liu for assis-
tance in using ZGen, as well as veriﬁcation of the
task setup for a valid comparison. Jiangming Liu
also assisted in pointing out a discrepancy in the im-
plementation of an earlier version of our NGRAM
decoder, the resolution of which improved BLEU
performance.

References

[Brew1992] Chris Brew. 1992. Letting the cat out of the
bag: Generation for shake-and-bake MT. In Proceed-
ings of the 14th Conference on Computational Lin-
guistics - Volume 2, COLING ’92, pages 610–616,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

[Brown et al.1990] Peter F Brown, John Cocke, Stephen
A Della Pietra, Vincent J Della Pietra, Fredrick Je-
linek, John D Lafferty, Robert L Mercer, and Paul S
Roossin. 1990. A statistical approach to machine
translation. Computational linguistics, 16(2):79–85.
[de Gispert et al.2014] Adri`a de Gispert, Marcus Tomalin,
and Bill Byrne. 2014. Word ordering with phrase-
based grammars. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 259–268, Gothen-
burg, Sweden, April. Association for Computational
Linguistics.

[Elman1990] Jeffrey L. Elman. 1990. Finding structure

in time. Cognitive Science, 14(2):179 – 211.

[Graves2013] Alex Graves.

2013.

quences with recurrent neural networks.
abs/1308.0850.

Generating se-
CoRR,

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Comput., 9(8):1735–1780, November.

[Horvat and Byrne2014] Matic Horvat

and William
Byrne.
2014. A graph-based approach to string
regeneration. In Proceedings of the Student Research
Workshop at the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 85–95, Gothenburg, Sweden, April.
Association for Computational Linguistics.

[Koehn2010] Philipp Koehn. 2010. Statistical Machine
Translation. Cambridge University Press, New York,
NY, USA, 1st edition.

[Liu and Zhang2015] Jiangming Liu and Yue Zhang.
2015. An empirical comparison between n-gram and
syntactic language models for word ordering. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 369–378,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

[Zhang et al.2012] Yue Zhang, Graeme Blackwood, and
Stephen Clark. 2012. Syntax-based word ordering in-
corporating a large-scale language model. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ’12, pages 736–746, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Liu et al.2015] Yijia Liu, Yue Zhang, Wanxiang Che, and
Bing Qin. 2015. Transition-based syntactic lineariza-
tion.
In Proceedings of the 2015 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 113–122, Denver, Colorado, May–June.
Association for Computational Linguistics.

[Marcus et al.1993] Mitchell P. Marcus, Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: The penn tree-
bank. Computational Linguistics, 19(2):313–330.
[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, AKBC-WEKEX ’12, pages
95–100, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2002. Bleu: A
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ’02,
pages 311–318, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Rasooli and Tetreault2015] Mohammad Sadegh Rasooli
and Joel R. Tetreault. 2015. Yara parser: A fast and
accurate dependency parser. CoRR, abs/1503.06733.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Zaremba et al.2014] Wojciech Zaremba, Ilya Sutskever,
and Oriol Vinyals. 2014. Recurrent neural network
regularization. CoRR, abs/1409.2329.

[Zhang and Clark2011] Yue Zhang and Stephen Clark.
2011. Syntax-based grammaticality improvement us-
ing ccg and guided search. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’11, pages 1147–1157, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Zhang and Clark2015] Yue Zhang and Stephen Clark.
2015. Discriminative syntax-based word ordering for
text generation. Comput. Linguist., 41(3):503–538,
September.

6
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
3
3
6
8
0
.
4
0
6
1
:
v
i
X
r
a

Word Ordering Without Syntax

Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber
Harvard University
{schmaltz@fas,srush@seas,shieber@seas}.harvard.edu

Abstract

neural networks via, in part, the network’s ability to
predict word order in simple sentences. He notes,

Recent work on word ordering has argued that
syntactic structure is important, or even re-
quired, for effectively recovering the order of
a sentence. We ﬁnd that, in fact, an n-gram
language model with a simple heuristic gives
strong results on this task. Furthermore, we
show that a long short-term memory (LSTM)
language model is even more effective at re-
covering order, with our basic model outper-
forming a state-of-the-art syntactic model by
11.5 BLEU points. Additional data and larger
beams yield further gains, at the expense of
training and search time.

1 Introduction

recovering the origi-
We address the task of
nal word order of a shufﬂed sentence,
referred
to as bag generation (Brown et al., 1990), shake-
and-bake generation (Brew, 1992), or more re-
linearization, as standardized in a recent
cently,
line of
iso-
research as a method useful
lating the performance of text-to-text generation
models (Zhang and Clark, 2011; Liu et al., 2015;
Liu and Zhang, 2015; Zhang and Clark, 2015). The
predominant argument of the more recent works is
that jointly recovering explicit syntactic structure is
crucial for determining the correct word order of the
original sentence. As such, these methods either
generate or rely on given parse structure to repro-
duce the order.

for

Independently, Elman (1990) explored lineariza-
tion in his seminal work on recurrent neural net-
works. Elman judged the capacity of early recurrent

The order of words in sentences reﬂects a num-
ber of constraints. . . Syntactic structure, selective
restrictions, subcategorization, and discourse con-
siderations are among the many factors which
join together to ﬁx the order in which words oc-
cur. . . [T]here is an abstract structure which un-
derlies the surface strings and it is this structure
which provides a more insightful basis for under-
standing the constraints on word order. . . . It is,
therefore, an interesting question to ask whether a
network can learn any aspects of that underlying
abstract structure (Elman, 1990).

neural

recurrent

networks

Recently,

have
reemerged as a powerful tool for learning the latent
structure of language.
In particular, work on long
short-term memory (LSTM) networks for language
modeling has provided improvements in perplexity.
We revisit Elman’s question by applying LSTMs
to the word-ordering task, without any explicit syn-
tactic modeling. We ﬁnd that language models are
in general effective for linearization relative to ex-
isting syntactic approaches, with LSTMs in particu-
lar outperforming the state-of-the-art by 11.5 BLEU
points, with further gains observed when training
with additional text and decoding with larger beams.

2 Background: Linearization

The task of linearization is to recover the original or-
der of a shufﬂed sentence. We assume a vocabulary
V and are given a sequence of out-of-order phrases
x1, . . . , xN , with xn ∈ V + for 1 ≤ n ≤ N . Deﬁne
M as the total number of tokens (i.e., the sum of the
lengths of the phrases). We consider two varieties of

the task: (1) WORDS, where each xn consists of a
single word and M = N , and (2) WORDS+BNPS,
where base noun phrases (noun phrases not con-
taining inner noun phrases) are also provided and
M ≥ N . The second has become a standard for-
mulation in recent literature.

Given input x, we deﬁne the output set Y to be
all possible permutations over the N elements of x,
where ˆy ∈ Y is the permutation generating the true
order. We aim to ﬁnd ˆy, or a permutation close to
it. We produce a linearization by (approximately)
optimizing a learned scoring function f over the set
of permutations, y∗ = arg maxy∈Y f (x, y).

3 Related Work: Syntactic Linearization

Recent approaches to linearization have been based
on reconstructing the syntactic structure to produce
the word order. Let Z represent all projective de-
pendency parse trees over M words. The objec-
tive is to ﬁnd y∗, z∗ = arg maxy∈Y,z∈Z f (x, y, z)
where f is now over both the syntactic structure
and the linearization. The current state of the art
on the Penn Treebank (PTB) (Marcus et al., 1993),
without external data, of Liu et al. (2015) uses a
transition-based parser with beam search to con-
struct a sentence and a parse tree. The scoring func-
tion is a linear model f (x, y) = θ⊤Φ(x, y, z) and
is trained with an early update structured percep-
tron to match both a given order and syntactic tree.
The feature function Φ includes features on the syn-
tactic tree. This work improves upon past work
which used best-ﬁrst search over a similar objective
(Zhang and Clark, 2011).

In follow-up work, Liu and Zhang (2015) argue
that syntactic models yield improvements over pure
surface n-gram models for the WORDS+BNPS case.
This result holds particularly on longer sentences
and even when the syntactic trees used in training
are of low quality. The n-gram decoder of this work
utilizes a single beam, discarding the probabilities
of internal, non-boundary words in the BNPs when
comparing hypotheses. We revisit this comparison
between syntactic models and surface-level models,
utilizing a surface-level decoder with heuristic fu-
ture costs and an alternative approach for scoring
partial hypotheses for the WORDS+BNPS case.

The
gram models for the word ordering task.
work of de Gispert et al. (2014) demonstrates im-
provements over the earlier syntactic model of
Zhang et al. (2012) by applying an n-gram language
model over the space of word permutations re-
stricted to concatenations of phrases seen in a large
corpus. Horvat and Byrne (2014) models the search
for the highest probability permutation of words un-
der an n-gram model as a Travelling Salesman Prob-
lem; however, direct comparisons to existing works
are not provided.

4 LM-Based Linearization

In contrast to the recent syntax-based approaches,
we use an LM directly for word ordering. We
consider two types of language models:
an n-
gram model and a long short-term memory network
(Hochreiter and Schmidhuber, 1997). For the pur-
pose of this work, we deﬁne a common abstraction
for both models. Let h ∈ H be the current state of
the model, with h0 as the initial state. Upon seeing
a word wi ∈ V, the LM advances to a new state
hi = δ(wi, hi−1). At any time, the LM can be
queried to produce an estimate of the probability of
the next word q(wi, hi−1) ≈ p(wi | w1, . . . , wi−1).
For n-gram language models, H, δ, and q can natu-
rally be deﬁned respectively as the state space, tran-
sition model, and edge costs of a ﬁnite-state ma-
chine.

LSTMs are a type of recurrent neural network
(RNN) that are conducive to learning long-distance
dependencies through the use of an internal mem-
ory cell. Existing work with LSTMs has gener-
ated state-of-the-art results in language modeling
(Zaremba et al., 2014), along with a variety of other
NLP tasks.

In our notation we deﬁne H as the hidden states
and cell states of a multi-layer LSTM, δ as the
LSTM update function, and q as a ﬁnal afﬁne trans-
formation and softmax given as q(∗, hi−1; θq) =
softmax(W h(L)
i−1 + b) where h(L)
i−1 is the top hid-
den layer and θq = (W , b) are parameters. We di-
rect readers to the work of Graves (2013) for a full
description of the LSTM update.

Additional previous work has also explored n-

For both models, we simply deﬁne the scoring

Algorithm 1 LM beam-search word ordering
1: procedure ORDER(x1 . . . xN , K, g)
2: B0 ← h(hi, {1, . . . , N }, 0, h0)i
for m = 0, . . . , M − 1 do
3:
for k = 1, . . . , |Bm| do
4:
(y, R, s, h) ← B(k)
m
for i ∈ R do
(s′, h′
for word w in phrase xi do
s′ ← s′ + log q(w, h′
h′

) ← (s, h)

5:
6:
7:
8:
9:
10:

← δ(w, h′

)

)

11:
12:
13:

14:

return BM

j ← m + |xi|
Bj ← Bj + (y + xi, R − i, s′, h′
keep top-K of Bj by f (x, y) + g(R)

)

function as

N

X
n=1

f (x, y) =

log p(xy(n) | xy(1), . . . , xy(n−1))

where the phrase probabilities are calculated word-
by-word by our language model.

Searching over all permutations Y is
in-
tractable, so we instead follow past work on lin-
earization (Liu et al., 2015) and LSTM generation
(Sutskever et al., 2014) in adapting beam search for
our generation step. Our work differs from the beam
search approach for the WORDS+BNPS case of pre-
vious work in that we maintain multiple beams, as
in stack decoding for phrase-based machine trans-
lation (Koehn, 2010), allowing us to incorporate
the probabilities of internal, non-boundary words
in the BNPs. Additionally, for both WORDS and
WORDS+BNPS, we also include an estimate of fu-
ture cost in order to improve search accuracy.
Beam search maintains M + 1
beams,
B0, . . . , BM , each containing at most
the top-
K partial hypotheses of that
length. A partial
hypothesis is a 4-tuple (y, R, s, h), where y is a
partial ordering, R is the set of remaining indices to
be ordered, s is the score of the partial linearization
f (x, y), and h is the current LM state. Each step
consists of expanding all next possible phrases and
adding the next hypothesis to a later beam. The full
beam search is given in Algorithm 1.

As part of the beam search scoring function we
also include a future cost g, an estimate of the score

WORDS WORDS+BNPS

Model

ZGEN-64
ZGEN-64+POS

NGRAM-64 (NO g)
NGRAM-64
NGRAM-512
LSTM-64
LSTM-512

ZGEN-64+LM+GW+POS
LSTM-64+GW
LSTM-512+GW

30.9
–

32.0
37.0
38.6
40.5
42.7

–
41.1
44.5

49.4
50.8

51.3
54.3
55.6
60.9
63.2

52.4
63.1
65.8

Table 1: BLEU score comparison on the PTB test
set. Results from previous works (for ZGEN) are
those provided by the respective authors, except for
the WORDS task. The ﬁnal number in the model
identiﬁer is the beam size, +GW indicates additional
Gigaword data. Models marked with +POS are pro-
vided with a POS dictionary derived from the PTB
training set.

contribution of the remaining elements in R. To-
gether, f (x, y) + g(R) gives a noisy estimate of the
total score, which is used to determine the K best
elements in the beam. In our experiments we use a
very simple unigram future cost estimate, g(R) =
Pi∈R Pw∈xi log p(w).

5 Experiments

Setup Experiments are on PTB with sections 2-
21 as training, 22 as validation, and 23 as test1.
We utilize two UNK types, one for initial upper-
case tokens and one for all other low-frequency to-
kens; end sentence tokens; and start/end tokens,
which are treated as words, to mark BNPs for the
WORDS+BNPS task. We also use a special symbol
to replace tokens that contain at least one numeric
character. We otherwise train with punctuation and
the original case of each token, resulting in a vocab-
ulary containing around 16K types from around 1M
training tokens.

For experiments marked GW we augment

the
PTB with a subset of
the Annotated Giga-
word corpus (Napoles et al., 2012). We follow

1In

practice,

and
Liu and Zhang (2015) use section 0 instead of 22 for vali-
dation (author correspondence).

in Liu et al. (2015)

results

the

BNP

g

GW

1

10

64

128

256

512

•
•
•

•
•

•

•

•
•

•
•

•

•

LSTM

53.6
59.4
60.1
26.8
36.8
35.5

49.7
53.6
27.1
34.6

58.0
62.2
64.2
33.8
40.7
40.7

52.6
55.6
32.6
37.5

59.1
62.9
64.9
35.3
41.7
41.7

53.2
56.2
33.8
38.1

NGRAM

41.7
47.6
48.4
15.4
25.0
23.8

40.6
45.7
14.6
27.1

60.0
63.6
65.6
36.5
42.0
42.9

54.0
56.6
35.1
38.4

60.6
64.3
66.2
38.0
42.5
43.7

54.7
56.6
35.8
38.7

Table 2: BLEU results on the validation set for
the LSTM and NGRAM model with varying beam
sizes, future costs, additional data, and use of base
noun phrases.

Liu and Zhang (2015) and train on a sample of 900k
Agence France-Presse sentences combined with the
The GW models beneﬁt
full PTB training set.
from both additional data and a larger vocabulary of
around 25K types, which reduces unknowns in the
validation and test sets.

We compare the models of Liu et al. (2015)
(known as ZGEN), a 5-gram LM using Kneser-Ney
smoothing (NGRAM)2, and an LSTM. We experi-
ment on the WORDS and WORDS+BNPS tasks, and
we also experiment with including future costs (g),
the Gigaword data (GW), and varying beam size.
We retrain ZGEN using publicly available code3 to
replicate published results.
is
The LSTM model

in size and
similar
architecture
to the medium LSTM setup of
Zaremba et al. (2014)4. Our implementation uses
the Torch5 framework and is publicly available6.

We compare the performance of the models using
the BLEU metric (Papineni et al., 2002). In gener-
ation if there are multiple tokens of identical UNK
type, we randomly replace each with possible un-
used tokens in the original source before calculating
BLEU. For comparison purposes, we use the BLEU

2We use the KenLM Language Model Toolkit (https://

kheafield.com/code/kenlm/).

3https://github.com/SUTDNLP/ZGen
4We hypothesize that additional gains are possible via a

larger model and model averaging, ceteris paribus.

5http://torch.ch
6https://github.com/allenschmaltz/word_

ordering

LSTM-512
LSTM-64
ZGen-64
LSTM-1

ZGen-64
LSTM-64

5

10

15

20

25
Sentence length

30

35

40

)

%

(
U
E
L
B

90

70

50

30

s
n
e
k
o
T

12000

8000

4000

0
0.0

0.2

0.4
0.6
Distortion rate

0.8

1.0

Figure 1: Experiments on the PTB validation on the
WORDS+BNPS models: (a) Performance on the set
by length on sentences from length 2 to length 40.
(b) The distribution of token distortion, binned at 0.1
intervals.

script distributed with the publicly available ZGEN
code.

Results Our main results are shown in Table 1.
On the WORDS+BNPS task the NGRAM-64 model
scores nearly 5 points higher than the syntax-
based model ZGEN-64.
The LSTM-64 then
surpasses NGRAM-64 by more than 5 BLEU
Differences on the WORDS task are
points.
Incorporat-
smaller, but show a similar pattern.
ing Gigaword further increases the result another 2
points. Notably, the NGRAM model outperforms the
combined result of ZGEN-64+LM+GW+POS from
Liu and Zhang (2015), which uses a 4-gram model
trained on Gigaword. We believe this is because
the combined ZGEN model incorporates the n-gram
scores as discretized indicator features instead of us-
ing the probability directly.7 A beam of 512 yields a
further improvement at the cost of search time.

To further explore the impact of search accuracy,

7In work of Liu and Zhang (2015), with the given decoder,
N-grams only yielded a small further improvement over the syn-
tactic models when discretized versions of the LM probabilities
were incorporated as indicator features in the syntactic models.

WORDS WORDS+BNPS

els, for both high- and low- resource languages and
domains.

Model
ZGEN-64(z∗)
ZGEN-64

NGRAM-64
NGRAM-512
LSTM-64
LSTM-512

39.7
40.8

46.1
47.2
51.3
52.8

64.9
65.2

67.0
67.8
71.9
73.1

Table 3: Unlabeled attachment scores (UAS) on
the PTB validation set after parsing and aligning
the output. For ZGEN we also include a result us-
ing the tree z∗ produced directly by the system.
For WORDS+BNPS, internal BNP arcs are always
counted as correct.

Table 2 shows the results of various models with
beam widths ranging from 1 (greedy search) to 512,
and also with and without future costs g. We see that
for the better models there is a steady increase in ac-
curacy even with large beams, indicating that search
errors are made even with relatively large beams.

One proposed advantage of syntax in lineariza-
tion models is that it can better capture long-distance
relationships. Figure 1 shows results by sentence
length and distortion, which is deﬁned as the abso-
lute difference between a token’s index position in
y∗ and ˆy, normalized by M . The LSTM model ex-
hibits consistently better performance than existing
syntax models across sentence lengths and generates
fewer long-range distortions than the ZGEN model.
Finally, Table 3 compares the syntactic ﬂuency of
the output. As a lightweight test, we parse the output
with the Yara Parser (Rasooli and Tetreault, 2015)
and compare the unlabeled attachment scores (UAS)
to the trees produced by the syntactic system. We
ﬁrst align the gold head to each output token.
(In
cases where the alignment is not one-to-one, we ran-
domly sample among the possibilities.) The models
with no knowledge of syntax are able to recover a
higher proportion of gold arcs.

6 Conclusion

Strong surface-level language models recover word
order more accurately than the models trained with
explicit syntactic annotations appearing in a recent
series of papers. This has implications for the utility
of costly syntactic annotations in generation mod-

Acknowledgments

We thank Yue Zhang and Jiangming Liu for assis-
tance in using ZGen, as well as veriﬁcation of the
task setup for a valid comparison. Jiangming Liu
also assisted in pointing out a discrepancy in the im-
plementation of an earlier version of our NGRAM
decoder, the resolution of which improved BLEU
performance.

References

[Brew1992] Chris Brew. 1992. Letting the cat out of the
bag: Generation for shake-and-bake MT. In Proceed-
ings of the 14th Conference on Computational Lin-
guistics - Volume 2, COLING ’92, pages 610–616,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

[Brown et al.1990] Peter F Brown, John Cocke, Stephen
A Della Pietra, Vincent J Della Pietra, Fredrick Je-
linek, John D Lafferty, Robert L Mercer, and Paul S
Roossin. 1990. A statistical approach to machine
translation. Computational linguistics, 16(2):79–85.
[de Gispert et al.2014] Adri`a de Gispert, Marcus Tomalin,
and Bill Byrne. 2014. Word ordering with phrase-
based grammars. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 259–268, Gothen-
burg, Sweden, April. Association for Computational
Linguistics.

[Elman1990] Jeffrey L. Elman. 1990. Finding structure

in time. Cognitive Science, 14(2):179 – 211.

[Graves2013] Alex Graves.

2013.

quences with recurrent neural networks.
abs/1308.0850.

Generating se-
CoRR,

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Comput., 9(8):1735–1780, November.

[Horvat and Byrne2014] Matic Horvat

and William
Byrne.
2014. A graph-based approach to string
regeneration. In Proceedings of the Student Research
Workshop at the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 85–95, Gothenburg, Sweden, April.
Association for Computational Linguistics.

[Koehn2010] Philipp Koehn. 2010. Statistical Machine
Translation. Cambridge University Press, New York,
NY, USA, 1st edition.

[Liu and Zhang2015] Jiangming Liu and Yue Zhang.
2015. An empirical comparison between n-gram and
syntactic language models for word ordering. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 369–378,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

[Zhang et al.2012] Yue Zhang, Graeme Blackwood, and
Stephen Clark. 2012. Syntax-based word ordering in-
corporating a large-scale language model. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ’12, pages 736–746, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Liu et al.2015] Yijia Liu, Yue Zhang, Wanxiang Che, and
Bing Qin. 2015. Transition-based syntactic lineariza-
tion.
In Proceedings of the 2015 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 113–122, Denver, Colorado, May–June.
Association for Computational Linguistics.

[Marcus et al.1993] Mitchell P. Marcus, Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: The penn tree-
bank. Computational Linguistics, 19(2):313–330.
[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, AKBC-WEKEX ’12, pages
95–100, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2002. Bleu: A
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ’02,
pages 311–318, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Rasooli and Tetreault2015] Mohammad Sadegh Rasooli
and Joel R. Tetreault. 2015. Yara parser: A fast and
accurate dependency parser. CoRR, abs/1503.06733.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Zaremba et al.2014] Wojciech Zaremba, Ilya Sutskever,
and Oriol Vinyals. 2014. Recurrent neural network
regularization. CoRR, abs/1409.2329.

[Zhang and Clark2011] Yue Zhang and Stephen Clark.
2011. Syntax-based grammaticality improvement us-
ing ccg and guided search. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’11, pages 1147–1157, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Zhang and Clark2015] Yue Zhang and Stephen Clark.
2015. Discriminative syntax-based word ordering for
text generation. Comput. Linguist., 41(3):503–538,
September.

6
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
3
3
6
8
0
.
4
0
6
1
:
v
i
X
r
a

Word Ordering Without Syntax

Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber
Harvard University
{schmaltz@fas,srush@seas,shieber@seas}.harvard.edu

Abstract

neural networks via, in part, the network’s ability to
predict word order in simple sentences. He notes,

Recent work on word ordering has argued that
syntactic structure is important, or even re-
quired, for effectively recovering the order of
a sentence. We ﬁnd that, in fact, an n-gram
language model with a simple heuristic gives
strong results on this task. Furthermore, we
show that a long short-term memory (LSTM)
language model is even more effective at re-
covering order, with our basic model outper-
forming a state-of-the-art syntactic model by
11.5 BLEU points. Additional data and larger
beams yield further gains, at the expense of
training and search time.

1 Introduction

recovering the origi-
We address the task of
nal word order of a shufﬂed sentence,
referred
to as bag generation (Brown et al., 1990), shake-
and-bake generation (Brew, 1992), or more re-
linearization, as standardized in a recent
cently,
line of
iso-
research as a method useful
lating the performance of text-to-text generation
models (Zhang and Clark, 2011; Liu et al., 2015;
Liu and Zhang, 2015; Zhang and Clark, 2015). The
predominant argument of the more recent works is
that jointly recovering explicit syntactic structure is
crucial for determining the correct word order of the
original sentence. As such, these methods either
generate or rely on given parse structure to repro-
duce the order.

for

Independently, Elman (1990) explored lineariza-
tion in his seminal work on recurrent neural net-
works. Elman judged the capacity of early recurrent

The order of words in sentences reﬂects a num-
ber of constraints. . . Syntactic structure, selective
restrictions, subcategorization, and discourse con-
siderations are among the many factors which
join together to ﬁx the order in which words oc-
cur. . . [T]here is an abstract structure which un-
derlies the surface strings and it is this structure
which provides a more insightful basis for under-
standing the constraints on word order. . . . It is,
therefore, an interesting question to ask whether a
network can learn any aspects of that underlying
abstract structure (Elman, 1990).

neural

recurrent

networks

Recently,

have
reemerged as a powerful tool for learning the latent
structure of language.
In particular, work on long
short-term memory (LSTM) networks for language
modeling has provided improvements in perplexity.
We revisit Elman’s question by applying LSTMs
to the word-ordering task, without any explicit syn-
tactic modeling. We ﬁnd that language models are
in general effective for linearization relative to ex-
isting syntactic approaches, with LSTMs in particu-
lar outperforming the state-of-the-art by 11.5 BLEU
points, with further gains observed when training
with additional text and decoding with larger beams.

2 Background: Linearization

The task of linearization is to recover the original or-
der of a shufﬂed sentence. We assume a vocabulary
V and are given a sequence of out-of-order phrases
x1, . . . , xN , with xn ∈ V + for 1 ≤ n ≤ N . Deﬁne
M as the total number of tokens (i.e., the sum of the
lengths of the phrases). We consider two varieties of

the task: (1) WORDS, where each xn consists of a
single word and M = N , and (2) WORDS+BNPS,
where base noun phrases (noun phrases not con-
taining inner noun phrases) are also provided and
M ≥ N . The second has become a standard for-
mulation in recent literature.

Given input x, we deﬁne the output set Y to be
all possible permutations over the N elements of x,
where ˆy ∈ Y is the permutation generating the true
order. We aim to ﬁnd ˆy, or a permutation close to
it. We produce a linearization by (approximately)
optimizing a learned scoring function f over the set
of permutations, y∗ = arg maxy∈Y f (x, y).

3 Related Work: Syntactic Linearization

Recent approaches to linearization have been based
on reconstructing the syntactic structure to produce
the word order. Let Z represent all projective de-
pendency parse trees over M words. The objec-
tive is to ﬁnd y∗, z∗ = arg maxy∈Y,z∈Z f (x, y, z)
where f is now over both the syntactic structure
and the linearization. The current state of the art
on the Penn Treebank (PTB) (Marcus et al., 1993),
without external data, of Liu et al. (2015) uses a
transition-based parser with beam search to con-
struct a sentence and a parse tree. The scoring func-
tion is a linear model f (x, y) = θ⊤Φ(x, y, z) and
is trained with an early update structured percep-
tron to match both a given order and syntactic tree.
The feature function Φ includes features on the syn-
tactic tree. This work improves upon past work
which used best-ﬁrst search over a similar objective
(Zhang and Clark, 2011).

In follow-up work, Liu and Zhang (2015) argue
that syntactic models yield improvements over pure
surface n-gram models for the WORDS+BNPS case.
This result holds particularly on longer sentences
and even when the syntactic trees used in training
are of low quality. The n-gram decoder of this work
utilizes a single beam, discarding the probabilities
of internal, non-boundary words in the BNPs when
comparing hypotheses. We revisit this comparison
between syntactic models and surface-level models,
utilizing a surface-level decoder with heuristic fu-
ture costs and an alternative approach for scoring
partial hypotheses for the WORDS+BNPS case.

The
gram models for the word ordering task.
work of de Gispert et al. (2014) demonstrates im-
provements over the earlier syntactic model of
Zhang et al. (2012) by applying an n-gram language
model over the space of word permutations re-
stricted to concatenations of phrases seen in a large
corpus. Horvat and Byrne (2014) models the search
for the highest probability permutation of words un-
der an n-gram model as a Travelling Salesman Prob-
lem; however, direct comparisons to existing works
are not provided.

4 LM-Based Linearization

In contrast to the recent syntax-based approaches,
we use an LM directly for word ordering. We
consider two types of language models:
an n-
gram model and a long short-term memory network
(Hochreiter and Schmidhuber, 1997). For the pur-
pose of this work, we deﬁne a common abstraction
for both models. Let h ∈ H be the current state of
the model, with h0 as the initial state. Upon seeing
a word wi ∈ V, the LM advances to a new state
hi = δ(wi, hi−1). At any time, the LM can be
queried to produce an estimate of the probability of
the next word q(wi, hi−1) ≈ p(wi | w1, . . . , wi−1).
For n-gram language models, H, δ, and q can natu-
rally be deﬁned respectively as the state space, tran-
sition model, and edge costs of a ﬁnite-state ma-
chine.

LSTMs are a type of recurrent neural network
(RNN) that are conducive to learning long-distance
dependencies through the use of an internal mem-
ory cell. Existing work with LSTMs has gener-
ated state-of-the-art results in language modeling
(Zaremba et al., 2014), along with a variety of other
NLP tasks.

In our notation we deﬁne H as the hidden states
and cell states of a multi-layer LSTM, δ as the
LSTM update function, and q as a ﬁnal afﬁne trans-
formation and softmax given as q(∗, hi−1; θq) =
softmax(W h(L)
i−1 + b) where h(L)
i−1 is the top hid-
den layer and θq = (W , b) are parameters. We di-
rect readers to the work of Graves (2013) for a full
description of the LSTM update.

Additional previous work has also explored n-

For both models, we simply deﬁne the scoring

Algorithm 1 LM beam-search word ordering
1: procedure ORDER(x1 . . . xN , K, g)
2: B0 ← h(hi, {1, . . . , N }, 0, h0)i
for m = 0, . . . , M − 1 do
3:
for k = 1, . . . , |Bm| do
4:
(y, R, s, h) ← B(k)
m
for i ∈ R do
(s′, h′
for word w in phrase xi do
s′ ← s′ + log q(w, h′
h′

) ← (s, h)

5:
6:
7:
8:
9:
10:

← δ(w, h′

)

)

11:
12:
13:

14:

return BM

j ← m + |xi|
Bj ← Bj + (y + xi, R − i, s′, h′
keep top-K of Bj by f (x, y) + g(R)

)

function as

N

X
n=1

f (x, y) =

log p(xy(n) | xy(1), . . . , xy(n−1))

where the phrase probabilities are calculated word-
by-word by our language model.

Searching over all permutations Y is
in-
tractable, so we instead follow past work on lin-
earization (Liu et al., 2015) and LSTM generation
(Sutskever et al., 2014) in adapting beam search for
our generation step. Our work differs from the beam
search approach for the WORDS+BNPS case of pre-
vious work in that we maintain multiple beams, as
in stack decoding for phrase-based machine trans-
lation (Koehn, 2010), allowing us to incorporate
the probabilities of internal, non-boundary words
in the BNPs. Additionally, for both WORDS and
WORDS+BNPS, we also include an estimate of fu-
ture cost in order to improve search accuracy.
Beam search maintains M + 1
beams,
B0, . . . , BM , each containing at most
the top-
K partial hypotheses of that
length. A partial
hypothesis is a 4-tuple (y, R, s, h), where y is a
partial ordering, R is the set of remaining indices to
be ordered, s is the score of the partial linearization
f (x, y), and h is the current LM state. Each step
consists of expanding all next possible phrases and
adding the next hypothesis to a later beam. The full
beam search is given in Algorithm 1.

As part of the beam search scoring function we
also include a future cost g, an estimate of the score

WORDS WORDS+BNPS

Model

ZGEN-64
ZGEN-64+POS

NGRAM-64 (NO g)
NGRAM-64
NGRAM-512
LSTM-64
LSTM-512

ZGEN-64+LM+GW+POS
LSTM-64+GW
LSTM-512+GW

30.9
–

32.0
37.0
38.6
40.5
42.7

–
41.1
44.5

49.4
50.8

51.3
54.3
55.6
60.9
63.2

52.4
63.1
65.8

Table 1: BLEU score comparison on the PTB test
set. Results from previous works (for ZGEN) are
those provided by the respective authors, except for
the WORDS task. The ﬁnal number in the model
identiﬁer is the beam size, +GW indicates additional
Gigaword data. Models marked with +POS are pro-
vided with a POS dictionary derived from the PTB
training set.

contribution of the remaining elements in R. To-
gether, f (x, y) + g(R) gives a noisy estimate of the
total score, which is used to determine the K best
elements in the beam. In our experiments we use a
very simple unigram future cost estimate, g(R) =
Pi∈R Pw∈xi log p(w).

5 Experiments

Setup Experiments are on PTB with sections 2-
21 as training, 22 as validation, and 23 as test1.
We utilize two UNK types, one for initial upper-
case tokens and one for all other low-frequency to-
kens; end sentence tokens; and start/end tokens,
which are treated as words, to mark BNPs for the
WORDS+BNPS task. We also use a special symbol
to replace tokens that contain at least one numeric
character. We otherwise train with punctuation and
the original case of each token, resulting in a vocab-
ulary containing around 16K types from around 1M
training tokens.

For experiments marked GW we augment

the
PTB with a subset of
the Annotated Giga-
word corpus (Napoles et al., 2012). We follow

1In

practice,

and
Liu and Zhang (2015) use section 0 instead of 22 for vali-
dation (author correspondence).

in Liu et al. (2015)

results

the

BNP

g

GW

1

10

64

128

256

512

•
•
•

•
•

•

•

•
•

•
•

•

•

LSTM

53.6
59.4
60.1
26.8
36.8
35.5

49.7
53.6
27.1
34.6

58.0
62.2
64.2
33.8
40.7
40.7

52.6
55.6
32.6
37.5

59.1
62.9
64.9
35.3
41.7
41.7

53.2
56.2
33.8
38.1

NGRAM

41.7
47.6
48.4
15.4
25.0
23.8

40.6
45.7
14.6
27.1

60.0
63.6
65.6
36.5
42.0
42.9

54.0
56.6
35.1
38.4

60.6
64.3
66.2
38.0
42.5
43.7

54.7
56.6
35.8
38.7

Table 2: BLEU results on the validation set for
the LSTM and NGRAM model with varying beam
sizes, future costs, additional data, and use of base
noun phrases.

Liu and Zhang (2015) and train on a sample of 900k
Agence France-Presse sentences combined with the
The GW models beneﬁt
full PTB training set.
from both additional data and a larger vocabulary of
around 25K types, which reduces unknowns in the
validation and test sets.

We compare the models of Liu et al. (2015)
(known as ZGEN), a 5-gram LM using Kneser-Ney
smoothing (NGRAM)2, and an LSTM. We experi-
ment on the WORDS and WORDS+BNPS tasks, and
we also experiment with including future costs (g),
the Gigaword data (GW), and varying beam size.
We retrain ZGEN using publicly available code3 to
replicate published results.
is
The LSTM model

in size and
similar
architecture
to the medium LSTM setup of
Zaremba et al. (2014)4. Our implementation uses
the Torch5 framework and is publicly available6.

We compare the performance of the models using
the BLEU metric (Papineni et al., 2002). In gener-
ation if there are multiple tokens of identical UNK
type, we randomly replace each with possible un-
used tokens in the original source before calculating
BLEU. For comparison purposes, we use the BLEU

2We use the KenLM Language Model Toolkit (https://

kheafield.com/code/kenlm/).

3https://github.com/SUTDNLP/ZGen
4We hypothesize that additional gains are possible via a

larger model and model averaging, ceteris paribus.

5http://torch.ch
6https://github.com/allenschmaltz/word_

ordering

LSTM-512
LSTM-64
ZGen-64
LSTM-1

ZGen-64
LSTM-64

5

10

15

20

25
Sentence length

30

35

40

)

%

(
U
E
L
B

90

70

50

30

s
n
e
k
o
T

12000

8000

4000

0
0.0

0.2

0.4
0.6
Distortion rate

0.8

1.0

Figure 1: Experiments on the PTB validation on the
WORDS+BNPS models: (a) Performance on the set
by length on sentences from length 2 to length 40.
(b) The distribution of token distortion, binned at 0.1
intervals.

script distributed with the publicly available ZGEN
code.

Results Our main results are shown in Table 1.
On the WORDS+BNPS task the NGRAM-64 model
scores nearly 5 points higher than the syntax-
based model ZGEN-64.
The LSTM-64 then
surpasses NGRAM-64 by more than 5 BLEU
Differences on the WORDS task are
points.
Incorporat-
smaller, but show a similar pattern.
ing Gigaword further increases the result another 2
points. Notably, the NGRAM model outperforms the
combined result of ZGEN-64+LM+GW+POS from
Liu and Zhang (2015), which uses a 4-gram model
trained on Gigaword. We believe this is because
the combined ZGEN model incorporates the n-gram
scores as discretized indicator features instead of us-
ing the probability directly.7 A beam of 512 yields a
further improvement at the cost of search time.

To further explore the impact of search accuracy,

7In work of Liu and Zhang (2015), with the given decoder,
N-grams only yielded a small further improvement over the syn-
tactic models when discretized versions of the LM probabilities
were incorporated as indicator features in the syntactic models.

WORDS WORDS+BNPS

els, for both high- and low- resource languages and
domains.

Model
ZGEN-64(z∗)
ZGEN-64

NGRAM-64
NGRAM-512
LSTM-64
LSTM-512

39.7
40.8

46.1
47.2
51.3
52.8

64.9
65.2

67.0
67.8
71.9
73.1

Table 3: Unlabeled attachment scores (UAS) on
the PTB validation set after parsing and aligning
the output. For ZGEN we also include a result us-
ing the tree z∗ produced directly by the system.
For WORDS+BNPS, internal BNP arcs are always
counted as correct.

Table 2 shows the results of various models with
beam widths ranging from 1 (greedy search) to 512,
and also with and without future costs g. We see that
for the better models there is a steady increase in ac-
curacy even with large beams, indicating that search
errors are made even with relatively large beams.

One proposed advantage of syntax in lineariza-
tion models is that it can better capture long-distance
relationships. Figure 1 shows results by sentence
length and distortion, which is deﬁned as the abso-
lute difference between a token’s index position in
y∗ and ˆy, normalized by M . The LSTM model ex-
hibits consistently better performance than existing
syntax models across sentence lengths and generates
fewer long-range distortions than the ZGEN model.
Finally, Table 3 compares the syntactic ﬂuency of
the output. As a lightweight test, we parse the output
with the Yara Parser (Rasooli and Tetreault, 2015)
and compare the unlabeled attachment scores (UAS)
to the trees produced by the syntactic system. We
ﬁrst align the gold head to each output token.
(In
cases where the alignment is not one-to-one, we ran-
domly sample among the possibilities.) The models
with no knowledge of syntax are able to recover a
higher proportion of gold arcs.

6 Conclusion

Strong surface-level language models recover word
order more accurately than the models trained with
explicit syntactic annotations appearing in a recent
series of papers. This has implications for the utility
of costly syntactic annotations in generation mod-

Acknowledgments

We thank Yue Zhang and Jiangming Liu for assis-
tance in using ZGen, as well as veriﬁcation of the
task setup for a valid comparison. Jiangming Liu
also assisted in pointing out a discrepancy in the im-
plementation of an earlier version of our NGRAM
decoder, the resolution of which improved BLEU
performance.

References

[Brew1992] Chris Brew. 1992. Letting the cat out of the
bag: Generation for shake-and-bake MT. In Proceed-
ings of the 14th Conference on Computational Lin-
guistics - Volume 2, COLING ’92, pages 610–616,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

[Brown et al.1990] Peter F Brown, John Cocke, Stephen
A Della Pietra, Vincent J Della Pietra, Fredrick Je-
linek, John D Lafferty, Robert L Mercer, and Paul S
Roossin. 1990. A statistical approach to machine
translation. Computational linguistics, 16(2):79–85.
[de Gispert et al.2014] Adri`a de Gispert, Marcus Tomalin,
and Bill Byrne. 2014. Word ordering with phrase-
based grammars. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 259–268, Gothen-
burg, Sweden, April. Association for Computational
Linguistics.

[Elman1990] Jeffrey L. Elman. 1990. Finding structure

in time. Cognitive Science, 14(2):179 – 211.

[Graves2013] Alex Graves.

2013.

quences with recurrent neural networks.
abs/1308.0850.

Generating se-
CoRR,

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Comput., 9(8):1735–1780, November.

[Horvat and Byrne2014] Matic Horvat

and William
Byrne.
2014. A graph-based approach to string
regeneration. In Proceedings of the Student Research
Workshop at the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 85–95, Gothenburg, Sweden, April.
Association for Computational Linguistics.

[Koehn2010] Philipp Koehn. 2010. Statistical Machine
Translation. Cambridge University Press, New York,
NY, USA, 1st edition.

[Liu and Zhang2015] Jiangming Liu and Yue Zhang.
2015. An empirical comparison between n-gram and
syntactic language models for word ordering. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 369–378,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

[Zhang et al.2012] Yue Zhang, Graeme Blackwood, and
Stephen Clark. 2012. Syntax-based word ordering in-
corporating a large-scale language model. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ’12, pages 736–746, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Liu et al.2015] Yijia Liu, Yue Zhang, Wanxiang Che, and
Bing Qin. 2015. Transition-based syntactic lineariza-
tion.
In Proceedings of the 2015 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 113–122, Denver, Colorado, May–June.
Association for Computational Linguistics.

[Marcus et al.1993] Mitchell P. Marcus, Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: The penn tree-
bank. Computational Linguistics, 19(2):313–330.
[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, AKBC-WEKEX ’12, pages
95–100, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2002. Bleu: A
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ’02,
pages 311–318, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Rasooli and Tetreault2015] Mohammad Sadegh Rasooli
and Joel R. Tetreault. 2015. Yara parser: A fast and
accurate dependency parser. CoRR, abs/1503.06733.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Zaremba et al.2014] Wojciech Zaremba, Ilya Sutskever,
and Oriol Vinyals. 2014. Recurrent neural network
regularization. CoRR, abs/1409.2329.

[Zhang and Clark2011] Yue Zhang and Stephen Clark.
2011. Syntax-based grammaticality improvement us-
ing ccg and guided search. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’11, pages 1147–1157, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Zhang and Clark2015] Yue Zhang and Stephen Clark.
2015. Discriminative syntax-based word ordering for
text generation. Comput. Linguist., 41(3):503–538,
September.

6
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
3
3
6
8
0
.
4
0
6
1
:
v
i
X
r
a

Word Ordering Without Syntax

Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber
Harvard University
{schmaltz@fas,srush@seas,shieber@seas}.harvard.edu

Abstract

neural networks via, in part, the network’s ability to
predict word order in simple sentences. He notes,

Recent work on word ordering has argued that
syntactic structure is important, or even re-
quired, for effectively recovering the order of
a sentence. We ﬁnd that, in fact, an n-gram
language model with a simple heuristic gives
strong results on this task. Furthermore, we
show that a long short-term memory (LSTM)
language model is even more effective at re-
covering order, with our basic model outper-
forming a state-of-the-art syntactic model by
11.5 BLEU points. Additional data and larger
beams yield further gains, at the expense of
training and search time.

1 Introduction

recovering the origi-
We address the task of
nal word order of a shufﬂed sentence,
referred
to as bag generation (Brown et al., 1990), shake-
and-bake generation (Brew, 1992), or more re-
linearization, as standardized in a recent
cently,
line of
iso-
research as a method useful
lating the performance of text-to-text generation
models (Zhang and Clark, 2011; Liu et al., 2015;
Liu and Zhang, 2015; Zhang and Clark, 2015). The
predominant argument of the more recent works is
that jointly recovering explicit syntactic structure is
crucial for determining the correct word order of the
original sentence. As such, these methods either
generate or rely on given parse structure to repro-
duce the order.

for

Independently, Elman (1990) explored lineariza-
tion in his seminal work on recurrent neural net-
works. Elman judged the capacity of early recurrent

The order of words in sentences reﬂects a num-
ber of constraints. . . Syntactic structure, selective
restrictions, subcategorization, and discourse con-
siderations are among the many factors which
join together to ﬁx the order in which words oc-
cur. . . [T]here is an abstract structure which un-
derlies the surface strings and it is this structure
which provides a more insightful basis for under-
standing the constraints on word order. . . . It is,
therefore, an interesting question to ask whether a
network can learn any aspects of that underlying
abstract structure (Elman, 1990).

neural

recurrent

networks

Recently,

have
reemerged as a powerful tool for learning the latent
structure of language.
In particular, work on long
short-term memory (LSTM) networks for language
modeling has provided improvements in perplexity.
We revisit Elman’s question by applying LSTMs
to the word-ordering task, without any explicit syn-
tactic modeling. We ﬁnd that language models are
in general effective for linearization relative to ex-
isting syntactic approaches, with LSTMs in particu-
lar outperforming the state-of-the-art by 11.5 BLEU
points, with further gains observed when training
with additional text and decoding with larger beams.

2 Background: Linearization

The task of linearization is to recover the original or-
der of a shufﬂed sentence. We assume a vocabulary
V and are given a sequence of out-of-order phrases
x1, . . . , xN , with xn ∈ V + for 1 ≤ n ≤ N . Deﬁne
M as the total number of tokens (i.e., the sum of the
lengths of the phrases). We consider two varieties of

the task: (1) WORDS, where each xn consists of a
single word and M = N , and (2) WORDS+BNPS,
where base noun phrases (noun phrases not con-
taining inner noun phrases) are also provided and
M ≥ N . The second has become a standard for-
mulation in recent literature.

Given input x, we deﬁne the output set Y to be
all possible permutations over the N elements of x,
where ˆy ∈ Y is the permutation generating the true
order. We aim to ﬁnd ˆy, or a permutation close to
it. We produce a linearization by (approximately)
optimizing a learned scoring function f over the set
of permutations, y∗ = arg maxy∈Y f (x, y).

3 Related Work: Syntactic Linearization

Recent approaches to linearization have been based
on reconstructing the syntactic structure to produce
the word order. Let Z represent all projective de-
pendency parse trees over M words. The objec-
tive is to ﬁnd y∗, z∗ = arg maxy∈Y,z∈Z f (x, y, z)
where f is now over both the syntactic structure
and the linearization. The current state of the art
on the Penn Treebank (PTB) (Marcus et al., 1993),
without external data, of Liu et al. (2015) uses a
transition-based parser with beam search to con-
struct a sentence and a parse tree. The scoring func-
tion is a linear model f (x, y) = θ⊤Φ(x, y, z) and
is trained with an early update structured percep-
tron to match both a given order and syntactic tree.
The feature function Φ includes features on the syn-
tactic tree. This work improves upon past work
which used best-ﬁrst search over a similar objective
(Zhang and Clark, 2011).

In follow-up work, Liu and Zhang (2015) argue
that syntactic models yield improvements over pure
surface n-gram models for the WORDS+BNPS case.
This result holds particularly on longer sentences
and even when the syntactic trees used in training
are of low quality. The n-gram decoder of this work
utilizes a single beam, discarding the probabilities
of internal, non-boundary words in the BNPs when
comparing hypotheses. We revisit this comparison
between syntactic models and surface-level models,
utilizing a surface-level decoder with heuristic fu-
ture costs and an alternative approach for scoring
partial hypotheses for the WORDS+BNPS case.

The
gram models for the word ordering task.
work of de Gispert et al. (2014) demonstrates im-
provements over the earlier syntactic model of
Zhang et al. (2012) by applying an n-gram language
model over the space of word permutations re-
stricted to concatenations of phrases seen in a large
corpus. Horvat and Byrne (2014) models the search
for the highest probability permutation of words un-
der an n-gram model as a Travelling Salesman Prob-
lem; however, direct comparisons to existing works
are not provided.

4 LM-Based Linearization

In contrast to the recent syntax-based approaches,
we use an LM directly for word ordering. We
consider two types of language models:
an n-
gram model and a long short-term memory network
(Hochreiter and Schmidhuber, 1997). For the pur-
pose of this work, we deﬁne a common abstraction
for both models. Let h ∈ H be the current state of
the model, with h0 as the initial state. Upon seeing
a word wi ∈ V, the LM advances to a new state
hi = δ(wi, hi−1). At any time, the LM can be
queried to produce an estimate of the probability of
the next word q(wi, hi−1) ≈ p(wi | w1, . . . , wi−1).
For n-gram language models, H, δ, and q can natu-
rally be deﬁned respectively as the state space, tran-
sition model, and edge costs of a ﬁnite-state ma-
chine.

LSTMs are a type of recurrent neural network
(RNN) that are conducive to learning long-distance
dependencies through the use of an internal mem-
ory cell. Existing work with LSTMs has gener-
ated state-of-the-art results in language modeling
(Zaremba et al., 2014), along with a variety of other
NLP tasks.

In our notation we deﬁne H as the hidden states
and cell states of a multi-layer LSTM, δ as the
LSTM update function, and q as a ﬁnal afﬁne trans-
formation and softmax given as q(∗, hi−1; θq) =
softmax(W h(L)
i−1 + b) where h(L)
i−1 is the top hid-
den layer and θq = (W , b) are parameters. We di-
rect readers to the work of Graves (2013) for a full
description of the LSTM update.

Additional previous work has also explored n-

For both models, we simply deﬁne the scoring

Algorithm 1 LM beam-search word ordering
1: procedure ORDER(x1 . . . xN , K, g)
2: B0 ← h(hi, {1, . . . , N }, 0, h0)i
for m = 0, . . . , M − 1 do
3:
for k = 1, . . . , |Bm| do
4:
(y, R, s, h) ← B(k)
m
for i ∈ R do
(s′, h′
for word w in phrase xi do
s′ ← s′ + log q(w, h′
h′

) ← (s, h)

5:
6:
7:
8:
9:
10:

← δ(w, h′

)

)

11:
12:
13:

14:

return BM

j ← m + |xi|
Bj ← Bj + (y + xi, R − i, s′, h′
keep top-K of Bj by f (x, y) + g(R)

)

function as

N

X
n=1

f (x, y) =

log p(xy(n) | xy(1), . . . , xy(n−1))

where the phrase probabilities are calculated word-
by-word by our language model.

Searching over all permutations Y is
in-
tractable, so we instead follow past work on lin-
earization (Liu et al., 2015) and LSTM generation
(Sutskever et al., 2014) in adapting beam search for
our generation step. Our work differs from the beam
search approach for the WORDS+BNPS case of pre-
vious work in that we maintain multiple beams, as
in stack decoding for phrase-based machine trans-
lation (Koehn, 2010), allowing us to incorporate
the probabilities of internal, non-boundary words
in the BNPs. Additionally, for both WORDS and
WORDS+BNPS, we also include an estimate of fu-
ture cost in order to improve search accuracy.
Beam search maintains M + 1
beams,
B0, . . . , BM , each containing at most
the top-
K partial hypotheses of that
length. A partial
hypothesis is a 4-tuple (y, R, s, h), where y is a
partial ordering, R is the set of remaining indices to
be ordered, s is the score of the partial linearization
f (x, y), and h is the current LM state. Each step
consists of expanding all next possible phrases and
adding the next hypothesis to a later beam. The full
beam search is given in Algorithm 1.

As part of the beam search scoring function we
also include a future cost g, an estimate of the score

WORDS WORDS+BNPS

Model

ZGEN-64
ZGEN-64+POS

NGRAM-64 (NO g)
NGRAM-64
NGRAM-512
LSTM-64
LSTM-512

ZGEN-64+LM+GW+POS
LSTM-64+GW
LSTM-512+GW

30.9
–

32.0
37.0
38.6
40.5
42.7

–
41.1
44.5

49.4
50.8

51.3
54.3
55.6
60.9
63.2

52.4
63.1
65.8

Table 1: BLEU score comparison on the PTB test
set. Results from previous works (for ZGEN) are
those provided by the respective authors, except for
the WORDS task. The ﬁnal number in the model
identiﬁer is the beam size, +GW indicates additional
Gigaword data. Models marked with +POS are pro-
vided with a POS dictionary derived from the PTB
training set.

contribution of the remaining elements in R. To-
gether, f (x, y) + g(R) gives a noisy estimate of the
total score, which is used to determine the K best
elements in the beam. In our experiments we use a
very simple unigram future cost estimate, g(R) =
Pi∈R Pw∈xi log p(w).

5 Experiments

Setup Experiments are on PTB with sections 2-
21 as training, 22 as validation, and 23 as test1.
We utilize two UNK types, one for initial upper-
case tokens and one for all other low-frequency to-
kens; end sentence tokens; and start/end tokens,
which are treated as words, to mark BNPs for the
WORDS+BNPS task. We also use a special symbol
to replace tokens that contain at least one numeric
character. We otherwise train with punctuation and
the original case of each token, resulting in a vocab-
ulary containing around 16K types from around 1M
training tokens.

For experiments marked GW we augment

the
PTB with a subset of
the Annotated Giga-
word corpus (Napoles et al., 2012). We follow

1In

practice,

and
Liu and Zhang (2015) use section 0 instead of 22 for vali-
dation (author correspondence).

in Liu et al. (2015)

results

the

BNP

g

GW

1

10

64

128

256

512

•
•
•

•
•

•

•

•
•

•
•

•

•

LSTM

53.6
59.4
60.1
26.8
36.8
35.5

49.7
53.6
27.1
34.6

58.0
62.2
64.2
33.8
40.7
40.7

52.6
55.6
32.6
37.5

59.1
62.9
64.9
35.3
41.7
41.7

53.2
56.2
33.8
38.1

NGRAM

41.7
47.6
48.4
15.4
25.0
23.8

40.6
45.7
14.6
27.1

60.0
63.6
65.6
36.5
42.0
42.9

54.0
56.6
35.1
38.4

60.6
64.3
66.2
38.0
42.5
43.7

54.7
56.6
35.8
38.7

Table 2: BLEU results on the validation set for
the LSTM and NGRAM model with varying beam
sizes, future costs, additional data, and use of base
noun phrases.

Liu and Zhang (2015) and train on a sample of 900k
Agence France-Presse sentences combined with the
The GW models beneﬁt
full PTB training set.
from both additional data and a larger vocabulary of
around 25K types, which reduces unknowns in the
validation and test sets.

We compare the models of Liu et al. (2015)
(known as ZGEN), a 5-gram LM using Kneser-Ney
smoothing (NGRAM)2, and an LSTM. We experi-
ment on the WORDS and WORDS+BNPS tasks, and
we also experiment with including future costs (g),
the Gigaword data (GW), and varying beam size.
We retrain ZGEN using publicly available code3 to
replicate published results.
is
The LSTM model

in size and
similar
architecture
to the medium LSTM setup of
Zaremba et al. (2014)4. Our implementation uses
the Torch5 framework and is publicly available6.

We compare the performance of the models using
the BLEU metric (Papineni et al., 2002). In gener-
ation if there are multiple tokens of identical UNK
type, we randomly replace each with possible un-
used tokens in the original source before calculating
BLEU. For comparison purposes, we use the BLEU

2We use the KenLM Language Model Toolkit (https://

kheafield.com/code/kenlm/).

3https://github.com/SUTDNLP/ZGen
4We hypothesize that additional gains are possible via a

larger model and model averaging, ceteris paribus.

5http://torch.ch
6https://github.com/allenschmaltz/word_

ordering

LSTM-512
LSTM-64
ZGen-64
LSTM-1

ZGen-64
LSTM-64

5

10

15

20

25
Sentence length

30

35

40

)

%

(
U
E
L
B

90

70

50

30

s
n
e
k
o
T

12000

8000

4000

0
0.0

0.2

0.4
0.6
Distortion rate

0.8

1.0

Figure 1: Experiments on the PTB validation on the
WORDS+BNPS models: (a) Performance on the set
by length on sentences from length 2 to length 40.
(b) The distribution of token distortion, binned at 0.1
intervals.

script distributed with the publicly available ZGEN
code.

Results Our main results are shown in Table 1.
On the WORDS+BNPS task the NGRAM-64 model
scores nearly 5 points higher than the syntax-
based model ZGEN-64.
The LSTM-64 then
surpasses NGRAM-64 by more than 5 BLEU
Differences on the WORDS task are
points.
Incorporat-
smaller, but show a similar pattern.
ing Gigaword further increases the result another 2
points. Notably, the NGRAM model outperforms the
combined result of ZGEN-64+LM+GW+POS from
Liu and Zhang (2015), which uses a 4-gram model
trained on Gigaword. We believe this is because
the combined ZGEN model incorporates the n-gram
scores as discretized indicator features instead of us-
ing the probability directly.7 A beam of 512 yields a
further improvement at the cost of search time.

To further explore the impact of search accuracy,

7In work of Liu and Zhang (2015), with the given decoder,
N-grams only yielded a small further improvement over the syn-
tactic models when discretized versions of the LM probabilities
were incorporated as indicator features in the syntactic models.

WORDS WORDS+BNPS

els, for both high- and low- resource languages and
domains.

Model
ZGEN-64(z∗)
ZGEN-64

NGRAM-64
NGRAM-512
LSTM-64
LSTM-512

39.7
40.8

46.1
47.2
51.3
52.8

64.9
65.2

67.0
67.8
71.9
73.1

Table 3: Unlabeled attachment scores (UAS) on
the PTB validation set after parsing and aligning
the output. For ZGEN we also include a result us-
ing the tree z∗ produced directly by the system.
For WORDS+BNPS, internal BNP arcs are always
counted as correct.

Table 2 shows the results of various models with
beam widths ranging from 1 (greedy search) to 512,
and also with and without future costs g. We see that
for the better models there is a steady increase in ac-
curacy even with large beams, indicating that search
errors are made even with relatively large beams.

One proposed advantage of syntax in lineariza-
tion models is that it can better capture long-distance
relationships. Figure 1 shows results by sentence
length and distortion, which is deﬁned as the abso-
lute difference between a token’s index position in
y∗ and ˆy, normalized by M . The LSTM model ex-
hibits consistently better performance than existing
syntax models across sentence lengths and generates
fewer long-range distortions than the ZGEN model.
Finally, Table 3 compares the syntactic ﬂuency of
the output. As a lightweight test, we parse the output
with the Yara Parser (Rasooli and Tetreault, 2015)
and compare the unlabeled attachment scores (UAS)
to the trees produced by the syntactic system. We
ﬁrst align the gold head to each output token.
(In
cases where the alignment is not one-to-one, we ran-
domly sample among the possibilities.) The models
with no knowledge of syntax are able to recover a
higher proportion of gold arcs.

6 Conclusion

Strong surface-level language models recover word
order more accurately than the models trained with
explicit syntactic annotations appearing in a recent
series of papers. This has implications for the utility
of costly syntactic annotations in generation mod-

Acknowledgments

We thank Yue Zhang and Jiangming Liu for assis-
tance in using ZGen, as well as veriﬁcation of the
task setup for a valid comparison. Jiangming Liu
also assisted in pointing out a discrepancy in the im-
plementation of an earlier version of our NGRAM
decoder, the resolution of which improved BLEU
performance.

References

[Brew1992] Chris Brew. 1992. Letting the cat out of the
bag: Generation for shake-and-bake MT. In Proceed-
ings of the 14th Conference on Computational Lin-
guistics - Volume 2, COLING ’92, pages 610–616,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

[Brown et al.1990] Peter F Brown, John Cocke, Stephen
A Della Pietra, Vincent J Della Pietra, Fredrick Je-
linek, John D Lafferty, Robert L Mercer, and Paul S
Roossin. 1990. A statistical approach to machine
translation. Computational linguistics, 16(2):79–85.
[de Gispert et al.2014] Adri`a de Gispert, Marcus Tomalin,
and Bill Byrne. 2014. Word ordering with phrase-
based grammars. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 259–268, Gothen-
burg, Sweden, April. Association for Computational
Linguistics.

[Elman1990] Jeffrey L. Elman. 1990. Finding structure

in time. Cognitive Science, 14(2):179 – 211.

[Graves2013] Alex Graves.

2013.

quences with recurrent neural networks.
abs/1308.0850.

Generating se-
CoRR,

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Comput., 9(8):1735–1780, November.

[Horvat and Byrne2014] Matic Horvat

and William
Byrne.
2014. A graph-based approach to string
regeneration. In Proceedings of the Student Research
Workshop at the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 85–95, Gothenburg, Sweden, April.
Association for Computational Linguistics.

[Koehn2010] Philipp Koehn. 2010. Statistical Machine
Translation. Cambridge University Press, New York,
NY, USA, 1st edition.

[Liu and Zhang2015] Jiangming Liu and Yue Zhang.
2015. An empirical comparison between n-gram and
syntactic language models for word ordering. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 369–378,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

[Zhang et al.2012] Yue Zhang, Graeme Blackwood, and
Stephen Clark. 2012. Syntax-based word ordering in-
corporating a large-scale language model. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ’12, pages 736–746, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Liu et al.2015] Yijia Liu, Yue Zhang, Wanxiang Che, and
Bing Qin. 2015. Transition-based syntactic lineariza-
tion.
In Proceedings of the 2015 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 113–122, Denver, Colorado, May–June.
Association for Computational Linguistics.

[Marcus et al.1993] Mitchell P. Marcus, Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: The penn tree-
bank. Computational Linguistics, 19(2):313–330.
[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, AKBC-WEKEX ’12, pages
95–100, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2002. Bleu: A
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ’02,
pages 311–318, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Rasooli and Tetreault2015] Mohammad Sadegh Rasooli
and Joel R. Tetreault. 2015. Yara parser: A fast and
accurate dependency parser. CoRR, abs/1503.06733.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Zaremba et al.2014] Wojciech Zaremba, Ilya Sutskever,
and Oriol Vinyals. 2014. Recurrent neural network
regularization. CoRR, abs/1409.2329.

[Zhang and Clark2011] Yue Zhang and Stephen Clark.
2011. Syntax-based grammaticality improvement us-
ing ccg and guided search. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’11, pages 1147–1157, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Zhang and Clark2015] Yue Zhang and Stephen Clark.
2015. Discriminative syntax-based word ordering for
text generation. Comput. Linguist., 41(3):503–538,
September.

