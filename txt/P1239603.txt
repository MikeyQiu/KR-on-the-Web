Fast, Flexible Models for Discovering Topic Correlation across
Weakly-Related Collections

Jingwei Zhang1, Aaron Gerow2, Jaan Altosaar3, James Evans2,4, Richard Jean So5

1Department of Computer Science, Columbia University
jz2541@columbia.edu
2 Computation Institute, University of Chicago

{

@uchicago.edu

gerow,jevans
}
3 Department of Physics, Princeton University
altosaar@princeton.edu
4 Department of Sociology, University of Chicago
5 Department of English Language and Literature, University of Chicago
richardjeanso@uchicago.edu

Abstract

This paper

Weak topic correlation across document
collections with different numbers of
topics in individual collections presents
challenges for existing cross-collection
topic models.
introduces
two probabilistic topic models, Correlated
LDA (C-LDA) and Correlated HDP (C-
HDP). These address problems that can
arise when analyzing large, asymmetric,
and potentially weakly-related collections.
Topic correlations in weakly-related col-
lections typically lie in the tail of the topic
distribution, where they would be over-
looked by models unable to ﬁt large num-
bers of topics. To efﬁciently model this
long tail for large-scale analysis, our mod-
els implement a parallel sampling algo-
rithm based on the Metropolis-Hastings
and alias methods (Yuan et al., 2015).
The models are ﬁrst evaluated on syn-
thetic data, generated to simulate vari-
ous collection-level asymmetries. We then
present a case study of modeling over
300k documents in collections of sciences
and humanities research from JSTOR.

1

Introduction

Comparing large text collections is a critical task
for the curation and analysis of human cultural
history. Achievements of research and schol-
arship are most accessible through textual arti-
facts, which are increasingly available in digital
archives. Text-based research, often undertaken
by humanists, historians, lexicographers, and cor-

pus linguists, explores patterns of words in docu-
ments across time-periods and distinct collections
of text. Here, we introduce two new topic models
designed to compare large collections, Correlated
LDA (C-LDA) and Correlated HDP (C-HDP),
which are sensitive to document-topic asymme-
try (where collections have different topic distribu-
tions) and topic-word asymmetry (where a single
topic has different word distributions in each col-
lection). These models seek to address termino-
logical questions, such as how a topic on physics
is articulated distinctively in scientiﬁc compared
to humanistic research. Accommodating poten-
tial collection-level asymmetries is particularly
important when researchers seek to analyze col-
lections with little prior knowledge about shared
or collection-speciﬁc topic structure. Our mod-
els extend existing cross-collection approaches to
accommodate these asymmetries and implement
an efﬁcient parallel sampling algorithm enabling
users to examine the long tail of topics in particu-
larly large collections.

Using topic models for comparative text min-
ing was introduced by Zhai et al. (2004), who de-
veloped the ccMix model which extended pLSA
(Hofmann, 1999). Later work by Paul and Girju
(2009) developed ccLDA, which adopted the hier-
archical Bayes framework of Latent Dirichlet Al-
location or LDA (Blei et al., 2003). These mod-
els account for topic-word asymmetry by assum-
ing variation in the vocabularies of topics is due
to collection-level differences. Nevertheless, they
require the same topics to be present in each col-
lection. These models are useful for comparing
collections under speciﬁc assumptions, but cannot
accommodate collection-topic asymmetry (which

5
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
6
5
4
0
.
8
0
5
1
:
v
i
X
r
a

arises in collections that do not share every topic
or that have different numbers of topics). In situa-
tions where collections do not share all topics, the
results often include junk, mixed, or sparse top-
ics, making them difﬁcult to interpret (Paul and
Girju, 2009). Such asymmetries make it difﬁcult
to use models like ccLDA and ccMix when little
is known about collections in advance. This mo-
tivates our efforts to model variation in the long
tail of topic distributions, where correlations are
more likely to appear when collections are weakly
related.

C-LDA and C-HDP extend ccLDA (Paul and
Girju, 2009) to accommodate collection-topic
level asymmetries, particularly by allowing non-
common topics to appear in each collection. This
added ﬂexibility allows our models to discover
topic correlations across arbitrary collections with
different numbers of topics, even when there are
few (or unknown) numbers of common topics. To
demonstrate the effectiveness of our models, we
evaluate them on synthetic data and show that they
outperform related models such as ccLDA and dif-
ferential topic models (Chen et al., 2014). We then
ﬁt C-LDA to two large collections of humanities
and sciences documents from JSTOR. Such histor-
ical analyses of text would be intractable without
an efﬁcient sampler. An optimized sampler is re-
quired in such situations because common topics
in weakly-correlated collections are usually found
in the tail of the document-topic distribution of a
sufﬁciently large set of topics. To make this fea-
sible on large datasets such as JSTOR, we employ
a parallelized Metropolis-Hastings (Kronmal and
Peterson Jr, 1979) and alias-table sampling frame-
work, adapted from LightLDA (Yuan et al., 2015).
These optimizations, which achieve
(1) amor-
tized sampling time per token, allow our models
to be ﬁt to large corpora with up to thousands of
topics in a matter of hours — an order of magni-
tude speed-up from ccLDA.

O

After reviewing work related to topic modeling
across collections, section 3 describes C-LDA and
C-HDP, and then details their technical relation-
ship to existing models. Section 5 introduces the
synthetic data and part of the JSTOR corpus used
in our evaluations. We then compare our models’
performances to other models in terms of held-
out perplexity and a measure of distinguishabil-
ity. The ﬁnal results section exempliﬁes the use
of C-LDA in a qualitative analysis of humanities

and sciences research. We conclude with a brief
discussion of the strengths of C-LDA and C-HDP,
and outline directions for future work and applica-
tions.

2 Related Work

Our models seek to enable users to compare large
collections that may only be weakly correlated
and that may contain different numbers of topics.
While topic models could be ﬁt to separate collec-
tions to make post-hoc comparisons (Denny et al.,
2014; Yang et al., 2011), our goal is to account for
both document-topic asymmetry and topic-word
asymmetry “in-model”. In short, we seek to model
the correlation between arbitrary collections. Pri-
oritizing in-model solutions for document-topic
asymmetry has been explored elsewhere, such as
in hierarchical Dirichlet processes (HDP), which
use an additional level to account for collection
variations in document-topic distributions (Teh et
al., 2006).

One method designed to model

topic-word
asymmetry is ccMix (Zhai et al., 2004), which
models the generative probability of a word in
topic z from collection c as a mixture of shared
and collection-speciﬁc distributions θz:

p(w) = λc p(w

θz) + (1
|

−

λc) p(w

θz,c)
|

where θz,c is collection-speciﬁc and λc controls
the mixing between shared and collection-speciﬁc
topics. ccLDA extends ccMix to the LDA frame-
work and adds a beta prior over λc that reduces
sensitivity to input parameters (Paul and Girju,
2009). Another approach, differential topic mod-
els (Chen et al., 2014), is based on hierarchical
Bayesian models over topic-word distributions.
This method uses the transformed Pitman-Yor pro-
cess (TPYP) to model topic-word distributions in
each collection, with shared common base mea-
sures. As (Paul and Girju, 2009) note, ccLDA
cannot accommodate a topic if it is not com-
mon across collections — an assumption made by
ccMix, ccLDA and the TPYP. In a situation where
a topic is found in only one collection, it would
either dominate the shared topic portion (resulting
in a noisy, collection-speciﬁc portion), or it would
appear as a mixed topic, revealing two sets of un-
related words (Newman et al., 2010b). C-LDA
ameliorates this situation by allowing the number
of common and non-common topics to be speci-
ﬁed separately and by efﬁciently sampling the tail

of the document-topic distribution, allowing users
to examine less prominent regions of the topic
space. C-HDP also grants collections document-
topic independence using a hierarchical structure
to model the differences between collections.

O

Due to increased demand for scalable topic
model implementations, there has been a prolif-
eration of optimized methods for efﬁcient infer-
ence, such as SparseLDA (Yao et al., 2009) and
AliasLDA (Li et al., 2014). AliasLDA achieves
(Kd) complexity by using the Metropolis-
O
Hastings-Walker algorithm and an alias table to
(1) time. Al-
sample topic-word distributions in
though this strategy introduces temporal staleness
in the updates of sufﬁcient statistics, the lag is
overcome by more iterations, and converges sig-
niﬁcantly faster. A similar technique by Yuan et al.
(2015), LightLDA, employs cycle-based Metropo-
lis Hastings mixing with alias tables for both
document-topic and topic-word distributions. De-
spite introducing lag in the sufﬁcient statistics,
this method achieves
(1) amortized sampling
O
complexity and results in even faster convergence
than AliasLDA. In addition to being fully paral-
lelized, C-LDA adopts this sampling framework to
make comparing large collections more tractable
for large numbers of topics. Our models’ efﬁcient
sampling methods allow users to ﬁt large num-
bers of topics to big datasets where variation might
not be observed in sub-sampled datasets or models
with fewer topics.

3 The Models

3.1 Correlated LDA

In ccLDA (and ccMix), each topic has shared
and collection-speciﬁc components for each col-
lection. C-LDA extends ccLDA to make it more
robust with respect to topic asymmetries between
collections (Figure 1a). The crucial extension is
that by allowing each collection to deﬁne a set of
non-common topics in addition to common top-
ics, the model removes an assumption imposed by
ccLDA and other inter-collection models, namely
that collections have the same number of topics.
As a result, C-LDA is suitable for collections with-
out a large proportion of common topics, and can
also reduce noise (discussed in Section 2). To
achieve this, C-LDA assumes document d in col-
lection c has a multinomial document-topic dis-
tribution θ with an asymmetric Dirichlet prior
for Kc topics, where the ﬁrst K∅ are common

across collections. It is also possible to introduce
a tree structure into the model that uses a bino-
mial distribution to decide whether a word was
drawn from common or non-common topics. This
yields collection-speciﬁc background topics by us-
ing a binomial distribution instead of a multino-
mial. However, we prefer the simpler, non-tree
version because background topics are unneces-
sary when using an asymmetric α prior (Wallach
et al., 2009a).

The generative process for C-LDA is as follows:

1. Sample a distribution φk (shared component) from Dir(β)
and a distribution σk from Beta(δ1, δ2) for each common
topic k ∈ {1, . . . , K ∅};

2. For each collection c, sample a distribution φc

k (collection-
speciﬁc component) from Dir(β) for each common topic
k ∈ {1, . . . , K ∅} and non-common topic k ∈ {K ∅ +
1, . . . , Kc};

3. For each document d in c, sample a distribution θ from

Dir(αc);

4. For each word wi in d:

(a) Sample a topic zi ∈ {1, . . . , Kc} from Multi(θ);
(b) If zi ≤ K ∅, sample yi from Binomial(σzi );
(c) Sample wi from Multi(φξ
null
c

, zi ≤ K ∅ and yi = 0;
, otherwise.

zi ), where

ξ =

(cid:26)

∃

Note that to capture common topics, K∅ should
c where Kc = K∅. Other-
be set such that
wise, words sampled as a non-common topic will
not have information about non-common topics in
other collections. Then a “common-topic word”
is found among non-common topics in all col-
lections (a local minima) and it will take a long
time to stabilize as a common topic. To avoid
this, when determining the number of topics for
sampling, the number of non-common topics for
the collection with the smallest number of total
topics should be zero. After inference, to distin-
guish common and non-common topics in this col-
lection, we model σ independently by assuming
collections have the same mixing ratio for com-
mon topics. With this reasonable assumption and
an asymmetric α, common topics become sparse
enough that some σ distributions reduce nearly to
0, distinguishing them as non-common topics. Al-
though this may seem counterintuitive, it does not
negatively affect results.

Three kinds of collection-level imbalance can
confound inter-collection topic models: 1) in the
numbers of topics between collections, 2) in the
numbers of documents between collections, and
3) in the document-topic distributions. Each of

α

θ

z

w

C

D

W

y

δ

σ

K∅

β

φ
K∅ + (cid:80) Kc

γ

H

G0

Gc

Gd

z

w

C

D

W

y

δ

σ

K(∞)

α0

α1

β

φ

K(∞)

Figure 1: Graphical models of C-LDA (a; left) and C-HDP (b; right).

these can cause topics in different collections to
have signiﬁcantly different numbers of words as-
signed to the same topic. In this way, a topic can
be dominated by the collection comprising most
of its words. C-LDA addresses imbalances in the
document-topic distributions between collections
by estimating α. For imbalance in the number of
topics and documents, C-LDA mimics document
over-sampling in the Gibbs sampler using a differ-
ent unit-value in the word count table for each col-
lection. Speciﬁcally, a unit ηc is chosen for each
collection such that the average equivalent num-
ber of assigned words per-topic ((cid:80)
d∈c ηcNd/Kc,
where Nd is the length of document d) is equal.
This process both increases the topic quality (in
terms of collection balance) in the resulting held-
out perplexity of the model.

3.2 Correlated HDP

∃

c such
To alleviate C-LDA’s requirement that
that Kc = K∅, we introduce a variant of the
model, the correlated hierarchical Dirichlet pro-
cess (C-HDP), that uses a 3-level hierarchical
Dirichlet process (Teh et al., 2006). The gener-
ative process for C-HDP is the same as C-LDA
shown above, except that here we assume a word’s
topic, z, is generated by a hierarchical Dirichlet
process:

|

G0

DP(γ, H)
DP(α0, G0)
DP(α1, Gc)
Gd

γ, H
Gc
α0, G0
|
α1, Gc
Gd
|
Gd
z
|

∼
∼
∼
∼
where G0 is a base measure for each collection-
level Dirichlet process, and Gc are base measures
of document-level Dirichlet processes in each col-
lection (Figure 1b). Thus, documents from the

same collection will have similar topic distribu-
tions compared to those from other collections,
and collections are allowed to have distinct sets of
topics due to the use of HDP.

4

Inference

4.1 Posterior Inference in C-LDA

C-LDA can be trained using collapsed Gibbs sam-
pling with φ, θ, and σ integrated out. Given the
status assignments of other words, the sampling
distribution for word wi is given by:

w, y−i, z−i, δ, α, β)
|

p(yi, zi
(N (d, zi) + αc,zi)
(cid:124)
(cid:123)(cid:122)
(cid:125)
qd

∝





(cid:124)

×

N (yi, zi) + δyi
N (zi) + (cid:80)
N (wi, zi, c) + β
N (zi, c) + V β

k δk ×

(cid:123)(cid:122)
qw

N (wi, yi, zi, ζ) + β
N (yi, zi, ζ) + V β

K∅

zi

≤

zi > K∅

(cid:125)

(1)

where ζ =

(cid:26) ∗
c

yi = 0
yi = 1 , N (

· · ·

) is the number of

status assignments for (

), not including wi.

· · ·
Inference in C-LDA employs two optimiza-
tions: a parallelized sampler and an efﬁcient sam-
pling algorithm (Algorithm 1). We use the paral-
lel schema in (Smola and Narayanamurthy, 2010;
Lu et al., 2013) which applies atomic updates to
the sufﬁcient statistics to avoid race conditions.
The key idea behind the optimized sampler is the
combination of alias tables and the Metropolis-
Hastings method (MH), adapted from (Yuan et al.,
2015; Li et al., 2014). Metropolis-Hastings is a
Markov chain Monte Carlo method that uses a pro-
posal distribution to approximate the true distribu-

Algorithm 1 Sampling in C-LDA

repeat

for all documents {d} in parallel do

for words {w} in d do

z ← CYCLEMH(p, qw, qd, z)
sample y given z

Atomic update sufﬁcient statics

Estimate α
until convergence

procedure CYCLEMH(p, qw, qd, z)

for i = 1 to N do

if i is even then

else

proposal q ← qw

proposal q ← qd

sample z(cid:48) ∼ ALIASTABLE(q)
if RandUnif(1) < min(1, p(z(cid:48))q(z)

p(z)q(z(cid:48)) ) then

z ← z(cid:48)

return z

tion when exact sampling is difﬁcult. In a compli-
mentary way, Walker’s alias method (2004) allows
one to effectively sample from a discrete distribu-
tion by using an alias table, constructed in
(K)
O
time, from which we can sample in
(1). Thus,
reusing the sampler K times as the proposal distri-
bution for Metropolis-Hastings yields
(1) amor-
tized sampling time per-token.

O

O

Notice that in Eq. 1, the sampling distribution is
the product of a single document-dependent term
qd and a single word-dependent term qw. After
burn-in, both terms will be sparse (without the
smoothing factor). It is therefore reasonable to use
qd and qw as cycle proposals (Yuan et al., 2015),
alternating them in each Metropolis-Hastings step.
Our experiments show that the primary drawback
of this method — stale sufﬁcient statistics — does
not empirically affect convergence. Our imple-
mentation uses proposal distributions qw and qd,
with y marginalized out. After the Metropolis-
Hastings steps, y is sampled to update z, to reduce
the size of the alias tables, yielding even faster
convergence.

Lastly, the use of an asymmetric α allows C-
LDA to discover correlations between less dom-
inant topics across collections (Wallach et al.,
2009a). We use Minka’s ﬁxed-point method, with
a gamma hyper-prior to optimize αc for each col-
lection separately (Wallach, 2008). All other hy-
perparameters were ﬁxed during inference.

4.2 Posterior Inference in C-HDP

C-HDP uses the block sampling algorithm de-
scribed in (Chen et al., 2011), which is based on

the Chinese restaurant process metaphor. Here,
rather than tracking all assignments (as the sam-
plers given in (Teh et al., 2006)), table indicators
are used to track only the start of new tables, which
allows us to adopt the same sampling framework
as C-LDA. In the Chinese restaurant process, each
Dirichlet process in the hierarchical structure is
represented as a restaurant with an inﬁnite num-
ber of tables, each serving the same dish. New
customers can either join a table with existing cus-
tomers, or start a new table. If a new table is cho-
sen, a proxy customer will be sent to the parent
restaurant to determine the dish served to that ta-
ble.

∅

In the block sampler, indicators are used to de-
note a customer creating a table (or tables) up to
level u (0 as the root, 1 for collection level, and 2
for the document level), and u =
indicates no
table has been created. For example, when a cus-
tomer creates a table at the collection level, and
the proxy customer in the collection level creates
a table at the root level, u is 0. With this metaphor,
let nlz be the number of customers (including their
proxies) served dish z at restaurant l, and let tlz be
the number of tables serving dish z at restaurant
l (l = 0 for root, l = c for collection level or
l = d for document level), with N0 = (cid:80)
z n0z and
Nc = (cid:80)
z ncz. By the chain rule, the conditional
probability of the state assignments for wi, given
all others, is

∝

p(yi, zi, ui|w, y−i, z−i, u−i, . . .)
N (y, z) + δy
N (z) + (cid:80)
k δk

γα0
γ+N0

N (w, y, z, ζ) + β
N (y, z, ζ) + V β

×

×




α0
γ+N0
Sncz +1
tcz
Sncz
tcz

α0+N1
α1

S

Sncz +1
tcz +1
Sncz
tcz
ndz +1
S
tdz +1
ndz
S
tdz
ndz +1
tdz
S

ndz
tdz

S

ndz +1
tdz +1
ndz
S
tdz

n2

0z (tcz +1)(tdz +1)
(n0z +1)(ncz +1)(ndz +1)

(tdz +1)(ncz −tcz +1)
(ncz +1)(ndz +1)

ndz −tdz +1
ndz +1

u = 0

u = 1

u = 2

u = ∅

Here, Sn
t is the Stirling number, the ratios of which
can be efﬁciently precomputed (Buntine and Hut-
ter, 2010). The concentration parameters γ, α0,
and α1 can be sampled using the auxiliary variable
method (Teh et al., 2006).

Note that because conditional probability has
the same separability as C-LDA (to give term qw
and qd), the same sampling framework can be
used with two alterations: 1) when a new topic
is created or removed at the root, collection, or
document level, the related alias tables must be
reset, which makes the sampling slightly slower

Figure 2: Held-out perplexity of C-LDA, C-HDP, ccLDA and TPYP ﬁt to synthetic data, where K1 =
K2 = K (a; left) and data with an asymmetric number of topics (b; right).

O

than
(1), and 2) while the document alias table
samples z and u simultaneously, after sampling z
from the word alias table u must be sampled using
tlc/nlz (Chen et al., 2011). Parallelizing C-HDP
requires an additional empirical method of merg-
ing new topics between threads (Newman et al.,
2009), which is outside of the scope of this work.
Our implementation of both models, C-LDA and
C-HDP, are open-sourced online 1.

5 Experiments

5.1 Model Comparison

We use perplexity on held-out documents to eval-
uate the performance of C-LDA and C-HDP. In all
experiments, the gamma prior for α in C-LDA was
set to (1, 1), and (5, 0.1), (5, 0.1), (0.1, 0.1) for
γ, α0, α1 respectively in C-HDP. In the hold-out
procedure, 20% of documents were randomly se-
lected as test data. LDA, C-LDA and ccLDA were
run for 1,000 iterations and C-HDP and the TII-
variant of TPYP for 1,500 iterations (unless oth-
erwise noted), all of which converged to a state
where change in perplexity was less than 1% for
ten consecutive iterations.

Perplexity was calculated from the marginal
likelihood of a held-out document p(w
Φ, α), es-
|
timated using the “left-to-right” method (Wallach
et al., 2009b). Because it is difﬁcult to vali-
date real-world data that exhibits different kinds
of asymmetry, we use synthetic data generated
speciﬁcally for our evaluation tasks (AlSumait et
al., 2009; Wallach et al., 2009b; Kucukelbir and
Blei, 2014).

5.1.1 Topic Correlation
C-LDA is unique in the amount of freedom it al-
lows when setting the number of topics for col-

1https://github.com/iceboal/correlated-lda

lections. To assess the models’ performances with
various topic correlations in a fair setting, we gen-
erated two collections of synthetic data by fol-
lowing the generative process (varying the num-
ber of topics) and measured the models’ perplex-
ities against the ground truth parameters. In each
experiment, two collections were generated, each
with 1,000 documents containing 50 words each,
over a vocabulary of 3,000. β and δ were ﬁxed at
0.01 and 1.0 respectively, and α was asymmetri-
cally deﬁned as 1/(i + √Kc) for i
1].

[0, Kc

∈

−

Completely shared topics The assumptions im-
posed by ccLDA and TPYP effectively make them
a special case of our model where K∅ = K1 =
K2 = . . .. To compare results, data was gener-
ated such that all numbers of topics were equal to
[10, 90]. Additionally, all models were con-
K
ﬁgured to use this ground truth parameter when
training. Not surprisingly, ccLDA, C-LDA, and C-
HDP have almost the same perplexity with respect
to K because their structure is the same when all
topics are shared (Figure 2a).

∈

Asymmetric numbers of topics To explore the
effect of asymmetry in the number of topics, data
was generated such that one collection had K1
∈
[20, 60] topics while a second had a ﬁxed K2 = 40
topics. The number of shared topics was set to
K∅ = 20. The parameters for C-LDA and C-HDP
(initial values) were set to ground truths, and, to
retain a fair comparison, versions of ccLDA and
TPYP were ﬁt with both K = K1 and K = K2.

We ﬁnd that ccLDA performs nearly as well as
C-LDA and C-HDP when there is more symme-
try between collection, namely when K1
K2
(Figure 2b). TPYP, on the other hand, performs
well with more topics (2
max(K1, K2) where
the ground truth is K1 & K2). In contrast, C-LDA

×

≈

and C-HDP perform more consistently than other
models across varying degrees of asymmetry.

Partially-shared topics When collections have
the same number of topics, C-LDA, C-HDP and
ccLDA exhibit adequate ﬂexibility, resulting in
similar perplexities. When collections have in-
creasingly few common topics, however, common
and non-common topics from ccLDA are con-
siderably less distinguishable than those from C-
LDA. To evaluate the models’ abilities in such sit-
uations, data was generated for two collections
having K1 = K2 = 50 topics, but with the
shared number of topics K∅
[5, 45]. We also
set δ(0) = δ(1) = 5, and for comparison to ccLDA
we used K = 50.

∈

To measure this distinguishability, we examine
the inferred σ. Recall that σ indicates what per-
centage of a common topic is shared. When a topic
is actually non-common, the value of σ should be
small. We sort σk for k
[1, K] in reverse and
use

∈

Figure 3: Distinguishability (Eq. 2) of topics ﬁt
with C-LDA,C-HDP and ccLDA. Blues lines de-
note ¯σcommon and red denote ¯σnon-common.

proxy human judgements of topic quality, is de-
ﬁned for a topic k as:

C(k) =

2

−

n(n

1)

n
(cid:88)

(wi,wj )∈k
i<j

log

D(wi, wj) + 1
D(wi)D(wj)

¯σcommon = 1
K∅

(cid:80)K∅

k=1 σk

¯σnon-common =

1
K−K∅

(cid:80)K

k=K∅+1 σk

(2)

)
·

the

computes

document

co-
where D(
To accommodate coherence with
occurrence.
common topics in C-LDA that have shared and
collection-speciﬁc components we deﬁne mutual
coherence, MC(k), as

as measures of how well common and non-
common topics were learned2. ¯σcommon is the av-
erage of the K∅ largest σ values, and ¯σnon-common
is the average of the rest. When δ(0) = δ(1) in the
synthetic data, σ in the common portion should be
0.5, whereas it should be 0 in the non-common
part. Figure 3 shows that C-LDA better distin-
guishes between common and non-common top-
ics, especially when K∅ is small. This allows non-
common topics to be separated from the results by
examining the value of σ. C-HDP has similar per-
formance but larger σ values. In ccLDA, all topics
are shared between collections which means that
common and non-common topics are mixed. As
expected, ccLDA performs similarly when all top-
ics are common across collections.

5.2 Semantic Coherence

MC(k) =

1
n2

n
(cid:88)

log

D(wi, wj) + 1
D(wi)D(wj)

wi∈shared,
wj ∈collection-speciﬁc

so that for each collection, C(k) (2n words) is
equal to C(k, shared) + C(k, collection-speciﬁc)
+ MC(k). Table 1 shows the semantic coherence
of topics ﬁt with ccLDA and C-LDA. We used a
10% sample of JSTOR due to the limited speed of
ccLDA, using 50 (common) topics for ccLDA / C-
LDA, and 250 non-common humanities topics for
C-LDA. Although these settings are different for
the models, the science topics are still comparable
because they both have 50 topics. We found that
C-LDA provides improved coherence in nearly all
situations.

Semantic coherence is a corpus-based metric of
the quality of a topic, deﬁned as the average pair-
wise similarity of the top n words (Newman et al.,
2010a; Mimno et al., 2011). A PMI-based form
of coherence, which has been found to be the best

2TPYP is not comparable using this metric, but its hierar-

chical structure will cause topics to mix naturally.

5.2.1

Inference Efﬁciency

To compare the model efﬁciency, we timed runs
on a sample of 5,036 documents from JSTOR (in-
troduced in the next section) with a 20% held-
out and set K = K1 = K2 = 200 run on a
commodity computer with four cores and 16GB
of memory. Figure 4a shows the perplexity over

Figure 4: Using JSTOR: perplexity vs. runtime and iterations (a; left) and perplexity vs. K (b; right).

Coherence

shared component

collection-speciﬁc

all documents
-8.83
-9.04
-7.22
-8.11

science
-7.73
-8.22
-3.68
-5.68

humanities
-8.04
-8.27
-6.11
-7.12

science
-8.38
-8.38
-8.25
-8.24

humanities
-8.14
-8.15
-8.09
-7.88

C-LDA
ccLDA
C-LDA
ccLDA

Mutual Coherence
shared & collection-speciﬁc
humanities
science
-8.37
-8.54
-8.40
-8.69
-7.97
-7.75
-7.95
-8.22

Table 1: Average semantic coherence of the 50 common topics from JSTOR (top) and the average of the
10 best common topics judged by the mean value of different types of coherence (bottom).

time and iterations. The inference algorithm intro-
duces some staleness, which yields slower conver-
gence in the ﬁrst 200 iterations. This effect, how-
ever, is outweighed in both C-LDA and C-HDP by
the increased sampling speed. With 8 threads, C-
LDA not only converges faster, but yields lower
perplexity, likely due to threads introducing addi-
tional stochasticity.

5.3 Performance on JSTOR

To compare our models against slower models, we
sampled 2,465 documents from JSTOR, withhold-
ing 20% as testing set. We ﬁt a model with 100
common and 50 non-common initial topics us-
ing C-HDP, which produced 272 root topics after
2,000 iterations.The perplexity scores are roughly
the same when C-LDA uses the same average
number of topics per collection (Figure 4b), ex-
cept when numbers of topics are very asymmet-
ric. Our model begins to outperform ccLDA after
80 topics. C-HDP did not, however, out-perform
C-LDA despite the original HDP outperforming
LDA. This could be do to the fact that the hier-
archical structure of C-HDP is considerably differ-
ent than the typical 2-level HDP. Held-out perplex-
ity on real data provides a quantitative evaluation
of our models’ performance in a real-world set-
ting. However, the goal of our models is to enable
a deeper analysis of large, weakly-related corpora,
which we next discuss.

5.4 Qualitative Analysis

Our models are designed to enable researchers to
compare collections of text in a way that is scal-
able and sensitive to collection-level asymmetries.
To demonstrate that C-LDA can ﬁll this role, we
ﬁt a model to the entire JSTOR sciences and hu-
manities collections with 100 science topics and
1000 humanities topics (to reveal the less popu-
lar science-related topics in the humanities), and
β = 0.01, δ = 1.0. JSTOR includes books and
journal publications in over 9 million documents
across nearly 3 thousand journals. We used the
journal Science to represent a collection of scien-
tiﬁc research and 76 humanist journals to repre-
sent humanities research3. Words were lemma-
tized, and the most and least frequent words dis-
carded. The ﬁnal humanities collection contained
149,734 documents and the sciences collection
had 160,680 documents, with a combined vocabu-
lary of 21,513 unique words. Together, these col-
lections typify a real-world situation where there
is likely some, but not overwhelming correlation.
The results indicate that the sciences and hu-
manities share several topics. Both exhibit an in-
terest in a “non-human” theme (common topic #2;
Table 2). This topic is quite similar in both collec-
tions (pig and monkey for science documents; bird
and gorilla for humanities documents), while their
shared component forms a cohesive topic (animal,

3The list is available at http://j.mp/humanities-txt.

shared
animal
specie
dog
wild
wolf
monkey
horse
sheep
lion
cat

Topic 2

science
pig
ﬂy
monkey
guinea
primate
worm
dog
cat
mammal
cattle

humanities
beast
creature
nonhuman
natural
humanity
bird
living
gorilla
brute
ape

shared
economic
government
economy
trade
major
growth
capital
industry
institution
support

Topic 21
science
cost
industry
company
price
market
product
income
industrial
business
private

humanities
rural
local
community
village
region
urban
country
area
regional
population

shared
particle
physic
physicist
energy
experiment
event
measurement
atom
interaction
atomic

Topic 23
science
energy
electron
ray
ion
atom
particle
mass
neutron
proton
nucleus

humanities
universe
quantum
physic
technical
scientiﬁc
relativity
physical
mechanic
law
reality

Table 2: Three topics from the JSTOR collections with their top words in shared and speciﬁc components.
Complete results available at http://j.mp/jstor-html.

specie, and monkey). This kind of correlation is
also evident in topic #23, about physics. While the
science documents clearly represent research in
particle physics, it is interesting to ﬁnd the topic is
also represented by humanist research focused on
cultural representations of science. This reﬂects a
growing interest in science and technology studies
that has gained recent traction in the humanities.
Despite their differences, both collections engage
with a similar theme, seen in the shared compo-
nent with words like particle, energy and atom.

The results also indicate that while sciences and
humanities documents can share themes, they of-
ten diverge in how they are discussed. For exam-
ple, common topic #21 could be identiﬁed as eco-
nomic or capitalist, but in the collection-speciﬁc
components, the two disciplines differ in their ar-
ticulatation. Science uses terms like price and
market, indicating an acceptance of free-market
capitalism (especially as it affects the practice of
science), while the humanities, which has long
been critical of free-market capitalism, uses terms
like rural and community, highlighting cultural
facets of modern economics. These results pro-
vide evidence about how ideas move between the
sciences and humanities — a phenomenon that
constitutes a growing area of research for histori-
ans (Galison, 2003; Canales, 2015). C-LDA pro-
vides empirical, measurable, and reproducible ev-
idence of the shared research between these disci-
plines, as well as how concepts are articulated.

6 Discussion

Our models provide a robust way to explore
large and potentially weakly-related text collec-
tions without
the
data. Like ccLDA and TPYP, our models ac-
count for topic-word variation at the collection
level. The models accommodate asymmetry in

imposing assumptions about

the numbers of topics (set in C-LDA, ﬁt in C-
HDP) and provide an efﬁcient inference method
which allows them to ﬁt data with large values
for K, which can help ﬁnd correlations in less
prevalent topics. Our primary contribution is our
models’ ability to accommodate asymmetries be-
tween arbitrary collections. JSTOR, the world’s
largest digital collection of humanities research,
was an ideal application setting given the size,
asymmetry, and comprehensiveness of the human-
ities collection. As we show, humanities and
science research exhibit asymmetries with regard
to vocabulary and topic structure — asymmetries
that would be systematically overlooked using ex-
isting models. By characterizing common top-
ics as mixtures of shared and collection-speciﬁc
components, we can capture a kind of topic-level
homophily, where similar themes are articulated
in different ways due to word-, document-, and
collection-level variation. Future work on these
models could explore methods to ﬁt non-common
topics for both collections. In general, C-LDA and
C-HDP can be used whenever documents are sam-
pled from ostensibly different populations, where
the nature of the difference is unknown.

Acknowledgements

Thanks to David Blei for advice on applications
of the model. This work contains analysis of pri-
vate, or otherwise restricted data, made available
to James Evans and Eamon Duede by ITHAKA
(JSTOR), the opinions of whom are not repre-
sented in this paper. Jaan Altosaar acknowledges
support from the Natural Sciences and Engineer-
ing Research Council of Canada. This work was
supported by a grant from the Templeton Foun-
dation to the Metaknowledge Research Network
and by grant #1158803 from the National Science
Foundation.

References

Loulwah AlSumait, Daniel Barbar´a, James Gentle, and
2009. Topic signiﬁcance
Carlotta Domeniconi.
In Machine
ranking of LDA generative models.
Learning and Knowledge Discovery in Databases,
pages 67–82. Springer.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Wray Buntine and Marcus Hutter. 2010. A Bayesian
arXiv

the Poisson-Dirichlet process.

view of
preprint arXiv:1007.0296.

Jimena Canales.

2015.

The Physicist and the
Philosopher: Einstein, Bergson, and the Debate that
Changed our Understanding of Time. Princeton
University Press, Princeton, NJ.

Changyou Chen, Lan Du, and Wray Buntine. 2011.
Sampling table conﬁgurations for the hierarchical
In Machine Learning
Poisson-Dirichlet process.
and Knowledge Discovery in Databases, pages 296–
311. Springer.

Changyou Chen, Wray Buntine, Nan Ding, Lexing Xie,
and Lan Du. 2014. Differential topic models. IEEE
Transactions on Pattern Analysis and Machine In-
telligence.

M. Denny, J. ben Aaron, H. Wallach, and B. Desmarais.
2014. Modeling email network content and struc-
ture. In the 72nd Annual Midwest Political Science
Association Conference, 2014; the Northeast Politi-
cal Methodology Meeting, 2014; the 7th Annual Po-
litical Networks Conference, the Society for Political
Methodology 31st Annual Summer Meeting.

Peter Galison. 2003. Poincar’s Maps. Norton, New

York, NY.

Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 50–57.

Richard A Kronmal and Arthur V Peterson Jr. 1979.
On the alias method for generating random variables
from a discrete distribution. The American Statisti-
cian, 33(4):214–218.

Alp Kucukelbir and David M Blei. 2014. Proﬁle pre-
dictive inference. arXiv preprint arXiv:1411.0292.

Aaron Q Li, Amr Ahmed, Sujith Ravi, and Alexan-
der J Smola. 2014. Reducing the sampling com-
plexity of topic models. In Proceedings of the 20th
ACM SIGKDD international conference on knowl-
edge discovery and data mining, pages 891–900.

Mian Lu, Ge Bai, Qiong Luo, Jie Tang, and Jiuxin
Zhao. 2013. Accelerating topic model training on
a single machine. In Web Technologies and Appli-
cations, pages 184–195. Springer.

George Marsaglia, Wai Wan Tsang, and Jingbo Wang.
2004. Fast generation of discrete random variables.
Journal of Statistical Software, 11:1–8.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262–
272.

David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. The Journal of Machine Learning Re-
search, 10:1801–1828.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin.
2010a. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100–108.

David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010b. Evaluating
In Proceedings
topic models for digital libraries.
of the 10th Annual Joint Conference on Digital Li-
braries, JCDL ’10, pages 215–224. ACM.

Michael Paul and Roxana Girju. 2009. Cross-cultural
analysis of blogs and forums with mixed-collection
In Proceedings of the 2009 Confer-
topic models.
ence on Empirical Methods in Natural Language
Processing: Volume 3, pages 1408–1417.

Alexander Smola and Shravan Narayanamurthy. 2010.
An architecture for parallel topic models. Proceed-
ings of the VLDB Endowment, 3(1-2):703–710.

Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the american statistical associa-
tion, 101(476).

Hanna M Wallach, David Mimno, and Andrew Mccal-
lum. 2009a. Rethinking LDA: Why priors matter.
In Advances in Neural Information Processing Sys-
tems, pages 1973–1981.

Hanna M. Wallach, Iain Murray, Ruslan Salakhutdi-
nov, and David Mimno. 2009b. Evaluation methods
In Proceedings of the 26th An-
for topic models.
nual International Conference on Machine Learn-
ing, ICML ’09, pages 1105–1112, New York, NY,
USA. ACM.

Hanna M Wallach. 2008. Structured topic models for
language. Unpublished doctoral dissertation, Univ.
of Cambridge.

Tze-I Yang, Andrew J Torget, and Rada Mihalcea.
2011. Topic modeling on historical newspapers. In
Proceedings of the 5th ACL-HLT Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, pages 96–104.

Limin Yao, David Mimno, and Andrew McCallum.
2009. Efﬁcient methods for topic model inference
In Proceed-
on streaming document collections.
ings of the 15th ACM SIGKDD international con-
ference on knowledge discovery and data mining,
pages 937–946.

Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang
Wei, Xun Zheng, Eric Po Xing, Tie-Yan Liu, and
Wei-Ying Ma. 2015. Lightlda: Big topic models
on modest computer clusters. In Proceedings of the
24th International Conference on World Wide Web,
pages 1351–1361. International World Wide Web
Conferences Steering Committee.

ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative
In Proceedings of the tenth ACM
text mining.
SIGKDD international conference on knowledge
discovery and data mining, pages 743–748.

Fast, Flexible Models for Discovering Topic Correlation across
Weakly-Related Collections

Jingwei Zhang1, Aaron Gerow2, Jaan Altosaar3, James Evans2,4, Richard Jean So5

1Department of Computer Science, Columbia University
jz2541@columbia.edu
2 Computation Institute, University of Chicago

{

@uchicago.edu

gerow,jevans
}
3 Department of Physics, Princeton University
altosaar@princeton.edu
4 Department of Sociology, University of Chicago
5 Department of English Language and Literature, University of Chicago
richardjeanso@uchicago.edu

Abstract

This paper

Weak topic correlation across document
collections with different numbers of
topics in individual collections presents
challenges for existing cross-collection
topic models.
introduces
two probabilistic topic models, Correlated
LDA (C-LDA) and Correlated HDP (C-
HDP). These address problems that can
arise when analyzing large, asymmetric,
and potentially weakly-related collections.
Topic correlations in weakly-related col-
lections typically lie in the tail of the topic
distribution, where they would be over-
looked by models unable to ﬁt large num-
bers of topics. To efﬁciently model this
long tail for large-scale analysis, our mod-
els implement a parallel sampling algo-
rithm based on the Metropolis-Hastings
and alias methods (Yuan et al., 2015).
The models are ﬁrst evaluated on syn-
thetic data, generated to simulate vari-
ous collection-level asymmetries. We then
present a case study of modeling over
300k documents in collections of sciences
and humanities research from JSTOR.

1

Introduction

Comparing large text collections is a critical task
for the curation and analysis of human cultural
history. Achievements of research and schol-
arship are most accessible through textual arti-
facts, which are increasingly available in digital
archives. Text-based research, often undertaken
by humanists, historians, lexicographers, and cor-

pus linguists, explores patterns of words in docu-
ments across time-periods and distinct collections
of text. Here, we introduce two new topic models
designed to compare large collections, Correlated
LDA (C-LDA) and Correlated HDP (C-HDP),
which are sensitive to document-topic asymme-
try (where collections have different topic distribu-
tions) and topic-word asymmetry (where a single
topic has different word distributions in each col-
lection). These models seek to address termino-
logical questions, such as how a topic on physics
is articulated distinctively in scientiﬁc compared
to humanistic research. Accommodating poten-
tial collection-level asymmetries is particularly
important when researchers seek to analyze col-
lections with little prior knowledge about shared
or collection-speciﬁc topic structure. Our mod-
els extend existing cross-collection approaches to
accommodate these asymmetries and implement
an efﬁcient parallel sampling algorithm enabling
users to examine the long tail of topics in particu-
larly large collections.

Using topic models for comparative text min-
ing was introduced by Zhai et al. (2004), who de-
veloped the ccMix model which extended pLSA
(Hofmann, 1999). Later work by Paul and Girju
(2009) developed ccLDA, which adopted the hier-
archical Bayes framework of Latent Dirichlet Al-
location or LDA (Blei et al., 2003). These mod-
els account for topic-word asymmetry by assum-
ing variation in the vocabularies of topics is due
to collection-level differences. Nevertheless, they
require the same topics to be present in each col-
lection. These models are useful for comparing
collections under speciﬁc assumptions, but cannot
accommodate collection-topic asymmetry (which

5
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
6
5
4
0
.
8
0
5
1
:
v
i
X
r
a

arises in collections that do not share every topic
or that have different numbers of topics). In situa-
tions where collections do not share all topics, the
results often include junk, mixed, or sparse top-
ics, making them difﬁcult to interpret (Paul and
Girju, 2009). Such asymmetries make it difﬁcult
to use models like ccLDA and ccMix when little
is known about collections in advance. This mo-
tivates our efforts to model variation in the long
tail of topic distributions, where correlations are
more likely to appear when collections are weakly
related.

C-LDA and C-HDP extend ccLDA (Paul and
Girju, 2009) to accommodate collection-topic
level asymmetries, particularly by allowing non-
common topics to appear in each collection. This
added ﬂexibility allows our models to discover
topic correlations across arbitrary collections with
different numbers of topics, even when there are
few (or unknown) numbers of common topics. To
demonstrate the effectiveness of our models, we
evaluate them on synthetic data and show that they
outperform related models such as ccLDA and dif-
ferential topic models (Chen et al., 2014). We then
ﬁt C-LDA to two large collections of humanities
and sciences documents from JSTOR. Such histor-
ical analyses of text would be intractable without
an efﬁcient sampler. An optimized sampler is re-
quired in such situations because common topics
in weakly-correlated collections are usually found
in the tail of the document-topic distribution of a
sufﬁciently large set of topics. To make this fea-
sible on large datasets such as JSTOR, we employ
a parallelized Metropolis-Hastings (Kronmal and
Peterson Jr, 1979) and alias-table sampling frame-
work, adapted from LightLDA (Yuan et al., 2015).
These optimizations, which achieve
(1) amor-
tized sampling time per token, allow our models
to be ﬁt to large corpora with up to thousands of
topics in a matter of hours — an order of magni-
tude speed-up from ccLDA.

O

After reviewing work related to topic modeling
across collections, section 3 describes C-LDA and
C-HDP, and then details their technical relation-
ship to existing models. Section 5 introduces the
synthetic data and part of the JSTOR corpus used
in our evaluations. We then compare our models’
performances to other models in terms of held-
out perplexity and a measure of distinguishabil-
ity. The ﬁnal results section exempliﬁes the use
of C-LDA in a qualitative analysis of humanities

and sciences research. We conclude with a brief
discussion of the strengths of C-LDA and C-HDP,
and outline directions for future work and applica-
tions.

2 Related Work

Our models seek to enable users to compare large
collections that may only be weakly correlated
and that may contain different numbers of topics.
While topic models could be ﬁt to separate collec-
tions to make post-hoc comparisons (Denny et al.,
2014; Yang et al., 2011), our goal is to account for
both document-topic asymmetry and topic-word
asymmetry “in-model”. In short, we seek to model
the correlation between arbitrary collections. Pri-
oritizing in-model solutions for document-topic
asymmetry has been explored elsewhere, such as
in hierarchical Dirichlet processes (HDP), which
use an additional level to account for collection
variations in document-topic distributions (Teh et
al., 2006).

One method designed to model

topic-word
asymmetry is ccMix (Zhai et al., 2004), which
models the generative probability of a word in
topic z from collection c as a mixture of shared
and collection-speciﬁc distributions θz:

p(w) = λc p(w

θz) + (1
|

−

λc) p(w

θz,c)
|

where θz,c is collection-speciﬁc and λc controls
the mixing between shared and collection-speciﬁc
topics. ccLDA extends ccMix to the LDA frame-
work and adds a beta prior over λc that reduces
sensitivity to input parameters (Paul and Girju,
2009). Another approach, differential topic mod-
els (Chen et al., 2014), is based on hierarchical
Bayesian models over topic-word distributions.
This method uses the transformed Pitman-Yor pro-
cess (TPYP) to model topic-word distributions in
each collection, with shared common base mea-
sures. As (Paul and Girju, 2009) note, ccLDA
cannot accommodate a topic if it is not com-
mon across collections — an assumption made by
ccMix, ccLDA and the TPYP. In a situation where
a topic is found in only one collection, it would
either dominate the shared topic portion (resulting
in a noisy, collection-speciﬁc portion), or it would
appear as a mixed topic, revealing two sets of un-
related words (Newman et al., 2010b). C-LDA
ameliorates this situation by allowing the number
of common and non-common topics to be speci-
ﬁed separately and by efﬁciently sampling the tail

of the document-topic distribution, allowing users
to examine less prominent regions of the topic
space. C-HDP also grants collections document-
topic independence using a hierarchical structure
to model the differences between collections.

O

Due to increased demand for scalable topic
model implementations, there has been a prolif-
eration of optimized methods for efﬁcient infer-
ence, such as SparseLDA (Yao et al., 2009) and
AliasLDA (Li et al., 2014). AliasLDA achieves
(Kd) complexity by using the Metropolis-
O
Hastings-Walker algorithm and an alias table to
(1) time. Al-
sample topic-word distributions in
though this strategy introduces temporal staleness
in the updates of sufﬁcient statistics, the lag is
overcome by more iterations, and converges sig-
niﬁcantly faster. A similar technique by Yuan et al.
(2015), LightLDA, employs cycle-based Metropo-
lis Hastings mixing with alias tables for both
document-topic and topic-word distributions. De-
spite introducing lag in the sufﬁcient statistics,
this method achieves
(1) amortized sampling
O
complexity and results in even faster convergence
than AliasLDA. In addition to being fully paral-
lelized, C-LDA adopts this sampling framework to
make comparing large collections more tractable
for large numbers of topics. Our models’ efﬁcient
sampling methods allow users to ﬁt large num-
bers of topics to big datasets where variation might
not be observed in sub-sampled datasets or models
with fewer topics.

3 The Models

3.1 Correlated LDA

In ccLDA (and ccMix), each topic has shared
and collection-speciﬁc components for each col-
lection. C-LDA extends ccLDA to make it more
robust with respect to topic asymmetries between
collections (Figure 1a). The crucial extension is
that by allowing each collection to deﬁne a set of
non-common topics in addition to common top-
ics, the model removes an assumption imposed by
ccLDA and other inter-collection models, namely
that collections have the same number of topics.
As a result, C-LDA is suitable for collections with-
out a large proportion of common topics, and can
also reduce noise (discussed in Section 2). To
achieve this, C-LDA assumes document d in col-
lection c has a multinomial document-topic dis-
tribution θ with an asymmetric Dirichlet prior
for Kc topics, where the ﬁrst K∅ are common

across collections. It is also possible to introduce
a tree structure into the model that uses a bino-
mial distribution to decide whether a word was
drawn from common or non-common topics. This
yields collection-speciﬁc background topics by us-
ing a binomial distribution instead of a multino-
mial. However, we prefer the simpler, non-tree
version because background topics are unneces-
sary when using an asymmetric α prior (Wallach
et al., 2009a).

The generative process for C-LDA is as follows:

1. Sample a distribution φk (shared component) from Dir(β)
and a distribution σk from Beta(δ1, δ2) for each common
topic k ∈ {1, . . . , K ∅};

2. For each collection c, sample a distribution φc

k (collection-
speciﬁc component) from Dir(β) for each common topic
k ∈ {1, . . . , K ∅} and non-common topic k ∈ {K ∅ +
1, . . . , Kc};

3. For each document d in c, sample a distribution θ from

Dir(αc);

4. For each word wi in d:

(a) Sample a topic zi ∈ {1, . . . , Kc} from Multi(θ);
(b) If zi ≤ K ∅, sample yi from Binomial(σzi );
(c) Sample wi from Multi(φξ
null
c

, zi ≤ K ∅ and yi = 0;
, otherwise.

zi ), where

ξ =

(cid:26)

∃

Note that to capture common topics, K∅ should
c where Kc = K∅. Other-
be set such that
wise, words sampled as a non-common topic will
not have information about non-common topics in
other collections. Then a “common-topic word”
is found among non-common topics in all col-
lections (a local minima) and it will take a long
time to stabilize as a common topic. To avoid
this, when determining the number of topics for
sampling, the number of non-common topics for
the collection with the smallest number of total
topics should be zero. After inference, to distin-
guish common and non-common topics in this col-
lection, we model σ independently by assuming
collections have the same mixing ratio for com-
mon topics. With this reasonable assumption and
an asymmetric α, common topics become sparse
enough that some σ distributions reduce nearly to
0, distinguishing them as non-common topics. Al-
though this may seem counterintuitive, it does not
negatively affect results.

Three kinds of collection-level imbalance can
confound inter-collection topic models: 1) in the
numbers of topics between collections, 2) in the
numbers of documents between collections, and
3) in the document-topic distributions. Each of

α

θ

z

w

C

D

W

y

δ

σ

K∅

β

φ
K∅ + (cid:80) Kc

γ

H

G0

Gc

Gd

z

w

C

D

W

y

δ

σ

K(∞)

α0

α1

β

φ

K(∞)

Figure 1: Graphical models of C-LDA (a; left) and C-HDP (b; right).

these can cause topics in different collections to
have signiﬁcantly different numbers of words as-
signed to the same topic. In this way, a topic can
be dominated by the collection comprising most
of its words. C-LDA addresses imbalances in the
document-topic distributions between collections
by estimating α. For imbalance in the number of
topics and documents, C-LDA mimics document
over-sampling in the Gibbs sampler using a differ-
ent unit-value in the word count table for each col-
lection. Speciﬁcally, a unit ηc is chosen for each
collection such that the average equivalent num-
ber of assigned words per-topic ((cid:80)
d∈c ηcNd/Kc,
where Nd is the length of document d) is equal.
This process both increases the topic quality (in
terms of collection balance) in the resulting held-
out perplexity of the model.

3.2 Correlated HDP

∃

c such
To alleviate C-LDA’s requirement that
that Kc = K∅, we introduce a variant of the
model, the correlated hierarchical Dirichlet pro-
cess (C-HDP), that uses a 3-level hierarchical
Dirichlet process (Teh et al., 2006). The gener-
ative process for C-HDP is the same as C-LDA
shown above, except that here we assume a word’s
topic, z, is generated by a hierarchical Dirichlet
process:

|

G0

DP(γ, H)
DP(α0, G0)
DP(α1, Gc)
Gd

γ, H
Gc
α0, G0
|
α1, Gc
Gd
|
Gd
z
|

∼
∼
∼
∼
where G0 is a base measure for each collection-
level Dirichlet process, and Gc are base measures
of document-level Dirichlet processes in each col-
lection (Figure 1b). Thus, documents from the

same collection will have similar topic distribu-
tions compared to those from other collections,
and collections are allowed to have distinct sets of
topics due to the use of HDP.

4

Inference

4.1 Posterior Inference in C-LDA

C-LDA can be trained using collapsed Gibbs sam-
pling with φ, θ, and σ integrated out. Given the
status assignments of other words, the sampling
distribution for word wi is given by:

w, y−i, z−i, δ, α, β)
|

p(yi, zi
(N (d, zi) + αc,zi)
(cid:124)
(cid:123)(cid:122)
(cid:125)
qd

∝





(cid:124)

×

N (yi, zi) + δyi
N (zi) + (cid:80)
N (wi, zi, c) + β
N (zi, c) + V β

k δk ×

(cid:123)(cid:122)
qw

N (wi, yi, zi, ζ) + β
N (yi, zi, ζ) + V β

K∅

zi

≤

zi > K∅

(cid:125)

(1)

where ζ =

(cid:26) ∗
c

yi = 0
yi = 1 , N (

· · ·

) is the number of

status assignments for (

), not including wi.

· · ·
Inference in C-LDA employs two optimiza-
tions: a parallelized sampler and an efﬁcient sam-
pling algorithm (Algorithm 1). We use the paral-
lel schema in (Smola and Narayanamurthy, 2010;
Lu et al., 2013) which applies atomic updates to
the sufﬁcient statistics to avoid race conditions.
The key idea behind the optimized sampler is the
combination of alias tables and the Metropolis-
Hastings method (MH), adapted from (Yuan et al.,
2015; Li et al., 2014). Metropolis-Hastings is a
Markov chain Monte Carlo method that uses a pro-
posal distribution to approximate the true distribu-

Algorithm 1 Sampling in C-LDA

repeat

for all documents {d} in parallel do

for words {w} in d do

z ← CYCLEMH(p, qw, qd, z)
sample y given z

Atomic update sufﬁcient statics

Estimate α
until convergence

procedure CYCLEMH(p, qw, qd, z)

for i = 1 to N do

if i is even then

else

proposal q ← qw

proposal q ← qd

sample z(cid:48) ∼ ALIASTABLE(q)
if RandUnif(1) < min(1, p(z(cid:48))q(z)

p(z)q(z(cid:48)) ) then

z ← z(cid:48)

return z

tion when exact sampling is difﬁcult. In a compli-
mentary way, Walker’s alias method (2004) allows
one to effectively sample from a discrete distribu-
tion by using an alias table, constructed in
(K)
O
time, from which we can sample in
(1). Thus,
reusing the sampler K times as the proposal distri-
bution for Metropolis-Hastings yields
(1) amor-
tized sampling time per-token.

O

O

Notice that in Eq. 1, the sampling distribution is
the product of a single document-dependent term
qd and a single word-dependent term qw. After
burn-in, both terms will be sparse (without the
smoothing factor). It is therefore reasonable to use
qd and qw as cycle proposals (Yuan et al., 2015),
alternating them in each Metropolis-Hastings step.
Our experiments show that the primary drawback
of this method — stale sufﬁcient statistics — does
not empirically affect convergence. Our imple-
mentation uses proposal distributions qw and qd,
with y marginalized out. After the Metropolis-
Hastings steps, y is sampled to update z, to reduce
the size of the alias tables, yielding even faster
convergence.

Lastly, the use of an asymmetric α allows C-
LDA to discover correlations between less dom-
inant topics across collections (Wallach et al.,
2009a). We use Minka’s ﬁxed-point method, with
a gamma hyper-prior to optimize αc for each col-
lection separately (Wallach, 2008). All other hy-
perparameters were ﬁxed during inference.

4.2 Posterior Inference in C-HDP

C-HDP uses the block sampling algorithm de-
scribed in (Chen et al., 2011), which is based on

the Chinese restaurant process metaphor. Here,
rather than tracking all assignments (as the sam-
plers given in (Teh et al., 2006)), table indicators
are used to track only the start of new tables, which
allows us to adopt the same sampling framework
as C-LDA. In the Chinese restaurant process, each
Dirichlet process in the hierarchical structure is
represented as a restaurant with an inﬁnite num-
ber of tables, each serving the same dish. New
customers can either join a table with existing cus-
tomers, or start a new table. If a new table is cho-
sen, a proxy customer will be sent to the parent
restaurant to determine the dish served to that ta-
ble.

∅

In the block sampler, indicators are used to de-
note a customer creating a table (or tables) up to
level u (0 as the root, 1 for collection level, and 2
for the document level), and u =
indicates no
table has been created. For example, when a cus-
tomer creates a table at the collection level, and
the proxy customer in the collection level creates
a table at the root level, u is 0. With this metaphor,
let nlz be the number of customers (including their
proxies) served dish z at restaurant l, and let tlz be
the number of tables serving dish z at restaurant
l (l = 0 for root, l = c for collection level or
l = d for document level), with N0 = (cid:80)
z n0z and
Nc = (cid:80)
z ncz. By the chain rule, the conditional
probability of the state assignments for wi, given
all others, is

∝

p(yi, zi, ui|w, y−i, z−i, u−i, . . .)
N (y, z) + δy
N (z) + (cid:80)
k δk

γα0
γ+N0

N (w, y, z, ζ) + β
N (y, z, ζ) + V β

×

×




α0
γ+N0
Sncz +1
tcz
Sncz
tcz

α0+N1
α1

S

Sncz +1
tcz +1
Sncz
tcz
ndz +1
S
tdz +1
ndz
S
tdz
ndz +1
tdz
S

ndz
tdz

S

ndz +1
tdz +1
ndz
S
tdz

n2

0z (tcz +1)(tdz +1)
(n0z +1)(ncz +1)(ndz +1)

(tdz +1)(ncz −tcz +1)
(ncz +1)(ndz +1)

ndz −tdz +1
ndz +1

u = 0

u = 1

u = 2

u = ∅

Here, Sn
t is the Stirling number, the ratios of which
can be efﬁciently precomputed (Buntine and Hut-
ter, 2010). The concentration parameters γ, α0,
and α1 can be sampled using the auxiliary variable
method (Teh et al., 2006).

Note that because conditional probability has
the same separability as C-LDA (to give term qw
and qd), the same sampling framework can be
used with two alterations: 1) when a new topic
is created or removed at the root, collection, or
document level, the related alias tables must be
reset, which makes the sampling slightly slower

Figure 2: Held-out perplexity of C-LDA, C-HDP, ccLDA and TPYP ﬁt to synthetic data, where K1 =
K2 = K (a; left) and data with an asymmetric number of topics (b; right).

O

than
(1), and 2) while the document alias table
samples z and u simultaneously, after sampling z
from the word alias table u must be sampled using
tlc/nlz (Chen et al., 2011). Parallelizing C-HDP
requires an additional empirical method of merg-
ing new topics between threads (Newman et al.,
2009), which is outside of the scope of this work.
Our implementation of both models, C-LDA and
C-HDP, are open-sourced online 1.

5 Experiments

5.1 Model Comparison

We use perplexity on held-out documents to eval-
uate the performance of C-LDA and C-HDP. In all
experiments, the gamma prior for α in C-LDA was
set to (1, 1), and (5, 0.1), (5, 0.1), (0.1, 0.1) for
γ, α0, α1 respectively in C-HDP. In the hold-out
procedure, 20% of documents were randomly se-
lected as test data. LDA, C-LDA and ccLDA were
run for 1,000 iterations and C-HDP and the TII-
variant of TPYP for 1,500 iterations (unless oth-
erwise noted), all of which converged to a state
where change in perplexity was less than 1% for
ten consecutive iterations.

Perplexity was calculated from the marginal
likelihood of a held-out document p(w
Φ, α), es-
|
timated using the “left-to-right” method (Wallach
et al., 2009b). Because it is difﬁcult to vali-
date real-world data that exhibits different kinds
of asymmetry, we use synthetic data generated
speciﬁcally for our evaluation tasks (AlSumait et
al., 2009; Wallach et al., 2009b; Kucukelbir and
Blei, 2014).

5.1.1 Topic Correlation
C-LDA is unique in the amount of freedom it al-
lows when setting the number of topics for col-

1https://github.com/iceboal/correlated-lda

lections. To assess the models’ performances with
various topic correlations in a fair setting, we gen-
erated two collections of synthetic data by fol-
lowing the generative process (varying the num-
ber of topics) and measured the models’ perplex-
ities against the ground truth parameters. In each
experiment, two collections were generated, each
with 1,000 documents containing 50 words each,
over a vocabulary of 3,000. β and δ were ﬁxed at
0.01 and 1.0 respectively, and α was asymmetri-
cally deﬁned as 1/(i + √Kc) for i
1].

[0, Kc

∈

−

Completely shared topics The assumptions im-
posed by ccLDA and TPYP effectively make them
a special case of our model where K∅ = K1 =
K2 = . . .. To compare results, data was gener-
ated such that all numbers of topics were equal to
[10, 90]. Additionally, all models were con-
K
ﬁgured to use this ground truth parameter when
training. Not surprisingly, ccLDA, C-LDA, and C-
HDP have almost the same perplexity with respect
to K because their structure is the same when all
topics are shared (Figure 2a).

∈

Asymmetric numbers of topics To explore the
effect of asymmetry in the number of topics, data
was generated such that one collection had K1
∈
[20, 60] topics while a second had a ﬁxed K2 = 40
topics. The number of shared topics was set to
K∅ = 20. The parameters for C-LDA and C-HDP
(initial values) were set to ground truths, and, to
retain a fair comparison, versions of ccLDA and
TPYP were ﬁt with both K = K1 and K = K2.

We ﬁnd that ccLDA performs nearly as well as
C-LDA and C-HDP when there is more symme-
try between collection, namely when K1
K2
(Figure 2b). TPYP, on the other hand, performs
well with more topics (2
max(K1, K2) where
the ground truth is K1 & K2). In contrast, C-LDA

×

≈

and C-HDP perform more consistently than other
models across varying degrees of asymmetry.

Partially-shared topics When collections have
the same number of topics, C-LDA, C-HDP and
ccLDA exhibit adequate ﬂexibility, resulting in
similar perplexities. When collections have in-
creasingly few common topics, however, common
and non-common topics from ccLDA are con-
siderably less distinguishable than those from C-
LDA. To evaluate the models’ abilities in such sit-
uations, data was generated for two collections
having K1 = K2 = 50 topics, but with the
shared number of topics K∅
[5, 45]. We also
set δ(0) = δ(1) = 5, and for comparison to ccLDA
we used K = 50.

∈

To measure this distinguishability, we examine
the inferred σ. Recall that σ indicates what per-
centage of a common topic is shared. When a topic
is actually non-common, the value of σ should be
small. We sort σk for k
[1, K] in reverse and
use

∈

Figure 3: Distinguishability (Eq. 2) of topics ﬁt
with C-LDA,C-HDP and ccLDA. Blues lines de-
note ¯σcommon and red denote ¯σnon-common.

proxy human judgements of topic quality, is de-
ﬁned for a topic k as:

C(k) =

2

−

n(n

1)

n
(cid:88)

(wi,wj )∈k
i<j

log

D(wi, wj) + 1
D(wi)D(wj)

¯σcommon = 1
K∅

(cid:80)K∅

k=1 σk

¯σnon-common =

1
K−K∅

(cid:80)K

k=K∅+1 σk

(2)

)
·

the

computes

document

co-
where D(
To accommodate coherence with
occurrence.
common topics in C-LDA that have shared and
collection-speciﬁc components we deﬁne mutual
coherence, MC(k), as

as measures of how well common and non-
common topics were learned2. ¯σcommon is the av-
erage of the K∅ largest σ values, and ¯σnon-common
is the average of the rest. When δ(0) = δ(1) in the
synthetic data, σ in the common portion should be
0.5, whereas it should be 0 in the non-common
part. Figure 3 shows that C-LDA better distin-
guishes between common and non-common top-
ics, especially when K∅ is small. This allows non-
common topics to be separated from the results by
examining the value of σ. C-HDP has similar per-
formance but larger σ values. In ccLDA, all topics
are shared between collections which means that
common and non-common topics are mixed. As
expected, ccLDA performs similarly when all top-
ics are common across collections.

5.2 Semantic Coherence

MC(k) =

1
n2

n
(cid:88)

log

D(wi, wj) + 1
D(wi)D(wj)

wi∈shared,
wj ∈collection-speciﬁc

so that for each collection, C(k) (2n words) is
equal to C(k, shared) + C(k, collection-speciﬁc)
+ MC(k). Table 1 shows the semantic coherence
of topics ﬁt with ccLDA and C-LDA. We used a
10% sample of JSTOR due to the limited speed of
ccLDA, using 50 (common) topics for ccLDA / C-
LDA, and 250 non-common humanities topics for
C-LDA. Although these settings are different for
the models, the science topics are still comparable
because they both have 50 topics. We found that
C-LDA provides improved coherence in nearly all
situations.

Semantic coherence is a corpus-based metric of
the quality of a topic, deﬁned as the average pair-
wise similarity of the top n words (Newman et al.,
2010a; Mimno et al., 2011). A PMI-based form
of coherence, which has been found to be the best

2TPYP is not comparable using this metric, but its hierar-

chical structure will cause topics to mix naturally.

5.2.1

Inference Efﬁciency

To compare the model efﬁciency, we timed runs
on a sample of 5,036 documents from JSTOR (in-
troduced in the next section) with a 20% held-
out and set K = K1 = K2 = 200 run on a
commodity computer with four cores and 16GB
of memory. Figure 4a shows the perplexity over

Figure 4: Using JSTOR: perplexity vs. runtime and iterations (a; left) and perplexity vs. K (b; right).

Coherence

shared component

collection-speciﬁc

all documents
-8.83
-9.04
-7.22
-8.11

science
-7.73
-8.22
-3.68
-5.68

humanities
-8.04
-8.27
-6.11
-7.12

science
-8.38
-8.38
-8.25
-8.24

humanities
-8.14
-8.15
-8.09
-7.88

C-LDA
ccLDA
C-LDA
ccLDA

Mutual Coherence
shared & collection-speciﬁc
humanities
science
-8.37
-8.54
-8.40
-8.69
-7.97
-7.75
-7.95
-8.22

Table 1: Average semantic coherence of the 50 common topics from JSTOR (top) and the average of the
10 best common topics judged by the mean value of different types of coherence (bottom).

time and iterations. The inference algorithm intro-
duces some staleness, which yields slower conver-
gence in the ﬁrst 200 iterations. This effect, how-
ever, is outweighed in both C-LDA and C-HDP by
the increased sampling speed. With 8 threads, C-
LDA not only converges faster, but yields lower
perplexity, likely due to threads introducing addi-
tional stochasticity.

5.3 Performance on JSTOR

To compare our models against slower models, we
sampled 2,465 documents from JSTOR, withhold-
ing 20% as testing set. We ﬁt a model with 100
common and 50 non-common initial topics us-
ing C-HDP, which produced 272 root topics after
2,000 iterations.The perplexity scores are roughly
the same when C-LDA uses the same average
number of topics per collection (Figure 4b), ex-
cept when numbers of topics are very asymmet-
ric. Our model begins to outperform ccLDA after
80 topics. C-HDP did not, however, out-perform
C-LDA despite the original HDP outperforming
LDA. This could be do to the fact that the hier-
archical structure of C-HDP is considerably differ-
ent than the typical 2-level HDP. Held-out perplex-
ity on real data provides a quantitative evaluation
of our models’ performance in a real-world set-
ting. However, the goal of our models is to enable
a deeper analysis of large, weakly-related corpora,
which we next discuss.

5.4 Qualitative Analysis

Our models are designed to enable researchers to
compare collections of text in a way that is scal-
able and sensitive to collection-level asymmetries.
To demonstrate that C-LDA can ﬁll this role, we
ﬁt a model to the entire JSTOR sciences and hu-
manities collections with 100 science topics and
1000 humanities topics (to reveal the less popu-
lar science-related topics in the humanities), and
β = 0.01, δ = 1.0. JSTOR includes books and
journal publications in over 9 million documents
across nearly 3 thousand journals. We used the
journal Science to represent a collection of scien-
tiﬁc research and 76 humanist journals to repre-
sent humanities research3. Words were lemma-
tized, and the most and least frequent words dis-
carded. The ﬁnal humanities collection contained
149,734 documents and the sciences collection
had 160,680 documents, with a combined vocabu-
lary of 21,513 unique words. Together, these col-
lections typify a real-world situation where there
is likely some, but not overwhelming correlation.
The results indicate that the sciences and hu-
manities share several topics. Both exhibit an in-
terest in a “non-human” theme (common topic #2;
Table 2). This topic is quite similar in both collec-
tions (pig and monkey for science documents; bird
and gorilla for humanities documents), while their
shared component forms a cohesive topic (animal,

3The list is available at http://j.mp/humanities-txt.

shared
animal
specie
dog
wild
wolf
monkey
horse
sheep
lion
cat

Topic 2

science
pig
ﬂy
monkey
guinea
primate
worm
dog
cat
mammal
cattle

humanities
beast
creature
nonhuman
natural
humanity
bird
living
gorilla
brute
ape

shared
economic
government
economy
trade
major
growth
capital
industry
institution
support

Topic 21
science
cost
industry
company
price
market
product
income
industrial
business
private

humanities
rural
local
community
village
region
urban
country
area
regional
population

shared
particle
physic
physicist
energy
experiment
event
measurement
atom
interaction
atomic

Topic 23
science
energy
electron
ray
ion
atom
particle
mass
neutron
proton
nucleus

humanities
universe
quantum
physic
technical
scientiﬁc
relativity
physical
mechanic
law
reality

Table 2: Three topics from the JSTOR collections with their top words in shared and speciﬁc components.
Complete results available at http://j.mp/jstor-html.

specie, and monkey). This kind of correlation is
also evident in topic #23, about physics. While the
science documents clearly represent research in
particle physics, it is interesting to ﬁnd the topic is
also represented by humanist research focused on
cultural representations of science. This reﬂects a
growing interest in science and technology studies
that has gained recent traction in the humanities.
Despite their differences, both collections engage
with a similar theme, seen in the shared compo-
nent with words like particle, energy and atom.

The results also indicate that while sciences and
humanities documents can share themes, they of-
ten diverge in how they are discussed. For exam-
ple, common topic #21 could be identiﬁed as eco-
nomic or capitalist, but in the collection-speciﬁc
components, the two disciplines differ in their ar-
ticulatation. Science uses terms like price and
market, indicating an acceptance of free-market
capitalism (especially as it affects the practice of
science), while the humanities, which has long
been critical of free-market capitalism, uses terms
like rural and community, highlighting cultural
facets of modern economics. These results pro-
vide evidence about how ideas move between the
sciences and humanities — a phenomenon that
constitutes a growing area of research for histori-
ans (Galison, 2003; Canales, 2015). C-LDA pro-
vides empirical, measurable, and reproducible ev-
idence of the shared research between these disci-
plines, as well as how concepts are articulated.

6 Discussion

Our models provide a robust way to explore
large and potentially weakly-related text collec-
tions without
the
data. Like ccLDA and TPYP, our models ac-
count for topic-word variation at the collection
level. The models accommodate asymmetry in

imposing assumptions about

the numbers of topics (set in C-LDA, ﬁt in C-
HDP) and provide an efﬁcient inference method
which allows them to ﬁt data with large values
for K, which can help ﬁnd correlations in less
prevalent topics. Our primary contribution is our
models’ ability to accommodate asymmetries be-
tween arbitrary collections. JSTOR, the world’s
largest digital collection of humanities research,
was an ideal application setting given the size,
asymmetry, and comprehensiveness of the human-
ities collection. As we show, humanities and
science research exhibit asymmetries with regard
to vocabulary and topic structure — asymmetries
that would be systematically overlooked using ex-
isting models. By characterizing common top-
ics as mixtures of shared and collection-speciﬁc
components, we can capture a kind of topic-level
homophily, where similar themes are articulated
in different ways due to word-, document-, and
collection-level variation. Future work on these
models could explore methods to ﬁt non-common
topics for both collections. In general, C-LDA and
C-HDP can be used whenever documents are sam-
pled from ostensibly different populations, where
the nature of the difference is unknown.

Acknowledgements

Thanks to David Blei for advice on applications
of the model. This work contains analysis of pri-
vate, or otherwise restricted data, made available
to James Evans and Eamon Duede by ITHAKA
(JSTOR), the opinions of whom are not repre-
sented in this paper. Jaan Altosaar acknowledges
support from the Natural Sciences and Engineer-
ing Research Council of Canada. This work was
supported by a grant from the Templeton Foun-
dation to the Metaknowledge Research Network
and by grant #1158803 from the National Science
Foundation.

References

Loulwah AlSumait, Daniel Barbar´a, James Gentle, and
2009. Topic signiﬁcance
Carlotta Domeniconi.
In Machine
ranking of LDA generative models.
Learning and Knowledge Discovery in Databases,
pages 67–82. Springer.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Wray Buntine and Marcus Hutter. 2010. A Bayesian
arXiv

the Poisson-Dirichlet process.

view of
preprint arXiv:1007.0296.

Jimena Canales.

2015.

The Physicist and the
Philosopher: Einstein, Bergson, and the Debate that
Changed our Understanding of Time. Princeton
University Press, Princeton, NJ.

Changyou Chen, Lan Du, and Wray Buntine. 2011.
Sampling table conﬁgurations for the hierarchical
In Machine Learning
Poisson-Dirichlet process.
and Knowledge Discovery in Databases, pages 296–
311. Springer.

Changyou Chen, Wray Buntine, Nan Ding, Lexing Xie,
and Lan Du. 2014. Differential topic models. IEEE
Transactions on Pattern Analysis and Machine In-
telligence.

M. Denny, J. ben Aaron, H. Wallach, and B. Desmarais.
2014. Modeling email network content and struc-
ture. In the 72nd Annual Midwest Political Science
Association Conference, 2014; the Northeast Politi-
cal Methodology Meeting, 2014; the 7th Annual Po-
litical Networks Conference, the Society for Political
Methodology 31st Annual Summer Meeting.

Peter Galison. 2003. Poincar’s Maps. Norton, New

York, NY.

Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 50–57.

Richard A Kronmal and Arthur V Peterson Jr. 1979.
On the alias method for generating random variables
from a discrete distribution. The American Statisti-
cian, 33(4):214–218.

Alp Kucukelbir and David M Blei. 2014. Proﬁle pre-
dictive inference. arXiv preprint arXiv:1411.0292.

Aaron Q Li, Amr Ahmed, Sujith Ravi, and Alexan-
der J Smola. 2014. Reducing the sampling com-
plexity of topic models. In Proceedings of the 20th
ACM SIGKDD international conference on knowl-
edge discovery and data mining, pages 891–900.

Mian Lu, Ge Bai, Qiong Luo, Jie Tang, and Jiuxin
Zhao. 2013. Accelerating topic model training on
a single machine. In Web Technologies and Appli-
cations, pages 184–195. Springer.

George Marsaglia, Wai Wan Tsang, and Jingbo Wang.
2004. Fast generation of discrete random variables.
Journal of Statistical Software, 11:1–8.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262–
272.

David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. The Journal of Machine Learning Re-
search, 10:1801–1828.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin.
2010a. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100–108.

David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010b. Evaluating
In Proceedings
topic models for digital libraries.
of the 10th Annual Joint Conference on Digital Li-
braries, JCDL ’10, pages 215–224. ACM.

Michael Paul and Roxana Girju. 2009. Cross-cultural
analysis of blogs and forums with mixed-collection
In Proceedings of the 2009 Confer-
topic models.
ence on Empirical Methods in Natural Language
Processing: Volume 3, pages 1408–1417.

Alexander Smola and Shravan Narayanamurthy. 2010.
An architecture for parallel topic models. Proceed-
ings of the VLDB Endowment, 3(1-2):703–710.

Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the american statistical associa-
tion, 101(476).

Hanna M Wallach, David Mimno, and Andrew Mccal-
lum. 2009a. Rethinking LDA: Why priors matter.
In Advances in Neural Information Processing Sys-
tems, pages 1973–1981.

Hanna M. Wallach, Iain Murray, Ruslan Salakhutdi-
nov, and David Mimno. 2009b. Evaluation methods
In Proceedings of the 26th An-
for topic models.
nual International Conference on Machine Learn-
ing, ICML ’09, pages 1105–1112, New York, NY,
USA. ACM.

Hanna M Wallach. 2008. Structured topic models for
language. Unpublished doctoral dissertation, Univ.
of Cambridge.

Tze-I Yang, Andrew J Torget, and Rada Mihalcea.
2011. Topic modeling on historical newspapers. In
Proceedings of the 5th ACL-HLT Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, pages 96–104.

Limin Yao, David Mimno, and Andrew McCallum.
2009. Efﬁcient methods for topic model inference
In Proceed-
on streaming document collections.
ings of the 15th ACM SIGKDD international con-
ference on knowledge discovery and data mining,
pages 937–946.

Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang
Wei, Xun Zheng, Eric Po Xing, Tie-Yan Liu, and
Wei-Ying Ma. 2015. Lightlda: Big topic models
on modest computer clusters. In Proceedings of the
24th International Conference on World Wide Web,
pages 1351–1361. International World Wide Web
Conferences Steering Committee.

ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative
In Proceedings of the tenth ACM
text mining.
SIGKDD international conference on knowledge
discovery and data mining, pages 743–748.

Fast, Flexible Models for Discovering Topic Correlation across
Weakly-Related Collections

Jingwei Zhang1, Aaron Gerow2, Jaan Altosaar3, James Evans2,4, Richard Jean So5

1Department of Computer Science, Columbia University
jz2541@columbia.edu
2 Computation Institute, University of Chicago

{

@uchicago.edu

gerow,jevans
}
3 Department of Physics, Princeton University
altosaar@princeton.edu
4 Department of Sociology, University of Chicago
5 Department of English Language and Literature, University of Chicago
richardjeanso@uchicago.edu

Abstract

This paper

Weak topic correlation across document
collections with different numbers of
topics in individual collections presents
challenges for existing cross-collection
topic models.
introduces
two probabilistic topic models, Correlated
LDA (C-LDA) and Correlated HDP (C-
HDP). These address problems that can
arise when analyzing large, asymmetric,
and potentially weakly-related collections.
Topic correlations in weakly-related col-
lections typically lie in the tail of the topic
distribution, where they would be over-
looked by models unable to ﬁt large num-
bers of topics. To efﬁciently model this
long tail for large-scale analysis, our mod-
els implement a parallel sampling algo-
rithm based on the Metropolis-Hastings
and alias methods (Yuan et al., 2015).
The models are ﬁrst evaluated on syn-
thetic data, generated to simulate vari-
ous collection-level asymmetries. We then
present a case study of modeling over
300k documents in collections of sciences
and humanities research from JSTOR.

1

Introduction

Comparing large text collections is a critical task
for the curation and analysis of human cultural
history. Achievements of research and schol-
arship are most accessible through textual arti-
facts, which are increasingly available in digital
archives. Text-based research, often undertaken
by humanists, historians, lexicographers, and cor-

pus linguists, explores patterns of words in docu-
ments across time-periods and distinct collections
of text. Here, we introduce two new topic models
designed to compare large collections, Correlated
LDA (C-LDA) and Correlated HDP (C-HDP),
which are sensitive to document-topic asymme-
try (where collections have different topic distribu-
tions) and topic-word asymmetry (where a single
topic has different word distributions in each col-
lection). These models seek to address termino-
logical questions, such as how a topic on physics
is articulated distinctively in scientiﬁc compared
to humanistic research. Accommodating poten-
tial collection-level asymmetries is particularly
important when researchers seek to analyze col-
lections with little prior knowledge about shared
or collection-speciﬁc topic structure. Our mod-
els extend existing cross-collection approaches to
accommodate these asymmetries and implement
an efﬁcient parallel sampling algorithm enabling
users to examine the long tail of topics in particu-
larly large collections.

Using topic models for comparative text min-
ing was introduced by Zhai et al. (2004), who de-
veloped the ccMix model which extended pLSA
(Hofmann, 1999). Later work by Paul and Girju
(2009) developed ccLDA, which adopted the hier-
archical Bayes framework of Latent Dirichlet Al-
location or LDA (Blei et al., 2003). These mod-
els account for topic-word asymmetry by assum-
ing variation in the vocabularies of topics is due
to collection-level differences. Nevertheless, they
require the same topics to be present in each col-
lection. These models are useful for comparing
collections under speciﬁc assumptions, but cannot
accommodate collection-topic asymmetry (which

5
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
6
5
4
0
.
8
0
5
1
:
v
i
X
r
a

arises in collections that do not share every topic
or that have different numbers of topics). In situa-
tions where collections do not share all topics, the
results often include junk, mixed, or sparse top-
ics, making them difﬁcult to interpret (Paul and
Girju, 2009). Such asymmetries make it difﬁcult
to use models like ccLDA and ccMix when little
is known about collections in advance. This mo-
tivates our efforts to model variation in the long
tail of topic distributions, where correlations are
more likely to appear when collections are weakly
related.

C-LDA and C-HDP extend ccLDA (Paul and
Girju, 2009) to accommodate collection-topic
level asymmetries, particularly by allowing non-
common topics to appear in each collection. This
added ﬂexibility allows our models to discover
topic correlations across arbitrary collections with
different numbers of topics, even when there are
few (or unknown) numbers of common topics. To
demonstrate the effectiveness of our models, we
evaluate them on synthetic data and show that they
outperform related models such as ccLDA and dif-
ferential topic models (Chen et al., 2014). We then
ﬁt C-LDA to two large collections of humanities
and sciences documents from JSTOR. Such histor-
ical analyses of text would be intractable without
an efﬁcient sampler. An optimized sampler is re-
quired in such situations because common topics
in weakly-correlated collections are usually found
in the tail of the document-topic distribution of a
sufﬁciently large set of topics. To make this fea-
sible on large datasets such as JSTOR, we employ
a parallelized Metropolis-Hastings (Kronmal and
Peterson Jr, 1979) and alias-table sampling frame-
work, adapted from LightLDA (Yuan et al., 2015).
These optimizations, which achieve
(1) amor-
tized sampling time per token, allow our models
to be ﬁt to large corpora with up to thousands of
topics in a matter of hours — an order of magni-
tude speed-up from ccLDA.

O

After reviewing work related to topic modeling
across collections, section 3 describes C-LDA and
C-HDP, and then details their technical relation-
ship to existing models. Section 5 introduces the
synthetic data and part of the JSTOR corpus used
in our evaluations. We then compare our models’
performances to other models in terms of held-
out perplexity and a measure of distinguishabil-
ity. The ﬁnal results section exempliﬁes the use
of C-LDA in a qualitative analysis of humanities

and sciences research. We conclude with a brief
discussion of the strengths of C-LDA and C-HDP,
and outline directions for future work and applica-
tions.

2 Related Work

Our models seek to enable users to compare large
collections that may only be weakly correlated
and that may contain different numbers of topics.
While topic models could be ﬁt to separate collec-
tions to make post-hoc comparisons (Denny et al.,
2014; Yang et al., 2011), our goal is to account for
both document-topic asymmetry and topic-word
asymmetry “in-model”. In short, we seek to model
the correlation between arbitrary collections. Pri-
oritizing in-model solutions for document-topic
asymmetry has been explored elsewhere, such as
in hierarchical Dirichlet processes (HDP), which
use an additional level to account for collection
variations in document-topic distributions (Teh et
al., 2006).

One method designed to model

topic-word
asymmetry is ccMix (Zhai et al., 2004), which
models the generative probability of a word in
topic z from collection c as a mixture of shared
and collection-speciﬁc distributions θz:

p(w) = λc p(w

θz) + (1
|

−

λc) p(w

θz,c)
|

where θz,c is collection-speciﬁc and λc controls
the mixing between shared and collection-speciﬁc
topics. ccLDA extends ccMix to the LDA frame-
work and adds a beta prior over λc that reduces
sensitivity to input parameters (Paul and Girju,
2009). Another approach, differential topic mod-
els (Chen et al., 2014), is based on hierarchical
Bayesian models over topic-word distributions.
This method uses the transformed Pitman-Yor pro-
cess (TPYP) to model topic-word distributions in
each collection, with shared common base mea-
sures. As (Paul and Girju, 2009) note, ccLDA
cannot accommodate a topic if it is not com-
mon across collections — an assumption made by
ccMix, ccLDA and the TPYP. In a situation where
a topic is found in only one collection, it would
either dominate the shared topic portion (resulting
in a noisy, collection-speciﬁc portion), or it would
appear as a mixed topic, revealing two sets of un-
related words (Newman et al., 2010b). C-LDA
ameliorates this situation by allowing the number
of common and non-common topics to be speci-
ﬁed separately and by efﬁciently sampling the tail

of the document-topic distribution, allowing users
to examine less prominent regions of the topic
space. C-HDP also grants collections document-
topic independence using a hierarchical structure
to model the differences between collections.

O

Due to increased demand for scalable topic
model implementations, there has been a prolif-
eration of optimized methods for efﬁcient infer-
ence, such as SparseLDA (Yao et al., 2009) and
AliasLDA (Li et al., 2014). AliasLDA achieves
(Kd) complexity by using the Metropolis-
O
Hastings-Walker algorithm and an alias table to
(1) time. Al-
sample topic-word distributions in
though this strategy introduces temporal staleness
in the updates of sufﬁcient statistics, the lag is
overcome by more iterations, and converges sig-
niﬁcantly faster. A similar technique by Yuan et al.
(2015), LightLDA, employs cycle-based Metropo-
lis Hastings mixing with alias tables for both
document-topic and topic-word distributions. De-
spite introducing lag in the sufﬁcient statistics,
this method achieves
(1) amortized sampling
O
complexity and results in even faster convergence
than AliasLDA. In addition to being fully paral-
lelized, C-LDA adopts this sampling framework to
make comparing large collections more tractable
for large numbers of topics. Our models’ efﬁcient
sampling methods allow users to ﬁt large num-
bers of topics to big datasets where variation might
not be observed in sub-sampled datasets or models
with fewer topics.

3 The Models

3.1 Correlated LDA

In ccLDA (and ccMix), each topic has shared
and collection-speciﬁc components for each col-
lection. C-LDA extends ccLDA to make it more
robust with respect to topic asymmetries between
collections (Figure 1a). The crucial extension is
that by allowing each collection to deﬁne a set of
non-common topics in addition to common top-
ics, the model removes an assumption imposed by
ccLDA and other inter-collection models, namely
that collections have the same number of topics.
As a result, C-LDA is suitable for collections with-
out a large proportion of common topics, and can
also reduce noise (discussed in Section 2). To
achieve this, C-LDA assumes document d in col-
lection c has a multinomial document-topic dis-
tribution θ with an asymmetric Dirichlet prior
for Kc topics, where the ﬁrst K∅ are common

across collections. It is also possible to introduce
a tree structure into the model that uses a bino-
mial distribution to decide whether a word was
drawn from common or non-common topics. This
yields collection-speciﬁc background topics by us-
ing a binomial distribution instead of a multino-
mial. However, we prefer the simpler, non-tree
version because background topics are unneces-
sary when using an asymmetric α prior (Wallach
et al., 2009a).

The generative process for C-LDA is as follows:

1. Sample a distribution φk (shared component) from Dir(β)
and a distribution σk from Beta(δ1, δ2) for each common
topic k ∈ {1, . . . , K ∅};

2. For each collection c, sample a distribution φc

k (collection-
speciﬁc component) from Dir(β) for each common topic
k ∈ {1, . . . , K ∅} and non-common topic k ∈ {K ∅ +
1, . . . , Kc};

3. For each document d in c, sample a distribution θ from

Dir(αc);

4. For each word wi in d:

(a) Sample a topic zi ∈ {1, . . . , Kc} from Multi(θ);
(b) If zi ≤ K ∅, sample yi from Binomial(σzi );
(c) Sample wi from Multi(φξ
null
c

, zi ≤ K ∅ and yi = 0;
, otherwise.

zi ), where

ξ =

(cid:26)

∃

Note that to capture common topics, K∅ should
c where Kc = K∅. Other-
be set such that
wise, words sampled as a non-common topic will
not have information about non-common topics in
other collections. Then a “common-topic word”
is found among non-common topics in all col-
lections (a local minima) and it will take a long
time to stabilize as a common topic. To avoid
this, when determining the number of topics for
sampling, the number of non-common topics for
the collection with the smallest number of total
topics should be zero. After inference, to distin-
guish common and non-common topics in this col-
lection, we model σ independently by assuming
collections have the same mixing ratio for com-
mon topics. With this reasonable assumption and
an asymmetric α, common topics become sparse
enough that some σ distributions reduce nearly to
0, distinguishing them as non-common topics. Al-
though this may seem counterintuitive, it does not
negatively affect results.

Three kinds of collection-level imbalance can
confound inter-collection topic models: 1) in the
numbers of topics between collections, 2) in the
numbers of documents between collections, and
3) in the document-topic distributions. Each of

α

θ

z

w

C

D

W

y

δ

σ

K∅

β

φ
K∅ + (cid:80) Kc

γ

H

G0

Gc

Gd

z

w

C

D

W

y

δ

σ

K(∞)

α0

α1

β

φ

K(∞)

Figure 1: Graphical models of C-LDA (a; left) and C-HDP (b; right).

these can cause topics in different collections to
have signiﬁcantly different numbers of words as-
signed to the same topic. In this way, a topic can
be dominated by the collection comprising most
of its words. C-LDA addresses imbalances in the
document-topic distributions between collections
by estimating α. For imbalance in the number of
topics and documents, C-LDA mimics document
over-sampling in the Gibbs sampler using a differ-
ent unit-value in the word count table for each col-
lection. Speciﬁcally, a unit ηc is chosen for each
collection such that the average equivalent num-
ber of assigned words per-topic ((cid:80)
d∈c ηcNd/Kc,
where Nd is the length of document d) is equal.
This process both increases the topic quality (in
terms of collection balance) in the resulting held-
out perplexity of the model.

3.2 Correlated HDP

∃

c such
To alleviate C-LDA’s requirement that
that Kc = K∅, we introduce a variant of the
model, the correlated hierarchical Dirichlet pro-
cess (C-HDP), that uses a 3-level hierarchical
Dirichlet process (Teh et al., 2006). The gener-
ative process for C-HDP is the same as C-LDA
shown above, except that here we assume a word’s
topic, z, is generated by a hierarchical Dirichlet
process:

|

G0

DP(γ, H)
DP(α0, G0)
DP(α1, Gc)
Gd

γ, H
Gc
α0, G0
|
α1, Gc
Gd
|
Gd
z
|

∼
∼
∼
∼
where G0 is a base measure for each collection-
level Dirichlet process, and Gc are base measures
of document-level Dirichlet processes in each col-
lection (Figure 1b). Thus, documents from the

same collection will have similar topic distribu-
tions compared to those from other collections,
and collections are allowed to have distinct sets of
topics due to the use of HDP.

4

Inference

4.1 Posterior Inference in C-LDA

C-LDA can be trained using collapsed Gibbs sam-
pling with φ, θ, and σ integrated out. Given the
status assignments of other words, the sampling
distribution for word wi is given by:

w, y−i, z−i, δ, α, β)
|

p(yi, zi
(N (d, zi) + αc,zi)
(cid:124)
(cid:123)(cid:122)
(cid:125)
qd

∝





(cid:124)

×

N (yi, zi) + δyi
N (zi) + (cid:80)
N (wi, zi, c) + β
N (zi, c) + V β

k δk ×

(cid:123)(cid:122)
qw

N (wi, yi, zi, ζ) + β
N (yi, zi, ζ) + V β

K∅

zi

≤

zi > K∅

(cid:125)

(1)

where ζ =

(cid:26) ∗
c

yi = 0
yi = 1 , N (

· · ·

) is the number of

status assignments for (

), not including wi.

· · ·
Inference in C-LDA employs two optimiza-
tions: a parallelized sampler and an efﬁcient sam-
pling algorithm (Algorithm 1). We use the paral-
lel schema in (Smola and Narayanamurthy, 2010;
Lu et al., 2013) which applies atomic updates to
the sufﬁcient statistics to avoid race conditions.
The key idea behind the optimized sampler is the
combination of alias tables and the Metropolis-
Hastings method (MH), adapted from (Yuan et al.,
2015; Li et al., 2014). Metropolis-Hastings is a
Markov chain Monte Carlo method that uses a pro-
posal distribution to approximate the true distribu-

Algorithm 1 Sampling in C-LDA

repeat

for all documents {d} in parallel do

for words {w} in d do

z ← CYCLEMH(p, qw, qd, z)
sample y given z

Atomic update sufﬁcient statics

Estimate α
until convergence

procedure CYCLEMH(p, qw, qd, z)

for i = 1 to N do

if i is even then

else

proposal q ← qw

proposal q ← qd

sample z(cid:48) ∼ ALIASTABLE(q)
if RandUnif(1) < min(1, p(z(cid:48))q(z)

p(z)q(z(cid:48)) ) then

z ← z(cid:48)

return z

tion when exact sampling is difﬁcult. In a compli-
mentary way, Walker’s alias method (2004) allows
one to effectively sample from a discrete distribu-
tion by using an alias table, constructed in
(K)
O
time, from which we can sample in
(1). Thus,
reusing the sampler K times as the proposal distri-
bution for Metropolis-Hastings yields
(1) amor-
tized sampling time per-token.

O

O

Notice that in Eq. 1, the sampling distribution is
the product of a single document-dependent term
qd and a single word-dependent term qw. After
burn-in, both terms will be sparse (without the
smoothing factor). It is therefore reasonable to use
qd and qw as cycle proposals (Yuan et al., 2015),
alternating them in each Metropolis-Hastings step.
Our experiments show that the primary drawback
of this method — stale sufﬁcient statistics — does
not empirically affect convergence. Our imple-
mentation uses proposal distributions qw and qd,
with y marginalized out. After the Metropolis-
Hastings steps, y is sampled to update z, to reduce
the size of the alias tables, yielding even faster
convergence.

Lastly, the use of an asymmetric α allows C-
LDA to discover correlations between less dom-
inant topics across collections (Wallach et al.,
2009a). We use Minka’s ﬁxed-point method, with
a gamma hyper-prior to optimize αc for each col-
lection separately (Wallach, 2008). All other hy-
perparameters were ﬁxed during inference.

4.2 Posterior Inference in C-HDP

C-HDP uses the block sampling algorithm de-
scribed in (Chen et al., 2011), which is based on

the Chinese restaurant process metaphor. Here,
rather than tracking all assignments (as the sam-
plers given in (Teh et al., 2006)), table indicators
are used to track only the start of new tables, which
allows us to adopt the same sampling framework
as C-LDA. In the Chinese restaurant process, each
Dirichlet process in the hierarchical structure is
represented as a restaurant with an inﬁnite num-
ber of tables, each serving the same dish. New
customers can either join a table with existing cus-
tomers, or start a new table. If a new table is cho-
sen, a proxy customer will be sent to the parent
restaurant to determine the dish served to that ta-
ble.

∅

In the block sampler, indicators are used to de-
note a customer creating a table (or tables) up to
level u (0 as the root, 1 for collection level, and 2
for the document level), and u =
indicates no
table has been created. For example, when a cus-
tomer creates a table at the collection level, and
the proxy customer in the collection level creates
a table at the root level, u is 0. With this metaphor,
let nlz be the number of customers (including their
proxies) served dish z at restaurant l, and let tlz be
the number of tables serving dish z at restaurant
l (l = 0 for root, l = c for collection level or
l = d for document level), with N0 = (cid:80)
z n0z and
Nc = (cid:80)
z ncz. By the chain rule, the conditional
probability of the state assignments for wi, given
all others, is

∝

p(yi, zi, ui|w, y−i, z−i, u−i, . . .)
N (y, z) + δy
N (z) + (cid:80)
k δk

γα0
γ+N0

N (w, y, z, ζ) + β
N (y, z, ζ) + V β

×

×




α0
γ+N0
Sncz +1
tcz
Sncz
tcz

α0+N1
α1

S

Sncz +1
tcz +1
Sncz
tcz
ndz +1
S
tdz +1
ndz
S
tdz
ndz +1
tdz
S

ndz
tdz

S

ndz +1
tdz +1
ndz
S
tdz

n2

0z (tcz +1)(tdz +1)
(n0z +1)(ncz +1)(ndz +1)

(tdz +1)(ncz −tcz +1)
(ncz +1)(ndz +1)

ndz −tdz +1
ndz +1

u = 0

u = 1

u = 2

u = ∅

Here, Sn
t is the Stirling number, the ratios of which
can be efﬁciently precomputed (Buntine and Hut-
ter, 2010). The concentration parameters γ, α0,
and α1 can be sampled using the auxiliary variable
method (Teh et al., 2006).

Note that because conditional probability has
the same separability as C-LDA (to give term qw
and qd), the same sampling framework can be
used with two alterations: 1) when a new topic
is created or removed at the root, collection, or
document level, the related alias tables must be
reset, which makes the sampling slightly slower

Figure 2: Held-out perplexity of C-LDA, C-HDP, ccLDA and TPYP ﬁt to synthetic data, where K1 =
K2 = K (a; left) and data with an asymmetric number of topics (b; right).

O

than
(1), and 2) while the document alias table
samples z and u simultaneously, after sampling z
from the word alias table u must be sampled using
tlc/nlz (Chen et al., 2011). Parallelizing C-HDP
requires an additional empirical method of merg-
ing new topics between threads (Newman et al.,
2009), which is outside of the scope of this work.
Our implementation of both models, C-LDA and
C-HDP, are open-sourced online 1.

5 Experiments

5.1 Model Comparison

We use perplexity on held-out documents to eval-
uate the performance of C-LDA and C-HDP. In all
experiments, the gamma prior for α in C-LDA was
set to (1, 1), and (5, 0.1), (5, 0.1), (0.1, 0.1) for
γ, α0, α1 respectively in C-HDP. In the hold-out
procedure, 20% of documents were randomly se-
lected as test data. LDA, C-LDA and ccLDA were
run for 1,000 iterations and C-HDP and the TII-
variant of TPYP for 1,500 iterations (unless oth-
erwise noted), all of which converged to a state
where change in perplexity was less than 1% for
ten consecutive iterations.

Perplexity was calculated from the marginal
likelihood of a held-out document p(w
Φ, α), es-
|
timated using the “left-to-right” method (Wallach
et al., 2009b). Because it is difﬁcult to vali-
date real-world data that exhibits different kinds
of asymmetry, we use synthetic data generated
speciﬁcally for our evaluation tasks (AlSumait et
al., 2009; Wallach et al., 2009b; Kucukelbir and
Blei, 2014).

5.1.1 Topic Correlation
C-LDA is unique in the amount of freedom it al-
lows when setting the number of topics for col-

1https://github.com/iceboal/correlated-lda

lections. To assess the models’ performances with
various topic correlations in a fair setting, we gen-
erated two collections of synthetic data by fol-
lowing the generative process (varying the num-
ber of topics) and measured the models’ perplex-
ities against the ground truth parameters. In each
experiment, two collections were generated, each
with 1,000 documents containing 50 words each,
over a vocabulary of 3,000. β and δ were ﬁxed at
0.01 and 1.0 respectively, and α was asymmetri-
cally deﬁned as 1/(i + √Kc) for i
1].

[0, Kc

∈

−

Completely shared topics The assumptions im-
posed by ccLDA and TPYP effectively make them
a special case of our model where K∅ = K1 =
K2 = . . .. To compare results, data was gener-
ated such that all numbers of topics were equal to
[10, 90]. Additionally, all models were con-
K
ﬁgured to use this ground truth parameter when
training. Not surprisingly, ccLDA, C-LDA, and C-
HDP have almost the same perplexity with respect
to K because their structure is the same when all
topics are shared (Figure 2a).

∈

Asymmetric numbers of topics To explore the
effect of asymmetry in the number of topics, data
was generated such that one collection had K1
∈
[20, 60] topics while a second had a ﬁxed K2 = 40
topics. The number of shared topics was set to
K∅ = 20. The parameters for C-LDA and C-HDP
(initial values) were set to ground truths, and, to
retain a fair comparison, versions of ccLDA and
TPYP were ﬁt with both K = K1 and K = K2.

We ﬁnd that ccLDA performs nearly as well as
C-LDA and C-HDP when there is more symme-
try between collection, namely when K1
K2
(Figure 2b). TPYP, on the other hand, performs
well with more topics (2
max(K1, K2) where
the ground truth is K1 & K2). In contrast, C-LDA

×

≈

and C-HDP perform more consistently than other
models across varying degrees of asymmetry.

Partially-shared topics When collections have
the same number of topics, C-LDA, C-HDP and
ccLDA exhibit adequate ﬂexibility, resulting in
similar perplexities. When collections have in-
creasingly few common topics, however, common
and non-common topics from ccLDA are con-
siderably less distinguishable than those from C-
LDA. To evaluate the models’ abilities in such sit-
uations, data was generated for two collections
having K1 = K2 = 50 topics, but with the
shared number of topics K∅
[5, 45]. We also
set δ(0) = δ(1) = 5, and for comparison to ccLDA
we used K = 50.

∈

To measure this distinguishability, we examine
the inferred σ. Recall that σ indicates what per-
centage of a common topic is shared. When a topic
is actually non-common, the value of σ should be
small. We sort σk for k
[1, K] in reverse and
use

∈

Figure 3: Distinguishability (Eq. 2) of topics ﬁt
with C-LDA,C-HDP and ccLDA. Blues lines de-
note ¯σcommon and red denote ¯σnon-common.

proxy human judgements of topic quality, is de-
ﬁned for a topic k as:

C(k) =

2

−

n(n

1)

n
(cid:88)

(wi,wj )∈k
i<j

log

D(wi, wj) + 1
D(wi)D(wj)

¯σcommon = 1
K∅

(cid:80)K∅

k=1 σk

¯σnon-common =

1
K−K∅

(cid:80)K

k=K∅+1 σk

(2)

)
·

the

computes

document

co-
where D(
To accommodate coherence with
occurrence.
common topics in C-LDA that have shared and
collection-speciﬁc components we deﬁne mutual
coherence, MC(k), as

as measures of how well common and non-
common topics were learned2. ¯σcommon is the av-
erage of the K∅ largest σ values, and ¯σnon-common
is the average of the rest. When δ(0) = δ(1) in the
synthetic data, σ in the common portion should be
0.5, whereas it should be 0 in the non-common
part. Figure 3 shows that C-LDA better distin-
guishes between common and non-common top-
ics, especially when K∅ is small. This allows non-
common topics to be separated from the results by
examining the value of σ. C-HDP has similar per-
formance but larger σ values. In ccLDA, all topics
are shared between collections which means that
common and non-common topics are mixed. As
expected, ccLDA performs similarly when all top-
ics are common across collections.

5.2 Semantic Coherence

MC(k) =

1
n2

n
(cid:88)

log

D(wi, wj) + 1
D(wi)D(wj)

wi∈shared,
wj ∈collection-speciﬁc

so that for each collection, C(k) (2n words) is
equal to C(k, shared) + C(k, collection-speciﬁc)
+ MC(k). Table 1 shows the semantic coherence
of topics ﬁt with ccLDA and C-LDA. We used a
10% sample of JSTOR due to the limited speed of
ccLDA, using 50 (common) topics for ccLDA / C-
LDA, and 250 non-common humanities topics for
C-LDA. Although these settings are different for
the models, the science topics are still comparable
because they both have 50 topics. We found that
C-LDA provides improved coherence in nearly all
situations.

Semantic coherence is a corpus-based metric of
the quality of a topic, deﬁned as the average pair-
wise similarity of the top n words (Newman et al.,
2010a; Mimno et al., 2011). A PMI-based form
of coherence, which has been found to be the best

2TPYP is not comparable using this metric, but its hierar-

chical structure will cause topics to mix naturally.

5.2.1

Inference Efﬁciency

To compare the model efﬁciency, we timed runs
on a sample of 5,036 documents from JSTOR (in-
troduced in the next section) with a 20% held-
out and set K = K1 = K2 = 200 run on a
commodity computer with four cores and 16GB
of memory. Figure 4a shows the perplexity over

Figure 4: Using JSTOR: perplexity vs. runtime and iterations (a; left) and perplexity vs. K (b; right).

Coherence

shared component

collection-speciﬁc

all documents
-8.83
-9.04
-7.22
-8.11

science
-7.73
-8.22
-3.68
-5.68

humanities
-8.04
-8.27
-6.11
-7.12

science
-8.38
-8.38
-8.25
-8.24

humanities
-8.14
-8.15
-8.09
-7.88

C-LDA
ccLDA
C-LDA
ccLDA

Mutual Coherence
shared & collection-speciﬁc
humanities
science
-8.37
-8.54
-8.40
-8.69
-7.97
-7.75
-7.95
-8.22

Table 1: Average semantic coherence of the 50 common topics from JSTOR (top) and the average of the
10 best common topics judged by the mean value of different types of coherence (bottom).

time and iterations. The inference algorithm intro-
duces some staleness, which yields slower conver-
gence in the ﬁrst 200 iterations. This effect, how-
ever, is outweighed in both C-LDA and C-HDP by
the increased sampling speed. With 8 threads, C-
LDA not only converges faster, but yields lower
perplexity, likely due to threads introducing addi-
tional stochasticity.

5.3 Performance on JSTOR

To compare our models against slower models, we
sampled 2,465 documents from JSTOR, withhold-
ing 20% as testing set. We ﬁt a model with 100
common and 50 non-common initial topics us-
ing C-HDP, which produced 272 root topics after
2,000 iterations.The perplexity scores are roughly
the same when C-LDA uses the same average
number of topics per collection (Figure 4b), ex-
cept when numbers of topics are very asymmet-
ric. Our model begins to outperform ccLDA after
80 topics. C-HDP did not, however, out-perform
C-LDA despite the original HDP outperforming
LDA. This could be do to the fact that the hier-
archical structure of C-HDP is considerably differ-
ent than the typical 2-level HDP. Held-out perplex-
ity on real data provides a quantitative evaluation
of our models’ performance in a real-world set-
ting. However, the goal of our models is to enable
a deeper analysis of large, weakly-related corpora,
which we next discuss.

5.4 Qualitative Analysis

Our models are designed to enable researchers to
compare collections of text in a way that is scal-
able and sensitive to collection-level asymmetries.
To demonstrate that C-LDA can ﬁll this role, we
ﬁt a model to the entire JSTOR sciences and hu-
manities collections with 100 science topics and
1000 humanities topics (to reveal the less popu-
lar science-related topics in the humanities), and
β = 0.01, δ = 1.0. JSTOR includes books and
journal publications in over 9 million documents
across nearly 3 thousand journals. We used the
journal Science to represent a collection of scien-
tiﬁc research and 76 humanist journals to repre-
sent humanities research3. Words were lemma-
tized, and the most and least frequent words dis-
carded. The ﬁnal humanities collection contained
149,734 documents and the sciences collection
had 160,680 documents, with a combined vocabu-
lary of 21,513 unique words. Together, these col-
lections typify a real-world situation where there
is likely some, but not overwhelming correlation.
The results indicate that the sciences and hu-
manities share several topics. Both exhibit an in-
terest in a “non-human” theme (common topic #2;
Table 2). This topic is quite similar in both collec-
tions (pig and monkey for science documents; bird
and gorilla for humanities documents), while their
shared component forms a cohesive topic (animal,

3The list is available at http://j.mp/humanities-txt.

shared
animal
specie
dog
wild
wolf
monkey
horse
sheep
lion
cat

Topic 2

science
pig
ﬂy
monkey
guinea
primate
worm
dog
cat
mammal
cattle

humanities
beast
creature
nonhuman
natural
humanity
bird
living
gorilla
brute
ape

shared
economic
government
economy
trade
major
growth
capital
industry
institution
support

Topic 21
science
cost
industry
company
price
market
product
income
industrial
business
private

humanities
rural
local
community
village
region
urban
country
area
regional
population

shared
particle
physic
physicist
energy
experiment
event
measurement
atom
interaction
atomic

Topic 23
science
energy
electron
ray
ion
atom
particle
mass
neutron
proton
nucleus

humanities
universe
quantum
physic
technical
scientiﬁc
relativity
physical
mechanic
law
reality

Table 2: Three topics from the JSTOR collections with their top words in shared and speciﬁc components.
Complete results available at http://j.mp/jstor-html.

specie, and monkey). This kind of correlation is
also evident in topic #23, about physics. While the
science documents clearly represent research in
particle physics, it is interesting to ﬁnd the topic is
also represented by humanist research focused on
cultural representations of science. This reﬂects a
growing interest in science and technology studies
that has gained recent traction in the humanities.
Despite their differences, both collections engage
with a similar theme, seen in the shared compo-
nent with words like particle, energy and atom.

The results also indicate that while sciences and
humanities documents can share themes, they of-
ten diverge in how they are discussed. For exam-
ple, common topic #21 could be identiﬁed as eco-
nomic or capitalist, but in the collection-speciﬁc
components, the two disciplines differ in their ar-
ticulatation. Science uses terms like price and
market, indicating an acceptance of free-market
capitalism (especially as it affects the practice of
science), while the humanities, which has long
been critical of free-market capitalism, uses terms
like rural and community, highlighting cultural
facets of modern economics. These results pro-
vide evidence about how ideas move between the
sciences and humanities — a phenomenon that
constitutes a growing area of research for histori-
ans (Galison, 2003; Canales, 2015). C-LDA pro-
vides empirical, measurable, and reproducible ev-
idence of the shared research between these disci-
plines, as well as how concepts are articulated.

6 Discussion

Our models provide a robust way to explore
large and potentially weakly-related text collec-
tions without
the
data. Like ccLDA and TPYP, our models ac-
count for topic-word variation at the collection
level. The models accommodate asymmetry in

imposing assumptions about

the numbers of topics (set in C-LDA, ﬁt in C-
HDP) and provide an efﬁcient inference method
which allows them to ﬁt data with large values
for K, which can help ﬁnd correlations in less
prevalent topics. Our primary contribution is our
models’ ability to accommodate asymmetries be-
tween arbitrary collections. JSTOR, the world’s
largest digital collection of humanities research,
was an ideal application setting given the size,
asymmetry, and comprehensiveness of the human-
ities collection. As we show, humanities and
science research exhibit asymmetries with regard
to vocabulary and topic structure — asymmetries
that would be systematically overlooked using ex-
isting models. By characterizing common top-
ics as mixtures of shared and collection-speciﬁc
components, we can capture a kind of topic-level
homophily, where similar themes are articulated
in different ways due to word-, document-, and
collection-level variation. Future work on these
models could explore methods to ﬁt non-common
topics for both collections. In general, C-LDA and
C-HDP can be used whenever documents are sam-
pled from ostensibly different populations, where
the nature of the difference is unknown.

Acknowledgements

Thanks to David Blei for advice on applications
of the model. This work contains analysis of pri-
vate, or otherwise restricted data, made available
to James Evans and Eamon Duede by ITHAKA
(JSTOR), the opinions of whom are not repre-
sented in this paper. Jaan Altosaar acknowledges
support from the Natural Sciences and Engineer-
ing Research Council of Canada. This work was
supported by a grant from the Templeton Foun-
dation to the Metaknowledge Research Network
and by grant #1158803 from the National Science
Foundation.

References

Loulwah AlSumait, Daniel Barbar´a, James Gentle, and
2009. Topic signiﬁcance
Carlotta Domeniconi.
In Machine
ranking of LDA generative models.
Learning and Knowledge Discovery in Databases,
pages 67–82. Springer.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Wray Buntine and Marcus Hutter. 2010. A Bayesian
arXiv

the Poisson-Dirichlet process.

view of
preprint arXiv:1007.0296.

Jimena Canales.

2015.

The Physicist and the
Philosopher: Einstein, Bergson, and the Debate that
Changed our Understanding of Time. Princeton
University Press, Princeton, NJ.

Changyou Chen, Lan Du, and Wray Buntine. 2011.
Sampling table conﬁgurations for the hierarchical
In Machine Learning
Poisson-Dirichlet process.
and Knowledge Discovery in Databases, pages 296–
311. Springer.

Changyou Chen, Wray Buntine, Nan Ding, Lexing Xie,
and Lan Du. 2014. Differential topic models. IEEE
Transactions on Pattern Analysis and Machine In-
telligence.

M. Denny, J. ben Aaron, H. Wallach, and B. Desmarais.
2014. Modeling email network content and struc-
ture. In the 72nd Annual Midwest Political Science
Association Conference, 2014; the Northeast Politi-
cal Methodology Meeting, 2014; the 7th Annual Po-
litical Networks Conference, the Society for Political
Methodology 31st Annual Summer Meeting.

Peter Galison. 2003. Poincar’s Maps. Norton, New

York, NY.

Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 50–57.

Richard A Kronmal and Arthur V Peterson Jr. 1979.
On the alias method for generating random variables
from a discrete distribution. The American Statisti-
cian, 33(4):214–218.

Alp Kucukelbir and David M Blei. 2014. Proﬁle pre-
dictive inference. arXiv preprint arXiv:1411.0292.

Aaron Q Li, Amr Ahmed, Sujith Ravi, and Alexan-
der J Smola. 2014. Reducing the sampling com-
plexity of topic models. In Proceedings of the 20th
ACM SIGKDD international conference on knowl-
edge discovery and data mining, pages 891–900.

Mian Lu, Ge Bai, Qiong Luo, Jie Tang, and Jiuxin
Zhao. 2013. Accelerating topic model training on
a single machine. In Web Technologies and Appli-
cations, pages 184–195. Springer.

George Marsaglia, Wai Wan Tsang, and Jingbo Wang.
2004. Fast generation of discrete random variables.
Journal of Statistical Software, 11:1–8.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262–
272.

David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. The Journal of Machine Learning Re-
search, 10:1801–1828.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin.
2010a. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100–108.

David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010b. Evaluating
In Proceedings
topic models for digital libraries.
of the 10th Annual Joint Conference on Digital Li-
braries, JCDL ’10, pages 215–224. ACM.

Michael Paul and Roxana Girju. 2009. Cross-cultural
analysis of blogs and forums with mixed-collection
In Proceedings of the 2009 Confer-
topic models.
ence on Empirical Methods in Natural Language
Processing: Volume 3, pages 1408–1417.

Alexander Smola and Shravan Narayanamurthy. 2010.
An architecture for parallel topic models. Proceed-
ings of the VLDB Endowment, 3(1-2):703–710.

Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the american statistical associa-
tion, 101(476).

Hanna M Wallach, David Mimno, and Andrew Mccal-
lum. 2009a. Rethinking LDA: Why priors matter.
In Advances in Neural Information Processing Sys-
tems, pages 1973–1981.

Hanna M. Wallach, Iain Murray, Ruslan Salakhutdi-
nov, and David Mimno. 2009b. Evaluation methods
In Proceedings of the 26th An-
for topic models.
nual International Conference on Machine Learn-
ing, ICML ’09, pages 1105–1112, New York, NY,
USA. ACM.

Hanna M Wallach. 2008. Structured topic models for
language. Unpublished doctoral dissertation, Univ.
of Cambridge.

Tze-I Yang, Andrew J Torget, and Rada Mihalcea.
2011. Topic modeling on historical newspapers. In
Proceedings of the 5th ACL-HLT Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, pages 96–104.

Limin Yao, David Mimno, and Andrew McCallum.
2009. Efﬁcient methods for topic model inference
In Proceed-
on streaming document collections.
ings of the 15th ACM SIGKDD international con-
ference on knowledge discovery and data mining,
pages 937–946.

Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang
Wei, Xun Zheng, Eric Po Xing, Tie-Yan Liu, and
Wei-Ying Ma. 2015. Lightlda: Big topic models
on modest computer clusters. In Proceedings of the
24th International Conference on World Wide Web,
pages 1351–1361. International World Wide Web
Conferences Steering Committee.

ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative
In Proceedings of the tenth ACM
text mining.
SIGKDD international conference on knowledge
discovery and data mining, pages 743–748.

Fast, Flexible Models for Discovering Topic Correlation across
Weakly-Related Collections

Jingwei Zhang1, Aaron Gerow2, Jaan Altosaar3, James Evans2,4, Richard Jean So5

1Department of Computer Science, Columbia University
jz2541@columbia.edu
2 Computation Institute, University of Chicago

{

@uchicago.edu

gerow,jevans
}
3 Department of Physics, Princeton University
altosaar@princeton.edu
4 Department of Sociology, University of Chicago
5 Department of English Language and Literature, University of Chicago
richardjeanso@uchicago.edu

Abstract

This paper

Weak topic correlation across document
collections with different numbers of
topics in individual collections presents
challenges for existing cross-collection
topic models.
introduces
two probabilistic topic models, Correlated
LDA (C-LDA) and Correlated HDP (C-
HDP). These address problems that can
arise when analyzing large, asymmetric,
and potentially weakly-related collections.
Topic correlations in weakly-related col-
lections typically lie in the tail of the topic
distribution, where they would be over-
looked by models unable to ﬁt large num-
bers of topics. To efﬁciently model this
long tail for large-scale analysis, our mod-
els implement a parallel sampling algo-
rithm based on the Metropolis-Hastings
and alias methods (Yuan et al., 2015).
The models are ﬁrst evaluated on syn-
thetic data, generated to simulate vari-
ous collection-level asymmetries. We then
present a case study of modeling over
300k documents in collections of sciences
and humanities research from JSTOR.

1

Introduction

Comparing large text collections is a critical task
for the curation and analysis of human cultural
history. Achievements of research and schol-
arship are most accessible through textual arti-
facts, which are increasingly available in digital
archives. Text-based research, often undertaken
by humanists, historians, lexicographers, and cor-

pus linguists, explores patterns of words in docu-
ments across time-periods and distinct collections
of text. Here, we introduce two new topic models
designed to compare large collections, Correlated
LDA (C-LDA) and Correlated HDP (C-HDP),
which are sensitive to document-topic asymme-
try (where collections have different topic distribu-
tions) and topic-word asymmetry (where a single
topic has different word distributions in each col-
lection). These models seek to address termino-
logical questions, such as how a topic on physics
is articulated distinctively in scientiﬁc compared
to humanistic research. Accommodating poten-
tial collection-level asymmetries is particularly
important when researchers seek to analyze col-
lections with little prior knowledge about shared
or collection-speciﬁc topic structure. Our mod-
els extend existing cross-collection approaches to
accommodate these asymmetries and implement
an efﬁcient parallel sampling algorithm enabling
users to examine the long tail of topics in particu-
larly large collections.

Using topic models for comparative text min-
ing was introduced by Zhai et al. (2004), who de-
veloped the ccMix model which extended pLSA
(Hofmann, 1999). Later work by Paul and Girju
(2009) developed ccLDA, which adopted the hier-
archical Bayes framework of Latent Dirichlet Al-
location or LDA (Blei et al., 2003). These mod-
els account for topic-word asymmetry by assum-
ing variation in the vocabularies of topics is due
to collection-level differences. Nevertheless, they
require the same topics to be present in each col-
lection. These models are useful for comparing
collections under speciﬁc assumptions, but cannot
accommodate collection-topic asymmetry (which

5
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
6
5
4
0
.
8
0
5
1
:
v
i
X
r
a

arises in collections that do not share every topic
or that have different numbers of topics). In situa-
tions where collections do not share all topics, the
results often include junk, mixed, or sparse top-
ics, making them difﬁcult to interpret (Paul and
Girju, 2009). Such asymmetries make it difﬁcult
to use models like ccLDA and ccMix when little
is known about collections in advance. This mo-
tivates our efforts to model variation in the long
tail of topic distributions, where correlations are
more likely to appear when collections are weakly
related.

C-LDA and C-HDP extend ccLDA (Paul and
Girju, 2009) to accommodate collection-topic
level asymmetries, particularly by allowing non-
common topics to appear in each collection. This
added ﬂexibility allows our models to discover
topic correlations across arbitrary collections with
different numbers of topics, even when there are
few (or unknown) numbers of common topics. To
demonstrate the effectiveness of our models, we
evaluate them on synthetic data and show that they
outperform related models such as ccLDA and dif-
ferential topic models (Chen et al., 2014). We then
ﬁt C-LDA to two large collections of humanities
and sciences documents from JSTOR. Such histor-
ical analyses of text would be intractable without
an efﬁcient sampler. An optimized sampler is re-
quired in such situations because common topics
in weakly-correlated collections are usually found
in the tail of the document-topic distribution of a
sufﬁciently large set of topics. To make this fea-
sible on large datasets such as JSTOR, we employ
a parallelized Metropolis-Hastings (Kronmal and
Peterson Jr, 1979) and alias-table sampling frame-
work, adapted from LightLDA (Yuan et al., 2015).
These optimizations, which achieve
(1) amor-
tized sampling time per token, allow our models
to be ﬁt to large corpora with up to thousands of
topics in a matter of hours — an order of magni-
tude speed-up from ccLDA.

O

After reviewing work related to topic modeling
across collections, section 3 describes C-LDA and
C-HDP, and then details their technical relation-
ship to existing models. Section 5 introduces the
synthetic data and part of the JSTOR corpus used
in our evaluations. We then compare our models’
performances to other models in terms of held-
out perplexity and a measure of distinguishabil-
ity. The ﬁnal results section exempliﬁes the use
of C-LDA in a qualitative analysis of humanities

and sciences research. We conclude with a brief
discussion of the strengths of C-LDA and C-HDP,
and outline directions for future work and applica-
tions.

2 Related Work

Our models seek to enable users to compare large
collections that may only be weakly correlated
and that may contain different numbers of topics.
While topic models could be ﬁt to separate collec-
tions to make post-hoc comparisons (Denny et al.,
2014; Yang et al., 2011), our goal is to account for
both document-topic asymmetry and topic-word
asymmetry “in-model”. In short, we seek to model
the correlation between arbitrary collections. Pri-
oritizing in-model solutions for document-topic
asymmetry has been explored elsewhere, such as
in hierarchical Dirichlet processes (HDP), which
use an additional level to account for collection
variations in document-topic distributions (Teh et
al., 2006).

One method designed to model

topic-word
asymmetry is ccMix (Zhai et al., 2004), which
models the generative probability of a word in
topic z from collection c as a mixture of shared
and collection-speciﬁc distributions θz:

p(w) = λc p(w

θz) + (1
|

−

λc) p(w

θz,c)
|

where θz,c is collection-speciﬁc and λc controls
the mixing between shared and collection-speciﬁc
topics. ccLDA extends ccMix to the LDA frame-
work and adds a beta prior over λc that reduces
sensitivity to input parameters (Paul and Girju,
2009). Another approach, differential topic mod-
els (Chen et al., 2014), is based on hierarchical
Bayesian models over topic-word distributions.
This method uses the transformed Pitman-Yor pro-
cess (TPYP) to model topic-word distributions in
each collection, with shared common base mea-
sures. As (Paul and Girju, 2009) note, ccLDA
cannot accommodate a topic if it is not com-
mon across collections — an assumption made by
ccMix, ccLDA and the TPYP. In a situation where
a topic is found in only one collection, it would
either dominate the shared topic portion (resulting
in a noisy, collection-speciﬁc portion), or it would
appear as a mixed topic, revealing two sets of un-
related words (Newman et al., 2010b). C-LDA
ameliorates this situation by allowing the number
of common and non-common topics to be speci-
ﬁed separately and by efﬁciently sampling the tail

of the document-topic distribution, allowing users
to examine less prominent regions of the topic
space. C-HDP also grants collections document-
topic independence using a hierarchical structure
to model the differences between collections.

O

Due to increased demand for scalable topic
model implementations, there has been a prolif-
eration of optimized methods for efﬁcient infer-
ence, such as SparseLDA (Yao et al., 2009) and
AliasLDA (Li et al., 2014). AliasLDA achieves
(Kd) complexity by using the Metropolis-
O
Hastings-Walker algorithm and an alias table to
(1) time. Al-
sample topic-word distributions in
though this strategy introduces temporal staleness
in the updates of sufﬁcient statistics, the lag is
overcome by more iterations, and converges sig-
niﬁcantly faster. A similar technique by Yuan et al.
(2015), LightLDA, employs cycle-based Metropo-
lis Hastings mixing with alias tables for both
document-topic and topic-word distributions. De-
spite introducing lag in the sufﬁcient statistics,
this method achieves
(1) amortized sampling
O
complexity and results in even faster convergence
than AliasLDA. In addition to being fully paral-
lelized, C-LDA adopts this sampling framework to
make comparing large collections more tractable
for large numbers of topics. Our models’ efﬁcient
sampling methods allow users to ﬁt large num-
bers of topics to big datasets where variation might
not be observed in sub-sampled datasets or models
with fewer topics.

3 The Models

3.1 Correlated LDA

In ccLDA (and ccMix), each topic has shared
and collection-speciﬁc components for each col-
lection. C-LDA extends ccLDA to make it more
robust with respect to topic asymmetries between
collections (Figure 1a). The crucial extension is
that by allowing each collection to deﬁne a set of
non-common topics in addition to common top-
ics, the model removes an assumption imposed by
ccLDA and other inter-collection models, namely
that collections have the same number of topics.
As a result, C-LDA is suitable for collections with-
out a large proportion of common topics, and can
also reduce noise (discussed in Section 2). To
achieve this, C-LDA assumes document d in col-
lection c has a multinomial document-topic dis-
tribution θ with an asymmetric Dirichlet prior
for Kc topics, where the ﬁrst K∅ are common

across collections. It is also possible to introduce
a tree structure into the model that uses a bino-
mial distribution to decide whether a word was
drawn from common or non-common topics. This
yields collection-speciﬁc background topics by us-
ing a binomial distribution instead of a multino-
mial. However, we prefer the simpler, non-tree
version because background topics are unneces-
sary when using an asymmetric α prior (Wallach
et al., 2009a).

The generative process for C-LDA is as follows:

1. Sample a distribution φk (shared component) from Dir(β)
and a distribution σk from Beta(δ1, δ2) for each common
topic k ∈ {1, . . . , K ∅};

2. For each collection c, sample a distribution φc

k (collection-
speciﬁc component) from Dir(β) for each common topic
k ∈ {1, . . . , K ∅} and non-common topic k ∈ {K ∅ +
1, . . . , Kc};

3. For each document d in c, sample a distribution θ from

Dir(αc);

4. For each word wi in d:

(a) Sample a topic zi ∈ {1, . . . , Kc} from Multi(θ);
(b) If zi ≤ K ∅, sample yi from Binomial(σzi );
(c) Sample wi from Multi(φξ
null
c

, zi ≤ K ∅ and yi = 0;
, otherwise.

zi ), where

ξ =

(cid:26)

∃

Note that to capture common topics, K∅ should
c where Kc = K∅. Other-
be set such that
wise, words sampled as a non-common topic will
not have information about non-common topics in
other collections. Then a “common-topic word”
is found among non-common topics in all col-
lections (a local minima) and it will take a long
time to stabilize as a common topic. To avoid
this, when determining the number of topics for
sampling, the number of non-common topics for
the collection with the smallest number of total
topics should be zero. After inference, to distin-
guish common and non-common topics in this col-
lection, we model σ independently by assuming
collections have the same mixing ratio for com-
mon topics. With this reasonable assumption and
an asymmetric α, common topics become sparse
enough that some σ distributions reduce nearly to
0, distinguishing them as non-common topics. Al-
though this may seem counterintuitive, it does not
negatively affect results.

Three kinds of collection-level imbalance can
confound inter-collection topic models: 1) in the
numbers of topics between collections, 2) in the
numbers of documents between collections, and
3) in the document-topic distributions. Each of

α

θ

z

w

C

D

W

y

δ

σ

K∅

β

φ
K∅ + (cid:80) Kc

γ

H

G0

Gc

Gd

z

w

C

D

W

y

δ

σ

K(∞)

α0

α1

β

φ

K(∞)

Figure 1: Graphical models of C-LDA (a; left) and C-HDP (b; right).

these can cause topics in different collections to
have signiﬁcantly different numbers of words as-
signed to the same topic. In this way, a topic can
be dominated by the collection comprising most
of its words. C-LDA addresses imbalances in the
document-topic distributions between collections
by estimating α. For imbalance in the number of
topics and documents, C-LDA mimics document
over-sampling in the Gibbs sampler using a differ-
ent unit-value in the word count table for each col-
lection. Speciﬁcally, a unit ηc is chosen for each
collection such that the average equivalent num-
ber of assigned words per-topic ((cid:80)
d∈c ηcNd/Kc,
where Nd is the length of document d) is equal.
This process both increases the topic quality (in
terms of collection balance) in the resulting held-
out perplexity of the model.

3.2 Correlated HDP

∃

c such
To alleviate C-LDA’s requirement that
that Kc = K∅, we introduce a variant of the
model, the correlated hierarchical Dirichlet pro-
cess (C-HDP), that uses a 3-level hierarchical
Dirichlet process (Teh et al., 2006). The gener-
ative process for C-HDP is the same as C-LDA
shown above, except that here we assume a word’s
topic, z, is generated by a hierarchical Dirichlet
process:

|

G0

DP(γ, H)
DP(α0, G0)
DP(α1, Gc)
Gd

γ, H
Gc
α0, G0
|
α1, Gc
Gd
|
Gd
z
|

∼
∼
∼
∼
where G0 is a base measure for each collection-
level Dirichlet process, and Gc are base measures
of document-level Dirichlet processes in each col-
lection (Figure 1b). Thus, documents from the

same collection will have similar topic distribu-
tions compared to those from other collections,
and collections are allowed to have distinct sets of
topics due to the use of HDP.

4

Inference

4.1 Posterior Inference in C-LDA

C-LDA can be trained using collapsed Gibbs sam-
pling with φ, θ, and σ integrated out. Given the
status assignments of other words, the sampling
distribution for word wi is given by:

w, y−i, z−i, δ, α, β)
|

p(yi, zi
(N (d, zi) + αc,zi)
(cid:124)
(cid:123)(cid:122)
(cid:125)
qd

∝





(cid:124)

×

N (yi, zi) + δyi
N (zi) + (cid:80)
N (wi, zi, c) + β
N (zi, c) + V β

k δk ×

(cid:123)(cid:122)
qw

N (wi, yi, zi, ζ) + β
N (yi, zi, ζ) + V β

K∅

zi

≤

zi > K∅

(cid:125)

(1)

where ζ =

(cid:26) ∗
c

yi = 0
yi = 1 , N (

· · ·

) is the number of

status assignments for (

), not including wi.

· · ·
Inference in C-LDA employs two optimiza-
tions: a parallelized sampler and an efﬁcient sam-
pling algorithm (Algorithm 1). We use the paral-
lel schema in (Smola and Narayanamurthy, 2010;
Lu et al., 2013) which applies atomic updates to
the sufﬁcient statistics to avoid race conditions.
The key idea behind the optimized sampler is the
combination of alias tables and the Metropolis-
Hastings method (MH), adapted from (Yuan et al.,
2015; Li et al., 2014). Metropolis-Hastings is a
Markov chain Monte Carlo method that uses a pro-
posal distribution to approximate the true distribu-

Algorithm 1 Sampling in C-LDA

repeat

for all documents {d} in parallel do

for words {w} in d do

z ← CYCLEMH(p, qw, qd, z)
sample y given z

Atomic update sufﬁcient statics

Estimate α
until convergence

procedure CYCLEMH(p, qw, qd, z)

for i = 1 to N do

if i is even then

else

proposal q ← qw

proposal q ← qd

sample z(cid:48) ∼ ALIASTABLE(q)
if RandUnif(1) < min(1, p(z(cid:48))q(z)

p(z)q(z(cid:48)) ) then

z ← z(cid:48)

return z

tion when exact sampling is difﬁcult. In a compli-
mentary way, Walker’s alias method (2004) allows
one to effectively sample from a discrete distribu-
tion by using an alias table, constructed in
(K)
O
time, from which we can sample in
(1). Thus,
reusing the sampler K times as the proposal distri-
bution for Metropolis-Hastings yields
(1) amor-
tized sampling time per-token.

O

O

Notice that in Eq. 1, the sampling distribution is
the product of a single document-dependent term
qd and a single word-dependent term qw. After
burn-in, both terms will be sparse (without the
smoothing factor). It is therefore reasonable to use
qd and qw as cycle proposals (Yuan et al., 2015),
alternating them in each Metropolis-Hastings step.
Our experiments show that the primary drawback
of this method — stale sufﬁcient statistics — does
not empirically affect convergence. Our imple-
mentation uses proposal distributions qw and qd,
with y marginalized out. After the Metropolis-
Hastings steps, y is sampled to update z, to reduce
the size of the alias tables, yielding even faster
convergence.

Lastly, the use of an asymmetric α allows C-
LDA to discover correlations between less dom-
inant topics across collections (Wallach et al.,
2009a). We use Minka’s ﬁxed-point method, with
a gamma hyper-prior to optimize αc for each col-
lection separately (Wallach, 2008). All other hy-
perparameters were ﬁxed during inference.

4.2 Posterior Inference in C-HDP

C-HDP uses the block sampling algorithm de-
scribed in (Chen et al., 2011), which is based on

the Chinese restaurant process metaphor. Here,
rather than tracking all assignments (as the sam-
plers given in (Teh et al., 2006)), table indicators
are used to track only the start of new tables, which
allows us to adopt the same sampling framework
as C-LDA. In the Chinese restaurant process, each
Dirichlet process in the hierarchical structure is
represented as a restaurant with an inﬁnite num-
ber of tables, each serving the same dish. New
customers can either join a table with existing cus-
tomers, or start a new table. If a new table is cho-
sen, a proxy customer will be sent to the parent
restaurant to determine the dish served to that ta-
ble.

∅

In the block sampler, indicators are used to de-
note a customer creating a table (or tables) up to
level u (0 as the root, 1 for collection level, and 2
for the document level), and u =
indicates no
table has been created. For example, when a cus-
tomer creates a table at the collection level, and
the proxy customer in the collection level creates
a table at the root level, u is 0. With this metaphor,
let nlz be the number of customers (including their
proxies) served dish z at restaurant l, and let tlz be
the number of tables serving dish z at restaurant
l (l = 0 for root, l = c for collection level or
l = d for document level), with N0 = (cid:80)
z n0z and
Nc = (cid:80)
z ncz. By the chain rule, the conditional
probability of the state assignments for wi, given
all others, is

∝

p(yi, zi, ui|w, y−i, z−i, u−i, . . .)
N (y, z) + δy
N (z) + (cid:80)
k δk

γα0
γ+N0

N (w, y, z, ζ) + β
N (y, z, ζ) + V β

×

×




α0
γ+N0
Sncz +1
tcz
Sncz
tcz

α0+N1
α1

S

Sncz +1
tcz +1
Sncz
tcz
ndz +1
S
tdz +1
ndz
S
tdz
ndz +1
tdz
S

ndz
tdz

S

ndz +1
tdz +1
ndz
S
tdz

n2

0z (tcz +1)(tdz +1)
(n0z +1)(ncz +1)(ndz +1)

(tdz +1)(ncz −tcz +1)
(ncz +1)(ndz +1)

ndz −tdz +1
ndz +1

u = 0

u = 1

u = 2

u = ∅

Here, Sn
t is the Stirling number, the ratios of which
can be efﬁciently precomputed (Buntine and Hut-
ter, 2010). The concentration parameters γ, α0,
and α1 can be sampled using the auxiliary variable
method (Teh et al., 2006).

Note that because conditional probability has
the same separability as C-LDA (to give term qw
and qd), the same sampling framework can be
used with two alterations: 1) when a new topic
is created or removed at the root, collection, or
document level, the related alias tables must be
reset, which makes the sampling slightly slower

Figure 2: Held-out perplexity of C-LDA, C-HDP, ccLDA and TPYP ﬁt to synthetic data, where K1 =
K2 = K (a; left) and data with an asymmetric number of topics (b; right).

O

than
(1), and 2) while the document alias table
samples z and u simultaneously, after sampling z
from the word alias table u must be sampled using
tlc/nlz (Chen et al., 2011). Parallelizing C-HDP
requires an additional empirical method of merg-
ing new topics between threads (Newman et al.,
2009), which is outside of the scope of this work.
Our implementation of both models, C-LDA and
C-HDP, are open-sourced online 1.

5 Experiments

5.1 Model Comparison

We use perplexity on held-out documents to eval-
uate the performance of C-LDA and C-HDP. In all
experiments, the gamma prior for α in C-LDA was
set to (1, 1), and (5, 0.1), (5, 0.1), (0.1, 0.1) for
γ, α0, α1 respectively in C-HDP. In the hold-out
procedure, 20% of documents were randomly se-
lected as test data. LDA, C-LDA and ccLDA were
run for 1,000 iterations and C-HDP and the TII-
variant of TPYP for 1,500 iterations (unless oth-
erwise noted), all of which converged to a state
where change in perplexity was less than 1% for
ten consecutive iterations.

Perplexity was calculated from the marginal
likelihood of a held-out document p(w
Φ, α), es-
|
timated using the “left-to-right” method (Wallach
et al., 2009b). Because it is difﬁcult to vali-
date real-world data that exhibits different kinds
of asymmetry, we use synthetic data generated
speciﬁcally for our evaluation tasks (AlSumait et
al., 2009; Wallach et al., 2009b; Kucukelbir and
Blei, 2014).

5.1.1 Topic Correlation
C-LDA is unique in the amount of freedom it al-
lows when setting the number of topics for col-

1https://github.com/iceboal/correlated-lda

lections. To assess the models’ performances with
various topic correlations in a fair setting, we gen-
erated two collections of synthetic data by fol-
lowing the generative process (varying the num-
ber of topics) and measured the models’ perplex-
ities against the ground truth parameters. In each
experiment, two collections were generated, each
with 1,000 documents containing 50 words each,
over a vocabulary of 3,000. β and δ were ﬁxed at
0.01 and 1.0 respectively, and α was asymmetri-
cally deﬁned as 1/(i + √Kc) for i
1].

[0, Kc

∈

−

Completely shared topics The assumptions im-
posed by ccLDA and TPYP effectively make them
a special case of our model where K∅ = K1 =
K2 = . . .. To compare results, data was gener-
ated such that all numbers of topics were equal to
[10, 90]. Additionally, all models were con-
K
ﬁgured to use this ground truth parameter when
training. Not surprisingly, ccLDA, C-LDA, and C-
HDP have almost the same perplexity with respect
to K because their structure is the same when all
topics are shared (Figure 2a).

∈

Asymmetric numbers of topics To explore the
effect of asymmetry in the number of topics, data
was generated such that one collection had K1
∈
[20, 60] topics while a second had a ﬁxed K2 = 40
topics. The number of shared topics was set to
K∅ = 20. The parameters for C-LDA and C-HDP
(initial values) were set to ground truths, and, to
retain a fair comparison, versions of ccLDA and
TPYP were ﬁt with both K = K1 and K = K2.

We ﬁnd that ccLDA performs nearly as well as
C-LDA and C-HDP when there is more symme-
try between collection, namely when K1
K2
(Figure 2b). TPYP, on the other hand, performs
well with more topics (2
max(K1, K2) where
the ground truth is K1 & K2). In contrast, C-LDA

×

≈

and C-HDP perform more consistently than other
models across varying degrees of asymmetry.

Partially-shared topics When collections have
the same number of topics, C-LDA, C-HDP and
ccLDA exhibit adequate ﬂexibility, resulting in
similar perplexities. When collections have in-
creasingly few common topics, however, common
and non-common topics from ccLDA are con-
siderably less distinguishable than those from C-
LDA. To evaluate the models’ abilities in such sit-
uations, data was generated for two collections
having K1 = K2 = 50 topics, but with the
shared number of topics K∅
[5, 45]. We also
set δ(0) = δ(1) = 5, and for comparison to ccLDA
we used K = 50.

∈

To measure this distinguishability, we examine
the inferred σ. Recall that σ indicates what per-
centage of a common topic is shared. When a topic
is actually non-common, the value of σ should be
small. We sort σk for k
[1, K] in reverse and
use

∈

Figure 3: Distinguishability (Eq. 2) of topics ﬁt
with C-LDA,C-HDP and ccLDA. Blues lines de-
note ¯σcommon and red denote ¯σnon-common.

proxy human judgements of topic quality, is de-
ﬁned for a topic k as:

C(k) =

2

−

n(n

1)

n
(cid:88)

(wi,wj )∈k
i<j

log

D(wi, wj) + 1
D(wi)D(wj)

¯σcommon = 1
K∅

(cid:80)K∅

k=1 σk

¯σnon-common =

1
K−K∅

(cid:80)K

k=K∅+1 σk

(2)

)
·

the

computes

document

co-
where D(
To accommodate coherence with
occurrence.
common topics in C-LDA that have shared and
collection-speciﬁc components we deﬁne mutual
coherence, MC(k), as

as measures of how well common and non-
common topics were learned2. ¯σcommon is the av-
erage of the K∅ largest σ values, and ¯σnon-common
is the average of the rest. When δ(0) = δ(1) in the
synthetic data, σ in the common portion should be
0.5, whereas it should be 0 in the non-common
part. Figure 3 shows that C-LDA better distin-
guishes between common and non-common top-
ics, especially when K∅ is small. This allows non-
common topics to be separated from the results by
examining the value of σ. C-HDP has similar per-
formance but larger σ values. In ccLDA, all topics
are shared between collections which means that
common and non-common topics are mixed. As
expected, ccLDA performs similarly when all top-
ics are common across collections.

5.2 Semantic Coherence

MC(k) =

1
n2

n
(cid:88)

log

D(wi, wj) + 1
D(wi)D(wj)

wi∈shared,
wj ∈collection-speciﬁc

so that for each collection, C(k) (2n words) is
equal to C(k, shared) + C(k, collection-speciﬁc)
+ MC(k). Table 1 shows the semantic coherence
of topics ﬁt with ccLDA and C-LDA. We used a
10% sample of JSTOR due to the limited speed of
ccLDA, using 50 (common) topics for ccLDA / C-
LDA, and 250 non-common humanities topics for
C-LDA. Although these settings are different for
the models, the science topics are still comparable
because they both have 50 topics. We found that
C-LDA provides improved coherence in nearly all
situations.

Semantic coherence is a corpus-based metric of
the quality of a topic, deﬁned as the average pair-
wise similarity of the top n words (Newman et al.,
2010a; Mimno et al., 2011). A PMI-based form
of coherence, which has been found to be the best

2TPYP is not comparable using this metric, but its hierar-

chical structure will cause topics to mix naturally.

5.2.1

Inference Efﬁciency

To compare the model efﬁciency, we timed runs
on a sample of 5,036 documents from JSTOR (in-
troduced in the next section) with a 20% held-
out and set K = K1 = K2 = 200 run on a
commodity computer with four cores and 16GB
of memory. Figure 4a shows the perplexity over

Figure 4: Using JSTOR: perplexity vs. runtime and iterations (a; left) and perplexity vs. K (b; right).

Coherence

shared component

collection-speciﬁc

all documents
-8.83
-9.04
-7.22
-8.11

science
-7.73
-8.22
-3.68
-5.68

humanities
-8.04
-8.27
-6.11
-7.12

science
-8.38
-8.38
-8.25
-8.24

humanities
-8.14
-8.15
-8.09
-7.88

C-LDA
ccLDA
C-LDA
ccLDA

Mutual Coherence
shared & collection-speciﬁc
humanities
science
-8.37
-8.54
-8.40
-8.69
-7.97
-7.75
-7.95
-8.22

Table 1: Average semantic coherence of the 50 common topics from JSTOR (top) and the average of the
10 best common topics judged by the mean value of different types of coherence (bottom).

time and iterations. The inference algorithm intro-
duces some staleness, which yields slower conver-
gence in the ﬁrst 200 iterations. This effect, how-
ever, is outweighed in both C-LDA and C-HDP by
the increased sampling speed. With 8 threads, C-
LDA not only converges faster, but yields lower
perplexity, likely due to threads introducing addi-
tional stochasticity.

5.3 Performance on JSTOR

To compare our models against slower models, we
sampled 2,465 documents from JSTOR, withhold-
ing 20% as testing set. We ﬁt a model with 100
common and 50 non-common initial topics us-
ing C-HDP, which produced 272 root topics after
2,000 iterations.The perplexity scores are roughly
the same when C-LDA uses the same average
number of topics per collection (Figure 4b), ex-
cept when numbers of topics are very asymmet-
ric. Our model begins to outperform ccLDA after
80 topics. C-HDP did not, however, out-perform
C-LDA despite the original HDP outperforming
LDA. This could be do to the fact that the hier-
archical structure of C-HDP is considerably differ-
ent than the typical 2-level HDP. Held-out perplex-
ity on real data provides a quantitative evaluation
of our models’ performance in a real-world set-
ting. However, the goal of our models is to enable
a deeper analysis of large, weakly-related corpora,
which we next discuss.

5.4 Qualitative Analysis

Our models are designed to enable researchers to
compare collections of text in a way that is scal-
able and sensitive to collection-level asymmetries.
To demonstrate that C-LDA can ﬁll this role, we
ﬁt a model to the entire JSTOR sciences and hu-
manities collections with 100 science topics and
1000 humanities topics (to reveal the less popu-
lar science-related topics in the humanities), and
β = 0.01, δ = 1.0. JSTOR includes books and
journal publications in over 9 million documents
across nearly 3 thousand journals. We used the
journal Science to represent a collection of scien-
tiﬁc research and 76 humanist journals to repre-
sent humanities research3. Words were lemma-
tized, and the most and least frequent words dis-
carded. The ﬁnal humanities collection contained
149,734 documents and the sciences collection
had 160,680 documents, with a combined vocabu-
lary of 21,513 unique words. Together, these col-
lections typify a real-world situation where there
is likely some, but not overwhelming correlation.
The results indicate that the sciences and hu-
manities share several topics. Both exhibit an in-
terest in a “non-human” theme (common topic #2;
Table 2). This topic is quite similar in both collec-
tions (pig and monkey for science documents; bird
and gorilla for humanities documents), while their
shared component forms a cohesive topic (animal,

3The list is available at http://j.mp/humanities-txt.

shared
animal
specie
dog
wild
wolf
monkey
horse
sheep
lion
cat

Topic 2

science
pig
ﬂy
monkey
guinea
primate
worm
dog
cat
mammal
cattle

humanities
beast
creature
nonhuman
natural
humanity
bird
living
gorilla
brute
ape

shared
economic
government
economy
trade
major
growth
capital
industry
institution
support

Topic 21
science
cost
industry
company
price
market
product
income
industrial
business
private

humanities
rural
local
community
village
region
urban
country
area
regional
population

shared
particle
physic
physicist
energy
experiment
event
measurement
atom
interaction
atomic

Topic 23
science
energy
electron
ray
ion
atom
particle
mass
neutron
proton
nucleus

humanities
universe
quantum
physic
technical
scientiﬁc
relativity
physical
mechanic
law
reality

Table 2: Three topics from the JSTOR collections with their top words in shared and speciﬁc components.
Complete results available at http://j.mp/jstor-html.

specie, and monkey). This kind of correlation is
also evident in topic #23, about physics. While the
science documents clearly represent research in
particle physics, it is interesting to ﬁnd the topic is
also represented by humanist research focused on
cultural representations of science. This reﬂects a
growing interest in science and technology studies
that has gained recent traction in the humanities.
Despite their differences, both collections engage
with a similar theme, seen in the shared compo-
nent with words like particle, energy and atom.

The results also indicate that while sciences and
humanities documents can share themes, they of-
ten diverge in how they are discussed. For exam-
ple, common topic #21 could be identiﬁed as eco-
nomic or capitalist, but in the collection-speciﬁc
components, the two disciplines differ in their ar-
ticulatation. Science uses terms like price and
market, indicating an acceptance of free-market
capitalism (especially as it affects the practice of
science), while the humanities, which has long
been critical of free-market capitalism, uses terms
like rural and community, highlighting cultural
facets of modern economics. These results pro-
vide evidence about how ideas move between the
sciences and humanities — a phenomenon that
constitutes a growing area of research for histori-
ans (Galison, 2003; Canales, 2015). C-LDA pro-
vides empirical, measurable, and reproducible ev-
idence of the shared research between these disci-
plines, as well as how concepts are articulated.

6 Discussion

Our models provide a robust way to explore
large and potentially weakly-related text collec-
tions without
the
data. Like ccLDA and TPYP, our models ac-
count for topic-word variation at the collection
level. The models accommodate asymmetry in

imposing assumptions about

the numbers of topics (set in C-LDA, ﬁt in C-
HDP) and provide an efﬁcient inference method
which allows them to ﬁt data with large values
for K, which can help ﬁnd correlations in less
prevalent topics. Our primary contribution is our
models’ ability to accommodate asymmetries be-
tween arbitrary collections. JSTOR, the world’s
largest digital collection of humanities research,
was an ideal application setting given the size,
asymmetry, and comprehensiveness of the human-
ities collection. As we show, humanities and
science research exhibit asymmetries with regard
to vocabulary and topic structure — asymmetries
that would be systematically overlooked using ex-
isting models. By characterizing common top-
ics as mixtures of shared and collection-speciﬁc
components, we can capture a kind of topic-level
homophily, where similar themes are articulated
in different ways due to word-, document-, and
collection-level variation. Future work on these
models could explore methods to ﬁt non-common
topics for both collections. In general, C-LDA and
C-HDP can be used whenever documents are sam-
pled from ostensibly different populations, where
the nature of the difference is unknown.

Acknowledgements

Thanks to David Blei for advice on applications
of the model. This work contains analysis of pri-
vate, or otherwise restricted data, made available
to James Evans and Eamon Duede by ITHAKA
(JSTOR), the opinions of whom are not repre-
sented in this paper. Jaan Altosaar acknowledges
support from the Natural Sciences and Engineer-
ing Research Council of Canada. This work was
supported by a grant from the Templeton Foun-
dation to the Metaknowledge Research Network
and by grant #1158803 from the National Science
Foundation.

References

Loulwah AlSumait, Daniel Barbar´a, James Gentle, and
2009. Topic signiﬁcance
Carlotta Domeniconi.
In Machine
ranking of LDA generative models.
Learning and Knowledge Discovery in Databases,
pages 67–82. Springer.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Wray Buntine and Marcus Hutter. 2010. A Bayesian
arXiv

the Poisson-Dirichlet process.

view of
preprint arXiv:1007.0296.

Jimena Canales.

2015.

The Physicist and the
Philosopher: Einstein, Bergson, and the Debate that
Changed our Understanding of Time. Princeton
University Press, Princeton, NJ.

Changyou Chen, Lan Du, and Wray Buntine. 2011.
Sampling table conﬁgurations for the hierarchical
In Machine Learning
Poisson-Dirichlet process.
and Knowledge Discovery in Databases, pages 296–
311. Springer.

Changyou Chen, Wray Buntine, Nan Ding, Lexing Xie,
and Lan Du. 2014. Differential topic models. IEEE
Transactions on Pattern Analysis and Machine In-
telligence.

M. Denny, J. ben Aaron, H. Wallach, and B. Desmarais.
2014. Modeling email network content and struc-
ture. In the 72nd Annual Midwest Political Science
Association Conference, 2014; the Northeast Politi-
cal Methodology Meeting, 2014; the 7th Annual Po-
litical Networks Conference, the Society for Political
Methodology 31st Annual Summer Meeting.

Peter Galison. 2003. Poincar’s Maps. Norton, New

York, NY.

Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 50–57.

Richard A Kronmal and Arthur V Peterson Jr. 1979.
On the alias method for generating random variables
from a discrete distribution. The American Statisti-
cian, 33(4):214–218.

Alp Kucukelbir and David M Blei. 2014. Proﬁle pre-
dictive inference. arXiv preprint arXiv:1411.0292.

Aaron Q Li, Amr Ahmed, Sujith Ravi, and Alexan-
der J Smola. 2014. Reducing the sampling com-
plexity of topic models. In Proceedings of the 20th
ACM SIGKDD international conference on knowl-
edge discovery and data mining, pages 891–900.

Mian Lu, Ge Bai, Qiong Luo, Jie Tang, and Jiuxin
Zhao. 2013. Accelerating topic model training on
a single machine. In Web Technologies and Appli-
cations, pages 184–195. Springer.

George Marsaglia, Wai Wan Tsang, and Jingbo Wang.
2004. Fast generation of discrete random variables.
Journal of Statistical Software, 11:1–8.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262–
272.

David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. The Journal of Machine Learning Re-
search, 10:1801–1828.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin.
2010a. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100–108.

David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010b. Evaluating
In Proceedings
topic models for digital libraries.
of the 10th Annual Joint Conference on Digital Li-
braries, JCDL ’10, pages 215–224. ACM.

Michael Paul and Roxana Girju. 2009. Cross-cultural
analysis of blogs and forums with mixed-collection
In Proceedings of the 2009 Confer-
topic models.
ence on Empirical Methods in Natural Language
Processing: Volume 3, pages 1408–1417.

Alexander Smola and Shravan Narayanamurthy. 2010.
An architecture for parallel topic models. Proceed-
ings of the VLDB Endowment, 3(1-2):703–710.

Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the american statistical associa-
tion, 101(476).

Hanna M Wallach, David Mimno, and Andrew Mccal-
lum. 2009a. Rethinking LDA: Why priors matter.
In Advances in Neural Information Processing Sys-
tems, pages 1973–1981.

Hanna M. Wallach, Iain Murray, Ruslan Salakhutdi-
nov, and David Mimno. 2009b. Evaluation methods
In Proceedings of the 26th An-
for topic models.
nual International Conference on Machine Learn-
ing, ICML ’09, pages 1105–1112, New York, NY,
USA. ACM.

Hanna M Wallach. 2008. Structured topic models for
language. Unpublished doctoral dissertation, Univ.
of Cambridge.

Tze-I Yang, Andrew J Torget, and Rada Mihalcea.
2011. Topic modeling on historical newspapers. In
Proceedings of the 5th ACL-HLT Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, pages 96–104.

Limin Yao, David Mimno, and Andrew McCallum.
2009. Efﬁcient methods for topic model inference
In Proceed-
on streaming document collections.
ings of the 15th ACM SIGKDD international con-
ference on knowledge discovery and data mining,
pages 937–946.

Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang
Wei, Xun Zheng, Eric Po Xing, Tie-Yan Liu, and
Wei-Ying Ma. 2015. Lightlda: Big topic models
on modest computer clusters. In Proceedings of the
24th International Conference on World Wide Web,
pages 1351–1361. International World Wide Web
Conferences Steering Committee.

ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative
In Proceedings of the tenth ACM
text mining.
SIGKDD international conference on knowledge
discovery and data mining, pages 743–748.

