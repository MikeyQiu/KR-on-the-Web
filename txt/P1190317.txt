8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
6
7
0
.
5
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

GUIDE ACTOR-CRITIC FOR CONTINUOUS CONTROL

Voot Tangkaratt
RIKEN AIP, Tokyo, Japan
voot.tangkaratt@riken.jp

Abbas Abdolmaleki
The University of Aveiro, Aveiro, Portugal
abbas.a@ua.pt

Masashi Sugiyama
RIKEN AIP, Tokyo, Japan
The University of Tokyo, Tokyo, Japan
masashi.sugiyama@riken.jp

ABSTRACT

Actor-critic methods solve reinforcement learning problems by updating a param-
eterized policy known as an actor in a direction that increases an estimate of the
expected return known as a critic. However, existing actor-critic methods only
use values or gradients of the critic to update the policy parameter. In this pa-
per, we propose a novel actor-critic method called the guide actor-critic (GAC).
GAC ﬁrstly learns a guide actor that locally maximizes the critic and then it up-
dates the policy parameter based on the guide actor by supervised learning. Our
main theoretical contributions are two folds. First, we show that GAC updates the
guide actor by performing second-order optimization in the action space where
the curvature matrix is based on the Hessians of the critic. Second, we show that
the deterministic policy gradient method is a special case of GAC when the Hes-
sians are ignored. Through experiments, we show that our method is a promising
reinforcement learning method for continuous controls.

1

INTRODUCTION

The goal of reinforcement learning (RL) is to learn an optimal policy that lets an agent achieve the
maximum cumulative rewards known as the return (Sutton & Barto, 1998). Reinforcement learning
has been shown to be effective in solving challenging artiﬁcial intelligence tasks such as playing
games (Mnih et al., 2015; Silver et al., 2016) and controlling robots (Deisenroth et al., 2013; Levine
et al., 2016).

Reinforcement learning methods can be classiﬁed into three categories: value-based, policy-based,
and actor-critic methods. Value-based methods learn an optimal policy by ﬁrstly learning a value
function that estimates the expected return. Then, they infer an optimal policy by choosing an
action that maximizes the learned value function. Choosing an action in this way requires solving a
maximization problem which is not trivial for continuous controls. While extensions to continuous
controls were considered recently, they are restrictive since speciﬁc structures of the value function
are assumed (Gu et al., 2016; Amos et al., 2017).

On the other hand, policy-based methods, also called policy search methods (Deisenroth et al.,
2013), learn a parameterized policy maximizing a sample approximation of the expected re-
turn without learning the value function. For instance, policy gradient methods such as REIN-
FORCE (Williams, 1992) use gradient ascent to update the policy parameter so that the probability
of observing high sample returns increases. Compared with value-based methods, policy search
methods are simpler and naturally applicable to continuous problems. Moreover, the sample return
is an unbiased estimator of the expected return and methods such as policy gradients are guaranteed
to converge to a locally optimal policy under standard regularity conditions (Sutton et al., 1999).
However, sample returns usually have high variance and this makes such policy search methods
converge too slowly.

Actor-critic methods combine the advantages of value-based and policy search methods. In these
methods, the parameterized policy is called an actor and the learned value-function is called a critic.

1

Published as a conference paper at ICLR 2018

The goal of these methods is to learn an actor that maximizes the critic. Since the critic is a low vari-
ance estimator of the expected return, these methods often converge much faster than policy search
methods. Prominent examples of these methods are actor-critic (Sutton et al., 1999; Konda & Tsit-
siklis, 2003), natural actor-critic (Peters & Schaal, 2008), trust-region policy optimization (Schul-
man et al., 2015a), and asynchronous advantage actor-critic (Mnih et al., 2016). While their ap-
proaches to learn the actor are different, they share a common property that they only use the value
of the critic, i.e., the zero-th order information, and ignore higher-order ones such as gradients and
Hessians w.r.t. actions of the critic1. To the best of our knowledge, the only actor-critic methods
that use gradients of the critic to update the actor are deterministic policy gradients (DPG) (Silver
et al., 2014) and stochastic value gradients (Heess et al., 2015). However, these two methods do not
utilize the second-order information of the critic.

In this paper, we argue that the second-order information of the critic is useful and should not be
ignored. A motivating example can be seen by comparing gradient ascent to the Newton method: the
Newton method which also uses the Hessian converges to a local optimum in a fewer iterations when
compared to gradient ascent which only uses the gradient (Nocedal & Wright, 2006). This suggests
that the Hessian of the critic can accelerate actor learning which leads to higher data efﬁciency.
However, the computational complexity of second-order methods is at least quadratic in terms of
the number of optimization variables. For this reason, applying second-order methods to optimize
the parameterized actor directly is prohibitively expensive and impractical for deep reinforcement
learning which represents the actor by deep neural networks.

Our contribution in this paper is a novel actor-critic method for continuous controls which we call
guide actor-critic (GAC). Unlike existing methods, the actor update of GAC utilizes the second-
order information of the critic in a computationally efﬁcient manner. This is achieved by separating
actor learning into two steps. In the ﬁrst step, we learn a non-parameterized Gaussian actor that
locally maximizes the critic under a Kullback-Leibler (KL) divergence constraint. Then, the Gaus-
sian actor is used as a guide for learning a parameterized actor by supervised learning. Our analysis
shows that learning the mean of the Gaussian actor is equivalent to performing a second-order update
in the action space where the curvature matrix is given by Hessians of the critic and the step-size is
controlled by the KL constraint. Furthermore, we establish a connection between GAC and DPG
where we show that DPG is a special case of GAC when the Hessians and KL constraint are ignored.

2 BACKGROUND

In this section, we ﬁrstly give a background of reinforcement learning. Then, we discuss existing
second-order methods for policy learning and their issue in deep reinforcement learning.

2.1 REINFORCEMENT LEARNING

We consider discrete-time Markov decision processes (MDPs) with continuous state space S Ď Rds
and continuous action space A Ď Rda . We denote the state and action at time step t P N by
st and at, respectively. The initial state s1 is determined by the initial state density s1 „ ppsq.
At time step t, the agent in state st takes an action at according to a policy at „ πpa|stq and
obtains a reward rt “ rpst, atq. Then, the next state st`1 is determined by the transition function
st`1 „ pps1|st, atq. A trajectory τ “ ps1, a1, r1, s2, . . . q gives us the cumulative rewards or return
t“1 γt´1rpst, atq, where the discount factor 0 ă γ ă 1 assigns different weights to
deﬁned as
rewards given at different time steps. The expected return of π after executing an action a in a state
s can be expressed through the action-value function which is deﬁned as

ř

8

ﬀ

Qπps, aq “ Eπpat|stqtě2,ppst`1|st,atqtě1

γt´1rpst, atq|s1 “ s, a1 “ a

,

(1)

«

8ÿ

t“1

where Ep r¨s denotes the expectation over the density p and the subscript t ě 1 indicates that the
expectation is taken over the densities at time steps t ě 1. We can deﬁne the expected return as
8ÿ

«

ﬀ

J pπq “ Epps1q,πpat|stqtě1,ppst`1|st,atqtě1

γt´1rpst, atq

“ Eppsq,πpa|sq rQπps, aqs .

(2)

1This is different from using the gradient of the critic w.r.t. critic parameters to update the critic itself.

t“1

2

Published as a conference paper at ICLR 2018

The goal of reinforcement learning is to ﬁnd an optimal policy that maximizes the expected return.
The policy search approach (Deisenroth et al., 2013) parameterizes π by a parameter θ P Rdθ and
ﬁnds θ‹ which maximizes the expected return:

θ‹ “ argmax

Eppsq,πθ pa|sq rQπθ ps, aqs .

θ

(3)

(4)

(5)

(7)

Policy gradient methods such as REINFORCE (Williams, 1992) solve this optimization problem by
gradient ascent:

θ Ð θ ` αEppsq,πθ pa|sq r∇θ log πθpa|sqQπθ ps, aqs ,

ř

where α ą 0 is a step-size. In policy search, the action-value function is commonly estimated by the
sample return: Qπθ ps, aq « 1
n,t γt´1rpst,n, at,nq obtained by collecting N trajectories using
N
πθ. The sample return is an unbiased estimator of the action-value function. However, it often has
high variance which leads to slow convergence.
An alternative approach is to estimate the action-value function by a critic denoted by pQps, aq
whose parameter is learned such that pQps, aq « Qπθ ps, aq. By replacing the action-value function
in Eq.(3) with the critic, we obtain the following optimization problem:

θ‹ “ argmax

Eppsq,πθ pa|sq

θ

ı

”
pQps, aq

.

The actor-critic method (Sutton et al., 1999) solves this optimization problem by gradient ascent:
”
∇θ log πθpa|sq

θ Ð θ ` αEppsq,πθ pa|sq

pQps, aq

ı

.

(6)

The gradient in Eq.(6) often has less variance than that in Eq.(4), which leads to faster convergence2.
A large class of actor-critic methods is based on this method (Peters & Schaal, 2008; Mnih et al.,
2016). As shown in these papers, these methods only use the value of the critic to learn the actor.

The deterministic policy gradients (DPG) method (Silver et al., 2014) is an actor-critic method that
uses the ﬁrst-order information of the critic. DPG updates a deterministic actor πθpsq by

θ Ð θ ` αEppsq

”
∇θπθpsq∇a

ı

pQps, aq|a“πθ psq

.

A method related to DPG is the stochastic value gradients (SVG) method (Heess et al., 2015) that
is able to learn a stochastic policy but it requires learning a model of the transition function.

The actor-critic methods described above only use up to the ﬁrst-order information of the critic when
learning the actor and ignore higher-order ones. Below, we discuss existing approaches that utilize
the second-order information by applying second-order optimization methods to solve Eq.(5).

2.2 SECOND-ORDER METHODS FOR POLICY LEARNING

The actor-critic methods described above are ﬁrst-order methods which update the optimization
variables based on the gradient of the objective function. First-order methods are popular in deep
learning thanks to their computational efﬁciency. However, it is known in machine learning that
second-order methods often lead to faster learning because they use the curvature information to
compute a better update direction, i.e., the steepest ascent direction along the curvature3.

The main idea of second-order methods is to rotate the gradient by the inverse of a curvature matrix.
For instance, second-order updates for the actor-critic method in Eq.(6) are in the following form:

θ Ð θ ` αG´1

!
Eppsq,πθ pa|sq

”
∇θ log πθpa|sq

ı)

pQps, aq

,

(8)

2This gradient is a biased estimator of the policy gradient in Eq.(4). However, it is unbiased under some

regularity conditions such as compatible function conditions (Sutton et al., 1999).

3We use the term “second-order methods” in a broad sense here, including quasi-Newton and natural gradi-

ent methods which approximate the curvature matrix by the ﬁrst-order information.

3

Published as a conference paper at ICLR 2018

where G P Rdθ ˆdθ is a curvature matrix. The behavior of second-order methods depend on the
deﬁnition of a curvature matrix. The most well-known second-order method is the Newton method
where its curvature matrix is the Hessian of the objective function w.r.t. the optimization variables:
∇θ log πθpa|sq∇θ log πθpa|sqJ ` ∇2

GHessian “ Eppsq,πθ pa|sq

θ log πθpa|sq

pQps, aq

”`

(9)

ı

˘

.

“

∇θ log πθpa|sq∇θ log πθpa|sqJ

The natural gradient method is another well-known second-order method which uses the Fisher
information matrix (FIM) as the curvature matrix (Amari, 1998):
GFIM “ Eppsq,πθ pa|sq

(10)
Unlike the Hessian matrix, FIM provides information about changes of the policy measured by an
approximated KL divergence: Eppsq rKLpπθpa|sq||πθ1pa|sqqs « pθ ´ θ1qJGFIMpθ ´ θ1q (Kakade,
2001). We can see that GHessian and GFIM are very similar but the former also contains the critic
and the Hessian of the actor while the latter does not. This suggests that the Hessian provides more
information than that in FIM. However, FIM is always positive semi-deﬁnite while the Hessian may
be indeﬁnite. Please see Furmston et al. (2016) for detailed comparisons between the two curvature
matrices in policy search4. Nonetheless, actor-critic methods based on natural gradient were shown
to be very efﬁcient (Peters & Schaal, 2008; Schulman et al., 2015a).

‰

.

We are not aware of existing work that considers second-order updates for DPG or SVG. However,
their second-order updates can be trivially derived. For example, a Newton update for DPG is

θ Ð θ ` αD´1

!
Eppsq

”
∇θπθpsq∇a

pQps, aq|a“πθ psq

ı)

,

where the pi, jq-th entry of the Hessian matrix D P Rdθ ˆdθ is

Dij “ Eppsq

∇2
a

pQps, aq|a“πθ psq

Bπθpsq
Bθj

`

B2πθpsq
BθiBθj

J

∇a

pQps, aq|a“πθ psq

.

(12)

«

J

Bπθpsq
Bθi

(11)

ﬀ

Note that Bπθpsq{Bθ and B2πθpsq{BθBθ1 are vectors since πθpsq is a vector-valued function. In-
terestingly, the Hessian of DPG contains the Hessians of the actor and the critic. In contrast, the
Hessian of the actor-critic method contains the Hessian of the actor and the value of the critic.

Second-order methods are appealing in reinforcement learning because they have high data efﬁ-
ciency. However, inverting the curvature matrix (or solving a linear system) requires cubic compu-
tational complexity in terms of the number of optimization variables. For this reason, the second-
order updates in Eq.(8) and Eq.(11) are impractical in deep reinforcement learning due to a large
number of weight parameters in deep neural networks. In such a scenario, an approximation of
the curvature matrix is required to reduce the computational burden. For instance, Furmston et al.
(2016) proposed to use only diagonal entries of an approximated Hessian matrix. However, this
approximation clearly leads to a loss of useful curvature information since the gradient is scaled but
not rotated. More recently, Wu et al. (2017) proposed a natural actor-critic method that approxi-
mates block-diagonal entries of FIM. However, this approximation corresponds to ignoring useful
correlations between weight parameters in different layers of neural networks.

3 GUIDE ACTOR-CRITIC

In this section, we propose the guide actor-critic (GAC) method that performs second-order updates
without the previously discussed computational issue. Unlike existing methods that directly learn
the parameterized actor from the critic, GAC separates the problem of learning the parameterized
actor into problems of 1) learning a guide actor that locally maximizes the critic, and 2) learning a
parameterized actor based on the guide actor. This separation allows us to perform a second-order
update for the guide actor where the dimensionality of the curvature matrix is independent of the
parameterization of the actor.

We formulate an optimization problem for learning the guide actor in Section 3.1 and present its so-
lution in Section 3.2. Then in Section 3.3 and Section 3.4, we show that the solution corresponds to
performing second-order updates. Finally, Section 3.5 presents the learning step for the parameter-
ized actor using supervised learning. The pseudo-code of our method is provided in Appendix B and
the source code is available at https://github.com/voot-t/guide-actor-critic.

4 Furmston et al. (2016) proposed an approximate Newton method for policy search. Their policy search

method was shown to perform better than methods based on gradient ascent and natural gradient ascent.

4

Published as a conference paper at ICLR 2018

3.1 OPTIMIZATION PROBLEM FOR GUIDE ACTOR

Our ﬁrst goal is to learn a guide actor that maximizes the critic. However, greedy maximization
should be avoided since the critic is a noisy estimate of the expected return and a greedy actor
may change too abruptly across learning iterations. Such a behavior is undesirable in real-world
problems, especially in robotics (Deisenroth et al., 2013).
Instead, we maximize the critic with
additional constraints:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

(13)

where ˜πpa|sq is the guide actor to be learned, πθpa|sq is the current parameterized actor that we
want to improve upon, and pβpsq is the state distribution induced by past trajectories. The objective
function differs from the one in Eq.(5) in two important aspects. First, we maximize for a policy
function ˜π and not for the policy parameter. This is more advantageous than optimizing for a policy
parameter since the policy function can be obtained in a closed form, as will be shown in the next
subsection. Second, the expectation is deﬁned over a state distribution from past trajectories and this
gives us off-policy methods with higher data efﬁciency. The ﬁrst constraint is the Kullback-Leibler
(KL) divergence constraint where KLpppxq||qpxqq “ Eppxq rlog ppxq ´ log qpxqs. The second con-
straint is the Shannon entropy constraint where Hpppxqq “ ´Eppxq rlog ppxqs. The KL constraint
is commonly used in reinforcement learning to prevent unstable behavior due to excessively greedy
update (Peters & Schaal, 2008; Peters et al., 2010; Levine & Koltun, 2013; Schulman et al., 2015a).
The entropy constraint is crucial for maintaining stochastic behavior and preventing premature con-
vergence (Ziebart et al., 2010; Abdolmaleki et al., 2015; Mnih et al., 2016; Haarnoja et al., 2017).
The ﬁnal constraint ensures that the guide actor is a proper probability density. The KL bound
(cid:15) ą 0 and the entropy bound ´8 ă κ ă 8 are hyper-parameters which control the exploration-
exploitation trade-off of the method. In practice, we ﬁx the value of (cid:15) and adaptively reduce the
value of κ based on the current actor’s entropy, as suggested by Abdolmaleki et al. (2015). More
details of these tuning parameters are given in Appendix C.

This optimization problem can be solved by the method of Lagrange multipliers. The solution is

˜πpa|sq 9 πθpa|sq

η‹`ω‹ exp

η‹

˜

¸

,

pQps, aq
η‹ ` ω‹

(14)

where η‹ ą 0 and ω‹ ą 0 are dual variables corresponding to the KL and entropy constraints,
respectively. The dual variable corresponding to the probability density constraint is contained in the
normalization term and is determined by η‹ and ω‹. These dual variables are obtained by minimizing
the dual function:

«

ż

gpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

πθpa|sq

η
η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

.

(15)

All derivations and proofs are given in Appendix A. The solution in Eq.(14) tells us that the
guide actor is obtained by weighting the current actor with pQps, aq.
If we set (cid:15) Ñ 0 then we
have ˜π « πθ and the actor is not updated. On the other hand, if we set (cid:15) Ñ 8 then we have
˜πpa|sq 9 expp

pQps, aq{ω‹q, which is a softmax policy where ω‹ is the temperature parameter.

3.2 LEARNING GUIDE ACTOR

Computing ˜πpa|sq and evaluating gpη, ωq are intractable for an arbitrary πθpa|sq. We overcome
this issue by imposing two assumptions. First, we assume that the actor is the Gaussian distribution:

πθpa|sq “ N pa|φθpsq, Σθpsqq,
where the mean φθpsq and covariance Σθpsq are functions parameterized by a policy parameter
θ. Second, we assume that Taylor’s approximation of pQps, aq is locally accurate up to the second-
order. More concretely, the second-order Taylor’s approximation using an arbitrary action a0 is

(16)

5

Published as a conference paper at ICLR 2018

given by

pQps, aq «

pQps, a0q ` pa ´ a0qJg0psq `

pa ´ a0qJH 0psqpa ´ a0q ` Op}a}3q,

(17)

1
2

pQps, aq|a“a0 are the gradient and Hessian of
where g0psq “ ∇a
the critic w.r.t. a evaluated at a0, respectively. By assuming that the higher order term Op}a}3q is
sufﬁciently small, we can rewrite Taylor’s approximation at a0 as

pQps, aq|a“a0 and H 0psq “ ∇2

a

pQ0ps, aq “

1
2

aJH 0psqa ` aJψ0psq ` ξ0psq,

(18)

pQps, a0q. Note that
where ψ0psq “ g0psq ´ H 0psqa0 and ξ0psq “ 1
H 0psq, ψ0psq, and ξ0psq depend on the value of a0 and do not depend on the value of a. This
dependency is explicitly denoted by the subscript. The choice of a0 will be discussed in Section 3.3.

0 H 0psqa0 ´ aJ

0 g0psq `

2 aJ

Substituting the Gaussian distribution and Taylor’s approximation into Eq.(14) yields another Gaus-
sian distribution ˜πpa|sq “ N pa|φ`psq, Σ`psqq, where the mean and covariance are given by

φ`psq “ F ´1psqLpsq, Σ`psq “ pη‹ ` ω‹qF ´1psq.

The matrix F psq P Rdaˆda and vector Lpsq P Rda are deﬁned as
θ psq ´ H 0psq, Lpsq “ η‹Σ´1

F psq “ η‹Σ´1

θ psqφθpsq ` ψ0psq.

The dual variables η‹ and ω‹ are obtained by minimizing the following dual function:

(19)

(20)

pgpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

«

d

ﬀ

|2πpη ` ωqF ´1
|2πΣθpsq|

η
η`ω

η psq|

`

Epβ psq

LηpsqJF ´1

η psqLηpsq ´ ηφθpsqJΣ´1

` const,

(21)

‰
θ psqφθpsq

“

1
2

where F ηpsq and Lηpsq are deﬁned similarly to F psq and Lpsq but with η instead of η‹.

The practical advantage of using the Gaussian distribution and Taylor’s approximation is that the
guide actor can be obtained in a closed form and the dual function can be evaluated through matrix-
vector products. The expectation over pβpsq can be approximated by e.g., samples drawn from a
replay buffer (Mnih et al., 2015). We require inverting F ηpsq to evaluate the dual function. However,
these matrices are computationally cheap to invert when the dimension of actions is not large.

As shown in Eq.(19), the mean and covariance of the guide actor is computed using both the gradient
and Hessian of the critic. Yet, these computations do not resemble second-order updates discussed
previously in Section 2.2. Below, we show that for a particular choice of a0, the mean computation
corresponds to a second-order update that rotates gradients by a curvature matrix.

3.3 GUIDE ACTOR LEARNING AS SECOND-ORDER OPTIMIZATION

For now we assume that the critic is an accurate estimator of the true action-value function. In this
case, the quality of the guide actor depends on the accuracy of sample approximation in pgpη, ωq and
the accuracy of Taylor’s approximation. To obtain an accurate Taylor’s approximation of pQps, aq
using an action a0, the action a0 should be in the vicinity of a. However, we did not directly use
pQps, aqq (see Eq.(14)).
any individual a to compute the guide actor, but we weight πθpa|sq by expp
Thus, to obtain an accurate Taylor’s approximation of the critic, the action a0 needs to be similar to
actions sampled from πθpa|sq. Based on this observation, we propose two approaches to perform
Taylor’s approximation.

Taylor’s approximation around the mean. In this approach, we perform Taylor’s approximation
using the mean of πθpa|sq. More speciﬁcally, we use a0 “ Eπθ pa|sq ras “ φθpsq for Eq.(18). In
this case, we can show that the mean update in Eq.(19) corresponds to performing a second-order
update in the action space to maximize pQps, aq:

φ`psq “ φθpsq ` F ´1
φθ

psq∇a

pQps, aq|a“φθ psq,

(22)

6

Published as a conference paper at ICLR 2018

θ psq ´ H φθ psq and H φθ psq “ ∇2
a

pQps, aq|a“φθ psq. This equivalence can
where F φθ psq “ η‹Σ´1
be shown by substitution and the proof is given in Appendix A.2. This update equation reveals that
the guide actor maximizes the critic by taking a step in the action space similarly to the Newton
method. However, the main difference lies in the curvature matrix where the Newton method uses
Hessians H φθ psq but we use a damped Hessian F φθ psq. The damping term η‹Σ´1
θ psq corresponds
to the effect of the KL constraint and can be viewed as a trust-region that controls the step-size. This
damping term is particularly important since Taylor’s approximation is accurate only locally and we
should not take a large step in each update (Nocedal & Wright, 2006).

Expectation of Taylor’s approximations.
Instead of using Taylor’s approximation around the
mean, we may use an expectation of Taylor’s approximation over the distribution. More concretely,
we deﬁne rQps, aq to be an expectation of pQ0ps, aq over πθpa0|sq:

rQps, aq “

1
2

aJEπθ pa0|sq rH 0psqs a ` aJEπθ pa0|sq rψ0psqs ` Eπθ pa0|sq rξ0psqs .

(23)

pQps, aq|a“a0 s and the expectation is computed w.r.t.
Note that Eπθ pa0|sqrH 0psqs “ Eπθ pa0|sqr∇2
a
the distribution πθ of a0. We use this notation to avoid confusion even though πθpa0|sq and πθpa|sq
are the same distribution. When Eq.(23) is used, the mean update does not directly correspond
to any second-order optimization step. However, under an (unrealistic) independence assumption
Eπθ pa0|sqrH 0psqa0s “ Eπθ pa0|sqrH 0psqsEπθ pa0|sqra0s, we can show that the mean update corre-
sponds to the following second-order optimization step:

φ`psq “ φθpsq ` Eπθ pa0|sq rF 0psqs´1 Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

,

(24)

where Eπθ pa0|sq rF 0psqs “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs. Interestingly, the mean is updated
by rotating an expected gradient using an expected Hessians. In practice, the expectations can be
approximated using sampled actions ta0,iuS
i“1 „ πθpa|sq. We believe that this sampling can be
advantageous for avoiding local optima. Note that when the expectation is approximated by a single
sample a0 „ πθpa|sq, we obtain the update in Eq.(24) regardless of the independence assumption.
In the remainder, we use F psq to denote both of F φθ psq and Eπθ pa0|sq rF 0psqs, and use Hpsq to
denote both of H φθ psq and Eπθ pa0|sqrH 0psqs. In the experiments, we use GAC-0 to refer to GAC
with Taylor’s approximation around the mean, and we use GAC-1 to refer to GAC with Taylor’s
approximation by a single sample a0 „ πθpa|sq.

3.4 GAUSS-NEWTON APPROXIMATION OF HESSIAN

The covariance update in Eq.(19) indicates that F psq “ η‹Σ´1
θ psq ´ Hpsq needs to be positive
deﬁnite. The matrix F psq is guaranteed to be positive deﬁnite if the Hessian matrix Hpsq is negative
semi-deﬁnite. However, this is not guaranteed in practice unless pQps, aq is a concave function in
terms of a. To overcome this issue, we ﬁrstly consider the following identity:
pQps, aq∇a

pQps, aq “ ´∇a

pQps, aqq expp´

pQps, aqJ ` ∇2

pQps, aqq.

Hpsq “ ∇2
a

a expp

(25)

The proof is given in Appendix A.3. The ﬁrst term is always negative semi-deﬁnite while the second
term is indeﬁnite. Therefore, a negative semi-deﬁnite approximation of the Hessian can be obtained
as

H 0psq « ´

pQps, aq∇a

pQps, aqJ

”
∇a

ı

.

a“a0

(26)

pQps, aqq and it will be small for high values
The second term in Eq.(25) is proportional to expp´
of pQps, aq. This implies that the approximation should gets more accurate as the policy approach
a local maxima of pQps, aq. We call this approximation Gauss-Newton approximation since it is
similar to the Gauss-Newton approximation for the Newton method (Nocedal & Wright, 2006).

3.5 LEARNING PARAMETERIZED ACTOR

The second step of GAC is to learn a parameterized actor that well represents the guide actor. Below,
we discuss two supervised learning approaches for learning a parameterized actor.

7

Published as a conference paper at ICLR 2018

3.5.1 FULLY-PARAMETERIZED GAUSSIAN POLICY

Since the guide actor is a Gaussian distribution with a state-dependent mean and covariance, a
natural choice for the parameterized actor is again a parameterized Gaussian distribution with a
state-dependent mean and covariance: πθpa|sq “ N pa|φθpsq, Σθpsqq. The parameter θ can be
learned by minimizing the expected KL divergence to the guide actor:

LKLpθq “ Epβ psq rKL pπθpa|sq||˜πpa|sqqs

„



TrpF psqΣθpsqq
η‹ ` ω‹

“ Epβ psq
”
}φθpsq ´ φ`psq}2

´ log |Σθpsq|
ı

`

LWpθq
η‹ ` ω‹ ` const,

(27)

where LWpθq “ Epβ psq
which only depends on θ of the mean function. The const term does not depend on θ.

is the weighted-mean-squared-error (WMSE)

F psq

Minimizing the KL divergence reveals connections between GAC and deterministic policy gradients
(DPG) (Silver et al., 2014). By computing the gradient of the WMSE, it can be shown that

∇θLWpθq
2

“ ´Epβ psq

ı

pQps, aq|a“φθ psq

”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

psq∇a

psq∇a

”
∇θφθpsq∇a
ı

` Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

ı

.

` η‹Epβ psq

´ η‹Epβ psq

ı

pQps, aq|a“φ`psq

(28)

The proof is given in Appendix A.4. The negative of the ﬁrst term is precisely equivalent to DPG.
Thus, updating the mean parameter by minimizing the KL loss with gradient descent can be regarded
as updating the mean parameter with biased DPG where the bias terms depend on η‹. We can verify
pQps, aq|a“φ`psq “ 0 when η‹ “ 0 and this is the case of (cid:15) Ñ 8. Thus, all bias terms vanish
that ∇a
when the KL constraint is ignored and the mean update of GAC coincides with DPG. However,
unlike DPG which learns a deterministic policy, we can learn both the mean and covariance in GAC.

3.5.2 GAUSSIAN POLICY WITH PARAMETERIZED MEAN

While a state-dependent parameterized covariance function is ﬂexible, we observe that learning
performance is sensitive to the initial parameter of the covariance function. For practical pur-
poses, we propose using a parametrized Gaussian distribution with state-independent covariance:
πθpa|sq “ N pa|φθpsq, Σq. This class of policies subsumes deterministic policies with additive in-
dependent Gaussian noise for exploration. To learn θ, we minimize the mean-squared-error (MSE):
“

(29)
‰
For the covariance, we use the average of the guide covariances: Σ “ pη‹ ` ω‹qEpβ psq
.
For computational efﬁciency, we execute a single gradient update in each learning iteration instead
of optimizing this loss function until convergence.

F ´1psq

}φθpsq ´ φ`psq}2
2

LMpθq “

Epβ psq

1
2

‰

“

.

Similarly to the above analysis, the gradient of the MSE w.r.t. θ can be expanded and rewritten into

∇θLMpθq “ Epβ psq

”
∇θφθpsqH ´1psq

´

∇a

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

.

(30)

¯ı

Again, the mean update of GAC coincides with DPG when we minimize the MSE and set η‹ “ 0
and Hpsq “ ´I where I is the identity matrix. We can also substitute these values back into
Eq.(22). By doing so, we can interpret DPG as a method that performs ﬁrst-order optimization in
the action space:

φ`psq “ φθpsq ` ∇a

pQps, aq|a“φθ psq,

(31)

and then uses the gradient in Eq.(30) to update the policy parameter. This interpretation shows that
DPG is a ﬁrst-order method that only uses the ﬁrst-order information of the critic for actor learning.
Therefore in principle, GAC, which uses the second-order information of the critic, should learn
faster than DPG.

8

Published as a conference paper at ICLR 2018

3.6 POLICY EVALUATION FOR CRITIC

Beside actor learning, the performance of actor-critic methods also depends on the accuracy of the
critic. We assume that the critic pQν ps, aq is represented by neural networks with a parameter ν.
We adopt the approach proposed by Lillicrap et al. (2015) with some adjustment to learn ν. More
concretely, we use gradient descent to minimize the squared Bellman error with a slowly moving
target critic:

ν Ð ν ´ α∇ν Epβ psq,βpa|sq,pps1|s,aq

pQν ps, aq ´ y

(32)

„´



¯

2

,

pQ¯ν ps1, a1qs is computed by
where α ą 0 is the step-size. The target value y “ rps, aq ` γEπpa1|s1qr
the target critic pQ¯ν ps1, a1q whose parameter ¯ν is updated by ¯ν Ð τ ν ` p1 ´ τ q¯ν for 0 ă τ ă 1.
As suggested by Lillicrap et al. (2015), the target critic improves the learning stability and we set
τ “ 0.001 in experiments. The expectation for the squared error is approximated using mini-batch
samples tpsn, an, rn, s1
n“1 drawn from a replay buffer. The expectation over the current actor
πpa1|s1q is approximated using samples ta1
n. We do not use a
target actor to compute y since the KL upper-bound already constrains the actor update and a target
actor will further slow it down. Note that we are not restricted to this evaluation method and more
efﬁcient methods such as Retrace (Munos et al., 2016) can also be used.

m“1 „ πθpa1|s1

nq for each s1

n,muM

nquN

pQν ps, aq and its outer product for the Gauss-Newton approxi-
Our method requires computing ∇a
mation. The computational complexity of the outer product operation is Opd2
aq and is inexpensive
when compared to the dimension of ν. For a linear-in-parameter model pQν ps, aq “ νJµps, aq,
the gradient can be efﬁciently computed for common choices of the basis function µ such as the
Gaussian function. For deep neural network models, the gradient can be computed by the automatic-
differentiation (Goodfellow et al., 2016) where its cost depends on the network architecture.

4 RELATED WORK

Besides the connections to DPG, our method is also related to existing methods as follows.

A similar optimization problem to Eq.(13) was considered by the model-free trajectory optimization
(MOTO) method (Akrour et al., 2016). Our method can be viewed as a non-trivial extension of
MOTO with two signiﬁcant novelties. First, MOTO learns a sequence of time-dependent log-linear
Gaussian policies πtpa|sq “ N pa|Bts`bt, Σtq, while our method learns a log-nonlinear Gaussian
policy. Second, MOTO learns a time-dependent critic given by pQtps, aq “ 1
2 aJCta ` aJDts `
aJct `ξtpsq and performs policy update with these functions. In contrast, our method learns a more
complex critic and performs Taylor’s approximation in each training step.

Besides MOTO, the optimization problem also resembles that of trust region policy optimization
(TRPO) (Schulman et al., 2015a). TRPO solves the following optimization problem:

max
θ1

Epπθ psq,πθ1 pa|sq

”
ı
pQps, aq

subject to Epπθ psq rKLpπθpa|sq||πθ1pa|sqqs ď (cid:15),

(33)

where pQps, aq may be replaced by an estimate of the advantage function (Schulman et al., 2015b).
There are two major differences between the two problems. First, TRPO optimizes the policy param-
eter while we optimize the guide actor. Second, TRPO solves the optimization problem by conjugate
gradient where the KL divergence is approximated by the Fisher information matrix, while we solve
the optimization problem in a closed form with a quadratic approximation of the critic.

Our method is also related to maximum-entropy RL (Ziebart et al., 2010; Azar et al., 2012; Haarnoja
et al., 2017; Nachum et al., 2017), which maximizes the expected cumulative reward with an addi-
Epπpsq rrpst, atq ` αHpπpat|stqqs, where α ą 0 is a trade-off parame-
tional entropy bonus:
ter. The optimal policy in maximum-entropy RL is the softmax policy given by

8
t“1

ř

π‹
MaxEntpa|sq “ exp

ˆ

Q‹

softps, aq ´ V ‹
α

softpsq

˙

ˆ

9 exp

˙

,

Q‹

softps, aq
α

(34)

9

Published as a conference paper at ICLR 2018

softps, aq and V ‹

where Q‹
tively (Haarnoja et al., 2017; Nachum et al., 2017). For a policy π, these are deﬁned as
softps, aq “ rps, aq ` γEpps1|s,aq

softpsq are the optimal soft action-value and state-value functions, respec-
“

(35)

Qπ

,

ż

V π
softpsq “ α log

exp

ˆ

Qπ

‰
V π
softps1q
˙
softps, aq
α

da.

(36)

The softmax policy and the soft state-value function in maximum-entropy RL closely resemble the
guide actor in Eq.(14) when η‹ “ 0 and the log-integral term in Eq.(15) when η “ 0, respectively,
except for the deﬁnition of action-value functions. To learn the optimal policy of maximum-entropy
RL, Haarnoja et al. (2017) proposed soft Q-learning which uses importance sampling to compute
the soft value functions and approximates the intractable policy using a separate policy function.
Our method largely differs from soft Q-learning since we use Taylor’s approximation to convert the
intractable integral into more convenient matrix-vector products.

The idea of ﬁrstly learning a non-parameterized policy and then later learning a parameterized policy
by supervised learning was considered previously in guided policy search (GPS) (Levine & Koltun,
2013). However, GPS learns the guide policy by trajectory optimization methods such as an iterative
linear-quadratic Gaussian regulator (Li & Todorov, 2004), which requires a model of the transition
function. In contrast, we learn the guide policy via the critic without learning the transition function.

5 EXPERIMENTAL RESULTS

We evaluate GAC on the OpenAI gym platform (Brockman et al., 2016) with the Mujoco Physics
simulator (Todorov et al., 2012). The actor and critic are neural networks with two hidden layers of
400 and 300 units, as described in Appendix C. We compare GAC-0 and GAC-1 against deep DPG
(DDPG) (Lillicrap et al., 2015), Q-learning with a normalized advantage function (Q-NAF) (Gu
et al., 2016), and TRPO (Schulman et al., 2015a;b). Figure 1 shows the learning performance on
9 continuous control tasks. Overall, both GAC-0 and GAC-1 perform comparably with existing
methods and they clearly outperform the other methods in Half-Cheetah.

The performance of GAC-0 and GAC-1 is comparable on these tasks, except on Humanoid where
GAC-1 learns faster. We expect GAC-0 to be more stable and reliable but easier to get stuck at
local optima. On the other hand, the randomness introduced by GAC-1 leads to high variance
approximation but this could help escape poor local optima. We conjecture GAC-S that uses S ą 1
samples for the averaged Taylor’s approximation should outperform both GAC-0 and GAC-1. While
this is computationally expensive, we can use parallel computation to reduce the computation time.

The expected returns of both GAC-0 and GAC-1 have high ﬂuctuations on the Hopper and Walker2D
tasks when compared to TRPO as can be seen in Figure 1g and Figure 1h. We observe that they can
learn good policies for these tasks in the middle of learning. However, the policies quickly diverge
to poor ones and then they are quickly improved to be good policies again. We believe that this
happens because the step-size F ´1psq “
of the guide actor in Eq. (22) can
be very large near local optima for Gauss-Newton approximation. That is, the gradients near local
pQps, aqJ
optima have small magnitude and this makes the approximation Hpsq “ ∇a
small as well. If η‹Σ´1 is also relatively small then the matrix F ´1psq can be very large. Thus,
under these conditions, GAC may use too large step sizes to compute the guide actor and this results
in high ﬂuctuations in performance. We expect that this scenario can be avoided by reducing the KL
bound (cid:15) or adding a regularization constant to the Gauss-Newton approximation.

˘
η‹Σ´1 ´ Hpsq

pQps, aq∇a

´1

`

Table 1 in Appendix C shows the wall-clock computation time. DDPG is computationally the most
efﬁcient method on all tasks. GAC has low computation costs on tasks with low dimensional actions
and its cost increases as the dimensionality of action increases. This high computation cost is due to
the dual optimization for ﬁnding the step-size parameters η and ω. We believe that the computation
cost of GAC can be signiﬁcantly reduced by letting η and ω be external tuning parameters.

6 CONCLUSION AND FUTURE WORK

Actor-critic methods are appealing for real-world problems due to their good data efﬁciency and
learning speed. However, existing actor-critic methods do not use second-order information of the

10

Published as a conference paper at ICLR 2018

(a) Inverted-Pend.

(b) Inv-Double-Pend.

(c) Reacher

(d) Swimmer

(e) Half-Cheetah

(f) Ant

(g) Hopper

(h) Walker2D

(i) Humanoid

Figure 1: Expected returns averaged over 10 trials. The x-axis indicates training time steps. The y-
axis indicates averaged return and higher is better. More clear ﬁgures are provided in Appendix C.2.

critic. In this paper, we established a novel framework that distinguishes itself from existing work
by utilizing Hessians of the critic for actor learning. Within this framework, we proposed a prac-
tical method that uses Gauss-Newton approximation instead of the Hessians. We showed through
experiments that our method is promising and thus the framework should be further investigated.

Our analysis showed that the proposed method is closely related to deterministic policy gradients
(DPG). However, DPG was also shown to be a limiting case of the stochastic policy gradients when
the policy variance approaches zero (Silver et al., 2014). It is currently unknown whether our frame-
work has a connection to the stochastic policy gradients as well, and ﬁnding such a connection is
our future work.

Our main goal in this paper was to provide a new actor-critic framework and we do not claim that our
method achieves the state-of-the-art performance. However, its performance can still be improved in
many directions. For instance, we may impose a KL constraint for a parameterized actor to improve
its stability, similarly to TRPO (Schulman et al., 2015a). We can also apply more efﬁcient policy
evaluation methods such as Retrace (Munos et al., 2016) to achieve better critic learning.

ACKNOWLEDGMENTS

MS was partially supported by KAKENHI 17H00757.

REFERENCES

Abbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, Nuno Lau, Lu´ıs Paulo Reis, and Gerhard Neu-
In Advances in Neural Information

mann. Model-Based Relative Entropy Stochastic Search.
Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki. Model-Free Trajec-
tory Optimization for Reinforcement Learning. In Proceedings of the 33nd International Confer-
ence on Machine Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Shun-ichi Amari. Natural Gradient Works Efﬁciently in Learning. Neural Computation, 10(2):

251–276, 1998.

Brandon Amos, Lei Xu, and J. Zico Kolter. Input Convex Neural Networks. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
August 6-11, 2017, Sydney, Australia, 2017.

Mohammad Gheshlaghi Azar, Vicenc¸ G´omez, and Hilbert J. Kappen. Dynamic Policy Program-

ming. Journal of Machine Learning Research, 13:3207–3245, 2012.

11

Published as a conference paper at ICLR 2018

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.

Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on Policy Search for Robotics.

Foundations and Trends in Robotics, 2(1-2):1–142, 2013.

Thomas Furmston, Guy Lever, and David Barber. Approximate Newton Methods for Policy Search
in Markov Decision Processes. Journal of Machine Learning Research, 17(227):1–51, 2016.

Xavier Glorot and Yoshua Bengio. Understanding the Difﬁculty of Training Deep Feedforward
Neural Networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the 13th In-
ternational Conference on Artiﬁcial Intelligence and Statistics, May 13-15, 2010, Sardinia, Italy,
2010.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous Deep Q-Learning
with Model-based Acceleration. In Proceedings of the 33nd International Conference on Machine
Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with
Deep Energy-Based Policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, August 6-11, 2017, Sydney, Australia, 2017.

Nicolas Heess, Gregory Wayne, David Silver, Timothy P. Lillicrap, Tom Erez, and Yuval Tassa.
In Advances in Neural

Learning Continuous Control Policies by Stochastic Value Gradients.
Information Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Sham Kakade. A Natural Policy Gradient. In Advances in Neural Information Processing Systems

14, December 3-8, 2001, Vancouver, British Columbia, Canada, 2001.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. CoRR,

abs/1412.6980, 2014.

Vijay R. Konda and John N. Tsitsiklis. On Actor-Critic Algorithms. SIAM Journal on Control and

Optimization, 42(4):1143–1166, April 2003.

Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the 30th International

Conference on Machine Learning, June 16-21, 2013, Atlanta, GA, USA, 2013.

Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep
Visuomotor Policies. Journal of Machine Learning Research, 17(1):1334–1373, January 2016.
ISSN 1532-4435.

Weiwei Li and Emanuel Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear Bio-
logical Movement Systems. In Proceedings of the 1st International Conference on Informatics in
Control, Automation and Robotics, August 25-28, 2004, Set´ubal, Portugal, pp. 222–229, 2004.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. CoRR,
abs/1509.02971, 2015.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement
Learning. Nature, 518(7540):529–533, February 2015. ISSN 00280836.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement
In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
Learning.
International Conference on Machine Learning, June 19-24, 2016, New York City, NY, USA,
2016.

12

Published as a conference paper at ICLR 2018

R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and Efﬁcient Off-
Policy Reinforcement Learning. In Advances in Neural Information Processing Systems 29, De-
cember 5-10, 2016, Barcelona, Spain, 2016.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the Gap Between
Value and Policy Based Reinforcement Learning. In Advances in Neural Information Processing
Systems 30, 4-9 December 2017, Long Beach, CA, USA, 2017.

Jorge Nocedal and Stephen J. Wright. Numerical Optimization, Second Edition. World Scientiﬁc,

2006.

Jan Peters and Stefan Schaal. Natural Actor-Critic. Neurocomputing, 71(7-9):1180–1190, 2008.

Jan Peters, Katharina M¨ulling, and Yasemin Altun. Relative Entropy Policy Search. In Proceedings
of the 24th AAAI Conference on Artiﬁcial Intelligence, July 11-15, 2010, Atlanta, Georgia, USA,
2010.

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region
Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning,
July 6-11, 2015, Lille, France, pp. 1889–1897, 2015a.

John Schulman, Philipp Moritz, Sergey Levine, Michael

High-Dimensional Continuous Control Using Generalized Advantage Estimation.
abs/1506.02438, 2015b.

I. Jordan, and Pieter Abbeel.
CoRR,

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic Policy Gradient Algorithms. In Proceedings of the 31st International Conference
on Machine Learning, June 21-26, 2014, Beijing, China, 2014.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game
of Go with Deep Neural Networks and Tree Search. Nature, 529(7587):484–489, 2016.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning - an Introduction. Adaptive

computation and machine learning. MIT Press, 1998.

Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy Gradi-
ent Methods for Reinforcement Learning with Function Approximation. In Advances in Neural
Information Processing Systems 12, November 29 - December 4, 1999, Colorado, USA, 1999.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics Engine for Model-Based Control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012,
Vilamoura, Algarve, Portugal, 2012.

Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforce-

ment Learning. Machine Learning, 8(3):229–256, 1992. doi: 10.1007/BF00992696.

Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable Trust-Region
Method for Deep Reinforcement Learning using Kronecker-factored Approximation. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, 4-9 December 2017, Long Beach, CA, USA,
2017.

Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling Interaction via the Principle
of Maximum Causal Entropy. In Proceedings of the 27th International Conference on Machine
Learning, June 21-24, 2010, Haifa, Israel, 2010.

13

Published as a conference paper at ICLR 2018

A DERIVATIONS AND PROOFS

A.1 DERIVATION OF SOLUTION AND DUAL FUNCTION OF GUIDE ACTOR

The solution of the optimization problem:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

can be obtained by the method of Lagrange multipliers. The derivation here follows the derivation of
similar optimization problems by Peters et al. (2010) and Abdolmaleki et al. (2015). The Lagrangian
of this optimization problem is

Lprπ, η, ω, νq “ Epβ psq,˜πpa|sq

` ηp(cid:15) ´ Epβ psq rKLp˜πpa|sq||πθpa|sqqsq

ı
”
pQps, aq

` ωpEpβ psq rHp˜πpa|sqqs ´ κq ` νpEpβ psqrπpa|sq ´ 1q,

(38)

where η, ω, and ν are the dual variables. Then, by taking derivative of L w.r.t. rπ we obtain

„ż ´

¯



BrπL “ Epβ psq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq

da

´ pη ` ω ´ νq.

(39)

We set this derivation to zero in order to obtain

0 “ Epβ psq

„ż ´

¯
pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq



da

´ pη ` ω ´ νq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq ´ pη ` ω ´ νq.

“

Then the solution is given by

rπpa|sq “ πθpa|sq

η
η`ω exp

9 πθpa|sq

η
η`ω exp

¸

ˆ

exp

´

˙

η ` ω ´ ν
η ` ω

˜

˜

pQps, aq
η ` ω
pQps, aq
η ` ω

¸

.

(37)

(40)

(41)

(42)

To obtain the dual function gpη, ωq, we substitute the solution to the constraint terms of the La-
grangian and this gives us

Lpη, ω, νq “ Epβ psq,rπpa|sq

ı

”
pQps, aq

«

´ pη ` ωqEpβ psq,rπpa|sq

pQps, aq
η ` ω

`
`

η
log πθpa|sq ´
η ` ω
˘
Epβ ,rπpa|sq ´ 1

ﬀ

η ` ω ´ ν
η ` ω

` ηEpβ psq,rπpa|sq rlog πθpa|sqs ` ν

` η(cid:15) ´ ωκ.

(43)

After some calculation, we obtain

Lpη, ω, νq “ η(cid:15) ´ ωκ ` Epβ psq rη ` ω ´ νs
«ż

“ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

η

πθpa|sq

η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

“ gpη, ωq,

(44)

where in the second line we use the fact that expp´ η`ω´ν

η`ω q is the normalization term of rπpa|sq.

14

Published as a conference paper at ICLR 2018

A.2 PROOF OF SECOND-ORDER OPTIMIZATION IN ACTION SPACE

Firstly, we show that GAC performs second-order optimization in the action space when Taylor’s
approximation is performed with a0 “ Eπpa|sq ras “ φθpsq. Recall that Taylor’s approximation
with φθ is given by

1
2

pQps, aq “
pQps, aq|a“φθ psq ´ H φθ psqφθpsq. By substituting ψφθ

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq,

(45)

psq into Lpsq “

where ψφθ
η‹Σ´1

psq “ ∇a

θ psqφθpsq ´ ψθpsq, we obtain

Lpsq “ η‹Σ´1
“ pη‹Σ´1

θ psqφθpsq ` ∇a
θ psq ´ H φθ psqqφθpsq ` ∇a
pQps, aq|a“φθ psq.

pQps, aq|a“φθ psq ´ H φθ psqφθpsq

pQps, aq|a“φθ psq

“ F psqφθpsq ` ∇a
Therefore, the mean update is equivalent to

φ`psq “ F ´1psqLpsq

“ φθpsq ` F ´1psq∇a

pQps, aq|a“φθ psq,

which is a second-order optimization step with a curvature matrix F psq “ η‹Σ´1

θ psq ´ H φθ psq.

Lpsq “ η‹Σ´1

Similarly, for the case where a set of samples ta0u „ πθpa0|sq “ N pa0|φθpsq, Σpsqq is used to
compute the averaged Taylor’s approximation, we obtain
θ psqφθpsq ` Eπθ pa0|sq

”
∇a
Then, by assuming that Eπθ pa0|sq rH 0psqa0psqs “ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s, we obtain
”
∇a
”
∇a

pQps, aq|a“a0
pQps, aq|a“a0

´ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s

´ Eπθ pa0|sq rH 0psqa0psqs .

θ psqφθpsq ` Eπθ pa0|sq

θ psqφθpsq ` Eπθ pa0|sq

pQps, aq|a“a0

Lpsq “ η‹Σ´1

“ η‹Σ´1

(48)

ı

ı

ı

´ Eπθ pa0|sq rH 0s φθpsq
ı

”
∇a

pQps, aq|a“a0

“ pη‹Σ´1

θ psq ´ Eπθ pa0|sq rH 0psqsqφθpsq ` Eπθ pa0|sq

“ F psqφθpsq ` Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

.

Therefore, we have a second-order optimization step

φ`psq “ φθpsq ` F ´1psqEπθ pa0|sq

(50)
where F ´1psq “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs is a curvature matrix. As described in
the main paper, this interpretation is only valid when the equality Eπθ pa0|sq rH 0psqa0psqs “
Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s holds. While this equality does not hold in general, it holds when
only one sample a0 „ πθpa0|sq is used. Nonetheless, we can still use the expectation of Taylor’s
approximation to perform policy update regardless of this assumption.

,

pQps, aq|a“a0

”
∇a

ı

(46)

(47)

(49)

A.3 PROOF OF GAUSS-NEWTON APPROXIMATION

Let f ps, aq “ expp

pQps, aqq, then the Hessian Hpsq “ ∇2

pQps, aq can be expressed as

a

Hpsq “ ∇a r∇a log f ps, aqs
“
∇af ps, aqf ps, aq´1
∇af ps, aq´1

“ ∇a

`

‰

˘

J

“ ∇af ps, aq
“ ∇af ps, aq∇f f ps, aq´1 p∇af ps, aqqJ ` ∇2
“ ´∇af ps, aqf ps, aq´2 p∇af ps, aqqJ ` ∇2

` ∇2

af ps, aqf ps, aq´1

af ps, aqf ps, aq´1
af ps, aqf ps, aq´1

`

˘ `

∇af ps, aqf ps, aq´1

“ ´
“ ´∇a log f ps, aq∇a log f ps, aqJ ` ∇2
a expp
“ ´∇a

pQps, aqJ ` ∇2

pQps, aq∇a

` ∇2
af ps, aqf ps, aq´1
pQps, aqq expp´

∇af ps, aqf ps, aq´1

pQps, aqq,

˘

J

af ps, aqf ps, aq´1

(51)

15

Published as a conference paper at ICLR 2018

which concludes the proof.

Beside Gauss-Newton approximation, an alternative approach is to impose a special structure on
pQps, aq so that Hessians are always negative semi-deﬁnite. In literature, there exists two special
structures that satisﬁes this requirement.

Normalized advantage function (NAF) (Gu et al., 2016): NAF represents the critic by a quadratic
function with a negative curvature:

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(52)

where a negative-deﬁnite matrix-valued function W psq, a vector-valued function bpsq and a scalar-
valued function V psq are parameterized functions whose their parameters are learned by policy
evaluation methods such as Q-learning (Sutton & Barto, 1998). With NAF, negative deﬁnite Hes-
sians can be simply obtained as Hpsq “ W psq. However, a signiﬁcant disadvantage of NAF is that
it assumes the action-value function is quadratic regardless of states and this is generally not true for
most reward functions. Moreover, the Hessians become action-independent even though the critic is
a function of actions.

Input convex neural networks (ICNNs) (Amos et al., 2017): ICNNs are neural networks with
special structures which make them convex w.r.t. their inputs. Since Hessians of concave functions
are always negative semi-deﬁnite, we may use ICNNs to represent a negative critic and directly use
its Hessians. However, similarly to NAF, ICNNs implicitly assume that the action-value function is
concave w.r.t. actions regardless of states and this is generally not true for most reward functions.

A.4 GRADIENT OF THE LOSS FUNCTIONS FOR LEARNING PARAMETERIZED ACTOR

We ﬁrst consider
is
N pa|φ`psq, Σ`psqq and the current actor is N pa|φθpsq, Σθpsqq. Taylor’s approximation of
pQps, aq at a0 “ φθpsq is

loss function where the guide actor

the weight mean-squared-error

pQps, aq “

1
2

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq.

(53)

By assuming that H φθ psq is strictly negative deﬁnite5, we can take a derivative of this approxima-
tion w.r.t. a and set it to zero to obtain a “ H ´1
psq. Replacing a by
φθ
φθpsq and φ`psq yields

pQps, aq ´ H ´1
φθ

psqψφθ

psq∇a

φθpsq “ H ´1
φθ
φ`psq “ H ´1
φθ

psq∇a

psq∇a

pQps, aq|a“φθ psq ´ H ´1
pQps, aq|a“φ`psq ´ H ´1

φθ

φθ

psqψφθ
psqψφθ

psq,

psq.

Recall that the weight mean-squared-error is deﬁned as

LWpθq “ Epβ psq

”
}φθpsq ´ φ`psq}2

F psq

ı

.

(54)

(55)

(56)

5This can be done by subtracting a small positive value to the diagonal entries of Gauss-Newton approxi-

mation.

16

Published as a conference paper at ICLR 2018

Then, we consider its gradient w.r.t. θ as follows:

∇θLWpθq “ 2Epβ psq
“ 2Epβ psq
“ 2Epβ psq

“

“

“

`

∇θφθpsqF psq
∇θφθpsqpη‹Σ´1
∇θφθpsqpη‹Σ´1
´
H ´1
φθ

ˆ

˘‰

`

φθpsq ´ φ`psq
θ psq ´ H φθ psqq
θ psq ´ H φθ psqq

˘‰

φθpsq ´ φ`psq

¯ı

psq∇a

pQps, aq|a“φθ psq ´ H ´1

φθ

psq∇a

pQps, aq|a“φ`psq

“ 2η‹Epβ psq

´

psq

”
θ psqH ´1
∇θφθpsqΣ´1
φθ
´
”
∇θφθpsq

` 2Epβ psq
”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

∇a
pQps, aq|a“φθ psq

pQps, aq|a“φθ psq ´ ∇a
pQps, aq|a“φθ psq
”
∇θφθpsq∇a
ı

∇a
pQps, aq|a“φ`psq ´ ∇a
ı
` 2Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

psq∇a

psq∇a

ı

.

` 2η‹Epβ psq

´ 2η‹Epβ psq

“ ´2Epβ psq

pQps, aq|a“φ`psq
¯ı

pQps, aq|a“φ`psq

¯ı

ı

(57)

This concludes the proof for the gradient in Eq.(28). Note that we should not directly replace
the mean functions in the weight mean-square-error by Eq.(54) and Eq.(55) before expanding the
gradient. This is because the analysis would require computing the gradient w.r.t. θ inside the
Hessians and this is not trivial. Moreover, when we perform gradient descent in practice, the mean
φ`psq is considered as a constant w.r.t. θ similarly to an output function in supervised learning.
The gradient for the mean-squared-error can be derived similarly. Let the mean-squared-error be
deﬁned as

LMpθq “

Epβ psq

}φθpsq ´ φ`psq}2
2

.

“

‰

1
2

(58)

Its gradient w.r.t. θ is given by

“

`

˘‰

∇θLMpθq “ Epβ psq
“ Epβ psq

∇θφθpsq
”
∇θφθpsqH ´1psq

φθpsq ´ φ`psq
´

∇a

which concludes the proof for the gradient in Eq.(30).

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

,

(59)

¯ı

To show that ∇a
˘
`
ηΣ´1psq ´ H φθ psq

´1

´
η‹Σ´1psqφθpsq ` ψφθ

pQps, aq|a“φ`psq “ 0 when η‹ “ 0, we directly substitute η‹ “ 0 into φ`psq “
¯

psqq

and this yield

φ`psq “ ´H ´1
φθ
pQps, aq|a“φ`psq ´ H ´1
Since φ`psq “ H ´1
psq∇a
psq from Eq.(55) and the Hessians are
φθ
pQps, aq|a“φ`psq “ 0. This is intuitive since without the KL constraint,
non-zero, it has to be that ∇a
the mean of the guide actor always be at the optima of the second-order Taylor’s approximation and
the gradients are zero at the optima.

psqψφθ

psqψφθ

psq.

(60)

φθ

A.5 RELATION TO Q-LEARNING WITH NORMALIZED ADVANTAGE FUNCTION

The normalized advantage function (NAF) (Gu et al., 2016) is deﬁned as

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(61)

where W psq is a negative deﬁnite matrix. Gu et al. (2016) proposed to perform Q-learning with
NAF by using the fact that argmaxa
Here, we show that GAC includes the Q-learning method by Gu et al. (2016) as its special case. This
can be shown by using NAF as a critic instead of performing Taylor’s approximation of the critic.

pQNAFps, aq “ bpsq and maxa

pQNAFps, aq “ V psq.

17

Published as a conference paper at ICLR 2018

pQNAFps, aq “

Firstly, we expand the quadratic term of NAF as follows:
1
2
1
2
1
2

aJW psqa ´ aJW psqbpsq `

aJW psqa ` aJψpsq ` ξpsq,

“

“

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq

bpsqJW psqbpsq ` V psq

(62)

where ψpsq “ ´W psqbpsq and ξpsq “ 1
2 bpsqJW psqbpsq ` V psq. By substituting the quadratic
model obtained by NAF into the GAC framework, the guide actor is now given by ˜πpa|sq “
N pa|φ`psq, Σ`psqqq with

φ`psq “ pη‹Σ´1psqq ´ W psqq´1pη‹Σ´1psqφθpsq ´ W psqbpsqq
Σ`psq “ pη‹ ` ω‹qpη‹Σ´1psqq ´ W psqq.

(64)
To obtain Q-learning with NAF, we set η‹ “ 0, i.e., we perform a greedy maximization where the
KL upper-bound approaches inﬁnity, and this yields

(63)

φ`psq “ ´W psq´1p´W psqbpsqq

(65)
which is the policy obtained by performing Q-learning with NAF. Thus, NAF with Q-learning is a
special case of GAC if Q-learning is also used in GAC to learn the critic.

“ bpsq,

The pseudo-code of GAC is given in Algorithm 1. The source code is available at https://
github.com/voot-t/guide-actor-critic.

B PSEUDO-CODE OF GAC

C EXPERIMENT DETAILS

C.1

IMPLEMENTATION

We try to follow the network architecture proposed by the authors of each baseline method as close as
possible. For GAC and DDPG, we use neural networks with two hidden layers for the actor network
and the critic network. For both networks the ﬁrst layer has 400 hidden units and the second layer
has 300 units. For NAF, we use neural networks with two hidden layers to represent each of the
functions bpsq, W psq and V psq where each layer has 200 hidden units. All hidden units use the
relu activation function except for the output of the actor network where we use the tanh activation
function to bound actions. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate
0.001 and 0.0001 for the critic network and the actor network, respectively. The moving average
step for target networks is set to τ “ 0.001. The maximum size of the replay buffer is set to
1000000. The mini-batches size is set to N “ 256. The weights of the actor and critic networks
are initialized as described by Glorot & Bengio (2010), except for the output layers where the initial
weights are drawn uniformly from Up´0.003, 0.003q, as described by Lillicrap et al. (2015). The
initial covariance Σ in GAC is set to be an identity matrix. DDPG and QNAF use the OU-process
with noise parameters θ “ 0.15 and σ “ 0.2 for exploration .

For TRPO, we use the implementation publicly available at https://github.com/openai/
baselines. We also use the provided network architecture and hyper-parameters except the batch
size where we use 1000 instead of 1024 since this is more suitable in our test setup.

For GAC, the KL upper-bound is ﬁxed to (cid:15) “ 0.0001. The entropy lower-bound κ is adjusted
heuristically by

κ “ maxp0.99pE ´ E0q ` E0, E0q,
(71)
where E « Epβ psq rHpπθpa|sqqs denotes the expected entropy of the current policy and E0 denotes
the entropy of a base policy N pa|0, 0.01Iq. This heuristic ensures that the lower-bound gradually
decreases but the lower-bound cannot be too small. We apply this heuristic update once every 5000
training steps. The dual function is minimize by the sequential least-squares quadratic programming
(SLSQP) method with an initial values η “ 0.05 and ω “ 0.05. The number of samples for
computing the target critic value is M “ 10.

18

Published as a conference paper at ICLR 2018

Algorithm 1 Guide actor critic

1: Input:

Initial actor πθpa|sq “ N pa|φθpsq, Σq, critic pQν ps, aq, target critic network

pQ¯ν ps, aq, KL bound (cid:15), entropy bound κ, learning rates α1, α2, and data buffer D “ H.

procedure COLLECT TRANSITION SAMPLE

Observe state st and sample action at „ N pa|φθpstq, Σq.
Execute at, receive reward rt and next state s1
t.
Add transition tpst, at, rt, s1

tqu to D.

2: for t “ 1, . . . , Tmax do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

end procedure
procedure LEARN

Sample N mini-batch samples tpsn, an, rn, s1
procedure UPDATE CRITIC
n,muM
Sample actions ta1
Compute yn, update ν by, e.g., Adam, and update ¯ν by moving average:

m“1 „ N pa|φθpsnq, Σq for each sn.

n“1 uniformly from D.

nquN

yn “ rn ` γ

pQ¯ν ps1

n, a1

n,mq,

ν Ð ν ´ α1

´
pQν psn, anq ´ yn

¯

2

,

∇ν

1
M

1
N

Mÿ

m“1
Nÿ

n“1

¯ν Ð τ ν ` p1 ´ τ q¯ν.

end procedure
procedure LEARN GUIDE ACTOR

Compute an,0 for each sn by an,0 “ φθpsnq or an,0 „ N pa|φθpsnq, Σq.
pQpsn, aq|a“an,0 and H 0pssq “ ´g0psnqg0psnqJ.
Compute g0psq “ ∇a
Solve for pη‹, ω‹q “ argminηą0,ωą0
Compute the guide actor rπpa|snq “ N pa|φ`psnq, Σ`psnqq for each sn.

pgpη, ωq by a non-linear optimization method.

end procedure
procedure UPDATE PARAMETERIZED ACTOR

Update policy parameter by, e.g., Adam, to minimize the MSE:

13:
14:
15:

16:
17:
18:
19:
20:
21:

θ Ð θ ´ α2

∇θ}φθpsnq ´ φ`psnq}2
2.

1
N

Nÿ

n“1

22:

Update policy covariance by averaging the guide covariances:

Σ Ð

Σ`psnq.

1
N

(66)

(67)

(68)

(69)

(70)

end procedure

end procedure

23:
24:
25: end for
26: Output: Learned actor πθpa|sq.

C.2 ENVIRONMENTS AND RESULTS

We perform experiments on the OpenAI gym platform (Brockman et al., 2016) with Mujoco Physics
simulator (Todorov et al., 2012) where all environments are v1. We use the state space, action space
and the reward function as provided and did not perform any normalization or gradient clipping.
The maximum time horizon in each episode is set to 1000. The discount factor γ “ 0.99 is only
used for learning and the test returns are computed without it.

Experiments are repeated for 10 times with different random seeds. The total computation time are
reported in Table 1. The ﬁgures below show the results averaged over 10 trials. The y-axis indicates
the averaged test returns where the test returns in each trial are computed once every 5000 training
time steps by executing 10 test episodes without exploration. The error bar indicates standard error.

19

Published as a conference paper at ICLR 2018

Table 1: The total computation time for training the policy for 1 million steps (0.1 million steps for
the Invert-Pendulum task). The mean and standard error are computed over 10 trials with the unit in
hours. TRPO is not included since it performs a lesser amount of update using batch data samples.

Task
Inv-Pend.
Inv-Double-Pend.
Reacher
Swimmer
Half-Cheetah
Ant
Hopper
Walker2D
Humanoid

GAC-1
1.13(0.09)
15.56(0.93)
17.23(0.87)
12.43(0.74)
29.82(1.76)
37.84(1.80)
18.99(1.06)
33.42(0.96)
111.07(2.99)

GAC-0
0.80(0.04)
15.67(0.77)
12.64(0.38)
11.94(0.74)
32.64(1.41)
40.57(3.06)
14.22(0.56)
31.71(1.84)
106.80(6.80)

DDPG
0.45(0.02)
9.04(0.29)
10.67(0.37)
9.61(0.52)
10.13(0.41)
9.75(0.37)
8.74(0.45)
7.92(0.11)
13.90(1.60)

QNAF
0.40(0.03)
7.47(0.22)
30.91(2.09)
32.44(2.21)
27.94(2.37)
27.09(0.94)
26.48(1.42)
26.94(1.99)
30.43(2.21)

Figure 2: Performance averaged over 10 trials on the Inverted Pendulum task.

20

Published as a conference paper at ICLR 2018

Figure 3: Performance averaged over 10 trials on the Inverted Double Pendulum task.

Figure 4: Performance averaged over 10 trials on the Reacher task.

21

Published as a conference paper at ICLR 2018

Figure 5: Performance averaged over 10 trials on the Swimmer task.

Figure 6: Performance averaged over 10 trials on the Half-Cheetah task.

22

Published as a conference paper at ICLR 2018

Figure 7: Performance averaged over 10 trials on the Ant task.

Figure 8: Performance averaged over 10 trials on the Hopper task.

23

Published as a conference paper at ICLR 2018

Figure 9: Performance averaged over 10 trials on the Walker2D task.

Figure 10: Performance averaged over 10 trials on the Humanoid task.

24

8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
6
7
0
.
5
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

GUIDE ACTOR-CRITIC FOR CONTINUOUS CONTROL

Voot Tangkaratt
RIKEN AIP, Tokyo, Japan
voot.tangkaratt@riken.jp

Abbas Abdolmaleki
The University of Aveiro, Aveiro, Portugal
abbas.a@ua.pt

Masashi Sugiyama
RIKEN AIP, Tokyo, Japan
The University of Tokyo, Tokyo, Japan
masashi.sugiyama@riken.jp

ABSTRACT

Actor-critic methods solve reinforcement learning problems by updating a param-
eterized policy known as an actor in a direction that increases an estimate of the
expected return known as a critic. However, existing actor-critic methods only
use values or gradients of the critic to update the policy parameter. In this pa-
per, we propose a novel actor-critic method called the guide actor-critic (GAC).
GAC ﬁrstly learns a guide actor that locally maximizes the critic and then it up-
dates the policy parameter based on the guide actor by supervised learning. Our
main theoretical contributions are two folds. First, we show that GAC updates the
guide actor by performing second-order optimization in the action space where
the curvature matrix is based on the Hessians of the critic. Second, we show that
the deterministic policy gradient method is a special case of GAC when the Hes-
sians are ignored. Through experiments, we show that our method is a promising
reinforcement learning method for continuous controls.

1

INTRODUCTION

The goal of reinforcement learning (RL) is to learn an optimal policy that lets an agent achieve the
maximum cumulative rewards known as the return (Sutton & Barto, 1998). Reinforcement learning
has been shown to be effective in solving challenging artiﬁcial intelligence tasks such as playing
games (Mnih et al., 2015; Silver et al., 2016) and controlling robots (Deisenroth et al., 2013; Levine
et al., 2016).

Reinforcement learning methods can be classiﬁed into three categories: value-based, policy-based,
and actor-critic methods. Value-based methods learn an optimal policy by ﬁrstly learning a value
function that estimates the expected return. Then, they infer an optimal policy by choosing an
action that maximizes the learned value function. Choosing an action in this way requires solving a
maximization problem which is not trivial for continuous controls. While extensions to continuous
controls were considered recently, they are restrictive since speciﬁc structures of the value function
are assumed (Gu et al., 2016; Amos et al., 2017).

On the other hand, policy-based methods, also called policy search methods (Deisenroth et al.,
2013), learn a parameterized policy maximizing a sample approximation of the expected re-
turn without learning the value function. For instance, policy gradient methods such as REIN-
FORCE (Williams, 1992) use gradient ascent to update the policy parameter so that the probability
of observing high sample returns increases. Compared with value-based methods, policy search
methods are simpler and naturally applicable to continuous problems. Moreover, the sample return
is an unbiased estimator of the expected return and methods such as policy gradients are guaranteed
to converge to a locally optimal policy under standard regularity conditions (Sutton et al., 1999).
However, sample returns usually have high variance and this makes such policy search methods
converge too slowly.

Actor-critic methods combine the advantages of value-based and policy search methods. In these
methods, the parameterized policy is called an actor and the learned value-function is called a critic.

1

Published as a conference paper at ICLR 2018

The goal of these methods is to learn an actor that maximizes the critic. Since the critic is a low vari-
ance estimator of the expected return, these methods often converge much faster than policy search
methods. Prominent examples of these methods are actor-critic (Sutton et al., 1999; Konda & Tsit-
siklis, 2003), natural actor-critic (Peters & Schaal, 2008), trust-region policy optimization (Schul-
man et al., 2015a), and asynchronous advantage actor-critic (Mnih et al., 2016). While their ap-
proaches to learn the actor are different, they share a common property that they only use the value
of the critic, i.e., the zero-th order information, and ignore higher-order ones such as gradients and
Hessians w.r.t. actions of the critic1. To the best of our knowledge, the only actor-critic methods
that use gradients of the critic to update the actor are deterministic policy gradients (DPG) (Silver
et al., 2014) and stochastic value gradients (Heess et al., 2015). However, these two methods do not
utilize the second-order information of the critic.

In this paper, we argue that the second-order information of the critic is useful and should not be
ignored. A motivating example can be seen by comparing gradient ascent to the Newton method: the
Newton method which also uses the Hessian converges to a local optimum in a fewer iterations when
compared to gradient ascent which only uses the gradient (Nocedal & Wright, 2006). This suggests
that the Hessian of the critic can accelerate actor learning which leads to higher data efﬁciency.
However, the computational complexity of second-order methods is at least quadratic in terms of
the number of optimization variables. For this reason, applying second-order methods to optimize
the parameterized actor directly is prohibitively expensive and impractical for deep reinforcement
learning which represents the actor by deep neural networks.

Our contribution in this paper is a novel actor-critic method for continuous controls which we call
guide actor-critic (GAC). Unlike existing methods, the actor update of GAC utilizes the second-
order information of the critic in a computationally efﬁcient manner. This is achieved by separating
actor learning into two steps. In the ﬁrst step, we learn a non-parameterized Gaussian actor that
locally maximizes the critic under a Kullback-Leibler (KL) divergence constraint. Then, the Gaus-
sian actor is used as a guide for learning a parameterized actor by supervised learning. Our analysis
shows that learning the mean of the Gaussian actor is equivalent to performing a second-order update
in the action space where the curvature matrix is given by Hessians of the critic and the step-size is
controlled by the KL constraint. Furthermore, we establish a connection between GAC and DPG
where we show that DPG is a special case of GAC when the Hessians and KL constraint are ignored.

2 BACKGROUND

In this section, we ﬁrstly give a background of reinforcement learning. Then, we discuss existing
second-order methods for policy learning and their issue in deep reinforcement learning.

2.1 REINFORCEMENT LEARNING

We consider discrete-time Markov decision processes (MDPs) with continuous state space S Ď Rds
and continuous action space A Ď Rda . We denote the state and action at time step t P N by
st and at, respectively. The initial state s1 is determined by the initial state density s1 „ ppsq.
At time step t, the agent in state st takes an action at according to a policy at „ πpa|stq and
obtains a reward rt “ rpst, atq. Then, the next state st`1 is determined by the transition function
st`1 „ pps1|st, atq. A trajectory τ “ ps1, a1, r1, s2, . . . q gives us the cumulative rewards or return
t“1 γt´1rpst, atq, where the discount factor 0 ă γ ă 1 assigns different weights to
deﬁned as
rewards given at different time steps. The expected return of π after executing an action a in a state
s can be expressed through the action-value function which is deﬁned as

ř

8

ﬀ

Qπps, aq “ Eπpat|stqtě2,ppst`1|st,atqtě1

γt´1rpst, atq|s1 “ s, a1 “ a

,

(1)

«

8ÿ

t“1

where Ep r¨s denotes the expectation over the density p and the subscript t ě 1 indicates that the
expectation is taken over the densities at time steps t ě 1. We can deﬁne the expected return as
8ÿ

«

ﬀ

J pπq “ Epps1q,πpat|stqtě1,ppst`1|st,atqtě1

γt´1rpst, atq

“ Eppsq,πpa|sq rQπps, aqs .

(2)

1This is different from using the gradient of the critic w.r.t. critic parameters to update the critic itself.

t“1

2

Published as a conference paper at ICLR 2018

The goal of reinforcement learning is to ﬁnd an optimal policy that maximizes the expected return.
The policy search approach (Deisenroth et al., 2013) parameterizes π by a parameter θ P Rdθ and
ﬁnds θ‹ which maximizes the expected return:

θ‹ “ argmax

Eppsq,πθ pa|sq rQπθ ps, aqs .

θ

(3)

(4)

(5)

(7)

Policy gradient methods such as REINFORCE (Williams, 1992) solve this optimization problem by
gradient ascent:

θ Ð θ ` αEppsq,πθ pa|sq r∇θ log πθpa|sqQπθ ps, aqs ,

ř

where α ą 0 is a step-size. In policy search, the action-value function is commonly estimated by the
sample return: Qπθ ps, aq « 1
n,t γt´1rpst,n, at,nq obtained by collecting N trajectories using
N
πθ. The sample return is an unbiased estimator of the action-value function. However, it often has
high variance which leads to slow convergence.
An alternative approach is to estimate the action-value function by a critic denoted by pQps, aq
whose parameter is learned such that pQps, aq « Qπθ ps, aq. By replacing the action-value function
in Eq.(3) with the critic, we obtain the following optimization problem:

θ‹ “ argmax

Eppsq,πθ pa|sq

θ

ı

”
pQps, aq

.

The actor-critic method (Sutton et al., 1999) solves this optimization problem by gradient ascent:
”
∇θ log πθpa|sq

θ Ð θ ` αEppsq,πθ pa|sq

pQps, aq

ı

.

(6)

The gradient in Eq.(6) often has less variance than that in Eq.(4), which leads to faster convergence2.
A large class of actor-critic methods is based on this method (Peters & Schaal, 2008; Mnih et al.,
2016). As shown in these papers, these methods only use the value of the critic to learn the actor.

The deterministic policy gradients (DPG) method (Silver et al., 2014) is an actor-critic method that
uses the ﬁrst-order information of the critic. DPG updates a deterministic actor πθpsq by

θ Ð θ ` αEppsq

”
∇θπθpsq∇a

ı

pQps, aq|a“πθ psq

.

A method related to DPG is the stochastic value gradients (SVG) method (Heess et al., 2015) that
is able to learn a stochastic policy but it requires learning a model of the transition function.

The actor-critic methods described above only use up to the ﬁrst-order information of the critic when
learning the actor and ignore higher-order ones. Below, we discuss existing approaches that utilize
the second-order information by applying second-order optimization methods to solve Eq.(5).

2.2 SECOND-ORDER METHODS FOR POLICY LEARNING

The actor-critic methods described above are ﬁrst-order methods which update the optimization
variables based on the gradient of the objective function. First-order methods are popular in deep
learning thanks to their computational efﬁciency. However, it is known in machine learning that
second-order methods often lead to faster learning because they use the curvature information to
compute a better update direction, i.e., the steepest ascent direction along the curvature3.

The main idea of second-order methods is to rotate the gradient by the inverse of a curvature matrix.
For instance, second-order updates for the actor-critic method in Eq.(6) are in the following form:

θ Ð θ ` αG´1

!
Eppsq,πθ pa|sq

”
∇θ log πθpa|sq

ı)

pQps, aq

,

(8)

2This gradient is a biased estimator of the policy gradient in Eq.(4). However, it is unbiased under some

regularity conditions such as compatible function conditions (Sutton et al., 1999).

3We use the term “second-order methods” in a broad sense here, including quasi-Newton and natural gradi-

ent methods which approximate the curvature matrix by the ﬁrst-order information.

3

Published as a conference paper at ICLR 2018

where G P Rdθ ˆdθ is a curvature matrix. The behavior of second-order methods depend on the
deﬁnition of a curvature matrix. The most well-known second-order method is the Newton method
where its curvature matrix is the Hessian of the objective function w.r.t. the optimization variables:
∇θ log πθpa|sq∇θ log πθpa|sqJ ` ∇2

GHessian “ Eppsq,πθ pa|sq

θ log πθpa|sq

pQps, aq

”`

(9)

ı

˘

.

“

∇θ log πθpa|sq∇θ log πθpa|sqJ

The natural gradient method is another well-known second-order method which uses the Fisher
information matrix (FIM) as the curvature matrix (Amari, 1998):
GFIM “ Eppsq,πθ pa|sq

(10)
Unlike the Hessian matrix, FIM provides information about changes of the policy measured by an
approximated KL divergence: Eppsq rKLpπθpa|sq||πθ1pa|sqqs « pθ ´ θ1qJGFIMpθ ´ θ1q (Kakade,
2001). We can see that GHessian and GFIM are very similar but the former also contains the critic
and the Hessian of the actor while the latter does not. This suggests that the Hessian provides more
information than that in FIM. However, FIM is always positive semi-deﬁnite while the Hessian may
be indeﬁnite. Please see Furmston et al. (2016) for detailed comparisons between the two curvature
matrices in policy search4. Nonetheless, actor-critic methods based on natural gradient were shown
to be very efﬁcient (Peters & Schaal, 2008; Schulman et al., 2015a).

‰

.

We are not aware of existing work that considers second-order updates for DPG or SVG. However,
their second-order updates can be trivially derived. For example, a Newton update for DPG is

θ Ð θ ` αD´1

!
Eppsq

”
∇θπθpsq∇a

pQps, aq|a“πθ psq

ı)

,

where the pi, jq-th entry of the Hessian matrix D P Rdθ ˆdθ is

Dij “ Eppsq

∇2
a

pQps, aq|a“πθ psq

Bπθpsq
Bθj

`

B2πθpsq
BθiBθj

J

∇a

pQps, aq|a“πθ psq

.

(12)

«

J

Bπθpsq
Bθi

(11)

ﬀ

Note that Bπθpsq{Bθ and B2πθpsq{BθBθ1 are vectors since πθpsq is a vector-valued function. In-
terestingly, the Hessian of DPG contains the Hessians of the actor and the critic. In contrast, the
Hessian of the actor-critic method contains the Hessian of the actor and the value of the critic.

Second-order methods are appealing in reinforcement learning because they have high data efﬁ-
ciency. However, inverting the curvature matrix (or solving a linear system) requires cubic compu-
tational complexity in terms of the number of optimization variables. For this reason, the second-
order updates in Eq.(8) and Eq.(11) are impractical in deep reinforcement learning due to a large
number of weight parameters in deep neural networks. In such a scenario, an approximation of
the curvature matrix is required to reduce the computational burden. For instance, Furmston et al.
(2016) proposed to use only diagonal entries of an approximated Hessian matrix. However, this
approximation clearly leads to a loss of useful curvature information since the gradient is scaled but
not rotated. More recently, Wu et al. (2017) proposed a natural actor-critic method that approxi-
mates block-diagonal entries of FIM. However, this approximation corresponds to ignoring useful
correlations between weight parameters in different layers of neural networks.

3 GUIDE ACTOR-CRITIC

In this section, we propose the guide actor-critic (GAC) method that performs second-order updates
without the previously discussed computational issue. Unlike existing methods that directly learn
the parameterized actor from the critic, GAC separates the problem of learning the parameterized
actor into problems of 1) learning a guide actor that locally maximizes the critic, and 2) learning a
parameterized actor based on the guide actor. This separation allows us to perform a second-order
update for the guide actor where the dimensionality of the curvature matrix is independent of the
parameterization of the actor.

We formulate an optimization problem for learning the guide actor in Section 3.1 and present its so-
lution in Section 3.2. Then in Section 3.3 and Section 3.4, we show that the solution corresponds to
performing second-order updates. Finally, Section 3.5 presents the learning step for the parameter-
ized actor using supervised learning. The pseudo-code of our method is provided in Appendix B and
the source code is available at https://github.com/voot-t/guide-actor-critic.

4 Furmston et al. (2016) proposed an approximate Newton method for policy search. Their policy search

method was shown to perform better than methods based on gradient ascent and natural gradient ascent.

4

Published as a conference paper at ICLR 2018

3.1 OPTIMIZATION PROBLEM FOR GUIDE ACTOR

Our ﬁrst goal is to learn a guide actor that maximizes the critic. However, greedy maximization
should be avoided since the critic is a noisy estimate of the expected return and a greedy actor
may change too abruptly across learning iterations. Such a behavior is undesirable in real-world
problems, especially in robotics (Deisenroth et al., 2013).
Instead, we maximize the critic with
additional constraints:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

(13)

where ˜πpa|sq is the guide actor to be learned, πθpa|sq is the current parameterized actor that we
want to improve upon, and pβpsq is the state distribution induced by past trajectories. The objective
function differs from the one in Eq.(5) in two important aspects. First, we maximize for a policy
function ˜π and not for the policy parameter. This is more advantageous than optimizing for a policy
parameter since the policy function can be obtained in a closed form, as will be shown in the next
subsection. Second, the expectation is deﬁned over a state distribution from past trajectories and this
gives us off-policy methods with higher data efﬁciency. The ﬁrst constraint is the Kullback-Leibler
(KL) divergence constraint where KLpppxq||qpxqq “ Eppxq rlog ppxq ´ log qpxqs. The second con-
straint is the Shannon entropy constraint where Hpppxqq “ ´Eppxq rlog ppxqs. The KL constraint
is commonly used in reinforcement learning to prevent unstable behavior due to excessively greedy
update (Peters & Schaal, 2008; Peters et al., 2010; Levine & Koltun, 2013; Schulman et al., 2015a).
The entropy constraint is crucial for maintaining stochastic behavior and preventing premature con-
vergence (Ziebart et al., 2010; Abdolmaleki et al., 2015; Mnih et al., 2016; Haarnoja et al., 2017).
The ﬁnal constraint ensures that the guide actor is a proper probability density. The KL bound
(cid:15) ą 0 and the entropy bound ´8 ă κ ă 8 are hyper-parameters which control the exploration-
exploitation trade-off of the method. In practice, we ﬁx the value of (cid:15) and adaptively reduce the
value of κ based on the current actor’s entropy, as suggested by Abdolmaleki et al. (2015). More
details of these tuning parameters are given in Appendix C.

This optimization problem can be solved by the method of Lagrange multipliers. The solution is

˜πpa|sq 9 πθpa|sq

η‹`ω‹ exp

η‹

˜

¸

,

pQps, aq
η‹ ` ω‹

(14)

where η‹ ą 0 and ω‹ ą 0 are dual variables corresponding to the KL and entropy constraints,
respectively. The dual variable corresponding to the probability density constraint is contained in the
normalization term and is determined by η‹ and ω‹. These dual variables are obtained by minimizing
the dual function:

«

ż

gpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

πθpa|sq

η
η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

.

(15)

All derivations and proofs are given in Appendix A. The solution in Eq.(14) tells us that the
guide actor is obtained by weighting the current actor with pQps, aq.
If we set (cid:15) Ñ 0 then we
have ˜π « πθ and the actor is not updated. On the other hand, if we set (cid:15) Ñ 8 then we have
˜πpa|sq 9 expp

pQps, aq{ω‹q, which is a softmax policy where ω‹ is the temperature parameter.

3.2 LEARNING GUIDE ACTOR

Computing ˜πpa|sq and evaluating gpη, ωq are intractable for an arbitrary πθpa|sq. We overcome
this issue by imposing two assumptions. First, we assume that the actor is the Gaussian distribution:

πθpa|sq “ N pa|φθpsq, Σθpsqq,
where the mean φθpsq and covariance Σθpsq are functions parameterized by a policy parameter
θ. Second, we assume that Taylor’s approximation of pQps, aq is locally accurate up to the second-
order. More concretely, the second-order Taylor’s approximation using an arbitrary action a0 is

(16)

5

Published as a conference paper at ICLR 2018

given by

pQps, aq «

pQps, a0q ` pa ´ a0qJg0psq `

pa ´ a0qJH 0psqpa ´ a0q ` Op}a}3q,

(17)

1
2

pQps, aq|a“a0 are the gradient and Hessian of
where g0psq “ ∇a
the critic w.r.t. a evaluated at a0, respectively. By assuming that the higher order term Op}a}3q is
sufﬁciently small, we can rewrite Taylor’s approximation at a0 as

pQps, aq|a“a0 and H 0psq “ ∇2

a

pQ0ps, aq “

1
2

aJH 0psqa ` aJψ0psq ` ξ0psq,

(18)

pQps, a0q. Note that
where ψ0psq “ g0psq ´ H 0psqa0 and ξ0psq “ 1
H 0psq, ψ0psq, and ξ0psq depend on the value of a0 and do not depend on the value of a. This
dependency is explicitly denoted by the subscript. The choice of a0 will be discussed in Section 3.3.

0 H 0psqa0 ´ aJ

0 g0psq `

2 aJ

Substituting the Gaussian distribution and Taylor’s approximation into Eq.(14) yields another Gaus-
sian distribution ˜πpa|sq “ N pa|φ`psq, Σ`psqq, where the mean and covariance are given by

φ`psq “ F ´1psqLpsq, Σ`psq “ pη‹ ` ω‹qF ´1psq.

The matrix F psq P Rdaˆda and vector Lpsq P Rda are deﬁned as
θ psq ´ H 0psq, Lpsq “ η‹Σ´1

F psq “ η‹Σ´1

θ psqφθpsq ` ψ0psq.

The dual variables η‹ and ω‹ are obtained by minimizing the following dual function:

(19)

(20)

pgpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

«

d

ﬀ

|2πpη ` ωqF ´1
|2πΣθpsq|

η
η`ω

η psq|

`

Epβ psq

LηpsqJF ´1

η psqLηpsq ´ ηφθpsqJΣ´1

` const,

(21)

‰
θ psqφθpsq

“

1
2

where F ηpsq and Lηpsq are deﬁned similarly to F psq and Lpsq but with η instead of η‹.

The practical advantage of using the Gaussian distribution and Taylor’s approximation is that the
guide actor can be obtained in a closed form and the dual function can be evaluated through matrix-
vector products. The expectation over pβpsq can be approximated by e.g., samples drawn from a
replay buffer (Mnih et al., 2015). We require inverting F ηpsq to evaluate the dual function. However,
these matrices are computationally cheap to invert when the dimension of actions is not large.

As shown in Eq.(19), the mean and covariance of the guide actor is computed using both the gradient
and Hessian of the critic. Yet, these computations do not resemble second-order updates discussed
previously in Section 2.2. Below, we show that for a particular choice of a0, the mean computation
corresponds to a second-order update that rotates gradients by a curvature matrix.

3.3 GUIDE ACTOR LEARNING AS SECOND-ORDER OPTIMIZATION

For now we assume that the critic is an accurate estimator of the true action-value function. In this
case, the quality of the guide actor depends on the accuracy of sample approximation in pgpη, ωq and
the accuracy of Taylor’s approximation. To obtain an accurate Taylor’s approximation of pQps, aq
using an action a0, the action a0 should be in the vicinity of a. However, we did not directly use
pQps, aqq (see Eq.(14)).
any individual a to compute the guide actor, but we weight πθpa|sq by expp
Thus, to obtain an accurate Taylor’s approximation of the critic, the action a0 needs to be similar to
actions sampled from πθpa|sq. Based on this observation, we propose two approaches to perform
Taylor’s approximation.

Taylor’s approximation around the mean. In this approach, we perform Taylor’s approximation
using the mean of πθpa|sq. More speciﬁcally, we use a0 “ Eπθ pa|sq ras “ φθpsq for Eq.(18). In
this case, we can show that the mean update in Eq.(19) corresponds to performing a second-order
update in the action space to maximize pQps, aq:

φ`psq “ φθpsq ` F ´1
φθ

psq∇a

pQps, aq|a“φθ psq,

(22)

6

Published as a conference paper at ICLR 2018

θ psq ´ H φθ psq and H φθ psq “ ∇2
a

pQps, aq|a“φθ psq. This equivalence can
where F φθ psq “ η‹Σ´1
be shown by substitution and the proof is given in Appendix A.2. This update equation reveals that
the guide actor maximizes the critic by taking a step in the action space similarly to the Newton
method. However, the main difference lies in the curvature matrix where the Newton method uses
Hessians H φθ psq but we use a damped Hessian F φθ psq. The damping term η‹Σ´1
θ psq corresponds
to the effect of the KL constraint and can be viewed as a trust-region that controls the step-size. This
damping term is particularly important since Taylor’s approximation is accurate only locally and we
should not take a large step in each update (Nocedal & Wright, 2006).

Expectation of Taylor’s approximations.
Instead of using Taylor’s approximation around the
mean, we may use an expectation of Taylor’s approximation over the distribution. More concretely,
we deﬁne rQps, aq to be an expectation of pQ0ps, aq over πθpa0|sq:

rQps, aq “

1
2

aJEπθ pa0|sq rH 0psqs a ` aJEπθ pa0|sq rψ0psqs ` Eπθ pa0|sq rξ0psqs .

(23)

pQps, aq|a“a0 s and the expectation is computed w.r.t.
Note that Eπθ pa0|sqrH 0psqs “ Eπθ pa0|sqr∇2
a
the distribution πθ of a0. We use this notation to avoid confusion even though πθpa0|sq and πθpa|sq
are the same distribution. When Eq.(23) is used, the mean update does not directly correspond
to any second-order optimization step. However, under an (unrealistic) independence assumption
Eπθ pa0|sqrH 0psqa0s “ Eπθ pa0|sqrH 0psqsEπθ pa0|sqra0s, we can show that the mean update corre-
sponds to the following second-order optimization step:

φ`psq “ φθpsq ` Eπθ pa0|sq rF 0psqs´1 Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

,

(24)

where Eπθ pa0|sq rF 0psqs “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs. Interestingly, the mean is updated
by rotating an expected gradient using an expected Hessians. In practice, the expectations can be
approximated using sampled actions ta0,iuS
i“1 „ πθpa|sq. We believe that this sampling can be
advantageous for avoiding local optima. Note that when the expectation is approximated by a single
sample a0 „ πθpa|sq, we obtain the update in Eq.(24) regardless of the independence assumption.
In the remainder, we use F psq to denote both of F φθ psq and Eπθ pa0|sq rF 0psqs, and use Hpsq to
denote both of H φθ psq and Eπθ pa0|sqrH 0psqs. In the experiments, we use GAC-0 to refer to GAC
with Taylor’s approximation around the mean, and we use GAC-1 to refer to GAC with Taylor’s
approximation by a single sample a0 „ πθpa|sq.

3.4 GAUSS-NEWTON APPROXIMATION OF HESSIAN

The covariance update in Eq.(19) indicates that F psq “ η‹Σ´1
θ psq ´ Hpsq needs to be positive
deﬁnite. The matrix F psq is guaranteed to be positive deﬁnite if the Hessian matrix Hpsq is negative
semi-deﬁnite. However, this is not guaranteed in practice unless pQps, aq is a concave function in
terms of a. To overcome this issue, we ﬁrstly consider the following identity:
pQps, aq∇a

pQps, aq “ ´∇a

pQps, aqq expp´

pQps, aqJ ` ∇2

pQps, aqq.

Hpsq “ ∇2
a

a expp

(25)

The proof is given in Appendix A.3. The ﬁrst term is always negative semi-deﬁnite while the second
term is indeﬁnite. Therefore, a negative semi-deﬁnite approximation of the Hessian can be obtained
as

H 0psq « ´

pQps, aq∇a

pQps, aqJ

”
∇a

ı

.

a“a0

(26)

pQps, aqq and it will be small for high values
The second term in Eq.(25) is proportional to expp´
of pQps, aq. This implies that the approximation should gets more accurate as the policy approach
a local maxima of pQps, aq. We call this approximation Gauss-Newton approximation since it is
similar to the Gauss-Newton approximation for the Newton method (Nocedal & Wright, 2006).

3.5 LEARNING PARAMETERIZED ACTOR

The second step of GAC is to learn a parameterized actor that well represents the guide actor. Below,
we discuss two supervised learning approaches for learning a parameterized actor.

7

Published as a conference paper at ICLR 2018

3.5.1 FULLY-PARAMETERIZED GAUSSIAN POLICY

Since the guide actor is a Gaussian distribution with a state-dependent mean and covariance, a
natural choice for the parameterized actor is again a parameterized Gaussian distribution with a
state-dependent mean and covariance: πθpa|sq “ N pa|φθpsq, Σθpsqq. The parameter θ can be
learned by minimizing the expected KL divergence to the guide actor:

LKLpθq “ Epβ psq rKL pπθpa|sq||˜πpa|sqqs

„



TrpF psqΣθpsqq
η‹ ` ω‹

“ Epβ psq
”
}φθpsq ´ φ`psq}2

´ log |Σθpsq|
ı

`

LWpθq
η‹ ` ω‹ ` const,

(27)

where LWpθq “ Epβ psq
which only depends on θ of the mean function. The const term does not depend on θ.

is the weighted-mean-squared-error (WMSE)

F psq

Minimizing the KL divergence reveals connections between GAC and deterministic policy gradients
(DPG) (Silver et al., 2014). By computing the gradient of the WMSE, it can be shown that

∇θLWpθq
2

“ ´Epβ psq

ı

pQps, aq|a“φθ psq

”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

psq∇a

psq∇a

”
∇θφθpsq∇a
ı

` Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

ı

.

` η‹Epβ psq

´ η‹Epβ psq

ı

pQps, aq|a“φ`psq

(28)

The proof is given in Appendix A.4. The negative of the ﬁrst term is precisely equivalent to DPG.
Thus, updating the mean parameter by minimizing the KL loss with gradient descent can be regarded
as updating the mean parameter with biased DPG where the bias terms depend on η‹. We can verify
pQps, aq|a“φ`psq “ 0 when η‹ “ 0 and this is the case of (cid:15) Ñ 8. Thus, all bias terms vanish
that ∇a
when the KL constraint is ignored and the mean update of GAC coincides with DPG. However,
unlike DPG which learns a deterministic policy, we can learn both the mean and covariance in GAC.

3.5.2 GAUSSIAN POLICY WITH PARAMETERIZED MEAN

While a state-dependent parameterized covariance function is ﬂexible, we observe that learning
performance is sensitive to the initial parameter of the covariance function. For practical pur-
poses, we propose using a parametrized Gaussian distribution with state-independent covariance:
πθpa|sq “ N pa|φθpsq, Σq. This class of policies subsumes deterministic policies with additive in-
dependent Gaussian noise for exploration. To learn θ, we minimize the mean-squared-error (MSE):
“

(29)
‰
For the covariance, we use the average of the guide covariances: Σ “ pη‹ ` ω‹qEpβ psq
.
For computational efﬁciency, we execute a single gradient update in each learning iteration instead
of optimizing this loss function until convergence.

F ´1psq

}φθpsq ´ φ`psq}2
2

LMpθq “

Epβ psq

1
2

‰

“

.

Similarly to the above analysis, the gradient of the MSE w.r.t. θ can be expanded and rewritten into

∇θLMpθq “ Epβ psq

”
∇θφθpsqH ´1psq

´

∇a

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

.

(30)

¯ı

Again, the mean update of GAC coincides with DPG when we minimize the MSE and set η‹ “ 0
and Hpsq “ ´I where I is the identity matrix. We can also substitute these values back into
Eq.(22). By doing so, we can interpret DPG as a method that performs ﬁrst-order optimization in
the action space:

φ`psq “ φθpsq ` ∇a

pQps, aq|a“φθ psq,

(31)

and then uses the gradient in Eq.(30) to update the policy parameter. This interpretation shows that
DPG is a ﬁrst-order method that only uses the ﬁrst-order information of the critic for actor learning.
Therefore in principle, GAC, which uses the second-order information of the critic, should learn
faster than DPG.

8

Published as a conference paper at ICLR 2018

3.6 POLICY EVALUATION FOR CRITIC

Beside actor learning, the performance of actor-critic methods also depends on the accuracy of the
critic. We assume that the critic pQν ps, aq is represented by neural networks with a parameter ν.
We adopt the approach proposed by Lillicrap et al. (2015) with some adjustment to learn ν. More
concretely, we use gradient descent to minimize the squared Bellman error with a slowly moving
target critic:

ν Ð ν ´ α∇ν Epβ psq,βpa|sq,pps1|s,aq

pQν ps, aq ´ y

(32)

„´



¯

2

,

pQ¯ν ps1, a1qs is computed by
where α ą 0 is the step-size. The target value y “ rps, aq ` γEπpa1|s1qr
the target critic pQ¯ν ps1, a1q whose parameter ¯ν is updated by ¯ν Ð τ ν ` p1 ´ τ q¯ν for 0 ă τ ă 1.
As suggested by Lillicrap et al. (2015), the target critic improves the learning stability and we set
τ “ 0.001 in experiments. The expectation for the squared error is approximated using mini-batch
samples tpsn, an, rn, s1
n“1 drawn from a replay buffer. The expectation over the current actor
πpa1|s1q is approximated using samples ta1
n. We do not use a
target actor to compute y since the KL upper-bound already constrains the actor update and a target
actor will further slow it down. Note that we are not restricted to this evaluation method and more
efﬁcient methods such as Retrace (Munos et al., 2016) can also be used.

m“1 „ πθpa1|s1

nq for each s1

n,muM

nquN

pQν ps, aq and its outer product for the Gauss-Newton approxi-
Our method requires computing ∇a
mation. The computational complexity of the outer product operation is Opd2
aq and is inexpensive
when compared to the dimension of ν. For a linear-in-parameter model pQν ps, aq “ νJµps, aq,
the gradient can be efﬁciently computed for common choices of the basis function µ such as the
Gaussian function. For deep neural network models, the gradient can be computed by the automatic-
differentiation (Goodfellow et al., 2016) where its cost depends on the network architecture.

4 RELATED WORK

Besides the connections to DPG, our method is also related to existing methods as follows.

A similar optimization problem to Eq.(13) was considered by the model-free trajectory optimization
(MOTO) method (Akrour et al., 2016). Our method can be viewed as a non-trivial extension of
MOTO with two signiﬁcant novelties. First, MOTO learns a sequence of time-dependent log-linear
Gaussian policies πtpa|sq “ N pa|Bts`bt, Σtq, while our method learns a log-nonlinear Gaussian
policy. Second, MOTO learns a time-dependent critic given by pQtps, aq “ 1
2 aJCta ` aJDts `
aJct `ξtpsq and performs policy update with these functions. In contrast, our method learns a more
complex critic and performs Taylor’s approximation in each training step.

Besides MOTO, the optimization problem also resembles that of trust region policy optimization
(TRPO) (Schulman et al., 2015a). TRPO solves the following optimization problem:

max
θ1

Epπθ psq,πθ1 pa|sq

”
ı
pQps, aq

subject to Epπθ psq rKLpπθpa|sq||πθ1pa|sqqs ď (cid:15),

(33)

where pQps, aq may be replaced by an estimate of the advantage function (Schulman et al., 2015b).
There are two major differences between the two problems. First, TRPO optimizes the policy param-
eter while we optimize the guide actor. Second, TRPO solves the optimization problem by conjugate
gradient where the KL divergence is approximated by the Fisher information matrix, while we solve
the optimization problem in a closed form with a quadratic approximation of the critic.

Our method is also related to maximum-entropy RL (Ziebart et al., 2010; Azar et al., 2012; Haarnoja
et al., 2017; Nachum et al., 2017), which maximizes the expected cumulative reward with an addi-
Epπpsq rrpst, atq ` αHpπpat|stqqs, where α ą 0 is a trade-off parame-
tional entropy bonus:
ter. The optimal policy in maximum-entropy RL is the softmax policy given by

8
t“1

ř

π‹
MaxEntpa|sq “ exp

ˆ

Q‹

softps, aq ´ V ‹
α

softpsq

˙

ˆ

9 exp

˙

,

Q‹

softps, aq
α

(34)

9

Published as a conference paper at ICLR 2018

softps, aq and V ‹

where Q‹
tively (Haarnoja et al., 2017; Nachum et al., 2017). For a policy π, these are deﬁned as
softps, aq “ rps, aq ` γEpps1|s,aq

softpsq are the optimal soft action-value and state-value functions, respec-
“

(35)

Qπ

,

ż

V π
softpsq “ α log

exp

ˆ

Qπ

‰
V π
softps1q
˙
softps, aq
α

da.

(36)

The softmax policy and the soft state-value function in maximum-entropy RL closely resemble the
guide actor in Eq.(14) when η‹ “ 0 and the log-integral term in Eq.(15) when η “ 0, respectively,
except for the deﬁnition of action-value functions. To learn the optimal policy of maximum-entropy
RL, Haarnoja et al. (2017) proposed soft Q-learning which uses importance sampling to compute
the soft value functions and approximates the intractable policy using a separate policy function.
Our method largely differs from soft Q-learning since we use Taylor’s approximation to convert the
intractable integral into more convenient matrix-vector products.

The idea of ﬁrstly learning a non-parameterized policy and then later learning a parameterized policy
by supervised learning was considered previously in guided policy search (GPS) (Levine & Koltun,
2013). However, GPS learns the guide policy by trajectory optimization methods such as an iterative
linear-quadratic Gaussian regulator (Li & Todorov, 2004), which requires a model of the transition
function. In contrast, we learn the guide policy via the critic without learning the transition function.

5 EXPERIMENTAL RESULTS

We evaluate GAC on the OpenAI gym platform (Brockman et al., 2016) with the Mujoco Physics
simulator (Todorov et al., 2012). The actor and critic are neural networks with two hidden layers of
400 and 300 units, as described in Appendix C. We compare GAC-0 and GAC-1 against deep DPG
(DDPG) (Lillicrap et al., 2015), Q-learning with a normalized advantage function (Q-NAF) (Gu
et al., 2016), and TRPO (Schulman et al., 2015a;b). Figure 1 shows the learning performance on
9 continuous control tasks. Overall, both GAC-0 and GAC-1 perform comparably with existing
methods and they clearly outperform the other methods in Half-Cheetah.

The performance of GAC-0 and GAC-1 is comparable on these tasks, except on Humanoid where
GAC-1 learns faster. We expect GAC-0 to be more stable and reliable but easier to get stuck at
local optima. On the other hand, the randomness introduced by GAC-1 leads to high variance
approximation but this could help escape poor local optima. We conjecture GAC-S that uses S ą 1
samples for the averaged Taylor’s approximation should outperform both GAC-0 and GAC-1. While
this is computationally expensive, we can use parallel computation to reduce the computation time.

The expected returns of both GAC-0 and GAC-1 have high ﬂuctuations on the Hopper and Walker2D
tasks when compared to TRPO as can be seen in Figure 1g and Figure 1h. We observe that they can
learn good policies for these tasks in the middle of learning. However, the policies quickly diverge
to poor ones and then they are quickly improved to be good policies again. We believe that this
happens because the step-size F ´1psq “
of the guide actor in Eq. (22) can
be very large near local optima for Gauss-Newton approximation. That is, the gradients near local
pQps, aqJ
optima have small magnitude and this makes the approximation Hpsq “ ∇a
small as well. If η‹Σ´1 is also relatively small then the matrix F ´1psq can be very large. Thus,
under these conditions, GAC may use too large step sizes to compute the guide actor and this results
in high ﬂuctuations in performance. We expect that this scenario can be avoided by reducing the KL
bound (cid:15) or adding a regularization constant to the Gauss-Newton approximation.

˘
η‹Σ´1 ´ Hpsq

pQps, aq∇a

´1

`

Table 1 in Appendix C shows the wall-clock computation time. DDPG is computationally the most
efﬁcient method on all tasks. GAC has low computation costs on tasks with low dimensional actions
and its cost increases as the dimensionality of action increases. This high computation cost is due to
the dual optimization for ﬁnding the step-size parameters η and ω. We believe that the computation
cost of GAC can be signiﬁcantly reduced by letting η and ω be external tuning parameters.

6 CONCLUSION AND FUTURE WORK

Actor-critic methods are appealing for real-world problems due to their good data efﬁciency and
learning speed. However, existing actor-critic methods do not use second-order information of the

10

Published as a conference paper at ICLR 2018

(a) Inverted-Pend.

(b) Inv-Double-Pend.

(c) Reacher

(d) Swimmer

(e) Half-Cheetah

(f) Ant

(g) Hopper

(h) Walker2D

(i) Humanoid

Figure 1: Expected returns averaged over 10 trials. The x-axis indicates training time steps. The y-
axis indicates averaged return and higher is better. More clear ﬁgures are provided in Appendix C.2.

critic. In this paper, we established a novel framework that distinguishes itself from existing work
by utilizing Hessians of the critic for actor learning. Within this framework, we proposed a prac-
tical method that uses Gauss-Newton approximation instead of the Hessians. We showed through
experiments that our method is promising and thus the framework should be further investigated.

Our analysis showed that the proposed method is closely related to deterministic policy gradients
(DPG). However, DPG was also shown to be a limiting case of the stochastic policy gradients when
the policy variance approaches zero (Silver et al., 2014). It is currently unknown whether our frame-
work has a connection to the stochastic policy gradients as well, and ﬁnding such a connection is
our future work.

Our main goal in this paper was to provide a new actor-critic framework and we do not claim that our
method achieves the state-of-the-art performance. However, its performance can still be improved in
many directions. For instance, we may impose a KL constraint for a parameterized actor to improve
its stability, similarly to TRPO (Schulman et al., 2015a). We can also apply more efﬁcient policy
evaluation methods such as Retrace (Munos et al., 2016) to achieve better critic learning.

ACKNOWLEDGMENTS

MS was partially supported by KAKENHI 17H00757.

REFERENCES

Abbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, Nuno Lau, Lu´ıs Paulo Reis, and Gerhard Neu-
In Advances in Neural Information

mann. Model-Based Relative Entropy Stochastic Search.
Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki. Model-Free Trajec-
tory Optimization for Reinforcement Learning. In Proceedings of the 33nd International Confer-
ence on Machine Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Shun-ichi Amari. Natural Gradient Works Efﬁciently in Learning. Neural Computation, 10(2):

251–276, 1998.

Brandon Amos, Lei Xu, and J. Zico Kolter. Input Convex Neural Networks. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
August 6-11, 2017, Sydney, Australia, 2017.

Mohammad Gheshlaghi Azar, Vicenc¸ G´omez, and Hilbert J. Kappen. Dynamic Policy Program-

ming. Journal of Machine Learning Research, 13:3207–3245, 2012.

11

Published as a conference paper at ICLR 2018

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.

Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on Policy Search for Robotics.

Foundations and Trends in Robotics, 2(1-2):1–142, 2013.

Thomas Furmston, Guy Lever, and David Barber. Approximate Newton Methods for Policy Search
in Markov Decision Processes. Journal of Machine Learning Research, 17(227):1–51, 2016.

Xavier Glorot and Yoshua Bengio. Understanding the Difﬁculty of Training Deep Feedforward
Neural Networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the 13th In-
ternational Conference on Artiﬁcial Intelligence and Statistics, May 13-15, 2010, Sardinia, Italy,
2010.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous Deep Q-Learning
with Model-based Acceleration. In Proceedings of the 33nd International Conference on Machine
Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with
Deep Energy-Based Policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, August 6-11, 2017, Sydney, Australia, 2017.

Nicolas Heess, Gregory Wayne, David Silver, Timothy P. Lillicrap, Tom Erez, and Yuval Tassa.
In Advances in Neural

Learning Continuous Control Policies by Stochastic Value Gradients.
Information Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Sham Kakade. A Natural Policy Gradient. In Advances in Neural Information Processing Systems

14, December 3-8, 2001, Vancouver, British Columbia, Canada, 2001.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. CoRR,

abs/1412.6980, 2014.

Vijay R. Konda and John N. Tsitsiklis. On Actor-Critic Algorithms. SIAM Journal on Control and

Optimization, 42(4):1143–1166, April 2003.

Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the 30th International

Conference on Machine Learning, June 16-21, 2013, Atlanta, GA, USA, 2013.

Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep
Visuomotor Policies. Journal of Machine Learning Research, 17(1):1334–1373, January 2016.
ISSN 1532-4435.

Weiwei Li and Emanuel Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear Bio-
logical Movement Systems. In Proceedings of the 1st International Conference on Informatics in
Control, Automation and Robotics, August 25-28, 2004, Set´ubal, Portugal, pp. 222–229, 2004.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. CoRR,
abs/1509.02971, 2015.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement
Learning. Nature, 518(7540):529–533, February 2015. ISSN 00280836.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement
In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
Learning.
International Conference on Machine Learning, June 19-24, 2016, New York City, NY, USA,
2016.

12

Published as a conference paper at ICLR 2018

R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and Efﬁcient Off-
Policy Reinforcement Learning. In Advances in Neural Information Processing Systems 29, De-
cember 5-10, 2016, Barcelona, Spain, 2016.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the Gap Between
Value and Policy Based Reinforcement Learning. In Advances in Neural Information Processing
Systems 30, 4-9 December 2017, Long Beach, CA, USA, 2017.

Jorge Nocedal and Stephen J. Wright. Numerical Optimization, Second Edition. World Scientiﬁc,

2006.

Jan Peters and Stefan Schaal. Natural Actor-Critic. Neurocomputing, 71(7-9):1180–1190, 2008.

Jan Peters, Katharina M¨ulling, and Yasemin Altun. Relative Entropy Policy Search. In Proceedings
of the 24th AAAI Conference on Artiﬁcial Intelligence, July 11-15, 2010, Atlanta, Georgia, USA,
2010.

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region
Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning,
July 6-11, 2015, Lille, France, pp. 1889–1897, 2015a.

John Schulman, Philipp Moritz, Sergey Levine, Michael

High-Dimensional Continuous Control Using Generalized Advantage Estimation.
abs/1506.02438, 2015b.

I. Jordan, and Pieter Abbeel.
CoRR,

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic Policy Gradient Algorithms. In Proceedings of the 31st International Conference
on Machine Learning, June 21-26, 2014, Beijing, China, 2014.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game
of Go with Deep Neural Networks and Tree Search. Nature, 529(7587):484–489, 2016.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning - an Introduction. Adaptive

computation and machine learning. MIT Press, 1998.

Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy Gradi-
ent Methods for Reinforcement Learning with Function Approximation. In Advances in Neural
Information Processing Systems 12, November 29 - December 4, 1999, Colorado, USA, 1999.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics Engine for Model-Based Control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012,
Vilamoura, Algarve, Portugal, 2012.

Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforce-

ment Learning. Machine Learning, 8(3):229–256, 1992. doi: 10.1007/BF00992696.

Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable Trust-Region
Method for Deep Reinforcement Learning using Kronecker-factored Approximation. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, 4-9 December 2017, Long Beach, CA, USA,
2017.

Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling Interaction via the Principle
of Maximum Causal Entropy. In Proceedings of the 27th International Conference on Machine
Learning, June 21-24, 2010, Haifa, Israel, 2010.

13

Published as a conference paper at ICLR 2018

A DERIVATIONS AND PROOFS

A.1 DERIVATION OF SOLUTION AND DUAL FUNCTION OF GUIDE ACTOR

The solution of the optimization problem:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

can be obtained by the method of Lagrange multipliers. The derivation here follows the derivation of
similar optimization problems by Peters et al. (2010) and Abdolmaleki et al. (2015). The Lagrangian
of this optimization problem is

Lprπ, η, ω, νq “ Epβ psq,˜πpa|sq

` ηp(cid:15) ´ Epβ psq rKLp˜πpa|sq||πθpa|sqqsq

ı
”
pQps, aq

` ωpEpβ psq rHp˜πpa|sqqs ´ κq ` νpEpβ psqrπpa|sq ´ 1q,

(38)

where η, ω, and ν are the dual variables. Then, by taking derivative of L w.r.t. rπ we obtain

„ż ´

¯



BrπL “ Epβ psq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq

da

´ pη ` ω ´ νq.

(39)

We set this derivation to zero in order to obtain

0 “ Epβ psq

„ż ´

¯
pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq



da

´ pη ` ω ´ νq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq ´ pη ` ω ´ νq.

“

Then the solution is given by

rπpa|sq “ πθpa|sq

η
η`ω exp

9 πθpa|sq

η
η`ω exp

¸

ˆ

exp

´

˙

η ` ω ´ ν
η ` ω

˜

˜

pQps, aq
η ` ω
pQps, aq
η ` ω

¸

.

(37)

(40)

(41)

(42)

To obtain the dual function gpη, ωq, we substitute the solution to the constraint terms of the La-
grangian and this gives us

Lpη, ω, νq “ Epβ psq,rπpa|sq

ı

”
pQps, aq

«

´ pη ` ωqEpβ psq,rπpa|sq

pQps, aq
η ` ω

`
`

η
log πθpa|sq ´
η ` ω
˘
Epβ ,rπpa|sq ´ 1

ﬀ

η ` ω ´ ν
η ` ω

` ηEpβ psq,rπpa|sq rlog πθpa|sqs ` ν

` η(cid:15) ´ ωκ.

(43)

After some calculation, we obtain

Lpη, ω, νq “ η(cid:15) ´ ωκ ` Epβ psq rη ` ω ´ νs
«ż

“ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

η

πθpa|sq

η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

“ gpη, ωq,

(44)

where in the second line we use the fact that expp´ η`ω´ν

η`ω q is the normalization term of rπpa|sq.

14

Published as a conference paper at ICLR 2018

A.2 PROOF OF SECOND-ORDER OPTIMIZATION IN ACTION SPACE

Firstly, we show that GAC performs second-order optimization in the action space when Taylor’s
approximation is performed with a0 “ Eπpa|sq ras “ φθpsq. Recall that Taylor’s approximation
with φθ is given by

1
2

pQps, aq “
pQps, aq|a“φθ psq ´ H φθ psqφθpsq. By substituting ψφθ

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq,

(45)

psq into Lpsq “

where ψφθ
η‹Σ´1

psq “ ∇a

θ psqφθpsq ´ ψθpsq, we obtain

Lpsq “ η‹Σ´1
“ pη‹Σ´1

θ psqφθpsq ` ∇a
θ psq ´ H φθ psqqφθpsq ` ∇a
pQps, aq|a“φθ psq.

pQps, aq|a“φθ psq ´ H φθ psqφθpsq

pQps, aq|a“φθ psq

“ F psqφθpsq ` ∇a
Therefore, the mean update is equivalent to

φ`psq “ F ´1psqLpsq

“ φθpsq ` F ´1psq∇a

pQps, aq|a“φθ psq,

which is a second-order optimization step with a curvature matrix F psq “ η‹Σ´1

θ psq ´ H φθ psq.

Lpsq “ η‹Σ´1

Similarly, for the case where a set of samples ta0u „ πθpa0|sq “ N pa0|φθpsq, Σpsqq is used to
compute the averaged Taylor’s approximation, we obtain
θ psqφθpsq ` Eπθ pa0|sq

”
∇a
Then, by assuming that Eπθ pa0|sq rH 0psqa0psqs “ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s, we obtain
”
∇a
”
∇a

pQps, aq|a“a0
pQps, aq|a“a0

´ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s

´ Eπθ pa0|sq rH 0psqa0psqs .

θ psqφθpsq ` Eπθ pa0|sq

θ psqφθpsq ` Eπθ pa0|sq

pQps, aq|a“a0

Lpsq “ η‹Σ´1

“ η‹Σ´1

(48)

ı

ı

ı

´ Eπθ pa0|sq rH 0s φθpsq
ı

”
∇a

pQps, aq|a“a0

“ pη‹Σ´1

θ psq ´ Eπθ pa0|sq rH 0psqsqφθpsq ` Eπθ pa0|sq

“ F psqφθpsq ` Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

.

Therefore, we have a second-order optimization step

φ`psq “ φθpsq ` F ´1psqEπθ pa0|sq

(50)
where F ´1psq “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs is a curvature matrix. As described in
the main paper, this interpretation is only valid when the equality Eπθ pa0|sq rH 0psqa0psqs “
Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s holds. While this equality does not hold in general, it holds when
only one sample a0 „ πθpa0|sq is used. Nonetheless, we can still use the expectation of Taylor’s
approximation to perform policy update regardless of this assumption.

,

pQps, aq|a“a0

”
∇a

ı

(46)

(47)

(49)

A.3 PROOF OF GAUSS-NEWTON APPROXIMATION

Let f ps, aq “ expp

pQps, aqq, then the Hessian Hpsq “ ∇2

pQps, aq can be expressed as

a

Hpsq “ ∇a r∇a log f ps, aqs
“
∇af ps, aqf ps, aq´1
∇af ps, aq´1

“ ∇a

`

‰

˘

J

“ ∇af ps, aq
“ ∇af ps, aq∇f f ps, aq´1 p∇af ps, aqqJ ` ∇2
“ ´∇af ps, aqf ps, aq´2 p∇af ps, aqqJ ` ∇2

` ∇2

af ps, aqf ps, aq´1

af ps, aqf ps, aq´1
af ps, aqf ps, aq´1

`

˘ `

∇af ps, aqf ps, aq´1

“ ´
“ ´∇a log f ps, aq∇a log f ps, aqJ ` ∇2
a expp
“ ´∇a

pQps, aqJ ` ∇2

pQps, aq∇a

` ∇2
af ps, aqf ps, aq´1
pQps, aqq expp´

∇af ps, aqf ps, aq´1

pQps, aqq,

˘

J

af ps, aqf ps, aq´1

(51)

15

Published as a conference paper at ICLR 2018

which concludes the proof.

Beside Gauss-Newton approximation, an alternative approach is to impose a special structure on
pQps, aq so that Hessians are always negative semi-deﬁnite. In literature, there exists two special
structures that satisﬁes this requirement.

Normalized advantage function (NAF) (Gu et al., 2016): NAF represents the critic by a quadratic
function with a negative curvature:

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(52)

where a negative-deﬁnite matrix-valued function W psq, a vector-valued function bpsq and a scalar-
valued function V psq are parameterized functions whose their parameters are learned by policy
evaluation methods such as Q-learning (Sutton & Barto, 1998). With NAF, negative deﬁnite Hes-
sians can be simply obtained as Hpsq “ W psq. However, a signiﬁcant disadvantage of NAF is that
it assumes the action-value function is quadratic regardless of states and this is generally not true for
most reward functions. Moreover, the Hessians become action-independent even though the critic is
a function of actions.

Input convex neural networks (ICNNs) (Amos et al., 2017): ICNNs are neural networks with
special structures which make them convex w.r.t. their inputs. Since Hessians of concave functions
are always negative semi-deﬁnite, we may use ICNNs to represent a negative critic and directly use
its Hessians. However, similarly to NAF, ICNNs implicitly assume that the action-value function is
concave w.r.t. actions regardless of states and this is generally not true for most reward functions.

A.4 GRADIENT OF THE LOSS FUNCTIONS FOR LEARNING PARAMETERIZED ACTOR

We ﬁrst consider
is
N pa|φ`psq, Σ`psqq and the current actor is N pa|φθpsq, Σθpsqq. Taylor’s approximation of
pQps, aq at a0 “ φθpsq is

loss function where the guide actor

the weight mean-squared-error

pQps, aq “

1
2

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq.

(53)

By assuming that H φθ psq is strictly negative deﬁnite5, we can take a derivative of this approxima-
tion w.r.t. a and set it to zero to obtain a “ H ´1
psq. Replacing a by
φθ
φθpsq and φ`psq yields

pQps, aq ´ H ´1
φθ

psqψφθ

psq∇a

φθpsq “ H ´1
φθ
φ`psq “ H ´1
φθ

psq∇a

psq∇a

pQps, aq|a“φθ psq ´ H ´1
pQps, aq|a“φ`psq ´ H ´1

φθ

φθ

psqψφθ
psqψφθ

psq,

psq.

Recall that the weight mean-squared-error is deﬁned as

LWpθq “ Epβ psq

”
}φθpsq ´ φ`psq}2

F psq

ı

.

(54)

(55)

(56)

5This can be done by subtracting a small positive value to the diagonal entries of Gauss-Newton approxi-

mation.

16

Published as a conference paper at ICLR 2018

Then, we consider its gradient w.r.t. θ as follows:

∇θLWpθq “ 2Epβ psq
“ 2Epβ psq
“ 2Epβ psq

“

“

“

`

∇θφθpsqF psq
∇θφθpsqpη‹Σ´1
∇θφθpsqpη‹Σ´1
´
H ´1
φθ

ˆ

˘‰

`

φθpsq ´ φ`psq
θ psq ´ H φθ psqq
θ psq ´ H φθ psqq

˘‰

φθpsq ´ φ`psq

¯ı

psq∇a

pQps, aq|a“φθ psq ´ H ´1

φθ

psq∇a

pQps, aq|a“φ`psq

“ 2η‹Epβ psq

´

psq

”
θ psqH ´1
∇θφθpsqΣ´1
φθ
´
”
∇θφθpsq

` 2Epβ psq
”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

∇a
pQps, aq|a“φθ psq

pQps, aq|a“φθ psq ´ ∇a
pQps, aq|a“φθ psq
”
∇θφθpsq∇a
ı

∇a
pQps, aq|a“φ`psq ´ ∇a
ı
` 2Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

psq∇a

psq∇a

ı

.

` 2η‹Epβ psq

´ 2η‹Epβ psq

“ ´2Epβ psq

pQps, aq|a“φ`psq
¯ı

pQps, aq|a“φ`psq

¯ı

ı

(57)

This concludes the proof for the gradient in Eq.(28). Note that we should not directly replace
the mean functions in the weight mean-square-error by Eq.(54) and Eq.(55) before expanding the
gradient. This is because the analysis would require computing the gradient w.r.t. θ inside the
Hessians and this is not trivial. Moreover, when we perform gradient descent in practice, the mean
φ`psq is considered as a constant w.r.t. θ similarly to an output function in supervised learning.
The gradient for the mean-squared-error can be derived similarly. Let the mean-squared-error be
deﬁned as

LMpθq “

Epβ psq

}φθpsq ´ φ`psq}2
2

.

“

‰

1
2

(58)

Its gradient w.r.t. θ is given by

“

`

˘‰

∇θLMpθq “ Epβ psq
“ Epβ psq

∇θφθpsq
”
∇θφθpsqH ´1psq

φθpsq ´ φ`psq
´

∇a

which concludes the proof for the gradient in Eq.(30).

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

,

(59)

¯ı

To show that ∇a
˘
`
ηΣ´1psq ´ H φθ psq

´1

´
η‹Σ´1psqφθpsq ` ψφθ

pQps, aq|a“φ`psq “ 0 when η‹ “ 0, we directly substitute η‹ “ 0 into φ`psq “
¯

psqq

and this yield

φ`psq “ ´H ´1
φθ
pQps, aq|a“φ`psq ´ H ´1
Since φ`psq “ H ´1
psq∇a
psq from Eq.(55) and the Hessians are
φθ
pQps, aq|a“φ`psq “ 0. This is intuitive since without the KL constraint,
non-zero, it has to be that ∇a
the mean of the guide actor always be at the optima of the second-order Taylor’s approximation and
the gradients are zero at the optima.

psqψφθ

psqψφθ

psq.

(60)

φθ

A.5 RELATION TO Q-LEARNING WITH NORMALIZED ADVANTAGE FUNCTION

The normalized advantage function (NAF) (Gu et al., 2016) is deﬁned as

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(61)

where W psq is a negative deﬁnite matrix. Gu et al. (2016) proposed to perform Q-learning with
NAF by using the fact that argmaxa
Here, we show that GAC includes the Q-learning method by Gu et al. (2016) as its special case. This
can be shown by using NAF as a critic instead of performing Taylor’s approximation of the critic.

pQNAFps, aq “ bpsq and maxa

pQNAFps, aq “ V psq.

17

Published as a conference paper at ICLR 2018

pQNAFps, aq “

Firstly, we expand the quadratic term of NAF as follows:
1
2
1
2
1
2

aJW psqa ´ aJW psqbpsq `

aJW psqa ` aJψpsq ` ξpsq,

“

“

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq

bpsqJW psqbpsq ` V psq

(62)

where ψpsq “ ´W psqbpsq and ξpsq “ 1
2 bpsqJW psqbpsq ` V psq. By substituting the quadratic
model obtained by NAF into the GAC framework, the guide actor is now given by ˜πpa|sq “
N pa|φ`psq, Σ`psqqq with

φ`psq “ pη‹Σ´1psqq ´ W psqq´1pη‹Σ´1psqφθpsq ´ W psqbpsqq
Σ`psq “ pη‹ ` ω‹qpη‹Σ´1psqq ´ W psqq.

(64)
To obtain Q-learning with NAF, we set η‹ “ 0, i.e., we perform a greedy maximization where the
KL upper-bound approaches inﬁnity, and this yields

(63)

φ`psq “ ´W psq´1p´W psqbpsqq

(65)
which is the policy obtained by performing Q-learning with NAF. Thus, NAF with Q-learning is a
special case of GAC if Q-learning is also used in GAC to learn the critic.

“ bpsq,

The pseudo-code of GAC is given in Algorithm 1. The source code is available at https://
github.com/voot-t/guide-actor-critic.

B PSEUDO-CODE OF GAC

C EXPERIMENT DETAILS

C.1

IMPLEMENTATION

We try to follow the network architecture proposed by the authors of each baseline method as close as
possible. For GAC and DDPG, we use neural networks with two hidden layers for the actor network
and the critic network. For both networks the ﬁrst layer has 400 hidden units and the second layer
has 300 units. For NAF, we use neural networks with two hidden layers to represent each of the
functions bpsq, W psq and V psq where each layer has 200 hidden units. All hidden units use the
relu activation function except for the output of the actor network where we use the tanh activation
function to bound actions. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate
0.001 and 0.0001 for the critic network and the actor network, respectively. The moving average
step for target networks is set to τ “ 0.001. The maximum size of the replay buffer is set to
1000000. The mini-batches size is set to N “ 256. The weights of the actor and critic networks
are initialized as described by Glorot & Bengio (2010), except for the output layers where the initial
weights are drawn uniformly from Up´0.003, 0.003q, as described by Lillicrap et al. (2015). The
initial covariance Σ in GAC is set to be an identity matrix. DDPG and QNAF use the OU-process
with noise parameters θ “ 0.15 and σ “ 0.2 for exploration .

For TRPO, we use the implementation publicly available at https://github.com/openai/
baselines. We also use the provided network architecture and hyper-parameters except the batch
size where we use 1000 instead of 1024 since this is more suitable in our test setup.

For GAC, the KL upper-bound is ﬁxed to (cid:15) “ 0.0001. The entropy lower-bound κ is adjusted
heuristically by

κ “ maxp0.99pE ´ E0q ` E0, E0q,
(71)
where E « Epβ psq rHpπθpa|sqqs denotes the expected entropy of the current policy and E0 denotes
the entropy of a base policy N pa|0, 0.01Iq. This heuristic ensures that the lower-bound gradually
decreases but the lower-bound cannot be too small. We apply this heuristic update once every 5000
training steps. The dual function is minimize by the sequential least-squares quadratic programming
(SLSQP) method with an initial values η “ 0.05 and ω “ 0.05. The number of samples for
computing the target critic value is M “ 10.

18

Published as a conference paper at ICLR 2018

Algorithm 1 Guide actor critic

1: Input:

Initial actor πθpa|sq “ N pa|φθpsq, Σq, critic pQν ps, aq, target critic network

pQ¯ν ps, aq, KL bound (cid:15), entropy bound κ, learning rates α1, α2, and data buffer D “ H.

procedure COLLECT TRANSITION SAMPLE

Observe state st and sample action at „ N pa|φθpstq, Σq.
Execute at, receive reward rt and next state s1
t.
Add transition tpst, at, rt, s1

tqu to D.

2: for t “ 1, . . . , Tmax do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

end procedure
procedure LEARN

Sample N mini-batch samples tpsn, an, rn, s1
procedure UPDATE CRITIC
n,muM
Sample actions ta1
Compute yn, update ν by, e.g., Adam, and update ¯ν by moving average:

m“1 „ N pa|φθpsnq, Σq for each sn.

n“1 uniformly from D.

nquN

yn “ rn ` γ

pQ¯ν ps1

n, a1

n,mq,

ν Ð ν ´ α1

´
pQν psn, anq ´ yn

¯

2

,

∇ν

1
M

1
N

Mÿ

m“1
Nÿ

n“1

¯ν Ð τ ν ` p1 ´ τ q¯ν.

end procedure
procedure LEARN GUIDE ACTOR

Compute an,0 for each sn by an,0 “ φθpsnq or an,0 „ N pa|φθpsnq, Σq.
pQpsn, aq|a“an,0 and H 0pssq “ ´g0psnqg0psnqJ.
Compute g0psq “ ∇a
Solve for pη‹, ω‹q “ argminηą0,ωą0
Compute the guide actor rπpa|snq “ N pa|φ`psnq, Σ`psnqq for each sn.

pgpη, ωq by a non-linear optimization method.

end procedure
procedure UPDATE PARAMETERIZED ACTOR

Update policy parameter by, e.g., Adam, to minimize the MSE:

13:
14:
15:

16:
17:
18:
19:
20:
21:

θ Ð θ ´ α2

∇θ}φθpsnq ´ φ`psnq}2
2.

1
N

Nÿ

n“1

22:

Update policy covariance by averaging the guide covariances:

Σ Ð

Σ`psnq.

1
N

(66)

(67)

(68)

(69)

(70)

end procedure

end procedure

23:
24:
25: end for
26: Output: Learned actor πθpa|sq.

C.2 ENVIRONMENTS AND RESULTS

We perform experiments on the OpenAI gym platform (Brockman et al., 2016) with Mujoco Physics
simulator (Todorov et al., 2012) where all environments are v1. We use the state space, action space
and the reward function as provided and did not perform any normalization or gradient clipping.
The maximum time horizon in each episode is set to 1000. The discount factor γ “ 0.99 is only
used for learning and the test returns are computed without it.

Experiments are repeated for 10 times with different random seeds. The total computation time are
reported in Table 1. The ﬁgures below show the results averaged over 10 trials. The y-axis indicates
the averaged test returns where the test returns in each trial are computed once every 5000 training
time steps by executing 10 test episodes without exploration. The error bar indicates standard error.

19

Published as a conference paper at ICLR 2018

Table 1: The total computation time for training the policy for 1 million steps (0.1 million steps for
the Invert-Pendulum task). The mean and standard error are computed over 10 trials with the unit in
hours. TRPO is not included since it performs a lesser amount of update using batch data samples.

Task
Inv-Pend.
Inv-Double-Pend.
Reacher
Swimmer
Half-Cheetah
Ant
Hopper
Walker2D
Humanoid

GAC-1
1.13(0.09)
15.56(0.93)
17.23(0.87)
12.43(0.74)
29.82(1.76)
37.84(1.80)
18.99(1.06)
33.42(0.96)
111.07(2.99)

GAC-0
0.80(0.04)
15.67(0.77)
12.64(0.38)
11.94(0.74)
32.64(1.41)
40.57(3.06)
14.22(0.56)
31.71(1.84)
106.80(6.80)

DDPG
0.45(0.02)
9.04(0.29)
10.67(0.37)
9.61(0.52)
10.13(0.41)
9.75(0.37)
8.74(0.45)
7.92(0.11)
13.90(1.60)

QNAF
0.40(0.03)
7.47(0.22)
30.91(2.09)
32.44(2.21)
27.94(2.37)
27.09(0.94)
26.48(1.42)
26.94(1.99)
30.43(2.21)

Figure 2: Performance averaged over 10 trials on the Inverted Pendulum task.

20

Published as a conference paper at ICLR 2018

Figure 3: Performance averaged over 10 trials on the Inverted Double Pendulum task.

Figure 4: Performance averaged over 10 trials on the Reacher task.

21

Published as a conference paper at ICLR 2018

Figure 5: Performance averaged over 10 trials on the Swimmer task.

Figure 6: Performance averaged over 10 trials on the Half-Cheetah task.

22

Published as a conference paper at ICLR 2018

Figure 7: Performance averaged over 10 trials on the Ant task.

Figure 8: Performance averaged over 10 trials on the Hopper task.

23

Published as a conference paper at ICLR 2018

Figure 9: Performance averaged over 10 trials on the Walker2D task.

Figure 10: Performance averaged over 10 trials on the Humanoid task.

24

8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
6
7
0
.
5
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

GUIDE ACTOR-CRITIC FOR CONTINUOUS CONTROL

Voot Tangkaratt
RIKEN AIP, Tokyo, Japan
voot.tangkaratt@riken.jp

Abbas Abdolmaleki
The University of Aveiro, Aveiro, Portugal
abbas.a@ua.pt

Masashi Sugiyama
RIKEN AIP, Tokyo, Japan
The University of Tokyo, Tokyo, Japan
masashi.sugiyama@riken.jp

ABSTRACT

Actor-critic methods solve reinforcement learning problems by updating a param-
eterized policy known as an actor in a direction that increases an estimate of the
expected return known as a critic. However, existing actor-critic methods only
use values or gradients of the critic to update the policy parameter. In this pa-
per, we propose a novel actor-critic method called the guide actor-critic (GAC).
GAC ﬁrstly learns a guide actor that locally maximizes the critic and then it up-
dates the policy parameter based on the guide actor by supervised learning. Our
main theoretical contributions are two folds. First, we show that GAC updates the
guide actor by performing second-order optimization in the action space where
the curvature matrix is based on the Hessians of the critic. Second, we show that
the deterministic policy gradient method is a special case of GAC when the Hes-
sians are ignored. Through experiments, we show that our method is a promising
reinforcement learning method for continuous controls.

1

INTRODUCTION

The goal of reinforcement learning (RL) is to learn an optimal policy that lets an agent achieve the
maximum cumulative rewards known as the return (Sutton & Barto, 1998). Reinforcement learning
has been shown to be effective in solving challenging artiﬁcial intelligence tasks such as playing
games (Mnih et al., 2015; Silver et al., 2016) and controlling robots (Deisenroth et al., 2013; Levine
et al., 2016).

Reinforcement learning methods can be classiﬁed into three categories: value-based, policy-based,
and actor-critic methods. Value-based methods learn an optimal policy by ﬁrstly learning a value
function that estimates the expected return. Then, they infer an optimal policy by choosing an
action that maximizes the learned value function. Choosing an action in this way requires solving a
maximization problem which is not trivial for continuous controls. While extensions to continuous
controls were considered recently, they are restrictive since speciﬁc structures of the value function
are assumed (Gu et al., 2016; Amos et al., 2017).

On the other hand, policy-based methods, also called policy search methods (Deisenroth et al.,
2013), learn a parameterized policy maximizing a sample approximation of the expected re-
turn without learning the value function. For instance, policy gradient methods such as REIN-
FORCE (Williams, 1992) use gradient ascent to update the policy parameter so that the probability
of observing high sample returns increases. Compared with value-based methods, policy search
methods are simpler and naturally applicable to continuous problems. Moreover, the sample return
is an unbiased estimator of the expected return and methods such as policy gradients are guaranteed
to converge to a locally optimal policy under standard regularity conditions (Sutton et al., 1999).
However, sample returns usually have high variance and this makes such policy search methods
converge too slowly.

Actor-critic methods combine the advantages of value-based and policy search methods. In these
methods, the parameterized policy is called an actor and the learned value-function is called a critic.

1

Published as a conference paper at ICLR 2018

The goal of these methods is to learn an actor that maximizes the critic. Since the critic is a low vari-
ance estimator of the expected return, these methods often converge much faster than policy search
methods. Prominent examples of these methods are actor-critic (Sutton et al., 1999; Konda & Tsit-
siklis, 2003), natural actor-critic (Peters & Schaal, 2008), trust-region policy optimization (Schul-
man et al., 2015a), and asynchronous advantage actor-critic (Mnih et al., 2016). While their ap-
proaches to learn the actor are different, they share a common property that they only use the value
of the critic, i.e., the zero-th order information, and ignore higher-order ones such as gradients and
Hessians w.r.t. actions of the critic1. To the best of our knowledge, the only actor-critic methods
that use gradients of the critic to update the actor are deterministic policy gradients (DPG) (Silver
et al., 2014) and stochastic value gradients (Heess et al., 2015). However, these two methods do not
utilize the second-order information of the critic.

In this paper, we argue that the second-order information of the critic is useful and should not be
ignored. A motivating example can be seen by comparing gradient ascent to the Newton method: the
Newton method which also uses the Hessian converges to a local optimum in a fewer iterations when
compared to gradient ascent which only uses the gradient (Nocedal & Wright, 2006). This suggests
that the Hessian of the critic can accelerate actor learning which leads to higher data efﬁciency.
However, the computational complexity of second-order methods is at least quadratic in terms of
the number of optimization variables. For this reason, applying second-order methods to optimize
the parameterized actor directly is prohibitively expensive and impractical for deep reinforcement
learning which represents the actor by deep neural networks.

Our contribution in this paper is a novel actor-critic method for continuous controls which we call
guide actor-critic (GAC). Unlike existing methods, the actor update of GAC utilizes the second-
order information of the critic in a computationally efﬁcient manner. This is achieved by separating
actor learning into two steps. In the ﬁrst step, we learn a non-parameterized Gaussian actor that
locally maximizes the critic under a Kullback-Leibler (KL) divergence constraint. Then, the Gaus-
sian actor is used as a guide for learning a parameterized actor by supervised learning. Our analysis
shows that learning the mean of the Gaussian actor is equivalent to performing a second-order update
in the action space where the curvature matrix is given by Hessians of the critic and the step-size is
controlled by the KL constraint. Furthermore, we establish a connection between GAC and DPG
where we show that DPG is a special case of GAC when the Hessians and KL constraint are ignored.

2 BACKGROUND

In this section, we ﬁrstly give a background of reinforcement learning. Then, we discuss existing
second-order methods for policy learning and their issue in deep reinforcement learning.

2.1 REINFORCEMENT LEARNING

We consider discrete-time Markov decision processes (MDPs) with continuous state space S Ď Rds
and continuous action space A Ď Rda . We denote the state and action at time step t P N by
st and at, respectively. The initial state s1 is determined by the initial state density s1 „ ppsq.
At time step t, the agent in state st takes an action at according to a policy at „ πpa|stq and
obtains a reward rt “ rpst, atq. Then, the next state st`1 is determined by the transition function
st`1 „ pps1|st, atq. A trajectory τ “ ps1, a1, r1, s2, . . . q gives us the cumulative rewards or return
t“1 γt´1rpst, atq, where the discount factor 0 ă γ ă 1 assigns different weights to
deﬁned as
rewards given at different time steps. The expected return of π after executing an action a in a state
s can be expressed through the action-value function which is deﬁned as

ř

8

ﬀ

Qπps, aq “ Eπpat|stqtě2,ppst`1|st,atqtě1

γt´1rpst, atq|s1 “ s, a1 “ a

,

(1)

«

8ÿ

t“1

where Ep r¨s denotes the expectation over the density p and the subscript t ě 1 indicates that the
expectation is taken over the densities at time steps t ě 1. We can deﬁne the expected return as
8ÿ

«

ﬀ

J pπq “ Epps1q,πpat|stqtě1,ppst`1|st,atqtě1

γt´1rpst, atq

“ Eppsq,πpa|sq rQπps, aqs .

(2)

1This is different from using the gradient of the critic w.r.t. critic parameters to update the critic itself.

t“1

2

Published as a conference paper at ICLR 2018

The goal of reinforcement learning is to ﬁnd an optimal policy that maximizes the expected return.
The policy search approach (Deisenroth et al., 2013) parameterizes π by a parameter θ P Rdθ and
ﬁnds θ‹ which maximizes the expected return:

θ‹ “ argmax

Eppsq,πθ pa|sq rQπθ ps, aqs .

θ

(3)

(4)

(5)

(7)

Policy gradient methods such as REINFORCE (Williams, 1992) solve this optimization problem by
gradient ascent:

θ Ð θ ` αEppsq,πθ pa|sq r∇θ log πθpa|sqQπθ ps, aqs ,

ř

where α ą 0 is a step-size. In policy search, the action-value function is commonly estimated by the
sample return: Qπθ ps, aq « 1
n,t γt´1rpst,n, at,nq obtained by collecting N trajectories using
N
πθ. The sample return is an unbiased estimator of the action-value function. However, it often has
high variance which leads to slow convergence.
An alternative approach is to estimate the action-value function by a critic denoted by pQps, aq
whose parameter is learned such that pQps, aq « Qπθ ps, aq. By replacing the action-value function
in Eq.(3) with the critic, we obtain the following optimization problem:

θ‹ “ argmax

Eppsq,πθ pa|sq

θ

ı

”
pQps, aq

.

The actor-critic method (Sutton et al., 1999) solves this optimization problem by gradient ascent:
”
∇θ log πθpa|sq

θ Ð θ ` αEppsq,πθ pa|sq

pQps, aq

ı

.

(6)

The gradient in Eq.(6) often has less variance than that in Eq.(4), which leads to faster convergence2.
A large class of actor-critic methods is based on this method (Peters & Schaal, 2008; Mnih et al.,
2016). As shown in these papers, these methods only use the value of the critic to learn the actor.

The deterministic policy gradients (DPG) method (Silver et al., 2014) is an actor-critic method that
uses the ﬁrst-order information of the critic. DPG updates a deterministic actor πθpsq by

θ Ð θ ` αEppsq

”
∇θπθpsq∇a

ı

pQps, aq|a“πθ psq

.

A method related to DPG is the stochastic value gradients (SVG) method (Heess et al., 2015) that
is able to learn a stochastic policy but it requires learning a model of the transition function.

The actor-critic methods described above only use up to the ﬁrst-order information of the critic when
learning the actor and ignore higher-order ones. Below, we discuss existing approaches that utilize
the second-order information by applying second-order optimization methods to solve Eq.(5).

2.2 SECOND-ORDER METHODS FOR POLICY LEARNING

The actor-critic methods described above are ﬁrst-order methods which update the optimization
variables based on the gradient of the objective function. First-order methods are popular in deep
learning thanks to their computational efﬁciency. However, it is known in machine learning that
second-order methods often lead to faster learning because they use the curvature information to
compute a better update direction, i.e., the steepest ascent direction along the curvature3.

The main idea of second-order methods is to rotate the gradient by the inverse of a curvature matrix.
For instance, second-order updates for the actor-critic method in Eq.(6) are in the following form:

θ Ð θ ` αG´1

!
Eppsq,πθ pa|sq

”
∇θ log πθpa|sq

ı)

pQps, aq

,

(8)

2This gradient is a biased estimator of the policy gradient in Eq.(4). However, it is unbiased under some

regularity conditions such as compatible function conditions (Sutton et al., 1999).

3We use the term “second-order methods” in a broad sense here, including quasi-Newton and natural gradi-

ent methods which approximate the curvature matrix by the ﬁrst-order information.

3

Published as a conference paper at ICLR 2018

where G P Rdθ ˆdθ is a curvature matrix. The behavior of second-order methods depend on the
deﬁnition of a curvature matrix. The most well-known second-order method is the Newton method
where its curvature matrix is the Hessian of the objective function w.r.t. the optimization variables:
∇θ log πθpa|sq∇θ log πθpa|sqJ ` ∇2

GHessian “ Eppsq,πθ pa|sq

θ log πθpa|sq

pQps, aq

”`

(9)

ı

˘

.

“

∇θ log πθpa|sq∇θ log πθpa|sqJ

The natural gradient method is another well-known second-order method which uses the Fisher
information matrix (FIM) as the curvature matrix (Amari, 1998):
GFIM “ Eppsq,πθ pa|sq

(10)
Unlike the Hessian matrix, FIM provides information about changes of the policy measured by an
approximated KL divergence: Eppsq rKLpπθpa|sq||πθ1pa|sqqs « pθ ´ θ1qJGFIMpθ ´ θ1q (Kakade,
2001). We can see that GHessian and GFIM are very similar but the former also contains the critic
and the Hessian of the actor while the latter does not. This suggests that the Hessian provides more
information than that in FIM. However, FIM is always positive semi-deﬁnite while the Hessian may
be indeﬁnite. Please see Furmston et al. (2016) for detailed comparisons between the two curvature
matrices in policy search4. Nonetheless, actor-critic methods based on natural gradient were shown
to be very efﬁcient (Peters & Schaal, 2008; Schulman et al., 2015a).

‰

.

We are not aware of existing work that considers second-order updates for DPG or SVG. However,
their second-order updates can be trivially derived. For example, a Newton update for DPG is

θ Ð θ ` αD´1

!
Eppsq

”
∇θπθpsq∇a

pQps, aq|a“πθ psq

ı)

,

where the pi, jq-th entry of the Hessian matrix D P Rdθ ˆdθ is

Dij “ Eppsq

∇2
a

pQps, aq|a“πθ psq

Bπθpsq
Bθj

`

B2πθpsq
BθiBθj

J

∇a

pQps, aq|a“πθ psq

.

(12)

«

J

Bπθpsq
Bθi

(11)

ﬀ

Note that Bπθpsq{Bθ and B2πθpsq{BθBθ1 are vectors since πθpsq is a vector-valued function. In-
terestingly, the Hessian of DPG contains the Hessians of the actor and the critic. In contrast, the
Hessian of the actor-critic method contains the Hessian of the actor and the value of the critic.

Second-order methods are appealing in reinforcement learning because they have high data efﬁ-
ciency. However, inverting the curvature matrix (or solving a linear system) requires cubic compu-
tational complexity in terms of the number of optimization variables. For this reason, the second-
order updates in Eq.(8) and Eq.(11) are impractical in deep reinforcement learning due to a large
number of weight parameters in deep neural networks. In such a scenario, an approximation of
the curvature matrix is required to reduce the computational burden. For instance, Furmston et al.
(2016) proposed to use only diagonal entries of an approximated Hessian matrix. However, this
approximation clearly leads to a loss of useful curvature information since the gradient is scaled but
not rotated. More recently, Wu et al. (2017) proposed a natural actor-critic method that approxi-
mates block-diagonal entries of FIM. However, this approximation corresponds to ignoring useful
correlations between weight parameters in different layers of neural networks.

3 GUIDE ACTOR-CRITIC

In this section, we propose the guide actor-critic (GAC) method that performs second-order updates
without the previously discussed computational issue. Unlike existing methods that directly learn
the parameterized actor from the critic, GAC separates the problem of learning the parameterized
actor into problems of 1) learning a guide actor that locally maximizes the critic, and 2) learning a
parameterized actor based on the guide actor. This separation allows us to perform a second-order
update for the guide actor where the dimensionality of the curvature matrix is independent of the
parameterization of the actor.

We formulate an optimization problem for learning the guide actor in Section 3.1 and present its so-
lution in Section 3.2. Then in Section 3.3 and Section 3.4, we show that the solution corresponds to
performing second-order updates. Finally, Section 3.5 presents the learning step for the parameter-
ized actor using supervised learning. The pseudo-code of our method is provided in Appendix B and
the source code is available at https://github.com/voot-t/guide-actor-critic.

4 Furmston et al. (2016) proposed an approximate Newton method for policy search. Their policy search

method was shown to perform better than methods based on gradient ascent and natural gradient ascent.

4

Published as a conference paper at ICLR 2018

3.1 OPTIMIZATION PROBLEM FOR GUIDE ACTOR

Our ﬁrst goal is to learn a guide actor that maximizes the critic. However, greedy maximization
should be avoided since the critic is a noisy estimate of the expected return and a greedy actor
may change too abruptly across learning iterations. Such a behavior is undesirable in real-world
problems, especially in robotics (Deisenroth et al., 2013).
Instead, we maximize the critic with
additional constraints:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

(13)

where ˜πpa|sq is the guide actor to be learned, πθpa|sq is the current parameterized actor that we
want to improve upon, and pβpsq is the state distribution induced by past trajectories. The objective
function differs from the one in Eq.(5) in two important aspects. First, we maximize for a policy
function ˜π and not for the policy parameter. This is more advantageous than optimizing for a policy
parameter since the policy function can be obtained in a closed form, as will be shown in the next
subsection. Second, the expectation is deﬁned over a state distribution from past trajectories and this
gives us off-policy methods with higher data efﬁciency. The ﬁrst constraint is the Kullback-Leibler
(KL) divergence constraint where KLpppxq||qpxqq “ Eppxq rlog ppxq ´ log qpxqs. The second con-
straint is the Shannon entropy constraint where Hpppxqq “ ´Eppxq rlog ppxqs. The KL constraint
is commonly used in reinforcement learning to prevent unstable behavior due to excessively greedy
update (Peters & Schaal, 2008; Peters et al., 2010; Levine & Koltun, 2013; Schulman et al., 2015a).
The entropy constraint is crucial for maintaining stochastic behavior and preventing premature con-
vergence (Ziebart et al., 2010; Abdolmaleki et al., 2015; Mnih et al., 2016; Haarnoja et al., 2017).
The ﬁnal constraint ensures that the guide actor is a proper probability density. The KL bound
(cid:15) ą 0 and the entropy bound ´8 ă κ ă 8 are hyper-parameters which control the exploration-
exploitation trade-off of the method. In practice, we ﬁx the value of (cid:15) and adaptively reduce the
value of κ based on the current actor’s entropy, as suggested by Abdolmaleki et al. (2015). More
details of these tuning parameters are given in Appendix C.

This optimization problem can be solved by the method of Lagrange multipliers. The solution is

˜πpa|sq 9 πθpa|sq

η‹`ω‹ exp

η‹

˜

¸

,

pQps, aq
η‹ ` ω‹

(14)

where η‹ ą 0 and ω‹ ą 0 are dual variables corresponding to the KL and entropy constraints,
respectively. The dual variable corresponding to the probability density constraint is contained in the
normalization term and is determined by η‹ and ω‹. These dual variables are obtained by minimizing
the dual function:

«

ż

gpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

πθpa|sq

η
η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

.

(15)

All derivations and proofs are given in Appendix A. The solution in Eq.(14) tells us that the
guide actor is obtained by weighting the current actor with pQps, aq.
If we set (cid:15) Ñ 0 then we
have ˜π « πθ and the actor is not updated. On the other hand, if we set (cid:15) Ñ 8 then we have
˜πpa|sq 9 expp

pQps, aq{ω‹q, which is a softmax policy where ω‹ is the temperature parameter.

3.2 LEARNING GUIDE ACTOR

Computing ˜πpa|sq and evaluating gpη, ωq are intractable for an arbitrary πθpa|sq. We overcome
this issue by imposing two assumptions. First, we assume that the actor is the Gaussian distribution:

πθpa|sq “ N pa|φθpsq, Σθpsqq,
where the mean φθpsq and covariance Σθpsq are functions parameterized by a policy parameter
θ. Second, we assume that Taylor’s approximation of pQps, aq is locally accurate up to the second-
order. More concretely, the second-order Taylor’s approximation using an arbitrary action a0 is

(16)

5

Published as a conference paper at ICLR 2018

given by

pQps, aq «

pQps, a0q ` pa ´ a0qJg0psq `

pa ´ a0qJH 0psqpa ´ a0q ` Op}a}3q,

(17)

1
2

pQps, aq|a“a0 are the gradient and Hessian of
where g0psq “ ∇a
the critic w.r.t. a evaluated at a0, respectively. By assuming that the higher order term Op}a}3q is
sufﬁciently small, we can rewrite Taylor’s approximation at a0 as

pQps, aq|a“a0 and H 0psq “ ∇2

a

pQ0ps, aq “

1
2

aJH 0psqa ` aJψ0psq ` ξ0psq,

(18)

pQps, a0q. Note that
where ψ0psq “ g0psq ´ H 0psqa0 and ξ0psq “ 1
H 0psq, ψ0psq, and ξ0psq depend on the value of a0 and do not depend on the value of a. This
dependency is explicitly denoted by the subscript. The choice of a0 will be discussed in Section 3.3.

0 H 0psqa0 ´ aJ

0 g0psq `

2 aJ

Substituting the Gaussian distribution and Taylor’s approximation into Eq.(14) yields another Gaus-
sian distribution ˜πpa|sq “ N pa|φ`psq, Σ`psqq, where the mean and covariance are given by

φ`psq “ F ´1psqLpsq, Σ`psq “ pη‹ ` ω‹qF ´1psq.

The matrix F psq P Rdaˆda and vector Lpsq P Rda are deﬁned as
θ psq ´ H 0psq, Lpsq “ η‹Σ´1

F psq “ η‹Σ´1

θ psqφθpsq ` ψ0psq.

The dual variables η‹ and ω‹ are obtained by minimizing the following dual function:

(19)

(20)

pgpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

«

d

ﬀ

|2πpη ` ωqF ´1
|2πΣθpsq|

η
η`ω

η psq|

`

Epβ psq

LηpsqJF ´1

η psqLηpsq ´ ηφθpsqJΣ´1

` const,

(21)

‰
θ psqφθpsq

“

1
2

where F ηpsq and Lηpsq are deﬁned similarly to F psq and Lpsq but with η instead of η‹.

The practical advantage of using the Gaussian distribution and Taylor’s approximation is that the
guide actor can be obtained in a closed form and the dual function can be evaluated through matrix-
vector products. The expectation over pβpsq can be approximated by e.g., samples drawn from a
replay buffer (Mnih et al., 2015). We require inverting F ηpsq to evaluate the dual function. However,
these matrices are computationally cheap to invert when the dimension of actions is not large.

As shown in Eq.(19), the mean and covariance of the guide actor is computed using both the gradient
and Hessian of the critic. Yet, these computations do not resemble second-order updates discussed
previously in Section 2.2. Below, we show that for a particular choice of a0, the mean computation
corresponds to a second-order update that rotates gradients by a curvature matrix.

3.3 GUIDE ACTOR LEARNING AS SECOND-ORDER OPTIMIZATION

For now we assume that the critic is an accurate estimator of the true action-value function. In this
case, the quality of the guide actor depends on the accuracy of sample approximation in pgpη, ωq and
the accuracy of Taylor’s approximation. To obtain an accurate Taylor’s approximation of pQps, aq
using an action a0, the action a0 should be in the vicinity of a. However, we did not directly use
pQps, aqq (see Eq.(14)).
any individual a to compute the guide actor, but we weight πθpa|sq by expp
Thus, to obtain an accurate Taylor’s approximation of the critic, the action a0 needs to be similar to
actions sampled from πθpa|sq. Based on this observation, we propose two approaches to perform
Taylor’s approximation.

Taylor’s approximation around the mean. In this approach, we perform Taylor’s approximation
using the mean of πθpa|sq. More speciﬁcally, we use a0 “ Eπθ pa|sq ras “ φθpsq for Eq.(18). In
this case, we can show that the mean update in Eq.(19) corresponds to performing a second-order
update in the action space to maximize pQps, aq:

φ`psq “ φθpsq ` F ´1
φθ

psq∇a

pQps, aq|a“φθ psq,

(22)

6

Published as a conference paper at ICLR 2018

θ psq ´ H φθ psq and H φθ psq “ ∇2
a

pQps, aq|a“φθ psq. This equivalence can
where F φθ psq “ η‹Σ´1
be shown by substitution and the proof is given in Appendix A.2. This update equation reveals that
the guide actor maximizes the critic by taking a step in the action space similarly to the Newton
method. However, the main difference lies in the curvature matrix where the Newton method uses
Hessians H φθ psq but we use a damped Hessian F φθ psq. The damping term η‹Σ´1
θ psq corresponds
to the effect of the KL constraint and can be viewed as a trust-region that controls the step-size. This
damping term is particularly important since Taylor’s approximation is accurate only locally and we
should not take a large step in each update (Nocedal & Wright, 2006).

Expectation of Taylor’s approximations.
Instead of using Taylor’s approximation around the
mean, we may use an expectation of Taylor’s approximation over the distribution. More concretely,
we deﬁne rQps, aq to be an expectation of pQ0ps, aq over πθpa0|sq:

rQps, aq “

1
2

aJEπθ pa0|sq rH 0psqs a ` aJEπθ pa0|sq rψ0psqs ` Eπθ pa0|sq rξ0psqs .

(23)

pQps, aq|a“a0 s and the expectation is computed w.r.t.
Note that Eπθ pa0|sqrH 0psqs “ Eπθ pa0|sqr∇2
a
the distribution πθ of a0. We use this notation to avoid confusion even though πθpa0|sq and πθpa|sq
are the same distribution. When Eq.(23) is used, the mean update does not directly correspond
to any second-order optimization step. However, under an (unrealistic) independence assumption
Eπθ pa0|sqrH 0psqa0s “ Eπθ pa0|sqrH 0psqsEπθ pa0|sqra0s, we can show that the mean update corre-
sponds to the following second-order optimization step:

φ`psq “ φθpsq ` Eπθ pa0|sq rF 0psqs´1 Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

,

(24)

where Eπθ pa0|sq rF 0psqs “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs. Interestingly, the mean is updated
by rotating an expected gradient using an expected Hessians. In practice, the expectations can be
approximated using sampled actions ta0,iuS
i“1 „ πθpa|sq. We believe that this sampling can be
advantageous for avoiding local optima. Note that when the expectation is approximated by a single
sample a0 „ πθpa|sq, we obtain the update in Eq.(24) regardless of the independence assumption.
In the remainder, we use F psq to denote both of F φθ psq and Eπθ pa0|sq rF 0psqs, and use Hpsq to
denote both of H φθ psq and Eπθ pa0|sqrH 0psqs. In the experiments, we use GAC-0 to refer to GAC
with Taylor’s approximation around the mean, and we use GAC-1 to refer to GAC with Taylor’s
approximation by a single sample a0 „ πθpa|sq.

3.4 GAUSS-NEWTON APPROXIMATION OF HESSIAN

The covariance update in Eq.(19) indicates that F psq “ η‹Σ´1
θ psq ´ Hpsq needs to be positive
deﬁnite. The matrix F psq is guaranteed to be positive deﬁnite if the Hessian matrix Hpsq is negative
semi-deﬁnite. However, this is not guaranteed in practice unless pQps, aq is a concave function in
terms of a. To overcome this issue, we ﬁrstly consider the following identity:
pQps, aq∇a

pQps, aq “ ´∇a

pQps, aqq expp´

pQps, aqJ ` ∇2

pQps, aqq.

Hpsq “ ∇2
a

a expp

(25)

The proof is given in Appendix A.3. The ﬁrst term is always negative semi-deﬁnite while the second
term is indeﬁnite. Therefore, a negative semi-deﬁnite approximation of the Hessian can be obtained
as

H 0psq « ´

pQps, aq∇a

pQps, aqJ

”
∇a

ı

.

a“a0

(26)

pQps, aqq and it will be small for high values
The second term in Eq.(25) is proportional to expp´
of pQps, aq. This implies that the approximation should gets more accurate as the policy approach
a local maxima of pQps, aq. We call this approximation Gauss-Newton approximation since it is
similar to the Gauss-Newton approximation for the Newton method (Nocedal & Wright, 2006).

3.5 LEARNING PARAMETERIZED ACTOR

The second step of GAC is to learn a parameterized actor that well represents the guide actor. Below,
we discuss two supervised learning approaches for learning a parameterized actor.

7

Published as a conference paper at ICLR 2018

3.5.1 FULLY-PARAMETERIZED GAUSSIAN POLICY

Since the guide actor is a Gaussian distribution with a state-dependent mean and covariance, a
natural choice for the parameterized actor is again a parameterized Gaussian distribution with a
state-dependent mean and covariance: πθpa|sq “ N pa|φθpsq, Σθpsqq. The parameter θ can be
learned by minimizing the expected KL divergence to the guide actor:

LKLpθq “ Epβ psq rKL pπθpa|sq||˜πpa|sqqs

„



TrpF psqΣθpsqq
η‹ ` ω‹

“ Epβ psq
”
}φθpsq ´ φ`psq}2

´ log |Σθpsq|
ı

`

LWpθq
η‹ ` ω‹ ` const,

(27)

where LWpθq “ Epβ psq
which only depends on θ of the mean function. The const term does not depend on θ.

is the weighted-mean-squared-error (WMSE)

F psq

Minimizing the KL divergence reveals connections between GAC and deterministic policy gradients
(DPG) (Silver et al., 2014). By computing the gradient of the WMSE, it can be shown that

∇θLWpθq
2

“ ´Epβ psq

ı

pQps, aq|a“φθ psq

”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

psq∇a

psq∇a

”
∇θφθpsq∇a
ı

` Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

ı

.

` η‹Epβ psq

´ η‹Epβ psq

ı

pQps, aq|a“φ`psq

(28)

The proof is given in Appendix A.4. The negative of the ﬁrst term is precisely equivalent to DPG.
Thus, updating the mean parameter by minimizing the KL loss with gradient descent can be regarded
as updating the mean parameter with biased DPG where the bias terms depend on η‹. We can verify
pQps, aq|a“φ`psq “ 0 when η‹ “ 0 and this is the case of (cid:15) Ñ 8. Thus, all bias terms vanish
that ∇a
when the KL constraint is ignored and the mean update of GAC coincides with DPG. However,
unlike DPG which learns a deterministic policy, we can learn both the mean and covariance in GAC.

3.5.2 GAUSSIAN POLICY WITH PARAMETERIZED MEAN

While a state-dependent parameterized covariance function is ﬂexible, we observe that learning
performance is sensitive to the initial parameter of the covariance function. For practical pur-
poses, we propose using a parametrized Gaussian distribution with state-independent covariance:
πθpa|sq “ N pa|φθpsq, Σq. This class of policies subsumes deterministic policies with additive in-
dependent Gaussian noise for exploration. To learn θ, we minimize the mean-squared-error (MSE):
“

(29)
‰
For the covariance, we use the average of the guide covariances: Σ “ pη‹ ` ω‹qEpβ psq
.
For computational efﬁciency, we execute a single gradient update in each learning iteration instead
of optimizing this loss function until convergence.

F ´1psq

}φθpsq ´ φ`psq}2
2

LMpθq “

Epβ psq

1
2

“

‰

.

Similarly to the above analysis, the gradient of the MSE w.r.t. θ can be expanded and rewritten into

∇θLMpθq “ Epβ psq

”
∇θφθpsqH ´1psq

´

∇a

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

.

(30)

¯ı

Again, the mean update of GAC coincides with DPG when we minimize the MSE and set η‹ “ 0
and Hpsq “ ´I where I is the identity matrix. We can also substitute these values back into
Eq.(22). By doing so, we can interpret DPG as a method that performs ﬁrst-order optimization in
the action space:

φ`psq “ φθpsq ` ∇a

pQps, aq|a“φθ psq,

(31)

and then uses the gradient in Eq.(30) to update the policy parameter. This interpretation shows that
DPG is a ﬁrst-order method that only uses the ﬁrst-order information of the critic for actor learning.
Therefore in principle, GAC, which uses the second-order information of the critic, should learn
faster than DPG.

8

Published as a conference paper at ICLR 2018

3.6 POLICY EVALUATION FOR CRITIC

Beside actor learning, the performance of actor-critic methods also depends on the accuracy of the
critic. We assume that the critic pQν ps, aq is represented by neural networks with a parameter ν.
We adopt the approach proposed by Lillicrap et al. (2015) with some adjustment to learn ν. More
concretely, we use gradient descent to minimize the squared Bellman error with a slowly moving
target critic:

ν Ð ν ´ α∇ν Epβ psq,βpa|sq,pps1|s,aq

pQν ps, aq ´ y

(32)

„´



¯

2

,

pQ¯ν ps1, a1qs is computed by
where α ą 0 is the step-size. The target value y “ rps, aq ` γEπpa1|s1qr
the target critic pQ¯ν ps1, a1q whose parameter ¯ν is updated by ¯ν Ð τ ν ` p1 ´ τ q¯ν for 0 ă τ ă 1.
As suggested by Lillicrap et al. (2015), the target critic improves the learning stability and we set
τ “ 0.001 in experiments. The expectation for the squared error is approximated using mini-batch
samples tpsn, an, rn, s1
n“1 drawn from a replay buffer. The expectation over the current actor
πpa1|s1q is approximated using samples ta1
n. We do not use a
target actor to compute y since the KL upper-bound already constrains the actor update and a target
actor will further slow it down. Note that we are not restricted to this evaluation method and more
efﬁcient methods such as Retrace (Munos et al., 2016) can also be used.

m“1 „ πθpa1|s1

nq for each s1

n,muM

nquN

pQν ps, aq and its outer product for the Gauss-Newton approxi-
Our method requires computing ∇a
mation. The computational complexity of the outer product operation is Opd2
aq and is inexpensive
when compared to the dimension of ν. For a linear-in-parameter model pQν ps, aq “ νJµps, aq,
the gradient can be efﬁciently computed for common choices of the basis function µ such as the
Gaussian function. For deep neural network models, the gradient can be computed by the automatic-
differentiation (Goodfellow et al., 2016) where its cost depends on the network architecture.

4 RELATED WORK

Besides the connections to DPG, our method is also related to existing methods as follows.

A similar optimization problem to Eq.(13) was considered by the model-free trajectory optimization
(MOTO) method (Akrour et al., 2016). Our method can be viewed as a non-trivial extension of
MOTO with two signiﬁcant novelties. First, MOTO learns a sequence of time-dependent log-linear
Gaussian policies πtpa|sq “ N pa|Bts`bt, Σtq, while our method learns a log-nonlinear Gaussian
policy. Second, MOTO learns a time-dependent critic given by pQtps, aq “ 1
2 aJCta ` aJDts `
aJct `ξtpsq and performs policy update with these functions. In contrast, our method learns a more
complex critic and performs Taylor’s approximation in each training step.

Besides MOTO, the optimization problem also resembles that of trust region policy optimization
(TRPO) (Schulman et al., 2015a). TRPO solves the following optimization problem:

max
θ1

Epπθ psq,πθ1 pa|sq

”
ı
pQps, aq

subject to Epπθ psq rKLpπθpa|sq||πθ1pa|sqqs ď (cid:15),

(33)

where pQps, aq may be replaced by an estimate of the advantage function (Schulman et al., 2015b).
There are two major differences between the two problems. First, TRPO optimizes the policy param-
eter while we optimize the guide actor. Second, TRPO solves the optimization problem by conjugate
gradient where the KL divergence is approximated by the Fisher information matrix, while we solve
the optimization problem in a closed form with a quadratic approximation of the critic.

Our method is also related to maximum-entropy RL (Ziebart et al., 2010; Azar et al., 2012; Haarnoja
et al., 2017; Nachum et al., 2017), which maximizes the expected cumulative reward with an addi-
Epπpsq rrpst, atq ` αHpπpat|stqqs, where α ą 0 is a trade-off parame-
tional entropy bonus:
ter. The optimal policy in maximum-entropy RL is the softmax policy given by

8
t“1

ř

π‹
MaxEntpa|sq “ exp

ˆ

Q‹

softps, aq ´ V ‹
α

softpsq

˙

ˆ

9 exp

˙

,

Q‹

softps, aq
α

(34)

9

Published as a conference paper at ICLR 2018

softps, aq and V ‹

where Q‹
tively (Haarnoja et al., 2017; Nachum et al., 2017). For a policy π, these are deﬁned as
softps, aq “ rps, aq ` γEpps1|s,aq

softpsq are the optimal soft action-value and state-value functions, respec-
“

(35)

Qπ

,

ż

V π
softpsq “ α log

exp

ˆ

Qπ

‰
V π
softps1q
˙
softps, aq
α

da.

(36)

The softmax policy and the soft state-value function in maximum-entropy RL closely resemble the
guide actor in Eq.(14) when η‹ “ 0 and the log-integral term in Eq.(15) when η “ 0, respectively,
except for the deﬁnition of action-value functions. To learn the optimal policy of maximum-entropy
RL, Haarnoja et al. (2017) proposed soft Q-learning which uses importance sampling to compute
the soft value functions and approximates the intractable policy using a separate policy function.
Our method largely differs from soft Q-learning since we use Taylor’s approximation to convert the
intractable integral into more convenient matrix-vector products.

The idea of ﬁrstly learning a non-parameterized policy and then later learning a parameterized policy
by supervised learning was considered previously in guided policy search (GPS) (Levine & Koltun,
2013). However, GPS learns the guide policy by trajectory optimization methods such as an iterative
linear-quadratic Gaussian regulator (Li & Todorov, 2004), which requires a model of the transition
function. In contrast, we learn the guide policy via the critic without learning the transition function.

5 EXPERIMENTAL RESULTS

We evaluate GAC on the OpenAI gym platform (Brockman et al., 2016) with the Mujoco Physics
simulator (Todorov et al., 2012). The actor and critic are neural networks with two hidden layers of
400 and 300 units, as described in Appendix C. We compare GAC-0 and GAC-1 against deep DPG
(DDPG) (Lillicrap et al., 2015), Q-learning with a normalized advantage function (Q-NAF) (Gu
et al., 2016), and TRPO (Schulman et al., 2015a;b). Figure 1 shows the learning performance on
9 continuous control tasks. Overall, both GAC-0 and GAC-1 perform comparably with existing
methods and they clearly outperform the other methods in Half-Cheetah.

The performance of GAC-0 and GAC-1 is comparable on these tasks, except on Humanoid where
GAC-1 learns faster. We expect GAC-0 to be more stable and reliable but easier to get stuck at
local optima. On the other hand, the randomness introduced by GAC-1 leads to high variance
approximation but this could help escape poor local optima. We conjecture GAC-S that uses S ą 1
samples for the averaged Taylor’s approximation should outperform both GAC-0 and GAC-1. While
this is computationally expensive, we can use parallel computation to reduce the computation time.

The expected returns of both GAC-0 and GAC-1 have high ﬂuctuations on the Hopper and Walker2D
tasks when compared to TRPO as can be seen in Figure 1g and Figure 1h. We observe that they can
learn good policies for these tasks in the middle of learning. However, the policies quickly diverge
to poor ones and then they are quickly improved to be good policies again. We believe that this
happens because the step-size F ´1psq “
of the guide actor in Eq. (22) can
be very large near local optima for Gauss-Newton approximation. That is, the gradients near local
pQps, aqJ
optima have small magnitude and this makes the approximation Hpsq “ ∇a
small as well. If η‹Σ´1 is also relatively small then the matrix F ´1psq can be very large. Thus,
under these conditions, GAC may use too large step sizes to compute the guide actor and this results
in high ﬂuctuations in performance. We expect that this scenario can be avoided by reducing the KL
bound (cid:15) or adding a regularization constant to the Gauss-Newton approximation.

˘
η‹Σ´1 ´ Hpsq

pQps, aq∇a

´1

`

Table 1 in Appendix C shows the wall-clock computation time. DDPG is computationally the most
efﬁcient method on all tasks. GAC has low computation costs on tasks with low dimensional actions
and its cost increases as the dimensionality of action increases. This high computation cost is due to
the dual optimization for ﬁnding the step-size parameters η and ω. We believe that the computation
cost of GAC can be signiﬁcantly reduced by letting η and ω be external tuning parameters.

6 CONCLUSION AND FUTURE WORK

Actor-critic methods are appealing for real-world problems due to their good data efﬁciency and
learning speed. However, existing actor-critic methods do not use second-order information of the

10

Published as a conference paper at ICLR 2018

(a) Inverted-Pend.

(b) Inv-Double-Pend.

(c) Reacher

(d) Swimmer

(e) Half-Cheetah

(f) Ant

(g) Hopper

(h) Walker2D

(i) Humanoid

Figure 1: Expected returns averaged over 10 trials. The x-axis indicates training time steps. The y-
axis indicates averaged return and higher is better. More clear ﬁgures are provided in Appendix C.2.

critic. In this paper, we established a novel framework that distinguishes itself from existing work
by utilizing Hessians of the critic for actor learning. Within this framework, we proposed a prac-
tical method that uses Gauss-Newton approximation instead of the Hessians. We showed through
experiments that our method is promising and thus the framework should be further investigated.

Our analysis showed that the proposed method is closely related to deterministic policy gradients
(DPG). However, DPG was also shown to be a limiting case of the stochastic policy gradients when
the policy variance approaches zero (Silver et al., 2014). It is currently unknown whether our frame-
work has a connection to the stochastic policy gradients as well, and ﬁnding such a connection is
our future work.

Our main goal in this paper was to provide a new actor-critic framework and we do not claim that our
method achieves the state-of-the-art performance. However, its performance can still be improved in
many directions. For instance, we may impose a KL constraint for a parameterized actor to improve
its stability, similarly to TRPO (Schulman et al., 2015a). We can also apply more efﬁcient policy
evaluation methods such as Retrace (Munos et al., 2016) to achieve better critic learning.

ACKNOWLEDGMENTS

MS was partially supported by KAKENHI 17H00757.

REFERENCES

Abbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, Nuno Lau, Lu´ıs Paulo Reis, and Gerhard Neu-
In Advances in Neural Information

mann. Model-Based Relative Entropy Stochastic Search.
Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki. Model-Free Trajec-
tory Optimization for Reinforcement Learning. In Proceedings of the 33nd International Confer-
ence on Machine Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Shun-ichi Amari. Natural Gradient Works Efﬁciently in Learning. Neural Computation, 10(2):

251–276, 1998.

Brandon Amos, Lei Xu, and J. Zico Kolter. Input Convex Neural Networks. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
August 6-11, 2017, Sydney, Australia, 2017.

Mohammad Gheshlaghi Azar, Vicenc¸ G´omez, and Hilbert J. Kappen. Dynamic Policy Program-

ming. Journal of Machine Learning Research, 13:3207–3245, 2012.

11

Published as a conference paper at ICLR 2018

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.

Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on Policy Search for Robotics.

Foundations and Trends in Robotics, 2(1-2):1–142, 2013.

Thomas Furmston, Guy Lever, and David Barber. Approximate Newton Methods for Policy Search
in Markov Decision Processes. Journal of Machine Learning Research, 17(227):1–51, 2016.

Xavier Glorot and Yoshua Bengio. Understanding the Difﬁculty of Training Deep Feedforward
Neural Networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the 13th In-
ternational Conference on Artiﬁcial Intelligence and Statistics, May 13-15, 2010, Sardinia, Italy,
2010.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous Deep Q-Learning
with Model-based Acceleration. In Proceedings of the 33nd International Conference on Machine
Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with
Deep Energy-Based Policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, August 6-11, 2017, Sydney, Australia, 2017.

Nicolas Heess, Gregory Wayne, David Silver, Timothy P. Lillicrap, Tom Erez, and Yuval Tassa.
In Advances in Neural

Learning Continuous Control Policies by Stochastic Value Gradients.
Information Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Sham Kakade. A Natural Policy Gradient. In Advances in Neural Information Processing Systems

14, December 3-8, 2001, Vancouver, British Columbia, Canada, 2001.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. CoRR,

abs/1412.6980, 2014.

Vijay R. Konda and John N. Tsitsiklis. On Actor-Critic Algorithms. SIAM Journal on Control and

Optimization, 42(4):1143–1166, April 2003.

Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the 30th International

Conference on Machine Learning, June 16-21, 2013, Atlanta, GA, USA, 2013.

Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep
Visuomotor Policies. Journal of Machine Learning Research, 17(1):1334–1373, January 2016.
ISSN 1532-4435.

Weiwei Li and Emanuel Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear Bio-
logical Movement Systems. In Proceedings of the 1st International Conference on Informatics in
Control, Automation and Robotics, August 25-28, 2004, Set´ubal, Portugal, pp. 222–229, 2004.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. CoRR,
abs/1509.02971, 2015.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement
Learning. Nature, 518(7540):529–533, February 2015. ISSN 00280836.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement
In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
Learning.
International Conference on Machine Learning, June 19-24, 2016, New York City, NY, USA,
2016.

12

Published as a conference paper at ICLR 2018

R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and Efﬁcient Off-
Policy Reinforcement Learning. In Advances in Neural Information Processing Systems 29, De-
cember 5-10, 2016, Barcelona, Spain, 2016.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the Gap Between
Value and Policy Based Reinforcement Learning. In Advances in Neural Information Processing
Systems 30, 4-9 December 2017, Long Beach, CA, USA, 2017.

Jorge Nocedal and Stephen J. Wright. Numerical Optimization, Second Edition. World Scientiﬁc,

2006.

Jan Peters and Stefan Schaal. Natural Actor-Critic. Neurocomputing, 71(7-9):1180–1190, 2008.

Jan Peters, Katharina M¨ulling, and Yasemin Altun. Relative Entropy Policy Search. In Proceedings
of the 24th AAAI Conference on Artiﬁcial Intelligence, July 11-15, 2010, Atlanta, Georgia, USA,
2010.

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region
Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning,
July 6-11, 2015, Lille, France, pp. 1889–1897, 2015a.

John Schulman, Philipp Moritz, Sergey Levine, Michael

High-Dimensional Continuous Control Using Generalized Advantage Estimation.
abs/1506.02438, 2015b.

I. Jordan, and Pieter Abbeel.
CoRR,

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic Policy Gradient Algorithms. In Proceedings of the 31st International Conference
on Machine Learning, June 21-26, 2014, Beijing, China, 2014.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game
of Go with Deep Neural Networks and Tree Search. Nature, 529(7587):484–489, 2016.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning - an Introduction. Adaptive

computation and machine learning. MIT Press, 1998.

Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy Gradi-
ent Methods for Reinforcement Learning with Function Approximation. In Advances in Neural
Information Processing Systems 12, November 29 - December 4, 1999, Colorado, USA, 1999.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics Engine for Model-Based Control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012,
Vilamoura, Algarve, Portugal, 2012.

Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforce-

ment Learning. Machine Learning, 8(3):229–256, 1992. doi: 10.1007/BF00992696.

Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable Trust-Region
Method for Deep Reinforcement Learning using Kronecker-factored Approximation. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, 4-9 December 2017, Long Beach, CA, USA,
2017.

Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling Interaction via the Principle
of Maximum Causal Entropy. In Proceedings of the 27th International Conference on Machine
Learning, June 21-24, 2010, Haifa, Israel, 2010.

13

Published as a conference paper at ICLR 2018

A DERIVATIONS AND PROOFS

A.1 DERIVATION OF SOLUTION AND DUAL FUNCTION OF GUIDE ACTOR

The solution of the optimization problem:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

can be obtained by the method of Lagrange multipliers. The derivation here follows the derivation of
similar optimization problems by Peters et al. (2010) and Abdolmaleki et al. (2015). The Lagrangian
of this optimization problem is

Lprπ, η, ω, νq “ Epβ psq,˜πpa|sq

` ηp(cid:15) ´ Epβ psq rKLp˜πpa|sq||πθpa|sqqsq

ı
”
pQps, aq

` ωpEpβ psq rHp˜πpa|sqqs ´ κq ` νpEpβ psqrπpa|sq ´ 1q,

(38)

where η, ω, and ν are the dual variables. Then, by taking derivative of L w.r.t. rπ we obtain

„ż ´

¯



BrπL “ Epβ psq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq

da

´ pη ` ω ´ νq.

(39)

We set this derivation to zero in order to obtain

0 “ Epβ psq

„ż ´

¯
pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq



da

´ pη ` ω ´ νq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq ´ pη ` ω ´ νq.

“

Then the solution is given by

rπpa|sq “ πθpa|sq

η
η`ω exp

9 πθpa|sq

η
η`ω exp

¸

ˆ

exp

´

˙

η ` ω ´ ν
η ` ω

˜

˜

pQps, aq
η ` ω
pQps, aq
η ` ω

¸

.

(37)

(40)

(41)

(42)

To obtain the dual function gpη, ωq, we substitute the solution to the constraint terms of the La-
grangian and this gives us

Lpη, ω, νq “ Epβ psq,rπpa|sq

ı

”
pQps, aq

«

´ pη ` ωqEpβ psq,rπpa|sq

pQps, aq
η ` ω

`
`

η
log πθpa|sq ´
η ` ω
˘
Epβ ,rπpa|sq ´ 1

ﬀ

η ` ω ´ ν
η ` ω

` ηEpβ psq,rπpa|sq rlog πθpa|sqs ` ν

` η(cid:15) ´ ωκ.

(43)

After some calculation, we obtain

Lpη, ω, νq “ η(cid:15) ´ ωκ ` Epβ psq rη ` ω ´ νs
«ż

“ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

η

πθpa|sq

η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

“ gpη, ωq,

(44)

where in the second line we use the fact that expp´ η`ω´ν

η`ω q is the normalization term of rπpa|sq.

14

Published as a conference paper at ICLR 2018

A.2 PROOF OF SECOND-ORDER OPTIMIZATION IN ACTION SPACE

Firstly, we show that GAC performs second-order optimization in the action space when Taylor’s
approximation is performed with a0 “ Eπpa|sq ras “ φθpsq. Recall that Taylor’s approximation
with φθ is given by

1
2

pQps, aq “
pQps, aq|a“φθ psq ´ H φθ psqφθpsq. By substituting ψφθ

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq,

(45)

psq into Lpsq “

where ψφθ
η‹Σ´1

psq “ ∇a

θ psqφθpsq ´ ψθpsq, we obtain

Lpsq “ η‹Σ´1
“ pη‹Σ´1

θ psqφθpsq ` ∇a
θ psq ´ H φθ psqqφθpsq ` ∇a
pQps, aq|a“φθ psq.

pQps, aq|a“φθ psq ´ H φθ psqφθpsq

pQps, aq|a“φθ psq

“ F psqφθpsq ` ∇a
Therefore, the mean update is equivalent to

φ`psq “ F ´1psqLpsq

“ φθpsq ` F ´1psq∇a

pQps, aq|a“φθ psq,

which is a second-order optimization step with a curvature matrix F psq “ η‹Σ´1

θ psq ´ H φθ psq.

Lpsq “ η‹Σ´1

Similarly, for the case where a set of samples ta0u „ πθpa0|sq “ N pa0|φθpsq, Σpsqq is used to
compute the averaged Taylor’s approximation, we obtain
θ psqφθpsq ` Eπθ pa0|sq

”
∇a
Then, by assuming that Eπθ pa0|sq rH 0psqa0psqs “ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s, we obtain
”
∇a
”
∇a

pQps, aq|a“a0
pQps, aq|a“a0

´ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s

´ Eπθ pa0|sq rH 0psqa0psqs .

θ psqφθpsq ` Eπθ pa0|sq

θ psqφθpsq ` Eπθ pa0|sq

pQps, aq|a“a0

Lpsq “ η‹Σ´1

“ η‹Σ´1

(48)

ı

ı

ı

´ Eπθ pa0|sq rH 0s φθpsq
ı

”
∇a

pQps, aq|a“a0

“ pη‹Σ´1

θ psq ´ Eπθ pa0|sq rH 0psqsqφθpsq ` Eπθ pa0|sq

“ F psqφθpsq ` Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

.

Therefore, we have a second-order optimization step

φ`psq “ φθpsq ` F ´1psqEπθ pa0|sq

(50)
where F ´1psq “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs is a curvature matrix. As described in
the main paper, this interpretation is only valid when the equality Eπθ pa0|sq rH 0psqa0psqs “
Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s holds. While this equality does not hold in general, it holds when
only one sample a0 „ πθpa0|sq is used. Nonetheless, we can still use the expectation of Taylor’s
approximation to perform policy update regardless of this assumption.

,

pQps, aq|a“a0

”
∇a

ı

(46)

(47)

(49)

A.3 PROOF OF GAUSS-NEWTON APPROXIMATION

Let f ps, aq “ expp

pQps, aqq, then the Hessian Hpsq “ ∇2

pQps, aq can be expressed as

a

Hpsq “ ∇a r∇a log f ps, aqs
“
∇af ps, aqf ps, aq´1
∇af ps, aq´1

“ ∇a

`

‰

˘

J

“ ∇af ps, aq
“ ∇af ps, aq∇f f ps, aq´1 p∇af ps, aqqJ ` ∇2
“ ´∇af ps, aqf ps, aq´2 p∇af ps, aqqJ ` ∇2

` ∇2

af ps, aqf ps, aq´1

af ps, aqf ps, aq´1
af ps, aqf ps, aq´1

`

˘ `

∇af ps, aqf ps, aq´1

“ ´
“ ´∇a log f ps, aq∇a log f ps, aqJ ` ∇2
a expp
“ ´∇a

pQps, aqJ ` ∇2

pQps, aq∇a

` ∇2
af ps, aqf ps, aq´1
pQps, aqq expp´

∇af ps, aqf ps, aq´1

pQps, aqq,

˘

J

af ps, aqf ps, aq´1

(51)

15

Published as a conference paper at ICLR 2018

which concludes the proof.

Beside Gauss-Newton approximation, an alternative approach is to impose a special structure on
pQps, aq so that Hessians are always negative semi-deﬁnite. In literature, there exists two special
structures that satisﬁes this requirement.

Normalized advantage function (NAF) (Gu et al., 2016): NAF represents the critic by a quadratic
function with a negative curvature:

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(52)

where a negative-deﬁnite matrix-valued function W psq, a vector-valued function bpsq and a scalar-
valued function V psq are parameterized functions whose their parameters are learned by policy
evaluation methods such as Q-learning (Sutton & Barto, 1998). With NAF, negative deﬁnite Hes-
sians can be simply obtained as Hpsq “ W psq. However, a signiﬁcant disadvantage of NAF is that
it assumes the action-value function is quadratic regardless of states and this is generally not true for
most reward functions. Moreover, the Hessians become action-independent even though the critic is
a function of actions.

Input convex neural networks (ICNNs) (Amos et al., 2017): ICNNs are neural networks with
special structures which make them convex w.r.t. their inputs. Since Hessians of concave functions
are always negative semi-deﬁnite, we may use ICNNs to represent a negative critic and directly use
its Hessians. However, similarly to NAF, ICNNs implicitly assume that the action-value function is
concave w.r.t. actions regardless of states and this is generally not true for most reward functions.

A.4 GRADIENT OF THE LOSS FUNCTIONS FOR LEARNING PARAMETERIZED ACTOR

We ﬁrst consider
is
N pa|φ`psq, Σ`psqq and the current actor is N pa|φθpsq, Σθpsqq. Taylor’s approximation of
pQps, aq at a0 “ φθpsq is

loss function where the guide actor

the weight mean-squared-error

pQps, aq “

1
2

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq.

(53)

By assuming that H φθ psq is strictly negative deﬁnite5, we can take a derivative of this approxima-
tion w.r.t. a and set it to zero to obtain a “ H ´1
psq. Replacing a by
φθ
φθpsq and φ`psq yields

pQps, aq ´ H ´1
φθ

psqψφθ

psq∇a

φθpsq “ H ´1
φθ
φ`psq “ H ´1
φθ

psq∇a

psq∇a

pQps, aq|a“φθ psq ´ H ´1
pQps, aq|a“φ`psq ´ H ´1

φθ

φθ

psqψφθ
psqψφθ

psq,

psq.

Recall that the weight mean-squared-error is deﬁned as

LWpθq “ Epβ psq

”
}φθpsq ´ φ`psq}2

F psq

ı

.

(54)

(55)

(56)

5This can be done by subtracting a small positive value to the diagonal entries of Gauss-Newton approxi-

mation.

16

Published as a conference paper at ICLR 2018

Then, we consider its gradient w.r.t. θ as follows:

∇θLWpθq “ 2Epβ psq
“ 2Epβ psq
“ 2Epβ psq

“

“

“

`

∇θφθpsqF psq
∇θφθpsqpη‹Σ´1
∇θφθpsqpη‹Σ´1
´
H ´1
φθ

ˆ

˘‰

`

φθpsq ´ φ`psq
θ psq ´ H φθ psqq
θ psq ´ H φθ psqq

˘‰

φθpsq ´ φ`psq

¯ı

psq∇a

pQps, aq|a“φθ psq ´ H ´1

φθ

psq∇a

pQps, aq|a“φ`psq

“ 2η‹Epβ psq

´

psq

”
θ psqH ´1
∇θφθpsqΣ´1
φθ
´
”
∇θφθpsq

` 2Epβ psq
”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

∇a
pQps, aq|a“φθ psq

pQps, aq|a“φθ psq ´ ∇a
pQps, aq|a“φθ psq
”
∇θφθpsq∇a
ı

∇a
pQps, aq|a“φ`psq ´ ∇a
ı
` 2Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

psq∇a

psq∇a

ı

.

` 2η‹Epβ psq

´ 2η‹Epβ psq

“ ´2Epβ psq

pQps, aq|a“φ`psq
¯ı

pQps, aq|a“φ`psq

¯ı

ı

(57)

This concludes the proof for the gradient in Eq.(28). Note that we should not directly replace
the mean functions in the weight mean-square-error by Eq.(54) and Eq.(55) before expanding the
gradient. This is because the analysis would require computing the gradient w.r.t. θ inside the
Hessians and this is not trivial. Moreover, when we perform gradient descent in practice, the mean
φ`psq is considered as a constant w.r.t. θ similarly to an output function in supervised learning.
The gradient for the mean-squared-error can be derived similarly. Let the mean-squared-error be
deﬁned as

LMpθq “

Epβ psq

}φθpsq ´ φ`psq}2
2

.

“

‰

1
2

(58)

Its gradient w.r.t. θ is given by

“

`

˘‰

∇θLMpθq “ Epβ psq
“ Epβ psq

∇θφθpsq
”
∇θφθpsqH ´1psq

φθpsq ´ φ`psq
´

∇a

which concludes the proof for the gradient in Eq.(30).

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

,

(59)

¯ı

To show that ∇a
˘
`
ηΣ´1psq ´ H φθ psq

´1

´
η‹Σ´1psqφθpsq ` ψφθ

pQps, aq|a“φ`psq “ 0 when η‹ “ 0, we directly substitute η‹ “ 0 into φ`psq “
¯

psqq

and this yield

φ`psq “ ´H ´1
φθ
pQps, aq|a“φ`psq ´ H ´1
Since φ`psq “ H ´1
psq∇a
psq from Eq.(55) and the Hessians are
φθ
pQps, aq|a“φ`psq “ 0. This is intuitive since without the KL constraint,
non-zero, it has to be that ∇a
the mean of the guide actor always be at the optima of the second-order Taylor’s approximation and
the gradients are zero at the optima.

psqψφθ

psqψφθ

psq.

(60)

φθ

A.5 RELATION TO Q-LEARNING WITH NORMALIZED ADVANTAGE FUNCTION

The normalized advantage function (NAF) (Gu et al., 2016) is deﬁned as

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(61)

where W psq is a negative deﬁnite matrix. Gu et al. (2016) proposed to perform Q-learning with
NAF by using the fact that argmaxa
Here, we show that GAC includes the Q-learning method by Gu et al. (2016) as its special case. This
can be shown by using NAF as a critic instead of performing Taylor’s approximation of the critic.

pQNAFps, aq “ bpsq and maxa

pQNAFps, aq “ V psq.

17

Published as a conference paper at ICLR 2018

pQNAFps, aq “

Firstly, we expand the quadratic term of NAF as follows:
1
2
1
2
1
2

aJW psqa ´ aJW psqbpsq `

aJW psqa ` aJψpsq ` ξpsq,

“

“

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq

bpsqJW psqbpsq ` V psq

(62)

where ψpsq “ ´W psqbpsq and ξpsq “ 1
2 bpsqJW psqbpsq ` V psq. By substituting the quadratic
model obtained by NAF into the GAC framework, the guide actor is now given by ˜πpa|sq “
N pa|φ`psq, Σ`psqqq with

φ`psq “ pη‹Σ´1psqq ´ W psqq´1pη‹Σ´1psqφθpsq ´ W psqbpsqq
Σ`psq “ pη‹ ` ω‹qpη‹Σ´1psqq ´ W psqq.

(64)
To obtain Q-learning with NAF, we set η‹ “ 0, i.e., we perform a greedy maximization where the
KL upper-bound approaches inﬁnity, and this yields

(63)

φ`psq “ ´W psq´1p´W psqbpsqq

(65)
which is the policy obtained by performing Q-learning with NAF. Thus, NAF with Q-learning is a
special case of GAC if Q-learning is also used in GAC to learn the critic.

“ bpsq,

The pseudo-code of GAC is given in Algorithm 1. The source code is available at https://
github.com/voot-t/guide-actor-critic.

B PSEUDO-CODE OF GAC

C EXPERIMENT DETAILS

C.1

IMPLEMENTATION

We try to follow the network architecture proposed by the authors of each baseline method as close as
possible. For GAC and DDPG, we use neural networks with two hidden layers for the actor network
and the critic network. For both networks the ﬁrst layer has 400 hidden units and the second layer
has 300 units. For NAF, we use neural networks with two hidden layers to represent each of the
functions bpsq, W psq and V psq where each layer has 200 hidden units. All hidden units use the
relu activation function except for the output of the actor network where we use the tanh activation
function to bound actions. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate
0.001 and 0.0001 for the critic network and the actor network, respectively. The moving average
step for target networks is set to τ “ 0.001. The maximum size of the replay buffer is set to
1000000. The mini-batches size is set to N “ 256. The weights of the actor and critic networks
are initialized as described by Glorot & Bengio (2010), except for the output layers where the initial
weights are drawn uniformly from Up´0.003, 0.003q, as described by Lillicrap et al. (2015). The
initial covariance Σ in GAC is set to be an identity matrix. DDPG and QNAF use the OU-process
with noise parameters θ “ 0.15 and σ “ 0.2 for exploration .

For TRPO, we use the implementation publicly available at https://github.com/openai/
baselines. We also use the provided network architecture and hyper-parameters except the batch
size where we use 1000 instead of 1024 since this is more suitable in our test setup.

For GAC, the KL upper-bound is ﬁxed to (cid:15) “ 0.0001. The entropy lower-bound κ is adjusted
heuristically by

κ “ maxp0.99pE ´ E0q ` E0, E0q,
(71)
where E « Epβ psq rHpπθpa|sqqs denotes the expected entropy of the current policy and E0 denotes
the entropy of a base policy N pa|0, 0.01Iq. This heuristic ensures that the lower-bound gradually
decreases but the lower-bound cannot be too small. We apply this heuristic update once every 5000
training steps. The dual function is minimize by the sequential least-squares quadratic programming
(SLSQP) method with an initial values η “ 0.05 and ω “ 0.05. The number of samples for
computing the target critic value is M “ 10.

18

Published as a conference paper at ICLR 2018

Algorithm 1 Guide actor critic

1: Input:

Initial actor πθpa|sq “ N pa|φθpsq, Σq, critic pQν ps, aq, target critic network

pQ¯ν ps, aq, KL bound (cid:15), entropy bound κ, learning rates α1, α2, and data buffer D “ H.

procedure COLLECT TRANSITION SAMPLE

Observe state st and sample action at „ N pa|φθpstq, Σq.
Execute at, receive reward rt and next state s1
t.
Add transition tpst, at, rt, s1

tqu to D.

2: for t “ 1, . . . , Tmax do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

end procedure
procedure LEARN

Sample N mini-batch samples tpsn, an, rn, s1
procedure UPDATE CRITIC
n,muM
Sample actions ta1
Compute yn, update ν by, e.g., Adam, and update ¯ν by moving average:

m“1 „ N pa|φθpsnq, Σq for each sn.

n“1 uniformly from D.

nquN

yn “ rn ` γ

pQ¯ν ps1

n, a1

n,mq,

ν Ð ν ´ α1

´
pQν psn, anq ´ yn

¯

2

,

∇ν

1
M

1
N

Mÿ

m“1
Nÿ

n“1

¯ν Ð τ ν ` p1 ´ τ q¯ν.

end procedure
procedure LEARN GUIDE ACTOR

Compute an,0 for each sn by an,0 “ φθpsnq or an,0 „ N pa|φθpsnq, Σq.
pQpsn, aq|a“an,0 and H 0pssq “ ´g0psnqg0psnqJ.
Compute g0psq “ ∇a
Solve for pη‹, ω‹q “ argminηą0,ωą0
Compute the guide actor rπpa|snq “ N pa|φ`psnq, Σ`psnqq for each sn.

pgpη, ωq by a non-linear optimization method.

end procedure
procedure UPDATE PARAMETERIZED ACTOR

Update policy parameter by, e.g., Adam, to minimize the MSE:

13:
14:
15:

16:
17:
18:
19:
20:
21:

θ Ð θ ´ α2

∇θ}φθpsnq ´ φ`psnq}2
2.

1
N

Nÿ

n“1

22:

Update policy covariance by averaging the guide covariances:

Σ Ð

Σ`psnq.

1
N

(66)

(67)

(68)

(69)

(70)

end procedure

end procedure

23:
24:
25: end for
26: Output: Learned actor πθpa|sq.

C.2 ENVIRONMENTS AND RESULTS

We perform experiments on the OpenAI gym platform (Brockman et al., 2016) with Mujoco Physics
simulator (Todorov et al., 2012) where all environments are v1. We use the state space, action space
and the reward function as provided and did not perform any normalization or gradient clipping.
The maximum time horizon in each episode is set to 1000. The discount factor γ “ 0.99 is only
used for learning and the test returns are computed without it.

Experiments are repeated for 10 times with different random seeds. The total computation time are
reported in Table 1. The ﬁgures below show the results averaged over 10 trials. The y-axis indicates
the averaged test returns where the test returns in each trial are computed once every 5000 training
time steps by executing 10 test episodes without exploration. The error bar indicates standard error.

19

Published as a conference paper at ICLR 2018

Table 1: The total computation time for training the policy for 1 million steps (0.1 million steps for
the Invert-Pendulum task). The mean and standard error are computed over 10 trials with the unit in
hours. TRPO is not included since it performs a lesser amount of update using batch data samples.

Task
Inv-Pend.
Inv-Double-Pend.
Reacher
Swimmer
Half-Cheetah
Ant
Hopper
Walker2D
Humanoid

GAC-1
1.13(0.09)
15.56(0.93)
17.23(0.87)
12.43(0.74)
29.82(1.76)
37.84(1.80)
18.99(1.06)
33.42(0.96)
111.07(2.99)

GAC-0
0.80(0.04)
15.67(0.77)
12.64(0.38)
11.94(0.74)
32.64(1.41)
40.57(3.06)
14.22(0.56)
31.71(1.84)
106.80(6.80)

DDPG
0.45(0.02)
9.04(0.29)
10.67(0.37)
9.61(0.52)
10.13(0.41)
9.75(0.37)
8.74(0.45)
7.92(0.11)
13.90(1.60)

QNAF
0.40(0.03)
7.47(0.22)
30.91(2.09)
32.44(2.21)
27.94(2.37)
27.09(0.94)
26.48(1.42)
26.94(1.99)
30.43(2.21)

Figure 2: Performance averaged over 10 trials on the Inverted Pendulum task.

20

Published as a conference paper at ICLR 2018

Figure 3: Performance averaged over 10 trials on the Inverted Double Pendulum task.

Figure 4: Performance averaged over 10 trials on the Reacher task.

21

Published as a conference paper at ICLR 2018

Figure 5: Performance averaged over 10 trials on the Swimmer task.

Figure 6: Performance averaged over 10 trials on the Half-Cheetah task.

22

Published as a conference paper at ICLR 2018

Figure 7: Performance averaged over 10 trials on the Ant task.

Figure 8: Performance averaged over 10 trials on the Hopper task.

23

Published as a conference paper at ICLR 2018

Figure 9: Performance averaged over 10 trials on the Walker2D task.

Figure 10: Performance averaged over 10 trials on the Humanoid task.

24

8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
6
7
0
.
5
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

GUIDE ACTOR-CRITIC FOR CONTINUOUS CONTROL

Voot Tangkaratt
RIKEN AIP, Tokyo, Japan
voot.tangkaratt@riken.jp

Abbas Abdolmaleki
The University of Aveiro, Aveiro, Portugal
abbas.a@ua.pt

Masashi Sugiyama
RIKEN AIP, Tokyo, Japan
The University of Tokyo, Tokyo, Japan
masashi.sugiyama@riken.jp

ABSTRACT

Actor-critic methods solve reinforcement learning problems by updating a param-
eterized policy known as an actor in a direction that increases an estimate of the
expected return known as a critic. However, existing actor-critic methods only
use values or gradients of the critic to update the policy parameter. In this pa-
per, we propose a novel actor-critic method called the guide actor-critic (GAC).
GAC ﬁrstly learns a guide actor that locally maximizes the critic and then it up-
dates the policy parameter based on the guide actor by supervised learning. Our
main theoretical contributions are two folds. First, we show that GAC updates the
guide actor by performing second-order optimization in the action space where
the curvature matrix is based on the Hessians of the critic. Second, we show that
the deterministic policy gradient method is a special case of GAC when the Hes-
sians are ignored. Through experiments, we show that our method is a promising
reinforcement learning method for continuous controls.

1

INTRODUCTION

The goal of reinforcement learning (RL) is to learn an optimal policy that lets an agent achieve the
maximum cumulative rewards known as the return (Sutton & Barto, 1998). Reinforcement learning
has been shown to be effective in solving challenging artiﬁcial intelligence tasks such as playing
games (Mnih et al., 2015; Silver et al., 2016) and controlling robots (Deisenroth et al., 2013; Levine
et al., 2016).

Reinforcement learning methods can be classiﬁed into three categories: value-based, policy-based,
and actor-critic methods. Value-based methods learn an optimal policy by ﬁrstly learning a value
function that estimates the expected return. Then, they infer an optimal policy by choosing an
action that maximizes the learned value function. Choosing an action in this way requires solving a
maximization problem which is not trivial for continuous controls. While extensions to continuous
controls were considered recently, they are restrictive since speciﬁc structures of the value function
are assumed (Gu et al., 2016; Amos et al., 2017).

On the other hand, policy-based methods, also called policy search methods (Deisenroth et al.,
2013), learn a parameterized policy maximizing a sample approximation of the expected re-
turn without learning the value function. For instance, policy gradient methods such as REIN-
FORCE (Williams, 1992) use gradient ascent to update the policy parameter so that the probability
of observing high sample returns increases. Compared with value-based methods, policy search
methods are simpler and naturally applicable to continuous problems. Moreover, the sample return
is an unbiased estimator of the expected return and methods such as policy gradients are guaranteed
to converge to a locally optimal policy under standard regularity conditions (Sutton et al., 1999).
However, sample returns usually have high variance and this makes such policy search methods
converge too slowly.

Actor-critic methods combine the advantages of value-based and policy search methods. In these
methods, the parameterized policy is called an actor and the learned value-function is called a critic.

1

Published as a conference paper at ICLR 2018

The goal of these methods is to learn an actor that maximizes the critic. Since the critic is a low vari-
ance estimator of the expected return, these methods often converge much faster than policy search
methods. Prominent examples of these methods are actor-critic (Sutton et al., 1999; Konda & Tsit-
siklis, 2003), natural actor-critic (Peters & Schaal, 2008), trust-region policy optimization (Schul-
man et al., 2015a), and asynchronous advantage actor-critic (Mnih et al., 2016). While their ap-
proaches to learn the actor are different, they share a common property that they only use the value
of the critic, i.e., the zero-th order information, and ignore higher-order ones such as gradients and
Hessians w.r.t. actions of the critic1. To the best of our knowledge, the only actor-critic methods
that use gradients of the critic to update the actor are deterministic policy gradients (DPG) (Silver
et al., 2014) and stochastic value gradients (Heess et al., 2015). However, these two methods do not
utilize the second-order information of the critic.

In this paper, we argue that the second-order information of the critic is useful and should not be
ignored. A motivating example can be seen by comparing gradient ascent to the Newton method: the
Newton method which also uses the Hessian converges to a local optimum in a fewer iterations when
compared to gradient ascent which only uses the gradient (Nocedal & Wright, 2006). This suggests
that the Hessian of the critic can accelerate actor learning which leads to higher data efﬁciency.
However, the computational complexity of second-order methods is at least quadratic in terms of
the number of optimization variables. For this reason, applying second-order methods to optimize
the parameterized actor directly is prohibitively expensive and impractical for deep reinforcement
learning which represents the actor by deep neural networks.

Our contribution in this paper is a novel actor-critic method for continuous controls which we call
guide actor-critic (GAC). Unlike existing methods, the actor update of GAC utilizes the second-
order information of the critic in a computationally efﬁcient manner. This is achieved by separating
actor learning into two steps. In the ﬁrst step, we learn a non-parameterized Gaussian actor that
locally maximizes the critic under a Kullback-Leibler (KL) divergence constraint. Then, the Gaus-
sian actor is used as a guide for learning a parameterized actor by supervised learning. Our analysis
shows that learning the mean of the Gaussian actor is equivalent to performing a second-order update
in the action space where the curvature matrix is given by Hessians of the critic and the step-size is
controlled by the KL constraint. Furthermore, we establish a connection between GAC and DPG
where we show that DPG is a special case of GAC when the Hessians and KL constraint are ignored.

2 BACKGROUND

In this section, we ﬁrstly give a background of reinforcement learning. Then, we discuss existing
second-order methods for policy learning and their issue in deep reinforcement learning.

2.1 REINFORCEMENT LEARNING

We consider discrete-time Markov decision processes (MDPs) with continuous state space S Ď Rds
and continuous action space A Ď Rda . We denote the state and action at time step t P N by
st and at, respectively. The initial state s1 is determined by the initial state density s1 „ ppsq.
At time step t, the agent in state st takes an action at according to a policy at „ πpa|stq and
obtains a reward rt “ rpst, atq. Then, the next state st`1 is determined by the transition function
st`1 „ pps1|st, atq. A trajectory τ “ ps1, a1, r1, s2, . . . q gives us the cumulative rewards or return
t“1 γt´1rpst, atq, where the discount factor 0 ă γ ă 1 assigns different weights to
deﬁned as
rewards given at different time steps. The expected return of π after executing an action a in a state
s can be expressed through the action-value function which is deﬁned as

ř

8

ﬀ

Qπps, aq “ Eπpat|stqtě2,ppst`1|st,atqtě1

γt´1rpst, atq|s1 “ s, a1 “ a

,

(1)

«

8ÿ

t“1

where Ep r¨s denotes the expectation over the density p and the subscript t ě 1 indicates that the
expectation is taken over the densities at time steps t ě 1. We can deﬁne the expected return as
8ÿ

«

ﬀ

J pπq “ Epps1q,πpat|stqtě1,ppst`1|st,atqtě1

γt´1rpst, atq

“ Eppsq,πpa|sq rQπps, aqs .

(2)

1This is different from using the gradient of the critic w.r.t. critic parameters to update the critic itself.

t“1

2

Published as a conference paper at ICLR 2018

The goal of reinforcement learning is to ﬁnd an optimal policy that maximizes the expected return.
The policy search approach (Deisenroth et al., 2013) parameterizes π by a parameter θ P Rdθ and
ﬁnds θ‹ which maximizes the expected return:

θ‹ “ argmax

Eppsq,πθ pa|sq rQπθ ps, aqs .

θ

(3)

(4)

(5)

(7)

Policy gradient methods such as REINFORCE (Williams, 1992) solve this optimization problem by
gradient ascent:

θ Ð θ ` αEppsq,πθ pa|sq r∇θ log πθpa|sqQπθ ps, aqs ,

ř

where α ą 0 is a step-size. In policy search, the action-value function is commonly estimated by the
sample return: Qπθ ps, aq « 1
n,t γt´1rpst,n, at,nq obtained by collecting N trajectories using
N
πθ. The sample return is an unbiased estimator of the action-value function. However, it often has
high variance which leads to slow convergence.
An alternative approach is to estimate the action-value function by a critic denoted by pQps, aq
whose parameter is learned such that pQps, aq « Qπθ ps, aq. By replacing the action-value function
in Eq.(3) with the critic, we obtain the following optimization problem:

θ‹ “ argmax

Eppsq,πθ pa|sq

θ

ı

”
pQps, aq

.

The actor-critic method (Sutton et al., 1999) solves this optimization problem by gradient ascent:
”
∇θ log πθpa|sq

θ Ð θ ` αEppsq,πθ pa|sq

pQps, aq

ı

.

(6)

The gradient in Eq.(6) often has less variance than that in Eq.(4), which leads to faster convergence2.
A large class of actor-critic methods is based on this method (Peters & Schaal, 2008; Mnih et al.,
2016). As shown in these papers, these methods only use the value of the critic to learn the actor.

The deterministic policy gradients (DPG) method (Silver et al., 2014) is an actor-critic method that
uses the ﬁrst-order information of the critic. DPG updates a deterministic actor πθpsq by

θ Ð θ ` αEppsq

”
∇θπθpsq∇a

ı

pQps, aq|a“πθ psq

.

A method related to DPG is the stochastic value gradients (SVG) method (Heess et al., 2015) that
is able to learn a stochastic policy but it requires learning a model of the transition function.

The actor-critic methods described above only use up to the ﬁrst-order information of the critic when
learning the actor and ignore higher-order ones. Below, we discuss existing approaches that utilize
the second-order information by applying second-order optimization methods to solve Eq.(5).

2.2 SECOND-ORDER METHODS FOR POLICY LEARNING

The actor-critic methods described above are ﬁrst-order methods which update the optimization
variables based on the gradient of the objective function. First-order methods are popular in deep
learning thanks to their computational efﬁciency. However, it is known in machine learning that
second-order methods often lead to faster learning because they use the curvature information to
compute a better update direction, i.e., the steepest ascent direction along the curvature3.

The main idea of second-order methods is to rotate the gradient by the inverse of a curvature matrix.
For instance, second-order updates for the actor-critic method in Eq.(6) are in the following form:

θ Ð θ ` αG´1

!
Eppsq,πθ pa|sq

”
∇θ log πθpa|sq

ı)

pQps, aq

,

(8)

2This gradient is a biased estimator of the policy gradient in Eq.(4). However, it is unbiased under some

regularity conditions such as compatible function conditions (Sutton et al., 1999).

3We use the term “second-order methods” in a broad sense here, including quasi-Newton and natural gradi-

ent methods which approximate the curvature matrix by the ﬁrst-order information.

3

Published as a conference paper at ICLR 2018

where G P Rdθ ˆdθ is a curvature matrix. The behavior of second-order methods depend on the
deﬁnition of a curvature matrix. The most well-known second-order method is the Newton method
where its curvature matrix is the Hessian of the objective function w.r.t. the optimization variables:
∇θ log πθpa|sq∇θ log πθpa|sqJ ` ∇2

GHessian “ Eppsq,πθ pa|sq

θ log πθpa|sq

pQps, aq

”`

(9)

ı

˘

.

“

∇θ log πθpa|sq∇θ log πθpa|sqJ

The natural gradient method is another well-known second-order method which uses the Fisher
information matrix (FIM) as the curvature matrix (Amari, 1998):
GFIM “ Eppsq,πθ pa|sq

(10)
Unlike the Hessian matrix, FIM provides information about changes of the policy measured by an
approximated KL divergence: Eppsq rKLpπθpa|sq||πθ1pa|sqqs « pθ ´ θ1qJGFIMpθ ´ θ1q (Kakade,
2001). We can see that GHessian and GFIM are very similar but the former also contains the critic
and the Hessian of the actor while the latter does not. This suggests that the Hessian provides more
information than that in FIM. However, FIM is always positive semi-deﬁnite while the Hessian may
be indeﬁnite. Please see Furmston et al. (2016) for detailed comparisons between the two curvature
matrices in policy search4. Nonetheless, actor-critic methods based on natural gradient were shown
to be very efﬁcient (Peters & Schaal, 2008; Schulman et al., 2015a).

‰

.

We are not aware of existing work that considers second-order updates for DPG or SVG. However,
their second-order updates can be trivially derived. For example, a Newton update for DPG is

θ Ð θ ` αD´1

!
Eppsq

”
∇θπθpsq∇a

pQps, aq|a“πθ psq

ı)

,

where the pi, jq-th entry of the Hessian matrix D P Rdθ ˆdθ is

Dij “ Eppsq

∇2
a

pQps, aq|a“πθ psq

Bπθpsq
Bθj

`

B2πθpsq
BθiBθj

J

∇a

pQps, aq|a“πθ psq

.

(12)

«

J

Bπθpsq
Bθi

(11)

ﬀ

Note that Bπθpsq{Bθ and B2πθpsq{BθBθ1 are vectors since πθpsq is a vector-valued function. In-
terestingly, the Hessian of DPG contains the Hessians of the actor and the critic. In contrast, the
Hessian of the actor-critic method contains the Hessian of the actor and the value of the critic.

Second-order methods are appealing in reinforcement learning because they have high data efﬁ-
ciency. However, inverting the curvature matrix (or solving a linear system) requires cubic compu-
tational complexity in terms of the number of optimization variables. For this reason, the second-
order updates in Eq.(8) and Eq.(11) are impractical in deep reinforcement learning due to a large
number of weight parameters in deep neural networks. In such a scenario, an approximation of
the curvature matrix is required to reduce the computational burden. For instance, Furmston et al.
(2016) proposed to use only diagonal entries of an approximated Hessian matrix. However, this
approximation clearly leads to a loss of useful curvature information since the gradient is scaled but
not rotated. More recently, Wu et al. (2017) proposed a natural actor-critic method that approxi-
mates block-diagonal entries of FIM. However, this approximation corresponds to ignoring useful
correlations between weight parameters in different layers of neural networks.

3 GUIDE ACTOR-CRITIC

In this section, we propose the guide actor-critic (GAC) method that performs second-order updates
without the previously discussed computational issue. Unlike existing methods that directly learn
the parameterized actor from the critic, GAC separates the problem of learning the parameterized
actor into problems of 1) learning a guide actor that locally maximizes the critic, and 2) learning a
parameterized actor based on the guide actor. This separation allows us to perform a second-order
update for the guide actor where the dimensionality of the curvature matrix is independent of the
parameterization of the actor.

We formulate an optimization problem for learning the guide actor in Section 3.1 and present its so-
lution in Section 3.2. Then in Section 3.3 and Section 3.4, we show that the solution corresponds to
performing second-order updates. Finally, Section 3.5 presents the learning step for the parameter-
ized actor using supervised learning. The pseudo-code of our method is provided in Appendix B and
the source code is available at https://github.com/voot-t/guide-actor-critic.

4 Furmston et al. (2016) proposed an approximate Newton method for policy search. Their policy search

method was shown to perform better than methods based on gradient ascent and natural gradient ascent.

4

Published as a conference paper at ICLR 2018

3.1 OPTIMIZATION PROBLEM FOR GUIDE ACTOR

Our ﬁrst goal is to learn a guide actor that maximizes the critic. However, greedy maximization
should be avoided since the critic is a noisy estimate of the expected return and a greedy actor
may change too abruptly across learning iterations. Such a behavior is undesirable in real-world
problems, especially in robotics (Deisenroth et al., 2013).
Instead, we maximize the critic with
additional constraints:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

(13)

where ˜πpa|sq is the guide actor to be learned, πθpa|sq is the current parameterized actor that we
want to improve upon, and pβpsq is the state distribution induced by past trajectories. The objective
function differs from the one in Eq.(5) in two important aspects. First, we maximize for a policy
function ˜π and not for the policy parameter. This is more advantageous than optimizing for a policy
parameter since the policy function can be obtained in a closed form, as will be shown in the next
subsection. Second, the expectation is deﬁned over a state distribution from past trajectories and this
gives us off-policy methods with higher data efﬁciency. The ﬁrst constraint is the Kullback-Leibler
(KL) divergence constraint where KLpppxq||qpxqq “ Eppxq rlog ppxq ´ log qpxqs. The second con-
straint is the Shannon entropy constraint where Hpppxqq “ ´Eppxq rlog ppxqs. The KL constraint
is commonly used in reinforcement learning to prevent unstable behavior due to excessively greedy
update (Peters & Schaal, 2008; Peters et al., 2010; Levine & Koltun, 2013; Schulman et al., 2015a).
The entropy constraint is crucial for maintaining stochastic behavior and preventing premature con-
vergence (Ziebart et al., 2010; Abdolmaleki et al., 2015; Mnih et al., 2016; Haarnoja et al., 2017).
The ﬁnal constraint ensures that the guide actor is a proper probability density. The KL bound
(cid:15) ą 0 and the entropy bound ´8 ă κ ă 8 are hyper-parameters which control the exploration-
exploitation trade-off of the method. In practice, we ﬁx the value of (cid:15) and adaptively reduce the
value of κ based on the current actor’s entropy, as suggested by Abdolmaleki et al. (2015). More
details of these tuning parameters are given in Appendix C.

This optimization problem can be solved by the method of Lagrange multipliers. The solution is

˜πpa|sq 9 πθpa|sq

η‹`ω‹ exp

η‹

˜

¸

,

pQps, aq
η‹ ` ω‹

(14)

where η‹ ą 0 and ω‹ ą 0 are dual variables corresponding to the KL and entropy constraints,
respectively. The dual variable corresponding to the probability density constraint is contained in the
normalization term and is determined by η‹ and ω‹. These dual variables are obtained by minimizing
the dual function:

«

ż

gpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

πθpa|sq

η
η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

.

(15)

All derivations and proofs are given in Appendix A. The solution in Eq.(14) tells us that the
guide actor is obtained by weighting the current actor with pQps, aq.
If we set (cid:15) Ñ 0 then we
have ˜π « πθ and the actor is not updated. On the other hand, if we set (cid:15) Ñ 8 then we have
˜πpa|sq 9 expp

pQps, aq{ω‹q, which is a softmax policy where ω‹ is the temperature parameter.

3.2 LEARNING GUIDE ACTOR

Computing ˜πpa|sq and evaluating gpη, ωq are intractable for an arbitrary πθpa|sq. We overcome
this issue by imposing two assumptions. First, we assume that the actor is the Gaussian distribution:

πθpa|sq “ N pa|φθpsq, Σθpsqq,
where the mean φθpsq and covariance Σθpsq are functions parameterized by a policy parameter
θ. Second, we assume that Taylor’s approximation of pQps, aq is locally accurate up to the second-
order. More concretely, the second-order Taylor’s approximation using an arbitrary action a0 is

(16)

5

Published as a conference paper at ICLR 2018

given by

pQps, aq «

pQps, a0q ` pa ´ a0qJg0psq `

pa ´ a0qJH 0psqpa ´ a0q ` Op}a}3q,

(17)

1
2

pQps, aq|a“a0 are the gradient and Hessian of
where g0psq “ ∇a
the critic w.r.t. a evaluated at a0, respectively. By assuming that the higher order term Op}a}3q is
sufﬁciently small, we can rewrite Taylor’s approximation at a0 as

pQps, aq|a“a0 and H 0psq “ ∇2

a

pQ0ps, aq “

1
2

aJH 0psqa ` aJψ0psq ` ξ0psq,

(18)

pQps, a0q. Note that
where ψ0psq “ g0psq ´ H 0psqa0 and ξ0psq “ 1
H 0psq, ψ0psq, and ξ0psq depend on the value of a0 and do not depend on the value of a. This
dependency is explicitly denoted by the subscript. The choice of a0 will be discussed in Section 3.3.

0 H 0psqa0 ´ aJ

0 g0psq `

2 aJ

Substituting the Gaussian distribution and Taylor’s approximation into Eq.(14) yields another Gaus-
sian distribution ˜πpa|sq “ N pa|φ`psq, Σ`psqq, where the mean and covariance are given by

φ`psq “ F ´1psqLpsq, Σ`psq “ pη‹ ` ω‹qF ´1psq.

The matrix F psq P Rdaˆda and vector Lpsq P Rda are deﬁned as
θ psq ´ H 0psq, Lpsq “ η‹Σ´1

F psq “ η‹Σ´1

θ psqφθpsq ` ψ0psq.

The dual variables η‹ and ω‹ are obtained by minimizing the following dual function:

(19)

(20)

pgpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

«

d

ﬀ

|2πpη ` ωqF ´1
|2πΣθpsq|

η
η`ω

η psq|

`

Epβ psq

LηpsqJF ´1

η psqLηpsq ´ ηφθpsqJΣ´1

` const,

(21)

‰
θ psqφθpsq

“

1
2

where F ηpsq and Lηpsq are deﬁned similarly to F psq and Lpsq but with η instead of η‹.

The practical advantage of using the Gaussian distribution and Taylor’s approximation is that the
guide actor can be obtained in a closed form and the dual function can be evaluated through matrix-
vector products. The expectation over pβpsq can be approximated by e.g., samples drawn from a
replay buffer (Mnih et al., 2015). We require inverting F ηpsq to evaluate the dual function. However,
these matrices are computationally cheap to invert when the dimension of actions is not large.

As shown in Eq.(19), the mean and covariance of the guide actor is computed using both the gradient
and Hessian of the critic. Yet, these computations do not resemble second-order updates discussed
previously in Section 2.2. Below, we show that for a particular choice of a0, the mean computation
corresponds to a second-order update that rotates gradients by a curvature matrix.

3.3 GUIDE ACTOR LEARNING AS SECOND-ORDER OPTIMIZATION

For now we assume that the critic is an accurate estimator of the true action-value function. In this
case, the quality of the guide actor depends on the accuracy of sample approximation in pgpη, ωq and
the accuracy of Taylor’s approximation. To obtain an accurate Taylor’s approximation of pQps, aq
using an action a0, the action a0 should be in the vicinity of a. However, we did not directly use
pQps, aqq (see Eq.(14)).
any individual a to compute the guide actor, but we weight πθpa|sq by expp
Thus, to obtain an accurate Taylor’s approximation of the critic, the action a0 needs to be similar to
actions sampled from πθpa|sq. Based on this observation, we propose two approaches to perform
Taylor’s approximation.

Taylor’s approximation around the mean. In this approach, we perform Taylor’s approximation
using the mean of πθpa|sq. More speciﬁcally, we use a0 “ Eπθ pa|sq ras “ φθpsq for Eq.(18). In
this case, we can show that the mean update in Eq.(19) corresponds to performing a second-order
update in the action space to maximize pQps, aq:

φ`psq “ φθpsq ` F ´1
φθ

psq∇a

pQps, aq|a“φθ psq,

(22)

6

Published as a conference paper at ICLR 2018

θ psq ´ H φθ psq and H φθ psq “ ∇2
a

pQps, aq|a“φθ psq. This equivalence can
where F φθ psq “ η‹Σ´1
be shown by substitution and the proof is given in Appendix A.2. This update equation reveals that
the guide actor maximizes the critic by taking a step in the action space similarly to the Newton
method. However, the main difference lies in the curvature matrix where the Newton method uses
Hessians H φθ psq but we use a damped Hessian F φθ psq. The damping term η‹Σ´1
θ psq corresponds
to the effect of the KL constraint and can be viewed as a trust-region that controls the step-size. This
damping term is particularly important since Taylor’s approximation is accurate only locally and we
should not take a large step in each update (Nocedal & Wright, 2006).

Expectation of Taylor’s approximations.
Instead of using Taylor’s approximation around the
mean, we may use an expectation of Taylor’s approximation over the distribution. More concretely,
we deﬁne rQps, aq to be an expectation of pQ0ps, aq over πθpa0|sq:

rQps, aq “

1
2

aJEπθ pa0|sq rH 0psqs a ` aJEπθ pa0|sq rψ0psqs ` Eπθ pa0|sq rξ0psqs .

(23)

pQps, aq|a“a0 s and the expectation is computed w.r.t.
Note that Eπθ pa0|sqrH 0psqs “ Eπθ pa0|sqr∇2
a
the distribution πθ of a0. We use this notation to avoid confusion even though πθpa0|sq and πθpa|sq
are the same distribution. When Eq.(23) is used, the mean update does not directly correspond
to any second-order optimization step. However, under an (unrealistic) independence assumption
Eπθ pa0|sqrH 0psqa0s “ Eπθ pa0|sqrH 0psqsEπθ pa0|sqra0s, we can show that the mean update corre-
sponds to the following second-order optimization step:

φ`psq “ φθpsq ` Eπθ pa0|sq rF 0psqs´1 Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

,

(24)

where Eπθ pa0|sq rF 0psqs “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs. Interestingly, the mean is updated
by rotating an expected gradient using an expected Hessians. In practice, the expectations can be
approximated using sampled actions ta0,iuS
i“1 „ πθpa|sq. We believe that this sampling can be
advantageous for avoiding local optima. Note that when the expectation is approximated by a single
sample a0 „ πθpa|sq, we obtain the update in Eq.(24) regardless of the independence assumption.
In the remainder, we use F psq to denote both of F φθ psq and Eπθ pa0|sq rF 0psqs, and use Hpsq to
denote both of H φθ psq and Eπθ pa0|sqrH 0psqs. In the experiments, we use GAC-0 to refer to GAC
with Taylor’s approximation around the mean, and we use GAC-1 to refer to GAC with Taylor’s
approximation by a single sample a0 „ πθpa|sq.

3.4 GAUSS-NEWTON APPROXIMATION OF HESSIAN

The covariance update in Eq.(19) indicates that F psq “ η‹Σ´1
θ psq ´ Hpsq needs to be positive
deﬁnite. The matrix F psq is guaranteed to be positive deﬁnite if the Hessian matrix Hpsq is negative
semi-deﬁnite. However, this is not guaranteed in practice unless pQps, aq is a concave function in
terms of a. To overcome this issue, we ﬁrstly consider the following identity:
pQps, aq∇a

pQps, aq “ ´∇a

pQps, aqq expp´

pQps, aqJ ` ∇2

pQps, aqq.

Hpsq “ ∇2
a

a expp

(25)

The proof is given in Appendix A.3. The ﬁrst term is always negative semi-deﬁnite while the second
term is indeﬁnite. Therefore, a negative semi-deﬁnite approximation of the Hessian can be obtained
as

H 0psq « ´

pQps, aq∇a

pQps, aqJ

”
∇a

ı

.

a“a0

(26)

pQps, aqq and it will be small for high values
The second term in Eq.(25) is proportional to expp´
of pQps, aq. This implies that the approximation should gets more accurate as the policy approach
a local maxima of pQps, aq. We call this approximation Gauss-Newton approximation since it is
similar to the Gauss-Newton approximation for the Newton method (Nocedal & Wright, 2006).

3.5 LEARNING PARAMETERIZED ACTOR

The second step of GAC is to learn a parameterized actor that well represents the guide actor. Below,
we discuss two supervised learning approaches for learning a parameterized actor.

7

Published as a conference paper at ICLR 2018

3.5.1 FULLY-PARAMETERIZED GAUSSIAN POLICY

Since the guide actor is a Gaussian distribution with a state-dependent mean and covariance, a
natural choice for the parameterized actor is again a parameterized Gaussian distribution with a
state-dependent mean and covariance: πθpa|sq “ N pa|φθpsq, Σθpsqq. The parameter θ can be
learned by minimizing the expected KL divergence to the guide actor:

LKLpθq “ Epβ psq rKL pπθpa|sq||˜πpa|sqqs

„



TrpF psqΣθpsqq
η‹ ` ω‹

“ Epβ psq
”
}φθpsq ´ φ`psq}2

´ log |Σθpsq|
ı

`

LWpθq
η‹ ` ω‹ ` const,

(27)

where LWpθq “ Epβ psq
which only depends on θ of the mean function. The const term does not depend on θ.

is the weighted-mean-squared-error (WMSE)

F psq

Minimizing the KL divergence reveals connections between GAC and deterministic policy gradients
(DPG) (Silver et al., 2014). By computing the gradient of the WMSE, it can be shown that

∇θLWpθq
2

“ ´Epβ psq

ı

pQps, aq|a“φθ psq

”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

psq∇a

psq∇a

”
∇θφθpsq∇a
ı

` Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

ı

.

` η‹Epβ psq

´ η‹Epβ psq

ı

pQps, aq|a“φ`psq

(28)

The proof is given in Appendix A.4. The negative of the ﬁrst term is precisely equivalent to DPG.
Thus, updating the mean parameter by minimizing the KL loss with gradient descent can be regarded
as updating the mean parameter with biased DPG where the bias terms depend on η‹. We can verify
pQps, aq|a“φ`psq “ 0 when η‹ “ 0 and this is the case of (cid:15) Ñ 8. Thus, all bias terms vanish
that ∇a
when the KL constraint is ignored and the mean update of GAC coincides with DPG. However,
unlike DPG which learns a deterministic policy, we can learn both the mean and covariance in GAC.

3.5.2 GAUSSIAN POLICY WITH PARAMETERIZED MEAN

While a state-dependent parameterized covariance function is ﬂexible, we observe that learning
performance is sensitive to the initial parameter of the covariance function. For practical pur-
poses, we propose using a parametrized Gaussian distribution with state-independent covariance:
πθpa|sq “ N pa|φθpsq, Σq. This class of policies subsumes deterministic policies with additive in-
dependent Gaussian noise for exploration. To learn θ, we minimize the mean-squared-error (MSE):
“

(29)
‰
For the covariance, we use the average of the guide covariances: Σ “ pη‹ ` ω‹qEpβ psq
.
For computational efﬁciency, we execute a single gradient update in each learning iteration instead
of optimizing this loss function until convergence.

F ´1psq

}φθpsq ´ φ`psq}2
2

LMpθq “

Epβ psq

1
2

‰

“

.

Similarly to the above analysis, the gradient of the MSE w.r.t. θ can be expanded and rewritten into

∇θLMpθq “ Epβ psq

”
∇θφθpsqH ´1psq

´

∇a

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

.

(30)

¯ı

Again, the mean update of GAC coincides with DPG when we minimize the MSE and set η‹ “ 0
and Hpsq “ ´I where I is the identity matrix. We can also substitute these values back into
Eq.(22). By doing so, we can interpret DPG as a method that performs ﬁrst-order optimization in
the action space:

φ`psq “ φθpsq ` ∇a

pQps, aq|a“φθ psq,

(31)

and then uses the gradient in Eq.(30) to update the policy parameter. This interpretation shows that
DPG is a ﬁrst-order method that only uses the ﬁrst-order information of the critic for actor learning.
Therefore in principle, GAC, which uses the second-order information of the critic, should learn
faster than DPG.

8

Published as a conference paper at ICLR 2018

3.6 POLICY EVALUATION FOR CRITIC

Beside actor learning, the performance of actor-critic methods also depends on the accuracy of the
critic. We assume that the critic pQν ps, aq is represented by neural networks with a parameter ν.
We adopt the approach proposed by Lillicrap et al. (2015) with some adjustment to learn ν. More
concretely, we use gradient descent to minimize the squared Bellman error with a slowly moving
target critic:

ν Ð ν ´ α∇ν Epβ psq,βpa|sq,pps1|s,aq

pQν ps, aq ´ y

(32)

„´



¯

2

,

pQ¯ν ps1, a1qs is computed by
where α ą 0 is the step-size. The target value y “ rps, aq ` γEπpa1|s1qr
the target critic pQ¯ν ps1, a1q whose parameter ¯ν is updated by ¯ν Ð τ ν ` p1 ´ τ q¯ν for 0 ă τ ă 1.
As suggested by Lillicrap et al. (2015), the target critic improves the learning stability and we set
τ “ 0.001 in experiments. The expectation for the squared error is approximated using mini-batch
samples tpsn, an, rn, s1
n“1 drawn from a replay buffer. The expectation over the current actor
πpa1|s1q is approximated using samples ta1
n. We do not use a
target actor to compute y since the KL upper-bound already constrains the actor update and a target
actor will further slow it down. Note that we are not restricted to this evaluation method and more
efﬁcient methods such as Retrace (Munos et al., 2016) can also be used.

m“1 „ πθpa1|s1

nq for each s1

n,muM

nquN

pQν ps, aq and its outer product for the Gauss-Newton approxi-
Our method requires computing ∇a
mation. The computational complexity of the outer product operation is Opd2
aq and is inexpensive
when compared to the dimension of ν. For a linear-in-parameter model pQν ps, aq “ νJµps, aq,
the gradient can be efﬁciently computed for common choices of the basis function µ such as the
Gaussian function. For deep neural network models, the gradient can be computed by the automatic-
differentiation (Goodfellow et al., 2016) where its cost depends on the network architecture.

4 RELATED WORK

Besides the connections to DPG, our method is also related to existing methods as follows.

A similar optimization problem to Eq.(13) was considered by the model-free trajectory optimization
(MOTO) method (Akrour et al., 2016). Our method can be viewed as a non-trivial extension of
MOTO with two signiﬁcant novelties. First, MOTO learns a sequence of time-dependent log-linear
Gaussian policies πtpa|sq “ N pa|Bts`bt, Σtq, while our method learns a log-nonlinear Gaussian
policy. Second, MOTO learns a time-dependent critic given by pQtps, aq “ 1
2 aJCta ` aJDts `
aJct `ξtpsq and performs policy update with these functions. In contrast, our method learns a more
complex critic and performs Taylor’s approximation in each training step.

Besides MOTO, the optimization problem also resembles that of trust region policy optimization
(TRPO) (Schulman et al., 2015a). TRPO solves the following optimization problem:

max
θ1

Epπθ psq,πθ1 pa|sq

”
ı
pQps, aq

subject to Epπθ psq rKLpπθpa|sq||πθ1pa|sqqs ď (cid:15),

(33)

where pQps, aq may be replaced by an estimate of the advantage function (Schulman et al., 2015b).
There are two major differences between the two problems. First, TRPO optimizes the policy param-
eter while we optimize the guide actor. Second, TRPO solves the optimization problem by conjugate
gradient where the KL divergence is approximated by the Fisher information matrix, while we solve
the optimization problem in a closed form with a quadratic approximation of the critic.

Our method is also related to maximum-entropy RL (Ziebart et al., 2010; Azar et al., 2012; Haarnoja
et al., 2017; Nachum et al., 2017), which maximizes the expected cumulative reward with an addi-
Epπpsq rrpst, atq ` αHpπpat|stqqs, where α ą 0 is a trade-off parame-
tional entropy bonus:
ter. The optimal policy in maximum-entropy RL is the softmax policy given by

8
t“1

ř

π‹
MaxEntpa|sq “ exp

ˆ

Q‹

softps, aq ´ V ‹
α

softpsq

˙

ˆ

9 exp

˙

,

Q‹

softps, aq
α

(34)

9

Published as a conference paper at ICLR 2018

softps, aq and V ‹

where Q‹
tively (Haarnoja et al., 2017; Nachum et al., 2017). For a policy π, these are deﬁned as
softps, aq “ rps, aq ` γEpps1|s,aq

softpsq are the optimal soft action-value and state-value functions, respec-
“

(35)

Qπ

,

ż

V π
softpsq “ α log

exp

ˆ

Qπ

‰
V π
softps1q
˙
softps, aq
α

da.

(36)

The softmax policy and the soft state-value function in maximum-entropy RL closely resemble the
guide actor in Eq.(14) when η‹ “ 0 and the log-integral term in Eq.(15) when η “ 0, respectively,
except for the deﬁnition of action-value functions. To learn the optimal policy of maximum-entropy
RL, Haarnoja et al. (2017) proposed soft Q-learning which uses importance sampling to compute
the soft value functions and approximates the intractable policy using a separate policy function.
Our method largely differs from soft Q-learning since we use Taylor’s approximation to convert the
intractable integral into more convenient matrix-vector products.

The idea of ﬁrstly learning a non-parameterized policy and then later learning a parameterized policy
by supervised learning was considered previously in guided policy search (GPS) (Levine & Koltun,
2013). However, GPS learns the guide policy by trajectory optimization methods such as an iterative
linear-quadratic Gaussian regulator (Li & Todorov, 2004), which requires a model of the transition
function. In contrast, we learn the guide policy via the critic without learning the transition function.

5 EXPERIMENTAL RESULTS

We evaluate GAC on the OpenAI gym platform (Brockman et al., 2016) with the Mujoco Physics
simulator (Todorov et al., 2012). The actor and critic are neural networks with two hidden layers of
400 and 300 units, as described in Appendix C. We compare GAC-0 and GAC-1 against deep DPG
(DDPG) (Lillicrap et al., 2015), Q-learning with a normalized advantage function (Q-NAF) (Gu
et al., 2016), and TRPO (Schulman et al., 2015a;b). Figure 1 shows the learning performance on
9 continuous control tasks. Overall, both GAC-0 and GAC-1 perform comparably with existing
methods and they clearly outperform the other methods in Half-Cheetah.

The performance of GAC-0 and GAC-1 is comparable on these tasks, except on Humanoid where
GAC-1 learns faster. We expect GAC-0 to be more stable and reliable but easier to get stuck at
local optima. On the other hand, the randomness introduced by GAC-1 leads to high variance
approximation but this could help escape poor local optima. We conjecture GAC-S that uses S ą 1
samples for the averaged Taylor’s approximation should outperform both GAC-0 and GAC-1. While
this is computationally expensive, we can use parallel computation to reduce the computation time.

The expected returns of both GAC-0 and GAC-1 have high ﬂuctuations on the Hopper and Walker2D
tasks when compared to TRPO as can be seen in Figure 1g and Figure 1h. We observe that they can
learn good policies for these tasks in the middle of learning. However, the policies quickly diverge
to poor ones and then they are quickly improved to be good policies again. We believe that this
happens because the step-size F ´1psq “
of the guide actor in Eq. (22) can
be very large near local optima for Gauss-Newton approximation. That is, the gradients near local
pQps, aqJ
optima have small magnitude and this makes the approximation Hpsq “ ∇a
small as well. If η‹Σ´1 is also relatively small then the matrix F ´1psq can be very large. Thus,
under these conditions, GAC may use too large step sizes to compute the guide actor and this results
in high ﬂuctuations in performance. We expect that this scenario can be avoided by reducing the KL
bound (cid:15) or adding a regularization constant to the Gauss-Newton approximation.

˘
η‹Σ´1 ´ Hpsq

pQps, aq∇a

´1

`

Table 1 in Appendix C shows the wall-clock computation time. DDPG is computationally the most
efﬁcient method on all tasks. GAC has low computation costs on tasks with low dimensional actions
and its cost increases as the dimensionality of action increases. This high computation cost is due to
the dual optimization for ﬁnding the step-size parameters η and ω. We believe that the computation
cost of GAC can be signiﬁcantly reduced by letting η and ω be external tuning parameters.

6 CONCLUSION AND FUTURE WORK

Actor-critic methods are appealing for real-world problems due to their good data efﬁciency and
learning speed. However, existing actor-critic methods do not use second-order information of the

10

Published as a conference paper at ICLR 2018

(a) Inverted-Pend.

(b) Inv-Double-Pend.

(c) Reacher

(d) Swimmer

(e) Half-Cheetah

(f) Ant

(g) Hopper

(h) Walker2D

(i) Humanoid

Figure 1: Expected returns averaged over 10 trials. The x-axis indicates training time steps. The y-
axis indicates averaged return and higher is better. More clear ﬁgures are provided in Appendix C.2.

critic. In this paper, we established a novel framework that distinguishes itself from existing work
by utilizing Hessians of the critic for actor learning. Within this framework, we proposed a prac-
tical method that uses Gauss-Newton approximation instead of the Hessians. We showed through
experiments that our method is promising and thus the framework should be further investigated.

Our analysis showed that the proposed method is closely related to deterministic policy gradients
(DPG). However, DPG was also shown to be a limiting case of the stochastic policy gradients when
the policy variance approaches zero (Silver et al., 2014). It is currently unknown whether our frame-
work has a connection to the stochastic policy gradients as well, and ﬁnding such a connection is
our future work.

Our main goal in this paper was to provide a new actor-critic framework and we do not claim that our
method achieves the state-of-the-art performance. However, its performance can still be improved in
many directions. For instance, we may impose a KL constraint for a parameterized actor to improve
its stability, similarly to TRPO (Schulman et al., 2015a). We can also apply more efﬁcient policy
evaluation methods such as Retrace (Munos et al., 2016) to achieve better critic learning.

ACKNOWLEDGMENTS

MS was partially supported by KAKENHI 17H00757.

REFERENCES

Abbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, Nuno Lau, Lu´ıs Paulo Reis, and Gerhard Neu-
In Advances in Neural Information

mann. Model-Based Relative Entropy Stochastic Search.
Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki. Model-Free Trajec-
tory Optimization for Reinforcement Learning. In Proceedings of the 33nd International Confer-
ence on Machine Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Shun-ichi Amari. Natural Gradient Works Efﬁciently in Learning. Neural Computation, 10(2):

251–276, 1998.

Brandon Amos, Lei Xu, and J. Zico Kolter. Input Convex Neural Networks. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
August 6-11, 2017, Sydney, Australia, 2017.

Mohammad Gheshlaghi Azar, Vicenc¸ G´omez, and Hilbert J. Kappen. Dynamic Policy Program-

ming. Journal of Machine Learning Research, 13:3207–3245, 2012.

11

Published as a conference paper at ICLR 2018

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.

Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on Policy Search for Robotics.

Foundations and Trends in Robotics, 2(1-2):1–142, 2013.

Thomas Furmston, Guy Lever, and David Barber. Approximate Newton Methods for Policy Search
in Markov Decision Processes. Journal of Machine Learning Research, 17(227):1–51, 2016.

Xavier Glorot and Yoshua Bengio. Understanding the Difﬁculty of Training Deep Feedforward
Neural Networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the 13th In-
ternational Conference on Artiﬁcial Intelligence and Statistics, May 13-15, 2010, Sardinia, Italy,
2010.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous Deep Q-Learning
with Model-based Acceleration. In Proceedings of the 33nd International Conference on Machine
Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with
Deep Energy-Based Policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, August 6-11, 2017, Sydney, Australia, 2017.

Nicolas Heess, Gregory Wayne, David Silver, Timothy P. Lillicrap, Tom Erez, and Yuval Tassa.
In Advances in Neural

Learning Continuous Control Policies by Stochastic Value Gradients.
Information Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Sham Kakade. A Natural Policy Gradient. In Advances in Neural Information Processing Systems

14, December 3-8, 2001, Vancouver, British Columbia, Canada, 2001.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. CoRR,

abs/1412.6980, 2014.

Vijay R. Konda and John N. Tsitsiklis. On Actor-Critic Algorithms. SIAM Journal on Control and

Optimization, 42(4):1143–1166, April 2003.

Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the 30th International

Conference on Machine Learning, June 16-21, 2013, Atlanta, GA, USA, 2013.

Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep
Visuomotor Policies. Journal of Machine Learning Research, 17(1):1334–1373, January 2016.
ISSN 1532-4435.

Weiwei Li and Emanuel Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear Bio-
logical Movement Systems. In Proceedings of the 1st International Conference on Informatics in
Control, Automation and Robotics, August 25-28, 2004, Set´ubal, Portugal, pp. 222–229, 2004.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. CoRR,
abs/1509.02971, 2015.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement
Learning. Nature, 518(7540):529–533, February 2015. ISSN 00280836.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement
In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
Learning.
International Conference on Machine Learning, June 19-24, 2016, New York City, NY, USA,
2016.

12

Published as a conference paper at ICLR 2018

R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and Efﬁcient Off-
Policy Reinforcement Learning. In Advances in Neural Information Processing Systems 29, De-
cember 5-10, 2016, Barcelona, Spain, 2016.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the Gap Between
Value and Policy Based Reinforcement Learning. In Advances in Neural Information Processing
Systems 30, 4-9 December 2017, Long Beach, CA, USA, 2017.

Jorge Nocedal and Stephen J. Wright. Numerical Optimization, Second Edition. World Scientiﬁc,

2006.

Jan Peters and Stefan Schaal. Natural Actor-Critic. Neurocomputing, 71(7-9):1180–1190, 2008.

Jan Peters, Katharina M¨ulling, and Yasemin Altun. Relative Entropy Policy Search. In Proceedings
of the 24th AAAI Conference on Artiﬁcial Intelligence, July 11-15, 2010, Atlanta, Georgia, USA,
2010.

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region
Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning,
July 6-11, 2015, Lille, France, pp. 1889–1897, 2015a.

John Schulman, Philipp Moritz, Sergey Levine, Michael

High-Dimensional Continuous Control Using Generalized Advantage Estimation.
abs/1506.02438, 2015b.

I. Jordan, and Pieter Abbeel.
CoRR,

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic Policy Gradient Algorithms. In Proceedings of the 31st International Conference
on Machine Learning, June 21-26, 2014, Beijing, China, 2014.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game
of Go with Deep Neural Networks and Tree Search. Nature, 529(7587):484–489, 2016.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning - an Introduction. Adaptive

computation and machine learning. MIT Press, 1998.

Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy Gradi-
ent Methods for Reinforcement Learning with Function Approximation. In Advances in Neural
Information Processing Systems 12, November 29 - December 4, 1999, Colorado, USA, 1999.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics Engine for Model-Based Control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012,
Vilamoura, Algarve, Portugal, 2012.

Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforce-

ment Learning. Machine Learning, 8(3):229–256, 1992. doi: 10.1007/BF00992696.

Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable Trust-Region
Method for Deep Reinforcement Learning using Kronecker-factored Approximation. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, 4-9 December 2017, Long Beach, CA, USA,
2017.

Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling Interaction via the Principle
of Maximum Causal Entropy. In Proceedings of the 27th International Conference on Machine
Learning, June 21-24, 2010, Haifa, Israel, 2010.

13

Published as a conference paper at ICLR 2018

A DERIVATIONS AND PROOFS

A.1 DERIVATION OF SOLUTION AND DUAL FUNCTION OF GUIDE ACTOR

The solution of the optimization problem:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

can be obtained by the method of Lagrange multipliers. The derivation here follows the derivation of
similar optimization problems by Peters et al. (2010) and Abdolmaleki et al. (2015). The Lagrangian
of this optimization problem is

Lprπ, η, ω, νq “ Epβ psq,˜πpa|sq

` ηp(cid:15) ´ Epβ psq rKLp˜πpa|sq||πθpa|sqqsq

ı
”
pQps, aq

` ωpEpβ psq rHp˜πpa|sqqs ´ κq ` νpEpβ psqrπpa|sq ´ 1q,

(38)

where η, ω, and ν are the dual variables. Then, by taking derivative of L w.r.t. rπ we obtain

„ż ´

¯



BrπL “ Epβ psq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq

da

´ pη ` ω ´ νq.

(39)

We set this derivation to zero in order to obtain

0 “ Epβ psq

„ż ´

¯
pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq



da

´ pη ` ω ´ νq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq ´ pη ` ω ´ νq.

“

Then the solution is given by

rπpa|sq “ πθpa|sq

η
η`ω exp

9 πθpa|sq

η
η`ω exp

¸

ˆ

exp

´

˙

η ` ω ´ ν
η ` ω

˜

˜

pQps, aq
η ` ω
pQps, aq
η ` ω

¸

.

(37)

(40)

(41)

(42)

To obtain the dual function gpη, ωq, we substitute the solution to the constraint terms of the La-
grangian and this gives us

Lpη, ω, νq “ Epβ psq,rπpa|sq

ı

”
pQps, aq

«

´ pη ` ωqEpβ psq,rπpa|sq

pQps, aq
η ` ω

`
`

η
log πθpa|sq ´
η ` ω
˘
Epβ ,rπpa|sq ´ 1

ﬀ

η ` ω ´ ν
η ` ω

` ηEpβ psq,rπpa|sq rlog πθpa|sqs ` ν

` η(cid:15) ´ ωκ.

(43)

After some calculation, we obtain

Lpη, ω, νq “ η(cid:15) ´ ωκ ` Epβ psq rη ` ω ´ νs
«ż

“ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

η

πθpa|sq

η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

“ gpη, ωq,

(44)

where in the second line we use the fact that expp´ η`ω´ν

η`ω q is the normalization term of rπpa|sq.

14

Published as a conference paper at ICLR 2018

A.2 PROOF OF SECOND-ORDER OPTIMIZATION IN ACTION SPACE

Firstly, we show that GAC performs second-order optimization in the action space when Taylor’s
approximation is performed with a0 “ Eπpa|sq ras “ φθpsq. Recall that Taylor’s approximation
with φθ is given by

1
2

pQps, aq “
pQps, aq|a“φθ psq ´ H φθ psqφθpsq. By substituting ψφθ

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq,

(45)

psq into Lpsq “

where ψφθ
η‹Σ´1

psq “ ∇a

θ psqφθpsq ´ ψθpsq, we obtain

Lpsq “ η‹Σ´1
“ pη‹Σ´1

θ psqφθpsq ` ∇a
θ psq ´ H φθ psqqφθpsq ` ∇a
pQps, aq|a“φθ psq.

pQps, aq|a“φθ psq ´ H φθ psqφθpsq

pQps, aq|a“φθ psq

“ F psqφθpsq ` ∇a
Therefore, the mean update is equivalent to

φ`psq “ F ´1psqLpsq

“ φθpsq ` F ´1psq∇a

pQps, aq|a“φθ psq,

which is a second-order optimization step with a curvature matrix F psq “ η‹Σ´1

θ psq ´ H φθ psq.

Lpsq “ η‹Σ´1

Similarly, for the case where a set of samples ta0u „ πθpa0|sq “ N pa0|φθpsq, Σpsqq is used to
compute the averaged Taylor’s approximation, we obtain
θ psqφθpsq ` Eπθ pa0|sq

”
∇a
Then, by assuming that Eπθ pa0|sq rH 0psqa0psqs “ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s, we obtain
”
∇a
”
∇a

pQps, aq|a“a0
pQps, aq|a“a0

´ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s

´ Eπθ pa0|sq rH 0psqa0psqs .

θ psqφθpsq ` Eπθ pa0|sq

θ psqφθpsq ` Eπθ pa0|sq

pQps, aq|a“a0

Lpsq “ η‹Σ´1

“ η‹Σ´1

(48)

ı

ı

ı

´ Eπθ pa0|sq rH 0s φθpsq
ı

”
∇a

pQps, aq|a“a0

“ pη‹Σ´1

θ psq ´ Eπθ pa0|sq rH 0psqsqφθpsq ` Eπθ pa0|sq

“ F psqφθpsq ` Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

.

Therefore, we have a second-order optimization step

φ`psq “ φθpsq ` F ´1psqEπθ pa0|sq

(50)
where F ´1psq “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs is a curvature matrix. As described in
the main paper, this interpretation is only valid when the equality Eπθ pa0|sq rH 0psqa0psqs “
Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s holds. While this equality does not hold in general, it holds when
only one sample a0 „ πθpa0|sq is used. Nonetheless, we can still use the expectation of Taylor’s
approximation to perform policy update regardless of this assumption.

,

pQps, aq|a“a0

”
∇a

ı

(46)

(47)

(49)

A.3 PROOF OF GAUSS-NEWTON APPROXIMATION

Let f ps, aq “ expp

pQps, aqq, then the Hessian Hpsq “ ∇2

pQps, aq can be expressed as

a

Hpsq “ ∇a r∇a log f ps, aqs
“
∇af ps, aqf ps, aq´1
∇af ps, aq´1

“ ∇a

`

‰

˘

J

“ ∇af ps, aq
“ ∇af ps, aq∇f f ps, aq´1 p∇af ps, aqqJ ` ∇2
“ ´∇af ps, aqf ps, aq´2 p∇af ps, aqqJ ` ∇2

` ∇2

af ps, aqf ps, aq´1

af ps, aqf ps, aq´1
af ps, aqf ps, aq´1

`

˘ `

∇af ps, aqf ps, aq´1

“ ´
“ ´∇a log f ps, aq∇a log f ps, aqJ ` ∇2
a expp
“ ´∇a

pQps, aqJ ` ∇2

pQps, aq∇a

` ∇2
af ps, aqf ps, aq´1
pQps, aqq expp´

∇af ps, aqf ps, aq´1

pQps, aqq,

˘

J

af ps, aqf ps, aq´1

(51)

15

Published as a conference paper at ICLR 2018

which concludes the proof.

Beside Gauss-Newton approximation, an alternative approach is to impose a special structure on
pQps, aq so that Hessians are always negative semi-deﬁnite. In literature, there exists two special
structures that satisﬁes this requirement.

Normalized advantage function (NAF) (Gu et al., 2016): NAF represents the critic by a quadratic
function with a negative curvature:

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(52)

where a negative-deﬁnite matrix-valued function W psq, a vector-valued function bpsq and a scalar-
valued function V psq are parameterized functions whose their parameters are learned by policy
evaluation methods such as Q-learning (Sutton & Barto, 1998). With NAF, negative deﬁnite Hes-
sians can be simply obtained as Hpsq “ W psq. However, a signiﬁcant disadvantage of NAF is that
it assumes the action-value function is quadratic regardless of states and this is generally not true for
most reward functions. Moreover, the Hessians become action-independent even though the critic is
a function of actions.

Input convex neural networks (ICNNs) (Amos et al., 2017): ICNNs are neural networks with
special structures which make them convex w.r.t. their inputs. Since Hessians of concave functions
are always negative semi-deﬁnite, we may use ICNNs to represent a negative critic and directly use
its Hessians. However, similarly to NAF, ICNNs implicitly assume that the action-value function is
concave w.r.t. actions regardless of states and this is generally not true for most reward functions.

A.4 GRADIENT OF THE LOSS FUNCTIONS FOR LEARNING PARAMETERIZED ACTOR

We ﬁrst consider
is
N pa|φ`psq, Σ`psqq and the current actor is N pa|φθpsq, Σθpsqq. Taylor’s approximation of
pQps, aq at a0 “ φθpsq is

loss function where the guide actor

the weight mean-squared-error

pQps, aq “

1
2

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq.

(53)

By assuming that H φθ psq is strictly negative deﬁnite5, we can take a derivative of this approxima-
tion w.r.t. a and set it to zero to obtain a “ H ´1
psq. Replacing a by
φθ
φθpsq and φ`psq yields

pQps, aq ´ H ´1
φθ

psqψφθ

psq∇a

φθpsq “ H ´1
φθ
φ`psq “ H ´1
φθ

psq∇a

psq∇a

pQps, aq|a“φθ psq ´ H ´1
pQps, aq|a“φ`psq ´ H ´1

φθ

φθ

psqψφθ
psqψφθ

psq,

psq.

Recall that the weight mean-squared-error is deﬁned as

LWpθq “ Epβ psq

”
}φθpsq ´ φ`psq}2

F psq

ı

.

(54)

(55)

(56)

5This can be done by subtracting a small positive value to the diagonal entries of Gauss-Newton approxi-

mation.

16

Published as a conference paper at ICLR 2018

Then, we consider its gradient w.r.t. θ as follows:

∇θLWpθq “ 2Epβ psq
“ 2Epβ psq
“ 2Epβ psq

“

“

“

`

∇θφθpsqF psq
∇θφθpsqpη‹Σ´1
∇θφθpsqpη‹Σ´1
´
H ´1
φθ

ˆ

˘‰

`

φθpsq ´ φ`psq
θ psq ´ H φθ psqq
θ psq ´ H φθ psqq

˘‰

φθpsq ´ φ`psq

¯ı

psq∇a

pQps, aq|a“φθ psq ´ H ´1

φθ

psq∇a

pQps, aq|a“φ`psq

“ 2η‹Epβ psq

´

psq

”
θ psqH ´1
∇θφθpsqΣ´1
φθ
´
”
∇θφθpsq

` 2Epβ psq
”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

∇a
pQps, aq|a“φθ psq

pQps, aq|a“φθ psq ´ ∇a
pQps, aq|a“φθ psq
”
∇θφθpsq∇a
ı

∇a
pQps, aq|a“φ`psq ´ ∇a
ı
` 2Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

psq∇a

psq∇a

ı

.

` 2η‹Epβ psq

´ 2η‹Epβ psq

“ ´2Epβ psq

pQps, aq|a“φ`psq
¯ı

pQps, aq|a“φ`psq

¯ı

ı

(57)

This concludes the proof for the gradient in Eq.(28). Note that we should not directly replace
the mean functions in the weight mean-square-error by Eq.(54) and Eq.(55) before expanding the
gradient. This is because the analysis would require computing the gradient w.r.t. θ inside the
Hessians and this is not trivial. Moreover, when we perform gradient descent in practice, the mean
φ`psq is considered as a constant w.r.t. θ similarly to an output function in supervised learning.
The gradient for the mean-squared-error can be derived similarly. Let the mean-squared-error be
deﬁned as

LMpθq “

Epβ psq

}φθpsq ´ φ`psq}2
2

.

“

‰

1
2

(58)

Its gradient w.r.t. θ is given by

“

`

˘‰

∇θLMpθq “ Epβ psq
“ Epβ psq

∇θφθpsq
”
∇θφθpsqH ´1psq

φθpsq ´ φ`psq
´

∇a

which concludes the proof for the gradient in Eq.(30).

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

,

(59)

¯ı

To show that ∇a
˘
`
ηΣ´1psq ´ H φθ psq

´1

´
η‹Σ´1psqφθpsq ` ψφθ

pQps, aq|a“φ`psq “ 0 when η‹ “ 0, we directly substitute η‹ “ 0 into φ`psq “
¯

psqq

and this yield

φ`psq “ ´H ´1
φθ
pQps, aq|a“φ`psq ´ H ´1
Since φ`psq “ H ´1
psq∇a
psq from Eq.(55) and the Hessians are
φθ
pQps, aq|a“φ`psq “ 0. This is intuitive since without the KL constraint,
non-zero, it has to be that ∇a
the mean of the guide actor always be at the optima of the second-order Taylor’s approximation and
the gradients are zero at the optima.

psqψφθ

psqψφθ

psq.

(60)

φθ

A.5 RELATION TO Q-LEARNING WITH NORMALIZED ADVANTAGE FUNCTION

The normalized advantage function (NAF) (Gu et al., 2016) is deﬁned as

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(61)

where W psq is a negative deﬁnite matrix. Gu et al. (2016) proposed to perform Q-learning with
NAF by using the fact that argmaxa
Here, we show that GAC includes the Q-learning method by Gu et al. (2016) as its special case. This
can be shown by using NAF as a critic instead of performing Taylor’s approximation of the critic.

pQNAFps, aq “ bpsq and maxa

pQNAFps, aq “ V psq.

17

Published as a conference paper at ICLR 2018

pQNAFps, aq “

Firstly, we expand the quadratic term of NAF as follows:
1
2
1
2
1
2

aJW psqa ´ aJW psqbpsq `

aJW psqa ` aJψpsq ` ξpsq,

“

“

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq

bpsqJW psqbpsq ` V psq

(62)

where ψpsq “ ´W psqbpsq and ξpsq “ 1
2 bpsqJW psqbpsq ` V psq. By substituting the quadratic
model obtained by NAF into the GAC framework, the guide actor is now given by ˜πpa|sq “
N pa|φ`psq, Σ`psqqq with

φ`psq “ pη‹Σ´1psqq ´ W psqq´1pη‹Σ´1psqφθpsq ´ W psqbpsqq
Σ`psq “ pη‹ ` ω‹qpη‹Σ´1psqq ´ W psqq.

(64)
To obtain Q-learning with NAF, we set η‹ “ 0, i.e., we perform a greedy maximization where the
KL upper-bound approaches inﬁnity, and this yields

(63)

φ`psq “ ´W psq´1p´W psqbpsqq

(65)
which is the policy obtained by performing Q-learning with NAF. Thus, NAF with Q-learning is a
special case of GAC if Q-learning is also used in GAC to learn the critic.

“ bpsq,

The pseudo-code of GAC is given in Algorithm 1. The source code is available at https://
github.com/voot-t/guide-actor-critic.

B PSEUDO-CODE OF GAC

C EXPERIMENT DETAILS

C.1

IMPLEMENTATION

We try to follow the network architecture proposed by the authors of each baseline method as close as
possible. For GAC and DDPG, we use neural networks with two hidden layers for the actor network
and the critic network. For both networks the ﬁrst layer has 400 hidden units and the second layer
has 300 units. For NAF, we use neural networks with two hidden layers to represent each of the
functions bpsq, W psq and V psq where each layer has 200 hidden units. All hidden units use the
relu activation function except for the output of the actor network where we use the tanh activation
function to bound actions. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate
0.001 and 0.0001 for the critic network and the actor network, respectively. The moving average
step for target networks is set to τ “ 0.001. The maximum size of the replay buffer is set to
1000000. The mini-batches size is set to N “ 256. The weights of the actor and critic networks
are initialized as described by Glorot & Bengio (2010), except for the output layers where the initial
weights are drawn uniformly from Up´0.003, 0.003q, as described by Lillicrap et al. (2015). The
initial covariance Σ in GAC is set to be an identity matrix. DDPG and QNAF use the OU-process
with noise parameters θ “ 0.15 and σ “ 0.2 for exploration .

For TRPO, we use the implementation publicly available at https://github.com/openai/
baselines. We also use the provided network architecture and hyper-parameters except the batch
size where we use 1000 instead of 1024 since this is more suitable in our test setup.

For GAC, the KL upper-bound is ﬁxed to (cid:15) “ 0.0001. The entropy lower-bound κ is adjusted
heuristically by

κ “ maxp0.99pE ´ E0q ` E0, E0q,
(71)
where E « Epβ psq rHpπθpa|sqqs denotes the expected entropy of the current policy and E0 denotes
the entropy of a base policy N pa|0, 0.01Iq. This heuristic ensures that the lower-bound gradually
decreases but the lower-bound cannot be too small. We apply this heuristic update once every 5000
training steps. The dual function is minimize by the sequential least-squares quadratic programming
(SLSQP) method with an initial values η “ 0.05 and ω “ 0.05. The number of samples for
computing the target critic value is M “ 10.

18

Published as a conference paper at ICLR 2018

Algorithm 1 Guide actor critic

1: Input:

Initial actor πθpa|sq “ N pa|φθpsq, Σq, critic pQν ps, aq, target critic network

pQ¯ν ps, aq, KL bound (cid:15), entropy bound κ, learning rates α1, α2, and data buffer D “ H.

procedure COLLECT TRANSITION SAMPLE

Observe state st and sample action at „ N pa|φθpstq, Σq.
Execute at, receive reward rt and next state s1
t.
Add transition tpst, at, rt, s1

tqu to D.

2: for t “ 1, . . . , Tmax do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

end procedure
procedure LEARN

Sample N mini-batch samples tpsn, an, rn, s1
procedure UPDATE CRITIC
n,muM
Sample actions ta1
Compute yn, update ν by, e.g., Adam, and update ¯ν by moving average:

m“1 „ N pa|φθpsnq, Σq for each sn.

n“1 uniformly from D.

nquN

yn “ rn ` γ

pQ¯ν ps1

n, a1

n,mq,

ν Ð ν ´ α1

´
pQν psn, anq ´ yn

¯

2

,

∇ν

1
M

1
N

Mÿ

m“1
Nÿ

n“1

¯ν Ð τ ν ` p1 ´ τ q¯ν.

end procedure
procedure LEARN GUIDE ACTOR

Compute an,0 for each sn by an,0 “ φθpsnq or an,0 „ N pa|φθpsnq, Σq.
pQpsn, aq|a“an,0 and H 0pssq “ ´g0psnqg0psnqJ.
Compute g0psq “ ∇a
Solve for pη‹, ω‹q “ argminηą0,ωą0
Compute the guide actor rπpa|snq “ N pa|φ`psnq, Σ`psnqq for each sn.

pgpη, ωq by a non-linear optimization method.

end procedure
procedure UPDATE PARAMETERIZED ACTOR

Update policy parameter by, e.g., Adam, to minimize the MSE:

13:
14:
15:

16:
17:
18:
19:
20:
21:

θ Ð θ ´ α2

∇θ}φθpsnq ´ φ`psnq}2
2.

1
N

Nÿ

n“1

22:

Update policy covariance by averaging the guide covariances:

Σ Ð

Σ`psnq.

1
N

(66)

(67)

(68)

(69)

(70)

end procedure

end procedure

23:
24:
25: end for
26: Output: Learned actor πθpa|sq.

C.2 ENVIRONMENTS AND RESULTS

We perform experiments on the OpenAI gym platform (Brockman et al., 2016) with Mujoco Physics
simulator (Todorov et al., 2012) where all environments are v1. We use the state space, action space
and the reward function as provided and did not perform any normalization or gradient clipping.
The maximum time horizon in each episode is set to 1000. The discount factor γ “ 0.99 is only
used for learning and the test returns are computed without it.

Experiments are repeated for 10 times with different random seeds. The total computation time are
reported in Table 1. The ﬁgures below show the results averaged over 10 trials. The y-axis indicates
the averaged test returns where the test returns in each trial are computed once every 5000 training
time steps by executing 10 test episodes without exploration. The error bar indicates standard error.

19

Published as a conference paper at ICLR 2018

Table 1: The total computation time for training the policy for 1 million steps (0.1 million steps for
the Invert-Pendulum task). The mean and standard error are computed over 10 trials with the unit in
hours. TRPO is not included since it performs a lesser amount of update using batch data samples.

Task
Inv-Pend.
Inv-Double-Pend.
Reacher
Swimmer
Half-Cheetah
Ant
Hopper
Walker2D
Humanoid

GAC-1
1.13(0.09)
15.56(0.93)
17.23(0.87)
12.43(0.74)
29.82(1.76)
37.84(1.80)
18.99(1.06)
33.42(0.96)
111.07(2.99)

GAC-0
0.80(0.04)
15.67(0.77)
12.64(0.38)
11.94(0.74)
32.64(1.41)
40.57(3.06)
14.22(0.56)
31.71(1.84)
106.80(6.80)

DDPG
0.45(0.02)
9.04(0.29)
10.67(0.37)
9.61(0.52)
10.13(0.41)
9.75(0.37)
8.74(0.45)
7.92(0.11)
13.90(1.60)

QNAF
0.40(0.03)
7.47(0.22)
30.91(2.09)
32.44(2.21)
27.94(2.37)
27.09(0.94)
26.48(1.42)
26.94(1.99)
30.43(2.21)

Figure 2: Performance averaged over 10 trials on the Inverted Pendulum task.

20

Published as a conference paper at ICLR 2018

Figure 3: Performance averaged over 10 trials on the Inverted Double Pendulum task.

Figure 4: Performance averaged over 10 trials on the Reacher task.

21

Published as a conference paper at ICLR 2018

Figure 5: Performance averaged over 10 trials on the Swimmer task.

Figure 6: Performance averaged over 10 trials on the Half-Cheetah task.

22

Published as a conference paper at ICLR 2018

Figure 7: Performance averaged over 10 trials on the Ant task.

Figure 8: Performance averaged over 10 trials on the Hopper task.

23

Published as a conference paper at ICLR 2018

Figure 9: Performance averaged over 10 trials on the Walker2D task.

Figure 10: Performance averaged over 10 trials on the Humanoid task.

24

8
1
0
2
 
b
e
F
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
6
7
0
.
5
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

GUIDE ACTOR-CRITIC FOR CONTINUOUS CONTROL

Voot Tangkaratt
RIKEN AIP, Tokyo, Japan
voot.tangkaratt@riken.jp

Abbas Abdolmaleki
The University of Aveiro, Aveiro, Portugal
abbas.a@ua.pt

Masashi Sugiyama
RIKEN AIP, Tokyo, Japan
The University of Tokyo, Tokyo, Japan
masashi.sugiyama@riken.jp

ABSTRACT

Actor-critic methods solve reinforcement learning problems by updating a param-
eterized policy known as an actor in a direction that increases an estimate of the
expected return known as a critic. However, existing actor-critic methods only
use values or gradients of the critic to update the policy parameter. In this pa-
per, we propose a novel actor-critic method called the guide actor-critic (GAC).
GAC ﬁrstly learns a guide actor that locally maximizes the critic and then it up-
dates the policy parameter based on the guide actor by supervised learning. Our
main theoretical contributions are two folds. First, we show that GAC updates the
guide actor by performing second-order optimization in the action space where
the curvature matrix is based on the Hessians of the critic. Second, we show that
the deterministic policy gradient method is a special case of GAC when the Hes-
sians are ignored. Through experiments, we show that our method is a promising
reinforcement learning method for continuous controls.

1

INTRODUCTION

The goal of reinforcement learning (RL) is to learn an optimal policy that lets an agent achieve the
maximum cumulative rewards known as the return (Sutton & Barto, 1998). Reinforcement learning
has been shown to be effective in solving challenging artiﬁcial intelligence tasks such as playing
games (Mnih et al., 2015; Silver et al., 2016) and controlling robots (Deisenroth et al., 2013; Levine
et al., 2016).

Reinforcement learning methods can be classiﬁed into three categories: value-based, policy-based,
and actor-critic methods. Value-based methods learn an optimal policy by ﬁrstly learning a value
function that estimates the expected return. Then, they infer an optimal policy by choosing an
action that maximizes the learned value function. Choosing an action in this way requires solving a
maximization problem which is not trivial for continuous controls. While extensions to continuous
controls were considered recently, they are restrictive since speciﬁc structures of the value function
are assumed (Gu et al., 2016; Amos et al., 2017).

On the other hand, policy-based methods, also called policy search methods (Deisenroth et al.,
2013), learn a parameterized policy maximizing a sample approximation of the expected re-
turn without learning the value function. For instance, policy gradient methods such as REIN-
FORCE (Williams, 1992) use gradient ascent to update the policy parameter so that the probability
of observing high sample returns increases. Compared with value-based methods, policy search
methods are simpler and naturally applicable to continuous problems. Moreover, the sample return
is an unbiased estimator of the expected return and methods such as policy gradients are guaranteed
to converge to a locally optimal policy under standard regularity conditions (Sutton et al., 1999).
However, sample returns usually have high variance and this makes such policy search methods
converge too slowly.

Actor-critic methods combine the advantages of value-based and policy search methods. In these
methods, the parameterized policy is called an actor and the learned value-function is called a critic.

1

Published as a conference paper at ICLR 2018

The goal of these methods is to learn an actor that maximizes the critic. Since the critic is a low vari-
ance estimator of the expected return, these methods often converge much faster than policy search
methods. Prominent examples of these methods are actor-critic (Sutton et al., 1999; Konda & Tsit-
siklis, 2003), natural actor-critic (Peters & Schaal, 2008), trust-region policy optimization (Schul-
man et al., 2015a), and asynchronous advantage actor-critic (Mnih et al., 2016). While their ap-
proaches to learn the actor are different, they share a common property that they only use the value
of the critic, i.e., the zero-th order information, and ignore higher-order ones such as gradients and
Hessians w.r.t. actions of the critic1. To the best of our knowledge, the only actor-critic methods
that use gradients of the critic to update the actor are deterministic policy gradients (DPG) (Silver
et al., 2014) and stochastic value gradients (Heess et al., 2015). However, these two methods do not
utilize the second-order information of the critic.

In this paper, we argue that the second-order information of the critic is useful and should not be
ignored. A motivating example can be seen by comparing gradient ascent to the Newton method: the
Newton method which also uses the Hessian converges to a local optimum in a fewer iterations when
compared to gradient ascent which only uses the gradient (Nocedal & Wright, 2006). This suggests
that the Hessian of the critic can accelerate actor learning which leads to higher data efﬁciency.
However, the computational complexity of second-order methods is at least quadratic in terms of
the number of optimization variables. For this reason, applying second-order methods to optimize
the parameterized actor directly is prohibitively expensive and impractical for deep reinforcement
learning which represents the actor by deep neural networks.

Our contribution in this paper is a novel actor-critic method for continuous controls which we call
guide actor-critic (GAC). Unlike existing methods, the actor update of GAC utilizes the second-
order information of the critic in a computationally efﬁcient manner. This is achieved by separating
actor learning into two steps. In the ﬁrst step, we learn a non-parameterized Gaussian actor that
locally maximizes the critic under a Kullback-Leibler (KL) divergence constraint. Then, the Gaus-
sian actor is used as a guide for learning a parameterized actor by supervised learning. Our analysis
shows that learning the mean of the Gaussian actor is equivalent to performing a second-order update
in the action space where the curvature matrix is given by Hessians of the critic and the step-size is
controlled by the KL constraint. Furthermore, we establish a connection between GAC and DPG
where we show that DPG is a special case of GAC when the Hessians and KL constraint are ignored.

2 BACKGROUND

In this section, we ﬁrstly give a background of reinforcement learning. Then, we discuss existing
second-order methods for policy learning and their issue in deep reinforcement learning.

2.1 REINFORCEMENT LEARNING

We consider discrete-time Markov decision processes (MDPs) with continuous state space S Ď Rds
and continuous action space A Ď Rda . We denote the state and action at time step t P N by
st and at, respectively. The initial state s1 is determined by the initial state density s1 „ ppsq.
At time step t, the agent in state st takes an action at according to a policy at „ πpa|stq and
obtains a reward rt “ rpst, atq. Then, the next state st`1 is determined by the transition function
st`1 „ pps1|st, atq. A trajectory τ “ ps1, a1, r1, s2, . . . q gives us the cumulative rewards or return
t“1 γt´1rpst, atq, where the discount factor 0 ă γ ă 1 assigns different weights to
deﬁned as
rewards given at different time steps. The expected return of π after executing an action a in a state
s can be expressed through the action-value function which is deﬁned as

ř

8

ﬀ

Qπps, aq “ Eπpat|stqtě2,ppst`1|st,atqtě1

γt´1rpst, atq|s1 “ s, a1 “ a

,

(1)

«

8ÿ

t“1

where Ep r¨s denotes the expectation over the density p and the subscript t ě 1 indicates that the
expectation is taken over the densities at time steps t ě 1. We can deﬁne the expected return as
8ÿ

«

ﬀ

J pπq “ Epps1q,πpat|stqtě1,ppst`1|st,atqtě1

γt´1rpst, atq

“ Eppsq,πpa|sq rQπps, aqs .

(2)

1This is different from using the gradient of the critic w.r.t. critic parameters to update the critic itself.

t“1

2

Published as a conference paper at ICLR 2018

The goal of reinforcement learning is to ﬁnd an optimal policy that maximizes the expected return.
The policy search approach (Deisenroth et al., 2013) parameterizes π by a parameter θ P Rdθ and
ﬁnds θ‹ which maximizes the expected return:

θ‹ “ argmax

Eppsq,πθ pa|sq rQπθ ps, aqs .

θ

(3)

(4)

(5)

(7)

Policy gradient methods such as REINFORCE (Williams, 1992) solve this optimization problem by
gradient ascent:

θ Ð θ ` αEppsq,πθ pa|sq r∇θ log πθpa|sqQπθ ps, aqs ,

ř

where α ą 0 is a step-size. In policy search, the action-value function is commonly estimated by the
sample return: Qπθ ps, aq « 1
n,t γt´1rpst,n, at,nq obtained by collecting N trajectories using
N
πθ. The sample return is an unbiased estimator of the action-value function. However, it often has
high variance which leads to slow convergence.
An alternative approach is to estimate the action-value function by a critic denoted by pQps, aq
whose parameter is learned such that pQps, aq « Qπθ ps, aq. By replacing the action-value function
in Eq.(3) with the critic, we obtain the following optimization problem:

θ‹ “ argmax

Eppsq,πθ pa|sq

θ

ı

”
pQps, aq

.

The actor-critic method (Sutton et al., 1999) solves this optimization problem by gradient ascent:
”
∇θ log πθpa|sq

θ Ð θ ` αEppsq,πθ pa|sq

pQps, aq

ı

.

(6)

The gradient in Eq.(6) often has less variance than that in Eq.(4), which leads to faster convergence2.
A large class of actor-critic methods is based on this method (Peters & Schaal, 2008; Mnih et al.,
2016). As shown in these papers, these methods only use the value of the critic to learn the actor.

The deterministic policy gradients (DPG) method (Silver et al., 2014) is an actor-critic method that
uses the ﬁrst-order information of the critic. DPG updates a deterministic actor πθpsq by

θ Ð θ ` αEppsq

”
∇θπθpsq∇a

ı

pQps, aq|a“πθ psq

.

A method related to DPG is the stochastic value gradients (SVG) method (Heess et al., 2015) that
is able to learn a stochastic policy but it requires learning a model of the transition function.

The actor-critic methods described above only use up to the ﬁrst-order information of the critic when
learning the actor and ignore higher-order ones. Below, we discuss existing approaches that utilize
the second-order information by applying second-order optimization methods to solve Eq.(5).

2.2 SECOND-ORDER METHODS FOR POLICY LEARNING

The actor-critic methods described above are ﬁrst-order methods which update the optimization
variables based on the gradient of the objective function. First-order methods are popular in deep
learning thanks to their computational efﬁciency. However, it is known in machine learning that
second-order methods often lead to faster learning because they use the curvature information to
compute a better update direction, i.e., the steepest ascent direction along the curvature3.

The main idea of second-order methods is to rotate the gradient by the inverse of a curvature matrix.
For instance, second-order updates for the actor-critic method in Eq.(6) are in the following form:

θ Ð θ ` αG´1

!
Eppsq,πθ pa|sq

”
∇θ log πθpa|sq

ı)

pQps, aq

,

(8)

2This gradient is a biased estimator of the policy gradient in Eq.(4). However, it is unbiased under some

regularity conditions such as compatible function conditions (Sutton et al., 1999).

3We use the term “second-order methods” in a broad sense here, including quasi-Newton and natural gradi-

ent methods which approximate the curvature matrix by the ﬁrst-order information.

3

Published as a conference paper at ICLR 2018

where G P Rdθ ˆdθ is a curvature matrix. The behavior of second-order methods depend on the
deﬁnition of a curvature matrix. The most well-known second-order method is the Newton method
where its curvature matrix is the Hessian of the objective function w.r.t. the optimization variables:
∇θ log πθpa|sq∇θ log πθpa|sqJ ` ∇2

GHessian “ Eppsq,πθ pa|sq

θ log πθpa|sq

pQps, aq

”`

(9)

ı

˘

.

“

∇θ log πθpa|sq∇θ log πθpa|sqJ

The natural gradient method is another well-known second-order method which uses the Fisher
information matrix (FIM) as the curvature matrix (Amari, 1998):
GFIM “ Eppsq,πθ pa|sq

(10)
Unlike the Hessian matrix, FIM provides information about changes of the policy measured by an
approximated KL divergence: Eppsq rKLpπθpa|sq||πθ1pa|sqqs « pθ ´ θ1qJGFIMpθ ´ θ1q (Kakade,
2001). We can see that GHessian and GFIM are very similar but the former also contains the critic
and the Hessian of the actor while the latter does not. This suggests that the Hessian provides more
information than that in FIM. However, FIM is always positive semi-deﬁnite while the Hessian may
be indeﬁnite. Please see Furmston et al. (2016) for detailed comparisons between the two curvature
matrices in policy search4. Nonetheless, actor-critic methods based on natural gradient were shown
to be very efﬁcient (Peters & Schaal, 2008; Schulman et al., 2015a).

‰

.

We are not aware of existing work that considers second-order updates for DPG or SVG. However,
their second-order updates can be trivially derived. For example, a Newton update for DPG is

θ Ð θ ` αD´1

!
Eppsq

”
∇θπθpsq∇a

pQps, aq|a“πθ psq

ı)

,

where the pi, jq-th entry of the Hessian matrix D P Rdθ ˆdθ is

Dij “ Eppsq

∇2
a

pQps, aq|a“πθ psq

Bπθpsq
Bθj

`

B2πθpsq
BθiBθj

J

∇a

pQps, aq|a“πθ psq

.

(12)

«

J

Bπθpsq
Bθi

(11)

ﬀ

Note that Bπθpsq{Bθ and B2πθpsq{BθBθ1 are vectors since πθpsq is a vector-valued function. In-
terestingly, the Hessian of DPG contains the Hessians of the actor and the critic. In contrast, the
Hessian of the actor-critic method contains the Hessian of the actor and the value of the critic.

Second-order methods are appealing in reinforcement learning because they have high data efﬁ-
ciency. However, inverting the curvature matrix (or solving a linear system) requires cubic compu-
tational complexity in terms of the number of optimization variables. For this reason, the second-
order updates in Eq.(8) and Eq.(11) are impractical in deep reinforcement learning due to a large
number of weight parameters in deep neural networks. In such a scenario, an approximation of
the curvature matrix is required to reduce the computational burden. For instance, Furmston et al.
(2016) proposed to use only diagonal entries of an approximated Hessian matrix. However, this
approximation clearly leads to a loss of useful curvature information since the gradient is scaled but
not rotated. More recently, Wu et al. (2017) proposed a natural actor-critic method that approxi-
mates block-diagonal entries of FIM. However, this approximation corresponds to ignoring useful
correlations between weight parameters in different layers of neural networks.

3 GUIDE ACTOR-CRITIC

In this section, we propose the guide actor-critic (GAC) method that performs second-order updates
without the previously discussed computational issue. Unlike existing methods that directly learn
the parameterized actor from the critic, GAC separates the problem of learning the parameterized
actor into problems of 1) learning a guide actor that locally maximizes the critic, and 2) learning a
parameterized actor based on the guide actor. This separation allows us to perform a second-order
update for the guide actor where the dimensionality of the curvature matrix is independent of the
parameterization of the actor.

We formulate an optimization problem for learning the guide actor in Section 3.1 and present its so-
lution in Section 3.2. Then in Section 3.3 and Section 3.4, we show that the solution corresponds to
performing second-order updates. Finally, Section 3.5 presents the learning step for the parameter-
ized actor using supervised learning. The pseudo-code of our method is provided in Appendix B and
the source code is available at https://github.com/voot-t/guide-actor-critic.

4 Furmston et al. (2016) proposed an approximate Newton method for policy search. Their policy search

method was shown to perform better than methods based on gradient ascent and natural gradient ascent.

4

Published as a conference paper at ICLR 2018

3.1 OPTIMIZATION PROBLEM FOR GUIDE ACTOR

Our ﬁrst goal is to learn a guide actor that maximizes the critic. However, greedy maximization
should be avoided since the critic is a noisy estimate of the expected return and a greedy actor
may change too abruptly across learning iterations. Such a behavior is undesirable in real-world
problems, especially in robotics (Deisenroth et al., 2013).
Instead, we maximize the critic with
additional constraints:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

(13)

where ˜πpa|sq is the guide actor to be learned, πθpa|sq is the current parameterized actor that we
want to improve upon, and pβpsq is the state distribution induced by past trajectories. The objective
function differs from the one in Eq.(5) in two important aspects. First, we maximize for a policy
function ˜π and not for the policy parameter. This is more advantageous than optimizing for a policy
parameter since the policy function can be obtained in a closed form, as will be shown in the next
subsection. Second, the expectation is deﬁned over a state distribution from past trajectories and this
gives us off-policy methods with higher data efﬁciency. The ﬁrst constraint is the Kullback-Leibler
(KL) divergence constraint where KLpppxq||qpxqq “ Eppxq rlog ppxq ´ log qpxqs. The second con-
straint is the Shannon entropy constraint where Hpppxqq “ ´Eppxq rlog ppxqs. The KL constraint
is commonly used in reinforcement learning to prevent unstable behavior due to excessively greedy
update (Peters & Schaal, 2008; Peters et al., 2010; Levine & Koltun, 2013; Schulman et al., 2015a).
The entropy constraint is crucial for maintaining stochastic behavior and preventing premature con-
vergence (Ziebart et al., 2010; Abdolmaleki et al., 2015; Mnih et al., 2016; Haarnoja et al., 2017).
The ﬁnal constraint ensures that the guide actor is a proper probability density. The KL bound
(cid:15) ą 0 and the entropy bound ´8 ă κ ă 8 are hyper-parameters which control the exploration-
exploitation trade-off of the method. In practice, we ﬁx the value of (cid:15) and adaptively reduce the
value of κ based on the current actor’s entropy, as suggested by Abdolmaleki et al. (2015). More
details of these tuning parameters are given in Appendix C.

This optimization problem can be solved by the method of Lagrange multipliers. The solution is

˜πpa|sq 9 πθpa|sq

η‹`ω‹ exp

η‹

˜

¸

,

pQps, aq
η‹ ` ω‹

(14)

where η‹ ą 0 and ω‹ ą 0 are dual variables corresponding to the KL and entropy constraints,
respectively. The dual variable corresponding to the probability density constraint is contained in the
normalization term and is determined by η‹ and ω‹. These dual variables are obtained by minimizing
the dual function:

«

ż

gpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

πθpa|sq

η
η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

.

(15)

All derivations and proofs are given in Appendix A. The solution in Eq.(14) tells us that the
guide actor is obtained by weighting the current actor with pQps, aq.
If we set (cid:15) Ñ 0 then we
have ˜π « πθ and the actor is not updated. On the other hand, if we set (cid:15) Ñ 8 then we have
˜πpa|sq 9 expp

pQps, aq{ω‹q, which is a softmax policy where ω‹ is the temperature parameter.

3.2 LEARNING GUIDE ACTOR

Computing ˜πpa|sq and evaluating gpη, ωq are intractable for an arbitrary πθpa|sq. We overcome
this issue by imposing two assumptions. First, we assume that the actor is the Gaussian distribution:

πθpa|sq “ N pa|φθpsq, Σθpsqq,
where the mean φθpsq and covariance Σθpsq are functions parameterized by a policy parameter
θ. Second, we assume that Taylor’s approximation of pQps, aq is locally accurate up to the second-
order. More concretely, the second-order Taylor’s approximation using an arbitrary action a0 is

(16)

5

Published as a conference paper at ICLR 2018

given by

pQps, aq «

pQps, a0q ` pa ´ a0qJg0psq `

pa ´ a0qJH 0psqpa ´ a0q ` Op}a}3q,

(17)

1
2

pQps, aq|a“a0 are the gradient and Hessian of
where g0psq “ ∇a
the critic w.r.t. a evaluated at a0, respectively. By assuming that the higher order term Op}a}3q is
sufﬁciently small, we can rewrite Taylor’s approximation at a0 as

pQps, aq|a“a0 and H 0psq “ ∇2

a

pQ0ps, aq “

1
2

aJH 0psqa ` aJψ0psq ` ξ0psq,

(18)

pQps, a0q. Note that
where ψ0psq “ g0psq ´ H 0psqa0 and ξ0psq “ 1
H 0psq, ψ0psq, and ξ0psq depend on the value of a0 and do not depend on the value of a. This
dependency is explicitly denoted by the subscript. The choice of a0 will be discussed in Section 3.3.

0 H 0psqa0 ´ aJ

0 g0psq `

2 aJ

Substituting the Gaussian distribution and Taylor’s approximation into Eq.(14) yields another Gaus-
sian distribution ˜πpa|sq “ N pa|φ`psq, Σ`psqq, where the mean and covariance are given by

φ`psq “ F ´1psqLpsq, Σ`psq “ pη‹ ` ω‹qF ´1psq.

The matrix F psq P Rdaˆda and vector Lpsq P Rda are deﬁned as
θ psq ´ H 0psq, Lpsq “ η‹Σ´1

F psq “ η‹Σ´1

θ psqφθpsq ` ψ0psq.

The dual variables η‹ and ω‹ are obtained by minimizing the following dual function:

(19)

(20)

pgpη, ωq “ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

log

«

d

ﬀ

|2πpη ` ωqF ´1
|2πΣθpsq|

η
η`ω

η psq|

`

Epβ psq

LηpsqJF ´1

η psqLηpsq ´ ηφθpsqJΣ´1

` const,

(21)

‰
θ psqφθpsq

“

1
2

where F ηpsq and Lηpsq are deﬁned similarly to F psq and Lpsq but with η instead of η‹.

The practical advantage of using the Gaussian distribution and Taylor’s approximation is that the
guide actor can be obtained in a closed form and the dual function can be evaluated through matrix-
vector products. The expectation over pβpsq can be approximated by e.g., samples drawn from a
replay buffer (Mnih et al., 2015). We require inverting F ηpsq to evaluate the dual function. However,
these matrices are computationally cheap to invert when the dimension of actions is not large.

As shown in Eq.(19), the mean and covariance of the guide actor is computed using both the gradient
and Hessian of the critic. Yet, these computations do not resemble second-order updates discussed
previously in Section 2.2. Below, we show that for a particular choice of a0, the mean computation
corresponds to a second-order update that rotates gradients by a curvature matrix.

3.3 GUIDE ACTOR LEARNING AS SECOND-ORDER OPTIMIZATION

For now we assume that the critic is an accurate estimator of the true action-value function. In this
case, the quality of the guide actor depends on the accuracy of sample approximation in pgpη, ωq and
the accuracy of Taylor’s approximation. To obtain an accurate Taylor’s approximation of pQps, aq
using an action a0, the action a0 should be in the vicinity of a. However, we did not directly use
pQps, aqq (see Eq.(14)).
any individual a to compute the guide actor, but we weight πθpa|sq by expp
Thus, to obtain an accurate Taylor’s approximation of the critic, the action a0 needs to be similar to
actions sampled from πθpa|sq. Based on this observation, we propose two approaches to perform
Taylor’s approximation.

Taylor’s approximation around the mean. In this approach, we perform Taylor’s approximation
using the mean of πθpa|sq. More speciﬁcally, we use a0 “ Eπθ pa|sq ras “ φθpsq for Eq.(18). In
this case, we can show that the mean update in Eq.(19) corresponds to performing a second-order
update in the action space to maximize pQps, aq:

φ`psq “ φθpsq ` F ´1
φθ

psq∇a

pQps, aq|a“φθ psq,

(22)

6

Published as a conference paper at ICLR 2018

θ psq ´ H φθ psq and H φθ psq “ ∇2
a

pQps, aq|a“φθ psq. This equivalence can
where F φθ psq “ η‹Σ´1
be shown by substitution and the proof is given in Appendix A.2. This update equation reveals that
the guide actor maximizes the critic by taking a step in the action space similarly to the Newton
method. However, the main difference lies in the curvature matrix where the Newton method uses
Hessians H φθ psq but we use a damped Hessian F φθ psq. The damping term η‹Σ´1
θ psq corresponds
to the effect of the KL constraint and can be viewed as a trust-region that controls the step-size. This
damping term is particularly important since Taylor’s approximation is accurate only locally and we
should not take a large step in each update (Nocedal & Wright, 2006).

Expectation of Taylor’s approximations.
Instead of using Taylor’s approximation around the
mean, we may use an expectation of Taylor’s approximation over the distribution. More concretely,
we deﬁne rQps, aq to be an expectation of pQ0ps, aq over πθpa0|sq:

rQps, aq “

1
2

aJEπθ pa0|sq rH 0psqs a ` aJEπθ pa0|sq rψ0psqs ` Eπθ pa0|sq rξ0psqs .

(23)

pQps, aq|a“a0 s and the expectation is computed w.r.t.
Note that Eπθ pa0|sqrH 0psqs “ Eπθ pa0|sqr∇2
a
the distribution πθ of a0. We use this notation to avoid confusion even though πθpa0|sq and πθpa|sq
are the same distribution. When Eq.(23) is used, the mean update does not directly correspond
to any second-order optimization step. However, under an (unrealistic) independence assumption
Eπθ pa0|sqrH 0psqa0s “ Eπθ pa0|sqrH 0psqsEπθ pa0|sqra0s, we can show that the mean update corre-
sponds to the following second-order optimization step:

φ`psq “ φθpsq ` Eπθ pa0|sq rF 0psqs´1 Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

,

(24)

where Eπθ pa0|sq rF 0psqs “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs. Interestingly, the mean is updated
by rotating an expected gradient using an expected Hessians. In practice, the expectations can be
approximated using sampled actions ta0,iuS
i“1 „ πθpa|sq. We believe that this sampling can be
advantageous for avoiding local optima. Note that when the expectation is approximated by a single
sample a0 „ πθpa|sq, we obtain the update in Eq.(24) regardless of the independence assumption.
In the remainder, we use F psq to denote both of F φθ psq and Eπθ pa0|sq rF 0psqs, and use Hpsq to
denote both of H φθ psq and Eπθ pa0|sqrH 0psqs. In the experiments, we use GAC-0 to refer to GAC
with Taylor’s approximation around the mean, and we use GAC-1 to refer to GAC with Taylor’s
approximation by a single sample a0 „ πθpa|sq.

3.4 GAUSS-NEWTON APPROXIMATION OF HESSIAN

The covariance update in Eq.(19) indicates that F psq “ η‹Σ´1
θ psq ´ Hpsq needs to be positive
deﬁnite. The matrix F psq is guaranteed to be positive deﬁnite if the Hessian matrix Hpsq is negative
semi-deﬁnite. However, this is not guaranteed in practice unless pQps, aq is a concave function in
terms of a. To overcome this issue, we ﬁrstly consider the following identity:
pQps, aq∇a

pQps, aq “ ´∇a

pQps, aqq expp´

pQps, aqJ ` ∇2

pQps, aqq.

Hpsq “ ∇2
a

a expp

(25)

The proof is given in Appendix A.3. The ﬁrst term is always negative semi-deﬁnite while the second
term is indeﬁnite. Therefore, a negative semi-deﬁnite approximation of the Hessian can be obtained
as

H 0psq « ´

pQps, aq∇a

pQps, aqJ

”
∇a

ı

.

a“a0

(26)

pQps, aqq and it will be small for high values
The second term in Eq.(25) is proportional to expp´
of pQps, aq. This implies that the approximation should gets more accurate as the policy approach
a local maxima of pQps, aq. We call this approximation Gauss-Newton approximation since it is
similar to the Gauss-Newton approximation for the Newton method (Nocedal & Wright, 2006).

3.5 LEARNING PARAMETERIZED ACTOR

The second step of GAC is to learn a parameterized actor that well represents the guide actor. Below,
we discuss two supervised learning approaches for learning a parameterized actor.

7

Published as a conference paper at ICLR 2018

3.5.1 FULLY-PARAMETERIZED GAUSSIAN POLICY

Since the guide actor is a Gaussian distribution with a state-dependent mean and covariance, a
natural choice for the parameterized actor is again a parameterized Gaussian distribution with a
state-dependent mean and covariance: πθpa|sq “ N pa|φθpsq, Σθpsqq. The parameter θ can be
learned by minimizing the expected KL divergence to the guide actor:

LKLpθq “ Epβ psq rKL pπθpa|sq||˜πpa|sqqs

„



TrpF psqΣθpsqq
η‹ ` ω‹

“ Epβ psq
”
}φθpsq ´ φ`psq}2

´ log |Σθpsq|
ı

`

LWpθq
η‹ ` ω‹ ` const,

(27)

where LWpθq “ Epβ psq
which only depends on θ of the mean function. The const term does not depend on θ.

is the weighted-mean-squared-error (WMSE)

F psq

Minimizing the KL divergence reveals connections between GAC and deterministic policy gradients
(DPG) (Silver et al., 2014). By computing the gradient of the WMSE, it can be shown that

∇θLWpθq
2

“ ´Epβ psq

ı

pQps, aq|a“φθ psq

”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

psq∇a

psq∇a

”
∇θφθpsq∇a
ı

` Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

ı

.

` η‹Epβ psq

´ η‹Epβ psq

ı

pQps, aq|a“φ`psq

(28)

The proof is given in Appendix A.4. The negative of the ﬁrst term is precisely equivalent to DPG.
Thus, updating the mean parameter by minimizing the KL loss with gradient descent can be regarded
as updating the mean parameter with biased DPG where the bias terms depend on η‹. We can verify
pQps, aq|a“φ`psq “ 0 when η‹ “ 0 and this is the case of (cid:15) Ñ 8. Thus, all bias terms vanish
that ∇a
when the KL constraint is ignored and the mean update of GAC coincides with DPG. However,
unlike DPG which learns a deterministic policy, we can learn both the mean and covariance in GAC.

3.5.2 GAUSSIAN POLICY WITH PARAMETERIZED MEAN

While a state-dependent parameterized covariance function is ﬂexible, we observe that learning
performance is sensitive to the initial parameter of the covariance function. For practical pur-
poses, we propose using a parametrized Gaussian distribution with state-independent covariance:
πθpa|sq “ N pa|φθpsq, Σq. This class of policies subsumes deterministic policies with additive in-
dependent Gaussian noise for exploration. To learn θ, we minimize the mean-squared-error (MSE):
“

(29)
‰
For the covariance, we use the average of the guide covariances: Σ “ pη‹ ` ω‹qEpβ psq
.
For computational efﬁciency, we execute a single gradient update in each learning iteration instead
of optimizing this loss function until convergence.

F ´1psq

}φθpsq ´ φ`psq}2
2

LMpθq “

Epβ psq

1
2

‰

“

.

Similarly to the above analysis, the gradient of the MSE w.r.t. θ can be expanded and rewritten into

∇θLMpθq “ Epβ psq

”
∇θφθpsqH ´1psq

´

∇a

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

.

(30)

¯ı

Again, the mean update of GAC coincides with DPG when we minimize the MSE and set η‹ “ 0
and Hpsq “ ´I where I is the identity matrix. We can also substitute these values back into
Eq.(22). By doing so, we can interpret DPG as a method that performs ﬁrst-order optimization in
the action space:

φ`psq “ φθpsq ` ∇a

pQps, aq|a“φθ psq,

(31)

and then uses the gradient in Eq.(30) to update the policy parameter. This interpretation shows that
DPG is a ﬁrst-order method that only uses the ﬁrst-order information of the critic for actor learning.
Therefore in principle, GAC, which uses the second-order information of the critic, should learn
faster than DPG.

8

Published as a conference paper at ICLR 2018

3.6 POLICY EVALUATION FOR CRITIC

Beside actor learning, the performance of actor-critic methods also depends on the accuracy of the
critic. We assume that the critic pQν ps, aq is represented by neural networks with a parameter ν.
We adopt the approach proposed by Lillicrap et al. (2015) with some adjustment to learn ν. More
concretely, we use gradient descent to minimize the squared Bellman error with a slowly moving
target critic:

ν Ð ν ´ α∇ν Epβ psq,βpa|sq,pps1|s,aq

pQν ps, aq ´ y

(32)

„´



¯

2

,

pQ¯ν ps1, a1qs is computed by
where α ą 0 is the step-size. The target value y “ rps, aq ` γEπpa1|s1qr
the target critic pQ¯ν ps1, a1q whose parameter ¯ν is updated by ¯ν Ð τ ν ` p1 ´ τ q¯ν for 0 ă τ ă 1.
As suggested by Lillicrap et al. (2015), the target critic improves the learning stability and we set
τ “ 0.001 in experiments. The expectation for the squared error is approximated using mini-batch
samples tpsn, an, rn, s1
n“1 drawn from a replay buffer. The expectation over the current actor
πpa1|s1q is approximated using samples ta1
n. We do not use a
target actor to compute y since the KL upper-bound already constrains the actor update and a target
actor will further slow it down. Note that we are not restricted to this evaluation method and more
efﬁcient methods such as Retrace (Munos et al., 2016) can also be used.

m“1 „ πθpa1|s1

nq for each s1

n,muM

nquN

pQν ps, aq and its outer product for the Gauss-Newton approxi-
Our method requires computing ∇a
mation. The computational complexity of the outer product operation is Opd2
aq and is inexpensive
when compared to the dimension of ν. For a linear-in-parameter model pQν ps, aq “ νJµps, aq,
the gradient can be efﬁciently computed for common choices of the basis function µ such as the
Gaussian function. For deep neural network models, the gradient can be computed by the automatic-
differentiation (Goodfellow et al., 2016) where its cost depends on the network architecture.

4 RELATED WORK

Besides the connections to DPG, our method is also related to existing methods as follows.

A similar optimization problem to Eq.(13) was considered by the model-free trajectory optimization
(MOTO) method (Akrour et al., 2016). Our method can be viewed as a non-trivial extension of
MOTO with two signiﬁcant novelties. First, MOTO learns a sequence of time-dependent log-linear
Gaussian policies πtpa|sq “ N pa|Bts`bt, Σtq, while our method learns a log-nonlinear Gaussian
policy. Second, MOTO learns a time-dependent critic given by pQtps, aq “ 1
2 aJCta ` aJDts `
aJct `ξtpsq and performs policy update with these functions. In contrast, our method learns a more
complex critic and performs Taylor’s approximation in each training step.

Besides MOTO, the optimization problem also resembles that of trust region policy optimization
(TRPO) (Schulman et al., 2015a). TRPO solves the following optimization problem:

max
θ1

Epπθ psq,πθ1 pa|sq

”
ı
pQps, aq

subject to Epπθ psq rKLpπθpa|sq||πθ1pa|sqqs ď (cid:15),

(33)

where pQps, aq may be replaced by an estimate of the advantage function (Schulman et al., 2015b).
There are two major differences between the two problems. First, TRPO optimizes the policy param-
eter while we optimize the guide actor. Second, TRPO solves the optimization problem by conjugate
gradient where the KL divergence is approximated by the Fisher information matrix, while we solve
the optimization problem in a closed form with a quadratic approximation of the critic.

Our method is also related to maximum-entropy RL (Ziebart et al., 2010; Azar et al., 2012; Haarnoja
et al., 2017; Nachum et al., 2017), which maximizes the expected cumulative reward with an addi-
Epπpsq rrpst, atq ` αHpπpat|stqqs, where α ą 0 is a trade-off parame-
tional entropy bonus:
ter. The optimal policy in maximum-entropy RL is the softmax policy given by

8
t“1

ř

π‹
MaxEntpa|sq “ exp

ˆ

Q‹

softps, aq ´ V ‹
α

softpsq

˙

ˆ

9 exp

˙

,

Q‹

softps, aq
α

(34)

9

Published as a conference paper at ICLR 2018

softps, aq and V ‹

where Q‹
tively (Haarnoja et al., 2017; Nachum et al., 2017). For a policy π, these are deﬁned as
softps, aq “ rps, aq ` γEpps1|s,aq

softpsq are the optimal soft action-value and state-value functions, respec-
“

(35)

Qπ

,

ż

V π
softpsq “ α log

exp

ˆ

Qπ

‰
V π
softps1q
˙
softps, aq
α

da.

(36)

The softmax policy and the soft state-value function in maximum-entropy RL closely resemble the
guide actor in Eq.(14) when η‹ “ 0 and the log-integral term in Eq.(15) when η “ 0, respectively,
except for the deﬁnition of action-value functions. To learn the optimal policy of maximum-entropy
RL, Haarnoja et al. (2017) proposed soft Q-learning which uses importance sampling to compute
the soft value functions and approximates the intractable policy using a separate policy function.
Our method largely differs from soft Q-learning since we use Taylor’s approximation to convert the
intractable integral into more convenient matrix-vector products.

The idea of ﬁrstly learning a non-parameterized policy and then later learning a parameterized policy
by supervised learning was considered previously in guided policy search (GPS) (Levine & Koltun,
2013). However, GPS learns the guide policy by trajectory optimization methods such as an iterative
linear-quadratic Gaussian regulator (Li & Todorov, 2004), which requires a model of the transition
function. In contrast, we learn the guide policy via the critic without learning the transition function.

5 EXPERIMENTAL RESULTS

We evaluate GAC on the OpenAI gym platform (Brockman et al., 2016) with the Mujoco Physics
simulator (Todorov et al., 2012). The actor and critic are neural networks with two hidden layers of
400 and 300 units, as described in Appendix C. We compare GAC-0 and GAC-1 against deep DPG
(DDPG) (Lillicrap et al., 2015), Q-learning with a normalized advantage function (Q-NAF) (Gu
et al., 2016), and TRPO (Schulman et al., 2015a;b). Figure 1 shows the learning performance on
9 continuous control tasks. Overall, both GAC-0 and GAC-1 perform comparably with existing
methods and they clearly outperform the other methods in Half-Cheetah.

The performance of GAC-0 and GAC-1 is comparable on these tasks, except on Humanoid where
GAC-1 learns faster. We expect GAC-0 to be more stable and reliable but easier to get stuck at
local optima. On the other hand, the randomness introduced by GAC-1 leads to high variance
approximation but this could help escape poor local optima. We conjecture GAC-S that uses S ą 1
samples for the averaged Taylor’s approximation should outperform both GAC-0 and GAC-1. While
this is computationally expensive, we can use parallel computation to reduce the computation time.

The expected returns of both GAC-0 and GAC-1 have high ﬂuctuations on the Hopper and Walker2D
tasks when compared to TRPO as can be seen in Figure 1g and Figure 1h. We observe that they can
learn good policies for these tasks in the middle of learning. However, the policies quickly diverge
to poor ones and then they are quickly improved to be good policies again. We believe that this
happens because the step-size F ´1psq “
of the guide actor in Eq. (22) can
be very large near local optima for Gauss-Newton approximation. That is, the gradients near local
pQps, aqJ
optima have small magnitude and this makes the approximation Hpsq “ ∇a
small as well. If η‹Σ´1 is also relatively small then the matrix F ´1psq can be very large. Thus,
under these conditions, GAC may use too large step sizes to compute the guide actor and this results
in high ﬂuctuations in performance. We expect that this scenario can be avoided by reducing the KL
bound (cid:15) or adding a regularization constant to the Gauss-Newton approximation.

˘
η‹Σ´1 ´ Hpsq

pQps, aq∇a

´1

`

Table 1 in Appendix C shows the wall-clock computation time. DDPG is computationally the most
efﬁcient method on all tasks. GAC has low computation costs on tasks with low dimensional actions
and its cost increases as the dimensionality of action increases. This high computation cost is due to
the dual optimization for ﬁnding the step-size parameters η and ω. We believe that the computation
cost of GAC can be signiﬁcantly reduced by letting η and ω be external tuning parameters.

6 CONCLUSION AND FUTURE WORK

Actor-critic methods are appealing for real-world problems due to their good data efﬁciency and
learning speed. However, existing actor-critic methods do not use second-order information of the

10

Published as a conference paper at ICLR 2018

(a) Inverted-Pend.

(b) Inv-Double-Pend.

(c) Reacher

(d) Swimmer

(e) Half-Cheetah

(f) Ant

(g) Hopper

(h) Walker2D

(i) Humanoid

Figure 1: Expected returns averaged over 10 trials. The x-axis indicates training time steps. The y-
axis indicates averaged return and higher is better. More clear ﬁgures are provided in Appendix C.2.

critic. In this paper, we established a novel framework that distinguishes itself from existing work
by utilizing Hessians of the critic for actor learning. Within this framework, we proposed a prac-
tical method that uses Gauss-Newton approximation instead of the Hessians. We showed through
experiments that our method is promising and thus the framework should be further investigated.

Our analysis showed that the proposed method is closely related to deterministic policy gradients
(DPG). However, DPG was also shown to be a limiting case of the stochastic policy gradients when
the policy variance approaches zero (Silver et al., 2014). It is currently unknown whether our frame-
work has a connection to the stochastic policy gradients as well, and ﬁnding such a connection is
our future work.

Our main goal in this paper was to provide a new actor-critic framework and we do not claim that our
method achieves the state-of-the-art performance. However, its performance can still be improved in
many directions. For instance, we may impose a KL constraint for a parameterized actor to improve
its stability, similarly to TRPO (Schulman et al., 2015a). We can also apply more efﬁcient policy
evaluation methods such as Retrace (Munos et al., 2016) to achieve better critic learning.

ACKNOWLEDGMENTS

MS was partially supported by KAKENHI 17H00757.

REFERENCES

Abbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, Nuno Lau, Lu´ıs Paulo Reis, and Gerhard Neu-
In Advances in Neural Information

mann. Model-Based Relative Entropy Stochastic Search.
Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki. Model-Free Trajec-
tory Optimization for Reinforcement Learning. In Proceedings of the 33nd International Confer-
ence on Machine Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Shun-ichi Amari. Natural Gradient Works Efﬁciently in Learning. Neural Computation, 10(2):

251–276, 1998.

Brandon Amos, Lei Xu, and J. Zico Kolter. Input Convex Neural Networks. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
August 6-11, 2017, Sydney, Australia, 2017.

Mohammad Gheshlaghi Azar, Vicenc¸ G´omez, and Hilbert J. Kappen. Dynamic Policy Program-

ming. Journal of Machine Learning Research, 13:3207–3245, 2012.

11

Published as a conference paper at ICLR 2018

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.

Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on Policy Search for Robotics.

Foundations and Trends in Robotics, 2(1-2):1–142, 2013.

Thomas Furmston, Guy Lever, and David Barber. Approximate Newton Methods for Policy Search
in Markov Decision Processes. Journal of Machine Learning Research, 17(227):1–51, 2016.

Xavier Glorot and Yoshua Bengio. Understanding the Difﬁculty of Training Deep Feedforward
Neural Networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the 13th In-
ternational Conference on Artiﬁcial Intelligence and Statistics, May 13-15, 2010, Sardinia, Italy,
2010.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous Deep Q-Learning
with Model-based Acceleration. In Proceedings of the 33nd International Conference on Machine
Learning, June 19-24, 2016, New York City, NY, USA, 2016.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with
Deep Energy-Based Policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, August 6-11, 2017, Sydney, Australia, 2017.

Nicolas Heess, Gregory Wayne, David Silver, Timothy P. Lillicrap, Tom Erez, and Yuval Tassa.
In Advances in Neural

Learning Continuous Control Policies by Stochastic Value Gradients.
Information Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.

Sham Kakade. A Natural Policy Gradient. In Advances in Neural Information Processing Systems

14, December 3-8, 2001, Vancouver, British Columbia, Canada, 2001.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. CoRR,

abs/1412.6980, 2014.

Vijay R. Konda and John N. Tsitsiklis. On Actor-Critic Algorithms. SIAM Journal on Control and

Optimization, 42(4):1143–1166, April 2003.

Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the 30th International

Conference on Machine Learning, June 16-21, 2013, Atlanta, GA, USA, 2013.

Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep
Visuomotor Policies. Journal of Machine Learning Research, 17(1):1334–1373, January 2016.
ISSN 1532-4435.

Weiwei Li and Emanuel Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear Bio-
logical Movement Systems. In Proceedings of the 1st International Conference on Informatics in
Control, Automation and Robotics, August 25-28, 2004, Set´ubal, Portugal, pp. 222–229, 2004.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. CoRR,
abs/1509.02971, 2015.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement
Learning. Nature, 518(7540):529–533, February 2015. ISSN 00280836.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement
In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
Learning.
International Conference on Machine Learning, June 19-24, 2016, New York City, NY, USA,
2016.

12

Published as a conference paper at ICLR 2018

R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and Efﬁcient Off-
Policy Reinforcement Learning. In Advances in Neural Information Processing Systems 29, De-
cember 5-10, 2016, Barcelona, Spain, 2016.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the Gap Between
Value and Policy Based Reinforcement Learning. In Advances in Neural Information Processing
Systems 30, 4-9 December 2017, Long Beach, CA, USA, 2017.

Jorge Nocedal and Stephen J. Wright. Numerical Optimization, Second Edition. World Scientiﬁc,

2006.

Jan Peters and Stefan Schaal. Natural Actor-Critic. Neurocomputing, 71(7-9):1180–1190, 2008.

Jan Peters, Katharina M¨ulling, and Yasemin Altun. Relative Entropy Policy Search. In Proceedings
of the 24th AAAI Conference on Artiﬁcial Intelligence, July 11-15, 2010, Atlanta, Georgia, USA,
2010.

John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region
Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning,
July 6-11, 2015, Lille, France, pp. 1889–1897, 2015a.

John Schulman, Philipp Moritz, Sergey Levine, Michael

High-Dimensional Continuous Control Using Generalized Advantage Estimation.
abs/1506.02438, 2015b.

I. Jordan, and Pieter Abbeel.
CoRR,

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic Policy Gradient Algorithms. In Proceedings of the 31st International Conference
on Machine Learning, June 21-26, 2014, Beijing, China, 2014.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game
of Go with Deep Neural Networks and Tree Search. Nature, 529(7587):484–489, 2016.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning - an Introduction. Adaptive

computation and machine learning. MIT Press, 1998.

Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy Gradi-
ent Methods for Reinforcement Learning with Function Approximation. In Advances in Neural
Information Processing Systems 12, November 29 - December 4, 1999, Colorado, USA, 1999.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics Engine for Model-Based Control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012,
Vilamoura, Algarve, Portugal, 2012.

Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforce-

ment Learning. Machine Learning, 8(3):229–256, 1992. doi: 10.1007/BF00992696.

Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable Trust-Region
Method for Deep Reinforcement Learning using Kronecker-factored Approximation. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, 4-9 December 2017, Long Beach, CA, USA,
2017.

Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling Interaction via the Principle
of Maximum Causal Entropy. In Proceedings of the 27th International Conference on Machine
Learning, June 21-24, 2010, Haifa, Israel, 2010.

13

Published as a conference paper at ICLR 2018

A DERIVATIONS AND PROOFS

A.1 DERIVATION OF SOLUTION AND DUAL FUNCTION OF GUIDE ACTOR

The solution of the optimization problem:

max
˜π

Epβ psq,˜πpa|sq

ı

”
pQps, aq

,

subject to Epβ psq rKLp˜πpa|sq||πθpa|sqqs ď (cid:15),
Epβ psq rHp˜πpa|sqqs ě κ,
Epβ psqrπpa|sq “ 1,

can be obtained by the method of Lagrange multipliers. The derivation here follows the derivation of
similar optimization problems by Peters et al. (2010) and Abdolmaleki et al. (2015). The Lagrangian
of this optimization problem is

Lprπ, η, ω, νq “ Epβ psq,˜πpa|sq

` ηp(cid:15) ´ Epβ psq rKLp˜πpa|sq||πθpa|sqqsq

ı
”
pQps, aq

` ωpEpβ psq rHp˜πpa|sqqs ´ κq ` νpEpβ psqrπpa|sq ´ 1q,

(38)

where η, ω, and ν are the dual variables. Then, by taking derivative of L w.r.t. rπ we obtain

„ż ´

¯



BrπL “ Epβ psq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq

da

´ pη ` ω ´ νq.

(39)

We set this derivation to zero in order to obtain

0 “ Epβ psq

„ż ´

¯
pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq



da

´ pη ` ω ´ νq

pQps, aq ´ pη ` ωq log rπpa|sq ` η log πθpa|sq ´ pη ` ω ´ νq.

“

Then the solution is given by

rπpa|sq “ πθpa|sq

η
η`ω exp

9 πθpa|sq

η
η`ω exp

¸

ˆ

exp

´

˙

η ` ω ´ ν
η ` ω

˜

˜

pQps, aq
η ` ω
pQps, aq
η ` ω

¸

.

(37)

(40)

(41)

(42)

To obtain the dual function gpη, ωq, we substitute the solution to the constraint terms of the La-
grangian and this gives us

Lpη, ω, νq “ Epβ psq,rπpa|sq

ı

”
pQps, aq

«

´ pη ` ωqEpβ psq,rπpa|sq

pQps, aq
η ` ω

`
`

η
log πθpa|sq ´
η ` ω
˘
Epβ ,rπpa|sq ´ 1

ﬀ

η ` ω ´ ν
η ` ω

` ηEpβ psq,rπpa|sq rlog πθpa|sqs ` ν

` η(cid:15) ´ ωκ.

(43)

After some calculation, we obtain

Lpη, ω, νq “ η(cid:15) ´ ωκ ` Epβ psq rη ` ω ´ νs
«ż

“ η(cid:15) ´ ωκ ` pη ` ωqEpβ psq

η

πθpa|sq

η`ω exp

˜

¸

ﬀ

pQps, aq
η ` ω

da

“ gpη, ωq,

(44)

where in the second line we use the fact that expp´ η`ω´ν

η`ω q is the normalization term of rπpa|sq.

14

Published as a conference paper at ICLR 2018

A.2 PROOF OF SECOND-ORDER OPTIMIZATION IN ACTION SPACE

Firstly, we show that GAC performs second-order optimization in the action space when Taylor’s
approximation is performed with a0 “ Eπpa|sq ras “ φθpsq. Recall that Taylor’s approximation
with φθ is given by

1
2

pQps, aq “
pQps, aq|a“φθ psq ´ H φθ psqφθpsq. By substituting ψφθ

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq,

(45)

psq into Lpsq “

where ψφθ
η‹Σ´1

psq “ ∇a

θ psqφθpsq ´ ψθpsq, we obtain

Lpsq “ η‹Σ´1
“ pη‹Σ´1

θ psqφθpsq ` ∇a
θ psq ´ H φθ psqqφθpsq ` ∇a
pQps, aq|a“φθ psq.

pQps, aq|a“φθ psq ´ H φθ psqφθpsq

pQps, aq|a“φθ psq

“ F psqφθpsq ` ∇a
Therefore, the mean update is equivalent to

φ`psq “ F ´1psqLpsq

“ φθpsq ` F ´1psq∇a

pQps, aq|a“φθ psq,

which is a second-order optimization step with a curvature matrix F psq “ η‹Σ´1

θ psq ´ H φθ psq.

Lpsq “ η‹Σ´1

Similarly, for the case where a set of samples ta0u „ πθpa0|sq “ N pa0|φθpsq, Σpsqq is used to
compute the averaged Taylor’s approximation, we obtain
θ psqφθpsq ` Eπθ pa0|sq

”
∇a
Then, by assuming that Eπθ pa0|sq rH 0psqa0psqs “ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s, we obtain
”
∇a
”
∇a

pQps, aq|a“a0
pQps, aq|a“a0

´ Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s

´ Eπθ pa0|sq rH 0psqa0psqs .

θ psqφθpsq ` Eπθ pa0|sq

θ psqφθpsq ` Eπθ pa0|sq

pQps, aq|a“a0

Lpsq “ η‹Σ´1

“ η‹Σ´1

(48)

ı

ı

ı

´ Eπθ pa0|sq rH 0s φθpsq
ı

”
∇a

pQps, aq|a“a0

“ pη‹Σ´1

θ psq ´ Eπθ pa0|sq rH 0psqsqφθpsq ` Eπθ pa0|sq

“ F psqφθpsq ` Eπθ pa0|sq

”
∇a

ı

pQps, aq|a“a0

.

Therefore, we have a second-order optimization step

φ`psq “ φθpsq ` F ´1psqEπθ pa0|sq

(50)
where F ´1psq “ η‹Σ´1
θ psq ´ Eπθ pa0|sq rH 0psqs is a curvature matrix. As described in
the main paper, this interpretation is only valid when the equality Eπθ pa0|sq rH 0psqa0psqs “
Eπθ pa0|sq rH 0s Eπθ pa0|sq ra0s holds. While this equality does not hold in general, it holds when
only one sample a0 „ πθpa0|sq is used. Nonetheless, we can still use the expectation of Taylor’s
approximation to perform policy update regardless of this assumption.

,

pQps, aq|a“a0

”
∇a

ı

(46)

(47)

(49)

A.3 PROOF OF GAUSS-NEWTON APPROXIMATION

Let f ps, aq “ expp

pQps, aqq, then the Hessian Hpsq “ ∇2

pQps, aq can be expressed as

a

Hpsq “ ∇a r∇a log f ps, aqs
“
∇af ps, aqf ps, aq´1
∇af ps, aq´1

“ ∇a

`

‰

˘

J

“ ∇af ps, aq
“ ∇af ps, aq∇f f ps, aq´1 p∇af ps, aqqJ ` ∇2
“ ´∇af ps, aqf ps, aq´2 p∇af ps, aqqJ ` ∇2

` ∇2

af ps, aqf ps, aq´1

af ps, aqf ps, aq´1
af ps, aqf ps, aq´1

`

˘ `

∇af ps, aqf ps, aq´1

“ ´
“ ´∇a log f ps, aq∇a log f ps, aqJ ` ∇2
a expp
“ ´∇a

pQps, aqJ ` ∇2

pQps, aq∇a

` ∇2
af ps, aqf ps, aq´1
pQps, aqq expp´

∇af ps, aqf ps, aq´1

pQps, aqq,

˘

J

af ps, aqf ps, aq´1

(51)

15

Published as a conference paper at ICLR 2018

which concludes the proof.

Beside Gauss-Newton approximation, an alternative approach is to impose a special structure on
pQps, aq so that Hessians are always negative semi-deﬁnite. In literature, there exists two special
structures that satisﬁes this requirement.

Normalized advantage function (NAF) (Gu et al., 2016): NAF represents the critic by a quadratic
function with a negative curvature:

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(52)

where a negative-deﬁnite matrix-valued function W psq, a vector-valued function bpsq and a scalar-
valued function V psq are parameterized functions whose their parameters are learned by policy
evaluation methods such as Q-learning (Sutton & Barto, 1998). With NAF, negative deﬁnite Hes-
sians can be simply obtained as Hpsq “ W psq. However, a signiﬁcant disadvantage of NAF is that
it assumes the action-value function is quadratic regardless of states and this is generally not true for
most reward functions. Moreover, the Hessians become action-independent even though the critic is
a function of actions.

Input convex neural networks (ICNNs) (Amos et al., 2017): ICNNs are neural networks with
special structures which make them convex w.r.t. their inputs. Since Hessians of concave functions
are always negative semi-deﬁnite, we may use ICNNs to represent a negative critic and directly use
its Hessians. However, similarly to NAF, ICNNs implicitly assume that the action-value function is
concave w.r.t. actions regardless of states and this is generally not true for most reward functions.

A.4 GRADIENT OF THE LOSS FUNCTIONS FOR LEARNING PARAMETERIZED ACTOR

We ﬁrst consider
is
N pa|φ`psq, Σ`psqq and the current actor is N pa|φθpsq, Σθpsqq. Taylor’s approximation of
pQps, aq at a0 “ φθpsq is

loss function where the guide actor

the weight mean-squared-error

pQps, aq “

1
2

aJH φθ psqa ` aJψφθ

psq ` ξφθ psq.

(53)

By assuming that H φθ psq is strictly negative deﬁnite5, we can take a derivative of this approxima-
tion w.r.t. a and set it to zero to obtain a “ H ´1
psq. Replacing a by
φθ
φθpsq and φ`psq yields

pQps, aq ´ H ´1
φθ

psqψφθ

psq∇a

φθpsq “ H ´1
φθ
φ`psq “ H ´1
φθ

psq∇a

psq∇a

pQps, aq|a“φθ psq ´ H ´1
pQps, aq|a“φ`psq ´ H ´1

φθ

φθ

psqψφθ
psqψφθ

psq,

psq.

Recall that the weight mean-squared-error is deﬁned as

LWpθq “ Epβ psq

”
}φθpsq ´ φ`psq}2

F psq

ı

.

(54)

(55)

(56)

5This can be done by subtracting a small positive value to the diagonal entries of Gauss-Newton approxi-

mation.

16

Published as a conference paper at ICLR 2018

Then, we consider its gradient w.r.t. θ as follows:

∇θLWpθq “ 2Epβ psq
“ 2Epβ psq
“ 2Epβ psq

“

“

“

`

∇θφθpsqF psq
∇θφθpsqpη‹Σ´1
∇θφθpsqpη‹Σ´1
´
H ´1
φθ

ˆ

˘‰

`

φθpsq ´ φ`psq
θ psq ´ H φθ psqq
θ psq ´ H φθ psqq

˘‰

φθpsq ´ φ`psq

¯ı

psq∇a

pQps, aq|a“φθ psq ´ H ´1

φθ

psq∇a

pQps, aq|a“φ`psq

“ 2η‹Epβ psq

´

psq

”
θ psqH ´1
∇θφθpsqΣ´1
φθ
´
”
∇θφθpsq

` 2Epβ psq
”
∇θφθpsq∇a
”
∇θφθpsqΣ´1psqH ´1
φθ
”
∇θφθpsqΣ´1psqH ´1
φθ

∇a
pQps, aq|a“φθ psq

pQps, aq|a“φθ psq ´ ∇a
pQps, aq|a“φθ psq
”
∇θφθpsq∇a
ı

∇a
pQps, aq|a“φ`psq ´ ∇a
ı
` 2Epβ psq
pQps, aq|a“φθ psq
pQps, aq|a“φ`psq

psq∇a

psq∇a

ı

.

` 2η‹Epβ psq

´ 2η‹Epβ psq

“ ´2Epβ psq

pQps, aq|a“φ`psq
¯ı

pQps, aq|a“φ`psq

¯ı

ı

(57)

This concludes the proof for the gradient in Eq.(28). Note that we should not directly replace
the mean functions in the weight mean-square-error by Eq.(54) and Eq.(55) before expanding the
gradient. This is because the analysis would require computing the gradient w.r.t. θ inside the
Hessians and this is not trivial. Moreover, when we perform gradient descent in practice, the mean
φ`psq is considered as a constant w.r.t. θ similarly to an output function in supervised learning.
The gradient for the mean-squared-error can be derived similarly. Let the mean-squared-error be
deﬁned as

LMpθq “

Epβ psq

}φθpsq ´ φ`psq}2
2

.

“

‰

1
2

(58)

Its gradient w.r.t. θ is given by

“

`

˘‰

∇θLMpθq “ Epβ psq
“ Epβ psq

∇θφθpsq
”
∇θφθpsqH ´1psq

φθpsq ´ φ`psq
´

∇a

which concludes the proof for the gradient in Eq.(30).

pQps, aq|a“φθ psq ´ ∇a

pQps, aq|a“φ`psq

,

(59)

¯ı

To show that ∇a
˘
`
ηΣ´1psq ´ H φθ psq

´1

´
η‹Σ´1psqφθpsq ` ψφθ

pQps, aq|a“φ`psq “ 0 when η‹ “ 0, we directly substitute η‹ “ 0 into φ`psq “
¯

psqq

and this yield

φ`psq “ ´H ´1
φθ
pQps, aq|a“φ`psq ´ H ´1
Since φ`psq “ H ´1
psq∇a
psq from Eq.(55) and the Hessians are
φθ
pQps, aq|a“φ`psq “ 0. This is intuitive since without the KL constraint,
non-zero, it has to be that ∇a
the mean of the guide actor always be at the optima of the second-order Taylor’s approximation and
the gradients are zero at the optima.

psqψφθ

psqψφθ

psq.

(60)

φθ

A.5 RELATION TO Q-LEARNING WITH NORMALIZED ADVANTAGE FUNCTION

The normalized advantage function (NAF) (Gu et al., 2016) is deﬁned as

pQNAFps, aq “

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq,

(61)

where W psq is a negative deﬁnite matrix. Gu et al. (2016) proposed to perform Q-learning with
NAF by using the fact that argmaxa
Here, we show that GAC includes the Q-learning method by Gu et al. (2016) as its special case. This
can be shown by using NAF as a critic instead of performing Taylor’s approximation of the critic.

pQNAFps, aq “ bpsq and maxa

pQNAFps, aq “ V psq.

17

Published as a conference paper at ICLR 2018

pQNAFps, aq “

Firstly, we expand the quadratic term of NAF as follows:
1
2
1
2
1
2

aJW psqa ´ aJW psqbpsq `

aJW psqa ` aJψpsq ` ξpsq,

“

“

1
2

pa ´ bpsqqJW psqpa ´ bpsqq ` V psq

bpsqJW psqbpsq ` V psq

(62)

where ψpsq “ ´W psqbpsq and ξpsq “ 1
2 bpsqJW psqbpsq ` V psq. By substituting the quadratic
model obtained by NAF into the GAC framework, the guide actor is now given by ˜πpa|sq “
N pa|φ`psq, Σ`psqqq with

φ`psq “ pη‹Σ´1psqq ´ W psqq´1pη‹Σ´1psqφθpsq ´ W psqbpsqq
Σ`psq “ pη‹ ` ω‹qpη‹Σ´1psqq ´ W psqq.

(64)
To obtain Q-learning with NAF, we set η‹ “ 0, i.e., we perform a greedy maximization where the
KL upper-bound approaches inﬁnity, and this yields

(63)

φ`psq “ ´W psq´1p´W psqbpsqq

(65)
which is the policy obtained by performing Q-learning with NAF. Thus, NAF with Q-learning is a
special case of GAC if Q-learning is also used in GAC to learn the critic.

“ bpsq,

The pseudo-code of GAC is given in Algorithm 1. The source code is available at https://
github.com/voot-t/guide-actor-critic.

B PSEUDO-CODE OF GAC

C EXPERIMENT DETAILS

C.1

IMPLEMENTATION

We try to follow the network architecture proposed by the authors of each baseline method as close as
possible. For GAC and DDPG, we use neural networks with two hidden layers for the actor network
and the critic network. For both networks the ﬁrst layer has 400 hidden units and the second layer
has 300 units. For NAF, we use neural networks with two hidden layers to represent each of the
functions bpsq, W psq and V psq where each layer has 200 hidden units. All hidden units use the
relu activation function except for the output of the actor network where we use the tanh activation
function to bound actions. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate
0.001 and 0.0001 for the critic network and the actor network, respectively. The moving average
step for target networks is set to τ “ 0.001. The maximum size of the replay buffer is set to
1000000. The mini-batches size is set to N “ 256. The weights of the actor and critic networks
are initialized as described by Glorot & Bengio (2010), except for the output layers where the initial
weights are drawn uniformly from Up´0.003, 0.003q, as described by Lillicrap et al. (2015). The
initial covariance Σ in GAC is set to be an identity matrix. DDPG and QNAF use the OU-process
with noise parameters θ “ 0.15 and σ “ 0.2 for exploration .

For TRPO, we use the implementation publicly available at https://github.com/openai/
baselines. We also use the provided network architecture and hyper-parameters except the batch
size where we use 1000 instead of 1024 since this is more suitable in our test setup.

For GAC, the KL upper-bound is ﬁxed to (cid:15) “ 0.0001. The entropy lower-bound κ is adjusted
heuristically by

κ “ maxp0.99pE ´ E0q ` E0, E0q,
(71)
where E « Epβ psq rHpπθpa|sqqs denotes the expected entropy of the current policy and E0 denotes
the entropy of a base policy N pa|0, 0.01Iq. This heuristic ensures that the lower-bound gradually
decreases but the lower-bound cannot be too small. We apply this heuristic update once every 5000
training steps. The dual function is minimize by the sequential least-squares quadratic programming
(SLSQP) method with an initial values η “ 0.05 and ω “ 0.05. The number of samples for
computing the target critic value is M “ 10.

18

Published as a conference paper at ICLR 2018

Algorithm 1 Guide actor critic

1: Input:

Initial actor πθpa|sq “ N pa|φθpsq, Σq, critic pQν ps, aq, target critic network

pQ¯ν ps, aq, KL bound (cid:15), entropy bound κ, learning rates α1, α2, and data buffer D “ H.

procedure COLLECT TRANSITION SAMPLE

Observe state st and sample action at „ N pa|φθpstq, Σq.
Execute at, receive reward rt and next state s1
t.
Add transition tpst, at, rt, s1

tqu to D.

2: for t “ 1, . . . , Tmax do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

end procedure
procedure LEARN

Sample N mini-batch samples tpsn, an, rn, s1
procedure UPDATE CRITIC
n,muM
Sample actions ta1
Compute yn, update ν by, e.g., Adam, and update ¯ν by moving average:

m“1 „ N pa|φθpsnq, Σq for each sn.

n“1 uniformly from D.

nquN

yn “ rn ` γ

pQ¯ν ps1

n, a1

n,mq,

ν Ð ν ´ α1

´
pQν psn, anq ´ yn

¯

2

,

∇ν

1
M

1
N

Mÿ

m“1
Nÿ

n“1

¯ν Ð τ ν ` p1 ´ τ q¯ν.

end procedure
procedure LEARN GUIDE ACTOR

Compute an,0 for each sn by an,0 “ φθpsnq or an,0 „ N pa|φθpsnq, Σq.
pQpsn, aq|a“an,0 and H 0pssq “ ´g0psnqg0psnqJ.
Compute g0psq “ ∇a
Solve for pη‹, ω‹q “ argminηą0,ωą0
Compute the guide actor rπpa|snq “ N pa|φ`psnq, Σ`psnqq for each sn.

pgpη, ωq by a non-linear optimization method.

end procedure
procedure UPDATE PARAMETERIZED ACTOR

Update policy parameter by, e.g., Adam, to minimize the MSE:

13:
14:
15:

16:
17:
18:
19:
20:
21:

θ Ð θ ´ α2

∇θ}φθpsnq ´ φ`psnq}2
2.

1
N

Nÿ

n“1

22:

Update policy covariance by averaging the guide covariances:

Σ Ð

Σ`psnq.

1
N

(66)

(67)

(68)

(69)

(70)

end procedure

end procedure

23:
24:
25: end for
26: Output: Learned actor πθpa|sq.

C.2 ENVIRONMENTS AND RESULTS

We perform experiments on the OpenAI gym platform (Brockman et al., 2016) with Mujoco Physics
simulator (Todorov et al., 2012) where all environments are v1. We use the state space, action space
and the reward function as provided and did not perform any normalization or gradient clipping.
The maximum time horizon in each episode is set to 1000. The discount factor γ “ 0.99 is only
used for learning and the test returns are computed without it.

Experiments are repeated for 10 times with different random seeds. The total computation time are
reported in Table 1. The ﬁgures below show the results averaged over 10 trials. The y-axis indicates
the averaged test returns where the test returns in each trial are computed once every 5000 training
time steps by executing 10 test episodes without exploration. The error bar indicates standard error.

19

Published as a conference paper at ICLR 2018

Table 1: The total computation time for training the policy for 1 million steps (0.1 million steps for
the Invert-Pendulum task). The mean and standard error are computed over 10 trials with the unit in
hours. TRPO is not included since it performs a lesser amount of update using batch data samples.

Task
Inv-Pend.
Inv-Double-Pend.
Reacher
Swimmer
Half-Cheetah
Ant
Hopper
Walker2D
Humanoid

GAC-1
1.13(0.09)
15.56(0.93)
17.23(0.87)
12.43(0.74)
29.82(1.76)
37.84(1.80)
18.99(1.06)
33.42(0.96)
111.07(2.99)

GAC-0
0.80(0.04)
15.67(0.77)
12.64(0.38)
11.94(0.74)
32.64(1.41)
40.57(3.06)
14.22(0.56)
31.71(1.84)
106.80(6.80)

DDPG
0.45(0.02)
9.04(0.29)
10.67(0.37)
9.61(0.52)
10.13(0.41)
9.75(0.37)
8.74(0.45)
7.92(0.11)
13.90(1.60)

QNAF
0.40(0.03)
7.47(0.22)
30.91(2.09)
32.44(2.21)
27.94(2.37)
27.09(0.94)
26.48(1.42)
26.94(1.99)
30.43(2.21)

Figure 2: Performance averaged over 10 trials on the Inverted Pendulum task.

20

Published as a conference paper at ICLR 2018

Figure 3: Performance averaged over 10 trials on the Inverted Double Pendulum task.

Figure 4: Performance averaged over 10 trials on the Reacher task.

21

Published as a conference paper at ICLR 2018

Figure 5: Performance averaged over 10 trials on the Swimmer task.

Figure 6: Performance averaged over 10 trials on the Half-Cheetah task.

22

Published as a conference paper at ICLR 2018

Figure 7: Performance averaged over 10 trials on the Ant task.

Figure 8: Performance averaged over 10 trials on the Hopper task.

23

Published as a conference paper at ICLR 2018

Figure 9: Performance averaged over 10 trials on the Walker2D task.

Figure 10: Performance averaged over 10 trials on the Humanoid task.

24

