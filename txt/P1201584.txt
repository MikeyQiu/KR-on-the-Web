Document-level Multi-aspect Sentiment Classiﬁcation by Jointly
Modeling Users, Aspects, and Overall Ratings

Junjie Li1,2, Haitong Yang3 and Chengqing Zong1,2,4
1 National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China
2 University of Chinese Academy of Sciences, Beijing, China
3 School of Computer, Central China Normal University, Wuhan 430079, China
4 CAS Center for Excellence in Brain Science and Intelligence Technology
{junjie.li, cqzong}@nlpr.ia.ac.cn, htyang@mail.ccnu.edu.cn

Abstract

Document-level multi-aspect sentiment classiﬁcation aims to predict user’s sentiment polarities
for different aspects of a product in a review. Existing approaches mainly focus on text informa-
tion. However, the authors (i.e. users) and overall ratings of reviews are ignored, both of which
are proved to be signiﬁcant on interpreting the sentiments of different aspects in this paper. There-
fore, we propose a model called Hierarchical User Aspect Rating Network (HUARN) to consider
user preference and overall ratings jointly. Speciﬁcally, HUARN adopts a hierarchical architec-
ture to encode word, sentence, and document level information. Then, user attention and aspect
attention are introduced into building sentence and document level representation. The document
representation is combined with user and overall rating information to predict aspect ratings of a
review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical
results on two real-world datasets show that HUARN achieves state-of-the-art performances.

1

Introduction

The ever-increasing popularity of online consumer review platforms, such as Tripadvisor1 and Yelp2, has
led to large amounts of online reviews that are often too numerous for users to analyze. Consequently,
there is a growing need for systems analyzing reviews automatically. Lots of approaches (Xia et al.,
2011; Socher et al., 2013; Tang et al., 2015a; Yang et al., 2016) usually focus on determining the overall
sentiment rating of a review. Actually, not only does a review express the general attitude of reviewer,
but it also conveys ﬁne-grained sentiments towards different aspects of corresponding products. Figure 1
shows an example where Bob posts a review about a hotel and gives scores on overall attitude, location,
room, and service respectively. The analysis of these aspect ratings could not only beneﬁt mining inter-
ested aspects for users, but also help companies better understand the major pros and cons of the product.
However, compared with the overall rating, users are less motivated to give aspect ratings. The reviews
without aspect ratings are rampant, which are more than 46% in a simple corpus-based statistics3. Ac-
cordingly, it is really useful to perform document-level multi-aspect sentiment classiﬁcation, whose goal
is to predict ratings for different aspects in a review (Yin et al., 2017).

Multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016) is a straightforward
approach for document-level multi-aspect sentiment classiﬁcation, which shares the input and hidden
layers to obtain a document representation as the input of different aspect-speciﬁc classiﬁers. However,
the representation fails to capture the differences between aspects. In fact, when we predict the sentiment
rating of service, the ﬁrst two sentences in Figure 1 are most helpful and other sentences are auxiliary or
even unnecessary for classifying service. Therefore, aspect-speciﬁc document representation is vital for
this task. To this end, Yin et al. (2017) use iterative attention module to mine aspect-speciﬁc words and
sentences based on a list of aspect keywords and obtain state-of-the-art results.

This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/

1https://www.tripadvisor.com/
2https://www.yelp.com/
3The result is computed on 387,805 reviews crawled from https://www.tripadvisor.com/.

Figure 1: An example of a review. The left part is the review content, the upper right part is the reviewer
Bob and overall rating of the review and bottom right part is different aspect ratings of the review. We
focus on incorporating user preference and overall rating into review content to infer aspect ratings.

Despite the success of methods mentioned above, they typically only use text information. Two kinds
of important information are ignored: users and overall ratings of reviews. The results of our statistical
analysis are convincing that the two factors have strong correlations with aspect ratings (Section 2). As
for users, different users may care about different aspects. When scoring aspects of a hotel, a business
traveler may be critical to service but lenient with price or room. Such preference obviously affects the
aspect ratings. Actually, many studies (Tang et al., 2015b; Chen et al., 2016; Dou, 2017) have shown that
user preference can boost the performance of a related task, document-level sentiment classiﬁcation that
predicts an overall polarity instead of multi-aspect ratings. For our multi-aspect sentiment classiﬁcation,
the overall rating is given, and it can provide prior information to aspect ratings. Usually, the two types
of rating are positive correlation. For example in Figure 1, the overall rating is 4 stars and the aspect
ratings are all not less than 4 stars.

Inspired by the above analysis, we propose a model called Hierarchical User Aspect Rating Net-
work (HUARN) to consider user preference and overall rating jointly for document-level multi-aspect
sentiment classiﬁcation. Speciﬁcally, HUARN utilizes a hierarchical structure to encode word, sen-
tence, to document level information. Then, user and aspect information are embedded as attentions over
word-level and sentence-level representation to construct a user-aspect-speciﬁc document representation.
Based on the document representation, users and overall ratings are combined to express their inﬂuences
on predicting aspect ratings. Finally, we adopt a multi-task framework to mutually enhance aspect rating
prediction between different aspects.

In summary, our main contributions are as follows:

• For document-level multi-aspect sentiment classiﬁcation, we validate the inﬂuences of users and

overall ratings in terms of aspect ratings on massive Tripadvisor reviews.

• To the best of our knowledge, this is the ﬁrst work to incorporate user preference and overall rating

into a uniﬁed model (HUARN) in this task.

• We conduct experiments on two real-world datasets to verify the effectiveness of HUARN. The
experimental results show that HUARN outperforms state-of-the-art methods signiﬁcantly. The
code and data for this paper are available at https://github.com/Junjieli0704/HUARN.

In this section, we ﬁrst introduce real-world datasets used in our work and present some explorations
about the impacts of user preference and overall ratings on aspect ratings.

2 Data and Observations

2.1 Data

We evaluate HUARN on two datasets: TripDMS and TripOUR. They are both crawled from Tripadvisor
website and contain seven aspects (value, room, location, cleanliness, check in, service, and business
service) which are provided by Tripadvisor website. The ﬁrst dataset is built by Yin et al.
(2017).
However, there is no available user information in this dataset, thus we create the second one. Statistics

Datasets
#docs
TripOUR 58,632
TripDMS 29,391

#users
1,702
N/A

#docs/user
34.44
N/A

#words/sen
17.80
18.0

#words/doc
181.03
251.7

Table 1: Statistics of our datasets. The rating scale of TripOUR and TripDMS are 1-5.

value
Datasets
TripOUR 43,258
TripDMS 28,778

room location
42,354
41,295
23,401
29,140

cleanliness
42,601
29,184

check in
1,283
23,373

service
58,449
28,322

business service
801
15,939

Table 2: The absolute number of rating of different aspects in TripOUR and TripDMS.

of our datasets are summarized in Table 1. Table 2 presents the absolute number of ratings of these
aspects in our datasets.

2.2 Observations

Effects of user preference. Inspired by Tang et al. (2015b), we argue that the inﬂuences of users include
the following two aspects: (1) user-rating consistency: different users have different characteristics in
scoring aspects, and aspect ratings from the same user are more consistent than those from different users.
(2) user-text consistency: different users have different word-using habits to express opinions and texts
from the same user are more consistent than those from different users. To verify these consistencies, we
conduct hypothesis testing as follows:

First, we construct three vectors vs, vr and va with equal number (l) of elements. vsi is obtained by
calculating a measurement between two reviews (di and d+
i ) posted by the same user, vri is obtained by
calculating a measurement between di and another random review and vai is a random aspect (such as
service), where i ∈ {1, 2, ..., l}.

For user-rating consistency, the measurement is calculated by ||y − y+|| for vs or ||y − y−|| for
vr, where y, y+, y− is aspect rating of review d, d+, d− with aspect vai respectively. For user-text
consistency, the measurement is calculated by the cosine similarity between bag-of-words representation
of two reviews. We perform a two-sample t-test on vs and vr. The null hypothesis is that there is
no difference between the two vectors, H0 : vs = vr; the alternative hypothesis is that the difference
between reviews with same user is less than with two random reviews , H1 : vs < vr. The t-test results,
p-values, show that there is strong evidence (with the signiﬁcance level α = 0.01) to reject the null
hypothesis in user-rating consistency test and user-text consistency test on TripOUR. In other words, we
observe the existence of user-rating consistency and user-text consistency in TripOUR.

Figure 2: Aspect rating distributions for different overall ratings in TripOUR and TripDMS.

Effects of overall ratings. When scoring a product, users may consider multiple aspects of the product.
If these aspects could meet the users’ requirement, they can give a high overall rating, otherwise, they
could give low scores. Therefore, overall rating can partly reﬂect the user’s attitudes to aspects, which

Figure 3: The architecture of HUARN. Left: some embedding symbols used in the ﬁgure. Example
aspects are service, business service, and room. Middle: Multi-task learning framework for document-
level multi-aspect sentiment classiﬁcation. Right: the architecture of HUARN for aspect service.

is called overall rating prior. To investigate effects of overall ratings, we compute the aspect rating dis-
tributions for different overall ratings from our two datasets and the distributions are shown in Figure 2.
We can conclude that high/low overall ratings often result in high/low aspect ratings. For example, when
the overall rating is 5 stars, more than 70% aspect ratings are not less than 4 stars in our datasets.

3 Methods

The analysis proves users and overall ratings are signiﬁcant on interpreting the sentiments of different
aspects. Therefore, we introduce these two kinds of information into HUARN and detail the model here.
First, we give the formalizations of document-level multi-aspect sentiment classiﬁcation (Figure 3(a)).
Afterwards, we discuss the multi-task learning framework for this task (Figure 3(b)) and how to obtain
document semantic representation via Hierarchical Bidirectional Gated Recurrent Unit network. At last,
we present user and aspect attention mechanism to construct user-aspect-speciﬁc representation and add
a concatenating layer to combine user, overall rating and document representation together (Figure 3(c)).
The enhanced document representation is used as features for predicting aspect ratings.

3.1 Formalizations

Suppose we have a corpus D about a speciﬁc domain (such as “hotel”) and m aspects {a1, a2, ..., am}
(such as service and room). Review d is a sample of D with n sentences {s1, s2, ..., sn}. Sentence si
consists of li words as {wi1, wi2, ..., wili}. The overall rating of review d is r and its author is user u.
Document-level multi-aspect sentiment classiﬁcation aims to predict aspect ratings for these reviews.

3.2 Multi-task Learning Framework

It is natural to model document-level multi-aspect sentiment classiﬁcation as a multi-task learning. First,
we can treat each aspect rating as a classiﬁcation task. Then, we share document encoder network to
obtain document representation and exploits different softmax classiﬁers to predict ratings of different
aspects. The main beneﬁt of the multi-task framework is that it can mutually enhance aspect rating
prediction between different aspects.

3.3 Hierarchical Bidirectional Gated Recurrent Network

Since a document is composed of multiple sentences, and a sentence is composed of multiple words,
we model the semantics of a document through a hierarchical structure from word-level, sentence-level
to document-level. To model the semantic representation of a sentence, we adopt bidirectional GRU
(Bi-GRU). Similarly, we also use Bi-GRU to learn document representations.

Given sentence si, we embed each word wij to vector wij. Then, we use a Bi-GRU to encode contex-
tual information of word wij into its hidden representation hij. Hidden states {hi1, hi2, ...hili} are feed

into an average pooling layer to obtain the sentence representation si. In sentence level, we also feed the
sentence vectors {s1, s2, ..., sn} into Bi-GRU and then obtain the document representation d similarly.

3.4 Encoding user, aspect, and overall rating

It is obvious that not all words (sentences) contribute equally to the sentence (document) meaning. To
consider user-text consistency and build an aspect-speciﬁc representation, we introduce user attention
and aspect attention. Speciﬁcally, we employ word (sentence) level user aspect attention to generate
sentence (document) representation.

Word-level Attention. We ﬁrst embed user u and aspect {ak|k ∈ 1, 2, ..., m} as continuous and real-
valued vector u and ak. Then, instead of feeding word-level hidden states (hij) to an average pooling
layer, we adopt a user aspect attention mechanism to extract user-aspect-speciﬁc words and obtain the
sentence representation as follows:

mij = tanh(Wwhhij + Wuu + Waak + bw)

wmij)

wmij)

exp(vT
j exp(vT
αijhij

(cid:80)

(cid:88)

αij =

sk
i =

j

where Wwh, Wwu, Wwa and bw are parameters in the attention layer. αij measures the importance of
the j-th word for user u and aspect ak and sk

i is the representation of sentence si for aspect ak.

Sentence-level Encoder and Attention. After obtaining sentence representation sk
also use a Bi-GRU to encode the sentences and get hidden representation hk

i for sk
i .

i for aspect ak, we

When classifying document based on different aspects, different sentences may have different inﬂu-
ences. Different users may also pay attention to different sentences. Therefore, in sentence level, we also
apply an attention mechanism with user vector u and aspect vector ak in sentence level to select informa-
tive sentences to compose user-aspect-speciﬁc document representation. The document representation
dk for aspect ak is obtained via:

i + Wsuu + Wsaak + bs)

βi =

ti = tanh(Wshhk
exp(vT
s ti)
i exp(vT
βihk
i

(cid:88)

(cid:80)

dk =

s ti)

i

where βi measures the importance of the i-th sentences for user u and aspect ak.

Concatenation Layer. To explicitly encode user-rating consistency and overall rating prior, we add
a concatenation layer. First, we embed overall rating r as continuous and real-valued vector r with gr
dimensions. Then, we generate review content representation ck by concatenating user embedding u,
rating embedding r and document vector dk:

ck = u ⊕ r ⊕ dk

3.5 Document-level Multi-aspect Sentiment Classiﬁcation

For each aspect, we obtain review representation {ck|k ∈ 1, 2, ..., m}. All these representations are high-
level representations of the combination of user, aspect, overall rating and document information. It can
be used as features for predicting aspect ratings. For aspect ak, we can use a softmax layer to project ck
into sentiment distribution p(d, k) over L classes:

p(d, k) = softmax(Wlkck + bk)

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

where pl(d, k) is used to represent the predicted probability of sentiment class l for d based on ak and
Wlk, bk are parameters of softmax layer for classifying review ck. Then we deﬁne the cross-entropy
error between gold sentiment distribution and our model’s sentiment distribution as our loss function :

(cid:88)

(cid:88)

L = −

L
(cid:88)

d∈D

k∈{1,2,...,m}

l=1

1{gd,k = l} · log(pl(d, k))

(9)

where 1{·} is the indicator function and gd,k represents the ground truth label for review d for aspect ak.

4 Experiments

In this section, we present data preprocessing and implementation details, all the comparison methods
and the empirical results on the task of document-level multi-aspect sentiment classiﬁcation.

4.1 Data Preprocessing and Implementation Details

We preprocess our datasets as follows: For TripDMS, we use the same splitting method as (Yin et al.,
2017). For TripOUR, we tokenize the dataset, split sentences by Stanford CoreNLP (Manning et al.,
2014) and randomly split them into training, development, and testing sets with 80/10/10%.

The model hyper-parameters are tuned based on the development sets. For word embeddings, we
use the pre-trained word embeddings provided by (Yin et al., 2017), whose embedding size is 200. For
user and overall rating embeddings, we initialize them randomly and set their dimensions to 200. For
aspect embeddings, we ﬁrst get aspect keywords4 from (Yin et al., 2017) and initial aspect embedding
by averaging word embeddings of these keywords belong to the aspect. The dimensions of all hidden
vectors are set to 150. To avoid model over-ﬁtting, we use dropout with rate of 0.2. All the parameters
are trained using Adam (Kingma and Ba, 2014) with a learning rate of 0.001.

4.2 Comparison Methods

We compare HUARN with the following baselines:

Majority is a heuristic baseline method, which assigns the majority sentiment category in the training

OverallRatingSame is also a heuristic baseline method, which assigns the overall rating of a review

set to aspect rating in the test dataset.

to its aspect ratings.

MajOverallRating splits the training instances into ﬁve clusters (per overall rating) and assigns the

most frequent rating for the seven aspects per cluster in the test dataset.

SVM and NBoW are SVM classiﬁers with different features. One with unigrams, bigrams as features

and another with the mean of word embeddings in a document as features.

CNN (Kim, 2014) performs a convolution operation over a sentence to extract words neighboring

features, then gets a ﬁxed-sized representation by a pooling layer.

HAN (Yang et al., 2016) models review in a hierarchical structure and utilizes an attention mechanism
to capture important words and sentences, which is only based on text information and achieves
state-of-the-art result in predicting overall rating of document.

MHCNN is an extended model of CNN with hierarchical architecture and multi-task framework.
MHAN is an extended model of HAN with multi-task framework.
DMSCMC (Yin et al., 2017) use iterative attention modules to build up aspect-speciﬁc representation
for review, and obtain state-of-the-art results in document-level multi-aspect sentiment classiﬁca-
tion.

HGRUN is the basic form of HUARN without user, aspect and overall rating.
HARN is a variant of HUARN, which abandons user information from HUARN.

4.3 Results

We use Accuracy and Mean Squared Error (MSE) as the evaluation metrics, and the results are shown
in Table 3. For heuristic methods, we can see that Majority performs very poor because it does not

4Sample keywords for service are service, food, breakfast, and buffet.

Models

Majority
OverallRatingSame
MajOverallRating
SVM
NBoW
CNN
HAN
MHCNN
MHAN
HGRUN
DMSCMC
HARN
HUARN

TripOUR

TripDMS

Accuracy↑ MSE↓ Accuracy↑ MSE↓
2.549†
3.446
3.273
1.963†
1.808†
1.456†
1.301†
1.398†
1.210†
1.303
1.083†
0.923
N/A

0.2389†
0.2012
0.2414
0.3526†
0.3909†
0.4335†
0.4468†
0.4379†
0.4494†
0.4435
0.4656†
0.4821*
N/A

0.3850
0.3074
0.3487
0.4635
0.4865
0.5054
0.5123
0.5108
0.5419
0.5392
0.5549
0.5815*
0.6070*

0.954
1.705
1.536
1.025
0.912
0.752
0.705
0.712
0.629
0.635
0.583
0.528
0.514

Table 3: Document-level multi-aspect sentiment classiﬁcation on our datasets. Our full model is
HUARN. The best performances in bold. “†” indicates that the result is reported from (Yin et al., 2017).
“*” indicates that the model signiﬁcantly outperforms DMSCMC. Statistical signiﬁcance testing has
been performed using paired t-test with p < 0.05.

capture any text information. OverallRatingSame and MajOverallRating are also very poor, even
though overall rating has strong correction with aspect ratings, it is not enough to decide aspect ratings
only based on it.

Compared with SVM, NBoW achieves higher accuracy by at least 2.3% in both datasets, which shows
that embedding features are more effective than unigram and bigram features on these two datasets.
When applying more complex neural networks (such as CNN and HAN), the model can achieve higher
accuracy by at least 1.5% in both datasets compared with NBoW. Additionally, we observe that the multi-
task learning and hierarchical architecture are beneﬁcial for neural networks. Performance on MHAN
and MHCNN are slightly better than HAN and CNN. Beyond that, we also ﬁnd attention mechanism is
useful. The only difference between MHAN and HGRUN is that MHAN uses attention mechanism to
obtain sentence and document representations while HGRUN utilizes an average pooling layer, which
results in the performance of MHAN is better than HGRUN. After obtaining aspect-aware representation
for the document, DMSCMC achieves best results and outperforms other baselines.

Compared to DMSCMC, HARN achieves improvements of 2.7% and 1.7% on TripOUR and
TripDMS respectively, which shows that the incorporation of overall rating and aspect attention helps
build up more discriminative representation. Moreover, when incorporating user information, our full
model (HUARN) can achieve improvements of 5.3% compared with DMSCMC on TripOUR5, which
shows user preference can beneﬁt the document-level multi-aspect sentiment classiﬁcation task.

5 Discussions

In this section, we ﬁrst give some discussions about the effects of users, aspects and overall ratings on
predicting aspect ratings, and then show case study for attention results and visualize user embeddings.

5.1 Effects of Users, Overall Ratings and Aspects

Users, overall ratings and aspects are three kinds of information in HUARN. We present the effects of
users, overall ratings, and aspects on document-level multi-aspect sentiment classiﬁcation in Table 4.
From the table, we can observe that: (1) Compared with user-agnostic models (line 1-4), user-aware
models (line 5-8) can achieve improvements of 2.2%, 2.5%, 1.2% and 2.5% in TripOUR, which shows

5Since there is no user information in TripDMS, we can only compare DMSCMC with HARN.

No.

1
2
3
4
5
6
7
8

Different information

TripOUR

TripDMS

–
(cid:33)

–
–
(cid:33)
(cid:33)

User OverallRating Aspect Accuracy↑ MSE↓ Accuracy↑ MSE↓
1.303
1.256
1.093
0.923
N/A
N/A
N/A
N/A

0.5392
0.5514
0.5719
0.5815
0.5640
0.5764
0.5839
0.6070

0.4435
0.4566
0.4740
0.4821
N/A
N/A
N/A
N/A

0.635
0.599
0.555
0.528
0.619
0.581
0.560
0.514

–
–
–
–
(cid:33)
(cid:33)
(cid:33)
(cid:33)

–
–
(cid:33)
(cid:33)

–
(cid:33)

–
(cid:33)

–
(cid:33)

Table 4: Effects of user, overall rating and aspect on document-level multi-aspect sentiment classiﬁca-
tion. Each line represents a variant of HUARN, where “(cid:33)” denotes the variant considers the speciﬁc
information, while “–” denotes not. For example, model in line 1 means HUARN abandons these three
kinds of information and degenerates into HGRUN.

that model encoding user information can obtain user-aware document representation and is more suit-
able for document-level multi-aspect sentiment classiﬁcation. (2) Compared with models without overall
rating information (line 1, 2, 5 and 6), models considering overall ratings (line 3, 4, 7, 8) can obtain 3.2%
(3.1%), 3.0% (2.6%), 1.9% (N/A) and 3.1% (N/A) improvements in accuracy in both datasets, which
indicates overall rating information is useful for building more discriminative document representation
and helpful for predicting aspect ratings. (3) Compared with models without aspect information (line 1,
3, 5 and 7), aspect-based models (line 2, 4, 6, 8) can obtain 1.2% (1.3%), 1.0% (0.8%), 1.2% (N/A) and
2.4% (N/A) improvements in accuracy in both datasets. It shows that aspect information is useful for
building aspect-aware document representation and helpful for predicting aspect ratings. (4) After users,
overall ratings, and aspects being considered jointly, our model obtains the best performance.

5.2 Visualization of User Embeddings

Figure 4: t-SNE visualization of user embeddings for different aspects in TripOUR. Blue square and red
triangle represent users are “High Score” users and “Low Score” users, respectively.

As different users have different aspect rating preferences and HUARN imports user embedding to
consider users, we identify whether such personalized information are encoded in user embedding. To
this end, we ﬁrst rank all users according to their average score in the training set for each aspect.
Then the top 100 users are labeled as “High Score” users and bottom 100 users are labeled as “Low
Score” users. Due to space limit, here we only show embeddings of users in four aspects (service, value,
cleanliness, location), which are top frequently scored by all users, in Figure 4. We ﬁnd “High Score”
users and “Low Score” users are separated apparently. The visualization shows that user embedding
learned by HUARN can encode personalized traits in scoring different aspects.

5.3 Case Study for Attention Results

To show the ability that HUARN captures user preference and aspect semantic meanings, we take one
sentence from TripOUR as example. The content of the sentence is “The food is good, but the price is

Figure 5: The attention visualization of words. Dark color means higher weight. (a), (b) and (c) show
word-level attention weights of MHAN, HARN and HUARN.

very expensive”, in which “good” is a general sentiment word and can be used to describe many aspects
(such as service, room and so on), while “expensive” is a aspect-speciﬁc sentiment word which only
applies to describe value. We visualize attention weights of the sentence in Figure 5.

From Figure 5(a), we can ﬁnd that considering word-level attention, MHAN distinguishes sentiment
words and non-sentiment words, however it is hard to identify the sentiment word is a general one or
an aspect-speciﬁc one. After adding aspect attention over word-level representations, HARN can distin-
guish these two kinds of sentiment words (Figure 5(b)). The attention weights of “good” for different
aspects are very close, while the attention weights of “expensive” for different aspects are different, and
the maximum is value. When adding user information into word-level attention, HUARN can also treat
“good” differently. From ﬁgure 5(c), we can ﬁnd attention weights of “good” are different for different
aspects, where weights for service, check in and room are higher than weights for other aspects. we
check all reviews of the sample review’s author and ﬁnd that he/she often (more than 80%) use “good”
to describe service, check in and room.

5.4 Error Analysis

Figure 6: Examples of error cases. GT means ground trouth and P means prediction result of HUARN.
Sentences in review text with darker color means higher attention weight for the sentence.

We analyze error cases in the experiments. Some examples of error cases are shown in Figure 6. We
can ﬁnd that HUARN is hard to select important sentences for aspects. For example, sentence ”the rooms
are large and the bed comfy.” and sentence ”however the carpet should have been replaced long time.” in
the ﬁrst sample in Figure 6 are all important to decide the rating of aspect room. However, HUARN pays
more attention to the former sentence when predicting rating of room and obtains the wrong result.

Based on the literature study, we ﬁnd that Yin et al. (2017) uses iterative attention models to build up
aspect-speciﬁc representation for review. It may alleviate this problem. We leave how to encode user and
overall rating information into DMSCMC as our future work.

6 Related Work

Multi-aspect sentiment classiﬁcation is an extensively studied task in sentiment analysis (Pang and Lee,
2008; Liu, 2012). Lu et al. (2011) propose Segmented Topic Model to model document and extract
features, then exploit support vector regression to predict aspect ratings based on these features. McAuley
et al. (2012) add a dependency term in ﬁnal multi-class SVM objective to consider the correction between
aspects. Many other studies (Titov and McDonald, 2008; Wang et al., 2010; Wang et al., 2011; Diao
et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multi-
aspect sentiment classiﬁcation as a subproblem by utilizing heuristic based methods or topic models.
However, these approaches often rely on strict assumptions about words and sentences, for example,
word syntax has been used to distinguish aspect word or sentiment word, or appending an speciﬁc aspect
to a sentence. Another related problem is called aspect-level sentiment classiﬁcation (Pontiki et al.,
2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al.
(2016) and Tang et al. (2016) employ attention-based LSTM and deep memory network for aspect-level
sentiment classiﬁcation, respectively. However, the task is sentence level. Document-level sentiment
classiﬁcation (Li and Zong, 2008; Li et al., 2010; Li et al., 2013; Xia et al., 2015; Yang et al., 2016) is
also a related research ﬁeld because we can treat single aspect sentiment classiﬁcation as an individual
document classiﬁcation task. However, they did not consider multiple aspects in a document.

In addition to these methods, the work of Yin et al. (2017) is the most related to ours, which focuses
on using iterative attention mechanism to build discriminative aspect-aware representation to perform
document-level multi-aspect sentiment classiﬁcation. However, it ignores the inﬂuences of users and
overall ratings on aspect ratings. Actually, many studies (Tang et al., 2015b; Tang et al., 2015c; Chen
et al., 2016; Li et al., 2016; Dou, 2017) have shown considering user preference can boost the perfor-
mance of Document-level Sentiment Classiﬁcation. Partially inspired by these approaches, we propose
HUARN to consider users, overall ratings and aspects jointly into document-level multi-aspect sentiment
classiﬁcation. Compared with these user-aware approaches, HUARN has some differences: (1) They do
not consider aspects. (2) Although Chen et al. (2016) and Dou (2017) embedding user to consider user-
text consistency to perform sentiment classiﬁcation, they ignore user-rating consistency. (3) Tang et al.
(2015b; 2015c) embed user in a matrix and build user-speciﬁc representation by a convolutional neural
network structure. However, it is hard to train with limited reviews for user matrix. Our motivation is
that (1) Aspect information is very useful for selecting informative words and sentences and building
up aspect-speciﬁc representation for document-level multi-aspect sentiment classiﬁcation, therefore, we
add aspect attention into our model. (2) Compared with user-text consistency, user-rating consistency
describes the correlation between users and ratings more directly. (3) Embedding user in a vector and
using attention mechanism to build user-speciﬁc representation is an effective way to consider users.
User embedding is enough to encode the relation between user and rating in The most important is that
it is easy to train.

7 Conclusion and Future work

In this paper, we present Hierarchical User Aspect Rating Network (HUARN) to incorporate user pref-
erence and overall rating into document-level multi-aspect sentiment classiﬁcation. HUARN encodes
different kinds of information (word, sentence and document) into a hierarchical structure. To consid-
er user preference and overall rating, HUARN introduces user information as attention over word-level
representation and sentence-level representation, and then generates review representation by combining
user, overall rating and document information. Extensive experiments show that our model outperforms
state-of-the-art methods signiﬁcantly. In the future, we will study how to encode user and overall rating
information into DMSCMC.

Acknowledgements

We thank Xiaomian Kang and Yang Zhao for valuable discussions. We also thank the anonymous re-
viewers for their suggestions. The research work descried in this paper has been supported by the Natural
Science Foundation of China under Grant No. 61333018 and 61673380.

References

Rich Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75.

Huimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin, and Zhiyuan Liu. 2016. Neural sentiment classiﬁcation

with user and product attention. In Proceedings of EMNLP.

Ronan Collobert, Jason Weston, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language

processing (almost) from scratch. Journal of Machine Learning Research, 12(1):2493–2537.

Qiming Diao, Minghui Qiu, Chao Yuan Wu, Alexander J. Smola, Jing Jiang, and Chong Wang. 2014. Jointly
modeling aspects, ratings and sentiments for movie recommendation (jmars). In Proceedings of KDD, pages
193–202.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network

for target-dependent twitter sentiment classiﬁcation. In Proceedings of ACL, pages 49–54.

Zi-Yi Dou. 2017. Capturing user and product information for document level sentiment analysis with deep
memory network. In Proceedings of EMNLP, pages 532–537, Copenhagen, Denmark, September. Association
for Computational Linguistics.

Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. Eprint Arxiv.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.

Shoushan Li and Chengqing Zong. 2008. Multi-domain sentiment classiﬁcation. In ACL 2008, Proceedings of
the 46th Annual Meeting of the Association for Computational Linguistics, June 15-20, 2008, Columbus, Ohio,
USA, Short Papers, pages 257–260.

Shoushan Li, Chu-Ren Huang, Guodong Zhou, and Sophia Yat Mei Lee. 2010. Employing personal/impersonal
views in supervised and semi-supervised sentiment classiﬁcation. In ACL 2010, Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden, pages 414–423.

Shoushan Li, Yunxia Xue, Zhongqing Wang, and Guodong Zhou. 2013. Active learning for cross-domain senti-

ment classiﬁcation. In Proceedings of IJCAI, pages 2127–2133.

Junjie Li, Haitong Yang, and Chengqing Zong. 2016. Sentiment classiﬁcation of social media text considering user
attributes. In Natural Language Understanding and Intelligent Applications - 5th CCF Conference on Natural
Language Processing and Chinese Computing, NLPCC 2016, and 24th International Conference on Computer
Processing of Oriental Languages, ICCPOL 2016, Kunming, China, December 2-6, 2016, Proceedings, pages
583–594.

Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis lectures on human language technologies,

5(1):1–167.

Bin Lu, Myle Ott, Claire Cardie, and Benjamin K Tsou. 2011. Multi-aspect sentiment analysis with topic models.

In Proceedings of ICDM Workshops, pages 81–88.

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-task sequence to

sequence learning. In Proceedings of ICLR, San Juan, Puerto Rico, May.

Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language processing toolkit. In ACL (System Demonstrations), pages 55–60.

Julian Mcauley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes and attributes from multi-aspect

reviews. In Proceedings of ICDM, pages 1020–1025.

Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information

retrieval, 2(1-2):1–135.

Nikolaos Pappas and Andrei Popescu-Belis. 2014. Explaining the stars: Weighted multiple-instance learning for
aspect-based sentiment analysis. In Proceedings of EMNLP, pages 455–466. Association for Computational
Linguistics.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Man-
andhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. Proceedings of International Workshop
on Semantic Evaluation at, pages 27–35.

Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad AL-
Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphee De Clercq, Veronique Hoste, Marianna Apidi-
anaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, N´uria Bel, Salud Mar´ıa Jim´enez-Zafra, and
In Proceedings of the 10th
G¨uls¸en Eryi˘git. 2016. Semeval-2016 task 5: Aspect based sentiment analysis.
International Workshop on Semantic Evaluation (SemEval-2016), pages 19–30. Association for Computational
Linguistics.

Kim Schouten and Flavius Frasincar. 2016. Survey on aspect-level sentiment analysis. IEEE Transactions on

Knowledge and Data Engineering, 28(3):813–830.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christo-
pher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceed-
ings of EMNLP.

Duyu Tang, Bing Qin, and Ting Liu. 2015a. Document modeling with gated recurrent neural network for sentiment
In Proceedings of EMNLP, pages 1422–1432, Lisbon, Portugal, September. Association for

classiﬁcation.
Computational Linguistics.

Duyu Tang, Bing Qin, and Ting Liu. 2015b. Learning semantic representations of users and products for document

level sentiment classiﬁcation. In Proceedings of ACL, pages 1014–1023, July.

Duyu Tang, Bing Qin, Yuekui Yang, and Yuekui Yang. 2015c. User modeling with neural network for review

rating prediction. In Proceedings of IJCAI, pages 1340–1346.

Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect level sentiment classiﬁcation with deep memory network.

arXiv preprint arXiv:1605.08900.

Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In

Proceedings of ACL, pages 308–316. Association for Computational Linguistics.

Zhiqiang Toh and Jian Su. 2016. Nlangp at semeval-2016 task 5: Improving aspect based sentiment analysis
In Proceedings of the 10th International Workshop on Semantic Evaluation

using neural network features.
(SemEval-2016), pages 282–288. Association for Computational Linguistics.

Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect rating analysis on review text data: a rating

regression approach. In Proceedings of KDD, pages 783–792.

Hongning Wang, Yue Lu, and Cheng Xiang Zhai. 2011. Latent aspect rating analysis without aspect keyword

supervision. In Proceedings of KDD, pages 618–626.

Yequan Wang, Minlie Huang, Li Zhao, and Xiaoyan Zhu. 2016. Attention-based lstm for aspect-level sentiment

classiﬁcation. In Proceedings of EMNLP.

Rui Xia, Chengqing Zong, and Shoushan Li. 2011. Ensemble of feature sets and classiﬁcation algorithms for

sentiment classiﬁcation. Information Sciences, 181(6):1138–1152.

Rui Xia, Feng Xu, Chengqing Zong, Qianmu Li, Yong Qi, and Tao Li. 2015. Dual sentiment analysis: Considering

two sides of one review. IEEE Transactions on Knowledge and Data Engineering, 27(8):2120–2133.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hierarchical

attention networks for document classiﬁcation. In HLT-NAACL, pages 1480–1489.

Yichun Yin, Yangqiu Song, and Ming Zhang. 2017. Document-level multi-aspect sentiment classiﬁcation as
machine comprehension. In Proceedings of EMNLP, pages 2044–2054. Association for Computational Lin-
guistics.

Document-level Multi-aspect Sentiment Classiﬁcation by Jointly
Modeling Users, Aspects, and Overall Ratings

Junjie Li1,2, Haitong Yang3 and Chengqing Zong1,2,4
1 National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China
2 University of Chinese Academy of Sciences, Beijing, China
3 School of Computer, Central China Normal University, Wuhan 430079, China
4 CAS Center for Excellence in Brain Science and Intelligence Technology
{junjie.li, cqzong}@nlpr.ia.ac.cn, htyang@mail.ccnu.edu.cn

Abstract

Document-level multi-aspect sentiment classiﬁcation aims to predict user’s sentiment polarities
for different aspects of a product in a review. Existing approaches mainly focus on text informa-
tion. However, the authors (i.e. users) and overall ratings of reviews are ignored, both of which
are proved to be signiﬁcant on interpreting the sentiments of different aspects in this paper. There-
fore, we propose a model called Hierarchical User Aspect Rating Network (HUARN) to consider
user preference and overall ratings jointly. Speciﬁcally, HUARN adopts a hierarchical architec-
ture to encode word, sentence, and document level information. Then, user attention and aspect
attention are introduced into building sentence and document level representation. The document
representation is combined with user and overall rating information to predict aspect ratings of a
review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical
results on two real-world datasets show that HUARN achieves state-of-the-art performances.

1

Introduction

The ever-increasing popularity of online consumer review platforms, such as Tripadvisor1 and Yelp2, has
led to large amounts of online reviews that are often too numerous for users to analyze. Consequently,
there is a growing need for systems analyzing reviews automatically. Lots of approaches (Xia et al.,
2011; Socher et al., 2013; Tang et al., 2015a; Yang et al., 2016) usually focus on determining the overall
sentiment rating of a review. Actually, not only does a review express the general attitude of reviewer,
but it also conveys ﬁne-grained sentiments towards different aspects of corresponding products. Figure 1
shows an example where Bob posts a review about a hotel and gives scores on overall attitude, location,
room, and service respectively. The analysis of these aspect ratings could not only beneﬁt mining inter-
ested aspects for users, but also help companies better understand the major pros and cons of the product.
However, compared with the overall rating, users are less motivated to give aspect ratings. The reviews
without aspect ratings are rampant, which are more than 46% in a simple corpus-based statistics3. Ac-
cordingly, it is really useful to perform document-level multi-aspect sentiment classiﬁcation, whose goal
is to predict ratings for different aspects in a review (Yin et al., 2017).

Multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016) is a straightforward
approach for document-level multi-aspect sentiment classiﬁcation, which shares the input and hidden
layers to obtain a document representation as the input of different aspect-speciﬁc classiﬁers. However,
the representation fails to capture the differences between aspects. In fact, when we predict the sentiment
rating of service, the ﬁrst two sentences in Figure 1 are most helpful and other sentences are auxiliary or
even unnecessary for classifying service. Therefore, aspect-speciﬁc document representation is vital for
this task. To this end, Yin et al. (2017) use iterative attention module to mine aspect-speciﬁc words and
sentences based on a list of aspect keywords and obtain state-of-the-art results.

This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/

1https://www.tripadvisor.com/
2https://www.yelp.com/
3The result is computed on 387,805 reviews crawled from https://www.tripadvisor.com/.

Figure 1: An example of a review. The left part is the review content, the upper right part is the reviewer
Bob and overall rating of the review and bottom right part is different aspect ratings of the review. We
focus on incorporating user preference and overall rating into review content to infer aspect ratings.

Despite the success of methods mentioned above, they typically only use text information. Two kinds
of important information are ignored: users and overall ratings of reviews. The results of our statistical
analysis are convincing that the two factors have strong correlations with aspect ratings (Section 2). As
for users, different users may care about different aspects. When scoring aspects of a hotel, a business
traveler may be critical to service but lenient with price or room. Such preference obviously affects the
aspect ratings. Actually, many studies (Tang et al., 2015b; Chen et al., 2016; Dou, 2017) have shown that
user preference can boost the performance of a related task, document-level sentiment classiﬁcation that
predicts an overall polarity instead of multi-aspect ratings. For our multi-aspect sentiment classiﬁcation,
the overall rating is given, and it can provide prior information to aspect ratings. Usually, the two types
of rating are positive correlation. For example in Figure 1, the overall rating is 4 stars and the aspect
ratings are all not less than 4 stars.

Inspired by the above analysis, we propose a model called Hierarchical User Aspect Rating Net-
work (HUARN) to consider user preference and overall rating jointly for document-level multi-aspect
sentiment classiﬁcation. Speciﬁcally, HUARN utilizes a hierarchical structure to encode word, sen-
tence, to document level information. Then, user and aspect information are embedded as attentions over
word-level and sentence-level representation to construct a user-aspect-speciﬁc document representation.
Based on the document representation, users and overall ratings are combined to express their inﬂuences
on predicting aspect ratings. Finally, we adopt a multi-task framework to mutually enhance aspect rating
prediction between different aspects.

In summary, our main contributions are as follows:

• For document-level multi-aspect sentiment classiﬁcation, we validate the inﬂuences of users and

overall ratings in terms of aspect ratings on massive Tripadvisor reviews.

• To the best of our knowledge, this is the ﬁrst work to incorporate user preference and overall rating

into a uniﬁed model (HUARN) in this task.

• We conduct experiments on two real-world datasets to verify the effectiveness of HUARN. The
experimental results show that HUARN outperforms state-of-the-art methods signiﬁcantly. The
code and data for this paper are available at https://github.com/Junjieli0704/HUARN.

In this section, we ﬁrst introduce real-world datasets used in our work and present some explorations
about the impacts of user preference and overall ratings on aspect ratings.

2 Data and Observations

2.1 Data

We evaluate HUARN on two datasets: TripDMS and TripOUR. They are both crawled from Tripadvisor
website and contain seven aspects (value, room, location, cleanliness, check in, service, and business
service) which are provided by Tripadvisor website. The ﬁrst dataset is built by Yin et al.
(2017).
However, there is no available user information in this dataset, thus we create the second one. Statistics

Datasets
#docs
TripOUR 58,632
TripDMS 29,391

#users
1,702
N/A

#docs/user
34.44
N/A

#words/sen
17.80
18.0

#words/doc
181.03
251.7

Table 1: Statistics of our datasets. The rating scale of TripOUR and TripDMS are 1-5.

value
Datasets
TripOUR 43,258
TripDMS 28,778

room location
42,354
41,295
23,401
29,140

cleanliness
42,601
29,184

check in
1,283
23,373

service
58,449
28,322

business service
801
15,939

Table 2: The absolute number of rating of different aspects in TripOUR and TripDMS.

of our datasets are summarized in Table 1. Table 2 presents the absolute number of ratings of these
aspects in our datasets.

2.2 Observations

Effects of user preference. Inspired by Tang et al. (2015b), we argue that the inﬂuences of users include
the following two aspects: (1) user-rating consistency: different users have different characteristics in
scoring aspects, and aspect ratings from the same user are more consistent than those from different users.
(2) user-text consistency: different users have different word-using habits to express opinions and texts
from the same user are more consistent than those from different users. To verify these consistencies, we
conduct hypothesis testing as follows:

First, we construct three vectors vs, vr and va with equal number (l) of elements. vsi is obtained by
calculating a measurement between two reviews (di and d+
i ) posted by the same user, vri is obtained by
calculating a measurement between di and another random review and vai is a random aspect (such as
service), where i ∈ {1, 2, ..., l}.

For user-rating consistency, the measurement is calculated by ||y − y+|| for vs or ||y − y−|| for
vr, where y, y+, y− is aspect rating of review d, d+, d− with aspect vai respectively. For user-text
consistency, the measurement is calculated by the cosine similarity between bag-of-words representation
of two reviews. We perform a two-sample t-test on vs and vr. The null hypothesis is that there is
no difference between the two vectors, H0 : vs = vr; the alternative hypothesis is that the difference
between reviews with same user is less than with two random reviews , H1 : vs < vr. The t-test results,
p-values, show that there is strong evidence (with the signiﬁcance level α = 0.01) to reject the null
hypothesis in user-rating consistency test and user-text consistency test on TripOUR. In other words, we
observe the existence of user-rating consistency and user-text consistency in TripOUR.

Figure 2: Aspect rating distributions for different overall ratings in TripOUR and TripDMS.

Effects of overall ratings. When scoring a product, users may consider multiple aspects of the product.
If these aspects could meet the users’ requirement, they can give a high overall rating, otherwise, they
could give low scores. Therefore, overall rating can partly reﬂect the user’s attitudes to aspects, which

Figure 3: The architecture of HUARN. Left: some embedding symbols used in the ﬁgure. Example
aspects are service, business service, and room. Middle: Multi-task learning framework for document-
level multi-aspect sentiment classiﬁcation. Right: the architecture of HUARN for aspect service.

is called overall rating prior. To investigate effects of overall ratings, we compute the aspect rating dis-
tributions for different overall ratings from our two datasets and the distributions are shown in Figure 2.
We can conclude that high/low overall ratings often result in high/low aspect ratings. For example, when
the overall rating is 5 stars, more than 70% aspect ratings are not less than 4 stars in our datasets.

3 Methods

The analysis proves users and overall ratings are signiﬁcant on interpreting the sentiments of different
aspects. Therefore, we introduce these two kinds of information into HUARN and detail the model here.
First, we give the formalizations of document-level multi-aspect sentiment classiﬁcation (Figure 3(a)).
Afterwards, we discuss the multi-task learning framework for this task (Figure 3(b)) and how to obtain
document semantic representation via Hierarchical Bidirectional Gated Recurrent Unit network. At last,
we present user and aspect attention mechanism to construct user-aspect-speciﬁc representation and add
a concatenating layer to combine user, overall rating and document representation together (Figure 3(c)).
The enhanced document representation is used as features for predicting aspect ratings.

3.1 Formalizations

Suppose we have a corpus D about a speciﬁc domain (such as “hotel”) and m aspects {a1, a2, ..., am}
(such as service and room). Review d is a sample of D with n sentences {s1, s2, ..., sn}. Sentence si
consists of li words as {wi1, wi2, ..., wili}. The overall rating of review d is r and its author is user u.
Document-level multi-aspect sentiment classiﬁcation aims to predict aspect ratings for these reviews.

3.2 Multi-task Learning Framework

It is natural to model document-level multi-aspect sentiment classiﬁcation as a multi-task learning. First,
we can treat each aspect rating as a classiﬁcation task. Then, we share document encoder network to
obtain document representation and exploits different softmax classiﬁers to predict ratings of different
aspects. The main beneﬁt of the multi-task framework is that it can mutually enhance aspect rating
prediction between different aspects.

3.3 Hierarchical Bidirectional Gated Recurrent Network

Since a document is composed of multiple sentences, and a sentence is composed of multiple words,
we model the semantics of a document through a hierarchical structure from word-level, sentence-level
to document-level. To model the semantic representation of a sentence, we adopt bidirectional GRU
(Bi-GRU). Similarly, we also use Bi-GRU to learn document representations.

Given sentence si, we embed each word wij to vector wij. Then, we use a Bi-GRU to encode contex-
tual information of word wij into its hidden representation hij. Hidden states {hi1, hi2, ...hili} are feed

into an average pooling layer to obtain the sentence representation si. In sentence level, we also feed the
sentence vectors {s1, s2, ..., sn} into Bi-GRU and then obtain the document representation d similarly.

3.4 Encoding user, aspect, and overall rating

It is obvious that not all words (sentences) contribute equally to the sentence (document) meaning. To
consider user-text consistency and build an aspect-speciﬁc representation, we introduce user attention
and aspect attention. Speciﬁcally, we employ word (sentence) level user aspect attention to generate
sentence (document) representation.

Word-level Attention. We ﬁrst embed user u and aspect {ak|k ∈ 1, 2, ..., m} as continuous and real-
valued vector u and ak. Then, instead of feeding word-level hidden states (hij) to an average pooling
layer, we adopt a user aspect attention mechanism to extract user-aspect-speciﬁc words and obtain the
sentence representation as follows:

mij = tanh(Wwhhij + Wuu + Waak + bw)

wmij)

wmij)

exp(vT
j exp(vT
αijhij

(cid:80)

(cid:88)

αij =

sk
i =

j

where Wwh, Wwu, Wwa and bw are parameters in the attention layer. αij measures the importance of
the j-th word for user u and aspect ak and sk

i is the representation of sentence si for aspect ak.

Sentence-level Encoder and Attention. After obtaining sentence representation sk
also use a Bi-GRU to encode the sentences and get hidden representation hk

i for sk
i .

i for aspect ak, we

When classifying document based on different aspects, different sentences may have different inﬂu-
ences. Different users may also pay attention to different sentences. Therefore, in sentence level, we also
apply an attention mechanism with user vector u and aspect vector ak in sentence level to select informa-
tive sentences to compose user-aspect-speciﬁc document representation. The document representation
dk for aspect ak is obtained via:

i + Wsuu + Wsaak + bs)

βi =

ti = tanh(Wshhk
exp(vT
s ti)
i exp(vT
βihk
i

(cid:88)

(cid:80)

dk =

s ti)

i

where βi measures the importance of the i-th sentences for user u and aspect ak.

Concatenation Layer. To explicitly encode user-rating consistency and overall rating prior, we add
a concatenation layer. First, we embed overall rating r as continuous and real-valued vector r with gr
dimensions. Then, we generate review content representation ck by concatenating user embedding u,
rating embedding r and document vector dk:

ck = u ⊕ r ⊕ dk

3.5 Document-level Multi-aspect Sentiment Classiﬁcation

For each aspect, we obtain review representation {ck|k ∈ 1, 2, ..., m}. All these representations are high-
level representations of the combination of user, aspect, overall rating and document information. It can
be used as features for predicting aspect ratings. For aspect ak, we can use a softmax layer to project ck
into sentiment distribution p(d, k) over L classes:

p(d, k) = softmax(Wlkck + bk)

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

where pl(d, k) is used to represent the predicted probability of sentiment class l for d based on ak and
Wlk, bk are parameters of softmax layer for classifying review ck. Then we deﬁne the cross-entropy
error between gold sentiment distribution and our model’s sentiment distribution as our loss function :

(cid:88)

(cid:88)

L = −

L
(cid:88)

d∈D

k∈{1,2,...,m}

l=1

1{gd,k = l} · log(pl(d, k))

(9)

where 1{·} is the indicator function and gd,k represents the ground truth label for review d for aspect ak.

4 Experiments

In this section, we present data preprocessing and implementation details, all the comparison methods
and the empirical results on the task of document-level multi-aspect sentiment classiﬁcation.

4.1 Data Preprocessing and Implementation Details

We preprocess our datasets as follows: For TripDMS, we use the same splitting method as (Yin et al.,
2017). For TripOUR, we tokenize the dataset, split sentences by Stanford CoreNLP (Manning et al.,
2014) and randomly split them into training, development, and testing sets with 80/10/10%.

The model hyper-parameters are tuned based on the development sets. For word embeddings, we
use the pre-trained word embeddings provided by (Yin et al., 2017), whose embedding size is 200. For
user and overall rating embeddings, we initialize them randomly and set their dimensions to 200. For
aspect embeddings, we ﬁrst get aspect keywords4 from (Yin et al., 2017) and initial aspect embedding
by averaging word embeddings of these keywords belong to the aspect. The dimensions of all hidden
vectors are set to 150. To avoid model over-ﬁtting, we use dropout with rate of 0.2. All the parameters
are trained using Adam (Kingma and Ba, 2014) with a learning rate of 0.001.

4.2 Comparison Methods

We compare HUARN with the following baselines:

Majority is a heuristic baseline method, which assigns the majority sentiment category in the training

OverallRatingSame is also a heuristic baseline method, which assigns the overall rating of a review

set to aspect rating in the test dataset.

to its aspect ratings.

MajOverallRating splits the training instances into ﬁve clusters (per overall rating) and assigns the

most frequent rating for the seven aspects per cluster in the test dataset.

SVM and NBoW are SVM classiﬁers with different features. One with unigrams, bigrams as features

and another with the mean of word embeddings in a document as features.

CNN (Kim, 2014) performs a convolution operation over a sentence to extract words neighboring

features, then gets a ﬁxed-sized representation by a pooling layer.

HAN (Yang et al., 2016) models review in a hierarchical structure and utilizes an attention mechanism
to capture important words and sentences, which is only based on text information and achieves
state-of-the-art result in predicting overall rating of document.

MHCNN is an extended model of CNN with hierarchical architecture and multi-task framework.
MHAN is an extended model of HAN with multi-task framework.
DMSCMC (Yin et al., 2017) use iterative attention modules to build up aspect-speciﬁc representation
for review, and obtain state-of-the-art results in document-level multi-aspect sentiment classiﬁca-
tion.

HGRUN is the basic form of HUARN without user, aspect and overall rating.
HARN is a variant of HUARN, which abandons user information from HUARN.

4.3 Results

We use Accuracy and Mean Squared Error (MSE) as the evaluation metrics, and the results are shown
in Table 3. For heuristic methods, we can see that Majority performs very poor because it does not

4Sample keywords for service are service, food, breakfast, and buffet.

Models

Majority
OverallRatingSame
MajOverallRating
SVM
NBoW
CNN
HAN
MHCNN
MHAN
HGRUN
DMSCMC
HARN
HUARN

TripOUR

TripDMS

Accuracy↑ MSE↓ Accuracy↑ MSE↓
2.549†
3.446
3.273
1.963†
1.808†
1.456†
1.301†
1.398†
1.210†
1.303
1.083†
0.923
N/A

0.2389†
0.2012
0.2414
0.3526†
0.3909†
0.4335†
0.4468†
0.4379†
0.4494†
0.4435
0.4656†
0.4821*
N/A

0.3850
0.3074
0.3487
0.4635
0.4865
0.5054
0.5123
0.5108
0.5419
0.5392
0.5549
0.5815*
0.6070*

0.954
1.705
1.536
1.025
0.912
0.752
0.705
0.712
0.629
0.635
0.583
0.528
0.514

Table 3: Document-level multi-aspect sentiment classiﬁcation on our datasets. Our full model is
HUARN. The best performances in bold. “†” indicates that the result is reported from (Yin et al., 2017).
“*” indicates that the model signiﬁcantly outperforms DMSCMC. Statistical signiﬁcance testing has
been performed using paired t-test with p < 0.05.

capture any text information. OverallRatingSame and MajOverallRating are also very poor, even
though overall rating has strong correction with aspect ratings, it is not enough to decide aspect ratings
only based on it.

Compared with SVM, NBoW achieves higher accuracy by at least 2.3% in both datasets, which shows
that embedding features are more effective than unigram and bigram features on these two datasets.
When applying more complex neural networks (such as CNN and HAN), the model can achieve higher
accuracy by at least 1.5% in both datasets compared with NBoW. Additionally, we observe that the multi-
task learning and hierarchical architecture are beneﬁcial for neural networks. Performance on MHAN
and MHCNN are slightly better than HAN and CNN. Beyond that, we also ﬁnd attention mechanism is
useful. The only difference between MHAN and HGRUN is that MHAN uses attention mechanism to
obtain sentence and document representations while HGRUN utilizes an average pooling layer, which
results in the performance of MHAN is better than HGRUN. After obtaining aspect-aware representation
for the document, DMSCMC achieves best results and outperforms other baselines.

Compared to DMSCMC, HARN achieves improvements of 2.7% and 1.7% on TripOUR and
TripDMS respectively, which shows that the incorporation of overall rating and aspect attention helps
build up more discriminative representation. Moreover, when incorporating user information, our full
model (HUARN) can achieve improvements of 5.3% compared with DMSCMC on TripOUR5, which
shows user preference can beneﬁt the document-level multi-aspect sentiment classiﬁcation task.

5 Discussions

In this section, we ﬁrst give some discussions about the effects of users, aspects and overall ratings on
predicting aspect ratings, and then show case study for attention results and visualize user embeddings.

5.1 Effects of Users, Overall Ratings and Aspects

Users, overall ratings and aspects are three kinds of information in HUARN. We present the effects of
users, overall ratings, and aspects on document-level multi-aspect sentiment classiﬁcation in Table 4.
From the table, we can observe that: (1) Compared with user-agnostic models (line 1-4), user-aware
models (line 5-8) can achieve improvements of 2.2%, 2.5%, 1.2% and 2.5% in TripOUR, which shows

5Since there is no user information in TripDMS, we can only compare DMSCMC with HARN.

No.

1
2
3
4
5
6
7
8

Different information

TripOUR

TripDMS

–
(cid:33)

–
–
(cid:33)
(cid:33)

User OverallRating Aspect Accuracy↑ MSE↓ Accuracy↑ MSE↓
1.303
1.256
1.093
0.923
N/A
N/A
N/A
N/A

0.5392
0.5514
0.5719
0.5815
0.5640
0.5764
0.5839
0.6070

0.4435
0.4566
0.4740
0.4821
N/A
N/A
N/A
N/A

0.635
0.599
0.555
0.528
0.619
0.581
0.560
0.514

–
–
–
–
(cid:33)
(cid:33)
(cid:33)
(cid:33)

–
–
(cid:33)
(cid:33)

–
(cid:33)

–
(cid:33)

–
(cid:33)

Table 4: Effects of user, overall rating and aspect on document-level multi-aspect sentiment classiﬁca-
tion. Each line represents a variant of HUARN, where “(cid:33)” denotes the variant considers the speciﬁc
information, while “–” denotes not. For example, model in line 1 means HUARN abandons these three
kinds of information and degenerates into HGRUN.

that model encoding user information can obtain user-aware document representation and is more suit-
able for document-level multi-aspect sentiment classiﬁcation. (2) Compared with models without overall
rating information (line 1, 2, 5 and 6), models considering overall ratings (line 3, 4, 7, 8) can obtain 3.2%
(3.1%), 3.0% (2.6%), 1.9% (N/A) and 3.1% (N/A) improvements in accuracy in both datasets, which
indicates overall rating information is useful for building more discriminative document representation
and helpful for predicting aspect ratings. (3) Compared with models without aspect information (line 1,
3, 5 and 7), aspect-based models (line 2, 4, 6, 8) can obtain 1.2% (1.3%), 1.0% (0.8%), 1.2% (N/A) and
2.4% (N/A) improvements in accuracy in both datasets. It shows that aspect information is useful for
building aspect-aware document representation and helpful for predicting aspect ratings. (4) After users,
overall ratings, and aspects being considered jointly, our model obtains the best performance.

5.2 Visualization of User Embeddings

Figure 4: t-SNE visualization of user embeddings for different aspects in TripOUR. Blue square and red
triangle represent users are “High Score” users and “Low Score” users, respectively.

As different users have different aspect rating preferences and HUARN imports user embedding to
consider users, we identify whether such personalized information are encoded in user embedding. To
this end, we ﬁrst rank all users according to their average score in the training set for each aspect.
Then the top 100 users are labeled as “High Score” users and bottom 100 users are labeled as “Low
Score” users. Due to space limit, here we only show embeddings of users in four aspects (service, value,
cleanliness, location), which are top frequently scored by all users, in Figure 4. We ﬁnd “High Score”
users and “Low Score” users are separated apparently. The visualization shows that user embedding
learned by HUARN can encode personalized traits in scoring different aspects.

5.3 Case Study for Attention Results

To show the ability that HUARN captures user preference and aspect semantic meanings, we take one
sentence from TripOUR as example. The content of the sentence is “The food is good, but the price is

Figure 5: The attention visualization of words. Dark color means higher weight. (a), (b) and (c) show
word-level attention weights of MHAN, HARN and HUARN.

very expensive”, in which “good” is a general sentiment word and can be used to describe many aspects
(such as service, room and so on), while “expensive” is a aspect-speciﬁc sentiment word which only
applies to describe value. We visualize attention weights of the sentence in Figure 5.

From Figure 5(a), we can ﬁnd that considering word-level attention, MHAN distinguishes sentiment
words and non-sentiment words, however it is hard to identify the sentiment word is a general one or
an aspect-speciﬁc one. After adding aspect attention over word-level representations, HARN can distin-
guish these two kinds of sentiment words (Figure 5(b)). The attention weights of “good” for different
aspects are very close, while the attention weights of “expensive” for different aspects are different, and
the maximum is value. When adding user information into word-level attention, HUARN can also treat
“good” differently. From ﬁgure 5(c), we can ﬁnd attention weights of “good” are different for different
aspects, where weights for service, check in and room are higher than weights for other aspects. we
check all reviews of the sample review’s author and ﬁnd that he/she often (more than 80%) use “good”
to describe service, check in and room.

5.4 Error Analysis

Figure 6: Examples of error cases. GT means ground trouth and P means prediction result of HUARN.
Sentences in review text with darker color means higher attention weight for the sentence.

We analyze error cases in the experiments. Some examples of error cases are shown in Figure 6. We
can ﬁnd that HUARN is hard to select important sentences for aspects. For example, sentence ”the rooms
are large and the bed comfy.” and sentence ”however the carpet should have been replaced long time.” in
the ﬁrst sample in Figure 6 are all important to decide the rating of aspect room. However, HUARN pays
more attention to the former sentence when predicting rating of room and obtains the wrong result.

Based on the literature study, we ﬁnd that Yin et al. (2017) uses iterative attention models to build up
aspect-speciﬁc representation for review. It may alleviate this problem. We leave how to encode user and
overall rating information into DMSCMC as our future work.

6 Related Work

Multi-aspect sentiment classiﬁcation is an extensively studied task in sentiment analysis (Pang and Lee,
2008; Liu, 2012). Lu et al. (2011) propose Segmented Topic Model to model document and extract
features, then exploit support vector regression to predict aspect ratings based on these features. McAuley
et al. (2012) add a dependency term in ﬁnal multi-class SVM objective to consider the correction between
aspects. Many other studies (Titov and McDonald, 2008; Wang et al., 2010; Wang et al., 2011; Diao
et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multi-
aspect sentiment classiﬁcation as a subproblem by utilizing heuristic based methods or topic models.
However, these approaches often rely on strict assumptions about words and sentences, for example,
word syntax has been used to distinguish aspect word or sentiment word, or appending an speciﬁc aspect
to a sentence. Another related problem is called aspect-level sentiment classiﬁcation (Pontiki et al.,
2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al.
(2016) and Tang et al. (2016) employ attention-based LSTM and deep memory network for aspect-level
sentiment classiﬁcation, respectively. However, the task is sentence level. Document-level sentiment
classiﬁcation (Li and Zong, 2008; Li et al., 2010; Li et al., 2013; Xia et al., 2015; Yang et al., 2016) is
also a related research ﬁeld because we can treat single aspect sentiment classiﬁcation as an individual
document classiﬁcation task. However, they did not consider multiple aspects in a document.

In addition to these methods, the work of Yin et al. (2017) is the most related to ours, which focuses
on using iterative attention mechanism to build discriminative aspect-aware representation to perform
document-level multi-aspect sentiment classiﬁcation. However, it ignores the inﬂuences of users and
overall ratings on aspect ratings. Actually, many studies (Tang et al., 2015b; Tang et al., 2015c; Chen
et al., 2016; Li et al., 2016; Dou, 2017) have shown considering user preference can boost the perfor-
mance of Document-level Sentiment Classiﬁcation. Partially inspired by these approaches, we propose
HUARN to consider users, overall ratings and aspects jointly into document-level multi-aspect sentiment
classiﬁcation. Compared with these user-aware approaches, HUARN has some differences: (1) They do
not consider aspects. (2) Although Chen et al. (2016) and Dou (2017) embedding user to consider user-
text consistency to perform sentiment classiﬁcation, they ignore user-rating consistency. (3) Tang et al.
(2015b; 2015c) embed user in a matrix and build user-speciﬁc representation by a convolutional neural
network structure. However, it is hard to train with limited reviews for user matrix. Our motivation is
that (1) Aspect information is very useful for selecting informative words and sentences and building
up aspect-speciﬁc representation for document-level multi-aspect sentiment classiﬁcation, therefore, we
add aspect attention into our model. (2) Compared with user-text consistency, user-rating consistency
describes the correlation between users and ratings more directly. (3) Embedding user in a vector and
using attention mechanism to build user-speciﬁc representation is an effective way to consider users.
User embedding is enough to encode the relation between user and rating in The most important is that
it is easy to train.

7 Conclusion and Future work

In this paper, we present Hierarchical User Aspect Rating Network (HUARN) to incorporate user pref-
erence and overall rating into document-level multi-aspect sentiment classiﬁcation. HUARN encodes
different kinds of information (word, sentence and document) into a hierarchical structure. To consid-
er user preference and overall rating, HUARN introduces user information as attention over word-level
representation and sentence-level representation, and then generates review representation by combining
user, overall rating and document information. Extensive experiments show that our model outperforms
state-of-the-art methods signiﬁcantly. In the future, we will study how to encode user and overall rating
information into DMSCMC.

Acknowledgements

We thank Xiaomian Kang and Yang Zhao for valuable discussions. We also thank the anonymous re-
viewers for their suggestions. The research work descried in this paper has been supported by the Natural
Science Foundation of China under Grant No. 61333018 and 61673380.

References

Rich Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75.

Huimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin, and Zhiyuan Liu. 2016. Neural sentiment classiﬁcation

with user and product attention. In Proceedings of EMNLP.

Ronan Collobert, Jason Weston, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language

processing (almost) from scratch. Journal of Machine Learning Research, 12(1):2493–2537.

Qiming Diao, Minghui Qiu, Chao Yuan Wu, Alexander J. Smola, Jing Jiang, and Chong Wang. 2014. Jointly
modeling aspects, ratings and sentiments for movie recommendation (jmars). In Proceedings of KDD, pages
193–202.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network

for target-dependent twitter sentiment classiﬁcation. In Proceedings of ACL, pages 49–54.

Zi-Yi Dou. 2017. Capturing user and product information for document level sentiment analysis with deep
memory network. In Proceedings of EMNLP, pages 532–537, Copenhagen, Denmark, September. Association
for Computational Linguistics.

Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. Eprint Arxiv.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.

Shoushan Li and Chengqing Zong. 2008. Multi-domain sentiment classiﬁcation. In ACL 2008, Proceedings of
the 46th Annual Meeting of the Association for Computational Linguistics, June 15-20, 2008, Columbus, Ohio,
USA, Short Papers, pages 257–260.

Shoushan Li, Chu-Ren Huang, Guodong Zhou, and Sophia Yat Mei Lee. 2010. Employing personal/impersonal
views in supervised and semi-supervised sentiment classiﬁcation. In ACL 2010, Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden, pages 414–423.

Shoushan Li, Yunxia Xue, Zhongqing Wang, and Guodong Zhou. 2013. Active learning for cross-domain senti-

ment classiﬁcation. In Proceedings of IJCAI, pages 2127–2133.

Junjie Li, Haitong Yang, and Chengqing Zong. 2016. Sentiment classiﬁcation of social media text considering user
attributes. In Natural Language Understanding and Intelligent Applications - 5th CCF Conference on Natural
Language Processing and Chinese Computing, NLPCC 2016, and 24th International Conference on Computer
Processing of Oriental Languages, ICCPOL 2016, Kunming, China, December 2-6, 2016, Proceedings, pages
583–594.

Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis lectures on human language technologies,

5(1):1–167.

Bin Lu, Myle Ott, Claire Cardie, and Benjamin K Tsou. 2011. Multi-aspect sentiment analysis with topic models.

In Proceedings of ICDM Workshops, pages 81–88.

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-task sequence to

sequence learning. In Proceedings of ICLR, San Juan, Puerto Rico, May.

Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language processing toolkit. In ACL (System Demonstrations), pages 55–60.

Julian Mcauley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes and attributes from multi-aspect

reviews. In Proceedings of ICDM, pages 1020–1025.

Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information

retrieval, 2(1-2):1–135.

Nikolaos Pappas and Andrei Popescu-Belis. 2014. Explaining the stars: Weighted multiple-instance learning for
aspect-based sentiment analysis. In Proceedings of EMNLP, pages 455–466. Association for Computational
Linguistics.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Man-
andhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. Proceedings of International Workshop
on Semantic Evaluation at, pages 27–35.

Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad AL-
Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphee De Clercq, Veronique Hoste, Marianna Apidi-
anaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, N´uria Bel, Salud Mar´ıa Jim´enez-Zafra, and
In Proceedings of the 10th
G¨uls¸en Eryi˘git. 2016. Semeval-2016 task 5: Aspect based sentiment analysis.
International Workshop on Semantic Evaluation (SemEval-2016), pages 19–30. Association for Computational
Linguistics.

Kim Schouten and Flavius Frasincar. 2016. Survey on aspect-level sentiment analysis. IEEE Transactions on

Knowledge and Data Engineering, 28(3):813–830.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christo-
pher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceed-
ings of EMNLP.

Duyu Tang, Bing Qin, and Ting Liu. 2015a. Document modeling with gated recurrent neural network for sentiment
In Proceedings of EMNLP, pages 1422–1432, Lisbon, Portugal, September. Association for

classiﬁcation.
Computational Linguistics.

Duyu Tang, Bing Qin, and Ting Liu. 2015b. Learning semantic representations of users and products for document

level sentiment classiﬁcation. In Proceedings of ACL, pages 1014–1023, July.

Duyu Tang, Bing Qin, Yuekui Yang, and Yuekui Yang. 2015c. User modeling with neural network for review

rating prediction. In Proceedings of IJCAI, pages 1340–1346.

Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect level sentiment classiﬁcation with deep memory network.

arXiv preprint arXiv:1605.08900.

Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In

Proceedings of ACL, pages 308–316. Association for Computational Linguistics.

Zhiqiang Toh and Jian Su. 2016. Nlangp at semeval-2016 task 5: Improving aspect based sentiment analysis
In Proceedings of the 10th International Workshop on Semantic Evaluation

using neural network features.
(SemEval-2016), pages 282–288. Association for Computational Linguistics.

Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect rating analysis on review text data: a rating

regression approach. In Proceedings of KDD, pages 783–792.

Hongning Wang, Yue Lu, and Cheng Xiang Zhai. 2011. Latent aspect rating analysis without aspect keyword

supervision. In Proceedings of KDD, pages 618–626.

Yequan Wang, Minlie Huang, Li Zhao, and Xiaoyan Zhu. 2016. Attention-based lstm for aspect-level sentiment

classiﬁcation. In Proceedings of EMNLP.

Rui Xia, Chengqing Zong, and Shoushan Li. 2011. Ensemble of feature sets and classiﬁcation algorithms for

sentiment classiﬁcation. Information Sciences, 181(6):1138–1152.

Rui Xia, Feng Xu, Chengqing Zong, Qianmu Li, Yong Qi, and Tao Li. 2015. Dual sentiment analysis: Considering

two sides of one review. IEEE Transactions on Knowledge and Data Engineering, 27(8):2120–2133.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hierarchical

attention networks for document classiﬁcation. In HLT-NAACL, pages 1480–1489.

Yichun Yin, Yangqiu Song, and Ming Zhang. 2017. Document-level multi-aspect sentiment classiﬁcation as
machine comprehension. In Proceedings of EMNLP, pages 2044–2054. Association for Computational Lin-
guistics.

