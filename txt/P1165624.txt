9
1
0
2
 
v
o
N
 
6
2
 
 
]

V
C
.
s
c
[
 
 
2
v
6
5
9
6
0
.
9
0
9
1
:
v
i
X
r
a

PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable
Makeup Transfer

Wentao Jiang1, Si Liu1, Chen Gao2,4, Jie Cao3,4, Ran He3,4, Jiashi Feng5, Shuicheng Yan6
1School of Computer Science and Engineering, Beihang University
2Institute of Information Engineering, Chinese Academy of Sciences
3Institute of Automation, Chinese Academy of Sciences
4University of Chinese Academy of Sciences 5National University of Singapore 6YITU Tech
{jiangwentao, liusi}@buaa.edu.cn, gaochen@iie.ac.cn, jie.cao@cripac.ia.ac.cn
rhe@nlpr.ia.ac.cn, elefjia@nus.edu.sg, shuicheng.yan@yitu-inc.com

Figure 1. Our model allows users to control both the shade of makeup and facial parts to transfer. The ﬁrst row on the left shows the results
of only transferring partial makeup style from the reference. The second row shows the results with controllable shades. Moreover, our
method can perform makeup transfer between images that have different poses and expressions, as shown on the right part of the ﬁgure.
Best viewed in color.

Abstract

In this paper, we address the makeup transfer task, which
aims to transfer the makeup from a reference image to a
source image. Existing methods have achieved promis-
ing progress in constrained scenarios, but transferring be-
tween images with large pose and expression differences is
still challenging. Besides, they cannot realize customizable
transfer that allows a controllable shade of makeup or spec-
iﬁes the part to transfer, which limits their applications. To
address these issues, we propose Pose and expression robust
Spatial-aware GAN (PSGAN). It ﬁrst utilizes Makeup Dis-
till Network to disentangle the makeup of the reference im-
age as two spatial-aware makeup matrices. Then, Attentive
Makeup Morphing module is introduced to specify how the
makeup of a pixel in the source image is morphed from the
reference image. With the makeup matrices and the source
image, Makeup Apply Network is used to perform makeup
transfer. Our PSGAN not only achieves state-of-the-art re-
sults even when large pose and expression differences exist

1

but also is able to perform partial and shade-controllable
makeup transfer. We also collected a dataset containing fa-
cial images with various poses and expressions for evalua-
tions.

1. Introduction

We explore the makeup transfer task, which aims to
transfer the makeup from an arbitrary reference image to a
source image. It is widely demanded in many popular por-
trait beautifying applications. Most existing makeup trans-
fer methods [17, 3, 2, 10] are based on Generative Adversar-
ial Networks (GANs) [9]. They generally use face parsing
maps and/or facial landmarks as a preprocessing step to fa-
cilitate the subsequent processing and adopt the framework
of CycleGAN [31] which is trained on unpaired sets of im-
ages, i.e., non-makeup images and with-makeup images.

However, existing methods mainly have two limitations.
Firstly, they only work well on frontal facial images with
neutral expression since they lack a specially designed mod-

ule to handle the misalignment of images and overﬁt on
frontal images. While in practical applications, an ideal
method should be pose and expression robust, which is
able to generate high-quality results even if source images
and reference images show different poses and expressions.
Secondly, the existing methods cannot perform customiz-
able makeup transfer since they encode makeup styles into
low dimension vectors which lose the spatial information.
An ideal makeup transfer method need be capable of realiz-
ing partial and shade-controllable makeup transfer. Partial
transfer indicates transferring the makeup of speciﬁed facial
regions separately, e.g., eye shadows or lipstick. Shade-
controllable transfer means the shade of the transferred
makeup can be controllable from light to heavy.

To solve these challenges, we propose a novel Pose and
expression robust Spatial-aware GAN, which consists of a
Makeup Distill Network (MDNet), an Attentive Makeup
Morphing (AMM) module and a Makeup Apply Network
(MANet). Different from the previous approaches that sim-
ply input two images into the network or recombine makeup
latent code and identity latent code to perform transfer,
we design PSGAN to transfer makeup through scaling and
shifting the feature map for only once, inspired by style
transfer methods [13, 5]. Comparing with general style
transfer, makeup transfer is more difﬁcult since the human
perception system is very sensitive to the artifacts on faces.
Also, makeup styles contain subtle details in each facial
region instead of general styles. To this end, we propose
MDNet to disentangle the makeup from the reference image
into two makeup matrices, i.e., the coefﬁcient matrix γ and
bias matrix β which both have the same spatial dimensions
with visual features. These matrices embed the makeup in-
formation and serve as the shifting and scaling parameters.
Then, γ and β are morphed and adapted to the source image
by the AMM module which calculates an attentive matrix A
to produce adapted makeup matrices γ(cid:48) and β(cid:48). The AMM
module utilizes the face parsing maps and facial landmarks
to build the pixel-wise correspondences between source im-
ages and reference images, which solves the misalignment
of faces. Finally, the proposed MANet conducts makeup
transfer through applying pixel-wise multiplication and ad-
dition on visual features using γ(cid:48) and β(cid:48).

Since the makeup style has been distilled in a spatial-
aware way, partial transfer can be realized by applying
masks pixel-wisely according to the face parsing results.
For example, in the top left panel of Figure 1, the lip
gloss, skin and eye shadow can be individually transferred
from the reference image to the source image. Shade-
controllable transfer can be realized through multiplying
the weights of makeup matrices by coefﬁcient within [0, 1].
As shown in the bottom left panel of Figure 1, where the
makeup shade is increasingly heavier. Moreover, the novel
AMM module effectively assists the generating of pose and

expression robust results, as shown in the right part of Fig-
ure 1. We also directly apply transfer to every frame of fa-
cial videos and still get nice and consistent results. With the
three novel components, PSGAN satisﬁes the requirements
we pose for an ideal customizable makeup transfer method.

We make the following contributions in this paper:

• To our best knowledge, PSGAN is the ﬁrst to si-
multaneously realize partial, shade-controllable, and
pose/expression robust makeup transfer, which facil-
itates the applications in the real-world environment.

• A MDNet is introduced to disentangle the makeup
from the reference image as two makeup matrices. The
spatial-aware makeup matrices enable the ﬂexible par-
tial and shade-controllable transfer.

• An AMM module that adaptively morphs the makeup
matrices to source images is proposed, which enables
pose and expression robust transfer.

• A new Makeup-Wild dataset containing images with
diverse poses and expressions is collected for better
evaluations.

2. Related Work

2.1. Makeup Transfer

Makeup transfer has been studied a lot these years
[25, 11, 16, 20, 19, 1]. BeautyGAN [17] ﬁrst proposed a
GAN framework with dual input and output for makeup
transfer and removal simultaneously. They also introduced
a makeup loss that matches the color histogram in different
parts of faces for instance-level makeup transfer. Beauty-
Glow [3] proposed a similar idea on the Glow framework
and decomposed makeup component and non-makeup com-
ponent. PairedCycleGAN [2] employed an additional dis-
criminator to guide makeup transfer using pseudo trans-
ferred images generated by warping the reference face to
the source face. LADN [10] leveraged additional multiple
overlapping local discriminators for dramatic makeup trans-
fer. However, the above approaches often fail on transfer-
ring in-the-wild images and cannot adjust transfer precisely
and partially, which limits their applications, such as the
makeup transfer in videos.

2.2. Style Transfer

Style transfer has been investigated extensively [7, 6, 14,
21, 24]. [8] proposed to derive image representations from
CNN, which can be separated and recombined to synthesize
images. Some methods are developed to solve the fast style
transfer problem. [5] found the vital role of normalization
in style transfer networks and achieved fast style transfer by
the conditional instance normalization. While their methods

2

(A) Illustration of PSGAN framework. MDNet distills makeup matrices from the reference image. AMM module applies the
Figure 2.
adapted makeup matrices to the output feature map of the third bottleneck of MANet to achieve makeup transfer. (B) Illustration of AMM
module. Green blocks with 136 (68×2) channels indicate relative position features of the pixels, which are then concatenated with C-
channel visual features. Thus, the attention map is computed for each pixel in the source image through the similarity of relative positions
and visual appearances. The adapted makeup matrices γ(cid:48) and β(cid:48) are produced by the AMM module, which are then multiplied and added
to feature maps of MANet element-wisely. The orange and the gray blocks in the ﬁgure indicate visual features with makeup and without
makeup. (C) Attention maps for a speciﬁc red point in the source image. Note that we only calculate attentive values for pixels that belong
to the same facial region. Thus, there are no response values on the lip and eye of the reference image.

can only transfer a ﬁxed set of styles and cannot adapt to ar-
bitrary new styles. Then, [13] proposed adaptive instance
normalization (AdaIN) that aligns the mean and variance
of the content features with those of the style features and
achieved arbitrary style transfer. Here, we propose spatial-
aware makeup transfer for each pixel rather than transfer-
ring a general style from the reference.

2.3. Attention Mechanism

Attention mechanism has been utilized in many areas
[28, 22, 12, 23].
[26] proposed the attention mechanism
in the natural language processing area by leveraging a self-
attention module to compute the response at a position in
a sequence (e.g., a sentence) by attending to all positions
and taking their weighted average in an embedding space.
[27] proposed the non-local network, which is to compute
the response at a position as a weighted sum of the features
at all positions.
Inspired by these works, we explore the
application of attention module by calculating the attention
between two feature maps. Unlike the non-local network
that only considers visual appearance similarities, our pro-

posed AMM module computes the weighted sum of another
feature map by considering both visual appearances and lo-
cations.

3. PSGAN

3.1. Formulation

Let X and Y be the source image domain and the refer-
ence image domain. Also, we utilize {xn}n=1,...,N , xn ∈
X and {ym}m=1,...,M , ym ∈ Y to represent the examples
of two domains respectively. Note that paired datasets are
not required. That is, the source and reference images have
different identities. We assume x is sampled from X ac-
cording to the distribution PX and y is sampled from Y
according to the distribution PY . Our proposed PSGAN
learns a transfer function G : {x, y} → ˜x, where the trans-
ferred image ˜x has the makeup style of the reference image
y and preserves the identity of the source image x.

3.2. Framework

Overall. The framework of PSGAN is shown in Figure
2 (A). Mathematically, it is formulated as ˜x = G(x, y). It

3

can be divided into three parts. 1) Makeup distill network.
MDNet extracts the makeup style from the reference image
y and represents it as two makeup matrices γ and β, which
have the same height and width as the feature map. 2) At-
tentive makeup morphing module. Since source images and
reference images may have large discrepancies in expres-
sions and poses, the extracted makeup matrices cannot be
directly applied to the source image x. We then propose an
AMM module to morph the two makeup matrices to two
new matrices γ(cid:48) and β(cid:48) which are adaptive to the source
image by considering the similarities between pixels of the
source and reference. 3) Makeup apply network. The adap-
tive makeup matrices γ(cid:48) and β(cid:48) are applied to the bottleneck
of the MANet to perform makeup transfer with pixel-level
guidance by element-wise multiplication and addition.

Makeup distill network.

The MDNet utilizes the
encoder-bottleneck architecture used in [4] without the de-
coder part. It disentangles the makeup related features, e.g.,
lip gloss, eye shadows, from the intrinsic facial features,
e.g., facial shape, the size of eyes. The makeup related
features are represented as two makeup matrices γ and β,
which are used to transfer the makeup by pixel-level opera-
tions. As shown in Figure 2 (B), the output feature map of
MDNet Vy ∈ RC×H×W is fed into two 1 × 1 convolution
layers to produce γ ∈ R1×H×W and β ∈ R1×H×W , where
C, H and W are the number of channels, height and width
of the feature map.

Attentive makeup morphing module. Since the source
and reference images may have different poses and expres-
sions, the obtained spatial-aware γ and β cannot be applied
directly to the source image. The proposed AMM module
calculates an attentive matrix A ∈ RHW ×HW to specify
how a pixel in the source image x is morphed from the pix-
els in the reference image y, where Ai,j indicates the atten-
tive value between the i-th pixel xi in image x and the j-th
pixel yj in image y.

Intuitively, makeup should be transferred between the
pixels with similar relative positions on the face, and the at-
tentive values between these pixels should be high. For ex-
ample, the lip gloss region of the transferred result ˜x should
be sampled from the corresponding lip gloss region of the
reference image y. To describe the relative positions, we
take the facial landmarks as anchor points. The relative po-
sition feature of pixel xi is represented by pi ∈ R136, which
is reﬂected in the differences of coordinates between pixel
xi and 68 facial landmarks, calculated by

pi = [f (xi) − f (l1), f (xi) − f (l2), . . . , f (xi) − f (l68),
g(xi) − g(l1), g(xi) − g(l2), . . . , g(xi) − g(l68)],
(1)

where f (·) and g(·) indicate the coordinates on x and y
axes, li indicates the i-th facial landmark obtained by the
2D facial landmark detector [29], which serves as the an-

chor point when calculating pi.
In order to handle faces
that occupy different sizes in images, we divide p by its
two-norm (i.e., p
(cid:107)p(cid:107) ) when calculating the attentive matrix.
Moreover, to avoid unreasonable sampling pixels with
similar relative positions but different semantics, we also
consider the visual similarities between pixels (e.g., xi and
yj), which are denoted as the similarities between vi and
vj that extracted from the third bottleneck of MANet and
MDNet respectively. To make the relative position to be
the primary concern, we multiply the visual features by a
weight when calculating A. Then, the relative position fea-
tures are resized and concatenated with the visual features
along the channel dimension. As Figure 2 (B) shows, the
attentive value Ai,j is computed by considering the similar-
ities of both visual appearances and relative positions via

(cid:17)

(cid:16)

,

(cid:80)

exp

I(mi

j exp

(cid:107)pj (cid:107) ]

Ai,j =

[wvi, pi

I(mi
(cid:17)

(cid:107)pi(cid:107) ]T [wvj, pj

[wvi, pi
(cid:16)

(cid:107)pj (cid:107) ]
(cid:107)pi(cid:107) ]T [wvj, pj

x = mj
y)
x = mj
y)
(2)
where [·, ·] denotes the concatenation operation, v ∈ RC
and p ∈ R136 indicate the visual features and relative posi-
tion features, w is the weight for visual features. I(·) is an
indicator function whose value is 1 if the inside formula is
true, mx, my ∈ {0, 1, . . . , N − 1}H×W are the face pars-
ing map of source image x and reference image y, where
N stands for the number of facial regions (N is 3 in our
experiments including eyes, lip and skin), mi
y indi-
cate the facial regions that xi and xj belong to. Note that
we only consider the pixels belonging to same facial region,
i.e., mi

y , by applying indicator function I(·).
Given a speciﬁc point that marked in red in the lower-left
corner of the nose in the source image, the middle image of
Figure 2 (C) shows its attention map by reshaping a spe-
ciﬁc row of the attentive matrix Ai,: ∈ R1×HW to H × W .
We can see that only the pixels around the left corner of the
nose have large values. After applying softmax, attentive
values become more gathered. This veriﬁes that our pro-
posed AMM module is able to locate semantically similar
pixels to attend.

x and mj

x = mj

We multiply attentive matrix A by the γ and β, and get
the morphed makeup matrices γ(cid:48) and β(cid:48). More speciﬁcally,
the matrices γ(cid:48) and β(cid:48) are computed by

γ(cid:48)
i =

β(cid:48)
i =

(cid:88)

j
(cid:88)

j

Ai,jγj;

Ai,jβj,

(3)

where i and j are the pixel index of x and y. After that,
the matrix γ(cid:48) ∈ R1×H×W and β(cid:48) ∈ R1×H×W are dupli-
cated and expanded along the channel dimension to produce
the makeup tensors Γ(cid:48) ∈ RC×H×W and B(cid:48) ∈ RC×H×W ,
which will be the input of MANet.

4

Makeup apply network. MANet utilizes a similar
encoder-bottleneck-decoder architecture as [4]. As shown
in Figure 2 (A), the encoder part of MANet shares the
same architecture with MDNet, but they do not share pa-
rameters. In the encoder part, we use instance normaliza-
tions that have no afﬁne parameters that make the feature
map to be a normal distribution.
In the bottleneck part,
the morphed makeup tensors Γ(cid:48) and B(cid:48) obtained by the
AMM module are applied to the source image feature map
Vx ∈ RC×H×W . The activation values of the transferred
feature map Vx

(cid:48) are calculated by

Vx

(cid:48) = Γ(cid:48)Vx + B(cid:48).

(4)

Eq. (4) gives the function of makeup transfer. The updated
(cid:48) is then fed to the subsequent decoder part
feature map Vx
of MANet to produce the transferred result.

3.3. Objective Function

Adversarial loss. We utilize two discriminators DX and
DY for the source image domain X and the reference im-
age domain Y , which try to discriminate between generated
images and real images and thus help the generators synthe-
size realistic outputs. Therefore, the adversarial loss Ladv
D ,
Ladv
G for discriminator and generator are computed by

D = −Ex∼PX [log DX (x)] − Ey∼PY [log DY (y)]
Ladv
− Ex∼PX ,y∼PY [log (1 − DX (G(y, x)))]
− Ex∼PX ,y∼PY [log (1 − DY (G(x, y)))]
G = − Ex∼PX ,y∼PY [log (DX (G(y, x)))]
Ladv
− Ex∼PX ,y∼PY [log (DY (G(x, y)))]

(5)
Cycle consistency loss. Due to the lack of triplets data
(source image, reference image, and transferred image), we
train the network in an unsupervised way. Here, we intro-
duce the cycle consistency loss proposed by [31]. We use
the L1 loss to constrain the reconstructed images and deﬁne
the cycle consistency loss Lcyc
G as

Lcyc
G = Ex∼PX ,y∼PY [(cid:107)G(G(x, y), x) − x(cid:107)1]
+ Ex∼PX ,y∼PY [(cid:107)G(G(y, x), y) − y(cid:107)1] .

(6)

Perceptual loss. When transferring the makeup style,
the transferred image is required to preserve personal iden-
tity. Instead of directly measuring differences at pixel-level,
we utilize a VGG-16 model pre-trained on ImageNet to
compare the activations of source images and generated im-
ages in the hidden layer. Let Fl(·) denote the output of the
l-th layer of VGG-16 model. We introduce the perceptual
loss Lper
G to measure their differences using L2 loss:

Lper
G = Ex∼PX ,y∼PY [(cid:107)Fl(G(x, y)) − Fl(x)(cid:107)2]
+ Ex∼PX ,y∼PY [(cid:107)Fl(G(y, x)) − Fl(y)(cid:107)2] .

(7)

G

Lmake
G

Makeup loss. To provide coarse guidance for makeup
transfer, we utilize the makeup loss proposed by [17].
Speciﬁcally, we perform histogram matching on the same
facial regions of x and y separately and then recombine the
results, denoted as HM (x, y). As a kind of pseudo ground
truth, HM (x, y) preserves the identity of x and has a simi-
lar color distribution with y. Then we calculate the makeup
loss Lmake

as coarse guidance by

= Ex∼PX ,y∼PY [(cid:107)G(x, y) − HW (x, y)(cid:107)2]
+ Ex∼PX ,y∼PY [(cid:107)G(y, x) − HW (y, x)(cid:107)2] .

(8)

Total loss. The loss LD and LG for discriminator and

generator of our approach can be expressed as

LD = λadvLadv
D

G + λcycLcyc

LG = λadvLadv

G + λmakeLmake
,
(9)
where λadv, λcyc, λper, λmake are the weights to balance
the multiple objectives.

G + λperLper

G

4. Experiments

4.1. Data Collection

Since the existing makeup datasets only consist of frontal
facial images with neutral expressions, we collect a new
Makeup-Wild dataset that contains facial images with vari-
ous poses and expressions as well as complex backgrounds
to test methods in the real-world environment. We col-
lect data from the Internet and then manually remove im-
ages with frontal face or neutral expression. After that, we
crop and resize the images to be 256 × 256 resolution with-
out alignment. Finally, 403 with-makeup images and 369
non-makeup images are collected to form the Makeup-Wild
dataset.

4.2. Experimental Setting and Details

We train our network using the training part of the MT
(Makeup Transfer) dataset [17, 3] and test it on the test-
ing part of MT dataset and the Makeup-Wild dataset. MT
dataset contains 1, 115 non-makeup images and 2, 719 with-
makeup images which are mostly well-aligned, with the res-
olution of 361 × 361 and the corresponding face parsing
results. We follow the splitting strategy of [17] to form the
train/test set and conduct frontal face experiments in the test
set of MT dataset since the examples in the test set are well-
aligned frontal facial images. To further prove the effec-
tiveness of PSGAN for handling pose and expression dif-
ferences, we use the Makeup-Wild dataset as an extra test
set. Note that we only train our network using the training
part of the MT dataset for a fair comparison.

For all experiments, we resize the images to 256×256,
and utilize the relu 4 1 feature layer of VGG-16 for calcu-

5

Figure 3. Without AMM module, the makeup transfer results (the
3rd column) are bad due to pose and expression differences be-
tween source and reference images.

lating perceptual loss. The weights of different loss func-
tions are set as λadv = 1, λcyc = 10, λper = 0.005,
λmake = 1, and the weight for visual feature in AMM is
set to 0.01. We train the model for 50 epochs optimized by
Adam [15] with learning rate of 0.0002 and batch size of 1.

4.3. Ablation Studies

Attentive makeup morphing module.

In PSGAN,
AMM module morphs the distilled makeup matrices γ and
β to γ(cid:48), β(cid:48). It alleviates the pose and expression differences
between source and reference images. The effectiveness of
the AMM module is shown in Figure 3. In the ﬁrst row,
the pose of source and reference images are very different.
The bangs of the reference image are transferred to the skin
of the source image without AMM. By applying AMM, the
pose misalignment is well solved. A similar observation
can be found in the second row: the expressions of source
and reference images are smiling and neutral respectively,
while the lip gloss is applied to the teeth region without the
AMM module shown in the third column. After integrat-
ing AMM, lip gloss is applied to the lip region, bypassing
the teeth area. The experiments demonstrate that the AMM
module can specify how a pixel in the source image is mor-
phed from pixels of the reference instead of mapping the
makeup from the same location directly.

The weight of visual feature in calculating A. In the
AMM module, we calculate the attentive matrix A by con-
sidering both the visual features v and relative positions p
using Eq. (2). Figure 4 demonstrates that if only relative
positions are considered by setting the weight to zero, the
attentive maps in the second column are similar to a 2D
Gaussian distribution. In the ﬁrst column of Figure 4, the
red point on the skin of the source may wrongly receive
makeup from the nostrils area in the reference image (1st
row). The attention map also crosses the face boundary and
covers the earrings (2nd row) which is unreasonable. Be-
sides, larger weights will lead to scattered and unreasonable
attention maps, as shown in the last column. After con-

Figure 4. Given a red point on the skin, the corresponding attention
maps with different weights on visual features are shown. With-
out using visual features, attention maps fail to avoid nostrils (1st
row, 2nd column) and wrongly crosses the facial boundary (2nd
row, 2nd column). While a larger weight leads to scattered and
unreasonable attention maps.

Figure 5. Given the source image (2nd column), the transferred im-
ages (3rd column) are generated by transferring the lipstick from
reference 1 and other makeup from reference 2.

sidering the appearance feature appropriately by setting the
weight to 0.01, the attention maps focus more on the skin
and also bypass the nostrils as well as background.

4.4. Partial and Interpolated Makeup Transfer

y2, B(cid:48)

y1 as well as Γ(cid:48)

Since the makeup matrices γ and β are spatial-aware, the
partial and interpolated transfer can be realized during test-
ing. To achieve partial makeup generation, we compute the
new makeup matrices by weighting the matrices using the
face parsing results. Let x, y1, and y2 denote a source image
and two reference images. We can obtain Γ(cid:48)
y1,
B(cid:48)
y2 by feeding the images to MDNet.
In addition, we can obtain the face parsing mask mx of x
through the existing deep learning method [30]. Suppose
we want to mix the lipstick from y1 and other makeup from
y2, we can ﬁrst obtain the binary mask of the lip, denoted
x ∈ {0, 1}H×W . Then, PSGAN can realize partial
as ml
makeup transfer by assigning different makeup parameters
on different pixels. By modifying Eq. (4), the partial trans-
ferred feature map Vx

(cid:48) can be calculated by

x and Γ(cid:48)

x, B(cid:48)

Vx

(cid:48) = (ml

xΓ(cid:48)
y1

+(1−ml

x)Γ(cid:48)
y2

)Vx+(ml

xB(cid:48)
y1

+(1−ml

).

x)B(cid:48)
y2
(10)

6

Figure 6. Results of interpolated makeup styles. If only one reference is used, adjusting the shade of makeup can be realized (1st row). If
two references are used (1st column and last column), the makeup of the transferred images is gradually changing from reference 1 towards
reference 2 from left to right (2nd rows).

Figure 5 shows the results by mixing the makeup styles
from two references partially. The results on the third col-
umn recombine the makeup of lip from reference 1 and
other part of makeup from reference 2, which are natural
and realistic. Also, only transferring the lipstick from ref-
erence 1 and remain other parts unchanged can be achieved
by assigning x = y2. The new feature of partial makeup
makes PSGAN realize the ﬂexible partial makeup transfer.
Moreover, we can interpolate the makeup with two ref-
erence images by a coefﬁcient α ∈ [0, 1]. We ﬁrst get the
makeup tensors of two references y1 and y2, and then com-
pute the new parameters by weighting them with the coefﬁ-
cient α. The resulted feature map Vx

(cid:48) is calculated by

Vx

(cid:48) = (αΓ(cid:48)
y1

+ (1 − α)Γ(cid:48)
y2

)Vx + (αB(cid:48)
y1

+ (1 − α)B(cid:48)
).
y2
(11)
Figure 6 shows the interpolated makeup transfer results with
one and two reference images. By feeding the new makeup
tensors into MANet, we yield a smooth transition between
two reference makeup styles. Similarly, we can adjust the
shade of transfer using only one reference image by assign-
ing x = y1. The generated results demonstrate that our PS-
GAN can not only control the shade of makeup transfer but
also generate a new style of makeup by mixing the makeup
tensors of two makeup styles.

We can also perform partial and interpolated transfer si-
multaneously by leveraging both the face parsing maps and
coefﬁcient thanks to the design of spatial-aware makeup
matrices. The above experiments have demonstrated that
PSGAN broadens the application range of makeup transfer
signiﬁcantly.

4.5. Comparison

We conduct comparison with general image-to-image
translation methos DIA [18] and CycleGAN [31] as well
as state-of-the-art makeup transfer methods BeautyGAN
(BGAN) [17], PairedCycleGAN (PGAN) [2], BeautyGlow
(BGlow) [3] and LADN [10]. Current makeup transfer

Method

BGAN [17]

PGAN [2]

BGlow [3]

LADN [10]

PSGAN

Functions

Shade

Part

Robust

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Table 1. Analysises of existing methods. “Shade”, “Part” and “Ro-
bust” indicate shade-controllable, partial and pose/expression ro-
bust transfer respectively.

Test set

PSGAN BGAN DIA CGAN LADN

MT

M-Wild

61.5

83.5

32.5

13.5

3.25

1.75

2.5

1.25

0.25

0.0

Table 2. Ratio selected as best (%).

methods leverage face parsing maps [2, 3, 17] and facial
landmarks [10] for training and realize different functions
as shown in Table 1.

Quantitative Comparison. We conduct a user study
for quantitative evaluation on Amazon Mechanical Turk
(AMT) that use BGAN, CGAN, DIA, and LADN as base-
lines. For a fair comparison, we only compare with methods
whose code and pre-train model are released since we can-
not guarantee a perfect re-implementation. We randomly
select 20 source images and 20 reference images from both
the MT test set and Makeup-Wild (M-Wild) dataset. Af-
ter using the above methods to perform makeup transfer be-
tween these images, we obtain 800 images for each method.
Then, 5 different workers are asked to choose the best im-
ages generated by ﬁve methods through considering image
realism and the similarity with reference makeup styles.
The generated images are shown in random order for a fair
comparison. Table 2 shows the human evaluation results.

7

Figure 7. Qualitative comparison. PSGAN is able to generate realistic images with the same makeup styles as the reference.

Figure 8. Qualitative comparison on M-Wild test set.

Our PSGAN outperforms other methods by a large margin,
especially on the M-Wild test set.

Qualitative Comparison. Figure 7 shows the qualita-
tive comparison of PSGAN with other state-of-the-art meth-
ods on frontal faces in neutral expressions. Since the code
of BeautyGlow and PairedCycleGAN is not released, we
follow the strategy of BeautyGlow which cropped the re-
sults from corresponding papers. The result produced by
DIA has an unnatural color on hair and background since
it performs transfer in the whole image. Comparatively,
the result of CycleGAN is more realistic than that of DIA,
but CycleGAN can only synthesize general makeup which
is not similar to the reference. The current makeup trans-
fer methods outperform the previous methods. However,
BeautyGlow fails to preserve the color of pupils and does
not have the same foundation makeup as reference. We also
use the pre-trained model released by the author of LADN,
which produces blurry transfer results and unnatural back-
ground. Compared to the baselines, our method is able to
generate vivid images with the same makeup styles as ref-
erence.

We also conduct a comparison on the M-Wild test set
with the state-of-the-art method (BeautyGAN and LADN)
that provide code and pre-trained model, as shown in Figure
8. Since the current methods lack an explicit mechanism to
guide the direction of make transfer at the pixel-level and
also overﬁt on frontal images, the makeup is applied in the

Figure 9. Video makeup transfer results of PSGAN.

wrong region of the face when dealing with images with
different poses and expressions. For example, the lip gloss
is transferred to the skin on the ﬁrst row of Figure 8. In the
second row, other methods fail to perform transfer on faces
with different sizes. However, the proposed AMM module
can accurately assign the makeup for every pixel through
calculating the similarities, which makes our results look
better.

4.6. Video Makeup Transfer

To transfer makeup for a person in the video is a chal-
lenging and meaningful task, which has wide prospects in
the applications. However, the pose and expression of a face
in the video are continuously changing which brings extra
difﬁculties. To examine the effectiveness of our method,
we simply perform makeup transfer on every frame of the
video, as shown in Figure 9. By incorporating the design of
PSGAN, we receive nice and stable transferred results.

5. Conclusion

In order to bring makeup transfer to real-world applica-
tions, we propose the Pose and expression robust Spatial-
Aware GAN (PSGAN) that ﬁrst distills the makeup style
into two makeup matrices from the reference and then

8

leverages an Attentive Makeup Morphing (AMM) module
to conduct makeup transfer accurately. The experiments
demonstrate our approach can achieve state-of-the-art trans-
fer results on both frontal facial images and facial images
that have various poses and expressions. Also, with the
spatial-aware makeup matrices, PSGAN can transfer the
makeup partially and adjust the shade of transfer, which
greatly broadens the application range of makeup transfer.
Moreover, we believe our novel framework can be used
in other conditional image synthesis problems that require
customizable and precise synthesis.

References

[1] Taleb Alashkar, Songyao Jiang, Shuyang Wang, and Yun Fu.
Examples-rules guided deep neural network for makeup rec-
ommendation. In AAAI, 2017. 2

[2] Huiwen Chang, Jingwan Lu, Fisher Yu, and Adam Finkel-
stein. Pairedcyclegan: Asymmetric style transfer for apply-
ing and removing makeup. In CVPR, 2018. 1, 2, 7

[3] Hung-Jen Chen, Ka-Ming Hui, Sishui Wang, Li-Wu Tsao,
Hong-Han Shuai, Wen-Huang Cheng, and National Chiao
Tung. Beautyglow : On-demand makeup transfer framework
with reversible generative network. In CVPR, 2019. 1, 2, 5,
7

[4] Yunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image
translation. In CVPR, 2017. 4, 5

[5] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kud-
lur. A learned representation for artistic style. ArXiv,
abs/1610.07629, 2016. 2

[6] Leon A. Gatys, Matthias Bethge, Aaron Hertzmann, and Eli
Shechtman. Preserving color in neural artistic style transfer.
ArXiv, abs/1606.05897, 2016. 2

[7] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
A neural algorithm of artistic style. ArXiv, abs/1508.06576,
2015. 2

[8] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
Image style transfer using convolutional neural networks. In
CVPR, 2016. 2

[9] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,
and Yoshua Bengio. Generative adversarial nets. In NeurIPS,
2014. 1

[10] Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, and
Chi-Keung Tang. Ladn: Local adversarial disentangling net-
work for facial makeup and de-makeup. In ICCV, 2019. 1,
2, 7

[11] Dong Guo and Terence Sim. Digital face makeup by exam-

[12] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-

ple. In CVPR, 2009. 2

works. In CVPR, 2017. 3

[13] Xun Huang and Serge J. Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. In ICCV,
2017. 2, 3

9

[14] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution.
In
ECCV, 2016. 2

[15] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. ArXiv, abs/1412.6980, 2014. 6
[16] Chen Li, Kun Zhou, and Stephen Lin. Simulating makeup
through physics-based manipulation of intrinsic image lay-
ers. In CVPR, 2015. 2

[17] Tingting Li, Ruihe Qian, Chao Dong, Si Liu, Qiong Yan,
Wenwu Zhu, and Liang Lin. Beautygan: Instance-level facial
makeup transfer with deep generative adversarial network. In
ACM MM, 2018. 1, 2, 5, 7

[18] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing
Kang. Visual attribute transfer through deep image analogy.
ACM TOG, 2017. 7

[19] Luoqi Liu, Junliang Xing, Si Liu, Hui Xu, Xi Zhou, and
Shuicheng Yan. ”wow! you are so beautiful today!”.
In
ACM MM, 2013. 2

[20] Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang, and Xiaochun
Cao. Makeup like a superstar: Deep localized makeup trans-
fer network. In IJCAI, 2016. 2

[21] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala.

Deep photo style transfer. In CVPR, 2017. 2

[22] Volodymyr Mnih, Nicolas Manfred Otto Heess, Alex
Graves, and Koray Kavukcuoglu. Recurrent models of vi-
sual attention. In NeurIPS, 2014. 3

[23] Alexander M. Rush, Sumit Chopra, and Jason Weston. A
neural attention model for abstractive sentence summariza-
tion. In EMNLP, 2015. 3

[24] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised

cross-domain image generation. ICLR, 2016. 2

[25] Wai-Shun Tong, Chi-Keung Tang, Michael S. Brown, and
In PG,

Ying-Qing Xu. Example-based cosmetic transfer.
2007. 2

[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS, 2017. 3
[27] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and
Kaiming He. Non-local neural networks. In CVPR, 2017.
3

[28] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C. Courville, Ruslan Salakhutdinov, Richard S.
Zemel, and Yoshua Bengio. Show, attend and tell: Neural
In ICML,
image caption generation with visual attention.
2015. 3

[29] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
convolutional networks. Signal Processing Letters, 2016. 4
[30] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
In
Wang, and Jiaya Jia. Pyramid scene parsing network.
CVPR, 2016. 6

[31] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017. 1, 5, 7

