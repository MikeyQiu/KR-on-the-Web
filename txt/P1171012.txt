Understanding Deep Image Representations by Inverting Them

Aravindh Mahendran
University of Oxford

Andrea Vedaldi
University of Oxford

4
1
0
2
 
v
o
N
 
6
2
 
 
]

V
C
.
s
c
[
 
 
1
v
5
3
0
0
.
2
1
4
1
:
v
i
X
r
a

Abstract

Image representations, from SIFT and Bag of Visual
Words to Convolutional Neural Networks (CNNs), are a
crucial component of almost any image understanding sys-
tem. Nevertheless, our understanding of them remains lim-
ited. In this paper we conduct a direct analysis of the visual
information contained in representations by asking the fol-
lowing question: given an encoding of an image, to which
extent is it possible to reconstruct the image itself? To an-
swer this question we contribute a general framework to in-
vert representations. We show that this method can invert
representations such as HOG and SIFT more accurately
than recent alternatives while being applicable to CNNs
too. We then use this technique to study the inverse of re-
cent state-of-the-art CNN image representations for the ﬁrst
time. Among our ﬁndings, we show that several layers in
CNNs retain photographically accurate information about
the image, with different degrees of geometric and photo-
metric invariance.

1. Introduction

Most image understanding and computer vision methods
build on image representations such as textons [15], his-
togram of oriented gradients (SIFT [18] and HOG [4]), bag
of visual words [3][25], sparse [35] and local coding [32],
super vector coding [37], VLAD [9], Fisher Vectors [21],
and, lately, deep neural networks, particularly of the convo-
lutional variety [13, 23, 36]. However, despite the progress
in the development of visual representations, their design is
still driven empirically and a good understanding of their
properties is lacking. While this is true of shallower hand-
crafted features, it is even more so for the latest generation
of deep representations, where millions of parameters are
learned from data.

In this paper we conduct a direct analysis of representa-
tions by characterising the image information that they re-
tain (Fig. 1). We do so by modeling a representation as a
function Φ(x) of the image x and then computing an ap-
proximated inverse φ−1, reconstructing x from the code
Φ(x). A common hypothesis is that representations col-
lapse irrelevant differences in images (e.g. illumination or

Figure 1. What is encoded by a CNN? The ﬁgure shows ﬁve
possible reconstructions of the reference image obtained from the
1,000-dimensional code extracted at the penultimate layer of a ref-
erence CNN[13] (before the softmax is applied) trained on the Im-
ageNet data. From the viewpoint of the model, all these images are
practically equivalent. This image is best viewed in color/screen.

viewpoint), so that Φ should not be uniquely invertible.
Hence, we pose this as a reconstruction problem and ﬁnd
a number of possible reconstructions rather than a single
one. By doing so, we obtain insights into the invariances
captured by the representation.

Our contributions are as follows. First, we propose a
general method to invert representations, including SIFT,
HOG, and CNNs (Sect. 2). Crucially, this method uses only
information from the image representation and a generic
natural image prior, starting from random noise as initial
solution, and hence captures only the information contained
in the representation itself. We discuss and evaluate differ-
ent regularization penalties as natural image priors. Sec-
ond, we show that, despite its simplicity and generality, this
method recovers signiﬁcantly better reconstructions from
DSIFT and HOG compared to recent alternatives [31]. As
we do so, we emphasise a number of subtle differences be-
tween these representations and their effect on invertibility.
Third, we apply the inversion technique to the analysis of
recent deep CNNs, exploring their invariance by sampling
possible approximate reconstructions. We relate this to the
depth of the representation, showing that the CNN gradually
builds an increasing amount of invariance, layer after layer.
Fourth, we study the locality of the information stored in

1

the representations by reconstructing images from selected
groups of neurons, either spatially or by channel.

The rest of the paper is organised as follows. Sect. 2 in-
troduces the inversion method, posing this as a regularised
regression problem and proposing a number of image priors
to aid the reconstruction. Sect. 3 introduces various repre-
sentations: HOG and DSIFT as examples of shallow repre-
sentations, and state-of-the-art CNNs as an example of deep
representations. It also shows how HOG and DSIFT can be
implemented as CNNs, simplifying the computation of their
derivatives. Sect. 4 and 5 apply the inversion technique to
the analysis of respectively shallow (HOG and DSIFT) and
deep (CNNs) representations. Finally, Sect. 6 summarises
our ﬁndings.

We use the matconvnet toolbox [30] for implementing

convolutional neural networks.

Related work. There is a signiﬁcant amount of work in un-
derstanding representations by means of visualisations. The
works most related to ours are Weinzaepfel et al. [33] and
Vondrick et al. [31] which invert sparse DSIFT and HOG
features respectively. While our goal is similar to theirs,
our method is substantially different from a technical view-
point, being based on the direct solution of a regularised
regression problem. The beneﬁt is that our technique ap-
plies equally to shallow (SIFT, HOG) and deep (CNN) rep-
resentations. Compared to existing inversion techniques
for dense shallow representations [31], it is also shown to
achieve superior results, both quantitatively and qualita-
tively.

An interesting conclusion of [31, 33] is that, while HOG
and SIFT may not be exactly invertible, they capture a sig-
niﬁcant amount of information about the image. This is in
apparent contradiction with the results of Tatu et al. [27]
who show that it is possible to make any two images
look nearly identical in SIFT space up to the injection of
adversarial noise. A symmetric effect was demonstrated
for CNNs by Szegedy et al. [26], where an imperceptible
amount of adversarial noise sufﬁces to change the predicted
class of an image. The apparent inconsistency is easily re-
solved, however, as the methods of [26, 27] require the in-
jection of high-pass structured noise which is very unlikely
to occur in natural images.

Our work is also related to the DeConvNet method of
Zeiler and Fergus [36], who backtrack the network com-
putations to identify which image patches are responsible
for certain neural activations. Simonyan et al. [24], how-
ever, demonstrated that DeConvNets can be interpreted as a
sensitivity analysis of the network input/output relation. A
consequence is that DeConvNets do not study the problem
of representation inversion in the sense adopted here, which
has signiﬁcant methodological consequences; for example,
DeConvNets require auxiliary information about the acti-
vations in several intermediate layers, while our inversion
uses only the ﬁnal image code. In other words, DeConvNets

look at how certain network outputs are obtained, whereas
we look for what information is preserved by the network
output.

The problem of inverting representations, particularly
CNN-based ones, is related to the problem of inverting
neural networks, which received signiﬁcant attention in the
past. Algorithms similar to the back-propagation technique
developed here were proposed by [14, 16, 19, 34], along
with alternative optimisation strategies based on sampling.
However, these methods did not use natural image priors as
we do, nor were applied to the current generation of deep
networks. Other works [10, 28] specialised on inverting
networks in the context of dynamical systems and will not
be discussed further here. Others [1] proposed to learn a
second neural network to act as the inverse of the original
one, but this is complicated by the fact that the inverse is
usually not unique. Finally, auto-encoder architectures [8]
train networks together with their inverses as a form of su-
pervision; here we are interested instead in visualising feed-
forward and discriminatively-trained CNNs now popular in
computer vision.

2. Inverting representations

This section introduces our method to compute an ap-
proximate inverse of an image representation. This is for-
mulated as the problem of ﬁnding an image whose repre-
sentation best matches the one given [34]. Formally, given
a representation function Φ : RH×W ×C → Rd and a rep-
resentation Φ0 = Φ(x0) to be inverted, reconstruction ﬁnds
the image x ∈ RH×W ×C that minimizes the objective:

x∗ = argmin

x∈RH×W ×C

(cid:96)(Φ(x), Φ0) + λR(x)

(1)

where the loss (cid:96) compares the image representation Φ(x) to
the target one Φ0 and R : RH×W ×C → R is a regulariser
capturing a natural image prior.

Minimising (1) results in an image x∗ that “resembles”
x0 from the viewpoint of the representation. While there
may be no unique solution to this problem, sampling the
space of possible reconstructions can be used to charac-
terise the space of images that the representation deems to
be equivalent, revealing its invariances.

We next discusses the choice of loss and regularizer.

Loss function. There are many possible choices of the loss
function (cid:96). While we use the Euclidean distance:

(cid:96)(Φ(x), Φ0) = (cid:107)Φ(x) − Φ0(cid:107)2,

(2)

it is possible to change the nature of the loss entirely, for ex-
ample to optimize selected neural responses. The latter was
used in [5, 24] to generate images representative of given
neurons.

Regularisers.
Discriminatively-trained representations
may discard a signiﬁcant amount of low-level image statis-

2

tics as these are usually not interesting for high-level tasks.
As this information is nonetheless useful for visualization, it
can be partially recovered by restricting the inversion to the
subset of natural images X ⊂ RH×W ×C. However, min-
imising over X requires addressing the challenge of mod-
eling this set. As a proxy one can incorporate in the re-
construction an appropriate image prior. Here we experi-
ment with two such priors. The ﬁrst one is simply the α-
norm Rα(x) = (cid:107)x(cid:107)α
α, where x is the vectorised and mean-
subtracted image. By choosing a relatively large exponent
(α = 6 is used in the experiments) the range of the image
is encouraged to stay within a target interval instead of di-
verging.

A second richer regulariser is total variation (TV)
RV β (x), encouraging images to consist of piece-wise con-
stant patches. For continuous functions (or distributions)
f : RH×W ⊃ Ω → R, the TV norm is given by:

RV β (f ) =

(u, v)

+

(u, v)

du dv

(cid:32)(cid:18) ∂f
∂u

(cid:90)

Ω

(cid:19)2

(cid:18) ∂f
∂v

(cid:19)2(cid:33) β

2

where β = 1. Here images are discrete (x ∈ RH×W ) and
the TV norm is replaced by the ﬁnite-difference approxima-
tion:

RV β (x) =

(cid:88)

(cid:16)

(xi,j+1 − xij)2 + (xi+1,j − xij)2(cid:17) β

2

.

i,j

It was observed empirically that the TV regularizer (β = 1)
in the presence of subsampling, also caused by max pooling
in CNNs, leads to “spikes” in the reconstruction. This is a
known problem in TV-based image interpolation (see e.g.
Fig. 3 in [2]) and is illustrated in Fig. 2.left when inverting
a layer in a CNN. The “spikes” occur at the locations of
the samples because: (1) the TV norm along any path be-
tween two samples depends only on the overall amount of
intensity change (not on the sharpness of the changes) and
(2) integrated on the 2D image, it is optimal to concentrate
sharp changes around a boundary with a small perimeter.
Hyper-Laplacian priors with β < 1 are often used as a better
match of the gradient statistics of natural images [12], but
they only exacerbate this issue.
Instead, we trade-off the
sharpness of the image with the removal of such artifacts
by choosing β > 1 which, by penalising large gradients,
distributes changes across regions rather than concentrating
them at a point or curve. We refer to this as the V β regular-
izer. As seen in Fig. 2 (right), the spikes are removed with
β = 2 but the image is washed out as edges are penalized
more than with β = 1.

When the target of the reconstruction is a colour image,

both regularisers are summed for each colour channel.

Figure 2. Left: Spikes in a inverse of norm1 features - detail
shown. Right: Spikes removed by a V β regularizer with β = 2.

from reasonable settings of the parameters. First, the loss is
replaced by the normalized version (cid:107)Φ(x) − Φ0(cid:107)2
2/(cid:107)Φ0(cid:107)2
2.
This ﬁxes its dynamic range, as after normalisation the loss
near the optimum can be expected to be contained in the
[0, 1) interval, touching zero at the optimum. In order to
make the dynamic range of the regulariser(s) comparable
one can aim for a solution x∗ which has roughly unitary
Euclidean norm. While representations are largely insensi-
tive to the scaling of the image range, this is not exactly true
for the ﬁrst few layers of CNNs, where biases are tuned to a
“natural” working range. This can be addressed by consid-
ering the objective (cid:107)Φ(σx) − Φ0(cid:107)2
2 + R(x) where
the scaling σ is the average Euclidean norm of natural im-
ages in a training set.

2/(cid:107)Φ0(cid:107)2

Second,

the multiplier λα of the α-norm regularizer
should be selected to encourage the reconstructed image
σx to be contained in a natural range [−B, B] (e.g. in
most CNN implementations B = 128).
If most pix-
els in σx have a magnitude similar to B, then Rα(x) ≈
HW Bα/σα, and λα ≈ σα/(HW Bα). A similar argu-
ment suggests to pick the V β-norm regulariser coefﬁcient
as λV β ≈ σβ/(HW (aB)β), where a is a small fraction
(e.g. a = 1%) relating the dynamic range of the image to
that of its gradient.

The ﬁnal form of the objective function is

(cid:107)Φ(σx) − Φ0(cid:107)2

2/(cid:107)Φ0(cid:107)2

2 + λαRα(x) + λV β RV β (x) (3)

It is in general non convex because of the nature of Φ. We
next discuss how to optimize it.

2.1. Optimisation

Finding an optimizer of the objective (1) may seem a
hopeless task as most representations Φ involve strong non-
linearities; in particular, deep representations are a chain
of several non-linear layers. Nevertheless, simple gradient
descent (GD) procedures have been shown to be very effec-
tive in learning such models from data, which is arguably
an even harder task. Hence, it is not unreasonable to use
GD to solve (1) too. We extend GD to incorporate a few ex-
tensions that proved useful in learning deep networks [13],
as discussed below.

Balancing the different terms. Balancing loss and regu-
lariser(s) requires some attention. While an optimal tuning
can be achieved by cross-validation, it is important to start

Momentum. GD is extended to use momentum:

µt+1 ← mµt − ηt∇E(x),

xt+1 ← xt + µt

3

where E(x) = (cid:96)(Φ(x), Φ0) + λR(x) is the objective func-
tion. The vector µt is a weighed average of the last several
gradients, with decaying factor m = 0.9. Learning pro-
ceeds a few hundred iterations with a ﬁxed learning rate ηt
and is reduced tenfold, until convergence.

Computing derivatives. Applying GD requires comput-
ing the derivatives of the loss function composed with the
representation Φ(x). While the squared Euclidean loss is
smooth, this is not the case for the representation. A key
feature of CNNs is the ability of computing the deriva-
tives of each computational layer, composing the latter in
an overall derivative of the whole function using back-
propagation. Our translation of HOG and DSIFT into CNN
allows us to apply the same technique to these computer
vision representations too.

3. Representations

This section describes the image representations stud-
ied in the paper: DSIFT (Dense-SIFT), HOG, and refer-
ence deep CNNs. Furthermore, it shows how to implement
DSIFT and HOG in a standard CNN framework in order to
compute their derivatives. Being able to compute deriva-
tives is the only requirement imposed by the algorithm of
Sect. 2.1.
Implementing DSIFT and HOG in a standard
CNN framework makes derivative computation convenient.

CNN-A: deep networks. As a reference deep network
we consider the Caffe-Alex [11] model (CNN-A), which
closely reproduces the network by Krizhevsky et al. [13].
This and many other similar networks alternate the fol-
lowing computational building blocks: linear convolution,
ReLU gating, spatial max-pooling, and group normalisa-
tion. Each such block takes as input a d-dimensional image
and produces as output a k-dimensional one. Blocks can
additionally pad the image (with zeros for the convolutional
blocks and with −∞ for max pooling) or subsample the
data. The last several layers are deemed “fully connected”
as the support of the linear ﬁlters coincides with the size of
the image; however, they are equivalent to ﬁltering layers in
all other respects. Table 2 details the structure of CNN-A.

CNN-DSIFT and CNN-HOG. This section shows how
DSIFT [17, 20] and HOG [4] can be implemented as CNNs.
This formalises the relation between CNNs and these stan-
dard representations.
It also makes derivative computa-
tion for these representations simple; for the inversion al-
gorithm of Sect. 2. The DSIFT and HOG implementations
in the VLFeat library [29] are used as numerical references.
These are equivalent to Lowe’s [17] SIFT and the DPM
V5 HOG [6, 7].

SIFT and HOG involve: computing and binning image
gradients, pooling binned gradients into cell histograms,
grouping cells into blocks, and normalising the blocks. De-
note by g the gradient at a given pixel and consider binning

this into one of K orientations (where K = 8 for SIFT and
K = 18 for HOG). This can be obtained in two steps: di-
rectional ﬁltering and gating. The k-th directional ﬁlter is
Gk = u1kGx + u2kGy where


uk =

(cid:21)

(cid:20)cos 2πk
K
sin 2πk
K

, Gx =



0
−1
0

0
0
0


0
 , Gy = G(cid:62)
1
x .
0

The output of a directional ﬁlter is the projection (cid:104)g, uk(cid:105) of
the gradient along direction uk. A suitable gating function
implements binning into a histogram element hk. DSIFT
uses bilinear orientation binning, given by

(cid:26)

hk = (cid:107)g(cid:107) max

0, 1 −

K
2π

cos−1 (cid:104)g, uk(cid:105)
(cid:107)g(cid:107)

(cid:27)

,

whereas HOG (in the DPM V5 variant) uses hard assign-
ments hk = (cid:107)g(cid:107)1 [(cid:104)g, uk(cid:105) > (cid:107)g(cid:107) cos π/K]. Filtering is
a standard CNN operation but these binning functions are
not. While their implementation is simple, an interesting
alternative is the approximated bilinear binning:
(cid:27)

(cid:26)

hk ≈ (cid:107)g(cid:107) max

(cid:104)g, uk(cid:105)
(cid:107)g(cid:107)
∝ max {0, (cid:104)g, uk(cid:105) − a(cid:107)g(cid:107)} ,

1
1 − a

0,

−

a
1 − a

a = cos 2π/K.

The norm-dependent offset (cid:107)g(cid:107) is still non-standard, but the
ReLU operator is, which shows to which extent approxi-
mate binning can be achieved in typical CNNs.

The next step is to pool the binned gradients into cell
histograms using bilinear spatial pooling, followed by ex-
tracting blocks of 2 × 2 (HOG) or 4 × 4 (SIFT) cells. Both
such operations can be implemented by banks of linear ﬁl-
ters. Cell blocks are then l2 normalised, which is a special
case of the standard local response normalisation layer. For
HOG, blocks are further decomposed back into cells, which
requires another ﬁlter bank. Finally, the descriptor values
are clamped from above by applying y = min{x, 0.2} to
each component, which can be reduced to a combination of
linear and ReLU layers.

The conclusion is that approximations to DSIFT and
HOG can be implemented with conventional CNN compo-
nents plus the non-conventional gradient norm offset. How-
ever, all the ﬁlters involved are much sparser and simpler
than the generic 3D ﬁlters in learned CNNs. Nonetheless,
in the rest of the paper we will use exact CNN equivalents of
DSIFT and HOG, using modiﬁed or additional CNN com-
ponents as needed. 1 These CNNs are numerically indis-

1This requires addressing a few more subtleties.

In DSIFT gradient
contributions are usually weighted by a Gaussian centered at each descrip-
tor (a 4 × 4 cell block); here we use the VLFeat approximation (fast op-
tion) of weighting cells rather than gradients, which can be incorporated in
the block-forming ﬁlters. In UoCTTI HOG, cells contain both oriented and
unoriented gradients (27 components in total) as well as 4 texture compo-
nents. The latter are ignored for simplicity, while the unoriented gradients
are obtained as average of the oriented ones in the block-forming ﬁlters.

4

descriptors
method
error (%)

HOG
HOGgle
66.20
±13.7

HOG HOGb DSIFT
our
our
10.67
28.10
±5.2
±7.9

our
10.89
±7.5

Table 1. Average reconstruction error of different representation
inversion methods, applied to HOG and DSIFT. HOGb denotes
HOG with bilinear orientation assignments. The standard devia-
tion shown is the standard deviation of the error and not the stan-
dard deviation of the mean error.

Figure 4. Effect of V β regularization. The same inversion algo-
rithm visualized in Fig. 3(d) is used with a smaller (λV β = 0.5),
comparable (λV β = 5.0), and larger (λV β = 50) regularisation
coefﬁcient.

tinguishable from the VLFeat reference implementations,
but, true to their CNN nature, allow computing the feature
derivatives as required by the algorithm of Sect. 2.

Next we apply the algorithm from Sect. 2 on CNN-A,

CNN-DSIFT and CNN-HOG to analyze our method.

4. Experiments with shallow representations

This section evaluates the representation inversion
method of Sect. 2 by applying it to HOG and DSIFT. The
analysis includes both a qualitative (Fig. 3) and quantitative
(Table 1) comparison with existing technique. The quanti-
tative evaluation reports a normalized reconstruction error
(cid:107)Φ(x∗) − Φ(xi)(cid:107)2/NΦ averaged over 100 images xi from
the ILSVRC 2012 challenge [22] validation data (images 1
to 100). A normalization is essential to place the Euclidean
distance in the context of the volume occupied by the fea-
tures: if the features are close together, then even an Eu-
clidean distance of 0.1 is very large, but if the features are
spread out, then even an Euclidean distance of 105 may be
very small. We use NΦ to be the average pairwise euclidean
distance between Φ(xi)’s across the 100 test images.

We ﬁx the parameters in equation 3 to λα = 2.16 × 108,

λV β = 5, and β = 2.

The closest alternative to our method is HOGgle, a tech-
nique introduced by Vondrick et al. [31] for the visual-
isation of HOG features. The HOGgle code is publicly
available from the authors’ website and is used through-
out these experiments. Crucially, HOGgle is pre-trained to
invert the UoCTTI implementation of HOG, which is nu-
merically equivalent to CNN-HOG (Sect. 3), allowing for a
direct comparison between algorithms.

Curiously, in UoCTTI HOG the l2 normalisation factor is computed con-
sidering only the unoriented gradient components in a block, but applied
to all, which requires modifying the normalization operator. Finally, when
blocks are decomposed back to cells, they are averaged rather than stacked
as in the original Dalal-Triggs HOG, which can be implemented in the
block-decomposition ﬁlters.

a

b

c

d

Figure 5. Test images for qualitative results.

Compared to our method, HOGgle is fast (2-3s vs 60s
on the same CPU) but not very accurate, as it is apparent
both qualitatively (Fig. 3.c vs d) and quantitatively (66%
vs 28% reconstruction error, see Table. 1).
Interestingly,
[31] propose a direct optimisation method similar to (1),
but show that it does not perform better than HOGgle. This
demonstrates the importance of the choice of regulariser
and the ability of computing the derivative of the represen-
tation. The effect of the regularizer λV β is further analysed
in Fig. 4 (and later in Table 3): without this prior infor-
mation, the reconstructions present a signiﬁcant amount of
discretization artifacts.

In terms of speed, an advantage of optimizing (1) is that
it can be switched to use GPU code immediately given the
underlying CNN framework; doing so results in a ten-fold
speedup. Furthermore the CNN-based implementation of
HOG and DSIFT wastes signiﬁcant resources using generic
ﬁltering code despite the particular nature of the ﬁlters in
these two representations. Hence we expect that an op-
timized implementation could be several times faster than
this.

It is also apparent that different representations can be
easier or harder to invert.
In particular, modifying HOG
to use bilinear gradient orientation assignments as SIFT
(Sect. 3) signiﬁcantly reduces the reconstruction error (from
28% down to 11%) and improves the reconstruction quality
(Fig. 3.e). More impressive is DSIFT: it is quantitatively
similar to HOG with bilinear orientations, but produces sig-
niﬁcantly more detailed images (Fig. 3.f). Since HOG uses
a ﬁner quantisation of the gradient compared to SIFT but
otherwise the same cell size and sampling, this result can
be imputed to the heavier block-normalisation of HOG that
evidently discards more image information than SIFT.

5. Experiments with deep representations

Figure 8. Effect of V β regularization on CNNs. Inversions of the
last layers of CNN-A for Fig. 5.d with a progressively larger regu-
lariser λV β . This image is best viewed in color/screen.

This section evaluates the inversion method applied to
CNN-A described in Sect. 3. Compared to CNN-HOG

5

(a) Orig.

(b) HOG

(c) HOGgle [31]

(d) HOG−1

(e) HOGb−1

(f) DSIFT−1

Figure 3. Reconstruction quality of different representation inversion methods, applied to HOG and DSIFT. HOGb denotes HOG with
bilinear orientation assignments. This image is best viewed on screen.

layer
name
type
channels
rec. ﬁeld

4

3

2

1

5

20
10
conv1 relu1 mpool1 norm1 conv2 relu2 mpool2 norm2 conv3 relu3 conv4 relu4 conv5 relu5 mpool5 fc6 relu6 fc7 relu7 fc8
cnv
cnv
cnv
relu
cnv
4096 4096 4096 4096 1000
384
96
355
355
355
99
11

relu mpool
256
163

relu mpool
256
51

relu mpool
96
11

nrm cnv
384
256
99
67

nrm cnv
256
96
51
19

relu
384
131

cnv
384
131

cnv
256
163

256
195

256
67

96
19

relu

relu

355

355

16

18

11

12

14

13

15

19

17

6

7

8

9

Table 2. CNN-A structure. The table speciﬁes the structure of CNN-A along with receptive ﬁeld size of each neuron. The ﬁlters in layers
from 16 to 20 operate as “fully connected”: given the standard image input size of 227 × 227 pixels, their support covers the whole image.
Note also that their receptive ﬁeld is larger than 227 pixels, but can be contained in the image domain due to padding.

and CNN-DSIFT, this network is signiﬁcantly larger and
deeper. It seems therefore that the inversion problem should
be considerably harder. Also, CNN-A is not handcrafted but
learned from 1.2M images of the ImageNet ILSVRC 2012
data [22].

The algorithm of Sect. 2.1 is used to invert the code ob-
tained from each individual CNN layer for 100 ILSVRC
validation images (these were not used to train the CNN-A
model [13]). Similar to Sect. 4, the normalized inversion er-
ror is computed and reported in Table 3. The experiment is
repeated by ﬁxing λα to a ﬁxed value of 2.16×108 and grad-
ually increasing λV β ten-folds, starting from a relatively
small value λ1 = 0.5. The ImageNet ILSVRC mean im-
age is added back to the reconstruction before visualisation
as this is subtracted when training the network. Somewhat
surprisingly, the quantitative results show that CNNs are, in
fact, not much harder to invert than HOG. The error rarely
exceeds 20%, which is comparable to the accuracy of HOG
(Sect. 4). The last layer is in particular easy to invert with
an average error of 8.5%.

We choose the regularizer coefﬁcients for each represen-
tation/layer based on a quantitative and qualitative study
of the reconstruction. We pick λ1 = 0.5 for layers 1-6,
λ2 = 5.0 for layers 7-12 and λ3 = 50 for layers 13-20. The
error value corresponding to these parameters is marked in
bold face in table 3. Increasing λV β causes a deterioration

for the ﬁrst layers, but for the latter layers it helps recover a
more visually interpretable reconstruction. Though this pa-
rameter can be tuned by cross validation on the normalized
reconstruction error, a selection based on qualitative analy-
sis is preferred because the method should yield images that
are visually meaningful.

Qualitatively, Fig. 6 illustrates the reconstruction for a
test image from each layer of CNN-A. The progression is
remarkable. The ﬁrst few layers are essentially an invert-
ible code of the image. All the convolutional layers main-
tain a photographically faithful representation of the image,
although with increasing fuzziness. The 4,096-dimensional
fully connected layers are perhaps more interesting, as they
invert back to a composition of parts similar but not iden-
tical to the ones found in the original image. Going from
relu7 to fc8 reduces the dimensionality further to just 1,000;
nevertheless some of these visual elements can still be iden-
tiﬁed. Similar effects can be observed in the reconstructions
in Fig. 7. This ﬁgure includes also the reconstruction of an
abstract pattern, which is not included in any of the Ima-
geNet classes; still, all CNN codes capture distinctive visual
features of the original pattern, clearly indicating that even
very deep layers capture visual information.

Next, Fig. 7 examines the invariance captured by the
CNN model by considering multiple reconstructions out of
each deep layer. A careful examination of these images re-

6

conv1

relu1

mpool1

norm1

conv2

relu2

mpool2

norm2

conv3

relu3

conv4

relu4

conv5

relu5

mpool5

fc6

relu6

fc7

relu7

fc8

Figure 6. CNN reconstruction. Reconstruction of the image of Fig. 5.a from each layer of CNN-A. To generate these results, the regular-
ization coefﬁcient for each layer is chosen to match the highlighted rows in table 3. This ﬁgure is best viewed in color/screen.

λV β

1

2

19 20
10
conv1 relu1 pool1 norm1 conv2 relu2 pool2 norm2 conv3 relu3 conv4 relu4 conv5 relu5 pool5 fc6 relu6 fc7 relu7 fc8
λ1 10.0 11.3 21.9 20.3 12.4 12.9 15.5 15.9 14.5 16.5 14.9 13.8 12.6 15.6 16.6 12.4 15.8 12.8 10.5 5.3

14

11

12

17

18

16

13

15

6

4

7

3

5

9

8

λ2

λ3

±5.0

±3.1 ±5.3 ±4.7

±5.0 ±5.5 ±9.2
±4.7 ±5.3 ±3.8 ±3.8 ±2.8 ±5.1 ±4.6 ±3.5 ±4.5 ±6.4 ±1.9 ±1.1
20.2 22.4 30.3 28.2 20.0 17.4 18.2 18.4 14.4 15.1 13.3 14.0 15.4 13.9 15.5 14.2 13.7 15.4 10.8 5.9
±9.3 ±10.3 ±13.6 ±7.6
±3.6 ±3.3 ±2.6 ±2.8 ±2.7 ±3.2 ±3.5 ±3.7 ±3.1 ±10.3 ±1.6 ±0.9
40.8 45.2 54.1 48.1 39.7 32.8 32.7 32.4 25.6 26.9 23.3 23.9 25.7 20.1 19.0 18.6 18.7 17.1 15.5 8.5
±5.6 ±5.2 ±4.1 ±4.6 ±4.3 ±4.3 ±4.3 ±4.9 ±3.8 ±3.4 ±2.1 ±1.3
±17.0 ±18.7 ±22.7 ±11.8

±9.1 ±7.7 ±8.0

±4.9 ±5.0 ±5.5

±7.0

±5.0

±4.6

Table 3. Inversion error for CNN-A. Average inversion percentage error (normalized) for all the layers of CNN-A and various amounts
of V β regularisation: λ1 = 0.5, λ2 = 10λ1 and λ3 = 100λ1. In bold face are the error values corresponding to the regularizer that works
best both qualitatively and quantitatively. The deviations speciﬁed in this table are the standard deviations of the errors and not the standard
deviations of the mean error value.

pool5

relu6

relu7

fc8

pool5

relu6

relu7

fc8

Figure 7. CNN invariances. Multiple reconstructions of the images of Fig. 5.c–d from different deep codes obtained from CNN-A. This
ﬁgure is best seen in colour/screen.

veals that the codes capture progressively larger deforma-
tions of the object. In the “ﬂamingo” reconstruction, in par-
ticular, relu7 and fc8 invert back to multiple copies of the
object/parts at different positions and scales.

tion. Considerably lowering the regulariser parameter still
yields very accurate inversions, but this time with barely any
resemblance to a natural image. This conﬁrms that CNNs
have strong non-natural confounders.

Note that all these and the original images are nearly in-
distinguishable from the viewpoint of the CNN model; it is
therefore interesting to note the lack of detail in the deep-
est reconstructions, showing that the network captures just a
sketch of the objects, which evidently sufﬁces for classiﬁca-

We now examine reconstructions obtained from subset
of neural responses in different CNN layers. Fig. 9 explores
the locality of the codes by reconstructing a central 5 × 5
patch of features in each layer. The regulariser encourages
portions of the image that do not contribute to the neural

7

conv1

relu1

mpool1

norm1

conv2

relu2

mpool2

norm2

conv3

relu3

conv4

relu4

conv5

relu5

Figure 9. CNN receptive ﬁeld. Reconstructions of the image of Fig. 5.a from the central 5 × 5 neuron ﬁelds at different depths of CNN-A.
The white box marks the ﬁeld of view of the 5 × 5 neuron ﬁeld. The ﬁeld of view is the entire image for conv5 and relu5.

conv1-grp1

norm1-grp1

norm2-grp1

conv1-grp1

norm1-grp1

norm2-grp1

conv1-grp2

norm1-grp2

norm2-grp2

conv1-grp2

norm1-grp2

norm2-grp2

Figure 10. CNN neural streams. Reconstructions of the images of Fig. 5.c-b from either of the two neural streams of CNN-A. This ﬁgure
is best seen in colour/screen.

responses to be switched off. The locality of the features is
obvious in the ﬁgure; what is less obvious is that the effec-
tive receptive ﬁeld of the neurons is in some cases signiﬁ-
cantly smaller than the theoretical one - shown as a white
box in the image.

Finally, Fig. 10 reconstructs images from a subset of fea-
ture channels. CNN-A contains in fact two subsets of fea-
ture channels which are independent for the ﬁrst several lay-
ers (up to norm2) [13]. Reconstructing from each subset
individually, clearly shows that one group is tuned towards
low-frequency colour information whereas the second one
is tuned to towards high-frequency luminance components.
Remarkably, this behaviour emerges naturally in the learned
network without any mechanism directly encouraging this
pattern.

6. Summary

Figure 11. Diversity in the CNN model. mpool5 reconstructions
show that the network retains rich information even at such deep
levels. This ﬁgure is best viewed in color/screen (zoom in).

than alternative reconstruction methods for HOG. Applied
to CNNs, the visualisations shed light on the information
represented at each layer. In particular, it is clear that a pro-
gressively more invariant and abstract notion of the image
content is formed in the network.

This paper proposed an optimisation method to invert
shallow and deep representations based on optimizing an
objective function with gradient descent. Compared to al-
ternatives, a key difference is the use of image priors such as
the V β norm that can recover the low-level image statistics
removed by the representation. This tool performs better

In the future, we shall experiment with more expres-
sive natural image priors and analyze the effect of network
hyper-parameters on the reconstructions. We shall extract
subsets of neurons that encode object parts and try to estab-
lish sub-networks that capture different details of the image.

8

[27] A. Tatu, F. Lauze, M. Nielsen, and B. Kimia. Exploring the rep-
resentation capabilities of the HOG descriptor. In ICCV Workshop,
2011.

[28] A. R. V´arkonyi-K´oczy and A. R¨ovid. Observer based iterative neu-
ral network model inversion. In IEEE Int. Conf. on Fuzzy Systems,
2005.

[29] A. Vedaldi. An open implementation of the SIFT detector and de-

scriptor. Technical Report 070012, UCLA CSD, 2007.

[30] A. Vedaldi and K. Lenc. MatConvNet: CNNs for MATLAB. http:

//www.vlfeat.org/matconvnet/, 2014.

[31] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. HOGgles:

Visualizing object detection features. In ICCV, 2013.

[32] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-
constrained linear coding for image classiﬁcation. CVPR, 2010.
[33] P. Weinzaepfel, H. J´egou, and P. P´erez. Reconstructing an image

from its local descriptors. In CVPR, 2011.

[34] R. J. Williams. Inverting a connectionist network mapping by back-

propagation of error. In Proc. CogSci, 1986.

[35] J. Yang, K. Yu, and T. Huang. Supervised translation-invariant sparse

coding. In CVPR, 2010.

[36] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-

tional networks. In ECCV, 2014.

[37] X. Zhou, K. Yu, T. Zhang, and T. S. Huang. Image classiﬁcation us-
ing super-vector coding of local image descriptors. In ECCV, 2010.

References

[1] C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon

Press, Oxford, 1995.

[2] Y. Chen, R. Ranftl, and T. Pock. A bi-level view of inpainting-based
image compression. In Proc. of Computer Vision Winter Workshop,
2014.

[3] G. Csurka, C. R. Dance, L. Dan, J. Willamowski, and C. Bray. Visual
categorization with bags of keypoints. In Proc. ECCV Workshop on
Stat. Learn. in Comp. Vision, 2004.

[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human

detection. In CVPR, 2005.

[5] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing
higher-layer features of a deep network. Technical Report 1341, Uni-
versity of Montreal, 2009.

[6] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan.
Object detection with discriminatively trained part based models.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
32(9):1627–1645, 2010.

[7] R. B. Girshick, P. F. Felzenszwalb, and D. McAllester. Discrim-
inatively trained deformable part models, release 5. http://
people.cs.uchicago.edu/˜rbg/latent-release5/.
[8] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality

of data with neural networks. Science, 313(5786), 2006.

[9] H. J´egou, M. Douze, C. Schmid, and P. P´erez. Aggregating local
descriptors into a compact image representation. In CVPR, 2010.
[10] C. A. Jensen, R. D. Reed, R. J. Marks, M. El-Sharkawi, J.-B. Jung,
R. Miyamoto, G. Anderson, and C. Eggen. Inversion of feedforward
neural networks: algorithms and applications. Proc. of the IEEE,
87(9), 1999.

[11] Y. Jia. Caffe: An open source convolutional architecture for fast
feature embedding. http://caffe.berkeleyvision.org/,
2013.

[12] D. Krishnan and R. Fergus. Fast image deconvolution using hyper-

laplacian priors. In NIPS, 2009.

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-
tion with deep convolutional neural networks. In NIPS, 2012.
[14] S. Lee and R. M. Kil. Inverse mapping of continuous functions using
local and global information. IEEE Trans. on Neural Networks, 5(3),
1994.

[15] T. Leung and J. Malik. Representing and recognizing the visual ap-
pearance of materials using three-dimensional textons. IJCV, 43(1),
2001.

[16] A. Linden and J. Kindermann. Inversion of multilayer nets. In Proc.

Int. Conf. on Neural Networks, 1989.

[17] D. G. Lowe. Object recognition from local scale-invariant features.

In ICCV, 1999.

[18] D. G. Lowe. Distinctive image features from scale-invariant key-

points. IJCV, 2(60):91–110, 2004.

[19] B.-L. Lu, H. Kita, and Y. Nishikawa. Inverting feedforward neural
networks using linear and nonlinear programming. IEEE Trans. on
Neural Networks, 10(6), 1999.

[20] E. Nowak, F. Jurie, and B. Triggs. Sampling strategies for bag-of-

features image classiﬁcation. In ECCV, 2006.

[21] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for

image categorizaton. In CVPR, 2006.

[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge,
2014.

[23] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-
Cun. Overfeat: Integrated recognition, localization and detection us-
ing convolutional networks. In CoRR, volume abs/1312.6229, 2014.
[24] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside con-
volutional networks: Visualising image classiﬁcation models and
saliency maps. In Proc. ICLR, 2014.

[25] J. Sivic and A. Zisserman. Video Google: A text retrieval approach

to object matching in videos. In ICCV, 2003.

[26] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Good-
Intriguing properties of neural networks.

fellow, and R. Fergus.
CoRR, abs/1312.6199, 2013.

9

