Universal Sentence Encoder

Daniel Cera, Yinfei Yanga, Sheng-yi Konga, Nan Huaa, Nicole Limtiacob,
Rhomni St. Johna, Noah Constanta, Mario Guajardo-C´espedesa, Steve Yuanc,
Chris Tara, Yun-Hsuan Sunga, Brian Stropea, Ray Kurzweila

aGoogle Research
Mountain View, CA

bGoogle Research
New York, NY

cGoogle
Cambridge, MA

8
1
0
2
 
r
p
A
 
2
1
 
 
]
L
C
.
s
c
[
 
 
2
v
5
7
1
1
1
.
3
0
8
1
:
v
i
X
r
a

Abstract

We present models for encoding sentences
into embedding vectors that speciﬁcally
target transfer learning to other NLP tasks.
The models are efﬁcient and result
in
accurate performance on diverse transfer
tasks. Two variants of the encoding mod-
els allow for trade-offs between accuracy
and compute resources. For both vari-
ants, we investigate and report the rela-
tionship between model complexity, re-
source consumption,
the availability of
transfer task training data, and task perfor-
mance. Comparisons are made with base-
lines that use word level transfer learning
via pretrained word embeddings as well
as baselines do not use any transfer learn-
ing. We ﬁnd that transfer learning using
sentence embeddings tends to outperform
word level transfer. With transfer learn-
ing via sentence embeddings, we observe
surprisingly good performance with min-
imal amounts of supervised training data
for a transfer task. We obtain encourag-
ing results on Word Embedding Associ-
ation Tests (WEAT) targeted at detecting
model bias. Our pre-trained sentence en-
coding models are made freely available
for download and on TF Hub.

1

Introduction

Limited amounts of training data are available for
many NLP tasks. This presents a challenge for
data hungry deep learning methods. Given the
high cost of annotating supervised training data,
very large training sets are usually not available
for most research or industry NLP tasks. Many
models address the problem by implicitly per-
forming limited transfer learning through the use

Figure 1: Sentence similarity scores using embed-
dings from the universal sentence encoder.

of pre-trained word embeddings such as those
produced by word2vec (Mikolov et al., 2013) or
GloVe (Pennington et al., 2014). However, recent
work has demonstrated strong transfer task per-
formance using pre-trained sentence level embed-
dings (Conneau et al., 2017).

In this paper, we present two models for produc-
ing sentence embeddings that demonstrate good
transfer to a number of other of other NLP tasks.
We include experiments with varying amounts of
transfer task training data to illustrate the relation-
ship between transfer task performance and train-
ing set size. We ﬁnd that our sentence embeddings
can be used to obtain surprisingly good task per-
formance with remarkably little task speciﬁc train-
ing data. The sentence encoding models are made
publicly available on TF Hub.

Engineering characteristics of models used for
transfer learning are an important consideration.
We discuss modeling trade-offs regarding mem-
ory requirements as well as compute time on CPU
and GPU. Resource consumption comparisons are
made for sentences of varying lengths.

import tensorflow_hub as hub

embed = hub.Module("https://tfhub.dev/google/"

"universal-sentence-encoder/1")

embedding = embed([

"The quick brown fox jumps over the lazy dog."])

Listing 1: Python example code for using the
universal sentence encoder.

2 Model Toolkit

We make available two new models for encoding
sentences into embedding vectors. One makes use
of the transformer (Vaswani et al., 2017) architec-
ture, while the other is formulated as a deep aver-
aging network (DAN) (Iyyer et al., 2015). Both
models are implemented in TensorFlow (Abadi
et al., 2016) and are available to download from
TF Hub:1

https://tfhub.dev/google/
universal-sentence-encoder/1

The models take as input English strings and
produce as output a ﬁxed dimensional embedding
representation of the string. Listing 1 provides a
minimal code snippet to convert a sentence into
a tensor containing its sentence embedding. The
embedding tensor can be used directly or in-
corporated into larger model graphs for speciﬁc
tasks.2

As illustrated in Figure 1, the sentence embed-
dings can be trivially used to compute sentence
level semantic similarity scores that achieve ex-
cellent performance on the semantic textual sim-
ilarity (STS) Benchmark (Cer et al., 2017). When
included within larger models, the sentence encod-
ing models can be ﬁne tuned for speciﬁc tasks us-
ing gradient based updates.

3 Encoders

We introduce the model architecture for our two
encoding models in this section. Our two encoders
have different design goals. One based on the
transformer architecture targets high accuracy at
the cost of greater model complexity and resource
consumption. The other targets efﬁcient inference
with slightly reduced accuracy.

1The encoding model for the DAN based encoder is al-
ready available. The transformer based encoder will be made
available at a later point.

2Visit https://colab.research.google.com/ to try the code
snippet in Listing 1. Example code and documentation is
available on the universal encoder website provided above.

3.1 Transformer

The transformer based sentence encoding model
constructs sentence embeddings using the en-
coding sub-graph of the transformer architecture
(Vaswani et al., 2017). This sub-graph uses at-
tention to compute context aware representations
of words in a sentence that take into account both
the ordering and identity of all the other words.
The context aware word representations are con-
verted to a ﬁxed length sentence encoding vector
by computing the element-wise sum of the repre-
sentations at each word position.3 The encoder
takes as input a lowercased PTB tokenized string
and outputs a 512 dimensional vector as the sen-
tence embedding.

The encoding model is designed to be as gen-
eral purpose as possible. This is accomplished
by using multi-task learning whereby a single
encoding model is used to feed multiple down-
stream tasks. The supported tasks include: a Skip-
Thought like task (Kiros et al., 2015) for the un-
supervised learning from arbitrary running text;
a conversational input-response task for the in-
clusion of parsed conversational data (Henderson
et al., 2017); and classiﬁcation tasks for train-
ing on supervised data. The Skip-Thought task
replaces the LSTM (Hochreiter and Schmidhu-
ber, 1997) used in the original formulation with
a model based on the Transformer architecture.

As will be shown in the experimental results
the transformer based encoder achieves
below,
the best overall transfer task performance. How-
ever, this comes at the cost of compute time and
memory usage scaling dramatically with sentence
length.

3.2 Deep Averaging Network (DAN)

The second encoding model makes use of a
deep averaging network (DAN) (Iyyer et al.,
2015) whereby input embeddings for words and
bi-grams are ﬁrst averaged together and then
passed through a feedforward deep neural network
(DNN) to produce sentence embeddings. Simi-
lar to the Transformer encoder, the DAN encoder
takes as input a lowercased PTB tokenized string
and outputs a 512 dimensional sentence embed-
ding. The DAN encoder is trained similarly to the
Transformer based encoder. We make use of mul-

3We then divide by the square root of the length of the
sentence so that the differences between short sentences are
not dominated by sentence length effects

titask learning whereby a single DAN encoder is
used to supply sentence embeddings for multiple
downstream tasks.

The primary advantage of the DAN encoder is
that compute time is linear in the length of the in-
put sequence. Similar to Iyyer et al. (2015), our re-
sults demonstrate that DANs achieve strong base-
line performance on text classiﬁcation tasks.

3.3 Encoder Training Data

Unsupervised training data for the sentence en-
coding models are drawn from a variety of web
sources. The sources are Wikipedia, web news,
web question-answer pages and discussion fo-
rums. We augment unsupervised learning with
training on supervised data from the Stanford Nat-
ural Language Inference (SNLI) corpus (Bowman
et al., 2015). Similar to the ﬁndings of Conneau
et al. (2017), we observe that training to SNLI im-
proves transfer performance.

4 Transfer Tasks

This section presents an overview of the data used
for the transfer learning experiments and the Word
Embedding Association Test (WEAT) data used to
characterize model bias.4 Table 1 summarizes the
number of samples provided by the test portion of
each evaluation set and, when available, the size
of the dev and training data.

MR : Movie review snippet sentiment on a ﬁve
star scale (Pang and Lee, 2005).

CR : Sentiment of sentences mined from cus-
tomer reviews (Hu and Liu, 2004).

: Subjectivity of sentences from movie re-

SUBJ
views and plot summaries (Pang and Lee, 2004).

MPQA : Phrase level opinion polarity from
news data (Wiebe et al., 2005).

TREC : Fine grained question classiﬁcation
sourced from TREC (Li and Roth, 2002).

SST : Binary phrase level sentiment classiﬁca-
tion (Socher et al., 2013).

STS Benchmark : Semantic textual similar-
ity (STS) between sentence pairs scored by Pear-
son correlation with human judgments (Cer et al.,
2017).

4For the datasets MR, CR, and SUBJ, SST, and TREC we
use the preparation of the data provided by Conneau et al.
(2017).

WEAT : Word pairs from the psychology liter-
ature on implicit association tests (IAT) that are
used to characterize model bias (Caliskan et al.,
2017).

Dataset
SST
STS Bench
TREC
MR
CR
SUBJ
MPQA

Train
67,349
5,749
5,452
-
-
-
-

Dev
872
1,500
-
-
-
-
-

Test
1,821
1,379
500
10,662
3,775
10,000
10,606

Table 1: Transfer task evaluation sets

5 Transfer Learning Models

For sentence classiﬁcation transfer tasks, the out-
put of the transformer and DAN sentence encoders
are provided to a task speciﬁc DNN. For the pair-
wise semantic similarity task, we directly assess
the similarity of the sentence embeddings pro-
duced by our two encoders. As shown Eq. 1, we
ﬁrst compute the cosine similarity of the two sen-
tence embeddings and then use arccos to convert
the cosine similarity into an angular distance.5

sim(u, v) =

1 − arccos

(cid:16)

(cid:18) u · v

||u|| ||v||

(cid:19)

(cid:17)

/π

(1)

5.1 Baselines

For each transfer task, we include baselines that
only make use of word level transfer and baselines
that make use of no transfer learning at all. For
word level transfer, we use word embeddings from
a word2vec skip-gram model trained on a corpus
of news data (Mikolov et al., 2013). The pre-
trained word embeddings are included as input to
two model types: a convolutional neural network
models (CNN) (Kim, 2014); a DAN. The base-
lines that use pretrained word embeddings allow
us to contrast word versus sentence level trans-
fer. Additional baseline CNN and DAN models
are trained without using any pretrained word or
sentence embeddings.

5.2 Combined Transfer Models

We explore combining the sentence and word level
transfer models by concatenating their representa-
tions prior to feeding the combined representation

5We ﬁnd that using a similarity based on angular distance

performs better on average than raw cosine similarity.

Model

MR

CR

SUBJ MPQA TREC

SST

USE D+DAN (w2v w.e.)
USE D+CNN (w2v w.e.)
USE T+DAN (w2v w.e.)
USE T+CNN (w2v w.e.)

USE D
USE T
USE D+DAN (lrn w.e.)
USE D+CNN (lrn w.e.)
USE T+DAN (lrn w.e.)
USE T+CNN (lrn w.e.)

DAN (w2v w.e.)
CNN (w2v w.e.)

DAN (lrn w.e.)
CNN (lrn w.e.)

94.72
97.67
95.51
98.07

81.71
82.04
86.66
87.45

87.01
85.87
88.14
87.32

77.11
78.20
81.32
81.18

Sentence & Word Embedding Transfer Learning
93.12
93.24
93.90
93.58
Sentence Embedding Transfer Learning
92.65
93.87
92.91
92.99
93.66
93.36
Word Embedding Transfer Learning
90.80
90.84
Baselines with No Transfer Learning
89.49
91.18

74.45
81.44
77.57
78.49
81.36
81.59

80.97
87.43
81.93
81.49
86.08
86.45

85.38
86.98
85.97
85.53
87.14
86.85

91.19
92.51
95.86
97.71
96.60
97.44

74.75
75.10

75.97
76.39

76.91
79.39

75.24
80.18

85.69
97.32

81.25
81.38

93.88
95.82

80.93
82.20

82.14
85.29
86.62
86.69

77.62
85.38
83.41
85.27
86.24
87.21

80.24
83.74

81.52
84.90

STS Bench
(dev / test)

0.763 / 0.719 (r)
0.814 / 0.782 (r)
–
–
–
–

–
–
–
–

–
–

–
–

Table 2: Model performance on transfer tasks. USE T is the universal sentence encoder (USE) using
Transformer. USE D is the universal encoder DAN model. Models tagged with w2v w.e. make use of
pre-training word2vec skip-gram embeddings for the transfer task model, while models tagged with lrn
w.e. use randomly initialized word embeddings that are learned only on the transfer task data. Accuracy
is reported for all evaluations except STS Bench where we report the Pearson correlation of the similar-
ity scores with human judgments. Pairwise similarity scores are computed directly using the sentence
embeddings from the universal sentence encoder as in Eq. (1).

to the transfer task classiﬁcation layers. For com-
pleteness, we also explore concatenating the rep-
resentations from sentence level transfer models
with the baseline models that do not make use of
word level transfer learning.

6 Experiments

Transfer task model hyperparamaters are tuned
using a combination of Vizier (Golovin et al.)
and light manual tuning. When available, model
hyperparameters are tuned using task dev sets.
Otherwise, hyperparameters are tuned by cross-
validation on the task training data when available
or the evaluation test data when neither training
nor dev data are provided. Training repeats ten
times for each transfer task model with different
randomly initialized weights and we report evalu-
ation results by averaging across runs.

Transfer learning is critically important when
training data for a target task is limited. We ex-
plore the impact on task performance of varying
the amount of training data available for the task
both with and without the use of transfer learning.
Contrasting the transformer and DAN based en-
coders, we demonstrate trade-offs in model com-
plexity and the amount of data required to reach a
desired level of accuracy on a task.

To assess bias in our encoding models, we eval-
uate the strength of various associations learned
by our model on WEAT word lists. We compare
our result to those of Caliskan et al. (2017) who
discovered that word embeddings could be used to
reproduce human performance on implicit associ-
ation tasks for both benign and potentially unde-
sirable associations.

7 Results

Transfer task performance is summarized in Ta-
ble 2. We observe that transfer learning from the
transformer based sentence encoder usually per-
forms as good or better than transfer learning from
the DAN encoder. Hoewver, transfer learning us-
ing the simpler and fast DAN encoder can for
some tasks perform as well or better than the more
sophisticated transformer encoder. Models that
make use of sentence level transfer learning tend
to perform better than models that only use word
level transfer. The best performance on most tasks
is obtained by models that make use of both sen-
tence and word level transfer.

Table 3 illustrates transfer task performance for
varying amounts of training data. We observe that,
for smaller quantities of data, sentence level trans-
fer learning can achieve surprisingly good task

Model

SST 16k

SST 32k

SST 67.3k

USE D+DNN (w2v w.e.)
USE D+CNN (w2v w.e.)
USE T+DNN (w2v w.e.)
USE T+CNN (w2v w.e.)

USE D
USE T
USE D+DNN (lrn w.e.)
USE D+CNN (lrn w.e.)
USE T+DNN (lrn w.e.)
USE T+CNN (lrn w.e.)

DNN (w2v w.e.)
CNN (w2v w.e.)

DNN (lrn w.e.)
CNN (lrn w.e.)

SST 2k

SST 8k

78.68
79.19
84.75
84.16

78.65
77.79
85.24
84.44

81.69
82.32
86.48
85.70

SST 4k
SST 1k
Sentence & Word Embedding Transfer Learning
81.14
79.07
82.70
79.75
86.44
85.05
85.22
84.77
Sentence Embedding Transfer Learning
77.39
85.18
79.01
79.84
84.55
84.23
Word Embedding Transfer Learning
73.03
74.91
Baselines with No Transfer Learning
73.70
74.90

76.38
84.25
78.68
77.74
84.87
83.73

79.02
85.63
82.31
81.83
85.96
85.74

78.38
85.83
82.31
82.64
85.62
86.06

77.47
84.85
75.90
77.28
84.51
82.66

78.07
81.04

66.87
67.98

77.85
79.14

71.23
71.81

66.34
68.10

69.67
71.80

78.29
80.83

77.42
78.86

81.47
83.56
86.38
86.38

77.79
85.59
82.14
84.24
85.86
86.97

79.81
81.98

80.15
82.72

82.14
85.29
86.62
86.69

77.62
85.38
83.41
85.27
86.24
87.21

80.24
83.74

81.52
84.90

Table 3: Task performance on SST for varying amounts of training data. SST 67.3k represents the full
training set. Using only 1,000 examples for training, transfer learning from USE T is able to obtain
performance that rivals many of the other models trained on the full 67.3 thousand example training set.

performance. As the training set size increases,
models that do not make use of transfer learning
approach the performance of the other models.

Table 4 contrasts Caliskan et al. (2017)’s ﬁnd-
ings on bias within GloVe embeddings with the
DAN variant of the universal encoder. Similar
to GloVe, our model reproduces human associa-
tions between ﬂowers vs. insects and pleasantness
vs. unpleasantness. However, our model demon-
strates weaker associations than GloVe for probes
targeted at revealing at ageism, racism and sex-
ism.6 The differences in word association patterns
can be attributed to differences in the training data
composition and the mixture of tasks used to train
the sentence embeddings.

7.1 Discussion

Transfer learning leads to performance improve-
ments on many tasks. Using transfer learning is
more critical when less training data is available.
When task performance is close, the correct mod-
eling choice should take into account engineer-
ing trade-offs regarding the memory and compute

resource requirements introduced by the different
models that could be used.

8 Resource Usage

This section describes memory and compute re-
source usage for the transformer and DAN sen-
tence encoding models for different sentence
lengths. Figure 2 plots model resource usage
against sentence length.

Compute Usage The transformer model time
complexity is O(n2) in sentence length, while the
DAN model is O(n). As seen in Figure 2 (a-b), for
short sentences, the transformer encoding model
is only moderately slower than the much simpler
DAN model. However, compute time for trans-
former increases noticeably as sentence length in-
creases. In contrast, the compute time for the DAN
model stays nearly constant as sentence length is
increased. Since the DAN model is remarkably
computational efﬁcient, using GPUs over CPUs
will often have a much larger practical impact for
the transformer based encoder.

6Researchers and developers are strongly encouraged to
independently verify whether biases in their overall model
or model components impacts their use case. For resources
on ML fairness visit https://developers.google.com/machine-
learning/fairness-overview/.

Memory Usage The transformer model space
complexity also scales quadratically, O(n2), in
sentence length, while the DAN model space com-
plexity is constant in the length of the sentence.

(a) CPU Time vs. Sentence Length

(b) GPU Time vs. Sentence Length

(c) Memory vs. Sentence Length

Figure 2: Model Resource Usage for both USE D and USE T at different batch sizes and sentence
lengths.

Target words

Attrib. words

Ref

Eur.-American vs Afr.-American names
Eur.-American vs. Afr.-American names
Eur.-American vs. Afr.-American names
Male vs. female names
Math vs. arts
Science vs. arts
Mental vs. physical disease
Young vs old peoples names
Flowers vs. insects
Instruments vs. Weapons

Pleasant vs. Unpleasant 1
Pleasant vs. Unpleasant from (a)
Pleasant vs. Unpleasant from (c)
Career vs family
Male vs. female terms
Male vs female terms
Temporary vs permanent
Pleasant vs unpleasant
Pleasant vs. Unpleasant
Pleasant vs Unpleasant

a
b
b
c
c
d
e
c
a
a

GloVe

Uni. Enc. (DAN)

d
1.41
1.50
1.28
1.81
1.06
1.24
1.38
1.21
1.50
1.53

p
10−8
10−4
10−3
10−3
0.018
10−2
10−2
10−2
10−7
10−7

d
0.361
-0.372
0.721
0.0248
0.588
0.236
1.60
1.01
1.38
1.44

p
0.035
0.87
0.015
0.48
0.12
0.32
0.0027
0.022
10−7
10−7

Table 4: Word Embedding Association Tests (WEAT) for GloVe and the Universal Encoder. Effect size
is reported as Cohen’s d over the mean cosine similarity scores across grouped attribute words. Statistical
signiﬁcance is reported for 1 tailed p-scores. The letters in the Ref column indicates the source of the IAT
word lists: (a) Greenwald et al. (1998) (b) Bertrand and Mullainathan (2004) (c) Nosek et al. (2002a)
(d) Nosek et al. (2002b) (e) Monteith and Pettit (2011).

Similar to compute usage, memory usage for the
transformer model increases quickly with sentence
length, while the memory usage for the DAN
model remains constant. We note that, for the
DAN model, memory usage is dominated by the
parameters used to store the model unigram and
bigram embeddings. Since the transformer model
only needs to store unigram embeddings, for short
sequences it requires nearly half as much memory
as the DAN model.

9 Conclusion

Both the transformer and DAN based universal en-
coding models provide sentence level embeddings
that demonstrate strong transfer performance on a
number of NLP tasks. The sentence level embed-
dings surpass the performance of transfer learn-
ing using word level embeddings alone. Models
that make use of sentence and word level transfer
achieve the best overall performance. We observe
that transfer learning is most helpful when limited

training data is available for the transfer task. The
encoding models make different trade-offs regard-
ing accuracy and model complexity that should
be considered when choosing the best model for
a particular application. The pre-trained encod-
ing models will be made publicly available for
research and use in applications that can beneﬁt
from a better understanding of natural language.

Acknowledgments

We thank our teammates from Descartes, Ai.h and
other Google groups for their feedback and sug-
gestions. Special thanks goes to Ben Packer and
Yoni Halpern for implementing the WEAT assess-
ments and discussions on model bias.

References

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,

Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-
sorﬂow: A system for large-scale machine learning.
In Proceedings of USENIX OSDI’16.

Marianne Bertrand and Sendhil Mullainathan. 2004.
Are emily and greg more employable than lakisha
and jamal?
a ﬁeld experiment on labor market
discrimination. The American Economic Review,
94(4).

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In In
Proceedings of NIPS.

Xin Li and Dan Roth. 2002. Learning question classi-

ﬁers. In Proceedings of COLING ’02.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of NIPS’13.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of EMNLP.

Lindsey L. Monteith and Jeremy W. Pettit. 2011. Im-
plicit and explicit stigmatizing attitudes and stereo-
types about depression. Journal of Social and Clin-
ical Psychology, 30(5).

Aylin Caliskan,

and Arvind
Joanna J. Bryson,
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183–186.

Brian A. Nosek, Mahzarin R. Banaji, and Anthony G.
Greenwald. 2002a. Harvesting implicit group at-
titudes and beliefs from a demonstration web site.
Group Dynamics, 6(1).

Brian A. Nosek, Mahzarin R. Banaji, and Anthony G
Greenwald. 2002b. Math = male, me = female,
therefore math me. Journal of Personality and So-
cial Psychology,, 83(1).

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL’04), Main Volume.

Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceedings of
ACL’05.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceeding of EMNLP.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of EMNLP.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of NIPS.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2):165–210.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation. In Proceedings of
SemEval-2017.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
arXiv preprint
natural language inference data.
arXiv:1705.02364.

Daniel Golovin, Benjamin Solnik, Subhodeep Moitra,
Greg Kochanski,
John Karro, and D. Sculley.
Google vizier: A service for black-box optimization.
In Proceedings of KDD ’17.

Anthony G. Greenwald, Debbie E. McGhee, and Jor-
dan L. K. Schwartz. 1998. Measuring individual
differences in implicit cognition: the implicit asso-
ciation test. Journal of personality and social psy-
chology, 74(6).

Matthew Henderson, Rami Al-Rfou, Brian Strope,
Yun-Hsuan Sung, L´aszl´o Luk´acs, Ruiqi Guo, San-
jiv Kumar, Balint Miklos, and Ray Kurzweil. 2017.
Efﬁcient natural language response suggestion for
smart reply. CoRR, abs/1705.00652.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of KDD
’04.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daum´e III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classiﬁcation.
In Proceedings of ACL/IJCNLP.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.

