Quinoa: a Q-function You Infer Normalized
Over Actions

Jonas Degrave1, Abbas Abdolmaleki1, Jost Tobias Springenberg1, Nicolas Heess1 and Martin Riedmiller1
1DeepMind

2018-12-08

9
1
0
2
 
v
o
N
 
5
 
 
]

G
L
.
s
c
[
 
 
1
v
1
3
8
1
0
.
1
1
9
1
:
v
i
X
r
a

We present an algorithm for learning an approximate action-value soft Q-function in the relative entropy
regularised reinforcement learning setting, for which an optimal improved policy can be recovered in
closed form. We use recent advances in normalising ﬂows for parametrising the policy together with a
learned value-function; and show how this combination can be used to implicitly represent Q-values of
an arbitrary policy in continuous action space. Using simple temporal diﬀerence learning on the Q-values
then leads to a uniﬁed objective for policy and value learning. We show how this approach considerably
simpliﬁes standard Actor-Critic oﬀ-policy algorithms, removing the need for a policy optimisation step.
We perform experiments on a range of established reinforcement learning benchmarks, demonstrating
that our approach allows for complex, multimodal policy distributions in continuous action spaces, while
keeping the process of sampling from the policy both fast and exact.

1. Introduction

Oﬀ-policy actor-critic algorithms, in combination with deep neural networks, hold promise for solving
problems in continuous control, as they can be used to learn complex non-linear policies in a data-eﬃcient
manner [11]. Typically deep actor-critic approaches consist of two steps. First, a neural network is used
to ﬁt the Q-values of the current policy. After that, a parametric policy – often a conditional Gaussian
distribution – is learned by maximising these learned Q-values. These two steps are then iterated to
convergence. Ideally, the second policy optimisation step would not be needed. After all, optimising a
policy against a learned Q-function just transforms action-preferences into a normalised distribution.
This optimisation step cannot produce new information which was not already encoded in the Q-function.
It can however introduce sub-optimal behaviour through approximation errors; either due to the choice
in the parametric policy distribution or due to numerical ﬁtting errors.

As a consequence, the idea of learning a Q-function from which an improved policy can be obtained
without additional optimisation, has been previously considered by learning normalised advantage
functions (NAF) [2, 5] or using compatible function approximation with Gaussian policies [16]. While
appealing in theory, these approaches come with the caveat that they put additional constraints on
the Q-function, such as being locally quadratic in action space. This limits the expressiveness of the
Q-function, making it no longer able to correctly ﬁt any set of Q-values.

In this work, we propose an algorithm which learns a soft Q-function globally while providing the
optimal policy in closed form. We call this algorithm Quinoa, a Q-function you Infer Normalised Over
Actions as we can directly perform inference on the optimal policy, which is the soft Q-function normalised
over the action dimensions. We ﬁnd that the key to allowing unrestricted Q-functions that allow for
inference of the optimal policy, is to use a richer class of policy parametrisations. In particular, we use
normalising ﬂows as they can be universal density function approximators. In the next section, we will
explain how we derive our soft Q-function, starting from a relative entropy regularised RL objective, as
considered in REPS [12], TRPO [15], MPO [1] and SAC [8].

Corresponding author(s): grave@deepmind.com

Quinoa: a Q-function You Infer Normalized Over Actions

2. Background

We consider the standard discounted reinforcement learning (RL) problem deﬁned by a Markov decision
process (MDP). The MDP consists of continuous states s, actions a, transition probabilities p(st +1|st , at )
which specify the probability of transitioning from state st to st +1 under action at , a reward function
r (s, a) ∈ (cid:82) and the discount factor γ ∈ [0, 1). The policy πθ (a|s) with parameters θ is a probability
distribution over actions a given a state s. For brevity, we drop the subscript θ in the following. Together
with the transition probabilities, these give rise to a state-visitation distribution µπ (s). We consider the
relative entropy regularised RL setting that encourages the policy to trade oﬀ reward with policy entropy.
Unlike the regular expected reward objective, this can provide advantages when it is desirable to learn
multiple solutions for a given task. More generally, it can help to regularise the policy by preventing
collapse of the state-conditional action distribution. We deﬁne the relative entropy of policy π compared
(cid:104)
to a reference policy ˜π as: DKL[π , ˜π |s] = (cid:69)a∼π (· |s)
, noting that if ˜π is uniform we recover
log
the entropy H [π ]. The goal for the RL algorithm is to maximise the expected sum of discounted future
returns, regularised with this relative entropy:

(cid:16) π (a |s)
˜π (a |s)

(cid:17)(cid:105)

J (π ) = (cid:69)
π,p

(cid:34) ∞
(cid:213)

i=0

(cid:12)
(cid:12)
γ irt (si , ai ) − α DKL[π , ˜π |si ]
(cid:12)
s0, a0, ai ∼ π (·|s), si ∼ p(·|si−1, ai−1)
(cid:12)
(cid:12)

(cid:35)

.

We deﬁne the soft action-value function associated with policy π as the expected cumulative dis-
counted return when choosing action a in state s and acting subsequently according to policy π factor γ ,
as

Qs

π (a, s) = J (π ){s0=s,a0=a } = (cid:69)
π,p

γ irt (si , ai ) − α DKL[π , ˜π |si ]

(cid:34) ∞
(cid:213)

i=0

(cid:35)

(cid:12)
(cid:12)
s0 = s, a0 = a
(cid:12)
(cid:12)
(cid:12)

.

Observing that the action at time t does not inﬂuence the KL at t , this action value function can be
expressed recursively as

Qs

π (at , st ) =

(cid:69)
st +1∼p(· |st ,at )

(cid:2)r (st , at ) + γV s

π (st +1)(cid:3) ,

where V s

π (s) = (cid:69)π [Qs

π (s, a)] − α DKL[π , ˜π |s] is known as the soft value function of π .

3. Quinoa

We would like to ﬁnd a soft-optimal policy π , which in every state maximises the soft Q-function Qs
π (a, s).
This in turn would locally maximise our objective J (π ) at each state under the assumption that Qs
π
is suﬃciently accurate. Solving for π comes with one caveat: ﬁnding the multiplier α trading the
regularisation with the reward is hard, as the magnitude of the reward can diﬀer signiﬁcantly over the
course of the training process. We therefore optimise Qs subject to a hard constraint on the relative
entropy between the policy π and the prior ˜π , as the parameters through that approach are easier to set
in practice [1, 12]:

π = argmax

π

(cid:69)
s∼µπ

(cid:2)Qs

π (a, s)π (a|s)(cid:3) subject to (cid:69)
s∼µπ

(cid:2) DKL[π , ˜π |s](cid:3) < ϵ and ∀s : (cid:69)

(cid:2)π (a|s)(cid:3) = 1,

a

where the last constraint ensures that π is normalised. We solve this constrained optimisation problem
using the method of Lagrange multipliers, automatically obtaining an optimal α for a given ϵ. The details
of this procedure are given in the Appendix.

2

Quinoa: a Q-function You Infer Normalized Over Actions

Figure 1 | The performance of Quinoa compared with SVG(0) [9]. Shown are the median performance
across 5 seeds, together with the minimum and maximum performance. We show these performances
for three domains from the DeepMind Control Suite as illustrated on the bottom right; from left to right:
Cheetah, Walker and Hopper.

Taking into account the constraint that π (a|s) is a distribution, we obtain that the optimal policy

π (a|s) for a given Qs

π (a, s) is given by

π (a|s) =

˜π (a|s) exp

∫

˜π (a(cid:48)|s) exp

(cid:16) Q s

π (a,s)
α

(cid:16) Q s

π (a(cid:48),s)
α

(cid:17)

(cid:17)

da(cid:48)

this not only deﬁnes an improved policy, but also establishes a relation between the soft action-value
Qs
π (a, s) and the policy π . To act according to π , we need a way to infer actions from Q-values. There
are three main viable approaches. Firstly, we could learn a parametric Q-function and then project the
exponentiated Q-values onto a parametric π . This approach has been considered in Haarnoja et al. [8]
and was extended to use rich parametric policies in Haarnoja et al. [7]. Secondly, we could aim to sample
from π directly, for instance via importance sampling based on samples from ˜π (a|s) reweighed with
exp(Qs
π (a, s)/α) [6]. This approach is known to have high-variance and is compute intensive. Finally, we
could parameterise the Q-function, restricting its expressiveness, such that we can obtain π in closed
form. Using a Gaussian distribution for the policy would recover the NAF setting [5], but this restricts
Qs
π (a, s) to be quadratic in action space. In this paper we follow the third approach yet make use of a rich
policy class of normalising ﬂows, allowing the soft action-value function Qs
π to be a universal function
approximator.

To achieve this we ﬁrst solve Equation 1 for Qs

π (a, s) to ﬁnd the following equation.

Qs

π (a, s) = V s

π (s) + α log

where V s

π (s) = α log

˜π (a|s) exp(Qs

π (a, s)/α) da

∫

π (a|s)
˜π (a|s)

(1)

(2)

3

Quinoa: a Q-function You Infer Normalized Over Actions

(cid:2)Qs

π (a, s)(cid:3) = V s

It makes sense to call the ﬁrst term the soft value function, since taking the expectation of both sides of
π (s) + α DKL[π , ˜π |s], which corresponds to the deﬁnition of the soft
the equation gives (cid:69)π
value function. Additionally, the second term in Equation 2 can be interpreted as a soft version of the
advantage function A(a, s) = α log(π (a|s)/ ˜π (a|s)), where diﬀerences in log-likelihoods are interpreted
as advantages. Given this sum, a natural way to parameterise Q becomes apparent. We can choose
to parameterise V s
π (s) as a deep neural network, and π (a|s) as a density modelled by a normalising
ﬂow [13]. These can be universal density estimators [10] and hence allow Qs
π to model arbitrary
functions. In the following, we chose to use a Real NVP architecture [4] for our policy, as we can both
sample and infer the probability density function eﬃciently1. In order to condition the Real NVP on the
state s, we concatenate s to the input of every neural network inside the Real NVP.

Using this parametrisation, we can ﬁt Qs

π (a, s) directly by minimising the squared temporal diﬀerence

error

(cid:20) (cid:16)

min
θ,ϕ

(cid:69)µπ (s),p

r (s, a) + γV s

π (s (cid:48); ϕ (cid:48)) − Qs

π (a, s; θ, ϕ)

| s (cid:48) ∼ p(s (cid:48)|s, a)

,

(3)

(cid:17) 2

(cid:21)

where θ denote policy parameters, ϕ are value function parameters and ϕ (cid:48) are the parameters of a
target value function, that are periodically copied from ϕ; and Qs
π , V s are given as in Equation 2. We
approximate the expectation over transition and state visitation distribution by samples from a replay
buﬀer. A full algorithm listing of the procedure is given in Algorithms 1 and 2.

Figure 2 | An illustration of the distribution of the policy π (a|s) in the state of the walker s illustrated
on the left. In the three scatter plots on the right we plotted 1000 randomly drawn a ∼ π (·|s), where
respectively action dimension a1 is scattered against a2, a3 against a4 and a5 against a6. To make the
density diﬀerences clearer, we added a kernel density estimation on these samples. Note the ﬁnite
support of the policy. As can be seen, some properties of the richer policy class are utilised, such as having
high-skew and non-linearly correlated exploration noise. In the ﬁrst scatter plot the policy also displays
multimodal behaviour, with a mode in at least three corners of the action domain of the marginalised
distribution. It is apparent that the resulting policy is not normally distributed.

4. Results

We ran experiments across three domains from the DeepMind control suite [17], the walker, the cheetah
and the hopper, as depicted in Figure 1. Our neural networks were initialised such that Q(a, s) is
identically zero in all states and actions, which means that our initial policy π (a|s) is exactly uniform.
All neural networks have weight normalisation with an initialisation based on the statistics of the ﬁrst
batch [14]. In order to deal with the gradients of the squashing operations in the Real NVP, we clip the
gradient norm to 1. We set the learning rate to 0.001 and update the target network every 1000 steps.

1We note that to the best of our knowledge there is no formal proof that Real NVP’s are universal density function
approximators, nor any counterexamples of why they would not be. Other ﬂows such as Neural Autoregressive Flows [10]
could be used when a formal proof is required.

4

Quinoa: a Q-function You Infer Normalized Over Actions

As we can see, the performance of Quinoa is similar to the one obtained by SVG(0) [9] for the

cheetah and the walker tasks. On the hopper task, the performance is slightly lacking behind.

When analysing the policies obtained, we ﬁnd that using this richer class of distributions for a policy
shows a distinct behaviour which is hard to obtain using a Gaussian policy. First of all, the actions samples
from this policy have limited support. As shown in Figure 2, we can observe that in some states the policy
has high-skew and non-linearly correlated exploration noise during the training process. Therefore, it is
clearly not following a normal distribution.

Moreover, the policy shows some multimodal behaviour during training. This can be explained by the
fact that the converged policy for the walker domain has actions in the extremities for most states. The
policy depicted has not converged yet, but it has learned that it prefers to take actions in the extremities.
In this state however, it does not know which one yet, resulting in a multimodal distribution. The scatter
plots also show how some dimensions of the action space have already collapsed, while others remain
high in variance in order to keep the entropy large and keep exploring the action space.

5. Conclusion

In this paper, we describe a new parametrisation of the soft Q-function, such that the optimal policy
can be obtained in closed form. This approach removes the need for a policy optimisation step from the
learning process, simplifying standard actor-critic algorithms. We show that our algorithm is able to
work across a range of tasks. We have illustrated that the policy is able to have an arbitrary distribution
for its exploration noise. Moreover, we have shown that given this additional degree of freedom, the
resulting policy does not show a Gaussian behaviour, with long tails and non-linearly correlated noise.
We ﬁnd in some states the actions of the policy are distributed multimodally. In the future, we will work
on expanding this approach to harder tasks.

References

[1] A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller. Maximum a

posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.

[2] L. C. Baird. Reinforcement learning in continuous time: Advantage updating. In Neural Networks,
1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on,
volume 4, pages 2448–2453. IEEE, 1994.

[3] J. Degrave, A. Abdolmaleki, J. Springenberg, N. Heess, and M. Riedmiller. Quinoa: a Q-function

you infer normalized over actions. In Deep RL Workshop/NeurIPS, 2018.

[4] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. arXiv preprint

arXiv:1605.08803, 2016.

[5] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep q-learning with model-based

acceleration. In International Conference on Machine Learning, pages 2829–2838, 2016.

[6] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based

policies. arXiv preprint arXiv:1702.08165, 2017.

[7] T. Haarnoja, K. Hartikainen, P. Abbeel, and S. Levine. Latent space policies for hierarchical

reinforcement learning. arXiv preprint arXiv:1804.02808, 2018.

[8] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Oﬀ-policy maximum entropy deep

reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.

[9] N. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control

5

Quinoa: a Q-function You Infer Normalized Over Actions

arXiv:1804.00779, 2018.

Atlanta, 2010.

arXiv:1505.05770, 2015.

policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pages
2944–2952, 2015.

[10] C.-W. Huang, D. Krueger, A. Lacoste, and A. Courville. Neural autoregressive ﬂows. arXiv preprint

[11] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–1190, 2008.

[12] J. Peters, K. Mülling, and Y. Altun. Relative entropy policy search. In AAAI, pages 1607–1612.

[13] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. arXiv preprint

[14] T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in Neural Information Processing Systems, pages
901–909, 2016.

[15] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In

International Conference on Machine Learning, pages 1889–1897, 2015.

[16] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in neural information processing systems, pages
1057–1063, 2000.

[17] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel,

A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.

This is a free, open access paper provided by DeepMind. The ﬁnal version of this work was published on
workshops at NeurIPS on 2018-12-08. Cite as:

J. Degrave, A. Abdolmaleki, J. Springenberg, N. Heess, and M. Riedmiller. Quinoa: a Q-function you
infer normalized over actions. In Deep RL Workshop/NeurIPS, 2018

Citing this work

Funding

This research was funded by DeepMind. The authors declare no competing ﬁnancial interests.

6

Quinoa: a Q-function You Infer Normalized Over Actions

6. Appendix

6.1. Solving the Q-function in closed form

We want to ﬁnd the optimal policy π (a|s) which optimises Qs
relative entropy between the policy π and the prior ˜π .
π (a, s)π (a|s)(cid:3) subject to (cid:69)
s∼µπ

π = argmax

(cid:69)
s∼µπ

(cid:2)Qs

π

π (a, s) subject to a hard constraint on the

(cid:2) DKL[π , ˜π |s](cid:3) < ϵ and ∀s : (cid:69)

(cid:2)π (a|s)(cid:3) = 1,

a

where the last constraint ensures that π is normalised. We solve this constrained optimisation problem
using the method of Lagrange multipliers, obtaining an optimal α for a given ϵ automatically. We solve
this constrained optimisation problem using the method of Lagrange multipliers. Here, the Lagrange
function we construct is

L = (cid:69)
s∼µπ

(cid:2)Qs

π (a, s)π (a|s)(cid:3) + α (cid:0)ϵ − (cid:69)
s∼µπ

(cid:2) DKL[π , ˜π |s](cid:3) (cid:1) + β(s)(cid:0)1 − (cid:69)

(cid:2)π (a|s)(cid:3) (cid:1)

a

with Lagrange multipliers α ≥ 0 and β(s). Next we maximise the Lagrangian L w.r.t the primal variable
π . The derivative w.r.t π (a|s) for an action a in a state s, making the approximation that µπ (s) and
Qs

π (a, s) are independent of π is the following:

Setting this derivative to zero, we ﬁnd the policy which satisﬁes the Lagrangian in every state s.

∂ L
∂π

= Qs

π (a, s) − α log

− α − β(s).

π (a|s)
˜π (a|s)

π (a|s) = ˜π exp(Qs

π (a, s)/α) exp(−1 − β(s)/α)

Taking into account the constraint that π (a|s) is a distribution, we obtain that the optimal policy π (a|s)
for a given Qs
π (a, s)/α)2, from which Quinoa derives its name.
Note that π (a|s) is always positive, so we have fulﬁlled the two conditions for it to be a probability
density function.

π (a, s) is given by softmaxa(log ˜π (a|s) + Qs

At this point we can derive the dual function, by substituting the parametrisation for Qs

π (a, s) in the

Lagrangian L.

(cid:34)

(cid:34)

L(α) = αϵ + α (cid:69)
s∼µπ

log (cid:69)
a∼π

exp

(cid:16)V s
π (s)
α

+ log

(cid:17)

π (a|s)
˜π (a|s)

(cid:35) (cid:35)

When we minimise this convex function in α , we ﬁnd the optimal temperature for our distribution. Even
though there is no analytic solution to this equation, the temperature can be computed eﬃciently using
regula falsi to ﬁnd the zero in the derivative under the constraint α ≥ 0:

∂ L(α)
∂α

(cid:34)

(cid:34)

= ϵ + (cid:69)
s∼µπ

log (cid:69)
a∼π

exp

(cid:35)

(cid:17)

(cid:16)Qs

π (a, s)
α

− (cid:69)
a∼π

(cid:34)

Qs

π (a, s)
α

(cid:35) (cid:35)

(cid:16)Qs

(cid:17)

π (a, s)
α

softmax
a

Finally, we have all the elements to write the algorithm for both the actor and the learner, which run

asynchronously in parallel. These are written out in Algorithm 1 and Algorithm 2.

2Here, we deﬁne a continuous softmax operation as softmaxx (y) =

∫

exp(y)
exp(y) dx

7

Quinoa: a Q-function You Infer Normalized Over Actions

Algorithm 1 Actor algorithm
Input: policy π (a|s) with parameters θ shared with the learner
Input: replay buﬀer ρ shared with the learner
Input: environment e
1: loop while π (a|s) not converged
2:

loop while e not terminated

3:

4:

5:

6:

get state s from environment e
sample a from policy π (·|s)
send a to environment e

end loop
send trajectory to replay buﬀer ρ

7:
8: end loop

π (s) with parameters ϕ

Algorithm 2 Learner algorithm
Input: policy π (a|s) with parameters θ shared with the actor
Input: replay buﬀer ρ shared with the actor
Input: prior policy ˜π (a|s) with parameters ˜θ
Input: soft value V s
Input: KL-constraint ϵ
Input: Discount γ
1: loop while π (a|s) not converged
sample (s, a, r, s (cid:48)) from ρ
2:
DKL = log π (a|s) − log ˜π (a|s)
ﬁnd optimal α minimising α ϵ + α log (cid:69)a[exp (V s
q = α DKL +V s
q(cid:48) = r + γV s
optimise ϕ and θ to minimise (q − q(cid:48))2 with gradient descent
every 1000 iterations: ˜π (a|s) ← π (a|s)

π (s)
π (s (cid:48)) and stop gradient

5:

3:

4:

7:

6:

π (s)/α + DKL)]

8:
9: end loop

8

