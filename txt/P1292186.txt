Deep Ordinal Regression Network for Monocular Depth Estimation

Huan Fu1 Mingming Gong2,3 Chaohui Wang4 Kayhan Batmanghelich2 Dacheng Tao1
1UBTECH Sydney AI Centre, SIT, FEIT, The University of Sydney, Australia
2Department of Biomedical Informatics, University of Pittsburgh
3Department of Philosophy, Carnegie Mellon University
4Universit´e Paris-Est, LIGM (UMR 8049), CNRS, ENPC, ESIEE Paris, UPEM, Marne-la-Vall´ee, France

hufu6371@uni., dacheng.tao@

sydney.edu.au

mig73, kayhan@

pitt.edu

chaohui.wang@u-pem.fr

{

}

{

}

8
1
0
2
 
n
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
6
4
4
2
0
.
6
0
8
1
:
v
i
X
r
a

Abstract

Monocular depth estimation, which plays a crucial role
in understanding 3D scene geometry, is an ill-posed prob-
lem. Recent methods have gained signiﬁcant improve-
ment by exploring image-level information and hierarchi-
cal features from deep convolutional neural networks (DC-
NNs). These methods model depth estimation as a regres-
sion problem and train the regression networks by mini-
mizing mean squared error, which suffers from slow con-
vergence and unsatisfactory local solutions. Besides, ex-
isting depth estimation networks employ repeated spatial
pooling operations, resulting in undesirable low-resolution
feature maps. To obtain high-resolution depth maps, skip-
connections or multi-layer deconvolution networks are re-
quired, which complicates network training and consumes
much more computations. To eliminate or at least largely
reduce these problems, we introduce a spacing-increasing
discretization (SID) strategy to discretize depth and recast
depth network learning as an ordinal regression problem.
By training the network using an ordinary regression loss,
our method achieves much higher accuracy and faster con-
vergence in synch. Furthermore, we adopt a multi-scale
network structure which avoids unnecessary spatial pool-
ing and captures multi-scale information in parallel.

The method described in this paper achieves state-of-
the-art results on four challenging benchmarks, i.e., KITTI
[18], ScanNet [10], Make3D [51], and NYU Depth v2 [43],
and win the 1st prize in Robust Vision Challenge 2018.
Code has been made available at: https://github.
com/hufu6371/DORN .

1. Introduction

Estimating depth from 2D images is a crucial step of
scene reconstruction and understanding tasks, such as 3D
object recognition, segmentation, and detection. In this pa-

Figure 1: Estimated Depth by DORN. MSE: Training our net-
work via MSE in log space, where ground truths are continuous
depth values. DORN: The proposed deep ordinal regression net-
work. Depth values in the black part are not provided by KITTI.

per, we examine the problem of Monocular Depth Estima-
tion from a single image (abbr. as MDE hereafter).

Compared to depth estimation from stereo images or
video sequences, in which signiﬁcant progresses have been
made [21, 31, 28, 46], the progress of MDE is slow. MDE is
an ill-posed problem: a single 2D image may be produced
from an inﬁnite number of distinct 3D scenes. To overcome
this inherent ambiguity, typical methods resort to exploiting
statistically meaningful monocular cues or features, such as
perspective and texture information, object sizes, object lo-
cations, and occlusions [51, 26, 34, 50, 28].

Recently,

some works have signiﬁcantly improved
the MDE performance with the use of DCNN-based models
[40, 57, 48, 11, 30, 33, 35, 3], demonstrating that deep fea-
tures are superior to handcrafted features. These methods
address the MDE problem by learning a DCNN to estimate
the continuous depth map. Since this problem is a standard
regression problem, mean squared error (MSE) in log-space
or its variants are usually adopted as the loss function. Al-
though optimizing a regression network can achieve a rea-

1

sonable solution, we ﬁnd that the convergence is rather slow
and the ﬁnal solution is far from satisfactory.

In addition, existing depth estimation networks [11, 17,
33, 35, 40, 59] usually apply standard DCNNs designed ini-
tially for image classiﬁcation in a full convolutional manner
as the feature extractors. In these networks, repeated spa-
tial pooling quickly reduce the spatial resolution of feature
maps (usually stride of 32), which is undesirable for depth
estimation. Though high-resolution depth maps can be ob-
tained by incorporating higher-resolution feature maps via
multi-layer deconvolutional networks [35, 17, 33], multi-
scale networks [40, 11] or skip-connection [59], such a pro-
cessing would not only require additional computational
and memory costs, but also complicate the network archi-
tecture and the training procedure.

In contrast to existing developments for MDE, we pro-
pose to discretize continuous depth into a number of inter-
vals and cast the depth network learning as an ordinal re-
gression problem, and present how to involve ordinal re-
gression into a dense prediction task via DCNNs. More
speciﬁcally, we propose to perform the discretization using
a spacing-increasing discretization (SID) strategy instead of
the uniform discretization (UD) strategy, motivated by the
fact that the uncertainty in depth prediction increases along
with the underlying ground-truth depth, which indicates that
it would be better to allow a relatively larger error when
predicting a larger depth value to avoid over-strengthened
inﬂuence of large depth values on the training process. Af-
ter obtaining the discrete depth values, we train the network
by an ordinal regression loss, which takes into account the
ordering of discrete depth values.

To ease network training and save computational cost,
we introduce a network architecture which avoids unnec-
essary subsampling and captures multi-scale information in
a simpler way instead of skip-connections. Inspired by re-
cent advances in scene parsing [62, 4, 6, 64], we ﬁrst re-
move subsampling in the last few pooling layers and apply
dilated convolutions to obtain large receptive ﬁelds. Then,
multi-scale information is extracted from the last pooling
layer by applying dilated convolution with multiple dilation
rates. Finally, we develop a full-image encoder which cap-
tures image-level information efﬁciently at a signiﬁcantly
lower cost of memory than the fully-connected full-image
encoders [2, 12, 11, 37, 30]. The whole network is trained
in an end-to-end manner without stage-wise training or it-
erative reﬁnement. Experiments on four challenging bench-
marks, i.e., KITTI [18], ScanNet [10], Make3D [51, 50] and
NYU Depth v2 [43], demonstrate that the proposed method
achieves state-of-the-art results, and outperforms recent al-
gorithms by a signiﬁcant margin.

The remainder of this paper is organized as follows. Af-
ter a brief review of related literatures in Sec. 2, we present
In Sec. 4, be-
in Sec. 3 the proposed method in detail.

sides the qualitative and quantitative performance on those
benchmarks, we also evaluate multiple basic instantiations
of the proposed method to analyze the effects of those core
factors. Finally, we conclude the whole paper in Sec. 5.

2. Related Work

Depth Estimation is essential for understanding the 3D
structure of scenes from 2D images. Early works fo-
cused on depth estimation from stereo images by devel-
oping geometry-based algorithms [52, 14, 13] that rely on
point correspondences between images and triangulation to
estimate the depth. In a seminal work [50], Saxena et al.
learned the depth from monocular cues in 2D images via su-
pervised learning. Since then, a variety of approaches have
been proposed to exploit the monocular cues using hand-
crafted representations [51, 26, 34, 38, 8, 32, 1, 55, 47, 16,
22, 61]. Since handcrafted features alone can only cap-
ture local information, probabilistic graphic models such
as Markov Random Fields (MRFs) are often built based
on these features to incorporate long-range and global cues
[51, 65, 41]. Another successful way to make use of global
cues is the DepthTransfer method [28] which uses GIST
global scene features [45] to search for candidate images
that are “similar” to the input image from a database con-
taining RGBD images.

Given the success of DCNNs in image understanding,
many depth estimation networks have been proposed in re-
cent years [20, 63, 37, 42, 54, 58, 48, 40, 29]. Thanks
to multi-level contextual and structural information from
powerful very deep networks (e.g., VGG [56] and ResNet
[24]), depth estimation has been boosted to a new accuracy
level [11, 17, 33, 35, 59]. The main hurdle is that the re-
peated pooling operations in these deep feature extractors
quickly decrease the spatial resolution of feature maps (usu-
ally stride 32). Eigen et al. [12, 11] applied multi-scale net-
works which stage-wisely reﬁne estimated depth map from
low spatial resolution to high spatial resolution via indepen-
dent networks. Xie et al. [59] adopted the skip-connection
strategy to fuse low-spatial resolution depth map in deeper
layers with high-spatial resolution depth map in lower lay-
ers. More recent works [17, 33, 35] apply multi-layer
deconvolutional networks to recover coarse-to-ﬁne depth.
Rather than solely relying on deep networks, some methods
incorporate conditional random ﬁelds to further improve
the quality of estimated depth maps [57, 40]. To improve
efﬁciency, Roy and Todorovic [48] proposed the Neural
Regression Forest method which allows for parallelizable
training of “shallow” CNNs.

Recently, unsupervised or semi-supervised learning is
introduced to learn depth estimation networks [17, 33].
These methods design reconstruction losses to estimate
the disparity map by recovering a right view with a left
view. Also, some weakly-supervised methods considering

Figure 2: Illustration of the network architecture. The network consists of a dense feature extractor, multi-scale feature learner (ASPP),
cross channel information learner (the pure 1 × 1 convolutional branch), a full-image encoder and an ordinal regression optimizer. The
Conv components here are all with kernel size of 1 × 1. The ASPP module consists of 3 dilated convolutional layers with kernel size of
3 × 3 and dilated rate of 6, 12 and 18 respectively [6]. The supervised information of our network is discrete depth values output by the
discretization using the SID strategy. The whole network is optimized by our ordinal regression training loss in an end-to-end fashion.

pair-wise ranking information were proposed to roughly
estimate and compare depth [66, 7].

Ordinal Regression [25, 23] aims to learn a rule to predict
labels from an ordinal scale. Most literatures modify well-
studied classiﬁcation algorithms to address ordinal regres-
sion algorithms. For example, Shashua and Levin [53] han-
dled multiple thresholds by developing a new SVM. Cam-
mer and Singer [9] generalized the online perceptron al-
gorithms with multiple thresholds to do ordinal regression.
Another way is to formulate ordinal regression as a set of
binary classiﬁcation subproblems. For instance, Frank and
Hall [15] applied some decision trees as binary classiﬁers
for ordinal regression. In computer vision, ordinal regres-
sion has been combined with DCNNs to address the age
estimation problem [44].

3. Method

This section ﬁrst introduces the architecture of our deep
ordinal regression network; then presents the SID strategy
to divide continuous depth values into discrete values; and
ﬁnally details how the network parameters can be learned
in the ordinal regression framework.

3.1. Network Architecture

As shown in Fig. 2, the divised network consists of two
parts, i.e., a dense feature extractor and a scene understand-
ing modular, and outputs multi-channel dense ordinal labels
given an image.

3.1.1 Dense Feature Extractor

Previous depth estimation networks [11, 17, 33, 35, 40, 59]
usually apply standard DCNNs originally designed for im-
age recognition as the feature extractor. However, the re-

peated combination of max-pooling and striding signiﬁ-
cantly reduces the spatial resolution of the feature maps.
Also, to incorporate multi-scale information and reconstruct
high-resolution depth maps, some partial remedies, includ-
ing stage-wise reﬁnement [12, 11], skip connection [59]
and multi-layer deconvolution network [17, 33, 35] can be
adopted, which nevertheless not only requires additional
computational and memory cost, but also complicates the
network architecture and the training procedure. Following
some recent scene parsing network [62, 4, 6, 64], we advo-
cate removing the last few downsampling operators of DC-
NNs and inserting holes to ﬁlters in the subsequent conv
layers, called dilated convolution, to enlarge the ﬁeld-of-
view of ﬁlters without decreasing spatial resolution or in-
creasing number of parameters.

3.1.2 Scene Understanding Modular

Figure 3: Full-Image Encoders. Top: the full-image encoder
implemented by pure f c layers [12, 11, 2] (δ < 1.25: 0.910);
Bottom: Our proposed encoder (δ < 1.25: 0.915).

The scene understanding modular consists of three par-
allel components, i.e., an atrous spatial pyramid pooling

(ASPP) module [5, 6], a cross-channel leaner, and a full-
image encoder. ASPP is employed to extract features from
multiple large receptive ﬁelds via dilated convolutional op-
erations. The dilation rates are 6, 12 and 18, respectively.
1 convolutional branch can learn complex
The pure 1
cross-channel interactions. The full-image encoder captures
global contextual information and can greatly clarify local
confusions in depth estimation [57, 12, 11, 2].

×

h

F

×

×

×

F
h

C ×

w from F with dimension C

Though previous methods have incorporated full-image
encoders, our full-image encoder contains fewer parame-
with
ters. As shown in Fig. 3, to obtain global feature
w,
dimension
a common fc-fashion method accomplishes this by using
connects
fully-connected layers, where each element in
to all the image features, implying a global understanding
of the entire image. However, this method contains a pro-
hibitively large number of parameters, which is difﬁcult to
train and is memory consuming. In contrast, we ﬁrst make
use of an average pooling layer with a small kernel size and
stride to reduce the spatial dimensions, followed by a f c
layer to obtain a feature vector with dimension
. Then,
channels of feature maps
we treat the feature vector as
C
with spatial dimensions of 1
1, and add a conv layer with
1 as a cross-channel parametric pool-
the kernel size of 1
ing structure. Finally, we copy the feature vector to
along
F
share the same
spatial dimensions so that each location of
understanding of the entire image.

×

×

F

C

The obtained features from the aforementioned compo-
nents are concatenated to achieve a comprehensive under-
standing of the input image. Also, we add two additional
convolutional layers with the kernel size of 1
1, where the
former one reduces the feature dimension and learns com-
plex cross-channel interactions, and the later one transforms
the features into multi-channel dense ordinal labels.

×

3.2. Spacing-Increasing Discretization

Figure 4: Discrete Intervals. Illustration of UD (middle) and
SID (bottom) to discretize depth interval [α, β] into ﬁve sub-
intervals. See Eq. 1 for details.

To quantize a depth interval [α, β] into a set of repre-
sentative discrete values, a common way is the uniform
discretization (UD). However, as the depth value becomes

larger, the information for depth estimation is less rich,
meaning that the estimation error of larger depth values is
generally larger. Hence, using the UD strategy would in-
duce an over-strengthened loss for the large depth values.
To this end, we propose to perform the discretization using
the SID strategy (as shown in Fig. 4), which uniformed dis-
cretizes a given depth interval in log space to down-weight
the training losses in regions with large depth values, so that
our depth estimation network is capable to more accurately
predict relatively small and medium depth and to rationally
estimate large depth values. Assuming that a depth interval
[α, β] needs to be discretized into K sub-intervals, UD and
SID can be formulated as:

UD:

SID:

ti = α + (β
ti = elog(α)+ log(β/α)∗i

α)

−

∗

K

,

i/K,

(1)

where ti ∈ {
are discretization thresholds. In
our paper, we add a shift ξ to both α and β to obtain α∗ and
β∗ so that α∗ = α + ξ = 1.0, and apply SID on [α∗, β∗].

t0, t1, ..., tK}

3.3. Learning and Inference

After obtaining the discrete depth values, it is straightfor-
ward to turn the standard regression problem into a multi-
class classiﬁcation problem, and adopts softmax regression
loss to learn the parameters in our depth estimation net-
work. However, typical multi-class classiﬁcation losses ig-
nore the ordered information between the discrete labels,
while depth values have a strong ordinal correlation since
they form a well-ordered set. Thus, we cast the depth es-
timation problem as an ordinal regression problem and de-
velop an ordinal loss to learn our network parameters.

×

Let χ = ϕ(I, Φ) denote the feature maps of size W

×
C given an image I, where Φ is the parameters involved
H
in the dense feature extractor and the scene understanding
modular. Y = ψ(χ, Θ) of size W
2K denotes
×
the ordinal outputs for each spatial locations, where Θ =
1) contains weight vectors. And l(w,h) ∈
(θ0, θ1, ..., θ2K
−
1
0, 1, ..., K
is the discrete label produced by SID at
{
}
spatial location (w, h). Our ordinal loss
(χ, Θ) is deﬁned
as the average of pixelwise ordinal loss Ψ(h, w, χ, Θ) over
the entire image domain:

H

×

−

L

(χ, Θ) =

L

−

W

1

H

1

−

−

1

N
l(w,h)−1

w=0
(cid:88)

(cid:88)h=0

Ψ(w, h, χ, Θ),

Ψ(h, w, χ, Θ) =

log(

k
(w,h))

P

(2)

(cid:88)k=0
K

1

−

+

(cid:88)k=l(w,h)

(log(1

k
(w,h))),

− P

k

(w,h) = P (ˆl(w,h) > k

χ, Θ),

|

P

Figure 5: Depth Prediction on KITTI. Image, ground truth, Eigen [12], LRC [19], and our DORN. Ground truth has been interpolated
for visualization. Pixels with distance > 80m in LRC are masked out.

N

= W

H, and ˆl(w,h) is the estimated discrete
where
value decoding from y(w,h). We choose softmax function to
k
(w,h) from y(w,h,2k) and y(w,h,2k+1) as follows:
compute

×

P

k
(w,h) =

P

ey(w,h,2k+1)
ey(w,h,2k) + ey(w,h,2k+1)

,

(3)

where y(w,h,i) = θT

χ. Minimizing
i x(w,h), and x(w,h) ∈
(χ, Θ) ensures that predictions farther from the true label

L
incur a greater penalty than those closer to the true label.

The minimization of

(χ, Θ) can be done via an iterative
optimization algorithm. Taking derivate with respect to θi,
the gradient takes the following form:

L

∂

L

(χ, Θ)
∂θi

=

−

W

1

H

1

−

−

1

∂Ψ(w, h, χ, Θ)
∂θi

,

∂Ψ(w, h, χ, Θ)
∂θ2k+1
∂Ψ(w, h, χ, Θ)
∂θ2k

(cid:88)h=0

w=0
(cid:88)

N
∂Ψ(w, h, χ, Θ)
∂θ2k

,

=

−

= x(w,h)η(l(w,h) > k)(

(4)

1)

k
(w,h) −
k
(w,h),

P
k)

+ x(w,h)η(l(w,h) ≤
where k
) is an indicator function
, and η(
1
·
}
such that η(true) = 1 and η(false) = 0. We the can optimize
our network via backpropagation.

0, 1, ..., K

∈ {

−

P

In the inference phase, after obtaining ordinal labels for
each position of image I, the predicted depth value ˆd(w,h)
is decoded as:

ˆd(w,h) =

tˆl(w,h)

+ tˆl(w,h)+1
2

ξ,

−

ˆl(w,h) =

k
(w,h) >= 0.5).

η(

P

K

1

−

(cid:88)k=0

4. Experiments

To demonstrate the effectiveness of our depth estimator,
we present a number of experiments examining different
aspects of our approach. After introducing the implemen-
tation details, we evaluate our methods on three challeng-
ing outdoor datasets, i.e. KITTI [18], Make3D [50, 51] and
NYU Depth v2 [43]. The evaluation metrics are following
previous works [12, 40]. Some ablation studies based on
KITTI are discussed to give a more detailed analysis of our
method.
Implementation Details We implement our depth estima-
tion network based on the public deep learning platform
Caffe [27]. The learning strategy applies a polynomial de-
cay with a base learning rate of 0.0001 and the power of 0.9.
Momentum and weight decay are set to 0.9 and 0.0005 re-
spectively. The iteration number is set to 300K for KITTI,
50K for Make3D, and 3M for NYU Depth v2, and batch
size is set to 3. We ﬁnd that further increasing the itera-
tion number can only slightly improve the performance. We
adopt both VGG-16 [56] and ResNet-101 [24] as our fea-
ture extractors, and initialize their parameters via the pre-
trained classiﬁcation model on ILSVRC [49]. Since fea-
tures in ﬁrst few layers only contain general low-level infor-
mation, we ﬁxed the parameters of conv1 and conv2 blocks
in ResNet after initialization. Also, the batch normaliza-
tion parameters in ResNet are directly initialized and ﬁxed
during training progress. Data augmentation strategies are
following [12].
In the test phase, we split each image to
some overlapping windows according the cropping method
in the training phase, and obtain the predicted depth values
in overlapped regions by averaging the predictions.

(5)

4.1. Benchmark Perfomance

KITTI The KITTI dataset [18] contains outdoor scenes
1241 captured by
with images of resolution about 375

×

Method
Ofﬁcial Baseline
DORN

abs rel.
0.25
0.14

imae
0.17
0.10

irmse
0.21
0.13

log mae
0.24
0.13

log rmse mae
0.42
0.22

0.29
0.17

rmse
0.53
0.29

scale invar.
0.05
0.02

sq. rel.
0.14
0.06

Table 1: Scores on the online ScanNet evaluation server. See https://goo.gl/8keUQN.

and evaluate our method on the ScanNet online test server.

Method
Ofﬁcial Baseline
DORN

SILog sqErrorRel absErrorRel
18.19
11.77

14.24
8.78

7.32
2.23

iRMSE
18.50
12.98

Table 2: Scores on the online KITTI evaluation server. See
https://goo.gl/iXuhiN.

×

×

×

cameras and depth sensors in a driving car. All the 61 scenes
from the “city”, “residential”, “road” and “Campus” cate-
gories are used as our training/test sets. We test on 697 im-
ages from 29 scenes split by Eigen et al. [12], and train on
about 23488 images from the remaining 32 scenes. We train
our model on a random crop of size 385
513. For some
other details, we set the maximal ordinal label for KITTI
as 80, and evaluate our results on a pre-deﬁned center crop-
ping following [12] with the depth ranging from 0m to 80m
and 0m to 50m. Note that, a single model is trained on the
full depth range, and is tested on data with different depth
ranges.
Make3D The Make3D dataset [50, 51] contains 534 out-
door images, 400 for training, and 134 for testing, with the
1704, and provides the ground truth
resolution of 2272
×
305. We re-
depth map with a small resolution of 55
duce the resolution of all images to 568
426, and train our
model on a random crop of size 513
385. Following previ-
ous works, we report C1 (depth range from 0m to 80m) and
C2 (depth range from 0m to 70m) error on this dataset us-
ing three commonly used evaluation metrics [28, 40]. For
the VGG model, we train our DORN on a depth range of
0m to 80m from scratch (ImageNet model), and evaluate
results using the same model for C1 and C2 . However,
for ResNet, we learn two separate models for C1 and C2
respectively.
NYU Depth v2 The NYU Depth v2 [43] dataset contains
464 indoor video scenes taken with a Microsoft Kinect cam-
era. We train our DORN using all images (about 120K)
from the 249 training scenes, and test on the 694-image
test set following previous works. To speed up training, all
the images are reduced to the resolution of 288
384 from
640. And the model are trained on random crops of
480
×
size 257
353. We report our scores on a pre-deﬁned center
cropping by Eigen [12].
ScanNet The ScanNet [10] dataset is also a challenging
benchmark which contains various indoor scenes. We train
our model on the ofﬁcially provided 24353 training and
513,
validation images with a random crop size of 385

×

×

×

×

Figure 6: Depth Prediction on Make3D. Image, ground truth,
and our DORN. Pixels with distance > 70m are masked out.

Performance Tab. 3 and Tab. 4 give the results on two
outdoor datasets, i.e., KITTI and Make3D. It can be seen
that our DORN improves the accuracy by 5% ∼ 30% in
terms of all metrics compared with previous works in all
settings. Some qualitative results are shown in Fig. 5 and
Fig. 6. In Tab. 5, our DORN outperforms other methods on
NYU Depth v2, which is one of the largest indoor bench-
marks. The results suggest that our method is applicable to
both indoor and outdoor data. We evaluate our method on
the online KITTI evaluation server and the online ScanNet
evaluation server. As shown in Tab. 2 and 1, our DORN
signiﬁcantly outperforms the ofﬁcially provided baselines.

4.2. Ablation Studies

We conduct various ablation studies to analyze the de-
tails of our approach. Results are shown in Tab. 6, Tab. 7,

Method

cap

higher is better

δ < 1.25 δ < 1.252

Make3D [51]
Eigen et al. [12]
Liu et al. [40]
LRC (CS + K) [19]
Kuznietsov et al. [33]
DORN (VGG)
DORN (ResNet)
Garg et al. [17]
LRC (CS + K) [19]
Kuznietsov et al. [33]
DORN (VGG)
DORN (ResNet)

0 - 80 m
0 - 80 m
0 - 80 m
0 - 80 m
0 - 80 m
0 - 80 m
0 - 80 m
0 - 50 m
0 - 50 m
0 - 50 m
0 - 50 m
0 - 50 m

0.601
0.692
0.647
0.861
0.862
0.915
0.932
0.740
0.873
0.875
0.920
0.936

0.820
0.899
0.882
0.949
0.960
0.980
0.984
0.904
0.954
0.964
0.982
0.985

lower is better

δ < 1.253 Abs Rel Squa Rel RMSE RMSElog
3.012
1.515
1.841
0.898
0.741
0.376
0.307
1.080
0.657
0.595
0.324
0.268

0.361
0.270
0.289
0.206
0.189
0.132
0.120
0.273
0.194
0.179
0.128
0.116

8.734
7.156
6.986
4.935
4.621
3.056
2.727
5.104
3.729
3.518
2.517
2.271

0.926
0.967
0.961
0.976
0.986
0.993
0.994
0.962
0.979
0.988
0.994
0.995

0.280
0.190
0.217
0.114
0.113
0.081
0.072
0.169
0.108
0.108
0.079
0.071

Table 3: Performance on KITTI. All the methods are evaluated on the test split by Eigen et al. [12]. LRC (CS + K): LRC pre-train their
model on Cityscapes and ﬁne tune on KITTI.

Method

Make3D [51]
Liu et al. [39]
DepthTransfer [28]
Liu et al. [41]
Li et al. [36]
Liu et al. [40]
Roy et al. [48]
Laina et al. [35]
LRC-Deep3D [59]
LRC [19]

MS-CRF [60]
DORN (VGG)
DORN (ResNet)

C1 error
log10
-
-

rms
-
-

rel
-
-

rel

C2 error
log10
0.370 0.187
0.379 0.148

rms
-
-

-

-

0.355 0.127 9.20 0.361 0.148 15.10
0.335 0.137 9.49 0.338 0.134 12.60
0.278 0.092 7.12 0.279 0.102 10.27
0.287 0.109 7.36 0.287 0.122 14.09
0.260 0.119 12.40
-
-
-
-
-

-
-
-
-
0.184 0.065 4.38 0.198
8.56
0.236 0.082 7.02 0.238 0.087 10.01
0.157 0.062 3.97 0.162 0.067 7.32

-
-
-
-

-
0.176 0.072 4.46
1.000 2.527 19.11
0.443 0.156 11.513
Kuznietsov et al. [33] 0.421 0.190 8.24

Table 4: Performance on Make3D. LRC-Deep3D [59] is adopt-
ing LRC [19] on Deep3D model [59].

Fig. 1, and Fig. 7, and discussed in detail.

-

-

-
-

-
-

δ1

δ3

δ2

rel

0.447 0.745 0.897 0.349

log10
rms
1.214
-
0.35 0.131 1.2
0.335 0.127 1.06
-

Method
Make3D [51]
DepthTransfer [28]
Liu et al. [41]
Ladicky et al. [34]
Li et al. [36]
Wang et al. [57]
Roy et al. [48]
Liu et al. [40]
Eigen et al. [11]

-
-
0.542 0.829 0.941
0.621 0.886 0.968 0.232 0.094 0.821
0.824
0.605 0.890 0.970 0.220
0.744
0.187
0.650 0.906 0.976 0.213 0.087 0.759
0.641
0.769 0.950 0.988 0.158
Chakrabarti et al. [2] 0.806 0.958 0.987 0.149
0.620
0.629 0.889 0.971 0.194 0.083 0.790
0.789 0.955 0.988 0.152 0.064 0.611
0.811 0.953 0.988 0.127 0.055 0.573
0.788 0.958 0.991 0.143 0.063 0.635
0.811 0.954 0.987 0.121 0.052 0.586
0.828 0.965 0.992 0.115 0.051 0.509

Laina et al. [35]
Li et al. [37]
Laina et al. [35]†
Li et al. [37]†
MS-CRF [60]†
DORN†

-
-

-
-

-

-

-

Table 5: Performance on NYU Depth v2. δi: δ < 1.25i. †:
ResNet based model.

4.2.1 Depth Discretization

Depth discretization is critical to performance improve-
ment, because it allows us to apply classiﬁcation and ordinal
regression losses to optimize the network parameters. Ac-
cording to scores in Tab. 6, training by regression on con-
tinuous depth seems to converge to a poorer solution than
the other two methods, and our ordinal regression network
achieves the best performance. There is an obvious gap be-
tween approaches where depth is discretized by SID and
UD, respectively. Besides, when replacing our ordinal re-
gression loss by an advantage regression loss (i.e. BerHu),
our DORN still obtain much higher scores. Thus, we can

conclude that: (i) SID is important and can further improve
the performance compared to UD; (i) discretizing depth and
training using a multi-class classiﬁcation loss is better than
training using regression losses; (iii) exploring the ordinal
correlation among depth drives depth estimation networks
to converge to even better solutions.

Furthermore, we also train the network using RMSElog
on discrete depth values obtained by SID, and report the re-
sults in Tab. 6. We can see that MSE-SID performs slightly
better than MSE, which demonstrates that quantization er-
rors are nearly ignorable in depth estimation. The beneﬁts
of discretization through the use of ordinal regression losses
far exceeds the cost of depth discretization.

Variant

Iteration

MSE
MSE-SID
MCC-UD
MCC-SID
DORN-UD
DORN-SID
berHu†
DORN†

1M
0.6M
0.3M
0.3M
0.3M
0.3M
0.6M
0.3M

δ < 1.25
0.864
0.865
0.892
0.906
0.900
0.915
0.909
0.932

higher is better
δ < 1.252
0.969
0.970
0.970
0.976
0.973
0.980
0.978
0.984

δ < 1.253
0.991
0.992
0.988
0.991
0.991
0.993
0.992
0.994

Abs Rel
0.109
0.108
0.093
0.084
0.091
0.081
0.086
0.072

lower is better

Squa Rel
0.527
0.520
0.474
0.417
0.452
0.376
0.385
0.307

RMSE
3.660
3.636
3.438
3.201
3.339
3.056
3.365
2.727

RMSElog
0.164
0.163
0.155
0.142
0.148
0.132
0.136
0.120

Table 6: Depth Discretization and Ordinal Regression. MSE: mean squared error in log space. MCC: multi-class classiﬁcation.
DORN: proposed ordinal regression. Note that training by MSE for 1M iterations only slightly improve the performance compared with
0.5M (about 0.001 on δ < 1.25). berHu: the reverse Huber loss. †: ResNet based model.

4.2.2 Full-image Encoder

Variant
w/o full-image encoder
fc-fashion
our encoder

δ < 1.25 Abs Rel RMSElog
0.092
0.085
0.081

0.906
0.910
0.915

0.143
0.137
0.132

Params
0M
753M
51M

Table 7: Full-image Encoder. Parameters here is computed by
some common settings in Eigen [12] and our DORN.

C

From Tab. 7, a full-image encoder is important to further
boost the performance. Our full-image encoder yields a lit-
tle higher scores than fc type encoders [2, 12, 11, 37, 30],
but signiﬁcantly reduce the number of parameters. For
to 512, m to 2048
example, we set C to 512 (VGG),
(Eigen [12, 11]), and k to 4 in Fig. 3. Because of limited
computation resources, when implementing the fc-fashion
encoder, we downsampled the resolution of F using the
to the required resolution.
stride of 3, and upsampled
With an input image of size 385
513, h and w will
be 49 and 65 respectively in our network. The number
of parameters in f c-fashion encoder and our encoder is
1
753M , and
m
w
9 ∗
51M , respectively. From the
is
experimental results and parameter analysis, it can be seen
that our full-image encoder performs better while requires
less computational resources.

C + m2 + 1
h
∗
∗
h
C +
4 ∗

∗
w
4 ∗

C ∗ C ≈

∗ C ∗

9 ∗

C ∗

m

×

≈

F

w

h

∗

4.2.3 How Many Intervals

To illustrate the sensitivity to the number of intervals, we
discretizing depth into various number of intervals via SID.
As shown in Fig. 7, with a range of 40 to 120 intervals, our
DORN has a score in [0.908, 0.915] regarding δ < 1.25, and
a score in [3.056, 3.125] in terms of RMSE, and is thereby
robust to a long range of depth interval numbers. We can
also see that neither too few nor too many depth intervals are
rational for depth estimation: too few depth intervals cause

Figure 7: Performance Ranging Different Intervals via SID.
Left: accuracy on δ < 1.25. Right: evaluation errors on RMSE.

large quantization error, while too many depth intervals lose
the advantage of discretization.

5. Conclusion

In this paper, we have developed an deep ordinal re-
gression network (DORN) for monocular depth estimation
MDE from a single image, consisting of a clean CNN ar-
chitecture and some effective strategies for network opti-
mization. Our method is motivated by two aspects: (i) to
obtain high-resolution depth map, previous depth estima-
tion networks require incorporating multi-scale features as
well as full-image features in a complex architecture, which
complicates network training and largely increases the com-
putational cost; (ii) training a regression network for depth
estimation suffers from slow convergence and unsatisfac-
tory local solutions. To this end, we ﬁrst introduced a sim-
ple depth estimation network which takes advantage of di-
lated convolution technique and a novel full-image encoder
to directly obtain a high-resolution depth map. Moreover,
an effective depth discretization strategy and an ordinal re-
gression training loss were intergrated to improve the train-
ing of our network so as to largely increase the estimation
accuracy. The proposed method achieves the state-of-the-
art performance on the KITTI, ScanNet, Make3D and NYU

Depth v2 datasets. In the future, we will investigate new
approximations to depth and extend our framework to other
dense prediction problems.

6. Acknowledgement

This research was supported by Australian Research
Council Projects FL-170100117 and DP-180103424. This
work was partially supported by SAP SE and CNRS INS2I-
JCJC-INVISANA. We gratefully acknowledge the support
of NVIDIA Corporation with the donation of the Titan X
Pascal GPU used for this research. This research was par-
tially supported by research grant from Pﬁzer titled ”De-
veloping Statistical Method to Jointly Model Genotype and
High Dimensional Imaging Endophenotype.” We were also
grateful for the computational resources provided by Pitts-
burgh Super Computing grant number TG-ASC170024.

References

WACV, 2016. 2

[1] M. H. Baig and L. Torresani. Coupled depth learning.

In

[2] A. Chakrabarti, J. Shao, and G. Shakhnarovich. Depth from
a single image by harmonizing overcomplete local network
predictions. In NIPS, 2016. 2, 3, 4, 7

[3] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner,
M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3D:
Learning from RGB-D data in indoor environments. 3DV,
2017. 1

[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs. In ICLR, 2015. 2,
3

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv:1606.00915, 2016. 3

[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Re-
thinking atrous convolution for semantic image segmenta-
tion. arXiv preprint arXiv:1706.05587, 2017. 2, 3

[7] W. Chen, Z. Fu, D. Yang, and J. Deng. Single-image depth

perception in the wild. In NIPS, 2016. 3

[8] S. Choi, D. Min, B. Ham, Y. Kim, C. Oh, and K. Sohn. Depth
analogy: Data-driven approach for single image depth es-
IEEE TIP, 24(12):5953–
timation using gradient samples.
5966, 2015. 2

[9] K. Crammer and Y. Singer. Pranking with ranking. In NIPS,

2002. 3

[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. Scannet: Richly-annotated 3d reconstruc-
tions of indoor scenes. In CVPR, 2017. 1, 2, 6

[11] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In ICCV, 2015. 1, 2, 3, 4, 7, 8

[12] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
NIPS, 2014. 2, 3, 4, 5, 6, 7, 8

[13] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep-
stereo: Learning to predict new views from the world’s im-
agery. In CVPR, 2016. 2

[14] D. Forsyth and J. Ponce. Computer Vision: a Modern Ap-

proach. Prentice Hall, 2002. 2

[15] E. Frank and M. Hall. A simple approach to ordinal classiﬁ-

cation. ECML, 2001. 3

[16] R. Furukawa, R. Sagawa, and H. Kawasaki. Depth estima-
tion using structured light ﬂow – analysis of projected pattern
ﬂow on an object’s surface. In ICCV, 2017. 2

[17] R. Garg, G. Carneiro, and I. Reid. Unsupervised cnn for
single view depth estimation: Geometry to the rescue.
In
ECCV, 2016. 2, 3, 7

[18] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets

robotics: The kitti dataset. IJRR, 2013. 1, 2, 5

[19] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsuper-
vised monocular depth estimation with left-right consistency.
CVPR, 2017. 5, 7

[20] R. A. G¨uler, G. Trigeorgis, E. Antonakos, P. Snape,
S. Zafeiriou, and I. Kokkinos. Densereg: Fully convolutional
dense shape regression in-the-wild. In CVPR, 2016. 2
[21] H. Ha, S. Im, J. Park, H.-G. Jeon, and I. S. Kweon. High-
quality depth from uncalibrated small motion clip. In CVPR,
2016. 1

[22] C. Hane, L. Ladicky, and M. Pollefeys. Direction matters:
Depth estimation with a surface normal classiﬁer. In CVPR,
2015. 2

[23] F. E. Harrell Jr. Regression modeling strategies: with appli-
cations to linear models, logistic and ordinal regression, and
survival analysis. Springer, 2015. 3

[24] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 2, 5

[25] R. Herbrich, T. Graepel, and K. Obermayer. Support vector

learning for ordinal regression. 1999. 3

[26] D. Hoiem, A. A. Efros, and M. Hebert. Recovering surface
layout from an image. IJCV, 75(1):151–172, 2007. 1, 2
[27] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014. 5

[28] K. Karsch, C. Liu, and S. B. Kang. Depth transfer: Depth
extraction from video using non-parametric sampling. IEEE
TPAMI, 36(11):2144–2158, 2014. 1, 2, 6, 7

[29] A. Kendall and Y. Gal. What uncertainties do we need in
bayesian deep learning for computer vision? In NIPS, 2017.
2

[30] S. Kim, K. Park, K. Sohn, and S. Lin. Uniﬁed depth predic-
tion and intrinsic image decomposition from a single image
via joint convolutional neural ﬁelds. In ECCV, 2016. 1, 2, 7
[31] N. Kong and M. J. Black. Intrinsic depth: Improving depth

transfer with intrinsic images. In ICCV, 2015. 1

[32] J. Konrad, M. Wang, P. Ishwar, C. Wu, and D. Mukherjee.
Learning-based, automatic 2d-to-3d image and video con-
version. IEEE TIP, 22(9):3485–3496, 2013. 2

[33] Y. Kuznietsov, J. St¨uckler, and B. Leibe. Semi-supervised
deep learning for monocular depth map prediction. CVPR,
2017. 1, 2, 3, 7

[55] J. Shi, X. Tao, L. Xu, and J. Jia. Break ames room illusion:
depth from general single images. ACM TOG, 34(6):225,
2015. 2

[56] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
2, 5

[57] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. Yuille.
Towards uniﬁed depth and semantic prediction from a single
image. In CVPR, 2015. 1, 2, 4, 7

[58] X. Wang, D. Fouhey, and A. Gupta. Designing deep net-
works for surface normal estimation. In CVPR, 2015. 2
[59] J. Xie, R. Girshick, and A. Farhadi. Deep3d: Fully au-
tomatic 2d-to-3d video conversion with deep convolutional
neural networks. In ECCV, 2016. 2, 3, 7

[60] D. Xu, E. Ricci, W. Ouyang, X. Wang, and N. Sebe. Multi-
scale continuous crfs as sequential deep networks for monoc-
ular depth estimation. In CVPR, 2017. 7

[61] X. You, Q. Li, D. Tao, W. Ou, and M. Gong. Local metric
learning for exemplar-based object detection. IEEE TCSVT,
24(8):1265–1276, 2014. 2

[62] F. Yu and V. Koltun. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016. 2, 3

[63] Z. Zhang, A. G. Schwing, S. Fidler, and R. Urtasun. Monoc-
ular object instance segmentation and depth ordering with
cnns. In ICCV, 2015. 2

[64] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene

parsing network. In CVPR, 2017. 2, 3

[65] W. Zhuo, M. Salzmann, X. He, and M. Liu. Indoor scene
structure analysis for single image depth estimation.
In
CVPR, 2015. 2

[66] D. Zoran, P. Isola, D. Krishnan, and W. T. Freeman. Learning
ordinal relationships for mid-level vision. In ICCV, 2015. 3

[34] L. Ladicky, J. Shi, and M. Pollefeys. Pulling things out of

perspective. In CVPR, 2014. 1, 2, 7

[35] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and
N. Navab. Deeper depth prediction with fully convolutional
residual networks. In 3DV, 2016. 1, 2, 3, 7

[36] B. Li, C. Shen, Y. Dai, A. van den Hengel, and M. He. Depth
and surface normal estimation from monocular images using
regression on deep features and hierarchical crfs. In CVPR,
2015. 7

[37] J. Li, R. Klein, and A. Yao. A two-streamed network for
estimating ﬁne-scaled depth maps from single rgb images.
In ICCV, 2017. 2, 7

[38] X. Li, H. Qin, Y. Wang, Y. Zhang, and Q. Dai. Dept: depth
estimation by parameter transfer for single still images. In
ACCV, 2014. 2

[39] B. Liu, S. Gould, and D. Koller. Single image depth estima-
tion from predicted semantic labels. In CVPR, 2010. 7
[40] F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from sin-
gle monocular images using deep convolutional neural ﬁelds.
IEEE TPAMI, 38(10):2024–2039, 2016. 1, 2, 3, 5, 6, 7
[41] M. Liu, M. Salzmann, and X. He. Discrete-continuous depth
estimation from a single image. In CVPR, 2014. 2, 7
[42] T. Narihira, M. Maire, and S. X. Yu. Learning lightness from
human judgement on relative reﬂectance. In CVPR, 2015. 2
[43] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor
segmentation and support inference from rgbd images.
In
ECCV, 2012. 1, 2, 5, 6

[44] Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua. Ordinal
regression with multiple output cnn for age estimation.
In
CVPR, 2016. 3

[45] A. Oliva and A. Torralba. Modeling the shape of the scene:
IJCV,

A holistic representation of the spatial envelope.
42(3):145–175, 2001. 2

[46] A. Rajagopalan, S. Chaudhuri, and U. Mudenagudi. Depth
estimation and image restoration using defocused stereo
pairs. IEEE TPAMI, 26(11):1521–1525, 2004. 1

[47] R. Ranftl, V. Vineet, Q. Chen, and V. Koltun. Dense monoc-
ular depth estimation in complex dynamic scenes. In CVPR,
2016. 2

[48] A. Roy and S. Todorovic. Monocular depth estimation using

neural regression forest. In CVPR, 2016. 1, 2, 7

[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 115(3):211–252, 2015. 5
[50] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from

single monocular images. In NIPS, 2006. 1, 2, 5, 6

[51] A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learning 3d
IEEE TPAMI,

scene structure from a single still image.
31(5):824–840, 2009. 1, 2, 5, 6, 7

[52] D. Scharstein and R. Szeliski. A taxonomy and evaluation of
IJCV,
dense two-frame stereo correspondence algorithms.
47(1-3):7–42, 2002. 2

[53] A. Shashua and A. Levin. Ranking with large margin princi-

ple: Two approaches. In NIPS, 2003. 3

[54] E. Shelhamer, J. T. Barron, and T. Darrell. Scene intrinsics
and depth from a single image. In ICCV Workshop, 2015. 2

Deep Ordinal Regression Network for Monocular Depth Estimation

Huan Fu1 Mingming Gong2,3 Chaohui Wang4 Kayhan Batmanghelich2 Dacheng Tao1
1UBTECH Sydney AI Centre, SIT, FEIT, The University of Sydney, Australia
2Department of Biomedical Informatics, University of Pittsburgh
3Department of Philosophy, Carnegie Mellon University
4Universit´e Paris-Est, LIGM (UMR 8049), CNRS, ENPC, ESIEE Paris, UPEM, Marne-la-Vall´ee, France

hufu6371@uni., dacheng.tao@

sydney.edu.au

mig73, kayhan@

pitt.edu

chaohui.wang@u-pem.fr

{

}

{

}

8
1
0
2
 
n
u
J
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
6
4
4
2
0
.
6
0
8
1
:
v
i
X
r
a

Abstract

Monocular depth estimation, which plays a crucial role
in understanding 3D scene geometry, is an ill-posed prob-
lem. Recent methods have gained signiﬁcant improve-
ment by exploring image-level information and hierarchi-
cal features from deep convolutional neural networks (DC-
NNs). These methods model depth estimation as a regres-
sion problem and train the regression networks by mini-
mizing mean squared error, which suffers from slow con-
vergence and unsatisfactory local solutions. Besides, ex-
isting depth estimation networks employ repeated spatial
pooling operations, resulting in undesirable low-resolution
feature maps. To obtain high-resolution depth maps, skip-
connections or multi-layer deconvolution networks are re-
quired, which complicates network training and consumes
much more computations. To eliminate or at least largely
reduce these problems, we introduce a spacing-increasing
discretization (SID) strategy to discretize depth and recast
depth network learning as an ordinal regression problem.
By training the network using an ordinary regression loss,
our method achieves much higher accuracy and faster con-
vergence in synch. Furthermore, we adopt a multi-scale
network structure which avoids unnecessary spatial pool-
ing and captures multi-scale information in parallel.

The method described in this paper achieves state-of-
the-art results on four challenging benchmarks, i.e., KITTI
[18], ScanNet [10], Make3D [51], and NYU Depth v2 [43],
and win the 1st prize in Robust Vision Challenge 2018.
Code has been made available at: https://github.
com/hufu6371/DORN .

1. Introduction

Estimating depth from 2D images is a crucial step of
scene reconstruction and understanding tasks, such as 3D
object recognition, segmentation, and detection. In this pa-

Figure 1: Estimated Depth by DORN. MSE: Training our net-
work via MSE in log space, where ground truths are continuous
depth values. DORN: The proposed deep ordinal regression net-
work. Depth values in the black part are not provided by KITTI.

per, we examine the problem of Monocular Depth Estima-
tion from a single image (abbr. as MDE hereafter).

Compared to depth estimation from stereo images or
video sequences, in which signiﬁcant progresses have been
made [21, 31, 28, 46], the progress of MDE is slow. MDE is
an ill-posed problem: a single 2D image may be produced
from an inﬁnite number of distinct 3D scenes. To overcome
this inherent ambiguity, typical methods resort to exploiting
statistically meaningful monocular cues or features, such as
perspective and texture information, object sizes, object lo-
cations, and occlusions [51, 26, 34, 50, 28].

Recently,

some works have signiﬁcantly improved
the MDE performance with the use of DCNN-based models
[40, 57, 48, 11, 30, 33, 35, 3], demonstrating that deep fea-
tures are superior to handcrafted features. These methods
address the MDE problem by learning a DCNN to estimate
the continuous depth map. Since this problem is a standard
regression problem, mean squared error (MSE) in log-space
or its variants are usually adopted as the loss function. Al-
though optimizing a regression network can achieve a rea-

1

sonable solution, we ﬁnd that the convergence is rather slow
and the ﬁnal solution is far from satisfactory.

In addition, existing depth estimation networks [11, 17,
33, 35, 40, 59] usually apply standard DCNNs designed ini-
tially for image classiﬁcation in a full convolutional manner
as the feature extractors. In these networks, repeated spa-
tial pooling quickly reduce the spatial resolution of feature
maps (usually stride of 32), which is undesirable for depth
estimation. Though high-resolution depth maps can be ob-
tained by incorporating higher-resolution feature maps via
multi-layer deconvolutional networks [35, 17, 33], multi-
scale networks [40, 11] or skip-connection [59], such a pro-
cessing would not only require additional computational
and memory costs, but also complicate the network archi-
tecture and the training procedure.

In contrast to existing developments for MDE, we pro-
pose to discretize continuous depth into a number of inter-
vals and cast the depth network learning as an ordinal re-
gression problem, and present how to involve ordinal re-
gression into a dense prediction task via DCNNs. More
speciﬁcally, we propose to perform the discretization using
a spacing-increasing discretization (SID) strategy instead of
the uniform discretization (UD) strategy, motivated by the
fact that the uncertainty in depth prediction increases along
with the underlying ground-truth depth, which indicates that
it would be better to allow a relatively larger error when
predicting a larger depth value to avoid over-strengthened
inﬂuence of large depth values on the training process. Af-
ter obtaining the discrete depth values, we train the network
by an ordinal regression loss, which takes into account the
ordering of discrete depth values.

To ease network training and save computational cost,
we introduce a network architecture which avoids unnec-
essary subsampling and captures multi-scale information in
a simpler way instead of skip-connections. Inspired by re-
cent advances in scene parsing [62, 4, 6, 64], we ﬁrst re-
move subsampling in the last few pooling layers and apply
dilated convolutions to obtain large receptive ﬁelds. Then,
multi-scale information is extracted from the last pooling
layer by applying dilated convolution with multiple dilation
rates. Finally, we develop a full-image encoder which cap-
tures image-level information efﬁciently at a signiﬁcantly
lower cost of memory than the fully-connected full-image
encoders [2, 12, 11, 37, 30]. The whole network is trained
in an end-to-end manner without stage-wise training or it-
erative reﬁnement. Experiments on four challenging bench-
marks, i.e., KITTI [18], ScanNet [10], Make3D [51, 50] and
NYU Depth v2 [43], demonstrate that the proposed method
achieves state-of-the-art results, and outperforms recent al-
gorithms by a signiﬁcant margin.

The remainder of this paper is organized as follows. Af-
ter a brief review of related literatures in Sec. 2, we present
In Sec. 4, be-
in Sec. 3 the proposed method in detail.

sides the qualitative and quantitative performance on those
benchmarks, we also evaluate multiple basic instantiations
of the proposed method to analyze the effects of those core
factors. Finally, we conclude the whole paper in Sec. 5.

2. Related Work

Depth Estimation is essential for understanding the 3D
structure of scenes from 2D images. Early works fo-
cused on depth estimation from stereo images by devel-
oping geometry-based algorithms [52, 14, 13] that rely on
point correspondences between images and triangulation to
estimate the depth. In a seminal work [50], Saxena et al.
learned the depth from monocular cues in 2D images via su-
pervised learning. Since then, a variety of approaches have
been proposed to exploit the monocular cues using hand-
crafted representations [51, 26, 34, 38, 8, 32, 1, 55, 47, 16,
22, 61]. Since handcrafted features alone can only cap-
ture local information, probabilistic graphic models such
as Markov Random Fields (MRFs) are often built based
on these features to incorporate long-range and global cues
[51, 65, 41]. Another successful way to make use of global
cues is the DepthTransfer method [28] which uses GIST
global scene features [45] to search for candidate images
that are “similar” to the input image from a database con-
taining RGBD images.

Given the success of DCNNs in image understanding,
many depth estimation networks have been proposed in re-
cent years [20, 63, 37, 42, 54, 58, 48, 40, 29]. Thanks
to multi-level contextual and structural information from
powerful very deep networks (e.g., VGG [56] and ResNet
[24]), depth estimation has been boosted to a new accuracy
level [11, 17, 33, 35, 59]. The main hurdle is that the re-
peated pooling operations in these deep feature extractors
quickly decrease the spatial resolution of feature maps (usu-
ally stride 32). Eigen et al. [12, 11] applied multi-scale net-
works which stage-wisely reﬁne estimated depth map from
low spatial resolution to high spatial resolution via indepen-
dent networks. Xie et al. [59] adopted the skip-connection
strategy to fuse low-spatial resolution depth map in deeper
layers with high-spatial resolution depth map in lower lay-
ers. More recent works [17, 33, 35] apply multi-layer
deconvolutional networks to recover coarse-to-ﬁne depth.
Rather than solely relying on deep networks, some methods
incorporate conditional random ﬁelds to further improve
the quality of estimated depth maps [57, 40]. To improve
efﬁciency, Roy and Todorovic [48] proposed the Neural
Regression Forest method which allows for parallelizable
training of “shallow” CNNs.

Recently, unsupervised or semi-supervised learning is
introduced to learn depth estimation networks [17, 33].
These methods design reconstruction losses to estimate
the disparity map by recovering a right view with a left
view. Also, some weakly-supervised methods considering

Figure 2: Illustration of the network architecture. The network consists of a dense feature extractor, multi-scale feature learner (ASPP),
cross channel information learner (the pure 1 × 1 convolutional branch), a full-image encoder and an ordinal regression optimizer. The
Conv components here are all with kernel size of 1 × 1. The ASPP module consists of 3 dilated convolutional layers with kernel size of
3 × 3 and dilated rate of 6, 12 and 18 respectively [6]. The supervised information of our network is discrete depth values output by the
discretization using the SID strategy. The whole network is optimized by our ordinal regression training loss in an end-to-end fashion.

pair-wise ranking information were proposed to roughly
estimate and compare depth [66, 7].

Ordinal Regression [25, 23] aims to learn a rule to predict
labels from an ordinal scale. Most literatures modify well-
studied classiﬁcation algorithms to address ordinal regres-
sion algorithms. For example, Shashua and Levin [53] han-
dled multiple thresholds by developing a new SVM. Cam-
mer and Singer [9] generalized the online perceptron al-
gorithms with multiple thresholds to do ordinal regression.
Another way is to formulate ordinal regression as a set of
binary classiﬁcation subproblems. For instance, Frank and
Hall [15] applied some decision trees as binary classiﬁers
for ordinal regression. In computer vision, ordinal regres-
sion has been combined with DCNNs to address the age
estimation problem [44].

3. Method

This section ﬁrst introduces the architecture of our deep
ordinal regression network; then presents the SID strategy
to divide continuous depth values into discrete values; and
ﬁnally details how the network parameters can be learned
in the ordinal regression framework.

3.1. Network Architecture

As shown in Fig. 2, the divised network consists of two
parts, i.e., a dense feature extractor and a scene understand-
ing modular, and outputs multi-channel dense ordinal labels
given an image.

3.1.1 Dense Feature Extractor

Previous depth estimation networks [11, 17, 33, 35, 40, 59]
usually apply standard DCNNs originally designed for im-
age recognition as the feature extractor. However, the re-

peated combination of max-pooling and striding signiﬁ-
cantly reduces the spatial resolution of the feature maps.
Also, to incorporate multi-scale information and reconstruct
high-resolution depth maps, some partial remedies, includ-
ing stage-wise reﬁnement [12, 11], skip connection [59]
and multi-layer deconvolution network [17, 33, 35] can be
adopted, which nevertheless not only requires additional
computational and memory cost, but also complicates the
network architecture and the training procedure. Following
some recent scene parsing network [62, 4, 6, 64], we advo-
cate removing the last few downsampling operators of DC-
NNs and inserting holes to ﬁlters in the subsequent conv
layers, called dilated convolution, to enlarge the ﬁeld-of-
view of ﬁlters without decreasing spatial resolution or in-
creasing number of parameters.

3.1.2 Scene Understanding Modular

Figure 3: Full-Image Encoders. Top: the full-image encoder
implemented by pure f c layers [12, 11, 2] (δ < 1.25: 0.910);
Bottom: Our proposed encoder (δ < 1.25: 0.915).

The scene understanding modular consists of three par-
allel components, i.e., an atrous spatial pyramid pooling

(ASPP) module [5, 6], a cross-channel leaner, and a full-
image encoder. ASPP is employed to extract features from
multiple large receptive ﬁelds via dilated convolutional op-
erations. The dilation rates are 6, 12 and 18, respectively.
1 convolutional branch can learn complex
The pure 1
cross-channel interactions. The full-image encoder captures
global contextual information and can greatly clarify local
confusions in depth estimation [57, 12, 11, 2].

×

h

F

×

×

×

F
h

C ×

w from F with dimension C

Though previous methods have incorporated full-image
encoders, our full-image encoder contains fewer parame-
with
ters. As shown in Fig. 3, to obtain global feature
w,
dimension
a common fc-fashion method accomplishes this by using
connects
fully-connected layers, where each element in
to all the image features, implying a global understanding
of the entire image. However, this method contains a pro-
hibitively large number of parameters, which is difﬁcult to
train and is memory consuming. In contrast, we ﬁrst make
use of an average pooling layer with a small kernel size and
stride to reduce the spatial dimensions, followed by a f c
layer to obtain a feature vector with dimension
. Then,
channels of feature maps
we treat the feature vector as
C
with spatial dimensions of 1
1, and add a conv layer with
1 as a cross-channel parametric pool-
the kernel size of 1
ing structure. Finally, we copy the feature vector to
along
F
share the same
spatial dimensions so that each location of
understanding of the entire image.

×

×

F

C

The obtained features from the aforementioned compo-
nents are concatenated to achieve a comprehensive under-
standing of the input image. Also, we add two additional
convolutional layers with the kernel size of 1
1, where the
former one reduces the feature dimension and learns com-
plex cross-channel interactions, and the later one transforms
the features into multi-channel dense ordinal labels.

×

3.2. Spacing-Increasing Discretization

Figure 4: Discrete Intervals. Illustration of UD (middle) and
SID (bottom) to discretize depth interval [α, β] into ﬁve sub-
intervals. See Eq. 1 for details.

To quantize a depth interval [α, β] into a set of repre-
sentative discrete values, a common way is the uniform
discretization (UD). However, as the depth value becomes

larger, the information for depth estimation is less rich,
meaning that the estimation error of larger depth values is
generally larger. Hence, using the UD strategy would in-
duce an over-strengthened loss for the large depth values.
To this end, we propose to perform the discretization using
the SID strategy (as shown in Fig. 4), which uniformed dis-
cretizes a given depth interval in log space to down-weight
the training losses in regions with large depth values, so that
our depth estimation network is capable to more accurately
predict relatively small and medium depth and to rationally
estimate large depth values. Assuming that a depth interval
[α, β] needs to be discretized into K sub-intervals, UD and
SID can be formulated as:

UD:

SID:

ti = α + (β
ti = elog(α)+ log(β/α)∗i

α)

−

∗

K

,

i/K,

(1)

where ti ∈ {
are discretization thresholds. In
our paper, we add a shift ξ to both α and β to obtain α∗ and
β∗ so that α∗ = α + ξ = 1.0, and apply SID on [α∗, β∗].

t0, t1, ..., tK}

3.3. Learning and Inference

After obtaining the discrete depth values, it is straightfor-
ward to turn the standard regression problem into a multi-
class classiﬁcation problem, and adopts softmax regression
loss to learn the parameters in our depth estimation net-
work. However, typical multi-class classiﬁcation losses ig-
nore the ordered information between the discrete labels,
while depth values have a strong ordinal correlation since
they form a well-ordered set. Thus, we cast the depth es-
timation problem as an ordinal regression problem and de-
velop an ordinal loss to learn our network parameters.

×

Let χ = ϕ(I, Φ) denote the feature maps of size W

×
C given an image I, where Φ is the parameters involved
H
in the dense feature extractor and the scene understanding
modular. Y = ψ(χ, Θ) of size W
2K denotes
×
the ordinal outputs for each spatial locations, where Θ =
1) contains weight vectors. And l(w,h) ∈
(θ0, θ1, ..., θ2K
−
1
0, 1, ..., K
is the discrete label produced by SID at
{
}
spatial location (w, h). Our ordinal loss
(χ, Θ) is deﬁned
as the average of pixelwise ordinal loss Ψ(h, w, χ, Θ) over
the entire image domain:

H

×

−

L

(χ, Θ) =

L

−

W

1

H

1

−

−

1

N
l(w,h)−1

w=0
(cid:88)

(cid:88)h=0

Ψ(w, h, χ, Θ),

Ψ(h, w, χ, Θ) =

log(

k
(w,h))

P

(2)

(cid:88)k=0
K

1

−

+

(cid:88)k=l(w,h)

(log(1

k
(w,h))),

− P

k

(w,h) = P (ˆl(w,h) > k

χ, Θ),

|

P

Figure 5: Depth Prediction on KITTI. Image, ground truth, Eigen [12], LRC [19], and our DORN. Ground truth has been interpolated
for visualization. Pixels with distance > 80m in LRC are masked out.

N

= W

H, and ˆl(w,h) is the estimated discrete
where
value decoding from y(w,h). We choose softmax function to
k
(w,h) from y(w,h,2k) and y(w,h,2k+1) as follows:
compute

×

P

k
(w,h) =

P

ey(w,h,2k+1)
ey(w,h,2k) + ey(w,h,2k+1)

,

(3)

where y(w,h,i) = θT

χ. Minimizing
i x(w,h), and x(w,h) ∈
(χ, Θ) ensures that predictions farther from the true label

L
incur a greater penalty than those closer to the true label.

The minimization of

(χ, Θ) can be done via an iterative
optimization algorithm. Taking derivate with respect to θi,
the gradient takes the following form:

L

∂

L

(χ, Θ)
∂θi

=

−

W

1

H

1

−

−

1

∂Ψ(w, h, χ, Θ)
∂θi

,

∂Ψ(w, h, χ, Θ)
∂θ2k+1
∂Ψ(w, h, χ, Θ)
∂θ2k

(cid:88)h=0

w=0
(cid:88)

N
∂Ψ(w, h, χ, Θ)
∂θ2k

,

=

−

= x(w,h)η(l(w,h) > k)(

(4)

1)

k
(w,h) −
k
(w,h),

P
k)

+ x(w,h)η(l(w,h) ≤
where k
) is an indicator function
, and η(
1
·
}
such that η(true) = 1 and η(false) = 0. We the can optimize
our network via backpropagation.

0, 1, ..., K

∈ {

−

P

In the inference phase, after obtaining ordinal labels for
each position of image I, the predicted depth value ˆd(w,h)
is decoded as:

ˆd(w,h) =

tˆl(w,h)

+ tˆl(w,h)+1
2

ξ,

−

ˆl(w,h) =

k
(w,h) >= 0.5).

η(

P

K

1

−

(cid:88)k=0

4. Experiments

To demonstrate the effectiveness of our depth estimator,
we present a number of experiments examining different
aspects of our approach. After introducing the implemen-
tation details, we evaluate our methods on three challeng-
ing outdoor datasets, i.e. KITTI [18], Make3D [50, 51] and
NYU Depth v2 [43]. The evaluation metrics are following
previous works [12, 40]. Some ablation studies based on
KITTI are discussed to give a more detailed analysis of our
method.
Implementation Details We implement our depth estima-
tion network based on the public deep learning platform
Caffe [27]. The learning strategy applies a polynomial de-
cay with a base learning rate of 0.0001 and the power of 0.9.
Momentum and weight decay are set to 0.9 and 0.0005 re-
spectively. The iteration number is set to 300K for KITTI,
50K for Make3D, and 3M for NYU Depth v2, and batch
size is set to 3. We ﬁnd that further increasing the itera-
tion number can only slightly improve the performance. We
adopt both VGG-16 [56] and ResNet-101 [24] as our fea-
ture extractors, and initialize their parameters via the pre-
trained classiﬁcation model on ILSVRC [49]. Since fea-
tures in ﬁrst few layers only contain general low-level infor-
mation, we ﬁxed the parameters of conv1 and conv2 blocks
in ResNet after initialization. Also, the batch normaliza-
tion parameters in ResNet are directly initialized and ﬁxed
during training progress. Data augmentation strategies are
following [12].
In the test phase, we split each image to
some overlapping windows according the cropping method
in the training phase, and obtain the predicted depth values
in overlapped regions by averaging the predictions.

(5)

4.1. Benchmark Perfomance

KITTI The KITTI dataset [18] contains outdoor scenes
1241 captured by
with images of resolution about 375

×

Method
Ofﬁcial Baseline
DORN

abs rel.
0.25
0.14

imae
0.17
0.10

irmse
0.21
0.13

log mae
0.24
0.13

log rmse mae
0.42
0.22

0.29
0.17

rmse
0.53
0.29

scale invar.
0.05
0.02

sq. rel.
0.14
0.06

Table 1: Scores on the online ScanNet evaluation server. See https://goo.gl/8keUQN.

and evaluate our method on the ScanNet online test server.

Method
Ofﬁcial Baseline
DORN

SILog sqErrorRel absErrorRel
18.19
11.77

14.24
8.78

7.32
2.23

iRMSE
18.50
12.98

Table 2: Scores on the online KITTI evaluation server. See
https://goo.gl/iXuhiN.

×

×

×

cameras and depth sensors in a driving car. All the 61 scenes
from the “city”, “residential”, “road” and “Campus” cate-
gories are used as our training/test sets. We test on 697 im-
ages from 29 scenes split by Eigen et al. [12], and train on
about 23488 images from the remaining 32 scenes. We train
our model on a random crop of size 385
513. For some
other details, we set the maximal ordinal label for KITTI
as 80, and evaluate our results on a pre-deﬁned center crop-
ping following [12] with the depth ranging from 0m to 80m
and 0m to 50m. Note that, a single model is trained on the
full depth range, and is tested on data with different depth
ranges.
Make3D The Make3D dataset [50, 51] contains 534 out-
door images, 400 for training, and 134 for testing, with the
1704, and provides the ground truth
resolution of 2272
×
305. We re-
depth map with a small resolution of 55
duce the resolution of all images to 568
426, and train our
model on a random crop of size 513
385. Following previ-
ous works, we report C1 (depth range from 0m to 80m) and
C2 (depth range from 0m to 70m) error on this dataset us-
ing three commonly used evaluation metrics [28, 40]. For
the VGG model, we train our DORN on a depth range of
0m to 80m from scratch (ImageNet model), and evaluate
results using the same model for C1 and C2 . However,
for ResNet, we learn two separate models for C1 and C2
respectively.
NYU Depth v2 The NYU Depth v2 [43] dataset contains
464 indoor video scenes taken with a Microsoft Kinect cam-
era. We train our DORN using all images (about 120K)
from the 249 training scenes, and test on the 694-image
test set following previous works. To speed up training, all
the images are reduced to the resolution of 288
384 from
640. And the model are trained on random crops of
480
×
size 257
353. We report our scores on a pre-deﬁned center
cropping by Eigen [12].
ScanNet The ScanNet [10] dataset is also a challenging
benchmark which contains various indoor scenes. We train
our model on the ofﬁcially provided 24353 training and
513,
validation images with a random crop size of 385

×

×

×

×

Figure 6: Depth Prediction on Make3D. Image, ground truth,
and our DORN. Pixels with distance > 70m are masked out.

Performance Tab. 3 and Tab. 4 give the results on two
outdoor datasets, i.e., KITTI and Make3D. It can be seen
that our DORN improves the accuracy by 5% ∼ 30% in
terms of all metrics compared with previous works in all
settings. Some qualitative results are shown in Fig. 5 and
Fig. 6. In Tab. 5, our DORN outperforms other methods on
NYU Depth v2, which is one of the largest indoor bench-
marks. The results suggest that our method is applicable to
both indoor and outdoor data. We evaluate our method on
the online KITTI evaluation server and the online ScanNet
evaluation server. As shown in Tab. 2 and 1, our DORN
signiﬁcantly outperforms the ofﬁcially provided baselines.

4.2. Ablation Studies

We conduct various ablation studies to analyze the de-
tails of our approach. Results are shown in Tab. 6, Tab. 7,

Method

cap

higher is better

δ < 1.25 δ < 1.252

Make3D [51]
Eigen et al. [12]
Liu et al. [40]
LRC (CS + K) [19]
Kuznietsov et al. [33]
DORN (VGG)
DORN (ResNet)
Garg et al. [17]
LRC (CS + K) [19]
Kuznietsov et al. [33]
DORN (VGG)
DORN (ResNet)

0 - 80 m
0 - 80 m
0 - 80 m
0 - 80 m
0 - 80 m
0 - 80 m
0 - 80 m
0 - 50 m
0 - 50 m
0 - 50 m
0 - 50 m
0 - 50 m

0.601
0.692
0.647
0.861
0.862
0.915
0.932
0.740
0.873
0.875
0.920
0.936

0.820
0.899
0.882
0.949
0.960
0.980
0.984
0.904
0.954
0.964
0.982
0.985

lower is better

δ < 1.253 Abs Rel Squa Rel RMSE RMSElog
3.012
1.515
1.841
0.898
0.741
0.376
0.307
1.080
0.657
0.595
0.324
0.268

0.361
0.270
0.289
0.206
0.189
0.132
0.120
0.273
0.194
0.179
0.128
0.116

8.734
7.156
6.986
4.935
4.621
3.056
2.727
5.104
3.729
3.518
2.517
2.271

0.926
0.967
0.961
0.976
0.986
0.993
0.994
0.962
0.979
0.988
0.994
0.995

0.280
0.190
0.217
0.114
0.113
0.081
0.072
0.169
0.108
0.108
0.079
0.071

Table 3: Performance on KITTI. All the methods are evaluated on the test split by Eigen et al. [12]. LRC (CS + K): LRC pre-train their
model on Cityscapes and ﬁne tune on KITTI.

Method

Make3D [51]
Liu et al. [39]
DepthTransfer [28]
Liu et al. [41]
Li et al. [36]
Liu et al. [40]
Roy et al. [48]
Laina et al. [35]
LRC-Deep3D [59]
LRC [19]

MS-CRF [60]
DORN (VGG)
DORN (ResNet)

C1 error
log10
-
-

rms
-
-

rel
-
-

rel

C2 error
log10
0.370 0.187
0.379 0.148

rms
-
-

-

0.355 0.127 9.20 0.361 0.148 15.10
0.335 0.137 9.49 0.338 0.134 12.60
0.278 0.092 7.12 0.279 0.102 10.27
0.287 0.109 7.36 0.287 0.122 14.09
0.260 0.119 12.40
-
-
-
-
-

-
-
-
-
0.184 0.065 4.38 0.198
8.56
0.236 0.082 7.02 0.238 0.087 10.01
0.157 0.062 3.97 0.162 0.067 7.32

-
-
-
-

-
0.176 0.072 4.46
1.000 2.527 19.11
0.443 0.156 11.513
Kuznietsov et al. [33] 0.421 0.190 8.24

-

Table 4: Performance on Make3D. LRC-Deep3D [59] is adopt-
ing LRC [19] on Deep3D model [59].

Fig. 1, and Fig. 7, and discussed in detail.

-

-

-
-

-
-

δ1

δ3

δ2

rel

0.447 0.745 0.897 0.349

log10
rms
1.214
-
0.35 0.131 1.2
0.335 0.127 1.06
-

Method
Make3D [51]
DepthTransfer [28]
Liu et al. [41]
Ladicky et al. [34]
Li et al. [36]
Wang et al. [57]
Roy et al. [48]
Liu et al. [40]
Eigen et al. [11]

-
-
0.542 0.829 0.941
0.621 0.886 0.968 0.232 0.094 0.821
0.824
0.605 0.890 0.970 0.220
0.744
0.187
0.650 0.906 0.976 0.213 0.087 0.759
0.641
0.769 0.950 0.988 0.158
Chakrabarti et al. [2] 0.806 0.958 0.987 0.149
0.620
0.629 0.889 0.971 0.194 0.083 0.790
0.789 0.955 0.988 0.152 0.064 0.611
0.811 0.953 0.988 0.127 0.055 0.573
0.788 0.958 0.991 0.143 0.063 0.635
0.811 0.954 0.987 0.121 0.052 0.586
0.828 0.965 0.992 0.115 0.051 0.509

Laina et al. [35]
Li et al. [37]
Laina et al. [35]†
Li et al. [37]†
MS-CRF [60]†
DORN†

-
-

-
-

-

-

-

Table 5: Performance on NYU Depth v2. δi: δ < 1.25i. †:
ResNet based model.

4.2.1 Depth Discretization

Depth discretization is critical to performance improve-
ment, because it allows us to apply classiﬁcation and ordinal
regression losses to optimize the network parameters. Ac-
cording to scores in Tab. 6, training by regression on con-
tinuous depth seems to converge to a poorer solution than
the other two methods, and our ordinal regression network
achieves the best performance. There is an obvious gap be-
tween approaches where depth is discretized by SID and
UD, respectively. Besides, when replacing our ordinal re-
gression loss by an advantage regression loss (i.e. BerHu),
our DORN still obtain much higher scores. Thus, we can

conclude that: (i) SID is important and can further improve
the performance compared to UD; (i) discretizing depth and
training using a multi-class classiﬁcation loss is better than
training using regression losses; (iii) exploring the ordinal
correlation among depth drives depth estimation networks
to converge to even better solutions.

Furthermore, we also train the network using RMSElog
on discrete depth values obtained by SID, and report the re-
sults in Tab. 6. We can see that MSE-SID performs slightly
better than MSE, which demonstrates that quantization er-
rors are nearly ignorable in depth estimation. The beneﬁts
of discretization through the use of ordinal regression losses
far exceeds the cost of depth discretization.

Variant

Iteration

MSE
MSE-SID
MCC-UD
MCC-SID
DORN-UD
DORN-SID
berHu†
DORN†

1M
0.6M
0.3M
0.3M
0.3M
0.3M
0.6M
0.3M

δ < 1.25
0.864
0.865
0.892
0.906
0.900
0.915
0.909
0.932

higher is better
δ < 1.252
0.969
0.970
0.970
0.976
0.973
0.980
0.978
0.984

δ < 1.253
0.991
0.992
0.988
0.991
0.991
0.993
0.992
0.994

Abs Rel
0.109
0.108
0.093
0.084
0.091
0.081
0.086
0.072

lower is better

Squa Rel
0.527
0.520
0.474
0.417
0.452
0.376
0.385
0.307

RMSE
3.660
3.636
3.438
3.201
3.339
3.056
3.365
2.727

RMSElog
0.164
0.163
0.155
0.142
0.148
0.132
0.136
0.120

Table 6: Depth Discretization and Ordinal Regression. MSE: mean squared error in log space. MCC: multi-class classiﬁcation.
DORN: proposed ordinal regression. Note that training by MSE for 1M iterations only slightly improve the performance compared with
0.5M (about 0.001 on δ < 1.25). berHu: the reverse Huber loss. †: ResNet based model.

4.2.2 Full-image Encoder

Variant
w/o full-image encoder
fc-fashion
our encoder

δ < 1.25 Abs Rel RMSElog
0.092
0.085
0.081

0.143
0.137
0.132

0.906
0.910
0.915

Params
0M
753M
51M

Table 7: Full-image Encoder. Parameters here is computed by
some common settings in Eigen [12] and our DORN.

C

From Tab. 7, a full-image encoder is important to further
boost the performance. Our full-image encoder yields a lit-
tle higher scores than fc type encoders [2, 12, 11, 37, 30],
but signiﬁcantly reduce the number of parameters. For
to 512, m to 2048
example, we set C to 512 (VGG),
(Eigen [12, 11]), and k to 4 in Fig. 3. Because of limited
computation resources, when implementing the fc-fashion
encoder, we downsampled the resolution of F using the
to the required resolution.
stride of 3, and upsampled
With an input image of size 385
513, h and w will
be 49 and 65 respectively in our network. The number
of parameters in f c-fashion encoder and our encoder is
1
753M , and
m
w
9 ∗
51M , respectively. From the
is
experimental results and parameter analysis, it can be seen
that our full-image encoder performs better while requires
less computational resources.

C + m2 + 1
h
∗
∗
h
C +
4 ∗

∗
w
4 ∗

C ∗ C ≈

∗ C ∗

9 ∗

C ∗

m

×

≈

F

w

h

∗

4.2.3 How Many Intervals

To illustrate the sensitivity to the number of intervals, we
discretizing depth into various number of intervals via SID.
As shown in Fig. 7, with a range of 40 to 120 intervals, our
DORN has a score in [0.908, 0.915] regarding δ < 1.25, and
a score in [3.056, 3.125] in terms of RMSE, and is thereby
robust to a long range of depth interval numbers. We can
also see that neither too few nor too many depth intervals are
rational for depth estimation: too few depth intervals cause

Figure 7: Performance Ranging Different Intervals via SID.
Left: accuracy on δ < 1.25. Right: evaluation errors on RMSE.

large quantization error, while too many depth intervals lose
the advantage of discretization.

5. Conclusion

In this paper, we have developed an deep ordinal re-
gression network (DORN) for monocular depth estimation
MDE from a single image, consisting of a clean CNN ar-
chitecture and some effective strategies for network opti-
mization. Our method is motivated by two aspects: (i) to
obtain high-resolution depth map, previous depth estima-
tion networks require incorporating multi-scale features as
well as full-image features in a complex architecture, which
complicates network training and largely increases the com-
putational cost; (ii) training a regression network for depth
estimation suffers from slow convergence and unsatisfac-
tory local solutions. To this end, we ﬁrst introduced a sim-
ple depth estimation network which takes advantage of di-
lated convolution technique and a novel full-image encoder
to directly obtain a high-resolution depth map. Moreover,
an effective depth discretization strategy and an ordinal re-
gression training loss were intergrated to improve the train-
ing of our network so as to largely increase the estimation
accuracy. The proposed method achieves the state-of-the-
art performance on the KITTI, ScanNet, Make3D and NYU

Depth v2 datasets. In the future, we will investigate new
approximations to depth and extend our framework to other
dense prediction problems.

6. Acknowledgement

This research was supported by Australian Research
Council Projects FL-170100117 and DP-180103424. This
work was partially supported by SAP SE and CNRS INS2I-
JCJC-INVISANA. We gratefully acknowledge the support
of NVIDIA Corporation with the donation of the Titan X
Pascal GPU used for this research. This research was par-
tially supported by research grant from Pﬁzer titled ”De-
veloping Statistical Method to Jointly Model Genotype and
High Dimensional Imaging Endophenotype.” We were also
grateful for the computational resources provided by Pitts-
burgh Super Computing grant number TG-ASC170024.

References

WACV, 2016. 2

[1] M. H. Baig and L. Torresani. Coupled depth learning.

In

[2] A. Chakrabarti, J. Shao, and G. Shakhnarovich. Depth from
a single image by harmonizing overcomplete local network
predictions. In NIPS, 2016. 2, 3, 4, 7

[3] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner,
M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3D:
Learning from RGB-D data in indoor environments. 3DV,
2017. 1

[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
volutional nets and fully connected crfs. In ICLR, 2015. 2,
3

[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv:1606.00915, 2016. 3

[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Re-
thinking atrous convolution for semantic image segmenta-
tion. arXiv preprint arXiv:1706.05587, 2017. 2, 3

[7] W. Chen, Z. Fu, D. Yang, and J. Deng. Single-image depth

perception in the wild. In NIPS, 2016. 3

[8] S. Choi, D. Min, B. Ham, Y. Kim, C. Oh, and K. Sohn. Depth
analogy: Data-driven approach for single image depth es-
IEEE TIP, 24(12):5953–
timation using gradient samples.
5966, 2015. 2

[9] K. Crammer and Y. Singer. Pranking with ranking. In NIPS,

2002. 3

[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. Scannet: Richly-annotated 3d reconstruc-
tions of indoor scenes. In CVPR, 2017. 1, 2, 6

[11] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In ICCV, 2015. 1, 2, 3, 4, 7, 8

[12] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
NIPS, 2014. 2, 3, 4, 5, 6, 7, 8

[13] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep-
stereo: Learning to predict new views from the world’s im-
agery. In CVPR, 2016. 2

[14] D. Forsyth and J. Ponce. Computer Vision: a Modern Ap-

proach. Prentice Hall, 2002. 2

[15] E. Frank and M. Hall. A simple approach to ordinal classiﬁ-

cation. ECML, 2001. 3

[16] R. Furukawa, R. Sagawa, and H. Kawasaki. Depth estima-
tion using structured light ﬂow – analysis of projected pattern
ﬂow on an object’s surface. In ICCV, 2017. 2

[17] R. Garg, G. Carneiro, and I. Reid. Unsupervised cnn for
single view depth estimation: Geometry to the rescue.
In
ECCV, 2016. 2, 3, 7

[18] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets

robotics: The kitti dataset. IJRR, 2013. 1, 2, 5

[19] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsuper-
vised monocular depth estimation with left-right consistency.
CVPR, 2017. 5, 7

[20] R. A. G¨uler, G. Trigeorgis, E. Antonakos, P. Snape,
S. Zafeiriou, and I. Kokkinos. Densereg: Fully convolutional
dense shape regression in-the-wild. In CVPR, 2016. 2
[21] H. Ha, S. Im, J. Park, H.-G. Jeon, and I. S. Kweon. High-
quality depth from uncalibrated small motion clip. In CVPR,
2016. 1

[22] C. Hane, L. Ladicky, and M. Pollefeys. Direction matters:
Depth estimation with a surface normal classiﬁer. In CVPR,
2015. 2

[23] F. E. Harrell Jr. Regression modeling strategies: with appli-
cations to linear models, logistic and ordinal regression, and
survival analysis. Springer, 2015. 3

[24] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 2, 5

[25] R. Herbrich, T. Graepel, and K. Obermayer. Support vector

learning for ordinal regression. 1999. 3

[26] D. Hoiem, A. A. Efros, and M. Hebert. Recovering surface
layout from an image. IJCV, 75(1):151–172, 2007. 1, 2
[27] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014. 5

[28] K. Karsch, C. Liu, and S. B. Kang. Depth transfer: Depth
extraction from video using non-parametric sampling. IEEE
TPAMI, 36(11):2144–2158, 2014. 1, 2, 6, 7

[29] A. Kendall and Y. Gal. What uncertainties do we need in
bayesian deep learning for computer vision? In NIPS, 2017.
2

[30] S. Kim, K. Park, K. Sohn, and S. Lin. Uniﬁed depth predic-
tion and intrinsic image decomposition from a single image
via joint convolutional neural ﬁelds. In ECCV, 2016. 1, 2, 7
[31] N. Kong and M. J. Black. Intrinsic depth: Improving depth

transfer with intrinsic images. In ICCV, 2015. 1

[32] J. Konrad, M. Wang, P. Ishwar, C. Wu, and D. Mukherjee.
Learning-based, automatic 2d-to-3d image and video con-
version. IEEE TIP, 22(9):3485–3496, 2013. 2

[33] Y. Kuznietsov, J. St¨uckler, and B. Leibe. Semi-supervised
deep learning for monocular depth map prediction. CVPR,
2017. 1, 2, 3, 7

[55] J. Shi, X. Tao, L. Xu, and J. Jia. Break ames room illusion:
depth from general single images. ACM TOG, 34(6):225,
2015. 2

[56] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
2, 5

[57] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. Yuille.
Towards uniﬁed depth and semantic prediction from a single
image. In CVPR, 2015. 1, 2, 4, 7

[58] X. Wang, D. Fouhey, and A. Gupta. Designing deep net-
works for surface normal estimation. In CVPR, 2015. 2
[59] J. Xie, R. Girshick, and A. Farhadi. Deep3d: Fully au-
tomatic 2d-to-3d video conversion with deep convolutional
neural networks. In ECCV, 2016. 2, 3, 7

[60] D. Xu, E. Ricci, W. Ouyang, X. Wang, and N. Sebe. Multi-
scale continuous crfs as sequential deep networks for monoc-
ular depth estimation. In CVPR, 2017. 7

[61] X. You, Q. Li, D. Tao, W. Ou, and M. Gong. Local metric
learning for exemplar-based object detection. IEEE TCSVT,
24(8):1265–1276, 2014. 2

[62] F. Yu and V. Koltun. Multi-scale context aggregation by di-

lated convolutions. In ICLR, 2016. 2, 3

[63] Z. Zhang, A. G. Schwing, S. Fidler, and R. Urtasun. Monoc-
ular object instance segmentation and depth ordering with
cnns. In ICCV, 2015. 2

[64] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene

parsing network. In CVPR, 2017. 2, 3

[65] W. Zhuo, M. Salzmann, X. He, and M. Liu. Indoor scene
structure analysis for single image depth estimation.
In
CVPR, 2015. 2

[66] D. Zoran, P. Isola, D. Krishnan, and W. T. Freeman. Learning
ordinal relationships for mid-level vision. In ICCV, 2015. 3

[34] L. Ladicky, J. Shi, and M. Pollefeys. Pulling things out of

perspective. In CVPR, 2014. 1, 2, 7

[35] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and
N. Navab. Deeper depth prediction with fully convolutional
residual networks. In 3DV, 2016. 1, 2, 3, 7

[36] B. Li, C. Shen, Y. Dai, A. van den Hengel, and M. He. Depth
and surface normal estimation from monocular images using
regression on deep features and hierarchical crfs. In CVPR,
2015. 7

[37] J. Li, R. Klein, and A. Yao. A two-streamed network for
estimating ﬁne-scaled depth maps from single rgb images.
In ICCV, 2017. 2, 7

[38] X. Li, H. Qin, Y. Wang, Y. Zhang, and Q. Dai. Dept: depth
estimation by parameter transfer for single still images. In
ACCV, 2014. 2

[39] B. Liu, S. Gould, and D. Koller. Single image depth estima-
tion from predicted semantic labels. In CVPR, 2010. 7
[40] F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from sin-
gle monocular images using deep convolutional neural ﬁelds.
IEEE TPAMI, 38(10):2024–2039, 2016. 1, 2, 3, 5, 6, 7
[41] M. Liu, M. Salzmann, and X. He. Discrete-continuous depth
estimation from a single image. In CVPR, 2014. 2, 7
[42] T. Narihira, M. Maire, and S. X. Yu. Learning lightness from
human judgement on relative reﬂectance. In CVPR, 2015. 2
[43] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor
segmentation and support inference from rgbd images.
In
ECCV, 2012. 1, 2, 5, 6

[44] Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua. Ordinal
regression with multiple output cnn for age estimation.
In
CVPR, 2016. 3

[45] A. Oliva and A. Torralba. Modeling the shape of the scene:
IJCV,

A holistic representation of the spatial envelope.
42(3):145–175, 2001. 2

[46] A. Rajagopalan, S. Chaudhuri, and U. Mudenagudi. Depth
estimation and image restoration using defocused stereo
pairs. IEEE TPAMI, 26(11):1521–1525, 2004. 1

[47] R. Ranftl, V. Vineet, Q. Chen, and V. Koltun. Dense monoc-
ular depth estimation in complex dynamic scenes. In CVPR,
2016. 2

[48] A. Roy and S. Todorovic. Monocular depth estimation using

neural regression forest. In CVPR, 2016. 1, 2, 7

[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. IJCV, 115(3):211–252, 2015. 5
[50] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from

single monocular images. In NIPS, 2006. 1, 2, 5, 6

[51] A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learning 3d
IEEE TPAMI,

scene structure from a single still image.
31(5):824–840, 2009. 1, 2, 5, 6, 7

[52] D. Scharstein and R. Szeliski. A taxonomy and evaluation of
IJCV,
dense two-frame stereo correspondence algorithms.
47(1-3):7–42, 2002. 2

[53] A. Shashua and A. Levin. Ranking with large margin princi-

ple: Two approaches. In NIPS, 2003. 3

[54] E. Shelhamer, J. T. Barron, and T. Darrell. Scene intrinsics
and depth from a single image. In ICCV Workshop, 2015. 2

