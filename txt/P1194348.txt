SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance
Segmentation

Weiyue Wang1

Ronald Yu2
1University of Southern California
Los Angeles, California
{weiyuewa,qianguih,uneumann}@usc.edu

Qiangui Huang1

Ulrich Neumann1

2University of California, San Diego
San Diego, California
ronaldiscool@gmail.com

9
1
0
2
 
y
a
M
 
0
3
 
 
]

V
C
.
s
c
[
 
 
2
v
8
8
5
8
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

We introduce Similarity Group Proposal Network
(SGPN), a simple and intuitive deep learning framework for
3D object instance segmentation on point clouds. SGPN
uses a single network to predict point grouping proposals
and a corresponding semantic class for each proposal, from
which we can directly extract instance segmentation results.
Important to the effectiveness of SGPN is its novel represen-
tation of 3D instance segmentation results in the form of a
similarity matrix that indicates the similarity between each
pair of points in embedded feature space, thus producing
an accurate grouping proposal for each point. Experimen-
tal results on various 3D scenes show the effectiveness of
our method on 3D instance segmentation, and we also eval-
uate the capability of SGPN to improve 3D object detection
and semantic segmentation results. We also demonstrate
its ﬂexibility by seamlessly incorporating 2D CNN features
into the framework to boost performance.

1. Introduction

Instance segmentation on 2D images have achieved
promising results recently [18, 10, 31, 23]. With the rise of
autonomous driving and robotics applications, the demand
for 3D scene understanding and the availability of 3D scene
data has rapidly increased in recently. Unfortunately, the
literature for 3D instance segmentation and object detec-
tion lags far behind its 2D counterpart; scene understanding
with Convolutional Neural Networks (CNNs) [44, 45, 11]
on 3D volumetric data is limited by high memory and com-
putation cost. Recently, deep learning frameworks Point-
Net/Pointnet++ [33, 35] on point clouds open up more efﬁ-
cient and ﬂexible ways to handle 3D data.

Following the pioneering works in 2D scene understand-
ing, our goal is to develop a novel deep learning framework
trained end-to-end for 3D instance-aware semantic segmen-
tation on point clouds that, like established baseline systems
for 2D scene understanding tasks, is intuitive, simple, ﬂexi-

(a)

(b)

(c)
Instance segmentation for point clouds using
Figure 1:
SGPN. Different colors represent different instances.
(a)
Instance segmentation on complete real scenes. (b) Single
object part instance segmentation. (c) Instance segmenta-
tion on point clouds obtained from partial scans.

ble, and effective.

An important consideration for instance segmentation on
a point cloud is how to represent output results. Inspired by
the trend of predicting proposals for tasks with a variable
number of outputs, we introduce a Similarity Group Pro-
posal Network (SGPN), which formulates group proposals
of object instances by learning a novel 3D instance segmen-
tation representation in the form of a similarity matrix .

Our pipeline ﬁrst uses PointNet/PointNet++ to extract a
descriptive feature vector for each point in the point cloud.
As a form of similarity metric learning, we enforce the idea
that points belonging to the same object instance should
have very similar features; hence we measure the distance
between the features of each pair of points in order to form
a similarity matrix that indicates whether any given pair of

1

points belong to the same object instance.

The rows in our similarity matrix can be viewed as in-
stance candidates, which we combine with learned conﬁ-
dence scores in order to generate plausible group proposals.
We also learn a semantic segmentation map in order to clas-
sify each object instance obtained from our group proposals.
We are also able to directly derive tight 3D bounding boxes
for object detection.

By simply measuring the distance between overdeter-
mined feature representations of each pair of points, our
similarity matrix simpliﬁes our pipeline in that we remain
in the natural point cloud representation of deﬁning our ob-
jects by the relationships between points.

In summary, SGPN has three output branches for in-
stance segmentation on point clouds: a similarity matrix
yielding point-wise group proposals, a conﬁdence map for
pruning these proposals, and a semantic segmentation map
to give the class label for each group.

We evaluate our

framework on both 3D shapes
(ShapeNet [4]) and real 3D scenes (Stanford Indoor Se-
mantic Dataset [1] and NYUV2 [42]) and demonstrate that
SGPN achieves state-of-the-art results on 3D instance seg-
mentation. We also conduct comprehensive experiments
to show the capability of SGPN on achieving high perfor-
mance on 3D semantic segmentation and 3D object detec-
tion on point clouds. Although a minimalistic framework
with no bells and whistles already gives visually pleasing
results (Figure 1), we also demonstrate the ﬂexibility of
SGPN as we boost performance even more by seamlessly
integrating CNN features from RGBD images.

2. Related Works

2.1. Object Detection and Instance Segmentation

Recent advances in object detection [39, 14, 24, 37, 38,
26, 13, 25] and instance segmentation [23, 10, 9, 32, 31]
on 2D images have achieved promising results. R-CNN
[15] for 2D object detection established a baseline system
by introducing region proposals as candidate object regions.
Faster R-CNN [39] leveraged a CNN learning scheme and
proposed Region Proposal Networks(RPN). YOLO [37] di-
vided the image into grids and each grid cell produced
an object proposal. Many 2D instance segmentation ap-
proaches are based on segment proposals. DeepMask [31]
learns to generate segment proposals each with a corre-
sponding object score. Dai et al. [10] predict segment can-
didates from bounding box proposals. Mask R-CNN [18]
extended Faster R-CNN by adding a branch on top of RPN
to produce object masks for instance segmentation.

Following these pioneering 2D works, 3D bounding box
detection frameworks have emerged [40, 44, 45, 11, 5].
Song and Xiao [45] use a volumetric CNN to create 3D
RPN on a voxelized 3D scene and then use both the color

and depth data of the image in a joint 3D and 2D object
recognition network on each proposal. Deng and Latecki
[11] regress class-wise 3D bounding box models based on
RGBD image appearance features only. Armeni et al [1]
use a sliding shape method with CRF to perform 3D object
detection on point cloud. To the best of our knowledge, no
previous work exists that learns 3D instance segmentation.

2.2. 3D Deep Learning

Convolutional neural networks generalize well to 3D by
performing convolution on voxels for certain tasks such as
object classiﬁcation [34, 48, 27, 51, 41, 29, 30], shape
reconstruction [49, 17, 8] of simple objects, and 3D ob-
ject detection as mentioned in Section 2.1. However, vol-
umetric representation carry a high memory and compu-
tational cost and have strong limitations dealing with 3D
scenes [7, 1, 46]. Octree-based CNNs [41, 47, 48] have been
introduced recently, but they are less ﬂexible than volumet-
ric CNNs and still suffer from memory efﬁciency problems.
A point cloud is an intuitive, memory-efﬁcient 3D repre-
sentation well-suited for representing detailed, large scenes
for 3D instance segmentation using deep learning. Point-
Net/Pointnet++ [33, 35] recently introduce deep neural net-
works on 3D point clouds, learning successful results for
tasks such as object classiﬁcation and part and semantic
scene segmentation. We base our network architecture
off of PointNet/PointNet++, achieving a novel method that
learns 3D instance segmentation on point clouds.

2.3. Similarity Metric Learning

Our work is also closely related to similarity metric
learning, which has been widely used in deep learning on
various tasks such as person re-identiﬁcation [52], match-
ing [16], image retrival [12, 50] and face recognition [6].
Siamese CNNs [6, 43, 3] are used on tasks such as track-
ing [22] and one-shot learning [20] by measuring the simi-
larity of two input images. Alejandro et. al [28] introduced
an associative embedding method to group similar pixels for
multi-person pose estimation and 2D instance segmentation
by enforcing that pixels in the same group should have simi-
lar values in their embedding space without actually enforc-
ing what those exact values should be. Our method exploits
metric learning in a different way in that we regress the like-
lihood of two points belonging to the same group and for-
mulate the similarity matrix as group proposals to handle
variable number of instances.

3. Method

The goal of this paper is to take a 3D point cloud as in-
put and produce an object instance label for each point and
a class label for each instance. Utilizing recent develop-
ments in deep learning on point clouds [33, 35], we intro-
duce a Similarity Group Proposal Network (SGPN), which

consumes a 3D point cloud and outputs a set of instance
proposals that each contain the group of points inside the
instance as well as its class label. Section 3.1 introduces
the design and properties of SGPN. Section 3.2 proposes
an algorithm to merge similar groups and give each point
an instance label. Section 3.3 gives implementation details.
Figure 2 depicts the overview of our system.

3.1. Similarity Group Proposal Network

SGPN is a very simple and intuitive framework. As
shown in Figure 2, it ﬁrst passes a point cloud P of size
Np through a feed-forward feature extraction network in-
spired by PointNets [33, 35], learning both global and lo-
cal features in the point cloud. This feature extraction net-
work produces a matrix F . SGPN then diverges into three
branches that each pass F through a single PointNet layer to
obtain sized Np × Nf feature matrices FSIM , FCF , FSEM ,
which we respectively use to obtain a similarity matrix, a
conﬁdence map and a semantic segmentation map. The ith
row in a Np ×Nf feature matrix is a Nf -dimensional vector
that represents point Pi in an embedded feature space. Our
loss L is given by the sum of the losses from each of these
three branches: L = LSIM + LCF + LSEM . Our network
architecture can be found in the supplemental.

Similarity Matrix We propose a novel similarity matrix
S from which we can formulate group proposals to directly
recover accurate instance segmentation results. S is of di-
mensions Np × Np, and element Sij classiﬁes whether or
not points Pi and Pj belong to the same object instance.
Each row of S can be viewed as a proposed grouping of
points that form a candidate object instance.

We leverage that points belonging to the same object in-
stance should have similar features and lie very close to-
gether in feature space. We obtain S by, for each pair of
points {Pi, Pj}, simply subtracting their corresponding fea-
ture vectors {FSIMi, FSIMj } and taking the L2 norm such
that Sij = ||FSIMi − FSIMj ||2. This reduces the problem
of instance segmentation to learning an embedding space
where points in the same instance are close together and
those in different object instances are far apart.

For a better understanding of how SGPN captures corre-
lation between points, in Figure 3(a) we visualize the simi-
larity (euclidean distance in feature space) between a given
point and the rest of the points in the point cloud. Points in
different instances have greater euclidean distances in fea-
ture space and thus smaller similarities even though they
have the same semantic labels. For example, in the bottom-
right image of Figure 3(a), although the given table leg point
has greater similarity with the other table leg points than the
table top, it is still distinguishable from the other table leg.
We believe that a similarity matrix is a more natural and
simple representation for 3D instance segmentation on a

point cloud compared to traditional 2D instance segmenta-
tion representations. Most state-of-the-art 2D deep learning
methods for instance segmentation ﬁrst localize the image
into patches, which are then passed through a neural net-
work and segment a binary mask of the object.

While learning a binary mask in a bounding box is
a more natural representation for space-centric structures
such as images or volumetric grids where features are
largely deﬁned by which positions in a grid have strong sig-
nals, point clouds can be viewed as shape-centric structures
where information is encoded by the relationship between
the points in the cloud, so we would prefer to also deﬁne
instance segmentation output by the relationship between
points without working too much in grid space.

Hence we expect that a deep neural network could better
learn our similarity matrix, which compared to traditional
representations is a more natural and straightforward rep-
resentation for instance segmentation in a point cloud.

Double-Hinge Loss for Similarity Matrix As is the case
in [28], in our similarity matrix we do not need to precisely
regress the exact values of our features; we only optimize
the simpler objective that similar points should be close to-
gether in feature space. We deﬁne three potential similarity
classes for each pair of points {Pi, Pj}: 1) Pi and Pj belong
to the same object instance; 2) Pi and Pj share the same se-
mantic class but do not belong to the same object instance;
3) Pi and Pj do not share the same semantic class. Pairs
of points should lie progressively further away from each
other in feature space as their similarity class increases. We
deﬁne out loss as:

LSIM =

l(i, j)

Np
(cid:88)

Np
(cid:88)

i

j

l(i, j) =






||FSIMi − FSIMj ||2
Cij = 1
α max(0, K1 − ||FSIMi − FSIMj ||2) Cij = 2
Cij = 3
max(0, K2 − ||FSIMi − FSIMj ||2)

where Cij indicates which of the similarity classes deﬁned
above does the pair of points ({Pi, Pj)} belong to and
α, K1, K2 are constants such that α > 1, K2 > K1.

Although the second and third similarity class are treated
equivalently for the purposes of instance segmentation, dis-
tinguishing between them in LSIM using our double-hinge
loss allows our similarity matrix output branch and our se-
mantic segmentation output branch to mutually assist each
other for increased accuracy and convergence speed. Since
the semantic segmentation network is actually wrongly try-
ing to bring pairs of points in our second similarity class
closer together in feature space, we also add an α > 1 term
to increase the weight of our loss to dominate the gradient
from the semantic segmentation output branch.

Figure 2: Pipeline of our system for point cloud instance segmentation.

(a)

(b)

Figure 3: (a) Similarity (euclidean distance in feature space)
between a given point (indicated by red arrow) and the rest
of points. A darker color represents lower distance in fea-
ture space thus higher similarity. (b) Conﬁdence map. A
darker color represents higher conﬁdence.

At test time if Sij < T hS where T hS < K1, then points

pair Pi and Pj are in the same instance group.

Similarity Conﬁdence Map SGPN also feeds FCF
through an additional PointNet layer to predict a Np × 1
conﬁdence map CM reﬂecting how conﬁdently the model
believes that each grouping candidate is indeed a correct
object instance. Figure 3(b) provides a visualization of the
conﬁdence map; points located in the boundary area be-
tween parts have lower conﬁdence.

We regress conﬁdence scores based on ground truth
groups G represented as a Np × Np matrix identical in form
to our similarity matrix. If Pi is a background point that
does not belong to any object in the ground truth then the
row Gi will be all zeros. For each row in Si, we expect the
ground-truth value in the conﬁdence map CMi to be the in-
tersection over union (IoU) between the set of points in the
predicted group Si and the ground truth group Gi. Our loss
LCF is the L2 loss between the inferred and expected CM .
Although the loss LCF depends on the similarity ma-
trix output branch during training, at test time we run the
branches in parallel and only groups with conﬁdence greater
than a threshold T hC are considered valid group proposals.

Semantic Segmentation Network The semantic seg-
mentation map acts as a point-wise classiﬁer. SGPN passes
FSEM through an additional PointNet layer whose archi-
tecture depends on the number of possible semantic classes,
yielding the ﬁnal output MSEM , which is a Np × NC sized
matrix where NC is the number of possible object cate-
gories. MSEMij corresponds to the probability that point
Pi belongs to class Cj.

The loss LSEM is a weighted sum of the cross entropy
softmax loss for each row in the matrix. We use median fre-
quency balancing [2] and the weight assigned to a category
is ac = medianf req/f req(c), where f req(c) is the total
number of points of class c divided by the total number of
points in samples where c is present, and medianf req is
the median of these f req(c).

At test time, the class label for a group instance is as-
signed by calculating the mode of the semantic labels of the
points in that group.

3.2. Group Proposal Merging

The similarity matrix S produces Np group proposals,
many of which are noisy or represent the same object. We
ﬁrst discard proposals with predicted conﬁdence less than
T hC or cardinality less than T hM 2. We further prune
our proposals into clean, non-overlapping object instances
by applying Non-Maximum Suppression; groups with IoU
greater than T hM 1 are merged together by selecting the
group with the maximum cardinality.

Each point is then assigned to the group proposal that
contains it. In the rare case (∼ 2%) that after the merging
stage a point belongs to more than one ﬁnal group proposal,
this usually means that the point is at the boundary between
two object instances, which means that the effectiveness of
our network would be roughly the same regardless of which
group proposal the point is assigned to. Hence, with min-
imal loss in accuracy we randomly assign the point to any
one of the group proposals that contains it. We refer to this
process as GroupMerging throughout the rest of the paper.

3.3. Implementation Details

We use an ADAM [19] optimizer with initial learning
rate 0.0005, momentum 0.9 and batch size 4. The learn-
ing rate is divided by 2 every 20 epochs. The network is
trained with only the LSIM loss for the ﬁrst 5 epochs. In
our experiment, α is set to 2 initially and is increased by 2
every 5 epochs. This design makes the network more fo-
cused on separating features of points that belong to dif-
ferent object instances but have the same semantic labels.
K1, K2 are set to 1.0 and 2.0, respectively. We use per-
category histogram thresholding to get the threshold point
T hs for each testing sample. T hM 1 is set to 0.6 and T hM 2
is set to 200. T hC is set to 0.1. Our network is imple-
mented with Tensorﬂow and a single Nvidia GTX1080 Ti

GPU. It takes 16-17 hours to converge. At test time, SGPN
takes 40ms on an input point cloud with size 4096 × 9 with
PointNet++ as our baseline architecture. Further runtime
analysis can be found in Section 4.2. Code is availabel at
github.com/laughtervv/SGPN.

4. Experiments

following datasets:

We evaluate SGPN on 3D instance segmentation on the

• Stanford 3D Indoor Semantics Dataset (S3DIS) [1]:
This dataset contains 3D scans in 6 areas including 271
rooms. The input is a complete point cloud generated
from scans fused together from multiple views. Each
point has semantic labels and instance annotations.

• NYUV2 [42]: Partial point clouds are generated from
single view RGBD images. The dataset is annotated
with 3D bounding boxes and 2D semantic segmenta-
tion masks. We use the improved annotation in [11].
Since both 3D bounding boxes and 2D segmentation
masks annotations are given, ground truth 3D instance
segmentation labels for point clouds can be easily gen-
erated We follow the standard split with 795 training
images and 654 testing images.

• ShapeNet [4, 53] Part Segmentation: ShapeNet con-
tains 16, 881 shapes annotated with 50 types of parts
in total. Most object categories are labeled with two
to ﬁve parts. We use the ofﬁcial split of 795 training
samples and 654 testinn percentageg samples in our
experiments.

We also show the capability of SGPN to improve seman-
tic segmentation and 3D object detection. To validate the
ﬂexibility of SGPN, we also seamlessly incorporate 2D
CNN features into our network to boos performance on the
NYUV2 dataset.

4.1. S3DIS Instance Segmentation and 3D Object

Detection

We perform experiments on Stanford 3D Indoor Seman-
tic Dataset to evaluate our performance on large real scene
scans. Following experimental settings in PointNet [33],
points are uniformly sampled into blocks of area 1m × 1m.
Each point is labeled as one of 13 categories (chair, table,
ﬂoor, wall, clutter etc.) and represented by a 9D vector
(XYZ, RGB, and normalized location as to the room). At
train time we uniformly sample 4096 points in each block,
and at test time we use all points in the block as input.

SGPN uses PointNet as its baseline architecture for this
experiment.1 Figure 5 shows instance segmentation results
on S3DIS with SGPN. Different colors represent different

1PointNet [33] proposed a 3D detection system while PointNet++ [35]

instances. Point colors of the same group are not necessarily
the same as their counterparts in the ground truth since ob-
ject instances are unordered. To visualize instance classes,
we also add semantic segmentation results. SGPN achieves
good performance on various room types.

We also compare instance segmentation performance
with the following method (which we call Seg-Cluster):
Perform semantic segmentation using our network and then
select all points as seeds. Starting from a seed point, BFS
is used to search neighboring points with the same label. If
a cluster with more than 200 points has been found, it is
viewed as a valid group. Our GroupMerging algorithm is
then used to merge these valid groups.

We calculate the IoU on points between each predicted
and ground truth group. A detected instance is considered
as true positive if the IoU score is greater than a threshold.
The average precision (AP) is further calculated for instance
segmentation performance evaluation. Table 1 shows the
AP for every category with IoU threshold 0.5. To the best
of our knowledge, there are no existing instance segmenta-
tion method on point clouds for arbitrary object categories,
so we further demonstrate the capability of SGPN to handle
various objects by adding the 3D detection results of Ar-
meni et al.
[1] on S3DIS to Table 1. The difference in
evaluation metrics between our method and [1] is that the
IoU threshold of [1] is 0.5 on a 3D bounding box and the
IoU calculation of our method is on points. Despite this dif-
ference in metrics, we can still see our superior performance
on both large and small objects.

We see that a naive method like Seg-Cluster tends to
properly separate regions far away for large objects like the
ceiling and ﬂoor. However for small object, Seg-Cluster
fails to segment instances with the same label if they are
close to each other. Mean APs with different IoU thresh-
olds (0.25, 0.5, 0.75) are also evaluated in Table 2. Figure 4
shows qualitative comparison results.

Once we have instance segmentation results, we can
compute the bounding box for every instance and thus pro-
duce 3D object detection predictions. In Table 3, we com-
pare out method with the 3D object detection system intro-
duced in PointNet [33], which to the best of our knowledge
is the state-of-the-art method for 3D detection on S3DIS.
Detection performance is evaluated over 4 categories AP
with IoU threshold 0.5.

The method introduced in PointNet clusters points given
semantic segmentation results and uses a binary classiﬁca-
tion network for each category to separate close objects with
same categories. Our method outperforms it by a large mar-
gin, and unlike PointNet does not require an additional net-
work, which unnecessarily introduces additional complex-

does not. To make fair comparison, we use PointNet as our baseline ar-
chitecture for this experiment while using PointNet++ in Sections 4.2 and
4.3.

Mean
Seg-Cluster 17.40
36.30

SGPN

ceiling ﬂoor
70.01
80.12
83.67
58.42

wall
10.64
42.24

beam column window door
32.32
15.30
45.23
25.64

28.97
42.73

0.00
7.15

table
22.16
38.25

chair
27.76
47.05

sofa
0.00
0.00

bookcaseboard
21.52
31.68

0.06
13.57

Table 1: Results on instance segmentation in S3DIS scenes. The metric is AP(%) with IoU threshold 0.5. To the best of our
knowledge, there are no existing instance segmentation methods on point clouds for arbitrary object categories.

Seg-Cluster 34.8
52.6

SGPN

AP0.25 AP0.5 AP0.75
11.2
17.4
18.8
36.3

Table 2: Comparison results on instance segmentation with
different IoU thresholds in S3DIS scenes. Metric is mean
AP(%) over 13 categories.

Mean table chair sofa board
PointNet [33] 24.24 46.67 33.80 4.76 11.72
Seg-Cluster 18.72 33.44 22.8 5.38 13.07
30.20 49.90 40.87 6.96 13.28

SGPN

Table 3: Comparison results on 3D detection in S3DIS
scenes. SGPN uses PointNet as baseline. The metric is AP
with IoU threshold 0.5.

Mean IoU Accuracy

PointNet [33] 49.76
50.37

SGPN

79.66
80.78

Table 4: Results on semantic segmentation in S3DIS scenes.
SGPN uses PointNet as baseline. Metric is mean IoU(%)
over 13 classes (including clutter).

ity during both train and test time and local minima dur-
ing train time. SGPN can effectively separate the difﬁcult
cases of objects of the same semantic class but different in-
stances (c.f. Figure 4) since points in different instances are
far apart in feature space even though they have the same
semantic label. We further compare our semantic segmen-
tation results with PointNet in Table 4. SGPN outperforms
its baseline with the help of its similarity matrix.

4.2. NYUV2 Object Detection and Instance Seg-

mentation Evaluation

We evaluate the effectiveness of our approach on partial
3D scans on the NYUV2 dataset. In this dataset, 3D point
clouds are lifted from a single RGBD image. An image of
size H × W can produce H × W points. We subsample
this point cloud by resizing the image to H
4 and get the
corresponding points using a nearest neighbor search. Both
our training and testing experiments are conducted on such
a point cloud. PointNet++ is used as our baseline.

4 × W

In [36], 2D CNN features are combined 3D point cloud
for RGBD semantic segmentation. By leveraging the ﬂexi-
bility of SGPN, we also seamlessly integrate 2D CNN fea-
tures from RGB images to boost performance. A 2D CNN
consumes an RGBD map and extracts feature maps F2 with
size H
4 sub-sampled

4 × NF 2. Since there are H

4 × W

4 × W

(e)

(a)

(d)

(b)

(c)
Figure 4: Comparison results on S3DIS. (a) Ground Truth
for instance segmentation. Different colors represents dif-
ferent instances. (b) SGPN instance segmentation results.
(c) Seg-Cluster instance segmentation results. (d) Ground
Truth for semantic segmentation. (e) Semantic Segmenta-
tion and 3D detection results of SGPN. The color of the
detected bounding box for each object category is the same
as the semantic labels.

points for every image, a feature vector of size Nf 2 can
be extracted from F2 at each pixel location. Every fea-
ture vector is concatenated to F (a Np × NF feature ma-
trix produced by PointNet/PointNet++ as mentioned in Sec-
tion 3.1) for each corresponding point, yielding a feature
map of size NP × (NF + NF 2), which we then feed to
our output branches. Figure 6 illustrates this procedure;
we call this pipeline SGPN-CNN. In our experiments, we
use a pre-trained AlexNet model [21] (with the ﬁrst layer
stride 1) and extract F2 from the conv5 layer. We use
H × W = 316 × 415 and Np = 8137. The 2D CNN
and SGPN are trained jointly.

Evaluation is performed on 19 object categories. Fig-
ure 7 shows qualitative results on instance segmentation of
SGPN. Table 5 shows comparisons between Seg-Cluster
and our SGPN and CNN-SGPN frameworks on instance
segmentation. The evaluation metric is average precision
(AP) with IoU threshold 0.5.

The margin of improvement for SGPN compared to Seg-
Cluster is not as high as it is on S3DIS, because in this
dataset objects with the same semantic label are usually far
apart in Euclidean space. Additionally, naive methods like
Seg-Cluster beneﬁt since it is easy to separate a single in-
stance into parts since the points are not connected due to

Mean

Seg-Cluster 23.2 31.0 70.1 27.1 1.3 25.8 20.3 13.9 11.1 24.3 4.4 16.3 3.6 32.0 25.5 36.3 50.9 12.9 2.2 23.5
26.5 55.9 53.3 27.8 0.0 27.4 59.6 28.9 6.1 33.9 2.0 19.7 2.0 29.4 30.7 39.1 43.6 17.6 1.2 25.9
SGPN-CNN 30.5 56.4 55.4 35.2 0.0 42.6 50.6 23.1 21.1 31.8 7.5 22.7 6.4 39.9 33.5 42.4 54.8 21.3 3.8 32.1

SGPN

Table 5: Results on instance segmentation in NYUV2. The metric is AP with IoU 0.5.

Figure 5: SGPN instance segmentation results on S3DIS.
The ﬁrst row is the prediction results. The second row is
groud truths. Different colors represent different instances.
The third row is the predicted semantic segmentation re-
sults. The fourth row is the ground truths for semantic seg-
mentation.

Figure 6: Incorporating CNN features in SGPN.

occlusion in partial scanning. Table 5 also illustrates that
SGPN can effectively utilize CNN features. Instead of con-
catenating fully-connected layer of 2D and 3D networks as
in [45], we combine 2D and 3D features by considering
their geometric relationships.

(a)

(b)

(c)

(d)

Figure 7: SGPN instance segmentation results on NYUV2.
(a) Input point clouds. (b) Ground truths for instance seg-
mentation.
(c) Instance segmentation results with SGPN.
(d) Instance segmentation results with SGPN-CNN.

Mean
Deep Sliding Shapes [45] 37.55 58.2 36.1 27.2 28.7
Deng and Latecki [11] 35.55 46.4 33.1 33.3 29.4
36.25 44.4 30.4 46.1 24.4
41.30 50.8 34.8 49.4 30.2

SGPN
SGPN-CNN

Table 6: Comparison results on 3D detection (AP with IoU
0.5) in NYUV2. Please note we use point groups as infer-
ence while [45, 11] use large bounding box with invisible
regions as ground truth. Our prediction is the tight bound-
ing box on points which makes the IoU much smaller than
[45, 11].

We further calculate bounding boxes with instance seg-
mentation results. Table 6 compares our work with the
state-of-the-art works [45, 11] on NYUV2 3D object detec-
tion. Following the evaluation metric in [44], AP is calcu-
lated with IoU threshold 0.25 on 3D bounding boxes. The
NYUV2 dataset provides ground truth 3D bounding boxes
that encapsulate the whole object including the part that is
invisible in the depth image. Both [45] and [11] use these
large ground truth bounding boxes for inference.
In our
method, we infer point groupings, which lack information
of the invisible part of the object. Our output is the derived
tight bounding box around the grouped points in the partial
scan, which makes our IoUs much smaller than [45, 11].
However, we can still see the effectiveness of SGPN on the

Mean

air-
plane

bag cap

car chair head
phone

guitar knife lamp laptopmotor mug pistol rocket skate
board

table

[35] 84.6 80.4 80.9 60.0 76.8 88.1 83.7 90.2 82.6 76.9 94.7 68.0 91.2 82.1 59.9 78.2 87.5
SGPN 85.8 80.4 78.6 78.8 71.5 88.6 78.0 90.9 83.0 78.8 95.8 77.8 93.8 87.4 60.1 92.3 89.4

Table 7: Semantic segmentation results on ShapeNet part dataset. Metric is mean IoU(%) on points.

(a)

(b)

(c)

(d)

Figure 8: Qualitative results on ShapeNet Part Dataset.
(a) Generated ground truth for instance segmentation. (b)
SGPN instance segmentation results. (c) Semantic segmen-
tation results of PointNet++. (d) Semantic segmentation re-
sults of SGPN.

task of 3D object detection on partial scans as our method
achieves better performance on small objects.

Computation Speed To benchmark the testing time with
[45, 11] and make fair comparison, we run our framework
on an Nvidia K40 GPU. SGPN takes 170ms and around
400M GPU memory per sample. CNN-SGPN takes 300ms
and 1.4G GPU memory per sample. GroupMerging
takes 180ms on an Intel i7 CPU. However, the detection net
in [11] takes 739ms on an Nvidia Titan X GPU. In [45],
RPN takes 5.62s and ORN takes 13.93s per image on an
Nvidia K40 GPU. Our model improves the efﬁciency and
reduces GPU memory usage by a large margin.

4.3. ShapeNet Part Instance Segmentation

Following the settings in [35], point clouds are gener-
ated by uniformly sampling shapes from Shapenet [4]. In
our experiments we sample each shape into 2048 points.
The XYZ of points are fed into network as input with size
2048 × 3. To generate ground truth labels for part instance
segmentation from semantic segmentation results, we per-
form DBSCAN clustering on each part category of an object
to group points into instances. This experiment is conducted
as a toy example to demonstrate the effectiveness of our ap-
proach on instance segmentation for pointclouds.

We use Pointnet++ as our baseline. Figure 8(b) illus-
trates the instance segmentation results. For instance re-
sults, we again use different colors to represent different

instances, and point colors of the same group are not nec-
essarily the same as the ground truth. Since the generated
ground truths are not “real” ground truths, only qualitative
results are provided. SGPN achieves good results even un-
der challenging conditions. As we can see from the Fig-
ure 8, SGPN is able to group the chair legs into four in-
stances even though even in the ground truth DBSCAN can
not separate the chair legs apart.

The similarity matrix can also help the semantic segmen-
tation branch training. We compare SGPN to PointNet++
(i.e. our framework with solely a semantic segmentation
branch) on semantic segmentation in Table 7. The inputs
of both networks are point clouds of size 2048. Evaluation
metric is mIoU on points of each shape category. Our model
performs better than PointNet++ due to the similarity ma-
trix. Qualitative results are shown in Figure 8. Some false
segmentation prediction is reﬁned with the help of SGPN.

5. Conclusion

We present SGPN, an intuitive, simple, and ﬂexible
framework for 3D instance segmentation on point clouds.
With the introduction of the similarity matrix as our out-
put representation, group proposals with class predictions
can be easily generated from a single network. Experiments
show that our algorithm can achieve good performance on
instance segmentation for various 3D scenes and facilitate
the tasks of 3D object detection and semantic segmentation.

Future Work While a similarity matrix provides an intu-
itive representation and an easily deﬁned loss function, one
limitation of SGPN is that the size of the similarity matrix
scales quadratically as Np increases. Thus, although much
more memory efﬁcient than volumetric methods, SGPN
cannot process extremely large scenes on the order 105 or
more points. Future research directions can consider gener-
ating groups using seeds that are selected based on SGPN
to reduce the size of the similarity matrix. SGPN can also
be extended in future works to learn in a more unsupervised
setting or to learn more different kinds of data representa-
tions beyond instance segementation.

A. Network Architecture

In our experiments, we use both PointNet and Point-
Net++ as our baseline architectures. For the S3DIS dataset,
we use PointNet as our baseline for fair comparison with
the 3D object detection system described in the PointNet

paper [33]. The network architecture is the same as the se-
mantic segmentation network as stated in PointNet except
for the last two layers. Our F is the last 1 × 1 conv layer
with BatchNorm and ReLU in PointNet with 256 output
channels. FSIM , FCF , FSEM are 1 × 1 conv layers with
output channels (128, 128, 128), respectively.

For the NYUV2 dataset, we use PointNet++ as our base-
line. We use the same notations as PointNet++ to describe
our architecture:

SA(K, r, [l1, ..., ld]) is a set abstraction (SA) level with
K local regions of ball radius r using a PointNet architec-
ture of d 1 × 1 conv layers with output channels li(i =
1, ..., d). F P (l1, ..., ld) is a feature propagation (F P ) level
with d 1 × 1 conv layers. Our network architecture is:

SA(1024, 0.1, [32, 32, 64]),

SA(256, 0.2, [64, 64, 128]),

SA(128, 0.4, [128, 128, 256]),

SA(64, 0.8, [256, 256, 256]),

SA(16, 1.2, [256, 256, 512]),

F P (512, 256),

F P (256, 256),

F P (256, 256),

F P (256, 128),

F P (128, 128, 128, 128).

FSIM , FCF , FSEM are 1 × 1 conv layers with output chan-
nels (128, 128, 128) respectively.

For our experiments on the ShapeNet part dataset,
PointNet++ is used as our baseline. We use the same
network architecture as in the PointNet++ paper [35].
FSIM , FCF , FSEM are 1 × 1 conv layers with output chan-
nels (64, 64, 64), respectively.

B. Experiment Settings

B.1. S3DIS Dataset

Block Merging We divide each scene into 1m × 1m
blocks with overlapping sliding windows in a snake pat-
tern of stride 0.5m as is shown in Figure 9. The en-
tire scene is also divided into a 400 × 400 × 400 grid V .
Vk is used to indicate the instance label of cell k where
k ∈ [0, 400×400×400). Given V and point instance labels
for each block P L where P Lij represents the instance la-
bel of jth point in block i, a BlockMerging algorithm (refer
to Algorithm 1) is derived to merge object instances from
different blocks.

In Figure 10, we show more qualitative results of in-

stance segmentation with SGPN.

Figure 9: Dividing scene into blocks with overlap (top
view).

Algorithm 1: BlockMeriging

: V , P L

Input
Output: Point instance labels for the whole scene L

1 Initialize V with all elements −1;
2 GroupCount ← 0;
3 for every block i do
4

if i is the 1st block then

for every point Pj in block i do

Deﬁne k where Pj is located in the kth
cell of V ;
Vk ← P L1j;

end

else

for every instance Ij in block i do

Deﬁne VIj points in Ij are located in cells
VIj ;
Vt ← the cells in VIj that do not have
value −1;
if the frequency of the mode in Vt < 30
then

VIj ← GroupCount;
GroupCount ← GroupCount + 1;

VIj ← the mode of Vt;

else

end

end

end

20
21 end
22 for every point Pj in the whole scene do
23

Deﬁne k where Pj is located in the kth cell of V ;
Lj ← Vk;

24
25 end

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

Mean wall ﬂoor chair

coun-
ter
Seg-Cluster 32.5 34.2 87.6 61.4 40.4 20.7 43.1 19.2 33.8 33.5 44.4 60.0 39.8 24.5
35.1 55.5 86.7 64.4 41.1 40.7 42.7 36.1 39.6 38.6 45.1 50.3 15.9 27.0

table desk

book
shelf

SGPN

bath
tub

cur-
tain

toilet

sink

sofa

bed

door

win-
dow

0
0

0
0

fridge

shower
curtain
19.4 22.0
19.3 28.6

pic-
ture

0
0

cabi-
net

33.5
35.1

Table 8: Instance segmentation results on ScanNet(v1). The metric is AP (%) with IoU threshold 0.25. We observe 0 percent
AP on items that appear on the wall (door, window, picture) as they contain very little depth information and are almost all
incorrectly semantically labeled as the wall. Future works can explore addressing this problem.

C. More Experiments

C.1. ScanNet

We provide more experimental results on ScanNet [7].
This dataset contains 1513 scanned and reconstructed in-
door scenes. We use the ofﬁcial split with 1201 scenes for
training and 312 for testing. Following the same Block-
Merging procedure, each scene is divided into 1.5m×1.5m
blocks and each block is uniformly sampled into 4096
points for training. All points in the block are used at test
time. Each point is represented by a 9D vector (XYZ, RGB,
and normalized location with respect to the room scene).
PointNet++ is used as the baseline. The network architec-
ture is:

SA(1024, 0.1, [32, 32, 64]),

SA(256, 0.2, [64, 64, 128]),

SA(64, 0.4, [128, 128, 256]),

SA(16, 0.8, [256, 256, 512]),
F P (256, 256),

F P (256, 256),

F P (256, 128),

F P (128, 128, 128, 128).

And FSIM , FCF , FSEM are 1 × 1 conv layers with output
channels (128, 128, 128) respectively. Table 8 illustrates the
quantitative comparison results with Seg-Cluster. The met-
ric is average precision (AP) with IoU threshold 0.25. Fig-
ure 11 shows instance segmentation results on ScanNet.

References

[1] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,
M. Fischer, and S. Savarese. 3d semantic parsing of large-
scale indoor spaces. In CVPR, 2016. 2, 5

[2] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. TPAMI, 2017. 4

[3] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and
P. H. S. Torr. Fully-convolutional siamese networks for ob-
ject tracking. In ECCV Workshops, 2016. 2

[4] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,

et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015. 2, 5, 8

[5] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d
object detection network for autonomous driving. In CVPR,
2017. 2

[6] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005. 2

[7] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. Scannet: Richly-annotated 3d reconstruc-
tions of indoor scenes. In CVPR, 2017. 2, 10

[8] A. Dai, C. R. Qi, and M. Nießner. Shape completion using
In CVPR,

3d-encoder-predictor cnns and shape synthesis.
2017. 2

[9] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive

fully convolutional networks. In ECCV, 2016. 2

[10] J. Dai, K. He, and J. Sun. Instance-aware semantic segmen-
tation via multi-task network cascades. In CVPR, 2016. 1,
2

[11] Z. Deng and L. J. Latecki. Amodal detection of 3d objects:
Inferring 3d bounding boxes from 2d ones in rgb-depth im-
ages. In CVPR, 2017. 1, 2, 5, 7, 8

[12] A. Frome, Y. Singer, F. Sha, and J. Malik. Learning globally-
consistent local distance functions for shape-based image re-
trieval and classiﬁcation. In ICCV, 2007. 2

[13] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
Dssd: Deconvolutional single shot detector. arXiv preprint
arXiv:1701.06659, 2017. 2

[14] R. Girshick. Fast r-cnn. In ICCV, 2015. 2
[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 2

[16] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg.
Matchnet: Unifying feature and metric learning for patch-
based matching. In CVPR, 2015. 2

[17] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu. High-
resolution shape completion using deep neural networks for
In ICCV,
global structure and local geometry inference.
2017. 2

[18] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn.

In ICCV, 2017. 1, 2

[19] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 4

[20] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neu-
ral networks for one-shot image recognition. In ICML Deep
Learning Workshop, 2015. 2

Prediction

Ground Truth

Prediction

Ground Truth

Figure 10: Instance segmentation results on S3DIS with SGPN. Different colors represent different instances. The colors of
the same object in ground truth and prediction are not necessarily the same.

[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 6

Imagenet
In

[22] L. Leal-Taix, C. Canton-Ferrer, and K. Schindler. Learning
by tracking: siamese cnn for robust target association. CVPR
DeepVision Workshops, 2016. 2

[23] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei. Fully convolutional
instance-aware semantic segmentation. In CVPR, 2017. 1, 2
[24] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
In CVPR, 2017. 2

[25] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar. Focal

Prediction

Ground Truth

Prediction

Ground Truth

Figure 11: Instance segmentation results on ScanNet with SGPN. Different colors represent different instances. The colors
of the same object in ground truth and prediction are not necessarily the same.

loss for dense object detection. In CVPR, 2017. 2

2015. 2

[26] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
Fu, and A. C. Berg. SSD: Single shot multibox detector. In
ECCV, 2016. 2

[28] A. Newell and J. Deng. Associative embedding: End-to-end
learning for joint detection and grouping. In NIPS, 2016. 2,
3

[27] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
In IROS,

neural network for real-time object recognition.

[29] G. Pang and U. Neumann. 3d point cloud object detec-
tion with multi-view convolutional neural network. In ICPR,

[50] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiﬁcation. Journal of
Machine Learning Research, 2009. 2

[51] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In CVPR, 2015. 2

[52] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Deep metric learning for

person re-identiﬁcation. In ICPR, 2014. 2

[53] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, A. Lu,
Q. Huang, A. Sheffer, L. Guibas, et al. A scalable active
framework for region annotation in 3d shape collections.
ACM Transactions on Graphics (TOG), 2016. 5

2016. 2

[30] G. Pang, R. Qiu, J. Huang, S. You, and U. Neumann. Auto-
matic 3d industrial point cloud modeling and recognition. In
MVA, 2015. 2

[31] P. O. Pinheiro, R. Collobert, and P. Doll´ar. Learning to seg-

ment object candidates. In NIPS, 2015. 1, 2

[32] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-

ing to reﬁne object segments. In ECCV, 2016. 2

[33] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
CVPR, 2017. 1, 2, 3, 5, 6, 9

[34] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view cnns for object classi-
ﬁcation on 3d data. In CVPR, 2016. 2

[35] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
In NIPS, 2017. 1, 2, 3, 5, 8, 9

[36] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun. 3d graph
neural networks for rgbd semantic segmentation. In CVPR,
2017. 6

[37] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Uniﬁed, real-time object detection.
In
CVPR, 2016. 2

[38] J. Redmon and A. Farhadi. Yolo9000: Better, faster, stronger.

In CVPR, 2017. 2

[39] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 2

[40] Z. Ren and E. B. Sudderth. Three-dimensional object detec-
tion and layout prediction using clouds of oriented gradients.
In CVPR, 2016. 2

[41] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learning
deep 3d representations at high resolutions. In CVPR, 2017.
2

[42] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor seg-
mentation and support inference from rgbd images. ECCV,
2012. 2, 5

[43] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and
F. Moreno-Noguer. Discriminative learning of deep convo-
lutional feature point descriptors. In ICCV, 2015. 2

[44] S. Song and J. Xiao. Sliding shapes for 3d object detection

in depth images. In ECCV, 2014. 1, 2, 7

[45] S. Song and J. Xiao. Deep Sliding Shapes for amodal 3D
object detection in RGB-D images. In CVPR, 2016. 1, 2, 7,
8

[46] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and
T. Funkhouser. Semantic scene completion from a single
depth image. arXiv preprint arXiv:1611.08974, 2016. 2
[47] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3d outputs. In ICCV, 2017. 2

[48] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong.
O-cnn: Octree-based convolutional neural networks for 3d
shape analysis. ACM Transactions on Graphics (TOG),
2017. 2

[49] W. Wang, Q. Huang, S. You, C. Yang, and U. Neumann.
Shape inpainting using 3d generative adversarial network
and recurrent convolutional networks. In ICCV, 2017. 2

SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance
Segmentation

Weiyue Wang1

Ronald Yu2
1University of Southern California
Los Angeles, California
{weiyuewa,qianguih,uneumann}@usc.edu

Qiangui Huang1

Ulrich Neumann1

2University of California, San Diego
San Diego, California
ronaldiscool@gmail.com

9
1
0
2
 
y
a
M
 
0
3
 
 
]

V
C
.
s
c
[
 
 
2
v
8
8
5
8
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

We introduce Similarity Group Proposal Network
(SGPN), a simple and intuitive deep learning framework for
3D object instance segmentation on point clouds. SGPN
uses a single network to predict point grouping proposals
and a corresponding semantic class for each proposal, from
which we can directly extract instance segmentation results.
Important to the effectiveness of SGPN is its novel represen-
tation of 3D instance segmentation results in the form of a
similarity matrix that indicates the similarity between each
pair of points in embedded feature space, thus producing
an accurate grouping proposal for each point. Experimen-
tal results on various 3D scenes show the effectiveness of
our method on 3D instance segmentation, and we also eval-
uate the capability of SGPN to improve 3D object detection
and semantic segmentation results. We also demonstrate
its ﬂexibility by seamlessly incorporating 2D CNN features
into the framework to boost performance.

1. Introduction

Instance segmentation on 2D images have achieved
promising results recently [18, 10, 31, 23]. With the rise of
autonomous driving and robotics applications, the demand
for 3D scene understanding and the availability of 3D scene
data has rapidly increased in recently. Unfortunately, the
literature for 3D instance segmentation and object detec-
tion lags far behind its 2D counterpart; scene understanding
with Convolutional Neural Networks (CNNs) [44, 45, 11]
on 3D volumetric data is limited by high memory and com-
putation cost. Recently, deep learning frameworks Point-
Net/Pointnet++ [33, 35] on point clouds open up more efﬁ-
cient and ﬂexible ways to handle 3D data.

Following the pioneering works in 2D scene understand-
ing, our goal is to develop a novel deep learning framework
trained end-to-end for 3D instance-aware semantic segmen-
tation on point clouds that, like established baseline systems
for 2D scene understanding tasks, is intuitive, simple, ﬂexi-

(a)

(b)

(c)
Instance segmentation for point clouds using
Figure 1:
SGPN. Different colors represent different instances.
(a)
Instance segmentation on complete real scenes. (b) Single
object part instance segmentation. (c) Instance segmenta-
tion on point clouds obtained from partial scans.

ble, and effective.

An important consideration for instance segmentation on
a point cloud is how to represent output results. Inspired by
the trend of predicting proposals for tasks with a variable
number of outputs, we introduce a Similarity Group Pro-
posal Network (SGPN), which formulates group proposals
of object instances by learning a novel 3D instance segmen-
tation representation in the form of a similarity matrix .

Our pipeline ﬁrst uses PointNet/PointNet++ to extract a
descriptive feature vector for each point in the point cloud.
As a form of similarity metric learning, we enforce the idea
that points belonging to the same object instance should
have very similar features; hence we measure the distance
between the features of each pair of points in order to form
a similarity matrix that indicates whether any given pair of

1

points belong to the same object instance.

The rows in our similarity matrix can be viewed as in-
stance candidates, which we combine with learned conﬁ-
dence scores in order to generate plausible group proposals.
We also learn a semantic segmentation map in order to clas-
sify each object instance obtained from our group proposals.
We are also able to directly derive tight 3D bounding boxes
for object detection.

By simply measuring the distance between overdeter-
mined feature representations of each pair of points, our
similarity matrix simpliﬁes our pipeline in that we remain
in the natural point cloud representation of deﬁning our ob-
jects by the relationships between points.

In summary, SGPN has three output branches for in-
stance segmentation on point clouds: a similarity matrix
yielding point-wise group proposals, a conﬁdence map for
pruning these proposals, and a semantic segmentation map
to give the class label for each group.

We evaluate our

framework on both 3D shapes
(ShapeNet [4]) and real 3D scenes (Stanford Indoor Se-
mantic Dataset [1] and NYUV2 [42]) and demonstrate that
SGPN achieves state-of-the-art results on 3D instance seg-
mentation. We also conduct comprehensive experiments
to show the capability of SGPN on achieving high perfor-
mance on 3D semantic segmentation and 3D object detec-
tion on point clouds. Although a minimalistic framework
with no bells and whistles already gives visually pleasing
results (Figure 1), we also demonstrate the ﬂexibility of
SGPN as we boost performance even more by seamlessly
integrating CNN features from RGBD images.

2. Related Works

2.1. Object Detection and Instance Segmentation

Recent advances in object detection [39, 14, 24, 37, 38,
26, 13, 25] and instance segmentation [23, 10, 9, 32, 31]
on 2D images have achieved promising results. R-CNN
[15] for 2D object detection established a baseline system
by introducing region proposals as candidate object regions.
Faster R-CNN [39] leveraged a CNN learning scheme and
proposed Region Proposal Networks(RPN). YOLO [37] di-
vided the image into grids and each grid cell produced
an object proposal. Many 2D instance segmentation ap-
proaches are based on segment proposals. DeepMask [31]
learns to generate segment proposals each with a corre-
sponding object score. Dai et al. [10] predict segment can-
didates from bounding box proposals. Mask R-CNN [18]
extended Faster R-CNN by adding a branch on top of RPN
to produce object masks for instance segmentation.

Following these pioneering 2D works, 3D bounding box
detection frameworks have emerged [40, 44, 45, 11, 5].
Song and Xiao [45] use a volumetric CNN to create 3D
RPN on a voxelized 3D scene and then use both the color

and depth data of the image in a joint 3D and 2D object
recognition network on each proposal. Deng and Latecki
[11] regress class-wise 3D bounding box models based on
RGBD image appearance features only. Armeni et al [1]
use a sliding shape method with CRF to perform 3D object
detection on point cloud. To the best of our knowledge, no
previous work exists that learns 3D instance segmentation.

2.2. 3D Deep Learning

Convolutional neural networks generalize well to 3D by
performing convolution on voxels for certain tasks such as
object classiﬁcation [34, 48, 27, 51, 41, 29, 30], shape
reconstruction [49, 17, 8] of simple objects, and 3D ob-
ject detection as mentioned in Section 2.1. However, vol-
umetric representation carry a high memory and compu-
tational cost and have strong limitations dealing with 3D
scenes [7, 1, 46]. Octree-based CNNs [41, 47, 48] have been
introduced recently, but they are less ﬂexible than volumet-
ric CNNs and still suffer from memory efﬁciency problems.
A point cloud is an intuitive, memory-efﬁcient 3D repre-
sentation well-suited for representing detailed, large scenes
for 3D instance segmentation using deep learning. Point-
Net/Pointnet++ [33, 35] recently introduce deep neural net-
works on 3D point clouds, learning successful results for
tasks such as object classiﬁcation and part and semantic
scene segmentation. We base our network architecture
off of PointNet/PointNet++, achieving a novel method that
learns 3D instance segmentation on point clouds.

2.3. Similarity Metric Learning

Our work is also closely related to similarity metric
learning, which has been widely used in deep learning on
various tasks such as person re-identiﬁcation [52], match-
ing [16], image retrival [12, 50] and face recognition [6].
Siamese CNNs [6, 43, 3] are used on tasks such as track-
ing [22] and one-shot learning [20] by measuring the simi-
larity of two input images. Alejandro et. al [28] introduced
an associative embedding method to group similar pixels for
multi-person pose estimation and 2D instance segmentation
by enforcing that pixels in the same group should have simi-
lar values in their embedding space without actually enforc-
ing what those exact values should be. Our method exploits
metric learning in a different way in that we regress the like-
lihood of two points belonging to the same group and for-
mulate the similarity matrix as group proposals to handle
variable number of instances.

3. Method

The goal of this paper is to take a 3D point cloud as in-
put and produce an object instance label for each point and
a class label for each instance. Utilizing recent develop-
ments in deep learning on point clouds [33, 35], we intro-
duce a Similarity Group Proposal Network (SGPN), which

consumes a 3D point cloud and outputs a set of instance
proposals that each contain the group of points inside the
instance as well as its class label. Section 3.1 introduces
the design and properties of SGPN. Section 3.2 proposes
an algorithm to merge similar groups and give each point
an instance label. Section 3.3 gives implementation details.
Figure 2 depicts the overview of our system.

3.1. Similarity Group Proposal Network

SGPN is a very simple and intuitive framework. As
shown in Figure 2, it ﬁrst passes a point cloud P of size
Np through a feed-forward feature extraction network in-
spired by PointNets [33, 35], learning both global and lo-
cal features in the point cloud. This feature extraction net-
work produces a matrix F . SGPN then diverges into three
branches that each pass F through a single PointNet layer to
obtain sized Np × Nf feature matrices FSIM , FCF , FSEM ,
which we respectively use to obtain a similarity matrix, a
conﬁdence map and a semantic segmentation map. The ith
row in a Np ×Nf feature matrix is a Nf -dimensional vector
that represents point Pi in an embedded feature space. Our
loss L is given by the sum of the losses from each of these
three branches: L = LSIM + LCF + LSEM . Our network
architecture can be found in the supplemental.

Similarity Matrix We propose a novel similarity matrix
S from which we can formulate group proposals to directly
recover accurate instance segmentation results. S is of di-
mensions Np × Np, and element Sij classiﬁes whether or
not points Pi and Pj belong to the same object instance.
Each row of S can be viewed as a proposed grouping of
points that form a candidate object instance.

We leverage that points belonging to the same object in-
stance should have similar features and lie very close to-
gether in feature space. We obtain S by, for each pair of
points {Pi, Pj}, simply subtracting their corresponding fea-
ture vectors {FSIMi, FSIMj } and taking the L2 norm such
that Sij = ||FSIMi − FSIMj ||2. This reduces the problem
of instance segmentation to learning an embedding space
where points in the same instance are close together and
those in different object instances are far apart.

For a better understanding of how SGPN captures corre-
lation between points, in Figure 3(a) we visualize the simi-
larity (euclidean distance in feature space) between a given
point and the rest of the points in the point cloud. Points in
different instances have greater euclidean distances in fea-
ture space and thus smaller similarities even though they
have the same semantic labels. For example, in the bottom-
right image of Figure 3(a), although the given table leg point
has greater similarity with the other table leg points than the
table top, it is still distinguishable from the other table leg.
We believe that a similarity matrix is a more natural and
simple representation for 3D instance segmentation on a

point cloud compared to traditional 2D instance segmenta-
tion representations. Most state-of-the-art 2D deep learning
methods for instance segmentation ﬁrst localize the image
into patches, which are then passed through a neural net-
work and segment a binary mask of the object.

While learning a binary mask in a bounding box is
a more natural representation for space-centric structures
such as images or volumetric grids where features are
largely deﬁned by which positions in a grid have strong sig-
nals, point clouds can be viewed as shape-centric structures
where information is encoded by the relationship between
the points in the cloud, so we would prefer to also deﬁne
instance segmentation output by the relationship between
points without working too much in grid space.

Hence we expect that a deep neural network could better
learn our similarity matrix, which compared to traditional
representations is a more natural and straightforward rep-
resentation for instance segmentation in a point cloud.

Double-Hinge Loss for Similarity Matrix As is the case
in [28], in our similarity matrix we do not need to precisely
regress the exact values of our features; we only optimize
the simpler objective that similar points should be close to-
gether in feature space. We deﬁne three potential similarity
classes for each pair of points {Pi, Pj}: 1) Pi and Pj belong
to the same object instance; 2) Pi and Pj share the same se-
mantic class but do not belong to the same object instance;
3) Pi and Pj do not share the same semantic class. Pairs
of points should lie progressively further away from each
other in feature space as their similarity class increases. We
deﬁne out loss as:

LSIM =

l(i, j)

Np
(cid:88)

Np
(cid:88)

i

j

l(i, j) =






||FSIMi − FSIMj ||2
Cij = 1
α max(0, K1 − ||FSIMi − FSIMj ||2) Cij = 2
Cij = 3
max(0, K2 − ||FSIMi − FSIMj ||2)

where Cij indicates which of the similarity classes deﬁned
above does the pair of points ({Pi, Pj)} belong to and
α, K1, K2 are constants such that α > 1, K2 > K1.

Although the second and third similarity class are treated
equivalently for the purposes of instance segmentation, dis-
tinguishing between them in LSIM using our double-hinge
loss allows our similarity matrix output branch and our se-
mantic segmentation output branch to mutually assist each
other for increased accuracy and convergence speed. Since
the semantic segmentation network is actually wrongly try-
ing to bring pairs of points in our second similarity class
closer together in feature space, we also add an α > 1 term
to increase the weight of our loss to dominate the gradient
from the semantic segmentation output branch.

Figure 2: Pipeline of our system for point cloud instance segmentation.

(a)

(b)

Figure 3: (a) Similarity (euclidean distance in feature space)
between a given point (indicated by red arrow) and the rest
of points. A darker color represents lower distance in fea-
ture space thus higher similarity. (b) Conﬁdence map. A
darker color represents higher conﬁdence.

At test time if Sij < T hS where T hS < K1, then points

pair Pi and Pj are in the same instance group.

Similarity Conﬁdence Map SGPN also feeds FCF
through an additional PointNet layer to predict a Np × 1
conﬁdence map CM reﬂecting how conﬁdently the model
believes that each grouping candidate is indeed a correct
object instance. Figure 3(b) provides a visualization of the
conﬁdence map; points located in the boundary area be-
tween parts have lower conﬁdence.

We regress conﬁdence scores based on ground truth
groups G represented as a Np × Np matrix identical in form
to our similarity matrix. If Pi is a background point that
does not belong to any object in the ground truth then the
row Gi will be all zeros. For each row in Si, we expect the
ground-truth value in the conﬁdence map CMi to be the in-
tersection over union (IoU) between the set of points in the
predicted group Si and the ground truth group Gi. Our loss
LCF is the L2 loss between the inferred and expected CM .
Although the loss LCF depends on the similarity ma-
trix output branch during training, at test time we run the
branches in parallel and only groups with conﬁdence greater
than a threshold T hC are considered valid group proposals.

Semantic Segmentation Network The semantic seg-
mentation map acts as a point-wise classiﬁer. SGPN passes
FSEM through an additional PointNet layer whose archi-
tecture depends on the number of possible semantic classes,
yielding the ﬁnal output MSEM , which is a Np × NC sized
matrix where NC is the number of possible object cate-
gories. MSEMij corresponds to the probability that point
Pi belongs to class Cj.

The loss LSEM is a weighted sum of the cross entropy
softmax loss for each row in the matrix. We use median fre-
quency balancing [2] and the weight assigned to a category
is ac = medianf req/f req(c), where f req(c) is the total
number of points of class c divided by the total number of
points in samples where c is present, and medianf req is
the median of these f req(c).

At test time, the class label for a group instance is as-
signed by calculating the mode of the semantic labels of the
points in that group.

3.2. Group Proposal Merging

The similarity matrix S produces Np group proposals,
many of which are noisy or represent the same object. We
ﬁrst discard proposals with predicted conﬁdence less than
T hC or cardinality less than T hM 2. We further prune
our proposals into clean, non-overlapping object instances
by applying Non-Maximum Suppression; groups with IoU
greater than T hM 1 are merged together by selecting the
group with the maximum cardinality.

Each point is then assigned to the group proposal that
contains it. In the rare case (∼ 2%) that after the merging
stage a point belongs to more than one ﬁnal group proposal,
this usually means that the point is at the boundary between
two object instances, which means that the effectiveness of
our network would be roughly the same regardless of which
group proposal the point is assigned to. Hence, with min-
imal loss in accuracy we randomly assign the point to any
one of the group proposals that contains it. We refer to this
process as GroupMerging throughout the rest of the paper.

3.3. Implementation Details

We use an ADAM [19] optimizer with initial learning
rate 0.0005, momentum 0.9 and batch size 4. The learn-
ing rate is divided by 2 every 20 epochs. The network is
trained with only the LSIM loss for the ﬁrst 5 epochs. In
our experiment, α is set to 2 initially and is increased by 2
every 5 epochs. This design makes the network more fo-
cused on separating features of points that belong to dif-
ferent object instances but have the same semantic labels.
K1, K2 are set to 1.0 and 2.0, respectively. We use per-
category histogram thresholding to get the threshold point
T hs for each testing sample. T hM 1 is set to 0.6 and T hM 2
is set to 200. T hC is set to 0.1. Our network is imple-
mented with Tensorﬂow and a single Nvidia GTX1080 Ti

GPU. It takes 16-17 hours to converge. At test time, SGPN
takes 40ms on an input point cloud with size 4096 × 9 with
PointNet++ as our baseline architecture. Further runtime
analysis can be found in Section 4.2. Code is availabel at
github.com/laughtervv/SGPN.

4. Experiments

following datasets:

We evaluate SGPN on 3D instance segmentation on the

• Stanford 3D Indoor Semantics Dataset (S3DIS) [1]:
This dataset contains 3D scans in 6 areas including 271
rooms. The input is a complete point cloud generated
from scans fused together from multiple views. Each
point has semantic labels and instance annotations.

• NYUV2 [42]: Partial point clouds are generated from
single view RGBD images. The dataset is annotated
with 3D bounding boxes and 2D semantic segmenta-
tion masks. We use the improved annotation in [11].
Since both 3D bounding boxes and 2D segmentation
masks annotations are given, ground truth 3D instance
segmentation labels for point clouds can be easily gen-
erated We follow the standard split with 795 training
images and 654 testing images.

• ShapeNet [4, 53] Part Segmentation: ShapeNet con-
tains 16, 881 shapes annotated with 50 types of parts
in total. Most object categories are labeled with two
to ﬁve parts. We use the ofﬁcial split of 795 training
samples and 654 testinn percentageg samples in our
experiments.

We also show the capability of SGPN to improve seman-
tic segmentation and 3D object detection. To validate the
ﬂexibility of SGPN, we also seamlessly incorporate 2D
CNN features into our network to boos performance on the
NYUV2 dataset.

4.1. S3DIS Instance Segmentation and 3D Object

Detection

We perform experiments on Stanford 3D Indoor Seman-
tic Dataset to evaluate our performance on large real scene
scans. Following experimental settings in PointNet [33],
points are uniformly sampled into blocks of area 1m × 1m.
Each point is labeled as one of 13 categories (chair, table,
ﬂoor, wall, clutter etc.) and represented by a 9D vector
(XYZ, RGB, and normalized location as to the room). At
train time we uniformly sample 4096 points in each block,
and at test time we use all points in the block as input.

SGPN uses PointNet as its baseline architecture for this
experiment.1 Figure 5 shows instance segmentation results
on S3DIS with SGPN. Different colors represent different

1PointNet [33] proposed a 3D detection system while PointNet++ [35]

instances. Point colors of the same group are not necessarily
the same as their counterparts in the ground truth since ob-
ject instances are unordered. To visualize instance classes,
we also add semantic segmentation results. SGPN achieves
good performance on various room types.

We also compare instance segmentation performance
with the following method (which we call Seg-Cluster):
Perform semantic segmentation using our network and then
select all points as seeds. Starting from a seed point, BFS
is used to search neighboring points with the same label. If
a cluster with more than 200 points has been found, it is
viewed as a valid group. Our GroupMerging algorithm is
then used to merge these valid groups.

We calculate the IoU on points between each predicted
and ground truth group. A detected instance is considered
as true positive if the IoU score is greater than a threshold.
The average precision (AP) is further calculated for instance
segmentation performance evaluation. Table 1 shows the
AP for every category with IoU threshold 0.5. To the best
of our knowledge, there are no existing instance segmenta-
tion method on point clouds for arbitrary object categories,
so we further demonstrate the capability of SGPN to handle
various objects by adding the 3D detection results of Ar-
meni et al.
[1] on S3DIS to Table 1. The difference in
evaluation metrics between our method and [1] is that the
IoU threshold of [1] is 0.5 on a 3D bounding box and the
IoU calculation of our method is on points. Despite this dif-
ference in metrics, we can still see our superior performance
on both large and small objects.

We see that a naive method like Seg-Cluster tends to
properly separate regions far away for large objects like the
ceiling and ﬂoor. However for small object, Seg-Cluster
fails to segment instances with the same label if they are
close to each other. Mean APs with different IoU thresh-
olds (0.25, 0.5, 0.75) are also evaluated in Table 2. Figure 4
shows qualitative comparison results.

Once we have instance segmentation results, we can
compute the bounding box for every instance and thus pro-
duce 3D object detection predictions. In Table 3, we com-
pare out method with the 3D object detection system intro-
duced in PointNet [33], which to the best of our knowledge
is the state-of-the-art method for 3D detection on S3DIS.
Detection performance is evaluated over 4 categories AP
with IoU threshold 0.5.

The method introduced in PointNet clusters points given
semantic segmentation results and uses a binary classiﬁca-
tion network for each category to separate close objects with
same categories. Our method outperforms it by a large mar-
gin, and unlike PointNet does not require an additional net-
work, which unnecessarily introduces additional complex-

does not. To make fair comparison, we use PointNet as our baseline ar-
chitecture for this experiment while using PointNet++ in Sections 4.2 and
4.3.

Mean
Seg-Cluster 17.40
36.30

SGPN

ceiling ﬂoor
70.01
80.12
83.67
58.42

wall
10.64
42.24

beam column window door
32.32
15.30
45.23
25.64

28.97
42.73

0.00
7.15

table
22.16
38.25

chair
27.76
47.05

sofa
0.00
0.00

bookcaseboard
21.52
31.68

0.06
13.57

Table 1: Results on instance segmentation in S3DIS scenes. The metric is AP(%) with IoU threshold 0.5. To the best of our
knowledge, there are no existing instance segmentation methods on point clouds for arbitrary object categories.

Seg-Cluster 34.8
52.6

SGPN

AP0.25 AP0.5 AP0.75
11.2
17.4
18.8
36.3

Table 2: Comparison results on instance segmentation with
different IoU thresholds in S3DIS scenes. Metric is mean
AP(%) over 13 categories.

Mean table chair sofa board
PointNet [33] 24.24 46.67 33.80 4.76 11.72
Seg-Cluster 18.72 33.44 22.8 5.38 13.07
30.20 49.90 40.87 6.96 13.28

SGPN

Table 3: Comparison results on 3D detection in S3DIS
scenes. SGPN uses PointNet as baseline. The metric is AP
with IoU threshold 0.5.

Mean IoU Accuracy

PointNet [33] 49.76
50.37

SGPN

79.66
80.78

Table 4: Results on semantic segmentation in S3DIS scenes.
SGPN uses PointNet as baseline. Metric is mean IoU(%)
over 13 classes (including clutter).

ity during both train and test time and local minima dur-
ing train time. SGPN can effectively separate the difﬁcult
cases of objects of the same semantic class but different in-
stances (c.f. Figure 4) since points in different instances are
far apart in feature space even though they have the same
semantic label. We further compare our semantic segmen-
tation results with PointNet in Table 4. SGPN outperforms
its baseline with the help of its similarity matrix.

4.2. NYUV2 Object Detection and Instance Seg-

mentation Evaluation

We evaluate the effectiveness of our approach on partial
3D scans on the NYUV2 dataset. In this dataset, 3D point
clouds are lifted from a single RGBD image. An image of
size H × W can produce H × W points. We subsample
this point cloud by resizing the image to H
4 and get the
corresponding points using a nearest neighbor search. Both
our training and testing experiments are conducted on such
a point cloud. PointNet++ is used as our baseline.

4 × W

In [36], 2D CNN features are combined 3D point cloud
for RGBD semantic segmentation. By leveraging the ﬂexi-
bility of SGPN, we also seamlessly integrate 2D CNN fea-
tures from RGB images to boost performance. A 2D CNN
consumes an RGBD map and extracts feature maps F2 with
size H
4 sub-sampled

4 × NF 2. Since there are H

4 × W

4 × W

(e)

(a)

(d)

(b)

(c)
Figure 4: Comparison results on S3DIS. (a) Ground Truth
for instance segmentation. Different colors represents dif-
ferent instances. (b) SGPN instance segmentation results.
(c) Seg-Cluster instance segmentation results. (d) Ground
Truth for semantic segmentation. (e) Semantic Segmenta-
tion and 3D detection results of SGPN. The color of the
detected bounding box for each object category is the same
as the semantic labels.

points for every image, a feature vector of size Nf 2 can
be extracted from F2 at each pixel location. Every fea-
ture vector is concatenated to F (a Np × NF feature ma-
trix produced by PointNet/PointNet++ as mentioned in Sec-
tion 3.1) for each corresponding point, yielding a feature
map of size NP × (NF + NF 2), which we then feed to
our output branches. Figure 6 illustrates this procedure;
we call this pipeline SGPN-CNN. In our experiments, we
use a pre-trained AlexNet model [21] (with the ﬁrst layer
stride 1) and extract F2 from the conv5 layer. We use
H × W = 316 × 415 and Np = 8137. The 2D CNN
and SGPN are trained jointly.

Evaluation is performed on 19 object categories. Fig-
ure 7 shows qualitative results on instance segmentation of
SGPN. Table 5 shows comparisons between Seg-Cluster
and our SGPN and CNN-SGPN frameworks on instance
segmentation. The evaluation metric is average precision
(AP) with IoU threshold 0.5.

The margin of improvement for SGPN compared to Seg-
Cluster is not as high as it is on S3DIS, because in this
dataset objects with the same semantic label are usually far
apart in Euclidean space. Additionally, naive methods like
Seg-Cluster beneﬁt since it is easy to separate a single in-
stance into parts since the points are not connected due to

Mean

Seg-Cluster 23.2 31.0 70.1 27.1 1.3 25.8 20.3 13.9 11.1 24.3 4.4 16.3 3.6 32.0 25.5 36.3 50.9 12.9 2.2 23.5
26.5 55.9 53.3 27.8 0.0 27.4 59.6 28.9 6.1 33.9 2.0 19.7 2.0 29.4 30.7 39.1 43.6 17.6 1.2 25.9
SGPN-CNN 30.5 56.4 55.4 35.2 0.0 42.6 50.6 23.1 21.1 31.8 7.5 22.7 6.4 39.9 33.5 42.4 54.8 21.3 3.8 32.1

SGPN

Table 5: Results on instance segmentation in NYUV2. The metric is AP with IoU 0.5.

Figure 5: SGPN instance segmentation results on S3DIS.
The ﬁrst row is the prediction results. The second row is
groud truths. Different colors represent different instances.
The third row is the predicted semantic segmentation re-
sults. The fourth row is the ground truths for semantic seg-
mentation.

Figure 6: Incorporating CNN features in SGPN.

occlusion in partial scanning. Table 5 also illustrates that
SGPN can effectively utilize CNN features. Instead of con-
catenating fully-connected layer of 2D and 3D networks as
in [45], we combine 2D and 3D features by considering
their geometric relationships.

(a)

(b)

(c)

(d)

Figure 7: SGPN instance segmentation results on NYUV2.
(a) Input point clouds. (b) Ground truths for instance seg-
mentation.
(c) Instance segmentation results with SGPN.
(d) Instance segmentation results with SGPN-CNN.

Mean
Deep Sliding Shapes [45] 37.55 58.2 36.1 27.2 28.7
Deng and Latecki [11] 35.55 46.4 33.1 33.3 29.4
36.25 44.4 30.4 46.1 24.4
41.30 50.8 34.8 49.4 30.2

SGPN
SGPN-CNN

Table 6: Comparison results on 3D detection (AP with IoU
0.5) in NYUV2. Please note we use point groups as infer-
ence while [45, 11] use large bounding box with invisible
regions as ground truth. Our prediction is the tight bound-
ing box on points which makes the IoU much smaller than
[45, 11].

We further calculate bounding boxes with instance seg-
mentation results. Table 6 compares our work with the
state-of-the-art works [45, 11] on NYUV2 3D object detec-
tion. Following the evaluation metric in [44], AP is calcu-
lated with IoU threshold 0.25 on 3D bounding boxes. The
NYUV2 dataset provides ground truth 3D bounding boxes
that encapsulate the whole object including the part that is
invisible in the depth image. Both [45] and [11] use these
large ground truth bounding boxes for inference.
In our
method, we infer point groupings, which lack information
of the invisible part of the object. Our output is the derived
tight bounding box around the grouped points in the partial
scan, which makes our IoUs much smaller than [45, 11].
However, we can still see the effectiveness of SGPN on the

Mean

air-
plane

bag cap

car chair head
phone

guitar knife lamp laptopmotor mug pistol rocket skate
board

table

[35] 84.6 80.4 80.9 60.0 76.8 88.1 83.7 90.2 82.6 76.9 94.7 68.0 91.2 82.1 59.9 78.2 87.5
SGPN 85.8 80.4 78.6 78.8 71.5 88.6 78.0 90.9 83.0 78.8 95.8 77.8 93.8 87.4 60.1 92.3 89.4

Table 7: Semantic segmentation results on ShapeNet part dataset. Metric is mean IoU(%) on points.

(a)

(b)

(c)

(d)

Figure 8: Qualitative results on ShapeNet Part Dataset.
(a) Generated ground truth for instance segmentation. (b)
SGPN instance segmentation results. (c) Semantic segmen-
tation results of PointNet++. (d) Semantic segmentation re-
sults of SGPN.

task of 3D object detection on partial scans as our method
achieves better performance on small objects.

Computation Speed To benchmark the testing time with
[45, 11] and make fair comparison, we run our framework
on an Nvidia K40 GPU. SGPN takes 170ms and around
400M GPU memory per sample. CNN-SGPN takes 300ms
and 1.4G GPU memory per sample. GroupMerging
takes 180ms on an Intel i7 CPU. However, the detection net
in [11] takes 739ms on an Nvidia Titan X GPU. In [45],
RPN takes 5.62s and ORN takes 13.93s per image on an
Nvidia K40 GPU. Our model improves the efﬁciency and
reduces GPU memory usage by a large margin.

4.3. ShapeNet Part Instance Segmentation

Following the settings in [35], point clouds are gener-
ated by uniformly sampling shapes from Shapenet [4]. In
our experiments we sample each shape into 2048 points.
The XYZ of points are fed into network as input with size
2048 × 3. To generate ground truth labels for part instance
segmentation from semantic segmentation results, we per-
form DBSCAN clustering on each part category of an object
to group points into instances. This experiment is conducted
as a toy example to demonstrate the effectiveness of our ap-
proach on instance segmentation for pointclouds.

We use Pointnet++ as our baseline. Figure 8(b) illus-
trates the instance segmentation results. For instance re-
sults, we again use different colors to represent different

instances, and point colors of the same group are not nec-
essarily the same as the ground truth. Since the generated
ground truths are not “real” ground truths, only qualitative
results are provided. SGPN achieves good results even un-
der challenging conditions. As we can see from the Fig-
ure 8, SGPN is able to group the chair legs into four in-
stances even though even in the ground truth DBSCAN can
not separate the chair legs apart.

The similarity matrix can also help the semantic segmen-
tation branch training. We compare SGPN to PointNet++
(i.e. our framework with solely a semantic segmentation
branch) on semantic segmentation in Table 7. The inputs
of both networks are point clouds of size 2048. Evaluation
metric is mIoU on points of each shape category. Our model
performs better than PointNet++ due to the similarity ma-
trix. Qualitative results are shown in Figure 8. Some false
segmentation prediction is reﬁned with the help of SGPN.

5. Conclusion

We present SGPN, an intuitive, simple, and ﬂexible
framework for 3D instance segmentation on point clouds.
With the introduction of the similarity matrix as our out-
put representation, group proposals with class predictions
can be easily generated from a single network. Experiments
show that our algorithm can achieve good performance on
instance segmentation for various 3D scenes and facilitate
the tasks of 3D object detection and semantic segmentation.

Future Work While a similarity matrix provides an intu-
itive representation and an easily deﬁned loss function, one
limitation of SGPN is that the size of the similarity matrix
scales quadratically as Np increases. Thus, although much
more memory efﬁcient than volumetric methods, SGPN
cannot process extremely large scenes on the order 105 or
more points. Future research directions can consider gener-
ating groups using seeds that are selected based on SGPN
to reduce the size of the similarity matrix. SGPN can also
be extended in future works to learn in a more unsupervised
setting or to learn more different kinds of data representa-
tions beyond instance segementation.

A. Network Architecture

In our experiments, we use both PointNet and Point-
Net++ as our baseline architectures. For the S3DIS dataset,
we use PointNet as our baseline for fair comparison with
the 3D object detection system described in the PointNet

paper [33]. The network architecture is the same as the se-
mantic segmentation network as stated in PointNet except
for the last two layers. Our F is the last 1 × 1 conv layer
with BatchNorm and ReLU in PointNet with 256 output
channels. FSIM , FCF , FSEM are 1 × 1 conv layers with
output channels (128, 128, 128), respectively.

For the NYUV2 dataset, we use PointNet++ as our base-
line. We use the same notations as PointNet++ to describe
our architecture:

SA(K, r, [l1, ..., ld]) is a set abstraction (SA) level with
K local regions of ball radius r using a PointNet architec-
ture of d 1 × 1 conv layers with output channels li(i =
1, ..., d). F P (l1, ..., ld) is a feature propagation (F P ) level
with d 1 × 1 conv layers. Our network architecture is:

SA(1024, 0.1, [32, 32, 64]),

SA(256, 0.2, [64, 64, 128]),

SA(128, 0.4, [128, 128, 256]),

SA(64, 0.8, [256, 256, 256]),

SA(16, 1.2, [256, 256, 512]),

F P (512, 256),

F P (256, 256),

F P (256, 256),

F P (256, 128),

F P (128, 128, 128, 128).

FSIM , FCF , FSEM are 1 × 1 conv layers with output chan-
nels (128, 128, 128) respectively.

For our experiments on the ShapeNet part dataset,
PointNet++ is used as our baseline. We use the same
network architecture as in the PointNet++ paper [35].
FSIM , FCF , FSEM are 1 × 1 conv layers with output chan-
nels (64, 64, 64), respectively.

B. Experiment Settings

B.1. S3DIS Dataset

Block Merging We divide each scene into 1m × 1m
blocks with overlapping sliding windows in a snake pat-
tern of stride 0.5m as is shown in Figure 9. The en-
tire scene is also divided into a 400 × 400 × 400 grid V .
Vk is used to indicate the instance label of cell k where
k ∈ [0, 400×400×400). Given V and point instance labels
for each block P L where P Lij represents the instance la-
bel of jth point in block i, a BlockMerging algorithm (refer
to Algorithm 1) is derived to merge object instances from
different blocks.

In Figure 10, we show more qualitative results of in-

stance segmentation with SGPN.

Figure 9: Dividing scene into blocks with overlap (top
view).

Algorithm 1: BlockMeriging

: V , P L

Input
Output: Point instance labels for the whole scene L

1 Initialize V with all elements −1;
2 GroupCount ← 0;
3 for every block i do
4

if i is the 1st block then

for every point Pj in block i do

Deﬁne k where Pj is located in the kth
cell of V ;
Vk ← P L1j;

end

else

for every instance Ij in block i do

Deﬁne VIj points in Ij are located in cells
VIj ;
Vt ← the cells in VIj that do not have
value −1;
if the frequency of the mode in Vt < 30
then

VIj ← GroupCount;
GroupCount ← GroupCount + 1;

VIj ← the mode of Vt;

else

end

end

end

20
21 end
22 for every point Pj in the whole scene do
23

Deﬁne k where Pj is located in the kth cell of V ;
Lj ← Vk;

24
25 end

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

Mean wall ﬂoor chair

coun-
ter
Seg-Cluster 32.5 34.2 87.6 61.4 40.4 20.7 43.1 19.2 33.8 33.5 44.4 60.0 39.8 24.5
35.1 55.5 86.7 64.4 41.1 40.7 42.7 36.1 39.6 38.6 45.1 50.3 15.9 27.0

table desk

book
shelf

SGPN

bath
tub

cur-
tain

toilet

sink

sofa

bed

door

win-
dow

0
0

0
0

fridge

shower
curtain
19.4 22.0
19.3 28.6

pic-
ture

0
0

cabi-
net

33.5
35.1

Table 8: Instance segmentation results on ScanNet(v1). The metric is AP (%) with IoU threshold 0.25. We observe 0 percent
AP on items that appear on the wall (door, window, picture) as they contain very little depth information and are almost all
incorrectly semantically labeled as the wall. Future works can explore addressing this problem.

C. More Experiments

C.1. ScanNet

We provide more experimental results on ScanNet [7].
This dataset contains 1513 scanned and reconstructed in-
door scenes. We use the ofﬁcial split with 1201 scenes for
training and 312 for testing. Following the same Block-
Merging procedure, each scene is divided into 1.5m×1.5m
blocks and each block is uniformly sampled into 4096
points for training. All points in the block are used at test
time. Each point is represented by a 9D vector (XYZ, RGB,
and normalized location with respect to the room scene).
PointNet++ is used as the baseline. The network architec-
ture is:

SA(1024, 0.1, [32, 32, 64]),

SA(256, 0.2, [64, 64, 128]),

SA(64, 0.4, [128, 128, 256]),

SA(16, 0.8, [256, 256, 512]),
F P (256, 256),

F P (256, 256),

F P (256, 128),

F P (128, 128, 128, 128).

And FSIM , FCF , FSEM are 1 × 1 conv layers with output
channels (128, 128, 128) respectively. Table 8 illustrates the
quantitative comparison results with Seg-Cluster. The met-
ric is average precision (AP) with IoU threshold 0.25. Fig-
ure 11 shows instance segmentation results on ScanNet.

References

[1] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis,
M. Fischer, and S. Savarese. 3d semantic parsing of large-
scale indoor spaces. In CVPR, 2016. 2, 5

[2] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. TPAMI, 2017. 4

[3] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and
P. H. S. Torr. Fully-convolutional siamese networks for ob-
ject tracking. In ECCV Workshops, 2016. 2

[4] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,

et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015. 2, 5, 8

[5] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d
object detection network for autonomous driving. In CVPR,
2017. 2

[6] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005. 2

[7] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. Scannet: Richly-annotated 3d reconstruc-
tions of indoor scenes. In CVPR, 2017. 2, 10

[8] A. Dai, C. R. Qi, and M. Nießner. Shape completion using
In CVPR,

3d-encoder-predictor cnns and shape synthesis.
2017. 2

[9] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive

fully convolutional networks. In ECCV, 2016. 2

[10] J. Dai, K. He, and J. Sun. Instance-aware semantic segmen-
tation via multi-task network cascades. In CVPR, 2016. 1,
2

[11] Z. Deng and L. J. Latecki. Amodal detection of 3d objects:
Inferring 3d bounding boxes from 2d ones in rgb-depth im-
ages. In CVPR, 2017. 1, 2, 5, 7, 8

[12] A. Frome, Y. Singer, F. Sha, and J. Malik. Learning globally-
consistent local distance functions for shape-based image re-
trieval and classiﬁcation. In ICCV, 2007. 2

[13] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
Dssd: Deconvolutional single shot detector. arXiv preprint
arXiv:1701.06659, 2017. 2

[14] R. Girshick. Fast r-cnn. In ICCV, 2015. 2
[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014. 2

[16] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg.
Matchnet: Unifying feature and metric learning for patch-
based matching. In CVPR, 2015. 2

[17] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu. High-
resolution shape completion using deep neural networks for
In ICCV,
global structure and local geometry inference.
2017. 2

[18] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn.

In ICCV, 2017. 1, 2

[19] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014. 4

[20] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neu-
ral networks for one-shot image recognition. In ICML Deep
Learning Workshop, 2015. 2

Prediction

Ground Truth

Prediction

Ground Truth

Figure 10: Instance segmentation results on S3DIS with SGPN. Different colors represent different instances. The colors of
the same object in ground truth and prediction are not necessarily the same.

[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 6

Imagenet
In

[22] L. Leal-Taix, C. Canton-Ferrer, and K. Schindler. Learning
by tracking: siamese cnn for robust target association. CVPR
DeepVision Workshops, 2016. 2

[23] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei. Fully convolutional
instance-aware semantic segmentation. In CVPR, 2017. 1, 2
[24] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
In CVPR, 2017. 2

[25] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar. Focal

Prediction

Ground Truth

Prediction

Ground Truth

Figure 11: Instance segmentation results on ScanNet with SGPN. Different colors represent different instances. The colors
of the same object in ground truth and prediction are not necessarily the same.

loss for dense object detection. In CVPR, 2017. 2

2015. 2

[26] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
Fu, and A. C. Berg. SSD: Single shot multibox detector. In
ECCV, 2016. 2

[28] A. Newell and J. Deng. Associative embedding: End-to-end
learning for joint detection and grouping. In NIPS, 2016. 2,
3

[27] D. Maturana and S. Scherer. Voxnet: A 3d convolutional
In IROS,

neural network for real-time object recognition.

[29] G. Pang and U. Neumann. 3d point cloud object detec-
tion with multi-view convolutional neural network. In ICPR,

[50] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiﬁcation. Journal of
Machine Learning Research, 2009. 2

[51] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In CVPR, 2015. 2

[52] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Deep metric learning for

person re-identiﬁcation. In ICPR, 2014. 2

[53] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, A. Lu,
Q. Huang, A. Sheffer, L. Guibas, et al. A scalable active
framework for region annotation in 3d shape collections.
ACM Transactions on Graphics (TOG), 2016. 5

2016. 2

[30] G. Pang, R. Qiu, J. Huang, S. You, and U. Neumann. Auto-
matic 3d industrial point cloud modeling and recognition. In
MVA, 2015. 2

[31] P. O. Pinheiro, R. Collobert, and P. Doll´ar. Learning to seg-

ment object candidates. In NIPS, 2015. 1, 2

[32] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-

ing to reﬁne object segments. In ECCV, 2016. 2

[33] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
CVPR, 2017. 1, 2, 3, 5, 6, 9

[34] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view cnns for object classi-
ﬁcation on 3d data. In CVPR, 2016. 2

[35] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
In NIPS, 2017. 1, 2, 3, 5, 8, 9

[36] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun. 3d graph
neural networks for rgbd semantic segmentation. In CVPR,
2017. 6

[37] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You
only look once: Uniﬁed, real-time object detection.
In
CVPR, 2016. 2

[38] J. Redmon and A. Farhadi. Yolo9000: Better, faster, stronger.

In CVPR, 2017. 2

[39] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
NIPS, 2015. 2

[40] Z. Ren and E. B. Sudderth. Three-dimensional object detec-
tion and layout prediction using clouds of oriented gradients.
In CVPR, 2016. 2

[41] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learning
deep 3d representations at high resolutions. In CVPR, 2017.
2

[42] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor seg-
mentation and support inference from rgbd images. ECCV,
2012. 2, 5

[43] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and
F. Moreno-Noguer. Discriminative learning of deep convo-
lutional feature point descriptors. In ICCV, 2015. 2

[44] S. Song and J. Xiao. Sliding shapes for 3d object detection

in depth images. In ECCV, 2014. 1, 2, 7

[45] S. Song and J. Xiao. Deep Sliding Shapes for amodal 3D
object detection in RGB-D images. In CVPR, 2016. 1, 2, 7,
8

[46] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and
T. Funkhouser. Semantic scene completion from a single
depth image. arXiv preprint arXiv:1611.08974, 2016. 2
[47] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3d outputs. In ICCV, 2017. 2

[48] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong.
O-cnn: Octree-based convolutional neural networks for 3d
shape analysis. ACM Transactions on Graphics (TOG),
2017. 2

[49] W. Wang, Q. Huang, S. You, C. Yang, and U. Neumann.
Shape inpainting using 3d generative adversarial network
and recurrent convolutional networks. In ICCV, 2017. 2

