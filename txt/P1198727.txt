9
1
0
2
 
p
e
S
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
4
4
3
0
.
9
0
9
1
:
v
i
X
r
a

Iterative Spectral Method for Alternative Clustering

Stratis Ioannidis
Chieh Wu
David Kaeli
Xiangyu Li
Electrical and Computer Engineering Dept., Northeastern University, Boston, MA

Mario Sznaier
Jennifer G. Dy

Abstract

Given a dataset and an existing clustering as
input, alternative clustering aims to ﬁnd an
alternative partition. One of the state-of-the-
art approaches is Kernel Dimension Alterna-
tive Clustering (KDAC). We propose a novel
Iterative Spectral Method (ISM) that greatly
improves the scalability of KDAC. Our al-
gorithm is intuitive, relies on easily imple-
mentable spectral decompositions, and comes
with theoretical guarantees. Its computation
time improves upon existing implementations
of KDAC by as much as 5 orders of magni-
tude.

1 Introduction

Clustering, i.e., the process of grouping similar objects
in a dataset together, is a classic problem. It is exten-
sively used for exploratory data analysis. Traditional
clustering algorithms typically identify a single par-
titioning of a given dataset. However, data is often
multi-faceted and can be both interpreted and clus-
tered through multiple viewpoints (or, views). For ex-
ample, the same face data can be clustered based on
either identity or based on pose. In real applications,
partitions generated by a clustering algorithm may not
correspond to the view a user is interested in.

In this paper, we address the problem of ﬁnding an
alternative clustering, given a dataset and an existing,
pre-computed clustering. Ideally, one would like the
alternative clustering to be novel (i.e., non-redundant)
w.r.t. the existing clustering to reveal a new viewpoint
to the user. Simultaneously, one would like the result
to reveal partitions of high clustering quality. Sev-
eral recent papers propose algorithms for alternative

clustering [1–6]. Among them, Kernel Dimension Al-
ternative Clustering (KDAC) is a ﬂexible approach,
shown to have superior performance compared to sev-
eral competitors [6]. KDAC is as powerful as spec-
tral clustering in discovering arbitrarily-shaped clus-
ters (including ones that are not linearly separable)
that are non-redundant w.r.t. an existing clustering.
As an additional advantage, KDAC can simultane-
ously learn the subspace in which the alternative clus-
tering resides.

The ﬂexibility of KDAC comes at a price: the KDAC
formulation involves optimizing a non-convex cost
function constrained over the space of orthogonal ma-
trices (i.e, the Stiefel manifold). Niu et al. [6] pro-
posed a Dimension Growth (DG) heuristic for solv-
ing this optimization problem, which is nevertheless
highly computationally intensive. We elaborate on its
complexity in Section 2; experimentally, DG is quite
slow, with a convergence time of roughly 46 hours on
an Intel Xeon CPU, for a 624 sample-sized face data
(c.f. Section 4). This limits the applicability of KDAC
in interactive exploratory data analysis settings, which
often require results to be presented to a user within
a few seconds. It also limits the scalability of KDAC
to large data. Alternately, one can solve the KDAC
optimization problem by gradient descent on a Stiefel
manifold (SM) [7]. However, given the lack of convex-
ity, both DG or SM are prone to get trapped to local
minima. Multiple iterations with random initializa-
tions are required to ameliorate the eﬀect of locality.
This increases computation time, and also decreases
in eﬀectiveness as the dimensionality of the data in-
creases: the increase in dimension rapidly expands the
search space and the abundance of local minima. As
such, with both DG and SM, the clustering quality is
negatively aﬀected by an increase in dimension.

Our Contributions. Motivated by the above issues,
we make the following contributions:

Proceedings of the 21st International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2018, Lan-
zarote, Spain. PMLR: Volume 84. Copyright 2018 by the
author(s).

• We propose an Iterative Spectral Method (ISM), a
novel algorithm for solving the non-convex optimiza-
tion constrained on a Stiefel manifold problem inher-
ent in KDAC. Our algorithm has several highly de-

Iterative Spectral Method for Alternative Clustering

sirable properties. First, it signiﬁcantly outperforms
traditional methods such as DG and SM in terms of
both computation time and quality of the produced
alternative clustering. Second, the algorithm relies
on an intuitive use of iterative spectral decomposi-
tions, making it both easy to understand as well as
easy to implement, using oﬀ-the-shelf libraries.

• ISM has a natural

initialization,

constructed
through a Taylor approximation of the problem’s
Lagrangian. Therefore, high quality results can be
obtained without random restarts in search of a bet-
ter initialization. We show that this initialization is
a contribution in its own right, as its use improves
performance of competitor algorithms.

• We provide theoretical guarantees on its ﬁxed point.
In particular, we establish conditions under which
the ﬁxed point of ISM satisﬁes both the 1st and 2nd
order necessary conditions for local optimality.

• We extensively evaluate the performance of ISM in
solving KDAC with synthetic and real data under
various clustering quality and cost measures. Our
results show an improvement in execution time by
up to a factor of roughly 70 and 105, compared to
SM and DG, respectively. At the same time, ISM
outperforms SM and DG in clustering quality mea-
sures along with signiﬁcantly lower computational
cost.

Related Work. There exist two general modes of dis-
covering alternative clusterings – simultaneously or it-
eratively. Simulataneous approaches ﬁnd the multiple
alternative clusterings at the same time [3, 8–13]. It-
erative approaches ﬁnd an alternative clustering given
existing clustering [2]. Since this work focuses on the
iterative paradigm, we elaborate on the related work
along these lines. Alternative clustering methods dif-
fer in how they measure novelty and cluster quality.
Gondek and Hofmann [1] ﬁnd an alternative cluster-
ing by conditional information (CI) bottleneck. Bae
and Bailey [14] perform agglomerative clustering with
cannot-link constraints imposed on the data points
that belong together in the existing clustering. Cui
et al. [5] ﬁnd an alternative clustering by projecting
the data to a subspace orthogonal to the existing clus-
tering. Qi and Davidson [15] search for novelty by
minimizing the Kullback-Leiber (KL) divergence be-
tween the original data and the transformed data sub-
ject to the constraint that the sum-squared-error be-
tween samples in the projected space with the existing
clusters is small. Dang and Bailey [16] ﬁnd quality
clusters by maximizing the mutual information (MI)
between the alternative clusters and the data while si-
multaneously ensuring novelty by minimizing the MI
between alternative and existing clusterings.

KDAC [6] discovers an alternative clustering by max-

imizing for cluster quality based on the spectral clus-
tering objective and at the same time maximizing for
novelty based on a non-linear dependence measure,
HSIC [17], on the projected subspace of the alterna-
tive clustering. KDAC’s ability to detect arbitrarily-
shaped clusters is due to its use of the Hilbert-Schmidt
Independence Criterion (HSIC) [17] as a cluster qual-
ity measure. HSIC is motivated by the objective func-
tion of spectral clustering. Moreover, since HSIC mod-
els non-linear dependence, it is also utilized by KDAC
to measure novelty. In contrast, e.g., the orthogonal
subspace projection approach in [5] is limited, as it
only captures linear dependencies. Other approaches,
such as [1, 16], can take non-linear dependencies into
account by utilizing information theoretic measures.
However, doing so requires estimating joint probabil-
ity distributions. The advantage of KDAC over such
approaches is that it utilizes HSIC for measuring nov-
elty and cluster quality, which can capture non-linear
dependencies through kernels, without having to ex-
plicitly learn the joint probability distributions; em-
pirically, it signiﬁcantly outperforms aforementioned
schemes in clustering quality [6].

2 Kernel Dimension Alternative

Clustering (KDAC)

In alternative clustering, a dataset is provided along
with existing clustering labels. Given this as input,
we seek a new clustering that is (a) distinct from the
existing clustering, and (b) has high quality with re-
spect to a clustering quality measure. An example
illustrating this is shown in Figure 1. This dataset
comprises 400 points in R4. Projected to the ﬁrst two
dimensions, the dataset contains two clusters of inter-
twining parabolas shown as Clustering A. Projected
to the last two dimensions, the dataset contains two
Gaussian clusters shown as Clustering B. Points clus-
tered together in one view can be in diﬀerent clusters
in the alternative view. In alternative clustering, given
(a) the dataset, and (b) one of the two possible clus-
terings (e.g., Clustering B), we wish to discover the
alternative clustering illustrated by the diﬀerent view.

Figure 1: Four-dimensional moon dataset. Projection into the ﬁrst
two dimensions reveals diﬀerent clusters than projection to the latter
two dimensions.

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

Formally, let X ∈ Rn×d be a dataset with n samples
and d features, along with an existing clustering Y ∈
Rn×k, where k is the number of clusters. If xi belongs
to cluster j, then Yi,j = 1; otherwise, Yi,j = 0. We
wish to discover an alternative clustering U ∈ Rn×k
on some lower dimensional subspace of dimension q (cid:28)
d. Let W ∈ Rd×q be a projection matrix such that
XW ∈ Rn×q.

We seek the optimal projection W and clustering U
that maximizes the statistical dependence between
XW with U , yielding a high clustering quality, while
minimizing the dependence between XW and Y , en-
suring the novelty of the new clustering. Denoting DM
as a Dependence Measure function, and using λ as a
weighing constant, this optimization can be written as:

Maximize: DM(XW, U ) − λ DM(XW, Y ),

s.t : W T W = I, U T U = I.

(1a)

(1b)

As in spectral clustering, the labels of the alterna-
tive clustering are retrieved by performing K-means
on matrix U , treating its rows as samples. There are
many potential choices for DM. The most well-known
measures are correlation and mutual information (MI).
While correlation performs well in many applications,
it lacks the ability to measure non-linear relationships.
Although there is clear relationship in Clustering A in
Figure 1, correlation would mistakenly yield a value
of nearly 0. As a dependence measure, MI is superior
in that it also measures non-linear relationships. How-
ever, due to the probabilistic nature of its formulation,
a joint distribution is required. Depending on the dis-
tribution, the computation of MI can be prohibitive.

For these reasons, the Hilbert Schmidt Independence
Criterion (HSIC) [17] has been proposed for KDAC [6].
Like MI, it captures non-linear relationships. Unlike
MI, HSIC does not require estimating a joint distri-
bution, and it relaxes the need to discretize continu-
ous variables. In addition, as shown by Niu et al. [6],
HSIC is mathematically equivalent to spectral cluster-
ing, further implying that a high HSIC between the
data and U yields high clustering quality. A visual
comparison of HSIC and correlation can be found in
Figure 5 of Appendix I in the supplement.

Using HSIC as a dependence measure, the objective of
KDAC becomes

Maximize: HSIC(XW, U ) − λ HSIC(XW, Y ), (2a)
subject to: W T W = I, U T U = I.

(2b)

(n−1)2 Tr(KX HKY H). Here,
where HSIC(X, Y ) ≡
the variables KX and KY are Gram matrices, and
the H matrix is a centering matrix where H =

1

Algorithm 1: KDAC Algorithm
Input : dataset X, original clustering Y
Output: alternative clustering U
Initialize W0 using Winit from (12)
Initialize U0 from original clustering
Initialize D0 from W and original clustering
while (U not converged) or (W not converged) do

Update D
Update W by solving Equation (4)
Update U by solving Equation (3)
Clustering Result ← Apply K-means to U

Algorithm 2: ISM Algorithm
Input : U ,D,X, Y
Output: W ∗
Initialize W0 to the previous value of W in the master loop of

KDAC.

while W not converged do
W ← eigmin(Φ(W ));

n 1n1T

I − 1
n with 1 the n-sized vector of all ones.
The elements of KX and KY are calculated by ker-
functions kX (xi, xj) and kY (yi, yj). The ker-
nel
nel
functions for Y and U used in KDAC are
KY = Y Y T and KU = U U T , and the kernel
function for XW is the Gaussian kXW (xi, xj) =
exp(−Tr[(xi − xj)T W W T (xi − xj)]/(2σ2)). Due to
the equivalence of HSIC and spectral clustering, the
practice of normalizing the kernel KXW is adopted
from spectral clustering by Niu et al. [6]. That is,
for KXW the unnormalized Gram matrix, the nor-
malized matrix is deﬁned as D−1/2KXW D−1/2 where
D = diag(1T
n KXW ) is a diagonal matrix whose ele-
ments are the column-sums of KXW .

KDAC Algorithm. The optimization problem (2) is
non-convex. The KDAC algorithm solves (2) using al-
ternate maximization between the variables U , W and
D, updating each while holding the other two ﬁxed.
After convergence, motivated by spectral clustering,
U is discretized via K-means to provide the alterna-
tive clustering. The algorithm proceeds in an iterative
fashion, summarized in Algorithm 1. In each iteration,
variables D, U , and W are updated as follows:

Updating D: While holding U and W constant, D is
computed as D = diag(1T
n KXW ). Matrix D is subse-
quently treated as a scaling constant throughout the
rest of the iteration.

Updating U: Holding W and D constant and solving
for U , (2) reduces to :

maxU :U T U =I Tr(U T QU ),

(3)

where Q = HD−1/2KXW D−1/2H. This is precisely
spectral clustering [18]: (3) can be solved by setting
U ’s columns to the k most dominant eigenvectors of
Q, which can be done in O(n3) time.

Updating W: While holding U and D constant to

Iterative Spectral Method for Alternative Clustering

solve for W , (2) reduces to:

The Lagrangian of (4) is:

Minimize: F (W ) = − (cid:80)
subject to: W T W = I

i,j γi,je−

Tr[W T Ai,j W ]
2σ2

(4a)

(4b)

are

the

where γi,j
elements of matrix γ =
D−1/2H(U U T − λY Y T )HD−1/2, and Ai,j = (xi −
xj)(xi − xj)T (see Appendix A in the supplement for
the derivation). This objective, along with a Stiefel
Manifold constraint, W T W = I, pose a challenging
optimization problem as neither is convex. Niu et
al. [6] propose solving (4) through an algorithm termed
Dimensional Growth (DG). This algorithm solves for
W by computing individual columns of W separately
through gradient descent (GD). Given a set of com-
puted columns, the next column is computed by GD
projected to a subspace orthogonal to the span of com-
puted set. Since DG is based on GD, the computa-
tional complexity is dominated by computing the gra-
dient of (4a). The latter is given by:

∇F (W ) = (cid:80)n
i

(cid:80)n
j

γi,j
σ2 e−

Tr[W T Ai,j W ]
2σ2

Ai,jW.

(5)

The complexity of DG is O(tDGn2d2q), where n, d are
the dataset size and dimension, respectively, q is the di-
mension of the subspace of the alternative clustering,
and tDG is the number of iterations of gradient de-
scent. The calculation of the gradient contributes the
term O(n2d2q). Although this computation is highly
parallelizable, the algorithm still suﬀers from slow con-
vergence rate. Therefore, tDG often dominates the
computation cost.

An alternative approach to optimize (4) is through
classic methods for performing optimization on the
Stiefel Manifold (SM) [7]. The computational com-
plexity of this algorithm is dominated by the compu-
tation of the gradient and a matrix inversion with tSM
iterations. This yields a complexity of O(tSM n2d2 +
tSM d3) for SM. Finally, as gradient methods applied to
a non-convex objective, both SM and DG require mul-
tiple executions from random initialization points to
ﬁnd improved local minima. This approach becomes
less eﬀective as the dimension d increases.

3 An Iterative Spectral Method

The computation of KDAC is dominated by the W
updates in Algorithm 1. Instead of using DG or SM
to solve the optimization problem for W in KDAC, we
propose an Iterative Spectral Method (ISM). Our al-
gorithm is motivated from the following observations.

L(W, Λ) = − (cid:80)

i,j γi,j exp

(cid:16)

− Tr(W T Ai,j W )
2σ2

(cid:17)

Setting ∇W L(W, Λ) = 0 gives us the equation:

−

Tr(Λ(W T W − I))

1
2

Φ(W )W = W Λ,

(6)

(7)

where

Φ(W ) = (cid:80)

γi,j

σ2 exp(− Tr[W T Ai,j W ]

2σ2

i,j

)Ai,j,

(8)

and Λ is a diagonal matrix. Recall that a feasible W ,
satisfying (4b), is orthonormal. (7) is an eigenequa-
tion; thus, a stationary point W of the Lagrangian (6)
comprises of q eigenvectors of Φ(W ) as columns. Mo-
tivated by this observation, ISM attempts to ﬁnd such
a W in the following iterative fashion. Let W0 be an
initial matrix. Given Wk at iteration k, the matrix
Wk+1 is computed as:

Wk+1 = eigmin(Φ(Wk)),

k = 0, 1, 2, . . . ,

where the operator eigmin(A) returns a matrix whose
columns are the q eigenvectors corresponding to the
smallest eigenvalues of A.

ISM is summarized in Alg. 2. Several important ob-
servations are in order. First, the algorithm ensures
that Wk, for k ≥ 1, is feasible, by construction: se-
lected eigenvectors are orthonormal and satisfy (4b).
Second, it is also easy to see that a ﬁxed point of the
algorithm will also be a stationary point of the La-
grangian (6) (see also Lemma 3). Though it is harder
to prove, selecting eigenvectors corresponding to the
smallest eigenvalues is key: we show that this is pre-
cisely the property that relates a ﬁxed point of the al-
gorithm to the local minimum conditions (see Thm. 1).
Finally, ISM has several computational advantages.
For tISM iterations, the calculation of Φ(W ), and
the ensuing eigendecomposition yields a complexity of
O(tISM (n2d2 + d3)). Since q (cid:28) d, various approxima-
tion methods [19] [20] [21] can be employed to ﬁnd the
few eigenvectors. For example, the Coordinate-wise
Power Method [21], approximates the most dominant
eigenvalue at O(d) time, reducing ISM’s complexity to
O(tISM n2d2). This improvement is further conﬁrmed
experimentally (see Figure 3). Lastly, tISM is mag-
nitudes smaller than both tDG and tSM .
In general
tISM < 10, while tSM > 50 and tDG > 200.

3.1 Convergence Guarantees

As mentioned above, the selection of the eigenvectors
corresponding to the smallest eigenvalues of Φ(Wk)
is crucial for the establishment of a stationary point.
Namely, we establish the following theorem:

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

Theorem 1. For large enough σ (satisfying Inequal-
ity (10)), a ﬁxed point W ∗ of Algorithm 2 satisﬁes
the necessary conditions of a local minimum of (4) if
Φ(W ∗) is full rank.

Proof. The main body of the proof is organized into a
series of lemmas proved in the supplement. Our ﬁrst
auxiliary lemma (from [22]), establishes conditions
necessary for a stationary point of the Lagrangian to
constitute local minimum.

Lemma 1. [Nocedal,Wright, Theorem 12.5 [22]] (2nd
Order Necessary Conditions) Consider the optimiza-
tion problem: minW :h(W )=0 f (W ), where f : Rd×q →
R and h : Rd×q → Rq×q are twice continuously diﬀer-
entiable. Let L be the Lagrangian of this optimization
problem. Then, a local minimum must satisfy the fol-
lowing conditions:

∇W L(W ∗, Λ∗) = 0,
∇ΛL(W ∗, Λ∗) = 0,

Tr(Z T ∇2

W W L(W ∗, Λ∗)Z) ≥ 0
for all Z (cid:54)= 0, with ∇h(W ∗)T Z = 0.

(9a)

(9b)

(9c)

Lemma 4 we establish the following condition on σ:

σ2[min

( ¯Λ∗

i) − max

(Λ∗

j )] ≥

i

j

(cid:88)

i,j

|γi,j|
σ2 e−

Tr((W ∗T

Ai,j W ∗)

2σ2

Tr(AT

i,jAij).

(10)

Here, Λ∗ is the set of q smallest eigenvalues of Φ(W ),
and ¯Λ∗ is the set of the remaining eigenvalues. The
left-hand side (LHS) of the equation further motivates
ISM’s choice of eigenvectors corresponding to the q
smallest eigenvalues. This selection guarantees that
the LHS of the inequality is positive. Therefore, given
a large enough σ, Inequality (10) and the 2nd order
condition is satisﬁed.

Furthermore, this equation provides a reasonable sug-
gestion for the value of q. Since we wish to maxi-
mize the term (mini( ¯Λ∗
j )) to satisfy the
inequality, the value q should be set where this gap is
maximized. More formally, we will deﬁned

i) − maxj(Λ∗

δgap = min

( ¯Λ∗

i) − max

(Λ∗

j ).

i

j

(11)

as the eigengap.

Armed with this result, we next characterize the prop-
erties of a ﬁxed point of Algorithm 2:

3.2 Spectral Initialization via Taylor

Approximation

Lemma 2. Let W ∗ be a ﬁxed point of Algorithm 2.
Then it satisﬁes: Φ(W ∗)W ∗ = W ∗Λ∗, where Λ∗ ∈
Rq×q is a diagonal matrix containing the q smallest
eigenvalues of Φ(W ∗) and W ∗T

W ∗ = I.

The proof can be found in Appendix B. Our next re-
sult, whose proof is in Appendix C, states that a ﬁxed
point satisﬁes the 1st order conditions of Lemma 1.

Lemma 3. If W ∗ is a ﬁxed point and Λ∗ is as de-
ﬁned in Lemma 2, then W ∗, Λ∗ satisfy the 1st order
conditions (9a)(9b) of Lemma 1.

Our last lemma, whose proof is in Appendix D, estab-
lishes that a ﬁxed point satisﬁes the 2nd order condi-
tions of Lemma 1, for large enough σ.

Lemma 4. If W ∗ is a ﬁxed point, Λ∗ is as deﬁned
in Lemma 2, and Φ(W ∗) is full rank, then given a
large enough σ (satisfying Inequality (10)), W ∗ and
Λ∗ satisfy the 2nd order condition (9c) of Lemma 1.

Thm. 1 therefore follows.

Thm. 1 is stated in terms of a large enough σ; we can
characterize this constraint precisely. In the proof of

ISM admits a natural initialization point, constructed
via a Taylor approximation of the objective. As we
show experimentally in Section 4, this initialization is
a contribution in its own right: it improves both clus-
tering quality and convergence time for ISM as well
as competitor algorithms. To obtain a good initializa-
tion, observe that by using the 2nd order Taylor ap-
proximation of the objective function (4a) at W = 0,
the Lagrangian can be approximated by

˜L(W, Λ) ≈ − (cid:80)

i,j γi,j

1 − Tr(W T Ai,j W )

2σ2

(cid:16)

(cid:17)

+

Tr(Λ(I − W T W )).

1
2

(cid:104)(cid:80)

Setting ∇W ˜L(W, Λ) = 0 reduces the problem into a
simple eigendecomposition, namely, the one deﬁned by
(cid:105)
the system
W = W Λ. Hence, the 2nd
order Taylor approximation of the original cost objec-
tive has a closed form global minimum that can be
used as an initialization point, namely:

γi,j
σ2 Ai,j

i,j

Winit = eigmin((cid:80)

i,j γi,jAi,j/σ2).

(12)

We use this spectral initialization (SI) in the ﬁrst mas-
ter iteration of KDAC. In subsequent master itera-
tions, W0 (the starting point of ISM) is set to be the
last value to which ISM converged to previously.

Iterative Spectral Method for Alternative Clustering

SG

NMI ↑

CQ ↑

Novelty ↓

Cost ↓

Time ↓

ISM SI
ISM RI
SM SI
SM RI
DG SI
DG RI 0.93±0.196

1
0.4±0.49
1
1±0.0
0

2
1.6±0.478
1.99
1.79±0.398
1.629
0.99±0.0

0

0

1

0.0±0.0 -1.59±0.398 1.47±1.521

-1.2

-1.791

-1.316

0.02
0.04±0.01
0.404

1.732
1.51±0.102

0.03±0.104 -0.99±0.0

0.0±0.0 -1.48±0.381

LG
ISM SI
ISM RI
SM SI
SM RI
DG SI
DG RI

1
0.8±0.4
1
0.4±0.49
1
0.1±0.32

1.9
1.9±0.011
1.9
1.9±0.031
1.9
0.61±0.17

0
0.0±0.0
0
0.6±0.49
0

0.239
509
0.18±0.038
605±193
0.413
509
11.21±11.01
1095±479
1595.174
509.975
0.9±0.32 101.5±30.8 1688±551

2
2.0±0.0
1.998
2.0±0.0
1.998
1.27±0.6

2
2.0±0
2
2.0±0
2
1.6±0.04

0
0.5±0.5
0
0.5±0.5
0
0.8±0.4

149
269.98±43
149
277±18
149.697

0.575
3.07±1.4
2.653
258±374
359.188

161.65±102 3212±1368

0
0.04±0.1
0
0.72±0.4
0
1.0±0.0

15.3
15.59±0.8
15.3
17.7±1
15.4
16.2±0.06

0.3
0.55±0.2
0.452
102±48.5
3836.3
3588±304

1
0.4±0.4
1

Moon
ISM SI
ISM RI
SM SI
SM RI 0.27±0.335
DG SI
DG RI

1
0.09±0.3

1
0.91±0.2
1
0.22±0.4
1
0±0.0

MoonN
ISM SI
ISM RI
SM SI
SM RI
DG SI
DG RI

Flower
ISM SI
ISM RI
SM SI
SM RI
DG SI
DG RI

-
-
-
-
-
-

0.41
0.41±0.0
0.41
0.3±0.2
0.41
0.35±0.02

0
0.0±0.0
0
0.01±0.012
0
0.2±0.4

20
19.51±0.0
20
0.7±0.5
20
20.6±1

0.04
0.1±0.01
0.153
0.8±0.6
27.251
37.0±5

0.61

0.57

Faces
ISM SI
ISM RI 0.56±0.002 0.61±0.0
SM SI
SM RI 0.57±0.002 0.61±0
DG SI
DG RI 0.458±0.0450.565±0.0180.024±0.018 54.28±3 166070±4342

59.6
59.6±0.118
59.559
59.7±0.1
59.8

1.5
1.51±0.146
7.96
109±35.2
100591.071

0.004
0.0±0.0
0.004
0.004±0
0.004

0.564

0.562

0.608

0.6

0.37

WebKb
ISM SI
ISM RI 0.29±0.07
SM SI
SM RI 0.33±0.06 0.12±0.005 0.008±0.004 -0.11±0.004 13511±13342
DG SI
DG RI

231
137.45±15.7
9.945

0.1±0.005 0.616±0.04 727694±41068

0.286
0.54±0.2
3.296

0.01±0.002 -0.52±0.2

0.048
0.23±0

1.066
1.06±0

199887.134

-3.159

-1.019

-0.273

0.034

0.034

0.048

0

Table 1: The Normalized Mutual Information, Clustering Quality,
and clustering novelty are abbreviated in the ﬁrst 3 columns as NMI,
CQ, and Novelty. The cost of the objective and run time is dis-
played in the last two columns. For each optimization technique,
spectral initialization (SI) and random initialization (RI) are sepa-
rately tested. With RI, 10 random initial points have been tested
with their mean and std displayed.

Figure 3: Growth in dimension vs time in log/log scale. A slope of
1 is linear growth.

1, the alternative clustering emerges. Large Gaussian
(LG) contains four Gaussian clusters with 1000 sam-
ples and four dimensions is shown in the top center

Figure 2: Figure includes all original and alternative displayable
clusterings. The face data is originally clustered by the identity of
the individual. Here the average image of the alternative clustering
is displayed. It is observable that the original and alternative clus-
ters are all visually obvious clusters and provide alternative views.

4 Experimental Results

We experimentally validate the performance of ISM in
terms of both speed and clustering quality. The source
code is publicly available at github1. Because [6] has
already performed extensive comparisons of KDAC
against other alternative clustering methods, this sec-
tion will concentrate on comparing ISM to compet-
ing models for optimizing KDAC: Dimensional Growth
(DG) [6] and gradient descent on the Stiefel Manifold
(SM) [7]. SM is a generic approach, while DG is the
approach originally proposed to solve KDAC [6].

In addition to introducing a new algorithm for optimiz-
ing KDAC, we also proposed an intelligent initializa-
tion scheme based on Taylor approximation in Section
3.2, we call Spectral Initialization (SI). We also inves-
tigate how SI eﬀects the performance of the various
algorithms compared to the standard random initial-
ization (RI).

Datasets. We perform experiments on four synthetic
and three real datasets. The synthetic data are dis-
played in Figure 2. Small Gaussian (SG) contains four
Gaussian clusters with 40 samples and two features
shown at the top left of the ﬁgure. When this data
is projected down to feature 2, the original cluster-
ing is created. Rotating this projection onto feature

1https://github.com/neu-spiral/ISM

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

location. The Gaussian clusters are rotated by 45 de-
grees to reside in 3D, and the fourth dimension is gen-
erated from a uniform noise distribution. This syn-
thetic data is designed to test the response of KDAC
within a noisy environment. We generate two addi-
tional synthetic datasets shown in top right: Moon
and Moon+Noise (MoonN). Both datasets have the
ﬁrst two dimensions as two parabolas and the sec-
ond two dimensions as Gaussian clusters. The MoonN
dataset further includes three noisy dimensions gener-
ated from a uniform distribution with 1000 samples.
Since the Gaussian clusters have a compact structure
and the parabolas have a non-linear structure, these
datasets demonstrate KDAC’s ability to handle mixed
clustering structures in a uniformly noisy environment.
Due to the novelty of alternative clustering, there has
been very few public repository data that have at least
two alternative labels. The two traditional benchmark
datasets are CMU’s WebKB dataset [23] and face im-
ages from the UCI KDD repository [24]. CMU’s We-
bKB dataset consists of 1041 html pages from 4 uni-
versities. One labelling is based on universities, and
an alternative labelling based on topic (course, fac-
ulty, project, student). After preprocessing by remov-
ing rare and stop words, we are left with 500 words.
The face dataset consists of 640 images from 20 people
in four diﬀerent poses. Each image has 32x30 pixels.
Images are vectorized and PCA is then used to further
condense the dataset by keeping 85% of the variance,
resulting in a dataset of 624 samples and 20 features.
We set the existing clustering based on identity, and
seek an alternative clustering based on pose. The last
real dataset is the Flower image by Alain Nicolas [25],
a 350x256 pixel image. The RGB values of each pixel
is taken as a single sample, with repeated samples re-
moved. This results in a dataset of 256 samples and 3
features. Although this dataset does not have labels,
the quality of the alternative clustering can be visually
observed. With the exception of the high dimensional
WebKB data, the remaining datasets are visualized in
Figure 1 (b).

Hyperparameter Tuning. ISM has 3 hyperparam-
eters that require tuning, q, λ, and σ. The hyperpa-
rameter q with a potential range of (0,d) is initially set
to k, the number of clusters. A grid search was then
conducted for λ ∈ (0, 10] and σ ∈ (0, 10] to ﬁnd the
highest CQ (λ, σ) pair that satisﬁes inequality (10)
at an increment of 0.01.
In the event that Inequal-
ity (10) cannot be satisﬁed, the highest CQ closest to
satisfying (10) is used. At this point, since Φ(W ∗) is
computed, the q value that maximizes the eigengap
as deﬁned in Eq(11) can be used if (10) is not yet
satisﬁed. Note that competing models do not have a
natural way of selecting hyperparameters. Since all
competing models optimize the same objective, we re-

port all results using the same hyperparameters. To
ensure reproducible experiments, the hyperparameters
utilized in each experiment is provided in Appendix L.

Evaluation Method. To exhaustively evaluate ISM
on KDAC, both external and internal measures have
been recorded in Table 1. More speciﬁcally, col-
umn 1 and 3 (NMI and Novelty) are external mea-
sures because they compare alternative cluster assign-
ments against an externally known ground truth. To
make the comparison, Normalized Mutual Information
(NMI) as suggested by [26] was used. The NMI is a
measure between 0 to 1 with 0 denoting no relationship
and 1 as maximum relationship. Column 1 (NMI) is
calculated by computing the NMI between the ground
truth and the alternative cluster assignments. Column
3 (Novelty) is calculated by computing the NMI be-
tween the alternative cluster assignments against the
original label. Ideally, we wish for the NMI of the al-
ternative cluster assignments against the ground truth
to be 1, and 0 against the original clustering.
If we
let U and L be two clustering assignments, NMI can
, where
be calculated with N M I(L, U ) =

I(L,U )

√

H(L)H(U )

I(L, U ) is the mutual information between L and U ,
and H(L) and H(U ) are the entropies of L and U re-
spectively. Lastly, since no ground truth exists for the
Flower dataset, the NMI ﬁeld is not applicable and
indicated with a dash (-).

Columns 2, 4 and 5 (CQ, Cost, Time) are internal mea-
sures used to evaluate ISM. They are internal measures
since they are computed solely from the data and the
algorithm with no external knowledge applied during
the comparison. The clustering quality (CQ) is com-
puted with HSIC(XW, U ) with U as the clustering
solution. This is equivalent to the spectral cluster-
ing objective with high values denoting high clustering
quality. The cost quality records the objective function
of KDAC in Eq. (2). The last column, Time, measures
the execution time of the entire KDAC algorithm in
seconds on an Intel Xeon E7 processor.

In Table 1, we report the performance of the various
methods: our ISM, SM, and DG, paired with two ini-
tialization schemes: our spectral initialization (SI) and
random initialization (RI). For random initialization,
we repeat these 10 times and report the mean and
standard deviation for each measure. The optimal di-
rection of each measure is denoted by the ↑↓, with ↑
denoting a preference towards larger values and ↓ oth-
erwise. For each ﬁeld, the optimal result is printed in
bold font.

Performance Comparison Results. Table 1 illus-
trates that ISM with SI outperforms its competitors in
both internal and external quality measure for the ﬁrst
3 columns. Since it is possible for the objective cost

Iterative Spectral Method for Alternative Clustering

to achieve a low cost with a trivial solution based on
diﬀerent (σ, λ) pairs, the objective should be low, but
it is not always indicative of better clustering quality.
This is demonstrated in the case of WebKb dataset,
where SM+SI achieved a lower cost with a faster con-
vergence. However, upon inspecting the clustering al-
location in this case, we observed that it was a trivial
classiﬁcation, with almost all points in the same clus-
ter. Since the demand for a faster KDAC was the
original motivator, special attention should be paid to
the Time column. The execution time of ISM signiﬁ-
cantly outperforms the original approach (DG) as well
as the standard approach (SM). This speed improve-
ment is especially true in the Faces dataset where ISM
improves the speed by 5 folds.

The Eﬀect of Spectral Initialization. By com-
paring SI against RI, we can isolate the eﬀect of our
proposed spectral initialization (SI). Comparing the
rows, the spectral initialization improved both time
and clustering quality for all methods. From this ob-
servation, we conclude that SI contributes to clustering
quality by starting each algorithm at a desirable ini-
tialization. The convergence improvement came from
placing each algorithm closer to the local minimum.
Comparing all methods when using SI, we observed
that ISM optimization still outperformed other algo-
rithms in terms of time. From this, we conclude that
the proposed SI has a greater impact on the cluster-
ing quality while the ISM optimization technique con-
tribute towards faster convergence.

Scalability. Note that KDAC consists of optimiz-
ing U and W from Eq. (2). Since the computational
bottleneck resided in the optimization of W , ISM was
designed to speed up this portion. The scalability
analysis, therefore, will concentrate on only the op-
timization of W . ISM has a computational complex-
ity of O(tISM (n2d2 + d3) and the complexity can be
divided into the calculation of the derivative and the
eigendecomposition of Φ(W ). Although the deriva-
tive contributes to O(n2d2), it is a highly paralleliz-
able operation that can be rendered trivial through
the usage of GPUs. Therefore, ISM’s true bottleneck
is not the number of samples (n), but the number of
dimensions due to the O(d3) operation of eigendecom-
position. As the dimensionality of the data increase,
the complexity grows at a cubic rate. Yet, while this
growth of dimensionality may exacerbate algorithms
such as SM, the negative inﬂuence on ISM is limited.
This is because the O(d3) term from SM came from
a matrix inversion while ISM uses a spectral method.
Since only very few eigenvalues are required, there ex-
ists many approximation algorithms to speed up the
eigendecomposition [19] [20] [21]. To demonstrate this
point, Coordinate-wise Power Method (CPM) [21] was

implemented and compared to the eigendecomposition
operation from Numpy. In this experiment, noisy di-
mensions of uniform distributions are added to the
synthetic dataset of Large Gauss (LG) such that the
total dimension increase in multiples of 2. As the di-
mension of the data grows exponentially, the time of
execution is recorded in Figure 3 in log scale. Given a
log/log scale, a slope of 1 is linear while a slope of 2
is quadratic. From studying the slope of each opera-
tion, the eigendecomposition from Numpy grows at a
rate between linear and quadratic. However, by utiliz-
ing CPM, the growth becomes roughly linear. There-
fore, the reﬁnement of utilizing a spectral optimization
method is a key reason for speed improvement.

5 Conclusions

We have proposed an iterative spectral optimization
technique for solving a non-convex optimization prob-
lem while being constrained on a Stiefel manifold.
This new technique demonstrates speed improvement
for any algorithm that could be formulated into Eq
(4). The ISM algorithm is easy to implement with
existing software libraries. Due to the usage of the
spectral method, approximation techniques from ex-
isting research could be deployed to further speed up
the eigendecomposition. Accompanied with the algo-
rithm are the theoretical guarantees that satisfy the
ﬁrst and second order necessary conditions for a local
minimum. Besides these guarantees, we also proposed
a natural initialization point based on Taylor approx-
imation of the original cost function. Experiments on
synthetic and real data conﬁrmed that our proposed
spectral initialization improved the performance of all
the optimization algorithms, ISM, SM and DG. Simul-
taneously, the experiments demonstrate that our pro-
posed ISM algorithm had the best convergence rate
compared to competing models with up to 5 folds of
speed improvement. Although we focus this paper on
an important application of ISM, alternative cluster-
ing, the optimization algorithm proposed and guaran-
tees can be extended to other optimization problems
involving Gaussian-kernel like objectives constrained
over the Stiefel manifold, which is a common formula-
tion for dimensionality reduction with a kernel-based
objective. Understanding whether ISM can be applied
to such objectives and be leveraged to solve broader
classes of problems is a natural future direction for this
work.

Acknowledgements

We would like to acknowledge support for this project
from the NSF grant IIS-1546428. We would also like
to thank Yale Chang for his insightful discussions.

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

[14] Eric Bae and James Bailey. Coala: A novel approach
for the extraction of an alternate clustering of high
quality and high dissimilarity. In Data Mining, 2006.
ICDM’06. Sixth International Conference on, pages
53–62. IEEE, 2006.

[15] ZiJie Qi and Ian Davidson. A principled and ﬂexi-
ble framework for ﬁnding alternative clusterings. In
Proceedings of the 15th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 717–726. ACM, 2009.

[16] Xuan-Hong Dang and James Bailey. A hierarchical
information theoretic technique for the discovery of
non linear alternative clusterings.
In Proceedings of
the 16th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 573–582.
ACM, 2010.

[17] Arthur Gretton, Olivier Bousquet, Alex Smola, and
Bernhard Sch¨olkopf. Measuring statistical dependence
In International con-
with hilbert-schmidt norms.
ference on algorithmic learning theory, pages 63–77.
Springer, 2005.

[18] Ulrike Von Luxburg. A tutorial on spectral clustering.
Statistics and computing, 17(4):395–416, 2007.

[19] Max Vladymyrov and Miguel Carreira-Perpinan. The
variational nystrom method for large-scale spectral
problems.
In International Conference on Machine
Learning, pages 211–220, 2016.

[20] Peter Richt´arik. Generalized power method for sparse

principal component analysis.

[21] Qi Lei, Kai Zhong, and Inderjit S Dhillon. Coordinate-
wise power method. In Advances in Neural Informa-
tion Processing Systems, pages 2064–2072, 2016.

[22] Stephen Wright and Jorge Nocedal. Numerical opti-

mization. Springer Science, 35:67–68, 1999.

[23] CMU CMU. universities webkb data, 1997. 4.

[24] Stephen D Bay, Dennis Kibler, Michael J Pazzani, and
Padhraic Smyth. The uci kdd archive of large data sets
for data mining research and experimentation. ACM
SIGKDD Explorations Newsletter, 2(2):81–85, 2000.

[25] Particles

of

tessellations.
tessellations-nicolas.com/.
04-25.

http://en.
2017-

Accessed:

[26] A Strehl and J Chosh. Knowledge reuse framework
for combining multiple partitions. Journal of Machine
learning Research, 33(3):583–617.

References

[1] David Gondek and Thomas Hofmann. Non-redundant
data clustering. Knowledge and Information Systems,
12(1):1–24, 2007.

[2] Ying Cui, Xiaoli Z Fern, and Jennifer G Dy. Learn-
ing multiple nonredundant clusterings. ACM Trans-
actions on Knowledge Discovery from Data (TKDD),
4(3):15, 2010.

[3] Xuan Hong Dang and James Bailey. Generation of
alternative clusterings using the cami approach.
In
Proceedings of the 2010 SIAM International Confer-
ence on Data Mining, pages 118–129. SIAM, 2010.

[4] Ian Davidson and Zijie Qi. Finding alternative clus-
terings using constraints.
In Data Mining, 2008.
ICDM’08. Eighth IEEE International Conference on,
pages 773–778. IEEE, 2008.

[5] Ying Cui, Xiaoli Z Fern, and Jennifer G Dy. Non-
redundant multi-view clustering via orthogonaliza-
tion.
In Data Mining, 2007. ICDM 2007. Seventh
IEEE International Conference on, pages 133–142.
IEEE, 2007.

[6] Donglin Niu, Jennifer G Dy, and Michael I Jordan.
Iterative discovery of multiple alternativeclustering
views. IEEE transactions on pattern analysis and ma-
chine intelligence, 36(7):1340–1353, 2014.

[7] Zaiwen Wen and Wotao Yin. A feasible method for
optimization with orthogonality constraints. Mathe-
matical Programming, 142(1-2):397–434, 2013.

[8] Rich Caruana, Mohamed Elhawary, Nam Nguyen, and
Casey Smith. Meta clustering. In Data Mining, 2006.
ICDM’06. Sixth International Conference on, pages
107–118. IEEE, 2006.

[9] Anil K Jain, M Narasimha Murty, and Patrick J
Flynn. Data clustering: a review. ACM computing
surveys (CSUR), 31(3):264–323, 1999.

[10] Sajib Dasgupta and Vincent Ng. Mining clustering
dimensions. In Proceedings of the 27th International
Conference on Machine Learning (ICML-10), pages
263–270, 2010.

[11] Vikash K Mansinghka, Eric Jonas, Cap Petschulat,
Beau Cronin, Patrick Shafto, and Joshua B Tenen-
baum. Cross-categorization: A method for discover-
ing multiple overlapping clusterings. In Nonparamet-
ric Bayes Workshop at NIPS, 2009.

[12] Donglin Niu, Jennifer Dy, and Zoubin Ghahramani. A
nonparametric bayesian model for multiple clustering
with overlapping feature views.
In Artiﬁcial Intelli-
gence and Statistics, pages 814–822, 2012.

[13] Leonard Poon, Nevin L Zhang, Tao Chen, and
Yi Wang. Variable selection in model-based clustering:
To do or to facilitate. In Proceedings of the 27th In-
ternational Conference on Machine Learning (ICML-
10), pages 887–894, 2010.

Iterative Spectral Method for Alternative Clustering

Appendix A Derivation for Equation 4

Given the objective function,

max HSIC(XW, U ) − λ HSIC(XW, Y )
U, W
s.t

W T W = I, U T U = I.

Using the HSIC measure deﬁned, the objective function can be rewritten as

HSIC(XW, U ) − λ HSIC(XW, Y ) = Tr(HU U T HD

−1

2 KXW D

−1

−1

2 H(U U T − λY Y T )HD

2 ) − λ Tr(HY Y T HD
2 KXW )

−1

−1

2 KXW D

−1
2 )

= Tr(D
= Tr(γKXW )
= (cid:80)

i,j γi,jKXi,j .

where γ is a symmetric matrix and γ = H(U U T − λY Y T )H. By substituting the Gaussian kernel for KXi,j , the
objective function becomes

min
W

(cid:88)

−

γi,je−

Tr[W T Ai,j W ]
2σ2

i,j

s.t W T W = I.

Appendix B Proof for Lemma 2

Proof. Algorithm 2 sets the smallest q eigenvectors of Φ(Wk) as Wk+1. Since a ﬁxed point W ∗ is reached when
Wk = Wk+1, therefore W ∗ consists of the smallest eigenvectors of Φ(W ∗) and Λ∗ corresponds with a diagonal
matrix of eigenvavlues. Since the eigenvectors of Φ(W ∗) are orthonormal , W ∗T

W ∗ = I is also satisﬁed.

Appendix C Proof for Lemma 3

Proof. Using Equation (4) as the objective function, the corresponding Lagrangian and its gradient is written as

and

L(W, Λ) = −

γi,je−

Tr(W T Ai,j W )
2σ2

−

Tr(Λ(W T W − I)),

1
2

(cid:88)

i,j

∇W L(W, Λ) =

(cid:88)

i,j

γi,j
σ2 e−

Tr(W T Ai,j W )
2σ2

Ai,jW − W Λ.

By setting the gradient of the Lagrangian to zero, and using the deﬁnition of Φ(W ) from Equation (8), Equation
(14) can be written as

Φ(W )W = W Λ.

The gradient with respect to Λ is

Setting this gradient of the Lagrangian also to zero, condition (9b) is equivalent to

∇ΛL(W, Λ) = W T W − I.

W T W = I.

By Lemma 2, a ﬁxed point W ∗ and its corresponding Λ∗ satisfy (15) and (17), and the lemma follows.

(13)

(14)

(15)

(16)

(17)

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

Appendix D Proof for Lemma 4

The proof for Lemma 4 relies on the following three sublemmas. The ﬁrst two sublemmas demonstrate how the
2nd order conditions can be rewritten into a simpler form. With the simpler form, the third lemma demonstrates
how the 2nd order conditions of a local minimum are satisﬁed given a large enough σ.

Lemma 4.1. Let the directional derivative in the direction of Z be deﬁned as

Df (W )[Z] :=

lim
t → 0

f (W + tZ) − f (W )
t

.

Then the 2nd order condition of Lemma 4 can be written as

Tr(Z T D∇L[Z]) =

Tr((W ∗T

Ai,j W ∗ )

2σ2

(cid:20)
Tr(Z T Ai,jZ) −




(cid:88)



i,j

γi,j
σ2 e−

1
σ2 Tr(Z T Ai,jW ∗)2




(cid:21)



− Tr(Z T ZΛ∗),

(19)

for all Z such that

Proof. Observe ﬁrst that

Z T W ∗ + W ∗T

Z = 0.

∇2

W ∗W ∗ L(W ∗, Λ∗)Z = D∇L[Z],

where the directional derivative of the gradient D∇L[Z] is given by

D∇L[Z] =

lim
t → 0

∂
∂t

(cid:88)

i,j

γi,j
σ2 e−

Tr((W ∗+tZ)T Ai,j (W ∗+tZ))
2σ2

Ai,j(W ∗ + tZ) − (W ∗ + tZ)Λ.

D∇L[Z] = T1 + T2 − T3,

Tr((W ∗+tZ)T Ai,j (W ∗+tZ))
2σ2

Ai,jW ∗

Tr((W ∗T

Ai,j W ∗+tZT Ai,j W ∗+tW ∗T

Ai,j Z+t2ZT Ai,j Z)

2σ2

Ai,jW ∗

Tr((W ∗T

Ai,j W ∗ )

2σ2

Tr(Z T Ai,jW ∗ + W ∗T

Ai,jZ)Ai,jW ∗

This can be written as

where

T1 =

lim
t → 0

∂
∂t

∂
∂t

(cid:88)

i,j

(cid:88)

i,j

γi,j
σ2 e−

γi,j
σ2 e−

=

lim
t → 0

= −

= −

(cid:88)

i,j

(cid:88)

i,j

γi,j
2σ4 e−

γi,j
σ4 e−
∂
∂t

T2 =

lim
t → 0

=

(cid:88)

i,j

γi,j
σ2 e−
∂
∂t

lim
t → 0

T3 =

= ZΛ.

Tr((W ∗T

Ai,j W ∗ )

2σ2

Tr(Z T Ai,jW ∗)Ai,jW ∗

Tr((W ∗+tZ)T Ai,j (W ∗+tZ))
2σ2

Ai,jZ

(cid:88)

γi,j
σ2 te−

i,j
Tr(W ∗T

Ai,j W ∗ )

2σ2

Ai,jZ,

(W ∗ + tZ)Λ

Hence, putting all three terms together yields

as Ai,j = AT

i,j,

(25)

(18)

(20)

(21)

(22)

(23)

(24)

(26)

(27)

(28)

(29)

Iterative Spectral Method for Alternative Clustering

D∇L[Z] =




(cid:88)



i,j

γi,j
σ2 e−

Tr((W ∗T

Ai,j W ∗ )

2σ2

(cid:20)
Ai,jZ −

1
σ2 Tr(Z T Ai,jW ∗)Ai,jW ∗

− ZΛ.

(30)




(cid:21)



Hence,

Tr(Z T ∇2

W ∗W ∗ L(W ∗, Λ∗)Z) = Tr(Z T D∇L[Z]),

(31)




(cid:88)



i,j

=

γi,j
σ2 e−

Tr((W ∗T

Ai,j W ∗ )

2σ2

(cid:20)
Tr(Z T Ai,jZ) −

1
σ2 Tr(Z T Ai,jW ∗)2




(cid:21)



− Tr(Z T ZΛW ).

(32)

Next, let Z be such that Z (cid:54)= 0 and ∇h(W ∗)T Z = 0, where

h(W ∗) = W ∗T

W ∗ − I.

Therefore, the constraint condition can be written on Z in (9c) can be written as

∇h(W ∗)T Z =

lim
t → 0

∂
∂t

t

(W ∗ + tZ)T (W ∗ + tZ) − W ∗T

W ∗

= Z T W ∗ + W ∗T

Z = 0.

Using Equations (32) and (34) lemma 4.1 follows.

Recall from Lemma 2 that W ∗ consists of the q eigenvectors of Φ(W ∗) with the smallest eigenvalues. We deﬁne
¯W ∗ ∈ Rd×d−q as all other eigenvectors of Φ(W ∗). Because Z has the same dimension as W ∗, each column of
Z resides in the space of Rd. Since the eigenvectors of Φ(W ∗) span Rd, each column of Z can be represented as
a linear combination of the eigenvectors of Φ(W ∗). In other words, each column zi can therefore be written as
zi = W ∗P (i)
¯W ∗ ∈ Rd−q×1 represents the coordinates for the two sets of
eigenvectors. Using the same notation, we also deﬁne Λ∗ ∈ Rq×q as the eigenvalues corresponding to W ∗ and
¯Λ∗ ∈ Rd−q×d−q as the eigenvalues corresponding to ¯W ∗. The entire matrix Z can therefore be represented as

W ∗ ∈ Rq×1 and P (i)

¯W ∗ , where P (i)

W + ¯W ∗P (i)

Z = ¯W ∗P ¯W ∗ + W ∗PW ∗ .

Furthermore, it can be easily shown that PW ∗ is a skew symmetric matrix, or −PW ∗ = P T
Equation (20) into (35), the constraint can be rewritten as

W ∗ . By setting Z from

[P T

¯W ∗

¯W ∗T + P ∗T

W W ∗T

]W ∗ + W ∗T

[ ¯W ∗P ¯W ∗ + W ∗PW ∗ ] = 0.

Simplifying the equation yields the relationship

P ∗T
W + PW ∗ = 0.

Using these deﬁnitions, we deﬁne the following sublemma.
Lemma 4.2. Given a ﬁxed point W ∗ and a Z satisfying condition (20), the condition Tr(Z T D∇L[Z]) ≥ 0 is
equivalent to

Tr(P T

¯W ∗

¯Λ∗P ¯W ∗ ) − Tr(P ¯W ∗ Λ∗P T

¯W ∗ ) ≥ C2,

(33)

(34)

(35)

(36)

(37)

(38)

PW ∗ , P ¯W ∗ are given by Equation (35), and Λ∗, ¯Λ∗ are the diagonal matrices containing the bottom and top
eigenvalues of Φ(W ∗) respectively.

(39)

(40)

where

where

Proof. By condition (19),

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

C2 =

(cid:88)

i,j

γi,j
σ4 e−

Tr((W ∗T

Ai,j W ∗ )

2σ2

Tr(Z T Ai,jW ∗)2,

Tr(Z T D∇L[Z]) = C1 − C2 + C3,

C1 = Tr


Z T (cid:88)

i,j

γi,j
σ2 e−

Tr((W ∗T

Ai,j W ∗ )

2σ2



Ai,jZ

 ,

Tr((W ∗T

Ai,j W ∗ )

2σ2

Tr(Z T Ai,jW ∗)2,

C2 =

(cid:88)

i,j

γi,j
σ4 e−

C3 = − Tr(Z T ZΛ∗).

C1 can be written as

C1 = Tr




Z T (cid:88)

Tr((W ∗T

i,j

2σ2



Ai,j W ∗ )

Ai,jZ

γi,j
σ2 e−
= Tr(Z T Φ(W ∗)[ ¯W ∗P ¯W ∗ + W ∗PW ∗ ])
= Tr(Z T [Φ(W ∗) ¯W ∗P ¯W ∗ + Φ(W ∗)W ∗PW ∗ ])
= Tr(Z T [ ¯W ∗ ¯ΛP ¯W ∗ + W ∗ΛPW ∗ ])
¯W ∗T + P ∗T
W W ∗T
= Tr([P T
¯ΛP ¯W ∗ ) + Tr(P T

W ∗ ΛPW )

= Tr(P T

¯W ∗

¯W ∗

][ ¯W ∗ ¯ΛP ¯W ∗ + W ∗ΛPW ∗ ])

Similarly

C3 = − Tr(Z T ZΛ)
= − Tr([P T
= − Tr([P T
= − Tr(P T

¯W ∗T + P T
¯W ∗
¯W ∗ P ¯W ∗ + P T
¯W ∗ P ¯W ∗ Λ) − Tr(P T

W ∗ W ∗T
W ∗ PW ∗ ]Λ)

W ∗ PW ∗ Λ).

][ ¯W ∗P ¯W ∗ + W ∗PW ∗ ]Λ)

By deﬁnition of eigenvalues.

Given W ∗T

Substitute for Z
W ∗ = I, ¯W ∗T W ∗ = 0.

Because PW ∗ is a square skew symmetric matrix, the diagonal elements of PW ∗ P T
of PW ∗ P T

W ∗ . From this observation, we conclude that Tr(PW ∗ P T

W ∗ Λ) = Tr(P T

W ∗ PW ∗ Λ). Hence,

W ∗ is the same as the diagonal

C3 = − Tr(P ¯W ∗ ΛP T

¯W ∗ ) − Tr(P T

W ∗ ΛPW ∗ ).

Putting all 3 parts together yields

Tr(Z T D∇L[Z]) = Tr(P T
= Tr(P T

¯W ∗

¯W ∗

¯ΛP ¯W ∗ ) + Tr(P T
¯ΛP ¯W ∗ ) − Tr(P ¯W ∗ ΛP T

¯W ∗ ) − C2.

W ∗ ΛPW ∗ ) − C2 − Tr(P ¯W ∗ ΛP T

¯W ∗ ) − Tr(P T

W ∗ ΛPW ∗ )

(41)

Iterative Spectral Method for Alternative Clustering

The 2nd order condition (9c) is, therefore, satisﬁed, when

Tr(P T

¯W ∗

¯ΛP ¯W ∗ ) − Tr(P ¯W ∗ ΛP T

¯W ∗ ) ≥ C2.

Lemma 4.3. Given W ∗, ¯W ∗, ¯Λ∗, and Λ∗ as deﬁned in Equation (35), if the corresponding smallest eigenvalue
of ¯Λ∗ is larger than the largest eigenvalue of Λ∗, then given a large enough σ the condition (9c) of Lemma 1 is
satisﬁed.

Proof. To proof sublemma (4.3), we provide bounds on each of the terms in (42). Starting with C2 deﬁned at
(39). It has a trace term, (Tr(Z T AijW ∗))2 that can be rewritten as

(Tr(AijW ∗Z T ))2 = (Tr(AijW ∗P T

W ∗ W ∗T

+ AijW ∗P T
¯W ∗

¯W ∗T ))2.

Since Aij is symmetric and W ∗P T

W ∗ W ∗T

is skew-symmetric, then Tr(AijW ∗P T

W ∗ W ∗T

) = 0. Hence

(Tr(Z T AijW ∗))2 = (Tr(AijW ∗Z T ))2 = (Tr(AijW ∗P T
¯W ∗

¯W ∗T ))2

≤ Tr(AT

i,jAij) Tr(P T

¯W ∗ P ¯W ∗ )

where the last inequality follows from Cauchy-Schwartz inequality and that fact that W ∗T
I. Thus, C2 in (41) is bounded by

W ∗ = I and ¯W ∗T ¯W ∗ =

Similarly, the remaining terms in (40) can be bounded by

C2 ≤

(cid:88)

i,j

|γi,j|
σ4 e−

Tr((W ∗T

Ai,j W ∗ )

2σ2

Tr(AT

i,jAij) Tr(P T

¯W ∗ P ¯W ∗ )

C1 = Tr(P T
¯W ∗

¯Λ∗P ¯W ∗ ) ≥ min

( ¯Λ∗

i) Tr(P ¯W ∗ P T

¯W ∗ )

C3 = − Tr(P ¯W ∗ Λ∗P T

¯W ∗ ) ≥ − max

(Λ∗

i ) Tr(P T

¯W ∗ P ¯W ∗ ).

i

i

Using the bounds for each term, the Equation (42) can be rewritten as

[min
i

( ¯Λ∗

i) − max

(Λ∗

j )] Tr(P T

¯W ∗ P ¯W ∗ ) ≥

j

Tr((W ∗T

Ai,j W ∗ )

2σ2

Tr(AT

i,jAij) Tr(P T

¯W ∗ P ¯W ∗ )

[min
i

( ¯Λ∗

i) − max

(Λ∗

j )] ≥

j

Tr((W ∗T

Ai,j W ∗ )

2σ2

Tr(AT

i,jAij)

(cid:88)

i,j

(cid:88)

i,j

|γi,j|
σ4 e−

|γi,j|
σ4 e−

It should be noted that Λ∗ is a function of 1
the inequality by σ∗ to yield

σ2 . This relationship could be removed by multiplying both sides of

σ2[min

( ¯Λ∗

i) − max

(Λ∗

j )] ≥

i

j

(cid:88)

i,j

|γi,j|
σ2 e−

Tr((W ∗T

Ai,j W ∗ )

2σ2

Tr(AT

i,jAij).

Since σ2 is always a positive value, as long as all the eigenvalues from ¯Λ∗ is larger than all the eigenvalues from
Λ∗, the left hand side of the equation will always be greater than 0. As σ → ∞, the right hand side approaches
0, and the condition (9c) of Lemma 1 is satisﬁed.

As a side note, the eigen gap between min( ¯Λ∗) and max(Λ∗) controls the range of potential σ values i.e. the
larger the eigen gap the easier for σ to satisfy (51). Therefore, the ideal cutoﬀ point should have a large eigen
gap.

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

Appendix E Convergence Plot from Experiments

Figure 4 summarizes the convergence activity of various experiments. For each experiment, the top ﬁgure
provides the magnitude of the objective function. It can be seen that the values converges towards a ﬁxed point.
The middle plot provide updates of the gradient of the Lagrangian. It can be seen that the gradient converges
towards 0. The bottom plot shows the changes in W during each iteration. The change in W converge towards
0.

Figure 4: Convergence Results from the Experiments.

Iterative Spectral Method for Alternative Clustering

Appendix F Proof of Convergence

The convergence property of ISM has been analyzed and yields the following theorem.

Theorem 2. A sequence {Wk}k∈N generated by Algorithm 2 contains a convergent subsequence.

Proof. According to Bolzano-Weierstrass theorem, if we can show that the sequences generated from the 1st
order relaxation is bounded, it has a convergent subsequence. If we study the Equation Φ(W ) more closely, the
key driver of the sequence of Wk is the matrix Φ, therefore, if we can show that if this matrix is bounded, the
sequence itself is also bounded. We look inside the construction of the matrix itself.

Φn+1 =





(cid:88)

i,j

γi,j
σ2 e−

Tr(W T

n Ai,j Wn )
2σ2



Ai,j



From this equation, start with the matrix Ai,j = (xi − xj)(xi − xj)T . Since xi, xj are data points that are
always centered and scaled to a variance of 1, the size of this matrix is always constrained.
It also implies
that Ai,j is a PSD matrix. From this, the exponential term is always limited between the value of 0 and
1. The value of σ is a constant given from the initialization stage. Lastly, we have the γi,j term. Since
γ = D−1/2H(U U T − λY Y T )HD−1/2. The degree matrix came from the exponential kernel. Since the kernels
are bounded, D is also bounded. The centering matrix H and the previous clustering result Y can be considered
as bounded constants. Since the spectral embedding U is a orthonormal matrix, it is always bounded. From this,
given that the components of Φ is bounded, the inﬁnity norm of the Φ is always bounded. The eigenvalue matrix
of Λ is therefore also bounded. Using the Bolzano-Weierstrass Theorem, the sequence contains a convergent sub-
sequence. Given that Φ is a continuous function of W , by continuity, W also has a convergent sub-sequence.

Appendix G Proof for the initialization

Although the proof was originally shown through the usage of the 2nd order Taylor Approximation. A simpler
approach was later discovered to arrive to the same formulation faster. We ﬁrst note that Taylor’s Expansion
around 0 of an exponential is

ex = 1 + x +

+ ....

x2
2!

Given the objective Lagrangian in eq (6), we simplify the Lagrangian by using the Taylor approximation only
on the problematic exponential term. The approximation is expanded up to the 1st order centering around 0 to
yield

L ≈ −

γi,j

1 −

(cid:18)

Tr(W T Ai,jW )
2σ2

(cid:19)

1
2

(cid:88)

i,j

+

Tr(Λ(I − W T W )).

By taking the derivative of the approximated Lagrangian and setting the derivative to zero, an eigen-
value/eigenvector relationship emerges as

ΦW =

 W0 = W0Λ.





(cid:88)

i,j



γi,j
σ2 Ai,j

From this, we see that Φ0 is no longer a function of W . Using this Φ0 we can then calculate a closed form
solution for W0

Appendix H Proof for the computational complexity

For ISM, DG and SM, the bottleneck resides in the computation of the gradient.

f (W ) =

γi,je−

Tr(W T Ai,j W )
2σ2

(cid:88)

i,j

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

∂f
∂W





(cid:88)

i,j



Tr(W T Ai,j W )
2σ2

γi,j
σ2 e−

Ai,j

 W

∂f
∂W

=

(cid:88)

i,j

γi,j
σ2 e−

Tr(W T ∆xi,j ∆xT

i,j W )

2σ2



Ai,j

 W

=





xi,j ∈ Rd×1
W ∈ Rd×q

Where Ai,j = ∆xi,j∆xT

i,j. The variables have the following dimensions.

To compute a new W with DG, we ﬁrst mulitply ∆xT
i,jW , which is O(d). Note that W in DG is always 1 single
column. Next, it multiplies with its own transpose to yied O(d+q2). Then we compute Ai,j to get O(d+q2 +d2).
Since this operation needs to be added n2 times, we get, O(n2(d + q2 + d2)). Since d (cid:29) q, this notation reduces
down to O(n2d2). Let T1 be the number of iterations until convergence, then it becomes O(T1n2d2). Lastly, in
DG, this operation needs to be repeated q times, hence, O(T1n2d2q).
To compute a new W with SM, we ﬁrst mulitply ∆xT
i,jW , which is O(dq). Next, it multiplies with its own
transpose to yied O(dq + q2). Then we compute Ai,j to get O(dq + q2 + d2). Since this operation needs to be
added n2 times, we get, O(n2(dq + q2 + d2)). Since d (cid:29) q, this notation reduces down to O(n2d2). The SM
method requires the computation of the inverse of d × d matrix. Since inverses is cubic, it becomes O(n2d2 + d3).
Lastly, let T2 be the number of iterations until convergence, then it becomes O(T2(n2d2 + d3)).
To compute a new W with ISM, we ﬁrst mulitply ∆xT
i,jW , which is O(dq). Next, it multiplies with its own
transpose to yied O(dq + q2). Then we compute Ai,j to get O(dq + q2 + d2). Since this operation needs to be
added n2 times, we get, O(n2(dq + q2 + d2)). Since d (cid:29) q, this notation reduces down to O(n2d2). The ISM
method requires the computation of the eigen decomposition of d × d matrix. Since inverses is cubic, it becomes
O(n2d2 + d3). Lastly, let T3 be the number of iterations until convergence, then it becomes O(T3(n2d2 + d3)).

Appendix I Measure of Non-linear Relationship by HSIC Versus Correlation

The ﬁgure below demonstrates a visual comparison of HSIC and correlation. It can be seen that HSIC measures
non-linear relationships, while correlation does not.

Figure 5: Showing that HSIC captures non-linear information.

Appendix J Implementation Details of the Cost function

The computation of the cost using the formulation below is slow if it is implemented using a loop.

tr(W T Ai,j W )
2σ2

i,j γi,je−

min − (cid:80)
W
s.t W T W = I
W ∈ Rd×q
A ∈ Rd×d
γi,j ∈ R

(52)

Iterative Spectral Method for Alternative Clustering

Instead, we use the original formulation to derive a faster way to compute the cost.

Starting with the original cost function as

cost = HSIC(XW, U ) − λ HSIC(XW, Y )

cost = Tr(D−1/2KXW D−1/2HU U T H) − λ Tr(D−1/2KXW D−1/2HY Y T H)

When optimizing U , it is obvious that the 2nd portion does not eﬀect the optimization. Therefore, U can be
solved using the following form.

U =

argmin
U

Tr(U T HD−1/2KXW D−1/2HU )

If we are optimization for W , using the combination of the rotation property and the combination of the 2 traces,
the cost can be written as

cost = Tr([D−1/2H(U U T − λY Y T )HD−1/2]K).

In this form, it can be seen that the update of W matrix will only aﬀect the kernel K and the degree matrix D.
Therefore, it makes sens to treat the middle portion as a constant which we refer as Ψ.

Given that [D−1/2ΨD−1/2] is a symmetric matrix, from this form, we can convert the trace into an element wise
product (cid:12).

cost = Tr([D−1/2ΨD−1/2]K)

cost =

([D−1/2ΨD−1/2] (cid:12) K)i,j

(cid:88)

i,j

To further reduction the amount of operation, we let d be a vector of the diagonal elements of D−1/2, hence
d = diag(D−1/2), this equality hold.

D−1/2ΨD−1/2 = [ddT ] (cid:12) Ψ

Therefore, the ﬁnal cost function can be written in its simplest form as :

cost =

Γi,j =

(Ψ (cid:12) [ddT ] (cid:12) K)i,j

(cid:88)

i,j

(cid:88)

i,j

During update, as W update during each iteration, the matrix Ψ stays as a constant while ddT and K update.
The beneﬁt of this form minimize the complexity of the equation, while simplify cost into easily parallelizable
matrix multiplications. The equation also clearly separates the elements into portions that require an update
and portions that does not.

Appendix K Implementation Details of the Derivative

As it was shown from previous sections, the gradient of our cost function using the Gaussian Kernel is

∇f (W ) =

γi,jKi,jAi,j

W − 2W Λ

(cid:88)

(cid:20) 1
σ2

(cid:21)

.
If we let Ψ = (cid:2) 1

2σ2 γi,jKi,j

(cid:3) , it can be rewritten as

∇f (W ) =

Ψi,jAi,j

W − W Λ.

(cid:104)(cid:88)

(cid:105)

From this formulation, the optimal W is equivalent to the eigenvectors of the [(cid:80) Ψi,jAi,j]. According to ISM,
the q eigenvectors corresponding to the smallest eigenvalues is used for W . Since solving (cid:80) Ψi,jAi,j using a loop
is slow, we vectorize the formulation so that

,

,

where DΨ is the degree matrix of P si such that the diagonal elements are deﬁned as

and X ∈ Rn×d is the original data.

Appendix L Hyperparameters Used in Each Experiment

Wu, Ioannidis, Sznaier, Li, Kaeli, Dy

(cid:88)

Ψi,jAi,j = X T [DΨ − Ψ]X

(cid:88)

di,i =

Ψi,j

j

Gauss A
Gauss B 200
Moon 400
Moon+N 200
Flower
Face
Web KB

σ
1
5
0.1
0.2
2
3.1
18.7

λ
0.04
2
1
0.1
10
1
0.057

q
1
3
3
6
2
17
4

