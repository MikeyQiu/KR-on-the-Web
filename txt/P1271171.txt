Joint Topic-Semantic-aware Social Recommendation
for Online Voting

Hongwei Wang1,2, Jia Wang2, Miao Zhao2, Jiannong Cao2, Minyi Guo1∗
1Shanghai Jiao Tong University, 2The Hong Kong Polytechnic University
wanghongwei55@gmail.com,{csjiawang,csmiaozhao,csjcao}@comp.polyu.edu.hk,guo-my@cs.sjtu.edu.cn

7
1
0
2
 
c
e
D
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
7
0
0
.
2
1
7
1
:
v
i
X
r
a

ABSTRACT
Online voting is an emerging feature in social networks, in which
users can express their attitudes toward various issues and show
their unique interest. Online voting imposes new challenges on rec-
ommendation, because the propagation of votings heavily depends
on the structure of social networks as well as the content of vot-
ings. In this paper, we investigate how to utilize these two factors
in a comprehensive manner when doing voting recommendation.
First, due to the fact that existing text mining methods such as
topic model and semantic model cannot well process the content of
votings that is typically short and ambiguous, we propose a novel
Topic-Enhanced Word Embedding (TEWE) method to learn word
and document representation by jointly considering their topics
and semantics. Then we propose our Joint Topic-Semantic-aware
social Matrix Factorization (JTS-MF) model for voting recommenda-
tion. JTS-MF model calculates similarity among users and votings
by combining their TEWE representation and structural informa-
tion of social networks, and preserves this topic-semantic-social
similarity during matrix factorization. To evaluate the performance
of TEWE representation and JTS-MF model, we conduct extensive
experiments on real online voting dataset. The results prove the
efficacy of our approach against several state-of-the-art baselines.

KEYWORDS
Online voting; recommender systems; topic-enhanced word em-
bedding; matrix factorization

1 INTRODUCTION
Online voting [31] has recently become a popular function on so-
cial platforms, through which a user can share his opinion towards
various interested subjects, ranging from livelihood issues to en-
tertainment news. More advanced than simple like-dislike type of
votings, some social networks, such as Weibo1, have empowered
users to run their own voting campaigns. Users can freely initiate
votings on any topics of their own interests and customize voting
options. These votings are visible to the friends of initiator, who
can then choose to participate to make the votings further seen by
their friends or simply retweet the votings to their friends. In such
a way, in addition to the system recommendation, a voting can
widely propagate over the network along social paths. The voting
propagation scheme is shown in Fig. 1.

∗M. Guo is the corresponding author.
1http://www.weibo.com.

CIKM’17, Singapore
2017. 978-1-4503-4918-5/17/11. . . $15.00
DOI: 10.1145/3132847.3132889

Fig. 1: Propagation scheme of online voting.

Facing a large volume of diversified votings, a critical challenge
is to present “right” votings to the “right” person. An effective
recommender system is desired to be able to deal with information
overload [2] by precisely locating what votings favor each user most,
thus improves user experience and maximizes user engagement in
votings. Such a recommender system can also benefit a variety of
other online services such as personalized advertising [26], market
research [11], public opinion analysis [17], etc.

Despite the great importance, there is little prior work consider-
ing recommending votings to users in social networks. The chal-
lenges are two-fold. First, the propagation of online votings relies
heavily on the structure of social networks. A user can see the
votings initiated, participated or retweeted by his followees, which
implies that the user is more likely to be exposed to the votings
that his friends are involved in. Moreover, in most social networks,
a user can join different interest groups, which is another type
of social structure that potentially affects users’ voting behavior.
Though several prior works [3, 6, 10, 12, 25, 29, 31, 32] have been
proposed to leverage social network information in recommenda-
tion, it is still an open question how to comprehensively incorporate
structural social information into the task of voting recommenda-
tion considering its propagation pattern. Second, users’ interest
in votings is strongly connected with voting content presented in
question text (e.g., “Who is your favorite movie star?”). Topic model
[1] is regarded as a possible approach to mine the voting interests
through discovering the latent topic distribution of relevant voting
text. However, the voting questions are typically short and lack
sufficient topic information, leading to severe performance degra-
dation of topic models. Alternatively, semantic analytics [21] can
also possibly be used to mine voting interests through learning text
representations. However, such semantic models typically repre-
sent each word using a single vector, making them indiscriminative
for homonymy and polysemy, which are especially common in
voting questions (e.g., “Do you use apple products?” and “Do you
peel apple before eating?”). In brief, these inherent defects of the

CIKM’17, November 6–10, Singapore

H. Wang et al.

above models limit their power in the scenario of social voting
recommendation.

To address aforementioned challenges, in this paper, we propose
a novel Joint Topic-Semantic-aware Matrix Factorization (JTS-MF)
model for online voting recommendation. JTS-MF model considers
social network structure and representation of voting content in
a comprehensive manner. For social network structure, JTS-MF
model fully encodes the information of social relationship and group
affiliation into the objective function. We will further justify the
usage of social network structure in Section 3. For representation
of voting content, we propose a Topic-Enhanced Word Embedding
(TEWE) method to build a multi-prototype word and document2
representation, which jointly considers their topics and semantics.
The key idea of TEWE is to enable each word to have different rep-
resentations under different word topics and different documents.
We will detail TEWE in Section 5. Once obtaining TEWE repre-
sentation for each document, JTS-MF model combines them with
the structural information of social networks to calculate the topic-
semantic-social similarity among users and votings. The reason
of calculating such similarity is that, inspired by Locally Linear
Embedding [23], we try to preserve the similarity among users
and votings during matrix factorization, as it contains abundant
proximity information and can greatly benefit feature learning for
users and votings. JTS-MF model is detailed in Section 6.

We conduct extensive investigation on JTS-MF with real online
voting dataset. The experimental results in Section 7 demonstrate
that JTS-MF model achieves substantial gains compared with base-
lines. The results also prove that TEWE is able to well combine
topic and semantic information of texts and generates a better kind
of document representation.

In summary, the contributions of this paper are as follows:

• We formally formulate the online voting recommendation

problem, which has not been fully investigated yet.

• We indicate that user’s voting behavior is highly correlated
with social network structure by conducting thorough sta-
tistical measurements.

• We propose a novel Topic-Enhanced Word Embedding
model to jointly consider topics and semantics of words
and documents to learn their representation. TEWE takes
advantages of both topic model and semantic model, and
consequently learns more informative embeddings.

• We develop a novel matrix factorization based models,
named JTS-MF, for online voting recommendation. JTS-
MF is able to preserve the topic-semantic-social similarity
among users and votings from original embedding space
during learning process.

• We carry out extensive experiments on real online voting
dataset, the results of which reveal that JTS-MF signif-
icantly outperforms baseline (variant) methods, say for
example, surpassing basic matrix factorization model with
57%, 38% and 25% enhancement in terms of recall for top-1,
top-5 and top-20 recommendation, respectively.

2In this paper, a “document” can be related to a voting, a user or a group. A voting
document is the content of its question, a user document is formed by aggregating
all the documents of votings he participates, and a group document is formed by
aggregating all the documents of users who join the group.

2 RELATED WORK
2.1 Recommender Systems
Roughly speaking, existing recommender systems can be catego-
rized into three classes [2]: content-based, collaborative filtering,
and hybrid methods. Content-based methods [14, 34] make use of
user profiles or item descriptions as features for recommendation.
Collaborative filtering methods [22, 25, 28, 31] use either explicit
feedback (e.g., users’ ratings on items) or implicit feedback (e.g.,
users’ browsing records about items) data of user-item interactions
to find user preference and make the recommendation. In addition,
various models are incorporated into collaborative filtering, such
as Support Vector Machine [30], Restricted Boltzmann Machine
[24], and Stacked Denoising Auto Encoder [27]. Hybrid methods
[9, 16] combine content-based and collaborative filtering models
in many hybridization approaches, such as weighted, switching,
cascade and feature combination or augmentation.

2.2 Social Recommendation
Traditional recommender systems are vulnerable to data sparsity
problem and cold-start problem. To mitigate this issue, many ap-
proaches have been proposed to utilize social network information
in recommender systems [3, 6, 10, 12, 25, 29, 31, 32]. For exam-
ple, [12] represents a social network as a star-structured hybrid
graph centered on a social domain which connects with other item
domains to help improve the prediction accuracy. [10] investi-
gates the seed selection problem for viral marketing that considers
both effects of social influence and item inference for product rec-
ommendation. [29] studies the effects of strong and weak ties in
social recommendation, and extends Bayesian Personalized Rank-
ing model to incorporate the distinction of strong and weak ties.
However, the above works only utilize users’ social links without
considering the topic and semantic information for mining the
similarities among users and items, which we found quite help-
ful for voting recommendation tasks. Another difference between
these works and ours is that we also take social group affiliation
into consideration, which can further improve the performance of
recommendation.

2.3 Topic and Semantic Language Models
Latent Dirichlet Allocation (LDA) [1] is a well-known generative
topic model that learns the latent topic distributions for documents.
LDA is widely used in sentiment analysis [20], aspects and opin-
ions mining [33], and recommendation [5]. Word2vec [21] is gen-
erally recognized as a neural network model, which learns word
representations that capture precise syntactic and semantic word
relationships. Word2vec as well as associated Skip-Gram model are
extensively used in document classification [15], dependency parser
[4], and network embedding [8]. However, LDA and Word2vec are
not directly applicable in the scenario of voting recommendation
because the content of voting is usually short and ambiguous. As
a combination, [18] tries to learn topical word embeddings based
on both words and their topics. The difference between [18] and
ours is that we also take topics of documents into consideration,
which enables our model to learn a even more discriminative and
informative representations for words and documents.

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

Table 1: Basic Statistics of Weibo Dataset.

# users
# users with votings
# users with groups
# votings

1,011,389
525,589
723,913
185,387

# groups
# user-voting
# user-user
# user-group

299,077
3,908,024
83,636,677
5,643,534

3 BACKGROUND AND DATA ANALYSIS
In this section, we briefly introduce the background of Weibo voting
and present detailed analysis of Weibo voting dataset.

3.1 Background
Weibo is one of the most popular Chinese microblogging website
launched by Sina corporation, which is akin to a hybrid of Twitter
and Facebook platforms. Users on Weibo can follow each other,
write tweets and share with his followers. Users can also join
different groups based on their attributes (e.g., region) or interests
of topics (e.g., career).

Voting3 is one of the embedded features of Weibo. As of January
2013, more than 92 million users have participated in at least one
voting and more than 2.2 million ongoing votings are available on
Weibo every day. Any user can freely initiate, retweet and partici-
pate a voting campaign in Weibo. As shown in Fig. 1, votings can
propagate in two ways. The first way is through social propagation:
a user can see the voting initiated, retweeted or participated by his
followees and potentially participates the voting. The second way
is through Weibo voting recommendation list, which consists of
popular votings and personalized recommendation for each user.

3.2 Data Measurements
Our Weibo voting dataset comes from the technical team of Sina
Weibo, which contains detailed information about votings from No-
vember 2010 to January 2012, as well as other auxiliary information.
Specifically, the dataset includes users’ participation status on each
voting4, content of each voting, social connection between users,
name and category of each group, and user-group affiliation.

3.2.1 Basic statistics. The basic statistics are summarized in
Table 1. From Table 1 we can learn that, each user has 165.4 fol-
lowers/followees, participates 3.9 votings, and joins 5.6 groups on
average. If we only count users who participate at least one vot-
ing and users who join at least on group, the average number of
votings and average number of joined groups of each user is 7.4
and 7.8, respectively. Fig. 2 depicts the distribution curves of the
above statistics, where the meaning of each subfigure is given in
the caption.

To get an intuitive understanding of whether user’s voting be-
havior is correlated with his social relation and group affiliation,
we conduct the following two sets of statistical experiments:

3.2.2 Correlation between the number of common votings of user
pairs and the types of user pairs. We randomly select ten million
user pairs from the set of all users, and count the average number
of votings that the two users both participate under the following
four circumstances: 1) one of the users follows the other in the
pair, i.e., they are social-level friends; 2) the two users are in at

3http://www.weibo.com/vote?is all=1.
4We only know whether a user participated a voting or not, rather than user voting
results, i.e., we do not know which voting option a user chose.

(a)

(c)

(e)

(b)

(d)

(f)

Fig. 2: (a) Distribution of the number of votings participated
by a user; (b) Distribution of the number of participants of a
voting; (c) Distribution of the number of followers/followees
of a user; (d) Distribution of the number of users in a group;
(e) Distribution of the number of votings (may contain du-
plicated votings) participated by all users in a group; (f) Dis-
tribution of the number of groups joined by a user.

least one common group, i.e., they are group-leven friends; 3) the
two users are neither social-level friends nor group-level friends;
4) all cases. The results are plotted in Fig. 3a, which clearly shows
the difference among these cases. In fact, the average number of
common votings of social-level friends (3.54 × 10−4) and group
level friends (1.79 × 10−4) are 17.4 and 8.8 times higher than that of
“strangers” (2.04 × 10−5). The results demonstrate that if two users
are social-level or group-level friends, they are likely to participate
more votings in common.

3.2.3 Correlation between the probability of two users being
friends and whether they participate common voting. We first ran-
domly select ten thousand votings from the set of all votings. For
each sampled voting vj , we calculate the probability that two of
its participants are social-level or group-level friends, i.e., pj =
# of social/group-level friends among participants of vj
, where nj is the num-
nj ×(nj −1)/2
ber of vj ’ participants. We calculate pj over all sampled votings
and plot the average result (blue bar) in Fig. 3b. For comparison, we
also plot the result for randomly sampled set of users (green bar)
in Fig. 3b. It is clear that if two users ever participated common
voting, they are more likely to be social-level or group-level friends.
In fact, probabilities of two users being social-level or group-level
friends are raised by 5.3 and 3.6 times given the observation that
they are with common voting.

CIKM’17, November 6–10, Singapore

H. Wang et al.

(a)

(b)

Fig. 3: (a) Average number of common votings participated
by user ui and uk in four cases: 1. ui follows/is followed by
uk ; 2. ui and uk are in at least one common group; 3. ui and
uk have no social-level and group-level relationship; 4. all
cases; (b) Probability of two users being social-level or group-
level friends in two cases: 1. they ever participated at least
one common voting; 2. they are randomly sampled.

The above two findings effectively prove the strong correlation
between voting behavior and social network structure, which mo-
tivates us to take users’ social relation and group affiliation into
consideration when making voting recommendation.

4 PROBLEM FORMULATION
In this paper, we consider the problem of recommending Weibo
votings to users. We denote the set of all users, the set of all votings,
and the set of all groups by U = {u1, ..., uN }, V = {v1, ..., vM },
and G = {G1, ..., GL }, respectively. Moreover, we model three
types of relationship in Weibo platform: user-voting, user-user, and
user-group relationship as follows:

(1) The user-voting relationship for ui and vj is defined as

=

(1)

Iui,vj

i f ui participates vj ;
otherwise.

(cid:40)1,
0,
(2) The user-user relationship for ui and uk
(cid:40)1,
;
0,
to denote the set of ui ’s followees, and
to denote the set of ui ’s followers (“+” means “out”

i f ui f ollows uk
otherwise.

We further use F +
i
use F −
i
and “−” means “in”).

is defined as

Iui,uk

(2)

=

(3) The user-group relationship for ui and Gc is defined as

Iui,Gc

=

(cid:40)1,
0,

i f ui joins Gc ;
otherwise.

(3)

Given the above sets of users and votings as well as three types
of relationship, we aim to recommend a list of votings for each user,
in which the votings are not participated by the user but may be
interesting to him.

5 JOINT-TOPIC-SEMANTIC EMBEDDING
In this section, we explain how to learn the embeddings of users,
votings, and groups in a joint topic and semantic way, and apply the
embeddings to calculate similarities. We first introduce the methods
of learning topic information and semantic information by LDA and
Skip-Gram models, respectively, and propose our method which
combines these two models to learn more powerful embeddings.

5.1 Topic Distillation
In this subsection, we introduce how to profile users, votings, and
groups in terms of topic interest distribution by performing topic
distillation on the associated textual content information.

In general, LDA is a popular generative model to discover latent
topic information from a collection of documents [1]. In LDA, each
document d is represented as a multinomial distribution Θ
over a
d
set of topics, and each topic z is also represented as a multinomial
distribution Φz over a number of words. Subsequently, each word
position l in document d is assigned a topic zd,l
,
according to Θ
d
and the word wd,l
. By LDA
approach, the topic distribution for each document and the topic
assignment for each word can be obtained, which would be utilized
later in our proposed model.

is finally generated according to Φzd,l

’s and dvj

= ∪{dui |Iui,Gc

5. The document dui

= 1}, and the document dGc

Here, we discuss how to apply LDA in the scenario of Weibo
voting. According to the Weibo voting dataset, each voting vj
associates a sentence of question, which can be regarded as doc-
for user ui can thus be formed by
ument dvj
aggregating the content of all votings he participates, i.e., dui
=
for group Gc is formed by
∪{dvj |Iui,vj
aggregating documents of all its members, i.e., dGc
=
1}. Note that though our target is to learn the topic distributions of
all users, votings, and groups, it is inadvisable to train LDA model
on dui
’s because: (1) the entitled sentence associated with
a single voting is typically short-presented and topic-ambiguous;
(2) even with user-level voting content aggregation, some docu-
ments of inactive users are not long enough to accurately extract
the authentic topic distribution, yet showing relatively flat distribu-
tion over all the topics. Therefore, we choose to feed group-level
aggregated documents dGc
’s to LDA model as training samples.
The process of group-level voting content aggregation will cover
all the content the affiliated users are interested in and help better
identify their interests in terms of voting topic.
Denote Dir(α ) as the Dirichlet prior of Θ
d

, and Dir(β) as the
Dirichlet prior of Φz . Given α and β, the joint distribution of
document-topic distributions Θ, topic-word distributions Φ, topics
of words z, and a set of words w is

p(Θ, Φ, z, w |α , β)
z p(Φz |β) · (cid:206)
= (cid:206)
d

(cid:18)
p(Θ

d |α ) (cid:206)
l

(cid:16)
p(zd,l |Θ

d )p(wd,l |Φzd,l )

(cid:17) (cid:19)

,

(4)
where d traverses all group-level aggregated documents. In general,
it is computationally intractable to directly maximize the joint
likelihood in Eq. (4), thus Gibbs Sampling [7] is usually applied
to estimate the posterior probability p(z|w, α , β) and solve Θ, Φ
(w )
the
iteratively. Denote θ
the z-th component of Θ
d
z
w-th component of Φz . With the sampling results, Θ
and Φz can
d
be estimated as:

, and ϕ

(z)
d

θ

(z)
d
(w )
ϕ
z

(z)
= (cid:0)n
d
(w )
= (cid:0)n
z

+ α (z)(cid:1)/(cid:0) (cid:205)
z (n
+ β (w )(cid:1)/(cid:0) (cid:205)

+ α (z))(cid:1),
(w )
z
where α (z) is the z-th component of α , β (w ) is the w-th component
(w )
of β, n
z

z = 1, ..., Z ,
+ β (w ))(cid:1), w = 1, ..., V ,

is the observation counts of topic z for document d, n

(z)
d
w (n

(5)

(z)
d

is segmented by Jieba (https://github.com/fxsjy/jieba) and all stop words are

5dvj
removed.

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

L(D) =

log p(wt +c |wt ),

(6)

L(D) =

log p((cid:104)wt +c , zt +c , zd

t +c (cid:105)|(cid:104)wt , zt , zd

t (cid:105)),

(8)

is the frequency of word w assigned as topic z, Z is the number of
topics and V is vocabulary size.

So far, we have obtained the topic assignment for each word and
topic distribution for each group. Topic distributions for users and
votings can thus be inferred by using the learned model and Gibbs
Sampling, which is similar to the calculation of θ

in Eq. (5).

(z)
d

5.2 Semantic Distillation
In this subsection, we introduce how to profile users, votings, and
groups in terms of semantic information. Word embedding, which
represents each word using a vector, is widely used to capture
semantic information of words. Skip-Gram model is a well-known
framework for word embedding, which finds word representation
that are useful for predicting surrounding words in a document
given a target word in a sliding window [21]. More formally, given
a word sequence D = {w1, w2, . . . , wT }, the objective function of
Skip-Gram is to maximize the average log probability

1

T

T
(cid:213)

t =1

(cid:213)

−k ≤c ≤k
c(cid:44)0

where k is the training context size of the target word, which
can be a function of the centered work wt . The basic Skip-Gram
formulation defines p(wi |wt ) using the softmax function as follows:

p(wi |wt ) =

(cid:205)

exp(w(cid:62)

i wt )
exp(w(cid:62)wt )

,

w ∈V
where wi and wt are the vector representation of context word wi
and target word wt , respectively, and V is the vocabulary. To avoid
traversing the entire vocabulary, hierarchical softmax or negative
sampling are used in general during learning process [21].

(7)

5.3 Topic-Enhanced Word Embedding
In this subsection, we propose a joint topic and semantic learning
model, named Topic-Enhanced Word Embedding (TEWE), to ana-
lyze documents of users, votings, and groups. The motivation of
proposed TEWE is based on the following two observations: (1)
The voting content typically involves short texts. Even we infer
the topic distribution for each voting based on the learned topic-
word distribution from group-level aggregated documents, it is
still topic-ambiguous to some extent. (2) The Skip-Gram model
for word embedding assumes that each word always preserves a
single vector, which sometimes is indiscriminate under different
circumstances due to the homonymy and polysemy. Therefore, the
basic idea of TEWE is to preserve topic information of documents
and words when measuring the interaction between target word wt
and context word wi . In this way, a word with different associated
topics has different embeddings, and a word in documents with
different topics has different embeddings, too.

Specifically, rather than solely using the target word w to predict
context words in Skip-Gram, TEWE also jointly utilizes zw , the
topic of the word in a document, as well as zd
, the most likely
w
topic of the document that the word belongs to. Recall that in Sec-
tion 5.1, we have obtained the topic of each word zw and topic
can be calculated
distributions of each document Θ
d

, thus zd
w

(a) Skip-Gram

(b) TEWE

Fig. 4: Comparison between Skip-Gram and TEWE. The
gray circles in (a) indicate the embeddings of original words,
while the blue circles in (b) indicate the TEWE representa-
tion of pseudo words, which preserves semantic and topic
information of words and documents.

(z)
d

, where θ

= arg maxz θ

(z)
is the probability that docu-
as zd
w
d
ment d belongs to topic z, as introduced in Eq. (5). TEWE regards
each word-topics triplet (cid:104)w, zw , zd
w (cid:105) as a pseudo word and learns a
unique vector wz,zd for it. The objective function of TEWE is as
follows:

(cid:213)

1

T

T
(cid:213)

t =1

−k ≤c ≤k
c(cid:44)0
(cid:105)|(cid:104)wt , zt , zd

(cid:17) .

(cid:17)

t (cid:105)) =

wz,zd
i

p((cid:104)wi , zi , zd

i (cid:105)|(cid:104)wt , zt , zd

where p((cid:104)wi , zi , zd
i

t (cid:105)) is a softmax function as
exp (cid:16)
(cid:62)wz,zd
t
exp (cid:16)
wz,zd (cid:62)wz,zd
t
(9)
The comparison between Skip-Gram and TEWE is shown in Fig.
4. Instead of solely utilizing the target and context words as in Skip-
Gram, TEWE further preserves word topic and document topic
along with these words, and incorporates both topic and semantic
information in embedding learning.

(cid:104)w,z,zd (cid:105) ∈ (cid:104)V ,Z,Z (cid:105)

(cid:205)

Once obtaining TEWE representation for each pseudo word, the
representation of each document can be correspondingly derived
by aggregating the embeddings of its containing words weighted
by term frequency-inverse document frequency (TF-IDF) coefficient.
Specifically, for each document d, its TEWE can be calculated as

= (cid:205)

ed

w ∈d

TF-IDF(w, d) · wz,zd

,

(10)

where TF-IDF(w, d) is the product of the raw count of w in d and
the logarithmically scaled inverse fraction of the documents that
contains w, i.e., TF-IDF(w, d) = fw,d · log
(D is the set
of all documents). TEWE document representations can be used in
measuring inter-document similarities. For example, the similarity
can be calculated as the cosine
of two user documents dui
similarity between their TEWE representations, i.e.,

|D |
|d ∈D:w ∈d |

(cid:107)2
This similarity encodes both topic and semantic proximity informa-
tion of user documents, which implicitly reveals the similarity of
voting interests between two users.

e(cid:62)
euk
ui
(cid:107)eui (cid:107)2 (cid:107)euk

and duk

.

6 RECOMMENDATION MODEL
In this section, we present our Joint Topic-Semantic-aware Matrix
Factorization (JTS-MF) model for online social votings, in which
social relationship, group affiliation, and topic-semantic similarities
are combined and taken into account for voting recommendation in

CIKM’17, November 6–10, Singapore

H. Wang et al.

between two users from both topic-semantic interests and their
social influence perspectives.

To avoid the impact of different numbers of followees, we use
the normalized social-level similarity coefficient of users in JTS-MF,
which is defined as

,

=

(cid:205)

(12)

(cid:98)Si,k

Si,k

where F +
i

Si,k
k ∈ F+
i
denotes the set of ui ’s followees in social network.
6.1.2 Normalized group-level similarity coefficient of users. Group-
level similarity coefficient of users is represented by matrix G N ×N ,
which actually measures the topic-semantic similarity among users
from viewpoint of groups. For each ui , the group-level similarity
coefficient with respect to uk

is defined as

= (cid:213)

Gi,k

Iui,G · Iuk,G ·

G ∈ G

e(cid:62)
ui eG
(cid:107)eui (cid:107)2 (cid:107)eG (cid:107)2

,

(13)

where G represents the set of all groups, Iui,G and Iuk,G indicate
whether ui and uk
join group G respectively as described in Eq.
(3), and the last term is the topic-semantic similarity between user
reflects the interest
ui and group G. Essentially speaking, Gi,k
closeness between user ui and its group-level friend uk
by using
ui ’s topic-semantic engagement extent to the corresponding group.
We also normalize the group-level similarity coefficient of users as

where Gi is the set of ui ’s group-level friends in social network.

6.1.3 Normalized similarity coefficient of votings. Similarity coef-
ficient of votings is represented by matrix T M ×M , which is directly
defined as the topic-semantic similarity among votings, i.e.,

(cid:98)Gi,k

=

Gi,k
k ∈ Gi Gi,k

,

(cid:205)

Tj,t =

e(cid:62)
vj evt
(cid:107)evj (cid:107)2 (cid:107)evt (cid:107)2

.

Since the number of votings is typically huge, we only consider
the similarity between two votings with sufficiently high coefficient
value. Specifically, for each voting vj , we define a set of votings Vj
containing those votings whose similarity coefficients with vj ex-
ceed a threshold, i.e., Vj = {vt |Tj,t ≥ threshold}. Correspondingly,
the similarity coefficient of votings are normalized as

(cid:98)Tj,t =

Tj,t
t ∈Vj Tj,t

.

(cid:205)

6.2 Objective Function
Using the notations listed above, the objective function of JTS-MF
can be written as

L =

1
2

N
(cid:213)

M
(cid:213)

i =1

j=1

(cid:16)

I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17) 2

+ α
2

N
(cid:213)

i =1

(cid:13)
(cid:13)Qi −

(cid:213)

k ∈F+
i

(cid:98)Si,k Qk

(cid:13)
(cid:13)

2
2

+ β
2

N
(cid:213)

i =1

(cid:13)
(cid:13)Qi −

(cid:213)

k ∈Gi

(cid:98)Gi, k Qk

(cid:13)
(cid:13)

2

2 + γ
2

M
(cid:213)

j=1

(cid:13)
(cid:13)Pj −

(cid:213)

t ∈Vj

(cid:98)Tj, t Pt

(cid:16)

(cid:13)
(cid:13)

2

2 + λ
2

(cid:107)Q (cid:107)2
F

+ (cid:107)P (cid:107)2
F

(cid:17)

.

The basic idea of the objective function in Eq. (17) lies in that,
besides considering explicit feedback between users and votings,
we also impose penalties on the discrepancy among features of

(14)

(15)

(16)

(17)

Fig. 5: Graphic Model of JTS-MF.

a comprehensive manner. Motivated by Locally Linear Embedding
[23] which tries to preserve the local linear dependency among
inputs in the low-dimensional embedding space, we expect to keep
inter-user and inter-voting topic-semantic similarities in latent
feature space as well. To this end, in JTS-MF model, while the
rating Ri, j is factorized as user latent feature Qi and voting latent
feature Pj, we deliberately enforce Qi and Pj to be dependent on
their social-topic-semantic similar counterparts, respectively. The
graphic model of JTS-MF model is as shown in Figure 5.

6.1 Similarity Coefficients
In order to characterize the influence of inter-user common interests
and inter-voting content relevance, we first introduce the following
three similarity coefficients:

• Normalized social-level similarity coefficient of users: (cid:98)Si,k

,

where uk

is the social-level friend of ui ;

• Normalized group-level similarity coefficient of users: (cid:98)Gi,k

,

where uk

is the group-level friend of ui ;

• Normalized similarity coefficient of voting: (cid:98)Tj,t , where vj

and vt are two distinct votings.

Generally speaking, in JTS-MF, the latent feature Qi for user ui
is tied up with the latent feature of his social-level and group-level
friends who are weighted through (cid:98)Si,k
’s. Likewise, the
latent feature Pj for voting vj is tied up with the latent feature of
its similar votings, which are weighted through (cid:98)Tj,t ’s.

’s and (cid:98)Gi,k

·

Si,k

is defined as

= Iui,uk ·

e(cid:62)
ui euk
(cid:107)eui (cid:107)2 (cid:107)euk (cid:107)2

6.1.1 Normalized social-level similarity coefficient of users. Social-
level similarity coefficient of users is represented by matrix S N ×N ,
which incorporates both social relationship and user-user topic-
semantic similarity. Specifically, for each ui , the social-level simi-
larity coefficient with respect to uk
(cid:115) d−
+ d
k
d+
+ d−
i
k
indicates whether ui follows uk
is the out-degree of ui in the social network (i.e., d
in the social network (i.e., d−
is the in-degree of uk
k

as described in Eq. (2),
where Iui,uk
= |F +
+
+
|),
d
i
i
i
= |F −
d−
|), d
k
k
e(cid:62)
euk
is the smoothing constant (d = 1 in this paper), and
ui
(cid:107)eui (cid:107)2 (cid:107)euk
is the topic-semantic similarity between user ui and user uk
mentioned in Section 5.3.

+d
k
+d −
k
of local authority and local hub value to differentiate the impor-
counts the closeness
tance of different users [19]. Essentially, Si,k

incorporates the information

(cid:114) d −
d +
i

+ d

(cid:107)2
as

(11)

+d

,

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

similar users and similar votings. We give detailed explanation as
follows. The first term of Eq. (17) measures the mean squared error
between prediction and ground truth, where I (cid:48)
is the training
i, j
weights defined as

I (cid:48)
i, j

=

(cid:40)1,
Im,

i f ui participates vj
otherwise

.

(18)

defined in Eq. (1) as the
The reason we do not directly use Iui,vj
training weights is because we found a small and positive Im makes
the training process more robust and can greatly improve the results.
Ri, j is the actual rating of user ui on voting vj , and Qi P (cid:62)
is the
j
predicted value of Ri, j . Without loss of generality, in JTS-MF model,
we set Ri, j = 1 if ui participates vj and Ri, j = 0 otherwise.

’s. Weight (cid:98)Si,k

’s address both the followee uk

The second, third, and fourth terms of Eq. (17) measure the
penalty of discrepancy among similar users and similar votings. In
particular, the second term enforces user ui ’s latent feature Qi to
be similar to the weighted average of his like-minded followees’
profiles Qk
’s social
influence on ui as well as the degree of common voting interests
shared between uk
and ui . The third term enables user ui ’s latent
feature Qi to be similar to the weighted average of all his group
’s emphasize both the same group
peers’ profiles Qk
affiliation of users ui and uk
and also the tie strength between ui and
the associated group with respect to voting interests. This implies
that, among all group-level friends, ui would have more similar
latent feature with the users who frequently join those groups ui is
interested in. The fourth term ensures voting vj ’s latent feature Pj
to be similar to the weighted average of votings that share similar
topic-semantic information with vj .

’s. Weight (cid:98)Gi,k

Finally, the last term of Eq. (17) is the regularizer to prevent

over-fitting, and λ is the regularization weight.

The trade-off among user social-level similarities, user group-
level similarities, and voting similarities is controlled by the pa-
rameters α, β, and γ , respectively. Obviously, users’ social-level
similarity, users’ group-level similarity, or votings’ similarity is/are
ignored if α, β, or γ is/are set to 0, while increasing these values
shifts the trade-off more towards their respective directions.

6.3 Learning Algorithm
To solve the optimization in Eq. (17), we apply batch gradient
descent approach to minimize the objective function6. The gradients
of loss function in Eq. (17) with respect to each variable Qi and Pj
are as follows:

(cid:16)

−I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17)

Pj

∂L
∂Qi

=

M
(cid:213)

j =1
(cid:18)

(cid:213)

k ∈F+
i
(cid:213)

k ∈Gi

(cid:1) + (cid:213)
t ∈F−
i

(cid:1) + (cid:213)
t ∈U

+ α

(cid:0)Qi −

(cid:98)Si,k Qk

−(cid:98)St,i

(cid:0)Qt −

(cid:98)St,k Qk

(cid:18)

+ β

(cid:0)Qi −

(cid:98)Gi,k Qk

− (cid:98)Gt,i

(cid:0)Qt −

(cid:98)Gt,k Qk

+ λQi,

(cid:213)

k ∈F+
t
(cid:213)

k ∈Gi

(cid:19)

(cid:1)

(cid:19)

(cid:1)

(19)

(cid:16)

−I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17)

Qi

∂L
∂Pj

=

N
(cid:213)

i =1
(cid:18)

+ γ

(cid:0)Pj −

(cid:98)Tj, t Pt

−(cid:98)Tk, j

(cid:0)Pk −

(cid:213)

t ∈Vj

(cid:1) + (cid:213)
k ∈Vj

(cid:213)

t ∈Vk

(cid:19)

(cid:1)

(cid:98)Tk, t Pt

+ λPj .

(20)

To clearly understand the gradients in Eq. (19) and (20), it is
worth pointing out that Qi appears not only in the i-th sub-term
in the second and third lines of Eq. (17) explicitly, but also exists
in other t-th sub-terms followed by (cid:98)St,i or (cid:98)Gt,i , where ui plays as
one of the followees or group members of other users. The case is
similar for Pj. Given the gradients in Eq. (19) and (20), we list the
pseudo code of the learning algorithm for JTS-MF as follows:

(1) Randomly initialize Q and P;
(2) In each iteration of the algorithm, do:
a) update each Qi : Qi ← Qi − δ ∂L
;
∂Qi
b) update each Pj: Pj ← Pj − δ ∂L
;
∂Pj
until convergence, where δ is an configurable learning rate.

7 EXPERIMENTS
In this section, we evaluate our proposed JTS-MF model on the
aforementioned Weibo voting dataset7. We first introduce base-
lines and parameter settings used in the experiments, and then
present the experimental results of JTS-MF and the comparison
with baselines.

7.1 Baselines
We use the following seven methods as the baselines against JTS-
MF model. Note that the first three baselines are reduced versions
of JTS-MF, which only consider one particular type of similarity
among users or votings.

• JTS-MF(S) only considers social-level similarity of users,

i.e., sets β, γ = 0 in JTS-MF model.

• JTS-MF(G) only considers group-level similarity of users,

i.e., sets α, γ = 0 in JTS-MF model.

• JTS-MF(V) only considers similarity of votings, i.e., sets

α, β = 0 in JTS-MF model.

• MostPop recommends the most popular items to users,
i.e., the votings that have been participated by the most
numbers of users.

• Basic-MF [13] simply uses matrix factorization method
to predict the user-voting matrix while ignores additional
social relation, group affiliation and voting content infor-
mation.

for ed

• Topic-MF [1] is similar to JTS-MF except that we substi-
when calculating similarities in Eq. (11),
tute Θd
(13), and (15). Note that Θd
can also be viewed as the em-
bedding of document with respect to topics. Therefore,
Topic-MF only considers the topic similarity among users
and votings.

• Semantic-MF is similar to JTS-MF except that we use
the Skip-Gram model in [21] directly to learn the word
embeddings. Therefore, Semantic-MF only considers the
semantic similarity among users and votings.

6Note that it is impractical to apply Alternating Least Squares (ALS) method here
because it requires calculating the inverse of two matrices with extremely large size.

7Experiment code is provided at https://github.com/hwwang55/JTS-MF.

CIKM’17, November 6–10, Singapore

H. Wang et al.

Fig. 7. The parameter settings of α, β, γ , and dim are the same as in
Section 7.3.1. Fig. 7a, 7b, and 7c consistently demonstrate that JTS-
MF(S) performs best and JTS-MF(G) performs worst among three
types of reduced versions of JTS-MF. Note that JTS-MF(S) only con-
siders users’ social-level similarity and JTS-MF(G) only considers
users’ group-level similarity. Therefore, it could be concluded that
social-level friends are more helpful than group-level friends when
determining users’ voting interest. This is in accordance with our
intuition, since a user typically has much more group-level friends
than social-level friends, which inevitably dilutes its effectiveness
and brings noises into group-level relationship. In addition, the
result in Fig. 7 also demonstrates the effectiveness of the usage of
votings’ similarity. Furthermore, it can be evidently observed that
JTS-MF model outperforms its three reduced versions in all cases,
which proves that the three types of similarities are well combined
in JTS-MF model to achieve much better results.

7.3.3 Comparison of Models. To further compare JTS-MF model
with other baselines, we gradually increase k from 1 to 500 and
report the results in Table 2 with the best performance highlighted
in bold. The value of α, β, and γ for JTS-MF and its reduced models
are the same as in Section 7.3.1. The parameter settings are α = 2,
β = 60, γ = 15 for Topic-MF, α = 8, β = 120, γ = 20 for Semantic-
MF, and dim = 10 for Qi and Pj in all MF-based methods. The
above parameter settings are the optimal results of fine tuning for
given dim. In Table 2, several observations stand out:

• MostPop performs worst among all methods, because Most-
Pop simply recommends the most popular votings to all
users without considering users’ specific interests.

• Topic-MF and Semantic-MF outperforms Basic-MF, which
proves the usage of similarities with respect to topic and se-
mantic helpful for recommending votings. Besides, Semantic-
MF outperforms Topic-MF. This suggests that semantic
information is more accurate than topic information when
measuring similarities through mining short-length texts.
• JTS-MF outperforms Topic-MF and Semantic-MF. This is
the most important observation from Table 2, since it jus-
tifies our aforementioned claim that joint-topic-semantic
model can benefit from both topic and semantic aspects
and achieve better performance.

• The significance of JTS-MF over other models is evident for
small k. However, this margin becomes smaller when k gets
larger, and JTS-MF is even slightly inferior to JTS-MF(S)
when k ≥ 50. This means that users’ group-level simi-
larities and votings’ similarities “drag the feet” of JTS-MF
model when k is large. However, JTS-MF is still preferred
in practice, since a real recommender system would only
recommend a small set of votings to users in general.

7.4 Parameter Sensitivity
We investigate parameter sensitivity in this subsection. Specifically,
we evaluate how different value of trade-off parameters α, β, γ , and
different numbers of latent feature dimensions dim can affect the
performance.

7.4.1 Trade-off parameters. We fix dim = 10, keep two of the
trade-off parameters as 0, and vary the value of the left trade-off

Fig. 6: Convergence of JTS-MF models with respect to Re-
call@10.

7.2 Parameter Settings
We use GibbsLDA++8, an open-source implementation of LDA
using Gibbs sampling, to calculate topic information of words and
documents in JTS-MF and Topic-MF models. We set the number of
topics to 50 and leave all other parameters in LDA as default values.
For word embeddings in JTS-MF and Semantic-MF models, we use
the same settings as follows: length of embedding dimension as 50,
window size as 5, and number of negative samples as 3.

For all MF-based methods, we set the learning rate δ = 0.001 and
regularization weight λ = 0.5 by 10-fold cross validation. Typically,
we set Im = 0.01 in Eq. (18). Taking into consideration the balance
of experimental results and time complexity, we run 200 iterations
for each of the experiment cases. To conduct the recommendation
task, we randomly select 20% of users’ voting records in the dataset
as test set and use the remaining data as the trainning examples for
our JTS-MF model as well as all baselines. The choice of remaining
hyper-parameters (trade-off parameters α, β, γ , and dimension of
latent features dim) is discussed in Section 7.4.

To quantitatively analyze the performance of voting recommen-
dation, in our experiment, we use top-k recall (Recall@k), top-k
precision (Precision@k), and top-k micro-F1 (Micro-F1@k) as the
evaluation metrics.

7.3 Experiment Results

7.3.1

Study of convergence. To study the convergence of JTS-
MF model, we run the learning algorithm up to 200 iterations for
JTS-MF(S) with α = 10, JTS-MF(G) with β = 140, JTS-MF(V) with
γ = 30, JTS-MF with α = 10, β = 140, γ = 30 (dim = 10 for Qi and
Pj in all models), then calculate Recall@10 for every 10 iterations.
The result of convergence of JTS-MF models is plotted in Fig. 6.
From Fig. 6 we can see that, the recall of JTS-MF models rises
rapidly before 100 iterations, and starts to oscillate slightly after
around 150 iterations. The same changing pattern is observed for
all four JTS-MF variants. Therefore, we set the number of learning
iterations as 200 to achieve a balance between running time and
performance of models.

7.3.2

Study of JTS-MF. To study the performance of JTS-MF
model and the effectiveness of three types of similarities, we run
JTS-MF model as well as its three reduced versions on Weibo voting
dataset, and report the results of Recall, Precision, and Micro-F1 in

8GibbsLDA++: http://gibbslda.sourceforge.net

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

(a)

(b)

(c)

Fig. 7: (a) Recall@k, (b) Precision@k, and (c) Micro-F1@k of JTS-MF models.

(a)

(b)

(c)

(d)

Fig. 8: Parameter sensitivity with respect to (a) α, (b) β, (c) γ , and (d) dim.

Table 2: Result of Recall@k, Precision@k, and Micro-F1@k for JTS-MF model and baselines.

Model

Metric

k

50

100

500

JTS-MF(S)

JTS-MF(G)

JTS-MF(V)

JTS-MF

MostPop

Basic-MF

Topic-MF

Semantic-MF

1
0.0097
0.007416
0.008401
0.0065
0.004944
0.005601
0.0071
0.005439
0.006161

0.0099
0.007614
0.008625
0.0042
0.003221
0.003637
0.0063
0.004845
0.005489
0.0076
0.005834
0.006609
0.0093
0.007120
0.008065

2
0.0172
0.006575
0.009511
0.0133
0.005092
0.007365
0.0149
0.005685
0.008223

0.0178
0.006823
0.009868
0.0085
0.003261
0.004721
0.0129
0.004944
0.007151
0.0147
0.005636
0.008152
0.0169
0.006476
0.009368

5
0.0346
0.005300
0.009192
0.0275
0.004212
0.007306
0.0314
0.004805
0.008335

0.0381
0.005834
0.010118
0.0191
0.002921
0.005062
0.0274
0.004192
0.007271
0.0311
0.004766
0.008266
0.0333
0.005102
0.008849

10
0.0558
0.004271
0.007935
0.0457
0.003500
0.006503
0.0502
0.003846
0.007145

0.0606
0.004637
0.008615
0.0313
0.002403
0.004468
0.0446
0.003411
0.006337
0.0495
0.003787
0.007035
0.0545
0.004173
0.007752

20
0.0846
0.003238
0.006238
0.0752
0.002877
0.005542
0.0789
0.003021
0.005819

0.0908
0.003475
0.006695
0.0517
0.001972
0.003804
0.0727
0.002783
0.005361
0.0781
0.002991
0.005761
0.0860
0.003293
0.006342

Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1

0.1529
0.002341
0.004612
0.1360
0.002082
0.004102
0.1387
0.002124
0.004184
0.1520
0.002327
0.004585
0.0974
0.001482
0.002925
0.1368
0.002094
0.004125
0.1395
0.002136
0.004207
0.1471
0.002252
0.004437

0.2229
0.001707
0.003387
0.2051
0.001570
0.003116
0.2049
0.001568
0.003112
0.2187
0.001674
0.003322
0.1455
0.001119
0.002218
0.2050
0.001569
0.003114
0.2076
0.001589
0.003154
0.2142
0.001639
0.003254

0.4392
0.000672
0.001343
0.4216
0.000645
0.001289
0.4176
0.000639
0.001277
0.4297
0.000658
0.001314
0.3086
0.000469
0.000937
0.4198
0.000643
0.001283
0.4210
0.000644
0.001287
0.4293
0.000657
0.001313

parameter. Then we report Recall@10 in Fig. 8a, 8b, and 8c, respec-
tively.

As shown in Fig. 8a, the Recall@10 increases constantly as α gets
larger and reaches a maximum of 0.0558 when α = 10. This suggests
that the usage of users’ social-level similarity do help to improve the
recommendation performance. However, when α is too large (α =

12), the learning algorithm of JTS-MF is misled to wrong direction
when updating latent features of users and votings, resulting in
performance deterioration. The similar phenomenon are observed
in Fig. 8b and Fig. 8c, too. According to the results, when the
other two trade-off parameters are set to 0, Recall@10 reaches
the maximum when α = 10, β = 140, and γ = 30, respectively.

CIKM’17, November 6–10, Singapore

H. Wang et al.

Therefore, in previous experiments we adopt these optimal settings
for JTS-MF(S), JTS-MF(G), and JTS-MF(V), respectively, and use
their combination as the parameter settings in JTS-MF.

7.4.2 Dimension of latent features. We fix α = 10, β = 0, γ = 0
and tune the dimension of latent features of users and votings from
5 to 90. The result is shown in Fig. 8d. From the figure, we can
see clearly that the recall is increasing when dim gets larger, this
is because latent features with larger number of dimensions have
more capacity to characterize users and votings. But a larger dim
leads to more running time in experiments. Moreover, we notice
that the improvement of performance stagnates after dim reaches
80. On balance, we set dim = 10 in our experiment scenarios to
ensure the experiments can complete within rational time duration.

8 CONCLUSIONS
In this paper, we study the problem of recommending online vot-
ings to users in social networks. We first formalize the voting
recommendation problem and justify the motivation of leveraging
social structure and voting content information. To overcome the
limitations of topic models and semantic models when learning rep-
resentation of voting content, we propose Topic-Enhanced Word
Embedding method to jointly consider topics and semantics of
words and documents. We then propose our Joint-Topic-Semantic-
aware social Matrix Factorization model to learn latent features of
users and votings based on the social network structure and TEWE
representation. We conduct extensive experiments to evaluate JTS-
MF with Weibo voting dataset. The experimental results prove the
competitiveness of JTS-MF against other state-of-the-art baselines
and demonstrate the efficacy of TEWE representation.

ACKNOWLEDGMENTS
This work was partially sponsored by the National Basic Research
973 Program of China under Grant 2015CB352403, the NSFC Key
Grant (No. 61332004), PolyU Project of Strategic Importance 1-ZE26,
and HK-PolyU Grant 1-ZVHZ.

REFERENCES
[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet

[2]

allocation. In Journal of Machine Learning Research. 993–1022.
Jes´us Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Guti´errez.
2013. Recommender systems survey. Knowledge-Based Systems 46 (2013).
[3] Marco Bressan, Stefano Leucci, Alessandro Panconesi, Prabhakar Raghavan,
and Erisa Terolli. 2016. The Limits of Popularity-Based Recommendations, and
the Role of Social Ties. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 745–754.

[4] Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency

Parser using Neural Networks.. In EMNLP. 740–750.

[5] Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander J Smola, Jing Jiang, and
Chong Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie
recommendation (jmars). In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 193–202.

[6] Huiji Gao, Jiliang Tang, Xia Hu, and Huan Liu. 2015. Content-aware point of
interest recommendation on location-based social networks.. In AAAI. 1721–
1727.

[7] Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Pro-

ceedings of National Academy of Sciences (PNAS). 5228–5235.

[8] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 855–864.

[9] Liang Hu, Jian Cao, Guandong Xu, Longbing Cao, Zhiping Gu, and Can Zhu.
2013. Personalized recommendation via cross-domain triadic factorization. In
Proceedings of the 22nd international conference on World Wide Web. ACM, 595–
606.

[10] Hui-Ju Hung, Hong-Han Shuai, De-Nian Yang, Liang-Hao Huang, Wang-Chien
Lee, Jian Pei, and Ming-Syan Chen. 2016. When Social Influence Meets Item
Inference. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 915–924.
Janet Ilieva, Steve Baron, and Nigel M Healey. 2002. Online surveys in marketing
research: Pros and cons. International Journal of Market Research 44, 3 (2002),
361.

[11]

[12] Meng Jiang, Peng Cui, Xumin Chen, Fei Wang, Wenwu Zhu, and Shiqiang Yang.
2015. Social recommendation with cross-domain transferable knowledge. IEEE
Transactions on Knowledge and Data Engineering 27, 11 (2015), 3084–3097.
[13] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-

niques for recommender systems. Computer 42, 8 (2009).

[14] Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Proceedings of the

12th international conference on machine learning. 331–339.

[15] Quoc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences

and Documents.. In ICML, Vol. 14. 1188–1196.

[16] Wu-Jun Li, Dit-Yan Yeung, and Zhihua Zhang. 2011. Generalized latent factor
models for social network analysis. In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI), Barcelona, Spain. 1705.

[17] Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis.

In Mining text data. Springer, 415–463.

[18] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Sun Maosong. 2015. Topical word em-
beddings. In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence
(AAAI). AAAI, 2418–2424.

[19] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social
recommendation using probabilistic matrix factorization. In Proceedings of the
17th ACM conference on Information and knowledge management. ACM, 931–940.
[20] Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007.
Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings
of the 16th international conference on World Wide Web. ACM, 171–180.
[21] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Proceedings of the 26th International Conference on Neural Information Processing
Systems (NIPS). 3111–3119.

[22] Steffen Rendle and Christoph Freudenthaler. 2014. Improving pairwise learning
for item recommendation from implicit feedback. In Proceedings of the 7th ACM
international conference on Web search and data mining. ACM, 273–282.
[23] Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction

by locally linear embedding. science 290, 5500 (2000), 2323–2326.

[24] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltz-
mann machines for collaborative filtering. In Proceedings of the 24th international
conference on Machine learning. ACM, 791–798.

[25] Yue Shi, Alexandros Karatzoglou, Linas Baltrunas, Martha Larson, Nuria Oliver,
and Alan Hanjalic. 2012. CLiMF: learning to maximize reciprocal rank with
collaborative less-is-more filtering. In Proceedings of the sixth ACM conference on
Recommender systems. ACM, 139–146.

[26] Shaojie Tang and Jing Yuan. 2016. Optimizing Ad Allocation in Social Advertising.
In Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management. ACM, 1383–1392.

[27] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning
for recommender systems. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 1235–1244.
[28] Xin Wang, Roger Donaldson, Christopher Nell, Peter Gorniak, Martin Ester, and
Jiajun Bu. 2016. Recommending groups to users using user-group engagement
and time-dependent matrix factorization. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence. AAAI Press, 1331–1337.

[29] Xin Wang, Wei Lu, Martin Ester, Can Wang, and Chun Chen. 2016. Social
Recommendation with Strong and Weak Ties. In Proceedings of the 25th ACM
CIKM. ACM, 5–14.

[30] Zhonghang Xia, Yulin Dong, and Guangming Xing. 2006. Support vector ma-
chines for collaborative filtering. In Proceedings of the 44th annual Southeast
regional conference. ACM, 169–174.

[31] Xiwang Yang, Chao Liang, Miao Zhao, Hongwei Wang, Hao Ding, Yong Liu,
Yang Li, and Junlin Zhang. 2017. Collaborative filtering-based recommendation
of online social voting. IEEE Transactions on Computational Social Systems 4, 1
(2017), 1–13.

[32] Qin Zhang, Jia Wu, Peng Zhang, Guodong Long, Ivor W Tsang, and Chengqi
Zhang. 2016. Inferring Latent Network from Cascade Data for Dynamic Social
Recommendation. In Data Mining (ICDM), 2016 IEEE 16th International Conference
on. IEEE, 669–678.

[33] Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010.

Jointly
modeling aspects and opinions with a MaxEnt-LDA hybrid. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 56–65.

[34] Hengshu Zhu, Enhong Chen, Kuifei Yu, Huanhuan Cao, Hui Xiong, and Jilei
Tian. 2012. Mining personal context-aware preferences for mobile users. In 2012
IEEE 12th International Conference on Data Mining. IEEE, 1212–1217.

Joint Topic-Semantic-aware Social Recommendation
for Online Voting

Hongwei Wang1,2, Jia Wang2, Miao Zhao2, Jiannong Cao2, Minyi Guo1∗
1Shanghai Jiao Tong University, 2The Hong Kong Polytechnic University
wanghongwei55@gmail.com,{csjiawang,csmiaozhao,csjcao}@comp.polyu.edu.hk,guo-my@cs.sjtu.edu.cn

7
1
0
2
 
c
e
D
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
7
0
0
.
2
1
7
1
:
v
i
X
r
a

ABSTRACT
Online voting is an emerging feature in social networks, in which
users can express their attitudes toward various issues and show
their unique interest. Online voting imposes new challenges on rec-
ommendation, because the propagation of votings heavily depends
on the structure of social networks as well as the content of vot-
ings. In this paper, we investigate how to utilize these two factors
in a comprehensive manner when doing voting recommendation.
First, due to the fact that existing text mining methods such as
topic model and semantic model cannot well process the content of
votings that is typically short and ambiguous, we propose a novel
Topic-Enhanced Word Embedding (TEWE) method to learn word
and document representation by jointly considering their topics
and semantics. Then we propose our Joint Topic-Semantic-aware
social Matrix Factorization (JTS-MF) model for voting recommenda-
tion. JTS-MF model calculates similarity among users and votings
by combining their TEWE representation and structural informa-
tion of social networks, and preserves this topic-semantic-social
similarity during matrix factorization. To evaluate the performance
of TEWE representation and JTS-MF model, we conduct extensive
experiments on real online voting dataset. The results prove the
efficacy of our approach against several state-of-the-art baselines.

KEYWORDS
Online voting; recommender systems; topic-enhanced word em-
bedding; matrix factorization

1 INTRODUCTION
Online voting [31] has recently become a popular function on so-
cial platforms, through which a user can share his opinion towards
various interested subjects, ranging from livelihood issues to en-
tertainment news. More advanced than simple like-dislike type of
votings, some social networks, such as Weibo1, have empowered
users to run their own voting campaigns. Users can freely initiate
votings on any topics of their own interests and customize voting
options. These votings are visible to the friends of initiator, who
can then choose to participate to make the votings further seen by
their friends or simply retweet the votings to their friends. In such
a way, in addition to the system recommendation, a voting can
widely propagate over the network along social paths. The voting
propagation scheme is shown in Fig. 1.

∗M. Guo is the corresponding author.
1http://www.weibo.com.

CIKM’17, Singapore
2017. 978-1-4503-4918-5/17/11. . . $15.00
DOI: 10.1145/3132847.3132889

Fig. 1: Propagation scheme of online voting.

Facing a large volume of diversified votings, a critical challenge
is to present “right” votings to the “right” person. An effective
recommender system is desired to be able to deal with information
overload [2] by precisely locating what votings favor each user most,
thus improves user experience and maximizes user engagement in
votings. Such a recommender system can also benefit a variety of
other online services such as personalized advertising [26], market
research [11], public opinion analysis [17], etc.

Despite the great importance, there is little prior work consider-
ing recommending votings to users in social networks. The chal-
lenges are two-fold. First, the propagation of online votings relies
heavily on the structure of social networks. A user can see the
votings initiated, participated or retweeted by his followees, which
implies that the user is more likely to be exposed to the votings
that his friends are involved in. Moreover, in most social networks,
a user can join different interest groups, which is another type
of social structure that potentially affects users’ voting behavior.
Though several prior works [3, 6, 10, 12, 25, 29, 31, 32] have been
proposed to leverage social network information in recommenda-
tion, it is still an open question how to comprehensively incorporate
structural social information into the task of voting recommenda-
tion considering its propagation pattern. Second, users’ interest
in votings is strongly connected with voting content presented in
question text (e.g., “Who is your favorite movie star?”). Topic model
[1] is regarded as a possible approach to mine the voting interests
through discovering the latent topic distribution of relevant voting
text. However, the voting questions are typically short and lack
sufficient topic information, leading to severe performance degra-
dation of topic models. Alternatively, semantic analytics [21] can
also possibly be used to mine voting interests through learning text
representations. However, such semantic models typically repre-
sent each word using a single vector, making them indiscriminative
for homonymy and polysemy, which are especially common in
voting questions (e.g., “Do you use apple products?” and “Do you
peel apple before eating?”). In brief, these inherent defects of the

CIKM’17, November 6–10, Singapore

H. Wang et al.

above models limit their power in the scenario of social voting
recommendation.

To address aforementioned challenges, in this paper, we propose
a novel Joint Topic-Semantic-aware Matrix Factorization (JTS-MF)
model for online voting recommendation. JTS-MF model considers
social network structure and representation of voting content in
a comprehensive manner. For social network structure, JTS-MF
model fully encodes the information of social relationship and group
affiliation into the objective function. We will further justify the
usage of social network structure in Section 3. For representation
of voting content, we propose a Topic-Enhanced Word Embedding
(TEWE) method to build a multi-prototype word and document2
representation, which jointly considers their topics and semantics.
The key idea of TEWE is to enable each word to have different rep-
resentations under different word topics and different documents.
We will detail TEWE in Section 5. Once obtaining TEWE repre-
sentation for each document, JTS-MF model combines them with
the structural information of social networks to calculate the topic-
semantic-social similarity among users and votings. The reason
of calculating such similarity is that, inspired by Locally Linear
Embedding [23], we try to preserve the similarity among users
and votings during matrix factorization, as it contains abundant
proximity information and can greatly benefit feature learning for
users and votings. JTS-MF model is detailed in Section 6.

We conduct extensive investigation on JTS-MF with real online
voting dataset. The experimental results in Section 7 demonstrate
that JTS-MF model achieves substantial gains compared with base-
lines. The results also prove that TEWE is able to well combine
topic and semantic information of texts and generates a better kind
of document representation.

In summary, the contributions of this paper are as follows:

• We formally formulate the online voting recommendation

problem, which has not been fully investigated yet.

• We indicate that user’s voting behavior is highly correlated
with social network structure by conducting thorough sta-
tistical measurements.

• We propose a novel Topic-Enhanced Word Embedding
model to jointly consider topics and semantics of words
and documents to learn their representation. TEWE takes
advantages of both topic model and semantic model, and
consequently learns more informative embeddings.

• We develop a novel matrix factorization based models,
named JTS-MF, for online voting recommendation. JTS-
MF is able to preserve the topic-semantic-social similarity
among users and votings from original embedding space
during learning process.

• We carry out extensive experiments on real online voting
dataset, the results of which reveal that JTS-MF signif-
icantly outperforms baseline (variant) methods, say for
example, surpassing basic matrix factorization model with
57%, 38% and 25% enhancement in terms of recall for top-1,
top-5 and top-20 recommendation, respectively.

2In this paper, a “document” can be related to a voting, a user or a group. A voting
document is the content of its question, a user document is formed by aggregating
all the documents of votings he participates, and a group document is formed by
aggregating all the documents of users who join the group.

2 RELATED WORK
2.1 Recommender Systems
Roughly speaking, existing recommender systems can be catego-
rized into three classes [2]: content-based, collaborative filtering,
and hybrid methods. Content-based methods [14, 34] make use of
user profiles or item descriptions as features for recommendation.
Collaborative filtering methods [22, 25, 28, 31] use either explicit
feedback (e.g., users’ ratings on items) or implicit feedback (e.g.,
users’ browsing records about items) data of user-item interactions
to find user preference and make the recommendation. In addition,
various models are incorporated into collaborative filtering, such
as Support Vector Machine [30], Restricted Boltzmann Machine
[24], and Stacked Denoising Auto Encoder [27]. Hybrid methods
[9, 16] combine content-based and collaborative filtering models
in many hybridization approaches, such as weighted, switching,
cascade and feature combination or augmentation.

2.2 Social Recommendation
Traditional recommender systems are vulnerable to data sparsity
problem and cold-start problem. To mitigate this issue, many ap-
proaches have been proposed to utilize social network information
in recommender systems [3, 6, 10, 12, 25, 29, 31, 32]. For exam-
ple, [12] represents a social network as a star-structured hybrid
graph centered on a social domain which connects with other item
domains to help improve the prediction accuracy. [10] investi-
gates the seed selection problem for viral marketing that considers
both effects of social influence and item inference for product rec-
ommendation. [29] studies the effects of strong and weak ties in
social recommendation, and extends Bayesian Personalized Rank-
ing model to incorporate the distinction of strong and weak ties.
However, the above works only utilize users’ social links without
considering the topic and semantic information for mining the
similarities among users and items, which we found quite help-
ful for voting recommendation tasks. Another difference between
these works and ours is that we also take social group affiliation
into consideration, which can further improve the performance of
recommendation.

2.3 Topic and Semantic Language Models
Latent Dirichlet Allocation (LDA) [1] is a well-known generative
topic model that learns the latent topic distributions for documents.
LDA is widely used in sentiment analysis [20], aspects and opin-
ions mining [33], and recommendation [5]. Word2vec [21] is gen-
erally recognized as a neural network model, which learns word
representations that capture precise syntactic and semantic word
relationships. Word2vec as well as associated Skip-Gram model are
extensively used in document classification [15], dependency parser
[4], and network embedding [8]. However, LDA and Word2vec are
not directly applicable in the scenario of voting recommendation
because the content of voting is usually short and ambiguous. As
a combination, [18] tries to learn topical word embeddings based
on both words and their topics. The difference between [18] and
ours is that we also take topics of documents into consideration,
which enables our model to learn a even more discriminative and
informative representations for words and documents.

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

Table 1: Basic Statistics of Weibo Dataset.

# users
# users with votings
# users with groups
# votings

1,011,389
525,589
723,913
185,387

# groups
# user-voting
# user-user
# user-group

299,077
3,908,024
83,636,677
5,643,534

3 BACKGROUND AND DATA ANALYSIS
In this section, we briefly introduce the background of Weibo voting
and present detailed analysis of Weibo voting dataset.

3.1 Background
Weibo is one of the most popular Chinese microblogging website
launched by Sina corporation, which is akin to a hybrid of Twitter
and Facebook platforms. Users on Weibo can follow each other,
write tweets and share with his followers. Users can also join
different groups based on their attributes (e.g., region) or interests
of topics (e.g., career).

Voting3 is one of the embedded features of Weibo. As of January
2013, more than 92 million users have participated in at least one
voting and more than 2.2 million ongoing votings are available on
Weibo every day. Any user can freely initiate, retweet and partici-
pate a voting campaign in Weibo. As shown in Fig. 1, votings can
propagate in two ways. The first way is through social propagation:
a user can see the voting initiated, retweeted or participated by his
followees and potentially participates the voting. The second way
is through Weibo voting recommendation list, which consists of
popular votings and personalized recommendation for each user.

3.2 Data Measurements
Our Weibo voting dataset comes from the technical team of Sina
Weibo, which contains detailed information about votings from No-
vember 2010 to January 2012, as well as other auxiliary information.
Specifically, the dataset includes users’ participation status on each
voting4, content of each voting, social connection between users,
name and category of each group, and user-group affiliation.

3.2.1 Basic statistics. The basic statistics are summarized in
Table 1. From Table 1 we can learn that, each user has 165.4 fol-
lowers/followees, participates 3.9 votings, and joins 5.6 groups on
average. If we only count users who participate at least one vot-
ing and users who join at least on group, the average number of
votings and average number of joined groups of each user is 7.4
and 7.8, respectively. Fig. 2 depicts the distribution curves of the
above statistics, where the meaning of each subfigure is given in
the caption.

To get an intuitive understanding of whether user’s voting be-
havior is correlated with his social relation and group affiliation,
we conduct the following two sets of statistical experiments:

3.2.2 Correlation between the number of common votings of user
pairs and the types of user pairs. We randomly select ten million
user pairs from the set of all users, and count the average number
of votings that the two users both participate under the following
four circumstances: 1) one of the users follows the other in the
pair, i.e., they are social-level friends; 2) the two users are in at

3http://www.weibo.com/vote?is all=1.
4We only know whether a user participated a voting or not, rather than user voting
results, i.e., we do not know which voting option a user chose.

(a)

(c)

(e)

(b)

(d)

(f)

Fig. 2: (a) Distribution of the number of votings participated
by a user; (b) Distribution of the number of participants of a
voting; (c) Distribution of the number of followers/followees
of a user; (d) Distribution of the number of users in a group;
(e) Distribution of the number of votings (may contain du-
plicated votings) participated by all users in a group; (f) Dis-
tribution of the number of groups joined by a user.

least one common group, i.e., they are group-leven friends; 3) the
two users are neither social-level friends nor group-level friends;
4) all cases. The results are plotted in Fig. 3a, which clearly shows
the difference among these cases. In fact, the average number of
common votings of social-level friends (3.54 × 10−4) and group
level friends (1.79 × 10−4) are 17.4 and 8.8 times higher than that of
“strangers” (2.04 × 10−5). The results demonstrate that if two users
are social-level or group-level friends, they are likely to participate
more votings in common.

3.2.3 Correlation between the probability of two users being
friends and whether they participate common voting. We first ran-
domly select ten thousand votings from the set of all votings. For
each sampled voting vj , we calculate the probability that two of
its participants are social-level or group-level friends, i.e., pj =
# of social/group-level friends among participants of vj
, where nj is the num-
nj ×(nj −1)/2
ber of vj ’ participants. We calculate pj over all sampled votings
and plot the average result (blue bar) in Fig. 3b. For comparison, we
also plot the result for randomly sampled set of users (green bar)
in Fig. 3b. It is clear that if two users ever participated common
voting, they are more likely to be social-level or group-level friends.
In fact, probabilities of two users being social-level or group-level
friends are raised by 5.3 and 3.6 times given the observation that
they are with common voting.

CIKM’17, November 6–10, Singapore

H. Wang et al.

(a)

(b)

Fig. 3: (a) Average number of common votings participated
by user ui and uk in four cases: 1. ui follows/is followed by
uk ; 2. ui and uk are in at least one common group; 3. ui and
uk have no social-level and group-level relationship; 4. all
cases; (b) Probability of two users being social-level or group-
level friends in two cases: 1. they ever participated at least
one common voting; 2. they are randomly sampled.

The above two findings effectively prove the strong correlation
between voting behavior and social network structure, which mo-
tivates us to take users’ social relation and group affiliation into
consideration when making voting recommendation.

4 PROBLEM FORMULATION
In this paper, we consider the problem of recommending Weibo
votings to users. We denote the set of all users, the set of all votings,
and the set of all groups by U = {u1, ..., uN }, V = {v1, ..., vM },
and G = {G1, ..., GL }, respectively. Moreover, we model three
types of relationship in Weibo platform: user-voting, user-user, and
user-group relationship as follows:

(1) The user-voting relationship for ui and vj is defined as

=

(1)

Iui,vj

i f ui participates vj ;
otherwise.

(cid:40)1,
0,
(2) The user-user relationship for ui and uk
(cid:40)1,
;
0,
to denote the set of ui ’s followees, and
to denote the set of ui ’s followers (“+” means “out”

i f ui f ollows uk
otherwise.

We further use F +
i
use F −
i
and “−” means “in”).

is defined as

Iui,uk

(2)

=

(3) The user-group relationship for ui and Gc is defined as

Iui,Gc

=

(cid:40)1,
0,

i f ui joins Gc ;
otherwise.

(3)

Given the above sets of users and votings as well as three types
of relationship, we aim to recommend a list of votings for each user,
in which the votings are not participated by the user but may be
interesting to him.

5 JOINT-TOPIC-SEMANTIC EMBEDDING
In this section, we explain how to learn the embeddings of users,
votings, and groups in a joint topic and semantic way, and apply the
embeddings to calculate similarities. We first introduce the methods
of learning topic information and semantic information by LDA and
Skip-Gram models, respectively, and propose our method which
combines these two models to learn more powerful embeddings.

5.1 Topic Distillation
In this subsection, we introduce how to profile users, votings, and
groups in terms of topic interest distribution by performing topic
distillation on the associated textual content information.

In general, LDA is a popular generative model to discover latent
topic information from a collection of documents [1]. In LDA, each
document d is represented as a multinomial distribution Θ
over a
d
set of topics, and each topic z is also represented as a multinomial
distribution Φz over a number of words. Subsequently, each word
position l in document d is assigned a topic zd,l
,
according to Θ
d
and the word wd,l
. By LDA
approach, the topic distribution for each document and the topic
assignment for each word can be obtained, which would be utilized
later in our proposed model.

is finally generated according to Φzd,l

’s and dvj

= ∪{dui |Iui,Gc

5. The document dui

= 1}, and the document dGc

Here, we discuss how to apply LDA in the scenario of Weibo
voting. According to the Weibo voting dataset, each voting vj
associates a sentence of question, which can be regarded as doc-
for user ui can thus be formed by
ument dvj
aggregating the content of all votings he participates, i.e., dui
=
for group Gc is formed by
∪{dvj |Iui,vj
aggregating documents of all its members, i.e., dGc
=
1}. Note that though our target is to learn the topic distributions of
all users, votings, and groups, it is inadvisable to train LDA model
on dui
’s because: (1) the entitled sentence associated with
a single voting is typically short-presented and topic-ambiguous;
(2) even with user-level voting content aggregation, some docu-
ments of inactive users are not long enough to accurately extract
the authentic topic distribution, yet showing relatively flat distribu-
tion over all the topics. Therefore, we choose to feed group-level
aggregated documents dGc
’s to LDA model as training samples.
The process of group-level voting content aggregation will cover
all the content the affiliated users are interested in and help better
identify their interests in terms of voting topic.
Denote Dir(α ) as the Dirichlet prior of Θ
d

, and Dir(β) as the
Dirichlet prior of Φz . Given α and β, the joint distribution of
document-topic distributions Θ, topic-word distributions Φ, topics
of words z, and a set of words w is

p(Θ, Φ, z, w |α , β)
z p(Φz |β) · (cid:206)
= (cid:206)
d

(cid:18)
p(Θ

d |α ) (cid:206)
l

(cid:16)
p(zd,l |Θ

d )p(wd,l |Φzd,l )

(cid:17) (cid:19)

,

(4)
where d traverses all group-level aggregated documents. In general,
it is computationally intractable to directly maximize the joint
likelihood in Eq. (4), thus Gibbs Sampling [7] is usually applied
to estimate the posterior probability p(z|w, α , β) and solve Θ, Φ
(w )
the
iteratively. Denote θ
the z-th component of Θ
d
z
w-th component of Φz . With the sampling results, Θ
and Φz can
d
be estimated as:

, and ϕ

(z)
d

θ

(z)
d
(w )
ϕ
z

(z)
= (cid:0)n
d
(w )
= (cid:0)n
z

+ α (z)(cid:1)/(cid:0) (cid:205)
z (n
+ β (w )(cid:1)/(cid:0) (cid:205)

+ α (z))(cid:1),
(w )
z
where α (z) is the z-th component of α , β (w ) is the w-th component
(w )
of β, n
z

z = 1, ..., Z ,
+ β (w ))(cid:1), w = 1, ..., V ,

is the observation counts of topic z for document d, n

(z)
d
w (n

(5)

(z)
d

is segmented by Jieba (https://github.com/fxsjy/jieba) and all stop words are

5dvj
removed.

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

L(D) =

log p(wt +c |wt ),

(6)

L(D) =

log p((cid:104)wt +c , zt +c , zd

t +c (cid:105)|(cid:104)wt , zt , zd

t (cid:105)),

(8)

is the frequency of word w assigned as topic z, Z is the number of
topics and V is vocabulary size.

So far, we have obtained the topic assignment for each word and
topic distribution for each group. Topic distributions for users and
votings can thus be inferred by using the learned model and Gibbs
Sampling, which is similar to the calculation of θ

in Eq. (5).

(z)
d

5.2 Semantic Distillation
In this subsection, we introduce how to profile users, votings, and
groups in terms of semantic information. Word embedding, which
represents each word using a vector, is widely used to capture
semantic information of words. Skip-Gram model is a well-known
framework for word embedding, which finds word representation
that are useful for predicting surrounding words in a document
given a target word in a sliding window [21]. More formally, given
a word sequence D = {w1, w2, . . . , wT }, the objective function of
Skip-Gram is to maximize the average log probability

1

T

T
(cid:213)

t =1

(cid:213)

−k ≤c ≤k
c(cid:44)0

where k is the training context size of the target word, which
can be a function of the centered work wt . The basic Skip-Gram
formulation defines p(wi |wt ) using the softmax function as follows:

p(wi |wt ) =

(cid:205)

exp(w(cid:62)

i wt )
exp(w(cid:62)wt )

,

w ∈V
where wi and wt are the vector representation of context word wi
and target word wt , respectively, and V is the vocabulary. To avoid
traversing the entire vocabulary, hierarchical softmax or negative
sampling are used in general during learning process [21].

(7)

5.3 Topic-Enhanced Word Embedding
In this subsection, we propose a joint topic and semantic learning
model, named Topic-Enhanced Word Embedding (TEWE), to ana-
lyze documents of users, votings, and groups. The motivation of
proposed TEWE is based on the following two observations: (1)
The voting content typically involves short texts. Even we infer
the topic distribution for each voting based on the learned topic-
word distribution from group-level aggregated documents, it is
still topic-ambiguous to some extent. (2) The Skip-Gram model
for word embedding assumes that each word always preserves a
single vector, which sometimes is indiscriminate under different
circumstances due to the homonymy and polysemy. Therefore, the
basic idea of TEWE is to preserve topic information of documents
and words when measuring the interaction between target word wt
and context word wi . In this way, a word with different associated
topics has different embeddings, and a word in documents with
different topics has different embeddings, too.

Specifically, rather than solely using the target word w to predict
context words in Skip-Gram, TEWE also jointly utilizes zw , the
topic of the word in a document, as well as zd
, the most likely
w
topic of the document that the word belongs to. Recall that in Sec-
tion 5.1, we have obtained the topic of each word zw and topic
can be calculated
distributions of each document Θ
d

, thus zd
w

(a) Skip-Gram

(b) TEWE

Fig. 4: Comparison between Skip-Gram and TEWE. The
gray circles in (a) indicate the embeddings of original words,
while the blue circles in (b) indicate the TEWE representa-
tion of pseudo words, which preserves semantic and topic
information of words and documents.

(z)
d

, where θ

= arg maxz θ

(z)
is the probability that docu-
as zd
w
d
ment d belongs to topic z, as introduced in Eq. (5). TEWE regards
each word-topics triplet (cid:104)w, zw , zd
w (cid:105) as a pseudo word and learns a
unique vector wz,zd for it. The objective function of TEWE is as
follows:

(cid:213)

1

T

T
(cid:213)

t =1

−k ≤c ≤k
c(cid:44)0
(cid:105)|(cid:104)wt , zt , zd

(cid:17) .

(cid:17)

t (cid:105)) =

wz,zd
i

p((cid:104)wi , zi , zd

i (cid:105)|(cid:104)wt , zt , zd

where p((cid:104)wi , zi , zd
i

t (cid:105)) is a softmax function as
exp (cid:16)
(cid:62)wz,zd
t
exp (cid:16)
wz,zd (cid:62)wz,zd
t
(9)
The comparison between Skip-Gram and TEWE is shown in Fig.
4. Instead of solely utilizing the target and context words as in Skip-
Gram, TEWE further preserves word topic and document topic
along with these words, and incorporates both topic and semantic
information in embedding learning.

(cid:104)w,z,zd (cid:105) ∈ (cid:104)V ,Z,Z (cid:105)

(cid:205)

Once obtaining TEWE representation for each pseudo word, the
representation of each document can be correspondingly derived
by aggregating the embeddings of its containing words weighted
by term frequency-inverse document frequency (TF-IDF) coefficient.
Specifically, for each document d, its TEWE can be calculated as

= (cid:205)

ed

w ∈d

TF-IDF(w, d) · wz,zd

,

(10)

where TF-IDF(w, d) is the product of the raw count of w in d and
the logarithmically scaled inverse fraction of the documents that
contains w, i.e., TF-IDF(w, d) = fw,d · log
(D is the set
of all documents). TEWE document representations can be used in
measuring inter-document similarities. For example, the similarity
can be calculated as the cosine
of two user documents dui
similarity between their TEWE representations, i.e.,

|D |
|d ∈D:w ∈d |

(cid:107)2
This similarity encodes both topic and semantic proximity informa-
tion of user documents, which implicitly reveals the similarity of
voting interests between two users.

e(cid:62)
euk
ui
(cid:107)eui (cid:107)2 (cid:107)euk

and duk

.

6 RECOMMENDATION MODEL
In this section, we present our Joint Topic-Semantic-aware Matrix
Factorization (JTS-MF) model for online social votings, in which
social relationship, group affiliation, and topic-semantic similarities
are combined and taken into account for voting recommendation in

CIKM’17, November 6–10, Singapore

H. Wang et al.

between two users from both topic-semantic interests and their
social influence perspectives.

To avoid the impact of different numbers of followees, we use
the normalized social-level similarity coefficient of users in JTS-MF,
which is defined as

,

=

(cid:205)

(12)

(cid:98)Si,k

Si,k

where F +
i

Si,k
k ∈ F+
i
denotes the set of ui ’s followees in social network.
6.1.2 Normalized group-level similarity coefficient of users. Group-
level similarity coefficient of users is represented by matrix G N ×N ,
which actually measures the topic-semantic similarity among users
from viewpoint of groups. For each ui , the group-level similarity
coefficient with respect to uk

is defined as

= (cid:213)

Gi,k

Iui,G · Iuk,G ·

G ∈ G

e(cid:62)
ui eG
(cid:107)eui (cid:107)2 (cid:107)eG (cid:107)2

,

(13)

where G represents the set of all groups, Iui,G and Iuk,G indicate
whether ui and uk
join group G respectively as described in Eq.
(3), and the last term is the topic-semantic similarity between user
reflects the interest
ui and group G. Essentially speaking, Gi,k
closeness between user ui and its group-level friend uk
by using
ui ’s topic-semantic engagement extent to the corresponding group.
We also normalize the group-level similarity coefficient of users as

where Gi is the set of ui ’s group-level friends in social network.

6.1.3 Normalized similarity coefficient of votings. Similarity coef-
ficient of votings is represented by matrix T M ×M , which is directly
defined as the topic-semantic similarity among votings, i.e.,

(cid:98)Gi,k

=

Gi,k
k ∈ Gi Gi,k

,

(cid:205)

Tj,t =

e(cid:62)
vj evt
(cid:107)evj (cid:107)2 (cid:107)evt (cid:107)2

.

Since the number of votings is typically huge, we only consider
the similarity between two votings with sufficiently high coefficient
value. Specifically, for each voting vj , we define a set of votings Vj
containing those votings whose similarity coefficients with vj ex-
ceed a threshold, i.e., Vj = {vt |Tj,t ≥ threshold}. Correspondingly,
the similarity coefficient of votings are normalized as

(cid:98)Tj,t =

Tj,t
t ∈Vj Tj,t

.

(cid:205)

6.2 Objective Function
Using the notations listed above, the objective function of JTS-MF
can be written as

L =

1
2

N
(cid:213)

M
(cid:213)

i =1

j=1

(cid:16)

I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17) 2

+ α
2

N
(cid:213)

i =1

(cid:13)
(cid:13)Qi −

(cid:213)

k ∈F+
i

(cid:98)Si,k Qk

(cid:13)
(cid:13)

2
2

+ β
2

N
(cid:213)

i =1

(cid:13)
(cid:13)Qi −

(cid:213)

k ∈Gi

(cid:98)Gi, k Qk

(cid:13)
(cid:13)

2

2 + γ
2

M
(cid:213)

j=1

(cid:13)
(cid:13)Pj −

(cid:213)

t ∈Vj

(cid:98)Tj, t Pt

(cid:16)

(cid:13)
(cid:13)

2

2 + λ
2

(cid:107)Q (cid:107)2
F

+ (cid:107)P (cid:107)2
F

(cid:17)

.

The basic idea of the objective function in Eq. (17) lies in that,
besides considering explicit feedback between users and votings,
we also impose penalties on the discrepancy among features of

(14)

(15)

(16)

(17)

Fig. 5: Graphic Model of JTS-MF.

a comprehensive manner. Motivated by Locally Linear Embedding
[23] which tries to preserve the local linear dependency among
inputs in the low-dimensional embedding space, we expect to keep
inter-user and inter-voting topic-semantic similarities in latent
feature space as well. To this end, in JTS-MF model, while the
rating Ri, j is factorized as user latent feature Qi and voting latent
feature Pj, we deliberately enforce Qi and Pj to be dependent on
their social-topic-semantic similar counterparts, respectively. The
graphic model of JTS-MF model is as shown in Figure 5.

6.1 Similarity Coefficients
In order to characterize the influence of inter-user common interests
and inter-voting content relevance, we first introduce the following
three similarity coefficients:

• Normalized social-level similarity coefficient of users: (cid:98)Si,k

,

where uk

is the social-level friend of ui ;

• Normalized group-level similarity coefficient of users: (cid:98)Gi,k

,

where uk

is the group-level friend of ui ;

• Normalized similarity coefficient of voting: (cid:98)Tj,t , where vj

and vt are two distinct votings.

Generally speaking, in JTS-MF, the latent feature Qi for user ui
is tied up with the latent feature of his social-level and group-level
friends who are weighted through (cid:98)Si,k
’s. Likewise, the
latent feature Pj for voting vj is tied up with the latent feature of
its similar votings, which are weighted through (cid:98)Tj,t ’s.

’s and (cid:98)Gi,k

·

Si,k

is defined as

= Iui,uk ·

e(cid:62)
ui euk
(cid:107)eui (cid:107)2 (cid:107)euk (cid:107)2

6.1.1 Normalized social-level similarity coefficient of users. Social-
level similarity coefficient of users is represented by matrix S N ×N ,
which incorporates both social relationship and user-user topic-
semantic similarity. Specifically, for each ui , the social-level simi-
larity coefficient with respect to uk
(cid:115) d−
+ d
k
d+
+ d−
i
k
indicates whether ui follows uk
is the out-degree of ui in the social network (i.e., d
in the social network (i.e., d−
is the in-degree of uk
k

as described in Eq. (2),
where Iui,uk
= |F +
+
+
|),
d
i
i
i
= |F −
d−
|), d
k
k
e(cid:62)
euk
is the smoothing constant (d = 1 in this paper), and
ui
(cid:107)eui (cid:107)2 (cid:107)euk
is the topic-semantic similarity between user ui and user uk
mentioned in Section 5.3.

+d
k
+d −
k
of local authority and local hub value to differentiate the impor-
counts the closeness
tance of different users [19]. Essentially, Si,k

incorporates the information

(cid:114) d −
d +
i

+ d

(cid:107)2
as

(11)

+d

,

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

similar users and similar votings. We give detailed explanation as
follows. The first term of Eq. (17) measures the mean squared error
between prediction and ground truth, where I (cid:48)
is the training
i, j
weights defined as

I (cid:48)
i, j

=

(cid:40)1,
Im,

i f ui participates vj
otherwise

.

(18)

defined in Eq. (1) as the
The reason we do not directly use Iui,vj
training weights is because we found a small and positive Im makes
the training process more robust and can greatly improve the results.
Ri, j is the actual rating of user ui on voting vj , and Qi P (cid:62)
is the
j
predicted value of Ri, j . Without loss of generality, in JTS-MF model,
we set Ri, j = 1 if ui participates vj and Ri, j = 0 otherwise.

’s. Weight (cid:98)Si,k

’s address both the followee uk

The second, third, and fourth terms of Eq. (17) measure the
penalty of discrepancy among similar users and similar votings. In
particular, the second term enforces user ui ’s latent feature Qi to
be similar to the weighted average of his like-minded followees’
profiles Qk
’s social
influence on ui as well as the degree of common voting interests
shared between uk
and ui . The third term enables user ui ’s latent
feature Qi to be similar to the weighted average of all his group
’s emphasize both the same group
peers’ profiles Qk
affiliation of users ui and uk
and also the tie strength between ui and
the associated group with respect to voting interests. This implies
that, among all group-level friends, ui would have more similar
latent feature with the users who frequently join those groups ui is
interested in. The fourth term ensures voting vj ’s latent feature Pj
to be similar to the weighted average of votings that share similar
topic-semantic information with vj .

’s. Weight (cid:98)Gi,k

Finally, the last term of Eq. (17) is the regularizer to prevent

over-fitting, and λ is the regularization weight.

The trade-off among user social-level similarities, user group-
level similarities, and voting similarities is controlled by the pa-
rameters α, β, and γ , respectively. Obviously, users’ social-level
similarity, users’ group-level similarity, or votings’ similarity is/are
ignored if α, β, or γ is/are set to 0, while increasing these values
shifts the trade-off more towards their respective directions.

6.3 Learning Algorithm
To solve the optimization in Eq. (17), we apply batch gradient
descent approach to minimize the objective function6. The gradients
of loss function in Eq. (17) with respect to each variable Qi and Pj
are as follows:

(cid:16)

−I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17)

Pj

∂L
∂Qi

=

M
(cid:213)

j =1
(cid:18)

(cid:213)

k ∈F+
i
(cid:213)

k ∈Gi

(cid:1) + (cid:213)
t ∈F−
i

(cid:1) + (cid:213)
t ∈U

+ α

(cid:0)Qi −

(cid:98)Si,k Qk

−(cid:98)St,i

(cid:0)Qt −

(cid:98)St,k Qk

(cid:18)

+ β

(cid:0)Qi −

(cid:98)Gi,k Qk

− (cid:98)Gt,i

(cid:0)Qt −

(cid:98)Gt,k Qk

+ λQi,

(cid:213)

k ∈F+
t
(cid:213)

k ∈Gi

(cid:19)

(cid:1)

(cid:19)

(cid:1)

(19)

(cid:16)

−I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17)

Qi

∂L
∂Pj

=

N
(cid:213)

i =1
(cid:18)

+ γ

(cid:0)Pj −

(cid:98)Tj, t Pt

−(cid:98)Tk, j

(cid:0)Pk −

(cid:213)

t ∈Vj

(cid:1) + (cid:213)
k ∈Vj

(cid:213)

t ∈Vk

(cid:19)

(cid:1)

(cid:98)Tk, t Pt

+ λPj .

(20)

To clearly understand the gradients in Eq. (19) and (20), it is
worth pointing out that Qi appears not only in the i-th sub-term
in the second and third lines of Eq. (17) explicitly, but also exists
in other t-th sub-terms followed by (cid:98)St,i or (cid:98)Gt,i , where ui plays as
one of the followees or group members of other users. The case is
similar for Pj. Given the gradients in Eq. (19) and (20), we list the
pseudo code of the learning algorithm for JTS-MF as follows:

(1) Randomly initialize Q and P;
(2) In each iteration of the algorithm, do:
a) update each Qi : Qi ← Qi − δ ∂L
;
∂Qi
b) update each Pj: Pj ← Pj − δ ∂L
;
∂Pj
until convergence, where δ is an configurable learning rate.

7 EXPERIMENTS
In this section, we evaluate our proposed JTS-MF model on the
aforementioned Weibo voting dataset7. We first introduce base-
lines and parameter settings used in the experiments, and then
present the experimental results of JTS-MF and the comparison
with baselines.

7.1 Baselines
We use the following seven methods as the baselines against JTS-
MF model. Note that the first three baselines are reduced versions
of JTS-MF, which only consider one particular type of similarity
among users or votings.

• JTS-MF(S) only considers social-level similarity of users,

i.e., sets β, γ = 0 in JTS-MF model.

• JTS-MF(G) only considers group-level similarity of users,

i.e., sets α, γ = 0 in JTS-MF model.

• JTS-MF(V) only considers similarity of votings, i.e., sets

α, β = 0 in JTS-MF model.

• MostPop recommends the most popular items to users,
i.e., the votings that have been participated by the most
numbers of users.

• Basic-MF [13] simply uses matrix factorization method
to predict the user-voting matrix while ignores additional
social relation, group affiliation and voting content infor-
mation.

for ed

• Topic-MF [1] is similar to JTS-MF except that we substi-
when calculating similarities in Eq. (11),
tute Θd
(13), and (15). Note that Θd
can also be viewed as the em-
bedding of document with respect to topics. Therefore,
Topic-MF only considers the topic similarity among users
and votings.

• Semantic-MF is similar to JTS-MF except that we use
the Skip-Gram model in [21] directly to learn the word
embeddings. Therefore, Semantic-MF only considers the
semantic similarity among users and votings.

6Note that it is impractical to apply Alternating Least Squares (ALS) method here
because it requires calculating the inverse of two matrices with extremely large size.

7Experiment code is provided at https://github.com/hwwang55/JTS-MF.

CIKM’17, November 6–10, Singapore

H. Wang et al.

Fig. 7. The parameter settings of α, β, γ , and dim are the same as in
Section 7.3.1. Fig. 7a, 7b, and 7c consistently demonstrate that JTS-
MF(S) performs best and JTS-MF(G) performs worst among three
types of reduced versions of JTS-MF. Note that JTS-MF(S) only con-
siders users’ social-level similarity and JTS-MF(G) only considers
users’ group-level similarity. Therefore, it could be concluded that
social-level friends are more helpful than group-level friends when
determining users’ voting interest. This is in accordance with our
intuition, since a user typically has much more group-level friends
than social-level friends, which inevitably dilutes its effectiveness
and brings noises into group-level relationship. In addition, the
result in Fig. 7 also demonstrates the effectiveness of the usage of
votings’ similarity. Furthermore, it can be evidently observed that
JTS-MF model outperforms its three reduced versions in all cases,
which proves that the three types of similarities are well combined
in JTS-MF model to achieve much better results.

7.3.3 Comparison of Models. To further compare JTS-MF model
with other baselines, we gradually increase k from 1 to 500 and
report the results in Table 2 with the best performance highlighted
in bold. The value of α, β, and γ for JTS-MF and its reduced models
are the same as in Section 7.3.1. The parameter settings are α = 2,
β = 60, γ = 15 for Topic-MF, α = 8, β = 120, γ = 20 for Semantic-
MF, and dim = 10 for Qi and Pj in all MF-based methods. The
above parameter settings are the optimal results of fine tuning for
given dim. In Table 2, several observations stand out:

• MostPop performs worst among all methods, because Most-
Pop simply recommends the most popular votings to all
users without considering users’ specific interests.

• Topic-MF and Semantic-MF outperforms Basic-MF, which
proves the usage of similarities with respect to topic and se-
mantic helpful for recommending votings. Besides, Semantic-
MF outperforms Topic-MF. This suggests that semantic
information is more accurate than topic information when
measuring similarities through mining short-length texts.
• JTS-MF outperforms Topic-MF and Semantic-MF. This is
the most important observation from Table 2, since it jus-
tifies our aforementioned claim that joint-topic-semantic
model can benefit from both topic and semantic aspects
and achieve better performance.

• The significance of JTS-MF over other models is evident for
small k. However, this margin becomes smaller when k gets
larger, and JTS-MF is even slightly inferior to JTS-MF(S)
when k ≥ 50. This means that users’ group-level simi-
larities and votings’ similarities “drag the feet” of JTS-MF
model when k is large. However, JTS-MF is still preferred
in practice, since a real recommender system would only
recommend a small set of votings to users in general.

7.4 Parameter Sensitivity
We investigate parameter sensitivity in this subsection. Specifically,
we evaluate how different value of trade-off parameters α, β, γ , and
different numbers of latent feature dimensions dim can affect the
performance.

7.4.1 Trade-off parameters. We fix dim = 10, keep two of the
trade-off parameters as 0, and vary the value of the left trade-off

Fig. 6: Convergence of JTS-MF models with respect to Re-
call@10.

7.2 Parameter Settings
We use GibbsLDA++8, an open-source implementation of LDA
using Gibbs sampling, to calculate topic information of words and
documents in JTS-MF and Topic-MF models. We set the number of
topics to 50 and leave all other parameters in LDA as default values.
For word embeddings in JTS-MF and Semantic-MF models, we use
the same settings as follows: length of embedding dimension as 50,
window size as 5, and number of negative samples as 3.

For all MF-based methods, we set the learning rate δ = 0.001 and
regularization weight λ = 0.5 by 10-fold cross validation. Typically,
we set Im = 0.01 in Eq. (18). Taking into consideration the balance
of experimental results and time complexity, we run 200 iterations
for each of the experiment cases. To conduct the recommendation
task, we randomly select 20% of users’ voting records in the dataset
as test set and use the remaining data as the trainning examples for
our JTS-MF model as well as all baselines. The choice of remaining
hyper-parameters (trade-off parameters α, β, γ , and dimension of
latent features dim) is discussed in Section 7.4.

To quantitatively analyze the performance of voting recommen-
dation, in our experiment, we use top-k recall (Recall@k), top-k
precision (Precision@k), and top-k micro-F1 (Micro-F1@k) as the
evaluation metrics.

7.3 Experiment Results

7.3.1

Study of convergence. To study the convergence of JTS-
MF model, we run the learning algorithm up to 200 iterations for
JTS-MF(S) with α = 10, JTS-MF(G) with β = 140, JTS-MF(V) with
γ = 30, JTS-MF with α = 10, β = 140, γ = 30 (dim = 10 for Qi and
Pj in all models), then calculate Recall@10 for every 10 iterations.
The result of convergence of JTS-MF models is plotted in Fig. 6.
From Fig. 6 we can see that, the recall of JTS-MF models rises
rapidly before 100 iterations, and starts to oscillate slightly after
around 150 iterations. The same changing pattern is observed for
all four JTS-MF variants. Therefore, we set the number of learning
iterations as 200 to achieve a balance between running time and
performance of models.

7.3.2

Study of JTS-MF. To study the performance of JTS-MF
model and the effectiveness of three types of similarities, we run
JTS-MF model as well as its three reduced versions on Weibo voting
dataset, and report the results of Recall, Precision, and Micro-F1 in

8GibbsLDA++: http://gibbslda.sourceforge.net

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

(a)

(b)

(c)

Fig. 7: (a) Recall@k, (b) Precision@k, and (c) Micro-F1@k of JTS-MF models.

(a)

(b)

(c)

(d)

Fig. 8: Parameter sensitivity with respect to (a) α, (b) β, (c) γ , and (d) dim.

Table 2: Result of Recall@k, Precision@k, and Micro-F1@k for JTS-MF model and baselines.

Model

Metric

k

50

100

500

JTS-MF(S)

JTS-MF(G)

JTS-MF(V)

JTS-MF

MostPop

Basic-MF

Topic-MF

Semantic-MF

1
0.0097
0.007416
0.008401
0.0065
0.004944
0.005601
0.0071
0.005439
0.006161

0.0099
0.007614
0.008625
0.0042
0.003221
0.003637
0.0063
0.004845
0.005489
0.0076
0.005834
0.006609
0.0093
0.007120
0.008065

2
0.0172
0.006575
0.009511
0.0133
0.005092
0.007365
0.0149
0.005685
0.008223

0.0178
0.006823
0.009868
0.0085
0.003261
0.004721
0.0129
0.004944
0.007151
0.0147
0.005636
0.008152
0.0169
0.006476
0.009368

5
0.0346
0.005300
0.009192
0.0275
0.004212
0.007306
0.0314
0.004805
0.008335

0.0381
0.005834
0.010118
0.0191
0.002921
0.005062
0.0274
0.004192
0.007271
0.0311
0.004766
0.008266
0.0333
0.005102
0.008849

10
0.0558
0.004271
0.007935
0.0457
0.003500
0.006503
0.0502
0.003846
0.007145

0.0606
0.004637
0.008615
0.0313
0.002403
0.004468
0.0446
0.003411
0.006337
0.0495
0.003787
0.007035
0.0545
0.004173
0.007752

20
0.0846
0.003238
0.006238
0.0752
0.002877
0.005542
0.0789
0.003021
0.005819

0.0908
0.003475
0.006695
0.0517
0.001972
0.003804
0.0727
0.002783
0.005361
0.0781
0.002991
0.005761
0.0860
0.003293
0.006342

Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1

0.1529
0.002341
0.004612
0.1360
0.002082
0.004102
0.1387
0.002124
0.004184
0.1520
0.002327
0.004585
0.0974
0.001482
0.002925
0.1368
0.002094
0.004125
0.1395
0.002136
0.004207
0.1471
0.002252
0.004437

0.2229
0.001707
0.003387
0.2051
0.001570
0.003116
0.2049
0.001568
0.003112
0.2187
0.001674
0.003322
0.1455
0.001119
0.002218
0.2050
0.001569
0.003114
0.2076
0.001589
0.003154
0.2142
0.001639
0.003254

0.4392
0.000672
0.001343
0.4216
0.000645
0.001289
0.4176
0.000639
0.001277
0.4297
0.000658
0.001314
0.3086
0.000469
0.000937
0.4198
0.000643
0.001283
0.4210
0.000644
0.001287
0.4293
0.000657
0.001313

parameter. Then we report Recall@10 in Fig. 8a, 8b, and 8c, respec-
tively.

As shown in Fig. 8a, the Recall@10 increases constantly as α gets
larger and reaches a maximum of 0.0558 when α = 10. This suggests
that the usage of users’ social-level similarity do help to improve the
recommendation performance. However, when α is too large (α =

12), the learning algorithm of JTS-MF is misled to wrong direction
when updating latent features of users and votings, resulting in
performance deterioration. The similar phenomenon are observed
in Fig. 8b and Fig. 8c, too. According to the results, when the
other two trade-off parameters are set to 0, Recall@10 reaches
the maximum when α = 10, β = 140, and γ = 30, respectively.

CIKM’17, November 6–10, Singapore

H. Wang et al.

Therefore, in previous experiments we adopt these optimal settings
for JTS-MF(S), JTS-MF(G), and JTS-MF(V), respectively, and use
their combination as the parameter settings in JTS-MF.

7.4.2 Dimension of latent features. We fix α = 10, β = 0, γ = 0
and tune the dimension of latent features of users and votings from
5 to 90. The result is shown in Fig. 8d. From the figure, we can
see clearly that the recall is increasing when dim gets larger, this
is because latent features with larger number of dimensions have
more capacity to characterize users and votings. But a larger dim
leads to more running time in experiments. Moreover, we notice
that the improvement of performance stagnates after dim reaches
80. On balance, we set dim = 10 in our experiment scenarios to
ensure the experiments can complete within rational time duration.

8 CONCLUSIONS
In this paper, we study the problem of recommending online vot-
ings to users in social networks. We first formalize the voting
recommendation problem and justify the motivation of leveraging
social structure and voting content information. To overcome the
limitations of topic models and semantic models when learning rep-
resentation of voting content, we propose Topic-Enhanced Word
Embedding method to jointly consider topics and semantics of
words and documents. We then propose our Joint-Topic-Semantic-
aware social Matrix Factorization model to learn latent features of
users and votings based on the social network structure and TEWE
representation. We conduct extensive experiments to evaluate JTS-
MF with Weibo voting dataset. The experimental results prove the
competitiveness of JTS-MF against other state-of-the-art baselines
and demonstrate the efficacy of TEWE representation.

ACKNOWLEDGMENTS
This work was partially sponsored by the National Basic Research
973 Program of China under Grant 2015CB352403, the NSFC Key
Grant (No. 61332004), PolyU Project of Strategic Importance 1-ZE26,
and HK-PolyU Grant 1-ZVHZ.

REFERENCES
[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet

[2]

allocation. In Journal of Machine Learning Research. 993–1022.
Jes´us Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Guti´errez.
2013. Recommender systems survey. Knowledge-Based Systems 46 (2013).
[3] Marco Bressan, Stefano Leucci, Alessandro Panconesi, Prabhakar Raghavan,
and Erisa Terolli. 2016. The Limits of Popularity-Based Recommendations, and
the Role of Social Ties. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 745–754.

[4] Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency

Parser using Neural Networks.. In EMNLP. 740–750.

[5] Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander J Smola, Jing Jiang, and
Chong Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie
recommendation (jmars). In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 193–202.

[6] Huiji Gao, Jiliang Tang, Xia Hu, and Huan Liu. 2015. Content-aware point of
interest recommendation on location-based social networks.. In AAAI. 1721–
1727.

[7] Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Pro-

ceedings of National Academy of Sciences (PNAS). 5228–5235.

[8] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 855–864.

[9] Liang Hu, Jian Cao, Guandong Xu, Longbing Cao, Zhiping Gu, and Can Zhu.
2013. Personalized recommendation via cross-domain triadic factorization. In
Proceedings of the 22nd international conference on World Wide Web. ACM, 595–
606.

[10] Hui-Ju Hung, Hong-Han Shuai, De-Nian Yang, Liang-Hao Huang, Wang-Chien
Lee, Jian Pei, and Ming-Syan Chen. 2016. When Social Influence Meets Item
Inference. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 915–924.
Janet Ilieva, Steve Baron, and Nigel M Healey. 2002. Online surveys in marketing
research: Pros and cons. International Journal of Market Research 44, 3 (2002),
361.

[11]

[12] Meng Jiang, Peng Cui, Xumin Chen, Fei Wang, Wenwu Zhu, and Shiqiang Yang.
2015. Social recommendation with cross-domain transferable knowledge. IEEE
Transactions on Knowledge and Data Engineering 27, 11 (2015), 3084–3097.
[13] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-

niques for recommender systems. Computer 42, 8 (2009).

[14] Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Proceedings of the

12th international conference on machine learning. 331–339.

[15] Quoc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences

and Documents.. In ICML, Vol. 14. 1188–1196.

[16] Wu-Jun Li, Dit-Yan Yeung, and Zhihua Zhang. 2011. Generalized latent factor
models for social network analysis. In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI), Barcelona, Spain. 1705.

[17] Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis.

In Mining text data. Springer, 415–463.

[18] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Sun Maosong. 2015. Topical word em-
beddings. In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence
(AAAI). AAAI, 2418–2424.

[19] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social
recommendation using probabilistic matrix factorization. In Proceedings of the
17th ACM conference on Information and knowledge management. ACM, 931–940.
[20] Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007.
Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings
of the 16th international conference on World Wide Web. ACM, 171–180.
[21] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Proceedings of the 26th International Conference on Neural Information Processing
Systems (NIPS). 3111–3119.

[22] Steffen Rendle and Christoph Freudenthaler. 2014. Improving pairwise learning
for item recommendation from implicit feedback. In Proceedings of the 7th ACM
international conference on Web search and data mining. ACM, 273–282.
[23] Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction

by locally linear embedding. science 290, 5500 (2000), 2323–2326.

[24] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltz-
mann machines for collaborative filtering. In Proceedings of the 24th international
conference on Machine learning. ACM, 791–798.

[25] Yue Shi, Alexandros Karatzoglou, Linas Baltrunas, Martha Larson, Nuria Oliver,
and Alan Hanjalic. 2012. CLiMF: learning to maximize reciprocal rank with
collaborative less-is-more filtering. In Proceedings of the sixth ACM conference on
Recommender systems. ACM, 139–146.

[26] Shaojie Tang and Jing Yuan. 2016. Optimizing Ad Allocation in Social Advertising.
In Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management. ACM, 1383–1392.

[27] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning
for recommender systems. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 1235–1244.
[28] Xin Wang, Roger Donaldson, Christopher Nell, Peter Gorniak, Martin Ester, and
Jiajun Bu. 2016. Recommending groups to users using user-group engagement
and time-dependent matrix factorization. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence. AAAI Press, 1331–1337.

[29] Xin Wang, Wei Lu, Martin Ester, Can Wang, and Chun Chen. 2016. Social
Recommendation with Strong and Weak Ties. In Proceedings of the 25th ACM
CIKM. ACM, 5–14.

[30] Zhonghang Xia, Yulin Dong, and Guangming Xing. 2006. Support vector ma-
chines for collaborative filtering. In Proceedings of the 44th annual Southeast
regional conference. ACM, 169–174.

[31] Xiwang Yang, Chao Liang, Miao Zhao, Hongwei Wang, Hao Ding, Yong Liu,
Yang Li, and Junlin Zhang. 2017. Collaborative filtering-based recommendation
of online social voting. IEEE Transactions on Computational Social Systems 4, 1
(2017), 1–13.

[32] Qin Zhang, Jia Wu, Peng Zhang, Guodong Long, Ivor W Tsang, and Chengqi
Zhang. 2016. Inferring Latent Network from Cascade Data for Dynamic Social
Recommendation. In Data Mining (ICDM), 2016 IEEE 16th International Conference
on. IEEE, 669–678.

[33] Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010.

Jointly
modeling aspects and opinions with a MaxEnt-LDA hybrid. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 56–65.

[34] Hengshu Zhu, Enhong Chen, Kuifei Yu, Huanhuan Cao, Hui Xiong, and Jilei
Tian. 2012. Mining personal context-aware preferences for mobile users. In 2012
IEEE 12th International Conference on Data Mining. IEEE, 1212–1217.

Joint Topic-Semantic-aware Social Recommendation
for Online Voting

Hongwei Wang1,2, Jia Wang2, Miao Zhao2, Jiannong Cao2, Minyi Guo1∗
1Shanghai Jiao Tong University, 2The Hong Kong Polytechnic University
wanghongwei55@gmail.com,{csjiawang,csmiaozhao,csjcao}@comp.polyu.edu.hk,guo-my@cs.sjtu.edu.cn

7
1
0
2
 
c
e
D
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
7
0
0
.
2
1
7
1
:
v
i
X
r
a

ABSTRACT
Online voting is an emerging feature in social networks, in which
users can express their attitudes toward various issues and show
their unique interest. Online voting imposes new challenges on rec-
ommendation, because the propagation of votings heavily depends
on the structure of social networks as well as the content of vot-
ings. In this paper, we investigate how to utilize these two factors
in a comprehensive manner when doing voting recommendation.
First, due to the fact that existing text mining methods such as
topic model and semantic model cannot well process the content of
votings that is typically short and ambiguous, we propose a novel
Topic-Enhanced Word Embedding (TEWE) method to learn word
and document representation by jointly considering their topics
and semantics. Then we propose our Joint Topic-Semantic-aware
social Matrix Factorization (JTS-MF) model for voting recommenda-
tion. JTS-MF model calculates similarity among users and votings
by combining their TEWE representation and structural informa-
tion of social networks, and preserves this topic-semantic-social
similarity during matrix factorization. To evaluate the performance
of TEWE representation and JTS-MF model, we conduct extensive
experiments on real online voting dataset. The results prove the
efficacy of our approach against several state-of-the-art baselines.

KEYWORDS
Online voting; recommender systems; topic-enhanced word em-
bedding; matrix factorization

1 INTRODUCTION
Online voting [31] has recently become a popular function on so-
cial platforms, through which a user can share his opinion towards
various interested subjects, ranging from livelihood issues to en-
tertainment news. More advanced than simple like-dislike type of
votings, some social networks, such as Weibo1, have empowered
users to run their own voting campaigns. Users can freely initiate
votings on any topics of their own interests and customize voting
options. These votings are visible to the friends of initiator, who
can then choose to participate to make the votings further seen by
their friends or simply retweet the votings to their friends. In such
a way, in addition to the system recommendation, a voting can
widely propagate over the network along social paths. The voting
propagation scheme is shown in Fig. 1.

∗M. Guo is the corresponding author.
1http://www.weibo.com.

CIKM’17, Singapore
2017. 978-1-4503-4918-5/17/11. . . $15.00
DOI: 10.1145/3132847.3132889

Fig. 1: Propagation scheme of online voting.

Facing a large volume of diversified votings, a critical challenge
is to present “right” votings to the “right” person. An effective
recommender system is desired to be able to deal with information
overload [2] by precisely locating what votings favor each user most,
thus improves user experience and maximizes user engagement in
votings. Such a recommender system can also benefit a variety of
other online services such as personalized advertising [26], market
research [11], public opinion analysis [17], etc.

Despite the great importance, there is little prior work consider-
ing recommending votings to users in social networks. The chal-
lenges are two-fold. First, the propagation of online votings relies
heavily on the structure of social networks. A user can see the
votings initiated, participated or retweeted by his followees, which
implies that the user is more likely to be exposed to the votings
that his friends are involved in. Moreover, in most social networks,
a user can join different interest groups, which is another type
of social structure that potentially affects users’ voting behavior.
Though several prior works [3, 6, 10, 12, 25, 29, 31, 32] have been
proposed to leverage social network information in recommenda-
tion, it is still an open question how to comprehensively incorporate
structural social information into the task of voting recommenda-
tion considering its propagation pattern. Second, users’ interest
in votings is strongly connected with voting content presented in
question text (e.g., “Who is your favorite movie star?”). Topic model
[1] is regarded as a possible approach to mine the voting interests
through discovering the latent topic distribution of relevant voting
text. However, the voting questions are typically short and lack
sufficient topic information, leading to severe performance degra-
dation of topic models. Alternatively, semantic analytics [21] can
also possibly be used to mine voting interests through learning text
representations. However, such semantic models typically repre-
sent each word using a single vector, making them indiscriminative
for homonymy and polysemy, which are especially common in
voting questions (e.g., “Do you use apple products?” and “Do you
peel apple before eating?”). In brief, these inherent defects of the

CIKM’17, November 6–10, Singapore

H. Wang et al.

above models limit their power in the scenario of social voting
recommendation.

To address aforementioned challenges, in this paper, we propose
a novel Joint Topic-Semantic-aware Matrix Factorization (JTS-MF)
model for online voting recommendation. JTS-MF model considers
social network structure and representation of voting content in
a comprehensive manner. For social network structure, JTS-MF
model fully encodes the information of social relationship and group
affiliation into the objective function. We will further justify the
usage of social network structure in Section 3. For representation
of voting content, we propose a Topic-Enhanced Word Embedding
(TEWE) method to build a multi-prototype word and document2
representation, which jointly considers their topics and semantics.
The key idea of TEWE is to enable each word to have different rep-
resentations under different word topics and different documents.
We will detail TEWE in Section 5. Once obtaining TEWE repre-
sentation for each document, JTS-MF model combines them with
the structural information of social networks to calculate the topic-
semantic-social similarity among users and votings. The reason
of calculating such similarity is that, inspired by Locally Linear
Embedding [23], we try to preserve the similarity among users
and votings during matrix factorization, as it contains abundant
proximity information and can greatly benefit feature learning for
users and votings. JTS-MF model is detailed in Section 6.

We conduct extensive investigation on JTS-MF with real online
voting dataset. The experimental results in Section 7 demonstrate
that JTS-MF model achieves substantial gains compared with base-
lines. The results also prove that TEWE is able to well combine
topic and semantic information of texts and generates a better kind
of document representation.

In summary, the contributions of this paper are as follows:

• We formally formulate the online voting recommendation

problem, which has not been fully investigated yet.

• We indicate that user’s voting behavior is highly correlated
with social network structure by conducting thorough sta-
tistical measurements.

• We propose a novel Topic-Enhanced Word Embedding
model to jointly consider topics and semantics of words
and documents to learn their representation. TEWE takes
advantages of both topic model and semantic model, and
consequently learns more informative embeddings.

• We develop a novel matrix factorization based models,
named JTS-MF, for online voting recommendation. JTS-
MF is able to preserve the topic-semantic-social similarity
among users and votings from original embedding space
during learning process.

• We carry out extensive experiments on real online voting
dataset, the results of which reveal that JTS-MF signif-
icantly outperforms baseline (variant) methods, say for
example, surpassing basic matrix factorization model with
57%, 38% and 25% enhancement in terms of recall for top-1,
top-5 and top-20 recommendation, respectively.

2In this paper, a “document” can be related to a voting, a user or a group. A voting
document is the content of its question, a user document is formed by aggregating
all the documents of votings he participates, and a group document is formed by
aggregating all the documents of users who join the group.

2 RELATED WORK
2.1 Recommender Systems
Roughly speaking, existing recommender systems can be catego-
rized into three classes [2]: content-based, collaborative filtering,
and hybrid methods. Content-based methods [14, 34] make use of
user profiles or item descriptions as features for recommendation.
Collaborative filtering methods [22, 25, 28, 31] use either explicit
feedback (e.g., users’ ratings on items) or implicit feedback (e.g.,
users’ browsing records about items) data of user-item interactions
to find user preference and make the recommendation. In addition,
various models are incorporated into collaborative filtering, such
as Support Vector Machine [30], Restricted Boltzmann Machine
[24], and Stacked Denoising Auto Encoder [27]. Hybrid methods
[9, 16] combine content-based and collaborative filtering models
in many hybridization approaches, such as weighted, switching,
cascade and feature combination or augmentation.

2.2 Social Recommendation
Traditional recommender systems are vulnerable to data sparsity
problem and cold-start problem. To mitigate this issue, many ap-
proaches have been proposed to utilize social network information
in recommender systems [3, 6, 10, 12, 25, 29, 31, 32]. For exam-
ple, [12] represents a social network as a star-structured hybrid
graph centered on a social domain which connects with other item
domains to help improve the prediction accuracy. [10] investi-
gates the seed selection problem for viral marketing that considers
both effects of social influence and item inference for product rec-
ommendation. [29] studies the effects of strong and weak ties in
social recommendation, and extends Bayesian Personalized Rank-
ing model to incorporate the distinction of strong and weak ties.
However, the above works only utilize users’ social links without
considering the topic and semantic information for mining the
similarities among users and items, which we found quite help-
ful for voting recommendation tasks. Another difference between
these works and ours is that we also take social group affiliation
into consideration, which can further improve the performance of
recommendation.

2.3 Topic and Semantic Language Models
Latent Dirichlet Allocation (LDA) [1] is a well-known generative
topic model that learns the latent topic distributions for documents.
LDA is widely used in sentiment analysis [20], aspects and opin-
ions mining [33], and recommendation [5]. Word2vec [21] is gen-
erally recognized as a neural network model, which learns word
representations that capture precise syntactic and semantic word
relationships. Word2vec as well as associated Skip-Gram model are
extensively used in document classification [15], dependency parser
[4], and network embedding [8]. However, LDA and Word2vec are
not directly applicable in the scenario of voting recommendation
because the content of voting is usually short and ambiguous. As
a combination, [18] tries to learn topical word embeddings based
on both words and their topics. The difference between [18] and
ours is that we also take topics of documents into consideration,
which enables our model to learn a even more discriminative and
informative representations for words and documents.

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

Table 1: Basic Statistics of Weibo Dataset.

# users
# users with votings
# users with groups
# votings

1,011,389
525,589
723,913
185,387

# groups
# user-voting
# user-user
# user-group

299,077
3,908,024
83,636,677
5,643,534

3 BACKGROUND AND DATA ANALYSIS
In this section, we briefly introduce the background of Weibo voting
and present detailed analysis of Weibo voting dataset.

3.1 Background
Weibo is one of the most popular Chinese microblogging website
launched by Sina corporation, which is akin to a hybrid of Twitter
and Facebook platforms. Users on Weibo can follow each other,
write tweets and share with his followers. Users can also join
different groups based on their attributes (e.g., region) or interests
of topics (e.g., career).

Voting3 is one of the embedded features of Weibo. As of January
2013, more than 92 million users have participated in at least one
voting and more than 2.2 million ongoing votings are available on
Weibo every day. Any user can freely initiate, retweet and partici-
pate a voting campaign in Weibo. As shown in Fig. 1, votings can
propagate in two ways. The first way is through social propagation:
a user can see the voting initiated, retweeted or participated by his
followees and potentially participates the voting. The second way
is through Weibo voting recommendation list, which consists of
popular votings and personalized recommendation for each user.

3.2 Data Measurements
Our Weibo voting dataset comes from the technical team of Sina
Weibo, which contains detailed information about votings from No-
vember 2010 to January 2012, as well as other auxiliary information.
Specifically, the dataset includes users’ participation status on each
voting4, content of each voting, social connection between users,
name and category of each group, and user-group affiliation.

3.2.1 Basic statistics. The basic statistics are summarized in
Table 1. From Table 1 we can learn that, each user has 165.4 fol-
lowers/followees, participates 3.9 votings, and joins 5.6 groups on
average. If we only count users who participate at least one vot-
ing and users who join at least on group, the average number of
votings and average number of joined groups of each user is 7.4
and 7.8, respectively. Fig. 2 depicts the distribution curves of the
above statistics, where the meaning of each subfigure is given in
the caption.

To get an intuitive understanding of whether user’s voting be-
havior is correlated with his social relation and group affiliation,
we conduct the following two sets of statistical experiments:

3.2.2 Correlation between the number of common votings of user
pairs and the types of user pairs. We randomly select ten million
user pairs from the set of all users, and count the average number
of votings that the two users both participate under the following
four circumstances: 1) one of the users follows the other in the
pair, i.e., they are social-level friends; 2) the two users are in at

3http://www.weibo.com/vote?is all=1.
4We only know whether a user participated a voting or not, rather than user voting
results, i.e., we do not know which voting option a user chose.

(a)

(c)

(e)

(b)

(d)

(f)

Fig. 2: (a) Distribution of the number of votings participated
by a user; (b) Distribution of the number of participants of a
voting; (c) Distribution of the number of followers/followees
of a user; (d) Distribution of the number of users in a group;
(e) Distribution of the number of votings (may contain du-
plicated votings) participated by all users in a group; (f) Dis-
tribution of the number of groups joined by a user.

least one common group, i.e., they are group-leven friends; 3) the
two users are neither social-level friends nor group-level friends;
4) all cases. The results are plotted in Fig. 3a, which clearly shows
the difference among these cases. In fact, the average number of
common votings of social-level friends (3.54 × 10−4) and group
level friends (1.79 × 10−4) are 17.4 and 8.8 times higher than that of
“strangers” (2.04 × 10−5). The results demonstrate that if two users
are social-level or group-level friends, they are likely to participate
more votings in common.

3.2.3 Correlation between the probability of two users being
friends and whether they participate common voting. We first ran-
domly select ten thousand votings from the set of all votings. For
each sampled voting vj , we calculate the probability that two of
its participants are social-level or group-level friends, i.e., pj =
# of social/group-level friends among participants of vj
, where nj is the num-
nj ×(nj −1)/2
ber of vj ’ participants. We calculate pj over all sampled votings
and plot the average result (blue bar) in Fig. 3b. For comparison, we
also plot the result for randomly sampled set of users (green bar)
in Fig. 3b. It is clear that if two users ever participated common
voting, they are more likely to be social-level or group-level friends.
In fact, probabilities of two users being social-level or group-level
friends are raised by 5.3 and 3.6 times given the observation that
they are with common voting.

CIKM’17, November 6–10, Singapore

H. Wang et al.

(a)

(b)

Fig. 3: (a) Average number of common votings participated
by user ui and uk in four cases: 1. ui follows/is followed by
uk ; 2. ui and uk are in at least one common group; 3. ui and
uk have no social-level and group-level relationship; 4. all
cases; (b) Probability of two users being social-level or group-
level friends in two cases: 1. they ever participated at least
one common voting; 2. they are randomly sampled.

The above two findings effectively prove the strong correlation
between voting behavior and social network structure, which mo-
tivates us to take users’ social relation and group affiliation into
consideration when making voting recommendation.

4 PROBLEM FORMULATION
In this paper, we consider the problem of recommending Weibo
votings to users. We denote the set of all users, the set of all votings,
and the set of all groups by U = {u1, ..., uN }, V = {v1, ..., vM },
and G = {G1, ..., GL }, respectively. Moreover, we model three
types of relationship in Weibo platform: user-voting, user-user, and
user-group relationship as follows:

(1) The user-voting relationship for ui and vj is defined as

=

(1)

Iui,vj

i f ui participates vj ;
otherwise.

(cid:40)1,
0,
(2) The user-user relationship for ui and uk
(cid:40)1,
;
0,
to denote the set of ui ’s followees, and
to denote the set of ui ’s followers (“+” means “out”

i f ui f ollows uk
otherwise.

We further use F +
i
use F −
i
and “−” means “in”).

is defined as

Iui,uk

(2)

=

(3) The user-group relationship for ui and Gc is defined as

Iui,Gc

=

(cid:40)1,
0,

i f ui joins Gc ;
otherwise.

(3)

Given the above sets of users and votings as well as three types
of relationship, we aim to recommend a list of votings for each user,
in which the votings are not participated by the user but may be
interesting to him.

5 JOINT-TOPIC-SEMANTIC EMBEDDING
In this section, we explain how to learn the embeddings of users,
votings, and groups in a joint topic and semantic way, and apply the
embeddings to calculate similarities. We first introduce the methods
of learning topic information and semantic information by LDA and
Skip-Gram models, respectively, and propose our method which
combines these two models to learn more powerful embeddings.

5.1 Topic Distillation
In this subsection, we introduce how to profile users, votings, and
groups in terms of topic interest distribution by performing topic
distillation on the associated textual content information.

In general, LDA is a popular generative model to discover latent
topic information from a collection of documents [1]. In LDA, each
document d is represented as a multinomial distribution Θ
over a
d
set of topics, and each topic z is also represented as a multinomial
distribution Φz over a number of words. Subsequently, each word
position l in document d is assigned a topic zd,l
,
according to Θ
d
and the word wd,l
. By LDA
approach, the topic distribution for each document and the topic
assignment for each word can be obtained, which would be utilized
later in our proposed model.

is finally generated according to Φzd,l

’s and dvj

= ∪{dui |Iui,Gc

5. The document dui

= 1}, and the document dGc

Here, we discuss how to apply LDA in the scenario of Weibo
voting. According to the Weibo voting dataset, each voting vj
associates a sentence of question, which can be regarded as doc-
for user ui can thus be formed by
ument dvj
aggregating the content of all votings he participates, i.e., dui
=
for group Gc is formed by
∪{dvj |Iui,vj
aggregating documents of all its members, i.e., dGc
=
1}. Note that though our target is to learn the topic distributions of
all users, votings, and groups, it is inadvisable to train LDA model
on dui
’s because: (1) the entitled sentence associated with
a single voting is typically short-presented and topic-ambiguous;
(2) even with user-level voting content aggregation, some docu-
ments of inactive users are not long enough to accurately extract
the authentic topic distribution, yet showing relatively flat distribu-
tion over all the topics. Therefore, we choose to feed group-level
aggregated documents dGc
’s to LDA model as training samples.
The process of group-level voting content aggregation will cover
all the content the affiliated users are interested in and help better
identify their interests in terms of voting topic.
Denote Dir(α ) as the Dirichlet prior of Θ
d

, and Dir(β) as the
Dirichlet prior of Φz . Given α and β, the joint distribution of
document-topic distributions Θ, topic-word distributions Φ, topics
of words z, and a set of words w is

p(Θ, Φ, z, w |α , β)
z p(Φz |β) · (cid:206)
= (cid:206)
d

(cid:18)
p(Θ

d |α ) (cid:206)
l

(cid:16)
p(zd,l |Θ

d )p(wd,l |Φzd,l )

(cid:17) (cid:19)

,

(4)
where d traverses all group-level aggregated documents. In general,
it is computationally intractable to directly maximize the joint
likelihood in Eq. (4), thus Gibbs Sampling [7] is usually applied
to estimate the posterior probability p(z|w, α , β) and solve Θ, Φ
(w )
the
iteratively. Denote θ
the z-th component of Θ
d
z
w-th component of Φz . With the sampling results, Θ
and Φz can
d
be estimated as:

, and ϕ

(z)
d

θ

(z)
d
(w )
ϕ
z

(z)
= (cid:0)n
d
(w )
= (cid:0)n
z

+ α (z)(cid:1)/(cid:0) (cid:205)
z (n
+ β (w )(cid:1)/(cid:0) (cid:205)

+ α (z))(cid:1),
(w )
z
where α (z) is the z-th component of α , β (w ) is the w-th component
(w )
of β, n
z

z = 1, ..., Z ,
+ β (w ))(cid:1), w = 1, ..., V ,

is the observation counts of topic z for document d, n

(z)
d
w (n

(5)

(z)
d

is segmented by Jieba (https://github.com/fxsjy/jieba) and all stop words are

5dvj
removed.

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

L(D) =

log p(wt +c |wt ),

(6)

L(D) =

log p((cid:104)wt +c , zt +c , zd

t +c (cid:105)|(cid:104)wt , zt , zd

t (cid:105)),

(8)

is the frequency of word w assigned as topic z, Z is the number of
topics and V is vocabulary size.

So far, we have obtained the topic assignment for each word and
topic distribution for each group. Topic distributions for users and
votings can thus be inferred by using the learned model and Gibbs
Sampling, which is similar to the calculation of θ

in Eq. (5).

(z)
d

5.2 Semantic Distillation
In this subsection, we introduce how to profile users, votings, and
groups in terms of semantic information. Word embedding, which
represents each word using a vector, is widely used to capture
semantic information of words. Skip-Gram model is a well-known
framework for word embedding, which finds word representation
that are useful for predicting surrounding words in a document
given a target word in a sliding window [21]. More formally, given
a word sequence D = {w1, w2, . . . , wT }, the objective function of
Skip-Gram is to maximize the average log probability

1

T

T
(cid:213)

t =1

(cid:213)

−k ≤c ≤k
c(cid:44)0

where k is the training context size of the target word, which
can be a function of the centered work wt . The basic Skip-Gram
formulation defines p(wi |wt ) using the softmax function as follows:

p(wi |wt ) =

(cid:205)

exp(w(cid:62)

i wt )
exp(w(cid:62)wt )

,

w ∈V
where wi and wt are the vector representation of context word wi
and target word wt , respectively, and V is the vocabulary. To avoid
traversing the entire vocabulary, hierarchical softmax or negative
sampling are used in general during learning process [21].

(7)

5.3 Topic-Enhanced Word Embedding
In this subsection, we propose a joint topic and semantic learning
model, named Topic-Enhanced Word Embedding (TEWE), to ana-
lyze documents of users, votings, and groups. The motivation of
proposed TEWE is based on the following two observations: (1)
The voting content typically involves short texts. Even we infer
the topic distribution for each voting based on the learned topic-
word distribution from group-level aggregated documents, it is
still topic-ambiguous to some extent. (2) The Skip-Gram model
for word embedding assumes that each word always preserves a
single vector, which sometimes is indiscriminate under different
circumstances due to the homonymy and polysemy. Therefore, the
basic idea of TEWE is to preserve topic information of documents
and words when measuring the interaction between target word wt
and context word wi . In this way, a word with different associated
topics has different embeddings, and a word in documents with
different topics has different embeddings, too.

Specifically, rather than solely using the target word w to predict
context words in Skip-Gram, TEWE also jointly utilizes zw , the
topic of the word in a document, as well as zd
, the most likely
w
topic of the document that the word belongs to. Recall that in Sec-
tion 5.1, we have obtained the topic of each word zw and topic
can be calculated
distributions of each document Θ
d

, thus zd
w

(a) Skip-Gram

(b) TEWE

Fig. 4: Comparison between Skip-Gram and TEWE. The
gray circles in (a) indicate the embeddings of original words,
while the blue circles in (b) indicate the TEWE representa-
tion of pseudo words, which preserves semantic and topic
information of words and documents.

(z)
d

, where θ

= arg maxz θ

(z)
is the probability that docu-
as zd
w
d
ment d belongs to topic z, as introduced in Eq. (5). TEWE regards
each word-topics triplet (cid:104)w, zw , zd
w (cid:105) as a pseudo word and learns a
unique vector wz,zd for it. The objective function of TEWE is as
follows:

(cid:213)

1

T

T
(cid:213)

t =1

−k ≤c ≤k
c(cid:44)0
(cid:105)|(cid:104)wt , zt , zd

(cid:17) .

(cid:17)

t (cid:105)) =

wz,zd
i

p((cid:104)wi , zi , zd

i (cid:105)|(cid:104)wt , zt , zd

where p((cid:104)wi , zi , zd
i

t (cid:105)) is a softmax function as
exp (cid:16)
(cid:62)wz,zd
t
exp (cid:16)
wz,zd (cid:62)wz,zd
t
(9)
The comparison between Skip-Gram and TEWE is shown in Fig.
4. Instead of solely utilizing the target and context words as in Skip-
Gram, TEWE further preserves word topic and document topic
along with these words, and incorporates both topic and semantic
information in embedding learning.

(cid:104)w,z,zd (cid:105) ∈ (cid:104)V ,Z,Z (cid:105)

(cid:205)

Once obtaining TEWE representation for each pseudo word, the
representation of each document can be correspondingly derived
by aggregating the embeddings of its containing words weighted
by term frequency-inverse document frequency (TF-IDF) coefficient.
Specifically, for each document d, its TEWE can be calculated as

= (cid:205)

ed

w ∈d

TF-IDF(w, d) · wz,zd

,

(10)

where TF-IDF(w, d) is the product of the raw count of w in d and
the logarithmically scaled inverse fraction of the documents that
contains w, i.e., TF-IDF(w, d) = fw,d · log
(D is the set
of all documents). TEWE document representations can be used in
measuring inter-document similarities. For example, the similarity
can be calculated as the cosine
of two user documents dui
similarity between their TEWE representations, i.e.,

|D |
|d ∈D:w ∈d |

(cid:107)2
This similarity encodes both topic and semantic proximity informa-
tion of user documents, which implicitly reveals the similarity of
voting interests between two users.

e(cid:62)
euk
ui
(cid:107)eui (cid:107)2 (cid:107)euk

and duk

.

6 RECOMMENDATION MODEL
In this section, we present our Joint Topic-Semantic-aware Matrix
Factorization (JTS-MF) model for online social votings, in which
social relationship, group affiliation, and topic-semantic similarities
are combined and taken into account for voting recommendation in

CIKM’17, November 6–10, Singapore

H. Wang et al.

between two users from both topic-semantic interests and their
social influence perspectives.

To avoid the impact of different numbers of followees, we use
the normalized social-level similarity coefficient of users in JTS-MF,
which is defined as

,

=

(cid:205)

(12)

(cid:98)Si,k

Si,k

where F +
i

Si,k
k ∈ F+
i
denotes the set of ui ’s followees in social network.
6.1.2 Normalized group-level similarity coefficient of users. Group-
level similarity coefficient of users is represented by matrix G N ×N ,
which actually measures the topic-semantic similarity among users
from viewpoint of groups. For each ui , the group-level similarity
coefficient with respect to uk

is defined as

= (cid:213)

Gi,k

Iui,G · Iuk,G ·

G ∈ G

e(cid:62)
ui eG
(cid:107)eui (cid:107)2 (cid:107)eG (cid:107)2

,

(13)

where G represents the set of all groups, Iui,G and Iuk,G indicate
whether ui and uk
join group G respectively as described in Eq.
(3), and the last term is the topic-semantic similarity between user
reflects the interest
ui and group G. Essentially speaking, Gi,k
closeness between user ui and its group-level friend uk
by using
ui ’s topic-semantic engagement extent to the corresponding group.
We also normalize the group-level similarity coefficient of users as

where Gi is the set of ui ’s group-level friends in social network.

6.1.3 Normalized similarity coefficient of votings. Similarity coef-
ficient of votings is represented by matrix T M ×M , which is directly
defined as the topic-semantic similarity among votings, i.e.,

(cid:98)Gi,k

=

Gi,k
k ∈ Gi Gi,k

,

(cid:205)

Tj,t =

e(cid:62)
vj evt
(cid:107)evj (cid:107)2 (cid:107)evt (cid:107)2

.

Since the number of votings is typically huge, we only consider
the similarity between two votings with sufficiently high coefficient
value. Specifically, for each voting vj , we define a set of votings Vj
containing those votings whose similarity coefficients with vj ex-
ceed a threshold, i.e., Vj = {vt |Tj,t ≥ threshold}. Correspondingly,
the similarity coefficient of votings are normalized as

(cid:98)Tj,t =

Tj,t
t ∈Vj Tj,t

.

(cid:205)

6.2 Objective Function
Using the notations listed above, the objective function of JTS-MF
can be written as

L =

1
2

N
(cid:213)

M
(cid:213)

i =1

j=1

(cid:16)

I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17) 2

+ α
2

N
(cid:213)

i =1

(cid:13)
(cid:13)Qi −

(cid:213)

k ∈F+
i

(cid:98)Si,k Qk

(cid:13)
(cid:13)

2
2

+ β
2

N
(cid:213)

i =1

(cid:13)
(cid:13)Qi −

(cid:213)

k ∈Gi

(cid:98)Gi, k Qk

(cid:13)
(cid:13)

2

2 + γ
2

M
(cid:213)

j=1

(cid:13)
(cid:13)Pj −

(cid:213)

t ∈Vj

(cid:98)Tj, t Pt

(cid:16)

(cid:13)
(cid:13)

2

2 + λ
2

(cid:107)Q (cid:107)2
F

+ (cid:107)P (cid:107)2
F

(cid:17)

.

The basic idea of the objective function in Eq. (17) lies in that,
besides considering explicit feedback between users and votings,
we also impose penalties on the discrepancy among features of

(14)

(15)

(16)

(17)

Fig. 5: Graphic Model of JTS-MF.

a comprehensive manner. Motivated by Locally Linear Embedding
[23] which tries to preserve the local linear dependency among
inputs in the low-dimensional embedding space, we expect to keep
inter-user and inter-voting topic-semantic similarities in latent
feature space as well. To this end, in JTS-MF model, while the
rating Ri, j is factorized as user latent feature Qi and voting latent
feature Pj, we deliberately enforce Qi and Pj to be dependent on
their social-topic-semantic similar counterparts, respectively. The
graphic model of JTS-MF model is as shown in Figure 5.

6.1 Similarity Coefficients
In order to characterize the influence of inter-user common interests
and inter-voting content relevance, we first introduce the following
three similarity coefficients:

• Normalized social-level similarity coefficient of users: (cid:98)Si,k

,

where uk

is the social-level friend of ui ;

• Normalized group-level similarity coefficient of users: (cid:98)Gi,k

,

where uk

is the group-level friend of ui ;

• Normalized similarity coefficient of voting: (cid:98)Tj,t , where vj

and vt are two distinct votings.

Generally speaking, in JTS-MF, the latent feature Qi for user ui
is tied up with the latent feature of his social-level and group-level
friends who are weighted through (cid:98)Si,k
’s. Likewise, the
latent feature Pj for voting vj is tied up with the latent feature of
its similar votings, which are weighted through (cid:98)Tj,t ’s.

’s and (cid:98)Gi,k

·

Si,k

is defined as

= Iui,uk ·

e(cid:62)
ui euk
(cid:107)eui (cid:107)2 (cid:107)euk (cid:107)2

6.1.1 Normalized social-level similarity coefficient of users. Social-
level similarity coefficient of users is represented by matrix S N ×N ,
which incorporates both social relationship and user-user topic-
semantic similarity. Specifically, for each ui , the social-level simi-
larity coefficient with respect to uk
(cid:115) d−
+ d
k
d+
+ d−
i
k
indicates whether ui follows uk
is the out-degree of ui in the social network (i.e., d
in the social network (i.e., d−
is the in-degree of uk
k

as described in Eq. (2),
where Iui,uk
= |F +
+
+
|),
d
i
i
i
= |F −
d−
|), d
k
k
e(cid:62)
euk
is the smoothing constant (d = 1 in this paper), and
ui
(cid:107)eui (cid:107)2 (cid:107)euk
is the topic-semantic similarity between user ui and user uk
mentioned in Section 5.3.

+d
k
+d −
k
of local authority and local hub value to differentiate the impor-
counts the closeness
tance of different users [19]. Essentially, Si,k

incorporates the information

(cid:114) d −
d +
i

+ d

(cid:107)2
as

(11)

+d

,

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

similar users and similar votings. We give detailed explanation as
follows. The first term of Eq. (17) measures the mean squared error
between prediction and ground truth, where I (cid:48)
is the training
i, j
weights defined as

I (cid:48)
i, j

=

(cid:40)1,
Im,

i f ui participates vj
otherwise

.

(18)

defined in Eq. (1) as the
The reason we do not directly use Iui,vj
training weights is because we found a small and positive Im makes
the training process more robust and can greatly improve the results.
Ri, j is the actual rating of user ui on voting vj , and Qi P (cid:62)
is the
j
predicted value of Ri, j . Without loss of generality, in JTS-MF model,
we set Ri, j = 1 if ui participates vj and Ri, j = 0 otherwise.

’s. Weight (cid:98)Si,k

’s address both the followee uk

The second, third, and fourth terms of Eq. (17) measure the
penalty of discrepancy among similar users and similar votings. In
particular, the second term enforces user ui ’s latent feature Qi to
be similar to the weighted average of his like-minded followees’
profiles Qk
’s social
influence on ui as well as the degree of common voting interests
shared between uk
and ui . The third term enables user ui ’s latent
feature Qi to be similar to the weighted average of all his group
’s emphasize both the same group
peers’ profiles Qk
affiliation of users ui and uk
and also the tie strength between ui and
the associated group with respect to voting interests. This implies
that, among all group-level friends, ui would have more similar
latent feature with the users who frequently join those groups ui is
interested in. The fourth term ensures voting vj ’s latent feature Pj
to be similar to the weighted average of votings that share similar
topic-semantic information with vj .

’s. Weight (cid:98)Gi,k

Finally, the last term of Eq. (17) is the regularizer to prevent

over-fitting, and λ is the regularization weight.

The trade-off among user social-level similarities, user group-
level similarities, and voting similarities is controlled by the pa-
rameters α, β, and γ , respectively. Obviously, users’ social-level
similarity, users’ group-level similarity, or votings’ similarity is/are
ignored if α, β, or γ is/are set to 0, while increasing these values
shifts the trade-off more towards their respective directions.

6.3 Learning Algorithm
To solve the optimization in Eq. (17), we apply batch gradient
descent approach to minimize the objective function6. The gradients
of loss function in Eq. (17) with respect to each variable Qi and Pj
are as follows:

(cid:16)

−I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17)

Pj

∂L
∂Qi

=

M
(cid:213)

j =1
(cid:18)

(cid:213)

k ∈F+
i
(cid:213)

k ∈Gi

(cid:1) + (cid:213)
t ∈F−
i

(cid:1) + (cid:213)
t ∈U

+ α

(cid:0)Qi −

(cid:98)Si,k Qk

−(cid:98)St,i

(cid:0)Qt −

(cid:98)St,k Qk

(cid:18)

+ β

(cid:0)Qi −

(cid:98)Gi,k Qk

− (cid:98)Gt,i

(cid:0)Qt −

(cid:98)Gt,k Qk

+ λQi,

(cid:213)

k ∈F+
t
(cid:213)

k ∈Gi

(cid:19)

(cid:1)

(cid:19)

(cid:1)

(19)

(cid:16)

−I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17)

Qi

∂L
∂Pj

=

N
(cid:213)

i =1
(cid:18)

+ γ

(cid:0)Pj −

(cid:98)Tj, t Pt

−(cid:98)Tk, j

(cid:0)Pk −

(cid:213)

t ∈Vj

(cid:1) + (cid:213)
k ∈Vj

(cid:213)

t ∈Vk

(cid:19)

(cid:1)

(cid:98)Tk, t Pt

+ λPj .

(20)

To clearly understand the gradients in Eq. (19) and (20), it is
worth pointing out that Qi appears not only in the i-th sub-term
in the second and third lines of Eq. (17) explicitly, but also exists
in other t-th sub-terms followed by (cid:98)St,i or (cid:98)Gt,i , where ui plays as
one of the followees or group members of other users. The case is
similar for Pj. Given the gradients in Eq. (19) and (20), we list the
pseudo code of the learning algorithm for JTS-MF as follows:

(1) Randomly initialize Q and P;
(2) In each iteration of the algorithm, do:
a) update each Qi : Qi ← Qi − δ ∂L
;
∂Qi
b) update each Pj: Pj ← Pj − δ ∂L
;
∂Pj
until convergence, where δ is an configurable learning rate.

7 EXPERIMENTS
In this section, we evaluate our proposed JTS-MF model on the
aforementioned Weibo voting dataset7. We first introduce base-
lines and parameter settings used in the experiments, and then
present the experimental results of JTS-MF and the comparison
with baselines.

7.1 Baselines
We use the following seven methods as the baselines against JTS-
MF model. Note that the first three baselines are reduced versions
of JTS-MF, which only consider one particular type of similarity
among users or votings.

• JTS-MF(S) only considers social-level similarity of users,

i.e., sets β, γ = 0 in JTS-MF model.

• JTS-MF(G) only considers group-level similarity of users,

i.e., sets α, γ = 0 in JTS-MF model.

• JTS-MF(V) only considers similarity of votings, i.e., sets

α, β = 0 in JTS-MF model.

• MostPop recommends the most popular items to users,
i.e., the votings that have been participated by the most
numbers of users.

• Basic-MF [13] simply uses matrix factorization method
to predict the user-voting matrix while ignores additional
social relation, group affiliation and voting content infor-
mation.

for ed

• Topic-MF [1] is similar to JTS-MF except that we substi-
when calculating similarities in Eq. (11),
tute Θd
(13), and (15). Note that Θd
can also be viewed as the em-
bedding of document with respect to topics. Therefore,
Topic-MF only considers the topic similarity among users
and votings.

• Semantic-MF is similar to JTS-MF except that we use
the Skip-Gram model in [21] directly to learn the word
embeddings. Therefore, Semantic-MF only considers the
semantic similarity among users and votings.

6Note that it is impractical to apply Alternating Least Squares (ALS) method here
because it requires calculating the inverse of two matrices with extremely large size.

7Experiment code is provided at https://github.com/hwwang55/JTS-MF.

CIKM’17, November 6–10, Singapore

H. Wang et al.

Fig. 7. The parameter settings of α, β, γ , and dim are the same as in
Section 7.3.1. Fig. 7a, 7b, and 7c consistently demonstrate that JTS-
MF(S) performs best and JTS-MF(G) performs worst among three
types of reduced versions of JTS-MF. Note that JTS-MF(S) only con-
siders users’ social-level similarity and JTS-MF(G) only considers
users’ group-level similarity. Therefore, it could be concluded that
social-level friends are more helpful than group-level friends when
determining users’ voting interest. This is in accordance with our
intuition, since a user typically has much more group-level friends
than social-level friends, which inevitably dilutes its effectiveness
and brings noises into group-level relationship. In addition, the
result in Fig. 7 also demonstrates the effectiveness of the usage of
votings’ similarity. Furthermore, it can be evidently observed that
JTS-MF model outperforms its three reduced versions in all cases,
which proves that the three types of similarities are well combined
in JTS-MF model to achieve much better results.

7.3.3 Comparison of Models. To further compare JTS-MF model
with other baselines, we gradually increase k from 1 to 500 and
report the results in Table 2 with the best performance highlighted
in bold. The value of α, β, and γ for JTS-MF and its reduced models
are the same as in Section 7.3.1. The parameter settings are α = 2,
β = 60, γ = 15 for Topic-MF, α = 8, β = 120, γ = 20 for Semantic-
MF, and dim = 10 for Qi and Pj in all MF-based methods. The
above parameter settings are the optimal results of fine tuning for
given dim. In Table 2, several observations stand out:

• MostPop performs worst among all methods, because Most-
Pop simply recommends the most popular votings to all
users without considering users’ specific interests.

• Topic-MF and Semantic-MF outperforms Basic-MF, which
proves the usage of similarities with respect to topic and se-
mantic helpful for recommending votings. Besides, Semantic-
MF outperforms Topic-MF. This suggests that semantic
information is more accurate than topic information when
measuring similarities through mining short-length texts.
• JTS-MF outperforms Topic-MF and Semantic-MF. This is
the most important observation from Table 2, since it jus-
tifies our aforementioned claim that joint-topic-semantic
model can benefit from both topic and semantic aspects
and achieve better performance.

• The significance of JTS-MF over other models is evident for
small k. However, this margin becomes smaller when k gets
larger, and JTS-MF is even slightly inferior to JTS-MF(S)
when k ≥ 50. This means that users’ group-level simi-
larities and votings’ similarities “drag the feet” of JTS-MF
model when k is large. However, JTS-MF is still preferred
in practice, since a real recommender system would only
recommend a small set of votings to users in general.

7.4 Parameter Sensitivity
We investigate parameter sensitivity in this subsection. Specifically,
we evaluate how different value of trade-off parameters α, β, γ , and
different numbers of latent feature dimensions dim can affect the
performance.

7.4.1 Trade-off parameters. We fix dim = 10, keep two of the
trade-off parameters as 0, and vary the value of the left trade-off

Fig. 6: Convergence of JTS-MF models with respect to Re-
call@10.

7.2 Parameter Settings
We use GibbsLDA++8, an open-source implementation of LDA
using Gibbs sampling, to calculate topic information of words and
documents in JTS-MF and Topic-MF models. We set the number of
topics to 50 and leave all other parameters in LDA as default values.
For word embeddings in JTS-MF and Semantic-MF models, we use
the same settings as follows: length of embedding dimension as 50,
window size as 5, and number of negative samples as 3.

For all MF-based methods, we set the learning rate δ = 0.001 and
regularization weight λ = 0.5 by 10-fold cross validation. Typically,
we set Im = 0.01 in Eq. (18). Taking into consideration the balance
of experimental results and time complexity, we run 200 iterations
for each of the experiment cases. To conduct the recommendation
task, we randomly select 20% of users’ voting records in the dataset
as test set and use the remaining data as the trainning examples for
our JTS-MF model as well as all baselines. The choice of remaining
hyper-parameters (trade-off parameters α, β, γ , and dimension of
latent features dim) is discussed in Section 7.4.

To quantitatively analyze the performance of voting recommen-
dation, in our experiment, we use top-k recall (Recall@k), top-k
precision (Precision@k), and top-k micro-F1 (Micro-F1@k) as the
evaluation metrics.

7.3 Experiment Results

7.3.1

Study of convergence. To study the convergence of JTS-
MF model, we run the learning algorithm up to 200 iterations for
JTS-MF(S) with α = 10, JTS-MF(G) with β = 140, JTS-MF(V) with
γ = 30, JTS-MF with α = 10, β = 140, γ = 30 (dim = 10 for Qi and
Pj in all models), then calculate Recall@10 for every 10 iterations.
The result of convergence of JTS-MF models is plotted in Fig. 6.
From Fig. 6 we can see that, the recall of JTS-MF models rises
rapidly before 100 iterations, and starts to oscillate slightly after
around 150 iterations. The same changing pattern is observed for
all four JTS-MF variants. Therefore, we set the number of learning
iterations as 200 to achieve a balance between running time and
performance of models.

7.3.2

Study of JTS-MF. To study the performance of JTS-MF
model and the effectiveness of three types of similarities, we run
JTS-MF model as well as its three reduced versions on Weibo voting
dataset, and report the results of Recall, Precision, and Micro-F1 in

8GibbsLDA++: http://gibbslda.sourceforge.net

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

(a)

(b)

(c)

Fig. 7: (a) Recall@k, (b) Precision@k, and (c) Micro-F1@k of JTS-MF models.

(a)

(b)

(c)

(d)

Fig. 8: Parameter sensitivity with respect to (a) α, (b) β, (c) γ , and (d) dim.

Table 2: Result of Recall@k, Precision@k, and Micro-F1@k for JTS-MF model and baselines.

Model

Metric

k

50

100

500

JTS-MF(S)

JTS-MF(G)

JTS-MF(V)

JTS-MF

MostPop

Basic-MF

Topic-MF

Semantic-MF

1
0.0097
0.007416
0.008401
0.0065
0.004944
0.005601
0.0071
0.005439
0.006161

0.0099
0.007614
0.008625
0.0042
0.003221
0.003637
0.0063
0.004845
0.005489
0.0076
0.005834
0.006609
0.0093
0.007120
0.008065

2
0.0172
0.006575
0.009511
0.0133
0.005092
0.007365
0.0149
0.005685
0.008223

0.0178
0.006823
0.009868
0.0085
0.003261
0.004721
0.0129
0.004944
0.007151
0.0147
0.005636
0.008152
0.0169
0.006476
0.009368

5
0.0346
0.005300
0.009192
0.0275
0.004212
0.007306
0.0314
0.004805
0.008335

0.0381
0.005834
0.010118
0.0191
0.002921
0.005062
0.0274
0.004192
0.007271
0.0311
0.004766
0.008266
0.0333
0.005102
0.008849

10
0.0558
0.004271
0.007935
0.0457
0.003500
0.006503
0.0502
0.003846
0.007145

0.0606
0.004637
0.008615
0.0313
0.002403
0.004468
0.0446
0.003411
0.006337
0.0495
0.003787
0.007035
0.0545
0.004173
0.007752

20
0.0846
0.003238
0.006238
0.0752
0.002877
0.005542
0.0789
0.003021
0.005819

0.0908
0.003475
0.006695
0.0517
0.001972
0.003804
0.0727
0.002783
0.005361
0.0781
0.002991
0.005761
0.0860
0.003293
0.006342

Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1

0.1529
0.002341
0.004612
0.1360
0.002082
0.004102
0.1387
0.002124
0.004184
0.1520
0.002327
0.004585
0.0974
0.001482
0.002925
0.1368
0.002094
0.004125
0.1395
0.002136
0.004207
0.1471
0.002252
0.004437

0.2229
0.001707
0.003387
0.2051
0.001570
0.003116
0.2049
0.001568
0.003112
0.2187
0.001674
0.003322
0.1455
0.001119
0.002218
0.2050
0.001569
0.003114
0.2076
0.001589
0.003154
0.2142
0.001639
0.003254

0.4392
0.000672
0.001343
0.4216
0.000645
0.001289
0.4176
0.000639
0.001277
0.4297
0.000658
0.001314
0.3086
0.000469
0.000937
0.4198
0.000643
0.001283
0.4210
0.000644
0.001287
0.4293
0.000657
0.001313

parameter. Then we report Recall@10 in Fig. 8a, 8b, and 8c, respec-
tively.

As shown in Fig. 8a, the Recall@10 increases constantly as α gets
larger and reaches a maximum of 0.0558 when α = 10. This suggests
that the usage of users’ social-level similarity do help to improve the
recommendation performance. However, when α is too large (α =

12), the learning algorithm of JTS-MF is misled to wrong direction
when updating latent features of users and votings, resulting in
performance deterioration. The similar phenomenon are observed
in Fig. 8b and Fig. 8c, too. According to the results, when the
other two trade-off parameters are set to 0, Recall@10 reaches
the maximum when α = 10, β = 140, and γ = 30, respectively.

CIKM’17, November 6–10, Singapore

H. Wang et al.

Therefore, in previous experiments we adopt these optimal settings
for JTS-MF(S), JTS-MF(G), and JTS-MF(V), respectively, and use
their combination as the parameter settings in JTS-MF.

7.4.2 Dimension of latent features. We fix α = 10, β = 0, γ = 0
and tune the dimension of latent features of users and votings from
5 to 90. The result is shown in Fig. 8d. From the figure, we can
see clearly that the recall is increasing when dim gets larger, this
is because latent features with larger number of dimensions have
more capacity to characterize users and votings. But a larger dim
leads to more running time in experiments. Moreover, we notice
that the improvement of performance stagnates after dim reaches
80. On balance, we set dim = 10 in our experiment scenarios to
ensure the experiments can complete within rational time duration.

8 CONCLUSIONS
In this paper, we study the problem of recommending online vot-
ings to users in social networks. We first formalize the voting
recommendation problem and justify the motivation of leveraging
social structure and voting content information. To overcome the
limitations of topic models and semantic models when learning rep-
resentation of voting content, we propose Topic-Enhanced Word
Embedding method to jointly consider topics and semantics of
words and documents. We then propose our Joint-Topic-Semantic-
aware social Matrix Factorization model to learn latent features of
users and votings based on the social network structure and TEWE
representation. We conduct extensive experiments to evaluate JTS-
MF with Weibo voting dataset. The experimental results prove the
competitiveness of JTS-MF against other state-of-the-art baselines
and demonstrate the efficacy of TEWE representation.

ACKNOWLEDGMENTS
This work was partially sponsored by the National Basic Research
973 Program of China under Grant 2015CB352403, the NSFC Key
Grant (No. 61332004), PolyU Project of Strategic Importance 1-ZE26,
and HK-PolyU Grant 1-ZVHZ.

REFERENCES
[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet

[2]

allocation. In Journal of Machine Learning Research. 993–1022.
Jes´us Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Guti´errez.
2013. Recommender systems survey. Knowledge-Based Systems 46 (2013).
[3] Marco Bressan, Stefano Leucci, Alessandro Panconesi, Prabhakar Raghavan,
and Erisa Terolli. 2016. The Limits of Popularity-Based Recommendations, and
the Role of Social Ties. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 745–754.

[4] Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency

Parser using Neural Networks.. In EMNLP. 740–750.

[5] Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander J Smola, Jing Jiang, and
Chong Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie
recommendation (jmars). In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 193–202.

[6] Huiji Gao, Jiliang Tang, Xia Hu, and Huan Liu. 2015. Content-aware point of
interest recommendation on location-based social networks.. In AAAI. 1721–
1727.

[7] Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Pro-

ceedings of National Academy of Sciences (PNAS). 5228–5235.

[8] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 855–864.

[9] Liang Hu, Jian Cao, Guandong Xu, Longbing Cao, Zhiping Gu, and Can Zhu.
2013. Personalized recommendation via cross-domain triadic factorization. In
Proceedings of the 22nd international conference on World Wide Web. ACM, 595–
606.

[10] Hui-Ju Hung, Hong-Han Shuai, De-Nian Yang, Liang-Hao Huang, Wang-Chien
Lee, Jian Pei, and Ming-Syan Chen. 2016. When Social Influence Meets Item
Inference. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 915–924.
Janet Ilieva, Steve Baron, and Nigel M Healey. 2002. Online surveys in marketing
research: Pros and cons. International Journal of Market Research 44, 3 (2002),
361.

[11]

[12] Meng Jiang, Peng Cui, Xumin Chen, Fei Wang, Wenwu Zhu, and Shiqiang Yang.
2015. Social recommendation with cross-domain transferable knowledge. IEEE
Transactions on Knowledge and Data Engineering 27, 11 (2015), 3084–3097.
[13] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-

niques for recommender systems. Computer 42, 8 (2009).

[14] Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Proceedings of the

12th international conference on machine learning. 331–339.

[15] Quoc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences

and Documents.. In ICML, Vol. 14. 1188–1196.

[16] Wu-Jun Li, Dit-Yan Yeung, and Zhihua Zhang. 2011. Generalized latent factor
models for social network analysis. In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI), Barcelona, Spain. 1705.

[17] Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis.

In Mining text data. Springer, 415–463.

[18] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Sun Maosong. 2015. Topical word em-
beddings. In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence
(AAAI). AAAI, 2418–2424.

[19] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social
recommendation using probabilistic matrix factorization. In Proceedings of the
17th ACM conference on Information and knowledge management. ACM, 931–940.
[20] Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007.
Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings
of the 16th international conference on World Wide Web. ACM, 171–180.
[21] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Proceedings of the 26th International Conference on Neural Information Processing
Systems (NIPS). 3111–3119.

[22] Steffen Rendle and Christoph Freudenthaler. 2014. Improving pairwise learning
for item recommendation from implicit feedback. In Proceedings of the 7th ACM
international conference on Web search and data mining. ACM, 273–282.
[23] Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction

by locally linear embedding. science 290, 5500 (2000), 2323–2326.

[24] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltz-
mann machines for collaborative filtering. In Proceedings of the 24th international
conference on Machine learning. ACM, 791–798.

[25] Yue Shi, Alexandros Karatzoglou, Linas Baltrunas, Martha Larson, Nuria Oliver,
and Alan Hanjalic. 2012. CLiMF: learning to maximize reciprocal rank with
collaborative less-is-more filtering. In Proceedings of the sixth ACM conference on
Recommender systems. ACM, 139–146.

[26] Shaojie Tang and Jing Yuan. 2016. Optimizing Ad Allocation in Social Advertising.
In Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management. ACM, 1383–1392.

[27] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning
for recommender systems. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 1235–1244.
[28] Xin Wang, Roger Donaldson, Christopher Nell, Peter Gorniak, Martin Ester, and
Jiajun Bu. 2016. Recommending groups to users using user-group engagement
and time-dependent matrix factorization. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence. AAAI Press, 1331–1337.

[29] Xin Wang, Wei Lu, Martin Ester, Can Wang, and Chun Chen. 2016. Social
Recommendation with Strong and Weak Ties. In Proceedings of the 25th ACM
CIKM. ACM, 5–14.

[30] Zhonghang Xia, Yulin Dong, and Guangming Xing. 2006. Support vector ma-
chines for collaborative filtering. In Proceedings of the 44th annual Southeast
regional conference. ACM, 169–174.

[31] Xiwang Yang, Chao Liang, Miao Zhao, Hongwei Wang, Hao Ding, Yong Liu,
Yang Li, and Junlin Zhang. 2017. Collaborative filtering-based recommendation
of online social voting. IEEE Transactions on Computational Social Systems 4, 1
(2017), 1–13.

[32] Qin Zhang, Jia Wu, Peng Zhang, Guodong Long, Ivor W Tsang, and Chengqi
Zhang. 2016. Inferring Latent Network from Cascade Data for Dynamic Social
Recommendation. In Data Mining (ICDM), 2016 IEEE 16th International Conference
on. IEEE, 669–678.

[33] Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010.

Jointly
modeling aspects and opinions with a MaxEnt-LDA hybrid. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 56–65.

[34] Hengshu Zhu, Enhong Chen, Kuifei Yu, Huanhuan Cao, Hui Xiong, and Jilei
Tian. 2012. Mining personal context-aware preferences for mobile users. In 2012
IEEE 12th International Conference on Data Mining. IEEE, 1212–1217.

Joint Topic-Semantic-aware Social Recommendation
for Online Voting

Hongwei Wang1,2, Jia Wang2, Miao Zhao2, Jiannong Cao2, Minyi Guo1∗
1Shanghai Jiao Tong University, 2The Hong Kong Polytechnic University
wanghongwei55@gmail.com,{csjiawang,csmiaozhao,csjcao}@comp.polyu.edu.hk,guo-my@cs.sjtu.edu.cn

7
1
0
2
 
c
e
D
 
3
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
7
0
0
.
2
1
7
1
:
v
i
X
r
a

ABSTRACT
Online voting is an emerging feature in social networks, in which
users can express their attitudes toward various issues and show
their unique interest. Online voting imposes new challenges on rec-
ommendation, because the propagation of votings heavily depends
on the structure of social networks as well as the content of vot-
ings. In this paper, we investigate how to utilize these two factors
in a comprehensive manner when doing voting recommendation.
First, due to the fact that existing text mining methods such as
topic model and semantic model cannot well process the content of
votings that is typically short and ambiguous, we propose a novel
Topic-Enhanced Word Embedding (TEWE) method to learn word
and document representation by jointly considering their topics
and semantics. Then we propose our Joint Topic-Semantic-aware
social Matrix Factorization (JTS-MF) model for voting recommenda-
tion. JTS-MF model calculates similarity among users and votings
by combining their TEWE representation and structural informa-
tion of social networks, and preserves this topic-semantic-social
similarity during matrix factorization. To evaluate the performance
of TEWE representation and JTS-MF model, we conduct extensive
experiments on real online voting dataset. The results prove the
efficacy of our approach against several state-of-the-art baselines.

KEYWORDS
Online voting; recommender systems; topic-enhanced word em-
bedding; matrix factorization

1 INTRODUCTION
Online voting [31] has recently become a popular function on so-
cial platforms, through which a user can share his opinion towards
various interested subjects, ranging from livelihood issues to en-
tertainment news. More advanced than simple like-dislike type of
votings, some social networks, such as Weibo1, have empowered
users to run their own voting campaigns. Users can freely initiate
votings on any topics of their own interests and customize voting
options. These votings are visible to the friends of initiator, who
can then choose to participate to make the votings further seen by
their friends or simply retweet the votings to their friends. In such
a way, in addition to the system recommendation, a voting can
widely propagate over the network along social paths. The voting
propagation scheme is shown in Fig. 1.

∗M. Guo is the corresponding author.
1http://www.weibo.com.

CIKM’17, Singapore
2017. 978-1-4503-4918-5/17/11. . . $15.00
DOI: 10.1145/3132847.3132889

Fig. 1: Propagation scheme of online voting.

Facing a large volume of diversified votings, a critical challenge
is to present “right” votings to the “right” person. An effective
recommender system is desired to be able to deal with information
overload [2] by precisely locating what votings favor each user most,
thus improves user experience and maximizes user engagement in
votings. Such a recommender system can also benefit a variety of
other online services such as personalized advertising [26], market
research [11], public opinion analysis [17], etc.

Despite the great importance, there is little prior work consider-
ing recommending votings to users in social networks. The chal-
lenges are two-fold. First, the propagation of online votings relies
heavily on the structure of social networks. A user can see the
votings initiated, participated or retweeted by his followees, which
implies that the user is more likely to be exposed to the votings
that his friends are involved in. Moreover, in most social networks,
a user can join different interest groups, which is another type
of social structure that potentially affects users’ voting behavior.
Though several prior works [3, 6, 10, 12, 25, 29, 31, 32] have been
proposed to leverage social network information in recommenda-
tion, it is still an open question how to comprehensively incorporate
structural social information into the task of voting recommenda-
tion considering its propagation pattern. Second, users’ interest
in votings is strongly connected with voting content presented in
question text (e.g., “Who is your favorite movie star?”). Topic model
[1] is regarded as a possible approach to mine the voting interests
through discovering the latent topic distribution of relevant voting
text. However, the voting questions are typically short and lack
sufficient topic information, leading to severe performance degra-
dation of topic models. Alternatively, semantic analytics [21] can
also possibly be used to mine voting interests through learning text
representations. However, such semantic models typically repre-
sent each word using a single vector, making them indiscriminative
for homonymy and polysemy, which are especially common in
voting questions (e.g., “Do you use apple products?” and “Do you
peel apple before eating?”). In brief, these inherent defects of the

CIKM’17, November 6–10, Singapore

H. Wang et al.

above models limit their power in the scenario of social voting
recommendation.

To address aforementioned challenges, in this paper, we propose
a novel Joint Topic-Semantic-aware Matrix Factorization (JTS-MF)
model for online voting recommendation. JTS-MF model considers
social network structure and representation of voting content in
a comprehensive manner. For social network structure, JTS-MF
model fully encodes the information of social relationship and group
affiliation into the objective function. We will further justify the
usage of social network structure in Section 3. For representation
of voting content, we propose a Topic-Enhanced Word Embedding
(TEWE) method to build a multi-prototype word and document2
representation, which jointly considers their topics and semantics.
The key idea of TEWE is to enable each word to have different rep-
resentations under different word topics and different documents.
We will detail TEWE in Section 5. Once obtaining TEWE repre-
sentation for each document, JTS-MF model combines them with
the structural information of social networks to calculate the topic-
semantic-social similarity among users and votings. The reason
of calculating such similarity is that, inspired by Locally Linear
Embedding [23], we try to preserve the similarity among users
and votings during matrix factorization, as it contains abundant
proximity information and can greatly benefit feature learning for
users and votings. JTS-MF model is detailed in Section 6.

We conduct extensive investigation on JTS-MF with real online
voting dataset. The experimental results in Section 7 demonstrate
that JTS-MF model achieves substantial gains compared with base-
lines. The results also prove that TEWE is able to well combine
topic and semantic information of texts and generates a better kind
of document representation.

In summary, the contributions of this paper are as follows:

• We formally formulate the online voting recommendation

problem, which has not been fully investigated yet.

• We indicate that user’s voting behavior is highly correlated
with social network structure by conducting thorough sta-
tistical measurements.

• We propose a novel Topic-Enhanced Word Embedding
model to jointly consider topics and semantics of words
and documents to learn their representation. TEWE takes
advantages of both topic model and semantic model, and
consequently learns more informative embeddings.

• We develop a novel matrix factorization based models,
named JTS-MF, for online voting recommendation. JTS-
MF is able to preserve the topic-semantic-social similarity
among users and votings from original embedding space
during learning process.

• We carry out extensive experiments on real online voting
dataset, the results of which reveal that JTS-MF signif-
icantly outperforms baseline (variant) methods, say for
example, surpassing basic matrix factorization model with
57%, 38% and 25% enhancement in terms of recall for top-1,
top-5 and top-20 recommendation, respectively.

2In this paper, a “document” can be related to a voting, a user or a group. A voting
document is the content of its question, a user document is formed by aggregating
all the documents of votings he participates, and a group document is formed by
aggregating all the documents of users who join the group.

2 RELATED WORK
2.1 Recommender Systems
Roughly speaking, existing recommender systems can be catego-
rized into three classes [2]: content-based, collaborative filtering,
and hybrid methods. Content-based methods [14, 34] make use of
user profiles or item descriptions as features for recommendation.
Collaborative filtering methods [22, 25, 28, 31] use either explicit
feedback (e.g., users’ ratings on items) or implicit feedback (e.g.,
users’ browsing records about items) data of user-item interactions
to find user preference and make the recommendation. In addition,
various models are incorporated into collaborative filtering, such
as Support Vector Machine [30], Restricted Boltzmann Machine
[24], and Stacked Denoising Auto Encoder [27]. Hybrid methods
[9, 16] combine content-based and collaborative filtering models
in many hybridization approaches, such as weighted, switching,
cascade and feature combination or augmentation.

2.2 Social Recommendation
Traditional recommender systems are vulnerable to data sparsity
problem and cold-start problem. To mitigate this issue, many ap-
proaches have been proposed to utilize social network information
in recommender systems [3, 6, 10, 12, 25, 29, 31, 32]. For exam-
ple, [12] represents a social network as a star-structured hybrid
graph centered on a social domain which connects with other item
domains to help improve the prediction accuracy. [10] investi-
gates the seed selection problem for viral marketing that considers
both effects of social influence and item inference for product rec-
ommendation. [29] studies the effects of strong and weak ties in
social recommendation, and extends Bayesian Personalized Rank-
ing model to incorporate the distinction of strong and weak ties.
However, the above works only utilize users’ social links without
considering the topic and semantic information for mining the
similarities among users and items, which we found quite help-
ful for voting recommendation tasks. Another difference between
these works and ours is that we also take social group affiliation
into consideration, which can further improve the performance of
recommendation.

2.3 Topic and Semantic Language Models
Latent Dirichlet Allocation (LDA) [1] is a well-known generative
topic model that learns the latent topic distributions for documents.
LDA is widely used in sentiment analysis [20], aspects and opin-
ions mining [33], and recommendation [5]. Word2vec [21] is gen-
erally recognized as a neural network model, which learns word
representations that capture precise syntactic and semantic word
relationships. Word2vec as well as associated Skip-Gram model are
extensively used in document classification [15], dependency parser
[4], and network embedding [8]. However, LDA and Word2vec are
not directly applicable in the scenario of voting recommendation
because the content of voting is usually short and ambiguous. As
a combination, [18] tries to learn topical word embeddings based
on both words and their topics. The difference between [18] and
ours is that we also take topics of documents into consideration,
which enables our model to learn a even more discriminative and
informative representations for words and documents.

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

Table 1: Basic Statistics of Weibo Dataset.

# users
# users with votings
# users with groups
# votings

1,011,389
525,589
723,913
185,387

# groups
# user-voting
# user-user
# user-group

299,077
3,908,024
83,636,677
5,643,534

3 BACKGROUND AND DATA ANALYSIS
In this section, we briefly introduce the background of Weibo voting
and present detailed analysis of Weibo voting dataset.

3.1 Background
Weibo is one of the most popular Chinese microblogging website
launched by Sina corporation, which is akin to a hybrid of Twitter
and Facebook platforms. Users on Weibo can follow each other,
write tweets and share with his followers. Users can also join
different groups based on their attributes (e.g., region) or interests
of topics (e.g., career).

Voting3 is one of the embedded features of Weibo. As of January
2013, more than 92 million users have participated in at least one
voting and more than 2.2 million ongoing votings are available on
Weibo every day. Any user can freely initiate, retweet and partici-
pate a voting campaign in Weibo. As shown in Fig. 1, votings can
propagate in two ways. The first way is through social propagation:
a user can see the voting initiated, retweeted or participated by his
followees and potentially participates the voting. The second way
is through Weibo voting recommendation list, which consists of
popular votings and personalized recommendation for each user.

3.2 Data Measurements
Our Weibo voting dataset comes from the technical team of Sina
Weibo, which contains detailed information about votings from No-
vember 2010 to January 2012, as well as other auxiliary information.
Specifically, the dataset includes users’ participation status on each
voting4, content of each voting, social connection between users,
name and category of each group, and user-group affiliation.

3.2.1 Basic statistics. The basic statistics are summarized in
Table 1. From Table 1 we can learn that, each user has 165.4 fol-
lowers/followees, participates 3.9 votings, and joins 5.6 groups on
average. If we only count users who participate at least one vot-
ing and users who join at least on group, the average number of
votings and average number of joined groups of each user is 7.4
and 7.8, respectively. Fig. 2 depicts the distribution curves of the
above statistics, where the meaning of each subfigure is given in
the caption.

To get an intuitive understanding of whether user’s voting be-
havior is correlated with his social relation and group affiliation,
we conduct the following two sets of statistical experiments:

3.2.2 Correlation between the number of common votings of user
pairs and the types of user pairs. We randomly select ten million
user pairs from the set of all users, and count the average number
of votings that the two users both participate under the following
four circumstances: 1) one of the users follows the other in the
pair, i.e., they are social-level friends; 2) the two users are in at

3http://www.weibo.com/vote?is all=1.
4We only know whether a user participated a voting or not, rather than user voting
results, i.e., we do not know which voting option a user chose.

(a)

(c)

(e)

(b)

(d)

(f)

Fig. 2: (a) Distribution of the number of votings participated
by a user; (b) Distribution of the number of participants of a
voting; (c) Distribution of the number of followers/followees
of a user; (d) Distribution of the number of users in a group;
(e) Distribution of the number of votings (may contain du-
plicated votings) participated by all users in a group; (f) Dis-
tribution of the number of groups joined by a user.

least one common group, i.e., they are group-leven friends; 3) the
two users are neither social-level friends nor group-level friends;
4) all cases. The results are plotted in Fig. 3a, which clearly shows
the difference among these cases. In fact, the average number of
common votings of social-level friends (3.54 × 10−4) and group
level friends (1.79 × 10−4) are 17.4 and 8.8 times higher than that of
“strangers” (2.04 × 10−5). The results demonstrate that if two users
are social-level or group-level friends, they are likely to participate
more votings in common.

3.2.3 Correlation between the probability of two users being
friends and whether they participate common voting. We first ran-
domly select ten thousand votings from the set of all votings. For
each sampled voting vj , we calculate the probability that two of
its participants are social-level or group-level friends, i.e., pj =
# of social/group-level friends among participants of vj
, where nj is the num-
nj ×(nj −1)/2
ber of vj ’ participants. We calculate pj over all sampled votings
and plot the average result (blue bar) in Fig. 3b. For comparison, we
also plot the result for randomly sampled set of users (green bar)
in Fig. 3b. It is clear that if two users ever participated common
voting, they are more likely to be social-level or group-level friends.
In fact, probabilities of two users being social-level or group-level
friends are raised by 5.3 and 3.6 times given the observation that
they are with common voting.

CIKM’17, November 6–10, Singapore

H. Wang et al.

(a)

(b)

Fig. 3: (a) Average number of common votings participated
by user ui and uk in four cases: 1. ui follows/is followed by
uk ; 2. ui and uk are in at least one common group; 3. ui and
uk have no social-level and group-level relationship; 4. all
cases; (b) Probability of two users being social-level or group-
level friends in two cases: 1. they ever participated at least
one common voting; 2. they are randomly sampled.

The above two findings effectively prove the strong correlation
between voting behavior and social network structure, which mo-
tivates us to take users’ social relation and group affiliation into
consideration when making voting recommendation.

4 PROBLEM FORMULATION
In this paper, we consider the problem of recommending Weibo
votings to users. We denote the set of all users, the set of all votings,
and the set of all groups by U = {u1, ..., uN }, V = {v1, ..., vM },
and G = {G1, ..., GL }, respectively. Moreover, we model three
types of relationship in Weibo platform: user-voting, user-user, and
user-group relationship as follows:

(1) The user-voting relationship for ui and vj is defined as

=

(1)

Iui,vj

i f ui participates vj ;
otherwise.

(cid:40)1,
0,
(2) The user-user relationship for ui and uk
(cid:40)1,
;
0,
to denote the set of ui ’s followees, and
to denote the set of ui ’s followers (“+” means “out”

i f ui f ollows uk
otherwise.

We further use F +
i
use F −
i
and “−” means “in”).

is defined as

Iui,uk

(2)

=

(3) The user-group relationship for ui and Gc is defined as

Iui,Gc

=

(cid:40)1,
0,

i f ui joins Gc ;
otherwise.

(3)

Given the above sets of users and votings as well as three types
of relationship, we aim to recommend a list of votings for each user,
in which the votings are not participated by the user but may be
interesting to him.

5 JOINT-TOPIC-SEMANTIC EMBEDDING
In this section, we explain how to learn the embeddings of users,
votings, and groups in a joint topic and semantic way, and apply the
embeddings to calculate similarities. We first introduce the methods
of learning topic information and semantic information by LDA and
Skip-Gram models, respectively, and propose our method which
combines these two models to learn more powerful embeddings.

5.1 Topic Distillation
In this subsection, we introduce how to profile users, votings, and
groups in terms of topic interest distribution by performing topic
distillation on the associated textual content information.

In general, LDA is a popular generative model to discover latent
topic information from a collection of documents [1]. In LDA, each
document d is represented as a multinomial distribution Θ
over a
d
set of topics, and each topic z is also represented as a multinomial
distribution Φz over a number of words. Subsequently, each word
position l in document d is assigned a topic zd,l
,
according to Θ
d
and the word wd,l
. By LDA
approach, the topic distribution for each document and the topic
assignment for each word can be obtained, which would be utilized
later in our proposed model.

is finally generated according to Φzd,l

’s and dvj

= ∪{dui |Iui,Gc

5. The document dui

= 1}, and the document dGc

Here, we discuss how to apply LDA in the scenario of Weibo
voting. According to the Weibo voting dataset, each voting vj
associates a sentence of question, which can be regarded as doc-
for user ui can thus be formed by
ument dvj
aggregating the content of all votings he participates, i.e., dui
=
for group Gc is formed by
∪{dvj |Iui,vj
aggregating documents of all its members, i.e., dGc
=
1}. Note that though our target is to learn the topic distributions of
all users, votings, and groups, it is inadvisable to train LDA model
on dui
’s because: (1) the entitled sentence associated with
a single voting is typically short-presented and topic-ambiguous;
(2) even with user-level voting content aggregation, some docu-
ments of inactive users are not long enough to accurately extract
the authentic topic distribution, yet showing relatively flat distribu-
tion over all the topics. Therefore, we choose to feed group-level
aggregated documents dGc
’s to LDA model as training samples.
The process of group-level voting content aggregation will cover
all the content the affiliated users are interested in and help better
identify their interests in terms of voting topic.
Denote Dir(α ) as the Dirichlet prior of Θ
d

, and Dir(β) as the
Dirichlet prior of Φz . Given α and β, the joint distribution of
document-topic distributions Θ, topic-word distributions Φ, topics
of words z, and a set of words w is

p(Θ, Φ, z, w |α , β)
z p(Φz |β) · (cid:206)
= (cid:206)
d

(cid:18)
p(Θ

d |α ) (cid:206)
l

(cid:16)
p(zd,l |Θ

d )p(wd,l |Φzd,l )

(cid:17) (cid:19)

,

(4)
where d traverses all group-level aggregated documents. In general,
it is computationally intractable to directly maximize the joint
likelihood in Eq. (4), thus Gibbs Sampling [7] is usually applied
to estimate the posterior probability p(z|w, α , β) and solve Θ, Φ
(w )
the
iteratively. Denote θ
the z-th component of Θ
d
z
w-th component of Φz . With the sampling results, Θ
and Φz can
d
be estimated as:

, and ϕ

(z)
d

θ

(z)
d
(w )
ϕ
z

(z)
= (cid:0)n
d
(w )
= (cid:0)n
z

+ α (z)(cid:1)/(cid:0) (cid:205)
z (n
+ β (w )(cid:1)/(cid:0) (cid:205)

+ α (z))(cid:1),
(w )
z
where α (z) is the z-th component of α , β (w ) is the w-th component
(w )
of β, n
z

z = 1, ..., Z ,
+ β (w ))(cid:1), w = 1, ..., V ,

is the observation counts of topic z for document d, n

(z)
d
w (n

(5)

(z)
d

is segmented by Jieba (https://github.com/fxsjy/jieba) and all stop words are

5dvj
removed.

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

L(D) =

log p(wt +c |wt ),

(6)

L(D) =

log p((cid:104)wt +c , zt +c , zd

t +c (cid:105)|(cid:104)wt , zt , zd

t (cid:105)),

(8)

is the frequency of word w assigned as topic z, Z is the number of
topics and V is vocabulary size.

So far, we have obtained the topic assignment for each word and
topic distribution for each group. Topic distributions for users and
votings can thus be inferred by using the learned model and Gibbs
Sampling, which is similar to the calculation of θ

in Eq. (5).

(z)
d

5.2 Semantic Distillation
In this subsection, we introduce how to profile users, votings, and
groups in terms of semantic information. Word embedding, which
represents each word using a vector, is widely used to capture
semantic information of words. Skip-Gram model is a well-known
framework for word embedding, which finds word representation
that are useful for predicting surrounding words in a document
given a target word in a sliding window [21]. More formally, given
a word sequence D = {w1, w2, . . . , wT }, the objective function of
Skip-Gram is to maximize the average log probability

1

T

T
(cid:213)

t =1

(cid:213)

−k ≤c ≤k
c(cid:44)0

where k is the training context size of the target word, which
can be a function of the centered work wt . The basic Skip-Gram
formulation defines p(wi |wt ) using the softmax function as follows:

p(wi |wt ) =

(cid:205)

exp(w(cid:62)

i wt )
exp(w(cid:62)wt )

,

w ∈V
where wi and wt are the vector representation of context word wi
and target word wt , respectively, and V is the vocabulary. To avoid
traversing the entire vocabulary, hierarchical softmax or negative
sampling are used in general during learning process [21].

(7)

5.3 Topic-Enhanced Word Embedding
In this subsection, we propose a joint topic and semantic learning
model, named Topic-Enhanced Word Embedding (TEWE), to ana-
lyze documents of users, votings, and groups. The motivation of
proposed TEWE is based on the following two observations: (1)
The voting content typically involves short texts. Even we infer
the topic distribution for each voting based on the learned topic-
word distribution from group-level aggregated documents, it is
still topic-ambiguous to some extent. (2) The Skip-Gram model
for word embedding assumes that each word always preserves a
single vector, which sometimes is indiscriminate under different
circumstances due to the homonymy and polysemy. Therefore, the
basic idea of TEWE is to preserve topic information of documents
and words when measuring the interaction between target word wt
and context word wi . In this way, a word with different associated
topics has different embeddings, and a word in documents with
different topics has different embeddings, too.

Specifically, rather than solely using the target word w to predict
context words in Skip-Gram, TEWE also jointly utilizes zw , the
topic of the word in a document, as well as zd
, the most likely
w
topic of the document that the word belongs to. Recall that in Sec-
tion 5.1, we have obtained the topic of each word zw and topic
can be calculated
distributions of each document Θ
d

, thus zd
w

(a) Skip-Gram

(b) TEWE

Fig. 4: Comparison between Skip-Gram and TEWE. The
gray circles in (a) indicate the embeddings of original words,
while the blue circles in (b) indicate the TEWE representa-
tion of pseudo words, which preserves semantic and topic
information of words and documents.

(z)
d

, where θ

= arg maxz θ

(z)
is the probability that docu-
as zd
w
d
ment d belongs to topic z, as introduced in Eq. (5). TEWE regards
each word-topics triplet (cid:104)w, zw , zd
w (cid:105) as a pseudo word and learns a
unique vector wz,zd for it. The objective function of TEWE is as
follows:

(cid:213)

1

T

T
(cid:213)

t =1

−k ≤c ≤k
c(cid:44)0
(cid:105)|(cid:104)wt , zt , zd

(cid:17) .

(cid:17)

t (cid:105)) =

wz,zd
i

p((cid:104)wi , zi , zd

i (cid:105)|(cid:104)wt , zt , zd

where p((cid:104)wi , zi , zd
i

t (cid:105)) is a softmax function as
exp (cid:16)
(cid:62)wz,zd
t
exp (cid:16)
wz,zd (cid:62)wz,zd
t
(9)
The comparison between Skip-Gram and TEWE is shown in Fig.
4. Instead of solely utilizing the target and context words as in Skip-
Gram, TEWE further preserves word topic and document topic
along with these words, and incorporates both topic and semantic
information in embedding learning.

(cid:104)w,z,zd (cid:105) ∈ (cid:104)V ,Z,Z (cid:105)

(cid:205)

Once obtaining TEWE representation for each pseudo word, the
representation of each document can be correspondingly derived
by aggregating the embeddings of its containing words weighted
by term frequency-inverse document frequency (TF-IDF) coefficient.
Specifically, for each document d, its TEWE can be calculated as

= (cid:205)

ed

w ∈d

TF-IDF(w, d) · wz,zd

,

(10)

where TF-IDF(w, d) is the product of the raw count of w in d and
the logarithmically scaled inverse fraction of the documents that
contains w, i.e., TF-IDF(w, d) = fw,d · log
(D is the set
of all documents). TEWE document representations can be used in
measuring inter-document similarities. For example, the similarity
can be calculated as the cosine
of two user documents dui
similarity between their TEWE representations, i.e.,

|D |
|d ∈D:w ∈d |

(cid:107)2
This similarity encodes both topic and semantic proximity informa-
tion of user documents, which implicitly reveals the similarity of
voting interests between two users.

e(cid:62)
euk
ui
(cid:107)eui (cid:107)2 (cid:107)euk

and duk

.

6 RECOMMENDATION MODEL
In this section, we present our Joint Topic-Semantic-aware Matrix
Factorization (JTS-MF) model for online social votings, in which
social relationship, group affiliation, and topic-semantic similarities
are combined and taken into account for voting recommendation in

CIKM’17, November 6–10, Singapore

H. Wang et al.

between two users from both topic-semantic interests and their
social influence perspectives.

To avoid the impact of different numbers of followees, we use
the normalized social-level similarity coefficient of users in JTS-MF,
which is defined as

,

=

(cid:205)

(12)

(cid:98)Si,k

Si,k

where F +
i

Si,k
k ∈ F+
i
denotes the set of ui ’s followees in social network.
6.1.2 Normalized group-level similarity coefficient of users. Group-
level similarity coefficient of users is represented by matrix G N ×N ,
which actually measures the topic-semantic similarity among users
from viewpoint of groups. For each ui , the group-level similarity
coefficient with respect to uk

is defined as

= (cid:213)

Gi,k

Iui,G · Iuk,G ·

G ∈ G

e(cid:62)
ui eG
(cid:107)eui (cid:107)2 (cid:107)eG (cid:107)2

,

(13)

where G represents the set of all groups, Iui,G and Iuk,G indicate
whether ui and uk
join group G respectively as described in Eq.
(3), and the last term is the topic-semantic similarity between user
reflects the interest
ui and group G. Essentially speaking, Gi,k
closeness between user ui and its group-level friend uk
by using
ui ’s topic-semantic engagement extent to the corresponding group.
We also normalize the group-level similarity coefficient of users as

where Gi is the set of ui ’s group-level friends in social network.

6.1.3 Normalized similarity coefficient of votings. Similarity coef-
ficient of votings is represented by matrix T M ×M , which is directly
defined as the topic-semantic similarity among votings, i.e.,

(cid:98)Gi,k

=

Gi,k
k ∈ Gi Gi,k

,

(cid:205)

Tj,t =

e(cid:62)
vj evt
(cid:107)evj (cid:107)2 (cid:107)evt (cid:107)2

.

Since the number of votings is typically huge, we only consider
the similarity between two votings with sufficiently high coefficient
value. Specifically, for each voting vj , we define a set of votings Vj
containing those votings whose similarity coefficients with vj ex-
ceed a threshold, i.e., Vj = {vt |Tj,t ≥ threshold}. Correspondingly,
the similarity coefficient of votings are normalized as

(cid:98)Tj,t =

Tj,t
t ∈Vj Tj,t

.

(cid:205)

6.2 Objective Function
Using the notations listed above, the objective function of JTS-MF
can be written as

L =

1
2

N
(cid:213)

M
(cid:213)

i =1

j=1

(cid:16)

I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17) 2

+ α
2

N
(cid:213)

i =1

(cid:13)
(cid:13)Qi −

(cid:213)

k ∈F+
i

(cid:98)Si,k Qk

(cid:13)
(cid:13)

2
2

+ β
2

N
(cid:213)

i =1

(cid:13)
(cid:13)Qi −

(cid:213)

k ∈Gi

(cid:98)Gi, k Qk

(cid:13)
(cid:13)

2

2 + γ
2

M
(cid:213)

j=1

(cid:13)
(cid:13)Pj −

(cid:213)

t ∈Vj

(cid:98)Tj, t Pt

(cid:16)

(cid:13)
(cid:13)

2

2 + λ
2

(cid:107)Q (cid:107)2
F

+ (cid:107)P (cid:107)2
F

(cid:17)

.

The basic idea of the objective function in Eq. (17) lies in that,
besides considering explicit feedback between users and votings,
we also impose penalties on the discrepancy among features of

(14)

(15)

(16)

(17)

Fig. 5: Graphic Model of JTS-MF.

a comprehensive manner. Motivated by Locally Linear Embedding
[23] which tries to preserve the local linear dependency among
inputs in the low-dimensional embedding space, we expect to keep
inter-user and inter-voting topic-semantic similarities in latent
feature space as well. To this end, in JTS-MF model, while the
rating Ri, j is factorized as user latent feature Qi and voting latent
feature Pj, we deliberately enforce Qi and Pj to be dependent on
their social-topic-semantic similar counterparts, respectively. The
graphic model of JTS-MF model is as shown in Figure 5.

6.1 Similarity Coefficients
In order to characterize the influence of inter-user common interests
and inter-voting content relevance, we first introduce the following
three similarity coefficients:

• Normalized social-level similarity coefficient of users: (cid:98)Si,k

,

where uk

is the social-level friend of ui ;

• Normalized group-level similarity coefficient of users: (cid:98)Gi,k

,

where uk

is the group-level friend of ui ;

• Normalized similarity coefficient of voting: (cid:98)Tj,t , where vj

and vt are two distinct votings.

Generally speaking, in JTS-MF, the latent feature Qi for user ui
is tied up with the latent feature of his social-level and group-level
friends who are weighted through (cid:98)Si,k
’s. Likewise, the
latent feature Pj for voting vj is tied up with the latent feature of
its similar votings, which are weighted through (cid:98)Tj,t ’s.

’s and (cid:98)Gi,k

·

Si,k

is defined as

= Iui,uk ·

e(cid:62)
ui euk
(cid:107)eui (cid:107)2 (cid:107)euk (cid:107)2

6.1.1 Normalized social-level similarity coefficient of users. Social-
level similarity coefficient of users is represented by matrix S N ×N ,
which incorporates both social relationship and user-user topic-
semantic similarity. Specifically, for each ui , the social-level simi-
larity coefficient with respect to uk
(cid:115) d−
+ d
k
d+
+ d−
i
k
indicates whether ui follows uk
is the out-degree of ui in the social network (i.e., d
in the social network (i.e., d−
is the in-degree of uk
k

as described in Eq. (2),
where Iui,uk
= |F +
+
+
|),
d
i
i
i
= |F −
d−
|), d
k
k
e(cid:62)
euk
is the smoothing constant (d = 1 in this paper), and
ui
(cid:107)eui (cid:107)2 (cid:107)euk
is the topic-semantic similarity between user ui and user uk
mentioned in Section 5.3.

+d
k
+d −
k
of local authority and local hub value to differentiate the impor-
counts the closeness
tance of different users [19]. Essentially, Si,k

incorporates the information

(cid:114) d −
d +
i

+ d

(cid:107)2
as

(11)

+d

,

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

similar users and similar votings. We give detailed explanation as
follows. The first term of Eq. (17) measures the mean squared error
between prediction and ground truth, where I (cid:48)
is the training
i, j
weights defined as

I (cid:48)
i, j

=

(cid:40)1,
Im,

i f ui participates vj
otherwise

.

(18)

defined in Eq. (1) as the
The reason we do not directly use Iui,vj
training weights is because we found a small and positive Im makes
the training process more robust and can greatly improve the results.
Ri, j is the actual rating of user ui on voting vj , and Qi P (cid:62)
is the
j
predicted value of Ri, j . Without loss of generality, in JTS-MF model,
we set Ri, j = 1 if ui participates vj and Ri, j = 0 otherwise.

’s. Weight (cid:98)Si,k

’s address both the followee uk

The second, third, and fourth terms of Eq. (17) measure the
penalty of discrepancy among similar users and similar votings. In
particular, the second term enforces user ui ’s latent feature Qi to
be similar to the weighted average of his like-minded followees’
profiles Qk
’s social
influence on ui as well as the degree of common voting interests
shared between uk
and ui . The third term enables user ui ’s latent
feature Qi to be similar to the weighted average of all his group
’s emphasize both the same group
peers’ profiles Qk
affiliation of users ui and uk
and also the tie strength between ui and
the associated group with respect to voting interests. This implies
that, among all group-level friends, ui would have more similar
latent feature with the users who frequently join those groups ui is
interested in. The fourth term ensures voting vj ’s latent feature Pj
to be similar to the weighted average of votings that share similar
topic-semantic information with vj .

’s. Weight (cid:98)Gi,k

Finally, the last term of Eq. (17) is the regularizer to prevent

over-fitting, and λ is the regularization weight.

The trade-off among user social-level similarities, user group-
level similarities, and voting similarities is controlled by the pa-
rameters α, β, and γ , respectively. Obviously, users’ social-level
similarity, users’ group-level similarity, or votings’ similarity is/are
ignored if α, β, or γ is/are set to 0, while increasing these values
shifts the trade-off more towards their respective directions.

6.3 Learning Algorithm
To solve the optimization in Eq. (17), we apply batch gradient
descent approach to minimize the objective function6. The gradients
of loss function in Eq. (17) with respect to each variable Qi and Pj
are as follows:

(cid:16)

−I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17)

Pj

∂L
∂Qi

=

M
(cid:213)

j =1
(cid:18)

(cid:213)

k ∈F+
i
(cid:213)

k ∈Gi

(cid:1) + (cid:213)
t ∈F−
i

(cid:1) + (cid:213)
t ∈U

+ α

(cid:0)Qi −

(cid:98)Si,k Qk

−(cid:98)St,i

(cid:0)Qt −

(cid:98)St,k Qk

(cid:18)

+ β

(cid:0)Qi −

(cid:98)Gi,k Qk

− (cid:98)Gt,i

(cid:0)Qt −

(cid:98)Gt,k Qk

+ λQi,

(cid:213)

k ∈F+
t
(cid:213)

k ∈Gi

(cid:19)

(cid:1)

(cid:19)

(cid:1)

(19)

(cid:16)

−I (cid:48)
i, j

Ri, j − Qi P (cid:62)
j

(cid:17)

Qi

∂L
∂Pj

=

N
(cid:213)

i =1
(cid:18)

+ γ

(cid:0)Pj −

(cid:98)Tj, t Pt

−(cid:98)Tk, j

(cid:0)Pk −

(cid:213)

t ∈Vj

(cid:1) + (cid:213)
k ∈Vj

(cid:213)

t ∈Vk

(cid:19)

(cid:1)

(cid:98)Tk, t Pt

+ λPj .

(20)

To clearly understand the gradients in Eq. (19) and (20), it is
worth pointing out that Qi appears not only in the i-th sub-term
in the second and third lines of Eq. (17) explicitly, but also exists
in other t-th sub-terms followed by (cid:98)St,i or (cid:98)Gt,i , where ui plays as
one of the followees or group members of other users. The case is
similar for Pj. Given the gradients in Eq. (19) and (20), we list the
pseudo code of the learning algorithm for JTS-MF as follows:

(1) Randomly initialize Q and P;
(2) In each iteration of the algorithm, do:
a) update each Qi : Qi ← Qi − δ ∂L
;
∂Qi
b) update each Pj: Pj ← Pj − δ ∂L
;
∂Pj
until convergence, where δ is an configurable learning rate.

7 EXPERIMENTS
In this section, we evaluate our proposed JTS-MF model on the
aforementioned Weibo voting dataset7. We first introduce base-
lines and parameter settings used in the experiments, and then
present the experimental results of JTS-MF and the comparison
with baselines.

7.1 Baselines
We use the following seven methods as the baselines against JTS-
MF model. Note that the first three baselines are reduced versions
of JTS-MF, which only consider one particular type of similarity
among users or votings.

• JTS-MF(S) only considers social-level similarity of users,

i.e., sets β, γ = 0 in JTS-MF model.

• JTS-MF(G) only considers group-level similarity of users,

i.e., sets α, γ = 0 in JTS-MF model.

• JTS-MF(V) only considers similarity of votings, i.e., sets

α, β = 0 in JTS-MF model.

• MostPop recommends the most popular items to users,
i.e., the votings that have been participated by the most
numbers of users.

• Basic-MF [13] simply uses matrix factorization method
to predict the user-voting matrix while ignores additional
social relation, group affiliation and voting content infor-
mation.

for ed

• Topic-MF [1] is similar to JTS-MF except that we substi-
when calculating similarities in Eq. (11),
tute Θd
(13), and (15). Note that Θd
can also be viewed as the em-
bedding of document with respect to topics. Therefore,
Topic-MF only considers the topic similarity among users
and votings.

• Semantic-MF is similar to JTS-MF except that we use
the Skip-Gram model in [21] directly to learn the word
embeddings. Therefore, Semantic-MF only considers the
semantic similarity among users and votings.

6Note that it is impractical to apply Alternating Least Squares (ALS) method here
because it requires calculating the inverse of two matrices with extremely large size.

7Experiment code is provided at https://github.com/hwwang55/JTS-MF.

CIKM’17, November 6–10, Singapore

H. Wang et al.

Fig. 7. The parameter settings of α, β, γ , and dim are the same as in
Section 7.3.1. Fig. 7a, 7b, and 7c consistently demonstrate that JTS-
MF(S) performs best and JTS-MF(G) performs worst among three
types of reduced versions of JTS-MF. Note that JTS-MF(S) only con-
siders users’ social-level similarity and JTS-MF(G) only considers
users’ group-level similarity. Therefore, it could be concluded that
social-level friends are more helpful than group-level friends when
determining users’ voting interest. This is in accordance with our
intuition, since a user typically has much more group-level friends
than social-level friends, which inevitably dilutes its effectiveness
and brings noises into group-level relationship. In addition, the
result in Fig. 7 also demonstrates the effectiveness of the usage of
votings’ similarity. Furthermore, it can be evidently observed that
JTS-MF model outperforms its three reduced versions in all cases,
which proves that the three types of similarities are well combined
in JTS-MF model to achieve much better results.

7.3.3 Comparison of Models. To further compare JTS-MF model
with other baselines, we gradually increase k from 1 to 500 and
report the results in Table 2 with the best performance highlighted
in bold. The value of α, β, and γ for JTS-MF and its reduced models
are the same as in Section 7.3.1. The parameter settings are α = 2,
β = 60, γ = 15 for Topic-MF, α = 8, β = 120, γ = 20 for Semantic-
MF, and dim = 10 for Qi and Pj in all MF-based methods. The
above parameter settings are the optimal results of fine tuning for
given dim. In Table 2, several observations stand out:

• MostPop performs worst among all methods, because Most-
Pop simply recommends the most popular votings to all
users without considering users’ specific interests.

• Topic-MF and Semantic-MF outperforms Basic-MF, which
proves the usage of similarities with respect to topic and se-
mantic helpful for recommending votings. Besides, Semantic-
MF outperforms Topic-MF. This suggests that semantic
information is more accurate than topic information when
measuring similarities through mining short-length texts.
• JTS-MF outperforms Topic-MF and Semantic-MF. This is
the most important observation from Table 2, since it jus-
tifies our aforementioned claim that joint-topic-semantic
model can benefit from both topic and semantic aspects
and achieve better performance.

• The significance of JTS-MF over other models is evident for
small k. However, this margin becomes smaller when k gets
larger, and JTS-MF is even slightly inferior to JTS-MF(S)
when k ≥ 50. This means that users’ group-level simi-
larities and votings’ similarities “drag the feet” of JTS-MF
model when k is large. However, JTS-MF is still preferred
in practice, since a real recommender system would only
recommend a small set of votings to users in general.

7.4 Parameter Sensitivity
We investigate parameter sensitivity in this subsection. Specifically,
we evaluate how different value of trade-off parameters α, β, γ , and
different numbers of latent feature dimensions dim can affect the
performance.

7.4.1 Trade-off parameters. We fix dim = 10, keep two of the
trade-off parameters as 0, and vary the value of the left trade-off

Fig. 6: Convergence of JTS-MF models with respect to Re-
call@10.

7.2 Parameter Settings
We use GibbsLDA++8, an open-source implementation of LDA
using Gibbs sampling, to calculate topic information of words and
documents in JTS-MF and Topic-MF models. We set the number of
topics to 50 and leave all other parameters in LDA as default values.
For word embeddings in JTS-MF and Semantic-MF models, we use
the same settings as follows: length of embedding dimension as 50,
window size as 5, and number of negative samples as 3.

For all MF-based methods, we set the learning rate δ = 0.001 and
regularization weight λ = 0.5 by 10-fold cross validation. Typically,
we set Im = 0.01 in Eq. (18). Taking into consideration the balance
of experimental results and time complexity, we run 200 iterations
for each of the experiment cases. To conduct the recommendation
task, we randomly select 20% of users’ voting records in the dataset
as test set and use the remaining data as the trainning examples for
our JTS-MF model as well as all baselines. The choice of remaining
hyper-parameters (trade-off parameters α, β, γ , and dimension of
latent features dim) is discussed in Section 7.4.

To quantitatively analyze the performance of voting recommen-
dation, in our experiment, we use top-k recall (Recall@k), top-k
precision (Precision@k), and top-k micro-F1 (Micro-F1@k) as the
evaluation metrics.

7.3 Experiment Results

7.3.1

Study of convergence. To study the convergence of JTS-
MF model, we run the learning algorithm up to 200 iterations for
JTS-MF(S) with α = 10, JTS-MF(G) with β = 140, JTS-MF(V) with
γ = 30, JTS-MF with α = 10, β = 140, γ = 30 (dim = 10 for Qi and
Pj in all models), then calculate Recall@10 for every 10 iterations.
The result of convergence of JTS-MF models is plotted in Fig. 6.
From Fig. 6 we can see that, the recall of JTS-MF models rises
rapidly before 100 iterations, and starts to oscillate slightly after
around 150 iterations. The same changing pattern is observed for
all four JTS-MF variants. Therefore, we set the number of learning
iterations as 200 to achieve a balance between running time and
performance of models.

7.3.2

Study of JTS-MF. To study the performance of JTS-MF
model and the effectiveness of three types of similarities, we run
JTS-MF model as well as its three reduced versions on Weibo voting
dataset, and report the results of Recall, Precision, and Micro-F1 in

8GibbsLDA++: http://gibbslda.sourceforge.net

Joint Topic-Semantic-aware Social Recommendation for Online Voting

CIKM’17, November 6–10, Singapore

(a)

(b)

(c)

Fig. 7: (a) Recall@k, (b) Precision@k, and (c) Micro-F1@k of JTS-MF models.

(a)

(b)

(c)

(d)

Fig. 8: Parameter sensitivity with respect to (a) α, (b) β, (c) γ , and (d) dim.

Table 2: Result of Recall@k, Precision@k, and Micro-F1@k for JTS-MF model and baselines.

Model

Metric

k

50

100

500

JTS-MF(S)

JTS-MF(G)

JTS-MF(V)

JTS-MF

MostPop

Basic-MF

Topic-MF

Semantic-MF

1
0.0097
0.007416
0.008401
0.0065
0.004944
0.005601
0.0071
0.005439
0.006161

0.0099
0.007614
0.008625
0.0042
0.003221
0.003637
0.0063
0.004845
0.005489
0.0076
0.005834
0.006609
0.0093
0.007120
0.008065

2
0.0172
0.006575
0.009511
0.0133
0.005092
0.007365
0.0149
0.005685
0.008223

0.0178
0.006823
0.009868
0.0085
0.003261
0.004721
0.0129
0.004944
0.007151
0.0147
0.005636
0.008152
0.0169
0.006476
0.009368

5
0.0346
0.005300
0.009192
0.0275
0.004212
0.007306
0.0314
0.004805
0.008335

0.0381
0.005834
0.010118
0.0191
0.002921
0.005062
0.0274
0.004192
0.007271
0.0311
0.004766
0.008266
0.0333
0.005102
0.008849

10
0.0558
0.004271
0.007935
0.0457
0.003500
0.006503
0.0502
0.003846
0.007145

0.0606
0.004637
0.008615
0.0313
0.002403
0.004468
0.0446
0.003411
0.006337
0.0495
0.003787
0.007035
0.0545
0.004173
0.007752

20
0.0846
0.003238
0.006238
0.0752
0.002877
0.005542
0.0789
0.003021
0.005819

0.0908
0.003475
0.006695
0.0517
0.001972
0.003804
0.0727
0.002783
0.005361
0.0781
0.002991
0.005761
0.0860
0.003293
0.006342

Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1
Recall
Precision
Micro-F1

0.1529
0.002341
0.004612
0.1360
0.002082
0.004102
0.1387
0.002124
0.004184
0.1520
0.002327
0.004585
0.0974
0.001482
0.002925
0.1368
0.002094
0.004125
0.1395
0.002136
0.004207
0.1471
0.002252
0.004437

0.2229
0.001707
0.003387
0.2051
0.001570
0.003116
0.2049
0.001568
0.003112
0.2187
0.001674
0.003322
0.1455
0.001119
0.002218
0.2050
0.001569
0.003114
0.2076
0.001589
0.003154
0.2142
0.001639
0.003254

0.4392
0.000672
0.001343
0.4216
0.000645
0.001289
0.4176
0.000639
0.001277
0.4297
0.000658
0.001314
0.3086
0.000469
0.000937
0.4198
0.000643
0.001283
0.4210
0.000644
0.001287
0.4293
0.000657
0.001313

parameter. Then we report Recall@10 in Fig. 8a, 8b, and 8c, respec-
tively.

As shown in Fig. 8a, the Recall@10 increases constantly as α gets
larger and reaches a maximum of 0.0558 when α = 10. This suggests
that the usage of users’ social-level similarity do help to improve the
recommendation performance. However, when α is too large (α =

12), the learning algorithm of JTS-MF is misled to wrong direction
when updating latent features of users and votings, resulting in
performance deterioration. The similar phenomenon are observed
in Fig. 8b and Fig. 8c, too. According to the results, when the
other two trade-off parameters are set to 0, Recall@10 reaches
the maximum when α = 10, β = 140, and γ = 30, respectively.

CIKM’17, November 6–10, Singapore

H. Wang et al.

Therefore, in previous experiments we adopt these optimal settings
for JTS-MF(S), JTS-MF(G), and JTS-MF(V), respectively, and use
their combination as the parameter settings in JTS-MF.

7.4.2 Dimension of latent features. We fix α = 10, β = 0, γ = 0
and tune the dimension of latent features of users and votings from
5 to 90. The result is shown in Fig. 8d. From the figure, we can
see clearly that the recall is increasing when dim gets larger, this
is because latent features with larger number of dimensions have
more capacity to characterize users and votings. But a larger dim
leads to more running time in experiments. Moreover, we notice
that the improvement of performance stagnates after dim reaches
80. On balance, we set dim = 10 in our experiment scenarios to
ensure the experiments can complete within rational time duration.

8 CONCLUSIONS
In this paper, we study the problem of recommending online vot-
ings to users in social networks. We first formalize the voting
recommendation problem and justify the motivation of leveraging
social structure and voting content information. To overcome the
limitations of topic models and semantic models when learning rep-
resentation of voting content, we propose Topic-Enhanced Word
Embedding method to jointly consider topics and semantics of
words and documents. We then propose our Joint-Topic-Semantic-
aware social Matrix Factorization model to learn latent features of
users and votings based on the social network structure and TEWE
representation. We conduct extensive experiments to evaluate JTS-
MF with Weibo voting dataset. The experimental results prove the
competitiveness of JTS-MF against other state-of-the-art baselines
and demonstrate the efficacy of TEWE representation.

ACKNOWLEDGMENTS
This work was partially sponsored by the National Basic Research
973 Program of China under Grant 2015CB352403, the NSFC Key
Grant (No. 61332004), PolyU Project of Strategic Importance 1-ZE26,
and HK-PolyU Grant 1-ZVHZ.

REFERENCES
[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet

[2]

allocation. In Journal of Machine Learning Research. 993–1022.
Jes´us Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Guti´errez.
2013. Recommender systems survey. Knowledge-Based Systems 46 (2013).
[3] Marco Bressan, Stefano Leucci, Alessandro Panconesi, Prabhakar Raghavan,
and Erisa Terolli. 2016. The Limits of Popularity-Based Recommendations, and
the Role of Social Ties. In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 745–754.

[4] Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency

Parser using Neural Networks.. In EMNLP. 740–750.

[5] Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander J Smola, Jing Jiang, and
Chong Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie
recommendation (jmars). In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 193–202.

[6] Huiji Gao, Jiliang Tang, Xia Hu, and Huan Liu. 2015. Content-aware point of
interest recommendation on location-based social networks.. In AAAI. 1721–
1727.

[7] Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Pro-

ceedings of National Academy of Sciences (PNAS). 5228–5235.

[8] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 855–864.

[9] Liang Hu, Jian Cao, Guandong Xu, Longbing Cao, Zhiping Gu, and Can Zhu.
2013. Personalized recommendation via cross-domain triadic factorization. In
Proceedings of the 22nd international conference on World Wide Web. ACM, 595–
606.

[10] Hui-Ju Hung, Hong-Han Shuai, De-Nian Yang, Liang-Hao Huang, Wang-Chien
Lee, Jian Pei, and Ming-Syan Chen. 2016. When Social Influence Meets Item
Inference. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, 915–924.
Janet Ilieva, Steve Baron, and Nigel M Healey. 2002. Online surveys in marketing
research: Pros and cons. International Journal of Market Research 44, 3 (2002),
361.

[11]

[12] Meng Jiang, Peng Cui, Xumin Chen, Fei Wang, Wenwu Zhu, and Shiqiang Yang.
2015. Social recommendation with cross-domain transferable knowledge. IEEE
Transactions on Knowledge and Data Engineering 27, 11 (2015), 3084–3097.
[13] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-

niques for recommender systems. Computer 42, 8 (2009).

[14] Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Proceedings of the

12th international conference on machine learning. 331–339.

[15] Quoc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences

and Documents.. In ICML, Vol. 14. 1188–1196.

[16] Wu-Jun Li, Dit-Yan Yeung, and Zhihua Zhang. 2011. Generalized latent factor
models for social network analysis. In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI), Barcelona, Spain. 1705.

[17] Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis.

In Mining text data. Springer, 415–463.

[18] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Sun Maosong. 2015. Topical word em-
beddings. In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence
(AAAI). AAAI, 2418–2424.

[19] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social
recommendation using probabilistic matrix factorization. In Proceedings of the
17th ACM conference on Information and knowledge management. ACM, 931–940.
[20] Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007.
Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings
of the 16th international conference on World Wide Web. ACM, 171–180.
[21] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Proceedings of the 26th International Conference on Neural Information Processing
Systems (NIPS). 3111–3119.

[22] Steffen Rendle and Christoph Freudenthaler. 2014. Improving pairwise learning
for item recommendation from implicit feedback. In Proceedings of the 7th ACM
international conference on Web search and data mining. ACM, 273–282.
[23] Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction

by locally linear embedding. science 290, 5500 (2000), 2323–2326.

[24] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltz-
mann machines for collaborative filtering. In Proceedings of the 24th international
conference on Machine learning. ACM, 791–798.

[25] Yue Shi, Alexandros Karatzoglou, Linas Baltrunas, Martha Larson, Nuria Oliver,
and Alan Hanjalic. 2012. CLiMF: learning to maximize reciprocal rank with
collaborative less-is-more filtering. In Proceedings of the sixth ACM conference on
Recommender systems. ACM, 139–146.

[26] Shaojie Tang and Jing Yuan. 2016. Optimizing Ad Allocation in Social Advertising.
In Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management. ACM, 1383–1392.

[27] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning
for recommender systems. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 1235–1244.
[28] Xin Wang, Roger Donaldson, Christopher Nell, Peter Gorniak, Martin Ester, and
Jiajun Bu. 2016. Recommending groups to users using user-group engagement
and time-dependent matrix factorization. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence. AAAI Press, 1331–1337.

[29] Xin Wang, Wei Lu, Martin Ester, Can Wang, and Chun Chen. 2016. Social
Recommendation with Strong and Weak Ties. In Proceedings of the 25th ACM
CIKM. ACM, 5–14.

[30] Zhonghang Xia, Yulin Dong, and Guangming Xing. 2006. Support vector ma-
chines for collaborative filtering. In Proceedings of the 44th annual Southeast
regional conference. ACM, 169–174.

[31] Xiwang Yang, Chao Liang, Miao Zhao, Hongwei Wang, Hao Ding, Yong Liu,
Yang Li, and Junlin Zhang. 2017. Collaborative filtering-based recommendation
of online social voting. IEEE Transactions on Computational Social Systems 4, 1
(2017), 1–13.

[32] Qin Zhang, Jia Wu, Peng Zhang, Guodong Long, Ivor W Tsang, and Chengqi
Zhang. 2016. Inferring Latent Network from Cascade Data for Dynamic Social
Recommendation. In Data Mining (ICDM), 2016 IEEE 16th International Conference
on. IEEE, 669–678.

[33] Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010.

Jointly
modeling aspects and opinions with a MaxEnt-LDA hybrid. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 56–65.

[34] Hengshu Zhu, Enhong Chen, Kuifei Yu, Huanhuan Cao, Hui Xiong, and Jilei
Tian. 2012. Mining personal context-aware preferences for mobile users. In 2012
IEEE 12th International Conference on Data Mining. IEEE, 1212–1217.

