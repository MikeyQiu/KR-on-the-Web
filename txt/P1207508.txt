8
1
0
2
 
p
e
S
 
0
2
 
 
]

V
C
.
s
c
[
 
 
3
v
7
5
1
6
0
.
6
0
8
1
:
v
i
X
r
a

Object Level Visual Reasoning in Videos

Fabien Baradel1, Natalia Neverova2, Christian Wolf1,3,
Julien Mille4, and Greg Mori5

1 Universit´e Lyon, INSA Lyon, CNRS, LIRIS, F-69621, Villeurbanne, France,
firstname.lastname@liris.cnrs.fr
2 Facebook AI Research, Paris, France, nneverova@fb.com
3 INRIA, CITI Laboratory, Villeurbanne, France
4 Laboratoire d’Informatique de l’Univ. de Tours, INSA Centre Val de Loire,
41034, Blois, France, julien.mille@insa-cvl.fr
5 Simon Fraser University, Vancouver, Canada, mori@cs.sfu.ca

https://fabienbaradel.github.io/eccv18_object_level_visual_reasoning/

Abstract. Human activity recognition is typically addressed by detect-
ing key concepts like global and local motion, features related to object
classes present in the scene, as well as features related to the global con-
text. The next open challenges in activity recognition require a level of
understanding that pushes beyond this and call for models with capa-
bilities for ﬁne distinction and detailed comprehension of interactions
between actors and objects in a scene. We propose a model capable of
learning to reason about semantically meaningful spatio-temporal inter-
actions in videos. The key to our approach is a choice of performing
this reasoning at the object level through the integration of state of the
art object detection networks. This allows the model to learn detailed
spatial interactions that exist at a semantic, object-interaction relevant
level. We evaluate our method on three standard datasets (Twenty-BN
Something-Something, VLOG and EPIC Kitchens) and achieve state of
the art results on all of them. Finally, we show visualizations of the in-
teractions learned by the model, which illustrate object classes and their
interactions corresponding to diﬀerent activity classes.

Keywords: Video understanding · Human-object interaction

1

Introduction

The ﬁeld of video understanding is extremely diverse, ranging from extract-
ing highly detailed information captured by speciﬁcally designed motion cap-
ture systems [33] to making general sense of videos from the Web [1]. As in
the domain of image recognition, there exist a number of large-scale video
datasets [6,26,13,12,23,14], which allow the training of high-capacity deep learn-
ing models from massive amounts of data. These models enable detection of key
cues present in videos, such as global and local motion, various object categories
and global scene-level information, and often achieve impressive performance in
recognizing high-level, abstract concepts in the wild.

2

Baradel et al.

Fig. 1. Humans can understand what happened in a video (“the leftmost carrot was
chopped by the person”) given only a pair of frames. Along these lines, the goal of this
work is to explore the capabilities of higher-level reasoning in neural models operating
at the semantic level of objects and interactions.

However, recent attention has been directed toward a more thorough un-
derstanding of human-focused activity in diverse internet videos. These eﬀorts
range from atomic human actions [14] to ﬁne-grained object interactions [13]
to everyday, commonly occurring human-object interactions [12]. This returns
us to a human-centric viewpoint of activity recognition where it is not only the
presence of certain objects / scenes that dictate the activity present, but the
manner, order, and eﬀects of human interaction with these scene elements that
are necessary for understanding. In a sense, this is akin to the problems in current
3D human activity recognition datasets [33], but requires the more challenging
reasoning and understanding of diverse environments common to internet video
collections.

Humans are able to infer what happened in a video given only a few sam-
ple frames. In particular, they can infer complex activities happening between
pairs of frames. This faculty is called reasoning and is a key component of hu-
man intelligence. As an example we can consider the pair of images in Figure 1,
which shows a complex situation involving articulated objects (human, carrots
and knife), the change of location and composition of objects. For humans it is
straightforward to draw a conclusion on what happened (a carrot was chopped
by the human). Humans have this extraordinary ability of performing visual rea-
soning on very complicated tasks while it remains unattainable for contemporary
computer vision algorithms [37,11].

The ability to perform visual reasoning in computer vision algorithms is still
an open problem. Attempts have been made for learning interactions between
diﬀerent entities in images with promising results on Visual Question Answering.
There have been a number of attempts to equip neural models with reasoning
abilities by training them to solve Visual Question Answering (VQA) problems.
Among proposed solutions are prior-less data normalization [27], structuring
networks to model relationships [32,43] as well as more complex attention based

Object Level Visual Reasoning in Videos

3

mechanisms [18]. At the same time, it was shown that high performance on exist-
ing VQA datasets can be achieved by simply discovering biases in the data [21].
We extend these eﬀorts to object level reasoning in videos. Since a video is
a temporal sequence, we leverage time as an explicit causal signal to identify
causal object relations. Our approach is related to the concept of the “arrow of
the time” [28] involving the “one-way direction” or “asymmetry” of time. Causal
event occurs before the event it aﬀects (A
B). In Figure 1 the knife was used
before the carrot switched over to the chopped-up state on the right side. For
a video classiﬁcation problem, we want to identify a causal event A happening
in a video that aﬀects its label B. But instead of identifying this causal event
directly from pixels we want to identify it from an object level perspective. We
believe that such an approach would be able to learn causal signals.

→

Following this hypothesis we propose to make a bridge between object de-
tection and activity recognition. Object detection allows us to extract low-level
information from a scene with all the present object instances and their semantic
meanings. However, detailed activity understanding requires reasoning over these
semantic structures, determining which objects were involved in interactions, of
what nature, and what were the results of these. To compound problems, the
semantic structure of a scene may change during a video (e.g. a new object can
appear, a person may make a move from one point to another one of the scene).
We propose an Object Relation Network (ORN), a neural network mod-
ule for reasoning between detected semantic object instances through space and
time. The ORN has potential to address these issues and conduct relational
reasoning over object interactions for the purpose of activity recognition. A set
of object detection masks ranging over diﬀerent object categories and temporal
occurrences is input to the ORN. The ORN is able to infer pairwise relationships
between objects detected at varying diﬀerent moments in time.

Code and object masks predictions will be publicly available6.

2 Related work

Action Recognition. Action recognition has a long history in computer vi-
sion. Pre-deep learning approaches in action recognition focused on handcrafted
spatio-temporal features including space-time interest points like SIFT-3D, HOG3D,
IDT and aggregated them using bag-of-words techniques. Some hand-crafted rep-
resentations, like dense trajectories [42], still give competitive performance and
are frequently combined with deep learning.

In the recent past, work has shifted to deep learning. Early attempts adapt
2D convolutional networks to videos through temporal pooling and 3D convolu-
tions [2,40]. 3D convolutions are now widely adopted for activity recognition with
the introduction of feature transfer by inﬂating pre-trained 2D convolutional ker-
nels from image classiﬁcation models trained on ImageNet/ILSVRC [31] through
3D kernels [6]. The downside of 3D kernels is their computational complexity

6 https://github.com/fabienbaradel/object_level_visual_reasoning

4

Baradel et al.

and the large number of learnable parameters, leading to the introduction of
2.5D kernels, i.e. separable ﬁlters in the form of a 2D spatial kernel followed by
a temporal kernel [44]. An alternative to temporal convolutions are Recurrent
Neural Networks (RNNs) in their various gated forms (GRUs, LSTMs) [17,9].

Karpathy et al. [20] presented a wide study on diﬀerent ways of connecting
information in spatial and temporal dimensions through convolutions and pool-
ing. On very general datasets with coarse activity classes they have showed that
there was a small margin between classifying individual frames and classifying
videos with more sophisticated temporal aggregation.

Simoyan et al. [35] proposed a widely adopted two-stream architecture for
action recognition which extracts two diﬀerent streams, one processing raw RGB
input and one processing pre-computed optical ﬂow images. The method out-
performed the state of the art, but relies on rather small scale optical ﬂow com-
putations.

In slightly narrower settings, prior information on the video content can allow
more ﬁne-grained models. Articulated pose is widely used in cases where humans
are guaranteed to be present [33]. Pose estimation and activity recognition as a
joint (multi-task) problem has recently shown to improve both tasks [25]. Some-
what related to our work, Structural RNNs [19] perform activity recognition
by integrating features from semantic objects and their relationships. However,
they handle the temporal evolution of tracked objects in videos with a set of
RNNs, each of which corresponds to cliques in a graph which models the spatio-
temporal relationships between these objects. This graph is hand-crafted man-
ually for each application, though related work provides learnable connections
via gating functions [8].

Attention models are a way to structure deep networks in an often generic
way. They are able to iteratively focus attention to speciﬁc parts in the data with-
out requiring prior knowledge about part or object positions. In activity recog-
nition, they have gained some traction in recent years, either as soft-attention
on articulated pose (joints) [36], on feature map cells [34,39], on time [45] or on
parts in raw RGB input through diﬀerentiable crops [3].

When raw video data is globally fed into deep neural networks, they focus
on extracting spatio-temporal features and perform aggregations. It has been
shown that these techniques fail on challenging ﬁne-grained datasets, which re-
quire learning long temporal dependencies and human-object interactions. A
concentrated eﬀort has been made to create large scale datasets to overcome
these issues [13,12,23,14].

Relational Reasoning. Relational reasoning is a well studied ﬁeld for many
applications ranging from visual reasoning [32] to reasoning about physical
systems [4]. Battaglia et al. [4] introduce a fully-diﬀerentiable network physics
engine called Interaction Network (IN). IN learns to predict several physical
systems such as gravitational systems, rigid body dynamics, and mass-spring
systems. It shows impressive results; however, it learns from a virtual environ-
ment, which provides access to virtually unlimited training examples. Following
the same perspective, Santoro et al. [32] introduced Relation Network (RN),

Object Level Visual Reasoning in Videos

5

a plug-in module for reasoning in deep networks. RN shows human-level per-
formance in Visual Question Answering (VQA) by inferring pairwise “object”
relations. However, in contrast to our work, the term “object” in [32] does not
refer to semantically meaningful entities, but to discrete cells in feature maps.
The number of interactions therefore grows with feature map resolutions, which
makes it diﬃcult to scale. Furthermore, a recent study [21] has shown that some
of these results are subject to dataset bias and do not generalize well to small
changes in the settings of the dataset.

In the same line, a recent work [38] has shown promising results on discov-
ering objects and their interactions in an unsupervised manner using training
examples from virtual environments. In [41], attention and relational modules
are combined on a graph structure. From a diﬀerent perspective, [27] show that
relational reasoning can be learned for visual reasoning in a data driven way with-
out any prior using conditional batch normalization with a feature-wise aﬃne
transformation based on conditioning information. In an opposite approach, a
strong structural prior is learned in the form of a complex attention mecha-
nism: in [18], an external memory module combined with attention processes
over input images and text questions, performing iterative reasoning for VQA.
While most of the discussed work has been designed for VQA and for pre-
dictions on physical systems and environments, extensions have been proposed
for video understanding. Reasoning in videos on a mask or segmentation level
has been attempted for video prediction [24], where the goal was to leverage se-
mantic information to be able predict further into the future. Zhou et al [5] have
recently shown state-of-the-art performance on challenging datasets by extend-
ing Relation Network to video classiﬁcation. Their chosen entities are frames, on
which they employ RN to reason on a temporal level only through pairwise frame
relations. The approach is promising, but restricted to temporal contextual in-
formation without an understanding on a local object level, which is provided
by our approach.

Reasoning over sets of objects is somewhat related to reasoning from unstruc-
tured data points, as done in PointNet [29], designed to learn from unordered sets
of points. PointNet shares many properties with DeepSet [46] which is a more
general framework for extracting information from sets of objects. To some ex-
tent, our work is related to PointNet, as we handle unordered sets of objects in a
permutation invariant way. However, we have an object relation viewpoint that
directly reasons over relationships between these semantic entities.

3 Object-level Visual Reasoning in Space and Time

Our goal is to extract multiple types of cues from a video sequence: interactions
between predicted objects and their semantic classes, as well as local and global
motion in the scene. We formulate this objective as a neural architecture with
two heads: an activity head and an object head. Figure 2 gives a functional
overview of the model. Both heads share common features up to a certain layer
shown in red in the ﬁgure. The activity head, shown in orange in the ﬁgure,

6

Baradel et al.

Fig. 2. A functional overview of the model. A global convolutional model extracts
features and splits into two heads trained to predict, respectively activity classes and
object classes. The latter are predicted by pooling over object instance masks, which
are predicted by an additional convolutional model. The object instances are passed
through a visual reasoning module.

is a CNN-based architecture employing convolutional layers, including spatio-
temporal convolutions, able to extract global motion features. However, it is not
able to extract information from an object level perspective. We leverage the
object head to perform reasoning on the relationships between predicted object
instances.

Our main contribution is a new structured module called Object Relation
Network (ORN), which is able to perform spatio-temporal reasoning between
detected object instances in the video. ORN is able to reason by modeling how
objects move, appear and disappear and how they interact between two frames.
In this section, we will ﬁrst describe our main contribution, the ORN network.
We then provide details about object instance features, about the activity head,
and ﬁnally about the ﬁnal recognition task. In what follows, lowercase letters
denote 1D vectors while uppercase letters are used for 2D and 3D matrices or
higher order tensors. We assume that the input of our system is a video of T
frames denoted by X1:T = (Xt)T
t=1 where Xt is the RGB image at timestep t.
The goal is to learn a mapping from X1:T to activity classes y.

3.1 Object Relation Network

ORN (Object Relation Network) is a module for reasoning between semantic
objects through space and time. It captures object moves, arrivals and interac-
tions in an eﬃcient manner. We suppose that for each frame t, we have a set
of objects k with associated features ok
t . Objects and features are detected and
computed by the object head described in Section 3.2.

Reasoning about activities in videos is inherently temporal, as activities fol-
low the arrow of time [28], i.e. the causality of the time dimension imposes that
past actions have consequences in the future but not vice-versa. We handle this
by sampling: running a process over time t, and for each instant t, sampling a

Object Level Visual Reasoning in Videos

7

second frame t(cid:48) with t(cid:48)<t. Our network reasons on objects which interact be-
tween pairs of frames and their corresponding sets of objects Ot(cid:48) = (cid:8)ok
t(cid:48)
and Ot = (cid:8)ok
all input objects from the combined set of both frames:

(cid:9)K(cid:48)
k=1
(cid:9)K
k=1. The goal is to learn a general function deﬁned on the set of

t

gt = g(o1

t(cid:48), . . . , oK(cid:48)

t(cid:48) , o1

t , . . . , oK

t ).

(1)

The objects in this set are unordered, aside for the frame they belong to.

This task is related to a problem raised in the PointNet algorithm [29] dis-
cussed in Section 2. PointNet approximates a general function g over an item
set
as a symmetric function g(cid:48) on transformed elements of
=
S
the set

x1, x2, . . . , xN }
{

g(x1, x2, . . . , xN )

g(cid:48)(h(x1), h(x2), . . . , h(xN )).

(2)

≈

In [29], g(cid:48) is a max pooling operation. The authors show that it allows universal
approximation of continuous sets of functions given that the hidden representa-
tion (the output of the mapping h(
·

)) is of suﬃciently high dimension.
We argue that the approximation in (2) can be extended as follows:

g(x1, x2, . . . , xN )

fφ

≈

hc (

∪i

∈

c xi)

(3)

(cid:33)

(cid:32)

(cid:88)

c

∈C

C

is the set of cliques of a graph deﬁned over the item set

where
is the
concatenation operator and we chose the sum operator as symmetric function.
The input dimension of the non-linearity hc(
) depends on the size of clique c
·
is composed of
but maps to a ﬁxed output dimension H. In the case where
unary cliques only, form (3) decomposes like (2) with the exception of a diﬀerent
symmetry operator (sum instead of max pooling). Choosing diﬀerent graphical
structures through
will lead to diﬀerent terms in the summation and allows
modeling diﬀerent types of interactions between items in the item set.

∪

S

S

C

,

C

Note that the interactions between items in (3) are not exclusively modeled
). Indeed, it is interesting to note, that the graphical decomposi-
through hc(
·
tion provided by
leads to interactions which are diﬀerent from the interactions
the same decomposition would provide when used in a probabilistic graphical
model, like for instance a Markov Random Field (MRF). In particular, a de-
composition into unary terms only, as given in equation (2), does not lead to
independence between items, whereas an MRF with unary terms only is equiva-
lent to a distribution over independent random variables. This is a consequence
of the global mapping fφ(
), which is deﬁned on the sum over all direct inter-
·
actions. Higher-order interactions between several items not directly modeled
) can eventually be learned by the model through
through a non-linearity hc(
·
the joint output space of all hc(
), provided that the dimensionality H of this
·
space is high enough to incorporate all interactions. However, whereas the map-
ping hc(
) provides a direct model of interactions between pairs of items, learning
·
interactions between two items (j, k), which are not directly captured through a

8

Baradel et al.

Fig. 3. ORN in the object head operating on detected instances of objects.

clique c and its corresponding hc(
), requires learning a corresponding subspace
·
).
in the common output space spanned by all hc(
·

This leads to the question of how to deﬁne the trade-oﬀ between the com-
and the output dimensionality H of the mapping
), both of which will determine the complexity of the modeled interactions.
·
will increase the input dimension (and there-
) as well as the computational complexity
·

plexity of the decomposition
hc(
Increasing the size of cliques in
fore the capacity) of the mapping hc(
of the sum operation.

C

C

Inspired by relational networks [32], we chose to directly model inter-frame
interactions between pairs of objects (j, k) and leave modeling of higher-order
interactions to the output space of the mappings hθ and the global mapping fφ:

(4)

(5)

gt =

hθ(oj

t(cid:48), ok
t )

(cid:88)

j,k

rt = fφ(gt, rt

1)

−

In order to better directly model long-range interactions, we make the global
mapping fφ(

) recurrent, which leads to the following form:
·

,
·

where rt represents the recurrent object reasoning state at time t and gt is the
global inter-frame interaction inferred at time t such as described in Equation 4.
In practice, this is implemented as a GRU, but for simplicity we omitted the
) are implemented as an
gates in Equation (5). The pairwise mappings hθ(
·
·
MLP. Figure 3 provides a visual explanation of the object head’s operating
through time.

,

Our proposed ORN diﬀers from [32] in three main points:

Objects have a semantic deﬁnition — we model relationships with respect
to semantically meaningful entities (object instances) instead of feature map
cells which do not have a semantically meaningful spatial extent. We will show
in the experimental section that this is a key diﬀerence.

Object Level Visual Reasoning in Videos

9

Objects are selected from diﬀerent frames — we infer object pairwise
relations only between objects present in two diﬀerent sets. This is a key design
choice which allows our model to reason about changes in object relationships
over time.
Long range reasoning — integration of the object relations over time is re-
). Since reasoning from a full sequence cannot
current by using a RNN for fφ(
·
be done by inferring the relations between two frames, fφ(
) allows long range
·
reasoning on sequences of variable length.

3.2 Object instance features

t

(cid:9)K
The object features Ot = (cid:8)ok
k=1 for each frame t used for the ORN module
described above are computed and collected from local regions predicted by
a mask predictor. Independently for each frame Xt of the input data block,
we predict object instances as binary masks Bk
t and associated object class
predictions ck
t , a distribution over C classes. We use Mask-RCNN [15], which
is able to detect objects in a frame using region proposal networks [30] and
produces a high quality segmentation mask for each object instance.

The objective is to collect features for each object instance, which jointly
describe its appearance, the change in its appearance over time, and its shape,
i.e. the shape of the binary mask. In theory, appearance could also be described
by pooling the feature representation learned by the mask predictor (Mask R-
CNN). However, in practice we choose to pool features from the dedicated object
head such as shown in Figure 2, which also include motion through the spatio-
temporal convolutions shared with the activity head:

t = ROI-Pooling(Ut, Bk
uk
t )

(6)

where Ut is the feature map output by the object head, uk
vector of appearance and appearance change of object k.

t is a D-dimensional

Shape information from the binary mask Bk
t is extracted through the follow-
ing mapping function: bk
) is a MLP. Information about
t ), where gφ(
·
object k in image Xt is given by a concatenation of appearance, shape, and
object class: ok

t = gφ(Bk

t uk

t ck

t ].

t = [ bk

3.3 Global Motion and Context

Current approaches in video understanding focus on modeling the video from
a high-level perspective. By a stack of spatio-temporal convolution and pooling
they focus on learning global scene context information. Eﬀective activity recog-
nition requires integration of both of these sources: global information about the
entire video content in addition to relational reasoning for making ﬁne distinc-
tions regarding object interactions and properties.

In our method, local low-level reasoning is provided through object head
and the ORN module such as described above in Section 3.1. We complement

10

Baradel et al.

this representation by high-level context information described by Vt which are
feature outputs from the activity head (orange block in Figure 2).

We use spatial global average pooling over Vt to output T D-dimensional
feature vectors denoted by vt, where vt corresponds to the context information
of the video at timestep t.

We model the dynamics of the context information through time by employ-

ing a RNN fγ(

) given by:
·

where s is the hidden state of fγ(
context though time.

) and gives cues about the evolution of the
·

st = fγ(vt, st

1)

−

(7)

3.4 Recognition

Given an input video sequence X1:T , the two diﬀerent streams corresponding to
the activity head and the object head result in the two representations h and r,
respectively where h = (cid:80)
trt. Each representation is the hidden
state of the respective GRU, which were described in the preceding subsections.
Recall that h provides the global motion context while r provides the object
reasoning state output by the ORN module. We perform independent linear
classiﬁcation for each representation:

tht and r = (cid:80)

y1 = W h
y2 = Z r

(8)

(9)

where y1, y2 correspond to the logits from the activity head and the object head,
respectively, and W and Z are trainable weights (including biases). The ﬁnal
prediction is done by averaging logits y1 and y2 followed by softmax activation.

4 Network Architectures and feature dimensions

×

×

W

The input RGB images Xt are of size R3
H where W and H correspond to
the width and height and are of size 224 each. The object and activity heads
(orange and green in Figure 2) are a joint convolutional neural network with
Resnet50 architecture pre-trained on ImageNet/ILSVRC [31], with Conv1 and
Conv5 blocks being inﬂated to 2.5D convolutions [44] (3D convolutions with a
separable temporal dimension). This choice has been optimized on the validation
set, as explained in Section 6 and shown in Table 5.

The last conv5 layers have been split into two diﬀerent heads (activity head
and object head). The intermediate feature representations Ut and Vt are of di-
mensions 2048
7, respectively. We provide a higher
×
spatial resolution for the feature maps Ut of the object head to get more precise
local descriptors. This can be done by changing the stride of the initial conv5
layers from 2 to 1. Temporal convolutions have been conﬁgured to keep the same
time temporal dimension through the network.

14 and 2048

14

×

×

×

×

×

T

T

7

Object Level Visual Reasoning in Videos

11

Global spatial pooling of activity features results in a 2048 dimensional fea-
ture vector fed into a GRU with 512 dimensional hidden state st. ROI-Pooling
of object features results in 2048 dimensional feature vectors uk
t . The encoder of
the binary mask is a MLP with one hidden layer of size 100 and outputs a mask
embedding bk
t of dimension 100. The number of object classes is 80, which leads
in total to a 2229 dimensional object feature vector ok
t .

The non-linearity hθ(

) is implemented as an MLP with 2 hidden layers each
·
with 512 units and produces an 512 dimensional output space. fφ(
) is imple-
·
mented as a GRU with a 256 dimension hidden state rt. We use ReLU as the
activation function after each layer for each network.

5 Training

We train the model with a loss split into two terms:

Ltot =

L

(cid:16) ˆy1 + ˆy2
2

(cid:17)

, y

+

(cid:88)

(cid:88)

t

k

(ˆck

t , ck

t ).

L

(10)

L

is the cross-entropy loss. The ﬁrst term corresponds to supervised ac-
where
tivity class losses comparing two diﬀerent activity class predictions to the class
ground truth: ˆy1 is the prediction of the activity head, whereas ˆy2 is the pre-
diction of the object head, as given by Equations (8) and (9), respectively.

The second term is a loss which pushes the features U of the object towards
representations of the semantic object classes. The goal is to obtain features
related to, both, motion (through the layers shared with the activity head), as
well as as object classes. As ground-truth object classes are not available, we
deﬁne the loss as the cross-entropy between the class label ck
t predicted by the
mask predictor and a dedicated linear class prediction ˆck
t based on features uk
t ,
which, as we recall, are RoI-pooled from U:

t = R uk
ck
t

(11)

where R trainable parameters (biases integrated) learned end-to-end together
with the other parameters of the model.

We found that ﬁrst training the object head only and then the full network
was performing better. A ResNet50 network pretrained on ImageNet is modiﬁed
by inﬂating some of its ﬁlters to 2.5 convolutions (3D convolutions with the time
dimension separated), as described in Section 4; then by ﬁne-tuning.

We train the model using the Adam optimizer [22] with an initial learning
4 on 30 epochs and use early-stopping criterion on the validation set
50 minutes per epoch on 4

rate of 10−
for hyper-parameter optimization. Training takes
Titan XP GPUs with clips of 8 frames.

∼

12

Baradel et al.

6 Experimental results

We evaluated the method on three standard datasets, which represent diﬃcult
ﬁne-grained activity recognition tasks: the Something-Something dataset, the
VLOG dataset and the recently released EPIC Kitchens dataset.

Something-Something (SS) is a recent video classiﬁcation dataset with
108,000 example videos and 157 classes [13]. It shows humans performing diﬀer-
ent actions with diﬀerent objects, actions and objects being combined in diﬀerent
ways. Solving SS requires common sense reasoning and the state-of-the-art meth-
ods in activity recognition tend to fail, which makes this dataset challenging.

VLOG is a multi-label binary classiﬁcation of human-object interactions
recently released with 114,000 videos and 30 classes [12]. Classes correspond
to objects, and labels of a class are 1 if a person has touched a certain object
during the video, otherwise they are 0. It has recently been shown, that state-
of-the-art video based methods [6] are outperformed on VLOG by image based
methods like ResNet-50 [16], although these video methods outperform image
based ResNet-50 on large-scale video datasets like the Kinetics dataset [6]. This
suggests a gap between traditional datasets like Kinetics and the ﬁne-grained
dataset VLOG, making it particularly diﬃcult.

EPIC Kitchens (EPIC) is an egocentric video dataset recently released
containing 55 hours recording of daily activities [7]. This is the largest in ﬁrst-
person vision and the activities performed are non-scripted, which makes the
dataset very challenging and close to real world data. The dataset is densely
annotated and several tasks exist such as object detection, action recognition and
action prediction. We focus on action recognition with 39’594 action segments
in total and 125 actions classes (i.e verbs). Since the test set is not available yet
we conducted our experiments on the training set (28’561 videos). We use the
videos recorded by person 01 to person 25 for training (22’675 videos) and deﬁne
the validation set as the remaining videos (5’886 videos).

For all datasets we rescale the input video resolution to 256

256. While
training, we crop space-time blocks of 224
224 spatial resolution and L frames,
with L=8 for the SS dataset and L=4 for VLOG and EPIC. We do not perform
any other data augmentation. While training we extract L frames from the entire
video by splitting the video into L sub-sequences and randomly sampling one
frame per sub-sequence. The output sequence of size L is called a clip. A clip
aims to represent the full video with less frames. For testing we aggregate results
of 10 clips. We use lintel [10] for decoding video on the ﬂy.

×

×

The ablation study is done by using the train set as training data and we
report the result on the validation set. We compare against other state-of-the-
art approaches on the test set. For the ablation studies, we slightly decreased
the computational complexity of the model: the base network (including activity
and object heads) is a ResNet-18 instead of ResNet-50, a single clip of 4 frames
is extracted from a video at test time.

Comparison with other approaches. Table 1 shows the performance of
the proposed approach on the VLOG dataset. We outperform the state of the art

Object Level Visual Reasoning in Videos

13

Table 1. Results on Hand/Semantic Object Interaction Classiﬁcation (Average pre-
cision in % on the test set) on VLOG dataset. R50 and I3D implemented by [12].

s
r
e
p
a
p
/
k
o
o
b

e
b
u
t
/
e
l
t
t
o
b

g
n
i
d
d
e
b

P
A
m

g
a
b

d
e
b

l

w
o
b

x
o
b

h
s
u
r
b

e
n
o
h
p
-
l
l
e
c

t
e
n
i
b
a
c

g
n
i
h
t
o
l
c

p
u
c

r
o
o
d

s
r
e
w
a
r
d

d
o
o
f

k
r
o
f

e
f
i
n
k

p
o
t
p
a
l

e
v
a
w
o
r
c
i
m

n
e
v
o

l
i
c
n
e
p
/
n
e
p

r
o
t
a
r
e
g
i
r
f
e
r

w
o
l
l
i
p

e
t
a
l
p

n
o
o
p
s

k
n
i
s

l
a
m
i
n
a

d
e
ﬀ
u
t
s

h
s
u
r
b
h
t
o
o
t

e
l
b
a
t

l
e
w
o
t

R50 [16] 40.5 29.7 68.9 65.8 64.5 58.2 33.1 22.1 19.0 23.9 54.0 45.5 28.6 49.2 28.7 49.6 19.4 37.5 62.9 48.8 23.0 36.9 39.2 12.5 55.9 58.8 31.1 57.4 26.8 39.6 22.9
I3D [6]
39.7 24.9 71.7 71.4 62.5 57.1 27.1 19.2 33.9 20.7 50.6 45.8 24.7 54.7 19.1 50.8 19.3 41.9 54.0 27.5 21.4 37.4 42.9 12.6 42.5 60.4 33.9 46.0 23.5 59.6 34.7
Ours
44.7 30.2 72.3 70.7 64.9 59.8 38.2 24.6 26.3 22.4 64.5 47.2 35.4 57.9 25.2 48.5 24.5 40.2 72.0 54.1 26.5 39.9 48.6 15.2 53.5 60.7 36.8 52.8 27.9 64.0 37.6

≈

4.2 points (44.7% accuracy against
on this challenging dataset by a margin of
40.5% by [16]). As mentioned above, traditional video approaches tend to fail
on this challenging ﬁne-grained dataset, providing inferior results. Table 3 shows
performance on SS where we outperform the state of the art given by very recent
methods (+2.3 points). On EPIC we re-implement standard baselines and report
results on the validation set (Table 4) since the test set is not available. Our full
method reports an accuracy of 40.89 and outperforms baselines by a large margin
(
+7.9 points respectively for against CNN-2D and I3D based on a
+6.4 and
≈
ResNet-18).

≈

Eﬀect of object-level reasoning. Table 2 shows the importance of rea-
soning on the performance of the method. The baseline corresponds to the per-
formance obtained by the activity head trained alone (inﬂated ResNet, in the
ResNet-18 version for this table). No object level reasoning is present in this
baseline. The proposed approach (third line) including an object head and the
ORN module gains 0.8, 2.5 and 2.4 points compared to our baseline respectively
on SS, on EPIC and on VLOG. This indicates that the reasoning module is able
to extract complementary features compared to the activity head.

Using semantically deﬁned objects proved to be important and led to a gain of
2 points on EPIC and 2.3 points on VLOG for the full model (6/12.7 points using
the object head only) compared to an extension of Santoro et al. [32] operating
on pixel level. This indicates importance of object level reasoning. The gain on
SS is smaller (0.7 point with the full model and 7.8 points with the object head
only) and can be explained by the diﬀerence in spatial resolution of the videos.
Object detections and predictions of the binary masks are done using the initial
1183 and for EPIC
video resolution. The mean video resolution for VLOG is 660
157 for SS. Mask-RCNN has been trained on images of
is 640
×
resolution 800
800 and thus performs best on higher resolutions. The quality of
the object detector is important for leveraging object level understanding then
for the rest of the ablation study we focus on EPIC and VLOG datasets.

480 against 100

×

×

×

The function fφ in Equation (5) is an important design choice in our model.
In our proposed model, fφ is recurrent over time to ensure that the ORN module
captures long range reasoning over time, as shown in Equation (5). Removing
the recurrence in this equation leads to an MLP instead of a (gated) RNN, as
evaluated in row 4 of Table 2. Performance decreases by 1.1 point on VLOG
and 1.4 points on EPIC. The larger gap for EPIC compared to VLOG and
can arguably be explained by the fact that in SS actions cover the whole video,

14

Baradel et al.

Table 2. Ablation study with ResNet-18 backbone. Results in %: Top-1 accuracy for
EPIC and SS datasets, and mAP for VLOG dataset.

Method

Object type

EPIC

VLOG
obj. head 2 heads obj. head 2 heads obj. head 2 heads

SS

Baseline
ORN
ORN
ORN-mlp

-
pixel
COCO
COCO

ORN
ORN
ORN

COCO-visual
COCO-shape
COCO-class

ORN
ORN clique-1
ORN clique-3

COCO-intra
COCO
COCO

-
23.71
29.94
28.15

28.45
21.92
21.96

29.25
28.25
22.61

38.33
38.83
40.89
39.41

38.92
37.16
37.75

38.10
40.18
37.67

-
14.40
27.14
25.40

22.92
7.18
13.40

26.78
26.48
27.05

35.03
35.18
37.49
36.35

35.49
35.39
35.94

36.28
36.71
36.04

-
2.51
10.26
-

31.31
31.43
32.12
-

-
-
-

-
-
-

-
-
-

-
-
-

Table 3. Experimental results on the
Something-Something dataset (classi-
ﬁcation accuracy in % on the test set).

Table 4. Experimental results on the
EPIC Kitchens dataset (accuracy in %
on the validation set – methods with ∗
have been re-implemented).

Methods

C3D + Avg [13]
I3D [13]
MultiScale TRN [5]

Ours

Top1

21.50
27.63
33.60

35.97

Methods

Top1

R18 [16]∗
I3D-18 [6]∗

32.05
34.20

Ours

40.89

while solving VLOG requires detecting the right moment when the human-object
interaction occurs and thus long range reasoning plays a less important role.

Visual features extracted from object regions are the most discriminative,
however object shapes and labels also provide complementary information. Fi-
nally, the last part of Table 2 evaluates the eﬀect of the cliques size for model-
ing the interactions between objects and show that pairwise cliques outperform
cliques of size 1 and 3. We would like to recall, that even with unary cliques
only, interactions between objects are still modeled. However, the model needs
to ﬁnd subspaces in the hidden representations associated to each interaction.

CNN architecture and kernel inﬂations. The convolutional architecture
of the model was optimized over the validation set of the SS dataset, as shown
in Table 5. The architecture itself (in terms of numbers of layers, ﬁlters etc.)
is determined by pre-training on image classiﬁcation. We optimized the choice
of ﬁlter inﬂations from 2D to 2.5D or 3D for several convolutional blocks. This
has been optimized for the single head model and using a ResNet-18 variant
to speed up computation. Adding temporal convolutions increases performance
up to 100% w.r.t. to pure 2D baselines. This indicates, without surprise, that
motion is a strong cue. Inﬂating kernels to 2.5D on the input side and on the

Object Level Visual Reasoning in Videos

15

Table 5. Eﬀect of the CNN architecture (choice of kernel inﬂations) on a single head
ResNet-18 network. Accuracy in % on the validation set of Something-Something is
shown. 2.5D kernels are separable kernels: 2D followed by a 1D temporal.

Conv1

Conv2

Conv3

Conv4

Conv5

Aggreg

(cid:88)
-
(cid:88)
(cid:88)

2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D GAP RNN
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88) -
-
-

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
(cid:88) -
-
(cid:88) -
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
- (cid:88) (cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
- (cid:88) -
- (cid:88) -
- (cid:88) -
- (cid:88)
-
- (cid:88)
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
- (cid:88) -
- (cid:88) -
(cid:88) -
-
- (cid:88)
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
-
- (cid:88) -
-

- (cid:88) (cid:88) -
- (cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
-
-
-
-
-

-
-
-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-

SS

15.73
-
(cid:88) 15.88
31.42
-
27.58
-

31.28
32.06
32.25
31.31
32.79
33.77

28.71
31.42
20.05
22.52

output side provided best performances, suggesting that temporal integration
is required at a very low level (motion estimation) as well as on a very high
level, close to reasoning. Our study also corroborates recent research in activity
recognition, indicating that 2.5D kernels provide a good trade-oﬀ between high-
capacity and learnable numbers of parameters. Finally temporal integration via
RNN outperforms global average pooling over space and time. The choice of
a (gated) RNN for temporal integration of the activity head features proved
important (see Table 5) compared to global average pooling (GAP) over space
and time.

Visualizing the learned object interactions. Figure 4 shows visualiza-
tions of the pairwise object relationships the model learned from data, in partic-
ular from the VLOG dataset. Each graph is computed for a given activity class,
and strong arcs between two nodes in the graph indicate strong relationships
between the object classes, i.e. the model detects a high correlation between
these relationships and the activity. The graphs were obtained by thresholding
the summed activations of each pairwise relationship (j, k) in equation (4). Each
pair (j, k) can be assigned a pair of object classes cj
t through the predic-
tions of the object instance mask predictor. Integrating over all samples of the
dataset for a given class leads to the visualizations in Figure 4. We can see that
the object interactions are highly relevant to the detected activities: the person-
touches-bed activity is correlated to interactions between relevant object classes
person and bed. Similarly, activities human-bowl interaction and human-cup in-
teraction show interactions with the respective objects bowl and cup. Moreover,
other recovered relationships are highly correlated to the scene (for example,
dining-table and bowl for activity human-bowl interaction).

t and ck

16

Baradel et al.

Fig. 4. Example of object pairwise interactions learned by our model on VLOG for
four diﬀerent classes. Objects co-occurrences are at the top and learned pairwise objects
interactions are at the bottom. Line thickness indicates learned importance of a given
relation. Interactions have been normalized by the object co-occurrences.

Fig. 5. Examples of failure cases – a) small sized objects (on the left). Our model
detects a cell phone and a person but fails to detect hand-cell-phone contact; b) con-
fusion between semantically similar objects (on the right). The model falsly predicts
hand-cup contact instead of hand-glass-contact even though the wine glass is detected.

Finally, Figure 5 shows some failure cases, which are either due to errors

done by the object mask prediction (Mask R-CNN) or by the ORN itself.

7 Conclusion

We presented a method for activity recognition in videos which leverages ob-
ject instance detections for visual reasoning on object interactions over time.
The choice of reasoning over semantically well-deﬁned objects is key to our ap-
proach and outperforms state of the art methods which reason on grid-levels,
such as cells of convolutional feature maps. Temporal dependencies and causal
relationships are dealt with by integrating relationships between diﬀerent time
instants. We evaluated the method on three diﬃcult datasets, on which standard
approaches do not perform well, and report state-of-the-art results.

Acknowledgements. This work was funded by grant Deepvision (ANR-15-
CE23-0029, STPGP-479356-15), a joint French/Canadian call by ANR & NSERC.

Object Level Visual Reasoning in Videos

17

References

1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B.,
Vijayanarasimhan, S.: Youtube-8m: A large-scale video classiﬁcation benchmark.
arXiv preprint arxiv:1609.08675 (2016)

2. Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., Baskurt, A.: Sequential deep

learning for human action recognition. In: HBU (2011)

3. Baradel, F., Wolf, C., Mille, J., Taylor, G.: Glimpse clouds: Human activity recog-

nition from unstructured feature points. In: CVPR (2018)

4. Battaglia, P.W., Pascanu, R., Lai, M., Rezende, D.J., Kavukcuoglu, K.: Interaction

networks for learning about objects, relations and physics. In: NIPS (2016)

5. Bolei, Z., Zhang, A.A., Torralba, A.: Temporal relational reasoning in videos. In:

ECCV (2018)

6. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the

kinetics dataset. In: CVPR (2017)

7. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Scaling egocentric
vision: The epic-kitchens dataset. In: ECCV (2018)

8. Deng, Z., Vahdat, A., Hu, H., Mori, G.: Structure inference machines: Recurrent
neural networks for analyzing relations in group activity recognition. In: CVPR
(2016)

9. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S.,
Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: CVPR (2015)

10. Duke, B.: Lintel: Python video decoding. https://github.com/dukebw/lintel

(2018)

11. Fleuret, F., Li, T., Dubout, C., Wampler, E.K., Yantis, S., Geman, D.: Comparing
machines and humans on a visual categorization test. Proceedings of the National
Academy of Sciences of the United States of America 108 43, 17621–5 (2011)
12. Fouhey, D.F., Kuo, W., Efros, A.A., Malik, J.: From lifestyle vlogs to everyday

interactions. In: CVPR (2018)

13. Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim,
H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau,
C., Bax, I., Memisevic, R.: The ”something something” video database for learning
and evaluating visual common sense. In: ICCV (2017)

14. Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan,
S., Toderici, G., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: Ava: A
video dataset of spatio-temporally localized atomic visual actions. arXiv preprint
arXiv:1705.08421 (2017)

15. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: ICCV (2017)
16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

17. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation

18. Hudson, D., Manning, C.: Compositional attention networks for machine reasoning.

In: CVPR (2016)

9(8), 1735–1780 (1997)

In: ICLR (2018)

19. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: Deep Learning

on Spatio-Temporal Graphs. In: CVPR (2016)

20. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-
scale video classiﬁcation with convolutional neural networks. In: CVPR (2014)

18

Baradel et al.

21. Kim, J., Ricci, M., Serre, T.: Not-so-CLEVR: Visual relations strain feedforward

neural networks. arXiv preprint arXiv:1802.03390 (2018)

22. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICML (2015)
23. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Ka-
landitis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome:
Connecting language and vision using crowdsourced dense image annotations. In-
ternational Journal of Computer Vision (IJCV) 123, 32–73 (2017)

24. Luc, P., , Neverova, N., Couprie, C., Verbeek, J., LeCun, Y.: Predicting deeper

into the future of semantic segmentation. In: ICCV (2017)

25. Luvizon, D., Picard, D., Tabia, H.: 2d/3d pose estimation and action recognition

using multitask deep learning. In: CVPR (2018)

26. Monfort, M., Zhou, B., Bargal, S.A., Andonian, A., Yan, T., Ramakrishnan,
K., Brown, L., Fan, Q., Gutfruend, D., Vondrick, C., Oliva, A.: Moments
in time dataset: one million videos for event understanding. arXiv preprint
arXiv:1801.03150 (2018)

27. Perez, E., Vries, H.D., Strub, F., Dumoulin, V., Courville, A.: Learning visual rea-
soning without strong priors. In: ICML Machine Learning in Speech and Language
Processing Workshop (2017)

28. Pickup, L.C., Pan, Z., Wei, D., Shih, Y., Zhang, C., Zisserman, A., Scholkopf, B.,

Freeman, W.T.: Seeing the arrow of time. In: CVPR (2014)

29. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for

3d classiﬁcation and segmentation. In: CVPR (2017)

30. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

31. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A., Fei-Fei, L.: Imagenet large scale
visual recognition challenge. IJCV 115(3), 211–252 (2015)

32. Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia,
P., Lillicrap, T.: A simple neural network module for relational reasoning. In: NIPS
(2017)

33. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: NTU RGB+D: A Large Scale Dataset

for 3D Human Activity Analysis. In: CVPR (2016)

34. Sharma, S., Kiros, R., Salakhutdinov, R.: Action recognition using visual attention.

35. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-

In: ICLR Workshop (2016)

nition in videos. In: NIPS (2014)

36. Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An End-to-End Spatio-Temporal
Attention Model for Human Action Recognition from Skeleton Data. In: AAAI
(2016)

37. Stabinger, S., Rodr´ıguez-S´anchez, A., Piater, J.: 25 years of CNNs: Can we compare

to human abstraction capabilities? In: ICANN (2016)

38. van Steenkiste, S., Chang, M., Greﬀ, K., Schmidhuber, J.: Relational neural ex-
pectation maximization: Unsupervised discovery of objects and their interactions.
In: ICLR (2018)

39. Sun, L., Jia, K., Chen, K., Yeung, D., Shi, B.E., Savarese, S.: Lattice long short-

term memory for human action recognition. In: ICCV (2017)

40. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-

poral features with 3d convolutional networks. In: ICCV (2015)

41. Velikovi, P., Cucurull, G., Casanova, A., Romero, A., Li, P., Bengio, Y.: Graph

attention networks. In: ICLR (2018)

Object Level Visual Reasoning in Videos

19

42. Wang, H., Kl¨aser, A., Schmid, C., Liu, C.L.: Action Recognition by Dense Trajec-

tories. In: CVPR (2011)

43. Watters, N., Zoran, D., Weber, T., Battaglia, P., Pascanu, R., Tacchetti, A.: Visual
interaction networks: Learning a physics simulator from video. In: NIPS (2017)
44. Xie, S., Sun, C., Huang, J., Tu, Z., Murphy, K.: Rethinking spatiotemporal feature

learning for video understanding. arXiv prepring arxiv:1712.04851 (2017)

45. Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori, G., Fei-Fei, L.: Every mo-
ment counts: Dense detailed labeling of actions in complex videos. arXiv preprint
arXiv:1507.05738 (2015)

46. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.R., Smola,
A.J.: Deep sets. In: NIPS. pp. 3391–3401 (2017), http://papers.nips.cc/paper/
6931-deep-sets.pdf

8
1
0
2
 
p
e
S
 
0
2
 
 
]

V
C
.
s
c
[
 
 
3
v
7
5
1
6
0
.
6
0
8
1
:
v
i
X
r
a

Object Level Visual Reasoning in Videos

Fabien Baradel1, Natalia Neverova2, Christian Wolf1,3,
Julien Mille4, and Greg Mori5

1 Universit´e Lyon, INSA Lyon, CNRS, LIRIS, F-69621, Villeurbanne, France,
firstname.lastname@liris.cnrs.fr
2 Facebook AI Research, Paris, France, nneverova@fb.com
3 INRIA, CITI Laboratory, Villeurbanne, France
4 Laboratoire d’Informatique de l’Univ. de Tours, INSA Centre Val de Loire,
41034, Blois, France, julien.mille@insa-cvl.fr
5 Simon Fraser University, Vancouver, Canada, mori@cs.sfu.ca

https://fabienbaradel.github.io/eccv18_object_level_visual_reasoning/

Abstract. Human activity recognition is typically addressed by detect-
ing key concepts like global and local motion, features related to object
classes present in the scene, as well as features related to the global con-
text. The next open challenges in activity recognition require a level of
understanding that pushes beyond this and call for models with capa-
bilities for ﬁne distinction and detailed comprehension of interactions
between actors and objects in a scene. We propose a model capable of
learning to reason about semantically meaningful spatio-temporal inter-
actions in videos. The key to our approach is a choice of performing
this reasoning at the object level through the integration of state of the
art object detection networks. This allows the model to learn detailed
spatial interactions that exist at a semantic, object-interaction relevant
level. We evaluate our method on three standard datasets (Twenty-BN
Something-Something, VLOG and EPIC Kitchens) and achieve state of
the art results on all of them. Finally, we show visualizations of the in-
teractions learned by the model, which illustrate object classes and their
interactions corresponding to diﬀerent activity classes.

Keywords: Video understanding · Human-object interaction

1

Introduction

The ﬁeld of video understanding is extremely diverse, ranging from extract-
ing highly detailed information captured by speciﬁcally designed motion cap-
ture systems [33] to making general sense of videos from the Web [1]. As in
the domain of image recognition, there exist a number of large-scale video
datasets [6,26,13,12,23,14], which allow the training of high-capacity deep learn-
ing models from massive amounts of data. These models enable detection of key
cues present in videos, such as global and local motion, various object categories
and global scene-level information, and often achieve impressive performance in
recognizing high-level, abstract concepts in the wild.

2

Baradel et al.

Fig. 1. Humans can understand what happened in a video (“the leftmost carrot was
chopped by the person”) given only a pair of frames. Along these lines, the goal of this
work is to explore the capabilities of higher-level reasoning in neural models operating
at the semantic level of objects and interactions.

However, recent attention has been directed toward a more thorough un-
derstanding of human-focused activity in diverse internet videos. These eﬀorts
range from atomic human actions [14] to ﬁne-grained object interactions [13]
to everyday, commonly occurring human-object interactions [12]. This returns
us to a human-centric viewpoint of activity recognition where it is not only the
presence of certain objects / scenes that dictate the activity present, but the
manner, order, and eﬀects of human interaction with these scene elements that
are necessary for understanding. In a sense, this is akin to the problems in current
3D human activity recognition datasets [33], but requires the more challenging
reasoning and understanding of diverse environments common to internet video
collections.

Humans are able to infer what happened in a video given only a few sam-
ple frames. In particular, they can infer complex activities happening between
pairs of frames. This faculty is called reasoning and is a key component of hu-
man intelligence. As an example we can consider the pair of images in Figure 1,
which shows a complex situation involving articulated objects (human, carrots
and knife), the change of location and composition of objects. For humans it is
straightforward to draw a conclusion on what happened (a carrot was chopped
by the human). Humans have this extraordinary ability of performing visual rea-
soning on very complicated tasks while it remains unattainable for contemporary
computer vision algorithms [37,11].

The ability to perform visual reasoning in computer vision algorithms is still
an open problem. Attempts have been made for learning interactions between
diﬀerent entities in images with promising results on Visual Question Answering.
There have been a number of attempts to equip neural models with reasoning
abilities by training them to solve Visual Question Answering (VQA) problems.
Among proposed solutions are prior-less data normalization [27], structuring
networks to model relationships [32,43] as well as more complex attention based

Object Level Visual Reasoning in Videos

3

mechanisms [18]. At the same time, it was shown that high performance on exist-
ing VQA datasets can be achieved by simply discovering biases in the data [21].
We extend these eﬀorts to object level reasoning in videos. Since a video is
a temporal sequence, we leverage time as an explicit causal signal to identify
causal object relations. Our approach is related to the concept of the “arrow of
the time” [28] involving the “one-way direction” or “asymmetry” of time. Causal
event occurs before the event it aﬀects (A
B). In Figure 1 the knife was used
before the carrot switched over to the chopped-up state on the right side. For
a video classiﬁcation problem, we want to identify a causal event A happening
in a video that aﬀects its label B. But instead of identifying this causal event
directly from pixels we want to identify it from an object level perspective. We
believe that such an approach would be able to learn causal signals.

→

Following this hypothesis we propose to make a bridge between object de-
tection and activity recognition. Object detection allows us to extract low-level
information from a scene with all the present object instances and their semantic
meanings. However, detailed activity understanding requires reasoning over these
semantic structures, determining which objects were involved in interactions, of
what nature, and what were the results of these. To compound problems, the
semantic structure of a scene may change during a video (e.g. a new object can
appear, a person may make a move from one point to another one of the scene).
We propose an Object Relation Network (ORN), a neural network mod-
ule for reasoning between detected semantic object instances through space and
time. The ORN has potential to address these issues and conduct relational
reasoning over object interactions for the purpose of activity recognition. A set
of object detection masks ranging over diﬀerent object categories and temporal
occurrences is input to the ORN. The ORN is able to infer pairwise relationships
between objects detected at varying diﬀerent moments in time.

Code and object masks predictions will be publicly available6.

2 Related work

Action Recognition. Action recognition has a long history in computer vi-
sion. Pre-deep learning approaches in action recognition focused on handcrafted
spatio-temporal features including space-time interest points like SIFT-3D, HOG3D,
IDT and aggregated them using bag-of-words techniques. Some hand-crafted rep-
resentations, like dense trajectories [42], still give competitive performance and
are frequently combined with deep learning.

In the recent past, work has shifted to deep learning. Early attempts adapt
2D convolutional networks to videos through temporal pooling and 3D convolu-
tions [2,40]. 3D convolutions are now widely adopted for activity recognition with
the introduction of feature transfer by inﬂating pre-trained 2D convolutional ker-
nels from image classiﬁcation models trained on ImageNet/ILSVRC [31] through
3D kernels [6]. The downside of 3D kernels is their computational complexity

6 https://github.com/fabienbaradel/object_level_visual_reasoning

4

Baradel et al.

and the large number of learnable parameters, leading to the introduction of
2.5D kernels, i.e. separable ﬁlters in the form of a 2D spatial kernel followed by
a temporal kernel [44]. An alternative to temporal convolutions are Recurrent
Neural Networks (RNNs) in their various gated forms (GRUs, LSTMs) [17,9].

Karpathy et al. [20] presented a wide study on diﬀerent ways of connecting
information in spatial and temporal dimensions through convolutions and pool-
ing. On very general datasets with coarse activity classes they have showed that
there was a small margin between classifying individual frames and classifying
videos with more sophisticated temporal aggregation.

Simoyan et al. [35] proposed a widely adopted two-stream architecture for
action recognition which extracts two diﬀerent streams, one processing raw RGB
input and one processing pre-computed optical ﬂow images. The method out-
performed the state of the art, but relies on rather small scale optical ﬂow com-
putations.

In slightly narrower settings, prior information on the video content can allow
more ﬁne-grained models. Articulated pose is widely used in cases where humans
are guaranteed to be present [33]. Pose estimation and activity recognition as a
joint (multi-task) problem has recently shown to improve both tasks [25]. Some-
what related to our work, Structural RNNs [19] perform activity recognition
by integrating features from semantic objects and their relationships. However,
they handle the temporal evolution of tracked objects in videos with a set of
RNNs, each of which corresponds to cliques in a graph which models the spatio-
temporal relationships between these objects. This graph is hand-crafted man-
ually for each application, though related work provides learnable connections
via gating functions [8].

Attention models are a way to structure deep networks in an often generic
way. They are able to iteratively focus attention to speciﬁc parts in the data with-
out requiring prior knowledge about part or object positions. In activity recog-
nition, they have gained some traction in recent years, either as soft-attention
on articulated pose (joints) [36], on feature map cells [34,39], on time [45] or on
parts in raw RGB input through diﬀerentiable crops [3].

When raw video data is globally fed into deep neural networks, they focus
on extracting spatio-temporal features and perform aggregations. It has been
shown that these techniques fail on challenging ﬁne-grained datasets, which re-
quire learning long temporal dependencies and human-object interactions. A
concentrated eﬀort has been made to create large scale datasets to overcome
these issues [13,12,23,14].

Relational Reasoning. Relational reasoning is a well studied ﬁeld for many
applications ranging from visual reasoning [32] to reasoning about physical
systems [4]. Battaglia et al. [4] introduce a fully-diﬀerentiable network physics
engine called Interaction Network (IN). IN learns to predict several physical
systems such as gravitational systems, rigid body dynamics, and mass-spring
systems. It shows impressive results; however, it learns from a virtual environ-
ment, which provides access to virtually unlimited training examples. Following
the same perspective, Santoro et al. [32] introduced Relation Network (RN),

Object Level Visual Reasoning in Videos

5

a plug-in module for reasoning in deep networks. RN shows human-level per-
formance in Visual Question Answering (VQA) by inferring pairwise “object”
relations. However, in contrast to our work, the term “object” in [32] does not
refer to semantically meaningful entities, but to discrete cells in feature maps.
The number of interactions therefore grows with feature map resolutions, which
makes it diﬃcult to scale. Furthermore, a recent study [21] has shown that some
of these results are subject to dataset bias and do not generalize well to small
changes in the settings of the dataset.

In the same line, a recent work [38] has shown promising results on discov-
ering objects and their interactions in an unsupervised manner using training
examples from virtual environments. In [41], attention and relational modules
are combined on a graph structure. From a diﬀerent perspective, [27] show that
relational reasoning can be learned for visual reasoning in a data driven way with-
out any prior using conditional batch normalization with a feature-wise aﬃne
transformation based on conditioning information. In an opposite approach, a
strong structural prior is learned in the form of a complex attention mecha-
nism: in [18], an external memory module combined with attention processes
over input images and text questions, performing iterative reasoning for VQA.
While most of the discussed work has been designed for VQA and for pre-
dictions on physical systems and environments, extensions have been proposed
for video understanding. Reasoning in videos on a mask or segmentation level
has been attempted for video prediction [24], where the goal was to leverage se-
mantic information to be able predict further into the future. Zhou et al [5] have
recently shown state-of-the-art performance on challenging datasets by extend-
ing Relation Network to video classiﬁcation. Their chosen entities are frames, on
which they employ RN to reason on a temporal level only through pairwise frame
relations. The approach is promising, but restricted to temporal contextual in-
formation without an understanding on a local object level, which is provided
by our approach.

Reasoning over sets of objects is somewhat related to reasoning from unstruc-
tured data points, as done in PointNet [29], designed to learn from unordered sets
of points. PointNet shares many properties with DeepSet [46] which is a more
general framework for extracting information from sets of objects. To some ex-
tent, our work is related to PointNet, as we handle unordered sets of objects in a
permutation invariant way. However, we have an object relation viewpoint that
directly reasons over relationships between these semantic entities.

3 Object-level Visual Reasoning in Space and Time

Our goal is to extract multiple types of cues from a video sequence: interactions
between predicted objects and their semantic classes, as well as local and global
motion in the scene. We formulate this objective as a neural architecture with
two heads: an activity head and an object head. Figure 2 gives a functional
overview of the model. Both heads share common features up to a certain layer
shown in red in the ﬁgure. The activity head, shown in orange in the ﬁgure,

6

Baradel et al.

Fig. 2. A functional overview of the model. A global convolutional model extracts
features and splits into two heads trained to predict, respectively activity classes and
object classes. The latter are predicted by pooling over object instance masks, which
are predicted by an additional convolutional model. The object instances are passed
through a visual reasoning module.

is a CNN-based architecture employing convolutional layers, including spatio-
temporal convolutions, able to extract global motion features. However, it is not
able to extract information from an object level perspective. We leverage the
object head to perform reasoning on the relationships between predicted object
instances.

Our main contribution is a new structured module called Object Relation
Network (ORN), which is able to perform spatio-temporal reasoning between
detected object instances in the video. ORN is able to reason by modeling how
objects move, appear and disappear and how they interact between two frames.
In this section, we will ﬁrst describe our main contribution, the ORN network.
We then provide details about object instance features, about the activity head,
and ﬁnally about the ﬁnal recognition task. In what follows, lowercase letters
denote 1D vectors while uppercase letters are used for 2D and 3D matrices or
higher order tensors. We assume that the input of our system is a video of T
frames denoted by X1:T = (Xt)T
t=1 where Xt is the RGB image at timestep t.
The goal is to learn a mapping from X1:T to activity classes y.

3.1 Object Relation Network

ORN (Object Relation Network) is a module for reasoning between semantic
objects through space and time. It captures object moves, arrivals and interac-
tions in an eﬃcient manner. We suppose that for each frame t, we have a set
of objects k with associated features ok
t . Objects and features are detected and
computed by the object head described in Section 3.2.

Reasoning about activities in videos is inherently temporal, as activities fol-
low the arrow of time [28], i.e. the causality of the time dimension imposes that
past actions have consequences in the future but not vice-versa. We handle this
by sampling: running a process over time t, and for each instant t, sampling a

Object Level Visual Reasoning in Videos

7

second frame t(cid:48) with t(cid:48)<t. Our network reasons on objects which interact be-
tween pairs of frames and their corresponding sets of objects Ot(cid:48) = (cid:8)ok
t(cid:48)
and Ot = (cid:8)ok
all input objects from the combined set of both frames:

(cid:9)K(cid:48)
k=1
(cid:9)K
k=1. The goal is to learn a general function deﬁned on the set of

t

gt = g(o1

t(cid:48), . . . , oK(cid:48)

t(cid:48) , o1

t , . . . , oK

t ).

(1)

The objects in this set are unordered, aside for the frame they belong to.

This task is related to a problem raised in the PointNet algorithm [29] dis-
cussed in Section 2. PointNet approximates a general function g over an item
set
as a symmetric function g(cid:48) on transformed elements of
=
S
the set

x1, x2, . . . , xN }
{

g(x1, x2, . . . , xN )

g(cid:48)(h(x1), h(x2), . . . , h(xN )).

(2)

≈

In [29], g(cid:48) is a max pooling operation. The authors show that it allows universal
approximation of continuous sets of functions given that the hidden representa-
tion (the output of the mapping h(
·

)) is of suﬃciently high dimension.
We argue that the approximation in (2) can be extended as follows:

g(x1, x2, . . . , xN )

fφ

≈

hc (

∪i

∈

c xi)

(3)

(cid:33)

(cid:32)

(cid:88)

c

∈C

C

is the set of cliques of a graph deﬁned over the item set

where
is the
concatenation operator and we chose the sum operator as symmetric function.
The input dimension of the non-linearity hc(
) depends on the size of clique c
·
is composed of
but maps to a ﬁxed output dimension H. In the case where
unary cliques only, form (3) decomposes like (2) with the exception of a diﬀerent
symmetry operator (sum instead of max pooling). Choosing diﬀerent graphical
structures through
will lead to diﬀerent terms in the summation and allows
modeling diﬀerent types of interactions between items in the item set.

∪

S

S

C

,

C

Note that the interactions between items in (3) are not exclusively modeled
). Indeed, it is interesting to note, that the graphical decomposi-
through hc(
·
tion provided by
leads to interactions which are diﬀerent from the interactions
the same decomposition would provide when used in a probabilistic graphical
model, like for instance a Markov Random Field (MRF). In particular, a de-
composition into unary terms only, as given in equation (2), does not lead to
independence between items, whereas an MRF with unary terms only is equiva-
lent to a distribution over independent random variables. This is a consequence
of the global mapping fφ(
), which is deﬁned on the sum over all direct inter-
·
actions. Higher-order interactions between several items not directly modeled
) can eventually be learned by the model through
through a non-linearity hc(
·
the joint output space of all hc(
), provided that the dimensionality H of this
·
space is high enough to incorporate all interactions. However, whereas the map-
ping hc(
) provides a direct model of interactions between pairs of items, learning
·
interactions between two items (j, k), which are not directly captured through a

8

Baradel et al.

Fig. 3. ORN in the object head operating on detected instances of objects.

clique c and its corresponding hc(
), requires learning a corresponding subspace
·
).
in the common output space spanned by all hc(
·

This leads to the question of how to deﬁne the trade-oﬀ between the com-
and the output dimensionality H of the mapping
), both of which will determine the complexity of the modeled interactions.
·
will increase the input dimension (and there-
) as well as the computational complexity
·

plexity of the decomposition
hc(
Increasing the size of cliques in
fore the capacity) of the mapping hc(
of the sum operation.

C

C

Inspired by relational networks [32], we chose to directly model inter-frame
interactions between pairs of objects (j, k) and leave modeling of higher-order
interactions to the output space of the mappings hθ and the global mapping fφ:

(4)

(5)

gt =

hθ(oj

t(cid:48), ok
t )

(cid:88)

j,k

rt = fφ(gt, rt

1)

−

In order to better directly model long-range interactions, we make the global
mapping fφ(

) recurrent, which leads to the following form:
·

,
·

where rt represents the recurrent object reasoning state at time t and gt is the
global inter-frame interaction inferred at time t such as described in Equation 4.
In practice, this is implemented as a GRU, but for simplicity we omitted the
) are implemented as an
gates in Equation (5). The pairwise mappings hθ(
·
·
MLP. Figure 3 provides a visual explanation of the object head’s operating
through time.

,

Our proposed ORN diﬀers from [32] in three main points:

Objects have a semantic deﬁnition — we model relationships with respect
to semantically meaningful entities (object instances) instead of feature map
cells which do not have a semantically meaningful spatial extent. We will show
in the experimental section that this is a key diﬀerence.

Object Level Visual Reasoning in Videos

9

Objects are selected from diﬀerent frames — we infer object pairwise
relations only between objects present in two diﬀerent sets. This is a key design
choice which allows our model to reason about changes in object relationships
over time.
Long range reasoning — integration of the object relations over time is re-
). Since reasoning from a full sequence cannot
current by using a RNN for fφ(
·
be done by inferring the relations between two frames, fφ(
) allows long range
·
reasoning on sequences of variable length.

3.2 Object instance features

t

(cid:9)K
The object features Ot = (cid:8)ok
k=1 for each frame t used for the ORN module
described above are computed and collected from local regions predicted by
a mask predictor. Independently for each frame Xt of the input data block,
we predict object instances as binary masks Bk
t and associated object class
predictions ck
t , a distribution over C classes. We use Mask-RCNN [15], which
is able to detect objects in a frame using region proposal networks [30] and
produces a high quality segmentation mask for each object instance.

The objective is to collect features for each object instance, which jointly
describe its appearance, the change in its appearance over time, and its shape,
i.e. the shape of the binary mask. In theory, appearance could also be described
by pooling the feature representation learned by the mask predictor (Mask R-
CNN). However, in practice we choose to pool features from the dedicated object
head such as shown in Figure 2, which also include motion through the spatio-
temporal convolutions shared with the activity head:

t = ROI-Pooling(Ut, Bk
uk
t )

(6)

where Ut is the feature map output by the object head, uk
vector of appearance and appearance change of object k.

t is a D-dimensional

Shape information from the binary mask Bk
t is extracted through the follow-
ing mapping function: bk
) is a MLP. Information about
t ), where gφ(
·
object k in image Xt is given by a concatenation of appearance, shape, and
object class: ok

t = gφ(Bk

t uk

t ck

t ].

t = [ bk

3.3 Global Motion and Context

Current approaches in video understanding focus on modeling the video from
a high-level perspective. By a stack of spatio-temporal convolution and pooling
they focus on learning global scene context information. Eﬀective activity recog-
nition requires integration of both of these sources: global information about the
entire video content in addition to relational reasoning for making ﬁne distinc-
tions regarding object interactions and properties.

In our method, local low-level reasoning is provided through object head
and the ORN module such as described above in Section 3.1. We complement

10

Baradel et al.

this representation by high-level context information described by Vt which are
feature outputs from the activity head (orange block in Figure 2).

We use spatial global average pooling over Vt to output T D-dimensional
feature vectors denoted by vt, where vt corresponds to the context information
of the video at timestep t.

We model the dynamics of the context information through time by employ-

ing a RNN fγ(

) given by:
·

where s is the hidden state of fγ(
context though time.

) and gives cues about the evolution of the
·

st = fγ(vt, st

1)

−

(7)

3.4 Recognition

Given an input video sequence X1:T , the two diﬀerent streams corresponding to
the activity head and the object head result in the two representations h and r,
respectively where h = (cid:80)
trt. Each representation is the hidden
state of the respective GRU, which were described in the preceding subsections.
Recall that h provides the global motion context while r provides the object
reasoning state output by the ORN module. We perform independent linear
classiﬁcation for each representation:

tht and r = (cid:80)

y1 = W h
y2 = Z r

(8)

(9)

where y1, y2 correspond to the logits from the activity head and the object head,
respectively, and W and Z are trainable weights (including biases). The ﬁnal
prediction is done by averaging logits y1 and y2 followed by softmax activation.

4 Network Architectures and feature dimensions

×

×

W

The input RGB images Xt are of size R3
H where W and H correspond to
the width and height and are of size 224 each. The object and activity heads
(orange and green in Figure 2) are a joint convolutional neural network with
Resnet50 architecture pre-trained on ImageNet/ILSVRC [31], with Conv1 and
Conv5 blocks being inﬂated to 2.5D convolutions [44] (3D convolutions with a
separable temporal dimension). This choice has been optimized on the validation
set, as explained in Section 6 and shown in Table 5.

The last conv5 layers have been split into two diﬀerent heads (activity head
and object head). The intermediate feature representations Ut and Vt are of di-
mensions 2048
7, respectively. We provide a higher
×
spatial resolution for the feature maps Ut of the object head to get more precise
local descriptors. This can be done by changing the stride of the initial conv5
layers from 2 to 1. Temporal convolutions have been conﬁgured to keep the same
time temporal dimension through the network.

14 and 2048

14

×

×

×

×

×

T

T

7

Object Level Visual Reasoning in Videos

11

Global spatial pooling of activity features results in a 2048 dimensional fea-
ture vector fed into a GRU with 512 dimensional hidden state st. ROI-Pooling
of object features results in 2048 dimensional feature vectors uk
t . The encoder of
the binary mask is a MLP with one hidden layer of size 100 and outputs a mask
embedding bk
t of dimension 100. The number of object classes is 80, which leads
in total to a 2229 dimensional object feature vector ok
t .

The non-linearity hθ(

) is implemented as an MLP with 2 hidden layers each
·
with 512 units and produces an 512 dimensional output space. fφ(
) is imple-
·
mented as a GRU with a 256 dimension hidden state rt. We use ReLU as the
activation function after each layer for each network.

5 Training

We train the model with a loss split into two terms:

Ltot =

L

(cid:16) ˆy1 + ˆy2
2

(cid:17)

, y

+

(cid:88)

(cid:88)

t

k

(ˆck

t , ck

t ).

L

(10)

L

is the cross-entropy loss. The ﬁrst term corresponds to supervised ac-
where
tivity class losses comparing two diﬀerent activity class predictions to the class
ground truth: ˆy1 is the prediction of the activity head, whereas ˆy2 is the pre-
diction of the object head, as given by Equations (8) and (9), respectively.

The second term is a loss which pushes the features U of the object towards
representations of the semantic object classes. The goal is to obtain features
related to, both, motion (through the layers shared with the activity head), as
well as as object classes. As ground-truth object classes are not available, we
deﬁne the loss as the cross-entropy between the class label ck
t predicted by the
mask predictor and a dedicated linear class prediction ˆck
t based on features uk
t ,
which, as we recall, are RoI-pooled from U:

t = R uk
ck
t

(11)

where R trainable parameters (biases integrated) learned end-to-end together
with the other parameters of the model.

We found that ﬁrst training the object head only and then the full network
was performing better. A ResNet50 network pretrained on ImageNet is modiﬁed
by inﬂating some of its ﬁlters to 2.5 convolutions (3D convolutions with the time
dimension separated), as described in Section 4; then by ﬁne-tuning.

We train the model using the Adam optimizer [22] with an initial learning
4 on 30 epochs and use early-stopping criterion on the validation set
50 minutes per epoch on 4

rate of 10−
for hyper-parameter optimization. Training takes
Titan XP GPUs with clips of 8 frames.

∼

12

Baradel et al.

6 Experimental results

We evaluated the method on three standard datasets, which represent diﬃcult
ﬁne-grained activity recognition tasks: the Something-Something dataset, the
VLOG dataset and the recently released EPIC Kitchens dataset.

Something-Something (SS) is a recent video classiﬁcation dataset with
108,000 example videos and 157 classes [13]. It shows humans performing diﬀer-
ent actions with diﬀerent objects, actions and objects being combined in diﬀerent
ways. Solving SS requires common sense reasoning and the state-of-the-art meth-
ods in activity recognition tend to fail, which makes this dataset challenging.

VLOG is a multi-label binary classiﬁcation of human-object interactions
recently released with 114,000 videos and 30 classes [12]. Classes correspond
to objects, and labels of a class are 1 if a person has touched a certain object
during the video, otherwise they are 0. It has recently been shown, that state-
of-the-art video based methods [6] are outperformed on VLOG by image based
methods like ResNet-50 [16], although these video methods outperform image
based ResNet-50 on large-scale video datasets like the Kinetics dataset [6]. This
suggests a gap between traditional datasets like Kinetics and the ﬁne-grained
dataset VLOG, making it particularly diﬃcult.

EPIC Kitchens (EPIC) is an egocentric video dataset recently released
containing 55 hours recording of daily activities [7]. This is the largest in ﬁrst-
person vision and the activities performed are non-scripted, which makes the
dataset very challenging and close to real world data. The dataset is densely
annotated and several tasks exist such as object detection, action recognition and
action prediction. We focus on action recognition with 39’594 action segments
in total and 125 actions classes (i.e verbs). Since the test set is not available yet
we conducted our experiments on the training set (28’561 videos). We use the
videos recorded by person 01 to person 25 for training (22’675 videos) and deﬁne
the validation set as the remaining videos (5’886 videos).

For all datasets we rescale the input video resolution to 256

256. While
training, we crop space-time blocks of 224
224 spatial resolution and L frames,
with L=8 for the SS dataset and L=4 for VLOG and EPIC. We do not perform
any other data augmentation. While training we extract L frames from the entire
video by splitting the video into L sub-sequences and randomly sampling one
frame per sub-sequence. The output sequence of size L is called a clip. A clip
aims to represent the full video with less frames. For testing we aggregate results
of 10 clips. We use lintel [10] for decoding video on the ﬂy.

×

×

The ablation study is done by using the train set as training data and we
report the result on the validation set. We compare against other state-of-the-
art approaches on the test set. For the ablation studies, we slightly decreased
the computational complexity of the model: the base network (including activity
and object heads) is a ResNet-18 instead of ResNet-50, a single clip of 4 frames
is extracted from a video at test time.

Comparison with other approaches. Table 1 shows the performance of
the proposed approach on the VLOG dataset. We outperform the state of the art

Object Level Visual Reasoning in Videos

13

Table 1. Results on Hand/Semantic Object Interaction Classiﬁcation (Average pre-
cision in % on the test set) on VLOG dataset. R50 and I3D implemented by [12].

s
r
e
p
a
p
/
k
o
o
b

e
b
u
t
/
e
l
t
t
o
b

g
n
i
d
d
e
b

P
A
m

g
a
b

d
e
b

l

w
o
b

x
o
b

h
s
u
r
b

e
n
o
h
p
-
l
l
e
c

t
e
n
i
b
a
c

g
n
i
h
t
o
l
c

p
u
c

r
o
o
d

s
r
e
w
a
r
d

d
o
o
f

k
r
o
f

e
f
i
n
k

p
o
t
p
a
l

e
v
a
w
o
r
c
i
m

n
e
v
o

l
i
c
n
e
p
/
n
e
p

r
o
t
a
r
e
g
i
r
f
e
r

w
o
l
l
i
p

e
t
a
l
p

n
o
o
p
s

k
n
i
s

l
a
m
i
n
a

d
e
ﬀ
u
t
s

h
s
u
r
b
h
t
o
o
t

e
l
b
a
t

l
e
w
o
t

R50 [16] 40.5 29.7 68.9 65.8 64.5 58.2 33.1 22.1 19.0 23.9 54.0 45.5 28.6 49.2 28.7 49.6 19.4 37.5 62.9 48.8 23.0 36.9 39.2 12.5 55.9 58.8 31.1 57.4 26.8 39.6 22.9
I3D [6]
39.7 24.9 71.7 71.4 62.5 57.1 27.1 19.2 33.9 20.7 50.6 45.8 24.7 54.7 19.1 50.8 19.3 41.9 54.0 27.5 21.4 37.4 42.9 12.6 42.5 60.4 33.9 46.0 23.5 59.6 34.7
Ours
44.7 30.2 72.3 70.7 64.9 59.8 38.2 24.6 26.3 22.4 64.5 47.2 35.4 57.9 25.2 48.5 24.5 40.2 72.0 54.1 26.5 39.9 48.6 15.2 53.5 60.7 36.8 52.8 27.9 64.0 37.6

≈

4.2 points (44.7% accuracy against
on this challenging dataset by a margin of
40.5% by [16]). As mentioned above, traditional video approaches tend to fail
on this challenging ﬁne-grained dataset, providing inferior results. Table 3 shows
performance on SS where we outperform the state of the art given by very recent
methods (+2.3 points). On EPIC we re-implement standard baselines and report
results on the validation set (Table 4) since the test set is not available. Our full
method reports an accuracy of 40.89 and outperforms baselines by a large margin
(
+7.9 points respectively for against CNN-2D and I3D based on a
+6.4 and
≈
ResNet-18).

≈

Eﬀect of object-level reasoning. Table 2 shows the importance of rea-
soning on the performance of the method. The baseline corresponds to the per-
formance obtained by the activity head trained alone (inﬂated ResNet, in the
ResNet-18 version for this table). No object level reasoning is present in this
baseline. The proposed approach (third line) including an object head and the
ORN module gains 0.8, 2.5 and 2.4 points compared to our baseline respectively
on SS, on EPIC and on VLOG. This indicates that the reasoning module is able
to extract complementary features compared to the activity head.

Using semantically deﬁned objects proved to be important and led to a gain of
2 points on EPIC and 2.3 points on VLOG for the full model (6/12.7 points using
the object head only) compared to an extension of Santoro et al. [32] operating
on pixel level. This indicates importance of object level reasoning. The gain on
SS is smaller (0.7 point with the full model and 7.8 points with the object head
only) and can be explained by the diﬀerence in spatial resolution of the videos.
Object detections and predictions of the binary masks are done using the initial
1183 and for EPIC
video resolution. The mean video resolution for VLOG is 660
157 for SS. Mask-RCNN has been trained on images of
is 640
×
resolution 800
800 and thus performs best on higher resolutions. The quality of
the object detector is important for leveraging object level understanding then
for the rest of the ablation study we focus on EPIC and VLOG datasets.

480 against 100

×

×

×

The function fφ in Equation (5) is an important design choice in our model.
In our proposed model, fφ is recurrent over time to ensure that the ORN module
captures long range reasoning over time, as shown in Equation (5). Removing
the recurrence in this equation leads to an MLP instead of a (gated) RNN, as
evaluated in row 4 of Table 2. Performance decreases by 1.1 point on VLOG
and 1.4 points on EPIC. The larger gap for EPIC compared to VLOG and
can arguably be explained by the fact that in SS actions cover the whole video,

14

Baradel et al.

Table 2. Ablation study with ResNet-18 backbone. Results in %: Top-1 accuracy for
EPIC and SS datasets, and mAP for VLOG dataset.

Method

Object type

EPIC

VLOG
obj. head 2 heads obj. head 2 heads obj. head 2 heads

SS

Baseline
ORN
ORN
ORN-mlp

-
pixel
COCO
COCO

ORN
ORN
ORN

COCO-visual
COCO-shape
COCO-class

ORN
ORN clique-1
ORN clique-3

COCO-intra
COCO
COCO

-
23.71
29.94
28.15

28.45
21.92
21.96

29.25
28.25
22.61

38.33
38.83
40.89
39.41

38.92
37.16
37.75

38.10
40.18
37.67

-
14.40
27.14
25.40

22.92
7.18
13.40

26.78
26.48
27.05

35.03
35.18
37.49
36.35

35.49
35.39
35.94

36.28
36.71
36.04

-
2.51
10.26
-

31.31
31.43
32.12
-

-
-
-

-
-
-

-
-
-

-
-
-

Table 3. Experimental results on the
Something-Something dataset (classi-
ﬁcation accuracy in % on the test set).

Table 4. Experimental results on the
EPIC Kitchens dataset (accuracy in %
on the validation set – methods with ∗
have been re-implemented).

Methods

C3D + Avg [13]
I3D [13]
MultiScale TRN [5]

Ours

Top1

21.50
27.63
33.60

35.97

Methods

Top1

R18 [16]∗
I3D-18 [6]∗

32.05
34.20

Ours

40.89

while solving VLOG requires detecting the right moment when the human-object
interaction occurs and thus long range reasoning plays a less important role.

Visual features extracted from object regions are the most discriminative,
however object shapes and labels also provide complementary information. Fi-
nally, the last part of Table 2 evaluates the eﬀect of the cliques size for model-
ing the interactions between objects and show that pairwise cliques outperform
cliques of size 1 and 3. We would like to recall, that even with unary cliques
only, interactions between objects are still modeled. However, the model needs
to ﬁnd subspaces in the hidden representations associated to each interaction.

CNN architecture and kernel inﬂations. The convolutional architecture
of the model was optimized over the validation set of the SS dataset, as shown
in Table 5. The architecture itself (in terms of numbers of layers, ﬁlters etc.)
is determined by pre-training on image classiﬁcation. We optimized the choice
of ﬁlter inﬂations from 2D to 2.5D or 3D for several convolutional blocks. This
has been optimized for the single head model and using a ResNet-18 variant
to speed up computation. Adding temporal convolutions increases performance
up to 100% w.r.t. to pure 2D baselines. This indicates, without surprise, that
motion is a strong cue. Inﬂating kernels to 2.5D on the input side and on the

Object Level Visual Reasoning in Videos

15

Table 5. Eﬀect of the CNN architecture (choice of kernel inﬂations) on a single head
ResNet-18 network. Accuracy in % on the validation set of Something-Something is
shown. 2.5D kernels are separable kernels: 2D followed by a 1D temporal.

Conv1

Conv2

Conv3

Conv4

Conv5

Aggreg

(cid:88)
-
(cid:88)
(cid:88)

2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D GAP RNN
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88) -
-
-

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
(cid:88) -
-
(cid:88) -
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
- (cid:88) (cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
- (cid:88) -
- (cid:88) -
- (cid:88) -
- (cid:88)
-
- (cid:88)
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
- (cid:88) -
- (cid:88) -
(cid:88) -
-
- (cid:88)
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
-
- (cid:88) -
-

- (cid:88) (cid:88) -
- (cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
-
-
-
-
-

-
-
-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-

SS

15.73
-
(cid:88) 15.88
31.42
-
27.58
-

31.28
32.06
32.25
31.31
32.79
33.77

28.71
31.42
20.05
22.52

output side provided best performances, suggesting that temporal integration
is required at a very low level (motion estimation) as well as on a very high
level, close to reasoning. Our study also corroborates recent research in activity
recognition, indicating that 2.5D kernels provide a good trade-oﬀ between high-
capacity and learnable numbers of parameters. Finally temporal integration via
RNN outperforms global average pooling over space and time. The choice of
a (gated) RNN for temporal integration of the activity head features proved
important (see Table 5) compared to global average pooling (GAP) over space
and time.

Visualizing the learned object interactions. Figure 4 shows visualiza-
tions of the pairwise object relationships the model learned from data, in partic-
ular from the VLOG dataset. Each graph is computed for a given activity class,
and strong arcs between two nodes in the graph indicate strong relationships
between the object classes, i.e. the model detects a high correlation between
these relationships and the activity. The graphs were obtained by thresholding
the summed activations of each pairwise relationship (j, k) in equation (4). Each
pair (j, k) can be assigned a pair of object classes cj
t through the predic-
tions of the object instance mask predictor. Integrating over all samples of the
dataset for a given class leads to the visualizations in Figure 4. We can see that
the object interactions are highly relevant to the detected activities: the person-
touches-bed activity is correlated to interactions between relevant object classes
person and bed. Similarly, activities human-bowl interaction and human-cup in-
teraction show interactions with the respective objects bowl and cup. Moreover,
other recovered relationships are highly correlated to the scene (for example,
dining-table and bowl for activity human-bowl interaction).

t and ck

16

Baradel et al.

Fig. 4. Example of object pairwise interactions learned by our model on VLOG for
four diﬀerent classes. Objects co-occurrences are at the top and learned pairwise objects
interactions are at the bottom. Line thickness indicates learned importance of a given
relation. Interactions have been normalized by the object co-occurrences.

Fig. 5. Examples of failure cases – a) small sized objects (on the left). Our model
detects a cell phone and a person but fails to detect hand-cell-phone contact; b) con-
fusion between semantically similar objects (on the right). The model falsly predicts
hand-cup contact instead of hand-glass-contact even though the wine glass is detected.

Finally, Figure 5 shows some failure cases, which are either due to errors

done by the object mask prediction (Mask R-CNN) or by the ORN itself.

7 Conclusion

We presented a method for activity recognition in videos which leverages ob-
ject instance detections for visual reasoning on object interactions over time.
The choice of reasoning over semantically well-deﬁned objects is key to our ap-
proach and outperforms state of the art methods which reason on grid-levels,
such as cells of convolutional feature maps. Temporal dependencies and causal
relationships are dealt with by integrating relationships between diﬀerent time
instants. We evaluated the method on three diﬃcult datasets, on which standard
approaches do not perform well, and report state-of-the-art results.

Acknowledgements. This work was funded by grant Deepvision (ANR-15-
CE23-0029, STPGP-479356-15), a joint French/Canadian call by ANR & NSERC.

Object Level Visual Reasoning in Videos

17

References

1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B.,
Vijayanarasimhan, S.: Youtube-8m: A large-scale video classiﬁcation benchmark.
arXiv preprint arxiv:1609.08675 (2016)

2. Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., Baskurt, A.: Sequential deep

learning for human action recognition. In: HBU (2011)

3. Baradel, F., Wolf, C., Mille, J., Taylor, G.: Glimpse clouds: Human activity recog-

nition from unstructured feature points. In: CVPR (2018)

4. Battaglia, P.W., Pascanu, R., Lai, M., Rezende, D.J., Kavukcuoglu, K.: Interaction

networks for learning about objects, relations and physics. In: NIPS (2016)

5. Bolei, Z., Zhang, A.A., Torralba, A.: Temporal relational reasoning in videos. In:

ECCV (2018)

6. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the

kinetics dataset. In: CVPR (2017)

7. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Scaling egocentric
vision: The epic-kitchens dataset. In: ECCV (2018)

8. Deng, Z., Vahdat, A., Hu, H., Mori, G.: Structure inference machines: Recurrent
neural networks for analyzing relations in group activity recognition. In: CVPR
(2016)

9. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S.,
Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: CVPR (2015)

10. Duke, B.: Lintel: Python video decoding. https://github.com/dukebw/lintel

(2018)

11. Fleuret, F., Li, T., Dubout, C., Wampler, E.K., Yantis, S., Geman, D.: Comparing
machines and humans on a visual categorization test. Proceedings of the National
Academy of Sciences of the United States of America 108 43, 17621–5 (2011)
12. Fouhey, D.F., Kuo, W., Efros, A.A., Malik, J.: From lifestyle vlogs to everyday

interactions. In: CVPR (2018)

13. Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim,
H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau,
C., Bax, I., Memisevic, R.: The ”something something” video database for learning
and evaluating visual common sense. In: ICCV (2017)

14. Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan,
S., Toderici, G., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: Ava: A
video dataset of spatio-temporally localized atomic visual actions. arXiv preprint
arXiv:1705.08421 (2017)

15. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: ICCV (2017)
16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

17. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation

18. Hudson, D., Manning, C.: Compositional attention networks for machine reasoning.

In: CVPR (2016)

9(8), 1735–1780 (1997)

In: ICLR (2018)

19. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: Deep Learning

on Spatio-Temporal Graphs. In: CVPR (2016)

20. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-
scale video classiﬁcation with convolutional neural networks. In: CVPR (2014)

18

Baradel et al.

21. Kim, J., Ricci, M., Serre, T.: Not-so-CLEVR: Visual relations strain feedforward

neural networks. arXiv preprint arXiv:1802.03390 (2018)

22. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICML (2015)
23. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Ka-
landitis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome:
Connecting language and vision using crowdsourced dense image annotations. In-
ternational Journal of Computer Vision (IJCV) 123, 32–73 (2017)

24. Luc, P., , Neverova, N., Couprie, C., Verbeek, J., LeCun, Y.: Predicting deeper

into the future of semantic segmentation. In: ICCV (2017)

25. Luvizon, D., Picard, D., Tabia, H.: 2d/3d pose estimation and action recognition

using multitask deep learning. In: CVPR (2018)

26. Monfort, M., Zhou, B., Bargal, S.A., Andonian, A., Yan, T., Ramakrishnan,
K., Brown, L., Fan, Q., Gutfruend, D., Vondrick, C., Oliva, A.: Moments
in time dataset: one million videos for event understanding. arXiv preprint
arXiv:1801.03150 (2018)

27. Perez, E., Vries, H.D., Strub, F., Dumoulin, V., Courville, A.: Learning visual rea-
soning without strong priors. In: ICML Machine Learning in Speech and Language
Processing Workshop (2017)

28. Pickup, L.C., Pan, Z., Wei, D., Shih, Y., Zhang, C., Zisserman, A., Scholkopf, B.,

Freeman, W.T.: Seeing the arrow of time. In: CVPR (2014)

29. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for

3d classiﬁcation and segmentation. In: CVPR (2017)

30. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

31. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A., Fei-Fei, L.: Imagenet large scale
visual recognition challenge. IJCV 115(3), 211–252 (2015)

32. Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia,
P., Lillicrap, T.: A simple neural network module for relational reasoning. In: NIPS
(2017)

33. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: NTU RGB+D: A Large Scale Dataset

for 3D Human Activity Analysis. In: CVPR (2016)

34. Sharma, S., Kiros, R., Salakhutdinov, R.: Action recognition using visual attention.

35. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-

In: ICLR Workshop (2016)

nition in videos. In: NIPS (2014)

36. Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An End-to-End Spatio-Temporal
Attention Model for Human Action Recognition from Skeleton Data. In: AAAI
(2016)

37. Stabinger, S., Rodr´ıguez-S´anchez, A., Piater, J.: 25 years of CNNs: Can we compare

to human abstraction capabilities? In: ICANN (2016)

38. van Steenkiste, S., Chang, M., Greﬀ, K., Schmidhuber, J.: Relational neural ex-
pectation maximization: Unsupervised discovery of objects and their interactions.
In: ICLR (2018)

39. Sun, L., Jia, K., Chen, K., Yeung, D., Shi, B.E., Savarese, S.: Lattice long short-

term memory for human action recognition. In: ICCV (2017)

40. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-

poral features with 3d convolutional networks. In: ICCV (2015)

41. Velikovi, P., Cucurull, G., Casanova, A., Romero, A., Li, P., Bengio, Y.: Graph

attention networks. In: ICLR (2018)

Object Level Visual Reasoning in Videos

19

42. Wang, H., Kl¨aser, A., Schmid, C., Liu, C.L.: Action Recognition by Dense Trajec-

tories. In: CVPR (2011)

43. Watters, N., Zoran, D., Weber, T., Battaglia, P., Pascanu, R., Tacchetti, A.: Visual
interaction networks: Learning a physics simulator from video. In: NIPS (2017)
44. Xie, S., Sun, C., Huang, J., Tu, Z., Murphy, K.: Rethinking spatiotemporal feature

learning for video understanding. arXiv prepring arxiv:1712.04851 (2017)

45. Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori, G., Fei-Fei, L.: Every mo-
ment counts: Dense detailed labeling of actions in complex videos. arXiv preprint
arXiv:1507.05738 (2015)

46. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.R., Smola,
A.J.: Deep sets. In: NIPS. pp. 3391–3401 (2017), http://papers.nips.cc/paper/
6931-deep-sets.pdf

8
1
0
2
 
p
e
S
 
0
2
 
 
]

V
C
.
s
c
[
 
 
3
v
7
5
1
6
0
.
6
0
8
1
:
v
i
X
r
a

Object Level Visual Reasoning in Videos

Fabien Baradel1, Natalia Neverova2, Christian Wolf1,3,
Julien Mille4, and Greg Mori5

1 Universit´e Lyon, INSA Lyon, CNRS, LIRIS, F-69621, Villeurbanne, France,
firstname.lastname@liris.cnrs.fr
2 Facebook AI Research, Paris, France, nneverova@fb.com
3 INRIA, CITI Laboratory, Villeurbanne, France
4 Laboratoire d’Informatique de l’Univ. de Tours, INSA Centre Val de Loire,
41034, Blois, France, julien.mille@insa-cvl.fr
5 Simon Fraser University, Vancouver, Canada, mori@cs.sfu.ca

https://fabienbaradel.github.io/eccv18_object_level_visual_reasoning/

Abstract. Human activity recognition is typically addressed by detect-
ing key concepts like global and local motion, features related to object
classes present in the scene, as well as features related to the global con-
text. The next open challenges in activity recognition require a level of
understanding that pushes beyond this and call for models with capa-
bilities for ﬁne distinction and detailed comprehension of interactions
between actors and objects in a scene. We propose a model capable of
learning to reason about semantically meaningful spatio-temporal inter-
actions in videos. The key to our approach is a choice of performing
this reasoning at the object level through the integration of state of the
art object detection networks. This allows the model to learn detailed
spatial interactions that exist at a semantic, object-interaction relevant
level. We evaluate our method on three standard datasets (Twenty-BN
Something-Something, VLOG and EPIC Kitchens) and achieve state of
the art results on all of them. Finally, we show visualizations of the in-
teractions learned by the model, which illustrate object classes and their
interactions corresponding to diﬀerent activity classes.

Keywords: Video understanding · Human-object interaction

1

Introduction

The ﬁeld of video understanding is extremely diverse, ranging from extract-
ing highly detailed information captured by speciﬁcally designed motion cap-
ture systems [33] to making general sense of videos from the Web [1]. As in
the domain of image recognition, there exist a number of large-scale video
datasets [6,26,13,12,23,14], which allow the training of high-capacity deep learn-
ing models from massive amounts of data. These models enable detection of key
cues present in videos, such as global and local motion, various object categories
and global scene-level information, and often achieve impressive performance in
recognizing high-level, abstract concepts in the wild.

2

Baradel et al.

Fig. 1. Humans can understand what happened in a video (“the leftmost carrot was
chopped by the person”) given only a pair of frames. Along these lines, the goal of this
work is to explore the capabilities of higher-level reasoning in neural models operating
at the semantic level of objects and interactions.

However, recent attention has been directed toward a more thorough un-
derstanding of human-focused activity in diverse internet videos. These eﬀorts
range from atomic human actions [14] to ﬁne-grained object interactions [13]
to everyday, commonly occurring human-object interactions [12]. This returns
us to a human-centric viewpoint of activity recognition where it is not only the
presence of certain objects / scenes that dictate the activity present, but the
manner, order, and eﬀects of human interaction with these scene elements that
are necessary for understanding. In a sense, this is akin to the problems in current
3D human activity recognition datasets [33], but requires the more challenging
reasoning and understanding of diverse environments common to internet video
collections.

Humans are able to infer what happened in a video given only a few sam-
ple frames. In particular, they can infer complex activities happening between
pairs of frames. This faculty is called reasoning and is a key component of hu-
man intelligence. As an example we can consider the pair of images in Figure 1,
which shows a complex situation involving articulated objects (human, carrots
and knife), the change of location and composition of objects. For humans it is
straightforward to draw a conclusion on what happened (a carrot was chopped
by the human). Humans have this extraordinary ability of performing visual rea-
soning on very complicated tasks while it remains unattainable for contemporary
computer vision algorithms [37,11].

The ability to perform visual reasoning in computer vision algorithms is still
an open problem. Attempts have been made for learning interactions between
diﬀerent entities in images with promising results on Visual Question Answering.
There have been a number of attempts to equip neural models with reasoning
abilities by training them to solve Visual Question Answering (VQA) problems.
Among proposed solutions are prior-less data normalization [27], structuring
networks to model relationships [32,43] as well as more complex attention based

Object Level Visual Reasoning in Videos

3

mechanisms [18]. At the same time, it was shown that high performance on exist-
ing VQA datasets can be achieved by simply discovering biases in the data [21].
We extend these eﬀorts to object level reasoning in videos. Since a video is
a temporal sequence, we leverage time as an explicit causal signal to identify
causal object relations. Our approach is related to the concept of the “arrow of
the time” [28] involving the “one-way direction” or “asymmetry” of time. Causal
event occurs before the event it aﬀects (A
B). In Figure 1 the knife was used
before the carrot switched over to the chopped-up state on the right side. For
a video classiﬁcation problem, we want to identify a causal event A happening
in a video that aﬀects its label B. But instead of identifying this causal event
directly from pixels we want to identify it from an object level perspective. We
believe that such an approach would be able to learn causal signals.

→

Following this hypothesis we propose to make a bridge between object de-
tection and activity recognition. Object detection allows us to extract low-level
information from a scene with all the present object instances and their semantic
meanings. However, detailed activity understanding requires reasoning over these
semantic structures, determining which objects were involved in interactions, of
what nature, and what were the results of these. To compound problems, the
semantic structure of a scene may change during a video (e.g. a new object can
appear, a person may make a move from one point to another one of the scene).
We propose an Object Relation Network (ORN), a neural network mod-
ule for reasoning between detected semantic object instances through space and
time. The ORN has potential to address these issues and conduct relational
reasoning over object interactions for the purpose of activity recognition. A set
of object detection masks ranging over diﬀerent object categories and temporal
occurrences is input to the ORN. The ORN is able to infer pairwise relationships
between objects detected at varying diﬀerent moments in time.

Code and object masks predictions will be publicly available6.

2 Related work

Action Recognition. Action recognition has a long history in computer vi-
sion. Pre-deep learning approaches in action recognition focused on handcrafted
spatio-temporal features including space-time interest points like SIFT-3D, HOG3D,
IDT and aggregated them using bag-of-words techniques. Some hand-crafted rep-
resentations, like dense trajectories [42], still give competitive performance and
are frequently combined with deep learning.

In the recent past, work has shifted to deep learning. Early attempts adapt
2D convolutional networks to videos through temporal pooling and 3D convolu-
tions [2,40]. 3D convolutions are now widely adopted for activity recognition with
the introduction of feature transfer by inﬂating pre-trained 2D convolutional ker-
nels from image classiﬁcation models trained on ImageNet/ILSVRC [31] through
3D kernels [6]. The downside of 3D kernels is their computational complexity

6 https://github.com/fabienbaradel/object_level_visual_reasoning

4

Baradel et al.

and the large number of learnable parameters, leading to the introduction of
2.5D kernels, i.e. separable ﬁlters in the form of a 2D spatial kernel followed by
a temporal kernel [44]. An alternative to temporal convolutions are Recurrent
Neural Networks (RNNs) in their various gated forms (GRUs, LSTMs) [17,9].

Karpathy et al. [20] presented a wide study on diﬀerent ways of connecting
information in spatial and temporal dimensions through convolutions and pool-
ing. On very general datasets with coarse activity classes they have showed that
there was a small margin between classifying individual frames and classifying
videos with more sophisticated temporal aggregation.

Simoyan et al. [35] proposed a widely adopted two-stream architecture for
action recognition which extracts two diﬀerent streams, one processing raw RGB
input and one processing pre-computed optical ﬂow images. The method out-
performed the state of the art, but relies on rather small scale optical ﬂow com-
putations.

In slightly narrower settings, prior information on the video content can allow
more ﬁne-grained models. Articulated pose is widely used in cases where humans
are guaranteed to be present [33]. Pose estimation and activity recognition as a
joint (multi-task) problem has recently shown to improve both tasks [25]. Some-
what related to our work, Structural RNNs [19] perform activity recognition
by integrating features from semantic objects and their relationships. However,
they handle the temporal evolution of tracked objects in videos with a set of
RNNs, each of which corresponds to cliques in a graph which models the spatio-
temporal relationships between these objects. This graph is hand-crafted man-
ually for each application, though related work provides learnable connections
via gating functions [8].

Attention models are a way to structure deep networks in an often generic
way. They are able to iteratively focus attention to speciﬁc parts in the data with-
out requiring prior knowledge about part or object positions. In activity recog-
nition, they have gained some traction in recent years, either as soft-attention
on articulated pose (joints) [36], on feature map cells [34,39], on time [45] or on
parts in raw RGB input through diﬀerentiable crops [3].

When raw video data is globally fed into deep neural networks, they focus
on extracting spatio-temporal features and perform aggregations. It has been
shown that these techniques fail on challenging ﬁne-grained datasets, which re-
quire learning long temporal dependencies and human-object interactions. A
concentrated eﬀort has been made to create large scale datasets to overcome
these issues [13,12,23,14].

Relational Reasoning. Relational reasoning is a well studied ﬁeld for many
applications ranging from visual reasoning [32] to reasoning about physical
systems [4]. Battaglia et al. [4] introduce a fully-diﬀerentiable network physics
engine called Interaction Network (IN). IN learns to predict several physical
systems such as gravitational systems, rigid body dynamics, and mass-spring
systems. It shows impressive results; however, it learns from a virtual environ-
ment, which provides access to virtually unlimited training examples. Following
the same perspective, Santoro et al. [32] introduced Relation Network (RN),

Object Level Visual Reasoning in Videos

5

a plug-in module for reasoning in deep networks. RN shows human-level per-
formance in Visual Question Answering (VQA) by inferring pairwise “object”
relations. However, in contrast to our work, the term “object” in [32] does not
refer to semantically meaningful entities, but to discrete cells in feature maps.
The number of interactions therefore grows with feature map resolutions, which
makes it diﬃcult to scale. Furthermore, a recent study [21] has shown that some
of these results are subject to dataset bias and do not generalize well to small
changes in the settings of the dataset.

In the same line, a recent work [38] has shown promising results on discov-
ering objects and their interactions in an unsupervised manner using training
examples from virtual environments. In [41], attention and relational modules
are combined on a graph structure. From a diﬀerent perspective, [27] show that
relational reasoning can be learned for visual reasoning in a data driven way with-
out any prior using conditional batch normalization with a feature-wise aﬃne
transformation based on conditioning information. In an opposite approach, a
strong structural prior is learned in the form of a complex attention mecha-
nism: in [18], an external memory module combined with attention processes
over input images and text questions, performing iterative reasoning for VQA.
While most of the discussed work has been designed for VQA and for pre-
dictions on physical systems and environments, extensions have been proposed
for video understanding. Reasoning in videos on a mask or segmentation level
has been attempted for video prediction [24], where the goal was to leverage se-
mantic information to be able predict further into the future. Zhou et al [5] have
recently shown state-of-the-art performance on challenging datasets by extend-
ing Relation Network to video classiﬁcation. Their chosen entities are frames, on
which they employ RN to reason on a temporal level only through pairwise frame
relations. The approach is promising, but restricted to temporal contextual in-
formation without an understanding on a local object level, which is provided
by our approach.

Reasoning over sets of objects is somewhat related to reasoning from unstruc-
tured data points, as done in PointNet [29], designed to learn from unordered sets
of points. PointNet shares many properties with DeepSet [46] which is a more
general framework for extracting information from sets of objects. To some ex-
tent, our work is related to PointNet, as we handle unordered sets of objects in a
permutation invariant way. However, we have an object relation viewpoint that
directly reasons over relationships between these semantic entities.

3 Object-level Visual Reasoning in Space and Time

Our goal is to extract multiple types of cues from a video sequence: interactions
between predicted objects and their semantic classes, as well as local and global
motion in the scene. We formulate this objective as a neural architecture with
two heads: an activity head and an object head. Figure 2 gives a functional
overview of the model. Both heads share common features up to a certain layer
shown in red in the ﬁgure. The activity head, shown in orange in the ﬁgure,

6

Baradel et al.

Fig. 2. A functional overview of the model. A global convolutional model extracts
features and splits into two heads trained to predict, respectively activity classes and
object classes. The latter are predicted by pooling over object instance masks, which
are predicted by an additional convolutional model. The object instances are passed
through a visual reasoning module.

is a CNN-based architecture employing convolutional layers, including spatio-
temporal convolutions, able to extract global motion features. However, it is not
able to extract information from an object level perspective. We leverage the
object head to perform reasoning on the relationships between predicted object
instances.

Our main contribution is a new structured module called Object Relation
Network (ORN), which is able to perform spatio-temporal reasoning between
detected object instances in the video. ORN is able to reason by modeling how
objects move, appear and disappear and how they interact between two frames.
In this section, we will ﬁrst describe our main contribution, the ORN network.
We then provide details about object instance features, about the activity head,
and ﬁnally about the ﬁnal recognition task. In what follows, lowercase letters
denote 1D vectors while uppercase letters are used for 2D and 3D matrices or
higher order tensors. We assume that the input of our system is a video of T
frames denoted by X1:T = (Xt)T
t=1 where Xt is the RGB image at timestep t.
The goal is to learn a mapping from X1:T to activity classes y.

3.1 Object Relation Network

ORN (Object Relation Network) is a module for reasoning between semantic
objects through space and time. It captures object moves, arrivals and interac-
tions in an eﬃcient manner. We suppose that for each frame t, we have a set
of objects k with associated features ok
t . Objects and features are detected and
computed by the object head described in Section 3.2.

Reasoning about activities in videos is inherently temporal, as activities fol-
low the arrow of time [28], i.e. the causality of the time dimension imposes that
past actions have consequences in the future but not vice-versa. We handle this
by sampling: running a process over time t, and for each instant t, sampling a

Object Level Visual Reasoning in Videos

7

second frame t(cid:48) with t(cid:48)<t. Our network reasons on objects which interact be-
tween pairs of frames and their corresponding sets of objects Ot(cid:48) = (cid:8)ok
t(cid:48)
and Ot = (cid:8)ok
all input objects from the combined set of both frames:

(cid:9)K(cid:48)
k=1
(cid:9)K
k=1. The goal is to learn a general function deﬁned on the set of

t

gt = g(o1

t(cid:48), . . . , oK(cid:48)

t(cid:48) , o1

t , . . . , oK

t ).

(1)

The objects in this set are unordered, aside for the frame they belong to.

This task is related to a problem raised in the PointNet algorithm [29] dis-
cussed in Section 2. PointNet approximates a general function g over an item
set
as a symmetric function g(cid:48) on transformed elements of
=
S
the set

x1, x2, . . . , xN }
{

g(x1, x2, . . . , xN )

g(cid:48)(h(x1), h(x2), . . . , h(xN )).

(2)

≈

In [29], g(cid:48) is a max pooling operation. The authors show that it allows universal
approximation of continuous sets of functions given that the hidden representa-
tion (the output of the mapping h(
·

)) is of suﬃciently high dimension.
We argue that the approximation in (2) can be extended as follows:

g(x1, x2, . . . , xN )

fφ

≈

hc (

∪i

∈

c xi)

(3)

(cid:33)

(cid:32)

(cid:88)

c

∈C

C

is the set of cliques of a graph deﬁned over the item set

where
is the
concatenation operator and we chose the sum operator as symmetric function.
The input dimension of the non-linearity hc(
) depends on the size of clique c
·
is composed of
but maps to a ﬁxed output dimension H. In the case where
unary cliques only, form (3) decomposes like (2) with the exception of a diﬀerent
symmetry operator (sum instead of max pooling). Choosing diﬀerent graphical
structures through
will lead to diﬀerent terms in the summation and allows
modeling diﬀerent types of interactions between items in the item set.

∪

S

S

C

,

C

Note that the interactions between items in (3) are not exclusively modeled
). Indeed, it is interesting to note, that the graphical decomposi-
through hc(
·
tion provided by
leads to interactions which are diﬀerent from the interactions
the same decomposition would provide when used in a probabilistic graphical
model, like for instance a Markov Random Field (MRF). In particular, a de-
composition into unary terms only, as given in equation (2), does not lead to
independence between items, whereas an MRF with unary terms only is equiva-
lent to a distribution over independent random variables. This is a consequence
of the global mapping fφ(
), which is deﬁned on the sum over all direct inter-
·
actions. Higher-order interactions between several items not directly modeled
) can eventually be learned by the model through
through a non-linearity hc(
·
the joint output space of all hc(
), provided that the dimensionality H of this
·
space is high enough to incorporate all interactions. However, whereas the map-
ping hc(
) provides a direct model of interactions between pairs of items, learning
·
interactions between two items (j, k), which are not directly captured through a

8

Baradel et al.

Fig. 3. ORN in the object head operating on detected instances of objects.

clique c and its corresponding hc(
), requires learning a corresponding subspace
·
).
in the common output space spanned by all hc(
·

This leads to the question of how to deﬁne the trade-oﬀ between the com-
and the output dimensionality H of the mapping
), both of which will determine the complexity of the modeled interactions.
·
will increase the input dimension (and there-
) as well as the computational complexity
·

plexity of the decomposition
hc(
Increasing the size of cliques in
fore the capacity) of the mapping hc(
of the sum operation.

C

C

Inspired by relational networks [32], we chose to directly model inter-frame
interactions between pairs of objects (j, k) and leave modeling of higher-order
interactions to the output space of the mappings hθ and the global mapping fφ:

(4)

(5)

gt =

hθ(oj

t(cid:48), ok
t )

(cid:88)

j,k

rt = fφ(gt, rt

1)

−

In order to better directly model long-range interactions, we make the global
mapping fφ(

) recurrent, which leads to the following form:
·

,
·

where rt represents the recurrent object reasoning state at time t and gt is the
global inter-frame interaction inferred at time t such as described in Equation 4.
In practice, this is implemented as a GRU, but for simplicity we omitted the
) are implemented as an
gates in Equation (5). The pairwise mappings hθ(
·
·
MLP. Figure 3 provides a visual explanation of the object head’s operating
through time.

,

Our proposed ORN diﬀers from [32] in three main points:

Objects have a semantic deﬁnition — we model relationships with respect
to semantically meaningful entities (object instances) instead of feature map
cells which do not have a semantically meaningful spatial extent. We will show
in the experimental section that this is a key diﬀerence.

Object Level Visual Reasoning in Videos

9

Objects are selected from diﬀerent frames — we infer object pairwise
relations only between objects present in two diﬀerent sets. This is a key design
choice which allows our model to reason about changes in object relationships
over time.
Long range reasoning — integration of the object relations over time is re-
). Since reasoning from a full sequence cannot
current by using a RNN for fφ(
·
be done by inferring the relations between two frames, fφ(
) allows long range
·
reasoning on sequences of variable length.

3.2 Object instance features

t

(cid:9)K
The object features Ot = (cid:8)ok
k=1 for each frame t used for the ORN module
described above are computed and collected from local regions predicted by
a mask predictor. Independently for each frame Xt of the input data block,
we predict object instances as binary masks Bk
t and associated object class
predictions ck
t , a distribution over C classes. We use Mask-RCNN [15], which
is able to detect objects in a frame using region proposal networks [30] and
produces a high quality segmentation mask for each object instance.

The objective is to collect features for each object instance, which jointly
describe its appearance, the change in its appearance over time, and its shape,
i.e. the shape of the binary mask. In theory, appearance could also be described
by pooling the feature representation learned by the mask predictor (Mask R-
CNN). However, in practice we choose to pool features from the dedicated object
head such as shown in Figure 2, which also include motion through the spatio-
temporal convolutions shared with the activity head:

t = ROI-Pooling(Ut, Bk
uk
t )

(6)

where Ut is the feature map output by the object head, uk
vector of appearance and appearance change of object k.

t is a D-dimensional

Shape information from the binary mask Bk
t is extracted through the follow-
ing mapping function: bk
) is a MLP. Information about
t ), where gφ(
·
object k in image Xt is given by a concatenation of appearance, shape, and
object class: ok

t = gφ(Bk

t uk

t ck

t ].

t = [ bk

3.3 Global Motion and Context

Current approaches in video understanding focus on modeling the video from
a high-level perspective. By a stack of spatio-temporal convolution and pooling
they focus on learning global scene context information. Eﬀective activity recog-
nition requires integration of both of these sources: global information about the
entire video content in addition to relational reasoning for making ﬁne distinc-
tions regarding object interactions and properties.

In our method, local low-level reasoning is provided through object head
and the ORN module such as described above in Section 3.1. We complement

10

Baradel et al.

this representation by high-level context information described by Vt which are
feature outputs from the activity head (orange block in Figure 2).

We use spatial global average pooling over Vt to output T D-dimensional
feature vectors denoted by vt, where vt corresponds to the context information
of the video at timestep t.

We model the dynamics of the context information through time by employ-

ing a RNN fγ(

) given by:
·

where s is the hidden state of fγ(
context though time.

) and gives cues about the evolution of the
·

st = fγ(vt, st

1)

−

(7)

3.4 Recognition

Given an input video sequence X1:T , the two diﬀerent streams corresponding to
the activity head and the object head result in the two representations h and r,
respectively where h = (cid:80)
trt. Each representation is the hidden
state of the respective GRU, which were described in the preceding subsections.
Recall that h provides the global motion context while r provides the object
reasoning state output by the ORN module. We perform independent linear
classiﬁcation for each representation:

tht and r = (cid:80)

y1 = W h
y2 = Z r

(8)

(9)

where y1, y2 correspond to the logits from the activity head and the object head,
respectively, and W and Z are trainable weights (including biases). The ﬁnal
prediction is done by averaging logits y1 and y2 followed by softmax activation.

4 Network Architectures and feature dimensions

×

×

W

The input RGB images Xt are of size R3
H where W and H correspond to
the width and height and are of size 224 each. The object and activity heads
(orange and green in Figure 2) are a joint convolutional neural network with
Resnet50 architecture pre-trained on ImageNet/ILSVRC [31], with Conv1 and
Conv5 blocks being inﬂated to 2.5D convolutions [44] (3D convolutions with a
separable temporal dimension). This choice has been optimized on the validation
set, as explained in Section 6 and shown in Table 5.

The last conv5 layers have been split into two diﬀerent heads (activity head
and object head). The intermediate feature representations Ut and Vt are of di-
mensions 2048
7, respectively. We provide a higher
×
spatial resolution for the feature maps Ut of the object head to get more precise
local descriptors. This can be done by changing the stride of the initial conv5
layers from 2 to 1. Temporal convolutions have been conﬁgured to keep the same
time temporal dimension through the network.

14 and 2048

14

×

×

×

×

×

T

T

7

Object Level Visual Reasoning in Videos

11

Global spatial pooling of activity features results in a 2048 dimensional fea-
ture vector fed into a GRU with 512 dimensional hidden state st. ROI-Pooling
of object features results in 2048 dimensional feature vectors uk
t . The encoder of
the binary mask is a MLP with one hidden layer of size 100 and outputs a mask
embedding bk
t of dimension 100. The number of object classes is 80, which leads
in total to a 2229 dimensional object feature vector ok
t .

The non-linearity hθ(

) is implemented as an MLP with 2 hidden layers each
·
with 512 units and produces an 512 dimensional output space. fφ(
) is imple-
·
mented as a GRU with a 256 dimension hidden state rt. We use ReLU as the
activation function after each layer for each network.

5 Training

We train the model with a loss split into two terms:

Ltot =

L

(cid:16) ˆy1 + ˆy2
2

(cid:17)

, y

+

(cid:88)

(cid:88)

t

k

(ˆck

t , ck

t ).

L

(10)

L

is the cross-entropy loss. The ﬁrst term corresponds to supervised ac-
where
tivity class losses comparing two diﬀerent activity class predictions to the class
ground truth: ˆy1 is the prediction of the activity head, whereas ˆy2 is the pre-
diction of the object head, as given by Equations (8) and (9), respectively.

The second term is a loss which pushes the features U of the object towards
representations of the semantic object classes. The goal is to obtain features
related to, both, motion (through the layers shared with the activity head), as
well as as object classes. As ground-truth object classes are not available, we
deﬁne the loss as the cross-entropy between the class label ck
t predicted by the
mask predictor and a dedicated linear class prediction ˆck
t based on features uk
t ,
which, as we recall, are RoI-pooled from U:

t = R uk
ck
t

(11)

where R trainable parameters (biases integrated) learned end-to-end together
with the other parameters of the model.

We found that ﬁrst training the object head only and then the full network
was performing better. A ResNet50 network pretrained on ImageNet is modiﬁed
by inﬂating some of its ﬁlters to 2.5 convolutions (3D convolutions with the time
dimension separated), as described in Section 4; then by ﬁne-tuning.

We train the model using the Adam optimizer [22] with an initial learning
4 on 30 epochs and use early-stopping criterion on the validation set
50 minutes per epoch on 4

rate of 10−
for hyper-parameter optimization. Training takes
Titan XP GPUs with clips of 8 frames.

∼

12

Baradel et al.

6 Experimental results

We evaluated the method on three standard datasets, which represent diﬃcult
ﬁne-grained activity recognition tasks: the Something-Something dataset, the
VLOG dataset and the recently released EPIC Kitchens dataset.

Something-Something (SS) is a recent video classiﬁcation dataset with
108,000 example videos and 157 classes [13]. It shows humans performing diﬀer-
ent actions with diﬀerent objects, actions and objects being combined in diﬀerent
ways. Solving SS requires common sense reasoning and the state-of-the-art meth-
ods in activity recognition tend to fail, which makes this dataset challenging.

VLOG is a multi-label binary classiﬁcation of human-object interactions
recently released with 114,000 videos and 30 classes [12]. Classes correspond
to objects, and labels of a class are 1 if a person has touched a certain object
during the video, otherwise they are 0. It has recently been shown, that state-
of-the-art video based methods [6] are outperformed on VLOG by image based
methods like ResNet-50 [16], although these video methods outperform image
based ResNet-50 on large-scale video datasets like the Kinetics dataset [6]. This
suggests a gap between traditional datasets like Kinetics and the ﬁne-grained
dataset VLOG, making it particularly diﬃcult.

EPIC Kitchens (EPIC) is an egocentric video dataset recently released
containing 55 hours recording of daily activities [7]. This is the largest in ﬁrst-
person vision and the activities performed are non-scripted, which makes the
dataset very challenging and close to real world data. The dataset is densely
annotated and several tasks exist such as object detection, action recognition and
action prediction. We focus on action recognition with 39’594 action segments
in total and 125 actions classes (i.e verbs). Since the test set is not available yet
we conducted our experiments on the training set (28’561 videos). We use the
videos recorded by person 01 to person 25 for training (22’675 videos) and deﬁne
the validation set as the remaining videos (5’886 videos).

For all datasets we rescale the input video resolution to 256

256. While
training, we crop space-time blocks of 224
224 spatial resolution and L frames,
with L=8 for the SS dataset and L=4 for VLOG and EPIC. We do not perform
any other data augmentation. While training we extract L frames from the entire
video by splitting the video into L sub-sequences and randomly sampling one
frame per sub-sequence. The output sequence of size L is called a clip. A clip
aims to represent the full video with less frames. For testing we aggregate results
of 10 clips. We use lintel [10] for decoding video on the ﬂy.

×

×

The ablation study is done by using the train set as training data and we
report the result on the validation set. We compare against other state-of-the-
art approaches on the test set. For the ablation studies, we slightly decreased
the computational complexity of the model: the base network (including activity
and object heads) is a ResNet-18 instead of ResNet-50, a single clip of 4 frames
is extracted from a video at test time.

Comparison with other approaches. Table 1 shows the performance of
the proposed approach on the VLOG dataset. We outperform the state of the art

Object Level Visual Reasoning in Videos

13

Table 1. Results on Hand/Semantic Object Interaction Classiﬁcation (Average pre-
cision in % on the test set) on VLOG dataset. R50 and I3D implemented by [12].

s
r
e
p
a
p
/
k
o
o
b

e
b
u
t
/
e
l
t
t
o
b

g
n
i
d
d
e
b

P
A
m

g
a
b

d
e
b

l

w
o
b

x
o
b

h
s
u
r
b

e
n
o
h
p
-
l
l
e
c

t
e
n
i
b
a
c

g
n
i
h
t
o
l
c

p
u
c

r
o
o
d

s
r
e
w
a
r
d

d
o
o
f

k
r
o
f

e
f
i
n
k

p
o
t
p
a
l

e
v
a
w
o
r
c
i
m

n
e
v
o

l
i
c
n
e
p
/
n
e
p

r
o
t
a
r
e
g
i
r
f
e
r

w
o
l
l
i
p

e
t
a
l
p

n
o
o
p
s

k
n
i
s

l
a
m
i
n
a

d
e
ﬀ
u
t
s

h
s
u
r
b
h
t
o
o
t

e
l
b
a
t

l
e
w
o
t

R50 [16] 40.5 29.7 68.9 65.8 64.5 58.2 33.1 22.1 19.0 23.9 54.0 45.5 28.6 49.2 28.7 49.6 19.4 37.5 62.9 48.8 23.0 36.9 39.2 12.5 55.9 58.8 31.1 57.4 26.8 39.6 22.9
I3D [6]
39.7 24.9 71.7 71.4 62.5 57.1 27.1 19.2 33.9 20.7 50.6 45.8 24.7 54.7 19.1 50.8 19.3 41.9 54.0 27.5 21.4 37.4 42.9 12.6 42.5 60.4 33.9 46.0 23.5 59.6 34.7
Ours
44.7 30.2 72.3 70.7 64.9 59.8 38.2 24.6 26.3 22.4 64.5 47.2 35.4 57.9 25.2 48.5 24.5 40.2 72.0 54.1 26.5 39.9 48.6 15.2 53.5 60.7 36.8 52.8 27.9 64.0 37.6

≈

4.2 points (44.7% accuracy against
on this challenging dataset by a margin of
40.5% by [16]). As mentioned above, traditional video approaches tend to fail
on this challenging ﬁne-grained dataset, providing inferior results. Table 3 shows
performance on SS where we outperform the state of the art given by very recent
methods (+2.3 points). On EPIC we re-implement standard baselines and report
results on the validation set (Table 4) since the test set is not available. Our full
method reports an accuracy of 40.89 and outperforms baselines by a large margin
(
+7.9 points respectively for against CNN-2D and I3D based on a
+6.4 and
≈
ResNet-18).

≈

Eﬀect of object-level reasoning. Table 2 shows the importance of rea-
soning on the performance of the method. The baseline corresponds to the per-
formance obtained by the activity head trained alone (inﬂated ResNet, in the
ResNet-18 version for this table). No object level reasoning is present in this
baseline. The proposed approach (third line) including an object head and the
ORN module gains 0.8, 2.5 and 2.4 points compared to our baseline respectively
on SS, on EPIC and on VLOG. This indicates that the reasoning module is able
to extract complementary features compared to the activity head.

Using semantically deﬁned objects proved to be important and led to a gain of
2 points on EPIC and 2.3 points on VLOG for the full model (6/12.7 points using
the object head only) compared to an extension of Santoro et al. [32] operating
on pixel level. This indicates importance of object level reasoning. The gain on
SS is smaller (0.7 point with the full model and 7.8 points with the object head
only) and can be explained by the diﬀerence in spatial resolution of the videos.
Object detections and predictions of the binary masks are done using the initial
1183 and for EPIC
video resolution. The mean video resolution for VLOG is 660
157 for SS. Mask-RCNN has been trained on images of
is 640
×
resolution 800
800 and thus performs best on higher resolutions. The quality of
the object detector is important for leveraging object level understanding then
for the rest of the ablation study we focus on EPIC and VLOG datasets.

480 against 100

×

×

×

The function fφ in Equation (5) is an important design choice in our model.
In our proposed model, fφ is recurrent over time to ensure that the ORN module
captures long range reasoning over time, as shown in Equation (5). Removing
the recurrence in this equation leads to an MLP instead of a (gated) RNN, as
evaluated in row 4 of Table 2. Performance decreases by 1.1 point on VLOG
and 1.4 points on EPIC. The larger gap for EPIC compared to VLOG and
can arguably be explained by the fact that in SS actions cover the whole video,

14

Baradel et al.

Table 2. Ablation study with ResNet-18 backbone. Results in %: Top-1 accuracy for
EPIC and SS datasets, and mAP for VLOG dataset.

Method

Object type

EPIC

VLOG
obj. head 2 heads obj. head 2 heads obj. head 2 heads

SS

Baseline
ORN
ORN
ORN-mlp

-
pixel
COCO
COCO

ORN
ORN
ORN

COCO-visual
COCO-shape
COCO-class

ORN
ORN clique-1
ORN clique-3

COCO-intra
COCO
COCO

-
23.71
29.94
28.15

28.45
21.92
21.96

29.25
28.25
22.61

38.33
38.83
40.89
39.41

38.92
37.16
37.75

38.10
40.18
37.67

-
14.40
27.14
25.40

22.92
7.18
13.40

26.78
26.48
27.05

35.03
35.18
37.49
36.35

35.49
35.39
35.94

36.28
36.71
36.04

-
2.51
10.26
-

31.31
31.43
32.12
-

-
-
-

-
-
-

-
-
-

-
-
-

Table 3. Experimental results on the
Something-Something dataset (classi-
ﬁcation accuracy in % on the test set).

Table 4. Experimental results on the
EPIC Kitchens dataset (accuracy in %
on the validation set – methods with ∗
have been re-implemented).

Methods

C3D + Avg [13]
I3D [13]
MultiScale TRN [5]

Ours

Top1

21.50
27.63
33.60

35.97

Methods

Top1

R18 [16]∗
I3D-18 [6]∗

32.05
34.20

Ours

40.89

while solving VLOG requires detecting the right moment when the human-object
interaction occurs and thus long range reasoning plays a less important role.

Visual features extracted from object regions are the most discriminative,
however object shapes and labels also provide complementary information. Fi-
nally, the last part of Table 2 evaluates the eﬀect of the cliques size for model-
ing the interactions between objects and show that pairwise cliques outperform
cliques of size 1 and 3. We would like to recall, that even with unary cliques
only, interactions between objects are still modeled. However, the model needs
to ﬁnd subspaces in the hidden representations associated to each interaction.

CNN architecture and kernel inﬂations. The convolutional architecture
of the model was optimized over the validation set of the SS dataset, as shown
in Table 5. The architecture itself (in terms of numbers of layers, ﬁlters etc.)
is determined by pre-training on image classiﬁcation. We optimized the choice
of ﬁlter inﬂations from 2D to 2.5D or 3D for several convolutional blocks. This
has been optimized for the single head model and using a ResNet-18 variant
to speed up computation. Adding temporal convolutions increases performance
up to 100% w.r.t. to pure 2D baselines. This indicates, without surprise, that
motion is a strong cue. Inﬂating kernels to 2.5D on the input side and on the

Object Level Visual Reasoning in Videos

15

Table 5. Eﬀect of the CNN architecture (choice of kernel inﬂations) on a single head
ResNet-18 network. Accuracy in % on the validation set of Something-Something is
shown. 2.5D kernels are separable kernels: 2D followed by a 1D temporal.

Conv1

Conv2

Conv3

Conv4

Conv5

Aggreg

(cid:88)
-
(cid:88)
(cid:88)

2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D GAP RNN
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88) -
-
-

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
(cid:88) -
-
(cid:88) -
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
- (cid:88) (cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
- (cid:88) -
- (cid:88) -
- (cid:88) -
- (cid:88)
-
- (cid:88)
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
- (cid:88) -
- (cid:88) -
(cid:88) -
-
- (cid:88)
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
-
- (cid:88) -
-

- (cid:88) (cid:88) -
- (cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
-
-
-
-
-

-
-
-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-

SS

15.73
-
(cid:88) 15.88
31.42
-
27.58
-

31.28
32.06
32.25
31.31
32.79
33.77

28.71
31.42
20.05
22.52

output side provided best performances, suggesting that temporal integration
is required at a very low level (motion estimation) as well as on a very high
level, close to reasoning. Our study also corroborates recent research in activity
recognition, indicating that 2.5D kernels provide a good trade-oﬀ between high-
capacity and learnable numbers of parameters. Finally temporal integration via
RNN outperforms global average pooling over space and time. The choice of
a (gated) RNN for temporal integration of the activity head features proved
important (see Table 5) compared to global average pooling (GAP) over space
and time.

Visualizing the learned object interactions. Figure 4 shows visualiza-
tions of the pairwise object relationships the model learned from data, in partic-
ular from the VLOG dataset. Each graph is computed for a given activity class,
and strong arcs between two nodes in the graph indicate strong relationships
between the object classes, i.e. the model detects a high correlation between
these relationships and the activity. The graphs were obtained by thresholding
the summed activations of each pairwise relationship (j, k) in equation (4). Each
pair (j, k) can be assigned a pair of object classes cj
t through the predic-
tions of the object instance mask predictor. Integrating over all samples of the
dataset for a given class leads to the visualizations in Figure 4. We can see that
the object interactions are highly relevant to the detected activities: the person-
touches-bed activity is correlated to interactions between relevant object classes
person and bed. Similarly, activities human-bowl interaction and human-cup in-
teraction show interactions with the respective objects bowl and cup. Moreover,
other recovered relationships are highly correlated to the scene (for example,
dining-table and bowl for activity human-bowl interaction).

t and ck

16

Baradel et al.

Fig. 4. Example of object pairwise interactions learned by our model on VLOG for
four diﬀerent classes. Objects co-occurrences are at the top and learned pairwise objects
interactions are at the bottom. Line thickness indicates learned importance of a given
relation. Interactions have been normalized by the object co-occurrences.

Fig. 5. Examples of failure cases – a) small sized objects (on the left). Our model
detects a cell phone and a person but fails to detect hand-cell-phone contact; b) con-
fusion between semantically similar objects (on the right). The model falsly predicts
hand-cup contact instead of hand-glass-contact even though the wine glass is detected.

Finally, Figure 5 shows some failure cases, which are either due to errors

done by the object mask prediction (Mask R-CNN) or by the ORN itself.

7 Conclusion

We presented a method for activity recognition in videos which leverages ob-
ject instance detections for visual reasoning on object interactions over time.
The choice of reasoning over semantically well-deﬁned objects is key to our ap-
proach and outperforms state of the art methods which reason on grid-levels,
such as cells of convolutional feature maps. Temporal dependencies and causal
relationships are dealt with by integrating relationships between diﬀerent time
instants. We evaluated the method on three diﬃcult datasets, on which standard
approaches do not perform well, and report state-of-the-art results.

Acknowledgements. This work was funded by grant Deepvision (ANR-15-
CE23-0029, STPGP-479356-15), a joint French/Canadian call by ANR & NSERC.

Object Level Visual Reasoning in Videos

17

References

1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B.,
Vijayanarasimhan, S.: Youtube-8m: A large-scale video classiﬁcation benchmark.
arXiv preprint arxiv:1609.08675 (2016)

2. Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., Baskurt, A.: Sequential deep

learning for human action recognition. In: HBU (2011)

3. Baradel, F., Wolf, C., Mille, J., Taylor, G.: Glimpse clouds: Human activity recog-

nition from unstructured feature points. In: CVPR (2018)

4. Battaglia, P.W., Pascanu, R., Lai, M., Rezende, D.J., Kavukcuoglu, K.: Interaction

networks for learning about objects, relations and physics. In: NIPS (2016)

5. Bolei, Z., Zhang, A.A., Torralba, A.: Temporal relational reasoning in videos. In:

ECCV (2018)

6. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the

kinetics dataset. In: CVPR (2017)

7. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Scaling egocentric
vision: The epic-kitchens dataset. In: ECCV (2018)

8. Deng, Z., Vahdat, A., Hu, H., Mori, G.: Structure inference machines: Recurrent
neural networks for analyzing relations in group activity recognition. In: CVPR
(2016)

9. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S.,
Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: CVPR (2015)

10. Duke, B.: Lintel: Python video decoding. https://github.com/dukebw/lintel

(2018)

11. Fleuret, F., Li, T., Dubout, C., Wampler, E.K., Yantis, S., Geman, D.: Comparing
machines and humans on a visual categorization test. Proceedings of the National
Academy of Sciences of the United States of America 108 43, 17621–5 (2011)
12. Fouhey, D.F., Kuo, W., Efros, A.A., Malik, J.: From lifestyle vlogs to everyday

interactions. In: CVPR (2018)

13. Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim,
H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau,
C., Bax, I., Memisevic, R.: The ”something something” video database for learning
and evaluating visual common sense. In: ICCV (2017)

14. Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan,
S., Toderici, G., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: Ava: A
video dataset of spatio-temporally localized atomic visual actions. arXiv preprint
arXiv:1705.08421 (2017)

15. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: ICCV (2017)
16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

17. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation

18. Hudson, D., Manning, C.: Compositional attention networks for machine reasoning.

In: CVPR (2016)

9(8), 1735–1780 (1997)

In: ICLR (2018)

19. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: Deep Learning

on Spatio-Temporal Graphs. In: CVPR (2016)

20. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-
scale video classiﬁcation with convolutional neural networks. In: CVPR (2014)

18

Baradel et al.

21. Kim, J., Ricci, M., Serre, T.: Not-so-CLEVR: Visual relations strain feedforward

neural networks. arXiv preprint arXiv:1802.03390 (2018)

22. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICML (2015)
23. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Ka-
landitis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome:
Connecting language and vision using crowdsourced dense image annotations. In-
ternational Journal of Computer Vision (IJCV) 123, 32–73 (2017)

24. Luc, P., , Neverova, N., Couprie, C., Verbeek, J., LeCun, Y.: Predicting deeper

into the future of semantic segmentation. In: ICCV (2017)

25. Luvizon, D., Picard, D., Tabia, H.: 2d/3d pose estimation and action recognition

using multitask deep learning. In: CVPR (2018)

26. Monfort, M., Zhou, B., Bargal, S.A., Andonian, A., Yan, T., Ramakrishnan,
K., Brown, L., Fan, Q., Gutfruend, D., Vondrick, C., Oliva, A.: Moments
in time dataset: one million videos for event understanding. arXiv preprint
arXiv:1801.03150 (2018)

27. Perez, E., Vries, H.D., Strub, F., Dumoulin, V., Courville, A.: Learning visual rea-
soning without strong priors. In: ICML Machine Learning in Speech and Language
Processing Workshop (2017)

28. Pickup, L.C., Pan, Z., Wei, D., Shih, Y., Zhang, C., Zisserman, A., Scholkopf, B.,

Freeman, W.T.: Seeing the arrow of time. In: CVPR (2014)

29. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for

3d classiﬁcation and segmentation. In: CVPR (2017)

30. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

31. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A., Fei-Fei, L.: Imagenet large scale
visual recognition challenge. IJCV 115(3), 211–252 (2015)

32. Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia,
P., Lillicrap, T.: A simple neural network module for relational reasoning. In: NIPS
(2017)

33. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: NTU RGB+D: A Large Scale Dataset

for 3D Human Activity Analysis. In: CVPR (2016)

34. Sharma, S., Kiros, R., Salakhutdinov, R.: Action recognition using visual attention.

35. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-

In: ICLR Workshop (2016)

nition in videos. In: NIPS (2014)

36. Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An End-to-End Spatio-Temporal
Attention Model for Human Action Recognition from Skeleton Data. In: AAAI
(2016)

37. Stabinger, S., Rodr´ıguez-S´anchez, A., Piater, J.: 25 years of CNNs: Can we compare

to human abstraction capabilities? In: ICANN (2016)

38. van Steenkiste, S., Chang, M., Greﬀ, K., Schmidhuber, J.: Relational neural ex-
pectation maximization: Unsupervised discovery of objects and their interactions.
In: ICLR (2018)

39. Sun, L., Jia, K., Chen, K., Yeung, D., Shi, B.E., Savarese, S.: Lattice long short-

term memory for human action recognition. In: ICCV (2017)

40. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-

poral features with 3d convolutional networks. In: ICCV (2015)

41. Velikovi, P., Cucurull, G., Casanova, A., Romero, A., Li, P., Bengio, Y.: Graph

attention networks. In: ICLR (2018)

Object Level Visual Reasoning in Videos

19

42. Wang, H., Kl¨aser, A., Schmid, C., Liu, C.L.: Action Recognition by Dense Trajec-

tories. In: CVPR (2011)

43. Watters, N., Zoran, D., Weber, T., Battaglia, P., Pascanu, R., Tacchetti, A.: Visual
interaction networks: Learning a physics simulator from video. In: NIPS (2017)
44. Xie, S., Sun, C., Huang, J., Tu, Z., Murphy, K.: Rethinking spatiotemporal feature

learning for video understanding. arXiv prepring arxiv:1712.04851 (2017)

45. Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori, G., Fei-Fei, L.: Every mo-
ment counts: Dense detailed labeling of actions in complex videos. arXiv preprint
arXiv:1507.05738 (2015)

46. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.R., Smola,
A.J.: Deep sets. In: NIPS. pp. 3391–3401 (2017), http://papers.nips.cc/paper/
6931-deep-sets.pdf

8
1
0
2
 
p
e
S
 
0
2
 
 
]

V
C
.
s
c
[
 
 
3
v
7
5
1
6
0
.
6
0
8
1
:
v
i
X
r
a

Object Level Visual Reasoning in Videos

Fabien Baradel1, Natalia Neverova2, Christian Wolf1,3,
Julien Mille4, and Greg Mori5

1 Universit´e Lyon, INSA Lyon, CNRS, LIRIS, F-69621, Villeurbanne, France,
firstname.lastname@liris.cnrs.fr
2 Facebook AI Research, Paris, France, nneverova@fb.com
3 INRIA, CITI Laboratory, Villeurbanne, France
4 Laboratoire d’Informatique de l’Univ. de Tours, INSA Centre Val de Loire,
41034, Blois, France, julien.mille@insa-cvl.fr
5 Simon Fraser University, Vancouver, Canada, mori@cs.sfu.ca

https://fabienbaradel.github.io/eccv18_object_level_visual_reasoning/

Abstract. Human activity recognition is typically addressed by detect-
ing key concepts like global and local motion, features related to object
classes present in the scene, as well as features related to the global con-
text. The next open challenges in activity recognition require a level of
understanding that pushes beyond this and call for models with capa-
bilities for ﬁne distinction and detailed comprehension of interactions
between actors and objects in a scene. We propose a model capable of
learning to reason about semantically meaningful spatio-temporal inter-
actions in videos. The key to our approach is a choice of performing
this reasoning at the object level through the integration of state of the
art object detection networks. This allows the model to learn detailed
spatial interactions that exist at a semantic, object-interaction relevant
level. We evaluate our method on three standard datasets (Twenty-BN
Something-Something, VLOG and EPIC Kitchens) and achieve state of
the art results on all of them. Finally, we show visualizations of the in-
teractions learned by the model, which illustrate object classes and their
interactions corresponding to diﬀerent activity classes.

Keywords: Video understanding · Human-object interaction

1

Introduction

The ﬁeld of video understanding is extremely diverse, ranging from extract-
ing highly detailed information captured by speciﬁcally designed motion cap-
ture systems [33] to making general sense of videos from the Web [1]. As in
the domain of image recognition, there exist a number of large-scale video
datasets [6,26,13,12,23,14], which allow the training of high-capacity deep learn-
ing models from massive amounts of data. These models enable detection of key
cues present in videos, such as global and local motion, various object categories
and global scene-level information, and often achieve impressive performance in
recognizing high-level, abstract concepts in the wild.

2

Baradel et al.

Fig. 1. Humans can understand what happened in a video (“the leftmost carrot was
chopped by the person”) given only a pair of frames. Along these lines, the goal of this
work is to explore the capabilities of higher-level reasoning in neural models operating
at the semantic level of objects and interactions.

However, recent attention has been directed toward a more thorough un-
derstanding of human-focused activity in diverse internet videos. These eﬀorts
range from atomic human actions [14] to ﬁne-grained object interactions [13]
to everyday, commonly occurring human-object interactions [12]. This returns
us to a human-centric viewpoint of activity recognition where it is not only the
presence of certain objects / scenes that dictate the activity present, but the
manner, order, and eﬀects of human interaction with these scene elements that
are necessary for understanding. In a sense, this is akin to the problems in current
3D human activity recognition datasets [33], but requires the more challenging
reasoning and understanding of diverse environments common to internet video
collections.

Humans are able to infer what happened in a video given only a few sam-
ple frames. In particular, they can infer complex activities happening between
pairs of frames. This faculty is called reasoning and is a key component of hu-
man intelligence. As an example we can consider the pair of images in Figure 1,
which shows a complex situation involving articulated objects (human, carrots
and knife), the change of location and composition of objects. For humans it is
straightforward to draw a conclusion on what happened (a carrot was chopped
by the human). Humans have this extraordinary ability of performing visual rea-
soning on very complicated tasks while it remains unattainable for contemporary
computer vision algorithms [37,11].

The ability to perform visual reasoning in computer vision algorithms is still
an open problem. Attempts have been made for learning interactions between
diﬀerent entities in images with promising results on Visual Question Answering.
There have been a number of attempts to equip neural models with reasoning
abilities by training them to solve Visual Question Answering (VQA) problems.
Among proposed solutions are prior-less data normalization [27], structuring
networks to model relationships [32,43] as well as more complex attention based

Object Level Visual Reasoning in Videos

3

mechanisms [18]. At the same time, it was shown that high performance on exist-
ing VQA datasets can be achieved by simply discovering biases in the data [21].
We extend these eﬀorts to object level reasoning in videos. Since a video is
a temporal sequence, we leverage time as an explicit causal signal to identify
causal object relations. Our approach is related to the concept of the “arrow of
the time” [28] involving the “one-way direction” or “asymmetry” of time. Causal
event occurs before the event it aﬀects (A
B). In Figure 1 the knife was used
before the carrot switched over to the chopped-up state on the right side. For
a video classiﬁcation problem, we want to identify a causal event A happening
in a video that aﬀects its label B. But instead of identifying this causal event
directly from pixels we want to identify it from an object level perspective. We
believe that such an approach would be able to learn causal signals.

→

Following this hypothesis we propose to make a bridge between object de-
tection and activity recognition. Object detection allows us to extract low-level
information from a scene with all the present object instances and their semantic
meanings. However, detailed activity understanding requires reasoning over these
semantic structures, determining which objects were involved in interactions, of
what nature, and what were the results of these. To compound problems, the
semantic structure of a scene may change during a video (e.g. a new object can
appear, a person may make a move from one point to another one of the scene).
We propose an Object Relation Network (ORN), a neural network mod-
ule for reasoning between detected semantic object instances through space and
time. The ORN has potential to address these issues and conduct relational
reasoning over object interactions for the purpose of activity recognition. A set
of object detection masks ranging over diﬀerent object categories and temporal
occurrences is input to the ORN. The ORN is able to infer pairwise relationships
between objects detected at varying diﬀerent moments in time.

Code and object masks predictions will be publicly available6.

2 Related work

Action Recognition. Action recognition has a long history in computer vi-
sion. Pre-deep learning approaches in action recognition focused on handcrafted
spatio-temporal features including space-time interest points like SIFT-3D, HOG3D,
IDT and aggregated them using bag-of-words techniques. Some hand-crafted rep-
resentations, like dense trajectories [42], still give competitive performance and
are frequently combined with deep learning.

In the recent past, work has shifted to deep learning. Early attempts adapt
2D convolutional networks to videos through temporal pooling and 3D convolu-
tions [2,40]. 3D convolutions are now widely adopted for activity recognition with
the introduction of feature transfer by inﬂating pre-trained 2D convolutional ker-
nels from image classiﬁcation models trained on ImageNet/ILSVRC [31] through
3D kernels [6]. The downside of 3D kernels is their computational complexity

6 https://github.com/fabienbaradel/object_level_visual_reasoning

4

Baradel et al.

and the large number of learnable parameters, leading to the introduction of
2.5D kernels, i.e. separable ﬁlters in the form of a 2D spatial kernel followed by
a temporal kernel [44]. An alternative to temporal convolutions are Recurrent
Neural Networks (RNNs) in their various gated forms (GRUs, LSTMs) [17,9].

Karpathy et al. [20] presented a wide study on diﬀerent ways of connecting
information in spatial and temporal dimensions through convolutions and pool-
ing. On very general datasets with coarse activity classes they have showed that
there was a small margin between classifying individual frames and classifying
videos with more sophisticated temporal aggregation.

Simoyan et al. [35] proposed a widely adopted two-stream architecture for
action recognition which extracts two diﬀerent streams, one processing raw RGB
input and one processing pre-computed optical ﬂow images. The method out-
performed the state of the art, but relies on rather small scale optical ﬂow com-
putations.

In slightly narrower settings, prior information on the video content can allow
more ﬁne-grained models. Articulated pose is widely used in cases where humans
are guaranteed to be present [33]. Pose estimation and activity recognition as a
joint (multi-task) problem has recently shown to improve both tasks [25]. Some-
what related to our work, Structural RNNs [19] perform activity recognition
by integrating features from semantic objects and their relationships. However,
they handle the temporal evolution of tracked objects in videos with a set of
RNNs, each of which corresponds to cliques in a graph which models the spatio-
temporal relationships between these objects. This graph is hand-crafted man-
ually for each application, though related work provides learnable connections
via gating functions [8].

Attention models are a way to structure deep networks in an often generic
way. They are able to iteratively focus attention to speciﬁc parts in the data with-
out requiring prior knowledge about part or object positions. In activity recog-
nition, they have gained some traction in recent years, either as soft-attention
on articulated pose (joints) [36], on feature map cells [34,39], on time [45] or on
parts in raw RGB input through diﬀerentiable crops [3].

When raw video data is globally fed into deep neural networks, they focus
on extracting spatio-temporal features and perform aggregations. It has been
shown that these techniques fail on challenging ﬁne-grained datasets, which re-
quire learning long temporal dependencies and human-object interactions. A
concentrated eﬀort has been made to create large scale datasets to overcome
these issues [13,12,23,14].

Relational Reasoning. Relational reasoning is a well studied ﬁeld for many
applications ranging from visual reasoning [32] to reasoning about physical
systems [4]. Battaglia et al. [4] introduce a fully-diﬀerentiable network physics
engine called Interaction Network (IN). IN learns to predict several physical
systems such as gravitational systems, rigid body dynamics, and mass-spring
systems. It shows impressive results; however, it learns from a virtual environ-
ment, which provides access to virtually unlimited training examples. Following
the same perspective, Santoro et al. [32] introduced Relation Network (RN),

Object Level Visual Reasoning in Videos

5

a plug-in module for reasoning in deep networks. RN shows human-level per-
formance in Visual Question Answering (VQA) by inferring pairwise “object”
relations. However, in contrast to our work, the term “object” in [32] does not
refer to semantically meaningful entities, but to discrete cells in feature maps.
The number of interactions therefore grows with feature map resolutions, which
makes it diﬃcult to scale. Furthermore, a recent study [21] has shown that some
of these results are subject to dataset bias and do not generalize well to small
changes in the settings of the dataset.

In the same line, a recent work [38] has shown promising results on discov-
ering objects and their interactions in an unsupervised manner using training
examples from virtual environments. In [41], attention and relational modules
are combined on a graph structure. From a diﬀerent perspective, [27] show that
relational reasoning can be learned for visual reasoning in a data driven way with-
out any prior using conditional batch normalization with a feature-wise aﬃne
transformation based on conditioning information. In an opposite approach, a
strong structural prior is learned in the form of a complex attention mecha-
nism: in [18], an external memory module combined with attention processes
over input images and text questions, performing iterative reasoning for VQA.
While most of the discussed work has been designed for VQA and for pre-
dictions on physical systems and environments, extensions have been proposed
for video understanding. Reasoning in videos on a mask or segmentation level
has been attempted for video prediction [24], where the goal was to leverage se-
mantic information to be able predict further into the future. Zhou et al [5] have
recently shown state-of-the-art performance on challenging datasets by extend-
ing Relation Network to video classiﬁcation. Their chosen entities are frames, on
which they employ RN to reason on a temporal level only through pairwise frame
relations. The approach is promising, but restricted to temporal contextual in-
formation without an understanding on a local object level, which is provided
by our approach.

Reasoning over sets of objects is somewhat related to reasoning from unstruc-
tured data points, as done in PointNet [29], designed to learn from unordered sets
of points. PointNet shares many properties with DeepSet [46] which is a more
general framework for extracting information from sets of objects. To some ex-
tent, our work is related to PointNet, as we handle unordered sets of objects in a
permutation invariant way. However, we have an object relation viewpoint that
directly reasons over relationships between these semantic entities.

3 Object-level Visual Reasoning in Space and Time

Our goal is to extract multiple types of cues from a video sequence: interactions
between predicted objects and their semantic classes, as well as local and global
motion in the scene. We formulate this objective as a neural architecture with
two heads: an activity head and an object head. Figure 2 gives a functional
overview of the model. Both heads share common features up to a certain layer
shown in red in the ﬁgure. The activity head, shown in orange in the ﬁgure,

6

Baradel et al.

Fig. 2. A functional overview of the model. A global convolutional model extracts
features and splits into two heads trained to predict, respectively activity classes and
object classes. The latter are predicted by pooling over object instance masks, which
are predicted by an additional convolutional model. The object instances are passed
through a visual reasoning module.

is a CNN-based architecture employing convolutional layers, including spatio-
temporal convolutions, able to extract global motion features. However, it is not
able to extract information from an object level perspective. We leverage the
object head to perform reasoning on the relationships between predicted object
instances.

Our main contribution is a new structured module called Object Relation
Network (ORN), which is able to perform spatio-temporal reasoning between
detected object instances in the video. ORN is able to reason by modeling how
objects move, appear and disappear and how they interact between two frames.
In this section, we will ﬁrst describe our main contribution, the ORN network.
We then provide details about object instance features, about the activity head,
and ﬁnally about the ﬁnal recognition task. In what follows, lowercase letters
denote 1D vectors while uppercase letters are used for 2D and 3D matrices or
higher order tensors. We assume that the input of our system is a video of T
frames denoted by X1:T = (Xt)T
t=1 where Xt is the RGB image at timestep t.
The goal is to learn a mapping from X1:T to activity classes y.

3.1 Object Relation Network

ORN (Object Relation Network) is a module for reasoning between semantic
objects through space and time. It captures object moves, arrivals and interac-
tions in an eﬃcient manner. We suppose that for each frame t, we have a set
of objects k with associated features ok
t . Objects and features are detected and
computed by the object head described in Section 3.2.

Reasoning about activities in videos is inherently temporal, as activities fol-
low the arrow of time [28], i.e. the causality of the time dimension imposes that
past actions have consequences in the future but not vice-versa. We handle this
by sampling: running a process over time t, and for each instant t, sampling a

Object Level Visual Reasoning in Videos

7

second frame t(cid:48) with t(cid:48)<t. Our network reasons on objects which interact be-
tween pairs of frames and their corresponding sets of objects Ot(cid:48) = (cid:8)ok
t(cid:48)
and Ot = (cid:8)ok
all input objects from the combined set of both frames:

(cid:9)K(cid:48)
k=1
(cid:9)K
k=1. The goal is to learn a general function deﬁned on the set of

t

gt = g(o1

t(cid:48), . . . , oK(cid:48)

t(cid:48) , o1

t , . . . , oK

t ).

(1)

The objects in this set are unordered, aside for the frame they belong to.

This task is related to a problem raised in the PointNet algorithm [29] dis-
cussed in Section 2. PointNet approximates a general function g over an item
set
as a symmetric function g(cid:48) on transformed elements of
=
S
the set

x1, x2, . . . , xN }
{

g(x1, x2, . . . , xN )

g(cid:48)(h(x1), h(x2), . . . , h(xN )).

(2)

≈

In [29], g(cid:48) is a max pooling operation. The authors show that it allows universal
approximation of continuous sets of functions given that the hidden representa-
tion (the output of the mapping h(
·

)) is of suﬃciently high dimension.
We argue that the approximation in (2) can be extended as follows:

g(x1, x2, . . . , xN )

fφ

≈

hc (

∪i

∈

c xi)

(3)

(cid:33)

(cid:32)

(cid:88)

c

∈C

C

is the set of cliques of a graph deﬁned over the item set

where
is the
concatenation operator and we chose the sum operator as symmetric function.
The input dimension of the non-linearity hc(
) depends on the size of clique c
·
is composed of
but maps to a ﬁxed output dimension H. In the case where
unary cliques only, form (3) decomposes like (2) with the exception of a diﬀerent
symmetry operator (sum instead of max pooling). Choosing diﬀerent graphical
structures through
will lead to diﬀerent terms in the summation and allows
modeling diﬀerent types of interactions between items in the item set.

∪

S

S

C

,

C

Note that the interactions between items in (3) are not exclusively modeled
). Indeed, it is interesting to note, that the graphical decomposi-
through hc(
·
tion provided by
leads to interactions which are diﬀerent from the interactions
the same decomposition would provide when used in a probabilistic graphical
model, like for instance a Markov Random Field (MRF). In particular, a de-
composition into unary terms only, as given in equation (2), does not lead to
independence between items, whereas an MRF with unary terms only is equiva-
lent to a distribution over independent random variables. This is a consequence
of the global mapping fφ(
), which is deﬁned on the sum over all direct inter-
·
actions. Higher-order interactions between several items not directly modeled
) can eventually be learned by the model through
through a non-linearity hc(
·
the joint output space of all hc(
), provided that the dimensionality H of this
·
space is high enough to incorporate all interactions. However, whereas the map-
ping hc(
) provides a direct model of interactions between pairs of items, learning
·
interactions between two items (j, k), which are not directly captured through a

8

Baradel et al.

Fig. 3. ORN in the object head operating on detected instances of objects.

clique c and its corresponding hc(
), requires learning a corresponding subspace
·
).
in the common output space spanned by all hc(
·

This leads to the question of how to deﬁne the trade-oﬀ between the com-
and the output dimensionality H of the mapping
), both of which will determine the complexity of the modeled interactions.
·
will increase the input dimension (and there-
) as well as the computational complexity
·

plexity of the decomposition
hc(
Increasing the size of cliques in
fore the capacity) of the mapping hc(
of the sum operation.

C

C

Inspired by relational networks [32], we chose to directly model inter-frame
interactions between pairs of objects (j, k) and leave modeling of higher-order
interactions to the output space of the mappings hθ and the global mapping fφ:

(4)

(5)

gt =

hθ(oj

t(cid:48), ok
t )

(cid:88)

j,k

rt = fφ(gt, rt

1)

−

In order to better directly model long-range interactions, we make the global
mapping fφ(

) recurrent, which leads to the following form:
·

,
·

where rt represents the recurrent object reasoning state at time t and gt is the
global inter-frame interaction inferred at time t such as described in Equation 4.
In practice, this is implemented as a GRU, but for simplicity we omitted the
) are implemented as an
gates in Equation (5). The pairwise mappings hθ(
·
·
MLP. Figure 3 provides a visual explanation of the object head’s operating
through time.

,

Our proposed ORN diﬀers from [32] in three main points:

Objects have a semantic deﬁnition — we model relationships with respect
to semantically meaningful entities (object instances) instead of feature map
cells which do not have a semantically meaningful spatial extent. We will show
in the experimental section that this is a key diﬀerence.

Object Level Visual Reasoning in Videos

9

Objects are selected from diﬀerent frames — we infer object pairwise
relations only between objects present in two diﬀerent sets. This is a key design
choice which allows our model to reason about changes in object relationships
over time.
Long range reasoning — integration of the object relations over time is re-
). Since reasoning from a full sequence cannot
current by using a RNN for fφ(
·
be done by inferring the relations between two frames, fφ(
) allows long range
·
reasoning on sequences of variable length.

3.2 Object instance features

t

(cid:9)K
The object features Ot = (cid:8)ok
k=1 for each frame t used for the ORN module
described above are computed and collected from local regions predicted by
a mask predictor. Independently for each frame Xt of the input data block,
we predict object instances as binary masks Bk
t and associated object class
predictions ck
t , a distribution over C classes. We use Mask-RCNN [15], which
is able to detect objects in a frame using region proposal networks [30] and
produces a high quality segmentation mask for each object instance.

The objective is to collect features for each object instance, which jointly
describe its appearance, the change in its appearance over time, and its shape,
i.e. the shape of the binary mask. In theory, appearance could also be described
by pooling the feature representation learned by the mask predictor (Mask R-
CNN). However, in practice we choose to pool features from the dedicated object
head such as shown in Figure 2, which also include motion through the spatio-
temporal convolutions shared with the activity head:

t = ROI-Pooling(Ut, Bk
uk
t )

(6)

where Ut is the feature map output by the object head, uk
vector of appearance and appearance change of object k.

t is a D-dimensional

Shape information from the binary mask Bk
t is extracted through the follow-
ing mapping function: bk
) is a MLP. Information about
t ), where gφ(
·
object k in image Xt is given by a concatenation of appearance, shape, and
object class: ok

t = gφ(Bk

t uk

t ck

t ].

t = [ bk

3.3 Global Motion and Context

Current approaches in video understanding focus on modeling the video from
a high-level perspective. By a stack of spatio-temporal convolution and pooling
they focus on learning global scene context information. Eﬀective activity recog-
nition requires integration of both of these sources: global information about the
entire video content in addition to relational reasoning for making ﬁne distinc-
tions regarding object interactions and properties.

In our method, local low-level reasoning is provided through object head
and the ORN module such as described above in Section 3.1. We complement

10

Baradel et al.

this representation by high-level context information described by Vt which are
feature outputs from the activity head (orange block in Figure 2).

We use spatial global average pooling over Vt to output T D-dimensional
feature vectors denoted by vt, where vt corresponds to the context information
of the video at timestep t.

We model the dynamics of the context information through time by employ-

ing a RNN fγ(

) given by:
·

where s is the hidden state of fγ(
context though time.

) and gives cues about the evolution of the
·

st = fγ(vt, st

1)

−

(7)

3.4 Recognition

Given an input video sequence X1:T , the two diﬀerent streams corresponding to
the activity head and the object head result in the two representations h and r,
respectively where h = (cid:80)
trt. Each representation is the hidden
state of the respective GRU, which were described in the preceding subsections.
Recall that h provides the global motion context while r provides the object
reasoning state output by the ORN module. We perform independent linear
classiﬁcation for each representation:

tht and r = (cid:80)

y1 = W h
y2 = Z r

(8)

(9)

where y1, y2 correspond to the logits from the activity head and the object head,
respectively, and W and Z are trainable weights (including biases). The ﬁnal
prediction is done by averaging logits y1 and y2 followed by softmax activation.

4 Network Architectures and feature dimensions

×

×

W

The input RGB images Xt are of size R3
H where W and H correspond to
the width and height and are of size 224 each. The object and activity heads
(orange and green in Figure 2) are a joint convolutional neural network with
Resnet50 architecture pre-trained on ImageNet/ILSVRC [31], with Conv1 and
Conv5 blocks being inﬂated to 2.5D convolutions [44] (3D convolutions with a
separable temporal dimension). This choice has been optimized on the validation
set, as explained in Section 6 and shown in Table 5.

The last conv5 layers have been split into two diﬀerent heads (activity head
and object head). The intermediate feature representations Ut and Vt are of di-
mensions 2048
7, respectively. We provide a higher
×
spatial resolution for the feature maps Ut of the object head to get more precise
local descriptors. This can be done by changing the stride of the initial conv5
layers from 2 to 1. Temporal convolutions have been conﬁgured to keep the same
time temporal dimension through the network.

14 and 2048

14

×

×

×

×

×

T

T

7

Object Level Visual Reasoning in Videos

11

Global spatial pooling of activity features results in a 2048 dimensional fea-
ture vector fed into a GRU with 512 dimensional hidden state st. ROI-Pooling
of object features results in 2048 dimensional feature vectors uk
t . The encoder of
the binary mask is a MLP with one hidden layer of size 100 and outputs a mask
embedding bk
t of dimension 100. The number of object classes is 80, which leads
in total to a 2229 dimensional object feature vector ok
t .

The non-linearity hθ(

) is implemented as an MLP with 2 hidden layers each
·
with 512 units and produces an 512 dimensional output space. fφ(
) is imple-
·
mented as a GRU with a 256 dimension hidden state rt. We use ReLU as the
activation function after each layer for each network.

5 Training

We train the model with a loss split into two terms:

Ltot =

L

(cid:16) ˆy1 + ˆy2
2

(cid:17)

, y

+

(cid:88)

(cid:88)

t

k

(ˆck

t , ck

t ).

L

(10)

L

is the cross-entropy loss. The ﬁrst term corresponds to supervised ac-
where
tivity class losses comparing two diﬀerent activity class predictions to the class
ground truth: ˆy1 is the prediction of the activity head, whereas ˆy2 is the pre-
diction of the object head, as given by Equations (8) and (9), respectively.

The second term is a loss which pushes the features U of the object towards
representations of the semantic object classes. The goal is to obtain features
related to, both, motion (through the layers shared with the activity head), as
well as as object classes. As ground-truth object classes are not available, we
deﬁne the loss as the cross-entropy between the class label ck
t predicted by the
mask predictor and a dedicated linear class prediction ˆck
t based on features uk
t ,
which, as we recall, are RoI-pooled from U:

t = R uk
ck
t

(11)

where R trainable parameters (biases integrated) learned end-to-end together
with the other parameters of the model.

We found that ﬁrst training the object head only and then the full network
was performing better. A ResNet50 network pretrained on ImageNet is modiﬁed
by inﬂating some of its ﬁlters to 2.5 convolutions (3D convolutions with the time
dimension separated), as described in Section 4; then by ﬁne-tuning.

We train the model using the Adam optimizer [22] with an initial learning
4 on 30 epochs and use early-stopping criterion on the validation set
50 minutes per epoch on 4

rate of 10−
for hyper-parameter optimization. Training takes
Titan XP GPUs with clips of 8 frames.

∼

12

Baradel et al.

6 Experimental results

We evaluated the method on three standard datasets, which represent diﬃcult
ﬁne-grained activity recognition tasks: the Something-Something dataset, the
VLOG dataset and the recently released EPIC Kitchens dataset.

Something-Something (SS) is a recent video classiﬁcation dataset with
108,000 example videos and 157 classes [13]. It shows humans performing diﬀer-
ent actions with diﬀerent objects, actions and objects being combined in diﬀerent
ways. Solving SS requires common sense reasoning and the state-of-the-art meth-
ods in activity recognition tend to fail, which makes this dataset challenging.

VLOG is a multi-label binary classiﬁcation of human-object interactions
recently released with 114,000 videos and 30 classes [12]. Classes correspond
to objects, and labels of a class are 1 if a person has touched a certain object
during the video, otherwise they are 0. It has recently been shown, that state-
of-the-art video based methods [6] are outperformed on VLOG by image based
methods like ResNet-50 [16], although these video methods outperform image
based ResNet-50 on large-scale video datasets like the Kinetics dataset [6]. This
suggests a gap between traditional datasets like Kinetics and the ﬁne-grained
dataset VLOG, making it particularly diﬃcult.

EPIC Kitchens (EPIC) is an egocentric video dataset recently released
containing 55 hours recording of daily activities [7]. This is the largest in ﬁrst-
person vision and the activities performed are non-scripted, which makes the
dataset very challenging and close to real world data. The dataset is densely
annotated and several tasks exist such as object detection, action recognition and
action prediction. We focus on action recognition with 39’594 action segments
in total and 125 actions classes (i.e verbs). Since the test set is not available yet
we conducted our experiments on the training set (28’561 videos). We use the
videos recorded by person 01 to person 25 for training (22’675 videos) and deﬁne
the validation set as the remaining videos (5’886 videos).

For all datasets we rescale the input video resolution to 256

256. While
training, we crop space-time blocks of 224
224 spatial resolution and L frames,
with L=8 for the SS dataset and L=4 for VLOG and EPIC. We do not perform
any other data augmentation. While training we extract L frames from the entire
video by splitting the video into L sub-sequences and randomly sampling one
frame per sub-sequence. The output sequence of size L is called a clip. A clip
aims to represent the full video with less frames. For testing we aggregate results
of 10 clips. We use lintel [10] for decoding video on the ﬂy.

×

×

The ablation study is done by using the train set as training data and we
report the result on the validation set. We compare against other state-of-the-
art approaches on the test set. For the ablation studies, we slightly decreased
the computational complexity of the model: the base network (including activity
and object heads) is a ResNet-18 instead of ResNet-50, a single clip of 4 frames
is extracted from a video at test time.

Comparison with other approaches. Table 1 shows the performance of
the proposed approach on the VLOG dataset. We outperform the state of the art

Object Level Visual Reasoning in Videos

13

Table 1. Results on Hand/Semantic Object Interaction Classiﬁcation (Average pre-
cision in % on the test set) on VLOG dataset. R50 and I3D implemented by [12].

s
r
e
p
a
p
/
k
o
o
b

e
b
u
t
/
e
l
t
t
o
b

g
n
i
d
d
e
b

P
A
m

g
a
b

d
e
b

l

w
o
b

x
o
b

h
s
u
r
b

e
n
o
h
p
-
l
l
e
c

t
e
n
i
b
a
c

g
n
i
h
t
o
l
c

p
u
c

r
o
o
d

s
r
e
w
a
r
d

d
o
o
f

k
r
o
f

e
f
i
n
k

p
o
t
p
a
l

e
v
a
w
o
r
c
i
m

n
e
v
o

l
i
c
n
e
p
/
n
e
p

r
o
t
a
r
e
g
i
r
f
e
r

w
o
l
l
i
p

e
t
a
l
p

n
o
o
p
s

k
n
i
s

l
a
m
i
n
a

d
e
ﬀ
u
t
s

h
s
u
r
b
h
t
o
o
t

e
l
b
a
t

l
e
w
o
t

R50 [16] 40.5 29.7 68.9 65.8 64.5 58.2 33.1 22.1 19.0 23.9 54.0 45.5 28.6 49.2 28.7 49.6 19.4 37.5 62.9 48.8 23.0 36.9 39.2 12.5 55.9 58.8 31.1 57.4 26.8 39.6 22.9
I3D [6]
39.7 24.9 71.7 71.4 62.5 57.1 27.1 19.2 33.9 20.7 50.6 45.8 24.7 54.7 19.1 50.8 19.3 41.9 54.0 27.5 21.4 37.4 42.9 12.6 42.5 60.4 33.9 46.0 23.5 59.6 34.7
Ours
44.7 30.2 72.3 70.7 64.9 59.8 38.2 24.6 26.3 22.4 64.5 47.2 35.4 57.9 25.2 48.5 24.5 40.2 72.0 54.1 26.5 39.9 48.6 15.2 53.5 60.7 36.8 52.8 27.9 64.0 37.6

≈

4.2 points (44.7% accuracy against
on this challenging dataset by a margin of
40.5% by [16]). As mentioned above, traditional video approaches tend to fail
on this challenging ﬁne-grained dataset, providing inferior results. Table 3 shows
performance on SS where we outperform the state of the art given by very recent
methods (+2.3 points). On EPIC we re-implement standard baselines and report
results on the validation set (Table 4) since the test set is not available. Our full
method reports an accuracy of 40.89 and outperforms baselines by a large margin
(
+7.9 points respectively for against CNN-2D and I3D based on a
+6.4 and
≈
ResNet-18).

≈

Eﬀect of object-level reasoning. Table 2 shows the importance of rea-
soning on the performance of the method. The baseline corresponds to the per-
formance obtained by the activity head trained alone (inﬂated ResNet, in the
ResNet-18 version for this table). No object level reasoning is present in this
baseline. The proposed approach (third line) including an object head and the
ORN module gains 0.8, 2.5 and 2.4 points compared to our baseline respectively
on SS, on EPIC and on VLOG. This indicates that the reasoning module is able
to extract complementary features compared to the activity head.

Using semantically deﬁned objects proved to be important and led to a gain of
2 points on EPIC and 2.3 points on VLOG for the full model (6/12.7 points using
the object head only) compared to an extension of Santoro et al. [32] operating
on pixel level. This indicates importance of object level reasoning. The gain on
SS is smaller (0.7 point with the full model and 7.8 points with the object head
only) and can be explained by the diﬀerence in spatial resolution of the videos.
Object detections and predictions of the binary masks are done using the initial
1183 and for EPIC
video resolution. The mean video resolution for VLOG is 660
157 for SS. Mask-RCNN has been trained on images of
is 640
×
resolution 800
800 and thus performs best on higher resolutions. The quality of
the object detector is important for leveraging object level understanding then
for the rest of the ablation study we focus on EPIC and VLOG datasets.

480 against 100

×

×

×

The function fφ in Equation (5) is an important design choice in our model.
In our proposed model, fφ is recurrent over time to ensure that the ORN module
captures long range reasoning over time, as shown in Equation (5). Removing
the recurrence in this equation leads to an MLP instead of a (gated) RNN, as
evaluated in row 4 of Table 2. Performance decreases by 1.1 point on VLOG
and 1.4 points on EPIC. The larger gap for EPIC compared to VLOG and
can arguably be explained by the fact that in SS actions cover the whole video,

14

Baradel et al.

Table 2. Ablation study with ResNet-18 backbone. Results in %: Top-1 accuracy for
EPIC and SS datasets, and mAP for VLOG dataset.

Method

Object type

EPIC

VLOG
obj. head 2 heads obj. head 2 heads obj. head 2 heads

SS

Baseline
ORN
ORN
ORN-mlp

-
pixel
COCO
COCO

ORN
ORN
ORN

COCO-visual
COCO-shape
COCO-class

ORN
ORN clique-1
ORN clique-3

COCO-intra
COCO
COCO

-
23.71
29.94
28.15

28.45
21.92
21.96

29.25
28.25
22.61

38.33
38.83
40.89
39.41

38.92
37.16
37.75

38.10
40.18
37.67

-
14.40
27.14
25.40

22.92
7.18
13.40

26.78
26.48
27.05

35.03
35.18
37.49
36.35

35.49
35.39
35.94

36.28
36.71
36.04

-
2.51
10.26
-

31.31
31.43
32.12
-

-
-
-

-
-
-

-
-
-

-
-
-

Table 3. Experimental results on the
Something-Something dataset (classi-
ﬁcation accuracy in % on the test set).

Table 4. Experimental results on the
EPIC Kitchens dataset (accuracy in %
on the validation set – methods with ∗
have been re-implemented).

Methods

C3D + Avg [13]
I3D [13]
MultiScale TRN [5]

Ours

Top1

21.50
27.63
33.60

35.97

Methods

Top1

R18 [16]∗
I3D-18 [6]∗

32.05
34.20

Ours

40.89

while solving VLOG requires detecting the right moment when the human-object
interaction occurs and thus long range reasoning plays a less important role.

Visual features extracted from object regions are the most discriminative,
however object shapes and labels also provide complementary information. Fi-
nally, the last part of Table 2 evaluates the eﬀect of the cliques size for model-
ing the interactions between objects and show that pairwise cliques outperform
cliques of size 1 and 3. We would like to recall, that even with unary cliques
only, interactions between objects are still modeled. However, the model needs
to ﬁnd subspaces in the hidden representations associated to each interaction.

CNN architecture and kernel inﬂations. The convolutional architecture
of the model was optimized over the validation set of the SS dataset, as shown
in Table 5. The architecture itself (in terms of numbers of layers, ﬁlters etc.)
is determined by pre-training on image classiﬁcation. We optimized the choice
of ﬁlter inﬂations from 2D to 2.5D or 3D for several convolutional blocks. This
has been optimized for the single head model and using a ResNet-18 variant
to speed up computation. Adding temporal convolutions increases performance
up to 100% w.r.t. to pure 2D baselines. This indicates, without surprise, that
motion is a strong cue. Inﬂating kernels to 2.5D on the input side and on the

Object Level Visual Reasoning in Videos

15

Table 5. Eﬀect of the CNN architecture (choice of kernel inﬂations) on a single head
ResNet-18 network. Accuracy in % on the validation set of Something-Something is
shown. 2.5D kernels are separable kernels: 2D followed by a 1D temporal.

Conv1

Conv2

Conv3

Conv4

Conv5

Aggreg

(cid:88)
-
(cid:88)
(cid:88)

2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D 2D 3D 2.5D GAP RNN
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88) -
-
-

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
(cid:88) -
-
- (cid:88) -
(cid:88) -
-
(cid:88) -
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
- (cid:88) (cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
- (cid:88) -
- (cid:88) -
- (cid:88) -
- (cid:88)
-
- (cid:88)
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
-
- (cid:88) -
- (cid:88) -
(cid:88) -
-
- (cid:88)
-
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -

(cid:88) -
-
(cid:88) -
-
- (cid:88) -
- (cid:88)
-
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
(cid:88) -
-
- (cid:88) -
-

- (cid:88) (cid:88) -
- (cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
-
-
-
-
-

-
-
-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-
-
-
-

-

SS

15.73
-
(cid:88) 15.88
31.42
-
27.58
-

31.28
32.06
32.25
31.31
32.79
33.77

28.71
31.42
20.05
22.52

output side provided best performances, suggesting that temporal integration
is required at a very low level (motion estimation) as well as on a very high
level, close to reasoning. Our study also corroborates recent research in activity
recognition, indicating that 2.5D kernels provide a good trade-oﬀ between high-
capacity and learnable numbers of parameters. Finally temporal integration via
RNN outperforms global average pooling over space and time. The choice of
a (gated) RNN for temporal integration of the activity head features proved
important (see Table 5) compared to global average pooling (GAP) over space
and time.

Visualizing the learned object interactions. Figure 4 shows visualiza-
tions of the pairwise object relationships the model learned from data, in partic-
ular from the VLOG dataset. Each graph is computed for a given activity class,
and strong arcs between two nodes in the graph indicate strong relationships
between the object classes, i.e. the model detects a high correlation between
these relationships and the activity. The graphs were obtained by thresholding
the summed activations of each pairwise relationship (j, k) in equation (4). Each
pair (j, k) can be assigned a pair of object classes cj
t through the predic-
tions of the object instance mask predictor. Integrating over all samples of the
dataset for a given class leads to the visualizations in Figure 4. We can see that
the object interactions are highly relevant to the detected activities: the person-
touches-bed activity is correlated to interactions between relevant object classes
person and bed. Similarly, activities human-bowl interaction and human-cup in-
teraction show interactions with the respective objects bowl and cup. Moreover,
other recovered relationships are highly correlated to the scene (for example,
dining-table and bowl for activity human-bowl interaction).

t and ck

16

Baradel et al.

Fig. 4. Example of object pairwise interactions learned by our model on VLOG for
four diﬀerent classes. Objects co-occurrences are at the top and learned pairwise objects
interactions are at the bottom. Line thickness indicates learned importance of a given
relation. Interactions have been normalized by the object co-occurrences.

Fig. 5. Examples of failure cases – a) small sized objects (on the left). Our model
detects a cell phone and a person but fails to detect hand-cell-phone contact; b) con-
fusion between semantically similar objects (on the right). The model falsly predicts
hand-cup contact instead of hand-glass-contact even though the wine glass is detected.

Finally, Figure 5 shows some failure cases, which are either due to errors

done by the object mask prediction (Mask R-CNN) or by the ORN itself.

7 Conclusion

We presented a method for activity recognition in videos which leverages ob-
ject instance detections for visual reasoning on object interactions over time.
The choice of reasoning over semantically well-deﬁned objects is key to our ap-
proach and outperforms state of the art methods which reason on grid-levels,
such as cells of convolutional feature maps. Temporal dependencies and causal
relationships are dealt with by integrating relationships between diﬀerent time
instants. We evaluated the method on three diﬃcult datasets, on which standard
approaches do not perform well, and report state-of-the-art results.

Acknowledgements. This work was funded by grant Deepvision (ANR-15-
CE23-0029, STPGP-479356-15), a joint French/Canadian call by ANR & NSERC.

Object Level Visual Reasoning in Videos

17

References

1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B.,
Vijayanarasimhan, S.: Youtube-8m: A large-scale video classiﬁcation benchmark.
arXiv preprint arxiv:1609.08675 (2016)

2. Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., Baskurt, A.: Sequential deep

learning for human action recognition. In: HBU (2011)

3. Baradel, F., Wolf, C., Mille, J., Taylor, G.: Glimpse clouds: Human activity recog-

nition from unstructured feature points. In: CVPR (2018)

4. Battaglia, P.W., Pascanu, R., Lai, M., Rezende, D.J., Kavukcuoglu, K.: Interaction

networks for learning about objects, relations and physics. In: NIPS (2016)

5. Bolei, Z., Zhang, A.A., Torralba, A.: Temporal relational reasoning in videos. In:

ECCV (2018)

6. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the

kinetics dataset. In: CVPR (2017)

7. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Scaling egocentric
vision: The epic-kitchens dataset. In: ECCV (2018)

8. Deng, Z., Vahdat, A., Hu, H., Mori, G.: Structure inference machines: Recurrent
neural networks for analyzing relations in group activity recognition. In: CVPR
(2016)

9. Donahue, J., Hendricks, L.A., Guadarrama, S., Rohrbach, M., Venugopalan, S.,
Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: CVPR (2015)

10. Duke, B.: Lintel: Python video decoding. https://github.com/dukebw/lintel

(2018)

11. Fleuret, F., Li, T., Dubout, C., Wampler, E.K., Yantis, S., Geman, D.: Comparing
machines and humans on a visual categorization test. Proceedings of the National
Academy of Sciences of the United States of America 108 43, 17621–5 (2011)
12. Fouhey, D.F., Kuo, W., Efros, A.A., Malik, J.: From lifestyle vlogs to everyday

interactions. In: CVPR (2018)

13. Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim,
H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau,
C., Bax, I., Memisevic, R.: The ”something something” video database for learning
and evaluating visual common sense. In: ICCV (2017)

14. Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan,
S., Toderici, G., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: Ava: A
video dataset of spatio-temporally localized atomic visual actions. arXiv preprint
arXiv:1705.08421 (2017)

15. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: ICCV (2017)
16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

17. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation

18. Hudson, D., Manning, C.: Compositional attention networks for machine reasoning.

In: CVPR (2016)

9(8), 1735–1780 (1997)

In: ICLR (2018)

19. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: Deep Learning

on Spatio-Temporal Graphs. In: CVPR (2016)

20. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-
scale video classiﬁcation with convolutional neural networks. In: CVPR (2014)

18

Baradel et al.

21. Kim, J., Ricci, M., Serre, T.: Not-so-CLEVR: Visual relations strain feedforward

neural networks. arXiv preprint arXiv:1802.03390 (2018)

22. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICML (2015)
23. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Ka-
landitis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome:
Connecting language and vision using crowdsourced dense image annotations. In-
ternational Journal of Computer Vision (IJCV) 123, 32–73 (2017)

24. Luc, P., , Neverova, N., Couprie, C., Verbeek, J., LeCun, Y.: Predicting deeper

into the future of semantic segmentation. In: ICCV (2017)

25. Luvizon, D., Picard, D., Tabia, H.: 2d/3d pose estimation and action recognition

using multitask deep learning. In: CVPR (2018)

26. Monfort, M., Zhou, B., Bargal, S.A., Andonian, A., Yan, T., Ramakrishnan,
K., Brown, L., Fan, Q., Gutfruend, D., Vondrick, C., Oliva, A.: Moments
in time dataset: one million videos for event understanding. arXiv preprint
arXiv:1801.03150 (2018)

27. Perez, E., Vries, H.D., Strub, F., Dumoulin, V., Courville, A.: Learning visual rea-
soning without strong priors. In: ICML Machine Learning in Speech and Language
Processing Workshop (2017)

28. Pickup, L.C., Pan, Z., Wei, D., Shih, Y., Zhang, C., Zisserman, A., Scholkopf, B.,

Freeman, W.T.: Seeing the arrow of time. In: CVPR (2014)

29. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for

3d classiﬁcation and segmentation. In: CVPR (2017)

30. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object

detection with region proposal networks. In: NIPS (2015)

31. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A., Fei-Fei, L.: Imagenet large scale
visual recognition challenge. IJCV 115(3), 211–252 (2015)

32. Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia,
P., Lillicrap, T.: A simple neural network module for relational reasoning. In: NIPS
(2017)

33. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: NTU RGB+D: A Large Scale Dataset

for 3D Human Activity Analysis. In: CVPR (2016)

34. Sharma, S., Kiros, R., Salakhutdinov, R.: Action recognition using visual attention.

35. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-

In: ICLR Workshop (2016)

nition in videos. In: NIPS (2014)

36. Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An End-to-End Spatio-Temporal
Attention Model for Human Action Recognition from Skeleton Data. In: AAAI
(2016)

37. Stabinger, S., Rodr´ıguez-S´anchez, A., Piater, J.: 25 years of CNNs: Can we compare

to human abstraction capabilities? In: ICANN (2016)

38. van Steenkiste, S., Chang, M., Greﬀ, K., Schmidhuber, J.: Relational neural ex-
pectation maximization: Unsupervised discovery of objects and their interactions.
In: ICLR (2018)

39. Sun, L., Jia, K., Chen, K., Yeung, D., Shi, B.E., Savarese, S.: Lattice long short-

term memory for human action recognition. In: ICCV (2017)

40. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-

poral features with 3d convolutional networks. In: ICCV (2015)

41. Velikovi, P., Cucurull, G., Casanova, A., Romero, A., Li, P., Bengio, Y.: Graph

attention networks. In: ICLR (2018)

Object Level Visual Reasoning in Videos

19

42. Wang, H., Kl¨aser, A., Schmid, C., Liu, C.L.: Action Recognition by Dense Trajec-

tories. In: CVPR (2011)

43. Watters, N., Zoran, D., Weber, T., Battaglia, P., Pascanu, R., Tacchetti, A.: Visual
interaction networks: Learning a physics simulator from video. In: NIPS (2017)
44. Xie, S., Sun, C., Huang, J., Tu, Z., Murphy, K.: Rethinking spatiotemporal feature

learning for video understanding. arXiv prepring arxiv:1712.04851 (2017)

45. Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori, G., Fei-Fei, L.: Every mo-
ment counts: Dense detailed labeling of actions in complex videos. arXiv preprint
arXiv:1507.05738 (2015)

46. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.R., Smola,
A.J.: Deep sets. In: NIPS. pp. 3391–3401 (2017), http://papers.nips.cc/paper/
6931-deep-sets.pdf

