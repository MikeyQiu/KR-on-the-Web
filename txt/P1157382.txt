A Generative Word Embedding Model and its Low Rank Positive
Semideﬁnite Solution

Shaohua Li1, Jun Zhu2, Chunyan Miao1
1Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY),
Nanyang Technological University, Singapore
2Tsinghua University, P.R. China
lish0018@ntu.edu.sg, dcszj@tsinghua.edu.cn, ascymiao@ntu.edu.sg

Abstract

Most existing word embedding methods
can be categorized into Neural Embedding
Models and Matrix Factorization (MF)-
based methods. However some mod-
els are opaque to probabilistic interpre-
tation, and MF-based methods, typically
solved using Singular Value Decomposi-
tion (SVD), may incur loss of corpus in-
formation.
In addition, it is desirable to
incorporate global latent factors, such as
topics, sentiments or writing styles, into
the word embedding model. Since gen-
erative models provide a principled way
to incorporate latent factors, we propose a
generative word embedding model, which
is easy to interpret, and can serve as a
basis of more sophisticated latent factor
models. The model inference reduces to
a low rank weighted positive semideﬁnite
approximation problem.
Its optimization
is approached by eigendecomposition on a
submatrix, followed by online blockwise
regression, which is scalable and avoids
the information loss in SVD. In experi-
ments on 7 common benchmark datasets,
our vectors are competitive to word2vec,
and better than other MF-based methods.

1

Introduction

The task of word embedding is to model the distri-
bution of a word and its context words using their
corresponding vectors in a Euclidean space. Then
by doing regression on the relevant statistics de-
rived from a corpus, a set of vectors are recovered
which best ﬁt these statistics. These vectors, com-
monly referred to as the embeddings, capture se-
mantic/syntactic regularities between the words.

The core of a word embedding method is the
link function that connects the input — the embed-
dings, with the output — certain corpus statistics.

Based on the link function, the objective function
is developed. The reasonableness of the link func-
tion impacts the quality of the obtained embed-
dings, and different link functions are amenable
to different optimization algorithms, with different
scalability. Based on the forms of the link func-
tion and the optimization techniques, most meth-
ods can be divided into two classes: the traditional
neural embedding models, and more recent low
rank matrix factorization methods.

The neural embedding models use the softmax
link function to model the conditional distribution
of a word given its context (or vice versa) as a
function of the embeddings. The normalizer in the
softmax function brings intricacy to the optimiza-
tion, which is usually tackled by gradient-based
methods. The pioneering work was (Bengio et
al., 2006). Later Mnih and Hinton (2007) propose
three different link functions. However there are
interaction matrices between the embeddings in all
these models, which complicate and slow down
the training, hindering them from being trained
on huge corpora. Mikolov et al. (2013a) and
Mikolov et al. (2013b) greatly simplify the condi-
tional distribution, where the two embeddings in-
teract directly. They implemented the well-known
“word2vec”, which can be trained efﬁciently on
huge corpora. The obtained embeddings show ex-
cellent performance on various tasks.

Low-Rank Matrix Factorization (MF in short)
methods include various link functions and opti-
mization methods. The link functions are usu-
ally not softmax functions. MF methods aim to
reconstruct certain corpus statistics matrix by the
product of two low rank factor matrices. The ob-
jective is usually to minimize the reconstruction
error, optionally with other constraints.
In this
line of research, Levy and Goldberg (2014) ﬁnd
that “word2vec” is essentially doing stochastic
weighted factorization of the word-context point-
wise mutual information (PMI) matrix. They then

5
1
0
2
 
g
u
A
 
6
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
2
8
3
0
.
8
0
5
1
:
v
i
X
r
a

factorize this matrix directly as a new method.
Pennington et al. (2014) propose a bilinear regres-
sion function of the conditional distribution, from
which a weighted MF problem on the bigram log-
frequency matrix is formulated. Gradient Descent
is used to ﬁnd the embeddings. Recently, based
on the intuition that words can be organized in se-
mantic hierarchies, Yogatama et al. (2015) add hi-
erarchical sparse regularizers to the matrix recon-
struction error. With similar techniques, Faruqui
et al. (2015) reconstruct a set of pretrained embed-
dings using sparse vectors of greater dimensional-
ity. Dhillon et al. (2015) apply Canonical Corre-
lation Analysis (CCA) to the word matrix and the
context matrix, and use the canonical correlation
vectors between the two matrices as word embed-
dings. Stratos et al. (2014) and Stratos et al. (2015)
assume a Brown language model, and prove that
doing CCA on the bigram occurrences is equiva-
lent to ﬁnding a transformed solution of the lan-
guage model. Arora et al. (2015) assume there is a
hidden discourse vector on a random walk, which
determines the distribution of the current word.
The slowly evolving discourse vector puts a con-
straint on the embeddings in a small text window.
The maximum likelihood estimate of the embed-
dings within this text window approximately re-
duces to a squared norm objective.

There are two limitations in current word em-
bedding methods. The ﬁrst limitation is, all MF-
based methods map words and their context words
to two different sets of embeddings, and then em-
ploy Singular Value Decomposition (SVD) to ob-
tain a low rank approximation of the word-context
matrix M . As SVD factorizes M (cid:62)M , some in-
formation in M is lost, and the learned embed-
dings may not capture the most signiﬁcant regu-
larities in M . Appendix A gives a toy example on
which SVD does not work properly.

The second limitation is, a generative model for
documents parametered by embeddings is absent
in recent development. Although (Stratos et al.,
2014; Stratos et al., 2015; Arora et al., 2015) are
based on generative processes, the generative pro-
cesses are only for deriving the local relationship
between embeddings within a small text window,
leaving the likelihood of a document undeﬁned.
In addition, the learning objectives of some mod-
els, e.g. (Mikolov et al., 2013b, Eq.1), even have
no clear probabilistic interpretation. A genera-
tive word embedding model for documents is not

only easier to interpret and analyze, but more im-
portantly, provides a basis upon which document-
level global latent factors, such as document topics
(Wallach, 2006), sentiments (Lin and He, 2009),
writing styles (Zhao et al., 2011b), can be incor-
porated in a principled manner, to better model the
text distribution and extract relevant information.
Based on the above considerations, we pro-
pose to unify the embeddings of words and con-
text words. Our link function factorizes into three
parts: the interaction of two embeddings capturing
linear correlations of two words, a residual captur-
ing nonlinear or noisy correlations, and the uni-
gram priors. To reduce overﬁtting, we put Gaus-
sian priors on embeddings and residuals, and ap-
ply Jelinek-Mercer Smoothing to bigrams. Fur-
thermore, to model the probability of a sequence
of words, we assume that the contributions of
more than one context word approximately add up.
Thereby a generative model of documents is con-
structed, parameterized by embeddings and resid-
uals. The learning objective is to maximize the
corpus likelihood, which reduces to a weighted
low-rank positive semideﬁnite (PSD) approxima-
tion problem of the PMI matrix. A Block Co-
ordinate Descent algorithm is adopted to ﬁnd an
approximate solution. This algorithm is based
on Eigendecomposition, which avoids information
loss in SVD, but brings challenges to scalability.
We then exploit the sparsity of the weight matrix
and implement an efﬁcient online blockwise re-
gression algorithm. On seven benchmark datasets
covering similarity and analogy tasks, our method
achieves competitive and stable performance.

The source code of this method is provided at

https://github.com/askerlee/topicvec.

2 Notations and Deﬁnitions

Throughout the paper, we always use a uppercase
bold letter as S, V to denote a matrix or set, a low-
ercase bold letter as vwi to denote a vector, a nor-
mal uppercase letter as N, W to denote a scalar
constant, and a normal lowercase letter as si, wi to
denote a scalar variable.

Suppose a vocabulary S = {s1, · · · , sW } con-
sists of all the words, where W is the vocab-
ulary size. We further suppose s1, · · · , sW are
sorted in decending order of the frequency, i.e.
s1 is most frequent, and sW is least frequent.
A document di
is a sequence of words di =
(wi1, · · · , wiLi), wij ∈ S. A corpus is a collec-

Name
S

V

D
vsi
asisj
˜P (si,sj)
u

A

B

G

H

Description
Vocabulary {s1, · · · , sW }
Embedding matrix (vs1 , · · · , vsW )
Corpus {d1, · · · , dM }
Embedding of word si
Bigram residual for si, sj
Empirical probability of si, sj in the corpus
Unigram probability vector (P (s1),· · ·, P (sW ))
Residual matrix (asisj )
(cid:16)

(cid:17)

Conditional probability matrix

PMI matrix

PMI(si, sj)

(cid:16)

P (sj|si)
(cid:17)
(cid:16) ˜P (si, sj)
(cid:17)

Bigram empirical probability matrix

Table 1: Notation Table

tion of M documents D = {d1, · · · , dM }. In the
vocabulary, each word si is mapped to a vector vsi
in N -dimensional Euclidean space.

In a document, a sequence of words is referred
to as a text window, denoted by wi, · · · , wi+l, or
wi:wi+l in shorthand. A text window of chosen
size c before a word wi deﬁnes the context of wi
as wi−c, · · · , wi−1. Here wi is referred to as the
focus word. Each context word wi−j and the focus
word wi comprise a bigram wi−j, wi.

The Pointwise Mutual Information between two

words si, sj is deﬁned as

PMI(si, sj) = log

P (si, sj)
P (si)P (sj)

.

3 Link Function of Text

In this section, we formulate the probability of a
sequence of words as a function of their embed-
dings. We start from the link function of bigrams,
which is the building blocks of a long sequence.
Then this link function is extended to a text win-
dow with c context words, as a ﬁrst-order approx-
imation of the actual probability.

3.1 Link Function of Bigrams

We generalize the link function of “word2vec” and
“GloVe” to the following:

(cid:110)

(cid:111)

P (si, sj) = exp

P (si)P (sj) (1)

v(cid:62)
sj vsi + asisj
The rationale for (1) originates from the idea of
the Product of Experts in (Hinton, 2002). Sup-
pose different types of semantic/syntactic regu-
larities between si and sj are encoded in differ-
sj vsi} =
ent dimensions of vsi, vsj . As exp{v(cid:62)
(cid:81)
l exp{vsi,l · vsj ,l}, this means the effects of dif-
ferent regularities on the probability are combined

by multiplying together. If si and sj are indepen-
dent, their joint probability should be P (si)P (sj).
In the presence of correlations, the actual joint
probability P (si, sj) would be a scaling of it. The
scale factor reﬂects how much si and sj are pos-
itively or negatively correlated. Within the scale
factor, v(cid:62)
sj vsi captures linear interactions between
si and sj, the residual asisj captures nonlinear or
sj vsi is
noisy interactions. In applications, only v(cid:62)
of interest. Hence the bigger magnitude v(cid:62)
sj vsi is
of relative to asisj , the better.

Note that we do not assume asisj = asj si.
This provides the ﬂexibility P (si, sj) (cid:54)= P (sj, si),
agreeing with the asymmetry of bigrams in natu-
ral languages. At the same time, v(cid:62)
sj vsi imposes a
symmetric part between P (si, sj) and P (sj, si).

(1) is equivalent to

(cid:110)

P (sj|si)=exp

v(cid:62)
sj vsi + asisj + log P (sj)

, (2)

(cid:111)

log

P (sj|si)
P (sj)

= v(cid:62)

sj vsi + asisj .

(3) of all bigrams is represented in matrix form:

(3)

(4)

V (cid:62)V + A = G,

where G is the PMI matrix.

3.1.1 Gaussian Priors on Embeddings
When (1) is employed on the regression of empir-
ical bigram probabilities, a practical issue arises:
more and more bigrams have zero frequency as
the constituting words become less frequent. A
zero-frequency bigram does not necessarily imply
negative correlation between the two constituting
words; it could simply result from missing data.
But in this case, even after smoothing, (1) will
force v(cid:62)
sj vsi + asisj to be a big negative number,
making vsi overly long. The increased magnitude
of embeddings is a sign of overﬁtting.

To reduce overﬁtting of embeddings of infre-
quent words, we assign a Spherical Gaussian prior
N (0, 1
2µi

I) to vsi:
P (vsi) ∼ exp{−µi(cid:107)vsi(cid:107)2},
where the hyperparameter µi increases as the fre-
quency of si decreases.

3.1.2 Gaussian Priors on Residuals
We wish v(cid:62)
sj vsi in (1) captures as much corre-
lations between si and sj as possible. Thus the
smaller asisj is, the better. In addition, the more
frequent si, sj is in the corpus,
the less noise
there is in their empirical distribution, and thus the
residual asisj should be more heavily penalized.

To this end, we penalize the residual asisj
by f (˜P (si, sj))a2
sisj , where f (·) is a nonnega-
tive monotonic transformation, referred to as the
weighting function. Let hij denote ˜P (si, sj), then
the total penalty of all residuals are the square of
the weighted Frobenius norm of A:
sisj = (cid:107)A(cid:107)2

f (hij)a2

f (H).

(cid:88)

(5)

si,sj ∈S

By referring to “GloVe”, we use the following

√

weighting function, and ﬁnd it performs well:
(cid:112)hij < Ccut, i (cid:54)= j
(cid:112)hij ≥ Ccut, i (cid:54)= j
i = j


hij

Ccut
1

0

f (hij) =

,

where Ccut is chosen to cut the most frequent
0.02% of the bigrams off at 1. When si = sj, two
identical words usually have much smaller proba-
bility to collocate. Hence ˜P (si, si) does not reﬂect
the true correlation of a word to itself, and should
not put constraints to the embeddings. We elimi-
nate their effects by setting f (hii) to 0.

(cid:16)

If the domain of A is the whole space RW ×W ,
then this penalty is equivalent to a Gaussian prior
N
on each asisj . The variances of the
Gaussians are determined by the bigram empirical
probability matrix H.

1
2f (hij )

0,

(cid:17)

Jelinek-Mercer Smoothing of Bigrams

3.1.3
As another measure to reduce the impact of miss-
ing data, we apply the commonly used Jelinek-
Mercer Smoothing (Zhai and Lafferty, 2004)
to smooth the empirical conditional probability
˜P (sj|si) by the unigram probability ˜P (sj) as:
˜Psmoothed(sj|si) = (1−κ) ˜P (sj|si)+κP (sj). (6)
the smoothed bigram empirical
Accordingly,

joint probability is deﬁned as
˜P (si, sj) = (1−κ) ˜P (si, sj)+κP (si)P (sj). (7)
In practice, we ﬁnd κ = 0.02 yields good re-
sults. When κ ≥ 0.04, the obtained embeddings
begin to degrade with κ, indicating that smoothing
distorts the true bigram distributions.

3.2 Link Function of a Text Window

In the previous subsection, a regression link func-
tion of bigram probabilities is established.
In
this section, we adopt a ﬁrst-order approximation
based on Information Theory, and extend the link
function to a longer sequence w0, · · · , wc−1, wc.

Decomposing a distribution conditioned on n
random variables as the conditional distributions

on its subsets roots deeply in Information The-
ory. This is an intricate problem because there
could be both (pointwise) redundant information
and (pointwise) synergistic information among the
conditioning variables (Williams and Beer, 2010).
They are both functions of the PMI. Based on an
analysis of the complementing roles of these two
types of pointwise information, we assume they
are approximately equal and cancel each other
when computing the pointwise interaction infor-
mation. See Appendix B for a detailed discussion.
Following the above assumption, we have
PMI(w2; w0, w1) ≈ PMI(w2; w0)+PMI(w2; w1):

log

P (w0, w1|w2)
P (w0, w1)

≈log

P (w0|w2)
P (w0)

+log

P (w1|w2)
P (w1)

.

Plugging (1) and (3) into the above, we obtain
P (w0, w1, w2)
(cid:26) 2

≈ exp

(v(cid:62)

wivwj + awiwj ) +

log P (wi)

(cid:27)
.

(cid:88)

i,j=0
i(cid:54)=j

We extend the above assumption to that the
pointwise interaction information is still close to
0 within a longer text window. Accordingly the
above equation extends to a context of size c > 2:

P (w0, · · · , wc)
(cid:26) c

(cid:88)

≈ exp

i,j=0
i(cid:54)=j

(v(cid:62)

wivwj + awiwj ) +

log P (wi)

(cid:27)
.

2
(cid:88)

i=0

c
(cid:88)

i=0

From it derives the conditional distribution of

wc, given its context w0, · · · , wc−1:

P (wc | w0 : wc−1)=

(cid:26)

≈P (wc) exp

v(cid:62)
wc

P (w0, · · · , wc)
P (w0, · · · , wc−1)
c−1
(cid:27)
(cid:88)
.

awiwc

vwi +

i=0

c−1
(cid:88)

i=0

(8)

4 Generative Process and Likelihood

We proceed to assume the text is generated from a
Markov chain of order c, i.e., a word only depends
on words within its context of size c. Given the
hyperparameter µ = (µ1, · · ·, µW ), the generative
process of the whole corpus is:

1. For each word si, draw the embedding vsi

from N (0, 1
2µi

I);

(cid:17)

(cid:16)

0,

asisj from N

2. For each bigram si, sj, draw the residual
1
2f (hij )
3. For each document di, for the j-th word,
from S with probability

draw word wij
P (wij | wi,j−c : wi,j−1) deﬁned by (8).

;

5 Learning Algorithm

5.1 Learning Objective

The learning objective is to ﬁnd the embeddings
V that maximize the corpus log-likelihood (9).

Let xij denote the (smoothed) frequency of bi-

gram si, sj in the corpus. Then (9) is sorted as:

log p(D, V , A)

µi

vsi

V

hij

aij

A

d

vw0

vw1

· · ·

vwc

=C0 − log Z(H, µ) − (cid:107)A(cid:107)2

f (H) −

µi(cid:107)vsi(cid:107)2

W
(cid:88)

i=1

Figure 1: The Graphical Model of PSDVec

The above generative process for a document d is
presented as a graphical model in Figure 1.

Based on this generative process, the probabil-
ity of a document di can be derived as follows,
given the embeddings and residuals V , A:

P (di|V , A)
Li(cid:89)

(cid:26)

j−1
(cid:88)

j−1
(cid:88)

=

P (wij) exp

v(cid:62)

wij

vwik +

awikwij

j=1

k=j−c

k=j−c

The complete-data likelihood of the corpus is:

p(D, V , A)

=

N (0,

W
(cid:89)

i=1

I
2µi

)

W,W
(cid:89)

(cid:18)

N

0,

i,j=1

1
2f (hij)

(cid:19) M
(cid:89)

p(di|V, A)

=

1
Z(H, µ)

(cid:110)

exp

−

W,W
(cid:88)

f (hi,j)a2

sisj −

µi(cid:107)vsi(cid:107)2(cid:111)

i,j=1

(cid:26)

j−1
(cid:88)

P (wij) exp

v(cid:62)

wij

vwik +

awikwij

M,Li
(cid:89)
·
i,j=1

+

W,W
(cid:88)

i,j=1

xij(v(cid:62)

sivsj + asisj ).

(10)

As
the
(cid:80)W,W
i,j=1 xij(v(cid:62)

corpus
sivsj +asisj ) will

increases,
the
parameter prior terms. Then we can ignore the
prior terms when maximizing (10).

dominate

size

max
(cid:16)(cid:88)

(cid:88)

xij

xij(v(cid:62)
(cid:17)

· max

=

sivsj +asisj )

(cid:88) ˜Psmoothed(si, sj) log P (si, sj).

As both { ˜Psmoothed(si, sj)} and {P (si, sj)}
the above sum is maximized when

sum to 1,
P (si, sj) = ˜Psmoothed(si, sj).

(cid:27)
.

The maximum likelihood estimator is then:
P (sj|si) = ˜Psmoothed(sj|si),

v(cid:62)
sivsj + asisj = log

˜Psmoothed(sj|si)
P (sj)

.

(11)

B∗ =

Writing (11) in matrix form:
(cid:17)
(cid:16) ˜Psmoothed(sj|si)
G∗ = log B∗ − log u ⊗ (1 · · · 1),

si,sj ∈S

(12)

(cid:27)

,

where “⊗” is the outer product.
Now we ﬁx the values of v(cid:62)

sivsj + asisj at the

k=j−c

k=j−c

above optimal. The corpus likelihood becomes

where Z(H, µ) is the normalizing constant.

logarithm of both sides of

log p(D, V , A) =C1 − (cid:107)A(cid:107)2

f (H) −

µi(cid:107)vsi(cid:107)2,

W
(cid:88)

i=1

Taking the
p(D, A, V ) yields

log p(D, V , A)

=C0 − log Z(H, µ) − (cid:107)A(cid:107)2

f (H)−

µi(cid:107)vsi(cid:107)2

M,Li
(cid:88)

(cid:26)

+

v(cid:62)

wij

j−1
(cid:88)

j−1
(cid:88)

(cid:27)

vwik +

awikwij

,

(9)

i,j=1

k=j−c

k=j−c

where C0 = (cid:80)M,Li

i,j=1 log P (wij) is constant.

subject to V (cid:62)V + A = G∗,

(13)
where C1 = C0 + (cid:80) xij log ˜Psmoothed(si, sj) −
log Z(H, µ) is constant.

5.2 Learning V as Low Rank PSD

Approximation

Once G∗ has been estimated from the corpus using
(12), we seek V that maximizes (13). This is to
ﬁnd the maximum a posteriori (MAP) estimates
of V , A that satisfy V (cid:62)V + A = G∗. Applying
this constraint to (13), we obtain

i=1

W
(cid:88)

i=1

j−1
(cid:88)

W
(cid:88)

i=1

Algorithm 1 BCD algorithm for ﬁnding a unreg-
ularized rank-N weighted PSD approximant.
Input: matrix G∗, weight matrix W = f (H),
iteration number T , rank N
Randomly initialize X (0)
for t = 1, · · · , T do

Gt = W ◦ G∗ + (1 − W ) ◦ X (t−1)
X (t) = PSD Approximate(Gt, N )

end for
λ, Q = Eigen Decomposition(X (T ))
V ∗ = diag(λ
Output: V ∗

1
2 [1:N ]) · Q(cid:62)[1:N ]

arg max
V

log p(D, V , A)

= arg min
V

(cid:107)G∗−V (cid:62)V (cid:107)f (H) +

µi(cid:107)vsi(cid:107)2. (14)

W
(cid:88)

i=1

Let X = V (cid:62)V . Then X is positive semidef-
inite of rank N . Finding V that minimizes (14)
is equivalent to ﬁnding a rank-N weighted posi-
tive semideﬁnite approximant X of G∗, subject to
Tikhonov regularization. This problem does not
admit an analytic solution, and can only be solved
using local optimization methods.

First we consider a simpler case where all the
words in the vocabulary are enough frequent, and
thus Tikhonov regularization is unnecessary.
In
this case, we set ∀µi = 0, and (14) becomes an
unregularized optimization problem. We adopt the
Block Coordinate Descent (BCD) algorithm1 in
(Srebro et al., 2003) to approach this problem. The
original algorithm is to ﬁnd a generic rank-N ma-
trix for a weighted approximation problem, and
we tailor it by constraining the matrix within the
positive semideﬁnite manifold.

We summarize our learning algorithm in Al-
gorithm 1. Here “◦” is the entry-wise prod-
uct. We suppose the eigenvalues λ returned by
Eigen Decomposition(X) are in descending or-
der. Q(cid:62)[1:N ] extracts the 1 to N rows from Q(cid:62).
One key issue is how to initialize X. Srebro et
al. (2003) suggest to set X (0) =G∗, and point out
that X (0) = 0 is far from a local optimum, thus
requires more iterations. However we ﬁnd G∗ is
also far from a local optimum, and this setting con-
verges slowly too. Setting X (0) = G∗/2 usually

1It is referred to as an Expectation-Maximization algo-
rithm by the original authors, but we think this is a misnomer.

yields a satisfactory solution in a few iterations.

The subroutine PSD Approximate() computes
the unweighted nearest rank-N PSD approxima-
tion, measured in F-norm (Higham, 1988).

5.3 Online Blockwise Regression of V

the

does

essential

subroutine
In Algorithm 1,
PSD Approximate()
eigendecomposi-
tion on Gt, which is dense due to the logarithm
transformation. Eigendecomposition on a W × W
dense matrix requires O(W 2) space and O(W 3)
time, difﬁcult to scale up to a large vocabulary. In
addition, the majority of words in the vocabulary
are infrequent, and Tikhonov regularization is
necessary for them.

It is observed that, as words become less fre-
quent, fewer and fewer words appear around them
to form bigrams. Remind that the vocabulary
S = {s1, · · · , sW } are sorted in decending or-
der of the frequency, hence the lower-right blocks
of H and f (H) are very sparse, and cause these
blocks in (14) to contribute much less penalty rela-
tive to other regions. Therefore these blocks could
be ignored when doing regression, without sacri-
ﬁcing too much accuracy. This intuition leads to
the following online blockwise regression.

The basic idea is to select a small set (e.g.
30,000) of the most frequent words as the core
words, and partition the remaining noncore words
into sets of moderate sizes. Bigrams consist-
ing of two core words are referred to as core bi-
grams, which correspond to the top-left blocks of
G and f (H). The embeddings of core words
are learned approximately using Algorithm 1, on
the top-left blocks of G and f (H). Then we ﬁx
the embeddings of core words, and ﬁnd the em-
beddings of each set of noncore words in turn.
After ignoring the lower-right regions of G and
f (H) which correspond to bigrams of two non-
core words, the quadratic terms of noncore em-
beddings are ignored. Consequently, ﬁnding these
embeddings becomes a weighted ridge regression
problem, which can be solved efﬁciently in closed-
form. Finally we combine all embeddings to get
the embeddings of the whole vocabulary. The de-
tails are as follows:

1. Partition S into K consecutive groups
S1, · · · , Sk. Take K = 3 as an example.
The ﬁrst group is core words;

2. Accordingly partition G into K × K blocks,

in this example as





G11 G12 G13
G21 G22 G23
G31 G32 G33



 .

Partition
same way.
G11, f (H)11, A11 correspond to core bi-

f (H),A in

the

grams. Partition V into

(cid:0)

V 1
(cid:124)(cid:123)(cid:122)(cid:125)
S1
1 V 1 + A11 = G11 using Algorithm

V 3
(cid:124) (cid:123)(cid:122) (cid:125)
S3

V 2
(cid:124) (cid:123)(cid:122) (cid:125)
S2

;

(cid:1)

3. Solve V (cid:62)

1, and obtain core embeddings V ∗
1;

4. Set V 1 = V ∗

1, and ﬁnd V ∗
2 that minimizes
the total penalty of the 12-th and 21-th blocks
of residuals (the 22-th block is ignored due to
its high sparsity):

(cid:107)G12 − V (cid:62)

1 V 2(cid:107)2

f (H)12

arg min
V 2
+ (cid:107)G21 − V (cid:62)

2 V 1(cid:107)2

f (H)21

+

(cid:88)

µi(cid:107)vsi(cid:107)2

si∈S2
(cid:88)
+

si∈S2

= arg min
V 2

(cid:107)G12−V (cid:62)

1 V 2(cid:107)2

¯f (H)12

µi(cid:107)vsi(cid:107)2,

where ¯f (H)12 = f (H)12 + f (H)(cid:62)
21;
(cid:17)
G12 ◦ f (H)12 + G(cid:62)
21 ◦ f (H)(cid:62)
G12 =
21

(cid:16)

(cid:16)

(cid:17)

f (H)12 + f (H)(cid:62)
21

/
is the weighted aver-
age of G12 and G(cid:62)
21, “◦” and “/” are element-
wise product and division, respectively. The
columns in V 2 are independent, thus for each
vsi, it is a separate weighted ridge regression
problem, whose solution is (Holland, 1973):
1 diag( ¯f i)¯gi,
1 diag( ¯f i)V 1+µiI)−1V (cid:62)
v∗
si=(V (cid:62)
where ¯f i and ¯gi are columns corresponding
to si in ¯f (H)12 and G12, respectively;
5. For any other set of noncore words Sk, ﬁnd
V ∗
k that minimizes the total penalty of the 1k-
th and k1-th blocks, ignoring all other kj-th
and jk-th blocks;

6. Combine all subsets of embeddings to form

V ∗. Here V ∗ = (V ∗

1, V ∗

2, V ∗
3).

6 Experimental Results

We trained our model along with a few state-of-
the-art competitors on Wikipedia, and evaluated
the embeddings on 7 common benchmark sets.

6.1 Experimental Setup

Our own method is referred to as PSD. The com-
petitors include:

• (Mikolov et al., 2013b): word2vec2, or

SGNS in some literature;

2https://code.google.com/p/word2vec/

• (Levy and Goldberg, 2014): the PPMI ma-
trix without dimension reduction, and SVD
of PPMI matrix, both yielded by hyperwords;

• (Pennington et al., 2014): GloVe3;
• (Stratos et al., 2015): Singular4, which does
SVD-based CCA on the weighted bigram fre-
quency matrix;

• (Faruqui et al., 2015): Sparse5, which learns
new sparse embeddings in a higher dimen-
sional space from pretrained embeddings.

All models were trained on the English Wikipedia
snapshot in March 2015. After removing non-
textual elements and non-English words, 2.04 bil-
lion words were left. We used the default hyperpa-
rameters in Hyperwords when training PPMI and
SVD. Word2vec, GloVe and Singular were trained
with their own default hyperparameters.

The embedding sets PSD-Reg-180K and PSD-
Unreg-180K were trained using our online block-
wise regression. Both sets contain the embed-
dings of the most frequent 180,000 words, based
on 25,000 core words. PSD-Unreg-180K was
traind with all µi = 0, i.e. disabling Tikhonov
regularization. PSD-Reg-180K was trained with

µi =


2

4

8

i ∈ [25001, 80000]
i ∈ [80001, 130000]
i ∈ [130001, 180000]

, i.e.

increased

regularization as the sparsity increases. To con-
trast with the batch learning performance, the per-
formance of PSD-25K is listed, which contains the
core embeddings only. PSD-25K took advantages
that it contains much less false candidate words,
and some test tuples (generally harder ones) were
not evaluated due to missing words, thus its scores
are not comparable to others.

The benchmark sets are almost

Sparse was trained with PSD-180K-reg as the
input embeddings, with default hyperparameters.
to
those in (Levy et al., 2015), except that (Luong et
al., 2013)’s Rare Words is not included, as many
rare words are cut off at the frequency 100, mak-
ing more than 1/3 of test pairs invalid.

identical

Word Similarity There are 5 datasets: Word-
Sim Similarity (WS Sim) and WordSim Related-
ness (WS Rel) (Zesch et al., 2008; Agirre et al.,
2009), partitioned from WordSim353 (Finkelstein
et al., 2002); Bruni et al. (2012)’s MEN dataset;

3http://nlp.stanford.edu/projects/glove/
4https://github.com/karlstratos/singular
5https://github.com/mfaruqui/sparse-coding

Table 2: Performance of each method across different tasks.

Similarity Tasks

Analogy Tasks

Method
word2vec
PPMI
SVD
GloVe
Singular
Sparse
PSD-Reg-180K
PSD-Unreg-180K
PSD-25K

WS Sim WS Rel MEN Turk
0.663
0.659
0.524
0.641
0.581
0.625
0.676
0.675
0.678

0.543
0.678
0.608
0.630
0.684
0.585
0.679
0.663
0.676

0.731
0.717
0.711
0.756
0.747
0.725
0.764
0.753
0.765

0.742
0.735
0.687
0.759
0.763
0.739
0.792
0.786
0.801

SimLex
0.395
0.308
0.270
0.362
0.345
0.355
0.398
0.372
0.393

Google
0.734 / 0.742
0.476 / 0.524
0.230 / 0.240
0.535 / 0.544
0.440 / 0.508
0.240 / 0.282
0.602 / 0.623
0.566 / 0.598
0.671 / 0.695

MSR
0.650 / 0.674
0.183 / 0.217
0.123 / 0.113
0.408 / 0.435
0.364 / 0.399
0.253 / 0.274
0.465 / 0.507
0.424 / 0.468
0.533 / 0.586

Radinsky et al. (2011)’s Mechanical Turk dataset;
and (Hill et al., 2014)’s SimLex-999 dataset. The
embeddings were evaluated by the Spearman’s
rank correlation with the human ratings.

Word Analogy The two datasets are MSR’s
analogy dataset (Mikolov et al., 2013c), contain-
ing 8000 questions, and Google’s analogy dataset
(Mikolov et al., 2013a), with 19544 questions. Af-
ter ﬁltering questions involving out-of-vocabulary
words, i.e. words that appear less than 100 times
in the corpus, 7054 instances in MSR and 19364
instances in Google were left. The analogy ques-
tions were answered using 3CosAdd as well as
3CosMul proposed by Levy et al. (2014).

6.2 Results

Table 2 shows the results on all tasks. Word2vec
signiﬁcantly outperformed other methods on anal-
ogy tasks. PPMI and SVD performed much worse
on analogy tasks than reported in (Levy et al.,
2015), probably due to sub-optimal hyperparam-
eters. This suggests their performance is unstable.
The new embeddings yielded by Sparse systemat-
ically degraded compared to the old embeddings,
contradicting the claim in (Faruqui et al., 2015).

Our method PSD-Reg-180K performed well
consistently, and is best in 4 similarity tasks.
It performed worse than word2vec on analogy
tasks, but still better than other MF-based meth-
ods. By comparing to PSD-Unreg-180K, we see
Tikhonov regularization brings 1-4% performance
boost across tasks. In addition, on similarity tasks,
online blockwise regression only degrades slightly
compared to batch factorization. Their perfor-
mance gaps on analogy tasks were wider, but this
might be explained by the fact that some hard
cases were not counted in PSD-25K’s evaluation,

due to its limited vocabulary.

7 Conclusions and Future Work

In this paper, inspired by the link functions in
previous works, with the support from Informa-
tion Theory, we propose a new link function of a
text window, parameterized by the embeddings of
words and the residuals of bigrams. Based on the
link function, we establish a generative model of
documents. The learning objective is to ﬁnd a set
of embeddings maximizing their posterior likeli-
hood given the corpus. This objective is reduced to
weighted low-rank positive-semideﬁnite approxi-
mation, subject to Tikhonov regularization. Then
we adopt a Block Coordinate Descent algorithm,
jointly with an online blockwise regression algo-
rithm to ﬁnd an approximate solution. On seven
benchmark sets,
the learned embeddings show
competitive and stable performance.

In the future work, we will incorporate global
latent factors into this generative model, such as
topics, sentiments, or writing styles, and develop
more elaborate models of documents. Through
learning such latent factors, important summary
information of documents would be acquired,
which are useful in various applications.

Acknowledgments

We thank Omer Levy, Thomas Mach, Peilin Zhao,
Mingkui Tan, Zhiqiang Xu and Chunlin Wu for
their helpful discussions and insights. This re-
search is supported by the National Research
Foundation, Prime Minister’s Ofﬁce, Singapore
under its IDM Futures Funding Initiative and ad-
ministered by the Interactive and Digital Media
Programme Ofﬁce.

Appendix A Possible Trap in SVD

Suppose M is the bigram matrix of interest. SVD
embeddings are derived from the low rank approx-
imation of M (cid:62)M , by keeping the largest singular
values/vectors. When some of these singular val-
ues correspond to negative eigenvalues, undesir-
able correlations might be captured. The follow-
ing is an example of approximating a PMI matrix.
A vocabulary consists of 3 words s1, s2, s3.

Two corpora derive two PMI matrices:
(cid:17)
M (1) =

, M (2) =

(cid:16) 1.4 0.8 0
0.8 2.6 0
0 2
0

(cid:16) 0.2 −1.6 0
−1.6 −2.2 0
2

0

0

(cid:17)

.

0

the largest

They have identical left singular matrix and sin-
gular values (3, 2, 1), but their eigenvalues are
(3, 2, 1) and (−3, 2, 1), respectively.
In a rank-2 approximation,

two
singular values/vectors are kept, and M (1) and
M (2) yield identical SVD embeddings V =
( 0.45 0.89 0
0 1 ) (the rows may be scaled depending on
the algorithm, without affecting the validity of the
following conclusion). The embeddings of s1 and
s2 (columns 1 and 2 of V ) point at the same di-
rection, suggesting they are positively correlated.
However as M (2)
2,1 = −1.6 < 0, they are
actually negatively correlated in the second cor-
pus. This inconsistency is because the principal
eigenvalue of M (2) is negative, and yet the corre-
sponding singular value/vector is kept.
When using eigendecomposition,

the largest
two positive eigenvalues/eigenvectors are kept.
M (1) yields the same embeddings V . M (2)
yields V (2) = (cid:0) −0.89 0.45 0
(cid:1) , which correctly
0 1.41
preserves the negative correlation between s1, s2.

1,2 = M (2)

0

Appendix B Information Theory

Redundant information refers to the reduced un-
certainty by knowing the value of any one of the
conditioning variables (hence redundant). Syner-
gistic information is the reduced uncertainty as-
cribed to knowing all the values of conditioning
variables, that cannot be reduced by knowing the
value of any variable alone (hence synergistic).

The mutual information I(y; xi) and the redun-

dant information Rdn(y; x1, x2) are deﬁned as:

I(y; xi) = EP (xi,y)[log

P (y|xi)
P (y)

]

(cid:20)
Rdn(y; x1, x2) = EP (y)

EP (xi|y)[log

min
x1,x2
The synergistic information Syn(y; x1, x2) is
deﬁned as the PI-function in (Williams and Beer,
2010), skipped here.

P (y|xi)
P (y)

(cid:21)

]

Figure 2: Different types of information among
3 random variables y, x1, x2.
I(y; x1, x2) is
the mutual information between y and (x1, x2).
Rdn(y; x1, x2) and Syn(y; x1, x2) are the redun-
dant information and synergistic information be-
tween x1, x2, conditioning y, respectively.

The interaction information Int(x1, x2, y) mea-
sures the relative strength of Rdn(y; x1, x2) and
Syn(y; x1, x2) (Timme et al., 2014):

Int(x1, x2, y)

=Syn(y; x1, x2) − Rdn(y; x1, x2)
=I(y; x1, x2) − I(y; x1) − I(y; x2)

=EP (x1,x2,y)[log

P (x1)P (x2)P (y)P (x1, x2, y)
P (x1, x2)P (x1, y)P (x2, y)

]

Figure 2 shows the relationship of different
information among 3 random variables y, x1, x2
(based on Fig.1 in (Williams and Beer, 2010)).

PMI is the pointwise counterpart of mutual
information I. Similarly, all the above concepts
have their pointwise counterparts, obtained by
dropping the expectation operator. Speciﬁcally,
the pointwise interaction information is deﬁned as
PInt(x1, x2, y) = PMI(y; x1, x2) − PMI(y; x1) −
log P (x1)P (x2)P (y)P (x1,x2,y)
PMI(y; x2)
P (x1,x2)P (x1,y)P (x2,y) .
If we know PInt(x1, x2, y), we can recover
PMI(y; x1, x2) from the mutual information over
the variable subsets, and then recover the joint
distribution P (x1, x2, y).

=

As

the

pointwise

redundant

interaction terms,

information
PRdn(y; x1, x2) and the pointwise synergistic
information PSyn(y; x1, x2) are both higher-
order
their magnitudes are
usually much smaller than the PMI terms. We
assume they are approximately equal, and thus
cancel each other when computing PInt. Given
this, PInt
In the case of three
words w0, w1, w2, PInt(w0, w1, w2) = 0 leads to
PMI(w2; w0, w1) = PMI(w2; w0)+PMI(w2; w1).

is always 0.

References

[Agirre et al.2009] Eneko Agirre, Enrique Alfonseca,
Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor
Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 19–27. Association for Computa-
tional Linguistics.

[Arora et al.2015] S. Arora, Y. Li, Y. Liang, T. Ma, and
A. Risteski. 2015. Random walks on discourse
spaces: a new generative language model with ap-
plications to semantic word embeddings. ArXiv e-
prints, arXiv:1502.03520 [cs.LG].

[Bengio et al.2006] Yoshua Bengio, Holger Schwenk,
Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-
Luc Gauvain. 2006. Neural probabilistic language
models. In Innovations in Machine Learning, pages
137–186. Springer.

[Blei et al.2003] David M Blei, Andrew Y Ng, and
Michael I Jordan. 2003. Latent dirichlet allocation.
The Journal of Machine Learning Research, 3:993–
1022.

[Bruni et al.2012] Elia Bruni, Gemma Boleda, Marco
Baroni, and Nam-Khanh Tran. 2012. Distributional
semantics in technicolor. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1, pages
136–145. Association for Computational Linguis-
tics.

[Deerwester et al.1990] Scott C. Deerwester, Susan T
Dumais, and Richard A. Harshman. 1990. Index-
ing by latent semantic analysis. J. Am. Soc. Inf. Sci.

[Dhillon et al.2011] Paramveer Dhillon, Dean P Foster,
and Lyle H Ungar. 2011. Multi-view learning of
In Proceedings of Ad-
word embeddings via cca.
vances in Neural Information Processing Systems,
pages 199–207.

[Dhillon et al.2015] Paramveer S Dhillon, Dean P Fos-
ter, and Lyle H Ungar. 2015. Eigenwords: Spectral
word embeddings. The Journal of Machine Learn-
ing Research.

[Faruqui et al.2015] Manaal Faruqui, Yulia Tsvetkov,
Dani Yogatama, Chris Dyer, and Noah A. Smith.
2015. Sparse overcomplete word vector represen-
tations. In Proceedings of ACL 2015.

[Hill et al.2014] Felix Hill, Roi Reichart, and Anna Ko-
rhonen. 2014. Simlex-999: Evaluating semantic
models with (genuine) similarity estimation. CoRR,
abs/1408.3456.

[Hinton2002] Geoffrey Hinton. 2002. Training prod-
ucts of experts by minimizing contrastive diver-
gence. Neural Computation, 14(8):1771–1800.

[Holland1973] Paul W. Holland.

1973. Weighted
Ridge Regression: Combining Ridge and Ro-
bust Regression Methods. NBER Working Papers
0011, National Bureau of Economic Research, Inc,
September.

[Hsu et al.2012] Daniel Hsu, Sham M Kakade, and
Tong Zhang. 2012. A spectral algorithm for learn-
ing hidden markov models. Journal of Computer
and System Sciences, 78(5):1460–1480.

[Levy and Goldberg2014] Omer Levy and Yoav Gold-
berg. 2014. Neural word embeddings as implicit
matrix factorization. In Proceedings of NIPS 2014.

[Levy et al.2014] Omer Levy, Yoav Goldberg, and Is-
rael Ramat-Gan. 2014. Linguistic regularities in
In Pro-
sparse and explicit word representations.
ceedings of CoNLL-2014, page 171.

[Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido
Dagan. 2015.
Improving distributional similarity
with lessons learned from word embeddings. Trans-
actions of the Association for Computational Lin-
guistics, 3:211–225.

[Lin and He2009] Chenghua Lin and Yulan He. 2009.
Joint sentiment/topic model for sentiment analysis.
In Proceedings of the 18th ACM conference on In-
formation and Knowledge Management, pages 375–
384. ACM.

[Luong et al.2013] Minh-Thang

Richard
Socher, and Christopher D Manning. 2013. Better
word representations with recursive neural networks
for morphology. CoNLL-2013, 104.

Luong,

[Mach2012] T. Mach. 2012. Eigenvalue Algorithms
for Symmetric Hierarchical Matrices. Dissertation,
Chemnitz University of Technology.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
mation of word representations in vector space. In
Proceedings of Workshop at ICLR 2013.

[Finkelstein et al.2002] Lev

Finkelstein,

Evgeniy
Gabrilovich, Yossi Matias, Ehud Rivlin, Zach
Solan, Gadi Wolfman, and Eytan Ruppin. 2002.
Placing search in context: The concept revisited.
ACM Trans. Inf. Syst., 20(1):116–131, January.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases
and their compositionality. In Proceedings of NIPS
2013, pages 3111–3119.

[Higham1988] Nicholas J. Higham. 1988. Comput-
ing a nearest symmetric positive semideﬁnite matrix.
Linear Algebra and its Applications, 103(0):103 –
118.

[Mikolov et al.2013c] Tomas Mikolov, Wen-tau Yih,
and Geoffrey Zweig. 2013c. Linguistic regularities
In Pro-
in continuous space word representations.
ceedings of HLT-NAACL 2013, pages 746–751.

[Zesch et al.2008] Torsten Zesch, Christof M¨uller, and
Iryna Gurevych. 2008. Using wiktionary for com-
In Proceedings of
puting semantic relatedness.
AAAI 2008, volume 8, pages 861–866.

[Zhai and Lafferty2004] Chengxiang Zhai and John
Lafferty. 2004. A study of smoothing methods
for language models applied to information retrieval.
ACM Transactions on Information Systems (TOIS),
22(2):179–214.

[Zhao et al.2011a] Peilin Zhao, Steven CH Hoi, and
Rong Jin. 2011a. Double updating online learn-
ing. The Journal of Machine Learning Research,
12:1587–1615.

[Zhao et al.2011b] Wayne Xin Zhao, Jing Jiang, Jian-
shu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and
Xiaoming Li. 2011b. Comparing twitter and tra-
ditional media using topic models. In Advances in
Information Retrieval (Proceedings of the 33rd An-
nual European Conference on Information Retrieval
Research), pages 338–349. Springer.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey
Hinton. 2007. Three new graphical models for sta-
In Proceedings of the
tistical language modelling.
24th International Conference on Machine learning,
pages 641–648. ACM.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. Proceedings
of the Empiricial Methods in Natural Language Pro-
cessing (EMNLP 2014), 12.

[Radinsky et al.2011] Kira

Eugene
Radinsky,
Agichtein,
and Shaul
Evgeniy Gabrilovich,
Markovitch. 2011. A word at a time: Computing
word relatedness using temporal semantic analysis.
In Proceedings of the 20th International Conference
on World Wide Web, WWW ’11, pages 337–346,
New York, NY, USA. ACM.

[Srebro et al.2003] Nathan Srebro, Tommi Jaakkola,
et al. 2003. Weighted low-rank approximations. In
Proceedings of ICML 2003, volume 3, pages 720–
727.

[Stratos et al.2014] Karl

Stratos, Do-kyum Kim,
Michael Collins, and Daniel Hsu. 2014. A spectral
algorithm for learning class-based n-gram models of
natural language. In Proceedings of the Association
for Uncertainty in Artiﬁcial Intelligence.

[Stratos et al.2015] Karl Stratos, Michael Collins, and
Daniel Hsu. 2015. Model-based word embeddings
from decompositions of count matrices. In Proceed-
ings of ACL 2015.

[Tan et al.2014] M. Tan, I. W. Tsang, L. Wang, B. Van-
dereycken, and S. J. Pan. 2014. Riemannian pursuit
In Proceedings of ICML
for big matrix recovery.
2014, pages 1539–1547.

[Timme et al.2014] Nicholas Timme, Wesley Alford,
Benjamin Flecker, and John M Beggs. 2014. Syn-
ergy, redundancy, and multivariate information mea-
sures: an experimentalist’s perspective. Journal of
Computational Neuroscience, 36(2):119–140.

[Wallach2006] Hanna M Wallach. 2006. Topic mod-
eling: beyond bag-of-words. In Proceedings of the
23rd international conference on Machine learning,
pages 977–984. ACM.

[Williams and Beer2010] Paul L Williams and Ran-
2010. Nonnegative decomposi-
arXiv preprint

dall D Beer.
tion of multivariate information.
arXiv:1004.2515.

[Yan et al.2015] Yan Yan, Mingkui Tan, Ivor Tsang,
Yi Yang, Chengqi Zhang, and Qinfeng Shi. 2015.
Scalable maximum margin matrix factorization by
active riemannian subspace search. In Proceedings
of IJCAI 2015.

[Yogatama et al.2015] Dani

Manaal
Faruqui, Chris Dyer, and Noah A Smith.
2015.
Learning word representations with hierarchical
sparse coding. In Proceedings of ICML 2015.

Yogatama,

A Generative Word Embedding Model and its Low Rank Positive
Semideﬁnite Solution

Shaohua Li1, Jun Zhu2, Chunyan Miao1
1Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY),
Nanyang Technological University, Singapore
2Tsinghua University, P.R. China
lish0018@ntu.edu.sg, dcszj@tsinghua.edu.cn, ascymiao@ntu.edu.sg

Abstract

Most existing word embedding methods
can be categorized into Neural Embedding
Models and Matrix Factorization (MF)-
based methods. However some mod-
els are opaque to probabilistic interpre-
tation, and MF-based methods, typically
solved using Singular Value Decomposi-
tion (SVD), may incur loss of corpus in-
formation.
In addition, it is desirable to
incorporate global latent factors, such as
topics, sentiments or writing styles, into
the word embedding model. Since gen-
erative models provide a principled way
to incorporate latent factors, we propose a
generative word embedding model, which
is easy to interpret, and can serve as a
basis of more sophisticated latent factor
models. The model inference reduces to
a low rank weighted positive semideﬁnite
approximation problem.
Its optimization
is approached by eigendecomposition on a
submatrix, followed by online blockwise
regression, which is scalable and avoids
the information loss in SVD. In experi-
ments on 7 common benchmark datasets,
our vectors are competitive to word2vec,
and better than other MF-based methods.

1

Introduction

The task of word embedding is to model the distri-
bution of a word and its context words using their
corresponding vectors in a Euclidean space. Then
by doing regression on the relevant statistics de-
rived from a corpus, a set of vectors are recovered
which best ﬁt these statistics. These vectors, com-
monly referred to as the embeddings, capture se-
mantic/syntactic regularities between the words.

The core of a word embedding method is the
link function that connects the input — the embed-
dings, with the output — certain corpus statistics.

Based on the link function, the objective function
is developed. The reasonableness of the link func-
tion impacts the quality of the obtained embed-
dings, and different link functions are amenable
to different optimization algorithms, with different
scalability. Based on the forms of the link func-
tion and the optimization techniques, most meth-
ods can be divided into two classes: the traditional
neural embedding models, and more recent low
rank matrix factorization methods.

The neural embedding models use the softmax
link function to model the conditional distribution
of a word given its context (or vice versa) as a
function of the embeddings. The normalizer in the
softmax function brings intricacy to the optimiza-
tion, which is usually tackled by gradient-based
methods. The pioneering work was (Bengio et
al., 2006). Later Mnih and Hinton (2007) propose
three different link functions. However there are
interaction matrices between the embeddings in all
these models, which complicate and slow down
the training, hindering them from being trained
on huge corpora. Mikolov et al. (2013a) and
Mikolov et al. (2013b) greatly simplify the condi-
tional distribution, where the two embeddings in-
teract directly. They implemented the well-known
“word2vec”, which can be trained efﬁciently on
huge corpora. The obtained embeddings show ex-
cellent performance on various tasks.

Low-Rank Matrix Factorization (MF in short)
methods include various link functions and opti-
mization methods. The link functions are usu-
ally not softmax functions. MF methods aim to
reconstruct certain corpus statistics matrix by the
product of two low rank factor matrices. The ob-
jective is usually to minimize the reconstruction
error, optionally with other constraints.
In this
line of research, Levy and Goldberg (2014) ﬁnd
that “word2vec” is essentially doing stochastic
weighted factorization of the word-context point-
wise mutual information (PMI) matrix. They then

5
1
0
2
 
g
u
A
 
6
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
2
8
3
0
.
8
0
5
1
:
v
i
X
r
a

factorize this matrix directly as a new method.
Pennington et al. (2014) propose a bilinear regres-
sion function of the conditional distribution, from
which a weighted MF problem on the bigram log-
frequency matrix is formulated. Gradient Descent
is used to ﬁnd the embeddings. Recently, based
on the intuition that words can be organized in se-
mantic hierarchies, Yogatama et al. (2015) add hi-
erarchical sparse regularizers to the matrix recon-
struction error. With similar techniques, Faruqui
et al. (2015) reconstruct a set of pretrained embed-
dings using sparse vectors of greater dimensional-
ity. Dhillon et al. (2015) apply Canonical Corre-
lation Analysis (CCA) to the word matrix and the
context matrix, and use the canonical correlation
vectors between the two matrices as word embed-
dings. Stratos et al. (2014) and Stratos et al. (2015)
assume a Brown language model, and prove that
doing CCA on the bigram occurrences is equiva-
lent to ﬁnding a transformed solution of the lan-
guage model. Arora et al. (2015) assume there is a
hidden discourse vector on a random walk, which
determines the distribution of the current word.
The slowly evolving discourse vector puts a con-
straint on the embeddings in a small text window.
The maximum likelihood estimate of the embed-
dings within this text window approximately re-
duces to a squared norm objective.

There are two limitations in current word em-
bedding methods. The ﬁrst limitation is, all MF-
based methods map words and their context words
to two different sets of embeddings, and then em-
ploy Singular Value Decomposition (SVD) to ob-
tain a low rank approximation of the word-context
matrix M . As SVD factorizes M (cid:62)M , some in-
formation in M is lost, and the learned embed-
dings may not capture the most signiﬁcant regu-
larities in M . Appendix A gives a toy example on
which SVD does not work properly.

The second limitation is, a generative model for
documents parametered by embeddings is absent
in recent development. Although (Stratos et al.,
2014; Stratos et al., 2015; Arora et al., 2015) are
based on generative processes, the generative pro-
cesses are only for deriving the local relationship
between embeddings within a small text window,
leaving the likelihood of a document undeﬁned.
In addition, the learning objectives of some mod-
els, e.g. (Mikolov et al., 2013b, Eq.1), even have
no clear probabilistic interpretation. A genera-
tive word embedding model for documents is not

only easier to interpret and analyze, but more im-
portantly, provides a basis upon which document-
level global latent factors, such as document topics
(Wallach, 2006), sentiments (Lin and He, 2009),
writing styles (Zhao et al., 2011b), can be incor-
porated in a principled manner, to better model the
text distribution and extract relevant information.
Based on the above considerations, we pro-
pose to unify the embeddings of words and con-
text words. Our link function factorizes into three
parts: the interaction of two embeddings capturing
linear correlations of two words, a residual captur-
ing nonlinear or noisy correlations, and the uni-
gram priors. To reduce overﬁtting, we put Gaus-
sian priors on embeddings and residuals, and ap-
ply Jelinek-Mercer Smoothing to bigrams. Fur-
thermore, to model the probability of a sequence
of words, we assume that the contributions of
more than one context word approximately add up.
Thereby a generative model of documents is con-
structed, parameterized by embeddings and resid-
uals. The learning objective is to maximize the
corpus likelihood, which reduces to a weighted
low-rank positive semideﬁnite (PSD) approxima-
tion problem of the PMI matrix. A Block Co-
ordinate Descent algorithm is adopted to ﬁnd an
approximate solution. This algorithm is based
on Eigendecomposition, which avoids information
loss in SVD, but brings challenges to scalability.
We then exploit the sparsity of the weight matrix
and implement an efﬁcient online blockwise re-
gression algorithm. On seven benchmark datasets
covering similarity and analogy tasks, our method
achieves competitive and stable performance.

The source code of this method is provided at

https://github.com/askerlee/topicvec.

2 Notations and Deﬁnitions

Throughout the paper, we always use a uppercase
bold letter as S, V to denote a matrix or set, a low-
ercase bold letter as vwi to denote a vector, a nor-
mal uppercase letter as N, W to denote a scalar
constant, and a normal lowercase letter as si, wi to
denote a scalar variable.

Suppose a vocabulary S = {s1, · · · , sW } con-
sists of all the words, where W is the vocab-
ulary size. We further suppose s1, · · · , sW are
sorted in decending order of the frequency, i.e.
s1 is most frequent, and sW is least frequent.
A document di
is a sequence of words di =
(wi1, · · · , wiLi), wij ∈ S. A corpus is a collec-

Name
S

V

D
vsi
asisj
˜P (si,sj)
u

A

B

G

H

Description
Vocabulary {s1, · · · , sW }
Embedding matrix (vs1 , · · · , vsW )
Corpus {d1, · · · , dM }
Embedding of word si
Bigram residual for si, sj
Empirical probability of si, sj in the corpus
Unigram probability vector (P (s1),· · ·, P (sW ))
Residual matrix (asisj )
(cid:16)

(cid:17)

Conditional probability matrix

PMI matrix

PMI(si, sj)

(cid:16)

P (sj|si)
(cid:17)
(cid:16) ˜P (si, sj)
(cid:17)

Bigram empirical probability matrix

Table 1: Notation Table

tion of M documents D = {d1, · · · , dM }. In the
vocabulary, each word si is mapped to a vector vsi
in N -dimensional Euclidean space.

In a document, a sequence of words is referred
to as a text window, denoted by wi, · · · , wi+l, or
wi:wi+l in shorthand. A text window of chosen
size c before a word wi deﬁnes the context of wi
as wi−c, · · · , wi−1. Here wi is referred to as the
focus word. Each context word wi−j and the focus
word wi comprise a bigram wi−j, wi.

The Pointwise Mutual Information between two

words si, sj is deﬁned as

PMI(si, sj) = log

P (si, sj)
P (si)P (sj)

.

3 Link Function of Text

In this section, we formulate the probability of a
sequence of words as a function of their embed-
dings. We start from the link function of bigrams,
which is the building blocks of a long sequence.
Then this link function is extended to a text win-
dow with c context words, as a ﬁrst-order approx-
imation of the actual probability.

3.1 Link Function of Bigrams

We generalize the link function of “word2vec” and
“GloVe” to the following:

(cid:110)

(cid:111)

P (si, sj) = exp

P (si)P (sj) (1)

v(cid:62)
sj vsi + asisj
The rationale for (1) originates from the idea of
the Product of Experts in (Hinton, 2002). Sup-
pose different types of semantic/syntactic regu-
larities between si and sj are encoded in differ-
sj vsi} =
ent dimensions of vsi, vsj . As exp{v(cid:62)
(cid:81)
l exp{vsi,l · vsj ,l}, this means the effects of dif-
ferent regularities on the probability are combined

by multiplying together. If si and sj are indepen-
dent, their joint probability should be P (si)P (sj).
In the presence of correlations, the actual joint
probability P (si, sj) would be a scaling of it. The
scale factor reﬂects how much si and sj are pos-
itively or negatively correlated. Within the scale
factor, v(cid:62)
sj vsi captures linear interactions between
si and sj, the residual asisj captures nonlinear or
sj vsi is
noisy interactions. In applications, only v(cid:62)
of interest. Hence the bigger magnitude v(cid:62)
sj vsi is
of relative to asisj , the better.

Note that we do not assume asisj = asj si.
This provides the ﬂexibility P (si, sj) (cid:54)= P (sj, si),
agreeing with the asymmetry of bigrams in natu-
ral languages. At the same time, v(cid:62)
sj vsi imposes a
symmetric part between P (si, sj) and P (sj, si).

(1) is equivalent to

(cid:110)

P (sj|si)=exp

v(cid:62)
sj vsi + asisj + log P (sj)

, (2)

(cid:111)

log

P (sj|si)
P (sj)

= v(cid:62)

sj vsi + asisj .

(3) of all bigrams is represented in matrix form:

(3)

(4)

V (cid:62)V + A = G,

where G is the PMI matrix.

3.1.1 Gaussian Priors on Embeddings
When (1) is employed on the regression of empir-
ical bigram probabilities, a practical issue arises:
more and more bigrams have zero frequency as
the constituting words become less frequent. A
zero-frequency bigram does not necessarily imply
negative correlation between the two constituting
words; it could simply result from missing data.
But in this case, even after smoothing, (1) will
force v(cid:62)
sj vsi + asisj to be a big negative number,
making vsi overly long. The increased magnitude
of embeddings is a sign of overﬁtting.

To reduce overﬁtting of embeddings of infre-
quent words, we assign a Spherical Gaussian prior
N (0, 1
2µi

I) to vsi:
P (vsi) ∼ exp{−µi(cid:107)vsi(cid:107)2},
where the hyperparameter µi increases as the fre-
quency of si decreases.

3.1.2 Gaussian Priors on Residuals
We wish v(cid:62)
sj vsi in (1) captures as much corre-
lations between si and sj as possible. Thus the
smaller asisj is, the better. In addition, the more
frequent si, sj is in the corpus,
the less noise
there is in their empirical distribution, and thus the
residual asisj should be more heavily penalized.

To this end, we penalize the residual asisj
by f (˜P (si, sj))a2
sisj , where f (·) is a nonnega-
tive monotonic transformation, referred to as the
weighting function. Let hij denote ˜P (si, sj), then
the total penalty of all residuals are the square of
the weighted Frobenius norm of A:
sisj = (cid:107)A(cid:107)2

f (hij)a2

f (H).

(cid:88)

(5)

si,sj ∈S

By referring to “GloVe”, we use the following

√

weighting function, and ﬁnd it performs well:
(cid:112)hij < Ccut, i (cid:54)= j
(cid:112)hij ≥ Ccut, i (cid:54)= j
i = j


hij

Ccut
1

0

f (hij) =

,

where Ccut is chosen to cut the most frequent
0.02% of the bigrams off at 1. When si = sj, two
identical words usually have much smaller proba-
bility to collocate. Hence ˜P (si, si) does not reﬂect
the true correlation of a word to itself, and should
not put constraints to the embeddings. We elimi-
nate their effects by setting f (hii) to 0.

(cid:16)

If the domain of A is the whole space RW ×W ,
then this penalty is equivalent to a Gaussian prior
N
on each asisj . The variances of the
Gaussians are determined by the bigram empirical
probability matrix H.

1
2f (hij )

0,

(cid:17)

Jelinek-Mercer Smoothing of Bigrams

3.1.3
As another measure to reduce the impact of miss-
ing data, we apply the commonly used Jelinek-
Mercer Smoothing (Zhai and Lafferty, 2004)
to smooth the empirical conditional probability
˜P (sj|si) by the unigram probability ˜P (sj) as:
˜Psmoothed(sj|si) = (1−κ) ˜P (sj|si)+κP (sj). (6)
the smoothed bigram empirical
Accordingly,

joint probability is deﬁned as
˜P (si, sj) = (1−κ) ˜P (si, sj)+κP (si)P (sj). (7)
In practice, we ﬁnd κ = 0.02 yields good re-
sults. When κ ≥ 0.04, the obtained embeddings
begin to degrade with κ, indicating that smoothing
distorts the true bigram distributions.

3.2 Link Function of a Text Window

In the previous subsection, a regression link func-
tion of bigram probabilities is established.
In
this section, we adopt a ﬁrst-order approximation
based on Information Theory, and extend the link
function to a longer sequence w0, · · · , wc−1, wc.

Decomposing a distribution conditioned on n
random variables as the conditional distributions

on its subsets roots deeply in Information The-
ory. This is an intricate problem because there
could be both (pointwise) redundant information
and (pointwise) synergistic information among the
conditioning variables (Williams and Beer, 2010).
They are both functions of the PMI. Based on an
analysis of the complementing roles of these two
types of pointwise information, we assume they
are approximately equal and cancel each other
when computing the pointwise interaction infor-
mation. See Appendix B for a detailed discussion.
Following the above assumption, we have
PMI(w2; w0, w1) ≈ PMI(w2; w0)+PMI(w2; w1):

log

P (w0, w1|w2)
P (w0, w1)

≈log

P (w0|w2)
P (w0)

+log

P (w1|w2)
P (w1)

.

Plugging (1) and (3) into the above, we obtain
P (w0, w1, w2)
(cid:26) 2

≈ exp

(v(cid:62)

wivwj + awiwj ) +

log P (wi)

(cid:27)
.

(cid:88)

i,j=0
i(cid:54)=j

We extend the above assumption to that the
pointwise interaction information is still close to
0 within a longer text window. Accordingly the
above equation extends to a context of size c > 2:

P (w0, · · · , wc)
(cid:26) c

(cid:88)

≈ exp

i,j=0
i(cid:54)=j

(v(cid:62)

wivwj + awiwj ) +

log P (wi)

(cid:27)
.

2
(cid:88)

i=0

c
(cid:88)

i=0

From it derives the conditional distribution of

wc, given its context w0, · · · , wc−1:

P (wc | w0 : wc−1)=

(cid:26)

≈P (wc) exp

v(cid:62)
wc

P (w0, · · · , wc)
P (w0, · · · , wc−1)
c−1
(cid:27)
(cid:88)
.

awiwc

vwi +

i=0

c−1
(cid:88)

i=0

(8)

4 Generative Process and Likelihood

We proceed to assume the text is generated from a
Markov chain of order c, i.e., a word only depends
on words within its context of size c. Given the
hyperparameter µ = (µ1, · · ·, µW ), the generative
process of the whole corpus is:

1. For each word si, draw the embedding vsi

from N (0, 1
2µi

I);

(cid:17)

(cid:16)

0,

asisj from N

2. For each bigram si, sj, draw the residual
1
2f (hij )
3. For each document di, for the j-th word,
from S with probability

draw word wij
P (wij | wi,j−c : wi,j−1) deﬁned by (8).

;

5 Learning Algorithm

5.1 Learning Objective

The learning objective is to ﬁnd the embeddings
V that maximize the corpus log-likelihood (9).

Let xij denote the (smoothed) frequency of bi-

gram si, sj in the corpus. Then (9) is sorted as:

log p(D, V , A)

µi

vsi

V

hij

aij

A

d

vw0

vw1

· · ·

vwc

=C0 − log Z(H, µ) − (cid:107)A(cid:107)2

f (H) −

µi(cid:107)vsi(cid:107)2

W
(cid:88)

i=1

Figure 1: The Graphical Model of PSDVec

The above generative process for a document d is
presented as a graphical model in Figure 1.

Based on this generative process, the probabil-
ity of a document di can be derived as follows,
given the embeddings and residuals V , A:

P (di|V , A)
Li(cid:89)

(cid:26)

j−1
(cid:88)

j−1
(cid:88)

=

P (wij) exp

v(cid:62)

wij

vwik +

awikwij

j=1

k=j−c

k=j−c

The complete-data likelihood of the corpus is:

p(D, V , A)

=

N (0,

W
(cid:89)

i=1

I
2µi

)

W,W
(cid:89)

(cid:18)

N

0,

i,j=1

1
2f (hij)

(cid:19) M
(cid:89)

p(di|V, A)

=

1
Z(H, µ)

(cid:110)

exp

−

W,W
(cid:88)

f (hi,j)a2

sisj −

µi(cid:107)vsi(cid:107)2(cid:111)

i,j=1

(cid:26)

j−1
(cid:88)

P (wij) exp

v(cid:62)

wij

vwik +

awikwij

M,Li
(cid:89)
·
i,j=1

+

W,W
(cid:88)

i,j=1

xij(v(cid:62)

sivsj + asisj ).

(10)

As
the
(cid:80)W,W
i,j=1 xij(v(cid:62)

corpus
sivsj +asisj ) will

increases,
the
parameter prior terms. Then we can ignore the
prior terms when maximizing (10).

dominate

size

max
(cid:16)(cid:88)

(cid:88)

xij

xij(v(cid:62)
(cid:17)

· max

=

sivsj +asisj )

(cid:88) ˜Psmoothed(si, sj) log P (si, sj).

As both { ˜Psmoothed(si, sj)} and {P (si, sj)}
the above sum is maximized when

sum to 1,
P (si, sj) = ˜Psmoothed(si, sj).

(cid:27)
.

The maximum likelihood estimator is then:
P (sj|si) = ˜Psmoothed(sj|si),

v(cid:62)
sivsj + asisj = log

˜Psmoothed(sj|si)
P (sj)

.

(11)

B∗ =

Writing (11) in matrix form:
(cid:17)
(cid:16) ˜Psmoothed(sj|si)
G∗ = log B∗ − log u ⊗ (1 · · · 1),

si,sj ∈S

(12)

(cid:27)

,

where “⊗” is the outer product.
Now we ﬁx the values of v(cid:62)

sivsj + asisj at the

k=j−c

k=j−c

above optimal. The corpus likelihood becomes

where Z(H, µ) is the normalizing constant.

logarithm of both sides of

log p(D, V , A) =C1 − (cid:107)A(cid:107)2

f (H) −

µi(cid:107)vsi(cid:107)2,

W
(cid:88)

i=1

Taking the
p(D, A, V ) yields

log p(D, V , A)

=C0 − log Z(H, µ) − (cid:107)A(cid:107)2

f (H)−

µi(cid:107)vsi(cid:107)2

M,Li
(cid:88)

(cid:26)

+

v(cid:62)

wij

j−1
(cid:88)

j−1
(cid:88)

(cid:27)

vwik +

awikwij

,

(9)

i,j=1

k=j−c

k=j−c

where C0 = (cid:80)M,Li

i,j=1 log P (wij) is constant.

subject to V (cid:62)V + A = G∗,

(13)
where C1 = C0 + (cid:80) xij log ˜Psmoothed(si, sj) −
log Z(H, µ) is constant.

5.2 Learning V as Low Rank PSD

Approximation

Once G∗ has been estimated from the corpus using
(12), we seek V that maximizes (13). This is to
ﬁnd the maximum a posteriori (MAP) estimates
of V , A that satisfy V (cid:62)V + A = G∗. Applying
this constraint to (13), we obtain

i=1

W
(cid:88)

i=1

j−1
(cid:88)

W
(cid:88)

i=1

Algorithm 1 BCD algorithm for ﬁnding a unreg-
ularized rank-N weighted PSD approximant.
Input: matrix G∗, weight matrix W = f (H),
iteration number T , rank N
Randomly initialize X (0)
for t = 1, · · · , T do

Gt = W ◦ G∗ + (1 − W ) ◦ X (t−1)
X (t) = PSD Approximate(Gt, N )

end for
λ, Q = Eigen Decomposition(X (T ))
V ∗ = diag(λ
Output: V ∗

1
2 [1:N ]) · Q(cid:62)[1:N ]

arg max
V

log p(D, V , A)

= arg min
V

(cid:107)G∗−V (cid:62)V (cid:107)f (H) +

µi(cid:107)vsi(cid:107)2. (14)

W
(cid:88)

i=1

Let X = V (cid:62)V . Then X is positive semidef-
inite of rank N . Finding V that minimizes (14)
is equivalent to ﬁnding a rank-N weighted posi-
tive semideﬁnite approximant X of G∗, subject to
Tikhonov regularization. This problem does not
admit an analytic solution, and can only be solved
using local optimization methods.

First we consider a simpler case where all the
words in the vocabulary are enough frequent, and
thus Tikhonov regularization is unnecessary.
In
this case, we set ∀µi = 0, and (14) becomes an
unregularized optimization problem. We adopt the
Block Coordinate Descent (BCD) algorithm1 in
(Srebro et al., 2003) to approach this problem. The
original algorithm is to ﬁnd a generic rank-N ma-
trix for a weighted approximation problem, and
we tailor it by constraining the matrix within the
positive semideﬁnite manifold.

We summarize our learning algorithm in Al-
gorithm 1. Here “◦” is the entry-wise prod-
uct. We suppose the eigenvalues λ returned by
Eigen Decomposition(X) are in descending or-
der. Q(cid:62)[1:N ] extracts the 1 to N rows from Q(cid:62).
One key issue is how to initialize X. Srebro et
al. (2003) suggest to set X (0) =G∗, and point out
that X (0) = 0 is far from a local optimum, thus
requires more iterations. However we ﬁnd G∗ is
also far from a local optimum, and this setting con-
verges slowly too. Setting X (0) = G∗/2 usually

1It is referred to as an Expectation-Maximization algo-
rithm by the original authors, but we think this is a misnomer.

yields a satisfactory solution in a few iterations.

The subroutine PSD Approximate() computes
the unweighted nearest rank-N PSD approxima-
tion, measured in F-norm (Higham, 1988).

5.3 Online Blockwise Regression of V

the

does

essential

subroutine
In Algorithm 1,
PSD Approximate()
eigendecomposi-
tion on Gt, which is dense due to the logarithm
transformation. Eigendecomposition on a W × W
dense matrix requires O(W 2) space and O(W 3)
time, difﬁcult to scale up to a large vocabulary. In
addition, the majority of words in the vocabulary
are infrequent, and Tikhonov regularization is
necessary for them.

It is observed that, as words become less fre-
quent, fewer and fewer words appear around them
to form bigrams. Remind that the vocabulary
S = {s1, · · · , sW } are sorted in decending or-
der of the frequency, hence the lower-right blocks
of H and f (H) are very sparse, and cause these
blocks in (14) to contribute much less penalty rela-
tive to other regions. Therefore these blocks could
be ignored when doing regression, without sacri-
ﬁcing too much accuracy. This intuition leads to
the following online blockwise regression.

The basic idea is to select a small set (e.g.
30,000) of the most frequent words as the core
words, and partition the remaining noncore words
into sets of moderate sizes. Bigrams consist-
ing of two core words are referred to as core bi-
grams, which correspond to the top-left blocks of
G and f (H). The embeddings of core words
are learned approximately using Algorithm 1, on
the top-left blocks of G and f (H). Then we ﬁx
the embeddings of core words, and ﬁnd the em-
beddings of each set of noncore words in turn.
After ignoring the lower-right regions of G and
f (H) which correspond to bigrams of two non-
core words, the quadratic terms of noncore em-
beddings are ignored. Consequently, ﬁnding these
embeddings becomes a weighted ridge regression
problem, which can be solved efﬁciently in closed-
form. Finally we combine all embeddings to get
the embeddings of the whole vocabulary. The de-
tails are as follows:

1. Partition S into K consecutive groups
S1, · · · , Sk. Take K = 3 as an example.
The ﬁrst group is core words;

2. Accordingly partition G into K × K blocks,

in this example as





G11 G12 G13
G21 G22 G23
G31 G32 G33



 .

Partition
same way.
G11, f (H)11, A11 correspond to core bi-

f (H),A in

the

grams. Partition V into

(cid:0)

V 1
(cid:124)(cid:123)(cid:122)(cid:125)
S1
1 V 1 + A11 = G11 using Algorithm

V 3
(cid:124) (cid:123)(cid:122) (cid:125)
S3

V 2
(cid:124) (cid:123)(cid:122) (cid:125)
S2

;

(cid:1)

3. Solve V (cid:62)

1, and obtain core embeddings V ∗
1;

4. Set V 1 = V ∗

1, and ﬁnd V ∗
2 that minimizes
the total penalty of the 12-th and 21-th blocks
of residuals (the 22-th block is ignored due to
its high sparsity):

(cid:107)G12 − V (cid:62)

1 V 2(cid:107)2

f (H)12

arg min
V 2
+ (cid:107)G21 − V (cid:62)

2 V 1(cid:107)2

f (H)21

+

(cid:88)

µi(cid:107)vsi(cid:107)2

si∈S2
(cid:88)
+

si∈S2

= arg min
V 2

(cid:107)G12−V (cid:62)

1 V 2(cid:107)2

¯f (H)12

µi(cid:107)vsi(cid:107)2,

where ¯f (H)12 = f (H)12 + f (H)(cid:62)
21;
(cid:17)
G12 ◦ f (H)12 + G(cid:62)
21 ◦ f (H)(cid:62)
G12 =
21

(cid:16)

(cid:16)

(cid:17)

f (H)12 + f (H)(cid:62)
21

/
is the weighted aver-
age of G12 and G(cid:62)
21, “◦” and “/” are element-
wise product and division, respectively. The
columns in V 2 are independent, thus for each
vsi, it is a separate weighted ridge regression
problem, whose solution is (Holland, 1973):
1 diag( ¯f i)¯gi,
1 diag( ¯f i)V 1+µiI)−1V (cid:62)
v∗
si=(V (cid:62)
where ¯f i and ¯gi are columns corresponding
to si in ¯f (H)12 and G12, respectively;
5. For any other set of noncore words Sk, ﬁnd
V ∗
k that minimizes the total penalty of the 1k-
th and k1-th blocks, ignoring all other kj-th
and jk-th blocks;

6. Combine all subsets of embeddings to form

V ∗. Here V ∗ = (V ∗

1, V ∗

2, V ∗
3).

6 Experimental Results

We trained our model along with a few state-of-
the-art competitors on Wikipedia, and evaluated
the embeddings on 7 common benchmark sets.

6.1 Experimental Setup

Our own method is referred to as PSD. The com-
petitors include:

• (Mikolov et al., 2013b): word2vec2, or

SGNS in some literature;

2https://code.google.com/p/word2vec/

• (Levy and Goldberg, 2014): the PPMI ma-
trix without dimension reduction, and SVD
of PPMI matrix, both yielded by hyperwords;

• (Pennington et al., 2014): GloVe3;
• (Stratos et al., 2015): Singular4, which does
SVD-based CCA on the weighted bigram fre-
quency matrix;

• (Faruqui et al., 2015): Sparse5, which learns
new sparse embeddings in a higher dimen-
sional space from pretrained embeddings.

All models were trained on the English Wikipedia
snapshot in March 2015. After removing non-
textual elements and non-English words, 2.04 bil-
lion words were left. We used the default hyperpa-
rameters in Hyperwords when training PPMI and
SVD. Word2vec, GloVe and Singular were trained
with their own default hyperparameters.

The embedding sets PSD-Reg-180K and PSD-
Unreg-180K were trained using our online block-
wise regression. Both sets contain the embed-
dings of the most frequent 180,000 words, based
on 25,000 core words. PSD-Unreg-180K was
traind with all µi = 0, i.e. disabling Tikhonov
regularization. PSD-Reg-180K was trained with

µi =


2

4

8

i ∈ [25001, 80000]
i ∈ [80001, 130000]
i ∈ [130001, 180000]

, i.e.

increased

regularization as the sparsity increases. To con-
trast with the batch learning performance, the per-
formance of PSD-25K is listed, which contains the
core embeddings only. PSD-25K took advantages
that it contains much less false candidate words,
and some test tuples (generally harder ones) were
not evaluated due to missing words, thus its scores
are not comparable to others.

The benchmark sets are almost

Sparse was trained with PSD-180K-reg as the
input embeddings, with default hyperparameters.
to
those in (Levy et al., 2015), except that (Luong et
al., 2013)’s Rare Words is not included, as many
rare words are cut off at the frequency 100, mak-
ing more than 1/3 of test pairs invalid.

identical

Word Similarity There are 5 datasets: Word-
Sim Similarity (WS Sim) and WordSim Related-
ness (WS Rel) (Zesch et al., 2008; Agirre et al.,
2009), partitioned from WordSim353 (Finkelstein
et al., 2002); Bruni et al. (2012)’s MEN dataset;

3http://nlp.stanford.edu/projects/glove/
4https://github.com/karlstratos/singular
5https://github.com/mfaruqui/sparse-coding

Table 2: Performance of each method across different tasks.

Similarity Tasks

Analogy Tasks

Method
word2vec
PPMI
SVD
GloVe
Singular
Sparse
PSD-Reg-180K
PSD-Unreg-180K
PSD-25K

WS Sim WS Rel MEN Turk
0.663
0.659
0.524
0.641
0.581
0.625
0.676
0.675
0.678

0.543
0.678
0.608
0.630
0.684
0.585
0.679
0.663
0.676

0.731
0.717
0.711
0.756
0.747
0.725
0.764
0.753
0.765

0.742
0.735
0.687
0.759
0.763
0.739
0.792
0.786
0.801

SimLex
0.395
0.308
0.270
0.362
0.345
0.355
0.398
0.372
0.393

Google
0.734 / 0.742
0.476 / 0.524
0.230 / 0.240
0.535 / 0.544
0.440 / 0.508
0.240 / 0.282
0.602 / 0.623
0.566 / 0.598
0.671 / 0.695

MSR
0.650 / 0.674
0.183 / 0.217
0.123 / 0.113
0.408 / 0.435
0.364 / 0.399
0.253 / 0.274
0.465 / 0.507
0.424 / 0.468
0.533 / 0.586

Radinsky et al. (2011)’s Mechanical Turk dataset;
and (Hill et al., 2014)’s SimLex-999 dataset. The
embeddings were evaluated by the Spearman’s
rank correlation with the human ratings.

Word Analogy The two datasets are MSR’s
analogy dataset (Mikolov et al., 2013c), contain-
ing 8000 questions, and Google’s analogy dataset
(Mikolov et al., 2013a), with 19544 questions. Af-
ter ﬁltering questions involving out-of-vocabulary
words, i.e. words that appear less than 100 times
in the corpus, 7054 instances in MSR and 19364
instances in Google were left. The analogy ques-
tions were answered using 3CosAdd as well as
3CosMul proposed by Levy et al. (2014).

6.2 Results

Table 2 shows the results on all tasks. Word2vec
signiﬁcantly outperformed other methods on anal-
ogy tasks. PPMI and SVD performed much worse
on analogy tasks than reported in (Levy et al.,
2015), probably due to sub-optimal hyperparam-
eters. This suggests their performance is unstable.
The new embeddings yielded by Sparse systemat-
ically degraded compared to the old embeddings,
contradicting the claim in (Faruqui et al., 2015).

Our method PSD-Reg-180K performed well
consistently, and is best in 4 similarity tasks.
It performed worse than word2vec on analogy
tasks, but still better than other MF-based meth-
ods. By comparing to PSD-Unreg-180K, we see
Tikhonov regularization brings 1-4% performance
boost across tasks. In addition, on similarity tasks,
online blockwise regression only degrades slightly
compared to batch factorization. Their perfor-
mance gaps on analogy tasks were wider, but this
might be explained by the fact that some hard
cases were not counted in PSD-25K’s evaluation,

due to its limited vocabulary.

7 Conclusions and Future Work

In this paper, inspired by the link functions in
previous works, with the support from Informa-
tion Theory, we propose a new link function of a
text window, parameterized by the embeddings of
words and the residuals of bigrams. Based on the
link function, we establish a generative model of
documents. The learning objective is to ﬁnd a set
of embeddings maximizing their posterior likeli-
hood given the corpus. This objective is reduced to
weighted low-rank positive-semideﬁnite approxi-
mation, subject to Tikhonov regularization. Then
we adopt a Block Coordinate Descent algorithm,
jointly with an online blockwise regression algo-
rithm to ﬁnd an approximate solution. On seven
benchmark sets,
the learned embeddings show
competitive and stable performance.

In the future work, we will incorporate global
latent factors into this generative model, such as
topics, sentiments, or writing styles, and develop
more elaborate models of documents. Through
learning such latent factors, important summary
information of documents would be acquired,
which are useful in various applications.

Acknowledgments

We thank Omer Levy, Thomas Mach, Peilin Zhao,
Mingkui Tan, Zhiqiang Xu and Chunlin Wu for
their helpful discussions and insights. This re-
search is supported by the National Research
Foundation, Prime Minister’s Ofﬁce, Singapore
under its IDM Futures Funding Initiative and ad-
ministered by the Interactive and Digital Media
Programme Ofﬁce.

Appendix A Possible Trap in SVD

Suppose M is the bigram matrix of interest. SVD
embeddings are derived from the low rank approx-
imation of M (cid:62)M , by keeping the largest singular
values/vectors. When some of these singular val-
ues correspond to negative eigenvalues, undesir-
able correlations might be captured. The follow-
ing is an example of approximating a PMI matrix.
A vocabulary consists of 3 words s1, s2, s3.

Two corpora derive two PMI matrices:
(cid:17)
M (1) =

, M (2) =

(cid:16) 1.4 0.8 0
0.8 2.6 0
0 2
0

(cid:16) 0.2 −1.6 0
−1.6 −2.2 0
2

0

0

(cid:17)

.

0

the largest

They have identical left singular matrix and sin-
gular values (3, 2, 1), but their eigenvalues are
(3, 2, 1) and (−3, 2, 1), respectively.
In a rank-2 approximation,

two
singular values/vectors are kept, and M (1) and
M (2) yield identical SVD embeddings V =
( 0.45 0.89 0
0 1 ) (the rows may be scaled depending on
the algorithm, without affecting the validity of the
following conclusion). The embeddings of s1 and
s2 (columns 1 and 2 of V ) point at the same di-
rection, suggesting they are positively correlated.
However as M (2)
2,1 = −1.6 < 0, they are
actually negatively correlated in the second cor-
pus. This inconsistency is because the principal
eigenvalue of M (2) is negative, and yet the corre-
sponding singular value/vector is kept.
When using eigendecomposition,

the largest
two positive eigenvalues/eigenvectors are kept.
M (1) yields the same embeddings V . M (2)
yields V (2) = (cid:0) −0.89 0.45 0
(cid:1) , which correctly
0 1.41
preserves the negative correlation between s1, s2.

1,2 = M (2)

0

Appendix B Information Theory

Redundant information refers to the reduced un-
certainty by knowing the value of any one of the
conditioning variables (hence redundant). Syner-
gistic information is the reduced uncertainty as-
cribed to knowing all the values of conditioning
variables, that cannot be reduced by knowing the
value of any variable alone (hence synergistic).

The mutual information I(y; xi) and the redun-

dant information Rdn(y; x1, x2) are deﬁned as:

I(y; xi) = EP (xi,y)[log

P (y|xi)
P (y)

]

(cid:20)
Rdn(y; x1, x2) = EP (y)

EP (xi|y)[log

min
x1,x2
The synergistic information Syn(y; x1, x2) is
deﬁned as the PI-function in (Williams and Beer,
2010), skipped here.

P (y|xi)
P (y)

(cid:21)

]

Figure 2: Different types of information among
3 random variables y, x1, x2.
I(y; x1, x2) is
the mutual information between y and (x1, x2).
Rdn(y; x1, x2) and Syn(y; x1, x2) are the redun-
dant information and synergistic information be-
tween x1, x2, conditioning y, respectively.

The interaction information Int(x1, x2, y) mea-
sures the relative strength of Rdn(y; x1, x2) and
Syn(y; x1, x2) (Timme et al., 2014):

Int(x1, x2, y)

=Syn(y; x1, x2) − Rdn(y; x1, x2)
=I(y; x1, x2) − I(y; x1) − I(y; x2)

=EP (x1,x2,y)[log

P (x1)P (x2)P (y)P (x1, x2, y)
P (x1, x2)P (x1, y)P (x2, y)

]

Figure 2 shows the relationship of different
information among 3 random variables y, x1, x2
(based on Fig.1 in (Williams and Beer, 2010)).

PMI is the pointwise counterpart of mutual
information I. Similarly, all the above concepts
have their pointwise counterparts, obtained by
dropping the expectation operator. Speciﬁcally,
the pointwise interaction information is deﬁned as
PInt(x1, x2, y) = PMI(y; x1, x2) − PMI(y; x1) −
log P (x1)P (x2)P (y)P (x1,x2,y)
PMI(y; x2)
P (x1,x2)P (x1,y)P (x2,y) .
If we know PInt(x1, x2, y), we can recover
PMI(y; x1, x2) from the mutual information over
the variable subsets, and then recover the joint
distribution P (x1, x2, y).

=

As

the

pointwise

redundant

interaction terms,

information
PRdn(y; x1, x2) and the pointwise synergistic
information PSyn(y; x1, x2) are both higher-
order
their magnitudes are
usually much smaller than the PMI terms. We
assume they are approximately equal, and thus
cancel each other when computing PInt. Given
this, PInt
In the case of three
words w0, w1, w2, PInt(w0, w1, w2) = 0 leads to
PMI(w2; w0, w1) = PMI(w2; w0)+PMI(w2; w1).

is always 0.

References

[Agirre et al.2009] Eneko Agirre, Enrique Alfonseca,
Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor
Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 19–27. Association for Computa-
tional Linguistics.

[Arora et al.2015] S. Arora, Y. Li, Y. Liang, T. Ma, and
A. Risteski. 2015. Random walks on discourse
spaces: a new generative language model with ap-
plications to semantic word embeddings. ArXiv e-
prints, arXiv:1502.03520 [cs.LG].

[Bengio et al.2006] Yoshua Bengio, Holger Schwenk,
Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-
Luc Gauvain. 2006. Neural probabilistic language
models. In Innovations in Machine Learning, pages
137–186. Springer.

[Blei et al.2003] David M Blei, Andrew Y Ng, and
Michael I Jordan. 2003. Latent dirichlet allocation.
The Journal of Machine Learning Research, 3:993–
1022.

[Bruni et al.2012] Elia Bruni, Gemma Boleda, Marco
Baroni, and Nam-Khanh Tran. 2012. Distributional
semantics in technicolor. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1, pages
136–145. Association for Computational Linguis-
tics.

[Deerwester et al.1990] Scott C. Deerwester, Susan T
Dumais, and Richard A. Harshman. 1990. Index-
ing by latent semantic analysis. J. Am. Soc. Inf. Sci.

[Dhillon et al.2011] Paramveer Dhillon, Dean P Foster,
and Lyle H Ungar. 2011. Multi-view learning of
In Proceedings of Ad-
word embeddings via cca.
vances in Neural Information Processing Systems,
pages 199–207.

[Dhillon et al.2015] Paramveer S Dhillon, Dean P Fos-
ter, and Lyle H Ungar. 2015. Eigenwords: Spectral
word embeddings. The Journal of Machine Learn-
ing Research.

[Faruqui et al.2015] Manaal Faruqui, Yulia Tsvetkov,
Dani Yogatama, Chris Dyer, and Noah A. Smith.
2015. Sparse overcomplete word vector represen-
tations. In Proceedings of ACL 2015.

[Hill et al.2014] Felix Hill, Roi Reichart, and Anna Ko-
rhonen. 2014. Simlex-999: Evaluating semantic
models with (genuine) similarity estimation. CoRR,
abs/1408.3456.

[Hinton2002] Geoffrey Hinton. 2002. Training prod-
ucts of experts by minimizing contrastive diver-
gence. Neural Computation, 14(8):1771–1800.

[Holland1973] Paul W. Holland.

1973. Weighted
Ridge Regression: Combining Ridge and Ro-
bust Regression Methods. NBER Working Papers
0011, National Bureau of Economic Research, Inc,
September.

[Hsu et al.2012] Daniel Hsu, Sham M Kakade, and
Tong Zhang. 2012. A spectral algorithm for learn-
ing hidden markov models. Journal of Computer
and System Sciences, 78(5):1460–1480.

[Levy and Goldberg2014] Omer Levy and Yoav Gold-
berg. 2014. Neural word embeddings as implicit
matrix factorization. In Proceedings of NIPS 2014.

[Levy et al.2014] Omer Levy, Yoav Goldberg, and Is-
rael Ramat-Gan. 2014. Linguistic regularities in
In Pro-
sparse and explicit word representations.
ceedings of CoNLL-2014, page 171.

[Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido
Dagan. 2015.
Improving distributional similarity
with lessons learned from word embeddings. Trans-
actions of the Association for Computational Lin-
guistics, 3:211–225.

[Lin and He2009] Chenghua Lin and Yulan He. 2009.
Joint sentiment/topic model for sentiment analysis.
In Proceedings of the 18th ACM conference on In-
formation and Knowledge Management, pages 375–
384. ACM.

[Luong et al.2013] Minh-Thang

Richard
Socher, and Christopher D Manning. 2013. Better
word representations with recursive neural networks
for morphology. CoNLL-2013, 104.

Luong,

[Mach2012] T. Mach. 2012. Eigenvalue Algorithms
for Symmetric Hierarchical Matrices. Dissertation,
Chemnitz University of Technology.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
mation of word representations in vector space. In
Proceedings of Workshop at ICLR 2013.

[Finkelstein et al.2002] Lev

Finkelstein,

Evgeniy
Gabrilovich, Yossi Matias, Ehud Rivlin, Zach
Solan, Gadi Wolfman, and Eytan Ruppin. 2002.
Placing search in context: The concept revisited.
ACM Trans. Inf. Syst., 20(1):116–131, January.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases
and their compositionality. In Proceedings of NIPS
2013, pages 3111–3119.

[Higham1988] Nicholas J. Higham. 1988. Comput-
ing a nearest symmetric positive semideﬁnite matrix.
Linear Algebra and its Applications, 103(0):103 –
118.

[Mikolov et al.2013c] Tomas Mikolov, Wen-tau Yih,
and Geoffrey Zweig. 2013c. Linguistic regularities
In Pro-
in continuous space word representations.
ceedings of HLT-NAACL 2013, pages 746–751.

[Zesch et al.2008] Torsten Zesch, Christof M¨uller, and
Iryna Gurevych. 2008. Using wiktionary for com-
In Proceedings of
puting semantic relatedness.
AAAI 2008, volume 8, pages 861–866.

[Zhai and Lafferty2004] Chengxiang Zhai and John
Lafferty. 2004. A study of smoothing methods
for language models applied to information retrieval.
ACM Transactions on Information Systems (TOIS),
22(2):179–214.

[Zhao et al.2011a] Peilin Zhao, Steven CH Hoi, and
Rong Jin. 2011a. Double updating online learn-
ing. The Journal of Machine Learning Research,
12:1587–1615.

[Zhao et al.2011b] Wayne Xin Zhao, Jing Jiang, Jian-
shu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and
Xiaoming Li. 2011b. Comparing twitter and tra-
ditional media using topic models. In Advances in
Information Retrieval (Proceedings of the 33rd An-
nual European Conference on Information Retrieval
Research), pages 338–349. Springer.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey
Hinton. 2007. Three new graphical models for sta-
In Proceedings of the
tistical language modelling.
24th International Conference on Machine learning,
pages 641–648. ACM.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. Proceedings
of the Empiricial Methods in Natural Language Pro-
cessing (EMNLP 2014), 12.

[Radinsky et al.2011] Kira

Eugene
Radinsky,
Agichtein,
and Shaul
Evgeniy Gabrilovich,
Markovitch. 2011. A word at a time: Computing
word relatedness using temporal semantic analysis.
In Proceedings of the 20th International Conference
on World Wide Web, WWW ’11, pages 337–346,
New York, NY, USA. ACM.

[Srebro et al.2003] Nathan Srebro, Tommi Jaakkola,
et al. 2003. Weighted low-rank approximations. In
Proceedings of ICML 2003, volume 3, pages 720–
727.

[Stratos et al.2014] Karl

Stratos, Do-kyum Kim,
Michael Collins, and Daniel Hsu. 2014. A spectral
algorithm for learning class-based n-gram models of
natural language. In Proceedings of the Association
for Uncertainty in Artiﬁcial Intelligence.

[Stratos et al.2015] Karl Stratos, Michael Collins, and
Daniel Hsu. 2015. Model-based word embeddings
from decompositions of count matrices. In Proceed-
ings of ACL 2015.

[Tan et al.2014] M. Tan, I. W. Tsang, L. Wang, B. Van-
dereycken, and S. J. Pan. 2014. Riemannian pursuit
In Proceedings of ICML
for big matrix recovery.
2014, pages 1539–1547.

[Timme et al.2014] Nicholas Timme, Wesley Alford,
Benjamin Flecker, and John M Beggs. 2014. Syn-
ergy, redundancy, and multivariate information mea-
sures: an experimentalist’s perspective. Journal of
Computational Neuroscience, 36(2):119–140.

[Wallach2006] Hanna M Wallach. 2006. Topic mod-
eling: beyond bag-of-words. In Proceedings of the
23rd international conference on Machine learning,
pages 977–984. ACM.

[Williams and Beer2010] Paul L Williams and Ran-
2010. Nonnegative decomposi-
arXiv preprint

dall D Beer.
tion of multivariate information.
arXiv:1004.2515.

[Yan et al.2015] Yan Yan, Mingkui Tan, Ivor Tsang,
Yi Yang, Chengqi Zhang, and Qinfeng Shi. 2015.
Scalable maximum margin matrix factorization by
active riemannian subspace search. In Proceedings
of IJCAI 2015.

[Yogatama et al.2015] Dani

Manaal
Faruqui, Chris Dyer, and Noah A Smith.
2015.
Learning word representations with hierarchical
sparse coding. In Proceedings of ICML 2015.

Yogatama,

A Generative Word Embedding Model and its Low Rank Positive
Semideﬁnite Solution

Shaohua Li1, Jun Zhu2, Chunyan Miao1
1Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY),
Nanyang Technological University, Singapore
2Tsinghua University, P.R. China
lish0018@ntu.edu.sg, dcszj@tsinghua.edu.cn, ascymiao@ntu.edu.sg

Abstract

Most existing word embedding methods
can be categorized into Neural Embedding
Models and Matrix Factorization (MF)-
based methods. However some mod-
els are opaque to probabilistic interpre-
tation, and MF-based methods, typically
solved using Singular Value Decomposi-
tion (SVD), may incur loss of corpus in-
formation.
In addition, it is desirable to
incorporate global latent factors, such as
topics, sentiments or writing styles, into
the word embedding model. Since gen-
erative models provide a principled way
to incorporate latent factors, we propose a
generative word embedding model, which
is easy to interpret, and can serve as a
basis of more sophisticated latent factor
models. The model inference reduces to
a low rank weighted positive semideﬁnite
approximation problem.
Its optimization
is approached by eigendecomposition on a
submatrix, followed by online blockwise
regression, which is scalable and avoids
the information loss in SVD. In experi-
ments on 7 common benchmark datasets,
our vectors are competitive to word2vec,
and better than other MF-based methods.

1

Introduction

The task of word embedding is to model the distri-
bution of a word and its context words using their
corresponding vectors in a Euclidean space. Then
by doing regression on the relevant statistics de-
rived from a corpus, a set of vectors are recovered
which best ﬁt these statistics. These vectors, com-
monly referred to as the embeddings, capture se-
mantic/syntactic regularities between the words.

The core of a word embedding method is the
link function that connects the input — the embed-
dings, with the output — certain corpus statistics.

Based on the link function, the objective function
is developed. The reasonableness of the link func-
tion impacts the quality of the obtained embed-
dings, and different link functions are amenable
to different optimization algorithms, with different
scalability. Based on the forms of the link func-
tion and the optimization techniques, most meth-
ods can be divided into two classes: the traditional
neural embedding models, and more recent low
rank matrix factorization methods.

The neural embedding models use the softmax
link function to model the conditional distribution
of a word given its context (or vice versa) as a
function of the embeddings. The normalizer in the
softmax function brings intricacy to the optimiza-
tion, which is usually tackled by gradient-based
methods. The pioneering work was (Bengio et
al., 2006). Later Mnih and Hinton (2007) propose
three different link functions. However there are
interaction matrices between the embeddings in all
these models, which complicate and slow down
the training, hindering them from being trained
on huge corpora. Mikolov et al. (2013a) and
Mikolov et al. (2013b) greatly simplify the condi-
tional distribution, where the two embeddings in-
teract directly. They implemented the well-known
“word2vec”, which can be trained efﬁciently on
huge corpora. The obtained embeddings show ex-
cellent performance on various tasks.

Low-Rank Matrix Factorization (MF in short)
methods include various link functions and opti-
mization methods. The link functions are usu-
ally not softmax functions. MF methods aim to
reconstruct certain corpus statistics matrix by the
product of two low rank factor matrices. The ob-
jective is usually to minimize the reconstruction
error, optionally with other constraints.
In this
line of research, Levy and Goldberg (2014) ﬁnd
that “word2vec” is essentially doing stochastic
weighted factorization of the word-context point-
wise mutual information (PMI) matrix. They then

5
1
0
2
 
g
u
A
 
6
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
2
8
3
0
.
8
0
5
1
:
v
i
X
r
a

factorize this matrix directly as a new method.
Pennington et al. (2014) propose a bilinear regres-
sion function of the conditional distribution, from
which a weighted MF problem on the bigram log-
frequency matrix is formulated. Gradient Descent
is used to ﬁnd the embeddings. Recently, based
on the intuition that words can be organized in se-
mantic hierarchies, Yogatama et al. (2015) add hi-
erarchical sparse regularizers to the matrix recon-
struction error. With similar techniques, Faruqui
et al. (2015) reconstruct a set of pretrained embed-
dings using sparse vectors of greater dimensional-
ity. Dhillon et al. (2015) apply Canonical Corre-
lation Analysis (CCA) to the word matrix and the
context matrix, and use the canonical correlation
vectors between the two matrices as word embed-
dings. Stratos et al. (2014) and Stratos et al. (2015)
assume a Brown language model, and prove that
doing CCA on the bigram occurrences is equiva-
lent to ﬁnding a transformed solution of the lan-
guage model. Arora et al. (2015) assume there is a
hidden discourse vector on a random walk, which
determines the distribution of the current word.
The slowly evolving discourse vector puts a con-
straint on the embeddings in a small text window.
The maximum likelihood estimate of the embed-
dings within this text window approximately re-
duces to a squared norm objective.

There are two limitations in current word em-
bedding methods. The ﬁrst limitation is, all MF-
based methods map words and their context words
to two different sets of embeddings, and then em-
ploy Singular Value Decomposition (SVD) to ob-
tain a low rank approximation of the word-context
matrix M . As SVD factorizes M (cid:62)M , some in-
formation in M is lost, and the learned embed-
dings may not capture the most signiﬁcant regu-
larities in M . Appendix A gives a toy example on
which SVD does not work properly.

The second limitation is, a generative model for
documents parametered by embeddings is absent
in recent development. Although (Stratos et al.,
2014; Stratos et al., 2015; Arora et al., 2015) are
based on generative processes, the generative pro-
cesses are only for deriving the local relationship
between embeddings within a small text window,
leaving the likelihood of a document undeﬁned.
In addition, the learning objectives of some mod-
els, e.g. (Mikolov et al., 2013b, Eq.1), even have
no clear probabilistic interpretation. A genera-
tive word embedding model for documents is not

only easier to interpret and analyze, but more im-
portantly, provides a basis upon which document-
level global latent factors, such as document topics
(Wallach, 2006), sentiments (Lin and He, 2009),
writing styles (Zhao et al., 2011b), can be incor-
porated in a principled manner, to better model the
text distribution and extract relevant information.
Based on the above considerations, we pro-
pose to unify the embeddings of words and con-
text words. Our link function factorizes into three
parts: the interaction of two embeddings capturing
linear correlations of two words, a residual captur-
ing nonlinear or noisy correlations, and the uni-
gram priors. To reduce overﬁtting, we put Gaus-
sian priors on embeddings and residuals, and ap-
ply Jelinek-Mercer Smoothing to bigrams. Fur-
thermore, to model the probability of a sequence
of words, we assume that the contributions of
more than one context word approximately add up.
Thereby a generative model of documents is con-
structed, parameterized by embeddings and resid-
uals. The learning objective is to maximize the
corpus likelihood, which reduces to a weighted
low-rank positive semideﬁnite (PSD) approxima-
tion problem of the PMI matrix. A Block Co-
ordinate Descent algorithm is adopted to ﬁnd an
approximate solution. This algorithm is based
on Eigendecomposition, which avoids information
loss in SVD, but brings challenges to scalability.
We then exploit the sparsity of the weight matrix
and implement an efﬁcient online blockwise re-
gression algorithm. On seven benchmark datasets
covering similarity and analogy tasks, our method
achieves competitive and stable performance.

The source code of this method is provided at

https://github.com/askerlee/topicvec.

2 Notations and Deﬁnitions

Throughout the paper, we always use a uppercase
bold letter as S, V to denote a matrix or set, a low-
ercase bold letter as vwi to denote a vector, a nor-
mal uppercase letter as N, W to denote a scalar
constant, and a normal lowercase letter as si, wi to
denote a scalar variable.

Suppose a vocabulary S = {s1, · · · , sW } con-
sists of all the words, where W is the vocab-
ulary size. We further suppose s1, · · · , sW are
sorted in decending order of the frequency, i.e.
s1 is most frequent, and sW is least frequent.
A document di
is a sequence of words di =
(wi1, · · · , wiLi), wij ∈ S. A corpus is a collec-

Name
S

V

D
vsi
asisj
˜P (si,sj)
u

A

B

G

H

Description
Vocabulary {s1, · · · , sW }
Embedding matrix (vs1 , · · · , vsW )
Corpus {d1, · · · , dM }
Embedding of word si
Bigram residual for si, sj
Empirical probability of si, sj in the corpus
Unigram probability vector (P (s1),· · ·, P (sW ))
Residual matrix (asisj )
(cid:16)

(cid:17)

Conditional probability matrix

PMI matrix

PMI(si, sj)

(cid:16)

P (sj|si)
(cid:17)
(cid:16) ˜P (si, sj)
(cid:17)

Bigram empirical probability matrix

Table 1: Notation Table

tion of M documents D = {d1, · · · , dM }. In the
vocabulary, each word si is mapped to a vector vsi
in N -dimensional Euclidean space.

In a document, a sequence of words is referred
to as a text window, denoted by wi, · · · , wi+l, or
wi:wi+l in shorthand. A text window of chosen
size c before a word wi deﬁnes the context of wi
as wi−c, · · · , wi−1. Here wi is referred to as the
focus word. Each context word wi−j and the focus
word wi comprise a bigram wi−j, wi.

The Pointwise Mutual Information between two

words si, sj is deﬁned as

PMI(si, sj) = log

P (si, sj)
P (si)P (sj)

.

3 Link Function of Text

In this section, we formulate the probability of a
sequence of words as a function of their embed-
dings. We start from the link function of bigrams,
which is the building blocks of a long sequence.
Then this link function is extended to a text win-
dow with c context words, as a ﬁrst-order approx-
imation of the actual probability.

3.1 Link Function of Bigrams

We generalize the link function of “word2vec” and
“GloVe” to the following:

(cid:110)

(cid:111)

P (si, sj) = exp

P (si)P (sj) (1)

v(cid:62)
sj vsi + asisj
The rationale for (1) originates from the idea of
the Product of Experts in (Hinton, 2002). Sup-
pose different types of semantic/syntactic regu-
larities between si and sj are encoded in differ-
sj vsi} =
ent dimensions of vsi, vsj . As exp{v(cid:62)
(cid:81)
l exp{vsi,l · vsj ,l}, this means the effects of dif-
ferent regularities on the probability are combined

by multiplying together. If si and sj are indepen-
dent, their joint probability should be P (si)P (sj).
In the presence of correlations, the actual joint
probability P (si, sj) would be a scaling of it. The
scale factor reﬂects how much si and sj are pos-
itively or negatively correlated. Within the scale
factor, v(cid:62)
sj vsi captures linear interactions between
si and sj, the residual asisj captures nonlinear or
sj vsi is
noisy interactions. In applications, only v(cid:62)
of interest. Hence the bigger magnitude v(cid:62)
sj vsi is
of relative to asisj , the better.

Note that we do not assume asisj = asj si.
This provides the ﬂexibility P (si, sj) (cid:54)= P (sj, si),
agreeing with the asymmetry of bigrams in natu-
ral languages. At the same time, v(cid:62)
sj vsi imposes a
symmetric part between P (si, sj) and P (sj, si).

(1) is equivalent to

(cid:110)

P (sj|si)=exp

v(cid:62)
sj vsi + asisj + log P (sj)

, (2)

(cid:111)

log

P (sj|si)
P (sj)

= v(cid:62)

sj vsi + asisj .

(3) of all bigrams is represented in matrix form:

(3)

(4)

V (cid:62)V + A = G,

where G is the PMI matrix.

3.1.1 Gaussian Priors on Embeddings
When (1) is employed on the regression of empir-
ical bigram probabilities, a practical issue arises:
more and more bigrams have zero frequency as
the constituting words become less frequent. A
zero-frequency bigram does not necessarily imply
negative correlation between the two constituting
words; it could simply result from missing data.
But in this case, even after smoothing, (1) will
force v(cid:62)
sj vsi + asisj to be a big negative number,
making vsi overly long. The increased magnitude
of embeddings is a sign of overﬁtting.

To reduce overﬁtting of embeddings of infre-
quent words, we assign a Spherical Gaussian prior
N (0, 1
2µi

I) to vsi:
P (vsi) ∼ exp{−µi(cid:107)vsi(cid:107)2},
where the hyperparameter µi increases as the fre-
quency of si decreases.

3.1.2 Gaussian Priors on Residuals
We wish v(cid:62)
sj vsi in (1) captures as much corre-
lations between si and sj as possible. Thus the
smaller asisj is, the better. In addition, the more
frequent si, sj is in the corpus,
the less noise
there is in their empirical distribution, and thus the
residual asisj should be more heavily penalized.

To this end, we penalize the residual asisj
by f (˜P (si, sj))a2
sisj , where f (·) is a nonnega-
tive monotonic transformation, referred to as the
weighting function. Let hij denote ˜P (si, sj), then
the total penalty of all residuals are the square of
the weighted Frobenius norm of A:
sisj = (cid:107)A(cid:107)2

f (hij)a2

f (H).

(cid:88)

(5)

si,sj ∈S

By referring to “GloVe”, we use the following

√

weighting function, and ﬁnd it performs well:
(cid:112)hij < Ccut, i (cid:54)= j
(cid:112)hij ≥ Ccut, i (cid:54)= j
i = j


hij

Ccut
1

0

f (hij) =

,

where Ccut is chosen to cut the most frequent
0.02% of the bigrams off at 1. When si = sj, two
identical words usually have much smaller proba-
bility to collocate. Hence ˜P (si, si) does not reﬂect
the true correlation of a word to itself, and should
not put constraints to the embeddings. We elimi-
nate their effects by setting f (hii) to 0.

(cid:16)

If the domain of A is the whole space RW ×W ,
then this penalty is equivalent to a Gaussian prior
N
on each asisj . The variances of the
Gaussians are determined by the bigram empirical
probability matrix H.

1
2f (hij )

0,

(cid:17)

Jelinek-Mercer Smoothing of Bigrams

3.1.3
As another measure to reduce the impact of miss-
ing data, we apply the commonly used Jelinek-
Mercer Smoothing (Zhai and Lafferty, 2004)
to smooth the empirical conditional probability
˜P (sj|si) by the unigram probability ˜P (sj) as:
˜Psmoothed(sj|si) = (1−κ) ˜P (sj|si)+κP (sj). (6)
the smoothed bigram empirical
Accordingly,

joint probability is deﬁned as
˜P (si, sj) = (1−κ) ˜P (si, sj)+κP (si)P (sj). (7)
In practice, we ﬁnd κ = 0.02 yields good re-
sults. When κ ≥ 0.04, the obtained embeddings
begin to degrade with κ, indicating that smoothing
distorts the true bigram distributions.

3.2 Link Function of a Text Window

In the previous subsection, a regression link func-
tion of bigram probabilities is established.
In
this section, we adopt a ﬁrst-order approximation
based on Information Theory, and extend the link
function to a longer sequence w0, · · · , wc−1, wc.

Decomposing a distribution conditioned on n
random variables as the conditional distributions

on its subsets roots deeply in Information The-
ory. This is an intricate problem because there
could be both (pointwise) redundant information
and (pointwise) synergistic information among the
conditioning variables (Williams and Beer, 2010).
They are both functions of the PMI. Based on an
analysis of the complementing roles of these two
types of pointwise information, we assume they
are approximately equal and cancel each other
when computing the pointwise interaction infor-
mation. See Appendix B for a detailed discussion.
Following the above assumption, we have
PMI(w2; w0, w1) ≈ PMI(w2; w0)+PMI(w2; w1):

log

P (w0, w1|w2)
P (w0, w1)

≈log

P (w0|w2)
P (w0)

+log

P (w1|w2)
P (w1)

.

Plugging (1) and (3) into the above, we obtain
P (w0, w1, w2)
(cid:26) 2

≈ exp

(v(cid:62)

wivwj + awiwj ) +

log P (wi)

(cid:27)
.

(cid:88)

i,j=0
i(cid:54)=j

We extend the above assumption to that the
pointwise interaction information is still close to
0 within a longer text window. Accordingly the
above equation extends to a context of size c > 2:

P (w0, · · · , wc)
(cid:26) c

(cid:88)

≈ exp

i,j=0
i(cid:54)=j

(v(cid:62)

wivwj + awiwj ) +

log P (wi)

(cid:27)
.

2
(cid:88)

i=0

c
(cid:88)

i=0

From it derives the conditional distribution of

wc, given its context w0, · · · , wc−1:

P (wc | w0 : wc−1)=

(cid:26)

≈P (wc) exp

v(cid:62)
wc

P (w0, · · · , wc)
P (w0, · · · , wc−1)
c−1
(cid:27)
(cid:88)
.

awiwc

vwi +

i=0

c−1
(cid:88)

i=0

(8)

4 Generative Process and Likelihood

We proceed to assume the text is generated from a
Markov chain of order c, i.e., a word only depends
on words within its context of size c. Given the
hyperparameter µ = (µ1, · · ·, µW ), the generative
process of the whole corpus is:

1. For each word si, draw the embedding vsi

from N (0, 1
2µi

I);

(cid:17)

(cid:16)

0,

asisj from N

2. For each bigram si, sj, draw the residual
1
2f (hij )
3. For each document di, for the j-th word,
from S with probability

draw word wij
P (wij | wi,j−c : wi,j−1) deﬁned by (8).

;

5 Learning Algorithm

5.1 Learning Objective

The learning objective is to ﬁnd the embeddings
V that maximize the corpus log-likelihood (9).

Let xij denote the (smoothed) frequency of bi-

gram si, sj in the corpus. Then (9) is sorted as:

log p(D, V , A)

µi

vsi

V

hij

aij

A

d

vw0

vw1

· · ·

vwc

=C0 − log Z(H, µ) − (cid:107)A(cid:107)2

f (H) −

µi(cid:107)vsi(cid:107)2

W
(cid:88)

i=1

Figure 1: The Graphical Model of PSDVec

The above generative process for a document d is
presented as a graphical model in Figure 1.

Based on this generative process, the probabil-
ity of a document di can be derived as follows,
given the embeddings and residuals V , A:

P (di|V , A)
Li(cid:89)

(cid:26)

j−1
(cid:88)

j−1
(cid:88)

=

P (wij) exp

v(cid:62)

wij

vwik +

awikwij

j=1

k=j−c

k=j−c

The complete-data likelihood of the corpus is:

p(D, V , A)

=

N (0,

W
(cid:89)

i=1

I
2µi

)

W,W
(cid:89)

(cid:18)

N

0,

i,j=1

1
2f (hij)

(cid:19) M
(cid:89)

p(di|V, A)

=

1
Z(H, µ)

(cid:110)

exp

−

W,W
(cid:88)

f (hi,j)a2

sisj −

µi(cid:107)vsi(cid:107)2(cid:111)

i,j=1

(cid:26)

j−1
(cid:88)

P (wij) exp

v(cid:62)

wij

vwik +

awikwij

M,Li
(cid:89)
·
i,j=1

+

W,W
(cid:88)

i,j=1

xij(v(cid:62)

sivsj + asisj ).

(10)

As
the
(cid:80)W,W
i,j=1 xij(v(cid:62)

corpus
sivsj +asisj ) will

increases,
the
parameter prior terms. Then we can ignore the
prior terms when maximizing (10).

dominate

size

max
(cid:16)(cid:88)

(cid:88)

xij

xij(v(cid:62)
(cid:17)

· max

=

sivsj +asisj )

(cid:88) ˜Psmoothed(si, sj) log P (si, sj).

As both { ˜Psmoothed(si, sj)} and {P (si, sj)}
the above sum is maximized when

sum to 1,
P (si, sj) = ˜Psmoothed(si, sj).

(cid:27)
.

The maximum likelihood estimator is then:
P (sj|si) = ˜Psmoothed(sj|si),

v(cid:62)
sivsj + asisj = log

˜Psmoothed(sj|si)
P (sj)

.

(11)

B∗ =

Writing (11) in matrix form:
(cid:17)
(cid:16) ˜Psmoothed(sj|si)
G∗ = log B∗ − log u ⊗ (1 · · · 1),

si,sj ∈S

(12)

(cid:27)

,

where “⊗” is the outer product.
Now we ﬁx the values of v(cid:62)

sivsj + asisj at the

k=j−c

k=j−c

above optimal. The corpus likelihood becomes

where Z(H, µ) is the normalizing constant.

logarithm of both sides of

log p(D, V , A) =C1 − (cid:107)A(cid:107)2

f (H) −

µi(cid:107)vsi(cid:107)2,

W
(cid:88)

i=1

Taking the
p(D, A, V ) yields

log p(D, V , A)

=C0 − log Z(H, µ) − (cid:107)A(cid:107)2

f (H)−

µi(cid:107)vsi(cid:107)2

M,Li
(cid:88)

(cid:26)

+

v(cid:62)

wij

j−1
(cid:88)

j−1
(cid:88)

(cid:27)

vwik +

awikwij

,

(9)

i,j=1

k=j−c

k=j−c

where C0 = (cid:80)M,Li

i,j=1 log P (wij) is constant.

subject to V (cid:62)V + A = G∗,

(13)
where C1 = C0 + (cid:80) xij log ˜Psmoothed(si, sj) −
log Z(H, µ) is constant.

5.2 Learning V as Low Rank PSD

Approximation

Once G∗ has been estimated from the corpus using
(12), we seek V that maximizes (13). This is to
ﬁnd the maximum a posteriori (MAP) estimates
of V , A that satisfy V (cid:62)V + A = G∗. Applying
this constraint to (13), we obtain

i=1

W
(cid:88)

i=1

j−1
(cid:88)

W
(cid:88)

i=1

Algorithm 1 BCD algorithm for ﬁnding a unreg-
ularized rank-N weighted PSD approximant.
Input: matrix G∗, weight matrix W = f (H),
iteration number T , rank N
Randomly initialize X (0)
for t = 1, · · · , T do

Gt = W ◦ G∗ + (1 − W ) ◦ X (t−1)
X (t) = PSD Approximate(Gt, N )

end for
λ, Q = Eigen Decomposition(X (T ))
V ∗ = diag(λ
Output: V ∗

1
2 [1:N ]) · Q(cid:62)[1:N ]

arg max
V

log p(D, V , A)

= arg min
V

(cid:107)G∗−V (cid:62)V (cid:107)f (H) +

µi(cid:107)vsi(cid:107)2. (14)

W
(cid:88)

i=1

Let X = V (cid:62)V . Then X is positive semidef-
inite of rank N . Finding V that minimizes (14)
is equivalent to ﬁnding a rank-N weighted posi-
tive semideﬁnite approximant X of G∗, subject to
Tikhonov regularization. This problem does not
admit an analytic solution, and can only be solved
using local optimization methods.

First we consider a simpler case where all the
words in the vocabulary are enough frequent, and
thus Tikhonov regularization is unnecessary.
In
this case, we set ∀µi = 0, and (14) becomes an
unregularized optimization problem. We adopt the
Block Coordinate Descent (BCD) algorithm1 in
(Srebro et al., 2003) to approach this problem. The
original algorithm is to ﬁnd a generic rank-N ma-
trix for a weighted approximation problem, and
we tailor it by constraining the matrix within the
positive semideﬁnite manifold.

We summarize our learning algorithm in Al-
gorithm 1. Here “◦” is the entry-wise prod-
uct. We suppose the eigenvalues λ returned by
Eigen Decomposition(X) are in descending or-
der. Q(cid:62)[1:N ] extracts the 1 to N rows from Q(cid:62).
One key issue is how to initialize X. Srebro et
al. (2003) suggest to set X (0) =G∗, and point out
that X (0) = 0 is far from a local optimum, thus
requires more iterations. However we ﬁnd G∗ is
also far from a local optimum, and this setting con-
verges slowly too. Setting X (0) = G∗/2 usually

1It is referred to as an Expectation-Maximization algo-
rithm by the original authors, but we think this is a misnomer.

yields a satisfactory solution in a few iterations.

The subroutine PSD Approximate() computes
the unweighted nearest rank-N PSD approxima-
tion, measured in F-norm (Higham, 1988).

5.3 Online Blockwise Regression of V

the

does

essential

subroutine
In Algorithm 1,
PSD Approximate()
eigendecomposi-
tion on Gt, which is dense due to the logarithm
transformation. Eigendecomposition on a W × W
dense matrix requires O(W 2) space and O(W 3)
time, difﬁcult to scale up to a large vocabulary. In
addition, the majority of words in the vocabulary
are infrequent, and Tikhonov regularization is
necessary for them.

It is observed that, as words become less fre-
quent, fewer and fewer words appear around them
to form bigrams. Remind that the vocabulary
S = {s1, · · · , sW } are sorted in decending or-
der of the frequency, hence the lower-right blocks
of H and f (H) are very sparse, and cause these
blocks in (14) to contribute much less penalty rela-
tive to other regions. Therefore these blocks could
be ignored when doing regression, without sacri-
ﬁcing too much accuracy. This intuition leads to
the following online blockwise regression.

The basic idea is to select a small set (e.g.
30,000) of the most frequent words as the core
words, and partition the remaining noncore words
into sets of moderate sizes. Bigrams consist-
ing of two core words are referred to as core bi-
grams, which correspond to the top-left blocks of
G and f (H). The embeddings of core words
are learned approximately using Algorithm 1, on
the top-left blocks of G and f (H). Then we ﬁx
the embeddings of core words, and ﬁnd the em-
beddings of each set of noncore words in turn.
After ignoring the lower-right regions of G and
f (H) which correspond to bigrams of two non-
core words, the quadratic terms of noncore em-
beddings are ignored. Consequently, ﬁnding these
embeddings becomes a weighted ridge regression
problem, which can be solved efﬁciently in closed-
form. Finally we combine all embeddings to get
the embeddings of the whole vocabulary. The de-
tails are as follows:

1. Partition S into K consecutive groups
S1, · · · , Sk. Take K = 3 as an example.
The ﬁrst group is core words;

2. Accordingly partition G into K × K blocks,

in this example as





G11 G12 G13
G21 G22 G23
G31 G32 G33



 .

Partition
same way.
G11, f (H)11, A11 correspond to core bi-

f (H),A in

the

grams. Partition V into

(cid:0)

V 1
(cid:124)(cid:123)(cid:122)(cid:125)
S1
1 V 1 + A11 = G11 using Algorithm

V 3
(cid:124) (cid:123)(cid:122) (cid:125)
S3

V 2
(cid:124) (cid:123)(cid:122) (cid:125)
S2

;

(cid:1)

3. Solve V (cid:62)

1, and obtain core embeddings V ∗
1;

4. Set V 1 = V ∗

1, and ﬁnd V ∗
2 that minimizes
the total penalty of the 12-th and 21-th blocks
of residuals (the 22-th block is ignored due to
its high sparsity):

(cid:107)G12 − V (cid:62)

1 V 2(cid:107)2

f (H)12

arg min
V 2
+ (cid:107)G21 − V (cid:62)

2 V 1(cid:107)2

f (H)21

+

(cid:88)

µi(cid:107)vsi(cid:107)2

si∈S2
(cid:88)
+

si∈S2

= arg min
V 2

(cid:107)G12−V (cid:62)

1 V 2(cid:107)2

¯f (H)12

µi(cid:107)vsi(cid:107)2,

where ¯f (H)12 = f (H)12 + f (H)(cid:62)
21;
(cid:17)
G12 ◦ f (H)12 + G(cid:62)
21 ◦ f (H)(cid:62)
G12 =
21

(cid:16)

(cid:16)

(cid:17)

f (H)12 + f (H)(cid:62)
21

/
is the weighted aver-
age of G12 and G(cid:62)
21, “◦” and “/” are element-
wise product and division, respectively. The
columns in V 2 are independent, thus for each
vsi, it is a separate weighted ridge regression
problem, whose solution is (Holland, 1973):
1 diag( ¯f i)¯gi,
1 diag( ¯f i)V 1+µiI)−1V (cid:62)
v∗
si=(V (cid:62)
where ¯f i and ¯gi are columns corresponding
to si in ¯f (H)12 and G12, respectively;
5. For any other set of noncore words Sk, ﬁnd
V ∗
k that minimizes the total penalty of the 1k-
th and k1-th blocks, ignoring all other kj-th
and jk-th blocks;

6. Combine all subsets of embeddings to form

V ∗. Here V ∗ = (V ∗

1, V ∗

2, V ∗
3).

6 Experimental Results

We trained our model along with a few state-of-
the-art competitors on Wikipedia, and evaluated
the embeddings on 7 common benchmark sets.

6.1 Experimental Setup

Our own method is referred to as PSD. The com-
petitors include:

• (Mikolov et al., 2013b): word2vec2, or

SGNS in some literature;

2https://code.google.com/p/word2vec/

• (Levy and Goldberg, 2014): the PPMI ma-
trix without dimension reduction, and SVD
of PPMI matrix, both yielded by hyperwords;

• (Pennington et al., 2014): GloVe3;
• (Stratos et al., 2015): Singular4, which does
SVD-based CCA on the weighted bigram fre-
quency matrix;

• (Faruqui et al., 2015): Sparse5, which learns
new sparse embeddings in a higher dimen-
sional space from pretrained embeddings.

All models were trained on the English Wikipedia
snapshot in March 2015. After removing non-
textual elements and non-English words, 2.04 bil-
lion words were left. We used the default hyperpa-
rameters in Hyperwords when training PPMI and
SVD. Word2vec, GloVe and Singular were trained
with their own default hyperparameters.

The embedding sets PSD-Reg-180K and PSD-
Unreg-180K were trained using our online block-
wise regression. Both sets contain the embed-
dings of the most frequent 180,000 words, based
on 25,000 core words. PSD-Unreg-180K was
traind with all µi = 0, i.e. disabling Tikhonov
regularization. PSD-Reg-180K was trained with

µi =


2

4

8

i ∈ [25001, 80000]
i ∈ [80001, 130000]
i ∈ [130001, 180000]

, i.e.

increased

regularization as the sparsity increases. To con-
trast with the batch learning performance, the per-
formance of PSD-25K is listed, which contains the
core embeddings only. PSD-25K took advantages
that it contains much less false candidate words,
and some test tuples (generally harder ones) were
not evaluated due to missing words, thus its scores
are not comparable to others.

The benchmark sets are almost

Sparse was trained with PSD-180K-reg as the
input embeddings, with default hyperparameters.
to
those in (Levy et al., 2015), except that (Luong et
al., 2013)’s Rare Words is not included, as many
rare words are cut off at the frequency 100, mak-
ing more than 1/3 of test pairs invalid.

identical

Word Similarity There are 5 datasets: Word-
Sim Similarity (WS Sim) and WordSim Related-
ness (WS Rel) (Zesch et al., 2008; Agirre et al.,
2009), partitioned from WordSim353 (Finkelstein
et al., 2002); Bruni et al. (2012)’s MEN dataset;

3http://nlp.stanford.edu/projects/glove/
4https://github.com/karlstratos/singular
5https://github.com/mfaruqui/sparse-coding

Table 2: Performance of each method across different tasks.

Similarity Tasks

Analogy Tasks

Method
word2vec
PPMI
SVD
GloVe
Singular
Sparse
PSD-Reg-180K
PSD-Unreg-180K
PSD-25K

WS Sim WS Rel MEN Turk
0.663
0.659
0.524
0.641
0.581
0.625
0.676
0.675
0.678

0.543
0.678
0.608
0.630
0.684
0.585
0.679
0.663
0.676

0.742
0.735
0.687
0.759
0.763
0.739
0.792
0.786
0.801

0.731
0.717
0.711
0.756
0.747
0.725
0.764
0.753
0.765

SimLex
0.395
0.308
0.270
0.362
0.345
0.355
0.398
0.372
0.393

Google
0.734 / 0.742
0.476 / 0.524
0.230 / 0.240
0.535 / 0.544
0.440 / 0.508
0.240 / 0.282
0.602 / 0.623
0.566 / 0.598
0.671 / 0.695

MSR
0.650 / 0.674
0.183 / 0.217
0.123 / 0.113
0.408 / 0.435
0.364 / 0.399
0.253 / 0.274
0.465 / 0.507
0.424 / 0.468
0.533 / 0.586

Radinsky et al. (2011)’s Mechanical Turk dataset;
and (Hill et al., 2014)’s SimLex-999 dataset. The
embeddings were evaluated by the Spearman’s
rank correlation with the human ratings.

Word Analogy The two datasets are MSR’s
analogy dataset (Mikolov et al., 2013c), contain-
ing 8000 questions, and Google’s analogy dataset
(Mikolov et al., 2013a), with 19544 questions. Af-
ter ﬁltering questions involving out-of-vocabulary
words, i.e. words that appear less than 100 times
in the corpus, 7054 instances in MSR and 19364
instances in Google were left. The analogy ques-
tions were answered using 3CosAdd as well as
3CosMul proposed by Levy et al. (2014).

6.2 Results

Table 2 shows the results on all tasks. Word2vec
signiﬁcantly outperformed other methods on anal-
ogy tasks. PPMI and SVD performed much worse
on analogy tasks than reported in (Levy et al.,
2015), probably due to sub-optimal hyperparam-
eters. This suggests their performance is unstable.
The new embeddings yielded by Sparse systemat-
ically degraded compared to the old embeddings,
contradicting the claim in (Faruqui et al., 2015).

Our method PSD-Reg-180K performed well
consistently, and is best in 4 similarity tasks.
It performed worse than word2vec on analogy
tasks, but still better than other MF-based meth-
ods. By comparing to PSD-Unreg-180K, we see
Tikhonov regularization brings 1-4% performance
boost across tasks. In addition, on similarity tasks,
online blockwise regression only degrades slightly
compared to batch factorization. Their perfor-
mance gaps on analogy tasks were wider, but this
might be explained by the fact that some hard
cases were not counted in PSD-25K’s evaluation,

due to its limited vocabulary.

7 Conclusions and Future Work

In this paper, inspired by the link functions in
previous works, with the support from Informa-
tion Theory, we propose a new link function of a
text window, parameterized by the embeddings of
words and the residuals of bigrams. Based on the
link function, we establish a generative model of
documents. The learning objective is to ﬁnd a set
of embeddings maximizing their posterior likeli-
hood given the corpus. This objective is reduced to
weighted low-rank positive-semideﬁnite approxi-
mation, subject to Tikhonov regularization. Then
we adopt a Block Coordinate Descent algorithm,
jointly with an online blockwise regression algo-
rithm to ﬁnd an approximate solution. On seven
benchmark sets,
the learned embeddings show
competitive and stable performance.

In the future work, we will incorporate global
latent factors into this generative model, such as
topics, sentiments, or writing styles, and develop
more elaborate models of documents. Through
learning such latent factors, important summary
information of documents would be acquired,
which are useful in various applications.

Acknowledgments

We thank Omer Levy, Thomas Mach, Peilin Zhao,
Mingkui Tan, Zhiqiang Xu and Chunlin Wu for
their helpful discussions and insights. This re-
search is supported by the National Research
Foundation, Prime Minister’s Ofﬁce, Singapore
under its IDM Futures Funding Initiative and ad-
ministered by the Interactive and Digital Media
Programme Ofﬁce.

Appendix A Possible Trap in SVD

Suppose M is the bigram matrix of interest. SVD
embeddings are derived from the low rank approx-
imation of M (cid:62)M , by keeping the largest singular
values/vectors. When some of these singular val-
ues correspond to negative eigenvalues, undesir-
able correlations might be captured. The follow-
ing is an example of approximating a PMI matrix.
A vocabulary consists of 3 words s1, s2, s3.

Two corpora derive two PMI matrices:
(cid:17)
M (1) =

, M (2) =

(cid:16) 1.4 0.8 0
0.8 2.6 0
0 2
0

(cid:16) 0.2 −1.6 0
−1.6 −2.2 0
2

0

0

(cid:17)

.

0

the largest

They have identical left singular matrix and sin-
gular values (3, 2, 1), but their eigenvalues are
(3, 2, 1) and (−3, 2, 1), respectively.
In a rank-2 approximation,

two
singular values/vectors are kept, and M (1) and
M (2) yield identical SVD embeddings V =
( 0.45 0.89 0
0 1 ) (the rows may be scaled depending on
the algorithm, without affecting the validity of the
following conclusion). The embeddings of s1 and
s2 (columns 1 and 2 of V ) point at the same di-
rection, suggesting they are positively correlated.
However as M (2)
2,1 = −1.6 < 0, they are
actually negatively correlated in the second cor-
pus. This inconsistency is because the principal
eigenvalue of M (2) is negative, and yet the corre-
sponding singular value/vector is kept.
When using eigendecomposition,

the largest
two positive eigenvalues/eigenvectors are kept.
M (1) yields the same embeddings V . M (2)
yields V (2) = (cid:0) −0.89 0.45 0
(cid:1) , which correctly
0 1.41
preserves the negative correlation between s1, s2.

1,2 = M (2)

0

Appendix B Information Theory

Redundant information refers to the reduced un-
certainty by knowing the value of any one of the
conditioning variables (hence redundant). Syner-
gistic information is the reduced uncertainty as-
cribed to knowing all the values of conditioning
variables, that cannot be reduced by knowing the
value of any variable alone (hence synergistic).

The mutual information I(y; xi) and the redun-

dant information Rdn(y; x1, x2) are deﬁned as:

I(y; xi) = EP (xi,y)[log

P (y|xi)
P (y)

]

(cid:20)
Rdn(y; x1, x2) = EP (y)

EP (xi|y)[log

min
x1,x2
The synergistic information Syn(y; x1, x2) is
deﬁned as the PI-function in (Williams and Beer,
2010), skipped here.

P (y|xi)
P (y)

(cid:21)

]

Figure 2: Different types of information among
3 random variables y, x1, x2.
I(y; x1, x2) is
the mutual information between y and (x1, x2).
Rdn(y; x1, x2) and Syn(y; x1, x2) are the redun-
dant information and synergistic information be-
tween x1, x2, conditioning y, respectively.

The interaction information Int(x1, x2, y) mea-
sures the relative strength of Rdn(y; x1, x2) and
Syn(y; x1, x2) (Timme et al., 2014):

Int(x1, x2, y)

=Syn(y; x1, x2) − Rdn(y; x1, x2)
=I(y; x1, x2) − I(y; x1) − I(y; x2)

=EP (x1,x2,y)[log

P (x1)P (x2)P (y)P (x1, x2, y)
P (x1, x2)P (x1, y)P (x2, y)

]

Figure 2 shows the relationship of different
information among 3 random variables y, x1, x2
(based on Fig.1 in (Williams and Beer, 2010)).

PMI is the pointwise counterpart of mutual
information I. Similarly, all the above concepts
have their pointwise counterparts, obtained by
dropping the expectation operator. Speciﬁcally,
the pointwise interaction information is deﬁned as
PInt(x1, x2, y) = PMI(y; x1, x2) − PMI(y; x1) −
log P (x1)P (x2)P (y)P (x1,x2,y)
PMI(y; x2)
P (x1,x2)P (x1,y)P (x2,y) .
If we know PInt(x1, x2, y), we can recover
PMI(y; x1, x2) from the mutual information over
the variable subsets, and then recover the joint
distribution P (x1, x2, y).

=

As

the

pointwise

redundant

interaction terms,

information
PRdn(y; x1, x2) and the pointwise synergistic
information PSyn(y; x1, x2) are both higher-
order
their magnitudes are
usually much smaller than the PMI terms. We
assume they are approximately equal, and thus
cancel each other when computing PInt. Given
this, PInt
In the case of three
words w0, w1, w2, PInt(w0, w1, w2) = 0 leads to
PMI(w2; w0, w1) = PMI(w2; w0)+PMI(w2; w1).

is always 0.

References

[Agirre et al.2009] Eneko Agirre, Enrique Alfonseca,
Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor
Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 19–27. Association for Computa-
tional Linguistics.

[Arora et al.2015] S. Arora, Y. Li, Y. Liang, T. Ma, and
A. Risteski. 2015. Random walks on discourse
spaces: a new generative language model with ap-
plications to semantic word embeddings. ArXiv e-
prints, arXiv:1502.03520 [cs.LG].

[Bengio et al.2006] Yoshua Bengio, Holger Schwenk,
Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-
Luc Gauvain. 2006. Neural probabilistic language
models. In Innovations in Machine Learning, pages
137–186. Springer.

[Blei et al.2003] David M Blei, Andrew Y Ng, and
Michael I Jordan. 2003. Latent dirichlet allocation.
The Journal of Machine Learning Research, 3:993–
1022.

[Bruni et al.2012] Elia Bruni, Gemma Boleda, Marco
Baroni, and Nam-Khanh Tran. 2012. Distributional
semantics in technicolor. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1, pages
136–145. Association for Computational Linguis-
tics.

[Deerwester et al.1990] Scott C. Deerwester, Susan T
Dumais, and Richard A. Harshman. 1990. Index-
ing by latent semantic analysis. J. Am. Soc. Inf. Sci.

[Dhillon et al.2011] Paramveer Dhillon, Dean P Foster,
and Lyle H Ungar. 2011. Multi-view learning of
In Proceedings of Ad-
word embeddings via cca.
vances in Neural Information Processing Systems,
pages 199–207.

[Dhillon et al.2015] Paramveer S Dhillon, Dean P Fos-
ter, and Lyle H Ungar. 2015. Eigenwords: Spectral
word embeddings. The Journal of Machine Learn-
ing Research.

[Faruqui et al.2015] Manaal Faruqui, Yulia Tsvetkov,
Dani Yogatama, Chris Dyer, and Noah A. Smith.
2015. Sparse overcomplete word vector represen-
tations. In Proceedings of ACL 2015.

[Hill et al.2014] Felix Hill, Roi Reichart, and Anna Ko-
rhonen. 2014. Simlex-999: Evaluating semantic
models with (genuine) similarity estimation. CoRR,
abs/1408.3456.

[Hinton2002] Geoffrey Hinton. 2002. Training prod-
ucts of experts by minimizing contrastive diver-
gence. Neural Computation, 14(8):1771–1800.

[Holland1973] Paul W. Holland.

1973. Weighted
Ridge Regression: Combining Ridge and Ro-
bust Regression Methods. NBER Working Papers
0011, National Bureau of Economic Research, Inc,
September.

[Hsu et al.2012] Daniel Hsu, Sham M Kakade, and
Tong Zhang. 2012. A spectral algorithm for learn-
ing hidden markov models. Journal of Computer
and System Sciences, 78(5):1460–1480.

[Levy and Goldberg2014] Omer Levy and Yoav Gold-
berg. 2014. Neural word embeddings as implicit
matrix factorization. In Proceedings of NIPS 2014.

[Levy et al.2014] Omer Levy, Yoav Goldberg, and Is-
rael Ramat-Gan. 2014. Linguistic regularities in
In Pro-
sparse and explicit word representations.
ceedings of CoNLL-2014, page 171.

[Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido
Dagan. 2015.
Improving distributional similarity
with lessons learned from word embeddings. Trans-
actions of the Association for Computational Lin-
guistics, 3:211–225.

[Lin and He2009] Chenghua Lin and Yulan He. 2009.
Joint sentiment/topic model for sentiment analysis.
In Proceedings of the 18th ACM conference on In-
formation and Knowledge Management, pages 375–
384. ACM.

[Luong et al.2013] Minh-Thang

Richard
Socher, and Christopher D Manning. 2013. Better
word representations with recursive neural networks
for morphology. CoNLL-2013, 104.

Luong,

[Mach2012] T. Mach. 2012. Eigenvalue Algorithms
for Symmetric Hierarchical Matrices. Dissertation,
Chemnitz University of Technology.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
mation of word representations in vector space. In
Proceedings of Workshop at ICLR 2013.

[Finkelstein et al.2002] Lev

Finkelstein,

Evgeniy
Gabrilovich, Yossi Matias, Ehud Rivlin, Zach
Solan, Gadi Wolfman, and Eytan Ruppin. 2002.
Placing search in context: The concept revisited.
ACM Trans. Inf. Syst., 20(1):116–131, January.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases
and their compositionality. In Proceedings of NIPS
2013, pages 3111–3119.

[Higham1988] Nicholas J. Higham. 1988. Comput-
ing a nearest symmetric positive semideﬁnite matrix.
Linear Algebra and its Applications, 103(0):103 –
118.

[Mikolov et al.2013c] Tomas Mikolov, Wen-tau Yih,
and Geoffrey Zweig. 2013c. Linguistic regularities
In Pro-
in continuous space word representations.
ceedings of HLT-NAACL 2013, pages 746–751.

[Zesch et al.2008] Torsten Zesch, Christof M¨uller, and
Iryna Gurevych. 2008. Using wiktionary for com-
In Proceedings of
puting semantic relatedness.
AAAI 2008, volume 8, pages 861–866.

[Zhai and Lafferty2004] Chengxiang Zhai and John
Lafferty. 2004. A study of smoothing methods
for language models applied to information retrieval.
ACM Transactions on Information Systems (TOIS),
22(2):179–214.

[Zhao et al.2011a] Peilin Zhao, Steven CH Hoi, and
Rong Jin. 2011a. Double updating online learn-
ing. The Journal of Machine Learning Research,
12:1587–1615.

[Zhao et al.2011b] Wayne Xin Zhao, Jing Jiang, Jian-
shu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and
Xiaoming Li. 2011b. Comparing twitter and tra-
ditional media using topic models. In Advances in
Information Retrieval (Proceedings of the 33rd An-
nual European Conference on Information Retrieval
Research), pages 338–349. Springer.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey
Hinton. 2007. Three new graphical models for sta-
In Proceedings of the
tistical language modelling.
24th International Conference on Machine learning,
pages 641–648. ACM.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. Proceedings
of the Empiricial Methods in Natural Language Pro-
cessing (EMNLP 2014), 12.

[Radinsky et al.2011] Kira

Eugene
Radinsky,
Agichtein,
and Shaul
Evgeniy Gabrilovich,
Markovitch. 2011. A word at a time: Computing
word relatedness using temporal semantic analysis.
In Proceedings of the 20th International Conference
on World Wide Web, WWW ’11, pages 337–346,
New York, NY, USA. ACM.

[Srebro et al.2003] Nathan Srebro, Tommi Jaakkola,
et al. 2003. Weighted low-rank approximations. In
Proceedings of ICML 2003, volume 3, pages 720–
727.

[Stratos et al.2014] Karl

Stratos, Do-kyum Kim,
Michael Collins, and Daniel Hsu. 2014. A spectral
algorithm for learning class-based n-gram models of
natural language. In Proceedings of the Association
for Uncertainty in Artiﬁcial Intelligence.

[Stratos et al.2015] Karl Stratos, Michael Collins, and
Daniel Hsu. 2015. Model-based word embeddings
from decompositions of count matrices. In Proceed-
ings of ACL 2015.

[Tan et al.2014] M. Tan, I. W. Tsang, L. Wang, B. Van-
dereycken, and S. J. Pan. 2014. Riemannian pursuit
In Proceedings of ICML
for big matrix recovery.
2014, pages 1539–1547.

[Timme et al.2014] Nicholas Timme, Wesley Alford,
Benjamin Flecker, and John M Beggs. 2014. Syn-
ergy, redundancy, and multivariate information mea-
sures: an experimentalist’s perspective. Journal of
Computational Neuroscience, 36(2):119–140.

[Wallach2006] Hanna M Wallach. 2006. Topic mod-
eling: beyond bag-of-words. In Proceedings of the
23rd international conference on Machine learning,
pages 977–984. ACM.

[Williams and Beer2010] Paul L Williams and Ran-
2010. Nonnegative decomposi-
arXiv preprint

dall D Beer.
tion of multivariate information.
arXiv:1004.2515.

[Yan et al.2015] Yan Yan, Mingkui Tan, Ivor Tsang,
Yi Yang, Chengqi Zhang, and Qinfeng Shi. 2015.
Scalable maximum margin matrix factorization by
active riemannian subspace search. In Proceedings
of IJCAI 2015.

[Yogatama et al.2015] Dani

Manaal
Faruqui, Chris Dyer, and Noah A Smith.
2015.
Learning word representations with hierarchical
sparse coding. In Proceedings of ICML 2015.

Yogatama,

A Generative Word Embedding Model and its Low Rank Positive
Semideﬁnite Solution

Shaohua Li1, Jun Zhu2, Chunyan Miao1
1Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY),
Nanyang Technological University, Singapore
2Tsinghua University, P.R. China
lish0018@ntu.edu.sg, dcszj@tsinghua.edu.cn, ascymiao@ntu.edu.sg

Abstract

Most existing word embedding methods
can be categorized into Neural Embedding
Models and Matrix Factorization (MF)-
based methods. However some mod-
els are opaque to probabilistic interpre-
tation, and MF-based methods, typically
solved using Singular Value Decomposi-
tion (SVD), may incur loss of corpus in-
formation.
In addition, it is desirable to
incorporate global latent factors, such as
topics, sentiments or writing styles, into
the word embedding model. Since gen-
erative models provide a principled way
to incorporate latent factors, we propose a
generative word embedding model, which
is easy to interpret, and can serve as a
basis of more sophisticated latent factor
models. The model inference reduces to
a low rank weighted positive semideﬁnite
approximation problem.
Its optimization
is approached by eigendecomposition on a
submatrix, followed by online blockwise
regression, which is scalable and avoids
the information loss in SVD. In experi-
ments on 7 common benchmark datasets,
our vectors are competitive to word2vec,
and better than other MF-based methods.

1

Introduction

The task of word embedding is to model the distri-
bution of a word and its context words using their
corresponding vectors in a Euclidean space. Then
by doing regression on the relevant statistics de-
rived from a corpus, a set of vectors are recovered
which best ﬁt these statistics. These vectors, com-
monly referred to as the embeddings, capture se-
mantic/syntactic regularities between the words.

The core of a word embedding method is the
link function that connects the input — the embed-
dings, with the output — certain corpus statistics.

Based on the link function, the objective function
is developed. The reasonableness of the link func-
tion impacts the quality of the obtained embed-
dings, and different link functions are amenable
to different optimization algorithms, with different
scalability. Based on the forms of the link func-
tion and the optimization techniques, most meth-
ods can be divided into two classes: the traditional
neural embedding models, and more recent low
rank matrix factorization methods.

The neural embedding models use the softmax
link function to model the conditional distribution
of a word given its context (or vice versa) as a
function of the embeddings. The normalizer in the
softmax function brings intricacy to the optimiza-
tion, which is usually tackled by gradient-based
methods. The pioneering work was (Bengio et
al., 2006). Later Mnih and Hinton (2007) propose
three different link functions. However there are
interaction matrices between the embeddings in all
these models, which complicate and slow down
the training, hindering them from being trained
on huge corpora. Mikolov et al. (2013a) and
Mikolov et al. (2013b) greatly simplify the condi-
tional distribution, where the two embeddings in-
teract directly. They implemented the well-known
“word2vec”, which can be trained efﬁciently on
huge corpora. The obtained embeddings show ex-
cellent performance on various tasks.

Low-Rank Matrix Factorization (MF in short)
methods include various link functions and opti-
mization methods. The link functions are usu-
ally not softmax functions. MF methods aim to
reconstruct certain corpus statistics matrix by the
product of two low rank factor matrices. The ob-
jective is usually to minimize the reconstruction
error, optionally with other constraints.
In this
line of research, Levy and Goldberg (2014) ﬁnd
that “word2vec” is essentially doing stochastic
weighted factorization of the word-context point-
wise mutual information (PMI) matrix. They then

5
1
0
2
 
g
u
A
 
6
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
2
8
3
0
.
8
0
5
1
:
v
i
X
r
a

factorize this matrix directly as a new method.
Pennington et al. (2014) propose a bilinear regres-
sion function of the conditional distribution, from
which a weighted MF problem on the bigram log-
frequency matrix is formulated. Gradient Descent
is used to ﬁnd the embeddings. Recently, based
on the intuition that words can be organized in se-
mantic hierarchies, Yogatama et al. (2015) add hi-
erarchical sparse regularizers to the matrix recon-
struction error. With similar techniques, Faruqui
et al. (2015) reconstruct a set of pretrained embed-
dings using sparse vectors of greater dimensional-
ity. Dhillon et al. (2015) apply Canonical Corre-
lation Analysis (CCA) to the word matrix and the
context matrix, and use the canonical correlation
vectors between the two matrices as word embed-
dings. Stratos et al. (2014) and Stratos et al. (2015)
assume a Brown language model, and prove that
doing CCA on the bigram occurrences is equiva-
lent to ﬁnding a transformed solution of the lan-
guage model. Arora et al. (2015) assume there is a
hidden discourse vector on a random walk, which
determines the distribution of the current word.
The slowly evolving discourse vector puts a con-
straint on the embeddings in a small text window.
The maximum likelihood estimate of the embed-
dings within this text window approximately re-
duces to a squared norm objective.

There are two limitations in current word em-
bedding methods. The ﬁrst limitation is, all MF-
based methods map words and their context words
to two different sets of embeddings, and then em-
ploy Singular Value Decomposition (SVD) to ob-
tain a low rank approximation of the word-context
matrix M . As SVD factorizes M (cid:62)M , some in-
formation in M is lost, and the learned embed-
dings may not capture the most signiﬁcant regu-
larities in M . Appendix A gives a toy example on
which SVD does not work properly.

The second limitation is, a generative model for
documents parametered by embeddings is absent
in recent development. Although (Stratos et al.,
2014; Stratos et al., 2015; Arora et al., 2015) are
based on generative processes, the generative pro-
cesses are only for deriving the local relationship
between embeddings within a small text window,
leaving the likelihood of a document undeﬁned.
In addition, the learning objectives of some mod-
els, e.g. (Mikolov et al., 2013b, Eq.1), even have
no clear probabilistic interpretation. A genera-
tive word embedding model for documents is not

only easier to interpret and analyze, but more im-
portantly, provides a basis upon which document-
level global latent factors, such as document topics
(Wallach, 2006), sentiments (Lin and He, 2009),
writing styles (Zhao et al., 2011b), can be incor-
porated in a principled manner, to better model the
text distribution and extract relevant information.
Based on the above considerations, we pro-
pose to unify the embeddings of words and con-
text words. Our link function factorizes into three
parts: the interaction of two embeddings capturing
linear correlations of two words, a residual captur-
ing nonlinear or noisy correlations, and the uni-
gram priors. To reduce overﬁtting, we put Gaus-
sian priors on embeddings and residuals, and ap-
ply Jelinek-Mercer Smoothing to bigrams. Fur-
thermore, to model the probability of a sequence
of words, we assume that the contributions of
more than one context word approximately add up.
Thereby a generative model of documents is con-
structed, parameterized by embeddings and resid-
uals. The learning objective is to maximize the
corpus likelihood, which reduces to a weighted
low-rank positive semideﬁnite (PSD) approxima-
tion problem of the PMI matrix. A Block Co-
ordinate Descent algorithm is adopted to ﬁnd an
approximate solution. This algorithm is based
on Eigendecomposition, which avoids information
loss in SVD, but brings challenges to scalability.
We then exploit the sparsity of the weight matrix
and implement an efﬁcient online blockwise re-
gression algorithm. On seven benchmark datasets
covering similarity and analogy tasks, our method
achieves competitive and stable performance.

The source code of this method is provided at

https://github.com/askerlee/topicvec.

2 Notations and Deﬁnitions

Throughout the paper, we always use a uppercase
bold letter as S, V to denote a matrix or set, a low-
ercase bold letter as vwi to denote a vector, a nor-
mal uppercase letter as N, W to denote a scalar
constant, and a normal lowercase letter as si, wi to
denote a scalar variable.

Suppose a vocabulary S = {s1, · · · , sW } con-
sists of all the words, where W is the vocab-
ulary size. We further suppose s1, · · · , sW are
sorted in decending order of the frequency, i.e.
s1 is most frequent, and sW is least frequent.
A document di
is a sequence of words di =
(wi1, · · · , wiLi), wij ∈ S. A corpus is a collec-

Name
S

V

D
vsi
asisj
˜P (si,sj)
u

A

B

G

H

Description
Vocabulary {s1, · · · , sW }
Embedding matrix (vs1 , · · · , vsW )
Corpus {d1, · · · , dM }
Embedding of word si
Bigram residual for si, sj
Empirical probability of si, sj in the corpus
Unigram probability vector (P (s1),· · ·, P (sW ))
Residual matrix (asisj )
(cid:16)

(cid:17)

Conditional probability matrix

PMI matrix

PMI(si, sj)

(cid:16)

P (sj|si)
(cid:17)
(cid:16) ˜P (si, sj)
(cid:17)

Bigram empirical probability matrix

Table 1: Notation Table

tion of M documents D = {d1, · · · , dM }. In the
vocabulary, each word si is mapped to a vector vsi
in N -dimensional Euclidean space.

In a document, a sequence of words is referred
to as a text window, denoted by wi, · · · , wi+l, or
wi:wi+l in shorthand. A text window of chosen
size c before a word wi deﬁnes the context of wi
as wi−c, · · · , wi−1. Here wi is referred to as the
focus word. Each context word wi−j and the focus
word wi comprise a bigram wi−j, wi.

The Pointwise Mutual Information between two

words si, sj is deﬁned as

PMI(si, sj) = log

P (si, sj)
P (si)P (sj)

.

3 Link Function of Text

In this section, we formulate the probability of a
sequence of words as a function of their embed-
dings. We start from the link function of bigrams,
which is the building blocks of a long sequence.
Then this link function is extended to a text win-
dow with c context words, as a ﬁrst-order approx-
imation of the actual probability.

3.1 Link Function of Bigrams

We generalize the link function of “word2vec” and
“GloVe” to the following:

(cid:110)

(cid:111)

P (si, sj) = exp

P (si)P (sj) (1)

v(cid:62)
sj vsi + asisj
The rationale for (1) originates from the idea of
the Product of Experts in (Hinton, 2002). Sup-
pose different types of semantic/syntactic regu-
larities between si and sj are encoded in differ-
sj vsi} =
ent dimensions of vsi, vsj . As exp{v(cid:62)
(cid:81)
l exp{vsi,l · vsj ,l}, this means the effects of dif-
ferent regularities on the probability are combined

by multiplying together. If si and sj are indepen-
dent, their joint probability should be P (si)P (sj).
In the presence of correlations, the actual joint
probability P (si, sj) would be a scaling of it. The
scale factor reﬂects how much si and sj are pos-
itively or negatively correlated. Within the scale
factor, v(cid:62)
sj vsi captures linear interactions between
si and sj, the residual asisj captures nonlinear or
sj vsi is
noisy interactions. In applications, only v(cid:62)
of interest. Hence the bigger magnitude v(cid:62)
sj vsi is
of relative to asisj , the better.

Note that we do not assume asisj = asj si.
This provides the ﬂexibility P (si, sj) (cid:54)= P (sj, si),
agreeing with the asymmetry of bigrams in natu-
ral languages. At the same time, v(cid:62)
sj vsi imposes a
symmetric part between P (si, sj) and P (sj, si).

(1) is equivalent to

(cid:110)

P (sj|si)=exp

v(cid:62)
sj vsi + asisj + log P (sj)

, (2)

(cid:111)

log

P (sj|si)
P (sj)

= v(cid:62)

sj vsi + asisj .

(3) of all bigrams is represented in matrix form:

(3)

(4)

V (cid:62)V + A = G,

where G is the PMI matrix.

3.1.1 Gaussian Priors on Embeddings
When (1) is employed on the regression of empir-
ical bigram probabilities, a practical issue arises:
more and more bigrams have zero frequency as
the constituting words become less frequent. A
zero-frequency bigram does not necessarily imply
negative correlation between the two constituting
words; it could simply result from missing data.
But in this case, even after smoothing, (1) will
force v(cid:62)
sj vsi + asisj to be a big negative number,
making vsi overly long. The increased magnitude
of embeddings is a sign of overﬁtting.

To reduce overﬁtting of embeddings of infre-
quent words, we assign a Spherical Gaussian prior
N (0, 1
2µi

I) to vsi:
P (vsi) ∼ exp{−µi(cid:107)vsi(cid:107)2},
where the hyperparameter µi increases as the fre-
quency of si decreases.

3.1.2 Gaussian Priors on Residuals
We wish v(cid:62)
sj vsi in (1) captures as much corre-
lations between si and sj as possible. Thus the
smaller asisj is, the better. In addition, the more
frequent si, sj is in the corpus,
the less noise
there is in their empirical distribution, and thus the
residual asisj should be more heavily penalized.

To this end, we penalize the residual asisj
by f (˜P (si, sj))a2
sisj , where f (·) is a nonnega-
tive monotonic transformation, referred to as the
weighting function. Let hij denote ˜P (si, sj), then
the total penalty of all residuals are the square of
the weighted Frobenius norm of A:
sisj = (cid:107)A(cid:107)2

f (hij)a2

f (H).

(cid:88)

(5)

si,sj ∈S

By referring to “GloVe”, we use the following

√

weighting function, and ﬁnd it performs well:
(cid:112)hij < Ccut, i (cid:54)= j
(cid:112)hij ≥ Ccut, i (cid:54)= j
i = j


hij

Ccut
1

0

f (hij) =

,

where Ccut is chosen to cut the most frequent
0.02% of the bigrams off at 1. When si = sj, two
identical words usually have much smaller proba-
bility to collocate. Hence ˜P (si, si) does not reﬂect
the true correlation of a word to itself, and should
not put constraints to the embeddings. We elimi-
nate their effects by setting f (hii) to 0.

(cid:16)

If the domain of A is the whole space RW ×W ,
then this penalty is equivalent to a Gaussian prior
N
on each asisj . The variances of the
Gaussians are determined by the bigram empirical
probability matrix H.

1
2f (hij )

0,

(cid:17)

Jelinek-Mercer Smoothing of Bigrams

3.1.3
As another measure to reduce the impact of miss-
ing data, we apply the commonly used Jelinek-
Mercer Smoothing (Zhai and Lafferty, 2004)
to smooth the empirical conditional probability
˜P (sj|si) by the unigram probability ˜P (sj) as:
˜Psmoothed(sj|si) = (1−κ) ˜P (sj|si)+κP (sj). (6)
the smoothed bigram empirical
Accordingly,

joint probability is deﬁned as
˜P (si, sj) = (1−κ) ˜P (si, sj)+κP (si)P (sj). (7)
In practice, we ﬁnd κ = 0.02 yields good re-
sults. When κ ≥ 0.04, the obtained embeddings
begin to degrade with κ, indicating that smoothing
distorts the true bigram distributions.

3.2 Link Function of a Text Window

In the previous subsection, a regression link func-
tion of bigram probabilities is established.
In
this section, we adopt a ﬁrst-order approximation
based on Information Theory, and extend the link
function to a longer sequence w0, · · · , wc−1, wc.

Decomposing a distribution conditioned on n
random variables as the conditional distributions

on its subsets roots deeply in Information The-
ory. This is an intricate problem because there
could be both (pointwise) redundant information
and (pointwise) synergistic information among the
conditioning variables (Williams and Beer, 2010).
They are both functions of the PMI. Based on an
analysis of the complementing roles of these two
types of pointwise information, we assume they
are approximately equal and cancel each other
when computing the pointwise interaction infor-
mation. See Appendix B for a detailed discussion.
Following the above assumption, we have
PMI(w2; w0, w1) ≈ PMI(w2; w0)+PMI(w2; w1):

log

P (w0, w1|w2)
P (w0, w1)

≈log

P (w0|w2)
P (w0)

+log

P (w1|w2)
P (w1)

.

Plugging (1) and (3) into the above, we obtain
P (w0, w1, w2)
(cid:26) 2

≈ exp

(v(cid:62)

wivwj + awiwj ) +

log P (wi)

(cid:27)
.

(cid:88)

i,j=0
i(cid:54)=j

We extend the above assumption to that the
pointwise interaction information is still close to
0 within a longer text window. Accordingly the
above equation extends to a context of size c > 2:

P (w0, · · · , wc)
(cid:26) c

(cid:88)

≈ exp

i,j=0
i(cid:54)=j

(v(cid:62)

wivwj + awiwj ) +

log P (wi)

(cid:27)
.

2
(cid:88)

i=0

c
(cid:88)

i=0

From it derives the conditional distribution of

wc, given its context w0, · · · , wc−1:

P (wc | w0 : wc−1)=

(cid:26)

≈P (wc) exp

v(cid:62)
wc

P (w0, · · · , wc)
P (w0, · · · , wc−1)
c−1
(cid:27)
(cid:88)
.

awiwc

vwi +

i=0

c−1
(cid:88)

i=0

(8)

4 Generative Process and Likelihood

We proceed to assume the text is generated from a
Markov chain of order c, i.e., a word only depends
on words within its context of size c. Given the
hyperparameter µ = (µ1, · · ·, µW ), the generative
process of the whole corpus is:

1. For each word si, draw the embedding vsi

from N (0, 1
2µi

I);

(cid:17)

(cid:16)

0,

asisj from N

2. For each bigram si, sj, draw the residual
1
2f (hij )
3. For each document di, for the j-th word,
from S with probability

draw word wij
P (wij | wi,j−c : wi,j−1) deﬁned by (8).

;

5 Learning Algorithm

5.1 Learning Objective

The learning objective is to ﬁnd the embeddings
V that maximize the corpus log-likelihood (9).

Let xij denote the (smoothed) frequency of bi-

gram si, sj in the corpus. Then (9) is sorted as:

log p(D, V , A)

µi

vsi

V

hij

aij

A

d

vw0

vw1

· · ·

vwc

=C0 − log Z(H, µ) − (cid:107)A(cid:107)2

f (H) −

µi(cid:107)vsi(cid:107)2

W
(cid:88)

i=1

Figure 1: The Graphical Model of PSDVec

The above generative process for a document d is
presented as a graphical model in Figure 1.

Based on this generative process, the probabil-
ity of a document di can be derived as follows,
given the embeddings and residuals V , A:

P (di|V , A)
Li(cid:89)

(cid:26)

j−1
(cid:88)

j−1
(cid:88)

=

P (wij) exp

v(cid:62)

wij

vwik +

awikwij

j=1

k=j−c

k=j−c

The complete-data likelihood of the corpus is:

p(D, V , A)

=

N (0,

W
(cid:89)

i=1

I
2µi

)

W,W
(cid:89)

(cid:18)

N

0,

i,j=1

1
2f (hij)

(cid:19) M
(cid:89)

p(di|V, A)

=

1
Z(H, µ)

(cid:110)

exp

−

W,W
(cid:88)

f (hi,j)a2

sisj −

µi(cid:107)vsi(cid:107)2(cid:111)

i,j=1

(cid:26)

j−1
(cid:88)

P (wij) exp

v(cid:62)

wij

vwik +

awikwij

M,Li
(cid:89)
·
i,j=1

+

W,W
(cid:88)

i,j=1

xij(v(cid:62)

sivsj + asisj ).

(10)

As
the
(cid:80)W,W
i,j=1 xij(v(cid:62)

corpus
sivsj +asisj ) will

increases,
the
parameter prior terms. Then we can ignore the
prior terms when maximizing (10).

dominate

size

max
(cid:16)(cid:88)

(cid:88)

xij

xij(v(cid:62)
(cid:17)

· max

=

sivsj +asisj )

(cid:88) ˜Psmoothed(si, sj) log P (si, sj).

As both { ˜Psmoothed(si, sj)} and {P (si, sj)}
the above sum is maximized when

sum to 1,
P (si, sj) = ˜Psmoothed(si, sj).

(cid:27)
.

The maximum likelihood estimator is then:
P (sj|si) = ˜Psmoothed(sj|si),

v(cid:62)
sivsj + asisj = log

˜Psmoothed(sj|si)
P (sj)

.

(11)

B∗ =

Writing (11) in matrix form:
(cid:17)
(cid:16) ˜Psmoothed(sj|si)
G∗ = log B∗ − log u ⊗ (1 · · · 1),

si,sj ∈S

(12)

(cid:27)

,

where “⊗” is the outer product.
Now we ﬁx the values of v(cid:62)

sivsj + asisj at the

k=j−c

k=j−c

above optimal. The corpus likelihood becomes

where Z(H, µ) is the normalizing constant.

logarithm of both sides of

log p(D, V , A) =C1 − (cid:107)A(cid:107)2

f (H) −

µi(cid:107)vsi(cid:107)2,

W
(cid:88)

i=1

Taking the
p(D, A, V ) yields

log p(D, V , A)

=C0 − log Z(H, µ) − (cid:107)A(cid:107)2

f (H)−

µi(cid:107)vsi(cid:107)2

M,Li
(cid:88)

(cid:26)

+

v(cid:62)

wij

j−1
(cid:88)

j−1
(cid:88)

(cid:27)

vwik +

awikwij

,

(9)

i,j=1

k=j−c

k=j−c

where C0 = (cid:80)M,Li

i,j=1 log P (wij) is constant.

subject to V (cid:62)V + A = G∗,

(13)
where C1 = C0 + (cid:80) xij log ˜Psmoothed(si, sj) −
log Z(H, µ) is constant.

5.2 Learning V as Low Rank PSD

Approximation

Once G∗ has been estimated from the corpus using
(12), we seek V that maximizes (13). This is to
ﬁnd the maximum a posteriori (MAP) estimates
of V , A that satisfy V (cid:62)V + A = G∗. Applying
this constraint to (13), we obtain

i=1

W
(cid:88)

i=1

j−1
(cid:88)

W
(cid:88)

i=1

Algorithm 1 BCD algorithm for ﬁnding a unreg-
ularized rank-N weighted PSD approximant.
Input: matrix G∗, weight matrix W = f (H),
iteration number T , rank N
Randomly initialize X (0)
for t = 1, · · · , T do

Gt = W ◦ G∗ + (1 − W ) ◦ X (t−1)
X (t) = PSD Approximate(Gt, N )

end for
λ, Q = Eigen Decomposition(X (T ))
V ∗ = diag(λ
Output: V ∗

1
2 [1:N ]) · Q(cid:62)[1:N ]

arg max
V

log p(D, V , A)

= arg min
V

(cid:107)G∗−V (cid:62)V (cid:107)f (H) +

µi(cid:107)vsi(cid:107)2. (14)

W
(cid:88)

i=1

Let X = V (cid:62)V . Then X is positive semidef-
inite of rank N . Finding V that minimizes (14)
is equivalent to ﬁnding a rank-N weighted posi-
tive semideﬁnite approximant X of G∗, subject to
Tikhonov regularization. This problem does not
admit an analytic solution, and can only be solved
using local optimization methods.

First we consider a simpler case where all the
words in the vocabulary are enough frequent, and
thus Tikhonov regularization is unnecessary.
In
this case, we set ∀µi = 0, and (14) becomes an
unregularized optimization problem. We adopt the
Block Coordinate Descent (BCD) algorithm1 in
(Srebro et al., 2003) to approach this problem. The
original algorithm is to ﬁnd a generic rank-N ma-
trix for a weighted approximation problem, and
we tailor it by constraining the matrix within the
positive semideﬁnite manifold.

We summarize our learning algorithm in Al-
gorithm 1. Here “◦” is the entry-wise prod-
uct. We suppose the eigenvalues λ returned by
Eigen Decomposition(X) are in descending or-
der. Q(cid:62)[1:N ] extracts the 1 to N rows from Q(cid:62).
One key issue is how to initialize X. Srebro et
al. (2003) suggest to set X (0) =G∗, and point out
that X (0) = 0 is far from a local optimum, thus
requires more iterations. However we ﬁnd G∗ is
also far from a local optimum, and this setting con-
verges slowly too. Setting X (0) = G∗/2 usually

1It is referred to as an Expectation-Maximization algo-
rithm by the original authors, but we think this is a misnomer.

yields a satisfactory solution in a few iterations.

The subroutine PSD Approximate() computes
the unweighted nearest rank-N PSD approxima-
tion, measured in F-norm (Higham, 1988).

5.3 Online Blockwise Regression of V

the

does

essential

subroutine
In Algorithm 1,
PSD Approximate()
eigendecomposi-
tion on Gt, which is dense due to the logarithm
transformation. Eigendecomposition on a W × W
dense matrix requires O(W 2) space and O(W 3)
time, difﬁcult to scale up to a large vocabulary. In
addition, the majority of words in the vocabulary
are infrequent, and Tikhonov regularization is
necessary for them.

It is observed that, as words become less fre-
quent, fewer and fewer words appear around them
to form bigrams. Remind that the vocabulary
S = {s1, · · · , sW } are sorted in decending or-
der of the frequency, hence the lower-right blocks
of H and f (H) are very sparse, and cause these
blocks in (14) to contribute much less penalty rela-
tive to other regions. Therefore these blocks could
be ignored when doing regression, without sacri-
ﬁcing too much accuracy. This intuition leads to
the following online blockwise regression.

The basic idea is to select a small set (e.g.
30,000) of the most frequent words as the core
words, and partition the remaining noncore words
into sets of moderate sizes. Bigrams consist-
ing of two core words are referred to as core bi-
grams, which correspond to the top-left blocks of
G and f (H). The embeddings of core words
are learned approximately using Algorithm 1, on
the top-left blocks of G and f (H). Then we ﬁx
the embeddings of core words, and ﬁnd the em-
beddings of each set of noncore words in turn.
After ignoring the lower-right regions of G and
f (H) which correspond to bigrams of two non-
core words, the quadratic terms of noncore em-
beddings are ignored. Consequently, ﬁnding these
embeddings becomes a weighted ridge regression
problem, which can be solved efﬁciently in closed-
form. Finally we combine all embeddings to get
the embeddings of the whole vocabulary. The de-
tails are as follows:

1. Partition S into K consecutive groups
S1, · · · , Sk. Take K = 3 as an example.
The ﬁrst group is core words;

2. Accordingly partition G into K × K blocks,

in this example as





G11 G12 G13
G21 G22 G23
G31 G32 G33



 .

Partition
same way.
G11, f (H)11, A11 correspond to core bi-

f (H),A in

the

grams. Partition V into

(cid:0)

V 1
(cid:124)(cid:123)(cid:122)(cid:125)
S1
1 V 1 + A11 = G11 using Algorithm

V 3
(cid:124) (cid:123)(cid:122) (cid:125)
S3

V 2
(cid:124) (cid:123)(cid:122) (cid:125)
S2

;

(cid:1)

3. Solve V (cid:62)

1, and obtain core embeddings V ∗
1;

4. Set V 1 = V ∗

1, and ﬁnd V ∗
2 that minimizes
the total penalty of the 12-th and 21-th blocks
of residuals (the 22-th block is ignored due to
its high sparsity):

(cid:107)G12 − V (cid:62)

1 V 2(cid:107)2

f (H)12

arg min
V 2
+ (cid:107)G21 − V (cid:62)

2 V 1(cid:107)2

f (H)21

+

(cid:88)

µi(cid:107)vsi(cid:107)2

si∈S2
(cid:88)
+

si∈S2

= arg min
V 2

(cid:107)G12−V (cid:62)

1 V 2(cid:107)2

¯f (H)12

µi(cid:107)vsi(cid:107)2,

where ¯f (H)12 = f (H)12 + f (H)(cid:62)
21;
(cid:17)
G12 ◦ f (H)12 + G(cid:62)
21 ◦ f (H)(cid:62)
G12 =
21

(cid:16)

(cid:16)

(cid:17)

f (H)12 + f (H)(cid:62)
21

/
is the weighted aver-
age of G12 and G(cid:62)
21, “◦” and “/” are element-
wise product and division, respectively. The
columns in V 2 are independent, thus for each
vsi, it is a separate weighted ridge regression
problem, whose solution is (Holland, 1973):
1 diag( ¯f i)¯gi,
1 diag( ¯f i)V 1+µiI)−1V (cid:62)
v∗
si=(V (cid:62)
where ¯f i and ¯gi are columns corresponding
to si in ¯f (H)12 and G12, respectively;
5. For any other set of noncore words Sk, ﬁnd
V ∗
k that minimizes the total penalty of the 1k-
th and k1-th blocks, ignoring all other kj-th
and jk-th blocks;

6. Combine all subsets of embeddings to form

V ∗. Here V ∗ = (V ∗

1, V ∗

2, V ∗
3).

6 Experimental Results

We trained our model along with a few state-of-
the-art competitors on Wikipedia, and evaluated
the embeddings on 7 common benchmark sets.

6.1 Experimental Setup

Our own method is referred to as PSD. The com-
petitors include:

• (Mikolov et al., 2013b): word2vec2, or

SGNS in some literature;

2https://code.google.com/p/word2vec/

• (Levy and Goldberg, 2014): the PPMI ma-
trix without dimension reduction, and SVD
of PPMI matrix, both yielded by hyperwords;

• (Pennington et al., 2014): GloVe3;
• (Stratos et al., 2015): Singular4, which does
SVD-based CCA on the weighted bigram fre-
quency matrix;

• (Faruqui et al., 2015): Sparse5, which learns
new sparse embeddings in a higher dimen-
sional space from pretrained embeddings.

All models were trained on the English Wikipedia
snapshot in March 2015. After removing non-
textual elements and non-English words, 2.04 bil-
lion words were left. We used the default hyperpa-
rameters in Hyperwords when training PPMI and
SVD. Word2vec, GloVe and Singular were trained
with their own default hyperparameters.

The embedding sets PSD-Reg-180K and PSD-
Unreg-180K were trained using our online block-
wise regression. Both sets contain the embed-
dings of the most frequent 180,000 words, based
on 25,000 core words. PSD-Unreg-180K was
traind with all µi = 0, i.e. disabling Tikhonov
regularization. PSD-Reg-180K was trained with

µi =


2

4

8

i ∈ [25001, 80000]
i ∈ [80001, 130000]
i ∈ [130001, 180000]

, i.e.

increased

regularization as the sparsity increases. To con-
trast with the batch learning performance, the per-
formance of PSD-25K is listed, which contains the
core embeddings only. PSD-25K took advantages
that it contains much less false candidate words,
and some test tuples (generally harder ones) were
not evaluated due to missing words, thus its scores
are not comparable to others.

The benchmark sets are almost

Sparse was trained with PSD-180K-reg as the
input embeddings, with default hyperparameters.
to
those in (Levy et al., 2015), except that (Luong et
al., 2013)’s Rare Words is not included, as many
rare words are cut off at the frequency 100, mak-
ing more than 1/3 of test pairs invalid.

identical

Word Similarity There are 5 datasets: Word-
Sim Similarity (WS Sim) and WordSim Related-
ness (WS Rel) (Zesch et al., 2008; Agirre et al.,
2009), partitioned from WordSim353 (Finkelstein
et al., 2002); Bruni et al. (2012)’s MEN dataset;

3http://nlp.stanford.edu/projects/glove/
4https://github.com/karlstratos/singular
5https://github.com/mfaruqui/sparse-coding

Table 2: Performance of each method across different tasks.

Similarity Tasks

Analogy Tasks

Method
word2vec
PPMI
SVD
GloVe
Singular
Sparse
PSD-Reg-180K
PSD-Unreg-180K
PSD-25K

WS Sim WS Rel MEN Turk
0.663
0.659
0.524
0.641
0.581
0.625
0.676
0.675
0.678

0.543
0.678
0.608
0.630
0.684
0.585
0.679
0.663
0.676

0.742
0.735
0.687
0.759
0.763
0.739
0.792
0.786
0.801

0.731
0.717
0.711
0.756
0.747
0.725
0.764
0.753
0.765

SimLex
0.395
0.308
0.270
0.362
0.345
0.355
0.398
0.372
0.393

Google
0.734 / 0.742
0.476 / 0.524
0.230 / 0.240
0.535 / 0.544
0.440 / 0.508
0.240 / 0.282
0.602 / 0.623
0.566 / 0.598
0.671 / 0.695

MSR
0.650 / 0.674
0.183 / 0.217
0.123 / 0.113
0.408 / 0.435
0.364 / 0.399
0.253 / 0.274
0.465 / 0.507
0.424 / 0.468
0.533 / 0.586

Radinsky et al. (2011)’s Mechanical Turk dataset;
and (Hill et al., 2014)’s SimLex-999 dataset. The
embeddings were evaluated by the Spearman’s
rank correlation with the human ratings.

Word Analogy The two datasets are MSR’s
analogy dataset (Mikolov et al., 2013c), contain-
ing 8000 questions, and Google’s analogy dataset
(Mikolov et al., 2013a), with 19544 questions. Af-
ter ﬁltering questions involving out-of-vocabulary
words, i.e. words that appear less than 100 times
in the corpus, 7054 instances in MSR and 19364
instances in Google were left. The analogy ques-
tions were answered using 3CosAdd as well as
3CosMul proposed by Levy et al. (2014).

6.2 Results

Table 2 shows the results on all tasks. Word2vec
signiﬁcantly outperformed other methods on anal-
ogy tasks. PPMI and SVD performed much worse
on analogy tasks than reported in (Levy et al.,
2015), probably due to sub-optimal hyperparam-
eters. This suggests their performance is unstable.
The new embeddings yielded by Sparse systemat-
ically degraded compared to the old embeddings,
contradicting the claim in (Faruqui et al., 2015).

Our method PSD-Reg-180K performed well
consistently, and is best in 4 similarity tasks.
It performed worse than word2vec on analogy
tasks, but still better than other MF-based meth-
ods. By comparing to PSD-Unreg-180K, we see
Tikhonov regularization brings 1-4% performance
boost across tasks. In addition, on similarity tasks,
online blockwise regression only degrades slightly
compared to batch factorization. Their perfor-
mance gaps on analogy tasks were wider, but this
might be explained by the fact that some hard
cases were not counted in PSD-25K’s evaluation,

due to its limited vocabulary.

7 Conclusions and Future Work

In this paper, inspired by the link functions in
previous works, with the support from Informa-
tion Theory, we propose a new link function of a
text window, parameterized by the embeddings of
words and the residuals of bigrams. Based on the
link function, we establish a generative model of
documents. The learning objective is to ﬁnd a set
of embeddings maximizing their posterior likeli-
hood given the corpus. This objective is reduced to
weighted low-rank positive-semideﬁnite approxi-
mation, subject to Tikhonov regularization. Then
we adopt a Block Coordinate Descent algorithm,
jointly with an online blockwise regression algo-
rithm to ﬁnd an approximate solution. On seven
benchmark sets,
the learned embeddings show
competitive and stable performance.

In the future work, we will incorporate global
latent factors into this generative model, such as
topics, sentiments, or writing styles, and develop
more elaborate models of documents. Through
learning such latent factors, important summary
information of documents would be acquired,
which are useful in various applications.

Acknowledgments

We thank Omer Levy, Thomas Mach, Peilin Zhao,
Mingkui Tan, Zhiqiang Xu and Chunlin Wu for
their helpful discussions and insights. This re-
search is supported by the National Research
Foundation, Prime Minister’s Ofﬁce, Singapore
under its IDM Futures Funding Initiative and ad-
ministered by the Interactive and Digital Media
Programme Ofﬁce.

Appendix A Possible Trap in SVD

Suppose M is the bigram matrix of interest. SVD
embeddings are derived from the low rank approx-
imation of M (cid:62)M , by keeping the largest singular
values/vectors. When some of these singular val-
ues correspond to negative eigenvalues, undesir-
able correlations might be captured. The follow-
ing is an example of approximating a PMI matrix.
A vocabulary consists of 3 words s1, s2, s3.

Two corpora derive two PMI matrices:
(cid:17)
M (1) =

, M (2) =

(cid:16) 1.4 0.8 0
0.8 2.6 0
0 2
0

(cid:16) 0.2 −1.6 0
−1.6 −2.2 0
2

0

0

(cid:17)

.

0

the largest

They have identical left singular matrix and sin-
gular values (3, 2, 1), but their eigenvalues are
(3, 2, 1) and (−3, 2, 1), respectively.
In a rank-2 approximation,

two
singular values/vectors are kept, and M (1) and
M (2) yield identical SVD embeddings V =
( 0.45 0.89 0
0 1 ) (the rows may be scaled depending on
the algorithm, without affecting the validity of the
following conclusion). The embeddings of s1 and
s2 (columns 1 and 2 of V ) point at the same di-
rection, suggesting they are positively correlated.
However as M (2)
2,1 = −1.6 < 0, they are
actually negatively correlated in the second cor-
pus. This inconsistency is because the principal
eigenvalue of M (2) is negative, and yet the corre-
sponding singular value/vector is kept.
When using eigendecomposition,

the largest
two positive eigenvalues/eigenvectors are kept.
M (1) yields the same embeddings V . M (2)
yields V (2) = (cid:0) −0.89 0.45 0
(cid:1) , which correctly
0 1.41
preserves the negative correlation between s1, s2.

1,2 = M (2)

0

Appendix B Information Theory

Redundant information refers to the reduced un-
certainty by knowing the value of any one of the
conditioning variables (hence redundant). Syner-
gistic information is the reduced uncertainty as-
cribed to knowing all the values of conditioning
variables, that cannot be reduced by knowing the
value of any variable alone (hence synergistic).

The mutual information I(y; xi) and the redun-

dant information Rdn(y; x1, x2) are deﬁned as:

I(y; xi) = EP (xi,y)[log

P (y|xi)
P (y)

]

(cid:20)
Rdn(y; x1, x2) = EP (y)

EP (xi|y)[log

min
x1,x2
The synergistic information Syn(y; x1, x2) is
deﬁned as the PI-function in (Williams and Beer,
2010), skipped here.

P (y|xi)
P (y)

(cid:21)

]

Figure 2: Different types of information among
3 random variables y, x1, x2.
I(y; x1, x2) is
the mutual information between y and (x1, x2).
Rdn(y; x1, x2) and Syn(y; x1, x2) are the redun-
dant information and synergistic information be-
tween x1, x2, conditioning y, respectively.

The interaction information Int(x1, x2, y) mea-
sures the relative strength of Rdn(y; x1, x2) and
Syn(y; x1, x2) (Timme et al., 2014):

Int(x1, x2, y)

=Syn(y; x1, x2) − Rdn(y; x1, x2)
=I(y; x1, x2) − I(y; x1) − I(y; x2)

=EP (x1,x2,y)[log

P (x1)P (x2)P (y)P (x1, x2, y)
P (x1, x2)P (x1, y)P (x2, y)

]

Figure 2 shows the relationship of different
information among 3 random variables y, x1, x2
(based on Fig.1 in (Williams and Beer, 2010)).

PMI is the pointwise counterpart of mutual
information I. Similarly, all the above concepts
have their pointwise counterparts, obtained by
dropping the expectation operator. Speciﬁcally,
the pointwise interaction information is deﬁned as
PInt(x1, x2, y) = PMI(y; x1, x2) − PMI(y; x1) −
log P (x1)P (x2)P (y)P (x1,x2,y)
PMI(y; x2)
P (x1,x2)P (x1,y)P (x2,y) .
If we know PInt(x1, x2, y), we can recover
PMI(y; x1, x2) from the mutual information over
the variable subsets, and then recover the joint
distribution P (x1, x2, y).

=

As

the

pointwise

redundant

interaction terms,

information
PRdn(y; x1, x2) and the pointwise synergistic
information PSyn(y; x1, x2) are both higher-
order
their magnitudes are
usually much smaller than the PMI terms. We
assume they are approximately equal, and thus
cancel each other when computing PInt. Given
this, PInt
In the case of three
words w0, w1, w2, PInt(w0, w1, w2) = 0 leads to
PMI(w2; w0, w1) = PMI(w2; w0)+PMI(w2; w1).

is always 0.

References

[Agirre et al.2009] Eneko Agirre, Enrique Alfonseca,
Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor
Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 19–27. Association for Computa-
tional Linguistics.

[Arora et al.2015] S. Arora, Y. Li, Y. Liang, T. Ma, and
A. Risteski. 2015. Random walks on discourse
spaces: a new generative language model with ap-
plications to semantic word embeddings. ArXiv e-
prints, arXiv:1502.03520 [cs.LG].

[Bengio et al.2006] Yoshua Bengio, Holger Schwenk,
Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-
Luc Gauvain. 2006. Neural probabilistic language
models. In Innovations in Machine Learning, pages
137–186. Springer.

[Blei et al.2003] David M Blei, Andrew Y Ng, and
Michael I Jordan. 2003. Latent dirichlet allocation.
The Journal of Machine Learning Research, 3:993–
1022.

[Bruni et al.2012] Elia Bruni, Gemma Boleda, Marco
Baroni, and Nam-Khanh Tran. 2012. Distributional
semantics in technicolor. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1, pages
136–145. Association for Computational Linguis-
tics.

[Deerwester et al.1990] Scott C. Deerwester, Susan T
Dumais, and Richard A. Harshman. 1990. Index-
ing by latent semantic analysis. J. Am. Soc. Inf. Sci.

[Dhillon et al.2011] Paramveer Dhillon, Dean P Foster,
and Lyle H Ungar. 2011. Multi-view learning of
In Proceedings of Ad-
word embeddings via cca.
vances in Neural Information Processing Systems,
pages 199–207.

[Dhillon et al.2015] Paramveer S Dhillon, Dean P Fos-
ter, and Lyle H Ungar. 2015. Eigenwords: Spectral
word embeddings. The Journal of Machine Learn-
ing Research.

[Faruqui et al.2015] Manaal Faruqui, Yulia Tsvetkov,
Dani Yogatama, Chris Dyer, and Noah A. Smith.
2015. Sparse overcomplete word vector represen-
tations. In Proceedings of ACL 2015.

[Hill et al.2014] Felix Hill, Roi Reichart, and Anna Ko-
rhonen. 2014. Simlex-999: Evaluating semantic
models with (genuine) similarity estimation. CoRR,
abs/1408.3456.

[Hinton2002] Geoffrey Hinton. 2002. Training prod-
ucts of experts by minimizing contrastive diver-
gence. Neural Computation, 14(8):1771–1800.

[Holland1973] Paul W. Holland.

1973. Weighted
Ridge Regression: Combining Ridge and Ro-
bust Regression Methods. NBER Working Papers
0011, National Bureau of Economic Research, Inc,
September.

[Hsu et al.2012] Daniel Hsu, Sham M Kakade, and
Tong Zhang. 2012. A spectral algorithm for learn-
ing hidden markov models. Journal of Computer
and System Sciences, 78(5):1460–1480.

[Levy and Goldberg2014] Omer Levy and Yoav Gold-
berg. 2014. Neural word embeddings as implicit
matrix factorization. In Proceedings of NIPS 2014.

[Levy et al.2014] Omer Levy, Yoav Goldberg, and Is-
rael Ramat-Gan. 2014. Linguistic regularities in
In Pro-
sparse and explicit word representations.
ceedings of CoNLL-2014, page 171.

[Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido
Dagan. 2015.
Improving distributional similarity
with lessons learned from word embeddings. Trans-
actions of the Association for Computational Lin-
guistics, 3:211–225.

[Lin and He2009] Chenghua Lin and Yulan He. 2009.
Joint sentiment/topic model for sentiment analysis.
In Proceedings of the 18th ACM conference on In-
formation and Knowledge Management, pages 375–
384. ACM.

[Luong et al.2013] Minh-Thang

Richard
Socher, and Christopher D Manning. 2013. Better
word representations with recursive neural networks
for morphology. CoNLL-2013, 104.

Luong,

[Mach2012] T. Mach. 2012. Eigenvalue Algorithms
for Symmetric Hierarchical Matrices. Dissertation,
Chemnitz University of Technology.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
mation of word representations in vector space. In
Proceedings of Workshop at ICLR 2013.

[Finkelstein et al.2002] Lev

Finkelstein,

Evgeniy
Gabrilovich, Yossi Matias, Ehud Rivlin, Zach
Solan, Gadi Wolfman, and Eytan Ruppin. 2002.
Placing search in context: The concept revisited.
ACM Trans. Inf. Syst., 20(1):116–131, January.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases
and their compositionality. In Proceedings of NIPS
2013, pages 3111–3119.

[Higham1988] Nicholas J. Higham. 1988. Comput-
ing a nearest symmetric positive semideﬁnite matrix.
Linear Algebra and its Applications, 103(0):103 –
118.

[Mikolov et al.2013c] Tomas Mikolov, Wen-tau Yih,
and Geoffrey Zweig. 2013c. Linguistic regularities
In Pro-
in continuous space word representations.
ceedings of HLT-NAACL 2013, pages 746–751.

[Zesch et al.2008] Torsten Zesch, Christof M¨uller, and
Iryna Gurevych. 2008. Using wiktionary for com-
In Proceedings of
puting semantic relatedness.
AAAI 2008, volume 8, pages 861–866.

[Zhai and Lafferty2004] Chengxiang Zhai and John
Lafferty. 2004. A study of smoothing methods
for language models applied to information retrieval.
ACM Transactions on Information Systems (TOIS),
22(2):179–214.

[Zhao et al.2011a] Peilin Zhao, Steven CH Hoi, and
Rong Jin. 2011a. Double updating online learn-
ing. The Journal of Machine Learning Research,
12:1587–1615.

[Zhao et al.2011b] Wayne Xin Zhao, Jing Jiang, Jian-
shu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and
Xiaoming Li. 2011b. Comparing twitter and tra-
ditional media using topic models. In Advances in
Information Retrieval (Proceedings of the 33rd An-
nual European Conference on Information Retrieval
Research), pages 338–349. Springer.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey
Hinton. 2007. Three new graphical models for sta-
In Proceedings of the
tistical language modelling.
24th International Conference on Machine learning,
pages 641–648. ACM.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. Proceedings
of the Empiricial Methods in Natural Language Pro-
cessing (EMNLP 2014), 12.

[Radinsky et al.2011] Kira

Eugene
Radinsky,
Agichtein,
and Shaul
Evgeniy Gabrilovich,
Markovitch. 2011. A word at a time: Computing
word relatedness using temporal semantic analysis.
In Proceedings of the 20th International Conference
on World Wide Web, WWW ’11, pages 337–346,
New York, NY, USA. ACM.

[Srebro et al.2003] Nathan Srebro, Tommi Jaakkola,
et al. 2003. Weighted low-rank approximations. In
Proceedings of ICML 2003, volume 3, pages 720–
727.

[Stratos et al.2014] Karl

Stratos, Do-kyum Kim,
Michael Collins, and Daniel Hsu. 2014. A spectral
algorithm for learning class-based n-gram models of
natural language. In Proceedings of the Association
for Uncertainty in Artiﬁcial Intelligence.

[Stratos et al.2015] Karl Stratos, Michael Collins, and
Daniel Hsu. 2015. Model-based word embeddings
from decompositions of count matrices. In Proceed-
ings of ACL 2015.

[Tan et al.2014] M. Tan, I. W. Tsang, L. Wang, B. Van-
dereycken, and S. J. Pan. 2014. Riemannian pursuit
In Proceedings of ICML
for big matrix recovery.
2014, pages 1539–1547.

[Timme et al.2014] Nicholas Timme, Wesley Alford,
Benjamin Flecker, and John M Beggs. 2014. Syn-
ergy, redundancy, and multivariate information mea-
sures: an experimentalist’s perspective. Journal of
Computational Neuroscience, 36(2):119–140.

[Wallach2006] Hanna M Wallach. 2006. Topic mod-
eling: beyond bag-of-words. In Proceedings of the
23rd international conference on Machine learning,
pages 977–984. ACM.

[Williams and Beer2010] Paul L Williams and Ran-
2010. Nonnegative decomposi-
arXiv preprint

dall D Beer.
tion of multivariate information.
arXiv:1004.2515.

[Yan et al.2015] Yan Yan, Mingkui Tan, Ivor Tsang,
Yi Yang, Chengqi Zhang, and Qinfeng Shi. 2015.
Scalable maximum margin matrix factorization by
active riemannian subspace search. In Proceedings
of IJCAI 2015.

[Yogatama et al.2015] Dani

Manaal
Faruqui, Chris Dyer, and Noah A Smith.
2015.
Learning word representations with hierarchical
sparse coding. In Proceedings of ICML 2015.

Yogatama,

