7
1
0
2
 
v
o
N
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
1
5
5
8
0
.
5
0
7
1
:
v
i
X
r
a

Safe Model-based Reinforcement Learning with
Stability Guarantees

Felix Berkenkamp
Department of Computer Science
ETH Zurich
befelix@inf.ethz.ch

Matteo Turchetta
Department of Computer Science,
ETH Zurich
matteotu@inf.ethz.ch

Angela P. Schoellig
Institute for Aerospace Studies
University of Toronto
schoellig@utias.utoronto.ca

Andreas Krause
Department of Computer Science
ETH Zurich
krausea@ethz.ch

Abstract

Reinforcement learning is a powerful paradigm for learning optimal policies from
experimental data. However, to ﬁnd optimal policies, most reinforcement learning
algorithms explore all possible actions, which may be harmful for real-world sys-
tems. As a consequence, learning algorithms are rarely applied on safety-critical
systems in the real world. In this paper, we present a learning algorithm that
explicitly considers safety, deﬁned in terms of stability guarantees. Speciﬁcally,
we extend control-theoretic results on Lyapunov stability veriﬁcation and show
how to use statistical models of the dynamics to obtain high-performance control
policies with provable stability certiﬁcates. Moreover, under additional regularity
assumptions in terms of a Gaussian process prior, we prove that one can effectively
and safely collect data in order to learn about the dynamics and thus both improve
control performance and expand the safe region of the state space. In our experi-
ments, we show how the resulting algorithm can safely optimize a neural network
policy on a simulated inverted pendulum, without the pendulum ever falling down.

1

Introduction

While reinforcement learning (RL, [1]) algorithms have achieved impressive results in games, for
example on the Atari platform [2], they are rarely applied to real-world physical systems (e.g., robots)
outside of academia. The main reason is that RL algorithms provide optimal policies only in the
long-term, so that intermediate policies may be unsafe, break the system, or harm their environment.
This is especially true in safety-critical systems that can affect human lives. Despite this, safety in RL
has remained largely an open problem [3].

Consider, for example, a self-driving car. While it is desirable for the algorithm that drives the
car to improve over time (e.g., by adapting to driver preferences and changing environments), any
policy applied to the system has to guarantee safe driving. Thus, it is not possible to learn about the
system through random exploratory actions, which almost certainly lead to a crash. In order to avoid
this problem, the learning algorithm needs to consider its ability to safely recover from exploratory
actions. In particular, we want the car to be able to recover to a safe state, for example, driving at a
reasonable speed in the middle of the lane. This ability to recover is known as asymptotic stability
in control theory [4]. Speciﬁcally, we care about the region of attraction of the closed-loop system
under a policy. This is a subset of the state space that is forward invariant so that any state trajectory
that starts within this set stays within it for all times and converges to a goal state eventually.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

In this paper, we present a RL algorithm for continuous state-action spaces that provides these kind
of high-probability safety guarantees for policies. In particular, we show how, starting from an initial,
safe policy we can expand our estimate of the region of attraction by collecting data inside the safe
region and adapt the policy to both increase the region of attraction and improve control performance.

Related work Safety is an active research topic in RL and different deﬁnitions of safety exist [5, 6].
Discrete Markov decision processes (MDPs) are one class of tractable models that have been analyzed.
In risk-sensitive RL, one speciﬁes risk-aversion in the reward [7]. For example, [8] deﬁne risk as
the probability of driving the agent to a set of known, undesirable states. Similarly, robust MDPs
maximize rewards when transition probabilities are uncertain [9, 10]. Both [11] and [12] introduce
algorithms to safely explore MDPs so that the agent never gets stuck without safe actions. All these
methods require an accurate probabilistic model of the system.

In continuous state-action spaces, model-free policy search algorithms have been successful. These
update policies without a system model by repeatedly executing the same task [13]. In this set-
ting, [14] introduces safety guarantees in terms of constraint satisfaction that hold in expectation.
High-probability worst-case safety guarantees are available for methods based on Bayesian optimiza-
tion [15] together with Gaussian process models (GP, [16]) of the cost function. The algorithms
in [17] and [18] provide high-probability safety guarantees for any parameter that is evaluated on
the real system. These methods are used in [19] to safely optimize a parametric control policy on a
quadrotor. However, resulting policies are task-speciﬁc and require the system to be reset.

In the model-based RL setting, research has focused on safety in terms of state constraints. In
[20, 21], a priori known, safe global backup policies are used, while [22] learns to switch between
several safe policies. However, it is not clear how one may ﬁnd these policies in the ﬁrst place.
Other approaches use model predictive control with constraints, a model-based technique where the
control actions are optimized online. For example, [23] models uncertain environmental constraints,
while [24] uses approximate uncertainty propagation of GP dynamics along trajectories. In this
setting, robust feasability and constraint satisfaction can be guaranteed for a learned model with
bounded errors using robust model predictive control [25]. The method in [26] uses reachability
analysis to construct safe regions in the state space. The theoretical guarantees depend on the solution
to a partial differential equation, which is approximated.

Theoretical guarantees for the stability exist for the more tractable stability analysis and veriﬁcation
under a ﬁxed control policy. In control, stability of a known system can be veriﬁed using a Lyapunov
function [27]. A similar approach is used by [28] for deterministic, but unknown dynamics that are
modeled as a GP, which allows for provably safe learning of regions of attraction for ﬁxed policies.
Similar results are shown in [29] for stochastic systems that are modeled as a GP. They use Bayesian
quadrature to compute provably accurate estimates of the region of attraction. These approaches do
not update the policy.

Our contributions We introduce a novel algorithm that can safely optimize policies in continuous
state-action spaces while providing high-probability safety guarantees in terms of stability. Moreover,
we show that it is possible to exploit the regularity properties of the system in order to safely learn
about the dynamics and thus improve the policy and increase the estimated safe region of attraction
without ever leaving it. Speciﬁcally, starting from a policy that is known to stabilize the system
locally, we gather data at informative, safe points and improve the policy safely based on the improved
model of the system and prove that any exploration algorithm that gathers data at these points reaches
a natural notion of full exploration. We show how the theoretical results transfer to a practical
algorithm with safety guarantees and apply it to a simulated inverted pendulum stabilization task.

2 Background and Assumptions

We consider a deterministic, discrete-time dynamic system

xt+1 = f (xt, ut) = h(xt, ut) + g(xt, ut),

(1)
N. The true
with states x
∈ U ⊂
consist of two parts: h(xt, ut) is a known, prior model that can be
dynamics f :
obtained from ﬁrst principles, while g(xt, ut) represents a priori unknown model errors. While the
model errors are unknown, we can obtain noisy measurements of f (x, u) by driving the system to
the state x and taking action u. We want this system to behave in a certain way, e.g., the car driving

Rp and a discrete time index t

Rq and control actions u

∈ X ⊂
X × U → X

∈

2

on the road. To this end, we need to specify a control policy π :
that, given the current state,
determines the appropriate control action that drives the system to some goal state, which we set as
the origin without loss of generality [4]. We encode the performance requirements of how to drive
the system to the origin through a positive cost r(x, u) that is associated with states and actions and
has r(0, 0) = 0. The policy aims to minimize the cumulative, discounted costs for each starting state.

X → U

The goal is to safely learn about the dynamics from measurements and adapt the policy for perfor-
mance, without encountering system failures. Speciﬁcally, we deﬁne the safety constraint on the
state divergence that occurs when leaving the region of attraction. This means that adapting the
policy is not allowed to decrease the region of attraction and exploratory actions to learn about the
) are not allowed to drive the system outside the region of attraction. The region of
dynamics f (
·
attraction is not known a priori, but is implicitly deﬁned through the system dynamics and the choice
of policy. Thus, the policy not only deﬁnes performance as in typical RL, but also determines safety
and where we can obtain measurements.

Model assumptions In general, this kind of safe learning is impossible without further assumptions.
For example, in a discontinuous system even a slight change in the control policy can lead to drastically
different behavior. Moreover, to expand the safe set we need to generalize learned knowledge about
the dynamics to (potentially unsafe) states that we have not visited. To this end, we restrict ourselves
to the general and practically relevant class of models that are Lipschitz continuous. This is a typical
assumption in the control community [4]. Additionally, to ensure that the closed-loop system remains
Lipschitz continuous when the control policy is applied, we restrict policies to the rich class of
Lπ-Lipschitz continuous functions ΠL, which also contains certain types of neural networks [30].
) in (1) are Lh- and Lg Lipschitz continuous
) and g(
Assumption 1 (continuity). The dynamics h(
·
·
with respect to the 1-norm. The considered control policies π lie in a set ΠL of functions that
are Lπ-Lipschitz continuous with respect to the 1-norm.

To enable safe learning, we require a reliable statistical model. While we commit to GPs for the
exploration analysis, for safety any suitable, well-calibrated model is applicable.
Assumption 2 (well-calibrated model). Let µn(
) denote the posterior mean and covari-
) and Σn(
·
·
ance matrix functions of the statistical model of the dynamics (1) conditioned on n noisy measurements.
) = trace(Σ1/2
n (
)), there exists a βn > 0 such that with probability at least (1
With σn(
δ) it holds
·
·
, and u
for all n
≥

−
This assumption ensures that we can build conﬁdence intervals on the dynamics that, when scaled by
an appropriate constant βn, cover the true function with high probability. We introduce a speciﬁc
statistical model that fulﬁlls both assumptions under certain regularity assumptions in Sec. 3.

µn(x, u)
1
(cid:107)

f (x, u)
(cid:107)

βnσn(x, u).

0, x

∈ X

∈ U

that

−

≤

≥

0

R

X →

∈ X \ {

Lyapunov function To satisfy the speciﬁed safety constraints for safe learning, we require a tool
to determine whether individual states and actions are safe. In control theory, this safety is deﬁned
through the region of attraction, which can be computed for a ﬁxed policy using Lyapunov func-
0 with v(0) = 0
tions [4]. Lyapunov functions are continuously differentiable functions v :
and v(x) > 0 for all x
. The key idea behind using Lyapunov functions to show stability
}
of the system (1) is similar to that of gradient descent on strictly quasiconvex functions: if one can
show that, given a policy π, applying the dynamics f on the state maps it to strictly smaller values
on the Lyapunov function (‘going downhill’), then the state eventually converges to the equilibrium
point at the origin (minimum). In particular, the assumptions in Theorem 1 below imply that v is
strictly quasiconvex within the region of attraction if the dynamics are Lipschitz continuous. As a
result, the one step decrease property for all states within a level set guarantees eventual convergence
to the origin.
Theorem 1 ([4]). Let v be a Lyapunov function, f Lipschitz continuous dynamics, and π a policy. If
x
v(f (x, π(x))) < v(x) for all x within the level set
, c > 0, then
c
∈ X \ {
}
} |
xt = 0.
(c) for all t > 0 and limt

(c) is a region of attraction, so that x0

V
It is convenient to characterize the region of attraction through a level set of the Lyapunov function,
since it replaces the challenging test for convergence with a one-step decrease condition on the
Lyapunov function. For the theoretical analysis in this paper, we assume that a Lyapunov function is
given to determine the region of attraction. For ease of notation, we also assume ∂v(x)/∂x
= 0 for
all x
(c) are connected if c > 0. Since Lyapunov functions
are continuously differentiable, they are Lv-Lipschitz continuous over the compact set

0, which ensures that level sets

(c) implies xt

{
∈ V

∈ X \

(c) =

v(x)

∈ V

→∞

≤

V

V

0

.

X

3

In general, it is not easy to ﬁnd suitable Lyapunov functions. However, for physical models, like the
prior model h in (1), the energy of the system (e.g., kinetic and potential for mechanical systems) is a
good candidate Lyapunov function. Moreover, it has recently been shown that it is possible to compute
suitable Lyapunov functions [31, 32]. In our experiments, we exploit the fact that value functions in
RL are Lyapunov functions if the costs are strictly positive away from the origin. This follows directly
v(f (x, π(x))).
from the deﬁnition of the value function, where v(x) = r(x, π(x)) + v(f (x, π(x))
Thus, we can obtain Lyapunov candidates as a by-product of approximate dynamic programming.

≤

Initial safe policy Lastly, we need to ensure that there exists a safe starting point for the learning
process. Thus, we assume that we have an initial policy π0 that renders the origin of the system in (1)
x
0 . For example, this policy may be designed
asymptotically stable within some small set of states
using the prior model h in (1), since most models are locally accurate but deteriorate in quality as
x
state magnitude increases. This policy is explicitly not safe to use throughout the state space
0 .

S

X \ S

3 Theory

In this section, we use these assumptions for safe reinforcement learning. We start by computing the
region of attraction for a ﬁxed policy under the statistical model. Next, we optimize the policy in order
to expand the region of attraction. Lastly, we show that it is possible to safely learn about the dynamics
and, under additional assumptions about the model and the system’s reachability properties, that this
approach expands the estimated region of attraction safely. We consider an idealized algorithm that is
amenable to analysis, which we convert to a practical variant in Sec. 4. See Fig. 1 for an illustrative
run of the algorithm and examples of the sets deﬁned below.

−

−

±

Q

1(x, u))

n(x, u) := [v(µn

Region of attraction We start by computing the region of attraction for a ﬁxed policy. This is an
extension of the method in [28] to discrete-time systems. We want to use the Lyapunov decrease condi-
tion in Theorem 1 to guarantee safety for the statistical model of the dynamics. However, the posterior
uncertainty in the statistical model of the dynamics means that one step predictions about v(f (
)) are
·
uncertain too. We account for this by constructing high-probability conﬁdence intervals on v(f (x, u)):
1(x, u)]. From Assumption 2 together with the Lipschitz
Q
property of v, we know that v(f (x, u)) is contained in
δ). For
our exploration analysis, we need to ensure that safe state-actions cannot become unsafe; that is, an
0 remains safe (deﬁned later). To this end, we intersect the conﬁdence intervals:
initial set of safe set
n(x, u), where the set
n(x, u) :=
L∆vτ )
C
−∞
0(x, u) = R otherwise. Note that v(f (x, u)) is contained in
n(x, u) with
when (x, u)
C
the same (1
)) are deﬁned
δ) probability as in Assumption 2. The upper and lower bounds on v(f (
·
as un(x, u) := max

S
1
∩ Q
0 and
C
n(x, u) and ln(x, u) := min
C

n(x, u) with probability at least (1

n
C
−
∈ S
−

is initialized to

0(x, u) = (

n(x, u).

Lvβnσn

, v(x)

∈ V

un(x) < v(x) for all x

Given these high-probability conﬁdence intervals, the system is stable according to Theorem 1 if
v(f (x, u))
(c). However, it is intractable to verify this condition
directly on the continuous domain without additional, restrictive assumptions about the model.
[x]τ (cid:107)
τ
Instead, we consider a discretization of the state space
τ with the smallest l1 distance to x. Given this
holds for all x
discretization, we bound the decrease variation on the Lyapunov function for states in
τ and use the
Lipschitz continuity to generalize to the continuous state space
Theorem 2. Under Assumptions 1 and 2 with L∆v := LvLf (Lπ + 1) + Lv, let
τ for all x
tion of
X
and for some n
x
(c) with probability at least (1

τ be a discretiza-
τ with c > 0, u = π(x),
∩ X
L∆vτ, then v(f (x, π(x))) < v(x) holds for all
(c) is a region of attraction for (1) under policy π.

(cid:107)
∈ X
≤
0 it holds that un(x, u) < v(x)

. Here, [x]τ denotes the point in

into cells, so that

. If, for all x

[x]τ (cid:107)

such that

δ) and

⊂ X

∈ X

∈ V

τ
X

(c)

−

−

−

≤

−

≤

≥

−

X

X

X

X

x

x

C

C

C

(cid:107)

1

1

.

∈ V

−

V

The proof is given in Appendix A.1. Theorem 2 states that, given conﬁdence intervals on the statistical
model of the dynamics, it is sufﬁcient to check the stricter decrease condition in Theorem 2 on the
τ to guarantee the requirements for the region of attraction in the continuous
discretized domain
domain in Theorem 1. The bound in Theorem 2 becomes tight as the discretization constant τ
un(
go to zero. Thus, the discretization constant trades off computation costs for
and
)) as we obtain more measurement data and the posterior model
accuracy, while un approaches v(f (
·
v(x)
uncertainty about the dynamics, √βnσn decreases. The conﬁdence intervals on v(f (x, π(x))
and the corresponding estimated region of attraction (red line) can be seen in the bottom half of Fig. 1.

v(f (
·
|

X
)
·

−

))

−

|

Policy optimization So far, we have focused on estimating the region of attraction for a ﬁxed policy.
Safety is a property of states under a ﬁxed policy. This means that the policy directly determines

4

(a) Initial safe set (in red).

(b) Exploration: 15 data points.

(c) Final policy after 30 evaluations.

Figure 1: Example application of Algorithm 1. Due to input constraints, the system becomes unstable
for large states. We start from an initial, local policy π0 that has a small, safe region of attraction (red
lines) in Fig. 1(a). The algorithm selects safe, informative state-action pairs within
n (top, white
(cn) (red lines) of the
shaded), which can be evaluated without leaving the region of attraction
current policy πn. As we gather more data (blue crosses), the uncertainty in the model decreases
(top, background) and we use (3) to update the policy so that it lies within
n (top, red shaded) and
fulﬁlls the Lyapunov decrease condition. The algorithm converges to the largest safe set in Fig. 1(c).
It improves the policy without evaluating unsafe state-action pairs and thereby without system failure.

D

V

S

which states are safe. Speciﬁcally, to form a region of attraction all states in the discretizaton
τ
X
within a level set of the Lyapunov function need to fulﬁll the decrease condition in Theorem 2 that
depends on the policy choice. The set of all state-action pairs that fulﬁll this decrease condition is
given by

n = (cid:8)(x, u)

D

τ
∈ X

× U |

un(x, u)

v(x) <

−

L∆vτ (cid:9),

−

(2)

see Fig. 1(c) (top, red shaded). In order to estimate the region of attraction based on this set, we
need to commit to a policy. Speciﬁcally, we want to pick the policy that leads to the largest possible
region of attraction according to Theorem 2. This requires that for each discrete state in
τ the
corresponding state-action pair under the policy must be in the set
n. Thus, we optimize the policy
according to

D

X

πn, cn = argmax
π

ΠL,c

R>0

c,

∈

∈

such that for all x

(c)

τ : (x, π(x))

∈ V

∩ X

n.

∈ D

(3)

V

The region of attraction that corresponds to the optimized policy πn according to (3) is given
(cn), see Fig. 1(b). It is the largest level set of the Lyapunov function for which all state-action
by
pairs (x, πn(x)) that correspond to discrete states within
n. This
D
(cn) is a region of
means that these state-action pairs fulﬁll the requirements of Theorem 2 and
attraction of the true system under policy πn. The following theorem is thus a direct consequence
of Theorem 2 and (3).
Theorem 3. Let
R
we have with probability at least (1

πn be the true region of attraction of (1) under the policy πn. For any δ

τ are contained in

πn for all n > 0.

δ) that

(cn)

(cn)

∩ X

(0, 1),

∈

V

V

−

V

⊆ R

Thus, when we optimize the policy subject to the constraint in (3) the estimated region of attraction is
always an inner approximation of the true region of attraction. However, solving the optimization
problem in (3) is intractable in general. We approximate the policy update step in Sec. 4.

Collecting measurements Given these stability guarantees, it is natural to ask how one might obtain
data points in order to improve the model of g(
) and thus efﬁciently increase the region of attraction.
·
This question is difﬁcult to answer in general, since it depends on the property of the statistical model.
In particular, for general statistical models it is often not clear whether the conﬁdence intervals
contract sufﬁciently quickly. In the following, we make additional assumptions about the model and
(cn) in order to provide exploration guarantees. These assumptions allow us to
reachability within
highlight fundamental requirements for safe data acquisition and that safe exploration is possible.

V

5

We assume that the unknown model errors g(
) have bounded norm in a reproducing kernel Hilbert
·
space (RKHS, [33]) corresponding to a differentiable kernel k,
Bg. These are a class of
well-behaved functions of the form g(z) = (cid:80)∞i=0 αik(zi, z) deﬁned through representer points zi
and weights αi that decay sufﬁciently fast with i. This assumption ensures that g satisﬁes the
Lipschitz property in Assumption 1, see [28]. Moreover, with βn = Bg + 4σ
γn + 1 + ln(1/δ) we
can use GP models for the dynamics that fulﬁll Assumption 2 if the state if fully observable and the
measurement noise is σ-sub-Gaussian (e.g., bounded in [
σ, σ]), see [34]. Here γn is the information
capacity. It corresponds to the amount of mutual information that can be obtained about g from nq
measurements, a measure of the size of the function class encoded by the model. The information
capacity has a sublinear dependence on n for common kernels and upper bounds can be computed
efﬁciently [35]. More details about this model are given in Appendix A.2.

g(
(cid:107)

k
(cid:107)

)
·

(cid:112)

−

≤

τ

U

⊂ U

. We deﬁne exploration as the number of state-action pairs in

In order to quantify the exploration properties of our algorithm, we consider a discrete action
space
τ that we can
safely learn about without leaving the true region of attraction. Note that despite this discretization,
the policy takes values on the continuous domain. Moreover, instead of using the conﬁdence intervals
directly as in (3), we consider an algorithm that uses the Lipschitz constants to slowly expand the safe
set. We use this in our analysis to quantify the ability to generalize beyond the current safe set. In
practice, nearby states are sufﬁciently correlated under the model to enable generalization using (2).

× U

X

τ

S

0 of state-action pairs about which we can learn safely. Speciﬁcally, this
Suppose we are given a set
means that we have a policy such that, for any state-action pair (x, u) in
0, if we apply action u in
state x and then apply actions according to the policy, the state converges to the origin. Such a set
can be constructed using the initial policy π0 from Sec. 2 as
. Starting
from this set, we want to update the policy to expand the region of attraction according to Theorem 2.
To this end, we use the conﬁdence intervals on v(f (
0 to determine state-action
pairs that fulﬁll the decrease condition. We thus redeﬁne

S
)) for states inside
·

S
(x, π0(x))
{

n for the exploration analysis to

x
0 }

0 =

∈ S

x

S

|

τ
∈ X

τ

× U

|

un(x, u)

−

z(cid:48)
(cid:107)

(x, u)
(cid:107)

−

1 <

−

L∆vτ (cid:9).

(4)

D
v(x) + L∆v

n =

D

(cid:91)

(cid:8)z(cid:48)

(x,u)

n−1

∈S

D

n, we can again ﬁnd a region of attraction

This formulation is equivalent to (2), except that it uses the Lipschitz constant to generalize safety.
(cn) by committing to a policy according to (3).
Given
In order to expand this region of attraction effectively we need to decrease the posterior model
uncertainty about the dynamics of the GP by collecting measurements. However, to ensure safety
(cn), but also need to ensure that
as outlined in Sec. 2, we are not only restricted to states within
the state after taking an action is safe; that is, the dynamics map the state back into the region of
attraction

(cn). We again use the Lipschitz constant in order to determine this set,

V

V

V

n =

S

(cid:91)

(cid:8)z(cid:48)

z

∈S

n−1

(cn)

∈ V

∩ X

τ

τ

× U

|

un(z) + LvLf

z
(cid:107)

−

z(cid:48)

1
(cid:107)

≤

cn

.

}

(5)

The set
leaving the region of attraction, see Fig. 1 (top, white shaded).

n contains state-action pairs that we can safely evaluate under the current policy πn without

S

What remains is to deﬁne a strategy for collecting data points within
n to effectively decrease model
uncertainty. We speciﬁcally focus on the high-level requirements for any exploration scheme without
committing to a speciﬁc method. In practice, any (model-based) exploration strategy that aims to
decrease model uncertainty by driving the system to speciﬁc states may be used. Safety can be
ensured by picking actions according to πn whenever the exploration strategy reaches the boundary
(cn); that is, when un(x, u) > cn. This way, we can use πn as a backup policy
of the safe region
for exploration.

V

S

The high-level goal of the exploration strategy is to shrink the conﬁdence intervals at state-action
n in order to expand the safe region. Speciﬁcally, the exploration strategy should aim to visit
pairs
state-action pairs in
n at which we are the most uncertain about the dynamics; that is, where the
conﬁdence interval is the largest:

S

S

(xn, un) = argmax

un(x, u)

ln(x, u).

(x,u)

n

∈S

−

(6)

As we keep collecting data points according to (6), we decrease the uncertainty about the dynamics
for different actions throughout the region of attraction and adapt the policy, until eventually we

6

Algorithm 1 SAFELYAPUNOVLEARNING
1: Input: Initial safe policy π0, dynamics model
2: for all n = 1, . . . do
3:
4:
5:
6:
7:

∀
(x, u)
(cn)
τ
S
× U
}
{
Select (xn, un) within
S
Update GP with measurements f (xn, un) + (cid:15)n

Compute policy πn via SGD on (7)
cn = argmaxc c, such that

(cn)
∈ V
un(x, u)

n =

∈ V

GP

x

|

τ : un(x, πn(x))
∩ X
cn
≤

−

v(x) <

L∆vτ

−

n using (6) and drive system there with backup policy πn

(µ(z), k(z, z(cid:48)))

have gathered enough information in order to expand it. While (6) implicitly assumes that any
(cn) can be reached by the exploration policy, it achieves the high-level goal of any
state within
exploration algorithm that aims to reduce model uncertainty. In practice, any safe exploration scheme
is limited by unreachable parts of the state space.

V

We compare the active learning scheme in (6) to an oracle baseline that starts from the same initial
0 and knows v(f (x, u)) up to (cid:15) accuracy within the safe set. The oracle also uses knowledge
safe set
about the Lipschitz constants and the optimal policy in ΠL at each iteration. We denote the set that this
0) and provide a detailed deﬁnition in Appendix A.3.
baseline manages to determine as safe with R(cid:15)(

S

S

(cid:112)

Theorem 4. Assume σ-sub-Gaussian measurement noise and that
in (1) has RKHS norm smaller than Bg.
with βn = Bg + 4σ
n∗ be the smallest positive integer so that
Let
following holds jointly with probability at least (1

)
·
Under the assumptions of Theorem 2,
γn + 1 + ln(1/δ), and with measurements collected according to (6), let
+1)
0)
R(
2).
S
|
v(cid:15)2
L2
(0, 1), the

n∗
β2
n∗ γn∗ ≥
π be the true region of attraction of (1) under a policy π. For any (cid:15) > 0, and δ

where C = 8/ log(1 + σ−

the model error g(

δ) for all n > 0:

Cq(
|

R

∈

(i)

(cn)

V

πn

⊆ R

(ii) f (x, u)

−
(x, u)

n.

∈ S

∈ R

πn ∀

(iii) R(cid:15)(

0)

n
⊆ S

⊆

R0(

0).

S

S

V

Theorem 4 states that, when selecting data points according to (6), the estimated region of attrac-
(cn) is (i) contained in the true region of attraction under the current policy and (ii) selected
tion
data points do not cause the system to leave the region of attraction. This means that any exploration
method that considers the safety constraint (5) is able to safely learn about the system without leaving
the region of attraction. The last part of Theorem 4, (iii), states that after a ﬁnite number of data
points n∗ we achieve at least the exploration performance of the oracle baseline, while we do not
classify unsafe state-action pairs as safe. This means that the algorithm explores the largest region
)) smaller
of attraction possible for a given Lyapunov function with residual uncertaint about v(f (
·
than (cid:15). Details of the comparison baseline are given in the appendix. In practice, this means that any
exploration method that manages to reduce the maximal uncertainty about the dynamics within
n is
able to expand the region of attraction.

S

An example run of repeatedly evaluating (6) for a one-dimensional state-space is shown in Fig. 1. It
can be seen that, by only selecting data points within the current estimate of the region of attraction,
the algorithm can efﬁciently optimize the policy and expand the safe region over time.

4 Practical Implementation and Experiments

In the previous section, we have given strong theoretical results on safety and exploration for an
idealized algorithm that can solve (3). In this section, we provide a practical variant of the theoretical
algorithm in the previous section. In particular, while we retain safety guarantees, we sacriﬁce
exploration guarantees to obtain a more practical algorithm. This is summarized in Algorithm 1.

The policy optimization problem in (3) is intractable to solve and only considers safety, rather
than a performance metric. We propose to use an approximate policy update that that maximizes
approximate performance while providing stability guarantees. It proceeds by optimizing the policy
(cn) for the new, ﬁxed policy. This does not
ﬁrst and then computes the region of attraction
impact safety, since data is still only collected inside the region of attraction. Moreover, should the
optimization fail and the region of attraction decrease, one can always revert to the previous policy,
which is guaranteed to be safe.

V

7

(a) Estimated safe set.

(b) State trajectory (lower is better).

Figure 2: Optimization results for an inverted pendulum. Fig. 2(a) shows the initial safe set (yellow)
under the policy π0, while the green region represents the estimated region of attraction under the
optimized neural network policy. It is contained within the true region of attraction (white). Fig. 2(b)
shows the improved performance of the safely learned policy over the policy for the prior model.

In our experiments, we use approximate dynamic programming [36] to capture the performance of the
policy. Given a policy πθ with parameters θ, we compute an estimate of the cost-to-go Jπθ (
) for the
·
mean dynamics µn based on the cost r(x, u)
0. At each state, Jπθ (x) is the sum of γ-discounted
rewards encountered when following the policy πθ. The goal is to adapt the parameters of the policy
for minimum cost as measured by Jπθ , while ensuring that the safety constraint on the worst-case
decrease on the Lyapunov function in Theorem 2 is not violated. A Lagrangian formulation to this
constrained optimization problem is

≥

πn = argmin
ΠL
πθ

∈

(cid:88)

x

τ
∈X

r(x, πθ(x)) + γJπθ (µn

1(x, πθ(x)) + λ

un(x, πθ(x))

v(x) + L∆vτ

, (7)

(cid:16)

(cid:17)

−

−

where the ﬁrst term measures long-term cost to go and λ
0 is a Lagrange multiplier for the safety
constraint from Theorem 2. In our experiments, we use the value function as a Lyapunov function
candidate, v = J with r(
0, and set λ = 1. In this case, (7) corresponds to an high-probability
·
upper bound on the cost-to-go given the uncertainty in the dynamics. This is similar to worst-case
performance formulations found in robust MDPs [9, 10], which consider worst-case value functions
given parametric uncertainty in MDP transition model. Moreover, since L∆v depends on the Lipschitz
constant of the policy, this simultaneously serves as a regularizer on the parameters θ.

≥

≥

)

·

,

S

To verify safety, we use the GP conﬁdence intervals ln and un directly, as in (2). We also use
n for the active learning scheme, see Algorithm 1, Line 5. In practice, we
conﬁdence to compute
n to solve (3), but can use a global optimization method or
do not need to compute the entire set
S
(cn) to ﬁnd suitable state-actions. Moreover, measurements
even a random sampling scheme within
V
(cn), see Fig. 1(c). As
for actions that are far away from the current policy are unlikely to expand
we optimize (7) via gradient descent, the policy changes only locally. Thus, we can achieve better
data-efﬁciency by restricting the exploratory actions u with (x, u)
[πn(x)

¯u, πn(x) + ¯u] for some constant ¯u.

n to be close to πn, u

∈ S

∈

V

−

Computing the region of attraction by verifying the stability condition on a discretized domain suffers
from the curse of dimensionality. However, it is not necessary to update policies in real time. In
particular, since any policy that is returned by the algorithm is provably safe within some level set,
any of these policies can be used safely for an arbitrary number of time steps. To scale this method to
higher-dimensional system, one would have to consider an adaptive discretization for the veriﬁcation
as in [27].

Experiments A Python implementation of Algorithm 1 and the experiments based on Tensor-
Flow [37] and GPﬂow [38] is available at https://github.com/befelix/safe_learning.

We verify our approach on an inverted pendulum benchmark problem. The true, continuous-time
λ ˙ψ + u, where ψ is the angle, m the mass, g the
dynamics are given by ml2 ¨ψ = gml sin(ψ)
gravitational constant, and u the torque applied to the pendulum. The control torque is limited, so that
the pendulum necessarily falls down beyond a certain angle. We use a GP model for the discrete-time
dynamics, where the mean dynamics are given by a linearized and discretized model of the true

−

8

dynamics that considers a wrong, lower mass and neglects friction. As a result, the optimal policy for
the mean dynamics does not perform well and has a small region of attraction as it underactuates the
system. We use a combination of linear and Matérn kernels in order to capture the model errors that
result from parameter and integration errors.

For the policy, we use a neural network with two hidden layers and 32 neurons with ReLU activations
each. We compute a conservative estimate of the Lipschitz constant as in [30]. We use standard
approximate dynamic programming with a quadratic, normalized cost r(x, u) = xTQx + uTRu,
where Q and R are positive-deﬁnite, to compute the cost-to-go Jπθ . Speciﬁcally, we use a piecewise-
linear triangulation of the state-space as to approximate Jπθ , see [39]. This allows us to quickly
verify the assumptions that we made about the Lyapunov function in Sec. 2 using a graph search. In
practice, one may use other function approximators. We optimize the policy via stochastic gradient
descent on (7).

The theoretical conﬁdence intervals for the GP model are conservative. To enable more data-efﬁcient
learning, we ﬁx βn = 2. This corresponds to a high-probability decrease condition per-state, rather
than jointly over the state space. Moreover, we use local Lipschitz constants of the Lyapunov function
rather than the global one. While this does not affect guarantees, it greatly speeds up exploration.

For the initial policy, we use approximate dynamic programming to compute the optimal policy for
the prior mean dynamics. This policy is unstable for large deviations from the initial state and has poor
performance, as shown in Fig. 2(b). Under this initial, suboptimal policy, the system is stable within
a small region of the state-space Fig. 2(a). Starting from this initial safe set, the algorithm proceeds to
collect safe data points and improve the policy. As the uncertainty about the dynamics decreases, the
policy improves and the estimated region of attraction increases. The region of attraction after 50 data
(cn) is contained within the true safe region of the
points is shown in Fig. 2(a). The resulting set
optimized policy πn. At the same time, the control performance improves drastically relative to the
initial policy, as can be seen in Fig. 2(b). Overall, the approach enables safe learning about dynamic
systems, as all data points collected during learning are safely collected under the current policy.

V

5 Conclusion

Acknowledgments

References

1998.

We have shown how classical reinforcement learning can be combined with safety constraints in terms
of stability. Speciﬁcally, we showed how to safely optimize policies and give stability certiﬁcates
based on statistical models of the dynamics. Moreover, we provided theoretical safety and exploration
guarantees for an algorithm that can drive the system to desired state-action pairs during learning. We
believe that our results present an important ﬁrst step towards safe reinforcement learning algorithms
that are applicable to real-world problems.

This research was supported by SNSF grant 200020_159557, the Max Planck ETH Center for
Learning Systems, NSERC grant RGPIN-2014-04634, and the Ontario Early Researcher Award.

[1] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. MIT press,

[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan
Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement
learning. Nature, 518(7540):529–533, 2015.

[3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

Concrete problems in AI safety. arXiv:1606.06565 [cs], 2016.

[4] Hassan K. Khalil and J. W. Grizzle. Nonlinear systems, volume 3. Prentice Hall, 1996.

9

[5] Martin Pecka and Tomas Svoboda. Safe exploration techniques for reinforcement learning –
an overview. In Modelling and Simulation for Autonomous Systems, pages 357–375. Springer,
2014.

[6] Javier García and Fernando Fernández. A comprehensive survey on safe reinforcement learning.

Journal of Machine Learning Research (JMLR), 16:1437–1480, 2015.

[7] Stefano P. Coraluppi and Steven I. Marcus. Risk-sensitive and minimax control of discrete-time,

ﬁnite-state Markov decision processes. Automatica, 35(2):301–309, 1999.

[8] Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under

constraints. J. Artif. Intell. Res.(JAIR), 24:81–108, 2005.

[9] Aviv Tamar, Shie Mannor, and Huan Xu. Scaling Up Robust MDPs by Reinforcement Learning.

In Proc. of the International Conference on Machine Learning (ICML), 2014.

[10] Wolfram Wiesemann, Daniel Kuhn, and Berç Rustem. Robust Markov Decision Processes.

Mathematics of Operations Research, 38(1):153–183, 2012.

[11] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in Markov decision processes. In
Proc. of the International Conference on Machine Learning (ICML), pages 1711–1718, 2012.

[12] Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in ﬁnite markov

decision processes with gaussian processes. pages 4305–4313, 2016.

[13] Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In Proc. of the IEEE/RSJ

International Conference on Intelligent Robots and Systems, pages 2219–2225, 2006.

[14] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.

In Proc. of the International Conference on Machine Learning (ICML), 2017.

[15] Jonas Mockus. Bayesian approach to global optimization, volume 37 of Mathematics and Its

Applications. Springer, Dordrecht, 1989.

[16] Carl Edward Rasmussen and Christopher K.I Williams. Gaussian processes for machine

learning. MIT Press, Cambridge MA, 2006.

[17] Jens Schreiter, Duy Nguyen-Tuong, Mona Eberts, Bastian Bischoff, Heiner Markert, and Marc
Toussaint. Safe exploration for active learning with Gaussian processes. In Machine Learning
and Knowledge Discovery in Databases, number 9286, pages 133–149. Springer International
Publishing, 2015.

[18] Yanan Sui, Alkis Gotovos, Joel W. Burdick, and Andreas Krause. Safe exploration for optimiza-
tion with Gaussian processes. In Proc. of the International Conference on Machine Learning
(ICML), pages 997–1005, 2015.

[19] Felix Berkenkamp, Angela P. Schoellig, and Andreas Krause. Safe controller optimization for
quadrotors with Gaussian processes. In Proc. of the IEEE International Conference on Robotics
and Automation (ICRA), pages 493–496, 2016.

[20] J. Garcia and F. Fernandez. Safe exploration of state and action spaces in reinforcement learning.

Journal of Artiﬁcial Intelligence Research, pages 515–564, 2012.

[21] Alexander Hans, Daniel Schneegaß, Anton Maximilian Schäfer, and Steffen Udluft. Safe
exploration for reinforcement learning. In Proc. of the European Symposium on Artiﬁcial
Neural Networks (ESANN), pages 143–148, 2008.

[22] Theodore J. Perkins and Andrew G. Barto. Lyapunov design for safe reinforcement learning.

The Journal of Machine Learning Research, 3:803–832, 2003.

[23] Dorsa Sadigh and Ashish Kapoor. Safe control under uncertainty with Probabilistic Signal

Temporal Logic. In Proc. of Robotics: Science and Systems, 2016.

10

[24] Chris J. Ostafew, Angela P. Schoellig, and Timothy D. Barfoot. Robust constrained learning-
based NMPC enabling reliable mobile robot path tracking. The International Journal of Robotics
Research (IJRR), 35(13):1547–1536, 2016.

[25] Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, and Claire Tomlin. Provably safe and

robust learning-based model predictive control. Automatica, 49(5):1216–1226, 2013.

[26] Anayo K. Akametalu, Shahab Kaynama, Jaime F. Fisac, Melanie N. Zeilinger, Jeremy H.
Gillula, and Claire J. Tomlin. Reachability-based safe learning with Gaussian processes. In
Proc. of the IEEE Conference on Decision and Control (CDC), pages 1424–1431, 2014.

[27] Ruxandra Bobiti and Mircea Lazar. A sampling approach to ﬁnding Lyapunov functions for
nonlinear discrete-time systems. In Proc. of the European Control Conference (ECC), pages
561–566, 2016.

[28] Felix Berkenkamp, Riccardo Moriconi, Angela P. Schoellig, and Andreas Krause. Safe learning
of regions of attraction in nonlinear systems with Gaussian processes. In Proc. of the Conference
on Decision and Control (CDC), pages 4661–4666, 2016.

[29] Julia Vinogradska, Bastian Bischoff, Duy Nguyen-Tuong, Henner Schmidt, Anne Romer, and
Jan Peters. Stability of controllers for Gaussian process forward models. In Proceedings of the
International Conference on Machine Learning (ICML), pages 545–554, 2016.

[30] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
fellow, and Rob Fergus. Intriguing properties of neural networks. In Proc. of the International
Conference on Learning Representations (ICLR), 2014.

[31] Huijuan Li and Lars Grüne. Computation of local ISS Lyapunov functions for discrete-time sys-
tems via linear programming. Journal of Mathematical Analysis and Applications, 438(2):701–
719, 2016.

[32] Peter Giesl and Sigurdur Hafstein. Review on computational methods for Lyapunov functions.

Discrete and Continuous Dynamical Systems, Series B, 20(8):2291–2337, 2015.

[33] Bernhard Schölkopf. Learning with kernels: support vector machines, regularization, optimiza-
tion, and beyond. Adaptive computation and machine learning. MIT Press, Cambridge, Mass,
2002.

[34] Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. arXiv preprint

arXiv:1704.00445, 2017.

[35] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias Seeger. Gaussian Process
Optimization in the Bandit Setting: No Regret and Experimental Design. IEEE Transactions on
Information Theory, 58(5):3250–3265, 2012.

[36] Warren B. Powell. Approximate dynamic programming: solving the curses of dimensionality.

John Wiley & Sons, 2007.

[37] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,
Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,
Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray,
Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale
Machine Learning on Heterogeneous Distributed Systems. arXiv:1603.04467 [cs], 2016.

[38] Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke Fujii, Alexis
Boukouvalas, Pablo León-Villagrá, Zoubin Ghahramani, and James Hensman. GPﬂow: a
Gaussian process library using TensorFlow. Journal of Machine Learning Research, 18(40):1–6,
2017.

11

[39] Scott Davies. Multidimensional triangulation and interpolation for reinforcement learning. In
Proc. of the Conference on Neural Information Processing Systems (NIPS), pages 1005–1011,
1996.

[40] Andreas Christmann and Ingo Steinwart. Support Vector Machines. Information Science and

Statistics. Springer, New York, NY, 2008.

A Proofs

A.1 Stability veriﬁcation

Lemma 1. Using Assumptions 1 and 2, let
for all x
. Then, for all x
(cid:12)
(cid:12)v(µn

∈ X
(cid:0)v(f (z))

∈ X
1([z]τ ))

v([x]τ )

X
v(x)(cid:1)(cid:12)
(cid:12)

−

−

−

−

≤

where z = (x, π(x)) and [z]τ = ([x]τ , π([x]τ )).

τ be a discretization of

X
, we have with probability at least 1

such that
δ that

x
(cid:107)

[x]τ (cid:107)

1

−

≤

τ

−

Lvβnσn

1([z]τ ) + (LvLf (Lπ + 1) + Lv)τ, (8)

−

Proof. Let z = (x, π(x)), [z]τ = ([x]τ , π([x]τ )), and µ = µn

1, σ = σn

1. Then we have that

−

−

−

−

−

−

(cid:0)v(f (z))
v(f (z)) + v(x)

(cid:12)
v([x]τ )
(cid:12)v(µ([z]τ ))
(cid:12)
v([x]τ )
(cid:12)v(µ([z]τ ))
=
= (cid:12)
v(f ([z]τ )) + v(f ([z]τ ))
(cid:12)v(µ([z]τ ))
v(f ([z]τ ))(cid:12)
(cid:12) + (cid:12)
(cid:12)
(cid:12)v(µ([z]τ ))
f ([z]τ )
µ([z]τ )
1 + Lv
Lv
(cid:107)
(cid:107)
[z]τ −
Lvβnσ([z]τ ) + LvLf
(cid:107)

−
(cid:12)v(f ([z]τ ))
f ([z]τ )
(cid:107)
z
1 + Lv
(cid:107)

v(x)(cid:1)(cid:12)
(cid:12),
(cid:12)
(cid:12),
v(f (z)) + v(x)
(cid:12) + (cid:12)
v(f (z))(cid:12)
−
f (z)
1 + Lv
(cid:107)
−
[x]τ (cid:107)
x
1,
(cid:107)

≤
≤

−

−

−

≤

−

−

v([x]τ )(cid:12)
(cid:12),
v([x]τ )(cid:12)
(cid:12),
−
[x]τ (cid:107)
1,

−
(cid:12)v(x)
x
(cid:107)

−

where the last three inequalities follow from Assumptions 1 and 2 to last inequality follows
from Lemma 3. The result holds with probability at least 1
δ. By deﬁnition of the discretization
and the policy class ΠL we have on each grid cell that

−

z

(cid:107)

−

[z]τ (cid:107)

1 =

[x]τ (cid:107)
x
(cid:107)
−
x
τ + Lπ
−
(cid:107)
(Lπ + 1)τ,

π(x)
1 +
(cid:107)
[x]τ (cid:107)
1,

≤
≤

π([x]τ )

1,
(cid:107)

−

where the equality in the ﬁrst step follows from the deﬁnition of the 1-norm. Plugging this into the
previous bound yields

(cid:12)
(cid:12)v(µ([z]τ ))

v([x]τ )

−

−

(cid:0)v(f (z))

v(x)(cid:1)(cid:12)
(cid:12)

−

≤

Lvβnσ([z]τ ) + (LvLf (1 + Lπ) + Lv) τ,

which completes the proof.

Lemma 2. v(f (x, u))

n holds for all x

, u

, and n > 0 with probability at least (1

δ).

∈ Q

∈ X

∈ U

−

Proof. The proof is analogous to Lemma 1 and follows from Assumptions 1 and 2.

Corollary 1. v(f (x, u))
(1

δ).

∈ C

−

n holds for all x

, u

, and n > 0 with probability at least

∈ X

∈ U

Proof. Direct consequence of the fact that Lemma 2 holds jointly for all n > 0 with probability at
least 1

δ.

−

Lemma 1 show that the decrease on the Lyapunov function on the discrete grid
on the continuous domain
X
attraction using Theorem 1:

τ is close to that
. Given these conﬁdence intervals, we can now establish the region of

X

12

Theorem 2. Under Assumptions 1 and 2 with L∆v := LvLf (Lπ + 1) + Lv, let
τ for all x
tion of
X
and for some n
x
(c) with probability at least (1

τ be a discretiza-
τ with c > 0, u = π(x),
∩ X
L∆vτ, then v(f (x, π(x))) < v(x) holds for all
(c) is a region of attraction for (1) under policy π.

(cid:107)
∈ X
≤
0 it holds that un(x, u) < v(x)

. If, for all x

[x]τ (cid:107)

such that

δ) and

∈ V

(c)

−

≥

−

X

x

1

∈ V

−

V

Proof. Using Lemma 1 it holds that v(f (x, π(x))
with probability at least 1
can use Theorem 1 to conclude that

δ, since all discrete states xτ

−

−

(c) is a region of attraction for (1).

∈ V

∩ X

v(x) < 0 for all continuous states x

(c)
fulﬁll Theorem 2. Thus we

∈ V

(c)

Theorem 3. Let
R
we have with probability at least (1

πn be the true region of attraction of (1) under the policy πn. For any δ

(0, 1),

∈

δ) that

(cn)

πn for all n > 0.

V

⊆ R

Proof. Following the deﬁnition of
problem (3) that for all x
v(x) <

L∆vτ , see (2). The result

∈ D

n in (2), it is clear from the constraint in the optimization

n it holds that (x, πn(x))

n or, equivalently that un(x, π(x))

(cn)

πn then follows from Theorem 2.

∈ D

−

−

⊆ R

Note that the initialization of the conﬁdence intervals
fulﬁlled for the initial policy.

Q

0 ensures that the decrease condition is always

V

−

D

V

A.2 Gaussian process model

One particular assumption that satisﬁes both the Lipschitz continuity and allows us to use GPs as a
model of the dynamics is that the model errors g(x, u) live in some reproducing kernel Hilbert space
(RKHS, [40]) corresponding to a differentiable kernel k and have RKHS norm smaller than Bg [35].
In our theoretical analysis, we use this assumption to prove exploration guarantees.

GP

(µ(z), k(z, z(cid:48))) is a distribution over well-behaved, smooth functions f :

R (see Re-
A
mark 1 for the vector-case, Rq) that is parameterized by a mean function µ and a covariance function
(kernel) k, which encodes assumptions about the functions [16]. In our case, the mean is given by
the prior model h, while the kernel corresponds to the one in the RKHS. Given noisy measurements
of the dynamics, ˆf (z) = f (z) + (cid:15) with z = (x, u) at locations An =
, corrupted
}
(0, σ2) (we relax the Gaussian noise assumption in our
by independent, Gaussian noise (cid:15)
∼ N
1yn,
analysis), the posterior is a GP distribution again with mean, µn(z) = kn(z)T(Kn + σ2I)−
kn(z)T(Kn + σ2I)−
n(z) = kn(z, z).
covariance kn(z, z(cid:48)) = k(z, z(cid:48))
The vector yn = [ ˆf (z1)
h(zn)]T contains observed, noisy deviations from
the mean, kn(z) = [k(z, z1), . . . , k(z, zn)] contains the covariances between the test input z and the
data points in

n has entries [Kn](i,j) = k(zi, zj), and I is the identity matrix.

1kn(z(cid:48)), and variance σ2

h(z1), . . . , ˆf (zn)

z1, . . . , zn
{

X × U →

−
Rn

n, Kn

−

−

×

D

∈

Remark 1. In the case of multiple output dimensions (q > 1), we consider a function with one-
dimensional output f (cid:48)(x, u, i) :

R, with the output dimension indexed by i

∈
. This allows us to use the standard deﬁnitions of the RKHS norm and GP model.
I
}
{
In this case, we deﬁne the GP posterior distribution as µn(z) = [µn(z, 1), . . . , µn(z, q)]T and
σn(z) = (cid:80)
q σn(z, i), where the unusual deﬁnition of the standard deviation is used in Lemma 3.

X × U × I →

1, . . . , q

=

1

i

≤

≤

Given the previous assumptions, it follows from [28, Lemma 2] that the dynamics in (1) are Lipschitz
continuous with Lipschitz constant Lf = Lh + Lg, where Lg depends on the properties (smoothness)
of the kernel.

Moreover, we can construct high-probability conﬁdence intervals on the dynamics in (1) that ful-
ﬁll Assumption 2 using the GP model.
Lemma 3. ([35, Theorem 6]) Assume σ-sub-Gaussian noise and that the model error g(
in (1) has RKHS norm bounded by Bg. Choose βn = Bg + 4σ
with probability at least 1
δ, δ
f (x, u)
µn
(cid:107)

)
·
γn + 1 + ln(1/δ). Then,
it holds that

1(x, u)
(cid:107)

∈
1(x, u).

for all n

, and u

−
βnσn

(0, 1),

1, x

∈ X

∈ U

(cid:112)

≥

≤

−

−

−

1

βnσn(x, u, i) holds with
Proof. From [34, Theorem 2] it follows that
probability at least 1
q. Following Remark 1, we can model the multi-output
function as a single-output function over an extended parameter space. Thus the result directly
transfers by deﬁnition of the one norm and our deﬁnition of σn for multiple output dimensions

δ for all 1

f (x, u, i)

1(x, u, i)

| ≤

µn

−

≤

−

≤

−

i

|

13

in Remark 1. Note that by iteration n we have obtained nq measurements in the information
capacity γn.

That is, the true dynamics are contained within the GP posterior conﬁdence intervals with high
probability. The bound depends on the information capacity,

γn =

max
:
⊂X ×U ×I

I(yA; fA),

A

=nq
|

A
|
which is the maximum mutual information that could be gained about the dynamics f from samples.
= t) for many commonly used kernels
The information capacity has a sublinear dependence on n(
such as the linear, squared exponential, and Matérn kernels and it can be efﬁciently and accurately
approximated [35]. Note that we explicitly account for the q measurements that we get for each of
the q states in (9).
Remark 2. The GP model assumes Gaussian noise, while Lemma 3 considers σ-sub-Gaussian noise.
Moreover, we consider functions with bounded RKHS norm, rather than samples from a GP. Lemma 3
thus states that even though we make different assumptions than the model, the conﬁdence intervals
are conservative enough to capture the true function with high probability.

(9)

A.3 Safe exploration

Remark 3. In the following we assume that

n and

n are deﬁned as in (4) and (5).

D

S

Baseline As a baseline, we consider a class of algorithms that know about the Lipschitz continuity
properties of v, f , and π. In addition, we can learn about v(f (x, u)) up to some arbitrary statistical
accuracy (cid:15) by visiting state x and obtaining a measurement for the next state after applying action u,
but face the safety restrictions deﬁned in Sec. 2. Suppose we are given a set
of state-action pairs
about which we can learn safely. Speciﬁcally, this means that we have a policy such that, for any
state-action pair (x, u) in
, if we apply action u in state x and then apply actions according to the
policy, the state converges to the origin. Such a set can be constructed using the initial policy π0
from Sec. 2 as

0 =

x

S

S
(x, π0(x))
{

|

∈ S

x
.
0 }

S

The goal of the algorithm is to expand this set of states that we can learn about safely. Thus, we need
L∆vτ decrease
to estimate the region of attraction by certifying that state-action pairs achieve the
. We can then generalize the gained
condition in Theorem 2 by learning about state-action pairs in
knowledge to unseen states by exploiting the Lipschitz continuity,
Rdec(

: v(f (x, u))

v(x)+(cid:15)+L∆v

(x, u)

(cid:8)z

) =

−

S

τ

S

0
S

∪

τ
∈ X

× U

| ∃

∈ S

z
(cid:107)

(x, u)
1 <
(cid:107)

−

−

−

L∆vτ (cid:9),
(10)

S

S

S

) =

Rlev(

. We speciﬁcally include

where we use that we can learn v(f (x, u)) up to (cid:15) accuracy within
0 in
this set, to allow for initial policies that are safe, but does not meet the strict decrease requirements
of Theorem 2. Given that all states in Rdec(
) fulﬁll the requirements of Theorem 2, we can estimate
ΠL and estimating the
the corresponding region of attraction by committing to a control policy π
largest safe level set of the Lyapunov function. With
(cid:0) argmax c,

∀
ΠL to determine the largest level set,
encodes this operation. It optimizes over safe policies π
∈
such that all state-action pairs (x, π(x)) at discrete states x in the level set
(c)
τ fulﬁll the
decrease condition of Theorem 2. As a result, Rlev(Rdec(
)) is an estimate of the largest region
of attraction given the (cid:15)-accurate knowledge about state-action pairs in
. Based on this increased
region of attraction, there are more states that we can safely learn about. Speciﬁcally, we again use
the Lipschitz constant and statistical accuracy (cid:15) to determine all states that map back into the region
of attraction,

), the operator

τ , (x, π(x))

= Rdec(

such that

D
ΠL :

S
(c)

∈ D

(11)

∩ X

∩ X

∈ V

D

∈

∈

V

V

x

S

S

π

∃

(cid:1)

R(cid:15)(

) =

S

S∪

(cid:8)z(cid:48)

Rlev

τ (Rdec(
S

))

∈

τ

z

× U

| ∃

∈ S

: v(f (z))+(cid:15)+LvLf

z(cid:48)

z
(cid:107)

−

1

(cid:107)

max
Rlev(Rdec(

≤
x
∈

v(x)(cid:9),

S

))
(12)

) = Rlev(

where Rlev
τ (
contains state-action pairs that we can visit to
⊇ S
learn about the system. Repeatedly applying this operator leads the largest set of state-action pairs
that any safe algorithm with the same knowledge and restricted to policies in ΠL could hope to reach.

τ . Thus, R(cid:15)(

∩ X

D

D

S

)

)

14

Algorithm 2 Theoretical algorithm

1: Input: Initial safe policy
S
2: for all n = 1, . . . do
(cid:8)z(cid:48)
n = (cid:83)
3:
4:
ΠL,c
5:

D
πn, cn = argmaxπ
(cid:8)z(cid:48)
n = (cid:83)
S
(cid:8)z(cid:48)
= (cid:83)

(x,u)

∈S

∈S

n−1

n−1

z

z

∈S

n−1

(xn, un) = argmax(x,u)

∈
(cn)
Rlev
τ (

∈
∈ V
∈

6:
7:
8:
9:

0, dynamics model

(µ(z), k(z, z(cid:48)))

GP

−

un(x, u)

τ
|
such that for all x

v(x) + L∆v
(c)
∈ V
un(z) + LvLf
z
(cid:107)
z
−
(cid:107)

τ
|
un(z) + LvLf

z(cid:48)

(x, u)
1 <
−
(cid:107)
τ : (x, π(x))
cn
≤
}
maxx
∈

(cid:107)
∩ X
z(cid:48)
1
(cid:107)
1
≤
(cid:107)

−
z(cid:48)

Rlev(

τ

−
n
∈ D

n) v(x)
}

D

L∆vτ (cid:9),

ln(x, u)

τ
∈ X
R>0 c,

× U

∩ X
× U
n)
τ
|
D
× U
n un(x, u)
−
cn

∈S
un(z)

n =

z

(cn)

τ
S
|
Update GP with measurements f (xn, un) + (cid:15)n

× U

∈ V

≤

{

}

) =

Speciﬁcally, let R0
) is the set
(cid:15) (
(
S
S
of all state-action pars on the discrete grid that any algorithm could hope to classify as safe without
leaving this safe set. Moreover, Rlev(R(cid:15)(
)) is the largest corresponding region of attraction that
S
any algorithm can classify as safe for the given Lyapunov function.

) = R(cid:15)(Ri
(cid:15)(

)). Then R(cid:15)(

and Ri+1

) = limi

Ri
(cid:15)(

→∞

S

S

S

S

(cid:15)

Proofs In the following we implicitly assume that the assumptions of Lemma 3 hold and that βn is
x
deﬁned as speciﬁed within Lemma 3. Moreover, for ease of notation we assume that
0 is a level set
of the Lyapunov function v(
).
·
(cn) = Rlev(
n) and cn = maxx
D
∈

Lemma 4.

n) v(x)

Rlev(

V

S

D

Proof. Directly by deﬁnition, compare (3) and (11).

Remark 4. Lemma 4 allows us to write the proofs entirely in terms of operators, rather than having
(cn) and cn according
to deal with explicit policies. In the following and in Algorithm 2 we replace
to Lemma 4. This moves the deﬁnitions closer to the baseline and makes for an easier comparison.

V

We roughly follow the proof strategy in [18], but deal with the additional complexity of having safe
sets that are deﬁned in a more difﬁcult way (indirectly through the policy). This is non-trivial and
the safe sets are carefully designed in order to ensure that the algorithm works for general nonlinear
systems.

We start by listing some fundamental properties of the sets that we deﬁned below.
Lemma 5. It holds for all n

1 that

≥
τ , un+1(z)

τ , ln+1(z)

Rlev(

S
Rdec(

)

)

⊆

un(z)

≤

ln(z)

≥
Rlev(

)
R
Rdec(

)
R

S
)

⊆
R(cid:15)(

⊆

R

)

)

R(cid:15)(

S

S

⇒
1 =

R(cid:15)(

)

R(cid:15)(

⊆
n+1

R
n
⊇ D

⇒ D

(i)

(ii)

(iii)

(iv)

(v)

(vi)

(vii)

(viii)

(ix)

(x)

z

z

∀

∀

τ
∈ X
τ
∈ X

S ⊆ R

S ⊆ R

S ⊆ R

S ⊆ R

× U

× U
=

⇒

⇒

⇒

=

=

=

n
S

n
⊇ S

−

1
D
n
S

0
⊇ S
n
⊇ S

1

−

n
D

n
⊇ D

−

1

Proof. (i) and (ii) follow directly form the deﬁnition of

n.

C

∈
(x, π(x))

(iii) Let π

ΠL be a policy such that for some c > 0 it holds for all x

(c)

τ that

∈ S

. Then we have that (x, π(x))
ΠL :
cs = argmax c

s.t.

π

∃

∈

∈ V
. Thus it follows that with

∩ X

, since
(c)

∈ V

∈ R
x
∀

S ⊆ R
∩ X

τ , (x, π(x))

∈ S

(13)

15

and

we have that cr

(iv) Let z
(x, u)

Rdec(
∈
1 <
(cid:107)

−

π

cr = argmax c

s.t.
∃
cs. This implies
V

≥
). Then there exists (x, u)

∈
(cr)

S
L∆vτ . Since

S ⊆ R
))

ΠL :

(c)

x
∀
∩ X
(cs). The result follows.
⊇ V

τ , (x, π(x))

∈ V

∈ R

∈ S
we have that (x, u)

such that v(f (x, u))

−
as well and thus z

v(x) + (cid:15) + L∆v
Rdec(

∈ R

∈

(v)

=

Rlev(Rdec(

⇒

S ⊆ R
must exist an z
Since

S ⊆ R

∈ S

it follows that z

Rlev(Rdec(
such that v(f (z)) + (cid:15) + LvLf

)) due to (iii) and (iv). Since z(cid:48)
z(cid:48)

R

⊆

S

z
(cid:107)

−

1
(cid:107)

≤

maxx
∈

R(cid:15)(
S
∈
Rlev(Rdec(
S

x
follows from Rlev(Rdec(

∈
))

))
S
Rlev(Rdec(

S

⊆

R

max
Rlev(Rdec(

x

≤
R
)), so that we conclude that z(cid:48)

))

∈

v(x)

R(cid:15)(

).

R

∈

. Moreover,

v(x)

∈ R
max
Rlev(Rdec(

(vi) This follows directly by repeatedly applying the result of (v).

(14)

−

z
(cid:107)
).
R
), there
)) v(x).

(15)

(vii) Let z(cid:48)

∈ D
n
⊇ S
−

n
S

∃

(x, u)

1 : un(x, u)

n. Then
n
∈ S
−
1 it follows that (x, u)
∈ S
un+1(x, u)
−
v(x) + L∆v
un(x, u)

−
n as well. Moreover, we have
v(x) + L∆v
z(cid:48)
(cid:107)
z(cid:48)

−
(x, u)

v(x) + L∆v

(x, u)

z(cid:48)

−

(cid:107)

since un+1 is non-increasing, see (i). Thus z(cid:48)

≤

−

1
(cid:107)
1 <
(cid:107)

L∆vτ

−
n+1.

(cid:107)
∈ D
0 that u0(x, u) < v(x)

−

(viii) By deﬁnition of

0 we have for all (x, u)

that

C

(x, u)
1 <
(cid:107)

−

L∆vτ . Since

L∆vτ . Now we have

−

∈ S

v(x) + L∆v
v(x),
v(x),

(x, u)
(cid:107)

(x, u)
(cid:107)

1,

−

by Lemma 5 (i)

≤
<
which implies that (x, u)

−
−
−

u1(x, u)
= u1(x, u)
u0(x, u)
L∆vτ,
1.

−
∈ D

(ix) Proof by induction. We consider the base case, z

1 by (viii).
x
0 is a level set of the Lyapunov function v by assumption, we have that
τ ,

Moreover, since
Rlev(
x
0 . The previous statements together with (iii) imply that z
0) =
S
S
0 by (viii). Now, we have that
since
1
⊇ S
D

0, which implies that z

∈ D
Rlev
τ (

× U

∈ S

1)

D

∈

S

Moreover, by deﬁnition of

As a consequence,

z

u1(z) + LvLf

z
(cid:107)
0, we have for all (x, u)
C
u0(x, u) < v(x)

−

1 = u1(z)
(cid:107)

∈ S

L∆vτ < v(x).

≤
0 that

−

(i)

u0(z).

u0(x, u)

v(x),

(x,u)

max
≤
0
∈S
= max
Rlev(
S
∈
max
Rlev(
D

≤

∈

x

x

0)

1)

v(x),

v(x),

where the last inequality follows from (iii) and (viii). Thus we have z

∈ S
For the induction step, assume that for n
n
n
⊇ S
S
−
Rlev
Rlev
z(cid:48)
n+1)
τ (
n)
τ (
D
∈
to Lemma 5 (iii) and (vii) together with the induction assumption of
n
n
⊇ S
S
there must exist a z

2 we have z(cid:48)
n with
τ . This implies that z(cid:48)

n we must have that z(cid:48)

≥
× U

∈ S

∈ S

D

1.

∈

−

1. Now since
τ , due
1. Moreover,

× U

n
∈ S

1
−
un+1(z) + LvLf

⊆ S

n such that
z(cid:48)
z
(cid:107)

−

1,
(cid:107)

which in turn implies z
together with the induction assumption that

∈ S

n+1)
n+1. The last inequality follows from Lemma 5 (iii) and (vii)

∈

x

un(z) + LvLf

z
(cid:107)
v(x),

z(cid:48)

1,
(cid:107)

−

≤
≤

≤

x

∈

n)

max
Rlev(
D
max
D

Rlev(

v(x),

n
S

n
⊇ S

−

1.

16

(16)

(17)

(18)

(19)
(20)

(21)

(x) This is a direct consequence of (vii), (viii), and (ix).

n does not expand after
Given these set properties, we ﬁrst consider what happens if the safe set
collecting data points. We use these results later to conclude that the safe set must either expand or
that the maximum level set is reached. We denote by

S

the data point the is sampled according to (6).
Lemma 6. For any n1

n1 =

1, if

n0

≥

≥

S

S

n0, then for any n such that n0

n < n1, it holds that

≤

zn = (xn, un)

2βnσn(zn)

≤

(cid:115)

C1qβ2
n

nγn
n0

,

−

where C1 = 8/ log(1 + σ−

2).

Proof. We modify the results for q = 1 by [35] to this lemma, but use the different deﬁnition for βn
from [34]. Even though the goal of [35, Lemma 5.4] is different from ours, we can still apply their
reasoning to bound the amplitude of the conﬁdence interval of the dynamics. In particular, in [35,
Lemma 5.4], we have rn = 2βnσn
−
nσ2
n = 4β2
r2
n
−
(cid:32) q
(cid:88)

1(zn) with zn = (xn, un) according to Lemma 3. Then

1(zn),

(23)

(cid:33)2

= 4β2
n

σn

1(zn, i)

,

−

i=1
q
(cid:88)

i=1

q
(cid:88)

i=1

≤

≤

4β2
nq

σ2
n

−

1(zn, i)

(Jensen’s ineq.),

4β2

nqσ2C2

log(1 + σ−

2σ2
n

1(zn, i)),

−

n
(cid:88)

j=1

r2
j ≤

C1β2

nqγn

n

∀

≥

1

where C2 = σ−
that

2/ log(1 + σ−

2). The result then follows analogously to [35, Lemma 5.4] by noting

according to the deﬁnition of γn in this paper and using the Cauchy-Schwarz inequality.

The previous result allows us to bound the width of the conﬁdence intervals:
Corollary 2. For any n1
that

n0, then for any n such that n0

n1 =

1, if

n0

≥

≥

S

S

≤

n < n1, it holds

un(zn)

ln(zn)

Lv

−

≤

(cid:115)

C1qβ2
n

nγn
n0

,

−

where C1 = 8/ log(1 + σ−

2).

Proof. Direct consequence of Lemma 6 together with the deﬁnition of

and

C

.
Q

Corollary 3. For any n
vq

β2

n+Nn

Nn
γn+Nn ≥

C1L2
(cid:15)2

≥
and

S

1 with C1 as deﬁned in Lemma 6, let Nn be the smallest integer satisfying

n+Nn =

Nn, then, for any z

n+Nn it holds that

S
un(z)

ln(z)

(cid:15).

≤

−

∈ S

Proof. The result trivially follows from substituting Nn in the bound in Corollary 2.

17

(22)

(24)

(25)

(26)

(27)

(28)

(29)

Lemma 7. For any n

1, if R(cid:15)(

0)

S

\ S

=

n

, then R(cid:15)(
∅

S

n)

\ S

=

n

.
∅

≥

Proof. As in [18, Lemma 6]. Assume, to the contrary, that R(cid:15)(
tion R(cid:15)(
S
limit R(cid:15)(
S

n, therefore R(cid:15)(
n. But then, by Lemma 5,(vi) and (ix), we get

n)
n) =

⊇ S
S

n) =

S

S

. By deﬁni-
∅
S
n. Iteratively applying R(cid:15) to both sides, we get in the

\ S

n =

n)

R(cid:15)(

0)

R(cid:15)(

n) =

n,

S

S
which contradicts the assumption that R(cid:15)(

Lemma 8. For any n
1

δ:

≥

1, if R(cid:15)(

0)

S

\ S

−

S
=

.

∅

n

\ S

⊆

0)

S

=

n

n+Nn ⊃ S
S

n.

, then the following holds with probability at least
∅

Proof. By Lemma 7, we have that R(cid:15)(
n and z(cid:48)
z
n such that

R(cid:15)(

n)

S

n)

∈

S

\ S

∈ S

n
\ S

=

. By deﬁnition, this means that there exist
∅

v(f (z(cid:48))) + (cid:15) + LvLf

z
(cid:107)

z(cid:48)

1
(cid:107)

−

max
Rlev(Rdec(

n))

S

≤

x

∈

v(x)

Now we assume, to the contrary, that
(ix)). This implies that z
follows that

∈ X

× U

τ

τ

S
\ S

n+Nn =
n+Nn and z(cid:48)

n (the safe set cannot decrease due to Lemma 5
1. Due to Corollary 2, it

n+Nn =

n+Nn

S

∈ S

S

−

by (32)

by (iii), (iv) and (ix)

z
un+Nn (z(cid:48)) + LvLf
(cid:107)
v(f (z(cid:48))) + (cid:15) + LvLf
v(x)

max
Rlev(Rdec(

x

n))

z(cid:48)

1
(cid:107)
z(cid:48)

−
z
(cid:107)

−

1
(cid:107)

v(x)

n+Nn ))

≤
≤
=

x

∈

S
max
Rlev(Rdec(
∈
S
to conclude that z
n+Nn )

Thus,
Rlev(

D
n+Nn ⊇
n+Nn =

⊇
Rdec(
S
n+Nn
−

S

D
S

∈

n)).

Rlev(Rdec(
S
n+Nn ). Consider (x, u)
1 such that

Rdec(

S

∈

n+Nn

according to (5), we need to show that
S
To this end, we use Lemma 5 (iii) and show that

n+Nn ), we know that there exists a (x(cid:48), u(cid:48))

−

L∆vτ > v(f (x(cid:48), u(cid:48)))

−
un+Nn(x(cid:48), u(cid:48))

v(x(cid:48)) + (cid:15) + L∆v
v(x(cid:48)) + L∆v

(x, u)
(cid:107)
(x, u)

−

(cid:107)

1,

−
(x(cid:48), u(cid:48))

(x(cid:48), u(cid:48))
(cid:107)
1,
(cid:107)

−

≥

where the second inequality follows from Corollary 2. This implies that (x, u)

n and thus

n+Nn ). This, in turn, implies that z

n+Nn , which is a contradiction.

∈ D

Rdec(

n+Nn ⊇

D
Lemma 9. For any n

S

≥

0, the following holds with probability at least 1

δ:

−

∈ S

n
S

⊆

R0(

0).

S

Proof. Proof by induction. For the base case, n = 0, we have

R0(

0) by deﬁnition.

0
S

⊆
R0(

S
0). Let z

S

∈ S

n. Then, by

For the induction step, assume that for some n
deﬁnition,

1 such that

z(cid:48)

1,

≥

1

n
S

−

⊆

∃

n
∈ S

−

un(z(cid:48)) + LvLf

z
(cid:107)

z(cid:48)

1

(cid:107)

−

≤

x

max
Rlev(
D

∈

n)

v(x),

which, by Corollary 1, implies that

v(f (z(cid:48))) + LvLf

z
(cid:107)

z(cid:48)

1
(cid:107)

−

≤

x

max
Rlev(
D

∈

n)

v(x)

Now since z(cid:48)
to show that Rlev(

∈

R0(

S
n)

D

⊆

Rlev(Rdec(R(

0))) .

S

0) by the induction hypothesis, in order to conclude that z

R0(

0) we need

∈

S

18

(30)

(31)

(32)

(33)

(34)
(35)

(36)

∈

(37)

(38)

(39)

(40)

(41)

Let (x, u)

∈ D

n, then there exist (x(cid:48), z(cid:48))
1(x(cid:48), u(cid:48))

n
∈ S
−
v(x(cid:48)) + L∆v

un

1 such that

−
which, by Corollary 1, implies that

−

(x, u)
(cid:107)

(x(cid:48), u(cid:48))
(cid:107)

−

1 <

L∆vτ,

−

v(f (x(cid:48), u(cid:48)))

−
Rdec(R0(

v(x(cid:48)) + L∆v

(x, u)
(cid:107)
0)) since

1

(x(cid:48), u(cid:48))
1 <
(cid:107)

−

−

L∆vτ,

which means that (x, u)
holds by the induction hypothesis. We use (iii) to conclude that Rlev(
which concludes the proof.

R0(

n
S

⊆

∈

S

S

−

n)

D

⊆

0) and therefore (x(cid:48), u(cid:48))

Rlev(Rdec(R(

∈

Lemma 10. Let n∗ be the smallest integer, such that n∗
δ.
such that

n0 holds with probability 1

n0+Nn0 =
S

S

−

R0(

Nn∗ . Then, there exists n0
0)
|

S

≥ |

≤

n∗

Proof. By contradiction. Assume, to the contrary, that for all n
(ix) we know that
must have

⊆ S

n
S

n+Nn . Since Nn is increasing in n, we have that Nn

⊂ S

n
S

n∗,

≤

≤

n+Nn . From Lemma 5
Nn∗ . Thus, we

so that for any 0

j

≤

≤ |

R0(

0)
|

S

> j. In particular, for j =

R0(
|

0)
|

S

, we get

2Nn∗ · · ·

,

0
S

⊂ S

, it holds that

Nn∗ ⊂ S
SjTn∗ |
|
R0(
>
jNn∗ |
|
0) from Lemma 9.

|S

,

0)
|

S

which contradicts

jNn∗ ⊆
S

R0(

S

Corollary 4. Let n∗ be the smallest integer such that
C1L2
vq

n∗

then there exists a n0

n∗ such that

≤

Proof. A direct consequence of Lemma 10 and Corollary 3.

0)
|

,

S

R0(
|
(cid:15)2

βn∗ γn∗ ≥

n0+Nn0 =
S

S

n0.

A.4 Safety and policy adaptation

(42)

(43)

R0(

0)
S
0))),

S

(44)

(45)

(46)

In the following, we denote the true region of attraction of (1) under a policy π by
Lemma 11. Rlev(

πn for all n

n)

0.

π.

R

D

⊆ R

≥

Proof. By deﬁnition, we have for all (x, u)

L∆vτ

−

un(x(cid:48), u(cid:48))
−
v(f (x(cid:48), u(cid:48)))
v(f (x, u))

≥

≥
≥

−
v(x),

−

∈ D
v(x(cid:48)) + L∆v

n that the exists (x(cid:48), u(cid:48))
(x, u)
(cid:107)
v(x(cid:48)) + L∆v

−
(x, u)
(cid:107)

−

n
∈ S
(x(cid:48), u(cid:48))
1,
(cid:107)
(x(cid:48), u(cid:48))
1,
(cid:107)

−

1 such that

where the ﬁrst inequality follows from Corollary 1 and the second one by Lipschitz continuity,
see Lemma 1.
By deﬁnition of Rlev in (11), it follows that for all x
Moreover, Rlev(
from Theorem 2.

n.
D
n) is a level set of the Lyapunov function by deﬁnition. Thus the result follows

τ we have that (x, πn(x))

Rlev(

∈ D

∩ X

n)

D

∈

Lemma 12. f (x, u)

∈ R

πn ∀

(x, u)

n.

∈ S

Proof. This holds for
z(cid:48)
1 such that

S

n
∈ S

−

0 by deﬁnition. For n

1, by deﬁtion, we have for all z

n there exists an

∈ S

max
Rlev(
D

x

∈

n)

v(x)

un(z(cid:48)) + LvLf

z
(cid:107)
v(f (z(cid:48))) + LvLf
v(f (z))

−
z
(cid:107)

z(cid:48)

1

(cid:107)
z(cid:48)

−

1
(cid:107)

where the ﬁrst inequality follows from Corollary 1 and the second one by Lipschitz continuity,
see Lemma 1. Since Rlev(

πn by Lemma 11, we have that f (z)

πn .

n)

D

⊆ R

∈ R

≥

≥

≥
≥

19

(cid:112)

Theorem 4. Assume σ-sub-Gaussian measurement noise and that
in (1) has RKHS norm smaller than Bg.
with βn = Bg + 4σ
n∗ be the smallest positive integer so that
Let
following holds jointly with probability at least (1

)
·
Under the assumptions of Theorem 2,
γn + 1 + ln(1/δ), and with measurements collected according to (6), let
R(
+1)
0)
2).
S
|
v(cid:15)2
L2
(0, 1), the

n∗
β2
n∗ γn∗ ≥
π be the true region of attraction of (1) under a policy π. For any (cid:15) > 0, and δ

where C = 8/ log(1 + σ−

the model error g(

δ) for all n > 0:

Cq(
|

R

∈

(i)

(cn)

V

πn

⊆ R

(ii) f (x, u)

−
(x, u)

n.

∈ S

∈ R

πn ∀

(iii) R(cid:15)(

0)

n
⊆ S

⊆

R0(

0).

S

S

Proof. See Lemmas 11 and 12 for (i) and (ii), respectively. Part (iii) is a direct consequence
of Corollary 4 and Lemma 9.

20

7
1
0
2
 
v
o
N
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
1
5
5
8
0
.
5
0
7
1
:
v
i
X
r
a

Safe Model-based Reinforcement Learning with
Stability Guarantees

Felix Berkenkamp
Department of Computer Science
ETH Zurich
befelix@inf.ethz.ch

Matteo Turchetta
Department of Computer Science,
ETH Zurich
matteotu@inf.ethz.ch

Angela P. Schoellig
Institute for Aerospace Studies
University of Toronto
schoellig@utias.utoronto.ca

Andreas Krause
Department of Computer Science
ETH Zurich
krausea@ethz.ch

Abstract

Reinforcement learning is a powerful paradigm for learning optimal policies from
experimental data. However, to ﬁnd optimal policies, most reinforcement learning
algorithms explore all possible actions, which may be harmful for real-world sys-
tems. As a consequence, learning algorithms are rarely applied on safety-critical
systems in the real world. In this paper, we present a learning algorithm that
explicitly considers safety, deﬁned in terms of stability guarantees. Speciﬁcally,
we extend control-theoretic results on Lyapunov stability veriﬁcation and show
how to use statistical models of the dynamics to obtain high-performance control
policies with provable stability certiﬁcates. Moreover, under additional regularity
assumptions in terms of a Gaussian process prior, we prove that one can effectively
and safely collect data in order to learn about the dynamics and thus both improve
control performance and expand the safe region of the state space. In our experi-
ments, we show how the resulting algorithm can safely optimize a neural network
policy on a simulated inverted pendulum, without the pendulum ever falling down.

1

Introduction

While reinforcement learning (RL, [1]) algorithms have achieved impressive results in games, for
example on the Atari platform [2], they are rarely applied to real-world physical systems (e.g., robots)
outside of academia. The main reason is that RL algorithms provide optimal policies only in the
long-term, so that intermediate policies may be unsafe, break the system, or harm their environment.
This is especially true in safety-critical systems that can affect human lives. Despite this, safety in RL
has remained largely an open problem [3].

Consider, for example, a self-driving car. While it is desirable for the algorithm that drives the
car to improve over time (e.g., by adapting to driver preferences and changing environments), any
policy applied to the system has to guarantee safe driving. Thus, it is not possible to learn about the
system through random exploratory actions, which almost certainly lead to a crash. In order to avoid
this problem, the learning algorithm needs to consider its ability to safely recover from exploratory
actions. In particular, we want the car to be able to recover to a safe state, for example, driving at a
reasonable speed in the middle of the lane. This ability to recover is known as asymptotic stability
in control theory [4]. Speciﬁcally, we care about the region of attraction of the closed-loop system
under a policy. This is a subset of the state space that is forward invariant so that any state trajectory
that starts within this set stays within it for all times and converges to a goal state eventually.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

In this paper, we present a RL algorithm for continuous state-action spaces that provides these kind
of high-probability safety guarantees for policies. In particular, we show how, starting from an initial,
safe policy we can expand our estimate of the region of attraction by collecting data inside the safe
region and adapt the policy to both increase the region of attraction and improve control performance.

Related work Safety is an active research topic in RL and different deﬁnitions of safety exist [5, 6].
Discrete Markov decision processes (MDPs) are one class of tractable models that have been analyzed.
In risk-sensitive RL, one speciﬁes risk-aversion in the reward [7]. For example, [8] deﬁne risk as
the probability of driving the agent to a set of known, undesirable states. Similarly, robust MDPs
maximize rewards when transition probabilities are uncertain [9, 10]. Both [11] and [12] introduce
algorithms to safely explore MDPs so that the agent never gets stuck without safe actions. All these
methods require an accurate probabilistic model of the system.

In continuous state-action spaces, model-free policy search algorithms have been successful. These
update policies without a system model by repeatedly executing the same task [13]. In this set-
ting, [14] introduces safety guarantees in terms of constraint satisfaction that hold in expectation.
High-probability worst-case safety guarantees are available for methods based on Bayesian optimiza-
tion [15] together with Gaussian process models (GP, [16]) of the cost function. The algorithms
in [17] and [18] provide high-probability safety guarantees for any parameter that is evaluated on
the real system. These methods are used in [19] to safely optimize a parametric control policy on a
quadrotor. However, resulting policies are task-speciﬁc and require the system to be reset.

In the model-based RL setting, research has focused on safety in terms of state constraints. In
[20, 21], a priori known, safe global backup policies are used, while [22] learns to switch between
several safe policies. However, it is not clear how one may ﬁnd these policies in the ﬁrst place.
Other approaches use model predictive control with constraints, a model-based technique where the
control actions are optimized online. For example, [23] models uncertain environmental constraints,
while [24] uses approximate uncertainty propagation of GP dynamics along trajectories. In this
setting, robust feasability and constraint satisfaction can be guaranteed for a learned model with
bounded errors using robust model predictive control [25]. The method in [26] uses reachability
analysis to construct safe regions in the state space. The theoretical guarantees depend on the solution
to a partial differential equation, which is approximated.

Theoretical guarantees for the stability exist for the more tractable stability analysis and veriﬁcation
under a ﬁxed control policy. In control, stability of a known system can be veriﬁed using a Lyapunov
function [27]. A similar approach is used by [28] for deterministic, but unknown dynamics that are
modeled as a GP, which allows for provably safe learning of regions of attraction for ﬁxed policies.
Similar results are shown in [29] for stochastic systems that are modeled as a GP. They use Bayesian
quadrature to compute provably accurate estimates of the region of attraction. These approaches do
not update the policy.

Our contributions We introduce a novel algorithm that can safely optimize policies in continuous
state-action spaces while providing high-probability safety guarantees in terms of stability. Moreover,
we show that it is possible to exploit the regularity properties of the system in order to safely learn
about the dynamics and thus improve the policy and increase the estimated safe region of attraction
without ever leaving it. Speciﬁcally, starting from a policy that is known to stabilize the system
locally, we gather data at informative, safe points and improve the policy safely based on the improved
model of the system and prove that any exploration algorithm that gathers data at these points reaches
a natural notion of full exploration. We show how the theoretical results transfer to a practical
algorithm with safety guarantees and apply it to a simulated inverted pendulum stabilization task.

2 Background and Assumptions

We consider a deterministic, discrete-time dynamic system

xt+1 = f (xt, ut) = h(xt, ut) + g(xt, ut),

(1)
N. The true
with states x
∈ U ⊂
consist of two parts: h(xt, ut) is a known, prior model that can be
dynamics f :
obtained from ﬁrst principles, while g(xt, ut) represents a priori unknown model errors. While the
model errors are unknown, we can obtain noisy measurements of f (x, u) by driving the system to
the state x and taking action u. We want this system to behave in a certain way, e.g., the car driving

Rp and a discrete time index t

Rq and control actions u

∈ X ⊂
X × U → X

∈

2

on the road. To this end, we need to specify a control policy π :
that, given the current state,
determines the appropriate control action that drives the system to some goal state, which we set as
the origin without loss of generality [4]. We encode the performance requirements of how to drive
the system to the origin through a positive cost r(x, u) that is associated with states and actions and
has r(0, 0) = 0. The policy aims to minimize the cumulative, discounted costs for each starting state.

X → U

The goal is to safely learn about the dynamics from measurements and adapt the policy for perfor-
mance, without encountering system failures. Speciﬁcally, we deﬁne the safety constraint on the
state divergence that occurs when leaving the region of attraction. This means that adapting the
policy is not allowed to decrease the region of attraction and exploratory actions to learn about the
) are not allowed to drive the system outside the region of attraction. The region of
dynamics f (
·
attraction is not known a priori, but is implicitly deﬁned through the system dynamics and the choice
of policy. Thus, the policy not only deﬁnes performance as in typical RL, but also determines safety
and where we can obtain measurements.

Model assumptions In general, this kind of safe learning is impossible without further assumptions.
For example, in a discontinuous system even a slight change in the control policy can lead to drastically
different behavior. Moreover, to expand the safe set we need to generalize learned knowledge about
the dynamics to (potentially unsafe) states that we have not visited. To this end, we restrict ourselves
to the general and practically relevant class of models that are Lipschitz continuous. This is a typical
assumption in the control community [4]. Additionally, to ensure that the closed-loop system remains
Lipschitz continuous when the control policy is applied, we restrict policies to the rich class of
Lπ-Lipschitz continuous functions ΠL, which also contains certain types of neural networks [30].
) in (1) are Lh- and Lg Lipschitz continuous
) and g(
Assumption 1 (continuity). The dynamics h(
·
·
with respect to the 1-norm. The considered control policies π lie in a set ΠL of functions that
are Lπ-Lipschitz continuous with respect to the 1-norm.

To enable safe learning, we require a reliable statistical model. While we commit to GPs for the
exploration analysis, for safety any suitable, well-calibrated model is applicable.
Assumption 2 (well-calibrated model). Let µn(
) denote the posterior mean and covari-
) and Σn(
·
·
ance matrix functions of the statistical model of the dynamics (1) conditioned on n noisy measurements.
) = trace(Σ1/2
n (
)), there exists a βn > 0 such that with probability at least (1
With σn(
δ) it holds
·
·
, and u
for all n
≥

−
This assumption ensures that we can build conﬁdence intervals on the dynamics that, when scaled by
an appropriate constant βn, cover the true function with high probability. We introduce a speciﬁc
statistical model that fulﬁlls both assumptions under certain regularity assumptions in Sec. 3.

µn(x, u)
1
(cid:107)

f (x, u)
(cid:107)

βnσn(x, u).

0, x

∈ X

∈ U

that

−

≤

≥

0

R

X →

∈ X \ {

Lyapunov function To satisfy the speciﬁed safety constraints for safe learning, we require a tool
to determine whether individual states and actions are safe. In control theory, this safety is deﬁned
through the region of attraction, which can be computed for a ﬁxed policy using Lyapunov func-
0 with v(0) = 0
tions [4]. Lyapunov functions are continuously differentiable functions v :
and v(x) > 0 for all x
. The key idea behind using Lyapunov functions to show stability
}
of the system (1) is similar to that of gradient descent on strictly quasiconvex functions: if one can
show that, given a policy π, applying the dynamics f on the state maps it to strictly smaller values
on the Lyapunov function (‘going downhill’), then the state eventually converges to the equilibrium
point at the origin (minimum). In particular, the assumptions in Theorem 1 below imply that v is
strictly quasiconvex within the region of attraction if the dynamics are Lipschitz continuous. As a
result, the one step decrease property for all states within a level set guarantees eventual convergence
to the origin.
Theorem 1 ([4]). Let v be a Lyapunov function, f Lipschitz continuous dynamics, and π a policy. If
x
v(f (x, π(x))) < v(x) for all x within the level set
, c > 0, then
c
∈ X \ {
}
} |
xt = 0.
(c) for all t > 0 and limt

(c) is a region of attraction, so that x0

V
It is convenient to characterize the region of attraction through a level set of the Lyapunov function,
since it replaces the challenging test for convergence with a one-step decrease condition on the
Lyapunov function. For the theoretical analysis in this paper, we assume that a Lyapunov function is
given to determine the region of attraction. For ease of notation, we also assume ∂v(x)/∂x
= 0 for
all x
(c) are connected if c > 0. Since Lyapunov functions
are continuously differentiable, they are Lv-Lipschitz continuous over the compact set

0, which ensures that level sets

(c) implies xt

{
∈ V

∈ X \

(c) =

v(x)

∈ V

→∞

≤

V

V

0

.

X

3

In general, it is not easy to ﬁnd suitable Lyapunov functions. However, for physical models, like the
prior model h in (1), the energy of the system (e.g., kinetic and potential for mechanical systems) is a
good candidate Lyapunov function. Moreover, it has recently been shown that it is possible to compute
suitable Lyapunov functions [31, 32]. In our experiments, we exploit the fact that value functions in
RL are Lyapunov functions if the costs are strictly positive away from the origin. This follows directly
v(f (x, π(x))).
from the deﬁnition of the value function, where v(x) = r(x, π(x)) + v(f (x, π(x))
Thus, we can obtain Lyapunov candidates as a by-product of approximate dynamic programming.

≤

Initial safe policy Lastly, we need to ensure that there exists a safe starting point for the learning
process. Thus, we assume that we have an initial policy π0 that renders the origin of the system in (1)
x
0 . For example, this policy may be designed
asymptotically stable within some small set of states
using the prior model h in (1), since most models are locally accurate but deteriorate in quality as
x
state magnitude increases. This policy is explicitly not safe to use throughout the state space
0 .

S

X \ S

3 Theory

In this section, we use these assumptions for safe reinforcement learning. We start by computing the
region of attraction for a ﬁxed policy under the statistical model. Next, we optimize the policy in order
to expand the region of attraction. Lastly, we show that it is possible to safely learn about the dynamics
and, under additional assumptions about the model and the system’s reachability properties, that this
approach expands the estimated region of attraction safely. We consider an idealized algorithm that is
amenable to analysis, which we convert to a practical variant in Sec. 4. See Fig. 1 for an illustrative
run of the algorithm and examples of the sets deﬁned below.

−

−

±

Q

1(x, u))

n(x, u) := [v(µn

Region of attraction We start by computing the region of attraction for a ﬁxed policy. This is an
extension of the method in [28] to discrete-time systems. We want to use the Lyapunov decrease condi-
tion in Theorem 1 to guarantee safety for the statistical model of the dynamics. However, the posterior
uncertainty in the statistical model of the dynamics means that one step predictions about v(f (
)) are
·
uncertain too. We account for this by constructing high-probability conﬁdence intervals on v(f (x, u)):
1(x, u)]. From Assumption 2 together with the Lipschitz
Q
property of v, we know that v(f (x, u)) is contained in
δ). For
our exploration analysis, we need to ensure that safe state-actions cannot become unsafe; that is, an
0 remains safe (deﬁned later). To this end, we intersect the conﬁdence intervals:
initial set of safe set
n(x, u), where the set
n(x, u) :=
L∆vτ )
C
−∞
0(x, u) = R otherwise. Note that v(f (x, u)) is contained in
n(x, u) with
when (x, u)
C
the same (1
)) are deﬁned
δ) probability as in Assumption 2. The upper and lower bounds on v(f (
·
as un(x, u) := max

S
1
∩ Q
0 and
C
n(x, u) and ln(x, u) := min
C

n(x, u) with probability at least (1

n
C
−
∈ S
−

is initialized to

0(x, u) = (

n(x, u).

Lvβnσn

, v(x)

∈ V

un(x) < v(x) for all x

Given these high-probability conﬁdence intervals, the system is stable according to Theorem 1 if
v(f (x, u))
(c). However, it is intractable to verify this condition
directly on the continuous domain without additional, restrictive assumptions about the model.
[x]τ (cid:107)
τ
Instead, we consider a discretization of the state space
τ with the smallest l1 distance to x. Given this
holds for all x
discretization, we bound the decrease variation on the Lyapunov function for states in
τ and use the
Lipschitz continuity to generalize to the continuous state space
Theorem 2. Under Assumptions 1 and 2 with L∆v := LvLf (Lπ + 1) + Lv, let
τ for all x
tion of
X
and for some n
x
(c) with probability at least (1

τ be a discretiza-
τ with c > 0, u = π(x),
∩ X
L∆vτ, then v(f (x, π(x))) < v(x) holds for all
(c) is a region of attraction for (1) under policy π.

(cid:107)
∈ X
≤
0 it holds that un(x, u) < v(x)

. Here, [x]τ denotes the point in

into cells, so that

. If, for all x

[x]τ (cid:107)

such that

δ) and

⊂ X

∈ X

∈ V

τ
X

(c)

−

−

−

≤

−

≤

≥

−

X

X

X

X

x

x

C

C

C

(cid:107)

1

1

.

∈ V

−

V

The proof is given in Appendix A.1. Theorem 2 states that, given conﬁdence intervals on the statistical
model of the dynamics, it is sufﬁcient to check the stricter decrease condition in Theorem 2 on the
τ to guarantee the requirements for the region of attraction in the continuous
discretized domain
domain in Theorem 1. The bound in Theorem 2 becomes tight as the discretization constant τ
un(
go to zero. Thus, the discretization constant trades off computation costs for
and
)) as we obtain more measurement data and the posterior model
accuracy, while un approaches v(f (
·
v(x)
uncertainty about the dynamics, √βnσn decreases. The conﬁdence intervals on v(f (x, π(x))
and the corresponding estimated region of attraction (red line) can be seen in the bottom half of Fig. 1.

v(f (
·
|

X
)
·

−

))

−

|

Policy optimization So far, we have focused on estimating the region of attraction for a ﬁxed policy.
Safety is a property of states under a ﬁxed policy. This means that the policy directly determines

4

(a) Initial safe set (in red).

(b) Exploration: 15 data points.

(c) Final policy after 30 evaluations.

Figure 1: Example application of Algorithm 1. Due to input constraints, the system becomes unstable
for large states. We start from an initial, local policy π0 that has a small, safe region of attraction (red
lines) in Fig. 1(a). The algorithm selects safe, informative state-action pairs within
n (top, white
(cn) (red lines) of the
shaded), which can be evaluated without leaving the region of attraction
current policy πn. As we gather more data (blue crosses), the uncertainty in the model decreases
(top, background) and we use (3) to update the policy so that it lies within
n (top, red shaded) and
fulﬁlls the Lyapunov decrease condition. The algorithm converges to the largest safe set in Fig. 1(c).
It improves the policy without evaluating unsafe state-action pairs and thereby without system failure.

D

V

S

which states are safe. Speciﬁcally, to form a region of attraction all states in the discretizaton
τ
X
within a level set of the Lyapunov function need to fulﬁll the decrease condition in Theorem 2 that
depends on the policy choice. The set of all state-action pairs that fulﬁll this decrease condition is
given by

n = (cid:8)(x, u)

D

τ
∈ X

× U |

un(x, u)

v(x) <

−

L∆vτ (cid:9),

−

(2)

see Fig. 1(c) (top, red shaded). In order to estimate the region of attraction based on this set, we
need to commit to a policy. Speciﬁcally, we want to pick the policy that leads to the largest possible
region of attraction according to Theorem 2. This requires that for each discrete state in
τ the
corresponding state-action pair under the policy must be in the set
n. Thus, we optimize the policy
according to

D

X

πn, cn = argmax
π

ΠL,c

R>0

c,

∈

∈

such that for all x

(c)

τ : (x, π(x))

∈ V

∩ X

n.

∈ D

(3)

V

The region of attraction that corresponds to the optimized policy πn according to (3) is given
(cn), see Fig. 1(b). It is the largest level set of the Lyapunov function for which all state-action
by
pairs (x, πn(x)) that correspond to discrete states within
n. This
D
(cn) is a region of
means that these state-action pairs fulﬁll the requirements of Theorem 2 and
attraction of the true system under policy πn. The following theorem is thus a direct consequence
of Theorem 2 and (3).
Theorem 3. Let
R
we have with probability at least (1

πn be the true region of attraction of (1) under the policy πn. For any δ

τ are contained in

πn for all n > 0.

δ) that

(cn)

(cn)

∩ X

(0, 1),

∈

V

V

−

V

⊆ R

Thus, when we optimize the policy subject to the constraint in (3) the estimated region of attraction is
always an inner approximation of the true region of attraction. However, solving the optimization
problem in (3) is intractable in general. We approximate the policy update step in Sec. 4.

Collecting measurements Given these stability guarantees, it is natural to ask how one might obtain
data points in order to improve the model of g(
) and thus efﬁciently increase the region of attraction.
·
This question is difﬁcult to answer in general, since it depends on the property of the statistical model.
In particular, for general statistical models it is often not clear whether the conﬁdence intervals
contract sufﬁciently quickly. In the following, we make additional assumptions about the model and
(cn) in order to provide exploration guarantees. These assumptions allow us to
reachability within
highlight fundamental requirements for safe data acquisition and that safe exploration is possible.

V

5

We assume that the unknown model errors g(
) have bounded norm in a reproducing kernel Hilbert
·
space (RKHS, [33]) corresponding to a differentiable kernel k,
Bg. These are a class of
well-behaved functions of the form g(z) = (cid:80)∞i=0 αik(zi, z) deﬁned through representer points zi
and weights αi that decay sufﬁciently fast with i. This assumption ensures that g satisﬁes the
Lipschitz property in Assumption 1, see [28]. Moreover, with βn = Bg + 4σ
γn + 1 + ln(1/δ) we
can use GP models for the dynamics that fulﬁll Assumption 2 if the state if fully observable and the
measurement noise is σ-sub-Gaussian (e.g., bounded in [
σ, σ]), see [34]. Here γn is the information
capacity. It corresponds to the amount of mutual information that can be obtained about g from nq
measurements, a measure of the size of the function class encoded by the model. The information
capacity has a sublinear dependence on n for common kernels and upper bounds can be computed
efﬁciently [35]. More details about this model are given in Appendix A.2.

g(
(cid:107)

k
(cid:107)

)
·

(cid:112)

−

≤

τ

U

⊂ U

. We deﬁne exploration as the number of state-action pairs in

In order to quantify the exploration properties of our algorithm, we consider a discrete action
space
τ that we can
safely learn about without leaving the true region of attraction. Note that despite this discretization,
the policy takes values on the continuous domain. Moreover, instead of using the conﬁdence intervals
directly as in (3), we consider an algorithm that uses the Lipschitz constants to slowly expand the safe
set. We use this in our analysis to quantify the ability to generalize beyond the current safe set. In
practice, nearby states are sufﬁciently correlated under the model to enable generalization using (2).

× U

X

τ

S

0 of state-action pairs about which we can learn safely. Speciﬁcally, this
Suppose we are given a set
means that we have a policy such that, for any state-action pair (x, u) in
0, if we apply action u in
state x and then apply actions according to the policy, the state converges to the origin. Such a set
can be constructed using the initial policy π0 from Sec. 2 as
. Starting
from this set, we want to update the policy to expand the region of attraction according to Theorem 2.
To this end, we use the conﬁdence intervals on v(f (
0 to determine state-action
pairs that fulﬁll the decrease condition. We thus redeﬁne

S
)) for states inside
·

S
(x, π0(x))
{

n for the exploration analysis to

x
0 }

0 =

∈ S

x

S

|

τ
∈ X

τ

× U

|

un(x, u)

−

z(cid:48)
(cid:107)

(x, u)
(cid:107)

−

1 <

−

L∆vτ (cid:9).

(4)

D
v(x) + L∆v

n =

D

(cid:91)

(cid:8)z(cid:48)

(x,u)

n−1

∈S

D

n, we can again ﬁnd a region of attraction

This formulation is equivalent to (2), except that it uses the Lipschitz constant to generalize safety.
(cn) by committing to a policy according to (3).
Given
In order to expand this region of attraction effectively we need to decrease the posterior model
uncertainty about the dynamics of the GP by collecting measurements. However, to ensure safety
(cn), but also need to ensure that
as outlined in Sec. 2, we are not only restricted to states within
the state after taking an action is safe; that is, the dynamics map the state back into the region of
attraction

(cn). We again use the Lipschitz constant in order to determine this set,

V

V

V

n =

S

(cid:91)

(cid:8)z(cid:48)

z

∈S

n−1

(cn)

∈ V

∩ X

τ

τ

× U

|

un(z) + LvLf

z
(cid:107)

−

z(cid:48)

1
(cid:107)

≤

cn

.

}

(5)

The set
leaving the region of attraction, see Fig. 1 (top, white shaded).

n contains state-action pairs that we can safely evaluate under the current policy πn without

S

What remains is to deﬁne a strategy for collecting data points within
n to effectively decrease model
uncertainty. We speciﬁcally focus on the high-level requirements for any exploration scheme without
committing to a speciﬁc method. In practice, any (model-based) exploration strategy that aims to
decrease model uncertainty by driving the system to speciﬁc states may be used. Safety can be
ensured by picking actions according to πn whenever the exploration strategy reaches the boundary
(cn); that is, when un(x, u) > cn. This way, we can use πn as a backup policy
of the safe region
for exploration.

V

S

The high-level goal of the exploration strategy is to shrink the conﬁdence intervals at state-action
n in order to expand the safe region. Speciﬁcally, the exploration strategy should aim to visit
pairs
state-action pairs in
n at which we are the most uncertain about the dynamics; that is, where the
conﬁdence interval is the largest:

S

S

(xn, un) = argmax

un(x, u)

ln(x, u).

(x,u)

n

∈S

−

(6)

As we keep collecting data points according to (6), we decrease the uncertainty about the dynamics
for different actions throughout the region of attraction and adapt the policy, until eventually we

6

Algorithm 1 SAFELYAPUNOVLEARNING
1: Input: Initial safe policy π0, dynamics model
2: for all n = 1, . . . do
3:
4:
5:
6:
7:

∀
(x, u)
(cn)
τ
S
× U
}
{
Select (xn, un) within
S
Update GP with measurements f (xn, un) + (cid:15)n

Compute policy πn via SGD on (7)
cn = argmaxc c, such that

(cn)
∈ V
un(x, u)

n =

∈ V

GP

x

|

τ : un(x, πn(x))
∩ X
cn
≤

−

v(x) <

L∆vτ

−

n using (6) and drive system there with backup policy πn

(µ(z), k(z, z(cid:48)))

have gathered enough information in order to expand it. While (6) implicitly assumes that any
(cn) can be reached by the exploration policy, it achieves the high-level goal of any
state within
exploration algorithm that aims to reduce model uncertainty. In practice, any safe exploration scheme
is limited by unreachable parts of the state space.

V

We compare the active learning scheme in (6) to an oracle baseline that starts from the same initial
0 and knows v(f (x, u)) up to (cid:15) accuracy within the safe set. The oracle also uses knowledge
safe set
about the Lipschitz constants and the optimal policy in ΠL at each iteration. We denote the set that this
0) and provide a detailed deﬁnition in Appendix A.3.
baseline manages to determine as safe with R(cid:15)(

S

S

(cid:112)

Theorem 4. Assume σ-sub-Gaussian measurement noise and that
in (1) has RKHS norm smaller than Bg.
with βn = Bg + 4σ
n∗ be the smallest positive integer so that
Let
following holds jointly with probability at least (1

)
·
Under the assumptions of Theorem 2,
γn + 1 + ln(1/δ), and with measurements collected according to (6), let
+1)
0)
R(
2).
S
|
v(cid:15)2
L2
(0, 1), the

n∗
β2
n∗ γn∗ ≥
π be the true region of attraction of (1) under a policy π. For any (cid:15) > 0, and δ

where C = 8/ log(1 + σ−

the model error g(

δ) for all n > 0:

Cq(
|

R

∈

(i)

(cn)

V

πn

⊆ R

(ii) f (x, u)

−
(x, u)

n.

∈ S

∈ R

πn ∀

(iii) R(cid:15)(

0)

n
⊆ S

⊆

R0(

0).

S

S

V

Theorem 4 states that, when selecting data points according to (6), the estimated region of attrac-
(cn) is (i) contained in the true region of attraction under the current policy and (ii) selected
tion
data points do not cause the system to leave the region of attraction. This means that any exploration
method that considers the safety constraint (5) is able to safely learn about the system without leaving
the region of attraction. The last part of Theorem 4, (iii), states that after a ﬁnite number of data
points n∗ we achieve at least the exploration performance of the oracle baseline, while we do not
classify unsafe state-action pairs as safe. This means that the algorithm explores the largest region
)) smaller
of attraction possible for a given Lyapunov function with residual uncertaint about v(f (
·
than (cid:15). Details of the comparison baseline are given in the appendix. In practice, this means that any
exploration method that manages to reduce the maximal uncertainty about the dynamics within
n is
able to expand the region of attraction.

S

An example run of repeatedly evaluating (6) for a one-dimensional state-space is shown in Fig. 1. It
can be seen that, by only selecting data points within the current estimate of the region of attraction,
the algorithm can efﬁciently optimize the policy and expand the safe region over time.

4 Practical Implementation and Experiments

In the previous section, we have given strong theoretical results on safety and exploration for an
idealized algorithm that can solve (3). In this section, we provide a practical variant of the theoretical
algorithm in the previous section. In particular, while we retain safety guarantees, we sacriﬁce
exploration guarantees to obtain a more practical algorithm. This is summarized in Algorithm 1.

The policy optimization problem in (3) is intractable to solve and only considers safety, rather
than a performance metric. We propose to use an approximate policy update that that maximizes
approximate performance while providing stability guarantees. It proceeds by optimizing the policy
(cn) for the new, ﬁxed policy. This does not
ﬁrst and then computes the region of attraction
impact safety, since data is still only collected inside the region of attraction. Moreover, should the
optimization fail and the region of attraction decrease, one can always revert to the previous policy,
which is guaranteed to be safe.

V

7

(a) Estimated safe set.

(b) State trajectory (lower is better).

Figure 2: Optimization results for an inverted pendulum. Fig. 2(a) shows the initial safe set (yellow)
under the policy π0, while the green region represents the estimated region of attraction under the
optimized neural network policy. It is contained within the true region of attraction (white). Fig. 2(b)
shows the improved performance of the safely learned policy over the policy for the prior model.

In our experiments, we use approximate dynamic programming [36] to capture the performance of the
policy. Given a policy πθ with parameters θ, we compute an estimate of the cost-to-go Jπθ (
) for the
·
mean dynamics µn based on the cost r(x, u)
0. At each state, Jπθ (x) is the sum of γ-discounted
rewards encountered when following the policy πθ. The goal is to adapt the parameters of the policy
for minimum cost as measured by Jπθ , while ensuring that the safety constraint on the worst-case
decrease on the Lyapunov function in Theorem 2 is not violated. A Lagrangian formulation to this
constrained optimization problem is

≥

πn = argmin
ΠL
πθ

∈

(cid:88)

x

τ
∈X

r(x, πθ(x)) + γJπθ (µn

1(x, πθ(x)) + λ

un(x, πθ(x))

v(x) + L∆vτ

, (7)

(cid:16)

(cid:17)

−

−

where the ﬁrst term measures long-term cost to go and λ
0 is a Lagrange multiplier for the safety
constraint from Theorem 2. In our experiments, we use the value function as a Lyapunov function
candidate, v = J with r(
0, and set λ = 1. In this case, (7) corresponds to an high-probability
·
upper bound on the cost-to-go given the uncertainty in the dynamics. This is similar to worst-case
performance formulations found in robust MDPs [9, 10], which consider worst-case value functions
given parametric uncertainty in MDP transition model. Moreover, since L∆v depends on the Lipschitz
constant of the policy, this simultaneously serves as a regularizer on the parameters θ.

≥

≥

)

,

·

S

To verify safety, we use the GP conﬁdence intervals ln and un directly, as in (2). We also use
n for the active learning scheme, see Algorithm 1, Line 5. In practice, we
conﬁdence to compute
n to solve (3), but can use a global optimization method or
do not need to compute the entire set
S
(cn) to ﬁnd suitable state-actions. Moreover, measurements
even a random sampling scheme within
V
(cn), see Fig. 1(c). As
for actions that are far away from the current policy are unlikely to expand
we optimize (7) via gradient descent, the policy changes only locally. Thus, we can achieve better
data-efﬁciency by restricting the exploratory actions u with (x, u)
[πn(x)

¯u, πn(x) + ¯u] for some constant ¯u.

n to be close to πn, u

∈ S

∈

V

−

Computing the region of attraction by verifying the stability condition on a discretized domain suffers
from the curse of dimensionality. However, it is not necessary to update policies in real time. In
particular, since any policy that is returned by the algorithm is provably safe within some level set,
any of these policies can be used safely for an arbitrary number of time steps. To scale this method to
higher-dimensional system, one would have to consider an adaptive discretization for the veriﬁcation
as in [27].

Experiments A Python implementation of Algorithm 1 and the experiments based on Tensor-
Flow [37] and GPﬂow [38] is available at https://github.com/befelix/safe_learning.

We verify our approach on an inverted pendulum benchmark problem. The true, continuous-time
λ ˙ψ + u, where ψ is the angle, m the mass, g the
dynamics are given by ml2 ¨ψ = gml sin(ψ)
gravitational constant, and u the torque applied to the pendulum. The control torque is limited, so that
the pendulum necessarily falls down beyond a certain angle. We use a GP model for the discrete-time
dynamics, where the mean dynamics are given by a linearized and discretized model of the true

−

8

dynamics that considers a wrong, lower mass and neglects friction. As a result, the optimal policy for
the mean dynamics does not perform well and has a small region of attraction as it underactuates the
system. We use a combination of linear and Matérn kernels in order to capture the model errors that
result from parameter and integration errors.

For the policy, we use a neural network with two hidden layers and 32 neurons with ReLU activations
each. We compute a conservative estimate of the Lipschitz constant as in [30]. We use standard
approximate dynamic programming with a quadratic, normalized cost r(x, u) = xTQx + uTRu,
where Q and R are positive-deﬁnite, to compute the cost-to-go Jπθ . Speciﬁcally, we use a piecewise-
linear triangulation of the state-space as to approximate Jπθ , see [39]. This allows us to quickly
verify the assumptions that we made about the Lyapunov function in Sec. 2 using a graph search. In
practice, one may use other function approximators. We optimize the policy via stochastic gradient
descent on (7).

The theoretical conﬁdence intervals for the GP model are conservative. To enable more data-efﬁcient
learning, we ﬁx βn = 2. This corresponds to a high-probability decrease condition per-state, rather
than jointly over the state space. Moreover, we use local Lipschitz constants of the Lyapunov function
rather than the global one. While this does not affect guarantees, it greatly speeds up exploration.

For the initial policy, we use approximate dynamic programming to compute the optimal policy for
the prior mean dynamics. This policy is unstable for large deviations from the initial state and has poor
performance, as shown in Fig. 2(b). Under this initial, suboptimal policy, the system is stable within
a small region of the state-space Fig. 2(a). Starting from this initial safe set, the algorithm proceeds to
collect safe data points and improve the policy. As the uncertainty about the dynamics decreases, the
policy improves and the estimated region of attraction increases. The region of attraction after 50 data
(cn) is contained within the true safe region of the
points is shown in Fig. 2(a). The resulting set
optimized policy πn. At the same time, the control performance improves drastically relative to the
initial policy, as can be seen in Fig. 2(b). Overall, the approach enables safe learning about dynamic
systems, as all data points collected during learning are safely collected under the current policy.

V

5 Conclusion

Acknowledgments

References

1998.

We have shown how classical reinforcement learning can be combined with safety constraints in terms
of stability. Speciﬁcally, we showed how to safely optimize policies and give stability certiﬁcates
based on statistical models of the dynamics. Moreover, we provided theoretical safety and exploration
guarantees for an algorithm that can drive the system to desired state-action pairs during learning. We
believe that our results present an important ﬁrst step towards safe reinforcement learning algorithms
that are applicable to real-world problems.

This research was supported by SNSF grant 200020_159557, the Max Planck ETH Center for
Learning Systems, NSERC grant RGPIN-2014-04634, and the Ontario Early Researcher Award.

[1] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. MIT press,

[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan
Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement
learning. Nature, 518(7540):529–533, 2015.

[3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.

Concrete problems in AI safety. arXiv:1606.06565 [cs], 2016.

[4] Hassan K. Khalil and J. W. Grizzle. Nonlinear systems, volume 3. Prentice Hall, 1996.

9

[5] Martin Pecka and Tomas Svoboda. Safe exploration techniques for reinforcement learning –
an overview. In Modelling and Simulation for Autonomous Systems, pages 357–375. Springer,
2014.

[6] Javier García and Fernando Fernández. A comprehensive survey on safe reinforcement learning.

Journal of Machine Learning Research (JMLR), 16:1437–1480, 2015.

[7] Stefano P. Coraluppi and Steven I. Marcus. Risk-sensitive and minimax control of discrete-time,

ﬁnite-state Markov decision processes. Automatica, 35(2):301–309, 1999.

[8] Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under

constraints. J. Artif. Intell. Res.(JAIR), 24:81–108, 2005.

[9] Aviv Tamar, Shie Mannor, and Huan Xu. Scaling Up Robust MDPs by Reinforcement Learning.

In Proc. of the International Conference on Machine Learning (ICML), 2014.

[10] Wolfram Wiesemann, Daniel Kuhn, and Berç Rustem. Robust Markov Decision Processes.

Mathematics of Operations Research, 38(1):153–183, 2012.

[11] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in Markov decision processes. In
Proc. of the International Conference on Machine Learning (ICML), pages 1711–1718, 2012.

[12] Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in ﬁnite markov

decision processes with gaussian processes. pages 4305–4313, 2016.

[13] Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In Proc. of the IEEE/RSJ

International Conference on Intelligent Robots and Systems, pages 2219–2225, 2006.

[14] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.

In Proc. of the International Conference on Machine Learning (ICML), 2017.

[15] Jonas Mockus. Bayesian approach to global optimization, volume 37 of Mathematics and Its

Applications. Springer, Dordrecht, 1989.

[16] Carl Edward Rasmussen and Christopher K.I Williams. Gaussian processes for machine

learning. MIT Press, Cambridge MA, 2006.

[17] Jens Schreiter, Duy Nguyen-Tuong, Mona Eberts, Bastian Bischoff, Heiner Markert, and Marc
Toussaint. Safe exploration for active learning with Gaussian processes. In Machine Learning
and Knowledge Discovery in Databases, number 9286, pages 133–149. Springer International
Publishing, 2015.

[18] Yanan Sui, Alkis Gotovos, Joel W. Burdick, and Andreas Krause. Safe exploration for optimiza-
tion with Gaussian processes. In Proc. of the International Conference on Machine Learning
(ICML), pages 997–1005, 2015.

[19] Felix Berkenkamp, Angela P. Schoellig, and Andreas Krause. Safe controller optimization for
quadrotors with Gaussian processes. In Proc. of the IEEE International Conference on Robotics
and Automation (ICRA), pages 493–496, 2016.

[20] J. Garcia and F. Fernandez. Safe exploration of state and action spaces in reinforcement learning.

Journal of Artiﬁcial Intelligence Research, pages 515–564, 2012.

[21] Alexander Hans, Daniel Schneegaß, Anton Maximilian Schäfer, and Steffen Udluft. Safe
exploration for reinforcement learning. In Proc. of the European Symposium on Artiﬁcial
Neural Networks (ESANN), pages 143–148, 2008.

[22] Theodore J. Perkins and Andrew G. Barto. Lyapunov design for safe reinforcement learning.

The Journal of Machine Learning Research, 3:803–832, 2003.

[23] Dorsa Sadigh and Ashish Kapoor. Safe control under uncertainty with Probabilistic Signal

Temporal Logic. In Proc. of Robotics: Science and Systems, 2016.

10

[24] Chris J. Ostafew, Angela P. Schoellig, and Timothy D. Barfoot. Robust constrained learning-
based NMPC enabling reliable mobile robot path tracking. The International Journal of Robotics
Research (IJRR), 35(13):1547–1536, 2016.

[25] Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, and Claire Tomlin. Provably safe and

robust learning-based model predictive control. Automatica, 49(5):1216–1226, 2013.

[26] Anayo K. Akametalu, Shahab Kaynama, Jaime F. Fisac, Melanie N. Zeilinger, Jeremy H.
Gillula, and Claire J. Tomlin. Reachability-based safe learning with Gaussian processes. In
Proc. of the IEEE Conference on Decision and Control (CDC), pages 1424–1431, 2014.

[27] Ruxandra Bobiti and Mircea Lazar. A sampling approach to ﬁnding Lyapunov functions for
nonlinear discrete-time systems. In Proc. of the European Control Conference (ECC), pages
561–566, 2016.

[28] Felix Berkenkamp, Riccardo Moriconi, Angela P. Schoellig, and Andreas Krause. Safe learning
of regions of attraction in nonlinear systems with Gaussian processes. In Proc. of the Conference
on Decision and Control (CDC), pages 4661–4666, 2016.

[29] Julia Vinogradska, Bastian Bischoff, Duy Nguyen-Tuong, Henner Schmidt, Anne Romer, and
Jan Peters. Stability of controllers for Gaussian process forward models. In Proceedings of the
International Conference on Machine Learning (ICML), pages 545–554, 2016.

[30] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
fellow, and Rob Fergus. Intriguing properties of neural networks. In Proc. of the International
Conference on Learning Representations (ICLR), 2014.

[31] Huijuan Li and Lars Grüne. Computation of local ISS Lyapunov functions for discrete-time sys-
tems via linear programming. Journal of Mathematical Analysis and Applications, 438(2):701–
719, 2016.

[32] Peter Giesl and Sigurdur Hafstein. Review on computational methods for Lyapunov functions.

Discrete and Continuous Dynamical Systems, Series B, 20(8):2291–2337, 2015.

[33] Bernhard Schölkopf. Learning with kernels: support vector machines, regularization, optimiza-
tion, and beyond. Adaptive computation and machine learning. MIT Press, Cambridge, Mass,
2002.

[34] Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. arXiv preprint

arXiv:1704.00445, 2017.

[35] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias Seeger. Gaussian Process
Optimization in the Bandit Setting: No Regret and Experimental Design. IEEE Transactions on
Information Theory, 58(5):3250–3265, 2012.

[36] Warren B. Powell. Approximate dynamic programming: solving the curses of dimensionality.

John Wiley & Sons, 2007.

[37] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,
Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,
Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray,
Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale
Machine Learning on Heterogeneous Distributed Systems. arXiv:1603.04467 [cs], 2016.

[38] Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke Fujii, Alexis
Boukouvalas, Pablo León-Villagrá, Zoubin Ghahramani, and James Hensman. GPﬂow: a
Gaussian process library using TensorFlow. Journal of Machine Learning Research, 18(40):1–6,
2017.

11

[39] Scott Davies. Multidimensional triangulation and interpolation for reinforcement learning. In
Proc. of the Conference on Neural Information Processing Systems (NIPS), pages 1005–1011,
1996.

[40] Andreas Christmann and Ingo Steinwart. Support Vector Machines. Information Science and

Statistics. Springer, New York, NY, 2008.

A Proofs

A.1 Stability veriﬁcation

Lemma 1. Using Assumptions 1 and 2, let
for all x
. Then, for all x
(cid:12)
(cid:12)v(µn

∈ X
(cid:0)v(f (z))

∈ X
1([z]τ ))

v([x]τ )

X
v(x)(cid:1)(cid:12)
(cid:12)

−

−

−

−

≤

where z = (x, π(x)) and [z]τ = ([x]τ , π([x]τ )).

τ be a discretization of

X
, we have with probability at least 1

such that
δ that

x
(cid:107)

[x]τ (cid:107)

1

−

≤

τ

−

Lvβnσn

1([z]τ ) + (LvLf (Lπ + 1) + Lv)τ, (8)

−

Proof. Let z = (x, π(x)), [z]τ = ([x]τ , π([x]τ )), and µ = µn

1, σ = σn

1. Then we have that

−

−

−

−

−

−

(cid:0)v(f (z))
v(f (z)) + v(x)

(cid:12)
v([x]τ )
(cid:12)v(µ([z]τ ))
(cid:12)
v([x]τ )
(cid:12)v(µ([z]τ ))
=
= (cid:12)
v(f ([z]τ )) + v(f ([z]τ ))
(cid:12)v(µ([z]τ ))
v(f ([z]τ ))(cid:12)
(cid:12) + (cid:12)
(cid:12)
(cid:12)v(µ([z]τ ))
f ([z]τ )
µ([z]τ )
1 + Lv
Lv
(cid:107)
(cid:107)
[z]τ −
Lvβnσ([z]τ ) + LvLf
(cid:107)

−
(cid:12)v(f ([z]τ ))
f ([z]τ )
(cid:107)
z
1 + Lv
(cid:107)

v(x)(cid:1)(cid:12)
(cid:12),
(cid:12)
(cid:12),
v(f (z)) + v(x)
(cid:12) + (cid:12)
v(f (z))(cid:12)
−
f (z)
1 + Lv
(cid:107)
−
[x]τ (cid:107)
x
1,
(cid:107)

≤
≤

−

−

−

≤

−

−

v([x]τ )(cid:12)
(cid:12),
v([x]τ )(cid:12)
(cid:12),
−
[x]τ (cid:107)
1,

−
(cid:12)v(x)
x
(cid:107)

−

where the last three inequalities follow from Assumptions 1 and 2 to last inequality follows
from Lemma 3. The result holds with probability at least 1
δ. By deﬁnition of the discretization
and the policy class ΠL we have on each grid cell that

−

z

(cid:107)

−

[z]τ (cid:107)

1 =

[x]τ (cid:107)
x
(cid:107)
−
x
τ + Lπ
−
(cid:107)
(Lπ + 1)τ,

π(x)
1 +
(cid:107)
[x]τ (cid:107)
1,

≤
≤

π([x]τ )

1,
(cid:107)

−

where the equality in the ﬁrst step follows from the deﬁnition of the 1-norm. Plugging this into the
previous bound yields

(cid:12)
(cid:12)v(µ([z]τ ))

v([x]τ )

−

−

(cid:0)v(f (z))

v(x)(cid:1)(cid:12)
(cid:12)

−

≤

Lvβnσ([z]τ ) + (LvLf (1 + Lπ) + Lv) τ,

which completes the proof.

Lemma 2. v(f (x, u))

n holds for all x

, u

, and n > 0 with probability at least (1

δ).

∈ Q

∈ X

∈ U

−

Proof. The proof is analogous to Lemma 1 and follows from Assumptions 1 and 2.

Corollary 1. v(f (x, u))
(1

δ).

∈ C

−

n holds for all x

, u

, and n > 0 with probability at least

∈ X

∈ U

Proof. Direct consequence of the fact that Lemma 2 holds jointly for all n > 0 with probability at
least 1

δ.

−

Lemma 1 show that the decrease on the Lyapunov function on the discrete grid
on the continuous domain
X
attraction using Theorem 1:

τ is close to that
. Given these conﬁdence intervals, we can now establish the region of

X

12

Theorem 2. Under Assumptions 1 and 2 with L∆v := LvLf (Lπ + 1) + Lv, let
τ for all x
tion of
X
and for some n
x
(c) with probability at least (1

τ be a discretiza-
τ with c > 0, u = π(x),
∩ X
L∆vτ, then v(f (x, π(x))) < v(x) holds for all
(c) is a region of attraction for (1) under policy π.

(cid:107)
∈ X
≤
0 it holds that un(x, u) < v(x)

. If, for all x

[x]τ (cid:107)

such that

δ) and

∈ V

(c)

−

≥

−

X

x

1

∈ V

−

V

Proof. Using Lemma 1 it holds that v(f (x, π(x))
with probability at least 1
can use Theorem 1 to conclude that

δ, since all discrete states xτ

−

−

(c) is a region of attraction for (1).

∈ V

∩ X

v(x) < 0 for all continuous states x

(c)
fulﬁll Theorem 2. Thus we

∈ V

(c)

Theorem 3. Let
R
we have with probability at least (1

πn be the true region of attraction of (1) under the policy πn. For any δ

(0, 1),

∈

δ) that

(cn)

πn for all n > 0.

V

⊆ R

Proof. Following the deﬁnition of
problem (3) that for all x
v(x) <

L∆vτ , see (2). The result

∈ D

n in (2), it is clear from the constraint in the optimization

n it holds that (x, πn(x))

n or, equivalently that un(x, π(x))

(cn)

πn then follows from Theorem 2.

∈ D

−

−

⊆ R

Note that the initialization of the conﬁdence intervals
fulﬁlled for the initial policy.

Q

0 ensures that the decrease condition is always

V

−

D

V

A.2 Gaussian process model

One particular assumption that satisﬁes both the Lipschitz continuity and allows us to use GPs as a
model of the dynamics is that the model errors g(x, u) live in some reproducing kernel Hilbert space
(RKHS, [40]) corresponding to a differentiable kernel k and have RKHS norm smaller than Bg [35].
In our theoretical analysis, we use this assumption to prove exploration guarantees.

GP

(µ(z), k(z, z(cid:48))) is a distribution over well-behaved, smooth functions f :

R (see Re-
A
mark 1 for the vector-case, Rq) that is parameterized by a mean function µ and a covariance function
(kernel) k, which encodes assumptions about the functions [16]. In our case, the mean is given by
the prior model h, while the kernel corresponds to the one in the RKHS. Given noisy measurements
of the dynamics, ˆf (z) = f (z) + (cid:15) with z = (x, u) at locations An =
, corrupted
}
(0, σ2) (we relax the Gaussian noise assumption in our
by independent, Gaussian noise (cid:15)
∼ N
1yn,
analysis), the posterior is a GP distribution again with mean, µn(z) = kn(z)T(Kn + σ2I)−
kn(z)T(Kn + σ2I)−
n(z) = kn(z, z).
covariance kn(z, z(cid:48)) = k(z, z(cid:48))
The vector yn = [ ˆf (z1)
h(zn)]T contains observed, noisy deviations from
the mean, kn(z) = [k(z, z1), . . . , k(z, zn)] contains the covariances between the test input z and the
data points in

n has entries [Kn](i,j) = k(zi, zj), and I is the identity matrix.

1kn(z(cid:48)), and variance σ2

h(z1), . . . , ˆf (zn)

z1, . . . , zn
{

X × U →

−
Rn

n, Kn

−

−

×

D

∈

Remark 1. In the case of multiple output dimensions (q > 1), we consider a function with one-
dimensional output f (cid:48)(x, u, i) :

R, with the output dimension indexed by i

∈
. This allows us to use the standard deﬁnitions of the RKHS norm and GP model.
I
}
{
In this case, we deﬁne the GP posterior distribution as µn(z) = [µn(z, 1), . . . , µn(z, q)]T and
σn(z) = (cid:80)
q σn(z, i), where the unusual deﬁnition of the standard deviation is used in Lemma 3.

X × U × I →

1, . . . , q

=

1

i

≤

≤

Given the previous assumptions, it follows from [28, Lemma 2] that the dynamics in (1) are Lipschitz
continuous with Lipschitz constant Lf = Lh + Lg, where Lg depends on the properties (smoothness)
of the kernel.

Moreover, we can construct high-probability conﬁdence intervals on the dynamics in (1) that ful-
ﬁll Assumption 2 using the GP model.
Lemma 3. ([35, Theorem 6]) Assume σ-sub-Gaussian noise and that the model error g(
in (1) has RKHS norm bounded by Bg. Choose βn = Bg + 4σ
with probability at least 1
δ, δ
f (x, u)
µn
(cid:107)

)
·
γn + 1 + ln(1/δ). Then,
it holds that

1(x, u)
(cid:107)

∈
1(x, u).

for all n

, and u

−
βnσn

(0, 1),

1, x

∈ X

∈ U

(cid:112)

≥

−

≤

−

−

1

βnσn(x, u, i) holds with
Proof. From [34, Theorem 2] it follows that
probability at least 1
q. Following Remark 1, we can model the multi-output
function as a single-output function over an extended parameter space. Thus the result directly
transfers by deﬁnition of the one norm and our deﬁnition of σn for multiple output dimensions

δ for all 1

f (x, u, i)

1(x, u, i)

| ≤

µn

−

≤

−

≤

−

i

|

13

in Remark 1. Note that by iteration n we have obtained nq measurements in the information
capacity γn.

That is, the true dynamics are contained within the GP posterior conﬁdence intervals with high
probability. The bound depends on the information capacity,

γn =

max
:
⊂X ×U ×I

I(yA; fA),

A

=nq
|

A
|
which is the maximum mutual information that could be gained about the dynamics f from samples.
= t) for many commonly used kernels
The information capacity has a sublinear dependence on n(
such as the linear, squared exponential, and Matérn kernels and it can be efﬁciently and accurately
approximated [35]. Note that we explicitly account for the q measurements that we get for each of
the q states in (9).
Remark 2. The GP model assumes Gaussian noise, while Lemma 3 considers σ-sub-Gaussian noise.
Moreover, we consider functions with bounded RKHS norm, rather than samples from a GP. Lemma 3
thus states that even though we make different assumptions than the model, the conﬁdence intervals
are conservative enough to capture the true function with high probability.

(9)

A.3 Safe exploration

Remark 3. In the following we assume that

n and

n are deﬁned as in (4) and (5).

D

S

Baseline As a baseline, we consider a class of algorithms that know about the Lipschitz continuity
properties of v, f , and π. In addition, we can learn about v(f (x, u)) up to some arbitrary statistical
accuracy (cid:15) by visiting state x and obtaining a measurement for the next state after applying action u,
but face the safety restrictions deﬁned in Sec. 2. Suppose we are given a set
of state-action pairs
about which we can learn safely. Speciﬁcally, this means that we have a policy such that, for any
state-action pair (x, u) in
, if we apply action u in state x and then apply actions according to the
policy, the state converges to the origin. Such a set can be constructed using the initial policy π0
from Sec. 2 as

0 =

x

S

S
(x, π0(x))
{

|

∈ S

x
.
0 }

S

The goal of the algorithm is to expand this set of states that we can learn about safely. Thus, we need
L∆vτ decrease
to estimate the region of attraction by certifying that state-action pairs achieve the
. We can then generalize the gained
condition in Theorem 2 by learning about state-action pairs in
knowledge to unseen states by exploiting the Lipschitz continuity,
Rdec(

: v(f (x, u))

v(x)+(cid:15)+L∆v

(x, u)

(cid:8)z

) =

−

S

τ

S

0
S

∪

τ
∈ X

× U

| ∃

∈ S

z
(cid:107)

(x, u)
1 <
(cid:107)

−

−

−

L∆vτ (cid:9),
(10)

S

S

S

) =

Rlev(

. We speciﬁcally include

where we use that we can learn v(f (x, u)) up to (cid:15) accuracy within
0 in
this set, to allow for initial policies that are safe, but does not meet the strict decrease requirements
of Theorem 2. Given that all states in Rdec(
) fulﬁll the requirements of Theorem 2, we can estimate
ΠL and estimating the
the corresponding region of attraction by committing to a control policy π
largest safe level set of the Lyapunov function. With
(cid:0) argmax c,

∀
ΠL to determine the largest level set,
encodes this operation. It optimizes over safe policies π
∈
such that all state-action pairs (x, π(x)) at discrete states x in the level set
(c)
τ fulﬁll the
decrease condition of Theorem 2. As a result, Rlev(Rdec(
)) is an estimate of the largest region
of attraction given the (cid:15)-accurate knowledge about state-action pairs in
. Based on this increased
region of attraction, there are more states that we can safely learn about. Speciﬁcally, we again use
the Lipschitz constant and statistical accuracy (cid:15) to determine all states that map back into the region
of attraction,

), the operator

τ , (x, π(x))

= Rdec(

such that

D
ΠL :

S
(c)

∈ D

(11)

∩ X

∩ X

∈ V

D

∈

∈

V

V

x

S

S

π

∃

(cid:1)

R(cid:15)(

) =

S

S∪

(cid:8)z(cid:48)

Rlev

τ (Rdec(
S

))

∈

τ

z

× U

| ∃

∈ S

: v(f (z))+(cid:15)+LvLf

z(cid:48)

z
(cid:107)

−

1

(cid:107)

max
Rlev(Rdec(

≤
x
∈

v(x)(cid:9),

S

))
(12)

) = Rlev(

where Rlev
τ (
contains state-action pairs that we can visit to
⊇ S
learn about the system. Repeatedly applying this operator leads the largest set of state-action pairs
that any safe algorithm with the same knowledge and restricted to policies in ΠL could hope to reach.

τ . Thus, R(cid:15)(

∩ X

D

D

S

)

)

14

Algorithm 2 Theoretical algorithm

1: Input: Initial safe policy
S
2: for all n = 1, . . . do
(cid:8)z(cid:48)
n = (cid:83)
3:
4:
ΠL,c
5:

D
πn, cn = argmaxπ
(cid:8)z(cid:48)
n = (cid:83)
S
(cid:8)z(cid:48)
= (cid:83)

(x,u)

∈S

∈S

n−1

n−1

z

z

∈S

n−1

(xn, un) = argmax(x,u)

∈
(cn)
Rlev
τ (

∈
∈ V
∈

6:
7:
8:
9:

0, dynamics model

(µ(z), k(z, z(cid:48)))

GP

−

un(x, u)

τ
|
such that for all x

v(x) + L∆v
(c)
∈ V
un(z) + LvLf
z
(cid:107)
z
−
(cid:107)

τ
|
un(z) + LvLf

z(cid:48)

(x, u)
1 <
−
(cid:107)
τ : (x, π(x))
cn
≤
}
maxx
∈

(cid:107)
∩ X
z(cid:48)
1
(cid:107)
1
≤
(cid:107)

−
z(cid:48)

Rlev(

τ

−
n
∈ D

n) v(x)
}

D

L∆vτ (cid:9),

ln(x, u)

τ
∈ X
R>0 c,

× U

∩ X
× U
n)
τ
|
D
× U
n un(x, u)
−
cn

∈S
un(z)

n =

z

(cn)

τ
S
|
Update GP with measurements f (xn, un) + (cid:15)n

× U

∈ V

≤

{

}

) =

Speciﬁcally, let R0
) is the set
(cid:15) (
(
S
S
of all state-action pars on the discrete grid that any algorithm could hope to classify as safe without
leaving this safe set. Moreover, Rlev(R(cid:15)(
)) is the largest corresponding region of attraction that
S
any algorithm can classify as safe for the given Lyapunov function.

) = R(cid:15)(Ri
(cid:15)(

)). Then R(cid:15)(

and Ri+1

) = limi

Ri
(cid:15)(

→∞

S

S

S

S

(cid:15)

Proofs In the following we implicitly assume that the assumptions of Lemma 3 hold and that βn is
x
deﬁned as speciﬁed within Lemma 3. Moreover, for ease of notation we assume that
0 is a level set
of the Lyapunov function v(
).
·
(cn) = Rlev(
n) and cn = maxx
D
∈

Lemma 4.

n) v(x)

Rlev(

V

S

D

Proof. Directly by deﬁnition, compare (3) and (11).

Remark 4. Lemma 4 allows us to write the proofs entirely in terms of operators, rather than having
(cn) and cn according
to deal with explicit policies. In the following and in Algorithm 2 we replace
to Lemma 4. This moves the deﬁnitions closer to the baseline and makes for an easier comparison.

V

We roughly follow the proof strategy in [18], but deal with the additional complexity of having safe
sets that are deﬁned in a more difﬁcult way (indirectly through the policy). This is non-trivial and
the safe sets are carefully designed in order to ensure that the algorithm works for general nonlinear
systems.

We start by listing some fundamental properties of the sets that we deﬁned below.
Lemma 5. It holds for all n

1 that

≥
τ , un+1(z)

τ , ln+1(z)

Rlev(

S
Rdec(

)

)

⊆

un(z)

≤

ln(z)

≥
Rlev(

)
R
Rdec(

)
R

S
)

⊆
R(cid:15)(

⊆

R

)

)

R(cid:15)(

S

S

⇒
1 =

R(cid:15)(

)

R(cid:15)(

⊆
n+1

R
n
⊇ D

⇒ D

(i)

(ii)

(iii)

(iv)

(v)

(vi)

(vii)

(viii)

(ix)

(x)

z

z

∀

∀

τ
∈ X
τ
∈ X

S ⊆ R

S ⊆ R

S ⊆ R

S ⊆ R

× U

× U
=

⇒

⇒

⇒

=

=

=

n
S

n
⊇ S

−

1
D
n
S

0
⊇ S
n
⊇ S

1

−

n
D

n
⊇ D

−

1

Proof. (i) and (ii) follow directly form the deﬁnition of

n.

C

∈
(x, π(x))

(iii) Let π

ΠL be a policy such that for some c > 0 it holds for all x

(c)

τ that

∈ S

. Then we have that (x, π(x))
ΠL :
cs = argmax c

s.t.

π

∃

∈

∈ V
. Thus it follows that with

∩ X

, since
(c)

∈ V

∈ R
x
∀

S ⊆ R
∩ X

τ , (x, π(x))

∈ S

(13)

15

and

we have that cr

(iv) Let z
(x, u)

Rdec(
∈
1 <
(cid:107)

−

π

cr = argmax c

s.t.
∃
cs. This implies
V

≥
). Then there exists (x, u)

∈
(cr)

S
L∆vτ . Since

S ⊆ R
))

ΠL :

(c)

x
∀
∩ X
(cs). The result follows.
⊇ V

τ , (x, π(x))

∈ V

∈ R

∈ S
we have that (x, u)

such that v(f (x, u))

−
as well and thus z

v(x) + (cid:15) + L∆v
Rdec(

∈ R

∈

(v)

=

Rlev(Rdec(

⇒

S ⊆ R
must exist an z
Since

S ⊆ R

∈ S

it follows that z

Rlev(Rdec(
such that v(f (z)) + (cid:15) + LvLf

)) due to (iii) and (iv). Since z(cid:48)
z(cid:48)

R

⊆

S

z
(cid:107)

−

1
(cid:107)

≤

maxx
∈

R(cid:15)(
S
∈
Rlev(Rdec(
S

x
follows from Rlev(Rdec(

∈
))

))
S
Rlev(Rdec(

S

⊆

R

max
Rlev(Rdec(

x

≤
R
)), so that we conclude that z(cid:48)

))

∈

v(x)

R(cid:15)(

).

R

∈

. Moreover,

v(x)

∈ R
max
Rlev(Rdec(

(vi) This follows directly by repeatedly applying the result of (v).

(14)

−

z
(cid:107)
).
R
), there
)) v(x).

(15)

(vii) Let z(cid:48)

∈ D
n
⊇ S
−

n
S

∃

(x, u)

1 : un(x, u)

n. Then
n
∈ S
−
1 it follows that (x, u)
∈ S
un+1(x, u)
−
v(x) + L∆v
un(x, u)

−
n as well. Moreover, we have
v(x) + L∆v
z(cid:48)
(cid:107)
z(cid:48)

−
(x, u)

v(x) + L∆v

(x, u)

z(cid:48)

−

(cid:107)

since un+1 is non-increasing, see (i). Thus z(cid:48)

≤

−

1
(cid:107)
1 <
(cid:107)

L∆vτ

−
n+1.

(cid:107)
∈ D
0 that u0(x, u) < v(x)

−

(viii) By deﬁnition of

0 we have for all (x, u)

that

C

(x, u)
1 <
(cid:107)

−

L∆vτ . Since

L∆vτ . Now we have

−

∈ S

v(x) + L∆v
v(x),
v(x),

(x, u)
(cid:107)

(x, u)
(cid:107)

1,

−

by Lemma 5 (i)

≤
<
which implies that (x, u)

−
−
−

u1(x, u)
= u1(x, u)
u0(x, u)
L∆vτ,
1.

−
∈ D

(ix) Proof by induction. We consider the base case, z

1 by (viii).
x
0 is a level set of the Lyapunov function v by assumption, we have that
τ ,

Moreover, since
Rlev(
x
0 . The previous statements together with (iii) imply that z
0) =
S
S
0 by (viii). Now, we have that
since
1
⊇ S
D

0, which implies that z

∈ D
Rlev
τ (

× U

∈ S

1)

D

∈

S

Moreover, by deﬁnition of

As a consequence,

z

u1(z) + LvLf

z
(cid:107)
0, we have for all (x, u)
C
u0(x, u) < v(x)

−

1 = u1(z)
(cid:107)

∈ S

L∆vτ < v(x).

≤
0 that

−

(i)

u0(z).

u0(x, u)

v(x),

(x,u)

max
≤
0
∈S
= max
Rlev(
S
∈
max
Rlev(
D

≤

∈

x

x

0)

1)

v(x),

v(x),

where the last inequality follows from (iii) and (viii). Thus we have z

∈ S
For the induction step, assume that for n
n
n
⊇ S
S
−
Rlev
Rlev
z(cid:48)
n+1)
τ (
n)
τ (
D
∈
to Lemma 5 (iii) and (vii) together with the induction assumption of
n
n
⊇ S
S
there must exist a z

2 we have z(cid:48)
n with
τ . This implies that z(cid:48)

n we must have that z(cid:48)

≥
× U

∈ S

∈ S

D

1.

∈

−

1. Now since
τ , due
1. Moreover,

× U

n
∈ S

1
−
un+1(z) + LvLf

⊆ S

n such that
z(cid:48)
z
(cid:107)

−

1,
(cid:107)

which in turn implies z
together with the induction assumption that

∈ S

n+1)
n+1. The last inequality follows from Lemma 5 (iii) and (vii)

∈

x

un(z) + LvLf

z
(cid:107)
v(x),

z(cid:48)

1,
(cid:107)

−

≤
≤

≤

x

∈

n)

max
Rlev(
D
max
D

Rlev(

v(x),

n
S

n
⊇ S

−

1.

16

(16)

(17)

(18)

(19)
(20)

(21)

(x) This is a direct consequence of (vii), (viii), and (ix).

n does not expand after
Given these set properties, we ﬁrst consider what happens if the safe set
collecting data points. We use these results later to conclude that the safe set must either expand or
that the maximum level set is reached. We denote by

S

the data point the is sampled according to (6).
Lemma 6. For any n1

n1 =

1, if

n0

≥

≥

S

S

n0, then for any n such that n0

n < n1, it holds that

≤

zn = (xn, un)

2βnσn(zn)

≤

(cid:115)

C1qβ2
n

nγn
n0

,

−

where C1 = 8/ log(1 + σ−

2).

Proof. We modify the results for q = 1 by [35] to this lemma, but use the different deﬁnition for βn
from [34]. Even though the goal of [35, Lemma 5.4] is different from ours, we can still apply their
reasoning to bound the amplitude of the conﬁdence interval of the dynamics. In particular, in [35,
Lemma 5.4], we have rn = 2βnσn
−
nσ2
n = 4β2
r2
n
−
(cid:32) q
(cid:88)

1(zn) with zn = (xn, un) according to Lemma 3. Then

1(zn),

(23)

(cid:33)2

= 4β2
n

σn

1(zn, i)

,

−

i=1
q
(cid:88)

i=1

q
(cid:88)

i=1

≤

≤

4β2
nq

σ2
n

−

1(zn, i)

(Jensen’s ineq.),

4β2

nqσ2C2

log(1 + σ−

2σ2
n

1(zn, i)),

−

n
(cid:88)

j=1

r2
j ≤

C1β2

nqγn

n

∀

≥

1

where C2 = σ−
that

2/ log(1 + σ−

2). The result then follows analogously to [35, Lemma 5.4] by noting

according to the deﬁnition of γn in this paper and using the Cauchy-Schwarz inequality.

The previous result allows us to bound the width of the conﬁdence intervals:
Corollary 2. For any n1
that

n0, then for any n such that n0

n1 =

1, if

n0

≥

≥

S

S

≤

n < n1, it holds

un(zn)

ln(zn)

Lv

−

≤

(cid:115)

C1qβ2
n

nγn
n0

,

−

where C1 = 8/ log(1 + σ−

2).

Proof. Direct consequence of Lemma 6 together with the deﬁnition of

and

C

.
Q

Corollary 3. For any n
vq

β2

n+Nn

Nn
γn+Nn ≥

C1L2
(cid:15)2

≥
and

S

1 with C1 as deﬁned in Lemma 6, let Nn be the smallest integer satisfying

n+Nn =

Nn, then, for any z

n+Nn it holds that

S
un(z)

ln(z)

(cid:15).

≤

−

∈ S

Proof. The result trivially follows from substituting Nn in the bound in Corollary 2.

17

(22)

(24)

(25)

(26)

(27)

(28)

(29)

Lemma 7. For any n

1, if R(cid:15)(

0)

S

\ S

=

n

, then R(cid:15)(
∅

S

n)

\ S

=

n

.
∅

≥

Proof. As in [18, Lemma 6]. Assume, to the contrary, that R(cid:15)(
tion R(cid:15)(
S
limit R(cid:15)(
S

n, therefore R(cid:15)(
n. But then, by Lemma 5,(vi) and (ix), we get

n)
n) =

⊇ S
S

n) =

S

S

. By deﬁni-
∅
S
n. Iteratively applying R(cid:15) to both sides, we get in the

\ S

n =

n)

R(cid:15)(

0)

R(cid:15)(

n) =

n,

S

S
which contradicts the assumption that R(cid:15)(

Lemma 8. For any n
1

δ:

≥

1, if R(cid:15)(

0)

S

\ S

−

S
=

.

∅

n

\ S

⊆

0)

S

=

n

n+Nn ⊃ S
S

n.

, then the following holds with probability at least
∅

Proof. By Lemma 7, we have that R(cid:15)(
n and z(cid:48)
z
n such that

R(cid:15)(

n)

S

n)

∈

S

\ S

∈ S

n
\ S

=

. By deﬁnition, this means that there exist
∅

v(f (z(cid:48))) + (cid:15) + LvLf

z
(cid:107)

z(cid:48)

1
(cid:107)

−

max
Rlev(Rdec(

n))

S

≤

x

∈

v(x)

Now we assume, to the contrary, that
(ix)). This implies that z
follows that

∈ X

× U

τ

τ

S
\ S

n+Nn =
n+Nn and z(cid:48)

n (the safe set cannot decrease due to Lemma 5
1. Due to Corollary 2, it

n+Nn =

n+Nn

S

∈ S

S

−

by (32)

by (iii), (iv) and (ix)

z
un+Nn (z(cid:48)) + LvLf
(cid:107)
v(f (z(cid:48))) + (cid:15) + LvLf
v(x)

max
Rlev(Rdec(

x

n))

z(cid:48)

1
(cid:107)
z(cid:48)

−
z
(cid:107)

−

1
(cid:107)

v(x)

n+Nn ))

≤
≤
=

x

∈

S
max
Rlev(Rdec(
∈
S
to conclude that z
n+Nn )

Thus,
Rlev(

D
n+Nn ⊇
n+Nn =

⊇
Rdec(
S
n+Nn
−

S

D
S

∈

n)).

Rlev(Rdec(
S
n+Nn ). Consider (x, u)
1 such that

Rdec(

S

∈

n+Nn

according to (5), we need to show that
S
To this end, we use Lemma 5 (iii) and show that

n+Nn ), we know that there exists a (x(cid:48), u(cid:48))

−

L∆vτ > v(f (x(cid:48), u(cid:48)))

−
un+Nn(x(cid:48), u(cid:48))

v(x(cid:48)) + (cid:15) + L∆v
v(x(cid:48)) + L∆v

(x, u)
(cid:107)
(x, u)

−

(cid:107)

1,

−
(x(cid:48), u(cid:48))

(x(cid:48), u(cid:48))
(cid:107)
1,
(cid:107)

−

≥

where the second inequality follows from Corollary 2. This implies that (x, u)

n and thus

n+Nn ). This, in turn, implies that z

n+Nn , which is a contradiction.

∈ D

Rdec(

n+Nn ⊇

D
Lemma 9. For any n

S

≥

0, the following holds with probability at least 1

δ:

−

∈ S

n
S

⊆

R0(

0).

S

Proof. Proof by induction. For the base case, n = 0, we have

R0(

0) by deﬁnition.

0
S

⊆
R0(

S
0). Let z

S

∈ S

n. Then, by

For the induction step, assume that for some n
deﬁnition,

1 such that

z(cid:48)

1,

≥

1

n
S

−

⊆

∃

n
∈ S

−

un(z(cid:48)) + LvLf

z
(cid:107)

z(cid:48)

1

(cid:107)

−

≤

x

max
Rlev(
D

∈

n)

v(x),

which, by Corollary 1, implies that

v(f (z(cid:48))) + LvLf

z
(cid:107)

z(cid:48)

1
(cid:107)

−

≤

x

max
Rlev(
D

∈

n)

v(x)

Now since z(cid:48)
to show that Rlev(

∈

R0(

S
n)

D

⊆

Rlev(Rdec(R(

0))) .

S

0) by the induction hypothesis, in order to conclude that z

R0(

0) we need

∈

S

18

(30)

(31)

(32)

(33)

(34)
(35)

(36)

∈

(37)

(38)

(39)

(40)

(41)

Let (x, u)

∈ D

n, then there exist (x(cid:48), z(cid:48))
1(x(cid:48), u(cid:48))

n
∈ S
−
v(x(cid:48)) + L∆v

un

1 such that

−
which, by Corollary 1, implies that

−

(x, u)
(cid:107)

(x(cid:48), u(cid:48))
(cid:107)

−

1 <

L∆vτ,

−

v(f (x(cid:48), u(cid:48)))

−
Rdec(R0(

v(x(cid:48)) + L∆v

(x, u)
(cid:107)
0)) since

(x(cid:48), u(cid:48))
1 <
(cid:107)

−

−

L∆vτ,

which means that (x, u)
holds by the induction hypothesis. We use (iii) to conclude that Rlev(
which concludes the proof.

R0(

n
S

⊆

∈

S

S

−

1

n)

D

⊆

0) and therefore (x(cid:48), u(cid:48))

Rlev(Rdec(R(

∈

Lemma 10. Let n∗ be the smallest integer, such that n∗
δ.
such that

n0 holds with probability 1

n0+Nn0 =
S

S

−

R0(

Nn∗ . Then, there exists n0
0)
|

S

≥ |

≤

n∗

Proof. By contradiction. Assume, to the contrary, that for all n
(ix) we know that
must have

⊆ S

n
S

n+Nn . Since Nn is increasing in n, we have that Nn

⊂ S

n
S

n∗,

≤

≤

n+Nn . From Lemma 5
Nn∗ . Thus, we

so that for any 0

j

≤

≤ |

R0(

0)
|

S

> j. In particular, for j =

R0(
|

0)
|

S

, we get

2Nn∗ · · ·

,

0
S

⊂ S

, it holds that

Nn∗ ⊂ S
SjTn∗ |
|
R0(
>
jNn∗ |
|
0) from Lemma 9.

|S

,

0)
|

S

which contradicts

jNn∗ ⊆
S

R0(

S

Corollary 4. Let n∗ be the smallest integer such that
C1L2
vq

n∗

then there exists a n0

n∗ such that

≤

Proof. A direct consequence of Lemma 10 and Corollary 3.

0)
|

,

S

R0(
|
(cid:15)2

βn∗ γn∗ ≥

n0+Nn0 =
S

S

n0.

A.4 Safety and policy adaptation

(42)

(43)

R0(

0)
S
0))),

S

(44)

(45)

(46)

In the following, we denote the true region of attraction of (1) under a policy π by
Lemma 11. Rlev(

πn for all n

n)

0.

π.

R

D

⊆ R

≥

Proof. By deﬁnition, we have for all (x, u)

L∆vτ

−

un(x(cid:48), u(cid:48))
−
v(f (x(cid:48), u(cid:48)))
v(f (x, u))

≥

≥
≥

−
v(x),

−

∈ D
v(x(cid:48)) + L∆v

n that the exists (x(cid:48), u(cid:48))
(x, u)
(cid:107)
v(x(cid:48)) + L∆v

−
(x, u)
(cid:107)

−

n
∈ S
(x(cid:48), u(cid:48))
1,
(cid:107)
(x(cid:48), u(cid:48))
1,
(cid:107)

−

1 such that

where the ﬁrst inequality follows from Corollary 1 and the second one by Lipschitz continuity,
see Lemma 1.
By deﬁnition of Rlev in (11), it follows that for all x
Moreover, Rlev(
from Theorem 2.

n.
D
n) is a level set of the Lyapunov function by deﬁnition. Thus the result follows

τ we have that (x, πn(x))

Rlev(

∈ D

∩ X

n)

D

∈

Lemma 12. f (x, u)

∈ R

πn ∀

(x, u)

n.

∈ S

Proof. This holds for
z(cid:48)
1 such that

S

n
∈ S

−

0 by deﬁnition. For n

1, by deﬁtion, we have for all z

n there exists an

∈ S

max
Rlev(
D

x

∈

n)

v(x)

un(z(cid:48)) + LvLf

z
(cid:107)
v(f (z(cid:48))) + LvLf
v(f (z))

−
z
(cid:107)

z(cid:48)

1

(cid:107)
z(cid:48)

−

1
(cid:107)

where the ﬁrst inequality follows from Corollary 1 and the second one by Lipschitz continuity,
see Lemma 1. Since Rlev(

πn by Lemma 11, we have that f (z)

πn .

n)

D

⊆ R

∈ R

≥

≥

≥
≥

19

(cid:112)

Theorem 4. Assume σ-sub-Gaussian measurement noise and that
in (1) has RKHS norm smaller than Bg.
with βn = Bg + 4σ
n∗ be the smallest positive integer so that
Let
following holds jointly with probability at least (1

)
·
Under the assumptions of Theorem 2,
γn + 1 + ln(1/δ), and with measurements collected according to (6), let
R(
+1)
0)
2).
S
|
v(cid:15)2
L2
(0, 1), the

n∗
β2
n∗ γn∗ ≥
π be the true region of attraction of (1) under a policy π. For any (cid:15) > 0, and δ

where C = 8/ log(1 + σ−

the model error g(

δ) for all n > 0:

Cq(
|

R

∈

(i)

(cn)

V

πn

⊆ R

(ii) f (x, u)

−
(x, u)

n.

∈ S

∈ R

πn ∀

(iii) R(cid:15)(

0)

n
⊆ S

⊆

R0(

0).

S

S

Proof. See Lemmas 11 and 12 for (i) and (ii), respectively. Part (iii) is a direct consequence
of Corollary 4 and Lemma 9.

20

