Sparse Sequence-to-Sequence Models

Ben Peters† Vlad Niculae† and Andr´e F. T. Martins†‡
†Instituto de Telecomunicac¸ ˜oes, Lisbon, Portugal
‡Unbabel, Lisbon, Portugal
vlad@vene.ro,

benzurdopeters@gmail.com,

andre.martins@unbabel.com

9
1
0
2
 
n
u
J
 
2
1
 
 
]
L
C
.
s
c
[
 
 
2
v
2
0
7
5
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

d

r

a

w

d

</s>

66.4%
e
32.2%
n
1.4%

</s>

</s>

Sequence-to-sequence models are a powerful
workhorse of NLP. Most variants employ a
softmax transformation in both their attention
mechanism and output layer, leading to dense
alignments and strictly positive output proba-
bilities. This density is wasteful, making mod-
els less interpretable and assigning probability
mass to many implausible outputs. In this pa-
per, we propose sparse sequence-to-sequence
models, rooted in a new family of α-entmax
transformations, which includes softmax and
sparsemax as particular cases, and is sparse
for any α > 1. We provide fast algorithms
to evaluate these transformations and their gra-
dients, which scale well for large vocabulary
sizes. Our models are able to produce sparse
alignments and to assign nonzero probability
to a short list of plausible outputs, sometimes
rendering beam search exact. Experiments on
morphological inﬂection and machine transla-
tion reveal consistent gains over dense models.

1

Introduction

Attention-based sequence-to-sequence (seq2seq)
models have proven useful for a variety of NLP
applications, including machine translation (Bah-
danau et al., 2015; Vaswani et al., 2017), speech
recognition (Chorowski et al., 2015), abstractive
summarization (Chopra et al., 2016), and morpho-
logical inﬂection generation (Kann and Sch¨utze,
2016), among others. In part, their strength comes
from their ﬂexibility: many tasks can be formu-
lated as transducing a source sequence into a target
sequence of possibly different length.

However, conventional seq2seq models are
dense: they compute both attention weights and
output probabilities with the softmax function
(Bridle, 1990), which always returns positive val-
ues. This results in dense attention alignments, in
which each source position is attended to at each

Figure 1: The full beam search of our best perform-
ing morphological inﬂection model when generating
the past participle of the verb “draw”. The model gives
nonzero probability to exactly three hypotheses, includ-
ing the correct form (“drawn”) and the form that would
be correct if “draw” were regular (“drawed”).

target position, and in dense output probabilities,
in which each vocabulary type always has nonzero
probability of being generated. This contrasts with
traditional statistical machine translation systems,
which are based on sparse, hard alignments, and
decode by navigating through a sparse lattice of
phrase hypotheses. Can we transfer such notions
of sparsity to modern neural architectures? And if
so, do they improve performance?

In this paper, we provide an afﬁrmative an-
swer to both questions by proposing neural sparse
seq2seq models that replace the softmax trans-
formations (both in the attention and output) by
sparse transformations. Our innovations are
rooted in the recently proposed sparsemax transfor-
mation (Martins and Astudillo, 2016) and Fenchel-
Young losses (Blondel et al., 2019). Concretely,
we consider a family of transformations (dubbed
α-entmax), parametrized by a scalar α, based on
the Tsallis entropies (Tsallis, 1988). This family
includes softmax (α = 1) and sparsemax (α = 2)
as particular cases. Crucially, entmax transforms
are sparse for all α > 1.

Our models are able to produce both sparse at-
tention, a form of inductive bias that increases
focus on relevant source words and makes align-
ments more interpretable, and sparse output prob-
abilities, which together with auto-regressive mod-

Figure 2: Forced decoding using sparsemax attention and 1.5-entmax output for the German source sentence, “Dies
ist ein weiterer Blick auf den Baum des Lebens.” Predictions with nonzero probability are shown at each time step.
All other target types have probability exactly zero. When consecutive predictions consist of a single word, we
combine their borders to showcase auto-completion potential. The selected gold targets are in boldface.

els can lead to probability distributions that are
nonzero only for a ﬁnite subset of all possible
strings. In certain cases, a short list of plausible
outputs can be enumerated without ever exhausting
the beam (Figure 1), rendering beam search exact.
Sparse output seq2seq models can also be used for
adaptive, sparse next word suggestion (Figure 2).

Overall, our contributions are as follows:

• We propose an entmax sparse output layer,
together with a natural loss function. In large-
vocabulary settings, sparse outputs avoid wast-
ing probability mass on unlikely outputs, sub-
stantially improving accuracy. For tasks with
little output ambiguity, entmax losses, coupled
with beam search, can often produce exact ﬁ-
nite sets with only one or a few sequences. To
our knowledge, this is the ﬁrst study of sparse
output probabilities in seq2seq problems.

• We construct entmax sparse attention, im-
proving interpretability at no cost in accuracy.
We show that the entmax gradient has a simple
form (Proposition 2), revealing an insightful
missing link between softmax and sparsemax.

• We derive a novel exact algorithm for the case
of 1.5-entmax, achieving processing speed
close to softmax on the GPU, even with large
vocabulary sizes. For arbitrary α, we investi-
gate a GPU-friendly approximate algorithm.1

We experiment on two tasks: one character-level
with little ambiguity (morphological inﬂection
generation) and another word-level, with more am-
biguity (neural machine translation). The results

1Our standalone Pytorch entmax implementation is avail-

able at https://github.com/deep-spin/entmax.

show clear beneﬁts of our approach, both in terms
of accuracy and interpretability.

2 Background

The underlying architecture we focus on is an RNN-
based seq2seq with global attention and input-
feeding (Luong et al., 2015). We provide a brief
description of this architecture, with an emphasis
on the attention mapping and the loss function.

Notation. Scalars, vectors, and matrices are de-
noted respectively as a, a, and A. We denote the d–
probability simplex (the set of vectors representing
probability distributions over d choices) by (cid:52)d :=
{p ∈ Rd : p ≥ 0, (cid:107)p(cid:107)1 = 1}. We denote the pos-
itive part as [a]+ := max{a, 0}, and by [a]+ its
elementwise application to vectors. We denote the
indicator vector ey := [0, . . . , 0, 1
(cid:124)(cid:123)(cid:122)(cid:125)y

, 0, . . . , 0].

Encoder. Given an input sequence of tokens
x := [x1, . . . , xJ ], the encoder applies an embed-
ding lookup followed by K layered bidirectional
LSTMs (Hochreiter and Schmidhuber, 1997), re-
sulting in encoder states [h1, . . . , hJ ].

Decoder. The decoder generates output tokens
y1, . . . , yT , one at a time, terminated by a stop
symbol. At each time step t, it computes a proba-
bility distribution for the next generated word yt, as
follows. Given the current state st of the decoder
LSTM, an attention mechanism (Bahdanau et al.,
2015) computes a focused, ﬁxed-size summary of
the encodings [h1, . . . , hJ ], using st as a query vec-
tor. This is done by computing token-level scores
t W (z)hj, then taking a weighted average
zj := s(cid:62)

ct :=

πjhj, where π := softmax(z).

(1)

J
(cid:88)

j=1

The contextual output is the non-linear combina-
tion ot := tanh(W (o)[st; ct] + b(o)), yielding the
predictive distribution of the next word

p(yt = · | x, y1, ..., yt−1) := softmax(Vot + b).
(2)
The output ot, together with the embedding of the
predicted (cid:98)yt, feed into the decoder LSTM for the
next step, in an auto-regressive manner. The model
is trained to maximize the likelihood of the correct
target sentences, or equivalently, to minimize

L =

(cid:88)

|y|
(cid:88)

(x,y)∈D

t=1

(− log softmax(Vot))yt
(cid:124)
(cid:125)
(cid:123)(cid:122)
Lsoftmax(yt,Vot)

.

(3)

A central building block in the architecture is the
transformation softmax : Rd → (cid:52)d,

softmax(z)j :=

(4)

exp(zj)
i exp(zi)

,

(cid:80)

which maps a vector of scores z into a probability
distribution (i.e., a vector in (cid:52)d). As seen above,
the softmax mapping plays two crucial roles in the
decoder: ﬁrst, in computing normalized attention
weights (Eq. 1), second, in computing the predic-
tive probability distribution (Eq. 2). Since exp (cid:13) 0,
softmax never assigns a probability of zero to any
word, so we may never fully rule out non-important
input tokens from attention, nor unlikely words
from the generation vocabulary. While this may
be advantageous for dealing with uncertainty, it
may be preferrable to avoid dedicating model re-
sources to irrelevant words. In the next section, we
present a strategy for differentiable sparse prob-
ability mappings. We show that our approach can
be used to learn powerful seq2seq models with
sparse outputs and sparse attention mechanisms.

3 Sparse Attention and Outputs

3.1 The sparsemax mapping and loss

To pave the way to a more general family of sparse
attention and losses, we point out that softmax
(Eq. 4) is only one of many possible mappings
from Rd to (cid:52)d. Martins and Astudillo (2016) intro-
duce sparsemax: an alternative to softmax which
tends to yield sparse probability distributions:

sparsemax(z) := argmin
p∈(cid:52)d

(cid:107)p − z(cid:107)2.

(5)

Since Eq. 5 is a projection onto (cid:52)d, which tends
to yield sparse solutions, the predictive distribution

p(cid:63) := sparsemax(z) is likely to assign exactly
zero probability to low-scoring choices. They also
propose a corresponding loss function to replace
the negative log likelihood loss Lsoftmax (Eq. 3):

Lsparsemax(y, z) :=

(cid:0)(cid:107)ey −z(cid:107)2 −(cid:107)p(cid:63) −z(cid:107)2(cid:1), (6)

1
2

This loss is smooth and convex on z and has a
margin: it is zero if and only if zy ≥ zy(cid:48) + 1 for
any y(cid:48) (cid:54)= y (Martins and Astudillo, 2016, Proposi-
tion 3). Training models with the sparsemax loss
requires its gradient (cf. Appendix A.2):

∇zLsparsemax(y, z) = −ey + p(cid:63).

For using the sparsemax mapping in an attention
mechanism, Martins and Astudillo (2016) show
that it is differentiable almost everywhere, with

∂ sparsemax(z)
∂z

= diag(s) −

1
(cid:107)s(cid:107)1

ss(cid:62),

j > 0, otherwise sj = 0.

where sj = 1 if p(cid:63)
Entropy interpretation. At ﬁrst glance, sparse-
max appears very different from softmax, and a
strategy for producing other sparse probability map-
pings is not obvious. However, the connection be-
comes clear when considering the variational form
of softmax (Wainwright and Jordan, 2008):

softmax(z) = argmax

p(cid:62)z + HS(p),

(7)

p∈(cid:52)d

where HS(p) := − (cid:80)
Gibbs-Boltzmann-Shannon entropy with base e.

j pj log pj is the well-known

Likewise, letting HG(p) := 1
2

(cid:80)

j pj(1 − pj) be

the Gini entropy, we can rearrange Eq. 5 as

sparsemax(z) = argmax

p(cid:62)z + HG(p),

(8)

p∈(cid:52)d

crystallizing the connection between softmax and
sparsemax: they only differ in the choice of en-
tropic regularizer.

3.2 A new entmax mapping and loss family

The parallel above raises a question: can we ﬁnd
interesting interpolations between softmax and
sparsemax? We answer afﬁrmatively, by consid-
ering a generalization of the Shannon and Gini
entropies proposed by Tsallis (1988): a family of
entropies parametrized by a scalar α > 1 which we
call Tsallis α-entropies:

HT

α(p) :=

(cid:40) 1

α(α−1)
HS(p),

(cid:16)

(cid:80)
j

pj − pα
j

(cid:17)

, α (cid:54)= 1,

α = 1.

(9)

α = 1 (softmax)
α = 1.25
α = 1.5
α = 2 (sparsemax)
α = 4

1.0

0.5

0.0

−2

2

0
t

Figure 3: Illustration of entmax in the two-dimensional
case α-entmax([t, 0])1. All mappings except softmax
saturate at t = ±1/α−1. While sparsemax is piecewise
linear, mappings with 1 < α < 2 have smooth corners.

This family is continuous, i.e., limα→1 HT

α(p) =
HS(p) for any p ∈ (cid:52)d (cf. Appendix A.1). More-
over, HT
2 ≡ HG. Thus, Tsallis entropies interpolate
between the Shannon and Gini entropies. Starting
from the Tsallis entropies, we construct a probabil-
ity mapping, which we dub entmax:

α-entmax(z) := argmax
p∈(cid:52)d

p(cid:62)z + HT

α(p),

(10)

and, denoting p(cid:63) := α-entmax(z), a loss function

Lα(y, z) := (p(cid:63) − ey)(cid:62)z + HT

α(p(cid:63))

(11)

The motivation for this loss function resides in the
fact that it is a Fenchel-Young loss (Blondel et al.,
2019), as we brieﬂy explain in Appendix A.2. Then,
1-entmax ≡ softmax and 2-entmax ≡ sparsemax.
Similarly, L1 is the negative log likelihood, and
L2 is the sparsemax loss. For all α > 1, entmax
tends to produce sparse probability distributions,
yielding a function family continuously interpolat-
ing between softmax and sparsemax, cf. Figure 3.
The gradient of the entmax loss is

∇zLα(y, z) = −ey + p(cid:63).

(12)

Tsallis entmax losses have useful properties in-
cluding convexity, differentiability, and a hinge-
like separation margin property: the loss incurred
becomes zero when the score of the correct class is
separated by the rest by a margin of 1/α−1. When
separation is achieved, p(cid:63) = ey (Blondel et al.,
2019). This allows entmax seq2seq models to
be adaptive to the degree of uncertainty present:
decoders may make fully conﬁdent predictions at
“easy” time steps, while preserving sparse uncer-
tainty when a few choices are possible (as exem-
pliﬁed in Figure 2).

Tsallis entmax probability mappings have not,
to our knowledge, been used in attention mecha-
nisms. They inherit the desirable sparsity of sparse-
max, while exhibiting smoother, differentiable cur-
vature, whereas sparsemax is piecewise linear.

3.3 Computing the entmax mapping

Whether we want to use α-entmax as an attention
mapping, or Lα as a loss function, we must be able
to efﬁciently compute p(cid:63) = α-entmax(z), i.e., to
solve the maximization in Eq. 10. For α = 1, the
closed-form solution is given by Eq. 4. For α > 1,
given z, we show that there is a unique threshold τ
such that (Appendix C.1, Lemma 2):

α-entmax(z) = [(α − 1)z − τ 1]

1/α−1
+

,

(13)

i.e., entries with score zj ≤ τ/α−1 get zero prob-
ability. For sparsemax (α = 2), the problem
amounts to Euclidean projection onto (cid:52)d, for
which two types of algorithms are well studied:

i. exact, based on sorting (Held et al., 1974;
Michelot, 1986),

ii. iterative, bisection-based (Liu and Ye, 2009).

The bisection approach searches for the opti-
mal threshold τ numerically. Blondel et al. (2019)
generalize this approach in a way applicable to
α-entmax. The resulting algorithm is (cf. Ap-
pendix C.1 for details):

Algorithm 1 Compute α-entmax by bisection.

+

, set z ← (α − 1)z

1 Deﬁne p(τ ) := [z − τ ]1/α−1
2 τmin ← max(z) − 1; τmax ← max(z) − d1−α
3 for t ∈ 1, . . . , T do
4

τ ← (τmin + τmax)/2
Z ← (cid:80)
j pj(τ )
if Z < 1 then τmax ← τ else τmin ← τ

5

6
7 return 1/Z p(τ )

Algorithm 1 works by iteratively narrowing the
interval containing the exact solution by exactly
half. Line 7 ensures that approximate solutions are
valid probability distributions, i.e., that p(cid:63) ∈ (cid:52)d.
Although bisection is simple and effective, an ex-
act sorting-based algorithm, like for sparsemax, has
the potential to be faster and more accurate. More-
over, as pointed out by Condat (2016), when exact
solutions are required, it is possible to construct in-
puts z for which bisection requires arbitrarily many
iterations. To address these issues, we propose a

novel, exact algorithm for 1.5-entmax, halfway
between softmax and sparsemax.

Algorithm 2 Compute 1.5-entmax(z) exactly.

1 Sort z, yielding z[d] ≤ · · · ≤ z[1]; set z ← z/2
2 for ρ ∈ 1, . . . , d do
3 M (ρ) ← 1/ρ (cid:80)ρ
S (ρ) ← (cid:80)ρ
τ (ρ) ← M (ρ) − (cid:112)1/ρ (1 − S(ρ))

j=1z[j]
(cid:0)z[j] − M (ρ)(cid:1)2

j=1

5

4

6

7

if z[ρ+1] ≤ τ (ρ) ≤ z[ρ] then
return p(cid:63) = [z − τ 1]2
+

We give a full derivation in Appendix C.2. As
written, Algorithm 2 is O(d log d) because of the
sort; however, in practice, when the solution p(cid:63) has
no more than k nonzeros, we do not need to fully
sort z, just to ﬁnd the k largest values. Our experi-
ments in §4.2 reveal that a partial sorting approach
can be very efﬁcient and competitive with softmax
on the GPU, even for large d. Further speed-ups
might be available following the strategy of Condat
(2016), but our simple incremental method is very
easy to implement on the GPU using primitives
available in popular libraries (Paszke et al., 2017).
Our algorithm resembles the aforementioned
sorting-based algorithm for projecting onto the sim-
plex (Michelot, 1986). Both algorithms rely on
the optimality conditions implying an analytically-
solvable equation in τ : for sparsemax (α = 2),
this equation is linear, for α = 1.5 it is quadratic
(Eq. 36 in Appendix C.2). Thus, exact algorithms
may not be available for general values of α.

3.4 Gradient of the entmax mapping

The following result shows how to compute the
backward pass through α-entmax, a requirement
when using α-entmax as an attention mechanism.
Proposition 1. Let α ≥ 1. Assume we have com-
puted p(cid:63) = α-entmax(z), and deﬁne the vector

si =

(cid:40)

(p(cid:63)
0,

i )2−α, p(cid:63)

i > 0,
otherwise.

Then,

∂ α-entmax(z)
∂z

= diag(s) −

1
(cid:107)s(cid:107)1

ss(cid:62).

Proof: The result follows directly from the more
general Proposition 2, which we state and prove in

Appendix B, noting that

(cid:16) tα−t
α(α−1)

(cid:17)(cid:48)(cid:48)

= tα−2.

respectively (Martins and Astudillo, 2016, Eqs. 8
and 12), thereby providing another relationship
between the two mappings. Perhaps more inter-
estingly, Proposition 1 shows why the sparsemax
Jacobian depends only on the support and not
on the actual values of p(cid:63): the sparsemax Jacobian
is equal for p(cid:63) = [.99, .01, 0] and p(cid:63) = [.5, .5, 0].
This is not the case for α-entmax with α (cid:54)= 2,
suggesting that the gradients obtained with other
values of α may be more informative. Finally, we
point out that the gradient of entmax losses involves
the entmax mapping (Eq. 12), and therefore Propo-
sition 1 also gives the Hessian of the entmax loss.

4 Experiments

The previous section establishes the computational
building blocks required to train models with ent-
max sparse attention and loss functions. We now
put them to use for two important NLP tasks,
morphological inﬂection and machine translation.
These two tasks highlight the characteristics of
our innovations in different ways. Morphologi-
cal inﬂection is a character-level task with mostly
monotonic alignments, but the evaluation demands
exactness: the predicted sequence must match the
gold standard. On the other hand, machine transla-
tion uses a word-level vocabulary orders of mag-
nitude larger and forces a sparse output layer to
confront more ambiguity: any sentence has several
valid translations and it is not clear beforehand that
entmax will manage this well.

Despite the differences between the tasks, we
keep the architecture and training procedure as sim-
ilar as possible. We use two layers for encoder and
decoder LSTMs and apply dropout with probability
0.3. We train with Adam (Kingma and Ba, 2015),
with a base learning rate of 0.001, halved whenever
the loss increases on the validation set. We use a
batch size of 64. At test time, we select the model
with the best validation accuracy and decode with
a beam size of 5. We implemented all models with
OpenNMT-py (Klein et al., 2017).2

In our primary experiments, we use three α val-
ues for the attention and loss functions: α = 1 (soft-
max), α = 1.5 (to which our novel Algorithm 2 ap-
plies), and α = 2 (sparsemax). We also investigate
the effect of tuning α with increased granularity.

The gradient expression recovers the softmax
and sparsemax Jacobians with α = 1 and α = 2,

2Our experiment code is at https://github.com/

deep-spin/OpenNMT-entmax.

α

high

medium

output

attention

(avg.)

(ens.)

(avg.)

(ens.)

1

2

1.5

1
1.5
2
1
1.5
2
1
1.5
2

UZH (2018)

93.15
92.32
90.98
94.36
94.44
94.05
94.59
94.47
94.32

94.20
93.50
92.60
94.96
95.00
94.74
95.10
95.01
94.89

96.00

82.55
83.20
83.13
84.88
84.93
84.93
84.95
85.03
84.96

85.68
85.63
85.65
86.38
86.55
86.59
86.41
86.61
86.47

86.64

Table 1: Average per-language accuracy on the test set
(CoNLL–SIGMORPHON 2018 task 1) averaged or en-
sembled over three runs.

4.1 Morphological Inﬂection

The goal of morphological inﬂection is to produce
an inﬂected word form (such as “drawn”) given a
lemma (“draw”) and a set of morphological tags
({verb, past, participle}). We use the data
from task 1 of the CoNLL–SIGMORPHON 2018
shared task (Cotterell et al., 2018). shared task

training:

Training. We train models under two data
settings: high (approximately 10,000 samples
per language in 86 languages) and medium (ap-
proximately 1,000 training samples per language
in 102 languages). We depart from previous
work by using multilingual
each
model is trained on the data from all languages
in its data setting. This allows parameters to
be shared between languages, eliminates the
need to train language-speciﬁc models, and may
provide beneﬁts similar to other forms of data
augmentation (Bergmanis et al., 2017). Each
sample is presented as a pair: the source contains
the lemma concatenated to the morphological
tags and a special language identiﬁcation token
(Johnson et al., 2017; Peters et al., 2017), and
the target contains the inﬂected form. As an
example,
the source sequence for Figure 1 is
english verb participle past d r a w.
Although the set of inﬂectional tags is not sequen-
tial, treating it as such is simple to implement and
works well in practice (Kann and Sch¨utze, 2016).
All models use embedding and hidden state sizes
of 300. We validate at the end of every epoch in
the high setting and only once every ten epochs in
medium because of its smaller size.

Accuracy. Results are shown in Table 1. We re-
port the ofﬁcial metric of the shared task, word

accuracy averaged across languages. In addition to
the average results of three individual model runs,
we use an ensemble of those models, where we
decode by averaging the raw probabilities at each
time step. Our best sparse loss models beat the soft-
max baseline by nearly a full percentage point with
ensembling, and up to two and a half points in the
medium setting without ensembling. The choice of
attention has a smaller impact. In both data settings,
our best model on the validation set outperforms
all submissions from the 2018 shared task except
for UZH (Makarov and Clematide, 2018), which
uses a more involved imitation learning approach
and larger ensembles. In contrast, our only depar-
ture from standard seq2seq training is the drop-in
replacement of softmax by entmax.

Sparsity. Besides their accuracy, we observed
that entmax models made very sparse predictions:
the best conﬁguration in Table 1 concentrates all
probability mass into a single predicted sequence
in 81% validation samples in the high data set-
ting, and 66% in the more difﬁcult medium setting.
When the model does give probability mass to more
than one sequence, the predictions reﬂect reason-
able ambiguity, as shown in Figure 1. Besides en-
hancing interpretability, sparsity in the output also
has attractive properties for beam search decoding:
when the beam covers all nonzero-probability hy-
potheses, we have a certiﬁcate of globally optimal
decoding, rendering beam search exact. This is
the case on 87% of validation set sequences in the
high setting, and 79% in medium. To our knowl-
edge, this is the ﬁrst instance of a neural seq2seq
model that can offer optimality guarantees.

4.2 Machine Translation

We now turn to a highly different seq2seq regime
in which the vocabulary size is much larger, there
is a great deal of ambiguity, and sequences can
generally be translated in several ways. We train
models for three language pairs in both directions:

• IWSLT 2017 German ↔ English (DE↔EN, Cet-

tolo et al., 2017): training size 206,112.

• KFTT Japanese ↔ English (JA↔EN, Neubig,

2011): training size of 329,882.

• WMT 2016 Romanian ↔ English (RO↔EN, Bo-
jar et al., 2016): training size 612,422, diacritics
removed (following Sennrich et al., 2016b).

method

softmax
1.5-entmax
sparsemax

DE

EN

(cid:1)
25.70 ± 0.15
26.17 ± 0.13
24.69 ± 0.22

EN

DE

(cid:1)
21.86 ± 0.09
22.42 ± 0.08
20.82 ± 0.19

JA

EN

(cid:1)
20.22 ± 0.08
20.55 ± 0.30
18.54 ± 0.11

EN

JA

(cid:1)
25.21 ± 0.29
26.00 ± 0.31
23.84 ± 0.37

RO

EN

(cid:1)
29.12 ± 0.18
30.15 ± 0.06
29.20 ± 0.16

EN

RO

(cid:1)
28.12 ± 0.18
28.84 ± 0.10
28.03 ± 0.16

Table 2: Machine translation comparison of softmax, sparsemax, and the proposed 1.5-entmax as both attention
mapping and loss function. Reported is tokenized test BLEU averaged across three runs (higher is better).

Training. We use byte pair encoding (BPE; Sen-
nrich et al., 2016a) to ensure an open vocabulary.
We use separate segmentations with 25k merge
operations per language for RO↔EN and a joint
segmentation with 32k merges for the other lan-
guage pairs. DE↔EN is validated once every 5k
steps because of its smaller size, while the other
sets are validated once every 10k steps. We set
the maximum number of training steps at 120k for
RO↔EN and 100k for other language pairs. We use
500 dimensions for word vectors and hidden states.

Evaluation. Table 2 shows BLEU scores (Pa-
pineni et al., 2002) for the three models with
α ∈ {1, 1.5, 2}, using the same value of α for the
attention mechanism and loss function. We observe
that the 1.5-entmax conﬁguration consistently per-
forms best across all six choices of language pair
and direction. These results support the notion
that the optimal function is somewhere between
softmax and sparsemax, which motivates a more
ﬁne-grained search for α; we explore this next.

(cid:1)

Fine-grained impact of α. Algorithm 1 allows
us to further investigate the marginal effect of vary-
ing the attention α and the loss α, while keeping
the other ﬁxed. We report DE
EN validation ac-
curacy on a ﬁne-grained α grid in Figure 4. On
this dataset, moving from softmax toward sparser
attention (left) has a very small positive effect
on accuracy, suggesting that the beneﬁt in inter-
pretability does not hurt accuracy. The impact of
the loss function α (right) is much more visible:
there is a distinct optimal value around α = 1.33,
with performance decreasing for too large values.
Interpolating between softmax and sparsemax thus
inherits the beneﬁts of both, and our novel Algo-
rithm 2 for α = 1.5 is conﬁrmed to strike a good
middle ground. This experiment also conﬁrms that
bisection is effective in practice, despite being in-
exact. Extrapolating beyond the sparsemax loss
(α > 2) does not seem to perform well.

Sparsity.
In order to form a clearer idea of how
sparse entmax becomes, we measure the average

method

# attended

# target words

softmax
1.5-entmax
sparsemax

24.25
5.55
3.75

17993
16.13
7.55

Table 3: Average number of nonzeros in the attention
EN validation set.
and output distributions for the DE

(cid:1)

(cid:1)

number of nonzero indices on the DE
EN vali-
dation set and show it in Table 3. As expected,
1.5-entmax is less sparse than sparsemax as both
an attention mechanism and output layer. In the
attention mechanism, 1.5-entmax’s increased sup-
port size does not come at the cost of much in-
In the
terpretability, as Figure 5 demonstrates.
output layer, 1.5-entmax assigns positive proba-
bility to only 16.13 target types out of a vocabu-
lary of 17,993, meaning that the supported set of
words often has an intuitive interpretation. Fig-
ure 2 shows the sparsity of the 1.5-entmax output
layer in practice: the support becomes completely
concentrated when generating a phrase like “the
tree of life”, but grows when presenting a list of
synonyms (“view”, “look”, “glimpse”, and so on).
This has potential practical applications as a pre-
dictive translation system (Green et al., 2014),
where the model’s support set serves as a list of
candidate auto-completions at each time step.

Training time.
Importantly, the beneﬁts of spar-
sity do not come at a high computational cost.
Our proposed Algorithm 2 for 1.5-entmax runs
on the GPU at near-softmax speeds (Figure 6). For
other α values, bisection (Algorithm 1) is slightly
more costly, but practical even for large vocabulary
EN, bisection is capable of process-
sizes. On DE
ing about 10,500 target words per second on a sin-
gle Nvidia GeForce GTX 1080 GPU, compared to
13,000 words per second for 1.5-entmax with Algo-
rithm 2 and 14,500 words per second with softmax.
On the smaller-vocabulary morphology datasets,
Algorithm 2 is nearly as fast as softmax.

(cid:1)

n
o
i
t
a
d
i
l
a
v

y
c
a
r
u
c
c
a

63%
62%
61%
60%

1.00

1.25

2.00

2.25

1.00

1.25

1.50

1.75

2.00

2.25

output α

Figure 4: Effect of tuning α on DE

EN, for attention (left) and for output (right), while keeping the other α = 1.5.

1.75
1.50
attention α

(cid:1)

63.0%
61.5%
60.0%
58.5%
57.0%

n
o
i
t
a
d
i
l
a
v

y
c
a
r
u
c
c
a

softmax
1.5-entmax

2000

4000
seconds

6000

Figure 6: Training timing on three DE
ers show validation checkpoints for one of the runs.

EN runs. Mark-

(cid:1)

to future work exploring sparse attention mecha-
nisms and seq2seq models. We believe our paper
can foster interesting investigation in this area.

Losses for seq2seq models. Mostly motivated by
the challenges of large vocabulary sizes in seq2seq,
an important research direction tackles replacing
the cross-entropy loss with other losses or approx-
imations (Bengio and Sen´ecal, 2008; Morin and
Bengio, 2005; Kumar and Tsvetkov, 2019). While
differently motivated, some of the above strate-
gies (e.g., hierarchical prediction) could be com-
bined with our proposed sparse losses. Niculae
et al. (2018) use sparsity to predict interpretable
sets of structures. Since auto-regressive seq2seq
makes no factorization assumptions, their strategy
cannot be applied without approximations, such as
in Edunov et al. (2018).

6 Conclusion and Future Work

We proposed sparse sequence-to-sequence models
and provided fast algorithms to compute their at-
tention and output transformations. Our approach
yielded consistent improvements over dense mod-
els on morphological inﬂection and machine trans-
lation, while inducing interpretability in both atten-
tion and output distributions. Sparse output layers
also provide exactness when the number of possible
hypotheses does not exhaust beam search.

Given the ubiquity of softmax in NLP, entmax
has many potential applications. A natural next step
is to apply entmax to self-attention (Vaswani et al.,

Figure 5: Attention weights produced by the DE
1.5-entmax model. Nonzero weights are outlined.

(cid:1)

EN

5 Related Work

Sparse attention. Sparsity in the attention and in
the output have different, but related, motivations.
Sparse attention can be justiﬁed as a form of induc-
tive bias, since for tasks such as machine translation
one expects only a few source words to be relevant
for each translated word. Dense attention probabil-
ities are particularly harmful for long sequences,
as shown by Luong et al. (2015), who propose “lo-
cal attention” to mitigate this problem. Combining
sparse attention with fertility constraints has been
recently proposed by Malaviya et al. (2018). Hard
attention (Xu et al., 2015; Aharoni and Goldberg,
2017; Wu et al., 2018) selects exactly one source to-
ken. Its discrete, non-differentiable nature requires
imitation learning or Monte Carlo policy gradient
approximations, which drastically complicate train-
ing. In contrast, entmax is a differentiable, easy to
use, drop-in softmax replacement. A recent study
by Jain and Wallace (2019) tackles the limitations
of attention probabilities to provide interpretability.
They only study dense attention in classiﬁcation
tasks, where attention is less crucial for the ﬁnal
predictions. In their conclusions, the authors defer

2017). In a different vein, the strong morphological
inﬂection results point to usefulness in other tasks
where probability is concentrated in a small number
of hypotheses, such as speech recognition.

Acknowledgments

This work was supported by the European Re-
search Council (ERC StG DeepSPIN 758969),
and by the Fundac¸ ˜ao para a Ciˆencia e Tecnolo-
gia through contracts UID/EEA/50008/2019 and
CMUPERI/TIC/0046/2014 (GoLocal). We thank
Mathieu Blondel, Nikolay Bogoychev, Gonc¸alo
Correia, Erick Fonseca, Pedro Martins, Tsvetomila
Mihaylova, Miguel Rios, Marcos Treviso, and the
anonymous reviewers, for helpful discussion and
feedback.

References

Roee Aharoni and Yoav Goldberg. 2017. Morphologi-
cal inﬂection generation with hard monotonic atten-
tion. In Proc. ACL.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. ICLR.

Yoshua Bengio and Jean-S´ebastien Sen´ecal. 2008.
Adaptive importance sampling to accelerate train-
ing of a neural probabilistic language model. IEEE
Transactions on Neural Networks, 19(4):713–722.

Toms Bergmanis, Katharina Kann, Hinrich Sch¨utze,
and Sharon Goldwater. 2017. Training data augmen-
tation for low-resource morphological inﬂection. In
Proc. CoNLL–SIGMORPHON.

Mathieu Blondel, Andr´e FT Martins, and Vlad Nicu-
lae. 2019. Learning classiﬁers with Fenchel-Young
losses: Generalized entropies, margins, and algo-
rithms. In Proc. AISTATS.

Ondrej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Jimeno Yepes, Philipp Koehn, Varvara Lo-
gacheva, Christof Monz, et al. 2016. Findings of
the 2016 conference on machine translation. In ACL
WMT.

John S Bridle. 1990. Probabilistic interpretation of
feedforward classiﬁcation network outputs, with re-
lationships to statistical pattern recognition. In Neu-
rocomputing, pages 227–236. Springer.

Sumit Chopra, Michael Auli, and Alexander M Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proc. NAACL-
HLT.

Jan K Chorowski, Dzmitry Bahdanau, Dmitriy
Serdyuk, Kyunghyun Cho, and Yoshua Bengio.
2015. Attention-based models for speech recogni-
tion. In Proc. NeurIPS.

Laurent Condat. 2016. Fast projection onto the simplex
and the (cid:96)1 ball. Mathematical Programming, 158(1-
2):575–585.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
G˙eraldine Walther, Ekaterina Vylomova, Arya D
McCarthy, Katharina Kann, Sebastian Mielke, Gar-
rett Nicolai, Miikka Silfverberg, et al. 2018. The
CoNLL–SIGMORPHON 2018 shared task: Uni-
versal morphological reinﬂection. Proc. CoNLL–
SIGMORPHON.

John M Danskin. 1966. The theory of max-min, with
applications. SIAM Journal on Applied Mathemat-
ics, 14(4):641–664.

Sergey Edunov, Myle Ott, Michael Auli, David Grang-
ier, et al. 2018. Classical structured prediction losses
for sequence to sequence learning. In Proc. NAACL-
HLT.

Spence Green, Sida I Wang, Jason Chuang, Jeffrey
Heer, Sebastian Schuster, and Christopher D Man-
ning. 2014. Human effort and machine learnability
in computer aided translation. In Proc. EMNLP.

Michael Held, Philip Wolfe, and Harlan P Crowder.
1974. Validation of subgradient optimization. Math-
ematical Programming, 6(1):62–88.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Sarthak Jain and Byron C. Wallace. 2019. Attention is

not explanation. In Proc. NAACL-HLT.

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vi´egas, Martin Wattenberg, Greg Corrado,
et al. 2017. Googles multilingual neural machine
translation system: Enabling zero-shot translation.
Transactions of the Association for Computational
Linguistics, 5:339–351.

Katharina Kann and Hinrich Sch¨utze. 2016. MED: The
LMU system for the SIGMORPHON 2016 shared
In Proc. SIG-
task on morphological reinﬂection.
MORPHON.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proc. ICLR.

M Cettolo, M Federico, L Bentivogli, J Niehues,
S St¨uker, K Sudoh, K Yoshino, and C Federmann.
2017. Overview of the IWSLT 2017 evaluation cam-
paign. In Proc. IWSLT.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-
lart, and Alexander M Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
arXiv e-prints.

Sachin Kumar and Yulia Tsvetkov. 2019. Von Mises-
Fisher loss for training sequence to sequence models
with continuous outputs. In Proc. ICLR.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Edinburgh neural machine translation sys-
tems for WMT 16. In Proc. WMT.

Constantino Tsallis. 1988. Possible generalization of
Journal of Statistical

Boltzmann-Gibbs statistics.
Physics, 52:479–487.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. NeurIPS.

Martin J Wainwright and Michael I Jordan. 2008.
Graphical models, exponential families, and varia-
tional inference. Foundations and Trends® in Ma-
chine Learning, 1(1–2):1–305.

Shijie Wu, Pamela Shapiro, and Ryan Cotterell. 2018.
Hard non-monotonic attention for character-level
transduction. In Proc. EMNLP.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In Proc. ICML.

Jun Liu and Jieping Ye. 2009. Efﬁcient Euclidean pro-

jections in linear time. In Proc. ICML.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proc. EMNLP.

Peter Makarov and Simon Clematide. 2018. UZH at
CoNLL–SIGMORPHON 2018 shared task on uni-
versal morphological reinﬂection. Proc. CoNLL–
SIGMORPHON.

Chaitanya Malaviya, Pedro Ferreira, and Andr´e FT
Martins. 2018. Sparse and constrained attention for
neural machine translation. In Proc. ACL.

Andr´e FT Martins and Ram´on Fernandez Astudillo.
2016. From softmax to sparsemax: A sparse model
of attention and multi-label classiﬁcation. In Proc.
of ICML.

Christian Michelot. 1986. A ﬁnite algorithm for ﬁnd-
ing the projection of a point onto the canonical sim-
plex of Rn. Journal of Optimization Theory and Ap-
plications, 50(1):195–200.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proc. AISTATS.

Graham Neubig. 2011. The Kyoto free translation task.

http://www.phontron.com/kftt.

Vlad Niculae and Mathieu Blondel. 2017. A regular-
ized framework for sparse and structured neural at-
tention. In Proc. NeurIPS.

Vlad Niculae, Andr´e FT Martins, Mathieu Blondel, and
Claire Cardie. 2018. SparseMAP: Differentiable
sparse structured inference. In Proc. ICML.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in PyTorch.
In Proc. NeurIPS Autodiff Workshop.

Ben Peters, Jon Dehdari, and Josef van Genabith.
2017. Massively multilingual neural grapheme-to-
phoneme conversion. In Proc. Workshop on Build-
ing Linguistically Generalizable NLP Systems.

R Tyrrell Rockafellar. 1970. Convex Analysis. Prince-

ton University Press.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Neural machine translation of rare words
with subword units. In Proc. ACL.

Supplementary Material

A Background

A.1 Tsallis entropies

Recall the deﬁnition of the Tsallis family of entropies in Eq. 9 for α ≥ 1,

HT

α(p) :=

(cid:40) 1

α(α−1)
HS(p),

(cid:80)

(cid:16)
j

pj − pα
j

(cid:17)

,

if α (cid:54)= 1,

if α = 1.

This family is continuous in α, i.e., limα→1 HT
rewrite HT

α in separable form:

α(p) = HT

1 (p) for any p ∈ (cid:52)d. Proof: For simplicity, we

HT

α(p) =

hα(pj) with hα(t) :=

(cid:88)

j

(cid:40) t−tα

α(α−1) , α > 1,
−t log t, α = 1.

It sufﬁces to show that limα→1 hα(t) = h1(t) for t ∈ [0, 1]. Let f (α) := t − tα, and g(α) := α(α − 1).
Observe that f (1)

g(1) = 0/0, so we are in an indeterminate case. We take the derivatives of f and g:

f (cid:48)(α) = 0 − (exp(log tα))(cid:48) = − exp(log tα) · (α log t)(cid:48) = −tα log t,

and

g(cid:48)(α) = 2α − 1.

(14)

(15)

From l’Hˆopital’s rule,

lim
α→1

f (α)
g(α)

= lim
α→1

f (cid:48)(α)
g(cid:48)(α)

= −t log t = h1(t).

Note also that, as α → ∞, the denominator grows unbounded, so HT

∞ ≡ 0.

A.2 Fenchel-Young losses

In this section, we recall the deﬁnitions and properties essential for our construction of α-entmax. The
concepts below were formalized by Blondel et al. (2019) in more generality; we present below a less
general version, sufﬁcient for our needs.
Deﬁnition 1 (Probabilistic prediction function regularized by Ω). Let Ω : (cid:52)d → R ∪ {∞} be a strictly
convex regularization function. We deﬁne the prediction function πΩ as

πΩ(z) ∈ argmax
p∈(cid:52)d

(cid:0)p(cid:62)z − Ω(p)(cid:1)

(16)

Deﬁnition 2 (Fenchel-Young loss generated by Ω). Let Ω : (cid:52)d → R ∪ {∞} be a strictly convex regu-
larization function. Let y ∈ (cid:52) denote a ground-truth label (for example, y = ey if there is a unique
correct class y). Denote by z ∈ Rd the prediction scores produced by some model, and by p(cid:63) := πΩ(z)
the probabilistic predictions. The Fenchel-Young loss LΩ : Rd × (cid:52) → R+ generated by Ω is

LΩ(z; y) := Ω(y) − Ω(p(cid:63)) + z(cid:62)(p(cid:63) − y).

(17)

This justiﬁes our choice of entmax mapping and loss (Eqs. 10–11), as π−HT

= α-entmax and L−HT

= Lα.

α

α

Properties of Fenchel-Young losses.

1. Non-negativity. LΩ(z; y) ≥ 0 for any z ∈ Rd and y ∈ (cid:52)d.

2. Zero loss. LΩ(z; y) = 0 if and only if y = πΩ(z), i.e., the prediction is exactly correct.

3. Convexity. LΩ is convex in z.

4. Differentiability. LΩ is differentiable with ∇LΩ(z; y) = p(cid:63) − y.

5. Smoothness. If Ω is strongly convex, then LΩ is smooth.

6. Temperature scaling. For any constant t > 0, LtΩ(z; y) = tLΩ(z/t; y).

Characterizing the solution p(cid:63) of πΩ(z). To shed light on the generic probability mapping in Eq. 16,
we derive below the optimality conditions characterizing its solution. The optimality conditions are
essential not only for constructing algorithms for computing p(cid:63) (Appendix C), but also for deriving the
Jacobian of the mapping (Appendix B). The Lagrangian of the maximization in Eq. 16 is

L(p, ν, τ ) = Ω(p) − (z + ν)(cid:62)p + τ (1(cid:62)p − 1).

with subgradient

∂pL(p, ν, τ ) = ∂Ω(p) − z − ν + τ 1.

The subgradient KKT conditions are therefore:






z + ν − τ 1 ∈ ∂Ω(p)
p(cid:62)ν = 0

p ∈ (cid:52)d
ν ≥ 0.

(18)

(19)

(20)

(21)

(22)

(23)

Connection to softmax and sparsemax. We may now directly see that, when Ω(p) := (cid:80)
j pj log pj,
Eq. 20 becomes log pj = zj + νj − τ − 1, which can only be satisﬁed if pj > 0, thus ν = 0. Then,
pj = exp(zj )/Z, where Z := exp(τ + 1). From Eq. 22, Z must be such that pj sums to 1, yielding the
well-known softmax expression. In the case of sparsemax, note that for any p ∈ (cid:52)d, we have

Ω(p) = −HG(p) = 1/2

pj(pj − 1) = 1/2(cid:107)p(cid:107)2 −

pj

= 1/2(cid:107)p(cid:107)2 + const.

(cid:88)

j

(cid:88)

1
2

j

(cid:124) (cid:123)(cid:122) (cid:125)
=1

Thus, argmax
p∈(cid:52)d

p(cid:62)z + HG(p) = argmin
p∈(cid:52)d

(cid:16)

(cid:107)p(cid:107)2 − 2p(cid:62)z(cid:0)+(cid:107)z(cid:107)2(cid:1) (cid:17)

0.5

= argmin
p∈(cid:52)d

(cid:107)p − z(cid:107)2.

B Backward pass for generalized sparse attention mappings

When a mapping πΩ is used inside the computation graph of a neural network, the Jacobian of the
mapping has the important role of showing how to propagate error information, necessary when training
with gradient methods. In this section, we derive a new, simple expression for the Jacobian of generalized
sparse πΩ. We apply this result to obtain a simple form for the Jacobian of α-entmax mappings.

The proof is in two steps. First, we prove a lemma that shows that Jacobians are zero outside of the
support of the solution. Then, completing the result, we characterize the Jacobian at the nonzero indices.

Lemma 1 (Sparse attention mechanisms have sparse Jacobians). Let Ω : Rd → R be strongly convex. The
symmetric and satisfying
attention mapping πΩ is differentiable almost everywhere, with Jacobian

∂πΩ
∂z

∂(πΩ(z))i
∂zj

= 0 if

(πΩ(z))i = 0 or

(πΩ(z))j = 0.

Proof: Since Ω is strictly convex, the argmax in Eq. 16 is unique. Using Danskin’s theorem (Danskin,
1966), we may write

πΩ(z) = ∇ max
p∈(cid:52)

(cid:16)

(cid:17)
p(cid:62)z − Ω(p)

= ∇Ω∗(z).

Since Ω is strongly convex, the gradient of its conjugate Ω∗ is differentiable almost everywhere (Rockafel-
lar, 1970). Moreover, ∂πΩ

∂z is the Hessian of Ω∗, therefore it is symmetric, proving the ﬁrst two claims.

Recall the deﬁnition of a partial derivative,

∂(πΩ(z))i
∂zj

= lim
ε→0

1
ε

(πΩ(z + εej)i − πΩ(z)i) .

Denote by p(cid:63) := πΩ(z). We will show that for any j such that p(cid:63)

j = 0, and any ε ≥ 0,

πΩ(z − εej) = πΩ(z) = p(cid:63).

In other words, we consider only one side of the limit, namely subtracting a small non-negative ε. A
vector p(cid:63) solves the optimization problem in Eq. 16 if and only if there exists ν(cid:63) ∈ Rd and τ (cid:63) ∈ R
satisfying Eqs. 20–23. Let νε := ν(cid:63) + εej. We verify that (p(cid:63), νε, τ (cid:63)) satisﬁes the optimality conditions
for πΩ(z − εej), which implies that πΩ(z − εej) = πΩ(z). Since we add a non-negative quantity to ν(cid:63),
which is non-negative to begin with, (νε)j ≥ 0, and since p(cid:63)
j (νε)j = 0. Finally,

j = 0, we also satisfy p(cid:63)
z − εej + νε − τ (cid:63)1 = z − εej + ν(cid:63) + εej − τ (cid:63)1 ∈ ∂Ω(p(cid:63)).

1
ε (πΩ(z + εej)i − πΩ(z)i) = 0. If πΩ is differentiable at z, this one-sided limit

It follows that limε→0−
must agree with the derivative. Otherwise, the sparse one-sided limit is a generalized Jacobian.
Proposition 2. Let p(cid:63) := πΩ(z), with strongly convex and differentiable Ω. Denote the support of p(cid:63) by
S = (cid:8)j ∈ {1, . . . , d} : pj > 0(cid:9). If the second derivative hij = ∂2Ω
(p(cid:63)) exists for any i, j ∈ S, then
∂pi∂pj

∂πΩ
∂z

1
(cid:107)s(cid:107)1

= S −

ss(cid:62) where Sij =

(cid:40)

H −1
ij ,
0,

i, j ∈ S,
o.w.

,

and s = S1.

In particular, if Ω(p) = (cid:80)

j g(pj) with g twice differentiable on (0, 1], we have

∂πΩ
∂z

= diag s −

ss(cid:62) where

si =

1
(cid:107)s(cid:107)1

i )(cid:1)−1,

(cid:40)(cid:0)g(cid:48)(cid:48)(p(cid:63)
0,

i ∈ S,
o.w.

Proof: Lemma 1 veriﬁes that ∂(πΩ)i
= 0 for i, j /∈ S. It remains to ﬁnd the derivatives w.r.t. i, j ∈ S.
∂zj
Denote by ¯p(cid:63), ¯z the restriction of the corresponding vectors to the indices in the support S. The optimality
conditions on the support are

where g( ¯p) := (cid:0)∇Ω(p)(cid:1)(cid:12)

(cid:12)S, so ∂g

∂ ¯p ( ¯p(cid:63)) = H. Differentiating w.r.t. ¯z at p(cid:63) yields

(cid:26) g( ¯p) + τ 1 = ¯z
1(cid:62) ¯p = 1

(cid:26) H ∂ ¯p

∂ ¯z + 1 ∂τ
1(cid:62) ∂ ¯p

∂ ¯z = I
∂ ¯z = 0

(24)

(25)

Since Ω is strictly convex, H is invertible. From block Gaussian elimination (i.e., the Schur complement),

which can then be used to solve for ∂ ¯p

∂ ¯z giving

∂τ
∂ ¯z

= −

1
1(cid:62)H −11

1(cid:62)H −1,

∂ ¯p
∂ ¯z

= H −1 −

H −111(cid:62)H −1,

1
1(cid:62)H −11

yielding the desired result. When Ω is separable, H is diagonal, with Hii = g(cid:48)(cid:48)(p(cid:63)
expression which completes the proof.

i ), yielding the simpliﬁed

Connection to other differentiable attention results. Our result is similar, but simpler than Niculae
and Blondel (2017, Proposition 1), especially in the case of separable Ω. Crucially, our result does not
require that the second derivative exist outside of the support. As such, unlike the cited work, our result is
applicable in the case of α-entmax, where either g(cid:48)(cid:48)(t) = tα−2 or its reciprocal may not exist at t = 0.

C Algorithms for entmax

C.1 General thresholded form for bisection algorithms.

The following lemma provides a simpliﬁed form for the solution of α-entmax.
Lemma 2. For any z, there exists a unique τ (cid:63) such that

1/α−1
α-entmax(z) = [(α − 1)z − τ (cid:63)1]
+

.

(26)

Proof: We use the regularized prediction functions deﬁned in Appendix A.2. From both deﬁnitions,

We ﬁrst note that for all p ∈ (cid:52)d,

α-entmax(z) ≡ π−HT

(z).

α

−(α − 1)HT

α(p) =

pα
i + const.

(27)

1
α

d
(cid:88)

i=1

From the constant invariance and scaling properties of πΩ (Blondel et al., 2019, Proposition 1, items 4–5),

π−HT

α

(z) = πΩ((α − 1)z), with Ω(p) =

g(pj),

g(t) =

d
(cid:88)

j=1

tα
α

.

Using (Blondel et al., 2019, Proposition 5), noting that g(cid:48)(t) = tα−1 and (g(cid:48))−1(u) = u1/α−1, yields

1/α−1
πΩ(z) = [z − τ (cid:63)1]
+

,

1/α−1
and therefore α-entmax(z) = [(α − 1)z − τ (cid:63)1]
+

.

(28)

Uniqueness of τ (cid:63) follows from the fact that α-entmax has a unique solution p(cid:63), and Eq. 28 implies a
one-to-one mapping between p(cid:63) and τ (cid:63), as long as p(cid:63) ∈ (cid:52).
Corollary 2.1. For α = 1.5, Lemma 2 implies existence of a unique τ (cid:63) such that

1.5-entmax(z) = [z/2 − τ (cid:63)1]2
+.

C.2 An exact algorithm for entmax with α = 1.5: Derivation of Algorithm 2.

In this section, we derive an exact, sorting-based algorithm for 1.5-entmax. The key observation is that
the solution can be characterized by the size of its support, ρ(cid:63) = (cid:107)p(cid:63)(cid:107)0. Then, we can simply enumerate
all possible values of ρ ∈ {1, . . . , d} until the solution veriﬁes all optimality conditions. The challenge,
however, is expressing the threshold τ as a function of the support size ρ; for this, we rely on α = 1.5.
Proposition 3. Exact computation of 1.5-entmax(z)

Let z[d] ≤ · · · ≤ z[1] denote the sorted coordinates of z, and, for convenience, let z[d+1] := −∞. Deﬁne
the top-ρ mean, unnormalized variance, and induced threshold for ρ ∈ {1, . . . , d} as

Mz(ρ) :=

z[j], Sz(ρ) :=

(cid:0)z[j] − Mz(ρ)(cid:1)2 ,

τz(ρ) :=

1
ρ

ρ
(cid:88)

j=1

ρ
(cid:88)

j=1

(cid:40)

Mz(ρ) −
+∞,

(cid:113) 1−Sz(ρ)
ρ

, Sz(ρ) ≤ 1,
Sz(ρ) > 1.

Then,

(1.5-entmax(z))i =

− τz/2(ρ)

(cid:104) zi
2

(cid:105)2

,

+

for any ρ satisfying τz(ρ) ∈ [z[ρ+1], z[ρ]].

Proposition 3 implies the correctness of Algorithm 2. To prove it, we ﬁrst need the following lemma.
Lemma 3. Deﬁne τ (ρ) as in Proposition 3. Then, τ is non-decreasing, and there exists ρmax ∈ {1, . . . , d}
such that τ is ﬁnite for 1 ≤ ρ ≤ ρmax, and inﬁnite for ρ > ρmax.

The proof is slightly more technical, and we defer it to after the proof of the proposition.

Proof of Proposition 3. First, using Corollary 2.1 we reduce the problem of computing 1.5-entmax to

πΩ(z) := argmax
p∈(cid:52)d

p(cid:62)z −

(cid:88)

2/3 p

3/2
j

.

j

Denote by τ (cid:63) the optimal threshold as deﬁned in the corollary. We will show that τ (cid:63) = τ (ρ) for any ρ
satisfying τ (ρ) ∈ [z[ρ+1], z[ρ]], where we assume, for convenience, z[d+1] = −∞. The generic stationarity
condition in Eq. 20, applied to the problem in Eq. 30, takes the form

√

pj = νj + zj − τ

∀ 0 < j ≤ d

Since Ω is symmetric, πΩ is permutation-preserving (Blondel et al., 2019, Proposition 1, item 1), so
we may assume w.l.o.g. that z is sorted non-increasingly, i.e., z1 ≥ · · · ≥ zd; in other words, zj = z[j].
Therefore, the optimal p is also non-increasing. Denote by ρ an index such as pj ≥ 0 for 1 ≤ j ≤ ρ, and
pj = 0 for j > ρ. From the complementary slackness condition (21), νj = 0 for 1 ≤ j ≤ ρ, thus we may
split the stationarity conditions (31) into

For (32) to have solutions, the r.h.s. must be non-negative, i.e., τ ≤ zj for j ≤ ρ, so τ ≤ zρ. At the same
time, from dual feasability (23) we have νj = τ − zj ≥ 0 for j > ρ, therefore

Given ρ, we can solve for τ using (32) and primal feasability (22)

(cid:40)√

pj = zj − τ, ∀ 1 ≤ j ≤ ρ,
νj = τ − zj, ∀ ρ < j ≤ d.

τ (ρ) ∈ [zρ+1, zρ].

1 =

pj =

(zj − τ )2.

d
(cid:88)

j=1

ρ
(cid:88)

j=1

(29)

(30)

(31)

(32)

(33)

(34)

(35)

Expanding the squares and dividing by 2ρ yields the quadratic equation

τ 2 −

1
2

(cid:80)ρ

j=1 zj
ρ

τ +

(cid:80)ρ

j − 1

j=1 z2
2ρ

= 0,

(36)

with discriminant

(cid:80)ρ

+

=

1
ρ

1 − S(ρ)
ρ

∆(ρ) = (cid:0)M (ρ)(cid:1)2 −

j=1 z2
j
ρ
(X − E[X])2(cid:105)
(cid:104)
= E[X 2] − E[X]2. If S(ρ) > 1, ∆(ρ) < 0,
where we used the variance expression E
so there must exist an optimal ρ satisfying S(ρ) ∈ [0, 1]. Therefore, (36) has the two solutions τ±(ρ) =
M (ρ) ±
. However, τ+ leads to a contradiction: The mean M (ρ) is never smaller than the
smallest averaged term, so M (ρ) ≥ zρ, and thus τ+ ≥ zρ. At the same time, from (34), τ ≤ zρ, so τ must
equal zρ, which can only happen if M (ρ) = zρ and S(ρ) = 1. But M (ρ) = zρ only if z1 = · · · = zρ, in
which case S(ρ) = 0 (contradiction).

(cid:113) 1−S(ρ)
ρ

(37)

.

Therefore, τ (cid:63) = τ (ρ) = M (ρ) −
leads to the same value of τ (ρ). Pick any ρ1 < ρ2, both verifying (34). Therefore, ρ1 + 1 ≤ ρ2 and

for some ρ verifying (34). It remains to show that any such ρ

(cid:113) 1−S(ρ)
ρ

zρ1+1 ≤
(cid:124)(cid:123)(cid:122)(cid:125)
(34) for ρ1

τ (ρ1) ≤
(cid:124)(cid:123)(cid:122)(cid:125)
Lemma 3

τ (ρ2) ≤
(cid:124)(cid:123)(cid:122)(cid:125)
(34) for ρ2

zρ2 ≤
(cid:124)(cid:123)(cid:122)(cid:125)
z sorted

zρ1+1,

(38)

thus τ (ρ1) = τ (ρ2), and so any ρ verifying (34) satisﬁes τ (cid:63) = τ (ρ), concluding the proof.

Proof of Lemma 3. We regard τ (ρ) as an extended-value sequence, i.e., a function from N → R ∪ ∞.
The lemma makes a claim about the domain of the sequence τ , and a claim about its monotonicity. We
prove the two in turn.
Domain of τ . The threshold τ (ρ) is only ﬁnite for ρ ∈ T := (cid:8)ρ ∈ {1, . . . , d} : S(ρ) ≤ 1(cid:9), i.e., where
(1−S(ρ))/ρ ≥ 0. We show there exists ρmax such that T = {1, . . . , ρmax}. Choose ρmax as the largest
index satisfying S(ρmax) ≤ 1. By deﬁnition, ρ > ρmax implies ρ /∈ T . Remark that S(1) = 0, and
S(ρ + 1) − S(ρ) = (·)2 ≥ 0. Therefore, S is nondecreasing and, for any 1 ≤ ρ ≤ ρmax, 0 ≤ S(ρ) ≤ 1.

Monotonicity of τ . Fix ρ ∈ [ρmax − 1], assume w.l.o.g. that Mz(ρ) = 0, and deﬁne ˜z as

(cid:40)

˜z[j] =

j = ρ + 1,
x,
z[j], otherwise.

The ρ highest entries of ˜z are the same as in z, so M ˜z(ρ) = Mz(ρ) = 0, S ˜z(ρ) = Sz(ρ), and
τ ˜z(ρ) = τz(ρ). Denote (cid:101)τ (x) := τ ˜z(ρ + 1), and analogously (cid:102)M (x) and (cid:101)S(x). Then,

τz(ρ + 1) = (cid:101)τ (z[ρ+1]) ≥

min
x : (cid:101)S(x)∈[0,1]

(cid:101)τ (x) =: (cid:101)τ (x(cid:63))

We seek the lower bound (cid:101)τ (x(cid:63)) and show that (cid:101)τ (x(cid:63)) ≥ τz(ρ). From (39), this implies τz(ρ + 1) ≥ τz(ρ)
and, by transitivity, the monotonicity of τz.

It is easy to verify that the following incremental update expressions hold.

(cid:102)M (x) =

x
ρ + 1

,

(cid:101)S(x) = Sz(ρ) +

ρ
ρ + 1

x2.

We must solve the optimization problem

minimizex (cid:101)τ (x)

subject to

(cid:101)S(x) ∈ [0, 1].

(39)

(40)

(41)

The objective value is

(cid:101)τ (x) = (cid:102)M (x) −

(cid:115)

1 − (cid:101)S(x)
ρ + 1

=

1
ρ + 1

(cid:18)

(cid:113)(cid:0)1 − Sz(ρ)(cid:1)(ρ + 1) − ρx2

x −

(cid:19)

(42)

Ignoring the constraint for a moment and setting the gradient to 0 yields the solution

0 = (cid:101)τ (cid:48)(x(cid:63)) =

⇐⇒ ρx(cid:63) = −



1 +

ρx(cid:63)
(cid:113)(cid:0)1 − Sz(ρ)(cid:1)(ρ + 1) − ρx(cid:63)2

1
ρ + 1
(cid:113)(cid:0)1 − Sz(ρ)(cid:1)(ρ + 1) − ρx(cid:63)2,





(cid:115)

x(cid:63) = −

1 − Sz(ρ)
ρ

.

implying x(cid:63) < 0. Squaring both sides and rearranging yields the solution of the unconstrained problem,

We verify that x(cid:63) readily satisﬁes the constraints, thus it is a solution to the minimization in Eq. 41:

0 ≤ Sz(ρ) ≤ (cid:101)S(x(cid:63)) = Sz(ρ) +

1 − Sz(ρ)
ρ + 1

≤ Sz(ρ) +

1 − Sz(ρ)
2

≤ 1.

Plugging x(cid:63) into the objective yields

(cid:101)τ (x(cid:63)) =

(cid:32)

(cid:115)

−

1 − Sz(ρ)
ρ

(cid:113)

−

ρ(cid:0)1 − Sz(ρ)(cid:1)

(cid:33)

1
ρ + 1
(cid:115)

= −

(cid:115)

= −

1 − Sz(ρ)
ρ

1
ρ + 1

(1 + ρ)

1 − Sz(ρ)
ρ

= τz(ρ).

Therefore, (cid:101)τ (x) ≥ τz(ρ) for any valid x, proving that τz(ρ) ≤ τz(ρ + 1).

(43)

(44)

(45)

(46)

