Published as a conference paper at ICLR 2018

A COMPRESSED SENSING VIEW OF UNSUPERVISED
TEXT EMBEDDINGS, BAG-OF-n-GRAMS, AND LSTMS

Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi
Princeton University
{arora,mkhodak,nsaunshi}@cs.princeton.edu

Kiran Vodrahalli
Columbia University
kiran.vodrahalli@columbia.edu

ABSTRACT

Low-dimensional vector embeddings, computed using LSTMs or simpler tech-
niques, are a popular approach for capturing the “meaning” of text and a form of
unsupervised learning useful for downstream tasks. However, their power is not
theoretically understood. The current paper derives formal understanding by look-
ing at the subcase of linear embedding schemes. Using the theory of compressed
sensing we show that representations combining the constituent word vectors
are essentially information-preserving linear measurements of Bag-of-n-Grams
(BonG) representations of text. This leads to a new theoretical result about LSTMs:
low-dimensional embeddings derived from a low-memory LSTM are provably at
least as powerful on classiﬁcation tasks, up to small error, as a linear classiﬁer
over BonG vectors, a result that extensive empirical work has thus far been unable
to show. Our experiments support these theoretical ﬁndings and establish strong,
simple, and unsupervised baselines on standard benchmarks that in some cases are
state of the art among word-level methods. We also show a surprising new property
of embeddings such as GloVe and word2vec: they form a good sensing matrix for
text that is more efﬁcient than random matrices, the standard sparse recovery tool,
which may explain why they lead to better representations in practice.

1

INTRODUCTION

Much attention has been paid to using LSTMs (Hochreiter & Schmidhuber, 1997) and similar models
to compute text embeddings (Bengio et al., 2003; Collobert & Weston, 2008). Once trained, the
LSTM can sweep once or twice through a given piece of text, process it using only limited memory,
and output a vector with moderate dimensionality (a few hundred to a few thousand), which can be
used to measure text similarity via cosine similarity or as a featurization for downstream tasks.

The powers and limitations of this method have not been formally established. For example, can
such neural embeddings compete with and replace traditional linear classiﬁers trained on trivial
Bag-of-n-Grams (BonG) representations? Tweaked versions of BonG classiﬁers are known to be a
surprisingly powerful baseline (Wang & Manning, 2012) and have fast implementations (Joulin et al.,
2017). They continue to give better performance on many downstream supervised tasks such as IMDB
sentiment classiﬁcation (Maas et al., 2011) than purely unsupervised LSTM representations (Kiros
et al., 2015; Hill et al., 2016; Pagliardini et al., 2017). Even a very successful character-level (and thus
computation-intensive, taking a month of training) approach does not reach BonG performance on
datasets larger than IMDB (Radford et al., 2017). Meanwhile there is evidence suggesting that simpler
linear schemes give compact representations that provide most of the beneﬁts of word-level LSTM
embeddings (Wieting et al., 2016; Arora et al., 2017). These linear schemes consist of simply adding
up, with a few modiﬁcations, standard pretrained word embeddings such as GloVe or word2vec
(Mikolov et al., 2013; Pennington et al., 2014).

1

Published as a conference paper at ICLR 2018

The current paper ties these disparate threads together by giving an information-theoretic account
of linear text embeddings. We describe linear schemes that preserve n-gram information as low-
dimensional embeddings with provable guarantees for any text classiﬁcation task. The previous linear
schemes, which used unigram information, are subcases of our approach, but our best schemes can
also capture n-gram information with low additional overhead. Furthermore, we show that the original
unigram information can be (approximately) extracted from the low-dimensional embedding using
sparse recovery/compressed sensing (Candès & Tao, 2005). Our approach also ﬁts in the tradition
of the older work on distributed representations of structured objects, especially the works of Plate
(1995) and Kanerva (2009). The following are the main results achieved by this new world-view:

1. Using random vectors as word embeddings in our linear scheme (instead of pretrained
vectors) already allows us to rigorously show that low-memory LSTMs are provably at
least as good as every linear classiﬁer operating on the full BonG vector. This is a novel
theoretical result in deep learning, obtained relatively easily. By contrast, extensive empirical
study of this issue has been inconclusive (apart from character-level models, and even then
only on smaller datasets (Radford et al., 2017)). Note also that empirical work by its nature
can only establish performance on some available datasets, not on all possible classiﬁcation
tasks. We prove this theorem in Section 4 by providing a nontrivial generalization of a result
combining compressed sensing and learning (Calderbank et al., 2009). In fact, before our
work we do not know of any provable quantiﬁcation of the power of any text embedding.

2. We study theoretically and experimentally how our linear embedding scheme improves
when it uses pretrained embeddings (GloVe etc.) instead of random vectors. Empirically we
ﬁnd that this improves the ability to preserve Bag-of-Words (BoW) information, which has
the following restatement in the language of sparse recovery: word embeddings are better
than random matrices for “sensing” BoW signals (see Section 5). We give some theoretical
justiﬁcation for this surprising ﬁnding using a new sparse recovery property characterizing
when nonnegative signals can be reconstructed by (cid:96)1-minimization.

3. Section 6 provides empirical results supporting the above theoretical work, reporting ac-
curacy of our linear schemes on multiple standard classiﬁcation tasks. Our embeddings
are consistently competitive with recent results and perform much better than all previous
linear methods. Among unsupervised word-level representations they achieve state of the art
performance on both the binary and ﬁne-grained SST sentiment classiﬁcation tasks (Socher
et al., 2013). Since our document representations are fast, compositional, and simple to
implement given standard word embeddings, they provide strong baselines for future work.

2 RELATED WORK

Neural text embeddings are instances of distributed representations, long studied in connectionist
approaches because they decay gracefully with noise and allow distributed processing. Hinton
(1990) provided an early problem formulation, and Plate (1995) provided an elementary solution,
the holographic distributed representation, which represents structured objects using circular vector
convolution and has an easy and more compact implementation using the fast Fourier transform
(FFT). Plate suggested applying such ideas to text, where “structure” can be quantiﬁed using parse
trees and other graph structures. Our method is also closely related in form and composition to the
sparse distributed memory system of Kanerva (2009). In the unigram case our embedding reduces to
the familiar sum of word embeddings, which is known to be surprisingly powerful (Wieting et al.,
2016), and with a few modiﬁcations even more so (Arora et al., 2017).

2

Published as a conference paper at ICLR 2018

Representations of BonG vectors have been studied through the lens of compression by Paskov et al.
(2013), who computed representations based on classical lossless compression algorithms using a
linear program (LP). Their embeddings are still high-dimensional (d > 100K) and quite complicated
to implement. In contrast, linear projection schemes are simpler, more compact, and can leverage
readily available word embeddings. Pagliardini et al. (2017) also used a linear scheme, representing
documents as an average of learned word and bigram embeddings. However, the motivation and
beneﬁts of encoding BonGs in low-dimensions are not made explicit. The novelty in the current paper
is the connection to compressed sensing, which is concerned with recovering high-dimensional sparse
signals x ∈ RN from low-dimensional linear measurements Ax, speciﬁcally by studying conditions
on matrix A ∈ Rd×N when this is possible (see Appendix A for some background on compressed
sensing and the previous work of Calderbank et al. (2009) that we build upon).

3 DOCUMENT EMBEDDINGS

In this section we deﬁne the two types of representations that our analysis will relate:

1. high-dimensional BonG vectors counting the occurrences of each k-gram for k ≤ n

2. low-dimensional embeddings, from simple vector sums to novel n-gram-based embeddings

Although some of these representations have been previously studied and used, we deﬁne them so as
to make clear their connection via compressed sensing, i.e. that representations of the second type
are simply linear measurements of the ﬁrst.

We now deﬁne some notation. Let V be the number of words in the vocabulary and Vn be the number
of n-grams (independent of word order), so that V = V1. Furthermore set V sum
k≤n Vk and
V max
n = maxk≤n Vk. We will use words/n-grams and indices interchangeably, e.g. if (a, b) is the
ith of V2 bigrams then the one-hot vector e(a,b) will be 1 at index i. Where necessary we will use
{, } to denote a multi-set and (, ) to denote a tuple. For any m vectors vi ∈ Rd for i = 1, . . . , m
we deﬁne [v1, . . . , vm] to be their concatenation, which is thus an element of Rmd. Finally, for any
subset X ⊂ RN we denote by ∆X the set {x − x(cid:48) : x, x(cid:48) ∈ X }.

n = (cid:80)

3.1 THE BAG-OF-n-GRAMS VECTOR

Assigning to each word a unique index i ∈ [V ] we deﬁne the Bag-of-Words (BoW) representation
xBoW of a document to be the V -dimensional vector whose ith entry is the number of times word i
occurs in the document. The n-gram extension of BoW is the Bag-of-n-Grams (BonG) representation,
which counts the number of times any k-gram for k ≤ n appears in a document. Linear classiﬁcation
over such vectors has been found to be a strong baseline (Wang & Manning, 2012).

For ease of analysis we simplify the BonG approach by merging all n-grams in the vocabulary that
contain the same words but in a different order. We call these features n-cooccurrences and ﬁnd that
the modiﬁcation does not affect performance signiﬁcantly (see Table 3 in Appendix F.1). Formally for
a document w1, . . . , wT we deﬁne the Bag-of-n-Cooccurrences (BonC) vector as the concatenation

xBonC =

ewt

,

. . .

,

e{wt,...,wt+n−1}

(1)

(cid:34) T

(cid:88)

t=1

T −n+1
(cid:88)

t=1

(cid:35)

which is thus a V sum

n -dimensional vector. Note that for unigrams this is equivalent to the BoW vector.

3.2 LOW-DIMENSIONAL n-GRAM EMBEDDINGS

Now suppose each word w has a vector vw ∈ Rd for some d (cid:28) V . Then given a document
w1, . . . , wT we deﬁne its unigram embedding as zu = (cid:80)T
t=1 vwt. While this is a simple and widely
used featurization, we focus on the following straightforward relation with BoW: if A ∈ Rd×V is
a matrix whose columns are word vectors vw then AxBoW = (cid:80)T
t=1 vwt = zu. Thus
in terms of compressed sensing the unigram embedding of a document is a d-dimensional linear
measurement of its Bag-of-Words vector.

t=1 Aewt = (cid:80)T

3

Published as a conference paper at ICLR 2018

We could extend this unigram embedding to n-grams by ﬁrst deﬁning a representation for each n-
gram as the tensor product of the vectors of its constituent words. Thus for each bigram b = (w1, w2)
we would have vb = vw1 vT
t=1 vwt for each n-gram g = (w1, . . . , wn).
The document embedding would then be the sum of the tensor representations of all n-grams.

w2 and more generally vg = (cid:78)n

The major drawback of this approach is of course the blowup in dimension, which in practice
prevents its use beyond n = 2. To combat this a low-dimensional sketch or projection of the
tensor product can be used, such as the circular convolution operator of Plate (1995). Since we are
interested in representations that can also be constructed by an LSTM, we instead sketch this tensor
product using the element-wise multiplication operation, which we ﬁnd also usually works better
than circular convolution in practice (see Table 4 in Appendix F.1). Thus for the n-cooccurrence
g = {w1, . . . , wn}, we deﬁne the distributed cooccurrence (DisC) embedding ˜vg = d
t=1 vwt.
The coefﬁcient is required when the vectors vw are random and unit norm to ensure that the product
also has close to unit norm (see Lemma B.1). In addition to their convenient form, DisC embeddings
have nice theoretical and practical properties: they preserve the original embedding dimension, they
reduce to unigram (word) embeddings for n = 1, and under mild assumptions they satisfy useful
compressed sensing properties with overwhelming probability (Lemma 4.1).

2 (cid:74)n

n−1

We then deﬁne the DisC document embedding to be the nd-dimensional weighted concatenation,
over k ≤ n, of the sum of the DisC vectors of all k-grams in a document:

(cid:34)

z(n) =

C1

T
(cid:88)

t=1

˜vwt

,

. . .

, Cn

˜v{wt,...,wt+n−1}

(2)

T −n+1
(cid:88)

t=1

(cid:35)

Here scaling factors Ck are set so that all spans of d coordinates have roughly equal norm (for random
embeddings Ck = 1; for word embeddings Ck = 1/k works well). Note that since ˜vwt = vwt we
have z(1) = zu in the unigram case. Furthermore, as with unigram embeddings by comparing (1) and
(2) one can easily construct a (cid:80)n

n matrix A(n) such that z(n) = A(n)xBonC.

k=1 dn × V sum

3.3 LSTM REPRESENTATIONS

As discussed previously, LSTMs have become a common way to apply the expressive power of
RNNs, with success on a variety of classiﬁcation, representation, and sequence-to-sequence tasks.
For document representation, starting with h0 = 0m an m-memory LSTM initialized with word
vectors vw ∈ Rd takes in words w1, . . . , wT one-by-one and computes the document representation
ht = f (Tf (vwt, ht−1)) ◦ ht−1 + i(Ti(vwt, ht−1)) ◦ g(Tg(vwt, ht−1))
(3)
where ht ∈ Rm is the hidden representation at time t, the forget gate f , input gate i, and input
function g are a.e. differentiable nondecreasing elementwise “activation” functions Rm (cid:55)→ Rm, and
afﬁne transformations T∗(x, y) = W∗x + U∗y + b∗ have weight matrices W∗ ∈ Rm×d, U∗ ∈ Rm×m
and bias vectors b∗ ∈ Rm. The LSTM representation of a document is then the state at the last time
step, i.e. zLSTM = hT . Note that we will follow the convention of using LSTM memory to refer to
the dimensionality of the hidden states. Since the LSTM is initialized with an embedding for each
word it requires O(m2 + md + V d) computer memory, but the last term is just a lookup table so the
vocabulary size does not factor into iteration or representation complexity.

From our description of LSTMs it is intuitive to see that one can initialize the gates and input functions
so as to construct the DisC embeddings deﬁned in the previous section. We state this formally and
give the proof in the unigram case (the full proof appears in Appendix B.3):
Proposition 3.1. Given word vectors vw ∈ Rd, one can initialize an O(nd)-memory LSTM (3) that
takes in words w1, . . . , wT (padded by an end-of-document token assigned vector 0d) and constructs
the DisC embedding (2) (up to zero padding), i.e. such that for all documents zLSTM = z(n).

Proof (Unigram Case). Set f (x) = i(x) = g(x) = x, Tf (vwt, ht−1) = Ti(vwt, ht−1) = 1d, and
Tg(vwt, ht−1) = C1vwt. Then ht = ht−1 + C1vwt, so since h0 = 0d we have the ﬁnal LSTM
representation zLSTM = hT = C1

t=1 vwt = z(1).

(cid:80)t

By Proposition 3.1 we can construct a ﬁxed LSTM that can compute compressed BonC representations
on the ﬂy and be further trained by stochastic gradient descent using the same memory.

4

Published as a conference paper at ICLR 2018

4 LSTMS AS COMPRESSED LEARNERS

Our main contribution is to provide the ﬁrst rigorous analysis of the performance of the text embed-
dings that we are aware of, showing that the embeddings of Section 3.2 can provide performance on
downstream classiﬁcation tasks at least as well any linear classiﬁer over BonCs. Before stating the
theorem we make two mild simplifying assumptions on the BonC vectors:

1. The vectors are scaled by

1
√

T

n , where T is the maximum document length. This assumption

is made without loss of generality.

2. No n-cooccurrence contains a word more than once. While this is (infrequently) violated in
practice, the problem can be circumvented by merging words as a preprocessing step.

Theorem 4.1. Let S = {(xi, yi)}m
i=1 be drawn i.i.d. from a distribution D over BonC vectors of
documents of length at most T satisfying assumptions 1 and 2 above and let w0 be the linear classiﬁer
minimizing the logistic loss (cid:96)D. Then for dimension d = ˜Ω
and appropriate choice of
regularization coefﬁcient one can initialize an O(nd)-memory LSTM over i.i.d. word embeddings
vw ∼ U d{±1/
d} such that w.p. (1 − γ)(1 − 2δ) the classiﬁer ˆw minimizing the (cid:96)2-regularized
logistic loss over its representations satisﬁes

(cid:16) T
ε2 log nV max

√

n
γ

(cid:17)

(cid:96)D ( ˆw) ≤ (cid:96)D (w0) + O

(cid:107)w0(cid:107)2

ε +

log

(cid:32)

(cid:114)

1
m

(cid:33)

1
δ

The above theoretical bound shows that LSTMs match BonC performance as ε → 0, which can be
realized by increasing the embedding dimension d (c.f. Figure 5).

4.1 COMPRESSED SENSING AND LEARNING

Compressed sensing is concerned with recovering a high-dimensional k-sparse signal x ∈ RN from
a few linear measurements; given a design matrix A ∈ Rd×N this is formulated as

minimize (cid:107)w(cid:107)0

subject to Aw = z

where z = Ax is the measurement vector. As l0-minimization is NP-hard, research has focused on
sufﬁcient conditions for tractable recovery. One such condition is the Restricted Isometry Property
(RIP), for which Candès & Tao (2005) proved that (5) can be solved by convex relaxation:
Deﬁnition 4.1. A ∈ Rd×N is (X , ε)-RIP for some subset X ⊂ RN if ∀ x ∈ X

(1 − ε)(cid:107)x(cid:107)2 ≤ (cid:107)Ax(cid:107)2 ≤ (1 + ε)(cid:107)x(cid:107)2

We will abuse notation and say (k, ε)-RIP when X is the set of k-sparse vectors. This is the more
common deﬁnition, but ours allows a more general Theorem 4.2 and a tighter bound in Theorem 4.1.

Following these breakthroughs, Calderbank et al. (2009) studied whether it is possible to use the
low-dimensional output of compressed sensing as a surrogate representation for classiﬁcation. They
proved a learning-theoretic bound on the loss of an SVM classiﬁer in the compressed domain
compared to the best classiﬁer in the original domain. In this work we are interested in comparing
the performance of LSTMs with BonC representations, so we need to generalize the Calderbank et al.
(2009) result to handle Lipschitz losses and an arbitrary set X ⊂ RN of high-dimensional signals:
Theorem 4.2. For any subset X ⊂ RN containing the origin let A ∈ Rd×N be (∆X , ε)-RIP and
let m samples S = {(xi, yi)}m
i=1 ⊂ X × {−1, 1} be drawn i.i.d. from some distribution D over X
with (cid:107)x(cid:107)2 ≤ R. If (cid:96) is a λ-Lipschitz convex loss function and w0 ∈ RN is its minimizer over D
then w.p. 1 − 2δ the linear classiﬁer ˆwA ∈ Rd minimizing the (cid:96)2-regularized empirical loss function
(cid:96)SA (w) + 1

2 over the compressed sample SA = {(Axi, yi)}m

i=1 ⊂ Rd × {−1, 1} satisﬁes

2C (cid:107)w(cid:107)2

(cid:96)D( ˆwA) ≤ (cid:96)D(w0) + O

λR(cid:107)w0(cid:107)2

ε +

log

(7)

(cid:32)

(cid:114)

1
m

(cid:33)

1
δ

for appropriate choice of C. Recall that ∆X = {x − x(cid:48) : x, x(cid:48) ∈ X } for any X ⊂ RN .

(4)

(5)

(6)

5

Published as a conference paper at ICLR 2018

While a detailed proof of this theorem is spelled out in Appendix C, the main idea is to compare the
distributional loss incurred by a classiﬁer ˆw in the original space to the loss incurred by A ˆw in the
compressed space. We show that the minimizer of the regularized empirical loss in the original space
( ˆw) is a bounded-coefﬁcient linear combination of samples in S, so its loss depends only on inner
products between points in X . Thus using RIP and a generalization error result by Sridharan et al.
(2008) we can bound the loss of ˆwA, the regularized classiﬁer in the compressed domain. Note that
to get back from Theorem 4.2 the O(
ε) bound for k-sparse inputs of Calderbank et al. (2009) we
can set X to the be the set of k-sparse vectors and assume A is (2k, ε)-RIP.

√

4.2 PROOF OF MAIN RESULT

To apply Theorem 4.2 we need the design matrix A(n) transforming BonCs into the DisC embeddings
of Section 3.2 to satisfy the following RIP condition (Lemma 4.1), which we prove using a restricted
isometry result for structured random sampling matrices in Appendix D:
Lemma 4.1. Assume the setting of Theorem 4.1 and let A(n) be the nd × V sum
n matrix relating DisC
(cid:17)
(cid:16) T
ε2 log nV max
and BonC representations of any document by z(n) = A(n)xBonC. If d = ˜Ω
then A(n)

n
γ

(cid:16)
∆X (n)

(cid:17)
T , ε

is

-RIP w.p. 1 − γ, where X (n)

T

is the set of BonCs of documents of length at most T .

(cid:16)

∆X (n)

(cid:17)
T , ε

Proof of Theorem 4.1. Let ˆS = {(A(n)xi, yi) : (xi, yi) ∈ S}, where A(n) is as in Lemma 4.1. Then
by the same lemma A(n) is
is the set of BonC vectors of
documents of length at most T . By BonC assumption (1) all BonCs lie within the unit ball, so we can
apply Theorem 4.2 with (cid:96) the logistic loss, λ = 1, and R = 1 to get that a classiﬁer ˆw trained using (cid:96)2-
regularized logistic loss over ˆS will satisfy the required bound (4). Since by Proposition 3.1 one can
initialize an O(nd)-memory LSTM that takes in i.i.d. Rademacher word vectors vw ∼ U d{±1/
d}
such that zLSTM = z(n) = A(n)x ∀ x ∈ X (n)

-RIP w.p. 1 − γ, where X (n)

T , this completes the proof.

√

T

5 SPARSE RECOVERY WITH PRETRAINED EMBEDDINGS

Theorem 4.1 is proved using random vectors as the word embeddings in the scheme of Section 3.
However, in practice LSTMs are often initialized with standard word vectors such as GloVe. Such
embeddings cannot satisfy traditional compressed sensing properties such as RIP or incoherence. This
follows essentially from the deﬁnition: word embeddings seek to capture word similarity, so similar
words (e.g. synonyms) have embeddings with high inner product, which violates both properties.
Thus the efﬁcacy of real-life LSTMs must have some other explanation. But in this section we present
the surprising empirical ﬁnding that pretrained word embeddings are more efﬁcient than random
vectors at encoding and recovering BoW information via compressed sensing. We further sketch a
potential explanation for this result, though a rigorous explanation is left for subsequent work.

5.1 PRETRAINED EMBEDDINGS PRESERVE SPARSE INFORMATION

In recent years word embeddings have been discovered to have many remarkable properties, most
famously the ability to solve analogies (Mikolov et al., 2013). Our connection to compressed sensing
indicates that they should have another: preservation of sparse signals as low-dimensional linear
measurements. To examine this we subsample documents from the SST (Socher et al., 2013) and
IMDB (Maas et al., 2011) classiﬁcation datasets, embed them as d-dimensional unigram embeddings
z = Ax for d = 50, 100, 200, . . . , 1600 (where A ∈ Rd×V is the matrix of word embeddings and
x is a document’s BoW vector), solve the following LP, known as Basis Pursuit (BP), which is the
standard (cid:96)1-minimization problem for sparse recovery in the noiseless case (see Appendix A):

minimize (cid:107)w(cid:107)1

subject to Aw = z

(8)

Success is measured as the F1 score of retrieved words. We use Squared Norm (SN) vectors (Arora
et al., 2016) trained on a corpus of Amazon reviews (McAuley et al., 2015) and normalized i.i.d.
Rademacher vectors as a baseline. SN is used due to similarity to GloVe and its formulation via
an easy-to-analyze generative model that may provide a framework to understand the results (see
Appendix F.2), while the Amazon corpus is used for its semantic closeness to the sentiment datasets.

6

Published as a conference paper at ICLR 2018

Figure 1: Average F1-score of 200 recovered
BoW vectors from SST (left) and IMDB (right)
compared to dimension. Pretrained word embed-
dings (SN trained on Amazon reviews) need half
the dimensionality of normalized Rademacher
vectors to achieve near-perfect recovery. Note
that IMDB documents are on average more than
ten times longer than SST documents.

Figure 2: F1-score of 1000 recovered BoWs
compared to number of unique words. Real doc-
uments (left) are drawn from the SST and IMDB
corpora; random signals (right) are created by
picking words at random. For d = 200, pre-
trained embeddings are better than Rademacher
vectors as sensing vectors for natural language
BoW but are worse for random sparse signals.

Figures 1 and 2 show that pretrained embeddings require a lower dimension d than random vectors
to recover natural language BoW. This is surprising as the training objective goes against standard
conditions such as approximate isometry and incoherence; indeed as shown in Figure 2 recovery
is poor for randomly generated word collections. The latter outcome indicates that the fact that a
document is a set of mutually meaningful words is important for sparse recovery using embeddings
trained on co-occurrences. We achieve similar results with other objectives (e.g. GloVe/word2vec)
and other corpora (see Appendix F.1), although there is some sensitivity to the sparse recovery method,
as other (cid:96)1-minimization methods work well but greedy methods, such as Orthogonal Matching
Pursuit (OMP), work poorly, likely due to their dependence on incoherence (Tropp, 2004).

For the n-gram case (i.e. BonC recovery for n > 1), although we know by Lemma 4.1 that DisC
embeddings composed from random vectors satisfy RIP, for pretrained vectors it is unclear how to
reason about suitable n-gram embeddings without a rigorous understanding of the unigram case,
and experiments do not show the same recovery beneﬁts. One could perhaps do well by training on
cooccurrences of word tuples, but such embeddings could not be used by a word-level LSTM.

5.2 UNDERSTANDING SPARSE RECOVERY OF NATURAL LANGUAGE DOCUMENTS

As shown in Figure 2, the success of pretrained embeddings for linear sensing is a local phenomenon;
recovery is only efﬁcient for naturally occurring collections of words. However, applying statistical
RIP/incoherence ideas (Barg et al., 2015) to explain this is ruled out since they require collections to
be incoherent with high probability, whereas word embeddings are trained to give high inner product
to words appearing together. Thus an explanation must come from some other, weaker condition.
The usual necessary and sufﬁcient requirement for recovering all signals with support S ⊂ [N ] is the
local nullspace property (NSP), which stipulates that vectors in the kernel of A not have too much
mass on S (see Deﬁnition A.2). While NSP and related properties such as restricted eigenvalue (see
Deﬁnition A.3) are hard to check, we can impose some additional structure to formulate an intuitive,
veriﬁable perfect recovery condition for our setting. Speciﬁcally, since our signals (BoW vectors) are
nonnegative, we can improve upon solving BP (8) by instead solving nonnegative BP (BP+):

minimize (cid:107)w(cid:107)1

subject to Aw = z, w ≥ 0d

(9)
The following geometric result then characterizes when solutions of BP+ recover the correct signal:
Theorem 5.1 (Donoho & Tanner, 2005). Consider a matrix A ∈ Rd×N and an index subset S ⊂ [N ]
of size k. Then any nonnegative vector x ∈ RN
+ with support supp(x) = S is recovered from Ax by
BP+ iff the set AS of columns of A indexed by S comprise the vertices of a k-dimensional face of the
convex hull conv(A) of the columns of A together with the origin.

This theorem equates perfect recovery of a BoW vector via BP+ with the vectors of its words being
the vertices of some face of the polytope conv(A). The property holds for incoherent columns since
the vectors are far enough that no one vector is inside the simplex formed by any k others. On the
other hand, pretrained embeddings satisfy it by having commonly co-occurring words close together
and other words far away, making it easier to form a face from columns indexed by the support of a
BoW. We formalize this intuition as the Supporting Hyperplane Property (SHP):

7

Published as a conference paper at ICLR 2018

Figure 3: Proportion of 500 randomly sampled documents from SST (top) and IMDB (bottom) that
are perfectly recovered from linear measurements.

Deﬁnition 5.1. A matrix A ∈ Rd×N satisﬁes S-SHP for subset S ⊂ [N ] if its columns are in general
position and there is a hyperplane containing the set AS of columns of A indexed by S such that the
set of all other columns of A together with the origin are on one side of the hyperplane.

SHP is a very weak property implied by NSP (Corollary E.1). However, it can be checked by using
convex optimization to see if the hyperplane exists (Appendix E.2). Furthermore, we show (full
proof in Appendix E.1) that this hyperplane is the supporting hyperplane of the face of conv(A) with
vertices AS, from which it follows by Theorem 5.1 that SHP characterizes recovery using BP+:
Corollary 5.1. BP+ recovers any x ∈ RN

+ with supp(x) = S from Ax iff A satisﬁes S-SHP.

Proof Sketch. By Theorem 5.1 it sufﬁces to show equivalence of S-SHP with the column set AS
comprising the vertices of a k-dimensional face of conv(A). A face F of polytope P is deﬁned as its
intersection with some hyperplane such that all points in P \F lie on one side of the hyperplane.
( =⇒ ) Let F be the face of conv(A) formed by the columns AS. Then there must be a supporting
hyperplane H containing F . Since the columns of A are in general position, all columns AS = A\AS
lie in conv(A)\F and hence must all be on one side of H, so H is the desired hyperplane.
( ⇐= ) Let H be the hyperplane supporting AS, with all other columns on one side of H. By
convexity, H contains the simplex F of AS. Any point in conv(A)\F can be written as a convex
combination of points in F and columns AS, with a positive coefﬁcient on at least one of the columns,
and so must lie on the same side of H as AS. Thus AS comprises the vertices of a face F .

Thus perfect recovery of a BoW via BP+ is equivalent to the existence of a hyperplane separating
embeddings of words in the document from those of the rest of the vocabulary. Intuitively, words
in the same document are trained to have similar embeddings and so will be easier to separate out,
providing some justiﬁcation for why pretrained vectors are better for sensing. We verify that SHP
is indeed more likely to be satisﬁed by such designs in Figure 3, which also serves as an empirical
check of Corollary 5.1 since SHP satisfaction implies BP recovery as the latter can do no better
than BP+. We further compare to recovery using OMP/OMP+ (the latter removes negative values
and recomputes the set of atoms at each iteration); interestingly, while OMP+ recovers the correct
signal from SN almost as often as BP/BP+, it performs quite poorly for GloVe, indicating that these
embeddings may have quite different sensing properties despite similar training objectives.

As similarity properties that may explain these results also relate to downstream task performance, we
conjecture a relationship between embeddings, recovery, and classiﬁcation that may be understood
under a generative model (see Appendix F.2). However, the Section 4 bounds depend on RIP, not
recovery, so these experiments by themselves do not apply. They do show that the compressed sensing
framework remains relevant even in the case of non-random, pretrained word embeddings.

8

Published as a conference paper at ICLR 2018

Representation

BonC (1)

DisC (2)

n

1
2
3

1
2
3

d∗
V1
V sum
2
V sum
3

1600
3200
4800

MR

77.1
77.8
77.8

79.6
80.1
80.0

CR

77.0
78.1
78.3

81.0
81.5
81.3

SUBJ MPQA TREC SST (±1)
91.0
91.8
91.4

86.8
90.0
89.8

80.7
80.9
80.1

85.1
85.8
85.6

92.4
92.6
92.6

87.8
87.9
87.9

85.2
89.6
90.0

84.6
85.5
85.2

SST

36.8
39.0
42.3

45.7
46.4
46.7

IMDB

88.3
90.0
89.8

89.2
89.4
89.6

45.8
31.0
30.7

84.4
80.2
80.0

79.6
76.2
76.3

87.7
87.2
86.6

81.1
78.7
79.1

92.5
91.2
91.1

85.6
85.8
84.2

SIF1
Sent2Vec2
Sent2Vec2
CFL3

1600
700
700
100K+

1
1
2
5
Paragraph Vec.4
skip-thoughts4
SDAE5
CNN-LSTM6
byte mLSTM7
∗ Vocabulary sizes (i.e. BonC dimensions) vary by task; usually 10K-100K.
1 Arora et al. (2017) Reported performance of best hyperparameter using Amazon GloVe embeddings.
2,4,7 Pagliardini et al. (2017); Kiros et al. (2015); Radford et al. (2017) Evaluated latest pretrained models.

91.8
93.0
78.4
92.6

74.8
80.3
74.6
77.8

74.2
88.9
86.9
89.4

78.1
83.8
78.0
82.0

90.5
94.2
90.8
93.6

89.2
85.5
85.3
90.4

4800
2400
4800

4096

88.8

54.6

91.7

86.8

90.6

94.7

92.2

85.1

45.8

90.4

Note that the available skip-thoughts implementation fails on the IMDB and MRPC tasks

3,5,6 Paskov et al. (2013); Hill et al. (2016); Gan et al. (2017) From publication (+emb version of last two).

Table 1: Evaluation of DisC and recent unsupervised word-level approaches on standard classiﬁcation
tasks, with the character LSTM of Radford et al. (2017) shown for comparison. The top three results
for each dataset are bolded, the best is italicized, and the best word-level performance is underlined.

6 EMPIRICAL FINDINGS

Our theoretical results show that simple tensor product sketch-based n-gram embeddings can approach
BonG performance and be computed by a low-memory LSTM. In this section we compare these text
representations and others on several standard tasks, verifying that DisC performance approaches that
of BonCs as dimensionality increases and establishing several baselines for text classiﬁcation. Code to
reproduce results is provided at https://github.com/NLPrinceton/text_embedding.

Tasks: We test classiﬁcation on MR movie reviews (Pang & Lee, 2005), CR customer reviews (Hu &
Liu, 2004), SUBJ subjectivity dataset (Pang & Lee, 2004), MPQA opinion polarity subtask (Wiebe
et al., 2005), TREC question classiﬁcation (Li & Roth, 2002), SST sentiment classiﬁcation (binary
and ﬁne-grained) (Socher et al., 2013), and IMDB movie reviews (Maas et al., 2011). The ﬁrst four
are evaluated using 10-fold cross-validation, while the others have train-test splits. In all cases we use
logistic regression with (cid:96)2-regularization determined by cross-validation. We further test DisC on the
SICK relatedness and entailment tasks (Marelli et al., 2014) and the MRPC paraphrase detection task
(Dolan & Brockett, 2005). The inputs here are sentences pairs (a, b) and the standard featurization
for document embeddings xa and xb of a and b is [|xa − xb|, xa (cid:12) xb] (Tai et al., 2015). We use
logistic regression for SICK entailment and MRPC and use ridge regression to predict similarity
scores for SICK relatedness, with (cid:96)2-regularization determined by cross-validation. Since BonGs
are not used for pairwise tasks our theory says nothing about performance here; we include these
evaluations to show that our representations are also useful for other tasks.

Embeddings: In the main evaluation (Table 1) we use normalized 1600-dimensional GloVe embed-
dings (Pennington et al., 2014) trained on the Amazon Product Corpus (McAuley et al., 2015), which
are released at http://nlp.cs.princeton.edu/DisC. We also compare the SN vectors of
Section 5 trained on the same corpus with random vectors when varying the dimension (Figure 5).

9

Published as a conference paper at ICLR 2018

SICK-E MRPC (Acc./F1)

Rep.

DisC (2)

SIF
Sent2Vec
Sent2Vec

n

1
2
3

1
1
2

skip-thoughts
SDAE
CNN-LSTM

SICK-R (r/ρ)
73.6 / 71.0
75.6 / 72.1
76.2 / 72.2

73.7 / 68.5
69.3 / 64.6
70.1 / 65.3

82.4 / 76.0

81.9
83.2
82.5

82.4
78.6
78.7

83.2

73.1 / 81.8
70.8 / 79.0
73.1 / 81.6

73.5 / 82.1
71.3 / 80.7
70.0 / 79.3

73.7 / 80.7
76.4 / 83.8

73.8 / 81.4

byte mLSTM

78.5 / 72.1

80.0

Model information is the same as that in Table 1.

Table 2: Performance of DisC and other recent approaches
on pairwise similarity and classiﬁcation tasks. The top three
results for each task are bolded and the best is underlined.

Figure 4: Time needed to initialize
model, construct document representa-
tions, and train a linear classiﬁer on a
16-core compute node.

Figure 5: IMDB performance of unigram (left) and bigram
(right) DisC embeddings compared to the original dimension.

Figure 6: IMDB performance com-
pared to training sample size.

Results: We ﬁnd that DisC representation performs consistently well relative to recent unsupervised
methods; among word-level approaches it is the top performer on the SST tasks and competes on many
others with skip-thoughts and CNN-LSTM, both concatenations of two LSTM representations. While
success may be explained by training on a large and in-domain corpus, being able to use so much text
without extravagant computing resources is one of the advantages of a simple approach. Overall our
method is useful as a strong baseline, often beating BonCs and many more complicated approaches
while taking much less time to represent and train on documents than neural representations (Figure 4).

Finally, we analyze empirically how well our model approximates BonC performance. As predicted
by Theorem 4.1, the performance of random embeddings on IMDB approaches that of BonC as
dimension increases and the isometry distortion ε decreases (Figure 5). Using pretrained (SN) vectors,
DisC embeddings approach BonC performance much earlier, surpassing it in the unigram case.

7 CONCLUSION

In this paper we explored the connection between compressed sensing, learning, and natural language
representation. We ﬁrst related LSTM and BonG methods via word embeddings, coming up with sim-
ple new document embeddings based on tensor product sketches. Then we studied their classiﬁcation
performance, proving a generalization of the compressed learning result of Calderbank et al. (2009)
to convex Lipschitz losses and a bound on the loss of a low-dimensional LSTM classiﬁer in terms
of its (modiﬁed) BonG counterpart, an issue which neither experiments nor theory have been able
to resolve. Finally, we showed how pretrained embeddings ﬁt into this sparse recovery framework,
demonstrating and explaining their ability to efﬁciently preserve natural language information.

10

Published as a conference paper at ICLR 2018

ACKNOWLEDGMENTS

We thank Rong Ge, Holden Lee, and Divyarthi Mohan for helpful discussions at various stages of this
effort. The work in this paper was in part supported by NSF grants CCF-1302518 and CCF-1527371,
Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329

REFERENCES

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model

approach to pmi-based word embeddings. Transactions of the ACL, 4:385–399, 2016.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In Proceedings of the International Conference on Learning Representations, 2017.

Alexander Barg, Arya Mazumdar, and Rongrong Wang. Restricted isometry property of random

subdictionaries. IEEE Transactions on Information Theory, 61, 2015.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic

language model. Journal of Machine Learning Research, 3:1137–1155, 2003.

Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.

Robert Calderbank, Sina Jafarpour, and Robert Schapire. Compressed learning: Universal sparse

dimensionality reduction and learning in the measurement domain. Technical report, 2009.

Emmanuel Candès and Terence Tao. Decoding by linear programming. IEEE Transactions on

Information Theory, 51:4203–4215, 2005.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th International Conference on
Machine Learning, 2008.

Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In

Third International Workshop on Paraphrasing, 2005.

David L. Donoho and Jared Tanner. Sparse nonnegative solution of underdetermined linear equations
by linear programming. Proceedings of the National Academy of Sciences of the United States of
America., 102:9446–9451, 2005.

Simon Foucart and David Koslicki. Sparse recovery by means of nonnegative least squares. IEEE

Signal Processing Letters, 21:498–502, 2014.

Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing. 2013.

Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin Carin.
Learning generic sentence representations using convolutional neural networks. In Proceedings of
Empirical Methods in Natural Language Processing, 2017.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences

from unlabelled data. In Proceedings of the North American Chapter of the ACL, 2016.

Geoffrey Hinton. Mapping part-whole hierarchies into connectionist networks. Artiﬁcial Intelligence,

46:47–75, 1990.

1735–1780, 1997.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:

Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the 10th

ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efﬁcient
text classiﬁcation. In Proceedings of the 15th Conference of the European Chapter of the ACL,
2017.

11

Published as a conference paper at ICLR 2018

Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed represen-

tation with high-dimensional random vectors. Cognitive Computation, 1:139–159, 2009.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun,

and Sanja Fidler. Skip-thought vectors. In Neural Information Processing Systems, 2015.

Xin Li and Dan Roth. Learning question classiﬁers.
Conference on Computational Linguistics, 2002.

In Proceedings of the 19th International

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of
the ACL: Human Language Technologies, 2011.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto
Zamparelli. Semeval-2014 task 1: Evaluation of compositional distributional semantic model
on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th
International Workshop on Semantic Evaluation, 2014.

Julian McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and com-
plementary products. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2015.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeffrey Dean. Distributed representa-
tions of words and phrases and their compositionality. In Neural Information Processing Systems,
2013.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embeddings

using compositional n-gram features. arXiv, 2017.

Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the ACL,
2004.

Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization

with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the ACL, 2005.

Hristo S. Paskov, Robert West, John C. Mitchell, and Trevor J. Hastie. Compressive feature learning.

In Neural Information Processing Systems, 2013.

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Proceedings of Empirical Methods in Natural Language Processing, 2014.

Tony Plate. Holographic reduced representations. IEEE Transactions on Neural Networks, 1995.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering

sentiment, 2017. arXiv.

Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated

gaussian designs. Journal of Machine Learning Research, 11(Aug):2241–2259, 2010.

Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y.
Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of Empirical Methods in Natural Language Processing, 2013.

Karthik Sridharan, Nathan Srebro, and Shai Shalev-Schwartz. Fast rates for regularized objectives.

In Neural Information Processing Systems. 2008.

Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations
from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting
of the ACL, 2015.

Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on

Information Theory, 50, 2004.

12

Published as a conference paper at ICLR 2018

Sida Wang and Christopher D. Manning. Baselines and bigrams: Simple, good sentiment and topic

classiﬁcation. In Proceedings of the 50th Annual Meeting of the ACL, 2012.

Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions

in language. Language Resources and Evaluation, 39:165–210, 2005.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards universal paraphrastic
sentence embeddings. In Proceedings of the International Conference on Learning Representations,
2016.

A COMPRESSED SENSING BACKGROUND

The ﬁeld of compressed sensing is concerned with recovering a high-dimensional k-sparse signal
x ∈ RN from few linear measurements. In the noiseless case this is formulated as

(10)
where A ∈ Rd×N is the design matrix and z = Ax is the measurement vector. Since (cid:96)0-minimization
is NP-hard, a foundational approach is to use its convex surrogate, the (cid:96)1-norm, and characterize
when the solution to (10) is equivalent to that of the following LP, known as basis pursuit (BP):

subject to Aw = z

minimize (cid:107)w(cid:107)0

minimize (cid:107)w(cid:107)1

subject to Aw = z

(11)

Related approaches such as Basis Pursuit Denoising (LASSO) and the Dantzig Selector generalize
BP to handle signal or measurement noise (Foucart & Rauhut, 2013); however, the word embeddings
case is noiseless so these methods reduce to BP. Note that throughout Section 5 and the Appendix
we say that an (cid:96)1-minimization method recovers x from Ax if its optimal solution is unique and
equivalent to the optimal solution of (10).

An alternative way to approximately solve (10) is to use a greedy algorithm such as matching pursuit
(MP) or orthogonal matching pursuit (OMP), which pick basis vectors one at a time by multiplying
the measurement vector by AT and choosing the column with the largest inner product (Tropp, 2004).

A.1 SOME COMMON SPARSE RECOVERY CONDITIONS

One condition through which recovery can be guaranteed is the Restricted Isometry Property (RIP):
Deﬁnition A.1. A ∈ Rd×N is (k, ε)-RIP if for all k-sparse x ∈ RN

(1 − ε)(cid:107)x(cid:107)2 ≤ (cid:107)Ax(cid:107)2 ≤ (1 + ε)(cid:107)x(cid:107)2

√

A line of work started by Candès & Tao (2005) used the RIP property to characterize matrices A such
that (10) and (11) have the same minimizer for any k-sparse signal x; this occurs with overwhelming
probability when d = Ω (cid:0)k log N
Since the ability to recover a signal x from a representation Ax implies information preservation, a
natural next step is to consider learning after compression. Calderbank et al. (2009) show that for
m i.i.d. k-sparse samples {(xi, yi)}m
i=1 and a (2k, ε)-RIP matrix A, the hinge loss of a classiﬁer
trained on {(Axi, yi)}m
i=1 is bounded by that of the best linear classiﬁer over the original samples.
Theorem 4.2 provides a generalization of this result to any convex Lipschitz loss function.

dAij ∼ N (0, 1) ∀ i, j or

dAij ∼ U{−1, 1} ∀ i, j.

(cid:1) and

√

k

RIP is a strong requirement, both because it is not necessary for perfect, stable recovery of k-sparse
vectors using ˜O(k) measurements and because in certain settings we are interested in using the above
ideas to recover speciﬁc signals — those statistically likely to occur—rather than all k-sparse signals.
The usual necessary and sufﬁcient condition to recover any vector x ∈ RN with index support set
S ⊂ [N ] is the local nullspace property (NSP), which is implied by RIP:
Deﬁnition A.2 (Foucart & Rauhut, 2013). A matrix A ∈ Rd×N satisﬁes NSP for a set S ⊂ [N ] if
(cid:107)wS(cid:107)1 < (cid:107)wS(cid:107)1 for all nonzero w ∈ ker(A) = {v : Av = 0d}.
Theorem A.1 (Foucart & Rauhut, 2013). BP (11) recovers any x ∈ RN
iff A satisﬁes NSP for S.

+ with supp(x) = S from Ax

13

Published as a conference paper at ICLR 2018

A related condition that implies NSP is the local restricted eigenvalue property (REP):
Deﬁnition A.3 (Raskutti et al., 2010). A matrix A ∈ Rd×N satisﬁes γ-REP for a set S ⊂ [N ] if
(cid:107)Aw(cid:107)2 ≥ γ

d(cid:107)w(cid:107)2 whenever (cid:107)wS(cid:107)1 ≤ (cid:107)wS(cid:107)1.

√

Lastly, a simple condition that can sometimes provide recovery guarantees is mutual incoherence:
Deﬁnition A.4. A ∈ Rd×N is µ-incoherent if maxa,a(cid:48) |aT a(cid:48)| ≤ µ, where the maximum is taken over
any two distinct columns a, a(cid:48) of A.

While incoherence is easy to verify (unlike the previous recovery properties), word embeddings tend
to have high coherence due to the training objective pushing together vectors of co-occurring words.

A.2 CONDITIONS FOR RECOVERING NONNEGATIVE SIGNALS

Apart from incoherence, the properties above are hard show empirically. However, we are compress-
ing BoW vectors, so our signals are nonnegative and we can impose an additional constraint on (11):

minimize (cid:107)w(cid:107)1

subject to Aw = z, w ≥ 0d

(12)
The following geometric result provides guarantees for this nonnegative basis pursuit (BP+) problem:
Theorem A.2 (Donoho & Tanner, 2005). Consider a matrix A ∈ Rd×N and an index subset S ⊂ [N ]
of size k. Then any nonnegative vector x ∈ RN
+ with support supp(x) = S is recovered from Ax
by BP+ (12) iff the columns of A indexed by S comprise the vertices of a k-dimensional face of the
convex hull conv(A) of the columns of A together with the origin.

The polytope condition is equivalent to nonnegative NSP (NSP+), a weaker form of NSP:
Deﬁnition A.5 (Foucart & Koslicki, 2014). A matrix A ∈ Rd×N satisﬁes NSP+ for a set S ⊂ [N ] if

wS ≥ 0N =⇒

wi > 0 for all nonzero w ∈ ker(A).

N
(cid:80)
i=1

Lemma A.1. If A ∈ Rd×N satisﬁes NSP for some S ⊂ [N ] then it also satisﬁes NSP+ for S.

Proof (Adapted from Foucart & Koslicki (2014)). Since A satisﬁes NSP, we have (cid:107)wS(cid:107)1 < (cid:107)wS(cid:107)1.
Then for a nonzero w ∈ ker(A) such that wS ≥ 0 we will have

N
(cid:88)

i=1

wi =

wi +

wj ≥ −

|wi| +

|wj| = −(cid:107)wS(cid:107)1 + (cid:107)wS(cid:107)1 > 0

(cid:88)

i∈S

(cid:88)

j∈S

(cid:88)

i∈S

(cid:88)

j∈S

Lemma A.2. BP+ recovers any x ∈ RN

+ with supp(x) = S from Ax iff A satisﬁes NSP+ for S.

Proof. ( =⇒ ): For any nonzero w ∈ ker(A) such that wS ≥ 0, ∃ λ > 0 such that x + λw ≥ 0N
and A(x + λw) = Ax. Since BP+ uniquely recovers x, we have (cid:107)x + λw(cid:107)1 > (cid:107)x(cid:107)1, so NSP+
follows from the following inequality and the fact that λ is positive:

0 < (cid:107)x + λw(cid:107)1 − (cid:107)x(cid:107)1 =

(xi + λwi) −

xi = λ

wi =⇒

wi > 0

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

( ⇐= ): For any x(cid:48) ≥ 0 such that Ax(cid:48) = Ax we have that w = x(cid:48) − x ∈ ker(A) and wS = x(cid:48)
S

≥ 0

since the support of x is S. Thus by NSP+ we have that

wi > 0, which yields

N
(cid:88)

i=1

N
(cid:80)
i=1

(cid:107)x(cid:48)(cid:107)1 − (cid:107)x(cid:107)1 =

x(cid:48)
i −

xi =

wi > 0

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

Thus BP+ will recover x uniquely.

Lemma A.2 shows that NSP+ is equivalent to the polytope condition in Theorem A.2, as they are
both necessary and sufﬁcient conditions for BP+ recovery.

14

Published as a conference paper at ICLR 2018

Model

Bigram

Trigram

Representation MR
78.3
77.8

BonG
BonC

BonG
BonC

77.9
77.8

CR
78.2
78.1

78.3
78.3

SUBJ MPQA TREC SST (±1)
91.9
91.8

85.7
85.8

90.0
90.0

80.8
80.9

91.5
91.4

85.6
85.6

89.2
89.8

80.2
80.1

SST IMDB
90.0
39.1
90.0
39.0

39.1
42.3

90.2
89.8

Table 3: The performance of an l2-regularized logit classiﬁer over Bag-of-n-Grams (BonG) vectors
is generally similar to that of Bag-of-n-Cooccurrences (BonC) vectors for n = 2, 3 (largest differ-
ences bolded). Evaluation settings are the same as in Section 6. Note that for unigrams the two
representations are equivalent.

Model

Bigram

Trigram

Representation MR
80.1
79.8

DisC (2)
Conv. (13)

DisC (2)
Conv. (13)

80.0
79.7

CR
81.5
81.4

81.3
81.6

SUBJ MPQA TREC SST (±1)
92.6
92.6

85.5
84.3

87.9
87.9

89.6
89.4

92.6
92.8

87.9
87.9

90.0
89.8

85.2
84.2

SST IMDB
89.4
46.4
89.5
45.5

46.7
45.8

89.6
89.5

Table 4: Performance comparison of element-wise product (DisC) and circular convolution for
encoding local cooccurrences (best result for each task is bolded). Evaluation settings are the same
as in Section 6. Note that for unigrams the two representations are equivalent.

B DOCUMENT EMBEDDINGS

B.1 PERFORMANCE COMPARISON OF ALTERNATIVE REPRESENTATIONS

In this section we compare the performance of several alternative representations with the ones
presented in the main evaluation (Table 1). Table 3 provides a numerical justiﬁcation for our use of
unordered n-grams (cooccurrences) instead of n-grams, as the performance of the two featurizations
are closely comparable. In Table 4 we examine the use of circular convolution instead of elementwise
multiplication as linear measurements of BonC vectors Plate (1995). To construct the former from a
document w1, . . . , wT we compute

(cid:34) T

(cid:88)

t=1

vwt, . . . ,

T −n+1
(cid:88)

F −1

(cid:32)t+n−1
(cid:89)

t=1

τ =t

(cid:33)(cid:35)

F(vwτ )

(13)

where F is the discrete Fourier transform and F −1 its inverse. Note that for n = 1 this is equivalent
to the simple unigram embedding (and thus also to the DisC embedding in (2)).

B.2 SCALING FACTOR FOR DISC EMBEDDINGS

Lemma B.1. If word vectors vw ∈ Rd are drawn i.i.d. from U d{±1/
g = (w1, . . . , wn) we have E(cid:107)˜vg(cid:107)2
that all words in g are distinct if the word vectors are i.i.d. d-dimensional spherical Gaussians.

d} then for any n-gram
2 = 1. The same result holds true with the additional assumption

√

Proof.

E (cid:107)˜vg(cid:107)2

2 =

(cid:32)

d
(cid:88)

E

n−1
2

d

i=1

n
(cid:89)

k=1

(cid:33)2

vwk i

= dn−1

d
(cid:88)

E

(cid:32) n
(cid:89)

i=1

k=1

(cid:33)

v2
wk i

= dn−1

d
(cid:88)

n
(cid:89)

i=1

i=1

1
d

= 1

15

Published as a conference paper at ICLR 2018

B.3 PROOF OF PROPOSITION 3.1

Let f (x) = i(x) = g(x) = x with
(cid:18) 1nd
0(n−1)d

Tf (vwt, ht−1) =

(cid:19)

Ti(vwt, ht−1) =

Tg(vwt, ht−1) =

· · ·

. . .
. . .

0d×nd
...
...
...
0(n−2)d×nd

C1Id
...

Cnd

n−1

2 Id























Id
...
Id










vwt

I(n−2)d 0(n−2)d×d

0d×d

Id

0d×d












ht−1 +






1d
0(n−1)d
1d
0(n−2)d






I(n−2)d 0(n−2)d×d

Substituting these parameters into the LSTM update (3) and using h0 = 0 we have ∀ t > 0 that

ht =

Cnd

n−1
2

k=1 vwτ +k−1

=

Cnd

n−1
2

















C1

t
(cid:80)
τ =1

vwτ

...
(cid:74)n

t−n+1
(cid:80)
τ =1

vwt
...
k=1 vwt+k−n+1

(cid:74)n−1

































C1

t
(cid:80)
τ =1

˜vwτ

t−n+1
(cid:80)
τ =1

...
˜v{wτ ,...,wτ +n−1}
˜vwt
...
˜v{wt−n+2,...,wt}

















Thus

hT =

Cnd

n−1
2

T −n+1
(cid:80)
t=1

















C1

˜vwt

T
(cid:80)
t=1

...
˜v{wt,...,wt+n−1}

˜vwT
...
˜v{wT −n+2,...,wT }

















=







˜z(n)
˜vwT
...
˜v{wT −n+2,...,wT }







Note that ht ∈ R(2n−1)d so as desired the LSTM has O(nd)-memory. Although hT contains (n−1)d
more dimensions than ˜z(n), by padding the end of the document with an end-of-document token
whose word vector is 0d the entries in those dimensions will be set to zero by the update at the last
step. Thus up to zero padding we will have zLSTM = hT = ˜z(n).

C PROOF OF THEOREM 4.2

Throughout this section we assume the setting described in Theorem 4.2. Furthermore for some
positive constant C deﬁne the (cid:96)2-regularization of the loss function (cid:96) as

L(w) = (cid:96)(w) +

1
2C

(cid:107)w(cid:107)2
2

16

Published as a conference paper at ICLR 2018

Lemma C.1. Let ˆw be the classiﬁer obtained minimizing LS(w) = 1
m

(cid:96)(wT xi, yi) + 1

2C (cid:107)w(cid:107)2
2,

where (cid:96)(·, ·) is a convex λ-Lipschitz function in the ﬁrst cordinate. Then

m
(cid:80)
i=1

ˆw =

αiyixi

m
(cid:88)

i=1

(14)

where |αi| ≤ λC

m ∀ i. This result holds in the compressed domain as well.

Proof. If (cid:96) is an λ-Lipschitz function, its sub-gradient at every point is bounded by λ. So by convexity,
the unique optimizer is given by taking ﬁrst-order conditions:

0 = ∂wLS(w) =

+

∂wT xi(cid:96)(wT xi, yi)xi

w
C

1
m

m
(cid:88)

i=1

C
m

m
(cid:88)

i=1

=⇒ ˆw =

−yi∂ ˆwT xi(cid:96)( ˆwT xi, yi)yixi

(15)

Since (cid:96) is Lipschitz, |∂wT xi(cid:96)(wT xi, yi)| ≤ λ. Therefore the ﬁrst-order optimal solution (15) of ˆw
can be expressed as (14) for some α1, . . . , αm satisfying |αi| ≤ λC
m ∀ i, which is the desired result.

Lemma C.2. x, x(cid:48) ∈ X =⇒ (1 + ε)xT x(cid:48) − 2R2ε ≤ (Ax)T (Ax(cid:48)) ≤ (1 − ε)xT x(cid:48) + 2R2ε

Proof. Since A is (∆X , ε)-RIP we have (1 − ε)(cid:107)x − x(cid:48)(cid:107)2 ≤ (cid:107)A(x − x(cid:48))(cid:107)2 ≤ (1 + ε)(cid:107)x − x(cid:48)(cid:107)2.
Also since 0N ∈ X , A is also (X , ε)-RIP and the result then follows by the same argument as in
(Calderbank et al., 2009, Lemma 4.2-3).

Corollary C.1. (cid:107) ˆw(cid:107)2

2 ≤ λ2C 2R2 and (cid:107) ˆwA(cid:107)2

2 ≤ λ2C 2(1 + ε)2R2.

Proof. The ﬁrst bound follows by expanding (cid:107) ˆw(cid:107)2
expanding (cid:107) ˆwA(cid:107)2

2, applying Lemma C.2 to bound inner product distortion, and using (cid:107)x(cid:107)2 ≤ R.

2 and using (cid:107)x(cid:107)2 ≤ R; the second follows by

Lemma C.3. Let ˆw be the linear classiﬁer minimizing LS. Then

LD(A ˆw) ≤ LD( ˆw) + O(λ2CR2ε)

Proof. By Lemma C.1 we can re-express ˆw using Equation 14 and then apply the inequality from
Lemma C.2 to get

(A ˆw)T (Ax) =

αiyi(Axi)T (Ax)

αiyi

(cid:0)(1 − ε)xT

i x + 2R2ε(cid:1) +

(cid:88)

αiyi

(cid:0)(1 + ε)xT

i x − 2R2ε(cid:1)

i:αiyi<0

= ˆwT x − ε

|αiyi|xT

i x + 2R2ε

|αiyi| ≤ ˆwT x + 3λCR2ε

(A ˆw)T (Ax) =

αiyi(Axi)T (Ax)

αiyi

(cid:0)(1 + ε)xT

i x − 2R2ε(cid:1) +

(cid:88)

αiyi

(cid:0)(1 − ε)xT

i x + 2R2ε(cid:1)

i:αiyi<0

= ˆwT x + ε

|αiyi|xT

i x − 2R2ε

|αiyi| ≥ ˆwT x − 3λCR2ε

m
(cid:88)

i=1

(cid:88)

≤

i:αiyi≥0

m
(cid:88)

i=1

(cid:88)

≥

i:αiyi≥0

m
(cid:88)

i=1

m
(cid:88)

i=1

m
(cid:88)

i=1

m
(cid:88)

i=1

17

Published as a conference paper at ICLR 2018

for any x ∈ RN . Since (cid:96) is λ-Lipschitz taking expectations over D implies

(cid:96)D(A ˆw) ≤ (cid:96)D( ˆw) + 3λ2CR2ε

(16)

Substituting Equation 14 applying Lemma C.2 also yields
m
(cid:88)

m
(cid:88)

αiαjyiyj(Axi)T (Axj)

(cid:107)A ˆw(cid:107)2

2 =

αiαjyiyj

(cid:0)(1 − ε)xT

i xj + 2R2ε(cid:1)

αiαjyiyj

(cid:0)(1 + ε)xT

i xj − 2R2ε(cid:1)

i xj +

−|αiαjyiyj|εxT

i xj + 2R2|αiαjyiyj|ε

i=1

j=1
(cid:88)

≤

i,j:αiαj yiyj ≥0

(cid:88)

+

i,j:αiαj yiyj <0
αiαjyiyjxT

(cid:88)

≤

i,j
≤ (cid:107) ˆw(cid:107)2

2 + 3λ2C 2R2ε

(cid:88)

i,j

1
2C

which implies

Together the inequalities bounding the loss term (16) and the regularization term (17) imply the result.

1
2C

(cid:107)A ˆw(cid:107)2

2 ≤

(cid:107) ˆw(cid:107)2

2 +

λ2CR2ε

3
2

(17)

Lemma C.4. Let ˆw be the linear classiﬁer minimizing LS and let w∗ be the linear classiﬁer
minimizing LD. Then with probability 1 − γ

LD( ˆw) ≤ LD(w∗) + O

(cid:18) λ2CR2
m

log

(cid:19)

1
γ

This result holds in the compressed domain as well.

Proof. By Corollary C.1 we have that ˆw is contained in a closed convex subset independent of S.
Therefore since (cid:96) is λ-Lipschitz, L is 1
C -strongly convex, and (cid:107)x(cid:107)2 ≤ O(R), we have by (Sridharan
et al., 2008, Theorem 1) that with probability 1 − γ

LD( ˆw) − LD(w∗) ≤ 2 [LS( ˆw) − LS(w∗)]+ + O

(cid:18) λ2CR2
m

log

(cid:19)

1
γ

Then since by deﬁnition ˆw minimizes LS(w) we have that LS( ˆw) ≤ LS(w∗), which substituted into
the previous equation completes the proof.

Proof of Theorem 4.2. Applying Lemma C.4 in the compressed domain yields

(cid:96)D( ˆwA) ≤ (cid:96)D( ˆwA) +

(cid:107) ˆwA(cid:107)2

2 = LD( ˆwA) ≤ LD(w∗

1
2C

(cid:18) λ2CR2
m
A) ≤ LD(A ˆw), so together with Lemma C.3

A) + O

log

1
γ

(cid:19)

where w∗
and the previous inequality we have

A minimizes LD. By deﬁnition of w∗

A, LD(w∗

(cid:96)D( ˆwA) ≤ LD(A ˆw) + O

log

≤ LD( ˆw) + O

λ2CR2

ε +

log

(cid:18) λ2CR2
m

(cid:19)

1
γ

(cid:18)

(cid:18)

1
m

(cid:19)(cid:19)

1
γ

We now apply Lemma C.4 in the sparse domain to get

(cid:96)D( ˆwA) ≤ LD(w∗) + O

λ2CR2

ε +

log

(cid:18)

(cid:18)

(cid:19)(cid:19)

1
γ

where w∗ minimizes LD. By deﬁnition of w∗, LD(w∗) ≤ LD(w0) = (cid:96)D(w0) + 1
the previous inequality we have

2C (cid:107)w0(cid:107)2

2, so by

(cid:96)D( ˆwA) ≤ (cid:96)D(w0) +

(cid:107)w0(cid:107)2

2 + O

λ2CR2

ε +

log

1
2C

(cid:18)

1
m

(cid:19)(cid:19)

1
γ

Substituting the C that minimizes the r.h.s. of this inequality completes the proof.

1
m

(cid:18)

18

Published as a conference paper at ICLR 2018

D PROOF OF LEMMA 4.1

We assume the setting described in Lemma 4.1, where we are concerned with the RIP condition of
the matrix A(n) when multiplying vectors x ∈ X (n)
T , the set of BonC vectors for documents of length
at most T . This matrix can be written as
A1





A(n) =






0d×V1
...
0d×V1

0d×V2
. . .
. . .
· · ·

· · ·
. . .
. . .
0d×Vn−1

0d×Vn
...
0d×Vn
An






where Ap is the d × Vp matrix whose columns are the DisC embeddings of all p-grams in the
vocabulary (and thus A(1) = A1 = A, the matrix of the original word embeddings). Note that from
(1) any x ∈ X (n)
can be written as x = [x1, . . . , xn], where xp is a T -sparse vector whose entries
correspond to p-grams. Thus we also have A(n)x = [A1x1, . . . , Anxn].
(cid:16)
Lemma D.1. If Ap is (2k, ε)-RIP w.p. 1 − γ ∀ p ∈ [n] then A(n) is
1 − nγ.

-RIP w.p. at least

∆X (n)
k

, ε

(cid:17)

T

Proof. By union bound we have that Ap is (2k, ε)-RIP ∀ p ∈ [n] with probability at least 1 − nγ.
Thus by Deﬁnition 4.1 we have w.p. 1 − nγ that ∀ x ∈ ∆X (n)
n
(cid:88)

n
(cid:88)

k

(cid:107)A(n)x(cid:107)2

2 =

(cid:107)Apxp(cid:107)2

2 ≤

(1 + ε)2(cid:107)xp(cid:107)2

2 = (1 + ε)2(cid:107)x(cid:107)2
2

p=1

p=1

2. From Deﬁnition 4.1, taking the square root of both sides of

Similarly, (cid:107)A(n)x(cid:107)2
2 ≥ (1 − ε)2(cid:107)x(cid:107)2
both inequalities completes the proof.
Deﬁnition D.1 (Foucart & Rauhut, 2013). Let D be a distribution over a subset S ⊂ Rn. Then
the set Φ = {φ1, . . . , φN } of functions φi : S (cid:55)→ R is a bounded orthonormal system (BOS) with
constant B if we have ED(φiφj) = 1i=j ∀ i, j and sups∈S |φi(s)| ≤ B ∀ i. Note that by deﬁnition
B ≥ 1.
Theorem D.1 (Foucart & Rauhut, 2013). If d = ˜Ω
d × N matrix associated with a BOS with constant B then A is (k, ε)-RIP w.p. 1 − γ.
Lemma D.2. If d = ˜Ω
then for any p ∈ [n] the matrix Ap ∈ Rd×Vp of DisC embeddings is (T, ε)-RIP w.p. 1 − γ.

and the word embeddings are drawn i.i.d. from U d{±1/

for (ε, γ) ∈ (0, 1) and

(cid:16) T
ε2 log Vp

(cid:16) B2k
ε2

log N
γ

dA is a

d}

√

√

(cid:17)

(cid:17)

γ

√

√

(cid:0)x(i)(cid:1) = (cid:81)p

Proof. Note that by Theorem D.1 it sufﬁces to show that
dAp is a random sampling matrix
associated with a BOS with constant B = 1. Let D = U V {±1} be the uniform distribution over
V i.i.d. Rademacher random variables indexed by words in the vocabulary. Then by deﬁnition the
matrix Ap ∈ Rd×Vp can be constructed by drawing random variables x(1), . . . , x(d) i.i.d. from D
and assigning to the ijth entry of
dAp corresponding to the p-gram g = {g1, . . . , gp} the value
t=1 x(i)
gt , where each function φj : {±1}V (cid:55)→ R is uniquely associated to its p-gram.
φj
It remains to be shown that this set of functions is a BOS with constant B = 1.
(cid:1),
For any two p-grams g, g(cid:48) and their functions φi, φj we have ED(φiφj) = Ex∼D
which will be 1 iff each word in g ∪ g(cid:48) occurs an even number of times in the product and 0 otherwise.
Because all p-grams are uniquely deﬁned under any permutation of its words (i.e. we are in fact using
p-cooccurrences) and we have assumed that no p-gram contains a word more than once, each word
occurs an even number of times in the product iff g = g(cid:48) ⇐⇒ i = j. Furthermore we have that
|φi(x)| ≤ 1 ∀ x ∈ {±1}V ∀ i by construction. Thus according to Deﬁnition D.1 the set of functions
{φ1, . . . , φVp } associated to the p-grams in the vocabulary is a BOS with constant B = 1.

t=1 xgtxg(cid:48)

(cid:0)(cid:81)p

t

Proof of Lemma 4.1. Since d = ˜Ω
1 − γ

n ∀ p ∈ [n]. Applying Lemma D.1 yields the result.

(cid:16) T
ε2 log nV max

n
γ

(cid:17)

, Lemma D.2 implies that Ap is (2T, ε)-RIP w.p.

19

Published as a conference paper at ICLR 2018

E DETAILS OF THE SUPPORTING HYPERPLANE PROPERTY

In Section 5.2, Deﬁnition 5.1 we introduced the Supporting Hyperplane Property (SHP), which
by Corollary 5.1 characterizes when BP+ perfectly recovers a nonnegative signal. Together with
Lemmas A.1 and A.2 this fact also shows that SHP is a weaker condition than the well-known
nullspace property (NSP):
Corollary E.1. If a matrix A ∈ Rd×N with columns in general position satisﬁes NSP for some
S ⊂ [N ] then it also satisﬁes S-SHP.

In this section we give the entire proof of Corollary 5.1 and describe how to verify SHP given a
design matrix and a set of support indices.

E.1 PROOF OF COROLLARY 5.1

Recall that it sufﬁces to show equivalence of A being S-SHP with the columns AS forming the
vertices of a k-dimensional face of conv(A), where we can abuse notation to set A ∈ Rd×(N +1),
with the extra column being the origin 0d, so long as we constrain N + 1 (cid:54)∈ S.

( =⇒ ): The proof of the forward direction appeared in full in the proof sketch (see Section 5.2).
( ⇐= ): A subset F ⊂ Rd is a face of conv(A) if for some hyperplane H = {v : aT v − b = 0} we
have F = conv(A) ∩ H and conv(A)\F ⊆ H− = {v : aT v − b < 0}, where H− is the negative
halfspace of H. Deﬁne the simplex ∆m = {λ ∈ [0, 1]m : (cid:80)m
Since A is S-SHP we have a hyperplane H = {v : aT v − b = 0} containing the columns AS such
that AS ⊂ H−. Thus aT Ai − b = 0 ∀ i ∈ S and aT Ai − b < 0 ∀ i /∈ S. We also know that
F = {(cid:80)
i∈S λiAi : λ ∈ ∆|S|} ⊆ H by convexity of H. Since any point y ∈ conv(A)\F can be
written as y = (cid:80)N +1

i=1 λiAi for some λ ∈ ∆N +1 such that ∃ j /∈ S such that λj (cid:54)= 0, we have that

i=1 λi = 1}.

aT y − b =

λi(aT Ai − b) +

λj(aT Aj − b) =

λj(aT Aj − b) < 0

(cid:88)

i∈S

(cid:88)

j /∈S

(cid:88)

j /∈S

This implies that conv(A)\F ⊆ H− and F = conv(A) ∩ H, so since the columns of A are in general
position F is a k-dimensional face of conv(A) whose vertices are the columns AS.

E.2 VERIFYING SHP

Recall that a matrix Rd×N satisﬁes S-SHP for S ⊂ [N ] if there is a hyperplane containing the set of
all columns of A indexed by S and the set of all other columns together with the origin are on one
side of it. Due to Corollary 5.1, checking S-SHP allows us to know whether all nonnegative signals
with index support S will be recovered by BP+ without actually running the optimization on any one
of them. The property can be checked by solving a convex problem of the form
(cid:110) ˜AT

subject to

max

˜AT

(cid:88)

(cid:111)p

S h = 0|S|

i h + ε, 0

min
h∈Rd+1

i(cid:54)∈S

where

˜A =

(cid:19)

(cid:18) A 0d
1T
1
N

and ε > 0, p ≥ 1

The constraint enforces the property that the hyperplane contains all support embeddings, while the
optimal objective value is zero iff SHP is satisﬁed (this follows from the fact that scaling h does
not affect the constraint so if the minimal objective is zero for any single ε > 0 it is zero for all
ε > 0). The problem can be solved via using standard ﬁrst or second-order equality-constrained
convex optimization algorithms. We set ε = 1 and p = 3 (to get a C2 objective) and adapt the
second-order method from Boyd & Vandenberghe (2004, Chapter 10). Our implementation can be
found at https://github.com/NLPrinceton/sparse_recovery.

20

Published as a conference paper at ICLR 2018

Figure 7: Efﬁciency of pretrained embeddings as sensing vectors at d = 300 dimensions, measured
via the F1-score of the original BoW. 200 documents from each dataset were compressed and
recovered in this experiment. For fairness, the number of words V is the same for all embeddings
so all documents are required to be subsets of the vocabulary of all corpora. word2vec embeddings
trained on Google News and GloVe vectors trained on Common Crawl were obtained from public
repositories (Mikolov et al., 2013; Pennington et al., 2014) while Amazon and Wikipedia embeddings
were trained for 100 iterations using a symmetric window of size 10, a min count of 100, for SN/GloVe
a cooccurrence cutoff of 1000, and for word2vec a down-sampling frequency cutoff of 10−5 and a
negative example setting of 3. 300-dimensional normalized random vectors are used as a baseline.

F SPARSE RECOVERY WITH PRETRAINED EMBEDDINGS

F.1 PERFORMANCE COMPARISON OF ALTERNATIVE EMBEDDINGS

We show in Figure 7 that the surprising effectiveness of word embeddings as linear measurement
vectors for BoW signals holds for other embedding objectives and corpora as well. Speciﬁcally,
we see that widely used embeddings, when normalized, match the efﬁciency of random vectors for
retrieving SST BoW and are more efﬁcient when retrieving IMDB BoW. Interestingly, SN vectors are
most efﬁcient and are also the only embeddings for normalizing is not needed for good performance.

F.2 A MODEL-BASED THEORETICAL APPROACH

In Section 5.2 we gave some intuition for why pretrained word embeddings are efﬁcient sensing
vectors for natural language BoW by examining a geometric characterization of local equivalence due
to Donoho & Tanner (2005) in light of the usual similarity properties of word embeddings. However,
this analysis does not provide a rigorous theory for our empirical results. In this section we brieﬂy
discuss a model-based justiﬁcation that may lead to a stronger understanding.

We need a model relating BoW generation to the word embeddings trained over words co-occurring
in the same BoW. As a starting point consider the model of Arora et al. (2016), in which a corpus is
generated by a random walk ct over the surface of a ball in Rd; at each t a word w is emitted w.p.
P(w|ct) ∝ exp(cid:104)ct, vw(cid:105)

(18)

Minimizing the SN objective approximately maximizes corpus likelihood under this model.

Thus in an approximate sense a document of length T is generated by setting a context vector c
and emitting T words via (18) with ct = c. This model is a convenient one for analysis due its
simplicity and invariance to word order as well as the fact that the approximate maximum likelihood
document vector is the sum of the embeddings of words in the document. Building upon the intuition
established following Corollary 5.1 one can argue that, if we have the true latent SN vectors, then
embeddings of words in the same document (i.e. emitted by the same context vector) will be close to
each other and thus easy to separate from the embeddings of other words in the vocabulary.

However, we ﬁnd empirically that not all of the T words closest to the sum of the word embeddings
(i.e. the context vector) are the ones emitted; indeed individual word vectors in a document may
have small, even negative inner product with the context vector and still be recovered via BP. Thus
any further theoretical argument must also be able to handle the recovery of lower probability words
whose vectors are further away from the context vector than those of words that do not appear in the
document. We thus leave to future work the challenge of explaining why embeddings resulting from
this (or another) model provide such efﬁcient sensing matrices for natural language BoW.

21

Published as a conference paper at ICLR 2018

A COMPRESSED SENSING VIEW OF UNSUPERVISED
TEXT EMBEDDINGS, BAG-OF-n-GRAMS, AND LSTMS

Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi
Princeton University
{arora,mkhodak,nsaunshi}@cs.princeton.edu

Kiran Vodrahalli
Columbia University
kiran.vodrahalli@columbia.edu

ABSTRACT

Low-dimensional vector embeddings, computed using LSTMs or simpler tech-
niques, are a popular approach for capturing the “meaning” of text and a form of
unsupervised learning useful for downstream tasks. However, their power is not
theoretically understood. The current paper derives formal understanding by look-
ing at the subcase of linear embedding schemes. Using the theory of compressed
sensing we show that representations combining the constituent word vectors
are essentially information-preserving linear measurements of Bag-of-n-Grams
(BonG) representations of text. This leads to a new theoretical result about LSTMs:
low-dimensional embeddings derived from a low-memory LSTM are provably at
least as powerful on classiﬁcation tasks, up to small error, as a linear classiﬁer
over BonG vectors, a result that extensive empirical work has thus far been unable
to show. Our experiments support these theoretical ﬁndings and establish strong,
simple, and unsupervised baselines on standard benchmarks that in some cases are
state of the art among word-level methods. We also show a surprising new property
of embeddings such as GloVe and word2vec: they form a good sensing matrix for
text that is more efﬁcient than random matrices, the standard sparse recovery tool,
which may explain why they lead to better representations in practice.

1

INTRODUCTION

Much attention has been paid to using LSTMs (Hochreiter & Schmidhuber, 1997) and similar models
to compute text embeddings (Bengio et al., 2003; Collobert & Weston, 2008). Once trained, the
LSTM can sweep once or twice through a given piece of text, process it using only limited memory,
and output a vector with moderate dimensionality (a few hundred to a few thousand), which can be
used to measure text similarity via cosine similarity or as a featurization for downstream tasks.

The powers and limitations of this method have not been formally established. For example, can
such neural embeddings compete with and replace traditional linear classiﬁers trained on trivial
Bag-of-n-Grams (BonG) representations? Tweaked versions of BonG classiﬁers are known to be a
surprisingly powerful baseline (Wang & Manning, 2012) and have fast implementations (Joulin et al.,
2017). They continue to give better performance on many downstream supervised tasks such as IMDB
sentiment classiﬁcation (Maas et al., 2011) than purely unsupervised LSTM representations (Kiros
et al., 2015; Hill et al., 2016; Pagliardini et al., 2017). Even a very successful character-level (and thus
computation-intensive, taking a month of training) approach does not reach BonG performance on
datasets larger than IMDB (Radford et al., 2017). Meanwhile there is evidence suggesting that simpler
linear schemes give compact representations that provide most of the beneﬁts of word-level LSTM
embeddings (Wieting et al., 2016; Arora et al., 2017). These linear schemes consist of simply adding
up, with a few modiﬁcations, standard pretrained word embeddings such as GloVe or word2vec
(Mikolov et al., 2013; Pennington et al., 2014).

1

Published as a conference paper at ICLR 2018

The current paper ties these disparate threads together by giving an information-theoretic account
of linear text embeddings. We describe linear schemes that preserve n-gram information as low-
dimensional embeddings with provable guarantees for any text classiﬁcation task. The previous linear
schemes, which used unigram information, are subcases of our approach, but our best schemes can
also capture n-gram information with low additional overhead. Furthermore, we show that the original
unigram information can be (approximately) extracted from the low-dimensional embedding using
sparse recovery/compressed sensing (Candès & Tao, 2005). Our approach also ﬁts in the tradition
of the older work on distributed representations of structured objects, especially the works of Plate
(1995) and Kanerva (2009). The following are the main results achieved by this new world-view:

1. Using random vectors as word embeddings in our linear scheme (instead of pretrained
vectors) already allows us to rigorously show that low-memory LSTMs are provably at
least as good as every linear classiﬁer operating on the full BonG vector. This is a novel
theoretical result in deep learning, obtained relatively easily. By contrast, extensive empirical
study of this issue has been inconclusive (apart from character-level models, and even then
only on smaller datasets (Radford et al., 2017)). Note also that empirical work by its nature
can only establish performance on some available datasets, not on all possible classiﬁcation
tasks. We prove this theorem in Section 4 by providing a nontrivial generalization of a result
combining compressed sensing and learning (Calderbank et al., 2009). In fact, before our
work we do not know of any provable quantiﬁcation of the power of any text embedding.

2. We study theoretically and experimentally how our linear embedding scheme improves
when it uses pretrained embeddings (GloVe etc.) instead of random vectors. Empirically we
ﬁnd that this improves the ability to preserve Bag-of-Words (BoW) information, which has
the following restatement in the language of sparse recovery: word embeddings are better
than random matrices for “sensing” BoW signals (see Section 5). We give some theoretical
justiﬁcation for this surprising ﬁnding using a new sparse recovery property characterizing
when nonnegative signals can be reconstructed by (cid:96)1-minimization.

3. Section 6 provides empirical results supporting the above theoretical work, reporting ac-
curacy of our linear schemes on multiple standard classiﬁcation tasks. Our embeddings
are consistently competitive with recent results and perform much better than all previous
linear methods. Among unsupervised word-level representations they achieve state of the art
performance on both the binary and ﬁne-grained SST sentiment classiﬁcation tasks (Socher
et al., 2013). Since our document representations are fast, compositional, and simple to
implement given standard word embeddings, they provide strong baselines for future work.

2 RELATED WORK

Neural text embeddings are instances of distributed representations, long studied in connectionist
approaches because they decay gracefully with noise and allow distributed processing. Hinton
(1990) provided an early problem formulation, and Plate (1995) provided an elementary solution,
the holographic distributed representation, which represents structured objects using circular vector
convolution and has an easy and more compact implementation using the fast Fourier transform
(FFT). Plate suggested applying such ideas to text, where “structure” can be quantiﬁed using parse
trees and other graph structures. Our method is also closely related in form and composition to the
sparse distributed memory system of Kanerva (2009). In the unigram case our embedding reduces to
the familiar sum of word embeddings, which is known to be surprisingly powerful (Wieting et al.,
2016), and with a few modiﬁcations even more so (Arora et al., 2017).

2

Published as a conference paper at ICLR 2018

Representations of BonG vectors have been studied through the lens of compression by Paskov et al.
(2013), who computed representations based on classical lossless compression algorithms using a
linear program (LP). Their embeddings are still high-dimensional (d > 100K) and quite complicated
to implement. In contrast, linear projection schemes are simpler, more compact, and can leverage
readily available word embeddings. Pagliardini et al. (2017) also used a linear scheme, representing
documents as an average of learned word and bigram embeddings. However, the motivation and
beneﬁts of encoding BonGs in low-dimensions are not made explicit. The novelty in the current paper
is the connection to compressed sensing, which is concerned with recovering high-dimensional sparse
signals x ∈ RN from low-dimensional linear measurements Ax, speciﬁcally by studying conditions
on matrix A ∈ Rd×N when this is possible (see Appendix A for some background on compressed
sensing and the previous work of Calderbank et al. (2009) that we build upon).

3 DOCUMENT EMBEDDINGS

In this section we deﬁne the two types of representations that our analysis will relate:

1. high-dimensional BonG vectors counting the occurrences of each k-gram for k ≤ n

2. low-dimensional embeddings, from simple vector sums to novel n-gram-based embeddings

Although some of these representations have been previously studied and used, we deﬁne them so as
to make clear their connection via compressed sensing, i.e. that representations of the second type
are simply linear measurements of the ﬁrst.

We now deﬁne some notation. Let V be the number of words in the vocabulary and Vn be the number
of n-grams (independent of word order), so that V = V1. Furthermore set V sum
k≤n Vk and
V max
n = maxk≤n Vk. We will use words/n-grams and indices interchangeably, e.g. if (a, b) is the
ith of V2 bigrams then the one-hot vector e(a,b) will be 1 at index i. Where necessary we will use
{, } to denote a multi-set and (, ) to denote a tuple. For any m vectors vi ∈ Rd for i = 1, . . . , m
we deﬁne [v1, . . . , vm] to be their concatenation, which is thus an element of Rmd. Finally, for any
subset X ⊂ RN we denote by ∆X the set {x − x(cid:48) : x, x(cid:48) ∈ X }.

n = (cid:80)

3.1 THE BAG-OF-n-GRAMS VECTOR

Assigning to each word a unique index i ∈ [V ] we deﬁne the Bag-of-Words (BoW) representation
xBoW of a document to be the V -dimensional vector whose ith entry is the number of times word i
occurs in the document. The n-gram extension of BoW is the Bag-of-n-Grams (BonG) representation,
which counts the number of times any k-gram for k ≤ n appears in a document. Linear classiﬁcation
over such vectors has been found to be a strong baseline (Wang & Manning, 2012).

For ease of analysis we simplify the BonG approach by merging all n-grams in the vocabulary that
contain the same words but in a different order. We call these features n-cooccurrences and ﬁnd that
the modiﬁcation does not affect performance signiﬁcantly (see Table 3 in Appendix F.1). Formally for
a document w1, . . . , wT we deﬁne the Bag-of-n-Cooccurrences (BonC) vector as the concatenation

xBonC =

ewt

,

. . .

,

e{wt,...,wt+n−1}

(1)

(cid:34) T

(cid:88)

t=1

T −n+1
(cid:88)

t=1

(cid:35)

which is thus a V sum

n -dimensional vector. Note that for unigrams this is equivalent to the BoW vector.

3.2 LOW-DIMENSIONAL n-GRAM EMBEDDINGS

Now suppose each word w has a vector vw ∈ Rd for some d (cid:28) V . Then given a document
w1, . . . , wT we deﬁne its unigram embedding as zu = (cid:80)T
t=1 vwt. While this is a simple and widely
used featurization, we focus on the following straightforward relation with BoW: if A ∈ Rd×V is
a matrix whose columns are word vectors vw then AxBoW = (cid:80)T
t=1 vwt = zu. Thus
in terms of compressed sensing the unigram embedding of a document is a d-dimensional linear
measurement of its Bag-of-Words vector.

t=1 Aewt = (cid:80)T

3

Published as a conference paper at ICLR 2018

We could extend this unigram embedding to n-grams by ﬁrst deﬁning a representation for each n-
gram as the tensor product of the vectors of its constituent words. Thus for each bigram b = (w1, w2)
we would have vb = vw1 vT
t=1 vwt for each n-gram g = (w1, . . . , wn).
The document embedding would then be the sum of the tensor representations of all n-grams.

w2 and more generally vg = (cid:78)n

The major drawback of this approach is of course the blowup in dimension, which in practice
prevents its use beyond n = 2. To combat this a low-dimensional sketch or projection of the
tensor product can be used, such as the circular convolution operator of Plate (1995). Since we are
interested in representations that can also be constructed by an LSTM, we instead sketch this tensor
product using the element-wise multiplication operation, which we ﬁnd also usually works better
than circular convolution in practice (see Table 4 in Appendix F.1). Thus for the n-cooccurrence
g = {w1, . . . , wn}, we deﬁne the distributed cooccurrence (DisC) embedding ˜vg = d
t=1 vwt.
The coefﬁcient is required when the vectors vw are random and unit norm to ensure that the product
also has close to unit norm (see Lemma B.1). In addition to their convenient form, DisC embeddings
have nice theoretical and practical properties: they preserve the original embedding dimension, they
reduce to unigram (word) embeddings for n = 1, and under mild assumptions they satisfy useful
compressed sensing properties with overwhelming probability (Lemma 4.1).

2 (cid:74)n

n−1

We then deﬁne the DisC document embedding to be the nd-dimensional weighted concatenation,
over k ≤ n, of the sum of the DisC vectors of all k-grams in a document:

(cid:34)

z(n) =

C1

T
(cid:88)

t=1

˜vwt

,

. . .

, Cn

˜v{wt,...,wt+n−1}

(2)

T −n+1
(cid:88)

t=1

(cid:35)

Here scaling factors Ck are set so that all spans of d coordinates have roughly equal norm (for random
embeddings Ck = 1; for word embeddings Ck = 1/k works well). Note that since ˜vwt = vwt we
have z(1) = zu in the unigram case. Furthermore, as with unigram embeddings by comparing (1) and
(2) one can easily construct a (cid:80)n

n matrix A(n) such that z(n) = A(n)xBonC.

k=1 dn × V sum

3.3 LSTM REPRESENTATIONS

As discussed previously, LSTMs have become a common way to apply the expressive power of
RNNs, with success on a variety of classiﬁcation, representation, and sequence-to-sequence tasks.
For document representation, starting with h0 = 0m an m-memory LSTM initialized with word
vectors vw ∈ Rd takes in words w1, . . . , wT one-by-one and computes the document representation
ht = f (Tf (vwt, ht−1)) ◦ ht−1 + i(Ti(vwt, ht−1)) ◦ g(Tg(vwt, ht−1))
(3)
where ht ∈ Rm is the hidden representation at time t, the forget gate f , input gate i, and input
function g are a.e. differentiable nondecreasing elementwise “activation” functions Rm (cid:55)→ Rm, and
afﬁne transformations T∗(x, y) = W∗x + U∗y + b∗ have weight matrices W∗ ∈ Rm×d, U∗ ∈ Rm×m
and bias vectors b∗ ∈ Rm. The LSTM representation of a document is then the state at the last time
step, i.e. zLSTM = hT . Note that we will follow the convention of using LSTM memory to refer to
the dimensionality of the hidden states. Since the LSTM is initialized with an embedding for each
word it requires O(m2 + md + V d) computer memory, but the last term is just a lookup table so the
vocabulary size does not factor into iteration or representation complexity.

From our description of LSTMs it is intuitive to see that one can initialize the gates and input functions
so as to construct the DisC embeddings deﬁned in the previous section. We state this formally and
give the proof in the unigram case (the full proof appears in Appendix B.3):
Proposition 3.1. Given word vectors vw ∈ Rd, one can initialize an O(nd)-memory LSTM (3) that
takes in words w1, . . . , wT (padded by an end-of-document token assigned vector 0d) and constructs
the DisC embedding (2) (up to zero padding), i.e. such that for all documents zLSTM = z(n).

Proof (Unigram Case). Set f (x) = i(x) = g(x) = x, Tf (vwt, ht−1) = Ti(vwt, ht−1) = 1d, and
Tg(vwt, ht−1) = C1vwt. Then ht = ht−1 + C1vwt, so since h0 = 0d we have the ﬁnal LSTM
representation zLSTM = hT = C1

t=1 vwt = z(1).

(cid:80)t

By Proposition 3.1 we can construct a ﬁxed LSTM that can compute compressed BonC representations
on the ﬂy and be further trained by stochastic gradient descent using the same memory.

4

Published as a conference paper at ICLR 2018

4 LSTMS AS COMPRESSED LEARNERS

Our main contribution is to provide the ﬁrst rigorous analysis of the performance of the text embed-
dings that we are aware of, showing that the embeddings of Section 3.2 can provide performance on
downstream classiﬁcation tasks at least as well any linear classiﬁer over BonCs. Before stating the
theorem we make two mild simplifying assumptions on the BonC vectors:

1. The vectors are scaled by

1
√

T

n , where T is the maximum document length. This assumption

is made without loss of generality.

2. No n-cooccurrence contains a word more than once. While this is (infrequently) violated in
practice, the problem can be circumvented by merging words as a preprocessing step.

Theorem 4.1. Let S = {(xi, yi)}m
i=1 be drawn i.i.d. from a distribution D over BonC vectors of
documents of length at most T satisfying assumptions 1 and 2 above and let w0 be the linear classiﬁer
minimizing the logistic loss (cid:96)D. Then for dimension d = ˜Ω
and appropriate choice of
regularization coefﬁcient one can initialize an O(nd)-memory LSTM over i.i.d. word embeddings
vw ∼ U d{±1/
d} such that w.p. (1 − γ)(1 − 2δ) the classiﬁer ˆw minimizing the (cid:96)2-regularized
logistic loss over its representations satisﬁes

(cid:16) T
ε2 log nV max

√

n
γ

(cid:17)

(cid:96)D ( ˆw) ≤ (cid:96)D (w0) + O

(cid:107)w0(cid:107)2

ε +

log

(cid:32)

(cid:114)

1
m

(cid:33)

1
δ

The above theoretical bound shows that LSTMs match BonC performance as ε → 0, which can be
realized by increasing the embedding dimension d (c.f. Figure 5).

4.1 COMPRESSED SENSING AND LEARNING

Compressed sensing is concerned with recovering a high-dimensional k-sparse signal x ∈ RN from
a few linear measurements; given a design matrix A ∈ Rd×N this is formulated as

minimize (cid:107)w(cid:107)0

subject to Aw = z

where z = Ax is the measurement vector. As l0-minimization is NP-hard, research has focused on
sufﬁcient conditions for tractable recovery. One such condition is the Restricted Isometry Property
(RIP), for which Candès & Tao (2005) proved that (5) can be solved by convex relaxation:
Deﬁnition 4.1. A ∈ Rd×N is (X , ε)-RIP for some subset X ⊂ RN if ∀ x ∈ X

(1 − ε)(cid:107)x(cid:107)2 ≤ (cid:107)Ax(cid:107)2 ≤ (1 + ε)(cid:107)x(cid:107)2

We will abuse notation and say (k, ε)-RIP when X is the set of k-sparse vectors. This is the more
common deﬁnition, but ours allows a more general Theorem 4.2 and a tighter bound in Theorem 4.1.

Following these breakthroughs, Calderbank et al. (2009) studied whether it is possible to use the
low-dimensional output of compressed sensing as a surrogate representation for classiﬁcation. They
proved a learning-theoretic bound on the loss of an SVM classiﬁer in the compressed domain
compared to the best classiﬁer in the original domain. In this work we are interested in comparing
the performance of LSTMs with BonC representations, so we need to generalize the Calderbank et al.
(2009) result to handle Lipschitz losses and an arbitrary set X ⊂ RN of high-dimensional signals:
Theorem 4.2. For any subset X ⊂ RN containing the origin let A ∈ Rd×N be (∆X , ε)-RIP and
let m samples S = {(xi, yi)}m
i=1 ⊂ X × {−1, 1} be drawn i.i.d. from some distribution D over X
with (cid:107)x(cid:107)2 ≤ R. If (cid:96) is a λ-Lipschitz convex loss function and w0 ∈ RN is its minimizer over D
then w.p. 1 − 2δ the linear classiﬁer ˆwA ∈ Rd minimizing the (cid:96)2-regularized empirical loss function
(cid:96)SA (w) + 1

2 over the compressed sample SA = {(Axi, yi)}m

i=1 ⊂ Rd × {−1, 1} satisﬁes

2C (cid:107)w(cid:107)2

(cid:96)D( ˆwA) ≤ (cid:96)D(w0) + O

λR(cid:107)w0(cid:107)2

ε +

log

(7)

(cid:32)

(cid:114)

1
m

(cid:33)

1
δ

for appropriate choice of C. Recall that ∆X = {x − x(cid:48) : x, x(cid:48) ∈ X } for any X ⊂ RN .

(4)

(5)

(6)

5

Published as a conference paper at ICLR 2018

While a detailed proof of this theorem is spelled out in Appendix C, the main idea is to compare the
distributional loss incurred by a classiﬁer ˆw in the original space to the loss incurred by A ˆw in the
compressed space. We show that the minimizer of the regularized empirical loss in the original space
( ˆw) is a bounded-coefﬁcient linear combination of samples in S, so its loss depends only on inner
products between points in X . Thus using RIP and a generalization error result by Sridharan et al.
(2008) we can bound the loss of ˆwA, the regularized classiﬁer in the compressed domain. Note that
to get back from Theorem 4.2 the O(
ε) bound for k-sparse inputs of Calderbank et al. (2009) we
can set X to the be the set of k-sparse vectors and assume A is (2k, ε)-RIP.

√

4.2 PROOF OF MAIN RESULT

To apply Theorem 4.2 we need the design matrix A(n) transforming BonCs into the DisC embeddings
of Section 3.2 to satisfy the following RIP condition (Lemma 4.1), which we prove using a restricted
isometry result for structured random sampling matrices in Appendix D:
Lemma 4.1. Assume the setting of Theorem 4.1 and let A(n) be the nd × V sum
n matrix relating DisC
(cid:17)
(cid:16) T
ε2 log nV max
and BonC representations of any document by z(n) = A(n)xBonC. If d = ˜Ω
then A(n)

n
γ

(cid:16)
∆X (n)

(cid:17)
T , ε

is

-RIP w.p. 1 − γ, where X (n)

T

is the set of BonCs of documents of length at most T .

(cid:16)

∆X (n)

(cid:17)
T , ε

Proof of Theorem 4.1. Let ˆS = {(A(n)xi, yi) : (xi, yi) ∈ S}, where A(n) is as in Lemma 4.1. Then
by the same lemma A(n) is
is the set of BonC vectors of
documents of length at most T . By BonC assumption (1) all BonCs lie within the unit ball, so we can
apply Theorem 4.2 with (cid:96) the logistic loss, λ = 1, and R = 1 to get that a classiﬁer ˆw trained using (cid:96)2-
regularized logistic loss over ˆS will satisfy the required bound (4). Since by Proposition 3.1 one can
initialize an O(nd)-memory LSTM that takes in i.i.d. Rademacher word vectors vw ∼ U d{±1/
d}
such that zLSTM = z(n) = A(n)x ∀ x ∈ X (n)

-RIP w.p. 1 − γ, where X (n)

T , this completes the proof.

√

T

5 SPARSE RECOVERY WITH PRETRAINED EMBEDDINGS

Theorem 4.1 is proved using random vectors as the word embeddings in the scheme of Section 3.
However, in practice LSTMs are often initialized with standard word vectors such as GloVe. Such
embeddings cannot satisfy traditional compressed sensing properties such as RIP or incoherence. This
follows essentially from the deﬁnition: word embeddings seek to capture word similarity, so similar
words (e.g. synonyms) have embeddings with high inner product, which violates both properties.
Thus the efﬁcacy of real-life LSTMs must have some other explanation. But in this section we present
the surprising empirical ﬁnding that pretrained word embeddings are more efﬁcient than random
vectors at encoding and recovering BoW information via compressed sensing. We further sketch a
potential explanation for this result, though a rigorous explanation is left for subsequent work.

5.1 PRETRAINED EMBEDDINGS PRESERVE SPARSE INFORMATION

In recent years word embeddings have been discovered to have many remarkable properties, most
famously the ability to solve analogies (Mikolov et al., 2013). Our connection to compressed sensing
indicates that they should have another: preservation of sparse signals as low-dimensional linear
measurements. To examine this we subsample documents from the SST (Socher et al., 2013) and
IMDB (Maas et al., 2011) classiﬁcation datasets, embed them as d-dimensional unigram embeddings
z = Ax for d = 50, 100, 200, . . . , 1600 (where A ∈ Rd×V is the matrix of word embeddings and
x is a document’s BoW vector), solve the following LP, known as Basis Pursuit (BP), which is the
standard (cid:96)1-minimization problem for sparse recovery in the noiseless case (see Appendix A):

minimize (cid:107)w(cid:107)1

subject to Aw = z

(8)

Success is measured as the F1 score of retrieved words. We use Squared Norm (SN) vectors (Arora
et al., 2016) trained on a corpus of Amazon reviews (McAuley et al., 2015) and normalized i.i.d.
Rademacher vectors as a baseline. SN is used due to similarity to GloVe and its formulation via
an easy-to-analyze generative model that may provide a framework to understand the results (see
Appendix F.2), while the Amazon corpus is used for its semantic closeness to the sentiment datasets.

6

Published as a conference paper at ICLR 2018

Figure 1: Average F1-score of 200 recovered
BoW vectors from SST (left) and IMDB (right)
compared to dimension. Pretrained word embed-
dings (SN trained on Amazon reviews) need half
the dimensionality of normalized Rademacher
vectors to achieve near-perfect recovery. Note
that IMDB documents are on average more than
ten times longer than SST documents.

Figure 2: F1-score of 1000 recovered BoWs
compared to number of unique words. Real doc-
uments (left) are drawn from the SST and IMDB
corpora; random signals (right) are created by
picking words at random. For d = 200, pre-
trained embeddings are better than Rademacher
vectors as sensing vectors for natural language
BoW but are worse for random sparse signals.

Figures 1 and 2 show that pretrained embeddings require a lower dimension d than random vectors
to recover natural language BoW. This is surprising as the training objective goes against standard
conditions such as approximate isometry and incoherence; indeed as shown in Figure 2 recovery
is poor for randomly generated word collections. The latter outcome indicates that the fact that a
document is a set of mutually meaningful words is important for sparse recovery using embeddings
trained on co-occurrences. We achieve similar results with other objectives (e.g. GloVe/word2vec)
and other corpora (see Appendix F.1), although there is some sensitivity to the sparse recovery method,
as other (cid:96)1-minimization methods work well but greedy methods, such as Orthogonal Matching
Pursuit (OMP), work poorly, likely due to their dependence on incoherence (Tropp, 2004).

For the n-gram case (i.e. BonC recovery for n > 1), although we know by Lemma 4.1 that DisC
embeddings composed from random vectors satisfy RIP, for pretrained vectors it is unclear how to
reason about suitable n-gram embeddings without a rigorous understanding of the unigram case,
and experiments do not show the same recovery beneﬁts. One could perhaps do well by training on
cooccurrences of word tuples, but such embeddings could not be used by a word-level LSTM.

5.2 UNDERSTANDING SPARSE RECOVERY OF NATURAL LANGUAGE DOCUMENTS

As shown in Figure 2, the success of pretrained embeddings for linear sensing is a local phenomenon;
recovery is only efﬁcient for naturally occurring collections of words. However, applying statistical
RIP/incoherence ideas (Barg et al., 2015) to explain this is ruled out since they require collections to
be incoherent with high probability, whereas word embeddings are trained to give high inner product
to words appearing together. Thus an explanation must come from some other, weaker condition.
The usual necessary and sufﬁcient requirement for recovering all signals with support S ⊂ [N ] is the
local nullspace property (NSP), which stipulates that vectors in the kernel of A not have too much
mass on S (see Deﬁnition A.2). While NSP and related properties such as restricted eigenvalue (see
Deﬁnition A.3) are hard to check, we can impose some additional structure to formulate an intuitive,
veriﬁable perfect recovery condition for our setting. Speciﬁcally, since our signals (BoW vectors) are
nonnegative, we can improve upon solving BP (8) by instead solving nonnegative BP (BP+):

minimize (cid:107)w(cid:107)1

subject to Aw = z, w ≥ 0d

(9)
The following geometric result then characterizes when solutions of BP+ recover the correct signal:
Theorem 5.1 (Donoho & Tanner, 2005). Consider a matrix A ∈ Rd×N and an index subset S ⊂ [N ]
of size k. Then any nonnegative vector x ∈ RN
+ with support supp(x) = S is recovered from Ax by
BP+ iff the set AS of columns of A indexed by S comprise the vertices of a k-dimensional face of the
convex hull conv(A) of the columns of A together with the origin.

This theorem equates perfect recovery of a BoW vector via BP+ with the vectors of its words being
the vertices of some face of the polytope conv(A). The property holds for incoherent columns since
the vectors are far enough that no one vector is inside the simplex formed by any k others. On the
other hand, pretrained embeddings satisfy it by having commonly co-occurring words close together
and other words far away, making it easier to form a face from columns indexed by the support of a
BoW. We formalize this intuition as the Supporting Hyperplane Property (SHP):

7

Published as a conference paper at ICLR 2018

Figure 3: Proportion of 500 randomly sampled documents from SST (top) and IMDB (bottom) that
are perfectly recovered from linear measurements.

Deﬁnition 5.1. A matrix A ∈ Rd×N satisﬁes S-SHP for subset S ⊂ [N ] if its columns are in general
position and there is a hyperplane containing the set AS of columns of A indexed by S such that the
set of all other columns of A together with the origin are on one side of the hyperplane.

SHP is a very weak property implied by NSP (Corollary E.1). However, it can be checked by using
convex optimization to see if the hyperplane exists (Appendix E.2). Furthermore, we show (full
proof in Appendix E.1) that this hyperplane is the supporting hyperplane of the face of conv(A) with
vertices AS, from which it follows by Theorem 5.1 that SHP characterizes recovery using BP+:
Corollary 5.1. BP+ recovers any x ∈ RN

+ with supp(x) = S from Ax iff A satisﬁes S-SHP.

Proof Sketch. By Theorem 5.1 it sufﬁces to show equivalence of S-SHP with the column set AS
comprising the vertices of a k-dimensional face of conv(A). A face F of polytope P is deﬁned as its
intersection with some hyperplane such that all points in P \F lie on one side of the hyperplane.
( =⇒ ) Let F be the face of conv(A) formed by the columns AS. Then there must be a supporting
hyperplane H containing F . Since the columns of A are in general position, all columns AS = A\AS
lie in conv(A)\F and hence must all be on one side of H, so H is the desired hyperplane.
( ⇐= ) Let H be the hyperplane supporting AS, with all other columns on one side of H. By
convexity, H contains the simplex F of AS. Any point in conv(A)\F can be written as a convex
combination of points in F and columns AS, with a positive coefﬁcient on at least one of the columns,
and so must lie on the same side of H as AS. Thus AS comprises the vertices of a face F .

Thus perfect recovery of a BoW via BP+ is equivalent to the existence of a hyperplane separating
embeddings of words in the document from those of the rest of the vocabulary. Intuitively, words
in the same document are trained to have similar embeddings and so will be easier to separate out,
providing some justiﬁcation for why pretrained vectors are better for sensing. We verify that SHP
is indeed more likely to be satisﬁed by such designs in Figure 3, which also serves as an empirical
check of Corollary 5.1 since SHP satisfaction implies BP recovery as the latter can do no better
than BP+. We further compare to recovery using OMP/OMP+ (the latter removes negative values
and recomputes the set of atoms at each iteration); interestingly, while OMP+ recovers the correct
signal from SN almost as often as BP/BP+, it performs quite poorly for GloVe, indicating that these
embeddings may have quite different sensing properties despite similar training objectives.

As similarity properties that may explain these results also relate to downstream task performance, we
conjecture a relationship between embeddings, recovery, and classiﬁcation that may be understood
under a generative model (see Appendix F.2). However, the Section 4 bounds depend on RIP, not
recovery, so these experiments by themselves do not apply. They do show that the compressed sensing
framework remains relevant even in the case of non-random, pretrained word embeddings.

8

Published as a conference paper at ICLR 2018

Representation

BonC (1)

DisC (2)

n

1
2
3

1
2
3

d∗
V1
V sum
2
V sum
3

1600
3200
4800

MR

77.1
77.8
77.8

79.6
80.1
80.0

CR

77.0
78.1
78.3

81.0
81.5
81.3

SUBJ MPQA TREC SST (±1)
91.0
91.8
91.4

86.8
90.0
89.8

80.7
80.9
80.1

85.1
85.8
85.6

92.4
92.6
92.6

87.8
87.9
87.9

85.2
89.6
90.0

84.6
85.5
85.2

SST

36.8
39.0
42.3

45.7
46.4
46.7

IMDB

88.3
90.0
89.8

89.2
89.4
89.6

45.8
31.0
30.7

87.7
87.2
86.6

84.4
80.2
80.0

81.1
78.7
79.1

92.5
91.2
91.1

79.6
76.2
76.3

85.6
85.8
84.2

SIF1
Sent2Vec2
Sent2Vec2
CFL3

1600
700
700
100K+

1
1
2
5
Paragraph Vec.4
skip-thoughts4
SDAE5
CNN-LSTM6
byte mLSTM7
∗ Vocabulary sizes (i.e. BonC dimensions) vary by task; usually 10K-100K.
1 Arora et al. (2017) Reported performance of best hyperparameter using Amazon GloVe embeddings.
2,4,7 Pagliardini et al. (2017); Kiros et al. (2015); Radford et al. (2017) Evaluated latest pretrained models.

91.8
93.0
78.4
92.6

74.8
80.3
74.6
77.8

78.1
83.8
78.0
82.0

74.2
88.9
86.9
89.4

90.5
94.2
90.8
93.6

89.2
85.5
85.3
90.4

4800
2400
4800

4096

88.8

91.7

54.6

86.8

92.2

90.6

94.7

45.8

90.4

85.1

Note that the available skip-thoughts implementation fails on the IMDB and MRPC tasks

3,5,6 Paskov et al. (2013); Hill et al. (2016); Gan et al. (2017) From publication (+emb version of last two).

Table 1: Evaluation of DisC and recent unsupervised word-level approaches on standard classiﬁcation
tasks, with the character LSTM of Radford et al. (2017) shown for comparison. The top three results
for each dataset are bolded, the best is italicized, and the best word-level performance is underlined.

6 EMPIRICAL FINDINGS

Our theoretical results show that simple tensor product sketch-based n-gram embeddings can approach
BonG performance and be computed by a low-memory LSTM. In this section we compare these text
representations and others on several standard tasks, verifying that DisC performance approaches that
of BonCs as dimensionality increases and establishing several baselines for text classiﬁcation. Code to
reproduce results is provided at https://github.com/NLPrinceton/text_embedding.

Tasks: We test classiﬁcation on MR movie reviews (Pang & Lee, 2005), CR customer reviews (Hu &
Liu, 2004), SUBJ subjectivity dataset (Pang & Lee, 2004), MPQA opinion polarity subtask (Wiebe
et al., 2005), TREC question classiﬁcation (Li & Roth, 2002), SST sentiment classiﬁcation (binary
and ﬁne-grained) (Socher et al., 2013), and IMDB movie reviews (Maas et al., 2011). The ﬁrst four
are evaluated using 10-fold cross-validation, while the others have train-test splits. In all cases we use
logistic regression with (cid:96)2-regularization determined by cross-validation. We further test DisC on the
SICK relatedness and entailment tasks (Marelli et al., 2014) and the MRPC paraphrase detection task
(Dolan & Brockett, 2005). The inputs here are sentences pairs (a, b) and the standard featurization
for document embeddings xa and xb of a and b is [|xa − xb|, xa (cid:12) xb] (Tai et al., 2015). We use
logistic regression for SICK entailment and MRPC and use ridge regression to predict similarity
scores for SICK relatedness, with (cid:96)2-regularization determined by cross-validation. Since BonGs
are not used for pairwise tasks our theory says nothing about performance here; we include these
evaluations to show that our representations are also useful for other tasks.

Embeddings: In the main evaluation (Table 1) we use normalized 1600-dimensional GloVe embed-
dings (Pennington et al., 2014) trained on the Amazon Product Corpus (McAuley et al., 2015), which
are released at http://nlp.cs.princeton.edu/DisC. We also compare the SN vectors of
Section 5 trained on the same corpus with random vectors when varying the dimension (Figure 5).

9

Published as a conference paper at ICLR 2018

SICK-E MRPC (Acc./F1)

Rep.

DisC (2)

SIF
Sent2Vec
Sent2Vec

n

1
2
3

1
1
2

skip-thoughts
SDAE
CNN-LSTM

SICK-R (r/ρ)
73.6 / 71.0
75.6 / 72.1
76.2 / 72.2

73.7 / 68.5
69.3 / 64.6
70.1 / 65.3

82.4 / 76.0

81.9
83.2
82.5

82.4
78.6
78.7

83.2

73.1 / 81.8
70.8 / 79.0
73.1 / 81.6

73.5 / 82.1
71.3 / 80.7
70.0 / 79.3

73.7 / 80.7
76.4 / 83.8

73.8 / 81.4

byte mLSTM

78.5 / 72.1

80.0

Model information is the same as that in Table 1.

Table 2: Performance of DisC and other recent approaches
on pairwise similarity and classiﬁcation tasks. The top three
results for each task are bolded and the best is underlined.

Figure 4: Time needed to initialize
model, construct document representa-
tions, and train a linear classiﬁer on a
16-core compute node.

Figure 5: IMDB performance of unigram (left) and bigram
(right) DisC embeddings compared to the original dimension.

Figure 6: IMDB performance com-
pared to training sample size.

Results: We ﬁnd that DisC representation performs consistently well relative to recent unsupervised
methods; among word-level approaches it is the top performer on the SST tasks and competes on many
others with skip-thoughts and CNN-LSTM, both concatenations of two LSTM representations. While
success may be explained by training on a large and in-domain corpus, being able to use so much text
without extravagant computing resources is one of the advantages of a simple approach. Overall our
method is useful as a strong baseline, often beating BonCs and many more complicated approaches
while taking much less time to represent and train on documents than neural representations (Figure 4).

Finally, we analyze empirically how well our model approximates BonC performance. As predicted
by Theorem 4.1, the performance of random embeddings on IMDB approaches that of BonC as
dimension increases and the isometry distortion ε decreases (Figure 5). Using pretrained (SN) vectors,
DisC embeddings approach BonC performance much earlier, surpassing it in the unigram case.

7 CONCLUSION

In this paper we explored the connection between compressed sensing, learning, and natural language
representation. We ﬁrst related LSTM and BonG methods via word embeddings, coming up with sim-
ple new document embeddings based on tensor product sketches. Then we studied their classiﬁcation
performance, proving a generalization of the compressed learning result of Calderbank et al. (2009)
to convex Lipschitz losses and a bound on the loss of a low-dimensional LSTM classiﬁer in terms
of its (modiﬁed) BonG counterpart, an issue which neither experiments nor theory have been able
to resolve. Finally, we showed how pretrained embeddings ﬁt into this sparse recovery framework,
demonstrating and explaining their ability to efﬁciently preserve natural language information.

10

Published as a conference paper at ICLR 2018

ACKNOWLEDGMENTS

We thank Rong Ge, Holden Lee, and Divyarthi Mohan for helpful discussions at various stages of this
effort. The work in this paper was in part supported by NSF grants CCF-1302518 and CCF-1527371,
Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329

REFERENCES

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model

approach to pmi-based word embeddings. Transactions of the ACL, 4:385–399, 2016.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In Proceedings of the International Conference on Learning Representations, 2017.

Alexander Barg, Arya Mazumdar, and Rongrong Wang. Restricted isometry property of random

subdictionaries. IEEE Transactions on Information Theory, 61, 2015.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic

language model. Journal of Machine Learning Research, 3:1137–1155, 2003.

Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.

Robert Calderbank, Sina Jafarpour, and Robert Schapire. Compressed learning: Universal sparse

dimensionality reduction and learning in the measurement domain. Technical report, 2009.

Emmanuel Candès and Terence Tao. Decoding by linear programming. IEEE Transactions on

Information Theory, 51:4203–4215, 2005.

Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th International Conference on
Machine Learning, 2008.

Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In

Third International Workshop on Paraphrasing, 2005.

David L. Donoho and Jared Tanner. Sparse nonnegative solution of underdetermined linear equations
by linear programming. Proceedings of the National Academy of Sciences of the United States of
America., 102:9446–9451, 2005.

Simon Foucart and David Koslicki. Sparse recovery by means of nonnegative least squares. IEEE

Signal Processing Letters, 21:498–502, 2014.

Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing. 2013.

Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin Carin.
Learning generic sentence representations using convolutional neural networks. In Proceedings of
Empirical Methods in Natural Language Processing, 2017.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences

from unlabelled data. In Proceedings of the North American Chapter of the ACL, 2016.

Geoffrey Hinton. Mapping part-whole hierarchies into connectionist networks. Artiﬁcial Intelligence,

46:47–75, 1990.

1735–1780, 1997.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:

Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the 10th

ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efﬁcient
text classiﬁcation. In Proceedings of the 15th Conference of the European Chapter of the ACL,
2017.

11

Published as a conference paper at ICLR 2018

Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed represen-

tation with high-dimensional random vectors. Cognitive Computation, 1:139–159, 2009.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun,

and Sanja Fidler. Skip-thought vectors. In Neural Information Processing Systems, 2015.

Xin Li and Dan Roth. Learning question classiﬁers.
Conference on Computational Linguistics, 2002.

In Proceedings of the 19th International

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of
the ACL: Human Language Technologies, 2011.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto
Zamparelli. Semeval-2014 task 1: Evaluation of compositional distributional semantic model
on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th
International Workshop on Semantic Evaluation, 2014.

Julian McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and com-
plementary products. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2015.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeffrey Dean. Distributed representa-
tions of words and phrases and their compositionality. In Neural Information Processing Systems,
2013.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embeddings

using compositional n-gram features. arXiv, 2017.

Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the ACL,
2004.

Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization

with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the ACL, 2005.

Hristo S. Paskov, Robert West, John C. Mitchell, and Trevor J. Hastie. Compressive feature learning.

In Neural Information Processing Systems, 2013.

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Proceedings of Empirical Methods in Natural Language Processing, 2014.

Tony Plate. Holographic reduced representations. IEEE Transactions on Neural Networks, 1995.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering

sentiment, 2017. arXiv.

Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated

gaussian designs. Journal of Machine Learning Research, 11(Aug):2241–2259, 2010.

Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y.
Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of Empirical Methods in Natural Language Processing, 2013.

Karthik Sridharan, Nathan Srebro, and Shai Shalev-Schwartz. Fast rates for regularized objectives.

In Neural Information Processing Systems. 2008.

Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations
from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting
of the ACL, 2015.

Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on

Information Theory, 50, 2004.

12

Published as a conference paper at ICLR 2018

Sida Wang and Christopher D. Manning. Baselines and bigrams: Simple, good sentiment and topic

classiﬁcation. In Proceedings of the 50th Annual Meeting of the ACL, 2012.

Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions

in language. Language Resources and Evaluation, 39:165–210, 2005.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards universal paraphrastic
sentence embeddings. In Proceedings of the International Conference on Learning Representations,
2016.

A COMPRESSED SENSING BACKGROUND

The ﬁeld of compressed sensing is concerned with recovering a high-dimensional k-sparse signal
x ∈ RN from few linear measurements. In the noiseless case this is formulated as

(10)
where A ∈ Rd×N is the design matrix and z = Ax is the measurement vector. Since (cid:96)0-minimization
is NP-hard, a foundational approach is to use its convex surrogate, the (cid:96)1-norm, and characterize
when the solution to (10) is equivalent to that of the following LP, known as basis pursuit (BP):

subject to Aw = z

minimize (cid:107)w(cid:107)0

minimize (cid:107)w(cid:107)1

subject to Aw = z

(11)

Related approaches such as Basis Pursuit Denoising (LASSO) and the Dantzig Selector generalize
BP to handle signal or measurement noise (Foucart & Rauhut, 2013); however, the word embeddings
case is noiseless so these methods reduce to BP. Note that throughout Section 5 and the Appendix
we say that an (cid:96)1-minimization method recovers x from Ax if its optimal solution is unique and
equivalent to the optimal solution of (10).

An alternative way to approximately solve (10) is to use a greedy algorithm such as matching pursuit
(MP) or orthogonal matching pursuit (OMP), which pick basis vectors one at a time by multiplying
the measurement vector by AT and choosing the column with the largest inner product (Tropp, 2004).

A.1 SOME COMMON SPARSE RECOVERY CONDITIONS

One condition through which recovery can be guaranteed is the Restricted Isometry Property (RIP):
Deﬁnition A.1. A ∈ Rd×N is (k, ε)-RIP if for all k-sparse x ∈ RN

(1 − ε)(cid:107)x(cid:107)2 ≤ (cid:107)Ax(cid:107)2 ≤ (1 + ε)(cid:107)x(cid:107)2

√

A line of work started by Candès & Tao (2005) used the RIP property to characterize matrices A such
that (10) and (11) have the same minimizer for any k-sparse signal x; this occurs with overwhelming
probability when d = Ω (cid:0)k log N
Since the ability to recover a signal x from a representation Ax implies information preservation, a
natural next step is to consider learning after compression. Calderbank et al. (2009) show that for
m i.i.d. k-sparse samples {(xi, yi)}m
i=1 and a (2k, ε)-RIP matrix A, the hinge loss of a classiﬁer
trained on {(Axi, yi)}m
i=1 is bounded by that of the best linear classiﬁer over the original samples.
Theorem 4.2 provides a generalization of this result to any convex Lipschitz loss function.

dAij ∼ N (0, 1) ∀ i, j or

dAij ∼ U{−1, 1} ∀ i, j.

(cid:1) and

√

k

RIP is a strong requirement, both because it is not necessary for perfect, stable recovery of k-sparse
vectors using ˜O(k) measurements and because in certain settings we are interested in using the above
ideas to recover speciﬁc signals — those statistically likely to occur—rather than all k-sparse signals.
The usual necessary and sufﬁcient condition to recover any vector x ∈ RN with index support set
S ⊂ [N ] is the local nullspace property (NSP), which is implied by RIP:
Deﬁnition A.2 (Foucart & Rauhut, 2013). A matrix A ∈ Rd×N satisﬁes NSP for a set S ⊂ [N ] if
(cid:107)wS(cid:107)1 < (cid:107)wS(cid:107)1 for all nonzero w ∈ ker(A) = {v : Av = 0d}.
Theorem A.1 (Foucart & Rauhut, 2013). BP (11) recovers any x ∈ RN
iff A satisﬁes NSP for S.

+ with supp(x) = S from Ax

13

Published as a conference paper at ICLR 2018

A related condition that implies NSP is the local restricted eigenvalue property (REP):
Deﬁnition A.3 (Raskutti et al., 2010). A matrix A ∈ Rd×N satisﬁes γ-REP for a set S ⊂ [N ] if
(cid:107)Aw(cid:107)2 ≥ γ

d(cid:107)w(cid:107)2 whenever (cid:107)wS(cid:107)1 ≤ (cid:107)wS(cid:107)1.

√

Lastly, a simple condition that can sometimes provide recovery guarantees is mutual incoherence:
Deﬁnition A.4. A ∈ Rd×N is µ-incoherent if maxa,a(cid:48) |aT a(cid:48)| ≤ µ, where the maximum is taken over
any two distinct columns a, a(cid:48) of A.

While incoherence is easy to verify (unlike the previous recovery properties), word embeddings tend
to have high coherence due to the training objective pushing together vectors of co-occurring words.

A.2 CONDITIONS FOR RECOVERING NONNEGATIVE SIGNALS

Apart from incoherence, the properties above are hard show empirically. However, we are compress-
ing BoW vectors, so our signals are nonnegative and we can impose an additional constraint on (11):

minimize (cid:107)w(cid:107)1

subject to Aw = z, w ≥ 0d

(12)
The following geometric result provides guarantees for this nonnegative basis pursuit (BP+) problem:
Theorem A.2 (Donoho & Tanner, 2005). Consider a matrix A ∈ Rd×N and an index subset S ⊂ [N ]
of size k. Then any nonnegative vector x ∈ RN
+ with support supp(x) = S is recovered from Ax
by BP+ (12) iff the columns of A indexed by S comprise the vertices of a k-dimensional face of the
convex hull conv(A) of the columns of A together with the origin.

The polytope condition is equivalent to nonnegative NSP (NSP+), a weaker form of NSP:
Deﬁnition A.5 (Foucart & Koslicki, 2014). A matrix A ∈ Rd×N satisﬁes NSP+ for a set S ⊂ [N ] if

wS ≥ 0N =⇒

wi > 0 for all nonzero w ∈ ker(A).

N
(cid:80)
i=1

Lemma A.1. If A ∈ Rd×N satisﬁes NSP for some S ⊂ [N ] then it also satisﬁes NSP+ for S.

Proof (Adapted from Foucart & Koslicki (2014)). Since A satisﬁes NSP, we have (cid:107)wS(cid:107)1 < (cid:107)wS(cid:107)1.
Then for a nonzero w ∈ ker(A) such that wS ≥ 0 we will have

N
(cid:88)

i=1

wi =

wi +

wj ≥ −

|wi| +

|wj| = −(cid:107)wS(cid:107)1 + (cid:107)wS(cid:107)1 > 0

(cid:88)

i∈S

(cid:88)

j∈S

(cid:88)

i∈S

(cid:88)

j∈S

Lemma A.2. BP+ recovers any x ∈ RN

+ with supp(x) = S from Ax iff A satisﬁes NSP+ for S.

Proof. ( =⇒ ): For any nonzero w ∈ ker(A) such that wS ≥ 0, ∃ λ > 0 such that x + λw ≥ 0N
and A(x + λw) = Ax. Since BP+ uniquely recovers x, we have (cid:107)x + λw(cid:107)1 > (cid:107)x(cid:107)1, so NSP+
follows from the following inequality and the fact that λ is positive:

0 < (cid:107)x + λw(cid:107)1 − (cid:107)x(cid:107)1 =

(xi + λwi) −

xi = λ

wi =⇒

wi > 0

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

( ⇐= ): For any x(cid:48) ≥ 0 such that Ax(cid:48) = Ax we have that w = x(cid:48) − x ∈ ker(A) and wS = x(cid:48)
S

≥ 0

since the support of x is S. Thus by NSP+ we have that

wi > 0, which yields

N
(cid:88)

i=1

N
(cid:80)
i=1

(cid:107)x(cid:48)(cid:107)1 − (cid:107)x(cid:107)1 =

x(cid:48)
i −

xi =

wi > 0

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

Thus BP+ will recover x uniquely.

Lemma A.2 shows that NSP+ is equivalent to the polytope condition in Theorem A.2, as they are
both necessary and sufﬁcient conditions for BP+ recovery.

14

Published as a conference paper at ICLR 2018

Model

Bigram

Trigram

Representation MR
78.3
77.8

BonG
BonC

BonG
BonC

77.9
77.8

CR
78.2
78.1

78.3
78.3

SUBJ MPQA TREC SST (±1)
91.9
91.8

90.0
90.0

85.7
85.8

80.8
80.9

91.5
91.4

85.6
85.6

89.2
89.8

80.2
80.1

SST IMDB
90.0
39.1
90.0
39.0

39.1
42.3

90.2
89.8

Table 3: The performance of an l2-regularized logit classiﬁer over Bag-of-n-Grams (BonG) vectors
is generally similar to that of Bag-of-n-Cooccurrences (BonC) vectors for n = 2, 3 (largest differ-
ences bolded). Evaluation settings are the same as in Section 6. Note that for unigrams the two
representations are equivalent.

Model

Bigram

Trigram

Representation MR
80.1
79.8

DisC (2)
Conv. (13)

DisC (2)
Conv. (13)

80.0
79.7

CR
81.5
81.4

81.3
81.6

SUBJ MPQA TREC SST (±1)
92.6
92.6

85.5
84.3

87.9
87.9

89.6
89.4

92.6
92.8

87.9
87.9

90.0
89.8

85.2
84.2

SST IMDB
89.4
46.4
89.5
45.5

46.7
45.8

89.6
89.5

Table 4: Performance comparison of element-wise product (DisC) and circular convolution for
encoding local cooccurrences (best result for each task is bolded). Evaluation settings are the same
as in Section 6. Note that for unigrams the two representations are equivalent.

B DOCUMENT EMBEDDINGS

B.1 PERFORMANCE COMPARISON OF ALTERNATIVE REPRESENTATIONS

In this section we compare the performance of several alternative representations with the ones
presented in the main evaluation (Table 1). Table 3 provides a numerical justiﬁcation for our use of
unordered n-grams (cooccurrences) instead of n-grams, as the performance of the two featurizations
are closely comparable. In Table 4 we examine the use of circular convolution instead of elementwise
multiplication as linear measurements of BonC vectors Plate (1995). To construct the former from a
document w1, . . . , wT we compute

(cid:34) T

(cid:88)

t=1

vwt, . . . ,

T −n+1
(cid:88)

F −1

(cid:32)t+n−1
(cid:89)

t=1

τ =t

(cid:33)(cid:35)

F(vwτ )

(13)

where F is the discrete Fourier transform and F −1 its inverse. Note that for n = 1 this is equivalent
to the simple unigram embedding (and thus also to the DisC embedding in (2)).

B.2 SCALING FACTOR FOR DISC EMBEDDINGS

Lemma B.1. If word vectors vw ∈ Rd are drawn i.i.d. from U d{±1/
g = (w1, . . . , wn) we have E(cid:107)˜vg(cid:107)2
that all words in g are distinct if the word vectors are i.i.d. d-dimensional spherical Gaussians.

d} then for any n-gram
2 = 1. The same result holds true with the additional assumption

√

Proof.

E (cid:107)˜vg(cid:107)2

2 =

(cid:32)

d
(cid:88)

E

n−1
2

d

i=1

n
(cid:89)

k=1

(cid:33)2

vwk i

= dn−1

d
(cid:88)

E

(cid:32) n
(cid:89)

i=1

k=1

(cid:33)

v2
wk i

= dn−1

d
(cid:88)

n
(cid:89)

i=1

i=1

1
d

= 1

15

Published as a conference paper at ICLR 2018

B.3 PROOF OF PROPOSITION 3.1

Let f (x) = i(x) = g(x) = x with
(cid:18) 1nd
0(n−1)d

Tf (vwt, ht−1) =

(cid:19)

Ti(vwt, ht−1) =

Tg(vwt, ht−1) =

· · ·

. . .
. . .

0d×nd
...
...
...
0(n−2)d×nd

C1Id
...

Cnd

n−1

2 Id























Id
...
Id










vwt

I(n−2)d 0(n−2)d×d

0d×d

Id

0d×d












ht−1 +






1d
0(n−1)d
1d
0(n−2)d






I(n−2)d 0(n−2)d×d

Substituting these parameters into the LSTM update (3) and using h0 = 0 we have ∀ t > 0 that

ht =

Cnd

n−1
2

k=1 vwτ +k−1

=

Cnd

n−1
2

















C1

t
(cid:80)
τ =1

vwτ

...
(cid:74)n

t−n+1
(cid:80)
τ =1

vwt
...
k=1 vwt+k−n+1

(cid:74)n−1

































C1

t
(cid:80)
τ =1

˜vwτ

t−n+1
(cid:80)
τ =1

...
˜v{wτ ,...,wτ +n−1}
˜vwt
...
˜v{wt−n+2,...,wt}

















Thus

hT =

Cnd

n−1
2

T −n+1
(cid:80)
t=1

















C1

˜vwt

T
(cid:80)
t=1

...
˜v{wt,...,wt+n−1}

˜vwT
...
˜v{wT −n+2,...,wT }

















=







˜z(n)
˜vwT
...
˜v{wT −n+2,...,wT }







Note that ht ∈ R(2n−1)d so as desired the LSTM has O(nd)-memory. Although hT contains (n−1)d
more dimensions than ˜z(n), by padding the end of the document with an end-of-document token
whose word vector is 0d the entries in those dimensions will be set to zero by the update at the last
step. Thus up to zero padding we will have zLSTM = hT = ˜z(n).

C PROOF OF THEOREM 4.2

Throughout this section we assume the setting described in Theorem 4.2. Furthermore for some
positive constant C deﬁne the (cid:96)2-regularization of the loss function (cid:96) as

L(w) = (cid:96)(w) +

1
2C

(cid:107)w(cid:107)2
2

16

Published as a conference paper at ICLR 2018

Lemma C.1. Let ˆw be the classiﬁer obtained minimizing LS(w) = 1
m

(cid:96)(wT xi, yi) + 1

2C (cid:107)w(cid:107)2
2,

where (cid:96)(·, ·) is a convex λ-Lipschitz function in the ﬁrst cordinate. Then

m
(cid:80)
i=1

ˆw =

αiyixi

m
(cid:88)

i=1

(14)

where |αi| ≤ λC

m ∀ i. This result holds in the compressed domain as well.

Proof. If (cid:96) is an λ-Lipschitz function, its sub-gradient at every point is bounded by λ. So by convexity,
the unique optimizer is given by taking ﬁrst-order conditions:

0 = ∂wLS(w) =

+

∂wT xi(cid:96)(wT xi, yi)xi

w
C

1
m

m
(cid:88)

i=1

C
m

m
(cid:88)

i=1

=⇒ ˆw =

−yi∂ ˆwT xi(cid:96)( ˆwT xi, yi)yixi

(15)

Since (cid:96) is Lipschitz, |∂wT xi(cid:96)(wT xi, yi)| ≤ λ. Therefore the ﬁrst-order optimal solution (15) of ˆw
can be expressed as (14) for some α1, . . . , αm satisfying |αi| ≤ λC
m ∀ i, which is the desired result.

Lemma C.2. x, x(cid:48) ∈ X =⇒ (1 + ε)xT x(cid:48) − 2R2ε ≤ (Ax)T (Ax(cid:48)) ≤ (1 − ε)xT x(cid:48) + 2R2ε

Proof. Since A is (∆X , ε)-RIP we have (1 − ε)(cid:107)x − x(cid:48)(cid:107)2 ≤ (cid:107)A(x − x(cid:48))(cid:107)2 ≤ (1 + ε)(cid:107)x − x(cid:48)(cid:107)2.
Also since 0N ∈ X , A is also (X , ε)-RIP and the result then follows by the same argument as in
(Calderbank et al., 2009, Lemma 4.2-3).

Corollary C.1. (cid:107) ˆw(cid:107)2

2 ≤ λ2C 2R2 and (cid:107) ˆwA(cid:107)2

2 ≤ λ2C 2(1 + ε)2R2.

Proof. The ﬁrst bound follows by expanding (cid:107) ˆw(cid:107)2
expanding (cid:107) ˆwA(cid:107)2

2, applying Lemma C.2 to bound inner product distortion, and using (cid:107)x(cid:107)2 ≤ R.

2 and using (cid:107)x(cid:107)2 ≤ R; the second follows by

Lemma C.3. Let ˆw be the linear classiﬁer minimizing LS. Then

LD(A ˆw) ≤ LD( ˆw) + O(λ2CR2ε)

Proof. By Lemma C.1 we can re-express ˆw using Equation 14 and then apply the inequality from
Lemma C.2 to get

(A ˆw)T (Ax) =

αiyi(Axi)T (Ax)

αiyi

(cid:0)(1 − ε)xT

i x + 2R2ε(cid:1) +

(cid:88)

αiyi

(cid:0)(1 + ε)xT

i x − 2R2ε(cid:1)

i:αiyi<0

= ˆwT x − ε

|αiyi|xT

i x + 2R2ε

|αiyi| ≤ ˆwT x + 3λCR2ε

(A ˆw)T (Ax) =

αiyi(Axi)T (Ax)

αiyi

(cid:0)(1 + ε)xT

i x − 2R2ε(cid:1) +

(cid:88)

αiyi

(cid:0)(1 − ε)xT

i x + 2R2ε(cid:1)

i:αiyi<0

= ˆwT x + ε

|αiyi|xT

i x − 2R2ε

|αiyi| ≥ ˆwT x − 3λCR2ε

m
(cid:88)

i=1

(cid:88)

≤

i:αiyi≥0

m
(cid:88)

i=1

(cid:88)

≥

i:αiyi≥0

m
(cid:88)

i=1

m
(cid:88)

i=1

m
(cid:88)

i=1

m
(cid:88)

i=1

17

Published as a conference paper at ICLR 2018

for any x ∈ RN . Since (cid:96) is λ-Lipschitz taking expectations over D implies

(cid:96)D(A ˆw) ≤ (cid:96)D( ˆw) + 3λ2CR2ε

(16)

Substituting Equation 14 applying Lemma C.2 also yields
m
(cid:88)

m
(cid:88)

αiαjyiyj(Axi)T (Axj)

(cid:107)A ˆw(cid:107)2

2 =

αiαjyiyj

(cid:0)(1 − ε)xT

i xj + 2R2ε(cid:1)

αiαjyiyj

(cid:0)(1 + ε)xT

i xj − 2R2ε(cid:1)

i xj +

−|αiαjyiyj|εxT

i xj + 2R2|αiαjyiyj|ε

i=1

j=1
(cid:88)

≤

i,j:αiαj yiyj ≥0

(cid:88)

+

i,j:αiαj yiyj <0
αiαjyiyjxT

(cid:88)

≤

i,j
≤ (cid:107) ˆw(cid:107)2

2 + 3λ2C 2R2ε

(cid:88)

i,j

1
2C

which implies

Together the inequalities bounding the loss term (16) and the regularization term (17) imply the result.

1
2C

(cid:107)A ˆw(cid:107)2

2 ≤

(cid:107) ˆw(cid:107)2

2 +

λ2CR2ε

3
2

(17)

Lemma C.4. Let ˆw be the linear classiﬁer minimizing LS and let w∗ be the linear classiﬁer
minimizing LD. Then with probability 1 − γ

LD( ˆw) ≤ LD(w∗) + O

(cid:18) λ2CR2
m

log

(cid:19)

1
γ

This result holds in the compressed domain as well.

Proof. By Corollary C.1 we have that ˆw is contained in a closed convex subset independent of S.
Therefore since (cid:96) is λ-Lipschitz, L is 1
C -strongly convex, and (cid:107)x(cid:107)2 ≤ O(R), we have by (Sridharan
et al., 2008, Theorem 1) that with probability 1 − γ

LD( ˆw) − LD(w∗) ≤ 2 [LS( ˆw) − LS(w∗)]+ + O

(cid:18) λ2CR2
m

log

(cid:19)

1
γ

Then since by deﬁnition ˆw minimizes LS(w) we have that LS( ˆw) ≤ LS(w∗), which substituted into
the previous equation completes the proof.

Proof of Theorem 4.2. Applying Lemma C.4 in the compressed domain yields

(cid:96)D( ˆwA) ≤ (cid:96)D( ˆwA) +

(cid:107) ˆwA(cid:107)2

2 = LD( ˆwA) ≤ LD(w∗

1
2C

(cid:18) λ2CR2
m
A) ≤ LD(A ˆw), so together with Lemma C.3

A) + O

log

1
γ

(cid:19)

where w∗
and the previous inequality we have

A minimizes LD. By deﬁnition of w∗

A, LD(w∗

(cid:96)D( ˆwA) ≤ LD(A ˆw) + O

log

≤ LD( ˆw) + O

λ2CR2

ε +

log

(cid:18) λ2CR2
m

(cid:19)

1
γ

(cid:18)

(cid:18)

1
m

(cid:19)(cid:19)

1
γ

We now apply Lemma C.4 in the sparse domain to get

(cid:96)D( ˆwA) ≤ LD(w∗) + O

λ2CR2

ε +

log

(cid:18)

(cid:18)

(cid:19)(cid:19)

1
γ

where w∗ minimizes LD. By deﬁnition of w∗, LD(w∗) ≤ LD(w0) = (cid:96)D(w0) + 1
the previous inequality we have

2C (cid:107)w0(cid:107)2

2, so by

(cid:96)D( ˆwA) ≤ (cid:96)D(w0) +

(cid:107)w0(cid:107)2

2 + O

λ2CR2

ε +

log

1
2C

(cid:18)

1
m

(cid:19)(cid:19)

1
γ

Substituting the C that minimizes the r.h.s. of this inequality completes the proof.

1
m

(cid:18)

18

Published as a conference paper at ICLR 2018

D PROOF OF LEMMA 4.1

We assume the setting described in Lemma 4.1, where we are concerned with the RIP condition of
the matrix A(n) when multiplying vectors x ∈ X (n)
T , the set of BonC vectors for documents of length
at most T . This matrix can be written as
A1





A(n) =






0d×V1
...
0d×V1

0d×V2
. . .
. . .
· · ·

· · ·
. . .
. . .
0d×Vn−1

0d×Vn
...
0d×Vn
An






where Ap is the d × Vp matrix whose columns are the DisC embeddings of all p-grams in the
vocabulary (and thus A(1) = A1 = A, the matrix of the original word embeddings). Note that from
(1) any x ∈ X (n)
can be written as x = [x1, . . . , xn], where xp is a T -sparse vector whose entries
correspond to p-grams. Thus we also have A(n)x = [A1x1, . . . , Anxn].
(cid:16)
Lemma D.1. If Ap is (2k, ε)-RIP w.p. 1 − γ ∀ p ∈ [n] then A(n) is
1 − nγ.

-RIP w.p. at least

∆X (n)
k

, ε

(cid:17)

T

Proof. By union bound we have that Ap is (2k, ε)-RIP ∀ p ∈ [n] with probability at least 1 − nγ.
Thus by Deﬁnition 4.1 we have w.p. 1 − nγ that ∀ x ∈ ∆X (n)
n
(cid:88)

n
(cid:88)

k

(cid:107)A(n)x(cid:107)2

2 =

(cid:107)Apxp(cid:107)2

2 ≤

(1 + ε)2(cid:107)xp(cid:107)2

2 = (1 + ε)2(cid:107)x(cid:107)2
2

p=1

p=1

2. From Deﬁnition 4.1, taking the square root of both sides of

Similarly, (cid:107)A(n)x(cid:107)2
2 ≥ (1 − ε)2(cid:107)x(cid:107)2
both inequalities completes the proof.
Deﬁnition D.1 (Foucart & Rauhut, 2013). Let D be a distribution over a subset S ⊂ Rn. Then
the set Φ = {φ1, . . . , φN } of functions φi : S (cid:55)→ R is a bounded orthonormal system (BOS) with
constant B if we have ED(φiφj) = 1i=j ∀ i, j and sups∈S |φi(s)| ≤ B ∀ i. Note that by deﬁnition
B ≥ 1.
Theorem D.1 (Foucart & Rauhut, 2013). If d = ˜Ω
d × N matrix associated with a BOS with constant B then A is (k, ε)-RIP w.p. 1 − γ.
Lemma D.2. If d = ˜Ω
then for any p ∈ [n] the matrix Ap ∈ Rd×Vp of DisC embeddings is (T, ε)-RIP w.p. 1 − γ.

and the word embeddings are drawn i.i.d. from U d{±1/

for (ε, γ) ∈ (0, 1) and

(cid:16) T
ε2 log Vp

(cid:16) B2k
ε2

log N
γ

dA is a

d}

√

√

(cid:17)

(cid:17)

γ

√

√

(cid:0)x(i)(cid:1) = (cid:81)p

Proof. Note that by Theorem D.1 it sufﬁces to show that
dAp is a random sampling matrix
associated with a BOS with constant B = 1. Let D = U V {±1} be the uniform distribution over
V i.i.d. Rademacher random variables indexed by words in the vocabulary. Then by deﬁnition the
matrix Ap ∈ Rd×Vp can be constructed by drawing random variables x(1), . . . , x(d) i.i.d. from D
and assigning to the ijth entry of
dAp corresponding to the p-gram g = {g1, . . . , gp} the value
t=1 x(i)
gt , where each function φj : {±1}V (cid:55)→ R is uniquely associated to its p-gram.
φj
It remains to be shown that this set of functions is a BOS with constant B = 1.
(cid:1),
For any two p-grams g, g(cid:48) and their functions φi, φj we have ED(φiφj) = Ex∼D
which will be 1 iff each word in g ∪ g(cid:48) occurs an even number of times in the product and 0 otherwise.
Because all p-grams are uniquely deﬁned under any permutation of its words (i.e. we are in fact using
p-cooccurrences) and we have assumed that no p-gram contains a word more than once, each word
occurs an even number of times in the product iff g = g(cid:48) ⇐⇒ i = j. Furthermore we have that
|φi(x)| ≤ 1 ∀ x ∈ {±1}V ∀ i by construction. Thus according to Deﬁnition D.1 the set of functions
{φ1, . . . , φVp } associated to the p-grams in the vocabulary is a BOS with constant B = 1.

t=1 xgtxg(cid:48)

(cid:0)(cid:81)p

t

Proof of Lemma 4.1. Since d = ˜Ω
1 − γ

n ∀ p ∈ [n]. Applying Lemma D.1 yields the result.

(cid:16) T
ε2 log nV max

n
γ

(cid:17)

, Lemma D.2 implies that Ap is (2T, ε)-RIP w.p.

19

Published as a conference paper at ICLR 2018

E DETAILS OF THE SUPPORTING HYPERPLANE PROPERTY

In Section 5.2, Deﬁnition 5.1 we introduced the Supporting Hyperplane Property (SHP), which
by Corollary 5.1 characterizes when BP+ perfectly recovers a nonnegative signal. Together with
Lemmas A.1 and A.2 this fact also shows that SHP is a weaker condition than the well-known
nullspace property (NSP):
Corollary E.1. If a matrix A ∈ Rd×N with columns in general position satisﬁes NSP for some
S ⊂ [N ] then it also satisﬁes S-SHP.

In this section we give the entire proof of Corollary 5.1 and describe how to verify SHP given a
design matrix and a set of support indices.

E.1 PROOF OF COROLLARY 5.1

Recall that it sufﬁces to show equivalence of A being S-SHP with the columns AS forming the
vertices of a k-dimensional face of conv(A), where we can abuse notation to set A ∈ Rd×(N +1),
with the extra column being the origin 0d, so long as we constrain N + 1 (cid:54)∈ S.

( =⇒ ): The proof of the forward direction appeared in full in the proof sketch (see Section 5.2).
( ⇐= ): A subset F ⊂ Rd is a face of conv(A) if for some hyperplane H = {v : aT v − b = 0} we
have F = conv(A) ∩ H and conv(A)\F ⊆ H− = {v : aT v − b < 0}, where H− is the negative
halfspace of H. Deﬁne the simplex ∆m = {λ ∈ [0, 1]m : (cid:80)m
Since A is S-SHP we have a hyperplane H = {v : aT v − b = 0} containing the columns AS such
that AS ⊂ H−. Thus aT Ai − b = 0 ∀ i ∈ S and aT Ai − b < 0 ∀ i /∈ S. We also know that
F = {(cid:80)
i∈S λiAi : λ ∈ ∆|S|} ⊆ H by convexity of H. Since any point y ∈ conv(A)\F can be
written as y = (cid:80)N +1

i=1 λiAi for some λ ∈ ∆N +1 such that ∃ j /∈ S such that λj (cid:54)= 0, we have that

i=1 λi = 1}.

aT y − b =

λi(aT Ai − b) +

λj(aT Aj − b) =

λj(aT Aj − b) < 0

(cid:88)

i∈S

(cid:88)

j /∈S

(cid:88)

j /∈S

This implies that conv(A)\F ⊆ H− and F = conv(A) ∩ H, so since the columns of A are in general
position F is a k-dimensional face of conv(A) whose vertices are the columns AS.

E.2 VERIFYING SHP

Recall that a matrix Rd×N satisﬁes S-SHP for S ⊂ [N ] if there is a hyperplane containing the set of
all columns of A indexed by S and the set of all other columns together with the origin are on one
side of it. Due to Corollary 5.1, checking S-SHP allows us to know whether all nonnegative signals
with index support S will be recovered by BP+ without actually running the optimization on any one
of them. The property can be checked by solving a convex problem of the form
(cid:110) ˜AT

subject to

max

˜AT

(cid:88)

(cid:111)p

S h = 0|S|

i h + ε, 0

min
h∈Rd+1

i(cid:54)∈S

where

˜A =

(cid:19)

(cid:18) A 0d
1T
1
N

and ε > 0, p ≥ 1

The constraint enforces the property that the hyperplane contains all support embeddings, while the
optimal objective value is zero iff SHP is satisﬁed (this follows from the fact that scaling h does
not affect the constraint so if the minimal objective is zero for any single ε > 0 it is zero for all
ε > 0). The problem can be solved via using standard ﬁrst or second-order equality-constrained
convex optimization algorithms. We set ε = 1 and p = 3 (to get a C2 objective) and adapt the
second-order method from Boyd & Vandenberghe (2004, Chapter 10). Our implementation can be
found at https://github.com/NLPrinceton/sparse_recovery.

20

Published as a conference paper at ICLR 2018

Figure 7: Efﬁciency of pretrained embeddings as sensing vectors at d = 300 dimensions, measured
via the F1-score of the original BoW. 200 documents from each dataset were compressed and
recovered in this experiment. For fairness, the number of words V is the same for all embeddings
so all documents are required to be subsets of the vocabulary of all corpora. word2vec embeddings
trained on Google News and GloVe vectors trained on Common Crawl were obtained from public
repositories (Mikolov et al., 2013; Pennington et al., 2014) while Amazon and Wikipedia embeddings
were trained for 100 iterations using a symmetric window of size 10, a min count of 100, for SN/GloVe
a cooccurrence cutoff of 1000, and for word2vec a down-sampling frequency cutoff of 10−5 and a
negative example setting of 3. 300-dimensional normalized random vectors are used as a baseline.

F SPARSE RECOVERY WITH PRETRAINED EMBEDDINGS

F.1 PERFORMANCE COMPARISON OF ALTERNATIVE EMBEDDINGS

We show in Figure 7 that the surprising effectiveness of word embeddings as linear measurement
vectors for BoW signals holds for other embedding objectives and corpora as well. Speciﬁcally,
we see that widely used embeddings, when normalized, match the efﬁciency of random vectors for
retrieving SST BoW and are more efﬁcient when retrieving IMDB BoW. Interestingly, SN vectors are
most efﬁcient and are also the only embeddings for normalizing is not needed for good performance.

F.2 A MODEL-BASED THEORETICAL APPROACH

In Section 5.2 we gave some intuition for why pretrained word embeddings are efﬁcient sensing
vectors for natural language BoW by examining a geometric characterization of local equivalence due
to Donoho & Tanner (2005) in light of the usual similarity properties of word embeddings. However,
this analysis does not provide a rigorous theory for our empirical results. In this section we brieﬂy
discuss a model-based justiﬁcation that may lead to a stronger understanding.

We need a model relating BoW generation to the word embeddings trained over words co-occurring
in the same BoW. As a starting point consider the model of Arora et al. (2016), in which a corpus is
generated by a random walk ct over the surface of a ball in Rd; at each t a word w is emitted w.p.
P(w|ct) ∝ exp(cid:104)ct, vw(cid:105)

(18)

Minimizing the SN objective approximately maximizes corpus likelihood under this model.

Thus in an approximate sense a document of length T is generated by setting a context vector c
and emitting T words via (18) with ct = c. This model is a convenient one for analysis due its
simplicity and invariance to word order as well as the fact that the approximate maximum likelihood
document vector is the sum of the embeddings of words in the document. Building upon the intuition
established following Corollary 5.1 one can argue that, if we have the true latent SN vectors, then
embeddings of words in the same document (i.e. emitted by the same context vector) will be close to
each other and thus easy to separate from the embeddings of other words in the vocabulary.

However, we ﬁnd empirically that not all of the T words closest to the sum of the word embeddings
(i.e. the context vector) are the ones emitted; indeed individual word vectors in a document may
have small, even negative inner product with the context vector and still be recovered via BP. Thus
any further theoretical argument must also be able to handle the recovery of lower probability words
whose vectors are further away from the context vector than those of words that do not appear in the
document. We thus leave to future work the challenge of explaining why embeddings resulting from
this (or another) model provide such efﬁcient sensing matrices for natural language BoW.

21

