Discourse Representation Structure Parsing

Jiangming Liu

Shay B. Cohen
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Jiangming.Liu@ed.ac.uk, scohen@inf.ed.ac.uk, mlap@inf.ed.ac.uk

Mirella Lapata

Abstract

We introduce an open-domain neural se-
mantic parser which generates formal
meaning representations in the style of
Discourse Representation Theory (DRT;
Kamp and Reyle 1993). We propose a
method which transforms Discourse Rep-
resentation Structures (DRSs) to trees and
develop a structure-aware model which
decomposes the decoding process into
three stages: basic DRS structure pre-
diction, condition prediction (i.e., predi-
cates and relations), and referent predic-
tion (i.e., variables). Experimental results
on the Groningen Meaning Bank (GMB)
show that our model outperforms compet-
itive baselines by a wide margin.

1

Introduction

Semantic parsing is the task of mapping natural
language to machine interpretable meaning repre-
sentations. A variety of meaning representations
have been adopted over the years ranging from
functional query language (FunQL; Kate et al.
2005) to dependency-based compositional seman-
tics (λ-DCS; Liang et al. 2011), lambda calculus
(Zettlemoyer and Collins, 2005), abstract meaning
representations (Banarescu et al., 2013), and min-
imal recursion semantics (Copestake et al., 2005).
Existing semantic parsers are for the most part
data-driven using annotated examples consisting
of utterances and their meaning representations
(Zelle and Mooney, 1996; Wong and Mooney,
2006; Zettlemoyer and Collins, 2005). The suc-
cessful application of encoder-decoder models
(Sutskever et al., 2014; Bahdanau et al., 2015) to
a variety of NLP tasks has provided strong impe-
tus to treat semantic parsing as a sequence trans-
duction problem where an utterance is mapped
to a target meaning representation in string for-
mat (Dong and Lapata, 2016; Jia and Liang, 2016;
Koˇcisk`y et al., 2016). The fact that meaning rep-
resentations do not naturally conform to a lin-

ear ordering has also prompted efforts to develop
recurrent neural network architectures tailored to
tree or graph-structured decoding (Dong and La-
pata, 2016; Cheng et al., 2017; Yin and Neubig,
2017; Alvarez-Melis and Jaakkola, 2017; Rabi-
novich et al., 2017; Buys and Blunsom, 2017)

Most previous work focuses on building seman-
tic parsers for question answering tasks, such as
querying a database to retrieve an answer (Zelle
and Mooney, 1996; Cheng et al., 2017), or con-
versing with a ﬂight booking system (Dahl et al.,
1994). As a result, parsers trained on query-based
datasets work on restricted domains (e.g., restau-
rants, meetings; Wang et al. 2015), with limited
vocabularies, exhibiting limited compositionality,
and a small range of syntactic and semantic con-
structions. In this work, we focus on open-domain
semantic parsing and develop a general-purpose
system which generates formal meaning represen-
tations in the style of Discourse Representation
Theory (DRT; Kamp and Reyle 1993).

DRT is a popular theory of meaning represen-
tation designed to account for a variety of linguis-
tic phenomena, including the interpretation of pro-
nouns and temporal expressions within and across
sentences. Advantageously, it supports meaning
representations for entire texts rather than isolated
sentences which in turn can be translated into ﬁrst-
order logic. The Groningen Meaning Bank (GMB;
Bos et al. 2017) provides a large collection of
English texts annotated with Discourse Represen-
tation Structures (see Figure 1 for an example).
GMB integrates various levels of semantic anno-
tation (e.g., anaphora, named entities, thematic
roles, rhetorical relations) into a uniﬁed formal-
ism providing expressive meaning representations
for open-domain texts.

We treat DRT parsing as a structure prediction
problem. We develop a method to transform DRSs
to tree-based representations which can be fur-
ther linearized to bracketed string format. We ex-
amine a series of encoder-decoder models (Bah-
danau et al., 2015) differing in the way tree-

x1, e1, π1
statement(x1), say(e1), Cause(e1, x1), Topic(e1,π1)

π1:

k1:

x2
thing(x)

⇒

k2:

x6
thing(x6)

⇒

x3, s1, x3, x5, e2
Topic(s1, x3), dead(s1),
man(x3), of(x2, x3),
magazine(x4), on(x5,x4)
vest(x5), wear(e2),
Agent(e2, x2), Theme(e2, x5)

x7, s2, x8, x9, e3
Topic(s2, x7), dead(s2),
man(x7), of(x6, x7),
|x8| = 2, hand(x9),
in(x8, x9), grenade(x8)
carry(e3), Agent(e3, x6),
Theme(e3, x8)

continuation(k1, k2), parallel(k1, k2)

Figure 1: DRT meaning representation for the sentence The statement says each of the dead men wore
magazine vests and carried two hand grenades.

structured logical forms are generated and show
that a structure-aware decoder is paramount to
open-domain semantic parsing. Our proposed
model decomposes the decoding process into three
stages. The ﬁrst stage predicts the structure of
the meaning representation omitting details such
as predicates or variable names. The second stage
ﬁlls in missing predicates and relations (e.g., thing,
Agent) conditioning on the natural language input
and the previously predicted structure. Finally, the
third stage predicts variable names based on the
input and the information generated so far.

Decomposing decoding into these three steps
reduces the complexity of generating logical
forms since the model does not have to predict
deeply nested structures, their variables, and pred-
icates all at once. Moreover, the model is able to
take advantage of the GMB annotations more efﬁ-
ciently, e.g., examples with similar structures can
be effectively used in the ﬁrst stage despite being
very different in their lexical make-up. Finally, a
piecemeal mode of generation yields more accu-
rate predictions; since the output of every decod-
ing step serves as input to the next one, the model
is able to reﬁne its predictions taking progressively
more global context into account. Experimen-
tal results on the GMB show that our three-stage
decoder outperforms a vanilla encoder-decoder
model and a related variant which takes shallow
structure into account, by a wide margin.

Our contributions in this work are three-fold:
an open-domain semantic parser which yields dis-
course representation structures; a novel end-to-
end neural model equipped with a structured de-
coder which decomposes the parsing process into
three stages; a DRS-to-tree conversion method
which transforms DRSs to tree-based representa-
tions allowing for the application of structured de-

coders as well as sequential modeling. We release
our code1 and tree formatted version of the GMB
in the hope of driving further research in open-
domain semantic parsing.

2 Discourse Representation Theory

In this section we provide a brief overview of the
representational semantic formalism used in the
GMB. We refer the reader to Bos et al. (2017) and
Kamp and Reyle (1993) for more details.

Discourse Representation Theory (DRT; Kamp
and Reyle 1993) is a general framework for rep-
resenting the meaning of sentences and discourse
which can handle multiple linguistic phenom-
ena including anaphora, presuppositions, and tem-
poral expressions. The basic meaning-carrying
units in DRT are Discourse Representation Struc-
tures (DRSs), which are recursive formal mean-
ing structures that have a model-theoretic interpre-
tation and can be translated into ﬁrst-order logic
(Kamp and Reyle, 1993). Basic DRSs consist
of discourse referents (e.g., x, y) representing en-
tities in the discourse and discourse conditions
(e.g., man(x), magazine(y)) representing informa-
tion about discourse referents. Following conven-
tions in the DRT literature, we visualize DRSs in
a box-like format (see Figure 1).

GMB adopts a variant of DRT that uses a neo-
Davidsonian analysis of events (Kipper et al.,
2008), i.e., events are ﬁrst-order entities character-
ized by one-place predicate symbols (e.g., say(e1)
in Figure 1). In addition, it follows Projective Dis-
course Representation Theory (PDRT; Venhuizen
et al. 2013) an extension of DRT speciﬁcally de-
veloped to account for the interpretation of pre-
suppositions and related projection phenomena

1https://github.com/EdinburghNLP/EncDecDRSparsing

(e.g., conventional implicatures). In PDRT, each
basic DRS introduces a label, which can be bound
by a pointer indicating the interpretation site of
semantic content. To account for the rhetorical
structure of texts, GMB adopts Segmented Dis-
course Representation Theory (SDRT; Asher and
Lascarides 2003).
In SDRT, discourse segments
are linked with rhetorical relations reﬂecting dif-
ferent characteristics of textual coherence, such as
temporal order and communicative intentions (see
continuation(k1, k2) in Figure 1).

More formally, DRSs are expressions of type
(cid:104)expe(cid:105) (denoting individuals or discourse refer-
ents) and (cid:104)expt(cid:105) (i.e., truth values):

(cid:104)expe(cid:105) ::= (cid:104)re f (cid:105),

(cid:104)expt (cid:105) ::= (cid:104)drs(cid:105)|(cid:104)sdrs(cid:105),

(1)

discourse referents (cid:104)re f (cid:105) are in turn classiﬁed
into six categories, namely common referents (xn),
event referents (en), state referents (sn), segment
referents (kn), proposition referents (πn), and time
referents (tn). (cid:104)drs(cid:105) and (cid:104)sdrs(cid:105) denote basic and
segmented DRSs, respectively:

(cid:104)drs(cid:105) ::= (cid:104)pvar(cid:105) :

((cid:104)pvar(cid:105), (cid:104)re f (cid:105))∗
((cid:104)pvar(cid:105), (cid:104)condition(cid:105))∗ ,

(2)

(cid:104)sdrs(cid:105) ::=

k1 : (cid:104)expt (cid:105), k2 : (cid:104)expt (cid:105)
coo(k1, k2)

|

k1:(cid:104)expt (cid:105)
k2:(cid:104)expt (cid:105)
sub(k1, k2)

,

(3)

Basic DRSs consist of a set of referents ((cid:104)re f (cid:105))
and conditions ((cid:104)condition(cid:105)), whereas segmented
DRSs are recursive structures that combine two
(cid:104)expt(cid:105) by means of coordinating (coo) or subor-
dinating (sub) relations. DRS conditions can be
basic or complex:

(cid:104)condition(cid:105) ::= (cid:104)basic(cid:105)|(cid:104)complex(cid:105),

(4)

Basic conditions express properties of discourse
referents or relations between them:

(cid:104)basic(cid:105) ::= (cid:104)sym1(cid:105)((cid:104)expe(cid:105)) | (cid:104)sym2(cid:105)((cid:104)expe(cid:105), (cid:104)expe(cid:105))

| (cid:104)expe(cid:105) = (cid:104)expe(cid:105) | (cid:104)expe(cid:105) = (cid:104)num(cid:105)
| timex((cid:104)expe(cid:105), (cid:104)sym0(cid:105))
| named((cid:104)expe(cid:105), (cid:104)sym0(cid:105), class).

(5)

where (cid:104)symn(cid:105) denotes n-place predicates, (cid:104)num(cid:105)
denotes cardinal numbers, timex expresses tem-
timex(x7, 2005) denotes
poral information (e.g.,
the year 2005), and class refers to named entity
classes (e.g., location).

Complex conditions are unary or binary. Unary
conditions have one DRS as argument and rep-
resent negation (¬) and modal operators express-
ing necessity ((cid:50)) and possibility ((cid:51)). Condition

sections
00-99
20-99
10-19
00-09

# doc
10,000
7,970
1,038
992

# sent
62,010
49,411
6,483
6,116

# token
1,354,149
1,078,953
142,344
132,852

avg
21.84
21.83
21.95
21.72

Table 1: Statistics on the GMB (avg denotes the
average number of tokens per sentence).

(cid:104)re f (cid:105) : (cid:104)expt(cid:105) represents verbs with propositional
content (e.g., factive verbs). Binary conditions are
conditional statements (→) and questions.

(cid:104)complex(cid:105) ::= (cid:104)unary(cid:105) | (cid:104)binary(cid:105),

(6)

(cid:104)unary(cid:105) ::= ¬(cid:104)expt (cid:105) | (cid:50)(cid:104)expt (cid:105)|(cid:51)(cid:104)expt (cid:105)|(cid:104)re f (cid:105) : (cid:104)expt (cid:105)

(cid:104)binary(cid:105) ::=(cid:104)expt (cid:105)→(cid:104)expt (cid:105)|(cid:104)expt (cid:105)∨(cid:104)expt (cid:105)|(cid:104)expt (cid:105)?(cid:104)expt (cid:105)

3 The Groningen Meaning Bank Corpus

Corpus Creation DRSs in GMB were obtained
from Boxer (Bos, 2008, 2015), and then reﬁned
using expert linguists and crowdsourcing meth-
ods. Boxer constructs DRSs based on a pipeline of
tools involving POS-tagging, named entity recog-
nition, and parsing. Speciﬁcally, it relies on the
syntactic analysis of the C&C parser (Clark and
Curran, 2007), a general-purpose parser using the
framework of Combinatory Categorial Grammar
(CCG; Steedman 2001). DRSs are obtained from
CCG parses, with semantic composition being
guided by the CCG syntactic derivation.

Documents in the GMB were collected from
a variety of sources including Voice of America
(a newspaper published by the US Federal Gov-
ernment), the Open American National Corpus,
Aesop’s fables, humorous stories and jokes, and
country descriptions from the CIA World Fact-
book. The dataset consists of 10,000 documents
each annotated with a DRS. Various statistics on
the GMB are shown in Table 1. Bos et al. (2017)
recommend sections 20–99 for training, 10–19 for
tuning, and 00–09 for testing.

DRS-to-Tree Conversion As mentioned earlier,
DRSs in the GMB are displayed in a box-like for-
mat which is intuitive and easy to read but not par-
ticularly amenable to structure modeling. In this
section we discuss how DRSs were post-processed
and simpliﬁed into a tree-based format, which
served as input to our models.

The GMB provides DRS annotations per-
document. Our initial efforts have focused on
sentence-level DRS parsing which is undoubtedly

a necessary ﬁrst step for more global semantic rep-
It is relatively, straightforward to
resentations.
obtain sentence-level DRSs from document-level
annotations since referents and conditions are in-
dexed to tokens. We match each sentence in a doc-
ument with the DRS whose content bears the same
indices as the tokens occurring in the sentence.
This matching process yields 52,268 sentences for
training (sections 20–99), 5,172 sentences for de-
velopment (sections 10–19), (development), and
5,440 sentences for testing (sections 00–09).

In order to simplify the representation, we omit
referents in the top part of the DRS (e.g., x1, e1
and π1 in Figure 1) but preserve them in condi-
tions without any information loss. Also we ignore
pointers to DRSs since this information is implic-
itly captured through the typing and co-indexing
of referents. Deﬁnition (1) is simpliﬁed to:

(cid:104)drs(cid:105) ::= DRS((cid:104)condition(cid:105)∗),

(7)

where DRS() denotes a basic DRS. We also mod-
ify discourse referents to SDRSs (e.g., k1, k2 in
Figure 1) which we regard as elements bearing
scope over expressions (cid:104)expt(cid:105) and add a 2-place
predicate (cid:104)sym2(cid:105) to describe the discourse relation
between them. So, deﬁnition (3) becomes:

(cid:104)sdrs(cid:105) ::=SDRS(((cid:104)re f (cid:105)((cid:104)expt (cid:105)))∗
((cid:104)sym2(cid:105)((cid:104)re f (cid:105), (cid:104)re f (cid:105)))∗),

(8)

where SDRS() denotes a segmented DRS, and
(cid:104)re f (cid:105) are segment referents.

We treat cardinal numbers (cid:104)num(cid:105) and (cid:104)sym0(cid:105)
in relation timex as constants. We introduce the
binary predicate “card” to represent cardinality
(e.g., |x8| = 2 is card(x8, NUM)). We also sim-
plify (cid:104)expe(cid:105) = (cid:104)expe(cid:105) to eq((cid:104)expe(cid:105), (cid:104)expe(cid:105)) using
the binary relation “eq” (e.g., x1 = x2 becomes
eq(x1, x2)). Moreover, we ignore class in named
and transform named((cid:104)expe(cid:105), (cid:104)sym0(cid:105), class) into
(cid:104)sym1(cid:105)((cid:104)expe(cid:105)) (e.g., named(x2, mongolia, geo)
becomes mongolia(x2)). Consequently, basic con-
ditions (see deﬁnition (5)) are simpliﬁed to:

(cid:104)basic(cid:105) ::= (cid:104)sym1(cid:105)((cid:104)expe(cid:105))|(cid:104)sym2(cid:105)((cid:104)expe(cid:105), (cid:104)expe(cid:105))

(9)

Analogously, we treat unary and binary conditions
as scoped functions, and deﬁnition (6) becomes:

(cid:104)unary(cid:105) ::= ¬ | (cid:50) | (cid:51) | (cid:104)re f (cid:105)((cid:104)expt (cid:105))
(cid:104)binary(cid:105) ::= → | ∨ | ?((cid:104)expt (cid:105), (cid:104)expt (cid:105)),

(10)

Following the transformations described above,
the DRS in Figure 1 is converted into the tree in

Figure 2: Tree-based representation (top) of the
DRS in Figure 1 and its linearization (bottom).

Figure 2, which can be subsequently linearized
into a PTB-style bracketed sequence. It is impor-
tant to note that the conversion does not diminish
the complexity of DRSs. The average tree width
in the training set is 10.39 and tree depth is 4.64.

4 Semantic Parsing Models

We present below three encoder-decoder models
which are increasingly aware of the structure of
the DRT meaning representations. The models
take as input a natural language sentence X repre-
sented as w1, w2,. . . , wn, and generate a sequence
Y = (y1, y2, ..., ym), which is a linearized tree (see
Figure 2 bottom), where n is the length of the
sentence, and m the length of the generated DRS
sequence. We aim to estimate p(Y |X), the con-
ditional probability of the semantic parse tree Y
given natural language input X:

p(Y |X) = ∏
j

p(y j|Y j−1
1

, X n
1 )

4.1 Encoder

An encoder is used to represent the natural lan-
guage input X into vector representations. Each
token in a sentence is represented by a vec-
tor xk which is the concatenation of randomly
initialized embeddings ewi, pre-trained word em-
beddings ¯ewi, and lemma embeddings eli: xk =
tanh([ewi; ¯ewi; eli] ∗ W1 + b1), where W1 ∈ RD and
D is a shorthand for (dw + dp + dl) × dinput (sub-
scripts w, p, and l denote the dimensions of word
embeddings, pre-trained embeddings, and lemma
embeddings, respectively); b1 ∈ Rdinput and the
symbol ; denotes concatenation. Embeddings ewi

and eli are randomly initialized and tuned during
training, while ¯ewi are ﬁxed.

We use a bidirectional recurrent neural network
with long short-term memory units (bi-LSTM;
Hochreiter and Schmidhuber 1997) to encode nat-
ural language sentences:

[he1 : hen] = bi-LSTM(x1 : xn),

where hei denotes the hidden representation of the
encoder, and xi refers to the input representation of
the ith token in the sentence. Table 2 summarizes
the notation used throughout this paper.

4.2 Sequence Decoder

We employ a sequential decoder (Bahdanau et al.,
2015) as our baseline model with the architecture
shown in Figure 3(a). Our decoder is a (forward)
LSTM, which is conditionally initialized with the
hidden state of the encoder, i.e., we set hd0 = hen
and cd0 = cen, where c is a memory cell:

hd j = LSTM(ey j−1),

where hd j denotes the hidden representation of y j,
ey j are randomly initialized embeddings tuned dur-
ing training, and y0 denotes the start of sequence.
The decoder uses the contextual representation
of the encoder together with the embedding of the
previously predicted token to output the next token
from the vocabulary V :

s j = [hct j ; ey j−1] ∗W2 + b2,

where W2 ∈ R(denc+dy)×|V |, b2 ∈ R|V |, denc and dy
are the dimensions of the encoder hidden unit and
output representation, respectively, and hct j is ob-
tained using an attention mechanism:

where the weight β ji is computed by:

hct j =

β jihei,

n
∑
i=1

β ji =

e f (hd j ,hei )
∑k e f (hd j ,hek )

,

and f is the dot-product function. We obtain the
probability distribution over the output tokens as:

p j = p(y j|Y j−1

, X n

1

1 ) = SOFTMAX(s j)

Description

Symbol
X; Y
wi; yi
X j
i ; Y j
i
ewi ; eyi
¯ewi
eli
dw
dp
dl
dinput

sequence of words; outputs
the ith word; output
word; output sequence from position i to j
random embedding of word wi; of output yi
ﬁxed pretrained embedding of word wi
random embedding for lemma li
dimension of random word embedding
dimension of pretrained word embedding
the dimension of random lemma embedding
input dimension of encoder
denc; ddec hidden dimension of encoder; decoder
matrix of model parameters
vector of model parameters
representation of ith token
hidden representation of ith token
memory cell of ith token in encoder
hidden representation of ith token in decoder
memory cell of ith token in decoder
score vector of jth output in decoder
context representation of jth output
alignment from jth output to ith token
copy score of jth output from ith token
indicates tree structure (e.g. ˆY , ˆyi, ˆs j)
indicates DRS conditions (e.g. ¯Y , ¯yi, ¯s j)
indicates referents (e.g. ˙Y , ˙yi, ˙s j)

Wi
bi
xi
hei
cei
hdi
cdi
s j
hct j
βi
j
oi
j
ˆ
¯
˙

Table 2: Notation used throughout this paper.

4.3 Shallow Structure Decoder

The baseline decoder treats all conditions in a
DRS uniformly and has no means of distin-
guishing between conditions corresponding to to-
the predicate say(e1)
kens in a sentence (e.g.,
refers to the verb said) and semantic relations
(e.g., Cause(e1, x1)). Our second decoder attempts
to take this into account by distinguishing con-
ditions which are local and correspond to words
in a sentence from items which are more global
and express semantic content (see Figure 3(b)).
Speciﬁcally, we model sentence speciﬁc condi-
tions using a copying mechanism, and all other
conditions G which do not correspond to senten-
tial tokens (e.g., thematic roles, rhetorical rela-
tions) with an insertion mechanism.

Each token in a sentence is assigned a copying

score o ji:

o ji = h(cid:62)

d jW3hei,

where subscript ji denotes the ith token at jth time
step, and W3 ∈ Rddec×denc. All other conditions G
are assigned an insertion score:

s j = [hct j ; ey j−1] ∗W4 + b4,

where W4 ∈ R(denc+dy)×|G|, b4 ∈ R|G|, and hct j are
the same with the baseline decoder. We obtain the
probability distribution over output tokens as:

p j = p(y j|Y j−1

, X n

1

1 ) = SOFTMAX([o j; s j])

Figure 3: (a) baseline model; (b) shallow structure model; (c) deep structure model (scoring components
are not displayed): (c.1) predicts DRS structure, (c.2) predicts conditions, and (c.3) predicts referents.
Blue boxes are encoder hidden units, red boxes are decoder LSTM hidden units, green and yellow boxes
represent copy and insertion scores, respectively.

4.4 Deep Structure Decoder

As explained previously, our structure prediction
problem is rather challenging:
the length of a
bracketed DRS is nearly ﬁve times longer than
its corresponding sentence. As shown in Fig-
ure 1, a bracketed DRS, y1, y2, ..., yn consists of
three parts: internal structure ˆY = ˆy1, ˆy2, ... ˆyt (e.g.,
DRS( π1( SDRS(k1(DRS(→(DRS( )DRS( ))) k2(
DRS(→( DRS( ) DRS ( ) ) ) ) ) ) )), condi-
tions ¯Y = ¯y1, ¯y2, ..., ¯yr (e.g., statement, say, Topic),
and referents ˙Y = ˙y1, ˙y2, ..., ˙yv (e.g., x1, e1, π1),
where t + r ∗ 2 + v = n.2

Our third decoder (see Figure 3(c)) ﬁrst predicts
the structural make-up of the DRS, then the con-
ditions, and ﬁnally their referents in an end-to-end
framework. The probability distribution of struc-
tured output Y given natural language input X is
rewritten as:

p(Y |X) = p( ˆY , ¯Y , ˙Y |X)
= ∏ j p( ˆy j| ˆY j−1

1

, X)
× ∏ j p( ¯y j| ¯Y j−1
× ∏ j p( ˙y j| ˙Y j−1
1
, and ˙Y j−1

1

(11)

, ˆY j(cid:48)
, ¯Y j(cid:48)

1 , X)
1 , ˆY j(cid:48)(cid:48)

1 , X)

where ˆY j−1
denote the tree struc-
1
ture, conditions, and referents predicted so far.

, ¯Y j−1
1

1

2Each condition has one and only one right bracket.

ˆY j(cid:48)
1 denotes the structure predicted before condi-
tions ¯y j; ˆY j(cid:48)(cid:48)
1 are the structures and condi-
tions predicted before referents ˙y j. We next dis-
cuss how each decoder is modeled.

1 and ¯Y j(cid:48)

Structure Prediction To model basic DRS
structure we apply the shallow decoder discussed
in Section 4.3 and also shown in Figure 3(c.1). To-
kens in such structures correspond to parent nodes
in other words, they are all inserted
in a tree;
from G, and subsequently predicted tokens are
only scored with the insert score, i.e., ˆsi = si. The
hidden units of the decoder are:

ˆhd j = LSTM(e ˆy j−1),

And the probabilistic distribution over structure
denoting tokens is:

p(y j|Y j−1
1

, X) = SOFTMAX( ˆs j)

Condition Prediction DRS conditions are gen-
erated by taking previously predicted structures
into account, e.g., when “DRS(” or “SDRS(”
are predicted, their conditions will be generated
next. By mapping j to (k, mk), the sequence of
conditions can be rewritten as ¯y1, . . . , ¯y j, . . . , ¯yr =
¯y(1,1), ¯y(1,2), . . . , ¯y(k,mk), . . . , where ¯y(k,mk) is mkth

condition of structure token ˆyk. The correspond-
ing hidden units ˆhdk act as conditional input to the
decoder. Structure denoting tokens (e.g., “DRS(”
or “SDRS(”) are fed into the decoder one by one
to generate the corresponding conditions as:

e ¯y(k,0) = ˆhdk ∗W5 + b5,

where W5 ∈ Rddec×dy and b5 ∈ Rdy. The hidden unit
of the conditions decoder is computed as:

¯hd j = ¯hd(k,mk ) = LSTM(e ¯y(k,mk −1)),

Given hidden unit ¯hd j , we obtain the copy score ¯o j
and insert score ¯s j. The probabilistic distribution
over conditions is:

p( ¯y j| ¯Y j−1
1

, ˆY j(cid:48)

1 , X) = SOFTMAX([ ¯o j; ¯s j])

Referent Prediction Referents are generated
based on the structure and conditions of the
DRS. Each condition has at least one referent.
Similar to condition prediction, the sequence of
referents can be rewritten as ˙y1, . . . , ˙y j, . . . , ˙yv =
˙y(1,1), ˙y(1,2), . . . , ˙y(k,mk), . . . The hidden units of the
conditions decoder are fed into the referent de-
coder e ˙y(k,0) = ¯hdk ∗ W6 + b6, where W6 ∈ Rddec×dy,
b6 ∈ Rdy. The hidden unit of the referent decoder
is computed as:

˙hd j = ˙hd(k,mk ) = LSTM(e ˙y(k,mk −1)),

All referents are inserted from G, given hidden
unit ˙hd j (we only obtain the insert score ˙s j). The
probabilistic distribution over predicates is:

p( ˙y j| ˙Y j−1
1

, ¯Y j(cid:48)

1 , ˆY j(cid:48)(cid:48)

1 , X) = SOFTMAX( ˙s j).

Note that a single LSTM is adopted for structure,
condition and referent prediction. The mathematic
symbols are summarized in Table 2.

4.5 Training

The models are trained to minimize a cross-
entropy loss objective with (cid:96)2 regularization:

L(θ) = −∑
j

log p j +

||θ||2,

λ
2

5 Experimental Setup

Settings Our experiments were carried out on
the GMB following the tree conversion process
discussed in Section 3. We adopted the train-
ing, development, and testing partitions recom-
mended in Bos et al. (2017). We compared the
three models introduced in Section 4, namely the
baseline sequence decoder, the shallow structured
decoder and the deep structure decoder. We used
the same empirical hyper-parameters for all three
models. The dimensions of word and lemma em-
beddings were 64 and 32, respectively. The di-
mensions of hidden vectors were 256 for the en-
coder and 128 for the decoder. The encoder used
two hidden layers, whereas the decoder only one.
The dropout rate was 0.1. Pre-trained word em-
beddings (100 dimensions) were generated with
Word2Vec trained on the AFP portion of the En-
glish Gigaword corpus.3

Evaluation Due to the complex nature of our
structured prediction task, we cannot expect model
output to exactly match the gold standard. For
instance, the numbering of the referents may be
different, but nevertheless valid, or the order of
the children of a tree node (e.g., “DRS(india(x1)
say(e1))” and “DRS(say(e1) india(x1))” are the
same). We thus use F1 instead of exact match ac-
curacy. Speciﬁcally, we report D-match4 a metric
designed to evaluate scoped meaning representa-
tions and released as part of the distribution of the
Parallel Meaning Bank corpus (Abzianidze et al.,
2017). D-match is based on Smatch5, a metric
used to evaluate AMR graphs (Cai and Knight,
2013); it calculates F1 on discourse representa-
tion graphs (DRGs), i.e., triples of nodes, arcs, and
their referents, applying multiple restarts to obtain
a good referent (node) mapping between graphs.

We converted DRSs (predicted and goldstan-
dard) into DRGs following the top-down pro-
cedure described in Algorithm 1.6
ISCONDI-
TION returns true if the child is a condition
india(x1)), where three arcs are created,
(e.g.,
one is connected to a parent node and the other
two are connected to arg1 and arg2, respectively
ISQUANTIFIER returns true if the
(lines 7–12).
child is a quantiﬁer (e.g., π1, ¬ and (cid:50)) and three
arcs are created; one is connected to the parent
node, one to the referent that is created if and only

where θ is the set of parameters, and λ is a regu-
larization hyper-parameter (λ = 10−6). We used
stochastic gradient descent with Adam (Kingma
and Ba, 2014) to adjust the learning rate.

3The models are trained on a single GPU without batches.
4https://github.com/RikVN/D-match
5https://github.com/snowblink14/smatch
6We refer the interested reader to the supplementary ma-

terial for more details.

Algorithm 1 DRS to DRG Conversion

Input: T, tree-like DRS
Output: G, a set of edges
1: nb ← 0; nc ← 0; G ← Ø
2: stack ← []; R ← Ø
3: procedure TRAVELDRS(parent)
4:
5:
6:
7:

stack.append(bnb ); nb ← nb + 1
nodep ← stack.top
for child in parent do

if ISCONDITION(child) then
child.rel
−−−−−→ cnc }
arg1
−−→ child.arg1}
arg2
−−→ child.arg2}

G ← G ∪ {nodep
G ← G ∪ {cnc
G ← G ∪ {cnc
nc ← nc + 1
ADDREFERENT(nodep, child)
else if ISQUANTIFIER(child) then

14:

15:

child.class
−−−−−−→ cnc }
arg1
−−→ child.arg1}
arg1
−−→ bnb+1}

G ← G ∪ {nodep
G ← G ∪ {cnc
G ← G ∪ {cnc
nc ← nc + 1
if ISPROPSEG(child) then

end if
TRAVELDRS(child.nextDRS)

16:
17:
18:
19:
20:
21:
22:
end for
23:
stack.pop()
24:
25: end procedure
26: procedure ADDREFERENT(nodep, child)
27:

end if

ADDREFERENT(nodep, child)

if child.arg1 not in R then
G ← G ∪ {nodep
R ← R ∪ child.arg1

end if
if child.arg2 not in R then
G ← G ∪ {nodep
R ← R ∪ child.arg2

ref−→ child.arg1}

ref−→ child.arg2}

32:
33:
34:
35: end procedure
36: TRAVELDRS(T )
37: return G

end if

8:

9:

10:
11:
12:
13:

28:
29:
30:
31:

if the child is a proposition or segment (e.g., π1
and k1), and one is connected to the next DRS or
SDRS nodes (lines 13–20). The algorithm will re-
cursively travel all DRS or SDRS nodes (line 21).
Furthermore, arcs are introduced to connect DRS
or SDRS nodes to the referents that ﬁrst appear in
a condition (lines 26–35).

When comparing two DRGs, we calculate the
F1 over their arcs. For example consider the two
DRGs (a) and (b) shown in Figure 4. Let {b0 :
b0, x1 : x2, x2 : x3, c0 : c0, c1 : c2, c2 : c3} denote the
node alignment between them. The number of
matching arcs is eight, the number of arcs in the
gold DRG is nine, and the number of arcs in the
predicted DRG is 12. So recall is 8/9, precision is
8/12, and F1 is 76.19.

Figure 4:
dicted DRS (condition names are not shown).

(a) is the gold DRS and (b) is the pre-

6 Results

Table 3 compares our three models on the devel-
opment set. As can be seen, the shallow structured
decoder performs better than the baseline decoder,
and the proposed deep structure decoder outper-
forms both of them. Ablation experiments show
that without pre-trained word embeddings or word
lemma embeddings, the model generally performs
worse. Compared to lemma embeddings, pre-
trained word embeddings contribute more.

Table 4 shows our results on the test set. To
assess the degree to which the various decoders
contribute to DRS parsing, we report results when
predicting the full DRS structure (second block),
when ignoring referents (third block), and when
ignoring both referents and conditions (fourth
block). Overall, we observe that the shallow
structure model improves precision over the base-
line with a slight loss in recall, while the deep
structure model performs best by a large margin.
When referents are not taken into account (com-
pare the second and third blocks in Table 4), per-
formance improves across the board. When con-
ditions are additionally omitted, we observe fur-
ther performance gains. This is hardly surpris-
ing, since errors propagate from one stage to the
next when predicting full DRS structures. Fur-
ther analysis revealed that the parser performs
slightly better on (copy) conditions which cor-
respond to natural language tokens compared to
(insert) conditions (e.g., Topic, Agent) which are
generated from global semantic content (83.22 vs
80.63 F1). The parser is also better on sentences
which do not represent SDRSs (79.12 vs 68.36
F1) which is expected given that they usually cor-
respond to more elaborate structures. We also
found that rhetorical relations (linking segments)
are predicted fairly accurately, especially if they
are frequently attested (e.g., Continuation, Paral-
lel), while the parser has difﬁculty with relations
denoting contrast.

P (%) R (%) F1 (%)
Model
51.35 63.85 56.92
baseline
67.88 63.53 65.63
shallow
deep
79.01 75.65 77.29
deep (–pre)
78.47 73.43 75.87
deep (–pre & lem) 78.21 72.82 75.42

Table 3: GMB development set.

Model

P

baseline 52.21
shallow 66.61
79.27
deep

DRG
R
64.46
63.92
75.88

F1

P

57.69 47.20
65.24 66.05
77.54 82.87

F1

DRG w/o refs
R
58.93
62.93
79.40

P

DRG w/o refs & conds
R
71.80
62.91
88.51

F1
60.91
71.68
91.13

52.42 52.89
64.45 83.30
81.10 93.91

Table 4: GMB test set.

100

)

%

(

1
F

80

60

deep
shallow
baseline

10

15

20

25

30

sentence length

Figure 5: F1 score as a function of sentence length.

Figure 5 shows F1 performance for the three
parsers on sentences of different length. We ob-
serve a similar trend for all models: as sentence
length increases, model performance decreases.
The baseline and shallow models do not perform
well on short sentences which despite containing
fewer words, can still represent complex meaning
which is challenging to capture sequentially. On
the other hand, the performance of the deep model
is relatively stable. LSTMs in this case function
relatively well, as they are faced with the eas-
ier task of predicting meaning in different stages
(starting with a tree skeleton which is progres-
sively reﬁned). We provide examples of model
output in the supplementary material.

7 Related Work

Tree-structured Decoding A few recent ap-
proaches develop structured decoders which make
use of the syntax of meaning representations.
Dong and Lapata (2016) and Alvarez-Melis and
Jaakkola (2017) generate trees in a top-down fash-
ion, while in other work (Xiao et al., 2016; Kr-
ishnamurthy et al., 2017) the decoder generates
from a grammar that guarantees that predicted log-
ical forms are well-typed. In a similar vein, Yin
and Neubig (2017) generate abstract syntax trees
(ASTs) based on the application of production
rules deﬁned by the grammar. Rabinovich et al.
(2017) introduce a modular decoder whose various
components are dynamically composed according
to the generated tree structure. In comparison, our
model does not use grammar information explic-

itly. We ﬁrst decode the structure of the DRS, and
then ﬁll in details pertaining to its semantic con-
tent. Our model is not strictly speaking top-down,
we generate partial trees sequentially, and then ex-
pand non-terminal nodes, ensuring that when we
generate the children of a node, we have already
obtained the structure of the entire tree.

Wide-coverage Semantic Parsing Our model
is trained on the GMB (Bos et al., 2017), a richly
annotated resource in the style of DRT which
provides a unique opportunity for bootstrapping
wide-coverage semantic parsers. Boxer (Bos,
2008) was a precursor to the GMB, the ﬁrst se-
mantic parser of this kind, which deterministically
maps CCG derivations onto formal meaning rep-
resentations. Le and Zuidema (2012) were the ﬁrst
to train a semantic parser on an early release of the
GMB (2,000 documents; Basile et al. 2012), how-
ever, they abandon lambda calculus in favor of a
graph based representation. The latter is closely
related to AMR, a general-purpose meaning rep-
resentation language for broad-coverage text. In
AMR the meaning of a sentence is represented as
a rooted, directed, edge-labeled and leaf-labeled
graph. AMRs do not resemble classical meaning
representations and do not have a model-theoretic
interpretation. However, see Bos (2016) and Artzi
et al. (2015) for translations to ﬁrst-order logic.

8 Conclusions

We introduced a new end-to-end model for open-
domain semantic parsing. Experimental results on
the GMB show that our decoder is able to recover
discourse representation structures to a good de-
gree (77.54 F1), albeit with some simpliﬁcations.
In the future, we plan to model document-level
representations which are more in line with DRT
and the GMB annotations.

Acknowledgments We thank the anonymous
reviewers for their feedback and Johan Bos for an-
swering several questions relating to the GMB. We
gratefully acknowledge the support of the Euro-
pean Research Council (Lapata, Liu; award num-
ber 681760) and the EU H2020 project SUMMA
(Cohen, Liu; grant agreement 688139).

References

Lasha Abzianidze, Johannes Bjerva, Kilian Evang,
Hessel Haagsma, Rik van Noord, Pierre Ludmann,
Duc-Duy Nguyen, and Johan Bos. 2017. The paral-
lel meaning bank: Towards a multilingual corpus of
translations annotated with compositional meaning
representations. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics: Volume 2, Short Papers,
pages 242–247, Valencia, Spain.

David Alvarez-Melis and Tommi S. Jaakkola. 2017.
Tree-structured decoding with doubly-recurrent
In Proceedings of the 5th In-
neural networks.
ternational Conference on Learning Representation
(ICLR), Toulon, France.

Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
Broad-coverage CCG semantic parsing with AMR.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1699–1710, Lisbon, Portugal.

Nicholas Asher and Alex Lascarides. 2003. Logics of

conversation. Cambridge University Press.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In Proceedings of
learning to align and translate.
the 4th International Conference on Learning Rep-
resentations (ICLR), San Diego, California.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186, Soﬁa, Bulgaria.

Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey.

Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of the 2008 Conference
on Semantics in Text Processing, pages 277–286.

Johan Bos. 2015. Open-domain semantic parsing with
In Proceedings of the 20th Nordic Con-
Boxer.
ference of Computational Linguistics (NODALIDA
2015), pages 301–304. Link¨oping University Elec-
tronic Press, Sweden.

Johan Bos. 2016. Expressive power of abstract mean-
Computational Linguistics,

ing representations.
42(3):527–535.

Johan Bos, Valerio Basile, Kilian Evang, Noortje Ven-
huizen, and Johannes Bjerva. 2017. The gronin-
gen meaning bank. In Nancy Ide and James Puste-
jovsky, editors, Handbook of Linguistic Annotation,
volume 2, pages 463–496. Springer.

Jan Buys and Phil Blunsom. 2017. Robust incremen-
tal neural semantic graph parsing. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1215–1226, Vancouver, Canada.

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752, Soﬁa, Bulgaria.

Jianpeng Cheng, Siva Reddy, Vijay Saraswat, and
Mirella Lapata. 2017. Learning structured natural
language representations for semantic parsing.
In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 44–55, Vancouver, Canada.

Stephen Clark and James Curran. 2007. Wide-
coverage efﬁcient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.

Ann Copestake, Dan Flickinger, Carl Pollar, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 2–3(3):281–332.

Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, Christine Pao
David Pallett, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis task:
the atis-3 corpus. In Proceedings of the workshop on
ARPA Human Language Technology, pages 43–48,
Plainsboro, New Jersey.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany.

Hans Kamp and Uwe Reyle. 1993. From discourse to
logic; an introduction to modeltheoretic semantics
of natural language, formal logic and DRT.

Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the 20th National
Conference on Artiﬁcial Intelligence, pages 1062–
1068, Pittsburgh, PA.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 3rd International Conference on Learn-
ing Representations (ICLR), Banff, Canada.

Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 439–446, New York
City, USA.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
In Proceedings of the 54th An-
mantic parsing.
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1341–
1350, Berlin, Germany.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 440–450, Vancouver, Canada.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the 13th National
Conference on Artiﬁcial Intelligence, pages 1050–
1055, Portland, Oregon.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classiﬁcation with probabilistic categorial
grammars. In PProceedings of the 21st Conference
in Uncertainty in Artiﬁcial Intelligence, pages 658–
666, Edinburgh, Scotland.

Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classiﬁcation of
english verbs. Language Resources and Evaluation,
42(1):21–40.

Tom´aˇs Koˇcisk`y, G´abor Melis, Edward Grefenstette,
and
Chris Dyer, Wang Ling, Phil Blunsom,
Karl Moritz Hermann. 2016. Semantic parsing with
In Pro-
semi-supervised sequential autoencoders.
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1078–
1087, Austin, Texas.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1516–1526, Copen-
hagen, Denmark.

Phong Le and Willem Zuidema. 2012. Learning com-
positional semantics for open domain semantic pars-
ing. In Proceedings of the 24th International Con-
ference on Computational Linguistics (COLING),
pages 1535–1552, Mumbai, India.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590–599, Port-
land, Oregon.

Ella Rabinovich, Noam Ordan, and Shuly Wintner.
2017. Found in translation: Reconstructing phylo-
In Pro-
genetic language trees from translations.
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 530–540, Vancouver, Canada.

Mark Steedman. 2001. The Syntactic Process. The

MIT Press.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works.
In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

Noortje J. Venhuizen, Johan Bos, and Harm Brouwer.
2013. Parsimonious semantic representations with
projection pointers. In Proceedings of the 10th In-
ternational Conference on Computational Seman-
tics (IWCS 2013) – Long Papers, pages 252–263,
Potsdam, Germany.

Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 1332–1342,
Beijing, China.

Discourse Representation Structure Parsing

Jiangming Liu

Shay B. Cohen
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Jiangming.Liu@ed.ac.uk, scohen@inf.ed.ac.uk, mlap@inf.ed.ac.uk

Mirella Lapata

Abstract

We introduce an open-domain neural se-
mantic parser which generates formal
meaning representations in the style of
Discourse Representation Theory (DRT;
Kamp and Reyle 1993). We propose a
method which transforms Discourse Rep-
resentation Structures (DRSs) to trees and
develop a structure-aware model which
decomposes the decoding process into
three stages: basic DRS structure pre-
diction, condition prediction (i.e., predi-
cates and relations), and referent predic-
tion (i.e., variables). Experimental results
on the Groningen Meaning Bank (GMB)
show that our model outperforms compet-
itive baselines by a wide margin.

1

Introduction

Semantic parsing is the task of mapping natural
language to machine interpretable meaning repre-
sentations. A variety of meaning representations
have been adopted over the years ranging from
functional query language (FunQL; Kate et al.
2005) to dependency-based compositional seman-
tics (λ-DCS; Liang et al. 2011), lambda calculus
(Zettlemoyer and Collins, 2005), abstract meaning
representations (Banarescu et al., 2013), and min-
imal recursion semantics (Copestake et al., 2005).
Existing semantic parsers are for the most part
data-driven using annotated examples consisting
of utterances and their meaning representations
(Zelle and Mooney, 1996; Wong and Mooney,
2006; Zettlemoyer and Collins, 2005). The suc-
cessful application of encoder-decoder models
(Sutskever et al., 2014; Bahdanau et al., 2015) to
a variety of NLP tasks has provided strong impe-
tus to treat semantic parsing as a sequence trans-
duction problem where an utterance is mapped
to a target meaning representation in string for-
mat (Dong and Lapata, 2016; Jia and Liang, 2016;
Koˇcisk`y et al., 2016). The fact that meaning rep-
resentations do not naturally conform to a lin-

ear ordering has also prompted efforts to develop
recurrent neural network architectures tailored to
tree or graph-structured decoding (Dong and La-
pata, 2016; Cheng et al., 2017; Yin and Neubig,
2017; Alvarez-Melis and Jaakkola, 2017; Rabi-
novich et al., 2017; Buys and Blunsom, 2017)

Most previous work focuses on building seman-
tic parsers for question answering tasks, such as
querying a database to retrieve an answer (Zelle
and Mooney, 1996; Cheng et al., 2017), or con-
versing with a ﬂight booking system (Dahl et al.,
1994). As a result, parsers trained on query-based
datasets work on restricted domains (e.g., restau-
rants, meetings; Wang et al. 2015), with limited
vocabularies, exhibiting limited compositionality,
and a small range of syntactic and semantic con-
structions. In this work, we focus on open-domain
semantic parsing and develop a general-purpose
system which generates formal meaning represen-
tations in the style of Discourse Representation
Theory (DRT; Kamp and Reyle 1993).

DRT is a popular theory of meaning represen-
tation designed to account for a variety of linguis-
tic phenomena, including the interpretation of pro-
nouns and temporal expressions within and across
sentences. Advantageously, it supports meaning
representations for entire texts rather than isolated
sentences which in turn can be translated into ﬁrst-
order logic. The Groningen Meaning Bank (GMB;
Bos et al. 2017) provides a large collection of
English texts annotated with Discourse Represen-
tation Structures (see Figure 1 for an example).
GMB integrates various levels of semantic anno-
tation (e.g., anaphora, named entities, thematic
roles, rhetorical relations) into a uniﬁed formal-
ism providing expressive meaning representations
for open-domain texts.

We treat DRT parsing as a structure prediction
problem. We develop a method to transform DRSs
to tree-based representations which can be fur-
ther linearized to bracketed string format. We ex-
amine a series of encoder-decoder models (Bah-
danau et al., 2015) differing in the way tree-

x1, e1, π1
statement(x1), say(e1), Cause(e1, x1), Topic(e1,π1)

π1:

k1:

x2
thing(x)

⇒

k2:

x6
thing(x6)

⇒

x3, s1, x3, x5, e2
Topic(s1, x3), dead(s1),
man(x3), of(x2, x3),
magazine(x4), on(x5,x4)
vest(x5), wear(e2),
Agent(e2, x2), Theme(e2, x5)

x7, s2, x8, x9, e3
Topic(s2, x7), dead(s2),
man(x7), of(x6, x7),
|x8| = 2, hand(x9),
in(x8, x9), grenade(x8)
carry(e3), Agent(e3, x6),
Theme(e3, x8)

continuation(k1, k2), parallel(k1, k2)

Figure 1: DRT meaning representation for the sentence The statement says each of the dead men wore
magazine vests and carried two hand grenades.

structured logical forms are generated and show
that a structure-aware decoder is paramount to
open-domain semantic parsing. Our proposed
model decomposes the decoding process into three
stages. The ﬁrst stage predicts the structure of
the meaning representation omitting details such
as predicates or variable names. The second stage
ﬁlls in missing predicates and relations (e.g., thing,
Agent) conditioning on the natural language input
and the previously predicted structure. Finally, the
third stage predicts variable names based on the
input and the information generated so far.

Decomposing decoding into these three steps
reduces the complexity of generating logical
forms since the model does not have to predict
deeply nested structures, their variables, and pred-
icates all at once. Moreover, the model is able to
take advantage of the GMB annotations more efﬁ-
ciently, e.g., examples with similar structures can
be effectively used in the ﬁrst stage despite being
very different in their lexical make-up. Finally, a
piecemeal mode of generation yields more accu-
rate predictions; since the output of every decod-
ing step serves as input to the next one, the model
is able to reﬁne its predictions taking progressively
more global context into account. Experimen-
tal results on the GMB show that our three-stage
decoder outperforms a vanilla encoder-decoder
model and a related variant which takes shallow
structure into account, by a wide margin.

Our contributions in this work are three-fold:
an open-domain semantic parser which yields dis-
course representation structures; a novel end-to-
end neural model equipped with a structured de-
coder which decomposes the parsing process into
three stages; a DRS-to-tree conversion method
which transforms DRSs to tree-based representa-
tions allowing for the application of structured de-

coders as well as sequential modeling. We release
our code1 and tree formatted version of the GMB
in the hope of driving further research in open-
domain semantic parsing.

2 Discourse Representation Theory

In this section we provide a brief overview of the
representational semantic formalism used in the
GMB. We refer the reader to Bos et al. (2017) and
Kamp and Reyle (1993) for more details.

Discourse Representation Theory (DRT; Kamp
and Reyle 1993) is a general framework for rep-
resenting the meaning of sentences and discourse
which can handle multiple linguistic phenom-
ena including anaphora, presuppositions, and tem-
poral expressions. The basic meaning-carrying
units in DRT are Discourse Representation Struc-
tures (DRSs), which are recursive formal mean-
ing structures that have a model-theoretic interpre-
tation and can be translated into ﬁrst-order logic
(Kamp and Reyle, 1993). Basic DRSs consist
of discourse referents (e.g., x, y) representing en-
tities in the discourse and discourse conditions
(e.g., man(x), magazine(y)) representing informa-
tion about discourse referents. Following conven-
tions in the DRT literature, we visualize DRSs in
a box-like format (see Figure 1).

GMB adopts a variant of DRT that uses a neo-
Davidsonian analysis of events (Kipper et al.,
2008), i.e., events are ﬁrst-order entities character-
ized by one-place predicate symbols (e.g., say(e1)
in Figure 1). In addition, it follows Projective Dis-
course Representation Theory (PDRT; Venhuizen
et al. 2013) an extension of DRT speciﬁcally de-
veloped to account for the interpretation of pre-
suppositions and related projection phenomena

1https://github.com/EdinburghNLP/EncDecDRSparsing

(e.g., conventional implicatures). In PDRT, each
basic DRS introduces a label, which can be bound
by a pointer indicating the interpretation site of
semantic content. To account for the rhetorical
structure of texts, GMB adopts Segmented Dis-
course Representation Theory (SDRT; Asher and
Lascarides 2003).
In SDRT, discourse segments
are linked with rhetorical relations reﬂecting dif-
ferent characteristics of textual coherence, such as
temporal order and communicative intentions (see
continuation(k1, k2) in Figure 1).

More formally, DRSs are expressions of type
(cid:104)expe(cid:105) (denoting individuals or discourse refer-
ents) and (cid:104)expt(cid:105) (i.e., truth values):

(cid:104)expe(cid:105) ::= (cid:104)re f (cid:105),

(cid:104)expt (cid:105) ::= (cid:104)drs(cid:105)|(cid:104)sdrs(cid:105),

(1)

discourse referents (cid:104)re f (cid:105) are in turn classiﬁed
into six categories, namely common referents (xn),
event referents (en), state referents (sn), segment
referents (kn), proposition referents (πn), and time
referents (tn). (cid:104)drs(cid:105) and (cid:104)sdrs(cid:105) denote basic and
segmented DRSs, respectively:

(cid:104)drs(cid:105) ::= (cid:104)pvar(cid:105) :

((cid:104)pvar(cid:105), (cid:104)re f (cid:105))∗
((cid:104)pvar(cid:105), (cid:104)condition(cid:105))∗ ,

(2)

(cid:104)sdrs(cid:105) ::=

k1 : (cid:104)expt (cid:105), k2 : (cid:104)expt (cid:105)
coo(k1, k2)

|

k1:(cid:104)expt (cid:105)
k2:(cid:104)expt (cid:105)
sub(k1, k2)

,

(3)

Basic DRSs consist of a set of referents ((cid:104)re f (cid:105))
and conditions ((cid:104)condition(cid:105)), whereas segmented
DRSs are recursive structures that combine two
(cid:104)expt(cid:105) by means of coordinating (coo) or subor-
dinating (sub) relations. DRS conditions can be
basic or complex:

(cid:104)condition(cid:105) ::= (cid:104)basic(cid:105)|(cid:104)complex(cid:105),

(4)

Basic conditions express properties of discourse
referents or relations between them:

(cid:104)basic(cid:105) ::= (cid:104)sym1(cid:105)((cid:104)expe(cid:105)) | (cid:104)sym2(cid:105)((cid:104)expe(cid:105), (cid:104)expe(cid:105))

| (cid:104)expe(cid:105) = (cid:104)expe(cid:105) | (cid:104)expe(cid:105) = (cid:104)num(cid:105)
| timex((cid:104)expe(cid:105), (cid:104)sym0(cid:105))
| named((cid:104)expe(cid:105), (cid:104)sym0(cid:105), class).

(5)

where (cid:104)symn(cid:105) denotes n-place predicates, (cid:104)num(cid:105)
denotes cardinal numbers, timex expresses tem-
timex(x7, 2005) denotes
poral information (e.g.,
the year 2005), and class refers to named entity
classes (e.g., location).

Complex conditions are unary or binary. Unary
conditions have one DRS as argument and rep-
resent negation (¬) and modal operators express-
ing necessity ((cid:50)) and possibility ((cid:51)). Condition

sections
00-99
20-99
10-19
00-09

# doc
10,000
7,970
1,038
992

# sent
62,010
49,411
6,483
6,116

# token
1,354,149
1,078,953
142,344
132,852

avg
21.84
21.83
21.95
21.72

Table 1: Statistics on the GMB (avg denotes the
average number of tokens per sentence).

(cid:104)re f (cid:105) : (cid:104)expt(cid:105) represents verbs with propositional
content (e.g., factive verbs). Binary conditions are
conditional statements (→) and questions.

(cid:104)complex(cid:105) ::= (cid:104)unary(cid:105) | (cid:104)binary(cid:105),

(6)

(cid:104)unary(cid:105) ::= ¬(cid:104)expt (cid:105) | (cid:50)(cid:104)expt (cid:105)|(cid:51)(cid:104)expt (cid:105)|(cid:104)re f (cid:105) : (cid:104)expt (cid:105)

(cid:104)binary(cid:105) ::=(cid:104)expt (cid:105)→(cid:104)expt (cid:105)|(cid:104)expt (cid:105)∨(cid:104)expt (cid:105)|(cid:104)expt (cid:105)?(cid:104)expt (cid:105)

3 The Groningen Meaning Bank Corpus

Corpus Creation DRSs in GMB were obtained
from Boxer (Bos, 2008, 2015), and then reﬁned
using expert linguists and crowdsourcing meth-
ods. Boxer constructs DRSs based on a pipeline of
tools involving POS-tagging, named entity recog-
nition, and parsing. Speciﬁcally, it relies on the
syntactic analysis of the C&C parser (Clark and
Curran, 2007), a general-purpose parser using the
framework of Combinatory Categorial Grammar
(CCG; Steedman 2001). DRSs are obtained from
CCG parses, with semantic composition being
guided by the CCG syntactic derivation.

Documents in the GMB were collected from
a variety of sources including Voice of America
(a newspaper published by the US Federal Gov-
ernment), the Open American National Corpus,
Aesop’s fables, humorous stories and jokes, and
country descriptions from the CIA World Fact-
book. The dataset consists of 10,000 documents
each annotated with a DRS. Various statistics on
the GMB are shown in Table 1. Bos et al. (2017)
recommend sections 20–99 for training, 10–19 for
tuning, and 00–09 for testing.

DRS-to-Tree Conversion As mentioned earlier,
DRSs in the GMB are displayed in a box-like for-
mat which is intuitive and easy to read but not par-
ticularly amenable to structure modeling. In this
section we discuss how DRSs were post-processed
and simpliﬁed into a tree-based format, which
served as input to our models.

The GMB provides DRS annotations per-
document. Our initial efforts have focused on
sentence-level DRS parsing which is undoubtedly

a necessary ﬁrst step for more global semantic rep-
It is relatively, straightforward to
resentations.
obtain sentence-level DRSs from document-level
annotations since referents and conditions are in-
dexed to tokens. We match each sentence in a doc-
ument with the DRS whose content bears the same
indices as the tokens occurring in the sentence.
This matching process yields 52,268 sentences for
training (sections 20–99), 5,172 sentences for de-
velopment (sections 10–19), (development), and
5,440 sentences for testing (sections 00–09).

In order to simplify the representation, we omit
referents in the top part of the DRS (e.g., x1, e1
and π1 in Figure 1) but preserve them in condi-
tions without any information loss. Also we ignore
pointers to DRSs since this information is implic-
itly captured through the typing and co-indexing
of referents. Deﬁnition (1) is simpliﬁed to:

(cid:104)drs(cid:105) ::= DRS((cid:104)condition(cid:105)∗),

(7)

where DRS() denotes a basic DRS. We also mod-
ify discourse referents to SDRSs (e.g., k1, k2 in
Figure 1) which we regard as elements bearing
scope over expressions (cid:104)expt(cid:105) and add a 2-place
predicate (cid:104)sym2(cid:105) to describe the discourse relation
between them. So, deﬁnition (3) becomes:

(cid:104)sdrs(cid:105) ::=SDRS(((cid:104)re f (cid:105)((cid:104)expt (cid:105)))∗
((cid:104)sym2(cid:105)((cid:104)re f (cid:105), (cid:104)re f (cid:105)))∗),

(8)

where SDRS() denotes a segmented DRS, and
(cid:104)re f (cid:105) are segment referents.

We treat cardinal numbers (cid:104)num(cid:105) and (cid:104)sym0(cid:105)
in relation timex as constants. We introduce the
binary predicate “card” to represent cardinality
(e.g., |x8| = 2 is card(x8, NUM)). We also sim-
plify (cid:104)expe(cid:105) = (cid:104)expe(cid:105) to eq((cid:104)expe(cid:105), (cid:104)expe(cid:105)) using
the binary relation “eq” (e.g., x1 = x2 becomes
eq(x1, x2)). Moreover, we ignore class in named
and transform named((cid:104)expe(cid:105), (cid:104)sym0(cid:105), class) into
(cid:104)sym1(cid:105)((cid:104)expe(cid:105)) (e.g., named(x2, mongolia, geo)
becomes mongolia(x2)). Consequently, basic con-
ditions (see deﬁnition (5)) are simpliﬁed to:

(cid:104)basic(cid:105) ::= (cid:104)sym1(cid:105)((cid:104)expe(cid:105))|(cid:104)sym2(cid:105)((cid:104)expe(cid:105), (cid:104)expe(cid:105))

(9)

Analogously, we treat unary and binary conditions
as scoped functions, and deﬁnition (6) becomes:

(cid:104)unary(cid:105) ::= ¬ | (cid:50) | (cid:51) | (cid:104)re f (cid:105)((cid:104)expt (cid:105))
(cid:104)binary(cid:105) ::= → | ∨ | ?((cid:104)expt (cid:105), (cid:104)expt (cid:105)),

(10)

Following the transformations described above,
the DRS in Figure 1 is converted into the tree in

Figure 2: Tree-based representation (top) of the
DRS in Figure 1 and its linearization (bottom).

Figure 2, which can be subsequently linearized
into a PTB-style bracketed sequence. It is impor-
tant to note that the conversion does not diminish
the complexity of DRSs. The average tree width
in the training set is 10.39 and tree depth is 4.64.

4 Semantic Parsing Models

We present below three encoder-decoder models
which are increasingly aware of the structure of
the DRT meaning representations. The models
take as input a natural language sentence X repre-
sented as w1, w2,. . . , wn, and generate a sequence
Y = (y1, y2, ..., ym), which is a linearized tree (see
Figure 2 bottom), where n is the length of the
sentence, and m the length of the generated DRS
sequence. We aim to estimate p(Y |X), the con-
ditional probability of the semantic parse tree Y
given natural language input X:

p(Y |X) = ∏
j

p(y j|Y j−1
1

, X n
1 )

4.1 Encoder

An encoder is used to represent the natural lan-
guage input X into vector representations. Each
token in a sentence is represented by a vec-
tor xk which is the concatenation of randomly
initialized embeddings ewi, pre-trained word em-
beddings ¯ewi, and lemma embeddings eli: xk =
tanh([ewi; ¯ewi; eli] ∗ W1 + b1), where W1 ∈ RD and
D is a shorthand for (dw + dp + dl) × dinput (sub-
scripts w, p, and l denote the dimensions of word
embeddings, pre-trained embeddings, and lemma
embeddings, respectively); b1 ∈ Rdinput and the
symbol ; denotes concatenation. Embeddings ewi

and eli are randomly initialized and tuned during
training, while ¯ewi are ﬁxed.

We use a bidirectional recurrent neural network
with long short-term memory units (bi-LSTM;
Hochreiter and Schmidhuber 1997) to encode nat-
ural language sentences:

[he1 : hen] = bi-LSTM(x1 : xn),

where hei denotes the hidden representation of the
encoder, and xi refers to the input representation of
the ith token in the sentence. Table 2 summarizes
the notation used throughout this paper.

4.2 Sequence Decoder

We employ a sequential decoder (Bahdanau et al.,
2015) as our baseline model with the architecture
shown in Figure 3(a). Our decoder is a (forward)
LSTM, which is conditionally initialized with the
hidden state of the encoder, i.e., we set hd0 = hen
and cd0 = cen, where c is a memory cell:

hd j = LSTM(ey j−1),

where hd j denotes the hidden representation of y j,
ey j are randomly initialized embeddings tuned dur-
ing training, and y0 denotes the start of sequence.
The decoder uses the contextual representation
of the encoder together with the embedding of the
previously predicted token to output the next token
from the vocabulary V :

s j = [hct j ; ey j−1] ∗W2 + b2,

where W2 ∈ R(denc+dy)×|V |, b2 ∈ R|V |, denc and dy
are the dimensions of the encoder hidden unit and
output representation, respectively, and hct j is ob-
tained using an attention mechanism:

where the weight β ji is computed by:

hct j =

β jihei,

n
∑
i=1

β ji =

e f (hd j ,hei )
∑k e f (hd j ,hek )

,

and f is the dot-product function. We obtain the
probability distribution over the output tokens as:

p j = p(y j|Y j−1

, X n

1

1 ) = SOFTMAX(s j)

Description

Symbol
X; Y
wi; yi
X j
i ; Y j
i
ewi ; eyi
¯ewi
eli
dw
dp
dl
dinput

sequence of words; outputs
the ith word; output
word; output sequence from position i to j
random embedding of word wi; of output yi
ﬁxed pretrained embedding of word wi
random embedding for lemma li
dimension of random word embedding
dimension of pretrained word embedding
the dimension of random lemma embedding
input dimension of encoder
denc; ddec hidden dimension of encoder; decoder
matrix of model parameters
vector of model parameters
representation of ith token
hidden representation of ith token
memory cell of ith token in encoder
hidden representation of ith token in decoder
memory cell of ith token in decoder
score vector of jth output in decoder
context representation of jth output
alignment from jth output to ith token
copy score of jth output from ith token
indicates tree structure (e.g. ˆY , ˆyi, ˆs j)
indicates DRS conditions (e.g. ¯Y , ¯yi, ¯s j)
indicates referents (e.g. ˙Y , ˙yi, ˙s j)

Wi
bi
xi
hei
cei
hdi
cdi
s j
hct j
βi
j
oi
j
ˆ
¯
˙

Table 2: Notation used throughout this paper.

4.3 Shallow Structure Decoder

The baseline decoder treats all conditions in a
DRS uniformly and has no means of distin-
guishing between conditions corresponding to to-
the predicate say(e1)
kens in a sentence (e.g.,
refers to the verb said) and semantic relations
(e.g., Cause(e1, x1)). Our second decoder attempts
to take this into account by distinguishing con-
ditions which are local and correspond to words
in a sentence from items which are more global
and express semantic content (see Figure 3(b)).
Speciﬁcally, we model sentence speciﬁc condi-
tions using a copying mechanism, and all other
conditions G which do not correspond to senten-
tial tokens (e.g., thematic roles, rhetorical rela-
tions) with an insertion mechanism.

Each token in a sentence is assigned a copying

score o ji:

o ji = h(cid:62)

d jW3hei,

where subscript ji denotes the ith token at jth time
step, and W3 ∈ Rddec×denc. All other conditions G
are assigned an insertion score:

s j = [hct j ; ey j−1] ∗W4 + b4,

where W4 ∈ R(denc+dy)×|G|, b4 ∈ R|G|, and hct j are
the same with the baseline decoder. We obtain the
probability distribution over output tokens as:

p j = p(y j|Y j−1

, X n

1

1 ) = SOFTMAX([o j; s j])

Figure 3: (a) baseline model; (b) shallow structure model; (c) deep structure model (scoring components
are not displayed): (c.1) predicts DRS structure, (c.2) predicts conditions, and (c.3) predicts referents.
Blue boxes are encoder hidden units, red boxes are decoder LSTM hidden units, green and yellow boxes
represent copy and insertion scores, respectively.

4.4 Deep Structure Decoder

As explained previously, our structure prediction
problem is rather challenging:
the length of a
bracketed DRS is nearly ﬁve times longer than
its corresponding sentence. As shown in Fig-
ure 1, a bracketed DRS, y1, y2, ..., yn consists of
three parts: internal structure ˆY = ˆy1, ˆy2, ... ˆyt (e.g.,
DRS( π1( SDRS(k1(DRS(→(DRS( )DRS( ))) k2(
DRS(→( DRS( ) DRS ( ) ) ) ) ) ) )), condi-
tions ¯Y = ¯y1, ¯y2, ..., ¯yr (e.g., statement, say, Topic),
and referents ˙Y = ˙y1, ˙y2, ..., ˙yv (e.g., x1, e1, π1),
where t + r ∗ 2 + v = n.2

Our third decoder (see Figure 3(c)) ﬁrst predicts
the structural make-up of the DRS, then the con-
ditions, and ﬁnally their referents in an end-to-end
framework. The probability distribution of struc-
tured output Y given natural language input X is
rewritten as:

p(Y |X) = p( ˆY , ¯Y , ˙Y |X)
= ∏ j p( ˆy j| ˆY j−1

1

, X)
× ∏ j p( ¯y j| ¯Y j−1
× ∏ j p( ˙y j| ˙Y j−1
1
, and ˙Y j−1

1

(11)

, ˆY j(cid:48)
, ¯Y j(cid:48)

1 , X)
1 , ˆY j(cid:48)(cid:48)

1 , X)

where ˆY j−1
denote the tree struc-
1
ture, conditions, and referents predicted so far.

, ¯Y j−1
1

1

2Each condition has one and only one right bracket.

ˆY j(cid:48)
1 denotes the structure predicted before condi-
tions ¯y j; ˆY j(cid:48)(cid:48)
1 are the structures and condi-
tions predicted before referents ˙y j. We next dis-
cuss how each decoder is modeled.

1 and ¯Y j(cid:48)

Structure Prediction To model basic DRS
structure we apply the shallow decoder discussed
in Section 4.3 and also shown in Figure 3(c.1). To-
kens in such structures correspond to parent nodes
in other words, they are all inserted
in a tree;
from G, and subsequently predicted tokens are
only scored with the insert score, i.e., ˆsi = si. The
hidden units of the decoder are:

ˆhd j = LSTM(e ˆy j−1),

And the probabilistic distribution over structure
denoting tokens is:

p(y j|Y j−1
1

, X) = SOFTMAX( ˆs j)

Condition Prediction DRS conditions are gen-
erated by taking previously predicted structures
into account, e.g., when “DRS(” or “SDRS(”
are predicted, their conditions will be generated
next. By mapping j to (k, mk), the sequence of
conditions can be rewritten as ¯y1, . . . , ¯y j, . . . , ¯yr =
¯y(1,1), ¯y(1,2), . . . , ¯y(k,mk), . . . , where ¯y(k,mk) is mkth

condition of structure token ˆyk. The correspond-
ing hidden units ˆhdk act as conditional input to the
decoder. Structure denoting tokens (e.g., “DRS(”
or “SDRS(”) are fed into the decoder one by one
to generate the corresponding conditions as:

e ¯y(k,0) = ˆhdk ∗W5 + b5,

where W5 ∈ Rddec×dy and b5 ∈ Rdy. The hidden unit
of the conditions decoder is computed as:

¯hd j = ¯hd(k,mk ) = LSTM(e ¯y(k,mk −1)),

Given hidden unit ¯hd j , we obtain the copy score ¯o j
and insert score ¯s j. The probabilistic distribution
over conditions is:

p( ¯y j| ¯Y j−1
1

, ˆY j(cid:48)

1 , X) = SOFTMAX([ ¯o j; ¯s j])

Referent Prediction Referents are generated
based on the structure and conditions of the
DRS. Each condition has at least one referent.
Similar to condition prediction, the sequence of
referents can be rewritten as ˙y1, . . . , ˙y j, . . . , ˙yv =
˙y(1,1), ˙y(1,2), . . . , ˙y(k,mk), . . . The hidden units of the
conditions decoder are fed into the referent de-
coder e ˙y(k,0) = ¯hdk ∗ W6 + b6, where W6 ∈ Rddec×dy,
b6 ∈ Rdy. The hidden unit of the referent decoder
is computed as:

˙hd j = ˙hd(k,mk ) = LSTM(e ˙y(k,mk −1)),

All referents are inserted from G, given hidden
unit ˙hd j (we only obtain the insert score ˙s j). The
probabilistic distribution over predicates is:

p( ˙y j| ˙Y j−1
1

, ¯Y j(cid:48)

1 , ˆY j(cid:48)(cid:48)

1 , X) = SOFTMAX( ˙s j).

Note that a single LSTM is adopted for structure,
condition and referent prediction. The mathematic
symbols are summarized in Table 2.

4.5 Training

The models are trained to minimize a cross-
entropy loss objective with (cid:96)2 regularization:

L(θ) = −∑
j

log p j +

||θ||2,

λ
2

5 Experimental Setup

Settings Our experiments were carried out on
the GMB following the tree conversion process
discussed in Section 3. We adopted the train-
ing, development, and testing partitions recom-
mended in Bos et al. (2017). We compared the
three models introduced in Section 4, namely the
baseline sequence decoder, the shallow structured
decoder and the deep structure decoder. We used
the same empirical hyper-parameters for all three
models. The dimensions of word and lemma em-
beddings were 64 and 32, respectively. The di-
mensions of hidden vectors were 256 for the en-
coder and 128 for the decoder. The encoder used
two hidden layers, whereas the decoder only one.
The dropout rate was 0.1. Pre-trained word em-
beddings (100 dimensions) were generated with
Word2Vec trained on the AFP portion of the En-
glish Gigaword corpus.3

Evaluation Due to the complex nature of our
structured prediction task, we cannot expect model
output to exactly match the gold standard. For
instance, the numbering of the referents may be
different, but nevertheless valid, or the order of
the children of a tree node (e.g., “DRS(india(x1)
say(e1))” and “DRS(say(e1) india(x1))” are the
same). We thus use F1 instead of exact match ac-
curacy. Speciﬁcally, we report D-match4 a metric
designed to evaluate scoped meaning representa-
tions and released as part of the distribution of the
Parallel Meaning Bank corpus (Abzianidze et al.,
2017). D-match is based on Smatch5, a metric
used to evaluate AMR graphs (Cai and Knight,
2013); it calculates F1 on discourse representa-
tion graphs (DRGs), i.e., triples of nodes, arcs, and
their referents, applying multiple restarts to obtain
a good referent (node) mapping between graphs.

We converted DRSs (predicted and goldstan-
dard) into DRGs following the top-down pro-
cedure described in Algorithm 1.6
ISCONDI-
TION returns true if the child is a condition
india(x1)), where three arcs are created,
(e.g.,
one is connected to a parent node and the other
two are connected to arg1 and arg2, respectively
ISQUANTIFIER returns true if the
(lines 7–12).
child is a quantiﬁer (e.g., π1, ¬ and (cid:50)) and three
arcs are created; one is connected to the parent
node, one to the referent that is created if and only

where θ is the set of parameters, and λ is a regu-
larization hyper-parameter (λ = 10−6). We used
stochastic gradient descent with Adam (Kingma
and Ba, 2014) to adjust the learning rate.

3The models are trained on a single GPU without batches.
4https://github.com/RikVN/D-match
5https://github.com/snowblink14/smatch
6We refer the interested reader to the supplementary ma-

terial for more details.

Algorithm 1 DRS to DRG Conversion

Input: T, tree-like DRS
Output: G, a set of edges
1: nb ← 0; nc ← 0; G ← Ø
2: stack ← []; R ← Ø
3: procedure TRAVELDRS(parent)
4:
5:
6:
7:

stack.append(bnb ); nb ← nb + 1
nodep ← stack.top
for child in parent do

if ISCONDITION(child) then
child.rel
−−−−−→ cnc }
arg1
−−→ child.arg1}
arg2
−−→ child.arg2}

G ← G ∪ {nodep
G ← G ∪ {cnc
G ← G ∪ {cnc
nc ← nc + 1
ADDREFERENT(nodep, child)
else if ISQUANTIFIER(child) then

15:

14:

child.class
−−−−−−→ cnc }
arg1
−−→ child.arg1}
arg1
−−→ bnb+1}

G ← G ∪ {nodep
G ← G ∪ {cnc
G ← G ∪ {cnc
nc ← nc + 1
if ISPROPSEG(child) then

end if
TRAVELDRS(child.nextDRS)

16:
17:
18:
19:
20:
21:
22:
end for
23:
stack.pop()
24:
25: end procedure
26: procedure ADDREFERENT(nodep, child)
27:

end if

ADDREFERENT(nodep, child)

if child.arg1 not in R then
G ← G ∪ {nodep
R ← R ∪ child.arg1

end if
if child.arg2 not in R then
G ← G ∪ {nodep
R ← R ∪ child.arg2

ref−→ child.arg1}

ref−→ child.arg2}

32:
33:
34:
35: end procedure
36: TRAVELDRS(T )
37: return G

end if

8:

9:

10:
11:
12:
13:

28:
29:
30:
31:

if the child is a proposition or segment (e.g., π1
and k1), and one is connected to the next DRS or
SDRS nodes (lines 13–20). The algorithm will re-
cursively travel all DRS or SDRS nodes (line 21).
Furthermore, arcs are introduced to connect DRS
or SDRS nodes to the referents that ﬁrst appear in
a condition (lines 26–35).

When comparing two DRGs, we calculate the
F1 over their arcs. For example consider the two
DRGs (a) and (b) shown in Figure 4. Let {b0 :
b0, x1 : x2, x2 : x3, c0 : c0, c1 : c2, c2 : c3} denote the
node alignment between them. The number of
matching arcs is eight, the number of arcs in the
gold DRG is nine, and the number of arcs in the
predicted DRG is 12. So recall is 8/9, precision is
8/12, and F1 is 76.19.

Figure 4:
dicted DRS (condition names are not shown).

(a) is the gold DRS and (b) is the pre-

6 Results

Table 3 compares our three models on the devel-
opment set. As can be seen, the shallow structured
decoder performs better than the baseline decoder,
and the proposed deep structure decoder outper-
forms both of them. Ablation experiments show
that without pre-trained word embeddings or word
lemma embeddings, the model generally performs
worse. Compared to lemma embeddings, pre-
trained word embeddings contribute more.

Table 4 shows our results on the test set. To
assess the degree to which the various decoders
contribute to DRS parsing, we report results when
predicting the full DRS structure (second block),
when ignoring referents (third block), and when
ignoring both referents and conditions (fourth
block). Overall, we observe that the shallow
structure model improves precision over the base-
line with a slight loss in recall, while the deep
structure model performs best by a large margin.
When referents are not taken into account (com-
pare the second and third blocks in Table 4), per-
formance improves across the board. When con-
ditions are additionally omitted, we observe fur-
ther performance gains. This is hardly surpris-
ing, since errors propagate from one stage to the
next when predicting full DRS structures. Fur-
ther analysis revealed that the parser performs
slightly better on (copy) conditions which cor-
respond to natural language tokens compared to
(insert) conditions (e.g., Topic, Agent) which are
generated from global semantic content (83.22 vs
80.63 F1). The parser is also better on sentences
which do not represent SDRSs (79.12 vs 68.36
F1) which is expected given that they usually cor-
respond to more elaborate structures. We also
found that rhetorical relations (linking segments)
are predicted fairly accurately, especially if they
are frequently attested (e.g., Continuation, Paral-
lel), while the parser has difﬁculty with relations
denoting contrast.

P (%) R (%) F1 (%)
Model
51.35 63.85 56.92
baseline
67.88 63.53 65.63
shallow
deep
79.01 75.65 77.29
deep (–pre)
78.47 73.43 75.87
deep (–pre & lem) 78.21 72.82 75.42

Table 3: GMB development set.

Model

P

baseline 52.21
shallow 66.61
79.27
deep

DRG
R
64.46
63.92
75.88

F1

P

57.69 47.20
65.24 66.05
77.54 82.87

F1

DRG w/o refs
R
58.93
62.93
79.40

P

DRG w/o refs & conds
R
71.80
62.91
88.51

F1
60.91
71.68
91.13

52.42 52.89
64.45 83.30
81.10 93.91

Table 4: GMB test set.

100

)

%

(

1
F

80

60

deep
shallow
baseline

10

15

20

25

30

sentence length

Figure 5: F1 score as a function of sentence length.

Figure 5 shows F1 performance for the three
parsers on sentences of different length. We ob-
serve a similar trend for all models: as sentence
length increases, model performance decreases.
The baseline and shallow models do not perform
well on short sentences which despite containing
fewer words, can still represent complex meaning
which is challenging to capture sequentially. On
the other hand, the performance of the deep model
is relatively stable. LSTMs in this case function
relatively well, as they are faced with the eas-
ier task of predicting meaning in different stages
(starting with a tree skeleton which is progres-
sively reﬁned). We provide examples of model
output in the supplementary material.

7 Related Work

Tree-structured Decoding A few recent ap-
proaches develop structured decoders which make
use of the syntax of meaning representations.
Dong and Lapata (2016) and Alvarez-Melis and
Jaakkola (2017) generate trees in a top-down fash-
ion, while in other work (Xiao et al., 2016; Kr-
ishnamurthy et al., 2017) the decoder generates
from a grammar that guarantees that predicted log-
ical forms are well-typed. In a similar vein, Yin
and Neubig (2017) generate abstract syntax trees
(ASTs) based on the application of production
rules deﬁned by the grammar. Rabinovich et al.
(2017) introduce a modular decoder whose various
components are dynamically composed according
to the generated tree structure. In comparison, our
model does not use grammar information explic-

itly. We ﬁrst decode the structure of the DRS, and
then ﬁll in details pertaining to its semantic con-
tent. Our model is not strictly speaking top-down,
we generate partial trees sequentially, and then ex-
pand non-terminal nodes, ensuring that when we
generate the children of a node, we have already
obtained the structure of the entire tree.

Wide-coverage Semantic Parsing Our model
is trained on the GMB (Bos et al., 2017), a richly
annotated resource in the style of DRT which
provides a unique opportunity for bootstrapping
wide-coverage semantic parsers. Boxer (Bos,
2008) was a precursor to the GMB, the ﬁrst se-
mantic parser of this kind, which deterministically
maps CCG derivations onto formal meaning rep-
resentations. Le and Zuidema (2012) were the ﬁrst
to train a semantic parser on an early release of the
GMB (2,000 documents; Basile et al. 2012), how-
ever, they abandon lambda calculus in favor of a
graph based representation. The latter is closely
related to AMR, a general-purpose meaning rep-
resentation language for broad-coverage text. In
AMR the meaning of a sentence is represented as
a rooted, directed, edge-labeled and leaf-labeled
graph. AMRs do not resemble classical meaning
representations and do not have a model-theoretic
interpretation. However, see Bos (2016) and Artzi
et al. (2015) for translations to ﬁrst-order logic.

8 Conclusions

We introduced a new end-to-end model for open-
domain semantic parsing. Experimental results on
the GMB show that our decoder is able to recover
discourse representation structures to a good de-
gree (77.54 F1), albeit with some simpliﬁcations.
In the future, we plan to model document-level
representations which are more in line with DRT
and the GMB annotations.

Acknowledgments We thank the anonymous
reviewers for their feedback and Johan Bos for an-
swering several questions relating to the GMB. We
gratefully acknowledge the support of the Euro-
pean Research Council (Lapata, Liu; award num-
ber 681760) and the EU H2020 project SUMMA
(Cohen, Liu; grant agreement 688139).

References

Lasha Abzianidze, Johannes Bjerva, Kilian Evang,
Hessel Haagsma, Rik van Noord, Pierre Ludmann,
Duc-Duy Nguyen, and Johan Bos. 2017. The paral-
lel meaning bank: Towards a multilingual corpus of
translations annotated with compositional meaning
representations. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics: Volume 2, Short Papers,
pages 242–247, Valencia, Spain.

David Alvarez-Melis and Tommi S. Jaakkola. 2017.
Tree-structured decoding with doubly-recurrent
In Proceedings of the 5th In-
neural networks.
ternational Conference on Learning Representation
(ICLR), Toulon, France.

Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
Broad-coverage CCG semantic parsing with AMR.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1699–1710, Lisbon, Portugal.

Nicholas Asher and Alex Lascarides. 2003. Logics of

conversation. Cambridge University Press.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In Proceedings of
learning to align and translate.
the 4th International Conference on Learning Rep-
resentations (ICLR), San Diego, California.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186, Soﬁa, Bulgaria.

Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey.

Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of the 2008 Conference
on Semantics in Text Processing, pages 277–286.

Johan Bos. 2015. Open-domain semantic parsing with
In Proceedings of the 20th Nordic Con-
Boxer.
ference of Computational Linguistics (NODALIDA
2015), pages 301–304. Link¨oping University Elec-
tronic Press, Sweden.

Johan Bos. 2016. Expressive power of abstract mean-
Computational Linguistics,

ing representations.
42(3):527–535.

Johan Bos, Valerio Basile, Kilian Evang, Noortje Ven-
huizen, and Johannes Bjerva. 2017. The gronin-
gen meaning bank. In Nancy Ide and James Puste-
jovsky, editors, Handbook of Linguistic Annotation,
volume 2, pages 463–496. Springer.

Jan Buys and Phil Blunsom. 2017. Robust incremen-
tal neural semantic graph parsing. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1215–1226, Vancouver, Canada.

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752, Soﬁa, Bulgaria.

Jianpeng Cheng, Siva Reddy, Vijay Saraswat, and
Mirella Lapata. 2017. Learning structured natural
language representations for semantic parsing.
In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 44–55, Vancouver, Canada.

Stephen Clark and James Curran. 2007. Wide-
coverage efﬁcient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.

Ann Copestake, Dan Flickinger, Carl Pollar, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 2–3(3):281–332.

Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, Christine Pao
David Pallett, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the atis task:
the atis-3 corpus. In Proceedings of the workshop on
ARPA Human Language Technology, pages 43–48,
Plainsboro, New Jersey.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany.

Hans Kamp and Uwe Reyle. 1993. From discourse to
logic; an introduction to modeltheoretic semantics
of natural language, formal logic and DRT.

Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the 20th National
Conference on Artiﬁcial Intelligence, pages 1062–
1068, Pittsburgh, PA.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 3rd International Conference on Learn-
ing Representations (ICLR), Banff, Canada.

Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 439–446, New York
City, USA.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
In Proceedings of the 54th An-
mantic parsing.
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1341–
1350, Berlin, Germany.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 440–450, Vancouver, Canada.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the 13th National
Conference on Artiﬁcial Intelligence, pages 1050–
1055, Portland, Oregon.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classiﬁcation with probabilistic categorial
grammars. In PProceedings of the 21st Conference
in Uncertainty in Artiﬁcial Intelligence, pages 658–
666, Edinburgh, Scotland.

Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classiﬁcation of
english verbs. Language Resources and Evaluation,
42(1):21–40.

Tom´aˇs Koˇcisk`y, G´abor Melis, Edward Grefenstette,
and
Chris Dyer, Wang Ling, Phil Blunsom,
Karl Moritz Hermann. 2016. Semantic parsing with
In Pro-
semi-supervised sequential autoencoders.
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1078–
1087, Austin, Texas.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1516–1526, Copen-
hagen, Denmark.

Phong Le and Willem Zuidema. 2012. Learning com-
positional semantics for open domain semantic pars-
ing. In Proceedings of the 24th International Con-
ference on Computational Linguistics (COLING),
pages 1535–1552, Mumbai, India.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590–599, Port-
land, Oregon.

Ella Rabinovich, Noam Ordan, and Shuly Wintner.
2017. Found in translation: Reconstructing phylo-
In Pro-
genetic language trees from translations.
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 530–540, Vancouver, Canada.

Mark Steedman. 2001. The Syntactic Process. The

MIT Press.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works.
In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

Noortje J. Venhuizen, Johan Bos, and Harm Brouwer.
2013. Parsimonious semantic representations with
projection pointers. In Proceedings of the 10th In-
ternational Conference on Computational Seman-
tics (IWCS 2013) – Long Papers, pages 252–263,
Potsdam, Germany.

Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 1332–1342,
Beijing, China.

