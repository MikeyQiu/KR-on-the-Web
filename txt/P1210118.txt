DICOD: Distributed Convolutional Coordinate Descent for
Convolutional Sparse Coding

Moreau Thomas 1 Oudre Laurent 2 Vayatis Nicolas 1

8
1
0
2
 
y
a
M
 
3
1
 
 
]

G
L
.
s
c
[
 
 
2
v
7
8
0
0
1
.
5
0
7
1
:
v
i
X
r
a

Abstract

In this paper, we introduce DICOD, a convolu-
tional sparse coding algorithm which builds shift
invariant representations for long signals. This
algorithm is designed to run in a distributed set-
ting, with local message passing, making it com-
It is based on coordinate
munication efﬁcient.
descent and uses locally greedy updates which
accelerate the resolution compared to greedy co-
ordinate selection. We prove the convergence
of this algorithm and highlight its computational
speed-up which is super-linear in the number of
cores used. We also provide empirical evidence
for the acceleration properties of our algorithm
compared to state-of-the-art methods.

1. Convolutional Representation for Long

Signals

Sparse coding aims at building sparse linear representations
of a data set based on a dictionary of basic elements called
atoms.
It has proven to be useful in many applications,
ranging from EEG analysis to images and audio processing
(Adler et al., 2013; Kavukcuoglu et al., 2010; Mairal et al.,
2010; Grosse et al., 2007). Convolutional sparse coding is a
specialization of this approach, focused on building sparse,
shift-invariant representations of signals. Such representa-
tions present a major interest for applications like segmen-
tation or classiﬁcation as they separate the shape and the lo-
calization of patterns in a signal. This is typically the case
for physiological signals which can be composed of recur-
rent patterns linked to speciﬁc behavior in the human body
such as the characteristic heartbeat pattern in ECG record-
ings. Depending on the context, the dictionary can either
be ﬁxed analytically (e.g. wavelets, see Mallat 2008), or

1CMLA, ENS Paris-Saclay, Universit´e Paris-Saclay, Cachan,
France 2L2TI, Universit´e Paris 13, Villetaneuse, France. Cor-
respondence to: Moreau Thomas <thomas.moreau@cmla.ens-
cachan.fr>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

learned from the data (Bristow et al., 2013; Mairal et al.,
2010).

Several algorithms have been proposed to solve the
convolutional sparse coding.
The Fast Iterative Soft-
Thresholding Algorithm (FISTA) was adapted for convo-
lutional problems in Chalasani et al. (2013) and uses prox-
imal gradient descent to compute the representation. The
Feature Sign Search (FSS), introduced in Grosse et al.
(2007), solves at each step a quadratic subproblem for an
active set of the estimated nonzero coefﬁcients and the
Fast Convolutional Sparse Coding (FCSC) of Bristow et al.
(2013) is based on Alternating Direction Method of Multi-
pliers (ADMM). Finally, the coordinate descent (CD) has
been extended by Kavukcuoglu et al. (2010) to solve the
convolutional sparse coding. This method greedily opti-
mizes one coordinate at each iteration using fast local up-
dates. We refer the reader to Wohlberg (2016) for a detailed
presentation of these algorithms.

To our knowledge, there is no scalable version of these al-
gorithms for long signals. This is a typical situation, for
instance, in physiological signal processing where sensor
information can be collected for a few hours with sampling
frequencies ranging from 100 to 1000Hz. The existing
algorithms for generic (cid:96)1-regularized optimization can be
accelerated by improving the computational complexity of
each iteration. A ﬁrst approach to improve the complexity
of these algorithms is to estimate the non-zero coefﬁcients
of the optimal solution to reduce the dimension of the op-
timization space, using either screening (El Ghaoui et al.,
2012; Fercoq et al., 2015) or active-set algorithms (John-
son & Guestrin, 2015). Another possibility is to develop
parallel algorithms which compute multiple updates simul-
taneously. Recent studies have considered distributing co-
ordinate descent algorithms for general (cid:96)1-regularized min-
imization (Scherrer et al., 2012a;b; Bradley et al., 2011; Yu
et al., 2012). These papers propose synchronous algorithms
using either locks or synchronizing steps to ensure the con-
vergence in general cases. You et al. (2016) derive an asyn-
chronous distributed algorithm for the projected coordinate
descent which uses centralized communication and ﬁnely
tuned step size to ensure the convergence of their method.

In the present paper, we design a novel distributed algo-

DICOD: Distributed Convolutional Sparse Coding

rithm tailored for the convolutional problem which is based
on coordinate descent, named Distributed Convolution Co-
ordinate Descent (DICOD). DICOD is asynchronous and
each process can run independently without locks or syn-
chronization steps. This algorithm uses a local communi-
cation scheme to reduce the number messages between the
processes and does not rely on external learning rates. We
also prove that this algorithm scales super-linearly with the
number of cores compared to the sequential CD, up to cer-
tain limitations.

In Section 2, we introduce the DICOD algorithm for the
resolution of convolutional sparse coding. Then, we prove
in Section 3 that DICOD converges to the optimal solution
for a wide range of settings and we analyze its complex-
ity. Finally, Section 4 presents numerical experiments that
illustrate the beneﬁts of the DICOD algorithm with respect
to other state-of-the-art algorithms and validate our theo-
retical analysis.

2. Distributed Convolutional Coordinate

Descent (DICOD)

Notations. The space of multivariate signals of length T
in RP is denoted by X P
T . For these signals, their value at
is denoted by X[t] ∈ RP and for all
time t ∈
0, T − 1
(cid:74)
(cid:75)
, X[t] = 000P . The indicator function of t0 is
0, T − 1
t /∈
(cid:75)
(cid:74)
denoted 111t0. For any signal X ∈ X P
T , the reversed signal
is deﬁned as (cid:101)X[t] = X[T − t], the d-norm is deﬁned as
(cid:17)1/d

(cid:107)X(cid:107)d =
and the replacement opera-
tor as Φt0(X)[t] = (1 − 111t0 (t))X[t] , which replaces the
value at time t0 by 0. Finally, for L, W ∈ N∗, the convolu-
tion between Z ∈ X 1
W is a multivariate signal
Z ∗DDD ∈ X P
,

L and DDD ∈ X P
T with T =L+W −1 such that for t ∈

t=0 (cid:107)X[t](cid:107)d
d

(cid:16)(cid:80)T −1

0, T −1

(cid:74)

(cid:75)

(Z ∗ DDD)[t] ∆=

Z[t − τ ]DDD[τ ] .

W −1
(cid:88)

τ =0

This section reviews in Subsection 2.1 the convolutional
sparse coding as an (cid:96)1-regularized optimization problem
and the coordinate descent algorithm to solve it. Then, Sub-
section 2.2 and Subsection 2.3 respectively introduce the
Distributed Convolutional Coordinate Descent (DICOD)
and the Sequential DICOD (SeqDICOD) algorithms to ef-
ﬁciently solve convolutional sparse coding for long sig-
nals. Finally, Subsection 2.4 discusses related work on (cid:96)1-
regularized coordinate descent algorithms.

2.1. Coordinate Descent for Convolutional Sparse

Coding

Convolutional Sparse Coding. Consider the multivari-
ate signal X ∈ X P

⊂ X P

W be a set of

T . Let DDD =

DDDk

(cid:111)K

(cid:110)

k=1

K patterns with W (cid:28) T and Z = {Zk}K
L be a set
of K activation signals with L=T −W +1. The convolu-
tional sparse representation models a multivariate signal X
as the sum of K convolutions between a local pattern DDDk
and an activation signal Zk such that:

k=1 ⊂ X 1

X[t] =

(Zk ∗ DDDk)[t] + E[t],

∀t ∈

0, T − 1
(cid:75)

(cid:74)

, (1)

K
(cid:88)

k=1

with E ∈ X P
T representing an additive noise term. This
model also assumes that the coding signals Zk are sparse,
in the sense that only few entries are nonzero in each signal.
The sparsity property forces the representation to display
localized patterns in the signal. Note that this model can be
extended to higher order signals such as images by using
the proper convolution operator. In this study, we focus on
1D-convolution for the sake of simplicity.

Given a dictionary of patterns DDD, convolutional sparse cod-
ing aims to retrieve the sparse decomposition Z ∗ associated
to the signal X by solving the following (cid:96)1-regularized op-
timization problem

Z ∗ = argmin

E(Z) ,

Z=(Z1,...ZK )

E(Z) ∆=

X −

Zk ∗ DDDk

+ λ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

where

(2)

K
(cid:88)

k=1

(cid:13)
(cid:13)Zk

(cid:13)
(cid:13)1 ,

(3)

for a given regularization parameter λ > 0 . The problem
formulation (2) can be interpreted as a special case of the
LASSO problem with a band circulant matrix. Therefore,
classical optimization techniques designed for LASSO can
easily be applied to solve it with the same convergence
guarantees. Kavukcuoglu et al. (2010) adapted the coor-
dinate descent to efﬁciently solve the convolutional sparse
coding.

Convolutional Coordinate Descent. The coordinate de-
scent is a method which updates one coordinate at each it-
eration. This type of optimization algorithms is efﬁcient for
sparse optimization problem since few coefﬁcients need to
be updated to ﬁnd the optimal solution and the greedy se-
lection of updated coordinates is a good strategy to achieve
fast convergence to the optimal point. Algorithm 1 summa-
rizes the greedy convolutional coordinate descent.

The method proposed by Kavukcuoglu et al. (2010) itera-
tively updates at each iteration one coordinate (k0, t0) of
the coding signal Z to its optimal value Z (cid:48)
[t0] when all
k0
other coordinates are ﬁxed. A closed form solution exists
to compute the value Z (cid:48)
k0

[t0] for the update,

Z (cid:48)
k0

[t0] =

1
(cid:107)DDDk0 (cid:107)2
2

Sh(βk0[t0], λ),

(4)

DICOD: Distributed Convolutional Sparse Coding

Algorithm 1 Greedy Coordinate Descent
1: Input: DDD, X, parameter (cid:15) > 0
2: C =
×
0, L − 1
(cid:75)
(cid:75)
3: Initialization: ∀(k, t) ∈ C,

1, K

(cid:74)

(cid:74)

(cid:18)

(cid:19)

Zk[t] = 0, βk[t] =

(cid:102)DDDk ∗ X

[t]

4: repeat

5:

6:

k[t] =

∀(k, t) ∈ C, Z (cid:48)

1
(cid:107)DDDk(cid:107)2
2
Choose (k0, t0) = arg max
|∆Zk[t]|
(k,t)∈C
Update β using (5) and Zk0[t0] ← Z (cid:48)
k0

Sh(βk[t], λ) ,

[t0]

7:
8: until |∆Zk0 [t0]| < (cid:15)

Algorithm 2 DICODM
1: Input: DDD, X, parameter (cid:15) > 0
2: In parallel for m = 1 · · · M
3: For all (k, t) in Cm, initialize βk[t] and Zk[t]
4: repeat
5:
6:
7:

Receive messages and update β with (5)
∀(k, t) ∈ Cm, compute Z (cid:48)
Choose (k0, t0) = arg max
Update β with (5) and Zk0 [t0] ← Z (cid:48)
k0
if t0 − mLM < W then

8:
9:
10:
11:
12:
13: until for all cores, |∆Zk0 [t0]| < (cid:15)

if (m + 1)LM − t0 < W then

Send (k0, t0, ∆Zk0 [t0]) to core m − 1

Send (k0, t0, ∆Zk0 [t0]) to core m + 1

k[t] with (4)

|∆Zk[t]|

(k,t)∈Cm

[t0]

with the soft thresholding operator deﬁned as

Sh(u,λ)=sign(u)max(|u|−λ,0).

and an auxiliary variable β ∈ X K

L deﬁned as

βk[t]=

(cid:102)DDDk∗

X−

Zk(cid:48)∗DDDk(cid:48)−Φt

(cid:0)Zk

(cid:1)∗DDDk

[t] ,
















K
(cid:88)

k(cid:48)=1
k(cid:48)(cid:54)=k
















Note that βk[t] is simply the residual when Zk[t] is equal
to 0.

The success of this algorithm highly depends on the efﬁ-
ciency in computing this coordinate update. For problem
(2), Kavukcuoglu et al. (2010) show that if at iteration q, the
coefﬁcient (k0, t0) of Z (q) is updated to the value Z (cid:48)
[t0],
k0
then it is possible to compute β(q+1) from β(q) using

β(q+1)
k

[t] = β(q)

k [t] − Sk,k0[t − t0]∆Z (q)
k0

[t0],

(5)

for all (k, t) (cid:54)= (k0, t0) with Sk,l[t] = ( (cid:102)DDDk ∗ DDDl)[t] . For all
t /∈
, S[t] is zero. Thus, only O(KW )
operations are needed to maintain β up-to-date with the
current estimate Z. In the following,

−W + 1, W − 1
(cid:75)

(cid:74)

∆Ek0 [t0] = E(Z (q)) − E(Z (q+1))

denotes the cost variation obtained when the coefﬁcient
(k0, t0) is replaced by its optimal value Z (cid:48)
k0

[t0].

The selection of the updated coordinate (k0, t0) can follow
different strategies. Cyclic updates (Friedman et al., 2007)
and random updates (Shalev-Shwartz & Tewari, 2009) are
efﬁcient strategies as they have a O (cid:0)1(cid:1) computational
complexity. Osher & Li (2009) propose to select the co-
ordinate greedily to maximize the cost reduction of the up-
date. In this case, the coordinate is chosen as the one with

the largest difference max(k,t)
value Zk[t] and the value Z (cid:48)

k[t] with

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∆Zk[t]
(cid:12) between its current

∆Zk[t] = Zk[t] − Z (cid:48)

k[t]

(6)

This strategy is computationally more expensive, with a
cost of O (cid:0)KT (cid:1) but it has a better convergence rate (Nu-
tini et al., 2015). In this paper, we focus on the greedy ap-
proach as it aims to get the largest gain from each update.
Moreover, as the updates in the greedy scheme are more
complex to compute, distributing them provides a larger
speedup compare to other strategies.

The procedure is run until maxk,t |∆Zk[t]| becomes
smaller than a speciﬁed tolerance parameter (cid:15).

2.2. Distributed Convolutional Coordinate Descent

(DICOD)

For convolutional sparse coding, the coordinate descent up-
dates are only weakly dependent as it is shown in (5). It is
thus natural to parallelize it for this problem.

DICOD. Algorithm 2 describes the steps of DICOD with
M workers. Each worker m ∈
is in charge
of updating the coefﬁcients of a segment Cm of length
LM = L/M deﬁned by:

1, M

(cid:75)

(cid:74)

(cid:26)

Cm=

(k,t) ; k∈
(cid:74)

(cid:75)

1,K

, t∈(cid:114)(m−1)LM , mLM −1(cid:122)

.

(cid:27)

The local updates are performed in parallel for all the
cores using the greedy coordinate descent
introduced
in Subsection 2.1. When a core m updates the co-
ordinate (k0, t0) such that t0 ∈
(m − 1)LM +
W, mLM − W
the updated coefﬁcients of β are all
,
contained in Cm and there is no need to update β on

(cid:75)

(cid:74)

DICOD: Distributed Convolutional Sparse Coding

Cm updated in (k0, t0)

Cm+1 updated in (k1, t1)

∆Zk0 [t0], k0, t0

∆Zk0

[t0]

Z

β

t0−S

t0

No message

t0+S t1−S

t1

∆Zk1

[t1]

Z

β
t1+S

Figure 1. Communication process in DICOD for two cores Cm and Cm+1. (red) The process needs to send a message to its neighbor
as it updates a coefﬁcient with t0 located near the border of the core’s segment, in the interference zone. (green) The update in t1 is
independent of other cores.

(cid:75)

In these cases,

the other cores.
lent to a sequential update. When t0∈
(cid:16)
(cid:74)
(m−1)LM ,(m−1)LM +W
resp. t0∈
(cid:74)

the update is equiva-
mLM −W,mLM
(cid:17)
(cid:75)
, some of the co-
efﬁcients of β in core m + 1 (resp. m − 1) need to be up-
dated and the update is not local anymore. This can be done
by sending the position of updated coordinate (k0, t0), and
the value of the update ∆Zk0 [t0] to the neighboring core.
Inter-
Figure 1 illustrates this communication process.
processes communications are very limited in DICOD. One
node communicates with its neighbors only when it updates
coefﬁcients close to the extremity of its segment. When the
size of the segment is reasonably large compared to the size
of the patterns, only a small part of the iterations needs to
send messages. We cannot apply the stopping criterion of
CD in each worker of DICOD, as this criterion might not
be reached globally. The updates in the neighbor cores can
break this criterion. To avoid this issue, the convergence is
considered to be reached once all the cores achieve this cri-
terion simultaneously. Workers that reach this state locally
are paused, waiting for incoming communication or for the
global convergence to be reached.

The key point that allows distributing the convolutional co-
ordinate descent algorithm is that the solutions on time seg-
ments that are not overlapping are only weakly dependent.
Equation (5) shows that a local change has impact on a seg-
ment of length 2W − 1 centered around the updated coor-
dinate. Thus, if two coordinates which are far enough were
updated simultaneously, the resulting point Z is the same
as if these two coordinates had been updated sequentially.
By splitting the signal into continuous segments over mul-
tiple cores, coordinates can be updated independently on
each core up to certain limits.

Interferences. When two coefﬁcients
and
(k1, t1) are updated by two neighboring cores simultane-
ously, the updates might not be independent and cannot be
considered to be sequential. The local version of β used
for the second update does not account for the ﬁrst update.
We say that the updates are interfering. The cost reduction

(k0, t0)

resulting from these two updates is denoted ∆Ek0,k1 [t0, t1]
and simple computations, detailed in Proposition A.2,
show that

∆Ek0,k1 [t0, t1] =

iterative steps
(cid:122)
(cid:123)
(cid:125)(cid:124)
∆Ek0 [t0] + ∆Ek1[t1]

,
− Sk0,k1 [t1 − t0]∆Zk0[t0]∆Zk1 [t1]
(cid:123)(cid:122)
(cid:125)
interference

(cid:124)

(7)

If |t1 − t0| ≥ W , then Sk0,k1[t1 − t0] = 0 and the updates
can be considered to be sequential as the interference term
is zero. When |t1 −t0| < W , the interference term does not
vanish but Section 3 shows that under mild assumption, this
term can be controlled and it does not make the algorithm
diverge.

2.3. Randomized Locally Greedy Coordinate Descent

(SeqDICOD)

The theoretical analysis in Theorem 3 shows that DICOD
provides a super-linear acceleration compared to the greedy
coordinate descent. This result is supported with the nu-
merical experiment presented in Figure 4. The super-linear
speed up results from a double acceleration, provided by
the parallelization of the updates – we update M coefﬁ-
cients at each iteration – and also by the reduction of the
complexity of each iteration. Indeed, each core computes
greedy updates with linear in complexity on 1/M -th of the
signal. This super-linear speed-up means that running DI-
COD sequentially will still provide a speed-up compared
to the greedy coordinate descent algorithm.

Algorithm 3 presents SeqDICOD. This algorithm is a se-
quential version of DICOD. At each step, one segment Cm
is selected uniformly at random between the M segments.
The greedy coordinate descent algorithm is applied locally
on this segment. This update is only locally greedy and

DICOD: Distributed Convolutional Sparse Coding

Algorithm 3 Locally
SeqDICODM
1: Input: DDD, X, parameter (cid:15) > 0, number of segments

coordinate

descent

greedy

external parameters.

M

2: Initialize βk[t] and Zk[t] for all (k, t) in C
3: Initialize dZm = +∞ for m ∈
4: repeat
5:
6:
7:

Randomly select m ∈
∀(k, t) ∈ Cm, compute Z (cid:48)
Choose (k0, t0) = argmax
(k,t)∈Cm

(cid:75)
k[t] with (4)
|∆Zk[t]|

1, M

1, M

(cid:74)

(cid:75)

(cid:74)

8:
9:

10:

[t0]

Update β with (5)
Update the current point estimate Zk0[t0](q+1) ←
Z (cid:48)
k0
Update max
(cid:12)
(cid:12)
(cid:12)Zk0[t0](q+1) − Z (cid:48)

updates
[t0]

vector

dZm

=

(cid:12)
(cid:12)
(cid:12)

k0

11: until (cid:107)dZ(cid:107)∞ < (cid:15) and (cid:107)∆Z(cid:107)∞ < (cid:15)

maximizes

(k0, t0) = argmax
(k,t)∈Cm

|∆Zk[t]|

This coordinate is then updated to its optimal value Z (cid:48)
[t0].
k0
In this case, there is no interference as the segments are not
updated simultaneously.

Note that if M = T , this algorithm becomes very close
to the randomized coordinate descent. The coordinate is
selected greedily only between the K different channels of
the signal Z at the selected time. So the selection of M
depends on a tradeoff between the randomized coordinate
descent and the greedy coordinate descent.

2.4. Discussion

This algorithm differs from the existing paradigm to dis-
tribute CD (Scherrer et al., 2012a;b; Bradley et al., 2011;
Yu et al., 2012; You et al., 2016) as it does not rely on
centralized communication. Indeed, other parallel coordi-
nate descent algorithms rely on a parameter server, which
is an extra worker that holds the current value of Z. As
the size of the problem and the number of nodes grow, the
communication cost can rapidly become an issue with this
kind of centralized communication. The natural workload
split proposed with DICOD allows for more efﬁcient in-
teractions between the workers and reduces the need for
inter-node communications. Moreover, to prevent the in-
terferences breaking the convergence, existing algorithms
rely either on synchronous updates (Bradley et al., 2011;
Yu et al., 2012) or on reduced step size in the updates (You
et al., 2016; Scherrer et al., 2012a). In both case, they are
less efﬁcient than our asynchronous greedy algorithm that
can leverage the convolutional structure of the problem to
use both large updates and independent processes without

As seen in the introduction, another way to improve the
computational complexity of sparse coding algorithms is
to estimate the non-zero coefﬁcients of the optimal solution
in order to reduce the dimension of the optimization space.
As this research direction is orthogonal to the paralleliza-
tion of the coordinate descent, it would be possible to com-
bine our algorithm with either screening (El Ghaoui et al.,
2012; Fercoq et al., 2015) or active-set methods (Johnson
& Guestrin, 2015). The evaluation of the performances of
our algorithm with these strategies is left for future work.

3. Properties of DICOD

Convergence of DICOD. The magnitude of the interfer-
ence is related to the value of the cross-correlation between
dictionary elements, as shown in Proposition 1. Thus, when
the interferences have low probability and small magni-
tude, the distributed algorithm behaves as if the updates
were applied sequentially, resulting in a large acceleration
compared to the sequential CD algorithm.
Proposition 1. For concurrent updates for coefﬁcients
(k0, t0) and (k1, t1) of a sparse code Z, the cost update
∆Ek0k1[t0, t1] is lower bounded by

∆Ek0k1[t0, t1] ≥∆Ek0[t0] + ∆Ek1 [t1]

− 2

Sk0,k1[t0 − t1]
(cid:107)DDDk0(cid:107)2(cid:107)DDDk1 (cid:107)2

(cid:112)∆Ek0[t0]∆Ek1[t1].
(8)

The proof of this proposition is given in Appendix C.1. It
relies on the (cid:107)DDDk(cid:107)2
2-strong convexity of (4), which gives
for all Z. Using this inequality

2∆Ek[t](Z)

√

|∆Zk[t]| ≤
with (7) yields the result.

(cid:107)DDDk(cid:107)2

This proposition controls the magnitude of the interfer-
ence using the cost reduction associated to a single update.
When the correlations between the different elements of the
dictionary are small enough, the interfering update does not
increase the cost function. The updates are less efﬁcient but
do not worsen the current estimate. Using this control on
the interferences, we can prove the convergence of DICOD.

Theorem 2. Consider the following hypotheses,
that
H1. For

(k0,t0),(k1,t1)

such

t0(cid:54)=t1,

all
Sk0,k1 [t0−t1]
(cid:107)DDDk0 (cid:107)2(cid:107)DDDk1 (cid:107)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 1 .

H2. There exists A ∈ N∗ such that all cores m ∈
(cid:75)
are updated at least once between iteration i and i + A if
the solution is not locally optimal.
H3. The delay in communication between the processes is
inferior to the update time.

1, M

(cid:74)

DICOD: Distributed Convolutional Sparse Coding

Under (H1)-(H2)-(H3), the DICOD algorithm converges
to the optimal solution Z ∗ of (2).

Corollary 4. The expected speedup E[Scd(M )] when
M α → 0 is such that

Assumption (H1) is satisﬁed as long as the dictionary el-
ements are not replicated in shifted positions in the dictio-
nary. It ensures that the cost is updated in the right direction
at each step. This assumption can be linked to the shifted
mutual coherence introduced in Papyan et al. (2016).

Hypothesis (H2) ensures that all coefﬁcients are updated
regularly if they are not already optimal. This analysis is
not valid when one of the cores fails. As only one core is
responsible for the update of a local segment, if a worker
fails, this segment cannot be updated anymore and thus the
algorithm will not converge to the optimal solution.

Finally, under (H3), an interference only results from one
update on each core. Multiple interferences occur when a
core updates multiple coefﬁcients in the border of its seg-
ment before receiving the communication from other pro-
cesses border updates. When T (cid:29) W , the probability of
multiple interference is low and this hypothesis can be re-
laxed if the updates are not concentrated on the borders.

Proof sketch for Theorem 2. The full proof can be found
in Appendix C.2. The main argument in proving the con-
vergence is to show that most of the updates can be con-
sidered sequentially and that the remaining updates do
not increase the cost of the current point. By (H3), for
a given iteration, a core can interfere with at most one
other core. Thus, without loss of generality, we can
consider that at each step q,
the variation of the cost
E is either ∆Ek0[t0](Z (q)) or ∆Ek0k1[t0, t1](Z (q)), for
some (k0, t0), (k1, t1) ∈
1, K
. Proposi-
tion 1 and (H1) proves that ∆Ek0k1[t0, t1](Z (q)) ≥ 0.
For a single update ∆Ek0[t0](Z (q)), the update is equiva-
lent to a sequential update in CD, with the coordinate cho-
sen randomly between the best in each segments. Thus,
∆Ek0 [t0](Z (q)) > 0 and the convergence is eventually
proved using results from Osher & Li (2009).

0, T − 1
(cid:75)

×

(cid:75)

(cid:74)

(cid:74)

Speedup of DICOD. We denote Scd(M ) the speedup of
DICOD compared to the sequential greedy CD. This quan-
tiﬁes the number of iterations that can be run by DICOD
during one iteration of CD.
Theorem 3. Let α = W
4 and if
the non-zero coefﬁcients of the sparse code are distributed
the expected speedup E[Scd(M )] is
uniformly in time,
lower bounded by

T and M ∈ N∗ . If αM < 1

E[Scd(M )] ≥ M 2(1 − 2α2M 2 (cid:16)

1 + 2α2M 2(cid:17) M

2 −1

) .

This result can be simpliﬁed when the interference proba-
bility (αM )2 is small.

E[Scd(M )] (cid:38)
α→0

M 2(1 − 2α2M 2 + O(α4M 4)) .

Proof sketch for Theorem 3. The full proof can be found
in Appendix D. There are two aspects involved in DI-
COD speedup: the computational complexity and the ac-
celeration due to the parallel updates. As stated in Subsec-
tion 2.1, the complexity of each iteration for CD is linear
with the length of the input signal T . In DICOD, each core
runs on a segment of size T
M . This accelerates the execu-
tion of individual updates by a factor M . Moreover, all
the cores compute their update simultaneously. The up-
dates without interference are equivalent to sequential up-
dates. Interfering updates happen with probability (cid:0)M α(cid:1)2
and do not increase the cost. Thus, one iteration of DI-
COD with Ni interferences provides a cost variation equiv-
alent to M − 2Ni iterations using sequential CD and, in
expectation, it is equivalent to M − 2E[Ni] iterations of
DICOD. The probability of interference depends on the ra-
tio between the length of the segments used for each core
and the size of the dictionary. If all the updates are spread
uniformly on each segment, the probability of interference

. The expected
between 2 neighboring cores is
number of interference E[Ni] can be upper bounded using
this probability and this yields the desired result.

(cid:17)2

(cid:16) MW
T

The overall speedup of DICOD is super-linear compared to
sequential greedy CD for the regime where αM (cid:28) 1. It
is almost quadratic for small M but as M grows, there is
a sharp transition that signiﬁcantly deteriorates the accel-
eration provided by DICOD. Section 4 empirically high-
lights this behavior. For a given α, it is possible to approxi-
mate the optimal number of cores M to solve convolutional
sparse coding problems.

Note that this super-linear speed up is due to the fact that
CD is inefﬁcient for long signals, as its iterations are com-
putationally too expensive to be competitive with the other
methods. The fact that we have a super-linear speed-up
means that running DICOD sequentially will provide an
acceleration compared to CD (see Subsection 2.3). For the
sequential run of DICOD, called SeqDICOD, we have a
linear speed-up in comparison to CD, when M is small
enough. Indeed, the iteration cost is divided by M as we
only need to ﬁnd the maximal update on a local segment
of size T
W , the iteration cost
does not decrease anymore as updating β costs O (cid:0)KW (cid:1)
and ﬁnding the best coordinate has the same complexity.

M . When increasing M over T

DICOD: Distributed Convolutional Sparse Coding

Figure 2. Evolution of the loss function for DICOD, SeqDICOD,
CD, FCSC and FISTA while solving sparse coding for a signal
generated with default parameters relatively to the number of it-
erations.

Figure 3. Evolution of the loss function for DICOD, SeqDICOD,
CD, FCSC and FISTA while solving sparse coding for a signal
generated with default parameters, relatively to time. This high-
lights the speed of the algorithm on the given problem.

4. Numerical Results

All the numerical experiments are run on ﬁve Linux ma-
chines with 16 to 24 Intel Xeon 2.70 GHz processors and
at least 64 GB of RAM on local network. We use a combi-
nation of Python, C++ and the OpenMPI 1.6 for the algo-
rithms implementation. The code to reproduce the ﬁgures
is available online 1. The run time denotes the time for the
system to run the full algorithm pipeline, from cold start
and includes for instance the time to start the sub-processes.
The convergence refers to the variation of the cost with the
number of iterations and the speed to the variation of the
cost relative to time.

Long convolutional Sparse Coding Signals. To further
validate our algorithm, we generate signals and test the per-
formances of DICOD compared to state-of-the-art methods
proposed to solve convolutional sparse coding. We gen-
erate a signal X of length T in RP following the model
described in (1). The K dictionary atoms DDDk of length
W are drawn as a generic dictionary. First, each entry is
sampled from a Gaussian distribution. Then, each pattern
is normalized such that (cid:107)DDDk(cid:107)2 = 1. The sparse code en-
tries are drawn from a Bernoulli-Gaussian distribution with
Bernoulli parameter ρ = 0.007, mean 0 and standard vari-
ation σ = 10 . The noise term E is chosen as a Gaus-
sian white noise with variance 1. The default values for
the dimensions are set to W = 200, K = 25, P = 7,
T = 600 × W and we used λ = 1.

Algorithms Comparison. DICOD is compared to the
main state-of-the-art optimization algorithms for convo-
lutional sparse coding: Fast Convolutional Sparse Cod-

1see the supplementary materials

ing (FCSC) from Bristow et al. (2013), Fast Iterative Soft
Thresholding Algorithm (FISTA) using Fourier domain
computation as described in Wohlberg (2016), the greedy
convolutional coordinate descent (CD, Kavukcuoglu et al.
2010) and the randomized coordinate descent (RCD, Nes-
terov 2012). All the speciﬁc parameters for these algo-
rithms are ﬁxed based on the authors’ recommendations.
DICODM denotes the DICOD algorithm run using M
cores. We also include SeqDICODM , for M ∈ {60, 600},
the sequential run of the DICOD algorithm using M seg-
ments, as described in Algorithm 3.

Figure 2 shows that the evolution of the performances of
SeqDICOD relatively to the iterations are very close to
the performances of CD. The difference between these two
algorithms is that the updates are only locally greedy in
SeqDICOD. As there is little difference visible between the
two curves, this means that in this case, the computed up-
dates are essentially the same. The differences are larger
for SeqDICOD600, as the choice of coordinates are more
localized in this case. The performance of DICOD60 and
DICOD30 are also close to the iteration-wise performances
of CD and SeqDICOD. The small differences between DI-
COD and SeqDICOD result from the iterations where there
are interferences.
Indeed, if two iterations interfere, the
cost does not decrease as much as if the iterations were
done sequentially. Thus, it requires more steps to reach the
same accuracy with DICOD60 than with SeqDICOD and
with DICOD30, as there are more interferences when the
number of cores M increases. This explains the discrep-
ancy in the decrease of the cost around the iteration 105.
However, the number of extra steps required is quite low
compared to the total number of steps and the performances
are mostly not affected by the interferences. The perfor-
mances of RCD in terms of iterations are much slower than

DICOD: Distributed Convolutional Sparse Coding

the greedy methods. Indeed, as only a few coefﬁcients are
useful, it takes many iterations to draw them randomly. In
comparison, the greedy methods are focused on the coef-
ﬁcients which largely divert from their optimal value, and
are thus most likely to be important. Another observation is
that the performance in term of number of iterations of the
global methods FCSC and FISTA are much better than the
methods based on local updates. As each iteration can up-
date all the coefﬁcients for FISTA, the number of iterations
needed to reach the optimal solution is indeed smaller than
for CD, where only one coordinate is updated at a time.

In Figure 3, the speed of theses algorithms can be ob-
served. Even though it needs many more iterations to con-
verge, the randomized coordinate descent is faster than the
greedy coordinate descent. Indeed, for very long signals,
the iteration complexity of greedy CD is prohibitive. How-
ever, using the locally greedy updates, with SeqDICOD60
the greedy algorithm can be made
and SeqDICOD600,
more efﬁcient. SeqDICOD600 is also faster than the other
state-of-the-art algorithms FISTA and FCSC. The choice
of M = 600 is a good tradeoff for SeqDICOD as it means
that the segments are of the size of the dictionary W . With
this choice for M = T
W , the computational complexity of
choosing a coordinate is O (cid:0)KW (cid:1) and the complexity of
maintaining β is also O (cid:0)KW (cid:1). Thus, the iterations of this
algorithm have the same complexity as RCD but are more
efﬁcient.

The distributed algorithm DICOD is faster compared to all
the other sequential algorithms and the speed up increases
with the number of cores. Also, DICOD has a shorter ini-
tialization time compared to the other algorithms. The ﬁrst
point in each curve indicates the time taken by the initial-
ization. For all the other methods, the computations for
constants – necessary to accelerate the iterations – have a
computational cost equivalent to the on of the gradient eval-
uation. As the segments of signal in DICOD are smaller,
the initialization time is also reduced. This shows that the
overhead of starting the cores is balanced by the reduction
of the initial computation for long signals. For shorter sig-
nals, we have observed that the initialization time is of the
same order as the other methods. The spawning overhead is
indeed constant whereas the constants are cheaper to com-
pute for small signals.

Speedup Evaluation. Figure 4 displays the speedup of
DICOD as a function of the number of cores. We used 10
generated problems for 2 signal lengths T = 150 · W and
T = 750 · W with W = 200 and we solved them using
DICODM with a number of cores M ranging from 1 to
75. The blue dots display the average running time for a
given number of workers. For both setups, the speedup is
super-linear up to the point where M α = 1
2 . For small

Figure 4. Speedup of DICOD as a function of the number of pro-
cesses used, average over 10 run on different generated signals.
This highlights a sharp transition between a regime of quadratic
speedups and the regime where the interference are slowing down
drastically the convergence.

M the speedup is very close to quadratic and a sharp tran-
sition occurs as the number of cores grows. The verti-
cal solid green line indicates the approximate position of
the maximal speedup given in Corollary 4 and the dashed
lined is the expected theoretical run time derived from the
same expression. The transition after the maximum is very
sharp. This approximation of the speedup for small values
of M α is close to the experimental speedup observed with
DICOD. The computed optimal value of M ∗ is close to the
optimal number of cores in these two examples.

5. Conclusion

In this work, we introduced an asynchronous distributed
algorithm that is able to speed up the resolution of the Con-
volutional Sparse Coding problem for long signals. This
algorithm is guaranteed to converge to the optimal solution
of (2) and scales superlinearly with the number of cores
used to distribute it. These claims are supported by numer-
ical experiments highlighting the performances of DICOD
compared to other state-of-the-art methods. Our proofs rely
extensively on the use of one dimensional convolutions. In
this setting, a process m only has two neighbors m − 1 and
m+1. This ensures that there is no high order interferences
between the updates. Our analysis does not apply straight-
forwardly to distributed computation using square patches
of images as the higher order interferences are more com-
plicated to handle. A way to apply our algorithm with these
guarantees to images is to split the signals along only one
direction, to avoid higher order interferences. The exten-
sion of our results to this case is an interesting direction for
future work.

DICOD: Distributed Convolutional Sparse Coding

References

Adler, A., Elad, Michael, Hel-Or, Y., and Rivlin, E. Sparse
Coding with Anomaly Detection. In IEEE International
Workshop on Machine Learning for Signal Processing
(MLSP), pp. 22 – 25, Southampton, United Kingdom,
2013.

Bradley, Joseph K., Kyrola, Aapo, Bickson, Danny, and
Guestrin, Carlos. Parallel Coordinate Descent for (cid:96)1-
Regularized Loss Minimization. In International Con-
ference on Machine Learning (ICML), pp. 321–328,
Bellevue, WA, USA, 2011.

Bristow, Hilton, Eriksson, Anders, and Lucey, Simon. Fast
In IEEE Conference on
convolutional sparse coding.
Computer Vision and Pattern Recognition (CVPR), pp.
391–398, Portland, OR, USA, 2013.

Mallat, St´ephane. A Wavelet Tour of Signal Processing.

Academic press, 2008.

Moreau, Thomas. Convolutional Sparse Representations
– application to physiological signals and interpretab-
ility for Deep Learning. PhD thesis, CMLA, ENS Paris-
Saclay, Universit´e Paris-Saclay, 2017.

Nesterov, Yuri. Efﬁciency of coordinate descent methods
on huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012.

Nutini, Julie, Schmidt, Mark, Laradji, Issam H, Friedlan-
der, Michael P., and Koepke, Hoyt. Coordinate Descent
Converges Faster with the Gauss-Southwell Rule Than
Random Selection. In International Conference on Ma-
chine Learning (ICML), pp. 1632–1641, Lille, France,
2015.

Chalasani, Rakesh, Principe, Jose C., and Ramakrishnan,
Naveen. A fast proximal method for convolutional
sparse coding. In International Joint Conference on Neu-
ral Networks (IJCNN), pp. 1–5, Dallas, TX, USA, 2013.

Osher, Stanley and Li, Yingying. Coordinate descent op-
timization for (cid:96)1 minimization with application to com-
pressed sensing; a greedy algorithm. Inverse Problems
and Imaging, 3(3):487–503, 2009.

El Ghaoui, Laurent, Viallon, Vivian, and Rabbani, Tarek.
Safe feature elimination for the LASSO and sparse su-
pervised learning problems. Journal of Machine Learn-
ing Research (JMLR), 8(4):667–698, 2012.

Fercoq, Olivier, Gramfort, Alexandre, and Salmon, Joseph.
Mind the duality gap : safer rules for the Lasso. In Inter-
national Conference on Machine Learning (ICML), pp.
333–342, Lille, France, 2015.

Friedman, Jerome, Hastie, Trevor, H¨oﬂing, Holger, and
Tibshirani, Robert. Pathwise coordinate optimization.
The Annals of Applied Statistics, 1(2):302–332, 2007.

Grosse, Roger, Raina, Rajat, Kwong, Helen, and Ng, An-
drew Y. Shift-Invariant Sparse Coding for Audio Classi-
ﬁcation. Cortex, 8:9, 2007.

Johnson, Tyler and Guestrin, Carlos. Blitz: A Principled
Meta-Algorithm for Scaling Sparse Optimization. In In-
ternational Conference on Machine Learning (ICML),
pp. 1171–1179, Lille, France, 2015.

Kavukcuoglu, Koray, Sermanet, Pierre, Boureau, Y-lan,
Gregor, Karol, and Lecun, Yann. Learning Convolu-
In
tional Feature Hierarchies for Visual Recognition.
Advances in Neural Information Processing Systems
(NIPS), pp. 1090–1098, Vancouver, Canada, 2010.

Mairal, Julien, Bach, Francis, Ponce, Jean, and Sapiro,
Guillermo. Online Learning for Matrix Factorization and
Sparse Coding. Journal of Machine Learning Research
(JMLR), 11(1):19–60, 2010.

Papyan, Vardan, Sulam, Jeremias, and Elad, Michael.
Working Locally Thinking Globally - Part II: Theoret-
ical Guarantees for Convolutional Sparse Coding. arXiv
preprint, arXiv:1607(02009), 2016.

Scherrer, Chad, Halappanavar, Mahantesh, Tewari, Am-
buj, and Haglin, David. Scaling Up Coordinate De-
scent Algorithms for Large (cid:96)1 Regularization Problems.
Technical report, Paciﬁc Northwest National Laboratory
(PNNL), 2012a.

Scherrer, Chad, Tewari, Ambuj, Halappanavar, Mahantesh,
and Haglin, David J. Feature Clustering for Accelerating
Parallel Coordinate Descent. In Advances in Neural In-
formation Processing Systems (NIPS), pp. 28–36, South
Lake Tahoe, United States, 2012b.

Shalev-Shwartz, Shai and Tewari, A. Stochastic Methods
for (cid:96)1-regularized Loss Minimization. In International
Conference on Machine Learning (ICML), pp. 929–936,
Montreal, Canada, 2009.

Wohlberg, Brendt. Efﬁcient Algorithms for Convolutional
IEEE Transactions on Image

Sparse Representations.
Processing, 25(1), 2016.

You, Yang, Lian, Xiangru, Liu, Ji, Yu, Hsiang-Fu, Dhillon,
Inderjit S., Demmel, James, and Hsieh, Cho-Jui. Asyn-
chronous Parallel Greedy Coordinate Descent.
In
Advances in Neural Information Processing Systems
(NIPS), pp. 4682–4690, Barcelona, Spain, 2016.

Yu, Hsiang Fu, Hsieh, Cho Jui, Si, Si, and Dhillon, Inder-
jit. Scalable coordinate descent approaches to parallel

DICOD: Distributed Convolutional Sparse Coding

matrix factorization for recommender systems. In IEEE
International Conference on Data Mining (ICDM), pp.
765–774, Brussels, Belgium, 2012.

Supplemetary materials – DICOD: Distributed Convolutional Coordinate
Descent for Convolutional Sparse Coding

A. Computation for the cost updates

When a coefﬁcient Zk[t] is updated to u, the cost update is a simple function of Zk[t] and u.
Proposition A.1. The update of the weight in (k0, t0) from the value Zk0[t0] in Z to u ∈ R in Z (1) gives a cost cost
variation:

ek0,t0(u) = E(Z) − E(Z (1))

=

(cid:107)DDDk0(cid:107)2
2
2

(Zk0[t0]2 − u2) − βk0[t0](Zk0[t0] − u) + λ(|Zk0[t0]| − |u|).

Proof. Let αk0[t] = (X − (cid:80)K

k=1 Zk ∗ DDDk)[t] + DDDk0[t − t0]Zk0[t0] for all t ∈

0..T − 1
(cid:75)
(cid:74)

and

Z (1)

k [t] =






u,
Zk[t],

if (k, t) = (k0, t0)
elsewhere

.

ek0,t0(u) =

X −

Zk ∗ DDDk



[t] + λ

(cid:107)Zk(cid:107)1 −


2

K
(cid:88)

k=1

1
2

T −1
(cid:88)

t=0



X −

K
(cid:88)

k=1

Z (1)

k ∗ DDDk



[t] + λ

(cid:107)Z (1)

k (cid:107)1


2

K
(cid:88)

k=1

αk0[t] − DDDk0 [t − t0]Zk0[t0]

−

αk0[t] − DDDk0[t − t0]u

+ λ(|Zk0 [t0]| − |u|)

(cid:17)2

K
(cid:88)

k=1

(cid:17)2

T −1
(cid:88)

t=0

1
2

T −1
(cid:88)

(cid:16)

t=0

DDDk0[t − t0]2(Zk0[t0]2 − u2) −

αk0[t]DDDk0 [t − t0](Zk0[t0] − u) + λ(|Zk0 [t0]| − |u|)

(Zk0[t0]2 − u2) − ((cid:103)DDDk0 ∗ αk0)[t]
(Zk0 [t0] − u) + λ(|Zk0[t0]| − |u|)
(cid:125)

(cid:124)

(cid:123)(cid:122)
βk0 [t0]

1
2

1
2

1
2

=

=

=



T −1
(cid:88)

t=0

T −1
(cid:88)

(cid:16)

t=0

T −1
(cid:88)

t=0
(cid:107)DDDk0(cid:107)2
2
2

This conclude our proof.

Using this result, we can derive the optimal value Z (cid:48)
k0
optimization problem:

[t0] to update the coefﬁcient (k0, t0) as the solution of the following

Z (cid:48)
k0

[t0] = arg min
y∈R

ek0,t0 (u) ∼ arg min
u∈R

(cid:16)

(cid:107)DDDk0(cid:107)2
2
2

(cid:17)2

u − βk0[t0]

+ λ|u| .

(9)

[t1], we obtain the

In the case where two coefﬁcients (k0, t0), (k1, t1) are updated in the same iteration to values u and Z (cid:48)
k1
following cost variation.
Proposition A.2. The update of the weight Zk0[t0] and Zk1[t1] to values Z (cid:48)
k0
gives an update of the cost:

[t0] and Z (cid:48)
k1

[t1] with ∆Zk[t] = Zk[t] − Z (cid:48)

k[t]

∆Ek0k1[t0, t1] = ∆Ek0 [t0] + ∆Ek1[t1] − Sk0,k1 [t0 − t1]∆Zk0[t0]∆Zk1 [t1]

DICOD: Distributed Convolutional Sparse Coding

Proof. We deﬁne Z (1)

k [t] =






Zk0[t0],
Zk1[t1],
Zk[t],

if (k, t) = (k0, t0)
if (k, t) = (k1, t1)
otherwise

.

Let α[t] = (X − (cid:80)K
We have α[t] = αk0[t] + DDDk1 [t − t1]Zk1[t1] = αk1[t] + DDDk0[t − t0]Zk0[t0].

k=1 ZkDDDk)[t] + DDDk0[t − t0]Zk0 [t0] + DDDk1 [t − t1]Zk1[t1].

∆Ek0k1[t0, t1] =

X −

Zk ∗ DDDk

λ(cid:107)Zk(cid:107)1 −

X −

Z (1)

k ∗ DDDk



[t] + λ

(cid:107)Z (1)

k (cid:107)1

K
(cid:88)

k=1


 [t]2 +

1
2

K
(cid:88)

k=1

=

α[t] − DDDk0[t − t0]Zk0 [t0] − DDDk1 [t − t1]Zk1[t1]

+ λ(|Zk0 [t0]| − |Z (cid:48)
k0

[t0]|)

K
(cid:88)

k=1



T −1
(cid:88)

t=0

(cid:17)2

(cid:17)2

K
(cid:88)

k=1


2



α[t] − DDDk0 [t − t0]Z (cid:48)
k0

[t0] − DDDk1[t − t1]Z (cid:48)
k1

[t1]

+ λ(|Zk1[t1]| − |Z (cid:48)
k1

[t1]|)

t=0

DDDk0 [t − t0]2(Zk0 [t0]2 − Z (cid:48)
k0

[t0]2) + DDDk1 [t − t1]2(Zk1[t1]2 − Z (cid:48)
k1

[t1]2)





1
2

1
2

T −1
(cid:88)

t=0

T −1
(cid:88)

(cid:16)

t=0

T −1
(cid:88)

(cid:16)

−

1
2

=

1
2

T −1
(cid:88)

t=0

−

T −1
(cid:88)

t=0


αk0 [t]DDDk0 [t − t0]∆Zk0[t0] + αk1 [t1]DDDk1[t − t]∆Zk1[t1]

+ DDDk0[t − t0]DDDk1 [t − t1](∆Zk0 [t0]Z (cid:48)
k1

[t1] + ∆Zk1[t1]Z (cid:48)
k0

[t0])


− DDDk0[t − t0]DDDk1 [t − t1](Zk0 [t0]Zk1[t1] − Z (cid:48)
k0

[t0]Z (cid:48)
k1

[t1])



+ λ(|Zk0[t0]| − |Z (cid:48)
k0
=∆Ek0[t0] + ∆Ek1 [t1]

[t0]| + |Zk1[t1]| − |Z (cid:48)
k1

[t1]|)

−

T −1
(cid:88)

t=0

DDDk0[t − t0]DDDk1[t − t1]

(cid:20)
Zk0 [t0]Zk1[t1] − Z (cid:48)
k0

[t0]Zk1[t1] − Zk0[t0]Z (cid:48)
k1

[t1] + Z (cid:48)
k1

[t1]Z (cid:48)
k0

[t0]

(cid:21)

=∆Ek0[t0] + ∆Ek1[t1] −

DDDk0[t]DDDk1[t + t0 − t1](Zk0 [t0] − Z (cid:48)
k0

[t0])(Zk1 [t1] − Z (cid:48)
k1

[t1])

T −1
(cid:88)

t=0

=∆Ek0 [t0] + ∆Ek1[t1] − (cid:103)DDDk0 ∗ DDDk1 [t0 − t1]∆Zk0[t0]∆Zk1 [t1]

By deﬁnition of Sk0,k1[t] = (cid:103)DDDk0 ∗ DDDk1[t]. This conclude our proof.

B. Intermediate results

Consider solving a convex problem of the form:

min E(Z) = F (Z) +

gi(Zk[t])

(10)

where F is differentiable and convex, and gi is convex. Let us ﬁrst recall a theorem stated and proved in (Osher & Li,
2009).

Theorem B.1. Suppose F (z) is smooth and convex, with

≤ M , and E is strictly convex with respect to any

one variable Zi, then the statement that u = (u1, u2, . . . un) is an optimal solution of (10) is equivalent to the statement
that every component ui is an optimal solution of E with respect to the variable ui for any i.

L−1
(cid:88)

K
(cid:88)

t=0

k=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2F
∂ui∂uj

(cid:12)
(cid:12)
(cid:12)
(cid:12)∞

DICOD: Distributed Convolutional Sparse Coding

In the convolutional sparse coding problem, the function F (Z) = 1
k=1 Zk ∗ DDDk(cid:107) is smooth and convex and its
Hessian is constant. The following Lemme B.2, can be used to show that the function E restricted to one of its variable is
strictly convex and thus satisﬁes the condition of B.1.
Lemme B.2. The function f : R → R deﬁned for α, λ > 0 and b ∈ R by f (x) = α

2 (x − b)2 + λ|x| is α-strongly convex.

2 (cid:107)X − (cid:80)K

Proof. The property of monotone subdifferential states that a function f is α-strongly convex if and only if

∀(x, x(cid:48)),

(cid:104)f (x) − f (x(cid:48)), x − x(cid:48)(cid:105) ≥ α(cid:107)x − x(cid:48)(cid:107)2
2

∂f =

α(x − b) + λsign(x)
−αb + λt, for t ∈ (cid:2)−1, 1(cid:3)

if x (cid:54)= 0
if x = 0






Let us deﬁne the subdifferential of f :

The inequality is an equality for x = x(cid:48).
If x(cid:48) = 0, we get for |t| ≤ 1:

If x(cid:48) (cid:54)= 0, we get:

(cid:104)α(x − b) + λsign(x) + αb − λt), x(cid:105) = αx2 + λ (|x| − tx)
(cid:125)

(cid:124)

(cid:123)(cid:122)
≥0

≥ αx2 = α(x − x(cid:48))2

(cid:104)α(x − x(cid:48)) + λ(sign(x) − sign(x(cid:48)), x − x(cid:48)(cid:105) = α(x − x(cid:48))2 + λ(|x| + |x(cid:48)| − sign(x)x(cid:48) − sign(x(cid:48))x)

(|x| + |x(cid:48)|)
= α(x − x(cid:48))2 + λ(1 − sign(x)sign(x(cid:48)))
(cid:125)
(cid:124)

(cid:123)(cid:122)
≥0

≥ α(x − x(cid:48))2

Thus f is α-strongly convex.

This can be applied to the function ek, t deﬁned in (9), showing that the problem in one coordinate (k, t) is (cid:107)DDDk(cid:107)2
convex.

2-strongly

C. Proof of convergence for DICOD (Theorem 2)

C.1. Lower bound on interference

We deﬁne

Ck0,k1[t] =

Sk0,k1[t]
(cid:107)DDDk0(cid:107)2(cid:107)DDDk1(cid:107)2

Let us ﬁrst show how Ck0,k1 controls the interfering cost update.
Proposition 1. In case of concurrent update for coefﬁcients (k0, t0) and (k1, t1), the cost update ∆Ek0k1 [t0, t1] is bounded
as

∆Ek0k1[t0, t1] ≥∆Ek0[t0] + ∆Ek1 [t1] − 2Ck0k1[t0 − t1](cid:112)∆Ek0[t0]∆Ek1[t1].

Proof. The problem in one coordinate (k, t) given all the other can be reduced (9). Simple computations show that:

We have shown in Lemme B.2 that ek,t is (cid:107)DDDk(cid:107)2
fact that Z (cid:48)

k[t] is optimal for ek,t

2-Strong convex. Thus by deﬁnition of the strong convexity, and using the

∆Ek[t] = ek,t(Zk[t]) − ek,t(Z (cid:48)

k[t]).

|ek,t(Zk[t]) − ek,t(Z (cid:48)

k[t])| ≥

(Zk[t] − Z (cid:48)

k[t])2

(cid:107)DDDk(cid:107)2
2
2

i.e., |∆Zk[t]| ≤

, and the result is obtained using this inequality with Proposition A.2.

√

2∆Ek[t]
(cid:107)DDDk(cid:107)2

(11)

(12)

(13)

DICOD: Distributed Convolutional Sparse Coding

C.2. Proof of Theorem 2

Theorem 2. If the following hypothesis are veriﬁed
H1. For all (k0,t0),(k1,t1) such that t0(cid:54)=t1,

|Ck0k1[t0 − t1]| < 1 .
H2. There exists A ∈ N∗ such that all cores m ∈ [M ] are updated at least once between iteration q and q + A if the
solution is not locally optimal, i.e. ∆Zk[t] = 0 for all (k, t) ∈ Cm
H3. The delay in communication between the processes is inferior to the update time.

Then, the DICOD algorithm using the greedy updates (k0, t0) = arg max(k,t)∈Cm |∆Zk[t]| converges to the optimal
solution Z ∗ of (2).

Proof. If several updates (k0, t0), (k1, t1), . . . (km, tm) are updated in parallel without interference, then the update is
equivalent to the sequential updates of each (kq, tq). We thus consider that for each step i, without loss of generality that

∆E(i) =






[t0],

∆E(i)
k0
∆E(i)
k0k1

[t0, t1],

if there is no interference
otherwise

k [t] = 0, then Z (i) is coordinate wise optimal. Using the result from B.1, Z (i) is optimal. Thus if Z (i) is

If ∀(k, t), ∆Z (i)
not optimal, ∆E(i)
k0

[t0] > 0.

Using Proposition 1 and (H1)

so the update ∆E(i) is positive.

∆E(i)
k0k1

[t0, t1] >

∆E(i)
k0

[t0] −

∆E(i)
k1

[t1]

≥ 0 ,

(cid:32)(cid:113)

(cid:113)

(cid:33)2

The sequence (E(Z (i)))n is decreasing and bounded by 0. It converges to E∗ and ∆E(i)−−−−→
0. As lim(cid:107)z(cid:107)∞→∞ E(z) =
n→∞
+∞, there exist M ≥ 0, i0 ≥ 0 such that (cid:107)Z (i)(cid:107)∞ ≤ M for all i > i0. Thus, there exist a subsequence (Z iq )q such that
Z iq −−−→
q→∞

¯z. By continuity of E, E∗ = E(¯z)

Then, we show that Z (i) converges to a point ¯z such that each coordinate is optimal for the one coordinate problem. By
Proposition 1, the sequence (Z (i))i is (cid:96)∞-bounded. It admits at least a limit point Z (iq) −−−→
¯z. Moreover, the sequence
q→∞

Z (i) is a Cauchy sequence for the norm (cid:96)∞ as for p, q > 0

(cid:107)Z (p) − Z (q)(cid:107)2

∞ ≤

∆E(l)

(cid:88)

l>q
(cid:16)

2
(cid:107)D(cid:107)2

2
(cid:107)D(cid:107)2

∞,2

∞,2

=

E(Z q) − E∗(cid:17)

→
q→∞

0

Thus Z (i) converges to ¯z.
Let m denote one of the M cores and (k, t) be coordinates in Cm. We consider the function hk,t : RK×L → R such that

We recall that

h(z) = Z (cid:48)

k[t] =

Sh(βk[t], λ) .

1
(cid:107)DDDk(cid:107)2
2

βk[t](Z) =

(cid:102)DDDk ∗

X −

Zk(cid:48) ∗ DDDk(cid:48) − Φt

(cid:0)Zk

(cid:1) ∗ DDDk

[t]
















K
(cid:88)

k(cid:48)=1
k(cid:48)(cid:54)=k
















DICOD: Distributed Convolutional Sparse Coding

The function φ : Z → βk[t](Z) is linear. As Sh is continuous in its ﬁrst coordinate and h(Z) = Sh(φ(Z), λ), the function
hk,t is continuous. For (k, t) ∈ Cm, the gap between ¯Zk[t] and ¯Z (cid:48)

k[t] is such that

Using (H2), for all i ∈ N, if Z (i)
is (kqi , tqi) ∈ Cm. As no update are done on Cm coefﬁcients between the updates i and qi, Z (i)
of the update,

k [t] is not optimal, there exist qi ∈ [i, i + A] such that the updated coefﬁcient at iteration qi
[t]. By deﬁnition

k [t] = Z (qi)

k

| ¯Zk[t] − ¯Z (cid:48)

k[t]| = | ¯Zk[t] − hk,t( ¯Zk[t])|
|Z (i)
k [t] − h(Z (i)
k [t] − y(i)

= lim
i→∞

|Z (i)

k [t]|

= lim
i→∞

k [t])|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Z (i)
k [t] − y(i)

(cid:12)
(cid:12)
k [t]
(cid:12)
(cid:12)

=

≤

≤

Z (qi)
k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Z (qi)
(cid:12)
kqi
(cid:12)
√

[t] − y(qi)

k

(cid:12)
(cid:12)
[t]
(cid:12)
(cid:12)

[tqi] − y(qi)
kqi

(cid:12)
(cid:12)
[tqi]
(cid:12)
(cid:12)

2∆E(qi)
(cid:107)DDDkqi

(cid:107)

→
i→∞

0

(14)

(greedy updates)

(Proposition 1)

(cid:12)
(cid:12)
Using this results with (14),
(cid:12)
is optimal for the problem (2).

¯Zk[t] − ¯yk[t]

(cid:12)
(cid:12)
(cid:12) = 0. This proves that ¯z is optimal in each coordinate. By B.1, the limit point ¯z

D. Proof of DICOD speedup (Theorem 3)

Theorem 3. Let α = W
uniformly in time, the expected speedup E[Scd(M )] is lower bounded by

T and M ∈ N∗ . If αM < 1

4 and if the non-zero coefﬁcients of the sparse code are distributed

E[Scd(M )] ≥ M 2(1 − 2α2M 2 (cid:16)

1 + 2α2M 2(cid:17) M

2 −1

) .

T and M ∈ N∗ . If αM < 1
Theorem 3. Let α = W
uniformly in time, the expected speedup E[Sdicod(M )] is lower bounded by

4 and if the non zero coefﬁcients of the sparse code are distributed

E[Sdicod(M )] ≥ M 2(1 − 2α2M 2 (cid:16)

1 + 2α2M 2(cid:17) M

2 −1

) .

This result can be simpliﬁed when the interference probability (αM )2 is small.
Corollary 4. The expected speedup E[Scd(M )] when M α → 0 is such that

Corollary 4. Under the same hypothesis, the expected speedup E[Sdicod(M )] when (M α)2 → 0 is

E[Scd(M )] (cid:38)
α→0

M 2(1 − 2α2M 2 + O(α4M 4)) .

E[Sdicod(M )] (cid:38)
α→0

M 2(1 − 2α2M 2 + O(α4M 4)) .

Proof. There are two aspects involved in DICOD speedup: the computational complexity and the acceleration due to the
parallel updates.

As stated in Section 3, the complexity of each iteration for CD is linear with the length of the input signal T . The dominant
operation is the one that ﬁnd the maximal coordinate. In DICOD, each core runs the same iterations on a segment of size

DICOD: Distributed Convolutional Sparse Coding

T

M . The hypothesis αM < 1
one core of DICOD can run M local iteration as the complexity of each iteration is divided by M .

4 ensures that the dominant operation is ﬁnding the maxima. Thus, when CD run one iteration,

The other aspect of the acceleration is the parallel update of Z. All the cores perform their update simultaneously and each
update happening without interference can be considered as a sequential update. Interfering updates do not degrade the
cost. Thus, one iteration of DICOD with Ni interference is equivalent to M − 2 ∗ Ninterf iterations using CD and thus,

E[Ndicod] = M − 2 ∗ E[Ninterf ]

(15)

The probability of interference depends on the ratio between the length of the segments used for each cores and the size
If all the updates are spread uniformly on each segment, the probability of interference between 2
of the dictionary.
(cid:16) MW
T

neighboring cores is

= (M α)2.

(cid:17)2

A process can only creates one interference with one of its neighbors. Thus, an upper bound on the probability to get
exactly j ∈ [0, M

2 ] interferences is

P(Ni = j) ≤

(2α2M 2)j

(cid:19)

(cid:18) M
2
j

Using this result, we can upper bound the expected number of interferences for the algorithm

E[Ninterf ] =

jP(Ninterf = j) , ≤

M

2(cid:88)

j=1

M

2(cid:88)

j=1

j

(cid:19)

(2α2M 2)j,

(cid:18) M
2
j
1 + 2α2M 2(cid:17) M

2 −1

.

≤ α2M 3 (cid:16)

Pluggin this result in (15) gives us:

E[Ndicod] ≥ M (1 − 2α2M 2 (cid:16)

1 + 2α2M 2(cid:17) M

2 −1

) ,

M (1 − 2α2M 2 + O(α4M 4)) .

(cid:38)
α→0

(16)

Finally, by combining the two source of speedup, we obtain the desired result.

E[Sdicod(M )] ≥ M 2(1 − 2α2M 2 (cid:16)

1 + 2α2M 2(cid:17) M

2 −1

) .

