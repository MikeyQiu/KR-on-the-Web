ARXIV PREPRINT

1

Multispectral Video Fusion for
Non-contact Monitoring of
Respiratory Rate and Apnea

Gaetano Scebba, Giulia Da Poian, and Walter Karlen

0
2
0
2
 
r
p
A
 
1
2
 
 
]
P
S
.
s
s
e
e
[
 
 
1
v
4
3
8
9
0
.
4
0
0
2
:
v
i
X
r
a

Abstract— Continuous monitoring of respiratory activity
is desirable in many clinical applications to detect respi-
ratory events. Non-contact monitoring of respiration can
be achieved with near- and far-infrared spectrum cameras.
However, current technologies are not sufﬁciently robust
to be used in clinical applications. For example, they fail
to estimate an accurate respiratory rate (RR) during apnea.
We present a novel algorithm based on multispectral data
fusion that aims at estimating RR also during apnea. The
algorithm independently addresses the RR estimation and
apnea detection tasks. Respiratory information is extracted
from multiple sources and fed into an RR estimator and an
apnea detector whose results are fused into a ﬁnal respi-
ratory activity estimation. We evaluated the system retro-
spectively using data from 30 healthy adults who performed
diverse controlled breathing tasks while lying supine in a
dark room and reproduced central and obstructive apneic
events. Combining multiple respiratory information from
multispectral cameras improved the root mean square error
(RMSE) accuracy of the RR estimation from up to 4.64
monospectral data down to 1.60 breaths/min. The median
F1 scores for classifying obstructive (0.75 to 0.86) and
central apnea (0.75 to 0.93) also improved. Furthermore, the
independent consideration of apnea detection led to a more
robust system (RMSE of 4.44 vs. 7.96 breaths/min). Our
ﬁndings may represent a step towards the use of cameras
for vital sign monitoring in medical applications.

Index Terms— Respiratory rate, apnea, non-contact
physiological monitoring, nearables, multispectral fusion,
thermal imaging.

I. INTRODUCTION

M ONITORING respiratory activity is critical for assess-

ing the state of health in humans. Respiratory rate (RR)
is an important vital sign and a strong predictor of severe ill-
ness [1]. While normal RR ranges between 12-20 breaths/min
in healthy adults at rest, RR outside of this range strongly
correlates with speciﬁc adverse events, such as congestive
heart failure [2]. Furthermore, monitoring respiratory activity
is used to diagnose breathing disorders and lung diseases, such
as pneumonia and central or obstructive sleep apnea. Despite
its clinical relevance, the continuous and accurate monitoring
of RR is highly undervalued [3].

This work was supported by the Swiss National Science Foundation

(SNSF) under Grant 150640.

G. Scebba, G. Da Poian and W. Karlen are with the Mobile Health
Systems Lab, Institute of Robotics and Intelligent Systems, Department
of Health Sciences and Technology, ETH Zurich, Switzerland (e-mails:
gaetano.scebba@hest.ethz.ch, walter.karlen@ieee.org)

Fig. 1: Quantifying respiratory activity using far-infrared (FIR)
and near-infrared (NIR) cameras. Dedicated fusion models
exploit
the respiratory information from the multispectral
cameras and estimate respiratory rate (RR) and detect apnea.

Current approaches to objectively assess respiratory activity
require direct contact with the patient. The obtrusiveness of
these approaches reduces their acceptability in the clinical
and telemedicine settings [4]. Therefore, there has been an
increased interest
in developing nearables for non-contact,
unobtrusive monitoring of vital signs. One of these approaches
includes the estimation of RR using cameras [5]. The ubiqui-
tousness of cameras and the advancement in computer vision
enable physiological monitoring in more natural settings at
home or in emergency settings for triage of potentially conta-
gious patients [6]. Furthermore, the absence of direct contact
with the skin allows for the monitoring of pre-term infants and
other patients who suffer from skin irritations due to prolonged
use of electrodes [7]. Another advantage of using cameras is
the direct recognition of motion artifacts that reduce the vital
sign quality and trigger false alarms [8].

Currently, cameras to monitor RR are not widely adopted
in clinical settings as they lack accuracy during continuous
operation. The robustness of the measurement method is
impacted in many ways. RGB cameras operating in the vis-
ible spectrum require moderate illumination, preventing their
application in dark environments [9]. Most far-infrared (FIR)
cameras are high-end products and excessively expensive,
whereas consumer-accessible FIR cameras are challenged by
low pixel resolution and sampling rates, which makes the
localization of a speciﬁc region of interest (ROI) difﬁcult [10].
An important limitation of most current algorithms for camera-
based respiration monitoring is that they have only been tested
under optimal conditions and with a low number of subjects.
Furthermore, many of these algorithms only implement a

2

ARXIV PREPRINT

trivial apnea detection that fails under the presence of noise.
We propose a novel non-contact system to monitor res-
piratory activity by combining near-infrared (NIR) and FIR
cameras (Fig. 1). Our innovative approach extracts multiple
streams of respiratory information from independent video
modalities and then applies two dedicated data fusion models
to determine whether the subject is breathing (apnea detec-
tion), and at which frequency (RR estimation). We designed
and conducted a series of experiments to demonstrate that
fusing multispectral videos leads to a signiﬁcant improvement
in the estimation of RR when compared to state-of-the-art
methods, and increases the robustness of RR estimates when
apnea events are present.

A. Multisensory fusion

II. BACKGROUND

Data fusion has been successfully implemented in biomed-
ical signal processing for RR estimation with conventional
contact sensors [11]. Numerous approaches fuse either the
RR from photoplethysmographic (PPG) [12]–[14] or electro-
cardiographic (ECG) waveform modulations [15]–[17]. Others
propose combining multiple sensor modalities, such as ECG
and PPG [18]–[20], ECG and thoracic impedance [21], and
ECG and accelerometry [22]. Fusing multiple vital signs is
also advantageous in detecting apneic events. Furthermore,
models exist that combine oxygen saturation with other modal-
ities, such as ECG [23], [24], electroencephalography [25],
respiratory effort and ECG [26], nasal airﬂow [27] or tracheal
sounds [28].

In this work, we build on several of these approaches
that have demonstrated the beneﬁt of fusing multisensory
information to increase accuracy and robustness of the RR
estimation and apnea detection. However, while existing lit-
erature focused on vital signs obtained from contact sensors,
our data fusion algorithms leverage respiratory signs that can
be solely obtained from video data, thus enabling non-contact
estimation of respiratory activity.

B. Respiratory activity detectable with digital cameras

Respiratory activity and its absence are characterized by
speciﬁc patterns that can be extracted through appropriate
analysis of videos recorded with RGB, NIR or FIR cameras.
Four respiratory signs are directly observable: 1) respiratory
induced motion, 2) thermal airﬂow variation, 3) respiratory
plethysmographic modulation, and 4) apneic events.

1) Respiratory induced motion (RM): RM of the torso is the
most noticeable respiratory sign and is visible to the naked
eye. It
is produced by the volume variation of the lungs
that generate the inﬂow and outﬂow of air. In particular,
the lungs can be expanded and contracted generating the
periodic motion of the torso in two distinct sections [29]. The
motion of the abdominal wall is caused by the diaphragm that
vertically lengthens and shortens the lungs, and the elevation
and depression of the rib cage increases and decreases the
anteroposterior diameter of the lungs [30]. When an obstruc-
tive apneic event occurs, the collapse of the upper airways
prevents air exchange, but a persistent respiratory movement

of the torso is still present [31]. However, during a central
apneic event, the RM is completely absent. Several algorithms
have been proposed to quantify the RM. Differential image
processing [32], [33] or motion tracking of salient points
extracted from the chest ROI with optical ﬂow [34] and dense
optical ﬂow [35] enable the extraction of respiratory activity.
2) Thermal airﬂow (TA) signal: The temperature ﬂuctuations
of the airways due to airﬂow are a respiratory sign that can be
detected with FIR cameras [36]. The physical phenomenon is
based on the radiative and convective heat transfer component
during the breathing cycle [37], which results in a periodic
increase and decrease of the temperature at the tissues around
the nasal cavity. These observable temperature ﬂuctuations
are quantiﬁable in a FIR video as pixel intensity variations
of the nostrils ROI [37]–[39]. During obstructive and central
apnea there is a suspension of the air exchange between lungs
and atmosphere and consequently, no temperature variation is
observable at the nostrils.

3) Respiratory plethysmographic (RP) modulation: The var-
ious respiratory modulations of the PPG waveform are well
described in literature [12]. The most prominent respiratory
sign observable with cameras is the PPG baseline modulation.
Each respiratory cycle causes variations of the intra-thoracic
pressure that induce changes in the blood exchange between
the pulmonary and systemic circulation. This phenomenon
results in a periodic variation of the baseline of the PPG
waveform. In the event of obstructive apnea, the persistent mo-
tion of the torso continues to induce the intrathoracic pressure
variations. During a central apnea event, the RP modulation
is drastically reduced in amplitude due to the absence of any
RM. The PPG baseline modulation can be quantiﬁed through
the linear combination of the RGB components in a video of
skin [5]. Other studies apply blind-source separation [40], [41],
continuous wavelet ﬁltering [42], or auto-regressive models [4]
to extract the RP modulation.

4) Apneic events: Apneic events are deﬁned as the suspen-
sion of respiratory activity lasting for more than 10 seconds
[43]. Apnea detection algorithms frequently focus on detecting
an absence of RR [5], [44]. However, this is not reliable as
frequency derived RR does not tend towards zero if there
is noise present. Furthermore, this simplistic view on apnea
detection prevents the distinction between the absence of
breathing and the absence of a respiratory signal, as well as
between obstructive and central apnea.

Pereira et al. have exploited the potential of combining
multiple respiratory sources of information obtained from
cameras [45]. They propose a black-box fusion approach, in
which the TA ﬂuctuations and the RM obtained from high-end
FIR cameras are considered as independent data sources [44].
In contrast to the above described work, we present the
ﬁrst camera-based approach to monitor a broader range of
respiratory activity by combining multiple types of respiratory
modulations extracted from multiple modalities. Our approach
simultaneously extracts multiple respiratory signals and with
specialized data fusion models estimates the RR and classiﬁes
apnea. With a third fusion stage we combine that information
to produce an apnea-aware RR quantiﬁcation.

SCEBBA et al.: MULTISPECTRAL VIDEO FUSION FOR NON-CONTACT MONITORING OF RESPIRATORY RATE AND APNEA (2020)

3

Fig. 2: Multispectral camera-based respiratory monitoring system. (a) The videos from far-infrared (FIR) and near-infrared
(NIR) cameras are processed with a (b) Multispectral Region of Interest (ROI) localization algorithm [10]. The localized ROIs
are used to extract (c) the Thermal Airﬂow (TA) signal from the nostril ROI, and (d) the Respiratory Motion signal both from
the FIR and NIR chest ROIs. (e) In the Signal Quality based fusion (SQb Fusion), (e1) Respiratory Rate (RR) and Signal to
Noise Ratio (SNR) are computed by the frequency analysis of the TA, and the Respiratory Motion from the FIR (RMFIR) and
the NIR (RMNIR). (e2) The weighted median combines all RR estimates using their singnal-to-noise-ratio (SNR) as weights
and calculates a RR estimate. (f) The apnea detector (f1) extracts time and frequency features from the TA, RMNIR, and RMFIR
signals and (f2) classiﬁes the signals into apnea or breathing with an ensemble of support vector machines. (g) The smart
signal quality based fusion (S2Fusion) fuses the results from the SQb Fusion and the apnea classiﬁer (h) to obtain an apnea
sensitive estimation of RR.

III. ALGORITHM DEVELOPMENT

We developed a system for estimating respiratory activity
from synchronized NIR and FIR cameras (Fig. 2). We chose
this pair as they can be operated independently of visible
light. Our algorithmic approach consisted of three main steps.
Firstly, we localized the nostril and chest ROIs in both the
NIR and FIR spectra. Secondly, three respiratory signals were
extracted from these ROIs. Finally, using our novel Smart Sig-
nal quality based Fusion (S2Fusion), a RR estimate obtained
from a modulation fusion model and an apnea classiﬁcation
model were combined to obtain a more accurate quantiﬁcation
of the respiratory activity.

A. Multispectral ROI localization

The detection and tracking of the ROIs was built upon a
multispectral ROI detector that we previously proposed in [10].
1) Image registration: In order to ensure the pixel-to-pixel
correlation between the FIR and NIR frames, we applied
a spatial image registration model, consisting of an afﬁne
transformation T , such as

T =

(cid:20)sx
0

0
sy

(cid:21)

,

tx
ty

(1)

where sx and sy speciﬁed the scaling factors and tx and
ty the translation factors. The deﬁnition of these factors was
dependent on the distance between the object of interest and
the camera [10]. Therefore, we deﬁned the transformation
model during the calibration phase of the cameras and the

same factors were applied for all the recordings. The spatial
alignment between FIR and NIR frames enabled the projection
of the ROIs identiﬁed in the NIR frame to the FIR frame.

2) Detection and tracking: Five facial landmarks were de-
tected in the NIR frame by applying a cascade convolutional
neural network (CCNN) [46]. We obtained the nostril ROI as
a rectangle centered over the nose landmark, with 20 pixel
height and 15 pixel width. To locate the chest ROI, we used
the coordinates of the chin landmark, which were derived as
the bottom ordinate of the face box obtained from the CCNN
model. The ﬁnal chest ROI was deﬁned using frame size
dependent geometrical relations.

After successful ROI detection, an object tracker compen-
sated for the movements of the subjects during the recording.
As the chest ROI was deﬁned by geometrical dependencies
between the nostrils and face position, we tracked the nostril
ROI on the NIR video only. The tracker ﬁrst extracted feature
points from the ROI using the minimum eigenvalue algorithm
[47], then tracked these points with the Kanade-Lucas-Tomasi
single points tracker [48], followed by correcting potential
tracking failures with the method described by Kalal et al.
[49]. To further mitigate potential
tracking errors, we re-
triggered the detection of the nostril ROI every 10 seconds.

B. Signal extraction

We extracted the RM from NIR and FIR and the TA
from FIR videos. As a preprocessing step, we applied median
ﬁltering with a 3×3 mask to each NIR frame, and we enhanced

4

ARXIV PREPRINT

the contrast in each FIR frame by normalizing the pixel value
to the interval [0, 1]. Following the preprocessing, we extracted
the respiratory signals from each of the ROIs. The signals were
then ﬁltered and the RR was extracted from the maximal power
spectral density.

1) Thermal airﬂow: The thermal ﬂuctuations at the nostril
ROI resulted in a pixel intensity variation in the FIR video.
Thus, given a correctly identiﬁed ROI at time t, we computed
the TAFIR signal as the spatial average of the pixel intensity
within the nostril ROI (Fig. 2 c) such as,

T AFIR(t) =

IROI(x, y, t),

(2)

1
W H

W
(cid:88)

H
(cid:88)

x

y

where W and H are the width and height of the ROI, and
IROI(x, y, t) is the pixel intensity at pixel x, y at time t.

2) Respiratory induced motion: The periodic motion of the
torso was extracted using Farnebck dense optical ﬂow algo-
rithm [50]. In particular, we used the vertical velocity proﬁles
of each tracked pixel, which were obtained as the ratio between
the pixel displacement derived by the optical ﬂow algorithm
and the time between two consecutive frames. In contrast to
estimating the optical ﬂow for all the pixels of each frame [34],
we restricted the optical ﬂow calculation to the pixels within
the chest ROI only. For this, we divided the ROI into a 5×7
cell grid and averaged the velocity proﬁles within each cell,
obtaining 35 velocity proﬁles (Fig. 2 d). To reduce the number
of the velocity proﬁles for further processing, we calculated
the standard deviation σp of each proﬁle over a window of 12
s and then excluded the proﬁles that did not meet the criterion,
such as

σp < median(Σp) + 2 · IQR(Σp),

(3)

where Σp is a vector containing the standard deviation of the
35 velocity proﬁles and IQR was the interquartile range. The
RM signal was obtained by averaging the remaining velocity
proﬁles. As the multispectral ROI localization enabled the
identiﬁcation of the chest ROI in the NIR and FIR frames, we
extracted a RM signal from each spectral video, thus obtaining
the RMNIR and RMFIR signals (Fig. 2 d).

3) RR estimation: The TAFIR, RMNIR, and RMFIR signals
were processed to estimate the respective RR values. For each
signal, we applied the regularized least squares detrending
algorithm introduced by Tarvainen et al. [51], with the smooth-
ing parameter λ = 300. In the case of the TA signals, we also
applied a Butterworth band pass ﬁlter, with ﬁlter order set to
2 and frequency range to 0.015 0.75 Hz. We did not apply
any ﬁlter to the RMNIR and RMFIR signals to preserve their
information content for the apneic detection task. The signals
were then windowed and the Lomb-Scargle power spectral
density (PSD) [52], [53] computed. The frequency with the
highest power density in the range of 0.015 0.75 Hz was
selected and converted to breaths/min. The Lomb-Scargle PSD
estimator was chosen because cameras often have unstable
frame rates and it did not require an evenly sampled time
series.

C. Smart signal quality based fusion (S2Fusion)

The S2Fusion was designed as a multilevel data fusion
algorithm that processes respiratory signals extracted from
independent multispectral videos in order to quantify the res-
piratory activity. Practically, it consisted of three components:
1) a signal quality based fusion (SQb Fusion) to merge the
RR estimates computed from each respiratory signal, 2) an
apnea detector to extract temporal and spectral features from
the respiratory signals and classify them as either an apnea
or respiratory epoch, and 3) a ﬁnal fusion to combine both
models into a respiratory activity estimation (Fig. 2 g).

1) Signal quality based fusion: The SQb Fusion algorithm
estimated the RR considering the signal quality from each of
the three respiratory signals (TAFIR, RMNIR, and RMFIR). The
signal-to-noise ratio was computed such as

SNRi =






P (f )

(cid:80)fpeak+ k
2
fpeak− k
2
P (f ) + (cid:80)fs

(cid:80)fpeak− k
0

2






i

,

(4)

P (f )

f

peak+ k
2

where P (f ) was the Lomb-Scargle PSD, fpeak was the
frequency of
the peak with the highest power density,
k = 2 breaths/min was the margin parameter [54], and fs the
sampling frequency. RRSQb was obtained from the weighted
median using the RRi from each signal i and the weights wi
such as

wi =

SNRi
i SNRi

(cid:80)

· M,

(5)

where M = 24 was an empirically determined scaling factor.
2) Apnea detector: We separated respiratory epochs from
apneic epochs with an ensemble of support vector machines
(SVMs, Fig. S1, Supplementary Material). In addition to the
SNR used for the SQb fusion, we extracted time domain
features such as the number of mean crossings, variance, and
standard deviation from the 12 s windows. These features
highlighted the morphological changes of the signals during
the apneic events. In contrast, the frequency domain feature
SNR described the periodicity of the breathing activity. All
features were standardized to have zero mean and unit variance
before they were fed into the SVM ensemble classiﬁer. Test
features were standardized based on training data statistics.
The SVM consisted of one layer of regression models Ri
and one classiﬁcation model C (Fig. S1, supplementary ma-
terial). Each Ri extracted the posterior probability P r(Y =
apnea|Xi) for the set features Xi to belong to an apneic
epoch. This way, each Ri consisted of an expert model for
each respiratory signal. The purpose of C was to aggregate
the P r from each expert Ri and to compute the ﬁnal output
yapnea. The design of the SVM ensemble was inspired by the
work of Schwab et al. on multitasking networks [55].

IV. METHODS AND MATERIALS

We conducted a series of experiments to test whether 1) the
SQb Fusion algorithm is more accurate in estimating the RR
compared to a monospectral approach, 2) the combination of
respiratory sources extracted from independent multispectral
cameras increases the detection rate of apneic events, 3) the
S2Fusion algorithm produces robust RR during apneic events

SCEBBA et al.: MULTISPECTRAL VIDEO FUSION FOR NON-CONTACT MONITORING OF RESPIRATORY RATE AND APNEA (2020)

5

and can substantially improve the accuracy during recordings
that include central or obstructive apneic events, and 4) there
is a performance bias towards a subject’s sex.

A. Experimental protocol

To evaluate the performance of our video-based respiratory
monitoring algorithm, we designed an experimental protocol
consisting of one task where participants were breathing
spontaneously and four tasks where they followed different
breathing patterns that included apneic events. Participants lay
on a bed in a supine position and for each task breathed
through the nose. During the tasks with apneic events, they
breathed following the rhythm of a metronome. We only
considered the supine body posture because it is the most
common position leading to apneic events during sleep [56].
To challenge our algorithms, all videos were recorded in a
dark environment (< 5 Lux).

Spontaneous Breathing Participants breathed sponta-
neously for 4.5 min. During this task, participants turned their
head 45o left and right for 2 min each (Fig. 3 a).

Central Apnea Participants breathed for 3 min with a
constant RR of 10 breaths/min and performed 3 central apneic
events, each lasting 20 s (Fig. 3 b).

Obstructive Apnea Participants breathed for 3 mins with a
constant RR of 10 breaths/min and performed 3 obstructive
apneic events, each lasting 20 s. For each apneic event
occurred, participants were instructed to simulate a blockage
of the airways, and to keep the thorax moving to mimic an
ongoing respiratory effort (Fig. 3 c).

Central Apnea - Blanket Participants breathed for 3 min
with a constant RR of 10 breaths/min and performed 3 central
apneic events, each lasting 20 s. During the entire duration of
this task, participants were covered to the chin with a blanket
to hide the chest contours (Fig. 3 d).

Central Apnea - Arbitrary duration Participants breathed
with a constant RR of 10 breaths/min and 2 central apneic
events whose duration was based on the participants breath
holding capacity (Fig. 3 e).

The study was conducted according to the ethical guide-
lines of Helsinki. Institutional research ethics board approval
was obtained (ETH Zurich EK 2017-N-60). After informed
written consent, healthy volunteers with no history of cardio-
respiratory disease were enrolled in the study.

B. Experimental setup

The setup consisted of two cameras interfaced with a
Raspberry Pi 3 B microcomputer (Raspberry Pi Foundation,
Cambridge, UK). Custom software was developed to ensure
simultaneous recording of the cameras, video compression,
and data storing on the SD card. NIR videos were recorded
with the See3cam CU40 (econ-Systems, Chennai, India) with
a frame rate of 15 Hz and a resolution of 336×190 pixels. FIR
videos were recorded with a FLIR Lepton version 3.5 (FLIR
Systems Inc., Wilsonville, United States) with an average
frame rate of 8.7 Hz and a resolution of 160×120 pixels.
We used a NIR LED array to enable recordings in insufﬁcient
lighting.

Fig. 3: The experimental protocol consisted of 5 different
tasks: (a) spontaneous breathing for 4.5 minutes; constant
breathing at 10 breaths/min interrupted by apneic events that
simulated (b) central, (c) obstructive, (d) central while the
body was covered with a blanket, and (e) central with arbitrary
duration based on subject breath holding capacity.

Reference respiratory effort was recorded using a certiﬁed
ezRIP module (Philips Respironics, Pennsylvania, USA) at a
sampling rate of 50 Hz. The device used two elastic bands
equipped with piezo-resistive sensors placed over the thorax
and abdomen, which is one of the recommended setups for
measuring respiratory effort in clinical polysomnography [43].
Collected videos and reference respiratory signals were
transferred to a PC, where additional processing and analy-
sis were performed with Matlab R2018b (MathWorks Inc.,
Natick, USA). The sliding window size was set to 12 s for
all processing steps and was recomputed at the lowest camera
sampling rate (FIR, ~8.7 Hz).

C. SVM ensemble training and testing

The adoption of the SVM ensemble for apnea detection
implied a split of the dataset that could ensure a fair evaluation
of the model. For this reason, we applied the leave-one-sub-
ject-out validation scheme, which is a particular case of the
K-fold validation with K equal to the number of subjects. This
validation scheme allowed us to maximize the amount of data
used for training while accounting for potential high variance
problems [57]. At each run, the training and test sets included
the data of N-1 subjects and 1 subject respectively, with N
equal to the number of recruited subjects. The training set
consisted of the recordings obtained from Central Apnea and
Obstructive Apnea, whereas the test set included the recordings
obtained from all the tasks with apneic events. We did not
include the recordings obtained from the Spontaneous Breath-
ing, Central Apnea arbitrary duration, and Central Apnea
covered with blanket tasks in the training set because they
did not provide additional information regarding the signal
morphology of apneic events and would have increased the
class imbalance (Fig. S2, Supplementary Material).

6

D. Evaluation

The performance of the SQb Fusion algorithm for RR esti-
mation was compared against algorithms that estimate the RR
from single respiratory signals before fusion. Additionally, we
implemented two baseline fusion algorithms based on mean
and median of the RR estimates obtained from RMNIR, RMFIR,
and TAFIR signals. The RR estimates of all the evaluated
algorithms were reported independently of their underlying
estimation quality.

The performance of the SVM ensemble for apnea detection,
which was trained with the features extracted from all signals
(RMNIR+RMFIR+TAFIR), was compared to the performance
of three baseline SVM binary classiﬁers, trained with the
features extracted from the individual RMNIR, RMFIR, and
TAFIR signals (Fig. S3, Supplementary Materials).

We evaluated the performance of the S2Fusion to quantify
the respiratory activity on a combined experiment that includes
both the RR estimation and apnea detection tasks. In particular,
we compared it against the SQb Fusion using all the record-
ings that included apneic events. To guarantee an objective
evaluation without overlap between training and test set in the
S2Fusion evaluation experiment, we applied the same leave-
one-subject-out validation scheme introduced in section IV-C.

E. Performance metrics

In order to describe the performance of the evaluated algo-
rithms, we evaluated the accuracy (agreement between the RR
estimates and a reference) and robustness (accuracy distribu-
tion in challenging or altering situations, specifically apneic
events or differences in demographics).

To obtain accuracy, we calculated Bland-Altman plots, Pear-
son’s correlation coefﬁcient, and the root mean square error
(RMSE). As reference RR, we used the RR estimates obtained
from the thorax respiratory effort sensor. For comparison, we
pooled all the estimated RRs within non-overlapping segments
of 15 s and computed the median thereof. Bias and limits of
agreement (LoA) for Bland-Altman analysis were calculated
using the formulas for repeated measurements [58]. Pearson’s
correlation coefﬁcient r was computed using a 95% conﬁ-
dence interval. The RMSE was computed between RRref and
each fusion algorithm across segments for each subject and
overall. F1 score, sensitivity, and speciﬁcity were calculated
to compare the apnea classiﬁcation.

To describe robustness, across-subject RMSE distributions
were split by sex and displayed as boxplots. Also, we com-
pared the across-subject RMSE distributions between the RR
estimates obtained from the S2Fusion and the SQb Fusion al-
gorithms to evaluate each apneic event task. Middle, bottom,
and top horizontal lines of boxes depicted the median, lower,
and upper quartile, respectively, and crosses depicted outliers.
We tested the normality of the RMSE distributions using the
Shapiro-Wilk test and compared them with the Wilcoxon Rank
Sum test. Multiple distribution comparisons were Bonferroni
corrected. Signiﬁcance levels were set to p < 0.001 (***) or
p < 0.05 (*) unless stated otherwise.

ARXIV PREPRINT

Fig. 4: Comparison of respiratory signals from one subject
during a central apnea task for (from top) reference respiratory
effort
from the thorax, the abdomen, TAFIR, RMFIR, RMNIR
signals, and respiratory activity (RA) estimations. SQb Fusion
and S2Fusion algorithm were only available after the ﬁrst 12
s window was collected.

V. RESULTS

We obtained videos of 30 healthy participants (17 females
and 13 males, mean age: 27 ± 3 y). Subjects varied in facial
appearance with a Fitzpatrick skin tone between type I and
VI (I:2, II:7, III:19, IV:1, V:1, VI:0). Seven males had facial
hair. A total of 492 minutes of video were recorded and
available for analysis. An illustrative example of the signals
obtained from the reference contact sensors and cameras, as
well as estimation output, is depicted in Fig 4. Motion artifacts
substantially corrupted the recording of one subject.

A. Respiratory rate estimation performance

A total of 135 minutes (540 segments) of Spontaneous
Breathing task were evaluated for the RR estimation accuracy.
The SQb fusion showed the highest accuracy (Fig. 5). This
was supported by the highest correlation (r=0.92),
lowest
RMSE (1.6 breaths/min), and the best agreement from the
Bland-Altman analysis. The bias was 0.47 breaths/min and
the LoA were 3.46 and -2.51 breaths/min, outperforming all

