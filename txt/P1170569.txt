8
1
0
2
 
r
a

M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
1
1
0
0
.
1
1
7
1
:
v
i
X
r
a

Multi-Task Learning by Deep Collaboration and
Application in Facial Landmark Detection

Ludovic Trottier

Philippe Giguère

Brahim Chaib-draa

ludovic.trottier.1@ulaval.ca
{philippe.giguere,brahim.chaib-draa}@ift.ulaval.ca

Laval University, Québec, Canada

Abstract. Convolutional neural networks (CNNs) have become the most
successful approach in many vision-related domains. However, they are
limited to domains where data is abundant. Recent works have looked
at multi-task learning (MTL) to mitigate data scarcity by leveraging
domain-speciﬁc information from related tasks. In this paper, we present
a novel soft-parameter sharing mechanism for CNNs in a MTL setting,
which we refer to as Deep Collaboration. We propose taking into ac-
count the notion that task relevance depends on depth by using lateral
transformation blocs with skip connections. This allows extracting task-
speciﬁc features at various depth without sacriﬁcing features relevant to
all tasks. We show that CNNs connected with our Deep Collaboration
obtain better accuracy on facial landmark detection with related tasks.
We ﬁnally verify that our approach eﬀectively allows knowledge sharing
by showing depth-speciﬁc inﬂuence of tasks that we know are related.

1 Introduction

Over the past few years, Convolutional Neural Networks (CNNs) have become
the leading approach in many vision-related tasks [1]. Their ability to learn
a hierarchy of increasingly abstract concepts allows them to transform com-
plex high-dimensional input images into simple low-dimensional output features.
CNNs have been used in many settings, but their need to have a large amount
of data during training has restricted them to domains where data is abundant.
Optimizing CNNs is tricky not only because of problems like vanishing / ex-
ploding gradients [2], but also because they typically have many parameters to
be learned. While previous works have looked at supervised and unsupervised
pre-training to improve generalization, others have considered casting their orig-
inal single-task problem into a new Multi-Task Learning (MTL) problem [3]. As
Caruana (1998) [4] explained in his seminal work: “MTL improves generalization
by leveraging the domain-speciﬁc information contained in the training signals
of related tasks". Exploring new ways to more eﬃciently gather information from
related tasks would help to further improve generalization on the main one.

MTL has proven its value in several domains over the years. It has become a
dominant ﬁeld of machine learning [5], with many inﬂuential works [6]. Although
MTL dates back several years, recent major advances in Deep Learning (DL)

2

Multi-Task Learning by Deep Collaboration

opened up opportunities for novel contributions. Works on grasping [7], pedes-
trian detection [8], natural language processing [9], face recognition [10][11] and
object detection [12] helped MTL make a resurgence in the DL community. They
have shown the potential of MTL to mitigate data scarcity when training deep
networks, which has inﬂuenced its growing popularity

MTL approaches can generally be divided into two major categories: hard
and soft parameter sharing [13]. Hard-parameter sharing dates back to the orig-
inal work of Caruana (1998) and is the most common of the two. Approaches
in this category have a shared central section with many heads (one per task).
Features from speciﬁc tasks compete together and those relevant to all tasks are
favored. Recent works in DL have shown that hard-parameter sharing can be
successful [14][15][7][11]. However, a too large emphasis on features relevant to all
tasks can be harmful for learning high-level features speciﬁc to a particular task.
These types of speciﬁc features are usually needed to obtain a good representa-
tion for the particular task. Also, shared layers are prone to be contaminated by
noise coming from noxious tasks [16]. These limitations can be detrimental even
though hard-parameter sharing reduces the risk of over-ﬁtting [17].

Soft-parameter sharing has been proposed as an alternative to alleviate these
drawbacks. Approaches in this category substitute the shared central section by
separate task-speciﬁc CNNs, but provide a knowledge sharing mechanism to
connect them. Each CNN can then learn task-speciﬁc features and share their
knowledge without interfering with others. Recent works in this category have
looked at regularizing the distance between task-speciﬁc parameters with a ‘2
norm [18] or a trace norm [19], training shared and private LSTM submod-
ules [16], partitioning the hidden layers into subspaces [20] and regularizing the
FC layers with tensor normal priors [21]. In the domain of continual learning,
progressive network [22] has also shown promising results for sequential transfer
learning, by employing lateral connections to previously learned networks.

In this paper, we present a novel soft-parameter knowledge sharing mecha-
nism for connecting task-speciﬁc CNNs in a MTL framework. We refer to our
approach as Deep Collaboration. We deﬁne connectivity in terms of a collabo-
rative block that uses two non-linear transformations with lateral connections.
One aggregates task-speciﬁc features into global features, and the other merges
back the global features into each task-speciﬁc CNN. Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
We evaluated our approach on the problem of facial landmark detection in a
MTL framework and obtained better results in comparison to other approaches
of the literature. We further assess the objectivity of our training framework by
randomly varying the contribution of each related tasks. Finally, we verify that
our collaborative block enables knowledge sharing with an ablation study that
shows the depth-speciﬁc inﬂuence of tasks that we know are related.

The content of our paper is organized as follows. In Section 2, we present
related works in MTL and facial landmark detection. We elaborate on our ap-
proach in Section 3, and present experimental results in Section 4. We ﬁnally
conclude our paper in Section 5. Our code is available here: [23].

Trottier et al.

3

2 Related Work

2.1 Multi-Task Learning

Our proposed Deep Collaboration knowledge sharing mechanism is related to
other existing approaches. One is Cross-Stitch (XS) [12], which connects task-
speciﬁc CNNs by linearly combining their feature maps at certain depths. One
drawback of XS is that it is limited to capture only linear dependencies between
each CNN. In contrast to XS, our approach uses non-linear transformations in
order to capture more complex dependencies.

Another related approach is Tasks-Constrained Deep Convolutional Network
(TCDCN) [15]. The authors proposed an early-stopping criterion to remove aux-
iliary tasks that start to overﬁt before becoming detrimental to the main task.
This approach has however several hyper-parameters to be selected manually.
For each task, it has an hyper-parameter controlling the period length of the
local window and a threshold that stops the task when the criterion exceeds it.
Unlike TCDCN, our approach has no hyper-parameters that need to be tuned to
the tasks at hand. Our collaborative block consists of a series of Batch Normal-
ization [24], ReLU [25], and convolutional layers shaped in a standard setting
that is commonly found in nowadays works.

Our proposed approach is also related to HyperFace [14]. The authors pro-
posed to fuse the layers at various depth and exploit features of diﬀerent levels
of complexity. Their goal was to allow low-level features with better localization
properties to help tasks such as landmark localization and pose detection, and al-
low high-level features with better class-speciﬁc properties to help tasks like face
detection and gender recognition. Although HyperFace is in the hard-parameter
sharing category and is not entirely related to our approach, the idea of feature
fusion is also central in our work. Instead of fusing the features at intermediate
layers of a single CNN, our approach aggregates same-level features of multiple
CNNs, at diﬀerent depth independently.

2.2 Facial Landmark Detection

Facial Landmark Detection (FLD) is an essential component in many face-
related tasks [26][27][28][29]. FLD can be described as follows: given the image of
a face of a person, the goal is to predict the (x, y)-position of speciﬁc landmarks
associated with key features of the visage. Applications such as face recogni-
tion [30], face validation [31], facial feature detection and tacking [32] rely on
the ability to correctly ﬁnd the location of these distinct facial landmarks in order
to succeed. Localizing facial key points like the center of the eyes, the corners of
the mouth, the tip of the nose and the earlobes is however a challenging problem
when many lighting conditions, head poses, facial expressions and occlusions in-
crease diversity of the face images. In addition to integrating this variability into
the estimation process, a FLD model must also take into account a number of
correlated factors. For instance, although both an angry person and a sad person
have frowned eyebrows, an angry person will have pinched lips while a sad person

4

Multi-Task Learning by Deep Collaboration

will have sunken mouth corners [33]. A particularity of datasets geared towards
FLD is that they are particularly well-suited for MTL. In addition to contain-
ing the position of the facial landmarks, these datasets also contain a number
of other labels that can be used to deﬁned auxiliary tasks. Gender recognition,
smile recognition, glasses recognition or face orientation are examples of tasks
often chosen to evaluate MTL approaches.

3 Deep Collaboration

Given T task-speciﬁc Convolutional Neural Networks (CNNs), our goal is to con-
nect them with lateral connections in order to allow domain-speciﬁc information
sharing. We deﬁne connectivity in terms of a collaborative block containing two
distinct non-linear transformations. One aggregates task-speciﬁc features into
global features, and the other merges back the global features into each task-
speciﬁc CNN. Our collaborative block is diﬀerentiable and can be dropped in
any existing CNN architectures as a whole. For this reason, we make no assump-
tion about the structure of the task-speciﬁc CNNs. Our approach can even work
with diﬀerent CNNs, but for the sake of simplicity, we suppose that the CNNs
are the same. We refer to it as the underlying network.

We also decompose the underlying network as a series of blocks. Each block
can be as small as a single layer, as large as the whole network itself, or based
on simple rules, such as grouping all layers with matching spatial dimensions or
grouping every n subsequent layers. The arrangement of the layers into blocks
does not change the composition of the underlying network. We only use it to
make explicit the depth at which we connect the task-speciﬁc CNNs.

Since our collaborative block can be inserted at any depth, we also drop the
depth index on the feature maps to further simplify the equations. As such,
we deﬁne the feature map output of a block at a certain depth as xt, where
t ∈ {1 . . . T } is the task index. Our approach takes as input all task-speciﬁc
feature maps xt and processes them into new feature maps yt as follows:

z = H([x1, . . . , xT ]) ,

yt = ReLU (xt + Ft([xt, z])) ,

(1)

where H and Ft represent the central and the task-speciﬁc aggregations re-
spectively, and [·] denotes depth-wise concatenation. We refer to Eq. (1) as our
collaborative block. The goal of H is to combine all task-speciﬁc feature maps xt
into a global feature map z representing uniﬁed knowledge, while the goal of F
is to merge back the global feature map z with each task-speciﬁc input xt. The
compositional structure of H and F is as follows:

H(·) = (ReLU ◦ BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,
F(·) = (BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,

(2)

(3)

where BN stands for Batch Normalization [24], Conv(h×w) for a standard convo-
lutional layer with ﬁlters of size (h × w), and ◦ is the usual function composition.
The ﬁrst Conv(1×1) layer in H divides the number of feature maps by a factor

Trottier et al.

5

Fig. 1. Example of our collaborative block applied on the feature maps of two task-
speciﬁc networks. The input feature maps (shown in 1(cid:13)) are ﬁrst concatenated depth-
wise and transformed into a global feature map ( 2(cid:13)). The global feature map is then
concatenate with each input feature map individually and transformed into task-speciﬁc
feature maps ( 3(cid:13)). Each resulting feature map is then added back to the input feature
map using a skip connection ( 4(cid:13)), which gives the ﬁnal outputs of the block ( 5(cid:13)).

of 4, while the ﬁrst Conv(1×1) layer in F divides it to match the size of xt. An
illustration of our collaborative block is shown in Fig. 1.

One particularity of our approach is that we use a skip connection in mapping
F. Recent works [34][35][36][37][38] have shown that networks with identity skip
connections are more easily able to learn proper input-output mappings. Inspired
by these works, we opted for an identity skip connection in F in order to more
easily learn the proper mapping to integrate domain-speciﬁc information from
the other tasks. In particular, identity skip connections put an incentive on
learning the identity mapping. We can see this by the ease at which the network
can obtain the identity mapping by simply pushing all the weights in F towards
zero. In our MTL context, the identity mapping can be seen as a way to remove
the inﬂuence of the global features z. This allows to take into account the cases
where integrating z back to the task-speciﬁc features xt would not help.

Another motivation for using an identity skip connection around the global
feature map z comes from the fact that depth inﬂuences the relevance of each
task towards another. Some task-speciﬁc CNNs can beneﬁt more when they
share their low-level features than their high-level features, while other beneﬁt
more in the other way. For instance, tasks such as landmark localization and
pose detection proﬁt more from low-level features containing better localization

6

Multi-Task Learning by Deep Collaboration

Fig. 2. Deep Collaboration Network (DCNet) using ResNet18 as underlying network
in a MTL setting on the MTFL dataset. The top part shows the block structure of
ResNet18 interleaved with our proposed collaborative block, while the bottom part
details each residual and task-speciﬁc FC blocks.

properties, while tasks such as face detection and gender recognition proﬁt more
from class-speciﬁc high-level features. Considering that CNNs learn a hierarchy
of increasingly abstract features, our collaborative block can take into account
task relevance by deactivating a diﬀerent set of residual mappings Ft based on
the depth at which it is inserted. An example of such specialization will be shown
in our ablative study in Section 4.4.

Fig. 2 presents an example of inserting our collaborative block at diﬀerent
depths in a MTL framework on the MTFL dataset [15]. In this particular case,
we opted for a ResNet18 as underlying network. We refer to this network as our
Deep Collaboration Network (DCNet). As we can see in the top part of the ﬁgure,
integrating our approach comes down to interleaving the underlying network
block structure with our collaborative block. Each collaborative block receives
as input the output of each task-speciﬁc block, processes them as detailed in
Eq. (1), and sends the result back to each task-speciﬁc network. Adding our
approach to any underlying network can be done by simply following the same
pattern of interleaving the network block structure with our collaborative block.

4 Experiments

In this section, we detail our Multi-Task Learning (MTL) training framework and
present our experiments in Facial Landmark Detection (FLD) tasks. We further
evaluate the eﬀect of data scarcity on performance and illustrate an example of
knowledge sharing between task-speciﬁc CNNs with an ablation study.

Trottier et al.

7

4.1 Multi-Task Learning Training Framework

The goal of Facial Landmark Detection (FLD) is to predict the (x, y)-position
of speciﬁc landmarks associated with key features of the visage. While the num-
ber and type of landmarks are speciﬁc to each dataset, examples of standard
landmarks to be predicted are the corners of the mouth, the tip of the nose
and the center of the eyes. In addition to the facial landmarks, each dataset
further deﬁnes a number of related tasks. These related tasks also vary from
one dataset to another, and are typically gender recognition, smile recognition,
glasses recognition or face orientation.

On a more technical level, we deﬁne a learning framework in which we treat
each task as a classiﬁcation problem. While this is straightforward for gender,
smile and glasses recognition as they are already classiﬁcation tasks, it is a
bit more tricky for face orientation and FLD. For face orientation, instead of
predicting the roll, yaw and pitch real value as in a regression problem, we
divide each component into 30 degrees wide bins and predict the label of the bin
corresponding to the value. Similarly for FLD, rather than predicting the real
(x, y)-position of each landmark, we divide the image into 1 pixel wide bins and
predict the label of the bin corresponding to the value. Note that we still use
the original real values when comparing our prediction with the ground truth,
so that we incorporate our approximation errors in the ﬁnal score.

We report our results using the landmark failure rate metric [15], which is
deﬁned as follows: we ﬁrst compute the mean distance between the predicted
landmarks and the ground truth landmarks, then normalize it by the inter-
ocular distance from the center of the eyes. A normalized mean distance greater
than 10% is reported as a failure.

4.2 Facial Landmark Detection on the MTFL Task

As a ﬁrst experiment, we performed facial landmark detection on the Multi-Task
Facial Landmark (MTFL) task [15]. The dataset contains 12,995 face images an-
notated with ﬁve facial landmarks and four related attributes of gender, smiling,
wearing glasses and face proﬁle (ﬁve proﬁles in total). The training set has 10,000
images, while the test set has 2,995 images. We perform four sets of experiments
using an ImageNet pre-trained AlexNet, an ImageNet pre-trained ResNet18, an
un-pretrained AlexNet and an un-pretrained ResNet18 as underlying networks.
For AlexNet, we apply our collaborative block after each max pooling layer,
while for ResNet18, we do as shown in Fig. 2.

We compare our approach to several other approaches of the literature. We
include single-task learning (AN-S when using AlexNet as underlying network,
RN-S when using ResNet18), hard-parameter sharing MTL (AN and RN), hard-
parameter sharing MTL where the central section is widened to match the num-
ber of parameters of our approach (ANx and RNx), HyperFace (HF) [14], Tasks-
Constrained Deep Convolutional Network (TCDCN) [15], Cross-Stitch (XS) [12]
and XS widen to match the number of parameters of our approach (XSx). Ex-
cept for TCDCN, we train each network ourselves three times for 300 epochs and

8

Multi-Task Learning by Deep Collaboration

Fig. 3. Landmark failure rates (%) on the MTFL task. The reported values are the
average over the last ﬁve epochs, averaged over three tries. The left plot presents our
results with AlexNet as the underlying network, while the right one with ResNet18.
AN-S and RN-S stand for single-task training, AN and RN for multi-task training with
a single central network, ANx and RNx for multi-task training with a single central
network widen to match the number of parameters of our approach, HF for HyperFace,
TCDCN for [15]’s approach and XS for Cross-Stitch. In each instance, the left column
(blue) is for un-pretrained networks, while the right column (green) is for pre-trained
networks. Our proposed approach obtains the lowest failure rates overall.

report landmark failure rates averaged over the last ﬁve epochs, further averaged
over the three tries.

Fig. 3 presents our FLD results on the MTFL dataset. The left part of the
ﬁgure corresponds to using AlexNet as underlying network, while the right one
corresponds to ResNet18. The top part reports the landmark failure rates, while
the bottom part reports the mean error. In each plot, the left bar (blue) is for
un-pretrained network, while the right bar (green) is for ImageNet pre-trained
network. In addition, Fig. 4 shows example predictions from DCNet with pre-
trained ResNet18 as underlying network. The ﬁrst two examples were reported as
successes, while the last two are failures. The ground truth elements are colored
in green, while our predictions are colored in blue. We also include the labels of
the related tasks: gender, smiling, wearing glasses and face proﬁle.

The results of Fig. 3 show that our proposed approach obtained the lowest
failure rates and mean error in each case. Indeed, our DCNet with un-pretrained
and pre-trained AlexNet as underlying network obtained 19.67% and 19.96%
failure rates respectively, and 14.95% and 13.52% with ResNet18. This is sig-
niﬁcantly lower than the other approaches to which we compare ourselves. For
instance, with AlexNet, HF had 27.75% and 27.32%, XS had 26.41% and 25.65%,

Trottier et al.

9

MTFL

AFLW

Fig. 4. Example predictions of our DCNet with pre-trained ResNet18 as underlying
network on the MTFL and AFLW task. For MTFL, the ﬁrst two examples are successes,
while last two are failure cases. For AFLW, the ﬁrst three examples are successes, while
the last one is a failure case. Elements in green correspond to ground truth, while those
in blue correspond to predictions. Facial landmarks are shown as small dots, and related
tasks labels are displayed on the side. As we can see, over-exposition and tilted face
proﬁle can have a large impact on the prediction quality.

TCDCN had 25.00%1, and XSx had 25.23%. With ResNet18, XS had 18.43% and
15.52% respectively, and XSx had 17.28. We obtained the highest improvements
when using AlexNet as the underlying network when comparing to XS. With un-
pretrained and pre-trained AlexNet, we obtained improvements of 6.74% and
5.69%, while we obtained 3.48% and 2.00% with ResNet18. Performing MTL
with our approach can thus improve performance over using other approaches
of the literature.

Another result that we can see from Fig. 3 is that our soft-parameter shar-
ing approach obtains higher performance than the hard-parameter sharing ap-
proaches with matching number of parameters. For instance, increasing the
number of parameters of hard-parameter sharing AlexNet lowers it error rate
from 28.02% (AN) to 26.88% (ANx), but our approach lowers it further to
19.67%. Similarly, increasing the number of parameters of hard-parameter shar-
ing ResNet18 lowers it error rate from 20.05% (RN) to 16.75% (RNx), but our
approach lowers it further to 14.95%. These results are interesting because they
show that while increasing the number of parameters is an eﬀortless avenue to
improve performance, it has limitations. Developing novel approaches to enhance
network connectivity in a soft-parameter sharing setting seems more rewarding.
This may help to motivate new eﬀorts in this avenue to further leverage domain-
information of related tasks.

1 Zhang et. al only provided results with pre-trained AlexNet [15]

10

Multi-Task Learning by Deep Collaboration

Table 1. Landmark failure rate results on the AFLW dataset using a pre-trained
ResNet18 as underlying network. The presented values are averaged over the last ﬁve
epochs, further averaged over three tries. The ﬁrst column is the train / test ratio,
and the subsequent ones are the networks: single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS). Our approach obtains the best perfor-
mance in all cases, except the ﬁrst one where we observe over-ﬁtting.

Train / Test Ratio

0.1 / 0.9
0.3 / 0.7
0.5 / 0.5
0.7 / 0.3
0.9 / 0.1

RN-S

57.39
31.84
23.41
21.47
13.03

Networks

RN

58.00
32.00
23.31
21.92
12.80

XS

73.06
36.24
26.02
22.37
13.51

Ours

60.64
29.73
20.77
18.50
10.82

4.3 Eﬀect of Data Scarcity on the AFLW Task

As second experiment, we evaluated the inﬂuence of the number of training
examples to simulate data scarcity on the Annotated Facial Landmarks in the
Wild (AFLW) task [39]. The dataset has 21,123 Flickr images, and each image
can contain more than one face. Instead of using the images as provided, we
process them using the available face bounding boxes. We extract all faces with
visible landmarks, which gives a total of 2,111 images. This dataset deﬁnes 21
facial landmarks and has 3 related tasks (gender, wearing glasses and face ori-
entation). For face orientation, we divide the roll, yaw and pitch into 30 degrees
wide bins (14 bins in total), and predict the label corresponding to each real
value.

Our experiment works as follows. With a pre-trained ResNet18 as underlying
network, we compare our approach to single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS) by training on a varying number
of images. We use ﬁve diﬀerent train / test ratios, starting with 0.1 / 0.9 up
to 0.9 / 0.1 by 0.2 increment. In other words, we train each approaches on the
ﬁrst r% of the available images and test on the other (1 − r)%, then repeat
for all the other train / test ratios. We use the same training framework as in
section 4.2. We train each network three times for 300 epochs, and report the
landmark failure rate averaged over the last ﬁve epochs, further averaged over
the three tries. Example predictions are shown in Fig. 4.

As we can see in Table 1, our approach obtained the best performance in
all cases except the ﬁrst one. Indeed, we observe between 1.98% and 6.51%
improvements with train / test ratios from 0.3 / 0.7 to 0.9 / 0.1, while we obtain
a negative relative change of 3.25% with train / test ratio of 0.1 / 0.9. However,
since all multi-task approaches obtained higher failure rates than the single-task
approach, this suggests that the networks are over-ﬁtting the small training set.
Nonetheless, these results show that we can obtain better performance using our
approach.

Trottier et al.

11

Fig. 5. Landmark failure rate improvement (in %) of our approach compared to XS
when sampling random task weights. We used a pre-trained ResNet18 as underlying
network. The histogram at the left and the plot at the top right represents performance
improvement achieved by our proposed approach (positive value means lower failure
rates), while the plot at the bottom right corresponds to the log of the task weights. Our
approach outperformed XS in 86 out of the 100 tries, thus empirically demonstrating
that our learning framework was not unfavorable towards XS and that our approach
is less sensitive to the task weights λ.

One particularity that we observe in Table 1 is that the XS network has
relatively high failure rates. In the previous experiment of Section 4.2, XS had
either similar or better performance than the other approaches (except ours).
This could be due to our current multi-task learning framework that is unfavor-
able towards XS. In order to investigate whether this is the case, we perform
the following additional experiment. Using a pre-trained ResNet18 as underlying
network, we compare our approach to XS by training each network 100 times
using task weights randomly sampled from a log-uniform distribution. Speciﬁ-
cally, we ﬁrst sample from a uniform distribution γ ∼ U(log(1e−4), log(1)), then
use λ = exp(γ) as the weight. We trained both XS and our approach for 300
epochs with the same task weights using a train / test ratio of 0.5 / 0.5.

Figure 5 presents the results of this experiment. The plot at the top right
of the ﬁgure represents the landmark failure rate improvement (in %) of our
approach compared to XS, while the plot at the bottom right corresponds to the
log of the task weights for each try. In 86 out of the 100 tries, our approach had
a positive failure rate improvement, that is, obtained lower failure rates than
XS. As we can see in the histogram at the left of Fig. 5, the improvement rate
is normally distributed around 2.80%, has a median improvement of 3.66% and
a maximum improvement of 9.83%. Even though we sampled at random the
weights of the related tasks, our approach outperforms XS in the majority of the
cases. Our learning framework was therefore not unfavorable toward XS.

4.4 Illustration of Knowledge Sharing With an Ablation Study

As third experiment, we perform an ablation study on the MTFL task [15] with
an un-pretrained ResNet18 as underlying network. The goal of this experiment

12

Multi-Task Learning by Deep Collaboration

Fig. 6. Results of our ablation study on the MTFL dataset with an un-pretrained
ResNet18 as underlying network. We remove each task-speciﬁc features from each re-
spective central aggregation layer and evaluate the eﬀect on landmark failure rate. The
rows represent the task-speciﬁc CNNs, while the columns correspond to the network
block structure. Blocks with a high saturated color were found to have a large impact
on failure rate. In particular, this ablative study shows that the inﬂuence of high-
level face proﬁle features is large within our proposed architecture. This corroborates
with the well-known fact that the location of facial landmarks is closely dependent on
the orientation of the face. This constitutes an empirical evidence of domain-speciﬁc
information sharing via our approach.

is to verify that our collaborative block eﬀectively enables knowledge sharing
between task-speciﬁc CNNs. To do so, we evaluate the impact, on facial landmark
detection, of removing the contribution of each task-speciﬁc features. We zero out
the designated feature map xt before concatenation at the input of the central
aggregation H. The network is trained using the same framework as explained
in Sec. 4.1, and the ablation study is performed at test time on the test set when
training is done.

Figure 6 presents the results of our ablation study. The rows represent each
task-speciﬁc CNN, while the columns correspond to the network block structure.
The blocks are ordered from left (input) to right (output), while the task-speciﬁc
networks are ordered from top (main task) to bottom (related tasks). The color
saturation indicates the inﬂuence of removing the task-speciﬁc feature maps from
the central aggregation at the corresponding depth. A high saturation reﬂects
high inﬂuence on failure rate, while a low saturation reﬂects low inﬂuence.

As ﬁrst result, removing features from the facial landmark detection network
signiﬁcantly increases landmark failure rate. For instance, we observe a negative
(worse) relative change of 29.72% and 47.00% in failure rate by removing fea-
tures from Block 3 and Block 2 respectively. This illustrates that the main-task
network both contributed to and fed from the global features computed by the
central aggregation H. The CNN for landmark detection had the possibility to
remove the contribution of the global features, and so isolate itself from the other
CNNs, but the opposite occurred. We actually observe a mutual inﬂuence be-
tween the CNNs, where the task-speciﬁc features from the facial landmark CNN

Trottier et al.

13

inﬂuence the quality of the global features, which in turn inﬂuence the quality
of the subsequent task-speciﬁc features.

Another result that we can see from Fig. 6 is that Block 5 of task Proﬁle
has the highest inﬂuence on failure rate. We observe a negative relative change
of 83.87% by removing the features maps of task Proﬁle from the central ag-
gregation. What is particularly interesting in this case is that we observe this
high relative change at Block 5, which corresponds to the highest block in the
network. Since the block lies at the top of the network, it outputs features with
a high level of abstraction. We therefore expect that these features represent
high-level factors of variation corresponding to face orientation, which should
look like a rotation matrix. It therefore makes sense that features representing
the orientation of the face would be useful to predict facial landmarks, since
we know that the location of the facial landmarks is closely dependent on the
orientation of the face. The landmark CNN can use these rich features to bet-
ter rotate the predicted facial landmarks. This is indeed what we observe in
Fig. 6. These results constitute an empirical evidence that our approach allows
leveraging domain-speciﬁc information from related tasks.

4.5 Facial Landmark Detection With MTCNN

As ﬁnal experiment, we performed an experimental evaluation using the recent
Multi-Task Cascaded Convolutional Network (MTCNN) [27]. The authors pro-
posed a cascade structure of three stages, where each stage is composed of a
multi-task CNN. MTCNN performs predictions in a coarse-to-ﬁne manner. The
CNN of the ﬁrst stage generates (in a fully-convolutional way) many hypothe-
ses about the position of the face and the facial landmarks, and the subsequent
second and third stages reﬁnes them. The CNNs are trained sequentially with
hard-negative mining, in a hard-parameter sharing setting.

We implemented our approach in the available code project [40] and com-
pared ourself to MTCNN. We followed the provided hard negative mining recipe
and generated our images. For landmark detection, we used the LFWNet [26]
and CelebA [41] datasets, and generated 600k face images with facial landmarks.
For face detection, we used the WIDER [42] dataset, and generated 1.5M face
images with a bounding box. We trained a MTCNN with the stage networks
connected with our collaborative block, and a standard MTCNN with widened
stage networks to match the number of parameters.

On the test set of MTFL [15] dataset, standard MTCNN obtained a land-
mark failure rate of 37.85%, a mean error of 0.0996 and failed to detect a face
112 times, while our approach obtained better performances with 28.97%, 0.0930
and 79 respectively. Note that the reason MTCNN obtains worse performance
than our Deep Collaboration Network (DCNet), as reported in Fig. 3, is because
it has fewer parameters. DCNet has about 85M parameters, while the sum of
all three stage-CNN in MTCNN is about 2M. This is because MTCNN is care-
fully designed to balance computational speed and landmark detection precision.
It can predict many faces in high-dimensional images with a low computation
burden. An example of its prediction capability is shown in Fig. 7.

14

Multi-Task Learning by Deep Collaboration

Fig. 7. MTCNN predictions on the photo of the 2018 Oscar nominees (image resolution
of 2983 × 1197). The stage-CNNs are trained using our proposed collaborative block.
The coarse-to-ﬁne detection scheme employed by MTCNN allows predicting many faces
in high-dimensional images with low computational burden.

5 Conclusion and Future Work

In this paper, we proposed a novel soft-parameter knowledge sharing mechanism
based on lateral connections for Multi-Task Learning (MTL). Our proposed ap-
proach implements connectivity in term of a collaborative block, which uses two
distinct non-linear transformations. The ﬁrst one aggregates task-speciﬁc fea-
tures into global features, and the other merges back the global features into each
task-speciﬁc Convolutional Neural Network (CNN). Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
Our results on facial landmark detection tasks showed that networks connected
with our proposed collaborative block outperformed the other state-of-the-art
approaches, including the recent Cross-Stitch and MTCNN approach. We ver-
ify that our collaborative block eﬀectively enables knowledge sharing between
task-speciﬁc CNNs with an ablation study. We observed that the CNNs incor-
porated features with a varying level of abstraction from the other CNNs, by
observing the depth-speciﬁc inﬂuence of tasks that we know are related. These
results constituted an empirical evidence that our approach allows leveraging
domain-speciﬁc information from related tasks. Evaluating our proposed ap-
proach on other MTL problems could be an interesting avenue for future works.
For instance, the recurrent networks used to solve natural language processing
problems could beneﬁt from our approach.

Acknowledgements

We gratefully acknowledge the support of NVIDIA Corporation for providing a
Tesla Titan X for our experiments through their Hardware Grant Program.

Trottier et al.

15

References

1. Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classiﬁcation with deep convo-

lutional neural networks. In: NIPS. (2012) 1097–1105

2. Hochreiter, S.: The vanishing gradient problem during learning recurrent neural
nets and problem solutions. International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems 6(02) (1998) 107–116

A survey on multi-task learning.

arXiv preprint

3. Zhang, Y., Yang, Q.:
arXiv:1707.08114 (2017)

4. Caruana, R.: Multitask learning. In: Learning to learn. Springer (1998) 95–133
5. Zhang, M.L., Zhou, Z.H.: A review on multi-label learning algorithms. TKDE

6. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: KDD, ACM (2004)

26(8) (2014) 1819–1837

109–117

7. Pinto, L., Gupta, A.: Learning to push by grasping: Using multiple tasks for

eﬀective learning. In: ICRA, IEEE (2017) 2161–2168

8. Tian, Y., Luo, P., Wang, X., Tang, X.: Pedestrian detection aided by deep learning

semantic tasks. In: CVPR. (2015) 5079–5087

9. Liu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y.Y.: Representation learning
using multi-task deep neural networks for semantic classiﬁcation and information
retrieval. In: HLT-NAACL. (2015) 912–921

10. Yim, J., Jung, H., Yoo, B., Choi, C., Park, D., Kim, J.: Rotating your face using

multi-task deep neural network. In: CVPR. (2015) 676–684

11. Yin, X., Liu, X.: Multi-task convolutional neural network for face recognition.

arXiv preprint arXiv:1702.04710 (2017)

12. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-

task learning. In: CVPR. (2016) 3994–4003

13. Ruder, S.: An overview of multi-task learning in deep neural networks. CoRR

abs/1706.05098 (2017)

14. Ranjan, R., Patel, V.M., Chellappa, R.: Hyperface: A deep multi-task learning
framework for face detection, landmark localization, pose estimation, and gender
recognition. arXiv preprint arXiv:1603.01249 (2016)

15. Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi-

task learning. In: ECCV, Springer (2014) 94–108

16. Liu, P., Qiu, X., Huang, X.: Adversarial multi-task learning for text classiﬁcation.

(2017)

17. Baxter, J.: A bayesian/information theoretic model of learning to learn via multiple

task sampling. Machine learning 28(1) (1997) 7–39

18. Duong, L., Cohn, T., Bird, S., Cook, P.: Low resource dependency parsing: Cross-
lingual parameter sharing in a neural network parser. In: ACL. Volume 2. (2015)
845–850

19. Yang, Y., Hospedales, T.M.: Trace norm regularised deep multi-task learning.

arXiv preprint arXiv:1606.04038 (2016)

20. Ruder, S., Bingel, J., Augenstein, I., Søgaard, A.: Sluice networks: Learning what
to share between loosely related tasks. arXiv preprint arXiv:1705.08142 (2017)
21. Long, M., Wang, J.: Learning multiple tasks with deep relationship networks.

arXiv preprint arXiv:1506.02117 (2015)

22. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv
preprint arXiv:1606.04671 (2016)

16

Multi-Task Learning by Deep Collaboration

23. Trottier, L.: Deep collaboration network in pytorch.

https://github.com/

ltrottier/deep-collaboration-network

24. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 448–456

25. Nair, V., Hinton, G.: Rectiﬁed linear units improve restricted boltzmann machines.

In: ICML. (2010) 807–814

26. Sun, Y., Wang, X., Tang, X.: Deep convolutional network cascade for facial point

detection. In: CVPR. (2013) 3476–3483

27. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using
multitask cascaded convolutional networks. IEEE Signal Processing Letters 23(10)
(2016) 1499–1503

28. Jourabloo, A., Liu, X.: Large-pose face alignment via cnn-based dense 3d model

ﬁtting. In: CVPR. (2016) 4188–4196

29. Baltrušaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial

behavior analysis toolkit. In: WACV, IEEE (2016) 1–10

30. Ding, C., Tao, D.: Robust face recognition via multimodal deep face representation.

IEEE Transactions on Multimedia 17(11) (2015) 2049–2058

31. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to

human-level performance in face veriﬁcation. In: CVPR. (2014) 1701–1708

32. Zhang, C., Zhang, Z.: Improving multiview face detection with multi-task deep

convolutional neural networks. In: WACV, IEEE (2014) 1036–1041

33. Fabian Benitez-Quiroz, C., Srinivasan, R., Martinez, A.M.: Emotionet: An accu-
rate, real-time algorithm for the automatic annotation of a million facial expres-
sions in the wild. In: CVPR. (2016) 5562–5570

34. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

35. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: ECCV, Springer (2016) 630–645

In: CVPR. (2016) 770–778

36. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with

stochastic depth. In: ECCV, Springer (2016) 646–661

37. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations

for deep neural networks. In: CVPR, IEEE (2017) 5987–5995

38. Veit, A., Wilber, M.J., Belongie, S.: Residual networks behave like ensembles of

relatively shallow networks. In: NIPS. (2016) 550–558

39. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization. In:
ICCV Workshops, IEEE (2011) 2144–2151

40. Kim, K.K.: Deep learning face detection and recognition, implemented by pytorch.

https://github.com/kuaikuaikim/DFace

41. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:

42. Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark.

ICCV. (2015)

In: CVPR. (2016) 5525–5533

8
1
0
2
 
r
a

M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
1
1
0
0
.
1
1
7
1
:
v
i
X
r
a

Multi-Task Learning by Deep Collaboration and
Application in Facial Landmark Detection

Ludovic Trottier

Philippe Giguère

Brahim Chaib-draa

ludovic.trottier.1@ulaval.ca
{philippe.giguere,brahim.chaib-draa}@ift.ulaval.ca

Laval University, Québec, Canada

Abstract. Convolutional neural networks (CNNs) have become the most
successful approach in many vision-related domains. However, they are
limited to domains where data is abundant. Recent works have looked
at multi-task learning (MTL) to mitigate data scarcity by leveraging
domain-speciﬁc information from related tasks. In this paper, we present
a novel soft-parameter sharing mechanism for CNNs in a MTL setting,
which we refer to as Deep Collaboration. We propose taking into ac-
count the notion that task relevance depends on depth by using lateral
transformation blocs with skip connections. This allows extracting task-
speciﬁc features at various depth without sacriﬁcing features relevant to
all tasks. We show that CNNs connected with our Deep Collaboration
obtain better accuracy on facial landmark detection with related tasks.
We ﬁnally verify that our approach eﬀectively allows knowledge sharing
by showing depth-speciﬁc inﬂuence of tasks that we know are related.

1 Introduction

Over the past few years, Convolutional Neural Networks (CNNs) have become
the leading approach in many vision-related tasks [1]. Their ability to learn
a hierarchy of increasingly abstract concepts allows them to transform com-
plex high-dimensional input images into simple low-dimensional output features.
CNNs have been used in many settings, but their need to have a large amount
of data during training has restricted them to domains where data is abundant.
Optimizing CNNs is tricky not only because of problems like vanishing / ex-
ploding gradients [2], but also because they typically have many parameters to
be learned. While previous works have looked at supervised and unsupervised
pre-training to improve generalization, others have considered casting their orig-
inal single-task problem into a new Multi-Task Learning (MTL) problem [3]. As
Caruana (1998) [4] explained in his seminal work: “MTL improves generalization
by leveraging the domain-speciﬁc information contained in the training signals
of related tasks". Exploring new ways to more eﬃciently gather information from
related tasks would help to further improve generalization on the main one.

MTL has proven its value in several domains over the years. It has become a
dominant ﬁeld of machine learning [5], with many inﬂuential works [6]. Although
MTL dates back several years, recent major advances in Deep Learning (DL)

2

Multi-Task Learning by Deep Collaboration

opened up opportunities for novel contributions. Works on grasping [7], pedes-
trian detection [8], natural language processing [9], face recognition [10][11] and
object detection [12] helped MTL make a resurgence in the DL community. They
have shown the potential of MTL to mitigate data scarcity when training deep
networks, which has inﬂuenced its growing popularity

MTL approaches can generally be divided into two major categories: hard
and soft parameter sharing [13]. Hard-parameter sharing dates back to the orig-
inal work of Caruana (1998) and is the most common of the two. Approaches
in this category have a shared central section with many heads (one per task).
Features from speciﬁc tasks compete together and those relevant to all tasks are
favored. Recent works in DL have shown that hard-parameter sharing can be
successful [14][15][7][11]. However, a too large emphasis on features relevant to all
tasks can be harmful for learning high-level features speciﬁc to a particular task.
These types of speciﬁc features are usually needed to obtain a good representa-
tion for the particular task. Also, shared layers are prone to be contaminated by
noise coming from noxious tasks [16]. These limitations can be detrimental even
though hard-parameter sharing reduces the risk of over-ﬁtting [17].

Soft-parameter sharing has been proposed as an alternative to alleviate these
drawbacks. Approaches in this category substitute the shared central section by
separate task-speciﬁc CNNs, but provide a knowledge sharing mechanism to
connect them. Each CNN can then learn task-speciﬁc features and share their
knowledge without interfering with others. Recent works in this category have
looked at regularizing the distance between task-speciﬁc parameters with a ‘2
norm [18] or a trace norm [19], training shared and private LSTM submod-
ules [16], partitioning the hidden layers into subspaces [20] and regularizing the
FC layers with tensor normal priors [21]. In the domain of continual learning,
progressive network [22] has also shown promising results for sequential transfer
learning, by employing lateral connections to previously learned networks.

In this paper, we present a novel soft-parameter knowledge sharing mecha-
nism for connecting task-speciﬁc CNNs in a MTL framework. We refer to our
approach as Deep Collaboration. We deﬁne connectivity in terms of a collabo-
rative block that uses two non-linear transformations with lateral connections.
One aggregates task-speciﬁc features into global features, and the other merges
back the global features into each task-speciﬁc CNN. Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
We evaluated our approach on the problem of facial landmark detection in a
MTL framework and obtained better results in comparison to other approaches
of the literature. We further assess the objectivity of our training framework by
randomly varying the contribution of each related tasks. Finally, we verify that
our collaborative block enables knowledge sharing with an ablation study that
shows the depth-speciﬁc inﬂuence of tasks that we know are related.

The content of our paper is organized as follows. In Section 2, we present
related works in MTL and facial landmark detection. We elaborate on our ap-
proach in Section 3, and present experimental results in Section 4. We ﬁnally
conclude our paper in Section 5. Our code is available here: [23].

Trottier et al.

3

2 Related Work

2.1 Multi-Task Learning

Our proposed Deep Collaboration knowledge sharing mechanism is related to
other existing approaches. One is Cross-Stitch (XS) [12], which connects task-
speciﬁc CNNs by linearly combining their feature maps at certain depths. One
drawback of XS is that it is limited to capture only linear dependencies between
each CNN. In contrast to XS, our approach uses non-linear transformations in
order to capture more complex dependencies.

Another related approach is Tasks-Constrained Deep Convolutional Network
(TCDCN) [15]. The authors proposed an early-stopping criterion to remove aux-
iliary tasks that start to overﬁt before becoming detrimental to the main task.
This approach has however several hyper-parameters to be selected manually.
For each task, it has an hyper-parameter controlling the period length of the
local window and a threshold that stops the task when the criterion exceeds it.
Unlike TCDCN, our approach has no hyper-parameters that need to be tuned to
the tasks at hand. Our collaborative block consists of a series of Batch Normal-
ization [24], ReLU [25], and convolutional layers shaped in a standard setting
that is commonly found in nowadays works.

Our proposed approach is also related to HyperFace [14]. The authors pro-
posed to fuse the layers at various depth and exploit features of diﬀerent levels
of complexity. Their goal was to allow low-level features with better localization
properties to help tasks such as landmark localization and pose detection, and al-
low high-level features with better class-speciﬁc properties to help tasks like face
detection and gender recognition. Although HyperFace is in the hard-parameter
sharing category and is not entirely related to our approach, the idea of feature
fusion is also central in our work. Instead of fusing the features at intermediate
layers of a single CNN, our approach aggregates same-level features of multiple
CNNs, at diﬀerent depth independently.

2.2 Facial Landmark Detection

Facial Landmark Detection (FLD) is an essential component in many face-
related tasks [26][27][28][29]. FLD can be described as follows: given the image of
a face of a person, the goal is to predict the (x, y)-position of speciﬁc landmarks
associated with key features of the visage. Applications such as face recogni-
tion [30], face validation [31], facial feature detection and tacking [32] rely on
the ability to correctly ﬁnd the location of these distinct facial landmarks in order
to succeed. Localizing facial key points like the center of the eyes, the corners of
the mouth, the tip of the nose and the earlobes is however a challenging problem
when many lighting conditions, head poses, facial expressions and occlusions in-
crease diversity of the face images. In addition to integrating this variability into
the estimation process, a FLD model must also take into account a number of
correlated factors. For instance, although both an angry person and a sad person
have frowned eyebrows, an angry person will have pinched lips while a sad person

4

Multi-Task Learning by Deep Collaboration

will have sunken mouth corners [33]. A particularity of datasets geared towards
FLD is that they are particularly well-suited for MTL. In addition to contain-
ing the position of the facial landmarks, these datasets also contain a number
of other labels that can be used to deﬁned auxiliary tasks. Gender recognition,
smile recognition, glasses recognition or face orientation are examples of tasks
often chosen to evaluate MTL approaches.

3 Deep Collaboration

Given T task-speciﬁc Convolutional Neural Networks (CNNs), our goal is to con-
nect them with lateral connections in order to allow domain-speciﬁc information
sharing. We deﬁne connectivity in terms of a collaborative block containing two
distinct non-linear transformations. One aggregates task-speciﬁc features into
global features, and the other merges back the global features into each task-
speciﬁc CNN. Our collaborative block is diﬀerentiable and can be dropped in
any existing CNN architectures as a whole. For this reason, we make no assump-
tion about the structure of the task-speciﬁc CNNs. Our approach can even work
with diﬀerent CNNs, but for the sake of simplicity, we suppose that the CNNs
are the same. We refer to it as the underlying network.

We also decompose the underlying network as a series of blocks. Each block
can be as small as a single layer, as large as the whole network itself, or based
on simple rules, such as grouping all layers with matching spatial dimensions or
grouping every n subsequent layers. The arrangement of the layers into blocks
does not change the composition of the underlying network. We only use it to
make explicit the depth at which we connect the task-speciﬁc CNNs.

Since our collaborative block can be inserted at any depth, we also drop the
depth index on the feature maps to further simplify the equations. As such,
we deﬁne the feature map output of a block at a certain depth as xt, where
t ∈ {1 . . . T } is the task index. Our approach takes as input all task-speciﬁc
feature maps xt and processes them into new feature maps yt as follows:

z = H([x1, . . . , xT ]) ,

yt = ReLU (xt + Ft([xt, z])) ,

(1)

where H and Ft represent the central and the task-speciﬁc aggregations re-
spectively, and [·] denotes depth-wise concatenation. We refer to Eq. (1) as our
collaborative block. The goal of H is to combine all task-speciﬁc feature maps xt
into a global feature map z representing uniﬁed knowledge, while the goal of F
is to merge back the global feature map z with each task-speciﬁc input xt. The
compositional structure of H and F is as follows:

H(·) = (ReLU ◦ BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,
F(·) = (BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,

(2)

(3)

where BN stands for Batch Normalization [24], Conv(h×w) for a standard convo-
lutional layer with ﬁlters of size (h × w), and ◦ is the usual function composition.
The ﬁrst Conv(1×1) layer in H divides the number of feature maps by a factor

Trottier et al.

5

Fig. 1. Example of our collaborative block applied on the feature maps of two task-
speciﬁc networks. The input feature maps (shown in 1(cid:13)) are ﬁrst concatenated depth-
wise and transformed into a global feature map ( 2(cid:13)). The global feature map is then
concatenate with each input feature map individually and transformed into task-speciﬁc
feature maps ( 3(cid:13)). Each resulting feature map is then added back to the input feature
map using a skip connection ( 4(cid:13)), which gives the ﬁnal outputs of the block ( 5(cid:13)).

of 4, while the ﬁrst Conv(1×1) layer in F divides it to match the size of xt. An
illustration of our collaborative block is shown in Fig. 1.

One particularity of our approach is that we use a skip connection in mapping
F. Recent works [34][35][36][37][38] have shown that networks with identity skip
connections are more easily able to learn proper input-output mappings. Inspired
by these works, we opted for an identity skip connection in F in order to more
easily learn the proper mapping to integrate domain-speciﬁc information from
the other tasks. In particular, identity skip connections put an incentive on
learning the identity mapping. We can see this by the ease at which the network
can obtain the identity mapping by simply pushing all the weights in F towards
zero. In our MTL context, the identity mapping can be seen as a way to remove
the inﬂuence of the global features z. This allows to take into account the cases
where integrating z back to the task-speciﬁc features xt would not help.

Another motivation for using an identity skip connection around the global
feature map z comes from the fact that depth inﬂuences the relevance of each
task towards another. Some task-speciﬁc CNNs can beneﬁt more when they
share their low-level features than their high-level features, while other beneﬁt
more in the other way. For instance, tasks such as landmark localization and
pose detection proﬁt more from low-level features containing better localization

6

Multi-Task Learning by Deep Collaboration

Fig. 2. Deep Collaboration Network (DCNet) using ResNet18 as underlying network
in a MTL setting on the MTFL dataset. The top part shows the block structure of
ResNet18 interleaved with our proposed collaborative block, while the bottom part
details each residual and task-speciﬁc FC blocks.

properties, while tasks such as face detection and gender recognition proﬁt more
from class-speciﬁc high-level features. Considering that CNNs learn a hierarchy
of increasingly abstract features, our collaborative block can take into account
task relevance by deactivating a diﬀerent set of residual mappings Ft based on
the depth at which it is inserted. An example of such specialization will be shown
in our ablative study in Section 4.4.

Fig. 2 presents an example of inserting our collaborative block at diﬀerent
depths in a MTL framework on the MTFL dataset [15]. In this particular case,
we opted for a ResNet18 as underlying network. We refer to this network as our
Deep Collaboration Network (DCNet). As we can see in the top part of the ﬁgure,
integrating our approach comes down to interleaving the underlying network
block structure with our collaborative block. Each collaborative block receives
as input the output of each task-speciﬁc block, processes them as detailed in
Eq. (1), and sends the result back to each task-speciﬁc network. Adding our
approach to any underlying network can be done by simply following the same
pattern of interleaving the network block structure with our collaborative block.

4 Experiments

In this section, we detail our Multi-Task Learning (MTL) training framework and
present our experiments in Facial Landmark Detection (FLD) tasks. We further
evaluate the eﬀect of data scarcity on performance and illustrate an example of
knowledge sharing between task-speciﬁc CNNs with an ablation study.

Trottier et al.

7

4.1 Multi-Task Learning Training Framework

The goal of Facial Landmark Detection (FLD) is to predict the (x, y)-position
of speciﬁc landmarks associated with key features of the visage. While the num-
ber and type of landmarks are speciﬁc to each dataset, examples of standard
landmarks to be predicted are the corners of the mouth, the tip of the nose
and the center of the eyes. In addition to the facial landmarks, each dataset
further deﬁnes a number of related tasks. These related tasks also vary from
one dataset to another, and are typically gender recognition, smile recognition,
glasses recognition or face orientation.

On a more technical level, we deﬁne a learning framework in which we treat
each task as a classiﬁcation problem. While this is straightforward for gender,
smile and glasses recognition as they are already classiﬁcation tasks, it is a
bit more tricky for face orientation and FLD. For face orientation, instead of
predicting the roll, yaw and pitch real value as in a regression problem, we
divide each component into 30 degrees wide bins and predict the label of the bin
corresponding to the value. Similarly for FLD, rather than predicting the real
(x, y)-position of each landmark, we divide the image into 1 pixel wide bins and
predict the label of the bin corresponding to the value. Note that we still use
the original real values when comparing our prediction with the ground truth,
so that we incorporate our approximation errors in the ﬁnal score.

We report our results using the landmark failure rate metric [15], which is
deﬁned as follows: we ﬁrst compute the mean distance between the predicted
landmarks and the ground truth landmarks, then normalize it by the inter-
ocular distance from the center of the eyes. A normalized mean distance greater
than 10% is reported as a failure.

4.2 Facial Landmark Detection on the MTFL Task

As a ﬁrst experiment, we performed facial landmark detection on the Multi-Task
Facial Landmark (MTFL) task [15]. The dataset contains 12,995 face images an-
notated with ﬁve facial landmarks and four related attributes of gender, smiling,
wearing glasses and face proﬁle (ﬁve proﬁles in total). The training set has 10,000
images, while the test set has 2,995 images. We perform four sets of experiments
using an ImageNet pre-trained AlexNet, an ImageNet pre-trained ResNet18, an
un-pretrained AlexNet and an un-pretrained ResNet18 as underlying networks.
For AlexNet, we apply our collaborative block after each max pooling layer,
while for ResNet18, we do as shown in Fig. 2.

We compare our approach to several other approaches of the literature. We
include single-task learning (AN-S when using AlexNet as underlying network,
RN-S when using ResNet18), hard-parameter sharing MTL (AN and RN), hard-
parameter sharing MTL where the central section is widened to match the num-
ber of parameters of our approach (ANx and RNx), HyperFace (HF) [14], Tasks-
Constrained Deep Convolutional Network (TCDCN) [15], Cross-Stitch (XS) [12]
and XS widen to match the number of parameters of our approach (XSx). Ex-
cept for TCDCN, we train each network ourselves three times for 300 epochs and

8

Multi-Task Learning by Deep Collaboration

Fig. 3. Landmark failure rates (%) on the MTFL task. The reported values are the
average over the last ﬁve epochs, averaged over three tries. The left plot presents our
results with AlexNet as the underlying network, while the right one with ResNet18.
AN-S and RN-S stand for single-task training, AN and RN for multi-task training with
a single central network, ANx and RNx for multi-task training with a single central
network widen to match the number of parameters of our approach, HF for HyperFace,
TCDCN for [15]’s approach and XS for Cross-Stitch. In each instance, the left column
(blue) is for un-pretrained networks, while the right column (green) is for pre-trained
networks. Our proposed approach obtains the lowest failure rates overall.

report landmark failure rates averaged over the last ﬁve epochs, further averaged
over the three tries.

Fig. 3 presents our FLD results on the MTFL dataset. The left part of the
ﬁgure corresponds to using AlexNet as underlying network, while the right one
corresponds to ResNet18. The top part reports the landmark failure rates, while
the bottom part reports the mean error. In each plot, the left bar (blue) is for
un-pretrained network, while the right bar (green) is for ImageNet pre-trained
network. In addition, Fig. 4 shows example predictions from DCNet with pre-
trained ResNet18 as underlying network. The ﬁrst two examples were reported as
successes, while the last two are failures. The ground truth elements are colored
in green, while our predictions are colored in blue. We also include the labels of
the related tasks: gender, smiling, wearing glasses and face proﬁle.

The results of Fig. 3 show that our proposed approach obtained the lowest
failure rates and mean error in each case. Indeed, our DCNet with un-pretrained
and pre-trained AlexNet as underlying network obtained 19.67% and 19.96%
failure rates respectively, and 14.95% and 13.52% with ResNet18. This is sig-
niﬁcantly lower than the other approaches to which we compare ourselves. For
instance, with AlexNet, HF had 27.75% and 27.32%, XS had 26.41% and 25.65%,

Trottier et al.

9

MTFL

AFLW

Fig. 4. Example predictions of our DCNet with pre-trained ResNet18 as underlying
network on the MTFL and AFLW task. For MTFL, the ﬁrst two examples are successes,
while last two are failure cases. For AFLW, the ﬁrst three examples are successes, while
the last one is a failure case. Elements in green correspond to ground truth, while those
in blue correspond to predictions. Facial landmarks are shown as small dots, and related
tasks labels are displayed on the side. As we can see, over-exposition and tilted face
proﬁle can have a large impact on the prediction quality.

TCDCN had 25.00%1, and XSx had 25.23%. With ResNet18, XS had 18.43% and
15.52% respectively, and XSx had 17.28. We obtained the highest improvements
when using AlexNet as the underlying network when comparing to XS. With un-
pretrained and pre-trained AlexNet, we obtained improvements of 6.74% and
5.69%, while we obtained 3.48% and 2.00% with ResNet18. Performing MTL
with our approach can thus improve performance over using other approaches
of the literature.

Another result that we can see from Fig. 3 is that our soft-parameter shar-
ing approach obtains higher performance than the hard-parameter sharing ap-
proaches with matching number of parameters. For instance, increasing the
number of parameters of hard-parameter sharing AlexNet lowers it error rate
from 28.02% (AN) to 26.88% (ANx), but our approach lowers it further to
19.67%. Similarly, increasing the number of parameters of hard-parameter shar-
ing ResNet18 lowers it error rate from 20.05% (RN) to 16.75% (RNx), but our
approach lowers it further to 14.95%. These results are interesting because they
show that while increasing the number of parameters is an eﬀortless avenue to
improve performance, it has limitations. Developing novel approaches to enhance
network connectivity in a soft-parameter sharing setting seems more rewarding.
This may help to motivate new eﬀorts in this avenue to further leverage domain-
information of related tasks.

1 Zhang et. al only provided results with pre-trained AlexNet [15]

10

Multi-Task Learning by Deep Collaboration

Table 1. Landmark failure rate results on the AFLW dataset using a pre-trained
ResNet18 as underlying network. The presented values are averaged over the last ﬁve
epochs, further averaged over three tries. The ﬁrst column is the train / test ratio,
and the subsequent ones are the networks: single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS). Our approach obtains the best perfor-
mance in all cases, except the ﬁrst one where we observe over-ﬁtting.

Train / Test Ratio

0.1 / 0.9
0.3 / 0.7
0.5 / 0.5
0.7 / 0.3
0.9 / 0.1

RN-S

57.39
31.84
23.41
21.47
13.03

Networks

RN

58.00
32.00
23.31
21.92
12.80

XS

73.06
36.24
26.02
22.37
13.51

Ours

60.64
29.73
20.77
18.50
10.82

4.3 Eﬀect of Data Scarcity on the AFLW Task

As second experiment, we evaluated the inﬂuence of the number of training
examples to simulate data scarcity on the Annotated Facial Landmarks in the
Wild (AFLW) task [39]. The dataset has 21,123 Flickr images, and each image
can contain more than one face. Instead of using the images as provided, we
process them using the available face bounding boxes. We extract all faces with
visible landmarks, which gives a total of 2,111 images. This dataset deﬁnes 21
facial landmarks and has 3 related tasks (gender, wearing glasses and face ori-
entation). For face orientation, we divide the roll, yaw and pitch into 30 degrees
wide bins (14 bins in total), and predict the label corresponding to each real
value.

Our experiment works as follows. With a pre-trained ResNet18 as underlying
network, we compare our approach to single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS) by training on a varying number
of images. We use ﬁve diﬀerent train / test ratios, starting with 0.1 / 0.9 up
to 0.9 / 0.1 by 0.2 increment. In other words, we train each approaches on the
ﬁrst r% of the available images and test on the other (1 − r)%, then repeat
for all the other train / test ratios. We use the same training framework as in
section 4.2. We train each network three times for 300 epochs, and report the
landmark failure rate averaged over the last ﬁve epochs, further averaged over
the three tries. Example predictions are shown in Fig. 4.

As we can see in Table 1, our approach obtained the best performance in
all cases except the ﬁrst one. Indeed, we observe between 1.98% and 6.51%
improvements with train / test ratios from 0.3 / 0.7 to 0.9 / 0.1, while we obtain
a negative relative change of 3.25% with train / test ratio of 0.1 / 0.9. However,
since all multi-task approaches obtained higher failure rates than the single-task
approach, this suggests that the networks are over-ﬁtting the small training set.
Nonetheless, these results show that we can obtain better performance using our
approach.

Trottier et al.

11

Fig. 5. Landmark failure rate improvement (in %) of our approach compared to XS
when sampling random task weights. We used a pre-trained ResNet18 as underlying
network. The histogram at the left and the plot at the top right represents performance
improvement achieved by our proposed approach (positive value means lower failure
rates), while the plot at the bottom right corresponds to the log of the task weights. Our
approach outperformed XS in 86 out of the 100 tries, thus empirically demonstrating
that our learning framework was not unfavorable towards XS and that our approach
is less sensitive to the task weights λ.

One particularity that we observe in Table 1 is that the XS network has
relatively high failure rates. In the previous experiment of Section 4.2, XS had
either similar or better performance than the other approaches (except ours).
This could be due to our current multi-task learning framework that is unfavor-
able towards XS. In order to investigate whether this is the case, we perform
the following additional experiment. Using a pre-trained ResNet18 as underlying
network, we compare our approach to XS by training each network 100 times
using task weights randomly sampled from a log-uniform distribution. Speciﬁ-
cally, we ﬁrst sample from a uniform distribution γ ∼ U(log(1e−4), log(1)), then
use λ = exp(γ) as the weight. We trained both XS and our approach for 300
epochs with the same task weights using a train / test ratio of 0.5 / 0.5.

Figure 5 presents the results of this experiment. The plot at the top right
of the ﬁgure represents the landmark failure rate improvement (in %) of our
approach compared to XS, while the plot at the bottom right corresponds to the
log of the task weights for each try. In 86 out of the 100 tries, our approach had
a positive failure rate improvement, that is, obtained lower failure rates than
XS. As we can see in the histogram at the left of Fig. 5, the improvement rate
is normally distributed around 2.80%, has a median improvement of 3.66% and
a maximum improvement of 9.83%. Even though we sampled at random the
weights of the related tasks, our approach outperforms XS in the majority of the
cases. Our learning framework was therefore not unfavorable toward XS.

4.4 Illustration of Knowledge Sharing With an Ablation Study

As third experiment, we perform an ablation study on the MTFL task [15] with
an un-pretrained ResNet18 as underlying network. The goal of this experiment

12

Multi-Task Learning by Deep Collaboration

Fig. 6. Results of our ablation study on the MTFL dataset with an un-pretrained
ResNet18 as underlying network. We remove each task-speciﬁc features from each re-
spective central aggregation layer and evaluate the eﬀect on landmark failure rate. The
rows represent the task-speciﬁc CNNs, while the columns correspond to the network
block structure. Blocks with a high saturated color were found to have a large impact
on failure rate. In particular, this ablative study shows that the inﬂuence of high-
level face proﬁle features is large within our proposed architecture. This corroborates
with the well-known fact that the location of facial landmarks is closely dependent on
the orientation of the face. This constitutes an empirical evidence of domain-speciﬁc
information sharing via our approach.

is to verify that our collaborative block eﬀectively enables knowledge sharing
between task-speciﬁc CNNs. To do so, we evaluate the impact, on facial landmark
detection, of removing the contribution of each task-speciﬁc features. We zero out
the designated feature map xt before concatenation at the input of the central
aggregation H. The network is trained using the same framework as explained
in Sec. 4.1, and the ablation study is performed at test time on the test set when
training is done.

Figure 6 presents the results of our ablation study. The rows represent each
task-speciﬁc CNN, while the columns correspond to the network block structure.
The blocks are ordered from left (input) to right (output), while the task-speciﬁc
networks are ordered from top (main task) to bottom (related tasks). The color
saturation indicates the inﬂuence of removing the task-speciﬁc feature maps from
the central aggregation at the corresponding depth. A high saturation reﬂects
high inﬂuence on failure rate, while a low saturation reﬂects low inﬂuence.

As ﬁrst result, removing features from the facial landmark detection network
signiﬁcantly increases landmark failure rate. For instance, we observe a negative
(worse) relative change of 29.72% and 47.00% in failure rate by removing fea-
tures from Block 3 and Block 2 respectively. This illustrates that the main-task
network both contributed to and fed from the global features computed by the
central aggregation H. The CNN for landmark detection had the possibility to
remove the contribution of the global features, and so isolate itself from the other
CNNs, but the opposite occurred. We actually observe a mutual inﬂuence be-
tween the CNNs, where the task-speciﬁc features from the facial landmark CNN

Trottier et al.

13

inﬂuence the quality of the global features, which in turn inﬂuence the quality
of the subsequent task-speciﬁc features.

Another result that we can see from Fig. 6 is that Block 5 of task Proﬁle
has the highest inﬂuence on failure rate. We observe a negative relative change
of 83.87% by removing the features maps of task Proﬁle from the central ag-
gregation. What is particularly interesting in this case is that we observe this
high relative change at Block 5, which corresponds to the highest block in the
network. Since the block lies at the top of the network, it outputs features with
a high level of abstraction. We therefore expect that these features represent
high-level factors of variation corresponding to face orientation, which should
look like a rotation matrix. It therefore makes sense that features representing
the orientation of the face would be useful to predict facial landmarks, since
we know that the location of the facial landmarks is closely dependent on the
orientation of the face. The landmark CNN can use these rich features to bet-
ter rotate the predicted facial landmarks. This is indeed what we observe in
Fig. 6. These results constitute an empirical evidence that our approach allows
leveraging domain-speciﬁc information from related tasks.

4.5 Facial Landmark Detection With MTCNN

As ﬁnal experiment, we performed an experimental evaluation using the recent
Multi-Task Cascaded Convolutional Network (MTCNN) [27]. The authors pro-
posed a cascade structure of three stages, where each stage is composed of a
multi-task CNN. MTCNN performs predictions in a coarse-to-ﬁne manner. The
CNN of the ﬁrst stage generates (in a fully-convolutional way) many hypothe-
ses about the position of the face and the facial landmarks, and the subsequent
second and third stages reﬁnes them. The CNNs are trained sequentially with
hard-negative mining, in a hard-parameter sharing setting.

We implemented our approach in the available code project [40] and com-
pared ourself to MTCNN. We followed the provided hard negative mining recipe
and generated our images. For landmark detection, we used the LFWNet [26]
and CelebA [41] datasets, and generated 600k face images with facial landmarks.
For face detection, we used the WIDER [42] dataset, and generated 1.5M face
images with a bounding box. We trained a MTCNN with the stage networks
connected with our collaborative block, and a standard MTCNN with widened
stage networks to match the number of parameters.

On the test set of MTFL [15] dataset, standard MTCNN obtained a land-
mark failure rate of 37.85%, a mean error of 0.0996 and failed to detect a face
112 times, while our approach obtained better performances with 28.97%, 0.0930
and 79 respectively. Note that the reason MTCNN obtains worse performance
than our Deep Collaboration Network (DCNet), as reported in Fig. 3, is because
it has fewer parameters. DCNet has about 85M parameters, while the sum of
all three stage-CNN in MTCNN is about 2M. This is because MTCNN is care-
fully designed to balance computational speed and landmark detection precision.
It can predict many faces in high-dimensional images with a low computation
burden. An example of its prediction capability is shown in Fig. 7.

14

Multi-Task Learning by Deep Collaboration

Fig. 7. MTCNN predictions on the photo of the 2018 Oscar nominees (image resolution
of 2983 × 1197). The stage-CNNs are trained using our proposed collaborative block.
The coarse-to-ﬁne detection scheme employed by MTCNN allows predicting many faces
in high-dimensional images with low computational burden.

5 Conclusion and Future Work

In this paper, we proposed a novel soft-parameter knowledge sharing mechanism
based on lateral connections for Multi-Task Learning (MTL). Our proposed ap-
proach implements connectivity in term of a collaborative block, which uses two
distinct non-linear transformations. The ﬁrst one aggregates task-speciﬁc fea-
tures into global features, and the other merges back the global features into each
task-speciﬁc Convolutional Neural Network (CNN). Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
Our results on facial landmark detection tasks showed that networks connected
with our proposed collaborative block outperformed the other state-of-the-art
approaches, including the recent Cross-Stitch and MTCNN approach. We ver-
ify that our collaborative block eﬀectively enables knowledge sharing between
task-speciﬁc CNNs with an ablation study. We observed that the CNNs incor-
porated features with a varying level of abstraction from the other CNNs, by
observing the depth-speciﬁc inﬂuence of tasks that we know are related. These
results constituted an empirical evidence that our approach allows leveraging
domain-speciﬁc information from related tasks. Evaluating our proposed ap-
proach on other MTL problems could be an interesting avenue for future works.
For instance, the recurrent networks used to solve natural language processing
problems could beneﬁt from our approach.

Acknowledgements

We gratefully acknowledge the support of NVIDIA Corporation for providing a
Tesla Titan X for our experiments through their Hardware Grant Program.

Trottier et al.

15

References

1. Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classiﬁcation with deep convo-

lutional neural networks. In: NIPS. (2012) 1097–1105

2. Hochreiter, S.: The vanishing gradient problem during learning recurrent neural
nets and problem solutions. International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems 6(02) (1998) 107–116

A survey on multi-task learning.

arXiv preprint

3. Zhang, Y., Yang, Q.:
arXiv:1707.08114 (2017)

4. Caruana, R.: Multitask learning. In: Learning to learn. Springer (1998) 95–133
5. Zhang, M.L., Zhou, Z.H.: A review on multi-label learning algorithms. TKDE

6. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: KDD, ACM (2004)

26(8) (2014) 1819–1837

109–117

7. Pinto, L., Gupta, A.: Learning to push by grasping: Using multiple tasks for

eﬀective learning. In: ICRA, IEEE (2017) 2161–2168

8. Tian, Y., Luo, P., Wang, X., Tang, X.: Pedestrian detection aided by deep learning

semantic tasks. In: CVPR. (2015) 5079–5087

9. Liu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y.Y.: Representation learning
using multi-task deep neural networks for semantic classiﬁcation and information
retrieval. In: HLT-NAACL. (2015) 912–921

10. Yim, J., Jung, H., Yoo, B., Choi, C., Park, D., Kim, J.: Rotating your face using

multi-task deep neural network. In: CVPR. (2015) 676–684

11. Yin, X., Liu, X.: Multi-task convolutional neural network for face recognition.

arXiv preprint arXiv:1702.04710 (2017)

12. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-

task learning. In: CVPR. (2016) 3994–4003

13. Ruder, S.: An overview of multi-task learning in deep neural networks. CoRR

abs/1706.05098 (2017)

14. Ranjan, R., Patel, V.M., Chellappa, R.: Hyperface: A deep multi-task learning
framework for face detection, landmark localization, pose estimation, and gender
recognition. arXiv preprint arXiv:1603.01249 (2016)

15. Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi-

task learning. In: ECCV, Springer (2014) 94–108

16. Liu, P., Qiu, X., Huang, X.: Adversarial multi-task learning for text classiﬁcation.

(2017)

17. Baxter, J.: A bayesian/information theoretic model of learning to learn via multiple

task sampling. Machine learning 28(1) (1997) 7–39

18. Duong, L., Cohn, T., Bird, S., Cook, P.: Low resource dependency parsing: Cross-
lingual parameter sharing in a neural network parser. In: ACL. Volume 2. (2015)
845–850

19. Yang, Y., Hospedales, T.M.: Trace norm regularised deep multi-task learning.

arXiv preprint arXiv:1606.04038 (2016)

20. Ruder, S., Bingel, J., Augenstein, I., Søgaard, A.: Sluice networks: Learning what
to share between loosely related tasks. arXiv preprint arXiv:1705.08142 (2017)
21. Long, M., Wang, J.: Learning multiple tasks with deep relationship networks.

arXiv preprint arXiv:1506.02117 (2015)

22. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv
preprint arXiv:1606.04671 (2016)

16

Multi-Task Learning by Deep Collaboration

23. Trottier, L.: Deep collaboration network in pytorch.

https://github.com/

ltrottier/deep-collaboration-network

24. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 448–456

25. Nair, V., Hinton, G.: Rectiﬁed linear units improve restricted boltzmann machines.

In: ICML. (2010) 807–814

26. Sun, Y., Wang, X., Tang, X.: Deep convolutional network cascade for facial point

detection. In: CVPR. (2013) 3476–3483

27. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using
multitask cascaded convolutional networks. IEEE Signal Processing Letters 23(10)
(2016) 1499–1503

28. Jourabloo, A., Liu, X.: Large-pose face alignment via cnn-based dense 3d model

ﬁtting. In: CVPR. (2016) 4188–4196

29. Baltrušaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial

behavior analysis toolkit. In: WACV, IEEE (2016) 1–10

30. Ding, C., Tao, D.: Robust face recognition via multimodal deep face representation.

IEEE Transactions on Multimedia 17(11) (2015) 2049–2058

31. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to

human-level performance in face veriﬁcation. In: CVPR. (2014) 1701–1708

32. Zhang, C., Zhang, Z.: Improving multiview face detection with multi-task deep

convolutional neural networks. In: WACV, IEEE (2014) 1036–1041

33. Fabian Benitez-Quiroz, C., Srinivasan, R., Martinez, A.M.: Emotionet: An accu-
rate, real-time algorithm for the automatic annotation of a million facial expres-
sions in the wild. In: CVPR. (2016) 5562–5570

34. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

35. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: ECCV, Springer (2016) 630–645

In: CVPR. (2016) 770–778

36. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with

stochastic depth. In: ECCV, Springer (2016) 646–661

37. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations

for deep neural networks. In: CVPR, IEEE (2017) 5987–5995

38. Veit, A., Wilber, M.J., Belongie, S.: Residual networks behave like ensembles of

relatively shallow networks. In: NIPS. (2016) 550–558

39. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization. In:
ICCV Workshops, IEEE (2011) 2144–2151

40. Kim, K.K.: Deep learning face detection and recognition, implemented by pytorch.

https://github.com/kuaikuaikim/DFace

41. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:

42. Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark.

ICCV. (2015)

In: CVPR. (2016) 5525–5533

8
1
0
2
 
r
a

M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
1
1
0
0
.
1
1
7
1
:
v
i
X
r
a

Multi-Task Learning by Deep Collaboration and
Application in Facial Landmark Detection

Ludovic Trottier

Philippe Giguère

Brahim Chaib-draa

ludovic.trottier.1@ulaval.ca
{philippe.giguere,brahim.chaib-draa}@ift.ulaval.ca

Laval University, Québec, Canada

Abstract. Convolutional neural networks (CNNs) have become the most
successful approach in many vision-related domains. However, they are
limited to domains where data is abundant. Recent works have looked
at multi-task learning (MTL) to mitigate data scarcity by leveraging
domain-speciﬁc information from related tasks. In this paper, we present
a novel soft-parameter sharing mechanism for CNNs in a MTL setting,
which we refer to as Deep Collaboration. We propose taking into ac-
count the notion that task relevance depends on depth by using lateral
transformation blocs with skip connections. This allows extracting task-
speciﬁc features at various depth without sacriﬁcing features relevant to
all tasks. We show that CNNs connected with our Deep Collaboration
obtain better accuracy on facial landmark detection with related tasks.
We ﬁnally verify that our approach eﬀectively allows knowledge sharing
by showing depth-speciﬁc inﬂuence of tasks that we know are related.

1 Introduction

Over the past few years, Convolutional Neural Networks (CNNs) have become
the leading approach in many vision-related tasks [1]. Their ability to learn
a hierarchy of increasingly abstract concepts allows them to transform com-
plex high-dimensional input images into simple low-dimensional output features.
CNNs have been used in many settings, but their need to have a large amount
of data during training has restricted them to domains where data is abundant.
Optimizing CNNs is tricky not only because of problems like vanishing / ex-
ploding gradients [2], but also because they typically have many parameters to
be learned. While previous works have looked at supervised and unsupervised
pre-training to improve generalization, others have considered casting their orig-
inal single-task problem into a new Multi-Task Learning (MTL) problem [3]. As
Caruana (1998) [4] explained in his seminal work: “MTL improves generalization
by leveraging the domain-speciﬁc information contained in the training signals
of related tasks". Exploring new ways to more eﬃciently gather information from
related tasks would help to further improve generalization on the main one.

MTL has proven its value in several domains over the years. It has become a
dominant ﬁeld of machine learning [5], with many inﬂuential works [6]. Although
MTL dates back several years, recent major advances in Deep Learning (DL)

2

Multi-Task Learning by Deep Collaboration

opened up opportunities for novel contributions. Works on grasping [7], pedes-
trian detection [8], natural language processing [9], face recognition [10][11] and
object detection [12] helped MTL make a resurgence in the DL community. They
have shown the potential of MTL to mitigate data scarcity when training deep
networks, which has inﬂuenced its growing popularity

MTL approaches can generally be divided into two major categories: hard
and soft parameter sharing [13]. Hard-parameter sharing dates back to the orig-
inal work of Caruana (1998) and is the most common of the two. Approaches
in this category have a shared central section with many heads (one per task).
Features from speciﬁc tasks compete together and those relevant to all tasks are
favored. Recent works in DL have shown that hard-parameter sharing can be
successful [14][15][7][11]. However, a too large emphasis on features relevant to all
tasks can be harmful for learning high-level features speciﬁc to a particular task.
These types of speciﬁc features are usually needed to obtain a good representa-
tion for the particular task. Also, shared layers are prone to be contaminated by
noise coming from noxious tasks [16]. These limitations can be detrimental even
though hard-parameter sharing reduces the risk of over-ﬁtting [17].

Soft-parameter sharing has been proposed as an alternative to alleviate these
drawbacks. Approaches in this category substitute the shared central section by
separate task-speciﬁc CNNs, but provide a knowledge sharing mechanism to
connect them. Each CNN can then learn task-speciﬁc features and share their
knowledge without interfering with others. Recent works in this category have
looked at regularizing the distance between task-speciﬁc parameters with a ‘2
norm [18] or a trace norm [19], training shared and private LSTM submod-
ules [16], partitioning the hidden layers into subspaces [20] and regularizing the
FC layers with tensor normal priors [21]. In the domain of continual learning,
progressive network [22] has also shown promising results for sequential transfer
learning, by employing lateral connections to previously learned networks.

In this paper, we present a novel soft-parameter knowledge sharing mecha-
nism for connecting task-speciﬁc CNNs in a MTL framework. We refer to our
approach as Deep Collaboration. We deﬁne connectivity in terms of a collabo-
rative block that uses two non-linear transformations with lateral connections.
One aggregates task-speciﬁc features into global features, and the other merges
back the global features into each task-speciﬁc CNN. Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
We evaluated our approach on the problem of facial landmark detection in a
MTL framework and obtained better results in comparison to other approaches
of the literature. We further assess the objectivity of our training framework by
randomly varying the contribution of each related tasks. Finally, we verify that
our collaborative block enables knowledge sharing with an ablation study that
shows the depth-speciﬁc inﬂuence of tasks that we know are related.

The content of our paper is organized as follows. In Section 2, we present
related works in MTL and facial landmark detection. We elaborate on our ap-
proach in Section 3, and present experimental results in Section 4. We ﬁnally
conclude our paper in Section 5. Our code is available here: [23].

Trottier et al.

3

2 Related Work

2.1 Multi-Task Learning

Our proposed Deep Collaboration knowledge sharing mechanism is related to
other existing approaches. One is Cross-Stitch (XS) [12], which connects task-
speciﬁc CNNs by linearly combining their feature maps at certain depths. One
drawback of XS is that it is limited to capture only linear dependencies between
each CNN. In contrast to XS, our approach uses non-linear transformations in
order to capture more complex dependencies.

Another related approach is Tasks-Constrained Deep Convolutional Network
(TCDCN) [15]. The authors proposed an early-stopping criterion to remove aux-
iliary tasks that start to overﬁt before becoming detrimental to the main task.
This approach has however several hyper-parameters to be selected manually.
For each task, it has an hyper-parameter controlling the period length of the
local window and a threshold that stops the task when the criterion exceeds it.
Unlike TCDCN, our approach has no hyper-parameters that need to be tuned to
the tasks at hand. Our collaborative block consists of a series of Batch Normal-
ization [24], ReLU [25], and convolutional layers shaped in a standard setting
that is commonly found in nowadays works.

Our proposed approach is also related to HyperFace [14]. The authors pro-
posed to fuse the layers at various depth and exploit features of diﬀerent levels
of complexity. Their goal was to allow low-level features with better localization
properties to help tasks such as landmark localization and pose detection, and al-
low high-level features with better class-speciﬁc properties to help tasks like face
detection and gender recognition. Although HyperFace is in the hard-parameter
sharing category and is not entirely related to our approach, the idea of feature
fusion is also central in our work. Instead of fusing the features at intermediate
layers of a single CNN, our approach aggregates same-level features of multiple
CNNs, at diﬀerent depth independently.

2.2 Facial Landmark Detection

Facial Landmark Detection (FLD) is an essential component in many face-
related tasks [26][27][28][29]. FLD can be described as follows: given the image of
a face of a person, the goal is to predict the (x, y)-position of speciﬁc landmarks
associated with key features of the visage. Applications such as face recogni-
tion [30], face validation [31], facial feature detection and tacking [32] rely on
the ability to correctly ﬁnd the location of these distinct facial landmarks in order
to succeed. Localizing facial key points like the center of the eyes, the corners of
the mouth, the tip of the nose and the earlobes is however a challenging problem
when many lighting conditions, head poses, facial expressions and occlusions in-
crease diversity of the face images. In addition to integrating this variability into
the estimation process, a FLD model must also take into account a number of
correlated factors. For instance, although both an angry person and a sad person
have frowned eyebrows, an angry person will have pinched lips while a sad person

4

Multi-Task Learning by Deep Collaboration

will have sunken mouth corners [33]. A particularity of datasets geared towards
FLD is that they are particularly well-suited for MTL. In addition to contain-
ing the position of the facial landmarks, these datasets also contain a number
of other labels that can be used to deﬁned auxiliary tasks. Gender recognition,
smile recognition, glasses recognition or face orientation are examples of tasks
often chosen to evaluate MTL approaches.

3 Deep Collaboration

Given T task-speciﬁc Convolutional Neural Networks (CNNs), our goal is to con-
nect them with lateral connections in order to allow domain-speciﬁc information
sharing. We deﬁne connectivity in terms of a collaborative block containing two
distinct non-linear transformations. One aggregates task-speciﬁc features into
global features, and the other merges back the global features into each task-
speciﬁc CNN. Our collaborative block is diﬀerentiable and can be dropped in
any existing CNN architectures as a whole. For this reason, we make no assump-
tion about the structure of the task-speciﬁc CNNs. Our approach can even work
with diﬀerent CNNs, but for the sake of simplicity, we suppose that the CNNs
are the same. We refer to it as the underlying network.

We also decompose the underlying network as a series of blocks. Each block
can be as small as a single layer, as large as the whole network itself, or based
on simple rules, such as grouping all layers with matching spatial dimensions or
grouping every n subsequent layers. The arrangement of the layers into blocks
does not change the composition of the underlying network. We only use it to
make explicit the depth at which we connect the task-speciﬁc CNNs.

Since our collaborative block can be inserted at any depth, we also drop the
depth index on the feature maps to further simplify the equations. As such,
we deﬁne the feature map output of a block at a certain depth as xt, where
t ∈ {1 . . . T } is the task index. Our approach takes as input all task-speciﬁc
feature maps xt and processes them into new feature maps yt as follows:

z = H([x1, . . . , xT ]) ,

yt = ReLU (xt + Ft([xt, z])) ,

(1)

where H and Ft represent the central and the task-speciﬁc aggregations re-
spectively, and [·] denotes depth-wise concatenation. We refer to Eq. (1) as our
collaborative block. The goal of H is to combine all task-speciﬁc feature maps xt
into a global feature map z representing uniﬁed knowledge, while the goal of F
is to merge back the global feature map z with each task-speciﬁc input xt. The
compositional structure of H and F is as follows:

H(·) = (ReLU ◦ BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,
F(·) = (BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,

(2)

(3)

where BN stands for Batch Normalization [24], Conv(h×w) for a standard convo-
lutional layer with ﬁlters of size (h × w), and ◦ is the usual function composition.
The ﬁrst Conv(1×1) layer in H divides the number of feature maps by a factor

Trottier et al.

5

Fig. 1. Example of our collaborative block applied on the feature maps of two task-
speciﬁc networks. The input feature maps (shown in 1(cid:13)) are ﬁrst concatenated depth-
wise and transformed into a global feature map ( 2(cid:13)). The global feature map is then
concatenate with each input feature map individually and transformed into task-speciﬁc
feature maps ( 3(cid:13)). Each resulting feature map is then added back to the input feature
map using a skip connection ( 4(cid:13)), which gives the ﬁnal outputs of the block ( 5(cid:13)).

of 4, while the ﬁrst Conv(1×1) layer in F divides it to match the size of xt. An
illustration of our collaborative block is shown in Fig. 1.

One particularity of our approach is that we use a skip connection in mapping
F. Recent works [34][35][36][37][38] have shown that networks with identity skip
connections are more easily able to learn proper input-output mappings. Inspired
by these works, we opted for an identity skip connection in F in order to more
easily learn the proper mapping to integrate domain-speciﬁc information from
the other tasks. In particular, identity skip connections put an incentive on
learning the identity mapping. We can see this by the ease at which the network
can obtain the identity mapping by simply pushing all the weights in F towards
zero. In our MTL context, the identity mapping can be seen as a way to remove
the inﬂuence of the global features z. This allows to take into account the cases
where integrating z back to the task-speciﬁc features xt would not help.

Another motivation for using an identity skip connection around the global
feature map z comes from the fact that depth inﬂuences the relevance of each
task towards another. Some task-speciﬁc CNNs can beneﬁt more when they
share their low-level features than their high-level features, while other beneﬁt
more in the other way. For instance, tasks such as landmark localization and
pose detection proﬁt more from low-level features containing better localization

6

Multi-Task Learning by Deep Collaboration

Fig. 2. Deep Collaboration Network (DCNet) using ResNet18 as underlying network
in a MTL setting on the MTFL dataset. The top part shows the block structure of
ResNet18 interleaved with our proposed collaborative block, while the bottom part
details each residual and task-speciﬁc FC blocks.

properties, while tasks such as face detection and gender recognition proﬁt more
from class-speciﬁc high-level features. Considering that CNNs learn a hierarchy
of increasingly abstract features, our collaborative block can take into account
task relevance by deactivating a diﬀerent set of residual mappings Ft based on
the depth at which it is inserted. An example of such specialization will be shown
in our ablative study in Section 4.4.

Fig. 2 presents an example of inserting our collaborative block at diﬀerent
depths in a MTL framework on the MTFL dataset [15]. In this particular case,
we opted for a ResNet18 as underlying network. We refer to this network as our
Deep Collaboration Network (DCNet). As we can see in the top part of the ﬁgure,
integrating our approach comes down to interleaving the underlying network
block structure with our collaborative block. Each collaborative block receives
as input the output of each task-speciﬁc block, processes them as detailed in
Eq. (1), and sends the result back to each task-speciﬁc network. Adding our
approach to any underlying network can be done by simply following the same
pattern of interleaving the network block structure with our collaborative block.

4 Experiments

In this section, we detail our Multi-Task Learning (MTL) training framework and
present our experiments in Facial Landmark Detection (FLD) tasks. We further
evaluate the eﬀect of data scarcity on performance and illustrate an example of
knowledge sharing between task-speciﬁc CNNs with an ablation study.

Trottier et al.

7

4.1 Multi-Task Learning Training Framework

The goal of Facial Landmark Detection (FLD) is to predict the (x, y)-position
of speciﬁc landmarks associated with key features of the visage. While the num-
ber and type of landmarks are speciﬁc to each dataset, examples of standard
landmarks to be predicted are the corners of the mouth, the tip of the nose
and the center of the eyes. In addition to the facial landmarks, each dataset
further deﬁnes a number of related tasks. These related tasks also vary from
one dataset to another, and are typically gender recognition, smile recognition,
glasses recognition or face orientation.

On a more technical level, we deﬁne a learning framework in which we treat
each task as a classiﬁcation problem. While this is straightforward for gender,
smile and glasses recognition as they are already classiﬁcation tasks, it is a
bit more tricky for face orientation and FLD. For face orientation, instead of
predicting the roll, yaw and pitch real value as in a regression problem, we
divide each component into 30 degrees wide bins and predict the label of the bin
corresponding to the value. Similarly for FLD, rather than predicting the real
(x, y)-position of each landmark, we divide the image into 1 pixel wide bins and
predict the label of the bin corresponding to the value. Note that we still use
the original real values when comparing our prediction with the ground truth,
so that we incorporate our approximation errors in the ﬁnal score.

We report our results using the landmark failure rate metric [15], which is
deﬁned as follows: we ﬁrst compute the mean distance between the predicted
landmarks and the ground truth landmarks, then normalize it by the inter-
ocular distance from the center of the eyes. A normalized mean distance greater
than 10% is reported as a failure.

4.2 Facial Landmark Detection on the MTFL Task

As a ﬁrst experiment, we performed facial landmark detection on the Multi-Task
Facial Landmark (MTFL) task [15]. The dataset contains 12,995 face images an-
notated with ﬁve facial landmarks and four related attributes of gender, smiling,
wearing glasses and face proﬁle (ﬁve proﬁles in total). The training set has 10,000
images, while the test set has 2,995 images. We perform four sets of experiments
using an ImageNet pre-trained AlexNet, an ImageNet pre-trained ResNet18, an
un-pretrained AlexNet and an un-pretrained ResNet18 as underlying networks.
For AlexNet, we apply our collaborative block after each max pooling layer,
while for ResNet18, we do as shown in Fig. 2.

We compare our approach to several other approaches of the literature. We
include single-task learning (AN-S when using AlexNet as underlying network,
RN-S when using ResNet18), hard-parameter sharing MTL (AN and RN), hard-
parameter sharing MTL where the central section is widened to match the num-
ber of parameters of our approach (ANx and RNx), HyperFace (HF) [14], Tasks-
Constrained Deep Convolutional Network (TCDCN) [15], Cross-Stitch (XS) [12]
and XS widen to match the number of parameters of our approach (XSx). Ex-
cept for TCDCN, we train each network ourselves three times for 300 epochs and

8

Multi-Task Learning by Deep Collaboration

Fig. 3. Landmark failure rates (%) on the MTFL task. The reported values are the
average over the last ﬁve epochs, averaged over three tries. The left plot presents our
results with AlexNet as the underlying network, while the right one with ResNet18.
AN-S and RN-S stand for single-task training, AN and RN for multi-task training with
a single central network, ANx and RNx for multi-task training with a single central
network widen to match the number of parameters of our approach, HF for HyperFace,
TCDCN for [15]’s approach and XS for Cross-Stitch. In each instance, the left column
(blue) is for un-pretrained networks, while the right column (green) is for pre-trained
networks. Our proposed approach obtains the lowest failure rates overall.

report landmark failure rates averaged over the last ﬁve epochs, further averaged
over the three tries.

Fig. 3 presents our FLD results on the MTFL dataset. The left part of the
ﬁgure corresponds to using AlexNet as underlying network, while the right one
corresponds to ResNet18. The top part reports the landmark failure rates, while
the bottom part reports the mean error. In each plot, the left bar (blue) is for
un-pretrained network, while the right bar (green) is for ImageNet pre-trained
network. In addition, Fig. 4 shows example predictions from DCNet with pre-
trained ResNet18 as underlying network. The ﬁrst two examples were reported as
successes, while the last two are failures. The ground truth elements are colored
in green, while our predictions are colored in blue. We also include the labels of
the related tasks: gender, smiling, wearing glasses and face proﬁle.

The results of Fig. 3 show that our proposed approach obtained the lowest
failure rates and mean error in each case. Indeed, our DCNet with un-pretrained
and pre-trained AlexNet as underlying network obtained 19.67% and 19.96%
failure rates respectively, and 14.95% and 13.52% with ResNet18. This is sig-
niﬁcantly lower than the other approaches to which we compare ourselves. For
instance, with AlexNet, HF had 27.75% and 27.32%, XS had 26.41% and 25.65%,

Trottier et al.

9

MTFL

AFLW

Fig. 4. Example predictions of our DCNet with pre-trained ResNet18 as underlying
network on the MTFL and AFLW task. For MTFL, the ﬁrst two examples are successes,
while last two are failure cases. For AFLW, the ﬁrst three examples are successes, while
the last one is a failure case. Elements in green correspond to ground truth, while those
in blue correspond to predictions. Facial landmarks are shown as small dots, and related
tasks labels are displayed on the side. As we can see, over-exposition and tilted face
proﬁle can have a large impact on the prediction quality.

TCDCN had 25.00%1, and XSx had 25.23%. With ResNet18, XS had 18.43% and
15.52% respectively, and XSx had 17.28. We obtained the highest improvements
when using AlexNet as the underlying network when comparing to XS. With un-
pretrained and pre-trained AlexNet, we obtained improvements of 6.74% and
5.69%, while we obtained 3.48% and 2.00% with ResNet18. Performing MTL
with our approach can thus improve performance over using other approaches
of the literature.

Another result that we can see from Fig. 3 is that our soft-parameter shar-
ing approach obtains higher performance than the hard-parameter sharing ap-
proaches with matching number of parameters. For instance, increasing the
number of parameters of hard-parameter sharing AlexNet lowers it error rate
from 28.02% (AN) to 26.88% (ANx), but our approach lowers it further to
19.67%. Similarly, increasing the number of parameters of hard-parameter shar-
ing ResNet18 lowers it error rate from 20.05% (RN) to 16.75% (RNx), but our
approach lowers it further to 14.95%. These results are interesting because they
show that while increasing the number of parameters is an eﬀortless avenue to
improve performance, it has limitations. Developing novel approaches to enhance
network connectivity in a soft-parameter sharing setting seems more rewarding.
This may help to motivate new eﬀorts in this avenue to further leverage domain-
information of related tasks.

1 Zhang et. al only provided results with pre-trained AlexNet [15]

10

Multi-Task Learning by Deep Collaboration

Table 1. Landmark failure rate results on the AFLW dataset using a pre-trained
ResNet18 as underlying network. The presented values are averaged over the last ﬁve
epochs, further averaged over three tries. The ﬁrst column is the train / test ratio,
and the subsequent ones are the networks: single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS). Our approach obtains the best perfor-
mance in all cases, except the ﬁrst one where we observe over-ﬁtting.

Train / Test Ratio

0.1 / 0.9
0.3 / 0.7
0.5 / 0.5
0.7 / 0.3
0.9 / 0.1

RN-S

57.39
31.84
23.41
21.47
13.03

Networks

RN

58.00
32.00
23.31
21.92
12.80

XS

73.06
36.24
26.02
22.37
13.51

Ours

60.64
29.73
20.77
18.50
10.82

4.3 Eﬀect of Data Scarcity on the AFLW Task

As second experiment, we evaluated the inﬂuence of the number of training
examples to simulate data scarcity on the Annotated Facial Landmarks in the
Wild (AFLW) task [39]. The dataset has 21,123 Flickr images, and each image
can contain more than one face. Instead of using the images as provided, we
process them using the available face bounding boxes. We extract all faces with
visible landmarks, which gives a total of 2,111 images. This dataset deﬁnes 21
facial landmarks and has 3 related tasks (gender, wearing glasses and face ori-
entation). For face orientation, we divide the roll, yaw and pitch into 30 degrees
wide bins (14 bins in total), and predict the label corresponding to each real
value.

Our experiment works as follows. With a pre-trained ResNet18 as underlying
network, we compare our approach to single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS) by training on a varying number
of images. We use ﬁve diﬀerent train / test ratios, starting with 0.1 / 0.9 up
to 0.9 / 0.1 by 0.2 increment. In other words, we train each approaches on the
ﬁrst r% of the available images and test on the other (1 − r)%, then repeat
for all the other train / test ratios. We use the same training framework as in
section 4.2. We train each network three times for 300 epochs, and report the
landmark failure rate averaged over the last ﬁve epochs, further averaged over
the three tries. Example predictions are shown in Fig. 4.

As we can see in Table 1, our approach obtained the best performance in
all cases except the ﬁrst one. Indeed, we observe between 1.98% and 6.51%
improvements with train / test ratios from 0.3 / 0.7 to 0.9 / 0.1, while we obtain
a negative relative change of 3.25% with train / test ratio of 0.1 / 0.9. However,
since all multi-task approaches obtained higher failure rates than the single-task
approach, this suggests that the networks are over-ﬁtting the small training set.
Nonetheless, these results show that we can obtain better performance using our
approach.

Trottier et al.

11

Fig. 5. Landmark failure rate improvement (in %) of our approach compared to XS
when sampling random task weights. We used a pre-trained ResNet18 as underlying
network. The histogram at the left and the plot at the top right represents performance
improvement achieved by our proposed approach (positive value means lower failure
rates), while the plot at the bottom right corresponds to the log of the task weights. Our
approach outperformed XS in 86 out of the 100 tries, thus empirically demonstrating
that our learning framework was not unfavorable towards XS and that our approach
is less sensitive to the task weights λ.

One particularity that we observe in Table 1 is that the XS network has
relatively high failure rates. In the previous experiment of Section 4.2, XS had
either similar or better performance than the other approaches (except ours).
This could be due to our current multi-task learning framework that is unfavor-
able towards XS. In order to investigate whether this is the case, we perform
the following additional experiment. Using a pre-trained ResNet18 as underlying
network, we compare our approach to XS by training each network 100 times
using task weights randomly sampled from a log-uniform distribution. Speciﬁ-
cally, we ﬁrst sample from a uniform distribution γ ∼ U(log(1e−4), log(1)), then
use λ = exp(γ) as the weight. We trained both XS and our approach for 300
epochs with the same task weights using a train / test ratio of 0.5 / 0.5.

Figure 5 presents the results of this experiment. The plot at the top right
of the ﬁgure represents the landmark failure rate improvement (in %) of our
approach compared to XS, while the plot at the bottom right corresponds to the
log of the task weights for each try. In 86 out of the 100 tries, our approach had
a positive failure rate improvement, that is, obtained lower failure rates than
XS. As we can see in the histogram at the left of Fig. 5, the improvement rate
is normally distributed around 2.80%, has a median improvement of 3.66% and
a maximum improvement of 9.83%. Even though we sampled at random the
weights of the related tasks, our approach outperforms XS in the majority of the
cases. Our learning framework was therefore not unfavorable toward XS.

4.4 Illustration of Knowledge Sharing With an Ablation Study

As third experiment, we perform an ablation study on the MTFL task [15] with
an un-pretrained ResNet18 as underlying network. The goal of this experiment

12

Multi-Task Learning by Deep Collaboration

Fig. 6. Results of our ablation study on the MTFL dataset with an un-pretrained
ResNet18 as underlying network. We remove each task-speciﬁc features from each re-
spective central aggregation layer and evaluate the eﬀect on landmark failure rate. The
rows represent the task-speciﬁc CNNs, while the columns correspond to the network
block structure. Blocks with a high saturated color were found to have a large impact
on failure rate. In particular, this ablative study shows that the inﬂuence of high-
level face proﬁle features is large within our proposed architecture. This corroborates
with the well-known fact that the location of facial landmarks is closely dependent on
the orientation of the face. This constitutes an empirical evidence of domain-speciﬁc
information sharing via our approach.

is to verify that our collaborative block eﬀectively enables knowledge sharing
between task-speciﬁc CNNs. To do so, we evaluate the impact, on facial landmark
detection, of removing the contribution of each task-speciﬁc features. We zero out
the designated feature map xt before concatenation at the input of the central
aggregation H. The network is trained using the same framework as explained
in Sec. 4.1, and the ablation study is performed at test time on the test set when
training is done.

Figure 6 presents the results of our ablation study. The rows represent each
task-speciﬁc CNN, while the columns correspond to the network block structure.
The blocks are ordered from left (input) to right (output), while the task-speciﬁc
networks are ordered from top (main task) to bottom (related tasks). The color
saturation indicates the inﬂuence of removing the task-speciﬁc feature maps from
the central aggregation at the corresponding depth. A high saturation reﬂects
high inﬂuence on failure rate, while a low saturation reﬂects low inﬂuence.

As ﬁrst result, removing features from the facial landmark detection network
signiﬁcantly increases landmark failure rate. For instance, we observe a negative
(worse) relative change of 29.72% and 47.00% in failure rate by removing fea-
tures from Block 3 and Block 2 respectively. This illustrates that the main-task
network both contributed to and fed from the global features computed by the
central aggregation H. The CNN for landmark detection had the possibility to
remove the contribution of the global features, and so isolate itself from the other
CNNs, but the opposite occurred. We actually observe a mutual inﬂuence be-
tween the CNNs, where the task-speciﬁc features from the facial landmark CNN

Trottier et al.

13

inﬂuence the quality of the global features, which in turn inﬂuence the quality
of the subsequent task-speciﬁc features.

Another result that we can see from Fig. 6 is that Block 5 of task Proﬁle
has the highest inﬂuence on failure rate. We observe a negative relative change
of 83.87% by removing the features maps of task Proﬁle from the central ag-
gregation. What is particularly interesting in this case is that we observe this
high relative change at Block 5, which corresponds to the highest block in the
network. Since the block lies at the top of the network, it outputs features with
a high level of abstraction. We therefore expect that these features represent
high-level factors of variation corresponding to face orientation, which should
look like a rotation matrix. It therefore makes sense that features representing
the orientation of the face would be useful to predict facial landmarks, since
we know that the location of the facial landmarks is closely dependent on the
orientation of the face. The landmark CNN can use these rich features to bet-
ter rotate the predicted facial landmarks. This is indeed what we observe in
Fig. 6. These results constitute an empirical evidence that our approach allows
leveraging domain-speciﬁc information from related tasks.

4.5 Facial Landmark Detection With MTCNN

As ﬁnal experiment, we performed an experimental evaluation using the recent
Multi-Task Cascaded Convolutional Network (MTCNN) [27]. The authors pro-
posed a cascade structure of three stages, where each stage is composed of a
multi-task CNN. MTCNN performs predictions in a coarse-to-ﬁne manner. The
CNN of the ﬁrst stage generates (in a fully-convolutional way) many hypothe-
ses about the position of the face and the facial landmarks, and the subsequent
second and third stages reﬁnes them. The CNNs are trained sequentially with
hard-negative mining, in a hard-parameter sharing setting.

We implemented our approach in the available code project [40] and com-
pared ourself to MTCNN. We followed the provided hard negative mining recipe
and generated our images. For landmark detection, we used the LFWNet [26]
and CelebA [41] datasets, and generated 600k face images with facial landmarks.
For face detection, we used the WIDER [42] dataset, and generated 1.5M face
images with a bounding box. We trained a MTCNN with the stage networks
connected with our collaborative block, and a standard MTCNN with widened
stage networks to match the number of parameters.

On the test set of MTFL [15] dataset, standard MTCNN obtained a land-
mark failure rate of 37.85%, a mean error of 0.0996 and failed to detect a face
112 times, while our approach obtained better performances with 28.97%, 0.0930
and 79 respectively. Note that the reason MTCNN obtains worse performance
than our Deep Collaboration Network (DCNet), as reported in Fig. 3, is because
it has fewer parameters. DCNet has about 85M parameters, while the sum of
all three stage-CNN in MTCNN is about 2M. This is because MTCNN is care-
fully designed to balance computational speed and landmark detection precision.
It can predict many faces in high-dimensional images with a low computation
burden. An example of its prediction capability is shown in Fig. 7.

14

Multi-Task Learning by Deep Collaboration

Fig. 7. MTCNN predictions on the photo of the 2018 Oscar nominees (image resolution
of 2983 × 1197). The stage-CNNs are trained using our proposed collaborative block.
The coarse-to-ﬁne detection scheme employed by MTCNN allows predicting many faces
in high-dimensional images with low computational burden.

5 Conclusion and Future Work

In this paper, we proposed a novel soft-parameter knowledge sharing mechanism
based on lateral connections for Multi-Task Learning (MTL). Our proposed ap-
proach implements connectivity in term of a collaborative block, which uses two
distinct non-linear transformations. The ﬁrst one aggregates task-speciﬁc fea-
tures into global features, and the other merges back the global features into each
task-speciﬁc Convolutional Neural Network (CNN). Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
Our results on facial landmark detection tasks showed that networks connected
with our proposed collaborative block outperformed the other state-of-the-art
approaches, including the recent Cross-Stitch and MTCNN approach. We ver-
ify that our collaborative block eﬀectively enables knowledge sharing between
task-speciﬁc CNNs with an ablation study. We observed that the CNNs incor-
porated features with a varying level of abstraction from the other CNNs, by
observing the depth-speciﬁc inﬂuence of tasks that we know are related. These
results constituted an empirical evidence that our approach allows leveraging
domain-speciﬁc information from related tasks. Evaluating our proposed ap-
proach on other MTL problems could be an interesting avenue for future works.
For instance, the recurrent networks used to solve natural language processing
problems could beneﬁt from our approach.

Acknowledgements

We gratefully acknowledge the support of NVIDIA Corporation for providing a
Tesla Titan X for our experiments through their Hardware Grant Program.

Trottier et al.

15

References

1. Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classiﬁcation with deep convo-

lutional neural networks. In: NIPS. (2012) 1097–1105

2. Hochreiter, S.: The vanishing gradient problem during learning recurrent neural
nets and problem solutions. International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems 6(02) (1998) 107–116

A survey on multi-task learning.

arXiv preprint

3. Zhang, Y., Yang, Q.:
arXiv:1707.08114 (2017)

4. Caruana, R.: Multitask learning. In: Learning to learn. Springer (1998) 95–133
5. Zhang, M.L., Zhou, Z.H.: A review on multi-label learning algorithms. TKDE

6. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: KDD, ACM (2004)

26(8) (2014) 1819–1837

109–117

7. Pinto, L., Gupta, A.: Learning to push by grasping: Using multiple tasks for

eﬀective learning. In: ICRA, IEEE (2017) 2161–2168

8. Tian, Y., Luo, P., Wang, X., Tang, X.: Pedestrian detection aided by deep learning

semantic tasks. In: CVPR. (2015) 5079–5087

9. Liu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y.Y.: Representation learning
using multi-task deep neural networks for semantic classiﬁcation and information
retrieval. In: HLT-NAACL. (2015) 912–921

10. Yim, J., Jung, H., Yoo, B., Choi, C., Park, D., Kim, J.: Rotating your face using

multi-task deep neural network. In: CVPR. (2015) 676–684

11. Yin, X., Liu, X.: Multi-task convolutional neural network for face recognition.

arXiv preprint arXiv:1702.04710 (2017)

12. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-

task learning. In: CVPR. (2016) 3994–4003

13. Ruder, S.: An overview of multi-task learning in deep neural networks. CoRR

abs/1706.05098 (2017)

14. Ranjan, R., Patel, V.M., Chellappa, R.: Hyperface: A deep multi-task learning
framework for face detection, landmark localization, pose estimation, and gender
recognition. arXiv preprint arXiv:1603.01249 (2016)

15. Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi-

task learning. In: ECCV, Springer (2014) 94–108

16. Liu, P., Qiu, X., Huang, X.: Adversarial multi-task learning for text classiﬁcation.

(2017)

17. Baxter, J.: A bayesian/information theoretic model of learning to learn via multiple

task sampling. Machine learning 28(1) (1997) 7–39

18. Duong, L., Cohn, T., Bird, S., Cook, P.: Low resource dependency parsing: Cross-
lingual parameter sharing in a neural network parser. In: ACL. Volume 2. (2015)
845–850

19. Yang, Y., Hospedales, T.M.: Trace norm regularised deep multi-task learning.

arXiv preprint arXiv:1606.04038 (2016)

20. Ruder, S., Bingel, J., Augenstein, I., Søgaard, A.: Sluice networks: Learning what
to share between loosely related tasks. arXiv preprint arXiv:1705.08142 (2017)
21. Long, M., Wang, J.: Learning multiple tasks with deep relationship networks.

arXiv preprint arXiv:1506.02117 (2015)

22. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv
preprint arXiv:1606.04671 (2016)

16

Multi-Task Learning by Deep Collaboration

23. Trottier, L.: Deep collaboration network in pytorch.

https://github.com/

ltrottier/deep-collaboration-network

24. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 448–456

25. Nair, V., Hinton, G.: Rectiﬁed linear units improve restricted boltzmann machines.

In: ICML. (2010) 807–814

26. Sun, Y., Wang, X., Tang, X.: Deep convolutional network cascade for facial point

detection. In: CVPR. (2013) 3476–3483

27. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using
multitask cascaded convolutional networks. IEEE Signal Processing Letters 23(10)
(2016) 1499–1503

28. Jourabloo, A., Liu, X.: Large-pose face alignment via cnn-based dense 3d model

ﬁtting. In: CVPR. (2016) 4188–4196

29. Baltrušaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial

behavior analysis toolkit. In: WACV, IEEE (2016) 1–10

30. Ding, C., Tao, D.: Robust face recognition via multimodal deep face representation.

IEEE Transactions on Multimedia 17(11) (2015) 2049–2058

31. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to

human-level performance in face veriﬁcation. In: CVPR. (2014) 1701–1708

32. Zhang, C., Zhang, Z.: Improving multiview face detection with multi-task deep

convolutional neural networks. In: WACV, IEEE (2014) 1036–1041

33. Fabian Benitez-Quiroz, C., Srinivasan, R., Martinez, A.M.: Emotionet: An accu-
rate, real-time algorithm for the automatic annotation of a million facial expres-
sions in the wild. In: CVPR. (2016) 5562–5570

34. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

35. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: ECCV, Springer (2016) 630–645

In: CVPR. (2016) 770–778

36. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with

stochastic depth. In: ECCV, Springer (2016) 646–661

37. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations

for deep neural networks. In: CVPR, IEEE (2017) 5987–5995

38. Veit, A., Wilber, M.J., Belongie, S.: Residual networks behave like ensembles of

relatively shallow networks. In: NIPS. (2016) 550–558

39. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization. In:
ICCV Workshops, IEEE (2011) 2144–2151

40. Kim, K.K.: Deep learning face detection and recognition, implemented by pytorch.

https://github.com/kuaikuaikim/DFace

41. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:

42. Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark.

ICCV. (2015)

In: CVPR. (2016) 5525–5533

8
1
0
2
 
r
a

M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
1
1
0
0
.
1
1
7
1
:
v
i
X
r
a

Multi-Task Learning by Deep Collaboration and
Application in Facial Landmark Detection

Ludovic Trottier

Philippe Giguère

Brahim Chaib-draa

ludovic.trottier.1@ulaval.ca
{philippe.giguere,brahim.chaib-draa}@ift.ulaval.ca

Laval University, Québec, Canada

Abstract. Convolutional neural networks (CNNs) have become the most
successful approach in many vision-related domains. However, they are
limited to domains where data is abundant. Recent works have looked
at multi-task learning (MTL) to mitigate data scarcity by leveraging
domain-speciﬁc information from related tasks. In this paper, we present
a novel soft-parameter sharing mechanism for CNNs in a MTL setting,
which we refer to as Deep Collaboration. We propose taking into ac-
count the notion that task relevance depends on depth by using lateral
transformation blocs with skip connections. This allows extracting task-
speciﬁc features at various depth without sacriﬁcing features relevant to
all tasks. We show that CNNs connected with our Deep Collaboration
obtain better accuracy on facial landmark detection with related tasks.
We ﬁnally verify that our approach eﬀectively allows knowledge sharing
by showing depth-speciﬁc inﬂuence of tasks that we know are related.

1 Introduction

Over the past few years, Convolutional Neural Networks (CNNs) have become
the leading approach in many vision-related tasks [1]. Their ability to learn
a hierarchy of increasingly abstract concepts allows them to transform com-
plex high-dimensional input images into simple low-dimensional output features.
CNNs have been used in many settings, but their need to have a large amount
of data during training has restricted them to domains where data is abundant.
Optimizing CNNs is tricky not only because of problems like vanishing / ex-
ploding gradients [2], but also because they typically have many parameters to
be learned. While previous works have looked at supervised and unsupervised
pre-training to improve generalization, others have considered casting their orig-
inal single-task problem into a new Multi-Task Learning (MTL) problem [3]. As
Caruana (1998) [4] explained in his seminal work: “MTL improves generalization
by leveraging the domain-speciﬁc information contained in the training signals
of related tasks". Exploring new ways to more eﬃciently gather information from
related tasks would help to further improve generalization on the main one.

MTL has proven its value in several domains over the years. It has become a
dominant ﬁeld of machine learning [5], with many inﬂuential works [6]. Although
MTL dates back several years, recent major advances in Deep Learning (DL)

2

Multi-Task Learning by Deep Collaboration

opened up opportunities for novel contributions. Works on grasping [7], pedes-
trian detection [8], natural language processing [9], face recognition [10][11] and
object detection [12] helped MTL make a resurgence in the DL community. They
have shown the potential of MTL to mitigate data scarcity when training deep
networks, which has inﬂuenced its growing popularity

MTL approaches can generally be divided into two major categories: hard
and soft parameter sharing [13]. Hard-parameter sharing dates back to the orig-
inal work of Caruana (1998) and is the most common of the two. Approaches
in this category have a shared central section with many heads (one per task).
Features from speciﬁc tasks compete together and those relevant to all tasks are
favored. Recent works in DL have shown that hard-parameter sharing can be
successful [14][15][7][11]. However, a too large emphasis on features relevant to all
tasks can be harmful for learning high-level features speciﬁc to a particular task.
These types of speciﬁc features are usually needed to obtain a good representa-
tion for the particular task. Also, shared layers are prone to be contaminated by
noise coming from noxious tasks [16]. These limitations can be detrimental even
though hard-parameter sharing reduces the risk of over-ﬁtting [17].

Soft-parameter sharing has been proposed as an alternative to alleviate these
drawbacks. Approaches in this category substitute the shared central section by
separate task-speciﬁc CNNs, but provide a knowledge sharing mechanism to
connect them. Each CNN can then learn task-speciﬁc features and share their
knowledge without interfering with others. Recent works in this category have
looked at regularizing the distance between task-speciﬁc parameters with a ‘2
norm [18] or a trace norm [19], training shared and private LSTM submod-
ules [16], partitioning the hidden layers into subspaces [20] and regularizing the
FC layers with tensor normal priors [21]. In the domain of continual learning,
progressive network [22] has also shown promising results for sequential transfer
learning, by employing lateral connections to previously learned networks.

In this paper, we present a novel soft-parameter knowledge sharing mecha-
nism for connecting task-speciﬁc CNNs in a MTL framework. We refer to our
approach as Deep Collaboration. We deﬁne connectivity in terms of a collabo-
rative block that uses two non-linear transformations with lateral connections.
One aggregates task-speciﬁc features into global features, and the other merges
back the global features into each task-speciﬁc CNN. Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
We evaluated our approach on the problem of facial landmark detection in a
MTL framework and obtained better results in comparison to other approaches
of the literature. We further assess the objectivity of our training framework by
randomly varying the contribution of each related tasks. Finally, we verify that
our collaborative block enables knowledge sharing with an ablation study that
shows the depth-speciﬁc inﬂuence of tasks that we know are related.

The content of our paper is organized as follows. In Section 2, we present
related works in MTL and facial landmark detection. We elaborate on our ap-
proach in Section 3, and present experimental results in Section 4. We ﬁnally
conclude our paper in Section 5. Our code is available here: [23].

Trottier et al.

3

2 Related Work

2.1 Multi-Task Learning

Our proposed Deep Collaboration knowledge sharing mechanism is related to
other existing approaches. One is Cross-Stitch (XS) [12], which connects task-
speciﬁc CNNs by linearly combining their feature maps at certain depths. One
drawback of XS is that it is limited to capture only linear dependencies between
each CNN. In contrast to XS, our approach uses non-linear transformations in
order to capture more complex dependencies.

Another related approach is Tasks-Constrained Deep Convolutional Network
(TCDCN) [15]. The authors proposed an early-stopping criterion to remove aux-
iliary tasks that start to overﬁt before becoming detrimental to the main task.
This approach has however several hyper-parameters to be selected manually.
For each task, it has an hyper-parameter controlling the period length of the
local window and a threshold that stops the task when the criterion exceeds it.
Unlike TCDCN, our approach has no hyper-parameters that need to be tuned to
the tasks at hand. Our collaborative block consists of a series of Batch Normal-
ization [24], ReLU [25], and convolutional layers shaped in a standard setting
that is commonly found in nowadays works.

Our proposed approach is also related to HyperFace [14]. The authors pro-
posed to fuse the layers at various depth and exploit features of diﬀerent levels
of complexity. Their goal was to allow low-level features with better localization
properties to help tasks such as landmark localization and pose detection, and al-
low high-level features with better class-speciﬁc properties to help tasks like face
detection and gender recognition. Although HyperFace is in the hard-parameter
sharing category and is not entirely related to our approach, the idea of feature
fusion is also central in our work. Instead of fusing the features at intermediate
layers of a single CNN, our approach aggregates same-level features of multiple
CNNs, at diﬀerent depth independently.

2.2 Facial Landmark Detection

Facial Landmark Detection (FLD) is an essential component in many face-
related tasks [26][27][28][29]. FLD can be described as follows: given the image of
a face of a person, the goal is to predict the (x, y)-position of speciﬁc landmarks
associated with key features of the visage. Applications such as face recogni-
tion [30], face validation [31], facial feature detection and tacking [32] rely on
the ability to correctly ﬁnd the location of these distinct facial landmarks in order
to succeed. Localizing facial key points like the center of the eyes, the corners of
the mouth, the tip of the nose and the earlobes is however a challenging problem
when many lighting conditions, head poses, facial expressions and occlusions in-
crease diversity of the face images. In addition to integrating this variability into
the estimation process, a FLD model must also take into account a number of
correlated factors. For instance, although both an angry person and a sad person
have frowned eyebrows, an angry person will have pinched lips while a sad person

4

Multi-Task Learning by Deep Collaboration

will have sunken mouth corners [33]. A particularity of datasets geared towards
FLD is that they are particularly well-suited for MTL. In addition to contain-
ing the position of the facial landmarks, these datasets also contain a number
of other labels that can be used to deﬁned auxiliary tasks. Gender recognition,
smile recognition, glasses recognition or face orientation are examples of tasks
often chosen to evaluate MTL approaches.

3 Deep Collaboration

Given T task-speciﬁc Convolutional Neural Networks (CNNs), our goal is to con-
nect them with lateral connections in order to allow domain-speciﬁc information
sharing. We deﬁne connectivity in terms of a collaborative block containing two
distinct non-linear transformations. One aggregates task-speciﬁc features into
global features, and the other merges back the global features into each task-
speciﬁc CNN. Our collaborative block is diﬀerentiable and can be dropped in
any existing CNN architectures as a whole. For this reason, we make no assump-
tion about the structure of the task-speciﬁc CNNs. Our approach can even work
with diﬀerent CNNs, but for the sake of simplicity, we suppose that the CNNs
are the same. We refer to it as the underlying network.

We also decompose the underlying network as a series of blocks. Each block
can be as small as a single layer, as large as the whole network itself, or based
on simple rules, such as grouping all layers with matching spatial dimensions or
grouping every n subsequent layers. The arrangement of the layers into blocks
does not change the composition of the underlying network. We only use it to
make explicit the depth at which we connect the task-speciﬁc CNNs.

Since our collaborative block can be inserted at any depth, we also drop the
depth index on the feature maps to further simplify the equations. As such,
we deﬁne the feature map output of a block at a certain depth as xt, where
t ∈ {1 . . . T } is the task index. Our approach takes as input all task-speciﬁc
feature maps xt and processes them into new feature maps yt as follows:

z = H([x1, . . . , xT ]) ,

yt = ReLU (xt + Ft([xt, z])) ,

(1)

where H and Ft represent the central and the task-speciﬁc aggregations re-
spectively, and [·] denotes depth-wise concatenation. We refer to Eq. (1) as our
collaborative block. The goal of H is to combine all task-speciﬁc feature maps xt
into a global feature map z representing uniﬁed knowledge, while the goal of F
is to merge back the global feature map z with each task-speciﬁc input xt. The
compositional structure of H and F is as follows:

H(·) = (ReLU ◦ BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,
F(·) = (BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,

(2)

(3)

where BN stands for Batch Normalization [24], Conv(h×w) for a standard convo-
lutional layer with ﬁlters of size (h × w), and ◦ is the usual function composition.
The ﬁrst Conv(1×1) layer in H divides the number of feature maps by a factor

Trottier et al.

5

Fig. 1. Example of our collaborative block applied on the feature maps of two task-
speciﬁc networks. The input feature maps (shown in 1(cid:13)) are ﬁrst concatenated depth-
wise and transformed into a global feature map ( 2(cid:13)). The global feature map is then
concatenate with each input feature map individually and transformed into task-speciﬁc
feature maps ( 3(cid:13)). Each resulting feature map is then added back to the input feature
map using a skip connection ( 4(cid:13)), which gives the ﬁnal outputs of the block ( 5(cid:13)).

of 4, while the ﬁrst Conv(1×1) layer in F divides it to match the size of xt. An
illustration of our collaborative block is shown in Fig. 1.

One particularity of our approach is that we use a skip connection in mapping
F. Recent works [34][35][36][37][38] have shown that networks with identity skip
connections are more easily able to learn proper input-output mappings. Inspired
by these works, we opted for an identity skip connection in F in order to more
easily learn the proper mapping to integrate domain-speciﬁc information from
the other tasks. In particular, identity skip connections put an incentive on
learning the identity mapping. We can see this by the ease at which the network
can obtain the identity mapping by simply pushing all the weights in F towards
zero. In our MTL context, the identity mapping can be seen as a way to remove
the inﬂuence of the global features z. This allows to take into account the cases
where integrating z back to the task-speciﬁc features xt would not help.

Another motivation for using an identity skip connection around the global
feature map z comes from the fact that depth inﬂuences the relevance of each
task towards another. Some task-speciﬁc CNNs can beneﬁt more when they
share their low-level features than their high-level features, while other beneﬁt
more in the other way. For instance, tasks such as landmark localization and
pose detection proﬁt more from low-level features containing better localization

6

Multi-Task Learning by Deep Collaboration

Fig. 2. Deep Collaboration Network (DCNet) using ResNet18 as underlying network
in a MTL setting on the MTFL dataset. The top part shows the block structure of
ResNet18 interleaved with our proposed collaborative block, while the bottom part
details each residual and task-speciﬁc FC blocks.

properties, while tasks such as face detection and gender recognition proﬁt more
from class-speciﬁc high-level features. Considering that CNNs learn a hierarchy
of increasingly abstract features, our collaborative block can take into account
task relevance by deactivating a diﬀerent set of residual mappings Ft based on
the depth at which it is inserted. An example of such specialization will be shown
in our ablative study in Section 4.4.

Fig. 2 presents an example of inserting our collaborative block at diﬀerent
depths in a MTL framework on the MTFL dataset [15]. In this particular case,
we opted for a ResNet18 as underlying network. We refer to this network as our
Deep Collaboration Network (DCNet). As we can see in the top part of the ﬁgure,
integrating our approach comes down to interleaving the underlying network
block structure with our collaborative block. Each collaborative block receives
as input the output of each task-speciﬁc block, processes them as detailed in
Eq. (1), and sends the result back to each task-speciﬁc network. Adding our
approach to any underlying network can be done by simply following the same
pattern of interleaving the network block structure with our collaborative block.

4 Experiments

In this section, we detail our Multi-Task Learning (MTL) training framework and
present our experiments in Facial Landmark Detection (FLD) tasks. We further
evaluate the eﬀect of data scarcity on performance and illustrate an example of
knowledge sharing between task-speciﬁc CNNs with an ablation study.

Trottier et al.

7

4.1 Multi-Task Learning Training Framework

The goal of Facial Landmark Detection (FLD) is to predict the (x, y)-position
of speciﬁc landmarks associated with key features of the visage. While the num-
ber and type of landmarks are speciﬁc to each dataset, examples of standard
landmarks to be predicted are the corners of the mouth, the tip of the nose
and the center of the eyes. In addition to the facial landmarks, each dataset
further deﬁnes a number of related tasks. These related tasks also vary from
one dataset to another, and are typically gender recognition, smile recognition,
glasses recognition or face orientation.

On a more technical level, we deﬁne a learning framework in which we treat
each task as a classiﬁcation problem. While this is straightforward for gender,
smile and glasses recognition as they are already classiﬁcation tasks, it is a
bit more tricky for face orientation and FLD. For face orientation, instead of
predicting the roll, yaw and pitch real value as in a regression problem, we
divide each component into 30 degrees wide bins and predict the label of the bin
corresponding to the value. Similarly for FLD, rather than predicting the real
(x, y)-position of each landmark, we divide the image into 1 pixel wide bins and
predict the label of the bin corresponding to the value. Note that we still use
the original real values when comparing our prediction with the ground truth,
so that we incorporate our approximation errors in the ﬁnal score.

We report our results using the landmark failure rate metric [15], which is
deﬁned as follows: we ﬁrst compute the mean distance between the predicted
landmarks and the ground truth landmarks, then normalize it by the inter-
ocular distance from the center of the eyes. A normalized mean distance greater
than 10% is reported as a failure.

4.2 Facial Landmark Detection on the MTFL Task

As a ﬁrst experiment, we performed facial landmark detection on the Multi-Task
Facial Landmark (MTFL) task [15]. The dataset contains 12,995 face images an-
notated with ﬁve facial landmarks and four related attributes of gender, smiling,
wearing glasses and face proﬁle (ﬁve proﬁles in total). The training set has 10,000
images, while the test set has 2,995 images. We perform four sets of experiments
using an ImageNet pre-trained AlexNet, an ImageNet pre-trained ResNet18, an
un-pretrained AlexNet and an un-pretrained ResNet18 as underlying networks.
For AlexNet, we apply our collaborative block after each max pooling layer,
while for ResNet18, we do as shown in Fig. 2.

We compare our approach to several other approaches of the literature. We
include single-task learning (AN-S when using AlexNet as underlying network,
RN-S when using ResNet18), hard-parameter sharing MTL (AN and RN), hard-
parameter sharing MTL where the central section is widened to match the num-
ber of parameters of our approach (ANx and RNx), HyperFace (HF) [14], Tasks-
Constrained Deep Convolutional Network (TCDCN) [15], Cross-Stitch (XS) [12]
and XS widen to match the number of parameters of our approach (XSx). Ex-
cept for TCDCN, we train each network ourselves three times for 300 epochs and

8

Multi-Task Learning by Deep Collaboration

Fig. 3. Landmark failure rates (%) on the MTFL task. The reported values are the
average over the last ﬁve epochs, averaged over three tries. The left plot presents our
results with AlexNet as the underlying network, while the right one with ResNet18.
AN-S and RN-S stand for single-task training, AN and RN for multi-task training with
a single central network, ANx and RNx for multi-task training with a single central
network widen to match the number of parameters of our approach, HF for HyperFace,
TCDCN for [15]’s approach and XS for Cross-Stitch. In each instance, the left column
(blue) is for un-pretrained networks, while the right column (green) is for pre-trained
networks. Our proposed approach obtains the lowest failure rates overall.

report landmark failure rates averaged over the last ﬁve epochs, further averaged
over the three tries.

Fig. 3 presents our FLD results on the MTFL dataset. The left part of the
ﬁgure corresponds to using AlexNet as underlying network, while the right one
corresponds to ResNet18. The top part reports the landmark failure rates, while
the bottom part reports the mean error. In each plot, the left bar (blue) is for
un-pretrained network, while the right bar (green) is for ImageNet pre-trained
network. In addition, Fig. 4 shows example predictions from DCNet with pre-
trained ResNet18 as underlying network. The ﬁrst two examples were reported as
successes, while the last two are failures. The ground truth elements are colored
in green, while our predictions are colored in blue. We also include the labels of
the related tasks: gender, smiling, wearing glasses and face proﬁle.

The results of Fig. 3 show that our proposed approach obtained the lowest
failure rates and mean error in each case. Indeed, our DCNet with un-pretrained
and pre-trained AlexNet as underlying network obtained 19.67% and 19.96%
failure rates respectively, and 14.95% and 13.52% with ResNet18. This is sig-
niﬁcantly lower than the other approaches to which we compare ourselves. For
instance, with AlexNet, HF had 27.75% and 27.32%, XS had 26.41% and 25.65%,

Trottier et al.

9

MTFL

AFLW

Fig. 4. Example predictions of our DCNet with pre-trained ResNet18 as underlying
network on the MTFL and AFLW task. For MTFL, the ﬁrst two examples are successes,
while last two are failure cases. For AFLW, the ﬁrst three examples are successes, while
the last one is a failure case. Elements in green correspond to ground truth, while those
in blue correspond to predictions. Facial landmarks are shown as small dots, and related
tasks labels are displayed on the side. As we can see, over-exposition and tilted face
proﬁle can have a large impact on the prediction quality.

TCDCN had 25.00%1, and XSx had 25.23%. With ResNet18, XS had 18.43% and
15.52% respectively, and XSx had 17.28. We obtained the highest improvements
when using AlexNet as the underlying network when comparing to XS. With un-
pretrained and pre-trained AlexNet, we obtained improvements of 6.74% and
5.69%, while we obtained 3.48% and 2.00% with ResNet18. Performing MTL
with our approach can thus improve performance over using other approaches
of the literature.

Another result that we can see from Fig. 3 is that our soft-parameter shar-
ing approach obtains higher performance than the hard-parameter sharing ap-
proaches with matching number of parameters. For instance, increasing the
number of parameters of hard-parameter sharing AlexNet lowers it error rate
from 28.02% (AN) to 26.88% (ANx), but our approach lowers it further to
19.67%. Similarly, increasing the number of parameters of hard-parameter shar-
ing ResNet18 lowers it error rate from 20.05% (RN) to 16.75% (RNx), but our
approach lowers it further to 14.95%. These results are interesting because they
show that while increasing the number of parameters is an eﬀortless avenue to
improve performance, it has limitations. Developing novel approaches to enhance
network connectivity in a soft-parameter sharing setting seems more rewarding.
This may help to motivate new eﬀorts in this avenue to further leverage domain-
information of related tasks.

1 Zhang et. al only provided results with pre-trained AlexNet [15]

10

Multi-Task Learning by Deep Collaboration

Table 1. Landmark failure rate results on the AFLW dataset using a pre-trained
ResNet18 as underlying network. The presented values are averaged over the last ﬁve
epochs, further averaged over three tries. The ﬁrst column is the train / test ratio,
and the subsequent ones are the networks: single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS). Our approach obtains the best perfor-
mance in all cases, except the ﬁrst one where we observe over-ﬁtting.

Train / Test Ratio

0.1 / 0.9
0.3 / 0.7
0.5 / 0.5
0.7 / 0.3
0.9 / 0.1

RN-S

57.39
31.84
23.41
21.47
13.03

Networks

RN

58.00
32.00
23.31
21.92
12.80

XS

73.06
36.24
26.02
22.37
13.51

Ours

60.64
29.73
20.77
18.50
10.82

4.3 Eﬀect of Data Scarcity on the AFLW Task

As second experiment, we evaluated the inﬂuence of the number of training
examples to simulate data scarcity on the Annotated Facial Landmarks in the
Wild (AFLW) task [39]. The dataset has 21,123 Flickr images, and each image
can contain more than one face. Instead of using the images as provided, we
process them using the available face bounding boxes. We extract all faces with
visible landmarks, which gives a total of 2,111 images. This dataset deﬁnes 21
facial landmarks and has 3 related tasks (gender, wearing glasses and face ori-
entation). For face orientation, we divide the roll, yaw and pitch into 30 degrees
wide bins (14 bins in total), and predict the label corresponding to each real
value.

Our experiment works as follows. With a pre-trained ResNet18 as underlying
network, we compare our approach to single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS) by training on a varying number
of images. We use ﬁve diﬀerent train / test ratios, starting with 0.1 / 0.9 up
to 0.9 / 0.1 by 0.2 increment. In other words, we train each approaches on the
ﬁrst r% of the available images and test on the other (1 − r)%, then repeat
for all the other train / test ratios. We use the same training framework as in
section 4.2. We train each network three times for 300 epochs, and report the
landmark failure rate averaged over the last ﬁve epochs, further averaged over
the three tries. Example predictions are shown in Fig. 4.

As we can see in Table 1, our approach obtained the best performance in
all cases except the ﬁrst one. Indeed, we observe between 1.98% and 6.51%
improvements with train / test ratios from 0.3 / 0.7 to 0.9 / 0.1, while we obtain
a negative relative change of 3.25% with train / test ratio of 0.1 / 0.9. However,
since all multi-task approaches obtained higher failure rates than the single-task
approach, this suggests that the networks are over-ﬁtting the small training set.
Nonetheless, these results show that we can obtain better performance using our
approach.

Trottier et al.

11

Fig. 5. Landmark failure rate improvement (in %) of our approach compared to XS
when sampling random task weights. We used a pre-trained ResNet18 as underlying
network. The histogram at the left and the plot at the top right represents performance
improvement achieved by our proposed approach (positive value means lower failure
rates), while the plot at the bottom right corresponds to the log of the task weights. Our
approach outperformed XS in 86 out of the 100 tries, thus empirically demonstrating
that our learning framework was not unfavorable towards XS and that our approach
is less sensitive to the task weights λ.

One particularity that we observe in Table 1 is that the XS network has
relatively high failure rates. In the previous experiment of Section 4.2, XS had
either similar or better performance than the other approaches (except ours).
This could be due to our current multi-task learning framework that is unfavor-
able towards XS. In order to investigate whether this is the case, we perform
the following additional experiment. Using a pre-trained ResNet18 as underlying
network, we compare our approach to XS by training each network 100 times
using task weights randomly sampled from a log-uniform distribution. Speciﬁ-
cally, we ﬁrst sample from a uniform distribution γ ∼ U(log(1e−4), log(1)), then
use λ = exp(γ) as the weight. We trained both XS and our approach for 300
epochs with the same task weights using a train / test ratio of 0.5 / 0.5.

Figure 5 presents the results of this experiment. The plot at the top right
of the ﬁgure represents the landmark failure rate improvement (in %) of our
approach compared to XS, while the plot at the bottom right corresponds to the
log of the task weights for each try. In 86 out of the 100 tries, our approach had
a positive failure rate improvement, that is, obtained lower failure rates than
XS. As we can see in the histogram at the left of Fig. 5, the improvement rate
is normally distributed around 2.80%, has a median improvement of 3.66% and
a maximum improvement of 9.83%. Even though we sampled at random the
weights of the related tasks, our approach outperforms XS in the majority of the
cases. Our learning framework was therefore not unfavorable toward XS.

4.4 Illustration of Knowledge Sharing With an Ablation Study

As third experiment, we perform an ablation study on the MTFL task [15] with
an un-pretrained ResNet18 as underlying network. The goal of this experiment

12

Multi-Task Learning by Deep Collaboration

Fig. 6. Results of our ablation study on the MTFL dataset with an un-pretrained
ResNet18 as underlying network. We remove each task-speciﬁc features from each re-
spective central aggregation layer and evaluate the eﬀect on landmark failure rate. The
rows represent the task-speciﬁc CNNs, while the columns correspond to the network
block structure. Blocks with a high saturated color were found to have a large impact
on failure rate. In particular, this ablative study shows that the inﬂuence of high-
level face proﬁle features is large within our proposed architecture. This corroborates
with the well-known fact that the location of facial landmarks is closely dependent on
the orientation of the face. This constitutes an empirical evidence of domain-speciﬁc
information sharing via our approach.

is to verify that our collaborative block eﬀectively enables knowledge sharing
between task-speciﬁc CNNs. To do so, we evaluate the impact, on facial landmark
detection, of removing the contribution of each task-speciﬁc features. We zero out
the designated feature map xt before concatenation at the input of the central
aggregation H. The network is trained using the same framework as explained
in Sec. 4.1, and the ablation study is performed at test time on the test set when
training is done.

Figure 6 presents the results of our ablation study. The rows represent each
task-speciﬁc CNN, while the columns correspond to the network block structure.
The blocks are ordered from left (input) to right (output), while the task-speciﬁc
networks are ordered from top (main task) to bottom (related tasks). The color
saturation indicates the inﬂuence of removing the task-speciﬁc feature maps from
the central aggregation at the corresponding depth. A high saturation reﬂects
high inﬂuence on failure rate, while a low saturation reﬂects low inﬂuence.

As ﬁrst result, removing features from the facial landmark detection network
signiﬁcantly increases landmark failure rate. For instance, we observe a negative
(worse) relative change of 29.72% and 47.00% in failure rate by removing fea-
tures from Block 3 and Block 2 respectively. This illustrates that the main-task
network both contributed to and fed from the global features computed by the
central aggregation H. The CNN for landmark detection had the possibility to
remove the contribution of the global features, and so isolate itself from the other
CNNs, but the opposite occurred. We actually observe a mutual inﬂuence be-
tween the CNNs, where the task-speciﬁc features from the facial landmark CNN

Trottier et al.

13

inﬂuence the quality of the global features, which in turn inﬂuence the quality
of the subsequent task-speciﬁc features.

Another result that we can see from Fig. 6 is that Block 5 of task Proﬁle
has the highest inﬂuence on failure rate. We observe a negative relative change
of 83.87% by removing the features maps of task Proﬁle from the central ag-
gregation. What is particularly interesting in this case is that we observe this
high relative change at Block 5, which corresponds to the highest block in the
network. Since the block lies at the top of the network, it outputs features with
a high level of abstraction. We therefore expect that these features represent
high-level factors of variation corresponding to face orientation, which should
look like a rotation matrix. It therefore makes sense that features representing
the orientation of the face would be useful to predict facial landmarks, since
we know that the location of the facial landmarks is closely dependent on the
orientation of the face. The landmark CNN can use these rich features to bet-
ter rotate the predicted facial landmarks. This is indeed what we observe in
Fig. 6. These results constitute an empirical evidence that our approach allows
leveraging domain-speciﬁc information from related tasks.

4.5 Facial Landmark Detection With MTCNN

As ﬁnal experiment, we performed an experimental evaluation using the recent
Multi-Task Cascaded Convolutional Network (MTCNN) [27]. The authors pro-
posed a cascade structure of three stages, where each stage is composed of a
multi-task CNN. MTCNN performs predictions in a coarse-to-ﬁne manner. The
CNN of the ﬁrst stage generates (in a fully-convolutional way) many hypothe-
ses about the position of the face and the facial landmarks, and the subsequent
second and third stages reﬁnes them. The CNNs are trained sequentially with
hard-negative mining, in a hard-parameter sharing setting.

We implemented our approach in the available code project [40] and com-
pared ourself to MTCNN. We followed the provided hard negative mining recipe
and generated our images. For landmark detection, we used the LFWNet [26]
and CelebA [41] datasets, and generated 600k face images with facial landmarks.
For face detection, we used the WIDER [42] dataset, and generated 1.5M face
images with a bounding box. We trained a MTCNN with the stage networks
connected with our collaborative block, and a standard MTCNN with widened
stage networks to match the number of parameters.

On the test set of MTFL [15] dataset, standard MTCNN obtained a land-
mark failure rate of 37.85%, a mean error of 0.0996 and failed to detect a face
112 times, while our approach obtained better performances with 28.97%, 0.0930
and 79 respectively. Note that the reason MTCNN obtains worse performance
than our Deep Collaboration Network (DCNet), as reported in Fig. 3, is because
it has fewer parameters. DCNet has about 85M parameters, while the sum of
all three stage-CNN in MTCNN is about 2M. This is because MTCNN is care-
fully designed to balance computational speed and landmark detection precision.
It can predict many faces in high-dimensional images with a low computation
burden. An example of its prediction capability is shown in Fig. 7.

14

Multi-Task Learning by Deep Collaboration

Fig. 7. MTCNN predictions on the photo of the 2018 Oscar nominees (image resolution
of 2983 × 1197). The stage-CNNs are trained using our proposed collaborative block.
The coarse-to-ﬁne detection scheme employed by MTCNN allows predicting many faces
in high-dimensional images with low computational burden.

5 Conclusion and Future Work

In this paper, we proposed a novel soft-parameter knowledge sharing mechanism
based on lateral connections for Multi-Task Learning (MTL). Our proposed ap-
proach implements connectivity in term of a collaborative block, which uses two
distinct non-linear transformations. The ﬁrst one aggregates task-speciﬁc fea-
tures into global features, and the other merges back the global features into each
task-speciﬁc Convolutional Neural Network (CNN). Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
Our results on facial landmark detection tasks showed that networks connected
with our proposed collaborative block outperformed the other state-of-the-art
approaches, including the recent Cross-Stitch and MTCNN approach. We ver-
ify that our collaborative block eﬀectively enables knowledge sharing between
task-speciﬁc CNNs with an ablation study. We observed that the CNNs incor-
porated features with a varying level of abstraction from the other CNNs, by
observing the depth-speciﬁc inﬂuence of tasks that we know are related. These
results constituted an empirical evidence that our approach allows leveraging
domain-speciﬁc information from related tasks. Evaluating our proposed ap-
proach on other MTL problems could be an interesting avenue for future works.
For instance, the recurrent networks used to solve natural language processing
problems could beneﬁt from our approach.

Acknowledgements

We gratefully acknowledge the support of NVIDIA Corporation for providing a
Tesla Titan X for our experiments through their Hardware Grant Program.

Trottier et al.

15

References

1. Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classiﬁcation with deep convo-

lutional neural networks. In: NIPS. (2012) 1097–1105

2. Hochreiter, S.: The vanishing gradient problem during learning recurrent neural
nets and problem solutions. International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems 6(02) (1998) 107–116

A survey on multi-task learning.

arXiv preprint

3. Zhang, Y., Yang, Q.:
arXiv:1707.08114 (2017)

4. Caruana, R.: Multitask learning. In: Learning to learn. Springer (1998) 95–133
5. Zhang, M.L., Zhou, Z.H.: A review on multi-label learning algorithms. TKDE

6. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: KDD, ACM (2004)

26(8) (2014) 1819–1837

109–117

7. Pinto, L., Gupta, A.: Learning to push by grasping: Using multiple tasks for

eﬀective learning. In: ICRA, IEEE (2017) 2161–2168

8. Tian, Y., Luo, P., Wang, X., Tang, X.: Pedestrian detection aided by deep learning

semantic tasks. In: CVPR. (2015) 5079–5087

9. Liu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y.Y.: Representation learning
using multi-task deep neural networks for semantic classiﬁcation and information
retrieval. In: HLT-NAACL. (2015) 912–921

10. Yim, J., Jung, H., Yoo, B., Choi, C., Park, D., Kim, J.: Rotating your face using

multi-task deep neural network. In: CVPR. (2015) 676–684

11. Yin, X., Liu, X.: Multi-task convolutional neural network for face recognition.

arXiv preprint arXiv:1702.04710 (2017)

12. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-

task learning. In: CVPR. (2016) 3994–4003

13. Ruder, S.: An overview of multi-task learning in deep neural networks. CoRR

abs/1706.05098 (2017)

14. Ranjan, R., Patel, V.M., Chellappa, R.: Hyperface: A deep multi-task learning
framework for face detection, landmark localization, pose estimation, and gender
recognition. arXiv preprint arXiv:1603.01249 (2016)

15. Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi-

task learning. In: ECCV, Springer (2014) 94–108

16. Liu, P., Qiu, X., Huang, X.: Adversarial multi-task learning for text classiﬁcation.

(2017)

17. Baxter, J.: A bayesian/information theoretic model of learning to learn via multiple

task sampling. Machine learning 28(1) (1997) 7–39

18. Duong, L., Cohn, T., Bird, S., Cook, P.: Low resource dependency parsing: Cross-
lingual parameter sharing in a neural network parser. In: ACL. Volume 2. (2015)
845–850

19. Yang, Y., Hospedales, T.M.: Trace norm regularised deep multi-task learning.

arXiv preprint arXiv:1606.04038 (2016)

20. Ruder, S., Bingel, J., Augenstein, I., Søgaard, A.: Sluice networks: Learning what
to share between loosely related tasks. arXiv preprint arXiv:1705.08142 (2017)
21. Long, M., Wang, J.: Learning multiple tasks with deep relationship networks.

arXiv preprint arXiv:1506.02117 (2015)

22. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv
preprint arXiv:1606.04671 (2016)

16

Multi-Task Learning by Deep Collaboration

23. Trottier, L.: Deep collaboration network in pytorch.

https://github.com/

ltrottier/deep-collaboration-network

24. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 448–456

25. Nair, V., Hinton, G.: Rectiﬁed linear units improve restricted boltzmann machines.

In: ICML. (2010) 807–814

26. Sun, Y., Wang, X., Tang, X.: Deep convolutional network cascade for facial point

detection. In: CVPR. (2013) 3476–3483

27. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using
multitask cascaded convolutional networks. IEEE Signal Processing Letters 23(10)
(2016) 1499–1503

28. Jourabloo, A., Liu, X.: Large-pose face alignment via cnn-based dense 3d model

ﬁtting. In: CVPR. (2016) 4188–4196

29. Baltrušaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial

behavior analysis toolkit. In: WACV, IEEE (2016) 1–10

30. Ding, C., Tao, D.: Robust face recognition via multimodal deep face representation.

IEEE Transactions on Multimedia 17(11) (2015) 2049–2058

31. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to

human-level performance in face veriﬁcation. In: CVPR. (2014) 1701–1708

32. Zhang, C., Zhang, Z.: Improving multiview face detection with multi-task deep

convolutional neural networks. In: WACV, IEEE (2014) 1036–1041

33. Fabian Benitez-Quiroz, C., Srinivasan, R., Martinez, A.M.: Emotionet: An accu-
rate, real-time algorithm for the automatic annotation of a million facial expres-
sions in the wild. In: CVPR. (2016) 5562–5570

34. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

35. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: ECCV, Springer (2016) 630–645

In: CVPR. (2016) 770–778

36. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with

stochastic depth. In: ECCV, Springer (2016) 646–661

37. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations

for deep neural networks. In: CVPR, IEEE (2017) 5987–5995

38. Veit, A., Wilber, M.J., Belongie, S.: Residual networks behave like ensembles of

relatively shallow networks. In: NIPS. (2016) 550–558

39. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization. In:
ICCV Workshops, IEEE (2011) 2144–2151

40. Kim, K.K.: Deep learning face detection and recognition, implemented by pytorch.

https://github.com/kuaikuaikim/DFace

41. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:

42. Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark.

ICCV. (2015)

In: CVPR. (2016) 5525–5533

8
1
0
2
 
r
a

M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
1
1
0
0
.
1
1
7
1
:
v
i
X
r
a

Multi-Task Learning by Deep Collaboration and
Application in Facial Landmark Detection

Ludovic Trottier

Philippe Giguère

Brahim Chaib-draa

ludovic.trottier.1@ulaval.ca
{philippe.giguere,brahim.chaib-draa}@ift.ulaval.ca

Laval University, Québec, Canada

Abstract. Convolutional neural networks (CNNs) have become the most
successful approach in many vision-related domains. However, they are
limited to domains where data is abundant. Recent works have looked
at multi-task learning (MTL) to mitigate data scarcity by leveraging
domain-speciﬁc information from related tasks. In this paper, we present
a novel soft-parameter sharing mechanism for CNNs in a MTL setting,
which we refer to as Deep Collaboration. We propose taking into ac-
count the notion that task relevance depends on depth by using lateral
transformation blocs with skip connections. This allows extracting task-
speciﬁc features at various depth without sacriﬁcing features relevant to
all tasks. We show that CNNs connected with our Deep Collaboration
obtain better accuracy on facial landmark detection with related tasks.
We ﬁnally verify that our approach eﬀectively allows knowledge sharing
by showing depth-speciﬁc inﬂuence of tasks that we know are related.

1 Introduction

Over the past few years, Convolutional Neural Networks (CNNs) have become
the leading approach in many vision-related tasks [1]. Their ability to learn
a hierarchy of increasingly abstract concepts allows them to transform com-
plex high-dimensional input images into simple low-dimensional output features.
CNNs have been used in many settings, but their need to have a large amount
of data during training has restricted them to domains where data is abundant.
Optimizing CNNs is tricky not only because of problems like vanishing / ex-
ploding gradients [2], but also because they typically have many parameters to
be learned. While previous works have looked at supervised and unsupervised
pre-training to improve generalization, others have considered casting their orig-
inal single-task problem into a new Multi-Task Learning (MTL) problem [3]. As
Caruana (1998) [4] explained in his seminal work: “MTL improves generalization
by leveraging the domain-speciﬁc information contained in the training signals
of related tasks". Exploring new ways to more eﬃciently gather information from
related tasks would help to further improve generalization on the main one.

MTL has proven its value in several domains over the years. It has become a
dominant ﬁeld of machine learning [5], with many inﬂuential works [6]. Although
MTL dates back several years, recent major advances in Deep Learning (DL)

2

Multi-Task Learning by Deep Collaboration

opened up opportunities for novel contributions. Works on grasping [7], pedes-
trian detection [8], natural language processing [9], face recognition [10][11] and
object detection [12] helped MTL make a resurgence in the DL community. They
have shown the potential of MTL to mitigate data scarcity when training deep
networks, which has inﬂuenced its growing popularity

MTL approaches can generally be divided into two major categories: hard
and soft parameter sharing [13]. Hard-parameter sharing dates back to the orig-
inal work of Caruana (1998) and is the most common of the two. Approaches
in this category have a shared central section with many heads (one per task).
Features from speciﬁc tasks compete together and those relevant to all tasks are
favored. Recent works in DL have shown that hard-parameter sharing can be
successful [14][15][7][11]. However, a too large emphasis on features relevant to all
tasks can be harmful for learning high-level features speciﬁc to a particular task.
These types of speciﬁc features are usually needed to obtain a good representa-
tion for the particular task. Also, shared layers are prone to be contaminated by
noise coming from noxious tasks [16]. These limitations can be detrimental even
though hard-parameter sharing reduces the risk of over-ﬁtting [17].

Soft-parameter sharing has been proposed as an alternative to alleviate these
drawbacks. Approaches in this category substitute the shared central section by
separate task-speciﬁc CNNs, but provide a knowledge sharing mechanism to
connect them. Each CNN can then learn task-speciﬁc features and share their
knowledge without interfering with others. Recent works in this category have
looked at regularizing the distance between task-speciﬁc parameters with a ‘2
norm [18] or a trace norm [19], training shared and private LSTM submod-
ules [16], partitioning the hidden layers into subspaces [20] and regularizing the
FC layers with tensor normal priors [21]. In the domain of continual learning,
progressive network [22] has also shown promising results for sequential transfer
learning, by employing lateral connections to previously learned networks.

In this paper, we present a novel soft-parameter knowledge sharing mecha-
nism for connecting task-speciﬁc CNNs in a MTL framework. We refer to our
approach as Deep Collaboration. We deﬁne connectivity in terms of a collabo-
rative block that uses two non-linear transformations with lateral connections.
One aggregates task-speciﬁc features into global features, and the other merges
back the global features into each task-speciﬁc CNN. Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
We evaluated our approach on the problem of facial landmark detection in a
MTL framework and obtained better results in comparison to other approaches
of the literature. We further assess the objectivity of our training framework by
randomly varying the contribution of each related tasks. Finally, we verify that
our collaborative block enables knowledge sharing with an ablation study that
shows the depth-speciﬁc inﬂuence of tasks that we know are related.

The content of our paper is organized as follows. In Section 2, we present
related works in MTL and facial landmark detection. We elaborate on our ap-
proach in Section 3, and present experimental results in Section 4. We ﬁnally
conclude our paper in Section 5. Our code is available here: [23].

Trottier et al.

3

2 Related Work

2.1 Multi-Task Learning

Our proposed Deep Collaboration knowledge sharing mechanism is related to
other existing approaches. One is Cross-Stitch (XS) [12], which connects task-
speciﬁc CNNs by linearly combining their feature maps at certain depths. One
drawback of XS is that it is limited to capture only linear dependencies between
each CNN. In contrast to XS, our approach uses non-linear transformations in
order to capture more complex dependencies.

Another related approach is Tasks-Constrained Deep Convolutional Network
(TCDCN) [15]. The authors proposed an early-stopping criterion to remove aux-
iliary tasks that start to overﬁt before becoming detrimental to the main task.
This approach has however several hyper-parameters to be selected manually.
For each task, it has an hyper-parameter controlling the period length of the
local window and a threshold that stops the task when the criterion exceeds it.
Unlike TCDCN, our approach has no hyper-parameters that need to be tuned to
the tasks at hand. Our collaborative block consists of a series of Batch Normal-
ization [24], ReLU [25], and convolutional layers shaped in a standard setting
that is commonly found in nowadays works.

Our proposed approach is also related to HyperFace [14]. The authors pro-
posed to fuse the layers at various depth and exploit features of diﬀerent levels
of complexity. Their goal was to allow low-level features with better localization
properties to help tasks such as landmark localization and pose detection, and al-
low high-level features with better class-speciﬁc properties to help tasks like face
detection and gender recognition. Although HyperFace is in the hard-parameter
sharing category and is not entirely related to our approach, the idea of feature
fusion is also central in our work. Instead of fusing the features at intermediate
layers of a single CNN, our approach aggregates same-level features of multiple
CNNs, at diﬀerent depth independently.

2.2 Facial Landmark Detection

Facial Landmark Detection (FLD) is an essential component in many face-
related tasks [26][27][28][29]. FLD can be described as follows: given the image of
a face of a person, the goal is to predict the (x, y)-position of speciﬁc landmarks
associated with key features of the visage. Applications such as face recogni-
tion [30], face validation [31], facial feature detection and tacking [32] rely on
the ability to correctly ﬁnd the location of these distinct facial landmarks in order
to succeed. Localizing facial key points like the center of the eyes, the corners of
the mouth, the tip of the nose and the earlobes is however a challenging problem
when many lighting conditions, head poses, facial expressions and occlusions in-
crease diversity of the face images. In addition to integrating this variability into
the estimation process, a FLD model must also take into account a number of
correlated factors. For instance, although both an angry person and a sad person
have frowned eyebrows, an angry person will have pinched lips while a sad person

4

Multi-Task Learning by Deep Collaboration

will have sunken mouth corners [33]. A particularity of datasets geared towards
FLD is that they are particularly well-suited for MTL. In addition to contain-
ing the position of the facial landmarks, these datasets also contain a number
of other labels that can be used to deﬁned auxiliary tasks. Gender recognition,
smile recognition, glasses recognition or face orientation are examples of tasks
often chosen to evaluate MTL approaches.

3 Deep Collaboration

Given T task-speciﬁc Convolutional Neural Networks (CNNs), our goal is to con-
nect them with lateral connections in order to allow domain-speciﬁc information
sharing. We deﬁne connectivity in terms of a collaborative block containing two
distinct non-linear transformations. One aggregates task-speciﬁc features into
global features, and the other merges back the global features into each task-
speciﬁc CNN. Our collaborative block is diﬀerentiable and can be dropped in
any existing CNN architectures as a whole. For this reason, we make no assump-
tion about the structure of the task-speciﬁc CNNs. Our approach can even work
with diﬀerent CNNs, but for the sake of simplicity, we suppose that the CNNs
are the same. We refer to it as the underlying network.

We also decompose the underlying network as a series of blocks. Each block
can be as small as a single layer, as large as the whole network itself, or based
on simple rules, such as grouping all layers with matching spatial dimensions or
grouping every n subsequent layers. The arrangement of the layers into blocks
does not change the composition of the underlying network. We only use it to
make explicit the depth at which we connect the task-speciﬁc CNNs.

Since our collaborative block can be inserted at any depth, we also drop the
depth index on the feature maps to further simplify the equations. As such,
we deﬁne the feature map output of a block at a certain depth as xt, where
t ∈ {1 . . . T } is the task index. Our approach takes as input all task-speciﬁc
feature maps xt and processes them into new feature maps yt as follows:

z = H([x1, . . . , xT ]) ,

yt = ReLU (xt + Ft([xt, z])) ,

(1)

where H and Ft represent the central and the task-speciﬁc aggregations re-
spectively, and [·] denotes depth-wise concatenation. We refer to Eq. (1) as our
collaborative block. The goal of H is to combine all task-speciﬁc feature maps xt
into a global feature map z representing uniﬁed knowledge, while the goal of F
is to merge back the global feature map z with each task-speciﬁc input xt. The
compositional structure of H and F is as follows:

H(·) = (ReLU ◦ BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,
F(·) = (BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,

(2)

(3)

where BN stands for Batch Normalization [24], Conv(h×w) for a standard convo-
lutional layer with ﬁlters of size (h × w), and ◦ is the usual function composition.
The ﬁrst Conv(1×1) layer in H divides the number of feature maps by a factor

Trottier et al.

5

Fig. 1. Example of our collaborative block applied on the feature maps of two task-
speciﬁc networks. The input feature maps (shown in 1(cid:13)) are ﬁrst concatenated depth-
wise and transformed into a global feature map ( 2(cid:13)). The global feature map is then
concatenate with each input feature map individually and transformed into task-speciﬁc
feature maps ( 3(cid:13)). Each resulting feature map is then added back to the input feature
map using a skip connection ( 4(cid:13)), which gives the ﬁnal outputs of the block ( 5(cid:13)).

of 4, while the ﬁrst Conv(1×1) layer in F divides it to match the size of xt. An
illustration of our collaborative block is shown in Fig. 1.

One particularity of our approach is that we use a skip connection in mapping
F. Recent works [34][35][36][37][38] have shown that networks with identity skip
connections are more easily able to learn proper input-output mappings. Inspired
by these works, we opted for an identity skip connection in F in order to more
easily learn the proper mapping to integrate domain-speciﬁc information from
the other tasks. In particular, identity skip connections put an incentive on
learning the identity mapping. We can see this by the ease at which the network
can obtain the identity mapping by simply pushing all the weights in F towards
zero. In our MTL context, the identity mapping can be seen as a way to remove
the inﬂuence of the global features z. This allows to take into account the cases
where integrating z back to the task-speciﬁc features xt would not help.

Another motivation for using an identity skip connection around the global
feature map z comes from the fact that depth inﬂuences the relevance of each
task towards another. Some task-speciﬁc CNNs can beneﬁt more when they
share their low-level features than their high-level features, while other beneﬁt
more in the other way. For instance, tasks such as landmark localization and
pose detection proﬁt more from low-level features containing better localization

6

Multi-Task Learning by Deep Collaboration

Fig. 2. Deep Collaboration Network (DCNet) using ResNet18 as underlying network
in a MTL setting on the MTFL dataset. The top part shows the block structure of
ResNet18 interleaved with our proposed collaborative block, while the bottom part
details each residual and task-speciﬁc FC blocks.

properties, while tasks such as face detection and gender recognition proﬁt more
from class-speciﬁc high-level features. Considering that CNNs learn a hierarchy
of increasingly abstract features, our collaborative block can take into account
task relevance by deactivating a diﬀerent set of residual mappings Ft based on
the depth at which it is inserted. An example of such specialization will be shown
in our ablative study in Section 4.4.

Fig. 2 presents an example of inserting our collaborative block at diﬀerent
depths in a MTL framework on the MTFL dataset [15]. In this particular case,
we opted for a ResNet18 as underlying network. We refer to this network as our
Deep Collaboration Network (DCNet). As we can see in the top part of the ﬁgure,
integrating our approach comes down to interleaving the underlying network
block structure with our collaborative block. Each collaborative block receives
as input the output of each task-speciﬁc block, processes them as detailed in
Eq. (1), and sends the result back to each task-speciﬁc network. Adding our
approach to any underlying network can be done by simply following the same
pattern of interleaving the network block structure with our collaborative block.

4 Experiments

In this section, we detail our Multi-Task Learning (MTL) training framework and
present our experiments in Facial Landmark Detection (FLD) tasks. We further
evaluate the eﬀect of data scarcity on performance and illustrate an example of
knowledge sharing between task-speciﬁc CNNs with an ablation study.

Trottier et al.

7

4.1 Multi-Task Learning Training Framework

The goal of Facial Landmark Detection (FLD) is to predict the (x, y)-position
of speciﬁc landmarks associated with key features of the visage. While the num-
ber and type of landmarks are speciﬁc to each dataset, examples of standard
landmarks to be predicted are the corners of the mouth, the tip of the nose
and the center of the eyes. In addition to the facial landmarks, each dataset
further deﬁnes a number of related tasks. These related tasks also vary from
one dataset to another, and are typically gender recognition, smile recognition,
glasses recognition or face orientation.

On a more technical level, we deﬁne a learning framework in which we treat
each task as a classiﬁcation problem. While this is straightforward for gender,
smile and glasses recognition as they are already classiﬁcation tasks, it is a
bit more tricky for face orientation and FLD. For face orientation, instead of
predicting the roll, yaw and pitch real value as in a regression problem, we
divide each component into 30 degrees wide bins and predict the label of the bin
corresponding to the value. Similarly for FLD, rather than predicting the real
(x, y)-position of each landmark, we divide the image into 1 pixel wide bins and
predict the label of the bin corresponding to the value. Note that we still use
the original real values when comparing our prediction with the ground truth,
so that we incorporate our approximation errors in the ﬁnal score.

We report our results using the landmark failure rate metric [15], which is
deﬁned as follows: we ﬁrst compute the mean distance between the predicted
landmarks and the ground truth landmarks, then normalize it by the inter-
ocular distance from the center of the eyes. A normalized mean distance greater
than 10% is reported as a failure.

4.2 Facial Landmark Detection on the MTFL Task

As a ﬁrst experiment, we performed facial landmark detection on the Multi-Task
Facial Landmark (MTFL) task [15]. The dataset contains 12,995 face images an-
notated with ﬁve facial landmarks and four related attributes of gender, smiling,
wearing glasses and face proﬁle (ﬁve proﬁles in total). The training set has 10,000
images, while the test set has 2,995 images. We perform four sets of experiments
using an ImageNet pre-trained AlexNet, an ImageNet pre-trained ResNet18, an
un-pretrained AlexNet and an un-pretrained ResNet18 as underlying networks.
For AlexNet, we apply our collaborative block after each max pooling layer,
while for ResNet18, we do as shown in Fig. 2.

We compare our approach to several other approaches of the literature. We
include single-task learning (AN-S when using AlexNet as underlying network,
RN-S when using ResNet18), hard-parameter sharing MTL (AN and RN), hard-
parameter sharing MTL where the central section is widened to match the num-
ber of parameters of our approach (ANx and RNx), HyperFace (HF) [14], Tasks-
Constrained Deep Convolutional Network (TCDCN) [15], Cross-Stitch (XS) [12]
and XS widen to match the number of parameters of our approach (XSx). Ex-
cept for TCDCN, we train each network ourselves three times for 300 epochs and

8

Multi-Task Learning by Deep Collaboration

Fig. 3. Landmark failure rates (%) on the MTFL task. The reported values are the
average over the last ﬁve epochs, averaged over three tries. The left plot presents our
results with AlexNet as the underlying network, while the right one with ResNet18.
AN-S and RN-S stand for single-task training, AN and RN for multi-task training with
a single central network, ANx and RNx for multi-task training with a single central
network widen to match the number of parameters of our approach, HF for HyperFace,
TCDCN for [15]’s approach and XS for Cross-Stitch. In each instance, the left column
(blue) is for un-pretrained networks, while the right column (green) is for pre-trained
networks. Our proposed approach obtains the lowest failure rates overall.

report landmark failure rates averaged over the last ﬁve epochs, further averaged
over the three tries.

Fig. 3 presents our FLD results on the MTFL dataset. The left part of the
ﬁgure corresponds to using AlexNet as underlying network, while the right one
corresponds to ResNet18. The top part reports the landmark failure rates, while
the bottom part reports the mean error. In each plot, the left bar (blue) is for
un-pretrained network, while the right bar (green) is for ImageNet pre-trained
network. In addition, Fig. 4 shows example predictions from DCNet with pre-
trained ResNet18 as underlying network. The ﬁrst two examples were reported as
successes, while the last two are failures. The ground truth elements are colored
in green, while our predictions are colored in blue. We also include the labels of
the related tasks: gender, smiling, wearing glasses and face proﬁle.

The results of Fig. 3 show that our proposed approach obtained the lowest
failure rates and mean error in each case. Indeed, our DCNet with un-pretrained
and pre-trained AlexNet as underlying network obtained 19.67% and 19.96%
failure rates respectively, and 14.95% and 13.52% with ResNet18. This is sig-
niﬁcantly lower than the other approaches to which we compare ourselves. For
instance, with AlexNet, HF had 27.75% and 27.32%, XS had 26.41% and 25.65%,

Trottier et al.

9

MTFL

AFLW

Fig. 4. Example predictions of our DCNet with pre-trained ResNet18 as underlying
network on the MTFL and AFLW task. For MTFL, the ﬁrst two examples are successes,
while last two are failure cases. For AFLW, the ﬁrst three examples are successes, while
the last one is a failure case. Elements in green correspond to ground truth, while those
in blue correspond to predictions. Facial landmarks are shown as small dots, and related
tasks labels are displayed on the side. As we can see, over-exposition and tilted face
proﬁle can have a large impact on the prediction quality.

TCDCN had 25.00%1, and XSx had 25.23%. With ResNet18, XS had 18.43% and
15.52% respectively, and XSx had 17.28. We obtained the highest improvements
when using AlexNet as the underlying network when comparing to XS. With un-
pretrained and pre-trained AlexNet, we obtained improvements of 6.74% and
5.69%, while we obtained 3.48% and 2.00% with ResNet18. Performing MTL
with our approach can thus improve performance over using other approaches
of the literature.

Another result that we can see from Fig. 3 is that our soft-parameter shar-
ing approach obtains higher performance than the hard-parameter sharing ap-
proaches with matching number of parameters. For instance, increasing the
number of parameters of hard-parameter sharing AlexNet lowers it error rate
from 28.02% (AN) to 26.88% (ANx), but our approach lowers it further to
19.67%. Similarly, increasing the number of parameters of hard-parameter shar-
ing ResNet18 lowers it error rate from 20.05% (RN) to 16.75% (RNx), but our
approach lowers it further to 14.95%. These results are interesting because they
show that while increasing the number of parameters is an eﬀortless avenue to
improve performance, it has limitations. Developing novel approaches to enhance
network connectivity in a soft-parameter sharing setting seems more rewarding.
This may help to motivate new eﬀorts in this avenue to further leverage domain-
information of related tasks.

1 Zhang et. al only provided results with pre-trained AlexNet [15]

10

Multi-Task Learning by Deep Collaboration

Table 1. Landmark failure rate results on the AFLW dataset using a pre-trained
ResNet18 as underlying network. The presented values are averaged over the last ﬁve
epochs, further averaged over three tries. The ﬁrst column is the train / test ratio,
and the subsequent ones are the networks: single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS). Our approach obtains the best perfor-
mance in all cases, except the ﬁrst one where we observe over-ﬁtting.

Train / Test Ratio

0.1 / 0.9
0.3 / 0.7
0.5 / 0.5
0.7 / 0.3
0.9 / 0.1

RN-S

57.39
31.84
23.41
21.47
13.03

Networks

RN

58.00
32.00
23.31
21.92
12.80

XS

73.06
36.24
26.02
22.37
13.51

Ours

60.64
29.73
20.77
18.50
10.82

4.3 Eﬀect of Data Scarcity on the AFLW Task

As second experiment, we evaluated the inﬂuence of the number of training
examples to simulate data scarcity on the Annotated Facial Landmarks in the
Wild (AFLW) task [39]. The dataset has 21,123 Flickr images, and each image
can contain more than one face. Instead of using the images as provided, we
process them using the available face bounding boxes. We extract all faces with
visible landmarks, which gives a total of 2,111 images. This dataset deﬁnes 21
facial landmarks and has 3 related tasks (gender, wearing glasses and face ori-
entation). For face orientation, we divide the roll, yaw and pitch into 30 degrees
wide bins (14 bins in total), and predict the label corresponding to each real
value.

Our experiment works as follows. With a pre-trained ResNet18 as underlying
network, we compare our approach to single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS) by training on a varying number
of images. We use ﬁve diﬀerent train / test ratios, starting with 0.1 / 0.9 up
to 0.9 / 0.1 by 0.2 increment. In other words, we train each approaches on the
ﬁrst r% of the available images and test on the other (1 − r)%, then repeat
for all the other train / test ratios. We use the same training framework as in
section 4.2. We train each network three times for 300 epochs, and report the
landmark failure rate averaged over the last ﬁve epochs, further averaged over
the three tries. Example predictions are shown in Fig. 4.

As we can see in Table 1, our approach obtained the best performance in
all cases except the ﬁrst one. Indeed, we observe between 1.98% and 6.51%
improvements with train / test ratios from 0.3 / 0.7 to 0.9 / 0.1, while we obtain
a negative relative change of 3.25% with train / test ratio of 0.1 / 0.9. However,
since all multi-task approaches obtained higher failure rates than the single-task
approach, this suggests that the networks are over-ﬁtting the small training set.
Nonetheless, these results show that we can obtain better performance using our
approach.

Trottier et al.

11

Fig. 5. Landmark failure rate improvement (in %) of our approach compared to XS
when sampling random task weights. We used a pre-trained ResNet18 as underlying
network. The histogram at the left and the plot at the top right represents performance
improvement achieved by our proposed approach (positive value means lower failure
rates), while the plot at the bottom right corresponds to the log of the task weights. Our
approach outperformed XS in 86 out of the 100 tries, thus empirically demonstrating
that our learning framework was not unfavorable towards XS and that our approach
is less sensitive to the task weights λ.

One particularity that we observe in Table 1 is that the XS network has
relatively high failure rates. In the previous experiment of Section 4.2, XS had
either similar or better performance than the other approaches (except ours).
This could be due to our current multi-task learning framework that is unfavor-
able towards XS. In order to investigate whether this is the case, we perform
the following additional experiment. Using a pre-trained ResNet18 as underlying
network, we compare our approach to XS by training each network 100 times
using task weights randomly sampled from a log-uniform distribution. Speciﬁ-
cally, we ﬁrst sample from a uniform distribution γ ∼ U(log(1e−4), log(1)), then
use λ = exp(γ) as the weight. We trained both XS and our approach for 300
epochs with the same task weights using a train / test ratio of 0.5 / 0.5.

Figure 5 presents the results of this experiment. The plot at the top right
of the ﬁgure represents the landmark failure rate improvement (in %) of our
approach compared to XS, while the plot at the bottom right corresponds to the
log of the task weights for each try. In 86 out of the 100 tries, our approach had
a positive failure rate improvement, that is, obtained lower failure rates than
XS. As we can see in the histogram at the left of Fig. 5, the improvement rate
is normally distributed around 2.80%, has a median improvement of 3.66% and
a maximum improvement of 9.83%. Even though we sampled at random the
weights of the related tasks, our approach outperforms XS in the majority of the
cases. Our learning framework was therefore not unfavorable toward XS.

4.4 Illustration of Knowledge Sharing With an Ablation Study

As third experiment, we perform an ablation study on the MTFL task [15] with
an un-pretrained ResNet18 as underlying network. The goal of this experiment

12

Multi-Task Learning by Deep Collaboration

Fig. 6. Results of our ablation study on the MTFL dataset with an un-pretrained
ResNet18 as underlying network. We remove each task-speciﬁc features from each re-
spective central aggregation layer and evaluate the eﬀect on landmark failure rate. The
rows represent the task-speciﬁc CNNs, while the columns correspond to the network
block structure. Blocks with a high saturated color were found to have a large impact
on failure rate. In particular, this ablative study shows that the inﬂuence of high-
level face proﬁle features is large within our proposed architecture. This corroborates
with the well-known fact that the location of facial landmarks is closely dependent on
the orientation of the face. This constitutes an empirical evidence of domain-speciﬁc
information sharing via our approach.

is to verify that our collaborative block eﬀectively enables knowledge sharing
between task-speciﬁc CNNs. To do so, we evaluate the impact, on facial landmark
detection, of removing the contribution of each task-speciﬁc features. We zero out
the designated feature map xt before concatenation at the input of the central
aggregation H. The network is trained using the same framework as explained
in Sec. 4.1, and the ablation study is performed at test time on the test set when
training is done.

Figure 6 presents the results of our ablation study. The rows represent each
task-speciﬁc CNN, while the columns correspond to the network block structure.
The blocks are ordered from left (input) to right (output), while the task-speciﬁc
networks are ordered from top (main task) to bottom (related tasks). The color
saturation indicates the inﬂuence of removing the task-speciﬁc feature maps from
the central aggregation at the corresponding depth. A high saturation reﬂects
high inﬂuence on failure rate, while a low saturation reﬂects low inﬂuence.

As ﬁrst result, removing features from the facial landmark detection network
signiﬁcantly increases landmark failure rate. For instance, we observe a negative
(worse) relative change of 29.72% and 47.00% in failure rate by removing fea-
tures from Block 3 and Block 2 respectively. This illustrates that the main-task
network both contributed to and fed from the global features computed by the
central aggregation H. The CNN for landmark detection had the possibility to
remove the contribution of the global features, and so isolate itself from the other
CNNs, but the opposite occurred. We actually observe a mutual inﬂuence be-
tween the CNNs, where the task-speciﬁc features from the facial landmark CNN

Trottier et al.

13

inﬂuence the quality of the global features, which in turn inﬂuence the quality
of the subsequent task-speciﬁc features.

Another result that we can see from Fig. 6 is that Block 5 of task Proﬁle
has the highest inﬂuence on failure rate. We observe a negative relative change
of 83.87% by removing the features maps of task Proﬁle from the central ag-
gregation. What is particularly interesting in this case is that we observe this
high relative change at Block 5, which corresponds to the highest block in the
network. Since the block lies at the top of the network, it outputs features with
a high level of abstraction. We therefore expect that these features represent
high-level factors of variation corresponding to face orientation, which should
look like a rotation matrix. It therefore makes sense that features representing
the orientation of the face would be useful to predict facial landmarks, since
we know that the location of the facial landmarks is closely dependent on the
orientation of the face. The landmark CNN can use these rich features to bet-
ter rotate the predicted facial landmarks. This is indeed what we observe in
Fig. 6. These results constitute an empirical evidence that our approach allows
leveraging domain-speciﬁc information from related tasks.

4.5 Facial Landmark Detection With MTCNN

As ﬁnal experiment, we performed an experimental evaluation using the recent
Multi-Task Cascaded Convolutional Network (MTCNN) [27]. The authors pro-
posed a cascade structure of three stages, where each stage is composed of a
multi-task CNN. MTCNN performs predictions in a coarse-to-ﬁne manner. The
CNN of the ﬁrst stage generates (in a fully-convolutional way) many hypothe-
ses about the position of the face and the facial landmarks, and the subsequent
second and third stages reﬁnes them. The CNNs are trained sequentially with
hard-negative mining, in a hard-parameter sharing setting.

We implemented our approach in the available code project [40] and com-
pared ourself to MTCNN. We followed the provided hard negative mining recipe
and generated our images. For landmark detection, we used the LFWNet [26]
and CelebA [41] datasets, and generated 600k face images with facial landmarks.
For face detection, we used the WIDER [42] dataset, and generated 1.5M face
images with a bounding box. We trained a MTCNN with the stage networks
connected with our collaborative block, and a standard MTCNN with widened
stage networks to match the number of parameters.

On the test set of MTFL [15] dataset, standard MTCNN obtained a land-
mark failure rate of 37.85%, a mean error of 0.0996 and failed to detect a face
112 times, while our approach obtained better performances with 28.97%, 0.0930
and 79 respectively. Note that the reason MTCNN obtains worse performance
than our Deep Collaboration Network (DCNet), as reported in Fig. 3, is because
it has fewer parameters. DCNet has about 85M parameters, while the sum of
all three stage-CNN in MTCNN is about 2M. This is because MTCNN is care-
fully designed to balance computational speed and landmark detection precision.
It can predict many faces in high-dimensional images with a low computation
burden. An example of its prediction capability is shown in Fig. 7.

14

Multi-Task Learning by Deep Collaboration

Fig. 7. MTCNN predictions on the photo of the 2018 Oscar nominees (image resolution
of 2983 × 1197). The stage-CNNs are trained using our proposed collaborative block.
The coarse-to-ﬁne detection scheme employed by MTCNN allows predicting many faces
in high-dimensional images with low computational burden.

5 Conclusion and Future Work

In this paper, we proposed a novel soft-parameter knowledge sharing mechanism
based on lateral connections for Multi-Task Learning (MTL). Our proposed ap-
proach implements connectivity in term of a collaborative block, which uses two
distinct non-linear transformations. The ﬁrst one aggregates task-speciﬁc fea-
tures into global features, and the other merges back the global features into each
task-speciﬁc Convolutional Neural Network (CNN). Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
Our results on facial landmark detection tasks showed that networks connected
with our proposed collaborative block outperformed the other state-of-the-art
approaches, including the recent Cross-Stitch and MTCNN approach. We ver-
ify that our collaborative block eﬀectively enables knowledge sharing between
task-speciﬁc CNNs with an ablation study. We observed that the CNNs incor-
porated features with a varying level of abstraction from the other CNNs, by
observing the depth-speciﬁc inﬂuence of tasks that we know are related. These
results constituted an empirical evidence that our approach allows leveraging
domain-speciﬁc information from related tasks. Evaluating our proposed ap-
proach on other MTL problems could be an interesting avenue for future works.
For instance, the recurrent networks used to solve natural language processing
problems could beneﬁt from our approach.

Acknowledgements

We gratefully acknowledge the support of NVIDIA Corporation for providing a
Tesla Titan X for our experiments through their Hardware Grant Program.

Trottier et al.

15

References

1. Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classiﬁcation with deep convo-

lutional neural networks. In: NIPS. (2012) 1097–1105

2. Hochreiter, S.: The vanishing gradient problem during learning recurrent neural
nets and problem solutions. International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems 6(02) (1998) 107–116

A survey on multi-task learning.

arXiv preprint

3. Zhang, Y., Yang, Q.:
arXiv:1707.08114 (2017)

4. Caruana, R.: Multitask learning. In: Learning to learn. Springer (1998) 95–133
5. Zhang, M.L., Zhou, Z.H.: A review on multi-label learning algorithms. TKDE

6. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: KDD, ACM (2004)

26(8) (2014) 1819–1837

109–117

7. Pinto, L., Gupta, A.: Learning to push by grasping: Using multiple tasks for

eﬀective learning. In: ICRA, IEEE (2017) 2161–2168

8. Tian, Y., Luo, P., Wang, X., Tang, X.: Pedestrian detection aided by deep learning

semantic tasks. In: CVPR. (2015) 5079–5087

9. Liu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y.Y.: Representation learning
using multi-task deep neural networks for semantic classiﬁcation and information
retrieval. In: HLT-NAACL. (2015) 912–921

10. Yim, J., Jung, H., Yoo, B., Choi, C., Park, D., Kim, J.: Rotating your face using

multi-task deep neural network. In: CVPR. (2015) 676–684

11. Yin, X., Liu, X.: Multi-task convolutional neural network for face recognition.

arXiv preprint arXiv:1702.04710 (2017)

12. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-

task learning. In: CVPR. (2016) 3994–4003

13. Ruder, S.: An overview of multi-task learning in deep neural networks. CoRR

abs/1706.05098 (2017)

14. Ranjan, R., Patel, V.M., Chellappa, R.: Hyperface: A deep multi-task learning
framework for face detection, landmark localization, pose estimation, and gender
recognition. arXiv preprint arXiv:1603.01249 (2016)

15. Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi-

task learning. In: ECCV, Springer (2014) 94–108

16. Liu, P., Qiu, X., Huang, X.: Adversarial multi-task learning for text classiﬁcation.

(2017)

17. Baxter, J.: A bayesian/information theoretic model of learning to learn via multiple

task sampling. Machine learning 28(1) (1997) 7–39

18. Duong, L., Cohn, T., Bird, S., Cook, P.: Low resource dependency parsing: Cross-
lingual parameter sharing in a neural network parser. In: ACL. Volume 2. (2015)
845–850

19. Yang, Y., Hospedales, T.M.: Trace norm regularised deep multi-task learning.

arXiv preprint arXiv:1606.04038 (2016)

20. Ruder, S., Bingel, J., Augenstein, I., Søgaard, A.: Sluice networks: Learning what
to share between loosely related tasks. arXiv preprint arXiv:1705.08142 (2017)
21. Long, M., Wang, J.: Learning multiple tasks with deep relationship networks.

arXiv preprint arXiv:1506.02117 (2015)

22. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv
preprint arXiv:1606.04671 (2016)

16

Multi-Task Learning by Deep Collaboration

23. Trottier, L.: Deep collaboration network in pytorch.

https://github.com/

ltrottier/deep-collaboration-network

24. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 448–456

25. Nair, V., Hinton, G.: Rectiﬁed linear units improve restricted boltzmann machines.

In: ICML. (2010) 807–814

26. Sun, Y., Wang, X., Tang, X.: Deep convolutional network cascade for facial point

detection. In: CVPR. (2013) 3476–3483

27. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using
multitask cascaded convolutional networks. IEEE Signal Processing Letters 23(10)
(2016) 1499–1503

28. Jourabloo, A., Liu, X.: Large-pose face alignment via cnn-based dense 3d model

ﬁtting. In: CVPR. (2016) 4188–4196

29. Baltrušaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial

behavior analysis toolkit. In: WACV, IEEE (2016) 1–10

30. Ding, C., Tao, D.: Robust face recognition via multimodal deep face representation.

IEEE Transactions on Multimedia 17(11) (2015) 2049–2058

31. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to

human-level performance in face veriﬁcation. In: CVPR. (2014) 1701–1708

32. Zhang, C., Zhang, Z.: Improving multiview face detection with multi-task deep

convolutional neural networks. In: WACV, IEEE (2014) 1036–1041

33. Fabian Benitez-Quiroz, C., Srinivasan, R., Martinez, A.M.: Emotionet: An accu-
rate, real-time algorithm for the automatic annotation of a million facial expres-
sions in the wild. In: CVPR. (2016) 5562–5570

34. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

35. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: ECCV, Springer (2016) 630–645

In: CVPR. (2016) 770–778

36. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with

stochastic depth. In: ECCV, Springer (2016) 646–661

37. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations

for deep neural networks. In: CVPR, IEEE (2017) 5987–5995

38. Veit, A., Wilber, M.J., Belongie, S.: Residual networks behave like ensembles of

relatively shallow networks. In: NIPS. (2016) 550–558

39. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization. In:
ICCV Workshops, IEEE (2011) 2144–2151

40. Kim, K.K.: Deep learning face detection and recognition, implemented by pytorch.

https://github.com/kuaikuaikim/DFace

41. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:

42. Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark.

ICCV. (2015)

In: CVPR. (2016) 5525–5533

8
1
0
2
 
r
a

M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
1
1
0
0
.
1
1
7
1
:
v
i
X
r
a

Multi-Task Learning by Deep Collaboration and
Application in Facial Landmark Detection

Ludovic Trottier

Philippe Giguère

Brahim Chaib-draa

ludovic.trottier.1@ulaval.ca
{philippe.giguere,brahim.chaib-draa}@ift.ulaval.ca

Laval University, Québec, Canada

Abstract. Convolutional neural networks (CNNs) have become the most
successful approach in many vision-related domains. However, they are
limited to domains where data is abundant. Recent works have looked
at multi-task learning (MTL) to mitigate data scarcity by leveraging
domain-speciﬁc information from related tasks. In this paper, we present
a novel soft-parameter sharing mechanism for CNNs in a MTL setting,
which we refer to as Deep Collaboration. We propose taking into ac-
count the notion that task relevance depends on depth by using lateral
transformation blocs with skip connections. This allows extracting task-
speciﬁc features at various depth without sacriﬁcing features relevant to
all tasks. We show that CNNs connected with our Deep Collaboration
obtain better accuracy on facial landmark detection with related tasks.
We ﬁnally verify that our approach eﬀectively allows knowledge sharing
by showing depth-speciﬁc inﬂuence of tasks that we know are related.

1 Introduction

Over the past few years, Convolutional Neural Networks (CNNs) have become
the leading approach in many vision-related tasks [1]. Their ability to learn
a hierarchy of increasingly abstract concepts allows them to transform com-
plex high-dimensional input images into simple low-dimensional output features.
CNNs have been used in many settings, but their need to have a large amount
of data during training has restricted them to domains where data is abundant.
Optimizing CNNs is tricky not only because of problems like vanishing / ex-
ploding gradients [2], but also because they typically have many parameters to
be learned. While previous works have looked at supervised and unsupervised
pre-training to improve generalization, others have considered casting their orig-
inal single-task problem into a new Multi-Task Learning (MTL) problem [3]. As
Caruana (1998) [4] explained in his seminal work: “MTL improves generalization
by leveraging the domain-speciﬁc information contained in the training signals
of related tasks". Exploring new ways to more eﬃciently gather information from
related tasks would help to further improve generalization on the main one.

MTL has proven its value in several domains over the years. It has become a
dominant ﬁeld of machine learning [5], with many inﬂuential works [6]. Although
MTL dates back several years, recent major advances in Deep Learning (DL)

2

Multi-Task Learning by Deep Collaboration

opened up opportunities for novel contributions. Works on grasping [7], pedes-
trian detection [8], natural language processing [9], face recognition [10][11] and
object detection [12] helped MTL make a resurgence in the DL community. They
have shown the potential of MTL to mitigate data scarcity when training deep
networks, which has inﬂuenced its growing popularity

MTL approaches can generally be divided into two major categories: hard
and soft parameter sharing [13]. Hard-parameter sharing dates back to the orig-
inal work of Caruana (1998) and is the most common of the two. Approaches
in this category have a shared central section with many heads (one per task).
Features from speciﬁc tasks compete together and those relevant to all tasks are
favored. Recent works in DL have shown that hard-parameter sharing can be
successful [14][15][7][11]. However, a too large emphasis on features relevant to all
tasks can be harmful for learning high-level features speciﬁc to a particular task.
These types of speciﬁc features are usually needed to obtain a good representa-
tion for the particular task. Also, shared layers are prone to be contaminated by
noise coming from noxious tasks [16]. These limitations can be detrimental even
though hard-parameter sharing reduces the risk of over-ﬁtting [17].

Soft-parameter sharing has been proposed as an alternative to alleviate these
drawbacks. Approaches in this category substitute the shared central section by
separate task-speciﬁc CNNs, but provide a knowledge sharing mechanism to
connect them. Each CNN can then learn task-speciﬁc features and share their
knowledge without interfering with others. Recent works in this category have
looked at regularizing the distance between task-speciﬁc parameters with a ‘2
norm [18] or a trace norm [19], training shared and private LSTM submod-
ules [16], partitioning the hidden layers into subspaces [20] and regularizing the
FC layers with tensor normal priors [21]. In the domain of continual learning,
progressive network [22] has also shown promising results for sequential transfer
learning, by employing lateral connections to previously learned networks.

In this paper, we present a novel soft-parameter knowledge sharing mecha-
nism for connecting task-speciﬁc CNNs in a MTL framework. We refer to our
approach as Deep Collaboration. We deﬁne connectivity in terms of a collabo-
rative block that uses two non-linear transformations with lateral connections.
One aggregates task-speciﬁc features into global features, and the other merges
back the global features into each task-speciﬁc CNN. Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
We evaluated our approach on the problem of facial landmark detection in a
MTL framework and obtained better results in comparison to other approaches
of the literature. We further assess the objectivity of our training framework by
randomly varying the contribution of each related tasks. Finally, we verify that
our collaborative block enables knowledge sharing with an ablation study that
shows the depth-speciﬁc inﬂuence of tasks that we know are related.

The content of our paper is organized as follows. In Section 2, we present
related works in MTL and facial landmark detection. We elaborate on our ap-
proach in Section 3, and present experimental results in Section 4. We ﬁnally
conclude our paper in Section 5. Our code is available here: [23].

Trottier et al.

3

2 Related Work

2.1 Multi-Task Learning

Our proposed Deep Collaboration knowledge sharing mechanism is related to
other existing approaches. One is Cross-Stitch (XS) [12], which connects task-
speciﬁc CNNs by linearly combining their feature maps at certain depths. One
drawback of XS is that it is limited to capture only linear dependencies between
each CNN. In contrast to XS, our approach uses non-linear transformations in
order to capture more complex dependencies.

Another related approach is Tasks-Constrained Deep Convolutional Network
(TCDCN) [15]. The authors proposed an early-stopping criterion to remove aux-
iliary tasks that start to overﬁt before becoming detrimental to the main task.
This approach has however several hyper-parameters to be selected manually.
For each task, it has an hyper-parameter controlling the period length of the
local window and a threshold that stops the task when the criterion exceeds it.
Unlike TCDCN, our approach has no hyper-parameters that need to be tuned to
the tasks at hand. Our collaborative block consists of a series of Batch Normal-
ization [24], ReLU [25], and convolutional layers shaped in a standard setting
that is commonly found in nowadays works.

Our proposed approach is also related to HyperFace [14]. The authors pro-
posed to fuse the layers at various depth and exploit features of diﬀerent levels
of complexity. Their goal was to allow low-level features with better localization
properties to help tasks such as landmark localization and pose detection, and al-
low high-level features with better class-speciﬁc properties to help tasks like face
detection and gender recognition. Although HyperFace is in the hard-parameter
sharing category and is not entirely related to our approach, the idea of feature
fusion is also central in our work. Instead of fusing the features at intermediate
layers of a single CNN, our approach aggregates same-level features of multiple
CNNs, at diﬀerent depth independently.

2.2 Facial Landmark Detection

Facial Landmark Detection (FLD) is an essential component in many face-
related tasks [26][27][28][29]. FLD can be described as follows: given the image of
a face of a person, the goal is to predict the (x, y)-position of speciﬁc landmarks
associated with key features of the visage. Applications such as face recogni-
tion [30], face validation [31], facial feature detection and tacking [32] rely on
the ability to correctly ﬁnd the location of these distinct facial landmarks in order
to succeed. Localizing facial key points like the center of the eyes, the corners of
the mouth, the tip of the nose and the earlobes is however a challenging problem
when many lighting conditions, head poses, facial expressions and occlusions in-
crease diversity of the face images. In addition to integrating this variability into
the estimation process, a FLD model must also take into account a number of
correlated factors. For instance, although both an angry person and a sad person
have frowned eyebrows, an angry person will have pinched lips while a sad person

4

Multi-Task Learning by Deep Collaboration

will have sunken mouth corners [33]. A particularity of datasets geared towards
FLD is that they are particularly well-suited for MTL. In addition to contain-
ing the position of the facial landmarks, these datasets also contain a number
of other labels that can be used to deﬁned auxiliary tasks. Gender recognition,
smile recognition, glasses recognition or face orientation are examples of tasks
often chosen to evaluate MTL approaches.

3 Deep Collaboration

Given T task-speciﬁc Convolutional Neural Networks (CNNs), our goal is to con-
nect them with lateral connections in order to allow domain-speciﬁc information
sharing. We deﬁne connectivity in terms of a collaborative block containing two
distinct non-linear transformations. One aggregates task-speciﬁc features into
global features, and the other merges back the global features into each task-
speciﬁc CNN. Our collaborative block is diﬀerentiable and can be dropped in
any existing CNN architectures as a whole. For this reason, we make no assump-
tion about the structure of the task-speciﬁc CNNs. Our approach can even work
with diﬀerent CNNs, but for the sake of simplicity, we suppose that the CNNs
are the same. We refer to it as the underlying network.

We also decompose the underlying network as a series of blocks. Each block
can be as small as a single layer, as large as the whole network itself, or based
on simple rules, such as grouping all layers with matching spatial dimensions or
grouping every n subsequent layers. The arrangement of the layers into blocks
does not change the composition of the underlying network. We only use it to
make explicit the depth at which we connect the task-speciﬁc CNNs.

Since our collaborative block can be inserted at any depth, we also drop the
depth index on the feature maps to further simplify the equations. As such,
we deﬁne the feature map output of a block at a certain depth as xt, where
t ∈ {1 . . . T } is the task index. Our approach takes as input all task-speciﬁc
feature maps xt and processes them into new feature maps yt as follows:

z = H([x1, . . . , xT ]) ,

yt = ReLU (xt + Ft([xt, z])) ,

(1)

where H and Ft represent the central and the task-speciﬁc aggregations re-
spectively, and [·] denotes depth-wise concatenation. We refer to Eq. (1) as our
collaborative block. The goal of H is to combine all task-speciﬁc feature maps xt
into a global feature map z representing uniﬁed knowledge, while the goal of F
is to merge back the global feature map z with each task-speciﬁc input xt. The
compositional structure of H and F is as follows:

H(·) = (ReLU ◦ BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,
F(·) = (BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,

(2)

(3)

where BN stands for Batch Normalization [24], Conv(h×w) for a standard convo-
lutional layer with ﬁlters of size (h × w), and ◦ is the usual function composition.
The ﬁrst Conv(1×1) layer in H divides the number of feature maps by a factor

Trottier et al.

5

Fig. 1. Example of our collaborative block applied on the feature maps of two task-
speciﬁc networks. The input feature maps (shown in 1(cid:13)) are ﬁrst concatenated depth-
wise and transformed into a global feature map ( 2(cid:13)). The global feature map is then
concatenate with each input feature map individually and transformed into task-speciﬁc
feature maps ( 3(cid:13)). Each resulting feature map is then added back to the input feature
map using a skip connection ( 4(cid:13)), which gives the ﬁnal outputs of the block ( 5(cid:13)).

of 4, while the ﬁrst Conv(1×1) layer in F divides it to match the size of xt. An
illustration of our collaborative block is shown in Fig. 1.

One particularity of our approach is that we use a skip connection in mapping
F. Recent works [34][35][36][37][38] have shown that networks with identity skip
connections are more easily able to learn proper input-output mappings. Inspired
by these works, we opted for an identity skip connection in F in order to more
easily learn the proper mapping to integrate domain-speciﬁc information from
the other tasks. In particular, identity skip connections put an incentive on
learning the identity mapping. We can see this by the ease at which the network
can obtain the identity mapping by simply pushing all the weights in F towards
zero. In our MTL context, the identity mapping can be seen as a way to remove
the inﬂuence of the global features z. This allows to take into account the cases
where integrating z back to the task-speciﬁc features xt would not help.

Another motivation for using an identity skip connection around the global
feature map z comes from the fact that depth inﬂuences the relevance of each
task towards another. Some task-speciﬁc CNNs can beneﬁt more when they
share their low-level features than their high-level features, while other beneﬁt
more in the other way. For instance, tasks such as landmark localization and
pose detection proﬁt more from low-level features containing better localization

6

Multi-Task Learning by Deep Collaboration

Fig. 2. Deep Collaboration Network (DCNet) using ResNet18 as underlying network
in a MTL setting on the MTFL dataset. The top part shows the block structure of
ResNet18 interleaved with our proposed collaborative block, while the bottom part
details each residual and task-speciﬁc FC blocks.

properties, while tasks such as face detection and gender recognition proﬁt more
from class-speciﬁc high-level features. Considering that CNNs learn a hierarchy
of increasingly abstract features, our collaborative block can take into account
task relevance by deactivating a diﬀerent set of residual mappings Ft based on
the depth at which it is inserted. An example of such specialization will be shown
in our ablative study in Section 4.4.

Fig. 2 presents an example of inserting our collaborative block at diﬀerent
depths in a MTL framework on the MTFL dataset [15]. In this particular case,
we opted for a ResNet18 as underlying network. We refer to this network as our
Deep Collaboration Network (DCNet). As we can see in the top part of the ﬁgure,
integrating our approach comes down to interleaving the underlying network
block structure with our collaborative block. Each collaborative block receives
as input the output of each task-speciﬁc block, processes them as detailed in
Eq. (1), and sends the result back to each task-speciﬁc network. Adding our
approach to any underlying network can be done by simply following the same
pattern of interleaving the network block structure with our collaborative block.

4 Experiments

In this section, we detail our Multi-Task Learning (MTL) training framework and
present our experiments in Facial Landmark Detection (FLD) tasks. We further
evaluate the eﬀect of data scarcity on performance and illustrate an example of
knowledge sharing between task-speciﬁc CNNs with an ablation study.

Trottier et al.

7

4.1 Multi-Task Learning Training Framework

The goal of Facial Landmark Detection (FLD) is to predict the (x, y)-position
of speciﬁc landmarks associated with key features of the visage. While the num-
ber and type of landmarks are speciﬁc to each dataset, examples of standard
landmarks to be predicted are the corners of the mouth, the tip of the nose
and the center of the eyes. In addition to the facial landmarks, each dataset
further deﬁnes a number of related tasks. These related tasks also vary from
one dataset to another, and are typically gender recognition, smile recognition,
glasses recognition or face orientation.

On a more technical level, we deﬁne a learning framework in which we treat
each task as a classiﬁcation problem. While this is straightforward for gender,
smile and glasses recognition as they are already classiﬁcation tasks, it is a
bit more tricky for face orientation and FLD. For face orientation, instead of
predicting the roll, yaw and pitch real value as in a regression problem, we
divide each component into 30 degrees wide bins and predict the label of the bin
corresponding to the value. Similarly for FLD, rather than predicting the real
(x, y)-position of each landmark, we divide the image into 1 pixel wide bins and
predict the label of the bin corresponding to the value. Note that we still use
the original real values when comparing our prediction with the ground truth,
so that we incorporate our approximation errors in the ﬁnal score.

We report our results using the landmark failure rate metric [15], which is
deﬁned as follows: we ﬁrst compute the mean distance between the predicted
landmarks and the ground truth landmarks, then normalize it by the inter-
ocular distance from the center of the eyes. A normalized mean distance greater
than 10% is reported as a failure.

4.2 Facial Landmark Detection on the MTFL Task

As a ﬁrst experiment, we performed facial landmark detection on the Multi-Task
Facial Landmark (MTFL) task [15]. The dataset contains 12,995 face images an-
notated with ﬁve facial landmarks and four related attributes of gender, smiling,
wearing glasses and face proﬁle (ﬁve proﬁles in total). The training set has 10,000
images, while the test set has 2,995 images. We perform four sets of experiments
using an ImageNet pre-trained AlexNet, an ImageNet pre-trained ResNet18, an
un-pretrained AlexNet and an un-pretrained ResNet18 as underlying networks.
For AlexNet, we apply our collaborative block after each max pooling layer,
while for ResNet18, we do as shown in Fig. 2.

We compare our approach to several other approaches of the literature. We
include single-task learning (AN-S when using AlexNet as underlying network,
RN-S when using ResNet18), hard-parameter sharing MTL (AN and RN), hard-
parameter sharing MTL where the central section is widened to match the num-
ber of parameters of our approach (ANx and RNx), HyperFace (HF) [14], Tasks-
Constrained Deep Convolutional Network (TCDCN) [15], Cross-Stitch (XS) [12]
and XS widen to match the number of parameters of our approach (XSx). Ex-
cept for TCDCN, we train each network ourselves three times for 300 epochs and

8

Multi-Task Learning by Deep Collaboration

Fig. 3. Landmark failure rates (%) on the MTFL task. The reported values are the
average over the last ﬁve epochs, averaged over three tries. The left plot presents our
results with AlexNet as the underlying network, while the right one with ResNet18.
AN-S and RN-S stand for single-task training, AN and RN for multi-task training with
a single central network, ANx and RNx for multi-task training with a single central
network widen to match the number of parameters of our approach, HF for HyperFace,
TCDCN for [15]’s approach and XS for Cross-Stitch. In each instance, the left column
(blue) is for un-pretrained networks, while the right column (green) is for pre-trained
networks. Our proposed approach obtains the lowest failure rates overall.

report landmark failure rates averaged over the last ﬁve epochs, further averaged
over the three tries.

Fig. 3 presents our FLD results on the MTFL dataset. The left part of the
ﬁgure corresponds to using AlexNet as underlying network, while the right one
corresponds to ResNet18. The top part reports the landmark failure rates, while
the bottom part reports the mean error. In each plot, the left bar (blue) is for
un-pretrained network, while the right bar (green) is for ImageNet pre-trained
network. In addition, Fig. 4 shows example predictions from DCNet with pre-
trained ResNet18 as underlying network. The ﬁrst two examples were reported as
successes, while the last two are failures. The ground truth elements are colored
in green, while our predictions are colored in blue. We also include the labels of
the related tasks: gender, smiling, wearing glasses and face proﬁle.

The results of Fig. 3 show that our proposed approach obtained the lowest
failure rates and mean error in each case. Indeed, our DCNet with un-pretrained
and pre-trained AlexNet as underlying network obtained 19.67% and 19.96%
failure rates respectively, and 14.95% and 13.52% with ResNet18. This is sig-
niﬁcantly lower than the other approaches to which we compare ourselves. For
instance, with AlexNet, HF had 27.75% and 27.32%, XS had 26.41% and 25.65%,

Trottier et al.

9

MTFL

AFLW

Fig. 4. Example predictions of our DCNet with pre-trained ResNet18 as underlying
network on the MTFL and AFLW task. For MTFL, the ﬁrst two examples are successes,
while last two are failure cases. For AFLW, the ﬁrst three examples are successes, while
the last one is a failure case. Elements in green correspond to ground truth, while those
in blue correspond to predictions. Facial landmarks are shown as small dots, and related
tasks labels are displayed on the side. As we can see, over-exposition and tilted face
proﬁle can have a large impact on the prediction quality.

TCDCN had 25.00%1, and XSx had 25.23%. With ResNet18, XS had 18.43% and
15.52% respectively, and XSx had 17.28. We obtained the highest improvements
when using AlexNet as the underlying network when comparing to XS. With un-
pretrained and pre-trained AlexNet, we obtained improvements of 6.74% and
5.69%, while we obtained 3.48% and 2.00% with ResNet18. Performing MTL
with our approach can thus improve performance over using other approaches
of the literature.

Another result that we can see from Fig. 3 is that our soft-parameter shar-
ing approach obtains higher performance than the hard-parameter sharing ap-
proaches with matching number of parameters. For instance, increasing the
number of parameters of hard-parameter sharing AlexNet lowers it error rate
from 28.02% (AN) to 26.88% (ANx), but our approach lowers it further to
19.67%. Similarly, increasing the number of parameters of hard-parameter shar-
ing ResNet18 lowers it error rate from 20.05% (RN) to 16.75% (RNx), but our
approach lowers it further to 14.95%. These results are interesting because they
show that while increasing the number of parameters is an eﬀortless avenue to
improve performance, it has limitations. Developing novel approaches to enhance
network connectivity in a soft-parameter sharing setting seems more rewarding.
This may help to motivate new eﬀorts in this avenue to further leverage domain-
information of related tasks.

1 Zhang et. al only provided results with pre-trained AlexNet [15]

10

Multi-Task Learning by Deep Collaboration

Table 1. Landmark failure rate results on the AFLW dataset using a pre-trained
ResNet18 as underlying network. The presented values are averaged over the last ﬁve
epochs, further averaged over three tries. The ﬁrst column is the train / test ratio,
and the subsequent ones are the networks: single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS). Our approach obtains the best perfor-
mance in all cases, except the ﬁrst one where we observe over-ﬁtting.

Train / Test Ratio

0.1 / 0.9
0.3 / 0.7
0.5 / 0.5
0.7 / 0.3
0.9 / 0.1

RN-S

57.39
31.84
23.41
21.47
13.03

Networks

RN

58.00
32.00
23.31
21.92
12.80

XS

73.06
36.24
26.02
22.37
13.51

Ours

60.64
29.73
20.77
18.50
10.82

4.3 Eﬀect of Data Scarcity on the AFLW Task

As second experiment, we evaluated the inﬂuence of the number of training
examples to simulate data scarcity on the Annotated Facial Landmarks in the
Wild (AFLW) task [39]. The dataset has 21,123 Flickr images, and each image
can contain more than one face. Instead of using the images as provided, we
process them using the available face bounding boxes. We extract all faces with
visible landmarks, which gives a total of 2,111 images. This dataset deﬁnes 21
facial landmarks and has 3 related tasks (gender, wearing glasses and face ori-
entation). For face orientation, we divide the roll, yaw and pitch into 30 degrees
wide bins (14 bins in total), and predict the label corresponding to each real
value.

Our experiment works as follows. With a pre-trained ResNet18 as underlying
network, we compare our approach to single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS) by training on a varying number
of images. We use ﬁve diﬀerent train / test ratios, starting with 0.1 / 0.9 up
to 0.9 / 0.1 by 0.2 increment. In other words, we train each approaches on the
ﬁrst r% of the available images and test on the other (1 − r)%, then repeat
for all the other train / test ratios. We use the same training framework as in
section 4.2. We train each network three times for 300 epochs, and report the
landmark failure rate averaged over the last ﬁve epochs, further averaged over
the three tries. Example predictions are shown in Fig. 4.

As we can see in Table 1, our approach obtained the best performance in
all cases except the ﬁrst one. Indeed, we observe between 1.98% and 6.51%
improvements with train / test ratios from 0.3 / 0.7 to 0.9 / 0.1, while we obtain
a negative relative change of 3.25% with train / test ratio of 0.1 / 0.9. However,
since all multi-task approaches obtained higher failure rates than the single-task
approach, this suggests that the networks are over-ﬁtting the small training set.
Nonetheless, these results show that we can obtain better performance using our
approach.

Trottier et al.

11

Fig. 5. Landmark failure rate improvement (in %) of our approach compared to XS
when sampling random task weights. We used a pre-trained ResNet18 as underlying
network. The histogram at the left and the plot at the top right represents performance
improvement achieved by our proposed approach (positive value means lower failure
rates), while the plot at the bottom right corresponds to the log of the task weights. Our
approach outperformed XS in 86 out of the 100 tries, thus empirically demonstrating
that our learning framework was not unfavorable towards XS and that our approach
is less sensitive to the task weights λ.

One particularity that we observe in Table 1 is that the XS network has
relatively high failure rates. In the previous experiment of Section 4.2, XS had
either similar or better performance than the other approaches (except ours).
This could be due to our current multi-task learning framework that is unfavor-
able towards XS. In order to investigate whether this is the case, we perform
the following additional experiment. Using a pre-trained ResNet18 as underlying
network, we compare our approach to XS by training each network 100 times
using task weights randomly sampled from a log-uniform distribution. Speciﬁ-
cally, we ﬁrst sample from a uniform distribution γ ∼ U(log(1e−4), log(1)), then
use λ = exp(γ) as the weight. We trained both XS and our approach for 300
epochs with the same task weights using a train / test ratio of 0.5 / 0.5.

Figure 5 presents the results of this experiment. The plot at the top right
of the ﬁgure represents the landmark failure rate improvement (in %) of our
approach compared to XS, while the plot at the bottom right corresponds to the
log of the task weights for each try. In 86 out of the 100 tries, our approach had
a positive failure rate improvement, that is, obtained lower failure rates than
XS. As we can see in the histogram at the left of Fig. 5, the improvement rate
is normally distributed around 2.80%, has a median improvement of 3.66% and
a maximum improvement of 9.83%. Even though we sampled at random the
weights of the related tasks, our approach outperforms XS in the majority of the
cases. Our learning framework was therefore not unfavorable toward XS.

4.4 Illustration of Knowledge Sharing With an Ablation Study

As third experiment, we perform an ablation study on the MTFL task [15] with
an un-pretrained ResNet18 as underlying network. The goal of this experiment

12

Multi-Task Learning by Deep Collaboration

Fig. 6. Results of our ablation study on the MTFL dataset with an un-pretrained
ResNet18 as underlying network. We remove each task-speciﬁc features from each re-
spective central aggregation layer and evaluate the eﬀect on landmark failure rate. The
rows represent the task-speciﬁc CNNs, while the columns correspond to the network
block structure. Blocks with a high saturated color were found to have a large impact
on failure rate. In particular, this ablative study shows that the inﬂuence of high-
level face proﬁle features is large within our proposed architecture. This corroborates
with the well-known fact that the location of facial landmarks is closely dependent on
the orientation of the face. This constitutes an empirical evidence of domain-speciﬁc
information sharing via our approach.

is to verify that our collaborative block eﬀectively enables knowledge sharing
between task-speciﬁc CNNs. To do so, we evaluate the impact, on facial landmark
detection, of removing the contribution of each task-speciﬁc features. We zero out
the designated feature map xt before concatenation at the input of the central
aggregation H. The network is trained using the same framework as explained
in Sec. 4.1, and the ablation study is performed at test time on the test set when
training is done.

Figure 6 presents the results of our ablation study. The rows represent each
task-speciﬁc CNN, while the columns correspond to the network block structure.
The blocks are ordered from left (input) to right (output), while the task-speciﬁc
networks are ordered from top (main task) to bottom (related tasks). The color
saturation indicates the inﬂuence of removing the task-speciﬁc feature maps from
the central aggregation at the corresponding depth. A high saturation reﬂects
high inﬂuence on failure rate, while a low saturation reﬂects low inﬂuence.

As ﬁrst result, removing features from the facial landmark detection network
signiﬁcantly increases landmark failure rate. For instance, we observe a negative
(worse) relative change of 29.72% and 47.00% in failure rate by removing fea-
tures from Block 3 and Block 2 respectively. This illustrates that the main-task
network both contributed to and fed from the global features computed by the
central aggregation H. The CNN for landmark detection had the possibility to
remove the contribution of the global features, and so isolate itself from the other
CNNs, but the opposite occurred. We actually observe a mutual inﬂuence be-
tween the CNNs, where the task-speciﬁc features from the facial landmark CNN

Trottier et al.

13

inﬂuence the quality of the global features, which in turn inﬂuence the quality
of the subsequent task-speciﬁc features.

Another result that we can see from Fig. 6 is that Block 5 of task Proﬁle
has the highest inﬂuence on failure rate. We observe a negative relative change
of 83.87% by removing the features maps of task Proﬁle from the central ag-
gregation. What is particularly interesting in this case is that we observe this
high relative change at Block 5, which corresponds to the highest block in the
network. Since the block lies at the top of the network, it outputs features with
a high level of abstraction. We therefore expect that these features represent
high-level factors of variation corresponding to face orientation, which should
look like a rotation matrix. It therefore makes sense that features representing
the orientation of the face would be useful to predict facial landmarks, since
we know that the location of the facial landmarks is closely dependent on the
orientation of the face. The landmark CNN can use these rich features to bet-
ter rotate the predicted facial landmarks. This is indeed what we observe in
Fig. 6. These results constitute an empirical evidence that our approach allows
leveraging domain-speciﬁc information from related tasks.

4.5 Facial Landmark Detection With MTCNN

As ﬁnal experiment, we performed an experimental evaluation using the recent
Multi-Task Cascaded Convolutional Network (MTCNN) [27]. The authors pro-
posed a cascade structure of three stages, where each stage is composed of a
multi-task CNN. MTCNN performs predictions in a coarse-to-ﬁne manner. The
CNN of the ﬁrst stage generates (in a fully-convolutional way) many hypothe-
ses about the position of the face and the facial landmarks, and the subsequent
second and third stages reﬁnes them. The CNNs are trained sequentially with
hard-negative mining, in a hard-parameter sharing setting.

We implemented our approach in the available code project [40] and com-
pared ourself to MTCNN. We followed the provided hard negative mining recipe
and generated our images. For landmark detection, we used the LFWNet [26]
and CelebA [41] datasets, and generated 600k face images with facial landmarks.
For face detection, we used the WIDER [42] dataset, and generated 1.5M face
images with a bounding box. We trained a MTCNN with the stage networks
connected with our collaborative block, and a standard MTCNN with widened
stage networks to match the number of parameters.

On the test set of MTFL [15] dataset, standard MTCNN obtained a land-
mark failure rate of 37.85%, a mean error of 0.0996 and failed to detect a face
112 times, while our approach obtained better performances with 28.97%, 0.0930
and 79 respectively. Note that the reason MTCNN obtains worse performance
than our Deep Collaboration Network (DCNet), as reported in Fig. 3, is because
it has fewer parameters. DCNet has about 85M parameters, while the sum of
all three stage-CNN in MTCNN is about 2M. This is because MTCNN is care-
fully designed to balance computational speed and landmark detection precision.
It can predict many faces in high-dimensional images with a low computation
burden. An example of its prediction capability is shown in Fig. 7.

14

Multi-Task Learning by Deep Collaboration

Fig. 7. MTCNN predictions on the photo of the 2018 Oscar nominees (image resolution
of 2983 × 1197). The stage-CNNs are trained using our proposed collaborative block.
The coarse-to-ﬁne detection scheme employed by MTCNN allows predicting many faces
in high-dimensional images with low computational burden.

5 Conclusion and Future Work

In this paper, we proposed a novel soft-parameter knowledge sharing mechanism
based on lateral connections for Multi-Task Learning (MTL). Our proposed ap-
proach implements connectivity in term of a collaborative block, which uses two
distinct non-linear transformations. The ﬁrst one aggregates task-speciﬁc fea-
tures into global features, and the other merges back the global features into each
task-speciﬁc Convolutional Neural Network (CNN). Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
Our results on facial landmark detection tasks showed that networks connected
with our proposed collaborative block outperformed the other state-of-the-art
approaches, including the recent Cross-Stitch and MTCNN approach. We ver-
ify that our collaborative block eﬀectively enables knowledge sharing between
task-speciﬁc CNNs with an ablation study. We observed that the CNNs incor-
porated features with a varying level of abstraction from the other CNNs, by
observing the depth-speciﬁc inﬂuence of tasks that we know are related. These
results constituted an empirical evidence that our approach allows leveraging
domain-speciﬁc information from related tasks. Evaluating our proposed ap-
proach on other MTL problems could be an interesting avenue for future works.
For instance, the recurrent networks used to solve natural language processing
problems could beneﬁt from our approach.

Acknowledgements

We gratefully acknowledge the support of NVIDIA Corporation for providing a
Tesla Titan X for our experiments through their Hardware Grant Program.

Trottier et al.

15

References

1. Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classiﬁcation with deep convo-

lutional neural networks. In: NIPS. (2012) 1097–1105

2. Hochreiter, S.: The vanishing gradient problem during learning recurrent neural
nets and problem solutions. International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems 6(02) (1998) 107–116

A survey on multi-task learning.

arXiv preprint

3. Zhang, Y., Yang, Q.:
arXiv:1707.08114 (2017)

4. Caruana, R.: Multitask learning. In: Learning to learn. Springer (1998) 95–133
5. Zhang, M.L., Zhou, Z.H.: A review on multi-label learning algorithms. TKDE

6. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: KDD, ACM (2004)

26(8) (2014) 1819–1837

109–117

7. Pinto, L., Gupta, A.: Learning to push by grasping: Using multiple tasks for

eﬀective learning. In: ICRA, IEEE (2017) 2161–2168

8. Tian, Y., Luo, P., Wang, X., Tang, X.: Pedestrian detection aided by deep learning

semantic tasks. In: CVPR. (2015) 5079–5087

9. Liu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y.Y.: Representation learning
using multi-task deep neural networks for semantic classiﬁcation and information
retrieval. In: HLT-NAACL. (2015) 912–921

10. Yim, J., Jung, H., Yoo, B., Choi, C., Park, D., Kim, J.: Rotating your face using

multi-task deep neural network. In: CVPR. (2015) 676–684

11. Yin, X., Liu, X.: Multi-task convolutional neural network for face recognition.

arXiv preprint arXiv:1702.04710 (2017)

12. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-

task learning. In: CVPR. (2016) 3994–4003

13. Ruder, S.: An overview of multi-task learning in deep neural networks. CoRR

abs/1706.05098 (2017)

14. Ranjan, R., Patel, V.M., Chellappa, R.: Hyperface: A deep multi-task learning
framework for face detection, landmark localization, pose estimation, and gender
recognition. arXiv preprint arXiv:1603.01249 (2016)

15. Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi-

task learning. In: ECCV, Springer (2014) 94–108

16. Liu, P., Qiu, X., Huang, X.: Adversarial multi-task learning for text classiﬁcation.

(2017)

17. Baxter, J.: A bayesian/information theoretic model of learning to learn via multiple

task sampling. Machine learning 28(1) (1997) 7–39

18. Duong, L., Cohn, T., Bird, S., Cook, P.: Low resource dependency parsing: Cross-
lingual parameter sharing in a neural network parser. In: ACL. Volume 2. (2015)
845–850

19. Yang, Y., Hospedales, T.M.: Trace norm regularised deep multi-task learning.

arXiv preprint arXiv:1606.04038 (2016)

20. Ruder, S., Bingel, J., Augenstein, I., Søgaard, A.: Sluice networks: Learning what
to share between loosely related tasks. arXiv preprint arXiv:1705.08142 (2017)
21. Long, M., Wang, J.: Learning multiple tasks with deep relationship networks.

arXiv preprint arXiv:1506.02117 (2015)

22. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv
preprint arXiv:1606.04671 (2016)

16

Multi-Task Learning by Deep Collaboration

23. Trottier, L.: Deep collaboration network in pytorch.

https://github.com/

ltrottier/deep-collaboration-network

24. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 448–456

25. Nair, V., Hinton, G.: Rectiﬁed linear units improve restricted boltzmann machines.

In: ICML. (2010) 807–814

26. Sun, Y., Wang, X., Tang, X.: Deep convolutional network cascade for facial point

detection. In: CVPR. (2013) 3476–3483

27. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using
multitask cascaded convolutional networks. IEEE Signal Processing Letters 23(10)
(2016) 1499–1503

28. Jourabloo, A., Liu, X.: Large-pose face alignment via cnn-based dense 3d model

ﬁtting. In: CVPR. (2016) 4188–4196

29. Baltrušaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial

behavior analysis toolkit. In: WACV, IEEE (2016) 1–10

30. Ding, C., Tao, D.: Robust face recognition via multimodal deep face representation.

IEEE Transactions on Multimedia 17(11) (2015) 2049–2058

31. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to

human-level performance in face veriﬁcation. In: CVPR. (2014) 1701–1708

32. Zhang, C., Zhang, Z.: Improving multiview face detection with multi-task deep

convolutional neural networks. In: WACV, IEEE (2014) 1036–1041

33. Fabian Benitez-Quiroz, C., Srinivasan, R., Martinez, A.M.: Emotionet: An accu-
rate, real-time algorithm for the automatic annotation of a million facial expres-
sions in the wild. In: CVPR. (2016) 5562–5570

34. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

35. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: ECCV, Springer (2016) 630–645

In: CVPR. (2016) 770–778

36. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with

stochastic depth. In: ECCV, Springer (2016) 646–661

37. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations

for deep neural networks. In: CVPR, IEEE (2017) 5987–5995

38. Veit, A., Wilber, M.J., Belongie, S.: Residual networks behave like ensembles of

relatively shallow networks. In: NIPS. (2016) 550–558

39. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization. In:
ICCV Workshops, IEEE (2011) 2144–2151

40. Kim, K.K.: Deep learning face detection and recognition, implemented by pytorch.

https://github.com/kuaikuaikim/DFace

41. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:

42. Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark.

ICCV. (2015)

In: CVPR. (2016) 5525–5533

8
1
0
2
 
r
a

M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
1
1
1
0
0
.
1
1
7
1
:
v
i
X
r
a

Multi-Task Learning by Deep Collaboration and
Application in Facial Landmark Detection

Ludovic Trottier

Philippe Giguère

Brahim Chaib-draa

ludovic.trottier.1@ulaval.ca
{philippe.giguere,brahim.chaib-draa}@ift.ulaval.ca

Laval University, Québec, Canada

Abstract. Convolutional neural networks (CNNs) have become the most
successful approach in many vision-related domains. However, they are
limited to domains where data is abundant. Recent works have looked
at multi-task learning (MTL) to mitigate data scarcity by leveraging
domain-speciﬁc information from related tasks. In this paper, we present
a novel soft-parameter sharing mechanism for CNNs in a MTL setting,
which we refer to as Deep Collaboration. We propose taking into ac-
count the notion that task relevance depends on depth by using lateral
transformation blocs with skip connections. This allows extracting task-
speciﬁc features at various depth without sacriﬁcing features relevant to
all tasks. We show that CNNs connected with our Deep Collaboration
obtain better accuracy on facial landmark detection with related tasks.
We ﬁnally verify that our approach eﬀectively allows knowledge sharing
by showing depth-speciﬁc inﬂuence of tasks that we know are related.

1 Introduction

Over the past few years, Convolutional Neural Networks (CNNs) have become
the leading approach in many vision-related tasks [1]. Their ability to learn
a hierarchy of increasingly abstract concepts allows them to transform com-
plex high-dimensional input images into simple low-dimensional output features.
CNNs have been used in many settings, but their need to have a large amount
of data during training has restricted them to domains where data is abundant.
Optimizing CNNs is tricky not only because of problems like vanishing / ex-
ploding gradients [2], but also because they typically have many parameters to
be learned. While previous works have looked at supervised and unsupervised
pre-training to improve generalization, others have considered casting their orig-
inal single-task problem into a new Multi-Task Learning (MTL) problem [3]. As
Caruana (1998) [4] explained in his seminal work: “MTL improves generalization
by leveraging the domain-speciﬁc information contained in the training signals
of related tasks". Exploring new ways to more eﬃciently gather information from
related tasks would help to further improve generalization on the main one.

MTL has proven its value in several domains over the years. It has become a
dominant ﬁeld of machine learning [5], with many inﬂuential works [6]. Although
MTL dates back several years, recent major advances in Deep Learning (DL)

2

Multi-Task Learning by Deep Collaboration

opened up opportunities for novel contributions. Works on grasping [7], pedes-
trian detection [8], natural language processing [9], face recognition [10][11] and
object detection [12] helped MTL make a resurgence in the DL community. They
have shown the potential of MTL to mitigate data scarcity when training deep
networks, which has inﬂuenced its growing popularity

MTL approaches can generally be divided into two major categories: hard
and soft parameter sharing [13]. Hard-parameter sharing dates back to the orig-
inal work of Caruana (1998) and is the most common of the two. Approaches
in this category have a shared central section with many heads (one per task).
Features from speciﬁc tasks compete together and those relevant to all tasks are
favored. Recent works in DL have shown that hard-parameter sharing can be
successful [14][15][7][11]. However, a too large emphasis on features relevant to all
tasks can be harmful for learning high-level features speciﬁc to a particular task.
These types of speciﬁc features are usually needed to obtain a good representa-
tion for the particular task. Also, shared layers are prone to be contaminated by
noise coming from noxious tasks [16]. These limitations can be detrimental even
though hard-parameter sharing reduces the risk of over-ﬁtting [17].

Soft-parameter sharing has been proposed as an alternative to alleviate these
drawbacks. Approaches in this category substitute the shared central section by
separate task-speciﬁc CNNs, but provide a knowledge sharing mechanism to
connect them. Each CNN can then learn task-speciﬁc features and share their
knowledge without interfering with others. Recent works in this category have
looked at regularizing the distance between task-speciﬁc parameters with a ‘2
norm [18] or a trace norm [19], training shared and private LSTM submod-
ules [16], partitioning the hidden layers into subspaces [20] and regularizing the
FC layers with tensor normal priors [21]. In the domain of continual learning,
progressive network [22] has also shown promising results for sequential transfer
learning, by employing lateral connections to previously learned networks.

In this paper, we present a novel soft-parameter knowledge sharing mecha-
nism for connecting task-speciﬁc CNNs in a MTL framework. We refer to our
approach as Deep Collaboration. We deﬁne connectivity in terms of a collabo-
rative block that uses two non-linear transformations with lateral connections.
One aggregates task-speciﬁc features into global features, and the other merges
back the global features into each task-speciﬁc CNN. Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
We evaluated our approach on the problem of facial landmark detection in a
MTL framework and obtained better results in comparison to other approaches
of the literature. We further assess the objectivity of our training framework by
randomly varying the contribution of each related tasks. Finally, we verify that
our collaborative block enables knowledge sharing with an ablation study that
shows the depth-speciﬁc inﬂuence of tasks that we know are related.

The content of our paper is organized as follows. In Section 2, we present
related works in MTL and facial landmark detection. We elaborate on our ap-
proach in Section 3, and present experimental results in Section 4. We ﬁnally
conclude our paper in Section 5. Our code is available here: [23].

Trottier et al.

3

2 Related Work

2.1 Multi-Task Learning

Our proposed Deep Collaboration knowledge sharing mechanism is related to
other existing approaches. One is Cross-Stitch (XS) [12], which connects task-
speciﬁc CNNs by linearly combining their feature maps at certain depths. One
drawback of XS is that it is limited to capture only linear dependencies between
each CNN. In contrast to XS, our approach uses non-linear transformations in
order to capture more complex dependencies.

Another related approach is Tasks-Constrained Deep Convolutional Network
(TCDCN) [15]. The authors proposed an early-stopping criterion to remove aux-
iliary tasks that start to overﬁt before becoming detrimental to the main task.
This approach has however several hyper-parameters to be selected manually.
For each task, it has an hyper-parameter controlling the period length of the
local window and a threshold that stops the task when the criterion exceeds it.
Unlike TCDCN, our approach has no hyper-parameters that need to be tuned to
the tasks at hand. Our collaborative block consists of a series of Batch Normal-
ization [24], ReLU [25], and convolutional layers shaped in a standard setting
that is commonly found in nowadays works.

Our proposed approach is also related to HyperFace [14]. The authors pro-
posed to fuse the layers at various depth and exploit features of diﬀerent levels
of complexity. Their goal was to allow low-level features with better localization
properties to help tasks such as landmark localization and pose detection, and al-
low high-level features with better class-speciﬁc properties to help tasks like face
detection and gender recognition. Although HyperFace is in the hard-parameter
sharing category and is not entirely related to our approach, the idea of feature
fusion is also central in our work. Instead of fusing the features at intermediate
layers of a single CNN, our approach aggregates same-level features of multiple
CNNs, at diﬀerent depth independently.

2.2 Facial Landmark Detection

Facial Landmark Detection (FLD) is an essential component in many face-
related tasks [26][27][28][29]. FLD can be described as follows: given the image of
a face of a person, the goal is to predict the (x, y)-position of speciﬁc landmarks
associated with key features of the visage. Applications such as face recogni-
tion [30], face validation [31], facial feature detection and tacking [32] rely on
the ability to correctly ﬁnd the location of these distinct facial landmarks in order
to succeed. Localizing facial key points like the center of the eyes, the corners of
the mouth, the tip of the nose and the earlobes is however a challenging problem
when many lighting conditions, head poses, facial expressions and occlusions in-
crease diversity of the face images. In addition to integrating this variability into
the estimation process, a FLD model must also take into account a number of
correlated factors. For instance, although both an angry person and a sad person
have frowned eyebrows, an angry person will have pinched lips while a sad person

4

Multi-Task Learning by Deep Collaboration

will have sunken mouth corners [33]. A particularity of datasets geared towards
FLD is that they are particularly well-suited for MTL. In addition to contain-
ing the position of the facial landmarks, these datasets also contain a number
of other labels that can be used to deﬁned auxiliary tasks. Gender recognition,
smile recognition, glasses recognition or face orientation are examples of tasks
often chosen to evaluate MTL approaches.

3 Deep Collaboration

Given T task-speciﬁc Convolutional Neural Networks (CNNs), our goal is to con-
nect them with lateral connections in order to allow domain-speciﬁc information
sharing. We deﬁne connectivity in terms of a collaborative block containing two
distinct non-linear transformations. One aggregates task-speciﬁc features into
global features, and the other merges back the global features into each task-
speciﬁc CNN. Our collaborative block is diﬀerentiable and can be dropped in
any existing CNN architectures as a whole. For this reason, we make no assump-
tion about the structure of the task-speciﬁc CNNs. Our approach can even work
with diﬀerent CNNs, but for the sake of simplicity, we suppose that the CNNs
are the same. We refer to it as the underlying network.

We also decompose the underlying network as a series of blocks. Each block
can be as small as a single layer, as large as the whole network itself, or based
on simple rules, such as grouping all layers with matching spatial dimensions or
grouping every n subsequent layers. The arrangement of the layers into blocks
does not change the composition of the underlying network. We only use it to
make explicit the depth at which we connect the task-speciﬁc CNNs.

Since our collaborative block can be inserted at any depth, we also drop the
depth index on the feature maps to further simplify the equations. As such,
we deﬁne the feature map output of a block at a certain depth as xt, where
t ∈ {1 . . . T } is the task index. Our approach takes as input all task-speciﬁc
feature maps xt and processes them into new feature maps yt as follows:

z = H([x1, . . . , xT ]) ,

yt = ReLU (xt + Ft([xt, z])) ,

(1)

where H and Ft represent the central and the task-speciﬁc aggregations re-
spectively, and [·] denotes depth-wise concatenation. We refer to Eq. (1) as our
collaborative block. The goal of H is to combine all task-speciﬁc feature maps xt
into a global feature map z representing uniﬁed knowledge, while the goal of F
is to merge back the global feature map z with each task-speciﬁc input xt. The
compositional structure of H and F is as follows:

H(·) = (ReLU ◦ BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,
F(·) = (BN ◦ Conv(3×3) ◦ ReLU ◦ BN ◦ Conv(1×1))(·) ,

(2)

(3)

where BN stands for Batch Normalization [24], Conv(h×w) for a standard convo-
lutional layer with ﬁlters of size (h × w), and ◦ is the usual function composition.
The ﬁrst Conv(1×1) layer in H divides the number of feature maps by a factor

Trottier et al.

5

Fig. 1. Example of our collaborative block applied on the feature maps of two task-
speciﬁc networks. The input feature maps (shown in 1(cid:13)) are ﬁrst concatenated depth-
wise and transformed into a global feature map ( 2(cid:13)). The global feature map is then
concatenate with each input feature map individually and transformed into task-speciﬁc
feature maps ( 3(cid:13)). Each resulting feature map is then added back to the input feature
map using a skip connection ( 4(cid:13)), which gives the ﬁnal outputs of the block ( 5(cid:13)).

of 4, while the ﬁrst Conv(1×1) layer in F divides it to match the size of xt. An
illustration of our collaborative block is shown in Fig. 1.

One particularity of our approach is that we use a skip connection in mapping
F. Recent works [34][35][36][37][38] have shown that networks with identity skip
connections are more easily able to learn proper input-output mappings. Inspired
by these works, we opted for an identity skip connection in F in order to more
easily learn the proper mapping to integrate domain-speciﬁc information from
the other tasks. In particular, identity skip connections put an incentive on
learning the identity mapping. We can see this by the ease at which the network
can obtain the identity mapping by simply pushing all the weights in F towards
zero. In our MTL context, the identity mapping can be seen as a way to remove
the inﬂuence of the global features z. This allows to take into account the cases
where integrating z back to the task-speciﬁc features xt would not help.

Another motivation for using an identity skip connection around the global
feature map z comes from the fact that depth inﬂuences the relevance of each
task towards another. Some task-speciﬁc CNNs can beneﬁt more when they
share their low-level features than their high-level features, while other beneﬁt
more in the other way. For instance, tasks such as landmark localization and
pose detection proﬁt more from low-level features containing better localization

6

Multi-Task Learning by Deep Collaboration

Fig. 2. Deep Collaboration Network (DCNet) using ResNet18 as underlying network
in a MTL setting on the MTFL dataset. The top part shows the block structure of
ResNet18 interleaved with our proposed collaborative block, while the bottom part
details each residual and task-speciﬁc FC blocks.

properties, while tasks such as face detection and gender recognition proﬁt more
from class-speciﬁc high-level features. Considering that CNNs learn a hierarchy
of increasingly abstract features, our collaborative block can take into account
task relevance by deactivating a diﬀerent set of residual mappings Ft based on
the depth at which it is inserted. An example of such specialization will be shown
in our ablative study in Section 4.4.

Fig. 2 presents an example of inserting our collaborative block at diﬀerent
depths in a MTL framework on the MTFL dataset [15]. In this particular case,
we opted for a ResNet18 as underlying network. We refer to this network as our
Deep Collaboration Network (DCNet). As we can see in the top part of the ﬁgure,
integrating our approach comes down to interleaving the underlying network
block structure with our collaborative block. Each collaborative block receives
as input the output of each task-speciﬁc block, processes them as detailed in
Eq. (1), and sends the result back to each task-speciﬁc network. Adding our
approach to any underlying network can be done by simply following the same
pattern of interleaving the network block structure with our collaborative block.

4 Experiments

In this section, we detail our Multi-Task Learning (MTL) training framework and
present our experiments in Facial Landmark Detection (FLD) tasks. We further
evaluate the eﬀect of data scarcity on performance and illustrate an example of
knowledge sharing between task-speciﬁc CNNs with an ablation study.

Trottier et al.

7

4.1 Multi-Task Learning Training Framework

The goal of Facial Landmark Detection (FLD) is to predict the (x, y)-position
of speciﬁc landmarks associated with key features of the visage. While the num-
ber and type of landmarks are speciﬁc to each dataset, examples of standard
landmarks to be predicted are the corners of the mouth, the tip of the nose
and the center of the eyes. In addition to the facial landmarks, each dataset
further deﬁnes a number of related tasks. These related tasks also vary from
one dataset to another, and are typically gender recognition, smile recognition,
glasses recognition or face orientation.

On a more technical level, we deﬁne a learning framework in which we treat
each task as a classiﬁcation problem. While this is straightforward for gender,
smile and glasses recognition as they are already classiﬁcation tasks, it is a
bit more tricky for face orientation and FLD. For face orientation, instead of
predicting the roll, yaw and pitch real value as in a regression problem, we
divide each component into 30 degrees wide bins and predict the label of the bin
corresponding to the value. Similarly for FLD, rather than predicting the real
(x, y)-position of each landmark, we divide the image into 1 pixel wide bins and
predict the label of the bin corresponding to the value. Note that we still use
the original real values when comparing our prediction with the ground truth,
so that we incorporate our approximation errors in the ﬁnal score.

We report our results using the landmark failure rate metric [15], which is
deﬁned as follows: we ﬁrst compute the mean distance between the predicted
landmarks and the ground truth landmarks, then normalize it by the inter-
ocular distance from the center of the eyes. A normalized mean distance greater
than 10% is reported as a failure.

4.2 Facial Landmark Detection on the MTFL Task

As a ﬁrst experiment, we performed facial landmark detection on the Multi-Task
Facial Landmark (MTFL) task [15]. The dataset contains 12,995 face images an-
notated with ﬁve facial landmarks and four related attributes of gender, smiling,
wearing glasses and face proﬁle (ﬁve proﬁles in total). The training set has 10,000
images, while the test set has 2,995 images. We perform four sets of experiments
using an ImageNet pre-trained AlexNet, an ImageNet pre-trained ResNet18, an
un-pretrained AlexNet and an un-pretrained ResNet18 as underlying networks.
For AlexNet, we apply our collaborative block after each max pooling layer,
while for ResNet18, we do as shown in Fig. 2.

We compare our approach to several other approaches of the literature. We
include single-task learning (AN-S when using AlexNet as underlying network,
RN-S when using ResNet18), hard-parameter sharing MTL (AN and RN), hard-
parameter sharing MTL where the central section is widened to match the num-
ber of parameters of our approach (ANx and RNx), HyperFace (HF) [14], Tasks-
Constrained Deep Convolutional Network (TCDCN) [15], Cross-Stitch (XS) [12]
and XS widen to match the number of parameters of our approach (XSx). Ex-
cept for TCDCN, we train each network ourselves three times for 300 epochs and

8

Multi-Task Learning by Deep Collaboration

Fig. 3. Landmark failure rates (%) on the MTFL task. The reported values are the
average over the last ﬁve epochs, averaged over three tries. The left plot presents our
results with AlexNet as the underlying network, while the right one with ResNet18.
AN-S and RN-S stand for single-task training, AN and RN for multi-task training with
a single central network, ANx and RNx for multi-task training with a single central
network widen to match the number of parameters of our approach, HF for HyperFace,
TCDCN for [15]’s approach and XS for Cross-Stitch. In each instance, the left column
(blue) is for un-pretrained networks, while the right column (green) is for pre-trained
networks. Our proposed approach obtains the lowest failure rates overall.

report landmark failure rates averaged over the last ﬁve epochs, further averaged
over the three tries.

Fig. 3 presents our FLD results on the MTFL dataset. The left part of the
ﬁgure corresponds to using AlexNet as underlying network, while the right one
corresponds to ResNet18. The top part reports the landmark failure rates, while
the bottom part reports the mean error. In each plot, the left bar (blue) is for
un-pretrained network, while the right bar (green) is for ImageNet pre-trained
network. In addition, Fig. 4 shows example predictions from DCNet with pre-
trained ResNet18 as underlying network. The ﬁrst two examples were reported as
successes, while the last two are failures. The ground truth elements are colored
in green, while our predictions are colored in blue. We also include the labels of
the related tasks: gender, smiling, wearing glasses and face proﬁle.

The results of Fig. 3 show that our proposed approach obtained the lowest
failure rates and mean error in each case. Indeed, our DCNet with un-pretrained
and pre-trained AlexNet as underlying network obtained 19.67% and 19.96%
failure rates respectively, and 14.95% and 13.52% with ResNet18. This is sig-
niﬁcantly lower than the other approaches to which we compare ourselves. For
instance, with AlexNet, HF had 27.75% and 27.32%, XS had 26.41% and 25.65%,

Trottier et al.

9

MTFL

AFLW

Fig. 4. Example predictions of our DCNet with pre-trained ResNet18 as underlying
network on the MTFL and AFLW task. For MTFL, the ﬁrst two examples are successes,
while last two are failure cases. For AFLW, the ﬁrst three examples are successes, while
the last one is a failure case. Elements in green correspond to ground truth, while those
in blue correspond to predictions. Facial landmarks are shown as small dots, and related
tasks labels are displayed on the side. As we can see, over-exposition and tilted face
proﬁle can have a large impact on the prediction quality.

TCDCN had 25.00%1, and XSx had 25.23%. With ResNet18, XS had 18.43% and
15.52% respectively, and XSx had 17.28. We obtained the highest improvements
when using AlexNet as the underlying network when comparing to XS. With un-
pretrained and pre-trained AlexNet, we obtained improvements of 6.74% and
5.69%, while we obtained 3.48% and 2.00% with ResNet18. Performing MTL
with our approach can thus improve performance over using other approaches
of the literature.

Another result that we can see from Fig. 3 is that our soft-parameter shar-
ing approach obtains higher performance than the hard-parameter sharing ap-
proaches with matching number of parameters. For instance, increasing the
number of parameters of hard-parameter sharing AlexNet lowers it error rate
from 28.02% (AN) to 26.88% (ANx), but our approach lowers it further to
19.67%. Similarly, increasing the number of parameters of hard-parameter shar-
ing ResNet18 lowers it error rate from 20.05% (RN) to 16.75% (RNx), but our
approach lowers it further to 14.95%. These results are interesting because they
show that while increasing the number of parameters is an eﬀortless avenue to
improve performance, it has limitations. Developing novel approaches to enhance
network connectivity in a soft-parameter sharing setting seems more rewarding.
This may help to motivate new eﬀorts in this avenue to further leverage domain-
information of related tasks.

1 Zhang et. al only provided results with pre-trained AlexNet [15]

10

Multi-Task Learning by Deep Collaboration

Table 1. Landmark failure rate results on the AFLW dataset using a pre-trained
ResNet18 as underlying network. The presented values are averaged over the last ﬁve
epochs, further averaged over three tries. The ﬁrst column is the train / test ratio,
and the subsequent ones are the networks: single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS). Our approach obtains the best perfor-
mance in all cases, except the ﬁrst one where we observe over-ﬁtting.

Train / Test Ratio

0.1 / 0.9
0.3 / 0.7
0.5 / 0.5
0.7 / 0.3
0.9 / 0.1

RN-S

57.39
31.84
23.41
21.47
13.03

Networks

RN

58.00
32.00
23.31
21.92
12.80

XS

73.06
36.24
26.02
22.37
13.51

Ours

60.64
29.73
20.77
18.50
10.82

4.3 Eﬀect of Data Scarcity on the AFLW Task

As second experiment, we evaluated the inﬂuence of the number of training
examples to simulate data scarcity on the Annotated Facial Landmarks in the
Wild (AFLW) task [39]. The dataset has 21,123 Flickr images, and each image
can contain more than one face. Instead of using the images as provided, we
process them using the available face bounding boxes. We extract all faces with
visible landmarks, which gives a total of 2,111 images. This dataset deﬁnes 21
facial landmarks and has 3 related tasks (gender, wearing glasses and face ori-
entation). For face orientation, we divide the roll, yaw and pitch into 30 degrees
wide bins (14 bins in total), and predict the label corresponding to each real
value.

Our experiment works as follows. With a pre-trained ResNet18 as underlying
network, we compare our approach to single-task ResNet18 (RN-S), multi-task
ResNet18 (RN) and Cross-Stitch network (XS) by training on a varying number
of images. We use ﬁve diﬀerent train / test ratios, starting with 0.1 / 0.9 up
to 0.9 / 0.1 by 0.2 increment. In other words, we train each approaches on the
ﬁrst r% of the available images and test on the other (1 − r)%, then repeat
for all the other train / test ratios. We use the same training framework as in
section 4.2. We train each network three times for 300 epochs, and report the
landmark failure rate averaged over the last ﬁve epochs, further averaged over
the three tries. Example predictions are shown in Fig. 4.

As we can see in Table 1, our approach obtained the best performance in
all cases except the ﬁrst one. Indeed, we observe between 1.98% and 6.51%
improvements with train / test ratios from 0.3 / 0.7 to 0.9 / 0.1, while we obtain
a negative relative change of 3.25% with train / test ratio of 0.1 / 0.9. However,
since all multi-task approaches obtained higher failure rates than the single-task
approach, this suggests that the networks are over-ﬁtting the small training set.
Nonetheless, these results show that we can obtain better performance using our
approach.

Trottier et al.

11

Fig. 5. Landmark failure rate improvement (in %) of our approach compared to XS
when sampling random task weights. We used a pre-trained ResNet18 as underlying
network. The histogram at the left and the plot at the top right represents performance
improvement achieved by our proposed approach (positive value means lower failure
rates), while the plot at the bottom right corresponds to the log of the task weights. Our
approach outperformed XS in 86 out of the 100 tries, thus empirically demonstrating
that our learning framework was not unfavorable towards XS and that our approach
is less sensitive to the task weights λ.

One particularity that we observe in Table 1 is that the XS network has
relatively high failure rates. In the previous experiment of Section 4.2, XS had
either similar or better performance than the other approaches (except ours).
This could be due to our current multi-task learning framework that is unfavor-
able towards XS. In order to investigate whether this is the case, we perform
the following additional experiment. Using a pre-trained ResNet18 as underlying
network, we compare our approach to XS by training each network 100 times
using task weights randomly sampled from a log-uniform distribution. Speciﬁ-
cally, we ﬁrst sample from a uniform distribution γ ∼ U(log(1e−4), log(1)), then
use λ = exp(γ) as the weight. We trained both XS and our approach for 300
epochs with the same task weights using a train / test ratio of 0.5 / 0.5.

Figure 5 presents the results of this experiment. The plot at the top right
of the ﬁgure represents the landmark failure rate improvement (in %) of our
approach compared to XS, while the plot at the bottom right corresponds to the
log of the task weights for each try. In 86 out of the 100 tries, our approach had
a positive failure rate improvement, that is, obtained lower failure rates than
XS. As we can see in the histogram at the left of Fig. 5, the improvement rate
is normally distributed around 2.80%, has a median improvement of 3.66% and
a maximum improvement of 9.83%. Even though we sampled at random the
weights of the related tasks, our approach outperforms XS in the majority of the
cases. Our learning framework was therefore not unfavorable toward XS.

4.4 Illustration of Knowledge Sharing With an Ablation Study

As third experiment, we perform an ablation study on the MTFL task [15] with
an un-pretrained ResNet18 as underlying network. The goal of this experiment

12

Multi-Task Learning by Deep Collaboration

Fig. 6. Results of our ablation study on the MTFL dataset with an un-pretrained
ResNet18 as underlying network. We remove each task-speciﬁc features from each re-
spective central aggregation layer and evaluate the eﬀect on landmark failure rate. The
rows represent the task-speciﬁc CNNs, while the columns correspond to the network
block structure. Blocks with a high saturated color were found to have a large impact
on failure rate. In particular, this ablative study shows that the inﬂuence of high-
level face proﬁle features is large within our proposed architecture. This corroborates
with the well-known fact that the location of facial landmarks is closely dependent on
the orientation of the face. This constitutes an empirical evidence of domain-speciﬁc
information sharing via our approach.

is to verify that our collaborative block eﬀectively enables knowledge sharing
between task-speciﬁc CNNs. To do so, we evaluate the impact, on facial landmark
detection, of removing the contribution of each task-speciﬁc features. We zero out
the designated feature map xt before concatenation at the input of the central
aggregation H. The network is trained using the same framework as explained
in Sec. 4.1, and the ablation study is performed at test time on the test set when
training is done.

Figure 6 presents the results of our ablation study. The rows represent each
task-speciﬁc CNN, while the columns correspond to the network block structure.
The blocks are ordered from left (input) to right (output), while the task-speciﬁc
networks are ordered from top (main task) to bottom (related tasks). The color
saturation indicates the inﬂuence of removing the task-speciﬁc feature maps from
the central aggregation at the corresponding depth. A high saturation reﬂects
high inﬂuence on failure rate, while a low saturation reﬂects low inﬂuence.

As ﬁrst result, removing features from the facial landmark detection network
signiﬁcantly increases landmark failure rate. For instance, we observe a negative
(worse) relative change of 29.72% and 47.00% in failure rate by removing fea-
tures from Block 3 and Block 2 respectively. This illustrates that the main-task
network both contributed to and fed from the global features computed by the
central aggregation H. The CNN for landmark detection had the possibility to
remove the contribution of the global features, and so isolate itself from the other
CNNs, but the opposite occurred. We actually observe a mutual inﬂuence be-
tween the CNNs, where the task-speciﬁc features from the facial landmark CNN

Trottier et al.

13

inﬂuence the quality of the global features, which in turn inﬂuence the quality
of the subsequent task-speciﬁc features.

Another result that we can see from Fig. 6 is that Block 5 of task Proﬁle
has the highest inﬂuence on failure rate. We observe a negative relative change
of 83.87% by removing the features maps of task Proﬁle from the central ag-
gregation. What is particularly interesting in this case is that we observe this
high relative change at Block 5, which corresponds to the highest block in the
network. Since the block lies at the top of the network, it outputs features with
a high level of abstraction. We therefore expect that these features represent
high-level factors of variation corresponding to face orientation, which should
look like a rotation matrix. It therefore makes sense that features representing
the orientation of the face would be useful to predict facial landmarks, since
we know that the location of the facial landmarks is closely dependent on the
orientation of the face. The landmark CNN can use these rich features to bet-
ter rotate the predicted facial landmarks. This is indeed what we observe in
Fig. 6. These results constitute an empirical evidence that our approach allows
leveraging domain-speciﬁc information from related tasks.

4.5 Facial Landmark Detection With MTCNN

As ﬁnal experiment, we performed an experimental evaluation using the recent
Multi-Task Cascaded Convolutional Network (MTCNN) [27]. The authors pro-
posed a cascade structure of three stages, where each stage is composed of a
multi-task CNN. MTCNN performs predictions in a coarse-to-ﬁne manner. The
CNN of the ﬁrst stage generates (in a fully-convolutional way) many hypothe-
ses about the position of the face and the facial landmarks, and the subsequent
second and third stages reﬁnes them. The CNNs are trained sequentially with
hard-negative mining, in a hard-parameter sharing setting.

We implemented our approach in the available code project [40] and com-
pared ourself to MTCNN. We followed the provided hard negative mining recipe
and generated our images. For landmark detection, we used the LFWNet [26]
and CelebA [41] datasets, and generated 600k face images with facial landmarks.
For face detection, we used the WIDER [42] dataset, and generated 1.5M face
images with a bounding box. We trained a MTCNN with the stage networks
connected with our collaborative block, and a standard MTCNN with widened
stage networks to match the number of parameters.

On the test set of MTFL [15] dataset, standard MTCNN obtained a land-
mark failure rate of 37.85%, a mean error of 0.0996 and failed to detect a face
112 times, while our approach obtained better performances with 28.97%, 0.0930
and 79 respectively. Note that the reason MTCNN obtains worse performance
than our Deep Collaboration Network (DCNet), as reported in Fig. 3, is because
it has fewer parameters. DCNet has about 85M parameters, while the sum of
all three stage-CNN in MTCNN is about 2M. This is because MTCNN is care-
fully designed to balance computational speed and landmark detection precision.
It can predict many faces in high-dimensional images with a low computation
burden. An example of its prediction capability is shown in Fig. 7.

14

Multi-Task Learning by Deep Collaboration

Fig. 7. MTCNN predictions on the photo of the 2018 Oscar nominees (image resolution
of 2983 × 1197). The stage-CNNs are trained using our proposed collaborative block.
The coarse-to-ﬁne detection scheme employed by MTCNN allows predicting many faces
in high-dimensional images with low computational burden.

5 Conclusion and Future Work

In this paper, we proposed a novel soft-parameter knowledge sharing mechanism
based on lateral connections for Multi-Task Learning (MTL). Our proposed ap-
proach implements connectivity in term of a collaborative block, which uses two
distinct non-linear transformations. The ﬁrst one aggregates task-speciﬁc fea-
tures into global features, and the other merges back the global features into each
task-speciﬁc Convolutional Neural Network (CNN). Our collaborative block is
diﬀerentiable and can be dropped in any existing CNN architectures as a whole.
Our results on facial landmark detection tasks showed that networks connected
with our proposed collaborative block outperformed the other state-of-the-art
approaches, including the recent Cross-Stitch and MTCNN approach. We ver-
ify that our collaborative block eﬀectively enables knowledge sharing between
task-speciﬁc CNNs with an ablation study. We observed that the CNNs incor-
porated features with a varying level of abstraction from the other CNNs, by
observing the depth-speciﬁc inﬂuence of tasks that we know are related. These
results constituted an empirical evidence that our approach allows leveraging
domain-speciﬁc information from related tasks. Evaluating our proposed ap-
proach on other MTL problems could be an interesting avenue for future works.
For instance, the recurrent networks used to solve natural language processing
problems could beneﬁt from our approach.

Acknowledgements

We gratefully acknowledge the support of NVIDIA Corporation for providing a
Tesla Titan X for our experiments through their Hardware Grant Program.

Trottier et al.

15

References

1. Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classiﬁcation with deep convo-

lutional neural networks. In: NIPS. (2012) 1097–1105

2. Hochreiter, S.: The vanishing gradient problem during learning recurrent neural
nets and problem solutions. International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems 6(02) (1998) 107–116

A survey on multi-task learning.

arXiv preprint

3. Zhang, Y., Yang, Q.:
arXiv:1707.08114 (2017)

4. Caruana, R.: Multitask learning. In: Learning to learn. Springer (1998) 95–133
5. Zhang, M.L., Zhou, Z.H.: A review on multi-label learning algorithms. TKDE

6. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: KDD, ACM (2004)

26(8) (2014) 1819–1837

109–117

7. Pinto, L., Gupta, A.: Learning to push by grasping: Using multiple tasks for

eﬀective learning. In: ICRA, IEEE (2017) 2161–2168

8. Tian, Y., Luo, P., Wang, X., Tang, X.: Pedestrian detection aided by deep learning

semantic tasks. In: CVPR. (2015) 5079–5087

9. Liu, X., Gao, J., He, X., Deng, L., Duh, K., Wang, Y.Y.: Representation learning
using multi-task deep neural networks for semantic classiﬁcation and information
retrieval. In: HLT-NAACL. (2015) 912–921

10. Yim, J., Jung, H., Yoo, B., Choi, C., Park, D., Kim, J.: Rotating your face using

multi-task deep neural network. In: CVPR. (2015) 676–684

11. Yin, X., Liu, X.: Multi-task convolutional neural network for face recognition.

arXiv preprint arXiv:1702.04710 (2017)

12. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-

task learning. In: CVPR. (2016) 3994–4003

13. Ruder, S.: An overview of multi-task learning in deep neural networks. CoRR

abs/1706.05098 (2017)

14. Ranjan, R., Patel, V.M., Chellappa, R.: Hyperface: A deep multi-task learning
framework for face detection, landmark localization, pose estimation, and gender
recognition. arXiv preprint arXiv:1603.01249 (2016)

15. Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep multi-

task learning. In: ECCV, Springer (2014) 94–108

16. Liu, P., Qiu, X., Huang, X.: Adversarial multi-task learning for text classiﬁcation.

(2017)

17. Baxter, J.: A bayesian/information theoretic model of learning to learn via multiple

task sampling. Machine learning 28(1) (1997) 7–39

18. Duong, L., Cohn, T., Bird, S., Cook, P.: Low resource dependency parsing: Cross-
lingual parameter sharing in a neural network parser. In: ACL. Volume 2. (2015)
845–850

19. Yang, Y., Hospedales, T.M.: Trace norm regularised deep multi-task learning.

arXiv preprint arXiv:1606.04038 (2016)

20. Ruder, S., Bingel, J., Augenstein, I., Søgaard, A.: Sluice networks: Learning what
to share between loosely related tasks. arXiv preprint arXiv:1705.08142 (2017)
21. Long, M., Wang, J.: Learning multiple tasks with deep relationship networks.

arXiv preprint arXiv:1506.02117 (2015)

22. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv
preprint arXiv:1606.04671 (2016)

16

Multi-Task Learning by Deep Collaboration

23. Trottier, L.: Deep collaboration network in pytorch.

https://github.com/

ltrottier/deep-collaboration-network

24. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 448–456

25. Nair, V., Hinton, G.: Rectiﬁed linear units improve restricted boltzmann machines.

In: ICML. (2010) 807–814

26. Sun, Y., Wang, X., Tang, X.: Deep convolutional network cascade for facial point

detection. In: CVPR. (2013) 3476–3483

27. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using
multitask cascaded convolutional networks. IEEE Signal Processing Letters 23(10)
(2016) 1499–1503

28. Jourabloo, A., Liu, X.: Large-pose face alignment via cnn-based dense 3d model

ﬁtting. In: CVPR. (2016) 4188–4196

29. Baltrušaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial

behavior analysis toolkit. In: WACV, IEEE (2016) 1–10

30. Ding, C., Tao, D.: Robust face recognition via multimodal deep face representation.

IEEE Transactions on Multimedia 17(11) (2015) 2049–2058

31. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to

human-level performance in face veriﬁcation. In: CVPR. (2014) 1701–1708

32. Zhang, C., Zhang, Z.: Improving multiview face detection with multi-task deep

convolutional neural networks. In: WACV, IEEE (2014) 1036–1041

33. Fabian Benitez-Quiroz, C., Srinivasan, R., Martinez, A.M.: Emotionet: An accu-
rate, real-time algorithm for the automatic annotation of a million facial expres-
sions in the wild. In: CVPR. (2016) 5562–5570

34. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.

35. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: ECCV, Springer (2016) 630–645

In: CVPR. (2016) 770–778

36. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with

stochastic depth. In: ECCV, Springer (2016) 646–661

37. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations

for deep neural networks. In: CVPR, IEEE (2017) 5987–5995

38. Veit, A., Wilber, M.J., Belongie, S.: Residual networks behave like ensembles of

relatively shallow networks. In: NIPS. (2016) 550–558

39. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: A large-scale, real-world database for facial landmark localization. In:
ICCV Workshops, IEEE (2011) 2144–2151

40. Kim, K.K.: Deep learning face detection and recognition, implemented by pytorch.

https://github.com/kuaikuaikim/DFace

41. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:

42. Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark.

ICCV. (2015)

In: CVPR. (2016) 5525–5533

