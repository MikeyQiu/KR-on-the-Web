Spelling Error Correction Using a Nested RNN Model and Pseudo
Training Data

Hao Li1 and Yang Wang1 and Xinyu Liu2 and Zhichao Sheng1 and Si Wei1
{haoli5,yangwang2,zcsheng,siwei}@iﬂytek.com
liuxy.zoe2016@outlook.com

1iFLYTEK Research, Hefei, China
2International Department, The Afﬂiated School of SCNU, Guangzhou, China

Abstract

We propose a nested recurrent neural
network (nested RNN) model for En-
glish spelling error correction and generate
pseudo data based on phonetic similarity
to train it. The model fuses orthographic
information and context as a whole and
is trained in an end-to-end fashion. This
avoids feature engineering and does not
rely on a noisy channel model as in tra-
ditional methods. Experiments show that
the proposed method is superior to existing
systems in correcting spelling errors.

1

Introduction

Spelling error correction (Kukich, 1992; Jurafsky
and Martin, 2000) is a basic issue in grammati-
cal error correction (GEC) and is widely used in
practical scenarios (Wilbur et al., 2006). Most
traditional systems in this ﬁeld are based on Leven-
shtein Distance and statistical methods (Damerau,
1964; Kashyap and Oommen, 1983; Mays et al.,
1990; Kemighan et al., 1990; Toutanova and Moore,
2002; O’Hearn et al., 2008). Despite being success-
ful in GEC, neural network models have not drawn
much attention in correcting spelling errors.

Spelling errors are often divided into two cate-
gories: non-word errors and real-word errors (Ju-
rafsky and Martin, 2000). Non-word spelling errors
can be detected via a dictionary. Any word that is
not in a dictionary is an error. Real word spelling
errors are much more difﬁcult to identify since
the misspelled words are in the vocabulary (Mays
et al., 1990; O’Hearn et al., 2008).

Most traditional systems for spelling error cor-
rection are based on the noisy channel model,
where a true word is distorted as if passed through
a noisy communication channel (Kemighan et al.,
1990; Brill and C. Moore, 2001).

Recently, GEC becomes an active ﬁeld (Ng
et al., 2014) and data-driven methods are proposed
against spelling errors. Chollampatt and Ng (2017)
proposed character-level statistical machine trans-
lation to “translate” unknown words into correct
words. Sakaguchi et al. (2017a) proposed scRNN
based on psycholinguistics experiments. In many
neural network based GEC systems, words are split
into character level or BPE level to avoid OOV
words (Schmaltz et al., 2016; Ji et al., 2017; Chol-
lampatt and Tou Ng, 2018). These models consider
spelling errors and grammatical errors as a whole.
But their performance on spelling error correction
is limited due to a lack of training data in GEC
(Dahlmeier et al., 2013).

In this paper, we propose a nested RNN model
for spelling error correction and train it with pseudo
data. The contributions are listed as follows.

• We propose a stand-alone model for spelling
error correction, where both orthographic in-
formation and information are considered.

• We generate pseudo data based on phonetic

similarity to train the model.

2 Related work

Traditional spelling correction methods mostly de-
pends on the noisy channel model (Kemighan et al.,
1990; Church and Gale, 1991). Brill and C. Moore
(2001) improves it by splitting a word into parti-
tions. Toutanova and Moore (2002) incorporates
pronunciation with a letter-to-sound model.

Language models are also used to detect seman-
tic anomaly based on the context, especially for
dealing with real-word errors (Mays et al., 1990;
O’Hearn et al., 2008). But the probabilities from
noisy channel and language model are often not
commensurate. Besides, Word-Net are also used to
detect potential corrections (Hirst and Budanitsky,
2005).

8
1
0
2
 
v
o
N
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
8
3
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Neural networks are also adapted. Sakaguchi
et al. (2017a) proposed the scRNN model by ig-
noring the orders of internal characters of a word.
But the model actually lost much orthographic in-
formation. On the other hand, in many GEC sys-
tems, OOV words are split in character-based or
BPE-based units and fed into sequence-to-sequence
models (Xie et al., 2016; Schmaltz et al., 2016;
Ji et al., 2017; Chollampatt and Tou Ng, 2018).
By doing this, the task of spelling error correction
is fused into GEC and the two tasks are trained
end-to-end together. However, in the ﬁeld of GEC,
these systems suffer from a lack of labeled data
(Dahlmeier et al., 2013) and are not trained enough
against spelling errors. Hence, other GEC systems
would rather use traditional stand-alone methods,
such as enchant1, to correct the spelling errors be-
forehand (Sakaguchi et al., 2017b; Napoles and
Callison-Burch, 2017).

Compared to the methods above, the nested
RNN model is novel. First, it combines ortho-
graphic information and context and is trained in
an end-to-end fashion, avoiding parameter tuning.
Second, it can be trained with large-scale pseudo
data, without using human-generated resources.

3.1 CharRNN

Given a word w, we map its character sequence
(c1, c2, . . . , cL) into a series of character embed-
dings e:

e = (e1, e2, . . . , eL)
The CharRNN then encodes e into a sequence of
hidden vectors (s1, s2, . . . , sL). where the lth hid-
den state sl is computed as:

(1)

sl = GRUc(sl−1, el)

(2)

and GRU is the gated recurrent unit (Cho et al.,
2014), a modiﬁed version of the vanilla RNN.

We take the last hidden vector sL, denoted as
¯s, as a representation of w. Thus, the CharRNN
encodes a sentence with words (w1, w2, . . . , wn)
into a sequence of vectors ¯s:

¯s = (¯s1, ¯s2, . . . , ¯sN )

(3)

3.2 WordRNN

For the sentence above, the WordRNN takes ¯s as
input and creates a corresponding sequence of vec-
tors h:

h = (h1, h2, . . . , hN )

The nth hidden state hn is computed as

(4)

(5)

(6)

(7)

hn = [hf
n; hb
n]
w(hf
n = GRUf
hf
hb
w(hb
n = GRUb
w and GRUb

n−1, ¯sn)
n+1, ¯sn)

3

the Nested RNN model

We convert spelling error correction into a sequence
labeling problem, where a correct word is labeled
as itself and a misspelled word is labeled as its cor-
rected version. The model is shown in Fig 1. It con-
sists of a char-level RNN (CharRNN) and a word-
level one (WordRNN). The CharRNN collects or-
thographic information by reading each word as
a sequence of letters. The WordRNN predicts the
correct words by combining the orthographic infor-
mation with the context.

where GRUf
backward word-level GRU cell, respectively.

w denote the forward and

Then the probability of each target word yn is

computed as:

p(yn|e) = softmax(g(hn))

(8)

where g is a linear function that maps the hidden
state into a vector of target vocabulary size.

The model is trained by minimizing the cross-

entropy loss, which for a given (x, y) pair is

loss(x, y) = −

log p(yn|x)

(9)

N
(cid:88)

n=1

3.3 Processing unknown words

The nested RNN model has a limited output vo-
cabulary size. The out-of-vocabulary tokens in the
output are represented by a special UNK symbol.
Thus post-processing on the UNKs is needed for
inference. Noticing that for our spelling correction
model, the input and output words are matched
one-by-one, we simply replace all the UNKs in the
output with the corresponding source word.

Figure 1: Architecture of the Nested RNN Model

1https://github.com/AbiWord/enchant

4 Training with pseudo data

where Dp is the phonetic alphabet.

To train the nested RNN model, large quantity of
labeled data is needed, which is not available to our
knowledge. So we propose to train it with pseudo
generated data.

Words are often spelled in the way they sound
like, which means, a word is more easily mis-
spelled to those with similar pronunciation, e.g.
understand → understend, decision → dicision.
This property has been used for improving the noisy
channel model (Toutanova and Moore, 2002).

Inspired by this, we propose a simple method
to generate pseudo training data for spelling error
correction based on phonetic similarity.

4.1 Character-level confusion matrix

First, we collect words from a dictionary D and
train an attention-based NMT (Bahdanau et al.,
2015) model from word to pronunciation, which,
for a single word, takes its sequence of characters
as input and the corresponding sequence of pho-
netic symbols as output. Then for each word w in
D , we calculate the normalized attention weights
between phonetic symbols p and characters c:

Att(w, cl, pk), w ∈ D, 1 ≤ l ≤ L, 1 ≤ k ≤ K

(10)
where L and K are the number of characters and
phonetic symbols in word w.

We consider c and p to be “aligned” in w if
Att(w, c, p) exceeds some threshold θatt. If two
or more adjacent characters are aligned by a same
phonetic symbol, they will be merged to form a
new “character”. So in this chapter, the word “char-
acter” may refer to a real character or a sequence
of characters. For example, from the word beam
(/bi:m/), we may get the alignment pairs (b, b), (ea,
i:), (m, m).

Then for each character c and phonetic symbol p
in the alphabet, we count the number of times they
are aligned in the dictionary:
N (c, p) = (cid:80)

w∈D align(w, c, p)

(11)

where

align(w, c, p) =

(cid:26) 1, Att(w, c, p) > θatt

0, otherwise

(12)
For ordered character pair (c1, c2), deﬁne the pho-
netic confusion coefﬁcient as:

M (c1, c2) =

(cid:80)

p N (c1, p) · N (c2, p)

(cid:80)

(cid:80)

ˆc∈Dp

p N (c1, p) · N (ˆc, p)

Character pairs with large confusion coefﬁcients
tend to sound similarly, and therefore are more
likely to be confused in spelling.

4.2 Pseudo data for training

Next we begin to make spelling perturbation to
words. Given a word w, we randomly select a
source character c from word w and replace it with
a target character ˜c drawn from the following the
distribution:

psubstitute(c → ˜c) = M (c, ˜c)

(14)

When ˜c is a sequence of character that contains
c, an insertion error is made, and vice versa for
a deletion error. Hence, insertions, deletions are
substitutions are all included in the process.

For a large number of correct documents, we ran-
domly add the spelling errors to generate pseudo
training data and train the nested RNN model by
minimizing the cross-entropy loss with the per-
turbed sentences as inputs and the original sen-
tences as outputs.

5 Experiments

5.1 Training Data

We use words from a public dictionary and train
the character-level translations model from words
to phonetic symbols. After the model converges,
we obtain about 1000 character pairs with non-
zero confusion coefﬁcient by setting θatt = 0.2.
The One Billion Word Benchmark corpus (Chelba
et al., 2013) is used to generate pseudo data. We
make pseudo errors on words with a probability
of Perr = 0.05, which is selected according to
performance on development set.

5.2 Evaluation and baselines

Brill and C. Moore (2001) used a 10,000-word
corpus of common English spelling errors in their
paper, but the dataset is not available to us. Other
datasets in this ﬁeld, such as those proposed by
Roger Mitton 2 and Peter Norvig3, only consider
isolated errors and contain no context information.
Therefore, we build a new one based on JFLEG
(Napoles et al., 2017), a ﬂuency-oriented corpus
for developing and evaluating GEC. One annotator
corrects the spelling errors of the source sentences

(13)

2http://www.dcs.bbk.ac.uk/ ROGER/aspell.dat
3http://norvig.com/ngrams/spell-errors.txt

System

enchant
scRNN (Sakaguchi et al., 2017a)
LSTM-Char-CNN (Kim et al., 2016)
nested RNN

Development
R
53.47
50.46
55.32
56.94

F0.5
50.22
59.76
64.25
67.73

P
49.46
62.64
66.95
71.10

P
53.40
66.40
66.58
71.77

Test
R
58.33
56.08
58.33
61.26

F0.5
54.32
64.04
64.75
69.39

Table 1: Performance of spelling error correction on the JFLEG development and test set.

in JFLEG and leave other parts unchanged. 480 and
432 spelling errors are annotated for the develop
set and test set, respectively. Following Ng et al.
(2014), the annotations are made in .m2 format and
the F0.5 score computed by MaxMatch (M2) scorer
(Dahlmeier and Ng, 2011) is used as evaluation
metric.

We evaluate our model in comparison to PyEn-
chant4, scRNN (Sakaguchi et al., 2017a) and a
modiﬁed LSTM-Char-CNN (Kim et al., 2016).

The LSTM-Char-CNN was originally proposed
for language modeling (Kim et al., 2016) and later
fused in a GEC model (Schmaltz et al., 2016). We
modify it for spelling error correction by letting it
predict the current corrected word instead of the
next word in language modeling task. We reimple-
ment LSTM-Char-CNN and scRNN and train them
with the same pseudo data.

5.3 Training details and results

For the nested RNN, the character embedding size
is set to 64. The hidden unit sizes of CharRNN
and WordRNN are 256 and 512. The CharCNN in
LSTM-Char-CNN has 256 ﬁlters, 64 for each of
the width [1,2,3,4]. The character vocabulary size
and word vocabulary sizes are 84 and 30k for the 3
neural network models.

We train the models using Adam (Kingma and
Ba, 2014) for 50k steps with lr = 0.001, β1 =
0.9, β2 = 0.999, (cid:15) = 10−8 and then switch to
vanilla SGD with lr = 0.2 and halve the learn-
ing rate every 50k steps. We train a total of 200k
steps with mini-batchsize of 96, which takes 20
hours on a single Tesla K40m GPU for the nested
RNN. We evaluate the models on development set
every 10k steps and choose the best checkpoints
for test.

Table 1 shows the performance of the models on
the development and test set. As seen, all the neural
network methods outperform enchant by a large
margin and the proposed nested RNN substantially
improves upon other methods.

I though that was right .
source
I thought that was right .
reference
I though that was right .
enchant
I though that was right .
scRNN
LSTM-Char-CNN I thought that was right .
I thought that was right .
nested RNN
No matter they are smell or big .
source
No matter they are small or big .
reference
No matter they are smell or big .
enchant
No matter they are small or big .
scRNN
LSTM-Char-CNN No matter they are smell or big .
No matter they are small or big .
nested RNN

Table 2: Examples of real-word errors.

6 Analysis

Table 2 shows two examples of real-word errors
where the nested RNN outperforms other models.
The enchant detects errors mostly based on dictio-
nary looking up, and doesn’t perform well in these
cases. The scRNN ignores the order of internal
characters and relies heavily on the ﬁrst and last
characters to recognize a word, causing its failure
to correct the word “though” to “thought” since the
last character would change.

The last point we would emphasize is that the
LSTM-Char-CNN resembles the nested RNN ex-
cept that it uses a CharCNN to represent a word
instead of a CharRNN. This difference is crucial in
that CNN is “rigid” in ﬁnding insertions and dele-
tions errors. Here we give a intuitive explanation.
For example, given a character sequence pattern
[abcd], the corresponding 4-gram kernel matching
it is also [abcd]. If we delete the character “b”, the
source pattern becomes [acd], where the longest
sequence that can be matched is [cd], whose length
is only 1/2 of the original one. In contrast, for a
CharRNN, 3/4 of the pattern remains unchanged
in this condition since it reads characters sequen-
tially. Hence, the nest RNN is more suitable for
correcting insertion errors and deletion errors than
LSTM-Char-CNN.

References

4https://github.com/rfk/pyenchant

Dzmitry Bahdanau, Kyunghyun Cho, and Y Bengio.
2015. Neural machine translation by jointly learn-

ing to align and translate. In ICLR. volume 1409.

Eric Brill and Robert C. Moore. 2001. An improved er-
ror model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, and Phillipp Koehn. 2013. One bil-
lion word benchmark for measuring progress in sta-
In Proceedings of the
tistical language modeling.
Annual Conference of the International Speech Com-
munication Association, INTERSPEECH.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, Doha, Qatar, pages
1724–1734. http://www.aclweb.org/anthology/D14-
1179.

Shamil Chollampatt and Hwee Tou Ng. 2017. Con-
necting the dots: Towards human-level grammatical
In Proceedings of the 12th Work-
error correction.
shop on Innovative Use of NLP for Building Educa-
tional Applications. Association for Computational
Linguistics, Copenhagen, Denmark, pages 327–333.
http://www.aclweb.org/anthology/W17-5037.

Shamil Chollampatt and Hwee Tou Ng. 2018. A multi-
layer convolutional encoder-decoder neural network
for grammatical error correction. In Proceedings of
AAAI.

Kenneth W. Church

and William A. Gale.
Probability scoring for spelling correc-
Statistics and Computing 1(2):93–103.

1991.
tion.
https://doi.org/10.1007/BF01889984.

Daniel Dahlmeier and Hwee Tou Ng. 2011. Cor-
recting semantic collocation errors with l1-induced
In Proceedings of the 2011 Confer-
paraphrases.
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Edinburgh, Scotland, UK., pages 107–117.
http://www.aclweb.org/anthology/D11-1010.

Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei
Wu. 2013.
Building a large annotated corpus
of learner english: The nus corpus of learner
the Eighth Work-
english.
shop on Innovative Use of NLP for Building Ed-
ucational Applications. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 22–31.
http://www.aclweb.org/anthology/W13-1703.

In Proceedings of

Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Communi-
cations of the ACM 7(3):171–176.

Graeme Hirst and Alexander Budanitsky. 2005. Bu-
danitsky, a.: Correcting real-word spelling errors by
restoring lexical cohesion. nat. lang. eng. 11(1), 87-
111. Natural Language Engineering 11:87–111.

Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen
Gong, Steven Truong, and Jianfeng Gao. 2017. A
nested attention neural hybrid model for grammati-
cal error correction. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, Vancouver, Canada,
pages 753–762.
http://aclweb.org/anthology/P17-
1070.

Dan Jurafsky and James Martin. 2000. Speech and
Language Processing. Prentice Hall, Upper Saddle
River, New Jersey.

R.L. Kashyap and B.J. Oommen. 1983. Spelling cor-
rection using probabilistic methods. Pattern Recog-
nition Letters 2:147–154.

Mark D. Kemighan, Kenneth W. Church,

and
William A. Gale. 1990. A spelling correction
In
program based on a noisy channel model.
Proceedings of COLING 1990,
the 13th Interna-
tional Conference on Computational Linguistics.
http://www.aclweb.org/anthology/C90-2036.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan
der M. Rush. 2016. Character-aware neural lan-
guage models. In Proceedings of AAAI.

Diederik Kingma and Jimmy Ba. 2014. Adam: A

method for stochastic optimization. In ICLR.

Karen Kukich. 1992. Techniques for automatically cor-

recting words in text 24:377–439.

Eric Mays, Fred J. Damerau, and Robert Mercer. 1990.
Context based spelling correction. Information Pro-
cessing and Management 27:517–522.

adapting machine

Courtney Napoles and Chris Callison-Burch. 2017.
translation
Systematically
In Proceed-
for grammatical error correction.
ings of
the 12th Workshop on Innovative Use
of NLP for Building Educational Applica-
tions. Association for Computational Linguis-
tics, Copenhagen, Denmark,
pages 345–356.
http://www.aclweb.org/anthology/W17-5039.

Courtney Napoles, Keisuke Sakaguchi, and Joel
Tetreault. 2017. Jﬂeg: A ﬂuency corpus and bench-
mark for grammatical error correction. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers. Association for Computa-
tional Linguistics, Valencia, Spain, pages 229–234.
http://www.aclweb.org/anthology/E17-2037.

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Chris-
tian Hadiwinoto, Raymond Hendy Susanto,
The conll-2014
and Christopher Bryant. 2014.
correction.
shared task on grammatical

error

In Proceedings of
the Eighteenth Conference
on Computational Natural Language Learning:
Shared Task. Association
for Computational
Linguistics, Baltimore, Maryland, pages 1–14.
http://www.aclweb.org/anthology/W14-1701.

L O’Hearn, Graeme Hirst, and Alexander Budanitsky.
2008. Real-word spelling correction with trigrams:
A reconsideration of the mays, damerau, and mer-
In International conference on intelli-
cer model.
gent text processing and computational linguistics.
Springer, Berlin, Heidelberg, pages 605–616.

Keisuke Sakaguchi, Kevin Duh, Matt Post, and Ben-
jamin Van Durme. 2017a. Robsut wrod reocginiton
via semi-character recurrent neural network. In Pro-
ceedings of AAAI.

Keisuke Sakaguchi, Matt Post,

and Benjamin
Van Durme. 2017b. Grammatical error correction
with neural reinforcement learning. In Proceedings
of the Eighth International Joint Conference on
Natural Language Processing (Volume 2: Short
Papers). Asian Federation of Natural Language
pages 366–372.
Processing, Taipei, Taiwan,
http://www.aclweb.org/anthology/I17-2062.

Allen Schmaltz, Yoon Kim, Alexander M. Rush,
and Stuart Shieber. 2016.
Sentence-level gram-
matical error identiﬁcation as sequence-to-sequence
the 11th Work-
correction.
shop on Innovative Use of NLP for Building Ed-
ucational Applications. Association for Computa-
tional Linguistics, San Diego, CA, pages 242–251.
http://www.aclweb.org/anthology/W16-0528.

In Proceedings of

Kristina Toutanova and Bob Moore. 2002. Pronuncia-
tion modeling for improved spelling correction. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics.

W Wilbur, Won Kim, and Natalie Xie. 2006. Spelling
correction in the pubmed search engine 9:543–564.

Z. Xie, Avati A., Arivazhagan N., D. Jurafsky, and
A. Y. Ng. 2016. Neural language correctionwith
arXiv preprint arXiv:
character-based attention.
1603.09727 .

