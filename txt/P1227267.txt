High-performance stock index trading: making eﬀective use of a deep long
short-term memory network

Chariton Chalvatzisa,∗, Dimitrios Hristu-Varsakelis1,a

aUniversity of Macedonia, Department of Applied Informatics, Egnatia 156, Thessaloniki, 54006, Greece

9
1
0
2
 
y
a
M
 
8
 
 
]
T
S
.
n
i
f
-
q
[
 
 
2
v
5
2
1
3
0
.
2
0
9
1
:
v
i
X
r
a

Abstract

We present a deep long short-term memory (LSTM)-based neural network for predicting asset prices, to-

gether with a successful trading strategy for generating proﬁts based on the model’s predictions. Our work

is motivated by the fact that the eﬀectiveness of any prediction model is inherently coupled to the trading

strategy it is used with, and vise versa. This highlights the diﬃculty in developing models and strategies

which are jointly optimal, but also points to avenues of investigation which are broader than prevailing ap-

proaches. Our LSTM model is structurally simple and generates predictions based on price observations

over a modest number of past trading days. The model’s architecture is tuned to promote proﬁtability, as

opposed to accuracy, under a strategy that does not trade simply based on whether the price is predicted

to rise or fall, but rather takes advantage of the distribution of predicted returns, and the fact that a pre-

diction’s position within that distribution carries useful information about the expected proﬁtability of a

trade. The proposed model and trading strategy were tested on the S&P 500, Dow Jones Industrial Average

(DJIA), NASDAQ and Russel 2000 stock indices, and achieved cumulative returns of 340%, 185%, 371%

and 360%, respectively, over 2010-2018, far outperforming the benchmark buy-and-hold strategy as well as

other recent eﬀorts.
Keywords: Finance, LSTM, Deep Learning, Stock Prediction, Automatic trading

1. Introduction

Equity prediction lies at the core of the investment management profession and has also attracted signif-

icant attention from academia. One of the open questions has been whether (and how) one can forecast the

behavior of stocks and then act accordingly to generate “excess returns”, i.e., proﬁt in excess of those gen-

erated by the market itself (Fama and French, 1993). Towards that end, signiﬁcant eﬀort has been put into

∗Corresponding author
Email addresses: charis.chalvatzis@uom.edu.gr (Chariton Chalvatzis), dcv@uom.gr (Dimitrios Hristu-Varsakelis)

Preprint submitted to Decision Support Systems

May 9, 2019

predicting the price of major U.S. stock indices such as the S&P 500 or the Dow Jones Industrial Average

(DJIA) (Huck, 2009, 2010; Sethi et al., 2014; Krauss et al., 2017; Bao et al., 2017) as well as that of individ-

ual stocks, using techniques ranging from early linear models (Fama and French, 2004) to machine learning

and neural network-based approaches (Sermpinis et al., 2013; Deng et al., 2017; Minh et al., 2018). There

are two components which are generally part of the overall discussion on “intelligent” or automatic trading:

a predictive model whose purpose is to anticipate an asset’s future price, and a trading strategy which uses

the model’s predictions to generate proﬁt. Developing a scheme with consistently superior performance is

diﬃcult, in part because of the “joint” nature of the goal: proﬁts ultimately depend both on the prediction

model as well as the trading strategy it is being used with, and changing either of the two aﬀects the ﬁnal

outcome. Thus, in principle, these two components should be designed and optimized together, a challenge

which currently appears to be out of reach, due to the seemingly endless variety of possible models and

trading strategies. Instead, most of the relevant research (to be reviewed shortly) has focused on schemes

that do quite well by improving on speciﬁc aspects of the overall prediction/trading process.

One approach has been to aim for a prediction model which is “as good as possible” at guessing the

“next” price of an asset, and then use that guess to inform trading decisions (Bao et al., 2017; Baek and

Kim, 2018; Zhou et al., 2019). This is frequently done by means of a so-called directional “up-down”

trading strategy which buys the asset when the predicted price is higher than the current price and sells if it

is lower. While this approach can be eﬀective, it is based on an implicit assumption that better short-term

price predictions lead to higher proﬁtability. This is not true in general: prediction accuracy is typically

measured with a symmetric loss function (e.g., mean absolute error) which is indiﬀerent to the sign of the

prediction error, and yet that sign may be quite important when trading. In fact, it is possible for one model

to lead to trades which are always proﬁtable while another, having better predictive accuracy, to record only

losses1. An alternative is to forgo predictions of the asset price itself and instead consider a directional

model which predicts, with as high an accuracy as possible, whether the price will rise or fall compared to

its current level, essentially acting as a binary classiﬁer (Fischer and Krauss, 2018; Zhong and Enke, 2017;

Chiang et al., 2016). Then, the trading strategy is again to simply buy or sell the asset based on the model’s

1Consider for example the four-day price series Y = [101, 100, 98, 101], and two possible predictions of its last three samples,
P1 = [95, 92, 105] and P2 = [102, 101, 97], both generated on the ﬁrst day, when the price was 101. P1 has a mean absolute error
of 5 when measured against the last three samples of Y, while P2’s error is 3. However, P1 is always correct on the direction of
the price movement of Y, i.e., its predictions are always higher (resp.lower) than the previous price of Y when the price will indeed

rise (resp. fall) in the next sample, while P2 is always wrong.

2

“recommendation”. This approach is “self-consistent” from the point of view that the model’s predictions

are judged against in the same setting as the trading strategy, i.e., being correct on the direction of price

movement.

While one can of course consider many possible variations on trading strategies and accompanying

models, there are two key points to be recognized. First, prediction models should be viewed and evaluated

in the context of the trading strategies they are used with (Leitch and Tanner, 1991). Thus, prediction

accuracy should not monopolize our focus: large(r) prediction errors do not necessarily preclude a model

from being proﬁtable under the “right” trading strategy, just like low errors do not by themselves guarantee

proﬁtability. Second, because the joint model/strategy optimum appears to be elusive, one may try to

improve overall proﬁtability by adapting the trading strategy to to make “fuller” use of the information

contained within the model’s predictions; that information may generally be much more than whether the

price will rise or fall in the next time step.

This paper’s contribution is to build on the ideas outlined in the previous paragraph by proposing a

novel model-strategy pair which together are eﬀective in generating proﬁts with reasonable trading risk,

and compare favorably to standard benchmarks as well as recent works. Our model consists of a deep long

short-term memory (LSTM)-based neural network which is to predict an asset’s price based on a rolling

window of past price data. Structurally, the network will be simple so that it can be trained and updated

fast, making it suitable for intra-day trading, if desired. A key feature will be that the network’s output layer

will have access to the entire evolution of the network’s hidden states as the input layer is “exposed” to a

sequence of past prices. This will allow us to achieve a predictive accuracy similar to that of more complex

models, while keeping our architecture “small” in terms of structure and data required. Our LSTM network

will initially be trained to achieve low price prediction errors, as opposed to high proﬁtability which is our

ultimate goal. We will bring proﬁtability into the fold in two ways: a) the network’s hyper-parameters will

be selected to maximize proﬁt (instead of accuracy, as is typically done in the literature (Zhong and Enke,

2017; Zhou et al., 2019)) in conjunction with b) a novel event-based2 trading strategy which seeks to take

advantage of the information available not in any single price prediction, but in the entire distribution of

predicted returns. In broad terms, our strategy will not act simply on whether the predicted price is higher

or lower compared to its current value, but will instead make decisions based on the expected proﬁtability

of a possible trade, conditioned on the prediction’s relative position within the aforementioned distribution.

2We will use the term “event” in the sense of probability theory (e.g., a sample of a random variable falling within a speciﬁed

interval on the real line), as opposed to ﬁnance where it commonly refers to “external” events (corporate ﬁlings, legal actions, etc.).

3

As we will see, such a strategy can be optimized in some ways and will lead to signiﬁcant gains.

We will test our proposed approach on major U.S. stock indices, namely the S&P 500 (including div-

idends), the Dow Jones Industrial Average (DJIA), the NASDAQ and the Russel 2000 (R2000), over an

almost 8-year long period (October 2010 - May 2018). These indices attract signiﬁcant attention from mar-

ket practitioners, both in terms of strategy benchmarking and for monitoring market performance. They

will also serve as a common ground for comparison with other studies. When it comes to discussing the

performance of our approach, we will provide the “standard” annualized and cumulative proﬁts attained,

but also additional metrics that relate to trading risk, which are very much of interest when assessing any

investment but are often missing from similar studies. Moreover, our relatively long testing period will

allow us to draw safer conclusions as to our scheme’s proﬁtability over time.

The remainder of the paper is organized as follows. Section 2 discusses relevant literature, including

details on proﬁtability and overall performance. In Section 3 we present our proposed prediction model and

trading strategy, as well as a class of capital allocation policies which are optimal for our trading strategy.

Section 4 discusses model training and allocation policy selection. The proﬁtability and overall performance

of our model and strategy, as well as comparisons with recent works and “naive” benchmark approaches of

interest, are discussed in Section 5.

2. Literature Review

We can identify two signiﬁcant clusters in the literature which are most relevant to this work. One con-

tains studies in which prediction and proﬁtability evaluation are “disconnected” in the sense discussed in the

previous Section: a model is trained to predict the value of an asset, but that prediction is then used with a

“directional” trading strategy. Studies that fall within this category include Bao et al. (2017) who described

a three-step process (de-noising of the stock price series, stacked auto-encoders, and a recurrent neural net-

work using LSTM cells) achieving strong results, including a sum of daily returns whose yearly average was

approximately 46% and 65% for the S&P 500 and DJIA, respectively, over the period 2010-2016. Baek and

Kim (2018) explored the role of limited data when training neural networks and the resulting deterioration

in prediction accuracy, and introduced a modular architecture consisting of two LSTM networks. Using a

20 trading day window and the S&P500 as their underlying asset the authors reported low prediction errors

4

(e.g., a 12.058 mean absolute error) and a 96.55% cumulative return3, beating the benchmark buy-and-hold4

strategy (78.74%) over the period from 2/1/2008 to 7/26/2017 (we will conventionally use the U.S. format,

mm/dd/yyyy for reporting dates throughout).

Several studies have applied feed-forward or convolutional neural networks (CNN) to ﬁnancial time-

series. A recent example is Zhou et al. (2019), which introduced the notion of empirical mode decom-

position and a factorization machine-based neural network to predict the future stock price trend. For the

S&P500, using a one-year test period (the year 2011), the authors reported 13.039, 17.6591, and 1.05% for

the mean absolute, root mean squared and mean percentage errors, respectively. Combining their predic-

tions with a long-short5 trading strategy on the same index, they reported an average annualized return6 of

25% and a Sharpe ratio7 of 2.6. Another interesting CNN-based approach is that of Sezer and Ozbayoglu

(2018) in which price time-series are transformed into 2-D images which are then fed into a trading model to

produce a “buy”, “sell” or “hold” recommendation. Trading on well-known exchange-traded funds (ETFs)

as well as the constituents of the DJIA, that approach could yield an annualized return of 13.01% for the

ETF portfolio and 12.59% for the DJIA constituents portfolio, over 2007-2017. Finally, Krauss et al. (2017)

is an example of using a portfolio of stocks to outperform an index, as opposed to trading the index itself.

That work tested several machine learning models, including deep neural networks, gradient-boosted trees

and random forests in order to perform statistical arbitrage on the S&P500 using daily predictions from

1992 to 2015. Before transaction costs, the authors reported mean daily excess returns of 0.45% for their

ensemble model during 12/1992-10/2015, however returns were negative during that period’s last ﬁve years

(2010-2015).

A second important cluster of works contains studies in which the loss function used to train the pre-

(cid:81)N

3Cumulative return (CR) is the product of the sequence of gross returns over a given time period, minus unity, i.e., CR =
2 si/si−1 − 1 for a stock or other asset whose value is si over trading days i = 1, 2, . . . , N.
4A standard benchmark for gauging the performance of a trading strategy over a given time period, is the so-called buy-and-

hold (BnH) portfolio which is formed simply by buying the asset on the ﬁrst day of the period under consideration, and selling it

on the last day.

5 A Long-Short trading strategy is one in which in addition to buying, one is also allowed to sell short (borrowd) assets.
6Annualized return (AR) is the per-year geometric average of returns over a given time period, minus unity, i.e., AR = (1 +
CR)252/N − 1, for an asset whose value is si over trading days i = 1, 2, . . . , N, where 252 is the nominal number of trading days in a
calendar year.

7The Sharpe ratio (SR) is a measure of reward-to-risk performance, deﬁned as the diﬀerence between AR and the federal funds
rate (which is considered to be the “risk-free” rate, essentially at zero following the ﬁnancial crisis of 2007 − 2008), divided by the

annualized volatility (AV) of the returns. AV is the standard deviation of returns over a given time period, multiplied by the square

root of the number of trading days in a year (conventionally 252).

5

diction model is “consistent” with the trading strategy, e.g., the model predicts the direction of the price

movement - but not the numerical value of the price - which is then used with a directional trading strategy.

Sethi et al. (2014) presented a neural network which used only two features to predict the direction (up or

down) of next-day price movement for the largest 100 stocks (in terms of market capitalization) in the S&P

500 index. Those stocks were then combined into a portfolio in order to “beat” the index itself. Using data

from 2006-2013 and assuming a cost of 0.02% per transaction, that approach achieved a 16.8% annualized

return. Recently, Fischer and Krauss (2018) also studied the daily direction of the S&P 500 constituents

between 1/1/1990 − 9/30/2015, comparing various machine learning techniques. Their approach was to

use a single feature, namely the standardized one day return over the past trading year, in order to predict the

direction of the constituent stocks, and ultimately the probability for each stock to out-/under-perform the

cross-sectional median. The authors reported mean daily returns of 0.46% and a Sharpe Ratio of 5.8, which

indicates low risk / high reward prior to transaction costs. However, the returns were not consistent through-

out the testing period, and were close to zero during its last 5 years. Chiang et al. (2016) proposed a neural

network for forecasting stock price movement, using a binary output to indicate buy/hold or sell trading

suggestions. The authors applied a denoising process to the input time-series before feeding it to the neural

network. Their model, applied to a variety of ETFs over a 1-year period 1/1/2010-12/31/2010, achieved a

strong cumulative return of 41.89% in the case of SPY8, before transaction costs, and even higher returns

when short-selling was allowed. That approach also provides a good example of how very few trades (14

in one year) can yield good performance when trading individual assets.

The work in (Zhong and Enke, 2017) also studied the daily direction of the S&P 500 index using 60 input

features and various dimensionality reduction techniques. The study period covered 6/1/2003-5/31/2013.

The best model in that study reached a 58.1% accuracy, however the accuracy of a “naive” or other baseline

strategy over the same period was not reported. Using a trading strategy which either bought the S&P 500

or invested in a one-month T-bill resulted in a mean daily return of 0.1% with a standard deviation of 0.7%

over their test period of 377 days ending on 5/31/2013. Finally, an interesting attempt at event-driven stock

market prediction using CNNs is Ding et al. (2015). The model proposed therein also produces a binary

output, (i.e., the stock price is predicted to increase or decrease). Using a directional trading strategy over

a test period from 02/22/2013-11/21/2013, that approach was applied to 15 stocks in the S&P500, with a

reported mean cumulative return of 167.85%.

8SPY is an Exchange Traded Fund (ETF) which tracks the S&P500 index.

6

It is important to note that several of the works cited here report average returns (Bao et al., 2017;

Zhong and Enke, 2017) but no annualized or cumulative returns. Unfortunately, the arithmetic average of

returns is not informative as to actual capital growth / proﬁtability, because it cannot be used to infer annual

or cumulative return9. Also, as one might expect, it is diﬃcult to declare a single “winner” among the

diﬀerent approaches: not all authors report the same proﬁtability metrics, and the evaluation periods vary;

even when comparing against the same asset and testing period, one cannot safely draw conclusions if the

testing period is rather short (e.g., a single year, as in Chiang et al. (2016); Zhou et al. (2019); Zhong and

Enke (2017); Ding et al. (2015)). Finally, there is also a signiﬁcant number of works on the use of machine

learning methods and neural networks in particular, where the goal is exclusively accurate price prediction,

with no discussion of further use of that prediction, be it for trading or any other purpose. Representative

papers include Rather et al. (2015); Chong et al. (2017); Gu et al. (2018). The volume of research in this

category is substantial but we will not delve deeper here because prediction accuracy by itself is not the

focus of this work.

3. Proposed model and trading strategy

We proceed to describe a LSTM neural network which will be used to predict stock or index prices,

followed by a trading strategy that attempts to take advantage of the model’s predictions - despite their

inaccuracies - in a way which will be made precise shortly.

3.1. Network Architecture

Our proposed network follows the popular LSTM architecture (Hochreiter and Schmidhuber, 1997),

commonly used in recurrent neural network applications (Russakovsky et al., 2015; Silver et al., 2017; Wu

et al., 2016; Foster et al., 2018). The basic LSTM cell, shown in Fig. 1, receives at each time t = 0, 1, 2, ...

a B × I batch of length-I input vectors, xt (where B is batch size and I is the number of input features),

and utilizes a number of multiplicative gates in order to control information ﬂow through the network. In

addition to the usual hidden state, ht, of size B × H, where H is the number of hidden units, the LSTM cell

includes an “internal” or “memory” state, ct, also of size B × H in our case. The three gates, input (i), output

(o) and forget ( f ), which have feed-forward and recurrent connections, receive the input xt, and previous

9For example, if we invest 1 euro in an asset which achieves consecutive returns of -99%, 100%, and 100%, our investment

would be worth just 4 cents, i.e., a cumulative return of -96%, while the average return would be approximately 33%.

7

Figure 1: LSTM cell diagram (based on Olah (2018)).

hidden state ht−1, to produce

where σ is the sigmoid function10 applied element-wise, the bias terms b; are of size B × H, and W˙,x and

W˙,h are weight matrices with dimensions I × H and H × H, respectively. From the input, output and forget

terms, (it, ot, ft - all of size B × H), the next instances of the hidden and memory states are computed as:

it = σ(xtWix + ht−1Wih + bi)

ot = σ(xtWox + ht−1Woh + bo)

ft = σ(xtW f x + ht−1W f h + b f ),

ˆht = xtWhx + ht−1Whh + bˆht
ct = ft (cid:12) ct−1 + it (cid:12) tanh(ˆht)

ht = tanh(ct) (cid:12) ot,

(1)

(2)

(3)

(4)

(5)

(6)

where (cid:12) denotes element-wise multiplication. The input gate controls which elements of the internal state

we are going to update, while the forget gate determines which elements of the internal state (ct) will be

“eliminated”. In the case of deep networks with multiple LSTM layers, the hidden states generated by each

layer are used as inputs to the next layer described by another instance of Eqs. (1)-(6), where the input terms

xt in Eqs. (1)-(4) are replaced with the corresponding hidden states of the previous layer.

The un-rolled architecture of our network is depicted in Fig. 2. Operationally, the network receives a

10σ(x) =

1
1+exp(−x)

8

Figure 2: High level overview of the “un-rolled” LSTM architecture (for the case of two LSTM layers) over length-T input

sequences. The hidden states of the last LSTM layer are collected, reshaped and fed through a fully connected layer to produce the

desired outputs ˆy.

sequence of numerical vectors (or batches thereof) xt−T +1, ..., xt, where T is a ﬁxed “time window” size (for

our purposes T will be measured in trading days), and produces a scalar output sequence of ˆyt−T +2, ..., ˆyt+1

which in our context will represent the predicted price of a stock or other asset. We note that for each time

t, the quantity of interest in the output sequence is its last element, ˆyt+1 (i.e., the predicted price for the

“next” time period), while the previous elements, ˆyt−T +2, ..., ˆyt, correspond to “predictions” for the prices

yt−T +2, ..., yt, which are already known at time t. Although the inclusion of these output elements may seem

superﬂuous, opting for sequence-to-sequence training will make for better predictive accuracy as well as

proﬁtability, as we will see further on. The input vectors are fed into an LSTM cell sequentially, resulting

in the sequence of hidden state vectors ht−T +1, ..., ht. These are in turn used as inputs to the next LSTM

layer. The hidden state vectors generated by the last LSTM layer (Fig. 2 depicts only two LSTM layers

although one can introduce additional ones) are fed through a linear fully-connected layer to produce the

9

output sequence. In short, we have

ˆy = hWˆyh + bˆy,

(7)

where ˆy is (B · T ) × O, and O is the output dimension. The term ˆy contains the predicted output vectors

[ˆyt−T +2, ..., ˆyt+1]T , stacked in size-B batches for each time within our size-T window, h is the similarly-

stacked (B · T ) × H matrix of the hidden state vectors ht−T +1, ..., ht produced by the last LSTM layer (each

batch contributing T rows), and Wˆyh, bˆy are the weight matrix and bias vectors, sized H × O and (B · T ) × O,

respectively. For our purposes, the feature vectors, xt, will include an asset’s daily adjusted close price,
, intra-day high, y(H)
yt, opening price, y(O)

, intra-day low, y(L)

, closing price, y(C)

, and the previous day’s

t

t

t

t

adjusted close, yt−1, all obtained online (YahooFinance, 2018), i.e.,

xt = [yt, y(O)

t

, y(L)
t

, y(H)
t

, y(C)
t

, yt−1].

The particular choices of the remaining network parameters (e.g., number of hidden states, H, and window

size, T ), will be discussed in Sec. 4.3.

There are two design choices that we would like to highlight with respect to the network architecture.

One has to do with the use of the entire sequence of hidden states, ht−T +1, ..., ht, produced by the last LSTM

layer, when it comes to computing outputs. More typically, one could simply let the hidden state evolve

while the input sequence xt−T +1, ..., xt is “fed in”, and use only its “last” value, ht, to compute the prediction

ˆyt+1. Doing so, however, essentially implies an expectation that all of the “useful” information present in

the sequence will be encoded into the last hidden state. On the other hand, utilizing the entire hidden state

sequence will allow us to look for information in the time-evolution of the hidden state within the rolling

window, which will be prove to be beneﬁcial, as we will see shortly. A second important design choice

is the handling of the hidden state vectors by reshaping them into a (B · T ) × H matrix, to be fed into a

single fully-connected linear activation layer as per Eq. 7. By doing so, we avoid having to use one fully-

connected layer per time step, t − T + 1, ..., t, which would introduce T “copies” of Eq. 7 for handling each
of the ht−T +1, ..., ht separately, resulting in a total of T × (H × O + B × O) weights to be adjusted. Using a
single dense layer as outlined above requires only H × O + B × T × O weights, a signiﬁcantly lower number

as the window size, T , grows. This choice will allow us to train with a very small batch size, which will

directly lead to short training times while exploiting information in the time history of the hidden states, and

will also make our approach applicable in multi-asset settings or when frequent training is required e.g., in

intraday trading.

10

3.2. Trading Strategy and Allocation

As we have mentioned in Sec. 2, it is common to make trading decisions based on a prediction model’s

directional accuracy, even when the model was trained to attain low mean squared or absolute errors. That is,

the decision to buy or sell is determined by whether the model predicts an upwards or downwards movement

of the asset price over the next time interval. Here, we depart from prior approaches by taking advantage

of the fact that it is possible to be nearly “agnostic” about an event occurring, i.e., the direction (up/down)

of an asset’s price movement, and yet to be able to glean signiﬁcant information about the proﬁtability of a

trade when conditioning on particular events. In our case the events will have to do with the relative location

of predictions within their own distribution.

To make matters precise, we begin by deﬁning the notions of allocation policy and trading strategy:

Deﬁnition 1 (Allocation policy). Let M be a prediction model that produces at each time t an estimate,
ˆyt+1, of an asset’s future value yt+1, the latter resulting from some underlying stochastic process. Let ˆrt =
ˆyt+1/yt − 1 be the model’s one-step predicted returns at time t, and Dˆr their distribution. An n-bin allocation
policy is a pair (Q, A), where Q ∈ Rn−1 is a vector of percentiles of Dˆr in increasing order, and A ∈ {R ∪ ∅}n
a vector of allocations in units of the asset (e.g., number of shares to purchase), where the element ∅ denotes

sale of all units of the asset held.

Deﬁnition 2 (Trading strategy). Given a prediction model, M, the current asset price, yt, and an n-bin
allocation policy (Q, A), our proposed trading strategy consists of the following steps, executed at each time

t:

• Let

• Query M for the asset’s predicted return ˆrt in the next time step, t + 1.

i =






if
1
j + 1 if
if
n

ˆrt < Q1
ˆrt ∈ [Q j, Q j+1),
ˆrt ≥ Qn−1.

• If Ai > 0 and we are currently not holding any of the asset, buy Ai units of the asset. If Ai > 0 and we

are already holding some of the asset, or if Ai = 0, do nothing.

• If Ai = ∅, sell any and all units of the asset held.

In our case, the model M will be the LSTM network described in the previous Section. Intuitively,

the percentiles in Q partition the real line into n “bins”, and the trading strategy is to buy Ai units of the

asset whenever the predicted return lies in bin i and we do not already own the asset. For simplicity, we

11

will consider allocation policies in which Q1 = 0 and A1 = ∅, Ai>1 ≥ 0, i.e, we always sell all of our

asset if the predicted return for the next trading day is negative, that being the only event on which we

sell. Of course, one can envision variations of the above strategy, including negative allocation values

corresponding to “short sales” (i.e., selling an amount of the asset under the obligation to buy it back at

a later time, with the expectation that its value will have declined), assigning the “sell signal”, ∅, to more

than one bins as determined by Q, using time-varying values of Ai depending on the available cash in our

portfolio, or measuring the allocations Ai in monetary amounts as opposed to units of the asset. Some of

these options are worth exploring in their own right, but we will limit ourselves to the setting previously

described because of space considerations.

Remark: We note that the proposed trading policy can be viewed as a generalization of the classical

directional “up-down” trading strategy. If we set Q = 0, i.e., we use only one separating point, and A =

[∅, 1]T , then there will be only two bins, one for positive predicted returns (in which case we buy 1 unit of the

asset), and one for negative predicted returns (in which case we sell), which is exactly how the directional

strategy works.

3.2.1. Optimizing the allocation policy

Armed with the above deﬁnitions, we can now consider the question of what allocation policy is optimal

for a given prediction model while following the trading strategy of Def. 2. We will be buying some

units of the asset, Ai, when our model’s predicted return lies in bin i > 1, and will then hold those units

until the predicted return falls within bin 1. Over time, this process generates a set of holding intervals,
Ii j = [τi j + 1, ωi j], where τi j denotes the time of the j-th buy transaction while the predicted return was in

bin i > 1, and ωi j the time of the following sell transaction (i.e., the ﬁrst time following τi j that the predicted

return falls within bin 1). It should be clear from Def. 2 that the intervals Ii j will be non-overlapping (because

we never execute a buy when already holding the asset) and that their union will be the entire set of times

during which we hold a nonzero amount of the asset.

Our allocation policy risks a monetary amount of Ai · yτi j to purchase Ai units of the asset at each time

τi j. The net proﬁt generated from the at-risk amount at the end of the holding interval Ii j (i.e., after a single

buy-sell cycle) is therefore

Ai(yωi j − yτi j).

(8)

Over time, there will be a number of instances, Ni, that the asset was purchased while at bin i and the total

12

(9)

(10)

(11)

net proﬁt after M = (cid:80) Ni buy-sell transactions will then be

resulting in an expected net proﬁt of

n(cid:88)

Ni(cid:88)

G =

Ai

(yωi j − yτi j),

i=2

j=1

E{G} =

AiE

(yωi j − yτi j)

.

n(cid:88)

i=2





Ni(cid:88)

j=1





The expectation terms in Eq. 10 represent the expected sum of diﬀerences in asset price over each bin’s

holding intervals, that is, the expected total rise (or fall) in asset price conditioned on buying when the

expected return falls within each particular bin and following the proposed trading strategy. It is clear from

Eq. 10 that the expected total proﬁt will be maximized with respect to the allocations Ai if each Ai is set

to zero whenever the associated expectation term is negative, and to as large a value as possible when the
expected price diﬀerence is positive. Assuming an practical upper limit, Amax, to the number of asset units

we are able or willing to buy each time, the optimal allocation values are thus

Ai>1 =





Amax

if E

j=1(yωi j − yτi j)

> 0,

(cid:110)(cid:80)Ni

(cid:111)

0

otherwise.

In practice, the expectations terms in Eq. 11 will be approximated empirically from in-sample data, therefore

their sign could be wrongly estimated, especially when they happen to lie close to zero. For this reason, one
may wish to slightly alter Eq. 11 so that we buy Amax when the expected sum of price diﬀerences is greater

than some small threshold (cid:15) > 0, to reduce the chance of “betting on loosing bins”. Finally, we note that we

are taking Amax to be constant (across time and across the bins i) for the sake of simplicity, although other

choices are possible as we have previously mentioned.

4. Model training and allocation policy selection

As we have seen, the speciﬁcation of the optimal allocation and trading strategy described in the previ-

ous Section, requires the (empirical) distribution of the predicted returns which are generated by our LSTM

model. That is, we need a trained model in order to obtain concrete values for the percentiles in Q and

the “bins” they induce, as well as the estimated sums of price diﬀerences per bin that will determine the

allocations, A, in Eq. 11. A step-by-step summary of our approach is as follows:

13

• Train and test the LSTM network over a rolling window of size T , where for each trading day, t, in the

period from 1/1/2005-1/1/2008, the network’s weights are ﬁrst adjusted using price data from the im-

mediate past, (i.e., input [xt−T , . . . , xt−1], and output target [yt−T +1, . . . , yt]); then, the network is given

[xt−T +1, . . . , xt] as input and asked to predict the price sequence once step ahead, [ˆyt−T +2, . . . , ˆyt+1],

from which we keep the next day’s adjusted closing price, ˆyt+1, and calculate the predicted return, ˆrt.

• Use the distribution Dˆr of the predicted returns generated by the rolling-window process to determine
the vector of cutoﬀ points, Q. These points could be chosen to have constant values, or to correspond

to speciﬁc percentiles of Dˆr, or in any other consistent manner.

• For each bin determined by Q, use the history of input data and corresponding model predictions over

1/1/2005-1/1/2008 to determine the buy and sell events that would have resulted by having followed

the trading strategy of Def. 2. Compute the asset price diﬀerences for each buy-sell cycle and sum

over each bin, i, in which the corresponding buy transactions took place (Eq. 10). Set the allocations

Ai>1 to their optimal values as per Eq. 11.

• Given the allocation policy, (Q, A), test the proﬁtability of the trading strategy and all variants of our

LSTM model with respect to a range of hyper-parameters, over a subsequent time period, 1/2/2008-

12/31/2009, again training the network daily based on input data from the previous T trading days

before predicting the next day’s price. Select the hyper-parameter values which yield the most prof-

itable – as opposed to most accurate – model.

• Apply the selected model to a new data sample (1/4/2010-5/1/2018) and evaluate its proﬁtability11.

We will use the term out-of-sample (instead of testing) when referring to this period, in order to avoid

confusion with the rolling training-testing procedure used to make predictions. Similarly, the pe-

riod 1/1/2005-12/31/2009 used to construct the initial allocation policy and perform hyper-parameter

selection will be referred to as in-sample.

We go on to discuss important details and practical considerations for each of these steps.

4.1. LSTM training

In contrast to other studies, e.g., (Bao et al., 2017), which make use of a more traditional training -

validation - testing framework, we found that a rolling training-testing window approach was better suited

11Proﬁtability will simply be G/C, where G is the net proﬁt, as calculated in (9) and C is the initial capital invested

14

to our setting, where at each time, t, the network is trained using data from the immediate past and then asked

to predict “tomorrow’s” price (at time t + 1). By updating the model weights at each step, using the most

recent information before making a prediction, we avoid “lag” or “time disconnection” between the training

and testing data (as would be the case, for example if our training data was “far” into the past compared

to the time in which we are asked to make a prediction) and allow the model to better “react” to changing

regimes in the price sequence as time progresses, leading to it being more proﬁtable as we shall see. As

we noted in Sec. 3.1, we used a sequence-to-sequence training schema because it led to signiﬁcantly higher

cumulative returns compared to sequence-to-value training. We speculate that asking the network to predict

sequences of the asset price over a time interval, combined with the use of all hidden vectors produced in

that interval, allows the network to learn possible latent “micro-structure” within the price sequence (e.g.,

trend or other short-term characteristics), thus facilitating the training process.

The network was trained using back-propagation through time (BPTT), with the ADAM optimizer,

implemented in python 3 and Tensorﬂow v1.8. Network weights were initialized from a uniform distribution

(using the Glorot-Xavier uniform method) and LSTM cell states were initialized to zero values. We applied

exponential decay to our learning rate, and trained for 1600 iterations. The number of iterations was chosen

to avoid over-ﬁtting by observing both the training and testing errors, using a mean-squared loss function. It

is important to note that the rolling window training process, as described above, used a batch size of 1 (i.e.,

the network was trained each time on a single T -sized window of the data, xt). Our numerical experiments

showed that for our proposed architecture, larger batch sizes did not increase the ultimate proﬁtability of

the model and trading strategy, and sometimes even led to lower performance12. Depending on the choice

hyperparameters (listed in Section 4.3), the time required to compute one rolling window step was between

7 and 30 seconds on an modestly equipped computer (Intel i7 CPU and 16 GB of RAM).

4.2. Allocation policy selection

The overall number and values of the allocation policy’s cutoﬀ points (Q) were determined after numer-

ical experimentation to ensure that all of the bins deﬁned by Q included an adequate population of samples

and were suﬃcient in number to allow us to discern diﬀerences in the proﬁts realized by the trading strategy

when executing a buy in diﬀerent bins. We found that an eﬀective approach, in terms of ultimate proﬁtabil-

ity, was to select the cutoﬀ points in Q using percentiles of the distribution of the absolute expected returns,

12For example, training with a batch size of 50 on the S&P500 index resulted in twice the mean average percent error and a

15% lower cumulative return compared to training with a batch size of 1, with full details on trading performance to be given in

Section 5.

15

D|ˆr| Thus, the Q1 cutoﬀ was set to zero (so that as per Def. 2 we sell any holdings if our model’s predicted re-
turn is negative), and another six cutoﬀ points, Q2, . . . , Q7, were chosen to correspond to the ﬁrst six deciles

(10%, 20%, 30%, 40%, 50%, and 60% points) of the distribution of absolute predicted returns, splitting the

set of predicted returns into eight bins. For each bin, i, we could then use historical predicted and actual
prices to calculate the sum of price diﬀerences (as per in Eq. 11) and determine the optimal allocation Ai.

We emphasize that the above choices with respect to Q were the result of experimentation, and that the

elements in Q could in principle be optimized; however, doing so is not trivial, and we will not pursue it

here because of space considerations.

In practice, it is be advantageous to allow the allocation policy (Q, A) to adapt to new data rather than

be ﬁxed for a signiﬁcant amount of time, because the distributions of the predicted and realized returns are

unlikely to be stationary. As the model makes a new prediction each day, we have a chance to add the new

data to the distribution of predicted returns and adjust the cutoﬀ points in Q. In the same vein, as the trading

strategy generates new buy-sell decisions, we may re-calculate the allocations A. There are many options

here, such as to accumulate the new information on returns and trading decisions as in a growing window

that stretches to the earliest available data, or to again use a rolling window so that as each day new data is

added to the distribution, “old” data is removed.

With the above in mind, we chose to update the percentiles in Q at every time step, using a growing

window of data that began 120 time steps prior to our ﬁrst out-of-sample prediction. For example, in order to

making a prediction for 1/4/2010 - the ﬁrst day of our out-of-sample period - the model’s predicted returns

between 7/15/2009 - 1/1/2010 were included in the empirical distribution used to calculate Q. This was

done in order to avoid the problem of initially having no data (e.g., on the ﬁrst day of our out-of-sample

period) from which to calculate the Qi. The 120 step “bootstrap” period was selected after experimenting

with several alternatives, including using more or less of the initial history of predicted returns, or using

a ﬁxed-size rolling window to determine how much of the “past” should be used. With respect to the
allocations, Ai, when computing the per-bin sums of price diﬀerences in Eq. 11, we found that it was

best in practice (i.e., increased proﬁtability) to include all of the available history of buy-sell events and

corresponding price diﬀerences, up to the beginning of our data sample (1/1/2005). Moving forward, as

buy and sell decisions were made at each time step, the resulting price diﬀerences were included in the sums

of Eq. 11 and the allocations A were updated, if necessary. An instance of the set of bins, corresponding

cutoﬀ points and sums of price diﬀerences for each bin (computed for use on the ﬁrst trading day of the

16

out-of-sample period, 1/04/2010) are listed in Table 1. For example, if the predicted return for 1/4/2010

for the S&P500 falls within the 30-40% bin (or, equivalently in the interval [0.65%, 0.97%)), we do not buy

any of the asset (corresponding allocation set to zero) because if we did, the expected price diﬀerence in the

asset would be negative at the time we sell the asset.

S&P500

DJIA

NASDAQ

R2000

Bin i (deciles)

1: SELL

Qi

0

δi

-

Qi

0

δi

-

Qi

0

δi

-

Qi

0

δi

-

2: 0%-10%

0.12% 126.97

0.01% −1785.45

0.05% 62.37

0.01% 139.19

3: 10%-20%

0.38% 99.92

0.02% −1529.37

0.10% 520.54

0.03% 90.76

4: 20%-30%

0.65% 131.35

0.03% 904.60

0.12% 52.27

0.06% 23.19

5: 30%-40%

0.97% −66.71

0.04% −88.28

0.16% −59.84

0.07% 87.32

6: 40%-50%

1.18% 128.67

0.06% 1062.01

0.18% 206.65

0.10% 1.64

7: 50%-60%

1.44% −191.68

0.08% 381.46

0.24% 254.78

0.12% 6.17

8: 60%-100%

-

222.85

-

3850.18

-

719.68

-

248.96

Table 1: Snapshot of the cut-oﬀ points, Qi (reported as percentages), for each bin-interval of the predicted return distribution. The
Qi mark the upper values of each bin, and are computed using the model’s predicted returns from 7/15/2009 -1/04/2010. The δi
denote empirical estimates of the corresponding sums of price diﬀerences from Eq. 11 when using the proposed trading strategy,
over the period 1/1/2005-1/04/2010.

As previously discussed, it is optimal to buy Ai = Amax units only when the predicted return falls within
a bin whose realized sum of price diﬀerences is positive. In practice, the choice of Amax is usually subject

to budget constraints or investment mandates which limit the maximum number of units to purchase. This

maximum could also be tied to the risk proﬁle of the investor. In our study, Amax was set to the maximum

number of units one could buy with their available capital at the start of the trading period. For example,

at the beginning of our out-of-sample period, 1/04/2010, the price of the S&P 500 index was $1, 132.989,
and therefore if our initial capital was $28, 365 we would set Amax = 25. On a practical note, however,

because stock indices are not directly tradeable, one can instead trade one of the available Exchange Traded

Funds (ETFs) which track the index and are considered one of the most accessible and cheapest options for

investing in indices. Therefore, in order to apply our approach to the S&P500, we could choose to trade

SPY, a popular ETF which tracks that index, and – based on the aforementioned initial capital and SPY price
of $94.55 on 1/04/2010 – set Amax = 300. In the experiments detailed in the next Section, we will choose

to trade four well-known ETFs, namely SPY for the S&P500, DIA for the DJIA, IWM for the R2000 and

ONEQ for the NASDAQ.

17

4.3. Hyper Parameter Tuning

A grid-search approach was used to select the parameters of our neural network, namely the number of

LSTM layers, number of units per LSTM layer (H), the dropout parameter (dropout was applied only on the

input of a given neuron), and the length T of the input sequence. Because our ultimate goal is proﬁtability,

this meant evaluating the cumulative returns of our model under each combination of parameters, in order

to identify the most proﬁtable variant (which could be diﬀerent depending on the stock or stock index under

consideration). For each choice of hyper-parameters listed in Table 2, we carried out the rolling training-

testing procedure described above, over the period 1/1/2005-1/1/2008, and calculated an initial allocation

strategy (Q, A). Following that, we used data from 1/2/2008-12/31/2009, to calculate the proﬁt realized

Parameters

Nr. of LSTM layers

Range

2, 3

Nr. of units (H)

32, 64, 128

Input sequence length (T )

11, 22, 44

Dropout

0,50,70%

Table 2: Hyper-parameter grid-search values.

by applying our trading strategy (while the allocation policy evolved each trading day, as described in the

previous Section). This subsequent period, containing 505 trading days, contained a suﬃcient number of

rolling windows and corresponding predictions from which to compare performance for each choice of

hyper-parameters. In addition, it encompasses the global ﬁnancial crisis of 2009 and is long enough to

capture various market cycles. The model parameters leading to the highest cumulative return for each

of the four stock indices of interest were thus identiﬁed and are listed in Table 3. On a small 6-machine,

Parameters

S&P 500 DJIA NASDAQ R2000

Nr. of LSTM layers

Nr. of units (H)

Input sequence length (T )

3

64

22

3

64

22

3

32

22

3

32

11

Dropout

50%

70%

50%

50%

Table 3: Grid-search: optimal hyper-parameter values.

96-CPU, 96Gb RAM computing cluster, the entire grid search process was easily parallelized. For smaller

network conﬁgurations, e.g., input sequences of length 11, using 2 layers and 32 neurons, the process

18

described above took a few (≤ 5) hours to complete, while conﬁgurations with length-44 input sequences

required up to 3 days.

5. Results

Having settled on a choice of hyper-parameters, we tested the performance (annualized returns and other

measures of interest) of the resulting model(s) and associated trading strategy on our out-of-sample period,

1/4/2010-5/1/2018, which we had set aside for this purpose. Summarizing the procedure outlined in the

previous Section, for each trading day, t, within this period, we updated the LSTM network’s weights by

training it on the time window of input data from t − T, ..., t − 1 with a target output of yt, and then computed
the prediction, ˆyt+1, of the next day’s closing price, using input data over the window t − T + 1,...,t. Next,
we determined the predicted return, ˆrt = ˆyt+1/yt − 1, and the bin, i, (based on the percentiles in Q) within

which it lies. We then bought or sold the amount of asset units (or corresponding ETF in case of an index)

prescribed by the allocation policy (i.e., buy Ai units if i > 1 and we did not already own the asset, and sell
all of our units if i = 1) If a buy was executed, we recorded the index of the bin, i, and the asset price at that

time. When that asset was later sold, the proﬁt obtained from that transaction (i.e., sell price minus buying

price) was added to the sum of price diﬀerences term in Eq. 11, and we recalculated the corresponding

allocation Ai if the update caused a change in the sign of the sum. Finally, we included ˆrt to the growing set

of previous predicted returns and adjusted the decile points in Q to account for their new distribution, thus

completing our update of the allocation policy, (Q, A).

We proceed with the main results concerning i) the prediction accuracy, and ii) the proﬁtability of our

proposed scheme over the out-of-sample period, for each of the stock indices studied. We will also discuss

comparisons against recent eﬀorts, as well as “naive” prediction approaches and trading strategies.

5.1. Error metrics and prediction performance

Although our central goal is to achieve good trading performance, we will nevertheless begin by evaluat-

ing our proposed LSTM model using some of the metrics often cited in studies involving prediction accuracy

including mean directional accuracy13 (MDA) which in less formal terms measures how accurate a model is

at predicting the correct direction (up/down) of stock price movement, mean squared error14 (MSE), mean

1(sign( ˆyt − yt−1) · sign(yt − yt−1)), where the indicator function 1(x) returns 1 if x is positive, zero otherwise,

yt and ˆyt are the realized and predicted prices for time t, respectively, N is the length of the out-the-sample sequence.

13MDA = 1
N

(cid:80)N
t=1

14MSE = 1
N

(cid:80)N

t=1(yt − ˆyt)2.

19

absolute error15 (MAE), mean absolute percentage Error16 (MAPE) and correlation coeﬃcient17 (R2). Ta-

ble 4 shows the predictive performance of the proposed LSTM model for each of the four stock indices,

for the same out-of- sample period (1/4/2010-5/1/2018). Overall, our model achieves a MAPE below 1%

Metrics S&P 500

DJIA

NASDAQ R2000

MDA

51.05%

MAPE

0.66%

11.07

256

MAE

MSE
R2

51%

0.6%

95.33

19047

50.47% 50.02%

0.8%

30.8

1893

0.9%

9.01

142

99.94% 99.94% 99.95% 99.90%

Table 4: Error metrics for the LSTM model.

and a MDA narrowly above 50%, for all indices. However, applying the Pesaran-Timmermann (PT) to the

predicted vs. actual direction of price movements did not conﬁrm that the LSTM model accurately predicts

price movement direction in a statistically signiﬁcant manner. The test statistics and corresponding p-values

were (PT=-3.61, p=0.999), (PT=-1.84, p=0.9674), (PT=-0.64, p=0.7378), and (PT=-2.03, p=0.9789), for

the S&P500, DIJA, NASDAQ and R2000 indices, respectively. The fact that our LSTM model does not pro-

vide a signiﬁcant advantage in predicting next-day price movement direction implies that our model may

not be eﬀective when used with the day-to-day directional “up-down” strategy often used in the literature. It

does not, however, preclude us from being able to produce signiﬁcant proﬁts under other trading strategies,

such as the one proposed in Sec. 3.2 which does not require directional accuracy on the part of the model.

When compared against other recent studies, e.g., Bao et al. (2017), our model has signiﬁcantly lower

MAPE for S&P500 and DIJA (see Table 5), while the R2 coeﬃcient is better in both cases (using the same

test period as in Bao et al. (2017)). Looking at Zhou et al. (2019) and the metrics reported therein (see

Table 6), for the same out-of-sample period used in that work, our model performs comes in very close for

the S&P500 and performs better for NASDAQ with respect to MAE and RMSE. Finally, Table 7 compares

with Baek and Kim (2018), where our model outperforms in all three statistics (MAPE, MAE, MSE).

We note that our model relies on a relatively simple architecture with no pre-processing layers, making it

(cid:80)N

15MAE = 1
N
16MAPE = 1
N

17R2 =

(cid:113)

(cid:80)N

t=1 |yt − ˆyt|.
(cid:80)N
|yt− ˆyt|
.
t=1
yt
t=1(yt − ¯y)( ˆyt − ¯ˆy)
(cid:113)

(cid:80)N

t=1(yt − ¯y)2

(cid:80)N

t=1( ˆyt − ¯ˆy)2

respectively.

20

, where ¯y and ¯ˆy are the averages of the actual (yt) and predicted (ˆyt) price sequences,

Table 5: Comparison with Bao et al. (2017) (S&P500 and DIJA) for the out-of-sample period used therein (10/1/2010 -

9/30/2016).

S&P500

DIJA

Metics Bao et al. (2017) LSTM Bao et al. (2017)

LSTM

MAPE
R2

1.1%

0.946

0.7%

0.999

1.1%

0.949

0.6%

0.999%

S&P500

NASDAQ

Metrics Zhou et al. (2019)

LSTM Zhou et al. (2019)

LSTM

MAPE

MAE

RMSE

1.05%

13.03

1.07%

13.19

17.6591

18.4715

1.08%

52.4773

70.4576

1.17%

30.8463

41.1703

Table 6: Comparison with Zhou et al. (2019) (S&P500 and NASDAQ), for the out-of-sample period used therein (the year 2011).

S&P500

Metrics Baek and Kim (2018) LSTM

MAPE

MAE

MSE

1.0759%

12.058

342.48

0.8%

11.65

276.27

Table 7: Comparison with Baek and Kim (2018) (for the S&P500), for the out-of-sample period used therein 01.02.2008 -

7.26.2017.

simple to implement and fast to train. This is to be contrasted with more elaborate designs, such as the three-

layered approach of Bao et al. (2017) passing data through a wavelet transformation and autoencoder neural

network before reaching the LSTM model, or the “dual” LSTM models of Baek and Kim (2018). Despite its

simplicity, our model’s predictive performance is similar to that of more complex approaches, and compares

favorably to studies with long out-of-sample periods (Bao et al. (2017), Baek and Kim (2018)).

We emphasize the fact that the comparisons and discussion of the model’s predictive performance -

although encouraging - are given here mainly for the sake of providing a fuller picture, and that good

predictive performance will mean little unless our model also performs favorably in terms of proﬁtability

(to be discussed shortly). In fact, there is an important point of caution to keep in mind regarding predictive

accuracy, and it has to do with the proper context when citing MAPE, MAE and MSE values. Speciﬁcally,

when it comes to asset prices, daily changes are usually small on a relative basis, and thus a MAPE of

21

less than 1%, for example, is neither unusual nor surprising. In fact, a naive model which “predicts” that
tomorrow’s price will simply be equal to today’s (i.e., ˆyt+1 = yt), applied to the S&P500, would typically
yield very low error metrics, e.g., MAPE = 0.64%, MAE=20.0203, MSE=844.186 over our out-of-sample

period, or a MAPE of 0.83% for the period examined in Baek and Kim (2018) (to be compared with a

MAPE of 1.0759% reported in that work). Of course, this naive model is completely useless from the point

of view of trading because it does not allow us to make any decision on whether to buy or sell, highlighting

the fact that models with similar prediction accuracy can have vastly diﬀerent proﬁtability, while favorable

MAPE, MAE, and MSE scores do not by themselves guarantee success in trading. When the ultimate goal

is proﬁtability, the prediction model should be evaluated in the context of a speciﬁc trading strategy, and

one may achieve high(er) returns by looking beyond average errors, as the trading and allocation strategies

of Section 3.2 do.

5.2. Proﬁtability

We now turn our attention to the proﬁtability of our proposed LSTM model and trading strategy. The

results presented below are labeled according to the stock indices studied, however – as noted in Sec. 4.2

– all trading is done using their corresponding tracking ETFs, namely SPY for the S&P500, DIA for the

DJIA, IWM for the R2000 and ONEQ for the NASDAQ, since the indices themselves are not tradeable.

The cumulative returns we obtained when trading over 1/04/2010-5/1/2018 (listed in Table 8), signiﬁcantly

outperformed the benchmark buy-and-hold strategy (340% vs 136%, 185% vs 137%, 371% vs 229%, and

361% vs 164% for the S&P500, DJIA, NASDAQ and R2000 indices, respectively); the same was true on

an annualized basis. A sample comparison illustrating the time history of capital growth using our approach

vs buy-and-hold is shown in Figure 3.

Our superior performance over the buy and hold strategy, comes at the cost of a higher standard deviation

of returns achieved (24.3% vs 14.9%, 24.6% vs 14%, 38.3% vs 16.8%, 33% vs 20% for the S&P500, DJIA,

NASDAQ and R2000, respectively), leading to lower corresponding Sharpe ratios (except for the S&P500)

compared to buy-and-hold. Draw-down, a measure of investment risk deﬁned as the largest peak-to-trough

decline during the investment’s life-cycle, was also aﬀected, with our model having larger negative peak-

to-trough returns versus buy-and-hold (−20% vs −11.2%, −14.3% vs −10.7%, −28% vs −12.5%, −24.3%

vs −15.8% for the S&P500, DJIA, NASDAQ and R2000, respectively). This is expected, given that we are

comparing a semi-active trading strategy against the “passive” buy-and-hold strategy that executes only a

single buy; however, the draw-down is not excessive, and our trading strategy is able to more than make

22

Proﬁtability Measures

CR

AR

AV

SR

DD

S&P 500

LSTM 339.6% 19.5% 24.3% 0.8 −20.0%

BnH

136.4% 10.9% 14.9% 0.7 −11.2%

DJIA

LSTM 185.1% 13.3% 24.6% 0.5 −14.3%

BnH

136.6% 10.8% 14.0% 0.8 −10.7%

NASDAQ LSTM 370.9% 20.3% 38.3% 0.5 −28.0%

BnH

228.9% 15.2% 16.8% 0.9 −12.5%

R2000

LSTM 360.6% 20.0% 33.0% 0.6 −24.8%

BnH

163.5% 12.2% 20.0% 0.6 −15.8%

Table 8: Returns and risk statistics for the out-of-sample period (1/04/2010-5/1/2018). CR denotes cumulative return, AR is
annualized return, AV is annualized volatility, SR is the Sharpe ratio, DD is draw-down, deﬁned as the largest peak-to-trough

percent decline during an investment’s lifetime; LSTM is the our proposed model and trading strategy, while BnH represents the

buy-and-hold strategy in which the asset is bought once at the beginning of the trading period and sold at the end. The S&P500

was traded via the SPY ETF, the DJIA via DIA, the R2000 via IWM, and the NASDAQ via ONEQ.

Figure 3: Growth of one monetary unit invested using our proposed model (LSTM) vs. the buy-and-hold strategy, on the S&P500.

up for it as is evident from its superior annualized and cumulative returns. Finally, our trading strategy

executed 535, 394, 494, and 544 trades for the S&P500, DJI, NASDAQ, and R2000, respectively, over

the 2117 trading days in the out-of-sample period. On average, this corresponds to approximately one

23

transaction per week (ranging from once per four trading days for the R2000, to once per six days for the

DJI). We note that the returns cited above ignore transaction costs. We have chosen to do this because our

strategy trades only one asset, does not take short positions where costs have a larger impact, and trades

sparingly as noted above. Moreover, transaction costs for institution-class investors are minimal, ranging

from 2 to 5 basis points per trade regardless of the transaction amount, while retail investors can nowadays

trade without cost through online brokers18.

5.2.1. Comparisons with recent works

Besides the benchmark buy-and-hold strategy, the results given in the previous Section indicate that our

proposed model outperforms those in several recent works which attempt to beat the major stock indices

used here, most often the S&P500. A direct comparison to Baek and Kim (2018) (also trading the S&P500

over a long out-of-sample period, 1/2/2008-7/26/2017), shows that our approach outperforms by a signif-

icant margin in terms of cumulative return (334.36% vs 96.55%). Next to Sezer and Ozbayoglu (2018),

which employed deep convolutional neural networks instead of LSTMs, our model achieved an annualized

return of 18.31% compared to the 10.77% reported in that work when trading the same tracking EFT as we

have (SPY), or 13.01% when using a portfolio of ETFs, over the period of 1/1/2007-12/31/2016. Other

studies with relatively long out-of-sample periods, include Krauss et al. (2017) and Fischer and Krauss

(2018); compared to those, our model yields a signiﬁcantly greater cumulative return (204% vs −17.96%

and −15%, respectively, over their out-of-sample period, 2010-2015) and, as a result, a higher positive

Sharpe ratio (0.8 vs negative for the other two works), noting however that those works traded a portfolio of

stocks derived from the S&P500 and not the index (or ETF) itself. Finally, we note that our results are not

directly comparable with those in works who report high average or summed returns (e.g., Bao et al. (2017)

and others) from which unfortunately one cannot deduce annualized returns or other “standard” measures

of trading performance, as we have explained in Section 2.

The works cited thus far reported performance over multi-year periods. Direct comparisons between

diﬀerent approaches over short evaluation periods must be made cautiously because superior performance

over any short period does not necessarily imply sustainable long-term results, making it diﬃcult to say

which method is better. Having said that however, compared to the model from Zhou et al. (2019) for the

S&P500, using the same out-of-sample period (the year 2011), our approach attained a higher cumulative

18Examples of brokers

that oﬀer

free retail

trades

include RobinHood (https://robinhood.com),

and Vanguard

(https://investor.vanguard.com/investing/transaction-fees-commissions/etfs)

24

return of 33.98% vs 25%. Finally, in comparison with Chiang et al. (2016) during their out of sample

period (the year 2010) our approach also performs strongly (80.90% vs. 41.89% when using their long-only

trading strategy).

5.2.2. Attributing the overall performance

As we have previously stated, altering either the trading strategy or the prediction model will aﬀect the

overall scheme’s proﬁtability. This brings up the question of “how much” of the performance attained is due

to the proposed LSTM architecture versus the trading and allocation strategy which we have outlined. This

is diﬃcult to answer fully, partly because it requires rating the performance of many alternative prediction

models when paired with our proposed trading strategy, as well as diﬀerent trading strategies to be used

with our proposed LSTM model. Here, we will opt to gain some insight by examining the performance of

a simple autoregressive integrated moving average (ARIMA) predictor model under our proposed trading

strategy, as well as the performance of our LSTM network under the classic directional “up-down”strategy

whose variants are used frequently throughout the literature (Bao et al., 2017; Baek and Kim, 2018; Zhou

et al., 2019). Of course the ARIMA model lacks sophistication, but its purpose here will be to serve as a

“canonical” baseline case only.

To proceed with our analysis, we ﬁtted an ARI MAp,d,q model to the daily (adjusted close) price sequence
of each stock index under consideration, where p, d and q are the orders of the autoregressive, diﬀerence,

and moving average terms, respectively. In each case, the model was ﬁtted to the price data of the in-sample

period (1/1/2005 - 12/31/2009), and the optimal values of the p, d, and q, were determined by searching

over the integers (up to a maximum of order 3) and selecting the model with the lowest AIC and BIC. The

selected models were then examined to ensure that their coeﬃcients were statistically signiﬁcant at the 5%

level. The resulting model orders (p, d, q) were: (2, 1, 1) for the S&P500, (3, 2, 1) for the DJIA, (3, 2, 2) for

the NASDAQ, and (2, 1, 0) for the R2000 index. The model coeﬃcients are not listed here for the sake of

brevity.

In terms of MAPE, MAE and MSE, the predictive performance of the ARIMA models was close to

but generally slightly worse than that of our proposed LSTM model (see Table 9, where for convenience

we have also inlcuded the LSTM-based error metrics from Table 4). A Diebold-Mariano (DM) test was

performed to determine whether the LSTM price forecasts were more or less accurate than those of the

ARIMA model(s). Under the null hypothesis that the ARIMA forecast was more accurate than that of the

LSTM-based model, the test statistics and corresponding p-values for the S&P500, DJIA, NASDAQ and

25

S&P 500

DJIA

NASDAQ

R2000

Metrics ARI MA2,1,1

LSTM ARI MA3,2,2

LSTM ARI MA3,2,1
51.05%

50.50%

49.69%

0.7%

22.81

0.66%

11.07

1983.18

256

0.6%

104.04

23555

51%

0.6%

95.33

19047

50.88%

1.08%

42, 28

3798

LSTM ARI MA2,1,0
50.47%

49.88%

LSTM

50.02%

0.8%

30.8

1893

0.9%

9.17

147.6

0.9%

9.01

142

99.94%

99.94%

99.91%

99.94%

99.89%

99.95%

99.88%

99.9%

MDA

MAPE

MAE

MSE
R2

Table 9: Error metrics for the LSTM and ARIMA models. The indices on the ARIMA models (p,d,q) denote the number of AR(p),
diﬀerence (d) and MA(q) terms, respectively. LSTM error metrics are from Table 4

R2000 were (DM =−4.30, p-value=0.000), (DM =34.58, p-value=1.000), (DM=26.32, p-value=1.000),

and (DM=−2.86, p-value=0.002), respectively. This indicates that at 5% conﬁdence level the LSTM model

was a better (one-step) predictor than the ARIMA model only in the cases of the S&P500 and the R2000

but of course, as we have alluded to earlier, proﬁts – rather than statistical measures of accuracy – should

be used to evaluate stock market forecasts (Leitch and Tanner, 1991).

Turning to the central issue of proﬁtability, we can elucidate the eﬀect of the proposed trading strategy

alone, by examining the performance “boost” it gives the LSTM and ARIMA models, compared to when

those same models are paired with the directional “up-down” trading strategy where each trading day we

buy if the predicted price is greater than the current price, and sell if it is lower. The proﬁtability (cumula-

tive returns) of each model-strategy combination is shown in Table 10, for each of the four stock indices.

Unsurprisingly, the linear ARIMA model was well behind the LSTM model in terms of cumulative returns;

its returns also had roughly twice the annualized standard deviation19. Two things are worth observing,

however. First, our proposed trading and asset allocation policy leads to a signiﬁcant boost in cumula-

tive returns with both models, sometimes allowing even the (simplistic) ARIMA model to outperform the

buy-and-hold strategy (S&P500, NASDAQ). Second, when using the “up-down” strategy, the LSTM model

signiﬁcantly outperforms the ARIMA model for all stock indices, despite the fact that the two models were

similar in terms of accuracy, as previously discussed; in that case, however, the LSTM-based returns were

much weaker relative to the buy-and-hold approach (or indeed to the studies mentioned in Section 5.2.1).

The fact that the LSTM model can perform strongly (Section 5.2) but not when used with a strategy other

than the one proposed here, highlights the fact that it is fruitful to treat the design of the prediction model

19under the proposed trading strategy, AV for ARIMA vs LSTM was 44.78% vs 24.3%, 50.69% vs 24.6%, 76.84% vs 38.3%,

and 60.91% vs 33%, for the S&P500, DJIA, NASDAQ and R200, respectively.

26

Trading Strategies

Proposed

“Up-Down”

339.6%

194.5%

83.56%

58.15%

185.1%

51.65%

85.39%

16.49%

370.9%

293.03%

115.2%

83.35%

360.6%

42.39%

111.4%

37.96%

S&P 500

LSTM

DJIA

LSTM

NASDAQ LSTM

R2000

LSTM

ARI MA2,1,1
BnH: 136.4%

ARI MA3,2,1
BnH: 136.6%

ARI MA3,2,2
BnH: 228.9%

ARI MA2,1,0
BnH: 163.5%

Table 10: Cumulative returns over the out-of-sample period (1/04/2010-5/1/2018) using the proposed trading and allocation policy
versus the directional “Up-Down” strategy. LSTM stands for our proposed model while BnH represents the buy-and-hold strategy

in which the asset is bought once at the beginning of the trading period and sold at the end.

and trading strategy as a joint problem, despite the diﬃculty involved.

6. Conclusions and future work

Motivated by the complexity involved in designing eﬀective models for stock price prediction together

with accompanying trading strategies, as well as the prevalence of “directional” approaches, we presented

a simple LSTM-based model for predicting asset prices, together with a strategy that takes advantage of the

model’s predictions in order to make proﬁtable trades. Our approach is not focused solely on constructing a

more precise or more directionally accurate (in terms of whether the price will rise or fall) model; instead,

we exploit the distribution of the model’s predicted returns, and the fact that the “location” of a prediction

within that distribution carries information about the expected proﬁtability of a trade to be executed based on

that prediction. The trading policy we described departs from the oft-used directional “up/down” strategy,

allowing us to harness more of the information contained within the model’s predictions, even then that

model is relatively simple.

Our proposed model architecture consists of a simple yet eﬀective deep LSTM neural network, which

uses a small number of features related to an asset’s price over a relatively small number of time steps in

27

the past (ranging from 11 to 22), in order to predict the price in the next time period. Novel aspects of our

approach include: i) the use of the entire history of the LSTM cell’s hidden states as inputs to the output

layer, as the network is exposed to an input sequence, ii) a trading strategy that takes advantage of useful

information that becomes available on the proﬁtability of a trade once we condition on the predicted return’s

position within its own distribution, and iii) selecting the network’s hyper-parameters to identify the most

proﬁtable model variant (in conjunction with the trading strategy used), instead of the one with lowest pre-

diction error, recognizing the fact that predictive accuracy alone does not guarantee proﬁtability. Our design

choices with respect to the LSTM network allowed us to “economize” on the architecture of the prediction

model, using 2-3 LSTM layers, input sequences of 11-22 time-steps, and 32-128 neurons, depending on the

stock index studied. Our model is frugal in terms of training data and fast in terms of training-testing times

(updating the model and trading strategy requires between 7 and 30 seconds on a typical desktop computer),

and is thus also suitable for intra-day or multi-stock/index prediction applications.

The performance of our proposed model and trading strategy was tested on four major US stock indices,

namely the S&P500, the DJIA, the NASDAQ and the Russel 2000 over the period 10/1/2010-5/1/2018.

To the best of our knowledge, besides “beating” the indices themselves, our model and trading strategy out-

perform those proposed in several recent works using multi-year testing periods, in terms of the cumulative

or annualized returns attained while also doing well in terms of volatility and draw-down. With respect

to works that used short (e.g. single-year) testing periods, our scheme outperformed some in their chosen

comparison periods, or could achieve lower vs. higher returns if the time period was shifted, keeping in

mind that it is diﬃcult to reach safe conclusions when comparing over any one short time interval. Based

on the overall proﬁtability results and rather long testing period used in this work, we feel that our approach

shows signiﬁcant promise.

Opportunities for future work include further experimentation with more sophisticated allocation and

trading strategies, the possible inclusion of short sales, as well as the use of a variable portion of the time

history of the LSTM hidden states when making predictions, to see where the optimum lies. Also, it would

be of interest to optimize the manner in which the percentiles of the predicted return distribution are chosen

when forming our allocation strategy, in order to maximize proﬁtability and reduce risk. Finally one could

in principle use the proposed distribution-based trading strategy with any other model that outputs price

predictions, and it would be interesting to see what performance gains, if any, could be achieved.

28

References

References

Baek, Y. and Kim, H. Y. (2018). ModAugNet: A new forecasting framework for stock market index value with an overﬁtting

prevention LSTM module and a prediction LSTM module. Expert Systems with Applications, 113:457–480.

Bao, W., Yue, J., and Rao, Y. (2017). A deep learning framework for ﬁnancial time series using stacked autoencoders and long-short

Chiang, W.-C., Enke, D., Wu, T., and Wang, R. (2016). An adaptive stock index trading decision support system. Expert Systems

term memory. PLoS ONE, 12(7): e0180944.

with Applications, 59(C):195–207.

Chong, E., Han, C., and Park, F. C. (2017). Deep learning networks for stock market analysis and prediction: methodology, data

representations, and case studies. Expert Systems with Applications, 83(C):187–205.

Deng, Y., Bao, F., Kong, Y., Ren, Z., and Dai, Q. (2017). Deep direct reinforcement learning for ﬁnancial signal representation and

trading. IEEE Transactions on Neural Networks and Learning Systems, 28(3):653–664.

Ding, X., Zhang, Y., Liu, T., and Duan, J. (2015). Deep learning for event-driven stock prediction. In Proceedings of the 24th

International Conference on Artiﬁcial Intelligence, IJCAI’15, pages 2327–2333. AAAI Press.

Fama, E. F. and French, K. R. (1993). Common risk factors in the returns on stocks and bonds. Journal of Financial Economics,

Fama, E. F. and French, K. R. (2004). The capital asset pricing model: Theory and evidence. Journal of Economic Perspectives,

Fischer, T. and Krauss, C. (2018). Deep learning with long short-term memory networks for ﬁnancial market predictions. European

Journal of Operational Research, 270(2):654–669.

Foster, G., Vaswani, A., Uszkoreit, J., Macherey, W., Kaiser, L., Firat, O., Jones, L., Shazeer, N., Wu, Y., Bapna, A., Johnson, M.,

Schuster, M., Chen, Z., Hughes, M., Parmar, N., and Chen, M. X. (2018). The best of both worlds: Combining recent advances

in neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,

ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 76–86.

Gu, S., Kelly, B. T., and Xiu, D. (2018). Empirical asset pricing via machine learning. Paper in Swiss Finance Institute Research

Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735–1780.

Huck, N. (2009). Pairs selection and outranking: An application to the S&P 100 index. European Journal of Operational Research,

Huck, N. (2010). Pairs trading and outranking: The multi-step-ahead forecasting case. European Journal of Operational Research,

33(1):3–56.

18(3):25–46.

Paper No. 18-71.

196(2):819–825.

207(3):1702–1716.

Krauss, C., Do, X. A., and Huck, N. (2017). Deep neural networks, gradient-boosted trees, random forests: Statistical arbitrage on

the S&P 500. European Journal of Operational Research, 259(2):689–702.

Leitch, G. and Tanner, J. E. (1991). Economic forecast evaluation: Proﬁts versus the conventional error measures. American

Economic Review, 81(3):580–90.

Minh, D. L., Sadeghi-Niaraki, A., Huy, H. D., Min, K., and Moon, H. (2018). Deep learning approach for short-term stock trends

prediction based on two-stream gated recurrent unit network. IEEE Access, 6:55392–55404.

Olah, C.

(2018).

Understanding LSTM Networks, August 17 (2015).

http://colah.github.io/posts/

2015-08-Understanding-LSTMs/[Accessed: 1st May 2018].

Rather, A. M., Agarwal, A., and Sastry, V. (2015). Recurrent neural network and a hybrid model for prediction of stock returns.

Expert Systems with Applications, 42(6):3234–3241.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg,

29

A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision

Sermpinis, G., Theoﬁlatos, K., Karathanasopoulos, A., Georgopoulos, E. F., and Dunis, C. (2013). Forecasting foreign exchange

rates with adaptive neural networks using radial-basis functions and Particle Swarm Optimization. European Journal of Oper-

(IJCV), 115(3):211–252.

ational Research, 225(3):528–540.

Sethi, M., Treleaven, P., and Rollin, S. D. B. (2014). Beating the S&P 500 index a successful neural network approach. In 2014

International Joint Conference on Neural Networks (IJCNN), pages 3074–3077.

Sezer, O. and Ozbayoglu, M. (2018). Algorithmic ﬁnancial trading with deep convolutional neural networks: Time series to image

conversion approach. Applied Soft Computing, 70:525–538.

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen,

Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., and Hassabis, D. (2017). Mastering the game of Go

without human knowledge. Nature, 550:354359.

Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J.,

Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang,

W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. (2016). Google’s neural
machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.

YahooFinance (2018). Yahoo Finance, Symbol Lookup (2018). https://finance.yahoo.com/quote/[Accessed: 1st May

Zhong, X. and Enke, D. (2017). Forecasting daily stock market return using dimensionality reduction. Expert Systems with

2018].

Applications, 67:126–139.

Zhou, F., Zhou, H., Yang, Z., and Yang, L. (2019). EMD2FNN: A strategy combining empirical mode decomposition and factor-

ization machine based neural network for stock market trend prediction. Expert Systems with Applications, 115:136–151.

30

