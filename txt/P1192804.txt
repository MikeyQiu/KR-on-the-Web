7
1
0
2
 
r
a

M
 
1
2
 
 
]
L
C
.
s
c
[
 
 
2
v
6
0
9
3
0
.
3
0
7
1
:
v
i
X
r
a

Massive Exploration of Neural Machine Translation
Architectures

Denny Britz∗†, Anna Goldie∗, Minh-Thang Luong, Quoc Le
{dennybritz,agoldie,thangluong,qvl}@google.com
Google Brain

Abstract

the ﬁrst

Neural Machine Translation (NMT) has
shown remarkable progress over the past
few years with production systems now
being deployed to end-users. One major
drawback of current architectures is that
they are expensive to train, typically re-
quiring days to weeks of GPU time to
converge. This makes exhaustive hyper-
parameter search, as is commonly done
with other neural network architectures,
In this work,
prohibitively expensive.
we present
large-scale analy-
sis of NMT architecture hyperparameters.
We report empirical results and variance
numbers for several hundred experimental
runs, corresponding to over 250,000 GPU
hours on the standard WMT English to
German translation task. Our experiments
lead to novel insights and practical advice
for building and extending NMT architec-
tures. As part of this contribution, we
release an open-source NMT framework1
that enables researchers to easily experi-
ment with novel techniques and reproduce
state of the art results.

1

Introduction

Neural Machine Translation (NMT) (Kalchbren-
ner and Blunsom, 2013; Sutskever et al., 2014;
Cho et al., 2014) is an end-to-end approach to
automated translation. NMT has shown impres-
sive results (Jean et al., 2015; Luong et al., 2015b;
Sennrich et al., 2016a; Wu et al., 2016) sur-
passing those of phrase-based systems while ad-
dressing shortcomings such as the need for hand-

∗Both authors contributed equally to this work.
†Work done as a member of the Google Brain Residency

program (g.co/brainresidency).
1https://github.com/google/seq2seq/

1

engineered features. The most popular approaches
to NMT are based on an encoder-decoder architec-
ture consisting of two recurrent neural networks
(RNNs) and an attention mechanism that aligns
target with source tokens (Bahdanau et al., 2015;
Luong et al., 2015a).

One shortcoming of current NMT architectures
is the amount of compute required to train them.
Training on real-world datasets of several million
examples typically requires dozens of GPUs and
convergence time is on the order of days to weeks
(Wu et al., 2016). While sweeping across large hy-
perparameter spaces is common in Computer Vi-
sion (Huang et al., 2016b), such exploration would
be prohibitively expensive for NMT models, lim-
iting researchers to well-established architectures
and hyperparameter choices. Furthermore, there
have been no large-scale studies of how architec-
tural hyperparameters affect the performance of
NMT systems. As a result, it remains unclear why
these models perform as well as they do, as well
as how we might improve them.

In this work, we present the ﬁrst comprehen-
sive analysis of architectural hyperparameters for
Neural Machine Translation systems. Using a to-
tal of more than 250,000 GPU hours, we explore
common variations of NMT architectures and pro-
vide insight into which architectural choices mat-
ter most. We report BLEU scores, perplexities,
model sizes, and convergence time for all ex-
periments, including variance numbers calculated
across several runs of each experiment.
In ad-
dition, we release to the public a new software
framework that was used to run the experiments.

In summary, the main contributions of this work

are as follows:

• We provide immediately applicable insights
into the optimization of Neural Machine
Translation models, as well as promising di-
rections for future research. For example, we

found that deep encoders are more difﬁcult
to optimize than decoders, that dense resid-
ual connections yield better performance than
regular residual connections,
that LSTMs
outperform GRUs, and that a well-tuned
beam search is crucial to obtaining state of
the art results. By presenting practical advice
for choosing baseline architectures, we help
researchers avoid wasting time on unpromis-
ing model variations.

• We also establish the extent to which metrics
such as BLEU are inﬂuenced by random ini-
tialization and slight hyperparameter varia-
tion, helping researchers to distinguish statis-
tically signiﬁcant results from random noise.

• Finally, we release an open source package
based on TensorFlow, speciﬁcally designed
for implementing reproducible state of the
art sequence-to-sequence models. All experi-
ments were run using this framework and we
hope to accelerate future research by releas-
ing it to the public. We also release all con-
ﬁguration ﬁles and processing scripts needed
to reproduce the experiments in this paper.

2 Background and Preliminaries

2.1 Neural Machine Translation

Our models are based on an encoder-decoder ar-
chitecture with attention mechanism (Bahdanau
et al., 2015; Luong et al., 2015a), as shown in ﬁg-
ure 1. An encoder function fenc takes as input a
sequence of source tokens x = (x1, ..., xm) and
produces a sequence of states h = (h1, ..., hm).
In our base model, fenc is a bi-directional RNN
and the state hi corresponds to the concatenation
of the states produced by the backward and for-
←−
ward RNNs, hi = [
hi ] .The decoder fdec is
an RNN that predicts the probability of a target
sequence y = (y1, ..., yk) based on h. The proba-
bility of each target token yi ∈ 1, ...V is predicted
based on the recurrent state in the decoder RNN
si, the previous words, y<i, and a context vector
ci. The context vector ci is also called the atten-
tion vector and is calculated as a weighted average
of the source states.

−→
hi ;

(cid:88)

ci =

aijhj

j

ˆaij
j ˆaij

(cid:80)

aij =

ˆaij = att(si, hj)

(1)

(2)

(3)

Here, att(si, hj) is an attention function that
calculates an unnormalized alignment score be-
tween the encoder state hj and the decoder state
si. In our base model, we use a function of the
form att(si, hj) = (cid:104)Whhj, Wssi(cid:105), where the ma-
trices W are used to transform the source and tar-
get states into a representation of the same size.

The decoder outputs a distribution over a vocab-

ulary of ﬁxed-size V :

P (yi|y1, ..., yi−1, x)
= softmax(W [si; ci] + b)

The whole model is trained end-to-end by min-
imizing the negative log likelihood of the target
words using stochastic gradient descent.

3 Experimental Setup

3.1 Datasets and Preprocessing

run all

We
experiments on the WMT’15
English→German task consisting of 4.5M sen-
tence pairs, obtained by combining the Europarl
v7, News Commentary v10, and Common Crawl
corpora. We use newstest2013 as our validation
set and newstest2014 and newstest2015 as our test
sets. To test for generality, we also ran a small
number of experiments on English→French trans-
lation, and we found that the performance was
highly correlated with that of English→German
but that it took much longer to train models on the
larger English→French dataset. Given that trans-
lation from the morphologically richer German is
also considered a more challenging task, we felt
justiﬁed in using the English→German translation
task for this hyperparameter sweep.

We tokenize and clean all datasets with the
scripts in Moses2 and learn shared subword units
using Byte Pair Encoding (BPE) (Sennrich et al.,
2016b) using 32,000 merge operations for a ﬁnal
vocabulary size of approximately 37k. We discov-
ered that data preprocessing can have a large im-
pact on ﬁnal numbers, and since we wish to enable

2https://github.com/moses-smt/mosesdecoder/

Figure 1: Encoder-Decoder architecture with attention module. Section numbers reference experiments
corresponding to the components.

reproducibility, we release our data preprocessing
scripts together with the NMT framework to the
public. For more details on data preprocessing pa-
rameters, we refer the reader to the code release.

3.2 Training Setup and Software

All of the following experiments are run using
our own software framework based on Tensor-
Flow (Abadi et al., 2016). We purposely built
this framework to enable reproducible state-of-
the-art implementations of Neural Machine Trans-
lation architectures. As part of our contribution,
we are releasing the framework and all conﬁgura-
tion ﬁles needed to reproduce our results. Train-
ing is performed on Nvidia Tesla K40m and Tesla
K80 GPUs, distributed over 8 parallel workers and
6 parameter servers per experiment. We use a
batch size of 128 and decode using beam search
with a beam width of 10 and the length normaliza-
tion penalty of 0.6 described in (Wu et al., 2016).
BLEU scores are calculated on tokenized data us-
ing the multi-bleu.perl script in Moses3. Each ex-
periment is run for a maximum of 2.5M steps and
replicated 4 times with different initializations.
We save model checkpoints every 30 minutes and
choose the best checkpoint based on the validation
set BLEU score. We report mean and standard de-

3https://github.com/moses-

smt/mosesdecoder/blob/master/scripts/generic/multi-
bleu.perl

viation as well as highest scores (as per cross val-
idation) for each experiment.

3.3 Baseline Model

Based on a review of previous literature, we chose
a baseline model that we knew would perform rea-
sonably well. Our goal was to keep the baseline
model simple and standard, not to advance the
start of the art. The model (described in 2.1) con-
sists of a 2-layer bidirectional encoder (1 layer in
each direction), and a 2 layer decoder with a mul-
tiplicative (Luong et al., 2015a) attention mecha-
nism. We use 512-unit GRU (Cho et al., 2014)
cells for both the encoder and decoder and apply
Dropout of 0.2 at the input of each cell. We train
using the Adam optimizer and a ﬁxed learning rate
of 0.0001 without decay. The embedding dimen-
sionality is set to 512. A more detailed description
of all model hyperparameters can be found in the
supplementary material.

In each of the following experiments, the hy-
perparameters of the baseline model are held con-
stant, except for the one hyperparameter being
studied. We hope that this allows us to isolate the
effect of various hyperparameter changes. We rec-
ognize that this procedure does not account for in-
teractions between hyperparameters, and we per-
form additional experiments when we believe such
interactions are likely to occur (e.g. skip connec-
tions and number of layers).

4 Experiments and Discussion

For the sake of brevity, we only report mean
BLEU, standard deviation, highest BLEU in
parantheses, and model size in the following ta-
bles. Log perplexity, tokens/sec and convergence
times can be found in the supplementary material
tables.

4.1 Embedding Dimensionality

With a large vocabulary, the embedding layer can
account for a large fraction of the model param-
eters. Historically, researchers have used 620-
dimensional (Bahdanau et al., 2015) or 1024-
dimensional (Luong et al., 2015a) embeddings.
We expected larger embeddings to result in bet-
ter BLEU scores, or at least lower perplexities, but
we found that this wasn’t always the case. While
Table 1 shows that 2048-dimensional embeddings
yielded the overall best result, they only did so
by a small margin. Even small 128-dimensional
embeddings performed surprisingly well, while
converging almost twice as quickly. We found
that gradient updates to both small and large em-
beddings did not differ signiﬁcantly and that the
norm of gradient updates to the embedding matrix
stayed approximately constant throughout train-
ing regardless of size. We also did not observe
overﬁtting with large embeddings and training log
perplexity was approximately equal across exper-
iments, suggesting that the model does not make
efﬁcient use of the extra parameters and that there
may be a need for better optimization techniques.
Alternatively, it could be the case that models with
large embeddings simply need much more than
2.5M steps to converge to the best solution.

Dim newstest2013
21.50 ± 0.16 (21.66)
128
21.73 ± 0.09 (21.85)
256
21.78 ± 0.05 (21.83)
512
21.36 ± 0.27 (21.67)
1024
2048 21.86 ± 0.17 (22.08)

Params
36.13M
46.20M
66.32M
106.58M
187.09M

Table 1: BLEU scores on newstest2013, varying
the embedding dimensionality.

on small sequence tasks of a few thousand exam-
ples, we are not aware of such studies in large-
scale NMT settings.

A motivation for gated cells such as the GRU
and LSTM is the vanishing gradient problem.
Using vanilla RNN cells, deep networks cannot
efﬁciently propagate information and gradients
through multiple layers and time steps. However,
with an attention-based model, we believe that the
decoder should be able to make decisions almost
exclusively based on the current input and the at-
tention context and we hypothesize that the gating
mechanism in the decoder is not strictly necessary.
This hypothesis is supported by the fact that we
always initialize the decoder state to zero instead
of passing the encoder state, meaning that the de-
coder state does not contain information about the
encoded source. We test our hypothesis by using
a vanilla RNN cell in the decoder only (Vanilla-
Dec below). For the LSTM and GRU variants we
replace cells in both the encoder and decoder. We
use LSTM cells without peephole connections and
initialize the forget bias of both LSTM and GRU
cells to 1.

Cell
LSTM
GRU
Vanilla-Dec

newstest2013
22.22 ± 0.08 (22.33)
21.78 ± 0.05 (21.83)
15.38 ± 0.28 (15.73)

Params
68.95M
66.32M
63.18M

Table 2: BLEU scores on newstest2013, varying
the type of encoder and decoder cell.

In our experiments, LSTM cells consistently
outperformed GRU cells. Since the computational
bottleneck in our architecture is the softmax opera-
tion we did not observe large difference in training
speed between LSTM and GRU cells. Somewhat
to our surprise, we found that the vanilla decoder
is unable to learn nearly as well as the gated vari-
ant. This suggests that the decoder indeed passes
information in its own state throughout multiple
time steps instead of relying solely on the atten-
tion mechanism and current input (which includes
the previous attention context).
It could also be
the case that the gating mechanism is necessary to
mask out irrelevant parts of the inputs.

4.2 RNN Cell Variant

Both LSTM (Hochreiter and Schmidhuber, 1997)
and GRU (Cho et al., 2014) cells are commonly
used in NMT architectures. While there exist stud-
ies (Greff et al., 2016) that explore cell variants

4.3 Encoder and Decoder Depth

We generally expect deeper networks to converge
to better solutions than shallower ones (He et al.,
2016). While some work (Luong et al., 2015b;

Zhou et al., 2016; Luong and Manning, 2016; Wu
et al., 2016) has achieved state of the art results
using deep networks, others (Jean et al., 2015;
Chung et al., 2016; Sennrich et al., 2016b) have
achieved similar results with far shallower ones.
Hence, it is unclear how important depth is, and
whether shallow networks are capable of produc-
ing results competitive with those of deep net-
works. Here, we explore the effect of both encoder
and decoder depth up to 8 layers. For the bidi-
rectional encoder, we separately stack the RNNs
in both directions. For example, the Enc-8 model
corresponds to one forward and one backward 4-
layer RNN. For deeper networks, we also exper-
iment with two variants of residual connections
(He et al., 2016) to encourage gradient ﬂow. In the
standard variant, shown in equation (4), we insert
residual connections between consecutive layers.
If h(l)
, h(l)
t (x(l)
t−1) is the RNN output of layer l at
t
time step t, then:

x(l+1)
t

= h(l)

t (x(l)

t

, h(l)

t−1) + x(l)

t

(4)

where x(0)

t

are the embedded input

tokens.
We also explore a dense (”ResD” below) variant
of residual connections similar to those used by
In
(Huang et al., 2016a) in Image Recognition.
this variant, we add skip connections from each
layer to all other layers:

x(l+1)
t

= h(l)

t (x(l)

t

, h(l)

t−1) +

x(j)
t

(5)

l
(cid:88)

j=0

Our implementation differs from (Huang et al.,
2016a) in that we use an addition instead of a con-
catenation operation in order to keep the state size
constant.

Table 3 shows results of varying encoder and
decoder depth with and without residual connec-
tion. We found no clear evidence that encoder
depth beyond two layers is necessary, but found
deeper models with residual connections to be sig-
niﬁcantly more likely to diverge during training.
The best deep residual models achieved good re-
sults, but only one of four runs converged, as sug-
gested by the large standard deviation.

On the decoder side, deeper models outper-
formed shallower ones by a small margin, and
we found that without residual connections,
it
was impossible for us to train decoders with 8

Figure 2: Training plots for deep decoder with and
without residual connections, showing log per-
plexity on the eval set.

newstest2013
Depth
21.78 ± 0.05 (21.83)
Enc-2
21.85 ± 0.32 (22.23)
Enc-4
21.32 ± 0.14 (21.51)
Enc-8
19.23 ± 1.96 (21.97)
Enc-8-Res
Enc-8-ResD 17.30 ± 2.64 (21.03)
21.76 ± 0.12 (21.93)
Dec-1
21.78 ± 0.05 (21.83)
Dec-2
22.37 ± 0.10 (22.51)
Dec-4
17.48 ± 0.25 (17.82)
Dec-4-Res
Dec-4-ResD 21.10 ± 0.24 (21.43)
01.42 ± 0.23 (1.66)
Dec-8
16.99 ± 0.42 (17.47)
Dec-8-Res
Dec-8-ResD 20.97 ± 0.34 (21.42)

Params
66.32M
69.47M
75.77M
75.77M
75.77M
64.75M
66.32M
69.47M
68.69M
68.69M
75.77M
75.77M
75.77M

Table 3: BLEU scores on newstest2013, varying
the encoder and decoder depth and type of residual
connections.

or more layers. Across the deep decoder exper-
iments, dense residual connections consistently
outperformed regular residual connections and
converged much faster in terms of step count, as
shown in ﬁgure 2. We expected deep models to
perform better (Zhou et al., 2016; Szegedy et al.,
2015) across the board, and we believe that our
experiments demonstrate the need for more robust
techniques for optimizing deep sequential models.
For example, we may need a better-tuned SGD op-
timizer or some form of batch normalization, in
order to robustly train deep networks with residual
connections.

4.4 Unidirectional vs. Bidirectional Encoder

In the literature, we see bidirectional encoders
(Bahdanau et al., 2015), unidirectional encoders

(Luong et al., 2015a), and a mix of both (Wu
et al., 2016) being used. Bidirectional encoders
are able to create representations that take into ac-
count both past and future inputs, while unidirec-
tional encoders can only take past inputs into ac-
count. The beneﬁt of unidirectional encoders is
that their computation can be easily parallelized on
GPUs, allowing them to run faster than their bidi-
rectional counterparts. We are not aware of any
studies that explore the necessity of bidirectional-
ity. In this set of experiments, we explore unidirec-
tional encoders of varying depth with and without
reversed source inputs, as this is a commonly used
trick that allows the encoder to create richer repre-
sentations for earlier words. Given that errors on
the decoder side can easily cascade, the correct-
ness of early words has disproportionate impact.

newstest2013
Cell
21.78 ± 0.05 (21.83)
Bidi-2
20.54 ± 0.16 (20.73)
Uni-1
Uni-1R 21.16 ± 0.35 (21.64)
20.98 ± 0.10 (21.07)
Uni-2
Uni-2R 21.76 ± 0.21 (21.93)
21.47 ± 0.22 (21.70)
Uni-4
Uni-4R 21.32 ± 0.42 (21.89)

Params
66.32M
63.44M
63.44M
65.01M
65.01M
68.16M
68.16M

Table 4: BLEU scores on newstest2013, varying
the type of encoder. The ”R” sufﬁx indicates a
reversed source sequence.

Table 4 shows that bidirectional encoders gen-
erally outperform unidirectional encoders, but not
by a large margin. The encoders with reversed
source consistently outperform their non-reversed
counterparts, but do not beat shallower bidirec-
tional encoders.

4.5 Attention Mechanism

The two most commonly used attention mecha-
nisms are the additive (Bahdanau et al., 2015) vari-
ant, equation (6) below, and the computationally
less expensive multiplicative variant (Luong et al.,
2015a), equation (7) below. Given an attention key
hj (an encoder state) and attention query si (a de-
coder state), the attention score for each pair is cal-
culated as follows:

score(hj, si) = (cid:104)v, tanh(W1hj + W2si)(cid:105)
score(hj, si) = (cid:104)W1hj, W2si(cid:105)

(6)

(7)

We call the dimensionality of W1hj and W2si
the ”attention dimensionality” and vary it from
128 to 1024 by changing the layer size. We also
experiment with using no attention mechanism by
initializing the decoder state with the last encoder
state (None-State), or concatenating the last de-
coder state to each decoder input (None-Input).
The results are shown in Table 5.

Attention
Mul-128
Mul-256
Mul-512
Mul-1024
Add-128
Add-256
Add-512
Add-1028
None-State
None-Input

newstest2013
22.03 ± 0.08 (22.14)
22.33 ± 0.28 (22.64)
21.78 ± 0.05 (21.83)
18.22 ± 0.03 (18.26)
22.23 ± 0.11 (22.38)
22.33 ± 0.04 (22.39)
22.47 ± 0.27 (22.79)
22.10 ± 0.18 (22.36)
9.98 ± 0.28 (10.25)
11.57 ± 0.30 (11.85)

Params
65.73M
65.93M
66.32M
67.11M
65.73M
65.93M
66.33M
67.11M
64.23M
64.49M

Table 5: BLEU scores on newstest2013, varying
the type of attention mechanism.

We found that the parameterized additive atten-
tion mechanism slightly but consistently outper-
formed the multiplicative one, with the attention
dimensionality having little effect.

While we did expect the attention-based mod-
els to signiﬁcantly outperform those without an
attention mechanism, we were surprised by just
how poorly the ”Non-Input” models fared, given
that they had access to encoder information at
each time step.
Furthermore, we found that
the attention-based models exhibited signiﬁcantly
larger gradient updates to decoder states through-
out training. This suggests that the attention mech-
anism acts more like a ”weighted skip connection”
that optimizes gradient ﬂow than like a ”memory”
that allows the encoder to access source states, as
is commonly stated in the literature. We believe
that further research in this direction is necessary
to shed light on the role of the attention mecha-
nism and whether it may be purely a vehicle for
easier optimization.

4.6 Beam Search Strategies

Beam Search is a commonly used technique to
ﬁnd target sequences that maximize some scoring
function s(y, x) through tree search. In the sim-
plest case, the score to be maximized is the log
probability of the target sequence given the source.

Recently, extensions such as coverage penalties
(Tu et al., 2016) and length normalizations (Wu
et al., 2016) have been shown to improve decod-
ing results. It has also been observed (Tu et al.,
2017) that very large beam sizes, even with length
penalty, perform worse than smaller ones. Thus,
choosing the correct beam width can be crucial to
achieving the best results.

Beam
newstest2013
20.66 ± 0.31 (21.08)
B1
21.55 ± 0.26 (21.94)
B3
21.60 ± 0.28 (22.03)
B5
21.57 ± 0.26 (21.91)
B10
21.47 ± 0.30 (21.77)
B25
21.10 ± 0.31 (21.39)
B100
21.71 ± 0.25 (22.04)
B10-LP-0.5
B10-LP-1.0 21.80 ± 0.25 (22.16)

Params
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M

Table 6: BLEU scores on newstest2013, varying
the beam width and adding length penalties (LP).

Table 6 shows the effect of varying beam widths
and adding length normalization penalties. A
beam width of 1 corresponds to greedy search. We
found that a well-tuned beam search is crucial to
achieving good results, and that it leads to consis-
tent gains of more than one BLEU point. Similar
to (Tu et al., 2017) we found that very large beams
yield worse results and that there is a ”sweet spot”
of optimal beam width. We believe that further re-
search into the robustness of hyperparameters in
beam search is crucial to progress in NMT. We
also experimented with a coverage penalty, but
found no additional gain over a sufﬁciently large
length penalty.

4.7 Final System Comparison

Finally, we compare our best performing model
across all experiments (base model with 512-
dimensional additive attention), as chosen on the
newstest2013 validation set, to historical results
found in the literature in Table 8. While not the
focus on this work, we were able to achieve fur-
ther improvements by combining all of our in-
sights into a single model described in Table 7.

Although we do not offer architectural innova-
tions, we do show that through careful hyperpa-
rameter tuning and good initialization, it is pos-
sible to achieve state of the art performance on
standard WMT benchmarks. Our model is outper-
formed only by (Wu et al., 2016), a model which

Hyperparameter Value
embedding dim
rnn cell variant
encoder depth
decoder depth
attention dim
attention type
encoder
beam size
length penalty

512
LSTMCell
4
4
512
Bahdanau
bidirectional
10
1.0

Table 7: Hyperparameter settings for our ﬁnal
combined model, consisting of all of the individu-
ally optimized values.

is signiﬁcantly more complex and lacks a public
implementation.

Model
Ours (experimental)
Ours (combined)
OpenNMT
Luong
BPE-Char
BPE
RNNSearch-LV
RNNSearch
Deep-Att*
GNMT*
Deep-Conv*

newstest14
22.03
22.19
19.34
20.9
21.5
-
19.4
-
20.6
24.61
-

newstest15
24.75
25.23
-
-
23.9
20.5
-
16.5
-
-
24.3

Table 8: Comparison to RNNSearch (Jean et al.,
2015), RNNSearch-LV (Jean et al., 2015), BPE
(Sennrich et al., 2016b), BPE-Char (Chung et al.,
2016), Deep-Att (Zhou et al., 2016), Luong (Lu-
ong et al., 2015a), Deep-Conv (Gehring et al.,
2016), GNMT (Wu et al., 2016), and OpenNMT
(Klein et al., 2017). Systems with an * do not have
a public implementation.

5 Open Source Release

We demonstrated empirically how small changes
to hyperparameter values and different initializa-
tion can affect results, and how seemingly triv-
ial factors such as a well-tuned beam search are
crucial. To move towards reproducible research,
we believe it is important that researchers start
building upon common frameworks and data pro-
cessing pipelines. With this goal in mind, we
speciﬁcally built a modular software framework

that allows researchers to explore novel archi-
tectures with minimal code changes, and deﬁne
experimental parameters in a reproducible man-
ner. While our initial experiments are in Ma-
chine Translation, our framework can easily be
adapted to problems in Summarization, Conversa-
tional Modeling or Image-To-Text. Systems such
as OpenNMT (Klein et al., 2017) share similar
goals, but do not yet achieve state of the art re-
sults (see Table 8) and lack what we believe to be
crucial features, such as distributed training sup-
port. We hope that by open sourcing our experi-
mental toolkit, we enable the ﬁeld to make more
rapid progress in the future.
is

freely available

All of our

code

at

https://github.com/google/seq2seq/.

6 Conclusion

We conducted what we believe to be the ﬁrst large-
scale analysis of architecture variations for Neural
Machine Translation, teasing apart the key factors
to achieving state of the art results. We demon-
strated a number of surprising insights, including
the fact that beam search tuning is just as crucial
as most architectural variations, and that with cur-
rent optimization techniques deep models do not
always outperform shallow ones. Here, we sum-
marize our practical ﬁndings:

• Large embeddings with 2048 dimensions
achieved the best results, but only by a small
margin. Even small embeddings with 128 di-
mensions seem to have sufﬁcient capacity to
capture most of the necessary semantic infor-
mation.

• A well-tuned beam search with length
penalty is crucial. Beam widths of 5 to 10
together with a length penalty of 1.0 seemed
to work well.

We highlighted several important research ques-
tions, including the efﬁcient use of embedding pa-
rameters (4.1), the role of attention mechanisms
as weighted skip connections (4.5) as opposed to
memory units, the need for better optimization
methods for deep recurrent networks (4.3), and the
need for a better beam search (4.6) robust to hyper-
parameter variations.

In addition, we release to the public an open
source NMT framework speciﬁcally built to ex-
plore architectural innovations and generate re-
producible experiments, along with conﬁguration
ﬁles for all our experiments.

Acknowledgments

We would like to thank Eugene Brevdo for adapt-
ing the TensorFlow RNN APIs in a way that
allowed us to write our framework much more
cleanly. We are also grateful to Andrew Dai and
Samy Bengio for their helpful feedback.

References

Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-
sorFlow: A system for large-scale machine learning.
In OSDI.

• LSTM Cells consistently outperformed GRU

Cells.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

• Bidirectional encoders with 2 to 4 layers per-
formed best. Deeper encoders were signiﬁ-
cantly more unstable to train, but show po-
tential if they can be optimized well.

• Deep 4-layer decoders slightly outperformed
shallower decoders. Residual connections
were necessary to train decoders with 8 lay-
ers and dense residual connections offer ad-
ditional robustness.

• Parameterized additive attention yielded the

overall best results.

Kyunghyun Cho, Bart van Merrienboer, C¸ aglar
G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. In EMNLP.

Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. A character-level decoder without ex-
plicit segmentation for neural machine translation.
In ACL.

Jonas Gehring, Michael Auli, David Grangier, and
Yann N. Dauphin. 2016.
A convolutional en-
coder model for neural machine translation. CoRR
abs/1611.02344.

Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık,
Bas R. Steunebrink, and J¨urgen Schmidhuber. 2016.
IEEE Transac-
LSTM: A search space odyssey.
tions on Neural Networks and Learning Systems
PP(99):1–11.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott E. Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. 2015. Going deeper with convolutions. In
CVPR.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In CVPR.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2017. Neural machine translation with
reconstruction. In AAAI.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In ACL.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR
abs/1609.08144.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei
Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. Trans-
actions of the Association for Computational Lin-
guistics 4:371–383.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural Computation

Long short-term memory.
9(8):1735–1780.

Gao Huang, Zhuang Liu, and Kilian Q. Weinberger.
2016a. Densely connected convolutional networks.
CoRR abs/1608.06993.

Jonathan Huang, Vivek Rathod, Chen Sun, Meng-
long Zhu, Anoop Korattikara, Alireza Fathi, Ian Fis-
cher, Zbigniew Wojna, Yang Song, Sergio Guadar-
rama, and Kevin Murphy. 2016b. Speed/accuracy
trade-offs for modern convolutional object detectors.
CoRR abs/1611.10012.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
ACL.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent

continuous translation models. In EMNLP.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. CoRR abs/1701.02810.

Minh-Thang Luong and Christopher D. Manning.
2016. Achieving open vocabulary neural machine
translation with hybrid word-character models.
In
ACL.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015a. Effective approaches to attention-
based neural machine translation. In EMNLP.

Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015b. Addressing
the rare word problem in neural machine translation.
In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Edinburgh neural machine translation sys-
tems for wmt 16. In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In ACL.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

7
1
0
2
 
r
a

M
 
1
2
 
 
]
L
C
.
s
c
[
 
 
2
v
6
0
9
3
0
.
3
0
7
1
:
v
i
X
r
a

Massive Exploration of Neural Machine Translation
Architectures

Denny Britz∗†, Anna Goldie∗, Minh-Thang Luong, Quoc Le
{dennybritz,agoldie,thangluong,qvl}@google.com
Google Brain

Abstract

the ﬁrst

Neural Machine Translation (NMT) has
shown remarkable progress over the past
few years with production systems now
being deployed to end-users. One major
drawback of current architectures is that
they are expensive to train, typically re-
quiring days to weeks of GPU time to
converge. This makes exhaustive hyper-
parameter search, as is commonly done
with other neural network architectures,
In this work,
prohibitively expensive.
we present
large-scale analy-
sis of NMT architecture hyperparameters.
We report empirical results and variance
numbers for several hundred experimental
runs, corresponding to over 250,000 GPU
hours on the standard WMT English to
German translation task. Our experiments
lead to novel insights and practical advice
for building and extending NMT architec-
tures. As part of this contribution, we
release an open-source NMT framework1
that enables researchers to easily experi-
ment with novel techniques and reproduce
state of the art results.

1

Introduction

Neural Machine Translation (NMT) (Kalchbren-
ner and Blunsom, 2013; Sutskever et al., 2014;
Cho et al., 2014) is an end-to-end approach to
automated translation. NMT has shown impres-
sive results (Jean et al., 2015; Luong et al., 2015b;
Sennrich et al., 2016a; Wu et al., 2016) sur-
passing those of phrase-based systems while ad-
dressing shortcomings such as the need for hand-

∗Both authors contributed equally to this work.
†Work done as a member of the Google Brain Residency

program (g.co/brainresidency).
1https://github.com/google/seq2seq/

1

engineered features. The most popular approaches
to NMT are based on an encoder-decoder architec-
ture consisting of two recurrent neural networks
(RNNs) and an attention mechanism that aligns
target with source tokens (Bahdanau et al., 2015;
Luong et al., 2015a).

One shortcoming of current NMT architectures
is the amount of compute required to train them.
Training on real-world datasets of several million
examples typically requires dozens of GPUs and
convergence time is on the order of days to weeks
(Wu et al., 2016). While sweeping across large hy-
perparameter spaces is common in Computer Vi-
sion (Huang et al., 2016b), such exploration would
be prohibitively expensive for NMT models, lim-
iting researchers to well-established architectures
and hyperparameter choices. Furthermore, there
have been no large-scale studies of how architec-
tural hyperparameters affect the performance of
NMT systems. As a result, it remains unclear why
these models perform as well as they do, as well
as how we might improve them.

In this work, we present the ﬁrst comprehen-
sive analysis of architectural hyperparameters for
Neural Machine Translation systems. Using a to-
tal of more than 250,000 GPU hours, we explore
common variations of NMT architectures and pro-
vide insight into which architectural choices mat-
ter most. We report BLEU scores, perplexities,
model sizes, and convergence time for all ex-
periments, including variance numbers calculated
across several runs of each experiment.
In ad-
dition, we release to the public a new software
framework that was used to run the experiments.

In summary, the main contributions of this work

are as follows:

• We provide immediately applicable insights
into the optimization of Neural Machine
Translation models, as well as promising di-
rections for future research. For example, we

found that deep encoders are more difﬁcult
to optimize than decoders, that dense resid-
ual connections yield better performance than
regular residual connections,
that LSTMs
outperform GRUs, and that a well-tuned
beam search is crucial to obtaining state of
the art results. By presenting practical advice
for choosing baseline architectures, we help
researchers avoid wasting time on unpromis-
ing model variations.

• We also establish the extent to which metrics
such as BLEU are inﬂuenced by random ini-
tialization and slight hyperparameter varia-
tion, helping researchers to distinguish statis-
tically signiﬁcant results from random noise.

• Finally, we release an open source package
based on TensorFlow, speciﬁcally designed
for implementing reproducible state of the
art sequence-to-sequence models. All experi-
ments were run using this framework and we
hope to accelerate future research by releas-
ing it to the public. We also release all con-
ﬁguration ﬁles and processing scripts needed
to reproduce the experiments in this paper.

2 Background and Preliminaries

2.1 Neural Machine Translation

Our models are based on an encoder-decoder ar-
chitecture with attention mechanism (Bahdanau
et al., 2015; Luong et al., 2015a), as shown in ﬁg-
ure 1. An encoder function fenc takes as input a
sequence of source tokens x = (x1, ..., xm) and
produces a sequence of states h = (h1, ..., hm).
In our base model, fenc is a bi-directional RNN
and the state hi corresponds to the concatenation
of the states produced by the backward and for-
←−
ward RNNs, hi = [
hi ] .The decoder fdec is
an RNN that predicts the probability of a target
sequence y = (y1, ..., yk) based on h. The proba-
bility of each target token yi ∈ 1, ...V is predicted
based on the recurrent state in the decoder RNN
si, the previous words, y<i, and a context vector
ci. The context vector ci is also called the atten-
tion vector and is calculated as a weighted average
of the source states.

−→
hi ;

(cid:88)

ci =

aijhj

j

ˆaij
j ˆaij

(cid:80)

aij =

ˆaij = att(si, hj)

(1)

(2)

(3)

Here, att(si, hj) is an attention function that
calculates an unnormalized alignment score be-
tween the encoder state hj and the decoder state
si. In our base model, we use a function of the
form att(si, hj) = (cid:104)Whhj, Wssi(cid:105), where the ma-
trices W are used to transform the source and tar-
get states into a representation of the same size.

The decoder outputs a distribution over a vocab-

ulary of ﬁxed-size V :

P (yi|y1, ..., yi−1, x)
= softmax(W [si; ci] + b)

The whole model is trained end-to-end by min-
imizing the negative log likelihood of the target
words using stochastic gradient descent.

3 Experimental Setup

3.1 Datasets and Preprocessing

run all

We
experiments on the WMT’15
English→German task consisting of 4.5M sen-
tence pairs, obtained by combining the Europarl
v7, News Commentary v10, and Common Crawl
corpora. We use newstest2013 as our validation
set and newstest2014 and newstest2015 as our test
sets. To test for generality, we also ran a small
number of experiments on English→French trans-
lation, and we found that the performance was
highly correlated with that of English→German
but that it took much longer to train models on the
larger English→French dataset. Given that trans-
lation from the morphologically richer German is
also considered a more challenging task, we felt
justiﬁed in using the English→German translation
task for this hyperparameter sweep.

We tokenize and clean all datasets with the
scripts in Moses2 and learn shared subword units
using Byte Pair Encoding (BPE) (Sennrich et al.,
2016b) using 32,000 merge operations for a ﬁnal
vocabulary size of approximately 37k. We discov-
ered that data preprocessing can have a large im-
pact on ﬁnal numbers, and since we wish to enable

2https://github.com/moses-smt/mosesdecoder/

Figure 1: Encoder-Decoder architecture with attention module. Section numbers reference experiments
corresponding to the components.

reproducibility, we release our data preprocessing
scripts together with the NMT framework to the
public. For more details on data preprocessing pa-
rameters, we refer the reader to the code release.

3.2 Training Setup and Software

All of the following experiments are run using
our own software framework based on Tensor-
Flow (Abadi et al., 2016). We purposely built
this framework to enable reproducible state-of-
the-art implementations of Neural Machine Trans-
lation architectures. As part of our contribution,
we are releasing the framework and all conﬁgura-
tion ﬁles needed to reproduce our results. Train-
ing is performed on Nvidia Tesla K40m and Tesla
K80 GPUs, distributed over 8 parallel workers and
6 parameter servers per experiment. We use a
batch size of 128 and decode using beam search
with a beam width of 10 and the length normaliza-
tion penalty of 0.6 described in (Wu et al., 2016).
BLEU scores are calculated on tokenized data us-
ing the multi-bleu.perl script in Moses3. Each ex-
periment is run for a maximum of 2.5M steps and
replicated 4 times with different initializations.
We save model checkpoints every 30 minutes and
choose the best checkpoint based on the validation
set BLEU score. We report mean and standard de-

3https://github.com/moses-

smt/mosesdecoder/blob/master/scripts/generic/multi-
bleu.perl

viation as well as highest scores (as per cross val-
idation) for each experiment.

3.3 Baseline Model

Based on a review of previous literature, we chose
a baseline model that we knew would perform rea-
sonably well. Our goal was to keep the baseline
model simple and standard, not to advance the
start of the art. The model (described in 2.1) con-
sists of a 2-layer bidirectional encoder (1 layer in
each direction), and a 2 layer decoder with a mul-
tiplicative (Luong et al., 2015a) attention mecha-
nism. We use 512-unit GRU (Cho et al., 2014)
cells for both the encoder and decoder and apply
Dropout of 0.2 at the input of each cell. We train
using the Adam optimizer and a ﬁxed learning rate
of 0.0001 without decay. The embedding dimen-
sionality is set to 512. A more detailed description
of all model hyperparameters can be found in the
supplementary material.

In each of the following experiments, the hy-
perparameters of the baseline model are held con-
stant, except for the one hyperparameter being
studied. We hope that this allows us to isolate the
effect of various hyperparameter changes. We rec-
ognize that this procedure does not account for in-
teractions between hyperparameters, and we per-
form additional experiments when we believe such
interactions are likely to occur (e.g. skip connec-
tions and number of layers).

4 Experiments and Discussion

For the sake of brevity, we only report mean
BLEU, standard deviation, highest BLEU in
parantheses, and model size in the following ta-
bles. Log perplexity, tokens/sec and convergence
times can be found in the supplementary material
tables.

4.1 Embedding Dimensionality

With a large vocabulary, the embedding layer can
account for a large fraction of the model param-
eters. Historically, researchers have used 620-
dimensional (Bahdanau et al., 2015) or 1024-
dimensional (Luong et al., 2015a) embeddings.
We expected larger embeddings to result in bet-
ter BLEU scores, or at least lower perplexities, but
we found that this wasn’t always the case. While
Table 1 shows that 2048-dimensional embeddings
yielded the overall best result, they only did so
by a small margin. Even small 128-dimensional
embeddings performed surprisingly well, while
converging almost twice as quickly. We found
that gradient updates to both small and large em-
beddings did not differ signiﬁcantly and that the
norm of gradient updates to the embedding matrix
stayed approximately constant throughout train-
ing regardless of size. We also did not observe
overﬁtting with large embeddings and training log
perplexity was approximately equal across exper-
iments, suggesting that the model does not make
efﬁcient use of the extra parameters and that there
may be a need for better optimization techniques.
Alternatively, it could be the case that models with
large embeddings simply need much more than
2.5M steps to converge to the best solution.

Dim newstest2013
21.50 ± 0.16 (21.66)
128
21.73 ± 0.09 (21.85)
256
21.78 ± 0.05 (21.83)
512
21.36 ± 0.27 (21.67)
1024
2048 21.86 ± 0.17 (22.08)

Params
36.13M
46.20M
66.32M
106.58M
187.09M

Table 1: BLEU scores on newstest2013, varying
the embedding dimensionality.

on small sequence tasks of a few thousand exam-
ples, we are not aware of such studies in large-
scale NMT settings.

A motivation for gated cells such as the GRU
and LSTM is the vanishing gradient problem.
Using vanilla RNN cells, deep networks cannot
efﬁciently propagate information and gradients
through multiple layers and time steps. However,
with an attention-based model, we believe that the
decoder should be able to make decisions almost
exclusively based on the current input and the at-
tention context and we hypothesize that the gating
mechanism in the decoder is not strictly necessary.
This hypothesis is supported by the fact that we
always initialize the decoder state to zero instead
of passing the encoder state, meaning that the de-
coder state does not contain information about the
encoded source. We test our hypothesis by using
a vanilla RNN cell in the decoder only (Vanilla-
Dec below). For the LSTM and GRU variants we
replace cells in both the encoder and decoder. We
use LSTM cells without peephole connections and
initialize the forget bias of both LSTM and GRU
cells to 1.

Cell
LSTM
GRU
Vanilla-Dec

newstest2013
22.22 ± 0.08 (22.33)
21.78 ± 0.05 (21.83)
15.38 ± 0.28 (15.73)

Params
68.95M
66.32M
63.18M

Table 2: BLEU scores on newstest2013, varying
the type of encoder and decoder cell.

In our experiments, LSTM cells consistently
outperformed GRU cells. Since the computational
bottleneck in our architecture is the softmax opera-
tion we did not observe large difference in training
speed between LSTM and GRU cells. Somewhat
to our surprise, we found that the vanilla decoder
is unable to learn nearly as well as the gated vari-
ant. This suggests that the decoder indeed passes
information in its own state throughout multiple
time steps instead of relying solely on the atten-
tion mechanism and current input (which includes
the previous attention context).
It could also be
the case that the gating mechanism is necessary to
mask out irrelevant parts of the inputs.

4.2 RNN Cell Variant

Both LSTM (Hochreiter and Schmidhuber, 1997)
and GRU (Cho et al., 2014) cells are commonly
used in NMT architectures. While there exist stud-
ies (Greff et al., 2016) that explore cell variants

4.3 Encoder and Decoder Depth

We generally expect deeper networks to converge
to better solutions than shallower ones (He et al.,
2016). While some work (Luong et al., 2015b;

Zhou et al., 2016; Luong and Manning, 2016; Wu
et al., 2016) has achieved state of the art results
using deep networks, others (Jean et al., 2015;
Chung et al., 2016; Sennrich et al., 2016b) have
achieved similar results with far shallower ones.
Hence, it is unclear how important depth is, and
whether shallow networks are capable of produc-
ing results competitive with those of deep net-
works. Here, we explore the effect of both encoder
and decoder depth up to 8 layers. For the bidi-
rectional encoder, we separately stack the RNNs
in both directions. For example, the Enc-8 model
corresponds to one forward and one backward 4-
layer RNN. For deeper networks, we also exper-
iment with two variants of residual connections
(He et al., 2016) to encourage gradient ﬂow. In the
standard variant, shown in equation (4), we insert
residual connections between consecutive layers.
If h(l)
, h(l)
t (x(l)
t−1) is the RNN output of layer l at
t
time step t, then:

x(l+1)
t

= h(l)

t (x(l)

t

, h(l)

t−1) + x(l)

t

(4)

where x(0)

t

are the embedded input

tokens.
We also explore a dense (”ResD” below) variant
of residual connections similar to those used by
In
(Huang et al., 2016a) in Image Recognition.
this variant, we add skip connections from each
layer to all other layers:

x(l+1)
t

= h(l)

t (x(l)

t

, h(l)

t−1) +

x(j)
t

(5)

l
(cid:88)

j=0

Our implementation differs from (Huang et al.,
2016a) in that we use an addition instead of a con-
catenation operation in order to keep the state size
constant.

Table 3 shows results of varying encoder and
decoder depth with and without residual connec-
tion. We found no clear evidence that encoder
depth beyond two layers is necessary, but found
deeper models with residual connections to be sig-
niﬁcantly more likely to diverge during training.
The best deep residual models achieved good re-
sults, but only one of four runs converged, as sug-
gested by the large standard deviation.

On the decoder side, deeper models outper-
formed shallower ones by a small margin, and
we found that without residual connections,
it
was impossible for us to train decoders with 8

Figure 2: Training plots for deep decoder with and
without residual connections, showing log per-
plexity on the eval set.

newstest2013
Depth
21.78 ± 0.05 (21.83)
Enc-2
21.85 ± 0.32 (22.23)
Enc-4
21.32 ± 0.14 (21.51)
Enc-8
19.23 ± 1.96 (21.97)
Enc-8-Res
Enc-8-ResD 17.30 ± 2.64 (21.03)
21.76 ± 0.12 (21.93)
Dec-1
21.78 ± 0.05 (21.83)
Dec-2
22.37 ± 0.10 (22.51)
Dec-4
17.48 ± 0.25 (17.82)
Dec-4-Res
Dec-4-ResD 21.10 ± 0.24 (21.43)
01.42 ± 0.23 (1.66)
Dec-8
16.99 ± 0.42 (17.47)
Dec-8-Res
Dec-8-ResD 20.97 ± 0.34 (21.42)

Params
66.32M
69.47M
75.77M
75.77M
75.77M
64.75M
66.32M
69.47M
68.69M
68.69M
75.77M
75.77M
75.77M

Table 3: BLEU scores on newstest2013, varying
the encoder and decoder depth and type of residual
connections.

or more layers. Across the deep decoder exper-
iments, dense residual connections consistently
outperformed regular residual connections and
converged much faster in terms of step count, as
shown in ﬁgure 2. We expected deep models to
perform better (Zhou et al., 2016; Szegedy et al.,
2015) across the board, and we believe that our
experiments demonstrate the need for more robust
techniques for optimizing deep sequential models.
For example, we may need a better-tuned SGD op-
timizer or some form of batch normalization, in
order to robustly train deep networks with residual
connections.

4.4 Unidirectional vs. Bidirectional Encoder

In the literature, we see bidirectional encoders
(Bahdanau et al., 2015), unidirectional encoders

(Luong et al., 2015a), and a mix of both (Wu
et al., 2016) being used. Bidirectional encoders
are able to create representations that take into ac-
count both past and future inputs, while unidirec-
tional encoders can only take past inputs into ac-
count. The beneﬁt of unidirectional encoders is
that their computation can be easily parallelized on
GPUs, allowing them to run faster than their bidi-
rectional counterparts. We are not aware of any
studies that explore the necessity of bidirectional-
ity. In this set of experiments, we explore unidirec-
tional encoders of varying depth with and without
reversed source inputs, as this is a commonly used
trick that allows the encoder to create richer repre-
sentations for earlier words. Given that errors on
the decoder side can easily cascade, the correct-
ness of early words has disproportionate impact.

newstest2013
Cell
21.78 ± 0.05 (21.83)
Bidi-2
20.54 ± 0.16 (20.73)
Uni-1
Uni-1R 21.16 ± 0.35 (21.64)
20.98 ± 0.10 (21.07)
Uni-2
Uni-2R 21.76 ± 0.21 (21.93)
21.47 ± 0.22 (21.70)
Uni-4
Uni-4R 21.32 ± 0.42 (21.89)

Params
66.32M
63.44M
63.44M
65.01M
65.01M
68.16M
68.16M

Table 4: BLEU scores on newstest2013, varying
the type of encoder. The ”R” sufﬁx indicates a
reversed source sequence.

Table 4 shows that bidirectional encoders gen-
erally outperform unidirectional encoders, but not
by a large margin. The encoders with reversed
source consistently outperform their non-reversed
counterparts, but do not beat shallower bidirec-
tional encoders.

4.5 Attention Mechanism

The two most commonly used attention mecha-
nisms are the additive (Bahdanau et al., 2015) vari-
ant, equation (6) below, and the computationally
less expensive multiplicative variant (Luong et al.,
2015a), equation (7) below. Given an attention key
hj (an encoder state) and attention query si (a de-
coder state), the attention score for each pair is cal-
culated as follows:

score(hj, si) = (cid:104)v, tanh(W1hj + W2si)(cid:105)
score(hj, si) = (cid:104)W1hj, W2si(cid:105)

(6)

(7)

We call the dimensionality of W1hj and W2si
the ”attention dimensionality” and vary it from
128 to 1024 by changing the layer size. We also
experiment with using no attention mechanism by
initializing the decoder state with the last encoder
state (None-State), or concatenating the last de-
coder state to each decoder input (None-Input).
The results are shown in Table 5.

Attention
Mul-128
Mul-256
Mul-512
Mul-1024
Add-128
Add-256
Add-512
Add-1028
None-State
None-Input

newstest2013
22.03 ± 0.08 (22.14)
22.33 ± 0.28 (22.64)
21.78 ± 0.05 (21.83)
18.22 ± 0.03 (18.26)
22.23 ± 0.11 (22.38)
22.33 ± 0.04 (22.39)
22.47 ± 0.27 (22.79)
22.10 ± 0.18 (22.36)
9.98 ± 0.28 (10.25)
11.57 ± 0.30 (11.85)

Params
65.73M
65.93M
66.32M
67.11M
65.73M
65.93M
66.33M
67.11M
64.23M
64.49M

Table 5: BLEU scores on newstest2013, varying
the type of attention mechanism.

We found that the parameterized additive atten-
tion mechanism slightly but consistently outper-
formed the multiplicative one, with the attention
dimensionality having little effect.

While we did expect the attention-based mod-
els to signiﬁcantly outperform those without an
attention mechanism, we were surprised by just
how poorly the ”Non-Input” models fared, given
that they had access to encoder information at
each time step.
Furthermore, we found that
the attention-based models exhibited signiﬁcantly
larger gradient updates to decoder states through-
out training. This suggests that the attention mech-
anism acts more like a ”weighted skip connection”
that optimizes gradient ﬂow than like a ”memory”
that allows the encoder to access source states, as
is commonly stated in the literature. We believe
that further research in this direction is necessary
to shed light on the role of the attention mecha-
nism and whether it may be purely a vehicle for
easier optimization.

4.6 Beam Search Strategies

Beam Search is a commonly used technique to
ﬁnd target sequences that maximize some scoring
function s(y, x) through tree search. In the sim-
plest case, the score to be maximized is the log
probability of the target sequence given the source.

Recently, extensions such as coverage penalties
(Tu et al., 2016) and length normalizations (Wu
et al., 2016) have been shown to improve decod-
ing results. It has also been observed (Tu et al.,
2017) that very large beam sizes, even with length
penalty, perform worse than smaller ones. Thus,
choosing the correct beam width can be crucial to
achieving the best results.

Beam
newstest2013
20.66 ± 0.31 (21.08)
B1
21.55 ± 0.26 (21.94)
B3
21.60 ± 0.28 (22.03)
B5
21.57 ± 0.26 (21.91)
B10
21.47 ± 0.30 (21.77)
B25
21.10 ± 0.31 (21.39)
B100
21.71 ± 0.25 (22.04)
B10-LP-0.5
B10-LP-1.0 21.80 ± 0.25 (22.16)

Params
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M

Table 6: BLEU scores on newstest2013, varying
the beam width and adding length penalties (LP).

Table 6 shows the effect of varying beam widths
and adding length normalization penalties. A
beam width of 1 corresponds to greedy search. We
found that a well-tuned beam search is crucial to
achieving good results, and that it leads to consis-
tent gains of more than one BLEU point. Similar
to (Tu et al., 2017) we found that very large beams
yield worse results and that there is a ”sweet spot”
of optimal beam width. We believe that further re-
search into the robustness of hyperparameters in
beam search is crucial to progress in NMT. We
also experimented with a coverage penalty, but
found no additional gain over a sufﬁciently large
length penalty.

4.7 Final System Comparison

Finally, we compare our best performing model
across all experiments (base model with 512-
dimensional additive attention), as chosen on the
newstest2013 validation set, to historical results
found in the literature in Table 8. While not the
focus on this work, we were able to achieve fur-
ther improvements by combining all of our in-
sights into a single model described in Table 7.

Although we do not offer architectural innova-
tions, we do show that through careful hyperpa-
rameter tuning and good initialization, it is pos-
sible to achieve state of the art performance on
standard WMT benchmarks. Our model is outper-
formed only by (Wu et al., 2016), a model which

Hyperparameter Value
embedding dim
rnn cell variant
encoder depth
decoder depth
attention dim
attention type
encoder
beam size
length penalty

512
LSTMCell
4
4
512
Bahdanau
bidirectional
10
1.0

Table 7: Hyperparameter settings for our ﬁnal
combined model, consisting of all of the individu-
ally optimized values.

is signiﬁcantly more complex and lacks a public
implementation.

Model
Ours (experimental)
Ours (combined)
OpenNMT
Luong
BPE-Char
BPE
RNNSearch-LV
RNNSearch
Deep-Att*
GNMT*
Deep-Conv*

newstest14
22.03
22.19
19.34
20.9
21.5
-
19.4
-
20.6
24.61
-

newstest15
24.75
25.23
-
-
23.9
20.5
-
16.5
-
-
24.3

Table 8: Comparison to RNNSearch (Jean et al.,
2015), RNNSearch-LV (Jean et al., 2015), BPE
(Sennrich et al., 2016b), BPE-Char (Chung et al.,
2016), Deep-Att (Zhou et al., 2016), Luong (Lu-
ong et al., 2015a), Deep-Conv (Gehring et al.,
2016), GNMT (Wu et al., 2016), and OpenNMT
(Klein et al., 2017). Systems with an * do not have
a public implementation.

5 Open Source Release

We demonstrated empirically how small changes
to hyperparameter values and different initializa-
tion can affect results, and how seemingly triv-
ial factors such as a well-tuned beam search are
crucial. To move towards reproducible research,
we believe it is important that researchers start
building upon common frameworks and data pro-
cessing pipelines. With this goal in mind, we
speciﬁcally built a modular software framework

that allows researchers to explore novel archi-
tectures with minimal code changes, and deﬁne
experimental parameters in a reproducible man-
ner. While our initial experiments are in Ma-
chine Translation, our framework can easily be
adapted to problems in Summarization, Conversa-
tional Modeling or Image-To-Text. Systems such
as OpenNMT (Klein et al., 2017) share similar
goals, but do not yet achieve state of the art re-
sults (see Table 8) and lack what we believe to be
crucial features, such as distributed training sup-
port. We hope that by open sourcing our experi-
mental toolkit, we enable the ﬁeld to make more
rapid progress in the future.
is

freely available

All of our

code

at

https://github.com/google/seq2seq/.

6 Conclusion

We conducted what we believe to be the ﬁrst large-
scale analysis of architecture variations for Neural
Machine Translation, teasing apart the key factors
to achieving state of the art results. We demon-
strated a number of surprising insights, including
the fact that beam search tuning is just as crucial
as most architectural variations, and that with cur-
rent optimization techniques deep models do not
always outperform shallow ones. Here, we sum-
marize our practical ﬁndings:

• Large embeddings with 2048 dimensions
achieved the best results, but only by a small
margin. Even small embeddings with 128 di-
mensions seem to have sufﬁcient capacity to
capture most of the necessary semantic infor-
mation.

• A well-tuned beam search with length
penalty is crucial. Beam widths of 5 to 10
together with a length penalty of 1.0 seemed
to work well.

We highlighted several important research ques-
tions, including the efﬁcient use of embedding pa-
rameters (4.1), the role of attention mechanisms
as weighted skip connections (4.5) as opposed to
memory units, the need for better optimization
methods for deep recurrent networks (4.3), and the
need for a better beam search (4.6) robust to hyper-
parameter variations.

In addition, we release to the public an open
source NMT framework speciﬁcally built to ex-
plore architectural innovations and generate re-
producible experiments, along with conﬁguration
ﬁles for all our experiments.

Acknowledgments

We would like to thank Eugene Brevdo for adapt-
ing the TensorFlow RNN APIs in a way that
allowed us to write our framework much more
cleanly. We are also grateful to Andrew Dai and
Samy Bengio for their helpful feedback.

References

Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-
sorFlow: A system for large-scale machine learning.
In OSDI.

• LSTM Cells consistently outperformed GRU

Cells.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

• Bidirectional encoders with 2 to 4 layers per-
formed best. Deeper encoders were signiﬁ-
cantly more unstable to train, but show po-
tential if they can be optimized well.

• Deep 4-layer decoders slightly outperformed
shallower decoders. Residual connections
were necessary to train decoders with 8 lay-
ers and dense residual connections offer ad-
ditional robustness.

• Parameterized additive attention yielded the

overall best results.

Kyunghyun Cho, Bart van Merrienboer, C¸ aglar
G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. In EMNLP.

Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. A character-level decoder without ex-
plicit segmentation for neural machine translation.
In ACL.

Jonas Gehring, Michael Auli, David Grangier, and
Yann N. Dauphin. 2016.
A convolutional en-
coder model for neural machine translation. CoRR
abs/1611.02344.

Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık,
Bas R. Steunebrink, and J¨urgen Schmidhuber. 2016.
IEEE Transac-
LSTM: A search space odyssey.
tions on Neural Networks and Learning Systems
PP(99):1–11.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott E. Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. 2015. Going deeper with convolutions. In
CVPR.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In CVPR.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2017. Neural machine translation with
reconstruction. In AAAI.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In ACL.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR
abs/1609.08144.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei
Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. Trans-
actions of the Association for Computational Lin-
guistics 4:371–383.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural Computation

Long short-term memory.
9(8):1735–1780.

Gao Huang, Zhuang Liu, and Kilian Q. Weinberger.
2016a. Densely connected convolutional networks.
CoRR abs/1608.06993.

Jonathan Huang, Vivek Rathod, Chen Sun, Meng-
long Zhu, Anoop Korattikara, Alireza Fathi, Ian Fis-
cher, Zbigniew Wojna, Yang Song, Sergio Guadar-
rama, and Kevin Murphy. 2016b. Speed/accuracy
trade-offs for modern convolutional object detectors.
CoRR abs/1611.10012.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
ACL.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent

continuous translation models. In EMNLP.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. CoRR abs/1701.02810.

Minh-Thang Luong and Christopher D. Manning.
2016. Achieving open vocabulary neural machine
translation with hybrid word-character models.
In
ACL.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015a. Effective approaches to attention-
based neural machine translation. In EMNLP.

Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015b. Addressing
the rare word problem in neural machine translation.
In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Edinburgh neural machine translation sys-
tems for wmt 16. In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In ACL.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

7
1
0
2
 
r
a

M
 
1
2
 
 
]
L
C
.
s
c
[
 
 
2
v
6
0
9
3
0
.
3
0
7
1
:
v
i
X
r
a

Massive Exploration of Neural Machine Translation
Architectures

Denny Britz∗†, Anna Goldie∗, Minh-Thang Luong, Quoc Le
{dennybritz,agoldie,thangluong,qvl}@google.com
Google Brain

Abstract

the ﬁrst

Neural Machine Translation (NMT) has
shown remarkable progress over the past
few years with production systems now
being deployed to end-users. One major
drawback of current architectures is that
they are expensive to train, typically re-
quiring days to weeks of GPU time to
converge. This makes exhaustive hyper-
parameter search, as is commonly done
with other neural network architectures,
In this work,
prohibitively expensive.
we present
large-scale analy-
sis of NMT architecture hyperparameters.
We report empirical results and variance
numbers for several hundred experimental
runs, corresponding to over 250,000 GPU
hours on the standard WMT English to
German translation task. Our experiments
lead to novel insights and practical advice
for building and extending NMT architec-
tures. As part of this contribution, we
release an open-source NMT framework1
that enables researchers to easily experi-
ment with novel techniques and reproduce
state of the art results.

1

Introduction

Neural Machine Translation (NMT) (Kalchbren-
ner and Blunsom, 2013; Sutskever et al., 2014;
Cho et al., 2014) is an end-to-end approach to
automated translation. NMT has shown impres-
sive results (Jean et al., 2015; Luong et al., 2015b;
Sennrich et al., 2016a; Wu et al., 2016) sur-
passing those of phrase-based systems while ad-
dressing shortcomings such as the need for hand-

∗Both authors contributed equally to this work.
†Work done as a member of the Google Brain Residency

program (g.co/brainresidency).
1https://github.com/google/seq2seq/

1

engineered features. The most popular approaches
to NMT are based on an encoder-decoder architec-
ture consisting of two recurrent neural networks
(RNNs) and an attention mechanism that aligns
target with source tokens (Bahdanau et al., 2015;
Luong et al., 2015a).

One shortcoming of current NMT architectures
is the amount of compute required to train them.
Training on real-world datasets of several million
examples typically requires dozens of GPUs and
convergence time is on the order of days to weeks
(Wu et al., 2016). While sweeping across large hy-
perparameter spaces is common in Computer Vi-
sion (Huang et al., 2016b), such exploration would
be prohibitively expensive for NMT models, lim-
iting researchers to well-established architectures
and hyperparameter choices. Furthermore, there
have been no large-scale studies of how architec-
tural hyperparameters affect the performance of
NMT systems. As a result, it remains unclear why
these models perform as well as they do, as well
as how we might improve them.

In this work, we present the ﬁrst comprehen-
sive analysis of architectural hyperparameters for
Neural Machine Translation systems. Using a to-
tal of more than 250,000 GPU hours, we explore
common variations of NMT architectures and pro-
vide insight into which architectural choices mat-
ter most. We report BLEU scores, perplexities,
model sizes, and convergence time for all ex-
periments, including variance numbers calculated
across several runs of each experiment.
In ad-
dition, we release to the public a new software
framework that was used to run the experiments.

In summary, the main contributions of this work

are as follows:

• We provide immediately applicable insights
into the optimization of Neural Machine
Translation models, as well as promising di-
rections for future research. For example, we

found that deep encoders are more difﬁcult
to optimize than decoders, that dense resid-
ual connections yield better performance than
regular residual connections,
that LSTMs
outperform GRUs, and that a well-tuned
beam search is crucial to obtaining state of
the art results. By presenting practical advice
for choosing baseline architectures, we help
researchers avoid wasting time on unpromis-
ing model variations.

• We also establish the extent to which metrics
such as BLEU are inﬂuenced by random ini-
tialization and slight hyperparameter varia-
tion, helping researchers to distinguish statis-
tically signiﬁcant results from random noise.

• Finally, we release an open source package
based on TensorFlow, speciﬁcally designed
for implementing reproducible state of the
art sequence-to-sequence models. All experi-
ments were run using this framework and we
hope to accelerate future research by releas-
ing it to the public. We also release all con-
ﬁguration ﬁles and processing scripts needed
to reproduce the experiments in this paper.

2 Background and Preliminaries

2.1 Neural Machine Translation

Our models are based on an encoder-decoder ar-
chitecture with attention mechanism (Bahdanau
et al., 2015; Luong et al., 2015a), as shown in ﬁg-
ure 1. An encoder function fenc takes as input a
sequence of source tokens x = (x1, ..., xm) and
produces a sequence of states h = (h1, ..., hm).
In our base model, fenc is a bi-directional RNN
and the state hi corresponds to the concatenation
of the states produced by the backward and for-
←−
ward RNNs, hi = [
hi ] .The decoder fdec is
an RNN that predicts the probability of a target
sequence y = (y1, ..., yk) based on h. The proba-
bility of each target token yi ∈ 1, ...V is predicted
based on the recurrent state in the decoder RNN
si, the previous words, y<i, and a context vector
ci. The context vector ci is also called the atten-
tion vector and is calculated as a weighted average
of the source states.

−→
hi ;

(cid:88)

ci =

aijhj

j

ˆaij
j ˆaij

(cid:80)

aij =

ˆaij = att(si, hj)

(1)

(2)

(3)

Here, att(si, hj) is an attention function that
calculates an unnormalized alignment score be-
tween the encoder state hj and the decoder state
si. In our base model, we use a function of the
form att(si, hj) = (cid:104)Whhj, Wssi(cid:105), where the ma-
trices W are used to transform the source and tar-
get states into a representation of the same size.

The decoder outputs a distribution over a vocab-

ulary of ﬁxed-size V :

P (yi|y1, ..., yi−1, x)
= softmax(W [si; ci] + b)

The whole model is trained end-to-end by min-
imizing the negative log likelihood of the target
words using stochastic gradient descent.

3 Experimental Setup

3.1 Datasets and Preprocessing

run all

We
experiments on the WMT’15
English→German task consisting of 4.5M sen-
tence pairs, obtained by combining the Europarl
v7, News Commentary v10, and Common Crawl
corpora. We use newstest2013 as our validation
set and newstest2014 and newstest2015 as our test
sets. To test for generality, we also ran a small
number of experiments on English→French trans-
lation, and we found that the performance was
highly correlated with that of English→German
but that it took much longer to train models on the
larger English→French dataset. Given that trans-
lation from the morphologically richer German is
also considered a more challenging task, we felt
justiﬁed in using the English→German translation
task for this hyperparameter sweep.

We tokenize and clean all datasets with the
scripts in Moses2 and learn shared subword units
using Byte Pair Encoding (BPE) (Sennrich et al.,
2016b) using 32,000 merge operations for a ﬁnal
vocabulary size of approximately 37k. We discov-
ered that data preprocessing can have a large im-
pact on ﬁnal numbers, and since we wish to enable

2https://github.com/moses-smt/mosesdecoder/

Figure 1: Encoder-Decoder architecture with attention module. Section numbers reference experiments
corresponding to the components.

reproducibility, we release our data preprocessing
scripts together with the NMT framework to the
public. For more details on data preprocessing pa-
rameters, we refer the reader to the code release.

3.2 Training Setup and Software

All of the following experiments are run using
our own software framework based on Tensor-
Flow (Abadi et al., 2016). We purposely built
this framework to enable reproducible state-of-
the-art implementations of Neural Machine Trans-
lation architectures. As part of our contribution,
we are releasing the framework and all conﬁgura-
tion ﬁles needed to reproduce our results. Train-
ing is performed on Nvidia Tesla K40m and Tesla
K80 GPUs, distributed over 8 parallel workers and
6 parameter servers per experiment. We use a
batch size of 128 and decode using beam search
with a beam width of 10 and the length normaliza-
tion penalty of 0.6 described in (Wu et al., 2016).
BLEU scores are calculated on tokenized data us-
ing the multi-bleu.perl script in Moses3. Each ex-
periment is run for a maximum of 2.5M steps and
replicated 4 times with different initializations.
We save model checkpoints every 30 minutes and
choose the best checkpoint based on the validation
set BLEU score. We report mean and standard de-

3https://github.com/moses-

smt/mosesdecoder/blob/master/scripts/generic/multi-
bleu.perl

viation as well as highest scores (as per cross val-
idation) for each experiment.

3.3 Baseline Model

Based on a review of previous literature, we chose
a baseline model that we knew would perform rea-
sonably well. Our goal was to keep the baseline
model simple and standard, not to advance the
start of the art. The model (described in 2.1) con-
sists of a 2-layer bidirectional encoder (1 layer in
each direction), and a 2 layer decoder with a mul-
tiplicative (Luong et al., 2015a) attention mecha-
nism. We use 512-unit GRU (Cho et al., 2014)
cells for both the encoder and decoder and apply
Dropout of 0.2 at the input of each cell. We train
using the Adam optimizer and a ﬁxed learning rate
of 0.0001 without decay. The embedding dimen-
sionality is set to 512. A more detailed description
of all model hyperparameters can be found in the
supplementary material.

In each of the following experiments, the hy-
perparameters of the baseline model are held con-
stant, except for the one hyperparameter being
studied. We hope that this allows us to isolate the
effect of various hyperparameter changes. We rec-
ognize that this procedure does not account for in-
teractions between hyperparameters, and we per-
form additional experiments when we believe such
interactions are likely to occur (e.g. skip connec-
tions and number of layers).

4 Experiments and Discussion

For the sake of brevity, we only report mean
BLEU, standard deviation, highest BLEU in
parantheses, and model size in the following ta-
bles. Log perplexity, tokens/sec and convergence
times can be found in the supplementary material
tables.

4.1 Embedding Dimensionality

With a large vocabulary, the embedding layer can
account for a large fraction of the model param-
eters. Historically, researchers have used 620-
dimensional (Bahdanau et al., 2015) or 1024-
dimensional (Luong et al., 2015a) embeddings.
We expected larger embeddings to result in bet-
ter BLEU scores, or at least lower perplexities, but
we found that this wasn’t always the case. While
Table 1 shows that 2048-dimensional embeddings
yielded the overall best result, they only did so
by a small margin. Even small 128-dimensional
embeddings performed surprisingly well, while
converging almost twice as quickly. We found
that gradient updates to both small and large em-
beddings did not differ signiﬁcantly and that the
norm of gradient updates to the embedding matrix
stayed approximately constant throughout train-
ing regardless of size. We also did not observe
overﬁtting with large embeddings and training log
perplexity was approximately equal across exper-
iments, suggesting that the model does not make
efﬁcient use of the extra parameters and that there
may be a need for better optimization techniques.
Alternatively, it could be the case that models with
large embeddings simply need much more than
2.5M steps to converge to the best solution.

Dim newstest2013
21.50 ± 0.16 (21.66)
128
21.73 ± 0.09 (21.85)
256
21.78 ± 0.05 (21.83)
512
21.36 ± 0.27 (21.67)
1024
2048 21.86 ± 0.17 (22.08)

Params
36.13M
46.20M
66.32M
106.58M
187.09M

Table 1: BLEU scores on newstest2013, varying
the embedding dimensionality.

on small sequence tasks of a few thousand exam-
ples, we are not aware of such studies in large-
scale NMT settings.

A motivation for gated cells such as the GRU
and LSTM is the vanishing gradient problem.
Using vanilla RNN cells, deep networks cannot
efﬁciently propagate information and gradients
through multiple layers and time steps. However,
with an attention-based model, we believe that the
decoder should be able to make decisions almost
exclusively based on the current input and the at-
tention context and we hypothesize that the gating
mechanism in the decoder is not strictly necessary.
This hypothesis is supported by the fact that we
always initialize the decoder state to zero instead
of passing the encoder state, meaning that the de-
coder state does not contain information about the
encoded source. We test our hypothesis by using
a vanilla RNN cell in the decoder only (Vanilla-
Dec below). For the LSTM and GRU variants we
replace cells in both the encoder and decoder. We
use LSTM cells without peephole connections and
initialize the forget bias of both LSTM and GRU
cells to 1.

Cell
LSTM
GRU
Vanilla-Dec

newstest2013
22.22 ± 0.08 (22.33)
21.78 ± 0.05 (21.83)
15.38 ± 0.28 (15.73)

Params
68.95M
66.32M
63.18M

Table 2: BLEU scores on newstest2013, varying
the type of encoder and decoder cell.

In our experiments, LSTM cells consistently
outperformed GRU cells. Since the computational
bottleneck in our architecture is the softmax opera-
tion we did not observe large difference in training
speed between LSTM and GRU cells. Somewhat
to our surprise, we found that the vanilla decoder
is unable to learn nearly as well as the gated vari-
ant. This suggests that the decoder indeed passes
information in its own state throughout multiple
time steps instead of relying solely on the atten-
tion mechanism and current input (which includes
the previous attention context).
It could also be
the case that the gating mechanism is necessary to
mask out irrelevant parts of the inputs.

4.2 RNN Cell Variant

Both LSTM (Hochreiter and Schmidhuber, 1997)
and GRU (Cho et al., 2014) cells are commonly
used in NMT architectures. While there exist stud-
ies (Greff et al., 2016) that explore cell variants

4.3 Encoder and Decoder Depth

We generally expect deeper networks to converge
to better solutions than shallower ones (He et al.,
2016). While some work (Luong et al., 2015b;

Zhou et al., 2016; Luong and Manning, 2016; Wu
et al., 2016) has achieved state of the art results
using deep networks, others (Jean et al., 2015;
Chung et al., 2016; Sennrich et al., 2016b) have
achieved similar results with far shallower ones.
Hence, it is unclear how important depth is, and
whether shallow networks are capable of produc-
ing results competitive with those of deep net-
works. Here, we explore the effect of both encoder
and decoder depth up to 8 layers. For the bidi-
rectional encoder, we separately stack the RNNs
in both directions. For example, the Enc-8 model
corresponds to one forward and one backward 4-
layer RNN. For deeper networks, we also exper-
iment with two variants of residual connections
(He et al., 2016) to encourage gradient ﬂow. In the
standard variant, shown in equation (4), we insert
residual connections between consecutive layers.
If h(l)
, h(l)
t (x(l)
t−1) is the RNN output of layer l at
t
time step t, then:

x(l+1)
t

= h(l)

t (x(l)

t

, h(l)

t−1) + x(l)

t

(4)

where x(0)

t

are the embedded input

tokens.
We also explore a dense (”ResD” below) variant
of residual connections similar to those used by
In
(Huang et al., 2016a) in Image Recognition.
this variant, we add skip connections from each
layer to all other layers:

x(l+1)
t

= h(l)

t (x(l)

t

, h(l)

t−1) +

x(j)
t

(5)

l
(cid:88)

j=0

Our implementation differs from (Huang et al.,
2016a) in that we use an addition instead of a con-
catenation operation in order to keep the state size
constant.

Table 3 shows results of varying encoder and
decoder depth with and without residual connec-
tion. We found no clear evidence that encoder
depth beyond two layers is necessary, but found
deeper models with residual connections to be sig-
niﬁcantly more likely to diverge during training.
The best deep residual models achieved good re-
sults, but only one of four runs converged, as sug-
gested by the large standard deviation.

On the decoder side, deeper models outper-
formed shallower ones by a small margin, and
we found that without residual connections,
it
was impossible for us to train decoders with 8

Figure 2: Training plots for deep decoder with and
without residual connections, showing log per-
plexity on the eval set.

newstest2013
Depth
21.78 ± 0.05 (21.83)
Enc-2
21.85 ± 0.32 (22.23)
Enc-4
21.32 ± 0.14 (21.51)
Enc-8
19.23 ± 1.96 (21.97)
Enc-8-Res
Enc-8-ResD 17.30 ± 2.64 (21.03)
21.76 ± 0.12 (21.93)
Dec-1
21.78 ± 0.05 (21.83)
Dec-2
22.37 ± 0.10 (22.51)
Dec-4
17.48 ± 0.25 (17.82)
Dec-4-Res
Dec-4-ResD 21.10 ± 0.24 (21.43)
01.42 ± 0.23 (1.66)
Dec-8
16.99 ± 0.42 (17.47)
Dec-8-Res
Dec-8-ResD 20.97 ± 0.34 (21.42)

Params
66.32M
69.47M
75.77M
75.77M
75.77M
64.75M
66.32M
69.47M
68.69M
68.69M
75.77M
75.77M
75.77M

Table 3: BLEU scores on newstest2013, varying
the encoder and decoder depth and type of residual
connections.

or more layers. Across the deep decoder exper-
iments, dense residual connections consistently
outperformed regular residual connections and
converged much faster in terms of step count, as
shown in ﬁgure 2. We expected deep models to
perform better (Zhou et al., 2016; Szegedy et al.,
2015) across the board, and we believe that our
experiments demonstrate the need for more robust
techniques for optimizing deep sequential models.
For example, we may need a better-tuned SGD op-
timizer or some form of batch normalization, in
order to robustly train deep networks with residual
connections.

4.4 Unidirectional vs. Bidirectional Encoder

In the literature, we see bidirectional encoders
(Bahdanau et al., 2015), unidirectional encoders

(Luong et al., 2015a), and a mix of both (Wu
et al., 2016) being used. Bidirectional encoders
are able to create representations that take into ac-
count both past and future inputs, while unidirec-
tional encoders can only take past inputs into ac-
count. The beneﬁt of unidirectional encoders is
that their computation can be easily parallelized on
GPUs, allowing them to run faster than their bidi-
rectional counterparts. We are not aware of any
studies that explore the necessity of bidirectional-
ity. In this set of experiments, we explore unidirec-
tional encoders of varying depth with and without
reversed source inputs, as this is a commonly used
trick that allows the encoder to create richer repre-
sentations for earlier words. Given that errors on
the decoder side can easily cascade, the correct-
ness of early words has disproportionate impact.

newstest2013
Cell
21.78 ± 0.05 (21.83)
Bidi-2
20.54 ± 0.16 (20.73)
Uni-1
Uni-1R 21.16 ± 0.35 (21.64)
20.98 ± 0.10 (21.07)
Uni-2
Uni-2R 21.76 ± 0.21 (21.93)
21.47 ± 0.22 (21.70)
Uni-4
Uni-4R 21.32 ± 0.42 (21.89)

Params
66.32M
63.44M
63.44M
65.01M
65.01M
68.16M
68.16M

Table 4: BLEU scores on newstest2013, varying
the type of encoder. The ”R” sufﬁx indicates a
reversed source sequence.

Table 4 shows that bidirectional encoders gen-
erally outperform unidirectional encoders, but not
by a large margin. The encoders with reversed
source consistently outperform their non-reversed
counterparts, but do not beat shallower bidirec-
tional encoders.

4.5 Attention Mechanism

The two most commonly used attention mecha-
nisms are the additive (Bahdanau et al., 2015) vari-
ant, equation (6) below, and the computationally
less expensive multiplicative variant (Luong et al.,
2015a), equation (7) below. Given an attention key
hj (an encoder state) and attention query si (a de-
coder state), the attention score for each pair is cal-
culated as follows:

score(hj, si) = (cid:104)v, tanh(W1hj + W2si)(cid:105)
score(hj, si) = (cid:104)W1hj, W2si(cid:105)

(6)

(7)

We call the dimensionality of W1hj and W2si
the ”attention dimensionality” and vary it from
128 to 1024 by changing the layer size. We also
experiment with using no attention mechanism by
initializing the decoder state with the last encoder
state (None-State), or concatenating the last de-
coder state to each decoder input (None-Input).
The results are shown in Table 5.

Attention
Mul-128
Mul-256
Mul-512
Mul-1024
Add-128
Add-256
Add-512
Add-1028
None-State
None-Input

newstest2013
22.03 ± 0.08 (22.14)
22.33 ± 0.28 (22.64)
21.78 ± 0.05 (21.83)
18.22 ± 0.03 (18.26)
22.23 ± 0.11 (22.38)
22.33 ± 0.04 (22.39)
22.47 ± 0.27 (22.79)
22.10 ± 0.18 (22.36)
9.98 ± 0.28 (10.25)
11.57 ± 0.30 (11.85)

Params
65.73M
65.93M
66.32M
67.11M
65.73M
65.93M
66.33M
67.11M
64.23M
64.49M

Table 5: BLEU scores on newstest2013, varying
the type of attention mechanism.

We found that the parameterized additive atten-
tion mechanism slightly but consistently outper-
formed the multiplicative one, with the attention
dimensionality having little effect.

While we did expect the attention-based mod-
els to signiﬁcantly outperform those without an
attention mechanism, we were surprised by just
how poorly the ”Non-Input” models fared, given
that they had access to encoder information at
each time step.
Furthermore, we found that
the attention-based models exhibited signiﬁcantly
larger gradient updates to decoder states through-
out training. This suggests that the attention mech-
anism acts more like a ”weighted skip connection”
that optimizes gradient ﬂow than like a ”memory”
that allows the encoder to access source states, as
is commonly stated in the literature. We believe
that further research in this direction is necessary
to shed light on the role of the attention mecha-
nism and whether it may be purely a vehicle for
easier optimization.

4.6 Beam Search Strategies

Beam Search is a commonly used technique to
ﬁnd target sequences that maximize some scoring
function s(y, x) through tree search. In the sim-
plest case, the score to be maximized is the log
probability of the target sequence given the source.

Recently, extensions such as coverage penalties
(Tu et al., 2016) and length normalizations (Wu
et al., 2016) have been shown to improve decod-
ing results. It has also been observed (Tu et al.,
2017) that very large beam sizes, even with length
penalty, perform worse than smaller ones. Thus,
choosing the correct beam width can be crucial to
achieving the best results.

Beam
newstest2013
20.66 ± 0.31 (21.08)
B1
21.55 ± 0.26 (21.94)
B3
21.60 ± 0.28 (22.03)
B5
21.57 ± 0.26 (21.91)
B10
21.47 ± 0.30 (21.77)
B25
21.10 ± 0.31 (21.39)
B100
21.71 ± 0.25 (22.04)
B10-LP-0.5
B10-LP-1.0 21.80 ± 0.25 (22.16)

Params
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M
66.32M

Table 6: BLEU scores on newstest2013, varying
the beam width and adding length penalties (LP).

Table 6 shows the effect of varying beam widths
and adding length normalization penalties. A
beam width of 1 corresponds to greedy search. We
found that a well-tuned beam search is crucial to
achieving good results, and that it leads to consis-
tent gains of more than one BLEU point. Similar
to (Tu et al., 2017) we found that very large beams
yield worse results and that there is a ”sweet spot”
of optimal beam width. We believe that further re-
search into the robustness of hyperparameters in
beam search is crucial to progress in NMT. We
also experimented with a coverage penalty, but
found no additional gain over a sufﬁciently large
length penalty.

4.7 Final System Comparison

Finally, we compare our best performing model
across all experiments (base model with 512-
dimensional additive attention), as chosen on the
newstest2013 validation set, to historical results
found in the literature in Table 8. While not the
focus on this work, we were able to achieve fur-
ther improvements by combining all of our in-
sights into a single model described in Table 7.

Although we do not offer architectural innova-
tions, we do show that through careful hyperpa-
rameter tuning and good initialization, it is pos-
sible to achieve state of the art performance on
standard WMT benchmarks. Our model is outper-
formed only by (Wu et al., 2016), a model which

Hyperparameter Value
embedding dim
rnn cell variant
encoder depth
decoder depth
attention dim
attention type
encoder
beam size
length penalty

512
LSTMCell
4
4
512
Bahdanau
bidirectional
10
1.0

Table 7: Hyperparameter settings for our ﬁnal
combined model, consisting of all of the individu-
ally optimized values.

is signiﬁcantly more complex and lacks a public
implementation.

Model
Ours (experimental)
Ours (combined)
OpenNMT
Luong
BPE-Char
BPE
RNNSearch-LV
RNNSearch
Deep-Att*
GNMT*
Deep-Conv*

newstest14
22.03
22.19
19.34
20.9
21.5
-
19.4
-
20.6
24.61
-

newstest15
24.75
25.23
-
-
23.9
20.5
-
16.5
-
-
24.3

Table 8: Comparison to RNNSearch (Jean et al.,
2015), RNNSearch-LV (Jean et al., 2015), BPE
(Sennrich et al., 2016b), BPE-Char (Chung et al.,
2016), Deep-Att (Zhou et al., 2016), Luong (Lu-
ong et al., 2015a), Deep-Conv (Gehring et al.,
2016), GNMT (Wu et al., 2016), and OpenNMT
(Klein et al., 2017). Systems with an * do not have
a public implementation.

5 Open Source Release

We demonstrated empirically how small changes
to hyperparameter values and different initializa-
tion can affect results, and how seemingly triv-
ial factors such as a well-tuned beam search are
crucial. To move towards reproducible research,
we believe it is important that researchers start
building upon common frameworks and data pro-
cessing pipelines. With this goal in mind, we
speciﬁcally built a modular software framework

that allows researchers to explore novel archi-
tectures with minimal code changes, and deﬁne
experimental parameters in a reproducible man-
ner. While our initial experiments are in Ma-
chine Translation, our framework can easily be
adapted to problems in Summarization, Conversa-
tional Modeling or Image-To-Text. Systems such
as OpenNMT (Klein et al., 2017) share similar
goals, but do not yet achieve state of the art re-
sults (see Table 8) and lack what we believe to be
crucial features, such as distributed training sup-
port. We hope that by open sourcing our experi-
mental toolkit, we enable the ﬁeld to make more
rapid progress in the future.
is

freely available

All of our

code

at

https://github.com/google/seq2seq/.

6 Conclusion

We conducted what we believe to be the ﬁrst large-
scale analysis of architecture variations for Neural
Machine Translation, teasing apart the key factors
to achieving state of the art results. We demon-
strated a number of surprising insights, including
the fact that beam search tuning is just as crucial
as most architectural variations, and that with cur-
rent optimization techniques deep models do not
always outperform shallow ones. Here, we sum-
marize our practical ﬁndings:

• Large embeddings with 2048 dimensions
achieved the best results, but only by a small
margin. Even small embeddings with 128 di-
mensions seem to have sufﬁcient capacity to
capture most of the necessary semantic infor-
mation.

• A well-tuned beam search with length
penalty is crucial. Beam widths of 5 to 10
together with a length penalty of 1.0 seemed
to work well.

We highlighted several important research ques-
tions, including the efﬁcient use of embedding pa-
rameters (4.1), the role of attention mechanisms
as weighted skip connections (4.5) as opposed to
memory units, the need for better optimization
methods for deep recurrent networks (4.3), and the
need for a better beam search (4.6) robust to hyper-
parameter variations.

In addition, we release to the public an open
source NMT framework speciﬁcally built to ex-
plore architectural innovations and generate re-
producible experiments, along with conﬁguration
ﬁles for all our experiments.

Acknowledgments

We would like to thank Eugene Brevdo for adapt-
ing the TensorFlow RNN APIs in a way that
allowed us to write our framework much more
cleanly. We are also grateful to Andrew Dai and
Samy Bengio for their helpful feedback.

References

Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-
sorFlow: A system for large-scale machine learning.
In OSDI.

• LSTM Cells consistently outperformed GRU

Cells.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

• Bidirectional encoders with 2 to 4 layers per-
formed best. Deeper encoders were signiﬁ-
cantly more unstable to train, but show po-
tential if they can be optimized well.

• Deep 4-layer decoders slightly outperformed
shallower decoders. Residual connections
were necessary to train decoders with 8 lay-
ers and dense residual connections offer ad-
ditional robustness.

• Parameterized additive attention yielded the

overall best results.

Kyunghyun Cho, Bart van Merrienboer, C¸ aglar
G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. In EMNLP.

Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. A character-level decoder without ex-
plicit segmentation for neural machine translation.
In ACL.

Jonas Gehring, Michael Auli, David Grangier, and
Yann N. Dauphin. 2016.
A convolutional en-
coder model for neural machine translation. CoRR
abs/1611.02344.

Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık,
Bas R. Steunebrink, and J¨urgen Schmidhuber. 2016.
IEEE Transac-
LSTM: A search space odyssey.
tions on Neural Networks and Learning Systems
PP(99):1–11.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott E. Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. 2015. Going deeper with convolutions. In
CVPR.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In CVPR.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2017. Neural machine translation with
reconstruction. In AAAI.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In ACL.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR
abs/1609.08144.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei
Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. Trans-
actions of the Association for Computational Lin-
guistics 4:371–383.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural Computation

Long short-term memory.
9(8):1735–1780.

Gao Huang, Zhuang Liu, and Kilian Q. Weinberger.
2016a. Densely connected convolutional networks.
CoRR abs/1608.06993.

Jonathan Huang, Vivek Rathod, Chen Sun, Meng-
long Zhu, Anoop Korattikara, Alireza Fathi, Ian Fis-
cher, Zbigniew Wojna, Yang Song, Sergio Guadar-
rama, and Kevin Murphy. 2016b. Speed/accuracy
trade-offs for modern convolutional object detectors.
CoRR abs/1611.10012.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
ACL.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent

continuous translation models. In EMNLP.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. CoRR abs/1701.02810.

Minh-Thang Luong and Christopher D. Manning.
2016. Achieving open vocabulary neural machine
translation with hybrid word-character models.
In
ACL.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015a. Effective approaches to attention-
based neural machine translation. In EMNLP.

Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015b. Addressing
the rare word problem in neural machine translation.
In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Edinburgh neural machine translation sys-
tems for wmt 16. In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In ACL.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

