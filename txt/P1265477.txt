9
1
0
2
 
r
a

M
 
8
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
2
5
5
0
.
7
0
8
1
:
v
i
X
r
a

Deep Clustering for Unsupervised Learning
of Visual Features

Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze

Facebook AI Research

Abstract. Clustering is a class of unsupervised learning methods that
has been extensively applied and studied in computer vision. Little work
has been done to adapt it to the end-to-end training of visual features
on large scale datasets. In this work, we present DeepCluster, a clus-
tering method that jointly learns the parameters of a neural network
and the cluster assignments of the resulting features. DeepCluster it-
eratively groups the features with a standard clustering algorithm, k-
means, and uses the subsequent assignments as supervision to update
the weights of the network. We apply DeepCluster to the unsupervised
training of convolutional neural networks on large datasets like ImageNet
and YFCC100M. The resulting model outperforms the current state of
the art by a signiﬁcant margin on all the standard benchmarks.

Keywords: unsupervised learning, clustering

1 Introduction

Pre-trained convolutional neural networks, or convnets, have become the build-
ing blocks in most computer vision applications [1,2,3,4]. They produce excellent
general-purpose features that can be used to improve the generalization of mod-
els learned on a limited amount of data [5]. The existence of ImageNet [6], a
large fully-supervised dataset, has been fueling advances in pre-training of con-
vnets. However, Stock and Cisse [7] have recently presented empirical evidence
that the performance of state-of-the-art classiﬁers on ImageNet is largely un-
derestimated, and little error is left unresolved. This explains in part why the
performance has been saturating despite the numerous novel architectures pro-
posed in recent years [2,8,9]. As a matter of fact, ImageNet is relatively small
by today’s standards; it “only” contains a million images that cover the speciﬁc
domain of object classiﬁcation. A natural way to move forward is to build a big-
ger and more diverse dataset, potentially consisting of billions of images. This,
in turn, would require a tremendous amount of manual annotations, despite
the expert knowledge in crowdsourcing accumulated by the community over the
years [10]. Replacing labels by raw metadata leads to biases in the visual rep-
resentations with unpredictable consequences [11]. This calls for methods that
can be trained on internet-scale datasets with no supervision.

Unsupervised learning has been widely studied in the Machine Learning com-
munity [12], and algorithms for clustering, dimensionality reduction or density

2

Mathilde Caron et al .

Fig. 1: Illustration of the proposed method: we iteratively cluster deep features
and use the cluster assignments as pseudo-labels to learn the parameters of the
convnet.

estimation are regularly used in computer vision applications [13,14,15]. For
example, the “bag of features” model uses clustering on handcrafted local de-
scriptors to produce good image-level features [16]. A key reason for their success
is that they can be applied on any speciﬁc domain or dataset, like satellite or
medical images, or on images captured with a new modality, like depth, where
annotations are not always available in quantity. Several works have shown that
it was possible to adapt unsupervised methods based on density estimation or di-
mensionality reduction to deep models [17,18], leading to promising all-purpose
visual features [19,20]. Despite the primeval success of clustering approaches in
image classiﬁcation, very few works [21,22] have been proposed to adapt them to
the end-to-end training of convnets, and never at scale. An issue is that clustering
methods have been primarily designed for linear models on top of ﬁxed features,
and they scarcely work if the features have to be learned simultaneously. For
example, learning a convnet with k-means would lead to a trivial solution where
the features are zeroed, and the clusters are collapsed into a single entity.

In this work, we propose a novel clustering approach for the large scale end-
to-end training of convnets. We show that it is possible to obtain useful general-
purpose visual features with a clustering framework. Our approach, summarized
in Figure 1, consists in alternating between clustering of the image descriptors
and updating the weights of the convnet by predicting the cluster assignments.
For simplicity, we focus our study on k-means, but other clustering approaches
can be used, like Power Iteration Clustering (PIC) [23]. The overall pipeline is
suﬃciently close to the standard supervised training of a convnet to reuse many
common tricks [24]. Unlike self-supervised methods [25,26,27], clustering has the
advantage of requiring little domain knowledge and no speciﬁc signal from the
inputs [28,29]. Despite its simplicity, our approach achieves signiﬁcantly higher
performance than previously published unsupervised methods on both ImageNet
classiﬁcation and transfer tasks.

Finally, we probe the robustness of our framework by modifying the exper-
imental protocol, in particular the training set and the convnet architecture.
The resulting set of experiments extends the discussion initiated by Doersch et
al . [25] on the impact of these choices on the performance of unsupervised meth-

Deep Clustering for Unsupervised Learning of Visual Features

3

ods. We demonstrate that our approach is robust to a change of architecture.
Replacing an AlexNet by a VGG [30] signiﬁcantly improves the quality of the
features and their subsequent transfer performance. More importantly, we dis-
cuss the use of ImageNet as a training set for unsupervised models. While it
helps understanding the impact of the labels on the performance of a network,
ImageNet has a particular image distribution inherited from its use for a ﬁne-
grained image classiﬁcation challenge: it is composed of well-balanced classes and
contains a wide variety of dog breeds for example. We consider, as an alternative,
random Flickr images from the YFCC100M dataset of Thomee et al . [31]. We
show that our approach maintains state-of-the-art performance when trained on
this uncured data distribution. Finally, current benchmarks focus on the capa-
bility of unsupervised convnets to capture class-level information. We propose
to also evaluate them on image retrieval benchmarks to measure their capability
to capture instance-level information.

In this paper, we make the following contributions: (i) a novel unsupervised
method for the end-to-end learning of convnets that works with any standard
clustering algorithm, like k-means, and requires minimal additional steps; (ii)
state-of-the-art performance on many standard transfer tasks used in unsuper-
vised learning; (iii) performance above the previous state of the art when trained
on an uncured image distribution; (iv) a discussion about the current evaluation
protocol in unsupervised feature learning.

2 Related Work

Unsupervised learning of features. Several approaches related to our work
learn deep models with no supervision. Coates and Ng [32] also use k-means
to pre-train convnets, but learn each layer sequentially in a bottom-up fashion,
while we do it in an end-to-end fashion. Other clustering losses [21,22,33,34] have
been considered to jointly learn convnet features and image clusters but they
have never been tested on a scale to allow a thorough study on modern convnet
architectures. Of particular interest, Yang et al . [21] iteratively learn convnet
features and clusters with a recurrent framework. Their model oﬀers promising
performance on small datasets but may be challenging to scale to the number of
images required for convnets to be competitive. Closer to our work, Bojanowski
and Joulin [19] learn visual features on a large dataset with a loss that attempts
to preserve the information ﬂowing through the network [35]. Their approach
discriminates between images in a similar way as examplar SVM [36], while we
are simply clustering them.

Self-supervised learning. A popular form of unsupervised learning, called
“self-supervised learning” [37], uses pretext tasks to replace the labels annotated
by humans by “pseudo-labels” directly computed from the raw input data. For
example, Doersch et al . [25] use the prediction of the relative position of patches
in an image as a pretext task, while Noroozi and Favaro [26] train a network to
spatially rearrange shuﬄed patches. Another use of spatial cues is the work of

4

Mathilde Caron et al .

Pathak et al . [38] where missing pixels are guessed based on their surrounding.
Paulin et al . [39] learn patch level Convolutional Kernel Network [40] using an
image retrieval setting. Others leverage the temporal signal available in videos by
predicting the camera transformation between consecutive frames [41], exploit-
ing the temporal coherence of tracked patches [29] or segmenting video based
on motion [27]. Appart from spatial and temporal coherence, many other sig-
nals have been explored: image colorization [28,42], cross-channel prediction [43],
sound [44] or instance counting [45]. More recently, several strategies for com-
bining multiple cues have been proposed [46,47]. Contrary to our work, these
approaches are domain dependent, requiring expert knowledge to carefully de-
sign a pretext task that may lead to transferable features.

Generative models. Recently, unsupervised learning has been making a lot
of progress on image generation. Typically, a parametrized mapping is learned
between a predeﬁned random noise and the images, with either an autoen-
coder [18,48,49,50,51], a generative adversarial network (GAN) [17] or more
directly with a reconstruction loss [52]. Of particular interest, the discrimina-
tor of a GAN can produce visual features, but their performance are relatively
disappointing [20]. Donahue et al . [20] and Dumoulin et al . [53] have shown
that adding an encoder to a GAN produces visual features that are much more
competitive.

After a short introduction to the supervised learning of convnets, we describe
our unsupervised approach as well as the speciﬁcities of its optimization.

3 Method

3.1 Preliminaries

Modern approaches to computer vision, based on statistical learning, require
good image featurization. In this context, convnets are a popular choice for
mapping raw images to a vector space of ﬁxed dimensionality. When trained on
enough data, they constantly achieve the best performance on standard classi-
ﬁcation benchmarks [8,54]. We denote by fθ the convnet mapping, where θ is
the set of corresponding parameters. We refer to the vector obtained by apply-
ing this mapping to an image as feature or representation. Given a training set
X = {x1, x2, . . . , xN } of N images, we want to ﬁnd a parameter θ∗ such that
the mapping fθ∗ produces good general-purpose features.

These parameters are traditionally learned with supervision, i.e. each image
xn is associated with a label yn in {0, 1}k. This label represents the image’s
membership to one of k possible predeﬁned classes. A parametrized classiﬁer gW
predicts the correct labels on top of the features fθ(xn). The parameters W of
the classiﬁer and the parameter θ of the mapping are then jointly learned by

Deep Clustering for Unsupervised Learning of Visual Features

5

optimizing the following problem:

min
θ,W

1
N

N
(cid:88)

n=1

(cid:96) (gW (fθ(xn)) , yn) ,

(1)

where (cid:96) is the multinomial logistic loss, also known as the negative log-softmax
function. This cost function is minimized using mini-batch stochastic gradient
descent [55] and backpropagation to compute the gradient [56].

3.2 Unsupervised learning by clustering

When θ is sampled from a Gaussian distribution, without any learning, fθ does
not produce good features. However the performance of such random features on
standard transfer tasks, is far above the chance level. For example, a multilayer
perceptron classiﬁer on top of the last convolutional layer of a random AlexNet
achieves 12% in accuracy on ImageNet while the chance is at 0.1% [26]. The
good performance of random convnets is intimately tied to their convolutional
structure which gives a strong prior on the input signal. The idea of this work is
to exploit this weak signal to bootstrap the discriminative power of a convnet.
We cluster the output of the convnet and use the subsequent cluster assignments
as “pseudo-labels” to optimize Eq. (1). This deep clustering (DeepCluster) ap-
proach iteratively learns the features and groups them.

Clustering has been widely studied and many approaches have been devel-
oped for a variety of circumstances. In the absence of points of comparisons,
we focus on a standard clustering algorithm, k-means. Preliminary results with
other clustering algorithms indicates that this choice is not crucial. k-means
takes a set of vectors as input, in our case the features fθ(xn) produced by the
convnet, and clusters them into k distinct groups based on a geometric crite-
rion. More precisely, it jointly learns a d × k centroid matrix C and the cluster
assignments yn of each image n by solving the following problem:

min
C∈Rd×k

1
N

N
(cid:88)

n=1

min
yn∈{0,1}k

(cid:107)fθ(xn) − Cyn(cid:107)2
2

such that

y(cid:62)
n 1k = 1.

(2)

Solving this problem provides a set of optimal assignments (y∗
n)n≤N and a cen-
troid matrix C ∗. These assignments are then used as pseudo-labels; we make no
use of the centroid matrix.

Overall, DeepCluster alternates between clustering the features to produce
pseudo-labels using Eq. (2) and updating the parameters of the convnet by
predicting these pseudo-labels using Eq. (1). This type of alternating procedure
is prone to trivial solutions; we describe how to avoid such degenerate solutions
in the next section.

3.3 Avoiding trivial solutions

The existence of trivial solutions is not speciﬁc to the unsupervised training of
neural networks, but to any method that jointly learns a discriminative classi-
ﬁer and the labels. Discriminative clustering suﬀers from this issue even when

6

Mathilde Caron et al .

applied to linear models [57]. Solutions are typically based on constraining or
penalizing the minimal number of points per cluster [58,59]. These terms are
computed over the whole dataset, which is not applicable to the training of con-
vnets on large scale datasets. In this section, we brieﬂy describe the causes of
these trivial solutions and give simple and scalable workarounds.

Empty clusters. A discriminative model learns decision boundaries between
classes. An optimal decision boundary is to assign all of the inputs to a sin-
gle cluster [57]. This issue is caused by the absence of mechanisms to prevent
from empty clusters and arises in linear models as much as in convnets. A com-
mon trick used in feature quantization [60] consists in automatically reassigning
empty clusters during the k-means optimization. More precisely, when a cluster
becomes empty, we randomly select a non-empty cluster and use its centroid
with a small random perturbation as the new centroid for the empty cluster. We
then reassign the points belonging to the non-empty cluster to the two resulting
clusters.

Trivial parametrization. If the vast majority of images is assigned to a few
clusters, the parameters θ will exclusively discriminate between them. In the
most dramatic scenario where all but one cluster are singleton, minimizing
Eq. (1) leads to a trivial parametrization where the convnet will predict the
same output regardless of the input. This issue also arises in supervised classiﬁ-
cation when the number of images per class is highly unbalanced. For example,
metadata, like hashtags, exhibits a Zipf distribution, with a few labels dominat-
ing the whole distribution [61]. A strategy to circumvent this issue is to sample
images based on a uniform distribution over the classes, or pseudo-labels. This is
equivalent to weight the contribution of an input to the loss function in Eq. (1)
by the inverse of the size of its assigned cluster.

3.4

Implementation details

Convnet architectures. For comparison with previous works, we use a stan-
dard AlexNet [54] architecture. It consists of ﬁve convolutional layers with 96,
256, 384, 384 and 256 ﬁlters; and of three fully connected layers. We remove the
Local Response Normalization layers and use batch normalization [24]. We also
consider a VGG-16 [30] architecture with batch normalization. Unsupervised
methods often do not work directly on color and diﬀerent strategies have been
considered as alternatives [25,26]. We apply a ﬁxed linear transformation based
on Sobel ﬁlters to remove color and increase local contrast [19,39].

Training data. We train DeepCluster on ImageNet [6] unless mentioned oth-
erwise. It contains 1.3M images uniformly distributed into 1, 000 classes.

Optimization. We cluster the central cropped images features and perform
data augmentation (random horizontal ﬂips and crops of random sizes and as-
pect ratios) when training the network. This enforces invariance to data aug-

Deep Clustering for Unsupervised Learning of Visual Features

7

(a) Clustering quality

(b) Cluster reassignment

(c) Inﬂuence of k

Fig. 2: Preliminary studies. (a): evolution of the clustering quality along train-
ing epochs; (b): evolution of cluster reassignments at each clustering step; (c):
validation mAP classiﬁcation performance for various choices of k.

mentation which is useful for feature learning [33]. The network is trained with
dropout [62], a constant step size, an (cid:96)2 penalization of the weights θ and a mo-
mentum of 0.9. Each mini-batch contains 256 images. For the clustering, features
are PCA-reduced to 256 dimensions, whitened and (cid:96)2-normalized. We use the
k-means implementation of Johnson et al . [60]. Note that running k-means takes
a third of the time because a forward pass on the full dataset is needed. One
could reassign the clusters every n epochs, but we found out that our setup on
ImageNet (updating the clustering every epoch) was nearly optimal. On Flickr,
the concept of epoch disappears: choosing the tradeoﬀ between the parameter
updates and the cluster reassignments is more subtle. We thus kept almost the
same setup as in ImageNet. We train the models for 500 epochs, which takes 12
days on a Pascal P100 GPU for AlexNet.

Hyperparameter selection. We select hyperparameters on a down-stream
task, i.e., object classiﬁcation on the validation set of Pascal VOC with no
ﬁne-tuning. We use the publicly available code of Kr¨ahenb¨uhl1.

4 Experiments

In a preliminary set of experiments, we study the behavior of DeepCluster dur-
ing training. We then qualitatively assess the ﬁlters learned with DeepCluster
before comparing our approach to previous state-of-the-art models on standard
benchmarks.

1 https://github.com/philkr/voc-classiﬁcation

8

Mathilde Caron et al .

Fig. 3: Filters from the ﬁrst layer of an AlexNet trained on unsupervised Ima-
geNet on raw RGB input (left) or after a Sobel ﬁltering (right).

4.1 Preliminary study

We measure the information shared between two diﬀerent assignments A and B
of the same data by the Normalized Mutual Information (NMI), deﬁned as:

NMI(A; B) =

I(A; B)
(cid:112)H(A)H(B)

where I denotes the mutual information and H the entropy. This measure can
be applied to any assignment coming from the clusters or the true labels. If the
two assignments A and B are independent, the NMI is equal to 0. If one of them
is deterministically predictable from the other, the NMI is equal to 1.

Relation between clusters and labels. Figure 2(a) shows the evolution of
the NMI between the cluster assignments and the ImageNet labels during train-
ing. It measures the capability of the model to predict class level information.
Note that we only use this measure for this analysis and not in any model se-
lection process. The dependence between the clusters and the labels increases
over time, showing that our features progressively capture information related
to object classes.

Number of reassignments between epochs. At each epoch, we reassign the
images to a new set of clusters, with no guarantee of stability. Measuring the
NMI between the clusters at epoch t − 1 and t gives an insight on the actual
stability of our model. Figure 2(b) shows the evolution of this measure during
training. The NMI is increasing, meaning that there are less and less reassign-
ments and the clusters are stabilizing over time. However, NMI saturates below
0.8, meaning that a signiﬁcant fraction of images are regularly reassigned be-
tween epochs. In practice, this has no impact on the training and the models do
not diverge.

Choosing the number of clusters. We measure the impact of the number k of
clusters used in k-means on the quality of the model. We report the same down-

Deep Clustering for Unsupervised Learning of Visual Features

9

conv1

conv3

conv5

Fig. 4: Filter visualization and top 9 activated images from a subset of 1 million
images from YFCC100M for target ﬁlters in the layers conv1, conv3 and conv5
of an AlexNet trained with DeepCluster on ImageNet. The ﬁlter visualization
is obtained by learning an input image that maximizes the response to a target
ﬁlter [64].

stream task as in the hyperparameter selection process, i.e. mAP on the Pascal
VOC 2007 classiﬁcation validation set. We vary k on a logarithmic scale, and
report results after 300 epochs in Figure 2(c). The performance after the same
number of epochs for every k may not be directly comparable, but it reﬂects
the hyper-parameter selection process used in this work. The best performance
is obtained with k = 10, 000. Given that we train our model on ImageNet, one
would expect k = 1000 to yield the best results, but apparently some amount of
over-segmentation is beneﬁcial.

4.2 Visualizations

First layer ﬁlters. Figure 3 shows the ﬁlters from the ﬁrst layer of an AlexNet
trained with DeepCluster on raw RGB images and images preprocessed with a
Sobel ﬁltering. The diﬃculty of learning convnets on raw images has been noted
before [19,25,26,39]. As shown in the left panel of Fig. 3, most ﬁlters capture only
color information that typically plays a little role for object classiﬁcation [63].
Filters obtained with Sobel preprocessing act like edge detectors.

Probing deeper layers. We assess the quality of a target ﬁlter by learning
an input image that maximizes its activation [65,66]. We follow the process
described by Yosinki et al . [64] with a cross entropy function between the target
ﬁlter and the other ﬁlters of the same layer. Figure 4 shows these synthetic
images as well as the 9 top activated images from a subset of 1 million images
from YFCC100M. As expected, deeper layers in the network seem to capture
larger textural structures. However, some ﬁlters in the last convolutional layers
seem to be simply replicating the texture already captured in previous layers,
as shown on the second row of Fig. 5. This result corroborates the observation

10

Mathilde Caron et al .

Filter 0

Filter 33

Filter 145

Filter 194

Filter 97

Filter 116

Filter 119

Filter 182

Fig. 5: Top 9 activated images from a random subset of 10 millions images from
YFCC100M for target ﬁlters in the last convolutional layer. The top row cor-
responds to ﬁlters sensitive to activations by images containing objects. The
bottom row exhibits ﬁlters more sensitive to stylistic eﬀects. For instance, the
ﬁlters 119 and 182 seem to be respectively excited by background blur and depth
of ﬁeld eﬀects.

by Zhang et al . [43] that features from conv3 or conv4 are more discriminative
than those from conv5.

Finally, Figure 5 shows the top 9 activated images of some conv5 ﬁlters that
seem to be semantically coherent. The ﬁlters on the top row contain information
about structures that highly corrolate with object classes. The ﬁlters on the
bottom row seem to trigger on style, like drawings or abstract shapes.

4.3 Linear classiﬁcation on activations

Following Zhang et al . [43], we train a linear classiﬁer on top of diﬀerent frozen
convolutional layers. This layer by layer comparison with supervised features
exhibits where a convnet starts to be task speciﬁc, i.e. specialized in object
classiﬁcation. We report the results of this experiment on ImageNet and the
Places dataset [67] in Table 1. We choose the hyperparameters by cross-validation
on the training set. On ImageNet, DeepCluster outperforms the state of the art
from conv3 to conv5 layers by 3−5%. The largest improvement is observed in the
conv4 layer, while the conv1 layer performs poorly, probably because the Sobel
ﬁltering discards color. Consistently with the ﬁlter visualizations of Sec. 4.2,
conv3 works better than conv5. Finally, the diﬀerence of performance between
DeepCluster and a supervised AlexNet grows signiﬁcantly on higher layers: at
layers conv2-conv3 the diﬀerence is only around 6%, but this diﬀerence rises to

Deep Clustering for Unsupervised Learning of Visual Features

11

ImageNet

Places

Method

Places labels
ImageNet labels
Random

Pathak et al . [38]
Doersch et al . [25]
Zhang et al . [28]
Donahue et al . [20]
Noroozi and Favaro [26]
Noroozi et al . [45]
Zhang et al . [43]

conv1 conv2 conv3 conv4 conv5

conv1 conv2 conv3 conv4 conv5

–
19.3
11.6

–
36.3
17.1

–
44.2
16.9

21.0
20.7
14.1
30.2
23.3
16.2
30.4
24.5
12.5
31.0
17.7
24.5
18.2 28.8
34.0
18.0 30.6 34.3
35.4
29.3
17.7

–
48.3
16.3

19.8
31.7
31.5
29.9
33.9
32.5
35.2

–
50.5
14.1

15.5
29.6
30.3
28.0
27.1
25.7
32.8

22.1
22.7
15.7

35.1
34.8
20.3

40.2
38.4
19.8

23.4
23.2
18.2
31.9
26.7
19.7
29.6
25.7
16.0
27.1
26.2
21.4
23.0
35.5
32.1
23.3 33.9 36.3
34.0
30.7
21.3

43.3
39.4
19.1

21.9
32.7
30.3
26.1
34.8
34.7
34.1

44.6
38.7
17.5

18.4
30.9
29.7
24.0
31.3
29.6
32.5

DeepCluster

12.9

29.2 38.2 39.8 36.1

18.6

30.8 37.0 37.5 33.1

Table 1: Linear classiﬁcation on ImageNet and Places using activations from the
convolutional layers of an AlexNet as features. We report classiﬁcation accuracy
on the central crop. Numbers for other methods are from Zhang et al . [43].

14.4% at conv5, marking where the AlexNet probably stores most of the class
level information. In the supplementary material, we also report the accuracy if
a MLP is trained on the last layer; DeepCluster outperforms the state of the art
by 8%.

The same experiment on the Places dataset provides some interesting in-
sights: like DeepCluster, a supervised model trained on ImageNet suﬀers from
a decrease of performance for higher layers (conv4 versus conv5). Moreover,
DeepCluster yields conv3-4 features that are comparable to those trained with
ImageNet labels. This suggests that when the target task is suﬃcently far from
the domain covered by ImageNet, labels are less important.

4.4 Pascal VOC 2007

Finally, we do a quantitative evaluation of DeepCluster on image classiﬁcation,
object detection and semantic segmentation on Pascal VOC. The relatively
small size of the training sets on Pascal VOC (2, 500 images) makes this setup
closer to a “real-world” application, where a model trained with heavy compu-
tational resources, is adapted to a task or a dataset with a small number of
instances. Detection results are obtained using fast-rcnn2; segmentation re-
sults are obtained using the code of Shelhamer et al .3. For classiﬁcation and
detection, we report the performance on the test set of Pascal VOC 2007 and
choose our hyperparameters on the validation set. For semantic segmentation,
following the related work, we report the performance on the validation set of
Pascal VOC 2012.

2 https://github.com/rbgirshick/py-faster-rcnn
3 https://github.com/shelhamer/fcn.berkeleyvision.org

12

Mathilde Caron et al .

Method

ImageNet labels
Random-rgb
Random-sobel

Pathak et al . [38]
Donahue et al . [20]∗
Pathak et al . [27]
Owens et al . [44]∗
Wang and Gupta [29]∗
Doersch et al . [25]∗
Bojanowski and Joulin [19]∗
Zhang et al . [28]∗
Zhang et al . [43]∗
Noroozi and Favaro [26]
Noroozi et al . [45]

Classiﬁcation

Detection

Segmentation

fc6-8 all

fc6-8 all

fc6-8 all

78.9
33.2
29.0

34.6
52.3
–
52.3
55.6
55.1
56.7
61.5
63.0
–
–

79.9
57.0
61.9

56.5
60.1
61.0
61.3
63.1
65.3
65.3
65.9
67.1
67.6
67.7

–
22.2
18.9

–
–
–
–

56.8
44.5
47.9

44.5
46.9
52.2
–

–

32.8† 47.2
51.1
33.7† 49.4
43.4† 46.9
46.7
53.2
51.4

–
–
–

–
15.2
13.0

–
–
–
–

–

48.0
30.1
32.0

29.7
35.2
–
–

–

26.0† 35.4†

26.7† 37.1†
35.8†
35.6
36.0
–
37.6
–
36.6
–

DeepCluster

70.4 73.7

51.4 55.4

43.2 45.1

Table 2: Comparison of the proposed approach to state-of-the-art unsupervised
feature learning on classiﬁcation, detection and segmentation on Pascal VOC.
∗ indicates the use of the data-dependent initialization of Kr¨ahenb¨uhl et al . [68].
Numbers for other methods produced by us are marked with a †.

Table 2 summarized the comparisons of DeepCluster with other feature-
learning approaches on the three tasks. As for the previous experiments, we
outperform previous unsupervised methods on all three tasks, in every setting.
The improvement with ﬁne-tuning over the state of the art is the largest on se-
mantic segmentation (7.5%). On detection, DeepCluster performs only slightly
better than previously published methods. Interestingly, a ﬁne-tuned random
network performs comparatively to many unsupervised methods, but performs
poorly if only fc6-8 are learned. For this reason, we also report detection and
segmentation with fc6-8 for DeepCluster and a few baselines. These tasks are
closer to a real application where ﬁne-tuning is not possible. It is in this setting
that the gap between our approach and the state of the art is the greater (up to
9% on classiﬁcation).

5 Discussion

The current standard for the evaluation of an unsupervised method involves the
use of an AlexNet architecture trained on ImageNet and tested on class-level
tasks. To understand and measure the various biases introduced by this pipeline
on DeepCluster, we consider a diﬀerent training set, a diﬀerent architecture and
an instance-level recognition task.

Deep Clustering for Unsupervised Learning of Visual Features

13

Classiﬁcation Detection

Segmentation

Method

Training set

fc6-8 all

fc6-8 all

fc6-8 all

Best competitor

ImageNet

63.0

67.7

43.4† 53.2

35.8†

DeepCluster
DeepCluster

ImageNet
YFCC100M

72.0
67.3

73.7
69.3

51.4
45.6

55.4
53.0

43.2
39.2

37.7

45.1
42.2

Table 3: Impact of the training set on the performance of DeepCluster mea-
sured on the Pascal VOC transfer tasks as described in Sec. 4.4. We compare
ImageNet with a subset of 1M images from YFCC100M [31]. Regardless of the
training set, DeepCluster outperforms the best published numbers on most tasks.
Numbers for other methods produced by us are marked with a †

5.1

ImageNet versus YFCC100M

ImageNet is a dataset designed for a ﬁne-grained object classiﬁcation chal-
lenge [69]. It is object oriented, manually annotated and organised into well
balanced object categories. By design, DeepCluster favors balanced clusters and,
as discussed above, our number of cluster k is somewhat comparable with the
number of labels in ImageNet. This may have given an unfair advantage to
DeepCluster over other unsupervised approaches when trained on ImageNet. To
measure the impact of this eﬀect, we consider a subset of randomly-selected 1M
images from the YFCC100M dataset [31] for the pre-training. Statistics on the
hashtags used in YFCC100M suggests that the underlying “object classes” are
severly unbalanced [61], leading to a data distribution less favorable to Deep-
Cluster.

Table 3 shows the diﬀerence in performance on Pascal VOC of DeepClus-
ter pre-trained on YFCC100M compared to ImageNet. As noted by Doersch et
al . [25], this dataset is not object oriented, hence the performance are expected to
drop by a few percents. However, even when trained on uncured Flickr images,
DeepCluster outperforms the current state of the art by a signiﬁcant margin
on most tasks (up to +4.3% on classiﬁcation and +4.5% on semantic segmen-
tation). We report the rest of the results in the supplementary material with
similar conclusions. This experiment validates that DeepCluster is robust to a
change of image distribution, leading to state-of-the-art general-purpose visual
features even if this distribution is not favorable to its design.

5.2 AlexNet versus VGG

In the supervised setting, deeper architectures like VGG or ResNet [8] have a
much higher accuracy on ImageNet than AlexNet. We should expect the same
improvement if these architectures are used with an unsupervised approach. Ta-
ble 4 compares a VGG-16 and an AlexNet trained with DeepCluster on ImageNet
and tested on the Pascal VOC 2007 object detection task with ﬁne-tuning. We
also report the numbers obtained with other unsupervised approaches [25,46].

14

Mathilde Caron et al .

Method

AlexNet VGG-16

Method

Oxford5K Paris6K

ImageNet labels
Random

Doersch et al . [25]
Wang and Gupta [29]
Wang et al . [46]

56.8
47.8

51.1
47.2
–

67.3
39.7

61.5
60.2
63.2

DeepCluster

55.4

65.9

Table 4: Pascal VOC 2007 object
detection with AlexNet and VGG-
16. Numbers are taken from Wang et
al . [46].

ImageNet labels
Random

Doersch et al . [25]
Wang et al . [46]

72.4
6.9

35.4
42.3

81.5
22.0

53.1
58.0

DeepCluster

61.0

72.0

Table 5: mAP on instance-level im-
age retrieval on Oxford and Paris
dataset with a VGG-16. We apply
R-MAC with a resolution of 1024
pixels and 3 grid levels [70].

Regardless of the approach, a deeper architecture leads to a signiﬁcant improve-
ment in performance on the target task. Training the VGG-16 with DeepCluster
gives a performance above the state of the art, bringing us to only 1.4 percents
below the supervised topline. Note that the diﬀerence between unsupervised and
supervised approaches remains in the same ballpark for both architectures (i.e.
1.4%). Finally, the gap with a random baseline grows for larger architectures,
justifying the relevance of unsupervised pre-training for complex architectures
when little supervised data is available.

5.3 Evaluation on instance retrieval

The previous benchmarks measure the capability of an unsupervised network to
capture class level information. They do not evaluate if it can diﬀerentiate images
at the instance level. To that end, we propose image retrieval as a down-stream
task. We follow the experimental protocol of Tolias et al . [70] on two datasets,
i.e., Oxford Buildings [71] and Paris [72]. Table 5 reports the performance of a
VGG-16 trained with diﬀerent approaches obtained with Sobel ﬁltering, except
for Doersch et al . [25] and Wang et al . [46]. This preprocessing improves by
5.5 points the mAP of a supervised VGG-16 on the Oxford dataset, but not on
Paris. This may translate in a similar advantage for DeepCluster, but it does not
account for the average diﬀerences of 19 points. Interestingly, random convnets
perform particularly poorly on this task compared to pre-trained models. This
suggests that image retrieval is a task where the pre-training is essential and
studying it as a down-stream task could give further insights about the quality
of the features produced by unsupervised approaches.

6 Conclusion

In this paper, we propose a scalable clustering approach for the unsupervised
learning of convnets. It iterates between clustering with k-means the features

Deep Clustering for Unsupervised Learning of Visual Features

15

produced by the convnet and updating its weights by predicting the cluster as-
signments as pseudo-labels in a discriminative loss. If trained on large dataset like
ImageNet or YFCC100M, it achieves performance that are signiﬁcantly better
than the previous state-of-the-art on every standard transfer task. Our approach
makes little assumption about the inputs, and does not require much domain
speciﬁc knowledge, making it a good candidate to learn deep representations
speciﬁc to domains where annotations are scarce.

Acknowledgement. We thank Alexandre Sablayrolles and the rest of the FAIR
team for their feedback and fruitful discussion around this paper. We would like
to particularly thank Ishan Misra for spotting an error in our evaluation setting
of Table 1.

16

Mathilde Caron et al .

References

1. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object

detection with region proposal networks. In: NIPS. (2015)

2. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs. arXiv preprint arXiv:1606.00915 (2016)

3. Weinzaepfel, P., Revaud, J., Harchaoui, Z., Schmid, C.: Deepﬂow: Large displace-

ment optical ﬂow with deep matching. In: ICCV. (2013)

4. Carreira, J., Agrawal, P., Fragkiadaki, K., Malik, J.: Human pose estimation with

iterative error feedback. In: CVPR. (2016)

5. Sharif Razavian, A., Azizpour, H., Sullivan, J., Carlsson, S.: Cnn features oﬀ-the-

shelf: an astounding baseline for recognition. In: CVPR workshops. (2014)

6. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale

hierarchical image database. In: CVPR. (2009)

7. Stock, P., Cisse, M.:

Convnets and imagenet beyond accuracy: Explana-
tions, bias detection, adversarial examples and model criticism. arXiv preprint
arXiv:1711.11443 (2017)

8. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: Surpassing human-

level performance on imagenet classiﬁcation. In: ICCV. (2015)

9. Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected

convolutional networks. arXiv preprint arXiv:1608.06993 (2016)

10. Kovashka, A., Russakovsky, O., Fei-Fei, L., Grauman, K., et al.: Crowdsourcing
in computer vision. Foundations and Trends R(cid:13) in Computer Graphics and Vision
10(3) (2016) 177–243

11. Misra, I., Zitnick, C.L., Mitchell, M., Girshick, R.: Seeing through the human
reporting bias: Visual classiﬁers from noisy human-centric labels. In: CVPR. (2016)
12. Friedman, J., Hastie, T., Tibshirani, R.: The elements of statistical learning. Vol-

ume 1. Springer series in statistics New York (2001)

13. Turk, M.A., Pentland, A.P.: Face recognition using eigenfaces. In: CVPR. (1991)
14. Shi, J., Malik, J.: Normalized cuts and image segmentation. TPAMI 22(8) (2000)

888–905

15. Joulin, A., Bach, F., Ponce, J.:
segmentation. In: CVPR. (2010)

Discriminative clustering for image co-

16. Csurka, G., Dance, C., Fan, L., Willamowski, J., Bray, C.: Visual categorization
with bags of keypoints. In: Workshop on statistical learning in computer vision,
ECCV. Volume 1., Prague (2004) 1–2

17. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)

18. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint

19. Bojanowski, P., Joulin, A.: Unsupervised learning by predicting noise.

ICML

arXiv:1312.6114 (2013)

(2017)

20. Donahue, J., Kr¨ahenb¨uhl, P., Darrell, T.: Adversarial feature learning. arXiv

21. Yang, J., Parikh, D., Batra, D.: Joint unsupervised learning of deep representations

preprint arXiv:1605.09782 (2016)

and image clusters. In: CVPR. (2016)

22. Xie, J., Girshick, R., Farhadi, A.: Unsupervised deep embedding for clustering

analysis. In: ICML. (2016)

23. Lin, F., Cohen, W.W.: Power iteration clustering. In: ICML. (2010)

Deep Clustering for Unsupervised Learning of Visual Features

17

24. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015)

25. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning

26. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving

by context prediction. In: ICCV. (2015)

jigsaw puzzles. In: ECCV. (2016)

27. Pathak, D., Girshick, R., Doll´ar, P., Darrell, T., Hariharan, B.: Learning features

by watching objects move. CVPR (2017)

28. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: ECCV. (2016)
29. Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos.

In: ICCV. (2015)

30. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556 (2014)

31. Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth,
D., Li, L.J.: The new data and new challenges in multimedia research. arXiv
preprint arXiv:1503.01817 (2015)

32. Coates, A., Ng, A.Y.: Learning feature representations with k-means. In: Neural

networks: Tricks of the trade. Springer (2012) 561–580

33. Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative un-
supervised feature learning with convolutional neural networks. In: NIPS. (2014)
34. Liao, R., Schwing, A., Zemel, R., Urtasun, R.: Learning deep parsimonious repre-

35. Linsker, R.: Towards an organizing principle for a layered perceptual network. In:

sentations. In: NIPS. (2016)

NIPS. (1988)

36. Malisiewicz, T., Gupta, A., Efros, A.A.: Ensemble of exemplar-svms for object

detection and beyond. In: ICCV. (2011)

37. de Sa, V.R.: Learning classiﬁcation with unlabeled data. In: NIPS. (1994)
38. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-

coders: Feature learning by inpainting. In: CVPR. (2016)

39. Paulin, M., Douze, M., Harchaoui, Z., Mairal, J., Perronin, F., Schmid, C.: Local
convolutional features with unsupervised training for image retrieval. In: ICCV.
(2015)

40. Mairal, J., Koniusz, P., Harchaoui, Z., Schmid, C.: Convolutional kernel networks.

In: NIPS. (2014)

41. Agrawal, P., Carreira, J., Malik, J.: Learning to see by moving. In: ICCV. (2015)
42. Larsson, G., Maire, M., Shakhnarovich, G.: Learning representations for automatic

colorization. In: ECCV. (2016)

43. Zhang, R., Isola, P., Efros, A.A.: Split-brain autoencoders: Unsupervised learning

by cross-channel prediction. arXiv preprint arXiv:1611.09842 (2016)

44. Owens, A., Wu, J., McDermott, J.H., Freeman, W.T., Torralba, A.: Ambient sound

provides supervision for visual learning. In: ECCV. (2016)

45. Noroozi, M., Pirsiavash, H., Favaro, P.: Representation learning by learning to

count. In: ICCV. (2017)

46. Wang, X., He, K., Gupta, A.: Transitive invariance for self-supervised visual rep-

resentation learning. arXiv preprint arXiv:1708.02901 (2017)

47. Doersch, C., Zisserman, A.: Multi-task self-supervised visual learning. (2017)
48. Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: Greedy layer-wise training

of deep networks. In: NIPS. (2007)

49. Huang, F.J., Boureau, Y.L., LeCun, Y., et al.: Unsupervised learning of invariant
feature hierarchies with applications to object recognition. In: CVPR. (2007)

18

Mathilde Caron et al .

50. Masci, J., Meier, U., Cire¸san, D., Schmidhuber, J.: Stacked convolutional auto-
encoders for hierarchical feature extraction. Artiﬁcial Neural Networks and Ma-
chine Learning (2011) 52–59

51. Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.A.: Stacked denois-
ing autoencoders: Learning useful representations in a deep network with a local
denoising criterion. JMLR 11(Dec) (2010) 3371–3408

52. Bojanowski, P., Joulin, A., Lopez-Paz, D., Szlam, A.: Optimizing the latent space

of generative networks. arXiv preprint arXiv:1707.05776 (2017)

53. Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky, M., Mastropietro, O.,
Courville, A.: Adversarially learned inference. arXiv preprint arXiv:1606.00704
(2016)

54. Krizhevsky, A., Sutskever, I., Hinton, G.E.:

Imagenet classiﬁcation with deep

convolutional neural networks. In: NIPS. (2012)

55. Bottou, L.: Stochastic gradient descent tricks. In: Neural networks: Tricks of the

trade. Springer (2012) 421–436

56. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to

document recognition. Proceedings of the IEEE 86(11) (1998) 2278–2324

57. Xu, L., Neufeld, J., Larson, B., Schuurmans, D.: Maximum margin clustering. In:

58. Bach, F.R., Harchaoui, Z.: Diﬀrac: a discriminative and ﬂexible framework for

59. Joulin, A., Bach, F.: A convex relaxation for weakly supervised classiﬁers. arXiv

60. Johnson, J., Douze, M., J´egou, H.: Billion-scale similarity search with gpus. arXiv

NIPS. (2005)

clustering. In: NIPS. (2008)

preprint arXiv:1206.6413 (2012)

preprint arXiv:1702.08734 (2017)

61. Joulin, A., van der Maaten, L., Jabri, A., Vasilache, N.: Learning visual features

from large weakly supervised data. In: ECCV. (2016)

62. Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overﬁtting. JMLR 15(1)
(2014) 1929–1958

63. Van De Sande, K., Gevers, T., Snoek, C.: Evaluating color descriptors for object

and scene recognition. TPAMI 32(9) (2010) 1582–1596

64. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., Lipson, H.: Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579 (2015)
65. Erhan, D., Bengio, Y., Courville, A., Vincent, P.: Visualizing higher-layer features

of a deep network. University of Montreal 1341 (2009) 3

66. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.

In: ECCV. (2014)

67. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features

for scene recognition using places database. In: NIPS. (2014)

68. Kr¨ahenb¨uhl, P., Doersch, C., Donahue, J., Darrell, T.: Data-dependent initializa-
tions of convolutional neural networks. arXiv preprint arXiv:1511.06856 (2015)
69. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-
nition challenge. IJCV 115(3) (2015) 211–252

70. Tolias, G., Sicre, R., J´egou, H.: Particular object retrieval with integral max-

pooling of cnn activations. arXiv preprint arXiv:1511.05879 (2015)

71. Philbin, J., Chum, O., Isard, M., Sivic, J., Zisserman, A.: Object retrieval with

large vocabularies and fast spatial matching. In: CVPR. (2007)

Deep Clustering for Unsupervised Learning of Visual Features

19

72. Philbin, J., Chum, O., Isard, M., Sivic, J., Zisserman, A.: Lost in quantization:
Improving particular object retrieval in large scale image databases. In: CVPR.
(2008)

73. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in

deep neural networks? In: NIPS. (2014)

74. Liang, X., Liu, S., Wei, Y., Liu, L., Lin, L., Yan, S.: Towards computational baby
learning: A weakly-supervised approach for object detection. In: ICCV. (2015)
75. Douze, M., J´egou, H., Johnson, J.: An evaluation of large-scale methods for image

instance and class discovery. arXiv preprint arXiv:1708.02898 (2017)

76. Cho, M., Lee, K.M.: Mode-seeking on graphs via random walks. (June 2012)

Appendix

1 Additional results

1.1 Classiﬁcation on ImageNet

Noroozi and Favaro [26] suggest to evaluate networks trained in an unsuper-
vised way by freezing the convolutional layers and retrain on ImageNet the fully
connected layers using labels and reporting accuracy on the validation set. This
experiment follows the observation of Yosinki et al . [73] that general-purpose
features appear in the convolutional layers of an AlexNet. We report a compar-
ison of DeepCluster to other AlexNet networks trained with no supervision, as
well as random and supervised baselines in Table 1.

DeepCluster outperforms state-of-the-art unsupervised methods by a signif-
icant margin, achieving 8% better accuracy than the previous best performing
method. This means that DeepCluster halves the gap with networks trained in
a supervised setting.

1.2 Stopping criterion

We monitor how the features learned with DeepCluster evolve along the train-
ing epochs on a down-stream task: object classiﬁcation on the validation set of
Pascal VOC with no ﬁne-tuning. We use this measure to select the hyperpa-
rameters of our model as well as to check when the features stop improving. In
Figure 1, we show the evolution of both the classiﬁcation accuracy on this task
and a measure of the clustering quality (NMI between the cluster assignments
and the true labels) throughout the training. Unsurprisingly, we notice that the
clustering and features qualities follow the same dynamic. The performance sat-
urates after 400 epochs of training.

2 Further discussion

In this section, we discuss some technical choices and variants of DeepCluster
more speciﬁcally.

2.1 Alternative clustering algorithm

Graph clustering. We consider Power Iteration Clustering (PIC) [23] as an
alternative clustering method. It has been shown to yield good performance for
large scale collections [75].

Since PIC is a graph clustering approach, we generate a nearest neighbor
graph by connecting all images to their 5 neighbors in the Euclidean space of

Appendix

21

Method

Pre-trained dataset Acc@1

Wang et al . [29]

YouTube100K [74]

Supervised
Supervised Sobel
Random

Doersch et al . [25]
Donahue et al . [20]
Noroozi and Favaro [26]
Zhang et al . [28]
Bojanowski and Joulin [19]

DeepCluster
DeepCluster

ImageNet
ImageNet
-

ImageNet
ImageNet
ImageNet
ImageNet
ImageNet

ImageNet
YFCC100M

59.7
57.8
12.0

29.8

30.4
32.2
34.6
35.2
36.0

44.0
39.6

Table 1: Comparison of DeepCluster to AlexNet features pre-trained supervis-
edly and unsupervisedly on diﬀerent datasets. A full multi-layer perceptron is
retrained on top of the frozen pre-trained features. We report classiﬁcation ac-
curacy (acc@1). Expect for Noroozi and Favaro [26], all the numbers are taken
from Bojanowski and Joulin [19].

Fig. 1: In red: validation mAP Pascal VOC classiﬁcation performance. In blue:
evolution of the clustering quality.

image descriptors. We denote by fθ(x) the output of the network with parameters
θ applied to image x. We use the sparse graph matrix G = Rn×n. We set the
diagonal of G to 0 and non-zero entries are deﬁned as

wij = e−

(cid:107)fθ (xi)−fθ (xj )(cid:107)2
σ2

with σ a bandwidth parameter. In this work, we use a variant of PIC [75,76]
that does:

1. Initialize v ← [1/n, ..., 1/n](cid:62) ∈ Rn;

22

2. Iterate

v ← N1(α(G + G(cid:62))v + (1 − α)v),
where α = 10−3 is a regularization parameter and N1 : v (cid:55)→ v/(cid:107)v(cid:107)1 the
L1-normalization function;

3. Let G(cid:48) be the directed unweighted subgraph of G where we keep edge i → j

of G such that

j = argmaxjwij(vj − vi).

If vi is a local maximum (ie. ∀j (cid:54)= i, vj ≤ vi), then no edge starts from it.
The clusters are given by the connected components of G(cid:48). Each cluster has
one local maximum.

An advantage of PIC clustering is not to require the setting beforehand of the
number of clusters. However, the parameter σ inﬂuences the number of clusters:
when it is larger, the edges become more uniform and the number of clusters
decreases, and the other way round when σ increased. In the following, we set
σ = 0.2.

As our PIC implementation relies on a graph of nearest neighbors, we show
in Figure 2 some query images and their 3 nearest neighbors in the feature space
with a network trained with the PIC version of DeepCluster and a random base-
line. A randomly initialized network performs quite well in some cases (sunset
query for example), where the image has a simple low-level structure. The top
row in Fig. 2 seems to represent a query for which the performance of the ran-
dom network is quite good whereas the bottom query is too complex for the
random network to retrieve good matches. Moreover, we notice that the nearest
neighbors matches are largely improved after the network has been trained with
DeepCluster.
Comparison with k-means. First, we give an insight about the distribution
of the images in the clusters. We show in Figure 3 the sizes of the clusters
produced by the k-means and PIC versions of DeepCluster at the last epoch of
training (this distribution is stable along the epochs). We observe that k-means
produces more balanced clusters than PIC. Indeed, for PIC, almost one third of
the clusters are of a size lower than 10 while the biggest cluster contains roughly
3000 examples. In this situation of very unbalanced clusters, it is important
in our method to train the convnet by sampling images based on a uniform
distribution over the clusters to prevent the biggest cluster from dominating the
training.

We report in Table 2 the results for the diﬀerent Pascal VOC transfer tasks
with a model trained with the PIC version of DeepCluster. For this set of transfer
tasks, the models trained with k-means and PIC versions of DeepCluster perform
in comparable ranges.

2.2 Variants of DeepCluster

In Table 3, we report the results of models trained with diﬀerent variants of
DeepCluster: a diﬀerent training set, an alternative clustering method or with-

Query

Random

DeepCluster PIC

Appendix

23

Fig. 2: Images and their 3 nearest neighbors in a subset of Flickr in the feature
space. The query images are shown on the left column. The following 3 columns
correspond to a randomly initialized network and the last 3 to the same network
after training with PIC DeepCluster.

Fig. 3: Sizes of clusters produced by the k-means and PIC versions of DeepCluster
at the last epoch of training.

out input preprocessing. In particular, we notice that the performance of Deep-
Cluster on raw RGB images degrades signiﬁcantly.

In Table 4, we compare the performance of DeepCluster depending on the
clustering method and the pre-training set. We evaluate this performance on
three diﬀerent classiﬁcation tasks: with ﬁne-tuning on Pascal VOC, without,
and by retraining the full MLP on ImageNet. We report the classiﬁcation accu-

24

Clustering algorithm Classiﬁcation Detection

Segmentation

fc6-8 all

fc6-8 all fc6-8 all

DeepCluster
DeepCluster

72.0
71.0
Table 2: Evaluation of PIC versus k-means on Pascal VOC transfer tasks.

51.4 55.4
53.6 54.4

k-means
PIC

73.7
73.0

43.2
42.4

45.1
43.8

ImageNet

Places

Method

Places labels
ImageNet labels
Random

conv1 conv2 conv3 conv4 conv5

conv1 conv2 conv3 conv4 conv5

–
19.3
11.6

–
36.3
17.1

–
44.2
16.9

–
48.3
16.3

–
50.5
14.1

22.1
22.7
15.7

35.1
34.8
20.3

40.2
38.4
19.8

43.3
39.4
19.1

44.6
38.7
17.5

Best competitor ImageNet

18.2 30.6

35.4

35.2

32.8

23.3 33.9 36.3

34.7

32.5

32.3 41.0 39.6 38.2
13.4
38.0
30.9
31.4
34.5
13.5
40.8 40.5 37.8
13.5
32.4
30.6
37.2
18.0 32.5 39.2

DeepCluster
DeepCluster YFCC100M
DeepCluster PIC
DeepCluster RGB
Table 3: Impact of diﬀerent variants of our method on the performance of Deep-
Cluster. We report classiﬁcation accuracy of linear classiﬁers on ImageNet and
Places using activations from the convolutional layers of an AlexNet as fea-
tures. We compare the standard version of DeepCluster to diﬀerent settings:
YFCC100M as pre-training set; PIC as clustering method; raw RGB inputs.

33.2 39.2 39.8 34.7
19.6
39.0
33.0
35.2
19.7
39.5 35.9
19.5
32.9
31.0
36.0
23.8 32.8

38.4
39.1
37.3

racy on the validation set. Overall, we notice that the regular version of Deep-
Cluster (with k-means) yields better results than the PIC variant on both Ima-
geNet and the uncured dataset YFCC100M.

Dataset

Clustering Pascal VOC ImageNet
fc6-8 all

fc6-8

k-means
PIC

ImageNet
ImageNet
YFCC100M k-means
YFCC100M PIC

72.0
71.0
67.3
66.0

73.7
73.0
69.3
69.0

44.0
45.9
39.6
38.6

Table 4: Performance of DeepCluster with diﬀerent pre-training sets and clus-
tering algorithms measured as classiﬁcation accuracy on Pascal VOC and Im-
ageNet.

Appendix

25

Fig. 4: Filter visualization and top 9 activated images (immediate right to the cor-
responding synthetic image) from a subset of 1 million images from YFCC100M
for target ﬁlters in the last convolutional layer of a VGG-16 trained with Deep-
Cluster.

3 Additional visualisation

3.1 Visualise VGG-16 features

We assess the quality of the representations learned by the VGG-16 convnet
with DeepCluster. To do so, we learn an input image that maximises the mean
activation of a target ﬁlter in the last convolutional layer. We also display the
top 9 images in a random subset of 1 million images from Flickr that activate
the target ﬁlter maximally. In Figure 4, we show some ﬁlters for which the top
9 images seem to be semantically or stylistically coherent. We observe that the
ﬁlters, learned without any supervision, capture quite complex structures. In
particular, in Figure 5, we display synthetic images that correspond to ﬁlters
that seem to focus on human characteristics.

3.2 AlexNet

It is interesting to investigate what clusters the unsupervised learning approach
actually learns. Fig. 2(a) in the paper suggests that our clusters are correlated
with ImageNet classes. In Figure 6, we look at the purity of the clusters with
the ImageNet ontology to see which concepts are learned. More precisely, we

26

Fig. 5: Filter visualization by learning an input image that maximizes the re-
sponse to a target ﬁlter [64] in the last convolutional layer of a VGG-16 convnet
trained with DeepCluster. Here, we manually select ﬁlters that seem to trigger
on human characteristics (eyes, noses, faces, ﬁngers, fringes, groups of people or
arms).

show the proportion of images belonging to a pure cluster for diﬀerent synsets
at diﬀerent depths in the ImageNet ontology (pure cluster = more than 70%
of images are from that synset). The correlation varies signiﬁcantly between
categories, with the highest correlations for animals and plants.

In Figure 7, we display the top 9 activated images from a random subset of
10 millions images from YFCC100M for the ﬁrst 100 target ﬁlters in the last
convolutional layer (we selected ﬁlters whose top activated images do not depict
humans). We observe that, even though our method is purely unsupervised, some
ﬁlters trigger on images containing particular classes of objects. Some other ﬁlters
respond strongly on particular stylistic eﬀects or textures.

4 Erratum [18/03/2019]

In the original version of the paper, we report classiﬁcation accuracy aver-
aged over 10 crops for the linear classiﬁer experiments on Places and ImageNet
datasets. However, other methods report accuracy of the central crop so our
comparison wasn’t fair. Nevertheless, it does not change the conclusion of these
experiments, our approach still outperforms the state of the art from conv3 to
conv5 layers. In Table 5, we show our results both for single and 10 crops.

Appendix

27

Fig. 6: Proportion of images belonging to a pure cluster for diﬀerent synsets in the
ImageNet ontology (pure cluster = more than 70% of images are from that synset).
We show synsets at depth 6, 9 and 13 in the ontology, with “zooms” on the animal
and mammal synsets.

ImageNet

Places

Method

conv1 conv2 conv3 conv4 conv5

conv1 conv2 conv3 conv4 conv5

DeepCluster single crop
DeepCluster 10 crops

12.9
13.4

29.2
32.3

38.2
41.0

39.8
39.6

36.1
38.2

18.6
19.6

30.8
33.2

37.0
39.2

37.5
39.8

33.1
34.7

Table 5: Linear classiﬁcation on ImageNet and Places using activations from the
convolutional layers of an AlexNet as features.

28

Filter 0

Filter 1

Filter 2

Filter 3

Filter 5

Filter 6

Filter 7

Filter 8

Filter 9

Filter 10

Filter 11

Filter 13

Filter 14

Filter 16

Filter 18

Filter 24

Filter 24

Filter 25

Filter 26

Filter 27

Filter 28

Filter 30

Filter 33

Filter 34

Filter 36

Filter 41

Filter 42

Filter 43

Appendix

29

Filter 45

Filter 52

Filter 55

Filter 56

Filter 57

Filter 58

Filter 59

Filter 60

Filter 62

Filter 63

Filter 65

Filter 66

Filter 67

Filter 68

Filter 69

Filter 70

Filter 74

Filter 75

Filter 76

Filter 78

30

Filter 79

Filter 81

Filter 83

Filter 84

Filter 85

Filter 88

Filter 90

Filter 91

Filter 92

Filter 93

Filter 96

Filter 97

Fig. 7: Top 9 activated images from a random subset of 10 millions images from
YFCC100M for the 100 ﬁrst ﬁlters in the last convolutional layer of an AlexNet
trained with DeepCluster (we do not display humans).

