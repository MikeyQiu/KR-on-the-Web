8
1
0
2
 
b
e
F
 
5
1
 
 
]

G
L
.
s
c
[
 
 
2
v
7
3
0
3
0
.
4
0
7
1
:
v
i
X
r
a

Learning from Multi-View Multi-Way Data via Structural
Factorization Machines
Lifang He∗
Cornell University
lifanghescut@gmail.com

Chun-Ta Lu
University of Illinois at Chicago
clu29@uic.edu

Hao Ding
Purdue University
haoding.tourist@gmail.com

Bokai Cao
University of Illinois at Chicago
caobokai@uic.edu

Philip S. Yu
University of Illinois at Chicago
Tsinghua University
psyu@cs.uic.edu

ABSTRACT
Real-world relations among entities can often be observed and de-
termined by diﬀerent perspectives/views. For example, the deci-
sion made by a user on whether to adopt an item relies on multi-
ple aspects such as the contextual information of the decision, the
item’s attributes, the user’s proﬁle and the reviews given by other
users. Diﬀerent views may exhibit multi-way interactions among
entities and provide complementary information. In this paper, we
introduce a multi-tensor-based approach that can preserve the un-
derlying structure of multi-view data in a generic predictive model.
Speciﬁcally, we propose structural factorization machines (SFMs)
that learn the common latent spaces shared by multi-view tensors
and automatically adjust the importance of each view in the predic-
tive model. Furthermore, the complexity of SFMs is linear in the
number of parameters, which make SFMs suitable to large-scale
problems. Extensive experiments on real-world datasets demon-
strate that the proposed SFMs outperform several state-of-the-art
methods in terms of prediction accuracy and computational cost.

CCS CONCEPTS
•Computing methodologies →Machine learning; Supervised
learning; Factorization methods;

KEYWORDS
Tensor Factorization; Multi-Way Interaction; Multi-View Learning

ACM Reference format:
Chun-Ta Lu, Lifang He, Hao Ding, Bokai Cao, and Philip S. Yu. 2018.
Learning from Multi-View Multi-Way Data via Structural Factorization Ma-
chines. In Proceedings of The 2018 Web Conference, Lyon, France, April 23–27,
2018 (WWW 2018), 10 pages.
DOI: https://doi.org/10.1145/3178876.3186071

∗Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
WWW 2018, Lyon, France
© 2018 ACM. 978-1-4503-5639-8.
DOI: https://doi.org/10.1145/3178876.3186071

1 INTRODUCTION
With the ability to access massive amounts of heterogeneous data
from multiple sources, multi-view data have become prevalent in
many real-world applications. For instance, in recommender sys-
tems, online review sites (like Amazon and Yelp) have access to
contextual information of shopping histories of users, the reviews
written by the users, the categorizations of the items, as well as the
friends of the users. Each view may exhibit pairwise interactions
(e.g., the friendships between users) or even higher-order interac-
tions (e.g., a customer write a review for a product) among entities
(such as customers, products, and reviews), and can be represented
in a multi-way data structure, i.e., tensor. Since diﬀerent views
usually provide complementary information [4, 6, 28], how to ef-
fectively incorporate information from multiple structural views is
critical to good prediction performance for various machine learn-
ing tasks.

Typically, a predictive model is deﬁned as a function of predic-
tor variables (e.g., the customer id, the product id, and the cate-
gories of the product) to some target (e.g., the rating). The most
common approach in predictive modeling for multi-view multi-
way data is to describe samples with feature vectors that are ﬂat-
tened and concatenated from structural views, and apply a vector-
based method, such as linear regression (LR) and support vector
machines (SVMs), to learn the target function from observed sam-
ples. Recent works have shown that linear models fail for tasks
with very sparse data [34]. A variety of methods have been pro-
posed to address the data sparsity issue by factorizing the mono-
mials (or feature interactions) with kernels, such as the ANOVA
kernels used in FMs [2, 34] and polynominal kernels used in poly-
nominal networks [3, 27]. However, the disadvantages of this ap-
proach are that (1) the important structural information of each
view will be discarded which may lead to the degraded prediction
performance and (2) the feature vectors can grow very large which
can make learning and prediction very slow or even infeasible, es-
pecially if each view involves relations of high cardinality. For
example, including the relation “friends of a user” in the feature
vector (represented by their IDs) can result in a very long feature
vector. Further, it will repeatedly appear in many samples that in-
volve the given user.

Matrix/tensor factorization models have been a topic of inter-
est in the areas of multi-way data analysis, e.g., community de-
tection [16], collaborative ﬁltering [23, 36], knowledge graph com-
pletion [43], and neuroimage analysis [15]. Assuming multi-view

WWW 2018, April 23–27, 2018, Lyon, France

C.-T. Lu et al.

data have the same underlying low-rank structure (at least in one
mode), coupled data analysis such as collective matrix factoriza-
tion (CMF) [38] and coupled matrix and tensor factorization (CMTF) [1]
that jointly factorize multiple matrices (or tensors) has been ap-
plied to applications such as clustering and missing data recov-
ery. However, they are only applicable to categorical variables.
Moreover, since existing coupled factorization models are unsu-
pervised, the importance of each structural view in modeling the
target value cannot be automatically learned. Furthermore, when
applying these models to data with rich meta information (e.g.,
friendships) but extremely sparse target values (e.g., ratings), it is
very likely the learning process will be dominated by the meta in-
formation without manual tuning some hyperparameters, e.g., the
weights of the ﬁtting error of each matrix/tensor in the objective
function [38], the weights of diﬀerent types of latent factors in the
predictive models [24], or the regularization hyperparamters of la-
tent factor alignment [29].

In this paper, we propose a general and ﬂexible framework for
learning the predictive structure from the complex relationships
within the multi-view multi-way data. Each view of an instance
in this framework is represented by a tensor that describes the
multi-way interactions of subsets of entities, and diﬀerent views
have some entities in common. Constructing the tensors for each
instance may not be realistic for real-world applications in terms
of space and computational complexity, and the model parameters
can have exponential growth and tend to be overﬁtting. In order
to preserve the structural information of multi-view data without
physically constructing the tensors, we introduce structural factor-
ization machines (SFMs) that can learn the consistent representa-
tions in the latent feature spaces shared in the multi-view tensors
while automatically adjust the contribution of each view in the
predictive model. Furthermore, we provide an eﬃcient method to
avoid redundant computing on repeating patterns stemming from
the relational structure of the data, such that SFMs can make the
same predictions but with largely speed up computation.

The contributions of this paper are summarized as follows:

• We introduce a novel multi-tensor framework for mining
data from heterogeneous domains, which can explore the
high order correlations underlying multi-view multi-way
data in a generic predictive model.

• We develop structural factorization machines (SFMs) tai-
lored for learning the common latent spaces shared in multi-
view tensors and automatically adjusting the importance
of each view in the predictive model. The complexity of
SFMs is linear in the number of features, which makes
SFMs suitable to large-scale problems.

• Extensive experiments on eight real-world datasets are per-
formed along with comparisons to existing state-of-the-
art factorization models to demonstrate its advantages.

The rest of this paper is organized as follows. In Section 2, we
brieﬂy review related work on factorization models and multi-view
learning. We introduce the preliminary concepts and problem def-
inition in Section 3. We then propose the framework for learning
multi-view multi-way data, and develop the structural factoriza-
tion machines (SFMs), and provide an eﬃcient computing method
in Section 4. The experimental results and parameter analysis are
reported in Section 5. Section 6 concludes this paper.

2 RELATED WORK
Feature Interactions. Rendle pioneered the concept of feature
interactions in Factorization Machines (FM) [34]. Juan et al. pre-
sented Field-aware Factorization Machines (FFM) [20] to allow each
feature to interact diﬀerently with another feature depending on
its ﬁeld. Novikov et al. proposed Exponential Machines (ExM)
[32] where the weight tensor is represented in a factorized format
called Tensor Train. Zhang et al. used FM to initialize the embed-
ding layer in a deep model [44]. Qu et al. added a product layer
on the top of the embedding layer to increase the model capacity
[33]. Other extensions of FM to deep architectures include Neural
Factorization Machines (NFM) [17] and Attentional Factorization
Machines (AFM) [40]. In order to eﬀectively model feature inter-
actions, a variety of models has been developed in the industry as
well. Microsoft studied feature interactions in deep models, includ-
ing Deep Semantic Similarity Model (DSSM) [19], Deep Crossing
[37] and Deep Embedding Forest [47]. They use features as raw
as possible without manually crafted combinatorial features, and
let deep neural networks take care of the rest. Alibaba proposed
a Deep Interest Network (DIN) [46] to learn user embeddings as a
function of ad embeddings. Google used deep neural networks to
learn from heterogeneous signals for YouTube recommendations
[9]. In addition, Wide & Deep Models [7] were developed for app
recommender systems in Google Play where the wide component
includes cross features that are good at memorization and the deep
component includes embedding layers for generalization. Guo et
al. proposed to use FM as the wide component in Wide & Deep
with shared embeddings in the deep component [11]. Wang et al.
developed the Deep & Cross Network (DCN) to learn explicit cross
features of bounded degree [39].

Multi-View Learning. Multi-view learning (MVL) is concerned
with predicting unknown values by taking multiple views into ac-
count. The traditional MVL refers to using relational features to
construct a set of disjoint views, and these uncorrelated views are
then used to model a target function to approximate the target con-
cept to be learned [12]. There are currently a plethora of studies
available for MVL. Interested readers are referred to [41] for a com-
prehensive survey of these techniques and applications. The most
related works to ours are [5, 6, 25] that introduced and explored the
tensor product operator to integrate diﬀerent views together in a
tensor. Lu et al. further studied the multi-view feature interactions
in the context of multi-task learning [28]. However, this approach
will introduce unexpected noise from the irrelevant feature interac-
tions that can even be exaggerated after combinations, thereby de-
grading performance as demonstrated in the experiments. Diﬀer-
ent from conventional MVL approaches, the proposed algorithm
can learn the common latent spaces shared in multi-view tensors
and automatically adjusting the importance of each view in the
predictive model.

3 PRELIMINARIES
In this section, we begin with a brief introduction to some related
concepts and notation in tensor algebra, and then proceed to for-
mulate the problem we are concerned with multi-view learning.

Structural Factorization Machines

WWW 2018, April 23–27, 2018, Lyon, France

Mode 1

Mode 3

Mode 2

View 1

Mode 4

View 2

1
˜x(1) = [1, x(1)]

˜x(2) = [1, x(2)]

1
˜x(3) = [1, x(3)]

1

1

˜x(4) = [1, x(4)]

View Bias

First-order 
Interactions

Second-order 
interactions

˜X (1)

˜X(2)

Third-order 
interaction

Figure 1: Example of multiple structural views, where ˜X(1) = ˜x(1) ◦ ˜x(2) ◦ ˜x(3) and ˜X(2) = ˜x(3) ◦ ˜x(4).

3.1 Tensor Basics and Notation
Tensor is a mathematical representation of a multi-way array. The
order of a tensor is the number of modes (or ways). A zero-order
tensor is a scalar, a ﬁrst-order tensor is a vector, a second-order
tensor is a matrix and a tensor of order three or higher is called a
higher-order tensor. An element of a vector x, a matrix X, or a ten-
sor X is denoted by xi , xi, j , xi, j,k , etc., depending on the number of
modes. All vectors are column vectors unless otherwise speciﬁed.
For an arbitrary matrix X ∈ RI ×J , its i-th row and j-th column
vector are denoted by xi and xj , respectively. Given two matrices
X, Y ∈ RI ×J , X ∗ Y denotes the element-wise (Hadamard) product
between X and Y, deﬁned as the matrix in RI ×J . An overview of
the basic symbols used in this paper can be found in Table 1.

Deﬁnition 3.1 (Inner product). The inner product of two same-
sized tensors X, Y ∈ RI1×I2×···×IM is deﬁned as the sum of the
products of their entries:

hX, Yi =

· · ·

xi1,i2, ...,iM yi1,i2, ...,iM .

(1)

I1

I2

IM

Õi1=1

Õi2=1

ÕiM =1

Deﬁnition 3.2 (Outer product). The outer product of two tensors
1×I ′
M is a (N + M)th-order

X ∈ RI1×I2×···×IN and Y ∈ RI ′
tensor denoted by X ◦ Y, and the elements are deﬁned by

2×···×I ′

(X ◦ Y)i1,i2, ...,i N ,i ′

1,i ′
for all values of the indices.

2, ...,i ′
M

= xi1,i2, ··· ,i N yi ′

1,i ′

2, ··· ,i ′
M

(2)

Notice that for rank-one tensors X = x(1) ◦ x(2) ◦ · · · ◦ x(M) and

Y = y(1) ◦ y(2) ◦ · · · ◦ y(M), it holds that

hX, Yi =

x(1), y(1)

x(2), y(2)

· · ·

x(M), y(M)

.

(3)

Deﬁnition 3.3 (CP factorization [22]). Given a tensor X ∈ RI1×I2×···×IM

D

E D

E

D

E

and an integer R, the CP factorization is deﬁned by factor matrices

Table 1: List of basic symbols.

Symbol Deﬁnition and description

x
x
X
X
X
[1 : N ]
h·, ·i
◦
∗

each lowercase letter represents a scalar
each boldface lowercase letter represents a vector
each boldface uppercase letter represents a matrix
each calligraphic letter represents a tensor
each gothic letter represent a general set or space
a set of integers in the range of 1 to N inclusively.
denotes inner product
denotes tensor product (outer product)
denotes Hadamard (element-wise) product

X(m) ∈ RIm ×R for m ∈ [1 : M], respectively, such that

R

X =

r ◦ x(2)
x(1)

r ◦ · · · ◦ x(M)

r

= JX(1), X(2), · · · , X(M)K ,

(4)

Õr =1
where x(m)
∈ RIm is the r -th column of the factor matrix X(m), and
r
J·K is used for shorthand notation of the sum of rank-one tensors.

3.2 Problem Formulation
Our problem is diﬀerent from conventional multi-view learning
approaches where multiple views of data are assumed independent
and disjoint, and each view is described by a vector. We formulate
the multi-view learning problem using coupled analysis of multi-
view features in the form of multiple tensors.

Suppose that the problem includes V views where each view
consists of a collection of subsets of entities (such as person, com-
pany, location, product) and diﬀerent views have some entities in
common. We denote a view as a tuple (x(1), x(2), · · · , x(M)), M ≥ 2,
where x(m) ∈ RIm is a feature vector associated with the entity m.
Inspired by [6], we construct tensor representation for each view

WWW 2018, April 23–27, 2018, Lyon, France

C.-T. Lu et al.

over its entities by

˜X = ˜x(1) ◦ ˜x(2) ◦ · · · ◦ ˜x(M) ∈ R(1+I1)×···×(1+IM ),
where ˜x(m) = [1; x(m)] ∈ R1+Im and ◦ is the outer product opera-
tor. In this manner, the full-order interactions 1 between entities
are embedded within the tensor structure, which not only provides
a uniﬁed and compact representation for each view, but also fa-
cilitate eﬃcient design methods. Fig. 1 shows an example of two
structural views, where the ﬁrst view consists of the full-order in-
teractions among the ﬁrst three modes (e.g., review text, item ID,
and user ID), and the second view consists of the full-order inter-
actions among the last two modes (e.g., user ID and friend IDs).

(cid:1)

n

(cid:9)

(cid:8)(cid:0)(cid:8)

, yn

| n ∈ [1 : N ]

n , ˜X(2)
˜X(1)

n , · · · , ˜X(V )

After generating the tensor representation for each view, we
deﬁne the multi-view learning problem as follows. Given a train-
ing set D =
, where
∈ R(1+I1)×···×(1+IMv ) is the tensor representation in the v-th
˜X(v)
(cid:9)
n
view for the n-th instance, yn is the response of the n-th instance,
Mv is the number of the constitutive modes in the v-th view, and
N is the number of labeled instances. We assume diﬀerent views
have common entities, thus the resulting tensors will share com-
mon modes, e.g., the third mode in Fig 1. As we are concerned
with predicting unknown values of multiple coupled tensors, our
goal is to leverage the relational information from all the views to
help predict the unlabeled instances, as well as to use the comple-
mentary information among diﬀerent views to improve the perfor-
mance. Speciﬁcally, we are interested in ﬁnding a predictive func-
tion f : X(1) × X(2) · · · × X(V ) → Y that minimizes the expected
loss, where X(v), v ∈ [1 : V ] is the input space in the v-th view and
Y is the output space.

4 METHODOLOGY
In this section, we ﬁrst discuss how to design the predictive models
for learning from multiple coupled tensors. We then derive struc-
tural factorization machines (SFMs) that can learn the common la-
tent spaces shared in multi-view coupled tensors and automatically
adjust the importance of each view in the predictive model.

4.1 Predictive Models
Without loss of generality, we take two views as an example to
introduce our basic design of the predictive models. Speciﬁcally,
we consider coupled analysis of a third-order tensor and a ma-
trix with one mode in common, as shown in Fig. 1. Given an in-
, where ˜X(1) = ˜x(1) ◦ ˜x(2) ◦ ˜x(3) ∈
put instance
R(1+I )×(1+J )×(1+K ) and ˜X(2) = ˜x(3) ◦ ˜x(4) ∈ R(1+K )×(1+L). An intu-
(cid:1)
itive solution is to build the following multiple linear model:

˜X(1), ˜X(2)

, y

(cid:0)(cid:8)

(cid:9)

f

˜X(1), ˜X(2)

=

˜W(1), ˜X(1)

+

˜W(2), ˜X(2)

(5)

(cid:16)n

o(cid:17)

D

where ˜W(1) ∈ R(1+I )×(1+J )×(1+K ) and ˜W(2) ∈ R(1+K )×(1+L) are the
weights for each view to be learned.

E

D

E

However, in this case it does not take into account the relations
and diﬀerences between two views.
In order to incorporate the
relations between two views and also discriminate the importance

1Full-order interactions range from the ﬁrst-order interactions (i.e., contributions of
single entity features) to the highest-order interactions (i.e., contributions of the outer
product of features from all entities).

of each view, we introduce an indicator vector ev ∈ RV for each
view v as

ev = [0, · · · , 0
   
   

, 1, 0, · · · , 0]T,

v-1

and transform the predictive model in Eq. (5) into
|

{z

}

f

˜X(1), ˜X(2)

=

ˆW(1), ˜X(1) ◦ e1

+

ˆW(2), ˜X(2) ◦ e2

,

(6)

E

E

D

(cid:16)n

o(cid:17)

D

where ˆW(1) ∈ R(1+I )×(1+J )×(1+K )×2 and ˆW(2) ∈ R(1+K )×(1+L)×2.

Directly learning the weight tensors

ˆWs leads to two draw-
backs. First, the weight parameters are learned independently for
diﬀerent modes and diﬀerent views. When the feature interactions
rarely (or even never) appear during training, it is unlikely to learn
the associated parameters appropriately. Second, the number of pa-
rameters in Eq. (6) is exponential to the number of features, which
can make the model prone to overﬁtting and ineﬀective on sparse
data. Here, we assume that each weight tensor has a low-rank
approximation, and ˆW(1) and ˆW(2) can be decomposed by CP fac-
torization as

ˆW(1) = J ˆΘ(1, 1), ˆΘ(1, 2), ˆΘ(1, 3), ΦK

= J[b(1, 1); Θ(1)], [b(1, 2); Θ(2)], [b(1, 3); Θ(3)], ΦK,

and

ˆW(2) = J ˆΘ(2, 3), ˆΘ(2, 4), ΦK = J[b(2, 3); Θ(3)], [b(2, 4); Θ(4)], ΦK,

where Θ(m) ∈ RIm ×R is the factor matrix for the features in the
m-th mode. It is worth noting that Θ(3) is shared in the two views.
Φ ∈ R2×R is the factor matrix for the view indicator, and b(v,m) ∈
R1×R , which is always associated with the constant one in ˜x(m) =
[1; x(m)], represents the bias factors of the m-th mode in the v-th
view. Through b(v,m), the lower-order interactions (the interac-
tions excluding the features from the m-th mode) in the v-th view
are explored in the predictive function.
Then we can transform Eq. (6) into

ˆW(1), ˜X(1) ◦ e1

+

ˆW(2), ˜X(2) ◦ e2

D
R

=

E
◦ ˆθ (1, 2)
r

D
◦ ˆθ (1, 3)
r

ˆθ (1, 1)
r

E

◦ ϕr , ˜x(1) ◦ ˜x(2) ◦ ˜x(3) ◦ e1

Õr =1 D
R
+

Õr =1 D
3

 
Öm=1
3

(cid:16)

ˆθ (2, 3)
r

◦ ˆθ (2, 4)
r

◦ ϕr , ˜x(3) ◦ ˜x(4) ◦ e2

=ϕ1

˜x(m)T ˆΘ(1,m)

∗

+ ϕ2

T

!

(cid:17)

E
˜x(m)T ˆΘ(2,m)

4

∗

 
Öm=3
T

(cid:16)

4

=ϕ1

∗

x(m)T Θ(m) + b(1,m)

+ ϕ2

∗

x(m)T Θ(m) + b(2,m)

(cid:16)

 
Öm=1

!
(cid:17)
(7)
where ∗ is the Hadamard (elementwise) product and ϕv ∈ R1×R

 
Öm=3

!

(cid:17)

(cid:16)

T

is the v-th row of the factor matrix Φ.

For convenience, we let h(m) = Θ(m)T

set of modes in the v-th views, π (v) =

and π (v, −m) =

m′ ∈SM (v),m′,m
Î

∗

(cid:16)

x(m), SM (v) denote the
h(m) + b(v,m)T
,

∗

m ∈SM (v)
(cid:16)
Î
h(m′) + b(v,m′)T

(cid:17)
. The predictive

E

T

!

(cid:17)

(cid:17)

Structural Factorization Machines

WWW 2018, April 23–27, 2018, Lyon, France

1

1

1

1

1

Θ(1)

Θ(2)

Θ(3)

Θ(4)

b(1,1)
h(1)

b(1,2)
h(2)

b(1,3)

h(3)
b(2,3)

b(2,4)
h(4)

x(1)

x(2)

x(3)

x(4)

e1

1

Φ

π(1)

φ1

f ( ˜X (1))

*

*

π(2)

φ2

f ( ˜X (1), ˜X (2))

f ( ˜X (2))

e2

1

Φ

Figure 2: Example of the computational graph in a structural factorization machine, given the input ˜X(1) and ˜X(2). By jointly
factorizing weight tensors, the h(m) can be regarded as the latent representation of the feature x(m) in m-th mode, and π (v) can
be regarded as the joint representation of all the modes in the v-th view, which can be easily computed through the Hadamard
product. The contribution of π (v) to the ﬁnal prediction score is automatically adjusted by the weight vector ϕv .

model for the general cases is given as follows

f ({ ˜X(v)}) =

ˆW(v), ˜X(v) ◦ ev

V

Õv=1 D
V
ϕv

=

=

Õv=1
V

Õv=1

E

∗

x(m)T Θ(m) + b(v,m)

(8)

ϕv

∗

h(m) + b(v,m)T

Öm ∈SM (v)

Öm ∈SM (v)

(cid:16)

(cid:16)

T

(cid:17)

(cid:17)

A graphical illustration of the proposed model is shown in Fig. 2.
We name this model as structural factorization machines (SFMs).
Clearly, the parameters are jointly factorized, which beneﬁts pa-
rameter estimation under sparsity since dependencies exist when
the interactions share the same features. Therefore, the model pa-
rameters can be eﬀectively learned without direct observations of
such interactions especially in highly sparse data. More impor-
ˆWs, there is no need
tantly, after factorizing the weight tensor
to construct the input tensor physically. Furthermore, the model
complexity is linear in the number of original features. In partic-
ular, the model complexity is O(R(V + I +
v Mv )), where Mv is
the number of modes in the v-th view.

Í

4.2 Learning Structural Factorization Machines
Following the traditional supervised learning framework, we pro-
pose to learn the model parameters by minimizing the following
regularized empirical risk:

R = 1
N

N

Õn=1

ℓ

f ({X(v)

n }), yn

+ λΩ(Φ, {Θ(m)}, {b(v,m)})

(9)

(cid:16)

(cid:17)
where ℓ is a prescribed loss function, Ω is the regularizer encoding
the prior knowledge of {Θ(m)} and Φ, and λ ≥ 0 is the regulariza-
tion parameter that controls the trade-oﬀ between the empirical
loss and the prior knowledge.

The partial derivative of R w.r.t. Θ(m) is given by
λ(Θ(m))
∂Θ(m)

∂ f
∂Θ(m)

∂R
∂Θ(m)

∂L
∂ f

+ λ

∂Ω

=

where ∂ L
∂f

= 1
N

∂ℓ1
∂f

, · · · , ∂ℓN
∂f

T

∈ RN .

For convenience, we let SV (m) denote the set of views that con-
i
, · · · , x(m)

h
tains them-th mode, X(m) = [x(m)
and Π(v, −m) = [π (v, −m)

N ], Π(v) = [π (v)
]T. We then have that

1
, · · · , π (v, −m)
N

, · · · , π (v)

1

1

N ]T

ϕv

∗ Π(v, −m)

(11)

∂L
∂ f

∂ f
∂Θ(m)

= X(m)

∂L
∂ f

(cid:19)

λ(b(v,m))
∂b(v,m)

Õv ∈SV (m) (cid:18)(cid:18)
©
 
«
∂ f
∂b(v,m)
∂L
∂ f

+ λ

ϕv

Similarly, the partial derivative of R w.r.t. b(v,m) is given by
∂Ω

∂R
∂b(v,m)

=

∂L
∂ f

= 1T

∗ Π(v, −m)

+ λ

(cid:18)(cid:18)

(cid:19)

(cid:19)

∂Ω

λ(b(v,m))
∂b(v,m)

The partial derivative of R w.r.t. Φ is given by

(cid:19)

ª
®
¬

∂R
∂Φ

=

∂ L
∂f

T

(cid:17)

(cid:20) (cid:16)

Π(1) ; · · · ;

T

Π(V )

∂ L
∂f

(cid:16)

(cid:17)

+ λ

∂Ω

λ(Φ)
∂Φ

(cid:21)

Finally, the gradient of R can be formed by vectorizing the par-
tial derivatives with respect to each factor matrix and concatenat-
ing them all, i.e.,

(10)

(12)

(13)

(14)

∇R =

vec( ∂R
∂Θ(1) )
...
vec( ∂R
∂Θ(M ) )
vec( ∂R
∂b(1, 1) )
...
∂R
∂b(V , M ) )
vec( ∂R
∂Φ )

vec(









































WWW 2018, April 23–27, 2018, Lyon, France

C.-T. Lu et al.

X

UserID

N

11

01

0

0

0

0

1

0

0

1

0

0

0

0

0

1

0

0

ItemID

Review 
Text

0

.4

0

.1

0

0

.2

.3

.2

.2

.2

.3

.3

.3

0

.5

Friends

.3

.3

.3

.5

.5

0

0

0

.5

.5

.3

.3

.3

0

0

.5

1

0

1

0

0

0

0

10

0

1

0

0

1

0

0

0

1

0

0

0

N

2

1

4

2

1

3

3

1

1

2

2

1

2

3

3

2

31

2

3

1

1

1

1

ψB(1)
ψB(2)
ψB(3)
ψB(4)
N1

01

0

1

0 0

0

0

1

N2

001

0 1

0

0

10

I1

I2

N3

.3

0

.6

.3

0

.4

0

.1

.2

.3

.2

0

XB(3)

N4

.3

0 .5

.3

.5

0

.5

0

0

.3

0

.5

I3

I4

XB(2)

XB(4)

.3

.6

.3

.3

.6

I

XB(1)

(a) Plain Format of 
Feature Matrix

(b) Relational Structure Representation

Figure 3: (a) Feature vectors of the same entity repeatedly ap-
pear in the plain formatted feature matrix X. (b) Repeating
patterns in X can be formalized by the relational structure B
of each mode. For example, the forth column of the feature
matrix X can be represented as x4 = [x(1)
]
= [xB(1)
; xB(4)
2
2

ψ (4); x(2)

ψ (4); x(4)

ψ (4); x(3)

; xB(2)
1

; xB(3)
4

ψ (4)

].

Once we have the function, R and gradient, ∇R, we can use any
gradient-based optimization algorithm to compute the factor ma-
trices. For the results presented in this paper, we use the Adaptive
Moment Estimation (Adam) optimization algorithm [21] for pa-
rameter updates. Adam is an adaptive version of gradient descent
that controls individual adaptive learning rates for diﬀerent param-
eters from estimates of ﬁrst and second moments of the gradient.
It combines the best properties of the AdaGrad [10], which works
well with sparse gradients, and RMSProp [18], which works well
in on-line and non-stationary settings. Readers can refer to [21]
for details of the Adam optimization algorithm.

4.3 Eﬃcient Computing with Relational

Structures

In relational domains, we can often observe that feature vectors of
the same entity repeatedly appear in the plain formatted feature
matrix X, where X = [X(1); · · · ; X(M)] ∈ RI ×N and X(m) ∈ RIm ×N
is the feature matrix in the m-th mode. Consider Fig. 3(a) as an
example, where the parts highlighted in yellow in the forth mode
(which represents the friends of the user) are repeatedly appear
in the ﬁrst three columns. Clearly, these repeating patterns stem
from the relational structure of the same entity.

In the following, we show how the proposed SFM method can
make use of relational structure of each mode, such that the learn-
ing and prediction can be scaled to predictor variables generated
from relational data involving relations of high cardinality. We
adopt the idea from [35] to avoid redundant computing on repeat-
ing patterns over a set of feature vectors.

Let B = {(XB(m) ,ψ B(m)

)}M

where XB(m)

m=1 be the set of relational structures,
∈ RIm ×Nm denotes the relational matrix of m-th

Category

Word

Item

X1

User

X2

Link

Item

User

X1
X2

Venue

Friend

City
X3

Category
X4

User

Country

X3
Age

X1
X2

Book

User

Author

(a) Amazon

(b) Yelp

(c) BookCrossing

Figure 4: Schema of the structural views in each dataset.

mode, ψ B(m)
: {1, · · · , N } → {1, · · · , Nm } denotes the mapping
from columns in the feature matrix X to columns within XB(m)
.
To shorten notation, the index B is dropped from the mapping ψ B
whenever it is clear which block the mapping belongs to. From B,
one can reconstruct X by concatenating the corresponding columns
of the relational matrices using the mappings. For instance, the fea-
ture vector xn of the n-th case in the plain feature matrix X is repre-
sented as xn = [x(1)
]. Fig. 3(b) shows an example how
the feature matrix can be represented in relational structures. Let
Nz (A) denote the number of non-zeros in a matrix A. The space
required for using relational structures to represent the input data
is |B| = N M +
), which is much smaller than Nz (X)
if there are repeating patterns in the feature matrix X.

ψ (n); · · · ; x(M)

ψ (n)

m Nz (XB(m)
Í

Now we can rewrite the predictive model in Eq. (8) as follows

f ({X(v)

n } =

ϕv

V

Õv=1

∗

hB(m)
ψ (n)

+ b(v,m)T

,

(15)

Öm ∈SM (v)

(cid:16)

(cid:17)

with the caches HB(m) = [hB(m)
hB(m)
xB(m)
j
j

1
, ∀j ∈ [1 : Nm].

= Θ(m)T

, · · · , hB(m)
Nm

] for each mode, where

This directly shows how N samples can be eﬃciently predicted:
(i) compute HB(m)
in O(RNz (XB(m)
)) for each mode, (ii) compute
N predictions with Eq. (15) using caches in O(RN (V +
v Mv )).
With the help of relational structures, SFMs can learn the same
parameters and make the same predictions but with a much lower
runtime complexity.

Í

5 EXPERIMENTS
5.1 Datasets
To evaluate the ability and applicability of the proposed SFMs, we
include a spectrum of large datasets from diﬀerent domains. The
statistics for each dataset is summarized in Table 2, the schema of
the structural views in each dataset is presented in Fig. 4, and the
details are as follows:

Amazon2: The ﬁrst group of datasets are from Amazon.com re-
cently introduced by [31]. This is among the largest datasets avail-
able that include review texts and metadata of items. Each top-
level category of products on Amazon.com has been constructed
as an independent dataset in [31]. In this paper, we take a variety
of large categories as listed in Tabel 2.

Each sample in these datasets has ﬁve modes, i.e., users, items,
review texts, categories, and linkage. The user mode and item

2http://jmcauley.ucsd.edu/data/amazon/

Structural Factorization Machines

WWW 2018, April 23–27, 2018, Lyon, France

Table 2: The statistics for each dataset. Nz (X ) and Nz (B) are the number of non-zeros in plain formatted feature matrix and
in relational structures, respectively. Game: Video Games, Cloth: Clothing, Shoes and Jewelry, Sport: Sports and Outdoors,
Health: Health and Personal Care, Home: Home and Kitchen, Elec: Electronics.

Dataset
Amazon
Game
Cloth
Sport
Health
Home
Elec

#Samples

231,780
278,677
296,337
346,355
551,682
1,689,188

Yelp

1,319,870

BX

244,848

#Users
24,303
39,387
35,598
38,609
66,569
192,403
#Users
88,009
#Users
24,325

#Items
10,672
23,033
18,357
18,534
28,237
63,001
#Venues
40,520
#Books
45,074

Mode

#Words
7,500
3,493
5,202
5,889
6,455
12,805
#Friends
88,009
#Countries
57

#Categories
193
1,175
1,432
849
970
967
#Categories
892
#Ages
8

#Links
17,974
107,139
73,040
80,379
99,090
89,259
#Cities
412
#Authors
17,178

Density Nz (X ) Nz (B)

0.089%
0.031%
0.045%
0.048%
0.029%
0.014%

32.9M 15.2M
25.6M 7.3M
34.2M 10.2M
33.6M 12.1M
46.8M 19.4M
161.5M 69M

0.037%

70.5M 1.4M

0.022%

1.2M

163K

mode are represented by one-hot encoding. The ℓ2-normalized TF-
IDF vector representation of review text 3 of the item given by the
user is used as the text mode. The category mode and linkage mode
consists of all the categories and all the co-purchasing items of the
item, which might be from other categories. The last two modes
are ℓ1-normalized.

Yelp4: It is a large-scale dataset consisting of venue reviews.
Each sample in this dataset contains ﬁve modes, i.e., users, venues,
friends, categories and cities. The user mode and venue mode are
represented by one-hot encoding. The friend mode consists of the
friends’ ids of users. The category mode and city mode consists of
all the categories and the city of the venue. The last three modes
are ℓ1-normalized.

BookCrossing (BX)5: It is a book review dataset collected from
the Book-Crossing community. Each sample in this dataset con-
tains ﬁve modes, i.e., users, books, countries, ages and authors.
The ages are split in eight bins as in [13]. The country mode and
age mode consist of the corresponding meta information of the
user. The author modes represents the authors of the book. All the
modes are represented by one-hot encoding.

The values of samples range within [1:5] in Amazon and Yelp

datasets, and range within [1:10] in BX dataset.

5.2 Comparison Methods
In order to demonstrate the eﬀectiveness of the proposed SFMs, we
compare a series of state-of-the-art methods.
Matrix Factorization (MF) is used to validate that meta informa-
tion is helpful for improving prediction performance. We use the
LIBMF implementation [8] for comparison in the experiment.
Factorization Machine (FM) [34] is the state-of-the-art method
in recommender systems. We compare with its higher-order ex-
tension [2] with up to second-order, and third-order feature inter-
actions, and denote them as FM-2 and FM-3.
Polynomial Network (PolyNet) [27] is a recently proposed method
that utilizes polynomial kernel on all features. We compare the

3Stemming, lemmatization, removing stop-words and words with frequency less than
100 times, etc., are handled beforehand.
4https://www.yelp.com/dataset-challenge
5http://www2.informatik.uni-freiburg.de/∼cziegler/BX/

augmented PolyNet (which adds a constant one to the feature vec-
tor [3]) with up to the second-order, and third-order kernel and
denote them as PolyNet-2 and PolyNet-3.
Multi-View Machine (MVM) [6] is a tensor factorization based
method that explores the latent representation embedded in the
full-order interactions among all the modes.
Structural Factorization Machine (SFM) is the proposed model
that learns the common latent spaces shared in multi-way data.

5.3 Experimental Settings
For each dataset, we randomly split 50%, 10%, and 40% of labeled
samples as training set, validation set, and testing set, respectively.
Validation sets are used for hyper-parameter tuning for each model.
Each of the validation and testing sets does not overlap with any
other set so as to ensure the sanity of the experiment. For sim-
plicity and fair comparison, in all the comparison methods, the
dimension of latent factors R = 20 and the maximum number
of epochs is set as 400 and we use early stop to obtain the best
results for each method. Forbenius norm regularizers are used
to avoid overﬁtting. The regularization hyper-parameter is tuned
from {10−5, 10−4, · · · , 100}.

All the methods except MF are implemented in TensorFlow, and
the parameters are initialized using scaling variance initializer [14].
We tune the scaling factor of initializer σ from {1, 2, 5, 10, 100} and
the learning rate η from {0.01, 0.1, 1} using the validation sets. In
the experiment, we set σ = 2 (default setting in TensorFlow) and
η = 0.01 for these methods except MVM. We found that MVM is
more sensitive to the conﬁguration, because MVM will element-
wisely multiply the latent factors of all the modes which leads to
an extremely small value approaching zero. σ = 10 and η = 0.1
yielded the best performance for MVM.

To investigate the performance of comparison methods, we adopt
mean squared error (MSE) on the test data as the evaluation met-
rics [30, 45]. The smaller value of the metric indicates the better
performance. Each experiment was repeated for 10 times, and the
mean and standard deviation of each metric in each data set were
reported. All experiments are conducted on a single machine with
Intel Xeon 6-Core CPUs of 2.4 GHz and equipped with a Maxwell
Titan X GPU.

WWW 2018, April 23–27, 2018, Lyon, France

C.-T. Lu et al.

Table 3: MSE comparison on all the datasets. The best results are listed in bold.

(b)
MVM
0.753 ± 0.007
0.725 ± 0.046
0.646 ± 0.019
0.807 ± 0.012
0.729 ± 0.067
0.792 ± 0.042
1.2575 ± 0.013
2.844 ± 0.024

(c)
FM-2
0.764 ± 0.006
0.678 ± 0.004
0.638 ± 0.003
0.779 ± 0.004
0.714 ± 0.002
0.776 ± 0.006
1.277 ± 0.002
2.766 ± 0.012

(d)
FM-3
0.749 ± 0.007
0.679 ± 0.004
0.632 ± 0.007
0.778 ± 0.004
0.714 ± 0.004
0.749 ± 0.007
1.277 ± 0.002
2.767 ± 0.014

(e)
PolyNet-2
0.749 ± 0.004
0.678 ± 0.007
0.631 ± 0.005
0.779 ± 0.005
0.690 ± 0.003
0.760 ± 0.004
1.272 ± 0.002
2.654 ± 0.013

(f)
PolyNet-3
0.748 ± 0.006
0.680 ± 0.005
0.632 ± 0.005
0.776 ± 0.005
0.692 ± 0.005
0.757 ± 0.001
1.272 ± 0.002
2.658 ± 0.013

(g)
SFM
0.723 ± 0.006
0.659 ± 0.013
0.614 ± 0.011
0.763 ± 0.019
0.678 ± 0.008
0.747 ± 0.006
1.256 ± 0.010
2.541 ± 0.025

Improvement of SFM verus
min(c,d) min(e,f)
3.35%
2.84%
2.79%
1.77%
1.72%
1.33%
1.19%
4.27%
2.41%

b
4.06%
9.03%
5.00%
5.47%
6.93%
5.69%
0.09%
10.66%
5.87%

3.52%
2.82%
2.91%
2.02%
5.00%
0.27%
1.58%
8.16%
3.29%

 

Dataset

(a)
MF
1.569 ± 0.005
Game
1.624 ± 0.009
Cloth
1.290 ± 0.004
Sport
1.568 ± 0.007
Health
1.591 ± 0.004
Home
1.756 ± 0.002
Elec
1.713 ± 0.003
Yelp
4.094 ± 0.025
BX
Average on all datasets

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

h
c
o
p
E

 
/
 
s
d
n
o
c
e
S

25

20

15

10

5

0

 

the latent factors of a feature are essentially learned from its in-
teractions with other features observed in the data, as can be ob-
served from its update rule. In FM and PolyNet, all the feature in-
teractions are taken into consideration without distinguishing the
features from diﬀerent modes. As a result, important feature in-
teractions (e.g., the interactions between the given user and her
friend) would be easily buried in irrelevant feature interactions
from the same modes (e.g., the interactions between the friends
of the same user). Hence, the learned latent factors are less rep-
resentative in FM and PolyNet, compared with the proposed SFM.
Besides, we can ﬁnd that including higher-order interactions in FM
and PolyNet (i.e., FM-3 and PolyNet-3) does not always improve
the performance. Instead, it may even degrade the performance,
as shown in Cloth, Yelp, and BX datasets. This is probably due
to overﬁtting, as they need to include more parameters to model
the interactions in higher orders while the datasets are extremely
sparse such that the parameters cannot be properly learned.

Compared to the MVM method, which models the full-order in-
teractions among all the modes, our proposed SFM leads to an av-
erage improvement of 5.87%. This is because not all the modes
are relevant, and some irrelevant feature interactions may intro-
duce unexpected noise to the learning task. The irrelevant infor-
mation can even be exaggerated after combinations, thereby de-
grading performance. This suggests that preserving the nature of
relational structure is important in building predictive models.

5.5 Computational Cost Analysis
Next, we investigate the computational cost for comparison meth-
ods. The averaged training time (seconds per epoch) required for
each dataset is shown in Fig. 5. We can easily ﬁnd that the proposed
SFM requires much less computational cost on all the datasets, es-
pecially for the Yelp dataset (roughly 11% of computational cost
required for training FM-3). The eﬃciency comes from the use
of relational structure representation. As shown in Table 2, the
number of non-zeros of the feature matrix Nz (X) is much larger
than the number of non-zeros of the relational structure represen-
tation Nz (B). The amount of repeating patterns is much higher
for the Yelp dataset than for the other dataset, because adding all
the friends of a user signiﬁcantly increases results in large repeat-
ing blocks in the plain feature matrix. Standard ML algorithms like
the compared methods have typically at best a linear complexity in
Nz (X), while using the relational structure representation for SFM

Game Cloth Sport Health Home Elec

Yelp

BX

Figure 5: Training Time (Seconds/Epoch) Comparison.

5.4 Performance Analysis
The experimental results are shown in Table 3. The best method
of each dataset is in bold. For clarity, on the right of the tables we
show the percentage improvement of the proposed SFM method
over a variety of methods. From these results, we can observe that
SFM consistently outperforms all the comparison methods. We
also make a few comparisons and summarize our ﬁndings as fol-
lows.

Compared with MF, SFM performs better with an average im-
provement of nearly 50%. MF usually performs well in practice [26,
34], while in datasets which are extremely sparse, as is shown
in our case, MF is unable to learn an accurate representation of
users/items. Thus MF under-performs other methods which takes
the meta information into consideration.

In both FM and PolyNet methods, the feature vectors from all
the modes are concatenated as a single input feature vector. The
major diﬀerence between these two methods is the choice of kernel
applied [2]. The polynomial kernel used in PolyNet considers all
monomials (the products of features), i.e., all combinations of fea-
tures with replacement. The ANOVA kernel used in FM considers
only monomials composed of distinct features, i.e., feature combi-
nations without replacement. Compared with the best results ob-
tained from FM methods and from PolyNet methods, SFM leads to
an average improvement of 3.3% and 2.4% in MSE, respectively.

The primary reason behind the results is how the latent factors
of each feature are learned. For any factorization based method,

Structural Factorization Machines

WWW 2018, April 23–27, 2018, Lyon, France

 

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

 

E
S
M
n
i
 
n
a
G

i

1.2

1

0.8

0.6

 

 

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

 

E
S
M
n
i
 
n
a
G

i

1.2

1

0.8

0.6

0.4

 

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

 

2.5

2

1.5

 

E
S
M
n
i
 
n
a
G

i

1

 

 

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

G
1

G
2
(a) Sport

G
3

G
G
1
2
(b) Health

G
3

G
1

G
2
(c) Yelp

G
3

G
1

G
2
(d) BX

G
3

Figure 6: Performance gain in MSE compared with MF for users with limited training samples. G1, G2, and G3 are groups of
users with [1, 3], [4, 6], and [7, 10] observed samples in the training set, respectively.

 

0.82

 

1.3

 

3

 

0.8

E
S
M

0.78

0.76

0.74

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

1.28

E
S
M

1.26

1.24

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

E
S
M

2.8

2.6

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

MVM
FM−2
FM−3
PolyNet−2
PolyNet−3
SFM

10

15

20

R

(a) Sport

10

15

20

R

(b) Health

R
(c) Yelp

30

0.72
 
5

30

1.22
 
5

10

15

20

30

2.4
 
5

10

20

30

15

R
(d) BX

Figure 7: Sensitivity analysis of the latent dimension R.

1

 

E
S
M
n
i
 
n
a
G

i

0.8

0.6

0.4

 

E
S
M

0.66

0.65

0.64

0.63

0.62

0.61

0.6
 
5

have a linear complexity in Nz (B). This experiment substantiates
the eﬃciency of the proposed SFM for large datasets.

5.6 Analysis of the Impact of Data Sparsity
We proceed by further studying the impact of data sparsity on
diﬀerent methods. As can be found in the experimental results,
the improvement of SFM over the traditional collaborative ﬁlter-
ing methods (e.g., MF) is signiﬁcant for datasets that are sparse,
mainly because the number of samples is too scarce to model the
items and users adequately. We verify this ﬁnding by comparing
the performance of comparison methods with MF on users with
limited training data. Shown in Fig. 6 is the gain of each method
compared with MF for users with limited training samples, where
G1, G2, and G3 are groups of users with [1, 3], [4, 6], and [7, 10]
observed samples in the training set. Due to space limit, we only
report the results from two Amazon datasets (Sport and Health)
It can be
while the observations still hold for the rest datasets.
seen that the proposed SFM gains the most in group G1, in which
the users have extremely few training items. The performance gain
starts to decrease with the number of training items available for
each user. The results indicate that including meta information
can be valuable information especially when limited information
available.

5.7 Sensitivity analysis
The number of latent factors R is an important hyperparameter for
the factorization models. We analyze diﬀerent values of R and re-
port the averaged results in Fig. 7. The results again show that SFM

consistently outperforms other methods with various values of R.
In contrast to ﬁndings in other related factorization models [42]
where prediction error can steadily get reduced with larger R, we
observe that the performance of each method is rather stable even
with the increasing of R.
It is reasonable in a general sense, as
the expressiveness of the model is enough to describe the informa-
tion embedded in data. Although larger R renders the model with
greater expressiveness, when the available observations regarding
the target values are too sparse but the meta information is rich,
only a few number of factors are required to ﬁt the data well.

6 CONCLUSIONS
In this paper, we introduce a generic framework for learning struc-
tural data from heterogeneous domains, which can explore the
high order correlations underlying multi-view multi-way data. We
develop structural factorization machines (SFMs) that learn the
common latent spaces shared in the multi-view tensors while au-
tomatically adjust the contribution of each view in the predictive
model. With the help of relational structure representation, we
further provide an eﬃcient approach to avoid unnecessary compu-
tation costs on repeating patterns of the multi-view data. It was
shown that the proposed SFMs outperform state-of-the-art factor-
ization models on eight large-scale datasets in terms of prediction
accuracy and computational cost.

WWW 2018, April 23–27, 2018, Lyon, France

C.-T. Lu et al.

ACKNOWLEDGMENTS
This work is supported in part by NSF through grants IIS-1526499,
and CNS-1626432, and NSFC 61672313, 61503253 and NSF of Guang-
dong Province (2017A030313339). We gratefully acknowledge the
support of NVIDIA Corporation with the donation of the Titan X
GPU used for this research.

REFERENCES
[1] Acar, E., Kolda, T. G., and Dunlavy, D. M. All-at-once optimization for cou-
pled matrix and tensor factorizations. arXiv preprint arXiv:1105.3422 (2011).
[2] Blondel, M., Fujino, A., Ueda, N., and Ishihata, M. Higher-order factoriza-
In Advances in Neural Information Processing Systems (2016),

tion machines.
pp. 3351–3359.

[3] Blondel, M., Ishihata, M., Fujino, A., and Ueda, N. Polynomial networks
and factorization machines: New insights and eﬃcient training algorithms. In
Proceedings of the 33nd International Conference on Machine Learning (2016),
pp. 850–858.

[4] Cao, B., He, L., Kong, X., Yu, P. S., Hao, Z., and Ragin, A. B. Tensor-based multi-
view feature selection with applications to brain diseases. In IEEE International
Conference on Data Mining (2014), pp. 40–49.

[5] Cao, B., Zheng, L., Zhang, C., Yu, P. S., Piscitello, A., Zulueta, J., Ajilore, O.,
Ryan, K., and Leow, A. D. Deepmood: Modeling mobile phone typing dynamics
for mood detection. In Proceedings of ACM SIGKDD international conference on
Knowledge discovery and data mining (2017), pp. 747–755.

[6] Cao, B., Zhou, H., Li, G., and Yu, P. S. Multi-view machines. In ACM Interna-

tional Conference on Web Search and Data Mining (2016), pp. 427–436.

[7] Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H.,
Anderson, G., Corrado, G., Chai, W., Ispir, M., et al. Wide & deep learning
for recommender systems. In DLRS (2016), ACM, pp. 7–10.

[8] Chin, W.-S., Yuan, B.-W., Yang, M.-Y., Zhuang, Y., Juan, Y.-C., and Lin, C.-
J. Libmf: A library for parallel matrix factorization in shared-memory systems.
The Journal of Machine Learning Research 17, 1 (2016), 2971–2975.

[9] Covington, P., Adams, J., and Sargin, E. Deep neural networks for youtube
In ACM Recommender Systems Conference (RecSys) (2016),

recommendations.
ACM, pp. 191–198.

[10] Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online
learning and stochastic optimization. The Journal of Machine Learning Research
12 (2011), 2121–2159.

[11] Guo, H., Tang, R., Ye, Y., Li, Z., and He, X. Deepfm: A factorization-machine
based neural network for ctr prediction. arXiv preprint arXiv:1703.04247 (2017).
[12] Guo, H., and Viktor, H. L. Mining relational data through correlation-based
multiple view validation. In Proceedings of ACM SIGKDD international confer-
ence on Knowledge discovery and data mining (2006), pp. 567–573.

[13] Harper, F. M., and Konstan, J. A. The movielens datasets: History and context.
ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4 (2016), 19.
[14] He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
international conference on computer vision (2015), pp. 1026–1034.

[15] He, L., Kong, X., Philip, S. Y., Ragin, A. B., Hao, Z., and Yang, X. Dusk: A dual
structure-preserving kernel for supervised tensor learning with applications to
neuroimages. matrix 3, 1 (2014), 2.

[16] He, L., Lu, C.-T., Ma, J., Cao, J., Shen, L., and Yu, P. S. Joint community and
structural hole spanner detection via harmonic modularity. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (2016), pp. 875–884.

[17] He, X., and Chua, T.-S. Neural factorization machines for sparse predictive
analytics. In Proceedings of International ACM SIGIR Conference on Research and
Development in Information Retrieval (2017).

[18] Hinton, G., Srivastava, N., and Swersky, K. Rmsprop: Divide the gradient by
a running average of its recent magnitude. Neural networks for machine learning,
Coursera lecture 6e (2012).

[19] Huang, P.-S., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. Learning deep
structured semantic models for web search using clickthrough data. In ACM In-
ternational Conference on Information and Knowledge Management (2013), ACM,
pp. 2333–2338.

[20] Juan, Y., Zhuang, Y., Chin, W.-S., and Lin, C.-J. Field-aware factorization ma-
chines for ctr prediction. In Proceedings of the 10th ACM Conference on Recom-
mender Systems (2016), ACM, pp. 43–50.

[21] Kingma, D., and Ba, J. Adam: A method for stochastic optimization. arXiv

[22] Kolda, T. G., and Bader, B. W. Tensor decompositions and applications. SIAM

preprint arXiv:1412.6980 (2014).

review 51, 3 (2009), 455–500.

[23] Koren, Y. Factorization meets the neighborhood: a multifaceted collabora-
In Proceedings of ACM SIGKDD international conference

tive ﬁltering model.
on Knowledge discovery and data mining (2008), pp. 426–434.

[24] Koren, Y. Factor in the neighbors: Scalable and accurate collaborative ﬁltering.
ACM Transactions on Knowledge Discovery from Data (TKDD) 4, 1 (2010), 1.
[25] Liang, T., He, L., Lu, C.-T., Chen, L., Yu, P. S., and Wu, J. A broad learning
approach for context-aware mobile application recommendation. In 2017 IEEE
International Conference on Data Mining (ICDM) (Nov. 2017), pp. 955–960.
[26] Ling, G., Lyu, M. R., and King, I. Ratings meet reviews, a combined approach to
recommend. In Proceedings of the 8th ACM Conference on Recommender systems
(2014), pp. 105–112.

[27] Livni, R., Shalev-Shwartz, S., and Shamir, O. On the computational eﬃciency
of training neural networks. In Advances in Neural Information Processing Sys-
tems (2014), pp. 855–863.

[28] Lu, C.-T., He, L., Shao, W., Cao, B., and Yu, P. S. Multilinear factorization
machines for multi-task multi-view learning. In Proceedings of the Tenth ACM
International Conference on Web Search and Data Mining (2017), pp. 701–709.

[29] Lu, C.-T., Xie, S., Shao, W., He, L., and Yu, P. S.

Item recommendation for
In Proceedings of International Joint Conference

emerging online businesses.
Artiﬁcial Intelligence (2016), pp. 3797–3803.

[30] McAuley, J., and Leskovec, J. Hidden factors and hidden topics: understanding
rating dimensions with review text. In Proceedings of the 7th ACM conference on
Recommender systems (2013), pp. 165–172.

[31] McAuley, J., Pandey, R., and Leskovec, J. Inferring networks of substitutable
and complementary products. In Proceedings of ACM SIGKDD international con-
ference on Knowledge discovery and data mining (2015), pp. 785–794.

[32] Novikov, A., Trofimov, M., and Oseledets, I. Exponential machines. In Inter-

national Conference on Learning Representations (2017).

[33] Qu, Y., Cai, H., Ren, K., Zhang, W., Yu, Y., Wen, Y., and Wang, J. Product-
In Data Mining (ICDM),

based neural networks for user response prediction.
2016 IEEE 16th International Conference on (2016), IEEE, pp. 1149–1154.

[34] Rendle, S. Factorization machines with libFM. Intelligent Systems and Technol-

ogy 3, 3 (2012), 57.

[35] Rendle, S. Scaling factorization machines to relational data. In Proceedings of

the VLDB Endowment (2013), vol. 6, VLDB Endowment, pp. 337–348.

[36] Rendle, S., and Schmidt-Thieme, L. Pairwise interaction tensor factorization
for personalized tag recommendation. In Proceedings of the third ACM interna-
tional conference on Web search and data mining (2010), ACM, pp. 81–90.
[37] Shan, Y., Hoens, T. R., Jiao, J., Wang, H., Yu, D., and Mao, J. Deep crossing:
Web-scale modeling without manually crafted combinatorial features. In Pro-
ceedings of ACM SIGKDD international conference on Knowledge discovery and
data mining (2016), ACM, pp. 255–262.

[38] Singh, A. P., and Gordon, G. J. Relational learning via collective matrix factor-
ization. In Proceedings of ACM SIGKDD international conference on Knowledge
discovery and data mining (2008), pp. 650–658.

[39] Wang, R., Fu, B., Fu, G., and Wang, M. Deep & cross network for ad click

predictions. arXiv preprint arXiv:1708.05123 (2017).

[40] Xiao, J., Ye, H., He, X., Zhang, H., Wu, F., and Chua, T.-S. Attentional fac-
torization machines: Learning the weight of feature interactions via attention
networks. In International Joint Conference on Artiﬁcial Intelligence (2017).
[41] Xu, C., Tao, D., and Xu, C. A survey on multi-view learning. arXiv:1304.5634

(2013).

[42] Yan, L., Li, W.-j., Xue, G.-R., and Han, D. Coupled group lasso for web-scale
CTR prediction in display advertising. In International Conference on Machine
Learning (2014), pp. 802–810.

[43] Zhang, J., Lu, C.-T., Cao, B., Chang, Y., and Yu, P. S. Connecting emerging
relationships from news via tensor factorization. In Proceedings of IEEE Interna-
tional Conference on Big Data (2017), IEEE.

[44] Zhang, W., Du, T., and Wang, J. Deep learning over multi-ﬁeld categorical data.
In European conference on information retrieval (2016), Springer, pp. 45–57.
[45] Zheng, L., Noroozi, V., and Yu, P. S. Joint deep modeling of users and items us-
ing reviews for recommendation. In Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining (2017), pp. 425–434.

[46] Zhou, G., Song, C., Zhu, X., Ma, X., Yan, Y., Dai, X., Zhu, H., Jin, J., Li, H., and
Gai, K. Deep interest network for click-through rate prediction. arXiv preprint
arXiv:1706.06978 (2017).

[47] Zhu, J., Shan, Y., Mao, J., Yu, D., Rahmanian, H., and Zhang, Y. Deep embed-
ding forest: Forest-based serving with deep embedding features. In Proceedings
of ACM SIGKDD international conference on Knowledge discovery and data min-
ing (2017).

