0
2
0
2
 
n
a
J
 
9
 
 
]

G
L
.
s
c
[
 
 
2
v
7
1
0
2
0
.
1
1
8
1
:
v
i
X
r
a

A General Theory of Equivariant CNNs on
Homogeneous Spaces

Taco S. Cohen
Qualcomm AI Research∗
Qualcomm Technologies Netherlands B.V.
tacos@qti.qualcomm.com

Mario Geiger
PCSL Research Group
EPFL
mario.geiger@epfl.ch

Maurice Weiler
QUVA Lab
U. of Amsterdam
m.weiler@uva.nl

Abstract

We present a general theory of Group equivariant Convolutional Neural Networks
(G-CNNs) on homogeneous spaces such as Euclidean space and the sphere. Feature
maps in these networks represent ﬁelds on a homogeneous base space, and layers
are equivariant maps between spaces of ﬁelds. The theory enables a systematic
classiﬁcation of all existing G-CNNs in terms of their symmetry group, base
space, and ﬁeld type. We also consider a fundamental question: what is the most
general kind of equivariant linear map between feature spaces (ﬁelds) of given
types? Following Mackey, we show that such maps correspond one-to-one with
convolutions using equivariant kernels, and characterize the space of such kernels.

1

Introduction

Through the use of convolution layers, Convolutional Neural Networks (CNNs) have a built-in
understanding of locality and translational symmetry that is inherent in many learning problems.
Because convolutions are translation equivariant (a shift of the input leads to a shift of the output),
convolution layers preserve the translation symmetry. This is important, because it means that further
layers of the network can also exploit the symmetry.

Motivated by the success of CNNs, many researchers have worked on generalizations, leading to a
growing body of work on Group equivariant CNNs (G-CNNs) for signals on Euclidean space and
the sphere [1–7] as well as graphs [8, 9]. With the proliferation of equivariant network layers, it has
become difﬁcult to see the relations between the various approaches. Furthermore, when faced with
a new modality (diffusion tensor MRI, say), it may not be immediately obvious how to create an
equivariant network for it, or whether a given kind of equivariant layer is the most general one.

In this paper we present a general theory of homogeneous G-CNNs. Feature spaces are modelled
as spaces of ﬁelds on a homogeneous space. They are characterized by a group of symmetries
G, a subgroup H ≤ G that together with G determines a homogeneous space B (cid:39) G/H, and a
representation ρ of H that determines the type of ﬁeld (vector, tensor, etc.). Related work is classiﬁed
by (G, H, ρ). The main theorems say that equivariant linear maps between ﬁelds over B can be
written as convolutions with an equivariant kernel, and that the space of equivariant kernels can be
realized in three equivalent ways. We will assume some familiarity with groups, cosets, quotients,
representations and related notions (see Appendix A).

This paper does not contain truly new mathematics (in the sense that a professional mathematician
with expertise in the relevant subjects would not be surprised by our results), but instead provides
a new formalism for the study of equivariant convolutional networks. This formalism turns out to
be a remarkably good ﬁt for describing real-world G-CNNs. Moreover, by describing G-CNNs in a
language used throughout modern physics and mathematics (ﬁelds, ﬁber bundles, etc.), it becomes
possible to apply knowledge gained over many decades in those domains to machine learning.

*Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

1.1 Overview of the Theory

This paper has two main parts. First, in Sec. 2, we introduce a mathematical model for convolutional
feature spaces. The basic idea is that feature maps represent ﬁelds over a homogeneous space. As it
turns out, deﬁning the notion of a ﬁeld is quite a bit of work. So in order to motivate the introduction
of each of the required concepts, we will in this section provide an overview of the relevant concepts
and their relations, using the example of a Spherical CNN with vector ﬁeld feature maps.

The second part of this paper (Section 3) is about maps between the feature spaces. We require these
to be equivariant, and focus in particular on the linear layers. The main theorems (3.1–3.4) show that
linear equivariant maps between the feature spaces are in one-to-one correspondence with equivariant
convolution kernels (i.e. convolution is all you need), and that the space of equivariant kernels can be
realized as a space of matrix-valued functions on a group, coset space, or double coset space, subject
to linear constraints.

In order to specify a convolutional feature space, we need to specify two things: a homogeneous
space B over which the ﬁeld is deﬁned, and the type of ﬁeld (e.g. vector ﬁeld, tensor ﬁeld, etc.). A
homogeneous space for a group G is a space B where for any two x, y ∈ B there is a transformation
g ∈ G that relates them via gx = y. Here we consider the example of a vector ﬁeld on the sphere
B = S2 with symmetry group G = SO(3), the group of 3D rotations. The sphere is a homogeneous
space for SO(3) because we can map any point on the sphere to any other via a rotation.

Formally, a ﬁeld is deﬁned as a section of a vector bundle associated to a principal bundle. In order
to understand what this means, we must ﬁrst know what a ﬁber bundle is (Sec. 2.1), and understand
how the group G can be viewed as a principal bundle (Sec. 2.2). Brieﬂy, a ﬁber bundle formalizes the
idea of parameterizing a set of identical spaces called ﬁbers by another space called the base space.

The ﬁrst way in which ﬁber bundles play a role in the theory is that the action
of G on B allows us to think of G as a “bundle of groups” or principal bundle.
Roughly speaking, this works as follows: if we ﬁx an origin o ∈ B, we
can consider the stabilizer subgroup H ≤ G of transformations that leave o
unchanged: H = {g ∈ G | go = o}. For example, on the sphere the stabilizer
is SO(2), the group of rotations around the axis through o (e.g. the north pole).
As we will see in Section 2.2, this allows us to view G as a bundle with base
space B (cid:39) G/H and a ﬁber H. This is shown for the sphere in Fig. 1 (cartoon).
In this case, we can think of SO(3) as a bundle of circles (H = SO(2)) over
the sphere, which itself is the quotient S2 (cid:39) SO(3)/ SO(2).

Figure 1: SO(3) as
a principal SO(2)
bundle over S2.

To deﬁne the associated bundle (Sec. 2.3) we take the principal bundle G and
replace the ﬁber H by a vector space V on which H acts linearly via a group
representation ρ. This yields a vector bundle with the same base space B and
a new ﬁber V . For example, the tangent bundle of S2 (Fig. 2) is obtained by
replacing the circular SO(2) ﬁbers in Fig. 1 by 2D planes. Under the action
of H = SO(2), a tangent vector at the north pole is rotated (even though
the north pole itself is ﬁxed by SO(2)), so we let ρ(h) be a 2 × 2 rotation
matrix. In a general convolutional feature space with n channels, V would be
an n-dimensional vector space. Finally, ﬁelds are deﬁned as sections of this

bundle, i.e. an assignment to each point x of an element in the ﬁber over x (see Fig. 3).

Figure 2: Tangent
bundle of S2.

Having deﬁned the feature space, we need to
specify how it transforms (e.g. say how a vector
ﬁeld on S2 is rotated). The natural way to trans-
form a ρ-ﬁeld is via the induced representation
π = IndG
H ρ of G (Section 2.4), which combines
the action of G on the base space B and the ac-
tion of ρ on the ﬁber V to produce an action on
sections of the associated bundle (See Figure 3).
Finally, having deﬁned the feature spaces and
their transformation laws, we can study equiv-
ariant linear maps between them (Section 3). In
Sec. 4–6 we cover implementation aspects, re-
lated work, and concrete examples, respectively.

Figure 3: Φ maps scalar ﬁelds to vector ﬁelds,
and is equivariant to the induced representation
πi = IndSO(3)

SO(2) ρi.

2

2 Convolutional Feature Spaces

2.1 Fiber Bundles

Intuitively, a ﬁber bundle is a parameterization of a set of isomorphic spaces (the ﬁbers) by another
space (the base). For example, we can think of a feature space in a classical CNN as a set of vector
spaces Vx (cid:39) Rn (n being the number of channels), one per position x in the plane [2]. This is an
example of a trivial bundle, because it is simply the Cartesian product of the plane and Rn. General
ﬁber bundles are only locally trivial, meaning that they locally look like a product while having a
different global topological structure.

The simplest example of a non-trivial bundle is the Mobius strip, which
locally looks like a product of the circle (the base) with a line segment
(the ﬁber), but is globally distinct from a cylinder (see Fig. 4). A more
practically relevant example is given by the tangent bundle of the sphere
(Fig. 2), which has as base space S2 and ﬁbers that look like R2, but is
topologically distinct from S2 × R2 as a bundle.
Formally, a bundle consists of topological spaces E (total space), B (base space), F (canonical
ﬁber), and a projection map p : E → B, satisfying a local triviality condition. Basically, this
condition says that locally, the bundle looks like a product U × F of a piece U ⊆ B of the base
space, and F the canonical ﬁber. Formally, the condition is that for every a ∈ E, there is an
1(U ) → U × F so that the
open neighbourhood U ⊆ B of p(a) and a homeomorphism ϕ : p−
1(U ) → U (where proj1(u, f ) = u). The
map p−
homeomorphism ϕ is said to locally trivialize the bundle above the trivializing neighbourhood U .

Figure 4: Cylinder and
Möbius strip

proj1−−−→ U agrees with p : p−

ϕ
−→ U × F

1(U )

1(x) is F , and ϕ is a homeomorphism, we
Considering that for any x ∈ U the preimage proj1−
1(x) for x ∈ B is also homeomorphic to F . Thus, we call Fx the
see that the preimage Fx = p−
ﬁber over x, and see that all ﬁbers are homeomorphic. Knowing this, we can denote a bundle by its
projection map p : E → B, leaving the canonical ﬁber F implicit.

Various more reﬁned notions of ﬁber bundle exist, each corresponding to a different kind of ﬁber. In
this paper we will work with principal bundles (bundles of groups) and vector bundles (bundles of
vector spaces).

A section s of a ﬁber bundle is an assignment to each x ∈ B of an element s(x) ∈ Fx. Formally, it is
a map s : B → E that satisﬁes p◦s = idB. If the bundle is trivial, a section is equivalent to a function
f : B → F , but for a non-trivial bundle we cannot continuously align all the ﬁbers simultaneously,
and so we must keep each s(x) in its own ﬁber Fx. Nevertheless, on a trivializing neighbourhood
U ⊆ B, we can describe the section as a function sU : U → F , by setting ϕ(s(x)) = (x, sU (x)).

2.2 G as a Principal H-Bundle

Recall (Sec. 1.1) that with every feature space of a G-CNN is associated a homogeneous space
B (e.g. the sphere, projective space, hyperbolic space, Grassmann & Stiefel manifolds, etc.), and
recall further that such a space has a stabilizer subgroup H = {g ∈ G | go = o} (this group being
independent of origin o up to isomorphism). As discussed in Appendix A, the cosets gH of H (e.g.
the circles in Fig. 1) partition G, and the set of cosets, denoted G/H (e.g. the sphere in Fig. 1), can
be identiﬁed with B (up to a choice of origin).

It is this partitioning of G into cosets that induces a special kind of bundle structure on G. The
projection map that deﬁnes the bundle structure sends an element g ∈ G to the coset gH it belongs
to. Thus, it is a map p : G → G/H, and we have a bundle with total space G, base space G/H and
canonical ﬁber H. Intuitively, this allows us to think of G as a base space G/H with a copy of H
attached at each point x ∈ G/H. The copies of H are glued together in a potentially twisted manner.

This bundle is called a principal H-bundle, because we have a transitive and ﬁxed-point free group
action G × H → G that preserves the ﬁbers. This action is given by right multiplication, g (cid:55)→ gh,
which preserves ﬁbers because p(gh) = ghH = gH = p(g). That is, by right-multiplying an
element g ∈ G by h ∈ H, we get an element gh that is in general different from g but is still within
the same coset (i.e. ﬁber). That the action is transitive and free on cosets follows immediately from
the group axioms.

3

One can think of a principal bundle as a bundle of generalized frames or gauges relative to which
geometrical quantities can be expressed numerically. Under this interpretation the ﬁber at x is a space
of generalized frames, and the action by H is a change of frame. For instance, each point on the
circles in Fig. 1 can be identiﬁed with a right-handed orthogonal frame, and the action of SO(2)
corresponds to a rotation of this frame. The group H may also include internal symmetries, such as
color space rotations, which do not relate in any way to the spatial dimensions of B.

In order to numerically represent a ﬁeld on some neighbourhood U ⊆ G/H, we need to choose a
frame for each x ∈ U in a continuous manner. This is formalized as a section of the principal bundle.
Recall that a section of p : G → G/H is a map s : G/H → G that satisﬁes p ◦ s = idG/H . Since
p projects g to its coset gH, the section chooses a representative s(gH) ∈ gH for each coset gH.
Non-trivial principal bundles do not have continuous global sections, but we can always use a local
section on U ⊆ G/H, and represent a ﬁeld on overlapping local patches covering G/H.

Aside from the right action of H, which turns G into a principal H-bundle, we also have a left
action of G on itself, as well as an action of G on the base space G/H. In general, the action of
G on G/H does not agree with the action on G, in that gs(x) (cid:54)= s(gx), because the action on G
includes a twist of the ﬁber. This twist is described by the function h : G/H × G → H deﬁned by
gs(x) = s(gx)h(x, g) (whenever both s(x) and s(gx) are deﬁned). This function will be used in
various calculations below. We note for the interested reader that h satisﬁes the cocycle condition
h(x, g1g2) = h(g2x, g1)h(x, g2).

2.3 The Associated Vector Bundle

Feature spaces are deﬁned as spaces of sections of the associated vector bundle, which we will now
deﬁne. In physics, a section of an associated bundle is simply called a ﬁeld.

p
−→ G/H, and
To deﬁne the associated vector bundle, we start with the principal H-bundle G
essentially replace the ﬁbers (cosets) by vector spaces V . The space V (cid:39) Rn carries a group
representation ρ of H that describes the transformation behaviour of the feature vectors in V under a
change of frame. These features could for instance transform as a scalar, a vector, a tensor, or some
other geometrical quantity [2, 6, 8]. Figure 3 shows an example of a vector ﬁeld (ρ(h) being a 2 × 2
rotation matrix in this case) and a scalar ﬁeld (ρ(h) = 1).

The ﬁrst step in constructing the associated vector bundle is to take the product G × V . In the context
of representation learning, we can think of an element (g, v) of G × V as a feature vector v ∈ V and
an associated pose variable g ∈ G that describes how the feature detector was steered to obtain v.
For instance, in a Spherical CNN [10] one would rotate a ﬁlter bank by g ∈ SO(3) and match it with
the input to obtain v. If we apply a transformation h ∈ H to g and simultaneously apply its inverse
1)v). In a Spherical CNN, this would correspond to a
to v, we get an equivalent element (gh, ρ(h−
change in orientation of the ﬁlters by h ∈ SO(2).

So in order to create the associated bundle, we take the quotient of the product G × V by this
action: A = G ×ρ V = (G × V )/H. In other words, the elements of A are orbits, deﬁned as
1)v) | h ∈ H}. The projection pA : A → G/H is deﬁned as pA([g, v]) = gH.
[g, v] = {(gh, ρ(h−
One may check that this is well deﬁned, i.e. independent of the orbit representative g of [g, v] =
1)v]. Thus, the associated bundle has base G/H and ﬁber V , meaning that locally it looks
[gh, ρ(h−
like G/H × V . We note that the associated bundle construction works for any principal H-bundle,
nog just p : G → G/H, which suggests a direction for further generalization [11].

A ﬁeld (“stack of feature maps”) is a section of the associated bundle, meaning that it is a map
s : G/H → A such that πρ ◦ s = idG/H . We will refer to the space of sections of the associated
vector bundle as I. Concretely, we have two ways to encode a section: as functions f : G → V
subject to a constraint, and as local functions from U ⊆ G/H to V . We will now deﬁne both.

2.3.1 Sections as Mackey Functions

The construction of the associated bundle as a product G × V subject to an equivalence relation
suggests a way to describe sections concretely: a section can be represented by a function f : G → V
subject to the equivariance condition

f (gh) = ρ(h−

1)f (g).

(1)

4

Such functions are called Mackey functions. They provide a redundant encoding of a section of A,
by encoding the value of the section relative to any choice of frame / section of the principal bundle
simultaneously, with the equivariance constraint ensuring consistency.

A linear combination of Mackey functions is a Mackey function, so they form a vector space, which we
will refer to as IG. Mackey functions are easy to work with because they allow a concrete and global
description of a ﬁeld, but their redundancy makes them unsuitable for computer implementation.

2.3.2 Local Sections as Functions on G/H

The associated bundle has base G/H and ﬁber V , so locally, we can describe a section as an
unconstrained function f : U → V where U ⊆ G/H is a trivializing neighbourhood (see Sec. 2.1).
We refer to the space of such sections as IC. Given a local section f ∈ IC, we can encode it as a
Mackey function through the following lifting isomorphism Λ : IC → IG:

[Λf ](g) = ρ(h(g)−
1f (cid:48)](x) = f (cid:48)(s(x)),

[Λ−

1)f (gH),

(2)

1g ∈ H and s(x) ∈ G is a coset representative for x ∈ G/H.
where h(g) = h(H, g) = s(gH)−
This map is analogous to the lifting deﬁned by [12] for scalar ﬁelds (i.e. ρ(h) = I), and can be
deﬁned more generally for any principal / associated bundle [13].

2.4 The Induced Representation

The induced representation π = IndG

H ρ describes the action of G on ﬁelds. In IG, it is deﬁned as:
(3)
[πG(g)f ](k) = f (g−

1k).

In IC, we can deﬁne the induced representation πC on a local neighbourhood U as
[πC(g)f ](x) = ρ(h(g−

(4)
Here we have assumed that h is deﬁned at (g−
is not, one would need to
change to a different section of G → G/H. One may verify, using the composition
4 does indeed deﬁne a representation of G. Moreover,
law for h (Sec.
one may verify that πG(g) ◦ Λ = Λ ◦ πC(g), i.e.
they deﬁne isomorphic representations.

1, x)−
1, x).

1)f (g−

that Eq.

1x).

2.2),

If it

We can interpret Eq. 4 as follows. To transform a ﬁeld,
1x to x, and we apply a trans-
we move the ﬁber at g−
formation to the ﬁber itself using ρ. This is visualized
in Fig. 5 for a planar vector ﬁeld. Some other exam-
ples include an RGB image (ρ(h) = I3), a ﬁeld of wind
directions on earth (ρ(h) a 2 × 2 rotation matrix), a diffu-
sion tensor MRI image (ρ(h) a representation of SO(3)
acting on 2-tensors), a regular G-CNN on Z3 [14, 15]
(ρ a regular representation of H).

3 Equivariant Maps and Convolutions

Figure 5: The rotation of a planar vector
ﬁeld in two steps: moving each vector to
its new position without changing its ori-
entation, and then rotating the vectors.

Each feature space in a G-CNN is deﬁned as the space of sections of some associated vector
bundle, deﬁned by a choice of base G/H and representation ρ of H that describes how the ﬁbers
transform. A layer in a G-CNN is a map between these feature spaces that is equivariant to the
induced representations acting on them. In this section we will show that equivariant linear maps can
always be written as a convolution-like operation using an equivariant kernel. We will ﬁrst derive this
result for the induced representation realized in the space IG of Mackey functions, and then convert
the result to local sections of the associated vector bundle in Section 3.2. We will assume that G is
locally compact and unimodular.
Consider adjacent feature spaces i = 1, 2 with a representation (ρi, Vi) of Hi ≤ G. Let πi = IndG
be the representation acting on I i
G. A bounded linear operator I 1

G can be written as

G → I 2

Hi ρi

[κ · f ](g) =

κ(g, g(cid:48))f (g(cid:48))dg(cid:48),

(5)

(cid:90)

G

5

using a two-argument linear operator-valued kernel κ : G × G → Hom(V1, V2), where Hom(V1, V2)
denotes the space of linear maps V1 → V2. Choosing bases, we get a matrix-valued kernel.
We are interested in the space of equivariant linear maps between induced representations, deﬁned
as H = HomG(I 1, I 2) = {Φ ∈ Hom(I 1, I 2) | Φπ1(g) = π2(g)Φ, ∀g ∈ G}. In order for Eq. 5 to
deﬁne an equivariant map Φ ∈ H, the kernel κ must satisfy a constraint. By (partially) resolving this
constraint, we will show that Eq. 5 can always be written as a cross-correlation1
Theorem 3.1. (convolution is all you need) An equivariant map Φ ∈ H can always be written as a
convolution-like integral.

Proof. Since we are only interested in equivariant maps, we get a constraint on κ. For all u, g ∈ G:

[κ · [π1(u)f ]](g) = [π2(u)[κ · f ]](g)

κ(g, g(cid:48))f (u−

1g(cid:48))dg(cid:48) =

κ(u−

1g, g(cid:48))f (g(cid:48))dg(cid:48)

(cid:90)

G
(cid:90)

G

⇔

⇔

⇔

⇔

(cid:90)

G

(cid:90)

G

κ(g, ug(cid:48)) = κ(u−

1g, g(cid:48))

κ(ug, ug(cid:48)) = κ(g, g(cid:48))

κ(g, ug(cid:48))f (g(cid:48))dg(cid:48) =

κ(u−

1g, g(cid:48))f (g(cid:48))dg(cid:48)

(6)

Hence, without loss of generality, we can deﬁne the two-argument kernel κ(·, ·) in terms of a
1g(cid:48)) = κ(ge, gg−
one-argument kernel: κ(g−
The application of κ to f thus reduces to a cross-correlation:

1g(cid:48)) = κ(g, g(cid:48)).

1g(cid:48)) ≡ κ(e, g−

[κ · f ](g) =

κ(g, g(cid:48))f (g(cid:48))dg(cid:48) =

κ(g−

1g(cid:48))f (g(cid:48))dg(cid:48) = [κ (cid:63) f ](g).

(7)

(cid:90)

G

(cid:90)

G

3.1 The Space of Equivariant Kernels

The constraint Eq. 6 implies a constraint on the one-argument kernel κ. The space of admissible
kernels is in one-to-one correspondence with the space of equivariant maps. Here we give three
different characterizations of this space of kernels. Detailed proofs can be found in Appendix B.
Theorem 3.2. H is isomorphic to the space of bi-equivariant kernels on G, deﬁned as:

KG = {κ : G → Hom(V1, V2) | κ(h2gh1) = ρ2(h2)κ(g)ρ1(h1),

∀g ∈ G, h1 ∈ H1, h2 ∈ H2}.

(8)

Proof. It is easily veriﬁed (see supp. mat.) that right equivariance follows from the fact that f ∈ I 1
G
is a Mackey function, and left equivariance follows from the requirement that κ (cid:63) f ∈ I 2
G should be a
Mackey function. The isomorphism is given by ΓG : KG → H deﬁned as [ΓGκ]f = κ (cid:63) f .

The analogous result for the two argument kernel is that κ(gh2, g(cid:48)h1) should be equal to
1
2 )κ(g, g(cid:48))ρ1(h1) for g, g(cid:48) ∈ G, h1 ∈ H1, h2 ∈ H2. This has the following interesting in-
ρ2(h−
terpretation: κ is a section of a certain associated bundle. We deﬁne a right-action of H1 × H2 on
G×G by setting (g, g(cid:48))·(h1, h2) = (gh1, g(cid:48)h2) and a representation ρ12 of H1 ×H2 on Hom(V1, V2)
1 ) for Ψ ∈ Hom(V1, V2). Then the constraint on κ(·, ·)
by setting ρ12(h1, h2)Ψ = ρ2(h2)Ψρ1(h−
1)κ((g, g(cid:48))). We recognize this as the condition
can be written as κ((g, g(cid:48))·(h1, h2)) = ρ12((h1, h2)−
of being a Mackey function (Eq. 1) for the bundle (G × G) ×ρ12 Hom(V1, V2).
There is another another way to characterize the space of equivariant kernels:
Theorem 3.3. H is isomorphic to the space of left-equivariant kernels on G/H1, deﬁned as:

1

KC = {←−κ : G/H1 → Hom(V1, V2) | ←−κ (h2x) = ρ2(h2)←−κ (x)ρ1(h1(x, h2)−

1),

(9)

∀h2 ∈ H2, x ∈ G/H1}

1As in most of the CNN literature, we will not be precise about distinguishing convolution and correlation.

6

Proof. using the decomposition g = s(gH1)h1(g) (see Appendix A), we can deﬁne

κ(g) = κ(s(gH1)h1(g)) = κ(s(gH1)) ρ1(h1(g)) ≡ ←−κ (gH1)ρ1(h1(g)),

(10)

This deﬁnes the lifting isomorphism for kernels, Λ
K
deﬁned in this way, κ satisﬁes right H1-equivariance.
We still have the left H2-equivariance constraint from Eq. 8, which translates to ←−κ as follows (details
in supp. mat.). For g ∈ G, h2 ∈ H2 and x ∈ G/H1,

: KC → KG. It is easy to verify that when

κ(h2g) = ρ2(h2)κ(g) ⇔ ←−κ (h2x) = ρ2(h2)←−κ (x)ρ1(h1(x, h2)−

1).

(11)

Theorem 3.4. H is isomorphic to the space of H γ(x)H1

2

-equivariant kernels on H2\G/H1:

KD = {¯κ : H2\G/H1 → Hom(V1, V2) | ¯κ(x) = ρ2(h)¯κ(x)ρx

1 (h)−

1,

∀x ∈ H2\G/H1, h ∈ H γ(x)H1

},

2

Where γ : H2\G/H1 → G is a choice of double coset representatives, and ρx
the stabilizer H γ(x)H1
= {h ∈ H2 | hγ(x)H1 = γ(x)H1} ≤ H1, deﬁned as

2

1 is a representation of

ρx
1 (h) = ρ1(h1(γ(x)H1, h)) = ρ1(γ(x)−

1hγ(x)),

(12)

(13)

Proof. In supplementary material. For examples, see Section 6.

3.2 Local Sections on G/H

We have seen that an equivariant map between spaces of Mackey functions can always be realized as
a cross-correlation on G, and we have studied the properties of the kernel, which can be encoded as
a kernel on G or G/H1 or H2\G/H1, subject to the appropriate constraints. When implementing
a G-CNN, it would be wasteful to use a Mackey function on G, so we need to understand what it
means for ﬁelds realized by local functions f : U → V for U ⊆ G/H1. This is done by sandwiching
the cross-correlation κ(cid:63) : I 1

G with the lifting isomorphisms Λi : I i

C → I i
G.

G → I 2

1
[Λ−

2 [κ (cid:63) [Λ1f ]]](x) =

κ(s2(x)−

1s1(y))f (y)dy

←−κ (s2(x)−

1y)ρ1(h1(s2(x)−

1s1(y)))f (y)dy

(14)

(cid:90)

G

(cid:90)

=

G/H1

Which we refer to as the ρ1-twisted cross-correlation on G/H1. We note that for semidirect product
groups, the ρ1 factor disappears and we are left with a standard cross-correlation on G/H1 with
an equivariant kernel ←−κ ∈ KC. We note the similarity of this expression to gauge equivariant
convolution as deﬁned in [11].

3.3 Equivariant Nonlinearities

The network as a whole is equivariant if all of its layers are equivariant. So our theory would not be
complete without a discussion of equivariant nonlinearities and other kinds of layers. In a regular
G-CNN [1], ρ is the regular representation of H, which means that it can be realized by permutation
matrices. Since permutations and pointwise nonlinearities commute, any such nonlinearity can be
used. For other kinds of representations ρ, special equivariant nonlinearities must be used. Some
choices include norm nonlinearities [3] for unitary representations, tensor product nonlinearities [8],
or gated nonlinearities where a scalar ﬁeld is normalized by a sigmoid and then multiplied by another
ﬁeld [6]. Other constructions, such as batchnorm and ResNets, can also be made equivariant [1, 2].
A comprehensive overview and comparison over equivariant nonlinearities can be found in [7].

7

4

Implementation

Several different approaches to implementing group equivariant CNNs have been proposed in the
literature. The implementation details thereby depend on the speciﬁc choice of symmetry group
G, the homogeneous space G/H, its discretization and the representation ρ. In any case, since the
equivariance constraints on convolution kernels are linear, the space of H-equivariant kernels is a
linear subspace of the unrestricted kernel space. This implies that it is sufﬁcient to solve for a basis
of H-equivariant kernels, in terms of which any equivariant kernel can be expanded using learned
weights.
A case of high practical importance are equivariant CNNs on Euclidean spaces Rd. Implementations
mostly operate on discrete pixel grids. In this case, the steerable kernel basis is typically pre-sampled
on a small grid, linearly combined during the forward pass, and then used in a standard convolution
routine. The sampling procedure requires particular attention since it might introduce aliasing
artifacts [4, 6]. A more in depth discussion of an implementation of equivariant CNNs, operating on
Euclidean pixel grids, is provided in [7]. Alternatively to processing signals on a pixel grid, signals on
Euclidean spaces might be sampled on an irregular point cloud. In this case the steerable kernel space
is typically implemented as an analytical function, which is subsequently sampled on the cloud [5].

Implementations of spherical CNNs depend on the choice of signal representation as well. In [10],
the authors choose a spectral approach to represent the signal and kernels in Fourier space. The
equivariant convolution is performed by exploiting the Fourier theorem. Other approaches deﬁne
the convolution spatially. In these cases, some grid on the sphere is chosen on which the signal
is sampled. As in the Euclidean case, the convolution is performed by matching the signal with a
H-equivariant kernel, which is being expanded in terms of a pre-computed basis.

5 Related Work

In Appendix D, we provide a systematic classiﬁcation of equivariant CNNs on homogeneous spaces,
according to the theory presented in this paper. Besides these references, several papers deserve
special mention. Most closely related is the work of [12], whose theory is analogous to ours, but only
covers scalar ﬁelds (corresponding to using a trivial representation ρ(h) = I in our theory). A proper
treatment of general ﬁelds as we do here is more difﬁcult, as it requires the use of ﬁber bundles and
induced representations. The ﬁrst use of induced representations and ﬁelds in CNNs is [2], and the
ﬁrst CNN on a non-trivial homogeneous space (the Sphere) is [16].

A framework for (non-convolutional) networks equivariant to ﬁnite groups was presented by [17], and
equivariant set and graph networks are analyzed by [18–21]. Our use of ﬁelds (with ρ block-diagonal)
can be viewed as a formalization of convolutional capsules [22, 23]. Other related work includes
[24–31]. A preliminary version of this paper appeared as [32].

For mathematical background, we recommend [13, 33–37]. The study of induced representations and
equivariant maps between them was pioneered by Mackey [38–41], who rigorously proved results
essentially similar to the ones in this paper, though presented in a more abstract form that may not be
easy to recognize as having relevance to the theory of equivariant CNNs.

6 Concrete Examples

6.1 The rotation group SO(3) and spherical CNNs

The group of 3D rotations SO(3) is a three-dimensional manifold that can be parameterized by ZYZ
Euler angles α ∈ [0, 2π), β ∈ [0, π] and γ ∈ [0, 2π), i.e. g = Z(α)Y (β)Z(γ), (where Z and Y
denote rotations around the Z and Y axes). For this example we choose H = H1 = H2 = SO(2) =
{Z(α) | α ∈ [0, 2π)} as the group of rotations around the Z-axis, i.e. the stabilizer subgroup of the
north pole of the sphere. A left H-coset is then a subset of SO(3) of the form

gH = {Z(α)Y (β)Z(γ)Z(α(cid:48)) | α(cid:48) ∈ [0, 2π)} = {Z(α)Y (β)Z(α(cid:48)) | α(cid:48) ∈ [0, 2π)}.

Thus, the coset space G/H is the sphere S2, parameterized by spherical coordinates α and β. As
expected, the stabilizer Hx of a point x ∈ S2 is the set of rotations around the axis through x, which
is isomorphic to H = SO(2).

8

Figure 6: Quotients of SO(3) and SE(3).

What about the double coset space (Appendix A.1)? The orbit of a point x(α, β) ∈ S2 under H is a
circle around the Z axis at lattitude β, so the double coset space H\G/H, which indexes these orbits,
is the segment [0, π) (see Fig. 6).

The section s : G/H → G may be deﬁned (almost everywhere) as s(α, β) = Z(α)Y (β) ∈ SO(3),
and γ(β) = Y (β) ∈ SO(3). Then the stabilizer H γ(β)H1
for β ∈ H\G/H is the set of Z-axis
rotations that leave the point γ(β)H1 = (0, β) ∈ S2 invariant. For the north and south pole (β = 0
or β = π), this stabilizer is all of H = SO(2), but for other points it is the trivial subgroup {e}.

2

Thus, according to Theorem 3.4, the equivariant kernels are matrix-valued functions on the segment
[0, π), that are mostly unconstrained (except at the poles). As functions on G/H1 (Theorem 3.3),
they are matrix-valued functions satisfying ←−κ (rx) = ρ2(r)←−κ (x)ρ1(h1(x, r)−
1) for r ∈ SO(2) and
x ∈ S2. This says that as a function on the sphere ←−κ is determined on SO(2)-orbits {rx | r ∈ SO(2)}
(lattitudinal circles around the Z axis) by its value on one point of the orbit. Indeed, if ρ(h) = 1 is
the trivial representation, we see that ←−κ is constant on these orbits, in agreement with [42] who use
isotropic ﬁlters. For ρ2 a regular representation of SO(2), we recover the non-isotropic method of
[10]. For segmentation tasks, one can use a trivial representation for ρ2 in the output layer to obtain a
scalar feature map on S2, analogous to [43]. Other choices, such as ρ the standard 2D representation
of SO(2), would make it possible to build spherical CNNs that can process vector ﬁelds, but this has
not been done yet.

6.2 The roto-translation group SE(3) and 3D Steerable CNNs

The group of rigid body motions SE(3) is a 6D manifold R3 (cid:111) SO(3). We choose H = H1 = H2 =
SO(3) (rotations around the origin). A left H-coset is a set of the form gH = trH = {trr(cid:48) | r(cid:48) ∈
SO(3)} = {tr | r ∈ SO(3)} where t is the translation component of g. Thus, the coset space G/H
is R3. The stabilizer Hx of a point x ∈ R3 is the set of rotations around x, which is isomorphic
to SO(3). The orbit of a point x ∈ R3 is a spherical shell of radius (cid:107)x(cid:107), so the double coset space
H\G/H, which indexes these orbits, is the set of radii [0, ∞).

Since SE(3) is a trivial principal SO(3) bundle, we can choose a global section s : G/H → G by
taking s(x) to be the translation by x. As double coset representatives we can choose γ((cid:107)x(cid:107)) to
be the translation by (0, 0, (cid:107)x(cid:107)). Then the stabilizer H γ(
for (cid:107)x(cid:107) ∈ H\G/H is the set of
(cid:107)
2
rotations around Z, i.e. SO(2), except for (cid:107)x(cid:107) = 0, where it is SO(3).

)H1

x

(cid:107)

For any representations ρ1, ρ2, the equivariant maps between sections of the associated vector bundle
are given by convolutions with matrix-valued kernels on R3 that satisfy ←−κ (rx) = ρ2(r)←−κ (x)ρ1(r−
1)
for r ∈ SO(3) and x ∈ R3. This follows from Theorem 3.3 with the simpliﬁcation h1(x, r) = r
for all r ∈ H, because SE(3) is a semidirect product (Appendix A.2). Alternatively, we can deﬁne
←−κ in terms of ¯κ, which is a kernel on H\G/H = [0, ∞) satisfying ¯κ(x) = ρ2(r)¯κ(x)ρ1(r) for
r ∈ SO(2) and x ∈ [0, ∞). This is in agreement with the results obtained by [6].

7 Conclusion

In this paper we have developed a general theory of equivariant convolutional networks on homoge-
neous spaces using the formalism of ﬁber bundles and ﬁelds. Field theories are the de facto standard
formalism for modern physical theories, and this paper shows that the same formalism can elegantly
describe the de facto standard learning machine: the convolutional network and its generalizations.
By connecting this very successful class of networks to modern theories in mathematics and physics,
our theory provides many opportunities for the development of new theoretical insights about deep
learning, and the development of new equivariant network architectures.

9

References

[1] Taco S Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of
The 33rd International Conference on Machine Learning (ICML), volume 48, pages 2990–2999,
2016.

[2] Taco S Cohen and Max Welling. Steerable CNNs. In ICLR, 2017.

[3] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Har-
monic networks: Deep translation and rotation equivariance. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.

[4] Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable ﬁlters for rotation
equivariant CNNs. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.

[5] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor ﬁeld networks: Rotation- and Translation-Equivariant neural networks for 3D
point clouds. arXiv:1802.08219 [cs.LG], 2018.

[6] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3D steerable
CNNs: Learning rotationally equivariant features in volumetric data. In Advances in Neural
Information Processing Systems (NeurIPS), 2018.

[7] Maurice Weiler and Gabriele Cesa. General E(2)-Equivariant Steerable CNNs. In Advances in

Neural Information Processing Systems (NeurIPS), 2019.

[8] Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning

atomic potentials. arXiv:1803.01588 [cs.LG], 2018.

[9] Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covari-
ant compositional networks for learning graphs. arXiv:1801.02144 [cs.LG], January 2018.

[10] Taco S Cohen, Mario Geiger, Jonas Koehler, and Max Welling. Spherical CNNs. In International

Conference on Learning Representations (ICLR), 2018.

[11] Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge Equivariant
Convolutional Networks and the Icosahedral CNN. In International Conference on Machine
Learning (ICML), 2019.

[12] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution
in neural networks to the action of compact groups. In International Conference on Machine
Learning (ICML), 2018.

[13] Mark Hamilton. Mathematical Gauge Theory: With Applications to the Standard Model of
Particle Physics. Universitext. Springer International Publishing, 2017. ISBN 978-3-319-68438-
3. doi: 10.1007/978-3-319-68439-0.

[14] Marysia Winkels and Taco S Cohen. 3D G-CNNs for pulmonary nodule detection. In Interna-

tional Conference on Medical Imaging with Deep Learning (MIDL), 2018.

[15] Daniel Worrall and Gabriel Brostow. CubeNet: Equivariance to 3D rotation and translation. In

European Conference on Computer Vision (ECCV), 2018.

[16] Taco S Cohen, Mario Geiger, Jonas Koehler, and Max Welling. Convolutional Networks for
Spherical Signals. In ICML Workshop on Principled Approaches to Deep Learning, 2017.

[17] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through Parameter-

Sharing. In International Conference on Machine Learning (ICML), 2017.

[18] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and Equivariant
Graph Networks. In International Conference on Learning Representations (ICLR), 2019.

[19] Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the Universality of
Invariant Networks. In International Conference on Machine Learning (ICML), 2019.

10

[20] Nimrod Segol and Yaron Lipman. On Universal Equivariant Set Networks. arXiv:1910.02421

[cs, stat], October 2019.

[21] Nicolas Keriven and Gabriel Peyré. Universal Invariant and Equivariant Graph Neural Networks.

In Neural Information Processing Systems (NeurIPS), 2019.

[22] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In
I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett, editors,
Advances in Neural Information Processing Systems 30, pages 3856–3866. Curran Associates,
Inc., 2017.

[23] Geoffrey Hinton, Nicholas Frosst, and Sara Sabour. Matrix capsules with EM routing. In

International Conference on Learning Representations (ICLR), 2018.

[24] Chris Olah. Groups and group convolutions.

https://colah.github.io/posts/

2014-12-Groups-Convolution/, 2014.

[25] R Gens and P Domingos. Deep symmetry networks. In Advances in Neural Information

Processing Systems (NIPS), 2014.

[26] Laurent Sifre and Stephane Mallat. Rotation, scaling and deformation invariant scattering for
texture discrimination. IEEE conference on Computer Vision and Pattern Recognition (CVPR),
2013.

[27] E Oyallon and S Mallat. Deep Roto-Translation scattering for object classiﬁcation. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages 2865–2873, 2015.

[28] Stéphane Mallat. Understanding deep convolutional networks. Philos. Trans. A Math. Phys.

Eng. Sci., 374(2065):20150203, April 2016.

[29] Jan J Koenderink. The brain a geometry engine. Psychol. Res., 52(2-3):122–127, 1990.

[30] Jan Koenderink and Andrea van Doorn. The structure of visual spaces. J. Math. Imaging Vis.,

31(2):171, April 2008.

Paris, 97(2-3):265–309, 2003.

[31] Jean Petitot. The neurogeometry of pinwheels as a sub-riemannian contact structure. J. Physiol.

[32] Taco S Cohen, Mario Geiger, and Maurice Weiler. Intertwiners between induced representations
(with applications to the theory of equivariant neural networks). arXiv:1803.10743 [cs.LG],
March 2018.

[33] R W Sharpe. Differential Geometry: Cartan’s Generalization of Klein’s Erlangen Program.

1997.

[34] Adam Marsh. Gauge theories and ﬁber bundles: Deﬁnitions, pictures, and results. July 2016.

[35] G B Folland. A Course in Abstract Harmonic Analysis. CRC Press, 1995.

[36] T Ceccherini-Silberstein, A Machí, F Scarabotti, and F Tolli. Induced representations and

mackey theory. J. Math. Sci., 156(1):11–28, January 2009.

[37] David Gurarie. Symmetries and Laplacians: Introduction to Harmonic Analysis, Group Repre-

sentations and Applications. Elsevier B.V., 1992.

[38] George W Mackey. On induced representations of groups. Amer. J. Math., 73(3):576–592, July

[39] George W Mackey. Induced representations of locally compact groups I. Ann. Math., 55(1):

1951.

101–139, 1952.

[40] George W Mackey.

Induced representations of locally compact groups II. the frobenius

reciprocity theorem. Ann. Math., 58(2):193–221, 1953.

[41] George W Mackey.

Induced Representations of Groups and Quantum Mechanics. W.A.

Benjamin Inc., New York-Amsterdam, 1968.

11

[42] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. 3D object
classiﬁcation and retrieval with spherical CNNs. In European Conference on Computer Vision
(ECCV), 2018.

[43] Jim Winkens, Jasper Linmans, Bastiaan S Veeling, Taco S. Cohen, and Max Welling. Improved
Semantic Segmentation for Histopathology using Rotation Equivariant Convolutional Networks.
In International Conference on Medical Imaging with Deep Learning (MIDL workshop), 2018.

[44] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine 34,
2017.

[45] Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, R E Howard, Wayne E
Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a Back-Propagation
network. In D S Touretzky, editor, Advances in Neural Information Processing Systems 2, pages
396–404. Morgan-Kaufmann, 1990.

[46] S Dieleman, J De Fauw, and K Kavukcuoglu. Exploiting cyclic symmetry in convolutional

neural networks. In International Conference on Machine Learning (ICML), 2016.

[47] Emiel Hoogeboom, Jorn W T Peters, Taco S Cohen, and Max Welling. HexaConv.

In

International Conference on Learning Representations (ICLR), 2018.

[48] Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In CVPR,

[49] Erik J Bekkers, Maxime W Lafarge, Mitko Veta, Koen A J Eppenhof, and Josien P W Pluim.
Roto-Translation covariant convolutional networks for medical image analysis. In Medical
Image Computing and Computer Assisted Intervention (MICCAI), 2018.

[50] Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector

ﬁeld networks. In International Conference on Computer Vision (ICCV), 2017.

[51] Rohan Ghosh and Anupam K Gupta. Scale steerable ﬁlters for locally scale-invariant convolu-

tional neural networks. arXiv preprint arXiv:1906.03861, 2019.

[52] Daniel E Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. In International

Conference on Machine Learning (ICML), 2019.

[53] Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks,

2017.

2019.

[54] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch–Gordan Nets: A Fully Fourier Space
Spherical Convolutional Neural Network. In Conference on Neural Information Processing
Systems (NeurIPS), 2018.

[55] Brandon Anderson, Truong-Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural

networks. arXiv preprint arXiv:1906.04015, 2019.

[56] Nathanaël Perraudin, Michaël Defferrard, Tomasz Kacprzak, and Raphael Sgier. DeepSphere:
Efﬁcient spherical Convolutional Neural Network with HEALPix sampling for cosmological
applications. Astronomy and Computing 27, 2018.

[57] Chiyu Jiang, Jingwei Huang, Karthik Kashinath, Prabhat, Philip Marcus, and Matthias Niessner.
Spherical CNNs on unstructured grids. In International Conference on Learning Representations
(ICLR), 2019.

12

A General facts about Groups and Quotients

Let G be a group and H a subgroup of G. A left coset of H in G is a set gH = {gh | h ∈ H} for
g ∈ G. The cosets form a partition of G. The set of all cosets is called the quotient space or coset
space, and is denoted G/H. There is a canonical projection p : G → G/H that assigns to each
element g the coset it is in. This can be written as p(g) = gH. Fig. 7 provides an illustration for the
group of symmetries of a triangle, and the subgroup H of reﬂections.

The quotient space carries a left action of G, which we denote with ux for u ∈ G and x ∈ G/H.
This works ﬁne because this action is associative with the group operation:

u(gH) = (ug)H.

(15)

for u, g ∈ G. One may verify that this action is well deﬁned, i.e. does not depend on the particular
coset representative g. Furthermore, the action is transitive, meaning that we can reach any coset
from any other coset by transforming it with an appropriate u ∈ G. A space like G/H on which G
acts transitively is called a homogeneous space for G. Indeed, any homogeneous space is isomorphic
to some quotient space G/H.

A section of p is a map s : G/H → G such that p ◦ s = idG/H . We can think of s as choosing a
coset representative for each coset, i.e. s(x) ∈ x. In general, although p is unique, s is not; there can
be many ways to choose coset representatives. However, the constructions we consider will always
be independent of the particular choice of section.

Although it is not strictly necessary, we will assume that s maps the coset H = eH of the identity to
the identity e ∈ G:

(16)
s(H) = e
We can always do this, for given a section s(cid:48) with s(cid:48)(H) = h (cid:54)= e, we can deﬁne the section
1s(cid:48)(H) = h−
1h = e. This is indeed a section,
s(x) = h−
1hx = x (where we used Eq. 15 which can be
for p(s(x)) = p(h−
rewritten as up(g) = p(ug)).

1s(cid:48)(hH) = h−
1p(s(cid:48)(hx)) = h−

1s(cid:48)(hx) so that s(H) = h−

1s(cid:48)(hx)) = h−

One useful rule of calculation is

(gs(x))H = g(s(x)H) = gx = s(gx)H,

for g ∈ G and x ∈ G/H. The projection onto H is necessary, for in general gs(x) (cid:54)= s(gx). These
two terms are however related, through a function h : G/H × G → H, deﬁned as follows:

That is,

gs(x) = s(gx)h(x, g)

h(x, g) = s(gx)−

1gs(x) .

We can think of h(x, g) as the element of H that we can apply to s(gx) (on the right) to get gs(x).
The h function will play an important role in the deﬁnition of the induced representation, and is
illustrated in Fig. 7.

From the ﬁber bundle perspective, we can interpret Eq. 19 as follows. The group G can be viewed as
a principal bundle with base space G/H and ﬁbers gH. If we apply g to the coset representative s(x),
we move to a different coset, namely the one represented by s(gx) (representing a different point
in the base space). Additionally, the ﬁber is twisted by the right action of h(x, g). That is, h(x, g)
moves s(gx) to another element in its coset, namely to gs(x).

The following composition rule for h is very useful in derivations:

h(x, g1g2) = s(g1g2x)−
= [s(g1g2x)−
= h(g2x, g1)h(x, g2)

1g1g2s(x)
1g1s(g2x)][s(g2x)−

1g2s(x)]

For elements h ∈ H, we ﬁnd:

Also, for any coset x,

h(H, h) = s(H)−

1hs(H) = h.

h(H, s(x)) = s(s(x)H)−

1s(x)s(H) = s(H) = e.

13

(17)

(18)

(19)

(20)

(21)

(22)

Figure 7: A Cayley diagram of the group D3 of symmetries of a triangle. The group is generated
by rotations r and ﬂips f . The elements of the group are indicated by hexagons. The red arrows
correspond to right multiplication by r, while the blue lines correspond to right multiplication by f .
Cosets of the group of ﬂips (H = {e, f }) are shaded in gray. As always, the cosets partition the group.
As coset representatives, we choose s(H) = e, s(rH) = r, and s(r2H) = r2f . The difference
between s(rx) and rs(x) is indicated. For this choice of section, we must set h(x, r) = h(rH, r) = f ,
so that s(rx)h(x, r) = (r2f )(f ) = r2 = rs(x).

(23)

(24)

(25)

(26)

(27)

(28)

Using Eq. 20 and 22, this yields,

for any h ∈ H and x ∈ G/H.

For x = H, Eq. 19 specializes to:

where we deﬁned

h(H, s(x)h) = h(hH, s(x))h(H, h) = h,

g = gs(H) = s(gH)h(H, g) ≡ s(gH)h(g),

h(g) = h(H, g) = s(gH)−

1g

This shows that we can always factorize g uniquely into a part s(gH) that represents the coset of g,
and a part h(g) ∈ H that tells us where g is within the coset:

A useful property of h(g) is that for any h ∈ H,

g = s(gH)h(g)

h(gh) = s(ghH)−

1gh = s(gH)−

1gh = h(g)h.

It is also easy to see that

h(s(x)) = e.

When dealing with different subgroups H1 and H2 of G (associated with the input and output space
of an intertwiner), we will write hi for an element of Hi, si : G/Hi → G, for the corresponding
section, and hi : G/Hi × G → Hi for the h-function (for i = 1, 2).

A.1 Double cosets

A (H2, H1)-double coset is a set of the form H2gH1 for H2, H1 subgroups of G. The space of
(H2, H1)-double cosets is called H2\G/H1 ≡ {H2gH1 | g ∈ G}. As with left cosets, we assume a
section γ : H2\G/H1 → G is given, satisfying γ(H2gH1) ∈ H2gH1.
The double coset space H2\G/H1 can be understood as the space of H2-orbits in G/H1, that is,
H2\G/H1 = {H2x|x ∈ G/H1}. Note that although G acts transitively on G/H1 (meaning that
there is only one G-orbit in G/H1), the subgroup H2 does not. Hence, the space G/H1 splits into
a number of disjoint orbits H2x (for x = gH1 ∈ G/H1), and these are precisely the double cosets
H2gH1.
Of course, H2 does act transitively within a single orbit H2x, sending x (cid:55)→ h2x (both of which are in
H2x, for x ∈ G/H1). In general this action is not necessarily ﬁxed point free which means that there

14

may exist some h2 ∈ H2 which map the left cosets to themselves. These are exactly the elements in
the stabilizer of x = gH1, given by

H x

2 = {h ∈ H2 | hx = x}

= {h ∈ H2 | hs1(x)H1 = s1(x)H1}
= {h ∈ H2 | hs1(x) ∈ s1(x)H1}
= {h ∈ H2 | h ∈ s1(x)H1s1(x)−
= s1(x)H1s1(x)−

1 ∩ H2.

1}

Clearly, H x
subgroup s1(x)−

2 is a subgroup of H2. Furthermore, H x
2 s1(x) = H1 ∩ s1(x)−

1H x

2 is conjugate to (and hence isomorphic to) the

1H2s1(x), which is a subgroup of H1.
2 ≡ H γ(x)H1

2

. Like the coset

For double cosets x ∈ H2\G/H1, we will overload the notation to H x
stabilizer, this double coset stabilizer can be expressed as

H x

2 = γ(x)H1γ(x)−

1 ∩ H2

A.2 Semidirect products

For a semidirect product group G, such as SE(2) = R2 (cid:111) SO(2), some things simplify. Let
G = N (cid:111) H where H ≤ G is a subgroup, N ≤ G is a normal subgroup and N ∩ H = {e}. For
every g ∈ G there is a unique way of decomposing it into nh where n ∈ N and h ∈ H. Thus, the
left H coset of g ∈ G depends only on the N part of g:

gH = nhH = nH

(31)

It follows that for a semidirect product group, we can deﬁne the section so that it always outputs an
element of N ⊆ G, instead of a general element of G. Speciﬁcally, we can set s(gH) = s(nhH) =
s(nH) = n. It follows that s(nx) = ns(x) ∀n ∈ N, x ∈ G/H. This allow us to simplify
expressions involving h:

(29)

(30)

(32)

h(x, g) = s(gx)−

1gs(x)

1gs(x)
gH)−

1gs(x)

= s(gs(x)H)−
1
= s(gs(x)g−
(cid:125)

(cid:124)

(cid:123)(cid:122)
N
∈

= (cid:0)gs(x)g−
1 s(gH)(cid:1)−
1g
= s(gH)−
= h(g)

1

gs(x)

A.3 Haar measure

When we integrate over a group G, we will use the Haar measure, which is the essentially unique
measure dg that is invariant in the following sense:

(cid:90)

G

(cid:90)

G

f (g)dg =

f (ug)dg ∀u ∈ G.

(33)

Such measures always exist for locally compact groups, thus covering most cases of interest [35].
For discrete groups, the Haar measure is the counting measure, and integration can be understood as
a discrete sum.

We can integrate over G/H by using an integral over G,

f (x)dx =

f (gH)dg.

(34)

(cid:90)

G/H

(cid:90)

G

15

B Proofs

B.1 Bi-equivariance of one-argument kernels on G

B.1.1 Left equivariance of κ

We want the result κ (cid:63) f (or κ · f ) to live in I 2
Mackey condition,

G, which means that this function has to satisfy the

(cid:90)

G

⇔

⇔
⇔

[κ (cid:63) f ](gh2) = ρ2(h−

2 )[κ (cid:63) f ](g)

1

κ((gh2)−

1g(cid:48))f (g(cid:48))dg(cid:48) = ρ2(h−

1
2 )

κ(g−

1g(cid:48))f (g(cid:48))dg(cid:48)

(35)

(cid:90)

G

1
2 g−
κ(h−

1g(cid:48)) = ρ2(h−
κ(h2g) = ρ2(h2)κ(g)

2 )κ(g−

1

1g(cid:48))

for all h2 ∈ H2 and g ∈ G.

B.1.2 Right equivariance of κ

The fact that f ∈ I 1
G satisﬁes the Mackey condition (f (gh) = ρ1(h)f (g) for h ∈ H1) implies a
symmetry in the correlation κ (cid:63) f . That is, if we apply a right-H1-shift to the kernel, i.e. [Rhκ](g) =
κ(gh), we ﬁnd that

[[Rhκ] (cid:63) f ](g) =

κ(g−

1uh)f (u)du

(cid:90)

G

(cid:90)

G

(cid:90)

G

=

=

κ(g−

1u)f (uh−

1)du

κ(g−

1u)ρ1(h)f (u)du.

It follows that we can take (for h ∈ H1),

κ(gh) = κ(g)ρ1(h).

B.2 Kernels on H2\G/H1

We have seen the space KC of H2-equivariant kernels on G/H1 appear in our analysis of both IG
and IC. Kernels in this space have to satisfy the constraint (for h ∈ H2):

Here we will show that this space is equivalent to the space

←−κ (hy) = ρ2(h)←−κ (y)ρ1(h1(y, h)−

1)

KD = {¯κ : H2\G/H1 → Hom(V1, V2) | ¯κ(x) = ρ2(h)¯κ(x)ρx

1 (h)−

1,

∀x ∈ H2\G/H1, h ∈ H γ(x)H1

},

2

1 of the stabilizer H γ(x)H1
where we deﬁned the representation ρx
ρx
1 (h) = ρ1(h1(γ(x)H1, h))
1hγ(x)),

2

,

= ρ1(γ(x)−
with the section γ : H2\G/H1 → G being deﬁned as in section A.1. To show the equivalence of KC
and KD, we deﬁne an ismorphism Ω

: KD → KC. We begin by deﬁning Ω−
K

1

:

K
¯κ(x) = [Ω−
K

1

←−κ ](x) = ←−κ (γ(x)H1).

We verify that for ←−κ ∈ KC we have ¯κ ∈ KD. Let h ∈ H γ(x)H1

, then

2

(36)

(37)

(38)

(39)

(40)

(41)

(42)

¯κ(x) = ←−κ (γ(x)H1)
= ←−κ (hγ(x)H1)
= ρ2(h)←−κ (γ(x)H1)ρ1(h1(γ(x)H1, h))−
= ρ2(h)¯κ(x)ρx

1 (h)−

1

1

16

To deﬁne Ω
may not be unique, because H2 does not in general act freely on G/H1.

, we use the decomposition y = hγ(H2y)H1 for y ∈ G/H1 and h ∈ H2. Note that h

K

←−κ (y) = [Ω

K

K

¯κ](y) = [Ω

¯κ](hγ(H2y)H1) = ρ2(h)¯κ(H2y)ρ1(h1(γ(H2y)H1, h))−

1.

(43)

We verify that for ¯κ ∈ KD we have ←−κ ∈ KC.
←−κ (h(cid:48)y) = ←−κ (h(cid:48)hγ(H2y)H1)

1

= ρ2(h(cid:48)h)¯κ(H2y)ρ1(h1(γ(H2y)H1, h(cid:48)h))−
= ρ2(h(cid:48)h)¯κ(H2y)ρ1(h1(hγ(H2y)H1, h(cid:48))h1(γ(H2y)H1, h))−
= ρ2(h(cid:48))ρ2(h)¯κ(H2y)ρ1(h1(γ(H2y)H1, h))−
= ρ2(h(cid:48))ρ2(h)¯κ(H2y)ρ1(h1(γ(H2y)H1, h))−
= ρ2(h(cid:48))←−κ (y)ρ1(h1(y, h(cid:48)))−

1

1

1ρ1(h1(hγ(H2y)H1, h(cid:48)))−
1ρ1(h1(y, h(cid:48)))−

1

1

We verify that Ω

1

are indeed inverses:

K
[Ω

and Ω−
K
1

[Ω−
K

K

←−κ ]](y) = [Ω

1

1

[Ω−
K

←−κ ]](hγ(H2y)H1)
K
←−κ ](H2y)ρ1(h1(γ(H2y)H1, h))−
= ρ2(h)[Ω−
K
= ρ2(h)←−κ (γ(H2y)H1)ρ1(h1(γ(H2y)H1, h))−
= ←−κ (hγ(H2y)H1)
= ←−κ (y).

1

1

In the other direction,

1
[Ω−
K

[Ω

K

¯κ]](x) = [Ω
= [Ω

¯κ](γ(x)H1)
¯κ](γ(H2γ(x)H1)H1)

K

K

1
= ρ2(e)¯κ(H2γ(x)H1)ρ1(h1(γ(H2γ(x)H1)H1, e))−
= ¯κ(x)

C Limitations of the Theory

(44)

(45)

(46)

The theory presented here is quite general but still has several limitations. Firstly, we only cover
ﬁelds over homogeneous spaces. Although ﬁelds can be deﬁned over more general manifolds,
and indeed there has been some effort aimed at deﬁning convolutional networks on general (or
Riemannian) manifolds [44], we restrict our attention to homogeneous spaces because they come
naturally equipped with a group action to which the network can be made equivariant. A more general
theory would not be able to make use of this additional structure.

For reasons of mathematical elegance and simplicity, the theory idealizes feature maps as ﬁelds over
a possibly continuous base space, but a computer implementation will usually involve discretizing
this space. A similar approach is used in signal processing, where discretization is justiﬁed by various
sampling theorems and band-limit assumptions. It seems likely that a similar theory can be developed
for deep networks, but this has not been done yet.

17

D Classiﬁcation of Equivariant CNNs

G
Z2
p4, p4m
"
p4, p4m
p6, p6m
Z3 (cid:111) H
Z3 (cid:111) H
R2 (cid:111) CN
"
"
"
SE(2)

H
{1}
C4, D4
"
C4, D4
C6, D6
D4, D4h, O, Oh
V, T4, O
CN
"
"
"
SO(2)

G/H
Z2
Z2
"
Z2
Z2
Z3
Z3
R2
"
"
"
R2
R2
R2
"
"
R3
"
"
"
"
S2
"
"
"
G/H
Table 1: A taxonomy of G-CNNs. Methods are classiﬁed by the group G they are equivariant to, the
subgroup H that acts on the ﬁbers, the base space G/H to which the ﬁbers are attached (implied by
G and H), and the type of ﬁeld ρ (regular, irreducible or trivial).

Reference
Lecun 1990 [45]
Cohen 2016 [1],
Dieleman 2016 [46]
Cohen 2017 [2]
Hoogeboom 2018 [47]
Winkels 2018 [14]
Worrall 2018 [15]
Weiler 2017 [4]
Zhou 2017 [48]
Bekkers 2018 [49]
Marcos 2017 [50]
Worrall 2017 [3]
Weiler 2019 [7]
Ghosh 2019 [51]
Worrall 2019 [52]
Sosnovik 2019 [53]
Kondor 2018 [8]
Thomas 2018 [5]
Weiler 2018 [6]
Kondor 2018 [54]
Anderson 2019 [55]
Cohen 2018 [10]
Esteves 2018 [42]
Perraudin 2018 [56]
Jiang 2019 [57]
Kondor 2018 [12]

R2 (cid:111) H ≤ E(2) O(2), SO(2), CN , DN
R2 (cid:111) (R+, ∗)
"
"
SE(3)
"
"
"
"
SO(3)
"
"
"
G

ρ
regular
regular
"
irrep & regular
regular
regular
regular
regular
"
"
irrep & regular
irrep
any representation
regular & trivial
regular
"
irrep
irrep & regular
irrep
irrep
irrep
regular
trivial
"
irrep
trivial

(R+, ∗)
"
"
SO(3)
"
"
"
"
SO(2)
"
"
"
H

18

