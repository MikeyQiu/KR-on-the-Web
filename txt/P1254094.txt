LiteFlowNet: A Lightweight Convolutional Neural Network
for Optical Flow Estimation

Tak-Wai Hui, Xiaoou Tang, Chen Change Loy
CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong
{twhui,xtang,ccloy}@ie.cuhk.edu.hk

8
1
0
2
 
y
a
M
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
6
3
0
7
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

FlowNet2 [14], the state-of-the-art convolutional neural
network (CNN) for optical ﬂow estimation, requires over
160M parameters to achieve accurate ﬂow estimation. In
this paper we present an alternative network that outper-
forms FlowNet2 on the challenging Sintel ﬁnal pass and
KITTI benchmarks, while being 30 times smaller in the
model size and 1.36 times faster in the running speed. This
is made possible by drilling down to architectural details
that might have been missed in the current frameworks: (1)
We present a more effective ﬂow inference approach at each
pyramid level through a lightweight cascaded network. It
not only improves ﬂow estimation accuracy through early
correction, but also permits seamless incorporation of de-
scriptor matching in our network. (2) We present a novel
ﬂow regularization layer to ameliorate the issue of outliers
and vague ﬂow boundaries by using a feature-driven lo-
cal convolution. (3) Our network owns an effective struc-
ture for pyramidal feature extraction and embraces fea-
ture warping rather than image warping as practiced in
FlowNet2. Our code and trained models are available at
https://github.com/twhui/LiteFlowNet.

1. Introduction

Optical ﬂow estimation is a long-standing problem in
computer vision. Due to the well-known aperture problem,
optical ﬂow is not directly measurable [12, 13]. Hence, the
estimation is typically solved by energy minimization in a
coarse-to-ﬁne framework [6, 20, 7, 36, 27, 22]. This class
of techniques, however, involves complex energy optimiza-
tion and thus it is not scalable for applications that demand
real-time estimation.

FlowNet [9] and its successor FlowNet2 [14], have
marked a milestone by using CNN for optical ﬂow esti-
mation. Their accuracies especially the successor are ap-
proaching that of state-of-the-art energy minimization ap-
proaches, while the speed is several orders of magnitude

Figure 1: Examples demonstrate the effectiveness of the proposed
components in LiteFlowNet for i) feature warping, ii) cascaded
ﬂow inference, and iii) ﬂow regularization. Enabled components
are indicated with bold black fonts.

faster. To push the envelop of accuracy, FlowNet2 is de-
signed as a cascade of variants of FlowNet that each net-
work in the cascade reﬁnes the preceding ﬂow ﬁeld by con-
tributing on the ﬂow increment between the ﬁrst image and
the warped second image. The model, as a result, com-
prises over 160M parameters, which could be formidable in
many applications. A recent network termed SPyNet [21]
attempts a network with smaller size of 1.2M parameters
by adopting image warping in each pyramid level. Nonethe-
less, the accuracy can only match that of FlowNet but not
FlowNet2. The objective of this study is to explore alter-
native CNN architectures for accurate ﬂow estimation yet
with high efﬁciency. Our work is inspired by the successes
of FlowNet2 and SPyNet, but we further drill down the key
elements to fully unleash the potential of deep convolutional
network combined with classical principles.

There are two general principles to improve the design
of FlowNet2 and SPyNet. The ﬁrst principle is pyrami-
dal feature extraction. The proposed network, dubbed Lite-
FlowNet, consists of an encoder and a decoder. The encoder
maps the given image pair, respectively, into two pyramids

1

of multi-scale high-dimensional features. The decoder then
estimates the ﬂow ﬁeld in a coarse-to-ﬁne framework. At
each pyramid level, the decoder infers the ﬂow ﬁeld by se-
lecting and using the features of the same resolution from
the feature pyramids. This design leads to a lighter network
compared to FlowNet2 that adopts U-Net architecture [23]
for ﬂow inference. In comparison to SPyNet, our network
separates the process of feature extraction and ﬂow estima-
tion. This helps us to better pinpoint the bottleneck of accu-
racy and model size.

The second general principle is feature warping.
FlowNet2 and SPyNet warp the second image towards the
ﬁrst image in the pair using the previous ﬂow estimate, and
then reﬁne the estimate using the feature maps generated by
the warped and the ﬁrst images. Warping an image and then
generating the feature maps of the warped image are two
ordered steps. We ﬁnd that the two steps can be reduced to
a single one by directly warping the feature maps of the sec-
ond image, which have been computed by the encoder. This
one-step feature warping process reduces the more discrimi-
native feature-space distance instead of the RGB-space dis-
tance between the two images. This makes our network
more powerful and efﬁcient in addressing the ﬂow problem.
We now highlight the more speciﬁc differences between
our network and existing CNN-based optical ﬂow estima-
tion frameworks:
1) Cascaded ﬂow inference – At each pyramid level, we in-
troduce a novel cascade of two lightweight networks. Each
of them has a feature warping (f-warp) layer to displace
the feature maps of the second image towards the ﬁrst im-
age using the ﬂow estimate from the previous level. Flow
residue is computed to further reduce the feature-space dis-
tance between the images. This design is advantageous to
the conventional design of using a single network for ﬂow
inference. First, the cascade progressively improves ﬂow
accuracy thus allowing an early correction of the estimate
without passing more errors to the next level. Second, this
design allows seamless integration with descriptor match-
ing. We assign a matching network to the ﬁrst inference.
Consequently, pixel-accuracy ﬂow ﬁeld can be generated
ﬁrst and then reﬁned to sub-pixel accuracy in the subse-
quent inference network. Since at each pyramid level the
feature-space distance between the images has been reduced
by feature warping, we can use a rather short displace-
ment than [9, 14] to establish the cost volume. Besides,
matching is performed only at sampled positions and thus a
sparse cost-volume is aggregated. This effectively reduces
the computational burden raised by the explicit matching.
2) Flow regularization – The cascaded ﬂow inference re-
sembles the role of data ﬁdelity in energy minimization
methods. Using data term alone, vague ﬂow boundaries
and undesired artifacts exist in ﬂow ﬁelds. To tackle this
problem, local ﬂow consistency and co-occurrence between

ﬂow boundaries and intensity edges are commonly used as
the cues to regularize ﬂow ﬁeld. Some of the representative
methods include anisotropic image-driven [32], image- and
ﬂow-driven [28], and complementary [36] regularizations.
After cascaded ﬂow inference, we allow the ﬂow ﬁeld to be
further regularized by our novel feature-driven local convo-
lution (f-lconv) layer1 at each pyramid level. The kernels of
such a local convolution are adaptive to the pyramidal fea-
tures from the encoder, ﬂow estimate and occlusion prob-
ability map. This makes the ﬂow regularization to be both
ﬂow- and image-aware. To our best knowledge, state-of-
the-art CNNs do not explore such a ﬂow regularization.

The effectiveness of the aforementioned contributions
are depicted in Figure 1. In summary, we propose a compact
LiteFlowNet to estimate optical ﬂow. Our network inno-
vates the useful elements from conventional methods. e.g.,
brightness constraint in data ﬁdelity to pyramidal CNN fea-
tures and image warping to CNN feature warping. More
speciﬁcally, we present a cascaded ﬂow inference with fea-
ture warping and ﬂow regularization in each pyramid level,
which are new in the literature. Overall, our network out-
performs FlowNet [9] and SPyNet [21] and is on par with
or outperforms the recent FlowNet2 [14] on public bench-
marks, while having 30 times fewer parameters and being
1.36 times faster than FlowNet2.

2. Related Work

Here, we brieﬂy review some of the major approaches

for optical ﬂow estimation.

Variational methods. Since the pioneering work by Horn
and Schunck [12], variational methods have dominated op-
tical ﬂow estimation. Brox et al. address illumination
changes by combining the brightness and gradient con-
stancy assumptions [6]. Brox et al. integrate rich descriptors
into variational formulation [7]. In DeepFlow [31], Weinza-
epfel et al. propose to correlate multi-scale patches and in-
corporate this as the matching term in functional. In Patch-
Match Filter [16], Lu et al. establish dense correspondence
using the superpixel-based PatchMatch [4]. Revaud et al.
propose a method EpicFlow that uses externally matched
ﬂows as initialization and then performs interpolation [22].
Zimmer et al. design the complementary regularization that
exploits directional information from the constraints im-
posed in data term [36]. Our network that infers optical
ﬂow and performs ﬂow regularization is inspired by the use
of data ﬁdelity and regularization in variational methods.

Machine learning methods. Black et al. propose to repre-
sent complex image motion as a linear combination of the
learned basis vectors [5]. Roth et al. formulates the prior

1We name it as feature-driven local convolution (f-lconv) layer in order
to distinguish it from local convolution (lconv) layer of which ﬁlter weights
are locally ﬁxed in conventional CNNs [29].

2

probability of ﬂow ﬁeld as Field-of-Experts model [26] that
captures higher order spatial statistics [25]. Sun et al. study
the probabilistic model of brightness inconstancy in a high-
order random ﬁeld framework [28]. Nir et al. represent
image motion using the over-parameterization model [19].
Rosenbaum et al. model the local statistics of optical ﬂow
using Gaussian mixtures [24]. Given a set of sparse
matches, Wulff et al. propose to regress them to a dense
ﬂow ﬁeld using a set of basis ﬂow ﬁelds (PCA-Flow) [33].
It can be shown that the parameterized model [5, 19, 33]
can be efﬁciently implemented using CNN.
CNN-based methods. In the work of Fischer et al. termed
FlowNet [9], a post-processing step that involves energy
minimization is required to reduce smoothing effect across
ﬂow boundaries. This process is not end-to-end trainable.
In our work, we present an end-to-end approach that per-
forms in-network ﬂow regularization using the proposed
f-lconv layer, which plays similar role as the regulariza-
tion term in variational methods.
In FlowNet2 [14], Ilg
et al. introduce a huge network cascade (over 160M pa-
rameters) that consists of variants of FlowNet. The cas-
cade improves ﬂow accuracy with an expense of model size
and computational complexity. Our model uses a more ef-
ﬁcient architecture containing 30 times fewer parameters
than FlowNet2 while the performance is on par with it. A
compact network termed SPyNet [21] from Ranjan et al. is
inspired from spatial pyramid. Nevertheless, the accuracy is
far below FlowNet2. A small-sized variant of our network
outperforms SPyNet while being 1.33 times smaller in the
model size. Zweig et al. present a network to interpolate
third-party sparse ﬂows but requiring off-the-shelf edge de-
tector [37]. DeepFlow [31] that involves convolution and
pooling operations is however not a CNN, since the “ﬁlter
weights” are non-trainable image patches. According to the
terminology used in FlowNet, DeepFlow uses correlation.

An alternative approach for establishing point correspon-
dence is to match image patches. Zagoruyko et al. ﬁrst in-
troduce to CNN-feature matching [35]. G¨uney et al. ﬁnd
feature representation and formulate optical ﬂow estimation
in MRF [11]. Bailer et al. [2] use multi-scale features and
then perform feature matching as Flow Fields [1]. Although
pixel-wise matching can establish accurate point correspon-
dence, the computational demand is too high for practical
use (it takes several seconds even a GPU is used). As a
tradeoff, Fischer et al. [9] and Ilg et al. [14] perform feature
matching only at a reduced spatial resolution. We reduce
the computational burden of feature matching by using a
short-ranged matching of warped CNN features at sampled
positions and a sub-pixel reﬁnement at every pyramid level.
We are inspired by the feature transformation used in
Spatial Transformer [15]. Our network uses the proposed
f-warp layer to displace each channel2 of the given vector-

2We can also use f-warp layer to displace each channel differently when

valued feature according to the provided ﬂow ﬁeld. Unlike
Spatial Transformer, f-warp layer is not fully constrained
and is a relaxed version of it as the ﬂow ﬁeld is not param-
eterized. While transformation in FlowNet2 and SPyNet is
limited to images, our decider network is a more generic
warping network that warps high-level CNN features.

3. LiteFlowNet

LiteFlowNet is composed of two compact sub-networks
that are specialized in pyramidal feature extraction and op-
tical ﬂow estimation as shown in Figure 2. Since the spatial
dimension of feature maps is contracting in feature extrac-
tion and that of ﬂow ﬁelds is expanding in ﬂow estimation,
we call the two sub-networks as NetC and NetE respec-
tively. NetC transforms any given image pair into two pyra-
mids of multi-scale high-dimensional features. NetE con-
sists of cascaded ﬂow inference and regularization modules
that estimate coarse-to-ﬁne ﬂow ﬁelds.

Pyramidal Feature Extraction. As shown in Figure 2,
NetC is a two-stream network in which the ﬁlter weights
are shared across the two streams. Each of them functions
as a feature descriptor that transforms an image I to a pyra-
mid of multi-scale high-dimensional features {Fk(I)} from
the highest spatial resolution (k = 1) to the lowest spatial
resolution (k = L). The pyramidal features are generated
by stride-s convolutions with the reduction of spatial reso-
lution by a factor s up the pyramid. In the following, we
omit the subscript k that indicates the level of pyramid for
brevity. We use Fi to represent CNN features for Ii. When
we discuss the operations in a pyramid level, the same op-
erations are applicable to other levels.

Feature Warping. At each pyramid level, a ﬂow ﬁeld is
inferred from high-level features F1 and F2 of images I1
and I2. Flow inference becomes more challenging if I1 and
I2 are captured far away from each other. With the motiva-
tion of image warping used in conventional methods [6, 20]
and recent CNNs [14, 21] for addressing large-displacement
ﬂow, we propose to reduce feature-space distance between
F1 and F2 by feature warping (f-warp). Speciﬁcally, F2
is warped towards F1 by f-warp via ﬂow estimate ˙x to
(cid:101)F2(x) (cid:44) F2(x + ˙x) ∼ F1(x). This allows our network to
infer residual ﬂow between F1 and (cid:101)F2 that has smaller ﬂow
magnitude (more details in Section 3.1) but not the com-
plete ﬂow ﬁeld that is more difﬁcult to infer. Unlike con-
ventional methods, f-warp is performed on high-level CNN
features but not on images. This makes our network more
powerful and efﬁcient in addressing the optical ﬂow prob-
lem. To allow end-to-end training, F is interpolated to (cid:101)F

multiple ﬂow ﬁelds are supplied. The usage, however, is beyond the scope
of this work.

3

Figure 2: The network structure of LiteFlowNet. For the ease of representation, only a 3-level design is shown. Given an image pair (I1
and I2), NetC generates two pyramids of high-level features (Fk(I1) in pink and Fk(I2) in red, k ∈ [1, 3]). NetE yields multi-scale ﬂow
ﬁelds that each of them is generated by a cascaded ﬂow inference module M :S (in blue color, including a descriptor matching unit M and
a sub-pixel reﬁnement unit S) and a regularization module R (in green color). Flow inference and regularization modules correspond to
data ﬁdelity and regularization terms in conventional energy minimization methods respectively.

for any sub-pixel displacement ˙x as follows:

(cid:101)F(x) =

(cid:88)

F(xi

s) (cid:0)1 − (cid:12)

(cid:12)xs − xi
s

(cid:12)
(cid:12)

(cid:1) (cid:0)1 − (cid:12)

(cid:12)ys − yi
s

(cid:1) ,

(cid:12)
(cid:12)

xi

s∈N (xs)

(1)
where xs = x + ˙x = (xs, ys)(cid:62) denotes the source coor-
dinates in the input feature map F that deﬁnes the sample
point, x = (x, y)(cid:62) denotes the target coordinates of the
regular grid in the interpolated feature map (cid:101)F, and N (xs)
denotes the four pixel neighbors of xs. The above bilinear
interpolation allows back-propagation during training as its
gradients can be efﬁciently computed [15].

3.1. Cascaded Flow Inference

At each pyramid level of NetE, pixel-by-pixel matching
of high-level features yields coarse ﬂow estimate. A subse-
quent reﬁnement on the coarse ﬂow further improves it to
sub-pixel accuracy.
First Flow Inference (descriptor matching). Point cor-
respondence between I1 and I2 is established through com-
puting correlation of high-level feature vectors in individual
pyramidal features F1 and F2 as follows:

c(x, d) = F1(x) · F2(x + d)/N,

(2)

where c is the matching cost between point x in F1 and
point x + d in F2, d ∈ Z is the displacement vector from x,
and N is the length of the feature vector. A cost volume C
is built by aggregating all the matching costs into a 3D grid.
We reduce the computational burden raised by cost-
volume processing [9, 14] in three ways: 1) We perform
short-range matching at every pyramid level instead of long-
range matching at a single level. 2) We reduce feature-space

distance between F1 and F2 by warping F2 towards F1
using our proposed f-warp through ﬂow estimate3 ˙x from
previous level. 3) We perform matching only at the sam-
pled positions in the pyramid levels of high-spatial resolu-
tion. The sparse cost volume is interpolated in the spatial
dimension to ﬁll the missed matching costs for the unsam-
pled positions. The ﬁrst two techniques effectively reduce
the searching space needed, while the last technique reduces
the frequency of matching per pyramid level.

In the descriptor matching unit M , residual ﬂow ∆ ˙xm
is inferred by ﬁltering the cost volume C as illustrated in
Figure 3. A complete ﬂow ﬁeld ˙xm is computed as follows:

˙xm = M (cid:0)C(F1, (cid:101)F2; d)(cid:1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
∆ ˙xm

+s ˙x↑s.

(3)

Second Flow Inference (sub-pixel reﬁnement). Since
the cost volume in descriptor matching unit is aggregated
by measuring pixel-by-pixel correlation, ﬂow estimate ˙xm
from the previous inference is only up to pixel-level accu-
racy. We introduce the second ﬂow inference in the wake
of descriptor matching as shown in Figure 3.
It aims to
reﬁne the pixel-level ﬂow ﬁeld ˙xm to sub-pixel accuracy.
This prevents erroneous ﬂows being ampliﬁed by upsam-
pling and passing to the next pyramid level. Speciﬁcally,
F2 is warped to (cid:101)F2 via ﬂow estimate ˙xm. Sub-pixel reﬁne-
ment unit S yields a more accurate ﬂow ﬁeld ˙xs by mini-
mizing feature-space distance between F1 and (cid:101)F2 through

3 ˙x from previous level needs to be upsampled in spatial resolution (de-
noted by “↑s”) and magnitude (multiplied by a scalar s) to s ˙x↑s for match-
ing the spatial resolution of the pyramidal features at the current level.

4

Figure 4: Folding and packing of f-lcon ﬁlters {g}. The (x, y)-
entry of 3D tensor ¯G(c) is a 3D column with size 1 × 1 × w2.
It corresponds to the unfolded w × w f-lcon ﬁlter g(x, y, c) to be
applied at position (x, y) of channel c in vector-valued feature F .

ﬂow inference, we replace F to ˙xs. Flow regularization
module R is deﬁned as follows:

˙xr = R( ˙xs; G).

(6)

The f-lcon ﬁlters need to be specialized for smoothing
ﬂow ﬁeld. It should behave as an averaging ﬁlter if the vari-
ation of ﬂow vectors over the patch is smooth. It should also
not over-smooth ﬂow ﬁeld across ﬂow boundary. We deﬁne
a feature-driven CNN distance metric D that estimates lo-
cal ﬂow variation using pyramidal feature F1, ﬂow ﬁeld ˙xs
from the cascaded ﬂow inference, and occlusion probabil-
ity map4 O. In summary, D is adaptively constructed by a
CNN unit RD as follows:

D = RD(F1, ˙xs, O).

(7)

With the introduction of feature-driven distance metric D,
each ﬁlter g of f-lcon is constructed as follows:

g(x, y, c) =

exp(−D(x, y, c)2)
(xi,yi)∈N (x,y) exp(−D(xi, yi, c)2)

,

(cid:80)

(8)

where N (x, y) denotes the neighborhood containing ω × ω
pixels centered at position (x, y).

Here, we provide a mechanism to perform f-lcon ef-
ﬁciently. For a C-channel input F , we use C tensors
¯G(1), ..., ¯G(C) to store f-lcon ﬁlter set G. As illustrated
in Figure 4, each f-lcon ﬁlter g(x, y, c) is folded into a
1 × 1 × w2 3D column and then packed into the (x, y)-
entry of a M × N × w2 3D tensor ¯G(c). Same folding and
packing operations are also applied to each patch in each
channel of F . This results C tensors ¯F (1), ..., ¯F (C) for F .
In this way, Equation (5) can be reformulated to:

Fg(c) = ¯G(c) (cid:12) ¯F (c),

(9)

where “(cid:12)” denotes element-wise dot product between the
corresponding columns of the tensors. With the abuse of

4We use the brightness error ||I2(x+ ˙x)−I1(x)||2 between the warped

second image and the ﬁrst image as the occlusion probability map.

Figure 3: A cascaded ﬂow inference module M :S in NetE. It con-
sists of a descriptor matching unit M and a sub-pixel reﬁnement
unit S. In M , f-warp transforms high-level feature F2 to (cid:101)F2 via
upscaled ﬂow ﬁeld 2 ˙x↑2 estimated at previous pyramid level. In
S, F2 is warped by ˙xm from M . In comparison to residual ﬂow
∆ ˙xm, more ﬂow adjustment exists at ﬂow boundaries in ∆ ˙xs.

computing residual ﬂow ∆ ˙xs as the following:

˙xs = S(cid:0)F1, (cid:101)F2, ˙xm

+ ˙xm.

(4)

(cid:1)

(cid:125)

(cid:124)

(cid:123)(cid:122)
∆ ˙xs

3.2. Flow Regularization

Cascaded ﬂow inference resembles the role of data ﬁ-
delity in conventional minimization methods. Using data
term alone, vague ﬂow boundaries and undesired artifacts
commonly exist in ﬂow ﬁeld [32, 36]. To tackle this prob-
lem, we propose to use a feature-driven local convolution
(f-lcon) to regularize ﬂow ﬁeld from the cascaded ﬂow in-
ference. The operation of f-lcon is well-governed by the
Laplacian formulation of diffusion of pixel values [30]. In
contrast to local convolution (lcon) used in conventional
CNNs [29], f-lcon is more generalized. Not only is a dis-
tinct ﬁlter used for each position of feature map, but the
ﬁlter is adaptively constructed for individual ﬂow patches.
Consider a general case, a vector-valued feature F that
has to be regularized has C channels and a spatial dimen-
sion M × N . Deﬁne G = {g} as the set of ﬁlters used in
f-lcon layer. The operation of f-lcon to F can be formulated
as follow:

fg(x, y, c) = g(x, y, c) ∗ f (x, y, c),

(5)

where “∗” denotes convolution, f (x, y, c) is a w × w patch
centered at position (x, y) of channel c in F , g(x, y, c) is the
corresponding w × w regularization ﬁlter, and fg(x, y, c) is
a scalar output for x = (x, y)(cid:62) and c = 1, 2, ..., C. To
be speciﬁc for regularizing ﬂow ﬁeld ˙xs from the cascaded

5

Table 1: AEE on the Chairs testing set. Models are trained on the
Chairs training set.

FlowNetS

FlowNetC

SPyNet

LiteFlowNetX-pre

LiteFlowNet-pre

2.71

2.19

2.63

2.25

1.57

notation, Fg(c) means the c-th xy-slice of the regularized
C-channel feature Fg. Equation (9) reduces the dimension
of tensors from M × N × w2 (right-hand side in prior to the
dot product) to M × N (left-hand side).

4. Experiments

Network Details. In LiteFlowNet, NetC generates 6-level
pyramidal features and NetE predicts ﬂow ﬁelds for levels
6 to 2. Flow ﬁeld in level 2 is upsampled to yield ﬂow ﬁeld
in level 1. We set the maximum searching radius in cost-
volume to 3 pixels (levels 6 to 4) or 6 pixels (levels 3 to 2).
Matching is performed at each position in pyramidal fea-
tures, except for levels 3 to 2 that it is performed at a regu-
larly sampled grid (a stride of 2). All convolution layers use
3 × 3 ﬁlters, except each last layer in descriptor matching
M , sub-pixel reﬁnement S, and ﬂow regularization R units
uses 5×5 (levels 4 to 3) or 7×7 (level 2) ﬁlters. Each convo-
lution layer is followed by a leaky rectiﬁed linear unit layer,
except f-lcon and the last layer in M , S and R CNN units.
More details can be found in the supplementary material.
Training Details. We train our network stage-wise by the
following steps: 1) NetC and M6:S6 of NetE is trained
for 300k iterations. 2) R6 together with the trained net-
work in step 1 is trained for 300k iterations. 3) For lev-
els k ∈ [5, 2], Mk:Sk followed by Rk is added into the
trained network each time. The new network cascade is
trained for 200k (level 2: 300k) iterations. Filter weights
are initialized from previous level. Learning rates are ini-
tially set to 1e-4, 5e-5, and 4e-5 for levels 6 to 4, 3 and
2 respectively. We reduce it by a factor of 2 starting at
120k, 160k, 200k, and 240k iterations. We use the same
loss weight, L2 training loss, Adam optimization, data aug-
mentation (including noise injection), and training sched-
ule 5 (Chairs [9] → Things3D [17]) as FlowNet2 [14]. We
denote LiteFlowNet-pre and LiteFlowNet as the networks
trained on Chairs and Chairs → Things3D, respectively.

4.1. Results

We compare several variants of LiteFlowNet to state-
of-the-art methods on public benchmarks including Fly-
ingChairs
[8],
KITTI12 [10], KITTI15 [18], and Middlebury [3].
FlyingChairs. We ﬁrst compare the intermediate results of

[9], Sintel clean and ﬁnal

(Chairs)

5We excluded a small amount of training data in Things3D undergoing
extremely large ﬂow displacement as advised by the authors (https://
github.com/lmb-freiburg/flownet2/issues).

different well-performing networks trained on Chairs alone
in Table 1. Average end-point error (AEE) is reported.
LiteFlowNet-pre outperforms the compared networks. No
intermediate result is available for FlowNet2 [14] as each
cascade is trained on the Chairs → Things3D sched-
ule individually. Since FlowNetC, FlowNetS (variants of
FlowNet [9]), and SPyNet [21] have fewer parameters than
FlowNet2 and the later two models do not perform fea-
ture matching, we also construct a small-size counterpart
LiteFlowNetX-pre by removing the matching part and
shrinking the model sizes of NetC and NetE by about 4 and
5 times, respectively. Despite that LiteFlowNetX-pre is 43
and 1.33 times smaller than FlowNetC and SPyNet, respec-
tively, it still outperforms these networks and is on par with
FlowNetC that uses explicit matching.

MPI Sintel.
In Table 2, LiteFlowNetX-pre outperforms
FlowNetS (and C) [9] and SPyNet [21] that are trained
on Chairs on all cases except the Middlebury benchmark.
LiteFlowNet, trained on the Chairs → Things3D sched-
ule, performs better than LiteFlowNet-pre as expected.
LiteFlowNet also outperforms SPyNet, FlowNet2-S (and -
C) [14]. We also ﬁne-tuned LiteFlowNet on a mixture of
Sintel clean and ﬁnal training data (LiteFlowNet-ft) using
the generalized Charbonnier loss [27]. No noise augmen-
tation was performed but we introduced image mirroring
to improve the diversity of the training set. LiteFlowNet-
ft outperforms FlowNet2-ft-sintel [14] and EpicFlow [22]
for Sintel ﬁnal testing set. Despite DC Flow [34] (a hy-
brid method consists of CNN and post-processing) per-
forms better than LiteFlowNet, its GPU runtime requires
several seconds that makes it formidable in many applica-
tions. Figure 5 shows some examples of ﬂow ﬁelds on Sin-
tel dataset. LiteFlowNet-ft and FlowNet2-ft-sintel perform
the best among the compared methods. As LiteFlowNet has
ﬂow regularization module, sharper ﬂow boundaries and
lesser artifacts can be observed in the generated ﬂow ﬁelds.

KITTI. LiteFlowNet consistently performs better than
LiteFlowNet-pre especially on KITTI15 as shown in Ta-
ble 2.
It also outperforms SPyNet [21] and FlowNet2-S
(and C) [14]. We also ﬁne-tuned LiteFlowNet on a mix-
ture of KITTI12 and KITTI15 training data (LiteFlowNet-
ft) using the same augmentation as the case of Sintel ex-
cept that we reduced the amount of augmentation for spa-
tial motion to ﬁt the driving scene. After ﬁne-tuning, Lite-
FlowNet generalizes well to real-world data. LiteFlowNet-
ft outperforms FlowNet2-ft-kitti [14].
Figure 6 shows
some examples of ﬂow ﬁelds on KITTI. As in the case
for Sintel, LiteFlowNet-ft and FlowNet2-ft-kitti performs
the best among the compared methods. Even though
LiteFlowNet and its variants perform pyramidal descrip-
tor matching in a limited searching range, it yields reliable
large-displacement ﬂow ﬁelds for real-world data due to the
feature warping (f-warp) layer introduced. More analysis

6

Table 2: AEE of different methods. The values in parentheses are the results of the networks on the data they were trained on, and hence are
not directly comparable to the others. Fl-all: Percentage of outliers averaged over all pixels. Inliers are deﬁned as EPE <3 pixels or <5%.
The best number for each category is highlighted in bold. (Note: 1The values are reported from [14]. 2We re-trained the model using the
code provided by the authors. 3,4,5The values are computed using the trained models provided by the authors. 4Large discrepancy exists as
the authors mistakenly evaluated the results on the disparity dataset. 5 Up-to-date dataset is used. 6Trained on Driving and Monkaa [17])

Method

Sintel clean
test
train

Sintel ﬁnal
train

test

KITTI12

KITTI15
train (Fl-all)

test (Fl-all)

Middlebury
test
train

LDOF1 [7]
DeepFlow1 [31]
Classic+NLP [27]
PCA-Layers1 [33]
EpicFlow1 [22]
FlowFields1 [1]

d Deep DiscreteFlow [11]
i
r
b
y
H

Bailer et al. [2]
DC Flow [34]

FlowNetS [9]
FlowNetS-ft [9]
FlowNetC [9]
FlowNetC-ft [9]
FlowNet2-S3 [14]
FlowNet2-S re-trained2
FlowNet2-C3 [14]
FlowNet2 [14]
FlowNet2-ft-sintel [14]
FlowNet2-ft-kitti [14]

SPyNet [21]
SPyNet-ft [21]
LiteFlowNetX-pre
LiteFlowNetX
LiteFlowNet-pre
LiteFlowNet
LiteFlowNet-ft

l
a
n
o
i
t
n
e
v
n
o
C

N
N
C

t
h
g
i
e
w
y
v
a
e
H

N
N
C

t
h
g
i
e
w
t
h
g
i
L

4.64
2.66
4.49
3.22
2.27
1.86

-
-
-

4.50
(3.66)
4.31
(3.78)
3.79
3.96
3.04
2.02
(1.45)
3.43

4.12
(3.17)
3.70
3.58
2.78
2.48
(1.35)

7.56
5.38
6.73
5.73
4.12
3.75

3.86
3.78
-

7.42
6.96
7.28
6.85
-
-
-
3.96
4.16
-

6.69
6.64
-
-
-
-
4.54

5.96
3.57
7.46
4.52
3.56
3.06

-
-
-

5.45
(4.44)
5.87
(5.28)
4.99
5.37
4.60
3.544
(2.194)
4.834
5.57
(4.32)
4.82
4.79
4.17
4.04
(1.78)

9.12
7.21
8.29
7.89
6.29
5.81

5.73
5.36
5.12

8.43
7.76
8.81
8.51
-
-
-
6.02
5.74
-

8.43
8.36
-
-
-
-
5.38

train

10.94
4.48
-
5.99
3.09
3.33

-
-
-

8.26
7.52
9.35
8.79
7.26
7.31
5.79
4.015
3.545
(1.435)
9.12
3.366
6.81
6.38
4.56
4.00
(1.05)

test

12.4
5.8
7.2
5.2
3.8
3.5

3.4
3.0
-

-
9.1
-
-
-
-
-
-
-
1.8

-
4.1
-
-
-
-
1.6

train

18.19
10.63
-
12.74
9.27
8.33

-
-
-

-
-
-
-
14.28
14.51
11.49
10.085
9.945
(2.365)
-
-
16.64
15.81
11.58
10.39
(1.62)

38.11%
26.52%
-
27.26%
27.18%
24.43%

-
-
-

-
-
-
-
51.06%
51.38%
44.09%
29.99%5
28.02%5
(8.88%5)
-
-
36.64%
34.90%
32.59%
28.50%
(5.58%)

-
29.18%
-
-
27.10%
-

21.17%
19.44%
14.86%

-
-
-
-
-
-
-
-
-
11.48%

-
35.07%
-
-
-
-
9.38%

0.44
0.25
0.22
0.66
0.31
0.27

-
-
-

1.09
0.98
1.15
0.93
1.04
1.13
0.98
0.35
0.35
0.56

0.33
0.33
0.45
0.46
0.45
0.39
0.30

0.56
0.42
0.32
-
0.39
0.33

-
-
-

-
-
-
-
-
-
-
0.52
-
-

0.58
0.58
-
-
-
-
0.40

Table 3: Number of training parameters and runtime. The model
for which the runtime is in parentheses is measured using Torch,
and hence are not directly comparable to the others using Caffe.
Abbreviation LFlowNet refers to LiteFlowNet.

Model

FlowNetC

SPyNet

Shallow

Deep
LFlowNetX

Very Deep

LFlowNet

FlowNet2

# layers
# param. (M)
Runtime (ms)

26
39.16
32.28

35
1.20
(129.83)

74
0.90
35.83

99
5.37
90.25

115
162.49
122.39

will be presented in Section 4.3.

Middlebury. LiteFlowNet has comparable performance
It outperforms FlowNetS
with conventional methods.
(and C) [9], FlowNet2-S (and C) [14], SPyNet [21], and
FlowNet2 [14]. On the benchmark, LiteFlowNet-ft refers
to the one ﬁne-tuned on Sintel.

4.2. Runtime and Parameters

We measure runtime of a CNN using a machine
equipped with an Intel Xeon E5 2.2GHz and an NVIDIA
GTX 1080. Timings are averaged over 100 runs for Sin-
tel image pairs of size 1024 × 436. As summarized in Ta-
ble 3, LiteFlowNet has about 30 times fewer parameters

Table 4: AEE of different variants of LiteFlowNet-pre trained on
Chairs dataset with some of the components disabled.

Variants

Feature Warping
Descriptor Matching
Sub-pix. Reﬁnement
Regularization

FlyingChairs (train)
Sintel clean (train)
Sintel ﬁnal (train)
KITTI12 (train)
KITTI15 (train)

M
(cid:55)
(cid:51)
(cid:55)
(cid:55)

3.75
4.70
5.69
9.22
18.24

MS
(cid:55)
(cid:51)
(cid:51)
(cid:55)

2.70
4.17
5.30
8.01
16.19

WM WSR WMS
(cid:51)
(cid:55)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:55)

(cid:51)
(cid:51)
(cid:55)
(cid:55)

2.98
3.54
4.81
6.17
14.52

1.63
3.19
4.63
5.03
13.20

1.82
2.90
4.45
4.83
12.32

ALL
(cid:51)
(cid:51)
(cid:51)
(cid:51)

1.57
2.78
4.17
4.56
11.58

than FlowNet2 [14] and is 1.36 times faster in runtime.
LiteFlowNetX, a variant of LiteFlowNet having a smaller
model size and without descriptor matching, has about 43
times fewer parameters than FlowNetC [9] and a compa-
rable runtime. LiteFlowNetX also has 1.33 times fewer pa-
rameters than SPyNet [21]. LiteFlowNet and its variants are
currently the most compact CNNs for ﬂow estimation.

4.3. Ablation Study

We

investigate

in
role of
LiteFlowNet-pre trained on Chairs by evaluating the per-

each component

the

7

Image overlay

Ground truth

FlowNetC [9]

FlowNet2 [14]

LiteFlowNet

First image

FlowNetC [9]

FlowNet2 [14]

FlowNet2-ft-sintel [14]

LiteFlowNet-ft

Figure 5: Examples of ﬂow ﬁelds from different methods on Sintel training sets for clean (top row), ﬁnal (middle row) passes, and the
testing set for ﬁnal pass (last row). Fine details are well preserved and less artifacts can be observed in the ﬂow ﬁelds of LiteFlowNet.

Image overlay

Ground truth

FlowNetC [9]

FlowNet2 [14]

LiteFlowNet

First Image

FlowNetC [9]

FlowNet2 [14]

FlowNet2-ft-kitti [14]

LiteFlowNet-ft

Figure 6: Examples of ﬂow ﬁelds from different methods on the training set (top) and the testing set (bottom) of KITTI15.

formance of different variants with some of the components
disabled. The AEE results are summarized in Table 4 and
examples of ﬂow ﬁelds are illustrated in Figure 7.

Feature Warping. We consider two variants LiteFlowNet-
pre (WM and WMS) and compare them to the counterparts
with warping disabled (M and MS). Flow ﬁelds from M and
MS are more vague. Large degradation in AEE is noticed
especially for KITTI12 (33%) and KITTI15 (25%). With
feature warping, pyramidal features that input to ﬂow infer-
ence are closer to each other. This facilitates ﬂow estimation
in subsequent pyramid level by computing residual ﬂow.

Descriptor Matching. We compare the variant WSR with-
out descriptor matching for which the ﬂow inference part
is made as deep as that in the unamended LiteFlowNet-
pre (ALL). No noticeable difference between the ﬂow ﬁelds
from WSR and ALL. Since the maximum displacement of
the example ﬂow ﬁeld is not very large (only 14.7 pixels),
accurate ﬂow ﬁeld can still be yielded from WSR. For eval-
uation covering a wide range of ﬂow displacement (espe-
cially large-displacement benchmark, KITTI), degradation
in AEE is noticed for WSR. This suggests that descriptor
matching is useful in addressing large-displacement ﬂow.

Sub-Pixel Reﬁnement. The ﬂow ﬁeld generated from
WMS is more crisp and contains more ﬁne details than
that generated from WM with sub-pixel reﬁnement dis-
abled. Less small-magnitude ﬂow artifacts (represented by
light color on the background) are also observed. Besides,
WMS achieves smaller AEE. Since descriptor matching es-
tablishes pixel-by-pixel correspondence, sub-pixel reﬁne-
ment is necessary to yield detail-preserving ﬂow ﬁeld.
Regularization. In comparison WMS with regularization
disabled to ALL, undesired artifacts exist in homogeneous
regions (represented by very dim color on the background)
of the ﬂow ﬁeld generated from WMS. Flow bleeding and
vague ﬂow boundaries are observed. Degradation in AEE is
also noticed. This suggests that the proposed feature-driven
local convolution (f-lcon) plays the vital role to smooth ﬂow
ﬁeld and maintain crisp ﬂow boundaries as regularization
term in conventional variational methods.

5. Conclusion

We have presented a compact network for accurate ﬂow
estimation. LiteFlowNet outperforms FlowNet [9] and is on
par with or outperforms the state-of-the-art FlowNet2 [14]

8

Figure 7: Examples of ﬂow ﬁelds from different variants of LiteFlowNet-pre trained on Chairs with some of the components disabled.
LiteFlowNet-pre is denoted as “All”. W = Feature Warping, M = Descriptor Matching, S = Sub-Pixel Reﬁnement, R = Regularization.

on public benchmarks while being faster in runtime and
30 times smaller in model size. Pyramidal feature extrac-
tion and feature warping (f-warp) help us to break the de
facto rule of accurate ﬂow network requiring large model
size. To address large-displacement and detail-preserving
ﬂows, LiteFlowNet exploits short-range matching to gener-
ate pixel-level ﬂow ﬁeld and further improves the estimate
to sub-pixel accuracy in the cascaded ﬂow inference. To
result crisp ﬂow boundaries, LiteFlowNet regularizes ﬂow
ﬁeld through feature-driven local convolution (f-lcon). With
its lightweight, accurate, and fast ﬂow computation, we ex-
pect that LiteFlowNet can be deployed to many applications
such as motion segmentation, action recognition, SLAM,
3D reconstruction and more.

Acknowledgement. This work is supported by SenseTime
Group Limited and the General Research Fund sponsored
by the Research Grants Council of the Hong Kong SAR
(CUHK 14241716, 14224316, 14209217).

6. Appendix

LiteFlowNet consists of two compact sub-networks,
namely NetC and NetE. NetC is a two-steam network in
which the two network streams share the same set of ﬁlters.
The input to NetC is an image pair (I1, I2). The network
architectures of the 6-level NetC and NetE at pyramid level
5 are provided in Table 5 and Tables 6 to 8, respectively.
We use sufﬁxes “M”, “S” and “R” to highlight the layers
that are used in descriptor matching, sub-pixel reﬁnement,
and ﬂow regularization units in NetE, respectively. We de-
clare a layer as “ﬂow” to highlight when the output is a
ﬂow ﬁeld. Our code and trained models are available at
https://github.com/twhui/LiteFlowNet. A
video clip (https://www.youtube.com/watch?v=
pfQ0zFwv-hM) and a supplementary material are avail-
able on our project page (http://mmlab.ie.cuhk.
edu.hk/projects/LiteFlowNet/) to showcase the
performance of LiteFlowNet and the effectiveness of the
proposed components in our network.

References

[1] C. Bailer, B. Taetz, and D. Stricker. Flow Fields: Dense
correspondence ﬁelds for highly accurate large displacement
optical ﬂow estimation. ICCV, pages 4015–4023, 2015. 3, 7
[2] C. Bailer, K. Varanasi, and D. Stricker. CNN-based patch
matching for optical ﬂow with thresholded hinge embedding
loss. CVPR, pages 3250–3259, 2017. 3, 7

[3] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and
R. Szeliski. A database and evaluation methodology for op-
tical ﬂow. IJCV, 92(1):1–31, 2011. 6

[4] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man. PatchMatch: A randomized correspondence algorithm
SIGGRAGH, pages 83–97,
for structural image editing.
2009. 2

[5] M. J. Black, Y. Yacoobt, A. D. Jepsont, and D. J. Fleets.
Learning parameterized models of image motion. CVPR,
pages 674–679, 1997. 2, 3

[6] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
ECCV, pages 25–36, 2004. 1, 2, 3

[7] T. Brox and J. Mailk. Large displacement optical ﬂow: De-
scriptor matching in variational motion estimation. PAMI,
33(3):500–513, 2011. 1, 2, 7

[8] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation.
ECCV, pages 611–625, 2012. 6

[9] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical ﬂow with convolutional net-
works. ICCV, pages 2758–2766, 2015. 1, 2, 3, 4, 6, 7, 8
[10] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? CVPR, pages 3354–3361, 2012. 6
[11] F. Gney and A. Geiger. Deep discrete ﬂow. ACCV, pages

207–224, 2016. 3, 7

[12] B. K. P. Horn and B. G. Schunck. Determining optical ﬂow.

Ariﬁcal Intelligence, 17:185–203, 1981. 1, 2

[13] T.-W. Hui and R. Chung. Determining motion directly
from normal ﬂows upon the use of a spherical eye platform.
CVPR, pages 2267–2274, 2013. 1

[14] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet2.0: Evolution of optical ﬂow estimation
with deep networks. CVPR, pages 2462–2470, 2017. 1, 2, 3,
4, 6, 7, 8

9

Table 5: The network details of NetC in LiteFlowNet. “# Ch. In / Out” means the number of channels of the input or the output features.
“conv” denotes convolution.

Layer name
conv1
conv2 1
conv2 2
conv2 3
conv3 1
conv3 2
conv4 1
conv4 2
conv5
conv6

Kernel
7×7
3×3
3×3
3×3
3×3
3×3
3×3
3×3
3×3
3×3

Stride
1
2
1
1
2
1
2
1
2
2

# Ch. In / Out
3 / 32
32 / 32
32 / 32
32 / 32
32 / 64
64 / 64
64 / 96
96 / 96
96 / 128
128 / 192

Input
I1 or I2
conv1
conv2 1
conv2 2
conv2 3
conv3 1
conv3 2
conv4 1
conv4 2
conv5

Table 6: The network details of the descriptor matching unit (M) of NetE in LiteFlowNet at pyramid level 5. “upconv”, “f-warp”, “corr”,
and “loss” denote the fractionally strided convolution (so-called deconvolution), feature warping, correlation, and the layer where training
loss is applied, respectively. Furthermore, “conv5a’ and “conv5b” denote the high-dimensional features of images I1 and I2 generated
from NetC at pyramid level 5.

Layer name
upconv5 M
f-warp5 M
corr5 M
conv5 1 M
conv5 2 M
conv5 3 M
conv5 4 M

Kernel
4×4
-
1×1
3×3
3×3
3×3
3×3

Stride
0.5
-
1
1
1
1
1

# Ch. In / Out
2 / 2
(128, 2) / 128
(128, 128) / 49
49 / 128
128 / 64
64 / 32
32 / 2
(2, 2) / 2

Input(s)
ﬂow6 R
conv5b, upconv5 M
conv5a, f-warp5 M
corr5 M
conv5 1 M
conv5 2 M
conv5 3 M
upconv5 M, conv5 4 M

ﬂow5 M, loss5 M element-wise sum

[15] M.

Jaderberg, K. Simonyan, A. Zisserman,

and
Spatial transformer networks. NIPS,

K. Kavukcuoglu.
pages 2017–2025, 2015. 3, 4

[16] J. Lu, H. Yang, D. Min, and M. N. Do. PatchMatch Fil-
ter: Efﬁcient edge-aware ﬁltering meets randomized search.
CVPR, pages 1854–1861, 2013. 2

[17] N. Mayer, E. Ilg, P. Husser, P. Fischer, D. Cremers, A. Doso-
vitskiy, and T. Brox. A large dataset to train convolutional
networks for disparity, optical ﬂow, and scene ﬂow estima-
tion. CVPR, pages 4040–4048, 2016. 6, 7

[18] M. Menze and A. Geiger. Object scene ﬂow for autonomous

vehicles. CVPR, pages 3061–3070, 2015. 6
[19] T. Nir, A. M. Bruckstein, and R. Kimmel.

Over-
parameterized variational optical ﬂow. IJCV, 76(2):205–216,
2008. 3

[20] N. Papenberg, A. Bruhn, T. Brox, S. Didas, and J. Weick-
ert. Highly accurate optic ﬂow computation with theoreti-
cally justiﬁed warping. IJCV, 67(2):141–158, 2006. 1, 3
[21] A. Ranjan and M. J. Black. Optical ﬂow estimation using a
spatial pyramid network. CVPR, pages 4161–4170, 2017. 1,
2, 3, 6, 7

[22] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. CVPR, pages 1164–1172, 2015. 1,
2, 6, 7

[23] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. MIC-
CAI, pages 234–241, 2015. 2

[24] D. Rosenbaum, D. Zoran, and Y. Weiss. Learning the local

statistics of optical ﬂow. NIPS, pages 2373–2381, 2013. 3

[25] S. Roth and M. Black. On the spatial statistics of optical

ﬂow. ICCV, pages 42–49, 2005. 3

[26] S. Roth and M. J. Black. Fields of experts: A framework for
learning image priors. CVPR, pages 860–867, 2005. 3

[27] D. Sun, S. Roth, and M. J. Black. A quantitative analysis of
current practices in optical ﬂow estimation and the principles
behind them. IJCV, 106(2):115–137, 2014. 1, 6, 7

[28] D. Sun, S. Roth, J. Lewis, and M. J. Black. Learning optical

ﬂow. ECCV, pages 83–97, 2008. 2, 3

[29] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace:
Closing the gap to human-level performance in face veriﬁca-
tion. CVPR, pages 1701–1708, 2014. 2, 5

[30] D. Tschumperl´e and R. Deriche. Vector-valued image reg-
ularization with PDEs: A common framework for different
applications. PAMI, 27(4):506–517, 2005. 5

[31] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
DeepFlow: Large displacement optical ﬂow with deep
matching. ICCV, pages 500–513, 2013. 2, 3, 7

10

Table 7: Network details of the sub-pixel reﬁnement unit (S) of NetE in LiteFlowNet at pyramid level 5.

Layer name
f-warp5 S
conv5 1 S
conv5 2 S
conv5 3 S
conv5 4 S
ﬂow5 S, loss5 S

Kernel
-
3×3
3×3
3×3
3×3
element-wise sum

Stride
-
1
1
1
1

# Ch. In / Out
(128, 2) / 128
258 / 128
128 / 64
64 / 32
32 / 2
(2, 2) / 2

Input(s)
conv5b, ﬂow5 M
conv5a, f-warp5 S, ﬂow5 M
conv5 1 S
conv5 2 S
conv5 3 S
ﬂow5 M, conv5 4 S

Table 8: Network details of the ﬂow regularization unit (R) of NetE in LiteFlowNet at pyramid level 5. “rgb-warp”, “norm”, “negsq”,
“softmax”, and “f-lcon” denote the image warping, L2 norm of the RGB brightness difference between the two input images, negative-
square, normalized exponential operation over each 1 × 1 × (# Ch. In) column in the 3-D tensor, and feature-driven local convolution,
respectively. Furthermore, “conv dist” that highlights the output of the convolution layer is used as the feature-driven distance metric D
Eq. (7) in the main manuscript. “im5a” and “im5b” denote the down-sized images of I1 and I2 at pyramid level 5, respectively

Layer name
rm-ﬂow5 R
rgb-warp5 R
norm5 R
conv5 1 R
conv5 2 R
conv5 3 R
conv5 4 R
conv5 5 R
conv5 6 R
conv5 dist R
negsq5 R
softmax5 R
f-lcon5 R (Out: ﬂow5 R), loss5 R

Stride

Kernel
remove mean
-

-
L2 norm

1
1
1
1
1
1
1

3×3
3×3
3×3
3×3
3×3
3×3
3×3
negative-square
1×1×9
3×3

1
1

# Ch. In / Out
2 / 2
(3, 2) / 3
(3, 3) / 1
131 / 128
128 / 128
128 / 64
64 / 64
64 / 32
32 / 32
32 / 9
9 / 9
9 / 9
(9, 2) / 2

Input(s)
ﬂow5 S
im5b, ﬂow5 S
im5a, rgb-warp5 R
conv5a, rm-ﬂow5 R, norm5 R
conv5 1 R
conv5 2 R
conv5 3 R
conv5 4 R
conv5 5 R
conv5 6 R
conv5 dist R
negsq5 R
softmax5 R, ﬂow5 S

[32] M. Werlberger, W. Trobin, T. Pock, A. Wedel, D. Cremers,
and H. Bischof. Anisotropic Huber-L1 optical ﬂow. BMVC,
2009. 2, 5

[33] J. Wulff and M. J. Black. Efﬁcient sparse-to-dense optical
ﬂow estimation using a learned basis and layers. CVPR,
pages 120–130, 2015. 3, 7

[34] J. Xu, R. Ranftl, and V. Koltun. Accurate optical ﬂow via
direct cost volume processings. CVPR, pages 1289–1297,
2017. 6, 7

[35] S. Zagoruyko and N. Komodakis. Learning to compare im-
age patches via convolutional neural networks. CVPR, pages
4353–4361, 2015. 3

[36] H. Zimmer, A. Bruhn, and J. Weickert. Optic ﬂow in har-

mony. IJCV, 93(3):368–388, 2011. 1, 2, 5

[37] S. Zweig and L. Wolf. InterpoNet, A brain inspired neural
network for optical ﬂow dense interpolation. CVPR, pages
4563–4572, 2017. 3

11

LiteFlowNet: A Lightweight Convolutional Neural Network
for Optical Flow Estimation

Tak-Wai Hui, Xiaoou Tang, Chen Change Loy
CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong
{twhui,xtang,ccloy}@ie.cuhk.edu.hk

8
1
0
2
 
y
a
M
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
6
3
0
7
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

FlowNet2 [14], the state-of-the-art convolutional neural
network (CNN) for optical ﬂow estimation, requires over
160M parameters to achieve accurate ﬂow estimation. In
this paper we present an alternative network that outper-
forms FlowNet2 on the challenging Sintel ﬁnal pass and
KITTI benchmarks, while being 30 times smaller in the
model size and 1.36 times faster in the running speed. This
is made possible by drilling down to architectural details
that might have been missed in the current frameworks: (1)
We present a more effective ﬂow inference approach at each
pyramid level through a lightweight cascaded network. It
not only improves ﬂow estimation accuracy through early
correction, but also permits seamless incorporation of de-
scriptor matching in our network. (2) We present a novel
ﬂow regularization layer to ameliorate the issue of outliers
and vague ﬂow boundaries by using a feature-driven lo-
cal convolution. (3) Our network owns an effective struc-
ture for pyramidal feature extraction and embraces fea-
ture warping rather than image warping as practiced in
FlowNet2. Our code and trained models are available at
https://github.com/twhui/LiteFlowNet.

1. Introduction

Optical ﬂow estimation is a long-standing problem in
computer vision. Due to the well-known aperture problem,
optical ﬂow is not directly measurable [12, 13]. Hence, the
estimation is typically solved by energy minimization in a
coarse-to-ﬁne framework [6, 20, 7, 36, 27, 22]. This class
of techniques, however, involves complex energy optimiza-
tion and thus it is not scalable for applications that demand
real-time estimation.

FlowNet [9] and its successor FlowNet2 [14], have
marked a milestone by using CNN for optical ﬂow esti-
mation. Their accuracies especially the successor are ap-
proaching that of state-of-the-art energy minimization ap-
proaches, while the speed is several orders of magnitude

Figure 1: Examples demonstrate the effectiveness of the proposed
components in LiteFlowNet for i) feature warping, ii) cascaded
ﬂow inference, and iii) ﬂow regularization. Enabled components
are indicated with bold black fonts.

faster. To push the envelop of accuracy, FlowNet2 is de-
signed as a cascade of variants of FlowNet that each net-
work in the cascade reﬁnes the preceding ﬂow ﬁeld by con-
tributing on the ﬂow increment between the ﬁrst image and
the warped second image. The model, as a result, com-
prises over 160M parameters, which could be formidable in
many applications. A recent network termed SPyNet [21]
attempts a network with smaller size of 1.2M parameters
by adopting image warping in each pyramid level. Nonethe-
less, the accuracy can only match that of FlowNet but not
FlowNet2. The objective of this study is to explore alter-
native CNN architectures for accurate ﬂow estimation yet
with high efﬁciency. Our work is inspired by the successes
of FlowNet2 and SPyNet, but we further drill down the key
elements to fully unleash the potential of deep convolutional
network combined with classical principles.

There are two general principles to improve the design
of FlowNet2 and SPyNet. The ﬁrst principle is pyrami-
dal feature extraction. The proposed network, dubbed Lite-
FlowNet, consists of an encoder and a decoder. The encoder
maps the given image pair, respectively, into two pyramids

1

of multi-scale high-dimensional features. The decoder then
estimates the ﬂow ﬁeld in a coarse-to-ﬁne framework. At
each pyramid level, the decoder infers the ﬂow ﬁeld by se-
lecting and using the features of the same resolution from
the feature pyramids. This design leads to a lighter network
compared to FlowNet2 that adopts U-Net architecture [23]
for ﬂow inference. In comparison to SPyNet, our network
separates the process of feature extraction and ﬂow estima-
tion. This helps us to better pinpoint the bottleneck of accu-
racy and model size.

The second general principle is feature warping.
FlowNet2 and SPyNet warp the second image towards the
ﬁrst image in the pair using the previous ﬂow estimate, and
then reﬁne the estimate using the feature maps generated by
the warped and the ﬁrst images. Warping an image and then
generating the feature maps of the warped image are two
ordered steps. We ﬁnd that the two steps can be reduced to
a single one by directly warping the feature maps of the sec-
ond image, which have been computed by the encoder. This
one-step feature warping process reduces the more discrimi-
native feature-space distance instead of the RGB-space dis-
tance between the two images. This makes our network
more powerful and efﬁcient in addressing the ﬂow problem.
We now highlight the more speciﬁc differences between
our network and existing CNN-based optical ﬂow estima-
tion frameworks:
1) Cascaded ﬂow inference – At each pyramid level, we in-
troduce a novel cascade of two lightweight networks. Each
of them has a feature warping (f-warp) layer to displace
the feature maps of the second image towards the ﬁrst im-
age using the ﬂow estimate from the previous level. Flow
residue is computed to further reduce the feature-space dis-
tance between the images. This design is advantageous to
the conventional design of using a single network for ﬂow
inference. First, the cascade progressively improves ﬂow
accuracy thus allowing an early correction of the estimate
without passing more errors to the next level. Second, this
design allows seamless integration with descriptor match-
ing. We assign a matching network to the ﬁrst inference.
Consequently, pixel-accuracy ﬂow ﬁeld can be generated
ﬁrst and then reﬁned to sub-pixel accuracy in the subse-
quent inference network. Since at each pyramid level the
feature-space distance between the images has been reduced
by feature warping, we can use a rather short displace-
ment than [9, 14] to establish the cost volume. Besides,
matching is performed only at sampled positions and thus a
sparse cost-volume is aggregated. This effectively reduces
the computational burden raised by the explicit matching.
2) Flow regularization – The cascaded ﬂow inference re-
sembles the role of data ﬁdelity in energy minimization
methods. Using data term alone, vague ﬂow boundaries
and undesired artifacts exist in ﬂow ﬁelds. To tackle this
problem, local ﬂow consistency and co-occurrence between

ﬂow boundaries and intensity edges are commonly used as
the cues to regularize ﬂow ﬁeld. Some of the representative
methods include anisotropic image-driven [32], image- and
ﬂow-driven [28], and complementary [36] regularizations.
After cascaded ﬂow inference, we allow the ﬂow ﬁeld to be
further regularized by our novel feature-driven local convo-
lution (f-lconv) layer1 at each pyramid level. The kernels of
such a local convolution are adaptive to the pyramidal fea-
tures from the encoder, ﬂow estimate and occlusion prob-
ability map. This makes the ﬂow regularization to be both
ﬂow- and image-aware. To our best knowledge, state-of-
the-art CNNs do not explore such a ﬂow regularization.

The effectiveness of the aforementioned contributions
are depicted in Figure 1. In summary, we propose a compact
LiteFlowNet to estimate optical ﬂow. Our network inno-
vates the useful elements from conventional methods. e.g.,
brightness constraint in data ﬁdelity to pyramidal CNN fea-
tures and image warping to CNN feature warping. More
speciﬁcally, we present a cascaded ﬂow inference with fea-
ture warping and ﬂow regularization in each pyramid level,
which are new in the literature. Overall, our network out-
performs FlowNet [9] and SPyNet [21] and is on par with
or outperforms the recent FlowNet2 [14] on public bench-
marks, while having 30 times fewer parameters and being
1.36 times faster than FlowNet2.

2. Related Work

Here, we brieﬂy review some of the major approaches

for optical ﬂow estimation.

Variational methods. Since the pioneering work by Horn
and Schunck [12], variational methods have dominated op-
tical ﬂow estimation. Brox et al. address illumination
changes by combining the brightness and gradient con-
stancy assumptions [6]. Brox et al. integrate rich descriptors
into variational formulation [7]. In DeepFlow [31], Weinza-
epfel et al. propose to correlate multi-scale patches and in-
corporate this as the matching term in functional. In Patch-
Match Filter [16], Lu et al. establish dense correspondence
using the superpixel-based PatchMatch [4]. Revaud et al.
propose a method EpicFlow that uses externally matched
ﬂows as initialization and then performs interpolation [22].
Zimmer et al. design the complementary regularization that
exploits directional information from the constraints im-
posed in data term [36]. Our network that infers optical
ﬂow and performs ﬂow regularization is inspired by the use
of data ﬁdelity and regularization in variational methods.

Machine learning methods. Black et al. propose to repre-
sent complex image motion as a linear combination of the
learned basis vectors [5]. Roth et al. formulates the prior

1We name it as feature-driven local convolution (f-lconv) layer in order
to distinguish it from local convolution (lconv) layer of which ﬁlter weights
are locally ﬁxed in conventional CNNs [29].

2

probability of ﬂow ﬁeld as Field-of-Experts model [26] that
captures higher order spatial statistics [25]. Sun et al. study
the probabilistic model of brightness inconstancy in a high-
order random ﬁeld framework [28]. Nir et al. represent
image motion using the over-parameterization model [19].
Rosenbaum et al. model the local statistics of optical ﬂow
using Gaussian mixtures [24]. Given a set of sparse
matches, Wulff et al. propose to regress them to a dense
ﬂow ﬁeld using a set of basis ﬂow ﬁelds (PCA-Flow) [33].
It can be shown that the parameterized model [5, 19, 33]
can be efﬁciently implemented using CNN.
CNN-based methods. In the work of Fischer et al. termed
FlowNet [9], a post-processing step that involves energy
minimization is required to reduce smoothing effect across
ﬂow boundaries. This process is not end-to-end trainable.
In our work, we present an end-to-end approach that per-
forms in-network ﬂow regularization using the proposed
f-lconv layer, which plays similar role as the regulariza-
tion term in variational methods.
In FlowNet2 [14], Ilg
et al. introduce a huge network cascade (over 160M pa-
rameters) that consists of variants of FlowNet. The cas-
cade improves ﬂow accuracy with an expense of model size
and computational complexity. Our model uses a more ef-
ﬁcient architecture containing 30 times fewer parameters
than FlowNet2 while the performance is on par with it. A
compact network termed SPyNet [21] from Ranjan et al. is
inspired from spatial pyramid. Nevertheless, the accuracy is
far below FlowNet2. A small-sized variant of our network
outperforms SPyNet while being 1.33 times smaller in the
model size. Zweig et al. present a network to interpolate
third-party sparse ﬂows but requiring off-the-shelf edge de-
tector [37]. DeepFlow [31] that involves convolution and
pooling operations is however not a CNN, since the “ﬁlter
weights” are non-trainable image patches. According to the
terminology used in FlowNet, DeepFlow uses correlation.

An alternative approach for establishing point correspon-
dence is to match image patches. Zagoruyko et al. ﬁrst in-
troduce to CNN-feature matching [35]. G¨uney et al. ﬁnd
feature representation and formulate optical ﬂow estimation
in MRF [11]. Bailer et al. [2] use multi-scale features and
then perform feature matching as Flow Fields [1]. Although
pixel-wise matching can establish accurate point correspon-
dence, the computational demand is too high for practical
use (it takes several seconds even a GPU is used). As a
tradeoff, Fischer et al. [9] and Ilg et al. [14] perform feature
matching only at a reduced spatial resolution. We reduce
the computational burden of feature matching by using a
short-ranged matching of warped CNN features at sampled
positions and a sub-pixel reﬁnement at every pyramid level.
We are inspired by the feature transformation used in
Spatial Transformer [15]. Our network uses the proposed
f-warp layer to displace each channel2 of the given vector-

2We can also use f-warp layer to displace each channel differently when

valued feature according to the provided ﬂow ﬁeld. Unlike
Spatial Transformer, f-warp layer is not fully constrained
and is a relaxed version of it as the ﬂow ﬁeld is not param-
eterized. While transformation in FlowNet2 and SPyNet is
limited to images, our decider network is a more generic
warping network that warps high-level CNN features.

3. LiteFlowNet

LiteFlowNet is composed of two compact sub-networks
that are specialized in pyramidal feature extraction and op-
tical ﬂow estimation as shown in Figure 2. Since the spatial
dimension of feature maps is contracting in feature extrac-
tion and that of ﬂow ﬁelds is expanding in ﬂow estimation,
we call the two sub-networks as NetC and NetE respec-
tively. NetC transforms any given image pair into two pyra-
mids of multi-scale high-dimensional features. NetE con-
sists of cascaded ﬂow inference and regularization modules
that estimate coarse-to-ﬁne ﬂow ﬁelds.

Pyramidal Feature Extraction. As shown in Figure 2,
NetC is a two-stream network in which the ﬁlter weights
are shared across the two streams. Each of them functions
as a feature descriptor that transforms an image I to a pyra-
mid of multi-scale high-dimensional features {Fk(I)} from
the highest spatial resolution (k = 1) to the lowest spatial
resolution (k = L). The pyramidal features are generated
by stride-s convolutions with the reduction of spatial reso-
lution by a factor s up the pyramid. In the following, we
omit the subscript k that indicates the level of pyramid for
brevity. We use Fi to represent CNN features for Ii. When
we discuss the operations in a pyramid level, the same op-
erations are applicable to other levels.

Feature Warping. At each pyramid level, a ﬂow ﬁeld is
inferred from high-level features F1 and F2 of images I1
and I2. Flow inference becomes more challenging if I1 and
I2 are captured far away from each other. With the motiva-
tion of image warping used in conventional methods [6, 20]
and recent CNNs [14, 21] for addressing large-displacement
ﬂow, we propose to reduce feature-space distance between
F1 and F2 by feature warping (f-warp). Speciﬁcally, F2
is warped towards F1 by f-warp via ﬂow estimate ˙x to
(cid:101)F2(x) (cid:44) F2(x + ˙x) ∼ F1(x). This allows our network to
infer residual ﬂow between F1 and (cid:101)F2 that has smaller ﬂow
magnitude (more details in Section 3.1) but not the com-
plete ﬂow ﬁeld that is more difﬁcult to infer. Unlike con-
ventional methods, f-warp is performed on high-level CNN
features but not on images. This makes our network more
powerful and efﬁcient in addressing the optical ﬂow prob-
lem. To allow end-to-end training, F is interpolated to (cid:101)F

multiple ﬂow ﬁelds are supplied. The usage, however, is beyond the scope
of this work.

3

Figure 2: The network structure of LiteFlowNet. For the ease of representation, only a 3-level design is shown. Given an image pair (I1
and I2), NetC generates two pyramids of high-level features (Fk(I1) in pink and Fk(I2) in red, k ∈ [1, 3]). NetE yields multi-scale ﬂow
ﬁelds that each of them is generated by a cascaded ﬂow inference module M :S (in blue color, including a descriptor matching unit M and
a sub-pixel reﬁnement unit S) and a regularization module R (in green color). Flow inference and regularization modules correspond to
data ﬁdelity and regularization terms in conventional energy minimization methods respectively.

for any sub-pixel displacement ˙x as follows:

(cid:101)F(x) =

(cid:88)

F(xi

s) (cid:0)1 − (cid:12)

(cid:12)xs − xi
s

(cid:12)
(cid:12)

(cid:1) (cid:0)1 − (cid:12)

(cid:12)ys − yi
s

(cid:1) ,

(cid:12)
(cid:12)

xi

s∈N (xs)

(1)
where xs = x + ˙x = (xs, ys)(cid:62) denotes the source coor-
dinates in the input feature map F that deﬁnes the sample
point, x = (x, y)(cid:62) denotes the target coordinates of the
regular grid in the interpolated feature map (cid:101)F, and N (xs)
denotes the four pixel neighbors of xs. The above bilinear
interpolation allows back-propagation during training as its
gradients can be efﬁciently computed [15].

3.1. Cascaded Flow Inference

At each pyramid level of NetE, pixel-by-pixel matching
of high-level features yields coarse ﬂow estimate. A subse-
quent reﬁnement on the coarse ﬂow further improves it to
sub-pixel accuracy.
First Flow Inference (descriptor matching). Point cor-
respondence between I1 and I2 is established through com-
puting correlation of high-level feature vectors in individual
pyramidal features F1 and F2 as follows:

c(x, d) = F1(x) · F2(x + d)/N,

(2)

where c is the matching cost between point x in F1 and
point x + d in F2, d ∈ Z is the displacement vector from x,
and N is the length of the feature vector. A cost volume C
is built by aggregating all the matching costs into a 3D grid.
We reduce the computational burden raised by cost-
volume processing [9, 14] in three ways: 1) We perform
short-range matching at every pyramid level instead of long-
range matching at a single level. 2) We reduce feature-space

distance between F1 and F2 by warping F2 towards F1
using our proposed f-warp through ﬂow estimate3 ˙x from
previous level. 3) We perform matching only at the sam-
pled positions in the pyramid levels of high-spatial resolu-
tion. The sparse cost volume is interpolated in the spatial
dimension to ﬁll the missed matching costs for the unsam-
pled positions. The ﬁrst two techniques effectively reduce
the searching space needed, while the last technique reduces
the frequency of matching per pyramid level.

In the descriptor matching unit M , residual ﬂow ∆ ˙xm
is inferred by ﬁltering the cost volume C as illustrated in
Figure 3. A complete ﬂow ﬁeld ˙xm is computed as follows:

˙xm = M (cid:0)C(F1, (cid:101)F2; d)(cid:1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
∆ ˙xm

+s ˙x↑s.

(3)

Second Flow Inference (sub-pixel reﬁnement). Since
the cost volume in descriptor matching unit is aggregated
by measuring pixel-by-pixel correlation, ﬂow estimate ˙xm
from the previous inference is only up to pixel-level accu-
racy. We introduce the second ﬂow inference in the wake
of descriptor matching as shown in Figure 3.
It aims to
reﬁne the pixel-level ﬂow ﬁeld ˙xm to sub-pixel accuracy.
This prevents erroneous ﬂows being ampliﬁed by upsam-
pling and passing to the next pyramid level. Speciﬁcally,
F2 is warped to (cid:101)F2 via ﬂow estimate ˙xm. Sub-pixel reﬁne-
ment unit S yields a more accurate ﬂow ﬁeld ˙xs by mini-
mizing feature-space distance between F1 and (cid:101)F2 through

3 ˙x from previous level needs to be upsampled in spatial resolution (de-
noted by “↑s”) and magnitude (multiplied by a scalar s) to s ˙x↑s for match-
ing the spatial resolution of the pyramidal features at the current level.

4

Figure 4: Folding and packing of f-lcon ﬁlters {g}. The (x, y)-
entry of 3D tensor ¯G(c) is a 3D column with size 1 × 1 × w2.
It corresponds to the unfolded w × w f-lcon ﬁlter g(x, y, c) to be
applied at position (x, y) of channel c in vector-valued feature F .

ﬂow inference, we replace F to ˙xs. Flow regularization
module R is deﬁned as follows:

˙xr = R( ˙xs; G).

(6)

The f-lcon ﬁlters need to be specialized for smoothing
ﬂow ﬁeld. It should behave as an averaging ﬁlter if the vari-
ation of ﬂow vectors over the patch is smooth. It should also
not over-smooth ﬂow ﬁeld across ﬂow boundary. We deﬁne
a feature-driven CNN distance metric D that estimates lo-
cal ﬂow variation using pyramidal feature F1, ﬂow ﬁeld ˙xs
from the cascaded ﬂow inference, and occlusion probabil-
ity map4 O. In summary, D is adaptively constructed by a
CNN unit RD as follows:

D = RD(F1, ˙xs, O).

(7)

With the introduction of feature-driven distance metric D,
each ﬁlter g of f-lcon is constructed as follows:

g(x, y, c) =

exp(−D(x, y, c)2)
(xi,yi)∈N (x,y) exp(−D(xi, yi, c)2)

,

(cid:80)

(8)

where N (x, y) denotes the neighborhood containing ω × ω
pixels centered at position (x, y).

Here, we provide a mechanism to perform f-lcon ef-
ﬁciently. For a C-channel input F , we use C tensors
¯G(1), ..., ¯G(C) to store f-lcon ﬁlter set G. As illustrated
in Figure 4, each f-lcon ﬁlter g(x, y, c) is folded into a
1 × 1 × w2 3D column and then packed into the (x, y)-
entry of a M × N × w2 3D tensor ¯G(c). Same folding and
packing operations are also applied to each patch in each
channel of F . This results C tensors ¯F (1), ..., ¯F (C) for F .
In this way, Equation (5) can be reformulated to:

Fg(c) = ¯G(c) (cid:12) ¯F (c),

(9)

where “(cid:12)” denotes element-wise dot product between the
corresponding columns of the tensors. With the abuse of

4We use the brightness error ||I2(x+ ˙x)−I1(x)||2 between the warped

second image and the ﬁrst image as the occlusion probability map.

Figure 3: A cascaded ﬂow inference module M :S in NetE. It con-
sists of a descriptor matching unit M and a sub-pixel reﬁnement
unit S. In M , f-warp transforms high-level feature F2 to (cid:101)F2 via
upscaled ﬂow ﬁeld 2 ˙x↑2 estimated at previous pyramid level. In
S, F2 is warped by ˙xm from M . In comparison to residual ﬂow
∆ ˙xm, more ﬂow adjustment exists at ﬂow boundaries in ∆ ˙xs.

computing residual ﬂow ∆ ˙xs as the following:

˙xs = S(cid:0)F1, (cid:101)F2, ˙xm

+ ˙xm.

(4)

(cid:1)

(cid:125)

(cid:124)

(cid:123)(cid:122)
∆ ˙xs

3.2. Flow Regularization

Cascaded ﬂow inference resembles the role of data ﬁ-
delity in conventional minimization methods. Using data
term alone, vague ﬂow boundaries and undesired artifacts
commonly exist in ﬂow ﬁeld [32, 36]. To tackle this prob-
lem, we propose to use a feature-driven local convolution
(f-lcon) to regularize ﬂow ﬁeld from the cascaded ﬂow in-
ference. The operation of f-lcon is well-governed by the
Laplacian formulation of diffusion of pixel values [30]. In
contrast to local convolution (lcon) used in conventional
CNNs [29], f-lcon is more generalized. Not only is a dis-
tinct ﬁlter used for each position of feature map, but the
ﬁlter is adaptively constructed for individual ﬂow patches.
Consider a general case, a vector-valued feature F that
has to be regularized has C channels and a spatial dimen-
sion M × N . Deﬁne G = {g} as the set of ﬁlters used in
f-lcon layer. The operation of f-lcon to F can be formulated
as follow:

fg(x, y, c) = g(x, y, c) ∗ f (x, y, c),

(5)

where “∗” denotes convolution, f (x, y, c) is a w × w patch
centered at position (x, y) of channel c in F , g(x, y, c) is the
corresponding w × w regularization ﬁlter, and fg(x, y, c) is
a scalar output for x = (x, y)(cid:62) and c = 1, 2, ..., C. To
be speciﬁc for regularizing ﬂow ﬁeld ˙xs from the cascaded

5

Table 1: AEE on the Chairs testing set. Models are trained on the
Chairs training set.

FlowNetS

FlowNetC

SPyNet

LiteFlowNetX-pre

LiteFlowNet-pre

2.71

2.19

2.63

2.25

1.57

notation, Fg(c) means the c-th xy-slice of the regularized
C-channel feature Fg. Equation (9) reduces the dimension
of tensors from M × N × w2 (right-hand side in prior to the
dot product) to M × N (left-hand side).

4. Experiments

Network Details. In LiteFlowNet, NetC generates 6-level
pyramidal features and NetE predicts ﬂow ﬁelds for levels
6 to 2. Flow ﬁeld in level 2 is upsampled to yield ﬂow ﬁeld
in level 1. We set the maximum searching radius in cost-
volume to 3 pixels (levels 6 to 4) or 6 pixels (levels 3 to 2).
Matching is performed at each position in pyramidal fea-
tures, except for levels 3 to 2 that it is performed at a regu-
larly sampled grid (a stride of 2). All convolution layers use
3 × 3 ﬁlters, except each last layer in descriptor matching
M , sub-pixel reﬁnement S, and ﬂow regularization R units
uses 5×5 (levels 4 to 3) or 7×7 (level 2) ﬁlters. Each convo-
lution layer is followed by a leaky rectiﬁed linear unit layer,
except f-lcon and the last layer in M , S and R CNN units.
More details can be found in the supplementary material.
Training Details. We train our network stage-wise by the
following steps: 1) NetC and M6:S6 of NetE is trained
for 300k iterations. 2) R6 together with the trained net-
work in step 1 is trained for 300k iterations. 3) For lev-
els k ∈ [5, 2], Mk:Sk followed by Rk is added into the
trained network each time. The new network cascade is
trained for 200k (level 2: 300k) iterations. Filter weights
are initialized from previous level. Learning rates are ini-
tially set to 1e-4, 5e-5, and 4e-5 for levels 6 to 4, 3 and
2 respectively. We reduce it by a factor of 2 starting at
120k, 160k, 200k, and 240k iterations. We use the same
loss weight, L2 training loss, Adam optimization, data aug-
mentation (including noise injection), and training sched-
ule 5 (Chairs [9] → Things3D [17]) as FlowNet2 [14]. We
denote LiteFlowNet-pre and LiteFlowNet as the networks
trained on Chairs and Chairs → Things3D, respectively.

4.1. Results

We compare several variants of LiteFlowNet to state-
of-the-art methods on public benchmarks including Fly-
ingChairs
[8],
KITTI12 [10], KITTI15 [18], and Middlebury [3].
FlyingChairs. We ﬁrst compare the intermediate results of

[9], Sintel clean and ﬁnal

(Chairs)

5We excluded a small amount of training data in Things3D undergoing
extremely large ﬂow displacement as advised by the authors (https://
github.com/lmb-freiburg/flownet2/issues).

different well-performing networks trained on Chairs alone
in Table 1. Average end-point error (AEE) is reported.
LiteFlowNet-pre outperforms the compared networks. No
intermediate result is available for FlowNet2 [14] as each
cascade is trained on the Chairs → Things3D sched-
ule individually. Since FlowNetC, FlowNetS (variants of
FlowNet [9]), and SPyNet [21] have fewer parameters than
FlowNet2 and the later two models do not perform fea-
ture matching, we also construct a small-size counterpart
LiteFlowNetX-pre by removing the matching part and
shrinking the model sizes of NetC and NetE by about 4 and
5 times, respectively. Despite that LiteFlowNetX-pre is 43
and 1.33 times smaller than FlowNetC and SPyNet, respec-
tively, it still outperforms these networks and is on par with
FlowNetC that uses explicit matching.

MPI Sintel.
In Table 2, LiteFlowNetX-pre outperforms
FlowNetS (and C) [9] and SPyNet [21] that are trained
on Chairs on all cases except the Middlebury benchmark.
LiteFlowNet, trained on the Chairs → Things3D sched-
ule, performs better than LiteFlowNet-pre as expected.
LiteFlowNet also outperforms SPyNet, FlowNet2-S (and -
C) [14]. We also ﬁne-tuned LiteFlowNet on a mixture of
Sintel clean and ﬁnal training data (LiteFlowNet-ft) using
the generalized Charbonnier loss [27]. No noise augmen-
tation was performed but we introduced image mirroring
to improve the diversity of the training set. LiteFlowNet-
ft outperforms FlowNet2-ft-sintel [14] and EpicFlow [22]
for Sintel ﬁnal testing set. Despite DC Flow [34] (a hy-
brid method consists of CNN and post-processing) per-
forms better than LiteFlowNet, its GPU runtime requires
several seconds that makes it formidable in many applica-
tions. Figure 5 shows some examples of ﬂow ﬁelds on Sin-
tel dataset. LiteFlowNet-ft and FlowNet2-ft-sintel perform
the best among the compared methods. As LiteFlowNet has
ﬂow regularization module, sharper ﬂow boundaries and
lesser artifacts can be observed in the generated ﬂow ﬁelds.

KITTI. LiteFlowNet consistently performs better than
LiteFlowNet-pre especially on KITTI15 as shown in Ta-
ble 2.
It also outperforms SPyNet [21] and FlowNet2-S
(and C) [14]. We also ﬁne-tuned LiteFlowNet on a mix-
ture of KITTI12 and KITTI15 training data (LiteFlowNet-
ft) using the same augmentation as the case of Sintel ex-
cept that we reduced the amount of augmentation for spa-
tial motion to ﬁt the driving scene. After ﬁne-tuning, Lite-
FlowNet generalizes well to real-world data. LiteFlowNet-
ft outperforms FlowNet2-ft-kitti [14].
Figure 6 shows
some examples of ﬂow ﬁelds on KITTI. As in the case
for Sintel, LiteFlowNet-ft and FlowNet2-ft-kitti performs
the best among the compared methods. Even though
LiteFlowNet and its variants perform pyramidal descrip-
tor matching in a limited searching range, it yields reliable
large-displacement ﬂow ﬁelds for real-world data due to the
feature warping (f-warp) layer introduced. More analysis

6

Table 2: AEE of different methods. The values in parentheses are the results of the networks on the data they were trained on, and hence are
not directly comparable to the others. Fl-all: Percentage of outliers averaged over all pixels. Inliers are deﬁned as EPE <3 pixels or <5%.
The best number for each category is highlighted in bold. (Note: 1The values are reported from [14]. 2We re-trained the model using the
code provided by the authors. 3,4,5The values are computed using the trained models provided by the authors. 4Large discrepancy exists as
the authors mistakenly evaluated the results on the disparity dataset. 5 Up-to-date dataset is used. 6Trained on Driving and Monkaa [17])

Method

Sintel clean
test
train

Sintel ﬁnal
train

test

KITTI12

KITTI15
train (Fl-all)

test (Fl-all)

Middlebury
test
train

LDOF1 [7]
DeepFlow1 [31]
Classic+NLP [27]
PCA-Layers1 [33]
EpicFlow1 [22]
FlowFields1 [1]

d Deep DiscreteFlow [11]
i
r
b
y
H

Bailer et al. [2]
DC Flow [34]

FlowNetS [9]
FlowNetS-ft [9]
FlowNetC [9]
FlowNetC-ft [9]
FlowNet2-S3 [14]
FlowNet2-S re-trained2
FlowNet2-C3 [14]
FlowNet2 [14]
FlowNet2-ft-sintel [14]
FlowNet2-ft-kitti [14]

SPyNet [21]
SPyNet-ft [21]
LiteFlowNetX-pre
LiteFlowNetX
LiteFlowNet-pre
LiteFlowNet
LiteFlowNet-ft

l
a
n
o
i
t
n
e
v
n
o
C

N
N
C

t
h
g
i
e
w
y
v
a
e
H

N
N
C

t
h
g
i
e
w
t
h
g
i
L

4.64
2.66
4.49
3.22
2.27
1.86

-
-
-

4.50
(3.66)
4.31
(3.78)
3.79
3.96
3.04
2.02
(1.45)
3.43

4.12
(3.17)
3.70
3.58
2.78
2.48
(1.35)

7.56
5.38
6.73
5.73
4.12
3.75

3.86
3.78
-

7.42
6.96
7.28
6.85
-
-
-
3.96
4.16
-

6.69
6.64
-
-
-
-
4.54

5.96
3.57
7.46
4.52
3.56
3.06

-
-
-

5.45
(4.44)
5.87
(5.28)
4.99
5.37
4.60
3.544
(2.194)
4.834
5.57
(4.32)
4.82
4.79
4.17
4.04
(1.78)

9.12
7.21
8.29
7.89
6.29
5.81

5.73
5.36
5.12

8.43
7.76
8.81
8.51
-
-
-
6.02
5.74
-

8.43
8.36
-
-
-
-
5.38

train

10.94
4.48
-
5.99
3.09
3.33

-
-
-

8.26
7.52
9.35
8.79
7.26
7.31
5.79
4.015
3.545
(1.435)
9.12
3.366
6.81
6.38
4.56
4.00
(1.05)

test

12.4
5.8
7.2
5.2
3.8
3.5

3.4
3.0
-

-
9.1
-
-
-
-
-
-
-
1.8

-
4.1
-
-
-
-
1.6

train

18.19
10.63
-
12.74
9.27
8.33

-
-
-

-
-
-
-
14.28
14.51
11.49
10.085
9.945
(2.365)
-
-
16.64
15.81
11.58
10.39
(1.62)

38.11%
26.52%
-
27.26%
27.18%
24.43%

-
-
-

-
-
-
-
51.06%
51.38%
44.09%
29.99%5
28.02%5
(8.88%5)
-
-
36.64%
34.90%
32.59%
28.50%
(5.58%)

-
29.18%
-
-
27.10%
-

21.17%
19.44%
14.86%

-
-
-
-
-
-
-
-
-
11.48%

-
35.07%
-
-
-
-
9.38%

0.44
0.25
0.22
0.66
0.31
0.27

-
-
-

1.09
0.98
1.15
0.93
1.04
1.13
0.98
0.35
0.35
0.56

0.33
0.33
0.45
0.46
0.45
0.39
0.30

0.56
0.42
0.32
-
0.39
0.33

-
-
-

-
-
-
-
-
-
-
0.52
-
-

0.58
0.58
-
-
-
-
0.40

Table 3: Number of training parameters and runtime. The model
for which the runtime is in parentheses is measured using Torch,
and hence are not directly comparable to the others using Caffe.
Abbreviation LFlowNet refers to LiteFlowNet.

Model

FlowNetC

SPyNet

Shallow

Deep
LFlowNetX

Very Deep

LFlowNet

FlowNet2

# layers
# param. (M)
Runtime (ms)

26
39.16
32.28

35
1.20
(129.83)

74
0.90
35.83

99
5.37
90.25

115
162.49
122.39

will be presented in Section 4.3.

Middlebury. LiteFlowNet has comparable performance
It outperforms FlowNetS
with conventional methods.
(and C) [9], FlowNet2-S (and C) [14], SPyNet [21], and
FlowNet2 [14]. On the benchmark, LiteFlowNet-ft refers
to the one ﬁne-tuned on Sintel.

4.2. Runtime and Parameters

We measure runtime of a CNN using a machine
equipped with an Intel Xeon E5 2.2GHz and an NVIDIA
GTX 1080. Timings are averaged over 100 runs for Sin-
tel image pairs of size 1024 × 436. As summarized in Ta-
ble 3, LiteFlowNet has about 30 times fewer parameters

Table 4: AEE of different variants of LiteFlowNet-pre trained on
Chairs dataset with some of the components disabled.

Variants

Feature Warping
Descriptor Matching
Sub-pix. Reﬁnement
Regularization

FlyingChairs (train)
Sintel clean (train)
Sintel ﬁnal (train)
KITTI12 (train)
KITTI15 (train)

M
(cid:55)
(cid:51)
(cid:55)
(cid:55)

3.75
4.70
5.69
9.22
18.24

MS
(cid:55)
(cid:51)
(cid:51)
(cid:55)

2.70
4.17
5.30
8.01
16.19

WM WSR WMS
(cid:51)
(cid:55)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:55)

(cid:51)
(cid:51)
(cid:55)
(cid:55)

2.98
3.54
4.81
6.17
14.52

1.63
3.19
4.63
5.03
13.20

1.82
2.90
4.45
4.83
12.32

ALL
(cid:51)
(cid:51)
(cid:51)
(cid:51)

1.57
2.78
4.17
4.56
11.58

than FlowNet2 [14] and is 1.36 times faster in runtime.
LiteFlowNetX, a variant of LiteFlowNet having a smaller
model size and without descriptor matching, has about 43
times fewer parameters than FlowNetC [9] and a compa-
rable runtime. LiteFlowNetX also has 1.33 times fewer pa-
rameters than SPyNet [21]. LiteFlowNet and its variants are
currently the most compact CNNs for ﬂow estimation.

4.3. Ablation Study

We

investigate

in
role of
LiteFlowNet-pre trained on Chairs by evaluating the per-

each component

the

7

Image overlay

Ground truth

FlowNetC [9]

FlowNet2 [14]

LiteFlowNet

First image

FlowNetC [9]

FlowNet2 [14]

FlowNet2-ft-sintel [14]

LiteFlowNet-ft

Figure 5: Examples of ﬂow ﬁelds from different methods on Sintel training sets for clean (top row), ﬁnal (middle row) passes, and the
testing set for ﬁnal pass (last row). Fine details are well preserved and less artifacts can be observed in the ﬂow ﬁelds of LiteFlowNet.

Image overlay

Ground truth

FlowNetC [9]

FlowNet2 [14]

LiteFlowNet

First Image

FlowNetC [9]

FlowNet2 [14]

FlowNet2-ft-kitti [14]

LiteFlowNet-ft

Figure 6: Examples of ﬂow ﬁelds from different methods on the training set (top) and the testing set (bottom) of KITTI15.

formance of different variants with some of the components
disabled. The AEE results are summarized in Table 4 and
examples of ﬂow ﬁelds are illustrated in Figure 7.

Feature Warping. We consider two variants LiteFlowNet-
pre (WM and WMS) and compare them to the counterparts
with warping disabled (M and MS). Flow ﬁelds from M and
MS are more vague. Large degradation in AEE is noticed
especially for KITTI12 (33%) and KITTI15 (25%). With
feature warping, pyramidal features that input to ﬂow infer-
ence are closer to each other. This facilitates ﬂow estimation
in subsequent pyramid level by computing residual ﬂow.

Descriptor Matching. We compare the variant WSR with-
out descriptor matching for which the ﬂow inference part
is made as deep as that in the unamended LiteFlowNet-
pre (ALL). No noticeable difference between the ﬂow ﬁelds
from WSR and ALL. Since the maximum displacement of
the example ﬂow ﬁeld is not very large (only 14.7 pixels),
accurate ﬂow ﬁeld can still be yielded from WSR. For eval-
uation covering a wide range of ﬂow displacement (espe-
cially large-displacement benchmark, KITTI), degradation
in AEE is noticed for WSR. This suggests that descriptor
matching is useful in addressing large-displacement ﬂow.

Sub-Pixel Reﬁnement. The ﬂow ﬁeld generated from
WMS is more crisp and contains more ﬁne details than
that generated from WM with sub-pixel reﬁnement dis-
abled. Less small-magnitude ﬂow artifacts (represented by
light color on the background) are also observed. Besides,
WMS achieves smaller AEE. Since descriptor matching es-
tablishes pixel-by-pixel correspondence, sub-pixel reﬁne-
ment is necessary to yield detail-preserving ﬂow ﬁeld.
Regularization. In comparison WMS with regularization
disabled to ALL, undesired artifacts exist in homogeneous
regions (represented by very dim color on the background)
of the ﬂow ﬁeld generated from WMS. Flow bleeding and
vague ﬂow boundaries are observed. Degradation in AEE is
also noticed. This suggests that the proposed feature-driven
local convolution (f-lcon) plays the vital role to smooth ﬂow
ﬁeld and maintain crisp ﬂow boundaries as regularization
term in conventional variational methods.

5. Conclusion

We have presented a compact network for accurate ﬂow
estimation. LiteFlowNet outperforms FlowNet [9] and is on
par with or outperforms the state-of-the-art FlowNet2 [14]

8

Figure 7: Examples of ﬂow ﬁelds from different variants of LiteFlowNet-pre trained on Chairs with some of the components disabled.
LiteFlowNet-pre is denoted as “All”. W = Feature Warping, M = Descriptor Matching, S = Sub-Pixel Reﬁnement, R = Regularization.

on public benchmarks while being faster in runtime and
30 times smaller in model size. Pyramidal feature extrac-
tion and feature warping (f-warp) help us to break the de
facto rule of accurate ﬂow network requiring large model
size. To address large-displacement and detail-preserving
ﬂows, LiteFlowNet exploits short-range matching to gener-
ate pixel-level ﬂow ﬁeld and further improves the estimate
to sub-pixel accuracy in the cascaded ﬂow inference. To
result crisp ﬂow boundaries, LiteFlowNet regularizes ﬂow
ﬁeld through feature-driven local convolution (f-lcon). With
its lightweight, accurate, and fast ﬂow computation, we ex-
pect that LiteFlowNet can be deployed to many applications
such as motion segmentation, action recognition, SLAM,
3D reconstruction and more.

Acknowledgement. This work is supported by SenseTime
Group Limited and the General Research Fund sponsored
by the Research Grants Council of the Hong Kong SAR
(CUHK 14241716, 14224316, 14209217).

6. Appendix

LiteFlowNet consists of two compact sub-networks,
namely NetC and NetE. NetC is a two-steam network in
which the two network streams share the same set of ﬁlters.
The input to NetC is an image pair (I1, I2). The network
architectures of the 6-level NetC and NetE at pyramid level
5 are provided in Table 5 and Tables 6 to 8, respectively.
We use sufﬁxes “M”, “S” and “R” to highlight the layers
that are used in descriptor matching, sub-pixel reﬁnement,
and ﬂow regularization units in NetE, respectively. We de-
clare a layer as “ﬂow” to highlight when the output is a
ﬂow ﬁeld. Our code and trained models are available at
https://github.com/twhui/LiteFlowNet. A
video clip (https://www.youtube.com/watch?v=
pfQ0zFwv-hM) and a supplementary material are avail-
able on our project page (http://mmlab.ie.cuhk.
edu.hk/projects/LiteFlowNet/) to showcase the
performance of LiteFlowNet and the effectiveness of the
proposed components in our network.

References

[1] C. Bailer, B. Taetz, and D. Stricker. Flow Fields: Dense
correspondence ﬁelds for highly accurate large displacement
optical ﬂow estimation. ICCV, pages 4015–4023, 2015. 3, 7
[2] C. Bailer, K. Varanasi, and D. Stricker. CNN-based patch
matching for optical ﬂow with thresholded hinge embedding
loss. CVPR, pages 3250–3259, 2017. 3, 7

[3] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and
R. Szeliski. A database and evaluation methodology for op-
tical ﬂow. IJCV, 92(1):1–31, 2011. 6

[4] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man. PatchMatch: A randomized correspondence algorithm
SIGGRAGH, pages 83–97,
for structural image editing.
2009. 2

[5] M. J. Black, Y. Yacoobt, A. D. Jepsont, and D. J. Fleets.
Learning parameterized models of image motion. CVPR,
pages 674–679, 1997. 2, 3

[6] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
ECCV, pages 25–36, 2004. 1, 2, 3

[7] T. Brox and J. Mailk. Large displacement optical ﬂow: De-
scriptor matching in variational motion estimation. PAMI,
33(3):500–513, 2011. 1, 2, 7

[8] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation.
ECCV, pages 611–625, 2012. 6

[9] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical ﬂow with convolutional net-
works. ICCV, pages 2758–2766, 2015. 1, 2, 3, 4, 6, 7, 8
[10] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? CVPR, pages 3354–3361, 2012. 6
[11] F. Gney and A. Geiger. Deep discrete ﬂow. ACCV, pages

207–224, 2016. 3, 7

[12] B. K. P. Horn and B. G. Schunck. Determining optical ﬂow.

Ariﬁcal Intelligence, 17:185–203, 1981. 1, 2

[13] T.-W. Hui and R. Chung. Determining motion directly
from normal ﬂows upon the use of a spherical eye platform.
CVPR, pages 2267–2274, 2013. 1

[14] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet2.0: Evolution of optical ﬂow estimation
with deep networks. CVPR, pages 2462–2470, 2017. 1, 2, 3,
4, 6, 7, 8

9

Table 5: The network details of NetC in LiteFlowNet. “# Ch. In / Out” means the number of channels of the input or the output features.
“conv” denotes convolution.

Layer name
conv1
conv2 1
conv2 2
conv2 3
conv3 1
conv3 2
conv4 1
conv4 2
conv5
conv6

Kernel
7×7
3×3
3×3
3×3
3×3
3×3
3×3
3×3
3×3
3×3

Stride
1
2
1
1
2
1
2
1
2
2

# Ch. In / Out
3 / 32
32 / 32
32 / 32
32 / 32
32 / 64
64 / 64
64 / 96
96 / 96
96 / 128
128 / 192

Input
I1 or I2
conv1
conv2 1
conv2 2
conv2 3
conv3 1
conv3 2
conv4 1
conv4 2
conv5

Table 6: The network details of the descriptor matching unit (M) of NetE in LiteFlowNet at pyramid level 5. “upconv”, “f-warp”, “corr”,
and “loss” denote the fractionally strided convolution (so-called deconvolution), feature warping, correlation, and the layer where training
loss is applied, respectively. Furthermore, “conv5a’ and “conv5b” denote the high-dimensional features of images I1 and I2 generated
from NetC at pyramid level 5.

Layer name
upconv5 M
f-warp5 M
corr5 M
conv5 1 M
conv5 2 M
conv5 3 M
conv5 4 M

Kernel
4×4
-
1×1
3×3
3×3
3×3
3×3

Stride
0.5
-
1
1
1
1
1

# Ch. In / Out
2 / 2
(128, 2) / 128
(128, 128) / 49
49 / 128
128 / 64
64 / 32
32 / 2
(2, 2) / 2

Input(s)
ﬂow6 R
conv5b, upconv5 M
conv5a, f-warp5 M
corr5 M
conv5 1 M
conv5 2 M
conv5 3 M
upconv5 M, conv5 4 M

ﬂow5 M, loss5 M element-wise sum

[15] M.

Jaderberg, K. Simonyan, A. Zisserman,

and
Spatial transformer networks. NIPS,

K. Kavukcuoglu.
pages 2017–2025, 2015. 3, 4

[16] J. Lu, H. Yang, D. Min, and M. N. Do. PatchMatch Fil-
ter: Efﬁcient edge-aware ﬁltering meets randomized search.
CVPR, pages 1854–1861, 2013. 2

[17] N. Mayer, E. Ilg, P. Husser, P. Fischer, D. Cremers, A. Doso-
vitskiy, and T. Brox. A large dataset to train convolutional
networks for disparity, optical ﬂow, and scene ﬂow estima-
tion. CVPR, pages 4040–4048, 2016. 6, 7

[18] M. Menze and A. Geiger. Object scene ﬂow for autonomous

vehicles. CVPR, pages 3061–3070, 2015. 6
[19] T. Nir, A. M. Bruckstein, and R. Kimmel.

Over-
parameterized variational optical ﬂow. IJCV, 76(2):205–216,
2008. 3

[20] N. Papenberg, A. Bruhn, T. Brox, S. Didas, and J. Weick-
ert. Highly accurate optic ﬂow computation with theoreti-
cally justiﬁed warping. IJCV, 67(2):141–158, 2006. 1, 3
[21] A. Ranjan and M. J. Black. Optical ﬂow estimation using a
spatial pyramid network. CVPR, pages 4161–4170, 2017. 1,
2, 3, 6, 7

[22] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. CVPR, pages 1164–1172, 2015. 1,
2, 6, 7

[23] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. MIC-
CAI, pages 234–241, 2015. 2

[24] D. Rosenbaum, D. Zoran, and Y. Weiss. Learning the local

statistics of optical ﬂow. NIPS, pages 2373–2381, 2013. 3

[25] S. Roth and M. Black. On the spatial statistics of optical

ﬂow. ICCV, pages 42–49, 2005. 3

[26] S. Roth and M. J. Black. Fields of experts: A framework for
learning image priors. CVPR, pages 860–867, 2005. 3

[27] D. Sun, S. Roth, and M. J. Black. A quantitative analysis of
current practices in optical ﬂow estimation and the principles
behind them. IJCV, 106(2):115–137, 2014. 1, 6, 7

[28] D. Sun, S. Roth, J. Lewis, and M. J. Black. Learning optical

ﬂow. ECCV, pages 83–97, 2008. 2, 3

[29] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace:
Closing the gap to human-level performance in face veriﬁca-
tion. CVPR, pages 1701–1708, 2014. 2, 5

[30] D. Tschumperl´e and R. Deriche. Vector-valued image reg-
ularization with PDEs: A common framework for different
applications. PAMI, 27(4):506–517, 2005. 5

[31] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
DeepFlow: Large displacement optical ﬂow with deep
matching. ICCV, pages 500–513, 2013. 2, 3, 7

10

Table 7: Network details of the sub-pixel reﬁnement unit (S) of NetE in LiteFlowNet at pyramid level 5.

Layer name
f-warp5 S
conv5 1 S
conv5 2 S
conv5 3 S
conv5 4 S
ﬂow5 S, loss5 S

Kernel
-
3×3
3×3
3×3
3×3
element-wise sum

Stride
-
1
1
1
1

# Ch. In / Out
(128, 2) / 128
258 / 128
128 / 64
64 / 32
32 / 2
(2, 2) / 2

Input(s)
conv5b, ﬂow5 M
conv5a, f-warp5 S, ﬂow5 M
conv5 1 S
conv5 2 S
conv5 3 S
ﬂow5 M, conv5 4 S

Table 8: Network details of the ﬂow regularization unit (R) of NetE in LiteFlowNet at pyramid level 5. “rgb-warp”, “norm”, “negsq”,
“softmax”, and “f-lcon” denote the image warping, L2 norm of the RGB brightness difference between the two input images, negative-
square, normalized exponential operation over each 1 × 1 × (# Ch. In) column in the 3-D tensor, and feature-driven local convolution,
respectively. Furthermore, “conv dist” that highlights the output of the convolution layer is used as the feature-driven distance metric D
Eq. (7) in the main manuscript. “im5a” and “im5b” denote the down-sized images of I1 and I2 at pyramid level 5, respectively

Layer name
rm-ﬂow5 R
rgb-warp5 R
norm5 R
conv5 1 R
conv5 2 R
conv5 3 R
conv5 4 R
conv5 5 R
conv5 6 R
conv5 dist R
negsq5 R
softmax5 R
f-lcon5 R (Out: ﬂow5 R), loss5 R

Stride

Kernel
remove mean
-

-
L2 norm

1
1
1
1
1
1
1

3×3
3×3
3×3
3×3
3×3
3×3
3×3
negative-square
1×1×9
3×3

1
1

# Ch. In / Out
2 / 2
(3, 2) / 3
(3, 3) / 1
131 / 128
128 / 128
128 / 64
64 / 64
64 / 32
32 / 32
32 / 9
9 / 9
9 / 9
(9, 2) / 2

Input(s)
ﬂow5 S
im5b, ﬂow5 S
im5a, rgb-warp5 R
conv5a, rm-ﬂow5 R, norm5 R
conv5 1 R
conv5 2 R
conv5 3 R
conv5 4 R
conv5 5 R
conv5 6 R
conv5 dist R
negsq5 R
softmax5 R, ﬂow5 S

[32] M. Werlberger, W. Trobin, T. Pock, A. Wedel, D. Cremers,
and H. Bischof. Anisotropic Huber-L1 optical ﬂow. BMVC,
2009. 2, 5

[33] J. Wulff and M. J. Black. Efﬁcient sparse-to-dense optical
ﬂow estimation using a learned basis and layers. CVPR,
pages 120–130, 2015. 3, 7

[34] J. Xu, R. Ranftl, and V. Koltun. Accurate optical ﬂow via
direct cost volume processings. CVPR, pages 1289–1297,
2017. 6, 7

[35] S. Zagoruyko and N. Komodakis. Learning to compare im-
age patches via convolutional neural networks. CVPR, pages
4353–4361, 2015. 3

[36] H. Zimmer, A. Bruhn, and J. Weickert. Optic ﬂow in har-

mony. IJCV, 93(3):368–388, 2011. 1, 2, 5

[37] S. Zweig and L. Wolf. InterpoNet, A brain inspired neural
network for optical ﬂow dense interpolation. CVPR, pages
4563–4572, 2017. 3

11

LiteFlowNet: A Lightweight Convolutional Neural Network
for Optical Flow Estimation

Tak-Wai Hui, Xiaoou Tang, Chen Change Loy
CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong
{twhui,xtang,ccloy}@ie.cuhk.edu.hk

8
1
0
2
 
y
a
M
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
6
3
0
7
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

FlowNet2 [14], the state-of-the-art convolutional neural
network (CNN) for optical ﬂow estimation, requires over
160M parameters to achieve accurate ﬂow estimation. In
this paper we present an alternative network that outper-
forms FlowNet2 on the challenging Sintel ﬁnal pass and
KITTI benchmarks, while being 30 times smaller in the
model size and 1.36 times faster in the running speed. This
is made possible by drilling down to architectural details
that might have been missed in the current frameworks: (1)
We present a more effective ﬂow inference approach at each
pyramid level through a lightweight cascaded network. It
not only improves ﬂow estimation accuracy through early
correction, but also permits seamless incorporation of de-
scriptor matching in our network. (2) We present a novel
ﬂow regularization layer to ameliorate the issue of outliers
and vague ﬂow boundaries by using a feature-driven lo-
cal convolution. (3) Our network owns an effective struc-
ture for pyramidal feature extraction and embraces fea-
ture warping rather than image warping as practiced in
FlowNet2. Our code and trained models are available at
https://github.com/twhui/LiteFlowNet.

1. Introduction

Optical ﬂow estimation is a long-standing problem in
computer vision. Due to the well-known aperture problem,
optical ﬂow is not directly measurable [12, 13]. Hence, the
estimation is typically solved by energy minimization in a
coarse-to-ﬁne framework [6, 20, 7, 36, 27, 22]. This class
of techniques, however, involves complex energy optimiza-
tion and thus it is not scalable for applications that demand
real-time estimation.

FlowNet [9] and its successor FlowNet2 [14], have
marked a milestone by using CNN for optical ﬂow esti-
mation. Their accuracies especially the successor are ap-
proaching that of state-of-the-art energy minimization ap-
proaches, while the speed is several orders of magnitude

Figure 1: Examples demonstrate the effectiveness of the proposed
components in LiteFlowNet for i) feature warping, ii) cascaded
ﬂow inference, and iii) ﬂow regularization. Enabled components
are indicated with bold black fonts.

faster. To push the envelop of accuracy, FlowNet2 is de-
signed as a cascade of variants of FlowNet that each net-
work in the cascade reﬁnes the preceding ﬂow ﬁeld by con-
tributing on the ﬂow increment between the ﬁrst image and
the warped second image. The model, as a result, com-
prises over 160M parameters, which could be formidable in
many applications. A recent network termed SPyNet [21]
attempts a network with smaller size of 1.2M parameters
by adopting image warping in each pyramid level. Nonethe-
less, the accuracy can only match that of FlowNet but not
FlowNet2. The objective of this study is to explore alter-
native CNN architectures for accurate ﬂow estimation yet
with high efﬁciency. Our work is inspired by the successes
of FlowNet2 and SPyNet, but we further drill down the key
elements to fully unleash the potential of deep convolutional
network combined with classical principles.

There are two general principles to improve the design
of FlowNet2 and SPyNet. The ﬁrst principle is pyrami-
dal feature extraction. The proposed network, dubbed Lite-
FlowNet, consists of an encoder and a decoder. The encoder
maps the given image pair, respectively, into two pyramids

1

of multi-scale high-dimensional features. The decoder then
estimates the ﬂow ﬁeld in a coarse-to-ﬁne framework. At
each pyramid level, the decoder infers the ﬂow ﬁeld by se-
lecting and using the features of the same resolution from
the feature pyramids. This design leads to a lighter network
compared to FlowNet2 that adopts U-Net architecture [23]
for ﬂow inference. In comparison to SPyNet, our network
separates the process of feature extraction and ﬂow estima-
tion. This helps us to better pinpoint the bottleneck of accu-
racy and model size.

The second general principle is feature warping.
FlowNet2 and SPyNet warp the second image towards the
ﬁrst image in the pair using the previous ﬂow estimate, and
then reﬁne the estimate using the feature maps generated by
the warped and the ﬁrst images. Warping an image and then
generating the feature maps of the warped image are two
ordered steps. We ﬁnd that the two steps can be reduced to
a single one by directly warping the feature maps of the sec-
ond image, which have been computed by the encoder. This
one-step feature warping process reduces the more discrimi-
native feature-space distance instead of the RGB-space dis-
tance between the two images. This makes our network
more powerful and efﬁcient in addressing the ﬂow problem.
We now highlight the more speciﬁc differences between
our network and existing CNN-based optical ﬂow estima-
tion frameworks:
1) Cascaded ﬂow inference – At each pyramid level, we in-
troduce a novel cascade of two lightweight networks. Each
of them has a feature warping (f-warp) layer to displace
the feature maps of the second image towards the ﬁrst im-
age using the ﬂow estimate from the previous level. Flow
residue is computed to further reduce the feature-space dis-
tance between the images. This design is advantageous to
the conventional design of using a single network for ﬂow
inference. First, the cascade progressively improves ﬂow
accuracy thus allowing an early correction of the estimate
without passing more errors to the next level. Second, this
design allows seamless integration with descriptor match-
ing. We assign a matching network to the ﬁrst inference.
Consequently, pixel-accuracy ﬂow ﬁeld can be generated
ﬁrst and then reﬁned to sub-pixel accuracy in the subse-
quent inference network. Since at each pyramid level the
feature-space distance between the images has been reduced
by feature warping, we can use a rather short displace-
ment than [9, 14] to establish the cost volume. Besides,
matching is performed only at sampled positions and thus a
sparse cost-volume is aggregated. This effectively reduces
the computational burden raised by the explicit matching.
2) Flow regularization – The cascaded ﬂow inference re-
sembles the role of data ﬁdelity in energy minimization
methods. Using data term alone, vague ﬂow boundaries
and undesired artifacts exist in ﬂow ﬁelds. To tackle this
problem, local ﬂow consistency and co-occurrence between

ﬂow boundaries and intensity edges are commonly used as
the cues to regularize ﬂow ﬁeld. Some of the representative
methods include anisotropic image-driven [32], image- and
ﬂow-driven [28], and complementary [36] regularizations.
After cascaded ﬂow inference, we allow the ﬂow ﬁeld to be
further regularized by our novel feature-driven local convo-
lution (f-lconv) layer1 at each pyramid level. The kernels of
such a local convolution are adaptive to the pyramidal fea-
tures from the encoder, ﬂow estimate and occlusion prob-
ability map. This makes the ﬂow regularization to be both
ﬂow- and image-aware. To our best knowledge, state-of-
the-art CNNs do not explore such a ﬂow regularization.

The effectiveness of the aforementioned contributions
are depicted in Figure 1. In summary, we propose a compact
LiteFlowNet to estimate optical ﬂow. Our network inno-
vates the useful elements from conventional methods. e.g.,
brightness constraint in data ﬁdelity to pyramidal CNN fea-
tures and image warping to CNN feature warping. More
speciﬁcally, we present a cascaded ﬂow inference with fea-
ture warping and ﬂow regularization in each pyramid level,
which are new in the literature. Overall, our network out-
performs FlowNet [9] and SPyNet [21] and is on par with
or outperforms the recent FlowNet2 [14] on public bench-
marks, while having 30 times fewer parameters and being
1.36 times faster than FlowNet2.

2. Related Work

Here, we brieﬂy review some of the major approaches

for optical ﬂow estimation.

Variational methods. Since the pioneering work by Horn
and Schunck [12], variational methods have dominated op-
tical ﬂow estimation. Brox et al. address illumination
changes by combining the brightness and gradient con-
stancy assumptions [6]. Brox et al. integrate rich descriptors
into variational formulation [7]. In DeepFlow [31], Weinza-
epfel et al. propose to correlate multi-scale patches and in-
corporate this as the matching term in functional. In Patch-
Match Filter [16], Lu et al. establish dense correspondence
using the superpixel-based PatchMatch [4]. Revaud et al.
propose a method EpicFlow that uses externally matched
ﬂows as initialization and then performs interpolation [22].
Zimmer et al. design the complementary regularization that
exploits directional information from the constraints im-
posed in data term [36]. Our network that infers optical
ﬂow and performs ﬂow regularization is inspired by the use
of data ﬁdelity and regularization in variational methods.

Machine learning methods. Black et al. propose to repre-
sent complex image motion as a linear combination of the
learned basis vectors [5]. Roth et al. formulates the prior

1We name it as feature-driven local convolution (f-lconv) layer in order
to distinguish it from local convolution (lconv) layer of which ﬁlter weights
are locally ﬁxed in conventional CNNs [29].

2

probability of ﬂow ﬁeld as Field-of-Experts model [26] that
captures higher order spatial statistics [25]. Sun et al. study
the probabilistic model of brightness inconstancy in a high-
order random ﬁeld framework [28]. Nir et al. represent
image motion using the over-parameterization model [19].
Rosenbaum et al. model the local statistics of optical ﬂow
using Gaussian mixtures [24]. Given a set of sparse
matches, Wulff et al. propose to regress them to a dense
ﬂow ﬁeld using a set of basis ﬂow ﬁelds (PCA-Flow) [33].
It can be shown that the parameterized model [5, 19, 33]
can be efﬁciently implemented using CNN.
CNN-based methods. In the work of Fischer et al. termed
FlowNet [9], a post-processing step that involves energy
minimization is required to reduce smoothing effect across
ﬂow boundaries. This process is not end-to-end trainable.
In our work, we present an end-to-end approach that per-
forms in-network ﬂow regularization using the proposed
f-lconv layer, which plays similar role as the regulariza-
tion term in variational methods.
In FlowNet2 [14], Ilg
et al. introduce a huge network cascade (over 160M pa-
rameters) that consists of variants of FlowNet. The cas-
cade improves ﬂow accuracy with an expense of model size
and computational complexity. Our model uses a more ef-
ﬁcient architecture containing 30 times fewer parameters
than FlowNet2 while the performance is on par with it. A
compact network termed SPyNet [21] from Ranjan et al. is
inspired from spatial pyramid. Nevertheless, the accuracy is
far below FlowNet2. A small-sized variant of our network
outperforms SPyNet while being 1.33 times smaller in the
model size. Zweig et al. present a network to interpolate
third-party sparse ﬂows but requiring off-the-shelf edge de-
tector [37]. DeepFlow [31] that involves convolution and
pooling operations is however not a CNN, since the “ﬁlter
weights” are non-trainable image patches. According to the
terminology used in FlowNet, DeepFlow uses correlation.

An alternative approach for establishing point correspon-
dence is to match image patches. Zagoruyko et al. ﬁrst in-
troduce to CNN-feature matching [35]. G¨uney et al. ﬁnd
feature representation and formulate optical ﬂow estimation
in MRF [11]. Bailer et al. [2] use multi-scale features and
then perform feature matching as Flow Fields [1]. Although
pixel-wise matching can establish accurate point correspon-
dence, the computational demand is too high for practical
use (it takes several seconds even a GPU is used). As a
tradeoff, Fischer et al. [9] and Ilg et al. [14] perform feature
matching only at a reduced spatial resolution. We reduce
the computational burden of feature matching by using a
short-ranged matching of warped CNN features at sampled
positions and a sub-pixel reﬁnement at every pyramid level.
We are inspired by the feature transformation used in
Spatial Transformer [15]. Our network uses the proposed
f-warp layer to displace each channel2 of the given vector-

2We can also use f-warp layer to displace each channel differently when

valued feature according to the provided ﬂow ﬁeld. Unlike
Spatial Transformer, f-warp layer is not fully constrained
and is a relaxed version of it as the ﬂow ﬁeld is not param-
eterized. While transformation in FlowNet2 and SPyNet is
limited to images, our decider network is a more generic
warping network that warps high-level CNN features.

3. LiteFlowNet

LiteFlowNet is composed of two compact sub-networks
that are specialized in pyramidal feature extraction and op-
tical ﬂow estimation as shown in Figure 2. Since the spatial
dimension of feature maps is contracting in feature extrac-
tion and that of ﬂow ﬁelds is expanding in ﬂow estimation,
we call the two sub-networks as NetC and NetE respec-
tively. NetC transforms any given image pair into two pyra-
mids of multi-scale high-dimensional features. NetE con-
sists of cascaded ﬂow inference and regularization modules
that estimate coarse-to-ﬁne ﬂow ﬁelds.

Pyramidal Feature Extraction. As shown in Figure 2,
NetC is a two-stream network in which the ﬁlter weights
are shared across the two streams. Each of them functions
as a feature descriptor that transforms an image I to a pyra-
mid of multi-scale high-dimensional features {Fk(I)} from
the highest spatial resolution (k = 1) to the lowest spatial
resolution (k = L). The pyramidal features are generated
by stride-s convolutions with the reduction of spatial reso-
lution by a factor s up the pyramid. In the following, we
omit the subscript k that indicates the level of pyramid for
brevity. We use Fi to represent CNN features for Ii. When
we discuss the operations in a pyramid level, the same op-
erations are applicable to other levels.

Feature Warping. At each pyramid level, a ﬂow ﬁeld is
inferred from high-level features F1 and F2 of images I1
and I2. Flow inference becomes more challenging if I1 and
I2 are captured far away from each other. With the motiva-
tion of image warping used in conventional methods [6, 20]
and recent CNNs [14, 21] for addressing large-displacement
ﬂow, we propose to reduce feature-space distance between
F1 and F2 by feature warping (f-warp). Speciﬁcally, F2
is warped towards F1 by f-warp via ﬂow estimate ˙x to
(cid:101)F2(x) (cid:44) F2(x + ˙x) ∼ F1(x). This allows our network to
infer residual ﬂow between F1 and (cid:101)F2 that has smaller ﬂow
magnitude (more details in Section 3.1) but not the com-
plete ﬂow ﬁeld that is more difﬁcult to infer. Unlike con-
ventional methods, f-warp is performed on high-level CNN
features but not on images. This makes our network more
powerful and efﬁcient in addressing the optical ﬂow prob-
lem. To allow end-to-end training, F is interpolated to (cid:101)F

multiple ﬂow ﬁelds are supplied. The usage, however, is beyond the scope
of this work.

3

Figure 2: The network structure of LiteFlowNet. For the ease of representation, only a 3-level design is shown. Given an image pair (I1
and I2), NetC generates two pyramids of high-level features (Fk(I1) in pink and Fk(I2) in red, k ∈ [1, 3]). NetE yields multi-scale ﬂow
ﬁelds that each of them is generated by a cascaded ﬂow inference module M :S (in blue color, including a descriptor matching unit M and
a sub-pixel reﬁnement unit S) and a regularization module R (in green color). Flow inference and regularization modules correspond to
data ﬁdelity and regularization terms in conventional energy minimization methods respectively.

for any sub-pixel displacement ˙x as follows:

(cid:101)F(x) =

(cid:88)

F(xi

s) (cid:0)1 − (cid:12)

(cid:12)xs − xi
s

(cid:12)
(cid:12)

(cid:1) (cid:0)1 − (cid:12)

(cid:12)ys − yi
s

(cid:1) ,

(cid:12)
(cid:12)

xi

s∈N (xs)

(1)
where xs = x + ˙x = (xs, ys)(cid:62) denotes the source coor-
dinates in the input feature map F that deﬁnes the sample
point, x = (x, y)(cid:62) denotes the target coordinates of the
regular grid in the interpolated feature map (cid:101)F, and N (xs)
denotes the four pixel neighbors of xs. The above bilinear
interpolation allows back-propagation during training as its
gradients can be efﬁciently computed [15].

3.1. Cascaded Flow Inference

At each pyramid level of NetE, pixel-by-pixel matching
of high-level features yields coarse ﬂow estimate. A subse-
quent reﬁnement on the coarse ﬂow further improves it to
sub-pixel accuracy.
First Flow Inference (descriptor matching). Point cor-
respondence between I1 and I2 is established through com-
puting correlation of high-level feature vectors in individual
pyramidal features F1 and F2 as follows:

c(x, d) = F1(x) · F2(x + d)/N,

(2)

where c is the matching cost between point x in F1 and
point x + d in F2, d ∈ Z is the displacement vector from x,
and N is the length of the feature vector. A cost volume C
is built by aggregating all the matching costs into a 3D grid.
We reduce the computational burden raised by cost-
volume processing [9, 14] in three ways: 1) We perform
short-range matching at every pyramid level instead of long-
range matching at a single level. 2) We reduce feature-space

distance between F1 and F2 by warping F2 towards F1
using our proposed f-warp through ﬂow estimate3 ˙x from
previous level. 3) We perform matching only at the sam-
pled positions in the pyramid levels of high-spatial resolu-
tion. The sparse cost volume is interpolated in the spatial
dimension to ﬁll the missed matching costs for the unsam-
pled positions. The ﬁrst two techniques effectively reduce
the searching space needed, while the last technique reduces
the frequency of matching per pyramid level.

In the descriptor matching unit M , residual ﬂow ∆ ˙xm
is inferred by ﬁltering the cost volume C as illustrated in
Figure 3. A complete ﬂow ﬁeld ˙xm is computed as follows:

˙xm = M (cid:0)C(F1, (cid:101)F2; d)(cid:1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
∆ ˙xm

+s ˙x↑s.

(3)

Second Flow Inference (sub-pixel reﬁnement). Since
the cost volume in descriptor matching unit is aggregated
by measuring pixel-by-pixel correlation, ﬂow estimate ˙xm
from the previous inference is only up to pixel-level accu-
racy. We introduce the second ﬂow inference in the wake
of descriptor matching as shown in Figure 3.
It aims to
reﬁne the pixel-level ﬂow ﬁeld ˙xm to sub-pixel accuracy.
This prevents erroneous ﬂows being ampliﬁed by upsam-
pling and passing to the next pyramid level. Speciﬁcally,
F2 is warped to (cid:101)F2 via ﬂow estimate ˙xm. Sub-pixel reﬁne-
ment unit S yields a more accurate ﬂow ﬁeld ˙xs by mini-
mizing feature-space distance between F1 and (cid:101)F2 through

3 ˙x from previous level needs to be upsampled in spatial resolution (de-
noted by “↑s”) and magnitude (multiplied by a scalar s) to s ˙x↑s for match-
ing the spatial resolution of the pyramidal features at the current level.

4

Figure 4: Folding and packing of f-lcon ﬁlters {g}. The (x, y)-
entry of 3D tensor ¯G(c) is a 3D column with size 1 × 1 × w2.
It corresponds to the unfolded w × w f-lcon ﬁlter g(x, y, c) to be
applied at position (x, y) of channel c in vector-valued feature F .

ﬂow inference, we replace F to ˙xs. Flow regularization
module R is deﬁned as follows:

˙xr = R( ˙xs; G).

(6)

The f-lcon ﬁlters need to be specialized for smoothing
ﬂow ﬁeld. It should behave as an averaging ﬁlter if the vari-
ation of ﬂow vectors over the patch is smooth. It should also
not over-smooth ﬂow ﬁeld across ﬂow boundary. We deﬁne
a feature-driven CNN distance metric D that estimates lo-
cal ﬂow variation using pyramidal feature F1, ﬂow ﬁeld ˙xs
from the cascaded ﬂow inference, and occlusion probabil-
ity map4 O. In summary, D is adaptively constructed by a
CNN unit RD as follows:

D = RD(F1, ˙xs, O).

(7)

With the introduction of feature-driven distance metric D,
each ﬁlter g of f-lcon is constructed as follows:

g(x, y, c) =

exp(−D(x, y, c)2)
(xi,yi)∈N (x,y) exp(−D(xi, yi, c)2)

,

(cid:80)

(8)

where N (x, y) denotes the neighborhood containing ω × ω
pixels centered at position (x, y).

Here, we provide a mechanism to perform f-lcon ef-
ﬁciently. For a C-channel input F , we use C tensors
¯G(1), ..., ¯G(C) to store f-lcon ﬁlter set G. As illustrated
in Figure 4, each f-lcon ﬁlter g(x, y, c) is folded into a
1 × 1 × w2 3D column and then packed into the (x, y)-
entry of a M × N × w2 3D tensor ¯G(c). Same folding and
packing operations are also applied to each patch in each
channel of F . This results C tensors ¯F (1), ..., ¯F (C) for F .
In this way, Equation (5) can be reformulated to:

Fg(c) = ¯G(c) (cid:12) ¯F (c),

(9)

where “(cid:12)” denotes element-wise dot product between the
corresponding columns of the tensors. With the abuse of

4We use the brightness error ||I2(x+ ˙x)−I1(x)||2 between the warped

second image and the ﬁrst image as the occlusion probability map.

Figure 3: A cascaded ﬂow inference module M :S in NetE. It con-
sists of a descriptor matching unit M and a sub-pixel reﬁnement
unit S. In M , f-warp transforms high-level feature F2 to (cid:101)F2 via
upscaled ﬂow ﬁeld 2 ˙x↑2 estimated at previous pyramid level. In
S, F2 is warped by ˙xm from M . In comparison to residual ﬂow
∆ ˙xm, more ﬂow adjustment exists at ﬂow boundaries in ∆ ˙xs.

computing residual ﬂow ∆ ˙xs as the following:

˙xs = S(cid:0)F1, (cid:101)F2, ˙xm

+ ˙xm.

(4)

(cid:1)

(cid:125)

(cid:124)

(cid:123)(cid:122)
∆ ˙xs

3.2. Flow Regularization

Cascaded ﬂow inference resembles the role of data ﬁ-
delity in conventional minimization methods. Using data
term alone, vague ﬂow boundaries and undesired artifacts
commonly exist in ﬂow ﬁeld [32, 36]. To tackle this prob-
lem, we propose to use a feature-driven local convolution
(f-lcon) to regularize ﬂow ﬁeld from the cascaded ﬂow in-
ference. The operation of f-lcon is well-governed by the
Laplacian formulation of diffusion of pixel values [30]. In
contrast to local convolution (lcon) used in conventional
CNNs [29], f-lcon is more generalized. Not only is a dis-
tinct ﬁlter used for each position of feature map, but the
ﬁlter is adaptively constructed for individual ﬂow patches.
Consider a general case, a vector-valued feature F that
has to be regularized has C channels and a spatial dimen-
sion M × N . Deﬁne G = {g} as the set of ﬁlters used in
f-lcon layer. The operation of f-lcon to F can be formulated
as follow:

fg(x, y, c) = g(x, y, c) ∗ f (x, y, c),

(5)

where “∗” denotes convolution, f (x, y, c) is a w × w patch
centered at position (x, y) of channel c in F , g(x, y, c) is the
corresponding w × w regularization ﬁlter, and fg(x, y, c) is
a scalar output for x = (x, y)(cid:62) and c = 1, 2, ..., C. To
be speciﬁc for regularizing ﬂow ﬁeld ˙xs from the cascaded

5

Table 1: AEE on the Chairs testing set. Models are trained on the
Chairs training set.

FlowNetS

FlowNetC

SPyNet

LiteFlowNetX-pre

LiteFlowNet-pre

2.71

2.19

2.63

2.25

1.57

notation, Fg(c) means the c-th xy-slice of the regularized
C-channel feature Fg. Equation (9) reduces the dimension
of tensors from M × N × w2 (right-hand side in prior to the
dot product) to M × N (left-hand side).

4. Experiments

Network Details. In LiteFlowNet, NetC generates 6-level
pyramidal features and NetE predicts ﬂow ﬁelds for levels
6 to 2. Flow ﬁeld in level 2 is upsampled to yield ﬂow ﬁeld
in level 1. We set the maximum searching radius in cost-
volume to 3 pixels (levels 6 to 4) or 6 pixels (levels 3 to 2).
Matching is performed at each position in pyramidal fea-
tures, except for levels 3 to 2 that it is performed at a regu-
larly sampled grid (a stride of 2). All convolution layers use
3 × 3 ﬁlters, except each last layer in descriptor matching
M , sub-pixel reﬁnement S, and ﬂow regularization R units
uses 5×5 (levels 4 to 3) or 7×7 (level 2) ﬁlters. Each convo-
lution layer is followed by a leaky rectiﬁed linear unit layer,
except f-lcon and the last layer in M , S and R CNN units.
More details can be found in the supplementary material.
Training Details. We train our network stage-wise by the
following steps: 1) NetC and M6:S6 of NetE is trained
for 300k iterations. 2) R6 together with the trained net-
work in step 1 is trained for 300k iterations. 3) For lev-
els k ∈ [5, 2], Mk:Sk followed by Rk is added into the
trained network each time. The new network cascade is
trained for 200k (level 2: 300k) iterations. Filter weights
are initialized from previous level. Learning rates are ini-
tially set to 1e-4, 5e-5, and 4e-5 for levels 6 to 4, 3 and
2 respectively. We reduce it by a factor of 2 starting at
120k, 160k, 200k, and 240k iterations. We use the same
loss weight, L2 training loss, Adam optimization, data aug-
mentation (including noise injection), and training sched-
ule 5 (Chairs [9] → Things3D [17]) as FlowNet2 [14]. We
denote LiteFlowNet-pre and LiteFlowNet as the networks
trained on Chairs and Chairs → Things3D, respectively.

4.1. Results

We compare several variants of LiteFlowNet to state-
of-the-art methods on public benchmarks including Fly-
ingChairs
[8],
KITTI12 [10], KITTI15 [18], and Middlebury [3].
FlyingChairs. We ﬁrst compare the intermediate results of

[9], Sintel clean and ﬁnal

(Chairs)

5We excluded a small amount of training data in Things3D undergoing
extremely large ﬂow displacement as advised by the authors (https://
github.com/lmb-freiburg/flownet2/issues).

different well-performing networks trained on Chairs alone
in Table 1. Average end-point error (AEE) is reported.
LiteFlowNet-pre outperforms the compared networks. No
intermediate result is available for FlowNet2 [14] as each
cascade is trained on the Chairs → Things3D sched-
ule individually. Since FlowNetC, FlowNetS (variants of
FlowNet [9]), and SPyNet [21] have fewer parameters than
FlowNet2 and the later two models do not perform fea-
ture matching, we also construct a small-size counterpart
LiteFlowNetX-pre by removing the matching part and
shrinking the model sizes of NetC and NetE by about 4 and
5 times, respectively. Despite that LiteFlowNetX-pre is 43
and 1.33 times smaller than FlowNetC and SPyNet, respec-
tively, it still outperforms these networks and is on par with
FlowNetC that uses explicit matching.

MPI Sintel.
In Table 2, LiteFlowNetX-pre outperforms
FlowNetS (and C) [9] and SPyNet [21] that are trained
on Chairs on all cases except the Middlebury benchmark.
LiteFlowNet, trained on the Chairs → Things3D sched-
ule, performs better than LiteFlowNet-pre as expected.
LiteFlowNet also outperforms SPyNet, FlowNet2-S (and -
C) [14]. We also ﬁne-tuned LiteFlowNet on a mixture of
Sintel clean and ﬁnal training data (LiteFlowNet-ft) using
the generalized Charbonnier loss [27]. No noise augmen-
tation was performed but we introduced image mirroring
to improve the diversity of the training set. LiteFlowNet-
ft outperforms FlowNet2-ft-sintel [14] and EpicFlow [22]
for Sintel ﬁnal testing set. Despite DC Flow [34] (a hy-
brid method consists of CNN and post-processing) per-
forms better than LiteFlowNet, its GPU runtime requires
several seconds that makes it formidable in many applica-
tions. Figure 5 shows some examples of ﬂow ﬁelds on Sin-
tel dataset. LiteFlowNet-ft and FlowNet2-ft-sintel perform
the best among the compared methods. As LiteFlowNet has
ﬂow regularization module, sharper ﬂow boundaries and
lesser artifacts can be observed in the generated ﬂow ﬁelds.

KITTI. LiteFlowNet consistently performs better than
LiteFlowNet-pre especially on KITTI15 as shown in Ta-
ble 2.
It also outperforms SPyNet [21] and FlowNet2-S
(and C) [14]. We also ﬁne-tuned LiteFlowNet on a mix-
ture of KITTI12 and KITTI15 training data (LiteFlowNet-
ft) using the same augmentation as the case of Sintel ex-
cept that we reduced the amount of augmentation for spa-
tial motion to ﬁt the driving scene. After ﬁne-tuning, Lite-
FlowNet generalizes well to real-world data. LiteFlowNet-
ft outperforms FlowNet2-ft-kitti [14].
Figure 6 shows
some examples of ﬂow ﬁelds on KITTI. As in the case
for Sintel, LiteFlowNet-ft and FlowNet2-ft-kitti performs
the best among the compared methods. Even though
LiteFlowNet and its variants perform pyramidal descrip-
tor matching in a limited searching range, it yields reliable
large-displacement ﬂow ﬁelds for real-world data due to the
feature warping (f-warp) layer introduced. More analysis

6

Table 2: AEE of different methods. The values in parentheses are the results of the networks on the data they were trained on, and hence are
not directly comparable to the others. Fl-all: Percentage of outliers averaged over all pixels. Inliers are deﬁned as EPE <3 pixels or <5%.
The best number for each category is highlighted in bold. (Note: 1The values are reported from [14]. 2We re-trained the model using the
code provided by the authors. 3,4,5The values are computed using the trained models provided by the authors. 4Large discrepancy exists as
the authors mistakenly evaluated the results on the disparity dataset. 5 Up-to-date dataset is used. 6Trained on Driving and Monkaa [17])

Method

Sintel clean
test
train

Sintel ﬁnal
train

test

KITTI12

KITTI15
train (Fl-all)

test (Fl-all)

Middlebury
test
train

LDOF1 [7]
DeepFlow1 [31]
Classic+NLP [27]
PCA-Layers1 [33]
EpicFlow1 [22]
FlowFields1 [1]

d Deep DiscreteFlow [11]
i
r
b
y
H

Bailer et al. [2]
DC Flow [34]

FlowNetS [9]
FlowNetS-ft [9]
FlowNetC [9]
FlowNetC-ft [9]
FlowNet2-S3 [14]
FlowNet2-S re-trained2
FlowNet2-C3 [14]
FlowNet2 [14]
FlowNet2-ft-sintel [14]
FlowNet2-ft-kitti [14]

SPyNet [21]
SPyNet-ft [21]
LiteFlowNetX-pre
LiteFlowNetX
LiteFlowNet-pre
LiteFlowNet
LiteFlowNet-ft

l
a
n
o
i
t
n
e
v
n
o
C

N
N
C

t
h
g
i
e
w
y
v
a
e
H

N
N
C

t
h
g
i
e
w
t
h
g
i
L

4.64
2.66
4.49
3.22
2.27
1.86

-
-
-

4.50
(3.66)
4.31
(3.78)
3.79
3.96
3.04
2.02
(1.45)
3.43

4.12
(3.17)
3.70
3.58
2.78
2.48
(1.35)

7.56
5.38
6.73
5.73
4.12
3.75

3.86
3.78
-

7.42
6.96
7.28
6.85
-
-
-
3.96
4.16
-

6.69
6.64
-
-
-
-
4.54

5.96
3.57
7.46
4.52
3.56
3.06

-
-
-

5.45
(4.44)
5.87
(5.28)
4.99
5.37
4.60
3.544
(2.194)
4.834
5.57
(4.32)
4.82
4.79
4.17
4.04
(1.78)

9.12
7.21
8.29
7.89
6.29
5.81

5.73
5.36
5.12

8.43
7.76
8.81
8.51
-
-
-
6.02
5.74
-

8.43
8.36
-
-
-
-
5.38

train

10.94
4.48
-
5.99
3.09
3.33

-
-
-

8.26
7.52
9.35
8.79
7.26
7.31
5.79
4.015
3.545
(1.435)
9.12
3.366
6.81
6.38
4.56
4.00
(1.05)

test

12.4
5.8
7.2
5.2
3.8
3.5

3.4
3.0
-

-
9.1
-
-
-
-
-
-
-
1.8

-
4.1
-
-
-
-
1.6

train

18.19
10.63
-
12.74
9.27
8.33

-
-
-

-
-
-
-
14.28
14.51
11.49
10.085
9.945
(2.365)
-
-
16.64
15.81
11.58
10.39
(1.62)

38.11%
26.52%
-
27.26%
27.18%
24.43%

-
-
-

-
-
-
-
51.06%
51.38%
44.09%
29.99%5
28.02%5
(8.88%5)
-
-
36.64%
34.90%
32.59%
28.50%
(5.58%)

-
29.18%
-
-
27.10%
-

21.17%
19.44%
14.86%

-
-
-
-
-
-
-
-
-
11.48%

-
35.07%
-
-
-
-
9.38%

0.44
0.25
0.22
0.66
0.31
0.27

-
-
-

1.09
0.98
1.15
0.93
1.04
1.13
0.98
0.35
0.35
0.56

0.33
0.33
0.45
0.46
0.45
0.39
0.30

0.56
0.42
0.32
-
0.39
0.33

-
-
-

-
-
-
-
-
-
-
0.52
-
-

0.58
0.58
-
-
-
-
0.40

Table 3: Number of training parameters and runtime. The model
for which the runtime is in parentheses is measured using Torch,
and hence are not directly comparable to the others using Caffe.
Abbreviation LFlowNet refers to LiteFlowNet.

Model

FlowNetC

SPyNet

Shallow

Deep
LFlowNetX

Very Deep

LFlowNet

FlowNet2

# layers
# param. (M)
Runtime (ms)

26
39.16
32.28

35
1.20
(129.83)

74
0.90
35.83

99
5.37
90.25

115
162.49
122.39

will be presented in Section 4.3.

Middlebury. LiteFlowNet has comparable performance
It outperforms FlowNetS
with conventional methods.
(and C) [9], FlowNet2-S (and C) [14], SPyNet [21], and
FlowNet2 [14]. On the benchmark, LiteFlowNet-ft refers
to the one ﬁne-tuned on Sintel.

4.2. Runtime and Parameters

We measure runtime of a CNN using a machine
equipped with an Intel Xeon E5 2.2GHz and an NVIDIA
GTX 1080. Timings are averaged over 100 runs for Sin-
tel image pairs of size 1024 × 436. As summarized in Ta-
ble 3, LiteFlowNet has about 30 times fewer parameters

Table 4: AEE of different variants of LiteFlowNet-pre trained on
Chairs dataset with some of the components disabled.

Variants

Feature Warping
Descriptor Matching
Sub-pix. Reﬁnement
Regularization

FlyingChairs (train)
Sintel clean (train)
Sintel ﬁnal (train)
KITTI12 (train)
KITTI15 (train)

M
(cid:55)
(cid:51)
(cid:55)
(cid:55)

3.75
4.70
5.69
9.22
18.24

MS
(cid:55)
(cid:51)
(cid:51)
(cid:55)

2.70
4.17
5.30
8.01
16.19

WM WSR WMS
(cid:51)
(cid:55)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:55)

(cid:51)
(cid:51)
(cid:55)
(cid:55)

2.98
3.54
4.81
6.17
14.52

1.63
3.19
4.63
5.03
13.20

1.82
2.90
4.45
4.83
12.32

ALL
(cid:51)
(cid:51)
(cid:51)
(cid:51)

1.57
2.78
4.17
4.56
11.58

than FlowNet2 [14] and is 1.36 times faster in runtime.
LiteFlowNetX, a variant of LiteFlowNet having a smaller
model size and without descriptor matching, has about 43
times fewer parameters than FlowNetC [9] and a compa-
rable runtime. LiteFlowNetX also has 1.33 times fewer pa-
rameters than SPyNet [21]. LiteFlowNet and its variants are
currently the most compact CNNs for ﬂow estimation.

4.3. Ablation Study

We

investigate

in
role of
LiteFlowNet-pre trained on Chairs by evaluating the per-

each component

the

7

Image overlay

Ground truth

FlowNetC [9]

FlowNet2 [14]

LiteFlowNet

First image

FlowNetC [9]

FlowNet2 [14]

FlowNet2-ft-sintel [14]

LiteFlowNet-ft

Figure 5: Examples of ﬂow ﬁelds from different methods on Sintel training sets for clean (top row), ﬁnal (middle row) passes, and the
testing set for ﬁnal pass (last row). Fine details are well preserved and less artifacts can be observed in the ﬂow ﬁelds of LiteFlowNet.

Image overlay

Ground truth

FlowNetC [9]

FlowNet2 [14]

LiteFlowNet

First Image

FlowNetC [9]

FlowNet2 [14]

FlowNet2-ft-kitti [14]

LiteFlowNet-ft

Figure 6: Examples of ﬂow ﬁelds from different methods on the training set (top) and the testing set (bottom) of KITTI15.

formance of different variants with some of the components
disabled. The AEE results are summarized in Table 4 and
examples of ﬂow ﬁelds are illustrated in Figure 7.

Feature Warping. We consider two variants LiteFlowNet-
pre (WM and WMS) and compare them to the counterparts
with warping disabled (M and MS). Flow ﬁelds from M and
MS are more vague. Large degradation in AEE is noticed
especially for KITTI12 (33%) and KITTI15 (25%). With
feature warping, pyramidal features that input to ﬂow infer-
ence are closer to each other. This facilitates ﬂow estimation
in subsequent pyramid level by computing residual ﬂow.

Descriptor Matching. We compare the variant WSR with-
out descriptor matching for which the ﬂow inference part
is made as deep as that in the unamended LiteFlowNet-
pre (ALL). No noticeable difference between the ﬂow ﬁelds
from WSR and ALL. Since the maximum displacement of
the example ﬂow ﬁeld is not very large (only 14.7 pixels),
accurate ﬂow ﬁeld can still be yielded from WSR. For eval-
uation covering a wide range of ﬂow displacement (espe-
cially large-displacement benchmark, KITTI), degradation
in AEE is noticed for WSR. This suggests that descriptor
matching is useful in addressing large-displacement ﬂow.

Sub-Pixel Reﬁnement. The ﬂow ﬁeld generated from
WMS is more crisp and contains more ﬁne details than
that generated from WM with sub-pixel reﬁnement dis-
abled. Less small-magnitude ﬂow artifacts (represented by
light color on the background) are also observed. Besides,
WMS achieves smaller AEE. Since descriptor matching es-
tablishes pixel-by-pixel correspondence, sub-pixel reﬁne-
ment is necessary to yield detail-preserving ﬂow ﬁeld.
Regularization. In comparison WMS with regularization
disabled to ALL, undesired artifacts exist in homogeneous
regions (represented by very dim color on the background)
of the ﬂow ﬁeld generated from WMS. Flow bleeding and
vague ﬂow boundaries are observed. Degradation in AEE is
also noticed. This suggests that the proposed feature-driven
local convolution (f-lcon) plays the vital role to smooth ﬂow
ﬁeld and maintain crisp ﬂow boundaries as regularization
term in conventional variational methods.

5. Conclusion

We have presented a compact network for accurate ﬂow
estimation. LiteFlowNet outperforms FlowNet [9] and is on
par with or outperforms the state-of-the-art FlowNet2 [14]

8

Figure 7: Examples of ﬂow ﬁelds from different variants of LiteFlowNet-pre trained on Chairs with some of the components disabled.
LiteFlowNet-pre is denoted as “All”. W = Feature Warping, M = Descriptor Matching, S = Sub-Pixel Reﬁnement, R = Regularization.

on public benchmarks while being faster in runtime and
30 times smaller in model size. Pyramidal feature extrac-
tion and feature warping (f-warp) help us to break the de
facto rule of accurate ﬂow network requiring large model
size. To address large-displacement and detail-preserving
ﬂows, LiteFlowNet exploits short-range matching to gener-
ate pixel-level ﬂow ﬁeld and further improves the estimate
to sub-pixel accuracy in the cascaded ﬂow inference. To
result crisp ﬂow boundaries, LiteFlowNet regularizes ﬂow
ﬁeld through feature-driven local convolution (f-lcon). With
its lightweight, accurate, and fast ﬂow computation, we ex-
pect that LiteFlowNet can be deployed to many applications
such as motion segmentation, action recognition, SLAM,
3D reconstruction and more.

Acknowledgement. This work is supported by SenseTime
Group Limited and the General Research Fund sponsored
by the Research Grants Council of the Hong Kong SAR
(CUHK 14241716, 14224316, 14209217).

6. Appendix

LiteFlowNet consists of two compact sub-networks,
namely NetC and NetE. NetC is a two-steam network in
which the two network streams share the same set of ﬁlters.
The input to NetC is an image pair (I1, I2). The network
architectures of the 6-level NetC and NetE at pyramid level
5 are provided in Table 5 and Tables 6 to 8, respectively.
We use sufﬁxes “M”, “S” and “R” to highlight the layers
that are used in descriptor matching, sub-pixel reﬁnement,
and ﬂow regularization units in NetE, respectively. We de-
clare a layer as “ﬂow” to highlight when the output is a
ﬂow ﬁeld. Our code and trained models are available at
https://github.com/twhui/LiteFlowNet. A
video clip (https://www.youtube.com/watch?v=
pfQ0zFwv-hM) and a supplementary material are avail-
able on our project page (http://mmlab.ie.cuhk.
edu.hk/projects/LiteFlowNet/) to showcase the
performance of LiteFlowNet and the effectiveness of the
proposed components in our network.

References

[1] C. Bailer, B. Taetz, and D. Stricker. Flow Fields: Dense
correspondence ﬁelds for highly accurate large displacement
optical ﬂow estimation. ICCV, pages 4015–4023, 2015. 3, 7
[2] C. Bailer, K. Varanasi, and D. Stricker. CNN-based patch
matching for optical ﬂow with thresholded hinge embedding
loss. CVPR, pages 3250–3259, 2017. 3, 7

[3] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and
R. Szeliski. A database and evaluation methodology for op-
tical ﬂow. IJCV, 92(1):1–31, 2011. 6

[4] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man. PatchMatch: A randomized correspondence algorithm
SIGGRAGH, pages 83–97,
for structural image editing.
2009. 2

[5] M. J. Black, Y. Yacoobt, A. D. Jepsont, and D. J. Fleets.
Learning parameterized models of image motion. CVPR,
pages 674–679, 1997. 2, 3

[6] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
ECCV, pages 25–36, 2004. 1, 2, 3

[7] T. Brox and J. Mailk. Large displacement optical ﬂow: De-
scriptor matching in variational motion estimation. PAMI,
33(3):500–513, 2011. 1, 2, 7

[8] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation.
ECCV, pages 611–625, 2012. 6

[9] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical ﬂow with convolutional net-
works. ICCV, pages 2758–2766, 2015. 1, 2, 3, 4, 6, 7, 8
[10] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? CVPR, pages 3354–3361, 2012. 6
[11] F. Gney and A. Geiger. Deep discrete ﬂow. ACCV, pages

207–224, 2016. 3, 7

[12] B. K. P. Horn and B. G. Schunck. Determining optical ﬂow.

Ariﬁcal Intelligence, 17:185–203, 1981. 1, 2

[13] T.-W. Hui and R. Chung. Determining motion directly
from normal ﬂows upon the use of a spherical eye platform.
CVPR, pages 2267–2274, 2013. 1

[14] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet2.0: Evolution of optical ﬂow estimation
with deep networks. CVPR, pages 2462–2470, 2017. 1, 2, 3,
4, 6, 7, 8

9

Table 5: The network details of NetC in LiteFlowNet. “# Ch. In / Out” means the number of channels of the input or the output features.
“conv” denotes convolution.

Layer name
conv1
conv2 1
conv2 2
conv2 3
conv3 1
conv3 2
conv4 1
conv4 2
conv5
conv6

Kernel
7×7
3×3
3×3
3×3
3×3
3×3
3×3
3×3
3×3
3×3

Stride
1
2
1
1
2
1
2
1
2
2

# Ch. In / Out
3 / 32
32 / 32
32 / 32
32 / 32
32 / 64
64 / 64
64 / 96
96 / 96
96 / 128
128 / 192

Input
I1 or I2
conv1
conv2 1
conv2 2
conv2 3
conv3 1
conv3 2
conv4 1
conv4 2
conv5

Table 6: The network details of the descriptor matching unit (M) of NetE in LiteFlowNet at pyramid level 5. “upconv”, “f-warp”, “corr”,
and “loss” denote the fractionally strided convolution (so-called deconvolution), feature warping, correlation, and the layer where training
loss is applied, respectively. Furthermore, “conv5a’ and “conv5b” denote the high-dimensional features of images I1 and I2 generated
from NetC at pyramid level 5.

Layer name
upconv5 M
f-warp5 M
corr5 M
conv5 1 M
conv5 2 M
conv5 3 M
conv5 4 M

Kernel
4×4
-
1×1
3×3
3×3
3×3
3×3

Stride
0.5
-
1
1
1
1
1

# Ch. In / Out
2 / 2
(128, 2) / 128
(128, 128) / 49
49 / 128
128 / 64
64 / 32
32 / 2
(2, 2) / 2

Input(s)
ﬂow6 R
conv5b, upconv5 M
conv5a, f-warp5 M
corr5 M
conv5 1 M
conv5 2 M
conv5 3 M
upconv5 M, conv5 4 M

ﬂow5 M, loss5 M element-wise sum

[15] M.

Jaderberg, K. Simonyan, A. Zisserman,

and
Spatial transformer networks. NIPS,

K. Kavukcuoglu.
pages 2017–2025, 2015. 3, 4

[16] J. Lu, H. Yang, D. Min, and M. N. Do. PatchMatch Fil-
ter: Efﬁcient edge-aware ﬁltering meets randomized search.
CVPR, pages 1854–1861, 2013. 2

[17] N. Mayer, E. Ilg, P. Husser, P. Fischer, D. Cremers, A. Doso-
vitskiy, and T. Brox. A large dataset to train convolutional
networks for disparity, optical ﬂow, and scene ﬂow estima-
tion. CVPR, pages 4040–4048, 2016. 6, 7

[18] M. Menze and A. Geiger. Object scene ﬂow for autonomous

vehicles. CVPR, pages 3061–3070, 2015. 6
[19] T. Nir, A. M. Bruckstein, and R. Kimmel.

Over-
parameterized variational optical ﬂow. IJCV, 76(2):205–216,
2008. 3

[20] N. Papenberg, A. Bruhn, T. Brox, S. Didas, and J. Weick-
ert. Highly accurate optic ﬂow computation with theoreti-
cally justiﬁed warping. IJCV, 67(2):141–158, 2006. 1, 3
[21] A. Ranjan and M. J. Black. Optical ﬂow estimation using a
spatial pyramid network. CVPR, pages 4161–4170, 2017. 1,
2, 3, 6, 7

[22] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. CVPR, pages 1164–1172, 2015. 1,
2, 6, 7

[23] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. MIC-
CAI, pages 234–241, 2015. 2

[24] D. Rosenbaum, D. Zoran, and Y. Weiss. Learning the local

statistics of optical ﬂow. NIPS, pages 2373–2381, 2013. 3

[25] S. Roth and M. Black. On the spatial statistics of optical

ﬂow. ICCV, pages 42–49, 2005. 3

[26] S. Roth and M. J. Black. Fields of experts: A framework for
learning image priors. CVPR, pages 860–867, 2005. 3

[27] D. Sun, S. Roth, and M. J. Black. A quantitative analysis of
current practices in optical ﬂow estimation and the principles
behind them. IJCV, 106(2):115–137, 2014. 1, 6, 7

[28] D. Sun, S. Roth, J. Lewis, and M. J. Black. Learning optical

ﬂow. ECCV, pages 83–97, 2008. 2, 3

[29] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace:
Closing the gap to human-level performance in face veriﬁca-
tion. CVPR, pages 1701–1708, 2014. 2, 5

[30] D. Tschumperl´e and R. Deriche. Vector-valued image reg-
ularization with PDEs: A common framework for different
applications. PAMI, 27(4):506–517, 2005. 5

[31] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
DeepFlow: Large displacement optical ﬂow with deep
matching. ICCV, pages 500–513, 2013. 2, 3, 7

10

Table 7: Network details of the sub-pixel reﬁnement unit (S) of NetE in LiteFlowNet at pyramid level 5.

Layer name
f-warp5 S
conv5 1 S
conv5 2 S
conv5 3 S
conv5 4 S
ﬂow5 S, loss5 S

Kernel
-
3×3
3×3
3×3
3×3
element-wise sum

Stride
-
1
1
1
1

# Ch. In / Out
(128, 2) / 128
258 / 128
128 / 64
64 / 32
32 / 2
(2, 2) / 2

Input(s)
conv5b, ﬂow5 M
conv5a, f-warp5 S, ﬂow5 M
conv5 1 S
conv5 2 S
conv5 3 S
ﬂow5 M, conv5 4 S

Table 8: Network details of the ﬂow regularization unit (R) of NetE in LiteFlowNet at pyramid level 5. “rgb-warp”, “norm”, “negsq”,
“softmax”, and “f-lcon” denote the image warping, L2 norm of the RGB brightness difference between the two input images, negative-
square, normalized exponential operation over each 1 × 1 × (# Ch. In) column in the 3-D tensor, and feature-driven local convolution,
respectively. Furthermore, “conv dist” that highlights the output of the convolution layer is used as the feature-driven distance metric D
Eq. (7) in the main manuscript. “im5a” and “im5b” denote the down-sized images of I1 and I2 at pyramid level 5, respectively

Layer name
rm-ﬂow5 R
rgb-warp5 R
norm5 R
conv5 1 R
conv5 2 R
conv5 3 R
conv5 4 R
conv5 5 R
conv5 6 R
conv5 dist R
negsq5 R
softmax5 R
f-lcon5 R (Out: ﬂow5 R), loss5 R

Stride

Kernel
remove mean
-

-
L2 norm

1
1
1
1
1
1
1

3×3
3×3
3×3
3×3
3×3
3×3
3×3
negative-square
1×1×9
3×3

1
1

# Ch. In / Out
2 / 2
(3, 2) / 3
(3, 3) / 1
131 / 128
128 / 128
128 / 64
64 / 64
64 / 32
32 / 32
32 / 9
9 / 9
9 / 9
(9, 2) / 2

Input(s)
ﬂow5 S
im5b, ﬂow5 S
im5a, rgb-warp5 R
conv5a, rm-ﬂow5 R, norm5 R
conv5 1 R
conv5 2 R
conv5 3 R
conv5 4 R
conv5 5 R
conv5 6 R
conv5 dist R
negsq5 R
softmax5 R, ﬂow5 S

[32] M. Werlberger, W. Trobin, T. Pock, A. Wedel, D. Cremers,
and H. Bischof. Anisotropic Huber-L1 optical ﬂow. BMVC,
2009. 2, 5

[33] J. Wulff and M. J. Black. Efﬁcient sparse-to-dense optical
ﬂow estimation using a learned basis and layers. CVPR,
pages 120–130, 2015. 3, 7

[34] J. Xu, R. Ranftl, and V. Koltun. Accurate optical ﬂow via
direct cost volume processings. CVPR, pages 1289–1297,
2017. 6, 7

[35] S. Zagoruyko and N. Komodakis. Learning to compare im-
age patches via convolutional neural networks. CVPR, pages
4353–4361, 2015. 3

[36] H. Zimmer, A. Bruhn, and J. Weickert. Optic ﬂow in har-

mony. IJCV, 93(3):368–388, 2011. 1, 2, 5

[37] S. Zweig and L. Wolf. InterpoNet, A brain inspired neural
network for optical ﬂow dense interpolation. CVPR, pages
4563–4572, 2017. 3

11

LiteFlowNet: A Lightweight Convolutional Neural Network
for Optical Flow Estimation

Tak-Wai Hui, Xiaoou Tang, Chen Change Loy
CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong
{twhui,xtang,ccloy}@ie.cuhk.edu.hk

8
1
0
2
 
y
a
M
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
6
3
0
7
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

FlowNet2 [14], the state-of-the-art convolutional neural
network (CNN) for optical ﬂow estimation, requires over
160M parameters to achieve accurate ﬂow estimation. In
this paper we present an alternative network that outper-
forms FlowNet2 on the challenging Sintel ﬁnal pass and
KITTI benchmarks, while being 30 times smaller in the
model size and 1.36 times faster in the running speed. This
is made possible by drilling down to architectural details
that might have been missed in the current frameworks: (1)
We present a more effective ﬂow inference approach at each
pyramid level through a lightweight cascaded network. It
not only improves ﬂow estimation accuracy through early
correction, but also permits seamless incorporation of de-
scriptor matching in our network. (2) We present a novel
ﬂow regularization layer to ameliorate the issue of outliers
and vague ﬂow boundaries by using a feature-driven lo-
cal convolution. (3) Our network owns an effective struc-
ture for pyramidal feature extraction and embraces fea-
ture warping rather than image warping as practiced in
FlowNet2. Our code and trained models are available at
https://github.com/twhui/LiteFlowNet.

1. Introduction

Optical ﬂow estimation is a long-standing problem in
computer vision. Due to the well-known aperture problem,
optical ﬂow is not directly measurable [12, 13]. Hence, the
estimation is typically solved by energy minimization in a
coarse-to-ﬁne framework [6, 20, 7, 36, 27, 22]. This class
of techniques, however, involves complex energy optimiza-
tion and thus it is not scalable for applications that demand
real-time estimation.

FlowNet [9] and its successor FlowNet2 [14], have
marked a milestone by using CNN for optical ﬂow esti-
mation. Their accuracies especially the successor are ap-
proaching that of state-of-the-art energy minimization ap-
proaches, while the speed is several orders of magnitude

Figure 1: Examples demonstrate the effectiveness of the proposed
components in LiteFlowNet for i) feature warping, ii) cascaded
ﬂow inference, and iii) ﬂow regularization. Enabled components
are indicated with bold black fonts.

faster. To push the envelop of accuracy, FlowNet2 is de-
signed as a cascade of variants of FlowNet that each net-
work in the cascade reﬁnes the preceding ﬂow ﬁeld by con-
tributing on the ﬂow increment between the ﬁrst image and
the warped second image. The model, as a result, com-
prises over 160M parameters, which could be formidable in
many applications. A recent network termed SPyNet [21]
attempts a network with smaller size of 1.2M parameters
by adopting image warping in each pyramid level. Nonethe-
less, the accuracy can only match that of FlowNet but not
FlowNet2. The objective of this study is to explore alter-
native CNN architectures for accurate ﬂow estimation yet
with high efﬁciency. Our work is inspired by the successes
of FlowNet2 and SPyNet, but we further drill down the key
elements to fully unleash the potential of deep convolutional
network combined with classical principles.

There are two general principles to improve the design
of FlowNet2 and SPyNet. The ﬁrst principle is pyrami-
dal feature extraction. The proposed network, dubbed Lite-
FlowNet, consists of an encoder and a decoder. The encoder
maps the given image pair, respectively, into two pyramids

1

of multi-scale high-dimensional features. The decoder then
estimates the ﬂow ﬁeld in a coarse-to-ﬁne framework. At
each pyramid level, the decoder infers the ﬂow ﬁeld by se-
lecting and using the features of the same resolution from
the feature pyramids. This design leads to a lighter network
compared to FlowNet2 that adopts U-Net architecture [23]
for ﬂow inference. In comparison to SPyNet, our network
separates the process of feature extraction and ﬂow estima-
tion. This helps us to better pinpoint the bottleneck of accu-
racy and model size.

The second general principle is feature warping.
FlowNet2 and SPyNet warp the second image towards the
ﬁrst image in the pair using the previous ﬂow estimate, and
then reﬁne the estimate using the feature maps generated by
the warped and the ﬁrst images. Warping an image and then
generating the feature maps of the warped image are two
ordered steps. We ﬁnd that the two steps can be reduced to
a single one by directly warping the feature maps of the sec-
ond image, which have been computed by the encoder. This
one-step feature warping process reduces the more discrimi-
native feature-space distance instead of the RGB-space dis-
tance between the two images. This makes our network
more powerful and efﬁcient in addressing the ﬂow problem.
We now highlight the more speciﬁc differences between
our network and existing CNN-based optical ﬂow estima-
tion frameworks:
1) Cascaded ﬂow inference – At each pyramid level, we in-
troduce a novel cascade of two lightweight networks. Each
of them has a feature warping (f-warp) layer to displace
the feature maps of the second image towards the ﬁrst im-
age using the ﬂow estimate from the previous level. Flow
residue is computed to further reduce the feature-space dis-
tance between the images. This design is advantageous to
the conventional design of using a single network for ﬂow
inference. First, the cascade progressively improves ﬂow
accuracy thus allowing an early correction of the estimate
without passing more errors to the next level. Second, this
design allows seamless integration with descriptor match-
ing. We assign a matching network to the ﬁrst inference.
Consequently, pixel-accuracy ﬂow ﬁeld can be generated
ﬁrst and then reﬁned to sub-pixel accuracy in the subse-
quent inference network. Since at each pyramid level the
feature-space distance between the images has been reduced
by feature warping, we can use a rather short displace-
ment than [9, 14] to establish the cost volume. Besides,
matching is performed only at sampled positions and thus a
sparse cost-volume is aggregated. This effectively reduces
the computational burden raised by the explicit matching.
2) Flow regularization – The cascaded ﬂow inference re-
sembles the role of data ﬁdelity in energy minimization
methods. Using data term alone, vague ﬂow boundaries
and undesired artifacts exist in ﬂow ﬁelds. To tackle this
problem, local ﬂow consistency and co-occurrence between

ﬂow boundaries and intensity edges are commonly used as
the cues to regularize ﬂow ﬁeld. Some of the representative
methods include anisotropic image-driven [32], image- and
ﬂow-driven [28], and complementary [36] regularizations.
After cascaded ﬂow inference, we allow the ﬂow ﬁeld to be
further regularized by our novel feature-driven local convo-
lution (f-lconv) layer1 at each pyramid level. The kernels of
such a local convolution are adaptive to the pyramidal fea-
tures from the encoder, ﬂow estimate and occlusion prob-
ability map. This makes the ﬂow regularization to be both
ﬂow- and image-aware. To our best knowledge, state-of-
the-art CNNs do not explore such a ﬂow regularization.

The effectiveness of the aforementioned contributions
are depicted in Figure 1. In summary, we propose a compact
LiteFlowNet to estimate optical ﬂow. Our network inno-
vates the useful elements from conventional methods. e.g.,
brightness constraint in data ﬁdelity to pyramidal CNN fea-
tures and image warping to CNN feature warping. More
speciﬁcally, we present a cascaded ﬂow inference with fea-
ture warping and ﬂow regularization in each pyramid level,
which are new in the literature. Overall, our network out-
performs FlowNet [9] and SPyNet [21] and is on par with
or outperforms the recent FlowNet2 [14] on public bench-
marks, while having 30 times fewer parameters and being
1.36 times faster than FlowNet2.

2. Related Work

Here, we brieﬂy review some of the major approaches

for optical ﬂow estimation.

Variational methods. Since the pioneering work by Horn
and Schunck [12], variational methods have dominated op-
tical ﬂow estimation. Brox et al. address illumination
changes by combining the brightness and gradient con-
stancy assumptions [6]. Brox et al. integrate rich descriptors
into variational formulation [7]. In DeepFlow [31], Weinza-
epfel et al. propose to correlate multi-scale patches and in-
corporate this as the matching term in functional. In Patch-
Match Filter [16], Lu et al. establish dense correspondence
using the superpixel-based PatchMatch [4]. Revaud et al.
propose a method EpicFlow that uses externally matched
ﬂows as initialization and then performs interpolation [22].
Zimmer et al. design the complementary regularization that
exploits directional information from the constraints im-
posed in data term [36]. Our network that infers optical
ﬂow and performs ﬂow regularization is inspired by the use
of data ﬁdelity and regularization in variational methods.

Machine learning methods. Black et al. propose to repre-
sent complex image motion as a linear combination of the
learned basis vectors [5]. Roth et al. formulates the prior

1We name it as feature-driven local convolution (f-lconv) layer in order
to distinguish it from local convolution (lconv) layer of which ﬁlter weights
are locally ﬁxed in conventional CNNs [29].

2

probability of ﬂow ﬁeld as Field-of-Experts model [26] that
captures higher order spatial statistics [25]. Sun et al. study
the probabilistic model of brightness inconstancy in a high-
order random ﬁeld framework [28]. Nir et al. represent
image motion using the over-parameterization model [19].
Rosenbaum et al. model the local statistics of optical ﬂow
using Gaussian mixtures [24]. Given a set of sparse
matches, Wulff et al. propose to regress them to a dense
ﬂow ﬁeld using a set of basis ﬂow ﬁelds (PCA-Flow) [33].
It can be shown that the parameterized model [5, 19, 33]
can be efﬁciently implemented using CNN.
CNN-based methods. In the work of Fischer et al. termed
FlowNet [9], a post-processing step that involves energy
minimization is required to reduce smoothing effect across
ﬂow boundaries. This process is not end-to-end trainable.
In our work, we present an end-to-end approach that per-
forms in-network ﬂow regularization using the proposed
f-lconv layer, which plays similar role as the regulariza-
tion term in variational methods.
In FlowNet2 [14], Ilg
et al. introduce a huge network cascade (over 160M pa-
rameters) that consists of variants of FlowNet. The cas-
cade improves ﬂow accuracy with an expense of model size
and computational complexity. Our model uses a more ef-
ﬁcient architecture containing 30 times fewer parameters
than FlowNet2 while the performance is on par with it. A
compact network termed SPyNet [21] from Ranjan et al. is
inspired from spatial pyramid. Nevertheless, the accuracy is
far below FlowNet2. A small-sized variant of our network
outperforms SPyNet while being 1.33 times smaller in the
model size. Zweig et al. present a network to interpolate
third-party sparse ﬂows but requiring off-the-shelf edge de-
tector [37]. DeepFlow [31] that involves convolution and
pooling operations is however not a CNN, since the “ﬁlter
weights” are non-trainable image patches. According to the
terminology used in FlowNet, DeepFlow uses correlation.

An alternative approach for establishing point correspon-
dence is to match image patches. Zagoruyko et al. ﬁrst in-
troduce to CNN-feature matching [35]. G¨uney et al. ﬁnd
feature representation and formulate optical ﬂow estimation
in MRF [11]. Bailer et al. [2] use multi-scale features and
then perform feature matching as Flow Fields [1]. Although
pixel-wise matching can establish accurate point correspon-
dence, the computational demand is too high for practical
use (it takes several seconds even a GPU is used). As a
tradeoff, Fischer et al. [9] and Ilg et al. [14] perform feature
matching only at a reduced spatial resolution. We reduce
the computational burden of feature matching by using a
short-ranged matching of warped CNN features at sampled
positions and a sub-pixel reﬁnement at every pyramid level.
We are inspired by the feature transformation used in
Spatial Transformer [15]. Our network uses the proposed
f-warp layer to displace each channel2 of the given vector-

2We can also use f-warp layer to displace each channel differently when

valued feature according to the provided ﬂow ﬁeld. Unlike
Spatial Transformer, f-warp layer is not fully constrained
and is a relaxed version of it as the ﬂow ﬁeld is not param-
eterized. While transformation in FlowNet2 and SPyNet is
limited to images, our decider network is a more generic
warping network that warps high-level CNN features.

3. LiteFlowNet

LiteFlowNet is composed of two compact sub-networks
that are specialized in pyramidal feature extraction and op-
tical ﬂow estimation as shown in Figure 2. Since the spatial
dimension of feature maps is contracting in feature extrac-
tion and that of ﬂow ﬁelds is expanding in ﬂow estimation,
we call the two sub-networks as NetC and NetE respec-
tively. NetC transforms any given image pair into two pyra-
mids of multi-scale high-dimensional features. NetE con-
sists of cascaded ﬂow inference and regularization modules
that estimate coarse-to-ﬁne ﬂow ﬁelds.

Pyramidal Feature Extraction. As shown in Figure 2,
NetC is a two-stream network in which the ﬁlter weights
are shared across the two streams. Each of them functions
as a feature descriptor that transforms an image I to a pyra-
mid of multi-scale high-dimensional features {Fk(I)} from
the highest spatial resolution (k = 1) to the lowest spatial
resolution (k = L). The pyramidal features are generated
by stride-s convolutions with the reduction of spatial reso-
lution by a factor s up the pyramid. In the following, we
omit the subscript k that indicates the level of pyramid for
brevity. We use Fi to represent CNN features for Ii. When
we discuss the operations in a pyramid level, the same op-
erations are applicable to other levels.

Feature Warping. At each pyramid level, a ﬂow ﬁeld is
inferred from high-level features F1 and F2 of images I1
and I2. Flow inference becomes more challenging if I1 and
I2 are captured far away from each other. With the motiva-
tion of image warping used in conventional methods [6, 20]
and recent CNNs [14, 21] for addressing large-displacement
ﬂow, we propose to reduce feature-space distance between
F1 and F2 by feature warping (f-warp). Speciﬁcally, F2
is warped towards F1 by f-warp via ﬂow estimate ˙x to
(cid:101)F2(x) (cid:44) F2(x + ˙x) ∼ F1(x). This allows our network to
infer residual ﬂow between F1 and (cid:101)F2 that has smaller ﬂow
magnitude (more details in Section 3.1) but not the com-
plete ﬂow ﬁeld that is more difﬁcult to infer. Unlike con-
ventional methods, f-warp is performed on high-level CNN
features but not on images. This makes our network more
powerful and efﬁcient in addressing the optical ﬂow prob-
lem. To allow end-to-end training, F is interpolated to (cid:101)F

multiple ﬂow ﬁelds are supplied. The usage, however, is beyond the scope
of this work.

3

Figure 2: The network structure of LiteFlowNet. For the ease of representation, only a 3-level design is shown. Given an image pair (I1
and I2), NetC generates two pyramids of high-level features (Fk(I1) in pink and Fk(I2) in red, k ∈ [1, 3]). NetE yields multi-scale ﬂow
ﬁelds that each of them is generated by a cascaded ﬂow inference module M :S (in blue color, including a descriptor matching unit M and
a sub-pixel reﬁnement unit S) and a regularization module R (in green color). Flow inference and regularization modules correspond to
data ﬁdelity and regularization terms in conventional energy minimization methods respectively.

for any sub-pixel displacement ˙x as follows:

(cid:101)F(x) =

(cid:88)

F(xi

s) (cid:0)1 − (cid:12)

(cid:12)xs − xi
s

(cid:12)
(cid:12)

(cid:1) (cid:0)1 − (cid:12)

(cid:12)ys − yi
s

(cid:1) ,

(cid:12)
(cid:12)

xi

s∈N (xs)

(1)
where xs = x + ˙x = (xs, ys)(cid:62) denotes the source coor-
dinates in the input feature map F that deﬁnes the sample
point, x = (x, y)(cid:62) denotes the target coordinates of the
regular grid in the interpolated feature map (cid:101)F, and N (xs)
denotes the four pixel neighbors of xs. The above bilinear
interpolation allows back-propagation during training as its
gradients can be efﬁciently computed [15].

3.1. Cascaded Flow Inference

At each pyramid level of NetE, pixel-by-pixel matching
of high-level features yields coarse ﬂow estimate. A subse-
quent reﬁnement on the coarse ﬂow further improves it to
sub-pixel accuracy.
First Flow Inference (descriptor matching). Point cor-
respondence between I1 and I2 is established through com-
puting correlation of high-level feature vectors in individual
pyramidal features F1 and F2 as follows:

c(x, d) = F1(x) · F2(x + d)/N,

(2)

where c is the matching cost between point x in F1 and
point x + d in F2, d ∈ Z is the displacement vector from x,
and N is the length of the feature vector. A cost volume C
is built by aggregating all the matching costs into a 3D grid.
We reduce the computational burden raised by cost-
volume processing [9, 14] in three ways: 1) We perform
short-range matching at every pyramid level instead of long-
range matching at a single level. 2) We reduce feature-space

distance between F1 and F2 by warping F2 towards F1
using our proposed f-warp through ﬂow estimate3 ˙x from
previous level. 3) We perform matching only at the sam-
pled positions in the pyramid levels of high-spatial resolu-
tion. The sparse cost volume is interpolated in the spatial
dimension to ﬁll the missed matching costs for the unsam-
pled positions. The ﬁrst two techniques effectively reduce
the searching space needed, while the last technique reduces
the frequency of matching per pyramid level.

In the descriptor matching unit M , residual ﬂow ∆ ˙xm
is inferred by ﬁltering the cost volume C as illustrated in
Figure 3. A complete ﬂow ﬁeld ˙xm is computed as follows:

˙xm = M (cid:0)C(F1, (cid:101)F2; d)(cid:1)
(cid:125)

(cid:124)

(cid:123)(cid:122)
∆ ˙xm

+s ˙x↑s.

(3)

Second Flow Inference (sub-pixel reﬁnement). Since
the cost volume in descriptor matching unit is aggregated
by measuring pixel-by-pixel correlation, ﬂow estimate ˙xm
from the previous inference is only up to pixel-level accu-
racy. We introduce the second ﬂow inference in the wake
of descriptor matching as shown in Figure 3.
It aims to
reﬁne the pixel-level ﬂow ﬁeld ˙xm to sub-pixel accuracy.
This prevents erroneous ﬂows being ampliﬁed by upsam-
pling and passing to the next pyramid level. Speciﬁcally,
F2 is warped to (cid:101)F2 via ﬂow estimate ˙xm. Sub-pixel reﬁne-
ment unit S yields a more accurate ﬂow ﬁeld ˙xs by mini-
mizing feature-space distance between F1 and (cid:101)F2 through

3 ˙x from previous level needs to be upsampled in spatial resolution (de-
noted by “↑s”) and magnitude (multiplied by a scalar s) to s ˙x↑s for match-
ing the spatial resolution of the pyramidal features at the current level.

4

Figure 4: Folding and packing of f-lcon ﬁlters {g}. The (x, y)-
entry of 3D tensor ¯G(c) is a 3D column with size 1 × 1 × w2.
It corresponds to the unfolded w × w f-lcon ﬁlter g(x, y, c) to be
applied at position (x, y) of channel c in vector-valued feature F .

ﬂow inference, we replace F to ˙xs. Flow regularization
module R is deﬁned as follows:

˙xr = R( ˙xs; G).

(6)

The f-lcon ﬁlters need to be specialized for smoothing
ﬂow ﬁeld. It should behave as an averaging ﬁlter if the vari-
ation of ﬂow vectors over the patch is smooth. It should also
not over-smooth ﬂow ﬁeld across ﬂow boundary. We deﬁne
a feature-driven CNN distance metric D that estimates lo-
cal ﬂow variation using pyramidal feature F1, ﬂow ﬁeld ˙xs
from the cascaded ﬂow inference, and occlusion probabil-
ity map4 O. In summary, D is adaptively constructed by a
CNN unit RD as follows:

D = RD(F1, ˙xs, O).

(7)

With the introduction of feature-driven distance metric D,
each ﬁlter g of f-lcon is constructed as follows:

g(x, y, c) =

exp(−D(x, y, c)2)
(xi,yi)∈N (x,y) exp(−D(xi, yi, c)2)

,

(cid:80)

(8)

where N (x, y) denotes the neighborhood containing ω × ω
pixels centered at position (x, y).

Here, we provide a mechanism to perform f-lcon ef-
ﬁciently. For a C-channel input F , we use C tensors
¯G(1), ..., ¯G(C) to store f-lcon ﬁlter set G. As illustrated
in Figure 4, each f-lcon ﬁlter g(x, y, c) is folded into a
1 × 1 × w2 3D column and then packed into the (x, y)-
entry of a M × N × w2 3D tensor ¯G(c). Same folding and
packing operations are also applied to each patch in each
channel of F . This results C tensors ¯F (1), ..., ¯F (C) for F .
In this way, Equation (5) can be reformulated to:

Fg(c) = ¯G(c) (cid:12) ¯F (c),

(9)

where “(cid:12)” denotes element-wise dot product between the
corresponding columns of the tensors. With the abuse of

4We use the brightness error ||I2(x+ ˙x)−I1(x)||2 between the warped

second image and the ﬁrst image as the occlusion probability map.

Figure 3: A cascaded ﬂow inference module M :S in NetE. It con-
sists of a descriptor matching unit M and a sub-pixel reﬁnement
unit S. In M , f-warp transforms high-level feature F2 to (cid:101)F2 via
upscaled ﬂow ﬁeld 2 ˙x↑2 estimated at previous pyramid level. In
S, F2 is warped by ˙xm from M . In comparison to residual ﬂow
∆ ˙xm, more ﬂow adjustment exists at ﬂow boundaries in ∆ ˙xs.

computing residual ﬂow ∆ ˙xs as the following:

˙xs = S(cid:0)F1, (cid:101)F2, ˙xm

+ ˙xm.

(4)

(cid:1)

(cid:125)

(cid:124)

(cid:123)(cid:122)
∆ ˙xs

3.2. Flow Regularization

Cascaded ﬂow inference resembles the role of data ﬁ-
delity in conventional minimization methods. Using data
term alone, vague ﬂow boundaries and undesired artifacts
commonly exist in ﬂow ﬁeld [32, 36]. To tackle this prob-
lem, we propose to use a feature-driven local convolution
(f-lcon) to regularize ﬂow ﬁeld from the cascaded ﬂow in-
ference. The operation of f-lcon is well-governed by the
Laplacian formulation of diffusion of pixel values [30]. In
contrast to local convolution (lcon) used in conventional
CNNs [29], f-lcon is more generalized. Not only is a dis-
tinct ﬁlter used for each position of feature map, but the
ﬁlter is adaptively constructed for individual ﬂow patches.
Consider a general case, a vector-valued feature F that
has to be regularized has C channels and a spatial dimen-
sion M × N . Deﬁne G = {g} as the set of ﬁlters used in
f-lcon layer. The operation of f-lcon to F can be formulated
as follow:

fg(x, y, c) = g(x, y, c) ∗ f (x, y, c),

(5)

where “∗” denotes convolution, f (x, y, c) is a w × w patch
centered at position (x, y) of channel c in F , g(x, y, c) is the
corresponding w × w regularization ﬁlter, and fg(x, y, c) is
a scalar output for x = (x, y)(cid:62) and c = 1, 2, ..., C. To
be speciﬁc for regularizing ﬂow ﬁeld ˙xs from the cascaded

5

Table 1: AEE on the Chairs testing set. Models are trained on the
Chairs training set.

FlowNetS

FlowNetC

SPyNet

LiteFlowNetX-pre

LiteFlowNet-pre

2.71

2.19

2.63

2.25

1.57

notation, Fg(c) means the c-th xy-slice of the regularized
C-channel feature Fg. Equation (9) reduces the dimension
of tensors from M × N × w2 (right-hand side in prior to the
dot product) to M × N (left-hand side).

4. Experiments

Network Details. In LiteFlowNet, NetC generates 6-level
pyramidal features and NetE predicts ﬂow ﬁelds for levels
6 to 2. Flow ﬁeld in level 2 is upsampled to yield ﬂow ﬁeld
in level 1. We set the maximum searching radius in cost-
volume to 3 pixels (levels 6 to 4) or 6 pixels (levels 3 to 2).
Matching is performed at each position in pyramidal fea-
tures, except for levels 3 to 2 that it is performed at a regu-
larly sampled grid (a stride of 2). All convolution layers use
3 × 3 ﬁlters, except each last layer in descriptor matching
M , sub-pixel reﬁnement S, and ﬂow regularization R units
uses 5×5 (levels 4 to 3) or 7×7 (level 2) ﬁlters. Each convo-
lution layer is followed by a leaky rectiﬁed linear unit layer,
except f-lcon and the last layer in M , S and R CNN units.
More details can be found in the supplementary material.
Training Details. We train our network stage-wise by the
following steps: 1) NetC and M6:S6 of NetE is trained
for 300k iterations. 2) R6 together with the trained net-
work in step 1 is trained for 300k iterations. 3) For lev-
els k ∈ [5, 2], Mk:Sk followed by Rk is added into the
trained network each time. The new network cascade is
trained for 200k (level 2: 300k) iterations. Filter weights
are initialized from previous level. Learning rates are ini-
tially set to 1e-4, 5e-5, and 4e-5 for levels 6 to 4, 3 and
2 respectively. We reduce it by a factor of 2 starting at
120k, 160k, 200k, and 240k iterations. We use the same
loss weight, L2 training loss, Adam optimization, data aug-
mentation (including noise injection), and training sched-
ule 5 (Chairs [9] → Things3D [17]) as FlowNet2 [14]. We
denote LiteFlowNet-pre and LiteFlowNet as the networks
trained on Chairs and Chairs → Things3D, respectively.

4.1. Results

We compare several variants of LiteFlowNet to state-
of-the-art methods on public benchmarks including Fly-
ingChairs
[8],
KITTI12 [10], KITTI15 [18], and Middlebury [3].
FlyingChairs. We ﬁrst compare the intermediate results of

[9], Sintel clean and ﬁnal

(Chairs)

5We excluded a small amount of training data in Things3D undergoing
extremely large ﬂow displacement as advised by the authors (https://
github.com/lmb-freiburg/flownet2/issues).

different well-performing networks trained on Chairs alone
in Table 1. Average end-point error (AEE) is reported.
LiteFlowNet-pre outperforms the compared networks. No
intermediate result is available for FlowNet2 [14] as each
cascade is trained on the Chairs → Things3D sched-
ule individually. Since FlowNetC, FlowNetS (variants of
FlowNet [9]), and SPyNet [21] have fewer parameters than
FlowNet2 and the later two models do not perform fea-
ture matching, we also construct a small-size counterpart
LiteFlowNetX-pre by removing the matching part and
shrinking the model sizes of NetC and NetE by about 4 and
5 times, respectively. Despite that LiteFlowNetX-pre is 43
and 1.33 times smaller than FlowNetC and SPyNet, respec-
tively, it still outperforms these networks and is on par with
FlowNetC that uses explicit matching.

MPI Sintel.
In Table 2, LiteFlowNetX-pre outperforms
FlowNetS (and C) [9] and SPyNet [21] that are trained
on Chairs on all cases except the Middlebury benchmark.
LiteFlowNet, trained on the Chairs → Things3D sched-
ule, performs better than LiteFlowNet-pre as expected.
LiteFlowNet also outperforms SPyNet, FlowNet2-S (and -
C) [14]. We also ﬁne-tuned LiteFlowNet on a mixture of
Sintel clean and ﬁnal training data (LiteFlowNet-ft) using
the generalized Charbonnier loss [27]. No noise augmen-
tation was performed but we introduced image mirroring
to improve the diversity of the training set. LiteFlowNet-
ft outperforms FlowNet2-ft-sintel [14] and EpicFlow [22]
for Sintel ﬁnal testing set. Despite DC Flow [34] (a hy-
brid method consists of CNN and post-processing) per-
forms better than LiteFlowNet, its GPU runtime requires
several seconds that makes it formidable in many applica-
tions. Figure 5 shows some examples of ﬂow ﬁelds on Sin-
tel dataset. LiteFlowNet-ft and FlowNet2-ft-sintel perform
the best among the compared methods. As LiteFlowNet has
ﬂow regularization module, sharper ﬂow boundaries and
lesser artifacts can be observed in the generated ﬂow ﬁelds.

KITTI. LiteFlowNet consistently performs better than
LiteFlowNet-pre especially on KITTI15 as shown in Ta-
ble 2.
It also outperforms SPyNet [21] and FlowNet2-S
(and C) [14]. We also ﬁne-tuned LiteFlowNet on a mix-
ture of KITTI12 and KITTI15 training data (LiteFlowNet-
ft) using the same augmentation as the case of Sintel ex-
cept that we reduced the amount of augmentation for spa-
tial motion to ﬁt the driving scene. After ﬁne-tuning, Lite-
FlowNet generalizes well to real-world data. LiteFlowNet-
ft outperforms FlowNet2-ft-kitti [14].
Figure 6 shows
some examples of ﬂow ﬁelds on KITTI. As in the case
for Sintel, LiteFlowNet-ft and FlowNet2-ft-kitti performs
the best among the compared methods. Even though
LiteFlowNet and its variants perform pyramidal descrip-
tor matching in a limited searching range, it yields reliable
large-displacement ﬂow ﬁelds for real-world data due to the
feature warping (f-warp) layer introduced. More analysis

6

Table 2: AEE of different methods. The values in parentheses are the results of the networks on the data they were trained on, and hence are
not directly comparable to the others. Fl-all: Percentage of outliers averaged over all pixels. Inliers are deﬁned as EPE <3 pixels or <5%.
The best number for each category is highlighted in bold. (Note: 1The values are reported from [14]. 2We re-trained the model using the
code provided by the authors. 3,4,5The values are computed using the trained models provided by the authors. 4Large discrepancy exists as
the authors mistakenly evaluated the results on the disparity dataset. 5 Up-to-date dataset is used. 6Trained on Driving and Monkaa [17])

Method

Sintel clean
test
train

Sintel ﬁnal
train

test

KITTI12

KITTI15
train (Fl-all)

test (Fl-all)

Middlebury
test
train

LDOF1 [7]
DeepFlow1 [31]
Classic+NLP [27]
PCA-Layers1 [33]
EpicFlow1 [22]
FlowFields1 [1]

d Deep DiscreteFlow [11]
i
r
b
y
H

Bailer et al. [2]
DC Flow [34]

FlowNetS [9]
FlowNetS-ft [9]
FlowNetC [9]
FlowNetC-ft [9]
FlowNet2-S3 [14]
FlowNet2-S re-trained2
FlowNet2-C3 [14]
FlowNet2 [14]
FlowNet2-ft-sintel [14]
FlowNet2-ft-kitti [14]

SPyNet [21]
SPyNet-ft [21]
LiteFlowNetX-pre
LiteFlowNetX
LiteFlowNet-pre
LiteFlowNet
LiteFlowNet-ft

l
a
n
o
i
t
n
e
v
n
o
C

N
N
C

t
h
g
i
e
w
y
v
a
e
H

N
N
C

t
h
g
i
e
w
t
h
g
i
L

4.64
2.66
4.49
3.22
2.27
1.86

-
-
-

4.50
(3.66)
4.31
(3.78)
3.79
3.96
3.04
2.02
(1.45)
3.43

4.12
(3.17)
3.70
3.58
2.78
2.48
(1.35)

7.56
5.38
6.73
5.73
4.12
3.75

3.86
3.78
-

7.42
6.96
7.28
6.85
-
-
-
3.96
4.16
-

6.69
6.64
-
-
-
-
4.54

5.96
3.57
7.46
4.52
3.56
3.06

-
-
-

5.45
(4.44)
5.87
(5.28)
4.99
5.37
4.60
3.544
(2.194)
4.834
5.57
(4.32)
4.82
4.79
4.17
4.04
(1.78)

9.12
7.21
8.29
7.89
6.29
5.81

5.73
5.36
5.12

8.43
7.76
8.81
8.51
-
-
-
6.02
5.74
-

8.43
8.36
-
-
-
-
5.38

train

10.94
4.48
-
5.99
3.09
3.33

-
-
-

8.26
7.52
9.35
8.79
7.26
7.31
5.79
4.015
3.545
(1.435)
9.12
3.366
6.81
6.38
4.56
4.00
(1.05)

test

12.4
5.8
7.2
5.2
3.8
3.5

3.4
3.0
-

-
9.1
-
-
-
-
-
-
-
1.8

-
4.1
-
-
-
-
1.6

train

18.19
10.63
-
12.74
9.27
8.33

-
-
-

-
-
-
-
14.28
14.51
11.49
10.085
9.945
(2.365)
-
-
16.64
15.81
11.58
10.39
(1.62)

38.11%
26.52%
-
27.26%
27.18%
24.43%

-
-
-

-
-
-
-
51.06%
51.38%
44.09%
29.99%5
28.02%5
(8.88%5)
-
-
36.64%
34.90%
32.59%
28.50%
(5.58%)

-
29.18%
-
-
27.10%
-

21.17%
19.44%
14.86%

-
-
-
-
-
-
-
-
-
11.48%

-
35.07%
-
-
-
-
9.38%

0.44
0.25
0.22
0.66
0.31
0.27

-
-
-

1.09
0.98
1.15
0.93
1.04
1.13
0.98
0.35
0.35
0.56

0.33
0.33
0.45
0.46
0.45
0.39
0.30

0.56
0.42
0.32
-
0.39
0.33

-
-
-

-
-
-
-
-
-
-
0.52
-
-

0.58
0.58
-
-
-
-
0.40

Table 3: Number of training parameters and runtime. The model
for which the runtime is in parentheses is measured using Torch,
and hence are not directly comparable to the others using Caffe.
Abbreviation LFlowNet refers to LiteFlowNet.

Model

FlowNetC

SPyNet

Shallow

Deep
LFlowNetX

Very Deep

LFlowNet

FlowNet2

# layers
# param. (M)
Runtime (ms)

26
39.16
32.28

35
1.20
(129.83)

74
0.90
35.83

99
5.37
90.25

115
162.49
122.39

will be presented in Section 4.3.

Middlebury. LiteFlowNet has comparable performance
It outperforms FlowNetS
with conventional methods.
(and C) [9], FlowNet2-S (and C) [14], SPyNet [21], and
FlowNet2 [14]. On the benchmark, LiteFlowNet-ft refers
to the one ﬁne-tuned on Sintel.

4.2. Runtime and Parameters

We measure runtime of a CNN using a machine
equipped with an Intel Xeon E5 2.2GHz and an NVIDIA
GTX 1080. Timings are averaged over 100 runs for Sin-
tel image pairs of size 1024 × 436. As summarized in Ta-
ble 3, LiteFlowNet has about 30 times fewer parameters

Table 4: AEE of different variants of LiteFlowNet-pre trained on
Chairs dataset with some of the components disabled.

Variants

Feature Warping
Descriptor Matching
Sub-pix. Reﬁnement
Regularization

FlyingChairs (train)
Sintel clean (train)
Sintel ﬁnal (train)
KITTI12 (train)
KITTI15 (train)

M
(cid:55)
(cid:51)
(cid:55)
(cid:55)

3.75
4.70
5.69
9.22
18.24

MS
(cid:55)
(cid:51)
(cid:51)
(cid:55)

2.70
4.17
5.30
8.01
16.19

WM WSR WMS
(cid:51)
(cid:55)
(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:55)

(cid:51)
(cid:51)
(cid:55)
(cid:55)

2.98
3.54
4.81
6.17
14.52

1.63
3.19
4.63
5.03
13.20

1.82
2.90
4.45
4.83
12.32

ALL
(cid:51)
(cid:51)
(cid:51)
(cid:51)

1.57
2.78
4.17
4.56
11.58

than FlowNet2 [14] and is 1.36 times faster in runtime.
LiteFlowNetX, a variant of LiteFlowNet having a smaller
model size and without descriptor matching, has about 43
times fewer parameters than FlowNetC [9] and a compa-
rable runtime. LiteFlowNetX also has 1.33 times fewer pa-
rameters than SPyNet [21]. LiteFlowNet and its variants are
currently the most compact CNNs for ﬂow estimation.

4.3. Ablation Study

We

investigate

in
role of
LiteFlowNet-pre trained on Chairs by evaluating the per-

each component

the

7

Image overlay

Ground truth

FlowNetC [9]

FlowNet2 [14]

LiteFlowNet

First image

FlowNetC [9]

FlowNet2 [14]

FlowNet2-ft-sintel [14]

LiteFlowNet-ft

Figure 5: Examples of ﬂow ﬁelds from different methods on Sintel training sets for clean (top row), ﬁnal (middle row) passes, and the
testing set for ﬁnal pass (last row). Fine details are well preserved and less artifacts can be observed in the ﬂow ﬁelds of LiteFlowNet.

Image overlay

Ground truth

FlowNetC [9]

FlowNet2 [14]

LiteFlowNet

First Image

FlowNetC [9]

FlowNet2 [14]

FlowNet2-ft-kitti [14]

LiteFlowNet-ft

Figure 6: Examples of ﬂow ﬁelds from different methods on the training set (top) and the testing set (bottom) of KITTI15.

formance of different variants with some of the components
disabled. The AEE results are summarized in Table 4 and
examples of ﬂow ﬁelds are illustrated in Figure 7.

Feature Warping. We consider two variants LiteFlowNet-
pre (WM and WMS) and compare them to the counterparts
with warping disabled (M and MS). Flow ﬁelds from M and
MS are more vague. Large degradation in AEE is noticed
especially for KITTI12 (33%) and KITTI15 (25%). With
feature warping, pyramidal features that input to ﬂow infer-
ence are closer to each other. This facilitates ﬂow estimation
in subsequent pyramid level by computing residual ﬂow.

Descriptor Matching. We compare the variant WSR with-
out descriptor matching for which the ﬂow inference part
is made as deep as that in the unamended LiteFlowNet-
pre (ALL). No noticeable difference between the ﬂow ﬁelds
from WSR and ALL. Since the maximum displacement of
the example ﬂow ﬁeld is not very large (only 14.7 pixels),
accurate ﬂow ﬁeld can still be yielded from WSR. For eval-
uation covering a wide range of ﬂow displacement (espe-
cially large-displacement benchmark, KITTI), degradation
in AEE is noticed for WSR. This suggests that descriptor
matching is useful in addressing large-displacement ﬂow.

Sub-Pixel Reﬁnement. The ﬂow ﬁeld generated from
WMS is more crisp and contains more ﬁne details than
that generated from WM with sub-pixel reﬁnement dis-
abled. Less small-magnitude ﬂow artifacts (represented by
light color on the background) are also observed. Besides,
WMS achieves smaller AEE. Since descriptor matching es-
tablishes pixel-by-pixel correspondence, sub-pixel reﬁne-
ment is necessary to yield detail-preserving ﬂow ﬁeld.
Regularization. In comparison WMS with regularization
disabled to ALL, undesired artifacts exist in homogeneous
regions (represented by very dim color on the background)
of the ﬂow ﬁeld generated from WMS. Flow bleeding and
vague ﬂow boundaries are observed. Degradation in AEE is
also noticed. This suggests that the proposed feature-driven
local convolution (f-lcon) plays the vital role to smooth ﬂow
ﬁeld and maintain crisp ﬂow boundaries as regularization
term in conventional variational methods.

5. Conclusion

We have presented a compact network for accurate ﬂow
estimation. LiteFlowNet outperforms FlowNet [9] and is on
par with or outperforms the state-of-the-art FlowNet2 [14]

8

Figure 7: Examples of ﬂow ﬁelds from different variants of LiteFlowNet-pre trained on Chairs with some of the components disabled.
LiteFlowNet-pre is denoted as “All”. W = Feature Warping, M = Descriptor Matching, S = Sub-Pixel Reﬁnement, R = Regularization.

on public benchmarks while being faster in runtime and
30 times smaller in model size. Pyramidal feature extrac-
tion and feature warping (f-warp) help us to break the de
facto rule of accurate ﬂow network requiring large model
size. To address large-displacement and detail-preserving
ﬂows, LiteFlowNet exploits short-range matching to gener-
ate pixel-level ﬂow ﬁeld and further improves the estimate
to sub-pixel accuracy in the cascaded ﬂow inference. To
result crisp ﬂow boundaries, LiteFlowNet regularizes ﬂow
ﬁeld through feature-driven local convolution (f-lcon). With
its lightweight, accurate, and fast ﬂow computation, we ex-
pect that LiteFlowNet can be deployed to many applications
such as motion segmentation, action recognition, SLAM,
3D reconstruction and more.

Acknowledgement. This work is supported by SenseTime
Group Limited and the General Research Fund sponsored
by the Research Grants Council of the Hong Kong SAR
(CUHK 14241716, 14224316, 14209217).

6. Appendix

LiteFlowNet consists of two compact sub-networks,
namely NetC and NetE. NetC is a two-steam network in
which the two network streams share the same set of ﬁlters.
The input to NetC is an image pair (I1, I2). The network
architectures of the 6-level NetC and NetE at pyramid level
5 are provided in Table 5 and Tables 6 to 8, respectively.
We use sufﬁxes “M”, “S” and “R” to highlight the layers
that are used in descriptor matching, sub-pixel reﬁnement,
and ﬂow regularization units in NetE, respectively. We de-
clare a layer as “ﬂow” to highlight when the output is a
ﬂow ﬁeld. Our code and trained models are available at
https://github.com/twhui/LiteFlowNet. A
video clip (https://www.youtube.com/watch?v=
pfQ0zFwv-hM) and a supplementary material are avail-
able on our project page (http://mmlab.ie.cuhk.
edu.hk/projects/LiteFlowNet/) to showcase the
performance of LiteFlowNet and the effectiveness of the
proposed components in our network.

References

[1] C. Bailer, B. Taetz, and D. Stricker. Flow Fields: Dense
correspondence ﬁelds for highly accurate large displacement
optical ﬂow estimation. ICCV, pages 4015–4023, 2015. 3, 7
[2] C. Bailer, K. Varanasi, and D. Stricker. CNN-based patch
matching for optical ﬂow with thresholded hinge embedding
loss. CVPR, pages 3250–3259, 2017. 3, 7

[3] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and
R. Szeliski. A database and evaluation methodology for op-
tical ﬂow. IJCV, 92(1):1–31, 2011. 6

[4] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man. PatchMatch: A randomized correspondence algorithm
SIGGRAGH, pages 83–97,
for structural image editing.
2009. 2

[5] M. J. Black, Y. Yacoobt, A. D. Jepsont, and D. J. Fleets.
Learning parameterized models of image motion. CVPR,
pages 674–679, 1997. 2, 3

[6] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
ECCV, pages 25–36, 2004. 1, 2, 3

[7] T. Brox and J. Mailk. Large displacement optical ﬂow: De-
scriptor matching in variational motion estimation. PAMI,
33(3):500–513, 2011. 1, 2, 7

[8] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation.
ECCV, pages 611–625, 2012. 6

[9] P. Fischer, A. Dosovitskiy, E. Ilg, P. H¨ausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical ﬂow with convolutional net-
works. ICCV, pages 2758–2766, 2015. 1, 2, 3, 4, 6, 7, 8
[10] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? CVPR, pages 3354–3361, 2012. 6
[11] F. Gney and A. Geiger. Deep discrete ﬂow. ACCV, pages

207–224, 2016. 3, 7

[12] B. K. P. Horn and B. G. Schunck. Determining optical ﬂow.

Ariﬁcal Intelligence, 17:185–203, 1981. 1, 2

[13] T.-W. Hui and R. Chung. Determining motion directly
from normal ﬂows upon the use of a spherical eye platform.
CVPR, pages 2267–2274, 2013. 1

[14] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet2.0: Evolution of optical ﬂow estimation
with deep networks. CVPR, pages 2462–2470, 2017. 1, 2, 3,
4, 6, 7, 8

9

Table 5: The network details of NetC in LiteFlowNet. “# Ch. In / Out” means the number of channels of the input or the output features.
“conv” denotes convolution.

Layer name
conv1
conv2 1
conv2 2
conv2 3
conv3 1
conv3 2
conv4 1
conv4 2
conv5
conv6

Kernel
7×7
3×3
3×3
3×3
3×3
3×3
3×3
3×3
3×3
3×3

Stride
1
2
1
1
2
1
2
1
2
2

# Ch. In / Out
3 / 32
32 / 32
32 / 32
32 / 32
32 / 64
64 / 64
64 / 96
96 / 96
96 / 128
128 / 192

Input
I1 or I2
conv1
conv2 1
conv2 2
conv2 3
conv3 1
conv3 2
conv4 1
conv4 2
conv5

Table 6: The network details of the descriptor matching unit (M) of NetE in LiteFlowNet at pyramid level 5. “upconv”, “f-warp”, “corr”,
and “loss” denote the fractionally strided convolution (so-called deconvolution), feature warping, correlation, and the layer where training
loss is applied, respectively. Furthermore, “conv5a’ and “conv5b” denote the high-dimensional features of images I1 and I2 generated
from NetC at pyramid level 5.

Layer name
upconv5 M
f-warp5 M
corr5 M
conv5 1 M
conv5 2 M
conv5 3 M
conv5 4 M

Kernel
4×4
-
1×1
3×3
3×3
3×3
3×3

Stride
0.5
-
1
1
1
1
1

# Ch. In / Out
2 / 2
(128, 2) / 128
(128, 128) / 49
49 / 128
128 / 64
64 / 32
32 / 2
(2, 2) / 2

Input(s)
ﬂow6 R
conv5b, upconv5 M
conv5a, f-warp5 M
corr5 M
conv5 1 M
conv5 2 M
conv5 3 M
upconv5 M, conv5 4 M

ﬂow5 M, loss5 M element-wise sum

[15] M.

Jaderberg, K. Simonyan, A. Zisserman,

and
Spatial transformer networks. NIPS,

K. Kavukcuoglu.
pages 2017–2025, 2015. 3, 4

[16] J. Lu, H. Yang, D. Min, and M. N. Do. PatchMatch Fil-
ter: Efﬁcient edge-aware ﬁltering meets randomized search.
CVPR, pages 1854–1861, 2013. 2

[17] N. Mayer, E. Ilg, P. Husser, P. Fischer, D. Cremers, A. Doso-
vitskiy, and T. Brox. A large dataset to train convolutional
networks for disparity, optical ﬂow, and scene ﬂow estima-
tion. CVPR, pages 4040–4048, 2016. 6, 7

[18] M. Menze and A. Geiger. Object scene ﬂow for autonomous

vehicles. CVPR, pages 3061–3070, 2015. 6
[19] T. Nir, A. M. Bruckstein, and R. Kimmel.

Over-
parameterized variational optical ﬂow. IJCV, 76(2):205–216,
2008. 3

[20] N. Papenberg, A. Bruhn, T. Brox, S. Didas, and J. Weick-
ert. Highly accurate optic ﬂow computation with theoreti-
cally justiﬁed warping. IJCV, 67(2):141–158, 2006. 1, 3
[21] A. Ranjan and M. J. Black. Optical ﬂow estimation using a
spatial pyramid network. CVPR, pages 4161–4170, 2017. 1,
2, 3, 6, 7

[22] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. CVPR, pages 1164–1172, 2015. 1,
2, 6, 7

[23] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. MIC-
CAI, pages 234–241, 2015. 2

[24] D. Rosenbaum, D. Zoran, and Y. Weiss. Learning the local

statistics of optical ﬂow. NIPS, pages 2373–2381, 2013. 3

[25] S. Roth and M. Black. On the spatial statistics of optical

ﬂow. ICCV, pages 42–49, 2005. 3

[26] S. Roth and M. J. Black. Fields of experts: A framework for
learning image priors. CVPR, pages 860–867, 2005. 3

[27] D. Sun, S. Roth, and M. J. Black. A quantitative analysis of
current practices in optical ﬂow estimation and the principles
behind them. IJCV, 106(2):115–137, 2014. 1, 6, 7

[28] D. Sun, S. Roth, J. Lewis, and M. J. Black. Learning optical

ﬂow. ECCV, pages 83–97, 2008. 2, 3

[29] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace:
Closing the gap to human-level performance in face veriﬁca-
tion. CVPR, pages 1701–1708, 2014. 2, 5

[30] D. Tschumperl´e and R. Deriche. Vector-valued image reg-
ularization with PDEs: A common framework for different
applications. PAMI, 27(4):506–517, 2005. 5

[31] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.
DeepFlow: Large displacement optical ﬂow with deep
matching. ICCV, pages 500–513, 2013. 2, 3, 7

10

Table 7: Network details of the sub-pixel reﬁnement unit (S) of NetE in LiteFlowNet at pyramid level 5.

Layer name
f-warp5 S
conv5 1 S
conv5 2 S
conv5 3 S
conv5 4 S
ﬂow5 S, loss5 S

Kernel
-
3×3
3×3
3×3
3×3
element-wise sum

Stride
-
1
1
1
1

# Ch. In / Out
(128, 2) / 128
258 / 128
128 / 64
64 / 32
32 / 2
(2, 2) / 2

Input(s)
conv5b, ﬂow5 M
conv5a, f-warp5 S, ﬂow5 M
conv5 1 S
conv5 2 S
conv5 3 S
ﬂow5 M, conv5 4 S

Table 8: Network details of the ﬂow regularization unit (R) of NetE in LiteFlowNet at pyramid level 5. “rgb-warp”, “norm”, “negsq”,
“softmax”, and “f-lcon” denote the image warping, L2 norm of the RGB brightness difference between the two input images, negative-
square, normalized exponential operation over each 1 × 1 × (# Ch. In) column in the 3-D tensor, and feature-driven local convolution,
respectively. Furthermore, “conv dist” that highlights the output of the convolution layer is used as the feature-driven distance metric D
Eq. (7) in the main manuscript. “im5a” and “im5b” denote the down-sized images of I1 and I2 at pyramid level 5, respectively

Layer name
rm-ﬂow5 R
rgb-warp5 R
norm5 R
conv5 1 R
conv5 2 R
conv5 3 R
conv5 4 R
conv5 5 R
conv5 6 R
conv5 dist R
negsq5 R
softmax5 R
f-lcon5 R (Out: ﬂow5 R), loss5 R

Stride

Kernel
remove mean
-

-
L2 norm

1
1
1
1
1
1
1

3×3
3×3
3×3
3×3
3×3
3×3
3×3
negative-square
1×1×9
3×3

1
1

# Ch. In / Out
2 / 2
(3, 2) / 3
(3, 3) / 1
131 / 128
128 / 128
128 / 64
64 / 64
64 / 32
32 / 32
32 / 9
9 / 9
9 / 9
(9, 2) / 2

Input(s)
ﬂow5 S
im5b, ﬂow5 S
im5a, rgb-warp5 R
conv5a, rm-ﬂow5 R, norm5 R
conv5 1 R
conv5 2 R
conv5 3 R
conv5 4 R
conv5 5 R
conv5 6 R
conv5 dist R
negsq5 R
softmax5 R, ﬂow5 S

[32] M. Werlberger, W. Trobin, T. Pock, A. Wedel, D. Cremers,
and H. Bischof. Anisotropic Huber-L1 optical ﬂow. BMVC,
2009. 2, 5

[33] J. Wulff and M. J. Black. Efﬁcient sparse-to-dense optical
ﬂow estimation using a learned basis and layers. CVPR,
pages 120–130, 2015. 3, 7

[34] J. Xu, R. Ranftl, and V. Koltun. Accurate optical ﬂow via
direct cost volume processings. CVPR, pages 1289–1297,
2017. 6, 7

[35] S. Zagoruyko and N. Komodakis. Learning to compare im-
age patches via convolutional neural networks. CVPR, pages
4353–4361, 2015. 3

[36] H. Zimmer, A. Bruhn, and J. Weickert. Optic ﬂow in har-

mony. IJCV, 93(3):368–388, 2011. 1, 2, 5

[37] S. Zweig and L. Wolf. InterpoNet, A brain inspired neural
network for optical ﬂow dense interpolation. CVPR, pages
4563–4572, 2017. 3

11

