7
1
0
2
 
l
u
J
 
7
 
 
]
L
M

.
t
a
t
s
[
 
 
5
v
7
7
2
6
0
.
3
0
6
1
:
v
i
X
r
a

Composing graphical models with neural networks
for structured representations and fast inference

Matthew James Johnson
Harvard University
mattjj@seas.harvard.edu

David Duvenaud
Harvard University
dduvenaud@seas.harvard.edu

Alexander B. Wiltschko
Harvard University, Twitter
awiltsch@fas.harvard.edu

Sandeep R. Datta
Harvard Medical School
srdatta@hms.harvard.edu

Ryan P. Adams
Harvard University, Twitter
rpa@seas.harvard.edu

Abstract

We propose a general modeling and inference framework that combines the com-
plementary strengths of probabilistic graphical models and deep learning methods.
Our model family composes latent graphical models with neural network obser-
vation likelihoods. For inference, we use recognition networks to produce local
evidence potentials, then combine them with the model distribution using efﬁcient
message-passing algorithms. All components are trained simultaneously with a
single stochastic variational inference objective. We illustrate this framework by
automatically segmenting and categorizing mouse behavior from raw depth video,
and demonstrate several other example models.

1

Introduction

Modeling often has two goals: ﬁrst, to learn a ﬂexible representation of complex high-dimensional
data, such as images or speech recordings, and second, to ﬁnd structure that is interpretable and
generalizes to new tasks. Probabilistic graphical models [1, 2] provide many tools to build structured
representations, but often make rigid assumptions and may require signiﬁcant feature engineering.
Alternatively, deep learning methods allow ﬂexible data representations to be learned automatically,
but may not directly encode interpretable or tractable probabilistic structure. Here we develop a
general modeling and inference framework that combines these complementary strengths.

Consider learning a generative model for video of a mouse. Learning interpretable representations for
such data, and comparing them as the animal’s genes are edited or its brain chemistry altered, gives
useful behavioral phenotyping tools for neuroscience and for high-throughput drug discovery [3].
Even though each image is encoded by hundreds of pixels, the data lie near a low-dimensional
nonlinear manifold. A useful generative model must not only learn this manifold but also provide
an interpretable representation of the mouse’s behavioral dynamics. A natural representation from
ethology [3] is that the mouse’s behavior is divided into brief, reused actions, such as darts, rears,
and grooming bouts. Therefore an appropriate model might switch between discrete states, with
each state representing the dynamics of a particular action. These two learning tasks — identifying
an image manifold and a structured dynamics model — are complementary: we want to learn the
image manifold in terms of coordinates in which the structured dynamics ﬁt well. A similar challenge
arises in speech [4], where high-dimensional spectrographic data lie near a low-dimensional manifold
because they are generated by a physical system with relatively few degrees of freedom [5] but also
include the discrete latent dynamical structure of phonemes, words, and grammar [6].

To address these challenges, we propose a new framework to design and learn models that couple
nonlinear likelihoods with structured latent variable representations. Our approach uses graphical
models for representing structured probability distributions while enabling fast exact inference
subroutines, and uses ideas from variational autoencoders [7, 8] for learning not only the nonlinear

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

(a) Data

(b) GMM

(c) Density net (VAE)

(d) GMM SVAE

Figure 1: Comparison of generative models ﬁt to spiral cluster data. See Section 2.1.

feature manifold but also bottom-up recognition networks to improve inference. Thus our method
enables the combination of ﬂexible deep learning feature models with structured Bayesian (and
even nonparametric [9]) priors. Our approach yields a single variational inference objective in
which all components of the model are learned simultaneously. Furthermore, we develop a scalable
ﬁtting algorithm that combines several advances in efﬁcient inference, including stochastic variational
inference [10], graphical model message passing [1], and backpropagation with the reparameterization
trick [7]. Thus our algorithm can leverage conjugate exponential family structure where it exists to
efﬁciently compute natural gradients with respect to some variational parameters, enabling effective
second-order optimization [11], while using backpropagation to compute gradients with respect to all
other parameters. We refer to our general approach as the structured variational autoencoder (SVAE).

2 Latent graphical models with neural net observations

In this paper we propose a broad family of models. Here we develop three speciﬁc examples.

2.1 Warped mixtures for arbitrary cluster shapes

One particularly natural structure used frequently in graphical models is the discrete mixture model.
By ﬁtting a discrete mixture model to data, we can discover natural clusters or units. These discrete
structures are difﬁcult to represent directly in neural network models.

Consider the problem of modeling the data y =
ﬁnding the clusters in data is to ﬁt a Gaussian mixture model (GMM) with a conjugate prior:
zn |

yn}
{
π iid
∼
However, the ﬁt GMM does not represent the natural clustering of the data (Fig. 1b). Its inﬂexible
Gaussian observation model limits its ability to parsimoniously ﬁt the data and their natural semantics.

N
n=1 shown in Fig. 1a. A standard approach to

(µk, Σk) iid
∼

(µk, Σk)
}
{

(µzn, Σzn ).

iid
∼ N

NIW(λ),

Dir(α),

yn |

K
k=1

zn,

∼

π

π

Instead of using a GMM, a more ﬂexible alternative would be a neural network density model:

γ

∼

xn

p(γ)

(1)
yn |
where µ(xn; γ) and Σ(xn; γ) depend on xn through some smooth parametric function, such as
multilayer perceptron (MLP), and where p(γ) is a Gaussian prior [12]. This model ﬁts the data
density well (Fig. 1c) but does not explicitly represent discrete mixture components, which might
provide insights into the data or natural units for generalization. See Fig. 2a for a graphical model.

(µ(xn; γ), Σ(xn; γ)),

(0, I),

∼ N

xn, γ iid

iid
∼ N

By composing a latent GMM with nonlinear observations, we can combine the modeling strengths of
both [13], learning both discrete clusters along with non-Gaussian cluster shapes:

π

Dir(α),

∼
xn

iid
∼ N

(µk, Σk) iid
∼

(µ(zn), Σ(zn)),

NIW(λ),

γ

p(γ)

yn |

xn, γ iid

∼ N

∼
(µ(xn; γ), Σ(xn; γ)).

zn |

π iid
∼

π

This combination of ﬂexibility and structure is shown in Fig. 1d. See Fig. 2b for a graphical model.

2.2 Latent linear dynamical systems for modeling video

Now we consider a harder problem: generatively modeling video. Since a video is a sequence of
image frames, a natural place to start is with a model for images. Kingma et al. [7] shows that the

2

(a) Latent Gaussian (b) Latent GMM

(c) Latent LDS

(d) Latent SLDS

Figure 2: Generative graphical models discussed in Section 2.

density network of Eq. (1) can accurately represent a dataset of high-dimensional images
terms of the low-dimensional latent variables

N
n=1 in
N
n=1, each with independent Gaussian distributions.
xn}
To extend this image model into a model for videos, we can introduce dependence through time
N
between the latent Gaussian samples
n=1. For instance, we can make each latent variable xn
depend on the previous latent variable xn−1 through a Gaussian linear dynamical system, writing

xn}
{

yn}
{

{

xn = Axn−1 + Bun,

un

iid
∼ N

(0, I),

A, B

Rm×m,

∈

where the matrices A and B have a conjugate prior. This model has low-dimensional latent states and
dynamics as well as a rich nonlinear generative model of images. In addition, the timescales of the
dynamics are represented directly in the eigenvalue spectrum of A, providing both interpretability
and a natural way to encode prior information. See Fig. 2c for a graphical model.

2.3 Latent switching linear dynamical systems for parsing behavior from video

As a ﬁnal example that combines both time series structure and discrete latent units, consider again
the behavioral phenotyping problem described in Section 1. Drawing on graphical modeling tools,
we can construct a latent switching linear dynamical system (SLDS) [14] to represent the data in
terms of continuous latent states that evolve according to a discrete library of linear dynamics, and
drawing on deep learning methods we can generate video frames with a neural network image model.

}

∈ {

1, 2, . . . , N

there is a discrete-valued latent state zn ∈ {

At each time n
that evolves
according to Markovian dynamics. The discrete state indexes a set of linear dynamical parameters,
Rm evolves according to the corresponding dynamics,
and the continuous-valued latent state xn ∈
zn−1, π
k=1 denotes the Markov transition matrix and πk ∈

where π =
+ is its kth row. We use the
same neural net observation model as in Section 2.2. This SLDS model combines both continuous
and discrete latent variables with rich nonlinear observations. See Fig. 2d for a graphical model.

xn = Azn xn−1 + Bzn un,

zn |
πk}
{

un
RK

iid
∼ N

1, 2, . . . , K

πzn−1,

(0, I),

∼

}

K

3 Structured mean ﬁeld inference and recognition networks

Why aren’t such rich hybrid models used more frequently? The main difﬁculty with combining rich
latent variable structure and ﬂexible likelihoods is inference. The most efﬁcient inference algorithms
used in graphical models, like structured mean ﬁeld and message passing, depend on conjugate
exponential family likelihoods to preserve tractable structure. When the observations are more
general, like neural network models, inference must either fall back to general algorithms that do not
exploit the model structure or else rely on bespoke algorithms developed for one model at a time.

In this section, we review inference ideas from conjugate exponential family probabilistic graphical
models and variational autoencoders, which we combine and generalize in the next section.

3.1

Inference in graphical models with conjugacy structure

Graphical models and exponential families provide many algorithmic tools for efﬁcient inference [15].
Given an exponential family latent variable model, when the observation model is a conjugate
exponential family, the conditional distributions stay in the same exponential families as in the prior
and hence allow for the same efﬁcient inference algorithms.

3

(a) VAE

(b) GMM SVAE

(c) LDS SVAE

(d) SLDS SVAE

Figure 3: Variational families and recognition networks for the VAE [7] and three SVAE examples.

For example, consider learning a Gaussian linear dynamical system model with linear Gaussian
N
observations. The generative model for latent states x =
n=1 is

N
n=1 and observations y =

xn}
{

xn = Axn−1 + Bun,

(0, I),

yn = Cxn + Dvn,

un

iid
∼ N

given parameters θ = (A, B, C, D) with a conjugate prior p(θ). To approximate the poste-
rior p(θ, x

y), consider the mean ﬁeld family q(θ)q(x) and the variational inference objective

yn}
{
(0, I),

vn

iid
∼ N

|

[ q(θ)q(x) ] = E

q(θ)q(x)

L

(cid:20)

log

p(θ)p(x

θ)p(y
q(θ)q(x)

|

|

(cid:21)

x, θ)

,

(2)

y) by
where we can optimize the variational family q(θ)q(x) to approximate the posterior p(θ, x
x, θ) is conjugate to the latent variable
maximizing Eq. (2). Because the observation model p(y
model p(x
[ q(θ)q(x) ] is itself a
Gaussian linear dynamical system with parameters that are simple functions of the expected statistics
of q(θ) and the data y. As a result, for ﬁxed q(θ) we can easily compute q∗(x) and use message
passing algorithms to perform exact inference in it. However, when the observation model is not
conjugate to the latent variable model, these algorithmically exploitable structures break down.

θ), for any ﬁxed q(θ) the optimal factor q∗(x) (cid:44) arg maxq(x) L

|

|

|

3.2 Recognition networks in variational autoencoders

The variational autoencoder (VAE) [7] handles general non-conjugate observation models by intro-
ducing recognition networks. For example, when a Gaussian latent variable model p(x) is paired with
a general nonlinear observation model p(y
y, γ) is non-Gaussian, and it is
difﬁcult to compute an optimal Gaussian approximation. The VAE instead learns to directly output a
suboptimal Gaussian factor q(x
y) by ﬁtting a parametric map from data y to a mean and covariance,
µ(y; φ) and Σ(y; φ), such as an MLP with parameters φ. By optimizing over φ, the VAE effectively
learns how to condition on non-conjugate observations y and produce a good approximating factor.

x, γ), the posterior p(x

|

|

|

4 Structured variational autoencoders

We can combine the tractability of conjugate graphical model inference with the ﬂexibility of
variational autoencoders. The main idea is to use a conditional random ﬁeld (CRF) variational family.
We learn recognition networks that output conjugate graphical model potentials instead of outputting
the complete variational distribution’s parameters directly. These potentials are then used in graphical
model inference algorithms in place of the non-conjugate observation likelihoods.

The SVAE algorithm computes stochastic gradients of a mean ﬁeld variational inference objective.
It can be viewed as a generalization both of the natural gradient SVI algorithm for conditionally
conjugate models [10] and of the AEVB algorithm for variational autoencoders [7]. Intuitively,
it proceeds by sampling a data minibatch, applying the recognition model to compute graphical
model potentials, and using graphical model inference algorithms to compute the variational factor,
combining the evidence from the potentials with the prior structure in the model. This variational
factor is then used to compute gradients of the mean ﬁeld objective. See Fig. 3 for graphical models
of the variational families with recognition networks for the models developed in Section 2.

In this section, we outline the SVAE model class more formally, write the mean ﬁeld variational
inference objective, and show how to efﬁciently compute unbiased stochastic estimates of its gradients.
The resulting algorithm for computing gradients of the mean ﬁeld objective, shown in Algorithm 1, is

4

Algorithm 1 Estimate SVAE lower bound and its gradients

Input: Variational parameters (ηθ, ηγ, φ), data sample y

function SVAEGRADIENTS(ηθ, ηγ, φ, y)

←

←

r(yn; φ)

ψ
(ˆx, ¯tx, KLlocal)
q(γ)
ˆγ
∼
N log p(y
L ←
η0
(cid:101)
∇ηθ L ←
θ −
return lower bound

PGMINFERENCE(ηθ, ψ)

−

ˆx, ˆγ)

N KLlocal
KL(q(θ)q(γ)
−
|
ηθ + N (¯tx, 1) + N (
∇ηx log p(y
∇ηθ L

, natural gradient (cid:101)

L

|

ˆx, ˆγ), 0)
, gradients

p(θ)p(γ))
(cid:107)

∇ηγ ,φL

(cid:46) Get evidence potentials
(cid:46) Combine evidence with prior
(cid:46) Sample observation parameters
(cid:46) Estimate variational bound
(cid:46) Compute natural gradient

function PGMINFERENCE(ηθ, ψ)

q∗(x)
←
return sample ˆx

∼

OPTIMIZELOCALFACTORS(ηθ, ψ)

q∗(x), statistics E

q∗(x)tx(x), divergence E

(cid:46) Fast message-passing inference

q(θ) KL(q∗(x)
(cid:107)

p(x

θ))

|

simple and efﬁcient and can be readily applied to a variety of learning problems and graphical model
structures. See the supplementals for details and proofs.

4.1 SVAE model class

To set up notation for a general SVAE, we ﬁrst deﬁne a conjugate pair of exponential family densities
on global latent variables θ and local latent variables x =
θ) be an exponential
family and let p(θ) be its corresponding natural exponential family conjugate prior, writing
log Zθ(η0

η0
θ , tθ(θ)
(cid:105) −
x(θ))(cid:9) = exp
η0
x(θ), tx(x)
,
where we used exponential family conjugacy to write tθ(θ) = (cid:0)η0
x(θ))(cid:1). The local
latent variables x could have additional structure, like including both discrete and continuous latent
variables or tractable graph structure, but here we keep the notation simple.

p(θ) = exp (cid:8)
(cid:104)
θ) = exp (cid:8)
(cid:104)

tθ(θ), (tx(x), 1)
log Zx(η0

θ )(cid:9) ,
log Zx(η0

N
n=1. Let p(x

{(cid:104)
x(θ),

xn}

p(x

(cid:105) −

(cid:105)}

−

{

|

|

Next, we deﬁne a general likelihood function. Let p(y
x, γ) be a general family of densities and
let p(γ) be an exponential family prior on its parameters. For example, each observation yn may
depend on the latent value xn through an MLP, as in the density network model of Section 2.
This generic non-conjugate observation model provides modeling ﬂexibility, yet the SVAE can still
leverage conjugate exponential family structure in inference, as we show next.

|

4.2 Stochastic variational inference algorithm

Though the general observation model p(y
x, γ) means that conjugate updates and natural gradient
SVI [10] cannot be directly applied, we show that by generalizing the recognition network idea we
can still approximately optimize out the local variational factors leveraging conjugacy structure.

|

For ﬁxed y, consider the mean ﬁeld family q(θ)q(γ)q(x) and the variational inference objective

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(3)

Without loss of generality we can take the global factor q(θ) to be in the same exponential family
as the prior p(θ), and we denote its natural parameters by ηθ. We restrict q(γ) to be in the same
exponential family as p(γ) with natural parameters ηγ. Finally, we restrict q(x) to be in the same
exponential family as p(x
θ), writing its natural parameter as ηx. Using these explicit variational
parameters, we write the mean ﬁeld variational inference objective in Eq. (3) as

(ηθ, ηγ, ηx).

|

(ηθ, ηγ, ηx), we consider choosing the variational
To perform efﬁcient optimization of the objective
L
parameter ηx as a function of the other parameters ηθ and ηγ. One natural choice is to set ηx to be a
. However, without conjugacy structure ﬁnding a local partial optimizer
local partial optimizer of
may be computationally expensive for general densities p(y
x, γ), and in the large data setting this
expensive optimization would have to be performed for each stochastic gradient update. Instead, we
choose ηx by optimizing over a surrogate objective (cid:98)
L
(cid:20)

with conjugacy structure, given by

L

(cid:21)

|

(ηθ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(x)

log

p(θ)p(x

|

θ) exp
{
q(θ)q(x)

ψ(x; y, φ)
}

, ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:105)

,

(cid:104)

L

5

r(y; φ)

}φ∈Rm is some parameterized class of functions that serves as the recognition model.
θ). We

where
{
Note that the potentials ψ(x; y, φ) have a form conjugate to the exponential family p(x
deﬁne η∗
x(ηθ, φ) (cid:44) arg min
η∗

along with the corresponding factor q∗(x),
x(ηθ, φ))

x(ηθ, φ) to be a local partial optimizer of (cid:98)
L
(ηθ, ηx, φ),
(cid:98)
L

η∗
x(ηθ, φ), tx(x)

q∗(x) = exp

log Zx(η∗

(cid:105) −

{(cid:104)

}

|

.

ηx

As with the variational autoencoder of Section 3.2, the resulting variational factor q∗(x) is suboptimal
for the variational objective
. However, because the surrogate objective has the same form as a
variational inference objective for a conjugate observation model, the factor q∗(x) not only is easy to
compute but also inherits exponential family and graphical model structure for tractable inference.

L

(ηθ, ηγ, η∗

x(ηθ, φ), the SVAE objective is

Given this choice of η∗
objective is a lower bound for the variational inference objective Eq. (3) in the following sense.
Proposition 4.1 (The SVAE objective lower-bounds the mean ﬁeld objective)
The SVAE objective function
max
q(x) L

in the sense that
Rm,

LSVAE(ηθ, ηγ, φ) (cid:44)

LSVAE lower-bounds the mean ﬁeld objective
L
≥ LSVAE(ηθ, ηγ, φ)
}φ∈Rm. Furthermore, if there is some φ∗
φ LSVAE(ηθ, ηγ, φ).
(ηθ, ηγ, ηx) = max

∈
x, γ), then the bound can be made tight in the sense that

for any parameterized function class
that ψ(x; y, φ∗) = E

q(γ) log p(y
|
[ q(θ)q(γ)q(x) ] = max

ηx L
r(y; φ)

[ q(θ)q(γ)q(x) ]

(ηθ, ηγ, ηx)

max
q(x) L

ηx L

max

≥

L

∈

φ

∀

{

x(ηθ, φ)). This

Rm such

LSVAE(ηθ, ηγ, φ) we are maximizing a lower
Thus by using gradient-based optimization to maximize
bound on the model log evidence log p(y). In particular, by optimizing over φ we are effectively
learning how to condition on observations so as to best approximate the posterior while maintaining
conjugacy structure. Furthermore, to provide the best lower bound we may choose the recognition
model function class

r(y; φ)

{

}φ∈Rm to be as rich as possible.

Choosing η∗
x(ηθ, φ) to be a local partial optimizer of (cid:98)
provides two computational advantages. First,
L
it allows η∗
x(ηθ, φ) and expectations with respect to q∗(x) to be computed efﬁciently by exploiting
exponential family graphical model structure. Second, it provides computationally efﬁcient ways to
estimate the natural gradient with respect to the latent model parameters, as we summarize next.
Proposition 4.2 (Natural gradient of the SVAE objective)
The natural gradient of the SVAE objective

LSVAE with respect to ηθ can be estimated as
∇

(cid:1) + (
(4)
θ, φ)). When there is only one local variational factor q(x), then we

q∗(x) [(tx(x), 1)]

2 log Zθ(ηθ))

θ + E

F (ηθ),

ηθ

∇

−

−1

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

(cid:101)
where F (η(cid:48)
(ηθ, ηγ, η∗
θ) =
can simplify the estimator to

x(η(cid:48)
∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

L

(cid:101)

θ + E

q∗(x) [(tx(x), 1)]

(cid:1) + (

ηθ

−

∇ηx L

(ηθ, ηγ, η∗

x(ηθ, φ)), 0).

(ηθ, ηγ, η∗

Note that the ﬁrst term in Eq. (4) is the same as the expression for the natural gradient in SVI for
F (ηθ) in the ﬁrst expression or, alternatively,
conjugate models [10], while a stochastic estimate of
x(ηθ, φ)) in the second expression is computed automatically
a stochastic estimate of
∇ηθ L
as part of the backward pass for computing the gradients with respect to the other parameters, as
described next. Thus we have an expression for the natural gradient with respect to the latent
model’s parameters that is almost as simple as the one for conjugate models, differing only by a term
involving the neural network likelihood function. Natural gradients are invariant to smooth invertible
reparameterizations of the variational family [16, 17] and provide effective second-order optimization
updates [18, 11].

∇

The gradients of the objective with respect
to the other variational parameters, namely
∇φLSVAE(ηθ, ηγ, φ), can be computed using the reparameterization trick
∇ηγ LSVAE(ηθ, ηγ, φ) and
and standard automatic differentiation techniques. To isolate the terms that require the reparameteri-
zation trick, we rearrange the objective as
q(γ)q∗(x) log p(y

−
The KL divergence terms are between members of the same tractable exponential families. An
unbiased estimate of the ﬁrst term can be computed by sampling ˆx
q(γ) and
computing

LSVAE(ηθ, ηγ, φ) = E

ˆx, ˆγ) with automatic differentiation.

KL(q(θ)q∗(x)

q∗(x) and ˆγ

KL(q(γ)

p(θ, x))

p(γ)).

x, γ)

−

∼

∼

(cid:107)

(cid:107)

|

∇ηγ ,φ log p(y

|

6

5 Related work

In addition to the papers already referenced, there are several recent papers to which this work is
related.

The two papers closest to this work are Krishnan et al. [19] and Archer et al. [20]. In Krishnan et al.
[19] the authors consider combining variational autoencoders with continuous state-space models,
emphasizing the relationship to linear dynamical systems (also called Kalman ﬁlter models). They
primarily focus on nonlinear dynamics and an RNN-based variational family, as well as allowing
control inputs. However, the approach does not extend to general graphical models or discrete latent
variables. It also does not leverage natural gradients or exact inference subroutines.

In Archer et al. [20] the authors also consider the problem of variational inference in general
continuous state space models but focus on using a structured Gaussian variational family without
considering parameter learning. As with Krishnan et al. [19], this approach does not include discrete
latent variables (or any latent variables other than the continuous states). However, the method they
develop could be used with an SVAE to handle inference with nonlinear dynamics.

In addition, both Gregor et al. [21] and Chung et al. [22] extend the variational autoencoder framework
to sequential models, though they focus on RNNs rather than probabilistic graphical models.

Finally, there is much related work on handling nonconjugate model terms in mean ﬁeld variational
inference. In Khan et al. [23] and Khan et al. [24] the authors present a general scheme that is
able to exploit conjugate exponential family structure while also handling arbitrary nonconjugate
model factors, including the nonconjugate observation models we consider here. In particular, they
propose using a proximal gradient framework and splitting the variational inference objective into a
difﬁcult term to be linearized (with respect to mean parameters) and a tractable concave term, so that
the resulting proximal gradient update is easy to compute, just like in a fully conjugate model. In
Knowles et al. [25], the authors propose performing natural gradient descent with respect to natural
parameters on each of the variational factors in turn, and they focus on approximating expectations of
nonconjugate energy terms in the objective with model-speciﬁc lower-bounds (rather than estimating
them with generic Monte Carlo). As in conjugate SVI [10], they observe that, on conjugate factors
and with an undamped update (i.e. a unit step size), the natural gradient update reduces to the standard
conjugate mean ﬁeld update.

In contrast to the approaches of Khan et al. [23], Khan et al. [24], and Knowles et al. [25], rather
than linearizing intractable terms around the current iterate, in this work we handle intractable terms
via recognition networks and amoritized inference (and the remaining tractable objective terms are
multi-concave in general, analogous to SVI [10]). That is, we use parametric function approximators
to learn to condition on evidence in a conjugate form. We expect these approaches to handling
nonconjugate objective terms may be complementary, and the best choice may be situation-dependent.
For models with local latent variables and datasets where minibatch-based updating is important,
using inference networks to compute local variational parameters in a ﬁxed-depth circuit (as in
the VAE [7, 8]) or optimizing out the local variational factors using fast conjugate updates (as in
conjugate SVI [10]) can be advantageous because in both cases local variational parameters for the
entire dataset need not be maintained across updates. The SVAE we propose here is a way to combine
the inference network and conjugate SVI approaches.

6 Experiments

We apply the SVAE to both synthetic and real data and demonstrate its ability to learn feature
representations and latent structure. Code is available at github.com/mattjj/svae.

6.1 LDS SVAE for modeling synthetic data

Consider a sequence of 1D images representing a dot bouncing from one side of the image to the
other, as shown at the top of Fig. 4. We use an LDS SVAE to ﬁnd a low-dimensional latent state
space representation along with a nonlinear image model. The model is able to represent the image
accurately and to make long-term predictions with uncertainty. See supplementals for details.

7

(a) Predictions after 200 training steps.

(b) Predictions after 1100 training steps.

Figure 4: Predictions from an LDS SVAE ﬁt to 1D dot image data at two stages of training. The
top panel shows an example sequence with time on the horizontal axis. The middle panel shows the
noiseless predictions given data up to the vertical line, while the bottom panel shows the latent states.

(a) Natural (blue) and standard (orange) gradient updates.

(b) Subspace of learned observation model.

Figure 5: Experimental results from LDS SVAE models on synthetic data and real mouse data.

This experiment also demonstrates the optimization advantages that can be provided by the natural
gradient updates. In Fig. 5a we compare natural gradient updates with standard gradient updates at
three different learning rates. The natural gradient algorithm not only learns much faster but also
is less dependent on parameterization details: while the natural gradient update used an untuned
stepsize of 0.1, the standard gradient dynamics at step sizes of both 0.1 and 0.05 resulted in some
matrix parameters to be updated to indeﬁnite values.

6.2 LDS SVAE for modeling video

We also apply an LDS SVAE to model depth video recordings of mouse behavior. We use the dataset
from Wiltschko et al. [3] in which a mouse is recorded from above using a Microsoft Kinect. We
used a subset consisting of 8 recordings, each of a distinct mouse, 20 minutes long at 30 frames per
second, for a total of 288000 video fames downsampled to 30

30 pixels.

We use MLP observation and recognition models with two hidden layers of 200 units each and a 10D
latent space. Fig. 5b shows images corresponding to a regular grid on a random 2D subspace of the
latent space, illustrating that the learned image manifold accurately captures smooth variation in the
mouse’s body pose. Fig. 6 shows predictions from the model paired with real data.

×

6.3 SLDS SVAE for parsing behavior

Finally, because the LDS SVAE can accurately represent the depth video over short timescales, we
apply the latent switching linear dynamical system (SLDS) model to discover the natural units of
behavior. Fig. 7 and Fig. 8 in the appendix show some of the discrete states that arise from ﬁtting an
SLDS SVAE with 30 discrete states to the depth video data. The discrete states that emerge show a
natural clustering of short-timescale patterns into behavioral units. See the supplementals for more.

8

Figure 6: Predictions from an LDS SVAE ﬁt to depth video. In each panel, the top is a sampled
prediction and the bottom is real data. The model is conditioned on observations to the left of the line.

(a) Extension into running

(b) Fall from rear

Figure 7: Examples of behavior states inferred from depth video. Each frame sequence is padded on
both sides, with a square in the lower-right of a frame depicting when the state is the most probable.

7 Conclusion

Structured variational autoencoders provide a general framework that combines some of the strengths
of probabilistic graphical models and deep learning methods. In particular, they use graphical models
both to give models rich latent representations and to enable fast variational inference with CRF-like
structured approximating distributions. To complement these structured representations, SVAEs use
neural networks to produce not only ﬂexible nonlinear observation models but also fast recognition
networks that map observations to conjugate graphical model potentials.

9

References

MIT Press, 2009.

[1] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.

[2] Kevin P Murphy. Machine Learning: a Probabilistic Perspective. MIT Press, 2012.
[3] Alexander B. Wiltschko, Matthew J. Johnson, Giuliano Iurilli, Ralph E. Peterson, Jesse M.
Katon, Stan L. Pashkovski, Victoria E. Abraira, Ryan P. Adams, and Sandeep Robert Datta.
“Mapping Sub-Second Structure in Mouse Behavior”. In: Neuron 88.6 (2015), pp. 1121–1135.
[4] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. “Deep neural
networks for acoustic modeling in speech recognition: The shared views of four research
groups”. In: Signal Processing Magazine, IEEE 29.6 (2012), pp. 82–97.

[5] Li Deng. “Computational models for speech production”. In: Computational Models of Speech

Pattern Processing. Springer, 1999, pp. 199–213.

[6] Li Deng. “Switching dynamic system models for speech articulation and acoustics”. In:
Mathematical Foundations of Speech and Language Processing. Springer, 2004, pp. 115–133.
[7] Diederik P. Kingma and Max Welling. “Auto-Encoding Variational Bayes”. In: International

Conference on Learning Representations (2014).

[8] Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. “Stochastic Backpropagation and
Approximate Inference in Deep Generative Models”. In: Proceedings of the 31st International
Conference on Machine Learning. 2014, pp. 1278–1286.

[9] Matthew J. Johnson and Alan S. Willsky. “Stochastic Variational Inference for Bayesian Time

Series Models”. In: International Conference on Machine Learning. 2014.

[10] Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. “Stochastic variational

[11]

inference”. In: Journal of Machine Learning Research (2013).
James Martens. “New insights and perspectives on the natural gradient method”. In: arXiv
preprint arXiv:1412.1193 (2015).

[12] David J.C. MacKay and Mark N. Gibbs. “Density networks”. In: Statistics and neural networks:

advances at the interface. Oxford University Press, Oxford (1999), pp. 129–144.

[13] Tomoharu Iwata, David Duvenaud, and Zoubin Ghahramani. “Warped Mixtures for Nonpara-
metric Cluster Shapes”. In: Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2013,
pp. 311–319.

[14] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. “Bayesian Nonparametric Inference
of Switching Dynamic Linear Models”. In: IEEE Transactions on Signal Processing 59.4
(2011).

[15] Martin J. Wainwright and Michael I. Jordan. “Graphical Models, Exponential Families, and

Variational Inference”. In: Foundations and Trends in Machine Learning (2008).

[16] Shun-Ichi Amari. “Natural gradient works efﬁciently in learning”. In: Neural computation

10.2 (1998), pp. 251–276.

[17] Shun-ichi Amari and Hiroshi Nagaoka. Methods of Information Geometry. American Mathe-

[18]

matical Society, 2007.
James Martens and Roger Grosse. “Optimizing Neural Networks with Kronecker-factored
Approximate Curvature”. In: arXiv preprint arXiv:1503.05671 (2015).

[19] Rahul G Krishnan, Uri Shalit, and David Sontag. “Deep Kalman Filters”. In: arXiv preprint

arXiv:1511.05121 (2015).

[20] Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. “Black
box variational inference for state space models”. In: arXiv preprint arXiv:1511.07367 (2015).
[21] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. “DRAW: A recurrent neural

[22]

network for image generation”. In: arXiv preprint arXiv:1502.04623 (2015).
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua
Bengio. “A recurrent latent variable model for sequential data”. In: Advances in Neural
information processing systems. 2015, pp. 2962–2970.

[23] Mohammad E Khan, Pierre Baqué, François Fleuret, and Pascal Fua. “Kullback-Leibler
proximal variational inference”. In: Advances in Neural Information Processing Systems. 2015,
pp. 3402–3410.

10

[24] Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, and Masashi Sugiyama.
“Faster Stochastic Variational Inference using Proximal-Gradient Methods with General Diver-
gence Functions”. In: Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2016.
[25] David A Knowles and Tom Minka. “Non-conjugate variational message passing for multino-
mial and binary regression”. In: Advances in Neural Information Processing Systems. 2011,
pp. 1701–1709.

[26] Dimitri P Bertsekas. Nonlinear programming. 2nd ed. Athena Scientiﬁc, 1999.
[27]

John M. Danskin. The theory of max-min and its application to weapons allocation problems.
Springer-Verlag, New York, 1967.

[28] Anthony-V Fiacco. Introduction to sensitivity and stability analysis in nonlinear programming.

[29]

[30]

Academic Press, Inc., 1984.
J Frederic Bonnans and Alexander Shapiro. Perturbation Analysis of Optimization Problems.
Springer Science & Business Media, 2000.
James Martens and Roger Grosse. “Optimizing Neural Networks with Kronecker-factored
Approximate Curvature”. In: Proceedings of the 32nd International Conference on Machine
Learning. 2015.

[31] David Duvenaud and Ryan P. Adams. “Black-box stochastic variational inference in ﬁve lines

of Python”. In: NIPS Workshop on Black-box Learning and Inference (2015).

11

In this section we ﬁx our notation for gradients and establish some basic deﬁnitions and results that
we use in the sequel.

A Optimization

A.1 Gradient notation

We follow the notation in Bertsekas [26, A.5]. In particular, if f : Rn
differentiable function, we deﬁne the gradient matrix of f , denoted
which the ith column is the gradient
That is,

Rm is a continuously
m matrix in
∇
fi(x) of fi, the ith coordinate function of f , for i = 1, 2, . . . , m.

f (x), to be the n

→

∇

×

f (x) = [

f1(x)

∇

∇

· · · ∇

fm(x)] .

∇

f is the Jacobian matrix of f , in which the ijth entry is the function ∂fi/∂xj.
R is continuously differentiable with continuously differentiable partial derivatives,
2f , to be the matrix in which the ijth entry is the

The transpose of
If f : Rn
→
then we deﬁne the Hessian matrix of f , denoted
function ∂2f /∂xi∂xj.
Rm
Finally, if f : Rn

R is a function of (x, y) with x

Rn and y

Rm, we write

∇

×

→

∈

∇xf (x, y) =

∇yf (x, y) =







∂f (x,y)
∂x1

...

∂f (x,y)
∂xm
(cid:18) ∂2f (x, y)
∂xi∂xj







,

(cid:19)

,

2
xyf (x, y) =

∇

2
yyf (x, y) =

∇

(cid:18) ∂2f (x, y)
∂xi∂yj

(cid:19)

.

2
xxf (x, y) =

∇

∈
∂f (x,y)
∂y1

...













∂f (x,y)
∂yn
(cid:18) ∂2f (x, y)
∂yi∂yj

(cid:19)

,

A.2 Local and partial optimizers

In this section we state the deﬁnitions of local partial optimizer and necessary conditions for optimality
that we use in the sequel.

Deﬁnition A.1 (Partial optimizer, local partial optimizer)
Let f : Rn
y∗

Rm an unconstrained partial optimizer of f given x if

R be an objective function to be maximized. For a ﬁxed x

Rm

→

×

∈

Rn, we call a point

∈

and we call y∗ an unconstrained local partial optimizer of f given x if there exists an (cid:15) > 0 such that

f (x, y)

f (x, y∗)

≤

Rm

y

∀

∈

f (x, y)

f (x, y∗)

≤

y with

∀

y

(cid:107)

−

y∗
(cid:107)

< (cid:15),

where

is any vector norm.

(cid:107) · (cid:107)

×

→

Rm

R be continuously differentiable. For ﬁxed x

Proposition A.2 (Necessary conditions for optimality, Prop. 3.1.1 of Bertsekas [26])
Let f : Rn
Rn if y∗
unconstrained local partial optimizer for f given x then
∇yf (x, y∗) = 0.
If instead x and y are subject to the constraints h(x, y) = 0 for some continuously differentiable
h : Rn
Rm and y∗ is a constrained local partial optimizer for f given x with the regularity
Rm
×
condition that

→
∇yh(x, y∗) is full rank, then there exists a Lagrange multiplier λ∗

Rm such that

Rm is an

∈

∈

∈

∇yf (x, y∗) +

∇yh(x, y∗)λ∗ = 0,

and hence the cost gradient
by the null space of

∇yh(x, y∗)T.

∇yf (x, y∗) is orthogonal to the ﬁrst-order feasible variations in y given

12

Note that the regularity condition on the constraints is not needed if the constraints are linear [26,
Prop. 3.3.7].
For a continuously differentiable function f : Rn

R, we say x∗ is a stationary point of f if
f (x∗) = 0. For general unconstrained smooth optimization, the limit points of gradient-based
∇
algorithms are guaranteed only to be stationary points of the objective, not necessarily local optima.
Block coordinate ascent methods, when available, provide slightly stronger guarantees: not only is
every limit point a stationary point of the objective, in addition each coordinate block is a partial
optimizer of the objective. Note that the objective functions we consider maximizing in the following
are bounded above.

→

A.3 Partial optimization and the Implicit Function Theorem

Let f : Rn
x
∈
y∗(x)

×
Rn and y

∈

Rm

R be a scalar-valued objective function of two unconstrained arguments
Rn a value
Rm be some function that assigns to each x

Rm, and let y∗ : Rn

→

Rm. Deﬁne the composite function g : Rn

→

∈

R as

∈

→

g(x) (cid:44) f (x, y∗(x))

and using the chain rule write its gradient as

g(x) =

∇

∇xf (x, y∗(x)) +

∇

y∗(x)

∇yf (x, y∗(x)).

(5)

∈

One choice of the function y∗(x) is to partially optimize f for any ﬁxed value of x. For example,
Rn, we could choose y∗ to satisfy
assuming that arg maxy f (x, y) is nonempty for every x
y∗(x)
arg maxy f (x, y), so that g(x) = maxy f (x, y).1 Similarly, if y∗(x) is chosen so that
∇yf (x, y∗(x)) = 0, which is satisﬁed when y∗(x) is an unconstrained local partial optimizer for f
given x, then the expression in Eq. (5) can be simpliﬁed as in the following proposition.
Proposition A.3 (Gradients of locally partially optimized objectives)
Let f : Rn
such that y∗(x) is differentiable, and deﬁne g(x) = f (x, y∗(x)). Then

R be continuously differentiable, let y∗ be a local partial optimizer of f given x

Rm

→

×

∈

g(x) =

∇

∇xf (x, y∗(x)).

Proof. If y∗ is an unconstrained local partial optimizer of f given x then it satisﬁes
and if y∗ is a regularly-constrained local partial optimizer then the feasible variation
orthogonal to the cost gradient
in Eq. (5) is zero.

∇yf (x, y∗) = 0,
y∗(x) is
∇
∇yf (x, y∗). In both cases the second term in the expression for
g(x)

∇

∇

In general, when y∗(x) is not a stationary point of f (x,
g(x) we
), to evaluate the gradient
·
y∗(x) in Eq. (5). However, this term may be difﬁcult to compute directly. The
need to evaluate
function y∗(x) may arise implicitly from some system of equations of the form h(x, y) = 0 for
Rm. For example, the value of y may
some continuously differentiable function h : Rn
→
be computed from x and h using a black-box iterative numerical algorithm. However, the Implicit
y∗(x) using only the derivatives of h and
Function Theorem provides another means to compute
the value of y∗(x).
Proposition A.4 (Implicit Function Theorem, Prop. A.25 of Bertsekas [26])
Let h : Rn

Rm be a function and ¯x

Rm be points such that

Rn and ¯y

Rm

Rm

∇

∇

×

∈

∈

×

→
1. h(¯x, ¯y) = 0

2. h is continuous and has a continuous nonsingular gradient matrix

∇yh(x, y) in an open set

containing (¯x, ¯y).

Then there exist open sets S¯x ⊆
function y∗ : S¯x →

S¯y such that ¯y = y∗(x) and h(x, y∗(x)) = 0 for all x

Rm containing ¯x and ¯y, respectively, and a continuous
S¯x. The function y∗ is

Rn and S¯y ⊆

∈

1For a discussion of differentiability issues when there is more than one optimizer, i.e. when arg maxy f (x, y)
has more than one element, see Danskin [27], Fiacco [28, Section 2.4], and Bonnans et al. [29, Chapter 4]. Here
we only consider the sensitivity of local stationary points and assume differentiability almost everywhere.

13

unique in the sense that if x
some p > 0, h is p times continuously differentiable, the same is true for y∗, and we have

S¯y, and h(x, y) = 0, then y = y∗(x). Furthermore, if for

S¯x, y

∈

∈

y∗(x) =

∇

−∇xh (x, y∗(x)) (

∇yh (x, y∗(x)))−1 ,

x

∀

∈

S¯x.

As a special case, the equations h(x, y) = 0 may be the ﬁrst-order stationary conditions of another
unconstrained optimization problem. That is, the value of y may be chosen by locally partially
optimizing the value of u(x, y) for a function u : Rn
R with no constraints on y, leading to
the following corollary.
Corollary A.5 (Implicit Function Theorem for optimization subroutines)
Let u : Rn
∇yu
×
satisﬁes the hypotheses of Proposition A.4 at some point (¯x, ¯y), and deﬁne y∗ as in Proposition A.4.
Then we have

R be a twice continuously differentiable function such that the choice h =

Rm

Rm

→

→

×

y∗(x) =

∇

−∇

2

xyu (x, y∗(x)) (cid:0)

2

yyu (x, y∗(x))(cid:1)−1

,

∇

x

∀

∈

S¯x.

B Exponential families

In this section we set up notation for exponential families and outline some basic results. Throughout
this section we take all densities to be absolutely continuous with respect to the appropriate Lebesgue
measure (when the underlying set
is discrete),
X
) (generated by Euclidean and discrete topologies,
and denote the Borel σ-algebra of a set
respectively). We assume measurability of all functions as necessary.

is Euclidean space) or counting measure (when

as

X

X

X

B

(

Given a statistic function tx :
of probability densities on

Rn and a base measure νX , we can deﬁne an exponential family

X →
relative to νX and indexed by natural parameter ηx ∈
,
(cid:105)}
|

ηx, tx(x)
is the standard inner product on Rn. We also deﬁne the partition function as

ηx ∈
∀

X
p(x

Rn,

ηx)

exp

{(cid:104)

∝

Rn by

where

,
(cid:104)·

·(cid:105)

(cid:90)

Zx(ηx) (cid:44)

exp

ηx, tx(x)

νX (dx)

{(cid:104)

(cid:105)}

and deﬁne H

Rn to be the set of all normalizable natural parameters,

⊆

H (cid:44)

η
{

∈

Rn : Zx(η) <

.

∞}

We can write the normalized probability density as

p(x

η) = exp

ηx, tx(x)

log Zx(ηx)

.

|

{(cid:104)
We say that an exponential family is regular if H is open, and minimal if there is no η
0
}
= 0 (νX -a.e.). We assume all families are regular and minimal.2 Finally, when
such that
we parameterize the family with some other coordinates θ, we write the natural parameter as a
continuous function ηx(θ) and write the density as

η, tx(x)
(cid:105)

(cid:105) −

\ {

Rn

∈

}

(cid:104)

(6)

p(x

θ) = exp

ηx(θ), tx(x)

log Zx(ηx(θ))

(cid:105) −
x (H) to be the open set of parameters that correspond to normalizable densities.

{(cid:104)

}

|

and take Θ = η−1
We summarize this notation in the following deﬁnition.
Deﬁnition B.1 (Exponential family of densities)
Given a measure space (
function ηx : Θ

(
X

X

B

,

), νX ), a statistic function tx :

→

Rn, the corresponding exponential family of densities relative to νX is

X →

Rn, and a natural parameter

where

is the log partition function.

p(x

θ) = exp

ηx(θ), tx(x)

log Zx(ηx(θ))

,

|

{(cid:104)

(cid:105) −

}

log Zx(ηx) (cid:44) log

(cid:90)

exp

ηx, tx(x)

νX (dx)

{(cid:104)

(cid:105)}

2Families that are not minimal, like the density of the categorical distribution, can be treated by restricting all
algebraic operations to the subspace spanned by the statistic, i.e. to the smallest V ⊂ Rn with range tx ⊆ V .

14

When we write exponential families of densities for different random variables, we change the
subscripts on the statistic function, natural parameter function, and log partition function to correspond
to the symbol used for the random variable. When the corresponding random variable is clear from
context, we drop the subscripts to simplify notation.

The next proposition shows that the log partition function of an exponential family generates cumu-
lants of the statistic.
Proposition B.2 (Gradients of log Z and expected statistics)
The gradient of the log partition function of an exponential family gives the expected sufﬁcient
statistic,

log Z(η) = E

p(x | η) [t(x)] ,

∇

where the expectation is over the random variable x with density p(x
generating function of t(x) can be written

|

η). More generally, the moment

Mt(x)(s) (cid:44) E

p(x | η)

(cid:104)

e(cid:104)s,t(x)(cid:105)(cid:105)

= elog Z(η+s)−log Z(η)

and so derivatives of log Z give cumulants of t(x), where the ﬁrst cumulant is the mean and the
second and third cumulants are the second and third central moments, respectively.

Given an exponential family of densities on
as in Deﬁnition B.1, we can deﬁne a related exponential
family of densities on Θ by deﬁning a statistic function tθ(θ) in terms of the functions ηx(θ) and
log Zx(ηx(θ)).
Deﬁnition B.3 (Natural exponential family conjugate prior)
Given the exponential family p(x
as the concatenation

θ) of Deﬁnition B.1, deﬁne the statistic function tθ : Θ

Rn+1

→

X

|

−
where the ﬁrst n coordinates of tθ(θ) are given by ηx(θ) and the last coordinate is given by
log Zx(ηx(θ)). We call the exponential family with statistic tθ(θ) the natural exponential family

tθ(θ) (cid:44) (ηx(θ),

log Zx(ηx(θ))) ,

−
conjugate prior to the density p(x

θ) and write

|

p(θ) = exp

ηθ, tθ(θ)

{(cid:104)

log Zθ(ηθ)
}

(cid:105) −

Rn+1 and the density is taken relative to some measure νΘ on (Θ,

where ηθ ∈
Notice that using tθ(θ) we can rewrite the original density p(x

θ) as

B

(Θ)).

|

p(x

log Zx(ηx(θ))

θ) = exp
= exp

ηx(θ), tx(x)
(cid:105) −
tθ(θ), (tx(x), 1)

|

{(cid:104)
{(cid:104)
This relationship is useful in Bayesian inference: when the exponential family p(x
θ) is a likelihood
function and the family p(θ) is used as a prior, the pair enjoy a convenient conjugacy property, as
summarized in the next proposition.
Proposition B.4 (Conjugacy)
Let the densities p(x
relations

θ) and p(θ) be deﬁned as in Deﬁnitions B.1 and B.3, respectively. We have the

(cid:105)}

}

|

|

.

p(θ, x) = exp
x) = exp
p(θ

|
and hence in particular the posterior p(θ
parameter ηθ + (tx(x), 1). Similarly, with multiple likelihood terms p(xi |
have

ηθ + (tx(x), 1), tθ(θ)
ηθ + (tx(x), 1), tθ(θ)

log Zθ(ηθ)
}
log Zθ(ηθ + (tx(x), 1))
}
x) is in the same exponential family as p(θ) with the natural
θ) for i = 1, 2, . . . , N we

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

(7)

|

N
(cid:89)

i=1

(cid:40)

N
(cid:88)

i=1

p(θ)

p(xi |

θ) = exp

ηθ +
(cid:104)

(tx(xi), 1), tθ(θ)

log Zθ(ηθ)

.

(8)

(cid:105) −

(cid:41)

Finally, we give a few more exponential family properties that are useful for gradient-based optimiza-
tion algorithms and variational inference. In particular, we note that the Fisher information matrix of
an exponential family can be computed as the Hessian matrix of its log partition function, and that
the KL divergence between two members of the same exponential family has a simple expression.

15

Deﬁnition B.5 (Score vector and Fisher information matrix)
Given a family of densities p(x
of the log density with respect to the parameter,
v(x, θ) (cid:44)
and the Fisher information matrix for the parameter θ is the covariance of the score,

∇θ log p(x

θ),

|

|

θ) indexed by a parameter θ, the score vector v(x, θ) is the gradient

I(θ) (cid:44) E (cid:2)v(x, θ)v(x, θ)T(cid:3) ,

where the expectation is taken over the random variable x with density p(x
used the identity E[v(x, θ)] = 0.
Proposition B.6 (Score and Fisher information for exponential families)
Given an exponential family of densities p(x
the score with respect to the natural parameter is given by
η) = t(x)

|
∇η log p(x
and the Fisher information matrix is given by
I(η) =

2 log Z(η).

v(x, η) =

log Z(η)

− ∇

|

|

∇

θ), and where we have

η) indexed by the natural parameter η, as in Eq. (6),

Proposition B.7 (KL divergence in an exponential family)
Given an exponential family of densities p(x
η) indexed by the natural parameter η, as in Eq. (6),
and two particular members with natural parameters η1 and η2, respectively, the KL divergence from
one to the other is

|

KL(p(x

η1)

p(x

η2)) (cid:44) E

p(x | η1)

|

(cid:107)

|

(cid:20)

log

(cid:21)

p(x
p(x

η1)
|
η2)
|
log Z(η1)

=

η1 −

(cid:104)

η2,

∇

(log Z(η1)

log Z(η2)).

(cid:105) −

−

C Natural gradient SVI for exponential families

In this section we give a derivation of the natural gradient stochastic variational inference (SVI)
method of Hoffman et al. [10] using our notation. We extend the algorithm in Section D.

C.1 SVI objective

θ) be an exponential family and p(θ) be its corresponding natural exponential family

|

Let p(x, y
prior as in Deﬁnitions B.1 and B.3, writing
p(θ) = exp (cid:8)
η0
θ , tθ(θ)
(cid:104)
θ) = exp (cid:8)
η0
xy(θ), txy(x, y)
(cid:104)
(cid:105) −
tθ(θ), (txy(x, y), 1)
= exp
where we have used tθ(θ) = (cid:0)η0
log Zxy(η0
xy(θ),
Given a ﬁxed observation y, for any density q(θ, x) = q(θ)q(x) we have
(cid:20)

(cid:105)}
xy(θ))(cid:1) in Eq. (9).

θ )(cid:9)
log Zxy(η0

log Zθ(η0

p(x, y

(cid:105) −

{(cid:104)

−

(cid:21)

|

xy(θ))(cid:9)

log p(y) = E

q(θ)q(x)

log

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)
p(θ)p(x, y
|
q(θ)q(x)

θ)

(cid:21)

θ)

E

≥

q(θ)q(x)

+ KL(q(θ)q(x)

p(θ, x

y))

(cid:107)

|

where we have used the fact that the KL divergence is always nonnegative. Therefore to choose
q(θ)q(x) to minimize the KL divergence to the posterior p(θ, x
y) we deﬁne the mean ﬁeld varia-
tional inference objective as

|

[ q(θ)q(x) ] (cid:44) E

q(θ)q(x)

L

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)

(cid:21)

θ)

and the mean ﬁeld variational inference problem as
maxq(θ)q(x)L

[ q(θ)q(x) ] .

(9)

(10)

(11)

The following proposition shows that because of the exponential family conjugacy structure, we
can ﬁx the parameterization of q(θ) and still optimize over all possible densities without loss of
generality.

16

Proposition C.1 (Optimal form of the global variational factor)
Given the mean ﬁeld optimization problem Eq. (11), for any ﬁxed q(x) the optimal factor q(θ) is
detetermined (νΘ-a.e.) by

θ + E
η0
(cid:104)
In particular, the optimal q(θ) is in the same exponential family as the prior p(θ).

q(x) [ (txy(x, y), 1) ] , tθ(θ)
(cid:105)

q(θ)

∝

exp (cid:8)

(cid:9) .

This proposition follows immediately from a more general lemma, which we reuse in the sequel.
Lemma C.2 (Optimizing a mean ﬁeld factor)
Let p(a, b, c) be a joint density and let q(a), q(b), and q(c) be mean ﬁeld factors. Consider the mean
ﬁeld variational inference objective

E

q(a)q(b)q(c)

(cid:20)

log

p(a, b, c)
q(a)q(b)q(c)

(cid:21)

.

For ﬁxed q(a) and q(c), the partially optimal factor q∗(b) over all possible densities,

q∗(b) (cid:44) arg max

E

q(a)q(b)q(c)

q(b)

(cid:20)

log

p(a, b, c)
q(a)q(b)q(c)

(cid:21)

,

(12)

is deﬁned (almost everywhere) by

In particular, if p(c
|
prior, and log p(b, c

|

q∗(b)

exp (cid:8)E

q(a)q(c) log p(a, b, c)(cid:9) .

∝
b, a) is an exponential family with p(b
|
a) is a multilinear polynomial in the statistics tb(b) and tc(c), written

a) its natural exponential family conjugate

log Zb(η0

b (a))(cid:9) ,

|

p(b

p(c

a) = exp (cid:8)
(cid:104)
|
b, a) = exp (cid:8)
(cid:104)
= exp (cid:8)
(cid:104)
c (a), then the optimal factor can be written
η∗
q(a)η0
b , tb(b)

η0
b (a), tb(b)
(cid:105) −
η0
c (b, a), tc(c)
tb(b), η0

(cid:105) −
c (a)T(tc(c), 1)

log Zb(η∗
b )

(cid:44) E

η∗
b

,

log Zc(η0
(cid:9) ,
(cid:105)

c (b, a))(cid:9)

b (a) + E

q(a)q(c)η0

{(cid:104)

(cid:105) −

As a special case, when c is conditionally independent of b given a, so that p(c
b (a) + E

tb(b), (tc(c), 1)

b) = exp

q(a)η0

(cid:44) E

p(c

η∗
b

,

|

{(cid:104)

}

(cid:105)}

for some matrix η0
q∗(b) = exp

c (a)T(tc(c), 1).
b, a) = p(c
q(c)(tc(c), 1).

|

|

b), then

Proof. Rewrite the objective in Eq. (12), dropping terms that are constant with respect to q(b), as

E

q(a)q(b)q(c)

(cid:20)

log

(cid:21)

p(a, b, c)
q(b)

= E

q(b)

(cid:2)E

q(a)q(c) log p(a, b, c)

log q(b)(cid:3)

−

q(a)q(c) log p(a, b, c)

log q(b)(cid:3)

−

= E

q(b)

(cid:2)log exp E
(cid:21)
(cid:20) q(b)
(cid:101)p(b)

E

q(b)

−

=

=

KL(q(b)

+ const

−
where we have deﬁned a new density (cid:101)p(b)
objective by setting the KL divergence to zero, choosing q(b)
rest follows from plugging in the exponential family densities.

exp (cid:8)E

∝

∝

(cid:107) (cid:101)p(b)) + const,
q(a)q(c) log p(a, b, c)(cid:9). We can maximize the
q(a)q(c) log p(a, b, c)(cid:9). The

exp (cid:8)E

Proposition C.1 justiﬁes parameterizing the density q(θ) with variational natural parameters ηθ as
ηθ, tθ(θ)
where the statistic function tθ and the log partition function log Zθ are the same as in the prior
family p(θ). Using this parameterization, we can deﬁne the mean ﬁeld objective as a function of the
parameters ηθ, partially optimizing over q(x),

log Zθ(ηθ)
}

q(θ) = exp

(cid:105) −

{(cid:104)

(ηθ) (cid:44) max
q(x)

E

L

q(θ)q(x)

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)

(cid:21)

θ)

.

(13)

The partial optimization over q(x) in Eq. (13) should be read as choosing q(x) to be a local partial
optimizer of Eq. (10); in general, it may be intractable to ﬁnd a global partial optimizer, and the results
that follow use only ﬁrst-order stationary conditions on q(x). We refer to this objective function,
where we locally partially optimize the mean ﬁeld objective Eq. (10) over q(x), as the SVI objective.

17

C.2 Easy natural gradients of the SVI objective

By again leveraging the conjugate exponential family structure, we can write a simple expression for
the gradient of the SVI objective, and even for its natural gradient.
Proposition C.3 (Gradient of the SVI objective)
Let the SVI objective

(ηθ) be deﬁned as in Eq. (13). Then the gradient
L
(ηθ) = (cid:0)

∇L
q∗(x) [ (txy(x, y), 1) ]

2 log Zθ(ηθ)(cid:1) (cid:0)η0

θ + E

(ηθ) is
(cid:1)

ηθ

∇L

∇

where q∗(x) is a local partial optimizer of the mean ﬁeld objective Eq. (10) for ﬁxed global variational
parameters ηθ.

−

Proof. First, note that because q∗(x) is a local partial optimizer for Eq. (10) by Proposition A.3, we
have

(ηθ) =

E

∇ηθ

∇L

q(θ)q∗(x)

(cid:20)

log

p(θ)p(x, y

|
q(θ)q∗(x)

(cid:21)

θ)

.

Next, we use the conjugate exponential family structure and Proposition B.4, Eq. (7), to expand

E

q(θ)q∗(x)

(cid:20)

log

(cid:21)

θ)

p(θ)p(x, y

|
q(θ)q∗(x)

=

θ + E
η0
(cid:104)

−
Note that we can use Proposition B.2 to replace E
respect to ηθ and using the product rule, we have
2 log Zθ(ηθ) (cid:0)η0

(ηθ) =

∇L

∇

θ + E

q∗(x)(txy(x, y), 1)

ηθ, E

q(θ)[tθ(θ)]
(cid:105)

−
log Zθ(ηθ)(cid:1) .

(cid:0)log Zθ(η0
θ )
q(θ)[tθ(θ)] with

−

∇

log Zθ(ηθ). Differentiating with

log Zθ(ηθ) +

− ∇
2 log Zθ(ηθ) (cid:0)η0

∇
θ + E

=

∇

q∗(x)(txy(x, y), 1)
log Zθ(ηθ)
q∗(x)(txy(x, y), 1)

(cid:1)

ηθ

(cid:1) .

ηθ

−

−

As an immediate result of Proposition C.3, the natural gradient [16] deﬁned by
(ηθ) (cid:44) (cid:0)

2 log Zθ(ηθ)(cid:1)−1

(ηθ)

(cid:101)
∇L

∇

∇L

has an even simpler expression.
Corollary C.4 (Natural gradient of the SVI objective)
The natural gradient of the SVI objective Eq. (13) is

(cid:101)
∇L

(ηθ) = η0

θ + E

q∗(x)[ (txy(x, y), 1) ]

ηθ.

−

The natural gradient corrects for a kind of curvature in the variational family and is invariant to
reparameterization of the family [17]. As a result, natural gradient ascent is effectively a second-
order quasi-Newton optimization algorithm, and using natural gradients can greatly accelerate the
convergence of gradient-based optimization algorithms [30, 11]. It is a remarkable consequence of
the exponential family structure that natural gradients of the partially optimized mean ﬁeld objective
with respect to the global variational parameters can be computed efﬁciently (without any backward
pass as would be required in generic reverse-mode differentiation). Indeed, the exponential family
conjugacy structure makes the natural gradient of the SVI objective even easier to compute than the
ﬂat gradient.

C.3 Stochastic natural gradients for large datasets

The real utility of natural gradient SVI is in its application to large datasets. Consider the model
N
composed of global latent variables θ, local latent variables x =
n=1,

N
n=1, and data y =

xn}
{

yn}

{

p(θ, x, y) = p(θ)

p(xn, yn |

θ),

N
(cid:89)

n=1

18

where each p(xn, yn |
ﬁxed observations y =

yn}
{

N
n=1, let

θ) is a copy of the same likelihood function with conjugate prior p(θ). For

q(θ, x) = q(θ)

q(xn)

N
(cid:89)

n=1

be a variational family to approximate the posterior p(θ, x
y) and consider the SVI objective given
by Eq. (13). Using Eq. (8) of Proposition B.4, it is straightforward to extend the natural gradient
expression in Corollary C.4 to an unbiased Monte Carlo estimate which samples terms in the sum
over data points.

|

Corollary C.5 (Unbiased Monte Carlo estimate of the SVI natural gradient)
Using the model and variational family

p(θ, x, y) = p(θ)

q(θ)q(x) = q(θ)

q(xn),

N
(cid:89)

n=1

p(xn, yn |

θ),

N
(cid:89)

n=1

where p(θ) and p(xn, yn |
Let the random index ˆn be sampled from the set
takes value n. Then

{

1, 2, . . . , N

}

θ) are a conjugate pair of exponential families, deﬁne

(ηθ) as in Eq. (13).
and let pn > 0 be the probability it

L

(ηθ) = Eˆn

(cid:101)
∇L

(cid:20)
η0
θ +

E

1
pˆn

q∗(x ˆn)[ (txy(xˆn, yˆn), 1) ]

(cid:21)

ηθ

,

−

where q∗(xˆn) is a local partial optimizer of

given q(θ).

L

Proof. Taking expectation over the index ˆn, we have

Eˆn

(cid:20) 1
pˆn

(cid:21)
q∗(x ˆn)[ (txy(xˆn, yˆn), 1) ]

E

=

E

pn
pn

q∗(xn)[ (txy(xn, yn), 1) ]

N
(cid:88)

n=1

N
(cid:88)

n=1

=

E

q∗(xn)[ (txy(xn, yn), 1) ] .

The remainder of the proof follows from Proposition B.4 and the same argument as in Proposition C.3.

The unbiased stochastic gradient developed in Corollary C.5 can be used in a scalable stochastic
gradient ascent algorithm. To simplify notation, in the following sections we drop the notation
θ) for n = 1, 2, . . . , N and return to working with a single
for multiple likelihood terms p(xn, yn |
likelihood term p(x, y

θ). The extension to multiple likelihood terms is immediate.

|

C.4 Conditinally conjugate models and block updating

The model classes often considered for natural gradient SVI, and the main model classes we consider
here, have additional conjugacy structure in the local latent variables. In this section we introduce
notation for this extra structure in terms of the additional local latent variables z and discuss the local
block coordinate optimization that is often performed to compute the factor q∗(z)q∗(x) for use in the
natural gradient expression.

Let p(z, x, y
conjugate prior, writing

|

θ) be an exponential family and p(θ) be its corresponding natural exponential family

p(z, x, y

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

θ )(cid:9) ,

η0
log Zθ(η0
θ , tθ(θ)
(cid:104)
(cid:105) −
η0
zxy(θ), tzxy(z, x, y)
(cid:104)
tθ(θ), (tzxy(z, x, y), 1)

(cid:105) −

zxy(θ))(cid:9)

log Zzxy(η0
,

{(cid:104)
where we have used tθ(θ) = (cid:0)η0
zxy(θ),
tzxy(z, x, y) be a multilinear polynomial in the statistics functions tx(x), ty(y), and tz(z), let p(z

(15)
zxy(θ))(cid:1) in Eq. (15). Additionally, let
θ),

log Zzxy(η0

(cid:105)}

−

(14)

|

19

p(x
|
to p(x

z, θ), and p(y

|
z, θ) and p(x

|

x, z, θ) = p(y

x, θ) be exponential families, and let p(z

θ) be a conjugate prior

z, θ) be a conjugate prior to p(y

x, θ), so that

|

|

|

p(z

p(x

p(y

θ) = exp (cid:8)
|
z, θ) = exp (cid:8)
= exp (cid:8)
x, θ) = exp (cid:8)
= exp (cid:8)

|

|

|

log Zz(η0

z (θ))(cid:9) ,

η0
z (θ), tz(z)
(cid:104)
(cid:105) −
η0
x(z, θ), tx(x)
(cid:104)
tz(z), η0
(cid:104)
η0
y(x, θ), ty(y)
(cid:104)
tx(x), η0
(cid:104)

(cid:105) −
x(θ)T(tx(x), 1)

(cid:105) −
y(θ)T(ty(y), 1)
(cid:105)

log Zx(η0
(cid:9) ,
(cid:105)
log Zy(η0
(cid:9) ,

x(z, θ))(cid:9)

y(x, z, θ))(cid:9)

(16)

(17)

(18)

for some matrices η0

x(θ) and η0

y(θ).

This model class includes many common models, including the latent Dirichlet allocation, switching
linear dynamical systems with linear-Gaussian emissions, and mixture models and hidden Markov
models with exponential family emissions. The conditionally conjugate structure is both powerful
and restrictive: while it potentially limits the expressiveness of the model class, it enables block
coordinate optimization with very simple and fast updates, as we show next. When conditionally
conjugate structure is not present, these local optimizations can instead be performed with generic
gradient-based methods and automatic differentiation [31].
Proposition C.6 (Unconstrained block coordinate ascent on q(z) and q(x))
Let p(θ, z, x, y) be a model as in Eqs. (14)-(18), and for ﬁxed data y let q(θ)q(z)q(x) be a corre-
sponding mean ﬁeld variational family for approximating the posterior p(θ, z, x

y), with

|

q(θ) = exp
q(z) = exp
q(x) = exp

ηθ, tθ(θ)
ηz, tz(z)
ηx, tx(x)

log Zθ(ηθ)
}
log Zz(ηz)
}
log Zx(ηx)
}

,
,
,

(cid:105) −
(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)
{(cid:104)

and with the mean ﬁeld variational inference objective

[ q(θ)q(z)q(x) ] = E

q(θ)q(z)q(x)

L

(cid:20)

log

p(θ)p(z

|

z, θ)p(y

θ)p(x
|
q(θ)q(z)q(x)

|

x, z, θ)

(cid:21)

.

Fixing the other factors, the partial optimizers q∗(z) and q∗(x) for
given by

L

over all possible densities are

q∗(z) (cid:44) arg max

[ q(θ)q(z)q(x) ] = exp

q∗(x) (cid:44) arg max

[ q(θ)q(z)q(x) ] = exp

q(z) L

q(x) L

η∗
z , tz(z)

log Zz(η∗
z )
}

,

(cid:105) −

η∗
x, tx(x)

log Zx(η∗
x)
}

,

(cid:105) −

{(cid:104)

{(cid:104)

with

z = E
η∗
x = E
η∗

z (θ) + E

q(θ)η0
q(θ)q(z)η0

q(θ)q(x)η0
x(θ)tz(z) + E

x(θ)T(tx(x), 1),
q(θ)η0

y(θ)T(ty(y), 1).

(19)

(20)

Proof. This proposition is a consequence of Lemma C.2 and the conjugacy structure.

log Zθ(ηθ),

log Zz(ηz), and

Proposition C.6 gives an efﬁcient block coordinate ascent algorithm: for ﬁxed ηθ, by alternatively
updating ηz and ηx according to Eqs. (19)-(20) we are guaranteed to converge to a stationary point
that is partially optimal in the parameters of each factor. In addition, performing each update requires
only computing expected sufﬁcient statistics in the variational factors, which means evaluating
log Zx(ηx), quantities that be computed anyway in a gradient-
∇
based optimization routine. The block coordinate ascent procedure leveraging this conditional
conjugacy structure is thus not only efﬁcient but also does not require a choice of step size.
Note in particular that this procedure produces parameters η∗
x(ηθ) that are partially optimal
(and hence stationary) for the objective. That is, deﬁning the parameterized mean ﬁeld variational
inference objective as L(ηθ, ηz, ηx) =
[ q(θ)q(z)q(x) ], for ﬁxed ηθ the block coordinate ascent
procedure has limit points η∗

z (ηθ) and η∗

L
x that satisfy

z and η∗

∇

∇

(ηθ, η∗

z (ηθ), η∗

x(ηθ)) = 0,

∇ηz L

(ηθ, η∗

z (ηθ), η∗

x(ηθ)) = 0.

∇ηxL

20

D The SVAE objective and its gradients

In this section we deﬁne the SVAE variational lower bound and show how to efﬁciently compute
unbiased stochastic estimates of its gradients, including an unbiased estimate of the natural gradient
with respect to the variational parameters with conjugacy structure. The setup here parallels the setup
for natural gradient SVI in Section C, but while SVI is restricted to complete-data conjugate models,
here we consider more general likelihood models.

D.1 SVAE objective

Let p(x
conjugate prior, as in Deﬁnitions B.1 and B.3, writing

|

θ) be an exponential family and let p(θ) be its corresponding natural exponential family

where we have used tθ(θ) = (cid:0)η0
x, γ) be a general family
of densities (not necessarily an exponential family) and let p(γ) be an exponential family prior on its
parameters of the form

x(θ),

−

|

p(x

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

η0
θ , tθ(θ)
(cid:105) −
(cid:104)
η0
x(θ), tx(x)
(cid:104)
(cid:105) −
tθ(θ), (tx(x), 1)

log Zθ(η0

θ )(cid:9) ,
log Zx(η0

{(cid:104)
log Zx(η0

,
(cid:105)}
x(θ))(cid:1) in Eq. (22). Let p(y

x(θ))(cid:9)

p(γ) = exp (cid:8)
(cid:104)

η0
γ, tγ(γ)

(cid:105) −

log Zγ(η0

γ)(cid:9) .

(21)

(22)

For ﬁxed y, consider the mean ﬁeld family of densities q(θ, γ, x) = q(θ)q(γ)q(x) and the mean ﬁeld
variational inference objective

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(23)

By the same argument as in Proposition C.1, without loss of generality we can take the global factor
q(θ) to be in the same exponential family as the prior p(θ), and we denote its natural parameters by
ηθ, writing

{(cid:104)

q(θ) = exp

ηθ, tθ(θ)
We restrict q(γ) to be in the same exponential family as p(γ) with natural parameters ηγ, writing
ηγ, tγ(γ)

log Zγ(ηγ)
}
Finally, we restrict3 q(x) to be in the same exponential family as p(x
θ), writing its natural parameter
as ηx. Using these explicit variational natural parameters, we rewrite the mean ﬁeld variational
inference objective in Eq. (23) as

log Zθ(ηθ)
}

q(γ) = exp

(cid:105) −

(cid:105) −

{(cid:104)

.

.

|

(ηθ, ηγ, ηx) (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(24)

To perform efﬁcient optimization in the objective
deﬁned in Eq. (24), we consider choosing the
variational parameter ηx as a function of the other parameters ηθ and ηγ. One natural choice is to set
, as in Section C. However, ﬁnding a local partial optimizer
ηx to be a local partial optimizer of
may be computationally expensive for general densities p(y
x, γ), and in the large data setting this
expensive optimization would have to be performed for each stochastic gradient update. Instead, we
choose ηx by optimizing over a surrogate objective (cid:98)
, which we design using exponential family
L
structure to be both easy to optimize and to share curvature properties with the mean ﬁeld objective

L

L

|

. The surrogate objective (cid:98)
L

L

is

(ηθ, ηγ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(γ)q(x)

(cid:20)

log

p(θ)p(γ)p(x

(cid:21)

|

ψ(x; y, φ)
θ) exp
}
{
q(θ)q(γ)q(x)
θ) exp
ψ(x; y, φ)
{
q(θ)q(x)

(cid:21)

}

+ const,

|

= E

q(θ)q(x)

(cid:20)

log

p(θ)p(x

(25)

3The parametric form for q(x) need not be restricted a priori, but rather without loss of generality given the
surrogate objective Eq. (25) and the form of ψ used in Eq. (26), the optimal factor q(x) is in the same family as
p(x | θ). We treat it as a restriction here so that we can proceed with more concrete notation.

21

|

where the constant does not depend on ηx. We deﬁne the function ψ(x; y, φ) to have a form related
to the exponential family p(x

θ),

r(y; φ)
{

ψ(x; y, φ) (cid:44)
}φ∈Rm is some class of functions parameterized by φ

r(y; φ), tx(x)
(cid:105)
(cid:104)

where
to be continuously differentiable in φ. We call r(y; φ) the recognition model. We deﬁne η∗
,
be a local partial optimizer of (cid:98)
L
x(ηθ, φ) (cid:44) arg min
η∗

(26)
Rm, which we assume only
x(ηθ, φ) to

∈

,

(ηθ, ηγ, ηx, φ),
(cid:98)
L

ηx

where the notation above should be interpreted as choosing η∗
x(ηθ, φ) to be a local argument of
maximum. The results to follow rely only on necessary ﬁrst-order conditions for unconstrained local
optimality.
Given this choice of function η∗

x(ηθ, φ), we deﬁne the SVAE objective to be
LSVAE(ηθ, ηγ, φ) (cid:44)

(ηθ, ηγ, η∗

x(ηθ, φ)),

L

(27)

where
tion problem to be

L

is the mean ﬁeld variational inference deﬁned in Eq. (24), and we deﬁne the SVAE optimiza-

maxηθ,ηγ ,φLSVAE(ηθ, ηγ, φ).

We summarize these deﬁnitions in the following.
Deﬁnition D.1 (SVAE objective)
Let

denote the mean ﬁeld variational inference objective

L

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

,

(28)

where the densities p(θ), p(γ), and p(x
nential family conjugate prior to p(x
variational factors as

|

θ) are exponential families and p(θ) is the natural expo-
|
θ), as in Eqs. (21)-(22). Given a parameterization of the

q(θ) = exp

ηθ, tθ(θ)

{(cid:104)

log Zθ(ηθ)
}

,
ηx, tx(x)

q(x) = exp

(cid:105) −

q(γ) = exp

{(cid:104)

ηγ, tγ(γ)
,

log Zx(ηx)

{(cid:104)

(cid:105) −

}

log Zγ(ηγ)
}

,

(cid:105) −

(ηθ, ηγ, ηx) denote the mean ﬁeld variational inference objective Eq. (28) as a function of these

let
variational parameters. We deﬁne the SVAE objective as

L

LSVAE(ηθ, ηγ, φ) (cid:44)

L

(ηθ, ηγ, η∗

x(ηθ, φ)),

where η∗

,
x(ηθ, φ) is deﬁned as a local partial optimizer of the surrogate objective (cid:98)
L

x(ηθ, φ) (cid:44) arg max
η∗

(ηθ, η∗
(cid:98)
L

x(ηθ, φ), φ),

ηx

where the surrogate objective (cid:98)
L

is deﬁned as
(cid:20)

(ηθ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(x)

log

ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:104)

,
(cid:105)

p(θ)p(x

|

ψ(x; y, φ)
θ) exp
}
{
q(θ)q(x)

(cid:21)

,

for some recognition model r(y; φ) parameterized by φ

Rm.

∈

LSVAE is a lower-bound for the partially-optimized mean ﬁeld variational

The SVAE objective
inference objective in the following sense.
Proposition D.2 (The SVAE objective lower-bounds the mean ﬁeld objective)
The SVAE objective function
the sense that

LSVAE lower-bounds the partially-optimized mean ﬁeld objective

in

L

max
q(x) L

[ q(θ)q(γ)q(x) ]

max

ηx L

≥

(ηθ, ηγ, ηx)

≥ LSVAE(ηθ, ηγ, φ)

φ

∀

∈

Rm,

22

for any choice of function class
such that

{

r(y; φ)

}φ∈Rm in Eq. (26). Furthermore, if there is some φ∗

∈

Rm

then the bound can be made tight in the sense that

ψ(x; y, φ∗) = E

q(γ) log p(y

x, γ)

|

max
q(x) L

[ q(θ)q(γ)q(x) ] = max

(ηθ, ηγ, ηx) = max

ηx L

φ LSVAE(ηθ, ηγ, φ).

Proof. The inequalities follow from the variational principle and the deﬁnition of the SVAE objective
LSVAE. In particular, by Lemma C.2 the optimal factor over all possible densities is given by
x(θ), tx(x)
(cid:105)

q(γ) log p(y

x, γ)(cid:9) ,

exp (cid:8)

q∗∗(x)

q(θ)η0

E
(cid:104)

+ E

(29)

∝

|

exp

while we restrict the factor q(x) to have a particular exponential family form indexed by parameter ηx,
namely q(x)
LSVAE we also restrict the parameter ηx to be
set to η∗
x(ηθ, φ), a particular function of ηθ and φ, rather than setting it to the value that maximizes
the mean ﬁeld objective
. Finally, equality holds when we can set φ to match the optimal ηx and
L
that choice yields the optimal factor given in Eq. (29).

. In the deﬁnition of

ηx, tx(x)

{(cid:104)

(cid:105)}

∝

Proposition D.2 motivates the SVAE optimization problem: by using gradient-based optimization to
LSVAE(ηθ, ηγ, φ) we are maximizing a lower-bound on the model evidence log p(y) and
maximize
correspondingly minimizing the KL divergence from our variational family to the target posterior.
Furthermore, it motivates choosing the recognition model function class
}φ∈Rm to be as rich
as possible.
As we show in the following, choosing η∗
objective (cid:98)
L
simple expression for an unbiased estimate of the natural gradient (cid:101)
Section D.2. Second, it allows η∗
structure, as we show in Section D.4.

x(ηθ, φ) to be a local partial optimizer of the surrogate
provides two signiﬁcant computational advantages. First, it allows us to provide a
∇ηθ LSVAE, as we describe next in
x(ηθ, φ) to be computed efﬁciently by exploiting exponential family

r(y; φ)
{

D.2 Estimating the natural gradient (cid:101)

∇ηθ LSVAE

x in terms of the surrogate objective (cid:98)
L

The deﬁnition of η∗
enables computationally efﬁcient
ways to estimate natural gradient with respect to the conjugate global variational parameters,
∇ηθ LSVAE(ηθ, ηγ, φ). The next proposition covers the case when the local latent variational factor
(cid:101)
q(x) has no additional factorization structure.
Proposition D.3 (Natural gradient of the SVAE objective)
When there is only one local latent variational factor q(x) (and no further factorization structure),
the natural gradient of the SVAE objective Eq. (27) with respect to the conjugate global variational
parameters ηθ is

(cid:101)

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

θ + E
∇ηxL
where the ﬁrst term is the SVI natural gradient from Corollary C.4, using

q∗(x) [(tx(x), 1)]

(cid:1) + (

ηθ

−

(ηθ, ηγ, η∗

x(ηθ, φ)), 0)

q∗(x) (cid:44) exp

η∗
x(ηθ, φ), tx(x)

log Zx(η∗

x(ηθ, φ))

{(cid:104)

(cid:105) −

,

}

and where a stochastic estimate of the second term is computed as part of the backward pass for the
gradient

(ηθ, ηγ, η∗

x(ηθ, φ)).

∇φL

Proof. First we use the chain rule, analogously to Eq. (5), to write the gradient as

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)

2 log Zθ(ηθ)(cid:1) (cid:0)η0
∇
∇ηθ η∗
x(ηθ, φ)) (
+ (
where the ﬁrst term is the same as the SVI gradient derived in Proposition C.3. In the case of SVI,
the second term is zero because η∗
, but for the SVAE objective
x is chosen as a partial optimizer of
the second term is nonzero in general, and the remainder of this proof amounts to deriving a simple
expression for it.

q∗(x) [ (txy(x, y), 1) ]
(ηθ, ηγ, η∗
x(ηθ, φ))) ,

θ + E

∇ηx L

(30)

ηθ

−

L

(cid:1)

23

We compute the term
using the Implicit Function Theorem given in Corollary A.5, which yields

x(ηθ, φ) in Eq. (30) in terms of the gradients of the surrogate objective (cid:98)
L

∇ηθ η∗

(ηθ, η∗

x(ηθ, φ), φ)

(cid:17)−1

.

(31)

∇ηθ η∗

x(ηθ, φ) =

−∇

First, we compute the gradient of (cid:98)
L
E

(ηθ, ηx, φ) =

∇ηx (cid:98)
L

(cid:16)

(ηθ, η∗

x(ηθ, φ), φ)

2
ηθηx (cid:98)
L
with respect to ηx, writing
p(x

∇

(cid:20)

(cid:20)

2
ηxηx (cid:98)
L

q(θ)q(x)

log

∇ηx

=
= (cid:0)

∇ηx
∇

(cid:2)
E
q(θ)η0
(cid:104)
2 log Zx(ηx)(cid:1) (cid:0)E

x(θ) + r(y; φ)

q(θ)η0

(cid:21)(cid:21)

|

)
}

ψ(x; y, φ)
θ) exp
{
q(x)
ηx,
x(θ) + r(y; φ)

∇

−

log Zx(ηx)
(cid:105)
(cid:1) ,
ηx

−

+ log Zx(ηx)(cid:3)

(32)

When there is only one local latent variational factor q(x) (and no further factorization structure), as
x(ηθ, φ), φ) = 0 and the fact that
a consequence of the ﬁrst-order stationary condition

∇ηx (cid:98)
L
η∗
x(ηθ, φ) = 0,
which is useful in simplifying the expressions to follow.

2 log Zx(ηx) is always positive deﬁnite for minimal exponential families, we have
x(θ) + r(y; φ)

(ηθ, η∗

q(θ)η0

(33)

∇

−

E

Continuing with the calculation of the terms in Eq. (31), we compute
expression in Eq. (32) again, writing

2
ηxηx (cid:98)
L

∇

by differentiating the

2
ηxηx (cid:98)
L

∇

(ηθ, η∗

x(ηθ, φ), φ) =

2 log Zx(η∗
3 log Zx(η∗
2 log Zx(η∗

x(ηθ, φ))
x(ηθ, φ))(cid:1)(cid:0)E
x(ηθ, φ)),

−∇
+ (cid:0)
=

∇

−∇

q(θ)η0

x(θ) + r(y; φ)

x(ηθ, φ)(cid:1)
η∗

−

(34)

where the last line follows from using the ﬁrst-order stationary condition Eq. (33). Next, we compute
by differentiating Eq. (32) with respect to ηθ to yield
the other term
2 log Zx(η∗

(cid:18)

(cid:19)

x(ηθ, φ))

∇

where the latter matrix is

0
x(ηθ, φ)) padded by a row of zeros.
Plugging these expressions back into Eq. (31) and cancelling, we arrive at

∇

∇

x(ηθ, φ), φ) = (cid:0)
2 log Zx(η∗

2 log Zθ(ηθ)(cid:1)

∇

2
ηθηx (cid:98)
L
∇
(ηθ, η∗
2
ηθηx (cid:98)
L

,

∇ηθ η∗
and so we have an expression for the gradient of the SVAE objective as

2 log Zθ(ηθ)

x(ηθ, φ) =

∇

,

(cid:19)

(cid:18)I
0

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)
∇
+ (cid:0)

2 log Zθ(ηθ)(cid:1) (cid:0)η0
2 log Zθ(ηθ)(cid:1) (
When we compute the natural gradient, the Fisher information matrix factors on the left of each term
cancel, yielding the result in the proposition.

q∗(x) [ (txy(x, y), 1) ]
(ηθ, ηγ, η∗

−
x(ηθ, φ)), 0) .

θ + E

∇ηx L

ηθ

∇

(cid:1)

The proof of Proposition D.3 uses the necessary condition for unconstrained local optimality to
simplify the expression in Eq. (34). This simpliﬁcation does not necessarily hold if ηx is constrained;
for example, if the factor q(x) has additional factorization structure, then there are additional (linear)
coordinate subspace constraints on ηx. Note also that when q(x) is a Gaussian family with ﬁxed
covariance (that is, with sufﬁcient statistics tx(x) = x) the same simpliﬁcation always applies
3 log Zx(ηx) = 0.
because third and higher-order cumulants are zero for such families and hence
More generally, when the local latent variables have additional factorization structure, as in the
Gaussian mixture model (GMM) and switching linear dynamical system (SLDS) examples, the
natural gradient with respect to ηθ can be estimated efﬁciently by writing Eq. (30) as
2 log Zθ(ηθ)(cid:1) (cid:0)η0
(cid:1)
[η(cid:48)

θ + E
x(η(cid:48)
(ηθ, ηγ, η∗
where we can recover the second term in Eq. (30) by using the chain rule. We can estimate this second
term directly using the reparameterization trick. Note that to compute the natural gradient estimate in
2 log Zθ(ηθ))−1 to this term because the convenient cancellation from
this case, we need to apply (
Proposition D.3 does not apply. When ηθ is of small dimension compared to ηγ, φ, and even ηx, this
additional computational cost is not large.

q∗(x) [ (txy(x, y), 1) ]
θ, φ))] ,

∇ηθ LSVAE = (cid:0)

θ (cid:55)→ L

ηθ

∇

∇

∇

∇

−

+

24

D.3 Estimating the gradients

∇φLSVAE and

∇ηγ LSVAE

∇φLSVAE(ηθ, ηγ, φ) and
To compute an unbiased stochastic estimate of
∇ηγ LSVAE(ηθ, ηγ, φ) we use the reparameterization trick [7], which is simply to differentiate a
LSVAE(ηθ, ηγ, φ) as a function of φ and ηγ. To isolate the terms
stochastic estimate of the objective
that require this sample-based approximation from those that can be computed directly, we rewrite
the objective as

the gradients

LSVAE(ηθ, ηγ, φ) = E

q(γ)q∗(x) log p(y

x, γ)

|

−

KL(q(θ)q(γ)q∗(x)

p(θ, γ, x))

(35)

where, as before,

q∗(x) (cid:44) exp

η∗
x(ηθ, φ), tx(x)

log Zx(η∗

x(ηθ, φ))

{(cid:104)
and so the dependence of the expression in Eq. (35) on φ is through η∗
Eq. (35) needs to be estimated with the reparameterization trick.

(cid:105) −

x(ηθ, φ). Only the ﬁrst term in

We summarize this procedure in the following proposition.
Proposition D.4 (Estimating
∇ηγ LSVAE)
Let ˆγ(ηγ)
of the gradients

∇φLSVAE and
q∗(x) be samples of q(γ) and q∗(x), respectively. Unbiased estimates
∼
∇φLSVAE(ηθ, ηγ, φ) and

∇ηγ LSVAE(ηθ, ηγ, φ) are given by

q(γ) and ˆx(φ)

∼

∇φLSVAE(ηθ, ηγ, φ)
∇ηγ LSVAE(ηθ, ηγ, φ)

≈ ∇φ log p(y
|
≈ ∇ηγ log p(y

|

ˆx(φ), ˆγ(ηγ))
ˆx(φ), ˆγ(ηγ))

− ∇φ KL(q(θ)q∗(x)
− ∇ηγ KL(q(γ)
(cid:107)

(cid:107)
p(γ)).

p(θ, x)),

Both of these gradients can be computed by automatically differentiating the Monte Carlo estimate of
LSVAE given by

LSVAE(ηθ, ηγ, φ)
with respect to ηγ and φ, respectively.

log p(y

≈

|

ˆx(φ), ˆγ(ηγ))

KL(q(θ)q(γ)q∗(x)

p(θ, γ, x))

−

(cid:107)

(cid:107)

}

D.4 Partially optimizing (cid:98)
L

using conjugacy structure

In Section D.1 we deﬁned the SVAE objective in terms of a function η∗
implicitly deﬁned in terms of ﬁrst-order stationary conditions for an auxiliary objective (cid:98)
L
Here we show how (cid:98)
L
conjugate model of Section C.4.

x(ηθ, φ), which was itself
(ηθ, ηx, φ).
admits efﬁcient local partial optimization in the same way as the conditionally

In this section we consider additional structure in the local latent variables. Speciﬁcally, as in
Section C.4, we introduce to the notation another set of local latent variables z in addition to the
local latent variables x. However, unlike Section C.4, we still consider general likelihood families
p(y

x, γ).

θ) be an exponential family and p(θ) be its corresponding natural exponential family

|
Let p(z, x
|
conjugate prior, writing

p(z, x

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

log Zθ(η0

(cid:105) −

η0
θ , tθ(θ)
(cid:104)
η0
zx(θ), tzx(z, x)
(cid:104)
(cid:105) −
tθ(θ), (tzx(z, x), 1)

θ )(cid:9) ,
log Zzx(η0

zx(θ))(cid:9)

{(cid:104)

(cid:105)}

(36)

where we have used tθ(θ) = (cid:0)η0
zx(θ),
a multilinear polynomial in the statistics tz(z) and tx(x), and let p(z
pair of exponential families, writing

log Zzx(η0

zx(θ))(cid:1) in Eq. (15). Additionally, let tzx(z, x) be
z, θ) be a conjugate
θ) and p(x

−

|

p(z

p(x

|

θ) = exp (cid:8)
η0
z (θ), tz(z)
(cid:104)
|
(cid:105) −
z, θ) = exp (cid:8)
η0
x(z, θ), tx(x)
(cid:104)
= exp (cid:8)
tz(z), η0
(cid:104)

(cid:105) −
x(θ)T(tx(x), 1)

log Zx(η0
(cid:9) .
(cid:105)

log Zz(η0

|
z (θ))(cid:9) ,

x(z, θ))(cid:9)

Let p(y
an exponential family prior on its parameters of the form

x, γ) be a general family of densities (not necessarily an exponential family) and let p(γ) be

|

p(γ) = exp (cid:8)
(cid:104)

η0
γ, tγ(γ)

(cid:105) −

log Zγ(η0

γ)(cid:9) .

25

The corresponding variational factors are

q(θ) = exp
q(z) = exp

ηθ, tθ(θ)
ηz, tz(z)

log Zθ(ηθ)
}
log Zz(ηz)
}

,
,

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

q(γ) = exp
q(x) = exp

ηγ, tγ(γ)
ηx, tx(x)

log Zγ(ηγ)
}
log Zx(ηx)
}

,
.

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

As in Section D.1, we construct the surrogate objective (cid:98)
L
and conjugacy structure. In particular, we construct (cid:98)
L
p(θ)p(γ)p(z

to allow us to exploit exponential family
to resemble the mean ﬁeld objective, namely
z, θ)p(y

x, γ)

(cid:21)

(cid:20)

(ηθ, ηγ, ηz, ηx) (cid:44) E

q(θ)q(γ)q(z)q(x)

log

,

θ)p(x
q(θ)q(γ)q(z)q(x)

|

|

L

but in (cid:98)
L
without much structure, with a more tractable approximation,

we replace the log p(y

|

x, γ) likelihood term, which may be a general family of densities

(ηθ, ηz, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(z)q(x)

(cid:20)

log

p(θ)p(z

|

θ)p(x

z, θ) exp
{
q(θ)q(z)q(x)

|

ψ(x; y, φ)
}

(cid:21)

,

where ψ(x; y, φ) is a function on x that resembles a conjugate likelihood for p(x

z, θ),

|

|

ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:105)

,

(cid:104)

Rm.

φ

∈

We then deﬁne η∗
given ﬁxed values of the
other parameters ηθ and φ, and in particular they satisfy the ﬁrst-order necessary optimality conditions

x(ηθ, φ) to be local partial optimizers of (cid:98)
L

z (ηθ, φ) and η∗

∇ηz (cid:98)
L

The SVAE objective is then

(ηθ, η∗

z (ηθ, φ), η∗

x(ηθ, φ), φ) = 0,

(ηθ, η∗

z (ηθ, φ), η∗

x(ηθ, φ), φ) = 0.

∇ηx (cid:98)
L

L

x(ηθ, φ)).

(ηθ, ηγ, η∗

z (ηθ, φ), η∗

LSVAE(ηθ, ηγ, φ) (cid:44)
The structure of the surrogate objective (cid:98)
is chosen so that it resembles the mean ﬁeld variational
L
inference objective for the conditionally conjugate model of Section C.4, and as a result we can
use the same block coordinate ascent algorithm to efﬁciently ﬁnd partial optimzers η∗
z (ηθ, φ) and
η∗
x(ηθ, φ).
Proposition D.5 (Computing η∗
Let the densities p(θ, γ, z, x, y) and q(θ)q(γ)q(z)q(x) and the objectives
Eqs. (36)-(37). The partial optimizers η∗
(cid:44) arg max
ηz

x, deﬁned by
η∗
x

(ηθ, ηz, ηx, φ),
(cid:98)
L

(ηθ, ηz, ηx, φ)
(cid:98)
L

LSVAE be as in

(cid:44) arg max
ηx

z (ηθ, φ) and η∗

z and η∗

x(ηθ, φ))

, (cid:98)
L

, and

(37)

η∗
z

L

q(θ)η0

z = E
η∗

with the other arguments ﬁxed, are are given by
z (θ) + E

q(θ)q(z)η0
and by alternating the expressions in Eq. (38) as updates we can compute η∗
.
local partial optimizers of (cid:98)
L

x(θ)T(tx(x), 1),

q(θ)q(x)η0

x = E
η∗

x(z, θ) + r(y; φ),
z (ηθ, φ) and η∗

(38)
x(ηθ, φ) as

Proof. These updates follow immediately from Lemma C.2. Note in particular that the stationary
conditions

= 0 yield the each expression in Eq. (38), respectively.

= 0 and

∇ηz (cid:98)
L

∇ηx (cid:98)
L

The other properties developed in Propositions D.2, D.3, and D.4 also hold true for this model because
it is a special case in which we have separated out the local variables, denoted x in earlier sections,
into two groups, denoted z and x here, to match the exponential family structure in p(z
θ) and
p(x
z, θ), and performed unconstrained optimization in each of the variational parameters. However,
the expression for the natural gradient is slightly simpler for this model than the corresponding
version of Proposition D.3.

|

|

E Experiment details and expanded ﬁgures

For the synthetic 1D dot video data, we trained an LDS SVAE on 80 random image sequences each
of length 50, using one sequence per update, and show the model’s future predictions given a preﬁx
of a longer sequence. We used MLP image and recognition models each with one hidden layer of 50
units and a latent state space of dimension 8.

26

(a) Beginning a rear

(b) Grooming

(c) Extension into running

27

(d) Fall from rear

Figure 8: Examples of behavior states inferred from depth video. For each state, four example frame
sequences are shown, including frames during which the given state was most probable according to
the variational distribution on the hidden state sequence. Each frame sequence is padded on both
sides, with a square in the lower-right of a frame depicting that the state was active in that frame. The
frame sequences are temporally subsampled to reduce their length, showing one of every four video
frames. Examples were chosen to have durations close to the median duration for that state.

7
1
0
2
 
l
u
J
 
7
 
 
]
L
M

.
t
a
t
s
[
 
 
5
v
7
7
2
6
0
.
3
0
6
1
:
v
i
X
r
a

Composing graphical models with neural networks
for structured representations and fast inference

Matthew James Johnson
Harvard University
mattjj@seas.harvard.edu

David Duvenaud
Harvard University
dduvenaud@seas.harvard.edu

Alexander B. Wiltschko
Harvard University, Twitter
awiltsch@fas.harvard.edu

Sandeep R. Datta
Harvard Medical School
srdatta@hms.harvard.edu

Ryan P. Adams
Harvard University, Twitter
rpa@seas.harvard.edu

Abstract

We propose a general modeling and inference framework that combines the com-
plementary strengths of probabilistic graphical models and deep learning methods.
Our model family composes latent graphical models with neural network obser-
vation likelihoods. For inference, we use recognition networks to produce local
evidence potentials, then combine them with the model distribution using efﬁcient
message-passing algorithms. All components are trained simultaneously with a
single stochastic variational inference objective. We illustrate this framework by
automatically segmenting and categorizing mouse behavior from raw depth video,
and demonstrate several other example models.

1

Introduction

Modeling often has two goals: ﬁrst, to learn a ﬂexible representation of complex high-dimensional
data, such as images or speech recordings, and second, to ﬁnd structure that is interpretable and
generalizes to new tasks. Probabilistic graphical models [1, 2] provide many tools to build structured
representations, but often make rigid assumptions and may require signiﬁcant feature engineering.
Alternatively, deep learning methods allow ﬂexible data representations to be learned automatically,
but may not directly encode interpretable or tractable probabilistic structure. Here we develop a
general modeling and inference framework that combines these complementary strengths.

Consider learning a generative model for video of a mouse. Learning interpretable representations for
such data, and comparing them as the animal’s genes are edited or its brain chemistry altered, gives
useful behavioral phenotyping tools for neuroscience and for high-throughput drug discovery [3].
Even though each image is encoded by hundreds of pixels, the data lie near a low-dimensional
nonlinear manifold. A useful generative model must not only learn this manifold but also provide
an interpretable representation of the mouse’s behavioral dynamics. A natural representation from
ethology [3] is that the mouse’s behavior is divided into brief, reused actions, such as darts, rears,
and grooming bouts. Therefore an appropriate model might switch between discrete states, with
each state representing the dynamics of a particular action. These two learning tasks — identifying
an image manifold and a structured dynamics model — are complementary: we want to learn the
image manifold in terms of coordinates in which the structured dynamics ﬁt well. A similar challenge
arises in speech [4], where high-dimensional spectrographic data lie near a low-dimensional manifold
because they are generated by a physical system with relatively few degrees of freedom [5] but also
include the discrete latent dynamical structure of phonemes, words, and grammar [6].

To address these challenges, we propose a new framework to design and learn models that couple
nonlinear likelihoods with structured latent variable representations. Our approach uses graphical
models for representing structured probability distributions while enabling fast exact inference
subroutines, and uses ideas from variational autoencoders [7, 8] for learning not only the nonlinear

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

(a) Data

(b) GMM

(c) Density net (VAE)

(d) GMM SVAE

Figure 1: Comparison of generative models ﬁt to spiral cluster data. See Section 2.1.

feature manifold but also bottom-up recognition networks to improve inference. Thus our method
enables the combination of ﬂexible deep learning feature models with structured Bayesian (and
even nonparametric [9]) priors. Our approach yields a single variational inference objective in
which all components of the model are learned simultaneously. Furthermore, we develop a scalable
ﬁtting algorithm that combines several advances in efﬁcient inference, including stochastic variational
inference [10], graphical model message passing [1], and backpropagation with the reparameterization
trick [7]. Thus our algorithm can leverage conjugate exponential family structure where it exists to
efﬁciently compute natural gradients with respect to some variational parameters, enabling effective
second-order optimization [11], while using backpropagation to compute gradients with respect to all
other parameters. We refer to our general approach as the structured variational autoencoder (SVAE).

2 Latent graphical models with neural net observations

In this paper we propose a broad family of models. Here we develop three speciﬁc examples.

2.1 Warped mixtures for arbitrary cluster shapes

One particularly natural structure used frequently in graphical models is the discrete mixture model.
By ﬁtting a discrete mixture model to data, we can discover natural clusters or units. These discrete
structures are difﬁcult to represent directly in neural network models.

Consider the problem of modeling the data y =
ﬁnding the clusters in data is to ﬁt a Gaussian mixture model (GMM) with a conjugate prior:
zn |

yn}
{
π iid
∼
However, the ﬁt GMM does not represent the natural clustering of the data (Fig. 1b). Its inﬂexible
Gaussian observation model limits its ability to parsimoniously ﬁt the data and their natural semantics.

N
n=1 shown in Fig. 1a. A standard approach to

(µk, Σk) iid
∼

(µk, Σk)
}
{

(µzn, Σzn ).

iid
∼ N

NIW(λ),

Dir(α),

yn |

K
k=1

zn,

∼

π

π

Instead of using a GMM, a more ﬂexible alternative would be a neural network density model:

γ

∼

xn

p(γ)

(1)
yn |
where µ(xn; γ) and Σ(xn; γ) depend on xn through some smooth parametric function, such as
multilayer perceptron (MLP), and where p(γ) is a Gaussian prior [12]. This model ﬁts the data
density well (Fig. 1c) but does not explicitly represent discrete mixture components, which might
provide insights into the data or natural units for generalization. See Fig. 2a for a graphical model.

(µ(xn; γ), Σ(xn; γ)),

(0, I),

∼ N

xn, γ iid

iid
∼ N

By composing a latent GMM with nonlinear observations, we can combine the modeling strengths of
both [13], learning both discrete clusters along with non-Gaussian cluster shapes:

π

Dir(α),

∼
xn

iid
∼ N

(µk, Σk) iid
∼

(µ(zn), Σ(zn)),

NIW(λ),

γ

p(γ)

yn |

xn, γ iid

∼ N

∼
(µ(xn; γ), Σ(xn; γ)).

zn |

π iid
∼

π

This combination of ﬂexibility and structure is shown in Fig. 1d. See Fig. 2b for a graphical model.

2.2 Latent linear dynamical systems for modeling video

Now we consider a harder problem: generatively modeling video. Since a video is a sequence of
image frames, a natural place to start is with a model for images. Kingma et al. [7] shows that the

2

(a) Latent Gaussian (b) Latent GMM

(c) Latent LDS

(d) Latent SLDS

Figure 2: Generative graphical models discussed in Section 2.

density network of Eq. (1) can accurately represent a dataset of high-dimensional images
terms of the low-dimensional latent variables

N
n=1 in
N
n=1, each with independent Gaussian distributions.
xn}
To extend this image model into a model for videos, we can introduce dependence through time
N
between the latent Gaussian samples
n=1. For instance, we can make each latent variable xn
depend on the previous latent variable xn−1 through a Gaussian linear dynamical system, writing

xn}
{

yn}
{

{

xn = Axn−1 + Bun,

un

iid
∼ N

(0, I),

A, B

Rm×m,

∈

where the matrices A and B have a conjugate prior. This model has low-dimensional latent states and
dynamics as well as a rich nonlinear generative model of images. In addition, the timescales of the
dynamics are represented directly in the eigenvalue spectrum of A, providing both interpretability
and a natural way to encode prior information. See Fig. 2c for a graphical model.

2.3 Latent switching linear dynamical systems for parsing behavior from video

As a ﬁnal example that combines both time series structure and discrete latent units, consider again
the behavioral phenotyping problem described in Section 1. Drawing on graphical modeling tools,
we can construct a latent switching linear dynamical system (SLDS) [14] to represent the data in
terms of continuous latent states that evolve according to a discrete library of linear dynamics, and
drawing on deep learning methods we can generate video frames with a neural network image model.

}

∈ {

1, 2, . . . , N

there is a discrete-valued latent state zn ∈ {

At each time n
that evolves
according to Markovian dynamics. The discrete state indexes a set of linear dynamical parameters,
Rm evolves according to the corresponding dynamics,
and the continuous-valued latent state xn ∈
zn−1, π
k=1 denotes the Markov transition matrix and πk ∈

where π =
+ is its kth row. We use the
same neural net observation model as in Section 2.2. This SLDS model combines both continuous
and discrete latent variables with rich nonlinear observations. See Fig. 2d for a graphical model.

xn = Azn xn−1 + Bzn un,

zn |
πk}
{

un
RK

iid
∼ N

1, 2, . . . , K

πzn−1,

(0, I),

∼

}

K

3 Structured mean ﬁeld inference and recognition networks

Why aren’t such rich hybrid models used more frequently? The main difﬁculty with combining rich
latent variable structure and ﬂexible likelihoods is inference. The most efﬁcient inference algorithms
used in graphical models, like structured mean ﬁeld and message passing, depend on conjugate
exponential family likelihoods to preserve tractable structure. When the observations are more
general, like neural network models, inference must either fall back to general algorithms that do not
exploit the model structure or else rely on bespoke algorithms developed for one model at a time.

In this section, we review inference ideas from conjugate exponential family probabilistic graphical
models and variational autoencoders, which we combine and generalize in the next section.

3.1

Inference in graphical models with conjugacy structure

Graphical models and exponential families provide many algorithmic tools for efﬁcient inference [15].
Given an exponential family latent variable model, when the observation model is a conjugate
exponential family, the conditional distributions stay in the same exponential families as in the prior
and hence allow for the same efﬁcient inference algorithms.

3

(a) VAE

(b) GMM SVAE

(c) LDS SVAE

(d) SLDS SVAE

Figure 3: Variational families and recognition networks for the VAE [7] and three SVAE examples.

For example, consider learning a Gaussian linear dynamical system model with linear Gaussian
N
observations. The generative model for latent states x =
n=1 is

N
n=1 and observations y =

xn}
{

xn = Axn−1 + Bun,

(0, I),

yn = Cxn + Dvn,

un

iid
∼ N

given parameters θ = (A, B, C, D) with a conjugate prior p(θ). To approximate the poste-
rior p(θ, x

y), consider the mean ﬁeld family q(θ)q(x) and the variational inference objective

yn}
{
(0, I),

vn

iid
∼ N

|

[ q(θ)q(x) ] = E

q(θ)q(x)

L

(cid:20)

log

p(θ)p(x

θ)p(y
q(θ)q(x)

|

|

(cid:21)

x, θ)

,

(2)

y) by
where we can optimize the variational family q(θ)q(x) to approximate the posterior p(θ, x
x, θ) is conjugate to the latent variable
maximizing Eq. (2). Because the observation model p(y
model p(x
[ q(θ)q(x) ] is itself a
Gaussian linear dynamical system with parameters that are simple functions of the expected statistics
of q(θ) and the data y. As a result, for ﬁxed q(θ) we can easily compute q∗(x) and use message
passing algorithms to perform exact inference in it. However, when the observation model is not
conjugate to the latent variable model, these algorithmically exploitable structures break down.

θ), for any ﬁxed q(θ) the optimal factor q∗(x) (cid:44) arg maxq(x) L

|

|

|

3.2 Recognition networks in variational autoencoders

The variational autoencoder (VAE) [7] handles general non-conjugate observation models by intro-
ducing recognition networks. For example, when a Gaussian latent variable model p(x) is paired with
a general nonlinear observation model p(y
y, γ) is non-Gaussian, and it is
difﬁcult to compute an optimal Gaussian approximation. The VAE instead learns to directly output a
suboptimal Gaussian factor q(x
y) by ﬁtting a parametric map from data y to a mean and covariance,
µ(y; φ) and Σ(y; φ), such as an MLP with parameters φ. By optimizing over φ, the VAE effectively
learns how to condition on non-conjugate observations y and produce a good approximating factor.

x, γ), the posterior p(x

|

|

|

4 Structured variational autoencoders

We can combine the tractability of conjugate graphical model inference with the ﬂexibility of
variational autoencoders. The main idea is to use a conditional random ﬁeld (CRF) variational family.
We learn recognition networks that output conjugate graphical model potentials instead of outputting
the complete variational distribution’s parameters directly. These potentials are then used in graphical
model inference algorithms in place of the non-conjugate observation likelihoods.

The SVAE algorithm computes stochastic gradients of a mean ﬁeld variational inference objective.
It can be viewed as a generalization both of the natural gradient SVI algorithm for conditionally
conjugate models [10] and of the AEVB algorithm for variational autoencoders [7]. Intuitively,
it proceeds by sampling a data minibatch, applying the recognition model to compute graphical
model potentials, and using graphical model inference algorithms to compute the variational factor,
combining the evidence from the potentials with the prior structure in the model. This variational
factor is then used to compute gradients of the mean ﬁeld objective. See Fig. 3 for graphical models
of the variational families with recognition networks for the models developed in Section 2.

In this section, we outline the SVAE model class more formally, write the mean ﬁeld variational
inference objective, and show how to efﬁciently compute unbiased stochastic estimates of its gradients.
The resulting algorithm for computing gradients of the mean ﬁeld objective, shown in Algorithm 1, is

4

Algorithm 1 Estimate SVAE lower bound and its gradients

Input: Variational parameters (ηθ, ηγ, φ), data sample y

function SVAEGRADIENTS(ηθ, ηγ, φ, y)

←

←

r(yn; φ)

ψ
(ˆx, ¯tx, KLlocal)
q(γ)
ˆγ
∼
N log p(y
L ←
η0
(cid:101)
∇ηθ L ←
θ −
return lower bound

PGMINFERENCE(ηθ, ψ)

−

ˆx, ˆγ)

N KLlocal
KL(q(θ)q(γ)
−
|
ηθ + N (¯tx, 1) + N (
∇ηx log p(y
∇ηθ L

, natural gradient (cid:101)

L

|

ˆx, ˆγ), 0)
, gradients

p(θ)p(γ))
(cid:107)

∇ηγ ,φL

(cid:46) Get evidence potentials
(cid:46) Combine evidence with prior
(cid:46) Sample observation parameters
(cid:46) Estimate variational bound
(cid:46) Compute natural gradient

function PGMINFERENCE(ηθ, ψ)

q∗(x)
←
return sample ˆx

∼

OPTIMIZELOCALFACTORS(ηθ, ψ)

q∗(x), statistics E

q∗(x)tx(x), divergence E

(cid:46) Fast message-passing inference

q(θ) KL(q∗(x)
(cid:107)

p(x

θ))

|

simple and efﬁcient and can be readily applied to a variety of learning problems and graphical model
structures. See the supplementals for details and proofs.

4.1 SVAE model class

To set up notation for a general SVAE, we ﬁrst deﬁne a conjugate pair of exponential family densities
on global latent variables θ and local latent variables x =
θ) be an exponential
family and let p(θ) be its corresponding natural exponential family conjugate prior, writing
log Zθ(η0

η0
θ , tθ(θ)
(cid:105) −
x(θ))(cid:9) = exp
η0
x(θ), tx(x)
,
where we used exponential family conjugacy to write tθ(θ) = (cid:0)η0
x(θ))(cid:1). The local
latent variables x could have additional structure, like including both discrete and continuous latent
variables or tractable graph structure, but here we keep the notation simple.

p(θ) = exp (cid:8)
(cid:104)
θ) = exp (cid:8)
(cid:104)

tθ(θ), (tx(x), 1)
log Zx(η0

θ )(cid:9) ,
log Zx(η0

N
n=1. Let p(x

{(cid:104)
x(θ),

xn}

p(x

(cid:105) −

(cid:105)}

−

{

|

|

Next, we deﬁne a general likelihood function. Let p(y
x, γ) be a general family of densities and
let p(γ) be an exponential family prior on its parameters. For example, each observation yn may
depend on the latent value xn through an MLP, as in the density network model of Section 2.
This generic non-conjugate observation model provides modeling ﬂexibility, yet the SVAE can still
leverage conjugate exponential family structure in inference, as we show next.

|

4.2 Stochastic variational inference algorithm

Though the general observation model p(y
x, γ) means that conjugate updates and natural gradient
SVI [10] cannot be directly applied, we show that by generalizing the recognition network idea we
can still approximately optimize out the local variational factors leveraging conjugacy structure.

|

For ﬁxed y, consider the mean ﬁeld family q(θ)q(γ)q(x) and the variational inference objective

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(3)

Without loss of generality we can take the global factor q(θ) to be in the same exponential family
as the prior p(θ), and we denote its natural parameters by ηθ. We restrict q(γ) to be in the same
exponential family as p(γ) with natural parameters ηγ. Finally, we restrict q(x) to be in the same
exponential family as p(x
θ), writing its natural parameter as ηx. Using these explicit variational
parameters, we write the mean ﬁeld variational inference objective in Eq. (3) as

(ηθ, ηγ, ηx).

|

(ηθ, ηγ, ηx), we consider choosing the variational
To perform efﬁcient optimization of the objective
L
parameter ηx as a function of the other parameters ηθ and ηγ. One natural choice is to set ηx to be a
. However, without conjugacy structure ﬁnding a local partial optimizer
local partial optimizer of
may be computationally expensive for general densities p(y
x, γ), and in the large data setting this
expensive optimization would have to be performed for each stochastic gradient update. Instead, we
choose ηx by optimizing over a surrogate objective (cid:98)
L
(cid:20)

with conjugacy structure, given by

L

(cid:21)

|

(ηθ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(x)

log

p(θ)p(x

|

θ) exp
{
q(θ)q(x)

ψ(x; y, φ)
}

, ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:105)

,

(cid:104)

L

5

r(y; φ)

}φ∈Rm is some parameterized class of functions that serves as the recognition model.
θ). We

where
{
Note that the potentials ψ(x; y, φ) have a form conjugate to the exponential family p(x
deﬁne η∗
x(ηθ, φ) (cid:44) arg min
η∗

along with the corresponding factor q∗(x),
x(ηθ, φ))

x(ηθ, φ) to be a local partial optimizer of (cid:98)
L
(ηθ, ηx, φ),
(cid:98)
L

η∗
x(ηθ, φ), tx(x)

q∗(x) = exp

log Zx(η∗

(cid:105) −

{(cid:104)

}

|

.

ηx

As with the variational autoencoder of Section 3.2, the resulting variational factor q∗(x) is suboptimal
for the variational objective
. However, because the surrogate objective has the same form as a
variational inference objective for a conjugate observation model, the factor q∗(x) not only is easy to
compute but also inherits exponential family and graphical model structure for tractable inference.

L

(ηθ, ηγ, η∗

x(ηθ, φ), the SVAE objective is

Given this choice of η∗
objective is a lower bound for the variational inference objective Eq. (3) in the following sense.
Proposition 4.1 (The SVAE objective lower-bounds the mean ﬁeld objective)
The SVAE objective function
max
q(x) L

in the sense that
Rm,

LSVAE(ηθ, ηγ, φ) (cid:44)

LSVAE lower-bounds the mean ﬁeld objective
L
≥ LSVAE(ηθ, ηγ, φ)
}φ∈Rm. Furthermore, if there is some φ∗
φ LSVAE(ηθ, ηγ, φ).
(ηθ, ηγ, ηx) = max

∈
x, γ), then the bound can be made tight in the sense that

for any parameterized function class
that ψ(x; y, φ∗) = E

q(γ) log p(y
|
[ q(θ)q(γ)q(x) ] = max

ηx L
r(y; φ)

[ q(θ)q(γ)q(x) ]

(ηθ, ηγ, ηx)

max
q(x) L

ηx L

max

≥

L

∈

φ

∀

{

x(ηθ, φ)). This

Rm such

LSVAE(ηθ, ηγ, φ) we are maximizing a lower
Thus by using gradient-based optimization to maximize
bound on the model log evidence log p(y). In particular, by optimizing over φ we are effectively
learning how to condition on observations so as to best approximate the posterior while maintaining
conjugacy structure. Furthermore, to provide the best lower bound we may choose the recognition
model function class

r(y; φ)

{

}φ∈Rm to be as rich as possible.

Choosing η∗
x(ηθ, φ) to be a local partial optimizer of (cid:98)
provides two computational advantages. First,
L
it allows η∗
x(ηθ, φ) and expectations with respect to q∗(x) to be computed efﬁciently by exploiting
exponential family graphical model structure. Second, it provides computationally efﬁcient ways to
estimate the natural gradient with respect to the latent model parameters, as we summarize next.
Proposition 4.2 (Natural gradient of the SVAE objective)
The natural gradient of the SVAE objective

LSVAE with respect to ηθ can be estimated as
∇

(cid:1) + (
(4)
θ, φ)). When there is only one local variational factor q(x), then we

q∗(x) [(tx(x), 1)]

2 log Zθ(ηθ))

θ + E

F (ηθ),

ηθ

∇

−

−1

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

(cid:101)
where F (η(cid:48)
(ηθ, ηγ, η∗
θ) =
can simplify the estimator to

x(η(cid:48)
∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

L

(cid:101)

θ + E

q∗(x) [(tx(x), 1)]

(cid:1) + (

ηθ

−

∇ηx L

(ηθ, ηγ, η∗

x(ηθ, φ)), 0).

(ηθ, ηγ, η∗

Note that the ﬁrst term in Eq. (4) is the same as the expression for the natural gradient in SVI for
F (ηθ) in the ﬁrst expression or, alternatively,
conjugate models [10], while a stochastic estimate of
x(ηθ, φ)) in the second expression is computed automatically
a stochastic estimate of
∇ηθ L
as part of the backward pass for computing the gradients with respect to the other parameters, as
described next. Thus we have an expression for the natural gradient with respect to the latent
model’s parameters that is almost as simple as the one for conjugate models, differing only by a term
involving the neural network likelihood function. Natural gradients are invariant to smooth invertible
reparameterizations of the variational family [16, 17] and provide effective second-order optimization
updates [18, 11].

∇

The gradients of the objective with respect
to the other variational parameters, namely
∇φLSVAE(ηθ, ηγ, φ), can be computed using the reparameterization trick
∇ηγ LSVAE(ηθ, ηγ, φ) and
and standard automatic differentiation techniques. To isolate the terms that require the reparameteri-
zation trick, we rearrange the objective as
q(γ)q∗(x) log p(y

−
The KL divergence terms are between members of the same tractable exponential families. An
unbiased estimate of the ﬁrst term can be computed by sampling ˆx
q(γ) and
computing

LSVAE(ηθ, ηγ, φ) = E

ˆx, ˆγ) with automatic differentiation.

KL(q(θ)q∗(x)

q∗(x) and ˆγ

KL(q(γ)

p(θ, x))

p(γ)).

x, γ)

−

∼

∼

(cid:107)

(cid:107)

|

∇ηγ ,φ log p(y

|

6

5 Related work

In addition to the papers already referenced, there are several recent papers to which this work is
related.

The two papers closest to this work are Krishnan et al. [19] and Archer et al. [20]. In Krishnan et al.
[19] the authors consider combining variational autoencoders with continuous state-space models,
emphasizing the relationship to linear dynamical systems (also called Kalman ﬁlter models). They
primarily focus on nonlinear dynamics and an RNN-based variational family, as well as allowing
control inputs. However, the approach does not extend to general graphical models or discrete latent
variables. It also does not leverage natural gradients or exact inference subroutines.

In Archer et al. [20] the authors also consider the problem of variational inference in general
continuous state space models but focus on using a structured Gaussian variational family without
considering parameter learning. As with Krishnan et al. [19], this approach does not include discrete
latent variables (or any latent variables other than the continuous states). However, the method they
develop could be used with an SVAE to handle inference with nonlinear dynamics.

In addition, both Gregor et al. [21] and Chung et al. [22] extend the variational autoencoder framework
to sequential models, though they focus on RNNs rather than probabilistic graphical models.

Finally, there is much related work on handling nonconjugate model terms in mean ﬁeld variational
inference. In Khan et al. [23] and Khan et al. [24] the authors present a general scheme that is
able to exploit conjugate exponential family structure while also handling arbitrary nonconjugate
model factors, including the nonconjugate observation models we consider here. In particular, they
propose using a proximal gradient framework and splitting the variational inference objective into a
difﬁcult term to be linearized (with respect to mean parameters) and a tractable concave term, so that
the resulting proximal gradient update is easy to compute, just like in a fully conjugate model. In
Knowles et al. [25], the authors propose performing natural gradient descent with respect to natural
parameters on each of the variational factors in turn, and they focus on approximating expectations of
nonconjugate energy terms in the objective with model-speciﬁc lower-bounds (rather than estimating
them with generic Monte Carlo). As in conjugate SVI [10], they observe that, on conjugate factors
and with an undamped update (i.e. a unit step size), the natural gradient update reduces to the standard
conjugate mean ﬁeld update.

In contrast to the approaches of Khan et al. [23], Khan et al. [24], and Knowles et al. [25], rather
than linearizing intractable terms around the current iterate, in this work we handle intractable terms
via recognition networks and amoritized inference (and the remaining tractable objective terms are
multi-concave in general, analogous to SVI [10]). That is, we use parametric function approximators
to learn to condition on evidence in a conjugate form. We expect these approaches to handling
nonconjugate objective terms may be complementary, and the best choice may be situation-dependent.
For models with local latent variables and datasets where minibatch-based updating is important,
using inference networks to compute local variational parameters in a ﬁxed-depth circuit (as in
the VAE [7, 8]) or optimizing out the local variational factors using fast conjugate updates (as in
conjugate SVI [10]) can be advantageous because in both cases local variational parameters for the
entire dataset need not be maintained across updates. The SVAE we propose here is a way to combine
the inference network and conjugate SVI approaches.

6 Experiments

We apply the SVAE to both synthetic and real data and demonstrate its ability to learn feature
representations and latent structure. Code is available at github.com/mattjj/svae.

6.1 LDS SVAE for modeling synthetic data

Consider a sequence of 1D images representing a dot bouncing from one side of the image to the
other, as shown at the top of Fig. 4. We use an LDS SVAE to ﬁnd a low-dimensional latent state
space representation along with a nonlinear image model. The model is able to represent the image
accurately and to make long-term predictions with uncertainty. See supplementals for details.

7

(a) Predictions after 200 training steps.

(b) Predictions after 1100 training steps.

Figure 4: Predictions from an LDS SVAE ﬁt to 1D dot image data at two stages of training. The
top panel shows an example sequence with time on the horizontal axis. The middle panel shows the
noiseless predictions given data up to the vertical line, while the bottom panel shows the latent states.

(a) Natural (blue) and standard (orange) gradient updates.

(b) Subspace of learned observation model.

Figure 5: Experimental results from LDS SVAE models on synthetic data and real mouse data.

This experiment also demonstrates the optimization advantages that can be provided by the natural
gradient updates. In Fig. 5a we compare natural gradient updates with standard gradient updates at
three different learning rates. The natural gradient algorithm not only learns much faster but also
is less dependent on parameterization details: while the natural gradient update used an untuned
stepsize of 0.1, the standard gradient dynamics at step sizes of both 0.1 and 0.05 resulted in some
matrix parameters to be updated to indeﬁnite values.

6.2 LDS SVAE for modeling video

We also apply an LDS SVAE to model depth video recordings of mouse behavior. We use the dataset
from Wiltschko et al. [3] in which a mouse is recorded from above using a Microsoft Kinect. We
used a subset consisting of 8 recordings, each of a distinct mouse, 20 minutes long at 30 frames per
second, for a total of 288000 video fames downsampled to 30

30 pixels.

We use MLP observation and recognition models with two hidden layers of 200 units each and a 10D
latent space. Fig. 5b shows images corresponding to a regular grid on a random 2D subspace of the
latent space, illustrating that the learned image manifold accurately captures smooth variation in the
mouse’s body pose. Fig. 6 shows predictions from the model paired with real data.

×

6.3 SLDS SVAE for parsing behavior

Finally, because the LDS SVAE can accurately represent the depth video over short timescales, we
apply the latent switching linear dynamical system (SLDS) model to discover the natural units of
behavior. Fig. 7 and Fig. 8 in the appendix show some of the discrete states that arise from ﬁtting an
SLDS SVAE with 30 discrete states to the depth video data. The discrete states that emerge show a
natural clustering of short-timescale patterns into behavioral units. See the supplementals for more.

8

Figure 6: Predictions from an LDS SVAE ﬁt to depth video. In each panel, the top is a sampled
prediction and the bottom is real data. The model is conditioned on observations to the left of the line.

(a) Extension into running

(b) Fall from rear

Figure 7: Examples of behavior states inferred from depth video. Each frame sequence is padded on
both sides, with a square in the lower-right of a frame depicting when the state is the most probable.

7 Conclusion

Structured variational autoencoders provide a general framework that combines some of the strengths
of probabilistic graphical models and deep learning methods. In particular, they use graphical models
both to give models rich latent representations and to enable fast variational inference with CRF-like
structured approximating distributions. To complement these structured representations, SVAEs use
neural networks to produce not only ﬂexible nonlinear observation models but also fast recognition
networks that map observations to conjugate graphical model potentials.

9

References

MIT Press, 2009.

[1] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.

[2] Kevin P Murphy. Machine Learning: a Probabilistic Perspective. MIT Press, 2012.
[3] Alexander B. Wiltschko, Matthew J. Johnson, Giuliano Iurilli, Ralph E. Peterson, Jesse M.
Katon, Stan L. Pashkovski, Victoria E. Abraira, Ryan P. Adams, and Sandeep Robert Datta.
“Mapping Sub-Second Structure in Mouse Behavior”. In: Neuron 88.6 (2015), pp. 1121–1135.
[4] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. “Deep neural
networks for acoustic modeling in speech recognition: The shared views of four research
groups”. In: Signal Processing Magazine, IEEE 29.6 (2012), pp. 82–97.

[5] Li Deng. “Computational models for speech production”. In: Computational Models of Speech

Pattern Processing. Springer, 1999, pp. 199–213.

[6] Li Deng. “Switching dynamic system models for speech articulation and acoustics”. In:
Mathematical Foundations of Speech and Language Processing. Springer, 2004, pp. 115–133.
[7] Diederik P. Kingma and Max Welling. “Auto-Encoding Variational Bayes”. In: International

Conference on Learning Representations (2014).

[8] Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. “Stochastic Backpropagation and
Approximate Inference in Deep Generative Models”. In: Proceedings of the 31st International
Conference on Machine Learning. 2014, pp. 1278–1286.

[9] Matthew J. Johnson and Alan S. Willsky. “Stochastic Variational Inference for Bayesian Time

Series Models”. In: International Conference on Machine Learning. 2014.

[10] Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. “Stochastic variational

[11]

inference”. In: Journal of Machine Learning Research (2013).
James Martens. “New insights and perspectives on the natural gradient method”. In: arXiv
preprint arXiv:1412.1193 (2015).

[12] David J.C. MacKay and Mark N. Gibbs. “Density networks”. In: Statistics and neural networks:

advances at the interface. Oxford University Press, Oxford (1999), pp. 129–144.

[13] Tomoharu Iwata, David Duvenaud, and Zoubin Ghahramani. “Warped Mixtures for Nonpara-
metric Cluster Shapes”. In: Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2013,
pp. 311–319.

[14] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. “Bayesian Nonparametric Inference
of Switching Dynamic Linear Models”. In: IEEE Transactions on Signal Processing 59.4
(2011).

[15] Martin J. Wainwright and Michael I. Jordan. “Graphical Models, Exponential Families, and

Variational Inference”. In: Foundations and Trends in Machine Learning (2008).

[16] Shun-Ichi Amari. “Natural gradient works efﬁciently in learning”. In: Neural computation

10.2 (1998), pp. 251–276.

[17] Shun-ichi Amari and Hiroshi Nagaoka. Methods of Information Geometry. American Mathe-

[18]

matical Society, 2007.
James Martens and Roger Grosse. “Optimizing Neural Networks with Kronecker-factored
Approximate Curvature”. In: arXiv preprint arXiv:1503.05671 (2015).

[19] Rahul G Krishnan, Uri Shalit, and David Sontag. “Deep Kalman Filters”. In: arXiv preprint

arXiv:1511.05121 (2015).

[20] Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. “Black
box variational inference for state space models”. In: arXiv preprint arXiv:1511.07367 (2015).
[21] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. “DRAW: A recurrent neural

[22]

network for image generation”. In: arXiv preprint arXiv:1502.04623 (2015).
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua
Bengio. “A recurrent latent variable model for sequential data”. In: Advances in Neural
information processing systems. 2015, pp. 2962–2970.

[23] Mohammad E Khan, Pierre Baqué, François Fleuret, and Pascal Fua. “Kullback-Leibler
proximal variational inference”. In: Advances in Neural Information Processing Systems. 2015,
pp. 3402–3410.

10

[24] Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, and Masashi Sugiyama.
“Faster Stochastic Variational Inference using Proximal-Gradient Methods with General Diver-
gence Functions”. In: Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2016.
[25] David A Knowles and Tom Minka. “Non-conjugate variational message passing for multino-
mial and binary regression”. In: Advances in Neural Information Processing Systems. 2011,
pp. 1701–1709.

[26] Dimitri P Bertsekas. Nonlinear programming. 2nd ed. Athena Scientiﬁc, 1999.
[27]

John M. Danskin. The theory of max-min and its application to weapons allocation problems.
Springer-Verlag, New York, 1967.

[28] Anthony-V Fiacco. Introduction to sensitivity and stability analysis in nonlinear programming.

[29]

[30]

Academic Press, Inc., 1984.
J Frederic Bonnans and Alexander Shapiro. Perturbation Analysis of Optimization Problems.
Springer Science & Business Media, 2000.
James Martens and Roger Grosse. “Optimizing Neural Networks with Kronecker-factored
Approximate Curvature”. In: Proceedings of the 32nd International Conference on Machine
Learning. 2015.

[31] David Duvenaud and Ryan P. Adams. “Black-box stochastic variational inference in ﬁve lines

of Python”. In: NIPS Workshop on Black-box Learning and Inference (2015).

11

In this section we ﬁx our notation for gradients and establish some basic deﬁnitions and results that
we use in the sequel.

A Optimization

A.1 Gradient notation

We follow the notation in Bertsekas [26, A.5]. In particular, if f : Rn
differentiable function, we deﬁne the gradient matrix of f , denoted
which the ith column is the gradient
That is,

Rm is a continuously
m matrix in
∇
fi(x) of fi, the ith coordinate function of f , for i = 1, 2, . . . , m.

f (x), to be the n

→

∇

×

f (x) = [

f1(x)

∇

∇

· · · ∇

fm(x)] .

∇

f is the Jacobian matrix of f , in which the ijth entry is the function ∂fi/∂xj.
R is continuously differentiable with continuously differentiable partial derivatives,
2f , to be the matrix in which the ijth entry is the

The transpose of
If f : Rn
→
then we deﬁne the Hessian matrix of f , denoted
function ∂2f /∂xi∂xj.
Rm
Finally, if f : Rn

R is a function of (x, y) with x

Rn and y

Rm, we write

∇

×

→

∈

∇xf (x, y) =

∇yf (x, y) =







∂f (x,y)
∂x1

...

∂f (x,y)
∂xm
(cid:18) ∂2f (x, y)
∂xi∂xj







,

(cid:19)

,

2
xyf (x, y) =

∇

2
yyf (x, y) =

∇

(cid:18) ∂2f (x, y)
∂xi∂yj

(cid:19)

.

2
xxf (x, y) =

∇

∈
∂f (x,y)
∂y1

...













∂f (x,y)
∂yn
(cid:18) ∂2f (x, y)
∂yi∂yj

(cid:19)

,

A.2 Local and partial optimizers

In this section we state the deﬁnitions of local partial optimizer and necessary conditions for optimality
that we use in the sequel.

Deﬁnition A.1 (Partial optimizer, local partial optimizer)
Let f : Rn
y∗

Rm an unconstrained partial optimizer of f given x if

R be an objective function to be maximized. For a ﬁxed x

Rm

→

×

∈

Rn, we call a point

∈

and we call y∗ an unconstrained local partial optimizer of f given x if there exists an (cid:15) > 0 such that

f (x, y)

f (x, y∗)

≤

Rm

y

∀

∈

f (x, y)

f (x, y∗)

≤

y with

∀

y

(cid:107)

−

y∗
(cid:107)

< (cid:15),

where

is any vector norm.

(cid:107) · (cid:107)

×

→

Rm

R be continuously differentiable. For ﬁxed x

Proposition A.2 (Necessary conditions for optimality, Prop. 3.1.1 of Bertsekas [26])
Let f : Rn
Rn if y∗
unconstrained local partial optimizer for f given x then
∇yf (x, y∗) = 0.
If instead x and y are subject to the constraints h(x, y) = 0 for some continuously differentiable
h : Rn
Rm and y∗ is a constrained local partial optimizer for f given x with the regularity
Rm
×
condition that

→
∇yh(x, y∗) is full rank, then there exists a Lagrange multiplier λ∗

Rm such that

Rm is an

∈

∈

∈

∇yf (x, y∗) +

∇yh(x, y∗)λ∗ = 0,

and hence the cost gradient
by the null space of

∇yh(x, y∗)T.

∇yf (x, y∗) is orthogonal to the ﬁrst-order feasible variations in y given

12

Note that the regularity condition on the constraints is not needed if the constraints are linear [26,
Prop. 3.3.7].
For a continuously differentiable function f : Rn

R, we say x∗ is a stationary point of f if
f (x∗) = 0. For general unconstrained smooth optimization, the limit points of gradient-based
∇
algorithms are guaranteed only to be stationary points of the objective, not necessarily local optima.
Block coordinate ascent methods, when available, provide slightly stronger guarantees: not only is
every limit point a stationary point of the objective, in addition each coordinate block is a partial
optimizer of the objective. Note that the objective functions we consider maximizing in the following
are bounded above.

→

A.3 Partial optimization and the Implicit Function Theorem

Let f : Rn
x
∈
y∗(x)

×
Rn and y

∈

Rm

R be a scalar-valued objective function of two unconstrained arguments
Rn a value
Rm be some function that assigns to each x

Rm, and let y∗ : Rn

→

Rm. Deﬁne the composite function g : Rn

→

∈

R as

∈

→

g(x) (cid:44) f (x, y∗(x))

and using the chain rule write its gradient as

g(x) =

∇

∇xf (x, y∗(x)) +

∇

y∗(x)

∇yf (x, y∗(x)).

(5)

∈

One choice of the function y∗(x) is to partially optimize f for any ﬁxed value of x. For example,
Rn, we could choose y∗ to satisfy
assuming that arg maxy f (x, y) is nonempty for every x
y∗(x)
arg maxy f (x, y), so that g(x) = maxy f (x, y).1 Similarly, if y∗(x) is chosen so that
∇yf (x, y∗(x)) = 0, which is satisﬁed when y∗(x) is an unconstrained local partial optimizer for f
given x, then the expression in Eq. (5) can be simpliﬁed as in the following proposition.
Proposition A.3 (Gradients of locally partially optimized objectives)
Let f : Rn
such that y∗(x) is differentiable, and deﬁne g(x) = f (x, y∗(x)). Then

R be continuously differentiable, let y∗ be a local partial optimizer of f given x

Rm

→

×

∈

g(x) =

∇

∇xf (x, y∗(x)).

Proof. If y∗ is an unconstrained local partial optimizer of f given x then it satisﬁes
and if y∗ is a regularly-constrained local partial optimizer then the feasible variation
orthogonal to the cost gradient
in Eq. (5) is zero.

∇yf (x, y∗) = 0,
y∗(x) is
∇
∇yf (x, y∗). In both cases the second term in the expression for
g(x)

∇

∇

In general, when y∗(x) is not a stationary point of f (x,
g(x) we
), to evaluate the gradient
·
y∗(x) in Eq. (5). However, this term may be difﬁcult to compute directly. The
need to evaluate
function y∗(x) may arise implicitly from some system of equations of the form h(x, y) = 0 for
Rm. For example, the value of y may
some continuously differentiable function h : Rn
→
be computed from x and h using a black-box iterative numerical algorithm. However, the Implicit
y∗(x) using only the derivatives of h and
Function Theorem provides another means to compute
the value of y∗(x).
Proposition A.4 (Implicit Function Theorem, Prop. A.25 of Bertsekas [26])
Let h : Rn

Rm be a function and ¯x

Rm be points such that

Rn and ¯y

Rm

Rm

∇

∇

×

∈

∈

×

→
1. h(¯x, ¯y) = 0

2. h is continuous and has a continuous nonsingular gradient matrix

∇yh(x, y) in an open set

containing (¯x, ¯y).

Then there exist open sets S¯x ⊆
function y∗ : S¯x →

S¯y such that ¯y = y∗(x) and h(x, y∗(x)) = 0 for all x

Rm containing ¯x and ¯y, respectively, and a continuous
S¯x. The function y∗ is

Rn and S¯y ⊆

∈

1For a discussion of differentiability issues when there is more than one optimizer, i.e. when arg maxy f (x, y)
has more than one element, see Danskin [27], Fiacco [28, Section 2.4], and Bonnans et al. [29, Chapter 4]. Here
we only consider the sensitivity of local stationary points and assume differentiability almost everywhere.

13

unique in the sense that if x
some p > 0, h is p times continuously differentiable, the same is true for y∗, and we have

S¯y, and h(x, y) = 0, then y = y∗(x). Furthermore, if for

S¯x, y

∈

∈

y∗(x) =

∇

−∇xh (x, y∗(x)) (

∇yh (x, y∗(x)))−1 ,

x

∀

∈

S¯x.

As a special case, the equations h(x, y) = 0 may be the ﬁrst-order stationary conditions of another
unconstrained optimization problem. That is, the value of y may be chosen by locally partially
optimizing the value of u(x, y) for a function u : Rn
R with no constraints on y, leading to
the following corollary.
Corollary A.5 (Implicit Function Theorem for optimization subroutines)
Let u : Rn
∇yu
×
satisﬁes the hypotheses of Proposition A.4 at some point (¯x, ¯y), and deﬁne y∗ as in Proposition A.4.
Then we have

R be a twice continuously differentiable function such that the choice h =

Rm

Rm

→

→

×

y∗(x) =

∇

−∇

2

xyu (x, y∗(x)) (cid:0)

2

yyu (x, y∗(x))(cid:1)−1

,

∇

x

∀

∈

S¯x.

B Exponential families

In this section we set up notation for exponential families and outline some basic results. Throughout
this section we take all densities to be absolutely continuous with respect to the appropriate Lebesgue
measure (when the underlying set
is discrete),
X
) (generated by Euclidean and discrete topologies,
and denote the Borel σ-algebra of a set
respectively). We assume measurability of all functions as necessary.

is Euclidean space) or counting measure (when

as

X

X

X

B

(

Given a statistic function tx :
of probability densities on

Rn and a base measure νX , we can deﬁne an exponential family

X →
relative to νX and indexed by natural parameter ηx ∈
,
(cid:105)}
|

ηx, tx(x)
is the standard inner product on Rn. We also deﬁne the partition function as

ηx ∈
∀

X
p(x

Rn,

ηx)

exp

{(cid:104)

∝

Rn by

where

,
(cid:104)·

·(cid:105)

(cid:90)

Zx(ηx) (cid:44)

exp

ηx, tx(x)

νX (dx)

{(cid:104)

(cid:105)}

and deﬁne H

Rn to be the set of all normalizable natural parameters,

⊆

H (cid:44)

η
{

∈

Rn : Zx(η) <

.

∞}

We can write the normalized probability density as

p(x

η) = exp

ηx, tx(x)

log Zx(ηx)

.

|

{(cid:104)
We say that an exponential family is regular if H is open, and minimal if there is no η
0
}
= 0 (νX -a.e.). We assume all families are regular and minimal.2 Finally, when
such that
we parameterize the family with some other coordinates θ, we write the natural parameter as a
continuous function ηx(θ) and write the density as

η, tx(x)
(cid:105)

(cid:105) −

\ {

Rn

∈

}

(cid:104)

(6)

p(x

θ) = exp

ηx(θ), tx(x)

log Zx(ηx(θ))

(cid:105) −
x (H) to be the open set of parameters that correspond to normalizable densities.

{(cid:104)

}

|

and take Θ = η−1
We summarize this notation in the following deﬁnition.
Deﬁnition B.1 (Exponential family of densities)
Given a measure space (
function ηx : Θ

(
X

X

B

,

), νX ), a statistic function tx :

→

Rn, the corresponding exponential family of densities relative to νX is

X →

Rn, and a natural parameter

where

is the log partition function.

p(x

θ) = exp

ηx(θ), tx(x)

log Zx(ηx(θ))

,

|

{(cid:104)

(cid:105) −

}

log Zx(ηx) (cid:44) log

(cid:90)

exp

ηx, tx(x)

νX (dx)

{(cid:104)

(cid:105)}

2Families that are not minimal, like the density of the categorical distribution, can be treated by restricting all
algebraic operations to the subspace spanned by the statistic, i.e. to the smallest V ⊂ Rn with range tx ⊆ V .

14

When we write exponential families of densities for different random variables, we change the
subscripts on the statistic function, natural parameter function, and log partition function to correspond
to the symbol used for the random variable. When the corresponding random variable is clear from
context, we drop the subscripts to simplify notation.

The next proposition shows that the log partition function of an exponential family generates cumu-
lants of the statistic.
Proposition B.2 (Gradients of log Z and expected statistics)
The gradient of the log partition function of an exponential family gives the expected sufﬁcient
statistic,

log Z(η) = E

p(x | η) [t(x)] ,

∇

where the expectation is over the random variable x with density p(x
generating function of t(x) can be written

|

η). More generally, the moment

Mt(x)(s) (cid:44) E

p(x | η)

(cid:104)

e(cid:104)s,t(x)(cid:105)(cid:105)

= elog Z(η+s)−log Z(η)

and so derivatives of log Z give cumulants of t(x), where the ﬁrst cumulant is the mean and the
second and third cumulants are the second and third central moments, respectively.

Given an exponential family of densities on
as in Deﬁnition B.1, we can deﬁne a related exponential
family of densities on Θ by deﬁning a statistic function tθ(θ) in terms of the functions ηx(θ) and
log Zx(ηx(θ)).
Deﬁnition B.3 (Natural exponential family conjugate prior)
Given the exponential family p(x
as the concatenation

θ) of Deﬁnition B.1, deﬁne the statistic function tθ : Θ

Rn+1

→

X

|

−
where the ﬁrst n coordinates of tθ(θ) are given by ηx(θ) and the last coordinate is given by
log Zx(ηx(θ)). We call the exponential family with statistic tθ(θ) the natural exponential family

tθ(θ) (cid:44) (ηx(θ),

log Zx(ηx(θ))) ,

−
conjugate prior to the density p(x

θ) and write

|

p(θ) = exp

ηθ, tθ(θ)

{(cid:104)

log Zθ(ηθ)
}

(cid:105) −

Rn+1 and the density is taken relative to some measure νΘ on (Θ,

where ηθ ∈
Notice that using tθ(θ) we can rewrite the original density p(x

θ) as

B

(Θ)).

|

p(x

log Zx(ηx(θ))

θ) = exp
= exp

ηx(θ), tx(x)
(cid:105) −
tθ(θ), (tx(x), 1)

|

{(cid:104)
{(cid:104)
This relationship is useful in Bayesian inference: when the exponential family p(x
θ) is a likelihood
function and the family p(θ) is used as a prior, the pair enjoy a convenient conjugacy property, as
summarized in the next proposition.
Proposition B.4 (Conjugacy)
Let the densities p(x
relations

θ) and p(θ) be deﬁned as in Deﬁnitions B.1 and B.3, respectively. We have the

(cid:105)}

}

|

|

.

p(θ, x) = exp
x) = exp
p(θ

|
and hence in particular the posterior p(θ
parameter ηθ + (tx(x), 1). Similarly, with multiple likelihood terms p(xi |
have

ηθ + (tx(x), 1), tθ(θ)
ηθ + (tx(x), 1), tθ(θ)

log Zθ(ηθ)
}
log Zθ(ηθ + (tx(x), 1))
}
x) is in the same exponential family as p(θ) with the natural
θ) for i = 1, 2, . . . , N we

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

(7)

|

N
(cid:89)

i=1

(cid:40)

N
(cid:88)

i=1

p(θ)

p(xi |

θ) = exp

ηθ +
(cid:104)

(tx(xi), 1), tθ(θ)

log Zθ(ηθ)

.

(8)

(cid:105) −

(cid:41)

Finally, we give a few more exponential family properties that are useful for gradient-based optimiza-
tion algorithms and variational inference. In particular, we note that the Fisher information matrix of
an exponential family can be computed as the Hessian matrix of its log partition function, and that
the KL divergence between two members of the same exponential family has a simple expression.

15

Deﬁnition B.5 (Score vector and Fisher information matrix)
Given a family of densities p(x
of the log density with respect to the parameter,
v(x, θ) (cid:44)
and the Fisher information matrix for the parameter θ is the covariance of the score,

∇θ log p(x

θ),

|

|

θ) indexed by a parameter θ, the score vector v(x, θ) is the gradient

I(θ) (cid:44) E (cid:2)v(x, θ)v(x, θ)T(cid:3) ,

where the expectation is taken over the random variable x with density p(x
used the identity E[v(x, θ)] = 0.
Proposition B.6 (Score and Fisher information for exponential families)
Given an exponential family of densities p(x
the score with respect to the natural parameter is given by
η) = t(x)

|
∇η log p(x
and the Fisher information matrix is given by
I(η) =

2 log Z(η).

v(x, η) =

log Z(η)

− ∇

|

|

∇

θ), and where we have

η) indexed by the natural parameter η, as in Eq. (6),

Proposition B.7 (KL divergence in an exponential family)
Given an exponential family of densities p(x
η) indexed by the natural parameter η, as in Eq. (6),
and two particular members with natural parameters η1 and η2, respectively, the KL divergence from
one to the other is

|

KL(p(x

η1)

p(x

η2)) (cid:44) E

p(x | η1)

|

(cid:107)

|

(cid:20)

log

(cid:21)

p(x
p(x

η1)
|
η2)
|
log Z(η1)

=

η1 −

(cid:104)

η2,

∇

(log Z(η1)

log Z(η2)).

(cid:105) −

−

C Natural gradient SVI for exponential families

In this section we give a derivation of the natural gradient stochastic variational inference (SVI)
method of Hoffman et al. [10] using our notation. We extend the algorithm in Section D.

C.1 SVI objective

θ) be an exponential family and p(θ) be its corresponding natural exponential family

|

Let p(x, y
prior as in Deﬁnitions B.1 and B.3, writing
p(θ) = exp (cid:8)
η0
θ , tθ(θ)
(cid:104)
θ) = exp (cid:8)
η0
xy(θ), txy(x, y)
(cid:104)
(cid:105) −
tθ(θ), (txy(x, y), 1)
= exp
where we have used tθ(θ) = (cid:0)η0
log Zxy(η0
xy(θ),
Given a ﬁxed observation y, for any density q(θ, x) = q(θ)q(x) we have
(cid:20)

(cid:105)}
xy(θ))(cid:1) in Eq. (9).

θ )(cid:9)
log Zxy(η0

log Zθ(η0

p(x, y

(cid:105) −

{(cid:104)

−

(cid:21)

|

xy(θ))(cid:9)

log p(y) = E

q(θ)q(x)

log

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)
p(θ)p(x, y
|
q(θ)q(x)

θ)

(cid:21)

θ)

E

≥

q(θ)q(x)

+ KL(q(θ)q(x)

p(θ, x

y))

(cid:107)

|

where we have used the fact that the KL divergence is always nonnegative. Therefore to choose
q(θ)q(x) to minimize the KL divergence to the posterior p(θ, x
y) we deﬁne the mean ﬁeld varia-
tional inference objective as

|

[ q(θ)q(x) ] (cid:44) E

q(θ)q(x)

L

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)

(cid:21)

θ)

and the mean ﬁeld variational inference problem as
maxq(θ)q(x)L

[ q(θ)q(x) ] .

(9)

(10)

(11)

The following proposition shows that because of the exponential family conjugacy structure, we
can ﬁx the parameterization of q(θ) and still optimize over all possible densities without loss of
generality.

16

Proposition C.1 (Optimal form of the global variational factor)
Given the mean ﬁeld optimization problem Eq. (11), for any ﬁxed q(x) the optimal factor q(θ) is
detetermined (νΘ-a.e.) by

θ + E
η0
(cid:104)
In particular, the optimal q(θ) is in the same exponential family as the prior p(θ).

q(x) [ (txy(x, y), 1) ] , tθ(θ)
(cid:105)

q(θ)

∝

exp (cid:8)

(cid:9) .

This proposition follows immediately from a more general lemma, which we reuse in the sequel.
Lemma C.2 (Optimizing a mean ﬁeld factor)
Let p(a, b, c) be a joint density and let q(a), q(b), and q(c) be mean ﬁeld factors. Consider the mean
ﬁeld variational inference objective

E

q(a)q(b)q(c)

(cid:20)

log

p(a, b, c)
q(a)q(b)q(c)

(cid:21)

.

For ﬁxed q(a) and q(c), the partially optimal factor q∗(b) over all possible densities,

q∗(b) (cid:44) arg max

E

q(a)q(b)q(c)

q(b)

(cid:20)

log

p(a, b, c)
q(a)q(b)q(c)

(cid:21)

,

(12)

is deﬁned (almost everywhere) by

In particular, if p(c
|
prior, and log p(b, c

|

q∗(b)

exp (cid:8)E

q(a)q(c) log p(a, b, c)(cid:9) .

∝
b, a) is an exponential family with p(b
|
a) is a multilinear polynomial in the statistics tb(b) and tc(c), written

a) its natural exponential family conjugate

log Zb(η0

b (a))(cid:9) ,

|

p(b

p(c

a) = exp (cid:8)
(cid:104)
|
b, a) = exp (cid:8)
(cid:104)
= exp (cid:8)
(cid:104)
c (a), then the optimal factor can be written
η∗
q(a)η0
b , tb(b)

η0
b (a), tb(b)
(cid:105) −
η0
c (b, a), tc(c)
tb(b), η0

(cid:105) −
c (a)T(tc(c), 1)

log Zb(η∗
b )

(cid:44) E

η∗
b

,

log Zc(η0
(cid:9) ,
(cid:105)

c (b, a))(cid:9)

b (a) + E

q(a)q(c)η0

{(cid:104)

(cid:105) −

As a special case, when c is conditionally independent of b given a, so that p(c
b (a) + E

tb(b), (tc(c), 1)

b) = exp

q(a)η0

(cid:44) E

p(c

η∗
b

,

|

{(cid:104)

}

(cid:105)}

for some matrix η0
q∗(b) = exp

c (a)T(tc(c), 1).
b, a) = p(c
q(c)(tc(c), 1).

|

|

b), then

Proof. Rewrite the objective in Eq. (12), dropping terms that are constant with respect to q(b), as

E

q(a)q(b)q(c)

(cid:20)

log

(cid:21)

p(a, b, c)
q(b)

= E

q(b)

(cid:2)E

q(a)q(c) log p(a, b, c)

log q(b)(cid:3)

−

q(a)q(c) log p(a, b, c)

log q(b)(cid:3)

−

= E

q(b)

(cid:2)log exp E
(cid:21)
(cid:20) q(b)
(cid:101)p(b)

E

q(b)

−

=

=

KL(q(b)

+ const

−
where we have deﬁned a new density (cid:101)p(b)
objective by setting the KL divergence to zero, choosing q(b)
rest follows from plugging in the exponential family densities.

exp (cid:8)E

∝

∝

(cid:107) (cid:101)p(b)) + const,
q(a)q(c) log p(a, b, c)(cid:9). We can maximize the
q(a)q(c) log p(a, b, c)(cid:9). The

exp (cid:8)E

Proposition C.1 justiﬁes parameterizing the density q(θ) with variational natural parameters ηθ as
ηθ, tθ(θ)
where the statistic function tθ and the log partition function log Zθ are the same as in the prior
family p(θ). Using this parameterization, we can deﬁne the mean ﬁeld objective as a function of the
parameters ηθ, partially optimizing over q(x),

log Zθ(ηθ)
}

q(θ) = exp

(cid:105) −

{(cid:104)

(ηθ) (cid:44) max
q(x)

E

L

q(θ)q(x)

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)

(cid:21)

θ)

.

(13)

The partial optimization over q(x) in Eq. (13) should be read as choosing q(x) to be a local partial
optimizer of Eq. (10); in general, it may be intractable to ﬁnd a global partial optimizer, and the results
that follow use only ﬁrst-order stationary conditions on q(x). We refer to this objective function,
where we locally partially optimize the mean ﬁeld objective Eq. (10) over q(x), as the SVI objective.

17

C.2 Easy natural gradients of the SVI objective

By again leveraging the conjugate exponential family structure, we can write a simple expression for
the gradient of the SVI objective, and even for its natural gradient.
Proposition C.3 (Gradient of the SVI objective)
Let the SVI objective

(ηθ) be deﬁned as in Eq. (13). Then the gradient
L
(ηθ) = (cid:0)

∇L
q∗(x) [ (txy(x, y), 1) ]

2 log Zθ(ηθ)(cid:1) (cid:0)η0

θ + E

(ηθ) is
(cid:1)

ηθ

∇L

∇

where q∗(x) is a local partial optimizer of the mean ﬁeld objective Eq. (10) for ﬁxed global variational
parameters ηθ.

−

Proof. First, note that because q∗(x) is a local partial optimizer for Eq. (10) by Proposition A.3, we
have

(ηθ) =

E

∇ηθ

∇L

q(θ)q∗(x)

(cid:20)

log

p(θ)p(x, y

|
q(θ)q∗(x)

(cid:21)

θ)

.

Next, we use the conjugate exponential family structure and Proposition B.4, Eq. (7), to expand

E

q(θ)q∗(x)

(cid:20)

log

(cid:21)

θ)

p(θ)p(x, y

|
q(θ)q∗(x)

=

θ + E
η0
(cid:104)

−
Note that we can use Proposition B.2 to replace E
respect to ηθ and using the product rule, we have
2 log Zθ(ηθ) (cid:0)η0

(ηθ) =

∇L

∇

θ + E

q∗(x)(txy(x, y), 1)

ηθ, E

q(θ)[tθ(θ)]
(cid:105)

−
log Zθ(ηθ)(cid:1) .

(cid:0)log Zθ(η0
θ )
q(θ)[tθ(θ)] with

−

∇

log Zθ(ηθ). Differentiating with

log Zθ(ηθ) +

− ∇
2 log Zθ(ηθ) (cid:0)η0

∇
θ + E

=

∇

q∗(x)(txy(x, y), 1)
log Zθ(ηθ)
q∗(x)(txy(x, y), 1)

(cid:1)

ηθ

(cid:1) .

ηθ

−

−

As an immediate result of Proposition C.3, the natural gradient [16] deﬁned by
(ηθ) (cid:44) (cid:0)

2 log Zθ(ηθ)(cid:1)−1

(ηθ)

(cid:101)
∇L

∇

∇L

has an even simpler expression.
Corollary C.4 (Natural gradient of the SVI objective)
The natural gradient of the SVI objective Eq. (13) is

(cid:101)
∇L

(ηθ) = η0

θ + E

q∗(x)[ (txy(x, y), 1) ]

ηθ.

−

The natural gradient corrects for a kind of curvature in the variational family and is invariant to
reparameterization of the family [17]. As a result, natural gradient ascent is effectively a second-
order quasi-Newton optimization algorithm, and using natural gradients can greatly accelerate the
convergence of gradient-based optimization algorithms [30, 11]. It is a remarkable consequence of
the exponential family structure that natural gradients of the partially optimized mean ﬁeld objective
with respect to the global variational parameters can be computed efﬁciently (without any backward
pass as would be required in generic reverse-mode differentiation). Indeed, the exponential family
conjugacy structure makes the natural gradient of the SVI objective even easier to compute than the
ﬂat gradient.

C.3 Stochastic natural gradients for large datasets

The real utility of natural gradient SVI is in its application to large datasets. Consider the model
N
composed of global latent variables θ, local latent variables x =
n=1,

N
n=1, and data y =

xn}
{

yn}

{

p(θ, x, y) = p(θ)

p(xn, yn |

θ),

N
(cid:89)

n=1

18

where each p(xn, yn |
ﬁxed observations y =

yn}
{

N
n=1, let

θ) is a copy of the same likelihood function with conjugate prior p(θ). For

q(θ, x) = q(θ)

q(xn)

N
(cid:89)

n=1

be a variational family to approximate the posterior p(θ, x
y) and consider the SVI objective given
by Eq. (13). Using Eq. (8) of Proposition B.4, it is straightforward to extend the natural gradient
expression in Corollary C.4 to an unbiased Monte Carlo estimate which samples terms in the sum
over data points.

|

Corollary C.5 (Unbiased Monte Carlo estimate of the SVI natural gradient)
Using the model and variational family

p(θ, x, y) = p(θ)

q(θ)q(x) = q(θ)

q(xn),

N
(cid:89)

n=1

p(xn, yn |

θ),

N
(cid:89)

n=1

where p(θ) and p(xn, yn |
Let the random index ˆn be sampled from the set
takes value n. Then

{

1, 2, . . . , N

}

θ) are a conjugate pair of exponential families, deﬁne

(ηθ) as in Eq. (13).
and let pn > 0 be the probability it

L

(ηθ) = Eˆn

(cid:101)
∇L

(cid:20)
η0
θ +

E

1
pˆn

q∗(x ˆn)[ (txy(xˆn, yˆn), 1) ]

(cid:21)

ηθ

,

−

where q∗(xˆn) is a local partial optimizer of

given q(θ).

L

Proof. Taking expectation over the index ˆn, we have

Eˆn

(cid:20) 1
pˆn

(cid:21)
q∗(x ˆn)[ (txy(xˆn, yˆn), 1) ]

E

=

E

pn
pn

q∗(xn)[ (txy(xn, yn), 1) ]

N
(cid:88)

n=1

N
(cid:88)

n=1

=

E

q∗(xn)[ (txy(xn, yn), 1) ] .

The remainder of the proof follows from Proposition B.4 and the same argument as in Proposition C.3.

The unbiased stochastic gradient developed in Corollary C.5 can be used in a scalable stochastic
gradient ascent algorithm. To simplify notation, in the following sections we drop the notation
θ) for n = 1, 2, . . . , N and return to working with a single
for multiple likelihood terms p(xn, yn |
likelihood term p(x, y

θ). The extension to multiple likelihood terms is immediate.

|

C.4 Conditinally conjugate models and block updating

The model classes often considered for natural gradient SVI, and the main model classes we consider
here, have additional conjugacy structure in the local latent variables. In this section we introduce
notation for this extra structure in terms of the additional local latent variables z and discuss the local
block coordinate optimization that is often performed to compute the factor q∗(z)q∗(x) for use in the
natural gradient expression.

Let p(z, x, y
conjugate prior, writing

|

θ) be an exponential family and p(θ) be its corresponding natural exponential family

p(z, x, y

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

θ )(cid:9) ,

η0
log Zθ(η0
θ , tθ(θ)
(cid:104)
(cid:105) −
η0
zxy(θ), tzxy(z, x, y)
(cid:104)
tθ(θ), (tzxy(z, x, y), 1)

(cid:105) −

zxy(θ))(cid:9)

log Zzxy(η0
,

{(cid:104)
where we have used tθ(θ) = (cid:0)η0
zxy(θ),
tzxy(z, x, y) be a multilinear polynomial in the statistics functions tx(x), ty(y), and tz(z), let p(z

(15)
zxy(θ))(cid:1) in Eq. (15). Additionally, let
θ),

log Zzxy(η0

(cid:105)}

−

(14)

|

19

p(x
|
to p(x

z, θ), and p(y

|
z, θ) and p(x

|

x, z, θ) = p(y

x, θ) be exponential families, and let p(z

θ) be a conjugate prior

z, θ) be a conjugate prior to p(y

x, θ), so that

|

|

|

p(z

p(x

p(y

θ) = exp (cid:8)
|
z, θ) = exp (cid:8)
= exp (cid:8)
x, θ) = exp (cid:8)
= exp (cid:8)

|

|

|

log Zz(η0

z (θ))(cid:9) ,

η0
z (θ), tz(z)
(cid:104)
(cid:105) −
η0
x(z, θ), tx(x)
(cid:104)
tz(z), η0
(cid:104)
η0
y(x, θ), ty(y)
(cid:104)
tx(x), η0
(cid:104)

(cid:105) −
x(θ)T(tx(x), 1)

(cid:105) −
y(θ)T(ty(y), 1)
(cid:105)

log Zx(η0
(cid:9) ,
(cid:105)
log Zy(η0
(cid:9) ,

x(z, θ))(cid:9)

y(x, z, θ))(cid:9)

(16)

(17)

(18)

for some matrices η0

x(θ) and η0

y(θ).

This model class includes many common models, including the latent Dirichlet allocation, switching
linear dynamical systems with linear-Gaussian emissions, and mixture models and hidden Markov
models with exponential family emissions. The conditionally conjugate structure is both powerful
and restrictive: while it potentially limits the expressiveness of the model class, it enables block
coordinate optimization with very simple and fast updates, as we show next. When conditionally
conjugate structure is not present, these local optimizations can instead be performed with generic
gradient-based methods and automatic differentiation [31].
Proposition C.6 (Unconstrained block coordinate ascent on q(z) and q(x))
Let p(θ, z, x, y) be a model as in Eqs. (14)-(18), and for ﬁxed data y let q(θ)q(z)q(x) be a corre-
sponding mean ﬁeld variational family for approximating the posterior p(θ, z, x

y), with

|

q(θ) = exp
q(z) = exp
q(x) = exp

ηθ, tθ(θ)
ηz, tz(z)
ηx, tx(x)

log Zθ(ηθ)
}
log Zz(ηz)
}
log Zx(ηx)
}

,
,
,

(cid:105) −
(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)
{(cid:104)

and with the mean ﬁeld variational inference objective

[ q(θ)q(z)q(x) ] = E

q(θ)q(z)q(x)

L

(cid:20)

log

p(θ)p(z

|

z, θ)p(y

θ)p(x
|
q(θ)q(z)q(x)

|

x, z, θ)

(cid:21)

.

Fixing the other factors, the partial optimizers q∗(z) and q∗(x) for
given by

L

over all possible densities are

q∗(z) (cid:44) arg max

[ q(θ)q(z)q(x) ] = exp

q∗(x) (cid:44) arg max

[ q(θ)q(z)q(x) ] = exp

q(z) L

q(x) L

η∗
z , tz(z)

log Zz(η∗
z )
}

,

(cid:105) −

η∗
x, tx(x)

log Zx(η∗
x)
}

,

(cid:105) −

{(cid:104)

{(cid:104)

with

z = E
η∗
x = E
η∗

z (θ) + E

q(θ)η0
q(θ)q(z)η0

q(θ)q(x)η0
x(θ)tz(z) + E

x(θ)T(tx(x), 1),
q(θ)η0

y(θ)T(ty(y), 1).

(19)

(20)

Proof. This proposition is a consequence of Lemma C.2 and the conjugacy structure.

log Zθ(ηθ),

log Zz(ηz), and

Proposition C.6 gives an efﬁcient block coordinate ascent algorithm: for ﬁxed ηθ, by alternatively
updating ηz and ηx according to Eqs. (19)-(20) we are guaranteed to converge to a stationary point
that is partially optimal in the parameters of each factor. In addition, performing each update requires
only computing expected sufﬁcient statistics in the variational factors, which means evaluating
log Zx(ηx), quantities that be computed anyway in a gradient-
∇
based optimization routine. The block coordinate ascent procedure leveraging this conditional
conjugacy structure is thus not only efﬁcient but also does not require a choice of step size.
Note in particular that this procedure produces parameters η∗
x(ηθ) that are partially optimal
(and hence stationary) for the objective. That is, deﬁning the parameterized mean ﬁeld variational
inference objective as L(ηθ, ηz, ηx) =
[ q(θ)q(z)q(x) ], for ﬁxed ηθ the block coordinate ascent
procedure has limit points η∗

z (ηθ) and η∗

L
x that satisfy

z and η∗

∇

∇

(ηθ, η∗

z (ηθ), η∗

x(ηθ)) = 0,

∇ηz L

(ηθ, η∗

z (ηθ), η∗

x(ηθ)) = 0.

∇ηxL

20

D The SVAE objective and its gradients

In this section we deﬁne the SVAE variational lower bound and show how to efﬁciently compute
unbiased stochastic estimates of its gradients, including an unbiased estimate of the natural gradient
with respect to the variational parameters with conjugacy structure. The setup here parallels the setup
for natural gradient SVI in Section C, but while SVI is restricted to complete-data conjugate models,
here we consider more general likelihood models.

D.1 SVAE objective

Let p(x
conjugate prior, as in Deﬁnitions B.1 and B.3, writing

|

θ) be an exponential family and let p(θ) be its corresponding natural exponential family

where we have used tθ(θ) = (cid:0)η0
x, γ) be a general family
of densities (not necessarily an exponential family) and let p(γ) be an exponential family prior on its
parameters of the form

x(θ),

−

|

p(x

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

η0
θ , tθ(θ)
(cid:105) −
(cid:104)
η0
x(θ), tx(x)
(cid:104)
(cid:105) −
tθ(θ), (tx(x), 1)

log Zθ(η0

θ )(cid:9) ,
log Zx(η0

{(cid:104)
log Zx(η0

,
(cid:105)}
x(θ))(cid:1) in Eq. (22). Let p(y

x(θ))(cid:9)

p(γ) = exp (cid:8)
(cid:104)

η0
γ, tγ(γ)

(cid:105) −

log Zγ(η0

γ)(cid:9) .

(21)

(22)

For ﬁxed y, consider the mean ﬁeld family of densities q(θ, γ, x) = q(θ)q(γ)q(x) and the mean ﬁeld
variational inference objective

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(23)

By the same argument as in Proposition C.1, without loss of generality we can take the global factor
q(θ) to be in the same exponential family as the prior p(θ), and we denote its natural parameters by
ηθ, writing

{(cid:104)

q(θ) = exp

ηθ, tθ(θ)
We restrict q(γ) to be in the same exponential family as p(γ) with natural parameters ηγ, writing
ηγ, tγ(γ)

log Zγ(ηγ)
}
Finally, we restrict3 q(x) to be in the same exponential family as p(x
θ), writing its natural parameter
as ηx. Using these explicit variational natural parameters, we rewrite the mean ﬁeld variational
inference objective in Eq. (23) as

log Zθ(ηθ)
}

q(γ) = exp

(cid:105) −

(cid:105) −

{(cid:104)

.

.

|

(ηθ, ηγ, ηx) (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(24)

To perform efﬁcient optimization in the objective
deﬁned in Eq. (24), we consider choosing the
variational parameter ηx as a function of the other parameters ηθ and ηγ. One natural choice is to set
, as in Section C. However, ﬁnding a local partial optimizer
ηx to be a local partial optimizer of
may be computationally expensive for general densities p(y
x, γ), and in the large data setting this
expensive optimization would have to be performed for each stochastic gradient update. Instead, we
choose ηx by optimizing over a surrogate objective (cid:98)
, which we design using exponential family
L
structure to be both easy to optimize and to share curvature properties with the mean ﬁeld objective

L

L

|

. The surrogate objective (cid:98)
L

L

is

(ηθ, ηγ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(γ)q(x)

(cid:20)

log

p(θ)p(γ)p(x

(cid:21)

|

ψ(x; y, φ)
θ) exp
}
{
q(θ)q(γ)q(x)
θ) exp
ψ(x; y, φ)
{
q(θ)q(x)

(cid:21)

}

+ const,

|

= E

q(θ)q(x)

(cid:20)

log

p(θ)p(x

(25)

3The parametric form for q(x) need not be restricted a priori, but rather without loss of generality given the
surrogate objective Eq. (25) and the form of ψ used in Eq. (26), the optimal factor q(x) is in the same family as
p(x | θ). We treat it as a restriction here so that we can proceed with more concrete notation.

21

|

where the constant does not depend on ηx. We deﬁne the function ψ(x; y, φ) to have a form related
to the exponential family p(x

θ),

r(y; φ)
{

ψ(x; y, φ) (cid:44)
}φ∈Rm is some class of functions parameterized by φ

r(y; φ), tx(x)
(cid:105)
(cid:104)

where
to be continuously differentiable in φ. We call r(y; φ) the recognition model. We deﬁne η∗
,
be a local partial optimizer of (cid:98)
L
x(ηθ, φ) (cid:44) arg min
η∗

(26)
Rm, which we assume only
x(ηθ, φ) to

∈

,

(ηθ, ηγ, ηx, φ),
(cid:98)
L

ηx

where the notation above should be interpreted as choosing η∗
x(ηθ, φ) to be a local argument of
maximum. The results to follow rely only on necessary ﬁrst-order conditions for unconstrained local
optimality.
Given this choice of function η∗

x(ηθ, φ), we deﬁne the SVAE objective to be
LSVAE(ηθ, ηγ, φ) (cid:44)

(ηθ, ηγ, η∗

x(ηθ, φ)),

L

(27)

where
tion problem to be

L

is the mean ﬁeld variational inference deﬁned in Eq. (24), and we deﬁne the SVAE optimiza-

maxηθ,ηγ ,φLSVAE(ηθ, ηγ, φ).

We summarize these deﬁnitions in the following.
Deﬁnition D.1 (SVAE objective)
Let

denote the mean ﬁeld variational inference objective

L

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

,

(28)

where the densities p(θ), p(γ), and p(x
nential family conjugate prior to p(x
variational factors as

|

θ) are exponential families and p(θ) is the natural expo-
|
θ), as in Eqs. (21)-(22). Given a parameterization of the

q(θ) = exp

ηθ, tθ(θ)

{(cid:104)

log Zθ(ηθ)
}

,
ηx, tx(x)

q(x) = exp

(cid:105) −

q(γ) = exp

{(cid:104)

ηγ, tγ(γ)
,

log Zx(ηx)

{(cid:104)

(cid:105) −

}

log Zγ(ηγ)
}

,

(cid:105) −

(ηθ, ηγ, ηx) denote the mean ﬁeld variational inference objective Eq. (28) as a function of these

let
variational parameters. We deﬁne the SVAE objective as

L

LSVAE(ηθ, ηγ, φ) (cid:44)

L

(ηθ, ηγ, η∗

x(ηθ, φ)),

where η∗

,
x(ηθ, φ) is deﬁned as a local partial optimizer of the surrogate objective (cid:98)
L

x(ηθ, φ) (cid:44) arg max
η∗

(ηθ, η∗
(cid:98)
L

x(ηθ, φ), φ),

ηx

where the surrogate objective (cid:98)
L

is deﬁned as
(cid:20)

(ηθ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(x)

log

ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:104)

,
(cid:105)

p(θ)p(x

|

ψ(x; y, φ)
θ) exp
}
{
q(θ)q(x)

(cid:21)

,

for some recognition model r(y; φ) parameterized by φ

Rm.

∈

LSVAE is a lower-bound for the partially-optimized mean ﬁeld variational

The SVAE objective
inference objective in the following sense.
Proposition D.2 (The SVAE objective lower-bounds the mean ﬁeld objective)
The SVAE objective function
the sense that

LSVAE lower-bounds the partially-optimized mean ﬁeld objective

in

L

max
q(x) L

[ q(θ)q(γ)q(x) ]

max

ηx L

≥

(ηθ, ηγ, ηx)

≥ LSVAE(ηθ, ηγ, φ)

φ

∀

∈

Rm,

22

for any choice of function class
such that

{

r(y; φ)

}φ∈Rm in Eq. (26). Furthermore, if there is some φ∗

∈

Rm

then the bound can be made tight in the sense that

ψ(x; y, φ∗) = E

q(γ) log p(y

x, γ)

|

max
q(x) L

[ q(θ)q(γ)q(x) ] = max

(ηθ, ηγ, ηx) = max

ηx L

φ LSVAE(ηθ, ηγ, φ).

Proof. The inequalities follow from the variational principle and the deﬁnition of the SVAE objective
LSVAE. In particular, by Lemma C.2 the optimal factor over all possible densities is given by
x(θ), tx(x)
(cid:105)

q(γ) log p(y

x, γ)(cid:9) ,

exp (cid:8)

q∗∗(x)

q(θ)η0

E
(cid:104)

+ E

(29)

∝

|

exp

while we restrict the factor q(x) to have a particular exponential family form indexed by parameter ηx,
namely q(x)
LSVAE we also restrict the parameter ηx to be
set to η∗
x(ηθ, φ), a particular function of ηθ and φ, rather than setting it to the value that maximizes
the mean ﬁeld objective
. Finally, equality holds when we can set φ to match the optimal ηx and
L
that choice yields the optimal factor given in Eq. (29).

. In the deﬁnition of

ηx, tx(x)

{(cid:104)

(cid:105)}

∝

Proposition D.2 motivates the SVAE optimization problem: by using gradient-based optimization to
LSVAE(ηθ, ηγ, φ) we are maximizing a lower-bound on the model evidence log p(y) and
maximize
correspondingly minimizing the KL divergence from our variational family to the target posterior.
Furthermore, it motivates choosing the recognition model function class
}φ∈Rm to be as rich
as possible.
As we show in the following, choosing η∗
objective (cid:98)
L
simple expression for an unbiased estimate of the natural gradient (cid:101)
Section D.2. Second, it allows η∗
structure, as we show in Section D.4.

x(ηθ, φ) to be a local partial optimizer of the surrogate
provides two signiﬁcant computational advantages. First, it allows us to provide a
∇ηθ LSVAE, as we describe next in
x(ηθ, φ) to be computed efﬁciently by exploiting exponential family

r(y; φ)
{

D.2 Estimating the natural gradient (cid:101)

∇ηθ LSVAE

x in terms of the surrogate objective (cid:98)
L

The deﬁnition of η∗
enables computationally efﬁcient
ways to estimate natural gradient with respect to the conjugate global variational parameters,
∇ηθ LSVAE(ηθ, ηγ, φ). The next proposition covers the case when the local latent variational factor
(cid:101)
q(x) has no additional factorization structure.
Proposition D.3 (Natural gradient of the SVAE objective)
When there is only one local latent variational factor q(x) (and no further factorization structure),
the natural gradient of the SVAE objective Eq. (27) with respect to the conjugate global variational
parameters ηθ is

(cid:101)

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

θ + E
∇ηxL
where the ﬁrst term is the SVI natural gradient from Corollary C.4, using

q∗(x) [(tx(x), 1)]

(cid:1) + (

ηθ

−

(ηθ, ηγ, η∗

x(ηθ, φ)), 0)

q∗(x) (cid:44) exp

η∗
x(ηθ, φ), tx(x)

log Zx(η∗

x(ηθ, φ))

{(cid:104)

(cid:105) −

,

}

and where a stochastic estimate of the second term is computed as part of the backward pass for the
gradient

(ηθ, ηγ, η∗

x(ηθ, φ)).

∇φL

Proof. First we use the chain rule, analogously to Eq. (5), to write the gradient as

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)

2 log Zθ(ηθ)(cid:1) (cid:0)η0
∇
∇ηθ η∗
x(ηθ, φ)) (
+ (
where the ﬁrst term is the same as the SVI gradient derived in Proposition C.3. In the case of SVI,
the second term is zero because η∗
, but for the SVAE objective
x is chosen as a partial optimizer of
the second term is nonzero in general, and the remainder of this proof amounts to deriving a simple
expression for it.

q∗(x) [ (txy(x, y), 1) ]
(ηθ, ηγ, η∗
x(ηθ, φ))) ,

θ + E

∇ηx L

(30)

ηθ

−

L

(cid:1)

23

We compute the term
using the Implicit Function Theorem given in Corollary A.5, which yields

x(ηθ, φ) in Eq. (30) in terms of the gradients of the surrogate objective (cid:98)
L

∇ηθ η∗

(ηθ, η∗

x(ηθ, φ), φ)

(cid:17)−1

.

(31)

∇ηθ η∗

x(ηθ, φ) =

−∇

First, we compute the gradient of (cid:98)
L
E

(ηθ, ηx, φ) =

∇ηx (cid:98)
L

(cid:16)

(ηθ, η∗

x(ηθ, φ), φ)

2
ηθηx (cid:98)
L
with respect to ηx, writing
p(x

∇

(cid:20)

(cid:20)

2
ηxηx (cid:98)
L

q(θ)q(x)

log

∇ηx

=
= (cid:0)

∇ηx
∇

(cid:2)
E
q(θ)η0
(cid:104)
2 log Zx(ηx)(cid:1) (cid:0)E

x(θ) + r(y; φ)

q(θ)η0

(cid:21)(cid:21)

|

)
}

ψ(x; y, φ)
θ) exp
{
q(x)
ηx,
x(θ) + r(y; φ)

∇

−

log Zx(ηx)
(cid:105)
(cid:1) ,
ηx

−

+ log Zx(ηx)(cid:3)

(32)

When there is only one local latent variational factor q(x) (and no further factorization structure), as
x(ηθ, φ), φ) = 0 and the fact that
a consequence of the ﬁrst-order stationary condition

∇ηx (cid:98)
L
η∗
x(ηθ, φ) = 0,
which is useful in simplifying the expressions to follow.

2 log Zx(ηx) is always positive deﬁnite for minimal exponential families, we have
x(θ) + r(y; φ)

(ηθ, η∗

q(θ)η0

(33)

∇

−

E

Continuing with the calculation of the terms in Eq. (31), we compute
expression in Eq. (32) again, writing

2
ηxηx (cid:98)
L

∇

by differentiating the

2
ηxηx (cid:98)
L

∇

(ηθ, η∗

x(ηθ, φ), φ) =

2 log Zx(η∗
3 log Zx(η∗
2 log Zx(η∗

x(ηθ, φ))
x(ηθ, φ))(cid:1)(cid:0)E
x(ηθ, φ)),

−∇
+ (cid:0)
=

∇

−∇

q(θ)η0

x(θ) + r(y; φ)

x(ηθ, φ)(cid:1)
η∗

−

(34)

where the last line follows from using the ﬁrst-order stationary condition Eq. (33). Next, we compute
by differentiating Eq. (32) with respect to ηθ to yield
the other term
2 log Zx(η∗

(cid:18)

(cid:19)

x(ηθ, φ))

∇

where the latter matrix is

0
x(ηθ, φ)) padded by a row of zeros.
Plugging these expressions back into Eq. (31) and cancelling, we arrive at

∇

∇

x(ηθ, φ), φ) = (cid:0)
2 log Zx(η∗

2 log Zθ(ηθ)(cid:1)

∇

2
ηθηx (cid:98)
L
∇
(ηθ, η∗
2
ηθηx (cid:98)
L

,

∇ηθ η∗
and so we have an expression for the gradient of the SVAE objective as

2 log Zθ(ηθ)

x(ηθ, φ) =

∇

,

(cid:19)

(cid:18)I
0

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)
∇
+ (cid:0)

2 log Zθ(ηθ)(cid:1) (cid:0)η0
2 log Zθ(ηθ)(cid:1) (
When we compute the natural gradient, the Fisher information matrix factors on the left of each term
cancel, yielding the result in the proposition.

q∗(x) [ (txy(x, y), 1) ]
(ηθ, ηγ, η∗

−
x(ηθ, φ)), 0) .

θ + E

∇ηx L

ηθ

∇

(cid:1)

The proof of Proposition D.3 uses the necessary condition for unconstrained local optimality to
simplify the expression in Eq. (34). This simpliﬁcation does not necessarily hold if ηx is constrained;
for example, if the factor q(x) has additional factorization structure, then there are additional (linear)
coordinate subspace constraints on ηx. Note also that when q(x) is a Gaussian family with ﬁxed
covariance (that is, with sufﬁcient statistics tx(x) = x) the same simpliﬁcation always applies
3 log Zx(ηx) = 0.
because third and higher-order cumulants are zero for such families and hence
More generally, when the local latent variables have additional factorization structure, as in the
Gaussian mixture model (GMM) and switching linear dynamical system (SLDS) examples, the
natural gradient with respect to ηθ can be estimated efﬁciently by writing Eq. (30) as
2 log Zθ(ηθ)(cid:1) (cid:0)η0
(cid:1)
[η(cid:48)

θ + E
x(η(cid:48)
(ηθ, ηγ, η∗
where we can recover the second term in Eq. (30) by using the chain rule. We can estimate this second
term directly using the reparameterization trick. Note that to compute the natural gradient estimate in
2 log Zθ(ηθ))−1 to this term because the convenient cancellation from
this case, we need to apply (
Proposition D.3 does not apply. When ηθ is of small dimension compared to ηγ, φ, and even ηx, this
additional computational cost is not large.

q∗(x) [ (txy(x, y), 1) ]
θ, φ))] ,

∇ηθ LSVAE = (cid:0)

θ (cid:55)→ L

ηθ

∇

∇

∇

∇

−

+

24

D.3 Estimating the gradients

∇φLSVAE and

∇ηγ LSVAE

∇φLSVAE(ηθ, ηγ, φ) and
To compute an unbiased stochastic estimate of
∇ηγ LSVAE(ηθ, ηγ, φ) we use the reparameterization trick [7], which is simply to differentiate a
LSVAE(ηθ, ηγ, φ) as a function of φ and ηγ. To isolate the terms
stochastic estimate of the objective
that require this sample-based approximation from those that can be computed directly, we rewrite
the objective as

the gradients

LSVAE(ηθ, ηγ, φ) = E

q(γ)q∗(x) log p(y

x, γ)

|

−

KL(q(θ)q(γ)q∗(x)

p(θ, γ, x))

(35)

where, as before,

q∗(x) (cid:44) exp

η∗
x(ηθ, φ), tx(x)

log Zx(η∗

x(ηθ, φ))

{(cid:104)
and so the dependence of the expression in Eq. (35) on φ is through η∗
Eq. (35) needs to be estimated with the reparameterization trick.

(cid:105) −

x(ηθ, φ). Only the ﬁrst term in

We summarize this procedure in the following proposition.
Proposition D.4 (Estimating
∇ηγ LSVAE)
Let ˆγ(ηγ)
of the gradients

∇φLSVAE and
q∗(x) be samples of q(γ) and q∗(x), respectively. Unbiased estimates
∼
∇φLSVAE(ηθ, ηγ, φ) and

∇ηγ LSVAE(ηθ, ηγ, φ) are given by

q(γ) and ˆx(φ)

∼

∇φLSVAE(ηθ, ηγ, φ)
∇ηγ LSVAE(ηθ, ηγ, φ)

≈ ∇φ log p(y
|
≈ ∇ηγ log p(y

|

ˆx(φ), ˆγ(ηγ))
ˆx(φ), ˆγ(ηγ))

− ∇φ KL(q(θ)q∗(x)
− ∇ηγ KL(q(γ)
(cid:107)

(cid:107)
p(γ)).

p(θ, x)),

Both of these gradients can be computed by automatically differentiating the Monte Carlo estimate of
LSVAE given by

LSVAE(ηθ, ηγ, φ)
with respect to ηγ and φ, respectively.

log p(y

≈

|

ˆx(φ), ˆγ(ηγ))

KL(q(θ)q(γ)q∗(x)

p(θ, γ, x))

−

(cid:107)

(cid:107)

}

D.4 Partially optimizing (cid:98)
L

using conjugacy structure

In Section D.1 we deﬁned the SVAE objective in terms of a function η∗
implicitly deﬁned in terms of ﬁrst-order stationary conditions for an auxiliary objective (cid:98)
L
Here we show how (cid:98)
L
conjugate model of Section C.4.

x(ηθ, φ), which was itself
(ηθ, ηx, φ).
admits efﬁcient local partial optimization in the same way as the conditionally

In this section we consider additional structure in the local latent variables. Speciﬁcally, as in
Section C.4, we introduce to the notation another set of local latent variables z in addition to the
local latent variables x. However, unlike Section C.4, we still consider general likelihood families
p(y

x, γ).

θ) be an exponential family and p(θ) be its corresponding natural exponential family

|
Let p(z, x
|
conjugate prior, writing

p(z, x

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

log Zθ(η0

(cid:105) −

η0
θ , tθ(θ)
(cid:104)
η0
zx(θ), tzx(z, x)
(cid:104)
(cid:105) −
tθ(θ), (tzx(z, x), 1)

θ )(cid:9) ,
log Zzx(η0

zx(θ))(cid:9)

{(cid:104)

(cid:105)}

(36)

where we have used tθ(θ) = (cid:0)η0
zx(θ),
a multilinear polynomial in the statistics tz(z) and tx(x), and let p(z
pair of exponential families, writing

log Zzx(η0

zx(θ))(cid:1) in Eq. (15). Additionally, let tzx(z, x) be
z, θ) be a conjugate
θ) and p(x

−

|

p(z

p(x

|

θ) = exp (cid:8)
η0
z (θ), tz(z)
(cid:104)
|
(cid:105) −
z, θ) = exp (cid:8)
η0
x(z, θ), tx(x)
(cid:104)
= exp (cid:8)
tz(z), η0
(cid:104)

(cid:105) −
x(θ)T(tx(x), 1)

log Zx(η0
(cid:9) .
(cid:105)

log Zz(η0

|
z (θ))(cid:9) ,

x(z, θ))(cid:9)

Let p(y
an exponential family prior on its parameters of the form

x, γ) be a general family of densities (not necessarily an exponential family) and let p(γ) be

|

p(γ) = exp (cid:8)
(cid:104)

η0
γ, tγ(γ)

(cid:105) −

log Zγ(η0

γ)(cid:9) .

25

The corresponding variational factors are

q(θ) = exp
q(z) = exp

ηθ, tθ(θ)
ηz, tz(z)

log Zθ(ηθ)
}
log Zz(ηz)
}

,
,

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

q(γ) = exp
q(x) = exp

ηγ, tγ(γ)
ηx, tx(x)

log Zγ(ηγ)
}
log Zx(ηx)
}

,
.

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

As in Section D.1, we construct the surrogate objective (cid:98)
L
and conjugacy structure. In particular, we construct (cid:98)
L
p(θ)p(γ)p(z

to allow us to exploit exponential family
to resemble the mean ﬁeld objective, namely
z, θ)p(y

x, γ)

(cid:21)

(cid:20)

(ηθ, ηγ, ηz, ηx) (cid:44) E

q(θ)q(γ)q(z)q(x)

log

,

θ)p(x
q(θ)q(γ)q(z)q(x)

|

|

L

but in (cid:98)
L
without much structure, with a more tractable approximation,

we replace the log p(y

|

x, γ) likelihood term, which may be a general family of densities

(ηθ, ηz, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(z)q(x)

(cid:20)

log

p(θ)p(z

|

θ)p(x

z, θ) exp
{
q(θ)q(z)q(x)

|

ψ(x; y, φ)
}

(cid:21)

,

where ψ(x; y, φ) is a function on x that resembles a conjugate likelihood for p(x

z, θ),

|

|

ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:105)

,

(cid:104)

Rm.

φ

∈

We then deﬁne η∗
given ﬁxed values of the
other parameters ηθ and φ, and in particular they satisfy the ﬁrst-order necessary optimality conditions

x(ηθ, φ) to be local partial optimizers of (cid:98)
L

z (ηθ, φ) and η∗

∇ηz (cid:98)
L

The SVAE objective is then

(ηθ, η∗

z (ηθ, φ), η∗

x(ηθ, φ), φ) = 0,

(ηθ, η∗

z (ηθ, φ), η∗

x(ηθ, φ), φ) = 0.

∇ηx (cid:98)
L

L

x(ηθ, φ)).

(ηθ, ηγ, η∗

z (ηθ, φ), η∗

LSVAE(ηθ, ηγ, φ) (cid:44)
The structure of the surrogate objective (cid:98)
is chosen so that it resembles the mean ﬁeld variational
L
inference objective for the conditionally conjugate model of Section C.4, and as a result we can
use the same block coordinate ascent algorithm to efﬁciently ﬁnd partial optimzers η∗
z (ηθ, φ) and
η∗
x(ηθ, φ).
Proposition D.5 (Computing η∗
Let the densities p(θ, γ, z, x, y) and q(θ)q(γ)q(z)q(x) and the objectives
Eqs. (36)-(37). The partial optimizers η∗
(cid:44) arg max
ηz

x, deﬁned by
η∗
x

(ηθ, ηz, ηx, φ),
(cid:98)
L

(ηθ, ηz, ηx, φ)
(cid:98)
L

LSVAE be as in

(cid:44) arg max
ηx

z (ηθ, φ) and η∗

z and η∗

x(ηθ, φ))

, (cid:98)
L

, and

(37)

η∗
z

L

q(θ)η0

z = E
η∗

with the other arguments ﬁxed, are are given by
z (θ) + E

q(θ)q(z)η0
and by alternating the expressions in Eq. (38) as updates we can compute η∗
.
local partial optimizers of (cid:98)
L

x(θ)T(tx(x), 1),

q(θ)q(x)η0

x = E
η∗

x(z, θ) + r(y; φ),
z (ηθ, φ) and η∗

(38)
x(ηθ, φ) as

Proof. These updates follow immediately from Lemma C.2. Note in particular that the stationary
conditions

= 0 yield the each expression in Eq. (38), respectively.

= 0 and

∇ηz (cid:98)
L

∇ηx (cid:98)
L

The other properties developed in Propositions D.2, D.3, and D.4 also hold true for this model because
it is a special case in which we have separated out the local variables, denoted x in earlier sections,
into two groups, denoted z and x here, to match the exponential family structure in p(z
θ) and
p(x
z, θ), and performed unconstrained optimization in each of the variational parameters. However,
the expression for the natural gradient is slightly simpler for this model than the corresponding
version of Proposition D.3.

|

|

E Experiment details and expanded ﬁgures

For the synthetic 1D dot video data, we trained an LDS SVAE on 80 random image sequences each
of length 50, using one sequence per update, and show the model’s future predictions given a preﬁx
of a longer sequence. We used MLP image and recognition models each with one hidden layer of 50
units and a latent state space of dimension 8.

26

(a) Beginning a rear

(b) Grooming

(c) Extension into running

27

(d) Fall from rear

Figure 8: Examples of behavior states inferred from depth video. For each state, four example frame
sequences are shown, including frames during which the given state was most probable according to
the variational distribution on the hidden state sequence. Each frame sequence is padded on both
sides, with a square in the lower-right of a frame depicting that the state was active in that frame. The
frame sequences are temporally subsampled to reduce their length, showing one of every four video
frames. Examples were chosen to have durations close to the median duration for that state.

7
1
0
2
 
l
u
J
 
7
 
 
]
L
M

.
t
a
t
s
[
 
 
5
v
7
7
2
6
0
.
3
0
6
1
:
v
i
X
r
a

Composing graphical models with neural networks
for structured representations and fast inference

Matthew James Johnson
Harvard University
mattjj@seas.harvard.edu

David Duvenaud
Harvard University
dduvenaud@seas.harvard.edu

Alexander B. Wiltschko
Harvard University, Twitter
awiltsch@fas.harvard.edu

Sandeep R. Datta
Harvard Medical School
srdatta@hms.harvard.edu

Ryan P. Adams
Harvard University, Twitter
rpa@seas.harvard.edu

Abstract

We propose a general modeling and inference framework that combines the com-
plementary strengths of probabilistic graphical models and deep learning methods.
Our model family composes latent graphical models with neural network obser-
vation likelihoods. For inference, we use recognition networks to produce local
evidence potentials, then combine them with the model distribution using efﬁcient
message-passing algorithms. All components are trained simultaneously with a
single stochastic variational inference objective. We illustrate this framework by
automatically segmenting and categorizing mouse behavior from raw depth video,
and demonstrate several other example models.

1

Introduction

Modeling often has two goals: ﬁrst, to learn a ﬂexible representation of complex high-dimensional
data, such as images or speech recordings, and second, to ﬁnd structure that is interpretable and
generalizes to new tasks. Probabilistic graphical models [1, 2] provide many tools to build structured
representations, but often make rigid assumptions and may require signiﬁcant feature engineering.
Alternatively, deep learning methods allow ﬂexible data representations to be learned automatically,
but may not directly encode interpretable or tractable probabilistic structure. Here we develop a
general modeling and inference framework that combines these complementary strengths.

Consider learning a generative model for video of a mouse. Learning interpretable representations for
such data, and comparing them as the animal’s genes are edited or its brain chemistry altered, gives
useful behavioral phenotyping tools for neuroscience and for high-throughput drug discovery [3].
Even though each image is encoded by hundreds of pixels, the data lie near a low-dimensional
nonlinear manifold. A useful generative model must not only learn this manifold but also provide
an interpretable representation of the mouse’s behavioral dynamics. A natural representation from
ethology [3] is that the mouse’s behavior is divided into brief, reused actions, such as darts, rears,
and grooming bouts. Therefore an appropriate model might switch between discrete states, with
each state representing the dynamics of a particular action. These two learning tasks — identifying
an image manifold and a structured dynamics model — are complementary: we want to learn the
image manifold in terms of coordinates in which the structured dynamics ﬁt well. A similar challenge
arises in speech [4], where high-dimensional spectrographic data lie near a low-dimensional manifold
because they are generated by a physical system with relatively few degrees of freedom [5] but also
include the discrete latent dynamical structure of phonemes, words, and grammar [6].

To address these challenges, we propose a new framework to design and learn models that couple
nonlinear likelihoods with structured latent variable representations. Our approach uses graphical
models for representing structured probability distributions while enabling fast exact inference
subroutines, and uses ideas from variational autoencoders [7, 8] for learning not only the nonlinear

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

(a) Data

(b) GMM

(c) Density net (VAE)

(d) GMM SVAE

Figure 1: Comparison of generative models ﬁt to spiral cluster data. See Section 2.1.

feature manifold but also bottom-up recognition networks to improve inference. Thus our method
enables the combination of ﬂexible deep learning feature models with structured Bayesian (and
even nonparametric [9]) priors. Our approach yields a single variational inference objective in
which all components of the model are learned simultaneously. Furthermore, we develop a scalable
ﬁtting algorithm that combines several advances in efﬁcient inference, including stochastic variational
inference [10], graphical model message passing [1], and backpropagation with the reparameterization
trick [7]. Thus our algorithm can leverage conjugate exponential family structure where it exists to
efﬁciently compute natural gradients with respect to some variational parameters, enabling effective
second-order optimization [11], while using backpropagation to compute gradients with respect to all
other parameters. We refer to our general approach as the structured variational autoencoder (SVAE).

2 Latent graphical models with neural net observations

In this paper we propose a broad family of models. Here we develop three speciﬁc examples.

2.1 Warped mixtures for arbitrary cluster shapes

One particularly natural structure used frequently in graphical models is the discrete mixture model.
By ﬁtting a discrete mixture model to data, we can discover natural clusters or units. These discrete
structures are difﬁcult to represent directly in neural network models.

Consider the problem of modeling the data y =
ﬁnding the clusters in data is to ﬁt a Gaussian mixture model (GMM) with a conjugate prior:
zn |

yn}
{
π iid
∼
However, the ﬁt GMM does not represent the natural clustering of the data (Fig. 1b). Its inﬂexible
Gaussian observation model limits its ability to parsimoniously ﬁt the data and their natural semantics.

N
n=1 shown in Fig. 1a. A standard approach to

(µk, Σk) iid
∼

(µk, Σk)
}
{

(µzn, Σzn ).

iid
∼ N

NIW(λ),

Dir(α),

yn |

K
k=1

zn,

∼

π

π

Instead of using a GMM, a more ﬂexible alternative would be a neural network density model:

γ

∼

xn

p(γ)

(1)
yn |
where µ(xn; γ) and Σ(xn; γ) depend on xn through some smooth parametric function, such as
multilayer perceptron (MLP), and where p(γ) is a Gaussian prior [12]. This model ﬁts the data
density well (Fig. 1c) but does not explicitly represent discrete mixture components, which might
provide insights into the data or natural units for generalization. See Fig. 2a for a graphical model.

(µ(xn; γ), Σ(xn; γ)),

(0, I),

∼ N

xn, γ iid

iid
∼ N

By composing a latent GMM with nonlinear observations, we can combine the modeling strengths of
both [13], learning both discrete clusters along with non-Gaussian cluster shapes:

π

Dir(α),

∼
xn

iid
∼ N

(µk, Σk) iid
∼

(µ(zn), Σ(zn)),

NIW(λ),

γ

p(γ)

yn |

xn, γ iid

∼ N

∼
(µ(xn; γ), Σ(xn; γ)).

zn |

π iid
∼

π

This combination of ﬂexibility and structure is shown in Fig. 1d. See Fig. 2b for a graphical model.

2.2 Latent linear dynamical systems for modeling video

Now we consider a harder problem: generatively modeling video. Since a video is a sequence of
image frames, a natural place to start is with a model for images. Kingma et al. [7] shows that the

2

(a) Latent Gaussian (b) Latent GMM

(c) Latent LDS

(d) Latent SLDS

Figure 2: Generative graphical models discussed in Section 2.

density network of Eq. (1) can accurately represent a dataset of high-dimensional images
terms of the low-dimensional latent variables

N
n=1 in
N
n=1, each with independent Gaussian distributions.
xn}
To extend this image model into a model for videos, we can introduce dependence through time
N
between the latent Gaussian samples
n=1. For instance, we can make each latent variable xn
depend on the previous latent variable xn−1 through a Gaussian linear dynamical system, writing

xn}
{

yn}
{

{

xn = Axn−1 + Bun,

un

iid
∼ N

(0, I),

A, B

Rm×m,

∈

where the matrices A and B have a conjugate prior. This model has low-dimensional latent states and
dynamics as well as a rich nonlinear generative model of images. In addition, the timescales of the
dynamics are represented directly in the eigenvalue spectrum of A, providing both interpretability
and a natural way to encode prior information. See Fig. 2c for a graphical model.

2.3 Latent switching linear dynamical systems for parsing behavior from video

As a ﬁnal example that combines both time series structure and discrete latent units, consider again
the behavioral phenotyping problem described in Section 1. Drawing on graphical modeling tools,
we can construct a latent switching linear dynamical system (SLDS) [14] to represent the data in
terms of continuous latent states that evolve according to a discrete library of linear dynamics, and
drawing on deep learning methods we can generate video frames with a neural network image model.

}

∈ {

1, 2, . . . , N

there is a discrete-valued latent state zn ∈ {

At each time n
that evolves
according to Markovian dynamics. The discrete state indexes a set of linear dynamical parameters,
Rm evolves according to the corresponding dynamics,
and the continuous-valued latent state xn ∈
zn−1, π
k=1 denotes the Markov transition matrix and πk ∈

where π =
+ is its kth row. We use the
same neural net observation model as in Section 2.2. This SLDS model combines both continuous
and discrete latent variables with rich nonlinear observations. See Fig. 2d for a graphical model.

xn = Azn xn−1 + Bzn un,

zn |
πk}
{

un
RK

iid
∼ N

1, 2, . . . , K

πzn−1,

(0, I),

∼

}

K

3 Structured mean ﬁeld inference and recognition networks

Why aren’t such rich hybrid models used more frequently? The main difﬁculty with combining rich
latent variable structure and ﬂexible likelihoods is inference. The most efﬁcient inference algorithms
used in graphical models, like structured mean ﬁeld and message passing, depend on conjugate
exponential family likelihoods to preserve tractable structure. When the observations are more
general, like neural network models, inference must either fall back to general algorithms that do not
exploit the model structure or else rely on bespoke algorithms developed for one model at a time.

In this section, we review inference ideas from conjugate exponential family probabilistic graphical
models and variational autoencoders, which we combine and generalize in the next section.

3.1

Inference in graphical models with conjugacy structure

Graphical models and exponential families provide many algorithmic tools for efﬁcient inference [15].
Given an exponential family latent variable model, when the observation model is a conjugate
exponential family, the conditional distributions stay in the same exponential families as in the prior
and hence allow for the same efﬁcient inference algorithms.

3

(a) VAE

(b) GMM SVAE

(c) LDS SVAE

(d) SLDS SVAE

Figure 3: Variational families and recognition networks for the VAE [7] and three SVAE examples.

For example, consider learning a Gaussian linear dynamical system model with linear Gaussian
N
observations. The generative model for latent states x =
n=1 is

N
n=1 and observations y =

xn}
{

xn = Axn−1 + Bun,

(0, I),

yn = Cxn + Dvn,

un

iid
∼ N

given parameters θ = (A, B, C, D) with a conjugate prior p(θ). To approximate the poste-
rior p(θ, x

y), consider the mean ﬁeld family q(θ)q(x) and the variational inference objective

yn}
{
(0, I),

vn

iid
∼ N

|

[ q(θ)q(x) ] = E

q(θ)q(x)

L

(cid:20)

log

p(θ)p(x

θ)p(y
q(θ)q(x)

|

|

(cid:21)

x, θ)

,

(2)

y) by
where we can optimize the variational family q(θ)q(x) to approximate the posterior p(θ, x
x, θ) is conjugate to the latent variable
maximizing Eq. (2). Because the observation model p(y
model p(x
[ q(θ)q(x) ] is itself a
Gaussian linear dynamical system with parameters that are simple functions of the expected statistics
of q(θ) and the data y. As a result, for ﬁxed q(θ) we can easily compute q∗(x) and use message
passing algorithms to perform exact inference in it. However, when the observation model is not
conjugate to the latent variable model, these algorithmically exploitable structures break down.

θ), for any ﬁxed q(θ) the optimal factor q∗(x) (cid:44) arg maxq(x) L

|

|

|

3.2 Recognition networks in variational autoencoders

The variational autoencoder (VAE) [7] handles general non-conjugate observation models by intro-
ducing recognition networks. For example, when a Gaussian latent variable model p(x) is paired with
a general nonlinear observation model p(y
y, γ) is non-Gaussian, and it is
difﬁcult to compute an optimal Gaussian approximation. The VAE instead learns to directly output a
suboptimal Gaussian factor q(x
y) by ﬁtting a parametric map from data y to a mean and covariance,
µ(y; φ) and Σ(y; φ), such as an MLP with parameters φ. By optimizing over φ, the VAE effectively
learns how to condition on non-conjugate observations y and produce a good approximating factor.

x, γ), the posterior p(x

|

|

|

4 Structured variational autoencoders

We can combine the tractability of conjugate graphical model inference with the ﬂexibility of
variational autoencoders. The main idea is to use a conditional random ﬁeld (CRF) variational family.
We learn recognition networks that output conjugate graphical model potentials instead of outputting
the complete variational distribution’s parameters directly. These potentials are then used in graphical
model inference algorithms in place of the non-conjugate observation likelihoods.

The SVAE algorithm computes stochastic gradients of a mean ﬁeld variational inference objective.
It can be viewed as a generalization both of the natural gradient SVI algorithm for conditionally
conjugate models [10] and of the AEVB algorithm for variational autoencoders [7]. Intuitively,
it proceeds by sampling a data minibatch, applying the recognition model to compute graphical
model potentials, and using graphical model inference algorithms to compute the variational factor,
combining the evidence from the potentials with the prior structure in the model. This variational
factor is then used to compute gradients of the mean ﬁeld objective. See Fig. 3 for graphical models
of the variational families with recognition networks for the models developed in Section 2.

In this section, we outline the SVAE model class more formally, write the mean ﬁeld variational
inference objective, and show how to efﬁciently compute unbiased stochastic estimates of its gradients.
The resulting algorithm for computing gradients of the mean ﬁeld objective, shown in Algorithm 1, is

4

Algorithm 1 Estimate SVAE lower bound and its gradients

Input: Variational parameters (ηθ, ηγ, φ), data sample y

function SVAEGRADIENTS(ηθ, ηγ, φ, y)

←

←

r(yn; φ)

ψ
(ˆx, ¯tx, KLlocal)
q(γ)
ˆγ
∼
N log p(y
L ←
η0
(cid:101)
∇ηθ L ←
θ −
return lower bound

PGMINFERENCE(ηθ, ψ)

−

ˆx, ˆγ)

N KLlocal
KL(q(θ)q(γ)
−
|
ηθ + N (¯tx, 1) + N (
∇ηx log p(y
∇ηθ L

, natural gradient (cid:101)

L

|

ˆx, ˆγ), 0)
, gradients

p(θ)p(γ))
(cid:107)

∇ηγ ,φL

(cid:46) Get evidence potentials
(cid:46) Combine evidence with prior
(cid:46) Sample observation parameters
(cid:46) Estimate variational bound
(cid:46) Compute natural gradient

function PGMINFERENCE(ηθ, ψ)

q∗(x)
←
return sample ˆx

∼

OPTIMIZELOCALFACTORS(ηθ, ψ)

q∗(x), statistics E

q∗(x)tx(x), divergence E

(cid:46) Fast message-passing inference

q(θ) KL(q∗(x)
(cid:107)

p(x

θ))

|

simple and efﬁcient and can be readily applied to a variety of learning problems and graphical model
structures. See the supplementals for details and proofs.

4.1 SVAE model class

To set up notation for a general SVAE, we ﬁrst deﬁne a conjugate pair of exponential family densities
on global latent variables θ and local latent variables x =
θ) be an exponential
family and let p(θ) be its corresponding natural exponential family conjugate prior, writing
log Zθ(η0

η0
θ , tθ(θ)
(cid:105) −
x(θ))(cid:9) = exp
η0
x(θ), tx(x)
,
where we used exponential family conjugacy to write tθ(θ) = (cid:0)η0
x(θ))(cid:1). The local
latent variables x could have additional structure, like including both discrete and continuous latent
variables or tractable graph structure, but here we keep the notation simple.

p(θ) = exp (cid:8)
(cid:104)
θ) = exp (cid:8)
(cid:104)

tθ(θ), (tx(x), 1)
log Zx(η0

θ )(cid:9) ,
log Zx(η0

N
n=1. Let p(x

{(cid:104)
x(θ),

xn}

p(x

(cid:105) −

(cid:105)}

−

{

|

|

Next, we deﬁne a general likelihood function. Let p(y
x, γ) be a general family of densities and
let p(γ) be an exponential family prior on its parameters. For example, each observation yn may
depend on the latent value xn through an MLP, as in the density network model of Section 2.
This generic non-conjugate observation model provides modeling ﬂexibility, yet the SVAE can still
leverage conjugate exponential family structure in inference, as we show next.

|

4.2 Stochastic variational inference algorithm

Though the general observation model p(y
x, γ) means that conjugate updates and natural gradient
SVI [10] cannot be directly applied, we show that by generalizing the recognition network idea we
can still approximately optimize out the local variational factors leveraging conjugacy structure.

|

For ﬁxed y, consider the mean ﬁeld family q(θ)q(γ)q(x) and the variational inference objective

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(3)

Without loss of generality we can take the global factor q(θ) to be in the same exponential family
as the prior p(θ), and we denote its natural parameters by ηθ. We restrict q(γ) to be in the same
exponential family as p(γ) with natural parameters ηγ. Finally, we restrict q(x) to be in the same
exponential family as p(x
θ), writing its natural parameter as ηx. Using these explicit variational
parameters, we write the mean ﬁeld variational inference objective in Eq. (3) as

(ηθ, ηγ, ηx).

|

(ηθ, ηγ, ηx), we consider choosing the variational
To perform efﬁcient optimization of the objective
L
parameter ηx as a function of the other parameters ηθ and ηγ. One natural choice is to set ηx to be a
. However, without conjugacy structure ﬁnding a local partial optimizer
local partial optimizer of
may be computationally expensive for general densities p(y
x, γ), and in the large data setting this
expensive optimization would have to be performed for each stochastic gradient update. Instead, we
choose ηx by optimizing over a surrogate objective (cid:98)
L
(cid:20)

with conjugacy structure, given by

L

(cid:21)

|

(ηθ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(x)

log

p(θ)p(x

|

θ) exp
{
q(θ)q(x)

ψ(x; y, φ)
}

, ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:105)

,

(cid:104)

L

5

r(y; φ)

}φ∈Rm is some parameterized class of functions that serves as the recognition model.
θ). We

where
{
Note that the potentials ψ(x; y, φ) have a form conjugate to the exponential family p(x
deﬁne η∗
x(ηθ, φ) (cid:44) arg min
η∗

along with the corresponding factor q∗(x),
x(ηθ, φ))

x(ηθ, φ) to be a local partial optimizer of (cid:98)
L
(ηθ, ηx, φ),
(cid:98)
L

η∗
x(ηθ, φ), tx(x)

q∗(x) = exp

log Zx(η∗

(cid:105) −

{(cid:104)

}

|

.

ηx

As with the variational autoencoder of Section 3.2, the resulting variational factor q∗(x) is suboptimal
for the variational objective
. However, because the surrogate objective has the same form as a
variational inference objective for a conjugate observation model, the factor q∗(x) not only is easy to
compute but also inherits exponential family and graphical model structure for tractable inference.

L

(ηθ, ηγ, η∗

x(ηθ, φ), the SVAE objective is

Given this choice of η∗
objective is a lower bound for the variational inference objective Eq. (3) in the following sense.
Proposition 4.1 (The SVAE objective lower-bounds the mean ﬁeld objective)
The SVAE objective function
max
q(x) L

in the sense that
Rm,

LSVAE(ηθ, ηγ, φ) (cid:44)

LSVAE lower-bounds the mean ﬁeld objective
L
≥ LSVAE(ηθ, ηγ, φ)
}φ∈Rm. Furthermore, if there is some φ∗
φ LSVAE(ηθ, ηγ, φ).
(ηθ, ηγ, ηx) = max

∈
x, γ), then the bound can be made tight in the sense that

for any parameterized function class
that ψ(x; y, φ∗) = E

q(γ) log p(y
|
[ q(θ)q(γ)q(x) ] = max

ηx L
r(y; φ)

[ q(θ)q(γ)q(x) ]

(ηθ, ηγ, ηx)

max
q(x) L

ηx L

max

≥

L

∈

φ

∀

{

x(ηθ, φ)). This

Rm such

LSVAE(ηθ, ηγ, φ) we are maximizing a lower
Thus by using gradient-based optimization to maximize
bound on the model log evidence log p(y). In particular, by optimizing over φ we are effectively
learning how to condition on observations so as to best approximate the posterior while maintaining
conjugacy structure. Furthermore, to provide the best lower bound we may choose the recognition
model function class

r(y; φ)

{

}φ∈Rm to be as rich as possible.

Choosing η∗
x(ηθ, φ) to be a local partial optimizer of (cid:98)
provides two computational advantages. First,
L
it allows η∗
x(ηθ, φ) and expectations with respect to q∗(x) to be computed efﬁciently by exploiting
exponential family graphical model structure. Second, it provides computationally efﬁcient ways to
estimate the natural gradient with respect to the latent model parameters, as we summarize next.
Proposition 4.2 (Natural gradient of the SVAE objective)
The natural gradient of the SVAE objective

LSVAE with respect to ηθ can be estimated as
∇

(cid:1) + (
(4)
θ, φ)). When there is only one local variational factor q(x), then we

q∗(x) [(tx(x), 1)]

2 log Zθ(ηθ))

θ + E

F (ηθ),

ηθ

∇

−

−1

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

(cid:101)
where F (η(cid:48)
(ηθ, ηγ, η∗
θ) =
can simplify the estimator to

x(η(cid:48)
∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

L

(cid:101)

θ + E

q∗(x) [(tx(x), 1)]

(cid:1) + (

ηθ

−

∇ηx L

(ηθ, ηγ, η∗

x(ηθ, φ)), 0).

(ηθ, ηγ, η∗

Note that the ﬁrst term in Eq. (4) is the same as the expression for the natural gradient in SVI for
F (ηθ) in the ﬁrst expression or, alternatively,
conjugate models [10], while a stochastic estimate of
x(ηθ, φ)) in the second expression is computed automatically
a stochastic estimate of
∇ηθ L
as part of the backward pass for computing the gradients with respect to the other parameters, as
described next. Thus we have an expression for the natural gradient with respect to the latent
model’s parameters that is almost as simple as the one for conjugate models, differing only by a term
involving the neural network likelihood function. Natural gradients are invariant to smooth invertible
reparameterizations of the variational family [16, 17] and provide effective second-order optimization
updates [18, 11].

∇

The gradients of the objective with respect
to the other variational parameters, namely
∇φLSVAE(ηθ, ηγ, φ), can be computed using the reparameterization trick
∇ηγ LSVAE(ηθ, ηγ, φ) and
and standard automatic differentiation techniques. To isolate the terms that require the reparameteri-
zation trick, we rearrange the objective as
q(γ)q∗(x) log p(y

−
The KL divergence terms are between members of the same tractable exponential families. An
unbiased estimate of the ﬁrst term can be computed by sampling ˆx
q(γ) and
computing

LSVAE(ηθ, ηγ, φ) = E

ˆx, ˆγ) with automatic differentiation.

KL(q(θ)q∗(x)

q∗(x) and ˆγ

KL(q(γ)

p(θ, x))

p(γ)).

x, γ)

−

∼

∼

(cid:107)

(cid:107)

|

∇ηγ ,φ log p(y

|

6

5 Related work

In addition to the papers already referenced, there are several recent papers to which this work is
related.

The two papers closest to this work are Krishnan et al. [19] and Archer et al. [20]. In Krishnan et al.
[19] the authors consider combining variational autoencoders with continuous state-space models,
emphasizing the relationship to linear dynamical systems (also called Kalman ﬁlter models). They
primarily focus on nonlinear dynamics and an RNN-based variational family, as well as allowing
control inputs. However, the approach does not extend to general graphical models or discrete latent
variables. It also does not leverage natural gradients or exact inference subroutines.

In Archer et al. [20] the authors also consider the problem of variational inference in general
continuous state space models but focus on using a structured Gaussian variational family without
considering parameter learning. As with Krishnan et al. [19], this approach does not include discrete
latent variables (or any latent variables other than the continuous states). However, the method they
develop could be used with an SVAE to handle inference with nonlinear dynamics.

In addition, both Gregor et al. [21] and Chung et al. [22] extend the variational autoencoder framework
to sequential models, though they focus on RNNs rather than probabilistic graphical models.

Finally, there is much related work on handling nonconjugate model terms in mean ﬁeld variational
inference. In Khan et al. [23] and Khan et al. [24] the authors present a general scheme that is
able to exploit conjugate exponential family structure while also handling arbitrary nonconjugate
model factors, including the nonconjugate observation models we consider here. In particular, they
propose using a proximal gradient framework and splitting the variational inference objective into a
difﬁcult term to be linearized (with respect to mean parameters) and a tractable concave term, so that
the resulting proximal gradient update is easy to compute, just like in a fully conjugate model. In
Knowles et al. [25], the authors propose performing natural gradient descent with respect to natural
parameters on each of the variational factors in turn, and they focus on approximating expectations of
nonconjugate energy terms in the objective with model-speciﬁc lower-bounds (rather than estimating
them with generic Monte Carlo). As in conjugate SVI [10], they observe that, on conjugate factors
and with an undamped update (i.e. a unit step size), the natural gradient update reduces to the standard
conjugate mean ﬁeld update.

In contrast to the approaches of Khan et al. [23], Khan et al. [24], and Knowles et al. [25], rather
than linearizing intractable terms around the current iterate, in this work we handle intractable terms
via recognition networks and amoritized inference (and the remaining tractable objective terms are
multi-concave in general, analogous to SVI [10]). That is, we use parametric function approximators
to learn to condition on evidence in a conjugate form. We expect these approaches to handling
nonconjugate objective terms may be complementary, and the best choice may be situation-dependent.
For models with local latent variables and datasets where minibatch-based updating is important,
using inference networks to compute local variational parameters in a ﬁxed-depth circuit (as in
the VAE [7, 8]) or optimizing out the local variational factors using fast conjugate updates (as in
conjugate SVI [10]) can be advantageous because in both cases local variational parameters for the
entire dataset need not be maintained across updates. The SVAE we propose here is a way to combine
the inference network and conjugate SVI approaches.

6 Experiments

We apply the SVAE to both synthetic and real data and demonstrate its ability to learn feature
representations and latent structure. Code is available at github.com/mattjj/svae.

6.1 LDS SVAE for modeling synthetic data

Consider a sequence of 1D images representing a dot bouncing from one side of the image to the
other, as shown at the top of Fig. 4. We use an LDS SVAE to ﬁnd a low-dimensional latent state
space representation along with a nonlinear image model. The model is able to represent the image
accurately and to make long-term predictions with uncertainty. See supplementals for details.

7

(a) Predictions after 200 training steps.

(b) Predictions after 1100 training steps.

Figure 4: Predictions from an LDS SVAE ﬁt to 1D dot image data at two stages of training. The
top panel shows an example sequence with time on the horizontal axis. The middle panel shows the
noiseless predictions given data up to the vertical line, while the bottom panel shows the latent states.

(a) Natural (blue) and standard (orange) gradient updates.

(b) Subspace of learned observation model.

Figure 5: Experimental results from LDS SVAE models on synthetic data and real mouse data.

This experiment also demonstrates the optimization advantages that can be provided by the natural
gradient updates. In Fig. 5a we compare natural gradient updates with standard gradient updates at
three different learning rates. The natural gradient algorithm not only learns much faster but also
is less dependent on parameterization details: while the natural gradient update used an untuned
stepsize of 0.1, the standard gradient dynamics at step sizes of both 0.1 and 0.05 resulted in some
matrix parameters to be updated to indeﬁnite values.

6.2 LDS SVAE for modeling video

We also apply an LDS SVAE to model depth video recordings of mouse behavior. We use the dataset
from Wiltschko et al. [3] in which a mouse is recorded from above using a Microsoft Kinect. We
used a subset consisting of 8 recordings, each of a distinct mouse, 20 minutes long at 30 frames per
second, for a total of 288000 video fames downsampled to 30

30 pixels.

We use MLP observation and recognition models with two hidden layers of 200 units each and a 10D
latent space. Fig. 5b shows images corresponding to a regular grid on a random 2D subspace of the
latent space, illustrating that the learned image manifold accurately captures smooth variation in the
mouse’s body pose. Fig. 6 shows predictions from the model paired with real data.

×

6.3 SLDS SVAE for parsing behavior

Finally, because the LDS SVAE can accurately represent the depth video over short timescales, we
apply the latent switching linear dynamical system (SLDS) model to discover the natural units of
behavior. Fig. 7 and Fig. 8 in the appendix show some of the discrete states that arise from ﬁtting an
SLDS SVAE with 30 discrete states to the depth video data. The discrete states that emerge show a
natural clustering of short-timescale patterns into behavioral units. See the supplementals for more.

8

Figure 6: Predictions from an LDS SVAE ﬁt to depth video. In each panel, the top is a sampled
prediction and the bottom is real data. The model is conditioned on observations to the left of the line.

(a) Extension into running

(b) Fall from rear

Figure 7: Examples of behavior states inferred from depth video. Each frame sequence is padded on
both sides, with a square in the lower-right of a frame depicting when the state is the most probable.

7 Conclusion

Structured variational autoencoders provide a general framework that combines some of the strengths
of probabilistic graphical models and deep learning methods. In particular, they use graphical models
both to give models rich latent representations and to enable fast variational inference with CRF-like
structured approximating distributions. To complement these structured representations, SVAEs use
neural networks to produce not only ﬂexible nonlinear observation models but also fast recognition
networks that map observations to conjugate graphical model potentials.

9

References

MIT Press, 2009.

[1] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.

[2] Kevin P Murphy. Machine Learning: a Probabilistic Perspective. MIT Press, 2012.
[3] Alexander B. Wiltschko, Matthew J. Johnson, Giuliano Iurilli, Ralph E. Peterson, Jesse M.
Katon, Stan L. Pashkovski, Victoria E. Abraira, Ryan P. Adams, and Sandeep Robert Datta.
“Mapping Sub-Second Structure in Mouse Behavior”. In: Neuron 88.6 (2015), pp. 1121–1135.
[4] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. “Deep neural
networks for acoustic modeling in speech recognition: The shared views of four research
groups”. In: Signal Processing Magazine, IEEE 29.6 (2012), pp. 82–97.

[5] Li Deng. “Computational models for speech production”. In: Computational Models of Speech

Pattern Processing. Springer, 1999, pp. 199–213.

[6] Li Deng. “Switching dynamic system models for speech articulation and acoustics”. In:
Mathematical Foundations of Speech and Language Processing. Springer, 2004, pp. 115–133.
[7] Diederik P. Kingma and Max Welling. “Auto-Encoding Variational Bayes”. In: International

Conference on Learning Representations (2014).

[8] Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. “Stochastic Backpropagation and
Approximate Inference in Deep Generative Models”. In: Proceedings of the 31st International
Conference on Machine Learning. 2014, pp. 1278–1286.

[9] Matthew J. Johnson and Alan S. Willsky. “Stochastic Variational Inference for Bayesian Time

Series Models”. In: International Conference on Machine Learning. 2014.

[10] Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. “Stochastic variational

[11]

inference”. In: Journal of Machine Learning Research (2013).
James Martens. “New insights and perspectives on the natural gradient method”. In: arXiv
preprint arXiv:1412.1193 (2015).

[12] David J.C. MacKay and Mark N. Gibbs. “Density networks”. In: Statistics and neural networks:

advances at the interface. Oxford University Press, Oxford (1999), pp. 129–144.

[13] Tomoharu Iwata, David Duvenaud, and Zoubin Ghahramani. “Warped Mixtures for Nonpara-
metric Cluster Shapes”. In: Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2013,
pp. 311–319.

[14] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. “Bayesian Nonparametric Inference
of Switching Dynamic Linear Models”. In: IEEE Transactions on Signal Processing 59.4
(2011).

[15] Martin J. Wainwright and Michael I. Jordan. “Graphical Models, Exponential Families, and

Variational Inference”. In: Foundations and Trends in Machine Learning (2008).

[16] Shun-Ichi Amari. “Natural gradient works efﬁciently in learning”. In: Neural computation

10.2 (1998), pp. 251–276.

[17] Shun-ichi Amari and Hiroshi Nagaoka. Methods of Information Geometry. American Mathe-

[18]

matical Society, 2007.
James Martens and Roger Grosse. “Optimizing Neural Networks with Kronecker-factored
Approximate Curvature”. In: arXiv preprint arXiv:1503.05671 (2015).

[19] Rahul G Krishnan, Uri Shalit, and David Sontag. “Deep Kalman Filters”. In: arXiv preprint

arXiv:1511.05121 (2015).

[20] Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. “Black
box variational inference for state space models”. In: arXiv preprint arXiv:1511.07367 (2015).
[21] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. “DRAW: A recurrent neural

[22]

network for image generation”. In: arXiv preprint arXiv:1502.04623 (2015).
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua
Bengio. “A recurrent latent variable model for sequential data”. In: Advances in Neural
information processing systems. 2015, pp. 2962–2970.

[23] Mohammad E Khan, Pierre Baqué, François Fleuret, and Pascal Fua. “Kullback-Leibler
proximal variational inference”. In: Advances in Neural Information Processing Systems. 2015,
pp. 3402–3410.

10

[24] Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, and Masashi Sugiyama.
“Faster Stochastic Variational Inference using Proximal-Gradient Methods with General Diver-
gence Functions”. In: Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2016.
[25] David A Knowles and Tom Minka. “Non-conjugate variational message passing for multino-
mial and binary regression”. In: Advances in Neural Information Processing Systems. 2011,
pp. 1701–1709.

[26] Dimitri P Bertsekas. Nonlinear programming. 2nd ed. Athena Scientiﬁc, 1999.
[27]

John M. Danskin. The theory of max-min and its application to weapons allocation problems.
Springer-Verlag, New York, 1967.

[28] Anthony-V Fiacco. Introduction to sensitivity and stability analysis in nonlinear programming.

[29]

[30]

Academic Press, Inc., 1984.
J Frederic Bonnans and Alexander Shapiro. Perturbation Analysis of Optimization Problems.
Springer Science & Business Media, 2000.
James Martens and Roger Grosse. “Optimizing Neural Networks with Kronecker-factored
Approximate Curvature”. In: Proceedings of the 32nd International Conference on Machine
Learning. 2015.

[31] David Duvenaud and Ryan P. Adams. “Black-box stochastic variational inference in ﬁve lines

of Python”. In: NIPS Workshop on Black-box Learning and Inference (2015).

11

In this section we ﬁx our notation for gradients and establish some basic deﬁnitions and results that
we use in the sequel.

A Optimization

A.1 Gradient notation

We follow the notation in Bertsekas [26, A.5]. In particular, if f : Rn
differentiable function, we deﬁne the gradient matrix of f , denoted
which the ith column is the gradient
That is,

Rm is a continuously
m matrix in
∇
fi(x) of fi, the ith coordinate function of f , for i = 1, 2, . . . , m.

f (x), to be the n

→

∇

×

f (x) = [

f1(x)

∇

∇

· · · ∇

fm(x)] .

∇

f is the Jacobian matrix of f , in which the ijth entry is the function ∂fi/∂xj.
R is continuously differentiable with continuously differentiable partial derivatives,
2f , to be the matrix in which the ijth entry is the

The transpose of
If f : Rn
→
then we deﬁne the Hessian matrix of f , denoted
function ∂2f /∂xi∂xj.
Rm
Finally, if f : Rn

R is a function of (x, y) with x

Rn and y

Rm, we write

∇

×

→

∈

∇xf (x, y) =

∇yf (x, y) =







∂f (x,y)
∂x1

...

∂f (x,y)
∂xm
(cid:18) ∂2f (x, y)
∂xi∂xj







,

(cid:19)

,

2
xyf (x, y) =

∇

2
yyf (x, y) =

∇

(cid:18) ∂2f (x, y)
∂xi∂yj

(cid:19)

.

2
xxf (x, y) =

∇

∈
∂f (x,y)
∂y1

...













∂f (x,y)
∂yn
(cid:18) ∂2f (x, y)
∂yi∂yj

(cid:19)

,

A.2 Local and partial optimizers

In this section we state the deﬁnitions of local partial optimizer and necessary conditions for optimality
that we use in the sequel.

Deﬁnition A.1 (Partial optimizer, local partial optimizer)
Let f : Rn
y∗

Rm an unconstrained partial optimizer of f given x if

R be an objective function to be maximized. For a ﬁxed x

Rm

→

×

∈

Rn, we call a point

∈

and we call y∗ an unconstrained local partial optimizer of f given x if there exists an (cid:15) > 0 such that

f (x, y)

f (x, y∗)

≤

Rm

y

∀

∈

f (x, y)

f (x, y∗)

≤

y with

∀

y

(cid:107)

−

y∗
(cid:107)

< (cid:15),

where

is any vector norm.

(cid:107) · (cid:107)

×

→

Rm

R be continuously differentiable. For ﬁxed x

Proposition A.2 (Necessary conditions for optimality, Prop. 3.1.1 of Bertsekas [26])
Let f : Rn
Rn if y∗
unconstrained local partial optimizer for f given x then
∇yf (x, y∗) = 0.
If instead x and y are subject to the constraints h(x, y) = 0 for some continuously differentiable
h : Rn
Rm and y∗ is a constrained local partial optimizer for f given x with the regularity
Rm
×
condition that

→
∇yh(x, y∗) is full rank, then there exists a Lagrange multiplier λ∗

Rm such that

Rm is an

∈

∈

∈

∇yf (x, y∗) +

∇yh(x, y∗)λ∗ = 0,

and hence the cost gradient
by the null space of

∇yh(x, y∗)T.

∇yf (x, y∗) is orthogonal to the ﬁrst-order feasible variations in y given

12

Note that the regularity condition on the constraints is not needed if the constraints are linear [26,
Prop. 3.3.7].
For a continuously differentiable function f : Rn

R, we say x∗ is a stationary point of f if
f (x∗) = 0. For general unconstrained smooth optimization, the limit points of gradient-based
∇
algorithms are guaranteed only to be stationary points of the objective, not necessarily local optima.
Block coordinate ascent methods, when available, provide slightly stronger guarantees: not only is
every limit point a stationary point of the objective, in addition each coordinate block is a partial
optimizer of the objective. Note that the objective functions we consider maximizing in the following
are bounded above.

→

A.3 Partial optimization and the Implicit Function Theorem

Let f : Rn
x
∈
y∗(x)

×
Rn and y

∈

Rm

R be a scalar-valued objective function of two unconstrained arguments
Rn a value
Rm be some function that assigns to each x

Rm, and let y∗ : Rn

→

Rm. Deﬁne the composite function g : Rn

→

∈

R as

∈

→

g(x) (cid:44) f (x, y∗(x))

and using the chain rule write its gradient as

g(x) =

∇

∇xf (x, y∗(x)) +

∇

y∗(x)

∇yf (x, y∗(x)).

(5)

∈

One choice of the function y∗(x) is to partially optimize f for any ﬁxed value of x. For example,
Rn, we could choose y∗ to satisfy
assuming that arg maxy f (x, y) is nonempty for every x
y∗(x)
arg maxy f (x, y), so that g(x) = maxy f (x, y).1 Similarly, if y∗(x) is chosen so that
∇yf (x, y∗(x)) = 0, which is satisﬁed when y∗(x) is an unconstrained local partial optimizer for f
given x, then the expression in Eq. (5) can be simpliﬁed as in the following proposition.
Proposition A.3 (Gradients of locally partially optimized objectives)
Let f : Rn
such that y∗(x) is differentiable, and deﬁne g(x) = f (x, y∗(x)). Then

R be continuously differentiable, let y∗ be a local partial optimizer of f given x

Rm

→

×

∈

g(x) =

∇

∇xf (x, y∗(x)).

Proof. If y∗ is an unconstrained local partial optimizer of f given x then it satisﬁes
and if y∗ is a regularly-constrained local partial optimizer then the feasible variation
orthogonal to the cost gradient
in Eq. (5) is zero.

∇yf (x, y∗) = 0,
y∗(x) is
∇
∇yf (x, y∗). In both cases the second term in the expression for
g(x)

∇

∇

In general, when y∗(x) is not a stationary point of f (x,
g(x) we
), to evaluate the gradient
·
y∗(x) in Eq. (5). However, this term may be difﬁcult to compute directly. The
need to evaluate
function y∗(x) may arise implicitly from some system of equations of the form h(x, y) = 0 for
Rm. For example, the value of y may
some continuously differentiable function h : Rn
→
be computed from x and h using a black-box iterative numerical algorithm. However, the Implicit
y∗(x) using only the derivatives of h and
Function Theorem provides another means to compute
the value of y∗(x).
Proposition A.4 (Implicit Function Theorem, Prop. A.25 of Bertsekas [26])
Let h : Rn

Rm be a function and ¯x

Rm be points such that

Rn and ¯y

Rm

Rm

∇

∇

×

∈

∈

×

→
1. h(¯x, ¯y) = 0

2. h is continuous and has a continuous nonsingular gradient matrix

∇yh(x, y) in an open set

containing (¯x, ¯y).

Then there exist open sets S¯x ⊆
function y∗ : S¯x →

S¯y such that ¯y = y∗(x) and h(x, y∗(x)) = 0 for all x

Rm containing ¯x and ¯y, respectively, and a continuous
S¯x. The function y∗ is

Rn and S¯y ⊆

∈

1For a discussion of differentiability issues when there is more than one optimizer, i.e. when arg maxy f (x, y)
has more than one element, see Danskin [27], Fiacco [28, Section 2.4], and Bonnans et al. [29, Chapter 4]. Here
we only consider the sensitivity of local stationary points and assume differentiability almost everywhere.

13

unique in the sense that if x
some p > 0, h is p times continuously differentiable, the same is true for y∗, and we have

S¯y, and h(x, y) = 0, then y = y∗(x). Furthermore, if for

S¯x, y

∈

∈

y∗(x) =

∇

−∇xh (x, y∗(x)) (

∇yh (x, y∗(x)))−1 ,

x

∀

∈

S¯x.

As a special case, the equations h(x, y) = 0 may be the ﬁrst-order stationary conditions of another
unconstrained optimization problem. That is, the value of y may be chosen by locally partially
optimizing the value of u(x, y) for a function u : Rn
R with no constraints on y, leading to
the following corollary.
Corollary A.5 (Implicit Function Theorem for optimization subroutines)
Let u : Rn
∇yu
×
satisﬁes the hypotheses of Proposition A.4 at some point (¯x, ¯y), and deﬁne y∗ as in Proposition A.4.
Then we have

R be a twice continuously differentiable function such that the choice h =

Rm

Rm

→

→

×

y∗(x) =

∇

−∇

2

xyu (x, y∗(x)) (cid:0)

2

yyu (x, y∗(x))(cid:1)−1

,

∇

x

∀

∈

S¯x.

B Exponential families

In this section we set up notation for exponential families and outline some basic results. Throughout
this section we take all densities to be absolutely continuous with respect to the appropriate Lebesgue
measure (when the underlying set
is discrete),
X
) (generated by Euclidean and discrete topologies,
and denote the Borel σ-algebra of a set
respectively). We assume measurability of all functions as necessary.

is Euclidean space) or counting measure (when

as

X

X

X

B

(

Given a statistic function tx :
of probability densities on

Rn and a base measure νX , we can deﬁne an exponential family

X →
relative to νX and indexed by natural parameter ηx ∈
,
(cid:105)}
|

ηx, tx(x)
is the standard inner product on Rn. We also deﬁne the partition function as

ηx ∈
∀

X
p(x

Rn,

ηx)

exp

{(cid:104)

∝

Rn by

where

,
(cid:104)·

·(cid:105)

(cid:90)

Zx(ηx) (cid:44)

exp

ηx, tx(x)

νX (dx)

{(cid:104)

(cid:105)}

and deﬁne H

Rn to be the set of all normalizable natural parameters,

⊆

H (cid:44)

η
{

∈

Rn : Zx(η) <

.

∞}

We can write the normalized probability density as

p(x

η) = exp

ηx, tx(x)

log Zx(ηx)

.

|

{(cid:104)
We say that an exponential family is regular if H is open, and minimal if there is no η
0
}
= 0 (νX -a.e.). We assume all families are regular and minimal.2 Finally, when
such that
we parameterize the family with some other coordinates θ, we write the natural parameter as a
continuous function ηx(θ) and write the density as

η, tx(x)
(cid:105)

(cid:105) −

\ {

Rn

∈

}

(cid:104)

(6)

p(x

θ) = exp

ηx(θ), tx(x)

log Zx(ηx(θ))

(cid:105) −
x (H) to be the open set of parameters that correspond to normalizable densities.

{(cid:104)

}

|

and take Θ = η−1
We summarize this notation in the following deﬁnition.
Deﬁnition B.1 (Exponential family of densities)
Given a measure space (
function ηx : Θ

(
X

X

B

,

), νX ), a statistic function tx :

→

Rn, the corresponding exponential family of densities relative to νX is

X →

Rn, and a natural parameter

where

is the log partition function.

p(x

θ) = exp

ηx(θ), tx(x)

log Zx(ηx(θ))

,

|

{(cid:104)

(cid:105) −

}

log Zx(ηx) (cid:44) log

(cid:90)

exp

ηx, tx(x)

νX (dx)

{(cid:104)

(cid:105)}

2Families that are not minimal, like the density of the categorical distribution, can be treated by restricting all
algebraic operations to the subspace spanned by the statistic, i.e. to the smallest V ⊂ Rn with range tx ⊆ V .

14

When we write exponential families of densities for different random variables, we change the
subscripts on the statistic function, natural parameter function, and log partition function to correspond
to the symbol used for the random variable. When the corresponding random variable is clear from
context, we drop the subscripts to simplify notation.

The next proposition shows that the log partition function of an exponential family generates cumu-
lants of the statistic.
Proposition B.2 (Gradients of log Z and expected statistics)
The gradient of the log partition function of an exponential family gives the expected sufﬁcient
statistic,

log Z(η) = E

p(x | η) [t(x)] ,

∇

where the expectation is over the random variable x with density p(x
generating function of t(x) can be written

|

η). More generally, the moment

Mt(x)(s) (cid:44) E

p(x | η)

(cid:104)

e(cid:104)s,t(x)(cid:105)(cid:105)

= elog Z(η+s)−log Z(η)

and so derivatives of log Z give cumulants of t(x), where the ﬁrst cumulant is the mean and the
second and third cumulants are the second and third central moments, respectively.

Given an exponential family of densities on
as in Deﬁnition B.1, we can deﬁne a related exponential
family of densities on Θ by deﬁning a statistic function tθ(θ) in terms of the functions ηx(θ) and
log Zx(ηx(θ)).
Deﬁnition B.3 (Natural exponential family conjugate prior)
Given the exponential family p(x
as the concatenation

θ) of Deﬁnition B.1, deﬁne the statistic function tθ : Θ

Rn+1

→

X

|

−
where the ﬁrst n coordinates of tθ(θ) are given by ηx(θ) and the last coordinate is given by
log Zx(ηx(θ)). We call the exponential family with statistic tθ(θ) the natural exponential family

tθ(θ) (cid:44) (ηx(θ),

log Zx(ηx(θ))) ,

−
conjugate prior to the density p(x

θ) and write

|

p(θ) = exp

ηθ, tθ(θ)

{(cid:104)

log Zθ(ηθ)
}

(cid:105) −

Rn+1 and the density is taken relative to some measure νΘ on (Θ,

where ηθ ∈
Notice that using tθ(θ) we can rewrite the original density p(x

θ) as

B

(Θ)).

|

p(x

log Zx(ηx(θ))

θ) = exp
= exp

ηx(θ), tx(x)
(cid:105) −
tθ(θ), (tx(x), 1)

|

{(cid:104)
{(cid:104)
This relationship is useful in Bayesian inference: when the exponential family p(x
θ) is a likelihood
function and the family p(θ) is used as a prior, the pair enjoy a convenient conjugacy property, as
summarized in the next proposition.
Proposition B.4 (Conjugacy)
Let the densities p(x
relations

θ) and p(θ) be deﬁned as in Deﬁnitions B.1 and B.3, respectively. We have the

(cid:105)}

}

|

|

.

p(θ, x) = exp
x) = exp
p(θ

|
and hence in particular the posterior p(θ
parameter ηθ + (tx(x), 1). Similarly, with multiple likelihood terms p(xi |
have

ηθ + (tx(x), 1), tθ(θ)
ηθ + (tx(x), 1), tθ(θ)

log Zθ(ηθ)
}
log Zθ(ηθ + (tx(x), 1))
}
x) is in the same exponential family as p(θ) with the natural
θ) for i = 1, 2, . . . , N we

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

(7)

|

N
(cid:89)

i=1

(cid:40)

N
(cid:88)

i=1

p(θ)

p(xi |

θ) = exp

ηθ +
(cid:104)

(tx(xi), 1), tθ(θ)

log Zθ(ηθ)

.

(8)

(cid:105) −

(cid:41)

Finally, we give a few more exponential family properties that are useful for gradient-based optimiza-
tion algorithms and variational inference. In particular, we note that the Fisher information matrix of
an exponential family can be computed as the Hessian matrix of its log partition function, and that
the KL divergence between two members of the same exponential family has a simple expression.

15

Deﬁnition B.5 (Score vector and Fisher information matrix)
Given a family of densities p(x
of the log density with respect to the parameter,
v(x, θ) (cid:44)
and the Fisher information matrix for the parameter θ is the covariance of the score,

∇θ log p(x

θ),

|

|

θ) indexed by a parameter θ, the score vector v(x, θ) is the gradient

I(θ) (cid:44) E (cid:2)v(x, θ)v(x, θ)T(cid:3) ,

where the expectation is taken over the random variable x with density p(x
used the identity E[v(x, θ)] = 0.
Proposition B.6 (Score and Fisher information for exponential families)
Given an exponential family of densities p(x
the score with respect to the natural parameter is given by
η) = t(x)

|
∇η log p(x
and the Fisher information matrix is given by
I(η) =

2 log Z(η).

v(x, η) =

log Z(η)

− ∇

|

|

∇

θ), and where we have

η) indexed by the natural parameter η, as in Eq. (6),

Proposition B.7 (KL divergence in an exponential family)
Given an exponential family of densities p(x
η) indexed by the natural parameter η, as in Eq. (6),
and two particular members with natural parameters η1 and η2, respectively, the KL divergence from
one to the other is

|

KL(p(x

η1)

p(x

η2)) (cid:44) E

p(x | η1)

|

(cid:107)

|

(cid:20)

log

(cid:21)

p(x
p(x

η1)
|
η2)
|
log Z(η1)

=

η1 −

(cid:104)

η2,

∇

(log Z(η1)

log Z(η2)).

(cid:105) −

−

C Natural gradient SVI for exponential families

In this section we give a derivation of the natural gradient stochastic variational inference (SVI)
method of Hoffman et al. [10] using our notation. We extend the algorithm in Section D.

C.1 SVI objective

θ) be an exponential family and p(θ) be its corresponding natural exponential family

|

Let p(x, y
prior as in Deﬁnitions B.1 and B.3, writing
p(θ) = exp (cid:8)
η0
θ , tθ(θ)
(cid:104)
θ) = exp (cid:8)
η0
xy(θ), txy(x, y)
(cid:104)
(cid:105) −
tθ(θ), (txy(x, y), 1)
= exp
where we have used tθ(θ) = (cid:0)η0
log Zxy(η0
xy(θ),
Given a ﬁxed observation y, for any density q(θ, x) = q(θ)q(x) we have
(cid:20)

(cid:105)}
xy(θ))(cid:1) in Eq. (9).

θ )(cid:9)
log Zxy(η0

log Zθ(η0

p(x, y

(cid:105) −

{(cid:104)

−

(cid:21)

|

xy(θ))(cid:9)

log p(y) = E

q(θ)q(x)

log

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)
p(θ)p(x, y
|
q(θ)q(x)

θ)

(cid:21)

θ)

E

≥

q(θ)q(x)

+ KL(q(θ)q(x)

p(θ, x

y))

(cid:107)

|

where we have used the fact that the KL divergence is always nonnegative. Therefore to choose
q(θ)q(x) to minimize the KL divergence to the posterior p(θ, x
y) we deﬁne the mean ﬁeld varia-
tional inference objective as

|

[ q(θ)q(x) ] (cid:44) E

q(θ)q(x)

L

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)

(cid:21)

θ)

and the mean ﬁeld variational inference problem as
maxq(θ)q(x)L

[ q(θ)q(x) ] .

(9)

(10)

(11)

The following proposition shows that because of the exponential family conjugacy structure, we
can ﬁx the parameterization of q(θ) and still optimize over all possible densities without loss of
generality.

16

Proposition C.1 (Optimal form of the global variational factor)
Given the mean ﬁeld optimization problem Eq. (11), for any ﬁxed q(x) the optimal factor q(θ) is
detetermined (νΘ-a.e.) by

θ + E
η0
(cid:104)
In particular, the optimal q(θ) is in the same exponential family as the prior p(θ).

q(x) [ (txy(x, y), 1) ] , tθ(θ)
(cid:105)

q(θ)

∝

exp (cid:8)

(cid:9) .

This proposition follows immediately from a more general lemma, which we reuse in the sequel.
Lemma C.2 (Optimizing a mean ﬁeld factor)
Let p(a, b, c) be a joint density and let q(a), q(b), and q(c) be mean ﬁeld factors. Consider the mean
ﬁeld variational inference objective

E

q(a)q(b)q(c)

(cid:20)

log

p(a, b, c)
q(a)q(b)q(c)

(cid:21)

.

For ﬁxed q(a) and q(c), the partially optimal factor q∗(b) over all possible densities,

q∗(b) (cid:44) arg max

E

q(a)q(b)q(c)

q(b)

(cid:20)

log

p(a, b, c)
q(a)q(b)q(c)

(cid:21)

,

(12)

is deﬁned (almost everywhere) by

In particular, if p(c
|
prior, and log p(b, c

|

q∗(b)

exp (cid:8)E

q(a)q(c) log p(a, b, c)(cid:9) .

∝
b, a) is an exponential family with p(b
|
a) is a multilinear polynomial in the statistics tb(b) and tc(c), written

a) its natural exponential family conjugate

log Zb(η0

b (a))(cid:9) ,

|

p(b

p(c

a) = exp (cid:8)
(cid:104)
|
b, a) = exp (cid:8)
(cid:104)
= exp (cid:8)
(cid:104)
c (a), then the optimal factor can be written
η∗
q(a)η0
b , tb(b)

η0
b (a), tb(b)
(cid:105) −
η0
c (b, a), tc(c)
tb(b), η0

(cid:105) −
c (a)T(tc(c), 1)

log Zb(η∗
b )

(cid:44) E

η∗
b

,

log Zc(η0
(cid:9) ,
(cid:105)

c (b, a))(cid:9)

b (a) + E

q(a)q(c)η0

{(cid:104)

(cid:105) −

As a special case, when c is conditionally independent of b given a, so that p(c
b (a) + E

tb(b), (tc(c), 1)

b) = exp

q(a)η0

(cid:44) E

p(c

η∗
b

,

|

{(cid:104)

}

(cid:105)}

for some matrix η0
q∗(b) = exp

c (a)T(tc(c), 1).
b, a) = p(c
q(c)(tc(c), 1).

|

|

b), then

Proof. Rewrite the objective in Eq. (12), dropping terms that are constant with respect to q(b), as

E

q(a)q(b)q(c)

(cid:20)

log

(cid:21)

p(a, b, c)
q(b)

= E

q(b)

(cid:2)E

q(a)q(c) log p(a, b, c)

log q(b)(cid:3)

−

q(a)q(c) log p(a, b, c)

log q(b)(cid:3)

−

= E

q(b)

(cid:2)log exp E
(cid:21)
(cid:20) q(b)
(cid:101)p(b)

E

q(b)

−

=

=

KL(q(b)

+ const

−
where we have deﬁned a new density (cid:101)p(b)
objective by setting the KL divergence to zero, choosing q(b)
rest follows from plugging in the exponential family densities.

exp (cid:8)E

∝

∝

(cid:107) (cid:101)p(b)) + const,
q(a)q(c) log p(a, b, c)(cid:9). We can maximize the
q(a)q(c) log p(a, b, c)(cid:9). The

exp (cid:8)E

Proposition C.1 justiﬁes parameterizing the density q(θ) with variational natural parameters ηθ as
ηθ, tθ(θ)
where the statistic function tθ and the log partition function log Zθ are the same as in the prior
family p(θ). Using this parameterization, we can deﬁne the mean ﬁeld objective as a function of the
parameters ηθ, partially optimizing over q(x),

log Zθ(ηθ)
}

q(θ) = exp

(cid:105) −

{(cid:104)

(ηθ) (cid:44) max
q(x)

E

L

q(θ)q(x)

(cid:20)

log

p(θ)p(x, y
|
q(θ)q(x)

(cid:21)

θ)

.

(13)

The partial optimization over q(x) in Eq. (13) should be read as choosing q(x) to be a local partial
optimizer of Eq. (10); in general, it may be intractable to ﬁnd a global partial optimizer, and the results
that follow use only ﬁrst-order stationary conditions on q(x). We refer to this objective function,
where we locally partially optimize the mean ﬁeld objective Eq. (10) over q(x), as the SVI objective.

17

C.2 Easy natural gradients of the SVI objective

By again leveraging the conjugate exponential family structure, we can write a simple expression for
the gradient of the SVI objective, and even for its natural gradient.
Proposition C.3 (Gradient of the SVI objective)
Let the SVI objective

(ηθ) be deﬁned as in Eq. (13). Then the gradient
L
(ηθ) = (cid:0)

∇L
q∗(x) [ (txy(x, y), 1) ]

2 log Zθ(ηθ)(cid:1) (cid:0)η0

θ + E

(ηθ) is
(cid:1)

ηθ

∇L

∇

where q∗(x) is a local partial optimizer of the mean ﬁeld objective Eq. (10) for ﬁxed global variational
parameters ηθ.

−

Proof. First, note that because q∗(x) is a local partial optimizer for Eq. (10) by Proposition A.3, we
have

(ηθ) =

E

∇ηθ

∇L

q(θ)q∗(x)

(cid:20)

log

p(θ)p(x, y

|
q(θ)q∗(x)

(cid:21)

θ)

.

Next, we use the conjugate exponential family structure and Proposition B.4, Eq. (7), to expand

E

q(θ)q∗(x)

(cid:20)

log

(cid:21)

θ)

p(θ)p(x, y

|
q(θ)q∗(x)

=

θ + E
η0
(cid:104)

−
Note that we can use Proposition B.2 to replace E
respect to ηθ and using the product rule, we have
2 log Zθ(ηθ) (cid:0)η0

(ηθ) =

∇L

∇

θ + E

q∗(x)(txy(x, y), 1)

ηθ, E

q(θ)[tθ(θ)]
(cid:105)

−
log Zθ(ηθ)(cid:1) .

(cid:0)log Zθ(η0
θ )
q(θ)[tθ(θ)] with

−

∇

log Zθ(ηθ). Differentiating with

log Zθ(ηθ) +

− ∇
2 log Zθ(ηθ) (cid:0)η0

∇
θ + E

=

∇

q∗(x)(txy(x, y), 1)
log Zθ(ηθ)
q∗(x)(txy(x, y), 1)

(cid:1)

ηθ

(cid:1) .

ηθ

−

−

As an immediate result of Proposition C.3, the natural gradient [16] deﬁned by
(ηθ) (cid:44) (cid:0)

2 log Zθ(ηθ)(cid:1)−1

(ηθ)

(cid:101)
∇L

∇

∇L

has an even simpler expression.
Corollary C.4 (Natural gradient of the SVI objective)
The natural gradient of the SVI objective Eq. (13) is

(cid:101)
∇L

(ηθ) = η0

θ + E

q∗(x)[ (txy(x, y), 1) ]

ηθ.

−

The natural gradient corrects for a kind of curvature in the variational family and is invariant to
reparameterization of the family [17]. As a result, natural gradient ascent is effectively a second-
order quasi-Newton optimization algorithm, and using natural gradients can greatly accelerate the
convergence of gradient-based optimization algorithms [30, 11]. It is a remarkable consequence of
the exponential family structure that natural gradients of the partially optimized mean ﬁeld objective
with respect to the global variational parameters can be computed efﬁciently (without any backward
pass as would be required in generic reverse-mode differentiation). Indeed, the exponential family
conjugacy structure makes the natural gradient of the SVI objective even easier to compute than the
ﬂat gradient.

C.3 Stochastic natural gradients for large datasets

The real utility of natural gradient SVI is in its application to large datasets. Consider the model
N
composed of global latent variables θ, local latent variables x =
n=1,

N
n=1, and data y =

xn}
{

yn}

{

p(θ, x, y) = p(θ)

p(xn, yn |

θ),

N
(cid:89)

n=1

18

where each p(xn, yn |
ﬁxed observations y =

yn}
{

N
n=1, let

θ) is a copy of the same likelihood function with conjugate prior p(θ). For

q(θ, x) = q(θ)

q(xn)

N
(cid:89)

n=1

be a variational family to approximate the posterior p(θ, x
y) and consider the SVI objective given
by Eq. (13). Using Eq. (8) of Proposition B.4, it is straightforward to extend the natural gradient
expression in Corollary C.4 to an unbiased Monte Carlo estimate which samples terms in the sum
over data points.

|

Corollary C.5 (Unbiased Monte Carlo estimate of the SVI natural gradient)
Using the model and variational family

p(θ, x, y) = p(θ)

q(θ)q(x) = q(θ)

q(xn),

N
(cid:89)

n=1

p(xn, yn |

θ),

N
(cid:89)

n=1

where p(θ) and p(xn, yn |
Let the random index ˆn be sampled from the set
takes value n. Then

{

1, 2, . . . , N

}

θ) are a conjugate pair of exponential families, deﬁne

(ηθ) as in Eq. (13).
and let pn > 0 be the probability it

L

(ηθ) = Eˆn

(cid:101)
∇L

(cid:20)
η0
θ +

E

1
pˆn

q∗(x ˆn)[ (txy(xˆn, yˆn), 1) ]

(cid:21)

ηθ

,

−

where q∗(xˆn) is a local partial optimizer of

given q(θ).

L

Proof. Taking expectation over the index ˆn, we have

Eˆn

(cid:20) 1
pˆn

(cid:21)
q∗(x ˆn)[ (txy(xˆn, yˆn), 1) ]

E

=

E

pn
pn

q∗(xn)[ (txy(xn, yn), 1) ]

N
(cid:88)

n=1

N
(cid:88)

n=1

=

E

q∗(xn)[ (txy(xn, yn), 1) ] .

The remainder of the proof follows from Proposition B.4 and the same argument as in Proposition C.3.

The unbiased stochastic gradient developed in Corollary C.5 can be used in a scalable stochastic
gradient ascent algorithm. To simplify notation, in the following sections we drop the notation
θ) for n = 1, 2, . . . , N and return to working with a single
for multiple likelihood terms p(xn, yn |
likelihood term p(x, y

θ). The extension to multiple likelihood terms is immediate.

|

C.4 Conditinally conjugate models and block updating

The model classes often considered for natural gradient SVI, and the main model classes we consider
here, have additional conjugacy structure in the local latent variables. In this section we introduce
notation for this extra structure in terms of the additional local latent variables z and discuss the local
block coordinate optimization that is often performed to compute the factor q∗(z)q∗(x) for use in the
natural gradient expression.

Let p(z, x, y
conjugate prior, writing

|

θ) be an exponential family and p(θ) be its corresponding natural exponential family

p(z, x, y

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

θ )(cid:9) ,

η0
log Zθ(η0
θ , tθ(θ)
(cid:104)
(cid:105) −
η0
zxy(θ), tzxy(z, x, y)
(cid:104)
tθ(θ), (tzxy(z, x, y), 1)

(cid:105) −

zxy(θ))(cid:9)

log Zzxy(η0
,

{(cid:104)
where we have used tθ(θ) = (cid:0)η0
zxy(θ),
tzxy(z, x, y) be a multilinear polynomial in the statistics functions tx(x), ty(y), and tz(z), let p(z

(15)
zxy(θ))(cid:1) in Eq. (15). Additionally, let
θ),

log Zzxy(η0

(cid:105)}

−

(14)

|

19

p(x
|
to p(x

z, θ), and p(y

|
z, θ) and p(x

|

x, z, θ) = p(y

x, θ) be exponential families, and let p(z

θ) be a conjugate prior

z, θ) be a conjugate prior to p(y

x, θ), so that

|

|

|

p(z

p(x

p(y

θ) = exp (cid:8)
|
z, θ) = exp (cid:8)
= exp (cid:8)
x, θ) = exp (cid:8)
= exp (cid:8)

|

|

|

log Zz(η0

z (θ))(cid:9) ,

η0
z (θ), tz(z)
(cid:104)
(cid:105) −
η0
x(z, θ), tx(x)
(cid:104)
tz(z), η0
(cid:104)
η0
y(x, θ), ty(y)
(cid:104)
tx(x), η0
(cid:104)

(cid:105) −
x(θ)T(tx(x), 1)

(cid:105) −
y(θ)T(ty(y), 1)
(cid:105)

log Zx(η0
(cid:9) ,
(cid:105)
log Zy(η0
(cid:9) ,

x(z, θ))(cid:9)

y(x, z, θ))(cid:9)

(16)

(17)

(18)

for some matrices η0

x(θ) and η0

y(θ).

This model class includes many common models, including the latent Dirichlet allocation, switching
linear dynamical systems with linear-Gaussian emissions, and mixture models and hidden Markov
models with exponential family emissions. The conditionally conjugate structure is both powerful
and restrictive: while it potentially limits the expressiveness of the model class, it enables block
coordinate optimization with very simple and fast updates, as we show next. When conditionally
conjugate structure is not present, these local optimizations can instead be performed with generic
gradient-based methods and automatic differentiation [31].
Proposition C.6 (Unconstrained block coordinate ascent on q(z) and q(x))
Let p(θ, z, x, y) be a model as in Eqs. (14)-(18), and for ﬁxed data y let q(θ)q(z)q(x) be a corre-
sponding mean ﬁeld variational family for approximating the posterior p(θ, z, x

y), with

|

q(θ) = exp
q(z) = exp
q(x) = exp

ηθ, tθ(θ)
ηz, tz(z)
ηx, tx(x)

log Zθ(ηθ)
}
log Zz(ηz)
}
log Zx(ηx)
}

,
,
,

(cid:105) −
(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)
{(cid:104)

and with the mean ﬁeld variational inference objective

[ q(θ)q(z)q(x) ] = E

q(θ)q(z)q(x)

L

(cid:20)

log

p(θ)p(z

|

z, θ)p(y

θ)p(x
|
q(θ)q(z)q(x)

|

x, z, θ)

(cid:21)

.

Fixing the other factors, the partial optimizers q∗(z) and q∗(x) for
given by

L

over all possible densities are

q∗(z) (cid:44) arg max

[ q(θ)q(z)q(x) ] = exp

q∗(x) (cid:44) arg max

[ q(θ)q(z)q(x) ] = exp

q(z) L

q(x) L

η∗
z , tz(z)

log Zz(η∗
z )
}

,

(cid:105) −

η∗
x, tx(x)

log Zx(η∗
x)
}

,

(cid:105) −

{(cid:104)

{(cid:104)

with

z = E
η∗
x = E
η∗

z (θ) + E

q(θ)η0
q(θ)q(z)η0

q(θ)q(x)η0
x(θ)tz(z) + E

x(θ)T(tx(x), 1),
q(θ)η0

y(θ)T(ty(y), 1).

(19)

(20)

Proof. This proposition is a consequence of Lemma C.2 and the conjugacy structure.

log Zθ(ηθ),

log Zz(ηz), and

Proposition C.6 gives an efﬁcient block coordinate ascent algorithm: for ﬁxed ηθ, by alternatively
updating ηz and ηx according to Eqs. (19)-(20) we are guaranteed to converge to a stationary point
that is partially optimal in the parameters of each factor. In addition, performing each update requires
only computing expected sufﬁcient statistics in the variational factors, which means evaluating
log Zx(ηx), quantities that be computed anyway in a gradient-
∇
based optimization routine. The block coordinate ascent procedure leveraging this conditional
conjugacy structure is thus not only efﬁcient but also does not require a choice of step size.
Note in particular that this procedure produces parameters η∗
x(ηθ) that are partially optimal
(and hence stationary) for the objective. That is, deﬁning the parameterized mean ﬁeld variational
inference objective as L(ηθ, ηz, ηx) =
[ q(θ)q(z)q(x) ], for ﬁxed ηθ the block coordinate ascent
procedure has limit points η∗

z (ηθ) and η∗

L
x that satisfy

z and η∗

∇

∇

(ηθ, η∗

z (ηθ), η∗

x(ηθ)) = 0,

∇ηz L

(ηθ, η∗

z (ηθ), η∗

x(ηθ)) = 0.

∇ηxL

20

D The SVAE objective and its gradients

In this section we deﬁne the SVAE variational lower bound and show how to efﬁciently compute
unbiased stochastic estimates of its gradients, including an unbiased estimate of the natural gradient
with respect to the variational parameters with conjugacy structure. The setup here parallels the setup
for natural gradient SVI in Section C, but while SVI is restricted to complete-data conjugate models,
here we consider more general likelihood models.

D.1 SVAE objective

Let p(x
conjugate prior, as in Deﬁnitions B.1 and B.3, writing

|

θ) be an exponential family and let p(θ) be its corresponding natural exponential family

where we have used tθ(θ) = (cid:0)η0
x, γ) be a general family
of densities (not necessarily an exponential family) and let p(γ) be an exponential family prior on its
parameters of the form

x(θ),

−

|

p(x

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

η0
θ , tθ(θ)
(cid:105) −
(cid:104)
η0
x(θ), tx(x)
(cid:104)
(cid:105) −
tθ(θ), (tx(x), 1)

log Zθ(η0

θ )(cid:9) ,
log Zx(η0

{(cid:104)
log Zx(η0

,
(cid:105)}
x(θ))(cid:1) in Eq. (22). Let p(y

x(θ))(cid:9)

p(γ) = exp (cid:8)
(cid:104)

η0
γ, tγ(γ)

(cid:105) −

log Zγ(η0

γ)(cid:9) .

(21)

(22)

For ﬁxed y, consider the mean ﬁeld family of densities q(θ, γ, x) = q(θ)q(γ)q(x) and the mean ﬁeld
variational inference objective

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(23)

By the same argument as in Proposition C.1, without loss of generality we can take the global factor
q(θ) to be in the same exponential family as the prior p(θ), and we denote its natural parameters by
ηθ, writing

{(cid:104)

q(θ) = exp

ηθ, tθ(θ)
We restrict q(γ) to be in the same exponential family as p(γ) with natural parameters ηγ, writing
ηγ, tγ(γ)

log Zγ(ηγ)
}
Finally, we restrict3 q(x) to be in the same exponential family as p(x
θ), writing its natural parameter
as ηx. Using these explicit variational natural parameters, we rewrite the mean ﬁeld variational
inference objective in Eq. (23) as

log Zθ(ηθ)
}

q(γ) = exp

(cid:105) −

(cid:105) −

{(cid:104)

.

|

.

(ηθ, ηγ, ηx) (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

.

(24)

To perform efﬁcient optimization in the objective
deﬁned in Eq. (24), we consider choosing the
variational parameter ηx as a function of the other parameters ηθ and ηγ. One natural choice is to set
, as in Section C. However, ﬁnding a local partial optimizer
ηx to be a local partial optimizer of
may be computationally expensive for general densities p(y
x, γ), and in the large data setting this
expensive optimization would have to be performed for each stochastic gradient update. Instead, we
choose ηx by optimizing over a surrogate objective (cid:98)
, which we design using exponential family
L
structure to be both easy to optimize and to share curvature properties with the mean ﬁeld objective

L

L

|

. The surrogate objective (cid:98)
L

L

is

(ηθ, ηγ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(γ)q(x)

(cid:20)

log

p(θ)p(γ)p(x

(cid:21)

|

ψ(x; y, φ)
θ) exp
}
{
q(θ)q(γ)q(x)
θ) exp
ψ(x; y, φ)
{
q(θ)q(x)

(cid:21)

}

+ const,

|

= E

q(θ)q(x)

(cid:20)

log

p(θ)p(x

(25)

3The parametric form for q(x) need not be restricted a priori, but rather without loss of generality given the
surrogate objective Eq. (25) and the form of ψ used in Eq. (26), the optimal factor q(x) is in the same family as
p(x | θ). We treat it as a restriction here so that we can proceed with more concrete notation.

21

|

where the constant does not depend on ηx. We deﬁne the function ψ(x; y, φ) to have a form related
to the exponential family p(x

θ),

r(y; φ)
{

ψ(x; y, φ) (cid:44)
}φ∈Rm is some class of functions parameterized by φ

r(y; φ), tx(x)
(cid:105)
(cid:104)

where
to be continuously differentiable in φ. We call r(y; φ) the recognition model. We deﬁne η∗
,
be a local partial optimizer of (cid:98)
L
x(ηθ, φ) (cid:44) arg min
η∗

(26)
Rm, which we assume only
x(ηθ, φ) to

∈

,

(ηθ, ηγ, ηx, φ),
(cid:98)
L

ηx

where the notation above should be interpreted as choosing η∗
x(ηθ, φ) to be a local argument of
maximum. The results to follow rely only on necessary ﬁrst-order conditions for unconstrained local
optimality.
Given this choice of function η∗

x(ηθ, φ), we deﬁne the SVAE objective to be
LSVAE(ηθ, ηγ, φ) (cid:44)

(ηθ, ηγ, η∗

x(ηθ, φ)),

L

(27)

where
tion problem to be

L

is the mean ﬁeld variational inference deﬁned in Eq. (24), and we deﬁne the SVAE optimiza-

maxηθ,ηγ ,φLSVAE(ηθ, ηγ, φ).

We summarize these deﬁnitions in the following.
Deﬁnition D.1 (SVAE objective)
Let

denote the mean ﬁeld variational inference objective

L

[ q(θ)q(γ)q(x) ] (cid:44) E

q(θ)q(γ)q(x)

L

(cid:20)

log

p(θ)p(γ)p(x

θ)p(y
|
q(θ)q(γ)q(x)

|

(cid:21)

x, γ)

,

(28)

where the densities p(θ), p(γ), and p(x
nential family conjugate prior to p(x
variational factors as

|

θ) are exponential families and p(θ) is the natural expo-
|
θ), as in Eqs. (21)-(22). Given a parameterization of the

q(θ) = exp

ηθ, tθ(θ)

{(cid:104)

log Zθ(ηθ)
}

,
ηx, tx(x)

q(x) = exp

(cid:105) −

q(γ) = exp

{(cid:104)

ηγ, tγ(γ)
,

log Zx(ηx)

{(cid:104)

(cid:105) −

}

log Zγ(ηγ)
}

,

(cid:105) −

(ηθ, ηγ, ηx) denote the mean ﬁeld variational inference objective Eq. (28) as a function of these

let
variational parameters. We deﬁne the SVAE objective as

L

LSVAE(ηθ, ηγ, φ) (cid:44)

L

(ηθ, ηγ, η∗

x(ηθ, φ)),

where η∗

,
x(ηθ, φ) is deﬁned as a local partial optimizer of the surrogate objective (cid:98)
L

x(ηθ, φ) (cid:44) arg max
η∗

(ηθ, η∗
(cid:98)
L

x(ηθ, φ), φ),

ηx

where the surrogate objective (cid:98)
L

is deﬁned as
(cid:20)

(ηθ, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(x)

log

ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:104)

,
(cid:105)

p(θ)p(x

|

ψ(x; y, φ)
θ) exp
}
{
q(θ)q(x)

(cid:21)

,

for some recognition model r(y; φ) parameterized by φ

Rm.

∈

LSVAE is a lower-bound for the partially-optimized mean ﬁeld variational

The SVAE objective
inference objective in the following sense.
Proposition D.2 (The SVAE objective lower-bounds the mean ﬁeld objective)
The SVAE objective function
the sense that

LSVAE lower-bounds the partially-optimized mean ﬁeld objective

in

L

max
q(x) L

[ q(θ)q(γ)q(x) ]

max

ηx L

≥

(ηθ, ηγ, ηx)

≥ LSVAE(ηθ, ηγ, φ)

φ

∀

∈

Rm,

22

for any choice of function class
such that

{

r(y; φ)

}φ∈Rm in Eq. (26). Furthermore, if there is some φ∗

∈

Rm

then the bound can be made tight in the sense that

ψ(x; y, φ∗) = E

q(γ) log p(y

x, γ)

|

max
q(x) L

[ q(θ)q(γ)q(x) ] = max

(ηθ, ηγ, ηx) = max

ηx L

φ LSVAE(ηθ, ηγ, φ).

Proof. The inequalities follow from the variational principle and the deﬁnition of the SVAE objective
LSVAE. In particular, by Lemma C.2 the optimal factor over all possible densities is given by
x(θ), tx(x)
(cid:105)

q(γ) log p(y

x, γ)(cid:9) ,

exp (cid:8)

q∗∗(x)

q(θ)η0

E
(cid:104)

+ E

(29)

∝

|

exp

while we restrict the factor q(x) to have a particular exponential family form indexed by parameter ηx,
namely q(x)
LSVAE we also restrict the parameter ηx to be
set to η∗
x(ηθ, φ), a particular function of ηθ and φ, rather than setting it to the value that maximizes
the mean ﬁeld objective
. Finally, equality holds when we can set φ to match the optimal ηx and
L
that choice yields the optimal factor given in Eq. (29).

. In the deﬁnition of

ηx, tx(x)

{(cid:104)

(cid:105)}

∝

Proposition D.2 motivates the SVAE optimization problem: by using gradient-based optimization to
LSVAE(ηθ, ηγ, φ) we are maximizing a lower-bound on the model evidence log p(y) and
maximize
correspondingly minimizing the KL divergence from our variational family to the target posterior.
Furthermore, it motivates choosing the recognition model function class
}φ∈Rm to be as rich
as possible.
As we show in the following, choosing η∗
objective (cid:98)
L
simple expression for an unbiased estimate of the natural gradient (cid:101)
Section D.2. Second, it allows η∗
structure, as we show in Section D.4.

x(ηθ, φ) to be a local partial optimizer of the surrogate
provides two signiﬁcant computational advantages. First, it allows us to provide a
∇ηθ LSVAE, as we describe next in
x(ηθ, φ) to be computed efﬁciently by exploiting exponential family

r(y; φ)
{

D.2 Estimating the natural gradient (cid:101)

∇ηθ LSVAE

x in terms of the surrogate objective (cid:98)
L

The deﬁnition of η∗
enables computationally efﬁcient
ways to estimate natural gradient with respect to the conjugate global variational parameters,
∇ηθ LSVAE(ηθ, ηγ, φ). The next proposition covers the case when the local latent variational factor
(cid:101)
q(x) has no additional factorization structure.
Proposition D.3 (Natural gradient of the SVAE objective)
When there is only one local latent variational factor q(x) (and no further factorization structure),
the natural gradient of the SVAE objective Eq. (27) with respect to the conjugate global variational
parameters ηθ is

(cid:101)

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)η0

θ + E
∇ηxL
where the ﬁrst term is the SVI natural gradient from Corollary C.4, using

q∗(x) [(tx(x), 1)]

(cid:1) + (

ηθ

−

(ηθ, ηγ, η∗

x(ηθ, φ)), 0)

q∗(x) (cid:44) exp

η∗
x(ηθ, φ), tx(x)

log Zx(η∗

x(ηθ, φ))

{(cid:104)

(cid:105) −

,

}

and where a stochastic estimate of the second term is computed as part of the backward pass for the
gradient

(ηθ, ηγ, η∗

x(ηθ, φ)).

∇φL

Proof. First we use the chain rule, analogously to Eq. (5), to write the gradient as

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)

2 log Zθ(ηθ)(cid:1) (cid:0)η0
∇
∇ηθ η∗
x(ηθ, φ)) (
+ (
where the ﬁrst term is the same as the SVI gradient derived in Proposition C.3. In the case of SVI,
the second term is zero because η∗
, but for the SVAE objective
x is chosen as a partial optimizer of
the second term is nonzero in general, and the remainder of this proof amounts to deriving a simple
expression for it.

q∗(x) [ (txy(x, y), 1) ]
(ηθ, ηγ, η∗
x(ηθ, φ))) ,

θ + E

∇ηx L

(30)

ηθ

−

L

(cid:1)

23

We compute the term
using the Implicit Function Theorem given in Corollary A.5, which yields

x(ηθ, φ) in Eq. (30) in terms of the gradients of the surrogate objective (cid:98)
L

∇ηθ η∗

(ηθ, η∗

x(ηθ, φ), φ)

(cid:17)−1

.

(31)

∇ηθ η∗

x(ηθ, φ) =

−∇

First, we compute the gradient of (cid:98)
L
E

(ηθ, ηx, φ) =

∇ηx (cid:98)
L

(cid:16)

(ηθ, η∗

x(ηθ, φ), φ)

2
ηθηx (cid:98)
L
with respect to ηx, writing
p(x

∇

(cid:20)

(cid:20)

2
ηxηx (cid:98)
L

q(θ)q(x)

log

∇ηx

=
= (cid:0)

∇ηx
∇

(cid:2)
E
q(θ)η0
(cid:104)
2 log Zx(ηx)(cid:1) (cid:0)E

x(θ) + r(y; φ)

q(θ)η0

(cid:21)(cid:21)

|

)
}

ψ(x; y, φ)
θ) exp
{
q(x)
ηx,
x(θ) + r(y; φ)

∇

−

log Zx(ηx)
(cid:105)
(cid:1) ,
ηx

−

+ log Zx(ηx)(cid:3)

(32)

When there is only one local latent variational factor q(x) (and no further factorization structure), as
x(ηθ, φ), φ) = 0 and the fact that
a consequence of the ﬁrst-order stationary condition

∇ηx (cid:98)
L
η∗
x(ηθ, φ) = 0,
which is useful in simplifying the expressions to follow.

2 log Zx(ηx) is always positive deﬁnite for minimal exponential families, we have
x(θ) + r(y; φ)

(ηθ, η∗

q(θ)η0

(33)

∇

−

E

Continuing with the calculation of the terms in Eq. (31), we compute
expression in Eq. (32) again, writing

2
ηxηx (cid:98)
L

∇

by differentiating the

2
ηxηx (cid:98)
L

∇

(ηθ, η∗

x(ηθ, φ), φ) =

2 log Zx(η∗
3 log Zx(η∗
2 log Zx(η∗

x(ηθ, φ))
x(ηθ, φ))(cid:1)(cid:0)E
x(ηθ, φ)),

−∇
+ (cid:0)
=

∇

−∇

q(θ)η0

x(θ) + r(y; φ)

x(ηθ, φ)(cid:1)
η∗

−

(34)

where the last line follows from using the ﬁrst-order stationary condition Eq. (33). Next, we compute
by differentiating Eq. (32) with respect to ηθ to yield
the other term
2 log Zx(η∗

(cid:18)

(cid:19)

x(ηθ, φ))

∇

where the latter matrix is

0
x(ηθ, φ)) padded by a row of zeros.
Plugging these expressions back into Eq. (31) and cancelling, we arrive at

∇

∇

x(ηθ, φ), φ) = (cid:0)
2 log Zx(η∗

2 log Zθ(ηθ)(cid:1)

∇

2
ηθηx (cid:98)
L
∇
(ηθ, η∗
2
ηθηx (cid:98)
L

,

∇ηθ η∗
and so we have an expression for the gradient of the SVAE objective as

2 log Zθ(ηθ)

x(ηθ, φ) =

∇

,

(cid:19)

(cid:18)I
0

∇ηθ LSVAE(ηθ, ηγ, φ) = (cid:0)
∇
+ (cid:0)

2 log Zθ(ηθ)(cid:1) (cid:0)η0
2 log Zθ(ηθ)(cid:1) (
When we compute the natural gradient, the Fisher information matrix factors on the left of each term
cancel, yielding the result in the proposition.

q∗(x) [ (txy(x, y), 1) ]
(ηθ, ηγ, η∗

−
x(ηθ, φ)), 0) .

θ + E

∇ηx L

ηθ

∇

(cid:1)

The proof of Proposition D.3 uses the necessary condition for unconstrained local optimality to
simplify the expression in Eq. (34). This simpliﬁcation does not necessarily hold if ηx is constrained;
for example, if the factor q(x) has additional factorization structure, then there are additional (linear)
coordinate subspace constraints on ηx. Note also that when q(x) is a Gaussian family with ﬁxed
covariance (that is, with sufﬁcient statistics tx(x) = x) the same simpliﬁcation always applies
3 log Zx(ηx) = 0.
because third and higher-order cumulants are zero for such families and hence
More generally, when the local latent variables have additional factorization structure, as in the
Gaussian mixture model (GMM) and switching linear dynamical system (SLDS) examples, the
natural gradient with respect to ηθ can be estimated efﬁciently by writing Eq. (30) as
2 log Zθ(ηθ)(cid:1) (cid:0)η0
(cid:1)
[η(cid:48)

θ + E
x(η(cid:48)
(ηθ, ηγ, η∗
where we can recover the second term in Eq. (30) by using the chain rule. We can estimate this second
term directly using the reparameterization trick. Note that to compute the natural gradient estimate in
2 log Zθ(ηθ))−1 to this term because the convenient cancellation from
this case, we need to apply (
Proposition D.3 does not apply. When ηθ is of small dimension compared to ηγ, φ, and even ηx, this
additional computational cost is not large.

q∗(x) [ (txy(x, y), 1) ]
θ, φ))] ,

∇ηθ LSVAE = (cid:0)

θ (cid:55)→ L

ηθ

∇

∇

∇

∇

−

+

24

D.3 Estimating the gradients

∇φLSVAE and

∇ηγ LSVAE

∇φLSVAE(ηθ, ηγ, φ) and
To compute an unbiased stochastic estimate of
∇ηγ LSVAE(ηθ, ηγ, φ) we use the reparameterization trick [7], which is simply to differentiate a
LSVAE(ηθ, ηγ, φ) as a function of φ and ηγ. To isolate the terms
stochastic estimate of the objective
that require this sample-based approximation from those that can be computed directly, we rewrite
the objective as

the gradients

LSVAE(ηθ, ηγ, φ) = E

q(γ)q∗(x) log p(y

x, γ)

|

−

KL(q(θ)q(γ)q∗(x)

p(θ, γ, x))

(35)

where, as before,

q∗(x) (cid:44) exp

η∗
x(ηθ, φ), tx(x)

log Zx(η∗

x(ηθ, φ))

{(cid:104)
and so the dependence of the expression in Eq. (35) on φ is through η∗
Eq. (35) needs to be estimated with the reparameterization trick.

(cid:105) −

x(ηθ, φ). Only the ﬁrst term in

We summarize this procedure in the following proposition.
Proposition D.4 (Estimating
∇ηγ LSVAE)
Let ˆγ(ηγ)
of the gradients

∇φLSVAE and
q∗(x) be samples of q(γ) and q∗(x), respectively. Unbiased estimates
∼
∇φLSVAE(ηθ, ηγ, φ) and

∇ηγ LSVAE(ηθ, ηγ, φ) are given by

q(γ) and ˆx(φ)

∼

∇φLSVAE(ηθ, ηγ, φ)
∇ηγ LSVAE(ηθ, ηγ, φ)

≈ ∇φ log p(y
|
≈ ∇ηγ log p(y

|

ˆx(φ), ˆγ(ηγ))
ˆx(φ), ˆγ(ηγ))

− ∇φ KL(q(θ)q∗(x)
− ∇ηγ KL(q(γ)
(cid:107)

(cid:107)
p(γ)).

p(θ, x)),

Both of these gradients can be computed by automatically differentiating the Monte Carlo estimate of
LSVAE given by

LSVAE(ηθ, ηγ, φ)
with respect to ηγ and φ, respectively.

log p(y

≈

|

ˆx(φ), ˆγ(ηγ))

KL(q(θ)q(γ)q∗(x)

p(θ, γ, x))

−

(cid:107)

(cid:107)

}

D.4 Partially optimizing (cid:98)
L

using conjugacy structure

In Section D.1 we deﬁned the SVAE objective in terms of a function η∗
implicitly deﬁned in terms of ﬁrst-order stationary conditions for an auxiliary objective (cid:98)
L
Here we show how (cid:98)
L
conjugate model of Section C.4.

x(ηθ, φ), which was itself
(ηθ, ηx, φ).
admits efﬁcient local partial optimization in the same way as the conditionally

In this section we consider additional structure in the local latent variables. Speciﬁcally, as in
Section C.4, we introduce to the notation another set of local latent variables z in addition to the
local latent variables x. However, unlike Section C.4, we still consider general likelihood families
p(y

x, γ).

θ) be an exponential family and p(θ) be its corresponding natural exponential family

|
Let p(z, x
|
conjugate prior, writing

p(z, x

p(θ) = exp (cid:8)
θ) = exp (cid:8)
= exp

|

log Zθ(η0

(cid:105) −

η0
θ , tθ(θ)
(cid:104)
η0
zx(θ), tzx(z, x)
(cid:104)
(cid:105) −
tθ(θ), (tzx(z, x), 1)

θ )(cid:9) ,
log Zzx(η0

zx(θ))(cid:9)

{(cid:104)

(cid:105)}

(36)

where we have used tθ(θ) = (cid:0)η0
zx(θ),
a multilinear polynomial in the statistics tz(z) and tx(x), and let p(z
pair of exponential families, writing

log Zzx(η0

zx(θ))(cid:1) in Eq. (15). Additionally, let tzx(z, x) be
z, θ) be a conjugate
θ) and p(x

−

|

p(z

p(x

|

θ) = exp (cid:8)
η0
z (θ), tz(z)
(cid:104)
|
(cid:105) −
z, θ) = exp (cid:8)
η0
x(z, θ), tx(x)
(cid:104)
= exp (cid:8)
tz(z), η0
(cid:104)

(cid:105) −
x(θ)T(tx(x), 1)

log Zx(η0
(cid:9) .
(cid:105)

log Zz(η0

|
z (θ))(cid:9) ,

x(z, θ))(cid:9)

Let p(y
an exponential family prior on its parameters of the form

x, γ) be a general family of densities (not necessarily an exponential family) and let p(γ) be

|

p(γ) = exp (cid:8)
(cid:104)

η0
γ, tγ(γ)

(cid:105) −

log Zγ(η0

γ)(cid:9) .

25

The corresponding variational factors are

q(θ) = exp
q(z) = exp

ηθ, tθ(θ)
ηz, tz(z)

log Zθ(ηθ)
}
log Zz(ηz)
}

,
,

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

q(γ) = exp
q(x) = exp

ηγ, tγ(γ)
ηx, tx(x)

log Zγ(ηγ)
}
log Zx(ηx)
}

,
.

(cid:105) −
(cid:105) −

{(cid:104)
{(cid:104)

As in Section D.1, we construct the surrogate objective (cid:98)
L
and conjugacy structure. In particular, we construct (cid:98)
L
p(θ)p(γ)p(z

to allow us to exploit exponential family
to resemble the mean ﬁeld objective, namely
z, θ)p(y

x, γ)

(cid:21)

(cid:20)

(ηθ, ηγ, ηz, ηx) (cid:44) E

q(θ)q(γ)q(z)q(x)

log

,

θ)p(x
q(θ)q(γ)q(z)q(x)

|

|

L

but in (cid:98)
L
without much structure, with a more tractable approximation,

we replace the log p(y

|

x, γ) likelihood term, which may be a general family of densities

(ηθ, ηz, ηx, φ) (cid:44) E
(cid:98)
L

q(θ)q(z)q(x)

(cid:20)

log

p(θ)p(z

|

θ)p(x

z, θ) exp
{
q(θ)q(z)q(x)

|

ψ(x; y, φ)
}

(cid:21)

,

where ψ(x; y, φ) is a function on x that resembles a conjugate likelihood for p(x

z, θ),

|

|

ψ(x; y, φ) (cid:44)

r(y; φ), tx(x)
(cid:105)

,

(cid:104)

Rm.

φ

∈

We then deﬁne η∗
given ﬁxed values of the
other parameters ηθ and φ, and in particular they satisfy the ﬁrst-order necessary optimality conditions

x(ηθ, φ) to be local partial optimizers of (cid:98)
L

z (ηθ, φ) and η∗

∇ηz (cid:98)
L

The SVAE objective is then

(ηθ, η∗

z (ηθ, φ), η∗

x(ηθ, φ), φ) = 0,

(ηθ, η∗

z (ηθ, φ), η∗

x(ηθ, φ), φ) = 0.

∇ηx (cid:98)
L

L

x(ηθ, φ)).

(ηθ, ηγ, η∗

z (ηθ, φ), η∗

LSVAE(ηθ, ηγ, φ) (cid:44)
The structure of the surrogate objective (cid:98)
is chosen so that it resembles the mean ﬁeld variational
L
inference objective for the conditionally conjugate model of Section C.4, and as a result we can
use the same block coordinate ascent algorithm to efﬁciently ﬁnd partial optimzers η∗
z (ηθ, φ) and
η∗
x(ηθ, φ).
Proposition D.5 (Computing η∗
Let the densities p(θ, γ, z, x, y) and q(θ)q(γ)q(z)q(x) and the objectives
Eqs. (36)-(37). The partial optimizers η∗
(cid:44) arg max
ηz

x, deﬁned by
η∗
x

(ηθ, ηz, ηx, φ),
(cid:98)
L

(ηθ, ηz, ηx, φ)
(cid:98)
L

LSVAE be as in

(cid:44) arg max
ηx

z (ηθ, φ) and η∗

z and η∗

x(ηθ, φ))

, (cid:98)
L

, and

(37)

η∗
z

L

q(θ)η0

z = E
η∗

with the other arguments ﬁxed, are are given by
z (θ) + E

q(θ)q(z)η0
and by alternating the expressions in Eq. (38) as updates we can compute η∗
.
local partial optimizers of (cid:98)
L

x(θ)T(tx(x), 1),

q(θ)q(x)η0

x = E
η∗

x(z, θ) + r(y; φ),
z (ηθ, φ) and η∗

(38)
x(ηθ, φ) as

Proof. These updates follow immediately from Lemma C.2. Note in particular that the stationary
conditions

= 0 yield the each expression in Eq. (38), respectively.

= 0 and

∇ηz (cid:98)
L

∇ηx (cid:98)
L

The other properties developed in Propositions D.2, D.3, and D.4 also hold true for this model because
it is a special case in which we have separated out the local variables, denoted x in earlier sections,
into two groups, denoted z and x here, to match the exponential family structure in p(z
θ) and
p(x
z, θ), and performed unconstrained optimization in each of the variational parameters. However,
the expression for the natural gradient is slightly simpler for this model than the corresponding
version of Proposition D.3.

|

|

E Experiment details and expanded ﬁgures

For the synthetic 1D dot video data, we trained an LDS SVAE on 80 random image sequences each
of length 50, using one sequence per update, and show the model’s future predictions given a preﬁx
of a longer sequence. We used MLP image and recognition models each with one hidden layer of 50
units and a latent state space of dimension 8.

26

(a) Beginning a rear

(b) Grooming

(c) Extension into running

27

(d) Fall from rear

Figure 8: Examples of behavior states inferred from depth video. For each state, four example frame
sequences are shown, including frames during which the given state was most probable according to
the variational distribution on the hidden state sequence. Each frame sequence is padded on both
sides, with a square in the lower-right of a frame depicting that the state was active in that frame. The
frame sequences are temporally subsampled to reduce their length, showing one of every four video
frames. Examples were chosen to have durations close to the median duration for that state.

