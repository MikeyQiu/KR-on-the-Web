When Image Denoising Meets High-Level Vision Tasks:
A Deep Learning Approach

Ding Liu1, Bihan Wen1, Xianming Liu2, Zhangyang Wang3, Thomas S. Huang1 ∗
1 University of Illinois at Urbana-Champaign, USA
2 Facebook Inc. 3 Texas A&M University, USA
{dingliu2,bwen3,t-huang1}@illinois.edu, xmliu@fb.com, atlaswang@tamu.edu

Abstract

Conventionally, image denoising and high-level vi-
sion tasks are handled separately in computer vi-
sion. In this paper, we cope with the two jointly
and explore the mutual inﬂuence between them.
First we propose a convolutional neural network for
image denoising which achieves the state-of-the-
art performance. Second we propose a deep neu-
ral network solution that cascades two modules for
image denoising and various high-level tasks, re-
spectively, and use the joint loss for updating only
the denoising network via back-propagation. We
demonstrate that on one hand, the proposed de-
noiser has the generality to overcome the perfor-
mance degradation of different high-level vision
tasks. On the other hand, with the guidance of high-
level vision information, the denoising network can
generate more visually appealing results. To the
best of our knowledge, this is the ﬁrst work inves-
tigating the beneﬁt of exploiting image semantics
simultaneously for image denoising and high-level
vision tasks via deep learning. The code is avail-
able online1.

8
1
0
2
 
r
p
A
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
4
8
2
4
0
.
6
0
7
1
:
v
i
X
r
a

(a)

Noisy input

CBM3D

1 Introduction
A common approach in computer vision is to separate low-
level vision problems, such as image restoration and enhance-
ment, from high-level vision problems, and solve them inde-
pendently. In this paper, we make their connection by show-
ing the mutual inﬂuence between the two, i.e., visual percep-
tion and semantics, and propose a new perspective of solving
both the low-level and high-level computer vision problems
in a single uniﬁed framework, as shown in Fig. 1(a).

Image denoising, as one representative of low-level vision
problems, is dedicated to recovering the underlying image
signal from its noisy measurement. Classical image denois-
ing methods take advantage of local or non-local structures
presented in the image [Aharon et al., 2006; Dabov et al.,
2007b; Mairal et al., 2009; Dong et al., 2013; Gu et al., 2014;

∗Ding Liu and Thomas S. Huang’s research work was supported
by the U.S. Army Research Ofﬁce under Grant W911NF-15-1-0317.
1https://github.com/Ding-Liu/DeepDenoising

Our method

Ground truth

(b)
Figure 1: (a) Upper: conventional semantic segmentation pipeline;
lower: our proposed framework for joint image denoising and se-
mantic segmentation. (b) Zoom-in regions of a noisy input, its de-
noised estimates using CBM3D and our proposed method, as well
as its ground truth.

Xu et al., 2015]. More recently, a number of deep learn-
ing models have been developed for image denoising which
demonstrated superior performance [Vincent et al., 2008;
Burger et al., 2012; Mao et al., 2016; Chen and Pock, 2017;
Zhang et al., 2017a]. Inspired by U-Net [Ronneberger et al.,
2015], we propose a convolutional neural network for image
denoising, which achieves the state-of-the-art performance.

While popular image denoising algorithms reconstruct im-
ages by minimizing the mean square error (MSE), important
image details are usually lost which leads to image quality
degradation, e.g., over-smoothing artifacts in some texture-
rich regions are commonly observed in the denoised output
from conventional methods, as shown in Fig. 1(b). To this

Figure 2: (a) Overview of our proposed denoising network. (b) Architecture of the feature encoding module. (c) Architecture of the feature
decoding module.

end, we propose a cascade architecture connecting image de-
noising to a high-level vision network. We jointly minimize
the image reconstruction loss and the high-level vision loss.
With the guidance of image semantic information, the denois-
ing network is able to further improve visual quality and gen-
erate more visually appealing outputs, which demonstrates
the importance of semantic information for image denoising.
When high-level vision tasks are conducted on noisy
data, an independent image restoration step is typically ap-
plied as preprocessing, which is suboptimal for the ultimate
goal [Wang et al., 2016; Wu et al., 2017; Liu et al., 2017].
Recent research reveals that neural networks trained for im-
age classiﬁcation can be easily fooled by small noise per-
turbation or other artiﬁcial patterns [Szegedy et al., 2013;
Nguyen et al., 2015]. Therefore, an application-driven de-
noiser should be capable of simultaneously removing noise
and preserving semantic-aware details for the high-level vi-
sion tasks. Under the proposed architecture, we systemati-
cally investigate the mutual inﬂuence between the low-level
and high-level vision networks. We show that the cascaded
network trained with the joint loss not only boosts the denois-
ing network performance via image semantic guidance, but
also substantially improves the accuracy of high-level vision
tasks. Moreover, our proposed training strategy makes the
trained denoising network robust enough to different high-
level vision tasks.
In other words, our denoising module
trained for one high-level vision task can be directly plugged
into other high-level tasks without ﬁnetuning either module,
which facilitates the training effort when applied to various
high-level tasks.

2 Method

We ﬁrst introduce the denoising network utilized in our
framework, and then explain the relationship between the im-
age denoising module and the module for high-level vision
tasks in detail.

2.1 Denoising Network
We propose a convolutional neural network for image denois-
ing, which takes a noisy image as input and outputs the re-
constructed image. This network conducts feature contrac-
tion and expansion through downsampling and upsampling

operations, respectively. Each pair of downsampling and up-
sampling operations brings the feature representation into a
new spatial scale, so that the whole network can process in-
formation on different scales.

Speciﬁcally, on each scale, the input is encoded after down-
sampling the features from the previous scale. After feature
encoding and decoding possibly with features on the next
scale, the output is upsampled and fused with the feature
on the previous scale. Such pairs of downsampling and up-
sampling steps can be nested to build deeper networks with
more spatial scales of feature representation, which gener-
ally leads to better restoration performance. Considering the
tradeoff between computation cost and restoration accuracy,
we choose three scales for the denoising network in our ex-
periments, while this framework can be easily extended for
more scales.

These operations together are designed to learn the residual
between the input and the target output and recover as many
details as possible, so we use a long-distance skip connection
to sum the output of these operations and the input image, in
order to generate the reconstructed image. The overview is in
Fig. 2 (a). Each module in this network will be elaborated as
follows.

Feature Encoding: We design one feature encoding mod-
ule on each scale. which is one convolutional layer plus one
residual block as in [He et al., 2016]. The architecture is dis-
played in Fig. 2 (b). Note that each convolutional layer is
immediately followed by spatial batch normalization and a
ReLU neuron. From top to down, the four convolutional lay-
ers have 128, 32, 32 and 128 kernels in size of 3×3, 1×1, 3×3
and 1 × 1, respectively. The output of the ﬁrst convolutioal
layer is passed through a skip connection for element-wise
sum with the output of the last convolutional layer.

Feature Decoding: The feature decoding module is de-
signed for fusing information from two adjacent scales. Two
fusion schemes are tested: (1) concatenation of features on
these two scales; (2) element-wise sum of them. Both of
them obtain similar denosing performance. Thus we choose
the ﬁrst scheme to accommodate feature representations of
different channel numbers from two scales. We use a similar
architecture as the feature encoding module except that the
number of kernels in the four convolutional layers are 256,
64, 64 and 256. Its architecture is in Fig. 2(c).

Figure 3: Overview of our proposed cascaded network.

Feature Downsampling & Upsampling: Downsampling
operations are adopted multiple times to progressively in-
crease the receptive ﬁeld of the following convolution kernels
and to reduce the computation cost by decreasing the feature
map size. The larger receptive ﬁeld enables the kernels to in-
corporate larger spatial context for denoising. We use 2 as the
downsampling and upsampling factors, and try two schemes
for downsampling in the experiments: (1) max pooling with
stride of 2; (2) conducting convolutions with stride of 2. Both
of them achieve similar denoisng performance in practice, so
we use the second scheme in the rest experiments for compu-
tation efﬁciency. Upsampling operations are implemented by
deconvolution with 4 × 4 kernels, which aim to expand the
feature map to the same spatial size as the previous scale.

Since all the operations in our proposed denoising network
are spatially invariant, it has the merit of handling input im-
ages of arbitrary size.

2.2 When Image Denoising Meets High-Level

Vision Tasks

We propose a robust deep architecture processing a noisy im-
age input, via cascading a network for denoising and the other
for high-level vision task, aiming to simultaneously:

1. reconstruct visually pleasing results guided by the high-
level vision information, as the output of the denoisnig
network;

2. attain sufﬁciently good accuracy across various high-
level vision tasks, when trained for only one high-level
vision task;

The overview of the proposed cascaded network is displayed
in Fig. 3. Speciﬁcally, given a noisy input image, the denos-
ing network is ﬁrst applied, and the denoised result is then fed
into the following network for high-level vision task, which
generates the high-level vision task output.

Training Strategy: First we initialize the network for
high-level vision task from a network that is well-trained in
the noiseless setting. We train the cascade of two networks in
an end-to-end manner while ﬁxing the weights in the network
for high-level vision task. Only the weights in the denois-
ing network are updated by the error back-propagated from
the following network for high-level vision task, which is
similar to minimizing the perceptual loss for image super-
resolution [Johnson et al., 2016]. The reason to adopt such
a training strategy is to make the trained denoising network
robust enough without losing the generality for various high-
level vision tasks. More speciﬁcally, our denoising module
trained for one high-level vision task can be directly plugged
into other high-level tasks without ﬁnetuning either the de-
noiser or the high-level network. Our approach not only

facilitates the training effort when applying the denoiser to
different high-level tasks while keeping the high-level vision
network performing consistently for noisy and noise-free im-
ages, but also enables the denoising network to produce high-
quality perceptual and semantically faithful results.

Loss: The reconstruction loss of the denoising network is
the mean squared error (MSE) between the denoising net-
work output and the noiseless image. The losses of the clas-
siﬁcation network and the segmentation network both are the
cross-entropy loss between the predicted label and the ground
truth label. The joint loss is deﬁned as the weighted sum of
the reconstruction loss and the loss for high-level vision task,
which can be represented as

L(F (x), y) = LD(FD(x), ˜x) + λLH (FH (FD(x)), y), (1)

where x is the noisy input image, ˜x is the noiseless image and
y is the ground truth label of high-level vision task. FD, FH
and F denote the denoising network, the network of high-
level vision task and the whole cascaded network, respec-
tively. LD, LH represent the losses of the denoising network
and the high-level vision task network, respectively, while L
is the joint loss, as illustrated in Fig. 3. λ is the weight for
balancing the losses LD and LH .

Image Denoising

3 Experiments
3.1
Our proposed denoising network takes RGB images as input,
and outputs the reconstructed images directly. We add inde-
pendent and identically distributed Gaussian noise with zero
mean to the original image as the noisy input image during
training. We use the training set as in [Chen et al., 2014].
The loss of training is equivalent to Eqn. 1 as λ = 0. We
use SGD with a batch size of 32, and the input patches are
48 × 48 pixels. The initial learning rate is set as 10−4 and
is divided by 10 after every 500,000 iterations. The training
is terminated after 1,500,000 iterations. We train a different
denoising network for each noise level in our experiment.

We compare our denoisnig network with several state-of-
the-art color image denoising approaches on various noise
levels: σ = 25, 35 and 50. We evaluate their denoising per-
formance over the widely used Kodak dataset2, which con-
sists of 24 color images. Table 1 shows the peak signal-
to-noise ratio (PSNR) results for CBM3D [Dabov et al.,
2007a], TNRD [Chen and Pock, 2017], MCWNNM [Xu et
al., 2017], DnCNN [Zhang et al., 2017a], and our proposed
method. We do not list other methods [Burger et al., 2012;

2http://r0k.us/graphics/kodak/

Table 1: Color image denoising results (PSNR) of different methods on Kodak dataset. The best result is shown in bold.

Image CBM3D TNRD MCWNNM DnCNN Proposed CBM3D MCWNNM DnCNN Proposed CBM3D TNRD MCWNNM DnCNN Proposed

σ = 25

σ = 35

σ = 50

01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Average

29.13
32.44
34.54
32.67
29.73
30.59
33.66
29.88
34.06
33.82
31.25
33.76
27.64
30.03
33.08
32.33
32.93
29.83
31.78
33.45
30.99
30.93
34.79
30.09
31.81

27.21
31.44
32.73
31.16
27.81
28.52
31.90
27.38
32.21
31.91
29.51
32.17
25.52
28.50
31.62
30.36
31.20
28.00
30.01
32.00
29.09
29.60
33.68
28.17
30.08

28.66
31.92
34.05
32.42
29.37
30.18
33.36
29.39
33.42
33.23
30.62
33.02
27.19
29.67
32.69
31.79
32.39
29.46
31.29
32.78
30.55
30.48
34.45
29.93
31.35

29.75
32.97
34.97
32.94
30.53
31.05
34.42
30.30
34.59
34.33
31.82
34.12
28.26
30.79
33.32
32.69
33.53
30.40
32.23
34.15
31.61
31.41
35.36
30.79
32.35

29.76
33.00
35.12
33.01
30.55
31.08
34.47
30.37
34.63
34.38
31.84
34.18
28.24
30.80
33.35
32.74
33.50
30.46
32.30
34.29
31.63
31.38
35.40
30.77
32.39

27.31
31.07
32.62
31.02
27.61
28.78
31.64
27.82
32.28
31.97
29.53
32.24
25.70
28.24
31.47
30.64
30.64
28.00
30.19
31.84
29.17
29.36
33.09
28.19
30.04

26.93
30.62
32.27
30.92
27.53
28.44
31.53
27.67
31.76
31.51
29.04
31.52
25.40
28.05
31.15
30.15
30.75
27.70
29.86
31.32
28.86
28.93
32.79
28.17
29.70

28.10
31.65
33.37
31.51
28.66
29.37
32.60
28.53
33.06
32.74
30.23
32.73
26.46
29.17
31.89
31.16
31.96
28.72
30.80
32.73
29.94
29.94
33.86
28.98
30.76

28.11
31.75
33.58
31.59
28.72
29.45
32.70
28.64
33.11
32.83
30.29
32.83
26.47
29.20
31.96
31.23
31.98
28.79
30.88
32.91
29.98
29.95
33.89
29.03
30.83

25.86
29.84
31.34
29.92
25.92
27.34
29.99
26.23
30.86
30.48
28.00
30.98
24.03
26.74
30.32
29.36
29.36
26.41
29.06
30.51
27.61
28.09
31.75
26.62
28.62

24.46
29.12
29.95
28.65
24.37
25.62
28.24
23.93
28.78
28.78
26.75
29.70
22.54
25.67
29.07
27.82
28.07
25.06
27.30
29.24
26.09
27.14
30.53
24.92
27.17

25.28
29.27
30.52
29.37
25.60
26.70
29.51
25.86
30.00
29.63
27.41
30.00
23.70
26.43
29.59
28.53
28.98
25.94
28.44
29.79
27.13
27.47
30.96
26.37
28.02

26.52
30.44
31.76
30.12
26.77
27.74
30.67
26.65
31.42
31.03
28.67
31.32
24.73
27.57
30.50
29.68
30.33
27.03
29.34
31.28
28.27
28.54
32.18
27.18
29.16

26.55
30.54
31.99
30.22
26.87
27.85
30.82
26.84
31.53
31.17
28.76
31.47
24.76
27.63
30.59
29.78
30.40
27.14
29.49
31.53
28.34
28.58
32.30
27.30
29.27

Zoran and Weiss, 2011; Gu et al., 2014; Zhang et al., 2017b]
whose average performance is wore than DnCNN. The imple-
mentation codes used are from the authors’ websites and the
default parameter settings are adopted in our experiments.3

It is clear that our proposed method outperforms all the
competing approaches quantitatively across different noise
levels. It achieves the highest PSNR in almost every image
of Kodak dataset.

3.2 When Image Denoising Meets High-Level

Vision Tasks

We choose two high-level vision tasks as representatives in
our study: image classiﬁcation and semantic segmentation,
which have been dominated by deep network based models.
We utilize two popular VGG-based deep networks in our sys-
tem for each task, respectively. VGG-16 in [Simonyan and
Zisserman, 2014] is employed for image classiﬁcation; we
select DeepLab-LargeFOV in [Chen et al., 2014] for seman-
tic segmentation. We follow the preprocessing protocols (e.g.
crop size, mean removal of each color channel) in [Simonyan
and Zisserman, 2014] and [Chen et al., 2014] accordingly
while training and deploying them in our experiments.

As for the cascaded network for image classiﬁcation
and the corresponding experiments, we train our model on
ILSVRC2012 training set, and evaluate the classiﬁcation ac-
curacy on ILSVRC2012 validation set. λ is empirically set as
0.25. As for the cascaded network for image semantic seg-
mentation and its corresponding experiments, we train our
model on the augmented training set of Pascal VOC 2012 as
in [Chen et al., 2014], and test on its validation set. λ is em-
pirically set as 0.5.

3For TNRD, we denoise each color channel using the grayscale
image denoising implementation which is from the authors’ website.
TNRD for σ = 35 is not publicly available, so we do not include
this case here.

High-Level Vision Information Guided Image Denoising
The typical metric used for image denoising is PSNR, which
has been shown to sometimes correlate poorly with human as-
sessment of visual quality [Huynh-Thu and Ghanbari, 2008].
Since PSNR depends on the reconstruction error between the
denoised output and the reference image, a model trained by
minimizing MSE on the image domain should always outper-
form a model trained by minimizing our proposed joint loss
(with the guidance of high-level vision semantics) in the met-
ric of PSNR. Therefore, we emphasize that the goal of our
following experiments is not to pursue the highest PSNR, but
to demonstrate the qualitative difference between the model
trained with our proposed joint loss and the model trained
with MSE on the image domain.

Fig. 4 displays two image denoising examples from Ko-
dak dataset. A visual comparison is illustrated for a zoom-
(II) and (III) are the denoising results using
in region:
CBM3D [Dabov et al., 2007a], and DnCNN [Zhang et al.,
2017a], respectively; (IV) is the proposed denoiser trained
separately without the guidance of high-level vision infor-
mation; (V) is the denoising result using the proposed de-
noising network trained jointly with a segmentation network.
We can ﬁnd that the results using CBM3D, DnCNN and our
separately trained denoiser generate oversmoothing regions,
while the jointly trained denoising network is able to recon-
struct the denoised image which preserves more details and
textures with better visual quality.

Generality of the Denoiser for High-Level Vision Tasks
We now investigate how the image denoising can enhance
the high-level vision applications, including image classiﬁca-
tion and semantic segmentation, over the ILSVRC2012 and
Pascal VOC 2012 datasets, respectively. The noisy images
(σ = 15, 30, 45, 60) are denoised and then fed into the VGG-
based networks for high-level vision tasks. To evaluate how
different denoising schemes contribute to the performance
of high-level vision tasks, we experiment with the following

(I)

(II)

(I)

(II)

(III)

(IV)

(III)

(IV)

(V)

(VI)

(V)

(VI)

(a)

(b)

Figure 4: (a) Two image denoising examples from Kodak dataset. We show (I) the ground truth image and the zoom-in regions of: (II) the
denoised image by CBM3D; (III) the denoised image by DnCNN; the denoising result of our proposed model (IV) without the guidance of
high-level vision information; (V) with the guidance of high-level vision information and (VI) the ground truth.

Table 2: Classiﬁcation accuracy after denoising noisy image input,
averaged over ILSVRC2012 validation dataset. Red is the best and
blue is the second best results.

Table 3: Segmentation results (mIoU) after denoising noisy image
input, averaged over Pascal VOC 2012 validation dataset. Red is the
best and blue is the second best results.

VGG CBM3D + Separate +

VGG

VGG

Joint

Joint Training
Training (Cross-Task)

VGG CBM3D + Separate +

Joint

Joint Training
Training (Cross-Task)

σ=15

σ=30

σ=45

σ=60

Top-1
Top-5
Top-1
Top-5
Top-1
Top-5
Top-1
Top-5

62.4
84.2
44.4
68.9
24.3
46.1
11.4
26.3

cases:

68.2
88.8
62.3
84.8
55.2
79.4
50.0
74.2

68.3
88.7
62.7
84.9
54.6
78.8
50.1
74.5

69.9
89.5
67.0
87.6
63.0
84.6
59.2
81.8

69.8
89.4
66.4
87.2
62.0
84.0
57.0
80.2

• noisy images are directly fed into the high-level vision
network, termed as VGG. This approach serves as the
baseline;

• noisy images are ﬁrst denoised by CBM3D, and then
termed as

fed into the high-level vision network,
CBM3D+VGG;

• noisy images are denoised via the separately trained de-
noising network, and then fed into the high-level vision
network, termed as Separate+VGG;

• our proposed approach: noisy images are processed by
the cascade of these two networks, which is trained us-

σ=15
σ=30
σ=45
σ=60

56.78
43.43
27.99
14.94

VGG

59.58
55.29
50.69
46.56

VGG

58.70
54.13
49.51
46.59

60.46
57.86
54.83
52.02

60.41
56.29
54.01
51.82

ing the joint loss, termed as Joint Training.

• a denoising network is trained with the classiﬁcation net-
work in our proposed approach, but then is connected to
the segmentation network and evaluated for the task of
semantic segmentation, or vice versa. This is to vali-
date the generality of our denoiser for various high-level
tasks, termed as Joint Training (Cross-Task).

Note that the weights in the high-level vision network are ini-
tialized from a well-trained network under the noiseless set-
ting and not updated during training in our experiments.

Table 2 and Table 3 list the performance of high-level vi-
sion tasks, i.e., top-1 and top5 accuracy for classiﬁcation
and mean intersection-over-union (IoU) without conditional
random ﬁeld (CRF) postprocessing for semantic segmenta-
tion. We notice that the baseline VGG approach obtains

(a)

(b)

(c)

(d)

Figure 5: Two semantic segmentation examples from Pascal VOC 2012 validation set. From left to right: (a) the ground truth image, the
denoised image using (b) the separately trained denoiser, (c) the denoiser trained with the reconstruction and segmentation joint loss, and (d)
the denoiser trained with the classiﬁcation network and evaluated for semantic segmentation. Their corresponding segmentation label maps
are shown below. The zoom-in region which generates inaccurate segmentation in (b) is displayed in the red box.

much lower accuracy than all the other cases, which shows
the necessity of image denoising as a preprocessing step for
high-level vision tasks on noisy data. When we only apply
denoising without considering high-level semantics (e.g., in
CBM3D+VGG and Separate+VGG), it also fails to achieve
high accuracy due to the artifacts introduced by the denoisers.
The proposed Joint Training approach achieves sufﬁciently
high accuracy across various noise levels.

As for the case of Joint Training (Cross-Task), ﬁrst we train
the denoising network jointly with the segmentation network
and then connect this denoiser to the classiﬁcation network.
As shown in Table 2, its accuracy remarkably outperforms the
cascade of a separately trained denoising network and a clas-
siﬁcation network (i.e., Separate+VGG), and is comparable
to our proposed model dedicatedly trained for classiﬁcation
(Joint Training). In addition, we use the denoising network
jointly trained with the classiﬁcation network, to connect the
segmentation network. Its mean IoU is much better than Sep-
arate+VGG in Table 3. These two experiments show the high-
level semantics of different tasks are universal in terms of
low-level vision tasks, which is in line with intuition, and the
denoiser trained in our method has the generality for various
high-level tasks.

Fig. 5 displays two visual examples of how the data-driven
denoising can enhance the semantic segmentation perfor-

mance. It is observed that the segmentation result of the de-
noised image from the separately trained denoising network
has lower accuracy compared to those using the joint loss and
the joint loss (cross-task), while the zoom-in region of its de-
noised image for inaccurate segmentation in Fig. 5 (b) con-
tains oversmoothing artifacts. On the contrary, both the Joint
Training and Joint Training (Cross-Task) approaches achieve
ﬁner segmentation result and produce more visually pleasing
denoised outputs simultaneously.

4 Conclusion

Exploring the connection between low-level vision and high-
level semantic tasks is of great practical value in various ap-
plications of computer vision. In this paper, we tackle this
challenge in a simple yet efﬁcient way by allowing the high-
level semantic information ﬂowing back to the low-level vi-
sion part, which achieves superior performance in both image
denoising and various high-level vision tasks. In our method,
the denoiser trained for one high-level task has the generality
to other high-level vision tasks. Overall, it provides a feasible
and robust solution in a deep learning fashion to real world
problems, which can be used to handle other corruptions [Liu
et al., 2016; Li et al., 2017].

References
[Aharon et al., 2006] Michal Aharon, Michael Elad, and Al-
fred Bruckstein. K-SVD : An algorithm for designing
overcomplete dictionaries for sparse representation. TSP,
54(11):4311–4322, 2006.

[Burger et al., 2012] Harold C Burger, Christian J Schuler,
and Stefan Harmeling. Image denoising: Can plain neural
In CVPR, pages 2392–
networks compete with bm3d?
2399. IEEE, 2012.

[Chen and Pock, 2017] Yunjin Chen and Thomas Pock.
Trainable nonlinear reaction diffusion: A ﬂexible frame-
work for fast and effective image restoration. TPAMI,
39(6):1256–1272, 2017.

[Chen et al., 2014] Liang-Chieh Chen, George Papandreou,
Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Se-
mantic image segmentation with deep convolutional nets
and fully connected crfs. arXiv preprint arXiv:1412.7062,
2014.

[Dabov et al., 2007a] Kostadin Dabov, Alessandro Foi,
Vladimir Katkovnik, and Karen Egiazarian. Color image
denoising via sparse 3d collaborative ﬁltering with group-
ing constraint in luminance-chrominance space. In ICIP,
volume 1, pages I–313. IEEE, 2007.

[Dabov et al., 2007b] Kostadin Dabov, Alessandro Foi,
Vladimir Katkovnik, and Karen Egiazarian. Image denois-
ing by sparse 3-d transform-domain collaborative ﬁltering.
TIP, 16(8):2080–2095, 2007.

[Dong et al., 2013] Weisheng Dong, Lei Zhang, Guangming
Shi, and Xin Li. Nonlocally centralized sparse representa-
tion for image restoration. TIP, 22(4):1620–1630, 2013.
[Gu et al., 2014] Shuhang Gu, Lei Zhang, Wangmeng Zuo,
and Xiangchu Feng. Weighted nuclear norm minimiza-
tion with application to image denoising. In CVPR, pages
2862–2869, 2014.

[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pages 770–778, 2016.

[Huynh-Thu and Ghanbari, 2008] Quan Huynh-Thu

and
Scope of validity of psnr in
Electronics letters,

Mohammed Ghanbari.
image/video quality assessment.
44(13):800–801, 2008.

[Johnson et al., 2016] Justin Johnson, Alexandre Alahi, and
Li Fei-Fei. Perceptual losses for real-time style transfer
and super-resolution. In ECCV, pages 694–711. Springer,
2016.

[Li et al., 2017] Boyi Li, Xiulian Peng, Zhangyang Wang,
Jizheng Xu, and Dan Feng. Aod-net: All-in-one dehaz-
ing network. In ICCV, 2017.

[Liu et al., 2016] Ding Liu, Zhaowen Wang, Bihan Wen,
Jianchao Yang, Wei Han, and Thomas S Huang. Ro-
bust single image super-resolution via deep networks with
sparse prior. IEEE TIP, 25(7):3194–3207, 2016.

[Liu et al., 2017] Ding Liu, Bowen Cheng, Zhangyang
Wang, Haichao Zhang, and Thomas S Huang. Enhance

visual recognition under adverse conditions via deep net-
works. arXiv preprint arXiv:1712.07732, 2017.

[Mairal et al., 2009] J. Mairal, F. Bach, J. Ponce, G. Sapiro,
and A. Zisserman. Non-local sparse models for image
restoration. In ICCV, 2009.

[Mao et al., 2016] Xiaojiao Mao, Chunhua Shen, and Yu-
Image restoration using very deep convolu-
Bin Yang.
tional encoder-decoder networks with symmetric skip con-
nections. In NIPS, pages 2802–2810. 2016.

[Nguyen et al., 2015] Anh Nguyen, Jason Yosinski, and Jeff
Clune. Deep neural networks are easily fooled: High con-
ﬁdence predictions for unrecognizable images. In CVPR,
pages 427–436, 2015.

[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-
cher, and Thomas Brox. U-net: Convolutional networks
In MICCAI, pages
for biomedical image segmentation.
234–241. Springer, 2015.

[Simonyan and Zisserman, 2014] Karen Simonyan and An-
Very deep convolutional networks
arXiv preprint

large-scale image recognition.

drew Zisserman.
for
arXiv:1409.1556, 2014.
[Szegedy et al., 2013] Christian

Szegedy,

Wojciech
Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Intriguing properties of
Goodfellow, and Rob Fergus.
neural networks. arXiv preprint arXiv:1312.6199, 2013.
[Vincent et al., 2008] Pascal Vincent, Hugo Larochelle,
Yoshua Bengio, and Pierre-Antoine Manzagol. Extract-
ing and composing robust features with denoising autoen-
coders. In ICML, pages 1096–1103. ACM, 2008.

[Wang et al., 2016] Zhangyang Wang,

Shiyu Chang,
Yingzhen Yang, Ding Liu, and Thomas S Huang. Study-
ing very low resolution recognition using deep networks.
In CVPR, pages 4792–4800, 2016.

[Wu et al., 2017] Jiqing Wu, Radu Timofte, Zhiwu Huang,
and Luc Van Gool. On the relation between color
arXiv preprint
image denoising and classiﬁcation.
arXiv:1704.01372, 2017.

[Xu et al., 2015] Jun Xu, Lei Zhang, Wangmeng Zuo, David
Zhang, and Xiangchu Feng. Patch group based nonlo-
cal self-similarity prior learning for image denoising. In
CVPR, pages 244–252, 2015.

[Xu et al., 2017] Jun Xu, Lei Zhang, David Zhang, and Xi-
angchu Feng. Multi-channel weighted nuclear norm min-
imization for real color image denoising. 2017.

[Zhang et al., 2017a] Kai Zhang, Wangmeng Zuo, Yunjin
Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian de-
noiser: Residual learning of deep cnn for image denoising.
TIP, 2017.

[Zhang et al., 2017b] Kai Zhang, Wangmeng Zuo, Shuhang
Gu, and Lei Zhang. Learning deep cnn denoiser prior for
image restoration. In CVPR, July 2017.

[Zoran and Weiss, 2011] Daniel Zoran and Yair Weiss. From
learning models of natural image patches to whole image
restoration. In ICCV, pages 479–486. IEEE, 2011.

When Image Denoising Meets High-Level Vision Tasks:
A Deep Learning Approach

Ding Liu1, Bihan Wen1, Xianming Liu2, Zhangyang Wang3, Thomas S. Huang1 ∗
1 University of Illinois at Urbana-Champaign, USA
2 Facebook Inc. 3 Texas A&M University, USA
{dingliu2,bwen3,t-huang1}@illinois.edu, xmliu@fb.com, atlaswang@tamu.edu

Abstract

Conventionally, image denoising and high-level vi-
sion tasks are handled separately in computer vi-
sion. In this paper, we cope with the two jointly
and explore the mutual inﬂuence between them.
First we propose a convolutional neural network for
image denoising which achieves the state-of-the-
art performance. Second we propose a deep neu-
ral network solution that cascades two modules for
image denoising and various high-level tasks, re-
spectively, and use the joint loss for updating only
the denoising network via back-propagation. We
demonstrate that on one hand, the proposed de-
noiser has the generality to overcome the perfor-
mance degradation of different high-level vision
tasks. On the other hand, with the guidance of high-
level vision information, the denoising network can
generate more visually appealing results. To the
best of our knowledge, this is the ﬁrst work inves-
tigating the beneﬁt of exploiting image semantics
simultaneously for image denoising and high-level
vision tasks via deep learning. The code is avail-
able online1.

8
1
0
2
 
r
p
A
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
4
8
2
4
0
.
6
0
7
1
:
v
i
X
r
a

(a)

Noisy input

CBM3D

1 Introduction
A common approach in computer vision is to separate low-
level vision problems, such as image restoration and enhance-
ment, from high-level vision problems, and solve them inde-
pendently. In this paper, we make their connection by show-
ing the mutual inﬂuence between the two, i.e., visual percep-
tion and semantics, and propose a new perspective of solving
both the low-level and high-level computer vision problems
in a single uniﬁed framework, as shown in Fig. 1(a).

Image denoising, as one representative of low-level vision
problems, is dedicated to recovering the underlying image
signal from its noisy measurement. Classical image denois-
ing methods take advantage of local or non-local structures
presented in the image [Aharon et al., 2006; Dabov et al.,
2007b; Mairal et al., 2009; Dong et al., 2013; Gu et al., 2014;

∗Ding Liu and Thomas S. Huang’s research work was supported
by the U.S. Army Research Ofﬁce under Grant W911NF-15-1-0317.
1https://github.com/Ding-Liu/DeepDenoising

Our method

Ground truth

(b)
Figure 1: (a) Upper: conventional semantic segmentation pipeline;
lower: our proposed framework for joint image denoising and se-
mantic segmentation. (b) Zoom-in regions of a noisy input, its de-
noised estimates using CBM3D and our proposed method, as well
as its ground truth.

Xu et al., 2015]. More recently, a number of deep learn-
ing models have been developed for image denoising which
demonstrated superior performance [Vincent et al., 2008;
Burger et al., 2012; Mao et al., 2016; Chen and Pock, 2017;
Zhang et al., 2017a]. Inspired by U-Net [Ronneberger et al.,
2015], we propose a convolutional neural network for image
denoising, which achieves the state-of-the-art performance.

While popular image denoising algorithms reconstruct im-
ages by minimizing the mean square error (MSE), important
image details are usually lost which leads to image quality
degradation, e.g., over-smoothing artifacts in some texture-
rich regions are commonly observed in the denoised output
from conventional methods, as shown in Fig. 1(b). To this

Figure 2: (a) Overview of our proposed denoising network. (b) Architecture of the feature encoding module. (c) Architecture of the feature
decoding module.

end, we propose a cascade architecture connecting image de-
noising to a high-level vision network. We jointly minimize
the image reconstruction loss and the high-level vision loss.
With the guidance of image semantic information, the denois-
ing network is able to further improve visual quality and gen-
erate more visually appealing outputs, which demonstrates
the importance of semantic information for image denoising.
When high-level vision tasks are conducted on noisy
data, an independent image restoration step is typically ap-
plied as preprocessing, which is suboptimal for the ultimate
goal [Wang et al., 2016; Wu et al., 2017; Liu et al., 2017].
Recent research reveals that neural networks trained for im-
age classiﬁcation can be easily fooled by small noise per-
turbation or other artiﬁcial patterns [Szegedy et al., 2013;
Nguyen et al., 2015]. Therefore, an application-driven de-
noiser should be capable of simultaneously removing noise
and preserving semantic-aware details for the high-level vi-
sion tasks. Under the proposed architecture, we systemati-
cally investigate the mutual inﬂuence between the low-level
and high-level vision networks. We show that the cascaded
network trained with the joint loss not only boosts the denois-
ing network performance via image semantic guidance, but
also substantially improves the accuracy of high-level vision
tasks. Moreover, our proposed training strategy makes the
trained denoising network robust enough to different high-
level vision tasks.
In other words, our denoising module
trained for one high-level vision task can be directly plugged
into other high-level tasks without ﬁnetuning either module,
which facilitates the training effort when applied to various
high-level tasks.

2 Method

We ﬁrst introduce the denoising network utilized in our
framework, and then explain the relationship between the im-
age denoising module and the module for high-level vision
tasks in detail.

2.1 Denoising Network
We propose a convolutional neural network for image denois-
ing, which takes a noisy image as input and outputs the re-
constructed image. This network conducts feature contrac-
tion and expansion through downsampling and upsampling

operations, respectively. Each pair of downsampling and up-
sampling operations brings the feature representation into a
new spatial scale, so that the whole network can process in-
formation on different scales.

Speciﬁcally, on each scale, the input is encoded after down-
sampling the features from the previous scale. After feature
encoding and decoding possibly with features on the next
scale, the output is upsampled and fused with the feature
on the previous scale. Such pairs of downsampling and up-
sampling steps can be nested to build deeper networks with
more spatial scales of feature representation, which gener-
ally leads to better restoration performance. Considering the
tradeoff between computation cost and restoration accuracy,
we choose three scales for the denoising network in our ex-
periments, while this framework can be easily extended for
more scales.

These operations together are designed to learn the residual
between the input and the target output and recover as many
details as possible, so we use a long-distance skip connection
to sum the output of these operations and the input image, in
order to generate the reconstructed image. The overview is in
Fig. 2 (a). Each module in this network will be elaborated as
follows.

Feature Encoding: We design one feature encoding mod-
ule on each scale. which is one convolutional layer plus one
residual block as in [He et al., 2016]. The architecture is dis-
played in Fig. 2 (b). Note that each convolutional layer is
immediately followed by spatial batch normalization and a
ReLU neuron. From top to down, the four convolutional lay-
ers have 128, 32, 32 and 128 kernels in size of 3×3, 1×1, 3×3
and 1 × 1, respectively. The output of the ﬁrst convolutioal
layer is passed through a skip connection for element-wise
sum with the output of the last convolutional layer.

Feature Decoding: The feature decoding module is de-
signed for fusing information from two adjacent scales. Two
fusion schemes are tested: (1) concatenation of features on
these two scales; (2) element-wise sum of them. Both of
them obtain similar denosing performance. Thus we choose
the ﬁrst scheme to accommodate feature representations of
different channel numbers from two scales. We use a similar
architecture as the feature encoding module except that the
number of kernels in the four convolutional layers are 256,
64, 64 and 256. Its architecture is in Fig. 2(c).

Figure 3: Overview of our proposed cascaded network.

Feature Downsampling & Upsampling: Downsampling
operations are adopted multiple times to progressively in-
crease the receptive ﬁeld of the following convolution kernels
and to reduce the computation cost by decreasing the feature
map size. The larger receptive ﬁeld enables the kernels to in-
corporate larger spatial context for denoising. We use 2 as the
downsampling and upsampling factors, and try two schemes
for downsampling in the experiments: (1) max pooling with
stride of 2; (2) conducting convolutions with stride of 2. Both
of them achieve similar denoisng performance in practice, so
we use the second scheme in the rest experiments for compu-
tation efﬁciency. Upsampling operations are implemented by
deconvolution with 4 × 4 kernels, which aim to expand the
feature map to the same spatial size as the previous scale.

Since all the operations in our proposed denoising network
are spatially invariant, it has the merit of handling input im-
ages of arbitrary size.

2.2 When Image Denoising Meets High-Level

Vision Tasks

We propose a robust deep architecture processing a noisy im-
age input, via cascading a network for denoising and the other
for high-level vision task, aiming to simultaneously:

1. reconstruct visually pleasing results guided by the high-
level vision information, as the output of the denoisnig
network;

2. attain sufﬁciently good accuracy across various high-
level vision tasks, when trained for only one high-level
vision task;

The overview of the proposed cascaded network is displayed
in Fig. 3. Speciﬁcally, given a noisy input image, the denos-
ing network is ﬁrst applied, and the denoised result is then fed
into the following network for high-level vision task, which
generates the high-level vision task output.

Training Strategy: First we initialize the network for
high-level vision task from a network that is well-trained in
the noiseless setting. We train the cascade of two networks in
an end-to-end manner while ﬁxing the weights in the network
for high-level vision task. Only the weights in the denois-
ing network are updated by the error back-propagated from
the following network for high-level vision task, which is
similar to minimizing the perceptual loss for image super-
resolution [Johnson et al., 2016]. The reason to adopt such
a training strategy is to make the trained denoising network
robust enough without losing the generality for various high-
level vision tasks. More speciﬁcally, our denoising module
trained for one high-level vision task can be directly plugged
into other high-level tasks without ﬁnetuning either the de-
noiser or the high-level network. Our approach not only

facilitates the training effort when applying the denoiser to
different high-level tasks while keeping the high-level vision
network performing consistently for noisy and noise-free im-
ages, but also enables the denoising network to produce high-
quality perceptual and semantically faithful results.

Loss: The reconstruction loss of the denoising network is
the mean squared error (MSE) between the denoising net-
work output and the noiseless image. The losses of the clas-
siﬁcation network and the segmentation network both are the
cross-entropy loss between the predicted label and the ground
truth label. The joint loss is deﬁned as the weighted sum of
the reconstruction loss and the loss for high-level vision task,
which can be represented as

L(F (x), y) = LD(FD(x), ˜x) + λLH (FH (FD(x)), y), (1)

where x is the noisy input image, ˜x is the noiseless image and
y is the ground truth label of high-level vision task. FD, FH
and F denote the denoising network, the network of high-
level vision task and the whole cascaded network, respec-
tively. LD, LH represent the losses of the denoising network
and the high-level vision task network, respectively, while L
is the joint loss, as illustrated in Fig. 3. λ is the weight for
balancing the losses LD and LH .

Image Denoising

3 Experiments
3.1
Our proposed denoising network takes RGB images as input,
and outputs the reconstructed images directly. We add inde-
pendent and identically distributed Gaussian noise with zero
mean to the original image as the noisy input image during
training. We use the training set as in [Chen et al., 2014].
The loss of training is equivalent to Eqn. 1 as λ = 0. We
use SGD with a batch size of 32, and the input patches are
48 × 48 pixels. The initial learning rate is set as 10−4 and
is divided by 10 after every 500,000 iterations. The training
is terminated after 1,500,000 iterations. We train a different
denoising network for each noise level in our experiment.

We compare our denoisnig network with several state-of-
the-art color image denoising approaches on various noise
levels: σ = 25, 35 and 50. We evaluate their denoising per-
formance over the widely used Kodak dataset2, which con-
sists of 24 color images. Table 1 shows the peak signal-
to-noise ratio (PSNR) results for CBM3D [Dabov et al.,
2007a], TNRD [Chen and Pock, 2017], MCWNNM [Xu et
al., 2017], DnCNN [Zhang et al., 2017a], and our proposed
method. We do not list other methods [Burger et al., 2012;

2http://r0k.us/graphics/kodak/

Table 1: Color image denoising results (PSNR) of different methods on Kodak dataset. The best result is shown in bold.

Image CBM3D TNRD MCWNNM DnCNN Proposed CBM3D MCWNNM DnCNN Proposed CBM3D TNRD MCWNNM DnCNN Proposed

σ = 25

σ = 35

σ = 50

01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Average

29.13
32.44
34.54
32.67
29.73
30.59
33.66
29.88
34.06
33.82
31.25
33.76
27.64
30.03
33.08
32.33
32.93
29.83
31.78
33.45
30.99
30.93
34.79
30.09
31.81

27.21
31.44
32.73
31.16
27.81
28.52
31.90
27.38
32.21
31.91
29.51
32.17
25.52
28.50
31.62
30.36
31.20
28.00
30.01
32.00
29.09
29.60
33.68
28.17
30.08

28.66
31.92
34.05
32.42
29.37
30.18
33.36
29.39
33.42
33.23
30.62
33.02
27.19
29.67
32.69
31.79
32.39
29.46
31.29
32.78
30.55
30.48
34.45
29.93
31.35

29.75
32.97
34.97
32.94
30.53
31.05
34.42
30.30
34.59
34.33
31.82
34.12
28.26
30.79
33.32
32.69
33.53
30.40
32.23
34.15
31.61
31.41
35.36
30.79
32.35

29.76
33.00
35.12
33.01
30.55
31.08
34.47
30.37
34.63
34.38
31.84
34.18
28.24
30.80
33.35
32.74
33.50
30.46
32.30
34.29
31.63
31.38
35.40
30.77
32.39

27.31
31.07
32.62
31.02
27.61
28.78
31.64
27.82
32.28
31.97
29.53
32.24
25.70
28.24
31.47
30.64
30.64
28.00
30.19
31.84
29.17
29.36
33.09
28.19
30.04

26.93
30.62
32.27
30.92
27.53
28.44
31.53
27.67
31.76
31.51
29.04
31.52
25.40
28.05
31.15
30.15
30.75
27.70
29.86
31.32
28.86
28.93
32.79
28.17
29.70

28.10
31.65
33.37
31.51
28.66
29.37
32.60
28.53
33.06
32.74
30.23
32.73
26.46
29.17
31.89
31.16
31.96
28.72
30.80
32.73
29.94
29.94
33.86
28.98
30.76

28.11
31.75
33.58
31.59
28.72
29.45
32.70
28.64
33.11
32.83
30.29
32.83
26.47
29.20
31.96
31.23
31.98
28.79
30.88
32.91
29.98
29.95
33.89
29.03
30.83

25.86
29.84
31.34
29.92
25.92
27.34
29.99
26.23
30.86
30.48
28.00
30.98
24.03
26.74
30.32
29.36
29.36
26.41
29.06
30.51
27.61
28.09
31.75
26.62
28.62

24.46
29.12
29.95
28.65
24.37
25.62
28.24
23.93
28.78
28.78
26.75
29.70
22.54
25.67
29.07
27.82
28.07
25.06
27.30
29.24
26.09
27.14
30.53
24.92
27.17

25.28
29.27
30.52
29.37
25.60
26.70
29.51
25.86
30.00
29.63
27.41
30.00
23.70
26.43
29.59
28.53
28.98
25.94
28.44
29.79
27.13
27.47
30.96
26.37
28.02

26.52
30.44
31.76
30.12
26.77
27.74
30.67
26.65
31.42
31.03
28.67
31.32
24.73
27.57
30.50
29.68
30.33
27.03
29.34
31.28
28.27
28.54
32.18
27.18
29.16

26.55
30.54
31.99
30.22
26.87
27.85
30.82
26.84
31.53
31.17
28.76
31.47
24.76
27.63
30.59
29.78
30.40
27.14
29.49
31.53
28.34
28.58
32.30
27.30
29.27

Zoran and Weiss, 2011; Gu et al., 2014; Zhang et al., 2017b]
whose average performance is wore than DnCNN. The imple-
mentation codes used are from the authors’ websites and the
default parameter settings are adopted in our experiments.3

It is clear that our proposed method outperforms all the
competing approaches quantitatively across different noise
levels. It achieves the highest PSNR in almost every image
of Kodak dataset.

3.2 When Image Denoising Meets High-Level

Vision Tasks

We choose two high-level vision tasks as representatives in
our study: image classiﬁcation and semantic segmentation,
which have been dominated by deep network based models.
We utilize two popular VGG-based deep networks in our sys-
tem for each task, respectively. VGG-16 in [Simonyan and
Zisserman, 2014] is employed for image classiﬁcation; we
select DeepLab-LargeFOV in [Chen et al., 2014] for seman-
tic segmentation. We follow the preprocessing protocols (e.g.
crop size, mean removal of each color channel) in [Simonyan
and Zisserman, 2014] and [Chen et al., 2014] accordingly
while training and deploying them in our experiments.

As for the cascaded network for image classiﬁcation
and the corresponding experiments, we train our model on
ILSVRC2012 training set, and evaluate the classiﬁcation ac-
curacy on ILSVRC2012 validation set. λ is empirically set as
0.25. As for the cascaded network for image semantic seg-
mentation and its corresponding experiments, we train our
model on the augmented training set of Pascal VOC 2012 as
in [Chen et al., 2014], and test on its validation set. λ is em-
pirically set as 0.5.

3For TNRD, we denoise each color channel using the grayscale
image denoising implementation which is from the authors’ website.
TNRD for σ = 35 is not publicly available, so we do not include
this case here.

High-Level Vision Information Guided Image Denoising
The typical metric used for image denoising is PSNR, which
has been shown to sometimes correlate poorly with human as-
sessment of visual quality [Huynh-Thu and Ghanbari, 2008].
Since PSNR depends on the reconstruction error between the
denoised output and the reference image, a model trained by
minimizing MSE on the image domain should always outper-
form a model trained by minimizing our proposed joint loss
(with the guidance of high-level vision semantics) in the met-
ric of PSNR. Therefore, we emphasize that the goal of our
following experiments is not to pursue the highest PSNR, but
to demonstrate the qualitative difference between the model
trained with our proposed joint loss and the model trained
with MSE on the image domain.

Fig. 4 displays two image denoising examples from Ko-
dak dataset. A visual comparison is illustrated for a zoom-
(II) and (III) are the denoising results using
in region:
CBM3D [Dabov et al., 2007a], and DnCNN [Zhang et al.,
2017a], respectively; (IV) is the proposed denoiser trained
separately without the guidance of high-level vision infor-
mation; (V) is the denoising result using the proposed de-
noising network trained jointly with a segmentation network.
We can ﬁnd that the results using CBM3D, DnCNN and our
separately trained denoiser generate oversmoothing regions,
while the jointly trained denoising network is able to recon-
struct the denoised image which preserves more details and
textures with better visual quality.

Generality of the Denoiser for High-Level Vision Tasks
We now investigate how the image denoising can enhance
the high-level vision applications, including image classiﬁca-
tion and semantic segmentation, over the ILSVRC2012 and
Pascal VOC 2012 datasets, respectively. The noisy images
(σ = 15, 30, 45, 60) are denoised and then fed into the VGG-
based networks for high-level vision tasks. To evaluate how
different denoising schemes contribute to the performance
of high-level vision tasks, we experiment with the following

(I)

(II)

(I)

(II)

(III)

(IV)

(III)

(IV)

(V)

(VI)

(V)

(VI)

(a)

(b)

Figure 4: (a) Two image denoising examples from Kodak dataset. We show (I) the ground truth image and the zoom-in regions of: (II) the
denoised image by CBM3D; (III) the denoised image by DnCNN; the denoising result of our proposed model (IV) without the guidance of
high-level vision information; (V) with the guidance of high-level vision information and (VI) the ground truth.

Table 2: Classiﬁcation accuracy after denoising noisy image input,
averaged over ILSVRC2012 validation dataset. Red is the best and
blue is the second best results.

Table 3: Segmentation results (mIoU) after denoising noisy image
input, averaged over Pascal VOC 2012 validation dataset. Red is the
best and blue is the second best results.

VGG CBM3D + Separate +

VGG

VGG

Joint

Joint Training
Training (Cross-Task)

VGG CBM3D + Separate +

Joint

Joint Training
Training (Cross-Task)

σ=15

σ=30

σ=45

σ=60

Top-1
Top-5
Top-1
Top-5
Top-1
Top-5
Top-1
Top-5

62.4
84.2
44.4
68.9
24.3
46.1
11.4
26.3

cases:

68.2
88.8
62.3
84.8
55.2
79.4
50.0
74.2

68.3
88.7
62.7
84.9
54.6
78.8
50.1
74.5

69.9
89.5
67.0
87.6
63.0
84.6
59.2
81.8

69.8
89.4
66.4
87.2
62.0
84.0
57.0
80.2

• noisy images are directly fed into the high-level vision
network, termed as VGG. This approach serves as the
baseline;

• noisy images are ﬁrst denoised by CBM3D, and then
termed as

fed into the high-level vision network,
CBM3D+VGG;

• noisy images are denoised via the separately trained de-
noising network, and then fed into the high-level vision
network, termed as Separate+VGG;

• our proposed approach: noisy images are processed by
the cascade of these two networks, which is trained us-

σ=15
σ=30
σ=45
σ=60

56.78
43.43
27.99
14.94

VGG

59.58
55.29
50.69
46.56

VGG

58.70
54.13
49.51
46.59

60.46
57.86
54.83
52.02

60.41
56.29
54.01
51.82

ing the joint loss, termed as Joint Training.

• a denoising network is trained with the classiﬁcation net-
work in our proposed approach, but then is connected to
the segmentation network and evaluated for the task of
semantic segmentation, or vice versa. This is to vali-
date the generality of our denoiser for various high-level
tasks, termed as Joint Training (Cross-Task).

Note that the weights in the high-level vision network are ini-
tialized from a well-trained network under the noiseless set-
ting and not updated during training in our experiments.

Table 2 and Table 3 list the performance of high-level vi-
sion tasks, i.e., top-1 and top5 accuracy for classiﬁcation
and mean intersection-over-union (IoU) without conditional
random ﬁeld (CRF) postprocessing for semantic segmenta-
tion. We notice that the baseline VGG approach obtains

(a)

(b)

(c)

(d)

Figure 5: Two semantic segmentation examples from Pascal VOC 2012 validation set. From left to right: (a) the ground truth image, the
denoised image using (b) the separately trained denoiser, (c) the denoiser trained with the reconstruction and segmentation joint loss, and (d)
the denoiser trained with the classiﬁcation network and evaluated for semantic segmentation. Their corresponding segmentation label maps
are shown below. The zoom-in region which generates inaccurate segmentation in (b) is displayed in the red box.

much lower accuracy than all the other cases, which shows
the necessity of image denoising as a preprocessing step for
high-level vision tasks on noisy data. When we only apply
denoising without considering high-level semantics (e.g., in
CBM3D+VGG and Separate+VGG), it also fails to achieve
high accuracy due to the artifacts introduced by the denoisers.
The proposed Joint Training approach achieves sufﬁciently
high accuracy across various noise levels.

As for the case of Joint Training (Cross-Task), ﬁrst we train
the denoising network jointly with the segmentation network
and then connect this denoiser to the classiﬁcation network.
As shown in Table 2, its accuracy remarkably outperforms the
cascade of a separately trained denoising network and a clas-
siﬁcation network (i.e., Separate+VGG), and is comparable
to our proposed model dedicatedly trained for classiﬁcation
(Joint Training). In addition, we use the denoising network
jointly trained with the classiﬁcation network, to connect the
segmentation network. Its mean IoU is much better than Sep-
arate+VGG in Table 3. These two experiments show the high-
level semantics of different tasks are universal in terms of
low-level vision tasks, which is in line with intuition, and the
denoiser trained in our method has the generality for various
high-level tasks.

Fig. 5 displays two visual examples of how the data-driven
denoising can enhance the semantic segmentation perfor-

mance. It is observed that the segmentation result of the de-
noised image from the separately trained denoising network
has lower accuracy compared to those using the joint loss and
the joint loss (cross-task), while the zoom-in region of its de-
noised image for inaccurate segmentation in Fig. 5 (b) con-
tains oversmoothing artifacts. On the contrary, both the Joint
Training and Joint Training (Cross-Task) approaches achieve
ﬁner segmentation result and produce more visually pleasing
denoised outputs simultaneously.

4 Conclusion

Exploring the connection between low-level vision and high-
level semantic tasks is of great practical value in various ap-
plications of computer vision. In this paper, we tackle this
challenge in a simple yet efﬁcient way by allowing the high-
level semantic information ﬂowing back to the low-level vi-
sion part, which achieves superior performance in both image
denoising and various high-level vision tasks. In our method,
the denoiser trained for one high-level task has the generality
to other high-level vision tasks. Overall, it provides a feasible
and robust solution in a deep learning fashion to real world
problems, which can be used to handle other corruptions [Liu
et al., 2016; Li et al., 2017].

References
[Aharon et al., 2006] Michal Aharon, Michael Elad, and Al-
fred Bruckstein. K-SVD : An algorithm for designing
overcomplete dictionaries for sparse representation. TSP,
54(11):4311–4322, 2006.

[Burger et al., 2012] Harold C Burger, Christian J Schuler,
and Stefan Harmeling. Image denoising: Can plain neural
In CVPR, pages 2392–
networks compete with bm3d?
2399. IEEE, 2012.

[Chen and Pock, 2017] Yunjin Chen and Thomas Pock.
Trainable nonlinear reaction diffusion: A ﬂexible frame-
work for fast and effective image restoration. TPAMI,
39(6):1256–1272, 2017.

[Chen et al., 2014] Liang-Chieh Chen, George Papandreou,
Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Se-
mantic image segmentation with deep convolutional nets
and fully connected crfs. arXiv preprint arXiv:1412.7062,
2014.

[Dabov et al., 2007a] Kostadin Dabov, Alessandro Foi,
Vladimir Katkovnik, and Karen Egiazarian. Color image
denoising via sparse 3d collaborative ﬁltering with group-
ing constraint in luminance-chrominance space. In ICIP,
volume 1, pages I–313. IEEE, 2007.

[Dabov et al., 2007b] Kostadin Dabov, Alessandro Foi,
Vladimir Katkovnik, and Karen Egiazarian. Image denois-
ing by sparse 3-d transform-domain collaborative ﬁltering.
TIP, 16(8):2080–2095, 2007.

[Dong et al., 2013] Weisheng Dong, Lei Zhang, Guangming
Shi, and Xin Li. Nonlocally centralized sparse representa-
tion for image restoration. TIP, 22(4):1620–1630, 2013.
[Gu et al., 2014] Shuhang Gu, Lei Zhang, Wangmeng Zuo,
and Xiangchu Feng. Weighted nuclear norm minimiza-
tion with application to image denoising. In CVPR, pages
2862–2869, 2014.

[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pages 770–778, 2016.

[Huynh-Thu and Ghanbari, 2008] Quan Huynh-Thu

and
Scope of validity of psnr in
Electronics letters,

Mohammed Ghanbari.
image/video quality assessment.
44(13):800–801, 2008.

[Johnson et al., 2016] Justin Johnson, Alexandre Alahi, and
Li Fei-Fei. Perceptual losses for real-time style transfer
and super-resolution. In ECCV, pages 694–711. Springer,
2016.

[Li et al., 2017] Boyi Li, Xiulian Peng, Zhangyang Wang,
Jizheng Xu, and Dan Feng. Aod-net: All-in-one dehaz-
ing network. In ICCV, 2017.

[Liu et al., 2016] Ding Liu, Zhaowen Wang, Bihan Wen,
Jianchao Yang, Wei Han, and Thomas S Huang. Ro-
bust single image super-resolution via deep networks with
sparse prior. IEEE TIP, 25(7):3194–3207, 2016.

[Liu et al., 2017] Ding Liu, Bowen Cheng, Zhangyang
Wang, Haichao Zhang, and Thomas S Huang. Enhance

visual recognition under adverse conditions via deep net-
works. arXiv preprint arXiv:1712.07732, 2017.

[Mairal et al., 2009] J. Mairal, F. Bach, J. Ponce, G. Sapiro,
and A. Zisserman. Non-local sparse models for image
restoration. In ICCV, 2009.

[Mao et al., 2016] Xiaojiao Mao, Chunhua Shen, and Yu-
Image restoration using very deep convolu-
Bin Yang.
tional encoder-decoder networks with symmetric skip con-
nections. In NIPS, pages 2802–2810. 2016.

[Nguyen et al., 2015] Anh Nguyen, Jason Yosinski, and Jeff
Clune. Deep neural networks are easily fooled: High con-
ﬁdence predictions for unrecognizable images. In CVPR,
pages 427–436, 2015.

[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-
cher, and Thomas Brox. U-net: Convolutional networks
In MICCAI, pages
for biomedical image segmentation.
234–241. Springer, 2015.

[Simonyan and Zisserman, 2014] Karen Simonyan and An-
Very deep convolutional networks
arXiv preprint

large-scale image recognition.

drew Zisserman.
for
arXiv:1409.1556, 2014.
[Szegedy et al., 2013] Christian

Szegedy,

Wojciech
Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Intriguing properties of
Goodfellow, and Rob Fergus.
neural networks. arXiv preprint arXiv:1312.6199, 2013.
[Vincent et al., 2008] Pascal Vincent, Hugo Larochelle,
Yoshua Bengio, and Pierre-Antoine Manzagol. Extract-
ing and composing robust features with denoising autoen-
coders. In ICML, pages 1096–1103. ACM, 2008.

[Wang et al., 2016] Zhangyang Wang,

Shiyu Chang,
Yingzhen Yang, Ding Liu, and Thomas S Huang. Study-
ing very low resolution recognition using deep networks.
In CVPR, pages 4792–4800, 2016.

[Wu et al., 2017] Jiqing Wu, Radu Timofte, Zhiwu Huang,
and Luc Van Gool. On the relation between color
arXiv preprint
image denoising and classiﬁcation.
arXiv:1704.01372, 2017.

[Xu et al., 2015] Jun Xu, Lei Zhang, Wangmeng Zuo, David
Zhang, and Xiangchu Feng. Patch group based nonlo-
cal self-similarity prior learning for image denoising. In
CVPR, pages 244–252, 2015.

[Xu et al., 2017] Jun Xu, Lei Zhang, David Zhang, and Xi-
angchu Feng. Multi-channel weighted nuclear norm min-
imization for real color image denoising. 2017.

[Zhang et al., 2017a] Kai Zhang, Wangmeng Zuo, Yunjin
Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian de-
noiser: Residual learning of deep cnn for image denoising.
TIP, 2017.

[Zhang et al., 2017b] Kai Zhang, Wangmeng Zuo, Shuhang
Gu, and Lei Zhang. Learning deep cnn denoiser prior for
image restoration. In CVPR, July 2017.

[Zoran and Weiss, 2011] Daniel Zoran and Yair Weiss. From
learning models of natural image patches to whole image
restoration. In ICCV, pages 479–486. IEEE, 2011.

