8
1
0
2
 
p
e
S
 
5
 
 
]

G
L
.
s
c
[
 
 
1
v
5
0
6
1
0
.
9
0
8
1
:
v
i
X
r
a

Anomaly Detection in the Presence of Missing Values

Thomas G. Dietterich
Oregon State University
Corvallis, Oregon
tgd@cs.orst.edu

Tadesse Zemicheal
Oregon State University
Corvallis, Oregon
zemichet@oregonstate.edu

ABSTRACT
Standard methods for anomaly detection assume that all features are
observed at both learning time and prediction time. Such methods
cannot process data containing missing values. This paper studies
five strategies for handling missing values in test queries: (a) mean
imputation, (b) MAP imputation, (c) reduction (reduced-dimension
anomaly detectors via feature bagging), (d) marginalization (for
density estimators only), and (e) proportional distribution (for tree-
based methods only). Our analysis suggests that MAP imputation
and proportional distribution should give better results than mean
imputation, reduction, and marginalization. These hypotheses are
largely confirmed by experimental studies on synthetic data and on
anomaly detection benchmark data sets using the Isolation Forest
(IF), LODA, and EGMM anomaly detection algorithms. However,
marginalization worked surprisingly well for EGMM, and there
are exceptions where reduction works well on some benchmark
problems. We recommend proportional distribution for IF, MAP
imputation for LODA, and marginalization for EGMM.

CCS CONCEPTS
• Computing methodologies → Machine learning; Anomaly
detection;

ACM Reference Format:
Thomas G. Dietterich and Tadesse Zemicheal. 2018. Anomaly Detection
in the Presence of Missing Values. In Proceedings of ACM SIGKDD 2018
Workshop August 20th, London UK (ODD’18). ACM, New York, NY, USA,
Article 4, 8 pages. https://doi.org/10.475/123_4

1 INTRODUCTION
In supervised learning, the problem of missing values in training
and test data has been studied for many years [10, 12]. In the past
decade, many general purpose algorithms have been developed for
anomaly detection [2, 3, 15]. However, with few exceptions, there
are no published methods for handling missing values at either
training time or prediction time for anomaly detection algorithms.
This paper presents five methods for handling missing values in
test queries. The paper defines the methods, analyzes them formally,
and studies them experimentally. The paper is organized as follows.
First, we describe three of the leading general purpose anomaly
detection algorithms, the Isolation Forest (IF) [6], LODA [7], and
EGMM [3]. Recent benchmarking studies by Emmott et al. [3] have

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ODD’18, 2018
© 2018 Copyright held by the owner/author(s).
ACM ISBN 123-4567-24-567/08/06. . . $15.00
https://doi.org/10.475/123_4

shown that these are among the most effective algorithms in the
literature. Then we review existing research on general methods
for handling missing values in machine learning and describe how
these general methods are instantiated for IF, LODA, and EGMM.
We then define and analyze the methods formally and develop a
set of hypotheses. Finally, we test these hypotheses on synthetic
data and benchmark data sets and summarize the results. From the
analysis and experiments, we conclude that the method of MAP
imputation works well across all three algorithms but that for IF,
the method of proportional distribution is more efficient, and for
EGMM, the method of marginalization gives better performance.

2 UNSUPERVISED ANOMALY DETECTION

ALGORITHMS

We study the standard setting of unsupervised anomaly detection.
At training time, we are given a data set Dtrain = {X1, . . . , XN },
where Xi ∈ Rd . We will index the features of each Xi by the sub-
script j = 1, . . . , d. The data comprise an unknown (and unlabeled)
mixture of “nominal” and “anomaly” points. We assume that the
anomalies make up only a small fraction of Dtrain. At test time, we
are given a second data set Dtest that is also a mixture of nomi-
nal and anomaly points. An anomaly detection algorithm analyzes
Dtrain and constructs an anomaly detector A : Rd (cid:55)→ R, which
is a function that assigns a real-valued anomaly score A(Xq ) to a
d-dimensional query vector Xq . The ideal anomaly detector assigns
high scores to the anomalous points and low scores to the nominal
points such that we can set a threshold that perfectly separates
these two classes. Anomaly detectors are typically evaluated ac-
cording to the area under the ROC curve (AUC), which is equal
to the probability that the anomaly detector will correctly rank a
randomly-chosen anomaly point above a randomly-chosen nominal
point.

The definition of an anomaly is domain-dependent, but our basic
assumption is that the anomaly points are being generated by a
process that is different from the process that generates the nominal
points. Virtually all algorithms for unsupervised anomaly detection
work by trying to identify outliers and then assuming that the
outliers are the anomalies. We call this strategy “anomaly detection
by outlier detection”.

We now describe each of the three anomaly detection algorithms

studied in this paper.

2.1 Isolation Forest
The Isolation Forest (IF) algorithm constructs an ensemble of isola-
tion trees from the training data. Each isolation tree is a binary tree
where each internal node n consists of a threshold test X j ≥ θn .
IF constructs the isolation tree top-down starting with the entire
training set Dtrain at the root node. At each node n, IF selects a

ODD’18, 2018

Thomas G. Dietterich and Tadesse Zemicheal

feature j at random, computes the minimum M j and maximum M j
values of the data Dn that arrive at this node, selects θn uniformly
at random from the interval (M j , M j ), and then splits the data into
two sets: Dleft of points Xi for which Xi, j ≥ θn and Dright of points
Xi for which Xi, j < θn . Splitting continues until every data point
is isolated in its own leaf node. In the IF paper, the authors describe
methods for early termination of the tree-growing process, which
can give large speedups, but we do not employ those in this paper.
IF creates an ensemble of trees by building each tree from a
subset of the training data. In the original IF paper, the authors
grow an ensemble of 100 trees using samples of size 256 drawn
uniformly without replacement from the training data.

At test time, an anomaly score is assigned as follows. The query
point Xq is “dropped” down each tree t = 1, . . . ,T to compute
an isolation depths d1, . . . , dT . The anomaly score is computed by
normalizing the average d of these isolation depths according to
the formula A(X ) = exp[− d
z ], where z is a theoretically-determined
expected isolation depth (computed from N ). Hence, the smaller
the value of d, the larger the anomaly score. The computation of
the isolation depth is somewhat subtle. Consider query point Xq
as it reaches node n at depth d in the tree. If n is a leaf node, then
the isolation depth is d. If n is an internal node (with test feature j
and threshold θj ), then an early termination test is performed. If
Xq, j < M j or Xq, j > M j , then the isolation depth is d. Otherwise,
Xq is routed to the left or right child node depending on the outcome
of the test Xq, j ≥ θj .

2.2 Lightweight Online Detector Of Anomalies

(LODA)

The LODA algorithm constructs an ensemble of T one-dimensional
histogram density estimators. Each density estimator, pt , t = 1, . . . ,T
is constructed as follows. First, LODA constructs a projection vector
wt by starting with the all-zero vector, selecting k =
d features at
random, and replacing those positions in wt with standard normal
random variates. Each training example Xi is then projected to the
real line as w⊤
t Xi , and a fixed-width histogram density estimator pt
is estimated via the method of Birgé and Rozenholc [1]. To compute
the anomaly score of a query point Xq , LODA computes the average
log density f (Xq ) = 1/T (cid:205)T
t =1 − log pt (w⊤
t Xq ). In our experiments,
we set T = 100.

√

2.3 Ensemble of Gaussian Mixture Models

(EGMM)

The EGMM algorithm [3, 13] learns an ensemble of Gaussian mix-
ture models (GMMs) by fitting 15 GMMs to bootstrap replicates of
the training data for each number of mixture components k = 3, 4, 5.
This gives 45 GMMs. For each k, we compute the mean out-of-bag
log likelihood and keep only those models corresponding to values
of k whose mean log likelihood was at least 85% of the log likeli-
hood for the best k. This model selection step discards values of k
whose GMMs that do not fit the data well. The anomaly score for
each test query xq is computed as 1/L (cid:205)L
− log pℓ(xq ), where L
is the number of fitted GMMs and pℓ(x) is the density assigned by
GMM ℓ to x.

ℓ=1

3 MISSING VALUES AND METHODS FOR

HANDLING THEM

Rubin [11] introduced a taxonomy of different kinds of missingness.
In this paper, we study feature values missing completely at ran-
dom (MCAR). In this form of missingness, the process that causes
features to be missing in X is completely independent from all of
the feature values in X .

Let P be the probability density function of the population from
which the training and test data sets Dtrain and Dtest are drawn.
Note that P assigns non-zero density to both nominal points and
anomaly points. Let A : Rd (cid:55)→ R be an anomaly detector that
assigns a real-valued anomaly score to each point X ∈ Rd .

Let J = {1, . . . , d} be a set indexing the features and M ⊂ J be a
set of feature values that are missing in a query instance Xq . Let
Xq [M] denote the features of Xq indexed by M and Xq [−M] be the
features indexed by the complementary set J \ M.

Definition 3.1. The ideal method for handling MCAR missing
values is to compute the expected value of the anomaly score con-
ditioned on the observed features:

A(X [−M]) =

P(X [M] | X [−M])A(X )dX [M].

∫

X [M ]

Evaluating this high-dimensional integral is generally infeasible.
One alternative is to approximate the integral by the conditional
MAP estimate:

Definition 3.2. MAP imputation computes

ˆX [M] := arg max
X [M ]

P(X [M] | X [−M])

and returns the anomaly score A([X [−M], ˆX [M]]).

An even weaker form of imputation is to just use the uncondi-

tional means:

Definition 3.3. Mean imputation estimates the mean value

X :=

P(X )XdX

∫

X

and then returns the anomaly score A[X [−M], X [M]].

An alternative to approximating the ideal method is to learn a
separate anomaly detector for xq that considers only the features
in −M. Following the work on Sarr-Tsechansky and Provost, we
will call this the “reduced” method.

Definition 3.4. The reduced method for handling missing values
is to train a separate anomaly detector A[−M] based on only the
features in the set −M and return the score A[−M](X [−M]).

The reduced method can be directly implemented by EGMM
by marginalizing out the missing features prior to computing the
surprise:

A[M](X [M]) = − log

P([X [M], −X [M]])dX [M].

∫

X [M ]

This is easily done for each Gaussian component in the each GMM
in EGMM.

Anomaly Detection in the Presence of Missing Values

ODD’18, 2018

It is informative to contrast this with the definition of the ideal
method. If we employed log surprise as our anomaly detector under
the ideal method, the anomaly score would be

A(X [−M]) =

P(X [M] | X [−M])(− log P([X [M], X [−M]]))dX [M],

∫

X [M ]

where the integral is outside the logarithm.

A disadvantage of the reduced method is that we must compute a
separate anomaly detector A[−M] for each pattern of non-missing
values. While this is easy for EGMM, it is much more difficult
for other anomaly detectors. One way to approximate this is to
create an ensemble of lower-dimensional anomaly detectors as
follows. Let A[−M1], . . . , A[−ML] be a set of lower-dimensional
anomaly detectors constructed using L different feature subsets
−M1, . . . , −ML. A common way to do this is known as“feature
bagging". Define each subset −Mi by selecting a set features of
size k < d at random and then training an anomaly detector using
only those features. Then, given a query X [−M], we can return
the average anomaly score using all anomaly detectors in the set
U [M] = {A[−Mi ] : −Mi ⊆ −M }.
Areduced(X [−M]) = 1
|U |

A(X [−M]).

(cid:213)

A∈U [M ]

This is exactly the method that was introduced by Pevn`y in LODA
[7]. It can be applied to the Isolation Forest by growing each isola-
tion tree using a subset −Mi of features.

For tree-based classifiers, Quinlan [9] introduced a method that
we will call “proportional distribution”. This method can be applied
by Isolation Forest as follows. Consider computing the isolation
depth in an isolation tree for a query point Xq . When the query
reaches internal node n with test X j ≥ θn , if Xq, j is missing, then
the query is sent down both the left and right child trees, which
recursively return isolation depth estimates dleft and dright. Node
n then returns the depth 1 + Pleftdleft
+ Prightdright, where Pleft
is the proportion of training examples for which X j ≥ θn and
= 1 − Pleft. We can view proportional distribution as an
Pright
approximation of the ideal method.

In this paper, we employ an approximation to MAP imputation
based on the R “mice” package [14] as reimplemented in the python
“fancyimpute” package. The mice imputation procedure works as
follows. First, it fills in each missing value via mean imputation.
Then it makes repeated Gibbs sampling passes through the test
data set. In each pass and for each feature j, mice fits a Bayesian
linear ridge regression model to predict X [j] as a function of all of
the remaining variables. We set the ridge regularization parameter
to 0.01. Let β[j] denote the fitted regression parameters. To impute
each missing value for feature j, mice samples a value from the
posterior predictive distribution: Xq [j] ∼ P(X [j]|Xq [−j], β). We
perform 110 passes and discard the first 10 (the “burn-in period”).
The final imputation for each feature is the mean of the 100 values
imputed during the Gibbs sampling.

3.1 Analysis of Missing Values Methods
Let us consider the situations under which we expect the various
missing values methods to work well. What do we mean by “work
well”? Let 0 ≤ ρ < 1 be the proportion of feature values that
are missing in a test query. We expect that as ρ increases, the

Figure 1: A two-dimensional illustration of correlated fea-
ture values. The MAP imputation is the green square; the
mean imputation is the blue square.

performance of the anomaly detection algorithms will decay, even
when we apply missing values methods. We will say that a method
works well if its performance decays slowly as ρ increases.

Imputation is only possible if there are correlations among the
feature values in the data. Hence, if all of the feature values are
independent, we would not expect any of the imputation methods
to work well—they should all give equivalent performance.

Now consider problems with feature correlations. When will
MAP imputation work better than mean imputation? Precisely
when the MAP estimate is different from the mean estimate. Figure 1
shows a simple example of a data set with 90% nominal (red) and
10% anomaly (turquoise) points. The two features are strongly
correlated so that if we observe the query Xq = (N A, −3.2), we can
impute the missing value X1. MAP imputation finds the highest
density point along the line X2 = −3.2, which is at X1 = −2.31,
whereas mean imputation uses the overall mean value of X1 which
is X1 = −1.20. Because the mean-imputed value (−1.20, −3.2) lies
in a low-density region, the anomaly detectors will assign it a high
anomaly score, whereas the MAP-imputed value of (−2.31, −3.2)
lies in a higher-density region and will be assigned a lower anomaly
score.

How does proportional distribution behave? Figure 2 shows what
it does on this same example data set. Each isolation tree divides
the feature space into axis-parallel rectangles recursively. In the
figure, we only show the relevant splitting thresholds to depth 5
for a single tree. Each of the four shaded regions corresponds to a
subtree rooted at depth 5, and the final isolation depth will be the
weighted average of the isolation depths in these four regions. We
can see that this is a crude approximation of integrating the isolation
depth along the dashed line, which is what the ideal method would
do. We can expect proportional distribution to work better than
MAP imputation in cases where the conditional distribution of a
missing value is multi-modal. MAP imputation will choose one
mode; mice might average values from multiple modes and end up

ODD’18, 2018

Thomas G. Dietterich and Tadesse Zemicheal

distribution did well in data sets where it was difficult to impute
missing values correctly. But in data sets where imputation was
possible—typically because of correlated information across multi-
ple features—imputation generally worked better. They concluded
that decision tree algorithms fail to include all redundant features
when growing the tree and, consequently, proportional distribution
cannot recover well from missing values. Experiments by Quin-
lan [8, 9] also suggested that proportion distribution works best
if the missing also missing at training time so that the decision
tree is forced to learn to recover from missing values by testing
additional features. Randomized ensembles, such as random forests
and isolation forests, are less likely to suffer from this problem.

Hao [4] found that proportional distribution also works well for
decision tree ensembles trained via gradient boosting. His studies
focused on missing values in sequence-to-sequence mapping using
conditional random fields.

The method that performed best in the studies by Sarr-Tsechansky
and Provost was the “reduced” approach of training a separate clas-
sifier for each pattern of missing values. The main drawback is the
cost in computation time and memory.

5 EXPERIMENTAL STUDY
To assess these methods for handling missing values, we conducted
two studies: a study on synthetic data sets and a study on several
UCI data sets [5]. We implemented our own versions of IF, LODA,
and EGMM to support the missing value methods described above.

5.1 Synthetic data
The goal of our synthetic data experiments is to evaluate the analy-
sis of Section 3.1. We designed four synthetic data sets, each con-
taining 3000 examples in 8-dimensional Euclidean space. In each
data set, 10% of the points are anomalies and the remaining 90%
are nominals. We describe each data set in turn along with the
hypotheses that it is intended to test. Each experiment is repeated
on 20 replications of each data set.

Uncorrelated: In this data set, all features are independent.
Nominals are sampled from a standard normal N (0, I ) and anom-
alies are sampled from N (3, I ). Our analysis predicts that mean
imputation, MAP imputation, and proportional distribution will
give very similar results while the reduced method may suffer from
using lower dimensional projections of the data.

Noise: For the second data set, we added 5 noise features to the
samples in the uncorrelated data set. The noise features are sampled
from Unif(−1, +1). We hypothesize that the reduced method may do
better on this, because the additional noise features might confuse
the imputation methods by introducing added variance.

Correlated: In this data set, points are generated from “cor-
related” multivariate distributions. Details of the data generation
process are given in the appendix. Our analysis predicts that MAP
imputation and proportional distribution will work better on corre-
lated data than mean imputation. We expect the reduced method
to perform worse because of its low-dimensional projections.

Mixture: The data points are sampled from a Gaussian mixture
model (GMM; see appendix for details) consisting of three clusters
of nominal points positioned at the vertices of a triangle in 8-d
space. The anomaly points are generated from a single Gaussian

Figure 2: Illustration of proportional distribution. Thin ver-
tical and horizontal lines correspond to thresholds θj at var-
ious internal nodes in an isolation tree. Shaded regions in-
dicate the subtrees whose isolation depths will be weighted
and summed by proportional distribution.

Figure 3: Illustration of EGMM behavior. X1 is marginalized
away to obtain P(X2) and then the density at X2 = −3.2 gives
the anomaly score.

with an imputed value in the middle of empty space. Proportional
distribution will integrate (approximately) across all modes.

Finally, Figure 3 shows how the marginalization method works.
It plots the density P(X2) = ∫
P(X1, X2)dX1. The value at P(X2 =
−3.2) is fairly large, so EGMM will correctly assign a low anomaly
score to this example.

X1

4 PREVIOUS STUDIES OF MISSING VALUES

IN SUPERVISED LEARNING

Saar-Tsechansky and Provost [12] provide an excellent review of
methods for handling missing values in supervised learning. They
studied mean imputation, proportional distribution, and reduced
classifiers. Sarr-Tsechansky and Provost found that proportional

Anomaly Detection in the Presence of Missing Values

ODD’18, 2018

positioned near the center of that triangle. Our analysis suggests
that proportional distribution may perform better on mixtures than
either MAP imputation or mean imputation.

For each data set and each anomaly detection algorithm, we
performed the following experiment. First, we applied the anomaly
detection algorithm to the data set to obtain a trained anomaly
detector. Then we created nine test data sets by inserting fraction
ρ of missing values in each data point in the data set, for ρ =
0, 0.1, . . . , 0.8. If ρ × d was not an integer, then we inserted missing
values such that on average, fraction ρ of the values were missing
in each query example. For example, suppose ρ = 0.3, d = 8, and
there are 1000 instances in the data set. To achieve an average of
2.4 missing values per instance, we insert 2 missing values in 600 of
the instances and 3 missing values into the remaining 400 instances
(chosen uniformly at random without replacement).

√

We then measured the AUC of the anomaly detector with each
of its missing values methods on each test data set. For IF, the
methods are mean imputation, mice imputation, proportional dis-
tribution, and reduced (using feature bagging with
d features
in each isolation tree). For LODA, the methods are mean impu-
tation, mice imputation, and reduced. For EGMM, the methods
are mean imputation, mice imputation, and marginalization. The
AUCs of the 20 replicates of each condition (e.g., mixture + EGMM
+ marginalization) are averaged to obtain a final AUC. We produce
two summaries of this data: decay plots and 50% missingness plots.
Figure 4 shows how the AUC decays as ρ increases. To normalize
across the four synthetic data sets, we compute the relative AUC,
which is equal to the AUC of each method divided by the AUC
of the anomaly detection algorithm when ρ = 0. These relative
AUC values are then averaged across the four synthetic data sets
(i.e., over 20 replications of each synthetic configuration for a total
of AUC 80 values). We observe that for IF, mice and proportional
distribution give much better results than mean imputation and
the reduced method. Note that the reduced method fails completely
for ρ = 0.7 and ρ = 0.8, because there are fewer than
d non-
missing features, so none of the low-dimensional isolation trees
can be evaluated. Similar plots for LODA and EGMM are given in
the appendix.

√

The second summary, shown in Figure 5, considers only ρ = 0.5
and plots the relative AUC for each of the four synthetic configu-
rations. This allows us to test the predictions arising from on our
analysis. As expected, on the uncorrelated data, all methods give
very similar performance, except that the reduced method is worse
than the three imputation methods. Adding noise slightly improves
the performance of all methods, although this is not statistically
significant. On correlated data, mice and proportional distribution
perform well, but mean imputation begins to suffer somewhat. Fi-
nally, on the mixture data, we observe that proportional distribution
is the best, as we predicted from our analysis. Mean imputation
gives very bad results (because the mean often lies far from all of
the data points), and the reduced method performs quite well.

The experiments confirm our hypotheses with the exception

that adding noise did not have any significant effect.

Figure 4: Mean relative AUC of IF as a function of the frac-
tion of missing values, ρ, averaged across the four synthetic
data set configurations.

Figure 5: Relative AUC of IF on the synthetic data sets for
ρ = 0.5, in ascending order by mice imputation.

5.2 UCI Benchmark Data Sets
The goal of our second study is to see how well the missingness
methods work on realistic data. We worked with the anomaly detec-
tion benchmarks constructed by Andrew Emmott [3] based on 13
supervised learning data sets from the UCI repository[5]. Emmott
refers to these 13 data sets as “mother sets”. He defines a mother
set by selecting some of the classes to be anomalies and the rest
of the classes to be nominal data points. He has created thousands
of benchmark data sets by systematically varying the proportion
of anomaly points, the difficulty of the anomalies, the degree to
which the anomaly points are clustered, and the level of irrelevant
features.

For our study, we chose 11 benchmark data sets from each mother
set benchmark collection as follows: First, we ranked all benchmark
data sets according the AUC scores computed using IF. We then
chose the data sets ranked 290th - 300th , where lower ranks are
easier. This ensures that our data sets are of medium difficulty, so
that when we inject missing values, the effect will not be overshad-
owed by the difficulty (or easiness) of the benchmark data. Table 1

ODD’18, 2018

Thomas G. Dietterich and Tadesse Zemicheal

Table 1: Description of the mother data sets.

Name
Abalone
Fault
Concrete
Image segment
Landsat
Letter recognition
Magic gamma Telescope
Page Blocks
Shuttle
Skin
Spambase
Wave
Wine

Mother Set Type
regression
binary
regression
multi class
multi class
multi class
binary
multi class
multi class
binary
binary
multi class
regression

N
1906
(277 - 1147)
(399 - 422)
(1190- 1320)
(4593 - 4833)
6000
6000
4600
6000
6000
(2512 - 2788)
3343
(3720 - 4113)

d
7
27
8
18
36
16
10
55
9
3
57
21
11

(%) anomalies
(0.1%, 0.5%, 1%, 10%)
(0.5%, 1%, 5%, 16%, 20%)
(0.6%, 1%, 5%, 10%)
(0.5%, 1%,5%,10%)
(0.1%, 0.5%, 1%,5%,19%)
(1%, 5%, 10%)
(0.1%, 0.5%, 1%, 5%)
(0.1%, 0.5%, 1%, 5%, 10%)
(0.1%, 0.5%, 1%, 5%, 10%)
(0.1%, 0.5%, 1%, 5%, 10%)
(0.1%, 0.5%, 1%, 5%, 10%)
(0.1%, 0.5%, 1%, 5%, 10%)
(0.1%, 0.5%, 1%, 5%, 10%)

Figure 8: Mean relative AUC of EGMM as a function of the
fraction of missing values, ρ, averaged across 12 mother sets.

Figure 6: Mean relative AUC of IF as a function of the frac-
tion of missing values, ρ, averaged across 13 mother sets.

Figure 9: Relative AUC of IF for ρ = 0.5, in ascending order
by mice imputation

Figure 7: Mean relative AUC of LODA as a function of the
fraction of missing values, ρ, averaged across 13 mother sets.

summarizes the benchmark data sets. “Mother Set type” indicates
the problem type of the original UCI mother set, N indicates sample
size (this can vary across the 11 benchmarks within each mother
set), d is the number of features, and “% anomalies” gives the relative
frequency of anomalies across the 11 benchmarks.

Figures 6, 7, and 8 show how the AUC decays as ρ increases.
Figure 9, 10, and 11 compare the relative AUC of the various missing
values methods for ρ = 0.5 across the different mother sets.

Figure 10: Relative AUC of LODA for ρ = 0.5, in ascending
order by mice imputation

Anomaly Detection in the Presence of Missing Values

ODD’18, 2018

and abalone mother sets and inferior on wine, wave, and landsat.
Further study is needed to understand whether this is due to the
cluster structure of skin and abalone as predicted by our analysis.

6 CONCLUSIONS
Based on our analysis and experiments, we make the following rec-
ommendations. First, implementations of Isolation Forest, LODA,
and EGMM should always include algorithms for handling miss-
ing values. These algorithms never hurt performance, and when
missing values are present, they can significantly improve perfor-
mance. Second, the proportional distribution method works well
for Isolation Forest. As it is more efficient than mice, it is our first
choice method. Third, mice imputation gave the best results for
LODA—much better than Pevn`y’s reduced method. We therefore
recommend it as the first choice for LODA. Fourth, for EGMM, the
marginalization method performed consistently and surprisingly
well, so we recommend it as the best method to use. Finally, we
observe that mice imputation worked quite well across all three
anomaly detection methods, so we recommend it as the first method
to try with any new anomaly detectors.

7 ACKNOWLEDGMENTS
This material is based upon work supported by the National Science
Foundation under Grant No. 1514550.

REFERENCES
[1] Lucien Birgé and Yves Rozenholc. 2006. How many bins should be put in a
regular histogram. ESAIM: Probability and Statistics 10 (2006), 24–45. https:
//doi.org/10.1051/ps

[2] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection.

Comput. Surveys 41, 3 (2009), 1–58.

[3] Andrew Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern, and Weng-
Keen Wong. 2015. Systematic construction of anomaly detection benchmarks from
real data. Technical Report. arXiv.

[4] Guohua Hao. 2009. Efficient Training and Feature Induction in Sequential Super-

vised Learning. Ph.D. Dissertation. Oregon State University.

[5] M. Lichman. 2013. UCI Machine Learning Repository. (2013). http://archive.ics.

uci.edu/ml

[6] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation forest. In Data
Mining, 2008. ICDM’08. Eighth IEEE International Conference on. IEEE, 413–422.
[7] Tomáš Pevn`y. 2016. Loda: Lightweight on-line detector of anomalies. Machine

Learning 102, 2 (2016), 275–304.

[8] J Ross Quinlan. 1987. Decision trees as probabilistic classifiers. In Proceedings of

the Fourth International Workshop on Machine Learning. 31–37.

[9] John Ross Quinlan. 1989. Unknown attribute values in induction. In Proceedings

of the International Machine Learning Workshop. 164–168.

[10] J Ross Quinlan. 2014. C4. 5: programs for machine learning. Elsevier.
[11] Donald B Rubin. 2004. Multiple imputation for nonresponse in surveys. John Wiley

& Sons.

[12] Maytal Saar-Tsechansky and Foster Provost. 2007. Handling missing values when
applying classification models. Journal of machine learning research 8, Jul (2007),
1623–1657.

[13] Ted E Senator, Henry G Goldberg, Alex Memory, William T Young, Brad Rees,
Robert Pierce, Daniel Huang, Matthew Reardon, David A Bader, Edmond Chow,
Irfan Essa, Joshua Jones, Vinay Bettadapura, Duen Horng Chau, Oded Green,
Oguz Kaya, Anita Zakrzewska, Erica Briscoe, Rudolph L Mappus IV, Robert Mc-
Coll, Lora Weiss, Thomas G Dietterich, Alan Fern, Weng-keen Wong, Shubhomoy
Das, Andrew Emmott, Jed Irvine, Daniel Corkill, Lisa Friedland, Amanda Gentzel,
David Jensen, Jay-yoon Lee, Danai Koutra, and Christos Faloutsos. 2013. Detect-
ing Insider Threats in a Real Corporate Database of Computer Usage Activity. In
KDD 2013. 9.

[14] Karin Groothuis-Oudshoorn Stef van Buuren. 2011. mice: Multivariate Imputation
by Chained Equations in R. Journal of statistical Software 45, 3 (2011), 1–67.
[15] Arthur Zimek, Erich Schubert, and Hans-Peter Kriegel. 2012. A survey on unsu-
pervised outlier detection in high-dimensional numerical data. Statistical Analysis
and Data Mining 5, 5 (2012), 363–387.

Figure 11: Relative AUC of EGMM for ρ = 0.5, in ascending
order by mice imputation

√

The most striking result is that the reduced method performs
very poorly for IF and LODA. This was expected for high values
of ρ, because when the number of non-missing values falls below
the number of features (
d) in the reduced anomaly detectors, the
AUC falls to 0.5. But even at ρ = 0.1, the reduced method is usually
inferior to the other methods. There are occasional exceptions such
as the letter recognition mother set for IF and LODA. A possible
explanation for the poor performance of the reduced method is that
each reduced anomaly detector has only
d features, and therefore
it is not able to assess the full normality/anomalousness of the test
queries.

√

Given the theoretical similarity between the reduced method and
the marginal method for EGMM, it is surprising how well marginal-
ization worked for EGMM. On average, for ρ ≥ 0.4, the marginal
method gave the best performance. Figure 11 shows that there are
six mother sets for which the marginal method is best and only one
(wave) where it is clearly inferior. Although the marginal method is
not computing theoretically-correct anomaly scores, it is comput-
ing a very effective scores. Note that the marginalization method
would require further refinement to work in practice. This is be-
cause it is not correct to compare a marginal distribution p1 over d1
dimensions to a distribution p2 over d2 (cid:44) d1 dimensions. All things
being equal, if d1 < d2, then p1(x) ≥ p2(x), because p1 disperses
the probability density over a lower-dimensional space. In our ex-
periments this was not a problem, because all of our test queries
had the same (or nearly the same) number of missing features.
In practice, it would be important to compute the tail probability
∫
X p(X )I[p(X ) ≤ p(Xq )]dX of all points X whose density is less
than the density of the query point p(Xq ). Unlike densities, it is
safe to compare probabilities.

Now let’s consider the three imputation methods: mice, mean,
and proportional distribution. A very robust finding is that mean
imputation performs worse than mice or proportional distribution.
This was expected based on our analysis. Another observation
is that mice and proportional distribution perform nearly iden-
tically, although there are some exceptions: Figure 9 shows that
proportional distribution is clearly superior to mice on the skin

ODD’18, 2018

Thomas G. Dietterich and Tadesse Zemicheal

A SYNTHETIC DATA GENERATION DETAILS
Correlated: To generate each point x of the nominal distribution,
we first sample a mean vector (cid:174)µ uniformly uniformly along the
multi-variate diagonal line segment that extends from (−3, . . . , −3)
to (+3, . . . , +3). Then x is sampled from N ((cid:174)µ, Σ), where Σ = diaд(c)+
ρ − diaд(cρ) and ρ ∈ {0.4, 0.6, 0.8, 1.2}. Each value of ρ was used
5 times to obtain a total of 20 data sets. Anomalies are generated
in a similar way but using the diagonal line that goes from b (cid:174)v +
(−3, . . . , −3) to b (cid:174)v + (+3, . . . , +3), where (cid:174)v is the vector (+1, −1, +1,
−1, . . . , +1, −1) and b = 2. The vector b (cid:174)v offsets the anomaly points
from the nominal points in an orthogonal direction.

3 N (µ1, Σ1)+ 1

Mixture: Nominals are generated from 1

3 N (µ2, Σ2)+
1
3 N (µ3, Σ3) where µ1 = (−3, −3, −3, . . . , −3), µ2 = (3, −3, 3, . . . , −3),
and µ3 = (3, 3, 3, . . . , 3). The covariance matrices are created using
Cholesky the decomposition: Σi = Li LT
i , where Li is the Cholesky
decomposition matrix of Σi . Each Li is defined as follows: L1 =
L3 = diaд(cd) + ρ − diaд(ρ, d) and L2 = diaд(d) + ρ − diaд(cρ, d)},
where ρ ∈ {0.4, 0.6, 0.8, 1.2}. Each value of ρ was used 5 times to
obtain a total of 20 data sets. Anomalies are generated from a single
Gaussian. The mean is constructed by first sampling its components
from U ni f (−1, 1) and then adding an offset of (2, −2, 2, . . . , 2). The
covariance matrix is the identify matrix. This positions the anom-
alies to be near the center of the three nominal components.

B GRAPHS OF ALL SYNTHETIC DATA

RESULTS

Figures 12 and 13 show the behavior of LODA on the synthetic data
sets. We observe that mice is much better than the other methods
on all four configurations and that it always beats LODA’s own
reduced method. Figures 14 and 15 show the behavior of EGMM on
the synthetic data sets. We observe that the marginal method gener-
ally gives the best performance across all configurations. However,
this is primarily due to its excellent performance on the Mixture
configuration, as mean imputation works slightly better on the
other three configurations.

Figure 13: Relative AUC of LODA on the synthetic data sets
for ρ = 0.5, in ascending order by mice imputation.

Figure 14: Mean relative AUC of EGMM as a function of the
fraction of missing values, ρ, averaged across the four syn-
thetic data set configurations.

Figure 12: Mean relative AUC of LODA as a function of the
fraction of missing values, ρ, averaged across the four syn-
thetic data set configurations.

Figure 15: Relative AUC of EGMM on the synthetic data sets
for ρ = 0.5, in ascending order by mice imputation.

