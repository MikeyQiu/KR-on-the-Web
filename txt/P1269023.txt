Reparameterization Gradients through Acceptance-Rejection
Sampling Algorithms

0
2
0
2
 
b
e
F
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
8
6
5
0
.
0
1
6
1
:
v
i
X
r
a

Christian A. Naesseth†‡ Francisco J. R. Ruiz‡§

Scott W. Linderman‡

David M. Blei‡

†Link¨oping University ‡Columbia University §University of Cambridge

Abstract

Variational inference using the reparameteri-
zation trick has enabled large-scale approx-
imate Bayesian inference in complex pro-
babilistic models,
leveraging stochastic op-
timization to sidestep intractable expecta-
tions. The reparameterization trick is appli-
cable when we can simulate a random vari-
able by applying a diﬀerentiable determinis-
tic function on an auxiliary random variable
whose distribution is ﬁxed. For many dis-
tributions of interest (such as the gamma or
Dirichlet), simulation of random variables re-
lies on acceptance-rejection sampling. The
discontinuity introduced by the accept–reject
step means that standard reparameterization
tricks are not applicable. We propose a new
method that lets us leverage reparameteriza-
tion gradients even when variables are out-
puts of a acceptance-rejection sampling algo-
rithm. Our approach enables reparameteri-
zation on a larger class of variational distribu-
tions. In several studies of real and synthetic
data, we show that the variance of the estima-
tor of the gradient is signiﬁcantly lower than
other state-of-the-art methods. This leads to
faster convergence of stochastic gradient vari-
ational inference.

1 Introduction

Variational inference [Hinton and van Camp, 1993,
Waterhouse et al., 1996, Jordan et al., 1999] under-
lies many recent advances in large scale probabilistic
modeling.
It has enabled sophisticated modeling of
complex domains such as images [Kingma and Welling,

Proceedings of the 20th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2017, Fort Laud-
erdale, Florida, USA. JMLR: W&CP volume 54. Copy-
right 2017 by the author(s).

2014] and text [Hoﬀman et al., 2013]. By deﬁnition,
the success of variational approaches depends on our
ability to (i) formulate a ﬂexible parametric family
of distributions; and (ii) optimize the parameters to
ﬁnd the member of this family that most closely ap-
proximates the true posterior. These two criteria are
at odds—the more ﬂexible the family, the more chal-
lenging the optimization problem. In this paper, we
present a novel method that enables more eﬃcient op-
timization for a large class of variational distributions,
namely, for distributions that we can eﬃciently sim-
ulate by acceptance-rejection sampling, or rejection
sampling for short.

For complex models, the variational parameters can
be optimized by stochastic gradient ascent on the evi-
dence lower bound (elbo), a lower bound on the
marginal likelihood of the data. There are two pri-
mary means of estimating the gradient of the elbo:
the score function estimator [Paisley et al., 2012, Ran-
ganath et al., 2014, Mnih and Gregor, 2014] and the
reparameterization trick [Kingma and Welling, 2014,
Rezende et al., 2014, Price, 1958, Bonnet, 1964], both
of which rely on Monte Carlo sampling. While the
reparameterization trick often yields lower variance
estimates and therefore leads to more eﬃcient opti-
mization, this approach has been limited in scope to a
few variational families (typically Gaussians). Indeed,
some lines of research have already tried to address
this limitation [Knowles, 2015, Ruiz et al., 2016].

There are two requirements to apply the reparameteri-
zation trick. The ﬁrst is that the random variable can
be obtained through a transformation of a simple ran-
dom variable, such as a uniform or standard normal;
the second is that the transformation be diﬀerentiable.
In this paper, we observe that all random variables we
simulate on our computers are ultimately transforma-
tions of uniforms, often followed by accept-reject steps.
So if the transformations are diﬀerentiable then we can
use these existing simulation algorithms to expand the
scope of the reparameterization trick.

Thus, we show how to use existing rejection samplers

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

to develop stochastic gradients of variational param-
eters. In short, each rejection sampler uses a highly-
tuned transformation that is well-suited for its distri-
bution. We can construct new reparameterization gra-
dients by “removing the lid” from these black boxes,
applying 65+ years of research on transformations [von
Neumann, 1951, Devroye, 1986] to variational infer-
ence. We demonstrate that this broadens the scope
of variational models amenable to eﬃcient inference
and provides lower-variance estimates of the gradient
compared to state-of-the-art approaches.

We ﬁrst review variational inference, with a focus on
stochastic gradient methods. We then present our key
contribution, rejection sampling variational inference
(rsvi), showing how to use eﬃcient rejection samplers
to produce low-variance stochastic gradients of the
variational objective. We study two concrete exam-
ples, analyzing rejection samplers for the gamma and
Dirichlet to produce new reparameterization gradients
for their corresponding variational factors. Finally, we
analyze two datasets with a deep exponential family
(def) [Ranganath et al., 2015], comparing rsvi to the
state of the art. We found that rsvi achieves a sig-
niﬁcant reduction in variance and faster convergence
of the elbo. Code for all experiments is provided at
github.com/blei-lab/ars-reparameterization.

2 Variational Inference

Let p(x, z) be a probabilistic model, i.e., a joint prob-
ability distribution of data x and latent (unobserved)
variables z. In Bayesian inference, we are interested
in the posterior distribution p(z
p(x) . For most
models, the posterior distribution is analytically in-
tractable and we have to use an approximation, such
as Monte Carlo methods or variational inference. In
this paper, we focus on variational inference.

x) = p(x,z)

|

In variational inference, we approximate the posterior
with a variational family of distributions q(z ; θ), pa-
rameterized by θ. Typically, we choose the variational
parameters θ that minimize the Kullback-Leibler (kl)
divergence between q(z ; θ) and p(z
x). This minimiza-
|
tion is equivalent to maximizing the elbo [Jordan
et al., 1999], deﬁned as

(θ) = Eq(z ;θ) [f (z)] + H[q(z ; θ)],
L
f (z) := log p(x, z),

(1)

H[q(z ; θ)] := Eq(z ;θ)[

log q(z ; θ)].

−

When the model and variational family satisfy con-
jugacy requirements, we can use coordinate ascent to
ﬁnd a local optimum of the elbo [Blei et al., 2016]. If
the conjugacy requirements are not satisﬁed, a com-
mon approach is to build a Monte Carlo estimator of

the gradient of the elbo [Paisley et al., 2012, Ran-
ganath et al., 2014, Mnih and Gregor, 2014, Sali-
mans and Knowles, 2013, Kingma and Welling, 2014].
This results in a stochastic optimization procedure,
where diﬀerent Monte Carlo estimators of the gradi-
ent amount to diﬀerent algorithms. We review below
two common estimators: the score function estimator
and the reparameterization trick.1

Score function estimator. The score function esti-
mator, also known as the log-derivative trick or rein-
force [Williams, 1992, Glynn, 1990], is a general way
to estimate the gradient of the elbo [Paisley et al.,
2012, Ranganath et al., 2014, Mnih and Gregor, 2014].
The score function estimator expresses the gradient as
an expectation with respect to q(z ; θ):

(θ) = Eq(z ;θ)[f (z)

θ

θ log q(z ; θ)] +

θH[q(z ; θ)].

L

∇

∇

∇
We then form Monte Carlo estimates by approx-
imating the expectation with independent samples
from the variational distribution. Though it is very
general, the score function estimator typically suf-
fers from high variance.
In practice we also need
to apply variance reduction techniques such as Rao-
Blackwellization [Casella and Robert, 1996] and con-
trol variates [Robert and Casella, 2004].

Reparameterization trick. The reparameteriza-
tion trick [Salimans and Knowles, 2013, Kingma and
Welling, 2014, Price, 1958, Bonnet, 1964] results in a
lower variance estimator compared to the score func-
tion, but it is not as generally applicable. It requires
that: (i) the latent variables z are continuous; and (ii)
we can simulate from q(z ; θ) as follows,

z = h(ε, θ),

with ε

s(ε).

(2)

∼

Here, s(ε) is a distribution that does not depend on
the variational parameters; it is typically a standard
normal or a standard uniform. Further, h(ε, θ) must
be diﬀerentiable with respect to θ. In statistics, this
is known as a non-central parameterization and has
been shown to be helpful in, e.g., Markov chain Monte
Carlo methods [Papaspiliopoulos et al., 2003].

Using (2), we can move the derivative inside the ex-
pectation and rewrite the gradient of the elbo as

(θ) = Es(ε) [

θ

zf (h(ε, θ))

θh(ε, θ)] +

θH[q(z ; θ)].

L

∇

∇

∇

∇
Empirically, the reparameterization trick has been
shown to be beneﬁcial over direct Monte Carlo es-
1In this paper, we assume for simplicity that the gra-
dient of the entropy ∇θH[q(z ; θ)] is available analytically.
The method that we propose in Section 3 can be easily
extended to handle non-analytical entropy terms. Indeed,
the resulting estimator of the gradient may have lower vari-
ance when the analytic gradient of the entropy is replaced
by its Monte Carlo estimate. Here we do not explore that.

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

timation of the gradient using the score fuction es-
timator [Salimans and Knowles, 2013, Kingma and
Welling, 2014, Titsias and L´azaro-Gredilla, 2014, Fan
et al., 2015]. Unfortunately, many distributions com-
monly used in variational inference, such as gamma
or Dirichlet, are not amenable to standard reparame-
terization because samples are generated using a rejec-
tion sampler [von Neumann, 1951, Robert and Casella,
2004], introducing discontinuities to the mapping. We
next show that taking a novel view of the acceptance-
rejection sampler lets us perform exact reparameteri-
zation.

3 Reparameterizing the

Acceptance-Rejection Sampler

The basic idea behind reparameterization is to rewrite
simulation from a complex distribution as a determin-
istic mapping of its parameters and a set of simpler
random variables. We can view the rejection sam-
pler as a complicated deterministic mapping of a (ran-
dom) number of simple random variables such as uni-
forms and normals. This makes it tempting to take
the standard reparameterization approach when we
consider random variables generated by rejection sam-
plers. However, this mapping is in general not contin-
uous, and thus moving the derivative inside the expec-
tation and using direct automatic diﬀerentiation would
not necessarily give the correct answer.

Our insight is that we can overcome this problem by in-
stead considering only the marginal over the accepted
sample, analytically integrating out the accept-reject
variable. Thus, the mapping comes from the proposal
step. This is continuous under mild assumptions, en-
abling us to greatly extend the class of variational fam-
ilies amenable to reparameterization.

We ﬁrst review rejection sampling and present the
reparameterized rejection sampler. Next we show how
to use it to calculate low-variance gradients of the
elbo. Finally, we present the complete stochastic op-
timization for variational inference, rsvi.

3.1 Reparameterized Rejection Sampling

Acceptance-Rejection sampling is a powerful way of
simulating random variables from complex distribu-
tions whose inverse cumulative distribution functions
are not available or are too expensive to evaluate [De-
vroye, 1986, Robert and Casella, 2004]. We consider an
alternative view of rejection sampling in which we ex-
plicitly make use of the reparameterization trick. This
view of the rejection sampler enables our variational
inference algorithm in Section 3.2.

To generate samples from a distribution q(z ; θ) us-

Algorithm 1 Reparameterized Rejection Sampling

Input: target q(z ; θ), proposal r(z ; θ), and constant

Mθ, with q(z ; θ)

Mθr(z ; θ)

q(z ; θ)

∼

≤

0

Output: ε such that h(ε, θ)
1: i
←
2: repeat
i
3:
Propose εi
4:
[0, 1]
Simulate ui
5:
6: until ui < q(h(εi,θ) ;θ)
Mθr(h(εi,θ) ;θ)
7: return εi

∼
∼ U

i + 1

s(ε)

←

≤

∞

ing rejection sampling, we ﬁrst sample from a proposal
distribution r(z ; θ) such that q(z ; θ)
Mθr(z ; θ) for
some Mθ <
. In our version of the rejection sampler,
we assume that the proposal distribution is reparame-
r(z ; θ) is equivalent
terizable, i.e., that generating z
to generating ε
s(ε) (where s(ε) does not depend
on θ) and then setting z = h(ε, θ) for a diﬀerentiable
function h(ε, θ). We then accept the sample with prob-
(cid:111)
; otherwise, we reject the
ability min
sample and repeat the process. We illustrate this in
Figure 1 and provide a summary of the method in Al-
gorithm 1, where we consider the output to be the
(accepted) variable ε, instead of z.

q(h(ε,θ) ;θ)
Mθr(h(ε,θ) ;θ)

1,

∼

∼

(cid:110)

The ability to simulate from r(z ; θ) by a reparameteri-
zation through a diﬀerentiable h(ε, θ) is not needed
for the rejection sampler to be valid. However, this
is indeed the case for the rejection sampler of many
common distributions.

3.2 The Reparameterized Rejection Sampler

in Variational Inference

We now use reparameterized rejection sampling to de-
velop a novel Monte Carlo estimator of the gradient
of the elbo. We ﬁrst rewrite the elbo in (1) as an
expectation in terms of the transformed variable ε,
(θ) = Eq(z ;θ) [f (z)] + H[q(z ; θ)]

L

= Eπ(ε ;θ) [f (h(ε, θ))] + H[q(z ; θ)].

(3)

In this expectation, π(ε ; θ) is the distribution of the
accepted sample ε in Algorithm 1. We construct it by
marginalizing over the auxiliary uniform variable u,

π(ε ; θ) =

π(ε, u ; θ)du

(cid:90)

(cid:90)

=

Mθs(ε)1

(cid:20)
0 < u <

q (h(ε, θ) ; θ)
Mθr (h(ε, θ) ; θ)

(cid:21)

du

= s(ε)

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

,

(4)

where 1[x
A] is the indicator function, and re-
call that Mθ is a constant used in the rejection sam-

∈

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

Figure 1: Example of a reparameterized rejection sampler for q(z ; θ) = Gamma(θ, 1), shown here with θ = 2. We
use the rejection sampling algorithm of Marsaglia and Tsang [2000], which is based on a nonlinear transformation
h(ε, θ) of a standard normal ε
(0, 1) (c.f. Eq. 10), and has acceptance probability of 0.98 for θ = 2. The
marginal density of the accepted value of ε (integrating out the acceptance variables, u1:i) is given by π(ε ; θ).
We compute unbiased estimates of the gradient of the elbo (6) via Monte Carlo, using Algorithm 1 to rejection
sample ε
π(ε ; θ). By reparameterizing in terms of ε, we obtain a low-variance estimator of the gradient for
challenging variational distributions.

∼ N

∼

∼

∼ U

s(ε) and u

pler. This can be seen by the algorithmic deﬁni-
tion of the rejection sampler, where we propose values
ε
[0, 1] until acceptance, i.e., un-
til u < q(h(ε,θ) ;θ)
Mθr(h(ε,θ) ;θ) . Eq. 3 follows intuitively, but we
formalize it in Proposition 1.
Proposition 1. Let f be any measurable function,
and ε
π(ε ; θ), deﬁned by (4) (and implicitly by Al-
gorithm 1). Then

∼

Eπ(ε ;θ) [f (h(ε, θ))] =

f (z)q(z ; θ)dz.

Proof. Using the deﬁnition of π(ε ; θ),

Eπ(ε ;θ) [f (h(ε, θ))] =

f (h(ε, θ)) s(ε)

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

dε

(cid:90)

=

f (z)r(z ; θ)

dz =

f (z)q(z ; θ)dz,

(cid:90)

q(z ; θ)
r(z ; θ)

where the second to last equality follows because
h(ε, θ), ε

s(ε) is a reparameterization of r(z ; θ).

∼

We can now compute the gradient of Eq(z ;θ)[f (z)]
based on Eq. 3,

θEπ(ε ;θ)[f (h(ε, θ))]

(cid:90)

(cid:90)

θEq(z ;θ)[f (z)] =
∇
= Eπ(ε ;θ)[

(cid:124)

+

∇
θf (h(ε, θ))]
∇
(cid:125)
(cid:123)(cid:122)
=:grep
(cid:20)
f (h(ε, θ))

+ Eπ(ε ;θ)

(cid:124)

∇
(cid:123)(cid:122)
=:gcor

(see the supplement for all details.) We deﬁne grep as
the reparameterization term, which takes advantage of
gradients with respect to the model and its latent vari-
ables; we deﬁne gcor as a correction term that accounts
for not using r(z ; θ)
q(z ; θ).
Using (5), the gradient of the elbo in (1) can be writ-
ten as

≡

θ

∇

L

(θ) = grep + gcor +

θH[q(z ; θ)],

(6)

∇

and thus we can build an unbiased one-sample Monte
(θ) as
Carlo estimator ˆg

θ
≈ ∇

L
ˆg := ˆgrep + ˆgcor +
ˆgrep =

zf (z)(cid:12)

∇

ˆgcor = f (h(ε, θ))

∇
(cid:12)z=h(ε,θ)∇
θ log

∇

θH[q(z ; θ)],
θh(ε, θ)

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

,

(7)

where ε is a sample generated using Algorithm 1. Of
course, one could generate more samples of ε and av-
erage, but we have found a single sample to suﬃce in
practice.

Note if h(ε, θ) is invertible in ε then we can simplify
the evaluation of the gradient of the log-ratio in gcor,

θ log

∇

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

=

(5)

θ log

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

(cid:21)
,

(cid:125)

θ log q(h(ε, θ) ; θ) +

θ log

∇

∇

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dh
dε

(cid:12)
(cid:12)
(ε, θ)
(cid:12)
(cid:12)

.

(8)

See the supplementary material for details.

where we have used the log-derivative trick and rewrit-
ten the integrals as expectations with respect to π(ε ; θ)

Alternatively, we could rewrite the gradient as an ex-
pectation with respect to s(ε) (this is an intermediate

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

step in the derivation shown in the supplement),

∇

θEq(z ;θ)[f (z)] = Es(ε)
(cid:20) q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

+ Es(ε)

(cid:20) q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ) ∇

f (h(ε, θ))

θ log

∇

(cid:21)

θf (h(ε, θ))

+

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

(cid:21)

,

and build an importance sampling-based Monte Carlo
estimator, in which the importance weights would be
q (h(ε, θ) ; θ) /r (h(ε, θ) ; θ). However, we would ex-
pect this approach to be beneﬁcial for low-dimensional
problems only, since for high-dimensional z the vari-
ance of the importance weights would be too high.

Algorithm 2 Rejection Sampling Variational Infer-
ence
Input: Data x, model p(x, z), variational

family

q(z ; θ)

Output: Variational parameters θ∗
1: repeat
2:
3:
4:
5: Update θn+1 = θn + ρnˆgn
6: until convergence

Run Algorithm 1 for θn to obtain a sample ε
Estimate the gradient ˆgn at θ = θn (Eq. 7)
Calculate the stepsize ρn (Eq. 9)

3.3 Full Algorithm

We now describe the full variational algorithm based
In Sec-
on reparameterizing the rejection sampler.
tion 5 we give concrete examples of how to reparame-
terize common variational families.

We make use of Eq. 6 to obtain a Monte Carlo esti-
mator of the gradient of the elbo. We use this es-
timate to take stochastic gradient steps. We use the
step-size sequence ρn proposed by Kucukelbir et al.
[2016] (also used by Ruiz et al. [2016]), which combines
rmsprop [Tieleman and Hinton, 2012] and Adagrad
[Duchi et al., 2011]. It is

ρn = η

n−1/2+δ

·
sn = t (ˆgn)2 + (1

·

(cid:16)

1 + √sn

(cid:17)−1

,

t)sn−1,

−

(9)

where n is the iteration number. We set δ = 10−16 and
t = 0.1, and we try diﬀerent values for η. (When θ is
a vector, the operations above are element-wise.)

We summarize the full method in Algorithm 2. We
refer to our method as rsvi.

4 Related Work

The reparameterization trick has also been used in
automatic diﬀerentiation variational inference (advi)

[Kucukelbir et al., 2015, 2016]. advi applies a trans-
formation to the random variables such that their sup-
port is on the reals and then places a Gaussian vari-
ational posterior approximation over the transformed
variable ε. In this way, advi allows for standard repa-
rameterization, but it cannot ﬁt gamma or Dirichlet
variational posteriors, for example. Thus, advi strug-
gles to approximate probability densities with singu-
In contrast,
larities, as noted by Ruiz et al. [2016].
our approach allows us to apply the reparameteriza-
tion trick on a wider class of variational distributions,
which may be more appropriate when the exact pos-
terior exhibits sparsity.

In the literature, we can ﬁnd other lines of research
that focus on extending the reparameterization gradi-
ent to other distributions. For the gamma distribution,
Knowles [2015] proposed a method based on approx-
imations of the inverse cumulative density function;
however, this approach is limited only to the gamma
distribution and it involves expensive computations.
For general expectations, Schulman et al. [2015] ex-
pressed the gradient as a sum of a reparameterization
term and a correction term to automatically estimate
the gradient in the context of stochastic computation
graphs. However, it is not possible to directly apply it
to variational inference with acceptance-rejection sam-
pling. This is due to discontinuities in the accept–
reject step and the fact that a rejection sampler pro-
duces a random number of random variables. Recently,
another line of work has focused on applying reparame-
terization to discrete latent variable models [Maddison
et al., 2017, Jang et al., 2017] through a continuous re-
laxation of the discrete space.
The generalized reparameterization (g-rep) method
[Ruiz et al., 2016] exploits the decomposition of the
gradient as grep + gcor by applying a transformation
based on standardization of the suﬃcient statistics of
z. Our approach diﬀers from g-rep: instead of search-
ing for a transformation of z that makes the distribu-
tion of ε weakly dependent on the variational parame-
ters (namely, standardization), we do the opposite by
choosing a transformation of a simple random variable
ε such that the distribution of z = h(ε, θ) is almost
equal to q(z ; θ). For that, we reuse the transforma-
tions typically used in rejection sampling. Rather than
having to derive a new transformation for each vari-
ational distribution, we leverage decades of research
on transformations in the rejection sampling literature
[Devroye, 1986]. In rejection sampling, these transfor-
mations (and the distributions of ε) are chosen so that
they have high acceptance probability, which means
0 with rsvi. In Sec-
we should expect to obtain gcor
tions 5 and 6 we compare rsvi with g-rep and show
that it exhibits signiﬁcantly lower variance, thus lead-

≈

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

ing to faster convergence of the inference algorithm.

Finally, another line of research in non-conjugate vari-
ational inference aims at developing more expressive
variational families [Salimans et al., 2015, Tran et al.,
2016, Maaløe et al., 2016, Ranganath et al., 2016].
rsvi can extend the reparameterization trick to these
methods as well, whenever rejection sampling is used
to generate the random variables.

5 Examples of Acceptance-Rejection

Reparameterization

As two examples, we study rejection sampling and re-
parameterization of two well-known distributions: the
gamma and Dirichlet. These have been widely used
as variational families for approximate Bayesian infer-
ence. We emphasize that rsvi is not limited to these
two cases, it applies to any variational family q(z ; θ)
for which a reparameterizable rejection sampler exists.
We provide other examples in the supplement.

5.1 Gamma Distribution

One of the most widely used rejection sampler is for the
gamma distribution. Indeed, the gamma distribution
is also used in practice to generate e.g. beta, Dirich-
let, and Student’s t-distributed random variables. The
gamma distribution, Gamma(α, β), is deﬁned by its
shape α and rate β.

For Gamma(α, 1) with α
1, Marsaglia and Tsang
≥
[2000] developed an eﬃcient rejection sampler. It uses
a truncated version of the following reparameterization

z = hGamma(ε, α) :=

α

(cid:18)

(cid:19) (cid:18)

1
3

−

1 +

ε
√9α

(cid:19)3

,

3

−

(10)

ε

s(ε) :=

(0, 1).

N

∼
When β
= 1, we divide z by the rate β and obtain a
sample distributed as Gamma(α, β). The acceptance
it exceeds 0.95 and 0.98 for
probability is very high:
α = 1 and α = 2, respectively. In fact, as α
we
have that π(ε ; θ)
s(ε), which means that the accep-
tance probability approaches 1. Figure 1 illustrates the
involved functions and distributions for shape α = 2.

→ ∞

→

For α < 1, we observe that z = u1/α ˜z is distributed as
Gamma(α, β) for ˜z
[0, 1]
[Stuart, 1962, Devroye, 1986], and apply the rejection
sampler above for ˜z.

Gamma(α+1, β) and u

∼ U

∼

We now study the quality of the transformation in
(10) for diﬀerent values of the shape parameter α.
Since π(ε ; θ)
, we should expect
the correction term gcor to decrease with α. We show

s(ε) as α

→ ∞

→

Figure 2: The correction term of rsvi, and as a result
the gradient variance, decreases with increasing shape
α. We plot absolute value of the gradient of the log-
ratio between the target (gamma) and proposal distri-
butions as a function of ε.

that in Figure 2, where we plot the log-ratio (8) from
the correction term as a function of ε for four val-
ues of α. We additionally show in Figure 3 that the
distribution π(ε ; θ) converges to s(ε) (a standard nor-
mal) as α increases. For large α, π(ε ; θ)
s(ε) and
the acceptance probability of the rejection sampler ap-
proaches 1, which makes the correction term negligi-
ble. In Figure 3, we also show that π(ε ; θ) converges
faster to a standard normal than the standardization
procedure used in g-rep. We exploit this property—
that performance improves with α—to artiﬁcially in-
crease the shape for any gamma distribution. We now
explain this trick, which we call shape augmentation.

≈

Shape augmentation. Here we show how to ex-
ploit the fact that the rejection sampler improves for
increasing shape α. We make repeated use of the trick
above, using uniform variables, to control the value of
α that goes into the rejection sampler. That is, to com-
pute the elbo for a Gamma(α, 1) distribution, we can
ﬁrst express the random variable as z = ˜z (cid:81)B
(for some positive integer B), ˜z
and ui
duction, since ˜zu

1
α+i−1
i=1 u
i
Gamma(α + B, 1)
∼
[0, 1]. This can be proved by in-

1
α+B−1
B
∼
˜zu
2, 1), etc. Hence,
Gamma(α + B
we can apply the rejection sampling framework for
˜z
Gamma(α + B, 1) instead of the original z. We
study the eﬀect of shape augmentation on the variance
in Section 5.2.

u
B−1 ∼

Gamma(α + B

i.i.d.
∼ U

1
α+B−1
B

1
α+B−2

1, 1),

−

−

∼

5.2 Dirichlet Distribution

The Dirichlet(α1:K) distribution, with concentra-
is a K-dimensional mul-
tion parameters α1:K,
tivariate distribution with K
free-
1 degrees of

−

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

Figure 3: In the distribution on the transformed space ε for a gamma distribution we can see that the rejection
sampling-inspired transformation converges faster to a standard normal. Therefore it is less dependent on the
parameter α, which implies a smaller correction term. We compare the transformation of rsvi (this paper) with
the standardization procedure suggested in g-rep [Ruiz et al., 2016], for shape parameters α =

1, 2, 10

{

.
}

component of the gradient, based on simulated data
from a Dirichlet distribution with K = 100 compo-
nents, uniform prior, and N = 100 trials. We compare
the variance of rsvi (for various shape augmentation
settings) with the g-rep approach [Ruiz et al., 2016].
rsvi performs better even without the augmentation
trick, and signiﬁcantly better with it.

6 Experiments

In Section 5 we compared rejection sampling varia-
tional inference (rsvi) with generalized reparameteri-
zation (g-rep) and found a substantial variance reduc-
tion on synthetic examples. Here we evaluate rsvi on a
more challenging model, the sparse gamma deep expo-
nential family (def) [Ranganath et al., 2015]. On two
real datasets, we compare rsvi with state-of-the-art
methods: automatic diﬀerentiation variational infer-
ence (advi) [Kucukelbir et al., 2015, 2016], black-box
variational inference (bbvi) [Ranganath et al., 2014],
and g-rep [Ruiz et al., 2016].

×

Data.
The datasets we consider are the Olivetti
faces2 and Neural Information Processing Systems
(nips) 2011 conference papers. The Olivetti faces
dataset consists of 64
64 gray-scale images of human
faces in 8 bits, i.e., the data is discrete and in the set
. In the nips dataset we have documents
0, . . . , 255
}
{
in a bag-of-words format with an eﬀective vocabulary
of 5715 words.
Model. The sparse gamma def [Ranganath et al.,
2015] is a multi-layered probabilistic model that mim-
ics the architecture of deep neural networks. It mo-
dels the data using a set of local latent variables z(cid:96)
n,k
where n indexes observations, k components, and (cid:96)
layers. These local variables are connected between
layers through global weights w(cid:96)
k,k(cid:48). The observations
are xn,d, where d denotes dimension. The joint proba-

2http://www.cl.cam.ac.uk/research/dtg/

attarchive/facedatabase.html

Figure 4: rsvi (this paper) achieves lower variance
compared to g-rep [Ruiz et al., 2016]. The estimated
variance is for the ﬁrst component of Dirichlet ap-
proximation to a multinomial likelihood with uniform
Dirichlet prior. Optimal concentration is α = 2, and
B denotes shape augmentation.

dom.
the fact
z1:K = ((cid:80)

To simulate
if ˜zk
that
(cid:96) ˜z(cid:96))−1 (˜z1, . . . , ˜zK)(cid:62)

∼

random variables we use
Gamma(αk, 1)
then

i.i.d.,

Dirichlet(α1:K).

∼

Thus, we make a change of variables to reduce the
problem to that of simulating independent gamma dis-
tributed random variables,

Eq(z1:K ;α1:K )[f (z1:K)] =
(cid:33) K
(cid:89)

(cid:32)

(cid:90)

=

f

˜z1:K
(cid:96)=1 ˜z(cid:96)

(cid:80)K

k=1

Gamma(˜zk ; αk, 1)d˜z1:K.

We apply the transformation in Section 5.1 for the
gamma-distributed variables, ˜zk = hGamma(εk, αk),
where the variables εk are generated by independent
gamma rejection samplers. To showcase this, we study
a simple conjugate model where the exact gradient and
posterior are available: a multinomial likelihood with
Dirichlet prior and Dirichlet variational distribution.
In Figure 4 we show the resulting variance of the ﬁrst

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

rsvi B = 1 rsvi B = 4

Min
Median
Max

6.0e
4
−
9.0e7
1.2e17

1.2e
3
−
2.9e7
3.4e14

g-rep
2.7e
3
−
1.6e12
1.5e17

rsvi B = 1 rsvi B = 4

Min
Median
Max

1.8e
3
−
1.2e4
1.4e12

1.5e
3
−
4.5e3
1.6e11

g-rep
2.6e
3
−
1.5e7
3.5e12

Table 1: The rsvi gradient (this paper) exhibits lower variance than g-rep [Ruiz et al., 2016]. We show estimated
variance, based on 10 samples, of g-rep and rsvi (for B = 1, 4 shape augmentation steps), for parameters at
the initialization point (left) and at iteration 2600 in rsvi (right), estimated for the nips data.

best one in Figure 5. We can see that rsvi has a sig-
niﬁcantly faster initial improvement than any of the
other methods.3 The wall-clock time for rsvi is based
on a Python implementation (average 1.5s per itera-
tion) using the automatic diﬀerentiation package au-
tograd [Maclaurin et al., 2015]. We found that rsvi is
approximately two times faster than g-rep for com-
parable implementations. One reason for this is that
the transformations based on rejection sampling are
cheaper to evaluate. Indeed, the research literature on
rejection sampling is heavily focused on ﬁnding cheap
and eﬃcient transformations.
For the nips dataset, we now compare the variance
of the gradients between the two estimators, rsvi and
g-rep, for diﬀerent shape augmentation steps B. In
Table 1 we show the minimum, median, and maxi-
mum values of the variance across all dimensions. We
can see that rsvi again clearly outperforms g-rep in
terms of variance. Moreover, increasing the number of
augmentation steps B provides even further improve-
ments.

We introduced rejection sampling variational inference
(rsvi), a method for deriving reparameterization gra-
dients when simulation from the variational distribu-
tion is done using a acceptance-rejection sampler. In
practice, rsvi leads to lower-variance gradients than
other state-of-the-art methods. Further, it enables re-
parameterization gradients for a large class of varia-
tional distributions, taking advantage of the eﬃcient
transformations developed in the rejection sampling
literature.

This work opens the door to other strategies that “re-
move the lid” from existing black-box samplers in the
service of variational inference. As future work, we
can consider more complicated simulation algorithms
with accept-reject-like steps, such as adaptive rejec-
tion sampling, importance sampling, sequential Monte
Carlo, or Markov chain Monte Carlo.

bilistic model is deﬁned as

7 Conclusions

Figure 5: rsvi (this paper) presents a signiﬁcantly
faster initial improvement of the evidence lower bound
(elbo) as a function of wall-clock time. The model
is a sparse gamma def, applied to the Olivetti faces
dataset, and we compare with advi [Kucukelbir et al.,
2016], bbvi [Ranganath et al., 2014], and g-rep [Ruiz
et al., 2016].

z(cid:96)
n,k ∼

Gamma

αz,

xn,d

Poisson

∼

(cid:33)

,

(cid:80)

αz
k,k(cid:48)z(cid:96)+1
k(cid:48) w(cid:96)
n,k(cid:48)
(cid:33)

w0

k,dz1

n,k

.

(cid:32)

(cid:32)

(cid:88)

k

(11)

We set αz = 0.1 in the experiments. All priors on the
weights are set to Gamma(0.1, 0.3), and the top-layer
local variables priors are set to Gamma(0.1, 0.1). We
use 3 layers, with 100, 40, and 15 components in each.
This is the same model that was studied by Ruiz et al.
[2016], where g-rep was shown to outperform both
bbvi (with control variates and Rao-Blackwellization),
as well as advi. In the experiments we follow their ap-
proach and parameterize the variational approximat-
ing gamma distribution using the shape and mean. To
avoid constrained optimization we use the transform
θ = log(1+exp(ϑ)) for non-negative variational param-
eters θ, and optimize ϑ in the unconstrained space.

Results.
η

0.75, 1, 2, 5
}

∈ {

For

the Olivetti

faces we explore
and show the resulting elbo of the

3The results of g-rep, advi and bbvi where reproduced

with permission from Ruiz et al. [2016].

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

Acknowledgements

Christian A. Naesseth is supported by CADICS, a Lin-
naeus Center, funded by the Swedish Research Coun-
cil (VR). Francisco J. R. Ruiz is supported by the
EU H2020 programme (Marie Sk(cid:32)lodowska-Curie grant
agreement 706760). Scott W. Linderman is supported
by the Simons Foundation SCGB-418011. This work
is supported by NSF IIS-1247664, ONR N00014-11-
1-0651, DARPA PPAML FA8750-14-2-0009, DARPA
SIMPLEX N66001-15-C-4032, Adobe, and the Alfred
P. Sloan Foundation. The authors would like to thank
Alp Kucukelbir and Dustin Tran for helpful comments
and discussion.

References

M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and
L. K. Saul. An introduction to variational meth-
ods for graphical models. Machine Learning, 37(2):
183–233, Nov. 1999.

D. P. Kingma and M. Welling. Auto-encoding varia-
tional Bayes. In International Conference on Learn-
ing Representations, 2014.

D. A. Knowles.

Stochastic gradient variational
for Gamma approximating distributions.

Bayes
arXiv:1509.01631v1, 2015.

A. Kucukelbir, R. Ranganath, A. Gelman, and D. M.
Blei. Automatic variational inference in Stan. In Ad-
vances in Neural Information Processing Systems,
2015.

D. M. Blei, A. Kucukelbir, and J. D. McAuliﬀe.
inference: A review for statisticians.

Variational
arXiv:1601.00670, 2016.

A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman,
and D. M. Blei. Automatic diﬀerentiation varia-
tional inference. arXiv:1603.00788, 2016.

G. Bonnet. Transformations des signaux al´eatoires a
travers les systemes non lin´eaires sans m´emoire. An-
nals of Telecommunications, 19(9):203–220, 1964.

G. Casella and C. P. Robert. Rao-Blackwellisation of
sampling schemes. Biometrika, 83(1):81–94, 1996.

L. Devroye. Non-Uniform Random Variate Genera-

tion. Springer-Verlag, 1986.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgra-
dient methods for online learning and stochastic op-
timization. Journal of Machine Learning Research,
12:2121–2159, jul 2011.

K. Fan, Z. Wang, J. Beck, J. Kwok, and K. A. Heller.
Fast second order stochastic backpropagation for
variational inference. In Advances in Neural Infor-
mation Processing Systems, 2015.

L. Maaløe, C. K. Sønderby, S. K. Sønderby, and
O. Winther. Auxiliary deep generative models.
In International Conference on Machine Learning,
2016.

D. Maclaurin, D. Duvenaud, M. Johnson, and R. P.
Adams. Autograd: Reverse-mode diﬀerentiation of
native Python, 2015. URL http://github.com/
HIPS/autograd.

C. J. Maddison, A. Mnih, and Y. W. Teh. The con-
crete distribution: A continuous relaxation of dis-
crete random variables. In International Conference
on Learning Representations, 2017.
(accepted for
publication).

G. Marsaglia and W. W. Tsang. A simple method for
generating gamma variables. ACM Transactions on
Mathematical Software, 26(3):363–372, Sept. 2000.

P. W. Glynn. Likelihood ratio gradient estimation for
stochastic systems. Communications of the ACM,
33(10):75–84, oct 1990.

A. Mnih and K. Gregor. Neural variational inference
In International

and learning in belief networks.
Conference on Machine Learning, 2014.

G. E. Hinton and D. van Camp. Keeping the neu-
ral networks simple by minimizing the description
length of the weights. In Proceedings of the Sixth An-
nual Conference on Computational Learning The-
ory, pages 5–13, New York, NY, USA, 1993. ACM.

M. D. Hoﬀman, D. M. Blei, C. Wang, and J. Paisley.
Stochastic variational inference. Journal of Machine
Learning Research, 14:1303–1347, May 2013.

E. Jang, S. Gu, and B. Poole. Categorical reparame-
In International
terization using gumbel-softmax.
Conference on Learning Representations, 2017. (ac-
cepted for publication).

J. W. Paisley, D. M. Blei, and M. I. Jordan. Variational
Bayesian inference with stochastic search. In Inter-
national Conference on Machine Learning, 2012.

O. Papaspiliopoulos, G. O. Roberts, and M. Sk¨old.
Non-centered parameterisations for hierarchical mo-
dels and data augmentation. In Bayesian Statistics
7: Proceedings of the Seventh Valencia International
Meeting, page 307. Oxford University Press, USA,
2003.

R. Price. A useful theorem for nonlinear devices having
Gaussian inputs. IRE Transactions on Information
Theory, 4(2):69–72, 1958.

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

S. Waterhouse, D. Mackay, and T. Robinson. Bayesian
In Advances in
methods for mixtures of experts.
Neural Information Processing Systems, pages 351–
357. MIT Press, 1996.

R. J. Williams. Simple statistical gradient-following
algorithms for connectionist reinforcement learning.
Machine Learning, 8(3–4):229–256, 1992.

R. Ranganath, S. Gerrish, and D. M. Blei. Black box
variational inference. In Artiﬁcial Intelligence and
Statistics, 2014.

R. Ranganath, L. Tang, L. Charlin, and D. M. Blei.
Deep exponential families. In Artiﬁcial Intelligence
and Statistics, 2015.

R. Ranganath, D. Tran, and D. M. Blei. Hierarchical
variational models. In International Conference on
Machine Learning, 2016.

D. J. Rezende, S. Mohamed, and D. Wierstra. Stochas-
tic backpropagation and approximate inference in
In International Confer-
deep generative models.
ence on Machine Learning, 2014.

C. Robert and G. Casella. Monte Carlo statistical
methods. Springer Science & Business Media, 2004.

F. J. R. Ruiz, M. K. Titsias, and D. M. Blei. The gen-
In Advances

eralized reparameterization gradient.
in Neural Information Processing Systems, 2016.

T. Salimans and D. A. Knowles. Fixed-form varia-
tional posterior approximation through stochastic
linear regression. Bayesian Analysis, 8(4):837–882,
2013.

T. Salimans, D. P. Kingma, and M. Welling. Markov
chain Monte Carlo and variational inference: Bridg-
ing the gap. In International Conference on Machine
Learning, 2015.

J. Schulman, N. Heess, T. Weber, and P. Abbeel.
Gradient estimation using stochastic computation
graphs. In Advances in Neural Information Process-
ing Systems, 2015.

A. Stuart. Gamma-distributed products of indepen-
dent random variables. Biometrika, 49:64–65, 1962.

T. Tieleman and G. Hinton. Lecture 6.5-RMSPROP:
Divide the gradient by a running average of its re-
cent magnitude. Coursera: Neural Networks for Ma-
chine Learning, 4, 2012.

M. K. Titsias and M. L´azaro-Gredilla. Doubly stochas-
tic variational Bayes for non-conjugate inference.
In International Conference on Machine Learning,
2014.

D. Tran, R. Ranganath, and D. M. Blei. The vari-
ational Gaussian process. In International Confer-
ence on Learning Representations, 2016.

J. von Neumann. Various Techniques Used in Connec-
tion with Random Digits. Journal of Research of
the National Bureau of Standards, 12:36–38, 1951.

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

A Supplementary Material

A.2 Derivation of the Gradient

A.1 Distribution of ε

Here we formalize the claim in the main manuscript
the accepted vari-
regarding the distribution of
able ε in the rejection sampler.
Recall
that
s(ε) is equivalent to z
z = h(ε, θ), ε
r(z ; θ), and
∼
Mθr(z ; θ). For simplicity we consider
that q(z ; θ)
the univariate continuous case in the exposition below,
but the result also holds for the discrete and multivari-
ate settings. The cumulative distribution function for
the accepted ε is given by

≤

∼

(cid:19)

q(h(Ei, θ) ; θ)
Mθr(h(Ei, θ) ; θ)
(cid:19) (cid:35)

P(E

ε) =

P(E

ε, E = Ei)

∞
(cid:88)

i=1

≤

Ei

ε, Ui <

≤

≤

(cid:34)

∞
(cid:88)

(cid:18)

=

P

(cid:18)

i=1

i−1
(cid:89)

j=1

=

=

=

i=1
(cid:90) ε

−∞

(cid:90) ε

−∞

P

Uj

q(h(Ej, θ) ; θ)
Mθr(h(Ej, θ) ; θ)

≥

∞
(cid:88)

(cid:90) ε

s(e)

q(h(e, θ) ; θ)
Mθr(h(e, θ) ; θ)

de

−∞

s(e)

q(h(e, θ) ; θ)
r(h(e, θ) ; θ)

de

1
Mθ ·

·

s(e)

q(h(e, θ) ; θ)
r(h(e, θ) ; θ)

de.

(cid:19)

i−1
(cid:89)

(cid:18)

j=1

∞
(cid:88)

(cid:18)

i=1

1
Mθ

1
Mθ

1

−

1

−

(cid:19)i−1

Here, we have applied that z = h(ε, θ), ε
r(z ; θ), and thus
reparameterization of z

∼

s(ε) is a

∼
q(h(Ej, θ) ; θ)
Mθr(h(Ej, θ) ; θ)

(cid:19)

(cid:18)

P

Uj

≥

(cid:90) ∞

−∞

=

(cid:18)
1

s(e)

= 1

= 1

−

−

1
Mθ
1
Mθ

Es(e)

Er(z ;θ)

(cid:19)

de

q(h(e, θ) ; θ)
Mθr(h(e, θ) ; θ)

(cid:21)

−
(cid:20) q(h(e, θ) ; θ)
r(h(e, θ) ; θ)
(cid:20) q(z ; θ)
r(z ; θ)

(cid:21)

= 1

1
Mθ

.

−

The density is obtained by taking the derivative of the
cumulative distribution function with respect to ε,

d
dε

P(E

≤

ε) = s(ε)

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

,

which is the expression from the main manuscript.

The motivation from the main manuscript is basically
a standard “area-under-the-curve” or geometric argu-
ment for rejection sampling [Robert and Casella, 2004],
but for ε instead of z.

for

We provide below details
the derivation of
the gradient. We assume that h is diﬀerentiable
(almost everywhere) with respect to θ, and that
f (h(ε, θ)) q(h(ε,θ) ;θ)
r(h(ε,θ) ;θ) is continuous in θ for all ε. Then,
we have

(cid:90)

(cid:90)

(cid:124)

∇
=

θEq(z ;θ)[f (z)] =
(cid:90)

(cid:18)

s(ε)

θ

∇
f (h(ε, θ))

θEπ(ε ;θ)[f (h(ε, θ))]
(cid:19)
q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

dε

=

s(ε)

∇
q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ) ∇

+

s(ε)f (h(ε, θ))

θ

∇

= Eπ(ε ;θ)[

θf (h(ε, θ))]
∇
(cid:125)
(cid:123)(cid:122)
=:grep
(cid:20)
f (h(ε, θ))

+ Eπ(ε ;θ)

(cid:124)

θf (h(ε, θ)) dε
(cid:18) q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)
+

(cid:19)

dε

θ log

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

(cid:21)
,

(cid:125)

∇
(cid:123)(cid:122)
=:gcor

where in the last step we have identiﬁed π(ε ; θ) and
made use of the log-derivative trick

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

θ

∇

=

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ) ∇

θ log

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

.

Gradient of Log-Ratio in gcor For invertible repa-
rameterizations we can simplify the evaluation of the
gradient of the log-ratio in gcor as follows using stan-
dard results on transformation of a random variable

θ log

∇

+

θ log

∇

=

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)
(cid:12)
dh
(cid:12)
(ε, θ)
(cid:12)
dε
(cid:12) − ∇

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∇

θ log q(h(ε, θ) ; θ)+

θ log s(h−1(h(ε, θ), θ))
(cid:125)

(cid:124)

(cid:123)(cid:122)
= s(ε)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dh
dε

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∇

=

θ log q(h(ε, θ) ; θ) +

θ log

(ε, θ)

.

∇

A.3 Examples of Reparameterizable

Rejection Samplers

We show in Table 2 some examples of reparameteriz-
able rejection samplers for three distributions, namely,
the gamma, the truncated normal, and the von Misses
distributions (for more examples, see Devroye [1986]).
We show the distribution q(z ; θ), the transformation
h(ε, θ), and the proposal s(ε) used in the rejection sam-
pler.

We show in Table 3 six examples of distributions that
can be reparameterized in terms of auxiliary gamma-
distributed random variables. We show the distribu-
tion q(z ; θ), the distribution of the auxiliary gamma
random variables p(˜z ; θ), and the mapping z = g(˜z, θ).

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

q(z ; θ)

Gamma(α, 1)

Truncated

(0, 1, a,

N

)

∞

h(ε, θ)

(cid:0)α

(cid:1) (cid:16)

1
3

−
(cid:112)a2

(cid:17)3

1 + ε√

9α−3

2 log ε

−
2ρ , ρ = r−

s(ε)

ε

ε

(0, 1)

∼ N

[0, 1]

∼ U

ε

1, 1]

[
−

∼ U

vonMises(κ)

sign(ε) arccos

(cid:16) 1+c cos(πε)
c+cos(πε)

(cid:17)

, c = 1+ρ2

√
2κ , r = 1 + √1 + 4κ2

2r

Table 2: Examples of reparameterizable rejection samplers; many more can be found in Devroye [1986]. The ﬁrst
column is the distribution, the second column is the transformation h(ε, θ), and the last column is the proposal
s(ε).

q(z ; θ)

g(˜z, θ)

p(˜z ; θ)

Beta(α, β)

Dirichlet(α1:K)

˜z1
˜z1 + ˜z2
(˜z1, . . . , ˜zK)(cid:62)

1
(cid:80)
(cid:96) ˜z(cid:96)

St(ν)

χ2(k)

F(d1, d2)

Nakagami(m, Ω)

(cid:114) ν
2˜z1

˜z2

2˜z

d2 ˜z1
d1 ˜z2
(cid:114)
Ω˜z
m

˜z1

Gamma(α, 1), ˜z2

Gamma(β, 1)

∼

˜zk

∼

∼

Gamma(αk, 1), k = 1, . . . , K

˜z1

Gamma(ν/2, 1), ˜z2

(0, 1)

∼ N

˜z

Gamma(k/2, 1)

∼

∼

∼

˜z1

Gamma(d1/2, 1), ˜z2

Gamma(d2/2, 1)

∼

˜z

Gamma(m, 1)

∼

Table 3: Examples of random variables as functions of auxiliary random variables with reparameterizable distri-
butions. The ﬁrst column is the distribution, the second column is a function g(˜z, θ) mapping from the auxiliary
variables to the desired variable, and the last column is the distribution of the auxiliary variables ˜z.

A.4 Reparameterizing the Gamma

above. The gradients of log q and

log r are given by

Distribution

We provide details on reparameterization of the
gamma distribution. In the following we consider rate
β = 1. Note that this is not a restriction, we can
always reparameterize the rate. The density of the
gamma random variable is given by

q(z ; α) =

zα−1e−z
Γ(α)

,

where Γ(α) is the gamma function. We make use of
the reparameterization deﬁned by

(cid:18)

(cid:19) (cid:18)

1
3

−

1 +

ε
√9α

(cid:19)3

,

3

−

z = h(ε, α) =

α

ε

∼ N

(0, 1).

Because h is invertible we can make use of the sim-
pliﬁed gradient of the log-ratio derived in Section A.2

dh(ε, α)
dα
(cid:18)

=

1 +

α log q(h(ε, α) ; α)

∇

= log(h(ε, α)) + (α

1)

−

h(ε, α) −

dh(ε,α)
dα

dh(ε, α)

ψ(α),

−

dα −
(cid:12)
(cid:12)
(ε, α)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dh
dε

log r(h(ε, α) ; α) =

α log

α

∇

−

=

2 (cid:0)α

1

−

∇

9ε
(cid:17)

(cid:1) −

1
3

(cid:16)

1 + ε√

9α−3

(9α

,

3
2

3)

−

where ψ(α) is the digamma function and

(cid:19)3

ε
√9α

3

−

−

2(9α

3
2

3)

27ε

−

(cid:18)

1 +

ε
√9α

(cid:19)2

.

3

−

Reparameterization Gradients through Acceptance-Rejection
Sampling Algorithms

0
2
0
2
 
b
e
F
 
2
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
8
6
5
0
.
0
1
6
1
:
v
i
X
r
a

Christian A. Naesseth†‡ Francisco J. R. Ruiz‡§

Scott W. Linderman‡

David M. Blei‡

†Link¨oping University ‡Columbia University §University of Cambridge

Abstract

Variational inference using the reparameteri-
zation trick has enabled large-scale approx-
imate Bayesian inference in complex pro-
babilistic models,
leveraging stochastic op-
timization to sidestep intractable expecta-
tions. The reparameterization trick is appli-
cable when we can simulate a random vari-
able by applying a diﬀerentiable determinis-
tic function on an auxiliary random variable
whose distribution is ﬁxed. For many dis-
tributions of interest (such as the gamma or
Dirichlet), simulation of random variables re-
lies on acceptance-rejection sampling. The
discontinuity introduced by the accept–reject
step means that standard reparameterization
tricks are not applicable. We propose a new
method that lets us leverage reparameteriza-
tion gradients even when variables are out-
puts of a acceptance-rejection sampling algo-
rithm. Our approach enables reparameteri-
zation on a larger class of variational distribu-
tions. In several studies of real and synthetic
data, we show that the variance of the estima-
tor of the gradient is signiﬁcantly lower than
other state-of-the-art methods. This leads to
faster convergence of stochastic gradient vari-
ational inference.

1 Introduction

Variational inference [Hinton and van Camp, 1993,
Waterhouse et al., 1996, Jordan et al., 1999] under-
lies many recent advances in large scale probabilistic
modeling.
It has enabled sophisticated modeling of
complex domains such as images [Kingma and Welling,

Proceedings of the 20th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2017, Fort Laud-
erdale, Florida, USA. JMLR: W&CP volume 54. Copy-
right 2017 by the author(s).

2014] and text [Hoﬀman et al., 2013]. By deﬁnition,
the success of variational approaches depends on our
ability to (i) formulate a ﬂexible parametric family
of distributions; and (ii) optimize the parameters to
ﬁnd the member of this family that most closely ap-
proximates the true posterior. These two criteria are
at odds—the more ﬂexible the family, the more chal-
lenging the optimization problem. In this paper, we
present a novel method that enables more eﬃcient op-
timization for a large class of variational distributions,
namely, for distributions that we can eﬃciently sim-
ulate by acceptance-rejection sampling, or rejection
sampling for short.

For complex models, the variational parameters can
be optimized by stochastic gradient ascent on the evi-
dence lower bound (elbo), a lower bound on the
marginal likelihood of the data. There are two pri-
mary means of estimating the gradient of the elbo:
the score function estimator [Paisley et al., 2012, Ran-
ganath et al., 2014, Mnih and Gregor, 2014] and the
reparameterization trick [Kingma and Welling, 2014,
Rezende et al., 2014, Price, 1958, Bonnet, 1964], both
of which rely on Monte Carlo sampling. While the
reparameterization trick often yields lower variance
estimates and therefore leads to more eﬃcient opti-
mization, this approach has been limited in scope to a
few variational families (typically Gaussians). Indeed,
some lines of research have already tried to address
this limitation [Knowles, 2015, Ruiz et al., 2016].

There are two requirements to apply the reparameteri-
zation trick. The ﬁrst is that the random variable can
be obtained through a transformation of a simple ran-
dom variable, such as a uniform or standard normal;
the second is that the transformation be diﬀerentiable.
In this paper, we observe that all random variables we
simulate on our computers are ultimately transforma-
tions of uniforms, often followed by accept-reject steps.
So if the transformations are diﬀerentiable then we can
use these existing simulation algorithms to expand the
scope of the reparameterization trick.

Thus, we show how to use existing rejection samplers

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

to develop stochastic gradients of variational param-
eters. In short, each rejection sampler uses a highly-
tuned transformation that is well-suited for its distri-
bution. We can construct new reparameterization gra-
dients by “removing the lid” from these black boxes,
applying 65+ years of research on transformations [von
Neumann, 1951, Devroye, 1986] to variational infer-
ence. We demonstrate that this broadens the scope
of variational models amenable to eﬃcient inference
and provides lower-variance estimates of the gradient
compared to state-of-the-art approaches.

We ﬁrst review variational inference, with a focus on
stochastic gradient methods. We then present our key
contribution, rejection sampling variational inference
(rsvi), showing how to use eﬃcient rejection samplers
to produce low-variance stochastic gradients of the
variational objective. We study two concrete exam-
ples, analyzing rejection samplers for the gamma and
Dirichlet to produce new reparameterization gradients
for their corresponding variational factors. Finally, we
analyze two datasets with a deep exponential family
(def) [Ranganath et al., 2015], comparing rsvi to the
state of the art. We found that rsvi achieves a sig-
niﬁcant reduction in variance and faster convergence
of the elbo. Code for all experiments is provided at
github.com/blei-lab/ars-reparameterization.

2 Variational Inference

Let p(x, z) be a probabilistic model, i.e., a joint prob-
ability distribution of data x and latent (unobserved)
variables z. In Bayesian inference, we are interested
in the posterior distribution p(z
p(x) . For most
models, the posterior distribution is analytically in-
tractable and we have to use an approximation, such
as Monte Carlo methods or variational inference. In
this paper, we focus on variational inference.

x) = p(x,z)

|

In variational inference, we approximate the posterior
with a variational family of distributions q(z ; θ), pa-
rameterized by θ. Typically, we choose the variational
parameters θ that minimize the Kullback-Leibler (kl)
divergence between q(z ; θ) and p(z
x). This minimiza-
|
tion is equivalent to maximizing the elbo [Jordan
et al., 1999], deﬁned as

(θ) = Eq(z ;θ) [f (z)] + H[q(z ; θ)],
L
f (z) := log p(x, z),

(1)

H[q(z ; θ)] := Eq(z ;θ)[

log q(z ; θ)].

−

When the model and variational family satisfy con-
jugacy requirements, we can use coordinate ascent to
ﬁnd a local optimum of the elbo [Blei et al., 2016]. If
the conjugacy requirements are not satisﬁed, a com-
mon approach is to build a Monte Carlo estimator of

the gradient of the elbo [Paisley et al., 2012, Ran-
ganath et al., 2014, Mnih and Gregor, 2014, Sali-
mans and Knowles, 2013, Kingma and Welling, 2014].
This results in a stochastic optimization procedure,
where diﬀerent Monte Carlo estimators of the gradi-
ent amount to diﬀerent algorithms. We review below
two common estimators: the score function estimator
and the reparameterization trick.1

Score function estimator. The score function esti-
mator, also known as the log-derivative trick or rein-
force [Williams, 1992, Glynn, 1990], is a general way
to estimate the gradient of the elbo [Paisley et al.,
2012, Ranganath et al., 2014, Mnih and Gregor, 2014].
The score function estimator expresses the gradient as
an expectation with respect to q(z ; θ):

(θ) = Eq(z ;θ)[f (z)

θ

θ log q(z ; θ)] +

θH[q(z ; θ)].

L

∇

∇

∇
We then form Monte Carlo estimates by approx-
imating the expectation with independent samples
from the variational distribution. Though it is very
general, the score function estimator typically suf-
fers from high variance.
In practice we also need
to apply variance reduction techniques such as Rao-
Blackwellization [Casella and Robert, 1996] and con-
trol variates [Robert and Casella, 2004].

Reparameterization trick. The reparameteriza-
tion trick [Salimans and Knowles, 2013, Kingma and
Welling, 2014, Price, 1958, Bonnet, 1964] results in a
lower variance estimator compared to the score func-
tion, but it is not as generally applicable. It requires
that: (i) the latent variables z are continuous; and (ii)
we can simulate from q(z ; θ) as follows,

z = h(ε, θ),

with ε

s(ε).

(2)

∼

Here, s(ε) is a distribution that does not depend on
the variational parameters; it is typically a standard
normal or a standard uniform. Further, h(ε, θ) must
be diﬀerentiable with respect to θ. In statistics, this
is known as a non-central parameterization and has
been shown to be helpful in, e.g., Markov chain Monte
Carlo methods [Papaspiliopoulos et al., 2003].

Using (2), we can move the derivative inside the ex-
pectation and rewrite the gradient of the elbo as

(θ) = Es(ε) [

θ

zf (h(ε, θ))

θh(ε, θ)] +

θH[q(z ; θ)].

L

∇

∇

∇

∇
Empirically, the reparameterization trick has been
shown to be beneﬁcial over direct Monte Carlo es-
1In this paper, we assume for simplicity that the gra-
dient of the entropy ∇θH[q(z ; θ)] is available analytically.
The method that we propose in Section 3 can be easily
extended to handle non-analytical entropy terms. Indeed,
the resulting estimator of the gradient may have lower vari-
ance when the analytic gradient of the entropy is replaced
by its Monte Carlo estimate. Here we do not explore that.

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

timation of the gradient using the score fuction es-
timator [Salimans and Knowles, 2013, Kingma and
Welling, 2014, Titsias and L´azaro-Gredilla, 2014, Fan
et al., 2015]. Unfortunately, many distributions com-
monly used in variational inference, such as gamma
or Dirichlet, are not amenable to standard reparame-
terization because samples are generated using a rejec-
tion sampler [von Neumann, 1951, Robert and Casella,
2004], introducing discontinuities to the mapping. We
next show that taking a novel view of the acceptance-
rejection sampler lets us perform exact reparameteri-
zation.

3 Reparameterizing the

Acceptance-Rejection Sampler

The basic idea behind reparameterization is to rewrite
simulation from a complex distribution as a determin-
istic mapping of its parameters and a set of simpler
random variables. We can view the rejection sam-
pler as a complicated deterministic mapping of a (ran-
dom) number of simple random variables such as uni-
forms and normals. This makes it tempting to take
the standard reparameterization approach when we
consider random variables generated by rejection sam-
plers. However, this mapping is in general not contin-
uous, and thus moving the derivative inside the expec-
tation and using direct automatic diﬀerentiation would
not necessarily give the correct answer.

Our insight is that we can overcome this problem by in-
stead considering only the marginal over the accepted
sample, analytically integrating out the accept-reject
variable. Thus, the mapping comes from the proposal
step. This is continuous under mild assumptions, en-
abling us to greatly extend the class of variational fam-
ilies amenable to reparameterization.

We ﬁrst review rejection sampling and present the
reparameterized rejection sampler. Next we show how
to use it to calculate low-variance gradients of the
elbo. Finally, we present the complete stochastic op-
timization for variational inference, rsvi.

3.1 Reparameterized Rejection Sampling

Acceptance-Rejection sampling is a powerful way of
simulating random variables from complex distribu-
tions whose inverse cumulative distribution functions
are not available or are too expensive to evaluate [De-
vroye, 1986, Robert and Casella, 2004]. We consider an
alternative view of rejection sampling in which we ex-
plicitly make use of the reparameterization trick. This
view of the rejection sampler enables our variational
inference algorithm in Section 3.2.

To generate samples from a distribution q(z ; θ) us-

Algorithm 1 Reparameterized Rejection Sampling

Input: target q(z ; θ), proposal r(z ; θ), and constant

Mθ, with q(z ; θ)

Mθr(z ; θ)

q(z ; θ)

∼

≤

0

Output: ε such that h(ε, θ)
1: i
←
2: repeat
i
3:
Propose εi
4:
[0, 1]
Simulate ui
5:
6: until ui < q(h(εi,θ) ;θ)
Mθr(h(εi,θ) ;θ)
7: return εi

∼
∼ U

i + 1

s(ε)

←

≤

∞

ing rejection sampling, we ﬁrst sample from a proposal
distribution r(z ; θ) such that q(z ; θ)
Mθr(z ; θ) for
some Mθ <
. In our version of the rejection sampler,
we assume that the proposal distribution is reparame-
r(z ; θ) is equivalent
terizable, i.e., that generating z
to generating ε
s(ε) (where s(ε) does not depend
on θ) and then setting z = h(ε, θ) for a diﬀerentiable
function h(ε, θ). We then accept the sample with prob-
(cid:111)
; otherwise, we reject the
ability min
sample and repeat the process. We illustrate this in
Figure 1 and provide a summary of the method in Al-
gorithm 1, where we consider the output to be the
(accepted) variable ε, instead of z.

q(h(ε,θ) ;θ)
Mθr(h(ε,θ) ;θ)

1,

∼

∼

(cid:110)

The ability to simulate from r(z ; θ) by a reparameteri-
zation through a diﬀerentiable h(ε, θ) is not needed
for the rejection sampler to be valid. However, this
is indeed the case for the rejection sampler of many
common distributions.

3.2 The Reparameterized Rejection Sampler

in Variational Inference

We now use reparameterized rejection sampling to de-
velop a novel Monte Carlo estimator of the gradient
of the elbo. We ﬁrst rewrite the elbo in (1) as an
expectation in terms of the transformed variable ε,
(θ) = Eq(z ;θ) [f (z)] + H[q(z ; θ)]

L

= Eπ(ε ;θ) [f (h(ε, θ))] + H[q(z ; θ)].

(3)

In this expectation, π(ε ; θ) is the distribution of the
accepted sample ε in Algorithm 1. We construct it by
marginalizing over the auxiliary uniform variable u,

π(ε ; θ) =

π(ε, u ; θ)du

(cid:90)

(cid:90)

=

Mθs(ε)1

(cid:20)
0 < u <

q (h(ε, θ) ; θ)
Mθr (h(ε, θ) ; θ)

(cid:21)

du

= s(ε)

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

,

(4)

where 1[x
A] is the indicator function, and re-
call that Mθ is a constant used in the rejection sam-

∈

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

Figure 1: Example of a reparameterized rejection sampler for q(z ; θ) = Gamma(θ, 1), shown here with θ = 2. We
use the rejection sampling algorithm of Marsaglia and Tsang [2000], which is based on a nonlinear transformation
h(ε, θ) of a standard normal ε
(0, 1) (c.f. Eq. 10), and has acceptance probability of 0.98 for θ = 2. The
marginal density of the accepted value of ε (integrating out the acceptance variables, u1:i) is given by π(ε ; θ).
We compute unbiased estimates of the gradient of the elbo (6) via Monte Carlo, using Algorithm 1 to rejection
sample ε
π(ε ; θ). By reparameterizing in terms of ε, we obtain a low-variance estimator of the gradient for
challenging variational distributions.

∼ N

∼

∼

∼ U

s(ε) and u

pler. This can be seen by the algorithmic deﬁni-
tion of the rejection sampler, where we propose values
ε
[0, 1] until acceptance, i.e., un-
til u < q(h(ε,θ) ;θ)
Mθr(h(ε,θ) ;θ) . Eq. 3 follows intuitively, but we
formalize it in Proposition 1.
Proposition 1. Let f be any measurable function,
and ε
π(ε ; θ), deﬁned by (4) (and implicitly by Al-
gorithm 1). Then

∼

Eπ(ε ;θ) [f (h(ε, θ))] =

f (z)q(z ; θ)dz.

Proof. Using the deﬁnition of π(ε ; θ),

Eπ(ε ;θ) [f (h(ε, θ))] =

f (h(ε, θ)) s(ε)

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

dε

(cid:90)

=

f (z)r(z ; θ)

dz =

f (z)q(z ; θ)dz,

(cid:90)

q(z ; θ)
r(z ; θ)

where the second to last equality follows because
h(ε, θ), ε

s(ε) is a reparameterization of r(z ; θ).

∼

We can now compute the gradient of Eq(z ;θ)[f (z)]
based on Eq. 3,

θEπ(ε ;θ)[f (h(ε, θ))]

(cid:90)

(cid:90)

θEq(z ;θ)[f (z)] =
∇
= Eπ(ε ;θ)[

(cid:124)

+

∇
θf (h(ε, θ))]
∇
(cid:125)
(cid:123)(cid:122)
=:grep
(cid:20)
f (h(ε, θ))

+ Eπ(ε ;θ)

(cid:124)

∇
(cid:123)(cid:122)
=:gcor

(see the supplement for all details.) We deﬁne grep as
the reparameterization term, which takes advantage of
gradients with respect to the model and its latent vari-
ables; we deﬁne gcor as a correction term that accounts
for not using r(z ; θ)
q(z ; θ).
Using (5), the gradient of the elbo in (1) can be writ-
ten as

≡

θ

∇

L

(θ) = grep + gcor +

θH[q(z ; θ)],

(6)

∇

and thus we can build an unbiased one-sample Monte
(θ) as
Carlo estimator ˆg

θ
≈ ∇

L
ˆg := ˆgrep + ˆgcor +
ˆgrep =

zf (z)(cid:12)

∇

ˆgcor = f (h(ε, θ))

∇
(cid:12)z=h(ε,θ)∇
θ log

∇

θH[q(z ; θ)],
θh(ε, θ)

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

,

(7)

where ε is a sample generated using Algorithm 1. Of
course, one could generate more samples of ε and av-
erage, but we have found a single sample to suﬃce in
practice.

Note if h(ε, θ) is invertible in ε then we can simplify
the evaluation of the gradient of the log-ratio in gcor,

θ log

∇

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

=

(5)

θ log

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

(cid:21)
,

(cid:125)

θ log q(h(ε, θ) ; θ) +

θ log

∇

∇

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dh
dε

(cid:12)
(cid:12)
(ε, θ)
(cid:12)
(cid:12)

.

(8)

See the supplementary material for details.

where we have used the log-derivative trick and rewrit-
ten the integrals as expectations with respect to π(ε ; θ)

Alternatively, we could rewrite the gradient as an ex-
pectation with respect to s(ε) (this is an intermediate

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

step in the derivation shown in the supplement),

∇

θEq(z ;θ)[f (z)] = Es(ε)
(cid:20) q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

+ Es(ε)

(cid:20) q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ) ∇

f (h(ε, θ))

θ log

∇

(cid:21)

θf (h(ε, θ))

+

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

(cid:21)

,

and build an importance sampling-based Monte Carlo
estimator, in which the importance weights would be
q (h(ε, θ) ; θ) /r (h(ε, θ) ; θ). However, we would ex-
pect this approach to be beneﬁcial for low-dimensional
problems only, since for high-dimensional z the vari-
ance of the importance weights would be too high.

Algorithm 2 Rejection Sampling Variational Infer-
ence
Input: Data x, model p(x, z), variational

family

q(z ; θ)

Output: Variational parameters θ∗
1: repeat
2:
3:
4:
5: Update θn+1 = θn + ρnˆgn
6: until convergence

Run Algorithm 1 for θn to obtain a sample ε
Estimate the gradient ˆgn at θ = θn (Eq. 7)
Calculate the stepsize ρn (Eq. 9)

3.3 Full Algorithm

We now describe the full variational algorithm based
In Sec-
on reparameterizing the rejection sampler.
tion 5 we give concrete examples of how to reparame-
terize common variational families.

We make use of Eq. 6 to obtain a Monte Carlo esti-
mator of the gradient of the elbo. We use this es-
timate to take stochastic gradient steps. We use the
step-size sequence ρn proposed by Kucukelbir et al.
[2016] (also used by Ruiz et al. [2016]), which combines
rmsprop [Tieleman and Hinton, 2012] and Adagrad
[Duchi et al., 2011]. It is

ρn = η

n−1/2+δ

·
sn = t (ˆgn)2 + (1

·

(cid:16)

1 + √sn

(cid:17)−1

,

t)sn−1,

−

(9)

where n is the iteration number. We set δ = 10−16 and
t = 0.1, and we try diﬀerent values for η. (When θ is
a vector, the operations above are element-wise.)

We summarize the full method in Algorithm 2. We
refer to our method as rsvi.

4 Related Work

The reparameterization trick has also been used in
automatic diﬀerentiation variational inference (advi)

[Kucukelbir et al., 2015, 2016]. advi applies a trans-
formation to the random variables such that their sup-
port is on the reals and then places a Gaussian vari-
ational posterior approximation over the transformed
variable ε. In this way, advi allows for standard repa-
rameterization, but it cannot ﬁt gamma or Dirichlet
variational posteriors, for example. Thus, advi strug-
gles to approximate probability densities with singu-
In contrast,
larities, as noted by Ruiz et al. [2016].
our approach allows us to apply the reparameteriza-
tion trick on a wider class of variational distributions,
which may be more appropriate when the exact pos-
terior exhibits sparsity.

In the literature, we can ﬁnd other lines of research
that focus on extending the reparameterization gradi-
ent to other distributions. For the gamma distribution,
Knowles [2015] proposed a method based on approx-
imations of the inverse cumulative density function;
however, this approach is limited only to the gamma
distribution and it involves expensive computations.
For general expectations, Schulman et al. [2015] ex-
pressed the gradient as a sum of a reparameterization
term and a correction term to automatically estimate
the gradient in the context of stochastic computation
graphs. However, it is not possible to directly apply it
to variational inference with acceptance-rejection sam-
pling. This is due to discontinuities in the accept–
reject step and the fact that a rejection sampler pro-
duces a random number of random variables. Recently,
another line of work has focused on applying reparame-
terization to discrete latent variable models [Maddison
et al., 2017, Jang et al., 2017] through a continuous re-
laxation of the discrete space.
The generalized reparameterization (g-rep) method
[Ruiz et al., 2016] exploits the decomposition of the
gradient as grep + gcor by applying a transformation
based on standardization of the suﬃcient statistics of
z. Our approach diﬀers from g-rep: instead of search-
ing for a transformation of z that makes the distribu-
tion of ε weakly dependent on the variational parame-
ters (namely, standardization), we do the opposite by
choosing a transformation of a simple random variable
ε such that the distribution of z = h(ε, θ) is almost
equal to q(z ; θ). For that, we reuse the transforma-
tions typically used in rejection sampling. Rather than
having to derive a new transformation for each vari-
ational distribution, we leverage decades of research
on transformations in the rejection sampling literature
[Devroye, 1986]. In rejection sampling, these transfor-
mations (and the distributions of ε) are chosen so that
they have high acceptance probability, which means
0 with rsvi. In Sec-
we should expect to obtain gcor
tions 5 and 6 we compare rsvi with g-rep and show
that it exhibits signiﬁcantly lower variance, thus lead-

≈

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

ing to faster convergence of the inference algorithm.

Finally, another line of research in non-conjugate vari-
ational inference aims at developing more expressive
variational families [Salimans et al., 2015, Tran et al.,
2016, Maaløe et al., 2016, Ranganath et al., 2016].
rsvi can extend the reparameterization trick to these
methods as well, whenever rejection sampling is used
to generate the random variables.

5 Examples of Acceptance-Rejection

Reparameterization

As two examples, we study rejection sampling and re-
parameterization of two well-known distributions: the
gamma and Dirichlet. These have been widely used
as variational families for approximate Bayesian infer-
ence. We emphasize that rsvi is not limited to these
two cases, it applies to any variational family q(z ; θ)
for which a reparameterizable rejection sampler exists.
We provide other examples in the supplement.

5.1 Gamma Distribution

One of the most widely used rejection sampler is for the
gamma distribution. Indeed, the gamma distribution
is also used in practice to generate e.g. beta, Dirich-
let, and Student’s t-distributed random variables. The
gamma distribution, Gamma(α, β), is deﬁned by its
shape α and rate β.

For Gamma(α, 1) with α
1, Marsaglia and Tsang
≥
[2000] developed an eﬃcient rejection sampler. It uses
a truncated version of the following reparameterization

z = hGamma(ε, α) :=

α

(cid:18)

(cid:19) (cid:18)

1
3

−

1 +

ε
√9α

(cid:19)3

,

3

−

(10)

ε

s(ε) :=

(0, 1).

N

∼
When β
= 1, we divide z by the rate β and obtain a
sample distributed as Gamma(α, β). The acceptance
it exceeds 0.95 and 0.98 for
probability is very high:
α = 1 and α = 2, respectively. In fact, as α
we
have that π(ε ; θ)
s(ε), which means that the accep-
tance probability approaches 1. Figure 1 illustrates the
involved functions and distributions for shape α = 2.

→ ∞

→

For α < 1, we observe that z = u1/α ˜z is distributed as
Gamma(α, β) for ˜z
[0, 1]
[Stuart, 1962, Devroye, 1986], and apply the rejection
sampler above for ˜z.

Gamma(α+1, β) and u

∼ U

∼

We now study the quality of the transformation in
(10) for diﬀerent values of the shape parameter α.
Since π(ε ; θ)
, we should expect
the correction term gcor to decrease with α. We show

s(ε) as α

→ ∞

→

Figure 2: The correction term of rsvi, and as a result
the gradient variance, decreases with increasing shape
α. We plot absolute value of the gradient of the log-
ratio between the target (gamma) and proposal distri-
butions as a function of ε.

that in Figure 2, where we plot the log-ratio (8) from
the correction term as a function of ε for four val-
ues of α. We additionally show in Figure 3 that the
distribution π(ε ; θ) converges to s(ε) (a standard nor-
mal) as α increases. For large α, π(ε ; θ)
s(ε) and
the acceptance probability of the rejection sampler ap-
proaches 1, which makes the correction term negligi-
ble. In Figure 3, we also show that π(ε ; θ) converges
faster to a standard normal than the standardization
procedure used in g-rep. We exploit this property—
that performance improves with α—to artiﬁcially in-
crease the shape for any gamma distribution. We now
explain this trick, which we call shape augmentation.

≈

Shape augmentation. Here we show how to ex-
ploit the fact that the rejection sampler improves for
increasing shape α. We make repeated use of the trick
above, using uniform variables, to control the value of
α that goes into the rejection sampler. That is, to com-
pute the elbo for a Gamma(α, 1) distribution, we can
ﬁrst express the random variable as z = ˜z (cid:81)B
(for some positive integer B), ˜z
and ui
duction, since ˜zu

1
α+i−1
i=1 u
i
Gamma(α + B, 1)
∼
[0, 1]. This can be proved by in-

1
α+B−1
B
∼
˜zu
2, 1), etc. Hence,
Gamma(α + B
we can apply the rejection sampling framework for
˜z
Gamma(α + B, 1) instead of the original z. We
study the eﬀect of shape augmentation on the variance
in Section 5.2.

u
B−1 ∼

Gamma(α + B

i.i.d.
∼ U

1
α+B−1
B

1
α+B−2

1, 1),

−

∼

−

5.2 Dirichlet Distribution

The Dirichlet(α1:K) distribution, with concentra-
is a K-dimensional mul-
tion parameters α1:K,
tivariate distribution with K
free-
1 degrees of

−

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

Figure 3: In the distribution on the transformed space ε for a gamma distribution we can see that the rejection
sampling-inspired transformation converges faster to a standard normal. Therefore it is less dependent on the
parameter α, which implies a smaller correction term. We compare the transformation of rsvi (this paper) with
the standardization procedure suggested in g-rep [Ruiz et al., 2016], for shape parameters α =

1, 2, 10

{

.
}

component of the gradient, based on simulated data
from a Dirichlet distribution with K = 100 compo-
nents, uniform prior, and N = 100 trials. We compare
the variance of rsvi (for various shape augmentation
settings) with the g-rep approach [Ruiz et al., 2016].
rsvi performs better even without the augmentation
trick, and signiﬁcantly better with it.

6 Experiments

In Section 5 we compared rejection sampling varia-
tional inference (rsvi) with generalized reparameteri-
zation (g-rep) and found a substantial variance reduc-
tion on synthetic examples. Here we evaluate rsvi on a
more challenging model, the sparse gamma deep expo-
nential family (def) [Ranganath et al., 2015]. On two
real datasets, we compare rsvi with state-of-the-art
methods: automatic diﬀerentiation variational infer-
ence (advi) [Kucukelbir et al., 2015, 2016], black-box
variational inference (bbvi) [Ranganath et al., 2014],
and g-rep [Ruiz et al., 2016].

×

Data.
The datasets we consider are the Olivetti
faces2 and Neural Information Processing Systems
(nips) 2011 conference papers. The Olivetti faces
dataset consists of 64
64 gray-scale images of human
faces in 8 bits, i.e., the data is discrete and in the set
. In the nips dataset we have documents
0, . . . , 255
}
{
in a bag-of-words format with an eﬀective vocabulary
of 5715 words.
Model. The sparse gamma def [Ranganath et al.,
2015] is a multi-layered probabilistic model that mim-
ics the architecture of deep neural networks. It mo-
dels the data using a set of local latent variables z(cid:96)
n,k
where n indexes observations, k components, and (cid:96)
layers. These local variables are connected between
layers through global weights w(cid:96)
k,k(cid:48). The observations
are xn,d, where d denotes dimension. The joint proba-

2http://www.cl.cam.ac.uk/research/dtg/

attarchive/facedatabase.html

Figure 4: rsvi (this paper) achieves lower variance
compared to g-rep [Ruiz et al., 2016]. The estimated
variance is for the ﬁrst component of Dirichlet ap-
proximation to a multinomial likelihood with uniform
Dirichlet prior. Optimal concentration is α = 2, and
B denotes shape augmentation.

dom.
the fact
z1:K = ((cid:80)

To simulate
if ˜zk
that
(cid:96) ˜z(cid:96))−1 (˜z1, . . . , ˜zK)(cid:62)

∼

random variables we use
Gamma(αk, 1)
then

i.i.d.,

Dirichlet(α1:K).

∼

Thus, we make a change of variables to reduce the
problem to that of simulating independent gamma dis-
tributed random variables,

Eq(z1:K ;α1:K )[f (z1:K)] =
(cid:33) K
(cid:89)

(cid:32)

(cid:90)

=

f

˜z1:K
(cid:96)=1 ˜z(cid:96)

(cid:80)K

k=1

Gamma(˜zk ; αk, 1)d˜z1:K.

We apply the transformation in Section 5.1 for the
gamma-distributed variables, ˜zk = hGamma(εk, αk),
where the variables εk are generated by independent
gamma rejection samplers. To showcase this, we study
a simple conjugate model where the exact gradient and
posterior are available: a multinomial likelihood with
Dirichlet prior and Dirichlet variational distribution.
In Figure 4 we show the resulting variance of the ﬁrst

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

rsvi B = 1 rsvi B = 4

Min
Median
Max

6.0e
4
−
9.0e7
1.2e17

1.2e
3
−
2.9e7
3.4e14

g-rep
2.7e
3
−
1.6e12
1.5e17

rsvi B = 1 rsvi B = 4

Min
Median
Max

1.8e
3
−
1.2e4
1.4e12

1.5e
3
−
4.5e3
1.6e11

g-rep
2.6e
3
−
1.5e7
3.5e12

Table 1: The rsvi gradient (this paper) exhibits lower variance than g-rep [Ruiz et al., 2016]. We show estimated
variance, based on 10 samples, of g-rep and rsvi (for B = 1, 4 shape augmentation steps), for parameters at
the initialization point (left) and at iteration 2600 in rsvi (right), estimated for the nips data.

best one in Figure 5. We can see that rsvi has a sig-
niﬁcantly faster initial improvement than any of the
other methods.3 The wall-clock time for rsvi is based
on a Python implementation (average 1.5s per itera-
tion) using the automatic diﬀerentiation package au-
tograd [Maclaurin et al., 2015]. We found that rsvi is
approximately two times faster than g-rep for com-
parable implementations. One reason for this is that
the transformations based on rejection sampling are
cheaper to evaluate. Indeed, the research literature on
rejection sampling is heavily focused on ﬁnding cheap
and eﬃcient transformations.
For the nips dataset, we now compare the variance
of the gradients between the two estimators, rsvi and
g-rep, for diﬀerent shape augmentation steps B. In
Table 1 we show the minimum, median, and maxi-
mum values of the variance across all dimensions. We
can see that rsvi again clearly outperforms g-rep in
terms of variance. Moreover, increasing the number of
augmentation steps B provides even further improve-
ments.

We introduced rejection sampling variational inference
(rsvi), a method for deriving reparameterization gra-
dients when simulation from the variational distribu-
tion is done using a acceptance-rejection sampler. In
practice, rsvi leads to lower-variance gradients than
other state-of-the-art methods. Further, it enables re-
parameterization gradients for a large class of varia-
tional distributions, taking advantage of the eﬃcient
transformations developed in the rejection sampling
literature.

This work opens the door to other strategies that “re-
move the lid” from existing black-box samplers in the
service of variational inference. As future work, we
can consider more complicated simulation algorithms
with accept-reject-like steps, such as adaptive rejec-
tion sampling, importance sampling, sequential Monte
Carlo, or Markov chain Monte Carlo.

bilistic model is deﬁned as

7 Conclusions

Figure 5: rsvi (this paper) presents a signiﬁcantly
faster initial improvement of the evidence lower bound
(elbo) as a function of wall-clock time. The model
is a sparse gamma def, applied to the Olivetti faces
dataset, and we compare with advi [Kucukelbir et al.,
2016], bbvi [Ranganath et al., 2014], and g-rep [Ruiz
et al., 2016].

z(cid:96)
n,k ∼

Gamma

αz,

xn,d

Poisson

∼

(cid:33)

,

(cid:80)

αz
k,k(cid:48)z(cid:96)+1
k(cid:48) w(cid:96)
n,k(cid:48)
(cid:33)

w0

k,dz1

n,k

.

(cid:32)

(cid:32)

(cid:88)

k

(11)

We set αz = 0.1 in the experiments. All priors on the
weights are set to Gamma(0.1, 0.3), and the top-layer
local variables priors are set to Gamma(0.1, 0.1). We
use 3 layers, with 100, 40, and 15 components in each.
This is the same model that was studied by Ruiz et al.
[2016], where g-rep was shown to outperform both
bbvi (with control variates and Rao-Blackwellization),
as well as advi. In the experiments we follow their ap-
proach and parameterize the variational approximat-
ing gamma distribution using the shape and mean. To
avoid constrained optimization we use the transform
θ = log(1+exp(ϑ)) for non-negative variational param-
eters θ, and optimize ϑ in the unconstrained space.

Results.
η

0.75, 1, 2, 5
}

∈ {

For

the Olivetti

faces we explore
and show the resulting elbo of the

3The results of g-rep, advi and bbvi where reproduced

with permission from Ruiz et al. [2016].

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

Acknowledgements

Christian A. Naesseth is supported by CADICS, a Lin-
naeus Center, funded by the Swedish Research Coun-
cil (VR). Francisco J. R. Ruiz is supported by the
EU H2020 programme (Marie Sk(cid:32)lodowska-Curie grant
agreement 706760). Scott W. Linderman is supported
by the Simons Foundation SCGB-418011. This work
is supported by NSF IIS-1247664, ONR N00014-11-
1-0651, DARPA PPAML FA8750-14-2-0009, DARPA
SIMPLEX N66001-15-C-4032, Adobe, and the Alfred
P. Sloan Foundation. The authors would like to thank
Alp Kucukelbir and Dustin Tran for helpful comments
and discussion.

References

M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and
L. K. Saul. An introduction to variational meth-
ods for graphical models. Machine Learning, 37(2):
183–233, Nov. 1999.

D. P. Kingma and M. Welling. Auto-encoding varia-
tional Bayes. In International Conference on Learn-
ing Representations, 2014.

D. A. Knowles.

Stochastic gradient variational
for Gamma approximating distributions.

Bayes
arXiv:1509.01631v1, 2015.

A. Kucukelbir, R. Ranganath, A. Gelman, and D. M.
Blei. Automatic variational inference in Stan. In Ad-
vances in Neural Information Processing Systems,
2015.

D. M. Blei, A. Kucukelbir, and J. D. McAuliﬀe.
inference: A review for statisticians.

Variational
arXiv:1601.00670, 2016.

A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman,
and D. M. Blei. Automatic diﬀerentiation varia-
tional inference. arXiv:1603.00788, 2016.

G. Bonnet. Transformations des signaux al´eatoires a
travers les systemes non lin´eaires sans m´emoire. An-
nals of Telecommunications, 19(9):203–220, 1964.

G. Casella and C. P. Robert. Rao-Blackwellisation of
sampling schemes. Biometrika, 83(1):81–94, 1996.

L. Devroye. Non-Uniform Random Variate Genera-

tion. Springer-Verlag, 1986.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgra-
dient methods for online learning and stochastic op-
timization. Journal of Machine Learning Research,
12:2121–2159, jul 2011.

K. Fan, Z. Wang, J. Beck, J. Kwok, and K. A. Heller.
Fast second order stochastic backpropagation for
variational inference. In Advances in Neural Infor-
mation Processing Systems, 2015.

L. Maaløe, C. K. Sønderby, S. K. Sønderby, and
O. Winther. Auxiliary deep generative models.
In International Conference on Machine Learning,
2016.

D. Maclaurin, D. Duvenaud, M. Johnson, and R. P.
Adams. Autograd: Reverse-mode diﬀerentiation of
native Python, 2015. URL http://github.com/
HIPS/autograd.

C. J. Maddison, A. Mnih, and Y. W. Teh. The con-
crete distribution: A continuous relaxation of dis-
crete random variables. In International Conference
on Learning Representations, 2017.
(accepted for
publication).

G. Marsaglia and W. W. Tsang. A simple method for
generating gamma variables. ACM Transactions on
Mathematical Software, 26(3):363–372, Sept. 2000.

P. W. Glynn. Likelihood ratio gradient estimation for
stochastic systems. Communications of the ACM,
33(10):75–84, oct 1990.

A. Mnih and K. Gregor. Neural variational inference
In International

and learning in belief networks.
Conference on Machine Learning, 2014.

G. E. Hinton and D. van Camp. Keeping the neu-
ral networks simple by minimizing the description
length of the weights. In Proceedings of the Sixth An-
nual Conference on Computational Learning The-
ory, pages 5–13, New York, NY, USA, 1993. ACM.

M. D. Hoﬀman, D. M. Blei, C. Wang, and J. Paisley.
Stochastic variational inference. Journal of Machine
Learning Research, 14:1303–1347, May 2013.

E. Jang, S. Gu, and B. Poole. Categorical reparame-
In International
terization using gumbel-softmax.
Conference on Learning Representations, 2017. (ac-
cepted for publication).

J. W. Paisley, D. M. Blei, and M. I. Jordan. Variational
Bayesian inference with stochastic search. In Inter-
national Conference on Machine Learning, 2012.

O. Papaspiliopoulos, G. O. Roberts, and M. Sk¨old.
Non-centered parameterisations for hierarchical mo-
dels and data augmentation. In Bayesian Statistics
7: Proceedings of the Seventh Valencia International
Meeting, page 307. Oxford University Press, USA,
2003.

R. Price. A useful theorem for nonlinear devices having
Gaussian inputs. IRE Transactions on Information
Theory, 4(2):69–72, 1958.

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

S. Waterhouse, D. Mackay, and T. Robinson. Bayesian
In Advances in
methods for mixtures of experts.
Neural Information Processing Systems, pages 351–
357. MIT Press, 1996.

R. J. Williams. Simple statistical gradient-following
algorithms for connectionist reinforcement learning.
Machine Learning, 8(3–4):229–256, 1992.

R. Ranganath, S. Gerrish, and D. M. Blei. Black box
variational inference. In Artiﬁcial Intelligence and
Statistics, 2014.

R. Ranganath, L. Tang, L. Charlin, and D. M. Blei.
Deep exponential families. In Artiﬁcial Intelligence
and Statistics, 2015.

R. Ranganath, D. Tran, and D. M. Blei. Hierarchical
variational models. In International Conference on
Machine Learning, 2016.

D. J. Rezende, S. Mohamed, and D. Wierstra. Stochas-
tic backpropagation and approximate inference in
In International Confer-
deep generative models.
ence on Machine Learning, 2014.

C. Robert and G. Casella. Monte Carlo statistical
methods. Springer Science & Business Media, 2004.

F. J. R. Ruiz, M. K. Titsias, and D. M. Blei. The gen-
In Advances

eralized reparameterization gradient.
in Neural Information Processing Systems, 2016.

T. Salimans and D. A. Knowles. Fixed-form varia-
tional posterior approximation through stochastic
linear regression. Bayesian Analysis, 8(4):837–882,
2013.

T. Salimans, D. P. Kingma, and M. Welling. Markov
chain Monte Carlo and variational inference: Bridg-
ing the gap. In International Conference on Machine
Learning, 2015.

J. Schulman, N. Heess, T. Weber, and P. Abbeel.
Gradient estimation using stochastic computation
graphs. In Advances in Neural Information Process-
ing Systems, 2015.

A. Stuart. Gamma-distributed products of indepen-
dent random variables. Biometrika, 49:64–65, 1962.

T. Tieleman and G. Hinton. Lecture 6.5-RMSPROP:
Divide the gradient by a running average of its re-
cent magnitude. Coursera: Neural Networks for Ma-
chine Learning, 4, 2012.

M. K. Titsias and M. L´azaro-Gredilla. Doubly stochas-
tic variational Bayes for non-conjugate inference.
In International Conference on Machine Learning,
2014.

D. Tran, R. Ranganath, and D. M. Blei. The vari-
ational Gaussian process. In International Confer-
ence on Learning Representations, 2016.

J. von Neumann. Various Techniques Used in Connec-
tion with Random Digits. Journal of Research of
the National Bureau of Standards, 12:36–38, 1951.

Christian A. Naesseth†‡, Francisco J. R. Ruiz‡§, Scott W. Linderman‡, David M. Blei‡

A Supplementary Material

A.2 Derivation of the Gradient

A.1 Distribution of ε

Here we formalize the claim in the main manuscript
the accepted vari-
regarding the distribution of
able ε in the rejection sampler.
Recall
that
s(ε) is equivalent to z
z = h(ε, θ), ε
r(z ; θ), and
∼
Mθr(z ; θ). For simplicity we consider
that q(z ; θ)
the univariate continuous case in the exposition below,
but the result also holds for the discrete and multivari-
ate settings. The cumulative distribution function for
the accepted ε is given by

≤

∼

(cid:19)

q(h(Ei, θ) ; θ)
Mθr(h(Ei, θ) ; θ)
(cid:19) (cid:35)

P(E

ε) =

P(E

ε, E = Ei)

∞
(cid:88)

i=1

≤

Ei

ε, Ui <

≤

≤

(cid:34)

∞
(cid:88)

(cid:18)

=

P

(cid:18)

i=1

i−1
(cid:89)

j=1

=

=

=

i=1
(cid:90) ε

−∞

(cid:90) ε

−∞

P

Uj

q(h(Ej, θ) ; θ)
Mθr(h(Ej, θ) ; θ)

≥

∞
(cid:88)

(cid:90) ε

s(e)

q(h(e, θ) ; θ)
Mθr(h(e, θ) ; θ)

de

−∞

s(e)

q(h(e, θ) ; θ)
r(h(e, θ) ; θ)

de

1
Mθ ·

·

s(e)

q(h(e, θ) ; θ)
r(h(e, θ) ; θ)

de.

(cid:19)

i−1
(cid:89)

(cid:18)

j=1

∞
(cid:88)

(cid:18)

i=1

1
Mθ

1
Mθ

1

−

1

−

(cid:19)i−1

Here, we have applied that z = h(ε, θ), ε
r(z ; θ), and thus
reparameterization of z

∼

s(ε) is a

∼
q(h(Ej, θ) ; θ)
Mθr(h(Ej, θ) ; θ)

(cid:19)

(cid:18)

P

Uj

≥

(cid:90) ∞

−∞

=

(cid:18)
1

s(e)

= 1

= 1

−

−

1
Mθ
1
Mθ

Es(e)

Er(z ;θ)

(cid:19)

de

q(h(e, θ) ; θ)
Mθr(h(e, θ) ; θ)

(cid:21)

−
(cid:20) q(h(e, θ) ; θ)
r(h(e, θ) ; θ)
(cid:20) q(z ; θ)
r(z ; θ)

(cid:21)

= 1

1
Mθ

.

−

The density is obtained by taking the derivative of the
cumulative distribution function with respect to ε,

d
dε

P(E

≤

ε) = s(ε)

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

,

which is the expression from the main manuscript.

The motivation from the main manuscript is basically
a standard “area-under-the-curve” or geometric argu-
ment for rejection sampling [Robert and Casella, 2004],
but for ε instead of z.

for

We provide below details
the derivation of
the gradient. We assume that h is diﬀerentiable
(almost everywhere) with respect to θ, and that
f (h(ε, θ)) q(h(ε,θ) ;θ)
r(h(ε,θ) ;θ) is continuous in θ for all ε. Then,
we have

(cid:90)

(cid:90)

(cid:124)

∇
=

θEq(z ;θ)[f (z)] =
(cid:90)

(cid:18)

s(ε)

θ

∇
f (h(ε, θ))

θEπ(ε ;θ)[f (h(ε, θ))]
(cid:19)
q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

dε

=

s(ε)

∇
q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ) ∇

+

s(ε)f (h(ε, θ))

θ

∇

= Eπ(ε ;θ)[

θf (h(ε, θ))]
∇
(cid:125)
(cid:123)(cid:122)
=:grep
(cid:20)
f (h(ε, θ))

+ Eπ(ε ;θ)

(cid:124)

θf (h(ε, θ)) dε
(cid:18) q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)
+

(cid:19)

dε

θ log

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)

(cid:21)
,

(cid:125)

∇
(cid:123)(cid:122)
=:gcor

where in the last step we have identiﬁed π(ε ; θ) and
made use of the log-derivative trick

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

θ

∇

=

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ) ∇

θ log

q (h(ε, θ) ; θ)
r (h(ε, θ) ; θ)

.

Gradient of Log-Ratio in gcor For invertible repa-
rameterizations we can simplify the evaluation of the
gradient of the log-ratio in gcor as follows using stan-
dard results on transformation of a random variable

θ log

∇

+

θ log

∇

=

q(h(ε, θ) ; θ)
r(h(ε, θ) ; θ)
(cid:12)
dh
(cid:12)
(ε, θ)
(cid:12)
dε
(cid:12) − ∇

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∇

θ log q(h(ε, θ) ; θ)+

θ log s(h−1(h(ε, θ), θ))
(cid:125)

(cid:124)

(cid:123)(cid:122)
= s(ε)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dh
dε

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∇

=

θ log q(h(ε, θ) ; θ) +

θ log

(ε, θ)

.

∇

A.3 Examples of Reparameterizable

Rejection Samplers

We show in Table 2 some examples of reparameteriz-
able rejection samplers for three distributions, namely,
the gamma, the truncated normal, and the von Misses
distributions (for more examples, see Devroye [1986]).
We show the distribution q(z ; θ), the transformation
h(ε, θ), and the proposal s(ε) used in the rejection sam-
pler.

We show in Table 3 six examples of distributions that
can be reparameterized in terms of auxiliary gamma-
distributed random variables. We show the distribu-
tion q(z ; θ), the distribution of the auxiliary gamma
random variables p(˜z ; θ), and the mapping z = g(˜z, θ).

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

q(z ; θ)

Gamma(α, 1)

Truncated

(0, 1, a,

N

)

∞

h(ε, θ)

(cid:0)α

(cid:1) (cid:16)

1
3

−
(cid:112)a2

(cid:17)3

1 + ε√

9α−3

2 log ε

−
2ρ , ρ = r−

s(ε)

ε

ε

(0, 1)

∼ N

[0, 1]

∼ U

ε

1, 1]

[
−

∼ U

vonMises(κ)

sign(ε) arccos

(cid:16) 1+c cos(πε)
c+cos(πε)

(cid:17)

, c = 1+ρ2

√
2κ , r = 1 + √1 + 4κ2

2r

Table 2: Examples of reparameterizable rejection samplers; many more can be found in Devroye [1986]. The ﬁrst
column is the distribution, the second column is the transformation h(ε, θ), and the last column is the proposal
s(ε).

q(z ; θ)

g(˜z, θ)

p(˜z ; θ)

Beta(α, β)

Dirichlet(α1:K)

˜z1
˜z1 + ˜z2
(˜z1, . . . , ˜zK)(cid:62)

1
(cid:80)
(cid:96) ˜z(cid:96)

St(ν)

χ2(k)

F(d1, d2)

Nakagami(m, Ω)

(cid:114) ν
2˜z1

˜z2

2˜z

d2 ˜z1
d1 ˜z2
(cid:114)
Ω˜z
m

˜z1

Gamma(α, 1), ˜z2

Gamma(β, 1)

∼

˜zk

∼

∼

Gamma(αk, 1), k = 1, . . . , K

˜z1

Gamma(ν/2, 1), ˜z2

(0, 1)

∼ N

˜z

Gamma(k/2, 1)

∼

∼

∼

˜z1

Gamma(d1/2, 1), ˜z2

Gamma(d2/2, 1)

∼

˜z

Gamma(m, 1)

∼

Table 3: Examples of random variables as functions of auxiliary random variables with reparameterizable distri-
butions. The ﬁrst column is the distribution, the second column is a function g(˜z, θ) mapping from the auxiliary
variables to the desired variable, and the last column is the distribution of the auxiliary variables ˜z.

A.4 Reparameterizing the Gamma

above. The gradients of log q and

log r are given by

Distribution

We provide details on reparameterization of the
gamma distribution. In the following we consider rate
β = 1. Note that this is not a restriction, we can
always reparameterize the rate. The density of the
gamma random variable is given by

q(z ; α) =

zα−1e−z
Γ(α)

,

where Γ(α) is the gamma function. We make use of
the reparameterization deﬁned by

(cid:18)

(cid:19) (cid:18)

1
3

−

1 +

ε
√9α

(cid:19)3

,

3

−

z = h(ε, α) =

α

ε

∼ N

(0, 1).

Because h is invertible we can make use of the sim-
pliﬁed gradient of the log-ratio derived in Section A.2

dh(ε, α)
dα
(cid:18)

=

1 +

α log q(h(ε, α) ; α)

∇

= log(h(ε, α)) + (α

1)

−

h(ε, α) −

dh(ε,α)
dα

dh(ε, α)

ψ(α),

−

dα −
(cid:12)
(cid:12)
(ε, α)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dh
dε

log r(h(ε, α) ; α) =

α log

α

∇

−

=

2 (cid:0)α

1

−

∇

9ε
(cid:17)

(cid:1) −

1
3

(cid:16)

1 + ε√

9α−3

(9α

,

3
2

3)

−

where ψ(α) is the digamma function and

(cid:19)3

ε
√9α

3

−

−

2(9α

3
2

3)

27ε

−

(cid:18)

1 +

ε
√9α

(cid:19)2

.

3

−

