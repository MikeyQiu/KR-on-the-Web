9
1
0
2
 
p
e
S
 
8
1
 
 
]

V
C
.
s
c
[
 
 
1
v
3
8
3
8
0
.
9
0
9
1
:
v
i
X
r
a

Continual learning: A comparative study on how
to defy forgetting in classiﬁcation tasks

Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia,
Ales Leonardis, Gregory Slabaugh, Tinne Tuytelaars

1

Abstract—Artiﬁcial neural networks thrive in solving the classiﬁcation problem for a particular rigid task, where the network resembles
a static entity of knowledge, acquired through generalized learning behaviour from a distinct training phase. However, endeavours to
extend this knowledge without targeting the original task usually result in a catastrophic forgetting of this task. Continual learning shifts
this paradigm towards a network that can continually accumulate knowledge over different tasks without the need for retraining from
scratch, with methods in particular aiming to alleviate forgetting. We focus on task-incremental classiﬁcation, where tasks arrive in a
batch-like fashion, and are delineated by clear boundaries. Our main contributions concern

1)
2)
3)

a taxonomy and extensive overview of the state-of-the-art,
a novel framework to continually determine stability-plasticity trade-off of the continual learner,
a comprehensive experimental comparison of 10 state-of-the-art continual learning methods and 4 baselines.

We empirically scrutinize which method performs best, both on balanced Tiny Imagenet and a large-scale unbalanced iNaturalist
datasets. We study the inﬂuence of model capacity, weight decay and dropout regularization, and the order in which the tasks are
presented, and qualitatively compare methods in terms of required memory, computation time and storage. We make code publicly
available upon acceptance of this paper.

Index Terms—Continual Learning, Lifelong Learning, Incremental Learning, Catastrophic Forgetting, Classiﬁcation, Neural Networks
✦

1 INTRODUCTION

In recent years, machine learning models have been
reported to exhibit or even surpass human level perfor-
mance on individual tasks, such as Atari games [1] or object
recognition [2]. While these results are impressive, they
are obtained with static models incapable of adapting or
expanding their behavior over time. As such, the training
process needs to be restarted each time new data becomes
available and the model needs an update. In a dynamic
world like ours, such a practice becomes intractable when
moving to real scenarios where the data is streaming or may
only be available temporarily due to storage constraints or
privacy issues. This calls for systems that adapt continually
and keep on learning over time.

An illuminating example are natural cognitive systems.
While humans may gradually forget some old information,
a complete loss of previous knowledge is rarely attested [3].
Humans tend to learn concepts sequentially. During this
process, revisiting of old concepts (i.e., observing examples
of already learned concepts) may occur but is not essential
to preserve the knowledge of the old concepts.

Without special measures, artiﬁcial neural networks,
trained with stochastic gradient descent, cannot learn in
this manner: they suffer from catastrophic forgetting of old
concepts as new ones are learned [3]. To circumvent this

• M. De Lange, R. Aljundi and T. Tuytelaars are with the Center for Process-
ing Speech and Images, Department Electrical Engineering, KU Leuven.
E-mail: {matthias.delange, rahaf.aljundi, tinne.tuytelaars}@kuleuven.be

• M. Masana is with Computer Vision Center, UAB.

E-mail: mmasana@cvc.uab.cat
S. Parisot, X. Jia, A. Leonardis, G. Slabaugh are with Huawei.

•

Manuscript received September 6, 2019.

problem, research on artiﬁcial neural networks has focused
mostly on static tasks, where the data is usually shufﬂed to
ensure i.i.d. conditions, and performance largely increases
with repeated revisiting of the training data over multiple
epochs.

Interestingly, catastrophic forgetting has also been
observed in biological systems [4]: when learning two
time events sequentially in rats, a complete wipe out of
the ﬁrst one occurs once the second is learned. This is
exactly the catastrophic forgetting observed also when a
simple neural network is used to model events presented
sequentially. Catastrophic interference is a direct result of
a more general problem in neural networks, the so-called
“stability-plasticity” dilemma [5]. While plasticity refers to
the ability of integrating new knowledge, stability indicates
the preservation of previous knowledge while new data is
encoded. This stability-plasticity trade-off is an essential
aspect in both artiﬁcial and biological neural intelligent
systems.

Continual Learning studies the problem of learning from
an inﬁnite stream of data, with the goal of gradually ex-
tending the acquired knowledge and using it for future
learning [6]. The data can stem from changing input do-
mains (e.g. varying imaging conditions) or be associated
with different tasks (e.g. different ﬁne-grained classiﬁcation
problems). Continual learning is also referred to as lifelong
learning [6], [7], [8], [9], [10], [11], sequential learning [12],
[13], [14] or incremental learning [15], [16], [17], [18], [19],
[20], [21]. The main criterion is the sequential nature of
the learning process where only a small portion of input

data from one or a few tasks is available at once. The main
challenge is to learn without catastrophic forgetting, that is:
the performance on a previously learned task or domain
should not (signiﬁcantly) degrade over time as new tasks or
domains are added.

To keep the focus, in this work we limit the scope of
our study in three ways. First, we focus on classiﬁcation
problems only. Classiﬁcation is, arguably, one of the most
established tasks for artiﬁcial neural networks, with good
performance using relatively simple, standard and well
understood network architectures. Second, we only consider
the task-incremental setting, where data arrives in batches
and one batch corresponds to one task (i.e. a new set of
categories to be learned). In other words, we assume that
for a given task, all data becomes available simultaneously.
This way, the task can be learned in an ofﬂine manner,
with multiple epochs over all the training data belonging
to the task and shufﬂing of the task data to ensure i.i.d.
conditions during training. Importantly, this only applies
to the current task data: data from previous or future tasks
cannot be accessed. However, optimizing for a new task will
result in catastrophic forgetting, with signiﬁcant drops in
performance for the old tasks, unless special measures are
taken. The effectiveness of those measures, under different
circumstances, is exactly what this paper is about. The third
limitation in the scope concerns a multi-head conﬁguration,
where each task is allocated an exclusive output layer or
head. This is in contrast to the even more challenging setup
where all tasks share a single head, which may introduce ad-
ditional interference in learning, and increases the amount
of output nodes to chose from at test time. Instead, we
assume it is known which task a given test sample belongs
to. The setup is described in more detail in Section 2. In
Section 7 we discuss the open issues towards tackling a more
general setting.

Early research works [22], [23], [24] developed several
strategies to mitigate the forgetting under the condition
of not storing the training data, mostly at a small scale
of just a few examples and considering shallow networks.
More recently, the catastrophic forgetting problem and the
continual learning paradigm received increased attention,
especially in the task-incremental learning setting as studied
here. There is, however, limited consensus on evaluation
protocols and datasets to be used. While all papers provide
some evidence that there is at least one setting (combination
of tasks, model, hyperparameters, etc.) under which the
proposed method reduces the forgetting and outperforms
a ﬁnetuning baseline and possibly some alternative meth-
ods, there is no comprehensive experimental comparison
performed to date.

“Which method really works best?” This question can only
be answered if we can set up a fair comparison, which
leads to another question: “How can the trade-off between
stability and plasticity be set in a consistent manner, using
only data from the current task (Section 4)?” We propose a
principled framework to do so and, within this framework,
study whether particular settings are more advantageous
for speciﬁc methods. In particular, we seek to answer the
following inquiries with our experiments (Section 6): “What
is the effect on forgetting when altering the model size, and
thus the capacity? Is it better to go for a wider or a deeper

2

model? Are the proposed methods only effective in a vanilla
setting or can they be combined with typical regularization
schemes such as weight decay or dropout? How do methods
cope with larger scale and unbalanced tasks? Does the
order in which the classiﬁcation tasks are presented matter?
How do they compare in terms of memory, storage and
computational requirements?” These are the questions we
aim to tackle in this survey.
Related work. Continual learning has been the subject of
several recent surveys [9], [25], [26], [27]. In [9], Parisi et
al. describe a wide range of methods and approaches, yet
without an empirical evaluation or comparison. In the same
vein, [25] descriptively surveys and formalizes continual
learning, but with an emphasis on dynamic environments
for robotics. Pf ¨ulb and Gepperth [26] perform an empirical
study on catastrophic forgetting and develop a protocol for
setting hyperparameters and method evaluation. However,
they only consider two methods, namely Elastic Weight
Consolidation (EWC) [28] and Incremental Moment Match-
ing (IMM) [29]. Also, their evaluation is limited to small
datasets. Farquhar and Gal [27] survey continual learning
evaluation strategies and highlight the shortcomings of the
common practice of using one dataset with different pixel
permutations (typically permuted MNIST [30]). In addition,
they show that incremental task learning under multi-head
settings hides a big part of the true difﬁculty of the problem.
While we agree with this statement, we still opted for this
setting for our survey, as it allows us to compare existing
methods without major modiﬁcations to the proposed algo-
rithms. They also propose a couple of desiderata for eval-
uating continual learning methods. However, their study
is limited to MNIST and Fashion-MNIST datasets which
are well known to be easy datasets and far from realistic
data encountered in practical applications. Further, Kemker
et al. [31] compare three methods, EWC [28], PathNet [32]
and replay based method GeppNet [17]. Their experiments
are performed on a simple fully connected network based
on three sequences of tasks composed of 3 datasets only:
MNIST, CUB-200 and AudioSet. None of these works sys-
tematically addresses the questions raised above.
Paper overview.
In Section 2, we describe the task incre-
mental setting adopted in most of the literature and also
used in this paper. Next, Section 3 surveys different ap-
proaches towards continual learning, structuring them into
three main groups: replay-based methods, regularization-
based methods and parameter isolation-based methods.
An important issue when aiming for a fair comparison of
methods is the selection of hyperparameters (in particular,
the learning rate and the stability-plasticity trade-off). In
Section 4 we introduce a novel framework to deal with
this problem without requiring access to data from previous
or future tasks. Section 5 provides details on the methods
selected for our experimental evaluation. Section 6 describes
the actual experiments and main ﬁndings, and compares the
methods qualitatively. In Section 7 we look further ahead,
highlighting additional challenges in the ﬁeld, moving be-
yond the task incremental setting towards true continual
learning. We emphasize the relation with other ﬁelds in
Section 8. Section 9 concludes the paper. Finally, we add
implementation details and further experimental results in
the supplemental material.

2 THE TASK INCREMENTAL LEARNING SETTING

Due to the difﬁculty of the general continual learning prob-
lem and the various challenges that have to be dealt with,
most methods relax the general setting to an easier task
incremental one.

The task incremental setting considers a sequence of
tasks. In this setting one task is received at a time along
with its training data. An ofﬂine training is then performed
until convergence. With X (t) a set of data samples for task t
and Y (t) the corresponding ground truth labels, (X (t), Y (t))
randomly drawn from a distribution D(t), the goal is to
control the statistical risk of all seen tasks given limited or
no access to data (X (t), Y (t)) from previous tasks t < T :

E

(X (t),Y (t))[ℓ(ft(X (t); θ), Y (t))]

(1)

T

t=1
X

where T is the number of tasks seen so far, ft represents
the output of the network for task t and θ represents the
network weights (parameters). For the current task (last
term in the summation of Eq. 1), the statistical risk can easily
be approximated by the empirical risk

1
NT

NT

i=1
X

ℓ(f (x(T )

i

; θ), y(T )
i

)

(2)

However, for the old tasks, the data is no longer available,
so the statistical risk cannot be evaluated for new parameter
values. How to determine the optimal parameters θT , with
the aim to (approximately) optimize Eq. 1, is the main
research focus in task incremental learning. The word task
here refers to an isolated training phase of a new batch of
data that belongs to a new group of classes, a new domain,
or a different output space (e.g. scene classiﬁcation v.s.
hand written digit classiﬁcation). As such, following [33], a
ﬁner categorization distinguishes between incremental class
learning, where P (Yt) = P (Yt+1) but {Yt} 6= {Yt+1},
incremental domain learning, where P (Xt) 6= P (Xt+1) and
P (Yt) = P (Yt+1), and task incremental learning, with
P (Yt) 6= P (Yt+1) and P (Xt) 6= P (Xt+1).

In this paper, our experiments will focus on the task
incremental learning setting for a sequence of classiﬁcation
tasks.

3 CONTINUAL LEARNING APPROACHES

Early works considered the catastrophic interference prob-
lem as observed when learning sequentially examples of
different input patterns (e.g. of different categories). Several
directions have been explored, such as reducing represen-
tation overlap [22], [34], [35], [36], [37], replaying samples
or virtual samples from the past [10], [24] or introducing
dual architectures [38], [39], [40]. However, due to resource
restrictions at the time, these works were mainly consider-
ing few examples (in the order of tens) and were based on
speciﬁc shallow architectures.

With the recent increased interest in neural networks,
continual learning and catastrophic forgetting also received
more attention. [30] studied empirically the forgetting when
learning two tasks sequentially using different activation

3

functions and dropout regularization [41]. [42] studied in-
cremental task learning from a theoretical perspective with
the goal of transferring knowledge to future tasks.

More recent works have addressed continual learning
with longer sequences of tasks and larger number of exam-
ples. In the following, we will review the most important
works. We distinguish three families, based on how task
speciﬁc information is stored and used throughout the se-
quential learning process:

• Replay-based methods
• Regularization-based methods
• Parameter isolation-based methods

Note that our categorization overlaps to some extent with
that introduced in [27], [43]. However, we believe it offers
a more general overview and covers most of the existing
works. A summary can be found in Figure 1.

3.1 Replay-based methods

This line of work stores samples in their raw format or
compressed in a generative model. The stored samples from
previous tasks are replayed while learning a new task to
alleviate forgetting. These samples/pseudo-samples can be
either used for rehearsal, approximating the joint training of
previous and current tasks, or to constrain the optimization
of the new task loss not to interfere with the previous tasks.
Rehearsal methods [18], [45], [46], [61] explicitly retrain on
a subset of stored samples while training on new tasks. The
performance of these methods is upper bounded by joint
training on previous and current tasks. Most notable is the
work by Rebufﬁ et al. on incremental class learning [18],
that stores a subset of exemplars per class, selected to best
approximate the mean of each class in the feature space
being learned. The method is constrained to a ﬁxed budget,
hence to accommodate new classes, old classes’ exemplars
are re-selected according to the same criterion. In settings
where data is streaming with no clear task boundaries, [44]
suggests the use of reservoir sampling to limit the number
of stored samples to a ﬁxed budget assuming an overall i.i.d.
distributed data stream.

While rehearsal might be prone to overﬁtting the subset
of stored samples and seems to be bounded by joint training,
constrained optimization is an alternative solution that leaves
more room for backward/forward transfer. As proposed
in GEM [50] under the task incremental setting, the key
idea is to only constrain the update of the new task to not
interfere with the previous tasks. This is achieved through
projecting the estimated gradient direction on the feasible
region outlined by previous tasks’ gradients through a ﬁrst
order Taylor series approximation. A-GEM [8] has relaxed
the problem to projection on one direction estimated by
randomly selected samples from a buffer of previous tasks
data. [43] have recently extended this solution to a pure
online continual learning setting where no task boundaries
are provided. They propose to select a subset of samples that
maximally approximate the feasible region of the historical
data.

In the absence of previous samples, pseudo rehearsal is an
alternative strategy used in the early works with shallow
neural networks. Random inputs and the outputs of pre-
vious model(s) given these inputs are used to approximate

4

Rehearsal

Constrained

Prior-focused

Data-focused

Replay-based
methods

Pseudo
Rehearsal

DGR [14]
PR [47]
CCLUGM [48]
LGM [49]

iCaRL [18]
ER [44]
SER [45]
TEM [46]

Continual Learning Methods

Regularization-based
methods

Parameter isolation
methods

Fixed
Network

Dynamic
Architectures

LwF [53]
LFL [54]
EBLL [11]
DMC [55]

PackNet [56]
PathNet [32]
Piggyback [57]
HAT [58]

PNN [59]
Expert Gate [7]
RCL [60]
DAN [19]

GEM [50]
A-GEM [8]
GSS [43]

EWC [28]
IMM [29]
SI [51]
R-EWC [52]
MAS [15]
Riemannian
Walk [16]

Fig. 1: A tree diagram illustrating the different continual learning families of methods and the different branches within
each family. Leaves list example methods.

the previous tasks samples [24]. With deep networks and
large input vectors (e.g. full resolution images) random
input cannot cover the input space [47]. Recently, generative
models have shown the ability to generate high quality im-
ages [62], [63] which opened up the possibility to model the
data generating distribution and retrain on the generated
examples [14]. However, this also adds to the complexity of
training the generative model continually, with extra care to
balance the retrieved examples and avoid the mode collapse
problem.

3.2 Regularization-based methods

When no storage of raw input is possible, an important line
of works proposes an extra regularization term in the loss
function to consolidate previous knowledge when learning
on new data. The constraint of not storing any historical
sample is mainly motivated by privacy reasons, as in the
case of medical applications, in addition to being memory
efﬁcient. We can further divide these methods into data
focused and prior focused methods.

3.2.1 Data-focused methods

The basic building block in data-focused methods is the
knowledge distillation from a previous model (trained on
a previous task) to the model being trained on the new
data. It was ﬁrst proposed by [10] to use the output of
previous tasks’ models given new task input images mainly
for improving the new task performance. It has been re-
introduced by LwF [53] to mitigate forgetting and transfer
knowledge, using the output of the previous model as soft
labels for previous tasks. Other works [54], [55] have been
introduced with related ideas, however, it has been shown
that this strategy is vulnerable to domain shift between
tasks [7]. In an attempt to overcome this issue, [11] proposed
to constrain the features of each task in its own learned
low dimensional space through an incremental integration
of shallow autoencoders.

3.2.2 Prior-focused methods

To mitigate forgetting, prior focused methods estimate a
distribution over the model parameters, which is used as
a prior when learning new data. As this quickly becomes

infeasible w.r.t. the number of parameters in deep neural
networks, parameters are usually assumed independent
and an importance weight is estimated for each parameter
in the neural network. During the training of later tasks,
changes to important parameters are penalized. Elastic
weight consolidation (EWC) [28] was the ﬁrst to establish
this approach. Variational Continual Learning (VCL) has
introduced a variational framework for this family [64].
[51] estimates the importance weights during the training
of a task based on the contribution of their update to the
decrease of the loss, while [15] suggests to estimate the
importance weights online based on unlabelled data which
allows for user adaptive settings and increased ﬂexibility in
deploying the method. While the prior focused family relies
on tasks boundaries to estimate the prior distribution, [65]
extends [15] to task free settings.

Overall, the soft penalty introduced in the regularization
family might not be sufﬁcient to restrict the optimization
process to stay in the feasible region of the previous tasks,
especially with long sequences [27], which might result in
an increased forgetting of earlier tasks, as we shall show in
the experiments (Section 6).

3.3 Parameter isolation-based methods

To prevent any possible forgetting of the previous tasks, in
this family, different subsets of the model parameters are
dedicated to each task. When there is no constraints on the
size of the architecture, this can be done by freezing the set
of parameters learned after each previous task and growing
new branches for new tasks [60], [66], or even making a
complete copy of the model for each new task [7].

Alternatively, under a ﬁxed architecture, methods pro-
ceed by identifying the parts that are used for the previous
tasks and masking them out during the training of the new
task. This can be either imposed at the parameters level [32],
[56] or at the neurons level as proposed in [58].

Most of these works require a tasks oracle to activate
the corresponding masks or task branch during prediction.
Expert gate [7] avoids this problem through learning an
auto-encoder gate.

In general, this family is restricted to the task incremental
setting and better suited for learning a long sequence of

tasks when models capacity is not constrained and optimal
performance is a priority.

4 CONTINUAL HYPERPARAMETER FRAMEWORK

Methods tackling the continual learning problem typically
involve extra hyper parameters to balance the stability-
plasticity tradeoff. These hyper parameters are in many
cases tuned via a grid search that uses held-out validation
data from all tasks, including previous ones. However, the
use of such validation data violates the continual learning
setting, namely the assumption of no access to previous
tasks data. This may lead to overoptimistic results, that can-
not be reproduced in a true continual learning setting. As it
is of our concern in this survey to provide a comprehensive
and fair study on the different continual learning methods,
we need to deﬁne a standard protocol to set the hyper
parameters of all methods adhering to the studied continual
learning setting. This allows not only for a fair comparison
over the existing approaches but for a general strategy that
can be used in future research as well as in industry to
deploy continual learning methods in real situations.

To comply with the considered setting of not requir-
ing data from previous tasks, our proposed protocol only
assumes access to the new task data. In order to achieve
the best balanced tradeoff between retaining previous tasks
knowledge, i.e. stability, and successfully integrating new
tasks information,
i.e. plasticity, we start off by setting
the hyper parameters to values that ensure minimal loss
on previous tasks performance. If a pre-deﬁned threshold
on the performance of the new task validation data can’t
be achieved, we then decay these parameters values until
reaching the desired performance on the new task. Algo-
rithm 1 illustrates the main steps of our protocol, which can
be divided into two main phases, to be repeated for each
new task:

on the new task data with a learning rate η∗

Maximal Plasticity Search. Starting from the model
trained on the preceding tasks, parameterized by θt, max-
imal plasticity search ﬁrst ﬁnetunes a copy of the model
θt′
F T obtained
via a coarse grid search with the goal of obtaining the
highest accuracy A∗
F T on a held-out validation set from the
new task. The accuracy A∗
F T represents the best accuracy
that can be achieved while disregarding the previous tasks.
Note that a continual learning method can still achieve a
better performance on the new task than ﬁne-tuning but this
stands as a reference point for our hyper parameter search.
In the second phase we train θt with
the acquired learning rate η∗
F T using the considered con-
tinual learning method with other hyper-parameters set to
their highest values ensuring minimum forgetting. To avoid
redundant stability decay iterations, decayed hyperparam-
eters are propagated to later tasks. We deﬁne a threshold p
indicating the tolerated decay on the new task performance
compared to ﬁne-tuning. When this threshold is not met, we
decrease the hyperparameters values and repeat this phase.
This corresponds to increasing the plasticity of the model in
order to reach the desired performance threshold.

Stability Decay.

5

Algorithm 1 Continual Hyperparameter Selection Frame-
work
input H hyperparameter set
input α ∈ [0, 1] decaying factor
input p ∈ [0, 1] accuracy drop threshold
input Ψ coarse learning rate grid
input Dt+1 new task data
require θt previous task model parameters
require CLM continual learning method

// Maximal Plasticity Search

Dt+1, η; θt

⊲ Finetuning accuracy

(cid:1)

⊲ Update best values

1: A∗ = 0
2: for η ∈ Ψ do
3:
4:
5:
6:
7: end for

end if

A ← Finetune
if A > A∗ then
(cid:0)

A∗, η∗ ← A, η

// Stability Decay

Dt+1, η∗; θt

A ← CLM
if A < (1 − p)A∗ then
(cid:0)

8: do
9:
10:
11:
12:
13: while A < (1 − p)A∗

H ← α · H

end if

(cid:1)

⊲ Hyperparameter decay

5 COMPARED METHODS

In Section 6, we carry out a comprehensive comparison
between representative methods from each of the three
families of continual learning approaches introduced in Sec-
tion 3. For clarity, we ﬁrst provide a brief description of the
selected methods, highlighting their main characteristics.

5.1 Replay-based methods

iCaRL [18] was the ﬁrst replay-based method, focused on
learning in a class-incremental way. Assuming a ﬁxed al-
located memory, it selects and stores those samples (also
called exemplars) closest to the feature mean of each class.
During training, along with minimizing the estimated loss
on the new classes, the method also minimizes the distilla-
tion loss between targets obtained from the predictions of
the previous model and current model predictions on the
previously learned classes. As the weight of this distillation
loss correlates with preservation of the previous knowledge,
we optimize this hyperparameter in our proposed protocol.
In our study, we consider the task incremental setting where
each task is represented by a group of classes. To perform
a fair comparison with other methods, iCaRL is also imple-
mented in a multi-head fashion.

GEM [50], on the other hand, exploits the exemplars
to solve a constrained optimization problem, projecting
the current task gradient in a feasible area outlined by
the previous task gradients. The authors observe increased
backward transfer by altering the gradient projection with a
small constant γ ≥ 0, constituting H in our framework.

The major drawback of replay-based methods is the
limited scalability, as storing of and computation on raw
input samples for each of the tasks becomes intractable with
an ever increasing amount of tasks. Although a ﬁxed mem-
ory consumption can be imposed, this limits the amount

6

(3)

of exemplars per task, decreasing the representativeness
of the exemplar sets w.r.t. the original task distribution.
Furthermore, storing the raw input samples may also lead
to privacy issues.

approximated by the empirical FIM to avoid additional
backward passes [70]. Therefore the importance weight Ωn
k
is deﬁned as the squared gradient of loss function L w.r.t.
parameter θn
k :

5.2 Regularization-based methods

This survey strongly focuses on regularization-based meth-
ods, comparing ﬁve methods in this family. The regular-
ization strength correlates to the amount of knowledge re-
tention, and therefore constitutes H in our hyperparameter
framework.

Learning without Forgetting (LwF) [53] retains knowl-
edge of preceding tasks by means of knowledge distilla-
tion [67]. Before training the new task, network outputs for
the new task data are recorded, and are subsequently used
during training to distill prior task knowledge. However,
the success of this method depends heavily on the new task
data and how strong it is related to prior tasks. Distribution
shifts with respect to the previously learned tasks can result
in a gradual error build-up to the prior tasks as more
dissimilar tasks are added [7], [53]. This error build-up
also applies in a class-incremental setup, as shown in [18].
Another drawback of this approach is the computational
overhead and a minimal additional memory consumption in
the preprocessing step for training, requiring a forward pass
for each of the new data points and the output recording
to be stored. LwF is speciﬁcally designed for classiﬁcation,
but has also been applied to other problems, such as object
detection [20].

Encoder Based Lifelong Learning (EBLL) [11] extends
the concept of LwF by also preserving important low dimen-
sional feature representations of previous tasks. For each
task, after training the network, an under-complete autoen-
coder is inserted between the feature extractor and classiﬁer
of the network and trained in an end-to-end fashion, learn-
ing a feature projection on a lower dimensional manifold,
optimized for the corresponding task. During training, an
additional regularization term impedes the current feature
projections to deviate from the optimal ones for the previ-
ous tasks. Although the required memory grows linearly
with the number of tasks, the autoencoder size is generally
only a small fraction of the backbone network. The main
computational overhead occurs in the autoencoder training
and collecting the feature projections for the samples in each
optimization step.

Elastic Weight Consolidation (EWC) [28] applies the
Bayesian framework for neural networks [68], which allows
to ﬁnd posterior distributions of parameters instead of mere
point estimates in parameter space, by introducing uncer-
tainty on the network parameters. Following sequential
Bayesian estimation, the old posterior of previous tasks
T1:n−1 constitutes the prior for new task Tn, founding a
mechanism to propagate old task importance weights. The
true posterior is intractable, and is therefore estimated using
a Laplace approximation, assuming a Gaussian with the pa-
rameters θn determining the mean and a diagonal precision
estimated by the Fisher Information Matrix (FIM), which
near a minimum shows to be equivalent to the positive
semi-deﬁnite second order derivative of the loss [69]. In
practice, for computational efﬁciency the FIM is typically

Ωn

k = E

(x,y)∼Dn

2

,

δL
δθn

"(cid:18)

k (cid:19)

#

and is only calculated after training for task Tn. In the
Bayesian intuition, parameter θn
k is deemed more important
with higher certainty, i.e. with higher precision. From the
optimization perspective, the second order derivative of the
loss represents the curvature in parameter space. Hence θn
k
with a steep surface signiﬁcantly alters the loss of task Tn,
and should therefore get a high importance weight Ωn
k .

A ﬁrst limitation of EWC is its need to store the FIM
for each of the learned tasks, resulting in a ever-increasing
memory requirement as the amount of tasks grows. Sec-
ondly, the task FIM is approximated after optimizing the
task, inducing gradients close to zero, resulting in very little
regularization. This is inherently coped with in our frame-
work, as the regularization strength is initially very high
and only lowers when stability decay is imposed. Variants
of EWC are proposed to address these issues in [71], [52]
and [16].

Synaptic

[51] breaks

Intelligence (SI)

the EWC
paradigm of determining the importance weights in a sep-
arate phase after training. Instead, they maintain an online
estimate ωn during training of a new task Tn, while taking
into account the importance estimates of all previous tasks
T1:(n−1):

Ωn

k =

n−1

t=1
X

ωt
k
k)2 + ξ

,

(∆θt

(4)

k

k − θt−1

k = θt

with ∆θt
the task-speciﬁc parameter distance,
and damping parameter ξ avoiding division by zero. Note
that the accumulated importance weights Ωn
k are still only
updated after training of task Tn as in EWC. Nevertheless,
the importance weights ωn
k for the newly trained task are
already acquired in an online fashion during training, and
therefore no additional inference and backward passes are
required to obtain the post-training gradients. However,
the efﬁcient online calculation knife cuts both ways. First,
the widely used stochastic gradient descent (SGD) incurs
noise in the approximated gradient during training, and
therefore the authors state that the importance weights tend
to be overestimated. Second, catastrophic forgetting of the
knowledge from a pretrained network becomes inevitable,
as importance weights can’t be retrieved. In another work,
Riemannian Walk [16] combines the SI path integral with an
online version of EWC to measure parameter importance.

Memory Aware Synapses (MAS) [15] redeﬁnes the im-
portance measure of parameters to an unsupervised setting.
Instead of calculating the gradients of the loss function L
as in (3), the authors obtain the gradients of the squared
L2 norm of the learned output function F of the network.
Therefore, the importance weights become:

Ωn

k = Ex∼Dn[

δ kF (x; θ)k2
2
δθn
k

] .

(5)

(6)

(7)

(8)

Previously discussed methods require supervised data for
the loss-based importance weight estimations, and are there-
fore conﬁned to the mere available task-speciﬁc training
data. MAS on the other hand, allows importance weights
to be estimated on an unsupervised held-out dataset, and is
therefore able to adapt to user-speciﬁc data.

Incremental Moment Matching (IMM) [29] estimates
Gaussian posteriors for each of the task parameters, in the
same vein as EWC, but inherently differs in its use of model
merging. In the merging step, the mixture of Gaussian pos-
teriors is approximated by a single Gaussian distribution,
i.e. a new set of merged parameters θ1:n and corresponding
covariances Σ1:n. Although the merging strategy implies a
single merged model for deployment, it requires to store a
model during training for each of the learned tasks. In their
work, two methods for the merge step are proposed: mean-
IMM and mode-IMM. In the former, the weights θk of the
task-speciﬁc networks are averaged following the weighted
sum:

θ1:n
k =

n

t

αt
kθt
k ,

X

k the mixing ratio of task Tt, subject to

with αt
k = 1.
The second merging method mode-IMM, instead aims for
the mode of the mixture of Gaussians. Here, the inverse
covariance or precision matrices are required, which is again
assumed diagonal and approximated by the FIM, equivalent
to the importance weights Ωn

k in (3), with:

P

n
t αt

n

kΩt
αt

kθt
k ,

θ1:n
k =

Ω1:n

k =

1
Ω1:n
k
n

t
X

t
X
kΩt
αt
k .

The importance weights of all tasks are mixed using mixing
ratios in (8), resulting in Ω1:n
k which is subsequently used
as a normalization constant when merging task-speciﬁc
model parameters in (7). When two models converge to a
different local minimum due to independent initialization,
simply averaging the models might result in an increased
loss, as there are no guarantees for a ﬂat or convex cost
surface between the two points in parameter space [72].
Therefore, IMM suggests three transfer techniques to aim for
an optimal solution in the interpolation of the task-speciﬁc
optimized models: i) Weight-Transfer initializes the network
of the new task Tn with the parameters of the previous
task Tn−1; ii) Drop-Transfer is a variant of dropout [73] with
the parameters of previous task Tn−1 as the zero point; iii)
L2-transfer is a variant of L2-regularization, again with the
parameters of previous task Tn−1 redeﬁning the zero point.
In this study, we compare mean-IMM and mode-IMM with
both weight-transfer and L2-transfer.

5.3 Parameter isolation-based methods

PackNet [56] iteratively assigns a subset of the parameters to
each of the consecutive tasks by constituting a correspond-
ing binary mask. For each new task Tn PackNet requires two
training phases. First, the network is trained while ﬁxing the
parameters θ1:n−1 assigned to previous tasks. After the ﬁrst
training phase, a predeﬁned proportion of the remaining
non-ﬁxed parameters is allotted to the new task, deﬁned
by mask mn. Selection of the parameters is determined

7

by highest magnitude, serving as indicator for parame-
ter importance in this work. In a second training round,
this subset of most important parameters θn is retrained.
However, besides ﬁxing all parameters of previous tasks
θ1:n−1, the remaining unassigned parameters are masked
out. This ensures preservation of performance for task Tn in
inference, utilizing solely parameters which are not masked
out by m1:n. Although PackNet allows explicit allocation of
network capacity to each task, it remains inherently limited
in the amount of tasks that can be assigned to a model.

6 EXPERIMENTS

In this section we ﬁrst discuss the experimental setup in
Section 6.1, followed by a comparison of all the methods on
a common BASE model in Section 6.2. The effects of changing
the capacity of this model are discussed in Section 6.3. Next,
in Section 6.4 we look at the effect of two popular methods
for regularization. We continue in Section 6.5, scrutinizing
the behaviour of continual learning methods in a real-world
setup, abandoning the artiﬁcially imposed balancedness
between tasks. In addition, we investigate the effect of the
task ordering in both the balanced and unbalanced setup in
Section 6.6. Finally in Section 6.7, we elucidate a qualitative
comparison in Table 8, which summarizes limitations and
resource requirements for the methods.

6.1 Experimental Setup

Datasets. We conduct image classiﬁcation experiments on
two datasets, the main characteristics of which are summa-
rized in Table 1. First, we use the Tiny Imagenet dataset [74].
This is a subset of 200 classes from ImageNet [75], rescaled
to image size 64 × 64. Each class contains 500 samples
subdivided into training (80%) and validation (20%), and
50 samples for evaluation. In order to construct a balanced
dataset, we assign an equal amount of 20 randomly chosen
classes to each task in a sequence of 10 consecutive tasks.
One could argue that such a class-incremental setting is
not a good ﬁt with our evaluation per task, i.e. using an
oracle at test time. We nevertheless opted for this setting,
as it ensures that all tasks are roughly similar in terms of
difﬁculty, size, and distribution, making the interpretation
of the results easier.

The second dataset is based on iNaturalist [76]. It aims
for a more real-world setting with a large number of ﬁne-
grained categories and highly imbalanced classes. On top,
we impose task imbalance and domain shifts between tasks
by assigning 10 super-categories of species as separate tasks.
We selected the most balanced 10 super-categories from the
total of 14 and only retained categories with at least 100
samples. More details on the statistics for each of these
tasks can be found in Table 7. We only utilize the training
data, subdivided in training (70%), validation (20%) and
evaluation (10%) sets, with all images measuring 800 × 600.
In this survey we scrutinize the effects of different task
orderings for both datasets in Section 6.6. Apart from that
section, discussed results are performed on a random order-
ing of tasks.

TABLE 1: The balanced Tiny Imagenet and unbalanced
iNaturalist dataset characteristics.

Tiny Imagenet

iNaturalist

Tasks
Classes per task
Training data per task
Validation data per task
Task Constitution

10
20
8k
1k
random class selection

10
5 to 314
0.6k to 66k
0.1k to 9k
supercategory

Models. We summarize in Table 2 the models used for
the experiments in this work. Due to the limited size of
Tiny Imagenet we can easily run experiments with differ-
ent models. This allows to analyze the inﬂuence of model
capacity (Section 6.3) and regularization for each of the
model conﬁgurations (Section 6.4). This is important, as the
effect of model size and architecture on the performance of
different continual learning methods has not received much
attention so far. We conﬁgure a BASE model, two models
with less (SMALL) and more (WIDE) units per layer, and a
DEEP model with more layers. The models are based on
a VGG conﬁguration [77] but with less parameters due to
the small image size. We reduce the feature extractor to
comprise 4 max-pooling layers, each preceded by a stack
of identical convolutional layers with a consistent 3 × 3
receptive ﬁeld. The ﬁrst max-pooling layer is preceded by
one conv. layer with 64 ﬁlters. Depending on the model, we
increase subsequent conv. layer stacks with a multiple of
factor 2. The models have a classiﬁer consisting of 2 fully
connected layers with each 128 units for the SMALL model
and 512 units for the other three models. The multi-head
setting imposes a separate softmax output layer for each
task. A detailed description can be found in supplemental.
The size of iNaturalist imposes arduous learning. There-
fore, we conduct the experiments solely for AlexNet [78],
pretrained on ImageNet.

Evaluation Metrics. To measure performance in the con-
tinual learning setup, we evaluate accuracy and forgetting
per task, after training each task. We deﬁne the measure of
forgetting [16] as the difference between the expectation of
acquired knowledge of a task, i.e. the accuracy when ﬁrst
learning a task, and the accuracy obtained after training
one or more additional tasks. In the ﬁgures, we focus on
the evolution of the accuracy for each task as more tasks
are added. In the tables, we report the average accuracy
and average forgetting on the ﬁnal model, obtained by
evaluating each task after learning the entire sequence of
ten tasks.

Baselines. The discussed continual learning methods in
Section 5 are compared against several baselines:

1)

2)

Finetuning uses the model
learned on previous
tasks as initialization and then optimizes the pa-
rameters for the current task. This baseline greedily
trains each task without considering performance
on previous tasks. This introduces catastrophic for-
getting and serves as a weak lower bound of the
average accuracy.
Joint training considers all data in the task se-
quence simultaneously, hence violating the contin-
ual learning setup (indicated with appended ’∗’ in

8

reported results). This baseline provides a weak
upper bound.

For the replay-based methods we consider two additional
ﬁnetuning baselines, extending baseline (1) with the beneﬁt
of using exemplars:

3) Basic rehearsal with Full Memory (R-FM) fully
exploits the total available exemplar memory R, and
incrementally divides the capacity equally over all
the previous tasks. This is a baseline for replay-
based methods deﬁning memory management poli-
cies to exploit all memory (e.g. iCaRL).

4) Basic rehearsal with Partial Memory (R-PM) pre-
allocates a ﬁxed amount of exemplar memory R/T
over all tasks T in the sequence. This assumes that
the amount of tasks T is known beforehand, as used
by methods lacking memory management policies
(e.g. GEM).

Replay Buffers. The replay-based methods (GEM, iCaRL)
and the corresponding baselines (R-PM, R-FM) can be con-
ﬁgured with arbitrary size of replay buffer. Too large buffers
would result in unfair comparison to the regularization and
parameter isolation-based methods, with in its limit even
holding all data for the previous tasks, as in joint learning,
which is not compliant with the continual learning setup.
Therefore, we use the memory required to store the BASE
model as a basic reference for the amount of exemplars to
store. This corresponds to the additional memory needed
for propagating importance weights in the prior-focused
methods (EWC, SI, MAS). For Tiny Imagenet, this gives
a total replay buffer capacity of 4.5k exemplars. We also
experiment with a buffer of 9k exemplars to examine the
inﬂuence of increased buffer capacity. Note that taking
the BASE model as reference implies that the replay-based
methods use a signiﬁcantly different amount of memory
than the non-replay based methods; more memory for the
SMALL model) and less memory for the WIDE and DEEP
models. Comparisons between replay and non-replay based
methods should thus only be done for the BASE model. In
the following, notation of the methods without replay buffer
size refers to the default 4.5k buffer.
Learning details. During training the models are optimized
with stochastic gradient descent with a momentum of 0.9.
Training lasts for 70 epochs unless preempted by early stop-
ping. Unless otherwise noticed (in particular, in Section 6.4),
we do not use any form of regularization. This may cause
some level of overﬁtting, but avoids a possible interference
with the incremental learning methods. We study these
effects in more detail in a separate section. The framework
we proposed in Section 4 can only be exploited in the case
of forgetting-related hyperparameters, in which case we set
p = 0.2 and α = 0.5. All methods discussed in Section 5
satisfy this requirement, except for IMM with L2 transfer for
which we could not identify a speciﬁc hyperparameter re-
lated to forgetting. For speciﬁc implementation details about
the continual learning methods we refer to the supplemental
material.

6.2 Comparing Methods on the BASE Network

Tiny Imagenet. We start the evaluation of the continual
learning methods discussed in Section 5 with a comparison

TABLE 2: Models used for the Tiny Imagenet and iNaturalist experiments.

9

Model

Feature Extractor

Classiﬁer (w/o head)

Total Parameters

Pretrained

Multi-head

Conv. Layers MaxPool

Parameters

FC layers

Parameters

Tiny Imagenet

SMALL
BASE
WIDE
DEEP

6
6
6
20

5

4
4
4
4

3

334k
1.15m
4.5m
4.28m

2.47m

2
2
2
2

279k
2.36k
4.46m
2.36k

613k
3.51m
8.95m
6.64m

57.0m

iNaturalist

AlexNet

2 (with Dropout)

54.5m

X(Imagenet) X

✗
✗
✗
✗

X
X
X
X

using the BASE Network, on the Tiny Imagenet dataset, with
random ordering of tasks. This provides a balanced setup,
making interpretation of the results easier.

based classiﬁcation, but improves over time, especially for
the ﬁrst few tasks. Doubling replay-buffer size to 9k en-
hances iCaRL further to even outperform PackNet (48.76%).

Figure 2 shows the results of the regularization and pa-
rameter isolation-based methods. The results of the replay-
based methods and the corresponding baselines can be
observed in Figure 3. Each of these ﬁgures consists of 10
subpanels, with each subpanel showing the evolution of the
test accuracy for a speciﬁc task (e.g. Task 1 for the leftmost
panel) as more tasks are added for training. Since the n-
th task is added for training only after n steps, the curves
get shorter as we move to the subpanels on the right. The
average accuracy and average forgetting after training the
whole sequence of tasks is given for each method in the
ﬁgure legend.

6.2.1 Discussion

General observations. As a reference, it is relevant to highlight
the soft upper bound obtained when training all tasks
jointly. This is indicated by the star symbol in each of the
subpanels. For the Tiny Imagenet dataset, all tasks have
the same number of classes, same amount of training data
and similar level of difﬁculty, resulting in similar accuracies
for all tasks under the joint training scheme. The average
accuracy for joint training on this dataset is 55.70%, while
random guessing would give an average accuracy of 5%.

Further, as has been reported ample times in the liter-
ature, the Finetuning baseline suffers severely from catas-
trophic forgetting: initially good results are obtained when
learning a task, but as soon as a new task is added, the
performance drops, resulting in a poor average accuracy of
only 21.30% and a high average forgetting of 26.90%.

With an average accuracy of 47.67%, PackNet shows
the highest overall performance of all continual learning
methods on the ﬁnal model (after training all 10 tasks),
i.e. without considering the doubled replay buffer size of
9k for which iCaRL attains 48.76%. When learning a new
task, the need for compression means PackNet only has
a fraction of the total model capacity available. Therefore,
it typically performs worse on the new task compared to
the other methods. However, by ﬁxing the task parameters
through masking, it allows complete knowledge retention
until the end of the task sequence (no forgetting yields ﬂat
curves in the ﬁgure), resulting in the highest accumulation
of knowledge at the end - at least when working with long
sequences, where forgetting errors gradually build up for
the other methods.

MAS and iCaRL show competitive results w.r.t. PackNet
(resp. 46.90% and 47.27%). iCaRL starts with a signiﬁcantly
lower accuracy for new tasks, due to its nearest-neighbour

Regularization-based methods. In prior experiments where we
did a grid search over a range of hyperparameters over
the whole sequence (rather than using the framework in-
troduced in Section 4), we observed MAS to be remarkably
more robust to the choice of hyperparameter values com-
pared to the two related methods EWC and SI. Switching
to the continual hyperparameter selection framework de-
scribed in Section 4, this robustness leads to superior results
for MAS compared to the other two (46.90% vs. 42.43% and
33.93). Especially the smaller amount of forgetting stands
out (1.58% vs 7.51% and 15, 77%). Further, SI underper-
forms compared to EWC on Tiny Imagenet. We hypothe-
size this may be due to the overﬁtting we observed when
training on the BASE model (see results in supplemental).
SI inherently constrains parameter importance estimation to
the training data only, which is opposed to EWC and MAS
able to determine parameter importance both on validation
and training data in a separate phase after training of the
task.

The data-driven methods LwF and EBLL obtain similar
results as EWC (41.91% and 45.34% vs 42.43%). EBLL
improves over LwF by residing closer to the optimal task
representations, lowering average forgetting and improving
accuracy. Apart from the ﬁrst few tasks, the curves for these
methods are quite ﬂat, indicating a low level of forgetting
(only 2.38% and 1.44%, for LwF and EBLL respectively).

Using importance weights to merge the models in mode-
IMM clearly is superior to mean-IMM. Mode-IMM catas-
trophically forgets in evaluation on the ﬁrst task, but shows
transitory recovering through backward transfer in all sub-
sequent tasks. Overall for Tiny Imagenet, the IMM methods
do not seem competitive with the other strategies for contin-
ual learning - especially if one takes into account that they
need to store all the models from previous tasks, making
them much more expensive in terms of storage.

Replay-based methods. iCaRL starts from the same model
as GEM and the regularization-based methods, but uses a
feature classiﬁer based on a nearest neighbor scheme. As
indicated earlier, this results in lower accuracies on the
ﬁrst task after training. Remarkably, for about half of the
tasks, the iCaRL accuracy increases when learning addi-
tional tasks, resulting in a salient negative average forgetting
of −1.11%. Such level of backward transfer is unusual. After
training all ten tasks, a competitive result of 47.27% average
accuracy can be reported. Comparing iCaRL to its baseline
R-FM shows signiﬁcant improvements over basic rehearsal
(47.27% vs 37.31%). Doubling the size of the replay buffer

(iCaRL 9k) increases performance even more, making iCaRL
outperform PackNet and all other methods with a global
best 48.76% average accuracy.

The results of GEM are signiﬁcantly lower than those
of iCaRL (45.13% vs 47.27%). GEM is originally designed
for an online learning setup, while in this comparison each
method can exploit multiple epochs for a task. Additional
experiments in supplemental exemplify the sensitivity of
GEM to the amount of epochs, compared to iCaRL, from
which we procure a GEM setup with 5 epochs for all the
experiments in this work. Furthermore, the lack of memory
management policy in GEM gives iCaRL a compelling ad-
vantage w.r.t. the amount of exemplars for the ﬁrst tasks,
e.g. training Task 2 comprises a replay buffer of Task 1 with
factor 10 (number of tasks) more exemplars. Surprisingly,
GEM 9k with twice as much exemplar capacity doesn’t
perform better than GEM 4.5k. This unexpected behavior
may be due to the random exemplar selection yielding a
less representative subset. Nevertheless, GEM convincingly
improves accuracy of the basic replay baseline R-PM with
the same partial memory scheme (45.13% vs 36.09%).

Comparing the two basic rehearsal baselines that use
the memory in different ways, we observe that the scheme
exploiting the full memory from the start (R-FM) gives
signiﬁcantly better results than R-PM for tasks 1 to 7, but
not for the last three. As more tasks are seen, both baselines
converge to the same memory scheme, where for the ﬁnal
task R-FM allocates an equal portion of memory to each task
in the sequence, hence equivalent to R-PM.

6.3 Effects of Model Capacity

Tiny Imagenet. A crucial design choice in continual learning
concerns the capacity of the network to be used. Aiming
to learn a long sequence of tasks, a model with high ca-
pacity capable of capturing the series of tasks knowledge
seems preferable. However, learning the ﬁrst task using
such model, with only data from a single task, holds the
risk of overﬁtting, jeopardizing generalization performance.
So far, we compared all methods on the BASE model. Next,
we study the effects of extending or reducing the capacity
of this architecture. Details of the models have been given
in Section 6.1 and supplemental material. In the following,
we discuss the results of the model choice, using again
the random ordering of Tiny Imagenet. These results are
summarized in the top part of Table 3 for regularization and
parameter isolation-based methods and Table 4 for replay-
based methods1.

6.3.1 Discussion

Overﬁtting. We observed overﬁtting for several models and
methods, not only for the WIDE and DEEP models. Com-
paring the different methods, SI seems quite vulnerable
to overﬁtting issues, while PackNet prevents overﬁtting to
some extent, thanks to the network compression phase.
General Observations. Selecting highest accuracy disregard-
ing which model is used, iCaRL 9k and PackNet remain on

1. Note that, for all of our observations, we also checked the results
obtained for the two other task orders (reported in the middle and
bottom part of Table 3 and Table 4). The reader can check that the
observations are quite consistent.

10

the lead with 49.94% and 49.09%. Further, LwF shows to be
competitive with MAS (resp. 46.79% and 46.90%).

Baselines. Taking a closer look at the ﬁnetuning baseline
results (see purple box in Table 3), we observe that it does
not reach the same level of accuracy with the SMALL model
as with the other models. In particular, the initial accuracies
are similar, yet the level of forgetting is much more severe,
due to the limited capacity of the model to learn new tasks.
On the other hand, for joint training (blue box), results
on the DEEP network are inferior compared to the shallower
networks, implying the deep architecture to be less suited
to accumulate all knowledge from the task sequence. As
the performance of the joint learning serves as a soft upper
bound for the continual learning methods, this already
serves as an indication that a deep model may not be
optimal for continual learning.

In accordance, replay baselines R-PM and R-FM show
to be quite model agnostic with all but the DEEP model
performing very similar. The baselines experience both less
forgetting and increased average accuracy when doubling
the replay buffer size.

SMALL model. Finetuning, SI and mean-IMM suffer from se-
vere forgetting (> 20%) imposed by the decreased capacity
of the network (see red underlining), making these com-
binations worthless in practice. Other methods experience
alleviated forgetting, with EWC most saliently beneﬁtting
from a small network (see green underlinings).

WIDE model. Inspecting the results for the wide model,
it’s remarkable that, in contrast to the results we see with
the other models, SI consistently outperforms EWC on the
WIDE model (see orange box; even more clear in middle and
bottom tables in Table 3, which we will discuss later). EWC
performs worse for the high capacity WIDE and DEEP mod-
els and, as previously discussed, attains best performance
for the SMALL model. LwF, EBLL and PackNet mainly reach
their top performance when using the WIDE model, with SI
performing most stable on both the WIDE and BASE models.
IMM shows increased performance when using the BASE
and WIDE model, for both mean and mode merging.

DEEP model. Over the whole line of methods (yellow box),
extending the BASE model with additional convolutional
layers results in lower performance. As we already observed
overﬁtting on the BASE model, the additional layers may
introduce extra unnecessary layers of abstraction, decreas-
ing overall performance. For the DEEP model iCaRL out-
performs all continual learning methods with both memory
sizes.

Model agnostic. Over all orderings which will be discussed in
Section 6.6, some of the methods don’t exhibit a preference
for any model, except for the common aversion for the
detrimental DEEP model. This is in general most salient for
all replay-based methods in Table 4, but also for PackNet,
MAS, LwF and EBLL in Table 3.

Conclusion. We can conclude that (too) deep model archi-
tectures do not provide a good match with the continual
learning setup. For the same amount of feature extractor pa-
rameters, WIDE models obtain signiﬁcant better results (on
average 10% better over all methods in Table 3 and Table 4).
Also too small models should be avoided, as the limited

Fig. 2: Comparison of regularization and parameter isolation-based continual learning methods on Tiny Imagenet for the
BASE model with random ordering, with average accuracy (average forgetting) for each method in the legend.

11

finetuning: 21.30 (26.90)
joint*: 55.70 (n/a)

PackNet: 47.67 (0.00)
SI: 33.93 (15.77)

EWC: 42.43 (7.51)
MAS: 46.90 (1.58)

LwF: 41.91 (3.08)
EBLL: 45.34 (1.44)

mean-IMM: 26.38 (22.43)
mode-IMM: 36.89 (0.98)

T1

T2

T3

T4

T5

T6

T7

T8

T9

T10

Evaluation on Task

T1

T2

T3

T4

T5

T6

T7

T8

T9

T10

Training Sequence Per Task

Fig. 3: Comparison of replay-based continual learning methods on Tiny Imagenet for the BASE model with random
ordering, with average accuracy (average forgetting) for each method in the legend.

finetuning: 21.30 (26.90)
joint*: 55.70 (n/a)

R-PM 4.5k: 36.09 (10.96)
R-PM 9k: 38.69 (7.23)

R-FM 4.5k: 37.31 (9.21)
R-FM 9k: 42.36 (3.94)

GEM 4.5k: 45.13 (4.96)
GEM 9k: 41.75 (5.18)

iCaRL 4.5k: 47.27 (-1.11)
iCaRL 9k: 48.76 (-1.76)

T1

T2

T3

T4

T5

T6

T7

T8

T9

T10

Evaluation on Task

60

50

40

30

20

10

%
 
y
c
a
r
u
c
c
A

60

50

40

30

20

10

%
 
y
c
a
r
u
c
c
A

T1

T2

T3

T4

T5

T6

T7

T8

T9

T10

Training Sequence Per Task

capacity that is available can cause forgetting. On average
we observe a modest 0.94% more forgetting for the SMALL
model compared to the BASE model. At the same time, for
some models, poor results may be due to overﬁtting, which
can possibly be overcome using regularization, as we will
study next.

6.4 Effects of Regularization

Tiny Imagenet. In the previous subsection we mentioned
the problem of overﬁtting in continual learning. Although
an evident solution would be to apply regularization, this
might interfere with the continual learning methods. There-
fore, we investigate the effects of two popular regularization
methods, namely dropout and weight decay, for the regular-
ization and parameter isolation-based methods in Table 5,
and for the replay-based methods in Table 6. For dropout

we set the probability of retaining the units to p = 0.5, and
weight decay applies a regularization strength of λ = 10−4.

6.4.1 Discussion

General observations. In Table 5 and Table 6, negative results
(where the regularization hurts performance) are under-
lined in red. As one can see, this happens quite a few times,
especially in combination with the SMALL model or with
weight decay.

Over all methods and models dropout mainly shows to
be fruitful. This is consistent with earlier observations [79].
There are, however, a few salient exceptions (discussed be-
low). Weight decay over the whole line mainly improves the
wide network accuracies. In the following, we will describe
the most notable observations and exceptions to the main
tendencies.

12

TABLE 3: Regularization and parameter isolation-based methods. Results on Tiny Imagenet for different model sizes with
random ordering of tasks (top), using the easy to hard (middle) and hard to easy (bottom): for each method, we report average
accuracy (average forgetting).

Model

ﬁnetuning

joint*

PackNet

SI

EWC

MAS

LwF

EBLL

mean-IMM mode-IMM

SMALL
BASE
WIDE
DEEP

16.25 (34.84)
21.30 (26.90)
25.28 (24.15)
20.82 (20.60)

57.00 (n/a)
55.70 (n/a)
57.29 (n/a)
51.04 (n/a)

49.09 (0.00)
47.67 (0.00)
48.39 (0.00)
34.75 (0.00)

23.91 (23.26)
33.93 (15.77)
33.86 (15.16)
24.53 (12.15)

45.13 (0.86)
42.43 (7.51)
31.10 (17.07)
29.14 (7.92)

40.58 (0.78)
46.90 (1.58)
45.08 (2.58)
33.58 (0.91)

44.06 (-0.44)
41.91 (3.08)
46.79 (1.19)
32.28 (2.58)

44.13 (-0.53)
45.34 (1.44)
46.25 (1.72)
27.78 (3.14)

19.02 (27.03)
26.38 (22.43)
23.31 (26.50)
21.28 (18.25)

29.63 (3.06)
36.89 (0.98)
36.42 (1.66)
27.51 (0.47)

Model

ﬁnetuning

joint*

PackNet

SI

EWC

MAS

LwF

EBLL

mean-IMM mode-IMM

SMALL
BASE
WIDE
DEEP

16.06 (35.40)
23.26 (24.85)
19.67 (29.95)
23.90 (17.91)

57.00 (n/a)
55.70 (n/a)
57.29 (n/a)
51.04 (n/a)

48.94 (0.00)
48.36 (0.00)
50.09 (0.00)
35.19 (0.00)

35.98 (13.02)
33.36 (14.28)
34.84 (13.14)
24.56 (14.41)

40.18 (7.88)
34.09 (13.25)
28.35 (21.16)
24.45 (15.22)

44.29 (1.77)
44.02 (1.30)
45.58 (1.52)
35.23 (2.37)

45.04 (1.89)
43.46 (2.53)
42.66 (1.07)
27.61 (3.70)

42.07 (1.73)
43.60 (1.71)
44.12 (3.46)
29.73 (1.95)

14.62 (36.77)
22.84 (24.33)
25.42 (23.90)
17.95 (22.38)

26.13 (3.03)
36.81 (-1.17)
38.68 (-1.09)
25.89 (-2.09)

Model

ﬁnetuning

joint*

PackNet

SI

EWC

MAS

LwF

EBLL

mean-IMM mode-IMM

SMALL
BASE
WIDE
DEEP

18.62 (28.68)
21.17 (22.73)
25.25 (22.69)
15.33 (20.66)

57.00 (n/a)
55.70 (n/a)
57.29 (n/a)
51.04 (n/a)

46.57 (0.00)
43.94 (0.00)
45.07 (0.00)
31.34 (0.00)

40.39 (4.50)
40.79 (2.58)
37.91 (8.05)
22.97 (13.23)

41.62 (3.54)
41.83 (1.51)
29.91 (15.77)
22.32 (13.86)

40.90 (1.35)
41.98 (0.14)
43.55 (-0.29)
32.99 (2.19)

42.36 (0.63)
41.58 (1.40)
43.87 (1.26)
30.77 (2.51)

43.60 (-0.05)
41.57 (0.82)
42.42 (0.85)
30.15 (2.67)

17.44 (31.05)
23.64 (20.17)
23.70 (22.15)
16.20 (21.45)

24.95 (1.66)
34.58 (0.23)
35.24 (-0.82)
26.38 (1.20)

TABLE 4: Replay based methods. Results on Tiny Imagenet for different model sizes with random ordering of tasks (top),
using the easy to hard (middle) and hard to easy (bottom): for each method, we report average accuracy (average forgetting).

Model

ﬁnetuning

joint*

R-PM 4.5k

R-PM 9k

R-FM 4.5k

R-FM 9k

GEM 4.5k

GEM 9k

iCaRL 4.5k

iCaRL 9k

SMALL
BASE
WIDE
DEEP

16.25 (34.84)
21.30 (26.90)
25.28 (24.15)
20.82 (20.60)

57.00 (n/a)
55.70 (n/a)
57.29 (n/a)
51.04 (n/a)

36.97 (12.21)
36.09 (10.96)
36.47 (12.45)
27.61 (7.14)

40.11 (9.23)
38.69 (7.23)
41.51 (5.15)
28.99 (6.27)

39.60 (9.57)
37.31 (9.21)
39.25 (9.26)
32.26 (3.33)

41.35 (6.33)
42.36 (3.94)
41.53 (6.02)
33.10 (4.70)

39.44 (7.38)
45.13 (4.96)
40.32 (6.97)
29.66 (6.57)

41.59 (4.17)
41.75 (5.18)
44.23 (3.94)
23.75 (6.93)

43.22 (-1.39)
47.27 (-1.11)
44.20 (-1.43)
36.12 (-0.93)

46.32 (-1.07)
48.76 (-1.76)
49.94 (-2.80)
37.16 (-1.64)

Model

ﬁnetuning

joint*

R-PM 4.5k

R-PM 9k

R-FM 4.5k

R-FM 9k

GEM 4.5k

GEM 9k

iCaRL 4.5k

iCaRL 9k

SMALL
BASE
WIDE
DEEP

16.06 (35.40)
23.26 (24.85)
19.67 (29.95)
23.90 (17.91)

57.00 (n/a)
55.70 (n/a)
57.29 (n/a)
51.04 (n/a)

37.09 (11.64)
36.70 (8.88)
39.43 (9.10)
30.98 (6.32)

40.82 (8.08)
39.85 (6.59)
42.55 (5.32)
32.06 (4.16)

38.47 (9.62)
38.27 (8.35)
40.67 (7.52)
29.62 (6.85)

40.81 (7.73)
40.95 (6.21)
43.23 (4.09)
33.92 (3.56)

39.07 (9.45)
30.73 (10.39)
44.49 (4.66)
25.29 (12.79)

42.16 (6.93)
40.20 (7.41)
40.78 (8.14)
29.09 (6.30)

48.55 (-1.17)
46.30 (-1.53)
48.13 (-2.01)
32.46 (-0.35)

46.67 (-2.03)
47.47 (-2.22)
44.59 (-2.44)
34.64 (-1.25)

Model

ﬁnetuning

joint*

R-PM 4.5k

R-PM 9k

R-FM 4.5k

R-FM 9k

GEM 4.5k

GEM 9k

iCaRL 4.5k

iCaRL 9k

SMALL
BASE
WIDE
DEEP

18.62 (28.68)
21.17 (22.73)
25.25 (22.69)
15.33 (20.66)

57.00 (n/a)
55.70 (n/a)
57.29 (n/a)
51.04 (n/a)

33.69 (11.06)
32.90 (8.98)
34.85 (7.94)
24.22 (6.67)

37.68 (6.34)
34.34 (7.50)
40.28 (6.67)
23.42 (5.93)

37.18 (7.13)
33.53 (9.08)
36.70 (6.63)
25.81 (6.06)

38.47 (4.88)
36.83 (5.43)
37.46 (5.62)
29.99 (2.91)

38.87 (6.78)
35.11 (7.44)
37.27 (7.86)
27.08 (6.47)

39.28 (7.44)
35.95 (6.68)
37.91 (6.94)
31.28 (5.52)

46.81 (-0.92)
43.29 (-0.47)
41.49 (-1.49)
30.95 (0.85)

46.90 (-1.61)
44.52 (-1.71)
49.66 (-2.90)
37.93 (-1.35)

Finetuning. Goodfellow et al. observe reduced catastrophic
forgetting in a transfer learning setup with ﬁnetuning when
using dropout [79]. Extended to learning a sequence of
10 consecutive tasks in our experiments, ﬁnetuning consis-
tently beneﬁts from dropout regularization. This is opposed
to weight decay which results in increased forgetting and
a lower performance on the ﬁnal model. In spite of the
good results for dropout, we regularly observe an increase
in the level of forgetting, which is compensated for by
starting from a better initial model, due to a reduction in
overﬁtting. Dropout leading to more forgetting is something
we also observe for many other methods (see blue boxes),
and becomes more severe as the task sequence grows in
length.

Joint training and PackNet. mainly relish higher accura-
cies with both regularization setups. By construction, they
beneﬁt from the regularization without interference issues.
PackNet even reaches a top performance of almost 56%, that
is 7% higher than the closest competitor (MAS).

SI. Most saliently thriving with regularization (with in-
creases in performance of around 10%, see green box) is
SI, which we previously found to be sensitive to overﬁtting.
The regularization aims to ﬁnd a solution to a well-posed

problem, stabilizing the path in parameter space. Therefore,
this provides a better importance weight estimation along
the path. Even then, it’s not competitive with the top per-
forming methods in terms of obtained average accuracy.

EWC and MAS. The prior-focused methods suffer from
interference when regularization is added. For dropout,
having more redundancy in the model means there are
less unimportant parameters left for learning the new tasks.
For weight decay, parameters that were deemed important
for the previous tasks are also decayed in each iteration,
affecting the performance on the older tasks. A similar effect
was noticed in [12]. However, in some cases, the effect
of this interference is again compensated for by having
better initial models. EWC only beneﬁts from dropout on
the WIDE and DEEP model, and similar to MAS prefers no
regularization on the SMALL model.

LwF and EBLL suffer from using dropout, with only the DEEP
model signiﬁcantly improving performance. For weight de-
cay the methods follow the general trend, enhancing the
WIDE net accuracies.

IMM with dropout exhibits higher performance only for the
WIDE and DEEP model, coming closer to the performance
obtained by the other methods.

iCaRL and GEM. Similar to the observations in Table 5,
there is a striking increased forgetting for dropout for all
methods in Table 6. Especially iCaRL shows an increase
in average forgetting, albeit consistently accompanied with
higher average accuracy. Except for the SMALL model, all
models for the replay baselines beneﬁt from dropout. For
GEM this beneﬁt is only notable for the WIDE and DEEP
models. For the replay-based methods weight decay doesn’t
improve average accuracy, nor forgetting, except mainly for
the WIDE model. In general, the inﬂuence of regularization
seems limited for the replay-based methods in Table 6
compared to the non-replay-based methods in Table 5.

6.5 Effects of a Real-world Setting

iNaturalist. Up to this point all experiments conducted on
Tiny Imagenet are nicely balanced, with an equal amount
of data and classes for each of the tasks. In further exper-
iments with iNaturalist we scrutinize a highly unbalanced
task sequence, both in terms of classes and available data
per task. We train AlexNet pretrained on Imagenet, and
track performance for each of the tasks in Figure 4a. The
experiments exclude the replay-based methods as they lack
a policy to cope with unbalanced data, which would make
comparison highly biased to our implementation.

6.5.1 Discussion

General observations. Overall PackNet shows the highest
average accuracy, tailgated by mode-IMM with superior
forgetting by exhibiting positive backward transfer. Both
PackNet and mode-IMM attain accuracies very close to joint
training, e.g. Task 2 and 6 (PackNet), and Task 1 and 7
(mode-IMM).

Performance drop. Evaluation on the ﬁrst 4 tasks shows
salient dips when learning Task 5 for mean-IMM and
regularization-based methods EWC, SI and MAS. In the
random ordering Task 5 is an extremely easy task of su-
percategory ’Fungi’ which contains only 5 classes and a
small amount of data. Using the expert gates to measure
relatedness, the ﬁrst four tasks show no particularly salient
relatedness peaks or dips for Task 5. Instead, this might
imply that rather the limited amount of training data with
only a few target classes enforces the network to overly ﬁt
to the speciﬁc task, causing forgetting of the previous tasks.

Path Integral. For iNaturalist we did not observe overﬁtting,
which might be the cause of the stable SI behaviour in
comparison to Tiny Imagenet.

LwF and EBLL. LwF catastrophically forgets on the unbal-
anced dataset with 13.77% forgetting, which is in high
contrast with the results acquired on Tiny Imagenet. The
supercategories in iNaturalist constitute a completely dif-
ferent task, imposing severe distribution shifts between the
tasks. In the contrary, Tiny Imagenet is constructed from
a subset of randomly collected classes, implying similar
levels of homogeneity between task distributions. On top,
EBLL constraints the new task features to reside closely to
the optimal presentation for previous task features, which
for the random ordering enforces forgetting to nearly halve
from 13.77% to 7.51%, resulting in a striking 7.91% increase
in average accuracy over LwF (from 45.39% to 53.30%).

13

6.6 Effects of Task Order

In this experiment we scrutinize how changing the task
order affects both the balanced (Tiny Imagenet) and unbal-
anced (iNaturalist) task sequences. Our hypothesis resem-
bles curriculum learning [80], implying knowledge is better
captured starting with the general easier tasks followed by
harder speciﬁc tasks.
Tiny Imagenet. The previously discussed results of regu-
larization and parameter isolation-based methods on top
of Table 3, and replay-based methods on top of Table 4
concern a random ordering. We deﬁne a new order based
on task difﬁculty, by measuring the accuracy over the 4
models obtained on held-out datasets for each of the tasks.
This results in an ordering from hard to easy consisting of
task sequence [5, 7, 10, 2, 9, 8, 6, 4, 3, 1] from the random
ordering, with the inverse order equal to the easy to hard
ordering.
iNaturalist constitutes three orderings deﬁned as follows.
First, we alphabetically order the supercategories to deﬁne
the random ordering, as depicted in Figure 4a. Further, the
two other orderings start with task ’Aves’ comprising most
data, whereafter the remaining tasks are selected based on
the average relatedness to the already included tasks in
the sequence. Relatedness is measured using Expert Gate
autoencoders following [7]. Selecting on highest relatedness
results in the related ordering in Figure 4b, whereas selecting
on lowest relatedness ends up with the unrelated ordering in
Figure 4c. We refer to Table 7 for the conﬁguration of the
three different orderings.

6.6.1 Discussion

Tiny Imagenet. The main observations on the random or-
dering for the BASE model in Section 6.2 and model capacity
in Section 6.3 remain valid for the two additional orderings,
with PackNet and iCaRL competing for highest average
accuracy, subsequently followed by MAS and LwF. In the
following, we will instead focus on general observations
between the three different orderings.
Task Ordering Hypothesis. Starting from our curriculum learn-
ing hypothesis we would expect the easy-to-hard ordering
to enhance performance w.r.t. the random ordering, and the
opposite effect for the hard-to-easy ordering. However, es-
pecially SI and EWC show unexpected better results for the
hard-to-easy ordering than for the easy-to-hard ordering.
For PackNet and MAS, we see a systematic improvement
when switching from hard-to-easy to easy-to-hard ordering.
The gain is, however, relatively small. Overall, the impact of
the task order seems insigniﬁcant.
Replay Buffer. Introducing the two other orderings, we now
observe that iCaRL doesn’t always improve when increasing
the replay buffer size for the easy-to-hard ordering (see
SMALL and WIDE model). More exemplars induce more
samples to distill knowledge from previous tasks, but might
deteriorate stochasticity in the estimated feature means in
the nearest-neighbour classiﬁer. GEM does not as consis-
tently beneﬁt from increased replay buffer size (GEM 9k),
which could also root in the reduced stochasticity of the
constraining gradients from previous tasks.
iNaturalist. The general observations for the random or-
dering remain consistent for the other orderings as well,

14

TABLE 5: Parameter isolation and regularization-based methods: effects of dropout (p = 0.5) and weight decay
(λ = 10−4) regularization on continual learning methods for the 4 model conﬁgurations on Tiny Imagenet.

Model

ﬁnetuning

joint*

PackNet

SI

EWC

MAS

LwF

EBLL

mean-IMM mode-IMM

SMALL No regularization
Dropout
Weight Decay

BASE

WIDE

DEEP

No regularization
Dropout
Weight Decay

No regularization
Dropout
Weight Decay

No regularization
Dropout
Weight Decay

16.25 (34.84)
19.52 (32.62)
15.06 (34.61)

21.30 (26.90)
29.23 (26.44)
19.14 (29.31)

25.28 (24.15)
30.76 (26.11)
22.78 (27.19)

20.82 (20.60)
23.05 (27.30)
19.48 (21.27)

57.00 (n/a)
55.97 (n/a)
56.96 (n/a)

55.70 (n/a)
61.43 (n/a)
57.12 (n/a)

57.29 (n/a)
62.27 (n/a)
59.62 (n/a)

51.04 (n/a)
59.58 (n/a)
54.60 (n/a)

49.09 (0.00)
50.73 (0.00)
49.90 (0.00)

47.67 (0.00)
54.28 (0.00)
48.28 (0.00)

48.39 (0.00)
55.96 (0.00)
47.77 (0.00)

34.75 (0.00)
46.22 (0.00)
36.99 (0.00)

23.91 (23.26)
38.34 (9.24)
37.99 (6.43)

33.93 (15.77)
43.15 (10.83)
39.65 (8.11)

33.86 (15.16)
43.74 (8.80)
42.44 (8.36)

24.53 (12.15)
32.76 (15.09)
26.04 (9.51)

45.13 (0.86)
40.02 (7.57)
41.22 (1.52)

42.43 (7.51)
42.09 (12.54)
44.35 (3.51)

31.10 (17.07)
33.94 (19.73)
37.45 (13.47)

29.14 (7.92)
31.16 (17.06)
22.47 (13.19)

40.58 (0.78)
40.26 (7.63)
37.37 (4.43)

46.90 (1.58)
48.98 (0.87)
44.29 (1.15)

45.08 (2.58)
47.92 (1.37)
47.24 (1.60)

33.58 (0.91)
39.07 (5.02)
19.35 (13.62)

44.06 (-0.44)
31.56 (18.38)
41.66 (-0.28)

44.13 (-0.53)
34.14 (13.20)
42.67 (-0.63)

41.91 (3.08)
41.49 (8.72)
40.91 (1.29)

46.79 (1.19)
45.04 (6.85)
48.11 (0.62)

32.28 (2.58)
37.89 (7.78)
33.15 (1.16)

45.34 (1.44)
44.66 (7.66)
41.26 (0.82)

46.25 (1.72)
46.19 (5.31)
48.17 (0.82)

27.78 (3.14)
36.85 (4.87)
31.71 (1.39)

19.02 (27.03)
21.71 (31.30)
22.33 (24.19)

26.38 (22.43)
21.11 (24.41)
25.85 (21.92)

23.31 (26.50)
32.42 (22.03)
28.33 (23.01)

21.28 (18.25)
26.38 (23.38)
18.93 (18.64)

29.63 (3.06)
29.35 (0.90)
30.59 (0.93)

36.89 (0.98)
34.20 (1.44)
37.49 (0.46)

36.42 (1.66)
42.41 (-0.93)
39.16 (1.41)

27.51 (0.47)
33.64 (-0.61
25.62 (-0.15)

TABLE 6: Replay based methods: effects of dropout (p = 0.5) and weight decay (λ = 10−4) regularization on continual
learning methods for the 4 model conﬁgurations on Tiny Imagenet.

Model

R-PM 4.5k

R-FM 4.5k

GEM 4.5k

iCaRL 4.5k

SMALL No regularization
Dropout
Weight Decay

No regularization
Dropout
Weight Decay

No regularization
Dropout
Weight Decay

BASE

WIDE

DEEP

36.97 (12.21)
35.50 (12.02)
35.57 (13.38)

36.09 (10.96)
43.32 (10.59)
36.11 (10.13)

36.47 (12.45)
42.20 (12.31)
39.75 (8.28)

No regularization
Dropout
Weight Decay

27.61 (7.14)
34.42 (9.57)
26.70 (9.01)

39.60 (9.57)
35.75 (9.87)
39.11 (9.49)

37.31 (9.21)
45.76 (7.38)
37.78 (8.01)

39.25 (9.26)
45.51 (8.85)
38.73 (10.01)

32.26 (3.33)
37.22 (7.65)
31.70 (4.86)

39.44 (7.38)
33.85 (6.25)
37.64 (5.11)

45.13 (4.96)
36.09 (12.13)
38.05 (8.74)

40.32 (6.97)
42.76 (6.33)
45.27 (5.92)

29.66 (6.57)
32.75 (8.15)
27.26 (5.94)

43.22 (-1.39)
44.82 (4.83)
44.90 (-0.81)

47.27 (-1.11)
48.42 (2.68)
45.97 (-2.32)

44.20 (-1.43)
45.59 (2.74)
46.58 (-1.38)

36.12 (-0.93)
41.77 (3.58)
33.70 (-0.47)

TABLE 7: The unbalanced iNaturalist task sequence details.

Task

Classes

Ordering

Amphibia
Animalia
Arachnida
Aves
Fungi
Insecta
Mammalia
Mollusca
Plantae
Reptilia

28
10
9
314
5
150
42
13
237
56

Samples

Validation
755
196
170
9362
91
3692
1231
241
4998
1447

Training
5319
1388
1192
65897
645
26059
8646
1706
35157
10173

Test
1519
397
341
18827
184
7443
2469
489
10045
2905

Random Related
1
2
3
4
5
6
7
8
9
10

4
5
8
1
6
9
2
7
10
3

Unrelated
10
9
7
1
2
3
8
4
5
6

with PackNet and mode-IMM the two highest performing
methods.

Small Task Dip. For the random ordering we observed a dip
in accuracies for all preceding tasks when learning the very
small Fungi task. Furthermore, for the unrelated ordering,
the Fungi (Task 2) also experiences a performance dip for
ﬁnetuning and EWC in Figure 4c (see leftmost evaluation
panel of Task 1). On top, EWC shows the same behaviour
for Fungi (Task 6) also in the related ordering (Figure 4b),
and therefore seems to be most susceptible to highly varying
task constitutions, and especially smaller tasks.

LwF and EBLL. Where EBLL improves LwF with a signiﬁcant
7.91% accuracy in the random ordering, and 1.1% in the re-
lated ordering, it performs similar to LwF for the unrelated
ordering.

SI and First Task. As observed on Tiny Imagenet as well, SI is
sensitive to overﬁtting when estimating importance weights

through the path integral. The same behaviour can be ob-
served for the random ordering which starts with a rather
small task Amphibia (5k training samples) showing very
unstable learning curves. The two other orderings start from
task Aves (65k training samples) from which the amount
of data might act as a regularizer and prevent overﬁtting.
Hence, the path integral can be stabilized, resulting in very
competitive results compared to EWC and MAS.

6.7 Qualitative Comparison

The previous experiments have a strong focus on coping
with catastrophic forgetting, whereas by construction each
method features additional advantages and limitations w.r.t.
the other methods. In Table 8 we visualize relative GPU
memory requirements and computation time for each of
the methods in a heatmap, and formalize the extra required
storage during the lifetime of the continual learner. Further,

(a) Random ordering.

15

finetuning: 45.59 (17.02)
joint*: 63.90 (n/a)

PackNet: 59.41 (0.00)
SI: 47.54 (13.85)

EWC: 54.02 (6.67)
MAS: 54.59 (4.97)

LwF: 45.39 (13.77)
EBLL: 53.30 (7.51)

mean-IMM: 49.82 (13.42)
mode-IMM: 55.61 (-0.29)

T1

T2

T3

T4

T7

T8

T9

T10

Evaluation on Task

T5

T6

100

80

60

40

20

%
 
y
c
a
r
u
c
c
A

100

80

60

40

20

%
 
y
c
a
r
u
c
c
A

100

80

60

40

20

%
 
y
c
a
r
u
c
c
A

T1

T2

T3

T4

T5

T6

T7

T8

T9

T10

Training Sequence Per Task

(b) Related ordering.

finetuning: 44.12 (16.27)
joint*: 63.90 (n/a)

PackNet: 57.33 (0.00)
SI: 50.77 (5.16)

EWC: 51.14 (8.00)
MAS: 51.06 (4.63)

LwF: 46.93 (8.23)
EBLL: 48.03 (8.02)

mean-IMM: 47.21 (12.34)
mode-IMM: 52.35 (-1.32)

T1

T2

T3

T4

T7

T8

T9

T10

Evaluation on Task

T5

T6

T1

T2

T3

T4

T5

T6

T7

T8

T9

T10

Training Sequence Per Task

(c) Unrelated ordering.

finetuning: 48.27 (11.46)
joint*: 63.90 (n/a)

PackNet: 58.20 (0.00)
SI: 51.77 (5.14)

EWC: 52.64 (6.04)
MAS: 50.19 (6.13)

LwF: 48.02 (7.79)
EBLL: 48.01 (6.98)

mean-IMM: 48.58 (10.83)
mode-IMM: 52.71 (-0.81)

T1

T2

T3

T4

T7

T8

T9

T10

Evaluation on Task

T5

T6

T1

T2

T3

T4

T5

T6

T7

T8

T9

T10

Training Sequence Per Task

Fig. 4: Continual learning methods accuracy plots for 3 different orderings of the iNaturalist dataset.

we emphasize task-agnosticity and privacy in our com-
parison. The heatmap is based on our experimental setup
for Tiny Imagenet with the BASE model conﬁgured with
batch size 200, all phases with multiple iterations have their
epochs set to 10, gridsearches are conﬁned to one node, and
replay-buffers have 450 exemplars allocated per task. Note
that we present the results in an indicative heatmap for a
more high level relative comparison, as the results highly
depend on our implementation.
GPU Memory for training is slightly higher for EBLL, which
requires the autoencoder to be loaded in memory as well.
At test time all memory consumption is more or less equal,
but shows some deviations for GEM and PackNet due to
our implementation. Especially PackNet is the only method
to load additional masks in memory during evaluation, and
hence increases memory requirements. However, the BASE
model size (54MB) and hence mask size are rather small
compared to concurrently loaded data (200 images of size
64 × 64 × 3 ). In our implementation we load all masks
at once for faster prediction at test time, with the cost of
increased memory consumption of the masks (factor 10
increase, one mask per task). This results in a doubling of
memory requirement at test time for PackNet.
Computation time for training is doubled for EBLL and
PackNet (see light green), as they both require an additional
training step, respectively for the autoencoder and com-
pression phase. iCaRL training requires additional forward
passes for each of the exemplar sets (factor 5 increase).
GEM requires on top of that additional backward passes
to acquire the gradient for each of the exemplar sets, and
subsequently solve a quadratic optimization problem (factor
10 increase). During testing, iCaRL nearest-neighbor classi-
ﬁer is vastly outperformed by a regular softmax classiﬁer in
terms of computation time (factor 45 increase).
Storage indicates the additional required memory to be allo-
cated to constitute the continual learning method. LwF and
iCaRL both store the previous task model for knowledge
distillation (although one could also store the recorded out-
puts instead ). On top of that, EBLL stores an autoencoder,
small in size A relative to M , for each of the previous tasks.
GEM instead needs to store the gradient for the exemplar
set of each seen tasks. The prior-focused methods EWC
and MAS store importance weights and the previous task
model, with SI also requiring a running estimate of the
current task importance weights. IMM, when naively imple-
mented, stores for each task the model after training, and
in mode-IMM additionally stores the importance weights.
More storage efﬁcient implementations propagate a merged
model instead. Finally, PackNet requires a mask equal to the
amount parameters in the model (M [bit]) for each task.
Task-agnostic and privacy. In deployment PackNet requires
an oracle to indicate the task for a given sample in order to
load the appropriate masks. Therefore, this setup inherently
prevents task-agnostic inference. Next, we emphasize the
privacy issues for replay-based methods iCaRL and GEM
when storing raw images as exemplars.

6.8 Experiments Summary

We try to summarize our main ﬁndings for each of the
methods in Table 9.

16

7 LOOKING AHEAD

In the experiments performed in this survey, we have con-
sidered an incremental task learning setting where tasks
associated with their training data are received one at the
time. An ofﬂine training is performed on each task data
until convergence. This setting requires knowledge of the
task boundaries (i.e. when tasks switch) and allows for
multiple passes over large batches of training data. Hence,
it resembles a relaxation of the desired continual learning
system that is more likely to be encountered in practice.
Below, we describe the general continual learning setting
in which continual learning methods are expected to be
deployed and outlines the main characteristics that future
developed methods should aim to realize.
The General Continual Learning setting considers an
inﬁnite stream of training data where at each time step,
the system receives a (number of) new sample(s) {(xt, yt)}
drawn non i.i.d from a current distribution Dt that could
itself experience sudden or gradual changes. The main goal
is to optimally learn a function f parametrized by θ that
minimizes a predeﬁned loss ℓ on the new sample(s) without
interfering with and possibly improving on those that were
learned previously. Desiderata of an ideal continual learning
scheme include:
1. Constant memory. The memory consumed by the contin-
ual learning paradigm should be constant w.r.t. the number
of tasks or the length of the data stream. This is to avoid the
need to deal with unbounded systems.
2. No task boundaries. Being able to learn from the input
data without requiring a clear task division brings great
ﬂexibility to the continual learning method and makes it
applicable to any scenario where data arrives in a never
ending manner.
3. Online learning. A largely ignored characteristic of con-
tinual learning is being able to learn from a continuous
stream of data without ofﬂine training of large batches or
separate tasks.
4. Forward transfer. This characteristic indicates the im-
portance of the previously acquired knowledge to aid the
learning of new tasks.
5. Backward transfer. A continual learning system shouldn’t
only aim at retaining previous knowledge but preferably
also at improving the performance on previous tasks when
learning future related tasks.
6. Problem agnostic. A continual learning method should
be general and not limited to a speciﬁc setting (e.g. only
classiﬁcation).
7. Adaptive. Being able to learn from unlabeled data would
increase the method applicability to cases where original
training data no longer exists and further open the door to
a speciﬁc user setting adaptation.
8. No test time oracle. A well designed continual learning
method shouldn’t rely on a task oracle to perform predic-
tions.
9. Task revisiting. When revisiting a previous task again,
the system should be able to successfully incorporate the
new task knowledge.
10. Graceful forgetting. Given an unbounded system and
inﬁnite stream of data, a selective forgetting of unimportant
information is an important mechanism to achieve a balance

17

TABLE 8: Qualitative comparison of the compared continual learning methods. The heatmap gives relative numbers to the
minimal observed value in the column.

Low

High

Number of Seen Tasks

T
M Model Size
R
A

Replay Buffer Size
Autoencoder Size

Category

Method

Memory

Compute

Task-agnostic
possible

Privacy
issues

Additional required
storage

Replay-based

Reg.-based

Param. iso.-based

train
1.24
iCARL
1.07
GEM
1.07
LWF
1.53
EBLL
1.09
SI
1.09
EWC
1.09
MAS
mean-IMM 1.01
mode-IMM 1.01
1.00
PackNet

test
1.00
1.29
1.10
1.08
1.05
1.05
1.05
1.03
1.03
1.94

train
5.16
9.76
1.18
2.05
1.04
1.02
1.06
1.00
1.14
2.43

test
45.6
3.64
1.86
1.34
1.61
1.88
1.88
1.18
1.00
2.40

X
X
X
X
X
X
X
X
X
✗

X
X
✗
✗
✗
✗
✗
✗
✗
✗

M + R
T · M + R
M
M + T · A
3 · M
2 · M
2 · M
T · M
2 · T · M
T · M [bit]

TABLE 9: Summary of our main ﬁndings. We report best results over all experiments (i.e. including regularization
experiments for Tiny Imagenet).

Method

Best Avg. Acc.
Tiny Imagenet

Best Avg. Acc.
iNaturalist

Suggested
Regularizer

Suggested
Model

Comments

Replay-Based

iCaRL 4.5k (9k)

48.55 (49.94)

dropout

small/base/wide

GEM 4.5k (9k)

45.27 (44.23)

none/dropout

small/base/wide

Regularization-Based
LwF

EBLL

SI

EWC

MAS

48.11

48.17

43.74

45.13

48.98

dropout/L2

base/wide

L2

L2

none

none

wide

wide

small

base/wide

x

x

48.02

53.30

51.77

54.02

54.59

mean/mode-IMM

32.42/42.41

49.82/55.61

none/dropout

base/wide

Parameter isolation-based
PackNet

55.96

59.41

dropout/L2

small/base/wide

- Least sensitive model capacity/regularization
- Privacy issues storing raw images
- No clear policy for unbalanced tasks

- Lead performance
- Designed for class incremental
- Continual Exemplar Management
- Nearest Neighbor classiﬁer

- Designed for online continual setup
- Sensitive to amount of epochs

- Invigorated by WIDE model
- Requires sample outputs on previous model

- Margin over LwF
- Autoencoder gridsearch for unbalanced tasks

- Efﬁcient training time over EWC/MAS
- Requires dropout or L2 (prone to overﬁtting)
- Most affected by task ordering

- Invigorated by small capacity model
- Deteriorates on WIDE model

- Lead regularization-based performance
- Hyperparameter robustness
- Unsupervised importance weight estimation

- Lead real-world performance (mode-IMM)
- mode-IMM outperforms mean-IMM
- Requires additional merging step

- Lead performance
- Efﬁcient memory (1 model, task-speciﬁc masks)
- No forgetting (after compression)
- Requires task oracle for prediction
- Requires retraining after compression

of stability and plasticity.

After establishing the general continual learning setting
and the main desiderata, it is important to identify the
critical differences between continual learning and other
closely related machine learning ﬁelds that share some of
the characteristics mentioned above with continual learning.

8 RELATION TO OTHER MACHINE
FIELDS

LEARNING

The ideas of knowledge sharing, adaptation and transfer
depicted in the outlined desiderata have been studied pre-
viously in machine learning and developed in related ﬁelds.
We will describe each of them brieﬂy and highlight the main
differences with continual learning (see also Figure 5).
Multi Task Learning. Multi-Task Learning (MTL) considers
learning multiple related tasks simultaneously using a set
or subset of shared parameters. It aims for a better gener-
alization and a reduced overﬁtting using shared knowledge
extracted from the related tasks. We refer to [21] for a survey
on the topic. Multi-task learning performs ofﬂine training on
all tasks simultaneously and doesn’t involve any adaptation
after the model has been deployed, as opposed to continual
learning.
Transfer Learning. Transfer learning aims to aid the learn-
ing process of a given task (the target) by exploiting
knowledge acquired from another task or domain (the
source). Transfer learning is mainly concerned with the
forward transfer desiderata of continual learning. However,
it doesn’t involve any continuous adaptation after learning
the target task. Moreover, the performance on the source
task(s) is not taken into account during transfer learning.
Domain Adaptation. Domain adaptation is a sub-ﬁeld of
transfer learning where the source and target tasks are the
same but drawn from different input domains. The target
domain data is unlabelled (or has only few labels) and the
goal is to adapt a model trained on the source domain to
perform well on the target domain. In other words, it relaxes
the classical machine learning assumption of having train-
ing and test data drawn from the same distribution [81]. As
mentioned above, as for transfer learning, domain adapta-
tion is unidirectional and doesn’t involve any accumulation
of knowledge [6].
Learning to Learn (Meta Learning). The old deﬁnition of
learning to learn was referring to the concept of improving
the learning behaviour of a model with training experience.
However, more recently, the common interpretation is the
ability for a faster adaptation on a task with few examples
given a large number of training tasks. While these ideas
seem quite close to continual learning, meta learning still
follows the same assumption of ofﬂine training but with
data being randomly drawn from a task training distribu-
tion and test data being tasks with few examples. Hence,
it is not capable, alone, of preventing forgetting on those
previous tasks.
Online Learning. In traditional ofﬂine learning, the entire
training data has to be made available prior to learning the
task. On the contrary, online learning studies algorithms that
learn to optimize predictive models over a stream of data
instances sequentially. We refer to [82], [83] for surveys on
the topic. Note that online learning assumes an i.i.d data

18

sampling procedure and considers a single task/domain,
which sets it apart from continual learning.
Open World Learning. Open world learning [84], [85] deals
with the problem of detecting new classes at test time,
hence avoiding wrong assignments to known classes. When
those new classes are then integrated in the model, it meets
the problem of incremental learning. As such, open world
learning can be seen as a sub task of continual learning.

9 CONCLUSION

In this work we scrutinized recent state-of-the-art continual
learning methods, conﬁning the study to task-incremental
classiﬁcation with a multi-head setup, as mainly applied in
literature. Within these outlines we proposed a constructive
taxonomy and tested each of the compared methods in an
attempt to answer several critical questions in the ﬁeld. In
order to fulﬁll the urge for a fair comparison compliant
with the continual learning paradigm, we proposed a novel
continual hyperparameter framework which dynamically
deﬁnes forgetting related hyperparameters, i.e. the stability-
plasticity trade-off is determined in a continual fashion
with only the current task data available. The overview
of our experimental ﬁndings in Table 9 shed an empirical
light on which methods perform best, supplemented by
recommendations for model capacity and the use of dropout
or weight decay. The experiments extend to two datasets,
namely Tiny Imagenet and iNaturalist, from which the latter
resembles a real-world dataset to challenge methods with an
unbalanced data distribution and highly varying tasks. On
top, we addressed the inﬂuence of task-ordering and found
it minimally inﬂuential towards the general trends of the
compared continual learning methods.

Although the state-of-the-art made some signiﬁcant
progress to tackle catastrophic forgetting in neural net-
works, the majority of results originates from a highly
conﬁned setup, leaving numerous opportunities for further
research to reach beyond classiﬁcation, multi-head evalua-
tion and a task-incremental setup.

ACKNOWLEDGMENTS

The authors would like to thank Huawei for funding this
research.

REFERENCES

[1] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai,
A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al.,
“A general reinforcement learning algorithm that masters chess,
shogi, and go through self-play,” Science, vol. 362, no. 6419, pp.
1140–1144, 2018.

[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet
large scale visual recognition challenge,” International journal of
computer vision, vol. 115, no. 3, pp. 211–252, 2015.

[3] R. M. French, “Catastrophic forgetting in connectionist networks,”

Trends in cognitive sciences, vol. 3, no. 4, pp. 128–135, 1999.

[4] R. M. French and A. Ferrara, “Modeling time perception in rats:
Evidence for catastrophic interference in animal
learning,” in
Proceedings of the 21st Annual Conference of the Cognitive Science
Conference. Citeseer, 1999, pp. 173–178.
S. Grossberg, Studies of mind and brain : neural principles of learning,
perception, development, cognition, and motor control, ser. Boston
studies in the philosophy of science 70. Dordrecht: Reidel, 1982.

[5]

19

Fig. 5: The main setup of each related machine learning ﬁeld, illustrating the differences with general continual learning
settings.

[6] Z. Chen and B. Liu, “Lifelong machine learning,” Synthesis Lectures
on Artiﬁcial Intelligence and Machine Learning, vol. 12, no. 3, pp. 1–
207, 2018.

[7] R. Aljundi, P. Chakravarty, and T. Tuytelaars, “Expert gate: Life-
long learning with a network of experts,” in CVPR, 2017, pp. 3366–
3375.

[8] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny,
“Efﬁcient lifelong learning with A-GEM,” in ICLR, 2019. [Online].
Available: https://openreview.net/forum?id=Hkf2 sC5FX
[9] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter,
“Continual lifelong learning with neural networks: A review,”
Neural Networks, 2019.

[10] D. L. Silver and R. E. Mercer, “The task rehearsal method of life-
long learning: Overcoming impoverished data,” in Conference of the
Canadian Society for Computational Studies of Intelligence. Springer,
2002, pp. 90–101.

[11] A. Rannen, R. Aljundi, M. B. Blaschko, and T. Tuytelaars, “Encoder

based lifelong learning,” in ICCV, 2017, pp. 1320–1328.

[12] R. Aljundi, M. Rohrbach, and T. Tuytelaars, “Selﬂess sequential

learning,” arXiv preprint arXiv:1806.05421, 2018.

[13] M. McCloskey and N. J. Cohen, “Catastrophic interference in
connectionist networks: The sequential learning problem,” in Psy-
chology of learning and motivation. Elsevier, 1989, vol. 24, pp. 109–
165.

[14] H. Shin, J. K. Lee, J. Kim, and J. Kim, “Continual learning with

deep generative replay,” in NeurIPS, 2017, pp. 2990–2999.

[15] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuyte-
laars, “Memory aware synapses: Learning what (not) to forget,”
in ECCV, 2018, pp. 139–154.

[16] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr, “Rieman-
nian walk for incremental learning: Understanding forgetting and
intransigence,” in ECCV, 2018, pp. 532–547.

[17] A. Gepperth and C. Karaoguz, “A bio-inspired incremental learn-
ing architecture for applied perceptual problems,” Cognitive Com-
putation, vol. 8, no. 5, pp. 924–934, 2016.

[18] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl:
Incremental classiﬁer and representation learning,” in CVPR, 2017,
pp. 2001–2010.

[19] A. Rosenfeld and J. K. Tsotsos, “Incremental learning through

deep adaptation,” TPAMI, 2018.

[20] K. Shmelkov, C. Schmid, and K. Alahari, “Incremental learning of
object detectors without catastrophic forgetting,” in ICCV, 2017,
pp. 3400–3409.

[21] Y. Zhang and Q. Yang, “A survey on multi-task learning,” arXiv

preprint arXiv:1707.08114, 2017.

[23] B. Ans and S. Rousset, “Avoiding catastrophic forgetting by
coupling two reverberating neural networks,” Comptes Rendus de
l’Acad´emie des Sciences-Series III-Sciences de la Vie, vol. 320, no. 12,
pp. 989–997, 1997.

[24] A. Robins, “Catastrophic forgetting, rehearsal and pseudore-
hearsal,” Connection Science, vol. 7, no. 2, pp. 123–146, 1995.
[25] T. Lesort, V. Lomonaco, A. Stoian, D. Maltoni, D. Filliat, and
learning for robotics,” arXiv

N. D´ıaz-Rodr´ıguez, “Continual
preprint arXiv:1907.00182, 2019.

[26] B. Pf ¨ulb and A. Gepperth, “A comprehensive, application-oriented
study of catastrophic forgetting in DNNs,” in ICLR, 2019. [Online].
Available: https://openreview.net/forum?id=BkloRs0qK7

[27] S. Farquhar and Y. Gal, “Towards robust evaluations of continual

learning,” arXiv preprint arXiv:1805.09733, 2018.

[28] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska
et al., “Overcoming catastrophic forgetting in neural networks,”
Proceedings of the national academy of sciences, p. 201611835, 2017.

[29] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang, “Overcom-
ing catastrophic forgetting by incremental moment matching,” in
NeurIPS, 2017, pp. 4652–4662.

[30] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio,
“An empirical investigation of catastrophic forgetting in gradient-
based neural networks,” arXiv preprint arXiv:1312.6211, 2013.
[31] R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan,
“Measuring catastrophic forgetting in neural networks,” in Thirty-
second AAAI conference on artiﬁcial intelligence, 2018.

[32] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu,
A. Pritzel, and D. Wierstra, “Pathnet: Evolution channels gradient
descent in super neural networks,” arXiv preprint arXiv:1701.08734,
2017.

[33] Y.-C. Hsu, Y.-C. Liu, and Z. Kira, “Re-evaluating continual learning
scenarios: A categorization and case for strong baselines,” arXiv
preprint arXiv:1810.12488, 2018.

[34] R. M. French, “Semi-distributed representations and catastrophic
forgetting in connectionist networks,” Connection Science, vol. 4,
no. 3-4, pp. 365–377, 1992.

[35] F. R. M, “Dynamically constraining connectionist networks to
produce distributed, orthogonal representations to reduce catas-
trophic interference,” network, vol. 1111, p. 00001, 1994.

[36] J. K. Kruschke, “Human category learning: Implications for back-
propagation models,” Connection Science, vol. 5, no. 1, pp. 3–36,
1993.

[37] S. A. Sloman and D. E. Rumelhart, “Reducing interference in
distributed memories through episodic gating,” Essays in honor
of WK Estes, vol. 1, pp. 227–248, 1992.

[22] J. K. Kruschke, “Alcove: an exemplar-based connectionist model
of category learning.” Psychological review, vol. 99, no. 1, p. 22, 1992.

[38] R. M. French, “Pseudo-recurrent connectionist networks: An
approach to the ’sensitivity-stability’ dilemma,” Connection

20

[62] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial
nets,” in NeurIPS, 2014, pp. 2672–2680.

[63] Y. Bengio and Y. LeCun, Eds., 2nd International Conference on
ICLR 2014, Banff, AB, Canada, April
Learning Representations,
14-16, 2014, Conference Track Proceedings, 2014. [Online]. Available:
https://openreview.net/group?id=ICLR.cc/2014

[64] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner, “Variational

continual learning,” arXiv preprint arXiv:1710.10628, 2017.

[65] R. Aljundi, K. Kelchtermans, and T. Tuytelaars, “Task-free contin-

ual learning,” in CVPR, 2019, pp. 11 254–11 263.

[66] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer,
J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell,
“Progressive Neural Networks,” Tech. Rep., 2016.
[Online].
Available: http://arxiv.org/abs/1606.04671

[67] G. Hinton and J. Dean, “Distilling the Knowledge in a Neural

Network,” Tech. Rep., 2015.

[68] D. J. C. MacKay, “A Practical Bayesian Framework for Backprop-

agation Networks,” Neural Computation, 1992.

[69] R. Pascanu and Y. Bengio, “Revisiting Natural Gradient
[Online]. Available:
jan
for Deep Networks,”
https://arxiv.org/abs/1301.3584
“New insights
[70] J. Martens,
natural
gradient method,”
http://arxiv.org/abs/1412.1193

the
[Online]. Available:

perspectives

and
2014.

2013.

on

[71] J. Schwarz, J. Luketina, W. M. Czarnecki, A. Grabska-Barwinska,
Y. W. Teh, R. Pascanu, and R. Hadsell, “Progress & Compress:
A scalable framework for continual learning,” Tech. Rep., 2018.
[Online]. Available: https://arxiv.org/pdf/1805.06370.pdf

[72] I. J. Goodfellow, O. Vinyals, and A. M. Saxe, “Qualitatively
characterizing neural network optimization problems,” dec 2014.
[Online]. Available: http://arxiv.org/abs/1412.6544

[73] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R.
Salakhutdinov, “Improving neural networks by preventing co-
adaptation of feature detectors,” 2012.

[74] Stanford, “Tiny ImageNet Challenge, CS231N Course.” [Online].

Available: https://tiny-imagenet.herokuapp.com/

[75] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Ima-
genet: A large-scale hierarchical image database,” in CVPR.
Ieee,
2009, pp. 248–255.

[76] “iNaturalist dataset: FGVC5 workshop at CVPR 2018.” [Online].

Available: https://www.kaggle.com/c/inaturalist-2018

[77] C. Chung, S. Patel, R. Lee, L. Fu, S. Reilly, T. Ho, J. Lionetti,
M. D. George, and P. Taylor, “Implementation of an integrated
computerized prescriber order-entry system for chemotherapy in
a multisite safety-net health system,” American Journal of Health-
System Pharmacy, 2018.

[78] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁca-
tion with deep convolutional neural networks,” in NeurIPS, 2012,
pp. 1097–1105.

[79] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio,
“An Empirical
Investigation of Catastrophic Forgetting in
Gradient-Based Neural Networks,” Tech. Rep. [Online]. Available:
https://arxiv.org/pdf/1312.6211.pdf

[80] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum
learning,” in Proceedings of the 26th annual international conference
on machine learning. ACM, 2009, pp. 41–48.

[81] G. Csurka, Domain adaptation in computer vision applications.

Springer, 2017.

[82] S. Shalev-Shwartz et al., “Online learning and online convex op-
timization,” Foundations and Trends R(cid:13) in Machine Learning, vol. 4,
no. 2, pp. 107–194, 2012.

[83] L. Bottou, “Online learning and stochastic approximations,” On-

line learning in neural networks, vol. 17, no. 9, p. 142, 1998.

[84] H. Xu, B. Liu, L. Shu, and P. S. Yu, “Learning to accept new classes
without training,” CoRR, vol. abs/1809.06004, 2018. [Online].
Available: http://arxiv.org/abs/1809.06004

[85] A. Bendale and T. Boult, “Towards open world recognition,” in

CVPR, 2015, pp. 1893–1902.

Science, vol. 9, no. 4, pp. 353–380, 1997. [Online]. Available:
https://doi.org/10.1080/095400997116595

[39] J. Rueckl, “Jumpnet: A multiple-memory connectionist architec-
ture,” Proceedings of the 15 th Annual Conference of the Cognitive
Science Society, no. 24, pp. 866–871, 1993.

[40] B. Ans and S. Rousset, “Avoiding catastrophic forgetting by
coupling two reverberating neural networks,” Comptes Rendus
de
la Vie,
- Series
vol. 320, no. 12, pp. 989 – 997, 1997.
[Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0764446997824729

l’Acadmie des Sciences

- Sciences de

III

[41] N. Srivastava, G. Hinton, A. Krizdhevsky, I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural net-
works from overﬁtting,” The Journal of Machine Learning Research,
vol. 15, no. 1, pp. 1929–1958, 2014.

[42] A. Pentina and C. H. Lampert, “A pac-bayesian bound for lifelong

learning.” in ICML, 2014, pp. 991–999.

[43] R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio, “Online continual
learning with no task boundaries,” CoRR, vol. abs/1903.08671,
2019. [Online]. Available: http://arxiv.org/abs/1903.08671

[44] D. Rolnick, A. Ahuja,
“Experience

G. Wayne,
CoRR,
abs/1811.11682,
vol.
http://arxiv.org/abs/1811.11682

replay

for
2018.

J. Schwarz, T. P. Lillicrap,

and
learning,”
continual
[Online]. Available:

[45] D. Isele and A. Cosgun, “Selective experience replay for lifelong
learning,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018.

[46] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K.
Dokania, P. H. Torr, and M. Ranzato, “Continual learning with
tiny episodic memories,” arXiv preprint arXiv:1902.10486, 2019.
[47] C. Atkinson, B. McCane, L. Szymanski, and A. V. Robins,
“Pseudo-recursal: Solving the catastrophic forgetting problem in
deep neural networks,” CoRR, vol. abs/1802.03875, 2018. [Online].
Available: http://arxiv.org/abs/1802.03875

[48] F. Lavda, J. Ramapuram, M. Gregorova, and A. Kalousis, “Contin-
ual classiﬁcation learning using generative models,” arXiv preprint
arXiv:1810.10612, 2018.

[49] J. Ramapuram, M. Gregorova, and A. Kalousis, “Lifelong genera-

tive modeling,” arXiv preprint arXiv:1705.09847, 2017.

[50] D. Lopez-Paz et al., “Gradient episodic memory for continual

learning,” in NeurIPS, 2017, pp. 6470–6479.

[51] F. Zenke, B. Poole, and S. Ganguli, “Continual learning through
synaptic intelligence,” in Proceedings of the 34th International Con-
ference on Machine Learning-Volume 70.
JMLR. org, 2017, pp. 3987–
3995.

[52] X. Liu, M. Masana, L. Herranz, J. Van de Weijer, A. M. Lopez, and
A. D. Bagdanov, “Rotate your Networks: Better Weight Consoli-
dation and Less Catastrophic Forgetting,” 2018.

[53] Z. Li and D. Hoiem, “Learning without forgetting,” in ECCV.

Springer, 2016, pp. 614–629.

[54] H. Jung, J. Ju, M. Jung, and J. Kim, “Less-forgetting Learning
in Deep Neural Networks,” Tech. Rep., 2016. [Online]. Available:
http://arxiv.org/abs/1607.00122

[55] J. Zhang,

J. Zhang, S. Ghosh, D. Li, S. Tasci, L. P. Heck,
H. Zhang, and C. J. Kuo, “Class-incremental learning via deep
model consolidation,” CoRR, vol. abs/1903.07864, 2019. [Online].
Available: http://arxiv.org/abs/1903.07864

[56] A. Mallya and S. Lazebnik, “Packnet: Adding multiple tasks to a
single network by iterative pruning,” in CVPR, 2018, pp. 7765–
7773.

[57] A. Mallya, D. Davis, and S. Lazebnik, “Piggyback: Adapting a
single network to multiple tasks by learning to mask weights,” in
ECCV, 2018, pp. 67–82.

[58] J. Serr`a, D. Sur´ıs, M. Miron, and A. Karatzoglou, “Overcoming
catastrophic forgetting with hard attention to the task,” arXiv
preprint arXiv:1801.01423, 2018.

[59] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirk-
patrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive
neural networks,” arXiv preprint arXiv:1606.04671, 2016.

[60] J. Xu and Z. Zhu,

“Reinforced Continual Learning,”
in NeurIPS,
Larochelle,
S.
K. Grauman, N. Cesa-Bianchi,
Eds.
Curran Associates, Inc., 2018, pp. 899–908. [Online]. Available:
http://papers.nips.cc/paper/7369-reinforced-continual-learning.pdf

Bengio, H. Wallach, H.

and R. Garnett,

[61] D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, and G. Wayne,

“Experience replay for continual learning,” 2018.

APPENDIX A
IMPLEMENTATION DETAILS
A.1 Model Setup

Tiny Imagenet VGG-based models.
Classiﬁers (fully connected units):

• C1: [128, 128] + multi-head
• C2: [512, 512] + multi-head

Full models(feature map count, M = max pooling):

•

SMALL: [64, M, 64, M, 64, 64, M, 128, 128, M] + C1
BASE: [64, M, 64, M, 128, 128, M, 256, 256, M] + C2
•
• WIDE: [64, M, 128, M, 256, 256, M, 512, 512, M] + C2
• DEEP: [64, M, (64,)*6, M, (128,)*6, M, (256,)*6, M] + C2

A.2 Framework Setup

coarse

For both Tiny Imagenet and iNaturalist maximal plas-
learning rate grid Ψ =
ticity search has
1e−2, 5e−3, 1e−3, 5e−4, 1e−4
. Tiny Imagenet starts from
scratch and therefore applies 2 additional stronger learning
(cid:9)
(cid:8)
rates Ψ ∪

for the ﬁrst tasks.
We set the ﬁnetuning accuracy drop margin to p = 0.2

1e−1, 5e−2

and decaying factor α = 0.5, unless mentioned otherwise.

(cid:8)

(cid:9)

A.3 Methods Setup

General setup for all methods:

Stochastic Gradient Descen with a momentum of 0.9.
•
• Max 70 training epochs with early stopping and
annealing of the learning rate: after 5 unimproved
iterations of the validation accuracy the learning rate
decays with factor 10, after 10 unimproved iterations
training terminates.

• All methods use batch size 200, except PackNet uses

batch size 32 as deﬁned in its original work.

• The baselines and PackNet start from scratch, other
methods continue from the same model trained for
the ﬁrst task.

• All softmax temperatures for knowledge distillation

are set to 2.

• Pytorch implementation.

We observed in our experiments that the initial hyperparam-
eter values of the methods consistently decay to a certain
order of magnitude. To avoid overhead, we start all methods
with the observed upper bound, with an additional inverse
decay step as margin.

Replay-based setup. GEM doesn’t specify a memory man-
agement policy and merely divides an equal portion of
memory to each of the tasks. The initial value for the
forgetting-related hyperparameter is set to 1.

In our task-incremental setup, iCARL fully exploits the
total available exemplar memory M , and incrementally
divides the capacity equally over all the seen tasks. Similar
to LwF, the initial knowledge distillation strength is 10.
Regularization-based setup. Initial regularization strength
based on consistently observed decays in the framework:
EWC 400, SI 400, MAS 3. LwF and EBLL both start with the
knowledge distillation strengths set to 10, EBLL loss w.r.t.
the code is initialized with 1. EBLL hyperparameters for

21

the autoencoder are determined in a gridsearch with code
dimension {100, 300} for Tiny Imagenet and {200, 500} for
iNaturalist, both with reconstruction regularization strength
in {0.01, 0.001}, with learning rate 0.01, and for 50 epochs.
For IMM on Tiny Imagenet dataset we observed no
clear inﬂuence of the L2 transfer regularization strength
(for values 10e−2, 10e−3, 10e−4), and therefore executed all
experiments with a ﬁxed value of 10e−2 instead. In order to
merge the models, all task models are equally weighted,
as there are no indications in how the values should be
determined in [29].
Parameter isolation-based setup. For PackNet the initial
value of the forgetting-related hyperparameter amounts
to 90% pruning per layer, decayed with factor 0.9 if not
satisfying ﬁnetuning threshold.

APPENDIX B
SUPPLEMENTAL RESULTS
B.1 Synaptic Intelligence (SI): Overﬁtting and Regular-
ization

The observed overﬁtting for SI on Tiny Imagenet is illus-
trated in Table 10 for the BASE model. Training accuracy
without regularization tends to reach 100% for all tasks,
with the validation phase merely attaining about half of this
accuracy. This results in an average discrepancy between
training and validation accuracy of 48.8%, indicate signiﬁ-
cant overﬁtting on the training data. However, this discrep-
ancy can be greatly reduced by the use of regularization,
such as dropout (20.6%) or weight decay (30.8%).

Without regularization the overﬁtting results of SI are
very similar to the ﬁnetuning baseline (48.8% and 48.0%
discrepancy). However, ﬁnetuning does not clearly bene-
ﬁt from regularization, retaining training accuracies near
100%. Observing different weight decay strengths in Ta-
ble 11, severely increasing the standard weight decay
strength in our experiments (λ = 0.0001) by up to a factor
100 merely reduces average training accuracy to 85.5%,
leaving a signiﬁcant discrepancy with the validation accu-
racy of 37.8%.
Continual

learning methods EWC and LwF in the
regularization-based family of SI, inherently reduce overﬁt-
ting (resp. 38.4% and 28.9% discrepancy) without any use
of the traditional weight decay and dropout regularizers.

B.2 Extra Replay-based Experiments: Epoch Sensitiv-
ity

In initial experiments with 70 epochs we observed infe-
rior performance of GEM w.r.t. iCARL and the rehearsal
baselines R-PM and R-PM. In the original GEM setup [50]
only a single epoch is assumed for each task, while in our
experiments methods get the advantage of multiple epochs.
This might impose a disadvantage when comparing GEM to
the other methods, wherefore we conduct this extra experi-
ment to attain a fair comparison (Table 12). As in the other
experiments in this work, we apply early stopping after
10 non-improved epochs (indicated with ∗), which makes
epochs 20 to 70 upper bounds of the actual trained epochs.
The GEM setup with only 5 epochs shows superior average
accuracies, indicating a better trade-off between the online

TABLE 10: We scrutinize the level of overﬁtting for SI compared to Finetuning, EWC and LwF. Training (train), validation
(val) accuracies and discrepancy (train− val) are reported for the BASE model on randomly ordered Tiny Imagenet.
Accuracies are determined on data solely from the indicated current task.

22

SI

No Regularization

Finetuning No Regularization

EWC

No Regularization

Dropout

L2

L2

L2

L2

Dropout

Dropout

Dropout

LwF

No Regularization

Accuracy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

T1

97.3
53.3
44.1

97.9
59.6
38.3

99.9
51.3
48.6

99.8
53.5
46.3

94.2
58.4
35.8

99.8
52.8
47.0

97.3
53.3
44.1

97.9
59.6
38.3

99.9
51.3
48.6

99.8
53.5
46.3

94.2
58.4
35.8

99.8
52.8
47.0

T2

99.9
48.9
51.0

85.0
54.2
30.9

97.2
46.3
50.9

100
47.4
52.6

96.2
57.2
39.0

99.9
49.8
50.1

99.8
48.4
51.4

94.8
56.6
38.2

100
48.9
51.1

100
50.8
49.3

96.7
56.4
40.3

100
50.0
50.0

T3

99.6
51.8
47.9

65.6
54.7
10.9

75.3
49.9
25.5

92.1
49.1
43.0

98.3
60.2
38.1

99.9
53.9
46.0

95.8
52.7
43.1

96.8
57.9
39.0

97.8
50.3
47.5

67.4
53.2
14.3

96.1
55.9
40.2

64.1
47.6
16.5

T4

90.0
51.2
38.8

70.6
56.4
14.2

75.2
49.0
26.2

100
51.8
48.2

98.0
59.6
38.4

100
51.4
48.6

69.4
51.7
17.7

94.8
58.0
36.8

94.6
49.5
45.1

100
51.7
48.3

97.4
55.6
41.8

100
47.5
52.5

T5

99.9
46.5
53.4

71.3
50.0
21.3

73.0
44.2
28.8

99.6
44.0
55.6

97.7
53.4
44.3

100
45.2
54.9

97.4
46.3
51.1

87.7
50.4
37.3

71.9
44.8
27.2

50.3
42.9
7.4

58.6
49.0
9.7

61.6
42.0
19.6

T6

98.8
51.2
47.6

70.0
55.7
14.4

76.3
50.1
26.2

100
51.4
48.6

97.9
58.0
40.0

100
50.1
49.9

92.7
52.5
40.2

84.4
55.4
29.0

93.8
48.7
45.1

99.6
47.8
51.8

93.0
54.2
38.8

99.8
46.4
53.4

T7

99.7
45.0
54.7

68.3
49.3
19.0

68.1
43.4
24.7

100
42.9
57.1

94.9
51.9
43.0

100
45.2
54.9

78.7
45.3
33.5

77.6
49.3
28.3

68.8
41.1
27.7

63.5
40.6
22.9

88.1
49.8
38.3

50.8
38.4
12.5

T8

99.9
50.7
49.2

76.1
56.2
20.0

67.5
48.5
19.0

87.9
49.3
38.6

98.8
59.4
39.4

72.7
48.1
24.6

85.8
53.1
32.7

83.0
55.7
27.3

82.9
50.0
32.9

58.9
48.1
10.9

67.9
54.4
13.5

69.9
46.6
23.3

T9

100
48.4
51.6

71.6
55.1
16.5

71.7
48.1
23.6

100
47.9
52.1

98.2
57.1
41.1

99.8
47.9
52.0

84.7
49.8
34.9

76.4
55.3
21.1

66.0
48.1
17.9

60.4
47.3
13.1

89.4
55.4
34.0

68.4
46.7
21.8

T10

98.4
48.6
49.8

72.6
52.6
20.1

82.0
47.6
34.4

85.8
47.9
37.9

93.4
55.9
37.6

100
46.2
53.8

84.4
49.5
34.9

87.3
54.1
33.2

74.4
47.4
27.1

70.8
46.5
24.4

98.7
52.7
46.1

62.0
43.2
18.9

Avg

98.4
49.5
48.8

74.9
54.4
20.6

78.6
47.8
30.8

96.5
48.5
48.0

96.7
57.1
39.7

97.2
49.0
48.2

88.6
50.2
38.4

88.1
55.2
32.9

85.0
48.0
37.0

77.1
48.2
28.9

88.0
54.2
33.9

77.6
46.1
31.5

TABLE 11: Finetuning with different weight decay strengths (λ), reported for the BASE model on randomly ordered Tiny
Imagenet. λ = 0.0001 is the standard setup for our experiments.

λ

0.0001

0.001

0.01

Accuracy

train
val
discrepancy

train
val
discrepancy

train
val
discrepancy

T1

99.8
52.8
47.0

99.6
52.9
46.7

69.2
49.4
19.9

T2

99.9
49.8
50.1

100
51.1
48.9

93.6
47.4
46.3

T3

99.9
53.9
46.0

100
54.3
45.7

96.2
51.2
45.0

T4

100
51.4
48.6

100
53.2
46.8

99.7
48.4
51.3

T5

100
45.2
54.9

100
47.5
52.5

52.9
43.3
9.6

T6

100
50.1
49.9

100
50.6
49.5

75.0
49.1
25.9

T7

100
45.2
54.9

50.0
43.0
7.0

74.7
43.6
31.2

T8

72.7
48.1
24.6

99.7
51.2
48.5

93.5
50.0
43.5

T9

99.8
47.9
52.0

100
50.9
49.1

100
48.8
51.2

T10

100
46.2
53.8

100
49.1
51.0

100
46.4
53.6

Avg

97.2
49.0
48.2

94.9
50.4
44.5

85.5
47.7
37.8

23

setup GEM was designed for and exploiting several epochs
to optimize for the current task. Therefore, we conduct all
GEM experiments in this work with 5 epochs.

The other replay-based method iCARL, on the other
hand, mainly beneﬁts from more epochs, stagnating at more
than 10 epochs, with only a small
increase in average
forgetting.

TABLE 12: GEM and iCARL sensitivity analysis on the
amount of epochs for average accuracy (average forgetting)
on the BASE network for Tiny Imagenet. Both exemplar
memories of size 4.5k and 9k are considered using the
standard setup of our experiments, i.e. early stopping after
10 non-improved epochs (indicated with ∗).

Epochs GEM 4.5k

GEM 9k

iCARL 4.5k

iCARL 9k

1
5
10
20*
30*
50*
70*

35.05 (4.04)
42.05 (6.40)
38.09 (10.05)
40.54 (9.08)
38.87 (9.28)
32.47 (12.50)
25.83 (14.67)

38.23 (5.00)
43.85 (4.42)
43.19 (7.12)
34.79 (12.07)
29.11 (13.33)
38.89 (9.70)
40.65 (7.17)

41.18 (-1.02)
46.97 (-1.95)
46.82 (-1.78)
47.44 (-1.23)
47.43 (-1.88)
47.22 (-1.43)
47.27 (-1.11)

41.05 (-0.68)
47.89 (-2.57)
47.55 (-2.06)
48.78 (-2.12)
49.27 (-2.58)
46.09 (-1.01)
48.76 (-1.76)

