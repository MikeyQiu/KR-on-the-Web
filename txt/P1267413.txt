Imitation Learning from Imperfect Demonstration

Yueh-Hua Wu1,2, Nontawat Charoenphakdee2,3, Han Bao2,3,
Voot Tangkaratt2, Masashi Sugiyama3,2

1 National Taiwan University
2 RIKEN Center for Advanced Intelligence Project
3 The University of Tokyo

9
1
0
2
 
n
a
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
3
v
7
8
3
9
0
.
1
0
9
1
:
v
i
X
r
a

Abstract

Imitation learning (IL) aims to learn an optimal policy from demonstrations. However,
such demonstrations are often imperfect since collecting optimal ones is costly. To eﬀectively
learn from imperfect demonstrations, we propose a novel approach that utilizes conﬁdence
scores, which describe the quality of demonstrations. More speciﬁcally, we propose two
conﬁdence-based IL methods, namely two-step importance weighting IL (2IWIL) and genera-
tive adversarial IL with imperfect demonstration and conﬁdence (IC-GAIL). We show that
conﬁdence scores given only to a small portion of sub-optimal demonstrations signiﬁcantly
improve the performance of IL both theoretically and empirically.

1

Introduction

Imitation learning (IL) has become of great interest because obtaining demonstrations is usually
easier than designing reward. Reward is a signal to instruct agents to complete the desired tasks.
However, ill-designed reward functions usually lead to unexpected behaviors [Amodei et al.,
2016; Dewey, 2014; Everitt and Hutter, 2016]. There are two main approaches that can be used
to solve IL: behavioral cloning (BC) [Schaal, 1999], which adopts supervised learning approaches
to learn an action predictor that is trained directly from demonstration data; and apprenticeship
learning (AL), which attempts to ﬁnd a policy that is better than the expert policy for a class of
cost functions [Abbeel and Ng, 2004]. Even though BC can be trained with supervised learning
approaches directly, it has been shown that BC cannot imitate the expert policy without a
large amount of demonstration data for not considering the transition of environments [Ross
et al., 2011]. In contrast, AL approaches learn from interacting with environments and optimize
objectives such as maximum entropy [Ziebart et al., 2008].

A state-of-the-art approach generative adversarial imitation learning (GAIL) is proposed by
Ho and Ermon [2016]. The method learns an optimal policy by performing occupancy measure
matching [Syed et al., 2008]. An advantage of the matching method is that it is robust to
demonstrations generated from a stochastic policy. Based on the concept proposed in GAIL,
variants have been developed recently for diﬀerent problem settings [Kostrikov et al., 2019; Li
et al., 2017].

Despite that GAIL is able to learn an optimal policy from optimal demonstrations, to apply
IL approaches to solve real-world tasks, the diﬃculty in obtaining such demonstration data
should be taken into consideration. However, demonstrations from an optimal policy (either
deterministic or stochastic) are usually assumed to be available in the above mentioned works,
which can be barely fulﬁlled by the fact that most of the accessible demonstrations are imperfect
or even from diﬀerent policies. For instance, to train an agent to play basketball with game-play

1

videos of the National Basketball Association, we should be aware that there are 14.3 turnovers
per game1, not to mention other kinds of mistakes that may not be recorded. The reason why
optimal demonstrations are hard to obtain can be attributed to the limited attention and the
presence of distractions, which make humans hard to follow optimal policies all the time. As a
result, some parts of the demonstrations may be optimal and the others are not.

To mitigate the above problem, we propose to use conﬁdence scores, which indicate the
probability that whether a given trajectory is optimal. In practice, obtaining conﬁdence scores can
be cheaper than collecting optimal demonstrations. It is because it requires merely the knowledge
of the optimal behavior to score but performing optimally requires not only such knowledge but
also strict physical conditions. For instance, to play basketball well, the capabilities of making
spontaneous decisions and intrinsic ﬁngertip control are required. Therefore, for real-world tasks,
the conﬁdence labelers are not necessarily expert at achieving the goal. They can be normal
enthusiasts such as audiences of basketball games.

To further reduce the additional cost to learn an optimal policy, we consider a more realistic
setting that the given demonstrations are partially equipped with conﬁdence. As a result, the goal
of this work is to utilize imperfect demonstrations where some are equipped with conﬁdence while
some are not (we refer to demonstrations without conﬁdence as “unlabeled demonstrations”).
In this work, we consider the setting where the given imperfect demonstrations are a mixture
of optimal and non-optimal demonstrations. The setting is common when the demonstrations
are collected via crowdsourcing [Hu et al., 2018; Serban et al., 2017; Shah et al., 2018] and
learning from diﬀerent sources such as videos [Liu et al., 2018; Pathak et al., 2017; Supancic III
and Ramanan, 2017; Tokmakov et al., 2017; Yeung et al., 2017], where demonstrations can be
generated from diﬀerent policies.

We propose two methods, two-step importance weighting imitation learning (2IWIL) and
generative adversarial imitation learning with imperfect demonstration and conﬁdence (IC-GAIL),
based on the idea of reweighting but from diﬀerent perspectives. To utilize both conﬁdence
and unlabeled data, for 2IWIL, it predicts conﬁdence scores for unlabeled data by optimizing
the proposed objective based on empirical risk minimization (ERM) [Vapnik, 1998], which has
ﬂexibility for diﬀerent loss functions, models, and optimizers; on the other hand, instead of
directly reweighting to the optimal distribution and perform GAIL with reweighting, IC-GAIL
reweights to the non-optimal distribution and match the optimal occupancy measure based
on our mixture distribution setting. Since the derived objective of IC-GAIL depends on the
proportion of the optimal demonstration in the demonstration mixture, we empirically show
that IC-GAIL converges slower than 2IWIL but achieves better performance, which forms a
trade-oﬀ between the two methods. We show that the proposed methods are both theoretically
and practically sound.

2 Related work

In this section, we provide a brief survey about making use of non-optimal demonstrations and
semi-supervised classiﬁcation with conﬁdence data.

2.1 Learning from non-optimal demonstrations

Learning from non-optimal demonstrations is nothing new in IL and reinforcement learning (RL)
literature, but previous works utilized diﬀerent information to learn a better policy. Distance
minimization inverse RL (DM-IRL) [Burchﬁel et al., 2016] utilized a feature function of states
and assumed that the true reward function is linear in the features. The feedback from human is

1https://www.basketball-reference.com/leagues/NBA_stats.html

2

an estimate of accumulated reward, which is harder to be given than conﬁdence because multiple
reward functions may correspond to the same optimal policy.

Semi-supervised IRL (SSIRL) [Valko et al., 2012] extends the IRL method proposed by Abbeel
and Ng [2004], where the reward function can be learned by matching the feature expectations of
the optimal demonstrations. The diﬀerence from Abbeel and Ng [2004] is that in SSIRL, optimal
and sub-optimal trajectories from other performers are given. Transductive SVM [Sch¨olkopf
et al., 1999] was used in place of vanilla SVM in Abbeel and Ng [2004] to recognize optimal
trajectories in the sub-optimal ones. In our setting, the conﬁdence scores are given instead of
the optimal demonstrations. DM-IRL and SSIRL are not suitable for high-dimensional problems
due to its dependence on the linearity of reward functions and good feature engineering.

2.2 Semi-supervised classiﬁcation with conﬁdence data

In our 2IWIL method, we train a probabilistic classiﬁer with conﬁdence and unlabeled data
by optimizing the proposed ERM objective. There are similar settings such as semi-supervised
classiﬁcation [Chapelle et al., 2006], where few hard-labeled data y ∈ {0, 1} and some unlabeled
data are given.

Zhou et al. [2014] proposed to use hard-labeled instances to estimate conﬁdence scores for
unlabeled samples using Gaussian mixture models and principal component analysis. Similarly,
for an input instance x, Wang et al. [2013] obtained an upper bound of conﬁdence Pr(y = 1|x)
with hard-labeled instances and a kernel density estimator, then treated the upper bound as an
estimate of probabilistic class labels.

Another related scheme was considered in El-Zahhar and El-Gayar [2010] where they consid-
ered soft labels z ∈ [0, 1] as fuzzy inputs and proposed a classiﬁcation approach based on k-nearest
neighbors. This method is diﬃcult to scale to high-dimensional tasks, and lacks theoretical
guarantees. Ishida et al. [2018] proposed another scheme that trains a classiﬁer only from positive
data equipped with conﬁdence. Our proposed method, 2IWIL, also considers training a classiﬁer
with conﬁdence scores of given demonstrations. Nevertheless, 2IWIL can train a classiﬁer from
fewer conﬁdence data, with the aid of a large number of unlabeled data.

3 Background

In this section, we provide backgrounds of RL and GAIL.

3.1 Reinforcement Learning

We consider the standard Markov Decision Process (MDP) [Sutton and Barto, 1998]. MDP
is represented by a tuple hS, A, P, R, γi, where S is the state space, A is the action space,
P(st+1|st, at) is the transition density of state st+1 at time step t + 1 given action at made under
state st at time step t, R(s, a) is the reward function, and γ ∈ (0, 1) is the discount factor.

A stochastic policy π(a|s) is a density of action a given state s. The performance of π is
evaluated in the γ-discounted inﬁnite horizon setting and its expectation can be represented
with respect to the trajectories generated by π:

Eπ[R(s, a)] = E

γtR(st, at)

,

#

(1)

where the expectation on the right-hand side is taken over the densities p0(s0), P(st+1|st, at),
and π(at|st) for all time steps t. Reinforcement learning algorithms [Sutton and Barto, 1998]
aim to maximize Eq. (1) with respect to π.

" ∞
X

t=0

3

To characterize the distribution of state-action pairs generated by an arbitrary policy π, the

occupancy measure is deﬁned as follows.

Deﬁnition 3.1 (Puterman [1994]). Deﬁne occupancy measure ρπ : S × A → R,

ρπ(s, a) = π(a|s)

γt Pr(st = s|π),

(2)

∞
X

t=0

where Pr(st = s|π) is the probability density of state s at time step t following policy π.

The occupancy measure of π, ρπ(s, a), can be interpreted as an unnormalized density of
state-action pairs. The occupancy measure plays an important role in IL literature because of
the following one-to-one correspondence with the policy.

Theorem 3.2. (Theorem 2 of Syed et al. [2008]) Suppose ρ is the occupancy measure for
πρ(a|s) (cid:44) ρ(s,a)

. Then πρ is the only policy whose occupancy measure is ρ.

P

a0 ρ(s,a0)

In this work, we also deﬁne the normalized occupancy measure p(s, a),

p(s, a) (cid:44) ρ(s, a)
P

s,a ρ(s, a)

=

ρ(s, a)
t=0 γt Pr(st = s|π)

P

s,a π(a|s) P∞
ρ(s, a)
t=0 γt = (1 − γ)ρ(s, a).
P∞
The normalized occupancy measure can be interpreted as a probability density of state-action
pairs that an agent experiences in the environment with policy π.

=

3.2 Generative adversarial imitation learning (GAIL)

The problem setting of IL is that given trajectories {(si, ai)}n
are interested in optimizing the agent policy πθ to recover the expert policy πE with {(si, ai)}n
and the MDP tuple without reward function R.

i=1 generated by an expert πE, we
i=1

GAIL [Ho and Ermon, 2016] is a state-of-the-art IL method that performs occupancy measure
matching to learn a parameterized policy. Occupancy measure matching aims to minimize the
objective d(ρπE, ρπθ ), where d is a distance function. The key idea behind GAIL is that it uses
generative adversarial training to estimate the distance and minimize it alternatively. To be
precise, the distance is the Jensen-Shannon divergence (JSD), which is estimated by solving a
binary classiﬁcation problem. This leads to the following min-max optimization problem:

min
θ

max
w

Es,a∼pθ [log Dw(s, a)] + Es,a∼popt[log(1 − Dw(s, a))],

(3)

where pθ and popt are the corresponding normalized occupancy measures for πθ and πopt
respectively. Dw is called a discriminator and it can be shown that if the discriminator has
inﬁnite capacity, the global optimum of Eq. (3) corresponds to the JSD up to a constant
[Goodfellow et al., 2014]. To update the agent policy πθ, GAIL treats the loss − log(Dw(s, a))
as a reward signal and the agent can be updated with RL methods such as trust region
policy optimization (TRPO) [Schulman et al., 2015]. A weakness of GAIL is that if the given
demonstrations are non-optimal then the learned policy will be non-optimal as well.

4

4

Imitation learning with conﬁdence and unlabeled data

In this section, we present two approaches to learning from imperfect demonstrations with
conﬁdence and unlabeled data. The ﬁrst approach is 2IWIL, which aims to learn a probabilistic
classiﬁer to predict conﬁdence scores of unlabeled demonstration data and then performs standard
GAIL with reweighted distribution. The second approach is IC-GAIL, which forgoes learning a
classiﬁer and learns an optimal policy by performing occupancy measure matching with unlabeled
demonstration data. Details of derivation and proofs in this section can be found in Appendix.

4.1 Problem setting

Firstly, we formalize the problem setting considered in this paper. For conciseness, in what
follows we use x in place of (s, a). Consider the case where given imperfect demonstrations
are sampled an optimal policy πopt and non-optimal policies Π = {πi}n
i=1. Denote that the
corresponding normalized occupancy measure of πopt and Π are popt and {pi}n
i=1, respectively.
The normalized occupancy measure p(x) of a state-action pair x is therefore the weighted sum
of popt and {pi}n

i=1,

p(x) =αpopt(x) +

νipi(x)

n
X

i=1
=αpopt(x) + (1 − α)pnon(x),

Pn

(1−α)

i=1 νi = 1 and pnon(x) = 1

where α + Pn
i=1 νipi(x). We may further follow traditional
classiﬁcation notation by deﬁning popt(x) (cid:44) p(x|y = +1) and pnon(x) (cid:44) p(x|y = −1), where
y = +1 indicates that x is drawn from the occupancy measure of the optimal policy and y = −1
indicates the non-optimal policies. Here, α = Pr(y = +1) is the class-prior probability of the
optimal policy. We further assume that an oracle labels state-action pairs in the demonstration
data with conﬁdence scores r(x) (cid:44) p(y = +1|x). Based on this, the normalized occupancy
measure of the optimal policy can be expressed by the Bayes’ rule as

p(x|y = +1) =

r(x)p(x)
α

.

(4)

We assume that labeling state-action pairs by the oracle can be costly and only some pairs are
labeled with conﬁdence. More precisely, we obtain demonstration datasets as follows,

Dc (cid:44) {(xc,i, ri)}nc
i=1
i.i.d.∼ p(x),
Du (cid:44) {xu,i}nu
i=1

i.i.d.∼ q(x, r),

where q(x, r) = p(x)pr(r|x) and pr(ri|x) = δ(ri − r(x)) is a delta distribution. Our goal is to
consider the case where Dc is scarce and we want to learn the optimal policy πopt with Dc and
Du jointly.

4.2 Two-step importance weighting imitation learning

We ﬁrst propose an approach based on the importance sampling scheme. By Eq. (4), the GAIL
objective in Eq. (3) can be rewritten as follows:

min
θ

max
w

Ex∼pθ [log Dw(x)] + Ex,r∼q

(cid:21)
log(1 − Dw(x))

.

(cid:20) r
α

(5)

5

In practice, we may use the mean of conﬁdence scores to estimate the class prior α. Although
we can reweight the conﬁdence data Dc to match the optimal distribution, we have a limited
number of conﬁdence data and it is diﬃcult to perform accurate sample estimation. To make
full use of unlabeled data, the key idea is to identify conﬁdence scores of the given unlabeled
data Du and reweight both conﬁdence data and unlabeled data. To achieve this, we train a
probabilistic classiﬁer from conﬁdence data and unlabeled data, where we call this learning
problem semi-conf (SC) classiﬁcation.

Let us ﬁrst consider a standard binary classiﬁcation problem to classify samples into popt
(y = +1) and pnon (y = −1). Let g : Rd → R be a prediction function and ‘ : R → R+ be a loss
function. The optimal classiﬁer can be learned by minimizing the following risk:

RPN,‘(g) = αEx∼popt [‘(g(x))] + (1 − α)Ex∼pnon [‘(−g(x))] ,

(6)

where PN stands for “positive-negative”. However, as we only have samples from the mixture
distribution p instead of samples separately drawn from popt and pnon, it is not straightforward
to conduct sample estimation of the risk in Eq. (6). To overcome this issue, we express the risk
in an alternative way that can be estimated only from Dc and Du in the following theorem.

Theorem 4.1. The classiﬁcation risk (6) can be equivalently expressed as

RSC,‘(g) = Ex,r∼q[r(‘(g(x)) − ‘(−g(x))) + (1 − β)‘(−g(x))] + Ex∼p[β‘(−g(x))],

(7)

where β ∈ [0, 1] is an arbitrary weight.

Thus, we can obtain a probabilistic classiﬁer by minimizing Eq. (7), which can be estimated
only with Dc and Du. Once we obtain the prediction function g, we can use it to give conﬁdence
scores for Du.

To make the prediction function g estimate conﬁdence accurately, the loss function ‘ in
Eq. (7) should come from a class of strictly proper composite loss [Buja et al., 2005; Reid and
Williamson, 2010]. Many losses such as the squared loss, logistic loss, and exponential loss
are proper composite. For example, if we obtain g∗
log that minimizes a logistic loss ‘log(z) =
(log(1 + exp(−z)), we can obtain conﬁdence scores by passing prediction outputs to a sigmoid
function bp(y = 1|x) = [1 + exp(−g∗
log(x))]−1 [Reid and Williamson, 2010]. On the other hand,
the hinge loss cannot be applied since it is not a proper composite loss and cannot estimate
conﬁdence reliably [Bartlett and Tewari, 2007; Reid and Williamson, 2010]. Therefore, we can
obtain a probabilistic classiﬁer from the prediction function g that learned from a strictly proper
composite loss. After obtaining a probabilistic classiﬁer, we optimize the importance weighted
objective in Eq. (5), where both Dc and Du are used to estimate the second expectation. We
summarize this training procedure in Algorithm 1.

Next, we discuss the choice of the combination coeﬃcient β. Since we have access to the
empirical unbiased estimator bRSC,‘(g) from Eq. (7), it is natural to ﬁnd the minimum variance
estimator among them. The following theorem gives the optimal β in terms of the estimator
variance.

Proposition 4.2 (variance minimality). Let σcov denote the covariance between n−1
‘(−g(xc,i))} and n−1
variance when β = clip[0,1]( nu

i=1 ‘(−g(xc,i)). For a ﬁxed g, the estimator bRSC,‘(g) has the minimum

σcov
Var(‘(−g(x)))

Pnc

ncnu
nc+nu

nc+nu

).2

+

c

c

Pnc

i=1 ri{‘(g(xc,i))−

Thus, β lies in (0, 1) when the covariance σcov is not so large. If β 6= 0, it means that the
unlabeled data Du does help the classiﬁer by reducing empirical variance when Eq. (7) is adopted.

2clip[l,u](v) (cid:44) max{l, min{v, u}}.

6

Algorithm 1 2IWIL
1: Input: Expert trajectories and conﬁdence Dc = {(xc,i, ri)}nc
2: Estimate the class prior by bα = 1
nc
3: Train a probabilistic classiﬁer by minimizing Eq. (7) with β = nu
4: Predict conﬁdence scores {bru,i}nu
5: for i = 0, 1, 2, ... do
6:

Sample trajectories {xi}na
Update the discriminator parameters by maximizing Eq. (5)
Update πθ with reward − log Dw(x) using TRPO

i=1 for {xu,i}nu
i=1

i=1 ∼ πθ

i=1 ri

Pnc

8:

7:

nu+nc

i=1, Du = {xu,i}nu
i=1

9: end for

However, computing the β that minimizes empirical variance is computationally ineﬃcient since
it involves computing σcov and Var(l(−g(x))). In practice, we use β = nu
for all experiments
by assuming that the covariance is small enough.

nc+nu

In our preliminary experiments, we sometimes observed that the empirical estimate bRSC,‘ of
Eq. (7) became negative and led to overﬁtting. We can mitigate this phenomenon by employing
a simple yet highly eﬀective technique from Kiryo et al. [2017], which is proposed to solve a
similar overﬁtting problem (see Appendix for implementation details).

4.2.1 Theoretical Analysis

Below, we show that the estimation error of Eq. (7) can be bounded. This means that its
minimizer is asymptotically equivalent to the minimizer of the standard classiﬁcation risk RPN,‘,
which provides a consistent estimator of p(y = +1|x). We provide the estimation error bound
with Rademacher complexity [Bartlett and Mendelson, 2002]. Denote Rn(G) be the Rademacher
complexity of the function class G with the sample size n.

Theorem 4.3. Let G be the hypothesis class we use. Assume that the loss function ‘ is ρ‘-Lipschitz
continuous, and that there exists a constant C‘ > 0 such that supx∈X ,y∈{±1} |‘(yg(x))| ≤ C‘ for
any g ∈ G. Let bg (cid:44) arg min
RSC,‘(g). For δ ∈ (0, 1), with probability
at least 1 − δ over repeated sampling of data for training bg,

bRSC,‘(g) and g∗ (cid:44) arg min

g∈G

g∈G

RSC,‘(bg) − RSC,‘(g∗) ≤ 16ρ‘((3 − β)Rnc(G) + βRnu(G)) + 4C‘

s

(cid:18)

log(8/δ)
2

(3 − β)n

− 1
− 1
c + βn
2
2
u

(cid:19)

.

Thus, we may safely obtain a probabilistic classiﬁer by minimizing bRSC,‘, which gives a

consistent estimator.

4.3 IC-GAIL

Since 2IWIL is a two-step approach by ﬁrst gathering more conﬁdence data and then conducting
importance sampling, the error may accumulate over two steps and degrade the performance.
Therefore, we propose IC-GAIL that can be trained in an end-to-end fashion and perform
occupancy measure matching with the optimal normalized occupancy measure popt directly.

Recall that p = αpopt +(1−α)pnon. Our key idea here is to minimize the divergence between p
and p0, where p0 = αpθ + (1 − α)pnon. Intuitively, the divergence between pθ and popt is minimized
if that between p and p0 is minimized. For Jensen-Shannon divergence, this intuition can be
justiﬁed in the following theorem.

7

Theorem 4.4. Denote that

V (πθ, Dw) = Ex∼p[log(1 − Dw(x))] + Ex∼p0[log Dw(x)],

and that C(πθ) = maxw V (πθ, Dw). Then, V (πθ, Dw) is maximized when Dw = p0
p+p0 ((cid:44) Dw∗),
and its maximum value is C(πθ) = − log 4 + 2JSD(pkp0). Thus, C(πθ) is minimized if and only
if pθ = popt almost everywhere.

Theorem 4.4 implies that the optimal policy can be found by solving the following objective,

min
θ

max
w

Ex∼p[log(1 − Dw(x))] + Ex∼p0[log Dw(x)].

(8)

The expectation in the ﬁrst term can be approximated from Du, while the expectation in the
second term is the weighted sum of the expectation over pθ and pnon. Data Da = {xa,i}na
i
sampled from pθ can be obtained by executing the current policy πθ. However, we cannot directly
obtain samples from pnon since it is unknown. To overcome this issue, we establish the following
theorem.

Theorem 4.5. V (πθ, Dw) can be transformed to eV (πθ, Dw), which is deﬁned as follows:
eV (πθ, Dw) = Ex∼p[log(1 − Dw(x))] + αEx∼pθ [log Dw(x)] + Ex,r∼q[(1 − r) log Dw(x)].
We can approximate Eq. (9) given ﬁnite samples Dc, Du, and Da. In practice, we perform
alternative gradient descent with respect to θ and w to solve this optimization problem. Below,
we show that the estimation error of eV can be bounded for a ﬁxed agent policy πθ.

(9)

4.3.1 Theoretical analysis

In this subsection, we show that the estimation error of Eq. (9) can be bounded, given a ﬁxed
agent policy πθ. Let bV (πθ, Dw) be the empirical estimate of Eq. (9).
Theorem 4.6. Let W be a parameter space for training the discriminator and DW (cid:44) {Dw | w ∈
W} be its hypothesis space. Assume that there exist a constant CL > 0 such that | log Dw(x)| ≤ CL
and | log(1 − Dw(x))| ≤ CL for any x ∈ X and w ∈ W. Assume that both log Dw(x) and
log(1 − Dw(x)) for any w ∈ W have Lipschitz norms no more than ρL > 0. For a ﬁxed
agent policy πθ, let {xa,i}na
bV (πθ, Dw), and

i=1 be a sample generated from πθ, D

(cid:44) arg max
w∈W

bw

V (πθ, Dw). Then, for δ ∈ (0, 1), the following holds with probability at least

Dw∗ (cid:44) arg max

w∈W

1 − δ:

V (πθ, Dw∗) − V (πθ, D

bw) ≤ 16ρL(Rnu(DW ) + αRna(DW ) + Rnc(DW )) + 4CL
Theorem 4.6 guarantees that the estimation of Eq. (9) provides a consistent maximizer with

n

respect to the original objective in Eq. (8) at each step of the discriminator training.

s

(cid:18)

log(6/δ)
2

− 1
u + αn
2

− 1
a + n
2

− 1
2
c

(cid:19)

.

4.3.2 Practical implementation of IC-GAIL

Even though Eq. (9) is theoretically supported, when the class prior α is low, the inﬂuence of the
agent become marginal in the discriminator training. This issue can be mitigated by thresholding
α in Eq. (9) as follows:

min
θ

max
w

Ex∼p[log(1 − Dw(x))] + λEx∼pθ [log Dw(x)] + (1 − λ)Ex,r∼q

(cid:20) (1 − r)
(1 − α)

(cid:21)
log Dw(x)

,

(10)

where λ = max{τ, α} and τ ∈ (0, 1]. The training procedure of IC-GAIL is summarized in
Algorithm 2. Note that Eq. (10) returns to Eq. (3) and learns an sub-optimal policy when τ = 1.

8

Algorithm 2 IC-GAIL
1: Input: Expert trajectories, conﬁdence, and weight threshold {xu,i}nu
2: Estimate the class prior by bα = 1
nc
3: λ = max{τ, bα}
4: for i = 0, 1, 2, ... do
5:

Sample trajectories {xi}na
Update the discriminator parameters by maximizing Eq. (10)
Update πθ with reward − log Dw(x) using TRPO

i=1 ∼ πθ

i=1 ri

Pnc

7:

6:

i=1, {(xc,i, ri)}nc

i=1, τ

8: end for

4.4 Discussion

To understand the diﬀerence between 2IWIL and IC-GAIL, we discuss it from three diﬀerent
perspectives: unlabeled data, conﬁdence data, and the class prior.

Role of unlabeled data: It should be noted that unlabeled data plays diﬀerent roles in
the two methods. In 2IWIL, we show that unlabeled data reduces the variance of the empirical
risk estimator as shown in Proposition 4.2.

On the other hand, in addition to making more accurate estimation, the usefulness of
unlabeled data in IC-GAIL is similar to guided exploration [Kang et al., 2018]. We may analogize
conﬁdence information in the imperfect demonstration setting to reward functions since both
of them allow agents to learn an optimal policy in IL and RL, respectively. Likewise, fewer
conﬁdence data can be analogous to sparse reward functions. Even though a small number of
conﬁdence data and sparse reward functions do not make objective such as Eqs. (1) and (5)
biased, they cause practical issues such as a deﬁciency in information for exploration. To mitigate
the problem, we imitate from sub-optimal demonstrations and use conﬁdence information to
reﬁne the learned policy, which is similar to Kang et al. [2018] in the sense that they imitate a
sub-optimal policy to guide RL algorithms in the sparse reward setting.

Role of conﬁdence data: Conﬁdence data is utilized to train a classiﬁer and to reweight
popt in 2IWIL, which causes the two-step training scheme and therefore the error is accumulated
in the prediction phase and the occupancy measure matching phase. Diﬀerently, IC-GAIL
instead compensates the pnon portion in the given imperfect demonstrations by mimicking the
composition of p. The advantage of IC-GAIL over 2IWIL is that it avoids the prediction error
by employing an end-to-end training scheme.

Inﬂuence of the class-prior α: The class prior in 2IWIL as shown in Eq. (5) serves as a
normalizing constant so that the weight r(x)
α for reweighting p to popt has unit mean. Consequently,
the class prior α does not aﬀect the convergence of the agent policy. On the other hand, the
term with respect to the agent pθ is directly scaled by α in Eq. (9) of IC-GAIL. To comprehend
the inﬂuence, we may expand the reward function from the discriminator − log D∗
w(x) =
− log
and it shows that the agent term is scaled
by
(1−α) , which makes the reward function prone to be a constant when α is small. Therefore
the agent learns slower than in 2IWIL, where the reward function is − log (pθ/(pθ + popt)).

(1−α) (popt + pθ) + 2pnon

(1−α) pθ + pnon

(cid:16)(cid:16) α

(cid:16) α

(cid:17)(cid:17)

/

(cid:17)

α

5 Experiments

In this section, we aim to answer the following questions with experiments. (1) Do 2IWIL and
IC-GAIL methods allow agents to learn near-optimal policies when limited conﬁdence information
is given? (2) Are the methods robust enough when the given conﬁdence is less accurate? and (3)
Do more unlabeled data results in better performance in terms of average return? The discussions

9

Figure 1: Learning curves of our 2IWIL and IC-GAIL versus baselines given imperfect demon-
strations. The x-axis is the number of training iterations and the shaded area indicates standard
error.

Table 1: Comparison between proposed methods (IC-GAIL and 2IWIL) and baselines.

Method

IC-GAIL
2IWIL

GAIL (U+C)
GAIL (C)
GAIL (reweight)

Input

objective

Du ∪ Dc
Du ∪ Dc
Du ∪ Dx
c
Dx
c
Dc

Eq. (9)
Eq. (7)
Eq. (3)
Eq. (3)
Eq. (5)

are given in Sec. 5.1, 5.2, and 5.3 respectively.

Setup To collect demonstration data, we train an optimal policy (πopt) using TRPO [Schul-
man et al., 2015] and select two intermediate policies (π1 and π2). The three policies are used to
generate the same number of state-action pairs. In real-world tasks, the conﬁdence should be
given by human labelers. We simulate such labelers by using a probabilistic classiﬁer p?(y = +1|x)
pre-trained with demonstration data and randomly choose 20% of demonstration data to label
conﬁdence scores r(x) = p?(y = +1|x).

(cid:44) {xc,i}nc

i=1, and Dx
u

We compare the proposed methods against three baselines. Denote that Dx
c
(cid:44) {ri}nc

i=1,
(cid:44) Du. GAIL (U+C) takes all the pairs as input without considering
Dr
c
conﬁdence. To show if reweighting using Eq. (5) makes diﬀerence, GAIL (C) and GAIL (Reweight)
use the same state-action pairs Dx
c but GAIL (Reweight) additionally utilizes reweighting with
conﬁdence information Dr
c . The baselines and the proposed methods are summarized in Table 1.
To assess our methods, we conduct experiments on Mujoco [Todorov et al., 2012]. Each
experiment is performed with ﬁve random seeds. The hyper-parameter τ of IC-GAIL is set to
0.7 for all tasks. To show the performance with respect to the optimal policy that we try to
imitate, the accumulative reward is normalized with that of the optimal policy and a uniform
random policy so that 1.0 indicates the optimal policy and 0.0 the random one. Due to space
limit, we defer implementation details, the performance of the optimal and the random policies,
the speciﬁcation of each task, and the uncropped ﬁgures of Ant-v2 to Appendix.

5.1 Performance comparison

The average return against training iterations in Fig. 1 shows that the proposed IC-GAIL and
2IWIL outperform other baselines by a large margin. Due to the mentioned experiment setup, the
class prior of the optimal demonstration distribution is around 33%. To interpret the experiment
results, we would like to emphasize that our experiments are under incomplete optimality setting

10

Figure 2: Learning curves of proposed methods with diﬀerent standard deviations of Gaussian
noise added to conﬁdence. The numbers in the legend indicate the standard deviation of the
Gaussian noise.

such that conﬁdence itself is not enough to learn the optimal policy as indicated by the GAIL
(Reweight) baseline. Since the diﬃculty of each task varies, we use diﬀerent number of nc + nu for
diﬀerent tasks. Our contribution is that in addition to the conﬁdence, our methods are able to
utilize the demonstration mixture (sub-optimal demonstration) and learn near-optimal policies.
We can observe that IC-GAIL converges slower than 2IWIL. As discussed in Section 4.4,
it can be attributed to that the term with respect to the agent in Eq. (10) is scaled by 0.7
as speciﬁed by τ , which decreases the inﬂuence of the agent policy in updating discriminator.
The faster convergence of 2IWIL can be an advantage over IC-GAIL when interactions with
environments are expensive. Even though the objective of IC-GAIL becomes biased by not using
the class prior α, it still converges to near-optimal policies in four tasks.

In Walker2d-v2, the improvement in performance of our methods is not as signiﬁcant as
in other tasks. We conjecture that it is caused by the insuﬃciency of conﬁdence information.
This can be veriﬁed by observing that the GAIL (Reweight) baseline in Walker2d-v2 gradually
converges to 0.2 whereas in other tasks it achieves the performance of at least 0.4. In HalfCheetah-
v2, we observe that the discriminator is stuck in a local maximum in the middle of learning,
which inﬂuences all methods signiﬁcantly.

The baseline GAIL (Reweight) surpasses GAIL (C) in all tasks, which shows that reweighting
enables the agent to learn policies that obtain higher average return. However, since the number
of conﬁdence instances is small, the information is not enough to derive the optimal policies.
GAIL (U+C) is the standard GAIL without considering conﬁdence information. Although
the baseline uses the same number of demonstrations nc + nu as our proposed methods, the
performance diﬀerence is signiﬁcant due to the use of conﬁdence.

5.2 Robustness to Gaussian noise in conﬁdence

In practice, the oracle that gives conﬁdence scores is basically human labelers and they may not
be able to accurately label conﬁdence all the time. To investigate robustness of our approaches
against noise in the conﬁdence scores, we further conduct an experiment on Ant-v2 where
the Gaussian noise is added to conﬁdence scores as follows: r(x) = p?(y = 1|x) + (cid:15), where
(cid:15) ∼ N (0, σ2). Fig. 2 shows the performance of our methods in this noisy conﬁdence scenario. It
reveals that both methods are quite robust to noisy conﬁdence, which suggests that the proposed
methods are robust enough to human labelers, who may not always correctly assign conﬁdence
scores.

11

Figure 3: Learning curves of the proposed methods with diﬀerent number of unlabeled data. The
numbers in the legend suggest the proportion of unlabeled data used as demonstrations. 1.0 is
the same as the data used in Fig. 1.

5.3 Inﬂuence of unlabeled data

In this experiment, we would like to evaluate the performance of both 2IWIL and IC-GAIL with
diﬀerent numbers of unlabeled data to verify whether unlabeled data is useful. As we can see in
Fig. 3, the performance of both methods grows as the number of unlabeled data increases, which
conﬁrms our motivation that using unlabeled data can improve the performance of imitation
learning when conﬁdence data is scarce. As discussed in Sec. 4.4, the diﬀerent roles of unlabeled
data in the two proposed methods result in dissimilar learning curves with respect to unlabeled
data.

6 Conclusion

In this work, we proposed two general approaches IC-GAIL and 2IWIL, which allow the agent to
utilize both conﬁdence and unlabeled data in imitation learning. The setting considered in this
paper is usually the case in real-world scenarios because collecting optimal demonstrations is
normally costly. In 2IWIL, we utilized unlabeled data to derive a risk estimator and obtained the
minimum variance with respect to the combination coeﬃcient β. 2IWIL predicts conﬁdence scores
for unlabeled data and matches the optimal occupancy measure based on the GAIL objective
with importance sampling. For IC-GAIL, we showed that the agent learns an optimal policy
by matching a mixture of normalized occupancy measures p0 with the normalized occupancy
measure of the given demonstrations p.

Practically, we conducted extensive experiments to show that our methods outperform
baselines by a large margin, to conﬁrm that our methods are robust to noise, and to verify that
unlabeled data has a positive correlation with the performance. The proposed approaches are
general and can be easily extended to other IL and IRL methods [Fu et al., 2018; Kostrikov
et al., 2019; Li et al., 2017].

For future work, we may extend it to a variety of applications such as discrete sequence
generation because the conﬁdence in our work can be treated as a property indicator. For
instance, to generate soluble chemicals, we may not have enough soluble chemicals, whereas
the Crippen function [Crippen and Snow, 1990] can be used to evaluate the solubility as the
conﬁdence in this work easily.

12

Acknowledgement

Bibliography

ICML, pages 1–8, 2004.

We thank Zhenghang Cui for helpful discussion. MS was supported by KAKENHI 17H00757, NC
was supported by MEXT scholarship, and HB was supported by JST, ACT-I, Grant Number
JPMJPR18UI, Japan.

Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e.

Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.

Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research, 3:463–482, 2002.

Peter L Bartlett and Ambuj Tewari. Sparseness vs estimating conditional probabilities: Some

asymptotic results. Journal of Machine Learning Research, 8:775–790, 2007.

Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability

estimation and classiﬁcation: Structure and applications. Technical report, 2005.

Benjamin Burchﬁel, Carlo Tomasi, and Ronald Parr. Distance minimization for reward learning

from scored trajectories. In AAAI, pages 3330–3336, 2016.

Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-Supervised Learning. MIT

press, 2006.

Gordon M Crippen and Mark E Snow. A 1.8 ˚a resolution potential function for protein folding.

Biopolymers: Original Research on Biomolecules, 29(10-11):1479–1489, 1990.

Daniel Dewey. Reinforcement learning and the reward engineering principle. In AAAI Spring

Symposium Series, 2014.

Mohamed M El-Zahhar and Neamat F El-Gayar. A semi-supervised learning approach for soft
labeled data. In International Conference on Intelligent Systems Design and Applications,
pages 1136–1141, 2010.

Tom Everitt and Marcus Hutter. Avoiding wireheading with value reinforcement learning. pages

12–22, 2016.

Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse

reinforcement learning. In ICLR, 2018.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, pages
2672–2680, 2014.

Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NeurIPS, pages

4565–4573, 2016.

Zehong Hu, Yitao Liang, Jie Zhang, Zhao Li, and Yang Liu. Inference aided reinforcement
learning for incentive mechanism design in crowdsourcing. In NeurIPS, pages 5508–5518, 2018.

13

Takashi Ishida, Gang Niu, and Masashi Sugiyama. Binary classiﬁcation from positive-conﬁdence

data. In NeurIPS, pages 5919–5930, 2018.

Bingyi Kang, Zequn Jie, and Jiashi Feng. Policy optimization with demonstrations. In ICML,

pages 2474–2483, 2018.

Ryuichi Kiryo, Gang Niu, Marthinus C du Plessis, and Masashi Sugiyama. Positive-unlabeled

learning with non-negative risk estimator. In NeurIPS, pages 1675–1685, 2017.

Ilya Kostrikov, Kumar Krishna Agrawal, Sergey Levine, and Jonathan Tompson. Addressing

sample ineﬃciency and reward bias in inverse reinforcement learning. In ICLR, 2019.

Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.

Springer, 1991.

Yunzhu Li, Jiaming Song, and Stefano Ermon. InfoGAIL: Interpretable imitation learning from

visual demonstrations. In NeurIPS, pages 3812–3822, 2017.

YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation:
In ICRA, pages

Learning to imitate behaviors from raw video via context translation.
1118–1125, 2018.

Colin McDiarmid. On the method of bounded diﬀerences. Surveys in Combinatorics, 141(1):

148–188, 1989.

Deepak Pathak, Ross B Girshick, Piotr Doll´ar, Trevor Darrell, and Bharath Hariharan. Learning

features by watching objects move. In CVPR, 2017.

Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0471619779.

Mark D Reid and Robert C Williamson. Composite binary losses. Journal of Machine Learning

Research, 11:2387–2422, 2010.

St´ephane Ross, Geoﬀrey Gordon, and Drew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proceedings of the fourteenth international
conference on artiﬁcial intelligence and statistics, pages 627–635, 2011.

Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in Cognitive Sciences,

3(6):233–242, 1999.

Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola. Advances in Kernel

Methods: Support Vector Learning. MIT press, 1999.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region

policy optimization. In ICML, pages 1889–1897, 2015.

Iulian V Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep
Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, et al. A deep
reinforcement learning chatbot. CoRR, abs/1709.02349, 2017.

Pararth Shah, Dilek Hakkani-Tur, Bing Liu, and Gokhan Tur. Bootstrapping a neural conver-
sational agent with dialogue self-play, crowdsourcing and on-line reinforcement learning. In
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 41–51, 2018.

14

James Steven Supancic III and Deva Ramanan. Tracking as online decision-making: Learning a
policy from streaming videos with reinforcement learning. In ICCV, pages 322–331, 2017.

Richard S Sutton and Andrew G Barto. Introduction to Reinforcement Learning, volume 135.

MIT press, 1998.

Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear

programming. In ICML, pages 1032–1039, 2008.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based

control. In IROS, pages 5026–5033, 2012.

Pavel Tokmakov, Karteek Alahari, and Cordelia Schmid. Learning motion patterns in videos. In

CVPR, pages 531–539, 2017.

Michal Valko, Mohammad Ghavamzadeh, and Alessandro Lazaric. Semi-supervised apprentice-

ship learning. In EWRL, pages 131–142, 2012.

Vladimir Vapnik. Statistical Learning Theory, volume 3. Wiley, New York, 1998.

Weihong Wang, Yang Wang, Fang Chen, and Arcot Sowmya. A weakly supervised approach for
object detection based on soft-label boosting. In IEEE Workshop on Applications of Computer
Vision, pages 331–338, 2013.

Serena Yeung, Vignesh Ramanathan, Olga Russakovsky, Liyue Shen, Greg Mori, and Li Fei-Fei.

Learning to learn from noisy web videos. In CVPR, pages 7455–7463, 2017.

Dingfu Zhou, Benjamin Quost, and Vincent Fr´emont. Soft label based semi-supervised boosting
for classiﬁcation and object recognition. In International Conference on Control Automation
Robotics & Vision, pages 1062–1067, 2014.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy

inverse reinforcement learning. In AAAI, pages 1433–1438, 2008.

15

A Proof for 2IWIL

A.1 Proof of Theorem 4.1

Theorem. The classiﬁcation risk (6) can be equivalently expressed as

RSC,‘(g) =Ex,r∼q[r(‘(g(x)) − ‘(−g(x))) + (1 − β)‘(−g(x))] + Ex∼p[β‘(−g(x))],

where β ∈ [0, 1] is an arbitrary weight.

Proof. Similar to Eq. (4), we may express p(x|y = −1) by the Bayes’ rule as

p(x|y = −1) =

(1 − r(x))p(x)
1 − α

.

(11)

Consequently, the statement can be conﬁrmed as follows:

RSC,‘(g) =

αp(x|y = +1)‘(g(x)) + (1 − α)p(x|y = −1)‘(−g(x))dx

α

r(x)p(x)
α

(1 − r(x))p(x)
1 − α

‘(g(x)) + (1 − α)

‘(−g(x))dx

(∵ Eqs. (4) and (11))

p(x)r(x)‘(g(x)) + p(x)(1 − r(x))‘(−g(x))dx

Z

Z

Z

Z

=

=

=

{r‘(g(x)) + (1 − r)‘(−g(x))} q(x, r)dxdr

=Ex,r∼q[r‘(g(x)) + (1 − r)‘(−g(x))]





=Ex,r∼q



r‘(g(x)) + (1 − r)‘(−g(x)) + β‘(−g(x)) − β‘(−g(x))

}

|

{z
=0

=Ex,r∼q[r(‘(g(x)) − ‘(−g(x))) + (1 − β)‘(−g(x))] + Ex∼p[β‘(−g(x))].

A.2 Proof of Proposition 4.2

Proposition. Let σcov denote the covariance between n−1
Pnc
n−1
c
when β = nu

i=1 ri{‘(g(xc,i)) − ‘(−g(xc,i))} and
i=1 ‘(−g(xc,i)). For a ﬁxed g, the estimator bRSC,‘(g) of Eq. (7) has the minimum variance

among estimators in the form of Eq. (7) for β ∈ [0, 1].

+

c

σcov
Var(‘(−g(x)))

ncnu
nc+nu

nc+nu

Pnc

Proof. Let

µ (cid:44)EDc,Du[ bRSC,‘(g)],
1
nc

µ1 (cid:44)EDc

ncX

"

#
‘(−g(xc,i))

w1 (cid:44)EDc

w2 (cid:44)EDc

λ (cid:44)EDc

1
nc
 

"





" 

i=1
ncX

i=1

1
nc

1
nc

ncX

i=1
ncX

i=1

= EDu

"

1
nu

nuX

i=1

#
‘(−g(xu,i))

#

= Ex∼p[‘(−g(x))],

r(xi)(‘(g(xc,i)) − ‘(−g(xc,i)))

,

r(xi)(‘(g(xc,i)) − ‘(−g(xc,i)))

!2
 ,

!  

1
nc

ncX

i=1

16

r(xi)(‘(g(xc,i) − ‘(−g(xc,i))))

‘(−g(xc,i))

,

!#













}

 

1
nc

ncX

i=1
(cid:20)(cid:16) 1
nc

σcov (cid:44)Cov

ri(‘(g(xc,i) − ‘(−g(xc,i)))),

‘(−g(xc,i))

= λ − w1µ1

1
nc

ncX

i=1

!

We may represent EDc

Pnc

i=1 ‘(−g(xc,i))

in terms of Var(‘(−g(x))) and µ1:



 

EDc



1
nc

ncX

i=1

‘(−g(xc,i))

 =

EDc

‘(−g(xc,i))2 + 2

‘(−g(xc,i))‘(−g(xc,j))

ncX

i−1
X

a

i=1

j=1

(cid:16)

ncEx∼p

h

‘(−g(x))2i

+ nc(nc − 1)Ex∼p [‘(−g(x))]2(cid:17)

(cid:17)2(cid:21)


ncX


i=1

!2

1
n2
c

1
n2
c
1
nc

=

=

Var(‘(−g(x))) + µ2
1.

Similarly, we obtain EDu[( 1
nu

Pnu

i=1 ‘(−g(xu,i)))2] = n−1

u Var(‘(−g(x))) + µ2

1. As a result,

Var( bRSC,‘(g))
(cid:20)(cid:16)

=EDc,Du

bRSC,‘(g)

(cid:17)2(cid:21)

− µ2




1



nc


|

ncX

i=1

{z
(A)

=EDc,Du

ri(‘(g(xc,i)) − ‘(−g(xc,i)))

+(1 − β)

‘(−g(xc,i))

+β

‘(−g(xu,i))

− µ2

ncX

i=1

1
nc
|

{z
(B)

nuX

i=1

1
nu
|

{z
(C)

= w2
|{z}
(A)2

+2(1 − β) λ

|{z}
(A)(B)

+2β w1µ1
| {z }
(A)(C)

+(1 − β)2

Var(‘(−g(x))) + µ2
1

}

(cid:18) 1
nc

|

+ 2(1 − β)β µ2
1
|{z}
(B)(C)

+β2

Var(‘(−g(x))) + µ2
1

−µ2

(cid:19)

}

{z
(B)2
(cid:18) 1
nu

|

}

(cid:19)

}

{z
(C)2

(cid:19)

=

w2 + 2λ − µ2 +

Var(‘(−g(x))) + µ2
1

−2

+ σcov

β + Var(‘(−g(x)))

(cid:18)

|

1
nc

{z
const.w.r.t.β

(cid:18) Var(‘(−g(x)))
nc

(cid:19)

}

(cid:19)

(cid:18) nc + nu
ncnu

β2

=Var(‘(−g(x)))

(cid:18) nc + nu
ncnu

(cid:19) (cid:18)

β −

(cid:18) nu

nc + nu

+

σcov
Var(‘(−g(x)))

ncnu
nc + nu

(cid:19)(cid:19)2

+ const.

Since Var(‘(−g(x)))

β = clip[0,1]

(cid:16) nu

nc+nu

(cid:17)

(cid:16) nc+nu
ncnu

+

σcov
Var(‘(−g(x)))

ncnu
nc+nu

(cid:17)

≥ 0, and β ∈ [0, 1], Var( bRSC,‘(g)) is minimized when

. Note that clip[l,u](v) = min{max{v, l}, u}.

A.3 Proof of Theorem 4.3

Theorem. Let G be the hypothesis class we use. Assume that the loss function ‘ is ρ‘-Lipschitz
continuous, and that there exists a constant C‘ > 0 such that supx∈X ,y∈{±1} |‘(yg(x))| ≤ C‘ for
any g ∈ G. Let bg (cid:44) arg min
RSC,‘(g). For δ ∈ (0, 1), with probability
at least 1 − δ over repeated sampling of data for training bg,

bRSC,‘(g) and g∗ (cid:44) arg min

g∈G

g∈G

RSC,‘(bg) − RSC,‘(g∗) ≤16ρ‘((3 − β)Rnc(G) + βRnu(G)) + 4C‘

s

(cid:18)

log(8/δ)
2

(3 − β)n

− 1
− 1
c + βn
2
2
u

(cid:19)

.

17

Proof. Note that bg and g∗ are the minimizers of bRSC,‘(g) and RSC,‘(g), respectively. Then,

RSC,‘(bg) − RSC,‘(g∗) = RSC,‘(bg) − bRSC,‘(bg) + bRSC,‘(bg) − bRSC,‘(g∗) + bRSC,‘(g∗) − RSC,‘(g∗)

(cid:16)

≤ sup
g∈G

≤ 2 sup
g∈G

RSC,‘(g) − bRSC,‘(g)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) .
(cid:12) bRSC,‘(g) − RSC,‘(g)

(cid:17)

+ 0 + sup
g∈G

(cid:16)

(cid:17)
bRSC,‘(g) − RSC,‘(g)

From now on, our goal is to bound the uniform deviation supg∈G

(cid:12)
(cid:12)
(cid:12) bRSC,‘(g) − RSC,‘(g)

(cid:12)
(cid:12)
(cid:12). Since

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) bRSC,‘(g) − RSC,‘(g)
(cid:12)

sup
g∈G

≤ sup
g∈G

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nc

ncX

i=1

1
nu

nuX

i=1

(cid:12)
(cid:12)
(cid:12)
+ β sup
(cid:12)
(cid:12)
g∈G
ncX

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nc

i=1

≤ sup
g∈G

+ (1 − β) sup
g∈G

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nc

ncX

i=1

{ri(‘(g(xc,i)) − ‘(−g(xc,i))) + (1 − β)‘(−g(xc,i))}

(cid:12)
− Ex,r∼q [r(‘(g(x)) − ‘(−g(x))) + (1 − β)‘(−g(x))]
(cid:12)
(cid:12)

‘(−g(xu,i)) − Ex∼p [‘(−g(x))]

(cid:12)
(cid:12)
ri‘(g(xc,i)) − Ex,r∼q[r‘(g(x))]
(cid:12)
(cid:12)
(cid:12)

+ sup
g∈G

1
nc

(cid:12)
(cid:12)
ri‘(−g(xc,i)) − Ex,r∼q[r‘(−g(x))]
(cid:12)
(cid:12)
(cid:12)

‘(−g(xc,i)) − Ex,r∼q[‘(−g(x))]

+ β sup
g∈G

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nu

nuX

i=1

(cid:12)
(cid:12)
‘(−g(xu,i)) − Ex∼p[‘(−g(x))]
(cid:12)
(cid:12)
(cid:12)

,

(12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ncX

i=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

all we need to do is to bound four terms appearing in the RHS independently, which can be
done by McDiarmid’s inequality [McDiarmid, 1989]. For the ﬁrst term, since Pnc
i=1 ri‘(g(xc,i)) −
Ex,r∼q[r‘(g(x))] is the bounded diﬀerence with a constant CL/nc for every replacement of xc,i,
McDiarmid’s inequality state that

ri‘(g(xc,i)) − Ex,r∼q[r‘(g(x))]

!

"

 

Pr

sup
g∈G

"

− E

1
nc

ncX

i=1
 

sup
g∈G

1
nc

ncX

i=1

which is equivalent to

ri‘(g(xc,i)) − Ex,r∼q[r‘(g(x))]

≥ ε

≤ exp

−

!#

#

 

!

,

2ε2
L/nc

C2

 

1
nc

ncX

i=1

sup
g∈G

ri‘(g(xc,i)) − Ex,r∼q[r‘(g(x))]

!

"

 

≤ E

sup
g∈G

1
nc

ncX

i=1

ri‘(g(xc,i)) − Ex,r∼q[r‘(g(x))]

+ CL

!#

s

log(8/δ)
2nc

,

with probability at least 1 − δ/8. Following the symmetrization device (Lemma 6.3 in Ledoux
and Talagrand [1991]) and Ledoux-Talagrand’s contraction inequality (Theorem 4.12 in Ledoux
and Talagrand [1991]), we obtain

"

E

 

1
nc

ncX

i=1

sup
g∈G

ri‘(g(xc,i)) − Ex,r∼q[r‘(g(x))]

≤ 2Rnc(‘ ◦ G)

(symmetrization)

!#

18

Note that 0 ≤ ri ≤ 1 for i = 1, . . . , nc. Thus, one-sided uniform deviation bound is obtained:
with probability at least 1 − δ/8,

≤ 4ρLRnc(G)

(contraction).

ri‘(g(xc,i)) − Ex,r∼q[r‘(g(x))]

≤ 4ρLRnc(G) + CL

Applying it twice, the two-sided uniform deviation bound is obtained: with probability at least
1 − δ/4,

!

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s

log(8/δ)
2nc

.

s

log(8/δ)
2nc

.

 

1
nc

ncX

i=1

sup
g∈G

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nc

ncX

i=1

sup
g∈G

ri‘(g(xc,i)) − Ex,r∼q[r‘(g(x))]

≤ 8ρLRnc(G) + 2CL

Similarly, the remaining three terms in the RHS of Eq. (12) can be bounded. Since the second,
third, and fourth terms are the bounded diﬀerences with constants CL/nc, CL/nc, and CL/nu,
respectively, the following inequalities hold with probability at least 1 − δ/4:

sup
g∈G

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
g∈G

sup
g∈G

ncX

i=1

1
nc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nu

ncX

(cid:12)
(cid:12)
ri‘(−g(xc,i)) − Ex,r∼q[r‘(−g(x))]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
‘(−g(xc,i)) − Ex,r∼q[‘(−g(x))]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
‘(−g(xu,i)) − Ex∼p[‘(−g(x))]
(cid:12)
(cid:12)
(cid:12)

i=1
nuX

i=1

≤ 8ρLRnc(G) + 2CL

≤ 8ρLRnc(G) + 2CL

≤ 8ρLRnu(G) + 2CL

s

s

s

log(8/δ)
2nc

log(8/δ)
2nc

,

,

log(8/δ)
2nu

.

After all, we can bound the original estimation error: with probability at least 1 − δ,

RSC,‘(bg) − RSC,‘(g∗) ≤16ρL((3 − β)Rnc(G) + βRnu(G)) + 4CL

s

(cid:18)

log(8/δ)
2

(3 − β)n

− 1
− 1
c + βn
2
2
u

(cid:19)

.

B Proof for IC-GAIL

B.1 Proof of Theorem 4.4

Theorem. Denote that

V (πθ, Dw) = Ex∼p[log(1 − Dw(x))] + Ex∼p0[log Dw(x)],

and that C(πθ) = maxw V (πθ, Dw). Then, V (πθ, Dw) is maximized when Dw = p0
w), and
its maximum value is C(πθ) = − log 4 + 2JSD(pkp0). Thus, C(πθ) is minimized if and only if
pθ = popt almost everywhere.

p+p0 ((cid:44) D∗

Proof. Given a ﬁxed agent policy πθ, the discriminator maximize the quantity V (πθ, Dw), which
can be rewritten in the same way we did in Eq. (13), such as

V (πθ, Dw) = Ex∼p[log(1 − Dw(x))] + Ex∼p0[log Dw(x)]

Z

=

p0(x) log Dw(x) + p(x) log(1 − Dw(x))dx.

19

This maximum is achieved when Dw(x) = Dw∗(x) = p0(x)
p0(x)+p(x) , with the same discussion as
Proposition 1 in Goodfellow et al. [2014]. As a result, we may derive maxw V (πθ, Dw) with
D∗

w(x),

C(πθ) = V (πθ, D∗

w) = Ex∼p

(cid:20)
log

(cid:21)

p
p0 + p

+ Ex∼p0

(cid:20)
log

p0
p0 + p

(cid:21)

,

where p0 = αpθ + (1 − α)pnon. Note that C(πθ) = Ex∼p[log 1
p0 = p. We may rewrite C(πθ) as follows:

2 ] + Ex∼p0[log 1

2 ] = − log 4 when

C(πθ) =Ex∼p

(cid:20)
log

(cid:21)

p
p0 + p
(cid:20)
log

+ Ex∼p0

(cid:20)
log

p0
(p0 + p)/2

(cid:21)

p0
p0 + p
(cid:21)

+ Ex∼p0

= − log 4 + Ex∼p

= − log 4 + 2JSD(pkp0),

(cid:20)
log

(cid:21)

p
(p0 + p)/2

where JSD(p1kp2) (cid:44) 1
Ep2[log
(p1+p2)/2 ] is Jensen-Shannon divergence. Since
2
Jensen-Shannon divergence is greater or equal to zero and it is minimized and only if p0 = p, we
obtain that C(πθ) is minimized if and only if

(p1+p2)/2 ] + 1

Ep1[log

2

p2

p1

p0 = p ⇒ αpθ + (1 − α)pnon = αpopt + (1 − α)pnon almost everywhere

⇒ pθ = popt almost everywhere.

B.2 Proof of Theorem 4.5

Theorem. V (πθ, Dw) can be transformed to eV (πθ, Dw), which is deﬁned as follows:

eV (πθ, Dw) = Ex∼p[log(1 − Dw(x))] + αEx∼pθ [log Dw(x)] + Ex,r∼q[(1 − r) log Dw(x)].

Proof. The statement can be conﬁrmed as follows:

Ex∼p[log(1 − Dw(x))] + Ex∼p0[log Dw(x)]

= Ex∼p[log(1 − Dw(x))] + αEx∼pθ [log Dw(x)] + (1 − α)Ex∼pnon[log Dw(x)]

= Ex∼p[log(1 − Dw(x))] + αEx∼pθ [log Dw(x)] + (1 − α)Ex,r∼q

(cid:20) 1 − r
1 − α

(cid:21)

log Dw(x)

= Ex∼p[log(1 − Dw(x))] + αEx∼pθ [log Dw(x)] + Ex,r∼q[(1 − r) log Dw(x)],

(13)

where the ﬁrst identity comes from the deﬁnition p0 = αpθ + (1 − α)pnon, and the second identity
holds since

log Dw(x)

p(x)dx

(note pnon(x) = p(x|y = −1))

Ex∼pnon[log Dw(x)] =

log Dw(x)pnon(x)dx

Z

Z

Z

=

=

1 − r(x)
1 − α

1 − r
1 − α

log Dw(x)
(cid:20) 1 − r
1 − α

= Ex,r∼q

q(x, r)dxdr

(cid:21)
log Dw(x)

.

20

B.3 Proof of Theorem 4.6

Theorem. Let W be a parameter space for training the discriminator and DW (cid:44) {Dw | w ∈ W}
be its hypothesis space. Assume that

max{

sup
x∈X ,w∈W

| log Dw(x)|,

sup
x∈X ,w∈W

| log(1 − Dw(x))|} ≤ CL

, and that max{supw∈W | log Dw(x) − log Dw(x0)|, supw∈W | log(1 − Dw(x)) − log(1 − Dw(x0))|} ≤
ρL|x − x0| for any x, x0 ∈ X . For a ﬁxed agent policy πθ, let D
bV (πθ, Dw) and
bw

(cid:44) arg max
w∈W

V (πθ, Dw). For δ ∈ (0, 1), with probability at least 1 − δ over repeated sampling

Dw∗ (cid:44) arg max

w∈W
of data for training D

bw,
V (πθ, Dw∗) − V (πθ, D

bw) ≤16ρL(Rnu(DW ) + αRna(DW ) + Rnc(DW ))
(cid:18)

s

(cid:19)

− 1
u + αn
2

− 1
a + n
2

− 1
2
c

n

.

+ 4CL

log(6/δ)
2

Proof. Denote V(w) (cid:44) V (πθ, Dw) and bV(w) (cid:44) bV (πθ, Dw). Note that bw and w∗ are the minimizers
of V(w) and bV(w), respectively. Then,

V(w∗) − V( bw) = V(w∗) − bV(w∗) + bV(w∗) − bV( bw) + bV( bw) − V( bw)
(cid:17)
bV(w) − V(w)

V(w) − bV(w)

(cid:17)

(cid:16)

(cid:16)

+ 0 + sup
w∈W

≤ sup
w∈W

≤ 2 sup
w∈W

(cid:12)
(cid:12)
(cid:12) bV(w) − V(w)

(cid:12)
(cid:12)
(cid:12) .

From now on, our goal is to bound the uniform deviation supw∈W

sup
w∈W

(cid:12)
(cid:12)
(cid:12) bV(w) − V(w)

(cid:12)
(cid:12)
(cid:12) ≤ sup
w∈W

log(1 − Dw(xu,i)) − Ex∼p [log(1 − Dw(x))]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12). Since
(cid:12) bV(w) − V(w)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

nuX

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nu

naX

i=1

i=1
(cid:12)
1
(cid:12)
(cid:12)
(cid:12)
na
(cid:12)
ncX

i=1

+ α sup
w∈W
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ sup
w∈W

1
nc

(cid:12)
(cid:12)
log Dw(xa,i) − Ex∼pθ [log Dw(x)]
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(1 − ri) log Dw(xc,i) − Ex,r∼q [(1 − r) log Dw(x)]
(cid:12)
(cid:12)
(cid:12)

,

(14)

three terms appearing in the RHS must be bounded independently, utilizing McDiarmid’s
inequality [McDiarmid, 1989]. For the ﬁrst term, since Pnu
i=1 log(1 − Dw(xu,i)) − Ex∼p[log(1 −
Dw(x))] has the bounded diﬀerence property with a constant CL/nu for every replacement of
xu,i, we can conclude by McDiarmid’s inequality that

log(1 − Dw(xu,i)) − Ex∼p[log(1 − Dw(x))]

log(1 − Dw(xu,i)) − Ex∼p[log(1 − Dw(x))]

≥ ε

≤ exp

−

#

#

 

!

,

2ε2
L/nu

C2

"

Pr

sup
w∈W

  nuX

i=1
"

−E

sup
w∈W

nuX

i=1

which is equivalent to

  nuX

i=1

sup
w∈W

!

!

log(1 − Dw(xu,i)) − Ex∼p[log(1 − Dw(x))]

21

"

≤ E

sup
w∈W

nuX

i=1

log(1 − Dw(xu,i)) − Ex∼p[log(1 − Dw(x))]

+ CL

#

s

log(6/δ)
2nu

,

with probability at least 1 − δ/6. Following symmetrization device (Lemma 6.3 in Ledoux and
Talagrand [1991]) and Ledoux-Talagrand’s contraction inequality (Theorem 4.12 in Ledoux and
Talagrand [1991]), we obtain

"

E

sup
w∈W

nuX

i=1

#
log(1 − Dw(xu,i)) − Ex∼p[log(1 − Dw(x))]

≤ 2Rnu(log ◦DW )

(symmetrization)

≤ 4ρLRnu(DW ).

(contraction inequality)

Thus, one-sided uniform deviation bound is obtained: with probability at least 1 − δ/6,

log(1 − Dw(xu,i)) − Ex∼p[log(1 − Dw(x))]

≤ 4ρLRnu(DW ) + CL

Applying it twice, the two-sided uniform deviation bound is obtained: with probability at least
1 − δ/3,

!

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s

log(6/δ)
2nu

.

s

log(6/δ)
2nu

.

log(1 − Dw(xu,i)) − Ex∼p[log(1 − Dw(x))]

≤ 8ρLRnu(DW ) + 2CL

  nuX

i=1

sup
w∈W

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

nuX

i=1

sup
w∈W

Similarly, the second and third terms on the RHS of Eq. (14) can be bounded. Since they
have the bounded diﬀerence property with constants CL/na and CL/nc, respectively (note that
|1 − r(x)| ≤ 1 for any x), both of the following inequalities hold independently with probability
at least 1 − δ/3:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

naX

i=1
ncX

i=1

sup
w∈W

sup
w∈W

(cid:12)
(cid:12)
log Dw(xa,i) − Ex∼pθ [log Dw(x)]
(cid:12)
(cid:12)
(cid:12)

≤ 8ρLRna(DW ) + 2CL

s

log(6/δ)
2na

,

(cid:12)
(cid:12)
(1 − ri) log Dw(xc,i) − Ex,r∼q[(1 − r) log Dw(x)]
(cid:12)
(cid:12)
(cid:12)

≤ 8ρLRnc(DW ) + 2CL

s

log(6/δ)
2nc

.

Combining the above all, we can bound the original estimation error: the following bound holds
with probability at least 1 − δ,

V(w∗) − V( bw) ≤16ρL(Rnu(DW ) + αRna(DW ) + Rnc(DW )) + 4CL

s

(cid:18)

log(6/δ)
2

− 1
u + αn
2

− 1
a + n
2

− 1
2
c

n

(cid:19)

.

C Implementation and Experimental Details

We use the same neural net architecture and hyper-parameters for all tasks. For the architectures
of all neural networks, we use two hidden layers with size 100 and Tanh as activation functions.
Please refer to Table 2 for more details. Speciﬁcation of each tasks is shown in Table 3, where we
show the average return of the optimal and the uniformly random policies. The average return
is used to normalize the performance in Sec. 5 so that 1.0 indicates the optimal policy and 0.0
the random policy.

22

Table 2: Hyper-parameters used for all tasks.

Hyper-parameters

value

γ
τ (Generalized Advantage Estimation)
batch size
learning rate (value network)
learning rate (discriminator)
optimizer
loss function (2IWIL)

0.995
0.97
5, 000
3 × 10−4
1 × 10−3
Adam
logistic loss

Table 3: Speciﬁcation of each tasks. Optimal policy and random policy columns indicate the
average return.

Tasks

S

A nu
R6
HalfCheetah-v2 R17
R17
Walker-v2
R6
R111 R8
Ant-v2
R2
R8
Swimmer-v2
R3
R11
Hopper-v2

2000
1600
480
20
16

nc
500
400
120
5
4

optimal policy random policy

3467.32
3694.13
4143.10
348.99
3250.67

-288.44
1.91
-72.30
2.31
18.04

C.1 Non-negative risk estimator

By observing the risk estimator of Eq. (7), it is possible that the empirical estimation is negative
and this may lead to overﬁtting [Kiryo et al., 2017]. Since we know that the expected risk is
nonnegative, we can borrow the idea from Kiryo et al. [2017] to mitigate this problem by simply
adding the max operator to prevent the empirical risk from becoming negative by ﬁrst rewriting
the empirical risk as

bRSC,‘(g) = bR+

C (g) + bR−

C,U (g),

(15)

where

and

bR+

C (g) =

r(xc,i)‘(g(xc,i)),

1
nc

ncX

i=1

bR−

C,U (g) =

1
nc

ncX

i=1

(1 − β − r(xi))‘(−g(xc,i)) +

β‘(−g(xu,i)).

1
nu

nuX

i=1

C,U ≥ 0 holds for all g. However, it is not the case for bR−

Note that R−
C,U (g), which is a potential
reason to overﬁt. Based on Eq. (15), we achieve the non-negative risk estimator that gives the
non-negative empirical risk as follows.

bRSC,‘(g) = bR+

C (g) + max

n
0, bR−

C,U (g)

o

.

(16)

23

C.2 Ant-v2 Figures

Figure 4: Learning curves of our 2IWIL and IC-GAIL versus baselines.

Figure 5: Learning curves of proposed methods with diﬀerent standard deviations of Gaussian
noise added to conﬁdence. The numbers in the legend indicate the standard deviation of the
Gaussian noise.

We empirically found that when using GAIL-based approaches in Ant-v2 environment, the
performance degrades quickly in early training stages. The uncropped ﬁgures are Figs. 4, 5
and 6.

24

Figure 6: Learning curves of the proposed methods with diﬀerent number of unlabeled data. The
numbers in the legend suggest the proportion of unlabeled data used as demonstrations.

25

