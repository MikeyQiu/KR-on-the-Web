Recurrent neural networks with specialized word embeddings
for health-domain named-entity recognition

Inigo Jauregi Unanuea,b, Ehsan Zare Borzeshib, Massimo Piccardia

aUniversity of Technology Sydney (UTS), Sydney, Australia
bCapital Markets Cooperative Research Center (CMCRC), Sydney, Australia

8
1
0
2
 
n
u
J
 
5
2
 
 
]
L
C
.
s
c
[
 
 
2
v
9
6
5
9
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

Background. Previous state-of-the-art systems on Drug Name Recognition (DNR) and Clinical Concept Extraction
(CCE) have focused on a combination of text “feature engineering” and conventional machine learning algorithms
such as conditional random ﬁelds and support vector machines. However, developing good features is inherently
heavily time-consuming. Conversely, more modern machine learning approaches such as recurrent neural networks
(RNNs) have proved capable of automatically learning eﬀective features from either random assignments or auto-
mated word “embeddings”.
Objectives. (i) To create a highly accurate DNR and CCE system that avoids conventional, time-consuming fea-
ture engineering. (ii) To create richer, more specialized word embeddings by using health domain datasets such as
MIMIC-III. (iii) To evaluate our systems over three contemporary datasets.
Methods. Two deep learning methods, namely the Bidirectional LSTM and the Bidirectional LSTM-CRF, are eval-
uated. A CRF model is set as the baseline to compare the deep learning systems to a traditional machine learning
approach. The same features are used for all the models.
Results. We have obtained the best results with the Bidirectional LSTM-CRF model, which has outperformed all
previously proposed systems. The specialized embeddings have helped to cover unusual words in DrugBank and
MedLine, but not in the i2b2/VA dataset.
Conclusion. We present a state-of-the-art system for DNR and CCE. Automated word embeddings has allowed us to
avoid costly feature engineering and achieve higher accuracy. Nevertheless, the embeddings need to be retrained over
datasets that are adequate for the domain, in order to adequately cover the domain-speciﬁc vocabulary.

Keywords: Neural networks (computer), Machine learning, Artiﬁcial intelligence, Clinical concept extraction, Drug
name recognition

1. Introduction

In recent years, the amount of digital information
generated from all sectors of society has increased
rapidly, and as a result, agriculture, industry, small busi-
nesses and, of course, healthcare, are becoming more
eﬃcient and productive thanks to the insights obtained
from the “Big Data”. However, in order to deal eﬀec-
tively with such large data, there is an ongoing need for
novel, scalable and more accurate analytic tools.

In the healthcare system, patients’ medical records
represent a big data source. Even though the records
contain very useful information about the patients, in
most cases the information consists of unstructured text
such as, among others, doctors’ notes, medical observa-
tions made by various physicians, and descriptions of

the recommended treatments. This type of data can-
not be analyzed using common statistical tools; rather,
they need to be approached by Natural Language Pro-
cessing (NLP) techniques. In this paper, we focus on a
well-known task in NLP, namely Named-Entity Recog-
nition (NER). The goal of NER is to automatically ﬁnd
“named entities” in text and classify them into prede-
ﬁned categories such as people, locations, companies,
time expressions etc.
In the case of specialized do-
mains, NER systems focus on text with speciﬁc dic-
tionaries and topics, together with dedicated sets of
In the health domain, the two most
named-entities.
important NER tasks are Clinical Concept Extraction
(CCE) and Drug Name Recognition (DNR). The for-
mer aims to identify mentions of clinical concepts in
patients’ records to help improve the organization and

Preprint version

June 26, 2018

Figure 1: (a) DNR and (b) CCE tasks examples, where ‘B’ (beginning) speciﬁes the start of a named entity, ‘I’ (inside) speciﬁes that the word is
part of the same named entity, and ‘O’ (outside) speciﬁes that the word is not part of any predeﬁned class

management of healthcare services. Named entities in
CCE can include test names, treatments, problems re-
lated to individual patients, and so forth. The latter
seeks to ﬁnd drug mentions in unstructured biomedical
texts to match drug names with their eﬀects and dis-
cover drug-drug interactions (DDIs). DNR is a key step
of pharmacovigilance (PV) which is concerned with the
detection and understanding of adverse eﬀects of drugs
and other drug-related problems. Figure 1 shows exam-
ples of both tasks.

NER is a challenging learning problem because in
most domains the training datasets are scarce, prevent-
ing a “brute-force” approach by exhaustive dictionaries.
Consequently, many systems rely on hand-crafted rules
and language-speciﬁc knowledge to solve this task. To
give a simple example of such rules, if the word begins
with a capital letter in the middle of the sentence, it can
be assumed to be a named entity in most cases. Never-
theless, these approaches are time-costly to develop, de-
pend considerably on the language and the domain, are
ineﬀective in the presence of informal sentences and ab-
breviations and, although they usually achieve high pre-
cision, suﬀer from low recall (i.e., they miss many en-
tities). Conversely, machine learning (ML) approaches
overcome all these limitations as they are intrinsically
robust to variations. Current state-of-the-art ML meth-
ods follow a two-step process: 1) feature engineering
and 2) automated classiﬁcation [1, 2, 3, 4]. The ﬁrst step
represents the text by numeric vectors using domain-
speciﬁc knowledge. The second step refers to the task
of classifying each word into a diﬀerent named-entity
class, with popular choices for the classiﬁer being the
linear-chain Conditional Random Fields (CRF), Struc-
tural Support Vector Machines (S-SVM) and maximum-
entropy classiﬁers. The drawback of this approach is
that feature engineering can be often as time-consuming
as the manual design of rules.

In recent years, the advent of deep learning has con-

2

tributed to signiﬁcantly overcome this problem [5, 6, 7].
The Long Short-Term Memory (LSTM) and its variants
(e.g., the Bidirectional LSTM), which are a speciﬁc type
of Recurrent Neural Networks (RNNs), have reported
very promising results [5]. In these models, words only
need to be assigned to random vectors, and during train-
ing the neural network is able to automatically learn im-
proved representations for them, completely bypassing
feature engineering. In order to further increase the per-
formance of these systems, the input vectors can alter-
natively be assigned with general-purpose word embed-
dings learned with GloVe or Word2vec [8, 9]. The aim
of general-purpose word embeddings is to map every
word in a dictionary to a numerical vector (the embed-
ding) so that the distance between the vectors somehow
reﬂects the semantic diﬀerence between the words. For
example, ‘cat’ and ‘dog’ should be closer in the vec-
tor space than ‘cat’ and ‘car’. The common principle
behind embedding approaches is that the meaning of a
word is conveyed by the words it is used with (its sur-
rounding words, or context). Therefore, the training
of the word embeddings only requires large, general-
purpose text corpora such as Wikipedia (400K unique
words) or Common Crawl (2.2M unique words), with-
out the need for any manual annotation. However,
drug and clinical concept recognition are very domain-
speciﬁc tasks, and many words might not appear in
general-domain datasets. In order to assign word em-
beddings to these specialized words, the embedding al-
gorithms need to be retrained using medical domain re-
sources such as the MIMIC-III corpora [10]. As well
as semantic word embeddings, character-level embed-
dings of words can also be automatically learned. Such
embeddings can capture typical preﬁxes and suﬃxes,
providing the classiﬁers with richer representations of
the words [5].

Preliminary results for the work presented in this pa-
per have obtained very promising accuracy in DNR and

CCE tasks using neural networks. Chalapathy et al.
[11] presented a DNR system that uses a Bidirectional
LSTM-CRF architecture with random assignments of
the input word vectors at the EMNLP 2016 Health Text
Mining and Information Analysis workshop. The re-
ported results were very close to the system that ranked
ﬁrst in the SemEval-2013 Task 9.1.
In Chalapathy et
al. [12], the authors leveraged the same architecture for
CCE at the Clinical NLP 2016 workshop, this time us-
ing pre-trained word embeddings from GloVe, and the
results outperformed previous systems over the 2010
i2b2/VA IRB Review dataset.
In this paper, we ex-
tend the previous research by training the deep networks
with more complex and specialized word embeddings.
Moreover, we explore the impact of augmenting the
word embeddings with conventional feature engineer-
ing. As methods, we compare contemporary recurrent
neural networks such as the Bidirectional LSTM and the
Bidirectional LSTM-CRF against a conventional ML
baseline (a CRF). We report state-of-the-art results in
both DNR and CCE.

2. Related work

Most of the research carried out in domain-speciﬁc
NER has combined supervised and semi-supervised ML
models with text feature engineering. For example,
the WBI-NER system that ranked ﬁrst in the SemEval-
2013 Task 9.1 (Recognition and classiﬁcation of phar-
macological substances, DNR) [3], is based on a linear-
chain CRF with specialized features. Other similar sys-
tems for DNR [2, 13] use various general- and domain-
speciﬁc features. In CCE, the same approach (feature
engineering + conventional ML classiﬁer) has achieved
the best results [4, 14].

In the recent years, there has been an increase in the
use of deep neural networks for a variety of NLP tasks,
including NER [5, 6, 7]. Pre-trained word embeddings
[8, 9, 15] have been used in traditional ML methods
[16, 17] and in neural networks, where Deconourt et
al.
[18] has achieved better performance than previ-
ously published systems in de-identiﬁcation of patient
notes. Cocos et al.
[19] have used the Bidirectional
LSTM model for labelling Adverse Drug Reactions in
pharmacovigilance. Xie et al. [20] have used a similar
model for studying the adverse eﬀects of e-cigarettes.
Wei et al. [21] have combined the output of a Bidirec-
tional LSTM and a CRF as input to an SVM classiﬁer
for disease name recognition. A possible drawback of
this approach is that the overall prediction is not struc-
tured and may miss on useful correlation between the
output variables.

3

In a work that is more related to ours, Jaganatha and
Yu [22] have employed a Bidirectional LSTM-CRF to
label named entities from electronic health records of
cancer patients. Their model diﬀers in the CRF output
module where the pairwise potentials are modelled us-
ing a Convolutional Neural Network (CNN) rather than
the usual transition matrix. Gridach [23] has also used
the Bidirectional LSTM-CRF for named-entity recogni-
tion in the biomedical domain.

The main diﬀerence and contribution of the proposed
approach is that it leverages specialized health-domain
embeddings created from a structured database. In the
experiments, these embeddings have been used jointly
with general-domain embeddings and they have proved
able to improve the accuracy in several cases. In addi-
tion, our work evaluates the use of hand-crafted features
in the system [24]. This aims to provide a comprehen-
sive feature comparison for health-domain named-entity
recognition based on LSTM models.

3. Methods

In this section we provide a description of the main
methods employed. First, we describe the conditional
random ﬁeld (CRF), a traditional machine learning ap-
proach for the classiﬁcation of sequences, which is
used as a baseline in the experiments. This baseline is
compared with two variants of a contemporary recur-
rent neural network, which are known as Bidirectional
LSTM and Bidirectional LSTM-CRF, respectively.

3.1. CRF

A CRF model is a well-known ML approach that has
been widely used in NER [25]. It predicts sequences
of labels (y) from sequences of measurements (x) tak-
ing into account the sequentiality of the data. In a CRF
model, p(y|x, w) is given in Eq. 1 below, where w notes
the model’s parameters, ψ(x, y) is the chosen feature
vector and Z(w, x) is the cumulative sum of p(y|x, w)
over all the possible y:

p(y|x, w) = exp(wT ψ(x, y))

Z(w, x)

The parameters of this model are typically learned
from a training set, (Y,X) = {xi, yi}, i = 1 . . . N, with
conditional maximum likelihood as in:

w = arg max
w

p(Y|X, w)

Once the model has been trained, the prediction of a
CRF is the sequence of labels maximizing the model for
the given input sequence and the learned parameters:

(1)

(2)

Figure 2: The Bidirectional LSTM-CRF with word-level and character-level word embeddings. In the example, word ‘sulfate’ is assumed to be the
5th word in a sentence and its only entity; ‘x˙5’ represents its word-level embedding (a single embedding for the whole word); ‘x˙5ˆ*’ represents
its character-level embedding, formed from the concatenation of the last hidden state of the forward and backward passes of a character-level
Bidirectional LSTM; ’h˙15’ are the hidden states of the main Bidirectional LSTM which become the inputs into a ﬁnal CRF; eventually, the CRF
provides the labeling.

y∗ = arg max

p(y|x, w)

y

(3)

The labels are typically predicted using a Viterbi-
style algorithm which provides the optimal prediction
for the measurement sequence as a whole. The model
is trained by maximizing the conditional likelihood, or
cross-entropy, over a given training set. For its imple-
mentation, we have used the HCRF library [26]. The
features used as input are described in Section 4. In the
experiments, we use the CRF as a useful baseline for
performance comparison with the proposed neural net-
works. Note that a CRF model is also used as the output
layer in the Bidirectional LSTM-CRF as explained in
the next section.

3.2. Bidirectional LSTM and bidirectional LSTM-CRF

RNNs are a type of neural network architecture in
which connections between units form a directed cycle,
creating an internal state and achieving dynamic tempo-
ral behavior. Thanks to their internal memory, RNNs

4

can process a sequence of vectors (x1, x2, ..., xn) as in-
put and produce another sequence (h1, h2, ..., hn) as out-
put that contains some extent of sequential information
about every vector in the input. However, these archi-
tectures in practice fail to learn long-term dependen-
cies in the sequences as they tend to be biased by the
most recent vectors [27]. The Long Short-Term Mem-
ory (LSTM) was therefore designed to overcome this is-
sue by incorporating a gated memory-cell that has been
shown to capture long-term dependencies [28]. Eq. 4
shows the implementations of the diﬀerent gates in the
LSTM [5], where it is the “input” gate, ct is the “cell”
gate, ot is the “output” gate, W are the weights of the
network, b are the biases, σ is the element-wise sigmoid
function, and (cid:12) is the element-wise product. The bidi-
rectional LSTM (B-LSTM) is just a variation, in which
←−
ht) repre-
both the left-to-right (
sentations of the input sentence are generated, and then
←−
concatenated ht = [
ht] in order to obtain the ﬁnal
representation.

−→
ht) and the right-to-left (

−→
ht;

it = σ(Wxixt + Whiht−1 + Wcict−1 + bi)
ct = (1 − it) (cid:12) ct−1+

it (cid:12) tanh(Wxcxt + Whcht−1 + bc)

(4)

ot = σ(Wxoxt + Whoht−1 + Wcoct + bo)
ht = ot (cid:12) tanh(ct)

When applying the LSTM in NER, the words in the
input sentence are ﬁrst mapped to numerical vectors.
These vectors can be random valued, a pre-trained word
embedding, domain-speciﬁc word features or any com-
bination of them. For each vector, the output of the
network are the posterior probabilities of each named-
entity class. An improvement of these networks has
been presented by Lample et al.
[5] using a CRF as
a ﬁnal output layer. This ﬁnal layer provides the sys-
tem with the ability to perform joint decoding of the
input sequence in a Viterbi-style manner. The result-
ing network is known as the Bidirectional LSTM-CRF
(B-LSTM-CRF). We test the LSTM models with the
same features used for the CRF in order to establish the
fairest-possible comparison. The features are described
in detail in Section 4. Fig.2 shows a descriptive diagram
of the B-LSTM-CRF.

4. Word features

As mentioned above, neural networks can learn
meaningful representations from random inizializations
of word embeddings. However, it has been proved that
pre-trained word embeddings can improve the perfor-
mance of the network [5, 12, 18, 24]. In this section, we
present the pre-trained embeddings employed in lieu of
the random assignments.

4.1. Specialized word embeddings

A word embedding maps a word to a numerical vec-
tor in a vector space, where semantically-similar words
are expected to be assigned similar vectors. To per-
form this mapping, we have used a well-known algo-
rithm called GloVe [8]. This algorithm learns word em-
beddings by looking at the co-occurrences of the word
in the training data, assuming that a word’s meaning
is mostly deﬁned by its context and, therefore, words
having similar contexts should have similar embed-
dings. GloVe can be trained from large, general-purpose
datasets such as Wikipedia, Gigaword5 or Common
Crawl without the need for any manual supervision. In
this work, we have experimented with diﬀerent general-
purpose, pre-trained word embeddings from the oﬃcial

5

GloVe website [29] and noticed that the embeddings
trained with Common Crawl (cc) (2.2M unique words)
were giving the best results. We have employed these
embeddings on their own, and also concatenated with
the MIMICIII embeddings (cc/mimic) (described in the
next paragraph). By default, the code always initial-
izes the word embedding of each unique word in the
dictionary with a unique random vector. In alternative,
we replace the random initialization with a pre-trained
embedding. However, although such datasets generate
good embeddings in many cases, for domain-speciﬁc
tasks such as DNR and CCE they can suﬀer from some
lack of vocabulary. As a matter of fact, in health cor-
pora it is common to ﬁnd very technical and unusual
words which are speciﬁc to the health domain. If GloVe
is trained only with general-purpose datasets, it is likely
that such words will be missing and will still have to be
assigned with random vectors. In all cases, the embed-
dings are updated during training by the backpropaga-
tion step.

In order to solve this problem, we have generated
a new word embedding by re-training GloVe with a
large health domain dataset called MIMIC-III [10]. This
dataset contains records of 53,423 distinct hospital ad-
missions of adults to an intensive care unit between
2001 and 2012. The data, structured in 26 tables, in-
clude information such as vital signs, observations of
care providers, diagnostic codes etc. We expect such a
dataset to contain many of the technical words from the
health domain that may not appear in general-domain
datasets, and as the size of MIMIC-III is suﬃciently
large, we should be able to extract meaningful vector
representations for these words. As a ﬁrst step, we
have selected a subset of the tables and columns, and
generated a new dataset where each selected cell to-
gether with the title of the corresponding column form
a pseudo-sentence. As the next step, we have used
this dataset to re-train GloVe, and concatenated these
specialized word embeddings with the others to create
vectors that contain information from both approaches.
Obviously, there are words that appear in the general
dataset, but not in MIMIC-III, and the vice versa.
In
such cases, the corresponding embedding is still as-
signed randomly. If a word does not appear in either
dataset, we assign its whole embedding randomly.

4.2. Character-level embeddings

Following Lample et al. [5] we also add character-
level embeddings of the words. Such embeddings re-
ﬂect the actual sequence of characters of a word and
have proven to be useful for speciﬁc-domain tasks and

morphologically-rich languages. Typically, they con-
tribute to catching preﬁxes and suﬃxes which are fre-
quent in the domain, and correctly classifying the cor-
responding words. As an example, a word ending in
“cycline” is very likely a drug name, and a character-
level embedding could help classify it correctly even
if the word was not present in the training vocabulary.
All the characters are initialized with a random embed-
ding, and then the embeddings are passed character-
by-character to a dedicated LSTM in both forward and
backward order. The ﬁnal outputs in the respective di-
rections promise to be useful encodings of the ending
and the beginning of the word. These character-level
embeddings are integral part of the LSTM architecture
and are not available in the CRF or other models. The
character embeddings, too, are updated during training
with backpropagation.

4.3. Feature augmentation

Conventional machine learning approaches for NER
usually have a feature engineering step. Lee et al. [24]
have shown that adding hand-crafted features to a neu-
ral network can contribute to increase the recall. In our
work, we try this approach with features similar to those
used by Lee et al. [24]. Fig.3 shows the list of features
used. The distinct values of each feature are encoded
onto short random vectors, for a total dimension of 146-
D. During training, these encodings are updated as part
of the backpropagation step.

5. Results

5.1. Datasets

Hereafter, we evaluate the models on three datasets
in the health domain. The ﬁrst is the 2010 i2b2/VA IRB
Revision (we refer to it as i2b2/VA for short in the fol-
lowing) and is used for evaluating CCE. This dataset is
a reduced set of the original 2010 i2b2/VA dataset that
is no longer distributed due to restrictions introduced
by the Institutional Review Board (IRB) in 2011 [30].
The other two datasets are DDI-DrugBank and DDI-
MedLine, both part of the SemEval-2013 Task 9.1 for
DNR [31]. Tables 1a and 1b describe the basic statistics
of the datasets. For the experiments, we have used the
oﬃcial training and test splits released with the distribu-
tions.

5.2. Evaluation metrics

We report the performance of the model in terms of
the F1-score. The F1-score is a very relevant measure as
it considers both the precision and the recall, computing

Documents
Sentences
problem
test
treatment

Training Test
256
170
27626
16315
12592
7073
9225
4608
9344
4844

(a) i2b2/VA

MedLine

DrugBank
Training Test Training Test
730
6577
124
3832
1770
9715

175
1627
520
234
36
1574

58
520
115
90
6
171

54
145
6
65
53
180

Documents
Sentences
drug n
group
brand
drug

(b) DrugBank and MedLine

Table 1: Statistics of the training and test datasets used in the experi-
ments

a weighted average of them. If we note as TP the num-
ber of true positives, FP the false positives and FN the
false negatives, we have:

precision =

T P
T P + FP
T P
recall =
T P + FN
F1 = 2 ∗ precision ∗ recall
precision + recall

(5)

However, it must be remarked that there are diﬀerent
ways of computing the precision and the recall, depend-
ing on what we consider as a correct or incorrect pre-
diction [32]. In this work, we employ the “strict” eval-
uation method, where both the entity class and its exact
boundaries are expected to be correct. We have used
the B-I-O tagging standard to annotate the text at word
level. In detail, ‘B’ means the beginning (ﬁrst word) of
a named entity; ‘I’ stands for ‘inside’, meaning that the
word is part of the same entity (for multi-word entities;
e.g., “albuterol sulfate”); and ‘O’ stands for ‘outside’,
meaning that the word is not part of any named entities.
Therefore, a valid annotation of a named entity always
begins with a ‘B’. An example is shown in Fig.4. All
the models used in this paper have been trained to pre-
dict explicit ‘B’ and ‘I’ labels for each entity class. The
evaluation includes a preprocessing step that converts
an ‘I’ prediction to a ‘B’ if it follows directly an ‘O’
prediction, thus making all predicted entities valid. An

6

Figure 3: Description of the hand-crafted features.

Figure 4: a) An example of an incorrect tagging in the “strict” evaluation method. b) An example of a correct tagging in the “strict” evaluation
method

entity is considered as correctly predicted only if all its
‘B’ and ‘I’ labels and all its classes are predicted cor-
rectly. In the example of Fig.4 the prediction will be
counted as a true positive only if all the four words “re-
cently diagnosed abdominal carcinomatosis” are tagged
as a single entity of the problem class. Every diﬀering
‘B’ prediction will instead be counted as a false positive.
The evaluation protocol explicitly counts only the true
positives and the false positives, and derives the false
negatives as (number of true entities true positives).

5.3. Training and hyperparameters

For an unbiased evaluation, all the trained models
have been tested blindly on unseen test data.
In or-
der to facilitate replication of the empirical results, we
have used a publicly-available library for the implemen-
tation of the neural networks (i.e.
the Theano neural
network toolkit [33]) and we release our code.1 To
operate, any machine learning model requires both a
set of parameters, which are learned automatically dur-
ing training, and some “hyper-parameters”, which have
to be selected manually. Therefore, we have divided
the training set of each dataset into two parts: a train-
ing set for learning the parameters (70%), and a val-
idation set (30%) for selecting the best hyperparame-
ters [34]. The hyper-parameters of the LSTM include
the number of hidden nodes (for both LSTM versions),
(Hw, Hc) ∈ {25, 50, 100}; the word embedding dimen-
sion, dw ∈ {50, 100, 200, 300, 600}; and the character

1HealthNER: https://github.com/ijauregiCMCRC/healthNER

7

Hyper-parameter
Word embedding dim (dw)

Word LSTM hidden layer dim (Hw)
Char embedding dim (dc)
Char LSTM hidden layer dim (Hc)
Dropout
Optimization

Value
300 (cc) or
600 (cc/mimic)
100
25
25
0.5
Stochastic
Gradient Descend
Learning rate
0.01
Concatenated hand-crafted features dim 146

Table 2: The hyper-parameters used in the experiments

embedding dimension, dc ∈ {25, 50, 100}. Additional
hyper-parameters include the learning rate and the drop-
out rate, which were left to their default values of [0.01]
and [0.5] respectively [35]. All weights in the net-
work, feature encodings and the words that do not have
a pre-trained word embedding have been initialized ran-
domly from the uniform distribution within range [1, 1],
and updated during training with backpropagation. The
number of training ”epochs” (i.e., iterations) was set to
100, selecting the epoch that obtained the best results
on the validation set. The best model from the valida-
tion set was ﬁnally tested on the unseen, independent
test set without any further tuning, and the correspond-
ing accuracy reported in the tables. Table 2 shows all
the hyper-parameters used for the experiments reported
in the Results section.

Model
Binarized Neural Embedding CRF [17]
CliNER [14]
Truecasing CRFSuite [36]
CRF + (random)
CRF + (features)
CRF + (cc)
CRF + (cc/mimic)
CRF + (cc/mimic) + (features)
B-LSTM + (random)
B-LSTM + (random) + (features)
B-LSTM + (cc)
B-LSTM + (cc) + (char)
B-LSTM + (cc/mimic) + (char)
B-LSTM + (cc/mimic) + (char) + (features)
B-LSTM-CRF + (random)
B-LSTM-CRF + (random) + (features)
B-LSTM-CRF + (cc)
B-LSTM-CRF + (cc) + (char)
B-LSTM-CRF + (cc/mimic) + (char)
B-LSTM-CRF + (cc/mimic) + (char) + (features)

F1-score (%)
82.80
80.00
75.86
11.27
25.53
53.72
58.28
64.09
65.43
69.49
75.17
76.79
77.19
77.59
75.05
77.81
82.85
83.35
82.70
83.29

(a) CCE results over the i2b2/VA dataset

Model

WBI-NER [3]
Hybrid-DDI [2]
Word2Vec+DINTO [1]
CRF + (random)
CRF + (features)
CRF + (cc)
CRF + (cc/mimic)
CRF + (cc/mimic) + (features)
B-LSTM + (random)
B-LSTM + (random) + (features)
B-LSTM+(cc)
B-LSTM+(cc) + (char)
B-LSTM + (cc/mimic) + (char)
B-LSTM + (cc/mimic) + (char) + (features)
B-LSTM-CRF + (random)
B-LSTM-CRF + (random) + (features)
B-LSTM-CRF + (cc)
B-LSTM-CRF + (cc) + (char)
B-LSTM-CRF + (cc/mimic) + (char)
B-LSTM-CRF + (cc/mimic) + (char) + (features)

DrugBank MedLine
F1(%)
87.80
80.00
75.00
28.70
44.52
43.42
53.12
66.45
65.09
75.43
71.75
84.35
83.63
84.06
69.50
75.78
79.03
87.87
88.38
87.42

F1(%)
58.10
37.00
57.00
13.65
20.19
32.62
30.87
29.36
21.28
30.88
42.39
43.33
44.39
45.92
44.60
43.36
57.87
59.02
60.66
59.75

(b) DNR results over the DrugBank and MedLine datasets

Table 3: Comparison of the results between the diﬀerent RNN models
and the state-of-the-art systems over the CNE and DNR tasks

5.4. Results

Table 3a and 3b shows the results of the proposed
models and the state-of-the-art systems on the CCE task
(i2b2/VA dataset) and DNR task (DrugBank and Med-
Line datasets), respectively.
In the following subsec-
tions, we discuss the results obtained for each task.

5.4.1. CCE results over the i2b2/VA dataset

On the i2b2/VA dataset (Table 3a), the Bidirectional
LSTM-CRF (B-LSTM-CRF) with Common Crawl em-
beddings (cc) and character-level embeddings (char)
as features has obtained the best results (83.35% F1-
score). The model has outperformed all systems from
the literature (top quadrant of Table 3a) which are all
based on conventional domain-speciﬁc feature engi-
It is important to note that deBruijin et al.
neering.

8

[4] had reported a higher accuracy on 2010 i2b2/VA
(85.23% F1-score), but their model was trained and
tested on the original version of the dataset which is
no longer available due to the restrictions introduced by
the Institutional Review Board. As for what special-
ized embeddings are concerned, Table 4 shows that the
general-domain dataset Common Crawl already con-
tains almost all the words in the dataset. Therefore,
adding the MIMIC-III embeddings (mimic) does not ex-
tend the vocabulary, and it brings no improvement. On
the other hand, the B-LSTM has improved by 0.3 pp
with the cc/mimic embeddings. Even though the mimic
embeddings do not cover signiﬁcant extra vocabulary,
they may have enriched the feature space. Conversely,
the cc/mimic embeddings have provided no improve-
ments with the B-LSTM-CRF. For this, we need to take
into account that the BLSTM-CRF already has a high
score (83.35% F1-score). Consequently, it may be more
diﬃcult to improve its results. Using conventional fea-
ture engineering has led to lower accuracy (77.81% F1-
score). Eventually, concatenating both the features and
the pre-trained embeddings showed no improvement
over the best model. Table 3a also shows the impor-
tance of using a ﬁnal CRF layer in the B-LSTM-CRF,
given that the B-LSTM alone was only able to achieve
a 77.59% F1 score. At its turn, the CRF baseline has
only obtained a 64.09% F1 score in its best conﬁgura-
tion, lower than any version of the LSTM.

5.4.2. DNR results over the DrugBank and MedLine

datasets

In the DNR task (Table 3b),

the proposed B-
LSTM-CRF with the concatenated word embeddings
(cc/mimic) and the character-level embeddings (char)
has improved over all the previous approaches on both
DrugBank (88.38% F1 score) and MedLine (60.66% F1
score). Table 4 shows that only 49% of the words in the
datasets have been found in the cc embeddings. How-
ever, when the concatenated embeddings (cc/mimic)
are used, the percentage of found words has increased
to 67% for DrugBank and 61% for MedLine, lead-
ing to better results in the classiﬁcation task. Words
that appear in the MIMIC-III dataset but are not con-
tained in Common Crawl are typically very technical
and domain-speciﬁc, such as drug names or treatments;
examples include: pentostatin, sitagliptin, hydrobro-
mide, organophosphate, pyhisiological and methima-
zole. In total, 1189 extra words have been mapped in
DrugBank and 716 in MedLine thanks to the use of
MIMIC-III. However, the B-LSTM has only obtained
an accuracy improvement on the MedLine dataset, but
not on DrugBank. This can be explained by the fact

Common Crawl

i2b2/VA
DrugBank
MedLine

(cc)
99.99 %
49.50 %
49.10 %

Common Crawl
+ MIMIC-III
(cc+mimic)
99.99 %
67.02 %
61.51 %

Table 4: Percentage of words assigned with pre-trained embeddings
in the train, dev and test of the respective datasets

that the accuracy of the B-LSTM on MedLine is very
low (44.33%) and, therefore, easy to improve. Instead,
on DrugBank the accuracy of the B-LSTM is already
very high (84.35% F1-score) and thus diﬃcult to im-
prove. With the B-LSTM-CRF, results with extra vo-
cabulary covered by the cc/mimic embeddings have im-
proved with both datasets.

As for what concerns the hand-crafted features, their
use has led to higher accuracy than with the Com-
mon Crawl embeddings on the DrugBank dataset in
two cases. However, the concatenation of the features
and the pre-trained embeddings has not improved the
best results. As in the CCE task, the B-LSTM-CRF
model has proved better than the B-LSTM alone on
both DrugBank (88.38% vs 84.35% F1-score) and Med-
Line (60.66% vs 45.92% F1-score.) Finally, we can
see that the use of the character-level embeddings has
led to higher relative improvements for DrugBank than
for the other two datasets. A plausible explanation for
this is that this dataset contains more words with dis-
tinctive preﬁxes and suﬃxes which are more eﬀectively
captured by the character-level embeddings.

In general, the CRF has signiﬁcantly underperformed
compared to the neural networks. We speculate that this
model may require more extensive feature engineering
to achieve a comparable performance, or that it may not
be able to achieve it at all. In particular, we see that the
CRF has performed the worst with MedLine. A possible
explanation can be found in the “curse of dimension-
ality”: MedLine is a small dataset (1627 training sen-
tences), while the overall dimensionality of the input
embeddings is 746. This makes the learning problem
very sparse and seems to seriously aﬀect a linear model
such as the CRF. On the contrary, the non-linear internal
architecture of the neural networks may in some cases
help reduce the eﬀective dimensionality and mollify this
problem.

9

5.4.3. DNR results over the DrugBank and MedLine

datasets

Table 5a and 5b break down the results by entity class
for the best model on each dataset. With the MedLine
dataset, we can notice the poor performance at detect-
In DrugBank, the same issue occurs with
ing brand.
entity class drug n. This issue is likely attributable to
the small sample size. Instead, the i2b2/VA dataset all
entity classes are detected with similar F1-scores, likely
owing to the larger number of samples per class. How-
ever, we see that brand achieves the second best F1-
score in DrugBank despite its relatively low frequency
in the dataset, and that drug n obtains a very poor per-
formance in MedLine even if it has the second highest
frequency. We identify two other main factors that may
have a major impact on the accuracy: (1) the average
length of the entities in each class, and (2) the number
of test entities that had not been seen during the training
stage. In this respect, the brand and drug entities are
usually very short (average ∼ 1 word), while the group
and drug n entities often have multiple words. Since
shorter entities are easier to predict correctly, brand ob-
tains better accuracy than group in DrugBank. On the
other hand, the drug n and group entities have similar
length, but in MedLine drug n obtains a very poor per-
formance. This is most likely because no entity of type
drug n that appears in the test set had been seen dur-
ing training. Conversely, a large percentage of the test
group entities had been seen during training and have
therefore proved easier to predict.

6. Conclusion

In this paper, we have set to investigate the eﬀec-
tiveness of the Bidirectional LSTM and Bidirectional
LSTM-CRF –two speciﬁc architectures of recurrent
neural networks– for drug name recognition and clini-
cal concept extraction, and compared them with a base-
line CRF model. As input features, we have applied
combinations of diﬀerent word embeddings (Common
Crawl and MIMIC-III), character-level embedding and
conventional feature engineering. We have showed that
the neural network models have obtained signiﬁcantly
better results than the CRF, and reported state-of-the-
art results over the i2b2/VA, DrugBank and MedLine
datasets using the B-LSTM-CRF model. We have also
provided evidence that retraining GloVe on a domain-
speciﬁc dataset such as MIMIC-III can help learn vec-
tor representations for domain-speciﬁc words and in-
crease the classiﬁcation accuracy. Finally, we have
showed that adding hand-crafted features does not fur-
ther improve performance since the neural networks can

B-LSTM-CRF+(cc)+(char)

Entity
problem
test
treatment

Precision Recall
83.62
81.29
85.01
84.74
83.55
83.36

F1-score
82.44
84.87
83.46

(a) i2b2/VA

Entity

group
drug
brand
drug n

DrugBank
Precision Recall
87.88
81.69
89.56
94.77
90.57
84.21
00.00
00.00

F1-score
84.67
91.83
87.27
00.00

MedLine
Precision Recall
60.22
69.14
77.33
73.89
16.67
100.00
25.57
68.18

F1-score
64.37
75.57
28.57
37.19

B-LSTM-CRF
+(cc+mimic)
+(char)

(b) DrugBank and MedLine

Table 5: Results by class for the B-LSTM-CRF with character-level and cc/mimic embeddings

learn useful word representations automatically from
pre-trained word embeddings. Consequently,
time-
consuming, domain-speciﬁc feature engineering can be
usefully avoided.

References

[1] I. Segura-Bedmar, V. Su´arez-Paniagua, P. Martınez, Exploring
in: 6th Interna-
word embedding for drug name recognition,
tional Workshop on Health Text Mining and Information Analy-
sis (LOUHI), (2015).

[2] A. B. Abacha, M. F. M. Chowdhury, A. Karanasiou, Y. Mrabet,
A. Lavelli, P. Zweigenbaum, Text mining for pharmacovigi-
lance: Using machine learning for drug name recognition and
drug–drug interaction extraction and classiﬁcation, Journal of
biomedical informatics 58 (2015) 122–132.

[3] T. Rockt¨aschel, T. Huber, M. Weidlich, U. Leser, Wbi-ner: The
impact of domain-speciﬁc features on the performance of iden-
tifying and classifying mentions of drugs, in: Second Joint Con-
ference on Lexical and Computational Semantics (* SEM), vol-
ume 2, (2013), pp. 356–363.

[4] B. de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, X. Zhu,
Machine-learned solutions for three stages of clinical informa-
tion extraction: the state of the art at i2b2 2010, Journal of the
American Medical Informatics Association 18 (2011) 557–562.
[5] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami,
C. Dyer, Neural architectures for named entity recognition,
arXiv preprint arXiv:1603.01360 (2016).

[6] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al., Deep
neural networks for acoustic modeling in speech recognition:
IEEE Signal Pro-
The shared views of four research groups,
cessing Magazine 29 (2012) 82–97.

[7] A. Krizhevsky, I. Sutskever, G. E. Hinton,

Imagenet classiﬁ-
in: Advances
cation with deep convolutional neural networks,
in neural information processing systems (NIPS), (2012), pp.
1097–1105.

[8] J. Pennington, R. Socher, C. D. Manning, Glove: Global vectors
in: Empirical Methods on Natural
for word representation.,
Language Processing (EMNLP), volume 14, (2014), pp. 1532–
1543.

[9] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Dis-
tributed representations of words and phrases and their composi-
tionality, in: Advances in neural information processing systems
(NIPS), (2013), pp. 3111–3119.

[10] A. E. Johnson, T. J. Pollard, L. Shen, L.-W. H. Lehman,
M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. A. Celi, R. G.
Mark, Mimic-iii, a freely accessible critical care database, Sci-
entiﬁc data 3 (2016).

[11] R. Chalapathy, E. Z. Borzeshi, M. Piccardi, An investigation of
recurrent neural architectures for drug name recognition, in: 7th
International Workshop on Health Text Mining and Information
Analysis (LOUHI), (2016).

[12] R. Chalapathy, E. Z. Borzeshi, M. Piccardi, Bidirectional lstm-
in: Clinical Natural Lan-

crf for clinical concept extraction,
guage Processing Workshop (ClinicalNLP), (2016).

[13] S. Liu, B. Tang, Q. Chen, X. Wang, X. Fan, Feature engineer-
ing for drug name recognition in biomedical texts: Feature con-
junction and feature selection, Computational and mathematical
methods in medicine (2015).

[14] W. Boag, K. Wacome, T. Naumann, A. Rumshisky, Cliner: A
lightweight tool for clinical named entity recognition, AMIA
Joint Summits on Clinical Research Informatics (poster) (2015).
[15] R. Lebret, R. Collobert, Word emdeddings through hellinger
pca, in: European Chapter of the Association for Computational
Linguistics (EACL), (2013).

[16] A. Nikfarjam, A. Sarker, K. OConnor, R. Ginn, G. Gonzalez,
Pharmacovigilance from social media: mining adverse drug re-
action mentions using sequence labeling with word embedding
cluster features, Journal of the American Medical Informatics
Association (2015).

[17] Y. Wu, J. Xu, M. Jiang, Y. Zhang, H. Xu, A study of neural
word embeddings for named entity recognition in clinical text,
in: AMIA Annual Symposium Proceedings, (2015), p. 1326.

[18] F. Dernoncourt, J. Y. Lee,

¨O. Uzuner, P. Szolovits, De-
identiﬁcation of patient notes with recurrent neural networks,
Journal of the American Medical Informatics Association 24
(2017) 596–606.

[19] A. Cocos, A. G. Fiks, A. J. Masino, Deep learning for pharma-
covigilance: recurrent neural network architectures for labeling
adverse drug reactions in twitter posts, Journal of the American
Medical Informatics Association 24 (2017) 813–821.

[20] J. Xie, X. Liu, D. Dajun Zeng, Mining e-cigarette adverse
events in social media using bi-lstm recurrent neural network

10

with word embedding representation, Journal of the American
Medical Informatics Association 25 (2017) 72–80.

[21] Q. Wei, T. Chen, R. Xu, Y. He, L. Gui, Disease named entity
recognition by combining conditional random ﬁelds and bidi-
rectional recurrent neural networks, Database (2016).

[22] A. N. Jagannatha, H. Yu, Structured prediction models for rnn
based sequence labeling in clinical text,
in: Proceedings of
the Conference on Empirical Methods in Natural Language Pro-
cessing. Conference on Empirical Methods in Natural Language
Processing, volume 2016, NIH Public Access, p. 856.

[23] M. Gridach, Character-level neural network for biomedical
named entity recognition, Journal of biomedical informatics 70
(2017) 85–91.

[24] J. Y. Lee, F. Dernoncourt, O. Uzuner, P. Szolovits, Feature-
augmented neural networks for patient note de-identiﬁcation,
in: Clinical Natural Language Processing Workshop (Clini-
calNLP), (2016).

[25] J. Laﬀerty, A. McCallum, F. Pereira, et al., Conditional ran-
dom ﬁelds: Probabilistic models for segmenting and labeling
sequence data, in: Proceedings of the eighteenth international
conference on machine learning, ICML, (2001), volume 1, pp.
282–289.

[26] Hcrf, http://multicomp.ict.usc.edu/?p=790, 2017. Ac-

cessed: 2017-05-01.

[27] Y. Bengio, P. Simard, P. Frasconi, Learning long-term depen-
dencies with gradient descent is diﬃcult, IEEE transactions on
neural networks 5 (1994) 157–166.

[28] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neu-

ral computation 9 (1997) 1735–1780.

[29] Glove,

https://nlp.stanford.edu/projects/glove/,

[30]

2014. Accessed: 2017-05-01.
¨O. Uzuner, B. R. South, S. Shen, S. L. DuVall, 2010 i2b2/va
challenge on concepts, assertions, and relations in clinical text,
Journal of the American Medical Informatics Association 18
(2011) 552–556.

[31] M. Herrero-Zazo, I. Segura-Bedmar, P. Mart´ınez, T. Declerck,
The ddi corpus: An annotated corpus with pharmacological sub-
stances and drug–drug interactions, Journal of biomedical infor-
matics 46 (2013) 914–920.

[32] D. Nadeau, S. Sekine, A survey of named entity recognition and
classiﬁcation, Lingvisticae Investigationes 30 (2007) 3–26.
[33] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu,
G. Desjardins, J. Turian, D. Warde-Farley, Y. Bengio, Theano:
A cpu and gpu math compiler in python, in: Proceedings of the
9th Python in Science Conference, (2010), pp. 1–7.

[34] J. Bergstra, Y. Bengio, Random search for hyper-parameter op-
timization, Journal of Machine Learning Research 13 (2012)
281–305.

[35] N. Srivastava, G. Hinton, A. Krizhevsky,

I. Sutskever,
R. Salakhutdinov, Dropout: A simple way to prevent neural
networks from overﬁtting, The Journal of Machine Learning
Research 15 (2014) 1929–1958.

[36] X. Fu, S. Ananiadou, Improving the extraction of clinical con-
cepts from clinical records, Proceedings of BioTxtM14 (2014).

11

