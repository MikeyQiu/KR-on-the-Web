Dependency or Span, End-to-End Uniform Semantic Role Labeling

Zuchao Li1,2,∗ , Shexia He1,2,∗, Hai Zhao1,2,†, Yiqing Zhang1,2, Zhuosheng Zhang1,2,
Xi Zhou3, Xiang Zhou3
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China
3CloudWalk Technology, Shanghai, China
{charlee,heshexia}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,
{zhangyiqing,zhangzs}@sjtu.edu.cn, {zhouxi,zhouxiang}@cloudwalk.cn

9
1
0
2
 
n
a
J
 
6
1
 
 
]
L
C
.
s
c
[
 
 
1
v
0
8
2
5
0
.
1
0
9
1
:
v
i
X
r
a

Abstract

Semantic role labeling (SRL) aims to discover the predicate-
argument structure of a sentence. End-to-end SRL without
syntactic input has received great attention. However, most
of them focus on either span-based or dependency-based se-
mantic representation form and only show speciﬁc model op-
timization respectively. Meanwhile, handling these two SRL
tasks uniformly was less successful. This paper presents an
end-to-end model for both dependency and span SRL with
a uniﬁed argument representation to deal with two differ-
ent types of argument annotations in a uniform fashion. Fur-
thermore, we jointly predict all predicates and arguments,
especially including long-term ignored predicate identiﬁca-
tion subtask. Our single model achieves new state-of-the-art
results on both span (CoNLL 2005, 2012) and dependency
(CoNLL 2008, 2009) SRL benchmarks.

Introduction
The purpose of semantic role labeling (SRL) is to derive
the meaning representation for a sentence, which is bene-
ﬁcial to a wide range of natural language processing (NLP)
tasks (Wang et al. 2016; Zhang et al. 2018). SRL can be
formed as four subtasks, including predicate detection, pred-
icate disambiguation, argument identiﬁcation and argument
classiﬁcation. For argument annotation, there are two for-
mulizations. One is based on text spans, namely span-based
SRL. The other is dependency-based SRL, which annotates
the syntactic head of argument rather than entire argument
span. Figure 1 shows example annotations.

Great progress has been made in syntactic parsing (Dozat
and Manning 2017; Li et al. 2018a; Li et al. 2018c). Most
traditional SRL methods rely heavily on syntactic features.
To alleviate the inconvenience, recent works (Zhou and Xu
2015; Marcheggiani, Frolov, and Titov 2017; He et al. 2017;
Tan et al. 2018; He et al. 2018a; He et al. 2018b; Cai et al.

∗ These authors made equal contribution.† Corresponding au-
thor. This paper was partially supported by National Key Research
and Development Program of China (No. 2017YFB0304100), Na-
tional Natural Science Foundation of China (No. 61672343 and No.
61733011), Key Project of National Society Science Foundation
of China (No. 15-ZDA041), The Art and Science Interdisciplinary
Funds of Shanghai Jiao Tong University (No. 14JCRZ04).
Copyright c(cid:13) 2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Examples of annotations in span (above) and de-
pendency (below) SRL.

2018) propose end-to-end models for SRL, putting syntax
aside and still achieving favorable results. However, these
systems focus on either span or dependency SRL, which mo-
tivates us to explore a uniform approach.

Both span and dependency are effective formal represen-
tations for semantics, though for a long time it has been kept
unknown which form, span or dependency, would be better
for the convenience and effectiveness of semantic machine
learning and later applications. Furthermore, researchers are
interested in two forms of SRL models that may beneﬁt from
each other rather than their separated development. This
topic has been roughly discussed in (Johansson and Nugues
2008a), who concluded that the (best) dependency SRL sys-
tem at then clearly outperformed the span-based (best) sys-
tem through gold syntactic structure transformation. How-
ever, Johansson and Nugues (2008a) like all other traditional
SRL models themselves had to adopt rich syntactic features,
and their comparison was done between two systems in quite
different building styles. Instead, this work will develop full
syntax-agnostic SRL systems with the same fashion for both
span and dependency representation, so that we can revisit
this issue under a more solid empirical basis.

In addition, most efforts focus on argument identiﬁcation
and classiﬁcation since span and dependency SRL corpora
have already marked predicate positions. Although no pred-
icate identiﬁcation is needed, it is not available in many
downstream applications. Therefore, predicate identiﬁcation
should be carefully handled in a complete practical SRL sys-
tem. To address this problem, He et al. (2018a) proposed
an end-to-end approach for jointly predicting predicates and
arguments for span SRL. Likewise, Cai et al. (2018) in-
troduced an end-to-end model to naturally cover all predi-
cate/argument identiﬁcation and classiﬁcation subtasks for
dependency SRL.

To jointly predict predicates and arguments, we present an

Span (CoNLL 2005)

Dependency (CoNLL 2009)

Time System
2008
2008

Punyakanok et al.
Toutanova et al.

SA ST Method
+
+

ILP
DP

2015

2015

FitzGerald et al.

+

structured

Zhou and Xu

deep BiLSTM

Time System
2009
Zhao et al.
2010 Bj¨orkelund et al.

SA ST Method
+
+

ME
global

structured

+

+

+

2017 He et al.

highway BiLSTM 83.1

Tan et al.
Strubell et al.

2018
2018
2018 He et al. (a)

+

self-attention
self-attention
ELMo

2019

Li et al. (b) AAAI

ELMo+biafﬁne

2016 Roth and Lapata
2017 Marcheggiani et al.
2017 Marcheggiani and Titov
2018 He et al. (b)
2018 Cai et al.
2018

Li et al. (a)

F1
86.2
86.9

87.3

87.7
87.7
88.0
89.5
89.6
89.8

90.4

+

+

+
+

+

PathLSTM
BiLSTM

+
+ GCNs
ELMo
+
biafﬁne
ELMo

+

ELMo+biafﬁne

F1
76.3
79.7

79.4

82.8

84.8
83.9
87.4

87.7

Table 1: A chronicle of related work for span and dependency SRL. SA represents syntax-aware system (no + indicates syntax-
agnostic system) and ST indicates sequence tagging model. F1 is the result of single model on ofﬁcial test set.

end-to-end framework for both span and dependency SRL.
Our model extends the span SRL model of He et al. (2018a),
directly regarding all words in a sentence as possible predi-
cates, considering all spans or words as potential arguments
and learning distributions over possible predicates. How-
ever, we differ by (1) introducing uniﬁed argument repre-
sentation to handle two different types of SRL tasks, and (2)
employing biafﬁne scorer to make decisions for predicate-
argument relationship.

The proposed models are evaluated on span SRL datasets:
CoNLL 2005 and 2012 data, as well as the dependency SRL
dataset of CoNLL 2008 and 2009 shared tasks. For span
SRL, our single model outperforms the previous best re-
sults by 0.3% and 0.5% F1-score on CoNLL 2005 and 2012
test sets respectively. For dependency SRL, we achieve new
state-of-the-art of 85.3% F1 and 90.4% F1 on CoNLL 2008
and 2009 benchmarks respectively.

Background

SRL is pioneered by Gildea and Jurafsky (2002), which uses
the PropBank conventions (Palmer, Gildea, and Kingsbury
2005). Conventionally, span SRL consists of two subtasks,
argument identiﬁcation and classiﬁcation. The former iden-
tiﬁes the arguments of a predicate, and the latter assigns
them semantic role labels, namely, determining the relation
between arguments and predicates. The PropBank deﬁnes
a set of semantic roles to label arguments, falling into two
categories: core and non-core roles. The core roles (A0-A5
and AA) indicate different semantics in predicate-argument
structure, while the non-core roles are modiﬁers (AM-adj)
where adj speciﬁes the adjunct type, such as temporal (AM-
TMP) and locative (AM-LOC) adjuncts. For example shown
in Figure 1, A0 is a proto-agent, representing the borrower.
Slightly different from span SRL in argument annotation,
dependency SRL labels the syntactic heads of arguments
rather than phrasal arguments, which was popularized by

CoNLL-2008 and CoNLL-2009 shared tasks1 (Surdeanu et
al. 2008; Hajiˇc et al. 2009). Furthermore, when no predicate
is given, two other indispensable subtasks of dependency
SRL are predicate identiﬁcation and disambiguation. One is
to identify all predicates in a sentence, and the other is to
determine the senses of predicates. As the example shown
in Figure 1, 01 indicates the ﬁrst sense from the PropBank
sense repository for predicate borrowed in the sentence.

Related Work
The traditional approaches on SRL were mostly about de-
signing hand-crafted feature templates and then employ lin-
ear classiﬁers such as (Pradhan et al. 2005; Punyakanok,
Roth, and Yih 2008; Zhao et al. 2009). Even though neu-
ral models were introduced, early work still paid more at-
tention on syntactic features. For example, FitzGerald et al.
(2015) integrated syntactic information into neural networks
with embedded lexicalized features, while Roth and Lapata
(2016) embedded syntactic dependency paths between pred-
icates and arguments. Similarly, Marcheggiani and Titov
(2017) leveraged the graph convolutional network to encode
syntax for dependency SRL. Recently, Strubell et al. (2018)
presented a multi-task neural model to incorporate auxiliary
syntactic information for SRL, Li et al. (2018b) adopted sev-
eral kinds of syntactic encoder for syntax encoding while He
et al. (2018b) used syntactic tree for argument pruning.

However, using syntax may be quite inconvenient some-
times, recent studies thus have attempted to build SRL sys-
tems without or with little syntactic guideline. Zhou and
Xu (2015) proposed the ﬁrst syntax-agnostic model for span
SRL using LSTM sequence labeling, while He et al. (2017)
further enhanced their model using highway bidirectional
LSTMs with constrained decoding. Later, Tan et al. (2018)

1CoNLL-2008 is an English-only task, while CoNLL-2009 ex-
tends to a multilingual one. Their main difference is that predicates
have been beforehand indicated for the latter. Or rather, CoNLL-
2009 does not need predicate identiﬁcation, but it is an indispens-
able subtask for CoNLL-2008.

presented a deep attentional neural network for applying
self-attention to span SRL task. Likewise for dependency
SRL, Marcheggiani, Frolov, and Titov (2017) proposed a
syntax-agnostic model with effective word representation
and obtained favorable results. Cai et al. (2018) built a full
end-to-end model with biafﬁne attention and outperformed
the previous state-of-the-art.

More recently, joint predicting both predicates and ar-
guments has attracted extensive interest on account of the
importance of predicate identiﬁcation, including (He et al.
2017; Strubell et al. 2018; He et al. 2018a; Cai et al. 2018)
and this work. In our preliminary experiments, we tried to
integrate the self-attention into our model, but it does not
provide any signiﬁcant performance gain on span or depen-
dency SRL, which is not consistent with the conclusion in
(Tan et al. 2018) and lets us exclude it from this work.

Generally, the above work is summarized in Table 1. Con-
sidering motivation, our work is most closely related to the
work of FitzGerald et al. (2015), which also tackles span and
dependency SRL in a uniform fashion. The essential differ-
ence is that their model employs the syntactic features and
takes pre-identiﬁed predicates as inputs, while our model
puts syntax aside and jointly learns and predicts predicates
and arguments.

Uniform End-to-End Model

Overview

Given a sentence s = w1, w2, . . . , wn, we attempt to predict
a set of predicate-argument-relation tuples Y ∈ P × A × R,
where P = {w1, w2, ..., wn} is the set of all possible pred-
icate tokens, A = {(wi, . . . , wj)|1 ≤ i ≤ j ≤ n} includes
all the candidate argument spans or dependencies2, and R
is the set of the semantic roles. To simplify the task, we
introduce a null label (cid:15) to indicate no relation between ar-
bitrary predicate-argument pair following He et al. (2018a).
As shown in Figure 2, our uniform SRL model includes four
main modules:

• token representation component to build token represen-

tation xi from word wi,

• a BiHLSTM encoder that directly takes sequential in-

puts,

• predicate and argument representation module to learn

candidate representations,

• a biafﬁne scorer which takes the candidate representa-

tions as input and predicts semantic roles.

Token Representation

We follow the bi-directional LSTM-CNN architecture (Chiu
and Nichols 2016), where convolutional neural networks
(CNNs) encode characters inside a word w into character-
level representation wchar then concatenated with its word-
level wword into context-independent representation. To fur-
ther enhance the word representation, we leverage an ex-
ternal representation welmo from pretrained ELMo (Embed-
dings from Language Models) layers according to Peters et

2When i=j, it means span degrades to dependency.

al. (2018). Eventually, the resulting token representation is
concatenated as

x = [wchar, wword, welmo].

Deep Encoder
The encoder in our model adopts the bidirectional LSTM
with highway connections (BiHLSTM) to contextualize the
representation into task-speciﬁc representation: xc
i ∈ X c :
X c = BiHLSTM({xi}), where the gated highway connec-
tions is used to alleviate the vanishing gradient problem
when training very deep BiLSTMs.

Predicate and Argument Representation
We employ contextualized representations for all candidate
arguments and predicates. As referred in (Dozat and Man-
ning 2017), applying a multi-layer perceptron (MLP) to the
recurrent output states before the classiﬁer has the advan-
tage of stripping away irrelevant information for the current
decision. Therefore, to distinguish the currently considered
predicate from its candidate arguments in SRL context, we
add an MLP layer to contextualized representations for ar-
gument ga and predicate gp candidates speciﬁc representa-
tions respectively with ReLU (Nair and Hinton 2010) as its
activation function:

ga = ReLU(MLPa(X c))
gp = ReLU(MLPp(X c))
To perform uniform SRL, we introduce uniﬁed argu-
ment representation. For dependency SRL, we assume sin-
gle word argument span by limiting the length of candidate
argument to be 1, so our model uses the ga as the ﬁnal ar-
gument representation ga
f directly. While for span SRL, we
utilize the approach of span representation from Lee et al.
(2017). Each candidate span representation ga

f is built by

END, hλ, size(λ)],

ga
f = [ga
ST ART and ga

START, ga
where ga
EN D are boundary representations, λ
indicates a span, size(λ) is a feature vector encoding the
size of span, and hλ is the speciﬁc notion of headedness
which is learned by attention mechanism (Bahdanau, Cho,
and Bengio 2014) over words in each span (where t is the
position inside span) as follows :
t = wattn · MLPattn(ga
µa
t )

νt =

exp(µa
t )
k=START exp(µa
k)

(cid:80)END

hλ =

END
(cid:88)

t=START

νt · ga
t

φp = wpMLPs
φa = waMLPs

p(gp),
a(ga

f ).

Scorers
For predicate and arguments, we introduce two unary scores
on their candidates:

Figure 2: The framework of our end-to-end model for uniform SRL.

For semantic role, we adopt a relation scorer with biafﬁne
attention (Dozat and Manning 2017):

the sentence s, which can be factorized as:

Φr(p, a) = Biafﬁne(gp, ga
f )
= {gp
+ WT

t }T W1ga
f
2 (gp
t ⊕ ga
f ) + b

(1)

(2)

where W1 and W2 respectively denote the weight matrix of
the bi-linear and the linear terms and b is the bias item.

The biafﬁne scorer differs from feed-forward networks
scorer in bilinear transformation. Since SRL can be regarded
as a classiﬁcation task, the distribution of classes is uneven
and the problem comes worse after the null labels are intro-
duced. The output layer of the model normally includes a
bias term designed to capture the prior probability of each
class, with the rest of the model focusing on learning the
likelihood of every classes occurring in data. The biafﬁne at-
tention as Dozat and Manning (2017) in our model directly
assigns a score for each speciﬁc semantic role and would
be helpful for semantic role prediction. Actually, (He et al.,
2018a) used a scorer as Equation (2), which is only a part of
our scorer including both Equations (1) and (2). Therefore,
our scorer would be more informative than previous models
such as (He et al. 2018a).

Training Objective

The model is trained to optimize the probability Pθ(ˆy|s) of
the predicate-argument-relation tuples ˆy(p,a,r) ∈ Y given

(cid:89)

(cid:89)

Pθ(y|s) =

Pθ(y(p,a,r)|s)

p∈P,a∈A,r∈R

=

p∈P,a∈A,r∈R

φ(p, a, r)
ˆr∈R φ(p, a, ˆr)

(cid:80)

where θ represents the model parameters, and φ(p, a, r) =
φp + φa + Φr(p, a), is the score for the predicate-argument-
relation tuple, including predicate score φp, argument score
φa and relation score Φr(p, a).

Our model adopts a biafﬁne scorer for semantic role la-
bel prediction, which is implemented as cross-entropy loss.
Moreover, our model is trained to minimize the negative
likehood of the golden structure y: J (s) = − log Pθ(y|s).
The score of null labels are enforced into φ(p, a, (cid:15)) = 0.
For predicates and arguments prediction, we train separated
scorers (φp and φa) in parallel fed to the biafﬁne scorer
for predicate and argument predication respectively, which
helps to reduce the chance of error propagation.

Candidates Pruning
The number of candidate arguments for a sentence of length
n is O(n2) for span SRL, and O(n) for dependency. As the
model deals with O(n) possible predicates, the computa-
tional complexity is O(n3 · |R|) for span, O(n2 · |R|) for
dependency, which is too computationally expensive.

To address this issue, we attempt to prune candidates us-
ing two beams for storing the candidate arguments and pred-
icates with size βpn and βan inspired by He et al. (2018a),

End-to-End

He et al. (2017) (Single)
Strubell et al. (2018)
He et al. (2018a)
Ours (Single)

He et al. (2017) (Ensemble)

CoNLL-2005 WSJ CoNLL-2005 Brown CoNLL-2012 (OntoNotes)

P

80.2
83.7
84.8
85.2

82.0

R

82.3
83.7
87.2
87.5

83.4

F1
81.2
83.7
86.0
86.3

82.7

P

67.6
72.6
73.9
74.7

69.7

R

69.6
69.7
78.4
78.1

70.5

F1
68.5
71.1
76.1
76.4

70.1

P

78.6
80.7
81.9
84.9

80.2

R

75.1
79.1
84.0
81.4

76.6

F1
76.8
79.9
82.9
83.1

78.4

Table 2: End-to-end span SRL results on CoNLL-2005 and CoNLL-2012 data, compared with previous systems in terms of
precision (P), recall (R), F1-score. The CoNLL-2005 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).

Systems

WSJ

Brown

P

−
J & N (2008b)
Zhao et al. (2009) −
Zhao et al. (2013) −
He et al. (2018b)
Cai et al.(2018)
Ours

83.9 82.7
84.7 85.2
84.5 86.1

P

R

F1
− 81.75 −
− 82.1 −
− 82.5 −
83.3 −
85.0 −
85.3

R

F1

− 69.06
−
−
−
−
−
−
− 72.5
74.2

74.6 73.8

Table 3: Dependency SRL results on CoNLL-2008 test sets.

where βp and βa are two manually setting thresholds. First,
the predicate and argument candidates are ranked according
to their predicted score (φp and φa) respectively, and then
we reduce the predicate and argument candidates with de-
ﬁned beams. Finally, we take the candidates from the beams
to participate the label prediction. Such pruning will reduce
the overall number of candidate tuples to O(n2 · |R|) for
both types of tasks. Furthermore, for span SRL, we set the
maximum length of candidate arguments to L, which may
decrease the number of candidate arguments to O(n).

SRL Constraints
According to PropBank semantic convention, predicate-
argument structure has to follow a few of global constraints
(Punyakanok, Roth, and Yih 2008; He et al. 2017), we thus
incorporate constraints on the output structure with a dy-
namic programing decoder during inference. These con-
straints are described as follows:

• Unique core roles (U): Each core role (A0-A5, AA)

should appear at most once for each predicate.

• Continuation roles (C): A continuation role C-X can ex-

ist only when its base role X is realized before it.

• Reference roles (R): A reference role R-X can exist only
when its base role X is realized (not necessarily before R-X).
• Non-overlapping (O): The semantic arguments for the

same predicate do not overlap in span SRL.

As C and R constraints lead to worse performance in our
models from our preliminary experiments, we only enforce
U and O constraints on span SRL and U constraints on de-
pendency SRL3.

Experiments
Our models4 are evaluated on two PropBank-style SRL
tasks: span and dependency. For span SRL, we test model
on the common span SRL datasets from CoNLL-2005 (Car-
reras and M`arquez 2005) and CoNLL-2012 (Pradhan et al.
2013) shared tasks. For dependency SRL, we experiment on
CoNLL 2008 (Surdeanu et al. 2008) and 2009 (Hajiˇc et al.
2009) benchmarks. As for the predicate disambiguation in
dependency SRL task, we follow the previous work (Roth
and Lapata 2016).
We consider

two SRL setups: end-to-end and pre-
identiﬁed predicates. For the former setup, our system
jointly predicts all the predicates and their arguments in one
shot, which turns into CoNLL-2008 setting for dependency
SRL. In order to compare with previous models, we also re-
port results with pre-identiﬁed predicates, where predicates
have been beforehand identiﬁed in corpora. Therefore, the
experimental results fall into two categories: end-to-end re-
sults and results with pre-identiﬁed predicates.

Datasets
CoNLL 2005 and 2012
The CoNLL-2005 shared task
focused on verbal predicates only for English. The CoNLL-
2005 dataset takes section 2-21 of Wall Street Journal (WSJ)
data as training set, and section 24 as development set. The
test set consists of section 23 of WSJ for in-domain evalua-
tion together with 3 sections from Brown corpus for out-of-
domain evaluation. The larger CoNLL-2012 dataset is ex-
tracted from OntoNotes v5.0 corpus, which contains both
verbal and nominal predicates.
CoNLL 2008 and 2009
CoNLL-2008 and the English
part of CoNLL-2009 shared tasks use the same English cor-
pus, which merges two treebanks, PropBank and NomBank.
NomBank is a complement to PropBank with similar seman-
tic convention for nominal predicate-argument structure an-
notation. Besides, the training, development and test splits
of English data are identical to that of CoNLL-2005.

Setup
Hyperparameters
the word em-
beddings are 300-dimensional GloVe vectors (Pennington,

In our experiments,

it may be regarded as length 1 sized span.

4Our code is available here: https://github.com/

3O constraint will be automatically satisﬁed for dependency, as

bcmi220/unisrl.

System

Pradhan et al. (2013) (Revised)
Zhou and Xu (2015)
He et al. (2017)
Tan et al. (2018)
Peters et al. (2018)
Strubell et al. (2018)
He et al. (2018a)
Ours

Punyakanok et al. (2008)
Toutanova et al. (2008)
FitzGerald et al. (2015)
He et al. (2017)
Tan et al. (2018)

CoNLL-2005 WSJ CoNLL-2005 Brown CoNLL-2012 (OntoNotes)

P

−
82.9
83.1
84.5
−
83.9
−
87.9

82.3
81.9
82.5
85.0
85.9

R

−
82.8
83.0
85.2
−
83.9
−
87.5

76.8
78.8
78.2
84.3
86.3

F1
−
82.8
83.1
84.8
−
83.9
87.4
87.7

79.4
80.3
80.3
84.6
86.1

P

−
70.7
72.9
73.5
−
73.3
−
80.6

73.4
−
74.5
74.9
74.6

R

−
68.2
71.4
74.6
−
71.8
−
80.4

62.9
−
70.0
72.4
75.0

F1
−
69.4
72.1
74.1
−
72.5
80.4
80.5

67.8
68.8
72.2
73.6
74.8

P

78.5
−
81.7
81.9
−
−
−
85.7

−
−
81.2
83.5
83.3

R

76.6
−
81.6
83.6
−
−
−
86.3

−
−
79.0
83.3
84.5

F1
77.5
81.3
81.7
82.7
84.6
−
85.5
86.0

−
−
80.1
83.4
83.9

Single

Ensemble

Table 4: Span SRL results with pre-identiﬁed predicates on CoNLL-2005 and CoNLL-2012 test sets.

Socher, and Manning 2014). The character representations
with dimension 8 randomly initialized. In the character
CNN, the convolutions have window sizes of 3, 4, and 5,
each consisting of 50 ﬁlters. Moreover, we use 3 stacked
bidirectional LSTMs with 200 dimensional hidden states.
The outputs of BiLSTM employs two 300-dimensional MLP
layers with the ReLU as activation function. Besides, we
use two 150-dimensional hidden MLP layers with ReLU to
score predicates and arguments respectively. For candidates
pruning, we follow the settings of He et al. (2018a), model-
ing spans up to length L = 30 for span SRL and L = 1 for
dependency SRL, using βp = 0.4 for pruning predicates and
βa = 0.8 for pruning arguments.
Training Details During training, we use the categorical
cross-entropy as objective, with Adam optimizer (Kingma
and Ba 2015) initial learning rate 0.001. We apply 0.5
dropout to the word embeddings and character CNN out-
puts and 0.2 dropout to all hidden layers and feature embed-
dings. In the LSTMs, we employ variational dropout masks
that are shared across timesteps (Gal and Ghahramani 2016),
with 0.4 dropout rate. All models are trained for up to 600
epochs with batch size 40 on a single NVIDIA GeForce
GTX 1080Ti GPU, which occupies 8 GB graphic memory
and takes 12 to 36 hours.

End-to-end Results

We present all results using the ofﬁcial evaluation script
from the CoNLL-2005 and CoNLL-2009 shared tasks, and
compare our model with previous state-of-the-art models.
Span SRL Table 2 shows results on CoNLL-2005 in-
domain (WSJ) and out-of-domain (Brown) test sets, as well
as the CoNLL-2012 test set (OntoNotes). The upper part of
table presents results from single models. Our model out-
performs the previous models with absolute improvements
in F1-score of 0.3% on CoNLL-2005 benchmark. Besides,
our single model performs even much better than all previ-
ous ensemble systems.
Dependency SRL Table 3 presents the results on CoNLL-

2008. J & N (2008b) (Johansson and Nugues 2008b) was
the highest ranked system in CoNLL-2008 shared task. We
obtain comparable results with the recent state-of-the-art
method (Cai et al. 2018), and our model surpasses the model
(He et al. 2018b) by 2% in F1-score.

Results with Pre-identiﬁed Predicates
To compare with to previous systems with pre-identiﬁed
predicates, we report results from our models as well.
Span SRL Table 4 shows that our model outperforms all
published systems, even the ensemble model (Tan et al.
2018), achieving the best results of 87.7%, 80.5% and 86.0%
in F1-score respectively.
Dependency SRL Table 5 compares the results of depen-
dency SRL on CoNLL-2009 English data. Our single model
gives a new state-of-the-art result of 90.4% F1 on WSJ. For
Brown data, the proposed syntax-agnostic model yields a
performance gain of 1.7% F1 over the syntax-aware model
(Li et al. 2018b).

Ablation
To investigate the contributions of ELMo representations
and biafﬁne scorer in our end-to-end model, we conduct a
series of ablation studies on the CoNLL-2005 and CoNLL-
2008 WSJ test sets, unless otherwise stated.

Table 6 compares F1 scores of He et al. (2018a) and our
model without ELMo representations. We observe that ef-
fect of ELMo is somewhat surprising, where removal of
the ELMo dramatically declines the performance by 3.3-
3.5 F1 on CoNLL-2005 WSJ. However, our model gives
quite stable performance for dependency SRL regardless of
whether ELMo is concatenated or not. The results indicate
that ELMo is more beneﬁcial to span SRL.

In order to better understand how the biafﬁne scorer inﬂu-
ences our model performance, we train our model with dif-
ferent scoring functions. To ensure a fair comparison with
the model (He et al. 2018a), we replace the biafﬁne scorer
with their scoring functions implemented with feed-forward

System

CoNLL-2009 WSJ

CoNLL-2009 Brown

Zhao et al. (2009)
FitzGerald et al. (2015) (Struct.)
Roth and Lapata (2016) (Global)
Marcheggiani et al. (2017)
Marcheggiani and Titov (2017)
He et al. (2018b)
Cai et al. (2018)
Li et al. (2018b)
Ours

FitzGerald et al. (2015)
Roth and Lapata (2016)
Marcheggiani and Titov (2017)

P

−
−
90.0
88.7
89.1
89.7
89.9
90.3
89.6

−
90.3
90.5

R

−
−
85.5
86.8
86.8
89.3
89.2
89.3
91.2

−
85.7
87.7

F1
86.2
87.3
87.7
87.7
88.0
89.5
89.6
89.8
90.4

87.7
87.9
89.1

P

−
−
78.6
79.4
78.5
81.9
79.8
80.6
81.7

−
79.7
80.8

R

−
−
73.8
76.2
75.9
76.9
78.3
79.0
81.4

−
73.6
77.1

F1
74.6
75.2
76.1
77.7
77.2
79.3
79.0
79.8
81.5

75.5
76.5
78.9

Single

Ensemble

Table 5: Dependency SRL results with pre-identiﬁed predicates on CoNLL-2009 English benchmark.

System

CoNLL-2005 CoNLL-2008

Cai et al. (2018)

He et al. (2018a)
w/o ELMo

Ours
w/o ELMo
w/o biafﬁne scorer

−

86.0
82.5

86.3
83.0
85.8

85.0

−
−

85.3
85.1
83.7

Dep F1 Span-converted F1 ∆ F1
1.61
84.32
J & N
85.93
1.21
89.20
Our system 90.41

WSJ

WSJ+
Brown

J & N
84.29
Our system 88.91

83.45
88.23

0.84
0.68

Table 7: Dependency vs. Span-converted Dependency on
CoNLL 2005, 2009 test sets with dependency evaluation.

Table 6: Effectiveness of ELMo representations and biafﬁne
scorer on the CoNLL 2005 and 2008 WSJ sets.

networks, and the results of removing biafﬁne scorer are also
presented in Table 6. We can see 0.5% and 1.6% F1 per-
formance degradation on CoNLL 2005 and 2008 WSJ re-
spectively. The comparison shows that the biafﬁne scorer is
more effective for scoring the relations between predicates
and arguments. Furthermore, these results show that biafﬁne
attention mechanism is applicable to span SRL.

Dependency or Span?
It is very hard to say which style of semantic formal rep-
resentation, dependency or span, would be more convenient
for machine learning as they adopt incomparable evaluation
metric. Recent researches (Peng et al. 2018) have proposed
to learn semantic parsers from multiple datasets in Framenet
style semantics, while our goal is to compare the quality
of different models in the span and dependency SRL for
Propbank style semantics. Following Johansson and Nugues
(2008a), we choose to directly compare their performance in
terms of dependency-style metric through a transformation
way. Using the head-ﬁnding algorithm in (Johansson and
Nugues 2008a) which used gold-standard syntax, we may
determine a set of head nodes for each span. This process
will output an upper bound performance measure about the
span conversion due to the use of gold syntax.

We do not train new models for the conversion and the
resulted comparison. Instead, we do the job on span-style

CoNLL 2005 test set and dependency-style CoNLL 2009
test set (WSJ and Brown), considering these two test sets
share the same text content. As the former only contains ver-
bal predicate-argument structures, for the latter, we discard
all nomial predicate-argument related results and predicate
disambiguation results during performance statistics. Table
7 shows the comparison.

On a more strict setting basis, the results from our same
model for span and dependency SRL verify the same conclu-
sion of Johansson and Nugues (2008a), namely, dependency
form is in a favor of machine learning effectiveness for SRL
even compared to the conversion upper bound of span form.

Conclusion

This paper presents an end-to-end neural model for both
span and dependency SRL, which may jointly learn and pre-
dict all predicates and arguments. We extend existing model
and introduce uniﬁed argument representation with biafﬁne
scorer to the uniform SRL for both span and dependency
representation forms. Our model achieves new state-of-the-
art results on the CoNLL 2005, 2012 and CoNLL 2008,
2009 benchmarks. Our results show that span and depen-
dency SRL can be effectively handled in a uniform fashion,
which for the ﬁrst time enables us to conveniently explore
the useful connection between two types of semantic repre-
sentation forms.

References
[2014] Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural
machine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473.
[2010] Bj¨orkelund, A.; Bernd, B.; Hafdell, L.; and Nugues,
P. 2010. A high-performance syntactic and semantic depen-
dency parser. In COLING.
[2018] Cai, J.; He, S.; Li, Z.; and Zhao, H.
2018. A
full end-to-end semantic role labeler, syntactic-agnostic over
syntactic-aware? In COLING.
[2005] Carreras, X., and M`arquez, L. 2005.
Introduction
to the CoNLL-2005 shared task: Semantic role labeling. In
CoNLL.
[2016] Chiu, J. P., and Nichols, E. 2016. Named entity recog-
nition with bidirectional LSTM-CNNs. TACL.
[2017] Dozat, T., and Manning, C. D. 2017. Deep biafﬁne
attention for neural dependency parsing. In ICLR.
[2015] FitzGerald, N.; T¨ackstr¨om, O.; Ganchev, K.; and Das,
D. 2015. Semantic role labeling with neural network factors.
In EMNLP.
[2016] Gal, Y., and Ghahramani, Z. 2016. A theoretically
grounded application of dropout in recurrent neural net-
works. In NIPS.
[2002] Gildea, D., and Jurafsky, D. 2002. Automatic labeling
of semantic roles. Computational linguistics.
[2009] Hajiˇc, J.; Ciaramita, M.; Johansson, R.; Kawahara,
D.; Mart´ı, M. A.; M`arquez, L.; Meyers, A.; Nivre, J.; Pad´o,
S.; ˇStˇep´anek, J.; Straˇn´ak, P.; Surdeanu, M.; Xue, N.; and
Zhang, Y. 2009. The CoNLL-2009 shared task: Syntac-
tic and semantic dependencies in multiple languages.
In
CoNLL.
[2017] He, L.; Lee, K.; Lewis, M.; and Zettlemoyer, L. 2017.
Deep semantic role labeling: What works and what’s next. In
ACL.
[2018a] He, L.; Lee, K.; Levy, O.; and Zettlemoyer, L.
2018a. Jointly predicting predicates and arguments in neural
semantic role labeling. In ACL.
[2018b] He, S.; Li, Z.; Zhao, H.; Bai, H.; and Liu, G. 2018b.
Syntax for semantic role labeling, to be, or not to be. In ACL.
[2008a] Johansson, R., and Nugues, P. 2008a. Dependency-
based semantic role labeling of PropBank. In EMNLP.
[2008b] Johansson, R., and Nugues, P. 2008b. Dependency-
based syntactic-semantic analysis with PropBank and Nom-
Bank. In CoNLL.
[2015] Kingma, D., and Ba, J. 2015. Adam: A method for
stochastic optimization. In ICLR.
[2017] Lee, K.; He, L.; Lewis, M.; and Zettlemoyer, L. 2017.
End-to-end neural coreference resolution. In EMNLP.
[2018a] Li, Z.; Cai, J.; He, S.; and Zhao, H. 2018a. Seq2seq
dependency parsing. In COLING.
[2018b] Li, Z.; He, S.; Cai, J.; Zhang, Z.; Zhao, H.; Liu, G.;
Li, L.; and Si, L. 2018b. A uniﬁed syntax-aware framework
for semantic role labeling. In EMNLP.

[2018c] Li, Z.; He, S.; Zhang, Z.; and Zhao, H. 2018c. Joint
learning of pos and dependencies for multilingual universal
dependency parsing. CoNLL.
[2017] Marcheggiani, D., and Titov, I. 2017. Encoding sen-
tences with graph convolutional networks for semantic role
labeling. In EMNLP.
[2017] Marcheggiani, D.; Frolov, A.; and Titov, I. 2017.
A simple and accurate syntax-agnostic neural model for
dependency-based semantic role labeling. In CoNLL.
[2010] Nair, V., and Hinton, G. E. 2010. Rectiﬁed linear
units improve restricted boltzmann machines. In ICML.
[2005] Palmer, M.; Gildea, D.; and Kingsbury, P. 2005. The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics.
[2018] Peng, H.; Thomson, S.; Swayamdipta, S.; and Smith,
N. A. 2018. Learning joint semantic parsers from disjoint
data. In NAACL.
[2014] Pennington, J.; Socher, R.; and Manning, C. 2014.
Glove: Global vectors for word representation. In EMNLP.
[2018] Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.;
Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contex-
tualized word representations. In NAACL: HLT.
[2005] Pradhan, S.; Ward, W.; Hacioglu, K.; Martin, J.; and
Jurafsky, D. 2005. Semantic role labeling using different
syntactic views. In ACL.
[2013] Pradhan, S.; Moschitti, A.; Xue, N.; Ng, H. T.;
Bj¨orkelund, A.; Uryupina, O.; Zhang, Y.; and Zhong, Z.
2013. Towards robust linguistic analysis using ontonotes.
In CoNLL.
[2008] Punyakanok, V.; Roth, D.; and Yih, W.-t. 2008. The
importance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
[2016] Roth, M., and Lapata, M. 2016. Neural semantic role
labeling with dependency path embeddings. In ACL.
[2018] Strubell, E.; Verga, P.; Andor, D.; Weiss, D.; and Mc-
callum, A. 2018. Linguistically-informed self-attention for
semantic role labeling. arXiv preprint.
[2008] Surdeanu, M.; Johansson, R.; Meyers, A.; M`arquez,
L.; and Nivre, J. 2008. The CoNLL 2008 shared task on joint
parsing of syntactic and semantic dependencies. In CoNLL.
[2018] Tan, Z.; Wang, M.; Xie, J.; Chen, Y.; and Shi, X.
2018. Deep semantic role labeling with self-attention.
In
AAAI.
[2008] Toutanova, K.; Haghighi, A.; and Manning, C. D.
2008. A global joint model for semantic role labeling. Com-
putational Linguistics.
[2016] Wang, R.; Zhao, H.; Ploux, S.; Lu, B.-L.; and
Utiyama, M. 2016. A bilingual graph-based semantic model
for statistical machine translation. In IJCAI.
[2018] Zhang, Z.; Wu, Y.; Li, Z.; He, S.; Zhao, H.; Zhou,
I know what you want: Se-
X.; and Zhou, X.
arXiv preprint
mantic learning for text comprehension.
arXiv:1809.02794.
[2009] Zhao, H.; Chen, W.; Kazama, J.; Uchimoto, K.; and
Torisawa, K. 2009. Multilingual dependency learning: Ex-

2018.

ploiting rich features for tagging syntactic and semantic de-
pendencies. In CoNLL.
[2009] Zhao, H.; Chen, W.; and Kit, C. 2009. Semantic de-
pendency parsing of NomBank and PropBank: An efﬁcient
integrated approach via a large-scale feature selection.
In
EMNLP.
[2013] Zhao, H.; Zhang, X.; and Kit, C. 2013. Integrative se-
mantic dependency parsing via efﬁcient large-scale feature
selection. Journal of Artiﬁcial Intelligence Research.
[2015] Zhou, J., and Xu, W. 2015. End-to-end learning of
semantic role labeling using recurrent neural networks. In
ACL.

