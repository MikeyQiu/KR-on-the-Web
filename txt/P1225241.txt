9
1
0
2
 
n
u
J
 
8
2
 
 
]
L
C
.
s
c
[
 
 
1
v
5
3
0
2
1
.
6
0
9
1
:
v
i
X
r
a

Multi-Criteria Chinese Word Segmentation
with Transformer

Xipeng Qiu, Hengzhi Pei, Hang Yan, Xuanjing Huang

a School of Computer Science, Fudan University
b Key Laboratory of Intelligent Information Processing, Fudan University
cNo 220, Handan Road, Shanghai, China

Abstract

Different linguistic perspectives cause many diverse segmentation criteria for Chinese
word segmentation (CWS). Most existing methods focus on improving the perfor-
mance of single-criterion CWS. However, it is interesting to exploit these heteroge-
In this
neous segmentation criteria and mine their common underlying knowledge.
paper, we propose a concise and effective model for multi-criteria CWS, which utilizes
a shared fully-connected self-attention model to segment the sentence according to a
criterion indicator. Experiments on eight datasets with heterogeneous segmentation
criteria show that the performance of each corpus obtains a signiﬁcant improvement,
compared to single-criterion learning.

Keywords: Chinese Word Segmentation, Multi-Criteria, Self-Attention, Transformer,
Natural Language Processing, Deep Learning

1. Introduction

Unlike English, Chinese sentences consist of continuous characters and lack obvi-
ous boundaries between Chinese words. Since words are usually regarded as the min-
imum semantic units, therefore Chinese word segmentation (CWS) becomes a prelim-
inary and important pre-processing step for the downstream Chinese natural language
processing (NLP) tasks. Currently, CWS has been studied with considerable efforts in
the NLP community, and the state-of-the-art CWS methods are based on supervised
machine learning algorithms. Most of them regard CWS as a character-based sequence
labeling problem [1], in which each character is assigned a label to indicate its bound-
ary information. Recently, researchers have tended to explore neural network based
approaches to reduce efforts of the feature engineering [2, 3, 4, 5, 6, 7, 8]. Although
these methods have made great progress, they considerably rely on the large-scale
high-quality annotated corpus. However, there are two main challenges to construct
a high-quality annotated CWS corpus. The ﬁrst is that annotating the segmentation

Email addresses: xpqiu@fudan.edu.cn (Xipeng Qiu), hzpei16@fudan.edu.cn (Hengzhi

Pei), hyan11@fudan.edu.cn (Hang Yan), xjhuang@fudan.edu.cn (Xuanjing Huang)

Preprint submitted to Elsevier

July 1, 2019

usually requires linguistic experts and its cost is extremely expensive. The second is
that there are several inconsistent segmentation criteria from different linguistic per-
spectives. Therefore, although several CWS corpora have been built with great efforts,
their segmentation criteria are different and their segmentations for one sentence are
usually inconsistent.

Table 1: Illustration of the different segmentation criteria.

Corpora
CTB
PKU
MSRA

Lin Dan won
赢得

林丹

the championship
总冠军

林 丹 赢得 总
赢得 总

林丹

冠军
冠军

As shown in Table 1, given a sentence “林丹赢得总冠军(Lin Dan won the cham-
pionship)”, the three commonly-used corpora, PKU’s People’s Daily (PKU) [9], Penn
Chinese Treebank (CTB) [10] and MSRA [11], use different segmentation criteria.
Although these criteria are inconsistent, they share some common knowledge. The
knowledge learned from a segmentation criterion can beneﬁt other criteria.

Currently, most of CWS methods focus on improving the performance of each
individual segmentation criterion. It is a waste of resources if we fail to fully exploit all
the corpora with different criteria. Therefore, it remains to be a challenging problem
on how to effectively utilize these resources.

In our previous work [12, 13], we consider a multi-criteria learning framework
for CWS. Speciﬁcally, we regard each segmentation criterion as a single task under
the framework of multi-task learning [14], where a shared layer is used to extract the
criteria-invariant features, and a private layer is used to extract the criteria-speciﬁc fea-
tures. However, it is unnecessary to use a speciﬁc private layer for each criterion.
These different criteria often have partial overlaps. For the example in Table 1, the
segmentation of “林丹(Lin Dan)” is the same as in PKU and MSRA criteria, and the
segmentation of “总|冠军(the championship)” is the same as in CTB and MSRA crite-
ria. All these three criteria have same segmentation for the word “赢得(won)”.

In this work, we propose a concise model for multi-criteria Chinese word segmen-
tation by integrating shared knowledge from multiple segmentation criteria. Inspired
by the success of the Transformer [15], a fully-connected self-attention network, we
design a fully shared architecture for multi-criteria CWS, where a shared encoder is
used to extract the criteria-aware contextual features, and a shared decoder is used to
predict the criteria-speciﬁc labels. Finally, we exploit the eight segmentation crite-
ria on the ﬁve simpliﬁed Chinese and three traditional Chinese corpora. Experiments
show that the proposed model is effective to improve the performance for multi-criteria
CWS.

The contributions of this paper could be summarized as follows.

• Multi-criteria learning is formally introduced for CWS, which aims to make full
use of the existing heterogeneous corpora. Although the segmentation criteria of
these corpora are different, they share lots of common knowledge and could help
each other.

2

• We proposed a concise model for multi-criteria CWS based on Transformer,
which adopts a single shared model to predict the different criteria-speciﬁc la-
bels. Due to the powerful ability of Transformer, we can use a simple control
variable to determine the criterion-speciﬁc segmented output.

• It is a ﬁrst attempt to train a Transformer from scratch for CWS task, which can
effectively extract the non-local interactions and alleviate the long-term depen-
dency problem of RNN and CNN.

2. Background

work.

In this section, we ﬁrst brieﬂy describe the related background knowledge of our

2.1. Neural Architecture for CWS

Usually, CWS task could be viewed as a character-based sequence labeling prob-
lem. Speciﬁcally, each character in a sentence X = {x1, . . . , xT } is labelled as one
of y ∈ L = {B, M, E, S}, indicating the begin, middle, end of a word, or a word
with single character. The aim of CWS task is to ﬁgure out the ground truth of labels
Y ∗ = {y∗

1, . . . , y∗

T }:

Y ∗ = arg max

p(Y |X).

Y ∈LT

(1)

Recently, deep learning methods have been widely used in segmenting Chinese
words and can effectively reduce the efforts of feature engineering. The popular ar-
chitecture of neural CWS could be characterized by three components: (1) a character
embedding layer; (2) an encoding layer to extract the contextual features, which con-
sists of several classical neural networks and (3) a decoding layer with conditional
random ﬁelds (CRF) [17] layer or multi-layer perceptron (MLP).

Embedding Layer: In neural models, the ﬁrst step is to map discrete language
symbols into distributed embedding space. Formally, each character xt is mapped as
ext ∈ Rde , where de is a hyper-parameter indicating the size of character embedding.
Encoding Layer: The encoding layer is to extract the contextual features for each
character. Usually, the recurrent neural network (RNN) or convolutional neural net-
work (CNN) is adopted as the encoding layer.

For example, a prevalent choice for the encoding layer is the bi-directional LSTM

(BiLSTM) [16], which could incorporate information from both sides of sequence.

ht = BiLSTM(ext,

−→
h t−1,

←−
h t+1, θe),

(2)

−→
h t and

where
respectively, θe denotes all the parameters in the BiLSTM layer.

←−
h t are the hidden states at step t of the forward and backward LSTMs

Besides BiLSTM, CNN is also alternatively used to extract features.
Decoding Layer: The extracted features are then sent to conditional random ﬁelds

(CRF) [17] layer or multi-layer perceptron (MLP) for tag inference.

3

(a) Single
CWS

criterion

(b) MTL-based Multi-criteria CWS

(c) Transformer-based
Multi-criteria CWS

Figure 1: Architectures of single-criterion and multi-criteria Chinese word segmentation. The red compo-
nents are shared.

When using CRF as decoding layer, p(Y |X) in Eq (1) could be formalized as:

where Ψ(Y |X) is the potential function. In ﬁrst order linear chain CRF, we have:

p(Y |X) =

Ψ(Y |X)
Y (cid:48)∈Ln Ψ(Y (cid:48)|X)

,

(cid:80)

n
(cid:89)

t=2

Ψ(Y |X) =

ψ(X, t, yt−1, yt),

ψ(x, t, y(cid:48), y) = exp(δ(X, t)y + by(cid:48)y),

where by(cid:48)y ∈ R is trainable parameters respective to label pair (y(cid:48), y), score function
δ(X, t) ∈ R|L| calculates scores of each label for tagging the t-th character:

δ(X, t) = W(cid:62)

δ ht + bδ,

where ht is the hidden state of encoder at step t, Wδ ∈ Rdh×|L| and bδ ∈ R|L| are
trainable parameters.

When using MLP as decoding layer, p(Y |X) in Eq (1) is directly predicted by a

MLP with softmax function as output layer.

p(yt|X) = MLP(ht, θd),

∀t ∈ [1, T ]

where θd denotes all the parameters in MLP layer.

2.2. Multi-Criteria CWS with Multi-Task Learning

Since annotations in Chinese word segmentation are valuable and expensive, it
is important to jointly train Chinese word segmentation with multiple heterogeneous

4

(3)

(4)

(5)

(6)

(7)

criteria to improve the performance. The multi-task learning framework is a suitable
way to exploit the shared information among these different criteria.

Formally, assuming that there are M corpora with heterogeneous segmentation cri-

teria, we refer Dm as corpus m with Nm samples:
n , Y (m)

Dm = {(X (m)

n

)}Nm
n=1,

(8)

(9)

(10)

where X (m)
m respectively.

n

and Y (m)
n

denote the i-th sentence and the corresponding label in corpus

To exploit information across multiple corpora, the encoding layer additionally in-
troduces a shared encoder, together with the original private encoder. The architecture
of MTL-based multi-criteria CWS is shown in Figure 1b.

Concretely, for corpus m, a shared encoder and a private encoder are ﬁrst used to

extract the criterion-agnostic and criterion-speciﬁc features.

H(s) =encs(eX ; θ(s)
e ),
H(m) =encm(eX ; θ(m)

),

∀m ∈ [1, M ]

e
where eX = {ex1 , · · · , exT } denotes the embeddings of the input characters x1, · · · , xT ,
encs(·) represents the shared encoder and encm(·) represents the private encoder for
corpus m; θ(s)
are the shared and private parameters respectively. The shared
e
and private encoders are usually implemented by the RNN or CNN network.

and θ(m)
e

Then a private decoder is used to predict criterion-speciﬁc labels. For the m-th

corpus, the probability of output labels is

pm(Y |X) = decm([H(s); H(m)]; θ(m)

),

d

∀m ∈ [1, M ]

(11)

where decm(·) is a private CRF or MLP decoder for corpus m, taking the shared and
private features as inputs, and θ(m)

is the parameters of the m-th private decoder.

d

Objective. The objective is to maximize the log likelihood of true labels on all the
corpora:

Jseg(Θm, Θs) =

log p(Y (m)

n

|X (m)

n ; Θm, Θs),

(12)

M
(cid:88)

Nm(cid:88)

m=1

n=1

d } and Θs = {E, θ(s)
, θ(m)
where Θm = {θ(m)
eters respectively; E is the embedding matrix.

e

e } denote all the private and shared param-

3. Proposed Model

In this work, we propose a more concise architecture for multi-criteria CWS, which
adopts the Transformer encoder [15] to extract the contextual features for each input
character. In our proposed architecture, both the encoder and decoder are shared by
all the criteria. The only difference is that a unique indicator is taken as input for
each criterion. Figure 1 illustrates the difference between our proposed model and the
previous models.

Figure 2 illustrates the proposed architecture for multi-criteria CWS. The detailed

description is as follows.

5

Figure 2: Proposed Model for Multi-Criteria Chinese Word Segmentation.

3.1. Embedding Layer

Given a character sentence X = {x1, . . . , xT }, we ﬁrst map it into a vector se-
quence. Besides the standard character embeddings, we introduce three extra embed-
dings: criterion embedding, bigram embedding, and position embedding.

1) Criterion Embedding: The criterion embedding is used to indicate its expected
output criterion. For the m-th criterion, we use e[m] to denote its embedding. To
simplicity, we directly add a special token [m] at the begin of X.

2) Bigram Embedding: Based on [5, 18, 19], bigram features can greatly beneﬁt
the task of CWS. Following their settings, we also introduce the bigram embedding to
augment the character-level unigram embedding. The bigram representation of charac-
ter xt is

e(cid:48)
xt = ext ⊕ ext−1xt,

where e denotes the embedding vector for the unigram and bigram, and ⊕ is the con-
catenation operator.

3) Position Embedding: To capture the order information of a sequence, a position
embedding P E is used for each position. The position embedding can be learnable
parameters or pre-deﬁned. In this work, we use the predeﬁned position embedding
following [15]. For the t-th character in a sentence, its position embedding is deﬁned
by

(13)

(14)

(15)

where i denotes the dimensional index of position embedding and d denotes the dimen-
sion of embedding vector.

P Et,2i = sin(t/100002i/d),
P Et,2i+1 = cos(t/100002i/d),

6

Finally, the embedding matrix of the sequence X = {x1, · · · , xT } with criterion

X = [e[m] + P E0; e(cid:48)

x1 + P E1; · · · ; e(cid:48)

xT + P ET ],

(16)

m is formulated as

3.2. Encoding Layer

In sequence modeling, RNN and CNN often suffer from the long-term depen-
dency problem and cannot effectively extract the non-local interactions in a sentence.
Recently, the fully-connected self-attention architecture, such as Transformer [15],
achieves great success in many NLP tasks, such as text classiﬁcation, machine transla-
tion.

In this work, we adopt the Transformer encoder as our encoding layer, in which
several multi-head self-attention layers are used to extract the contextual feature for
each character.

In each multi-head self-attention layer, we use the scaled dot-product attention
to model the intra-interactions of a sequence. Given a sequence of vectors H ∈
R(T +1)×dmodel , where (T + 1) and dmodel represent the length and the dimension
of the input vector sequence, the self-attention projects H into three different matri-
ces: the query matrix Q ∈ R(T +1)×dk , the key matrix K ∈ R(T +1)×dk and the value
matrix vector V ∈ R(T +1)×dv , and uses scaled dot-product attention to get the output
representation.

Q, K, V = HW Q, HW K, HW V
QK T
√
dk

Attn(Q, K, V ) = softmax(

)V,

where W Q ∈ Rdmodel×dk , W K ∈ Rdmodel×dk , W V ∈ Rdmodel×dv are learnable pa-
rameters and softmax() is performed row-wise.

To enhance the ability of self-attention, multi-head self-attention is introduced as
an extension of the single head self-attention, which jointly model the multiple inter-
actions from different representation spaces,

MultiHead(H) = [head1; ...; headk]W O,
headi = Attn(HW Q
where

i , HW K
i

, HW V

i ),

, W V

i , W K
i

where W O, W Q

i (i ∈ [1, k]) are learnable parameters.
Transformer encoder consists of several stacked multi-head self-attention layers
and fully-connected layers. Assuming the input of the self-attention layer is H, its
output ˜H is calculated by

(cid:16)

Z =layer-norm

H + MultiHead(H)

(cid:16)
˜H =layer-norm

Z + MLP(Z)

(cid:17)
,

(cid:17)
,

where layer-norm(·) represents the layer normalization [20] .

All the tasks with the different criteria use the same encoder. But with the different
criterion indicator m, the encoder can extract the criterion-aware representation for
each character.

(17)

(18)

(19)

(20)

(21)

(22)

7

Table 2: Details of the eight datasets after preprocessing. “Word Types” represents the number of unique
word. “Char Types” is the number of unique characters. “OOV Rate” is Out-Of-Vobulary rate.

Corpora

Words#

Chars# Word Types

Char Types

OOV

5
0
n
a
h
g
i
S

8
0
n
a
h
g
i
S

MSRA

AS

PKU

CITYU

CTB

CKIP

NCC

SXU

Train
Test

Train
Test

Train
Test

Train
Test

Train
Test

Train
Test

Train
Test

Train
Test

2.4M
0.1M

5.4M
0.1M

1.1M
0.1M

1.1M
0.2M

0.6M
0.1M

0.7M
0.1M

0.9M
0.2M

0.5M
0.1M

4.0M
0.2M

8.3M
0.2M

1.8M
0.2M

1.8M
0.4M

1.0M
0.1M

1.1M
0.1M

1.4M
0.2M

0.8M
0.2M

75.4K
11.9K

128.8K
18.0K

51.2K
12.5K

43.4K
23.2K

40.5K
11.9K

44.7K
14.2K

53.3K
20.9K

29.8K
11.6K

5.1K
2.8K

5.8K
3.4K

4.6K
2.9K

4.2K
3.6K

4.2K
2.9K

4.5K
3.1K

5.3K
3.9K

4.1K
2.8K

1.32%

2.20%

2.06%

3.69%

3.80%

4.29%

3.31%

2.60%

3.3. Decoding Layer

In the standard multi-task learning framework, each task has its own private decoder
to predict the task-speciﬁc labels. Different from the previous work, we use a shared
decoder for all the tasks since we have extracted the criterion-aware representation for
each character. In this work, we attempt two kinds of decoders: CRF and MLP. We use
the CRF as the default decoder since it is slightly better than MLP (see Sec. 4.4).

With the shared encoder and decoder, our model is more concise than the shared-

private architectures [12, 30].

4. Experiments

4.1. Datasets

We experiment on eight CWS datasets from SIGHAN2005 [11] and SIGHAN2008
[21]. Among them, the AS, CITYU, and CKIP datasets are in traditional Chinese,
while the MSRA, PKU, CTB, NCC, and SXU datasets are in simpliﬁed Chinese. Ex-
cept otherwise stated, AS, CITYU and CKIP are translated into simpliﬁed Chinese as
in [12, 13]. We randomly pick 10% instances from the training set as the develop-
ment set for all datasets. Similar to the previous work [12, 8], we preprocess all the
datasets by replacing the continuous Latin characters and digits with a unique token,
and converting all digits, punctuation and Latin letters to half-width to deal with the
full/half-width mismatch between training and test set. Table 2 gives the details of the
eight datasets after preprocessing.

8

We use the standard measures of precision, recall and F1 scores to evaluate Chinese
word segmentation [22]. The precision of Chinese word segmentation (denoted as P )
is calculated by the number of correctly segmented words versus the total number of
segmented words. The recall of Chinese word segmentation (denoted as R) is com-
puted by the number of correctly segmented words versus the total number of golden
words. Then we get F 1 value by F 1 = 2 ∗ P ∗ R/(P + R).

4.2. Experimental Settings
Pretrained Embedding. Based on on [5, 18, 19], n-gram features are of great beneﬁt to
Chinese word segmentation and POS tagging tasks, thus we use unigram and bigram
embeddings for our models. We ﬁrst pretrain unigram and bigram embeddings on
Chinese Wikipedia corpus by the method proposed in [23] which improves standard
word2vec by incorporating token order information. For a sentence with characters
“abcd...”, the unigram sequence is “a b c ...”; the bigram sequence is “ab bc cd ...”. In
the training phase of CWS, all pretrained embeddings are ﬁxed at the ﬁrst 50 epochs
and then updated during our experiments.

Hyper-parameters. The model is trained with Adam algorithm [24]. The development
set is used for parameter tuning. We use the CRF as the default decoder since it is
slightly better than MLP (see Sec. 4.4). All models are trained for 100 epochs, after
each training epoch, we test the model on the dev set, and models with the highest F 1 in
dev set are tested in the test set and we report its outcomes. The detail hyperparameters
can be found in Table 3.

Table 3: Hyper-Parameter Settings

Embedding Size

Hidden State Size dmodel
Transformer Layers

Attention Heads

Gradients Clip

Batch Size

100

256

6

4

5

128

0.33

Embedding Dropout Ratio

Initial Learning Rate

Annealing Rate

Max epochs

2e-3
.75t/5000
100

9

Since the numbers of layers and attention heads are important, we list the perfor-
mances with different settings in Table 4. Based on their performances, we use six
layers of self-attention, each layer with four attention heads.

Table 4: Average F1 values with different number of Transformer layers and heads are used.

# of Layer

# of head

avg F1

2
4
6
2
4
6

4
4
4
8
8
8

96.69
96.81
96.87
96.68
96.8
96.84

4.3. Overall Results

CWS datasets.

Table 5 shows the experiment results of the proposed model on test sets of eight

We ﬁrst compare our Transformer encoder with BiLSTM, Stacked Bi-LSTM and
Switch-LSTMs from [12, 13] in the single-criterion learning scenario. The compari-
son is presented in the upper block of Table 5. As we can see, Transformer outpaces
BiLSTMs, Stacked Bi-LSTM, and Switch-LSTMs both in F 1 value and OOV. Quan-
titatively speaking, Transformer obtains 96.47 in average F 1 value, while the previous
state-of-the-art result is 94.76 [13], the absolute increasement is 1.71. We argue that
the capability of the Transformer is greater than conventional LSTM models, and this
view can be consolidated by the 7.32 OOV improvement.

In the multi-criteria learning scenario, we compare Transformer with the multi-task
learning framework (MTL) [12] and Switch-LSTMs [13]. The lower block of Table
5 displays the contrast. Firstly, although different criteria are trained together, most
datasets (besides CTB) achieve better performance with respect to F 1 value. Compared
to the single-criterion scenario, 0.4 gain in average F 1 value is obtained by multi-
criteria scenario, which indicates the proposed criterion embedding is useful. Secondly,
compare with previous multi-criteria learning models, the model proposed in this paper
also achieves better average F 1 value. This is a sign that the proposed Transformer-
based multi-criteria CWS makes better use of different criteria datasets.

Figure 3 visualizes the 2D PCA projection of the learned embeddings of eight dif-
ferent criteria. Generally, the eight criteria are mapped into dispersed points in the em-
bedding space, which indicates each criterion is different from others. Among them,
MSRA is obviously different from others. A possible reason is that the named entity is
regarded as a unique word in the MSRA criterion, which is signiﬁcantly distinguishing
with other criteria.

4.4. Ablation Study

ablation study shown in Table 6.

To show the effectiveness of each component in our model, we also conduct an

The ﬁrst ablation study is to verify the effectiveness of the CRF decoder, which
is popular in most CWS models. The comparison between the ﬁrst two lines indicates
that with or without CRF does not make much difference since a model with CRF takes
longer time to train and inference, we suggest not to use CRF in Transformer models.

10

Table 5: Results of the proposed model on the test sets of eight CWS datasets. Here, P, R, F, OOV indicate
the precision, recall, F 1 value, and OOV recall rate respectively. The maximum F 1 value and OOV value
are highlighted for each dataset. There are two blocks. The upper block consists of single-criterion learning
models. Bi-LSTMs and stack-LSTMS are baselines and the results on them are reported in [12]. The lower
block consists of multi-criteria learning models.

Models

MSRA AS

PKU CTB CKIP CITYU NCC SXU Avg.

Single-Criterion Learning

Bi-LSTMs[12]

Stacked Bi-LSTM[12]

Switch-LSTMs [13]

Transformer

MTL [12]

Switch-LSTMs [13]

Transformer

P
R
F

95.7
93.64 93.67 95.19 92.44
95.99 94.77 92.93 95.42 93.69
95.3 93.06
93.3
95.84
OOV 66.28 70.07 66.09 76.47 72.12

94.2

95.2

95.69 93.89 94.1
92.4
95.81 94.54 92.66 95.4 93.39
95.75 94.22 93.37 95.3 92.89
71.5 67.92 75.44 70.5

OOV 65.55

96.07 93.83 95.92 97.13 92.02
96.86 95.21 95.56 97.05 93.76
96.46 94.51 95.74 97.09 92.88
71.6
72.7

81.8

77.8

OOV 69.9

P
R
F

98.14 96.61 96.06 96.26 95.97
95.51 96.73 96.57 95.35
98.07 96.26 96.39 96.43 95.66
OOV 73.75 73.05 72.82 82.82 79.05

98

Multi-Criteria Learning

95.95 94.17 94.86 96.02 93.82
96.14 95.11 93.78 96.33 94.7
96.04 94.64 94.32 96.18 94.26
73.5 72.67 82.48 77.59

OOV 71.6

97.69 94.42 96.24 97.09 94.53
97.87 96.03 96.05 97.43 95.45
97.78 95.22 96.15 97.26 94.99
77.33 69.88 83.89 77.69

OOV 64.2

98.03 96.84 95.88 96.79 96.92
98.06 96.05 96.95 97.18 96.11
98.05 96.44 96.41 96.99 96.51
82.89

87

OOV 78.92 76.39 78.91

P
R
F

P
R
F

P
R
F

P
R
F

P
R
F

94
94.15
94.07
65.79

94.13
93.99
94.06
66.35

93.69
93.73
93.71
59.8

96.44
96.2
96.32
83.72

95.39
95.7
95.55
81.4

95.85
96.59
96.22
73.58

97.03
96.78
96.91
86.91

91.86 95.11 93.95
92.47 95.23 94.33
92.17 95.17 94.14
59.11 71.27 68.4

91.81 94.99 94.03
92.62 95.37 94.22
92.21 95.18 94.12
57.39 69.69 68.04

91.81 95.02 94.44
92.43 96.13 95.09
92.12 95.57 94.76
67.3 69.55
55.5

95.56 97.08 96.52
96.59 97.09 96.51
95.57 97.08 96.47
71.81 77.95 76.87

92.46 96.07 94.84
93.19 96.01 95.12
92.83 96.04 94.98
63.31 77.1 74.96

94.07 96.88 95.85
94.17 97.62 96.4
94.12 97.25 96.12
69.76 78.69 74.38

95.85 97.52 96.86
96.24 97.69 96.88
96.04 97.61 96.87
79.3 85.08 81.92

Models

MSRA AS

PKU CTB CKIP CITYU NCC SXU Avg.

11

Figure 3: Proposed Model for Multi-Criteria Chinese Word Segmentation.

Table 6: Ablation exeperiments. The ﬁrst line presents the results of our full multi-criteria trained model.
The following lines are results of separately removing a certain part.

Models

MSRA AS

PKU CTB CKIP CITYU NCC SXU Avg.

Full Model
w/o CRF
w/o bigram
w/o pretrained emb.

98.05
98.02
97.41
97.51

96.44 96.41 96.99 96.51
96.59
96.42 96.41
96

96.9
96.25 96.71
96.06 96.02 96.47 96.22

96

96.91
96.87
96.31
95.99

96.04 97.61 96.87
96.83
97.5
95.96
94.62 96.84 96.27
94.82 96.76 96.23

The other two ablation studies are to evaluate the effect of the bigram feature and
pretrained embeddings. We can see that their effects vary in different datasets. Some
datasets are more sensitive to the bigram feature, while others are more sensitive to
pretrain embeddings. In terms of average performance, the bigram feature and pre-
trained embeddings are important and boost the performance greatly, but these two
components do not have a clear winner.

4.5. Joint Training on both Simpliﬁed and Traditional Corpora

In the above experiments, the traditional Chinese corpora (AS, CITYU, and CKIP)
are translated into simpliﬁed Chinese. However, the relation between simpliﬁed Chi-
nese characters and traditional Chinese characters are not deterministic. For example,
simpliﬁed word “代码”(“code” in simpliﬁed Chinese) is translated into its traditional
version as “程式碼”(“code” in traditional Chinese). Even the number of characters is
different. Therefore, it might be better if we can jointly train simpliﬁed and traditional
Chinese word segmentation.

12

We study 4 different ways to train our model on Simpliﬁed and Traditional Chinese

Corpora.

1. The ﬁrst way (“8Simp”) is to translate all the corpora into Simpliﬁed Chinese.
For the pretrained embeddings, we use the Simpliﬁed Chinese Wikipedia dump
to pretrain the unigram and bigram embeddings. This way is the same as the
previous experiments.

2. The second way (“8Trad”) is to translate all the corpora into Traditional Chinese.
For the pretrained embeddings, we ﬁrst convert the Wikipedia dump into tradi-
tional Chinese characters, then we use this converted corpus to pretrain unigram
and bigram embeddings.

3. The third way (“5Simp, 3Trad”) is to keep the original characters of each corpus.
The shared transformer encoder can take as input the Simpliﬁed or Traditional
Chinese sentences. In this way, we pretrain the joint Simpliﬁed and Traditional
Chinese embeddings in a joint embedding space. We merge the Wikipedia cor-
pora used in “8Trad” and “8Simp” to form a mixed corpus, which contains both
the Simpliﬁed and Traditional Chinese characters. The pretrained unigram and
bigram embeddings are learned on this mixed corpus.

4. The last way (“8Simp, 8Trad”) is to simultaneously train our model on both
the eight Simpliﬁed Chinese corpora in “8Simp” and the eight Traditional Chi-
nese corpora in “8Trad”. The pretrained word embeddings are same to “5Simp,
3Trad”.

The results are shown in Table 7, which indicate that there does not exist too much
difference between different logographs settings. Although in the setting “5Simp,
3Trad”, AS, CKIP and CITYU are in traditional Chinese, they also beneﬁt from this
multi-criteria scenario, since their performances are similar to “8Simp” and “8Trad”.

Table 7: This table presents the results of training simpliﬁed Chinese corpus and traditional Chinese corpus
together. “8Simp”, “8Trad” means all corpus are converted into simpliﬁed Chinese or traditional Chinese
respectively. “5Simp,3Trad” means 5 datasets are in simpliﬁed Chinese and 3 datasets(including AS, CITYU
and CKIP, these datasets are given as traditional Chinese.) are in traditional Chinese.

Models

MSRA

AS

PKU CTB CKIP CITYU NCC SXU Avg. F1

8Simp
8Trad
5Simp, 3Trad
8 Simp, 8 Trad

98.05
97.98
98.03
98.04

96.44
96.39
96.52
96.41

96.41
96.49
96.6
96.43

96.99
96.99
96.94
96.99

96.51
96.49
96.38
96.54

96.91
96.86
96.8
96.85

96.04
95.98
96.02
96.08

97.61
97.48
97.55
97.52

96.87
96.83
96.86
96.86

To better understand the quality of the learned joint embedding space of Simpliﬁed
and Traditional Chinese, we conduct a qualitative analysis by doing some case studies
in Table 8 to illustrate the most similar words for certain target words under different
methods. Explicitly, we present the top 8 words that are most similar to our target
word. Similar words are retrieved based on the cosine similarity calculated using the
learned embeddings.

As we can see, the Traditional Chinese words are similar to their Simpliﬁed Chinese
counterparts, and vice versa. The results shows that the Simpliﬁed and Traditional
Chinese characters and bigrams are aligned well in the joint embedding space.

13

Table 8: Case study for qualitative analysis. Given the target word, we list its top 8 similar words. The word
with red color indicates it is a Traditional Chinese word.

苹果(apple)

蘋果 (apple)

爱好(hobby) 愛好(hobby) 担心(worry)

擔心(worry)

微軟(Microsoft)

热爱(love)
熱愛(love)
兴趣(interest) 爱好(hobby) 怀疑(doubt)

坚果(nut)
谷歌(Google) 黃油(butter)
华为(Huawei) 現貨(goods in stock) 愛好(hobby) 興趣(interest) 顾虑(misgiving) 懷疑(doubt)
黄油(butter)
鲜果(fresh fruit) 京東(JD)
微软(Microsoft) 賣家(seller)
苹果(apple)
诺基(Nokia)
售後(after-sales)
蘋果(Apple)

梦想(dream) 夢想(dream) 担忧(concern) 擔憂(concern)
爱玩(Playful) 愛玩(Playful) 责怪(blame)
憂慮(anxiety)
痴迷(addict) 喜愛(adore) 伤心(sad)
責怪(blame)
乐趣(pleasure) 習慣(habbit) 嫌弃(disfavour) 傷心(sad)
喜爱(adore) 樂趣(pleasure) 忧虑(anxiety) 担心(worry)

關心(care)
顧慮(misgiving)

果凍(jelly)

关心(care)

4.6. Transfer Capability

Since except for the criterion embedding, the left parts of the our model are shared
between different criteria, we want to exploit whether a trained multi-criteria model
can be transferred to a new criteria only by learning a new criterion embedding with
few examples.

We leave-one-out strategy to evaluate the transfer capability of our Transformer-
based multi-criteria model. We ﬁrst train the model on seven datasets, then only learn
the new criterion embedding with a few training instances from the left dataset. This
scenario is also discussed in [13], we present their and our outcomes (averaged F 1
value) in Figure 4.

95

90

85

80

75

e
u
l
a
v

1
F
d
e
g
a
r
e
v
A

Switch-LSTMs
Switch-LSTMs-T
Transformer
Transformer-T

200

400

600

800

1,000

Number of Training Samples

Figure 4: Evaluation of the transfer capability. Switch-LSTMs and Transformer are trained on the given
instances from scratch. Switch-LSTMs-T and Transformer-T are learned in transfer fashion.

Firstly, for the different number of samples, our transferred model (Transformer-
T) always largely outperforms the model learnt from scratch. We believe this indi-
cates that learning a new criterion embedding is an effective way to transfer a trained

14

Transformer-based multi-criteria model to a new criterion. Secondly, the comparison
between Switch-LSTMs and Transformer indicates that both of them have poor per-
formances when just few samples are available. Thirdly, Transformer has superior
transferability than Switch-LSTMs [13], since the average F 1 values in the different
number of target samples are all better than its Switch-LSTMs counterparts.

5. Related Work

Much prior work has focused on exploiting heterogeneous annotation data to im-
prove various NLP tasks. Jiang et al. [25] proposed a stacking-based model which
could train a model for one speciﬁc desired annotation criterion by utilizing knowl-
edge from corpora with other heterogeneous annotations. Sun and Wan [26] proposed
a structure-based stacking model to reduce the approximation error, which makes use
of structured features such as sub-words. These models are unidirectional aid and
also suffer from error propagation problem. Qiu et al. [22] used multi-tasks learning
framework to improve the performance of POS tagging on two heterogeneous datasets.
Li et al. [27] proposed a coupled sequence labeling model which could directly learn
and infer two heterogeneous annotations. Chen et al. [28] adopted two neural mod-
els based on stacking framework and multi-view framework respectively, which boosts
POS-tagging performance by utilizing corpora in heterogeneous annotations.

In our previous work [12, 13], we proposed a multi-criteria learning framework
for CWS, which uses a shared layer to extract the common underlying features and a
private layer for each criterion to extract criteria-speciﬁc features. He et al. [29] used a
shared BiLSTM+CRF to deal with all the criteria by adding two artiﬁcial tokens at the
beginning and end of an input sentence to specify the required target criteria. Huang
et al. [30] proposed a domain adaptive segmenter to capture diverse criteria based on
Bidirectional Encoder Representations from Transformers (BERT) [31].

Unlike the above models, we propose a fully-shared model for multi-criteria CWS
based on Transformer. Given an input sentence, we just need a criterion indicator to
specify the output criterion. Thus, we can use a single model to produce different
segmented results for different criteria. It is also a ﬁrst attempt to utilize the popular
Transformer (learned from scratch) in CWS task.

6. Conclusion and Future Work

In this paper, we propose an effective framework for multi-criteria CWS by fully
exploiting the underlying shared knowledge across multiple heterogeneous criteria.
Experiments show that our proposed model is effective to extract the shared infor-
mation and achieve signiﬁcant improvements over the single-criterion methods.

In future work, we are planning to evaluate our model by incorporating other se-

quence labeling tasks, such as POS tagging and named entity recognition.

7. Acknowledgements

This work was supported by the National Key Research and Development Program

15

of China [grant numbers 2018YFC0831103], and Shanghai Municipal Science and
Technology Major Project(No.2018SHZDZX01)and ZJLab.

References

[1] N. Xue, Chinese word segmentation as character tagging, Computational Lin-

guistics and Chinese Language Processing 8 (2003) 29–48.

[2] X. Zheng, H. Chen, T. Xu, Deep learning for chinese word segmentation and
in: Proceedings of the 2013 Conference on Empirical Methods in

pos tagging,
Natural Language Processing, 2013, pp. 647–657.

[3] W. Pei, T. Ge, B. Chang, Max-margin tensor neural network for Chinese word
in: Proceedings of the 52nd Annual Meeting of the Association
segmentation,
for Computational Linguistics (Volume 1: Long Papers), volume 1, 2014, pp.
293–303.

[4] X. Chen, X. Qiu, C. Zhu, X. Huang, Gated recursive neural network for Chinese
in: Proceedings of Annual Meeting of the Association for

word segmentation,
Computational Linguistics., 2015.

[5] X. Chen, X. Qiu, C. Zhu, P. Liu, X. Huang, Long Short-Term Memory Neural

Networks for Chinese Word Segmentation., in: EMNLP, 2015, pp. 1197–1206.

[6] D. Cai, H. Zhao, Neural word segmentation learning for Chinese, arXiv preprint

arXiv:1606.04300 (2016).

[7] M. Zhang, Y. Zhang, G. Fu, Transition-based neural word segmentation, Pro-

ceedings of the 54nd ACL (2016).

[8] J. Ma, K. Ganchev, D. Weiss, State-of-the-art Chinese word segmentation with

Bi-LSTMs, arXiv preprint arXiv:1808.06511 (2018).

[9] S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun, H. Wang, Q. Zhao, W. Zhan,
Processing norms of modern Chinese corpus, Technical Report, Technical report,
2001.

[10] X. Fei, The part-of-speech tagging guidelines for the penn chinese treebank (3.0),
URL: http://www. cis. upenn. edu/˜ chinese/segguide. 3rd. ch. pdf (2000).

[11] T. Emerson, The second international Chinese word segmentation bakeoff,

in:
Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,
Jeju Island, Korea, 2005, pp. 123–133.

[12] X. Chen, Z. Shi, X. Qiu, X. Huang, Adversarial multi-criteria learning for Chi-
in: Proceedings of the 55th Annual Meeting of the
nese word segmentation,
Association for Computational Linguistics (Volume 1: Long Papers), volume 1,
2017, pp. 1193–1203.

16

[13] J. Gong, X. Chen, T. Gui, X. Qiu, Switch-lstms for multi-criteria chinese word

segmentation, arXiv preprint arXiv:1812.08033 (2018).

[14] R. Caruana, Multitask learning, Machine learning 28 (1997) 41–75.

[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
in: Advances in Neural

Ł. Kaiser, I. Polosukhin, Attention is all you need,
Information Processing Systems, 2017, pp. 5998–6008.

[16] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation 9

(1997) 1735–1780.

[17] J. D. Lafferty, A. McCallum, F. C. N. Pereira, Conditional random ﬁelds: Proba-
bilistic models for segmenting and labeling sequence data, in: Proceedings of the
Eighteenth International Conference on Machine Learning, 2001.

[18] Y. Shao, C. Hardmeier, J. Tiedemann, J. Nivre, Character-based joint segmen-
tation and pos tagging for chinese using bidirectional rnn-crf, arXiv preprint
arXiv:1704.01314 (2017).

[19] M. Zhang, N. Yu, G. Fu, A simple and effective neural model for joint word
segmentation and POS tagging, IEEE/ACM Transactions on Audio, Speech and
Language Processing (TASLP) 26 (2018) 1528–1538.

[20] L. J. Ba, R. Kiros, G. E. Hinton, Layer normalization, CoRR abs/1607.06450

(2016).

[21] G. Jin, X. Chen, The fourth international chinese language processing bakeoff:
Chinese word segmentation, named entity recognition and chinese pos tagging,
in: Sixth SIGHAN Workshop on Chinese Language Processing, 2008, p. 69.

[22] X. Qiu, J. Zhao, X. Huang, Joint chinese word segmentation and POS tagging on
heterogeneous annotated corpora with multiple task learning., in: EMNLP, 2013,
pp. 658–668.

[23] W. Ling, C. Dyer, A. W. Black, I. Trancoso, Two/too simple adaptations of
word2vec for syntax problems,
in: Proceedings of the 2015 Conference of the
North American Chapter of the Association for Computational Linguistics: Hu-
man Language Technologies, 2015, pp. 1299–1304.

[24] D. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint

arXiv:1412.6980 (2014).

[25] W. Jiang, L. Huang, Q. Liu, Automatic adaptation of annotation standards: Chi-
nese word segmentation and POS tagging: a case study, in: Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing, 2009, pp. 522–530.

17

[26] W. Sun, X. Wan, Reducing approximation and estimation errors for chinese lex-
ical processing with heterogeneous annotations, in: Proceedings of the 50th An-
nual Meeting of the Association for Computational Linguistics: Long Papers-
Volume 1, Association for Computational Linguistics, 2012, pp. 232–241.

[27] Z. Li, J. Chao, M. Zhang, W. Chen, Coupled sequence labeling on heterogeneous
annotations: POS tagging as a case study,
in: Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Processing, 2015.

[28] H. Chen, Y. Zhang, Q. Liu, Neural network for heterogeneous annotations, Pro-
ceedings of the 2016 Conference on Empirical Methods in Natural Language Pro-
cessing (2016).

[29] H. He, L. Wu, H. Yan, Z. Gao, Y. Feng, G. Townsend, Effective neural solu-
in: Smart Intelligent Computing and

tion for multi-criteria word segmentation,
Applications, Springer, 2019, pp. 133–142.

[30] W. Huang, X. Cheng, K. Chen, T. Wang, W. Chu, Toward fast and accurate
neural chinese word segmentation with multi-criteria learning, arXiv preprint
arXiv:1903.04190 (2019).

[31] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: pre-training of deep bidirec-

tional transformers for language understanding, CoRR abs/1810.04805 (2018).

18

