6
1
0
2
 
t
c
O
 
6
2

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
5
6
6
0
.
0
1
6
1
:
v
i
X
r
a

Single Pass PCA of Matrix Products

Shanshan Wu
The University of Texas at Austin
shanshan@utexas.edu

Srinadh Bhojanapalli
Toyota Technological Institute at Chicago
srinadh@ttic.edu

Sujay Sanghavi
The University of Texas at Austin
sanghavi@mail.utexas.edu

Alexandros G. Dimakis
The University of Texas at Austin
dimakis@austin.utexas.edu

October 27, 2016

Abstract

In this paper we present a new algorithm for computing a low rank approximation of the product
AT B by taking only a single pass of the two matrices A and B. The straightforward way to do this is to
(a) ﬁrst sketch A and B individually, and then (b) ﬁnd the top components using PCA on the sketch. Our
algorithm in contrast retains additional summary information about A, B (e.g. row and column norms
etc.) and uses this additional information to obtain an improved approximation from the sketches. Our
main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in
addition we also provide results from an Apache Spark implementation that shows better computational
and statistical performance on real-world and synthetic evaluation datasets.

1 Introduction

Given two large matrices A and B we study the problem of ﬁnding a low rank approximation of their
product AT B, using only one pass over the matrix elements. This problem has many applications in machine
learning and statistics. For example, if A = B, then this general problem reduces to Principal Component
Analysis (PCA). Another example is a low rank approximation of a co-occurrence matrix from large logs,
e.g., A may be a user-by-query matrix and B may be a user-by-ad matrix, so AT B computes the joint
counts for each query-ad pair. The matrices A and B can also be two large bag-of-word matrices. For this
case, each entry of AT B is the number of times a pair of words co-occurred together. As a fourth example,
AT B can be a cross-covariance matrix between two sets of variables, e.g., A and B may be genotype and
phenotype data collected on the same set of observations. A low rank approximation of the product matrix
is useful for Canonical Correlation Analysis (CCA) [8]. For all these examples, AT B captures pairwise
variable interactions and a low rank approximation is a way to efﬁciently represent the signiﬁcant pairwise
interactions in sub-quadratic space.

×

n (d

Let A and B be matrices of size d

n) assumed too large to ﬁt in main memory. To obtain a
rank-r approximation of AT B, a naive way is to compute AT B ﬁrst, and then perform truncated singular
value decomposition (SVD) of AT B. This algorithm needs O(n2d) time and O(n2) memory to compute
the product, followed by an SVD of the n
n matrix. An alternative option is to directly run power method
on AT B without explicitly computing the product. Such an algorithm will need to access the data matrices
A and B multiple times and the disk IO overhead for loading the matrices to memory multiple times will be
the major performance bottleneck.

≫

×

1

For this reason, a number of recent papers introduce randomized algorithms that require only a few
passes over the data, approximately linear memory, and also provide spectral norm guarantees. The key step
in these algorithms is to compute a smaller representation of data. This can be achieved by two different
methods: (1) dimensionality reduction, i.e., matrix sketching [29, 11, 26, 12]; (2) random sampling [14, 3].
The recent results of Cohen et al. [12] provide the strongest spectral norm guarantee of the former. They
show that a sketch size of O(˜r/ǫ2) sufﬁces for the sketched matrices
B to achieve a spectral error of ǫ,
where ˜r is the maximum stable rank of A and B. Note that
B is not the desired rank-r approximation
of AT B. On the other hand,
[3] is a recent sampling method with very good performance guarantees.
The authors consider entrywise sampling based on column norms, followed by a matrix completion step
to compute low rank approximations. There is also a lot of interesting work on streaming PCA, but none
can be directly applied to the general case when A is different from B (see Figure 4(c)). Please refer to
Appendix D for more discussions on related work.

AT

AT

e

e

e

e

Despite the signiﬁcant volume of prior work, there is no method that computes a rank-r approximation
of AT B when the entries of A and B are streaming in a single pass 1. Bhojanapalli et al. [3] consider a
two-pass algorithm which computes column norms in the ﬁrst pass and uses them to sample in a second
pass over the matrix elements. In this paper, we combine ideas from the sketching and sampling literature
to obtain the ﬁrst algorithm that requires only a single pass over the data.

Contributions:

•

•

•

•

We propose a one-pass algorithm SMP-PCA (which stands for Streaming Matrix Product PCA) that
computes a rank-r approximation of AT B in time O((nnz(A) + nnz(B)) ρ2r3 ˜r
). Here
nnz(
) is the number of non-zero entries, ρ is the condition number, ˜r is the maximum stable rank, and
·
η measures the spectral norm error. Existing two-pass algorithms such as [3] typically have longer
runtime than our algorithm (see Figure 3(a)). We also compare our algorithm with the simple idea that
ﬁrst sketches A and B separately and then performs SVD on the product of their sketches. We show
that our algorithm always achieves better accuracy and can perform arbitrarily better if the column
vectors of A and B come from a cone (see Figures 2, 4(b), 3(b)).

η2 + nr6ρ4 ˜r3

η4

The central idea of our algorithm is a novel rescaled JL embedding that combines information from
matrix sketches and vector norms. This allows us to get better estimates of dot products of high
dimensional vectors compared to previous sketching approaches. We explain the beneﬁt compared to
a naive JL embedding in Figure 2 and the related discussion; we believe it may be of more general
interest beyond low rank matrix approximations.

AT B
k

We prove that our algorithm recovers a low rank approximation of AT B up to an error that depends
on
, decaying with increasing sketch size and number of samples
k
(Theorem 3.1). The ﬁrst term is a consequence of low rank approximation and vanishes if AT B is
exactly rank-r. The second term results from matrix sketching and subsampling; the bounds have
similar dependencies as in [12].

(AT B)rk

AT B
k

and

−

We implement SMP-PCA in Apache Spark and perform several distributed experiments on synthetic
and real datasets. Our distributed implementation uses several design innovations described in Sec-
tion 4 and Appendix C.5 and it is the only Spark implementation that we are aware of that can handle
matrices that are large in both dimensions. Our experiments show that we improve by approximately
a factor of 2
in running time compared to the previous state of the art and scale gracefully as the
cluster size increases. The source code is available online [36].

×

1One straightforward idea is to sketch each matrix individually and perform SVD on the product of the sketches. We compare

against that scheme and show that we can perform arbitrarily better using our rescaled JL embedding.

2

•

In addition to better performance, our algorithm offers another advantage: It is possible to compute
low-rank approximations to AT B even when the entries of the two matrices arrive in some arbitrary
order (as would be the case in streaming logs). We can therefore discover signiﬁcant correlations even
when the original datasets cannot be stored, for example due to storage or privacy limitations.

2 Problem setting and algorithms

Rd×n2 that are stored in disk,
Consider the following problem: given two matrices A
ﬁnd a rank-r approximation of their product AT B. In particular, we are interested in the setting where
both A, B and AT B are too large to ﬁt into memory. This is common for modern large scale machine
learning applications. For this setting, we develop a single-pass algorithm SMP-PCA that computes the
rank-r approximation without explicitly forming the entire matrix AT B.

Rd×n1 and B

∈

∈

Notations. Throughout the paper, we use A(i, j) or Aij to denote (i, j) entry for any matrix A. Let
Ai and Aj be the i-th column vector and j-th row vector. We use
for
spectral (or operator) norm. The optimal rank-r approximation of matrix A is Ar, which can be found by
Rn1×n2 as the projection
SVD. Given a set Ω
⊂
of A on Ω, i.e., PΩ(A)(i, j) = A(i, j) if (i, j)

kF for Frobenius norm, and
A
k
Rn1×n2, we deﬁne PΩ(A)

[n2] and a matrix A

Ω and 0 otherwise.

A
k
k

[n1]

×

∈

∈

∈

2.1 SMP-PCA

Our algorithm SMP-PCA (Streaming Matrix Product PCA) takes four parameters as input: the desired rank
r, number of samples m, sketch size k, and the number of iterations T . Performance guarantee involving
these parameters is provided in Theorem 3.1. As illustrated in Figure 1, our algorithm has three main steps:
1) compute sketches and side information in one pass over A and B; 2) given partial information of A and
B, estimate important entries of AT B; 3) compute low rank approximation given estimates of a few entries
of AT B. Now we explain each step in detail.

B and the column norms

Figure 1: An overview of our algorithm. A single pass is performed over the data to produce the sketched
[n2]. We then compute the
,
matrices
A,
Bjk
Aik
k
k
×
Ω and
M ) through a biased sampling process, where PΩ(
sampled matrix PΩ(
M ) =
e
M as an estimator for AT B, and
zero otherwise. Here Ω represents the set of sampled entries. We deﬁne
f
M ) gives the
. Performing matrix completion on PΩ(
Aik · k
k
f

f
M (i, j) =
desired rank-r approximation.

compute its entry as

eBj
eAT
i
e
e
Bj k
Aik·k

M (i, j) if (i, j)

, for all (i, j)

Bjk ·

[n1]

f

∈

∈

e

k

Step 1: Compute sketches and side information in one pass over A and B. In this step we compute
(0, 1/k)

Rk×d is a random matrix with entries being i.i.d.

B := ΠB, where Π

A := ΠA and

sketches

f

N

f

e

e

∈

3

Algorithm 1 SMP-PCA: Streaming Matrix Product PCA

1: Input: A

∈
iterations: T

∈

Rd×n1, B

Rd×n2, desired rank: r, sketch size: k, number of samples: m, number of

2: Construct a random matrix Π
pass over A and B to obtain:

3: Sample each entry (i, j)

×
deﬁned in Eq.(1); maintain a set Ω

∈

∈
A = ΠA,

Rk×d, where Π(i, j)
B = ΠB, and

(0, 1/k),
,
Bjk
k

[k]
×
∼ N
,
[n1]
Aik
k
[n2] independently with probability ˆqij = min

[n2].
×
1, qij}
{
[n2] which stores all the sampled pairs (i, j).
Rn1×n2, where PΩ(

M (i, j) is given in Eq. (2). Calculate PΩ(

(i, j)
(i, j)

e
[n1]

[n1]
e

∈
∈

M )

∀
∀

×

⊂

[d]. Perform a single

, where qij is

Rn1×n2, where

Ω and zero otherwise.
f
M ), Ω, r, ˆq, T ), see Appendix A for more details.

f

∈

M ) =

f

∈

4: Deﬁne

M
M (i, j) if (i, j)
∈
f
5: Run WAltMin(PΩ(
6: Output:
U

f

Rn1×r and
f

∈

∈

V

b

b

Rn2×r.

random variables. It is known that Π satisﬁes an ”oblivious Johnson-Lindenstrauss (JL) guarantee” [29][34]
and it helps preserving the top row spaces of A and B [11]. Note that any sketching matrix Π that is an
oblivious subspace embedding can be considered here, e.g., sparse JL transform and randomized Hadamard
transform (see [12] for more discussion).

Besides
[n1]

A and

B, we also compute the L2 norms for all column vectors, i.e.,

, for all
[n2]. We use this additional information to design better estimates of AT B in the next step,
B to sample. Note that this is the only step that needs one

Bjk
k

Aik
k

and

AT

∈

(i, j)
×
and also to determine important entries of
e
e
pass over data.

Step 2: Estimate important entries of AT B by rescaled JL embedding. In this step we use partial
B. We ﬁrst determine
B to sample, and then propose a novel rescaled JL embedding for estimating those entries.

information obtained from the previous step to compute a few important entries of
what entries of

AT

AT

e

e

We sample entry (i, j) of AT B independently with probability ˆqij = min
e
1, qij}
{

, where
e

e

e

Bjk
+ k
2
B
2n1k
F
k
[n2] be the set of sampled entries (i, j). Since E(

Aik
( k
2
A
2n2k
F
k

qij = m

·

2

2

).

⊂

[n1]

Let Ω
i,j qij) = m, the expected number
of sampled entries is roughly m. The special form of qij ensures that we can draw m samples in O(n1 +
m log(n2)) time; we show how to do this in Appendix C.5.

P

×

Note that qij intuitively captures important entries of AT B by giving higher weight to heavy rows and
columns. We show in Section 3 that this sampling actually generates good approximation to the matrix
AT B.

The biased sampling distribution of Eq.

(1) is ﬁrst proposed by Bhojanapalli et al. [3]. However,
their algorithm [3] needs a second pass to compute the sampled entries, while we propose a novel way of
estimating dot products, using information obtained in the ﬁrst step.

Deﬁne

M

Rn1×n2 as

∈

f

M (i, j) =

Bjk ·

Aik · k
k

AT
Bj
i
Bjk
Aik · k
k
e
e
M , instead, we only calculate
e
e

.

Note that we will not compute and store
is denoted as PΩ(

M ), where PΩ(

f

We now explain the intuition of Eq. (2), and why

(i, j) entry of AT B, a straightforward way is to use
between vectors

f
Ai and

M (i, j) if (i, j)

M )(i, j) =
f

Ω and 0 otherwise.
f
B. To estimate the
M is a better estimator than
AT
θij is the angle
Bjk ·
Aik · k
Bj =
i
k
f
Bj. Since we already know the actual column norms, a potentially better estimator
e
e
e

AT
θij, where
e

cos

f

f

∈

e

e

e

e

M (i, j) for (i, j)

Ω. This matrix

∈

(1)

(2)

e

e

4

JL embedding
Rescaled JL embedding

2

1

0

-1

t
c
u
d
o
r
p

 
t

o
d

 

d
e

t

a
m

i
t
s
E

-2

-1

-0.5

0
True dot product

0.5

1

(a)

(b)

Figure 2: (a) Rescaled JL embedding (red dots) captures the dot products with smaller variance compared
to JL embedding (blue triangles). Mean squared error: 0.053 versus 0.129. (b) Lower ﬁgure illustrates how
to construct unit-norm vectors from a cone with angle θ. Let x be a ﬁxed unit-norm vector, and let t be a
(x + t) with probability
random Gaussian vector with expected norm tan(θ/2), we set y as either x + t or
AT B
half, and then normalize it. Upper ﬁgure plots the ratio of spectral norm errors
,
/
k
k
−
k
M has better
when the column vectors of A and B are unit vectors drawn from a cone with angle θ. Clearly,
B for all possible values of θ, especially when θ is small.
accuracy than

−
AT B
k

AT

AT

f

M

−

B

e

e

f

would be

e
Aik · k
k

e
Bjk ·

cos

θij. This removes the uncertainty that comes from distorted column norms2.

AT
i

e

Bj (JL embedding) and

Figure 2(a) compares the two estimators

M (i, j) (rescaled JL embedding)
for dot products. We plot simulation results on pairs of unit-norm vectors with different angles. The vectors
have dimension 1,000 and the sketching matrix has dimension 10-by-1,000. Clearly rescaling by the actual
norms help reduce the estimation uncertainty. This phenomenon is more prominent when the true dot
products are close to
1,
and hence the uncertainty from angles may produce smaller distortion compared to that from norms. In the
extreme case when cos θ =

1, which makes sense because cos θ has a small slope when cos θ approaches

1, rescaled JL embedding can perfectly recover the true dot product.

f

±

±

e

e

In the lower part of Figure 2(b) we illustrate how to construct unit-norm vectors from a cone with angle
θ. Given a ﬁxed unit-norm vector x, and a random Gaussian vector t with expected norm tan(θ/2), we
construct new vector y by randomly picking one from the two possible choices x + t and
(x + t), and then
renormalize it. Suppose the columns of A and B are unit vectors randomly drawn from a cone with angle
AT
θ, we plot the ratio of spectral norm errors
in Figure 2(b). We observe that
B and can be much better when θ approaches zero, which agrees with the trend
M always outperforms
indicated in Figure 2(a).
f
compute the low rank approximation of AT B from the samples using alternating least squares:

e
Step 3: Compute low rank approximation given estimates of few entries of AT B. Finally we

AT B
/
k
k

AT B
k

AT

f

M

−

−

−

B

e

e

e

k

±

min
U,V ∈Rn×r

wij(eT

i U V T ej −

M (i, j))2,

X(i,j)∈Ω

f

(3)

2We also tried using the cosine rule for computing the dot product, and another sketching method speciﬁcally designed for

preserving angles [4], but empirically those methods perform worse than our current estimator.

5

where wij = 1/ˆqij denotes the weights, and ei, ej are standard base vectors. This is a popular technique for
low rank recovery and matrix completion (see [3] and the references therein). After T iterations, we will get
a rank-r approximation of
M presented in the convenient factored form. This subroutine is quite standard,
so we defer the details to Appendix A.

f

3 Analysis

Now we present the main theoretical result. Theorem 3.1 characterizes the interaction between the sketch
[
AT Brk
size k, the sampling complexity m, the number of iterations T , and the spectral error
,
[
AT Br is the output of SMP-PCA, and (AT B)r is the optimal rank-r approximation of AT B. Note
where
= n2,
that the following theorem assumes that A and B have the same size. For the general case of n1 6
.
Theorem 3.1 is still valid by setting n = max
n1, n2}
{

(AT B)r −
k

Theorem 3.1. Given matrices A
kAk2
of AT B. Deﬁne ˜r = max
F
of (AT B)r, where σ∗
[
AT Br be the output of Algorithm SMP-PCA. If the input parameters k, m, and T satisfy

as the maximum stable rank, and ρ = σ∗
1
σ∗
r

}
i is the i-th singular values of AT B.

Rd×n, let (AT B)r be the optimal rank-r approximation
as the condition number

∈
kAk2 , kBk2
F
kBk2

Rd×n and B

Let

∈

{

(4)

(5)

(6)

(7)

2ρ2r3
k
2
F
k

·

k

≥

m

2
B
A
C1k
k
k
AT B
k
C2˜r2
γ

≥

·

max

˜r, 2 log(n)
}
{
η2

+ log (3/γ)

,

2

nr3ρ2 log(n)T 2
η2

,

·

2
2
B
F +
A
F
k
k
k
k
AT B
kF (cid:19)
k
kF +
A
ζ

log( k

≥

(cid:18)

T

B
k

kF

),

where C1 and C2 are some global constants independent of A and B. Then with probability at least 1
we have

−

γ,

(AT B)r −
k

[
AT Brk ≤

η

AT B
k

(AT B)rkF + ζ + ησ∗
r .

−

Remark 1. Compared to the two-pass algorithm proposed by [3], we notice that Eq. (7) contains an
additional error term ησ∗
r . This extra term captures the cost incurred when we are approximating entries of
AT B by Eq. (2) instead of using the actual values. The exact tradeoff between η and k is given by Eq. (4).
On one hand, we want to have a small k so that the sketched matrices can ﬁt into memory. On the other
hand, the parameter k controls how much information is lost during sketching, and a larger k gives a more
accurate estimation of the inner products.

kF is much smaller than

Remark 2. The dependence on kAk2
kF or
A
k

captures one difﬁcult situation for our algorithm.
If
AT B
kF , which could happen, e.g., when many column vectors
k
of A are orthogonal to those of B, then SMP-PCA requires many samples to work. This is reasonable.
Imagine that AT B is close to an identity matrix, then it may be hard to tell it from an all-zero matrix without
enough samples. Nevertheless, removing this dependence is an interesting direction for future research.

kAT BkF
B
k

F +kBk2
F

Remark 3. For the special case of A = B, SMP-PCA computes a rank-r approximation of ma-
trix AT A in a single pass. Theorem 3.1 provides an error bound in spectral norm for the residual matrix
[
(AT A)r −
AT Ar. Most results in the online PCA literature use Frobenius norm as performance measure.
Recently, [22] provides an online PCA algorithm with spectral norm guarantee. They achieves a spectral
norm bound of ǫσ∗
r+1, which is stronger than ours. However, their algorithm requires a target dimension

1 +σ∗

6

of O(r log n/ǫ2), i.e., the output is a matrix of size n-by-O(r log n/ǫ2), while the output of SMP-PCA is
simply n-by-r.

Remark 4. We defer our proofs to Appendix C. The proof proceeds in three steps. In Appendix C.2, we
show that the sampled matrix provides a good approximation of the actual matrix AT B. In Appendix C.3,
V and the
we show that there is a geometric decrease in the distance between the computed subspaces
optimal ones U ∗, V ∗ at each iteration of WAltMin algorithm. The spectral norm bound in Theorem 3.1 is
then proved using results from the previous two steps.

U ,

b

b

Computation Complexity. We now analyze the computation complexity of SMP-PCA. In Step 1, we
compute the sketched matrices of A and B, which requires O(nnz(A)k + nnz(B)k) ﬂops. Here nnz(
)
·
denotes the number of non-zero entries. The main job of Step 2 is to sample a set Ω and calculate the
corresponding inner products, which takes O(m log(n) + mk) ﬂops. Here we deﬁne n as max
for
simplicity. According to Eq. (4), we have log(n) = O(k), so Step 2 takes O(mk) ﬂops. In Step 3, we run
alternating least squares on the sampled matrix, which can be completed in O((mr2 + nr3)T ) ﬂops. Since
Eq. (5) indicates nr = O(m), the computation complexity of Step 3 is O(mr2T ). Therefore, SMP-PCA
has a total computation complexity O(nnz(A)k + nnz(B)k + mk + mr2T ).

n1, n2}
{

4 Numerical Experiments

Spark implementation. We implement our SMP-PCA in Apache Spark 1.6.2 [37]. For the purpose of
comparison, we also implement a two-pass algorithm LELA [3] in Spark3. The matrices A and B are
stored as a resilient distributed dataset (RDD) in disk (by setting its StorageLevel as DISK_ONLY). We
implement the two passes of LELA using the treeAggregate method. For SMP-PCA, we choose the
subsampled randomized Hadamard transform (SRHT) [32] as the sketching matrix 4. The biased sampling
procedure is performed using binary search (see Appendix C.5 for how to sample m elements in O(m log n)
time). After obtaining the sampled matrix, we run ALS (alternating least squares) to get the desired low-rank
matrices. More details can be found in [36].

Description of datasets. We test our algorithm on synthetic datasets and three real datasets: SIFT10K [20],

NIPS-BW [23], and URL-reputation [24]. For synthetic data, we generate matrices A and B as GD, where
G has entries independently drawn from standard Gaussian distribution, and D is a diagonal matrix with
Dii = 1/i. SIFT10K is a dataset of 10,000 images. Each image is represented by 128 features. We set A as
the image-by-feature matrix. The task here is to compute a low rank approximation of AT A, which is a stan-
dard PCA task. The NIPS-BW dataset contains bag-of-words features extracted from 1,500 NIPS papers.
We divide the papers into two subsets, and let A and B be the corresponding word-by-paper matrices, so
AT B computes the counts of co-occurred words between two sets of papers. The original URL-reputation
dataset has 2.4 million URLs. Each URL is represented by 3.2 million features, and is indicated as ma-
licious or benign. This dataset has been used previously for CCA [25]. Here we extract two subsets of
features, and let A and B be the corresponding URL-by-feature matrices. The goal is to compute a low rank
approximation of AT B, the cross-covariance matrix between two subsets of features.

Sample complexity. In Figure 4(a) we present simulation results on a small synthetic data with n = d =
5, 000 and r = 5. We observe that a phase transition occurs when the sample complexity m = Θ(nr log n).
This agrees with the experimental results shown in previous papers, see, e.g., [9, 3]. For the rest experiments
presented in this section, unless otherwise speciﬁed, we set r = 5, T = 10, and sampling complexity m as
4nr log n.

3To our best knowledge, this the ﬁrst distributed implementation of LELA.
4Compared to Gaussian sketch, SRHT reduces the runtime from O(ndk) to O(nd log d) and space cost from O(dk) to O(d),

while maintains the same quality of the output.

7

Runtime (sec) vs Cluster size

LELA
SMC-PCA

eAT eB)
SVD(
SMP-PCA
LELA
Optimal

SVD( eAT eB)
SMP-PCA
LELA
Optimal

3000

2000

1000

0.2

0.15

0.1

0.05

r
o
r
r
e
 
m
r
o
n
 
l
a
r
t
c
e
p
S

0

2

10

5

(a)

1000
Sketch size (k)

2000

1000
Sketch size (k)

2000

Figure 3: (a) Spark-1.6.2 running time on a 150GB dataset. All nodes are m.2xlarge EC2 instances. See [36]
for more details. (b) Spectral norm error achieved by three algorithms over two datasets: SIFT10K (left)
and NIPS-BW (right). We observe that SMP-PCA outperforms SVD(
B) by a factor of 1.8 for SIFT10K
and 1.1 for NIPS-BW. Besides, the error of SMP-PCA keeps decreasing as the sketch size k grows.

AT

0.3

0.25

0.2

0.15

0.1

0.05

(b)

e

e

Table 1: A comparison of spectral norm error over three datasets

Dataset

d

n

Algorithm Sketch size k

Error

Synthetic

100,000

100,000

URL-
malicious

URL-
benign

792,145

10,000

1,603,985

10,000

Optimal
LELA
SMP-PCA

Optimal
LELA
SMP-PCA

Optimal
LELA
SMP-PCA

-
-
2,000

-
-
2,000

-
-
2,000

0.0271
0.0274
0.0280

0.0163
0.0182
0.0188

0.0103
0.0105
0.0117

Comparison of SMP-PCA and LELA. We now compare the statistical performance of SMP-PCA
and LELA [3] on three real datasets and one synthetic dataset. As shown in Figure 3(b) and Table 1, LELA
always achieves a smaller spectral norm error than SMP-PCA, which makes sense because LELA takes
two passes and hence has more chances exploring the data. Besides, we observe that as the sketch size
increases, the error of SMP-PCA keeps decreasing and gets closer to that of LELA.

In Figure 3(a) we compare the runtime of SMP-PCA and LELA using a 150GB synthetic dataset on
m3.2xlarge Amazon EC2 instances5. The matrices A and B have dimension n = d = 100, 000. The sketch
dimension is set as k = 2, 000. We observe that the speedup achieved by SMP-PCA is more prominent for
small clusters (e.g., 56 mins versus 34 mins on a cluster of size two). This is possibly due to the increasing
spark overheads at larger clusters, see [17] for more related discussion.

Comparison of SMP-PCA and SVD(

AT

generating column vectors of A and B from a cone with angle θ. Here SVD(

B). In Figure 4(b) we repeat the experiment in Section 2 by
B) refers to computing

AT

5Each machine has 8 cores, 30GB memory, and 2×80GB SSD.

e

e

e

e

8

0.5

0.4

0.3

0.2

r
o
r
r
e
 
m
r
o
n
 
l
a
r
t
c
e
p
S

Ratio of errors vs theta

105

k = 400

k = 800

r Br

AT
SMP-PCA

 

r
o
r
r
e
m
r
o
n

 
l

a
r
t
c
e
p
S

1

0.8

0.6

0.4

0.2

2

1
# Samples / nrlogn

3

4

100

0

π/4

π/2 3π/4

π

200 400 600 800 1000
Sketch size (k)

(a)

(b)

(c)

Figure 4: (a) A phase transition occurs when the sample complexity m = Θ(nr log n). (b) This ﬁgure plots
the ratio of spectral norm error of SVD(
B) over that of SMP-PCA. The columns of A and B are unit
vectors drawn from a cone with angle θ. We see that the ratio of errors scales to inﬁnity as the cone angle
shrinks. (c) If the top r left singular vectors of A are orthogonal to those of B, the product AT
r Br is a very
poor low rank approximation of AT B.

AT

e

e

SVD on the sketched matrices6. We plot the ratio of the spectral norm error of SVD(
B) over that of
SMP-PCA, as a function of θ. Note that this is different from Figure 2(b), as now we take the effect of
random sampling and SVD into account. However, the trend in both ﬁgures are the same: SMP-PCA
B) and can be arbitrarily better as θ goes to zero.
always outperforms SVD(
AT
AT B
||
e
e

The y-axis represents spectral norm error, deﬁned as
approximation found by a speciﬁc algorithm. We observe that SMP-PCA outperforms SVD(
factor of 1.8 for SIFT10K and 1.1 for NIPS-BW.

B) on two real datasets SIFK10K and NIPS-BW.
[
AT Br is the rank-r
, where
AT
B) by a

AT
In Figure 3(b) we compare SMP-PCA and SVD(

[
AT B
AT Br||
/
||

AT

−

e

e

e

e

||

M is a better estimator for AT B than

Now we explain why SMP-PCA produces a more accurate result than SVD(

AT
B). The reasons are
e
AT
twofold. First, our rescaled JL embedding
B (Figure 2). Second,
e
the noise due to sampling is relatively small compared to the beneﬁt obtained from
M , and hence the ﬁnal
M ) still outperforms SVD(
result computed using PΩ(
e
e
f

r Br. Let Ar and Br be the optimal rank-r approximation of A and
B, we show that even if one could use existing methods (e.g., algorithms for streaming PCA) to estimate Ar
r Br can be a very poor low rank approximation of AT B. This is demonstrated in
and Br, their product AT
Figure 4(c), where we intentionally make the top r left singular vectors of A orthogonal to those of B.

Comparison of SMP-PCA and AT

B).

AT

f

f

e

e

e

e

5 Conclusion

We develop a novel one-pass algorithm SMP-PCA that directly computes a low rank approximation of a
matrix product, using ideas of matrix sketching and entrywise sampling. As a subroutine of our algorithm,
we propose rescaled JL for estimating entries of AT B, which has smaller error compared to the standard
estimator ˜AT ˜B. This we believe can be extended to other applications. Moreover, SMP-PCA allows the
non-zero entries of A and B to be presented in any arbitrary order, and hence can be used for steaming
applications. We design a distributed implementation for SMP-PCA. Our experimental results show that

6This can be done by standard power iteration based method, without explicitly forming the product matrix eAT eB, whose size

is too big to ﬁt into memory according to our assumption.

9

SMP-PCA can perform arbitrarily better than SVD(
rithms that require two or more passes over the data.

AT

B), and is signiﬁcantly faster compared to algo-

Acknowledgements We thank the anonymous reviewers for their valuable comments. This research has
e
been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000, and ARO
YIP W911NF-14-1-0258.

e

10

References

[1] D. Achlioptas and F. McSherry. Fast computation of low rank matrix approximations. In Proceedings
of the thirty-third annual ACM symposium on Theory of computing, pages 611–618. ACM, 2001.

[2] A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental pca. In Advances

in Neural Information Processing Systems, pages 3174–3182, 2013.

[3] S. Bhojanapalli, P. Jain, and S. Sanghavi. Tighter low-rank approximation via sampling the leveraged
element. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 902–920. SIAM, 2015.

[4] P. T. Boufounos. Angle-preserving quantized phase embeddings.
Applications. International Society for Optics and Photonics, 2013.

In SPIE Optical Engineering+

[5] C. Boutsidis, D. Garber, Z. Karnin, and E. Liberty. Online principal components analysis. In Pro-
ceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887–901.
SIAM, 2015.

[6] C. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized hadamard

transform. SIAM Journal on Matrix Analysis and Applications, 34(3):1301–1340, 2013.

[7] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Compu-

tational mathematics, 9(6):717–772, 2009.

[8] X. Chen, H. Liu, and J. G. Carbonell. Structured sparse canonical correlation analysis. In International

Conference on Artiﬁcial Intelligence and Statistics, pages 199–207, 2012.

[9] Y. Chen, S. Bhojanapalli, S. Sanghavi, and R. Ward. Completing any low-rank matrix, provably. arXiv

preprint arXiv:1306.2979, 2013.

[10] K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In Proceedings
of the forty-ﬁrst annual ACM symposium on Theory of computing, pages 205–214. ACM, 2009.

[11] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pages 81–90.
ACM, 2013.

[12] M. B. Cohen, J. Nelson, and D. P. Woodruff. Optimal approximate matrix product in terms of stable

rank. arXiv preprint arXiv:1507.02268, 2015.

[13] A. Deshpande and S. Vempala. Adaptive sampling and fast low-rank matrix approximation. In Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 292–
303. Springer, 2006.

[14] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices ii: Computing a

low-rank approximation to a matrix. SIAM Journal on Computing, 36(1):158–183, 2006.

[15] P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-error matrix
approximation: Column-based methods. In Approximation, Randomization, and Combinatorial Opti-
mization. Algorithms and Techniques, pages 316–326. Springer, 2006.

[16] A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo algorithms for ﬁnding low-rank approxima-

tions. Journal of the ACM (JACM), 51(6):1025–1041, 2004.

11

[17] A. Gittens, A. Devarakonda, E. Racah, M. F. Ringenburg, L. Gerhardt, J. Kottalam, J. Liu, K. J.
Maschhoff, S. Canon, J. Chhugani, P. Sharma, J. Yang, J. Demmel, J. Harrell, V. Krishnamurthy,
M. W. Mahoney, and Prabhat. Matrix factorization at scale: a comparison of scientiﬁc data analytics
in spark and C+MPI using three case studies. arXiv preprint arXiv:1607.01335, 2016.

[18] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algo-
rithms for constructing approximate matrix decompositions. SIAM review, 53(2):217–288, 2011.

[19] S. Har-Peled. Low rank matrix approximation in linear time. Manuscript. http://valis. cs. uiuc.

edu/sariel/papers/05/lrank/lrank. pdf, 2006.

[20] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. Pattern Analysis

and Machine Intelligence, IEEE Transactions on, 33(1):117–128, 2011.

[21] R. Kannan, S. S. Vempala, and D. P. Woodruff. Principal component analysis and higher correlations
for distributed data. In Proceedings of The 27th Conference on Learning Theory, pages 1040–1057,
2014.

[22] Z. Karnin and E. Liberty. Online pca with spectral bounds. In Proceedings of The 28th Conference on

Learning Theory (COLT), volume 40, pages 1129–1140, 2015.

[23] M. Lichman. UCI machine learning repository. http://archive.ics.uci.edu/ml, 2013.

[24] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Identifying suspicious urls: an application of large-
scale online learning. In Proceedings of the 26th annual international conference on machine learning,
pages 681–688. ACM, 2009.

[25] Z. Ma, Y. Lu, and D. Foster. Finding linear structure in large datasets with scalable canonical correla-

tion analysis. arXiv preprint arXiv:1506.08170, 2015.

[26] A. Magen and A. Zouzias. Low rank matrix-valued chernoff bounds and approximate matrix multipli-
cation. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,
pages 1422–1436. SIAM, 2011.

[27] I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited, streaming pca.

In Advances in Neural

Information Processing Systems, pages 2886–2894, 2013.

[28] N. H. Nguyen, T. T. Do, and T. D. Tran. A fast and efﬁcient algorithm for low-rank approximation of
a matrix. In Proceedings of the 41st annual ACM symposium on Theory of computing, pages 215–224.
ACM, 2009.

[29] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In Founda-
tions of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pages 143–152. IEEE,
2006.

[30] O. Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In Proceedings
of the 32nd International Conference on Machine Learning (ICML-15), pages 144–152, 2015.

[31] T. Tao. 254a, notes 3a: Eigenvalues and sums of hermitian matrices. Terence Tao’s blog, 2010.

[32] J. A. Tropp.

Improved analysis of the subsampled randomized hadamard transform. Advances in

Adaptive Data Analysis, pages 115–126, 2011.

12

[33] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational

Mathematics, 12(4):389–434, 2012.

[34] D. P. Woodruff. Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357,

2014.

[35] F. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. A fast randomized algorithm for the approximation

of matrices. Applied and Computational Harmonic Analysis, 25(3):335–366, 2008.

[36] S. Wu, S. Bhojanapalli, S. Sanghavi, and A. Dimakis. Github repository for ”single-pass pca of matrix

products”. https://github.com/wushanshan/MatrixProductPCA, 2016.

[37] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and
I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.
In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,
2012.

13

A Weighted alternating minimization

Algorithm 2 provides a detailed explanation of WAltMin, which follows a standard procedure for matrix
PΩ(A) to denote the Hadamard product between w and PΩ(A):
completion. We use RΩ(A) = w.
Rn1×n2 = 1/ˆqij is
RΩ(A)(i, j) = w(i, j)
the weight matrix. Similarly we deﬁne the matrix R1/2
PΩ(A)(i, j) for
(i, j)

Ω and 0 otherwise, where w
Ω (A) as R1/2

∗
PΩ(A)(i, j) for (i, j)

Ω and 0 otherwise.

Ω (A)(i, j) =

∈
w(i, j)

∈

∗

∗

The algorithm contains two parts: initialization (Step 2-6) and weighted alternating minimization (Step
M ) and then set row i of U (0)
7-10). In the ﬁrst part, we compute SVD of the weighted sampled matrix RΩ(
to be zero if its norm is larger than a threshold (Step 6). More details of this trim step can be found in [3].
In the second part, the goal is to solve the following non-convex problem by alternating minimization:

f

p

∈

min
U,V

wij(eT

i U V T ej −

M (i, j))2,

X(i,j)∈Ω

f

(8)

where ei, ej are standard base vectors. After running T iterations, the algorithm outputs a rank-r approxi-
mation of

M presented in the convenient factored form.

Ω0, . . . , Ω2T }
{

Algorithm 2 WAltMin [3]

f

∗

∀

∈

i, j

M )

M )

PΩ0(

Rn1×n2, Ω, r, ˆq, and T

f
M ) = w.

1: Input: PΩ(
2: wij = 1/ˆqij when ˆqij > 0, 0 else,
3: Divide Ω in 2T + 1 equal uniformly random subsets, i.e., Ω =
4: RΩ0(
5: U (0)Σ(0)(V (0))T = SVD(RΩ0(
6: Trim U (0) and let
f
7: for t = 0 to T
8:

U (0) be the output
1 do
V (t+1) = argminV k
b
U (t+1) = argminU k
9:
b
10: end for
U (T )
11: Output:
b

2
U (t)V T )
F
k
2
V (t+1))T )
F
k

U (
b
Rn2×r.
b

M
f
V (T )
f

Ω2t+1(
Ω2t+2(

R1/2
R1/2

Rn1×r and

M ), r)

f
M

f

−

−

−

∈

b

∈

b

B Technical Lemmas

We will frequently use the following concentration inequality in the proof.

Lemma B.1. (Matrix Bernstein’s Inequality [33]). Consider p independent random matrices X1, ...., Xp in
Rn×n, where each matrix has bounded deviation from its mean:

Xi −
||
Let the norm of the covariance matrix be

E[Xi]

L,

|| ≤

i.

∀

p

(Xi −

σ2 = max

E

(cid:12)
((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Then the following holds for all t
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi=1

"

E[Xi])T

(Xi −

E[Xi])T (Xi −

E[Xi])

p

"
Xi=1

,

E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2n exp( −

t2/2
σ2 + Lt/3

).

E[Xi])(Xi −
0:

≥
p

(Xi −

P

(cid:12)
"(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[Xi])
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

# ≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
14

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

A formal deﬁnition of JL transform is given below [29][34].

Deﬁnition B.2. A random matrix Π
for short, if with probability at least 1

Πv, Πv′

v, v′

i − h

i| ≤

ǫ

v
||

|| · ||

v′

.
||

|h

∈
−

Rk×d forms a JL transform with parameters ǫ, δ, f or JLT(ǫ, δ, f )
V it holds that
δ, for any f -element subset V

Rd, for all v, v′

⊂

∈

The following lemma [34] characterizes the tradeoff between the reduced dimension k and the error

level ǫ.

N

Lemma B.3. Let 0 < ǫ, δ < 1, and Π

Rk×d be a random matrix where the entries Π(i, j) are i.i.d.

(0, 1/k) random variables. If k = Ω(log(f /δ)ǫ−2), then Π is a JLT(ǫ, δ, f ).

∈

We now present two lemmas that connect

A

Rk×n and

B

Rd×n with A

Rd×n and B

Rd×n.

∈
∈
∈
Lemma B.4. Let 0 < ǫ, δ < 1, if k = Ω( log(2n/δ)
), then with probability at least 1
e

ǫ2

e

δ,

−

∈

(1

ǫ)

A

||

2
F ≤ ||
||

A

2
F ≤
||

−

(1 + ǫ)

A

2
F ,
||
||
AT B

(1

ǫ)

B

||

−
ǫ

A

2
F ≤ ||
||
||F .
B

AT
||

B

||F ≤
Proof. This is again a standard result of JL transformation, e.g., see Deﬁnition 2.3 and Theorem 2.1 of [34]
and Lemma 6 of [29] .

||F ||

−

e

e

e

e

||

B

2
F ≤
||

(1 + ǫ)

B

2
F ,
||

||

Lemma B.5. Let 0 < ǫ, δ < 1, if k = Θ( ˜r+log(1/δ)
rank, then with probability at least 1

δ,

ǫ2

−

), where ˜r = max

is the maximum stable

||A||2
F

||A||2 , ||B||2

F
||B||2

{

}

Proof. This follows from a recent paper [12].
e

e

AT
||

B

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

Using the above two lemmas, we can prove the following two lemmas that relate

AT
M is DA
deﬁned in Algorithm 1. A more compact deﬁnition of
.
matrices with (DA)ii =
Bj ||
/
Bj||
Ai||
/
Ai||
||
||
||
||
f
e
Lemma B.6. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( log(2n/δ)
e
e

and (DB)jj =

ǫ2

e

M with AT B, for

M
BDB, where DA and DB are diagonal
f
f

), then with probability at least 1

δ,

−

AT
i Bj| ≤

ǫ

Ai|| · ||
||

,
Bj||

M

||

−

AT B

||F ≤

ǫ

A

||

||F ||

B

||F .

Mij −
|
f

Proof. Let 0 < ǫ < 1/2, 0 < δ < 1, according to the Deﬁnition B.2 and Lemma B.3, we have that if
k = Ω( log(2n/δ)

), then with probability at least 1

δ, and for all i, j

f

ǫ2

1 + ǫ,

1

ǫ

−

≤

AT
i Bj|

as

−
(DB)jj ≤

1 + ǫ,

AT
i
|

e

Bj −
e

AT
i Bj| ≤

ǫ

Ai||||
||

.
Bj||

(9)

We can now bound

Mij −
|
AT
i
f
|

ǫ

1

≤

−

(DA)ii ≤
Mij −
|
AT
i Bj|
f
Bj(DA)ii(DB)jj −
Bj(1 + ǫ)2
e
{|
(1 + ǫ)2ǫ
e
e
{
Ai||||
||

,
Bj ||

AT
i

−
Bj||
Ai||||
||

max
e

max

7ǫ

ξ1=
ξ2

≤
ξ3

≤
ξ4

≤

AT
i Bj|
AT
AT
,
i Bj|
i
|
+ ((1 + ǫ)2
e

e

Bj(1

ǫ)2

−

−
AT
i Bj|
1)
|

AT
i Bj|}
, (1

−

−

15

ǫ)2ǫ

Ai||||
||

Bj||

+ (1

(1

−

−

AT
ǫ)2)
i Bj|}
|
(10)

Mij, ξ2 follows from the bound in Eq.(9), ξ3 follows from triangle
. Now rescaling ǫ as ǫ/7 gives the desired

where ξ1 follows from the deﬁnition of
inequality and Eq.(9), and ξ4 follows from
bound in Lemma B.6.
f
Mij −
f

||F =

Hence,

AT B

ij |

M

−

||

qP
Lemma B.7. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( ˜r+log(n/δ)

f

qP
ǫ2

AT
i Bj| ≤ ||
|
AT
i Bj|

2

≤

Ai||||

Bj ||
ij ǫ2

Proof. We can bound the spectral norm of the difference matrix as follows:

f

M

||

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

2

Ai||
||

Bj||
||

2 = ǫ

A

||F ||

B

||F .

||

), then with probability at least 1

δ,

−

M

||

−

AT B

ξ1=

||

f

AT
DA
BDB −
||
AT
DA||||
B
−
e
e
(1 + ǫ)2ǫ
e
||
B
A
7ǫ

A
e
||||
,
||

||||

||

B

≤ ||
ξ3

≤

≤

||

DAAT BDB + DAAT BDB −
AT B
DA||||
+
||||
||
B

DB||
+ (1 + ǫ)ǫ

AT B

+ ǫ

||||

A

A

DAAT B + DAAT B
I
DA −
+
DB −
||
B

||

I

||

||||

||

||

||||

||

AT B
||
AT B

||

−

||||

(11)

Mij, and ξ2 follows from Lemma B.5 and bound in Eq.(9). Rescaling

where ξ1 follows from the deﬁnition of
ǫ as ǫ/7 gives the desired bound in Lemma B.7.

f

We will frequently use the term with high probability. Here is a formal deﬁnition.

Deﬁnition B.8. We say that an event E occurs with high probability (w.h.p.) in n if the probability that its
complement ¯E happens is polynomially small, i.e., P r( ¯E) = O( 1

nα ) for some constant α > 0.

The following two lemmas deﬁne a ”nice” Π and when this happens with high probability.

Deﬁnition B.9. The random Gaussian matrix Π is ”nice” with parameter ǫ if for all (i, j) such that qij ≤
(i.e., qij = ˆqij), the sketched values

Mij satisﬁes the following two inequalities:

1

Mij|
|
ˆqij ≤
f

(1 + ǫ)

n
m

f
2
F +
||

A

(
||

B

2
F ),
||

||

X{j:ˆqij=qij } f

M 2
ij
ˆqij ≤

(1 + ǫ)

2n
m

A

(
||

2
F +
||

B

F )2.
2
||

||

Lemma B.10. If k = Ω( log(n)
w.h.p. in n.

ǫ2

), and 0 < ǫ < 1/14, then the random Gaussian matrix Π

Proof. According to Lemma B.6, if k = Ω( log(n)
ǫ

ǫ2

. In other words, the following holds with probability at least 1

δ:

), then w.h.p. in n, for all (i, j) we have

Ai|| · ||
||

Bj||

−

Mij| ≤ |
|
The above inequality is sufﬁcient for Π to be ”nice”:
f

Ai|| · ||
||

+ ǫ

AT
i Bj|

Bj|| ≤

(1 + ǫ)

Ai|| · ||
||

,
Bj||

(i, j)

∀

Rk×d is ”nice”

∈

AT
i Bj| ≤

Mij −
|
f

Mij
ˆqij ≤
f

(1 + ǫ) ||

Ai|| · ||
ˆqij

Bj||

≤

(1 + ǫ)

2 +
Ai||
(
||
( ||Ai||2
2n||A||2
F

2)/2
Bj||
||
+ ||Bj||2
2n||B||2
F

m

·

) ≤

(1 + ǫ)

n
m

A

(
||

2
F +
||

B

2
F )
||

||

16

M 2
ij
ˆqij ≤

(1 + ǫ)2

2

2

Bj||
||

Ai||
||
ˆqij

X{j:ˆqij=qij} f

X{j:ˆqij=qij}

4 +
Ai||
||
( ||Ai||2
2n||A||2
F

4

Bj||
||
+ ||Bj||2
2n||B||2
F

)

m

·

X{j:ˆqij=qij}
2n
2
F +
A
(
m
||
||

B

F )2.
2
||

||

(1 + ǫ)

(1 + ǫ)

≤

≤

ǫ2

Therefore, we conclude that if k = Ω( log(n)

), then Π is ”nice” w.h.p. in n.

C Proofs

C.1 Proof overview

We now present the key steps in proving Theorem 3.1. The framework is similar to that of LELA [3].

Our proof proceeds in three steps. In the ﬁrst step, we show that the sampled matrix provides a good
approximation of the actual matrix AT B. The result is summarized in Lemma C.1. Here RΩ(
M ) denotes
the sampled matrix weighted by the inverse of sampling probability (see Line 4 of Algorithm 2). Detailed
proof can be found in Appendix C.2. For consistency, we will use Ci (i = 1, 2, ...) to denote global constant
that can vary from step to step.

f

Lemma C.1. (Initialization) Let m and k satisfy the following conditions for sufﬁciently large constants C1
and C2:

then the following holds w.h.p. in n:

m

C1

≥

k

≥

||

A

B

2
2
F +
F
||
||
||
AT B
||F (cid:19)
||
2
A
||
||
AT B

˜r + log(n)
δ2

·

2 n
δ2 log(n),
B

,

(cid:18)

C2

2
||
2
F
||

||
||

RΩ(

M )

||

AT B

δ

AT B
||

||F .

|| ≤

−

In the second step, we show that at each iteration of WAltMin algorithm, there is a geometric decrease
V and the optimal ones U ∗, V ∗. The result is shown
in the distance between the computed subspaces
in Lemma C.2. Appendix C.3 provides the detailed proof. Here for any two orthonormal matrices X and
, where X⊥
Y , we deﬁne their distance as the principal angle based distance, i.e., dist(X, Y ) =
b
||
denotes the subspace orthogonal to X.

X T
||

⊥Y

U ,

f

b

Lemma C.2. (WAltMin Descent) Let k, m, and T satisfy the conditions stated in Theorem 3.1. Also,
(AT B)r||F . Let ˆU (t) and ˆV (t+1) be the t-th and
1
consider the case when
576ρr1.5 ||
(t+1)-th step iterates of the WAltMin procedure. Let U (t) and V (t+1) be the corresponding orthonormal
(U (t))i
1/2. Denote AT B as M , then the
||F and dist(U (t), U ∗)
matrices. Let
/
Ai||
A
||
||
||
γ/T :
following holds with probability at least 1

(AT B)r||F ≤

AT B
||

8√rρ

|| ≤

≤

−

dist(V t+1, V ∗)

dist(U t, U ∗) + η

M

Mr||F /σ∗

r + η,

−

||

−
1
2

≤

(V (t+1))j
||

|| ≤

8√rρ

/
Bj||
||
||

B

||F .

17

In the third step, we prove the spectral norm bound in Theorem 3.1 using results from the above two
lemmas. Comparing Lemma C.1 and C.2 with their counterparts of LELA (see Lemma C.2 and C.3 in [3]),
we notice that Lemma C.1 has the same bound as that of LELA, but the bound in Lemma C.2 contains
an extra term η. This term eventually leads to an additive error term ησ∗
r in Eq.(7). Detailed proof is in
Appendix C.4.

C.2 Proof of Lemma C.1

We ﬁrst prove the following lemma, which shows that RΩ(
we deﬁne CAB := (||A||2

F )2

.

F +||B||2
||AT B||2
F

Lemma C.3. Suppose Π is ﬁxed and is ”nice”. Let m
constant C1, then w.h.p. in n, the following is true:

M ) is close to

M . For simplicity of presentation,

f
C1 ·

≥

f

CAB

n
δ2 log(n) for sufﬁciently large global

RΩ(

M )

M

−

|| ≤

||

δ

AT B
||

||F .

Proof. This lemma can be proved in the same way as the proof of Lemma C.2 in [3]. The key idea is to use
f
the matrix Bernstein inequality. Let Xij = (δij −
j , where δij is a
random variable
ˆqij)wij
n
indicating whether the value at (i, j) has been sampled. Since Π is ﬁxed,
i,j=1 are independent zero
mean random matrices. Furthermore,

Xij}
{

0, 1
}
{

MijeieT

f

M )

M .

f
n
i,j=1 = RΩ(

−

Since Π is ”nice” with parameter 0 < ǫ < 1/14, we can bound the 1st and 2nd moment of Xij as

f

f

Xij}

i,j{

P

follows:

Xij ||
||

= max

(1

ˆqij)wij

{|

−

ˆqijwij
|

,
Mij|
f

ξ1

≤

Mij|
|
ˆqij
f

Mij|} ≤
f

(1 + ǫ)

n
m

A

(
||

2
F +
||

B

2
F );
||

||

σ2 = max

XijX T

ij 

E

,


Xij


E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
ˆqij −

1)

{(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(
|

= max

i



(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Xij
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

M 2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ij
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆqij
X{j:ˆqij=qij } f

M 2
ij |

ξ3

≤

f

ξ2= max
i

ˆqij(1

−

ˆqij)w2
ij

X T
ij Xij
(cid:12)
(cid:12)
}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(1 + ǫ)

ξ1

≤

(cid:12)
(cid:12)
Xj
(cid:12)
(cid:12)
(cid:12)
2
(cid:12)
F +
||

2n
m

A

(
||

B

F )2,
2
||

||

M 2
ij(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f

1. Now we can use matrix Bernstein inequality (see Lemma B.1) with t = δ

where ξ1 follows from Lemma B.10, ξ2 follows from a direct calculation, and ξ3 follows from the fact that
||F to show that
ˆqij ≤
n
δ2 log(n), then the desired inequality holds w.h.p. in n, where C1 is some global
if m
≥
constant independent of A and B. Note that since 0 < ǫ < 1/14, (1 + ǫ) < 2. Rescaling C1 gives the
desired result.

(1 + ǫ)C1CAB

AT B
||

Now we are ready to prove Lemma C.1, which is a counterpart of Lemma C.2 in [3].

Proof. We ﬁrst show that
||F holds w.h.p. in n over the randomness of Π. Note
that in Lemma C.3, we have shown that it is true for a ﬁxed and ”nice” Π, now we want to show that it also
holds w.h.p. in n even for a random chosen Π.

RΩ(

|| ≤

M )

f

f

M

−

||

δ

AT B
||

Let G be the event that we desire, i.e., G =

AT B
||
complimentary event. By conditioning on Π, we can bound the probability of ¯G as

RΩ(

|| ≤

AT

AT

B)

{||

−

B

δ

||F }

. Let ¯G be the

P r( ¯G) = P r( ¯G
P r( ¯G

≤

e
Π is ”nice”)P r(Π is ”nice”) + P r( ¯G
Π is not ”nice”)P r(Π is not ”nice”)
|
|
Π is ”nice”) + P r(Π is not ”nice”).
|

e

e

e

18

n
δ2 log(n), and k

, then both
CAB
≥
in n. Therefore, the the probability of ¯G is

log(n)
ǫ2

C2

According to Lemma C.3 and Lemma B.10, if m
events
polynomially small in n, i.e., the desired event G happens w.h.p. in n.
AT B

C1 ·
and P r(Π is ”nice”) happen w.h.p.

Π is ”nice”
G
|
{

Next we show that

M

≥

}

||
), then w.h.p. in n, we have

−

Θ( ˜r+log(n)
ǫ2
k = Θ( ˜r+log(n)

δ2

f
||A||2||B||2
||AT B||2
F

), then

M

||
By triangle inequality, we have

·

−
shown that w.h.p. in n, both terms are less than δ
inequality
−
statement of Lemma C.1.

AT B
||

AT B

RΩ(

|| ≤

AT

B)

||

f
||

δ

|| ≤
||

B

AT B

AT B
δ
||
M
−
AT B
−
f
RΩ(

||F holds w.h.p. in n. According to Lemma B.7, if k =
. Now let ǫ := δ ||AT B||F
||A||||B|| , we have that if
|| ≤
||
δ

A
ǫ
||
||||
AT B
||F holds w.h.p. in n.
||
AT B
. We have
+
M )
|| ≤ ||
−
||
||
AT B
||F . By rescaling δ as δ/2, we have that the desired
||
||F holds w.h.p. in n, when m and k are chosen according to the
f
f

AT B

RΩ(

|| ≤

M )

f

f

M

M

−

||

e

e

Because the bound of Lemma C.1 has the same form as that of Lemma C.2 in [3], the corollary
(AT B)r||F ≤

M ), which is stated here without proof:

AT B
||

−

if

of Lemma C.2 also holds for RΩ(
1
576κr1.5 ||

(AT B)r||F , then w.h.p. in n we have
U (0))i
/
Ai||
(
||
||
||

f
8√r

|| ≤

A

||F

and dist(

U (0), U ∗)

1/2,

≤

U (0) is the initial iterate produced by the WAltMin algorithm (see Step 6 of Algorithm 2). This

b

where
b
corollary will be used in the proof of Lemma C.2.

b

(AT B)r||F ≥
AT B
Similar to the original proof in [3], we can now consider two cases separately: (1)
||
1
(AT B)r||F . The ﬁrst case is simple: use
(AT B)r||F ≤
576ρr1.5 ||
Lemma C.1 and Wely’s inequality [31] already implies the desired bound in Theorem 3.1. To see why,
note that Lemma C.1 and Wely’s inequality imply that

(AT B)r||F ; (2)

1
576ρr1.5 ||

AT B
||

−

−

(AT B)r −
||
AT B

−

AT B

−
AT B
2
||

−

(RΩ(
M )r||
(AT B)r||
+
f
(AT B)r||
(AT B)r||

+ δ

ξ1

≤ ||
ξ2

≤ ||
ξ3

≤

RΩ(

M )

+

RΩ(

M )

||

AT B
||

−
||
AT B
||F +
RΩ(
f
||
||
||F ,

AT B
||

+ 2δ

f

M )

AT B
f

||

−

−

(RΩ(

M ))r||
AT B
f
||

−

+

(AT B)r||

(12)

where Mr denotes the best rank-r approximation of M , ξ1 follows triangle inequality, ξ2 follows from
Lemma C.1 and Wely’s inequality, and ξ3 follows from Lemma C.1. If
(AT B)r||F . Setting δ =
(AT B)r||F ≤
then
O(η/(ρr1.5)) in Eq.(12) gives the desired error bound in Theorem 3.1. Therefore, in the following analysis
we only need to consider the second case.

AT B
−
||
AT B
O(ρr1.5)
||

(AT B)r||F ≥
−

(AT B)r||F +
||

1
576ρr1.5 ||

AT B
||

AT B
||

||F =

−

(AT B)r||F ,

C.3 Proof of Lemma C.2

We ﬁrst prove the following lemma, which is a counterpart ofLemma C.5 in [3]. For simplicity of presenta-
tion, we use M to denote AT B in the following proof.

Lemma C.4. If m
C1 and C2, then the following holds with probability at least 1

C1nr log(n)T /(γδ2) and k

≥

≥

C2(˜r+log(n))/ǫ2 for sufﬁciently large global constants

(U (t))T RΩ(
||

M

−

Mr)

−

(U (t))T (M

Mr)

δ

M

|| ≤

||

−

−

A

||F ||

B

||F + ǫ

||

A

||

B

.
||

||||

γ/T :

−
Mr||F + δǫ

f

19

Proof. For a ﬁxed Π, we have that if m
least 1

γ/T :

≥

−

C1nr log(n)T /(γδ2), then following holds with probability at

(U (t))T RΩ(
||

M

−

Mr)

−

(U (t))T (

M

Mr)

δ

M

|| ≤

||

Mr||F .

−

−

(13)

The proof of Eq.(13) is exactly the same as the proof of Lemma C.5/B.6/B.2 in [3], so we omit its details
here. The key idea is to deﬁne a set of zero-mean random matrices Xij such that
Mr)
desired bound.

−
Mr), and then use second moment-based matrix Chebyshev inequality to obtain the

ij Xij = (U (t))T RΩ(

(U (t))T (

P

f

f

f

f

M

M

−

−

According to Lemma B.6 and Lemma B.7, if k = Θ((˜r + log(n))/ǫ2), then w.h.p. in n, the following

f

holds:

M

||

−

AT B

||F ≤

ǫ

A

||

||F ||

B

||F ,

M

||

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

(14)

Using triangle inequality, we have that if m and k satisfy the conditions of Lemma C.4, then the follow-

ing holds with probability at least 1

γ/T :

f

f

−

−

(U (t))T RΩ(
||
(U (t))T RΩ(

M

≤ ||
ξ1

δ

δ

δ

M

||

M
f

||

M

||

−

−

−

≤

≤
ξ2

≤

M
f
−
Mr||F +
f
||
Mr||F + δ
||
Mr||F + δǫ

Mr)

Mr)

−

−

(U (t))T (M
(U (t))T (

M

Mr)

||
Mr)

||

−

−

f

M

M

M

−

−

||
||F +
M
||
f
||F + ǫ
B
f
||F ||

M

A

||

M

||

−

A

B
f

,
||

||||

||

+

(U (t))T (M
||

−

M )

||

f

where ξ1 follows from Eq.(13), and ξ2 follows from Eq.(14).

Now we are ready to prove Lemma C.2. For simplicity, we focus on the rank-1 case here. Rank-
r proof follows a similar line of reasoning and can be obtained by combining the current proof with the
rank-r analysis in the original proof of LELA [3]. Note that compared to Lemma C.5 in [3], Lemma C.4
contains two extra terms δǫ
. Therefore, we need to be careful for steps that involve
Lemma C.4.

||F + ǫ

||F ||

||||

B

B

A

A

||

||

||

In the rank-1 case, we use ˆut and ˆvt+1 to denote the t-th and (t+1)-th step iterates (which are column

vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized vectors.

||F , for some constant c1.
Bj||
/
||
Bounding dist(vt+1, v∗):
In Lemma C.4, set ǫ = ||AT B||
AT B

η

Proof. This proof contains two parts. In the ﬁrst part, we will prove that the distance dist(vt+1, v∗) de-
creases geometrically over time. In the second part, we show that the j-th entry of vt+1 satisﬁes
c1||

vt+1
j
|

| ≤

B

2||A||||B||η and δ = η
2˜r , where 0 < η < 1, then we have δǫ
AT B
/2, and ǫ
B
||
γ/T , the following holds:

||F ≤
/2. Therefore, with probability at least

AT B
||

η2
2˜r ||

||F ||

|| ≤

|| ≤

||||

B

A

A

||

||

||

||

η

·

||A||F ||B||F
||A||||B||

1

−

M

M1)

(ut)T (M

M1)

η

M

(ut)T RΩ(
||
(ut)T RΩ(
||

Hence, we have

−

−

−
dist(ut, u∗)
||
Using the explicit formula for WAltMin update (see Eq.(46) and Eq.(47) in [3]), we can bound
ˆvt+1, v∗
h

M1||F /˜r + ησ∗
1.

||
M1||

as follows.

f
−

M1)

|| ≤

|| ≤

+ η

⊥i

f

M

M

−

−

−

||

and

M1||F /˜r + ησ∗
1.
M

(15)

ˆvt+1, v∗
h

i

ˆut
||

||h

ˆvt+1, v∗

/σ∗
i

1 ≥ h

ut, u∗

i −

1

δ1

1

− h

ut, u∗

2
i

−

1

δ1

1

−

M

(η ||

M1||F

−
˜rσ∗
1

+ η).

δ1

−

p

20

ˆut
||

ˆvt+1, v∗
⊥i

||h

/σ∗

1 ≤

1

1

− h

ut, u∗

2 +
i

1

δ1

−

δ1

p

1

−

δ1

(dist(ut, u∗) ||

M

M1||

+ η ||

M

−
σ∗
1

As discussed in the end of Appendix C.2, we only need to consider the case when
1
576ρr1.5

(AT B)r||F , where ρ = σ∗
||

AT B
−
||
r . In the rank-1 case, this condition reduces to
M
−
||
1
ut, u∗
20 ), and use the fact that
h
ˆvt+1, v∗
as
h

1
20 , η
ˆvt+1, v∗
h

For sufﬁciently small constants δ1 and η (e.g., δ1 ≤
and dist(u0, u∗)

1/2, we can further bound

1/σ∗

≤
and

⊥i

≤

i

M1||F

+ η).

−
˜rσ∗
1
(AT B)r||F ≤
σ∗
576 .
M1||F ≤
u0, u∗
i ≥ h

i

ˆut
||

||h

ˆvt+1, v∗

/σ∗
i

1 ≥ h

u0, u∗

i −

1

− h

u0, u∗

2
i

−

1
10 ≥

√3
2 −

2
10 ≥

1
2

.

ˆut
||

ˆvt+1, v∗
⊥i

||h

/σ∗

δ1

dist(ut, u∗) +

dist(ut, u∗) +

1 ≤
ξ1

≤

1
1
4

δ1

−
dist(ut, u∗) + 2(η

δ1)
−
M1||F /σ∗
1 and the assumption that δ1 is sufﬁciently small.

1 + η),

M

−

−

||

1

where ξ1 uses the fact that ˜r

Now we are ready to bound dist(vt+1, v∗) as

≥

M

(η ||

1

δ1

M1||F

−
˜rσ∗
1

+ η)

1
10

p

1
576(1

dist(vt+1, v∗) =

1

vt+1, v∗

− h

2 =
i

ˆvt+1, v∗
h
2 +

ˆvt+1, v∗
h
p
M
||

⊥i
Mr||F /σ∗

−

⊥i
ˆvt+1, v∗
h
1 + η),

p
1
2

ξ1

≤

dist(ut, u∗) + 4(η

ˆvt+1, v∗
⊥i
h
ˆvt+1, v∗
i
h

2 ≤
i

where ξ1 follows from substituting Eqs. (16) and (17). Rescaling η as η/4 gives the desired bound of
Lemma C.2 for the rank-1 case. Rank-r proof can be obtained by following a similar framework.

Bounding vt+1
In this step, we need to prove that the j-th entry of vt+1 satisﬁes

:

j

vt+1
j
|

| ≤

c1

||Bj||
||B||F

for all j, under the

assumption that ut satisﬁes the norm bound

||Ai||
||A||F
The proof follows very closely to the second part of proving Lemma C.3 in [3], except that an extra
Mij using Bernstein inequality. More
Mij. Note that if ˆqij = 1, then δij = 1, Xi = 0, so we only need to

multiplicative term (1 + ǫ) will show up when bounding
ˆqij)wijut
speciﬁcally, let Xi = (δij −
i
consider the case when ˆqij < 1, i.e., ˆqij = qij, where qij is deﬁned in Eq.(1).

i δijwijut
i

ut
i| ≤
|

for all i.

P

f

c1

Suppose Π is ﬁxed and its dimension satisﬁes k = Ω( log(n)

f

), then according to Lemma B.6, we have

ǫ2

that w.h.p. in n,

Hence, we have

Mij| ≤ |
|
f
M 2
ij
ˆqij
f

≤

ξ1

Mij|

+ ǫ

Ai|| · ||
||

Bj|| ≤

(1 + ǫ)

Ai|| · ||
||

,
Bj||

(i, j).

∀

m

2

(1 + ǫ)2
2
Bj||
Ai||
||
||
+ ||Bj||2
( ||Ai||2
2n||B||2
2n||A||2
·
F
F
c2
(ut
i)2
Ai||
1||
( ||Ai||2
ˆqij
2n||A||2
F

m

≤

ξ2

·

2n(1 + ǫ)2
m

) ≤

Bj||

2

A

2
F ,
||

||

· ||

2/
2
A
F
||
||
+ ||Bj||2
2n||B||2
F

) ≤

2nc2
1
m

,

(16)

(17)

(18)

(19)

(20)

(21)

where ξ1 follows from substituting Eqs.(19) and (1), and ξ2 follows from the assumption that
c1||

/
Ai||
||

||F .
A

ut
i| ≤
|

21

We can now bound the ﬁrst and second moments of Xi as

Xi| ≤ |
|

wijut
i

(ut
i)2
ˆqij s

M 2
ij
ˆqij
f

Mij| ≤ s
f

ξ1

≤

2nc1(1 + ǫ)
m

||F .
A
Bj||||
||

V ar(Xi) =

ˆqij(1

ˆqij)w2

ij(ut

i)2

M 2

ij ≤

(1 + ǫ)2

2

Ai||
||

Bj||
||

2

Xi

−

Xi
2nc2

1(1 + ǫ)2

m

ξ2

≤

2

Bj||
||

A

||

f
2
F ,
||

(ut
i)2
ˆqij

Xi

where ξ1 and ξ2 follows from substituting Eqs.(20) and (21).

The rest proof involves applying Bernstein’s inequality to derive a high-probability bound on

i Xi,
which is almost the same as the second part of proving Lemma C.3 in [3], so we omit the details here. The
only difference is that, because of the extra multiplicative term (1 + ǫ) in the bound of the ﬁrst and second
moments, the lower bound on the sample complexity m should also be multiplied by an extra (1 + ǫ)2 term.
By restricting 0 < ǫ < 1/2, this extra multiplicative term can be ignored as long as the original lower bound
of m contains a large enough constant.

P

C.4 Proof of Theorem 3.1

We now prove our main theorem for rank-1 case here. Rank-r proof follows a similar line of reasoning and
can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3].
vt+1 to denote the t-th and (t+1)-th step iterates (which are
Similar to the previous section, we use
column vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized
vectors.

ut and

b

b

The closed form solution for WAltMin update at t + 1 iteration is

ut
||

vt+1
j = σ∗
||

1v∗
j

i δijwijut
i δijwij(ut

iu∗
i
i)2 +

i δijwijut
i(
M
M1)ij
−
i δijwij(ut
i)2

.

P

f

P

Writing in matrix form, we get

b

b

ut
||

vt+1
j = σ∗
1h
||

u∗, ut

v∗
i

where B and C are diagonal matrices with Bjj =
vector RΩ(

M1)T ut with entries yj =

M

b

b

Each term of Eq.(22) can be bounded as follows.

−

f

C)v∗ + B−1y,

−

u∗, ut
1B−1(
σ∗
h
i δijwij(ut
i δijwijut
M
i(
P

B
i
−
i)2 and Cjj =
M1)ij.

−

P

i δijwijut

iu∗

i , and y is the

u∗, ut
(
h
||

B
i

P
C)v∗

|| ≤

f
dist(ut, u∗),

B−1
||

|| ≤

2,

y
||

||

=

RΩ(

M

||

−

M1)T ut

dist(ut, u∗)
||

M

M1||

−

+ η

M

||

M1||F /˜r + ησ∗
1,

−

where ξ1 follows directly from Lemma C.4. The proof of Eq.(23) is exactly the same as the proof of Lemma
B.3 and B.4 in [3].

f

According to Lemma C.2, since the distance is decreasing geometrically, after O(log( 1

ζ )) iterations we

get

(22)

(23)

(24)

(25)

dist(ut, u∗)

ζ + 2η

M

||

≤

M1||F /σ∗

−

1 + 2η.

P
P

−
ξ1

||

≤

22

Now we are ready to prove the spectral norm bound in Theorem 3.1:

M1 −
||
M1 −
(I
−

≤ ||

+

ut(
vt+1)T
||
ut(ut)T M1||
b
b
ut(ut)T )M1||
v∗
σ1h
||
i
u∗, ut
1B−1(
σ∗
h
||

ut(ut)T M1 −
vt+1)T
ut(
||
||
vt+1)T ]
ut
ut[(ut)T M1 − ||
(
||
||
||
b
b
vt+1)T
ut, u∗
(
b
b
||
C)v∗
b

− ||

ut

−

+

||

||

≤ ||
ξ1
1dist(ut, u∗) +
σ∗

1dist(ut, u∗) +
σ∗

1dist(ut, u∗) + 2σ∗
σ∗

5(ζσ∗
≤
= 5ζσ∗

1 + 2η
||
1 + 12η

M

M

||

−

−

+

B−1y
B
b
i
||
1dist(ut, u∗) + 2dist(ut, u∗)
||
M1||F + 2ησ∗
M1||F + 12ησ∗

||
M1||
−
M1||F + 2ησ∗

1) + 2η

M

M

−

||

1

1

≤
ξ2

≤
ξ3

≤
ξ4

+ 2η

M

||

M1||F /˜r + 2ησ∗

1

−

(26)

v∗,
where ξ1 follows from the deﬁnition of dist(ut, u∗), the fact that
= 1, and (ut)T M1 = σ1h
i
ξ2 follows from substituting Eq.(22), ξ3 follows from Eqs.(23) and (24), and ξ4 follows from the Eq.(25),
and fact that
1) (this will inﬂuence the number of iterations)
and also rescaling η to η/12 gives us the desired spectral norm error bound in Eq.(7). This completes our
proof of the rank-1 case. Rank-r proof follows a similar line of reasoning and can be obtained by combining
the current proof with the rank-r analysis in the original proof of LELA [3].

1. Rescaling ζ to ζ/(5σ∗

M1|| ≤

σ∗
1, ˜r

ut, u∗

ut
||

M

−

≥

||

||

C.5 Sampling

We describe a way to sample m elements in O(m log(n)) time using distribution qij deﬁned in Eq. (1).
Naively one can compute all the n2 entries of min
and toss a coin for each entry, which takes O(n2)
qij, 1
}
{
time. Instead of this binomial sampling we can switch to row wise multinomial sampling. For this, ﬁrst
estimate the expected number of samples per row mi = m( ||Ai||2
2n ). Now sample m1 entries from row
2||A||2
F
1 according to the multinomial distribution,

+ 1

m
m1 ·

( ||
2n

2

A1||
2
A
F
||
||

+ ||
2n

2

Bj||
2
B
F
||
||

) =

||A1||2
2n||A||2
F
||Ai||2
2||A||2
F

+ ||Bj||2
2n||B||2
F
+ 1
2n

.

q1j =

e

j

e

P

Note that
q1j = 1. To sample from this distribution, we can generate a random number in the interval
[0, 1], and then locate the corresponding column index by binary searching over the cumulative distribution
function (CDF) of
q1j. This takes O(n) time for setting up the distribution and O(m1 log(n)) time to
sample. For subsequent row i, we only need O(mi log(n)) time to sample mi entries. This is because for
binary search to work, only O(mi log(n)) entries of the CDF vector needs to be computed and checked.
Note that the speciﬁc form of
qij ensures that its CDF entries can be updated in an efﬁcient way (since we
only need to update the linear shift and scale). Hence, sampling m elements takes a total O(m log(n)) time.
Furthermore, the error in this model is bounded up to a factor of 2 of the error achieved by the Binomial
model [7] [21]. For more details please see our Spark implementation.

e

e

D Related work

Approximate matrix multiplication:

In the seminal work of [14], Drineas et al. give a randomized algorithm which samples few rows of A and
B and computes the approximate product. The distribution depends on the row norms of the matrices and

23

ǫ

||

||

A

A

B

B

−

−

||2||

||F ||

||2 ≤

AT B
||

AT B
||

For spectral norm bound of the form

||F . Later Sarlos [29] propose a sketching
the algorithm achieves an additive error proportional to
based algorithm, which computes sketched matrices and then outputs their product. The analysis for this
˜AT ˜B
algorithm is then improved by [10]. All of these results compare the error
||F in Frobenius
norm.
||2, the authors in [29, 11] show that
C
the sketch size needs to satisfy O(r/ǫ2), where r = rank(A) + rank(B). This dependence on rank is later
improved to stable rank in [26], but at the cost of a weaker dependence on ǫ. Recently, Cohen et al. [12]
further improve the dependence on ǫ and give a bound of O(˜r/ǫ2), where ˜r is the maximum stable rank. Note
that the sketching based algorithm does not output a low rank matrix. As shown in Figure 2, rescaling by
the actual column norms provide a better estimator than just using the sketched matrices. Furthermore, we
show that taking SVD on the sketched matrices gives higher error rate than our algorithm (see Figure 3(b)).
Low rank approximation: [16] introduced the problem of computing low rank approximation of a
given matrix using only few passes over the data. They gave an algorithm that samples few rows and
columns of the matrix and computes its SVD for low rank approximation. They show that this algorithm
achieves additive error guarantees in Frobenius norm. [15, 29, 19, 13] have later developed algorithms using
various sketching techniques like Gaussian projection, random Hadamard transform and volume sampling
that achieve relative error in Frobenius norm.[35, 28, 18, 6] improved the analysis of these algorithms and
provided error guarantees in spectral norm. More recently [11] presented an algorithm based on subspace
embedding that computes the sketches in the input sparsity time.

Another class of methods use entrywise sampling instead of sketching to compute low rank approxi-
mation. [1] considered an uniform entrywise sampling algorithm followed by SVD to compute low rank
approximation. This gives an additive approximation error. More recently [3] considered biased entrywise
sampling using leverage scores, followed by matrix completion to compute low rank approximation. While
this algorithm achieves relative error approximation, it takes two passes over the data.

There is also lot of interesting work on computing PCA over streaming data under some statistical
assumptions, e.g., [2, 27, 5, 30]. In contrast, our model does not put any assumptions on the input matrix.
Besides, our goal here is to get a low rank matrix and not just the subspace.

24

6
1
0
2
 
t
c
O
 
6
2

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
5
6
6
0
.
0
1
6
1
:
v
i
X
r
a

Single Pass PCA of Matrix Products

Shanshan Wu
The University of Texas at Austin
shanshan@utexas.edu

Srinadh Bhojanapalli
Toyota Technological Institute at Chicago
srinadh@ttic.edu

Sujay Sanghavi
The University of Texas at Austin
sanghavi@mail.utexas.edu

Alexandros G. Dimakis
The University of Texas at Austin
dimakis@austin.utexas.edu

October 27, 2016

Abstract

In this paper we present a new algorithm for computing a low rank approximation of the product
AT B by taking only a single pass of the two matrices A and B. The straightforward way to do this is to
(a) ﬁrst sketch A and B individually, and then (b) ﬁnd the top components using PCA on the sketch. Our
algorithm in contrast retains additional summary information about A, B (e.g. row and column norms
etc.) and uses this additional information to obtain an improved approximation from the sketches. Our
main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in
addition we also provide results from an Apache Spark implementation that shows better computational
and statistical performance on real-world and synthetic evaluation datasets.

1 Introduction

Given two large matrices A and B we study the problem of ﬁnding a low rank approximation of their
product AT B, using only one pass over the matrix elements. This problem has many applications in machine
learning and statistics. For example, if A = B, then this general problem reduces to Principal Component
Analysis (PCA). Another example is a low rank approximation of a co-occurrence matrix from large logs,
e.g., A may be a user-by-query matrix and B may be a user-by-ad matrix, so AT B computes the joint
counts for each query-ad pair. The matrices A and B can also be two large bag-of-word matrices. For this
case, each entry of AT B is the number of times a pair of words co-occurred together. As a fourth example,
AT B can be a cross-covariance matrix between two sets of variables, e.g., A and B may be genotype and
phenotype data collected on the same set of observations. A low rank approximation of the product matrix
is useful for Canonical Correlation Analysis (CCA) [8]. For all these examples, AT B captures pairwise
variable interactions and a low rank approximation is a way to efﬁciently represent the signiﬁcant pairwise
interactions in sub-quadratic space.

×

n (d

Let A and B be matrices of size d

n) assumed too large to ﬁt in main memory. To obtain a
rank-r approximation of AT B, a naive way is to compute AT B ﬁrst, and then perform truncated singular
value decomposition (SVD) of AT B. This algorithm needs O(n2d) time and O(n2) memory to compute
the product, followed by an SVD of the n
n matrix. An alternative option is to directly run power method
on AT B without explicitly computing the product. Such an algorithm will need to access the data matrices
A and B multiple times and the disk IO overhead for loading the matrices to memory multiple times will be
the major performance bottleneck.

≫

×

1

For this reason, a number of recent papers introduce randomized algorithms that require only a few
passes over the data, approximately linear memory, and also provide spectral norm guarantees. The key step
in these algorithms is to compute a smaller representation of data. This can be achieved by two different
methods: (1) dimensionality reduction, i.e., matrix sketching [29, 11, 26, 12]; (2) random sampling [14, 3].
The recent results of Cohen et al. [12] provide the strongest spectral norm guarantee of the former. They
show that a sketch size of O(˜r/ǫ2) sufﬁces for the sketched matrices
B to achieve a spectral error of ǫ,
where ˜r is the maximum stable rank of A and B. Note that
B is not the desired rank-r approximation
of AT B. On the other hand,
[3] is a recent sampling method with very good performance guarantees.
The authors consider entrywise sampling based on column norms, followed by a matrix completion step
to compute low rank approximations. There is also a lot of interesting work on streaming PCA, but none
can be directly applied to the general case when A is different from B (see Figure 4(c)). Please refer to
Appendix D for more discussions on related work.

AT

AT

e

e

e

e

Despite the signiﬁcant volume of prior work, there is no method that computes a rank-r approximation
of AT B when the entries of A and B are streaming in a single pass 1. Bhojanapalli et al. [3] consider a
two-pass algorithm which computes column norms in the ﬁrst pass and uses them to sample in a second
pass over the matrix elements. In this paper, we combine ideas from the sketching and sampling literature
to obtain the ﬁrst algorithm that requires only a single pass over the data.

Contributions:

•

•

•

•

We propose a one-pass algorithm SMP-PCA (which stands for Streaming Matrix Product PCA) that
computes a rank-r approximation of AT B in time O((nnz(A) + nnz(B)) ρ2r3 ˜r
). Here
nnz(
) is the number of non-zero entries, ρ is the condition number, ˜r is the maximum stable rank, and
·
η measures the spectral norm error. Existing two-pass algorithms such as [3] typically have longer
runtime than our algorithm (see Figure 3(a)). We also compare our algorithm with the simple idea that
ﬁrst sketches A and B separately and then performs SVD on the product of their sketches. We show
that our algorithm always achieves better accuracy and can perform arbitrarily better if the column
vectors of A and B come from a cone (see Figures 2, 4(b), 3(b)).

η2 + nr6ρ4 ˜r3

η4

The central idea of our algorithm is a novel rescaled JL embedding that combines information from
matrix sketches and vector norms. This allows us to get better estimates of dot products of high
dimensional vectors compared to previous sketching approaches. We explain the beneﬁt compared to
a naive JL embedding in Figure 2 and the related discussion; we believe it may be of more general
interest beyond low rank matrix approximations.

AT B
k

We prove that our algorithm recovers a low rank approximation of AT B up to an error that depends
on
, decaying with increasing sketch size and number of samples
k
(Theorem 3.1). The ﬁrst term is a consequence of low rank approximation and vanishes if AT B is
exactly rank-r. The second term results from matrix sketching and subsampling; the bounds have
similar dependencies as in [12].

(AT B)rk

AT B
k

and

−

We implement SMP-PCA in Apache Spark and perform several distributed experiments on synthetic
and real datasets. Our distributed implementation uses several design innovations described in Sec-
tion 4 and Appendix C.5 and it is the only Spark implementation that we are aware of that can handle
matrices that are large in both dimensions. Our experiments show that we improve by approximately
a factor of 2
in running time compared to the previous state of the art and scale gracefully as the
cluster size increases. The source code is available online [36].

×

1One straightforward idea is to sketch each matrix individually and perform SVD on the product of the sketches. We compare

against that scheme and show that we can perform arbitrarily better using our rescaled JL embedding.

2

•

In addition to better performance, our algorithm offers another advantage: It is possible to compute
low-rank approximations to AT B even when the entries of the two matrices arrive in some arbitrary
order (as would be the case in streaming logs). We can therefore discover signiﬁcant correlations even
when the original datasets cannot be stored, for example due to storage or privacy limitations.

2 Problem setting and algorithms

Rd×n2 that are stored in disk,
Consider the following problem: given two matrices A
ﬁnd a rank-r approximation of their product AT B. In particular, we are interested in the setting where
both A, B and AT B are too large to ﬁt into memory. This is common for modern large scale machine
learning applications. For this setting, we develop a single-pass algorithm SMP-PCA that computes the
rank-r approximation without explicitly forming the entire matrix AT B.

Rd×n1 and B

∈

∈

Notations. Throughout the paper, we use A(i, j) or Aij to denote (i, j) entry for any matrix A. Let
Ai and Aj be the i-th column vector and j-th row vector. We use
for
spectral (or operator) norm. The optimal rank-r approximation of matrix A is Ar, which can be found by
Rn1×n2 as the projection
SVD. Given a set Ω
⊂
of A on Ω, i.e., PΩ(A)(i, j) = A(i, j) if (i, j)

kF for Frobenius norm, and
A
k
Rn1×n2, we deﬁne PΩ(A)

[n2] and a matrix A

Ω and 0 otherwise.

A
k
k

[n1]

×

∈

∈

∈

2.1 SMP-PCA

Our algorithm SMP-PCA (Streaming Matrix Product PCA) takes four parameters as input: the desired rank
r, number of samples m, sketch size k, and the number of iterations T . Performance guarantee involving
these parameters is provided in Theorem 3.1. As illustrated in Figure 1, our algorithm has three main steps:
1) compute sketches and side information in one pass over A and B; 2) given partial information of A and
B, estimate important entries of AT B; 3) compute low rank approximation given estimates of a few entries
of AT B. Now we explain each step in detail.

B and the column norms

Figure 1: An overview of our algorithm. A single pass is performed over the data to produce the sketched
[n2]. We then compute the
,
matrices
A,
Bjk
Aik
k
k
×
Ω and
M ) through a biased sampling process, where PΩ(
sampled matrix PΩ(
M ) =
e
M as an estimator for AT B, and
zero otherwise. Here Ω represents the set of sampled entries. We deﬁne
f
M ) gives the
. Performing matrix completion on PΩ(
Aik · k
k
f

f
M (i, j) =
desired rank-r approximation.

compute its entry as

eBj
eAT
i
e
e
Bj k
Aik·k

M (i, j) if (i, j)

, for all (i, j)

Bjk ·

[n1]

f

∈

∈

e

k

Step 1: Compute sketches and side information in one pass over A and B. In this step we compute
(0, 1/k)

Rk×d is a random matrix with entries being i.i.d.

B := ΠB, where Π

A := ΠA and

sketches

f

N

f

e

e

∈

3

Algorithm 1 SMP-PCA: Streaming Matrix Product PCA

1: Input: A

∈
iterations: T

∈

Rd×n1, B

Rd×n2, desired rank: r, sketch size: k, number of samples: m, number of

2: Construct a random matrix Π
pass over A and B to obtain:

3: Sample each entry (i, j)

×
deﬁned in Eq.(1); maintain a set Ω

∈

∈
A = ΠA,

Rk×d, where Π(i, j)
B = ΠB, and

(0, 1/k),
,
Bjk
k

[k]
×
∼ N
,
[n1]
Aik
k
[n2] independently with probability ˆqij = min

[n2].
×
1, qij}
{
[n2] which stores all the sampled pairs (i, j).
Rn1×n2, where PΩ(

M (i, j) is given in Eq. (2). Calculate PΩ(

(i, j)
(i, j)

e
[n1]

[n1]
e

∈
∈

M )

∀
∀

×

⊂

[d]. Perform a single

, where qij is

Rn1×n2, where

Ω and zero otherwise.
f
M ), Ω, r, ˆq, T ), see Appendix A for more details.

f

∈

M ) =

f

∈

4: Deﬁne

M
M (i, j) if (i, j)
∈
f
5: Run WAltMin(PΩ(
6: Output:
U

f

Rn1×r and
f

∈

∈

V

b

b

Rn2×r.

random variables. It is known that Π satisﬁes an ”oblivious Johnson-Lindenstrauss (JL) guarantee” [29][34]
and it helps preserving the top row spaces of A and B [11]. Note that any sketching matrix Π that is an
oblivious subspace embedding can be considered here, e.g., sparse JL transform and randomized Hadamard
transform (see [12] for more discussion).

Besides
[n1]

A and

B, we also compute the L2 norms for all column vectors, i.e.,

, for all
[n2]. We use this additional information to design better estimates of AT B in the next step,
B to sample. Note that this is the only step that needs one

Bjk
k

Aik
k

and

AT

∈

(i, j)
×
and also to determine important entries of
e
e
pass over data.

Step 2: Estimate important entries of AT B by rescaled JL embedding. In this step we use partial
B. We ﬁrst determine
B to sample, and then propose a novel rescaled JL embedding for estimating those entries.

information obtained from the previous step to compute a few important entries of
what entries of

AT

AT

e

e

We sample entry (i, j) of AT B independently with probability ˆqij = min
e
1, qij}
{

, where
e

e

e

Bjk
+ k
2
B
2n1k
F
k
[n2] be the set of sampled entries (i, j). Since E(

Aik
( k
2
A
2n2k
F
k

qij = m

·

2

2

).

⊂

[n1]

Let Ω
i,j qij) = m, the expected number
of sampled entries is roughly m. The special form of qij ensures that we can draw m samples in O(n1 +
m log(n2)) time; we show how to do this in Appendix C.5.

P

×

Note that qij intuitively captures important entries of AT B by giving higher weight to heavy rows and
columns. We show in Section 3 that this sampling actually generates good approximation to the matrix
AT B.

The biased sampling distribution of Eq.

(1) is ﬁrst proposed by Bhojanapalli et al. [3]. However,
their algorithm [3] needs a second pass to compute the sampled entries, while we propose a novel way of
estimating dot products, using information obtained in the ﬁrst step.

Deﬁne

M

Rn1×n2 as

∈

f

M (i, j) =

Bjk ·

Aik · k
k

AT
Bj
i
Bjk
Aik · k
k
e
e
M , instead, we only calculate
e
e

.

Note that we will not compute and store
is denoted as PΩ(

M ), where PΩ(

f

We now explain the intuition of Eq. (2), and why

(i, j) entry of AT B, a straightforward way is to use
between vectors

f
Ai and

f

f

M (i, j) if (i, j)

M )(i, j) =
f

Ω and 0 otherwise.
f
B. To estimate the
M is a better estimator than
AT
θij is the angle
Bjk ·
Aik · k
Bj =
i
k
f
Bj. Since we already know the actual column norms, a potentially better estimator
e
e
e

AT
θij, where
e

cos

∈

e

e

e

e

M (i, j) for (i, j)

Ω. This matrix

∈

(1)

(2)

e

e

4

JL embedding
Rescaled JL embedding

2

1

0

-1

t
c
u
d
o
r
p

 
t

o
d

 

d
e

t

a
m

i
t
s
E

-2

-1

-0.5

0
True dot product

0.5

1

(a)

(b)

Figure 2: (a) Rescaled JL embedding (red dots) captures the dot products with smaller variance compared
to JL embedding (blue triangles). Mean squared error: 0.053 versus 0.129. (b) Lower ﬁgure illustrates how
to construct unit-norm vectors from a cone with angle θ. Let x be a ﬁxed unit-norm vector, and let t be a
(x + t) with probability
random Gaussian vector with expected norm tan(θ/2), we set y as either x + t or
AT B
half, and then normalize it. Upper ﬁgure plots the ratio of spectral norm errors
,
/
k
k
−
k
M has better
when the column vectors of A and B are unit vectors drawn from a cone with angle θ. Clearly,
B for all possible values of θ, especially when θ is small.
accuracy than

−
AT B
k

AT

AT

f

M

−

B

e

e

f

would be

e
Aik · k
k

e
Bjk ·

cos

θij. This removes the uncertainty that comes from distorted column norms2.

AT
i

e

Bj (JL embedding) and

Figure 2(a) compares the two estimators

M (i, j) (rescaled JL embedding)
for dot products. We plot simulation results on pairs of unit-norm vectors with different angles. The vectors
have dimension 1,000 and the sketching matrix has dimension 10-by-1,000. Clearly rescaling by the actual
norms help reduce the estimation uncertainty. This phenomenon is more prominent when the true dot
products are close to
1,
and hence the uncertainty from angles may produce smaller distortion compared to that from norms. In the
extreme case when cos θ =

1, which makes sense because cos θ has a small slope when cos θ approaches

1, rescaled JL embedding can perfectly recover the true dot product.

f

±

±

e

e

In the lower part of Figure 2(b) we illustrate how to construct unit-norm vectors from a cone with angle
θ. Given a ﬁxed unit-norm vector x, and a random Gaussian vector t with expected norm tan(θ/2), we
construct new vector y by randomly picking one from the two possible choices x + t and
(x + t), and then
renormalize it. Suppose the columns of A and B are unit vectors randomly drawn from a cone with angle
AT
θ, we plot the ratio of spectral norm errors
in Figure 2(b). We observe that
B and can be much better when θ approaches zero, which agrees with the trend
M always outperforms
indicated in Figure 2(a).
f
compute the low rank approximation of AT B from the samples using alternating least squares:

e
Step 3: Compute low rank approximation given estimates of few entries of AT B. Finally we

AT B
/
k
k

AT B
k

AT

f

M

−

−

−

B

e

e

e

k

±

min
U,V ∈Rn×r

wij(eT

i U V T ej −

M (i, j))2,

X(i,j)∈Ω

f

(3)

2We also tried using the cosine rule for computing the dot product, and another sketching method speciﬁcally designed for

preserving angles [4], but empirically those methods perform worse than our current estimator.

5

where wij = 1/ˆqij denotes the weights, and ei, ej are standard base vectors. This is a popular technique for
low rank recovery and matrix completion (see [3] and the references therein). After T iterations, we will get
a rank-r approximation of
M presented in the convenient factored form. This subroutine is quite standard,
so we defer the details to Appendix A.

f

3 Analysis

Now we present the main theoretical result. Theorem 3.1 characterizes the interaction between the sketch
[
AT Brk
size k, the sampling complexity m, the number of iterations T , and the spectral error
,
[
AT Br is the output of SMP-PCA, and (AT B)r is the optimal rank-r approximation of AT B. Note
where
= n2,
that the following theorem assumes that A and B have the same size. For the general case of n1 6
.
Theorem 3.1 is still valid by setting n = max
n1, n2}
{

(AT B)r −
k

Theorem 3.1. Given matrices A
kAk2
of AT B. Deﬁne ˜r = max
F
of (AT B)r, where σ∗
[
AT Br be the output of Algorithm SMP-PCA. If the input parameters k, m, and T satisfy

as the maximum stable rank, and ρ = σ∗
1
σ∗
r

}
i is the i-th singular values of AT B.

Rd×n, let (AT B)r be the optimal rank-r approximation
as the condition number

∈
kAk2 , kBk2
F
kBk2

Rd×n and B

Let

∈

{

(4)

(5)

(6)

(7)

2ρ2r3
k
2
F
k

·

k

≥

m

2
B
A
C1k
k
k
AT B
k
C2˜r2
γ

≥

·

max

˜r, 2 log(n)
}
{
η2

+ log (3/γ)

,

2

nr3ρ2 log(n)T 2
η2

,

·

2
2
B
F +
A
F
k
k
k
k
AT B
kF (cid:19)
k
kF +
A
ζ

log( k

≥

(cid:18)

T

B
k

kF

),

where C1 and C2 are some global constants independent of A and B. Then with probability at least 1
we have

−

γ,

(AT B)r −
k

[
AT Brk ≤

η

AT B
k

(AT B)rkF + ζ + ησ∗
r .

−

Remark 1. Compared to the two-pass algorithm proposed by [3], we notice that Eq. (7) contains an
additional error term ησ∗
r . This extra term captures the cost incurred when we are approximating entries of
AT B by Eq. (2) instead of using the actual values. The exact tradeoff between η and k is given by Eq. (4).
On one hand, we want to have a small k so that the sketched matrices can ﬁt into memory. On the other
hand, the parameter k controls how much information is lost during sketching, and a larger k gives a more
accurate estimation of the inner products.

kF is much smaller than

Remark 2. The dependence on kAk2
kF or
A
k

captures one difﬁcult situation for our algorithm.
If
AT B
kF , which could happen, e.g., when many column vectors
k
of A are orthogonal to those of B, then SMP-PCA requires many samples to work. This is reasonable.
Imagine that AT B is close to an identity matrix, then it may be hard to tell it from an all-zero matrix without
enough samples. Nevertheless, removing this dependence is an interesting direction for future research.

kAT BkF
B
k

F +kBk2
F

Remark 3. For the special case of A = B, SMP-PCA computes a rank-r approximation of ma-
trix AT A in a single pass. Theorem 3.1 provides an error bound in spectral norm for the residual matrix
[
(AT A)r −
AT Ar. Most results in the online PCA literature use Frobenius norm as performance measure.
Recently, [22] provides an online PCA algorithm with spectral norm guarantee. They achieves a spectral
norm bound of ǫσ∗
r+1, which is stronger than ours. However, their algorithm requires a target dimension

1 +σ∗

6

of O(r log n/ǫ2), i.e., the output is a matrix of size n-by-O(r log n/ǫ2), while the output of SMP-PCA is
simply n-by-r.

Remark 4. We defer our proofs to Appendix C. The proof proceeds in three steps. In Appendix C.2, we
show that the sampled matrix provides a good approximation of the actual matrix AT B. In Appendix C.3,
V and the
we show that there is a geometric decrease in the distance between the computed subspaces
optimal ones U ∗, V ∗ at each iteration of WAltMin algorithm. The spectral norm bound in Theorem 3.1 is
then proved using results from the previous two steps.

U ,

b

b

Computation Complexity. We now analyze the computation complexity of SMP-PCA. In Step 1, we
compute the sketched matrices of A and B, which requires O(nnz(A)k + nnz(B)k) ﬂops. Here nnz(
)
·
denotes the number of non-zero entries. The main job of Step 2 is to sample a set Ω and calculate the
corresponding inner products, which takes O(m log(n) + mk) ﬂops. Here we deﬁne n as max
for
simplicity. According to Eq. (4), we have log(n) = O(k), so Step 2 takes O(mk) ﬂops. In Step 3, we run
alternating least squares on the sampled matrix, which can be completed in O((mr2 + nr3)T ) ﬂops. Since
Eq. (5) indicates nr = O(m), the computation complexity of Step 3 is O(mr2T ). Therefore, SMP-PCA
has a total computation complexity O(nnz(A)k + nnz(B)k + mk + mr2T ).

n1, n2}
{

4 Numerical Experiments

Spark implementation. We implement our SMP-PCA in Apache Spark 1.6.2 [37]. For the purpose of
comparison, we also implement a two-pass algorithm LELA [3] in Spark3. The matrices A and B are
stored as a resilient distributed dataset (RDD) in disk (by setting its StorageLevel as DISK_ONLY). We
implement the two passes of LELA using the treeAggregate method. For SMP-PCA, we choose the
subsampled randomized Hadamard transform (SRHT) [32] as the sketching matrix 4. The biased sampling
procedure is performed using binary search (see Appendix C.5 for how to sample m elements in O(m log n)
time). After obtaining the sampled matrix, we run ALS (alternating least squares) to get the desired low-rank
matrices. More details can be found in [36].

Description of datasets. We test our algorithm on synthetic datasets and three real datasets: SIFT10K [20],

NIPS-BW [23], and URL-reputation [24]. For synthetic data, we generate matrices A and B as GD, where
G has entries independently drawn from standard Gaussian distribution, and D is a diagonal matrix with
Dii = 1/i. SIFT10K is a dataset of 10,000 images. Each image is represented by 128 features. We set A as
the image-by-feature matrix. The task here is to compute a low rank approximation of AT A, which is a stan-
dard PCA task. The NIPS-BW dataset contains bag-of-words features extracted from 1,500 NIPS papers.
We divide the papers into two subsets, and let A and B be the corresponding word-by-paper matrices, so
AT B computes the counts of co-occurred words between two sets of papers. The original URL-reputation
dataset has 2.4 million URLs. Each URL is represented by 3.2 million features, and is indicated as ma-
licious or benign. This dataset has been used previously for CCA [25]. Here we extract two subsets of
features, and let A and B be the corresponding URL-by-feature matrices. The goal is to compute a low rank
approximation of AT B, the cross-covariance matrix between two subsets of features.

Sample complexity. In Figure 4(a) we present simulation results on a small synthetic data with n = d =
5, 000 and r = 5. We observe that a phase transition occurs when the sample complexity m = Θ(nr log n).
This agrees with the experimental results shown in previous papers, see, e.g., [9, 3]. For the rest experiments
presented in this section, unless otherwise speciﬁed, we set r = 5, T = 10, and sampling complexity m as
4nr log n.

3To our best knowledge, this the ﬁrst distributed implementation of LELA.
4Compared to Gaussian sketch, SRHT reduces the runtime from O(ndk) to O(nd log d) and space cost from O(dk) to O(d),

while maintains the same quality of the output.

7

Runtime (sec) vs Cluster size

LELA
SMC-PCA

eAT eB)
SVD(
SMP-PCA
LELA
Optimal

SVD( eAT eB)
SMP-PCA
LELA
Optimal

3000

2000

1000

0.2

0.15

0.1

0.05

r
o
r
r
e
 
m
r
o
n
 
l
a
r
t
c
e
p
S

0

2

10

5

(a)

1000
Sketch size (k)

2000

1000
Sketch size (k)

2000

Figure 3: (a) Spark-1.6.2 running time on a 150GB dataset. All nodes are m.2xlarge EC2 instances. See [36]
for more details. (b) Spectral norm error achieved by three algorithms over two datasets: SIFT10K (left)
and NIPS-BW (right). We observe that SMP-PCA outperforms SVD(
B) by a factor of 1.8 for SIFT10K
and 1.1 for NIPS-BW. Besides, the error of SMP-PCA keeps decreasing as the sketch size k grows.

AT

0.3

0.25

0.2

0.15

0.1

0.05

(b)

e

e

Table 1: A comparison of spectral norm error over three datasets

Dataset

d

n

Algorithm Sketch size k

Error

Synthetic

100,000

100,000

URL-
malicious

URL-
benign

792,145

10,000

1,603,985

10,000

Optimal
LELA
SMP-PCA

Optimal
LELA
SMP-PCA

Optimal
LELA
SMP-PCA

-
-
2,000

-
-
2,000

-
-
2,000

0.0271
0.0274
0.0280

0.0163
0.0182
0.0188

0.0103
0.0105
0.0117

Comparison of SMP-PCA and LELA. We now compare the statistical performance of SMP-PCA
and LELA [3] on three real datasets and one synthetic dataset. As shown in Figure 3(b) and Table 1, LELA
always achieves a smaller spectral norm error than SMP-PCA, which makes sense because LELA takes
two passes and hence has more chances exploring the data. Besides, we observe that as the sketch size
increases, the error of SMP-PCA keeps decreasing and gets closer to that of LELA.

In Figure 3(a) we compare the runtime of SMP-PCA and LELA using a 150GB synthetic dataset on
m3.2xlarge Amazon EC2 instances5. The matrices A and B have dimension n = d = 100, 000. The sketch
dimension is set as k = 2, 000. We observe that the speedup achieved by SMP-PCA is more prominent for
small clusters (e.g., 56 mins versus 34 mins on a cluster of size two). This is possibly due to the increasing
spark overheads at larger clusters, see [17] for more related discussion.

Comparison of SMP-PCA and SVD(

AT

generating column vectors of A and B from a cone with angle θ. Here SVD(

B). In Figure 4(b) we repeat the experiment in Section 2 by
B) refers to computing

AT

5Each machine has 8 cores, 30GB memory, and 2×80GB SSD.

e

e

e

e

8

0.5

0.4

0.3

0.2

r
o
r
r
e
 
m
r
o
n
 
l
a
r
t
c
e
p
S

Ratio of errors vs theta

105

k = 400

k = 800

r Br

AT
SMP-PCA

 

r
o
r
r
e
m
r
o
n

 
l

a
r
t
c
e
p
S

1

0.8

0.6

0.4

0.2

2

1
# Samples / nrlogn

3

4

100

0

π/4

π/2 3π/4

π

200 400 600 800 1000
Sketch size (k)

(a)

(b)

(c)

Figure 4: (a) A phase transition occurs when the sample complexity m = Θ(nr log n). (b) This ﬁgure plots
the ratio of spectral norm error of SVD(
B) over that of SMP-PCA. The columns of A and B are unit
vectors drawn from a cone with angle θ. We see that the ratio of errors scales to inﬁnity as the cone angle
shrinks. (c) If the top r left singular vectors of A are orthogonal to those of B, the product AT
r Br is a very
poor low rank approximation of AT B.

AT

e

e

SVD on the sketched matrices6. We plot the ratio of the spectral norm error of SVD(
B) over that of
SMP-PCA, as a function of θ. Note that this is different from Figure 2(b), as now we take the effect of
random sampling and SVD into account. However, the trend in both ﬁgures are the same: SMP-PCA
B) and can be arbitrarily better as θ goes to zero.
always outperforms SVD(
AT
AT B
||
e
e

The y-axis represents spectral norm error, deﬁned as
approximation found by a speciﬁc algorithm. We observe that SMP-PCA outperforms SVD(
factor of 1.8 for SIFT10K and 1.1 for NIPS-BW.

B) on two real datasets SIFK10K and NIPS-BW.
[
AT Br is the rank-r
, where
AT
B) by a

AT
In Figure 3(b) we compare SMP-PCA and SVD(

[
AT B
AT Br||
/
||

AT

−

e

e

e

e

||

M is a better estimator for AT B than

Now we explain why SMP-PCA produces a more accurate result than SVD(

AT
B). The reasons are
e
AT
twofold. First, our rescaled JL embedding
B (Figure 2). Second,
e
the noise due to sampling is relatively small compared to the beneﬁt obtained from
M , and hence the ﬁnal
M ) still outperforms SVD(
result computed using PΩ(
e
e
f

r Br. Let Ar and Br be the optimal rank-r approximation of A and
B, we show that even if one could use existing methods (e.g., algorithms for streaming PCA) to estimate Ar
r Br can be a very poor low rank approximation of AT B. This is demonstrated in
and Br, their product AT
Figure 4(c), where we intentionally make the top r left singular vectors of A orthogonal to those of B.

Comparison of SMP-PCA and AT

B).

AT

f

f

e

e

e

e

5 Conclusion

We develop a novel one-pass algorithm SMP-PCA that directly computes a low rank approximation of a
matrix product, using ideas of matrix sketching and entrywise sampling. As a subroutine of our algorithm,
we propose rescaled JL for estimating entries of AT B, which has smaller error compared to the standard
estimator ˜AT ˜B. This we believe can be extended to other applications. Moreover, SMP-PCA allows the
non-zero entries of A and B to be presented in any arbitrary order, and hence can be used for steaming
applications. We design a distributed implementation for SMP-PCA. Our experimental results show that

6This can be done by standard power iteration based method, without explicitly forming the product matrix eAT eB, whose size

is too big to ﬁt into memory according to our assumption.

9

SMP-PCA can perform arbitrarily better than SVD(
rithms that require two or more passes over the data.

AT

B), and is signiﬁcantly faster compared to algo-

Acknowledgements We thank the anonymous reviewers for their valuable comments. This research has
e
been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000, and ARO
YIP W911NF-14-1-0258.

e

10

References

[1] D. Achlioptas and F. McSherry. Fast computation of low rank matrix approximations. In Proceedings
of the thirty-third annual ACM symposium on Theory of computing, pages 611–618. ACM, 2001.

[2] A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental pca. In Advances

in Neural Information Processing Systems, pages 3174–3182, 2013.

[3] S. Bhojanapalli, P. Jain, and S. Sanghavi. Tighter low-rank approximation via sampling the leveraged
element. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 902–920. SIAM, 2015.

[4] P. T. Boufounos. Angle-preserving quantized phase embeddings.
Applications. International Society for Optics and Photonics, 2013.

In SPIE Optical Engineering+

[5] C. Boutsidis, D. Garber, Z. Karnin, and E. Liberty. Online principal components analysis. In Pro-
ceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887–901.
SIAM, 2015.

[6] C. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized hadamard

transform. SIAM Journal on Matrix Analysis and Applications, 34(3):1301–1340, 2013.

[7] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Compu-

tational mathematics, 9(6):717–772, 2009.

[8] X. Chen, H. Liu, and J. G. Carbonell. Structured sparse canonical correlation analysis. In International

Conference on Artiﬁcial Intelligence and Statistics, pages 199–207, 2012.

[9] Y. Chen, S. Bhojanapalli, S. Sanghavi, and R. Ward. Completing any low-rank matrix, provably. arXiv

preprint arXiv:1306.2979, 2013.

[10] K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In Proceedings
of the forty-ﬁrst annual ACM symposium on Theory of computing, pages 205–214. ACM, 2009.

[11] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pages 81–90.
ACM, 2013.

[12] M. B. Cohen, J. Nelson, and D. P. Woodruff. Optimal approximate matrix product in terms of stable

rank. arXiv preprint arXiv:1507.02268, 2015.

[13] A. Deshpande and S. Vempala. Adaptive sampling and fast low-rank matrix approximation. In Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 292–
303. Springer, 2006.

[14] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices ii: Computing a

low-rank approximation to a matrix. SIAM Journal on Computing, 36(1):158–183, 2006.

[15] P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-error matrix
approximation: Column-based methods. In Approximation, Randomization, and Combinatorial Opti-
mization. Algorithms and Techniques, pages 316–326. Springer, 2006.

[16] A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo algorithms for ﬁnding low-rank approxima-

tions. Journal of the ACM (JACM), 51(6):1025–1041, 2004.

11

[17] A. Gittens, A. Devarakonda, E. Racah, M. F. Ringenburg, L. Gerhardt, J. Kottalam, J. Liu, K. J.
Maschhoff, S. Canon, J. Chhugani, P. Sharma, J. Yang, J. Demmel, J. Harrell, V. Krishnamurthy,
M. W. Mahoney, and Prabhat. Matrix factorization at scale: a comparison of scientiﬁc data analytics
in spark and C+MPI using three case studies. arXiv preprint arXiv:1607.01335, 2016.

[18] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algo-
rithms for constructing approximate matrix decompositions. SIAM review, 53(2):217–288, 2011.

[19] S. Har-Peled. Low rank matrix approximation in linear time. Manuscript. http://valis. cs. uiuc.

edu/sariel/papers/05/lrank/lrank. pdf, 2006.

[20] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. Pattern Analysis

and Machine Intelligence, IEEE Transactions on, 33(1):117–128, 2011.

[21] R. Kannan, S. S. Vempala, and D. P. Woodruff. Principal component analysis and higher correlations
for distributed data. In Proceedings of The 27th Conference on Learning Theory, pages 1040–1057,
2014.

[22] Z. Karnin and E. Liberty. Online pca with spectral bounds. In Proceedings of The 28th Conference on

Learning Theory (COLT), volume 40, pages 1129–1140, 2015.

[23] M. Lichman. UCI machine learning repository. http://archive.ics.uci.edu/ml, 2013.

[24] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Identifying suspicious urls: an application of large-
scale online learning. In Proceedings of the 26th annual international conference on machine learning,
pages 681–688. ACM, 2009.

[25] Z. Ma, Y. Lu, and D. Foster. Finding linear structure in large datasets with scalable canonical correla-

tion analysis. arXiv preprint arXiv:1506.08170, 2015.

[26] A. Magen and A. Zouzias. Low rank matrix-valued chernoff bounds and approximate matrix multipli-
cation. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,
pages 1422–1436. SIAM, 2011.

[27] I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited, streaming pca.

In Advances in Neural

Information Processing Systems, pages 2886–2894, 2013.

[28] N. H. Nguyen, T. T. Do, and T. D. Tran. A fast and efﬁcient algorithm for low-rank approximation of
a matrix. In Proceedings of the 41st annual ACM symposium on Theory of computing, pages 215–224.
ACM, 2009.

[29] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In Founda-
tions of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pages 143–152. IEEE,
2006.

[30] O. Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In Proceedings
of the 32nd International Conference on Machine Learning (ICML-15), pages 144–152, 2015.

[31] T. Tao. 254a, notes 3a: Eigenvalues and sums of hermitian matrices. Terence Tao’s blog, 2010.

[32] J. A. Tropp.

Improved analysis of the subsampled randomized hadamard transform. Advances in

Adaptive Data Analysis, pages 115–126, 2011.

12

[33] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational

Mathematics, 12(4):389–434, 2012.

[34] D. P. Woodruff. Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357,

2014.

[35] F. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. A fast randomized algorithm for the approximation

of matrices. Applied and Computational Harmonic Analysis, 25(3):335–366, 2008.

[36] S. Wu, S. Bhojanapalli, S. Sanghavi, and A. Dimakis. Github repository for ”single-pass pca of matrix

products”. https://github.com/wushanshan/MatrixProductPCA, 2016.

[37] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and
I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.
In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,
2012.

13

A Weighted alternating minimization

Algorithm 2 provides a detailed explanation of WAltMin, which follows a standard procedure for matrix
PΩ(A) to denote the Hadamard product between w and PΩ(A):
completion. We use RΩ(A) = w.
Rn1×n2 = 1/ˆqij is
RΩ(A)(i, j) = w(i, j)
the weight matrix. Similarly we deﬁne the matrix R1/2
PΩ(A)(i, j) for
(i, j)

Ω and 0 otherwise, where w
Ω (A) as R1/2

∗
PΩ(A)(i, j) for (i, j)

Ω and 0 otherwise.

Ω (A)(i, j) =

∈
w(i, j)

∈

∗

∗

The algorithm contains two parts: initialization (Step 2-6) and weighted alternating minimization (Step
M ) and then set row i of U (0)
7-10). In the ﬁrst part, we compute SVD of the weighted sampled matrix RΩ(
to be zero if its norm is larger than a threshold (Step 6). More details of this trim step can be found in [3].
In the second part, the goal is to solve the following non-convex problem by alternating minimization:

f

p

∈

min
U,V

wij(eT

i U V T ej −

M (i, j))2,

X(i,j)∈Ω

f

(8)

where ei, ej are standard base vectors. After running T iterations, the algorithm outputs a rank-r approxi-
mation of

M presented in the convenient factored form.

Ω0, . . . , Ω2T }
{

Algorithm 2 WAltMin [3]

f

∗

∀

∈

i, j

M )

M )

PΩ0(

Rn1×n2, Ω, r, ˆq, and T

f
M ) = w.

1: Input: PΩ(
2: wij = 1/ˆqij when ˆqij > 0, 0 else,
3: Divide Ω in 2T + 1 equal uniformly random subsets, i.e., Ω =
4: RΩ0(
5: U (0)Σ(0)(V (0))T = SVD(RΩ0(
6: Trim U (0) and let
f
7: for t = 0 to T
8:

U (0) be the output
1 do
V (t+1) = argminV k
b
U (t+1) = argminU k
9:
b
10: end for
U (T )
11: Output:
b

2
U (t)V T )
F
k
2
V (t+1))T )
F
k

U (
b
Rn2×r.
b

M
f
V (T )
f

Ω2t+1(
Ω2t+2(

R1/2
R1/2

Rn1×r and

M ), r)

f
M

f

−

−

−

∈

b

∈

b

B Technical Lemmas

We will frequently use the following concentration inequality in the proof.

Lemma B.1. (Matrix Bernstein’s Inequality [33]). Consider p independent random matrices X1, ...., Xp in
Rn×n, where each matrix has bounded deviation from its mean:

Xi −
||
Let the norm of the covariance matrix be

E[Xi]

L,

|| ≤

i.

∀

p

(Xi −

σ2 = max

E

(cid:12)
((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Then the following holds for all t
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi=1

"

E[Xi])T

(Xi −

E[Xi])T (Xi −

E[Xi])

p

"
Xi=1

,

E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2n exp( −

t2/2
σ2 + Lt/3

).

E[Xi])(Xi −
0:

≥
p

(Xi −

P

(cid:12)
"(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[Xi])
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

# ≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
14

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

A formal deﬁnition of JL transform is given below [29][34].

Deﬁnition B.2. A random matrix Π
for short, if with probability at least 1

Πv, Πv′

v, v′

i − h

i| ≤

ǫ

v
||

|| · ||

v′

.
||

|h

∈
−

Rk×d forms a JL transform with parameters ǫ, δ, f or JLT(ǫ, δ, f )
V it holds that
δ, for any f -element subset V

Rd, for all v, v′

⊂

∈

The following lemma [34] characterizes the tradeoff between the reduced dimension k and the error

level ǫ.

N

Lemma B.3. Let 0 < ǫ, δ < 1, and Π

Rk×d be a random matrix where the entries Π(i, j) are i.i.d.

(0, 1/k) random variables. If k = Ω(log(f /δ)ǫ−2), then Π is a JLT(ǫ, δ, f ).

∈

We now present two lemmas that connect

A

Rk×n and

B

Rd×n with A

Rd×n and B

Rd×n.

∈
∈
∈
Lemma B.4. Let 0 < ǫ, δ < 1, if k = Ω( log(2n/δ)
), then with probability at least 1
e

ǫ2

e

δ,

−

∈

(1

ǫ)

A

||

2
F ≤ ||
||

A

2
F ≤
||

−

(1 + ǫ)

A

2
F ,
||
||
AT B

(1

ǫ)

B

||

−
ǫ

A

2
F ≤ ||
||
||F .
B

AT
||

B

||F ≤
Proof. This is again a standard result of JL transformation, e.g., see Deﬁnition 2.3 and Theorem 2.1 of [34]
and Lemma 6 of [29] .

||F ||

−

e

e

e

e

||

B

2
F ≤
||

(1 + ǫ)

B

2
F ,
||

||

Lemma B.5. Let 0 < ǫ, δ < 1, if k = Θ( ˜r+log(1/δ)
rank, then with probability at least 1

δ,

ǫ2

−

), where ˜r = max

is the maximum stable

||A||2
F

||A||2 , ||B||2

F
||B||2

{

}

Proof. This follows from a recent paper [12].
e

e

AT
||

B

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

Using the above two lemmas, we can prove the following two lemmas that relate

AT
M is DA
deﬁned in Algorithm 1. A more compact deﬁnition of
.
matrices with (DA)ii =
Bj ||
/
Bj||
Ai||
/
Ai||
||
||
||
||
f
e
Lemma B.6. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( log(2n/δ)
e
e

and (DB)jj =

ǫ2

e

M with AT B, for

M
BDB, where DA and DB are diagonal
f
f

), then with probability at least 1

δ,

−

AT
i Bj| ≤

ǫ

Ai|| · ||
||

,
Bj||

M

||

−

AT B

||F ≤

ǫ

A

||

||F ||

B

||F .

Mij −
|
f

Proof. Let 0 < ǫ < 1/2, 0 < δ < 1, according to the Deﬁnition B.2 and Lemma B.3, we have that if
k = Ω( log(2n/δ)

), then with probability at least 1

δ, and for all i, j

f

ǫ2

1 + ǫ,

1

ǫ

−

≤

AT
i Bj|

as

−
(DB)jj ≤

1 + ǫ,

AT
i
|

e

Bj −
e

AT
i Bj| ≤

ǫ

Ai||||
||

.
Bj||

(9)

We can now bound

Mij −
|
AT
i
f
|

ǫ

1

≤

−

(DA)ii ≤
Mij −
|
AT
i Bj|
f
Bj(DA)ii(DB)jj −
Bj(1 + ǫ)2
e
{|
(1 + ǫ)2ǫ
e
e
{
Ai||||
||

,
Bj ||

AT
i

−
Bj||
Ai||||
||

max
e

max

7ǫ

ξ1=
ξ2

≤
ξ3

≤
ξ4

≤

AT
i Bj|
AT
AT
,
i Bj|
i
|
+ ((1 + ǫ)2
e

e

Bj(1

ǫ)2

−

−
AT
i Bj|
1)
|

AT
i Bj|}
, (1

−

−

15

ǫ)2ǫ

Ai||||
||

Bj||

+ (1

(1

−

−

AT
ǫ)2)
i Bj|}
|
(10)

Mij, ξ2 follows from the bound in Eq.(9), ξ3 follows from triangle
. Now rescaling ǫ as ǫ/7 gives the desired

where ξ1 follows from the deﬁnition of
inequality and Eq.(9), and ξ4 follows from
bound in Lemma B.6.
f
Mij −
f

||F =

Hence,

AT B

ij |

M

−

||

qP
Lemma B.7. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( ˜r+log(n/δ)

f

qP
ǫ2

AT
i Bj| ≤ ||
|
AT
i Bj|

2

≤

Ai||||

Bj ||
ij ǫ2

Proof. We can bound the spectral norm of the difference matrix as follows:

f

M

||

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

2

Ai||
||

Bj||
||

2 = ǫ

A

||F ||

B

||F .

||

), then with probability at least 1

δ,

−

M

||

−

AT B

ξ1=

||

f

AT
DA
BDB −
||
AT
DA||||
B
−
e
e
(1 + ǫ)2ǫ
e
||
B
A
7ǫ

A
e
||||
,
||

||||

||

B

≤ ||
ξ3

≤

≤

||

DAAT BDB + DAAT BDB −
AT B
DA||||
+
||||
||
B

DB||
+ (1 + ǫ)ǫ

AT B

+ ǫ

||||

A

A

DAAT B + DAAT B
I
DA −
+
DB −
||
B

||

I

||

||||

||

||

||||

||

AT B
||
AT B

||

−

||||

(11)

Mij, and ξ2 follows from Lemma B.5 and bound in Eq.(9). Rescaling

where ξ1 follows from the deﬁnition of
ǫ as ǫ/7 gives the desired bound in Lemma B.7.

f

We will frequently use the term with high probability. Here is a formal deﬁnition.

Deﬁnition B.8. We say that an event E occurs with high probability (w.h.p.) in n if the probability that its
complement ¯E happens is polynomially small, i.e., P r( ¯E) = O( 1

nα ) for some constant α > 0.

The following two lemmas deﬁne a ”nice” Π and when this happens with high probability.

Deﬁnition B.9. The random Gaussian matrix Π is ”nice” with parameter ǫ if for all (i, j) such that qij ≤
(i.e., qij = ˆqij), the sketched values

Mij satisﬁes the following two inequalities:

1

Mij|
|
ˆqij ≤
f

(1 + ǫ)

n
m

f
2
F +
||

A

(
||

B

2
F ),
||

||

X{j:ˆqij=qij } f

M 2
ij
ˆqij ≤

(1 + ǫ)

2n
m

A

(
||

2
F +
||

B

F )2.
2
||

||

Lemma B.10. If k = Ω( log(n)
w.h.p. in n.

ǫ2

), and 0 < ǫ < 1/14, then the random Gaussian matrix Π

Proof. According to Lemma B.6, if k = Ω( log(n)
ǫ

ǫ2

. In other words, the following holds with probability at least 1

δ:

), then w.h.p. in n, for all (i, j) we have

Ai|| · ||
||

Bj||

−

Mij| ≤ |
|
The above inequality is sufﬁcient for Π to be ”nice”:
f

Ai|| · ||
||

+ ǫ

AT
i Bj|

Bj|| ≤

(1 + ǫ)

Ai|| · ||
||

,
Bj||

(i, j)

∀

Rk×d is ”nice”

∈

AT
i Bj| ≤

Mij −
|
f

Mij
ˆqij ≤
f

(1 + ǫ) ||

Ai|| · ||
ˆqij

Bj||

≤

(1 + ǫ)

2 +
Ai||
(
||
( ||Ai||2
2n||A||2
F

2)/2
Bj||
||
+ ||Bj||2
2n||B||2
F

m

·

) ≤

(1 + ǫ)

n
m

A

(
||

2
F +
||

B

2
F )
||

||

16

M 2
ij
ˆqij ≤

(1 + ǫ)2

2

2

Bj||
||

Ai||
||
ˆqij

X{j:ˆqij=qij} f

X{j:ˆqij=qij}

4 +
Ai||
||
( ||Ai||2
2n||A||2
F

4

Bj||
||
+ ||Bj||2
2n||B||2
F

)

m

·

X{j:ˆqij=qij}
2n
2
F +
A
(
m
||
||

B

F )2.
2
||

||

(1 + ǫ)

(1 + ǫ)

≤

≤

ǫ2

Therefore, we conclude that if k = Ω( log(n)

), then Π is ”nice” w.h.p. in n.

C Proofs

C.1 Proof overview

We now present the key steps in proving Theorem 3.1. The framework is similar to that of LELA [3].

Our proof proceeds in three steps. In the ﬁrst step, we show that the sampled matrix provides a good
approximation of the actual matrix AT B. The result is summarized in Lemma C.1. Here RΩ(
M ) denotes
the sampled matrix weighted by the inverse of sampling probability (see Line 4 of Algorithm 2). Detailed
proof can be found in Appendix C.2. For consistency, we will use Ci (i = 1, 2, ...) to denote global constant
that can vary from step to step.

f

Lemma C.1. (Initialization) Let m and k satisfy the following conditions for sufﬁciently large constants C1
and C2:

then the following holds w.h.p. in n:

m

C1

≥

k

≥

||

A

B

2
2
F +
F
||
||
||
AT B
||F (cid:19)
||
2
A
||
||
AT B

˜r + log(n)
δ2

·

2 n
δ2 log(n),
B

,

(cid:18)

C2

2
||
2
F
||

||
||

RΩ(

M )

||

AT B

δ

AT B
||

||F .

|| ≤

−

In the second step, we show that at each iteration of WAltMin algorithm, there is a geometric decrease
V and the optimal ones U ∗, V ∗. The result is shown
in the distance between the computed subspaces
in Lemma C.2. Appendix C.3 provides the detailed proof. Here for any two orthonormal matrices X and
, where X⊥
Y , we deﬁne their distance as the principal angle based distance, i.e., dist(X, Y ) =
b
||
denotes the subspace orthogonal to X.

X T
||

⊥Y

U ,

f

b

Lemma C.2. (WAltMin Descent) Let k, m, and T satisfy the conditions stated in Theorem 3.1. Also,
(AT B)r||F . Let ˆU (t) and ˆV (t+1) be the t-th and
1
consider the case when
576ρr1.5 ||
(t+1)-th step iterates of the WAltMin procedure. Let U (t) and V (t+1) be the corresponding orthonormal
(U (t))i
1/2. Denote AT B as M , then the
||F and dist(U (t), U ∗)
matrices. Let
/
Ai||
A
||
||
||
γ/T :
following holds with probability at least 1

(AT B)r||F ≤

AT B
||

8√rρ

|| ≤

≤

−

dist(V t+1, V ∗)

dist(U t, U ∗) + η

M

Mr||F /σ∗

r + η,

−

||

−
1
2

≤

(V (t+1))j
||

|| ≤

8√rρ

/
Bj||
||
||

B

||F .

17

In the third step, we prove the spectral norm bound in Theorem 3.1 using results from the above two
lemmas. Comparing Lemma C.1 and C.2 with their counterparts of LELA (see Lemma C.2 and C.3 in [3]),
we notice that Lemma C.1 has the same bound as that of LELA, but the bound in Lemma C.2 contains
an extra term η. This term eventually leads to an additive error term ησ∗
r in Eq.(7). Detailed proof is in
Appendix C.4.

C.2 Proof of Lemma C.1

We ﬁrst prove the following lemma, which shows that RΩ(
we deﬁne CAB := (||A||2

F )2

.

F +||B||2
||AT B||2
F

Lemma C.3. Suppose Π is ﬁxed and is ”nice”. Let m
constant C1, then w.h.p. in n, the following is true:

M ) is close to

M . For simplicity of presentation,

f
C1 ·

≥

f

CAB

n
δ2 log(n) for sufﬁciently large global

RΩ(

M )

M

−

|| ≤

||

δ

AT B
||

||F .

Proof. This lemma can be proved in the same way as the proof of Lemma C.2 in [3]. The key idea is to use
f
the matrix Bernstein inequality. Let Xij = (δij −
j , where δij is a
random variable
ˆqij)wij
n
indicating whether the value at (i, j) has been sampled. Since Π is ﬁxed,
i,j=1 are independent zero
mean random matrices. Furthermore,

Xij}
{

0, 1
}
{

MijeieT

f

M )

M .

f
n
i,j=1 = RΩ(

−

Since Π is ”nice” with parameter 0 < ǫ < 1/14, we can bound the 1st and 2nd moment of Xij as

f

f

Xij}

i,j{

P

follows:

Xij ||
||

= max

(1

ˆqij)wij

{|

−

ˆqijwij
|

,
Mij|
f

ξ1

≤

Mij|
|
ˆqij
f

Mij|} ≤
f

(1 + ǫ)

n
m

A

(
||

2
F +
||

B

2
F );
||

||

σ2 = max

XijX T

ij 

E

,


Xij


E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
ˆqij −

1)

{(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(
|

= max

i



(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Xij
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

M 2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ij
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆqij
X{j:ˆqij=qij } f

M 2
ij |

ξ3

≤

f

ξ2= max
i

ˆqij(1

−

ˆqij)w2
ij

X T
ij Xij
(cid:12)
(cid:12)
}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(1 + ǫ)

ξ1

≤

(cid:12)
(cid:12)
Xj
(cid:12)
(cid:12)
(cid:12)
2
(cid:12)
F +
||

2n
m

A

(
||

B

F )2,
2
||

||

M 2
ij(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f

1. Now we can use matrix Bernstein inequality (see Lemma B.1) with t = δ

where ξ1 follows from Lemma B.10, ξ2 follows from a direct calculation, and ξ3 follows from the fact that
||F to show that
ˆqij ≤
n
δ2 log(n), then the desired inequality holds w.h.p. in n, where C1 is some global
if m
≥
constant independent of A and B. Note that since 0 < ǫ < 1/14, (1 + ǫ) < 2. Rescaling C1 gives the
desired result.

(1 + ǫ)C1CAB

AT B
||

Now we are ready to prove Lemma C.1, which is a counterpart of Lemma C.2 in [3].

Proof. We ﬁrst show that
||F holds w.h.p. in n over the randomness of Π. Note
that in Lemma C.3, we have shown that it is true for a ﬁxed and ”nice” Π, now we want to show that it also
holds w.h.p. in n even for a random chosen Π.

RΩ(

|| ≤

M )

f

f

M

−

||

δ

AT B
||

Let G be the event that we desire, i.e., G =

AT B
||
complimentary event. By conditioning on Π, we can bound the probability of ¯G as

RΩ(

|| ≤

AT

AT

B)

{||

−

B

δ

||F }

. Let ¯G be the

P r( ¯G) = P r( ¯G
P r( ¯G

≤

e
Π is ”nice”)P r(Π is ”nice”) + P r( ¯G
Π is not ”nice”)P r(Π is not ”nice”)
|
|
Π is ”nice”) + P r(Π is not ”nice”).
|

e

e

e

18

n
δ2 log(n), and k

, then both
CAB
≥
in n. Therefore, the the probability of ¯G is

log(n)
ǫ2

C2

According to Lemma C.3 and Lemma B.10, if m
events
polynomially small in n, i.e., the desired event G happens w.h.p. in n.
AT B

C1 ·
and P r(Π is ”nice”) happen w.h.p.

Π is ”nice”
G
|
{

Next we show that

M

≥

}

||
), then w.h.p. in n, we have

−

Θ( ˜r+log(n)
ǫ2
k = Θ( ˜r+log(n)

δ2

f
||A||2||B||2
||AT B||2
F

), then

M

||
By triangle inequality, we have

·

−
shown that w.h.p. in n, both terms are less than δ
inequality
−
statement of Lemma C.1.

AT B
||

AT B

RΩ(

|| ≤

AT

B)

||

f
||

δ

|| ≤
||

B

AT B

AT B
δ
||
M
−
AT B
−
f
RΩ(

||F holds w.h.p. in n. According to Lemma B.7, if k =
. Now let ǫ := δ ||AT B||F
||A||||B|| , we have that if
|| ≤
||
δ

A
ǫ
||
||||
AT B
||F holds w.h.p. in n.
||
AT B
. We have
+
M )
|| ≤ ||
−
||
||
AT B
||F . By rescaling δ as δ/2, we have that the desired
||
||F holds w.h.p. in n, when m and k are chosen according to the
f
f

AT B

RΩ(

|| ≤

M )

f

f

M

M

−

||

e

e

Because the bound of Lemma C.1 has the same form as that of Lemma C.2 in [3], the corollary
(AT B)r||F ≤

M ), which is stated here without proof:

AT B
||

−

if

of Lemma C.2 also holds for RΩ(
1
576κr1.5 ||

(AT B)r||F , then w.h.p. in n we have
U (0))i
/
Ai||
(
||
||
||

f
8√r

|| ≤

A

||F

and dist(

U (0), U ∗)

1/2,

≤

U (0) is the initial iterate produced by the WAltMin algorithm (see Step 6 of Algorithm 2). This

b

where
b
corollary will be used in the proof of Lemma C.2.

b

(AT B)r||F ≥
AT B
Similar to the original proof in [3], we can now consider two cases separately: (1)
||
1
(AT B)r||F . The ﬁrst case is simple: use
(AT B)r||F ≤
576ρr1.5 ||
Lemma C.1 and Wely’s inequality [31] already implies the desired bound in Theorem 3.1. To see why,
note that Lemma C.1 and Wely’s inequality imply that

(AT B)r||F ; (2)

1
576ρr1.5 ||

AT B
||

−

−

(AT B)r −
||
AT B

−

AT B

−
AT B
2
||

−

(RΩ(
M )r||
(AT B)r||
+
f
(AT B)r||
(AT B)r||

+ δ

ξ1

≤ ||
ξ2

≤ ||
ξ3

≤

RΩ(

M )

+

RΩ(

M )

||

AT B
||

−
||
AT B
||F +
RΩ(
f
||
||
||F ,

AT B
||

+ 2δ

f

M )

AT B
f

||

−

−

(RΩ(

M ))r||
AT B
f
||

−

+

(AT B)r||

(12)

where Mr denotes the best rank-r approximation of M , ξ1 follows triangle inequality, ξ2 follows from
Lemma C.1 and Wely’s inequality, and ξ3 follows from Lemma C.1. If
(AT B)r||F . Setting δ =
(AT B)r||F ≤
then
O(η/(ρr1.5)) in Eq.(12) gives the desired error bound in Theorem 3.1. Therefore, in the following analysis
we only need to consider the second case.

AT B
−
||
AT B
O(ρr1.5)
||

(AT B)r||F ≥
−

(AT B)r||F +
||

1
576ρr1.5 ||

AT B
||

AT B
||

||F =

−

(AT B)r||F ,

C.3 Proof of Lemma C.2

We ﬁrst prove the following lemma, which is a counterpart ofLemma C.5 in [3]. For simplicity of presenta-
tion, we use M to denote AT B in the following proof.

Lemma C.4. If m
C1 and C2, then the following holds with probability at least 1

C1nr log(n)T /(γδ2) and k

≥

≥

C2(˜r+log(n))/ǫ2 for sufﬁciently large global constants

(U (t))T RΩ(
||

M

−

Mr)

−

(U (t))T (M

Mr)

δ

M

|| ≤

||

−

−

A

||F ||

B

||F + ǫ

||

A

||

B

.
||

||||

γ/T :

−
Mr||F + δǫ

f

19

Proof. For a ﬁxed Π, we have that if m
least 1

γ/T :

≥

−

C1nr log(n)T /(γδ2), then following holds with probability at

(U (t))T RΩ(
||

M

−

Mr)

−

(U (t))T (

M

Mr)

δ

M

|| ≤

||

Mr||F .

−

−

(13)

The proof of Eq.(13) is exactly the same as the proof of Lemma C.5/B.6/B.2 in [3], so we omit its details
here. The key idea is to deﬁne a set of zero-mean random matrices Xij such that
Mr)
desired bound.

−
Mr), and then use second moment-based matrix Chebyshev inequality to obtain the

ij Xij = (U (t))T RΩ(

(U (t))T (

P

f

f

f

f

M

M

−

−

According to Lemma B.6 and Lemma B.7, if k = Θ((˜r + log(n))/ǫ2), then w.h.p. in n, the following

f

holds:

M

||

−

AT B

||F ≤

ǫ

A

||

||F ||

B

||F ,

M

||

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

(14)

Using triangle inequality, we have that if m and k satisfy the conditions of Lemma C.4, then the follow-

ing holds with probability at least 1

γ/T :

f

f

−

−

(U (t))T RΩ(
||
(U (t))T RΩ(

M

≤ ||
ξ1

δ

δ

δ

M

||

M
f

||

M

||

−

−

−

≤

≤
ξ2

≤

M
f
−
Mr||F +
f
||
Mr||F + δ
||
Mr||F + δǫ

Mr)

Mr)

−

−

(U (t))T (M
(U (t))T (

M

Mr)

||
Mr)

||

−

−

f

M

M

M

−

−

||
||F +
M
||
f
||F + ǫ
B
f
||F ||

M

A

||

M

||

−

A

B
f

,
||

||||

||

+

(U (t))T (M
||

−

M )

||

f

where ξ1 follows from Eq.(13), and ξ2 follows from Eq.(14).

Now we are ready to prove Lemma C.2. For simplicity, we focus on the rank-1 case here. Rank-
r proof follows a similar line of reasoning and can be obtained by combining the current proof with the
rank-r analysis in the original proof of LELA [3]. Note that compared to Lemma C.5 in [3], Lemma C.4
contains two extra terms δǫ
. Therefore, we need to be careful for steps that involve
Lemma C.4.

||F + ǫ

||F ||

||||

B

B

A

A

||

||

||

In the rank-1 case, we use ˆut and ˆvt+1 to denote the t-th and (t+1)-th step iterates (which are column

vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized vectors.

B

||F , for some constant c1.
Bj||
/
||
Bounding dist(vt+1, v∗):
In Lemma C.4, set ǫ = ||AT B||
AT B

η

Proof. This proof contains two parts. In the ﬁrst part, we will prove that the distance dist(vt+1, v∗) de-
creases geometrically over time. In the second part, we show that the j-th entry of vt+1 satisﬁes
c1||

vt+1
j
|

| ≤

2||A||||B||η and δ = η
2˜r , where 0 < η < 1, then we have δǫ
AT B
/2, and ǫ
B
||
γ/T , the following holds:

||F ≤
/2. Therefore, with probability at least

AT B
||

η2
2˜r ||

||F ||

|| ≤

|| ≤

||||

B

A

A

||

||

||

||

η

·

||A||F ||B||F
||A||||B||

1

−

M

M1)

(ut)T (M

M1)

η

M

(ut)T RΩ(
||
(ut)T RΩ(
||

Hence, we have

−

−

−
dist(ut, u∗)
||
Using the explicit formula for WAltMin update (see Eq.(46) and Eq.(47) in [3]), we can bound
ˆvt+1, v∗
h

M1||F /˜r + ησ∗
1.

||
M1||

as follows.

f
−

M1)

|| ≤

|| ≤

+ η

⊥i

f

M

M

−

−

−

||

and

M1||F /˜r + ησ∗
1.
M

(15)

ˆvt+1, v∗
h

i

ˆut
||

||h

ˆvt+1, v∗

/σ∗
i

1 ≥ h

ut, u∗

i −

1

δ1

1

− h

ut, u∗

2
i

−

1

δ1

1

−

M

(η ||

M1||F

−
˜rσ∗
1

+ η).

δ1

−

p

20

ˆut
||

ˆvt+1, v∗
⊥i

||h

/σ∗

1 ≤

1

1

− h

ut, u∗

2 +
i

1

δ1

−

δ1

p

1

−

δ1

(dist(ut, u∗) ||

M

M1||

+ η ||

M

−
σ∗
1

As discussed in the end of Appendix C.2, we only need to consider the case when
1
576ρr1.5

(AT B)r||F , where ρ = σ∗
||

AT B
−
||
r . In the rank-1 case, this condition reduces to
M
−
||
1
ut, u∗
20 ), and use the fact that
h
ˆvt+1, v∗
as
h

1
20 , η
ˆvt+1, v∗
h

For sufﬁciently small constants δ1 and η (e.g., δ1 ≤
and dist(u0, u∗)

1/2, we can further bound

1/σ∗

≤
and

⊥i

≤

i

M1||F

+ η).

−
˜rσ∗
1
(AT B)r||F ≤
σ∗
576 .
M1||F ≤
u0, u∗
i ≥ h

i

ˆut
||

||h

ˆvt+1, v∗

/σ∗
i

1 ≥ h

u0, u∗

i −

1

− h

u0, u∗

2
i

−

1
10 ≥

√3
2 −

2
10 ≥

1
2

.

ˆut
||

ˆvt+1, v∗
⊥i

||h

/σ∗

δ1

dist(ut, u∗) +

dist(ut, u∗) +

1 ≤
ξ1

≤

1
1
4

δ1

−
dist(ut, u∗) + 2(η

δ1)
−
M1||F /σ∗
1 and the assumption that δ1 is sufﬁciently small.

1 + η),

M

−

−

||

1

where ξ1 uses the fact that ˜r

Now we are ready to bound dist(vt+1, v∗) as

≥

M

(η ||

1

δ1

M1||F

−
˜rσ∗
1

+ η)

1
10

p

1
576(1

dist(vt+1, v∗) =

1

vt+1, v∗

− h

2 =
i

ˆvt+1, v∗
h
2 +

ˆvt+1, v∗
h
p
M
||

⊥i
Mr||F /σ∗

−

⊥i
ˆvt+1, v∗
h
1 + η),

p
1
2

ξ1

≤

dist(ut, u∗) + 4(η

ˆvt+1, v∗
⊥i
h
ˆvt+1, v∗
i
h

2 ≤
i

where ξ1 follows from substituting Eqs. (16) and (17). Rescaling η as η/4 gives the desired bound of
Lemma C.2 for the rank-1 case. Rank-r proof can be obtained by following a similar framework.

Bounding vt+1
In this step, we need to prove that the j-th entry of vt+1 satisﬁes

:

j

vt+1
j
|

| ≤

c1

||Bj||
||B||F

for all j, under the

assumption that ut satisﬁes the norm bound

||Ai||
||A||F
The proof follows very closely to the second part of proving Lemma C.3 in [3], except that an extra
Mij using Bernstein inequality. More
Mij. Note that if ˆqij = 1, then δij = 1, Xi = 0, so we only need to

multiplicative term (1 + ǫ) will show up when bounding
ˆqij)wijut
speciﬁcally, let Xi = (δij −
i
consider the case when ˆqij < 1, i.e., ˆqij = qij, where qij is deﬁned in Eq.(1).

i δijwijut
i

ut
i| ≤
|

for all i.

P

f

c1

Suppose Π is ﬁxed and its dimension satisﬁes k = Ω( log(n)

f

), then according to Lemma B.6, we have

ǫ2

that w.h.p. in n,

Hence, we have

Mij| ≤ |
|
f
M 2
ij
ˆqij
f

≤

ξ1

Mij|

+ ǫ

Ai|| · ||
||

Bj|| ≤

(1 + ǫ)

Ai|| · ||
||

,
Bj||

(i, j).

∀

m

2

(1 + ǫ)2
2
Bj||
Ai||
||
||
+ ||Bj||2
( ||Ai||2
2n||B||2
2n||A||2
·
F
F
c2
(ut
i)2
Ai||
1||
( ||Ai||2
ˆqij
2n||A||2
F

m

≤

ξ2

·

2n(1 + ǫ)2
m

) ≤

Bj||

2

A

2
F ,
||

||

· ||

2/
2
A
F
||
||
+ ||Bj||2
2n||B||2
F

) ≤

2nc2
1
m

,

(16)

(17)

(18)

(19)

(20)

(21)

where ξ1 follows from substituting Eqs.(19) and (1), and ξ2 follows from the assumption that
c1||

/
Ai||
||

||F .
A

ut
i| ≤
|

21

We can now bound the ﬁrst and second moments of Xi as

Xi| ≤ |
|

wijut
i

(ut
i)2
ˆqij s

M 2
ij
ˆqij
f

Mij| ≤ s
f

ξ1

≤

2nc1(1 + ǫ)
m

||F .
A
Bj||||
||

V ar(Xi) =

ˆqij(1

ˆqij)w2

ij(ut

i)2

M 2

ij ≤

(1 + ǫ)2

2

Ai||
||

Bj||
||

2

Xi

−

Xi
2nc2

1(1 + ǫ)2

m

ξ2

≤

2

Bj||
||

A

||

f
2
F ,
||

(ut
i)2
ˆqij

Xi

where ξ1 and ξ2 follows from substituting Eqs.(20) and (21).

The rest proof involves applying Bernstein’s inequality to derive a high-probability bound on

i Xi,
which is almost the same as the second part of proving Lemma C.3 in [3], so we omit the details here. The
only difference is that, because of the extra multiplicative term (1 + ǫ) in the bound of the ﬁrst and second
moments, the lower bound on the sample complexity m should also be multiplied by an extra (1 + ǫ)2 term.
By restricting 0 < ǫ < 1/2, this extra multiplicative term can be ignored as long as the original lower bound
of m contains a large enough constant.

P

C.4 Proof of Theorem 3.1

We now prove our main theorem for rank-1 case here. Rank-r proof follows a similar line of reasoning and
can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3].
vt+1 to denote the t-th and (t+1)-th step iterates (which are
Similar to the previous section, we use
column vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized
vectors.

ut and

b

b

The closed form solution for WAltMin update at t + 1 iteration is

ut
||

vt+1
j = σ∗
||

1v∗
j

i δijwijut
i δijwij(ut

iu∗
i
i)2 +

i δijwijut
i(
M
M1)ij
−
i δijwij(ut
i)2

.

P

f

P

Writing in matrix form, we get

b

b

ut
||

vt+1
j = σ∗
1h
||

u∗, ut

v∗
i

where B and C are diagonal matrices with Bjj =
vector RΩ(

M1)T ut with entries yj =

M

b

b

Each term of Eq.(22) can be bounded as follows.

−

f

C)v∗ + B−1y,

−

u∗, ut
1B−1(
σ∗
h
i δijwij(ut
i δijwijut
M
i(
P

B
i
−
i)2 and Cjj =
M1)ij.

−

P

i δijwijut

iu∗

i , and y is the

u∗, ut
(
h
||

B
i

P
C)v∗

|| ≤

f
dist(ut, u∗),

B−1
||

|| ≤

2,

y
||

||

=

RΩ(

M

||

−

M1)T ut

dist(ut, u∗)
||

M

M1||

−

+ η

M

||

M1||F /˜r + ησ∗
1,

−

where ξ1 follows directly from Lemma C.4. The proof of Eq.(23) is exactly the same as the proof of Lemma
B.3 and B.4 in [3].

f

According to Lemma C.2, since the distance is decreasing geometrically, after O(log( 1

ζ )) iterations we

get

(22)

(23)

(24)

(25)

dist(ut, u∗)

ζ + 2η

M

||

≤

M1||F /σ∗

−

1 + 2η.

P
P

−
ξ1

||

≤

22

Now we are ready to prove the spectral norm bound in Theorem 3.1:

M1 −
||
M1 −
(I
−

≤ ||

+

ut(
vt+1)T
||
ut(ut)T M1||
b
b
ut(ut)T )M1||
v∗
σ1h
||
i
u∗, ut
1B−1(
σ∗
h
||

ut(ut)T M1 −
vt+1)T
ut(
||
||
vt+1)T ]
ut
ut[(ut)T M1 − ||
(
||
||
||
b
b
vt+1)T
ut, u∗
(
b
b
||
C)v∗
b

− ||

ut

−

+

+

||

≤ ||
ξ1
1dist(ut, u∗) +
σ∗

1dist(ut, u∗) +
σ∗

1dist(ut, u∗) + 2σ∗
σ∗

5(ζσ∗
≤
= 5ζσ∗

1 + 2η
||
1 + 12η

M

M

||

−

−

||

B−1y
B
b
i
||
1dist(ut, u∗) + 2dist(ut, u∗)
||
M1||F + 2ησ∗
M1||F + 12ησ∗

||
M1||
−
M1||F + 2ησ∗

1) + 2η

M

M

−

||

1

1

≤
ξ2

≤
ξ3

≤
ξ4

+ 2η

M

||

M1||F /˜r + 2ησ∗

1

−

(26)

v∗,
where ξ1 follows from the deﬁnition of dist(ut, u∗), the fact that
= 1, and (ut)T M1 = σ1h
i
ξ2 follows from substituting Eq.(22), ξ3 follows from Eqs.(23) and (24), and ξ4 follows from the Eq.(25),
and fact that
1) (this will inﬂuence the number of iterations)
and also rescaling η to η/12 gives us the desired spectral norm error bound in Eq.(7). This completes our
proof of the rank-1 case. Rank-r proof follows a similar line of reasoning and can be obtained by combining
the current proof with the rank-r analysis in the original proof of LELA [3].

1. Rescaling ζ to ζ/(5σ∗

M1|| ≤

σ∗
1, ˜r

ut, u∗

ut
||

M

−

≥

||

||

C.5 Sampling

We describe a way to sample m elements in O(m log(n)) time using distribution qij deﬁned in Eq. (1).
Naively one can compute all the n2 entries of min
and toss a coin for each entry, which takes O(n2)
qij, 1
}
{
time. Instead of this binomial sampling we can switch to row wise multinomial sampling. For this, ﬁrst
estimate the expected number of samples per row mi = m( ||Ai||2
2n ). Now sample m1 entries from row
2||A||2
F
1 according to the multinomial distribution,

+ 1

m
m1 ·

( ||
2n

2

A1||
2
A
F
||
||

+ ||
2n

2

Bj||
2
B
F
||
||

) =

||A1||2
2n||A||2
F
||Ai||2
2||A||2
F

+ ||Bj||2
2n||B||2
F
+ 1
2n

.

q1j =

e

j

e

P

Note that
q1j = 1. To sample from this distribution, we can generate a random number in the interval
[0, 1], and then locate the corresponding column index by binary searching over the cumulative distribution
function (CDF) of
q1j. This takes O(n) time for setting up the distribution and O(m1 log(n)) time to
sample. For subsequent row i, we only need O(mi log(n)) time to sample mi entries. This is because for
binary search to work, only O(mi log(n)) entries of the CDF vector needs to be computed and checked.
Note that the speciﬁc form of
qij ensures that its CDF entries can be updated in an efﬁcient way (since we
only need to update the linear shift and scale). Hence, sampling m elements takes a total O(m log(n)) time.
Furthermore, the error in this model is bounded up to a factor of 2 of the error achieved by the Binomial
model [7] [21]. For more details please see our Spark implementation.

e

e

D Related work

Approximate matrix multiplication:

In the seminal work of [14], Drineas et al. give a randomized algorithm which samples few rows of A and
B and computes the approximate product. The distribution depends on the row norms of the matrices and

23

ǫ

||

||

A

A

B

B

−

−

||2||

||F ||

||2 ≤

AT B
||

AT B
||

For spectral norm bound of the form

||F . Later Sarlos [29] propose a sketching
the algorithm achieves an additive error proportional to
based algorithm, which computes sketched matrices and then outputs their product. The analysis for this
˜AT ˜B
algorithm is then improved by [10]. All of these results compare the error
||F in Frobenius
norm.
||2, the authors in [29, 11] show that
C
the sketch size needs to satisfy O(r/ǫ2), where r = rank(A) + rank(B). This dependence on rank is later
improved to stable rank in [26], but at the cost of a weaker dependence on ǫ. Recently, Cohen et al. [12]
further improve the dependence on ǫ and give a bound of O(˜r/ǫ2), where ˜r is the maximum stable rank. Note
that the sketching based algorithm does not output a low rank matrix. As shown in Figure 2, rescaling by
the actual column norms provide a better estimator than just using the sketched matrices. Furthermore, we
show that taking SVD on the sketched matrices gives higher error rate than our algorithm (see Figure 3(b)).
Low rank approximation: [16] introduced the problem of computing low rank approximation of a
given matrix using only few passes over the data. They gave an algorithm that samples few rows and
columns of the matrix and computes its SVD for low rank approximation. They show that this algorithm
achieves additive error guarantees in Frobenius norm. [15, 29, 19, 13] have later developed algorithms using
various sketching techniques like Gaussian projection, random Hadamard transform and volume sampling
that achieve relative error in Frobenius norm.[35, 28, 18, 6] improved the analysis of these algorithms and
provided error guarantees in spectral norm. More recently [11] presented an algorithm based on subspace
embedding that computes the sketches in the input sparsity time.

Another class of methods use entrywise sampling instead of sketching to compute low rank approxi-
mation. [1] considered an uniform entrywise sampling algorithm followed by SVD to compute low rank
approximation. This gives an additive approximation error. More recently [3] considered biased entrywise
sampling using leverage scores, followed by matrix completion to compute low rank approximation. While
this algorithm achieves relative error approximation, it takes two passes over the data.

There is also lot of interesting work on computing PCA over streaming data under some statistical
assumptions, e.g., [2, 27, 5, 30]. In contrast, our model does not put any assumptions on the input matrix.
Besides, our goal here is to get a low rank matrix and not just the subspace.

24

6
1
0
2
 
t
c
O
 
6
2

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
5
6
6
0
.
0
1
6
1
:
v
i
X
r
a

Single Pass PCA of Matrix Products

Shanshan Wu
The University of Texas at Austin
shanshan@utexas.edu

Srinadh Bhojanapalli
Toyota Technological Institute at Chicago
srinadh@ttic.edu

Sujay Sanghavi
The University of Texas at Austin
sanghavi@mail.utexas.edu

Alexandros G. Dimakis
The University of Texas at Austin
dimakis@austin.utexas.edu

October 27, 2016

Abstract

In this paper we present a new algorithm for computing a low rank approximation of the product
AT B by taking only a single pass of the two matrices A and B. The straightforward way to do this is to
(a) ﬁrst sketch A and B individually, and then (b) ﬁnd the top components using PCA on the sketch. Our
algorithm in contrast retains additional summary information about A, B (e.g. row and column norms
etc.) and uses this additional information to obtain an improved approximation from the sketches. Our
main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in
addition we also provide results from an Apache Spark implementation that shows better computational
and statistical performance on real-world and synthetic evaluation datasets.

1 Introduction

Given two large matrices A and B we study the problem of ﬁnding a low rank approximation of their
product AT B, using only one pass over the matrix elements. This problem has many applications in machine
learning and statistics. For example, if A = B, then this general problem reduces to Principal Component
Analysis (PCA). Another example is a low rank approximation of a co-occurrence matrix from large logs,
e.g., A may be a user-by-query matrix and B may be a user-by-ad matrix, so AT B computes the joint
counts for each query-ad pair. The matrices A and B can also be two large bag-of-word matrices. For this
case, each entry of AT B is the number of times a pair of words co-occurred together. As a fourth example,
AT B can be a cross-covariance matrix between two sets of variables, e.g., A and B may be genotype and
phenotype data collected on the same set of observations. A low rank approximation of the product matrix
is useful for Canonical Correlation Analysis (CCA) [8]. For all these examples, AT B captures pairwise
variable interactions and a low rank approximation is a way to efﬁciently represent the signiﬁcant pairwise
interactions in sub-quadratic space.

×

n (d

Let A and B be matrices of size d

n) assumed too large to ﬁt in main memory. To obtain a
rank-r approximation of AT B, a naive way is to compute AT B ﬁrst, and then perform truncated singular
value decomposition (SVD) of AT B. This algorithm needs O(n2d) time and O(n2) memory to compute
the product, followed by an SVD of the n
n matrix. An alternative option is to directly run power method
on AT B without explicitly computing the product. Such an algorithm will need to access the data matrices
A and B multiple times and the disk IO overhead for loading the matrices to memory multiple times will be
the major performance bottleneck.

≫

×

1

For this reason, a number of recent papers introduce randomized algorithms that require only a few
passes over the data, approximately linear memory, and also provide spectral norm guarantees. The key step
in these algorithms is to compute a smaller representation of data. This can be achieved by two different
methods: (1) dimensionality reduction, i.e., matrix sketching [29, 11, 26, 12]; (2) random sampling [14, 3].
The recent results of Cohen et al. [12] provide the strongest spectral norm guarantee of the former. They
show that a sketch size of O(˜r/ǫ2) sufﬁces for the sketched matrices
B to achieve a spectral error of ǫ,
where ˜r is the maximum stable rank of A and B. Note that
B is not the desired rank-r approximation
of AT B. On the other hand,
[3] is a recent sampling method with very good performance guarantees.
The authors consider entrywise sampling based on column norms, followed by a matrix completion step
to compute low rank approximations. There is also a lot of interesting work on streaming PCA, but none
can be directly applied to the general case when A is different from B (see Figure 4(c)). Please refer to
Appendix D for more discussions on related work.

AT

AT

e

e

e

e

Despite the signiﬁcant volume of prior work, there is no method that computes a rank-r approximation
of AT B when the entries of A and B are streaming in a single pass 1. Bhojanapalli et al. [3] consider a
two-pass algorithm which computes column norms in the ﬁrst pass and uses them to sample in a second
pass over the matrix elements. In this paper, we combine ideas from the sketching and sampling literature
to obtain the ﬁrst algorithm that requires only a single pass over the data.

Contributions:

•

•

•

•

We propose a one-pass algorithm SMP-PCA (which stands for Streaming Matrix Product PCA) that
computes a rank-r approximation of AT B in time O((nnz(A) + nnz(B)) ρ2r3 ˜r
). Here
nnz(
) is the number of non-zero entries, ρ is the condition number, ˜r is the maximum stable rank, and
·
η measures the spectral norm error. Existing two-pass algorithms such as [3] typically have longer
runtime than our algorithm (see Figure 3(a)). We also compare our algorithm with the simple idea that
ﬁrst sketches A and B separately and then performs SVD on the product of their sketches. We show
that our algorithm always achieves better accuracy and can perform arbitrarily better if the column
vectors of A and B come from a cone (see Figures 2, 4(b), 3(b)).

η2 + nr6ρ4 ˜r3

η4

The central idea of our algorithm is a novel rescaled JL embedding that combines information from
matrix sketches and vector norms. This allows us to get better estimates of dot products of high
dimensional vectors compared to previous sketching approaches. We explain the beneﬁt compared to
a naive JL embedding in Figure 2 and the related discussion; we believe it may be of more general
interest beyond low rank matrix approximations.

AT B
k

We prove that our algorithm recovers a low rank approximation of AT B up to an error that depends
on
, decaying with increasing sketch size and number of samples
k
(Theorem 3.1). The ﬁrst term is a consequence of low rank approximation and vanishes if AT B is
exactly rank-r. The second term results from matrix sketching and subsampling; the bounds have
similar dependencies as in [12].

(AT B)rk

AT B
k

and

−

We implement SMP-PCA in Apache Spark and perform several distributed experiments on synthetic
and real datasets. Our distributed implementation uses several design innovations described in Sec-
tion 4 and Appendix C.5 and it is the only Spark implementation that we are aware of that can handle
matrices that are large in both dimensions. Our experiments show that we improve by approximately
a factor of 2
in running time compared to the previous state of the art and scale gracefully as the
cluster size increases. The source code is available online [36].

×

1One straightforward idea is to sketch each matrix individually and perform SVD on the product of the sketches. We compare

against that scheme and show that we can perform arbitrarily better using our rescaled JL embedding.

2

•

In addition to better performance, our algorithm offers another advantage: It is possible to compute
low-rank approximations to AT B even when the entries of the two matrices arrive in some arbitrary
order (as would be the case in streaming logs). We can therefore discover signiﬁcant correlations even
when the original datasets cannot be stored, for example due to storage or privacy limitations.

2 Problem setting and algorithms

Rd×n2 that are stored in disk,
Consider the following problem: given two matrices A
ﬁnd a rank-r approximation of their product AT B. In particular, we are interested in the setting where
both A, B and AT B are too large to ﬁt into memory. This is common for modern large scale machine
learning applications. For this setting, we develop a single-pass algorithm SMP-PCA that computes the
rank-r approximation without explicitly forming the entire matrix AT B.

Rd×n1 and B

∈

∈

Notations. Throughout the paper, we use A(i, j) or Aij to denote (i, j) entry for any matrix A. Let
Ai and Aj be the i-th column vector and j-th row vector. We use
for
spectral (or operator) norm. The optimal rank-r approximation of matrix A is Ar, which can be found by
Rn1×n2 as the projection
SVD. Given a set Ω
⊂
of A on Ω, i.e., PΩ(A)(i, j) = A(i, j) if (i, j)

kF for Frobenius norm, and
A
k
Rn1×n2, we deﬁne PΩ(A)

[n2] and a matrix A

Ω and 0 otherwise.

A
k
k

[n1]

×

∈

∈

∈

2.1 SMP-PCA

Our algorithm SMP-PCA (Streaming Matrix Product PCA) takes four parameters as input: the desired rank
r, number of samples m, sketch size k, and the number of iterations T . Performance guarantee involving
these parameters is provided in Theorem 3.1. As illustrated in Figure 1, our algorithm has three main steps:
1) compute sketches and side information in one pass over A and B; 2) given partial information of A and
B, estimate important entries of AT B; 3) compute low rank approximation given estimates of a few entries
of AT B. Now we explain each step in detail.

B and the column norms

Figure 1: An overview of our algorithm. A single pass is performed over the data to produce the sketched
[n2]. We then compute the
,
matrices
A,
Bjk
Aik
k
k
×
Ω and
M ) through a biased sampling process, where PΩ(
sampled matrix PΩ(
M ) =
e
M as an estimator for AT B, and
zero otherwise. Here Ω represents the set of sampled entries. We deﬁne
f
M ) gives the
. Performing matrix completion on PΩ(
Aik · k
k
f

f
M (i, j) =
desired rank-r approximation.

compute its entry as

eBj
eAT
i
e
e
Bj k
Aik·k

M (i, j) if (i, j)

, for all (i, j)

Bjk ·

[n1]

f

∈

∈

e

k

Step 1: Compute sketches and side information in one pass over A and B. In this step we compute
(0, 1/k)

Rk×d is a random matrix with entries being i.i.d.

B := ΠB, where Π

A := ΠA and

sketches

f

N

f

e

e

∈

3

Algorithm 1 SMP-PCA: Streaming Matrix Product PCA

1: Input: A

∈
iterations: T

∈

Rd×n1, B

Rd×n2, desired rank: r, sketch size: k, number of samples: m, number of

2: Construct a random matrix Π
pass over A and B to obtain:

3: Sample each entry (i, j)

×
deﬁned in Eq.(1); maintain a set Ω

∈

∈
A = ΠA,

Rk×d, where Π(i, j)
B = ΠB, and

(0, 1/k),
,
Bjk
k

[k]
×
∼ N
,
[n1]
Aik
k
[n2] independently with probability ˆqij = min

[n2].
×
1, qij}
{
[n2] which stores all the sampled pairs (i, j).
Rn1×n2, where PΩ(

M (i, j) is given in Eq. (2). Calculate PΩ(

(i, j)
(i, j)

e
[n1]

[n1]
e

∈
∈

M )

∀
∀

×

⊂

[d]. Perform a single

, where qij is

Rn1×n2, where

Ω and zero otherwise.
f
M ), Ω, r, ˆq, T ), see Appendix A for more details.

f

∈

M ) =

f

∈

4: Deﬁne

M
M (i, j) if (i, j)
∈
f
5: Run WAltMin(PΩ(
6: Output:
U

f

Rn1×r and
f

∈

∈

V

b

b

Rn2×r.

random variables. It is known that Π satisﬁes an ”oblivious Johnson-Lindenstrauss (JL) guarantee” [29][34]
and it helps preserving the top row spaces of A and B [11]. Note that any sketching matrix Π that is an
oblivious subspace embedding can be considered here, e.g., sparse JL transform and randomized Hadamard
transform (see [12] for more discussion).

Besides
[n1]

A and

B, we also compute the L2 norms for all column vectors, i.e.,

, for all
[n2]. We use this additional information to design better estimates of AT B in the next step,
B to sample. Note that this is the only step that needs one

Bjk
k

Aik
k

and

AT

∈

(i, j)
×
and also to determine important entries of
e
e
pass over data.

Step 2: Estimate important entries of AT B by rescaled JL embedding. In this step we use partial
B. We ﬁrst determine
B to sample, and then propose a novel rescaled JL embedding for estimating those entries.

information obtained from the previous step to compute a few important entries of
what entries of

AT

AT

e

e

We sample entry (i, j) of AT B independently with probability ˆqij = min
e
1, qij}
{

, where
e

e

e

Bjk
+ k
2
B
2n1k
F
k
[n2] be the set of sampled entries (i, j). Since E(

Aik
( k
2
A
2n2k
F
k

qij = m

·

2

2

).

⊂

[n1]

Let Ω
i,j qij) = m, the expected number
of sampled entries is roughly m. The special form of qij ensures that we can draw m samples in O(n1 +
m log(n2)) time; we show how to do this in Appendix C.5.

P

×

Note that qij intuitively captures important entries of AT B by giving higher weight to heavy rows and
columns. We show in Section 3 that this sampling actually generates good approximation to the matrix
AT B.

The biased sampling distribution of Eq.

(1) is ﬁrst proposed by Bhojanapalli et al. [3]. However,
their algorithm [3] needs a second pass to compute the sampled entries, while we propose a novel way of
estimating dot products, using information obtained in the ﬁrst step.

Deﬁne

M

Rn1×n2 as

∈

f

M (i, j) =

Bjk ·

Aik · k
k

AT
Bj
i
Bjk
Aik · k
k
e
e
M , instead, we only calculate
e
e

.

Note that we will not compute and store
is denoted as PΩ(

M ), where PΩ(

f

We now explain the intuition of Eq. (2), and why

(i, j) entry of AT B, a straightforward way is to use
between vectors

f
Ai and

M (i, j) if (i, j)

M )(i, j) =
f

Ω and 0 otherwise.
f
B. To estimate the
M is a better estimator than
AT
θij is the angle
Bjk ·
Aik · k
Bj =
i
k
f
Bj. Since we already know the actual column norms, a potentially better estimator
e
e
e

AT
θij, where
e

cos

f

f

∈

e

e

e

e

M (i, j) for (i, j)

Ω. This matrix

∈

(1)

(2)

e

e

4

JL embedding
Rescaled JL embedding

2

1

0

-1

t
c
u
d
o
r
p

 
t

o
d

 

d
e

t

a
m

i
t
s
E

-2

-1

-0.5

0
True dot product

0.5

1

(a)

(b)

Figure 2: (a) Rescaled JL embedding (red dots) captures the dot products with smaller variance compared
to JL embedding (blue triangles). Mean squared error: 0.053 versus 0.129. (b) Lower ﬁgure illustrates how
to construct unit-norm vectors from a cone with angle θ. Let x be a ﬁxed unit-norm vector, and let t be a
(x + t) with probability
random Gaussian vector with expected norm tan(θ/2), we set y as either x + t or
AT B
half, and then normalize it. Upper ﬁgure plots the ratio of spectral norm errors
,
/
k
k
−
k
M has better
when the column vectors of A and B are unit vectors drawn from a cone with angle θ. Clearly,
B for all possible values of θ, especially when θ is small.
accuracy than

−
AT B
k

AT

AT

f

M

−

B

e

e

f

would be

e
Aik · k
k

e
Bjk ·

cos

θij. This removes the uncertainty that comes from distorted column norms2.

AT
i

e

Bj (JL embedding) and

Figure 2(a) compares the two estimators

M (i, j) (rescaled JL embedding)
for dot products. We plot simulation results on pairs of unit-norm vectors with different angles. The vectors
have dimension 1,000 and the sketching matrix has dimension 10-by-1,000. Clearly rescaling by the actual
norms help reduce the estimation uncertainty. This phenomenon is more prominent when the true dot
products are close to
1,
and hence the uncertainty from angles may produce smaller distortion compared to that from norms. In the
extreme case when cos θ =

1, which makes sense because cos θ has a small slope when cos θ approaches

1, rescaled JL embedding can perfectly recover the true dot product.

f

±

±

e

e

In the lower part of Figure 2(b) we illustrate how to construct unit-norm vectors from a cone with angle
θ. Given a ﬁxed unit-norm vector x, and a random Gaussian vector t with expected norm tan(θ/2), we
construct new vector y by randomly picking one from the two possible choices x + t and
(x + t), and then
renormalize it. Suppose the columns of A and B are unit vectors randomly drawn from a cone with angle
AT
θ, we plot the ratio of spectral norm errors
in Figure 2(b). We observe that
B and can be much better when θ approaches zero, which agrees with the trend
M always outperforms
indicated in Figure 2(a).
f
compute the low rank approximation of AT B from the samples using alternating least squares:

e
Step 3: Compute low rank approximation given estimates of few entries of AT B. Finally we

AT B
/
k
k

AT B
k

AT

f

M

−

−

−

B

e

e

e

k

±

min
U,V ∈Rn×r

wij(eT

i U V T ej −

M (i, j))2,

X(i,j)∈Ω

f

(3)

2We also tried using the cosine rule for computing the dot product, and another sketching method speciﬁcally designed for

preserving angles [4], but empirically those methods perform worse than our current estimator.

5

where wij = 1/ˆqij denotes the weights, and ei, ej are standard base vectors. This is a popular technique for
low rank recovery and matrix completion (see [3] and the references therein). After T iterations, we will get
a rank-r approximation of
M presented in the convenient factored form. This subroutine is quite standard,
so we defer the details to Appendix A.

f

3 Analysis

Now we present the main theoretical result. Theorem 3.1 characterizes the interaction between the sketch
[
AT Brk
size k, the sampling complexity m, the number of iterations T , and the spectral error
,
[
AT Br is the output of SMP-PCA, and (AT B)r is the optimal rank-r approximation of AT B. Note
where
= n2,
that the following theorem assumes that A and B have the same size. For the general case of n1 6
.
Theorem 3.1 is still valid by setting n = max
n1, n2}
{

(AT B)r −
k

Theorem 3.1. Given matrices A
kAk2
of AT B. Deﬁne ˜r = max
F
of (AT B)r, where σ∗
[
AT Br be the output of Algorithm SMP-PCA. If the input parameters k, m, and T satisfy

as the maximum stable rank, and ρ = σ∗
1
σ∗
r

}
i is the i-th singular values of AT B.

Rd×n, let (AT B)r be the optimal rank-r approximation
as the condition number

∈
kAk2 , kBk2
F
kBk2

Rd×n and B

Let

∈

{

(4)

(5)

(6)

(7)

2ρ2r3
k
2
F
k

·

k

≥

m

2
B
A
C1k
k
k
AT B
k
C2˜r2
γ

≥

·

max

˜r, 2 log(n)
}
{
η2

+ log (3/γ)

,

2

nr3ρ2 log(n)T 2
η2

,

·

2
2
B
F +
A
F
k
k
k
k
AT B
kF (cid:19)
k
kF +
A
ζ

log( k

≥

(cid:18)

T

B
k

kF

),

where C1 and C2 are some global constants independent of A and B. Then with probability at least 1
we have

−

γ,

(AT B)r −
k

[
AT Brk ≤

η

AT B
k

(AT B)rkF + ζ + ησ∗
r .

−

Remark 1. Compared to the two-pass algorithm proposed by [3], we notice that Eq. (7) contains an
additional error term ησ∗
r . This extra term captures the cost incurred when we are approximating entries of
AT B by Eq. (2) instead of using the actual values. The exact tradeoff between η and k is given by Eq. (4).
On one hand, we want to have a small k so that the sketched matrices can ﬁt into memory. On the other
hand, the parameter k controls how much information is lost during sketching, and a larger k gives a more
accurate estimation of the inner products.

kF is much smaller than

Remark 2. The dependence on kAk2
kF or
A
k

captures one difﬁcult situation for our algorithm.
If
AT B
kF , which could happen, e.g., when many column vectors
k
of A are orthogonal to those of B, then SMP-PCA requires many samples to work. This is reasonable.
Imagine that AT B is close to an identity matrix, then it may be hard to tell it from an all-zero matrix without
enough samples. Nevertheless, removing this dependence is an interesting direction for future research.

kAT BkF
B
k

F +kBk2
F

Remark 3. For the special case of A = B, SMP-PCA computes a rank-r approximation of ma-
trix AT A in a single pass. Theorem 3.1 provides an error bound in spectral norm for the residual matrix
[
(AT A)r −
AT Ar. Most results in the online PCA literature use Frobenius norm as performance measure.
Recently, [22] provides an online PCA algorithm with spectral norm guarantee. They achieves a spectral
norm bound of ǫσ∗
r+1, which is stronger than ours. However, their algorithm requires a target dimension

1 +σ∗

6

of O(r log n/ǫ2), i.e., the output is a matrix of size n-by-O(r log n/ǫ2), while the output of SMP-PCA is
simply n-by-r.

Remark 4. We defer our proofs to Appendix C. The proof proceeds in three steps. In Appendix C.2, we
show that the sampled matrix provides a good approximation of the actual matrix AT B. In Appendix C.3,
V and the
we show that there is a geometric decrease in the distance between the computed subspaces
optimal ones U ∗, V ∗ at each iteration of WAltMin algorithm. The spectral norm bound in Theorem 3.1 is
then proved using results from the previous two steps.

U ,

b

b

Computation Complexity. We now analyze the computation complexity of SMP-PCA. In Step 1, we
compute the sketched matrices of A and B, which requires O(nnz(A)k + nnz(B)k) ﬂops. Here nnz(
)
·
denotes the number of non-zero entries. The main job of Step 2 is to sample a set Ω and calculate the
corresponding inner products, which takes O(m log(n) + mk) ﬂops. Here we deﬁne n as max
for
simplicity. According to Eq. (4), we have log(n) = O(k), so Step 2 takes O(mk) ﬂops. In Step 3, we run
alternating least squares on the sampled matrix, which can be completed in O((mr2 + nr3)T ) ﬂops. Since
Eq. (5) indicates nr = O(m), the computation complexity of Step 3 is O(mr2T ). Therefore, SMP-PCA
has a total computation complexity O(nnz(A)k + nnz(B)k + mk + mr2T ).

n1, n2}
{

4 Numerical Experiments

Spark implementation. We implement our SMP-PCA in Apache Spark 1.6.2 [37]. For the purpose of
comparison, we also implement a two-pass algorithm LELA [3] in Spark3. The matrices A and B are
stored as a resilient distributed dataset (RDD) in disk (by setting its StorageLevel as DISK_ONLY). We
implement the two passes of LELA using the treeAggregate method. For SMP-PCA, we choose the
subsampled randomized Hadamard transform (SRHT) [32] as the sketching matrix 4. The biased sampling
procedure is performed using binary search (see Appendix C.5 for how to sample m elements in O(m log n)
time). After obtaining the sampled matrix, we run ALS (alternating least squares) to get the desired low-rank
matrices. More details can be found in [36].

Description of datasets. We test our algorithm on synthetic datasets and three real datasets: SIFT10K [20],

NIPS-BW [23], and URL-reputation [24]. For synthetic data, we generate matrices A and B as GD, where
G has entries independently drawn from standard Gaussian distribution, and D is a diagonal matrix with
Dii = 1/i. SIFT10K is a dataset of 10,000 images. Each image is represented by 128 features. We set A as
the image-by-feature matrix. The task here is to compute a low rank approximation of AT A, which is a stan-
dard PCA task. The NIPS-BW dataset contains bag-of-words features extracted from 1,500 NIPS papers.
We divide the papers into two subsets, and let A and B be the corresponding word-by-paper matrices, so
AT B computes the counts of co-occurred words between two sets of papers. The original URL-reputation
dataset has 2.4 million URLs. Each URL is represented by 3.2 million features, and is indicated as ma-
licious or benign. This dataset has been used previously for CCA [25]. Here we extract two subsets of
features, and let A and B be the corresponding URL-by-feature matrices. The goal is to compute a low rank
approximation of AT B, the cross-covariance matrix between two subsets of features.

Sample complexity. In Figure 4(a) we present simulation results on a small synthetic data with n = d =
5, 000 and r = 5. We observe that a phase transition occurs when the sample complexity m = Θ(nr log n).
This agrees with the experimental results shown in previous papers, see, e.g., [9, 3]. For the rest experiments
presented in this section, unless otherwise speciﬁed, we set r = 5, T = 10, and sampling complexity m as
4nr log n.

3To our best knowledge, this the ﬁrst distributed implementation of LELA.
4Compared to Gaussian sketch, SRHT reduces the runtime from O(ndk) to O(nd log d) and space cost from O(dk) to O(d),

while maintains the same quality of the output.

7

Runtime (sec) vs Cluster size

LELA
SMC-PCA

eAT eB)
SVD(
SMP-PCA
LELA
Optimal

SVD( eAT eB)
SMP-PCA
LELA
Optimal

3000

2000

1000

0.2

0.15

0.1

0.05

r
o
r
r
e
 
m
r
o
n
 
l
a
r
t
c
e
p
S

0

2

10

5

(a)

1000
Sketch size (k)

2000

1000
Sketch size (k)

2000

Figure 3: (a) Spark-1.6.2 running time on a 150GB dataset. All nodes are m.2xlarge EC2 instances. See [36]
for more details. (b) Spectral norm error achieved by three algorithms over two datasets: SIFT10K (left)
and NIPS-BW (right). We observe that SMP-PCA outperforms SVD(
B) by a factor of 1.8 for SIFT10K
and 1.1 for NIPS-BW. Besides, the error of SMP-PCA keeps decreasing as the sketch size k grows.

AT

0.3

0.25

0.2

0.15

0.1

0.05

(b)

e

e

Table 1: A comparison of spectral norm error over three datasets

Dataset

d

n

Algorithm Sketch size k

Error

Synthetic

100,000

100,000

URL-
malicious

URL-
benign

792,145

10,000

1,603,985

10,000

Optimal
LELA
SMP-PCA

Optimal
LELA
SMP-PCA

Optimal
LELA
SMP-PCA

-
-
2,000

-
-
2,000

-
-
2,000

0.0271
0.0274
0.0280

0.0163
0.0182
0.0188

0.0103
0.0105
0.0117

Comparison of SMP-PCA and LELA. We now compare the statistical performance of SMP-PCA
and LELA [3] on three real datasets and one synthetic dataset. As shown in Figure 3(b) and Table 1, LELA
always achieves a smaller spectral norm error than SMP-PCA, which makes sense because LELA takes
two passes and hence has more chances exploring the data. Besides, we observe that as the sketch size
increases, the error of SMP-PCA keeps decreasing and gets closer to that of LELA.

In Figure 3(a) we compare the runtime of SMP-PCA and LELA using a 150GB synthetic dataset on
m3.2xlarge Amazon EC2 instances5. The matrices A and B have dimension n = d = 100, 000. The sketch
dimension is set as k = 2, 000. We observe that the speedup achieved by SMP-PCA is more prominent for
small clusters (e.g., 56 mins versus 34 mins on a cluster of size two). This is possibly due to the increasing
spark overheads at larger clusters, see [17] for more related discussion.

Comparison of SMP-PCA and SVD(

AT

generating column vectors of A and B from a cone with angle θ. Here SVD(

B). In Figure 4(b) we repeat the experiment in Section 2 by
B) refers to computing

AT

5Each machine has 8 cores, 30GB memory, and 2×80GB SSD.

e

e

e

e

8

0.5

0.4

0.3

0.2

r
o
r
r
e
 
m
r
o
n
 
l
a
r
t
c
e
p
S

Ratio of errors vs theta

105

k = 400

k = 800

r Br

AT
SMP-PCA

 

r
o
r
r
e
m
r
o
n

 
l

a
r
t
c
e
p
S

1

0.8

0.6

0.4

0.2

2

1
# Samples / nrlogn

3

4

100

0

π/4

π/2 3π/4

π

200 400 600 800 1000
Sketch size (k)

(a)

(b)

(c)

Figure 4: (a) A phase transition occurs when the sample complexity m = Θ(nr log n). (b) This ﬁgure plots
the ratio of spectral norm error of SVD(
B) over that of SMP-PCA. The columns of A and B are unit
vectors drawn from a cone with angle θ. We see that the ratio of errors scales to inﬁnity as the cone angle
shrinks. (c) If the top r left singular vectors of A are orthogonal to those of B, the product AT
r Br is a very
poor low rank approximation of AT B.

AT

e

e

SVD on the sketched matrices6. We plot the ratio of the spectral norm error of SVD(
B) over that of
SMP-PCA, as a function of θ. Note that this is different from Figure 2(b), as now we take the effect of
random sampling and SVD into account. However, the trend in both ﬁgures are the same: SMP-PCA
B) and can be arbitrarily better as θ goes to zero.
always outperforms SVD(
AT
AT B
||
e
e

The y-axis represents spectral norm error, deﬁned as
approximation found by a speciﬁc algorithm. We observe that SMP-PCA outperforms SVD(
factor of 1.8 for SIFT10K and 1.1 for NIPS-BW.

B) on two real datasets SIFK10K and NIPS-BW.
[
AT Br is the rank-r
, where
AT
B) by a

AT
In Figure 3(b) we compare SMP-PCA and SVD(

[
AT B
AT Br||
/
||

AT

−

e

e

e

e

||

M is a better estimator for AT B than

Now we explain why SMP-PCA produces a more accurate result than SVD(

AT
B). The reasons are
e
AT
twofold. First, our rescaled JL embedding
B (Figure 2). Second,
e
the noise due to sampling is relatively small compared to the beneﬁt obtained from
M , and hence the ﬁnal
M ) still outperforms SVD(
result computed using PΩ(
e
e
f

r Br. Let Ar and Br be the optimal rank-r approximation of A and
B, we show that even if one could use existing methods (e.g., algorithms for streaming PCA) to estimate Ar
r Br can be a very poor low rank approximation of AT B. This is demonstrated in
and Br, their product AT
Figure 4(c), where we intentionally make the top r left singular vectors of A orthogonal to those of B.

Comparison of SMP-PCA and AT

B).

AT

f

f

e

e

e

e

5 Conclusion

We develop a novel one-pass algorithm SMP-PCA that directly computes a low rank approximation of a
matrix product, using ideas of matrix sketching and entrywise sampling. As a subroutine of our algorithm,
we propose rescaled JL for estimating entries of AT B, which has smaller error compared to the standard
estimator ˜AT ˜B. This we believe can be extended to other applications. Moreover, SMP-PCA allows the
non-zero entries of A and B to be presented in any arbitrary order, and hence can be used for steaming
applications. We design a distributed implementation for SMP-PCA. Our experimental results show that

6This can be done by standard power iteration based method, without explicitly forming the product matrix eAT eB, whose size

is too big to ﬁt into memory according to our assumption.

9

SMP-PCA can perform arbitrarily better than SVD(
rithms that require two or more passes over the data.

AT

B), and is signiﬁcantly faster compared to algo-

Acknowledgements We thank the anonymous reviewers for their valuable comments. This research has
e
been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000, and ARO
YIP W911NF-14-1-0258.

e

10

References

[1] D. Achlioptas and F. McSherry. Fast computation of low rank matrix approximations. In Proceedings
of the thirty-third annual ACM symposium on Theory of computing, pages 611–618. ACM, 2001.

[2] A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental pca. In Advances

in Neural Information Processing Systems, pages 3174–3182, 2013.

[3] S. Bhojanapalli, P. Jain, and S. Sanghavi. Tighter low-rank approximation via sampling the leveraged
element. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 902–920. SIAM, 2015.

[4] P. T. Boufounos. Angle-preserving quantized phase embeddings.
Applications. International Society for Optics and Photonics, 2013.

In SPIE Optical Engineering+

[5] C. Boutsidis, D. Garber, Z. Karnin, and E. Liberty. Online principal components analysis. In Pro-
ceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887–901.
SIAM, 2015.

[6] C. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized hadamard

transform. SIAM Journal on Matrix Analysis and Applications, 34(3):1301–1340, 2013.

[7] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Compu-

tational mathematics, 9(6):717–772, 2009.

[8] X. Chen, H. Liu, and J. G. Carbonell. Structured sparse canonical correlation analysis. In International

Conference on Artiﬁcial Intelligence and Statistics, pages 199–207, 2012.

[9] Y. Chen, S. Bhojanapalli, S. Sanghavi, and R. Ward. Completing any low-rank matrix, provably. arXiv

preprint arXiv:1306.2979, 2013.

[10] K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In Proceedings
of the forty-ﬁrst annual ACM symposium on Theory of computing, pages 205–214. ACM, 2009.

[11] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pages 81–90.
ACM, 2013.

[12] M. B. Cohen, J. Nelson, and D. P. Woodruff. Optimal approximate matrix product in terms of stable

rank. arXiv preprint arXiv:1507.02268, 2015.

[13] A. Deshpande and S. Vempala. Adaptive sampling and fast low-rank matrix approximation. In Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 292–
303. Springer, 2006.

[14] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices ii: Computing a

low-rank approximation to a matrix. SIAM Journal on Computing, 36(1):158–183, 2006.

[15] P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-error matrix
approximation: Column-based methods. In Approximation, Randomization, and Combinatorial Opti-
mization. Algorithms and Techniques, pages 316–326. Springer, 2006.

[16] A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo algorithms for ﬁnding low-rank approxima-

tions. Journal of the ACM (JACM), 51(6):1025–1041, 2004.

11

[17] A. Gittens, A. Devarakonda, E. Racah, M. F. Ringenburg, L. Gerhardt, J. Kottalam, J. Liu, K. J.
Maschhoff, S. Canon, J. Chhugani, P. Sharma, J. Yang, J. Demmel, J. Harrell, V. Krishnamurthy,
M. W. Mahoney, and Prabhat. Matrix factorization at scale: a comparison of scientiﬁc data analytics
in spark and C+MPI using three case studies. arXiv preprint arXiv:1607.01335, 2016.

[18] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algo-
rithms for constructing approximate matrix decompositions. SIAM review, 53(2):217–288, 2011.

[19] S. Har-Peled. Low rank matrix approximation in linear time. Manuscript. http://valis. cs. uiuc.

edu/sariel/papers/05/lrank/lrank. pdf, 2006.

[20] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. Pattern Analysis

and Machine Intelligence, IEEE Transactions on, 33(1):117–128, 2011.

[21] R. Kannan, S. S. Vempala, and D. P. Woodruff. Principal component analysis and higher correlations
for distributed data. In Proceedings of The 27th Conference on Learning Theory, pages 1040–1057,
2014.

[22] Z. Karnin and E. Liberty. Online pca with spectral bounds. In Proceedings of The 28th Conference on

Learning Theory (COLT), volume 40, pages 1129–1140, 2015.

[23] M. Lichman. UCI machine learning repository. http://archive.ics.uci.edu/ml, 2013.

[24] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Identifying suspicious urls: an application of large-
scale online learning. In Proceedings of the 26th annual international conference on machine learning,
pages 681–688. ACM, 2009.

[25] Z. Ma, Y. Lu, and D. Foster. Finding linear structure in large datasets with scalable canonical correla-

tion analysis. arXiv preprint arXiv:1506.08170, 2015.

[26] A. Magen and A. Zouzias. Low rank matrix-valued chernoff bounds and approximate matrix multipli-
cation. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,
pages 1422–1436. SIAM, 2011.

[27] I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited, streaming pca.

In Advances in Neural

Information Processing Systems, pages 2886–2894, 2013.

[28] N. H. Nguyen, T. T. Do, and T. D. Tran. A fast and efﬁcient algorithm for low-rank approximation of
a matrix. In Proceedings of the 41st annual ACM symposium on Theory of computing, pages 215–224.
ACM, 2009.

[29] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In Founda-
tions of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pages 143–152. IEEE,
2006.

[30] O. Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In Proceedings
of the 32nd International Conference on Machine Learning (ICML-15), pages 144–152, 2015.

[31] T. Tao. 254a, notes 3a: Eigenvalues and sums of hermitian matrices. Terence Tao’s blog, 2010.

[32] J. A. Tropp.

Improved analysis of the subsampled randomized hadamard transform. Advances in

Adaptive Data Analysis, pages 115–126, 2011.

12

[33] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational

Mathematics, 12(4):389–434, 2012.

[34] D. P. Woodruff. Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357,

2014.

[35] F. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. A fast randomized algorithm for the approximation

of matrices. Applied and Computational Harmonic Analysis, 25(3):335–366, 2008.

[36] S. Wu, S. Bhojanapalli, S. Sanghavi, and A. Dimakis. Github repository for ”single-pass pca of matrix

products”. https://github.com/wushanshan/MatrixProductPCA, 2016.

[37] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and
I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.
In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,
2012.

13

A Weighted alternating minimization

Algorithm 2 provides a detailed explanation of WAltMin, which follows a standard procedure for matrix
PΩ(A) to denote the Hadamard product between w and PΩ(A):
completion. We use RΩ(A) = w.
Rn1×n2 = 1/ˆqij is
RΩ(A)(i, j) = w(i, j)
the weight matrix. Similarly we deﬁne the matrix R1/2
PΩ(A)(i, j) for
(i, j)

Ω and 0 otherwise, where w
Ω (A) as R1/2

∗
PΩ(A)(i, j) for (i, j)

Ω and 0 otherwise.

Ω (A)(i, j) =

∈
w(i, j)

∈

∗

∗

The algorithm contains two parts: initialization (Step 2-6) and weighted alternating minimization (Step
M ) and then set row i of U (0)
7-10). In the ﬁrst part, we compute SVD of the weighted sampled matrix RΩ(
to be zero if its norm is larger than a threshold (Step 6). More details of this trim step can be found in [3].
In the second part, the goal is to solve the following non-convex problem by alternating minimization:

f

p

∈

min
U,V

wij(eT

i U V T ej −

M (i, j))2,

X(i,j)∈Ω

f

(8)

where ei, ej are standard base vectors. After running T iterations, the algorithm outputs a rank-r approxi-
mation of

M presented in the convenient factored form.

Ω0, . . . , Ω2T }
{

Algorithm 2 WAltMin [3]

f

∗

∀

∈

i, j

M )

M )

PΩ0(

Rn1×n2, Ω, r, ˆq, and T

f
M ) = w.

1: Input: PΩ(
2: wij = 1/ˆqij when ˆqij > 0, 0 else,
3: Divide Ω in 2T + 1 equal uniformly random subsets, i.e., Ω =
4: RΩ0(
5: U (0)Σ(0)(V (0))T = SVD(RΩ0(
6: Trim U (0) and let
f
7: for t = 0 to T
8:

U (0) be the output
1 do
V (t+1) = argminV k
b
U (t+1) = argminU k
9:
b
10: end for
U (T )
11: Output:
b

2
U (t)V T )
F
k
2
V (t+1))T )
F
k

U (
b
Rn2×r.
b

M
f
V (T )
f

Ω2t+1(
Ω2t+2(

R1/2
R1/2

Rn1×r and

M ), r)

f
M

f

−

−

−

∈

b

∈

b

B Technical Lemmas

We will frequently use the following concentration inequality in the proof.

Lemma B.1. (Matrix Bernstein’s Inequality [33]). Consider p independent random matrices X1, ...., Xp in
Rn×n, where each matrix has bounded deviation from its mean:

Xi −
||
Let the norm of the covariance matrix be

E[Xi]

L,

|| ≤

i.

∀

p

(Xi −

σ2 = max

E

(cid:12)
((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Then the following holds for all t
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi=1

"

E[Xi])T

(Xi −

E[Xi])T (Xi −

E[Xi])

p

"
Xi=1

,

E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2n exp( −

t2/2
σ2 + Lt/3

).

E[Xi])(Xi −
0:

≥
p

(Xi −

P

(cid:12)
"(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[Xi])
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

# ≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
14

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

A formal deﬁnition of JL transform is given below [29][34].

Deﬁnition B.2. A random matrix Π
for short, if with probability at least 1

Πv, Πv′

v, v′

i − h

i| ≤

ǫ

v
||

|| · ||

v′

.
||

|h

∈
−

Rk×d forms a JL transform with parameters ǫ, δ, f or JLT(ǫ, δ, f )
V it holds that
δ, for any f -element subset V

Rd, for all v, v′

⊂

∈

The following lemma [34] characterizes the tradeoff between the reduced dimension k and the error

level ǫ.

N

Lemma B.3. Let 0 < ǫ, δ < 1, and Π

Rk×d be a random matrix where the entries Π(i, j) are i.i.d.

(0, 1/k) random variables. If k = Ω(log(f /δ)ǫ−2), then Π is a JLT(ǫ, δ, f ).

∈

We now present two lemmas that connect

A

Rk×n and

B

Rd×n with A

Rd×n and B

Rd×n.

∈
∈
∈
Lemma B.4. Let 0 < ǫ, δ < 1, if k = Ω( log(2n/δ)
), then with probability at least 1
e

ǫ2

e

δ,

−

∈

(1

ǫ)

A

||

2
F ≤ ||
||

A

2
F ≤
||

−

(1 + ǫ)

A

2
F ,
||
||
AT B

(1

ǫ)

B

||

−
ǫ

A

2
F ≤ ||
||
||F .
B

AT
||

B

||F ≤
Proof. This is again a standard result of JL transformation, e.g., see Deﬁnition 2.3 and Theorem 2.1 of [34]
and Lemma 6 of [29] .

||F ||

−

e

e

e

e

||

B

2
F ≤
||

(1 + ǫ)

B

2
F ,
||

||

Lemma B.5. Let 0 < ǫ, δ < 1, if k = Θ( ˜r+log(1/δ)
rank, then with probability at least 1

δ,

ǫ2

−

), where ˜r = max

is the maximum stable

||A||2
F

||A||2 , ||B||2

F
||B||2

{

}

Proof. This follows from a recent paper [12].
e

e

AT
||

B

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

Using the above two lemmas, we can prove the following two lemmas that relate

AT
M is DA
deﬁned in Algorithm 1. A more compact deﬁnition of
.
matrices with (DA)ii =
Bj ||
/
Bj||
Ai||
/
Ai||
||
||
||
||
f
e
Lemma B.6. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( log(2n/δ)
e
e

and (DB)jj =

ǫ2

e

M with AT B, for

M
BDB, where DA and DB are diagonal
f
f

), then with probability at least 1

δ,

−

AT
i Bj| ≤

ǫ

Ai|| · ||
||

,
Bj||

M

||

−

AT B

||F ≤

ǫ

A

||

||F ||

B

||F .

Mij −
|
f

Proof. Let 0 < ǫ < 1/2, 0 < δ < 1, according to the Deﬁnition B.2 and Lemma B.3, we have that if
k = Ω( log(2n/δ)

), then with probability at least 1

δ, and for all i, j

f

ǫ2

1 + ǫ,

1

ǫ

−

≤

AT
i Bj|

as

−
(DB)jj ≤

1 + ǫ,

AT
i
|

e

Bj −
e

AT
i Bj| ≤

ǫ

Ai||||
||

.
Bj||

(9)

We can now bound

Mij −
|
AT
i
f
|

ǫ

1

≤

−

(DA)ii ≤
Mij −
|
AT
i Bj|
f
Bj(DA)ii(DB)jj −
Bj(1 + ǫ)2
e
{|
(1 + ǫ)2ǫ
e
e
{
Ai||||
||

,
Bj ||

AT
i

−
Bj||
Ai||||
||

max
e

max

7ǫ

ξ1=
ξ2

≤
ξ3

≤
ξ4

≤

AT
i Bj|
AT
AT
,
i Bj|
i
|
+ ((1 + ǫ)2
e

e

Bj(1

ǫ)2

−

−
AT
i Bj|
1)
|

AT
i Bj|}
, (1

−

−

15

ǫ)2ǫ

Ai||||
||

Bj||

+ (1

(1

−

−

AT
ǫ)2)
i Bj|}
|
(10)

Mij, ξ2 follows from the bound in Eq.(9), ξ3 follows from triangle
. Now rescaling ǫ as ǫ/7 gives the desired

where ξ1 follows from the deﬁnition of
inequality and Eq.(9), and ξ4 follows from
bound in Lemma B.6.
f
Mij −
f

||F =

Hence,

AT B

ij |

M

−

||

qP
Lemma B.7. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( ˜r+log(n/δ)

f

qP
ǫ2

AT
i Bj| ≤ ||
|
AT
i Bj|

2

≤

Ai||||

Bj ||
ij ǫ2

Proof. We can bound the spectral norm of the difference matrix as follows:

f

M

||

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

2

Ai||
||

Bj||
||

2 = ǫ

A

||F ||

B

||F .

||

), then with probability at least 1

δ,

−

M

||

−

AT B

ξ1=

||

f

AT
DA
BDB −
||
AT
DA||||
B
−
e
e
(1 + ǫ)2ǫ
e
||
B
A
7ǫ

A
e
||||
,
||

||||

||

B

≤ ||
ξ3

≤

≤

||

DAAT BDB + DAAT BDB −
AT B
DA||||
+
||||
||
B

DB||
+ (1 + ǫ)ǫ

AT B

+ ǫ

||||

A

A

DAAT B + DAAT B
I
DA −
+
DB −
||
B

||

I

||

||||

||

||

||||

||

AT B
||
AT B

||

−

||||

(11)

Mij, and ξ2 follows from Lemma B.5 and bound in Eq.(9). Rescaling

where ξ1 follows from the deﬁnition of
ǫ as ǫ/7 gives the desired bound in Lemma B.7.

f

We will frequently use the term with high probability. Here is a formal deﬁnition.

Deﬁnition B.8. We say that an event E occurs with high probability (w.h.p.) in n if the probability that its
complement ¯E happens is polynomially small, i.e., P r( ¯E) = O( 1

nα ) for some constant α > 0.

The following two lemmas deﬁne a ”nice” Π and when this happens with high probability.

Deﬁnition B.9. The random Gaussian matrix Π is ”nice” with parameter ǫ if for all (i, j) such that qij ≤
(i.e., qij = ˆqij), the sketched values

Mij satisﬁes the following two inequalities:

1

Mij|
|
ˆqij ≤
f

(1 + ǫ)

n
m

f
2
F +
||

A

(
||

B

2
F ),
||

||

X{j:ˆqij=qij } f

M 2
ij
ˆqij ≤

(1 + ǫ)

2n
m

A

(
||

2
F +
||

B

F )2.
2
||

||

Lemma B.10. If k = Ω( log(n)
w.h.p. in n.

ǫ2

), and 0 < ǫ < 1/14, then the random Gaussian matrix Π

Proof. According to Lemma B.6, if k = Ω( log(n)
ǫ

ǫ2

. In other words, the following holds with probability at least 1

δ:

), then w.h.p. in n, for all (i, j) we have

Ai|| · ||
||

Bj||

−

Mij| ≤ |
|
The above inequality is sufﬁcient for Π to be ”nice”:
f

Ai|| · ||
||

+ ǫ

AT
i Bj|

Bj|| ≤

(1 + ǫ)

Ai|| · ||
||

,
Bj||

(i, j)

∀

Rk×d is ”nice”

∈

AT
i Bj| ≤

Mij −
|
f

Mij
ˆqij ≤
f

(1 + ǫ) ||

Ai|| · ||
ˆqij

Bj||

≤

(1 + ǫ)

2 +
Ai||
(
||
( ||Ai||2
2n||A||2
F

2)/2
Bj||
||
+ ||Bj||2
2n||B||2
F

m

·

) ≤

(1 + ǫ)

n
m

A

(
||

2
F +
||

B

2
F )
||

||

16

M 2
ij
ˆqij ≤

(1 + ǫ)2

2

2

Bj||
||

Ai||
||
ˆqij

X{j:ˆqij=qij} f

X{j:ˆqij=qij}

4 +
Ai||
||
( ||Ai||2
2n||A||2
F

4

Bj||
||
+ ||Bj||2
2n||B||2
F

)

m

·

X{j:ˆqij=qij}
2n
2
F +
A
(
m
||
||

B

F )2.
2
||

||

(1 + ǫ)

(1 + ǫ)

≤

≤

ǫ2

Therefore, we conclude that if k = Ω( log(n)

), then Π is ”nice” w.h.p. in n.

C Proofs

C.1 Proof overview

We now present the key steps in proving Theorem 3.1. The framework is similar to that of LELA [3].

Our proof proceeds in three steps. In the ﬁrst step, we show that the sampled matrix provides a good
approximation of the actual matrix AT B. The result is summarized in Lemma C.1. Here RΩ(
M ) denotes
the sampled matrix weighted by the inverse of sampling probability (see Line 4 of Algorithm 2). Detailed
proof can be found in Appendix C.2. For consistency, we will use Ci (i = 1, 2, ...) to denote global constant
that can vary from step to step.

f

Lemma C.1. (Initialization) Let m and k satisfy the following conditions for sufﬁciently large constants C1
and C2:

then the following holds w.h.p. in n:

m

C1

≥

k

≥

||

A

B

2
2
F +
F
||
||
||
AT B
||F (cid:19)
||
2
A
||
||
AT B

˜r + log(n)
δ2

·

2 n
δ2 log(n),
B

,

(cid:18)

C2

2
||
2
F
||

||
||

RΩ(

M )

||

AT B

δ

AT B
||

||F .

|| ≤

−

In the second step, we show that at each iteration of WAltMin algorithm, there is a geometric decrease
V and the optimal ones U ∗, V ∗. The result is shown
in the distance between the computed subspaces
in Lemma C.2. Appendix C.3 provides the detailed proof. Here for any two orthonormal matrices X and
, where X⊥
Y , we deﬁne their distance as the principal angle based distance, i.e., dist(X, Y ) =
b
||
denotes the subspace orthogonal to X.

X T
||

⊥Y

U ,

f

b

Lemma C.2. (WAltMin Descent) Let k, m, and T satisfy the conditions stated in Theorem 3.1. Also,
(AT B)r||F . Let ˆU (t) and ˆV (t+1) be the t-th and
1
consider the case when
576ρr1.5 ||
(t+1)-th step iterates of the WAltMin procedure. Let U (t) and V (t+1) be the corresponding orthonormal
(U (t))i
1/2. Denote AT B as M , then the
||F and dist(U (t), U ∗)
matrices. Let
/
Ai||
A
||
||
||
γ/T :
following holds with probability at least 1

(AT B)r||F ≤

AT B
||

8√rρ

|| ≤

≤

−

dist(V t+1, V ∗)

dist(U t, U ∗) + η

M

Mr||F /σ∗

r + η,

−

||

−
1
2

≤

(V (t+1))j
||

|| ≤

8√rρ

/
Bj||
||
||

B

||F .

17

In the third step, we prove the spectral norm bound in Theorem 3.1 using results from the above two
lemmas. Comparing Lemma C.1 and C.2 with their counterparts of LELA (see Lemma C.2 and C.3 in [3]),
we notice that Lemma C.1 has the same bound as that of LELA, but the bound in Lemma C.2 contains
an extra term η. This term eventually leads to an additive error term ησ∗
r in Eq.(7). Detailed proof is in
Appendix C.4.

C.2 Proof of Lemma C.1

We ﬁrst prove the following lemma, which shows that RΩ(
we deﬁne CAB := (||A||2

F )2

.

F +||B||2
||AT B||2
F

Lemma C.3. Suppose Π is ﬁxed and is ”nice”. Let m
constant C1, then w.h.p. in n, the following is true:

M ) is close to

M . For simplicity of presentation,

f
C1 ·

≥

f

CAB

n
δ2 log(n) for sufﬁciently large global

RΩ(

M )

M

−

|| ≤

||

δ

AT B
||

||F .

Proof. This lemma can be proved in the same way as the proof of Lemma C.2 in [3]. The key idea is to use
f
the matrix Bernstein inequality. Let Xij = (δij −
j , where δij is a
random variable
ˆqij)wij
n
indicating whether the value at (i, j) has been sampled. Since Π is ﬁxed,
i,j=1 are independent zero
mean random matrices. Furthermore,

Xij}
{

0, 1
}
{

MijeieT

f

M )

M .

f
n
i,j=1 = RΩ(

−

Since Π is ”nice” with parameter 0 < ǫ < 1/14, we can bound the 1st and 2nd moment of Xij as

f

f

Xij}

i,j{

P

follows:

Xij ||
||

= max

(1

ˆqij)wij

{|

−

ˆqijwij
|

,
Mij|
f

ξ1

≤

Mij|
|
ˆqij
f

Mij|} ≤
f

(1 + ǫ)

n
m

A

(
||

2
F +
||

B

2
F );
||

||

σ2 = max

XijX T

ij 

E

,


Xij


E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
ˆqij −

1)

{(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(
|

= max

i



(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Xij
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

M 2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ij
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆqij
X{j:ˆqij=qij } f

M 2
ij |

ξ3

≤

f

ξ2= max
i

ˆqij(1

−

ˆqij)w2
ij

X T
ij Xij
(cid:12)
(cid:12)
}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(1 + ǫ)

ξ1

≤

(cid:12)
(cid:12)
Xj
(cid:12)
(cid:12)
(cid:12)
2
(cid:12)
F +
||

2n
m

A

(
||

B

F )2,
2
||

||

M 2
ij(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f

1. Now we can use matrix Bernstein inequality (see Lemma B.1) with t = δ

where ξ1 follows from Lemma B.10, ξ2 follows from a direct calculation, and ξ3 follows from the fact that
||F to show that
ˆqij ≤
n
δ2 log(n), then the desired inequality holds w.h.p. in n, where C1 is some global
if m
≥
constant independent of A and B. Note that since 0 < ǫ < 1/14, (1 + ǫ) < 2. Rescaling C1 gives the
desired result.

(1 + ǫ)C1CAB

AT B
||

Now we are ready to prove Lemma C.1, which is a counterpart of Lemma C.2 in [3].

Proof. We ﬁrst show that
||F holds w.h.p. in n over the randomness of Π. Note
that in Lemma C.3, we have shown that it is true for a ﬁxed and ”nice” Π, now we want to show that it also
holds w.h.p. in n even for a random chosen Π.

RΩ(

|| ≤

M )

f

f

M

−

||

δ

AT B
||

Let G be the event that we desire, i.e., G =

AT B
||
complimentary event. By conditioning on Π, we can bound the probability of ¯G as

RΩ(

|| ≤

AT

AT

B)

{||

−

B

δ

||F }

. Let ¯G be the

P r( ¯G) = P r( ¯G
P r( ¯G

≤

e
Π is ”nice”)P r(Π is ”nice”) + P r( ¯G
Π is not ”nice”)P r(Π is not ”nice”)
|
|
Π is ”nice”) + P r(Π is not ”nice”).
|

e

e

e

18

n
δ2 log(n), and k

, then both
CAB
≥
in n. Therefore, the the probability of ¯G is

log(n)
ǫ2

C2

According to Lemma C.3 and Lemma B.10, if m
events
polynomially small in n, i.e., the desired event G happens w.h.p. in n.
AT B

C1 ·
and P r(Π is ”nice”) happen w.h.p.

Π is ”nice”
G
|
{

Next we show that

M

≥

}

||
), then w.h.p. in n, we have

−

Θ( ˜r+log(n)
ǫ2
k = Θ( ˜r+log(n)

δ2

f
||A||2||B||2
||AT B||2
F

), then

M

||
By triangle inequality, we have

·

−
shown that w.h.p. in n, both terms are less than δ
inequality
−
statement of Lemma C.1.

AT B
||

AT B

RΩ(

|| ≤

AT

B)

||

f
||

δ

|| ≤
||

B

AT B

AT B
δ
||
M
−
AT B
−
f
RΩ(

||F holds w.h.p. in n. According to Lemma B.7, if k =
. Now let ǫ := δ ||AT B||F
||A||||B|| , we have that if
|| ≤
||
δ

A
ǫ
||
||||
AT B
||F holds w.h.p. in n.
||
AT B
. We have
+
M )
|| ≤ ||
−
||
||
AT B
||F . By rescaling δ as δ/2, we have that the desired
||
||F holds w.h.p. in n, when m and k are chosen according to the
f
f

AT B

RΩ(

|| ≤

M )

f

f

M

M

−

||

e

e

Because the bound of Lemma C.1 has the same form as that of Lemma C.2 in [3], the corollary
(AT B)r||F ≤

M ), which is stated here without proof:

AT B
||

−

if

of Lemma C.2 also holds for RΩ(
1
576κr1.5 ||

(AT B)r||F , then w.h.p. in n we have
U (0))i
/
Ai||
(
||
||
||

f
8√r

|| ≤

A

||F

and dist(

U (0), U ∗)

1/2,

≤

U (0) is the initial iterate produced by the WAltMin algorithm (see Step 6 of Algorithm 2). This

b

where
b
corollary will be used in the proof of Lemma C.2.

b

(AT B)r||F ≥
AT B
Similar to the original proof in [3], we can now consider two cases separately: (1)
||
1
(AT B)r||F . The ﬁrst case is simple: use
(AT B)r||F ≤
576ρr1.5 ||
Lemma C.1 and Wely’s inequality [31] already implies the desired bound in Theorem 3.1. To see why,
note that Lemma C.1 and Wely’s inequality imply that

(AT B)r||F ; (2)

1
576ρr1.5 ||

AT B
||

−

−

(AT B)r −
||
AT B

−

AT B

−
AT B
2
||

−

(RΩ(
M )r||
(AT B)r||
+
f
(AT B)r||
(AT B)r||

+ δ

ξ1

≤ ||
ξ2

≤ ||
ξ3

≤

RΩ(

M )

+

RΩ(

M )

||

AT B
||

−
||
AT B
||F +
RΩ(
f
||
||
||F ,

AT B
||

+ 2δ

f

M )

AT B
f

||

−

−

(RΩ(

M ))r||
AT B
f
||

−

+

(AT B)r||

(12)

where Mr denotes the best rank-r approximation of M , ξ1 follows triangle inequality, ξ2 follows from
Lemma C.1 and Wely’s inequality, and ξ3 follows from Lemma C.1. If
(AT B)r||F . Setting δ =
(AT B)r||F ≤
then
O(η/(ρr1.5)) in Eq.(12) gives the desired error bound in Theorem 3.1. Therefore, in the following analysis
we only need to consider the second case.

AT B
−
||
AT B
O(ρr1.5)
||

(AT B)r||F ≥
−

(AT B)r||F +
||

1
576ρr1.5 ||

AT B
||

AT B
||

||F =

−

(AT B)r||F ,

C.3 Proof of Lemma C.2

We ﬁrst prove the following lemma, which is a counterpart ofLemma C.5 in [3]. For simplicity of presenta-
tion, we use M to denote AT B in the following proof.

Lemma C.4. If m
C1 and C2, then the following holds with probability at least 1

C1nr log(n)T /(γδ2) and k

≥

≥

C2(˜r+log(n))/ǫ2 for sufﬁciently large global constants

(U (t))T RΩ(
||

M

−

Mr)

−

(U (t))T (M

Mr)

δ

M

|| ≤

||

−

−

A

||F ||

B

||F + ǫ

||

A

||

B

.
||

||||

γ/T :

−
Mr||F + δǫ

f

19

Proof. For a ﬁxed Π, we have that if m
least 1

γ/T :

≥

−

C1nr log(n)T /(γδ2), then following holds with probability at

(U (t))T RΩ(
||

M

−

Mr)

−

(U (t))T (

M

Mr)

δ

M

|| ≤

||

Mr||F .

−

−

(13)

The proof of Eq.(13) is exactly the same as the proof of Lemma C.5/B.6/B.2 in [3], so we omit its details
here. The key idea is to deﬁne a set of zero-mean random matrices Xij such that
Mr)
desired bound.

−
Mr), and then use second moment-based matrix Chebyshev inequality to obtain the

ij Xij = (U (t))T RΩ(

(U (t))T (

P

f

f

f

f

M

M

−

−

According to Lemma B.6 and Lemma B.7, if k = Θ((˜r + log(n))/ǫ2), then w.h.p. in n, the following

f

holds:

M

||

−

AT B

||F ≤

ǫ

A

||

||F ||

B

||F ,

M

||

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

(14)

Using triangle inequality, we have that if m and k satisfy the conditions of Lemma C.4, then the follow-

ing holds with probability at least 1

γ/T :

f

f

−

−

(U (t))T RΩ(
||
(U (t))T RΩ(

M

≤ ||
ξ1

δ

δ

δ

M

||

M
f

||

M

||

−

−

−

≤

≤
ξ2

≤

M
f
−
Mr||F +
f
||
Mr||F + δ
||
Mr||F + δǫ

Mr)

Mr)

−

−

(U (t))T (M
(U (t))T (

M

Mr)

||
Mr)

||

−

−

f

M

M

M

−

−

||
||F +
M
||
f
||F + ǫ
B
f
||F ||

M

A

||

M

||

−

A

B
f

,
||

||||

||

+

(U (t))T (M
||

−

M )

||

f

where ξ1 follows from Eq.(13), and ξ2 follows from Eq.(14).

Now we are ready to prove Lemma C.2. For simplicity, we focus on the rank-1 case here. Rank-
r proof follows a similar line of reasoning and can be obtained by combining the current proof with the
rank-r analysis in the original proof of LELA [3]. Note that compared to Lemma C.5 in [3], Lemma C.4
contains two extra terms δǫ
. Therefore, we need to be careful for steps that involve
Lemma C.4.

||F + ǫ

||F ||

||||

B

B

A

A

||

||

||

In the rank-1 case, we use ˆut and ˆvt+1 to denote the t-th and (t+1)-th step iterates (which are column

vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized vectors.

B

||F , for some constant c1.
Bj||
/
||
Bounding dist(vt+1, v∗):
In Lemma C.4, set ǫ = ||AT B||
AT B

η

Proof. This proof contains two parts. In the ﬁrst part, we will prove that the distance dist(vt+1, v∗) de-
creases geometrically over time. In the second part, we show that the j-th entry of vt+1 satisﬁes
c1||

vt+1
j
|

| ≤

2||A||||B||η and δ = η
2˜r , where 0 < η < 1, then we have δǫ
AT B
/2, and ǫ
B
||
γ/T , the following holds:

||F ≤
/2. Therefore, with probability at least

AT B
||

η2
2˜r ||

||F ||

|| ≤

|| ≤

||||

B

A

A

||

||

||

||

η

·

||A||F ||B||F
||A||||B||

1

−

M

M1)

(ut)T (M

M1)

η

M

(ut)T RΩ(
||
(ut)T RΩ(
||

Hence, we have

−

−

−
dist(ut, u∗)
||
Using the explicit formula for WAltMin update (see Eq.(46) and Eq.(47) in [3]), we can bound
ˆvt+1, v∗
h

M1||F /˜r + ησ∗
1.

||
M1||

as follows.

f
−

M1)

|| ≤

|| ≤

+ η

⊥i

f

M

M

−

−

−

||

and

M1||F /˜r + ησ∗
1.
M

(15)

ˆvt+1, v∗
h

i

ˆut
||

||h

ˆvt+1, v∗

/σ∗
i

1 ≥ h

ut, u∗

i −

1

δ1

1

− h

ut, u∗

2
i

−

1

δ1

1

−

M

(η ||

M1||F

−
˜rσ∗
1

+ η).

δ1

−

p

20

ˆut
||

ˆvt+1, v∗
⊥i

||h

/σ∗

1 ≤

1

1

− h

ut, u∗

2 +
i

1

δ1

−

δ1

p

1

−

δ1

(dist(ut, u∗) ||

M

M1||

+ η ||

M

−
σ∗
1

As discussed in the end of Appendix C.2, we only need to consider the case when
1
576ρr1.5

(AT B)r||F , where ρ = σ∗
||

AT B
−
||
r . In the rank-1 case, this condition reduces to
M
−
||
1
ut, u∗
20 ), and use the fact that
h
ˆvt+1, v∗
as
h

1
20 , η
ˆvt+1, v∗
h

For sufﬁciently small constants δ1 and η (e.g., δ1 ≤
and dist(u0, u∗)

1/2, we can further bound

1/σ∗

≤
and

⊥i

≤

i

M1||F

+ η).

−
˜rσ∗
1
(AT B)r||F ≤
σ∗
576 .
M1||F ≤
u0, u∗
i ≥ h

i

ˆut
||

||h

ˆvt+1, v∗

/σ∗
i

1 ≥ h

u0, u∗

i −

1

− h

u0, u∗

2
i

−

1
10 ≥

√3
2 −

2
10 ≥

1
2

.

ˆut
||

ˆvt+1, v∗
⊥i

||h

/σ∗

δ1

dist(ut, u∗) +

dist(ut, u∗) +

1 ≤
ξ1

≤

1
1
4

δ1

−
dist(ut, u∗) + 2(η

δ1)
−
M1||F /σ∗
1 and the assumption that δ1 is sufﬁciently small.

1 + η),

M

−

−

||

1

where ξ1 uses the fact that ˜r

Now we are ready to bound dist(vt+1, v∗) as

≥

M

(η ||

1

δ1

M1||F

−
˜rσ∗
1

+ η)

1
10

p

1
576(1

dist(vt+1, v∗) =

1

vt+1, v∗

− h

2 =
i

ˆvt+1, v∗
h
2 +

ˆvt+1, v∗
h
p
M
||

⊥i
Mr||F /σ∗

−

⊥i
ˆvt+1, v∗
h
1 + η),

p
1
2

ξ1

≤

dist(ut, u∗) + 4(η

ˆvt+1, v∗
⊥i
h
ˆvt+1, v∗
i
h

2 ≤
i

where ξ1 follows from substituting Eqs. (16) and (17). Rescaling η as η/4 gives the desired bound of
Lemma C.2 for the rank-1 case. Rank-r proof can be obtained by following a similar framework.

Bounding vt+1
In this step, we need to prove that the j-th entry of vt+1 satisﬁes

:

j

vt+1
j
|

| ≤

c1

||Bj||
||B||F

for all j, under the

assumption that ut satisﬁes the norm bound

||Ai||
||A||F
The proof follows very closely to the second part of proving Lemma C.3 in [3], except that an extra
Mij using Bernstein inequality. More
Mij. Note that if ˆqij = 1, then δij = 1, Xi = 0, so we only need to

multiplicative term (1 + ǫ) will show up when bounding
ˆqij)wijut
speciﬁcally, let Xi = (δij −
i
consider the case when ˆqij < 1, i.e., ˆqij = qij, where qij is deﬁned in Eq.(1).

i δijwijut
i

ut
i| ≤
|

for all i.

P

f

c1

Suppose Π is ﬁxed and its dimension satisﬁes k = Ω( log(n)

f

), then according to Lemma B.6, we have

ǫ2

that w.h.p. in n,

Hence, we have

Mij| ≤ |
|
f
M 2
ij
ˆqij
f

≤

ξ1

Mij|

+ ǫ

Ai|| · ||
||

Bj|| ≤

(1 + ǫ)

Ai|| · ||
||

,
Bj||

(i, j).

∀

m

2

(1 + ǫ)2
2
Bj||
Ai||
||
||
+ ||Bj||2
( ||Ai||2
2n||B||2
2n||A||2
·
F
F
c2
(ut
i)2
Ai||
1||
( ||Ai||2
ˆqij
2n||A||2
F

m

≤

ξ2

·

2n(1 + ǫ)2
m

) ≤

Bj||

2

A

2
F ,
||

||

· ||

2/
2
A
F
||
||
+ ||Bj||2
2n||B||2
F

) ≤

2nc2
1
m

,

(16)

(17)

(18)

(19)

(20)

(21)

where ξ1 follows from substituting Eqs.(19) and (1), and ξ2 follows from the assumption that
c1||

/
Ai||
||

||F .
A

ut
i| ≤
|

21

We can now bound the ﬁrst and second moments of Xi as

Xi| ≤ |
|

wijut
i

(ut
i)2
ˆqij s

M 2
ij
ˆqij
f

Mij| ≤ s
f

ξ1

≤

2nc1(1 + ǫ)
m

||F .
A
Bj||||
||

V ar(Xi) =

ˆqij(1

ˆqij)w2

ij(ut

i)2

M 2

ij ≤

(1 + ǫ)2

2

Ai||
||

Bj||
||

2

Xi

−

Xi
2nc2

1(1 + ǫ)2

m

ξ2

≤

2

Bj||
||

A

||

f
2
F ,
||

(ut
i)2
ˆqij

Xi

where ξ1 and ξ2 follows from substituting Eqs.(20) and (21).

The rest proof involves applying Bernstein’s inequality to derive a high-probability bound on

i Xi,
which is almost the same as the second part of proving Lemma C.3 in [3], so we omit the details here. The
only difference is that, because of the extra multiplicative term (1 + ǫ) in the bound of the ﬁrst and second
moments, the lower bound on the sample complexity m should also be multiplied by an extra (1 + ǫ)2 term.
By restricting 0 < ǫ < 1/2, this extra multiplicative term can be ignored as long as the original lower bound
of m contains a large enough constant.

P

C.4 Proof of Theorem 3.1

We now prove our main theorem for rank-1 case here. Rank-r proof follows a similar line of reasoning and
can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3].
vt+1 to denote the t-th and (t+1)-th step iterates (which are
Similar to the previous section, we use
column vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized
vectors.

ut and

b

b

The closed form solution for WAltMin update at t + 1 iteration is

ut
||

vt+1
j = σ∗
||

1v∗
j

i δijwijut
i δijwij(ut

iu∗
i
i)2 +

i δijwijut
i(
M
M1)ij
−
i δijwij(ut
i)2

.

P

f

P

Writing in matrix form, we get

b

b

ut
||

vt+1
j = σ∗
1h
||

u∗, ut

v∗
i

where B and C are diagonal matrices with Bjj =
vector RΩ(

M1)T ut with entries yj =

M

b

b

Each term of Eq.(22) can be bounded as follows.

−

f

C)v∗ + B−1y,

−

u∗, ut
1B−1(
σ∗
h
i δijwij(ut
i δijwijut
M
i(
P

B
i
−
i)2 and Cjj =
M1)ij.

−

P

i δijwijut

iu∗

i , and y is the

u∗, ut
(
h
||

B
i

P
C)v∗

|| ≤

f
dist(ut, u∗),

B−1
||

|| ≤

2,

y
||

||

=

RΩ(

M

||

−

M1)T ut

dist(ut, u∗)
||

M

M1||

−

+ η

M

||

M1||F /˜r + ησ∗
1,

−

where ξ1 follows directly from Lemma C.4. The proof of Eq.(23) is exactly the same as the proof of Lemma
B.3 and B.4 in [3].

f

According to Lemma C.2, since the distance is decreasing geometrically, after O(log( 1

ζ )) iterations we

get

(22)

(23)

(24)

(25)

dist(ut, u∗)

ζ + 2η

M

||

≤

M1||F /σ∗

−

1 + 2η.

P
P

−
ξ1

||

≤

22

Now we are ready to prove the spectral norm bound in Theorem 3.1:

M1 −
||
M1 −
(I
−

≤ ||

+

ut(
vt+1)T
||
ut(ut)T M1||
b
b
ut(ut)T )M1||
v∗
σ1h
||
i
u∗, ut
1B−1(
σ∗
h
||

ut(ut)T M1 −
vt+1)T
ut(
||
||
vt+1)T ]
ut
ut[(ut)T M1 − ||
(
||
||
||
b
b
vt+1)T
ut, u∗
(
b
b
||
C)v∗
b

− ||

ut

+

||

||

≤ ||
ξ1
1dist(ut, u∗) +
σ∗

1dist(ut, u∗) +
σ∗

1dist(ut, u∗) + 2σ∗
σ∗

5(ζσ∗
≤
= 5ζσ∗

1 + 2η
||
1 + 12η

M

M

||

−

−

+

−

B−1y
B
b
i
||
1dist(ut, u∗) + 2dist(ut, u∗)
||
M1||F + 2ησ∗
M1||F + 12ησ∗

||
M1||
−
M1||F + 2ησ∗

1) + 2η

M

M

−

||

1

1

≤
ξ2

≤
ξ3

≤
ξ4

+ 2η

M

||

M1||F /˜r + 2ησ∗

1

−

(26)

v∗,
where ξ1 follows from the deﬁnition of dist(ut, u∗), the fact that
= 1, and (ut)T M1 = σ1h
i
ξ2 follows from substituting Eq.(22), ξ3 follows from Eqs.(23) and (24), and ξ4 follows from the Eq.(25),
and fact that
1) (this will inﬂuence the number of iterations)
and also rescaling η to η/12 gives us the desired spectral norm error bound in Eq.(7). This completes our
proof of the rank-1 case. Rank-r proof follows a similar line of reasoning and can be obtained by combining
the current proof with the rank-r analysis in the original proof of LELA [3].

1. Rescaling ζ to ζ/(5σ∗

M1|| ≤

σ∗
1, ˜r

ut, u∗

ut
||

M

−

≥

||

||

C.5 Sampling

We describe a way to sample m elements in O(m log(n)) time using distribution qij deﬁned in Eq. (1).
Naively one can compute all the n2 entries of min
and toss a coin for each entry, which takes O(n2)
qij, 1
}
{
time. Instead of this binomial sampling we can switch to row wise multinomial sampling. For this, ﬁrst
estimate the expected number of samples per row mi = m( ||Ai||2
2n ). Now sample m1 entries from row
2||A||2
F
1 according to the multinomial distribution,

+ 1

m
m1 ·

( ||
2n

2

A1||
2
A
F
||
||

+ ||
2n

2

Bj||
2
B
F
||
||

) =

||A1||2
2n||A||2
F
||Ai||2
2||A||2
F

+ ||Bj||2
2n||B||2
F
+ 1
2n

.

q1j =

e

j

e

P

Note that
q1j = 1. To sample from this distribution, we can generate a random number in the interval
[0, 1], and then locate the corresponding column index by binary searching over the cumulative distribution
function (CDF) of
q1j. This takes O(n) time for setting up the distribution and O(m1 log(n)) time to
sample. For subsequent row i, we only need O(mi log(n)) time to sample mi entries. This is because for
binary search to work, only O(mi log(n)) entries of the CDF vector needs to be computed and checked.
Note that the speciﬁc form of
qij ensures that its CDF entries can be updated in an efﬁcient way (since we
only need to update the linear shift and scale). Hence, sampling m elements takes a total O(m log(n)) time.
Furthermore, the error in this model is bounded up to a factor of 2 of the error achieved by the Binomial
model [7] [21]. For more details please see our Spark implementation.

e

e

D Related work

Approximate matrix multiplication:

In the seminal work of [14], Drineas et al. give a randomized algorithm which samples few rows of A and
B and computes the approximate product. The distribution depends on the row norms of the matrices and

23

ǫ

||

||

A

A

B

B

−

−

||2||

||F ||

||2 ≤

AT B
||

AT B
||

For spectral norm bound of the form

||F . Later Sarlos [29] propose a sketching
the algorithm achieves an additive error proportional to
based algorithm, which computes sketched matrices and then outputs their product. The analysis for this
˜AT ˜B
algorithm is then improved by [10]. All of these results compare the error
||F in Frobenius
norm.
||2, the authors in [29, 11] show that
C
the sketch size needs to satisfy O(r/ǫ2), where r = rank(A) + rank(B). This dependence on rank is later
improved to stable rank in [26], but at the cost of a weaker dependence on ǫ. Recently, Cohen et al. [12]
further improve the dependence on ǫ and give a bound of O(˜r/ǫ2), where ˜r is the maximum stable rank. Note
that the sketching based algorithm does not output a low rank matrix. As shown in Figure 2, rescaling by
the actual column norms provide a better estimator than just using the sketched matrices. Furthermore, we
show that taking SVD on the sketched matrices gives higher error rate than our algorithm (see Figure 3(b)).
Low rank approximation: [16] introduced the problem of computing low rank approximation of a
given matrix using only few passes over the data. They gave an algorithm that samples few rows and
columns of the matrix and computes its SVD for low rank approximation. They show that this algorithm
achieves additive error guarantees in Frobenius norm. [15, 29, 19, 13] have later developed algorithms using
various sketching techniques like Gaussian projection, random Hadamard transform and volume sampling
that achieve relative error in Frobenius norm.[35, 28, 18, 6] improved the analysis of these algorithms and
provided error guarantees in spectral norm. More recently [11] presented an algorithm based on subspace
embedding that computes the sketches in the input sparsity time.

Another class of methods use entrywise sampling instead of sketching to compute low rank approxi-
mation. [1] considered an uniform entrywise sampling algorithm followed by SVD to compute low rank
approximation. This gives an additive approximation error. More recently [3] considered biased entrywise
sampling using leverage scores, followed by matrix completion to compute low rank approximation. While
this algorithm achieves relative error approximation, it takes two passes over the data.

There is also lot of interesting work on computing PCA over streaming data under some statistical
assumptions, e.g., [2, 27, 5, 30]. In contrast, our model does not put any assumptions on the input matrix.
Besides, our goal here is to get a low rank matrix and not just the subspace.

24

6
1
0
2
 
t
c
O
 
6
2

 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
5
6
6
0
.
0
1
6
1
:
v
i
X
r
a

Single Pass PCA of Matrix Products

Shanshan Wu
The University of Texas at Austin
shanshan@utexas.edu

Srinadh Bhojanapalli
Toyota Technological Institute at Chicago
srinadh@ttic.edu

Sujay Sanghavi
The University of Texas at Austin
sanghavi@mail.utexas.edu

Alexandros G. Dimakis
The University of Texas at Austin
dimakis@austin.utexas.edu

October 27, 2016

Abstract

In this paper we present a new algorithm for computing a low rank approximation of the product
AT B by taking only a single pass of the two matrices A and B. The straightforward way to do this is to
(a) ﬁrst sketch A and B individually, and then (b) ﬁnd the top components using PCA on the sketch. Our
algorithm in contrast retains additional summary information about A, B (e.g. row and column norms
etc.) and uses this additional information to obtain an improved approximation from the sketches. Our
main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in
addition we also provide results from an Apache Spark implementation that shows better computational
and statistical performance on real-world and synthetic evaluation datasets.

1 Introduction

Given two large matrices A and B we study the problem of ﬁnding a low rank approximation of their
product AT B, using only one pass over the matrix elements. This problem has many applications in machine
learning and statistics. For example, if A = B, then this general problem reduces to Principal Component
Analysis (PCA). Another example is a low rank approximation of a co-occurrence matrix from large logs,
e.g., A may be a user-by-query matrix and B may be a user-by-ad matrix, so AT B computes the joint
counts for each query-ad pair. The matrices A and B can also be two large bag-of-word matrices. For this
case, each entry of AT B is the number of times a pair of words co-occurred together. As a fourth example,
AT B can be a cross-covariance matrix between two sets of variables, e.g., A and B may be genotype and
phenotype data collected on the same set of observations. A low rank approximation of the product matrix
is useful for Canonical Correlation Analysis (CCA) [8]. For all these examples, AT B captures pairwise
variable interactions and a low rank approximation is a way to efﬁciently represent the signiﬁcant pairwise
interactions in sub-quadratic space.

×

n (d

Let A and B be matrices of size d

n) assumed too large to ﬁt in main memory. To obtain a
rank-r approximation of AT B, a naive way is to compute AT B ﬁrst, and then perform truncated singular
value decomposition (SVD) of AT B. This algorithm needs O(n2d) time and O(n2) memory to compute
the product, followed by an SVD of the n
n matrix. An alternative option is to directly run power method
on AT B without explicitly computing the product. Such an algorithm will need to access the data matrices
A and B multiple times and the disk IO overhead for loading the matrices to memory multiple times will be
the major performance bottleneck.

≫

×

1

For this reason, a number of recent papers introduce randomized algorithms that require only a few
passes over the data, approximately linear memory, and also provide spectral norm guarantees. The key step
in these algorithms is to compute a smaller representation of data. This can be achieved by two different
methods: (1) dimensionality reduction, i.e., matrix sketching [29, 11, 26, 12]; (2) random sampling [14, 3].
The recent results of Cohen et al. [12] provide the strongest spectral norm guarantee of the former. They
show that a sketch size of O(˜r/ǫ2) sufﬁces for the sketched matrices
B to achieve a spectral error of ǫ,
where ˜r is the maximum stable rank of A and B. Note that
B is not the desired rank-r approximation
of AT B. On the other hand,
[3] is a recent sampling method with very good performance guarantees.
The authors consider entrywise sampling based on column norms, followed by a matrix completion step
to compute low rank approximations. There is also a lot of interesting work on streaming PCA, but none
can be directly applied to the general case when A is different from B (see Figure 4(c)). Please refer to
Appendix D for more discussions on related work.

AT

AT

e

e

e

e

Despite the signiﬁcant volume of prior work, there is no method that computes a rank-r approximation
of AT B when the entries of A and B are streaming in a single pass 1. Bhojanapalli et al. [3] consider a
two-pass algorithm which computes column norms in the ﬁrst pass and uses them to sample in a second
pass over the matrix elements. In this paper, we combine ideas from the sketching and sampling literature
to obtain the ﬁrst algorithm that requires only a single pass over the data.

Contributions:

•

•

•

•

We propose a one-pass algorithm SMP-PCA (which stands for Streaming Matrix Product PCA) that
computes a rank-r approximation of AT B in time O((nnz(A) + nnz(B)) ρ2r3 ˜r
). Here
nnz(
) is the number of non-zero entries, ρ is the condition number, ˜r is the maximum stable rank, and
·
η measures the spectral norm error. Existing two-pass algorithms such as [3] typically have longer
runtime than our algorithm (see Figure 3(a)). We also compare our algorithm with the simple idea that
ﬁrst sketches A and B separately and then performs SVD on the product of their sketches. We show
that our algorithm always achieves better accuracy and can perform arbitrarily better if the column
vectors of A and B come from a cone (see Figures 2, 4(b), 3(b)).

η2 + nr6ρ4 ˜r3

η4

The central idea of our algorithm is a novel rescaled JL embedding that combines information from
matrix sketches and vector norms. This allows us to get better estimates of dot products of high
dimensional vectors compared to previous sketching approaches. We explain the beneﬁt compared to
a naive JL embedding in Figure 2 and the related discussion; we believe it may be of more general
interest beyond low rank matrix approximations.

AT B
k

We prove that our algorithm recovers a low rank approximation of AT B up to an error that depends
on
, decaying with increasing sketch size and number of samples
k
(Theorem 3.1). The ﬁrst term is a consequence of low rank approximation and vanishes if AT B is
exactly rank-r. The second term results from matrix sketching and subsampling; the bounds have
similar dependencies as in [12].

(AT B)rk

AT B
k

and

−

We implement SMP-PCA in Apache Spark and perform several distributed experiments on synthetic
and real datasets. Our distributed implementation uses several design innovations described in Sec-
tion 4 and Appendix C.5 and it is the only Spark implementation that we are aware of that can handle
matrices that are large in both dimensions. Our experiments show that we improve by approximately
a factor of 2
in running time compared to the previous state of the art and scale gracefully as the
cluster size increases. The source code is available online [36].

×

1One straightforward idea is to sketch each matrix individually and perform SVD on the product of the sketches. We compare

against that scheme and show that we can perform arbitrarily better using our rescaled JL embedding.

2

•

In addition to better performance, our algorithm offers another advantage: It is possible to compute
low-rank approximations to AT B even when the entries of the two matrices arrive in some arbitrary
order (as would be the case in streaming logs). We can therefore discover signiﬁcant correlations even
when the original datasets cannot be stored, for example due to storage or privacy limitations.

2 Problem setting and algorithms

Rd×n2 that are stored in disk,
Consider the following problem: given two matrices A
ﬁnd a rank-r approximation of their product AT B. In particular, we are interested in the setting where
both A, B and AT B are too large to ﬁt into memory. This is common for modern large scale machine
learning applications. For this setting, we develop a single-pass algorithm SMP-PCA that computes the
rank-r approximation without explicitly forming the entire matrix AT B.

Rd×n1 and B

∈

∈

Notations. Throughout the paper, we use A(i, j) or Aij to denote (i, j) entry for any matrix A. Let
Ai and Aj be the i-th column vector and j-th row vector. We use
for
spectral (or operator) norm. The optimal rank-r approximation of matrix A is Ar, which can be found by
Rn1×n2 as the projection
SVD. Given a set Ω
⊂
of A on Ω, i.e., PΩ(A)(i, j) = A(i, j) if (i, j)

kF for Frobenius norm, and
A
k
Rn1×n2, we deﬁne PΩ(A)

[n2] and a matrix A

Ω and 0 otherwise.

A
k
k

[n1]

×

∈

∈

∈

2.1 SMP-PCA

Our algorithm SMP-PCA (Streaming Matrix Product PCA) takes four parameters as input: the desired rank
r, number of samples m, sketch size k, and the number of iterations T . Performance guarantee involving
these parameters is provided in Theorem 3.1. As illustrated in Figure 1, our algorithm has three main steps:
1) compute sketches and side information in one pass over A and B; 2) given partial information of A and
B, estimate important entries of AT B; 3) compute low rank approximation given estimates of a few entries
of AT B. Now we explain each step in detail.

B and the column norms

Figure 1: An overview of our algorithm. A single pass is performed over the data to produce the sketched
[n2]. We then compute the
,
matrices
A,
Bjk
Aik
k
k
×
Ω and
M ) through a biased sampling process, where PΩ(
sampled matrix PΩ(
M ) =
e
M as an estimator for AT B, and
zero otherwise. Here Ω represents the set of sampled entries. We deﬁne
f
M ) gives the
. Performing matrix completion on PΩ(
Aik · k
k
f

f
M (i, j) =
desired rank-r approximation.

compute its entry as

eBj
eAT
i
e
e
Bj k
Aik·k

M (i, j) if (i, j)

, for all (i, j)

Bjk ·

[n1]

f

∈

∈

e

k

Step 1: Compute sketches and side information in one pass over A and B. In this step we compute
(0, 1/k)

Rk×d is a random matrix with entries being i.i.d.

B := ΠB, where Π

A := ΠA and

sketches

f

N

f

e

e

∈

3

Algorithm 1 SMP-PCA: Streaming Matrix Product PCA

1: Input: A

∈
iterations: T

∈

Rd×n1, B

Rd×n2, desired rank: r, sketch size: k, number of samples: m, number of

2: Construct a random matrix Π
pass over A and B to obtain:

3: Sample each entry (i, j)

×
deﬁned in Eq.(1); maintain a set Ω

∈

∈
A = ΠA,

Rk×d, where Π(i, j)
B = ΠB, and

(0, 1/k),
,
Bjk
k

[k]
×
∼ N
,
[n1]
Aik
k
[n2] independently with probability ˆqij = min

[n2].
×
1, qij}
{
[n2] which stores all the sampled pairs (i, j).
Rn1×n2, where PΩ(

M (i, j) is given in Eq. (2). Calculate PΩ(

(i, j)
(i, j)

e
[n1]

[n1]
e

∈
∈

M )

∀
∀

×

⊂

[d]. Perform a single

, where qij is

Rn1×n2, where

Ω and zero otherwise.
f
M ), Ω, r, ˆq, T ), see Appendix A for more details.

f

∈

M ) =

f

∈

4: Deﬁne

M
M (i, j) if (i, j)
∈
f
5: Run WAltMin(PΩ(
6: Output:
U

f

Rn1×r and
f

∈

∈

V

b

b

Rn2×r.

random variables. It is known that Π satisﬁes an ”oblivious Johnson-Lindenstrauss (JL) guarantee” [29][34]
and it helps preserving the top row spaces of A and B [11]. Note that any sketching matrix Π that is an
oblivious subspace embedding can be considered here, e.g., sparse JL transform and randomized Hadamard
transform (see [12] for more discussion).

Besides
[n1]

A and

B, we also compute the L2 norms for all column vectors, i.e.,

, for all
[n2]. We use this additional information to design better estimates of AT B in the next step,
B to sample. Note that this is the only step that needs one

Bjk
k

Aik
k

and

AT

∈

(i, j)
×
and also to determine important entries of
e
e
pass over data.

Step 2: Estimate important entries of AT B by rescaled JL embedding. In this step we use partial
B. We ﬁrst determine
B to sample, and then propose a novel rescaled JL embedding for estimating those entries.

information obtained from the previous step to compute a few important entries of
what entries of

AT

AT

e

e

We sample entry (i, j) of AT B independently with probability ˆqij = min
e
1, qij}
{

, where
e

e

e

Bjk
+ k
2
B
2n1k
F
k
[n2] be the set of sampled entries (i, j). Since E(

Aik
( k
2
A
2n2k
F
k

qij = m

·

2

2

).

⊂

[n1]

Let Ω
i,j qij) = m, the expected number
of sampled entries is roughly m. The special form of qij ensures that we can draw m samples in O(n1 +
m log(n2)) time; we show how to do this in Appendix C.5.

P

×

Note that qij intuitively captures important entries of AT B by giving higher weight to heavy rows and
columns. We show in Section 3 that this sampling actually generates good approximation to the matrix
AT B.

The biased sampling distribution of Eq.

(1) is ﬁrst proposed by Bhojanapalli et al. [3]. However,
their algorithm [3] needs a second pass to compute the sampled entries, while we propose a novel way of
estimating dot products, using information obtained in the ﬁrst step.

Deﬁne

M

Rn1×n2 as

∈

f

M (i, j) =

Bjk ·

Aik · k
k

AT
Bj
i
Bjk
Aik · k
k
e
e
M , instead, we only calculate
e
e

.

Note that we will not compute and store
is denoted as PΩ(

M ), where PΩ(

f

We now explain the intuition of Eq. (2), and why

(i, j) entry of AT B, a straightforward way is to use
between vectors

f
Ai and

M (i, j) if (i, j)

M )(i, j) =
f

Ω and 0 otherwise.
f
B. To estimate the
M is a better estimator than
AT
θij is the angle
Bjk ·
Aik · k
Bj =
i
k
f
Bj. Since we already know the actual column norms, a potentially better estimator
e
e
e

AT
θij, where
e

cos

f

f

∈

e

e

e

e

M (i, j) for (i, j)

Ω. This matrix

∈

(1)

(2)

e

e

4

JL embedding
Rescaled JL embedding

2

1

0

-1

t
c
u
d
o
r
p

 
t

o
d

 

d
e

t

a
m

i
t
s
E

-2

-1

-0.5

0
True dot product

0.5

1

(a)

(b)

Figure 2: (a) Rescaled JL embedding (red dots) captures the dot products with smaller variance compared
to JL embedding (blue triangles). Mean squared error: 0.053 versus 0.129. (b) Lower ﬁgure illustrates how
to construct unit-norm vectors from a cone with angle θ. Let x be a ﬁxed unit-norm vector, and let t be a
(x + t) with probability
random Gaussian vector with expected norm tan(θ/2), we set y as either x + t or
AT B
half, and then normalize it. Upper ﬁgure plots the ratio of spectral norm errors
,
/
k
k
−
k
M has better
when the column vectors of A and B are unit vectors drawn from a cone with angle θ. Clearly,
B for all possible values of θ, especially when θ is small.
accuracy than

−
AT B
k

AT

AT

f

M

−

B

e

e

f

would be

e
Aik · k
k

e
Bjk ·

cos

θij. This removes the uncertainty that comes from distorted column norms2.

AT
i

e

Bj (JL embedding) and

Figure 2(a) compares the two estimators

M (i, j) (rescaled JL embedding)
for dot products. We plot simulation results on pairs of unit-norm vectors with different angles. The vectors
have dimension 1,000 and the sketching matrix has dimension 10-by-1,000. Clearly rescaling by the actual
norms help reduce the estimation uncertainty. This phenomenon is more prominent when the true dot
products are close to
1,
and hence the uncertainty from angles may produce smaller distortion compared to that from norms. In the
extreme case when cos θ =

1, which makes sense because cos θ has a small slope when cos θ approaches

1, rescaled JL embedding can perfectly recover the true dot product.

f

±

±

e

e

In the lower part of Figure 2(b) we illustrate how to construct unit-norm vectors from a cone with angle
θ. Given a ﬁxed unit-norm vector x, and a random Gaussian vector t with expected norm tan(θ/2), we
construct new vector y by randomly picking one from the two possible choices x + t and
(x + t), and then
renormalize it. Suppose the columns of A and B are unit vectors randomly drawn from a cone with angle
AT
θ, we plot the ratio of spectral norm errors
in Figure 2(b). We observe that
B and can be much better when θ approaches zero, which agrees with the trend
M always outperforms
indicated in Figure 2(a).
f
compute the low rank approximation of AT B from the samples using alternating least squares:

e
Step 3: Compute low rank approximation given estimates of few entries of AT B. Finally we

AT B
/
k
k

AT B
k

AT

f

M

−

−

−

B

e

e

e

k

±

min
U,V ∈Rn×r

wij(eT

i U V T ej −

M (i, j))2,

X(i,j)∈Ω

f

(3)

2We also tried using the cosine rule for computing the dot product, and another sketching method speciﬁcally designed for

preserving angles [4], but empirically those methods perform worse than our current estimator.

5

where wij = 1/ˆqij denotes the weights, and ei, ej are standard base vectors. This is a popular technique for
low rank recovery and matrix completion (see [3] and the references therein). After T iterations, we will get
a rank-r approximation of
M presented in the convenient factored form. This subroutine is quite standard,
so we defer the details to Appendix A.

f

3 Analysis

Now we present the main theoretical result. Theorem 3.1 characterizes the interaction between the sketch
[
AT Brk
size k, the sampling complexity m, the number of iterations T , and the spectral error
,
[
AT Br is the output of SMP-PCA, and (AT B)r is the optimal rank-r approximation of AT B. Note
where
= n2,
that the following theorem assumes that A and B have the same size. For the general case of n1 6
.
Theorem 3.1 is still valid by setting n = max
n1, n2}
{

(AT B)r −
k

Theorem 3.1. Given matrices A
kAk2
of AT B. Deﬁne ˜r = max
F
of (AT B)r, where σ∗
[
AT Br be the output of Algorithm SMP-PCA. If the input parameters k, m, and T satisfy

as the maximum stable rank, and ρ = σ∗
1
σ∗
r

}
i is the i-th singular values of AT B.

Rd×n, let (AT B)r be the optimal rank-r approximation
as the condition number

∈
kAk2 , kBk2
F
kBk2

Rd×n and B

Let

∈

{

(4)

(5)

(6)

(7)

2ρ2r3
k
2
F
k

·

k

≥

m

2
B
A
C1k
k
k
AT B
k
C2˜r2
γ

≥

·

max

˜r, 2 log(n)
}
{
η2

+ log (3/γ)

,

2

nr3ρ2 log(n)T 2
η2

,

·

2
2
B
F +
A
F
k
k
k
k
AT B
kF (cid:19)
k
kF +
A
ζ

log( k

≥

(cid:18)

T

B
k

kF

),

where C1 and C2 are some global constants independent of A and B. Then with probability at least 1
we have

−

γ,

(AT B)r −
k

[
AT Brk ≤

η

AT B
k

(AT B)rkF + ζ + ησ∗
r .

−

Remark 1. Compared to the two-pass algorithm proposed by [3], we notice that Eq. (7) contains an
additional error term ησ∗
r . This extra term captures the cost incurred when we are approximating entries of
AT B by Eq. (2) instead of using the actual values. The exact tradeoff between η and k is given by Eq. (4).
On one hand, we want to have a small k so that the sketched matrices can ﬁt into memory. On the other
hand, the parameter k controls how much information is lost during sketching, and a larger k gives a more
accurate estimation of the inner products.

kF is much smaller than

Remark 2. The dependence on kAk2
kF or
A
k

captures one difﬁcult situation for our algorithm.
If
AT B
kF , which could happen, e.g., when many column vectors
k
of A are orthogonal to those of B, then SMP-PCA requires many samples to work. This is reasonable.
Imagine that AT B is close to an identity matrix, then it may be hard to tell it from an all-zero matrix without
enough samples. Nevertheless, removing this dependence is an interesting direction for future research.

kAT BkF
B
k

F +kBk2
F

Remark 3. For the special case of A = B, SMP-PCA computes a rank-r approximation of ma-
trix AT A in a single pass. Theorem 3.1 provides an error bound in spectral norm for the residual matrix
[
(AT A)r −
AT Ar. Most results in the online PCA literature use Frobenius norm as performance measure.
Recently, [22] provides an online PCA algorithm with spectral norm guarantee. They achieves a spectral
norm bound of ǫσ∗
r+1, which is stronger than ours. However, their algorithm requires a target dimension

1 +σ∗

6

of O(r log n/ǫ2), i.e., the output is a matrix of size n-by-O(r log n/ǫ2), while the output of SMP-PCA is
simply n-by-r.

Remark 4. We defer our proofs to Appendix C. The proof proceeds in three steps. In Appendix C.2, we
show that the sampled matrix provides a good approximation of the actual matrix AT B. In Appendix C.3,
V and the
we show that there is a geometric decrease in the distance between the computed subspaces
optimal ones U ∗, V ∗ at each iteration of WAltMin algorithm. The spectral norm bound in Theorem 3.1 is
then proved using results from the previous two steps.

U ,

b

b

Computation Complexity. We now analyze the computation complexity of SMP-PCA. In Step 1, we
compute the sketched matrices of A and B, which requires O(nnz(A)k + nnz(B)k) ﬂops. Here nnz(
)
·
denotes the number of non-zero entries. The main job of Step 2 is to sample a set Ω and calculate the
corresponding inner products, which takes O(m log(n) + mk) ﬂops. Here we deﬁne n as max
for
simplicity. According to Eq. (4), we have log(n) = O(k), so Step 2 takes O(mk) ﬂops. In Step 3, we run
alternating least squares on the sampled matrix, which can be completed in O((mr2 + nr3)T ) ﬂops. Since
Eq. (5) indicates nr = O(m), the computation complexity of Step 3 is O(mr2T ). Therefore, SMP-PCA
has a total computation complexity O(nnz(A)k + nnz(B)k + mk + mr2T ).

n1, n2}
{

4 Numerical Experiments

Spark implementation. We implement our SMP-PCA in Apache Spark 1.6.2 [37]. For the purpose of
comparison, we also implement a two-pass algorithm LELA [3] in Spark3. The matrices A and B are
stored as a resilient distributed dataset (RDD) in disk (by setting its StorageLevel as DISK_ONLY). We
implement the two passes of LELA using the treeAggregate method. For SMP-PCA, we choose the
subsampled randomized Hadamard transform (SRHT) [32] as the sketching matrix 4. The biased sampling
procedure is performed using binary search (see Appendix C.5 for how to sample m elements in O(m log n)
time). After obtaining the sampled matrix, we run ALS (alternating least squares) to get the desired low-rank
matrices. More details can be found in [36].

Description of datasets. We test our algorithm on synthetic datasets and three real datasets: SIFT10K [20],

NIPS-BW [23], and URL-reputation [24]. For synthetic data, we generate matrices A and B as GD, where
G has entries independently drawn from standard Gaussian distribution, and D is a diagonal matrix with
Dii = 1/i. SIFT10K is a dataset of 10,000 images. Each image is represented by 128 features. We set A as
the image-by-feature matrix. The task here is to compute a low rank approximation of AT A, which is a stan-
dard PCA task. The NIPS-BW dataset contains bag-of-words features extracted from 1,500 NIPS papers.
We divide the papers into two subsets, and let A and B be the corresponding word-by-paper matrices, so
AT B computes the counts of co-occurred words between two sets of papers. The original URL-reputation
dataset has 2.4 million URLs. Each URL is represented by 3.2 million features, and is indicated as ma-
licious or benign. This dataset has been used previously for CCA [25]. Here we extract two subsets of
features, and let A and B be the corresponding URL-by-feature matrices. The goal is to compute a low rank
approximation of AT B, the cross-covariance matrix between two subsets of features.

Sample complexity. In Figure 4(a) we present simulation results on a small synthetic data with n = d =
5, 000 and r = 5. We observe that a phase transition occurs when the sample complexity m = Θ(nr log n).
This agrees with the experimental results shown in previous papers, see, e.g., [9, 3]. For the rest experiments
presented in this section, unless otherwise speciﬁed, we set r = 5, T = 10, and sampling complexity m as
4nr log n.

3To our best knowledge, this the ﬁrst distributed implementation of LELA.
4Compared to Gaussian sketch, SRHT reduces the runtime from O(ndk) to O(nd log d) and space cost from O(dk) to O(d),

while maintains the same quality of the output.

7

Runtime (sec) vs Cluster size

LELA
SMC-PCA

eAT eB)
SVD(
SMP-PCA
LELA
Optimal

SVD( eAT eB)
SMP-PCA
LELA
Optimal

3000

2000

1000

0.2

0.15

0.1

0.05

r
o
r
r
e
 
m
r
o
n
 
l
a
r
t
c
e
p
S

0

2

10

5

(a)

1000
Sketch size (k)

2000

1000
Sketch size (k)

2000

Figure 3: (a) Spark-1.6.2 running time on a 150GB dataset. All nodes are m.2xlarge EC2 instances. See [36]
for more details. (b) Spectral norm error achieved by three algorithms over two datasets: SIFT10K (left)
and NIPS-BW (right). We observe that SMP-PCA outperforms SVD(
B) by a factor of 1.8 for SIFT10K
and 1.1 for NIPS-BW. Besides, the error of SMP-PCA keeps decreasing as the sketch size k grows.

AT

0.3

0.25

0.2

0.15

0.1

0.05

(b)

e

e

Table 1: A comparison of spectral norm error over three datasets

Dataset

d

n

Algorithm Sketch size k

Error

Synthetic

100,000

100,000

URL-
malicious

URL-
benign

792,145

10,000

1,603,985

10,000

Optimal
LELA
SMP-PCA

Optimal
LELA
SMP-PCA

Optimal
LELA
SMP-PCA

-
-
2,000

-
-
2,000

-
-
2,000

0.0271
0.0274
0.0280

0.0163
0.0182
0.0188

0.0103
0.0105
0.0117

Comparison of SMP-PCA and LELA. We now compare the statistical performance of SMP-PCA
and LELA [3] on three real datasets and one synthetic dataset. As shown in Figure 3(b) and Table 1, LELA
always achieves a smaller spectral norm error than SMP-PCA, which makes sense because LELA takes
two passes and hence has more chances exploring the data. Besides, we observe that as the sketch size
increases, the error of SMP-PCA keeps decreasing and gets closer to that of LELA.

In Figure 3(a) we compare the runtime of SMP-PCA and LELA using a 150GB synthetic dataset on
m3.2xlarge Amazon EC2 instances5. The matrices A and B have dimension n = d = 100, 000. The sketch
dimension is set as k = 2, 000. We observe that the speedup achieved by SMP-PCA is more prominent for
small clusters (e.g., 56 mins versus 34 mins on a cluster of size two). This is possibly due to the increasing
spark overheads at larger clusters, see [17] for more related discussion.

Comparison of SMP-PCA and SVD(

AT

generating column vectors of A and B from a cone with angle θ. Here SVD(

B). In Figure 4(b) we repeat the experiment in Section 2 by
B) refers to computing

AT

5Each machine has 8 cores, 30GB memory, and 2×80GB SSD.

e

e

e

e

8

0.5

0.4

0.3

0.2

r
o
r
r
e
 
m
r
o
n
 
l
a
r
t
c
e
p
S

Ratio of errors vs theta

105

k = 400

k = 800

r Br

AT
SMP-PCA

 

r
o
r
r
e
m
r
o
n

 
l

a
r
t
c
e
p
S

1

0.8

0.6

0.4

0.2

2

1
# Samples / nrlogn

3

4

100

0

π/4

π/2 3π/4

π

200 400 600 800 1000
Sketch size (k)

(a)

(b)

(c)

Figure 4: (a) A phase transition occurs when the sample complexity m = Θ(nr log n). (b) This ﬁgure plots
the ratio of spectral norm error of SVD(
B) over that of SMP-PCA. The columns of A and B are unit
vectors drawn from a cone with angle θ. We see that the ratio of errors scales to inﬁnity as the cone angle
shrinks. (c) If the top r left singular vectors of A are orthogonal to those of B, the product AT
r Br is a very
poor low rank approximation of AT B.

AT

e

e

SVD on the sketched matrices6. We plot the ratio of the spectral norm error of SVD(
B) over that of
SMP-PCA, as a function of θ. Note that this is different from Figure 2(b), as now we take the effect of
random sampling and SVD into account. However, the trend in both ﬁgures are the same: SMP-PCA
B) and can be arbitrarily better as θ goes to zero.
always outperforms SVD(
AT
AT B
||
e
e

The y-axis represents spectral norm error, deﬁned as
approximation found by a speciﬁc algorithm. We observe that SMP-PCA outperforms SVD(
factor of 1.8 for SIFT10K and 1.1 for NIPS-BW.

B) on two real datasets SIFK10K and NIPS-BW.
[
AT Br is the rank-r
, where
AT
B) by a

AT
In Figure 3(b) we compare SMP-PCA and SVD(

[
AT B
AT Br||
/
||

AT

−

e

e

e

e

||

M is a better estimator for AT B than

Now we explain why SMP-PCA produces a more accurate result than SVD(

AT
B). The reasons are
e
AT
twofold. First, our rescaled JL embedding
B (Figure 2). Second,
e
the noise due to sampling is relatively small compared to the beneﬁt obtained from
M , and hence the ﬁnal
M ) still outperforms SVD(
result computed using PΩ(
e
e
f

r Br. Let Ar and Br be the optimal rank-r approximation of A and
B, we show that even if one could use existing methods (e.g., algorithms for streaming PCA) to estimate Ar
r Br can be a very poor low rank approximation of AT B. This is demonstrated in
and Br, their product AT
Figure 4(c), where we intentionally make the top r left singular vectors of A orthogonal to those of B.

Comparison of SMP-PCA and AT

B).

AT

f

f

e

e

e

e

5 Conclusion

We develop a novel one-pass algorithm SMP-PCA that directly computes a low rank approximation of a
matrix product, using ideas of matrix sketching and entrywise sampling. As a subroutine of our algorithm,
we propose rescaled JL for estimating entries of AT B, which has smaller error compared to the standard
estimator ˜AT ˜B. This we believe can be extended to other applications. Moreover, SMP-PCA allows the
non-zero entries of A and B to be presented in any arbitrary order, and hence can be used for steaming
applications. We design a distributed implementation for SMP-PCA. Our experimental results show that

6This can be done by standard power iteration based method, without explicitly forming the product matrix eAT eB, whose size

is too big to ﬁt into memory according to our assumption.

9

SMP-PCA can perform arbitrarily better than SVD(
rithms that require two or more passes over the data.

AT

B), and is signiﬁcantly faster compared to algo-

Acknowledgements We thank the anonymous reviewers for their valuable comments. This research has
e
been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000, and ARO
YIP W911NF-14-1-0258.

e

10

References

[1] D. Achlioptas and F. McSherry. Fast computation of low rank matrix approximations. In Proceedings
of the thirty-third annual ACM symposium on Theory of computing, pages 611–618. ACM, 2001.

[2] A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental pca. In Advances

in Neural Information Processing Systems, pages 3174–3182, 2013.

[3] S. Bhojanapalli, P. Jain, and S. Sanghavi. Tighter low-rank approximation via sampling the leveraged
element. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 902–920. SIAM, 2015.

[4] P. T. Boufounos. Angle-preserving quantized phase embeddings.
Applications. International Society for Optics and Photonics, 2013.

In SPIE Optical Engineering+

[5] C. Boutsidis, D. Garber, Z. Karnin, and E. Liberty. Online principal components analysis. In Pro-
ceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887–901.
SIAM, 2015.

[6] C. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized hadamard

transform. SIAM Journal on Matrix Analysis and Applications, 34(3):1301–1340, 2013.

[7] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Compu-

tational mathematics, 9(6):717–772, 2009.

[8] X. Chen, H. Liu, and J. G. Carbonell. Structured sparse canonical correlation analysis. In International

Conference on Artiﬁcial Intelligence and Statistics, pages 199–207, 2012.

[9] Y. Chen, S. Bhojanapalli, S. Sanghavi, and R. Ward. Completing any low-rank matrix, provably. arXiv

preprint arXiv:1306.2979, 2013.

[10] K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In Proceedings
of the forty-ﬁrst annual ACM symposium on Theory of computing, pages 205–214. ACM, 2009.

[11] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing, pages 81–90.
ACM, 2013.

[12] M. B. Cohen, J. Nelson, and D. P. Woodruff. Optimal approximate matrix product in terms of stable

rank. arXiv preprint arXiv:1507.02268, 2015.

[13] A. Deshpande and S. Vempala. Adaptive sampling and fast low-rank matrix approximation. In Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 292–
303. Springer, 2006.

[14] P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices ii: Computing a

low-rank approximation to a matrix. SIAM Journal on Computing, 36(1):158–183, 2006.

[15] P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-error matrix
approximation: Column-based methods. In Approximation, Randomization, and Combinatorial Opti-
mization. Algorithms and Techniques, pages 316–326. Springer, 2006.

[16] A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo algorithms for ﬁnding low-rank approxima-

tions. Journal of the ACM (JACM), 51(6):1025–1041, 2004.

11

[17] A. Gittens, A. Devarakonda, E. Racah, M. F. Ringenburg, L. Gerhardt, J. Kottalam, J. Liu, K. J.
Maschhoff, S. Canon, J. Chhugani, P. Sharma, J. Yang, J. Demmel, J. Harrell, V. Krishnamurthy,
M. W. Mahoney, and Prabhat. Matrix factorization at scale: a comparison of scientiﬁc data analytics
in spark and C+MPI using three case studies. arXiv preprint arXiv:1607.01335, 2016.

[18] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algo-
rithms for constructing approximate matrix decompositions. SIAM review, 53(2):217–288, 2011.

[19] S. Har-Peled. Low rank matrix approximation in linear time. Manuscript. http://valis. cs. uiuc.

edu/sariel/papers/05/lrank/lrank. pdf, 2006.

[20] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. Pattern Analysis

and Machine Intelligence, IEEE Transactions on, 33(1):117–128, 2011.

[21] R. Kannan, S. S. Vempala, and D. P. Woodruff. Principal component analysis and higher correlations
for distributed data. In Proceedings of The 27th Conference on Learning Theory, pages 1040–1057,
2014.

[22] Z. Karnin and E. Liberty. Online pca with spectral bounds. In Proceedings of The 28th Conference on

Learning Theory (COLT), volume 40, pages 1129–1140, 2015.

[23] M. Lichman. UCI machine learning repository. http://archive.ics.uci.edu/ml, 2013.

[24] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Identifying suspicious urls: an application of large-
scale online learning. In Proceedings of the 26th annual international conference on machine learning,
pages 681–688. ACM, 2009.

[25] Z. Ma, Y. Lu, and D. Foster. Finding linear structure in large datasets with scalable canonical correla-

tion analysis. arXiv preprint arXiv:1506.08170, 2015.

[26] A. Magen and A. Zouzias. Low rank matrix-valued chernoff bounds and approximate matrix multipli-
cation. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,
pages 1422–1436. SIAM, 2011.

[27] I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited, streaming pca.

In Advances in Neural

Information Processing Systems, pages 2886–2894, 2013.

[28] N. H. Nguyen, T. T. Do, and T. D. Tran. A fast and efﬁcient algorithm for low-rank approximation of
a matrix. In Proceedings of the 41st annual ACM symposium on Theory of computing, pages 215–224.
ACM, 2009.

[29] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In Founda-
tions of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pages 143–152. IEEE,
2006.

[30] O. Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In Proceedings
of the 32nd International Conference on Machine Learning (ICML-15), pages 144–152, 2015.

[31] T. Tao. 254a, notes 3a: Eigenvalues and sums of hermitian matrices. Terence Tao’s blog, 2010.

[32] J. A. Tropp.

Improved analysis of the subsampled randomized hadamard transform. Advances in

Adaptive Data Analysis, pages 115–126, 2011.

12

[33] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational

Mathematics, 12(4):389–434, 2012.

[34] D. P. Woodruff. Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357,

2014.

[35] F. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. A fast randomized algorithm for the approximation

of matrices. Applied and Computational Harmonic Analysis, 25(3):335–366, 2008.

[36] S. Wu, S. Bhojanapalli, S. Sanghavi, and A. Dimakis. Github repository for ”single-pass pca of matrix

products”. https://github.com/wushanshan/MatrixProductPCA, 2016.

[37] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J. Franklin, S. Shenker, and
I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.
In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,
2012.

13

A Weighted alternating minimization

Algorithm 2 provides a detailed explanation of WAltMin, which follows a standard procedure for matrix
PΩ(A) to denote the Hadamard product between w and PΩ(A):
completion. We use RΩ(A) = w.
Rn1×n2 = 1/ˆqij is
RΩ(A)(i, j) = w(i, j)
the weight matrix. Similarly we deﬁne the matrix R1/2
PΩ(A)(i, j) for
(i, j)

Ω and 0 otherwise, where w
Ω (A) as R1/2

∗
PΩ(A)(i, j) for (i, j)

Ω and 0 otherwise.

Ω (A)(i, j) =

∈
w(i, j)

∈

∗

∗

The algorithm contains two parts: initialization (Step 2-6) and weighted alternating minimization (Step
M ) and then set row i of U (0)
7-10). In the ﬁrst part, we compute SVD of the weighted sampled matrix RΩ(
to be zero if its norm is larger than a threshold (Step 6). More details of this trim step can be found in [3].
In the second part, the goal is to solve the following non-convex problem by alternating minimization:

f

p

∈

min
U,V

wij(eT

i U V T ej −

M (i, j))2,

X(i,j)∈Ω

f

(8)

where ei, ej are standard base vectors. After running T iterations, the algorithm outputs a rank-r approxi-
mation of

M presented in the convenient factored form.

Ω0, . . . , Ω2T }
{

Algorithm 2 WAltMin [3]

f

∗

∀

∈

i, j

M )

M )

PΩ0(

Rn1×n2, Ω, r, ˆq, and T

f
M ) = w.

1: Input: PΩ(
2: wij = 1/ˆqij when ˆqij > 0, 0 else,
3: Divide Ω in 2T + 1 equal uniformly random subsets, i.e., Ω =
4: RΩ0(
5: U (0)Σ(0)(V (0))T = SVD(RΩ0(
6: Trim U (0) and let
f
7: for t = 0 to T
8:

U (0) be the output
1 do
V (t+1) = argminV k
b
U (t+1) = argminU k
9:
b
10: end for
U (T )
11: Output:
b

2
U (t)V T )
F
k
2
V (t+1))T )
F
k

U (
b
Rn2×r.
b

M
f
V (T )
f

Ω2t+1(
Ω2t+2(

R1/2
R1/2

Rn1×r and

M ), r)

f
M

f

−

−

−

∈

b

∈

b

B Technical Lemmas

We will frequently use the following concentration inequality in the proof.

Lemma B.1. (Matrix Bernstein’s Inequality [33]). Consider p independent random matrices X1, ...., Xp in
Rn×n, where each matrix has bounded deviation from its mean:

Xi −
||
Let the norm of the covariance matrix be

E[Xi]

L,

|| ≤

i.

∀

p

(Xi −

σ2 = max

E

(cid:12)
((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Then the following holds for all t
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi=1

"

E[Xi])T

(Xi −

E[Xi])T (Xi −

E[Xi])

p

"
Xi=1

,

E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2n exp( −

t2/2
σ2 + Lt/3

).

E[Xi])(Xi −
0:

≥
p

(Xi −

P

(cid:12)
"(cid:12)
Xi=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[Xi])
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

# ≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
14

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

A formal deﬁnition of JL transform is given below [29][34].

Deﬁnition B.2. A random matrix Π
for short, if with probability at least 1

Πv, Πv′

v, v′

i − h

i| ≤

ǫ

v
||

|| · ||

v′

.
||

|h

∈
−

Rk×d forms a JL transform with parameters ǫ, δ, f or JLT(ǫ, δ, f )
V it holds that
δ, for any f -element subset V

Rd, for all v, v′

⊂

∈

The following lemma [34] characterizes the tradeoff between the reduced dimension k and the error

level ǫ.

N

Lemma B.3. Let 0 < ǫ, δ < 1, and Π

Rk×d be a random matrix where the entries Π(i, j) are i.i.d.

(0, 1/k) random variables. If k = Ω(log(f /δ)ǫ−2), then Π is a JLT(ǫ, δ, f ).

∈

We now present two lemmas that connect

A

Rk×n and

B

Rd×n with A

Rd×n and B

Rd×n.

∈
∈
∈
Lemma B.4. Let 0 < ǫ, δ < 1, if k = Ω( log(2n/δ)
), then with probability at least 1
e

ǫ2

e

δ,

−

∈

(1

ǫ)

A

||

2
F ≤ ||
||

A

2
F ≤
||

−

(1 + ǫ)

A

2
F ,
||
||
AT B

(1

ǫ)

B

||

−
ǫ

A

2
F ≤ ||
||
||F .
B

AT
||

B

||F ≤
Proof. This is again a standard result of JL transformation, e.g., see Deﬁnition 2.3 and Theorem 2.1 of [34]
and Lemma 6 of [29] .

||F ||

−

e

e

e

e

||

B

2
F ≤
||

(1 + ǫ)

B

2
F ,
||

||

Lemma B.5. Let 0 < ǫ, δ < 1, if k = Θ( ˜r+log(1/δ)
rank, then with probability at least 1

δ,

ǫ2

−

), where ˜r = max

is the maximum stable

||A||2
F

||A||2 , ||B||2

F
||B||2

{

}

Proof. This follows from a recent paper [12].
e

e

AT
||

B

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

Using the above two lemmas, we can prove the following two lemmas that relate

AT
M is DA
deﬁned in Algorithm 1. A more compact deﬁnition of
.
matrices with (DA)ii =
Bj ||
/
Bj||
Ai||
/
Ai||
||
||
||
||
f
e
Lemma B.6. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( log(2n/δ)
e
e

and (DB)jj =

ǫ2

e

M with AT B, for

M
BDB, where DA and DB are diagonal
f
f

), then with probability at least 1

δ,

−

AT
i Bj| ≤

ǫ

Ai|| · ||
||

,
Bj||

M

||

−

AT B

||F ≤

ǫ

A

||

||F ||

B

||F .

Mij −
|
f

Proof. Let 0 < ǫ < 1/2, 0 < δ < 1, according to the Deﬁnition B.2 and Lemma B.3, we have that if
k = Ω( log(2n/δ)

), then with probability at least 1

δ, and for all i, j

f

ǫ2

1 + ǫ,

1

ǫ

−

≤

AT
i Bj|

as

−
(DB)jj ≤

1 + ǫ,

AT
i
|

e

Bj −
e

AT
i Bj| ≤

ǫ

Ai||||
||

.
Bj||

(9)

We can now bound

Mij −
|
AT
i
f
|

ǫ

1

≤

−

(DA)ii ≤
Mij −
|
AT
i Bj|
f
Bj(DA)ii(DB)jj −
Bj(1 + ǫ)2
e
{|
(1 + ǫ)2ǫ
e
e
{
Ai||||
||

,
Bj ||

AT
i

−
Bj||
Ai||||
||

max
e

max

7ǫ

ξ1=
ξ2

≤
ξ3

≤
ξ4

≤

AT
i Bj|
AT
AT
,
i Bj|
i
|
+ ((1 + ǫ)2
e

e

Bj(1

ǫ)2

−

−
AT
i Bj|
1)
|

AT
i Bj|}
, (1

−

−

15

ǫ)2ǫ

Ai||||
||

Bj||

+ (1

(1

−

−

AT
ǫ)2)
i Bj|}
|
(10)

Mij, ξ2 follows from the bound in Eq.(9), ξ3 follows from triangle
. Now rescaling ǫ as ǫ/7 gives the desired

where ξ1 follows from the deﬁnition of
inequality and Eq.(9), and ξ4 follows from
bound in Lemma B.6.
f
Mij −
f

||F =

Hence,

AT B

ij |

M

−

||

qP
Lemma B.7. Let 0 < ǫ < 1/14, 0 < δ < 1, if k = Ω( ˜r+log(n/δ)

f

qP
ǫ2

AT
i Bj| ≤ ||
|
AT
i Bj|

2

≤

Ai||||

Bj ||
ij ǫ2

Proof. We can bound the spectral norm of the difference matrix as follows:

f

M

||

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

2

Ai||
||

Bj||
||

2 = ǫ

A

||F ||

B

||F .

||

), then with probability at least 1

δ,

−

M

||

−

AT B

ξ1=

||

f

AT
DA
BDB −
||
AT
DA||||
B
−
e
e
(1 + ǫ)2ǫ
e
||
B
A
7ǫ

A
e
||||
,
||

||||

||

B

≤ ||
ξ3

≤

≤

||

DAAT BDB + DAAT BDB −
AT B
DA||||
+
||||
||
B

DB||
+ (1 + ǫ)ǫ

AT B

+ ǫ

||||

A

A

DAAT B + DAAT B
I
DA −
+
DB −
||
B

||

I

||

||||

||

||

||||

||

AT B
||
AT B

||

−

||||

(11)

Mij, and ξ2 follows from Lemma B.5 and bound in Eq.(9). Rescaling

where ξ1 follows from the deﬁnition of
ǫ as ǫ/7 gives the desired bound in Lemma B.7.

f

We will frequently use the term with high probability. Here is a formal deﬁnition.

Deﬁnition B.8. We say that an event E occurs with high probability (w.h.p.) in n if the probability that its
complement ¯E happens is polynomially small, i.e., P r( ¯E) = O( 1

nα ) for some constant α > 0.

The following two lemmas deﬁne a ”nice” Π and when this happens with high probability.

Deﬁnition B.9. The random Gaussian matrix Π is ”nice” with parameter ǫ if for all (i, j) such that qij ≤
(i.e., qij = ˆqij), the sketched values

Mij satisﬁes the following two inequalities:

1

Mij|
|
ˆqij ≤
f

(1 + ǫ)

n
m

f
2
F +
||

A

(
||

B

2
F ),
||

||

X{j:ˆqij=qij } f

M 2
ij
ˆqij ≤

(1 + ǫ)

2n
m

A

(
||

2
F +
||

B

F )2.
2
||

||

Lemma B.10. If k = Ω( log(n)
w.h.p. in n.

ǫ2

), and 0 < ǫ < 1/14, then the random Gaussian matrix Π

Proof. According to Lemma B.6, if k = Ω( log(n)
ǫ

ǫ2

. In other words, the following holds with probability at least 1

δ:

), then w.h.p. in n, for all (i, j) we have

Ai|| · ||
||

Bj||

−

Mij| ≤ |
|
The above inequality is sufﬁcient for Π to be ”nice”:
f

Ai|| · ||
||

+ ǫ

AT
i Bj|

Bj|| ≤

(1 + ǫ)

Ai|| · ||
||

,
Bj||

(i, j)

∀

Rk×d is ”nice”

∈

AT
i Bj| ≤

Mij −
|
f

Mij
ˆqij ≤
f

(1 + ǫ) ||

Ai|| · ||
ˆqij

Bj||

≤

(1 + ǫ)

2 +
Ai||
(
||
( ||Ai||2
2n||A||2
F

2)/2
Bj||
||
+ ||Bj||2
2n||B||2
F

m

·

) ≤

(1 + ǫ)

n
m

A

(
||

2
F +
||

B

2
F )
||

||

16

M 2
ij
ˆqij ≤

(1 + ǫ)2

2

2

Bj||
||

Ai||
||
ˆqij

X{j:ˆqij=qij} f

X{j:ˆqij=qij}

4 +
Ai||
||
( ||Ai||2
2n||A||2
F

4

Bj||
||
+ ||Bj||2
2n||B||2
F

)

m

·

X{j:ˆqij=qij}
2n
2
F +
A
(
m
||
||

B

F )2.
2
||

||

(1 + ǫ)

(1 + ǫ)

≤

≤

ǫ2

Therefore, we conclude that if k = Ω( log(n)

), then Π is ”nice” w.h.p. in n.

C Proofs

C.1 Proof overview

We now present the key steps in proving Theorem 3.1. The framework is similar to that of LELA [3].

Our proof proceeds in three steps. In the ﬁrst step, we show that the sampled matrix provides a good
approximation of the actual matrix AT B. The result is summarized in Lemma C.1. Here RΩ(
M ) denotes
the sampled matrix weighted by the inverse of sampling probability (see Line 4 of Algorithm 2). Detailed
proof can be found in Appendix C.2. For consistency, we will use Ci (i = 1, 2, ...) to denote global constant
that can vary from step to step.

f

Lemma C.1. (Initialization) Let m and k satisfy the following conditions for sufﬁciently large constants C1
and C2:

then the following holds w.h.p. in n:

m

C1

≥

k

≥

||

A

B

2
2
F +
F
||
||
||
AT B
||F (cid:19)
||
2
A
||
||
AT B

˜r + log(n)
δ2

·

2 n
δ2 log(n),
B

,

(cid:18)

C2

2
||
2
F
||

||
||

RΩ(

M )

||

AT B

δ

AT B
||

||F .

|| ≤

−

In the second step, we show that at each iteration of WAltMin algorithm, there is a geometric decrease
V and the optimal ones U ∗, V ∗. The result is shown
in the distance between the computed subspaces
in Lemma C.2. Appendix C.3 provides the detailed proof. Here for any two orthonormal matrices X and
, where X⊥
Y , we deﬁne their distance as the principal angle based distance, i.e., dist(X, Y ) =
b
||
denotes the subspace orthogonal to X.

X T
||

⊥Y

U ,

f

b

Lemma C.2. (WAltMin Descent) Let k, m, and T satisfy the conditions stated in Theorem 3.1. Also,
(AT B)r||F . Let ˆU (t) and ˆV (t+1) be the t-th and
1
consider the case when
576ρr1.5 ||
(t+1)-th step iterates of the WAltMin procedure. Let U (t) and V (t+1) be the corresponding orthonormal
(U (t))i
1/2. Denote AT B as M , then the
||F and dist(U (t), U ∗)
matrices. Let
/
Ai||
A
||
||
||
γ/T :
following holds with probability at least 1

(AT B)r||F ≤

AT B
||

8√rρ

|| ≤

≤

−

dist(V t+1, V ∗)

dist(U t, U ∗) + η

M

Mr||F /σ∗

r + η,

−

||

−
1
2

≤

(V (t+1))j
||

|| ≤

8√rρ

/
Bj||
||
||

B

||F .

17

In the third step, we prove the spectral norm bound in Theorem 3.1 using results from the above two
lemmas. Comparing Lemma C.1 and C.2 with their counterparts of LELA (see Lemma C.2 and C.3 in [3]),
we notice that Lemma C.1 has the same bound as that of LELA, but the bound in Lemma C.2 contains
an extra term η. This term eventually leads to an additive error term ησ∗
r in Eq.(7). Detailed proof is in
Appendix C.4.

C.2 Proof of Lemma C.1

We ﬁrst prove the following lemma, which shows that RΩ(
we deﬁne CAB := (||A||2

F )2

.

F +||B||2
||AT B||2
F

Lemma C.3. Suppose Π is ﬁxed and is ”nice”. Let m
constant C1, then w.h.p. in n, the following is true:

M ) is close to

M . For simplicity of presentation,

f
C1 ·

≥

f

CAB

n
δ2 log(n) for sufﬁciently large global

RΩ(

M )

M

−

|| ≤

||

δ

AT B
||

||F .

Proof. This lemma can be proved in the same way as the proof of Lemma C.2 in [3]. The key idea is to use
f
the matrix Bernstein inequality. Let Xij = (δij −
j , where δij is a
random variable
ˆqij)wij
n
indicating whether the value at (i, j) has been sampled. Since Π is ﬁxed,
i,j=1 are independent zero
mean random matrices. Furthermore,

Xij}
{

0, 1
}
{

MijeieT

f

M )

M .

f
n
i,j=1 = RΩ(

−

Since Π is ”nice” with parameter 0 < ǫ < 1/14, we can bound the 1st and 2nd moment of Xij as

f

f

Xij}

i,j{

P

follows:

Xij ||
||

= max

(1

ˆqij)wij

{|

−

ˆqijwij
|

,
Mij|
f

ξ1

≤

Mij|
|
ˆqij
f

Mij|} ≤
f

(1 + ǫ)

n
m

A

(
||

2
F +
||

B

2
F );
||

||

σ2 = max

XijX T

ij 

E

,


Xij


E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
ˆqij −

1)

{(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(
|

= max

i



(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Xij
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

M 2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ij
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆqij
X{j:ˆqij=qij } f

M 2
ij |

ξ3

≤

f

ξ2= max
i

ˆqij(1

−

ˆqij)w2
ij

X T
ij Xij
(cid:12)
(cid:12)
}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(1 + ǫ)

ξ1

≤

(cid:12)
(cid:12)
Xj
(cid:12)
(cid:12)
(cid:12)
2
(cid:12)
F +
||

2n
m

A

(
||

B

F )2,
2
||

||

M 2
ij(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f

1. Now we can use matrix Bernstein inequality (see Lemma B.1) with t = δ

where ξ1 follows from Lemma B.10, ξ2 follows from a direct calculation, and ξ3 follows from the fact that
||F to show that
ˆqij ≤
n
δ2 log(n), then the desired inequality holds w.h.p. in n, where C1 is some global
if m
≥
constant independent of A and B. Note that since 0 < ǫ < 1/14, (1 + ǫ) < 2. Rescaling C1 gives the
desired result.

(1 + ǫ)C1CAB

AT B
||

Now we are ready to prove Lemma C.1, which is a counterpart of Lemma C.2 in [3].

Proof. We ﬁrst show that
||F holds w.h.p. in n over the randomness of Π. Note
that in Lemma C.3, we have shown that it is true for a ﬁxed and ”nice” Π, now we want to show that it also
holds w.h.p. in n even for a random chosen Π.

RΩ(

|| ≤

M )

f

f

M

−

||

δ

AT B
||

Let G be the event that we desire, i.e., G =

AT B
||
complimentary event. By conditioning on Π, we can bound the probability of ¯G as

RΩ(

|| ≤

AT

AT

B)

{||

−

B

δ

||F }

. Let ¯G be the

P r( ¯G) = P r( ¯G
P r( ¯G

≤

e
Π is ”nice”)P r(Π is ”nice”) + P r( ¯G
Π is not ”nice”)P r(Π is not ”nice”)
|
|
Π is ”nice”) + P r(Π is not ”nice”).
|

e

e

e

18

n
δ2 log(n), and k

, then both
CAB
≥
in n. Therefore, the the probability of ¯G is

log(n)
ǫ2

C2

According to Lemma C.3 and Lemma B.10, if m
events
polynomially small in n, i.e., the desired event G happens w.h.p. in n.
AT B

C1 ·
and P r(Π is ”nice”) happen w.h.p.

Π is ”nice”
G
|
{

Next we show that

M

≥

}

||
), then w.h.p. in n, we have

−

Θ( ˜r+log(n)
ǫ2
k = Θ( ˜r+log(n)

δ2

f
||A||2||B||2
||AT B||2
F

), then

M

||
By triangle inequality, we have

·

−
shown that w.h.p. in n, both terms are less than δ
inequality
−
statement of Lemma C.1.

AT B
||

AT B

RΩ(

|| ≤

AT

B)

||

f
||

δ

|| ≤
||

B

AT B

AT B
δ
||
M
−
AT B
−
f
RΩ(

||F holds w.h.p. in n. According to Lemma B.7, if k =
. Now let ǫ := δ ||AT B||F
||A||||B|| , we have that if
|| ≤
||
δ

A
ǫ
||
||||
AT B
||F holds w.h.p. in n.
||
AT B
. We have
+
M )
|| ≤ ||
−
||
||
AT B
||F . By rescaling δ as δ/2, we have that the desired
||
||F holds w.h.p. in n, when m and k are chosen according to the
f
f

AT B

RΩ(

|| ≤

M )

f

f

M

M

−

||

e

e

Because the bound of Lemma C.1 has the same form as that of Lemma C.2 in [3], the corollary
(AT B)r||F ≤

M ), which is stated here without proof:

AT B
||

−

if

of Lemma C.2 also holds for RΩ(
1
576κr1.5 ||

(AT B)r||F , then w.h.p. in n we have
U (0))i
/
Ai||
(
||
||
||

f
8√r

|| ≤

A

||F

and dist(

U (0), U ∗)

1/2,

≤

U (0) is the initial iterate produced by the WAltMin algorithm (see Step 6 of Algorithm 2). This

b

where
b
corollary will be used in the proof of Lemma C.2.

b

(AT B)r||F ≥
AT B
Similar to the original proof in [3], we can now consider two cases separately: (1)
||
1
(AT B)r||F . The ﬁrst case is simple: use
(AT B)r||F ≤
576ρr1.5 ||
Lemma C.1 and Wely’s inequality [31] already implies the desired bound in Theorem 3.1. To see why,
note that Lemma C.1 and Wely’s inequality imply that

(AT B)r||F ; (2)

1
576ρr1.5 ||

AT B
||

−

−

(AT B)r −
||
AT B

−

AT B

−
AT B
2
||

−

(RΩ(
M )r||
(AT B)r||
+
f
(AT B)r||
(AT B)r||

+ δ

ξ1

≤ ||
ξ2

≤ ||
ξ3

≤

RΩ(

M )

+

RΩ(

M )

||

AT B
||

−
||
AT B
||F +
RΩ(
f
||
||
||F ,

AT B
||

+ 2δ

f

M )

AT B
f

||

−

−

(RΩ(

M ))r||
AT B
f
||

−

+

(AT B)r||

(12)

where Mr denotes the best rank-r approximation of M , ξ1 follows triangle inequality, ξ2 follows from
Lemma C.1 and Wely’s inequality, and ξ3 follows from Lemma C.1. If
(AT B)r||F . Setting δ =
(AT B)r||F ≤
then
O(η/(ρr1.5)) in Eq.(12) gives the desired error bound in Theorem 3.1. Therefore, in the following analysis
we only need to consider the second case.

AT B
−
||
AT B
O(ρr1.5)
||

(AT B)r||F ≥
−

(AT B)r||F +
||

1
576ρr1.5 ||

AT B
||

AT B
||

||F =

−

(AT B)r||F ,

C.3 Proof of Lemma C.2

We ﬁrst prove the following lemma, which is a counterpart ofLemma C.5 in [3]. For simplicity of presenta-
tion, we use M to denote AT B in the following proof.

Lemma C.4. If m
C1 and C2, then the following holds with probability at least 1

C1nr log(n)T /(γδ2) and k

≥

≥

C2(˜r+log(n))/ǫ2 for sufﬁciently large global constants

(U (t))T RΩ(
||

M

−

Mr)

−

(U (t))T (M

Mr)

δ

M

|| ≤

||

−

−

A

||F ||

B

||F + ǫ

||

A

||

B

.
||

||||

γ/T :

−
Mr||F + δǫ

f

19

Proof. For a ﬁxed Π, we have that if m
least 1

γ/T :

≥

−

C1nr log(n)T /(γδ2), then following holds with probability at

(U (t))T RΩ(
||

M

−

Mr)

−

(U (t))T (

M

Mr)

δ

M

|| ≤

||

Mr||F .

−

−

(13)

The proof of Eq.(13) is exactly the same as the proof of Lemma C.5/B.6/B.2 in [3], so we omit its details
here. The key idea is to deﬁne a set of zero-mean random matrices Xij such that
Mr)
desired bound.

−
Mr), and then use second moment-based matrix Chebyshev inequality to obtain the

ij Xij = (U (t))T RΩ(

(U (t))T (

P

f

f

f

f

M

M

−

−

According to Lemma B.6 and Lemma B.7, if k = Θ((˜r + log(n))/ǫ2), then w.h.p. in n, the following

f

holds:

M

||

−

AT B

||F ≤

ǫ

A

||

||F ||

B

||F ,

M

||

−

AT B

ǫ

A

||

B

.
||

||||

|| ≤

(14)

Using triangle inequality, we have that if m and k satisfy the conditions of Lemma C.4, then the follow-

ing holds with probability at least 1

γ/T :

f

f

−

−

(U (t))T RΩ(
||
(U (t))T RΩ(

M

≤ ||
ξ1

δ

δ

δ

M

||

M
f

||

M

||

−

−

−

≤

≤
ξ2

≤

M
f
−
Mr||F +
f
||
Mr||F + δ
||
Mr||F + δǫ

Mr)

Mr)

−

−

(U (t))T (M
(U (t))T (

M

Mr)

||
Mr)

||

−

−

f

M

M

M

−

−

||
||F +
M
||
f
||F + ǫ
B
f
||F ||

M

A

||

M

||

−

A

B
f

,
||

||||

||

+

(U (t))T (M
||

−

M )

||

f

where ξ1 follows from Eq.(13), and ξ2 follows from Eq.(14).

Now we are ready to prove Lemma C.2. For simplicity, we focus on the rank-1 case here. Rank-
r proof follows a similar line of reasoning and can be obtained by combining the current proof with the
rank-r analysis in the original proof of LELA [3]. Note that compared to Lemma C.5 in [3], Lemma C.4
contains two extra terms δǫ
. Therefore, we need to be careful for steps that involve
Lemma C.4.

||F + ǫ

||F ||

||||

B

B

A

A

||

||

||

In the rank-1 case, we use ˆut and ˆvt+1 to denote the t-th and (t+1)-th step iterates (which are column

vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized vectors.

||F , for some constant c1.
Bj||
/
||
Bounding dist(vt+1, v∗):
In Lemma C.4, set ǫ = ||AT B||
AT B

Proof. This proof contains two parts. In the ﬁrst part, we will prove that the distance dist(vt+1, v∗) de-
creases geometrically over time. In the second part, we show that the j-th entry of vt+1 satisﬁes
c1||

vt+1
j
|

| ≤

B

2||A||||B||η and δ = η
2˜r , where 0 < η < 1, then we have δǫ
AT B
/2, and ǫ
B
||
γ/T , the following holds:

||F ≤
/2. Therefore, with probability at least

AT B
||

η2
2˜r ||

||F ||

|| ≤

|| ≤

||||

B

A

A

||

||

||

||

η

η

·

||A||F ||B||F
||A||||B||

1

−

M

M1)

(ut)T (M

M1)

η

M

(ut)T RΩ(
||
(ut)T RΩ(
||

Hence, we have

−

−

−
dist(ut, u∗)
||
Using the explicit formula for WAltMin update (see Eq.(46) and Eq.(47) in [3]), we can bound
ˆvt+1, v∗
h

M1||F /˜r + ησ∗
1.

||
M1||

as follows.

f
−

M1)

|| ≤

|| ≤

+ η

⊥i

f

M

M

−

−

−

||

and

M1||F /˜r + ησ∗
1.
M

(15)

ˆvt+1, v∗
h

i

ˆut
||

||h

ˆvt+1, v∗

/σ∗
i

1 ≥ h

ut, u∗

i −

1

δ1

1

− h

ut, u∗

2
i

−

1

δ1

1

−

M

(η ||

M1||F

−
˜rσ∗
1

+ η).

δ1

−

p

20

ˆut
||

ˆvt+1, v∗
⊥i

||h

/σ∗

1 ≤

1

1

− h

ut, u∗

2 +
i

1

δ1

−

δ1

p

1

−

δ1

(dist(ut, u∗) ||

M

M1||

+ η ||

M

−
σ∗
1

As discussed in the end of Appendix C.2, we only need to consider the case when
1
576ρr1.5

(AT B)r||F , where ρ = σ∗
||

AT B
−
||
r . In the rank-1 case, this condition reduces to
M
−
||
1
ut, u∗
20 ), and use the fact that
h
ˆvt+1, v∗
as
h

1
20 , η
ˆvt+1, v∗
h

For sufﬁciently small constants δ1 and η (e.g., δ1 ≤
and dist(u0, u∗)

1/2, we can further bound

1/σ∗

≤
and

⊥i

≤

i

M1||F

+ η).

−
˜rσ∗
1
(AT B)r||F ≤
σ∗
576 .
M1||F ≤
u0, u∗
i ≥ h

i

ˆut
||

||h

ˆvt+1, v∗

/σ∗
i

1 ≥ h

u0, u∗

i −

1

− h

u0, u∗

2
i

−

1
10 ≥

√3
2 −

2
10 ≥

1
2

.

ˆut
||

ˆvt+1, v∗
⊥i

||h

/σ∗

δ1

dist(ut, u∗) +

dist(ut, u∗) +

1 ≤
ξ1

≤

1
1
4

δ1

−
dist(ut, u∗) + 2(η

δ1)
−
M1||F /σ∗
1 and the assumption that δ1 is sufﬁciently small.

1 + η),

M

−

−

||

1

where ξ1 uses the fact that ˜r

Now we are ready to bound dist(vt+1, v∗) as

≥

M

(η ||

1

δ1

M1||F

−
˜rσ∗
1

+ η)

1
10

p

1
576(1

dist(vt+1, v∗) =

1

vt+1, v∗

− h

2 =
i

ˆvt+1, v∗
h
2 +

ˆvt+1, v∗
h
p
M
||

⊥i
Mr||F /σ∗

−

⊥i
ˆvt+1, v∗
h
1 + η),

p
1
2

ξ1

≤

dist(ut, u∗) + 4(η

ˆvt+1, v∗
⊥i
h
ˆvt+1, v∗
i
h

2 ≤
i

where ξ1 follows from substituting Eqs. (16) and (17). Rescaling η as η/4 gives the desired bound of
Lemma C.2 for the rank-1 case. Rank-r proof can be obtained by following a similar framework.

Bounding vt+1
In this step, we need to prove that the j-th entry of vt+1 satisﬁes

:

j

vt+1
j
|

| ≤

c1

||Bj||
||B||F

for all j, under the

assumption that ut satisﬁes the norm bound

||Ai||
||A||F
The proof follows very closely to the second part of proving Lemma C.3 in [3], except that an extra
Mij using Bernstein inequality. More
Mij. Note that if ˆqij = 1, then δij = 1, Xi = 0, so we only need to

multiplicative term (1 + ǫ) will show up when bounding
ˆqij)wijut
speciﬁcally, let Xi = (δij −
i
consider the case when ˆqij < 1, i.e., ˆqij = qij, where qij is deﬁned in Eq.(1).

i δijwijut
i

ut
i| ≤
|

for all i.

P

f

c1

Suppose Π is ﬁxed and its dimension satisﬁes k = Ω( log(n)

f

), then according to Lemma B.6, we have

ǫ2

that w.h.p. in n,

Hence, we have

Mij| ≤ |
|
f
M 2
ij
ˆqij
f

≤

ξ1

Mij|

+ ǫ

Ai|| · ||
||

Bj|| ≤

(1 + ǫ)

Ai|| · ||
||

,
Bj||

(i, j).

∀

m

2

(1 + ǫ)2
2
Bj||
Ai||
||
||
+ ||Bj||2
( ||Ai||2
2n||B||2
2n||A||2
·
F
F
c2
(ut
i)2
Ai||
1||
( ||Ai||2
ˆqij
2n||A||2
F

m

≤

ξ2

·

2n(1 + ǫ)2
m

) ≤

Bj||

2

A

2
F ,
||

||

· ||

2/
2
A
F
||
||
+ ||Bj||2
2n||B||2
F

) ≤

2nc2
1
m

,

(16)

(17)

(18)

(19)

(20)

(21)

where ξ1 follows from substituting Eqs.(19) and (1), and ξ2 follows from the assumption that
c1||

/
Ai||
||

||F .
A

ut
i| ≤
|

21

We can now bound the ﬁrst and second moments of Xi as

Xi| ≤ |
|

wijut
i

(ut
i)2
ˆqij s

M 2
ij
ˆqij
f

Mij| ≤ s
f

ξ1

≤

2nc1(1 + ǫ)
m

||F .
A
Bj||||
||

V ar(Xi) =

ˆqij(1

ˆqij)w2

ij(ut

i)2

M 2

ij ≤

(1 + ǫ)2

2

Ai||
||

Bj||
||

2

Xi

−

Xi
2nc2

1(1 + ǫ)2

m

ξ2

≤

2

Bj||
||

A

||

f
2
F ,
||

(ut
i)2
ˆqij

Xi

where ξ1 and ξ2 follows from substituting Eqs.(20) and (21).

The rest proof involves applying Bernstein’s inequality to derive a high-probability bound on

i Xi,
which is almost the same as the second part of proving Lemma C.3 in [3], so we omit the details here. The
only difference is that, because of the extra multiplicative term (1 + ǫ) in the bound of the ﬁrst and second
moments, the lower bound on the sample complexity m should also be multiplied by an extra (1 + ǫ)2 term.
By restricting 0 < ǫ < 1/2, this extra multiplicative term can be ignored as long as the original lower bound
of m contains a large enough constant.

P

C.4 Proof of Theorem 3.1

We now prove our main theorem for rank-1 case here. Rank-r proof follows a similar line of reasoning and
can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3].
vt+1 to denote the t-th and (t+1)-th step iterates (which are
Similar to the previous section, we use
column vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized
vectors.

ut and

b

b

The closed form solution for WAltMin update at t + 1 iteration is

ut
||

vt+1
j = σ∗
||

1v∗
j

i δijwijut
i δijwij(ut

iu∗
i
i)2 +

i δijwijut
i(
M
M1)ij
−
i δijwij(ut
i)2

.

P

f

P

Writing in matrix form, we get

b

b

ut
||

vt+1
j = σ∗
1h
||

u∗, ut

v∗
i

where B and C are diagonal matrices with Bjj =
vector RΩ(

M1)T ut with entries yj =

M

b

b

Each term of Eq.(22) can be bounded as follows.

−

f

C)v∗ + B−1y,

−

u∗, ut
1B−1(
σ∗
h
i δijwij(ut
i δijwijut
M
i(
P

B
i
−
i)2 and Cjj =
M1)ij.

−

P

i δijwijut

iu∗

i , and y is the

u∗, ut
(
h
||

B
i

P
C)v∗

|| ≤

f
dist(ut, u∗),

B−1
||

|| ≤

2,

y
||

||

=

RΩ(

M

||

−

M1)T ut

dist(ut, u∗)
||

M

M1||

−

+ η

M

||

M1||F /˜r + ησ∗
1,

−

where ξ1 follows directly from Lemma C.4. The proof of Eq.(23) is exactly the same as the proof of Lemma
B.3 and B.4 in [3].

f

According to Lemma C.2, since the distance is decreasing geometrically, after O(log( 1

ζ )) iterations we

get

(22)

(23)

(24)

(25)

dist(ut, u∗)

ζ + 2η

M

||

≤

M1||F /σ∗

−

1 + 2η.

P
P

−
ξ1

||

≤

22

Now we are ready to prove the spectral norm bound in Theorem 3.1:

M1 −
||
M1 −
(I
−

≤ ||

+

ut(
vt+1)T
||
ut(ut)T M1||
b
b
ut(ut)T )M1||
v∗
σ1h
||
i
u∗, ut
1B−1(
σ∗
h
||

ut(ut)T M1 −
vt+1)T
ut(
||
||
vt+1)T ]
ut
ut[(ut)T M1 − ||
(
||
||
||
b
b
vt+1)T
ut, u∗
(
b
b
||
C)v∗
b

− ||

ut

+

||

≤ ||
ξ1
1dist(ut, u∗) +
σ∗

1dist(ut, u∗) +
σ∗

1dist(ut, u∗) + 2σ∗
σ∗

5(ζσ∗
≤
= 5ζσ∗

1 + 2η
||
1 + 12η

M

M

||

−

−

||

+

−

B−1y
B
b
i
||
1dist(ut, u∗) + 2dist(ut, u∗)
||
M1||F + 2ησ∗
M1||F + 12ησ∗

||
M1||
−
M1||F + 2ησ∗

1) + 2η

M

M

−

||

1

1

≤
ξ2

≤
ξ3

≤
ξ4

+ 2η

M

||

M1||F /˜r + 2ησ∗

1

−

(26)

v∗,
where ξ1 follows from the deﬁnition of dist(ut, u∗), the fact that
= 1, and (ut)T M1 = σ1h
i
ξ2 follows from substituting Eq.(22), ξ3 follows from Eqs.(23) and (24), and ξ4 follows from the Eq.(25),
and fact that
1) (this will inﬂuence the number of iterations)
and also rescaling η to η/12 gives us the desired spectral norm error bound in Eq.(7). This completes our
proof of the rank-1 case. Rank-r proof follows a similar line of reasoning and can be obtained by combining
the current proof with the rank-r analysis in the original proof of LELA [3].

1. Rescaling ζ to ζ/(5σ∗

M1|| ≤

σ∗
1, ˜r

ut, u∗

ut
||

M

≥

−

||

||

C.5 Sampling

We describe a way to sample m elements in O(m log(n)) time using distribution qij deﬁned in Eq. (1).
Naively one can compute all the n2 entries of min
and toss a coin for each entry, which takes O(n2)
qij, 1
}
{
time. Instead of this binomial sampling we can switch to row wise multinomial sampling. For this, ﬁrst
estimate the expected number of samples per row mi = m( ||Ai||2
2n ). Now sample m1 entries from row
2||A||2
F
1 according to the multinomial distribution,

+ 1

m
m1 ·

( ||
2n

2

A1||
2
A
F
||
||

+ ||
2n

2

Bj||
2
B
F
||
||

) =

||A1||2
2n||A||2
F
||Ai||2
2||A||2
F

+ ||Bj||2
2n||B||2
F
+ 1
2n

.

q1j =

e

j

e

P

Note that
q1j = 1. To sample from this distribution, we can generate a random number in the interval
[0, 1], and then locate the corresponding column index by binary searching over the cumulative distribution
function (CDF) of
q1j. This takes O(n) time for setting up the distribution and O(m1 log(n)) time to
sample. For subsequent row i, we only need O(mi log(n)) time to sample mi entries. This is because for
binary search to work, only O(mi log(n)) entries of the CDF vector needs to be computed and checked.
Note that the speciﬁc form of
qij ensures that its CDF entries can be updated in an efﬁcient way (since we
only need to update the linear shift and scale). Hence, sampling m elements takes a total O(m log(n)) time.
Furthermore, the error in this model is bounded up to a factor of 2 of the error achieved by the Binomial
model [7] [21]. For more details please see our Spark implementation.

e

e

D Related work

Approximate matrix multiplication:

In the seminal work of [14], Drineas et al. give a randomized algorithm which samples few rows of A and
B and computes the approximate product. The distribution depends on the row norms of the matrices and

23

ǫ

||

||

A

A

B

B

−

−

||2||

||F ||

||2 ≤

AT B
||

AT B
||

For spectral norm bound of the form

||F . Later Sarlos [29] propose a sketching
the algorithm achieves an additive error proportional to
based algorithm, which computes sketched matrices and then outputs their product. The analysis for this
˜AT ˜B
algorithm is then improved by [10]. All of these results compare the error
||F in Frobenius
norm.
||2, the authors in [29, 11] show that
C
the sketch size needs to satisfy O(r/ǫ2), where r = rank(A) + rank(B). This dependence on rank is later
improved to stable rank in [26], but at the cost of a weaker dependence on ǫ. Recently, Cohen et al. [12]
further improve the dependence on ǫ and give a bound of O(˜r/ǫ2), where ˜r is the maximum stable rank. Note
that the sketching based algorithm does not output a low rank matrix. As shown in Figure 2, rescaling by
the actual column norms provide a better estimator than just using the sketched matrices. Furthermore, we
show that taking SVD on the sketched matrices gives higher error rate than our algorithm (see Figure 3(b)).
Low rank approximation: [16] introduced the problem of computing low rank approximation of a
given matrix using only few passes over the data. They gave an algorithm that samples few rows and
columns of the matrix and computes its SVD for low rank approximation. They show that this algorithm
achieves additive error guarantees in Frobenius norm. [15, 29, 19, 13] have later developed algorithms using
various sketching techniques like Gaussian projection, random Hadamard transform and volume sampling
that achieve relative error in Frobenius norm.[35, 28, 18, 6] improved the analysis of these algorithms and
provided error guarantees in spectral norm. More recently [11] presented an algorithm based on subspace
embedding that computes the sketches in the input sparsity time.

Another class of methods use entrywise sampling instead of sketching to compute low rank approxi-
mation. [1] considered an uniform entrywise sampling algorithm followed by SVD to compute low rank
approximation. This gives an additive approximation error. More recently [3] considered biased entrywise
sampling using leverage scores, followed by matrix completion to compute low rank approximation. While
this algorithm achieves relative error approximation, it takes two passes over the data.

There is also lot of interesting work on computing PCA over streaming data under some statistical
assumptions, e.g., [2, 27, 5, 30]. In contrast, our model does not put any assumptions on the input matrix.
Besides, our goal here is to get a low rank matrix and not just the subspace.

24

