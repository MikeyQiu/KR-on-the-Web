Sequicity: Simplifying Task-oriented Dialogue Systems with Single
Sequence-to-Sequence Architectures

Wenqiang Lei‡∗, Xisen Jin§∗, Zhaochun Ren†, Xiangnan He‡, Min-Yen Kan‡, Dawei Yin†
‡National University of Singapore, Singapore
§Fudan University, Shanghai, China
†Data Science Lab, JD.com, Beijing, China

{wenqianglei,xisenjin}@gmail.com

renzhaochun@jd.com

kanmy@comp.nus.edu.sg xiangnanhe@gmail.com yindawei@acm.org

Abstract

Existing solutions to task-oriented dia-
logue systems follow pipeline designs
which introduce architectural complex-
ity and fragility. We propose a novel,
holistic, extendable framework based on
a single sequence-to-sequence (seq2seq)
model which can be optimized with su-
pervised or reinforcement learning. A
key contribution is that we design text
spans named belief spans to track dia-
logue believes, allowing task-oriented dia-
logue systems to be modeled in a seq2seq
way. Based on this, we propose a sim-
plistic Two Stage CopyNet instantiation
which demonstrates good scalability: sig-
niﬁcantly reducing model complexity in
terms of number of parameters and train-
ing time by an order of magnitude.
It
signiﬁcantly outperforms state-of-the-art
pipeline-based methods on two datasets
and retains a satisfactory entity match rate
on out-of-vocabulary (OOV) cases where
pipeline-designed competitors totally fail.

1

Introduction

The challenge of achieving both task comple-
tion and human-like response generation for task-
oriented dialogue systems is gaining research in-
terest. Wen et al. (2017b, 2016a, 2017a) pioneered
a set of models to address this challenge. Their
proposed architectures follow traditional pipeline
designs, where the belief tracking component is
the key component (Chen et al., 2017).

In the current paradigm, such a belief tracker
builds a complex multi-class classiﬁer for each

* Work performed during an internship at Data Science

Lab, JD.com.

slot (See §3.2) which can suffer from high com-
plexity, especially when the number of slots and
their values grow. Since all the possible slot values
have to be pre-deﬁned as classiﬁcation labels, such
trackers also cannot handle the requests that have
out-of-vocabulary (OOV) slot values. Moreover,
the belief tracker requires delexicalization, i.e., re-
placing slot values with their slot names in utter-
ances (Mrkˇsi´c et al., 2017). It does not scale well,
due to the lexical diversity. The belief tracker also
needs to be pre-trained, making the models unre-
alistic for end-to-end training (Eric and Manning,
2017a). While Eric and Manning (2017a,b) inves-
tigated building task-oriented dialogue systems by
using a seq2seq model, unfortunately, their meth-
ods are rather preliminary and do not perform well
in either task completion or response generation,
due to their omission of a belief tracker.

Questioning the basic pipeline architecture, in
this paper, we re-examine the tenets of belief
tracking in light of advances in deep learning. We
introduce the concept of a belief span (bspan), a
text span that tracks the belief states at each turn.
This leads to a new framework, named Sequicity,
with a single seq2seq model. Sequicity decom-
poses the task-oriented dialogue problem into the
generation of bspans and machine responses, con-
verting this problem into a sequence optimization
problem.
In practice, Sequicity decodes in two
stages: in the ﬁrst stage, it decodes a bspan to fa-
cilitate knowledge base (KB) search; in the sec-
ond, it decodes a machine response on the con-
dition of knowledge base search result and the
bspan.

Our method represents a shift in perspective
compared to existing work. Sequicity employs a
single seq2seq model, resulting in a vastly simpli-
ﬁed architecture. Unlike previous approaches with
an overly parameterized delexicalization-based
belief tracker, Sequicity achieves much less train-

ing time, better performance on larger a dataset
and an exceptional ability to handle OOV cases.
Furthermore, Sequicity is a theoretically and aes-
thetically appealing framework, as it achieves true
end-to-end trainability using only one seq2seq
model. As such, Sequicity leverages the rapid
development of seq2seq models (Gehring et al.,
2017; Vaswani et al., 2017; Yu et al., 2017) in de-
veloping solutions to task-oriented dialogue sce-
narios.
In our implementation, we improve on
CopyNet (Gu et al., 2016) to instantiate Sequic-
ity framework in this paper, as key words present
in bspans and machine responses recur from previ-
ous utterances. Extensive experiments conducted
on two benchmark datasets verify the effectiveness
of our proposed method.

Our contributions are fourfold:

(1) We pro-
pose the Sequicity framework, which handles both
task completion and response generation in a sin-
gle seq2seq model; (2) We present an implemen-
tation of the Sequicity framework, called Two
Stage CopyNet (TSCP), which has fewer number
of parameters and trains faster than state-of-the-art
baselines (Wen et al., 2017b, 2016a, 2017a); (3)
We demonstrate that TSCP signiﬁcantly outper-
forms state-of-the-art baselines on two large-scale
datasets, inclusive of scenarios involving OOV; (4)
We release source code of TSCP to assist the com-
munity to explore Sequicity1.

2 Related Work

Historically,
task-oriented dialog systems have
been built as pipelines of separately trained mod-
ules. A typical pipeline design contains four com-
ponents: 1) a user intent classiﬁer, 2) a belief
tracker, 3) a dialogue policy maker and a 4) re-
sponse generator. User intent detectors classify
user utterances to into one of the pre-deﬁned in-
tents. SVM, CNN and RNN models (Silva et al.,
2011; Hashemi et al., 2016; Shi et al., 2016) per-
form well for intent classiﬁcation. Belief track-
ers, which keep track of user goals and constraints
every turn (Henderson et al., 2014a,b; Kim et al.,
2017) are the most important component for task
accomplishment. They model the probability dis-
tribution of values over each slot (Lee, 2013).
Dialogue policy makers then generate the next
available system action. Recent experiments sug-
gest that reinforcement learning is a promising
paradigm to accomplish this task (Young et al.,

1http://github.com/WING-NUS/sequicity

2013a; Cuay´ahuitl et al., 2015; Liu and Lane,
2017), when state and action spaces are carefully
designed (Young et al., 2010). Finally, in the
response generation stage, pipeline designs usu-
ally pre-deﬁne ﬁxed templates where placehold-
ers are ﬁlled with slot values at runtime (Dhin-
gra et al., 2017; Williams et al., 2017; Henderson
et al., 2014b,a). However, this causes rather static
responses that could lower user satisfaction. Gen-
erating a ﬂuent, human-like response is considered
a separate topic, typiﬁed by the topic of conversa-
tion systems (Li et al., 2015).

3 Preliminaries

3.1 Encoder-Decoder Seq2seq Models

Current seq2seq models adopt encoder–decoder
structures. Given a source sequence of tokens
X = x1x2...xn, an encoder network represent X
as hidden states: H(x) = h(x)
n . Based
on H(x), a decoder network generates a target se-
quence of tokens Y = y1y2...ym whose likelihood
should be maximized given the training corpus.

2 ...h(x)

1 h(x)

As of late, the recurrent neural network with
attention (Att-RNN) is now considered a base-
line encoder–decoder architecture. Such networks
employ two (sometimes identical) RNNs, one for
encoding (i.e., generating H(x)) and another for
decoding. Particularly, for decoding yj, the de-
coder RNN takes the embedding yj−1 to gener-
ate a hidden vector h(y)
. Afterwards, the de-
j
coder attends to X: calculating attention scores
i ∈ H(x) and h(y)
between all h(x)
(Eq. (1)), and
j
then sums all h(x)
, weighted by their correspond-
ing attention scores (Eqs. (2)). The summed result
˜h(x)
as a single vector which is
j
mapped into an output space for a sof tmax oper-
ation (Eq. (3)) to decode the current token:

concatenates h(y)

j

i

uij = vT tanh(W1h(x)

i + W2h(y)
j )

˜h(x)
j =

n
(cid:88)

i=1

euij
i euij

(cid:80)

h(x)
i

yj = sof tmax(O

(cid:34) ˜h(x)
j
h(y)
j

(cid:35)

)

(1)

(2)

(3)

where v ∈ R1×l; W1, W2 ∈ Rl×d and O ∈
R|V |×d. d is embedding size and V is vocabulary
set and |V | is its size.

3.2 Belief Trackers

In the multi-turn scenarios, a belief tracker is the
key component for task completion as it records
key information from past
turns (Wen et al.,
2017b; Henderson et al., 2013, 2014a,b). Early
belief trackers are designed as Bayesian networks
where each node is a dialogue belief state (Paek
and Horvitz, 2000; Young et al., 2013b). Recent
work successfully represents belief trackers as dis-
criminative classiﬁers (Henderson et al., 2013;
Williams, 2012; Wen et al., 2017b).

Wen et al. (2017b) apply discrimination ap-
proaches (Henderson et al., 2013) to build one
classiﬁer for each slot in their belief tracker. Fol-
lowing the terminology of (Wen et al., 2017b),
a slot can be either informable or requestable,
which have been annotated in CamRes676 and
KVRET. Individually, an informable slot, speci-
ﬁed by user utterances in previous turns, is set to
a constraint for knowledge base search; whereas
a requestable slot records the user’s need in the
current dialogue. As an example of belief track-
ers in CamRes676, food type is an informable
slot, and a set of food types is also predeﬁned (e.g.,
Italian) as corresponding slot values. In (Wen
et al., 2017b), the informable slot food type is
recognized by a classiﬁer, which takes user utter-
ances as input to predict if and which type of food
should be activated, while the requestable slot of
address is a binary variable. address will be
set to true if the slot is requested by the user.

4 Method

We now describe the Sequicity framework, by ﬁrst
explaining the core concept of bspans. We then
instantiate the Sequicity framework with our intro-
duction of an improved CopyNet (Gu et al., 2016).

4.1 Belief Spans for Belief Tracking

The core of belief tracking is keeping track of in-
formable and requestable slot values when a di-
alogue progresses.
In the era of pipeline-based
methods, supervised classiﬁcation is a straightfor-
ward solution. However, we observe that this tra-
ditional architecture can be updated by applying
seq2seq models directly to the problem. In con-
trast to (Wen et al., 2017b) which treats slot val-
ues as classiﬁcation labels, we record them in a
text span, to be decoded by the model. This lever-
ages the state-of-the-art neural seq2seq models to
learn and dynamically generate them. Speciﬁcally,

our bspan has an information ﬁeld (marked with
<Inf></Inf>) to store values of informable
slots since only values are important for knowl-
edge base search. Bspans can also feature a re-
quested ﬁeld (marked with <Req></Req>), stor-
ing requestable slot names if the corresponding
value is True.

At turn t, given the user utterance Ut, we show
an example of both bspan Bt and machine re-
sponse Rt generation in Figure 1, where annotated
slot values at each turn are decoded into bspans.
B1 contains an information slot Italian be-
cause the user stated “Italian food” in U1. During
the second turn, the user adds an additional con-
straint cheap resulting in two slot values in B2’s
information ﬁeld. In the third turn, the user further
asks for the restaurant’s phone and address, which
are stored in requested slots of B3.
Our bspan solution is concise:

it simpliﬁes
multiple sophisticated classiﬁers with a single se-
quence model. Furthermore, it can be viewed as
an explicit data structure that expedite knowledge
base search as its format is ﬁxed: following (Wen
et al., 2017b), we use the informable slots values
directly for matching ﬁelds of entries in databases.

4.2 The Sequicity Framework

We make a key observation that at turn t, a sys-
tem only needs to refer to Bt−1, Rt−1 and Ut to
generate a new belief span Bt and machine re-
sponse Rt, without appealing to knowing all past
utterances. Such Markov assumption allows Se-
quicity to concatenate Bt−1, Rt−1 and Ut (de-
noted as Bt−1Rt−1Ut) as a source sequence for
seq2seq modeling, to generate Bt and Rt as tar-
get output sequences at each turn. More for-
mally, we represent the dialogue utterances as
{(B0R0U1; B1R1); (B1R1U2; B2R2); ...; (Bt−1
Rt−1Ut; BtRt)} where B0 and R0 are initialized
as empty sequences.
In this way, Sequicity ful-
ﬁlls both task accomplishment and response gen-
eration in an uniﬁed seq2seq model. Note that we
process Bt and Rt separately, as the belief state
Bt depends only on Bt−1Rt−1Ut, while the re-
sponse Rt is additionally conditioned on Bt and
the knowledge base search results (denoted as kt);
that is, Bt informs the Rt’s contents. For example,
Rt must include all the request slots from Bt when
communicating the entities fulﬁlling the requests
found in the knowledge base. Here, kt helps gen-
erate Rt pragmatically.

Figure 1: Sequicity overview. The left shows a sample dialogue; the right illustrates the Sequicity. Bt
is employed only by the model, and not visible to users. During training, we substitute slot values
with placeholders bearing the slot names for machine response. During testing, this is inverted: the
placeholders are replaced by actual slot values, according to the item selected from the knowledge base.

Generally, kt has three possibilities: 1) multiple
matches, 2) exact match and 3) no match, while
the machine responses differ accordingly. As an
example, let’s say a user requests an Italian restau-
rant. In the scenario of multiple matches, the sys-
tem should prompt for additional constraints for
disambiguation (such as restaurant price range).
In the second exact match scenario where a sin-
gle target (i.e., restaurant) has been found, the sys-
tem should inform the user their requested infor-
mation (e.g., restaurant address).
If no entity is
obtained, the system should inform the user and
perhaps generate a cooperative response to retry a
different constraint.

We thus formalize Sequicity as a seq2seq model
which encodes Bt−1Rt−1Ut jointly, but decodes
Bt and Rt separately, in two serial stages. In the
ﬁrst stage, the seq2seq model decodes Bt uncondi-
tionally (Eq. 4a). Once Bt obtained, the decoding
pauses to perform the requisite knowledge base
search based on Bt, resulting in kt. Afterwards,
the seq2seq model continues to the second decod-
ing stage, where Rt is generated on the additional
conditions of Bt and kt (Eq. 4b).

Bt = seq2seq(Bt−1Rt−1Ut|0, 0)
Rt = seq2seq(Bt−1Rt−1Ut|Bt, kt)

(4a)

(4b)

Sequicity is a general framework suitably imple-
mented by any of the various seq2seq models.
The additional modeling effort beyond a general

seq2seq model is to add the conditioning on Bt
and kt to decode the machine response Rt. For-
tunately, natural language generation with spe-
ciﬁc conditions has been extensively studied (Wen
et al., 2016b; Karpathy and Fei-Fei, 2015; Mei
et al., 2016) which can be employed within this
framework.

4.3 Sequicity Instantiation: A Two Stage

CopyNet

Although there are many possible instantiations,
in this work we purposefully choose a simplistic
architecture, leaving more sophisticated modeling
for future work. We term our instantiated model
a Two Stage CopyNet (TSCP). We denote the ﬁrst
m(cid:48) tokens of target sequence Y are Bt and the rests
are Rt, i.e. Bt = y1...ym(cid:48) and Rt = ym(cid:48)+1...ym.

Two-Stage CopyNet. We choose to improve
upon CopyNet (Gu et al., 2016) as our seq2seq
model. This is a natural choice as we observe
that target sequence generation often requires the
copying of tokens from the input sequence. Let’s
discuss this in more detail. From a probabilis-
tic point of view, the traditional encoder–decoder
structure learns a language model. To decode yj,
we can employ a sof tmax (e.g., Eq. 3) to calcu-
late the probability distribution over V i.e., P g
j (v)
where v ∈ V , and then choose the token with
the highest generation probability. However, in
our case, tokens in the target sequence Y might

be exactly copied from the input X (e.g., “Ital-
ian”). These copied words need to be explicitly
modeled. CopyNet (Gu et al., 2016) is a natural ﬁt
here, as it enlarges the decoding output space from
V to V ∪ X. For yj, it considers an additional
copy probability P c
j (v), indicating the likelihood
of yj copied from v ∈ X. Following (Gu et al.,
2016), the simple summation of both probabilities
Pj(v) = P g
j (v), v ∈ V ∪ X is treated as
the ﬁnal probability in the original paper.

j (v) + P c

In Sequicity, simply applying original CopyNet
architecture is insufﬁcient, since Bt and Rt have
different distributions. We here employ two sep-
arate RNN (GRU in our implementation) in de-
coder: one for Bt and the other for Rt. In the ﬁrst
decoding stage, we have a copy-attention mecha-
nism on X to decode Bt; then calculate the gener-
ation probability through attending to X as intro-
duced in Sec 3.1, as well as the copy probability
for each word v ∈ X following (Gu et al., 2016)
by Eq. 5:

P c

j (v) =

eψ(xi), j (cid:54) m(cid:48)

(5)

1
Z

|X|
(cid:88)

i:xi=v

In contrast to recent work (Eric and Manning,
2017a) that also employs a copy-attention mech-
anism to generate a knowledge-base search API
and machine responses, our proposed method ad-
vances in two aspects: on one hand, bspans
reduce the search space from U1R1...UtRt
to
Bt−1Rt−1Ut by compressing key points for the
task completion given past dialogues; on the other
hand, because bspans revisit context by only han-
dling the Bt with a ﬁxed length, the time com-
plexity of TSCP is only O(T ), comparing O(T 2)
in (Eric and Manning, 2017a).

Involving kt when decoding Rt. As kt has
three possible values: obtaining only one, multi-
ple or no entities. We let kt be a vector of three
dimensions, one of which signals a value. We ap-
pend kt to the embeddings yj , as shown in Eq. (9)
that is fed into an GRU for generating h(y)
j+1. This
approach is also referred to as Language Model
Type condition (Wen et al., 2016b)

y(cid:48)

j =

(cid:21)

(cid:20) yj
kt

, j ∈ [m(cid:48) + 1, m]

(9)

where Z is a normalization term and ψ(xi) is the
score of “copying” word xi and is calculated by:

4.4 Training

ψ(xi) = σ(h(x)

i

Wc)h(y)
j

, j (cid:54) m(cid:48)

T

(6)

where Wc ∈ Rd×d.

In the second decoding stage (i.e., decoding
Rt), we apply the last hidden state of Bt as the ini-
tial hidden state of the Rt GRU. However, as we
need to explicitly model the dependency on Bt, we
have copy-attention mechanism on Bt instead of
on X: treating all tokens of Bt as the candidate for
copying and attention. Speciﬁcally, we use hidden
state generated by Bt GRU, i.e., h(y)
m(cid:48) , to
calculate copying using Eqs. 7 and 8 and attention
score as introduced in Sec 3.1. It helps to reduce
search space because all key information of X for
task completion has been included in Bt.

1 , ..., h(y)

P c

j (v) =

1
Z

(cid:88)

i:yi=v

eψ(yi), i (cid:54) m(cid:48) < j (cid:54) m (7)

ψ(yi) = σ(h(y)

i

Wc)h(y)
j

T

, i (cid:54) m(cid:48) < j (cid:54) m (8)

The standard cross entropy is adopted as our ob-
jective function to train a language model:

m
(cid:88)

j=1

yjlogPj(yj)

(10)

In response generation, every token is treated
equally. However, in our case, tokens for task
completion are more important. For example,
when a user asks for the address of a restau-
rant, it matters more to decode the placeholder
<address> than decode words for language ﬂu-
ency. We can employ reinforcement learning to
ﬁne tune the trained response decoder with an em-
phasis to decode those important tokens.

Inspired by (Wen et al., 2017a), in the context
of reinforcement learning, the decoding network
can be viewed as a policy network, denoted as
πΘ(yj) for decoding yj (m(cid:48) + 1 (cid:54) j (cid:54) m). Ac-
cordingly, the choice of word yj is an action and
its hidden vector generated by decoding GRU is
the corresponding state. In reinforcement tuning
stage, the trained response decoder is the initial
policy network. By deﬁning a proper reward func-
tion r(j) for decoding yj, we can update the trained

Dataset
Size
Domains
Slot types
Distinct slot values
Dataset
Size
Domains
Slot types
Distinct slot values

Cam676
Train:408 / Test: 136 / Dev: 136
restaurant reservation
price, food style etc.
99
KVRET
Train:2425 / Test: 302 / Dev: 302
POI
calendar
poi, etc.
date, etc.
140
79

weather info.
location, etc.
65

Table 1: Dataset demographics. Following the
respective literature, Cam676 is split 3:1:1 and
KVRET is split 8:1:1, into training, developing
and testing sets, respectively.

response model with policy gradient:

1
m − m(cid:48)

m
(cid:88)

j=m(cid:48)+1

r(j) ∂logπΘ(yj)
∂Θ

(11)

where r(j) = r(j) + λr(j+1) + λ2r(j+2) +
... + λm−j+1r(m). To encourage our generated re-
sponse to answer the user requested information
but avoid long-winded response, we set the reward
at each step r(j) as follows: once the placeholder
of requested slot has been decoded, the reward for
current step is 1; otherwise, current step’s reward
is -0.1. λ is a decay parameter. Sec 5.2 for λ set-
tings.

5 Experiments

We assess the effectiveness of Sequicity in three
aspects: the task completion, the language qual-
ity, and the efﬁciency. The evaluation metrics are
listed as follows:

· BLEU to evaluate the language quality (Papineni
et al., 2002) of generated responses (hence top-1
candidate in (Wen et al., 2017b)).
· Entity match rate evaluates task completion.
According to (Wen et al., 2017b), it determines
if a system can generate all correct constraints to
search the indicated entities of the user. This met-
ric is either 0 or 1 for each dialogue.
· Success F1 evaluates task completion and is
modiﬁed from the success rate in (Wen et al.,
2017b, 2016a, 2017a). The original success rate
measures if the system answered all the requested
information (e.g. address, phone number). How-
ever, this metric only evaluates recall. A system
can easily achieve a perfect task success by always
responding all possible request slots. Instead, we
here use success F1 to balance both recall and pre-

cision. It is deﬁned as the F1 score of requested
slots answered in the current dialogue.
· Training time. The training time is important for
iteration cycle of a model in industry settings.

5.1 Datasets

We adopt the CamRest676 (Wen et al., 2017a)
and KVRET (Eric and Manning, 2017b) datasets.
Both datasets are created by a Wizard-of-Oz (Kel-
ley, 1984) method on Amazon Mechanical Turk
platform, where a pair of workers are recruited
to carry out a ﬂuent conversation to complete an
assigned task (e.g. restaurant reservation). Dur-
ing conversation, both informable and requestable
slots are recorded by workers.

CamRest676’s dialogs are in the single domain
of restaurant searching, while KVRET is broader,
containing three domains: calendar scheduling,
weather information retrieval and point of inter-
est (POI) Navigation. Detailed slot information in
each domain are shown in Table 1. We follow the
data splits of the original papers as shown in 1.

5.2 Parameter Settings

For all models, the hidden size and the embedding
size d is set to 50.
|V | is 800 for CamRes676
and 1400 for KVRET. We train our model with
an Adam optimizer (Kingma and Ba, 2015), with
a learning rate of 0.003 for supervised training and
0.0001 for reinforcement learning. Early stopping
is performed on developing set. In reinforcement
learning, the decay parameter λ is set to 0.8. We
also use beam search strategy for decoding, with a
beam size of 10.

5.3 Baselines and Comparisons

We ﬁrst compare our model with the state-of-the-
art baselines as follow:

• NDM (Wen et al., 2017b). As described in
Sec 1, it adopts pipeline designs with a be-
lief tracker component depending on delexi-
calization.

• NDM+Att+SS. Based on the NDM model, an
additional attention mechanism is performed
on the belief trackers and a snapshot learning
mechanism (Wen et al., 2016a) is adopted.

• LIDM (Wen et al., 2017a). Also based on
NDM, this model adopts neural variational
inference with reinforcement learning.

CamRes676

Mat.
(1) NDM
0.904
(2) NDM + Att + SS 0.904
0.912
(3) LIDM
N/A
(4) KVRN
0.927
(5) TSCP
0.851
(6) Att-RNN
(7) TSCP\kt
0.927
(8) TSCP\RL
0.927
(9) TSCP\Bt
0.888

BLEU Succ. F1 T imef ull T imeN.B. Mat.
0.724
91.9 min
0.212
0.724
93.7 min
0.240
0.721
97.7 min
0.246
0.459
21.4 min
0.134
0.845
0.253
7.3 min
0.805
7.2 min
0.248
0.845
7.2 min
0.232
0.845
4.1 min
0.234
0.628
22.9 min
0.197

8.6 min
10.4 min
14.4 min
–
–
–
–
–
–

0.832
0.836
0.840
N/A
0.854
0.774
0.835
0.834
0.809

KVRET

BLEU Succ. F1 T imef ull
285.5 min
0.186
289.7 min
0.188
312.8 min
0.173
46.9 min
0.184
0.219
25.5 min
23.0 min
0.208
25.3 min
0.168
17.5 min
0.191
42.7 min
0.182

0.741
0.745
0.762
0.540
0.811
0.801
0.759
0.774
0.755

T imeN.B.
29.3 min
33.5 min
56.6 min
–
–
–
–
–
–

Table 2: Model performance on CamRes676 and KVRET. This table is split into two parts: competitors
on the upper side and our ablation study on the bottom side. Mat. and Succ. F1 are for match rate and
success F1 respectively. Timefull column reports training time till converge. For NDM, NDM+Att+SS
and LIDM, we also calculate the training time for the rest parts except for the belief tracker (TimeN.B.).

• KVRN (Eric and Manning, 2017b) uses one
seq2seq model to generate response as well
as interacting with knowledge base. How-
ever, it does not incorporate a belief tracking
mechanism.

For NDM, NDM+Att+SS, LIDM, we run the
source code released by the original authors2. For
KVRN, we replicate it since there is no source
code available. We also performed an ablation
study to examine the effectiveness of each com-
ponent.

• TSCP\kt. We removed the conditioning on

kt when decoding Rt.

• TSCP\RL. We removed reinforcement learn-
ing which ﬁne tunes the models for response
generation.

• Att-RNN. The standard seq2seq baseline
as described in the preliminary section
(See §3.1).

• TSCP\Bt. We removed bspans for dialogue
state tracking. Instead, we adopt the method
in (Eric and Manning, 2017a): concatenat-
ing all past utterance in a dialogue into a
CopyNet to generate user information slots
for knowledge base search as well as machine
response.

5.4 Experimental Results

As shown in Table 2, TSCP outperforms all base-
lines (Row 5 vs. Rows 1–4) in task completion
(entity match rate, success F1) and language qual-
ity (BLEU). The more signiﬁcant performance of
TSCP in KVRET dataset indicates the scalability

2https://github.com/shawnwun/NNDIAL

of TSCP. It is because KVRET dataset has sig-
niﬁcant lexical variety, making it hard to perform
delexicalization for Wen et al.’s model (Rows 1–
3)3. However, CamRes676 is relatively small with
simple patterns where all systems work well. As
predicted, KVRN (Row 4) performs worse than
TSCP (Row 5) due to lack of belief tracking.

Compared with Wen et al.’s models (Rows 1–3),
TSCP takes a magnitude less time to train. Al-
though TSCP is implemented in PyTorch while
Wen et al.’s models in Theano, such speed com-
parison is still valid, as the rest of the NDM model
— apart from its belief tracker — has a compara-
ble training speed to TSCP (7.3 mins vs. 8.6 mins
on CamRes676 and 25.5 mins vs. 29.3 mins on
KVRET), where model complexities are similar.
The bottleneck in the time expense is due to belief
tracker training. In addition, Wen et al.’s models
perform better at the cost of more training time
(Rows 1, 2 and 3), suggesting the intrinsic com-
plexity of pipeline designs.

Importantly, ablation studies validate the ne-
cessity of bspans. With bspans, even a stan-
dard seq2seq model (Att-RNN, Row 6) beats so-
phisticated models such as attention copyNets
(TSCP\Bt, Row 9) in KVRET. Furthermore,
TSCP (Row 5) outperforms TSCP\Bt (Row 9) in
all aspects: task completion, language quality and
training speed. This validate our theoretical analy-
sis in Sec 4.3. Other components of TSCP are also
important. If we only use vanilla Attention-based
RNN instead of copyNet, all metrics for model
effectiveness decrease, validating our hypothesize
that the copied words need to be speciﬁcally mod-
eled. Secondly, BLEU score is sensitive to knowl-

3We use the delexicalization lexicon provided by the orig-

inal author of KVRET(Eric and Manning, 2017b)

5.6 Empirical Model Complexity

Traditional belief trackers like (Wen et al., 2017b)
are built as a multi-class classiﬁer, which mod-
els each individual slot and its corresponding val-
ues, introducing considerable model complexities.
This is especially severe in large datasets with a
number of slots and values. In contrast, Sequic-
ity reduces such a complex classiﬁer to a language
model. To compare the model complexities of two
approaches, we empirically measure model size.
We split KVRET dataset by their domains, result-
ing in three sub-datasets. We then accumulatively
add the sub-datasets into training set to examine
how the model size grows. We here selectively
present TSCP, NDM and its separately trained be-
lief tracker, since Wen et al.’s set of models share
similar model sizes.

edge base search result kt (Row 7 vs. Row 5).
By examining error cases, we ﬁnd that the system
is likely to generate common sentences like “you
are welcome” regardless of context, due to corpus
frequency. Finally, reinforcement learning effec-
tively helps both BLEU and success F1 although it
takes acceptable additional time for training.

5.5 OOV Tests

Previous work predeﬁnes all slot values in a be-
lief tracker. However, a user may request new at-
tributes that has not been predeﬁned as a classiﬁ-
cation label, which results in an entity mismatch.
TSCP employs copy mechanisms, gaining an in-
trinsic potential to handle OOV cases. To conduct
the OOV test, we synthesize OOV test instances
by adding a sufﬁx unk to existing slot ﬁllers. For
example, we change “I would like Chinese food”
into “I would like Chinese unk food.” We then
randomly make a proportion of testing data OOV
and measure its entity match rate. For simplicity,
we only show the three most representative mod-
els pre-trained in the in-vocabulary data: TSCP,
TSCP\Bt and NDM.

Figure 3: Model size sensitivity with respect to
KVRET. Distinct slot values of 79, 144, 284 cor-
respond to the number of slots in KVRET’s calen-
dar, calendar + weather info., and all 3 domains.

As shown in Figure 3, TSCP has a magni-
tude less number of parameters than NDM and
its model size is much less sensitive to distinct
slot values increasing.
It is because TSCP is a
seq2seq language model which has a approximate
linear complexity to vocabulary size. However,
NDM employs a belief tracker which dominates
its model size. The belief tracker is sensitive to the
increase of distinct slot values because it employs
complex structures to model each slot and corre-
sponding values. Here, we only perform empirical
evaluation, leaving theoretically complexity anal-
ysis for future works.

5.7 Discussions

In this section we discuss if Sequicity can tackle
inconsistent user requests , which happens when
users change their minds during a dialogue. Incon-
sistent user requests happen frequently and are dif-

(a) CamRes676

(b) KVRET

Figure 2: OOV tests. 0% OOV rate means no
OOV instance while 100% OOV rate means all in-
stances are changed to be OOV.

Compared with NDM, TSCP still performs well
when all slot ﬁllers are unknown. This is because
TSCP actually learns sentence patterns. For exam-
ple, CamRes676 dataset contains a frequent pat-
tern “I would like [food type] food” where the
[food type] should be copied in Bt regardless
what exact word it is. In addition, the performance
of TSCP\Bt decreases more sharply than TSCP as
more instances set to be OOV. This might be be-
cause handling OOV cases is much harder when
search space is large.

ﬁcult to tackle in belief tracking (Williams, 2012;
Williams et al., 2013). Unlike most of previous
pipeline-based work that explicitly deﬁnes model
actions for each situation, Sequicity is proposed to
directly handle various situations from the training
data with less manual intervention. Here, given
examples about restaurant reservation, we provide
three different scenarios to discuss:

• A user totally changes his mind. For ex-
ample, the user request a Japanese restaurant
ﬁrst and says “I dont want Japanese food any-
more, I’d like French now.” Then, all the slot
activated before should be invalid now. The
slot annotated for this turn is only French. Se-
quicity can learn this pattern, as long as it is
annotated in the training set.

• User requests cannot be found in the
KB (e.g., Japanese food). Then the sys-
tem should respond like “Sorry, there is no
Japanese food...”. Consequently, the user can
choose a different option: “OK, then French
food.” The activated slot Japanese will be
replaced as French, which our system can
learn. Therefore, an important pattern is the
machine-response (e.g., “there is no [XXX
constraint]”) in the immediate previous utter-
ance.

• Other cases. Sequicity is expected to gen-
erate both slot values in a belief span if it
doesn’t know which slot to replace. To main-
tain the belief span, we run a simple post-
processing script at each turn, which detects
whether two slot values have the same slot
name (e.g., food type) in a pre-deﬁned
slot name-value table. Then, such script only
keeps the slot value in the current turn of user
utterance. Given this script, Sequicity can ac-
curately discover the slot requested by a user
in each utterance. However, this script only
works when slot values are pre-deﬁned. For
inconsistent OOV requests, we need to build
another classiﬁer to recognize slot names for
slot values.

To sum up, Sequicity, as a framework, is able
to handle various inconsistent user input despite
its simple design. However, detailed implementa-
tions should be customized depends on different
applications.

6 Conclusion

We propose Sequicity, an extendable framework,
which tracks dialogue believes through the decod-
ing of novel text spans: belief spans. Such belief
spans enable a task-oriented dialogue system to be
holistically optimized in a single seq2seq model.
One simplistic instantiation of Sequicity, called
Two Stage CopyNet (TSCP), demonstrates better
effectiveness and scalability of Sequicity. Exper-
iments show that TSCP outperforms the state-of-
the-art baselines in both task accomplishment and
language quality. Moreover, our TSCP implemen-
tation also betters traditional pipeline architectures
by a magnitude in training time and adds the ca-
pability of handling OOV. Such properties are im-
portant for real-world customer service dialog sys-
tems where users’ inputs vary frequently and mod-
els need to be updated frequently. For our future
work, we will consider advanced instantiations for
Sequicity, and extend Sequicity to handle unsuper-
vised cases where information and requested slots
values are not annotated.

Acknowledgments

We would like to thank the anonymous reviewers
for their detailed comments and suggestions for
this paper. This work is also supported by the Na-
tional Research Foundation, Prime Ministers Of-
ﬁce, Singapore under its IRC@SG Funding Initia-
tive.

References

Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang
Tang. 2017. A survey on dialogue systems: Re-
arXiv preprint
cent advances and new frontiers.
arXiv:1711.01731 .

Heriberto Cuay´ahuitl, Simon Keizer, and Oliver
Strategic dialogue management
arXiv preprint

Lemon. 2015.
via deep reinforcement learning.
arXiv:1511.08099 .

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.
Towards end-to-end reinforcement learning of dia-
In Proceed-
logue agents for information access.
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). volume 1, pages 484–495.

Mihail Eric and Christopher D Manning. 2017a. A
copy-augmented sequence-to-sequence architecture
gives good performance on task-oriented dialogue.

Mihail Eric and Christopher D Manning. 2017b. Key-
value retrieval networks for task-oriented dialogue.
SIGDIAL .

Jonas Gehring, Michael Auli, David Grangier, and
Yann N Dauphin. 2017. A convolutional encoder
model for neural machine translation. ACL .

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Incorporating copying mechanism in

Li. 2016.
sequence-to-sequence learning. ACL .

Homa B Hashemi, Amir Asiaee, and Reiner Kraft.
2016. Query intent detection using convolutional
In International Conference on
neural networks.
Web Search and Data Mining, Workshop on Query
Understanding.

Matthew Henderson, Blaise Thomson, and Jason D
Williams. 2014a. The second dialog state tracking
challenge. In SIGDIAL Conference. pages 263–272.

Matthew Henderson, Blaise Thomson, and Jason D
Williams. 2014b. The third dialog state tracking
In Spoken Language Technology Work-
challenge.
shop (SLT), 2014 IEEE. IEEE, pages 324–329.

Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for the
In Proceedings of
dialog state tracking challenge.
the SIGDIAL 2013 Conference. pages 467–471.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
the IEEE conference
tions.
on computer vision and pattern recognition. pages
3128–3137.

In Proceedings of

John F Kelley. 1984. An iterative design methodol-
ogy for user-friendly natural language ofﬁce infor-
mation applications. ACM Transactions on Infor-
mation Systems (TOIS) 2(1):26–41.

Seokhwan Kim, Luis Fernando DHaro, Rafael E
Banchs, Jason D Williams, and Matthew Henderson.
2017. The fourth dialog state tracking challenge. In
Dialogues with Social Robots, Springer, pages 435–
449.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A

method for stochastic optimization.

Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In SIGDIAL Conference.
pages 442–451.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Bing Liu and Ian Lane. 2017. Iterative policy learning
in end-to-end trainable task-oriented neural dialog
models. arXiv preprint arXiv:1709.06136 .

Hongyuan Mei, Mohit Bansal, and Matthew R Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-ﬁne alignment. In
NAACL.

Nikola Mrkˇsi´c, Diarmuid O S´eaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. ACL .

Tim Paek and Eric Horvitz. 2000. Conversation as ac-
In Proceedings of the Six-
tion under uncertainty.
teenth conference on Uncertainty in artiﬁcial intel-
ligence. Morgan Kaufmann Publishers Inc., pages
455–464.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Yangyang Shi, Kaisheng Yao, Le Tian, and Daxin
Jiang. 2016. Deep lstm based feature mapping for
In Proceedings of the 2016
query classiﬁcation.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. pages 1501–1511.

Joao Silva, Lu´ısa Coheur, Ana Cristina Mendes, and
Andreas Wichert. 2011. From symbolic to sub-
symbolic information in question classiﬁcation. Ar-
tiﬁcial Intelligence Review 35(2):137–154.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. arXiv preprint arXiv:1706.03762 .

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,
David Vandyke, and Steve Young. 2016a. Condi-
tional generation and snapshot learning in neural di-
alogue systems. EMNLP .

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkˇsi´c,
Lina M. Rojas Barahona, Pei-Hao Su, Ste-
fan Ultes, David Vandyke, and Steve Young.
Conditional generation and snap-
2016b.
In
learning in neural dialogue systems.
shot
EMNLP. ACL, Austin, Texas, pages 2153–2162.
https://aclweb.org/anthology/D16-1233.

Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
Steve Young. 2017a. Latent intention dialogue mod-
els. ICML .

Tsung-Hsien Wen, David Vandyke, Nikola Mrksic,
Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017b. A network-
based end-to-end trainable task-oriented dialogue
system. EACL .

Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL 2013
Conference. pages 404–413.

Jason D Williams. 2012. A belief tracking challenge
In NAACL-HLT
task for spoken dialog systems.
Workshop on future directions and needs in the spo-
ken dialog community: tools and data. Association
for Computational Linguistics, pages 23–24.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: practical and efﬁcient
end-to-end dialog control with supervised and rein-
forcement learning. Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) .

Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for pomdp-based spoken dia-
logue management. Computer Speech & Language
24(2):150–174.

Steve Young, Milica Gaˇsi´c, Blaise Thomson, and Ja-
son D Williams. 2013a. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE 101(5):1160–1179.

Steve Young, Milica Gaˇsi´c, Blaise Thomson, and Ja-
son D Williams. 2013b. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE 101(5):1160–1179.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In AAAI. pages 2852–2858.

Sequicity: Simplifying Task-oriented Dialogue Systems with Single
Sequence-to-Sequence Architectures

Wenqiang Lei‡∗, Xisen Jin§∗, Zhaochun Ren†, Xiangnan He‡, Min-Yen Kan‡, Dawei Yin†
‡National University of Singapore, Singapore
§Fudan University, Shanghai, China
†Data Science Lab, JD.com, Beijing, China

{wenqianglei,xisenjin}@gmail.com

renzhaochun@jd.com

kanmy@comp.nus.edu.sg xiangnanhe@gmail.com yindawei@acm.org

Abstract

Existing solutions to task-oriented dia-
logue systems follow pipeline designs
which introduce architectural complex-
ity and fragility. We propose a novel,
holistic, extendable framework based on
a single sequence-to-sequence (seq2seq)
model which can be optimized with su-
pervised or reinforcement learning. A
key contribution is that we design text
spans named belief spans to track dia-
logue believes, allowing task-oriented dia-
logue systems to be modeled in a seq2seq
way. Based on this, we propose a sim-
plistic Two Stage CopyNet instantiation
which demonstrates good scalability: sig-
niﬁcantly reducing model complexity in
terms of number of parameters and train-
ing time by an order of magnitude.
It
signiﬁcantly outperforms state-of-the-art
pipeline-based methods on two datasets
and retains a satisfactory entity match rate
on out-of-vocabulary (OOV) cases where
pipeline-designed competitors totally fail.

1

Introduction

The challenge of achieving both task comple-
tion and human-like response generation for task-
oriented dialogue systems is gaining research in-
terest. Wen et al. (2017b, 2016a, 2017a) pioneered
a set of models to address this challenge. Their
proposed architectures follow traditional pipeline
designs, where the belief tracking component is
the key component (Chen et al., 2017).

In the current paradigm, such a belief tracker
builds a complex multi-class classiﬁer for each

* Work performed during an internship at Data Science

Lab, JD.com.

slot (See §3.2) which can suffer from high com-
plexity, especially when the number of slots and
their values grow. Since all the possible slot values
have to be pre-deﬁned as classiﬁcation labels, such
trackers also cannot handle the requests that have
out-of-vocabulary (OOV) slot values. Moreover,
the belief tracker requires delexicalization, i.e., re-
placing slot values with their slot names in utter-
ances (Mrkˇsi´c et al., 2017). It does not scale well,
due to the lexical diversity. The belief tracker also
needs to be pre-trained, making the models unre-
alistic for end-to-end training (Eric and Manning,
2017a). While Eric and Manning (2017a,b) inves-
tigated building task-oriented dialogue systems by
using a seq2seq model, unfortunately, their meth-
ods are rather preliminary and do not perform well
in either task completion or response generation,
due to their omission of a belief tracker.

Questioning the basic pipeline architecture, in
this paper, we re-examine the tenets of belief
tracking in light of advances in deep learning. We
introduce the concept of a belief span (bspan), a
text span that tracks the belief states at each turn.
This leads to a new framework, named Sequicity,
with a single seq2seq model. Sequicity decom-
poses the task-oriented dialogue problem into the
generation of bspans and machine responses, con-
verting this problem into a sequence optimization
problem.
In practice, Sequicity decodes in two
stages: in the ﬁrst stage, it decodes a bspan to fa-
cilitate knowledge base (KB) search; in the sec-
ond, it decodes a machine response on the con-
dition of knowledge base search result and the
bspan.

Our method represents a shift in perspective
compared to existing work. Sequicity employs a
single seq2seq model, resulting in a vastly simpli-
ﬁed architecture. Unlike previous approaches with
an overly parameterized delexicalization-based
belief tracker, Sequicity achieves much less train-

ing time, better performance on larger a dataset
and an exceptional ability to handle OOV cases.
Furthermore, Sequicity is a theoretically and aes-
thetically appealing framework, as it achieves true
end-to-end trainability using only one seq2seq
model. As such, Sequicity leverages the rapid
development of seq2seq models (Gehring et al.,
2017; Vaswani et al., 2017; Yu et al., 2017) in de-
veloping solutions to task-oriented dialogue sce-
narios.
In our implementation, we improve on
CopyNet (Gu et al., 2016) to instantiate Sequic-
ity framework in this paper, as key words present
in bspans and machine responses recur from previ-
ous utterances. Extensive experiments conducted
on two benchmark datasets verify the effectiveness
of our proposed method.

Our contributions are fourfold:

(1) We pro-
pose the Sequicity framework, which handles both
task completion and response generation in a sin-
gle seq2seq model; (2) We present an implemen-
tation of the Sequicity framework, called Two
Stage CopyNet (TSCP), which has fewer number
of parameters and trains faster than state-of-the-art
baselines (Wen et al., 2017b, 2016a, 2017a); (3)
We demonstrate that TSCP signiﬁcantly outper-
forms state-of-the-art baselines on two large-scale
datasets, inclusive of scenarios involving OOV; (4)
We release source code of TSCP to assist the com-
munity to explore Sequicity1.

2 Related Work

Historically,
task-oriented dialog systems have
been built as pipelines of separately trained mod-
ules. A typical pipeline design contains four com-
ponents: 1) a user intent classiﬁer, 2) a belief
tracker, 3) a dialogue policy maker and a 4) re-
sponse generator. User intent detectors classify
user utterances to into one of the pre-deﬁned in-
tents. SVM, CNN and RNN models (Silva et al.,
2011; Hashemi et al., 2016; Shi et al., 2016) per-
form well for intent classiﬁcation. Belief track-
ers, which keep track of user goals and constraints
every turn (Henderson et al., 2014a,b; Kim et al.,
2017) are the most important component for task
accomplishment. They model the probability dis-
tribution of values over each slot (Lee, 2013).
Dialogue policy makers then generate the next
available system action. Recent experiments sug-
gest that reinforcement learning is a promising
paradigm to accomplish this task (Young et al.,

1http://github.com/WING-NUS/sequicity

2013a; Cuay´ahuitl et al., 2015; Liu and Lane,
2017), when state and action spaces are carefully
designed (Young et al., 2010). Finally, in the
response generation stage, pipeline designs usu-
ally pre-deﬁne ﬁxed templates where placehold-
ers are ﬁlled with slot values at runtime (Dhin-
gra et al., 2017; Williams et al., 2017; Henderson
et al., 2014b,a). However, this causes rather static
responses that could lower user satisfaction. Gen-
erating a ﬂuent, human-like response is considered
a separate topic, typiﬁed by the topic of conversa-
tion systems (Li et al., 2015).

3 Preliminaries

3.1 Encoder-Decoder Seq2seq Models

Current seq2seq models adopt encoder–decoder
structures. Given a source sequence of tokens
X = x1x2...xn, an encoder network represent X
as hidden states: H(x) = h(x)
n . Based
on H(x), a decoder network generates a target se-
quence of tokens Y = y1y2...ym whose likelihood
should be maximized given the training corpus.

2 ...h(x)

1 h(x)

As of late, the recurrent neural network with
attention (Att-RNN) is now considered a base-
line encoder–decoder architecture. Such networks
employ two (sometimes identical) RNNs, one for
encoding (i.e., generating H(x)) and another for
decoding. Particularly, for decoding yj, the de-
coder RNN takes the embedding yj−1 to gener-
ate a hidden vector h(y)
. Afterwards, the de-
j
coder attends to X: calculating attention scores
i ∈ H(x) and h(y)
between all h(x)
(Eq. (1)), and
j
then sums all h(x)
, weighted by their correspond-
ing attention scores (Eqs. (2)). The summed result
˜h(x)
as a single vector which is
j
mapped into an output space for a sof tmax oper-
ation (Eq. (3)) to decode the current token:

concatenates h(y)

j

i

uij = vT tanh(W1h(x)

i + W2h(y)
j )

˜h(x)
j =

n
(cid:88)

i=1

euij
i euij

(cid:80)

h(x)
i

yj = sof tmax(O

(cid:34) ˜h(x)
j
h(y)
j

(cid:35)

)

(1)

(2)

(3)

where v ∈ R1×l; W1, W2 ∈ Rl×d and O ∈
R|V |×d. d is embedding size and V is vocabulary
set and |V | is its size.

3.2 Belief Trackers

In the multi-turn scenarios, a belief tracker is the
key component for task completion as it records
key information from past
turns (Wen et al.,
2017b; Henderson et al., 2013, 2014a,b). Early
belief trackers are designed as Bayesian networks
where each node is a dialogue belief state (Paek
and Horvitz, 2000; Young et al., 2013b). Recent
work successfully represents belief trackers as dis-
criminative classiﬁers (Henderson et al., 2013;
Williams, 2012; Wen et al., 2017b).

Wen et al. (2017b) apply discrimination ap-
proaches (Henderson et al., 2013) to build one
classiﬁer for each slot in their belief tracker. Fol-
lowing the terminology of (Wen et al., 2017b),
a slot can be either informable or requestable,
which have been annotated in CamRes676 and
KVRET. Individually, an informable slot, speci-
ﬁed by user utterances in previous turns, is set to
a constraint for knowledge base search; whereas
a requestable slot records the user’s need in the
current dialogue. As an example of belief track-
ers in CamRes676, food type is an informable
slot, and a set of food types is also predeﬁned (e.g.,
Italian) as corresponding slot values. In (Wen
et al., 2017b), the informable slot food type is
recognized by a classiﬁer, which takes user utter-
ances as input to predict if and which type of food
should be activated, while the requestable slot of
address is a binary variable. address will be
set to true if the slot is requested by the user.

4 Method

We now describe the Sequicity framework, by ﬁrst
explaining the core concept of bspans. We then
instantiate the Sequicity framework with our intro-
duction of an improved CopyNet (Gu et al., 2016).

4.1 Belief Spans for Belief Tracking

The core of belief tracking is keeping track of in-
formable and requestable slot values when a di-
alogue progresses.
In the era of pipeline-based
methods, supervised classiﬁcation is a straightfor-
ward solution. However, we observe that this tra-
ditional architecture can be updated by applying
seq2seq models directly to the problem. In con-
trast to (Wen et al., 2017b) which treats slot val-
ues as classiﬁcation labels, we record them in a
text span, to be decoded by the model. This lever-
ages the state-of-the-art neural seq2seq models to
learn and dynamically generate them. Speciﬁcally,

our bspan has an information ﬁeld (marked with
<Inf></Inf>) to store values of informable
slots since only values are important for knowl-
edge base search. Bspans can also feature a re-
quested ﬁeld (marked with <Req></Req>), stor-
ing requestable slot names if the corresponding
value is True.

At turn t, given the user utterance Ut, we show
an example of both bspan Bt and machine re-
sponse Rt generation in Figure 1, where annotated
slot values at each turn are decoded into bspans.
B1 contains an information slot Italian be-
cause the user stated “Italian food” in U1. During
the second turn, the user adds an additional con-
straint cheap resulting in two slot values in B2’s
information ﬁeld. In the third turn, the user further
asks for the restaurant’s phone and address, which
are stored in requested slots of B3.
Our bspan solution is concise:

it simpliﬁes
multiple sophisticated classiﬁers with a single se-
quence model. Furthermore, it can be viewed as
an explicit data structure that expedite knowledge
base search as its format is ﬁxed: following (Wen
et al., 2017b), we use the informable slots values
directly for matching ﬁelds of entries in databases.

4.2 The Sequicity Framework

We make a key observation that at turn t, a sys-
tem only needs to refer to Bt−1, Rt−1 and Ut to
generate a new belief span Bt and machine re-
sponse Rt, without appealing to knowing all past
utterances. Such Markov assumption allows Se-
quicity to concatenate Bt−1, Rt−1 and Ut (de-
noted as Bt−1Rt−1Ut) as a source sequence for
seq2seq modeling, to generate Bt and Rt as tar-
get output sequences at each turn. More for-
mally, we represent the dialogue utterances as
{(B0R0U1; B1R1); (B1R1U2; B2R2); ...; (Bt−1
Rt−1Ut; BtRt)} where B0 and R0 are initialized
as empty sequences.
In this way, Sequicity ful-
ﬁlls both task accomplishment and response gen-
eration in an uniﬁed seq2seq model. Note that we
process Bt and Rt separately, as the belief state
Bt depends only on Bt−1Rt−1Ut, while the re-
sponse Rt is additionally conditioned on Bt and
the knowledge base search results (denoted as kt);
that is, Bt informs the Rt’s contents. For example,
Rt must include all the request slots from Bt when
communicating the entities fulﬁlling the requests
found in the knowledge base. Here, kt helps gen-
erate Rt pragmatically.

Figure 1: Sequicity overview. The left shows a sample dialogue; the right illustrates the Sequicity. Bt
is employed only by the model, and not visible to users. During training, we substitute slot values
with placeholders bearing the slot names for machine response. During testing, this is inverted: the
placeholders are replaced by actual slot values, according to the item selected from the knowledge base.

Generally, kt has three possibilities: 1) multiple
matches, 2) exact match and 3) no match, while
the machine responses differ accordingly. As an
example, let’s say a user requests an Italian restau-
rant. In the scenario of multiple matches, the sys-
tem should prompt for additional constraints for
disambiguation (such as restaurant price range).
In the second exact match scenario where a sin-
gle target (i.e., restaurant) has been found, the sys-
tem should inform the user their requested infor-
mation (e.g., restaurant address).
If no entity is
obtained, the system should inform the user and
perhaps generate a cooperative response to retry a
different constraint.

We thus formalize Sequicity as a seq2seq model
which encodes Bt−1Rt−1Ut jointly, but decodes
Bt and Rt separately, in two serial stages. In the
ﬁrst stage, the seq2seq model decodes Bt uncondi-
tionally (Eq. 4a). Once Bt obtained, the decoding
pauses to perform the requisite knowledge base
search based on Bt, resulting in kt. Afterwards,
the seq2seq model continues to the second decod-
ing stage, where Rt is generated on the additional
conditions of Bt and kt (Eq. 4b).

Bt = seq2seq(Bt−1Rt−1Ut|0, 0)
Rt = seq2seq(Bt−1Rt−1Ut|Bt, kt)

(4a)

(4b)

Sequicity is a general framework suitably imple-
mented by any of the various seq2seq models.
The additional modeling effort beyond a general

seq2seq model is to add the conditioning on Bt
and kt to decode the machine response Rt. For-
tunately, natural language generation with spe-
ciﬁc conditions has been extensively studied (Wen
et al., 2016b; Karpathy and Fei-Fei, 2015; Mei
et al., 2016) which can be employed within this
framework.

4.3 Sequicity Instantiation: A Two Stage

CopyNet

Although there are many possible instantiations,
in this work we purposefully choose a simplistic
architecture, leaving more sophisticated modeling
for future work. We term our instantiated model
a Two Stage CopyNet (TSCP). We denote the ﬁrst
m(cid:48) tokens of target sequence Y are Bt and the rests
are Rt, i.e. Bt = y1...ym(cid:48) and Rt = ym(cid:48)+1...ym.

Two-Stage CopyNet. We choose to improve
upon CopyNet (Gu et al., 2016) as our seq2seq
model. This is a natural choice as we observe
that target sequence generation often requires the
copying of tokens from the input sequence. Let’s
discuss this in more detail. From a probabilis-
tic point of view, the traditional encoder–decoder
structure learns a language model. To decode yj,
we can employ a sof tmax (e.g., Eq. 3) to calcu-
late the probability distribution over V i.e., P g
j (v)
where v ∈ V , and then choose the token with
the highest generation probability. However, in
our case, tokens in the target sequence Y might

be exactly copied from the input X (e.g., “Ital-
ian”). These copied words need to be explicitly
modeled. CopyNet (Gu et al., 2016) is a natural ﬁt
here, as it enlarges the decoding output space from
V to V ∪ X. For yj, it considers an additional
copy probability P c
j (v), indicating the likelihood
of yj copied from v ∈ X. Following (Gu et al.,
2016), the simple summation of both probabilities
Pj(v) = P g
j (v), v ∈ V ∪ X is treated as
the ﬁnal probability in the original paper.

j (v) + P c

In Sequicity, simply applying original CopyNet
architecture is insufﬁcient, since Bt and Rt have
different distributions. We here employ two sep-
arate RNN (GRU in our implementation) in de-
coder: one for Bt and the other for Rt. In the ﬁrst
decoding stage, we have a copy-attention mecha-
nism on X to decode Bt; then calculate the gener-
ation probability through attending to X as intro-
duced in Sec 3.1, as well as the copy probability
for each word v ∈ X following (Gu et al., 2016)
by Eq. 5:

P c

j (v) =

eψ(xi), j (cid:54) m(cid:48)

(5)

1
Z

|X|
(cid:88)

i:xi=v

In contrast to recent work (Eric and Manning,
2017a) that also employs a copy-attention mech-
anism to generate a knowledge-base search API
and machine responses, our proposed method ad-
vances in two aspects: on one hand, bspans
reduce the search space from U1R1...UtRt
to
Bt−1Rt−1Ut by compressing key points for the
task completion given past dialogues; on the other
hand, because bspans revisit context by only han-
dling the Bt with a ﬁxed length, the time com-
plexity of TSCP is only O(T ), comparing O(T 2)
in (Eric and Manning, 2017a).

Involving kt when decoding Rt. As kt has
three possible values: obtaining only one, multi-
ple or no entities. We let kt be a vector of three
dimensions, one of which signals a value. We ap-
pend kt to the embeddings yj , as shown in Eq. (9)
that is fed into an GRU for generating h(y)
j+1. This
approach is also referred to as Language Model
Type condition (Wen et al., 2016b)

y(cid:48)

j =

(cid:21)

(cid:20) yj
kt

, j ∈ [m(cid:48) + 1, m]

(9)

where Z is a normalization term and ψ(xi) is the
score of “copying” word xi and is calculated by:

4.4 Training

ψ(xi) = σ(h(x)

i

Wc)h(y)
j

, j (cid:54) m(cid:48)

T

(6)

where Wc ∈ Rd×d.

In the second decoding stage (i.e., decoding
Rt), we apply the last hidden state of Bt as the ini-
tial hidden state of the Rt GRU. However, as we
need to explicitly model the dependency on Bt, we
have copy-attention mechanism on Bt instead of
on X: treating all tokens of Bt as the candidate for
copying and attention. Speciﬁcally, we use hidden
state generated by Bt GRU, i.e., h(y)
m(cid:48) , to
calculate copying using Eqs. 7 and 8 and attention
score as introduced in Sec 3.1. It helps to reduce
search space because all key information of X for
task completion has been included in Bt.

1 , ..., h(y)

P c

j (v) =

1
Z

(cid:88)

i:yi=v

eψ(yi), i (cid:54) m(cid:48) < j (cid:54) m (7)

ψ(yi) = σ(h(y)

i

Wc)h(y)
j

T

, i (cid:54) m(cid:48) < j (cid:54) m (8)

The standard cross entropy is adopted as our ob-
jective function to train a language model:

m
(cid:88)

j=1

yjlogPj(yj)

(10)

In response generation, every token is treated
equally. However, in our case, tokens for task
completion are more important. For example,
when a user asks for the address of a restau-
rant, it matters more to decode the placeholder
<address> than decode words for language ﬂu-
ency. We can employ reinforcement learning to
ﬁne tune the trained response decoder with an em-
phasis to decode those important tokens.

Inspired by (Wen et al., 2017a), in the context
of reinforcement learning, the decoding network
can be viewed as a policy network, denoted as
πΘ(yj) for decoding yj (m(cid:48) + 1 (cid:54) j (cid:54) m). Ac-
cordingly, the choice of word yj is an action and
its hidden vector generated by decoding GRU is
the corresponding state. In reinforcement tuning
stage, the trained response decoder is the initial
policy network. By deﬁning a proper reward func-
tion r(j) for decoding yj, we can update the trained

Dataset
Size
Domains
Slot types
Distinct slot values
Dataset
Size
Domains
Slot types
Distinct slot values

Cam676
Train:408 / Test: 136 / Dev: 136
restaurant reservation
price, food style etc.
99
KVRET
Train:2425 / Test: 302 / Dev: 302
POI
calendar
poi, etc.
date, etc.
140
79

weather info.
location, etc.
65

Table 1: Dataset demographics. Following the
respective literature, Cam676 is split 3:1:1 and
KVRET is split 8:1:1, into training, developing
and testing sets, respectively.

response model with policy gradient:

1
m − m(cid:48)

m
(cid:88)

j=m(cid:48)+1

r(j) ∂logπΘ(yj)
∂Θ

(11)

where r(j) = r(j) + λr(j+1) + λ2r(j+2) +
... + λm−j+1r(m). To encourage our generated re-
sponse to answer the user requested information
but avoid long-winded response, we set the reward
at each step r(j) as follows: once the placeholder
of requested slot has been decoded, the reward for
current step is 1; otherwise, current step’s reward
is -0.1. λ is a decay parameter. Sec 5.2 for λ set-
tings.

5 Experiments

We assess the effectiveness of Sequicity in three
aspects: the task completion, the language qual-
ity, and the efﬁciency. The evaluation metrics are
listed as follows:

· BLEU to evaluate the language quality (Papineni
et al., 2002) of generated responses (hence top-1
candidate in (Wen et al., 2017b)).
· Entity match rate evaluates task completion.
According to (Wen et al., 2017b), it determines
if a system can generate all correct constraints to
search the indicated entities of the user. This met-
ric is either 0 or 1 for each dialogue.
· Success F1 evaluates task completion and is
modiﬁed from the success rate in (Wen et al.,
2017b, 2016a, 2017a). The original success rate
measures if the system answered all the requested
information (e.g. address, phone number). How-
ever, this metric only evaluates recall. A system
can easily achieve a perfect task success by always
responding all possible request slots. Instead, we
here use success F1 to balance both recall and pre-

cision. It is deﬁned as the F1 score of requested
slots answered in the current dialogue.
· Training time. The training time is important for
iteration cycle of a model in industry settings.

5.1 Datasets

We adopt the CamRest676 (Wen et al., 2017a)
and KVRET (Eric and Manning, 2017b) datasets.
Both datasets are created by a Wizard-of-Oz (Kel-
ley, 1984) method on Amazon Mechanical Turk
platform, where a pair of workers are recruited
to carry out a ﬂuent conversation to complete an
assigned task (e.g. restaurant reservation). Dur-
ing conversation, both informable and requestable
slots are recorded by workers.

CamRest676’s dialogs are in the single domain
of restaurant searching, while KVRET is broader,
containing three domains: calendar scheduling,
weather information retrieval and point of inter-
est (POI) Navigation. Detailed slot information in
each domain are shown in Table 1. We follow the
data splits of the original papers as shown in 1.

5.2 Parameter Settings

For all models, the hidden size and the embedding
size d is set to 50.
|V | is 800 for CamRes676
and 1400 for KVRET. We train our model with
an Adam optimizer (Kingma and Ba, 2015), with
a learning rate of 0.003 for supervised training and
0.0001 for reinforcement learning. Early stopping
is performed on developing set. In reinforcement
learning, the decay parameter λ is set to 0.8. We
also use beam search strategy for decoding, with a
beam size of 10.

5.3 Baselines and Comparisons

We ﬁrst compare our model with the state-of-the-
art baselines as follow:

• NDM (Wen et al., 2017b). As described in
Sec 1, it adopts pipeline designs with a be-
lief tracker component depending on delexi-
calization.

• NDM+Att+SS. Based on the NDM model, an
additional attention mechanism is performed
on the belief trackers and a snapshot learning
mechanism (Wen et al., 2016a) is adopted.

• LIDM (Wen et al., 2017a). Also based on
NDM, this model adopts neural variational
inference with reinforcement learning.

CamRes676

Mat.
(1) NDM
0.904
(2) NDM + Att + SS 0.904
0.912
(3) LIDM
N/A
(4) KVRN
0.927
(5) TSCP
0.851
(6) Att-RNN
(7) TSCP\kt
0.927
(8) TSCP\RL
0.927
(9) TSCP\Bt
0.888

BLEU Succ. F1 T imef ull T imeN.B. Mat.
0.724
91.9 min
0.212
0.724
93.7 min
0.240
0.721
97.7 min
0.246
0.459
21.4 min
0.134
0.845
0.253
7.3 min
0.805
7.2 min
0.248
0.845
7.2 min
0.232
0.845
4.1 min
0.234
0.628
22.9 min
0.197

8.6 min
10.4 min
14.4 min
–
–
–
–
–
–

0.832
0.836
0.840
N/A
0.854
0.774
0.835
0.834
0.809

KVRET

BLEU Succ. F1 T imef ull
285.5 min
0.186
289.7 min
0.188
312.8 min
0.173
46.9 min
0.184
0.219
25.5 min
23.0 min
0.208
25.3 min
0.168
17.5 min
0.191
42.7 min
0.182

0.741
0.745
0.762
0.540
0.811
0.801
0.759
0.774
0.755

T imeN.B.
29.3 min
33.5 min
56.6 min
–
–
–
–
–
–

Table 2: Model performance on CamRes676 and KVRET. This table is split into two parts: competitors
on the upper side and our ablation study on the bottom side. Mat. and Succ. F1 are for match rate and
success F1 respectively. Timefull column reports training time till converge. For NDM, NDM+Att+SS
and LIDM, we also calculate the training time for the rest parts except for the belief tracker (TimeN.B.).

• KVRN (Eric and Manning, 2017b) uses one
seq2seq model to generate response as well
as interacting with knowledge base. How-
ever, it does not incorporate a belief tracking
mechanism.

For NDM, NDM+Att+SS, LIDM, we run the
source code released by the original authors2. For
KVRN, we replicate it since there is no source
code available. We also performed an ablation
study to examine the effectiveness of each com-
ponent.

• TSCP\kt. We removed the conditioning on

kt when decoding Rt.

• TSCP\RL. We removed reinforcement learn-
ing which ﬁne tunes the models for response
generation.

• Att-RNN. The standard seq2seq baseline
as described in the preliminary section
(See §3.1).

• TSCP\Bt. We removed bspans for dialogue
state tracking. Instead, we adopt the method
in (Eric and Manning, 2017a): concatenat-
ing all past utterance in a dialogue into a
CopyNet to generate user information slots
for knowledge base search as well as machine
response.

5.4 Experimental Results

As shown in Table 2, TSCP outperforms all base-
lines (Row 5 vs. Rows 1–4) in task completion
(entity match rate, success F1) and language qual-
ity (BLEU). The more signiﬁcant performance of
TSCP in KVRET dataset indicates the scalability

2https://github.com/shawnwun/NNDIAL

of TSCP. It is because KVRET dataset has sig-
niﬁcant lexical variety, making it hard to perform
delexicalization for Wen et al.’s model (Rows 1–
3)3. However, CamRes676 is relatively small with
simple patterns where all systems work well. As
predicted, KVRN (Row 4) performs worse than
TSCP (Row 5) due to lack of belief tracking.

Compared with Wen et al.’s models (Rows 1–3),
TSCP takes a magnitude less time to train. Al-
though TSCP is implemented in PyTorch while
Wen et al.’s models in Theano, such speed com-
parison is still valid, as the rest of the NDM model
— apart from its belief tracker — has a compara-
ble training speed to TSCP (7.3 mins vs. 8.6 mins
on CamRes676 and 25.5 mins vs. 29.3 mins on
KVRET), where model complexities are similar.
The bottleneck in the time expense is due to belief
tracker training. In addition, Wen et al.’s models
perform better at the cost of more training time
(Rows 1, 2 and 3), suggesting the intrinsic com-
plexity of pipeline designs.

Importantly, ablation studies validate the ne-
cessity of bspans. With bspans, even a stan-
dard seq2seq model (Att-RNN, Row 6) beats so-
phisticated models such as attention copyNets
(TSCP\Bt, Row 9) in KVRET. Furthermore,
TSCP (Row 5) outperforms TSCP\Bt (Row 9) in
all aspects: task completion, language quality and
training speed. This validate our theoretical analy-
sis in Sec 4.3. Other components of TSCP are also
important. If we only use vanilla Attention-based
RNN instead of copyNet, all metrics for model
effectiveness decrease, validating our hypothesize
that the copied words need to be speciﬁcally mod-
eled. Secondly, BLEU score is sensitive to knowl-

3We use the delexicalization lexicon provided by the orig-

inal author of KVRET(Eric and Manning, 2017b)

5.6 Empirical Model Complexity

Traditional belief trackers like (Wen et al., 2017b)
are built as a multi-class classiﬁer, which mod-
els each individual slot and its corresponding val-
ues, introducing considerable model complexities.
This is especially severe in large datasets with a
number of slots and values. In contrast, Sequic-
ity reduces such a complex classiﬁer to a language
model. To compare the model complexities of two
approaches, we empirically measure model size.
We split KVRET dataset by their domains, result-
ing in three sub-datasets. We then accumulatively
add the sub-datasets into training set to examine
how the model size grows. We here selectively
present TSCP, NDM and its separately trained be-
lief tracker, since Wen et al.’s set of models share
similar model sizes.

edge base search result kt (Row 7 vs. Row 5).
By examining error cases, we ﬁnd that the system
is likely to generate common sentences like “you
are welcome” regardless of context, due to corpus
frequency. Finally, reinforcement learning effec-
tively helps both BLEU and success F1 although it
takes acceptable additional time for training.

5.5 OOV Tests

Previous work predeﬁnes all slot values in a be-
lief tracker. However, a user may request new at-
tributes that has not been predeﬁned as a classiﬁ-
cation label, which results in an entity mismatch.
TSCP employs copy mechanisms, gaining an in-
trinsic potential to handle OOV cases. To conduct
the OOV test, we synthesize OOV test instances
by adding a sufﬁx unk to existing slot ﬁllers. For
example, we change “I would like Chinese food”
into “I would like Chinese unk food.” We then
randomly make a proportion of testing data OOV
and measure its entity match rate. For simplicity,
we only show the three most representative mod-
els pre-trained in the in-vocabulary data: TSCP,
TSCP\Bt and NDM.

Figure 3: Model size sensitivity with respect to
KVRET. Distinct slot values of 79, 144, 284 cor-
respond to the number of slots in KVRET’s calen-
dar, calendar + weather info., and all 3 domains.

As shown in Figure 3, TSCP has a magni-
tude less number of parameters than NDM and
its model size is much less sensitive to distinct
slot values increasing.
It is because TSCP is a
seq2seq language model which has a approximate
linear complexity to vocabulary size. However,
NDM employs a belief tracker which dominates
its model size. The belief tracker is sensitive to the
increase of distinct slot values because it employs
complex structures to model each slot and corre-
sponding values. Here, we only perform empirical
evaluation, leaving theoretically complexity anal-
ysis for future works.

5.7 Discussions

In this section we discuss if Sequicity can tackle
inconsistent user requests , which happens when
users change their minds during a dialogue. Incon-
sistent user requests happen frequently and are dif-

(a) CamRes676

(b) KVRET

Figure 2: OOV tests. 0% OOV rate means no
OOV instance while 100% OOV rate means all in-
stances are changed to be OOV.

Compared with NDM, TSCP still performs well
when all slot ﬁllers are unknown. This is because
TSCP actually learns sentence patterns. For exam-
ple, CamRes676 dataset contains a frequent pat-
tern “I would like [food type] food” where the
[food type] should be copied in Bt regardless
what exact word it is. In addition, the performance
of TSCP\Bt decreases more sharply than TSCP as
more instances set to be OOV. This might be be-
cause handling OOV cases is much harder when
search space is large.

ﬁcult to tackle in belief tracking (Williams, 2012;
Williams et al., 2013). Unlike most of previous
pipeline-based work that explicitly deﬁnes model
actions for each situation, Sequicity is proposed to
directly handle various situations from the training
data with less manual intervention. Here, given
examples about restaurant reservation, we provide
three different scenarios to discuss:

• A user totally changes his mind. For ex-
ample, the user request a Japanese restaurant
ﬁrst and says “I dont want Japanese food any-
more, I’d like French now.” Then, all the slot
activated before should be invalid now. The
slot annotated for this turn is only French. Se-
quicity can learn this pattern, as long as it is
annotated in the training set.

• User requests cannot be found in the
KB (e.g., Japanese food). Then the sys-
tem should respond like “Sorry, there is no
Japanese food...”. Consequently, the user can
choose a different option: “OK, then French
food.” The activated slot Japanese will be
replaced as French, which our system can
learn. Therefore, an important pattern is the
machine-response (e.g., “there is no [XXX
constraint]”) in the immediate previous utter-
ance.

• Other cases. Sequicity is expected to gen-
erate both slot values in a belief span if it
doesn’t know which slot to replace. To main-
tain the belief span, we run a simple post-
processing script at each turn, which detects
whether two slot values have the same slot
name (e.g., food type) in a pre-deﬁned
slot name-value table. Then, such script only
keeps the slot value in the current turn of user
utterance. Given this script, Sequicity can ac-
curately discover the slot requested by a user
in each utterance. However, this script only
works when slot values are pre-deﬁned. For
inconsistent OOV requests, we need to build
another classiﬁer to recognize slot names for
slot values.

To sum up, Sequicity, as a framework, is able
to handle various inconsistent user input despite
its simple design. However, detailed implementa-
tions should be customized depends on different
applications.

6 Conclusion

We propose Sequicity, an extendable framework,
which tracks dialogue believes through the decod-
ing of novel text spans: belief spans. Such belief
spans enable a task-oriented dialogue system to be
holistically optimized in a single seq2seq model.
One simplistic instantiation of Sequicity, called
Two Stage CopyNet (TSCP), demonstrates better
effectiveness and scalability of Sequicity. Exper-
iments show that TSCP outperforms the state-of-
the-art baselines in both task accomplishment and
language quality. Moreover, our TSCP implemen-
tation also betters traditional pipeline architectures
by a magnitude in training time and adds the ca-
pability of handling OOV. Such properties are im-
portant for real-world customer service dialog sys-
tems where users’ inputs vary frequently and mod-
els need to be updated frequently. For our future
work, we will consider advanced instantiations for
Sequicity, and extend Sequicity to handle unsuper-
vised cases where information and requested slots
values are not annotated.

Acknowledgments

We would like to thank the anonymous reviewers
for their detailed comments and suggestions for
this paper. This work is also supported by the Na-
tional Research Foundation, Prime Ministers Of-
ﬁce, Singapore under its IRC@SG Funding Initia-
tive.

References

Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang
Tang. 2017. A survey on dialogue systems: Re-
arXiv preprint
cent advances and new frontiers.
arXiv:1711.01731 .

Heriberto Cuay´ahuitl, Simon Keizer, and Oliver
Strategic dialogue management
arXiv preprint

Lemon. 2015.
via deep reinforcement learning.
arXiv:1511.08099 .

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.
Towards end-to-end reinforcement learning of dia-
In Proceed-
logue agents for information access.
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). volume 1, pages 484–495.

Mihail Eric and Christopher D Manning. 2017a. A
copy-augmented sequence-to-sequence architecture
gives good performance on task-oriented dialogue.

Mihail Eric and Christopher D Manning. 2017b. Key-
value retrieval networks for task-oriented dialogue.
SIGDIAL .

Jonas Gehring, Michael Auli, David Grangier, and
Yann N Dauphin. 2017. A convolutional encoder
model for neural machine translation. ACL .

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Incorporating copying mechanism in

Li. 2016.
sequence-to-sequence learning. ACL .

Homa B Hashemi, Amir Asiaee, and Reiner Kraft.
2016. Query intent detection using convolutional
In International Conference on
neural networks.
Web Search and Data Mining, Workshop on Query
Understanding.

Matthew Henderson, Blaise Thomson, and Jason D
Williams. 2014a. The second dialog state tracking
challenge. In SIGDIAL Conference. pages 263–272.

Matthew Henderson, Blaise Thomson, and Jason D
Williams. 2014b. The third dialog state tracking
In Spoken Language Technology Work-
challenge.
shop (SLT), 2014 IEEE. IEEE, pages 324–329.

Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for the
In Proceedings of
dialog state tracking challenge.
the SIGDIAL 2013 Conference. pages 467–471.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
the IEEE conference
tions.
on computer vision and pattern recognition. pages
3128–3137.

In Proceedings of

John F Kelley. 1984. An iterative design methodol-
ogy for user-friendly natural language ofﬁce infor-
mation applications. ACM Transactions on Infor-
mation Systems (TOIS) 2(1):26–41.

Seokhwan Kim, Luis Fernando DHaro, Rafael E
Banchs, Jason D Williams, and Matthew Henderson.
2017. The fourth dialog state tracking challenge. In
Dialogues with Social Robots, Springer, pages 435–
449.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A

method for stochastic optimization.

Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In SIGDIAL Conference.
pages 442–451.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Bing Liu and Ian Lane. 2017. Iterative policy learning
in end-to-end trainable task-oriented neural dialog
models. arXiv preprint arXiv:1709.06136 .

Hongyuan Mei, Mohit Bansal, and Matthew R Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-ﬁne alignment. In
NAACL.

Nikola Mrkˇsi´c, Diarmuid O S´eaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. ACL .

Tim Paek and Eric Horvitz. 2000. Conversation as ac-
In Proceedings of the Six-
tion under uncertainty.
teenth conference on Uncertainty in artiﬁcial intel-
ligence. Morgan Kaufmann Publishers Inc., pages
455–464.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Yangyang Shi, Kaisheng Yao, Le Tian, and Daxin
Jiang. 2016. Deep lstm based feature mapping for
In Proceedings of the 2016
query classiﬁcation.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. pages 1501–1511.

Joao Silva, Lu´ısa Coheur, Ana Cristina Mendes, and
Andreas Wichert. 2011. From symbolic to sub-
symbolic information in question classiﬁcation. Ar-
tiﬁcial Intelligence Review 35(2):137–154.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. arXiv preprint arXiv:1706.03762 .

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,
David Vandyke, and Steve Young. 2016a. Condi-
tional generation and snapshot learning in neural di-
alogue systems. EMNLP .

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkˇsi´c,
Lina M. Rojas Barahona, Pei-Hao Su, Ste-
fan Ultes, David Vandyke, and Steve Young.
Conditional generation and snap-
2016b.
In
learning in neural dialogue systems.
shot
EMNLP. ACL, Austin, Texas, pages 2153–2162.
https://aclweb.org/anthology/D16-1233.

Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
Steve Young. 2017a. Latent intention dialogue mod-
els. ICML .

Tsung-Hsien Wen, David Vandyke, Nikola Mrksic,
Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017b. A network-
based end-to-end trainable task-oriented dialogue
system. EACL .

Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL 2013
Conference. pages 404–413.

Jason D Williams. 2012. A belief tracking challenge
In NAACL-HLT
task for spoken dialog systems.
Workshop on future directions and needs in the spo-
ken dialog community: tools and data. Association
for Computational Linguistics, pages 23–24.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: practical and efﬁcient
end-to-end dialog control with supervised and rein-
forcement learning. Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) .

Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for pomdp-based spoken dia-
logue management. Computer Speech & Language
24(2):150–174.

Steve Young, Milica Gaˇsi´c, Blaise Thomson, and Ja-
son D Williams. 2013a. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE 101(5):1160–1179.

Steve Young, Milica Gaˇsi´c, Blaise Thomson, and Ja-
son D Williams. 2013b. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE 101(5):1160–1179.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In AAAI. pages 2852–2858.

