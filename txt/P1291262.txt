8
1
0
2
 
v
o
N
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
3
9
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Critical initialisation for deep signal propagation in
noisy rectiﬁer neural networks

Arnu Pretorius∗
Computer Science Division
CAIR†
Stellenbosch University

Elan Van Biljon
Computer Science Division
Stellenbosch University

Steve Kroon
Computer Science Division
Stellenbosch University

Herman Kamper
Department of Electrical and Electronic Engineering
Stellenbosch University

Abstract

Stochastic regularisation is an important weapon in the arsenal of a deep learning
practitioner. However, despite recent theoretical advances, our understanding of
how noise inﬂuences signal propagation in deep neural networks remains limited.
By extending recent work based on mean ﬁeld theory, we develop a new framework
for signal propagation in stochastic regularised neural networks. Our noisy signal
propagation theory can incorporate several common noise distributions, including
additive and multiplicative Gaussian noise as well as dropout. We use this frame-
work to investigate initialisation strategies for noisy ReLU networks. We show that
no critical initialisation strategy exists using additive noise, with signal propagation
exploding regardless of the selected noise distribution. For multiplicative noise
(e.g. dropout), we identify alternative critical initialisation strategies that depend
on the second moment of the noise distribution. Simulations and experiments on
real-world data conﬁrm that our proposed initialisation is able to stably propagate
signals in deep networks, while using an initialisation disregarding noise fails to do
so. Furthermore, we analyse correlation dynamics between inputs. Stronger noise
regularisation is shown to reduce the depth to which discriminatory information
about the inputs to a noisy ReLU network is able to propagate, even when initialised
at criticality. We support our theoretical predictions for these trainable depths with
simulations, as well as with experiments on MNIST and CIFAR-10.‡

1

Introduction

Over the last few years, advances in network design strategies have made it easier to train large
networks and have helped to reduce overﬁtting. These advances include improved weight initialisation
strategies (Glorot and Bengio, 2010; Saxe et al., 2014; Sussillo and Abbott, 2014; He et al., 2015;
Mishkin and Matas, 2016), non-saturating activation functions (Glorot et al., 2011) and stochastic
regularisation techniques (Srivastava et al., 2014). Authors have noted, for instance, the critical
dependence of successful training on noise-based methods such as dropout (Krizhevsky et al., 2012;
Dahl et al., 2013).

∗Correspondence: arnupretorius@gmail.com
†CSIR/SU Centre for Artiﬁcial Intelligence Research.
‡Code to reproduce all the results is available at https://github.com/ElanVB/noisy_signal_prop

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

Figure 1: Noisy layer recursion. The input xl−1 from the previous layer gets corrupted by the sampled
noise (cid:15)l−1, either by vector addition or component-wise multiplication, producing the noisy inputs
˜xl−1. The lth layer’s corrupted pre-activations are then computed by multiplication with the layer
weight matrix W l, followed by a vector addition of the biases bl. Finally, the inputs to the next layer
are simply the activations of the current layer, i.e. xl = φ(˜hl).

In many cases, successful results arise only from effective combination of these advances. Despite
this interdependence, our theoretical understanding of how these mechanisms and their interactions
affect neural networks remains impoverished.

One approach to studying these effects is through the lens of deep neural signal propagation. By
modelling the empirical input variance dynamics at the point of random initialisation, Saxe et al.
(2014) were able to derive equations capable of describing how signal propagates in nonlinear fully
connected feed-forward neural networks. This “mean ﬁeld” theory was subsequently extended by
Poole et al. (2016) and Schoenholz et al. (2017), in particular, to analyse signal correlation dynamics.
These analyses highlighted the existence of a critical boundary at initialisation, referred to as the “edge
of chaos”. This boundary deﬁnes a transition between ordered (vanishing), and chaotic (exploding)
regimes for neural signal propagation. Subsequently, the mean ﬁeld approximation to random neural
networks has been employed to analyse other popular neural architectures (Yang and Schoenholz,
2017; Xiao et al., 2018; Chen et al., 2018).

This paper focuses on the effect of noise on signal propagation in deep neural networks. Firstly we
ask: How is signal propagation in deep neural networks affected by noise? To gain some insight into
this question, we extend the mean ﬁeld theory developed by Schoenholz et al. (2017) for the special
case of dropout noise, into a generalised framework capable of describing the signal propagation
behaviour of stochastically regularised neural networks for different noise distributions.

Secondly we ask: How much are current weight initialisation strategies affected by noise-induced
regularisation in terms of their ability to initialise at a critical point for stable signal propagation?
Using our derived theory, we investigate this question speciﬁcally for rectiﬁed linear unit (ReLU)
networks. In particular, we show that no such critical initialisation exists for arbitrary zero-mean
additive noise distributions. However, for multiplicative noise, such an initialisation is shown to be
possible, given that it takes into account the amount of noise being injected into the network. Using
these insights, we derive novel critical initialisation strategies for several different multiplicative
noise distributions.

Finally, we ask: Given that a network is initialised at criticality, in what way does noise inﬂuence
the network’s ability to propagate useful information about its inputs? By analysing the correlation
between inputs as a function of depth in random deep ReLU networks, we highlight the following:
even though the statistics of individual inputs are able to propagate arbitrarily deep at criticality,
discriminatory information about the inputs becomes lost at shallower depths as the noise in the
network is increased. This is because in the later layers of a random noisy network, the internal
representations from different inputs become uniformly correlated. Therefore, the application of
noise regularisation directly limits the trainable depth of critically initialised ReLU networks.

2 Noisy signal propagation

We begin by presenting mean ﬁeld equations for stochastically regularised fully connected feed-
forward neural networks, allowing us to study noisy signal propagation for a variety of noise
distributions. To understand how noise inﬂuences signal propagation in a random network given an
input x0 ∈ RD0 , we inject noise into the model

˜hl = W l(xl−1 (cid:12) (cid:15)l−1) + bl, spa for l = 1, ..., L

(1)

2

using the operator (cid:12) to denote either addition or multiplication where (cid:15)l is an input noise vector,
sampled from a pre-speciﬁed noise distribution. For additive noise, the distribution is assumed to be
zero mean, for multiplicative noise distributions, the mean is assumed to be equal to one. The weights
W l ∈ RDl×Dl−1 and biases bl ∈ RDl are sampled i.i.d. from zero mean Gaussian distributions
with variances σ2
b , respectively, where Dl denotes the dimensionality of the lth hidden
layer in the network. The hidden layer activations xl = φ(˜hl) are computed element-wise using
an activation function φ(·), for layers l = 1, ..., L. Figure 1 illustrates this recursive sequence of
operations.

w/Dl−1 and σ2

To describe forward signal propagation for the model in (1), we make use of the mean ﬁeld approxi-
mation as in Poole et al. (2016) and analyse the statistics of the internal representations of the network
in expectation over the parameters and the noise. Since the weights and biases are sampled from zero
mean Gaussian distributions with pre-speciﬁed variances, we can approximate the distribution of the
pre-activations at layer l, in the large width limit, by a zero mean Gaussian with variance

˜ql = σ2
w

(cid:26)

(cid:20)

φ

Ez

(cid:16)(cid:112)

˜ql−1z

(cid:17)2(cid:21)

(cid:27)

(cid:12) µl−1
2

+ σ2
b ,

2 = E(cid:15)[((cid:15)l)2] is the
where z ∼ N (0, 1) (see Section A.1 in the supplementary material). Here, µl
second moment of the noise distribution being sampled from at layer l. The initial input variance is
given by q0 = 1
x0 · x0. Furthermore, to study the behaviour of a pair of signals from two different
D0
inputs, x0,a and x0,b, passing through the network, we can compute the covariance at each layer as

Ez1 [Ez2 [φ(˜u1)φ(˜u2)]] + σ2
b

ab = σ2
˜ql
w
(cid:113)

(cid:104)

˜ql−1
bb
(cid:113)

where ˜u1 =

˜ql−1
aa z1 and ˜u2 =

(cid:112)

˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

, with the correlation between

inputs at layer l given by ˜cl = ˜ql
aa ˜ql
˜ql
supplementary material for more details).

ab/

bb. Here, ql

aa is the variance of ˜hl,a

j

(see Section A.2 in the

For the backward pass, we use the equations derived in Schoenholz et al. (2017) to describe error
signal propagation.1 In the context of mean ﬁeld theory, the expected magnitude of the gradient at
each layer can be shown to be proportional to the variance of the error, ˜δl
˜δl+1
j W l+1
.
This allows for the distribution of the error signal at layer l to be approximated by a zero mean
Gaussian with variance

i) (cid:80)Dl+1

i = φ(cid:48)(˜hl

j=1

ji

Similarly, for noise regularised networks, the covariance between error signals can be shown to be

δ = ˜ql+1
˜ql
δ

Dl+1
Dl

σ2
w

Ez

(cid:20)
φ(cid:48) (cid:16)(cid:112)

˜qlz

(cid:17)2(cid:21)

.

ab,δ = ˜ql+1
˜ql
ab,δ

σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u1)φ(cid:48)(˜u2)]] ,

Dl+1
Dl+2

(2)

(3)

(4)

(5)

where ˜u1 and ˜u2 are deﬁned as was done in the forward pass.

Equations (2)-(5) fully capture the relevant statistics that govern signal propagation for a random
network during both the forward and the backward pass. In the remainder of this paper, we consider,
as was done by Schoenholz et al. (2017), the following necessary condition for training: “for a
random network to be trained information about the inputs should be able to propagate forward
through the network, and information about the gradients should be able to propagate backwards
through the network.” The behaviour of the network at this stage depends on the choice of activation,
noise regulariser and initial parameters. In the following section, we will focus on networks that use
the Rectiﬁed Linear Unit (ReLU) as activation function. The chosen noise regulariser is considered a
design choice left to the practitioner. Therefore, whether a random noisy ReLU network satisﬁes the
above stated necessary condition for training largely depends on the starting parameter values of the
network, i.e. its initialisation.

1It is, however, important to note that the derivation relies on the assumption that the weights used in the

forward pass are sampled independently from those used during backpropagation.

3

Figure 2: Deep signal propagation with and without noise. (a): Iterative variance map. (b): Variance
dynamics during forward signal propagation. In (a) and (b), lines correspond to theoretical predictions
and points to numerical simulations (means over 50 runs with shaded one standard deviation bounds),
for noiseless tanh (yellow) and noiseless ReLU (purple) networks, as well as for noisy tanh (red)
and noisy ReLU (brown) networks regularised using additive noise from a standard Gaussian. Both
tanh networks use (σw, σb) = (1, 0), the “Xavier” initialisation (Glorot and Bengio, 2010), while the
2, 0) the “He” initialisation (He et al., 2015). In our experiments,
ReLU networks use (σw, σb) = (
we use network layers consisting of 1000 hidden units (see Section C in the supplementary material
for more details on all our simulated experiments).

√

3 Critical initialisation for noisy rectiﬁer networks

Unlike the tanh nonlinearity investigated in previous work (Poole et al., 2016; Schoenholz et al.,
2017), rectifying activation functions such as ReLU are unbounded. This means that the statistics of
signal propagation through the network is not guaranteed to naturally stabilise through saturating
activations, as shown in Figure 2.

A point on the identity line in Figure 2 (a) represents a ﬁxed point to the recursive variance map
in equation (2). At a ﬁxed point, signal will stably propagate through the remaining layers of the
network. For tanh networks, such a ﬁxed point always exists irrespective of the initialisation, or
the amount of noise injected into the network. For ReLU networks, this is not the case. Consider
the “He” initialisation (He et al., 2015) for ReLU, commonly used in practice. In (b), we plot the
variance dynamics for this initialisation in purple and observe stable behaviour. But what happens
when we inject noise into each network? In the case of tanh (shown in red), the added noise simply
shifts the ﬁxed point to a new stable value. However, for ReLU, the noise entirely destroys the ﬁxed
point for the “He” initialisation, making signal propagation unstable. This can be seen in (a), where
the variance map for noisy ReLU (shown in brown) moves off the identity line entirely, causing the
signal in (b) to explode.

Therefore, to investigate whether signal can stably propagate through a random noisy ReLU network,
we examine (2) more closely, which for ReLU becomes (see Section B.1 in supplementary material)

˜ql = σ2
w

(cid:20) ˜ql−1
2

(cid:21)

(cid:12) µ2

+ σ2
b .

(6)

For ease of exposition we assume equal noise levels at each layer, i.e. µl
2 = µ2, ∀l. A critical
initialisation for a noisy ReLU network occurs when the tuple (σw, σb, µ2) provides a ﬁxed point ˜q∗,
to the recurrence in (6). This at least ensures that the statistics of individual inputs to the network will
be preserved throughout the ﬁrst forward pass. The existence of such a solution depends on the type
2 ˜q∗ + µ2σ2
w + σ2
of noise that is injected into the network. In the case of additive noise, ˜q∗ = σ2
b ,
w
√
implying that the only critical point initialisation for non-zero ˜q∗ is given by (σw, σb, µ2) = (
2, 0, 0).
Therefore, critical initialisation is not possible using any amount of zero-mean additive noise,
regardless of the noise distribution. For multiplicative noise, ˜q∗ = σ2
b , so the solution
w
(σw, σb, µ2) =
provides a critical initialisation for noise distributions with mean
one and a non-zero second moment µ2. For example, in the case of multiplicative Gaussian noise,
µ2 = σ2
. For dropout noise,

(cid:15) + 1, yielding critical initialisation with (σw, σb) =

2 ˜q∗µ2 + σ2

(cid:16)(cid:113) 2
µ2

(cid:16)(cid:113) 2

(cid:17)
σ2+1 , 0

, 0, µ2

(cid:17)

1

1

4

Table 1: Critical point initialisation for noisy ReLU networks.

DISTRIBUTION

P((cid:15))

— ADDITIVE NOISE —

µ2

σ2
(cid:15)

2β2

CRITICAL INITIALISATION

(σw, σb, σ(cid:15)) = (
√

(σw, σb, β) = (

√

2, 0, 0)

2, 0, 0)

N (0, σ2
(cid:15) )

Lap(0, β)

— MULTIPLICATIVE NOISE —

N (1, σ2
(cid:15) )

(σ2

(cid:15) + 1)

(σw, σb, σ(cid:15)) =

Lap(1, β)

(2β2 + 1)

(σw, σb, β) =

(cid:17)

(cid:16)(cid:113) 2
(cid:15) +1 , 0, σ(cid:15)
σ2
(cid:17)

(cid:16)(cid:113) 2

2β2+1 , 0, β

P oi(1)

P((cid:15) = 1
P((cid:15) = 0) = 1 − p

p ) = p,

2

1
p

(σw, σb, λ) = (1, 0, 1)

(σw, σb, p) = (

2p, 0, p)

√

GAUSSIAN

LAPLACE

GAUSSIAN

LAPLACE

POISSON

DROPOUT

Figure 3: Critical initialisation for noisy ReLU networks. (a): Iterative variance map. (b): Vari-
ance dynamics during forward signal propagation. In (a) and (b), lines correspond to theoretical
predictions and points to numerical simulations. Dropout (p = 0.6) is shown in green for dif-
w = 2(0.6) = 2
(critical), σ2
ferent initialisations, σ2
(exploding sig-
µ2
(0.6)−1 < 2
w = (0.85)2
nal) and σ2
(vanishing signal). Similarly, multiplicative Gaussian noise
µ2
(σ(cid:15) = 0.25) is shown in red with σ2
w =
(exploding) and
w = (0.75)2 2
σ2
( vanishing). (c): Variance critical boundary for initialisation, separating numerical
µ2
overﬂow and underﬂow signal propagation regimes.

w = (1.25)2 2
µ2

(0.25)2+1 = 2
µ2

(0.6)−1 > 2
µ2

w = (1.15)2

(critical), σ2

2

2

2

√

µ2 = 1/p (with p the probability of retaining a neuron); thus, to initialise at criticality, we must
2p, 0). Table 1 summarises critical initialisations for some commonly used
set (σw, σb) = (
noise distributions. We also note that similar results can be derived for other rectifying activation
functions; for example, for multiplicative noise the critical initialisation for parametric ReLU (PReLU)
activations (with slope parameter α) is given by (σw, σb, µ2) =

(cid:16)(cid:113) 2

(cid:17)

.

µ2(α2+1) , 0, µ2

To see the effect of initialising on or off the critical point for ReLU networks, Figure 3 compares
the predicted versus simulated variance dynamics for different initialisation schemes. For schemes
not initialising at criticality, the variance map in (a) no longer lies on the identity line and as a result
the forward propagating signal in (b) either explodes, or vanishes. In contrast, the initialisations
derived above lie on the critical boundary between these two extremes, as shown in (c) as a function
of the noise. By compensating for the amount of injected noise, the signal corresponding to the
initialisation σ2
is preserved in (b) throughout the entire forward pass, with roughly constant
variance dynamics.

w = 2
µ2

5

Figure 4: Propagating correlation information in noisy ReLU networks. (a): Iterative correlation
map with ﬁxed points indicated by “X” marks on the identity line. (b): Correlation dynamics during
forward signal propagation. In (a) and (b), lines correspond to theoretical predictions and points to
numerical simulations. All simulated networks were initialised at criticality for each noise type and
level. (c): Slope at the ﬁxed point correlation as a function of the amount of noise injected into the
network.

Next, we investigate the correlation dynamics between inputs. Assuming that (6) is at its ﬁxed point
˜q∗, which exists only if σ2
, the correlation map for a noisy ReLU network is given by (see
Section B.2 in supplementary material)

w = 2
µ2

˜cl =

1
µ2

(cid:40)

˜cl−1sin−1 (cid:0)˜cl−1(cid:1) + (cid:112)1 − (˜cl−1)2
π

+

˜cl−1
2

(cid:41)

.

Figure 4 plots this theoretical correlation map against simulated dynamics for different noise types
and levels. For no noise, the ﬁxed point c∗ in (a) is situated at one (marked with an “X” on the blue
line). The slope of the blue line indicates a non-decreasing function of the input correlations. After a
certain depth, inputs end up perfectly correlated irrespective of their starting correlation, as shown in
(b). In other words, random deep ReLU networks lose discriminatory information about their inputs
as the depth of the network increases, even when initialised at criticality. When noise is added to the
network, inputs decorrelate and c∗ moves away from one. However, more importantly, correlation
information in the inputs become lost at shallower depths as the noise level increases, as can be seen
in (b).

How quickly a random network loses information about its inputs depends on the rate of convergence
to the ﬁxed point c∗. Using this observation, Schoenholz et al. (2017) derived so-called depth scales
ξc, by assuming |cl − c∗| ∼ e−l/ξc. These scales essentially control the feasible depth at which
networks can be considered trainable, since they may still allow useful correlation information to
propagate through the network. In our case, the depth scale for a noisy ReLU network under this
assumption can be shown to be (see Section B.3 in supplementary material)

where

ξc = −1/ln [χ(c∗)] ,

χ(c∗) =

1
µ2π

(cid:104)
sin−1 (c∗) +

(cid:105)

.

π
2

(7)

(8)

(9)

The exponential rate assumption underlying the derivation of (8) is supported in Figure 5, where
for different noise types and levels, we plot |cl − c∗| as a function of depth on a log-scale, with
corresponding linear ﬁts (see panels (a) and (c)). We then compare the theoretical depth scales from
(8) to actual depth scales obtained through simulation (panels (b) and (d)), as a function of noise
and observe a good ﬁt for non-zero noise levels.4 We thus ﬁnd that noise limits the depth at which
critically initialised ReLU networks are expected to perform well through training.

4We note Hayou et al. (2018) recently showed that the rate of convergence for noiseless ReLU networks is
not exponential, but polynomial instead. Interestingly, keeping with the exponential rate assumption, we indeed
ﬁnd that the discrepancy between our theoretical depth scales from (8) and our simulated depth scales, is largest
at very low noise levels. However, at more typical noise levels, such as a dropout rate of p = 0.5 for example,
the assumption seems to provide a close ﬁt, with good agreement between theory and simulation.

6

Figure 5: Noise dependent depth scales for training. (a): Linear ﬁts (dashed lines) to |cl − c∗| as a
function of depth on a log-scale (solid lines) for varying amounts of dropout (p = 0.1 to p = 0.9
by 0.1). (b): Theoretical depth scales (solid lines) versus empirically inferred scales (dashed lines)
per dropout rate. Scales are inferred noting that if |cl − c∗| ∼ e−l/ξc, then a linear ﬁt, al + b, in
the logarithmic domain gives ξc ≈ − 1
a , for large l. In other words, the negative inverse slope of a
linear ﬁt to the log differences in correlation should match the theoretical values for ξc. Therefore,
we compare ξc = −1/ln [χ(c∗)] to − 1
a for different levels of noise. (c) - (d): Similar to (a) and (b),
but for Gaussian noise (σ(cid:15) = 0.1 to σ(cid:15) = 1.9 by 0.15).

We next brieﬂy discuss error signal propagation during the backward pass for noise regularised ReLU
networks. When critically initialised, the error variance recurrence relation in (4) for these networks
is (see Section B.4 in supplementary material)

with the covariance between error signals in (5), given by (see Section B.5 in supplementary material)

δ = ˜ql+1
˜ql
δ

Dl+1
Dlµ2

,

ab,δ = ˜ql+1
˜ql
ab,δ

Dl+1
Dl+2

χ(c∗).

(10)

(11)

Note the explicit dependence on the width of the layers of the network in (10) and (11). We ﬁrst
consider constant width networks, where Dl+1 = Dl, for all l = 1, ..., L. For any amount of
multiplicative noise, µ2 > 1, and we see from (10) that gradients will tend to vanish for large depths.
Furthermore, Figure 4 (c) plots χ(c∗) as a function of µ2. As µ2 increases from one, χ(c∗) decreases
from one. Therefore, from (11), we also ﬁnd that error signals from different inputs will tend to
decorrelate at large depths.

Interestingly, for non-constant width networks, stable gradient information propagation may still be
possible. If the network architecture adapts to the amount of noise being injected by having the widths
of the layers grow as Dl+1 = Dlµ2, then (10) should be at its ﬁxed point solution. For example, in
the case of dropout Dl+1 = Dl/p, which implies that for any p < 1, each successive layer in the
network needs to grow in width by a factor of 1/p to promote stable gradient ﬂow. Similarly, for
multiplicative Gaussian noise, Dl+1 = Dl(σ2
(cid:15) + 1), which requires the network to grow in width
unless σ2
(cid:15) = 0. Similarly, if Dl+2 = Dl+1χ(c∗) = Dlµ2χ(c∗) in (11), the covariance of the error
signal should be preserved during the backward pass, for arbitrary values of µ2 and χ(c∗).

7

Figure 6: Depth scale experiments on MNIST and CIFAR-10. (a) Variance propagation dynamics for
MNIST on and off the critical point initialisation (dashed black line) with dropout (p = 0.6). The
cyan curve represents the theoretical boundary at which numerical instability issues are predicted
to occur and is computed as L∗ = ln(K)/ln( σ2
2 µ2), where K is the largest (or smallest) positive
number representable by the computer. Speciﬁcally, we use 32-bit ﬂoating point numbers and set
K = 3.4028235 × 1038, if σ2
. (b) Depth scales ﬁt
to the training loss on MNIST for networks initialised at criticality for dropout rates p = 0.1 (severe
dropout) to p = 1 (no dropout). (c) Depth scales ﬁt to the validation loss on MNIST. (d) - (f): Similar
to (a) - (c), but for CIFAR-10. For each plot we highlight trends by smoothing the colour grid (for
non smoothed versions see Section C.5 in the supplementary material).

and K = 1.1754944 × 10−38, if σ2

w > 2
µ2

w < 2
µ2

w

4 Experimental results

From our analysis of deep noisy ReLU networks in the previous section, we expect that a necessary
condition for such a network to be trainable, is that the network be initialised at criticality. However,
whether the layer widths are varied or not for the sake of backpropagation, the correlation dynamics
in the forward pass may still limit the depth at which these networks perform well.

We therefore investigate the performance of noise-regularised deep ReLU networks on real-world
data. First, we validate the derived critical initialisation. As the depth of the network increases, any
initialisation strategy that does not factor in the effects of noise, will cause the forward propagating
signal to become increasingly unstable. For very deep networks, this might cause the signal to either
explode or vanish, even within the ﬁrst forward pass, making the network untrainable. To test this,
we sent inputs from MNIST and CIFAR-10 through ReLU networks using dropout (with p = 0.6) at
varying depths and for different initialisations of the network. Figure 6 (a) and (d) shows the evolution
of the input statistics as the input propagates through each network for the different data sets. For
initialisations not at criticality, the variance grows or shrinks rapidly to the point of causing numerical
overﬂow or underﬂow (indicated by black regions). For deep networks, this can happen well before
any signal is able to reach the output layer. In contrast, initialising at criticality (as shown by the
dashed black line), allows for the signal to propagate reliably even at very large depths. Furthermore,
given the ﬂoating point precision, if σ2
, we can predict the depth at which numerical overﬂow
(or underﬂow) will occur by solving for L∗ in K = (cid:0)σ2
q0, where K is the largest (or
smallest) positive number representable by the computer (see Section C.4 in supplementary material).
These predictions are shown by the cyan line and provide a good ﬁt to the empirical limiting depth
from numerical instability.

wµ2/2(cid:1)L∗

w (cid:54)= 2
µ2

We now turn to the issue of limited trainability. Due to the loss of correlation information between
inputs as a function of noise and network depth, we expect noisy ReLU networks not to be able to
perform well beyond certain depths. We investigated depth scales for ReLU networks with dropout
initialised at criticality: we trained 100 networks on MNIST and CIFAR-10 for 200 epochs using SGD
and a learning rate of 10−3 with dropout rates ranging from 0.1 to 1 for varying depths. The results

8

are shown in Figure 6 (see Section C.5 of the supplementary material for additional experimental
results). For each network conﬁguration and noise level, the critical initialisation σ2
was
used. We indeed observe a relationship between depth and noise on the loss of a network, even at
criticality. Interestingly, the line 6ξc (Schoenholz et al., 2017), seems to track the depth beyond
which the relative performance on the validation loss becomes poor, more so than on the training loss.
However, in both cases, we ﬁnd that even modest amounts of noise can limit performance.

w = 2
µ2

5 Discussion

By developing a general framework to study signal propagation in noisy neural networks, we were
able to show how different stochastic regularisation strategies may impact the ﬂow of information
in a deep network. Focusing speciﬁcally on ReLU networks, we derived novel critical initialisation
strategies for multiplicative noise distributions and showed that no such critical initialisations exist
for commonly used additive noise distributions. At criticality however, our theory predicts that the
statistics of the input should remain within a stable range during the forward pass and enable reliable
signal propagation for noise regularised deep ReLU networks. We veriﬁed these predictions by
comparing them with numerical simulations as well as experiments on MNIST and CIFAR-10 using
dropout and found good agreement.

Interestingly, we note that a dropout rate of p = 0.5 has often been found to work well for ReLU
networks (Srivastava et al., 2014). The critical initialisation corresponding to this rate is (σw, σb) =
√
2p, 0) = (1, 0). This is exactly the “Xavier” initialisation proposed by Glorot and Bengio (2010),
(
which prior to the development of the “He” initialisation, was often used in combination with
dropout (Simonyan and Zisserman, 2014). This could therefore help to explain the initial success
associated with this speciﬁc dropout rate. Similarly, Srivastava et al. (2014) reported that adding
multiplicative Gaussian noise where (cid:15) ∼ N (1, σ2
(cid:15) = 1, also seemed to perform well, for
= (1, 0), again corresponding to the “Xavier” method.
which the critical initialisation is

(cid:15) ), with σ2

(cid:17)
(cid:16)(cid:113) 2
(cid:15) +1 , 0
σ2

Although our initialisations ensure that individual input statistics are preserved, we further analysed
the correlation dynamics between inputs and found the following: at large depths inputs become
predictably correlated with each other based on the amount of noise injected into the network. As a
consequence, the representations for different inputs to a deep network may become indistinguishable
from each other in the later layers of the network. This can make training infeasible for noisy ReLU
networks of a certain depth and depends on the amount of noise regularisation being applied.

We now note the following shortcomings of our work: ﬁrstly, our ﬁndings only apply to fully
connected feed-forward neural networks and focus almost exclusively on the ReLU activation
function. Furthermore, we limit the scope of our architectural design to a recursive application of a
dense layer followed by a noise layer, whereas in practice a larger mix of layers is usually required to
solve a speciﬁc task.

Ultimately, we are interested in reducing the number of decisions that need to made when designing
deep neural networks and understanding the implications of those decisions on network behaviour
and performance. Any machine learning engineer exploring a neural network based solution to a
practical problem will be faced with a large number of possible design decisions. All these decisions
cost valuable time to explore. In this work, we hope to have at least provided some guidance in this
regard, speciﬁcally when choosing between different initialisation strategies for noise regularised
ReLU networks and understanding their associated implications.

Acknowledgements

We would like to thank the reviewers for their insightful comments which improved the quality of this
work. Furthermore, we would like to thank Google, the CSIR/SU Centre for Artiﬁcial Intelligence
Research (CAIR) as well as the Science Faculty and the Postgraduate and International Ofﬁce of
Stellenbosch University for ﬁnancial support. Finally, we gratefully acknowledge the support of
NVIDIA Corporation with the donation of a Titan Xp GPU used for this research.

9

References

X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep feedforward neural networks,”
in Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, 2010, pp.
249–256.

A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks,” Proceedings of the International Conference on Learning
Representations, 2014.

D. Sussillo and L. Abbott, “Random walk initialization for training very deep feedforward networks,”

arXiv preprint arXiv:1412.6558, 2014.

K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers: Surpassing human-level per-
formance on ImageNet classiﬁcation,” in Proceedings of the IEEE International Conference on
Computer Vision, 2015, pp. 1026–1034.

D. Mishkin and J. Matas, “All you need is a good init,” Proceedings of International Conference on

Learning Representations, 2016.

X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural networks,” in Proceedings of the

International Conference on Artiﬁcial Intelligence and Statistics, 2011, pp. 315–323.

N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple
way to prevent neural networks from overﬁtting.” Journal of Machine Learning Research, vol. 15,
no. 1, pp. 1929–1958, 2014.

A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation with deep convolutional
neural networks,” in Advances in Neural Information Processing Systems, 2012, pp. 1097–1105.

G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural networks for LVCSR using
rectiﬁed linear units and dropout,” in Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, 2013, pp. 8609–8613.

B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli, “Exponential expressivity in deep
neural networks through transient chaos,” in Advances in Neural Information Processing Systems,
2016, pp. 3360–3368.

S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein, “Deep information propagation,”

Proceedings of the International Conference on Learning Representations, 2017.

G. Yang and S. Schoenholz, “Mean ﬁeld residual networks: On the edge of chaos,” in Advances in

Neural Information Processing Systems, 2017, pp. 7103–7114.

L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington, “Dynamical isometry and
a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks,”
Proceedings of the International Conference on Machine Learning, 2018.

M. Chen, J. Pennington, and S. S. Schoenholz, “Dynamical isometry and a mean ﬁeld theory of RNNs:
Gating enables signal propagation in recurrent neural networks,” Proceedings of the International
Conference on Machine Learning, 2018.

S. Hayou, A. Doucet, and J. Rousseau, “On the selection of initialization and activation function for

deep neural networks,” arXiv preprint arXiv:1805.08266, 2018.

K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,”

arXiv preprint arXiv:1409.1556, 2014.

10

Supplementary Material

In this section, we provide additional details of derivations and experimental results presented in the
paper.

A Signal propagation in noise regularised neural networks

To review, given an input x0 ∈ RD0, we consider the following noisy random network model

˜hl = W l(xl−1 (cid:12) (cid:15)l−1) + bl, spa for l = 1, ..., L
(12)
where we inject noise into the model using the operator (cid:12) to denote either addition or multiplication.
The vector (cid:15)l is an input noise vector, sampled from a pre-speciﬁed noise distribution. For additive
noise, the distribution is assumed to be zero mean. Whereas for multiplicative noise distributions, the
mean is assumed to be equal to one. The weights W l ∈ RDl×Dl−1 and biases bl ∈ RDl are sampled
i.i.d. from zero mean Gaussian distributions with variances σ2
b , respectively, where
Dl denotes the dimensionality of the lth hidden layer in the network. The hidden layer activations
xl = φ(˜hl) are computed element-wise using an activation function φ(·), for layers l = 1, ..., L.

w/Dl−1 and σ2

A.1 Single input signal propagation

We consider the network’s behavior at initialisation. In this setting, the expected mean (over the
weights, biases and noise distribution) of a unit in the pre-activations ˜hl
j for a single signal passing
through the network will be zero with variance

˜ql = Ew,b,(cid:15)[(˜hl

j)2]

= Ew,(cid:15)[{wl,j · (xl−1

j (cid:12) (cid:15)l−1

j

)}2] + Eb[(bl

j)2]

= σ2
w

1
Dl−1

Dl−1
(cid:88)

j=1

(cid:104)
φ(˜hl−1
j

)2 (cid:12) E(cid:15)[((cid:15)l−1

)2]

j

+ σ2

b ,

(cid:105)

where we use wl,j to denote the j-th row of W l. The second last line relies on the bias distribution
being zero mean, while the ﬁnal step makes use of the independence between the inputs and the
noise in the multiplicative case, and the noise being zero mean in the additive case. Furthermore, to
ensure the expected value of the pre-activations remain unbiased, we only consider additive noise
distributions with zero mean and multiplicative noise distributions with a mean equal to one. As in
Poole et al. (2016), we make the self averaging assumption and consider the large layer width case
where the previous layer’s pre-activations are assumed to be Gaussian with zero mean and variance
˜ql−1. This gives the following noisy variance map

˜ql = σ2
w

(cid:26)

(cid:20)

φ

Ez

(cid:16)(cid:112)

˜ql−1z

(cid:17)2(cid:21)

(cid:27)

(cid:12) µl−1
2

+ σ2
b ,

(13)

where z ∼ N (0, 1) and µl
from at layer l. The initial input variance is given by q0 = 1
D0

2 = E(cid:15)[((cid:15)l)2] is the second moment of the noise distribution being sampled

x0 · x0.

A.2 Two input signal propagation

To study the behaviour of a pair of signals, x0,a and x0,b, passing through the network, we can
compute the covariance in expectation over the noise and the parameters as

ab = Ew,b,(cid:15)[˜hl,a
˜ql
j
(cid:104)(cid:16)
wl,j · (xl−1,a
= Ew,b,(cid:15)

˜hl,b
j ]

) + bl
j
(cid:17) (cid:16)
)

j

j

(cid:104)(cid:16)

(cid:104)(cid:16)

(cid:104)(cid:16)

= Ew,b,(cid:15)

wl,j · (xl−1,a

(cid:12) (cid:15)l−1,a
j

white + Ew,b,(cid:15)

wl,j · (xl−1,a

j

(cid:12) (cid:15)l−1,a
j

white + Ew,b,(cid:15)

white + Ew,b,(cid:15)

wl,j · (xl−1,b
j)2(cid:3) .

j

(cid:2)(bl

(cid:12) (cid:15)l−1,b
j

)

(cid:17)
)
(cid:17)

bl
j
(cid:105)

bl
j

(cid:12) (cid:15)l−1,a
j

(cid:17) (cid:16)

wl,j · (xl−1,b

j

(cid:17)(cid:105)

) + bl
j

(cid:12) (cid:15)l−1,b
j
(cid:17)(cid:105)

wl,j · (xl−1,b
(cid:105)

j

(cid:12) (cid:15)l−1,b
j

)

11

Since the noise is i.i.d and we have that Eb[bl

j] = 0, we ﬁnd that

(cid:104)(cid:16)

(cid:17) (cid:16)

ab = Ew
˜ql

wl,j · xl−1,a

j

wl,j · xl−1,b

j

(cid:17)(cid:105)

+ Eb

(cid:2)(bl

j)2(cid:3)

= σ2
w

1
Dl−1

Dl−1
(cid:88)

j=1

(cid:104)
φ

(cid:16)˜hl−1,a

(cid:17)

φ

j

(cid:16)˜hl−1,b

j

(cid:17)(cid:105)

+ σ2
b ,

which in the large width limit becomes

(cid:112)

where ˜u1 =
inputs at layer l given by

˜ql−1
aa z1 and ˜u2 =

Ez1 [Ez2 [φ(˜u1)φ(˜u2)]] + σ2
b

ab = σ2
˜ql
w
(cid:113)

˜ql−1
bb

(cid:104)

˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

, with the correlation between

(cid:113)

˜cl = ˜ql

ab/

aa ˜ql
˜ql

bb.

Here, zi ∼ N (0, 1) for i = 1, 2 and ql

aa is the variance of ˜hl,a
j .

B Signal propagation in noise regularised ReLU networks

In this section, we give additional details of theoretical results presented in the paper that were
speciﬁcally derived for noisy ReLU networks.

B.1 Variance of input signals

Let f (z) = e−z2/2

√

2π

, then the variance map in (13) using ReLU, i.e. φ(a) = max(0, a), becomes

˜ql = σ2
w

f (z)φ

˜ql−1z

dz

(cid:12) µ2 + σ2
b

(cid:16)(cid:112)

(cid:16)(cid:112)

(cid:21)

(cid:17)2

(cid:17)2

f (z)φ

˜ql−1z

dz +

f (z)φ

˜ql−1z

dz

(cid:12) µ2 + σ2
b

(cid:16)(cid:112)

(cid:17)2

(cid:21)

(cid:90) ∞

0

(cid:20)(cid:90) ∞

−∞

(cid:20)(cid:90) 0

−∞

(cid:20)
˜ql−1

(cid:20) ˜ql−1
2

= σ2
w

= σ2
w

= σ2
w

(cid:21)

f (z)z2dz

(cid:12) µ2 + σ2
b

(cid:90) ∞

0

(cid:21)

(cid:12) µ2

+ σ2
b .

B.2 Correlation between input signals

Assuming that the variance map in (18) is at its ﬁxed point ˜q∗, which exits only if σ2
correlation map in (16) for a noisy ReLU network is given by

w = 2
µ2

, the

˜cl =

2
µ2 ˜q∗

(cid:90) ∞

(cid:90) ∞

−∞

−∞

f (z1)f (z2)φ(˜u1)φ(˜u2)dz2dz1 + σ2
b

(19)

√

, ˜u1 =

˜q∗z1 and ˜u2 =

√

(cid:104)
˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

.

˜q∗

where φ(a) = max(a, 0), f (zi) = e−z2
Note that

√

i /2
2π

(14)

(15)

(16)

(17)

(18)

˜u1

˜u2

(cid:26)≥ 0, if z1 > 0
< 0, Otherwise
≥ 0, if z2 > −˜cl−1z1
< 0, Otherwise

√

(cid:40)

1−(˜cl−1)2

,

12

therefore (19) becomes

(cid:90) ∞

(cid:90) ∞

˜cl =

2
µ2 ˜q∗

0

√

−˜cl−1z1
1−(˜cl−1)2
(cid:90) ∞

(cid:90) ∞

0

√

−˜cl−1z1
1−(˜cl−1)2

(cid:90) ∞

(cid:90) ∞

√

−˜cl−1z1
1−(˜cl−1)2

=

=

w

2
µ2 ˜q∗ σ2
2˜cl−1
µ2

0

f (z1)f (z2)˜u1 ˜u2dz2dz1 + σ2
b

f (z1)f (z2)(cid:112)˜q∗z1

(cid:112)˜q∗

(cid:20)
˜cl−1z1 +

(cid:113)

(cid:21)

1 − (˜cl−1)2z2

dz2dz1 + σ2
b

f (z1)f (z2)z2

1dz2dz1

2(cid:112)1 − (˜cl−1)2
µ2

(cid:90) ∞

(cid:90) ∞

0

√

−˜cl−1z1
1−(˜cl−1)2

addsomewhitespacehere +

f (z1)f (z2)z1z2dz2dz1.

(20)

The ﬁrst term in (20) can then be written as




(cid:90) ∞

(cid:90) 0

2˜cl−1
µ2



0

√

−˜cl−1 z1
1−(˜cl−1)2

f (z1)f (z2)z2

1dz2dz1 +

f (z1)f (z2)z2

1dz2dz1

(21)

(cid:90) ∞

(cid:90) ∞

0

0






.

In (21), the ﬁrst term inside the braces is given by

(cid:90) ∞

(cid:90) 0

0

√

−˜cl−1z1
1−(˜cl−1)2

f (z1)f (z2)z2

1dz2dz1 =

f (z1)z2

1erf

(cid:90) ∞

0

1
2

1
2π

1
2π

=

=

(cid:34)
˜cl−1(cid:113)

(cid:20)
˜cl−1(cid:113)

(cid:33)

dz1

(cid:32)

˜cl−1z1
(cid:112)1 − (˜cl−1)
(cid:32)

1 − (˜cl−1)2 + tan−1

(cid:33)(cid:35)

˜cl−1
(cid:112)1 − (˜cl−1)2

1 − (˜cl−1)2 + sin−1 (cid:0)˜cl−1(cid:1)

(cid:21)

(22)

with erf(a) = 1
π

(cid:82) a
−a e−t2

dt. The second term inside the braces in (21) equals

Therfore, (21) becomes

(cid:90) ∞

(cid:90) ∞

0

0

f (z1)f (z2)z2

1dz2dz1 =

f (z1)z2

1dz1

(cid:90) ∞

0

1
2
1
4

=

.

(cid:113)

(˜cl−1)2
µ2π

1 − (˜cl−1)2 +

sin−1 (cid:0)˜cl−1(cid:1) +

˜cl−1
µ2π

˜cl−1
2µ2

Similarly, the second term in (20) can be split up as follows

2(cid:112)1 − (˜cl−1)2
µ2




(cid:90) ∞

(cid:90) 0



0

√

−˜cl−1z1
1−(˜cl−1)2

f (z1)f (z2)z1z2dz2dz1 +

f (z1)f (z2)z1z2dz2dz1

.

(cid:90) ∞

(cid:90) ∞

0

0

The ﬁrst term inside the braces of (25) is

(cid:90) ∞

(cid:90) 0

0

√

−˜cl−1z1
1−(˜cl−1 )2

f (z1)f (z2)z1z2dz2dz1 =

f (z1)z1

2(1−(˜cl−1)2) − 1

dz1

˜cl−1z2
1

(cid:35)

(cid:34)

−
e

(23)

(24)





(25)

(26)

0
(cid:26) 1 − (˜cl−1)2
√

2π

(cid:27)

−

1
√
2π

(cid:90) ∞

1
√
2π

=

1
√
2π
(˜cl−1)2
2π

= −

13

and the second term is

(cid:90) ∞

(cid:90) ∞

0

0

f (z1)f (z2)z1z2dz2dz1 =

f (z1)z1dz1

(cid:90) ∞

0

1
√
2π

1
2π

.

=

Putting these two terms together, (25) becomes

(cid:113)

−

(˜cl−1)2
µ2π

1 − (˜cl−1)2 +

1 − (˜cl−1)2.

(cid:113)

1
µ2π

Finally, summing all the terms in (24) and (28) gives (19) as

˜cl =

1
µ2

(cid:40)

˜cl−1sin−1 (cid:0)˜cl−1(cid:1) + (cid:112)1 − (˜cl−1)2
π

+

˜cl−1
2

(cid:41)

.

We note that for the noiseless case, (29) is identical to the result recently obtained by Hayou et al.
(2018), where the authors used a slightly different approach.

B.3 Depth scales for trainability

We recap the result in Schoenholz et al. (2017) and adapt the derivation for the speciﬁc case of a
noisy ReLU network. Let cl = c∗ + εl, such that as long as liml→∞cl = c∗ exist we have that ε → 0
as l → ∞. Then Schoenholz et al. (2017) derived the following asymptotic recurrence relation

εl+1 = εlχ(c∗) + O((εl)2),

where

√

1 = ˜u1 =

with ˜u∗
network where σ2

˜q∗z1 and ˜u∗

˜q∗

2 =
, we have that

w = 2
µ2

χ(c∗) = σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u∗

√

(cid:104)

˜c∗z1 + (cid:112)1 − (˜c∗)2z2

2)]] ,

1)φ(cid:48)(˜u∗
(cid:105)

. Now, speciﬁcally for a noisy ReLU

χ(c∗) =

f (z1)f (z2)φ(cid:48)(˜u∗

1)φ(cid:48)(˜u∗

2)dz2dz1

(cid:90) ∞

(cid:90) ∞

−∞
(cid:90) ∞

−∞
(cid:90) ∞

2
µ2
2
µ2

2
µ2

2
µ2
1
µ2π

=

=

=

=

0

(cid:90) ∞

0
(cid:34)

1
2π

f (z1)f (z2)dz2dz1

− c∗ z1√
1−(c∗ )2
(cid:34)

(cid:32)

f (z1)

erf

√

1
2
(cid:32)

tan−1

c∗
(cid:112)1 − (c∗)2
(cid:105)

c∗z1
2(cid:112)1 − (c∗)2
(cid:35)

(cid:33)

+

1
4

(cid:104)
sin−1 (c∗) +

π
2

(cid:33)

(cid:35)

+ 1

dz1

Note that χ(c∗) is a constant, thus for large l the solution to the recurrence relation in (30) is expected
to be exponential, i.e. εl ∼ e−l/ξc. Here ξc, is considered the depth scale, which controls how deep
discriminatory information about the inputs can propagate through the network. We can then solve
for ξc to ﬁnd

ξc = −1/ln(χ(c∗)) = −ln

(cid:20) sin−1 (c∗)
µ2π

+

1
2µ2

(cid:21)−1

.

14

(27)

(28)

(29)

(30)

(31)

(32)

(33)

B.4 Variance of error signals

Under the mean ﬁeld assumption, Schoenholz et al. (2017) approximates the error signal at layer l by
a zero mean Gaussian with variance

where ˜ql
noisy ReLU network we have that

i)2], with ˜δl

δ = E[(˜δl

i = φ(cid:48)(˜hl

˜δl+1
j W l+1

ji

. In our context, for a critically initialised

(cid:20)
φ(cid:48) (cid:16)(cid:112)

˜qlz

(cid:17)2(cid:21)

,

σ2
w

Ez

δ = ˜ql+1
˜ql
δ

Dl+1
Dl
i) (cid:80)Dl+1

j=1

δ = ˜ql+1
˜ql
δ

= ˜ql+1
δ

(cid:90) ∞

0

Dl+1
Dl
Dl+1
Dl

2
µ2
1
µ2

.

f (z)dz

B.5 Correlation between error signals

The covariance between error signals is approximated using

ab,δ = ˜ql+1
˜ql
ab,δ

σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u1)φ(cid:48)(˜u2)]] ,

Dl+1
Dl+2

where ˜u1 and ˜u2 are deﬁned as was done in the forward pass. Here, we simply use the result in (32)
for noisy ReLU networks to ﬁnd

ab,δ = ˜ql+1
˜ql
ab,δ

χ(c∗)

Dl+1
Dl+2
Dl+1

= ˜ql+1
ab,δ

(cid:2)sin−1 (c∗) + π
Dl+2µ2π

2

(cid:3)

.

(34)

(35)

(36)

(37)

(38)

(39)

In this section we provide additional details regarding our experiments in the paper. Code to reproduce
all the experiments is available at https://github.com/ElanVB/noisy_signal_prop.

C Experimental details

C.1

Input data

For all experiments the network input data properties that remain consistent (unless stated otherwise)
are as follows: each observation consists of 1000 features and each feature value is drawn i.i.d. from
a standard normal distribution.

C.2 Variance propagation dynamics

The experiments conducted to gather results for Figures 2 and 3 aim to empirically show the
relationship between the variances at arbitrary layers in a neural network.

Iterative map: For the results depicted in Figures 2 (a) and 3 (a), the experimental set up is as follows.
The data used as input to these experiments comprises of 30 sets of 30 observations. The input is
scaled such that the variance of observations within each set is the same and the variance across
each set is different and forms a range of qset ∈ [0, 15]. As such, our results are averaged over 30
observations and 50 samplings of initial weights to a single hidden-layer network.

Convergence dynamics: For the results depicted in Figures 2 (b) and 3 (b), the experimental set up is
as follows. The data used as input to these experiments comprises of a set of 50 observations scaled
such that each observation’s variance is four (q = 4). As such, our results are averaged over 50
observations and 50 samplings of initial weights to a 15 hidden-layer network.

C.3 Correlation propagation dynamics

The experiments conducted to gather results for Figure 4 and 5 aim to empirically show the relation-
ship between the correlations of observations at arbitrary layers in a neural network.

15

Iterative map: For the results depicted in Figure 4 (a), the experimental set up is as follows. The data
used as input to these experiments comprises of 50 sets of 50 observations. The ﬁrst observation in
each set is sampled from a standard normal distribution and subsequent observations are generated
such that the correlation between the ﬁrst element and the ith element form a range of corr0,i ∈ [0, 1].
As such, our results are averaged over 50 observations and 50 samplings of initial weights to a single
hidden-layer network.

Convergence dynamics: For the results depicted in Figure 4 (b), the experimental set up is as
follows. The data used as input to these experiments comprises of three sets of 50 equally correlated
observations. Each set has a different correlation value such that corrset ∈ {0, 0.5, 0.9}. As such, our
results are averaged over 50 observations and 50 samplings of initial weights to a 15 hidden-layer
network.

Conﬁrmation of exponential rate of convergence for correlations: This section discusses how the
results depicted in Figure 5 are acquired. These experiments support the assumption that the rate
of convergence for correlations is exponential when using noise regularisation with rectiﬁer neural
networks. The experimental set up for this section is very similar to that of the above convergence
dynamics experiment, the only difference being the statistics we calculate from the correlation values.
The aspect of this experiment that may seem the most unclear is the reason why we claim that the
negative inverse slope of a linear ﬁt to the log differences in correlation should match the theoretical
values for ξc. The derivation to justify this is as follows. If a good ﬁt of the form al + b can be found
in the logarithmic domain for the rate of convergence, it would strongly indicate that the convergence
rate is exponential. Following this, we set the problem up like so:

Let us now assume that ln (cid:0)|cl − c∗|(cid:1) can be linearly approximated:

Since we are concerned with deep neural networks, we can take the limit as l becomes arbitrarily
large and see that as l grows the effect of b decreases (liml→∞ |al| (cid:29) |b|). Thus, we continue like so:

Thus, we have come to the ﬁnding that if the correlation rate of convergence is exponential and we
work with deep neural networks, the negative inverse slope of a linear ﬁt to the log differences in
correlation should match the theoretical values for ξc. Figure 5 shows that the theory closely matches
this approximation.

C.4 Depth scales

This section handles the experiments conducted related to determining the maximum depth variance
information can stably propagate through a network and the depth at which these networks can be
trained, both depicted in Figure 6.

The MNIST and CIFAR-10 datasets were used and were pre-processed using standard techniques.
Throughout these experiments mini-batches of 128 observations were used.

Variance depth scales: The experiments depicted in Figures 6 (a) and (d) are interested in testing the
numerical stability of networks initialised using different σ2
w values while using 32-bit ﬂoating point

16

|cl − c∗| ≈ e−l/ξc

∴ ln (cid:0)|cl − c∗|(cid:1) ≈

−l
ξc

.

∴ ln (cid:0)|cl − c∗|(cid:1) ≈ al + b,

∴ al + b ≈

−l
ξc

,

∴ ξc ≈

−l
al + b

.

−l
al

lim
l→∞

ξc ≈ lim
l→∞
1
a

≈ −

.

numbers. To test the depth of stable variance propagation, a network with 1000 hidden layers is used.
The network used in this experiment makes use of dropout with p = 0.6, where p is the probability
of keeping a neuron’s value, thus the critical value for σ2
w is 1.2. As such, a linearly spaced range of
σ2
w ∈ [0.1, 2.5] is used to select 25 different values.

We use the following approach to predict the depth beyond which variances become numerically
unstable. At criticality for multiplicative noise (σw, σb) = ((cid:112)2/µ2, 0), however, for weights
initialised off this critical point (18) becomes

˜ql = ˜ql−1

(cid:20)
˜ql−2

=

(cid:19)

(cid:18) σ2
wµ2
2
(cid:18) σ2
wµ2
2
(cid:19)l

.

= ˜q0

(cid:18) σ2
wµ2
2

(cid:19)

(cid:19)(cid:21) (cid:18) σ2
wµ2
2

(40)

(41)

w > 2
µ2

If σ2
, we let ˜ql = K, where K is the largest positive number representable by the computer. In
our case, using 32-bit ﬂoating point precision, this number is equal to 3.4028235 × 1038. Otherwise,
we select K = 1.1754944 × 10−38, the smallest possible positive number. Furthermore,
if σ2
let L∗ represent the layer l in (40) at which the value K is reached, then we can scale our input data
such that ˜q0 = 1 and solve for L∗ to ﬁnd

w < 2
µ2

L∗ = ln(K)/ln

(cid:18) σ2
wµ2
2

(cid:19)

.

Therefore, we expect numerical instability issues to occur beyond a depth of L∗.

Trainable depth scales: The experiments depicted in Figures 6 (b), (c), (e) and (f) are concerned
with determining at what depth a critically initialised network with a speciﬁed dropout rate can train
effectively. To this end, 10 linearly spaced values for dropout on the range p ∈ [0.1, 1.0] and 10
linearly spaced network depths on the integer range l ∈ [2, 40] are tested.

The task presented to the network in this experiment is to learn the identity function within 200
epochs. As such, the network is set up as an auto-encoder and uses stochastic gradient decent with a
learning rate of 10−3. The input data is divided into a training and validation set, each containing
50000 and 10000 observations respectively.

C.5 Additional results

In this section we provide some additional experiments on the training dynamics of deep noisy ReLU
networks from different initialisations.

In Figure 7 we compare the standard “He” initialisation (blue) with the critical initialisation (green)
for a ReLU network with dropout regularisation (p = 0.8). By not initialising at criticality due to
dropout noise, the variance map for the “He” strategy no longer lies on the identity line in (a) and as
a result, the forward propagating signal can be seen to explode in (b). However, by compensating for
the amount of injected noise, the above derived critical initialisation for dropout preserves the signal
throughout the entire forward pass, with roughly constant variance dynamics.

Next, we provide some additional experiments on the trainability of deep ReLU networks with
dropout on real-world data sets.

From our analysis in the paper, we expect that as the depth of the network increases, any initialisation
strategy that does not factor in the effects of noise, will cause the forward propagating signal to
become increasingly unstable. For very deep networks, this might cause the signal to either explode
or vanish, even within the ﬁrst forward pass, making the network untrainable.

To test this, we trained a denoising autoencoder network with dropout noise (p = 0.6) on MNIST and
CIFAR-10 using squared reconstruction loss. We consider several network depths (L = 30, 100, 200),
learning rates (α = 0.1, 0.01, 0.001, 0.0001) and optimisation procedures (SGD and Adam), with
1000 neurons in each layer. The results for training on CIFAR-10 are shown in Figure 8 for both the
“He” intialisation (blue) and the critical dropout initialisation (green). (For MNIST, see Figure 9; the

17

Figure 7: Critical initialisation for ReLU networks with dropout. Lines correspond to theoretical
predictions and points to numerical simulations, for random ReLU networks with dropout (p = 0.8),
initialised according to the method proposed by He et al. (2015) (blue) and at criticality (green).
(a): Iterative variance map where the identity line is displayed as a dashed black line. (b): Variance
dynamics during forward signal propagation.

Figure 8: Comparing the “He” initialisation strategy to critical dropout initialisation for ReLU
networks using dropout (p = 0.6) on CIFAR-10. While networks initialised at criticality (green) are
able to train at large depths (L = 200) as seen in the bottom row, networks initialised with the “He”
strategy (blue) become untrainable irrespective of the chosen learning rate or optimisation procedure.
An “X” marks the point at which a network completely stopped training. Training losses and number
of network updates are shown in log-scale.

core trends and resulting conclusions regarding network trainability is the same for both data sets,
which we discuss below.)

As the depth increases, moving from the top to the bottom row in Figure 8, networks initialised at
the critical point for dropout seem to remain trainable even up to a depth of 200 layers (we see the
loss start to decrease over ﬁve epochs). In contrast, networks using the “He” initialisation become
increasingly more difﬁcult to train, with no training taking place at very large depths. These ﬁndings
make sense in terms of the variance dynamics analysed in the paper, however, these experimental
successes seem to run counter to our theoretical analysis of trainable depth scales (this contradiction
can also be seen in Figure 6). Understanding this discrepancy is of particular interest to us.

To verify that the lack of training in Figure 8 is due to poor signal propagation, we plot the empirical
variance of the pre-activations in Figure 10, for the ﬁrst forward pass of a 200 layer autoencoder

18

Figure 9: Comparing the “He” initialisation strategy to critical dropout initialisation for ReLU
networks using dropout (p = 0.6) on MNIST. While networks initialised at criticality (green) are
able to train at large depths (L = 200) as seen in the bottom row, networks initialised with the “He”
strategy (blue) become untrainable irrespective of the chosen learning rate or optimisation procedure.
An “X” marks the point at which a network completely stopped training. Training losses and number
of network updates are shown in log-scale.

Figure 10: Variance dynamics for signal propagation in the ﬁrst forward pass for a 200 layer
autoencoder network fed a batch of 500 training examples from CIFAR-10. (a) Exploding activation
variance (blue) reaching overﬂow levels (marked with a red “X”) for the “He” intialisation, with no
signal reaching the output layer (shown in log-scale). (b) Zoomed in display of the roughly constant
variance dynamics in (a) for the critical dropout initialisation.

network. For the “He” initialisation, the variance in (a) grows rapidly to the point of causing numerical
instability and overﬂow (indicated by the red dashed line), well before any signal is able to reach the
output layer. However as shown in (b), by initialising at criticality, signal is able to propagate reliably
even at large depths.

19

Figure 11: Depth scale experiments on MNIST and CIFAR-10. (a) Depth scales ﬁt to the training loss
on MNIST for networks initialised at criticality for dropout rates p = 0.1 (severe dropout) to p = 1
(no dropout). (b) Depth scales ﬁt to the validation loss on MNIST. (c) - (d): Similar to (a) - (c), but
for CIFAR-10.

20

8
1
0
2
 
v
o
N
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
3
9
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Critical initialisation for deep signal propagation in
noisy rectiﬁer neural networks

Arnu Pretorius∗
Computer Science Division
CAIR†
Stellenbosch University

Elan Van Biljon
Computer Science Division
Stellenbosch University

Steve Kroon
Computer Science Division
Stellenbosch University

Herman Kamper
Department of Electrical and Electronic Engineering
Stellenbosch University

Abstract

Stochastic regularisation is an important weapon in the arsenal of a deep learning
practitioner. However, despite recent theoretical advances, our understanding of
how noise inﬂuences signal propagation in deep neural networks remains limited.
By extending recent work based on mean ﬁeld theory, we develop a new framework
for signal propagation in stochastic regularised neural networks. Our noisy signal
propagation theory can incorporate several common noise distributions, including
additive and multiplicative Gaussian noise as well as dropout. We use this frame-
work to investigate initialisation strategies for noisy ReLU networks. We show that
no critical initialisation strategy exists using additive noise, with signal propagation
exploding regardless of the selected noise distribution. For multiplicative noise
(e.g. dropout), we identify alternative critical initialisation strategies that depend
on the second moment of the noise distribution. Simulations and experiments on
real-world data conﬁrm that our proposed initialisation is able to stably propagate
signals in deep networks, while using an initialisation disregarding noise fails to do
so. Furthermore, we analyse correlation dynamics between inputs. Stronger noise
regularisation is shown to reduce the depth to which discriminatory information
about the inputs to a noisy ReLU network is able to propagate, even when initialised
at criticality. We support our theoretical predictions for these trainable depths with
simulations, as well as with experiments on MNIST and CIFAR-10.‡

1

Introduction

Over the last few years, advances in network design strategies have made it easier to train large
networks and have helped to reduce overﬁtting. These advances include improved weight initialisation
strategies (Glorot and Bengio, 2010; Saxe et al., 2014; Sussillo and Abbott, 2014; He et al., 2015;
Mishkin and Matas, 2016), non-saturating activation functions (Glorot et al., 2011) and stochastic
regularisation techniques (Srivastava et al., 2014). Authors have noted, for instance, the critical
dependence of successful training on noise-based methods such as dropout (Krizhevsky et al., 2012;
Dahl et al., 2013).

∗Correspondence: arnupretorius@gmail.com
†CSIR/SU Centre for Artiﬁcial Intelligence Research.
‡Code to reproduce all the results is available at https://github.com/ElanVB/noisy_signal_prop

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

Figure 1: Noisy layer recursion. The input xl−1 from the previous layer gets corrupted by the sampled
noise (cid:15)l−1, either by vector addition or component-wise multiplication, producing the noisy inputs
˜xl−1. The lth layer’s corrupted pre-activations are then computed by multiplication with the layer
weight matrix W l, followed by a vector addition of the biases bl. Finally, the inputs to the next layer
are simply the activations of the current layer, i.e. xl = φ(˜hl).

In many cases, successful results arise only from effective combination of these advances. Despite
this interdependence, our theoretical understanding of how these mechanisms and their interactions
affect neural networks remains impoverished.

One approach to studying these effects is through the lens of deep neural signal propagation. By
modelling the empirical input variance dynamics at the point of random initialisation, Saxe et al.
(2014) were able to derive equations capable of describing how signal propagates in nonlinear fully
connected feed-forward neural networks. This “mean ﬁeld” theory was subsequently extended by
Poole et al. (2016) and Schoenholz et al. (2017), in particular, to analyse signal correlation dynamics.
These analyses highlighted the existence of a critical boundary at initialisation, referred to as the “edge
of chaos”. This boundary deﬁnes a transition between ordered (vanishing), and chaotic (exploding)
regimes for neural signal propagation. Subsequently, the mean ﬁeld approximation to random neural
networks has been employed to analyse other popular neural architectures (Yang and Schoenholz,
2017; Xiao et al., 2018; Chen et al., 2018).

This paper focuses on the effect of noise on signal propagation in deep neural networks. Firstly we
ask: How is signal propagation in deep neural networks affected by noise? To gain some insight into
this question, we extend the mean ﬁeld theory developed by Schoenholz et al. (2017) for the special
case of dropout noise, into a generalised framework capable of describing the signal propagation
behaviour of stochastically regularised neural networks for different noise distributions.

Secondly we ask: How much are current weight initialisation strategies affected by noise-induced
regularisation in terms of their ability to initialise at a critical point for stable signal propagation?
Using our derived theory, we investigate this question speciﬁcally for rectiﬁed linear unit (ReLU)
networks. In particular, we show that no such critical initialisation exists for arbitrary zero-mean
additive noise distributions. However, for multiplicative noise, such an initialisation is shown to be
possible, given that it takes into account the amount of noise being injected into the network. Using
these insights, we derive novel critical initialisation strategies for several different multiplicative
noise distributions.

Finally, we ask: Given that a network is initialised at criticality, in what way does noise inﬂuence
the network’s ability to propagate useful information about its inputs? By analysing the correlation
between inputs as a function of depth in random deep ReLU networks, we highlight the following:
even though the statistics of individual inputs are able to propagate arbitrarily deep at criticality,
discriminatory information about the inputs becomes lost at shallower depths as the noise in the
network is increased. This is because in the later layers of a random noisy network, the internal
representations from different inputs become uniformly correlated. Therefore, the application of
noise regularisation directly limits the trainable depth of critically initialised ReLU networks.

2 Noisy signal propagation

We begin by presenting mean ﬁeld equations for stochastically regularised fully connected feed-
forward neural networks, allowing us to study noisy signal propagation for a variety of noise
distributions. To understand how noise inﬂuences signal propagation in a random network given an
input x0 ∈ RD0 , we inject noise into the model

˜hl = W l(xl−1 (cid:12) (cid:15)l−1) + bl, spa for l = 1, ..., L

(1)

2

using the operator (cid:12) to denote either addition or multiplication where (cid:15)l is an input noise vector,
sampled from a pre-speciﬁed noise distribution. For additive noise, the distribution is assumed to be
zero mean, for multiplicative noise distributions, the mean is assumed to be equal to one. The weights
W l ∈ RDl×Dl−1 and biases bl ∈ RDl are sampled i.i.d. from zero mean Gaussian distributions
with variances σ2
b , respectively, where Dl denotes the dimensionality of the lth hidden
layer in the network. The hidden layer activations xl = φ(˜hl) are computed element-wise using
an activation function φ(·), for layers l = 1, ..., L. Figure 1 illustrates this recursive sequence of
operations.

w/Dl−1 and σ2

To describe forward signal propagation for the model in (1), we make use of the mean ﬁeld approxi-
mation as in Poole et al. (2016) and analyse the statistics of the internal representations of the network
in expectation over the parameters and the noise. Since the weights and biases are sampled from zero
mean Gaussian distributions with pre-speciﬁed variances, we can approximate the distribution of the
pre-activations at layer l, in the large width limit, by a zero mean Gaussian with variance

˜ql = σ2
w

(cid:26)

(cid:20)

φ

Ez

(cid:16)(cid:112)

˜ql−1z

(cid:17)2(cid:21)

(cid:27)

(cid:12) µl−1
2

+ σ2
b ,

2 = E(cid:15)[((cid:15)l)2] is the
where z ∼ N (0, 1) (see Section A.1 in the supplementary material). Here, µl
second moment of the noise distribution being sampled from at layer l. The initial input variance is
given by q0 = 1
x0 · x0. Furthermore, to study the behaviour of a pair of signals from two different
D0
inputs, x0,a and x0,b, passing through the network, we can compute the covariance at each layer as

Ez1 [Ez2 [φ(˜u1)φ(˜u2)]] + σ2
b

ab = σ2
˜ql
w
(cid:113)

(cid:104)

˜ql−1
bb
(cid:113)

where ˜u1 =

˜ql−1
aa z1 and ˜u2 =

(cid:112)

˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

, with the correlation between

inputs at layer l given by ˜cl = ˜ql
aa ˜ql
˜ql
supplementary material for more details).

ab/

bb. Here, ql

aa is the variance of ˜hl,a

j

(see Section A.2 in the

For the backward pass, we use the equations derived in Schoenholz et al. (2017) to describe error
signal propagation.1 In the context of mean ﬁeld theory, the expected magnitude of the gradient at
each layer can be shown to be proportional to the variance of the error, ˜δl
˜δl+1
j W l+1
.
This allows for the distribution of the error signal at layer l to be approximated by a zero mean
Gaussian with variance

i) (cid:80)Dl+1

i = φ(cid:48)(˜hl

j=1

ji

Similarly, for noise regularised networks, the covariance between error signals can be shown to be

δ = ˜ql+1
˜ql
δ

Dl+1
Dl

σ2
w

Ez

(cid:20)
φ(cid:48) (cid:16)(cid:112)

˜qlz

(cid:17)2(cid:21)

.

ab,δ = ˜ql+1
˜ql
ab,δ

σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u1)φ(cid:48)(˜u2)]] ,

Dl+1
Dl+2

(2)

(3)

(4)

(5)

where ˜u1 and ˜u2 are deﬁned as was done in the forward pass.

Equations (2)-(5) fully capture the relevant statistics that govern signal propagation for a random
network during both the forward and the backward pass. In the remainder of this paper, we consider,
as was done by Schoenholz et al. (2017), the following necessary condition for training: “for a
random network to be trained information about the inputs should be able to propagate forward
through the network, and information about the gradients should be able to propagate backwards
through the network.” The behaviour of the network at this stage depends on the choice of activation,
noise regulariser and initial parameters. In the following section, we will focus on networks that use
the Rectiﬁed Linear Unit (ReLU) as activation function. The chosen noise regulariser is considered a
design choice left to the practitioner. Therefore, whether a random noisy ReLU network satisﬁes the
above stated necessary condition for training largely depends on the starting parameter values of the
network, i.e. its initialisation.

1It is, however, important to note that the derivation relies on the assumption that the weights used in the

forward pass are sampled independently from those used during backpropagation.

3

Figure 2: Deep signal propagation with and without noise. (a): Iterative variance map. (b): Variance
dynamics during forward signal propagation. In (a) and (b), lines correspond to theoretical predictions
and points to numerical simulations (means over 50 runs with shaded one standard deviation bounds),
for noiseless tanh (yellow) and noiseless ReLU (purple) networks, as well as for noisy tanh (red)
and noisy ReLU (brown) networks regularised using additive noise from a standard Gaussian. Both
tanh networks use (σw, σb) = (1, 0), the “Xavier” initialisation (Glorot and Bengio, 2010), while the
2, 0) the “He” initialisation (He et al., 2015). In our experiments,
ReLU networks use (σw, σb) = (
we use network layers consisting of 1000 hidden units (see Section C in the supplementary material
for more details on all our simulated experiments).

√

3 Critical initialisation for noisy rectiﬁer networks

Unlike the tanh nonlinearity investigated in previous work (Poole et al., 2016; Schoenholz et al.,
2017), rectifying activation functions such as ReLU are unbounded. This means that the statistics of
signal propagation through the network is not guaranteed to naturally stabilise through saturating
activations, as shown in Figure 2.

A point on the identity line in Figure 2 (a) represents a ﬁxed point to the recursive variance map
in equation (2). At a ﬁxed point, signal will stably propagate through the remaining layers of the
network. For tanh networks, such a ﬁxed point always exists irrespective of the initialisation, or
the amount of noise injected into the network. For ReLU networks, this is not the case. Consider
the “He” initialisation (He et al., 2015) for ReLU, commonly used in practice. In (b), we plot the
variance dynamics for this initialisation in purple and observe stable behaviour. But what happens
when we inject noise into each network? In the case of tanh (shown in red), the added noise simply
shifts the ﬁxed point to a new stable value. However, for ReLU, the noise entirely destroys the ﬁxed
point for the “He” initialisation, making signal propagation unstable. This can be seen in (a), where
the variance map for noisy ReLU (shown in brown) moves off the identity line entirely, causing the
signal in (b) to explode.

Therefore, to investigate whether signal can stably propagate through a random noisy ReLU network,
we examine (2) more closely, which for ReLU becomes (see Section B.1 in supplementary material)

˜ql = σ2
w

(cid:20) ˜ql−1
2

(cid:21)

(cid:12) µ2

+ σ2
b .

(6)

For ease of exposition we assume equal noise levels at each layer, i.e. µl
2 = µ2, ∀l. A critical
initialisation for a noisy ReLU network occurs when the tuple (σw, σb, µ2) provides a ﬁxed point ˜q∗,
to the recurrence in (6). This at least ensures that the statistics of individual inputs to the network will
be preserved throughout the ﬁrst forward pass. The existence of such a solution depends on the type
2 ˜q∗ + µ2σ2
w + σ2
of noise that is injected into the network. In the case of additive noise, ˜q∗ = σ2
b ,
w
√
implying that the only critical point initialisation for non-zero ˜q∗ is given by (σw, σb, µ2) = (
2, 0, 0).
Therefore, critical initialisation is not possible using any amount of zero-mean additive noise,
regardless of the noise distribution. For multiplicative noise, ˜q∗ = σ2
b , so the solution
w
(σw, σb, µ2) =
provides a critical initialisation for noise distributions with mean
one and a non-zero second moment µ2. For example, in the case of multiplicative Gaussian noise,
µ2 = σ2
. For dropout noise,

(cid:15) + 1, yielding critical initialisation with (σw, σb) =

2 ˜q∗µ2 + σ2

(cid:16)(cid:113) 2
µ2

(cid:16)(cid:113) 2

(cid:17)
σ2+1 , 0

, 0, µ2

(cid:17)

1

1

4

Table 1: Critical point initialisation for noisy ReLU networks.

DISTRIBUTION

P((cid:15))

— ADDITIVE NOISE —

µ2

σ2
(cid:15)

2β2

CRITICAL INITIALISATION

(σw, σb, σ(cid:15)) = (
√

(σw, σb, β) = (

√

2, 0, 0)

2, 0, 0)

N (0, σ2
(cid:15) )

Lap(0, β)

— MULTIPLICATIVE NOISE —

N (1, σ2
(cid:15) )

(σ2

(cid:15) + 1)

(σw, σb, σ(cid:15)) =

Lap(1, β)

(2β2 + 1)

(σw, σb, β) =

(cid:17)

(cid:16)(cid:113) 2
(cid:15) +1 , 0, σ(cid:15)
σ2
(cid:17)

(cid:16)(cid:113) 2

2β2+1 , 0, β

P oi(1)

P((cid:15) = 1
P((cid:15) = 0) = 1 − p

p ) = p,

2

1
p

(σw, σb, λ) = (1, 0, 1)

(σw, σb, p) = (

2p, 0, p)

√

GAUSSIAN

LAPLACE

GAUSSIAN

LAPLACE

POISSON

DROPOUT

Figure 3: Critical initialisation for noisy ReLU networks. (a): Iterative variance map. (b): Vari-
ance dynamics during forward signal propagation. In (a) and (b), lines correspond to theoretical
predictions and points to numerical simulations. Dropout (p = 0.6) is shown in green for dif-
w = 2(0.6) = 2
(critical), σ2
ferent initialisations, σ2
(exploding sig-
µ2
(0.6)−1 < 2
w = (0.85)2
nal) and σ2
(vanishing signal). Similarly, multiplicative Gaussian noise
µ2
(σ(cid:15) = 0.25) is shown in red with σ2
w =
(exploding) and
w = (0.75)2 2
σ2
( vanishing). (c): Variance critical boundary for initialisation, separating numerical
µ2
overﬂow and underﬂow signal propagation regimes.

w = (1.25)2 2
µ2

(0.25)2+1 = 2
µ2

(0.6)−1 > 2
µ2

w = (1.15)2

(critical), σ2

2

2

2

√

µ2 = 1/p (with p the probability of retaining a neuron); thus, to initialise at criticality, we must
2p, 0). Table 1 summarises critical initialisations for some commonly used
set (σw, σb) = (
noise distributions. We also note that similar results can be derived for other rectifying activation
functions; for example, for multiplicative noise the critical initialisation for parametric ReLU (PReLU)
activations (with slope parameter α) is given by (σw, σb, µ2) =

(cid:16)(cid:113) 2

(cid:17)

.

µ2(α2+1) , 0, µ2

To see the effect of initialising on or off the critical point for ReLU networks, Figure 3 compares
the predicted versus simulated variance dynamics for different initialisation schemes. For schemes
not initialising at criticality, the variance map in (a) no longer lies on the identity line and as a result
the forward propagating signal in (b) either explodes, or vanishes. In contrast, the initialisations
derived above lie on the critical boundary between these two extremes, as shown in (c) as a function
of the noise. By compensating for the amount of injected noise, the signal corresponding to the
initialisation σ2
is preserved in (b) throughout the entire forward pass, with roughly constant
variance dynamics.

w = 2
µ2

5

Figure 4: Propagating correlation information in noisy ReLU networks. (a): Iterative correlation
map with ﬁxed points indicated by “X” marks on the identity line. (b): Correlation dynamics during
forward signal propagation. In (a) and (b), lines correspond to theoretical predictions and points to
numerical simulations. All simulated networks were initialised at criticality for each noise type and
level. (c): Slope at the ﬁxed point correlation as a function of the amount of noise injected into the
network.

Next, we investigate the correlation dynamics between inputs. Assuming that (6) is at its ﬁxed point
˜q∗, which exists only if σ2
, the correlation map for a noisy ReLU network is given by (see
Section B.2 in supplementary material)

w = 2
µ2

˜cl =

1
µ2

(cid:40)

˜cl−1sin−1 (cid:0)˜cl−1(cid:1) + (cid:112)1 − (˜cl−1)2
π

+

˜cl−1
2

(cid:41)

.

Figure 4 plots this theoretical correlation map against simulated dynamics for different noise types
and levels. For no noise, the ﬁxed point c∗ in (a) is situated at one (marked with an “X” on the blue
line). The slope of the blue line indicates a non-decreasing function of the input correlations. After a
certain depth, inputs end up perfectly correlated irrespective of their starting correlation, as shown in
(b). In other words, random deep ReLU networks lose discriminatory information about their inputs
as the depth of the network increases, even when initialised at criticality. When noise is added to the
network, inputs decorrelate and c∗ moves away from one. However, more importantly, correlation
information in the inputs become lost at shallower depths as the noise level increases, as can be seen
in (b).

How quickly a random network loses information about its inputs depends on the rate of convergence
to the ﬁxed point c∗. Using this observation, Schoenholz et al. (2017) derived so-called depth scales
ξc, by assuming |cl − c∗| ∼ e−l/ξc. These scales essentially control the feasible depth at which
networks can be considered trainable, since they may still allow useful correlation information to
propagate through the network. In our case, the depth scale for a noisy ReLU network under this
assumption can be shown to be (see Section B.3 in supplementary material)

where

ξc = −1/ln [χ(c∗)] ,

χ(c∗) =

1
µ2π

(cid:104)
sin−1 (c∗) +

(cid:105)

.

π
2

(7)

(8)

(9)

The exponential rate assumption underlying the derivation of (8) is supported in Figure 5, where
for different noise types and levels, we plot |cl − c∗| as a function of depth on a log-scale, with
corresponding linear ﬁts (see panels (a) and (c)). We then compare the theoretical depth scales from
(8) to actual depth scales obtained through simulation (panels (b) and (d)), as a function of noise
and observe a good ﬁt for non-zero noise levels.4 We thus ﬁnd that noise limits the depth at which
critically initialised ReLU networks are expected to perform well through training.

4We note Hayou et al. (2018) recently showed that the rate of convergence for noiseless ReLU networks is
not exponential, but polynomial instead. Interestingly, keeping with the exponential rate assumption, we indeed
ﬁnd that the discrepancy between our theoretical depth scales from (8) and our simulated depth scales, is largest
at very low noise levels. However, at more typical noise levels, such as a dropout rate of p = 0.5 for example,
the assumption seems to provide a close ﬁt, with good agreement between theory and simulation.

6

Figure 5: Noise dependent depth scales for training. (a): Linear ﬁts (dashed lines) to |cl − c∗| as a
function of depth on a log-scale (solid lines) for varying amounts of dropout (p = 0.1 to p = 0.9
by 0.1). (b): Theoretical depth scales (solid lines) versus empirically inferred scales (dashed lines)
per dropout rate. Scales are inferred noting that if |cl − c∗| ∼ e−l/ξc, then a linear ﬁt, al + b, in
the logarithmic domain gives ξc ≈ − 1
a , for large l. In other words, the negative inverse slope of a
linear ﬁt to the log differences in correlation should match the theoretical values for ξc. Therefore,
we compare ξc = −1/ln [χ(c∗)] to − 1
a for different levels of noise. (c) - (d): Similar to (a) and (b),
but for Gaussian noise (σ(cid:15) = 0.1 to σ(cid:15) = 1.9 by 0.15).

We next brieﬂy discuss error signal propagation during the backward pass for noise regularised ReLU
networks. When critically initialised, the error variance recurrence relation in (4) for these networks
is (see Section B.4 in supplementary material)

with the covariance between error signals in (5), given by (see Section B.5 in supplementary material)

δ = ˜ql+1
˜ql
δ

Dl+1
Dlµ2

,

ab,δ = ˜ql+1
˜ql
ab,δ

Dl+1
Dl+2

χ(c∗).

(10)

(11)

Note the explicit dependence on the width of the layers of the network in (10) and (11). We ﬁrst
consider constant width networks, where Dl+1 = Dl, for all l = 1, ..., L. For any amount of
multiplicative noise, µ2 > 1, and we see from (10) that gradients will tend to vanish for large depths.
Furthermore, Figure 4 (c) plots χ(c∗) as a function of µ2. As µ2 increases from one, χ(c∗) decreases
from one. Therefore, from (11), we also ﬁnd that error signals from different inputs will tend to
decorrelate at large depths.

Interestingly, for non-constant width networks, stable gradient information propagation may still be
possible. If the network architecture adapts to the amount of noise being injected by having the widths
of the layers grow as Dl+1 = Dlµ2, then (10) should be at its ﬁxed point solution. For example, in
the case of dropout Dl+1 = Dl/p, which implies that for any p < 1, each successive layer in the
network needs to grow in width by a factor of 1/p to promote stable gradient ﬂow. Similarly, for
multiplicative Gaussian noise, Dl+1 = Dl(σ2
(cid:15) + 1), which requires the network to grow in width
unless σ2
(cid:15) = 0. Similarly, if Dl+2 = Dl+1χ(c∗) = Dlµ2χ(c∗) in (11), the covariance of the error
signal should be preserved during the backward pass, for arbitrary values of µ2 and χ(c∗).

7

Figure 6: Depth scale experiments on MNIST and CIFAR-10. (a) Variance propagation dynamics for
MNIST on and off the critical point initialisation (dashed black line) with dropout (p = 0.6). The
cyan curve represents the theoretical boundary at which numerical instability issues are predicted
to occur and is computed as L∗ = ln(K)/ln( σ2
2 µ2), where K is the largest (or smallest) positive
number representable by the computer. Speciﬁcally, we use 32-bit ﬂoating point numbers and set
K = 3.4028235 × 1038, if σ2
. (b) Depth scales ﬁt
to the training loss on MNIST for networks initialised at criticality for dropout rates p = 0.1 (severe
dropout) to p = 1 (no dropout). (c) Depth scales ﬁt to the validation loss on MNIST. (d) - (f): Similar
to (a) - (c), but for CIFAR-10. For each plot we highlight trends by smoothing the colour grid (for
non smoothed versions see Section C.5 in the supplementary material).

and K = 1.1754944 × 10−38, if σ2

w > 2
µ2

w < 2
µ2

w

4 Experimental results

From our analysis of deep noisy ReLU networks in the previous section, we expect that a necessary
condition for such a network to be trainable, is that the network be initialised at criticality. However,
whether the layer widths are varied or not for the sake of backpropagation, the correlation dynamics
in the forward pass may still limit the depth at which these networks perform well.

We therefore investigate the performance of noise-regularised deep ReLU networks on real-world
data. First, we validate the derived critical initialisation. As the depth of the network increases, any
initialisation strategy that does not factor in the effects of noise, will cause the forward propagating
signal to become increasingly unstable. For very deep networks, this might cause the signal to either
explode or vanish, even within the ﬁrst forward pass, making the network untrainable. To test this,
we sent inputs from MNIST and CIFAR-10 through ReLU networks using dropout (with p = 0.6) at
varying depths and for different initialisations of the network. Figure 6 (a) and (d) shows the evolution
of the input statistics as the input propagates through each network for the different data sets. For
initialisations not at criticality, the variance grows or shrinks rapidly to the point of causing numerical
overﬂow or underﬂow (indicated by black regions). For deep networks, this can happen well before
any signal is able to reach the output layer. In contrast, initialising at criticality (as shown by the
dashed black line), allows for the signal to propagate reliably even at very large depths. Furthermore,
given the ﬂoating point precision, if σ2
, we can predict the depth at which numerical overﬂow
(or underﬂow) will occur by solving for L∗ in K = (cid:0)σ2
q0, where K is the largest (or
smallest) positive number representable by the computer (see Section C.4 in supplementary material).
These predictions are shown by the cyan line and provide a good ﬁt to the empirical limiting depth
from numerical instability.

wµ2/2(cid:1)L∗

w (cid:54)= 2
µ2

We now turn to the issue of limited trainability. Due to the loss of correlation information between
inputs as a function of noise and network depth, we expect noisy ReLU networks not to be able to
perform well beyond certain depths. We investigated depth scales for ReLU networks with dropout
initialised at criticality: we trained 100 networks on MNIST and CIFAR-10 for 200 epochs using SGD
and a learning rate of 10−3 with dropout rates ranging from 0.1 to 1 for varying depths. The results

8

are shown in Figure 6 (see Section C.5 of the supplementary material for additional experimental
results). For each network conﬁguration and noise level, the critical initialisation σ2
was
used. We indeed observe a relationship between depth and noise on the loss of a network, even at
criticality. Interestingly, the line 6ξc (Schoenholz et al., 2017), seems to track the depth beyond
which the relative performance on the validation loss becomes poor, more so than on the training loss.
However, in both cases, we ﬁnd that even modest amounts of noise can limit performance.

w = 2
µ2

5 Discussion

By developing a general framework to study signal propagation in noisy neural networks, we were
able to show how different stochastic regularisation strategies may impact the ﬂow of information
in a deep network. Focusing speciﬁcally on ReLU networks, we derived novel critical initialisation
strategies for multiplicative noise distributions and showed that no such critical initialisations exist
for commonly used additive noise distributions. At criticality however, our theory predicts that the
statistics of the input should remain within a stable range during the forward pass and enable reliable
signal propagation for noise regularised deep ReLU networks. We veriﬁed these predictions by
comparing them with numerical simulations as well as experiments on MNIST and CIFAR-10 using
dropout and found good agreement.

Interestingly, we note that a dropout rate of p = 0.5 has often been found to work well for ReLU
networks (Srivastava et al., 2014). The critical initialisation corresponding to this rate is (σw, σb) =
√
2p, 0) = (1, 0). This is exactly the “Xavier” initialisation proposed by Glorot and Bengio (2010),
(
which prior to the development of the “He” initialisation, was often used in combination with
dropout (Simonyan and Zisserman, 2014). This could therefore help to explain the initial success
associated with this speciﬁc dropout rate. Similarly, Srivastava et al. (2014) reported that adding
multiplicative Gaussian noise where (cid:15) ∼ N (1, σ2
(cid:15) = 1, also seemed to perform well, for
= (1, 0), again corresponding to the “Xavier” method.
which the critical initialisation is

(cid:15) ), with σ2

(cid:17)
(cid:16)(cid:113) 2
(cid:15) +1 , 0
σ2

Although our initialisations ensure that individual input statistics are preserved, we further analysed
the correlation dynamics between inputs and found the following: at large depths inputs become
predictably correlated with each other based on the amount of noise injected into the network. As a
consequence, the representations for different inputs to a deep network may become indistinguishable
from each other in the later layers of the network. This can make training infeasible for noisy ReLU
networks of a certain depth and depends on the amount of noise regularisation being applied.

We now note the following shortcomings of our work: ﬁrstly, our ﬁndings only apply to fully
connected feed-forward neural networks and focus almost exclusively on the ReLU activation
function. Furthermore, we limit the scope of our architectural design to a recursive application of a
dense layer followed by a noise layer, whereas in practice a larger mix of layers is usually required to
solve a speciﬁc task.

Ultimately, we are interested in reducing the number of decisions that need to made when designing
deep neural networks and understanding the implications of those decisions on network behaviour
and performance. Any machine learning engineer exploring a neural network based solution to a
practical problem will be faced with a large number of possible design decisions. All these decisions
cost valuable time to explore. In this work, we hope to have at least provided some guidance in this
regard, speciﬁcally when choosing between different initialisation strategies for noise regularised
ReLU networks and understanding their associated implications.

Acknowledgements

We would like to thank the reviewers for their insightful comments which improved the quality of this
work. Furthermore, we would like to thank Google, the CSIR/SU Centre for Artiﬁcial Intelligence
Research (CAIR) as well as the Science Faculty and the Postgraduate and International Ofﬁce of
Stellenbosch University for ﬁnancial support. Finally, we gratefully acknowledge the support of
NVIDIA Corporation with the donation of a Titan Xp GPU used for this research.

9

References

X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep feedforward neural networks,”
in Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, 2010, pp.
249–256.

A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks,” Proceedings of the International Conference on Learning
Representations, 2014.

D. Sussillo and L. Abbott, “Random walk initialization for training very deep feedforward networks,”

arXiv preprint arXiv:1412.6558, 2014.

K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers: Surpassing human-level per-
formance on ImageNet classiﬁcation,” in Proceedings of the IEEE International Conference on
Computer Vision, 2015, pp. 1026–1034.

D. Mishkin and J. Matas, “All you need is a good init,” Proceedings of International Conference on

Learning Representations, 2016.

X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural networks,” in Proceedings of the

International Conference on Artiﬁcial Intelligence and Statistics, 2011, pp. 315–323.

N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple
way to prevent neural networks from overﬁtting.” Journal of Machine Learning Research, vol. 15,
no. 1, pp. 1929–1958, 2014.

A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation with deep convolutional
neural networks,” in Advances in Neural Information Processing Systems, 2012, pp. 1097–1105.

G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural networks for LVCSR using
rectiﬁed linear units and dropout,” in Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, 2013, pp. 8609–8613.

B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli, “Exponential expressivity in deep
neural networks through transient chaos,” in Advances in Neural Information Processing Systems,
2016, pp. 3360–3368.

S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein, “Deep information propagation,”

Proceedings of the International Conference on Learning Representations, 2017.

G. Yang and S. Schoenholz, “Mean ﬁeld residual networks: On the edge of chaos,” in Advances in

Neural Information Processing Systems, 2017, pp. 7103–7114.

L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington, “Dynamical isometry and
a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks,”
Proceedings of the International Conference on Machine Learning, 2018.

M. Chen, J. Pennington, and S. S. Schoenholz, “Dynamical isometry and a mean ﬁeld theory of RNNs:
Gating enables signal propagation in recurrent neural networks,” Proceedings of the International
Conference on Machine Learning, 2018.

S. Hayou, A. Doucet, and J. Rousseau, “On the selection of initialization and activation function for

deep neural networks,” arXiv preprint arXiv:1805.08266, 2018.

K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,”

arXiv preprint arXiv:1409.1556, 2014.

10

Supplementary Material

In this section, we provide additional details of derivations and experimental results presented in the
paper.

A Signal propagation in noise regularised neural networks

To review, given an input x0 ∈ RD0, we consider the following noisy random network model

˜hl = W l(xl−1 (cid:12) (cid:15)l−1) + bl, spa for l = 1, ..., L
(12)
where we inject noise into the model using the operator (cid:12) to denote either addition or multiplication.
The vector (cid:15)l is an input noise vector, sampled from a pre-speciﬁed noise distribution. For additive
noise, the distribution is assumed to be zero mean. Whereas for multiplicative noise distributions, the
mean is assumed to be equal to one. The weights W l ∈ RDl×Dl−1 and biases bl ∈ RDl are sampled
i.i.d. from zero mean Gaussian distributions with variances σ2
b , respectively, where
Dl denotes the dimensionality of the lth hidden layer in the network. The hidden layer activations
xl = φ(˜hl) are computed element-wise using an activation function φ(·), for layers l = 1, ..., L.

w/Dl−1 and σ2

A.1 Single input signal propagation

We consider the network’s behavior at initialisation. In this setting, the expected mean (over the
weights, biases and noise distribution) of a unit in the pre-activations ˜hl
j for a single signal passing
through the network will be zero with variance

˜ql = Ew,b,(cid:15)[(˜hl

j)2]

= Ew,(cid:15)[{wl,j · (xl−1

j (cid:12) (cid:15)l−1

j

)}2] + Eb[(bl

j)2]

= σ2
w

1
Dl−1

Dl−1
(cid:88)

j=1

(cid:104)
φ(˜hl−1
j

)2 (cid:12) E(cid:15)[((cid:15)l−1

)2]

j

+ σ2

b ,

(cid:105)

where we use wl,j to denote the j-th row of W l. The second last line relies on the bias distribution
being zero mean, while the ﬁnal step makes use of the independence between the inputs and the
noise in the multiplicative case, and the noise being zero mean in the additive case. Furthermore, to
ensure the expected value of the pre-activations remain unbiased, we only consider additive noise
distributions with zero mean and multiplicative noise distributions with a mean equal to one. As in
Poole et al. (2016), we make the self averaging assumption and consider the large layer width case
where the previous layer’s pre-activations are assumed to be Gaussian with zero mean and variance
˜ql−1. This gives the following noisy variance map

˜ql = σ2
w

(cid:26)

(cid:20)

φ

Ez

(cid:16)(cid:112)

˜ql−1z

(cid:17)2(cid:21)

(cid:27)

(cid:12) µl−1
2

+ σ2
b ,

(13)

where z ∼ N (0, 1) and µl
from at layer l. The initial input variance is given by q0 = 1
D0

2 = E(cid:15)[((cid:15)l)2] is the second moment of the noise distribution being sampled

x0 · x0.

A.2 Two input signal propagation

To study the behaviour of a pair of signals, x0,a and x0,b, passing through the network, we can
compute the covariance in expectation over the noise and the parameters as

ab = Ew,b,(cid:15)[˜hl,a
˜ql
j
(cid:104)(cid:16)
wl,j · (xl−1,a
= Ew,b,(cid:15)

˜hl,b
j ]

) + bl
j
(cid:17) (cid:16)
)

j

j

(cid:104)(cid:16)

(cid:104)(cid:16)

(cid:104)(cid:16)

= Ew,b,(cid:15)

wl,j · (xl−1,a

(cid:12) (cid:15)l−1,a
j

white + Ew,b,(cid:15)

wl,j · (xl−1,a

j

(cid:12) (cid:15)l−1,a
j

white + Ew,b,(cid:15)

white + Ew,b,(cid:15)

wl,j · (xl−1,b
j)2(cid:3) .

j

(cid:2)(bl

(cid:12) (cid:15)l−1,b
j

)

(cid:17)
)
(cid:17)

bl
j
(cid:105)

bl
j

(cid:12) (cid:15)l−1,a
j

(cid:17) (cid:16)

wl,j · (xl−1,b

j

(cid:17)(cid:105)

) + bl
j

(cid:12) (cid:15)l−1,b
j
(cid:17)(cid:105)

wl,j · (xl−1,b
(cid:105)

j

(cid:12) (cid:15)l−1,b
j

)

11

Since the noise is i.i.d and we have that Eb[bl

j] = 0, we ﬁnd that

(cid:104)(cid:16)

(cid:17) (cid:16)

ab = Ew
˜ql

wl,j · xl−1,a

j

wl,j · xl−1,b

j

(cid:17)(cid:105)

+ Eb

(cid:2)(bl

j)2(cid:3)

= σ2
w

1
Dl−1

Dl−1
(cid:88)

j=1

(cid:104)
φ

(cid:16)˜hl−1,a

(cid:17)

φ

j

(cid:16)˜hl−1,b

j

(cid:17)(cid:105)

+ σ2
b ,

which in the large width limit becomes

(cid:112)

where ˜u1 =
inputs at layer l given by

˜ql−1
aa z1 and ˜u2 =

Ez1 [Ez2 [φ(˜u1)φ(˜u2)]] + σ2
b

ab = σ2
˜ql
w
(cid:113)

˜ql−1
bb

(cid:104)

˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

, with the correlation between

(cid:113)

˜cl = ˜ql

ab/

aa ˜ql
˜ql

bb.

Here, zi ∼ N (0, 1) for i = 1, 2 and ql

aa is the variance of ˜hl,a
j .

B Signal propagation in noise regularised ReLU networks

In this section, we give additional details of theoretical results presented in the paper that were
speciﬁcally derived for noisy ReLU networks.

B.1 Variance of input signals

Let f (z) = e−z2/2

√

2π

, then the variance map in (13) using ReLU, i.e. φ(a) = max(0, a), becomes

˜ql = σ2
w

f (z)φ

˜ql−1z

dz

(cid:12) µ2 + σ2
b

(cid:16)(cid:112)

(cid:16)(cid:112)

(cid:21)

(cid:17)2

(cid:17)2

f (z)φ

˜ql−1z

dz +

f (z)φ

˜ql−1z

dz

(cid:12) µ2 + σ2
b

(cid:16)(cid:112)

(cid:17)2

(cid:21)

(cid:90) ∞

0

(cid:20)(cid:90) ∞

−∞

(cid:20)(cid:90) 0

−∞

(cid:20)
˜ql−1

(cid:20) ˜ql−1
2

= σ2
w

= σ2
w

= σ2
w

(cid:21)

f (z)z2dz

(cid:12) µ2 + σ2
b

(cid:90) ∞

0

(cid:21)

(cid:12) µ2

+ σ2
b .

B.2 Correlation between input signals

Assuming that the variance map in (18) is at its ﬁxed point ˜q∗, which exits only if σ2
correlation map in (16) for a noisy ReLU network is given by

w = 2
µ2

, the

˜cl =

2
µ2 ˜q∗

(cid:90) ∞

(cid:90) ∞

−∞

−∞

f (z1)f (z2)φ(˜u1)φ(˜u2)dz2dz1 + σ2
b

(19)

√

, ˜u1 =

˜q∗z1 and ˜u2 =

√

(cid:104)
˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

.

˜q∗

where φ(a) = max(a, 0), f (zi) = e−z2
Note that

√

i /2
2π

(14)

(15)

(16)

(17)

(18)

˜u1

˜u2

(cid:26)≥ 0, if z1 > 0
< 0, Otherwise
≥ 0, if z2 > −˜cl−1z1
< 0, Otherwise

√

(cid:40)

1−(˜cl−1)2

,

12

therefore (19) becomes

(cid:90) ∞

(cid:90) ∞

˜cl =

2
µ2 ˜q∗

0

√

−˜cl−1z1
1−(˜cl−1)2
(cid:90) ∞

(cid:90) ∞

0

√

−˜cl−1z1
1−(˜cl−1)2

(cid:90) ∞

(cid:90) ∞

√

−˜cl−1z1
1−(˜cl−1)2

=

=

w

2
µ2 ˜q∗ σ2
2˜cl−1
µ2

0

f (z1)f (z2)˜u1 ˜u2dz2dz1 + σ2
b

f (z1)f (z2)(cid:112)˜q∗z1

(cid:112)˜q∗

(cid:20)
˜cl−1z1 +

(cid:113)

(cid:21)

1 − (˜cl−1)2z2

dz2dz1 + σ2
b

f (z1)f (z2)z2

1dz2dz1

2(cid:112)1 − (˜cl−1)2
µ2

(cid:90) ∞

(cid:90) ∞

0

√

−˜cl−1z1
1−(˜cl−1)2

addsomewhitespacehere +

f (z1)f (z2)z1z2dz2dz1.

(20)

The ﬁrst term in (20) can then be written as




(cid:90) ∞

(cid:90) 0

2˜cl−1
µ2



0

√

−˜cl−1 z1
1−(˜cl−1)2

f (z1)f (z2)z2

1dz2dz1 +

f (z1)f (z2)z2

1dz2dz1

(21)

(cid:90) ∞

(cid:90) ∞

0

0






.

In (21), the ﬁrst term inside the braces is given by

(cid:90) ∞

(cid:90) 0

0

√

−˜cl−1z1
1−(˜cl−1)2

f (z1)f (z2)z2

1dz2dz1 =

f (z1)z2

1erf

(cid:90) ∞

0

1
2

1
2π

1
2π

=

=

(cid:34)
˜cl−1(cid:113)

(cid:20)
˜cl−1(cid:113)

(cid:33)

dz1

(cid:32)

˜cl−1z1
(cid:112)1 − (˜cl−1)
(cid:32)

1 − (˜cl−1)2 + tan−1

(cid:33)(cid:35)

˜cl−1
(cid:112)1 − (˜cl−1)2

1 − (˜cl−1)2 + sin−1 (cid:0)˜cl−1(cid:1)

(cid:21)

(22)

with erf(a) = 1
π

(cid:82) a
−a e−t2

dt. The second term inside the braces in (21) equals

Therfore, (21) becomes

(cid:90) ∞

(cid:90) ∞

0

0

f (z1)f (z2)z2

1dz2dz1 =

f (z1)z2

1dz1

(cid:90) ∞

0

1
2
1
4

=

.

(cid:113)

(˜cl−1)2
µ2π

1 − (˜cl−1)2 +

sin−1 (cid:0)˜cl−1(cid:1) +

˜cl−1
µ2π

˜cl−1
2µ2

Similarly, the second term in (20) can be split up as follows

2(cid:112)1 − (˜cl−1)2
µ2




(cid:90) ∞

(cid:90) 0



0

√

−˜cl−1z1
1−(˜cl−1)2

f (z1)f (z2)z1z2dz2dz1 +

f (z1)f (z2)z1z2dz2dz1

.

(cid:90) ∞

(cid:90) ∞

0

0

The ﬁrst term inside the braces of (25) is

(cid:90) ∞

(cid:90) 0

0

√

−˜cl−1z1
1−(˜cl−1 )2

f (z1)f (z2)z1z2dz2dz1 =

f (z1)z1

2(1−(˜cl−1)2) − 1

dz1

˜cl−1z2
1

(cid:35)

(cid:34)

−
e

(23)

(24)





(25)

(26)

0
(cid:26) 1 − (˜cl−1)2
√

2π

(cid:27)

−

1
√
2π

(cid:90) ∞

1
√
2π

=

1
√
2π
(˜cl−1)2
2π

= −

13

and the second term is

(cid:90) ∞

(cid:90) ∞

0

0

f (z1)f (z2)z1z2dz2dz1 =

f (z1)z1dz1

(cid:90) ∞

0

1
√
2π

1
2π

.

=

Putting these two terms together, (25) becomes

(cid:113)

−

(˜cl−1)2
µ2π

1 − (˜cl−1)2 +

1 − (˜cl−1)2.

(cid:113)

1
µ2π

Finally, summing all the terms in (24) and (28) gives (19) as

˜cl =

1
µ2

(cid:40)

˜cl−1sin−1 (cid:0)˜cl−1(cid:1) + (cid:112)1 − (˜cl−1)2
π

+

˜cl−1
2

(cid:41)

.

We note that for the noiseless case, (29) is identical to the result recently obtained by Hayou et al.
(2018), where the authors used a slightly different approach.

B.3 Depth scales for trainability

We recap the result in Schoenholz et al. (2017) and adapt the derivation for the speciﬁc case of a
noisy ReLU network. Let cl = c∗ + εl, such that as long as liml→∞cl = c∗ exist we have that ε → 0
as l → ∞. Then Schoenholz et al. (2017) derived the following asymptotic recurrence relation

εl+1 = εlχ(c∗) + O((εl)2),

where

√

1 = ˜u1 =

with ˜u∗
network where σ2

˜q∗z1 and ˜u∗

˜q∗

2 =
, we have that

w = 2
µ2

χ(c∗) = σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u∗

√

(cid:104)

˜c∗z1 + (cid:112)1 − (˜c∗)2z2

2)]] ,

1)φ(cid:48)(˜u∗
(cid:105)

. Now, speciﬁcally for a noisy ReLU

χ(c∗) =

f (z1)f (z2)φ(cid:48)(˜u∗

1)φ(cid:48)(˜u∗

2)dz2dz1

(cid:90) ∞

(cid:90) ∞

−∞
(cid:90) ∞

−∞
(cid:90) ∞

2
µ2
2
µ2

2
µ2

2
µ2
1
µ2π

=

=

=

=

0

(cid:90) ∞

0
(cid:34)

1
2π

f (z1)f (z2)dz2dz1

− c∗ z1√
1−(c∗ )2
(cid:34)

(cid:32)

f (z1)

erf

√

1
2
(cid:32)

tan−1

c∗
(cid:112)1 − (c∗)2
(cid:105)

c∗z1
2(cid:112)1 − (c∗)2
(cid:35)

(cid:33)

+

1
4

(cid:104)
sin−1 (c∗) +

π
2

(cid:33)

(cid:35)

+ 1

dz1

Note that χ(c∗) is a constant, thus for large l the solution to the recurrence relation in (30) is expected
to be exponential, i.e. εl ∼ e−l/ξc. Here ξc, is considered the depth scale, which controls how deep
discriminatory information about the inputs can propagate through the network. We can then solve
for ξc to ﬁnd

ξc = −1/ln(χ(c∗)) = −ln

(cid:20) sin−1 (c∗)
µ2π

+

1
2µ2

(cid:21)−1

.

14

(27)

(28)

(29)

(30)

(31)

(32)

(33)

B.4 Variance of error signals

Under the mean ﬁeld assumption, Schoenholz et al. (2017) approximates the error signal at layer l by
a zero mean Gaussian with variance

where ˜ql
noisy ReLU network we have that

i)2], with ˜δl

δ = E[(˜δl

i = φ(cid:48)(˜hl

˜δl+1
j W l+1

ji

. In our context, for a critically initialised

(cid:20)
φ(cid:48) (cid:16)(cid:112)

˜qlz

(cid:17)2(cid:21)

,

σ2
w

Ez

δ = ˜ql+1
˜ql
δ

Dl+1
Dl
i) (cid:80)Dl+1

j=1

δ = ˜ql+1
˜ql
δ

= ˜ql+1
δ

(cid:90) ∞

0

Dl+1
Dl
Dl+1
Dl

2
µ2
1
µ2

.

f (z)dz

B.5 Correlation between error signals

The covariance between error signals is approximated using

ab,δ = ˜ql+1
˜ql
ab,δ

σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u1)φ(cid:48)(˜u2)]] ,

Dl+1
Dl+2

where ˜u1 and ˜u2 are deﬁned as was done in the forward pass. Here, we simply use the result in (32)
for noisy ReLU networks to ﬁnd

ab,δ = ˜ql+1
˜ql
ab,δ

χ(c∗)

Dl+1
Dl+2
Dl+1

= ˜ql+1
ab,δ

(cid:2)sin−1 (c∗) + π
Dl+2µ2π

2

(cid:3)

.

(34)

(35)

(36)

(37)

(38)

(39)

In this section we provide additional details regarding our experiments in the paper. Code to reproduce
all the experiments is available at https://github.com/ElanVB/noisy_signal_prop.

C Experimental details

C.1

Input data

For all experiments the network input data properties that remain consistent (unless stated otherwise)
are as follows: each observation consists of 1000 features and each feature value is drawn i.i.d. from
a standard normal distribution.

C.2 Variance propagation dynamics

The experiments conducted to gather results for Figures 2 and 3 aim to empirically show the
relationship between the variances at arbitrary layers in a neural network.

Iterative map: For the results depicted in Figures 2 (a) and 3 (a), the experimental set up is as follows.
The data used as input to these experiments comprises of 30 sets of 30 observations. The input is
scaled such that the variance of observations within each set is the same and the variance across
each set is different and forms a range of qset ∈ [0, 15]. As such, our results are averaged over 30
observations and 50 samplings of initial weights to a single hidden-layer network.

Convergence dynamics: For the results depicted in Figures 2 (b) and 3 (b), the experimental set up is
as follows. The data used as input to these experiments comprises of a set of 50 observations scaled
such that each observation’s variance is four (q = 4). As such, our results are averaged over 50
observations and 50 samplings of initial weights to a 15 hidden-layer network.

C.3 Correlation propagation dynamics

The experiments conducted to gather results for Figure 4 and 5 aim to empirically show the relation-
ship between the correlations of observations at arbitrary layers in a neural network.

15

Iterative map: For the results depicted in Figure 4 (a), the experimental set up is as follows. The data
used as input to these experiments comprises of 50 sets of 50 observations. The ﬁrst observation in
each set is sampled from a standard normal distribution and subsequent observations are generated
such that the correlation between the ﬁrst element and the ith element form a range of corr0,i ∈ [0, 1].
As such, our results are averaged over 50 observations and 50 samplings of initial weights to a single
hidden-layer network.

Convergence dynamics: For the results depicted in Figure 4 (b), the experimental set up is as
follows. The data used as input to these experiments comprises of three sets of 50 equally correlated
observations. Each set has a different correlation value such that corrset ∈ {0, 0.5, 0.9}. As such, our
results are averaged over 50 observations and 50 samplings of initial weights to a 15 hidden-layer
network.

Conﬁrmation of exponential rate of convergence for correlations: This section discusses how the
results depicted in Figure 5 are acquired. These experiments support the assumption that the rate
of convergence for correlations is exponential when using noise regularisation with rectiﬁer neural
networks. The experimental set up for this section is very similar to that of the above convergence
dynamics experiment, the only difference being the statistics we calculate from the correlation values.
The aspect of this experiment that may seem the most unclear is the reason why we claim that the
negative inverse slope of a linear ﬁt to the log differences in correlation should match the theoretical
values for ξc. The derivation to justify this is as follows. If a good ﬁt of the form al + b can be found
in the logarithmic domain for the rate of convergence, it would strongly indicate that the convergence
rate is exponential. Following this, we set the problem up like so:

Let us now assume that ln (cid:0)|cl − c∗|(cid:1) can be linearly approximated:

Since we are concerned with deep neural networks, we can take the limit as l becomes arbitrarily
large and see that as l grows the effect of b decreases (liml→∞ |al| (cid:29) |b|). Thus, we continue like so:

Thus, we have come to the ﬁnding that if the correlation rate of convergence is exponential and we
work with deep neural networks, the negative inverse slope of a linear ﬁt to the log differences in
correlation should match the theoretical values for ξc. Figure 5 shows that the theory closely matches
this approximation.

C.4 Depth scales

This section handles the experiments conducted related to determining the maximum depth variance
information can stably propagate through a network and the depth at which these networks can be
trained, both depicted in Figure 6.

The MNIST and CIFAR-10 datasets were used and were pre-processed using standard techniques.
Throughout these experiments mini-batches of 128 observations were used.

Variance depth scales: The experiments depicted in Figures 6 (a) and (d) are interested in testing the
numerical stability of networks initialised using different σ2
w values while using 32-bit ﬂoating point

16

|cl − c∗| ≈ e−l/ξc

∴ ln (cid:0)|cl − c∗|(cid:1) ≈

−l
ξc

.

∴ ln (cid:0)|cl − c∗|(cid:1) ≈ al + b,

∴ al + b ≈

−l
ξc

,

∴ ξc ≈

−l
al + b

.

−l
al

lim
l→∞

ξc ≈ lim
l→∞
1
a

≈ −

.

numbers. To test the depth of stable variance propagation, a network with 1000 hidden layers is used.
The network used in this experiment makes use of dropout with p = 0.6, where p is the probability
of keeping a neuron’s value, thus the critical value for σ2
w is 1.2. As such, a linearly spaced range of
σ2
w ∈ [0.1, 2.5] is used to select 25 different values.

We use the following approach to predict the depth beyond which variances become numerically
unstable. At criticality for multiplicative noise (σw, σb) = ((cid:112)2/µ2, 0), however, for weights
initialised off this critical point (18) becomes

˜ql = ˜ql−1

(cid:20)
˜ql−2

=

(cid:19)

(cid:18) σ2
wµ2
2
(cid:18) σ2
wµ2
2
(cid:19)l

.

= ˜q0

(cid:18) σ2
wµ2
2

(cid:19)

(cid:19)(cid:21) (cid:18) σ2
wµ2
2

(40)

(41)

w > 2
µ2

If σ2
, we let ˜ql = K, where K is the largest positive number representable by the computer. In
our case, using 32-bit ﬂoating point precision, this number is equal to 3.4028235 × 1038. Otherwise,
we select K = 1.1754944 × 10−38, the smallest possible positive number. Furthermore,
if σ2
let L∗ represent the layer l in (40) at which the value K is reached, then we can scale our input data
such that ˜q0 = 1 and solve for L∗ to ﬁnd

w < 2
µ2

L∗ = ln(K)/ln

(cid:18) σ2
wµ2
2

(cid:19)

.

Therefore, we expect numerical instability issues to occur beyond a depth of L∗.

Trainable depth scales: The experiments depicted in Figures 6 (b), (c), (e) and (f) are concerned
with determining at what depth a critically initialised network with a speciﬁed dropout rate can train
effectively. To this end, 10 linearly spaced values for dropout on the range p ∈ [0.1, 1.0] and 10
linearly spaced network depths on the integer range l ∈ [2, 40] are tested.

The task presented to the network in this experiment is to learn the identity function within 200
epochs. As such, the network is set up as an auto-encoder and uses stochastic gradient decent with a
learning rate of 10−3. The input data is divided into a training and validation set, each containing
50000 and 10000 observations respectively.

C.5 Additional results

In this section we provide some additional experiments on the training dynamics of deep noisy ReLU
networks from different initialisations.

In Figure 7 we compare the standard “He” initialisation (blue) with the critical initialisation (green)
for a ReLU network with dropout regularisation (p = 0.8). By not initialising at criticality due to
dropout noise, the variance map for the “He” strategy no longer lies on the identity line in (a) and as
a result, the forward propagating signal can be seen to explode in (b). However, by compensating for
the amount of injected noise, the above derived critical initialisation for dropout preserves the signal
throughout the entire forward pass, with roughly constant variance dynamics.

Next, we provide some additional experiments on the trainability of deep ReLU networks with
dropout on real-world data sets.

From our analysis in the paper, we expect that as the depth of the network increases, any initialisation
strategy that does not factor in the effects of noise, will cause the forward propagating signal to
become increasingly unstable. For very deep networks, this might cause the signal to either explode
or vanish, even within the ﬁrst forward pass, making the network untrainable.

To test this, we trained a denoising autoencoder network with dropout noise (p = 0.6) on MNIST and
CIFAR-10 using squared reconstruction loss. We consider several network depths (L = 30, 100, 200),
learning rates (α = 0.1, 0.01, 0.001, 0.0001) and optimisation procedures (SGD and Adam), with
1000 neurons in each layer. The results for training on CIFAR-10 are shown in Figure 8 for both the
“He” intialisation (blue) and the critical dropout initialisation (green). (For MNIST, see Figure 9; the

17

Figure 7: Critical initialisation for ReLU networks with dropout. Lines correspond to theoretical
predictions and points to numerical simulations, for random ReLU networks with dropout (p = 0.8),
initialised according to the method proposed by He et al. (2015) (blue) and at criticality (green).
(a): Iterative variance map where the identity line is displayed as a dashed black line. (b): Variance
dynamics during forward signal propagation.

Figure 8: Comparing the “He” initialisation strategy to critical dropout initialisation for ReLU
networks using dropout (p = 0.6) on CIFAR-10. While networks initialised at criticality (green) are
able to train at large depths (L = 200) as seen in the bottom row, networks initialised with the “He”
strategy (blue) become untrainable irrespective of the chosen learning rate or optimisation procedure.
An “X” marks the point at which a network completely stopped training. Training losses and number
of network updates are shown in log-scale.

core trends and resulting conclusions regarding network trainability is the same for both data sets,
which we discuss below.)

As the depth increases, moving from the top to the bottom row in Figure 8, networks initialised at
the critical point for dropout seem to remain trainable even up to a depth of 200 layers (we see the
loss start to decrease over ﬁve epochs). In contrast, networks using the “He” initialisation become
increasingly more difﬁcult to train, with no training taking place at very large depths. These ﬁndings
make sense in terms of the variance dynamics analysed in the paper, however, these experimental
successes seem to run counter to our theoretical analysis of trainable depth scales (this contradiction
can also be seen in Figure 6). Understanding this discrepancy is of particular interest to us.

To verify that the lack of training in Figure 8 is due to poor signal propagation, we plot the empirical
variance of the pre-activations in Figure 10, for the ﬁrst forward pass of a 200 layer autoencoder

18

Figure 9: Comparing the “He” initialisation strategy to critical dropout initialisation for ReLU
networks using dropout (p = 0.6) on MNIST. While networks initialised at criticality (green) are
able to train at large depths (L = 200) as seen in the bottom row, networks initialised with the “He”
strategy (blue) become untrainable irrespective of the chosen learning rate or optimisation procedure.
An “X” marks the point at which a network completely stopped training. Training losses and number
of network updates are shown in log-scale.

Figure 10: Variance dynamics for signal propagation in the ﬁrst forward pass for a 200 layer
autoencoder network fed a batch of 500 training examples from CIFAR-10. (a) Exploding activation
variance (blue) reaching overﬂow levels (marked with a red “X”) for the “He” intialisation, with no
signal reaching the output layer (shown in log-scale). (b) Zoomed in display of the roughly constant
variance dynamics in (a) for the critical dropout initialisation.

network. For the “He” initialisation, the variance in (a) grows rapidly to the point of causing numerical
instability and overﬂow (indicated by the red dashed line), well before any signal is able to reach the
output layer. However as shown in (b), by initialising at criticality, signal is able to propagate reliably
even at large depths.

19

Figure 11: Depth scale experiments on MNIST and CIFAR-10. (a) Depth scales ﬁt to the training loss
on MNIST for networks initialised at criticality for dropout rates p = 0.1 (severe dropout) to p = 1
(no dropout). (b) Depth scales ﬁt to the validation loss on MNIST. (c) - (d): Similar to (a) - (c), but
for CIFAR-10.

20

8
1
0
2
 
v
o
N
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
3
9
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Critical initialisation for deep signal propagation in
noisy rectiﬁer neural networks

Arnu Pretorius∗
Computer Science Division
CAIR†
Stellenbosch University

Elan Van Biljon
Computer Science Division
Stellenbosch University

Steve Kroon
Computer Science Division
Stellenbosch University

Herman Kamper
Department of Electrical and Electronic Engineering
Stellenbosch University

Abstract

Stochastic regularisation is an important weapon in the arsenal of a deep learning
practitioner. However, despite recent theoretical advances, our understanding of
how noise inﬂuences signal propagation in deep neural networks remains limited.
By extending recent work based on mean ﬁeld theory, we develop a new framework
for signal propagation in stochastic regularised neural networks. Our noisy signal
propagation theory can incorporate several common noise distributions, including
additive and multiplicative Gaussian noise as well as dropout. We use this frame-
work to investigate initialisation strategies for noisy ReLU networks. We show that
no critical initialisation strategy exists using additive noise, with signal propagation
exploding regardless of the selected noise distribution. For multiplicative noise
(e.g. dropout), we identify alternative critical initialisation strategies that depend
on the second moment of the noise distribution. Simulations and experiments on
real-world data conﬁrm that our proposed initialisation is able to stably propagate
signals in deep networks, while using an initialisation disregarding noise fails to do
so. Furthermore, we analyse correlation dynamics between inputs. Stronger noise
regularisation is shown to reduce the depth to which discriminatory information
about the inputs to a noisy ReLU network is able to propagate, even when initialised
at criticality. We support our theoretical predictions for these trainable depths with
simulations, as well as with experiments on MNIST and CIFAR-10.‡

1

Introduction

Over the last few years, advances in network design strategies have made it easier to train large
networks and have helped to reduce overﬁtting. These advances include improved weight initialisation
strategies (Glorot and Bengio, 2010; Saxe et al., 2014; Sussillo and Abbott, 2014; He et al., 2015;
Mishkin and Matas, 2016), non-saturating activation functions (Glorot et al., 2011) and stochastic
regularisation techniques (Srivastava et al., 2014). Authors have noted, for instance, the critical
dependence of successful training on noise-based methods such as dropout (Krizhevsky et al., 2012;
Dahl et al., 2013).

∗Correspondence: arnupretorius@gmail.com
†CSIR/SU Centre for Artiﬁcial Intelligence Research.
‡Code to reproduce all the results is available at https://github.com/ElanVB/noisy_signal_prop

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

Figure 1: Noisy layer recursion. The input xl−1 from the previous layer gets corrupted by the sampled
noise (cid:15)l−1, either by vector addition or component-wise multiplication, producing the noisy inputs
˜xl−1. The lth layer’s corrupted pre-activations are then computed by multiplication with the layer
weight matrix W l, followed by a vector addition of the biases bl. Finally, the inputs to the next layer
are simply the activations of the current layer, i.e. xl = φ(˜hl).

In many cases, successful results arise only from effective combination of these advances. Despite
this interdependence, our theoretical understanding of how these mechanisms and their interactions
affect neural networks remains impoverished.

One approach to studying these effects is through the lens of deep neural signal propagation. By
modelling the empirical input variance dynamics at the point of random initialisation, Saxe et al.
(2014) were able to derive equations capable of describing how signal propagates in nonlinear fully
connected feed-forward neural networks. This “mean ﬁeld” theory was subsequently extended by
Poole et al. (2016) and Schoenholz et al. (2017), in particular, to analyse signal correlation dynamics.
These analyses highlighted the existence of a critical boundary at initialisation, referred to as the “edge
of chaos”. This boundary deﬁnes a transition between ordered (vanishing), and chaotic (exploding)
regimes for neural signal propagation. Subsequently, the mean ﬁeld approximation to random neural
networks has been employed to analyse other popular neural architectures (Yang and Schoenholz,
2017; Xiao et al., 2018; Chen et al., 2018).

This paper focuses on the effect of noise on signal propagation in deep neural networks. Firstly we
ask: How is signal propagation in deep neural networks affected by noise? To gain some insight into
this question, we extend the mean ﬁeld theory developed by Schoenholz et al. (2017) for the special
case of dropout noise, into a generalised framework capable of describing the signal propagation
behaviour of stochastically regularised neural networks for different noise distributions.

Secondly we ask: How much are current weight initialisation strategies affected by noise-induced
regularisation in terms of their ability to initialise at a critical point for stable signal propagation?
Using our derived theory, we investigate this question speciﬁcally for rectiﬁed linear unit (ReLU)
networks. In particular, we show that no such critical initialisation exists for arbitrary zero-mean
additive noise distributions. However, for multiplicative noise, such an initialisation is shown to be
possible, given that it takes into account the amount of noise being injected into the network. Using
these insights, we derive novel critical initialisation strategies for several different multiplicative
noise distributions.

Finally, we ask: Given that a network is initialised at criticality, in what way does noise inﬂuence
the network’s ability to propagate useful information about its inputs? By analysing the correlation
between inputs as a function of depth in random deep ReLU networks, we highlight the following:
even though the statistics of individual inputs are able to propagate arbitrarily deep at criticality,
discriminatory information about the inputs becomes lost at shallower depths as the noise in the
network is increased. This is because in the later layers of a random noisy network, the internal
representations from different inputs become uniformly correlated. Therefore, the application of
noise regularisation directly limits the trainable depth of critically initialised ReLU networks.

2 Noisy signal propagation

We begin by presenting mean ﬁeld equations for stochastically regularised fully connected feed-
forward neural networks, allowing us to study noisy signal propagation for a variety of noise
distributions. To understand how noise inﬂuences signal propagation in a random network given an
input x0 ∈ RD0 , we inject noise into the model

˜hl = W l(xl−1 (cid:12) (cid:15)l−1) + bl, spa for l = 1, ..., L

(1)

2

using the operator (cid:12) to denote either addition or multiplication where (cid:15)l is an input noise vector,
sampled from a pre-speciﬁed noise distribution. For additive noise, the distribution is assumed to be
zero mean, for multiplicative noise distributions, the mean is assumed to be equal to one. The weights
W l ∈ RDl×Dl−1 and biases bl ∈ RDl are sampled i.i.d. from zero mean Gaussian distributions
with variances σ2
b , respectively, where Dl denotes the dimensionality of the lth hidden
layer in the network. The hidden layer activations xl = φ(˜hl) are computed element-wise using
an activation function φ(·), for layers l = 1, ..., L. Figure 1 illustrates this recursive sequence of
operations.

w/Dl−1 and σ2

To describe forward signal propagation for the model in (1), we make use of the mean ﬁeld approxi-
mation as in Poole et al. (2016) and analyse the statistics of the internal representations of the network
in expectation over the parameters and the noise. Since the weights and biases are sampled from zero
mean Gaussian distributions with pre-speciﬁed variances, we can approximate the distribution of the
pre-activations at layer l, in the large width limit, by a zero mean Gaussian with variance

˜ql = σ2
w

(cid:26)

(cid:20)

φ

Ez

(cid:16)(cid:112)

˜ql−1z

(cid:17)2(cid:21)

(cid:27)

(cid:12) µl−1
2

+ σ2
b ,

2 = E(cid:15)[((cid:15)l)2] is the
where z ∼ N (0, 1) (see Section A.1 in the supplementary material). Here, µl
second moment of the noise distribution being sampled from at layer l. The initial input variance is
given by q0 = 1
x0 · x0. Furthermore, to study the behaviour of a pair of signals from two different
D0
inputs, x0,a and x0,b, passing through the network, we can compute the covariance at each layer as

Ez1 [Ez2 [φ(˜u1)φ(˜u2)]] + σ2
b

ab = σ2
˜ql
w
(cid:113)

(cid:104)

˜ql−1
bb
(cid:113)

where ˜u1 =

˜ql−1
aa z1 and ˜u2 =

(cid:112)

˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

, with the correlation between

inputs at layer l given by ˜cl = ˜ql
aa ˜ql
˜ql
supplementary material for more details).

ab/

bb. Here, ql

aa is the variance of ˜hl,a

j

(see Section A.2 in the

For the backward pass, we use the equations derived in Schoenholz et al. (2017) to describe error
signal propagation.1 In the context of mean ﬁeld theory, the expected magnitude of the gradient at
each layer can be shown to be proportional to the variance of the error, ˜δl
˜δl+1
j W l+1
.
This allows for the distribution of the error signal at layer l to be approximated by a zero mean
Gaussian with variance

i) (cid:80)Dl+1

i = φ(cid:48)(˜hl

j=1

ji

Similarly, for noise regularised networks, the covariance between error signals can be shown to be

δ = ˜ql+1
˜ql
δ

Dl+1
Dl

σ2
w

Ez

(cid:20)
φ(cid:48) (cid:16)(cid:112)

˜qlz

(cid:17)2(cid:21)

.

ab,δ = ˜ql+1
˜ql
ab,δ

σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u1)φ(cid:48)(˜u2)]] ,

Dl+1
Dl+2

(2)

(3)

(4)

(5)

where ˜u1 and ˜u2 are deﬁned as was done in the forward pass.

Equations (2)-(5) fully capture the relevant statistics that govern signal propagation for a random
network during both the forward and the backward pass. In the remainder of this paper, we consider,
as was done by Schoenholz et al. (2017), the following necessary condition for training: “for a
random network to be trained information about the inputs should be able to propagate forward
through the network, and information about the gradients should be able to propagate backwards
through the network.” The behaviour of the network at this stage depends on the choice of activation,
noise regulariser and initial parameters. In the following section, we will focus on networks that use
the Rectiﬁed Linear Unit (ReLU) as activation function. The chosen noise regulariser is considered a
design choice left to the practitioner. Therefore, whether a random noisy ReLU network satisﬁes the
above stated necessary condition for training largely depends on the starting parameter values of the
network, i.e. its initialisation.

1It is, however, important to note that the derivation relies on the assumption that the weights used in the

forward pass are sampled independently from those used during backpropagation.

3

Figure 2: Deep signal propagation with and without noise. (a): Iterative variance map. (b): Variance
dynamics during forward signal propagation. In (a) and (b), lines correspond to theoretical predictions
and points to numerical simulations (means over 50 runs with shaded one standard deviation bounds),
for noiseless tanh (yellow) and noiseless ReLU (purple) networks, as well as for noisy tanh (red)
and noisy ReLU (brown) networks regularised using additive noise from a standard Gaussian. Both
tanh networks use (σw, σb) = (1, 0), the “Xavier” initialisation (Glorot and Bengio, 2010), while the
2, 0) the “He” initialisation (He et al., 2015). In our experiments,
ReLU networks use (σw, σb) = (
we use network layers consisting of 1000 hidden units (see Section C in the supplementary material
for more details on all our simulated experiments).

√

3 Critical initialisation for noisy rectiﬁer networks

Unlike the tanh nonlinearity investigated in previous work (Poole et al., 2016; Schoenholz et al.,
2017), rectifying activation functions such as ReLU are unbounded. This means that the statistics of
signal propagation through the network is not guaranteed to naturally stabilise through saturating
activations, as shown in Figure 2.

A point on the identity line in Figure 2 (a) represents a ﬁxed point to the recursive variance map
in equation (2). At a ﬁxed point, signal will stably propagate through the remaining layers of the
network. For tanh networks, such a ﬁxed point always exists irrespective of the initialisation, or
the amount of noise injected into the network. For ReLU networks, this is not the case. Consider
the “He” initialisation (He et al., 2015) for ReLU, commonly used in practice. In (b), we plot the
variance dynamics for this initialisation in purple and observe stable behaviour. But what happens
when we inject noise into each network? In the case of tanh (shown in red), the added noise simply
shifts the ﬁxed point to a new stable value. However, for ReLU, the noise entirely destroys the ﬁxed
point for the “He” initialisation, making signal propagation unstable. This can be seen in (a), where
the variance map for noisy ReLU (shown in brown) moves off the identity line entirely, causing the
signal in (b) to explode.

Therefore, to investigate whether signal can stably propagate through a random noisy ReLU network,
we examine (2) more closely, which for ReLU becomes (see Section B.1 in supplementary material)

˜ql = σ2
w

(cid:20) ˜ql−1
2

(cid:21)

(cid:12) µ2

+ σ2
b .

(6)

For ease of exposition we assume equal noise levels at each layer, i.e. µl
2 = µ2, ∀l. A critical
initialisation for a noisy ReLU network occurs when the tuple (σw, σb, µ2) provides a ﬁxed point ˜q∗,
to the recurrence in (6). This at least ensures that the statistics of individual inputs to the network will
be preserved throughout the ﬁrst forward pass. The existence of such a solution depends on the type
2 ˜q∗ + µ2σ2
w + σ2
of noise that is injected into the network. In the case of additive noise, ˜q∗ = σ2
b ,
w
√
implying that the only critical point initialisation for non-zero ˜q∗ is given by (σw, σb, µ2) = (
2, 0, 0).
Therefore, critical initialisation is not possible using any amount of zero-mean additive noise,
regardless of the noise distribution. For multiplicative noise, ˜q∗ = σ2
b , so the solution
w
(σw, σb, µ2) =
provides a critical initialisation for noise distributions with mean
one and a non-zero second moment µ2. For example, in the case of multiplicative Gaussian noise,
µ2 = σ2
. For dropout noise,

(cid:15) + 1, yielding critical initialisation with (σw, σb) =

2 ˜q∗µ2 + σ2

(cid:16)(cid:113) 2
µ2

(cid:16)(cid:113) 2

(cid:17)
σ2+1 , 0

, 0, µ2

(cid:17)

1

1

4

Table 1: Critical point initialisation for noisy ReLU networks.

DISTRIBUTION

P((cid:15))

— ADDITIVE NOISE —

µ2

σ2
(cid:15)

2β2

CRITICAL INITIALISATION

(σw, σb, σ(cid:15)) = (
√

(σw, σb, β) = (

√

2, 0, 0)

2, 0, 0)

N (0, σ2
(cid:15) )

Lap(0, β)

— MULTIPLICATIVE NOISE —

N (1, σ2
(cid:15) )

(σ2

(cid:15) + 1)

(σw, σb, σ(cid:15)) =

Lap(1, β)

(2β2 + 1)

(σw, σb, β) =

(cid:17)

(cid:16)(cid:113) 2
(cid:15) +1 , 0, σ(cid:15)
σ2
(cid:17)

(cid:16)(cid:113) 2

2β2+1 , 0, β

P oi(1)

P((cid:15) = 1
P((cid:15) = 0) = 1 − p

p ) = p,

2

1
p

(σw, σb, λ) = (1, 0, 1)

(σw, σb, p) = (

2p, 0, p)

√

GAUSSIAN

LAPLACE

GAUSSIAN

LAPLACE

POISSON

DROPOUT

Figure 3: Critical initialisation for noisy ReLU networks. (a): Iterative variance map. (b): Vari-
ance dynamics during forward signal propagation. In (a) and (b), lines correspond to theoretical
predictions and points to numerical simulations. Dropout (p = 0.6) is shown in green for dif-
w = 2(0.6) = 2
(critical), σ2
ferent initialisations, σ2
(exploding sig-
µ2
(0.6)−1 < 2
w = (0.85)2
nal) and σ2
(vanishing signal). Similarly, multiplicative Gaussian noise
µ2
(σ(cid:15) = 0.25) is shown in red with σ2
w =
(exploding) and
w = (0.75)2 2
σ2
( vanishing). (c): Variance critical boundary for initialisation, separating numerical
µ2
overﬂow and underﬂow signal propagation regimes.

w = (1.25)2 2
µ2

(0.25)2+1 = 2
µ2

(0.6)−1 > 2
µ2

w = (1.15)2

(critical), σ2

2

2

2

√

µ2 = 1/p (with p the probability of retaining a neuron); thus, to initialise at criticality, we must
2p, 0). Table 1 summarises critical initialisations for some commonly used
set (σw, σb) = (
noise distributions. We also note that similar results can be derived for other rectifying activation
functions; for example, for multiplicative noise the critical initialisation for parametric ReLU (PReLU)
activations (with slope parameter α) is given by (σw, σb, µ2) =

(cid:16)(cid:113) 2

(cid:17)

.

µ2(α2+1) , 0, µ2

To see the effect of initialising on or off the critical point for ReLU networks, Figure 3 compares
the predicted versus simulated variance dynamics for different initialisation schemes. For schemes
not initialising at criticality, the variance map in (a) no longer lies on the identity line and as a result
the forward propagating signal in (b) either explodes, or vanishes. In contrast, the initialisations
derived above lie on the critical boundary between these two extremes, as shown in (c) as a function
of the noise. By compensating for the amount of injected noise, the signal corresponding to the
initialisation σ2
is preserved in (b) throughout the entire forward pass, with roughly constant
variance dynamics.

w = 2
µ2

5

Figure 4: Propagating correlation information in noisy ReLU networks. (a): Iterative correlation
map with ﬁxed points indicated by “X” marks on the identity line. (b): Correlation dynamics during
forward signal propagation. In (a) and (b), lines correspond to theoretical predictions and points to
numerical simulations. All simulated networks were initialised at criticality for each noise type and
level. (c): Slope at the ﬁxed point correlation as a function of the amount of noise injected into the
network.

Next, we investigate the correlation dynamics between inputs. Assuming that (6) is at its ﬁxed point
˜q∗, which exists only if σ2
, the correlation map for a noisy ReLU network is given by (see
Section B.2 in supplementary material)

w = 2
µ2

˜cl =

1
µ2

(cid:40)

˜cl−1sin−1 (cid:0)˜cl−1(cid:1) + (cid:112)1 − (˜cl−1)2
π

+

˜cl−1
2

(cid:41)

.

Figure 4 plots this theoretical correlation map against simulated dynamics for different noise types
and levels. For no noise, the ﬁxed point c∗ in (a) is situated at one (marked with an “X” on the blue
line). The slope of the blue line indicates a non-decreasing function of the input correlations. After a
certain depth, inputs end up perfectly correlated irrespective of their starting correlation, as shown in
(b). In other words, random deep ReLU networks lose discriminatory information about their inputs
as the depth of the network increases, even when initialised at criticality. When noise is added to the
network, inputs decorrelate and c∗ moves away from one. However, more importantly, correlation
information in the inputs become lost at shallower depths as the noise level increases, as can be seen
in (b).

How quickly a random network loses information about its inputs depends on the rate of convergence
to the ﬁxed point c∗. Using this observation, Schoenholz et al. (2017) derived so-called depth scales
ξc, by assuming |cl − c∗| ∼ e−l/ξc. These scales essentially control the feasible depth at which
networks can be considered trainable, since they may still allow useful correlation information to
propagate through the network. In our case, the depth scale for a noisy ReLU network under this
assumption can be shown to be (see Section B.3 in supplementary material)

where

ξc = −1/ln [χ(c∗)] ,

χ(c∗) =

1
µ2π

(cid:104)
sin−1 (c∗) +

(cid:105)

.

π
2

(7)

(8)

(9)

The exponential rate assumption underlying the derivation of (8) is supported in Figure 5, where
for different noise types and levels, we plot |cl − c∗| as a function of depth on a log-scale, with
corresponding linear ﬁts (see panels (a) and (c)). We then compare the theoretical depth scales from
(8) to actual depth scales obtained through simulation (panels (b) and (d)), as a function of noise
and observe a good ﬁt for non-zero noise levels.4 We thus ﬁnd that noise limits the depth at which
critically initialised ReLU networks are expected to perform well through training.

4We note Hayou et al. (2018) recently showed that the rate of convergence for noiseless ReLU networks is
not exponential, but polynomial instead. Interestingly, keeping with the exponential rate assumption, we indeed
ﬁnd that the discrepancy between our theoretical depth scales from (8) and our simulated depth scales, is largest
at very low noise levels. However, at more typical noise levels, such as a dropout rate of p = 0.5 for example,
the assumption seems to provide a close ﬁt, with good agreement between theory and simulation.

6

Figure 5: Noise dependent depth scales for training. (a): Linear ﬁts (dashed lines) to |cl − c∗| as a
function of depth on a log-scale (solid lines) for varying amounts of dropout (p = 0.1 to p = 0.9
by 0.1). (b): Theoretical depth scales (solid lines) versus empirically inferred scales (dashed lines)
per dropout rate. Scales are inferred noting that if |cl − c∗| ∼ e−l/ξc, then a linear ﬁt, al + b, in
the logarithmic domain gives ξc ≈ − 1
a , for large l. In other words, the negative inverse slope of a
linear ﬁt to the log differences in correlation should match the theoretical values for ξc. Therefore,
we compare ξc = −1/ln [χ(c∗)] to − 1
a for different levels of noise. (c) - (d): Similar to (a) and (b),
but for Gaussian noise (σ(cid:15) = 0.1 to σ(cid:15) = 1.9 by 0.15).

We next brieﬂy discuss error signal propagation during the backward pass for noise regularised ReLU
networks. When critically initialised, the error variance recurrence relation in (4) for these networks
is (see Section B.4 in supplementary material)

with the covariance between error signals in (5), given by (see Section B.5 in supplementary material)

δ = ˜ql+1
˜ql
δ

Dl+1
Dlµ2

,

ab,δ = ˜ql+1
˜ql
ab,δ

Dl+1
Dl+2

χ(c∗).

(10)

(11)

Note the explicit dependence on the width of the layers of the network in (10) and (11). We ﬁrst
consider constant width networks, where Dl+1 = Dl, for all l = 1, ..., L. For any amount of
multiplicative noise, µ2 > 1, and we see from (10) that gradients will tend to vanish for large depths.
Furthermore, Figure 4 (c) plots χ(c∗) as a function of µ2. As µ2 increases from one, χ(c∗) decreases
from one. Therefore, from (11), we also ﬁnd that error signals from different inputs will tend to
decorrelate at large depths.

Interestingly, for non-constant width networks, stable gradient information propagation may still be
possible. If the network architecture adapts to the amount of noise being injected by having the widths
of the layers grow as Dl+1 = Dlµ2, then (10) should be at its ﬁxed point solution. For example, in
the case of dropout Dl+1 = Dl/p, which implies that for any p < 1, each successive layer in the
network needs to grow in width by a factor of 1/p to promote stable gradient ﬂow. Similarly, for
multiplicative Gaussian noise, Dl+1 = Dl(σ2
(cid:15) + 1), which requires the network to grow in width
unless σ2
(cid:15) = 0. Similarly, if Dl+2 = Dl+1χ(c∗) = Dlµ2χ(c∗) in (11), the covariance of the error
signal should be preserved during the backward pass, for arbitrary values of µ2 and χ(c∗).

7

Figure 6: Depth scale experiments on MNIST and CIFAR-10. (a) Variance propagation dynamics for
MNIST on and off the critical point initialisation (dashed black line) with dropout (p = 0.6). The
cyan curve represents the theoretical boundary at which numerical instability issues are predicted
to occur and is computed as L∗ = ln(K)/ln( σ2
2 µ2), where K is the largest (or smallest) positive
number representable by the computer. Speciﬁcally, we use 32-bit ﬂoating point numbers and set
K = 3.4028235 × 1038, if σ2
. (b) Depth scales ﬁt
to the training loss on MNIST for networks initialised at criticality for dropout rates p = 0.1 (severe
dropout) to p = 1 (no dropout). (c) Depth scales ﬁt to the validation loss on MNIST. (d) - (f): Similar
to (a) - (c), but for CIFAR-10. For each plot we highlight trends by smoothing the colour grid (for
non smoothed versions see Section C.5 in the supplementary material).

and K = 1.1754944 × 10−38, if σ2

w > 2
µ2

w < 2
µ2

w

4 Experimental results

From our analysis of deep noisy ReLU networks in the previous section, we expect that a necessary
condition for such a network to be trainable, is that the network be initialised at criticality. However,
whether the layer widths are varied or not for the sake of backpropagation, the correlation dynamics
in the forward pass may still limit the depth at which these networks perform well.

We therefore investigate the performance of noise-regularised deep ReLU networks on real-world
data. First, we validate the derived critical initialisation. As the depth of the network increases, any
initialisation strategy that does not factor in the effects of noise, will cause the forward propagating
signal to become increasingly unstable. For very deep networks, this might cause the signal to either
explode or vanish, even within the ﬁrst forward pass, making the network untrainable. To test this,
we sent inputs from MNIST and CIFAR-10 through ReLU networks using dropout (with p = 0.6) at
varying depths and for different initialisations of the network. Figure 6 (a) and (d) shows the evolution
of the input statistics as the input propagates through each network for the different data sets. For
initialisations not at criticality, the variance grows or shrinks rapidly to the point of causing numerical
overﬂow or underﬂow (indicated by black regions). For deep networks, this can happen well before
any signal is able to reach the output layer. In contrast, initialising at criticality (as shown by the
dashed black line), allows for the signal to propagate reliably even at very large depths. Furthermore,
given the ﬂoating point precision, if σ2
, we can predict the depth at which numerical overﬂow
(or underﬂow) will occur by solving for L∗ in K = (cid:0)σ2
q0, where K is the largest (or
smallest) positive number representable by the computer (see Section C.4 in supplementary material).
These predictions are shown by the cyan line and provide a good ﬁt to the empirical limiting depth
from numerical instability.

wµ2/2(cid:1)L∗

w (cid:54)= 2
µ2

We now turn to the issue of limited trainability. Due to the loss of correlation information between
inputs as a function of noise and network depth, we expect noisy ReLU networks not to be able to
perform well beyond certain depths. We investigated depth scales for ReLU networks with dropout
initialised at criticality: we trained 100 networks on MNIST and CIFAR-10 for 200 epochs using SGD
and a learning rate of 10−3 with dropout rates ranging from 0.1 to 1 for varying depths. The results

8

are shown in Figure 6 (see Section C.5 of the supplementary material for additional experimental
results). For each network conﬁguration and noise level, the critical initialisation σ2
was
used. We indeed observe a relationship between depth and noise on the loss of a network, even at
criticality. Interestingly, the line 6ξc (Schoenholz et al., 2017), seems to track the depth beyond
which the relative performance on the validation loss becomes poor, more so than on the training loss.
However, in both cases, we ﬁnd that even modest amounts of noise can limit performance.

w = 2
µ2

5 Discussion

By developing a general framework to study signal propagation in noisy neural networks, we were
able to show how different stochastic regularisation strategies may impact the ﬂow of information
in a deep network. Focusing speciﬁcally on ReLU networks, we derived novel critical initialisation
strategies for multiplicative noise distributions and showed that no such critical initialisations exist
for commonly used additive noise distributions. At criticality however, our theory predicts that the
statistics of the input should remain within a stable range during the forward pass and enable reliable
signal propagation for noise regularised deep ReLU networks. We veriﬁed these predictions by
comparing them with numerical simulations as well as experiments on MNIST and CIFAR-10 using
dropout and found good agreement.

Interestingly, we note that a dropout rate of p = 0.5 has often been found to work well for ReLU
networks (Srivastava et al., 2014). The critical initialisation corresponding to this rate is (σw, σb) =
√
2p, 0) = (1, 0). This is exactly the “Xavier” initialisation proposed by Glorot and Bengio (2010),
(
which prior to the development of the “He” initialisation, was often used in combination with
dropout (Simonyan and Zisserman, 2014). This could therefore help to explain the initial success
associated with this speciﬁc dropout rate. Similarly, Srivastava et al. (2014) reported that adding
multiplicative Gaussian noise where (cid:15) ∼ N (1, σ2
(cid:15) = 1, also seemed to perform well, for
= (1, 0), again corresponding to the “Xavier” method.
which the critical initialisation is

(cid:15) ), with σ2

(cid:17)
(cid:16)(cid:113) 2
(cid:15) +1 , 0
σ2

Although our initialisations ensure that individual input statistics are preserved, we further analysed
the correlation dynamics between inputs and found the following: at large depths inputs become
predictably correlated with each other based on the amount of noise injected into the network. As a
consequence, the representations for different inputs to a deep network may become indistinguishable
from each other in the later layers of the network. This can make training infeasible for noisy ReLU
networks of a certain depth and depends on the amount of noise regularisation being applied.

We now note the following shortcomings of our work: ﬁrstly, our ﬁndings only apply to fully
connected feed-forward neural networks and focus almost exclusively on the ReLU activation
function. Furthermore, we limit the scope of our architectural design to a recursive application of a
dense layer followed by a noise layer, whereas in practice a larger mix of layers is usually required to
solve a speciﬁc task.

Ultimately, we are interested in reducing the number of decisions that need to made when designing
deep neural networks and understanding the implications of those decisions on network behaviour
and performance. Any machine learning engineer exploring a neural network based solution to a
practical problem will be faced with a large number of possible design decisions. All these decisions
cost valuable time to explore. In this work, we hope to have at least provided some guidance in this
regard, speciﬁcally when choosing between different initialisation strategies for noise regularised
ReLU networks and understanding their associated implications.

Acknowledgements

We would like to thank the reviewers for their insightful comments which improved the quality of this
work. Furthermore, we would like to thank Google, the CSIR/SU Centre for Artiﬁcial Intelligence
Research (CAIR) as well as the Science Faculty and the Postgraduate and International Ofﬁce of
Stellenbosch University for ﬁnancial support. Finally, we gratefully acknowledge the support of
NVIDIA Corporation with the donation of a Titan Xp GPU used for this research.

9

References

X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep feedforward neural networks,”
in Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, 2010, pp.
249–256.

A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks,” Proceedings of the International Conference on Learning
Representations, 2014.

D. Sussillo and L. Abbott, “Random walk initialization for training very deep feedforward networks,”

arXiv preprint arXiv:1412.6558, 2014.

K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers: Surpassing human-level per-
formance on ImageNet classiﬁcation,” in Proceedings of the IEEE International Conference on
Computer Vision, 2015, pp. 1026–1034.

D. Mishkin and J. Matas, “All you need is a good init,” Proceedings of International Conference on

Learning Representations, 2016.

X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural networks,” in Proceedings of the

International Conference on Artiﬁcial Intelligence and Statistics, 2011, pp. 315–323.

N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple
way to prevent neural networks from overﬁtting.” Journal of Machine Learning Research, vol. 15,
no. 1, pp. 1929–1958, 2014.

A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation with deep convolutional
neural networks,” in Advances in Neural Information Processing Systems, 2012, pp. 1097–1105.

G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural networks for LVCSR using
rectiﬁed linear units and dropout,” in Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, 2013, pp. 8609–8613.

B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli, “Exponential expressivity in deep
neural networks through transient chaos,” in Advances in Neural Information Processing Systems,
2016, pp. 3360–3368.

S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein, “Deep information propagation,”

Proceedings of the International Conference on Learning Representations, 2017.

G. Yang and S. Schoenholz, “Mean ﬁeld residual networks: On the edge of chaos,” in Advances in

Neural Information Processing Systems, 2017, pp. 7103–7114.

L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington, “Dynamical isometry and
a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks,”
Proceedings of the International Conference on Machine Learning, 2018.

M. Chen, J. Pennington, and S. S. Schoenholz, “Dynamical isometry and a mean ﬁeld theory of RNNs:
Gating enables signal propagation in recurrent neural networks,” Proceedings of the International
Conference on Machine Learning, 2018.

S. Hayou, A. Doucet, and J. Rousseau, “On the selection of initialization and activation function for

deep neural networks,” arXiv preprint arXiv:1805.08266, 2018.

K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,”

arXiv preprint arXiv:1409.1556, 2014.

10

Supplementary Material

In this section, we provide additional details of derivations and experimental results presented in the
paper.

A Signal propagation in noise regularised neural networks

To review, given an input x0 ∈ RD0, we consider the following noisy random network model

˜hl = W l(xl−1 (cid:12) (cid:15)l−1) + bl, spa for l = 1, ..., L
(12)
where we inject noise into the model using the operator (cid:12) to denote either addition or multiplication.
The vector (cid:15)l is an input noise vector, sampled from a pre-speciﬁed noise distribution. For additive
noise, the distribution is assumed to be zero mean. Whereas for multiplicative noise distributions, the
mean is assumed to be equal to one. The weights W l ∈ RDl×Dl−1 and biases bl ∈ RDl are sampled
i.i.d. from zero mean Gaussian distributions with variances σ2
b , respectively, where
Dl denotes the dimensionality of the lth hidden layer in the network. The hidden layer activations
xl = φ(˜hl) are computed element-wise using an activation function φ(·), for layers l = 1, ..., L.

w/Dl−1 and σ2

A.1 Single input signal propagation

We consider the network’s behavior at initialisation. In this setting, the expected mean (over the
weights, biases and noise distribution) of a unit in the pre-activations ˜hl
j for a single signal passing
through the network will be zero with variance

˜ql = Ew,b,(cid:15)[(˜hl

j)2]

= Ew,(cid:15)[{wl,j · (xl−1

j (cid:12) (cid:15)l−1

j

)}2] + Eb[(bl

j)2]

= σ2
w

1
Dl−1

Dl−1
(cid:88)

j=1

(cid:104)
φ(˜hl−1
j

)2 (cid:12) E(cid:15)[((cid:15)l−1

)2]

j

+ σ2

b ,

(cid:105)

where we use wl,j to denote the j-th row of W l. The second last line relies on the bias distribution
being zero mean, while the ﬁnal step makes use of the independence between the inputs and the
noise in the multiplicative case, and the noise being zero mean in the additive case. Furthermore, to
ensure the expected value of the pre-activations remain unbiased, we only consider additive noise
distributions with zero mean and multiplicative noise distributions with a mean equal to one. As in
Poole et al. (2016), we make the self averaging assumption and consider the large layer width case
where the previous layer’s pre-activations are assumed to be Gaussian with zero mean and variance
˜ql−1. This gives the following noisy variance map

˜ql = σ2
w

(cid:26)

(cid:20)

φ

Ez

(cid:16)(cid:112)

˜ql−1z

(cid:17)2(cid:21)

(cid:27)

(cid:12) µl−1
2

+ σ2
b ,

(13)

where z ∼ N (0, 1) and µl
from at layer l. The initial input variance is given by q0 = 1
D0

2 = E(cid:15)[((cid:15)l)2] is the second moment of the noise distribution being sampled

x0 · x0.

A.2 Two input signal propagation

To study the behaviour of a pair of signals, x0,a and x0,b, passing through the network, we can
compute the covariance in expectation over the noise and the parameters as

ab = Ew,b,(cid:15)[˜hl,a
˜ql
j
(cid:104)(cid:16)
wl,j · (xl−1,a
= Ew,b,(cid:15)

˜hl,b
j ]

) + bl
j
(cid:17) (cid:16)
)

j

j

(cid:104)(cid:16)

(cid:104)(cid:16)

(cid:104)(cid:16)

= Ew,b,(cid:15)

wl,j · (xl−1,a

(cid:12) (cid:15)l−1,a
j

white + Ew,b,(cid:15)

wl,j · (xl−1,a

j

(cid:12) (cid:15)l−1,a
j

white + Ew,b,(cid:15)

white + Ew,b,(cid:15)

wl,j · (xl−1,b
j)2(cid:3) .

j

(cid:2)(bl

(cid:12) (cid:15)l−1,b
j

)

(cid:17)
)
(cid:17)

bl
j
(cid:105)

bl
j

(cid:12) (cid:15)l−1,a
j

(cid:17) (cid:16)

wl,j · (xl−1,b

j

(cid:17)(cid:105)

) + bl
j

(cid:12) (cid:15)l−1,b
j
(cid:17)(cid:105)

wl,j · (xl−1,b
(cid:105)

j

(cid:12) (cid:15)l−1,b
j

)

11

Since the noise is i.i.d and we have that Eb[bl

j] = 0, we ﬁnd that

(cid:104)(cid:16)

(cid:17) (cid:16)

ab = Ew
˜ql

wl,j · xl−1,a

j

wl,j · xl−1,b

j

(cid:17)(cid:105)

+ Eb

(cid:2)(bl

j)2(cid:3)

= σ2
w

1
Dl−1

Dl−1
(cid:88)

j=1

(cid:104)
φ

(cid:16)˜hl−1,a

(cid:17)

φ

j

(cid:16)˜hl−1,b

j

(cid:17)(cid:105)

+ σ2
b ,

which in the large width limit becomes

(cid:112)

where ˜u1 =
inputs at layer l given by

˜ql−1
aa z1 and ˜u2 =

Ez1 [Ez2 [φ(˜u1)φ(˜u2)]] + σ2
b

ab = σ2
˜ql
w
(cid:113)

˜ql−1
bb

(cid:104)

˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

, with the correlation between

(cid:113)

˜cl = ˜ql

ab/

aa ˜ql
˜ql

bb.

Here, zi ∼ N (0, 1) for i = 1, 2 and ql

aa is the variance of ˜hl,a
j .

B Signal propagation in noise regularised ReLU networks

In this section, we give additional details of theoretical results presented in the paper that were
speciﬁcally derived for noisy ReLU networks.

B.1 Variance of input signals

Let f (z) = e−z2/2

√

2π

, then the variance map in (13) using ReLU, i.e. φ(a) = max(0, a), becomes

˜ql = σ2
w

f (z)φ

˜ql−1z

dz

(cid:12) µ2 + σ2
b

(cid:16)(cid:112)

(cid:16)(cid:112)

(cid:21)

(cid:17)2

(cid:17)2

f (z)φ

˜ql−1z

dz +

f (z)φ

˜ql−1z

dz

(cid:12) µ2 + σ2
b

(cid:16)(cid:112)

(cid:17)2

(cid:21)

(cid:90) ∞

0

(cid:20)(cid:90) ∞

−∞

(cid:20)(cid:90) 0

−∞

(cid:20)
˜ql−1

(cid:20) ˜ql−1
2

= σ2
w

= σ2
w

= σ2
w

(cid:21)

f (z)z2dz

(cid:12) µ2 + σ2
b

(cid:90) ∞

0

(cid:21)

(cid:12) µ2

+ σ2
b .

B.2 Correlation between input signals

Assuming that the variance map in (18) is at its ﬁxed point ˜q∗, which exits only if σ2
correlation map in (16) for a noisy ReLU network is given by

w = 2
µ2

, the

˜cl =

2
µ2 ˜q∗

(cid:90) ∞

(cid:90) ∞

−∞

−∞

f (z1)f (z2)φ(˜u1)φ(˜u2)dz2dz1 + σ2
b

(19)

√

, ˜u1 =

˜q∗z1 and ˜u2 =

√

(cid:104)
˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

.

˜q∗

where φ(a) = max(a, 0), f (zi) = e−z2
Note that

√

i /2
2π

(14)

(15)

(16)

(17)

(18)

˜u1

˜u2

(cid:26)≥ 0, if z1 > 0
< 0, Otherwise
≥ 0, if z2 > −˜cl−1z1
< 0, Otherwise

√

(cid:40)

1−(˜cl−1)2

,

12

therefore (19) becomes

(cid:90) ∞

(cid:90) ∞

˜cl =

2
µ2 ˜q∗

0

√

−˜cl−1z1
1−(˜cl−1)2
(cid:90) ∞

(cid:90) ∞

0

√

−˜cl−1z1
1−(˜cl−1)2

(cid:90) ∞

(cid:90) ∞

√

−˜cl−1z1
1−(˜cl−1)2

=

=

w

2
µ2 ˜q∗ σ2
2˜cl−1
µ2

0

f (z1)f (z2)˜u1 ˜u2dz2dz1 + σ2
b

f (z1)f (z2)(cid:112)˜q∗z1

(cid:112)˜q∗

(cid:20)
˜cl−1z1 +

(cid:113)

(cid:21)

1 − (˜cl−1)2z2

dz2dz1 + σ2
b

f (z1)f (z2)z2

1dz2dz1

2(cid:112)1 − (˜cl−1)2
µ2

(cid:90) ∞

(cid:90) ∞

0

√

−˜cl−1z1
1−(˜cl−1)2

addsomewhitespacehere +

f (z1)f (z2)z1z2dz2dz1.

(20)

The ﬁrst term in (20) can then be written as




(cid:90) ∞

(cid:90) 0

2˜cl−1
µ2



0

√

−˜cl−1 z1
1−(˜cl−1)2

f (z1)f (z2)z2

1dz2dz1 +

f (z1)f (z2)z2

1dz2dz1

(21)

(cid:90) ∞

(cid:90) ∞

0

0






.

In (21), the ﬁrst term inside the braces is given by

(cid:90) ∞

(cid:90) 0

0

√

−˜cl−1z1
1−(˜cl−1)2

f (z1)f (z2)z2

1dz2dz1 =

f (z1)z2

1erf

(cid:90) ∞

0

1
2

1
2π

1
2π

=

=

(cid:34)
˜cl−1(cid:113)

(cid:20)
˜cl−1(cid:113)

(cid:33)

dz1

(cid:32)

˜cl−1z1
(cid:112)1 − (˜cl−1)
(cid:32)

1 − (˜cl−1)2 + tan−1

(cid:33)(cid:35)

˜cl−1
(cid:112)1 − (˜cl−1)2

1 − (˜cl−1)2 + sin−1 (cid:0)˜cl−1(cid:1)

(cid:21)

(22)

with erf(a) = 1
π

(cid:82) a
−a e−t2

dt. The second term inside the braces in (21) equals

Therfore, (21) becomes

(cid:90) ∞

(cid:90) ∞

0

0

f (z1)f (z2)z2

1dz2dz1 =

f (z1)z2

1dz1

(cid:90) ∞

0

1
2
1
4

=

.

(cid:113)

(˜cl−1)2
µ2π

1 − (˜cl−1)2 +

sin−1 (cid:0)˜cl−1(cid:1) +

˜cl−1
µ2π

˜cl−1
2µ2

Similarly, the second term in (20) can be split up as follows

2(cid:112)1 − (˜cl−1)2
µ2




(cid:90) ∞

(cid:90) 0



0

√

−˜cl−1z1
1−(˜cl−1)2

f (z1)f (z2)z1z2dz2dz1 +

f (z1)f (z2)z1z2dz2dz1

.

(cid:90) ∞

(cid:90) ∞

0

0

The ﬁrst term inside the braces of (25) is

(cid:90) ∞

(cid:90) 0

0

√

−˜cl−1z1
1−(˜cl−1 )2

f (z1)f (z2)z1z2dz2dz1 =

f (z1)z1

2(1−(˜cl−1)2) − 1

dz1

˜cl−1z2
1

(cid:35)

(cid:34)

−
e

(23)

(24)





(25)

(26)

0
(cid:26) 1 − (˜cl−1)2
√

2π

(cid:27)

−

1
√
2π

(cid:90) ∞

1
√
2π

=

1
√
2π
(˜cl−1)2
2π

= −

13

and the second term is

(cid:90) ∞

(cid:90) ∞

0

0

f (z1)f (z2)z1z2dz2dz1 =

f (z1)z1dz1

(cid:90) ∞

0

1
√
2π

1
2π

.

=

Putting these two terms together, (25) becomes

(cid:113)

−

(˜cl−1)2
µ2π

1 − (˜cl−1)2 +

1 − (˜cl−1)2.

(cid:113)

1
µ2π

Finally, summing all the terms in (24) and (28) gives (19) as

˜cl =

1
µ2

(cid:40)

˜cl−1sin−1 (cid:0)˜cl−1(cid:1) + (cid:112)1 − (˜cl−1)2
π

+

˜cl−1
2

(cid:41)

.

We note that for the noiseless case, (29) is identical to the result recently obtained by Hayou et al.
(2018), where the authors used a slightly different approach.

B.3 Depth scales for trainability

We recap the result in Schoenholz et al. (2017) and adapt the derivation for the speciﬁc case of a
noisy ReLU network. Let cl = c∗ + εl, such that as long as liml→∞cl = c∗ exist we have that ε → 0
as l → ∞. Then Schoenholz et al. (2017) derived the following asymptotic recurrence relation

εl+1 = εlχ(c∗) + O((εl)2),

where

√

1 = ˜u1 =

with ˜u∗
network where σ2

˜q∗z1 and ˜u∗

˜q∗

2 =
, we have that

w = 2
µ2

χ(c∗) = σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u∗

√

(cid:104)

˜c∗z1 + (cid:112)1 − (˜c∗)2z2

2)]] ,

1)φ(cid:48)(˜u∗
(cid:105)

. Now, speciﬁcally for a noisy ReLU

χ(c∗) =

f (z1)f (z2)φ(cid:48)(˜u∗

1)φ(cid:48)(˜u∗

2)dz2dz1

(cid:90) ∞

(cid:90) ∞

−∞
(cid:90) ∞

−∞
(cid:90) ∞

2
µ2
2
µ2

2
µ2

2
µ2
1
µ2π

=

=

=

=

0

(cid:90) ∞

0
(cid:34)

1
2π

f (z1)f (z2)dz2dz1

− c∗ z1√
1−(c∗ )2
(cid:34)

(cid:32)

f (z1)

erf

√

1
2
(cid:32)

tan−1

c∗
(cid:112)1 − (c∗)2
(cid:105)

c∗z1
2(cid:112)1 − (c∗)2
(cid:35)

(cid:33)

+

1
4

(cid:104)
sin−1 (c∗) +

π
2

(cid:33)

(cid:35)

+ 1

dz1

Note that χ(c∗) is a constant, thus for large l the solution to the recurrence relation in (30) is expected
to be exponential, i.e. εl ∼ e−l/ξc. Here ξc, is considered the depth scale, which controls how deep
discriminatory information about the inputs can propagate through the network. We can then solve
for ξc to ﬁnd

ξc = −1/ln(χ(c∗)) = −ln

(cid:20) sin−1 (c∗)
µ2π

+

1
2µ2

(cid:21)−1

.

14

(27)

(28)

(29)

(30)

(31)

(32)

(33)

B.4 Variance of error signals

Under the mean ﬁeld assumption, Schoenholz et al. (2017) approximates the error signal at layer l by
a zero mean Gaussian with variance

where ˜ql
noisy ReLU network we have that

i)2], with ˜δl

δ = E[(˜δl

i = φ(cid:48)(˜hl

˜δl+1
j W l+1

ji

. In our context, for a critically initialised

(cid:20)
φ(cid:48) (cid:16)(cid:112)

˜qlz

(cid:17)2(cid:21)

,

σ2
w

Ez

δ = ˜ql+1
˜ql
δ

Dl+1
Dl
i) (cid:80)Dl+1

j=1

δ = ˜ql+1
˜ql
δ

= ˜ql+1
δ

(cid:90) ∞

0

Dl+1
Dl
Dl+1
Dl

2
µ2
1
µ2

.

f (z)dz

B.5 Correlation between error signals

The covariance between error signals is approximated using

ab,δ = ˜ql+1
˜ql
ab,δ

σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u1)φ(cid:48)(˜u2)]] ,

Dl+1
Dl+2

where ˜u1 and ˜u2 are deﬁned as was done in the forward pass. Here, we simply use the result in (32)
for noisy ReLU networks to ﬁnd

ab,δ = ˜ql+1
˜ql
ab,δ

χ(c∗)

Dl+1
Dl+2
Dl+1

= ˜ql+1
ab,δ

(cid:2)sin−1 (c∗) + π
Dl+2µ2π

2

(cid:3)

.

(34)

(35)

(36)

(37)

(38)

(39)

In this section we provide additional details regarding our experiments in the paper. Code to reproduce
all the experiments is available at https://github.com/ElanVB/noisy_signal_prop.

C Experimental details

C.1

Input data

For all experiments the network input data properties that remain consistent (unless stated otherwise)
are as follows: each observation consists of 1000 features and each feature value is drawn i.i.d. from
a standard normal distribution.

C.2 Variance propagation dynamics

The experiments conducted to gather results for Figures 2 and 3 aim to empirically show the
relationship between the variances at arbitrary layers in a neural network.

Iterative map: For the results depicted in Figures 2 (a) and 3 (a), the experimental set up is as follows.
The data used as input to these experiments comprises of 30 sets of 30 observations. The input is
scaled such that the variance of observations within each set is the same and the variance across
each set is different and forms a range of qset ∈ [0, 15]. As such, our results are averaged over 30
observations and 50 samplings of initial weights to a single hidden-layer network.

Convergence dynamics: For the results depicted in Figures 2 (b) and 3 (b), the experimental set up is
as follows. The data used as input to these experiments comprises of a set of 50 observations scaled
such that each observation’s variance is four (q = 4). As such, our results are averaged over 50
observations and 50 samplings of initial weights to a 15 hidden-layer network.

C.3 Correlation propagation dynamics

The experiments conducted to gather results for Figure 4 and 5 aim to empirically show the relation-
ship between the correlations of observations at arbitrary layers in a neural network.

15

Iterative map: For the results depicted in Figure 4 (a), the experimental set up is as follows. The data
used as input to these experiments comprises of 50 sets of 50 observations. The ﬁrst observation in
each set is sampled from a standard normal distribution and subsequent observations are generated
such that the correlation between the ﬁrst element and the ith element form a range of corr0,i ∈ [0, 1].
As such, our results are averaged over 50 observations and 50 samplings of initial weights to a single
hidden-layer network.

Convergence dynamics: For the results depicted in Figure 4 (b), the experimental set up is as
follows. The data used as input to these experiments comprises of three sets of 50 equally correlated
observations. Each set has a different correlation value such that corrset ∈ {0, 0.5, 0.9}. As such, our
results are averaged over 50 observations and 50 samplings of initial weights to a 15 hidden-layer
network.

Conﬁrmation of exponential rate of convergence for correlations: This section discusses how the
results depicted in Figure 5 are acquired. These experiments support the assumption that the rate
of convergence for correlations is exponential when using noise regularisation with rectiﬁer neural
networks. The experimental set up for this section is very similar to that of the above convergence
dynamics experiment, the only difference being the statistics we calculate from the correlation values.
The aspect of this experiment that may seem the most unclear is the reason why we claim that the
negative inverse slope of a linear ﬁt to the log differences in correlation should match the theoretical
values for ξc. The derivation to justify this is as follows. If a good ﬁt of the form al + b can be found
in the logarithmic domain for the rate of convergence, it would strongly indicate that the convergence
rate is exponential. Following this, we set the problem up like so:

Let us now assume that ln (cid:0)|cl − c∗|(cid:1) can be linearly approximated:

Since we are concerned with deep neural networks, we can take the limit as l becomes arbitrarily
large and see that as l grows the effect of b decreases (liml→∞ |al| (cid:29) |b|). Thus, we continue like so:

Thus, we have come to the ﬁnding that if the correlation rate of convergence is exponential and we
work with deep neural networks, the negative inverse slope of a linear ﬁt to the log differences in
correlation should match the theoretical values for ξc. Figure 5 shows that the theory closely matches
this approximation.

C.4 Depth scales

This section handles the experiments conducted related to determining the maximum depth variance
information can stably propagate through a network and the depth at which these networks can be
trained, both depicted in Figure 6.

The MNIST and CIFAR-10 datasets were used and were pre-processed using standard techniques.
Throughout these experiments mini-batches of 128 observations were used.

Variance depth scales: The experiments depicted in Figures 6 (a) and (d) are interested in testing the
numerical stability of networks initialised using different σ2
w values while using 32-bit ﬂoating point

16

|cl − c∗| ≈ e−l/ξc

∴ ln (cid:0)|cl − c∗|(cid:1) ≈

−l
ξc

.

∴ ln (cid:0)|cl − c∗|(cid:1) ≈ al + b,

∴ al + b ≈

−l
ξc

,

∴ ξc ≈

−l
al + b

.

−l
al

lim
l→∞

ξc ≈ lim
l→∞
1
a

≈ −

.

numbers. To test the depth of stable variance propagation, a network with 1000 hidden layers is used.
The network used in this experiment makes use of dropout with p = 0.6, where p is the probability
of keeping a neuron’s value, thus the critical value for σ2
w is 1.2. As such, a linearly spaced range of
σ2
w ∈ [0.1, 2.5] is used to select 25 different values.

We use the following approach to predict the depth beyond which variances become numerically
unstable. At criticality for multiplicative noise (σw, σb) = ((cid:112)2/µ2, 0), however, for weights
initialised off this critical point (18) becomes

˜ql = ˜ql−1

(cid:20)
˜ql−2

=

(cid:19)

(cid:18) σ2
wµ2
2
(cid:18) σ2
wµ2
2
(cid:19)l

.

= ˜q0

(cid:18) σ2
wµ2
2

(cid:19)

(cid:19)(cid:21) (cid:18) σ2
wµ2
2

(40)

(41)

w > 2
µ2

If σ2
, we let ˜ql = K, where K is the largest positive number representable by the computer. In
our case, using 32-bit ﬂoating point precision, this number is equal to 3.4028235 × 1038. Otherwise,
we select K = 1.1754944 × 10−38, the smallest possible positive number. Furthermore,
if σ2
let L∗ represent the layer l in (40) at which the value K is reached, then we can scale our input data
such that ˜q0 = 1 and solve for L∗ to ﬁnd

w < 2
µ2

L∗ = ln(K)/ln

(cid:18) σ2
wµ2
2

(cid:19)

.

Therefore, we expect numerical instability issues to occur beyond a depth of L∗.

Trainable depth scales: The experiments depicted in Figures 6 (b), (c), (e) and (f) are concerned
with determining at what depth a critically initialised network with a speciﬁed dropout rate can train
effectively. To this end, 10 linearly spaced values for dropout on the range p ∈ [0.1, 1.0] and 10
linearly spaced network depths on the integer range l ∈ [2, 40] are tested.

The task presented to the network in this experiment is to learn the identity function within 200
epochs. As such, the network is set up as an auto-encoder and uses stochastic gradient decent with a
learning rate of 10−3. The input data is divided into a training and validation set, each containing
50000 and 10000 observations respectively.

C.5 Additional results

In this section we provide some additional experiments on the training dynamics of deep noisy ReLU
networks from different initialisations.

In Figure 7 we compare the standard “He” initialisation (blue) with the critical initialisation (green)
for a ReLU network with dropout regularisation (p = 0.8). By not initialising at criticality due to
dropout noise, the variance map for the “He” strategy no longer lies on the identity line in (a) and as
a result, the forward propagating signal can be seen to explode in (b). However, by compensating for
the amount of injected noise, the above derived critical initialisation for dropout preserves the signal
throughout the entire forward pass, with roughly constant variance dynamics.

Next, we provide some additional experiments on the trainability of deep ReLU networks with
dropout on real-world data sets.

From our analysis in the paper, we expect that as the depth of the network increases, any initialisation
strategy that does not factor in the effects of noise, will cause the forward propagating signal to
become increasingly unstable. For very deep networks, this might cause the signal to either explode
or vanish, even within the ﬁrst forward pass, making the network untrainable.

To test this, we trained a denoising autoencoder network with dropout noise (p = 0.6) on MNIST and
CIFAR-10 using squared reconstruction loss. We consider several network depths (L = 30, 100, 200),
learning rates (α = 0.1, 0.01, 0.001, 0.0001) and optimisation procedures (SGD and Adam), with
1000 neurons in each layer. The results for training on CIFAR-10 are shown in Figure 8 for both the
“He” intialisation (blue) and the critical dropout initialisation (green). (For MNIST, see Figure 9; the

17

Figure 7: Critical initialisation for ReLU networks with dropout. Lines correspond to theoretical
predictions and points to numerical simulations, for random ReLU networks with dropout (p = 0.8),
initialised according to the method proposed by He et al. (2015) (blue) and at criticality (green).
(a): Iterative variance map where the identity line is displayed as a dashed black line. (b): Variance
dynamics during forward signal propagation.

Figure 8: Comparing the “He” initialisation strategy to critical dropout initialisation for ReLU
networks using dropout (p = 0.6) on CIFAR-10. While networks initialised at criticality (green) are
able to train at large depths (L = 200) as seen in the bottom row, networks initialised with the “He”
strategy (blue) become untrainable irrespective of the chosen learning rate or optimisation procedure.
An “X” marks the point at which a network completely stopped training. Training losses and number
of network updates are shown in log-scale.

core trends and resulting conclusions regarding network trainability is the same for both data sets,
which we discuss below.)

As the depth increases, moving from the top to the bottom row in Figure 8, networks initialised at
the critical point for dropout seem to remain trainable even up to a depth of 200 layers (we see the
loss start to decrease over ﬁve epochs). In contrast, networks using the “He” initialisation become
increasingly more difﬁcult to train, with no training taking place at very large depths. These ﬁndings
make sense in terms of the variance dynamics analysed in the paper, however, these experimental
successes seem to run counter to our theoretical analysis of trainable depth scales (this contradiction
can also be seen in Figure 6). Understanding this discrepancy is of particular interest to us.

To verify that the lack of training in Figure 8 is due to poor signal propagation, we plot the empirical
variance of the pre-activations in Figure 10, for the ﬁrst forward pass of a 200 layer autoencoder

18

Figure 9: Comparing the “He” initialisation strategy to critical dropout initialisation for ReLU
networks using dropout (p = 0.6) on MNIST. While networks initialised at criticality (green) are
able to train at large depths (L = 200) as seen in the bottom row, networks initialised with the “He”
strategy (blue) become untrainable irrespective of the chosen learning rate or optimisation procedure.
An “X” marks the point at which a network completely stopped training. Training losses and number
of network updates are shown in log-scale.

Figure 10: Variance dynamics for signal propagation in the ﬁrst forward pass for a 200 layer
autoencoder network fed a batch of 500 training examples from CIFAR-10. (a) Exploding activation
variance (blue) reaching overﬂow levels (marked with a red “X”) for the “He” intialisation, with no
signal reaching the output layer (shown in log-scale). (b) Zoomed in display of the roughly constant
variance dynamics in (a) for the critical dropout initialisation.

network. For the “He” initialisation, the variance in (a) grows rapidly to the point of causing numerical
instability and overﬂow (indicated by the red dashed line), well before any signal is able to reach the
output layer. However as shown in (b), by initialising at criticality, signal is able to propagate reliably
even at large depths.

19

Figure 11: Depth scale experiments on MNIST and CIFAR-10. (a) Depth scales ﬁt to the training loss
on MNIST for networks initialised at criticality for dropout rates p = 0.1 (severe dropout) to p = 1
(no dropout). (b) Depth scales ﬁt to the validation loss on MNIST. (c) - (d): Similar to (a) - (c), but
for CIFAR-10.

20

8
1
0
2
 
v
o
N
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
3
9
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Critical initialisation for deep signal propagation in
noisy rectiﬁer neural networks

Arnu Pretorius∗
Computer Science Division
CAIR†
Stellenbosch University

Elan Van Biljon
Computer Science Division
Stellenbosch University

Steve Kroon
Computer Science Division
Stellenbosch University

Herman Kamper
Department of Electrical and Electronic Engineering
Stellenbosch University

Abstract

Stochastic regularisation is an important weapon in the arsenal of a deep learning
practitioner. However, despite recent theoretical advances, our understanding of
how noise inﬂuences signal propagation in deep neural networks remains limited.
By extending recent work based on mean ﬁeld theory, we develop a new framework
for signal propagation in stochastic regularised neural networks. Our noisy signal
propagation theory can incorporate several common noise distributions, including
additive and multiplicative Gaussian noise as well as dropout. We use this frame-
work to investigate initialisation strategies for noisy ReLU networks. We show that
no critical initialisation strategy exists using additive noise, with signal propagation
exploding regardless of the selected noise distribution. For multiplicative noise
(e.g. dropout), we identify alternative critical initialisation strategies that depend
on the second moment of the noise distribution. Simulations and experiments on
real-world data conﬁrm that our proposed initialisation is able to stably propagate
signals in deep networks, while using an initialisation disregarding noise fails to do
so. Furthermore, we analyse correlation dynamics between inputs. Stronger noise
regularisation is shown to reduce the depth to which discriminatory information
about the inputs to a noisy ReLU network is able to propagate, even when initialised
at criticality. We support our theoretical predictions for these trainable depths with
simulations, as well as with experiments on MNIST and CIFAR-10.‡

1

Introduction

Over the last few years, advances in network design strategies have made it easier to train large
networks and have helped to reduce overﬁtting. These advances include improved weight initialisation
strategies (Glorot and Bengio, 2010; Saxe et al., 2014; Sussillo and Abbott, 2014; He et al., 2015;
Mishkin and Matas, 2016), non-saturating activation functions (Glorot et al., 2011) and stochastic
regularisation techniques (Srivastava et al., 2014). Authors have noted, for instance, the critical
dependence of successful training on noise-based methods such as dropout (Krizhevsky et al., 2012;
Dahl et al., 2013).

∗Correspondence: arnupretorius@gmail.com
†CSIR/SU Centre for Artiﬁcial Intelligence Research.
‡Code to reproduce all the results is available at https://github.com/ElanVB/noisy_signal_prop

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

Figure 1: Noisy layer recursion. The input xl−1 from the previous layer gets corrupted by the sampled
noise (cid:15)l−1, either by vector addition or component-wise multiplication, producing the noisy inputs
˜xl−1. The lth layer’s corrupted pre-activations are then computed by multiplication with the layer
weight matrix W l, followed by a vector addition of the biases bl. Finally, the inputs to the next layer
are simply the activations of the current layer, i.e. xl = φ(˜hl).

In many cases, successful results arise only from effective combination of these advances. Despite
this interdependence, our theoretical understanding of how these mechanisms and their interactions
affect neural networks remains impoverished.

One approach to studying these effects is through the lens of deep neural signal propagation. By
modelling the empirical input variance dynamics at the point of random initialisation, Saxe et al.
(2014) were able to derive equations capable of describing how signal propagates in nonlinear fully
connected feed-forward neural networks. This “mean ﬁeld” theory was subsequently extended by
Poole et al. (2016) and Schoenholz et al. (2017), in particular, to analyse signal correlation dynamics.
These analyses highlighted the existence of a critical boundary at initialisation, referred to as the “edge
of chaos”. This boundary deﬁnes a transition between ordered (vanishing), and chaotic (exploding)
regimes for neural signal propagation. Subsequently, the mean ﬁeld approximation to random neural
networks has been employed to analyse other popular neural architectures (Yang and Schoenholz,
2017; Xiao et al., 2018; Chen et al., 2018).

This paper focuses on the effect of noise on signal propagation in deep neural networks. Firstly we
ask: How is signal propagation in deep neural networks affected by noise? To gain some insight into
this question, we extend the mean ﬁeld theory developed by Schoenholz et al. (2017) for the special
case of dropout noise, into a generalised framework capable of describing the signal propagation
behaviour of stochastically regularised neural networks for different noise distributions.

Secondly we ask: How much are current weight initialisation strategies affected by noise-induced
regularisation in terms of their ability to initialise at a critical point for stable signal propagation?
Using our derived theory, we investigate this question speciﬁcally for rectiﬁed linear unit (ReLU)
networks. In particular, we show that no such critical initialisation exists for arbitrary zero-mean
additive noise distributions. However, for multiplicative noise, such an initialisation is shown to be
possible, given that it takes into account the amount of noise being injected into the network. Using
these insights, we derive novel critical initialisation strategies for several different multiplicative
noise distributions.

Finally, we ask: Given that a network is initialised at criticality, in what way does noise inﬂuence
the network’s ability to propagate useful information about its inputs? By analysing the correlation
between inputs as a function of depth in random deep ReLU networks, we highlight the following:
even though the statistics of individual inputs are able to propagate arbitrarily deep at criticality,
discriminatory information about the inputs becomes lost at shallower depths as the noise in the
network is increased. This is because in the later layers of a random noisy network, the internal
representations from different inputs become uniformly correlated. Therefore, the application of
noise regularisation directly limits the trainable depth of critically initialised ReLU networks.

2 Noisy signal propagation

We begin by presenting mean ﬁeld equations for stochastically regularised fully connected feed-
forward neural networks, allowing us to study noisy signal propagation for a variety of noise
distributions. To understand how noise inﬂuences signal propagation in a random network given an
input x0 ∈ RD0 , we inject noise into the model

˜hl = W l(xl−1 (cid:12) (cid:15)l−1) + bl, spa for l = 1, ..., L

(1)

2

using the operator (cid:12) to denote either addition or multiplication where (cid:15)l is an input noise vector,
sampled from a pre-speciﬁed noise distribution. For additive noise, the distribution is assumed to be
zero mean, for multiplicative noise distributions, the mean is assumed to be equal to one. The weights
W l ∈ RDl×Dl−1 and biases bl ∈ RDl are sampled i.i.d. from zero mean Gaussian distributions
with variances σ2
b , respectively, where Dl denotes the dimensionality of the lth hidden
layer in the network. The hidden layer activations xl = φ(˜hl) are computed element-wise using
an activation function φ(·), for layers l = 1, ..., L. Figure 1 illustrates this recursive sequence of
operations.

w/Dl−1 and σ2

To describe forward signal propagation for the model in (1), we make use of the mean ﬁeld approxi-
mation as in Poole et al. (2016) and analyse the statistics of the internal representations of the network
in expectation over the parameters and the noise. Since the weights and biases are sampled from zero
mean Gaussian distributions with pre-speciﬁed variances, we can approximate the distribution of the
pre-activations at layer l, in the large width limit, by a zero mean Gaussian with variance

˜ql = σ2
w

(cid:26)

(cid:20)

φ

Ez

(cid:16)(cid:112)

˜ql−1z

(cid:17)2(cid:21)

(cid:27)

(cid:12) µl−1
2

+ σ2
b ,

2 = E(cid:15)[((cid:15)l)2] is the
where z ∼ N (0, 1) (see Section A.1 in the supplementary material). Here, µl
second moment of the noise distribution being sampled from at layer l. The initial input variance is
given by q0 = 1
x0 · x0. Furthermore, to study the behaviour of a pair of signals from two different
D0
inputs, x0,a and x0,b, passing through the network, we can compute the covariance at each layer as

Ez1 [Ez2 [φ(˜u1)φ(˜u2)]] + σ2
b

ab = σ2
˜ql
w
(cid:113)

(cid:104)

˜ql−1
bb
(cid:113)

where ˜u1 =

˜ql−1
aa z1 and ˜u2 =

(cid:112)

˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

, with the correlation between

inputs at layer l given by ˜cl = ˜ql
aa ˜ql
˜ql
supplementary material for more details).

ab/

bb. Here, ql

aa is the variance of ˜hl,a

j

(see Section A.2 in the

For the backward pass, we use the equations derived in Schoenholz et al. (2017) to describe error
signal propagation.1 In the context of mean ﬁeld theory, the expected magnitude of the gradient at
each layer can be shown to be proportional to the variance of the error, ˜δl
˜δl+1
j W l+1
.
This allows for the distribution of the error signal at layer l to be approximated by a zero mean
Gaussian with variance

i) (cid:80)Dl+1

i = φ(cid:48)(˜hl

j=1

ji

Similarly, for noise regularised networks, the covariance between error signals can be shown to be

δ = ˜ql+1
˜ql
δ

Dl+1
Dl

σ2
w

Ez

(cid:20)
φ(cid:48) (cid:16)(cid:112)

˜qlz

(cid:17)2(cid:21)

.

ab,δ = ˜ql+1
˜ql
ab,δ

σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u1)φ(cid:48)(˜u2)]] ,

Dl+1
Dl+2

(2)

(3)

(4)

(5)

where ˜u1 and ˜u2 are deﬁned as was done in the forward pass.

Equations (2)-(5) fully capture the relevant statistics that govern signal propagation for a random
network during both the forward and the backward pass. In the remainder of this paper, we consider,
as was done by Schoenholz et al. (2017), the following necessary condition for training: “for a
random network to be trained information about the inputs should be able to propagate forward
through the network, and information about the gradients should be able to propagate backwards
through the network.” The behaviour of the network at this stage depends on the choice of activation,
noise regulariser and initial parameters. In the following section, we will focus on networks that use
the Rectiﬁed Linear Unit (ReLU) as activation function. The chosen noise regulariser is considered a
design choice left to the practitioner. Therefore, whether a random noisy ReLU network satisﬁes the
above stated necessary condition for training largely depends on the starting parameter values of the
network, i.e. its initialisation.

1It is, however, important to note that the derivation relies on the assumption that the weights used in the

forward pass are sampled independently from those used during backpropagation.

3

Figure 2: Deep signal propagation with and without noise. (a): Iterative variance map. (b): Variance
dynamics during forward signal propagation. In (a) and (b), lines correspond to theoretical predictions
and points to numerical simulations (means over 50 runs with shaded one standard deviation bounds),
for noiseless tanh (yellow) and noiseless ReLU (purple) networks, as well as for noisy tanh (red)
and noisy ReLU (brown) networks regularised using additive noise from a standard Gaussian. Both
tanh networks use (σw, σb) = (1, 0), the “Xavier” initialisation (Glorot and Bengio, 2010), while the
2, 0) the “He” initialisation (He et al., 2015). In our experiments,
ReLU networks use (σw, σb) = (
we use network layers consisting of 1000 hidden units (see Section C in the supplementary material
for more details on all our simulated experiments).

√

3 Critical initialisation for noisy rectiﬁer networks

Unlike the tanh nonlinearity investigated in previous work (Poole et al., 2016; Schoenholz et al.,
2017), rectifying activation functions such as ReLU are unbounded. This means that the statistics of
signal propagation through the network is not guaranteed to naturally stabilise through saturating
activations, as shown in Figure 2.

A point on the identity line in Figure 2 (a) represents a ﬁxed point to the recursive variance map
in equation (2). At a ﬁxed point, signal will stably propagate through the remaining layers of the
network. For tanh networks, such a ﬁxed point always exists irrespective of the initialisation, or
the amount of noise injected into the network. For ReLU networks, this is not the case. Consider
the “He” initialisation (He et al., 2015) for ReLU, commonly used in practice. In (b), we plot the
variance dynamics for this initialisation in purple and observe stable behaviour. But what happens
when we inject noise into each network? In the case of tanh (shown in red), the added noise simply
shifts the ﬁxed point to a new stable value. However, for ReLU, the noise entirely destroys the ﬁxed
point for the “He” initialisation, making signal propagation unstable. This can be seen in (a), where
the variance map for noisy ReLU (shown in brown) moves off the identity line entirely, causing the
signal in (b) to explode.

Therefore, to investigate whether signal can stably propagate through a random noisy ReLU network,
we examine (2) more closely, which for ReLU becomes (see Section B.1 in supplementary material)

˜ql = σ2
w

(cid:20) ˜ql−1
2

(cid:21)

(cid:12) µ2

+ σ2
b .

(6)

For ease of exposition we assume equal noise levels at each layer, i.e. µl
2 = µ2, ∀l. A critical
initialisation for a noisy ReLU network occurs when the tuple (σw, σb, µ2) provides a ﬁxed point ˜q∗,
to the recurrence in (6). This at least ensures that the statistics of individual inputs to the network will
be preserved throughout the ﬁrst forward pass. The existence of such a solution depends on the type
2 ˜q∗ + µ2σ2
w + σ2
of noise that is injected into the network. In the case of additive noise, ˜q∗ = σ2
b ,
w
√
implying that the only critical point initialisation for non-zero ˜q∗ is given by (σw, σb, µ2) = (
2, 0, 0).
Therefore, critical initialisation is not possible using any amount of zero-mean additive noise,
regardless of the noise distribution. For multiplicative noise, ˜q∗ = σ2
b , so the solution
w
(σw, σb, µ2) =
provides a critical initialisation for noise distributions with mean
one and a non-zero second moment µ2. For example, in the case of multiplicative Gaussian noise,
µ2 = σ2
. For dropout noise,

(cid:15) + 1, yielding critical initialisation with (σw, σb) =

2 ˜q∗µ2 + σ2

(cid:16)(cid:113) 2
µ2

(cid:16)(cid:113) 2

(cid:17)
σ2+1 , 0

, 0, µ2

(cid:17)

1

1

4

Table 1: Critical point initialisation for noisy ReLU networks.

DISTRIBUTION

P((cid:15))

— ADDITIVE NOISE —

µ2

σ2
(cid:15)

2β2

CRITICAL INITIALISATION

(σw, σb, σ(cid:15)) = (
√

(σw, σb, β) = (

√

2, 0, 0)

2, 0, 0)

N (0, σ2
(cid:15) )

Lap(0, β)

— MULTIPLICATIVE NOISE —

N (1, σ2
(cid:15) )

(σ2

(cid:15) + 1)

(σw, σb, σ(cid:15)) =

Lap(1, β)

(2β2 + 1)

(σw, σb, β) =

(cid:17)

(cid:16)(cid:113) 2
(cid:15) +1 , 0, σ(cid:15)
σ2
(cid:17)

(cid:16)(cid:113) 2

2β2+1 , 0, β

P oi(1)

P((cid:15) = 1
P((cid:15) = 0) = 1 − p

p ) = p,

2

1
p

(σw, σb, λ) = (1, 0, 1)

(σw, σb, p) = (

2p, 0, p)

√

GAUSSIAN

LAPLACE

GAUSSIAN

LAPLACE

POISSON

DROPOUT

Figure 3: Critical initialisation for noisy ReLU networks. (a): Iterative variance map. (b): Vari-
ance dynamics during forward signal propagation. In (a) and (b), lines correspond to theoretical
predictions and points to numerical simulations. Dropout (p = 0.6) is shown in green for dif-
w = 2(0.6) = 2
(critical), σ2
ferent initialisations, σ2
(exploding sig-
µ2
(0.6)−1 < 2
w = (0.85)2
nal) and σ2
(vanishing signal). Similarly, multiplicative Gaussian noise
µ2
(σ(cid:15) = 0.25) is shown in red with σ2
w =
(exploding) and
w = (0.75)2 2
σ2
( vanishing). (c): Variance critical boundary for initialisation, separating numerical
µ2
overﬂow and underﬂow signal propagation regimes.

w = (1.25)2 2
µ2

(0.25)2+1 = 2
µ2

(0.6)−1 > 2
µ2

w = (1.15)2

(critical), σ2

2

2

2

√

µ2 = 1/p (with p the probability of retaining a neuron); thus, to initialise at criticality, we must
2p, 0). Table 1 summarises critical initialisations for some commonly used
set (σw, σb) = (
noise distributions. We also note that similar results can be derived for other rectifying activation
functions; for example, for multiplicative noise the critical initialisation for parametric ReLU (PReLU)
activations (with slope parameter α) is given by (σw, σb, µ2) =

(cid:16)(cid:113) 2

(cid:17)

.

µ2(α2+1) , 0, µ2

To see the effect of initialising on or off the critical point for ReLU networks, Figure 3 compares
the predicted versus simulated variance dynamics for different initialisation schemes. For schemes
not initialising at criticality, the variance map in (a) no longer lies on the identity line and as a result
the forward propagating signal in (b) either explodes, or vanishes. In contrast, the initialisations
derived above lie on the critical boundary between these two extremes, as shown in (c) as a function
of the noise. By compensating for the amount of injected noise, the signal corresponding to the
initialisation σ2
is preserved in (b) throughout the entire forward pass, with roughly constant
variance dynamics.

w = 2
µ2

5

Figure 4: Propagating correlation information in noisy ReLU networks. (a): Iterative correlation
map with ﬁxed points indicated by “X” marks on the identity line. (b): Correlation dynamics during
forward signal propagation. In (a) and (b), lines correspond to theoretical predictions and points to
numerical simulations. All simulated networks were initialised at criticality for each noise type and
level. (c): Slope at the ﬁxed point correlation as a function of the amount of noise injected into the
network.

Next, we investigate the correlation dynamics between inputs. Assuming that (6) is at its ﬁxed point
˜q∗, which exists only if σ2
, the correlation map for a noisy ReLU network is given by (see
Section B.2 in supplementary material)

w = 2
µ2

˜cl =

1
µ2

(cid:40)

˜cl−1sin−1 (cid:0)˜cl−1(cid:1) + (cid:112)1 − (˜cl−1)2
π

+

˜cl−1
2

(cid:41)

.

Figure 4 plots this theoretical correlation map against simulated dynamics for different noise types
and levels. For no noise, the ﬁxed point c∗ in (a) is situated at one (marked with an “X” on the blue
line). The slope of the blue line indicates a non-decreasing function of the input correlations. After a
certain depth, inputs end up perfectly correlated irrespective of their starting correlation, as shown in
(b). In other words, random deep ReLU networks lose discriminatory information about their inputs
as the depth of the network increases, even when initialised at criticality. When noise is added to the
network, inputs decorrelate and c∗ moves away from one. However, more importantly, correlation
information in the inputs become lost at shallower depths as the noise level increases, as can be seen
in (b).

How quickly a random network loses information about its inputs depends on the rate of convergence
to the ﬁxed point c∗. Using this observation, Schoenholz et al. (2017) derived so-called depth scales
ξc, by assuming |cl − c∗| ∼ e−l/ξc. These scales essentially control the feasible depth at which
networks can be considered trainable, since they may still allow useful correlation information to
propagate through the network. In our case, the depth scale for a noisy ReLU network under this
assumption can be shown to be (see Section B.3 in supplementary material)

where

ξc = −1/ln [χ(c∗)] ,

χ(c∗) =

1
µ2π

(cid:104)
sin−1 (c∗) +

(cid:105)

.

π
2

(7)

(8)

(9)

The exponential rate assumption underlying the derivation of (8) is supported in Figure 5, where
for different noise types and levels, we plot |cl − c∗| as a function of depth on a log-scale, with
corresponding linear ﬁts (see panels (a) and (c)). We then compare the theoretical depth scales from
(8) to actual depth scales obtained through simulation (panels (b) and (d)), as a function of noise
and observe a good ﬁt for non-zero noise levels.4 We thus ﬁnd that noise limits the depth at which
critically initialised ReLU networks are expected to perform well through training.

4We note Hayou et al. (2018) recently showed that the rate of convergence for noiseless ReLU networks is
not exponential, but polynomial instead. Interestingly, keeping with the exponential rate assumption, we indeed
ﬁnd that the discrepancy between our theoretical depth scales from (8) and our simulated depth scales, is largest
at very low noise levels. However, at more typical noise levels, such as a dropout rate of p = 0.5 for example,
the assumption seems to provide a close ﬁt, with good agreement between theory and simulation.

6

Figure 5: Noise dependent depth scales for training. (a): Linear ﬁts (dashed lines) to |cl − c∗| as a
function of depth on a log-scale (solid lines) for varying amounts of dropout (p = 0.1 to p = 0.9
by 0.1). (b): Theoretical depth scales (solid lines) versus empirically inferred scales (dashed lines)
per dropout rate. Scales are inferred noting that if |cl − c∗| ∼ e−l/ξc, then a linear ﬁt, al + b, in
the logarithmic domain gives ξc ≈ − 1
a , for large l. In other words, the negative inverse slope of a
linear ﬁt to the log differences in correlation should match the theoretical values for ξc. Therefore,
we compare ξc = −1/ln [χ(c∗)] to − 1
a for different levels of noise. (c) - (d): Similar to (a) and (b),
but for Gaussian noise (σ(cid:15) = 0.1 to σ(cid:15) = 1.9 by 0.15).

We next brieﬂy discuss error signal propagation during the backward pass for noise regularised ReLU
networks. When critically initialised, the error variance recurrence relation in (4) for these networks
is (see Section B.4 in supplementary material)

with the covariance between error signals in (5), given by (see Section B.5 in supplementary material)

δ = ˜ql+1
˜ql
δ

Dl+1
Dlµ2

,

ab,δ = ˜ql+1
˜ql
ab,δ

Dl+1
Dl+2

χ(c∗).

(10)

(11)

Note the explicit dependence on the width of the layers of the network in (10) and (11). We ﬁrst
consider constant width networks, where Dl+1 = Dl, for all l = 1, ..., L. For any amount of
multiplicative noise, µ2 > 1, and we see from (10) that gradients will tend to vanish for large depths.
Furthermore, Figure 4 (c) plots χ(c∗) as a function of µ2. As µ2 increases from one, χ(c∗) decreases
from one. Therefore, from (11), we also ﬁnd that error signals from different inputs will tend to
decorrelate at large depths.

Interestingly, for non-constant width networks, stable gradient information propagation may still be
possible. If the network architecture adapts to the amount of noise being injected by having the widths
of the layers grow as Dl+1 = Dlµ2, then (10) should be at its ﬁxed point solution. For example, in
the case of dropout Dl+1 = Dl/p, which implies that for any p < 1, each successive layer in the
network needs to grow in width by a factor of 1/p to promote stable gradient ﬂow. Similarly, for
multiplicative Gaussian noise, Dl+1 = Dl(σ2
(cid:15) + 1), which requires the network to grow in width
unless σ2
(cid:15) = 0. Similarly, if Dl+2 = Dl+1χ(c∗) = Dlµ2χ(c∗) in (11), the covariance of the error
signal should be preserved during the backward pass, for arbitrary values of µ2 and χ(c∗).

7

Figure 6: Depth scale experiments on MNIST and CIFAR-10. (a) Variance propagation dynamics for
MNIST on and off the critical point initialisation (dashed black line) with dropout (p = 0.6). The
cyan curve represents the theoretical boundary at which numerical instability issues are predicted
to occur and is computed as L∗ = ln(K)/ln( σ2
2 µ2), where K is the largest (or smallest) positive
number representable by the computer. Speciﬁcally, we use 32-bit ﬂoating point numbers and set
K = 3.4028235 × 1038, if σ2
. (b) Depth scales ﬁt
to the training loss on MNIST for networks initialised at criticality for dropout rates p = 0.1 (severe
dropout) to p = 1 (no dropout). (c) Depth scales ﬁt to the validation loss on MNIST. (d) - (f): Similar
to (a) - (c), but for CIFAR-10. For each plot we highlight trends by smoothing the colour grid (for
non smoothed versions see Section C.5 in the supplementary material).

and K = 1.1754944 × 10−38, if σ2

w > 2
µ2

w < 2
µ2

w

4 Experimental results

From our analysis of deep noisy ReLU networks in the previous section, we expect that a necessary
condition for such a network to be trainable, is that the network be initialised at criticality. However,
whether the layer widths are varied or not for the sake of backpropagation, the correlation dynamics
in the forward pass may still limit the depth at which these networks perform well.

We therefore investigate the performance of noise-regularised deep ReLU networks on real-world
data. First, we validate the derived critical initialisation. As the depth of the network increases, any
initialisation strategy that does not factor in the effects of noise, will cause the forward propagating
signal to become increasingly unstable. For very deep networks, this might cause the signal to either
explode or vanish, even within the ﬁrst forward pass, making the network untrainable. To test this,
we sent inputs from MNIST and CIFAR-10 through ReLU networks using dropout (with p = 0.6) at
varying depths and for different initialisations of the network. Figure 6 (a) and (d) shows the evolution
of the input statistics as the input propagates through each network for the different data sets. For
initialisations not at criticality, the variance grows or shrinks rapidly to the point of causing numerical
overﬂow or underﬂow (indicated by black regions). For deep networks, this can happen well before
any signal is able to reach the output layer. In contrast, initialising at criticality (as shown by the
dashed black line), allows for the signal to propagate reliably even at very large depths. Furthermore,
given the ﬂoating point precision, if σ2
, we can predict the depth at which numerical overﬂow
(or underﬂow) will occur by solving for L∗ in K = (cid:0)σ2
q0, where K is the largest (or
smallest) positive number representable by the computer (see Section C.4 in supplementary material).
These predictions are shown by the cyan line and provide a good ﬁt to the empirical limiting depth
from numerical instability.

wµ2/2(cid:1)L∗

w (cid:54)= 2
µ2

We now turn to the issue of limited trainability. Due to the loss of correlation information between
inputs as a function of noise and network depth, we expect noisy ReLU networks not to be able to
perform well beyond certain depths. We investigated depth scales for ReLU networks with dropout
initialised at criticality: we trained 100 networks on MNIST and CIFAR-10 for 200 epochs using SGD
and a learning rate of 10−3 with dropout rates ranging from 0.1 to 1 for varying depths. The results

8

are shown in Figure 6 (see Section C.5 of the supplementary material for additional experimental
results). For each network conﬁguration and noise level, the critical initialisation σ2
was
used. We indeed observe a relationship between depth and noise on the loss of a network, even at
criticality. Interestingly, the line 6ξc (Schoenholz et al., 2017), seems to track the depth beyond
which the relative performance on the validation loss becomes poor, more so than on the training loss.
However, in both cases, we ﬁnd that even modest amounts of noise can limit performance.

w = 2
µ2

5 Discussion

By developing a general framework to study signal propagation in noisy neural networks, we were
able to show how different stochastic regularisation strategies may impact the ﬂow of information
in a deep network. Focusing speciﬁcally on ReLU networks, we derived novel critical initialisation
strategies for multiplicative noise distributions and showed that no such critical initialisations exist
for commonly used additive noise distributions. At criticality however, our theory predicts that the
statistics of the input should remain within a stable range during the forward pass and enable reliable
signal propagation for noise regularised deep ReLU networks. We veriﬁed these predictions by
comparing them with numerical simulations as well as experiments on MNIST and CIFAR-10 using
dropout and found good agreement.

Interestingly, we note that a dropout rate of p = 0.5 has often been found to work well for ReLU
networks (Srivastava et al., 2014). The critical initialisation corresponding to this rate is (σw, σb) =
√
2p, 0) = (1, 0). This is exactly the “Xavier” initialisation proposed by Glorot and Bengio (2010),
(
which prior to the development of the “He” initialisation, was often used in combination with
dropout (Simonyan and Zisserman, 2014). This could therefore help to explain the initial success
associated with this speciﬁc dropout rate. Similarly, Srivastava et al. (2014) reported that adding
multiplicative Gaussian noise where (cid:15) ∼ N (1, σ2
(cid:15) = 1, also seemed to perform well, for
= (1, 0), again corresponding to the “Xavier” method.
which the critical initialisation is

(cid:15) ), with σ2

(cid:17)
(cid:16)(cid:113) 2
(cid:15) +1 , 0
σ2

Although our initialisations ensure that individual input statistics are preserved, we further analysed
the correlation dynamics between inputs and found the following: at large depths inputs become
predictably correlated with each other based on the amount of noise injected into the network. As a
consequence, the representations for different inputs to a deep network may become indistinguishable
from each other in the later layers of the network. This can make training infeasible for noisy ReLU
networks of a certain depth and depends on the amount of noise regularisation being applied.

We now note the following shortcomings of our work: ﬁrstly, our ﬁndings only apply to fully
connected feed-forward neural networks and focus almost exclusively on the ReLU activation
function. Furthermore, we limit the scope of our architectural design to a recursive application of a
dense layer followed by a noise layer, whereas in practice a larger mix of layers is usually required to
solve a speciﬁc task.

Ultimately, we are interested in reducing the number of decisions that need to made when designing
deep neural networks and understanding the implications of those decisions on network behaviour
and performance. Any machine learning engineer exploring a neural network based solution to a
practical problem will be faced with a large number of possible design decisions. All these decisions
cost valuable time to explore. In this work, we hope to have at least provided some guidance in this
regard, speciﬁcally when choosing between different initialisation strategies for noise regularised
ReLU networks and understanding their associated implications.

Acknowledgements

We would like to thank the reviewers for their insightful comments which improved the quality of this
work. Furthermore, we would like to thank Google, the CSIR/SU Centre for Artiﬁcial Intelligence
Research (CAIR) as well as the Science Faculty and the Postgraduate and International Ofﬁce of
Stellenbosch University for ﬁnancial support. Finally, we gratefully acknowledge the support of
NVIDIA Corporation with the donation of a Titan Xp GPU used for this research.

9

References

X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep feedforward neural networks,”
in Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, 2010, pp.
249–256.

A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks,” Proceedings of the International Conference on Learning
Representations, 2014.

D. Sussillo and L. Abbott, “Random walk initialization for training very deep feedforward networks,”

arXiv preprint arXiv:1412.6558, 2014.

K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers: Surpassing human-level per-
formance on ImageNet classiﬁcation,” in Proceedings of the IEEE International Conference on
Computer Vision, 2015, pp. 1026–1034.

D. Mishkin and J. Matas, “All you need is a good init,” Proceedings of International Conference on

Learning Representations, 2016.

X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural networks,” in Proceedings of the

International Conference on Artiﬁcial Intelligence and Statistics, 2011, pp. 315–323.

N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: a simple
way to prevent neural networks from overﬁtting.” Journal of Machine Learning Research, vol. 15,
no. 1, pp. 1929–1958, 2014.

A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation with deep convolutional
neural networks,” in Advances in Neural Information Processing Systems, 2012, pp. 1097–1105.

G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural networks for LVCSR using
rectiﬁed linear units and dropout,” in Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, 2013, pp. 8609–8613.

B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli, “Exponential expressivity in deep
neural networks through transient chaos,” in Advances in Neural Information Processing Systems,
2016, pp. 3360–3368.

S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein, “Deep information propagation,”

Proceedings of the International Conference on Learning Representations, 2017.

G. Yang and S. Schoenholz, “Mean ﬁeld residual networks: On the edge of chaos,” in Advances in

Neural Information Processing Systems, 2017, pp. 7103–7114.

L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington, “Dynamical isometry and
a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks,”
Proceedings of the International Conference on Machine Learning, 2018.

M. Chen, J. Pennington, and S. S. Schoenholz, “Dynamical isometry and a mean ﬁeld theory of RNNs:
Gating enables signal propagation in recurrent neural networks,” Proceedings of the International
Conference on Machine Learning, 2018.

S. Hayou, A. Doucet, and J. Rousseau, “On the selection of initialization and activation function for

deep neural networks,” arXiv preprint arXiv:1805.08266, 2018.

K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,”

arXiv preprint arXiv:1409.1556, 2014.

10

Supplementary Material

In this section, we provide additional details of derivations and experimental results presented in the
paper.

A Signal propagation in noise regularised neural networks

To review, given an input x0 ∈ RD0, we consider the following noisy random network model

˜hl = W l(xl−1 (cid:12) (cid:15)l−1) + bl, spa for l = 1, ..., L
(12)
where we inject noise into the model using the operator (cid:12) to denote either addition or multiplication.
The vector (cid:15)l is an input noise vector, sampled from a pre-speciﬁed noise distribution. For additive
noise, the distribution is assumed to be zero mean. Whereas for multiplicative noise distributions, the
mean is assumed to be equal to one. The weights W l ∈ RDl×Dl−1 and biases bl ∈ RDl are sampled
i.i.d. from zero mean Gaussian distributions with variances σ2
b , respectively, where
Dl denotes the dimensionality of the lth hidden layer in the network. The hidden layer activations
xl = φ(˜hl) are computed element-wise using an activation function φ(·), for layers l = 1, ..., L.

w/Dl−1 and σ2

A.1 Single input signal propagation

We consider the network’s behavior at initialisation. In this setting, the expected mean (over the
weights, biases and noise distribution) of a unit in the pre-activations ˜hl
j for a single signal passing
through the network will be zero with variance

˜ql = Ew,b,(cid:15)[(˜hl

j)2]

= Ew,(cid:15)[{wl,j · (xl−1

j (cid:12) (cid:15)l−1

j

)}2] + Eb[(bl

j)2]

= σ2
w

1
Dl−1

Dl−1
(cid:88)

j=1

(cid:104)
φ(˜hl−1
j

)2 (cid:12) E(cid:15)[((cid:15)l−1

)2]

j

+ σ2

b ,

(cid:105)

where we use wl,j to denote the j-th row of W l. The second last line relies on the bias distribution
being zero mean, while the ﬁnal step makes use of the independence between the inputs and the
noise in the multiplicative case, and the noise being zero mean in the additive case. Furthermore, to
ensure the expected value of the pre-activations remain unbiased, we only consider additive noise
distributions with zero mean and multiplicative noise distributions with a mean equal to one. As in
Poole et al. (2016), we make the self averaging assumption and consider the large layer width case
where the previous layer’s pre-activations are assumed to be Gaussian with zero mean and variance
˜ql−1. This gives the following noisy variance map

˜ql = σ2
w

(cid:26)

(cid:20)

φ

Ez

(cid:16)(cid:112)

˜ql−1z

(cid:17)2(cid:21)

(cid:27)

(cid:12) µl−1
2

+ σ2
b ,

(13)

where z ∼ N (0, 1) and µl
from at layer l. The initial input variance is given by q0 = 1
D0

2 = E(cid:15)[((cid:15)l)2] is the second moment of the noise distribution being sampled

x0 · x0.

A.2 Two input signal propagation

To study the behaviour of a pair of signals, x0,a and x0,b, passing through the network, we can
compute the covariance in expectation over the noise and the parameters as

ab = Ew,b,(cid:15)[˜hl,a
˜ql
j
(cid:104)(cid:16)
wl,j · (xl−1,a
= Ew,b,(cid:15)

˜hl,b
j ]

) + bl
j
(cid:17) (cid:16)
)

j

j

(cid:104)(cid:16)

(cid:104)(cid:16)

(cid:104)(cid:16)

= Ew,b,(cid:15)

wl,j · (xl−1,a

(cid:12) (cid:15)l−1,a
j

white + Ew,b,(cid:15)

wl,j · (xl−1,a

j

(cid:12) (cid:15)l−1,a
j

white + Ew,b,(cid:15)

white + Ew,b,(cid:15)

wl,j · (xl−1,b
j)2(cid:3) .

j

(cid:2)(bl

(cid:12) (cid:15)l−1,b
j

)

(cid:17)
)
(cid:17)

bl
j
(cid:105)

bl
j

(cid:12) (cid:15)l−1,a
j

(cid:17) (cid:16)

wl,j · (xl−1,b

j

(cid:17)(cid:105)

) + bl
j

(cid:12) (cid:15)l−1,b
j
(cid:17)(cid:105)

wl,j · (xl−1,b
(cid:105)

j

(cid:12) (cid:15)l−1,b
j

)

11

Since the noise is i.i.d and we have that Eb[bl

j] = 0, we ﬁnd that

(cid:104)(cid:16)

(cid:17) (cid:16)

ab = Ew
˜ql

wl,j · xl−1,a

j

wl,j · xl−1,b

j

(cid:17)(cid:105)

+ Eb

(cid:2)(bl

j)2(cid:3)

= σ2
w

1
Dl−1

Dl−1
(cid:88)

j=1

(cid:104)
φ

(cid:16)˜hl−1,a

(cid:17)

φ

j

(cid:16)˜hl−1,b

j

(cid:17)(cid:105)

+ σ2
b ,

which in the large width limit becomes

(cid:112)

where ˜u1 =
inputs at layer l given by

˜ql−1
aa z1 and ˜u2 =

Ez1 [Ez2 [φ(˜u1)φ(˜u2)]] + σ2
b

ab = σ2
˜ql
w
(cid:113)

˜ql−1
bb

(cid:104)

˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

, with the correlation between

(cid:113)

˜cl = ˜ql

ab/

aa ˜ql
˜ql

bb.

Here, zi ∼ N (0, 1) for i = 1, 2 and ql

aa is the variance of ˜hl,a
j .

B Signal propagation in noise regularised ReLU networks

In this section, we give additional details of theoretical results presented in the paper that were
speciﬁcally derived for noisy ReLU networks.

B.1 Variance of input signals

Let f (z) = e−z2/2

√

2π

, then the variance map in (13) using ReLU, i.e. φ(a) = max(0, a), becomes

˜ql = σ2
w

f (z)φ

˜ql−1z

dz

(cid:12) µ2 + σ2
b

(cid:16)(cid:112)

(cid:16)(cid:112)

(cid:21)

(cid:17)2

(cid:17)2

f (z)φ

˜ql−1z

dz +

f (z)φ

˜ql−1z

dz

(cid:12) µ2 + σ2
b

(cid:16)(cid:112)

(cid:17)2

(cid:21)

(cid:90) ∞

0

(cid:20)(cid:90) ∞

−∞

(cid:20)(cid:90) 0

−∞

(cid:20)
˜ql−1

(cid:20) ˜ql−1
2

= σ2
w

= σ2
w

= σ2
w

(cid:21)

f (z)z2dz

(cid:12) µ2 + σ2
b

(cid:90) ∞

0

(cid:21)

(cid:12) µ2

+ σ2
b .

B.2 Correlation between input signals

Assuming that the variance map in (18) is at its ﬁxed point ˜q∗, which exits only if σ2
correlation map in (16) for a noisy ReLU network is given by

w = 2
µ2

, the

˜cl =

2
µ2 ˜q∗

(cid:90) ∞

(cid:90) ∞

−∞

−∞

f (z1)f (z2)φ(˜u1)φ(˜u2)dz2dz1 + σ2
b

(19)

√

, ˜u1 =

˜q∗z1 and ˜u2 =

√

(cid:104)
˜cl−1z1 + (cid:112)1 − (˜cl−1)2z2

(cid:105)

.

˜q∗

where φ(a) = max(a, 0), f (zi) = e−z2
Note that

√

i /2
2π

(14)

(15)

(16)

(17)

(18)

˜u1

˜u2

(cid:26)≥ 0, if z1 > 0
< 0, Otherwise
≥ 0, if z2 > −˜cl−1z1
< 0, Otherwise

√

(cid:40)

1−(˜cl−1)2

,

12

therefore (19) becomes

(cid:90) ∞

(cid:90) ∞

˜cl =

2
µ2 ˜q∗

0

√

−˜cl−1z1
1−(˜cl−1)2
(cid:90) ∞

(cid:90) ∞

0

√

−˜cl−1z1
1−(˜cl−1)2

(cid:90) ∞

(cid:90) ∞

√

−˜cl−1z1
1−(˜cl−1)2

=

=

w

2
µ2 ˜q∗ σ2
2˜cl−1
µ2

0

f (z1)f (z2)˜u1 ˜u2dz2dz1 + σ2
b

f (z1)f (z2)(cid:112)˜q∗z1

(cid:112)˜q∗

(cid:20)
˜cl−1z1 +

(cid:113)

(cid:21)

1 − (˜cl−1)2z2

dz2dz1 + σ2
b

f (z1)f (z2)z2

1dz2dz1

2(cid:112)1 − (˜cl−1)2
µ2

(cid:90) ∞

(cid:90) ∞

0

√

−˜cl−1z1
1−(˜cl−1)2

addsomewhitespacehere +

f (z1)f (z2)z1z2dz2dz1.

(20)

The ﬁrst term in (20) can then be written as




(cid:90) ∞

(cid:90) 0

2˜cl−1
µ2



0

√

−˜cl−1 z1
1−(˜cl−1)2

f (z1)f (z2)z2

1dz2dz1 +

f (z1)f (z2)z2

1dz2dz1

(21)

(cid:90) ∞

(cid:90) ∞

0

0






.

In (21), the ﬁrst term inside the braces is given by

(cid:90) ∞

(cid:90) 0

0

√

−˜cl−1z1
1−(˜cl−1)2

f (z1)f (z2)z2

1dz2dz1 =

f (z1)z2

1erf

(cid:90) ∞

0

1
2

1
2π

1
2π

=

=

(cid:34)
˜cl−1(cid:113)

(cid:20)
˜cl−1(cid:113)

(cid:33)

dz1

(cid:32)

˜cl−1z1
(cid:112)1 − (˜cl−1)
(cid:32)

1 − (˜cl−1)2 + tan−1

(cid:33)(cid:35)

˜cl−1
(cid:112)1 − (˜cl−1)2

1 − (˜cl−1)2 + sin−1 (cid:0)˜cl−1(cid:1)

(cid:21)

(22)

with erf(a) = 1
π

(cid:82) a
−a e−t2

dt. The second term inside the braces in (21) equals

Therfore, (21) becomes

(cid:90) ∞

(cid:90) ∞

0

0

f (z1)f (z2)z2

1dz2dz1 =

f (z1)z2

1dz1

(cid:90) ∞

0

1
2
1
4

=

.

(cid:113)

(˜cl−1)2
µ2π

1 − (˜cl−1)2 +

sin−1 (cid:0)˜cl−1(cid:1) +

˜cl−1
µ2π

˜cl−1
2µ2

Similarly, the second term in (20) can be split up as follows

2(cid:112)1 − (˜cl−1)2
µ2




(cid:90) ∞

(cid:90) 0



0

√

−˜cl−1z1
1−(˜cl−1)2

f (z1)f (z2)z1z2dz2dz1 +

f (z1)f (z2)z1z2dz2dz1

.

(cid:90) ∞

(cid:90) ∞

0

0

The ﬁrst term inside the braces of (25) is

(cid:90) ∞

(cid:90) 0

0

√

−˜cl−1z1
1−(˜cl−1 )2

f (z1)f (z2)z1z2dz2dz1 =

f (z1)z1

2(1−(˜cl−1)2) − 1

dz1

˜cl−1z2
1

(cid:35)

(cid:34)

−
e

(23)

(24)





(25)

(26)

0
(cid:26) 1 − (˜cl−1)2
√

2π

(cid:27)

−

1
√
2π

(cid:90) ∞

1
√
2π

=

1
√
2π
(˜cl−1)2
2π

= −

13

and the second term is

(cid:90) ∞

(cid:90) ∞

0

0

f (z1)f (z2)z1z2dz2dz1 =

f (z1)z1dz1

(cid:90) ∞

0

1
√
2π

1
2π

.

=

Putting these two terms together, (25) becomes

(cid:113)

−

(˜cl−1)2
µ2π

1 − (˜cl−1)2 +

1 − (˜cl−1)2.

(cid:113)

1
µ2π

Finally, summing all the terms in (24) and (28) gives (19) as

˜cl =

1
µ2

(cid:40)

˜cl−1sin−1 (cid:0)˜cl−1(cid:1) + (cid:112)1 − (˜cl−1)2
π

+

˜cl−1
2

(cid:41)

.

We note that for the noiseless case, (29) is identical to the result recently obtained by Hayou et al.
(2018), where the authors used a slightly different approach.

B.3 Depth scales for trainability

We recap the result in Schoenholz et al. (2017) and adapt the derivation for the speciﬁc case of a
noisy ReLU network. Let cl = c∗ + εl, such that as long as liml→∞cl = c∗ exist we have that ε → 0
as l → ∞. Then Schoenholz et al. (2017) derived the following asymptotic recurrence relation

εl+1 = εlχ(c∗) + O((εl)2),

where

√

1 = ˜u1 =

with ˜u∗
network where σ2

˜q∗z1 and ˜u∗

˜q∗

2 =
, we have that

w = 2
µ2

χ(c∗) = σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u∗

√

(cid:104)

˜c∗z1 + (cid:112)1 − (˜c∗)2z2

2)]] ,

1)φ(cid:48)(˜u∗
(cid:105)

. Now, speciﬁcally for a noisy ReLU

χ(c∗) =

f (z1)f (z2)φ(cid:48)(˜u∗

1)φ(cid:48)(˜u∗

2)dz2dz1

(cid:90) ∞

(cid:90) ∞

−∞
(cid:90) ∞

−∞
(cid:90) ∞

2
µ2
2
µ2

2
µ2

2
µ2
1
µ2π

=

=

=

=

0

(cid:90) ∞

0
(cid:34)

1
2π

f (z1)f (z2)dz2dz1

− c∗ z1√
1−(c∗ )2
(cid:34)

(cid:32)

f (z1)

erf

√

1
2
(cid:32)

tan−1

c∗
(cid:112)1 − (c∗)2
(cid:105)

c∗z1
2(cid:112)1 − (c∗)2
(cid:35)

(cid:33)

+

1
4

(cid:104)
sin−1 (c∗) +

π
2

(cid:33)

(cid:35)

+ 1

dz1

Note that χ(c∗) is a constant, thus for large l the solution to the recurrence relation in (30) is expected
to be exponential, i.e. εl ∼ e−l/ξc. Here ξc, is considered the depth scale, which controls how deep
discriminatory information about the inputs can propagate through the network. We can then solve
for ξc to ﬁnd

ξc = −1/ln(χ(c∗)) = −ln

(cid:20) sin−1 (c∗)
µ2π

+

1
2µ2

(cid:21)−1

.

14

(27)

(28)

(29)

(30)

(31)

(32)

(33)

B.4 Variance of error signals

Under the mean ﬁeld assumption, Schoenholz et al. (2017) approximates the error signal at layer l by
a zero mean Gaussian with variance

where ˜ql
noisy ReLU network we have that

i)2], with ˜δl

δ = E[(˜δl

i = φ(cid:48)(˜hl

˜δl+1
j W l+1

ji

. In our context, for a critically initialised

(cid:20)
φ(cid:48) (cid:16)(cid:112)

˜qlz

(cid:17)2(cid:21)

,

σ2
w

Ez

δ = ˜ql+1
˜ql
δ

Dl+1
Dl
i) (cid:80)Dl+1

j=1

δ = ˜ql+1
˜ql
δ

= ˜ql+1
δ

(cid:90) ∞

0

Dl+1
Dl
Dl+1
Dl

2
µ2
1
µ2

.

f (z)dz

B.5 Correlation between error signals

The covariance between error signals is approximated using

ab,δ = ˜ql+1
˜ql
ab,δ

σ2
w

Ez1 [Ez2 [φ(cid:48)(˜u1)φ(cid:48)(˜u2)]] ,

Dl+1
Dl+2

where ˜u1 and ˜u2 are deﬁned as was done in the forward pass. Here, we simply use the result in (32)
for noisy ReLU networks to ﬁnd

ab,δ = ˜ql+1
˜ql
ab,δ

χ(c∗)

Dl+1
Dl+2
Dl+1

= ˜ql+1
ab,δ

(cid:2)sin−1 (c∗) + π
Dl+2µ2π

2

(cid:3)

.

(34)

(35)

(36)

(37)

(38)

(39)

In this section we provide additional details regarding our experiments in the paper. Code to reproduce
all the experiments is available at https://github.com/ElanVB/noisy_signal_prop.

C Experimental details

C.1

Input data

For all experiments the network input data properties that remain consistent (unless stated otherwise)
are as follows: each observation consists of 1000 features and each feature value is drawn i.i.d. from
a standard normal distribution.

C.2 Variance propagation dynamics

The experiments conducted to gather results for Figures 2 and 3 aim to empirically show the
relationship between the variances at arbitrary layers in a neural network.

Iterative map: For the results depicted in Figures 2 (a) and 3 (a), the experimental set up is as follows.
The data used as input to these experiments comprises of 30 sets of 30 observations. The input is
scaled such that the variance of observations within each set is the same and the variance across
each set is different and forms a range of qset ∈ [0, 15]. As such, our results are averaged over 30
observations and 50 samplings of initial weights to a single hidden-layer network.

Convergence dynamics: For the results depicted in Figures 2 (b) and 3 (b), the experimental set up is
as follows. The data used as input to these experiments comprises of a set of 50 observations scaled
such that each observation’s variance is four (q = 4). As such, our results are averaged over 50
observations and 50 samplings of initial weights to a 15 hidden-layer network.

C.3 Correlation propagation dynamics

The experiments conducted to gather results for Figure 4 and 5 aim to empirically show the relation-
ship between the correlations of observations at arbitrary layers in a neural network.

15

Iterative map: For the results depicted in Figure 4 (a), the experimental set up is as follows. The data
used as input to these experiments comprises of 50 sets of 50 observations. The ﬁrst observation in
each set is sampled from a standard normal distribution and subsequent observations are generated
such that the correlation between the ﬁrst element and the ith element form a range of corr0,i ∈ [0, 1].
As such, our results are averaged over 50 observations and 50 samplings of initial weights to a single
hidden-layer network.

Convergence dynamics: For the results depicted in Figure 4 (b), the experimental set up is as
follows. The data used as input to these experiments comprises of three sets of 50 equally correlated
observations. Each set has a different correlation value such that corrset ∈ {0, 0.5, 0.9}. As such, our
results are averaged over 50 observations and 50 samplings of initial weights to a 15 hidden-layer
network.

Conﬁrmation of exponential rate of convergence for correlations: This section discusses how the
results depicted in Figure 5 are acquired. These experiments support the assumption that the rate
of convergence for correlations is exponential when using noise regularisation with rectiﬁer neural
networks. The experimental set up for this section is very similar to that of the above convergence
dynamics experiment, the only difference being the statistics we calculate from the correlation values.
The aspect of this experiment that may seem the most unclear is the reason why we claim that the
negative inverse slope of a linear ﬁt to the log differences in correlation should match the theoretical
values for ξc. The derivation to justify this is as follows. If a good ﬁt of the form al + b can be found
in the logarithmic domain for the rate of convergence, it would strongly indicate that the convergence
rate is exponential. Following this, we set the problem up like so:

Let us now assume that ln (cid:0)|cl − c∗|(cid:1) can be linearly approximated:

Since we are concerned with deep neural networks, we can take the limit as l becomes arbitrarily
large and see that as l grows the effect of b decreases (liml→∞ |al| (cid:29) |b|). Thus, we continue like so:

Thus, we have come to the ﬁnding that if the correlation rate of convergence is exponential and we
work with deep neural networks, the negative inverse slope of a linear ﬁt to the log differences in
correlation should match the theoretical values for ξc. Figure 5 shows that the theory closely matches
this approximation.

C.4 Depth scales

This section handles the experiments conducted related to determining the maximum depth variance
information can stably propagate through a network and the depth at which these networks can be
trained, both depicted in Figure 6.

The MNIST and CIFAR-10 datasets were used and were pre-processed using standard techniques.
Throughout these experiments mini-batches of 128 observations were used.

Variance depth scales: The experiments depicted in Figures 6 (a) and (d) are interested in testing the
numerical stability of networks initialised using different σ2
w values while using 32-bit ﬂoating point

16

|cl − c∗| ≈ e−l/ξc

∴ ln (cid:0)|cl − c∗|(cid:1) ≈

−l
ξc

.

∴ ln (cid:0)|cl − c∗|(cid:1) ≈ al + b,

∴ al + b ≈

−l
ξc

,

∴ ξc ≈

−l
al + b

.

−l
al

lim
l→∞

ξc ≈ lim
l→∞
1
a

≈ −

.

numbers. To test the depth of stable variance propagation, a network with 1000 hidden layers is used.
The network used in this experiment makes use of dropout with p = 0.6, where p is the probability
of keeping a neuron’s value, thus the critical value for σ2
w is 1.2. As such, a linearly spaced range of
σ2
w ∈ [0.1, 2.5] is used to select 25 different values.

We use the following approach to predict the depth beyond which variances become numerically
unstable. At criticality for multiplicative noise (σw, σb) = ((cid:112)2/µ2, 0), however, for weights
initialised off this critical point (18) becomes

˜ql = ˜ql−1

(cid:20)
˜ql−2

=

(cid:19)

(cid:18) σ2
wµ2
2
(cid:18) σ2
wµ2
2
(cid:19)l

.

= ˜q0

(cid:18) σ2
wµ2
2

(cid:19)

(cid:19)(cid:21) (cid:18) σ2
wµ2
2

(40)

(41)

w > 2
µ2

If σ2
, we let ˜ql = K, where K is the largest positive number representable by the computer. In
our case, using 32-bit ﬂoating point precision, this number is equal to 3.4028235 × 1038. Otherwise,
we select K = 1.1754944 × 10−38, the smallest possible positive number. Furthermore,
if σ2
let L∗ represent the layer l in (40) at which the value K is reached, then we can scale our input data
such that ˜q0 = 1 and solve for L∗ to ﬁnd

w < 2
µ2

L∗ = ln(K)/ln

(cid:18) σ2
wµ2
2

(cid:19)

.

Therefore, we expect numerical instability issues to occur beyond a depth of L∗.

Trainable depth scales: The experiments depicted in Figures 6 (b), (c), (e) and (f) are concerned
with determining at what depth a critically initialised network with a speciﬁed dropout rate can train
effectively. To this end, 10 linearly spaced values for dropout on the range p ∈ [0.1, 1.0] and 10
linearly spaced network depths on the integer range l ∈ [2, 40] are tested.

The task presented to the network in this experiment is to learn the identity function within 200
epochs. As such, the network is set up as an auto-encoder and uses stochastic gradient decent with a
learning rate of 10−3. The input data is divided into a training and validation set, each containing
50000 and 10000 observations respectively.

C.5 Additional results

In this section we provide some additional experiments on the training dynamics of deep noisy ReLU
networks from different initialisations.

In Figure 7 we compare the standard “He” initialisation (blue) with the critical initialisation (green)
for a ReLU network with dropout regularisation (p = 0.8). By not initialising at criticality due to
dropout noise, the variance map for the “He” strategy no longer lies on the identity line in (a) and as
a result, the forward propagating signal can be seen to explode in (b). However, by compensating for
the amount of injected noise, the above derived critical initialisation for dropout preserves the signal
throughout the entire forward pass, with roughly constant variance dynamics.

Next, we provide some additional experiments on the trainability of deep ReLU networks with
dropout on real-world data sets.

From our analysis in the paper, we expect that as the depth of the network increases, any initialisation
strategy that does not factor in the effects of noise, will cause the forward propagating signal to
become increasingly unstable. For very deep networks, this might cause the signal to either explode
or vanish, even within the ﬁrst forward pass, making the network untrainable.

To test this, we trained a denoising autoencoder network with dropout noise (p = 0.6) on MNIST and
CIFAR-10 using squared reconstruction loss. We consider several network depths (L = 30, 100, 200),
learning rates (α = 0.1, 0.01, 0.001, 0.0001) and optimisation procedures (SGD and Adam), with
1000 neurons in each layer. The results for training on CIFAR-10 are shown in Figure 8 for both the
“He” intialisation (blue) and the critical dropout initialisation (green). (For MNIST, see Figure 9; the

17

Figure 7: Critical initialisation for ReLU networks with dropout. Lines correspond to theoretical
predictions and points to numerical simulations, for random ReLU networks with dropout (p = 0.8),
initialised according to the method proposed by He et al. (2015) (blue) and at criticality (green).
(a): Iterative variance map where the identity line is displayed as a dashed black line. (b): Variance
dynamics during forward signal propagation.

Figure 8: Comparing the “He” initialisation strategy to critical dropout initialisation for ReLU
networks using dropout (p = 0.6) on CIFAR-10. While networks initialised at criticality (green) are
able to train at large depths (L = 200) as seen in the bottom row, networks initialised with the “He”
strategy (blue) become untrainable irrespective of the chosen learning rate or optimisation procedure.
An “X” marks the point at which a network completely stopped training. Training losses and number
of network updates are shown in log-scale.

core trends and resulting conclusions regarding network trainability is the same for both data sets,
which we discuss below.)

As the depth increases, moving from the top to the bottom row in Figure 8, networks initialised at
the critical point for dropout seem to remain trainable even up to a depth of 200 layers (we see the
loss start to decrease over ﬁve epochs). In contrast, networks using the “He” initialisation become
increasingly more difﬁcult to train, with no training taking place at very large depths. These ﬁndings
make sense in terms of the variance dynamics analysed in the paper, however, these experimental
successes seem to run counter to our theoretical analysis of trainable depth scales (this contradiction
can also be seen in Figure 6). Understanding this discrepancy is of particular interest to us.

To verify that the lack of training in Figure 8 is due to poor signal propagation, we plot the empirical
variance of the pre-activations in Figure 10, for the ﬁrst forward pass of a 200 layer autoencoder

18

Figure 9: Comparing the “He” initialisation strategy to critical dropout initialisation for ReLU
networks using dropout (p = 0.6) on MNIST. While networks initialised at criticality (green) are
able to train at large depths (L = 200) as seen in the bottom row, networks initialised with the “He”
strategy (blue) become untrainable irrespective of the chosen learning rate or optimisation procedure.
An “X” marks the point at which a network completely stopped training. Training losses and number
of network updates are shown in log-scale.

Figure 10: Variance dynamics for signal propagation in the ﬁrst forward pass for a 200 layer
autoencoder network fed a batch of 500 training examples from CIFAR-10. (a) Exploding activation
variance (blue) reaching overﬂow levels (marked with a red “X”) for the “He” intialisation, with no
signal reaching the output layer (shown in log-scale). (b) Zoomed in display of the roughly constant
variance dynamics in (a) for the critical dropout initialisation.

network. For the “He” initialisation, the variance in (a) grows rapidly to the point of causing numerical
instability and overﬂow (indicated by the red dashed line), well before any signal is able to reach the
output layer. However as shown in (b), by initialising at criticality, signal is able to propagate reliably
even at large depths.

19

Figure 11: Depth scale experiments on MNIST and CIFAR-10. (a) Depth scales ﬁt to the training loss
on MNIST for networks initialised at criticality for dropout rates p = 0.1 (severe dropout) to p = 1
(no dropout). (b) Depth scales ﬁt to the validation loss on MNIST. (c) - (d): Similar to (a) - (c), but
for CIFAR-10.

20

