Model-Based Policy Search for Automatic Tuning of
Multivariate PID Controllers

Andreas Doerr1,2, Duy Nguyen-Tuong1, Alonso Marco2, Stefan Schaal2,3, Sebastian Trimpe2

7
1
0
2
 
r
a

M
 
8
 
 
]

G
L
.
s
c
[
 
 
1
v
9
9
8
2
0
.
3
0
7
1
:
v
i
X
r
a

Abstract— PID control architectures are widely used in indus-
trial applications. Despite their low number of open parameters,
tuning multiple, coupled PID controllers can become tedious
in practice. In this paper, we extend PILCO, a model-based
policy search framework, to automatically tune multivariate
PID controllers purely based on data observed on an otherwise
unknown system. The system’s state is extended appropriately
to frame the PID policy as a static state feedback policy. This
renders PID tuning possible as the solution of a ﬁnite horizon
optimal control problem without further a priori knowledge.
The framework is applied to the task of balancing an inverted
pendulum on a seven degree-of-freedom robotic arm, thereby
demonstrating its capabilities of fast and data-efﬁcient policy
learning, even on complex real world problems.

I. INTRODUCTION

Proportional, Integral and Derivative (PID) control struc-
tures are still the main control tool being used in industrial
applications, in particular in the process industry [1], but
also in automotive applications [2] and in low-level control
in robotics [3]. The large share of PID controlled applications
is mainly due to the past record of success,
the wide
availability, and the simplicity in use of this technique.

In practice, control design is still often achieved by tedious
manual tuning or by heuristic PID tuning rules [4]. More
advanced tuning concepts are most frequently developed
for Single-Input-Single-Output (SISO) systems [5], [6]. For
Multi-Input-Multi-Output (MIMO) systems, popular tuning
methods, such as biggest log-modulus and the dominant pole
placement tuning method [7], strive to tune each control loop
individually, followed by a collective de-tuning to stabilize
the multi-loop system. These tuning methods, however, rely
on linear process models and require stable processes. For
general PID control structures where multiple controllers act
on each input, controller design is usually conducted by
decoupling the process, subsequently allowing the design of
individual SISO PIDs. One example are online adjusted pre-
compensators, which decouple the process transfer function
matrix [8].

In this paper, we extend Probabilistic Inference for Learn-
ing COntrol (PILCO) [9], a framework for iteratively im-

*This research was supported in part by Robert Bosch GmbH,
the
Max Planck Society, National Science Foundation grants IIS-1205249, IIS-
1017134, EECS-0926052, the Ofﬁce of Naval Research, and the Okawa
Foundation.

1Bosch Center for Artiﬁcial Intelligence, Renningen, Germany.

{andreas.doerr3, duy.nguyen-tuong}@de.bosch.com

2Autonomous Motion Department at the Max Planck Institute for Intel-

ligent Systems, T¨ubingen, Germany.
{amarco, strimpe}@tue.mpg.de, sschaal@is.mpg.de

3Computational Learning and Motor Control lab at the University of

Southern California, Los Angeles, CA, USA.

Fig. 1. Humanoid upper-body robot, Apollo, balancing an inverted pendu-
lum. Using the proposed framework, coupled PID and PD controllers are
trained to stabilize the pole in the central, upright position without requiring
prior knowledge of the pendulum system dynamics.

proving a controller based on its expected ﬁnite horizon cost
as predicted from a learned system model. This model is
learned in a fully Bayesian setting using Gaussian Process
Regression (GPR) as a non-parametric function estimator
[10]. Gaussian Processes (GPs) as probability distributions
over a function space do not only provide a prediction but
also an uncertainty measure of the process dynamics. The
uncertainty component can be utilized to implement cautious
control [11] or trigger further exploration to improve the
model knowledge [12]. As it is based on the full nonlinear
system model, this framework takes into account all process
couplings for controller tuning. In contrast to some of the
aforementioned PID tuning concepts, e.g. SISO or multi-
loop tuning methods, this framework imposes no structural
restrictions on the multivariate PID control design.

PILCO has been successfully applied to reinforcement
learning tasks, such as inverted pendulum swing-up [9],
control of low-cost manipulators [13], and real-world appli-
cations like throttle valve control [14]. These examples focus
mostly on nonlinear state feedback policies. In industrial
applications, however, it is desirable to obtain interpretable
control designs, which is the case for PID control structures
rather than for arbitrarily complex nonlinear control designs.
Contribution of the paper: We propose a general frame-
work for multivariate PID controller tuning based on PILCO.
In particular,
the developed framework (i) is applicable
to nonlinear MIMO systems with arbitrary couplings; (ii)
allows for the tuning of arbitrary multivariate PID structures;

2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including

Accepted ﬁnal version. To appear in 2017 IEEE International Conference on Robotics and Automation.
c
(cid:13)
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of
any copyrighted component of this work in other works.

π(xt; θ)

x0, z0 (15)

xt, zt

(10)

˜zt

(11)

ut

GP ˆf

xt+1

zt+1

(14)

iterate for t

[0, H]

∈

Fig. 2. Computations for one time step in the long-term prediction. Black:
original PILCO state propagation steps. Red: Augmented state propagation
to accommodate PID policy optimization.

(iii) involves a probabilistic treatment of recorded system
data to iteratively improve control performance without a
priori knowledge; and (iv) requires no process model. The
auto-tuning method is demonstrated in pole balancing ex-
periments on Apollo, a complex robot platform as shown in
Fig. 1, coping with imperfect low-level tracking controllers
and unobserved dynamics.

Outline of the paper: The paper continues with an intro-
duction of the policy search problem and PILCO in Sec. II.
The proposed multivariate PID tuning framework is then de-
veloped in Sec. III. Section IV presents the results of applying
the framework for tuning coupled PID controllers on Apollo.
The paper concludes with remarks and propositions for future
work in Sec. V.

II. PROBABILISTIC INFERENCE FOR CONTROL

In this section, we introduce the policy search problem
and notation. Subsequently, the main ideas of PILCO [9],
[15] are brieﬂy discussed.

A. Problem Statement

We consider discrete time dynamic systems of the form

xt+1 = f (xt, ut) + (cid:15)t ,
(1)
with continuously valued state xt ∈ RD and input ut ∈
RF . The system dynamics f is not known a priori. We
assume a fully measurable state, which is corrupted by zero-
mean independent and identically distributed (i.i.d.) Gaussian
noise, i.e. (cid:15)t ∼ N (0, Σ(cid:15)).

One speciﬁc reinforcement learning formulation aims at

minimizing the expected cost-to-go given by

J =

E[c(xt, ut; t)],

(2)

T
(cid:88)

t=0

where
cost
an immediate, possibly time dependent
c(xt, ut; t) penalizes undesired system behavior. Policy
search methods optimize the expected cost-to-go J by se-
lecting the best out of a range of policies ut = π(xt; θ)
parametrized by θ. Particularly in model-based policy search
frameworks, a model ˆf of the system dynamics f is utilized
to predict the system behavior and to optimize the policy.

B. PILCO

PILCO as a speciﬁc model-based policy search frame-
work emphasizes data-efﬁciency and consistent handling of
uncertainty when constructing the system dynamics model
ˆf . To incorporate all available data from policy rollouts

(experiments) on the actual system, a Gaussian Process
(GP) [16] is utilized as a non-parametric, probabilistic model.
The PILCO framework is outlined in Alg. 1. In the inner
loop, a simulated rollout is conducted based on the dynamics
model ˆf and the current policy π(xt; θ). The system’s
state xt is propagated over a ﬁnite prediction horizon H
the system’s initial state x0 ∼ N (µ0, Σ0) as
starting at
visualized for one time step in Fig. 2 in black for the standard
PILCO rollout. The posterior distribution p(xt+1|xt, ut) is
approximated in each time step by a Gaussian distribution
using moment matching [17] yielding an approximately
Gaussian marginal distribution of the long-term predictions
p(x0), . . . , p(xH ). The expected long-term cost (2) of this
rollout as well as its gradients with respect to the policy pa-
rameters θ can be computed analytically. Policy optimization
can then be conducted based on this prediction method using
standard gradient-based optimization techniques.
Starting with an initial random policy1,

the algorithm
optimizes the cost-to-go by repeatedly executing the policy
thereby gathering new data and building
on the system,
the dynamics model, subsequently improving the policy
iteratively, until the task has been learned.

Algorithm 1 PILCO

record {xt, ut}t=1...T

1: Experiment: Execute random policy;
2:
3: Train initial GP dynamics model xt+1 = ˆf (xt, ut)
4: repeat
5:
6:
7:
8:

Simulation: Predict J(θ) given ˆf , π(xt; θ)
Analytically compute gradient dJ(θ)/dθ
Gradient-based policy update (e.g. CG, BFGS)

repeat

9:
10:

until convergence: θ∗ = arg min J(θ)
Experiment: Execute π(xt; θ∗); record {xt, ut}t=1...T

Update dynamics model ˆf using all recorded data

11:
12: until task learned
13: return π∗(xt; θ∗)

III. POLICY SEARCH FOR PID CONTROL

We ﬁrst introduce the general description of the considered
PID control structure in Sec. III-A; the necessary modiﬁca-
tions to the policy search framework are derived in Sec. III-
B to Sec. III-D, followed by an analytic derivation of the
required cost function gradients in Sec. III-E.

A. PID Control Policies

The control output of a scalar PID controller is given by

ut = Kpet + Ki

e(τ )dτ + Kd ˙et

(cid:90) t
∆T
·

0

et = xdes,t − xt ,

(3)

(4)

1A random policy might be a parametrized policy with randomly chosen
parameters (cf. [18]) or a random input signal exciting the system to generate
initial dynamics data.

e1

e2

u1

u2

PID1

PID2

Sys 1

e1

e2

PID1

PID2

u

+

Sys 2

1) Desired Goal State: The desired set-point or target
trajectory state2 xdes,t ∼ N (µdes,t, Σdes,t) is independent of
zt yielding:

Fig. 3. Possible PID structures, exempliﬁed with two controllers. Left:
Individual PID controllers acting on different system inputs. Right: Combi-
nation of PID controllers acting on the same system input.

(cid:21)

(cid:20) zt
xdes,t

∼ N

(cid:21)

(cid:18)(cid:20) µz
µdes,t

,

(cid:20)Σz

0 Σdes,t

(cid:21)(cid:19)

0

.

where e(τ ) denotes the continuous error signal. The current
desired state xdes,t can be either a constant set-point or a
time-variable goal trajectory. This controller is agnostic to
the system dynamics and depends only on the system’s error.
Each controller is parametrized by its proportional, integral
and derivative gain (θPID = (Kp, Ki, Kd)).

A general PID control structure C(s) for MIMO processes
(1) can be described in transfer function notation by a D ×F
transfer function matrix

C(s) =






c11(s)
...
cF 1(s)

· · ·
. . .
· · ·




 ,

c1D(s)
...
cF D(s)

(5)

where s denotes the complex Laplace variable and cij(s)
are of PID type. The multivariate error is given by et =
xdes,t − xt ∈ RD such that the multivariate input becomes
u(s) = C(s)e(s). PID literature typically distinguishes be-
tween tuning methods for multi-loop PID control and mul-
tivariable PID control systems. The former have a diagonal
transfer function matrix C(s) whereas the latter allows PID
controllers on all elements of C(s), i.e. all combinations of
errors and inputs can be controlled, thus allowing additional
cross couplings.

With this framework, we address the general class of
multivariable PID control systems with no restrictions on
the elements of C(s). Examples for possible PID structures
are shown in Fig. 3.

B. System State Augmentation

We present a sequence of state augmentations such that
any multivariable PID controller (5) can be represented as
a parametrized static state feedback law. A visualization of
the state augmentation integrated into the one-step-ahead
prediction is shown in red in Fig. 2 in comparison with
the standard PILCO setting (in black). Given a Gaussian
distributed initial state x0, the resulting predicted states will
remain Gaussian for the presented augmentations.

To obtain the required error states for each controller (3),
we deﬁne a new system state zt that keeps track of the error
at the previous time step and the accumulated error,

zt := (xt, et

1, ∆T

eτ ) ,

−

(6)

t
1
(cid:88)
−

τ =0

where ∆T is the system’s sampling time. For simplicity,
we denote vectors as tuples (v1, . . . , vn), where vi may be
vectors themselves. The following augmentations are made
to obtain the necessary policy inputs:

(7)

(8)

(9)

2) Error States: The current error is a linear function of
zt and xdes,t. The current error derivative and integrated error
are approximated by

˙et ≈

1

−

,

et − et
∆T
t
1
(cid:88)
−

τ =0

e(τ )dτ ≈ ∆T

eτ + ∆T et .

(cid:90) t
∆T
·

0

Both approximations are linear transformations of the aug-
mented state. The resulting augmented state distribution re-
mains Gaussian as it is a linear transformation of a Gaussian
random variable (see Appendix).

The computation of the derivative error is prone to mea-
surement noise. Yet, this framework can readily be extended
to incorporate a low-pass ﬁltered error derivative, which we
omit for notational simplicity. In this case, additional historic
error states would be added to the state zt to provide the input
for a low-pass Finite Impulse Response (FIR) ﬁlter.

The fully augmented state is given by

˜zt := (zt, xdes,t, et, ∆T

eτ ,

t
(cid:88)

τ =0

et − et
∆T

1

−

) .

(10)

C. PID as Static State Feedback

Based on the augmented state ˜zt, the PID control policy
for multivariate controllers can be expressed as a static state
feedback policy:

ut = APID(˜z(3)

, ˜z(4)
t

t
(cid:32)

= APID

et, ∆T

, ˜z(5)
t )
t
(cid:88)

et,

τ =0

et − et
∆T

1

−

(cid:33)

,

(11)

where ˜z(i)
indicates the i-th term of (10). The speciﬁc
t
structure of the multivariate PID control law is deﬁned by the
parameters in APID. For example, PID structures as shown
in Fig. 3 would be represented by

(cid:20)Kp,1

0 Ki,1

0 Kd,1

Aleft =
0 Kd,2
Aright = (cid:2)Kp,1 Kp,2 Ki,1 Ki,2 Kd,1 Kd,2

0 Kp,2

0 Ki,2

,
(cid:3) .

(cid:21)

0

(12)

(13)

D. State Propagation

Given the Gaussian distributed state and control input
as derived in Sec. III-B and Sec. III-C,
the next system
state is computed using the GP dynamics model ˆf . PILCO
approximates the predictive distribution p(xt+1) by a Gaus-
sian distribution using exact moment matching. From the

2Drawing the desired state from a Gaussian distribution yields better

generalization to unseen targets as discussed in [13].

dynamics model output xt+1 and the current error stored in
˜zt, the next state is obtained as

zt+1 = (xt+1, ˜z(3)

, ˜z(4)

t

t ) = (xt+1, et, ∆T

eτ ) .

(14)

t
(cid:88)

τ =0

Iterating (6) to (14), the long-term prediction can be com-
puted over the prediction horizon H as shown in Fig. 2. For
the initial state, we deﬁne

z0 := (x0, xdes,0 − x0, 0) .

(15)

E. Cost Function Derivatives

Given the presented augmentation and propagation steps,
the expected cost gradient can be computed analytically such
that the policy can be efﬁciently optimized using gradient-
based methods. We summarize the high-level policy gradient
derivation steps to point out the modiﬁcations to standard
PILCO that are necessary to allow PID policy optimization.
The expected cost3 derivative is obtained as

dJ(θ)
dθ

=

H
(cid:88)

t=1

d
dθ

Ezt[c(zt)]
(cid:123)(cid:122)
(cid:125)
(cid:124)
=:
E

t

=

T
(cid:88)

t=1

dEt
dp(zt)

dp(zt)
dθ

.

(16)

To simplify the notation, we write dp(zt) to denote the sufﬁ-
cient statistics derivatives dµt and dΣt of a Gaussian random
variable p(zt) ∼ N (µt, Σt) (analogous to the treatment in
[15]). The gradient of the immediate loss with respect to
the state distribution, dEt/dp(zt), is readily available for
most standard cost functions like quadratic or saturated
exponential terms and Gaussian input distributions (cf. [13]).
The gradient for each predicted state in the long-term rollout
is obtained by applying the chain rule to (14) resulting in

dp(zt+1)
dθ

=

δp(zt+1)
δp(˜zt)

dp(˜zt)
dθ

+

δp(zt+1)
δp(xt+1)

dp(xt+1)
dθ

. (17)

The derivatives highlighted in blue are computed for the
linear transformation in (14) according to the general rules
for linear transformations on Gaussian random variables
as summarized in the appendix. Based on the dynamics
model prediction as detailed in Sec. III-D, the gradient of
the dynamics model output xt+1 is given by
δp(xt+1)
δp(ut)

δp(xt+1)
δp(˜zt)

dp(xt+1)
dθ

dp(ut)
dθ

dp(˜zt)
dθ

(18)

=

+

.

The derivatives shown in red can be computed analytically
for the speciﬁc dynamics model [17]. Applying the chain
rule for the policy output p(ut) obtained by (11) yields

dp(ut)
dθ

=

δp(ut)
δp(˜zt)

dp(˜zt)
dθ

+

δp(ut)
δθ

,

Fig. 4. Comparison of commanded (blue) and executed (red) end effector
acceleration for a policy rollout on Apollo. Additionally to the tracking
errors introduced by the joint level controllers, the system shows further
deterioration in acceleration tracking due to errors in the inverse dynamics
model and due to noise and delays.

Again, the part marked in blue is computed for the linear
transformation (10). Starting from the initial state where
dp(z0)/dθ = 0, we obtain the gradients for all states with
respect to the policy parameters dp(zt)/dθ by iteratively
applying (17) to (20) for all time steps t.

IV. EXPERIMENTAL EVALUATION

To demonstrate the capabilities of the presented frame-
work to automatically tune coupled PID controllers without
prior system knowledge and in a data-efﬁcient fashion, we
consider the problem of balancing an inverted pendulum on
the Apollo robot as shown in Fig. 1. The inverted pendulum
is a well-known benchmark in the control and reinforcement
learning communities [19], [20]. Demonstrations of the iter-
ative learning process and the resulting optimized policy can
be found in the supplementary video material.

A. Experimental Setup

We employ an imperfect

inverse dynamics model of
Apollo’s seven Degree-of-Freedom (DoF) robotic arm to
compute the joint torques necessary to track the desired end
effector acceleration ut [21]. Technical details concerning the
hardware platform can be found in [22], where reinforcement
learning on this platform has been addressed using Bayesian
Optimization techniques. The state of the system xt com-
prises the end effector position x, velocity ˙x, pendulum angle
φ, and angular velocity ˙φ.

During the rollouts, the commanded acceleration is limited
to umax = 3 m/s2 for safety reasons. Test policies are executed
for 20 s or interrupted once safety limits (xmax = 0.3 m,
θmax = 30◦) are violated. The control signal is computed at
100 Hz and low-pass ﬁltered by a second order Butterworth
ﬁlter having a cut-off frequency of 20 Hz.

The policy is optimized on a prediction horizon of T =

10 s based on a saturated loss function [9] given by

(19)

c(et, ¯ut) = 1 − exp(−(eT

t Qet + ¯uT

t R¯ut)/2) .

(21)

The derivatives marked in blue are introduced by the linear
control law (11) and can be computed as summarized in the
appendix. The gradient of the augmented state is given by

dp(˜zt)
dθ

=

dp(˜zt)
dp(zt)

dp(zt)
dθ

(20)

3We only address cost on the state zt for simplicity. For practical
implementations, cost on u can be included by adding past inputs into
the system state as shown in Sec. IV.

For balancing the pendulum in the upright position the
desired trajectory is given by xdes,t = 0. Weights are set to
Q = diag(1/0.22, 1/0.022) for end effector and pendulum
position error, and to R = 1/0.42 for the control input.
The selected cost function saturates quickly for x > w if
x is weighted by 1/w2. End effector position and input are
therefore only penalized lightly, which permits higher gains
while the pendulum angle is stabilized as it is penalized much
stronger.

B. PID Control Structure Setup

We employ a PID controller on the position error ex =
xdes,t−x and a PD controller acting on the pendulum angular
error eθ = θdes,t − θ. The resulting control structure is shown
in the right plot of Fig. 3. The PID/PD control structure
with integral control on the end effector position serves to
correct for any static bias in the angle measurement (e.g.,
from imperfect calibration) as is explained in [23, p. 67].
This structure has successfully been used for other balancing
problems [22], [23].

The speciﬁc structure is chosen based on prior knowledge
about the problem at hand as detailed in [22]. The integrator’s
contribution is required to counteract any pendulum angle
measurement bias introduced by imperfect sensor calibra-
tion. The policy parametrization is therefore given by θ =
(Kp,x, Ki,x, Kd,x, Kp,θ, Kd,θ). Assuming no prior knowledge,
we initialize the policy to zero. Both controllers are coupled
by the system dynamics and can therefore not be tuned
independently, which makes the inverted pendulum a well
suited benchmark for multivariate PID controller tuning.

C. Modiﬁcations for Dynamics Model Learning

When ﬁrst training GP dynamics models xt+1 = ˆf (xt, ut)
in the standard way (Sec. II), this did not lead to acceptable
models for long-term predictions and thus successful con-
troller learning. The problems are caused by imperfections
in the inverse dynamics model, joint friction and stiction,
as well as sensor and actuator delay. These factors add
unobserved states and therefore additional dynamics to the
system, corrupting the measured data. This is visible in
Fig. 4, where the desired acceleration and the numerically
computed actual acceleration are visualized for a policy
rollout on the robot. Several adaptations to the standard GP
dynamics model learning framework were required to obtain
a good prediction model, which we explain next.

In contrast

1) Gaussian Process Setup:

to the model
in (1), we train the GP models to predict the difference
between the current and the next state ∆xt = xt+1 − xt.
We compute a sparse GP using Snelson’s approximation [24]
having a covariance parametrized by 400 inducing pseudo-
input points. For both GP models, hyperparameters θGP,i =
(l1, l2, σf, σn) including lengthscales for each dimension,
as well as signal and noise variance have to be chosen.
We chose the maximum likelihood estimate (MLE) on the
basis of the previously gathered system data to compute the
hyperparameters. These hyperparameters are kept constant
during the iterative policy learning.

2) NARX Dynamics Model: Instead of modeling the sys-
tem’s full, four dimensional state and its dynamics, two
independent GPs are trained to model the dynamics of the
measured state parts; the end effector and pendulum position.
The missing information about the system’s velocities and
potential latent states is recovered by employing a Nonlinear
AutoRegressive eXogenous model (NARX) [25] of the form

xt+1 = ˆf (xt, . . . , xt

n, ut, . . . , ut

m) .

−

−

(22)

Fig. 5. Expected cost-to-go optimization results. Left: Iterative improve-
ment in predicted loss (blue) compared to the cost observed in a single
robot experiment (red). Right: Optimization results for each iteration.

Information on latent states is implicitly encoded in the
measured historic states and inputs. Different
lengths of
history might be required for individual parts of the system’s
state depending on the dynamics’ time scales. We optimize
the number of historic states individually for end effector
position, pendulum position and control input. The NSGA
II optimizer [26] is employed to compute the pareto front
of the model’s prediction error (using the same dataset as
previously used for hyperparameter tuning) and the size of
the NARX history. For our problem, we ended up with
a new state of dimensionality 14 given by xNARX :=
(xt, . . . , xt
6). The hidden dy-
namics between commanded input and executed acceleration
(cf. Fig. 4) requires a longer history in the input state to
capture all relevant effects.

3, φt, . . . , φt

2, ut, . . . , ut

−

−

−

3) Data Preprocessing: The recorded data is downsam-
pled to 25 Hz and non-causally low-pass ﬁltered with a 2nd
order Butterworth ﬁlter and cut-off frequency 12.5 Hz. The
downsampled control input is obtained by averaging the input
signal on each sampling interval.

D. PID Learning Results on Apollo

The iterative learning experiment is visualized in Fig. 5. To
gather initial data about the system, four random rollouts are
conducted, applying a white noise input urnd,t ∼ N (0, 1 m/s2)
to the system. The ﬁrst iteration is based on the dynamics
model learned from these random rollouts. The left ﬁgure
visualizes the optimized predicted loss for each iteration in
comparison to the actual loss obtained from evaluating (21)
on one sample rollout of the actual system. The drop of the
predicted loss in iteration 5 shows that by that time, sufﬁcient
data about the system dynamics has been gathered to ﬁnd
a stabilizing policy parametrization. In the right ﬁgure, the
predicted loss optimization is visualized for each iteration
individually, as a function of the number of linesearches
conducted by the BFGS optimizer.

In Fig. 6, we visualize the dynamics learning progress,
showing predicted system behavior and actual rollout for
the dynamics models and policies as obtained at the ﬁrst,
intermediate and ﬁnal stage of the iterative learning process.
The iterative improvement in model prediction accuracy and
the improvement in the PID policy is clearly visible. The ﬁnal

(a) Iteration 1

(b) Iteration 3

(c) Iteration 7

Fig. 6. Predicted system behavior (dashed lines, error-bars indicate 95 % conﬁdence intervals) and experimental results (solid lines) visualized for the ﬁrst
(a), an intermediate (b) and the ﬁnal iteration (c) of the policy learning process. The end effector position (blue) and the angular position (red) are shown.

Appropriately dealing with hidden and low-level dynam-
ics, problems found in almost all real-world applications,
were major hurdles in the experimental application. In par-
ticular, we found that learning of dynamics models geared
towards long-term predictions is key to successful ﬁnite
horizon policy optimization, but
largely unaddressed by
current approaches. The presented dynamics model structure
and learning framework helps to alleviate this problem.
Principled ways for improving the learning of long-term
prediction models shall be addressed in future work.

APPENDIX

A linear transformation of a Gaussian random variable

X ∼ N (µX , ΣX ) ∈ RD is given by

Y = AX + b = N (µY , ΣY ) ∈ RP ,

(23)

D and b ∈ RP

where A ∈ RP
×
×
distribution of X and Y is given by
(cid:20) ΣX
CT ΣT

(cid:20)X
Y

(cid:20)µX
µY

∼ N (

(cid:21)

(cid:21)

,

1. The joint probability

ΣX C

(cid:21)
)

X ΣY

(24)

µY = AµX + b, ΣY = AΣX AT , C = AT

(25)

The non-zero partial derivatives of Y ’s sufﬁcient statistics

are given by

= A ∈ RP

D,

×

= A ⊗ A ∈ RP 2

×

D2

= µT

X ⊗ I ∈ RP

DP ,

×

= I ∈ RP

P

×

= δilδkj,

∈ RP D

×

D2

,

∈ RP 2

×

P D

δ(ΣY )
δA

δC
δA

= δlj(AT ΣX )ki + δkj(ΣX A)il

where ⊗ is the Kronecker product and δij is the Kronecker
delta (cf. [27] for useful matrix derivatives).

(26)

(27)

(28)

(29)

where

δµY
δµX

δµY
δA

δCkl
δAij

δ(ΣY )kl
δAij

δΣY
δΣX

δµY
δb

Fig. 7. Disturbance rejection of the optimized PID policy. End effector
position (blue) and pendulum angle (red) display the closed loop response
of the optimized PID policy to manually introduced disturbances.

dynamics model is accurately predicting the stabilization of
the system by the optimized policy.

In this example, the total interaction time with the physical
system is only 106 seconds, demonstrating fast and data-
efﬁcient learning. This model-based method outperforms a
model-free, Bayesian optimization method (cf. [22]), with
respect
to the number of rollouts on the actual system.
The policy optimization itself is carried out ofﬂine, and the
predicted system behavior can be utilized to set appropriate
safety boundaries to test new controllers without damaging
the system.

To demonstrate the robustness of the learned policy, the
system is manually deﬂected. Disturbances in pendulum
angle (cf. Fig. 7) and end effector position are dispelled fast
and without overshoot. By commanding a non-zero desired
trajectory, the learned PID controller can be utilized for
tracking tasks as demonstrated in the supplementary video
for a sinusoidal end-effector trajectory.

V. CONCLUSION

The proposed framework for multivariate PID tuning is
ﬂexible with respect to possible PID control structures and
process dynamics. In particular, it is able to cope with general
nonlinear MIMO processes and multivariate PID structures.
The presented framework can readily be extended to cas-
caded PID structures and tracking controllers by considering
multiple different [18] and time-varying goal states.

REFERENCES

[1] P. Cominos and N. Munro, “PID controllers: Recent tuning methods
and design to speciﬁcation,” IET Proceedings on Control Theory and
Applications, vol. 149, no. 1, pp. 46–53, 2002.

[2] F. Jiang and Z. Gao, “An application of nonlinear PID control to a
class of truck ABS problems,” in IEEE Conference on Decision and
Control, vol. 1, 2001, pp. 516–521.

[3] B. Siciliano, L. Sciavicco, L. Villani, and G. Oriolo, Robotics: mod-
Springer Science & Business Media,

elling, planning and control.
2010.

[4] A. O’Dwyer, Handbook of PI and PID controller tuning rules. World

Scientiﬁc, 2009, vol. 57.

[5] K. J.

˚Astr¨om and T. H¨agglund, Advanced PID control.
Instrumentation, Systems and Automation Society, 2006.

ISA-The

[6] J. Berner, T. H¨agglund, and K. J.

˚Astr¨om, “Asymmetric relay au-
totuning - practical features for industrial use,” Control Engineering
Practice, vol. 54, pp. 231–245, 9 2016.

[7] M. A. Johnson and M. H. Moradi, PID control. Springer, 2005.
[8] T. Yamamoto and S. Shah, “Design and experimental evaluation of a
multivariable self-tuning PID controller,” IET Proceedings on Control
Theory and Applications, vol. 151, no. 5, pp. 645–652, 2004.

[9] M. Deisenroth and C. E. Rasmussen, “PILCO: A model-based and
data-efﬁcient approach to policy search,” in Proceedings of the Inter-
national Conference on Machine Learning (ICML), 2011, pp. 465–
472.

[10] J. Kocijan, A. Girard, B. Banko, and R. Murray-Smith, “Dynamic
systems identiﬁcation with Gaussian processes,” Mathematical and
Computer Modelling of Dynamical Systems, vol. 11, no. 4, pp. 411–
424, 2005.

[11] R. Murray-Smith, D. Sbarbaro, C. E. Rasmussen, and A. Girard,
“Adaptive, cautious, predictive control with Gaussian process priors,”
13th IFAC Symposium on System Identiﬁcation, pp. 1195–1200, 2003.
[12] R. Murray-Smith and D. Sbarbaro, “Nonlinear adaptive control using
non-parametric Gaussian process prior models,” 15th Triennial World
Congress of the International Federation of Automatic Control (IFAC),
2002.

[13] M. P. Deisenroth, C. E. Rasmussen, and D. Fox, Learning to control
a low-cost manipulator using data-efﬁcient reinforcement learning.
MIT Press Journals, 2011, vol. 7, pp. 57–64.

[14] B. Bischoff, D. Nguyen-Tuong, T. Koller, H. Markert, and A. Knoll,
“Learning throttle valve control using policy search,” in Machine

Learning and Knowledge Discovery in Databases.
pp. 49–64.

Springer, 2013,

[15] M. P. Deisenroth, D. Fox, and C. E. Rasmussen, “Gaussian processes
for data-efﬁcient learning in robotics and control,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 37, no. 2, pp. 408–
423, 2015.

[16] C. K. Williams and C. E. Rasmussen, Gaussian processes for machine

learning. MIT Press, 2005.

[17] J. Q. Candela, A. Girard, J. Larsen, and C. E. Rasmussen, “Propagation
of uncertainty in Bayesian kernel models-application to multiple-
step ahead forecasting,” in Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP),
vol. 2, 2003, pp. II–701.

[18] M. P. Deisenroth and D. Fox, “Multiple-target reinforcement learning
with a single policy,” in ICML Workshop on Planning and Acting with
Uncertain Models. Citeseer, 2011.

[19] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press Cambridge, 1998, vol. 1, no. 1.

[20] C. W. Anderson, “Learning to control an inverted pendulum using
neural networks,” IEEE Control Systems Magazine, vol. 9, no. 3, pp.
31–37, 1989.

[21] L. Righetti, M. Kalakrishnan, P. Pastor, J. Binney, J. Kelly, R. C.
Voorhies, G. S. Sukhatme, and S. Schaal, “An autonomous manipu-
lation system based on force control and optimization,” Autonomous
Robots, vol. 36, no. 1-2, pp. 11–30, 2014.

[22] A. Marco, P. Hennig, J. Bohg, S. Schaal, and S. Trimpe, “Automatic
LQR tuning based on Gaussian process global optimization,” in
Proceedings of the IEEE International Conference on Robotics and
Automation (ICRA), 2016, pp. 270–277.

[23] S. Trimpe and R. DAndrea, “The balancing cube: A dynamic sculpture
as test bed for distributed estimation and control,” IEEE Control
Systems, vol. 32, no. 6, pp. 48–75, Dec 2012.

[24] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using
pseudo-inputs,” in Advances in Neural Information Processing Sys-
tems, 2005, pp. 1257–1264.

[25] S. A. Billings, Nonlinear system identiﬁcation: NARMAX methods in
John Wiley &

the time, frequency, and spatio-temporal domains.
Sons, 2013.

[26] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elitist
multiobjective genetic algorithm: NSGA-II,” IEEE Transactions on
Evolutionary Computation, vol. 6, no. 2, pp. 182–197, 2002.

[27] K. B. Petersen, M. S. Pedersen, et al., “The matrix cookbook,”

Technical University of Denmark, vol. 7, p. 15, 2008.

