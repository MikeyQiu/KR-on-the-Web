Temporal Tessellation: A Uniﬁed Approach for Video Analysis

Dotan Kaufman1, Gil Levi1, Tal Hassner2,3, and Lior Wolf1,4

1The Blavatnik School of Computer Science , Tel Aviv University, Israel
2Information Sciences Institute , USC , CA, USA
3The Open University of Israel, Israel
4Facebook AI Research

7
1
0
2
 
r
p
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
5
9
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We present a general approach to video understanding,
inspired by semantic transfer techniques that have been suc-
cessfully used for 2D image analysis. Our method con-
siders a video to be a 1D sequence of clips, each one as-
sociated with its own semantics. The nature of these se-
mantics – natural language captions or other labels – de-
pends on the task at hand. A test video is processed by
forming correspondences between its clips and the clips of
reference videos with known semantics, following which,
reference semantics can be transferred to the test video.
We describe two matching methods, both designed to en-
sure that (a) reference clips appear similar to test clips and
(b), taken together, the semantics of the selected reference
clips is consistent and maintains temporal coherence. We
use our method for video captioning on the LSMDC’16
benchmark, video summarization on the SumMe and TV-
Sum benchmarks, Temporal Action Detection on the Thu-
mos2014 benchmark, and sound prediction on the Greatest
Hits benchmark. Our method not only surpasses the state
of the art, in four out of ﬁve benchmarks, but importantly, it
is the only single method we know of that was successfully
applied to such a diverse range of tasks.

1. Introduction

Despite decades of research, video understanding still
challenges computer vision. The reasons for this are many,
and include the hurdles of collecting, labeling and process-
ing video data, which is typically much larger yet less abun-
dant than images. Another reason is the inherent ambiguity
of actions in videos which often defy attempts to attach di-
chotomic labels to video sequences [25]

Rather than attempting to assign videos with single ac-
tion labels (in the same way that 2D images are assigned

Figure 1. Tessellation for temporal coherence. For video cap-
tioning, given a query video (top), we seek reference video clips
with similar semantics. Our tessellation ensures that the semantics
assigned to the test clip are not only the most relevant (the ﬁve
options for each clip) but also preserve temporal coherence (green
path). Ground truth captions are provided in blue.

object classes in, say, the ImageNet collection [46]) an in-
creasing number of efforts focus on other representations
for the semantics of videos. One popular approach as-
signs videos with natural language text annotations which
describe the events taking place in the video [4, 43]. Sys-
tems are then designed to automatically predict these anno-
tations. Others attach video sequences with numeric values
indicating what parts of the video are more interesting or
important [13]. Machine vision is then expected to deter-
mine the importance of each part of the video and summa-
rize videos by keeping only their most important parts.

Although impressive progress was made on these and
other video understanding problems, this progress was often
made disjointedly: separate specialized systems were uti-
lized that were tailored to obtain state of the art performance
on different video understanding problems. Still lacking is
a uniﬁed general approach to solving these different tasks.
Our approach is inspired by recent 2D dense correspon-
dence estimation methods (e.g., [16, 33]). These methods

1

were successfully shown to solve a variety of image un-
derstanding problems by transferring per-pixel semantics
from reference images to query images. This general ap-
proach was effectively applied to a variety of tasks, includ-
ing single view depth estimation, semantic segmentation
and more. We take an analogous approach, applying similar
techniques to 1D video sequences rather than 2D images.

Speciﬁcally, image based methods combine local, per-
pixel appearance similarity with global, spatial smoothness.
We instead combine local, per-region appearance similarity
with global semantics smoothness, or temporal coherence.
Fig. 1 offers an example of this, showing how temporal co-
herence improves the text captions assigned to a video.

Our contributions are as follows: (a) We describe a novel
method for matching test video clips to reference clips. Ref-
erences are assumed to be associated with semantics rep-
resenting the task at hand. Therefore, by this matching
we transfer semantics from reference to test videos. This
process seeks to match clips which share similar appear-
ances while maintaining semantic coherency between the
assigned reference clips. (b) We discuss two techniques for
maintaining temporal coherency: the ﬁrst uses unsupervised
learning for this purpose whereas the second is supervised.
Finally, (c), we show that our method is general by pre-
senting state of the art results on three recent and challeng-
ing video understanding tasks, previously addressed sepa-
rately: Video caption generation on the LSMDC’16 bench-
mark [45], video summarization on the SumMe [13] and
TVSum [52] benchmarks, and action detection on the THU-
In addition, we report results
MOS’14 benchmark [20].
comparable to the state of the art on the Greatest Hits bench-
mark [37] for sound prediction from video. Importantly, we
will publicly release our code and models.1

2. Related work

Video annotation. Signiﬁcant progress was made in the
relatively short time since work on video annotation / cap-
tion generation began. Early methods such as [1, 18, 36, 67]
attempted to cluster captions and videos and applied this
for video retrieval. Others [12, 26, 57] generated sentence
representations by ﬁrst identifying semantic video content
(e.g., verb, noun, etc.) using classiﬁers tailored for particu-
lar objects and events. They then produce template based
sentences. This approach, however, does not scale well,
since it requires substantial efforts to provide suitable train-
ing data for the classiﬁers, as well as limits the possible
sentences that the model can produce.

More recently, and following the success of image an-
notation systems based on deep networks such as [8, 63],
similar techniques were applied to videos [8, 54, 61, 68].
Whereas image based methods used convolutional neural

1See: github.com/dot27/temporal-tessellation

networks (CNN) for this purpose, application to video in-
volve temporal data, which led to the use of recurrent neural
networks (RNN), particularly short-term memory networks
(LSTM) [17]. We also use CNN and LSTM models but in
fundamentally different ways, as we later explain in Sec. 4.

Video summarization. This task involves selecting the
subset of a query video’s frames which represents its most
important content. Early methods developed for this pur-
pose relied on manually speciﬁed cues for determining
which parts of a video are important and should be retained.
A few such examples include [5, 40, 52, 72].

More recently,

the focus shifted towards supervised
learning methods [11, 13, 14, 73], which assume that train-
ing videos also provide manually speciﬁed labels indicat-
ing the importance of different video scenes. These meth-
ods sometimes use multiple individual-tailored decisions to
choose video portions for the summary [13, 14] and often
rely on the determinantal point process (DPP) in order to
increase the diversity of selected video subsets [3, 11, 73].
Unlike video description, LSTM based methods were
only considered for summarization very recently [74].
Their use of LSTM is also very different from ours.

Temporal action detection. Early work on video ac-
tion recognition relied on hand crafted space-time fea-
tures [24, 29, 64]. More recently, deep methods have been
proposed [19, 21, 56], many of which learn deep visual and
motion features [31, 50, 59, 66]. Along with the devel-
opment of stronger methods, larger and more challenging
benchmarks were proposed [15, 25, 27, 53]. Most datasets,
however, used trimmed, temporally segmented videos, i.e:
short clips which contain only a single action.

Recently, similar to the shift toward classiﬁcation com-
bined with localization in object recognition, some of the
focus shifted toward more challenging and realistic sce-
narios of classifying untrimmed videos [10, 20]. In these
datasets, a given video can be up to a few minutes in length,
different actions occur at different times in the video and
in some parts of the video no clear action occurs. These
datasets are also used for classiﬁcation, i.e. determining the
main action taking place in the video. A more challenging
task, however, is the combination of classiﬁcation with tem-
poral detection: determining which action, if any, is taking
place at each time interval in the video.

In order to tackle temporal action detection in untrimmed
videos, Yuan et al. [71] encode visual features at different
temporal resolutions followed by a classiﬁer to obtain clas-
siﬁcation scores at different time scales. Escorcia et al [9]
focus instead on a fast method for obtaining action pro-
posals from untrimmed videos, which later can be fed to
an action classiﬁer. Instead of using action classiﬁers, our
method relies on matching against a gallery of temporally
segmented action clips.

2

3. Preliminaries

Our approach assumes that test videos are partitioned
into clips. It then matches each test clip with a reference
(training) clip. Matching is performed with two goals in
mind. First, at the clip level, we select reference clips which
are visually similar to the input. Second, at the video level,
we select a sequence of clips which best preserves the tem-
poral semantic coherency. Taken in sequence, the order of
selected, reference semantics should adhere to the temporal
manner in which they appeared in the training videos.

Following this step, the semantics associated with se-
lected reference clips can be transferred to test clips. This
allows us to reason about the test video using information
from our reference. This approach is general, since it al-
lows for different types of semantics to be stored and trans-
ferred from reference, training videos to the test videos.
This can include, in particular, textual annotations, action
labels, manual annotations of interesting frames and others.
Thus, different semantics represent different video under-
standing problems which our method can be used to solve.

3.1. Encoding video content

We assume that training and test videos are partitioned
into sequences of clips. A clip C consists of a few consec-
utive frames Ii, i ∈ 1..n where n is the number of frames
in the clip. Our tessellation approach is agnostic to the par-
ticular method chosen to represent these clips. Of course,
The more robust and discriminative the representation, the
better we expect our results to be. We, therefore, chose the
following two step process, based on the recent state of the
art video representations of [30].

Step 1: Representing a single frame. Given a frame Ii
we use an off the shelf CNN to encode its appearance. We
found the VGG-19 CNN to be well suited for this pur-
pose. This network was recently proposed in [51] and used
to obtain state of the art results on the ImageNet, large
scale image recognition benchmark (ILSVRC) [46]. In their
work, [51] used the last layer of this network to predict Im-
ageNet class labels, represented as one-hot encodings. We
instead treat this network as a feature transform function
f : I (cid:55)→ a(cid:48) which for image (frame) I returns the 4, 096D
response vector from the penultimate layer of the network.
To provide robustness to local translations, we extract
these features by oversampling: I is cropped ten times at
different offsets around the center of the frame. These
cropped frames are normalized by subtracting the mean
value of each color channel and then fed to the network.
Finally, the ten 4, 096D response vectors returned by the
network are pooled into a single vector by element-wise
averaging. Principle component analysis (PCA) is further
used to reduce the dimensionality of these features to 500D,
giving us the ﬁnal, per frame representation a ∈ R500.

Step 2: Representing multiple frames. Once the frames
are encoded, we pool them to obtain a representation for
the entire clip. Pooling is performed by Recurrent Neural
Network Fisher Vector (RNN-FV) encoding [30].

Speciﬁcally, We use their RNN, trained to predict the
feature encoding of a future frame, ai, given the encodings
for its k preceding frames, (ai−k, ..., ai−1). This RNN was
trained on the training set from the Large Scale Movie De-
scription Challenge [45], containing roughly 100K videos.
We apply the RNN-FV to the representations produced for
all of the frames in the clip. The gradient of the last layer of
this RNN is then taken as a 100,500D representation for the
entire sequence of frames in C. We again use PCA for di-
mensionality reduction, this time mapping the features pro-
duced by the RNN-FV to 2,000D dimensions, resulting in
our pooled representation A ∈ R2,000. We refer to [30] for
more details about this process.

3.2. Encoding semantics

As previously mentioned, the nature of the semantics
associated with a video depends on the task at hand. For
tasks such as action detection and video summarization, for
which the supervision signal is of low dimension, the se-
mantic space of the labels has only a few bits of informa-
tion per segment and is not discriminative enough between
segments. In this case, we take the semantic space VS to
be the same as the appearance space VA and take both to
be the pooled representation A.

Textual semantics In video captioning, in which the text
data provides a rich source of information, our method
largely beneﬁts from having a separate semantic represen-
tation that is based on the label data.

We tested several representations for video semantics
and chose the recent Fisher Vector of a Hybrid Gaussian-
Laplacian Mixture Model (FV-HGLMM) [23], since it pro-
vided the best results in our initial cross-validation experi-
ments.

Brieﬂy, we assume a textual semantic representation, s
for a clip C, where s is a string containing natural language
words. We use word2vec [34] to map the sequence of words
in s to a vector of numbers, (s1, ..., sm), where m is the
number of words in s and can be different for different clips.
FV-HGLMM then maps this sequence of numbers to a vec-
tor S ∈ RM of ﬁxed dimensionality, M .

FV-HGLMM is based on the well-known Fisher Vectors
(FV) [39, 49, 55]. The standard Gaussian Mixture Models
(GMM) typically used to produce FV representations are
replaced here with a Hybrid Gaussian-Laplacian Mixture
Model which was shown in [23] to be effective for image
annotation. We refer to that paper for more details.

3

3.3. The joint semantics video space (SVS)

Clip representations and their associated semantics are
all mapped to the joint SVS. We aim to map the appearance
of each clip and its assigned semantics to two neighboring
points in the SVS. By doing so, given an appearance rep-
resentation for a query clip, we can search for potential se-
mantic assignments for this clip in our reference set using
standard Euclidean distance. This property will later be-
come important in Sec. 4.2.

In practice, all clip appearance representations A and
their associated semantic representations S are jointly
mapped to the SVS using regularized Canonical Correlation
Analysis (CCA) [62] where the CCA mapping is trained
using the ground truth semantics provided with each bench-
mark. In our experiments, the CCA regularization parame-
ter is ﬁxed to be a tenth of the largest eigenvalue of the cross
domain covariance matrix computed by CCA. For each clip,
CCA projects A and S (appearance and semantics, respec-
tively) to VA and VS.

4. Tessellation

We assume a data set of training (reference) clips, VA
j ,
and their associated semantics, VS
j , represented as de-
scribed in Sec. 3. Here, j ∈ 1..N indexes the entire data
set of N clips. Since these clips may come from different
videos, j does not necessarily reﬂect temporal order.

Given a test video, we process its clips following 3.1
and 3.3, obtaining a sequence of clip representations, UA
in
i
the SVS, where consecutive index values for i ∈ M , repre-
sent consecutive clips in a test video with M clips. Our goal
is to match each UA
i with a data set semantic representation
VS

ji while optimizing the following two requirements:

1. Semantics-appearance similarity. The representa-
tion for the test clip appearance is similar to the repre-
sentation of the selected semantics.

2. Temporal coherence. The selected semantics are or-
dered similar to their occurrences in the training set.

Drawing on the analogy to spatial correspondence estima-
tion methods such as SIFT ﬂow [33], the ﬁrst requirement is
a data term and the second is a smoothness term, albeit with
two important distinctions: First, the data term matches test
appearances to reference semantics directly, building on the
joint embedding of semantics and appearances in the SVS.
Second, we deﬁne the smoothness term in terms of associ-
ated semantics and not pixel coordinates.

4.1. Local Tessellation

Given the sequence of appearance representations U =
1 , ..., UA
M ) for the test sequence, we seek a correspond-
) (here,

(UA
ing set of reference semantics V = (VS
j1

, ..., VS
jM

again, j indexes the N clips in the reference set). The local
tessellation method employs only the semantics-appearance
similarity. In other words, we associate each test clip UA
i ,
with the following training clip:

V ∗
ji

= arg min
Vj

||UA

i − VS
j ||

(1)

4.2. Tessellation Distribution

We make the Markovian assumption that the semantics
assigned to input clip i, only depend on the appearance of
clip i and the semantics assigned to its preceding clip, i − 1.
This gives the standard factorization of the joint distribution
for the clip appearances and their selected semantics:

P (V, U) =P (VS
j1

)P (UA

1 |VS
j1

)×

(2)

M
(cid:89)

i=2

P (VS
ji

|VS

ji−1

)P (UA

i |VS
ji

).

We set the priors P (VS
) to be the uniform distribution.
j1
Due to our mapping of both appearances and semantics to
the joint SVS, we can deﬁne both posterior probabilities
simply using the L2-norm of these representations:

P (UA

i |VS
|VS

j ) ∝ exp (−||UA
) ∝ exp (−||VS
ji

i − VS
− VS

ji−1

j ||2)

||2)

ji−1

P (VS
ji

(3)

(4)

Ostensibly, We can now apply the standard Viterbi
method [41] to obtain a sequence V which maximizes this
probability.
In practice, we used a slightly modiﬁed ver-
sion of this method, and, when possible, a novel method de-
signed to better exploit our training data to predict database
matches. These are explained below.

4.3. Restricted Viterbi Method.

Given the test clip appearance representations U, the

Viterbi method provides an assignment V ∗ such that,

V ∗ = arg max

P (V, U).

V

(5)

i |VS

We found that in practice P (UA
j ) is a long-tail distri-
bution, with only a few dataset elements VS
j near enough to
any UA
i for their probability to be more than near-zero. We,
therefore, restrict the Viterbi method in two ways. First,
we consider only the r(cid:48) = 5 nearest neighboring database
semantics features. Second, we apply a threshold on the
probability of our data term, Eq. (3), and do not consider
semantics VS
j falling below this threshold, except for the
ﬁrst nearest neighbor. Therefore, the number of available
assignments for each clip is 1 ≤ r ≤ 5. This process is
illustrated in Figure 2 (left).

4

Figure 2. Our two non-local tessellations. Left: Tessellation by restricted Viterbi. For a query video (top), our method ﬁnds visually
similar videos and selects the clips that preserve temporal coherence using the Viterbi Method. The ground truth captions are shown in
blue, the closest caption is shown in pink. Note that our method does not always select clips with the closest captions but the ones that best
preserve temporal coherence. Right: Tessellation by predicting the dynamics of semantics. Given a query video (top) and a previous clip
selection, we use an LSTM to predict the most accurate semantics for the next clip.

Method
BUPT CIST AI lab∗
IIT Kanpur∗
Aalto University∗
Shetty and Laaksonen [47]
Yu et al [70]
S2VT [61]
Appearance Matching
Local Tessellation
Unsupervised Tessellation
Supervised Tessellation

CIDEr-D BLEU-4 BLEU-1 BLEU-2 BLEU-3 METEOR ROUGE
.047
.003
.001
.024
.049
.051
.026
.042
.043
.044

.075
.070
.033
.046
.070
.070
.046
.056
.055
.057

.152
.138
.069
.108
.149
.157
.110
.130
.137
.135

.151
.116
.007
.119
.157
.162
.118
.144
.146
.151

.072
.042
.037
.044
.082
.088
.042
.098
.102
.109

.005
.004
.002
.003
.007
.007
.003
.007
.007
.008

.013
.011
.005
.007
.017
.017
.008
.016
.016
.017

Table 1. Video annotation results on the LSMDC’16 challenge [45]. CIDEr-D and BLEU-4 values were found to be the most correlated
with human annotations in [44, 60]. Our results on these metrics far outperform others. * Denotes results which appear in the online
challenge result board, but were never published. They are included here as reference.

4.4. Predicting the Dynamics of Semantics

The Viterbi method of Sec. 4.3 is efﬁcient and requires
only unsupervised training. Its use of the smoothness term
of Eq. (3), however, results in potentially constant semantic
assignments, where for any ji, VS

ji can equal VS

ji−1.

In cases where reference clips are abundant and come
from continuous video sources, we provide a more effective
method of ensuring smoothness. This is done by supervised
learning of how the semantic labels associated with video
clips change over time, and by using that to predict the as-
signment VS

ji for UA
i .

Our process is illustrated in Fig. 2 (right). We train an
LSTM RNN [17] on the semantic and appearance represen-
tations of the training set video clips. We use this network
as a function:

g(VS

0 , VS

1 , ..., VS

1 , ..., UA

i−1, UA

i ) = VS
i ,

i−1, UA
VS

0 = 0,

which predicts the semantic representation VS
i for the clip
at time i given the semantic representation, VS
i−1, assigned
to the preceding clip and the appearance of the current clip,
UA
i . The labeled examples used to train g are taken from the
training set, following the processing described in Sec. 3.2
and 3.3 in order to produce 2,000D post-CCA vectors. Each
pair of previous ground truth semantics and current clip ap-
pearance in the training data provides one sample for train-
ing the LSTM. We employ two hidden layers, each with
1, 000 LSTM cells. The output, which predicts the seman-
tics of the next clip, is also 2,000D.

Given a test video, we begin by processing it as in
Sec. 4.3. In particular, for each of its clip representations
UA
i , we select r ≤ 5 nearest neighboring semantics from
the training set. At each time step i, we feed the clip and its
assigned semantics from the preceding clip at time i − 1 to
our LSTM predictor g. We thus obtain an estimate for the
semantics we expect to see at time i, ˆVS
i .
Of course, the predicted vector ˆVS
i cannot necessarily

(6)

5

be interpreted as a semantic label: not all points in the SVS
have semantic interpretations (in fact, many instead repre-
sent appearance information and more do not represent any-
thing at all). We thus choose a representation VS
ji out of the
r selected for this clip, such that || ˆVS
||2 is smallest.

i − VS
ji

5. Experiments

We apply our method to four separate video understand-
ing tasks: video annotation, video summarization, temporal
action detection, and sound prediction.
Importantly, pre-
vious work was separately tailored to each of these tasks;
we are unaware of any previously proposed single method
which reported results on such a diverse range of video un-
derstanding problems. Contrary to the others, our method
was applied to all of these tasks similarly.

5.1. Video Annotation

In our annotation experiments, we used the movie
annotation benchmark deﬁned by the 2016 Large
Scale Movie Description and Understanding Challenge
(LSMDC16) [45]. LSMDC16 presents a uniﬁed version
of
the recently published large-scale movie datasets,
M-VAD [58] and MPII-MD [43]. The joint dataset contains
188 movies, divided to short (4-20 seconds) video clips
with associated sentence descriptions. A total of 200
movies are available, from which 153, 12 and 17 are used
for training, validation, and testing, respectively (20 more
were used for blind tests, not performed here).

Table 1 present annotation results. We focus primarily
on the CIDEr-D [60] and the BLEU-4 [38] measures, since
they are the only ones that are known to be well correlated
with human perception [44, 60]. Other metrics are pro-
vided here for completeness. These measures are: BLEU1–
3 [38], METEOR [7], and ROUGE-L [32]. We compare
our method with several published and unpublished sys-
tems. The results include the following three variants of
our pipeline.
Local tessellation. Our baseline system uses per-clip near-
est neighbor matching in the SVS in order to choose refer-
ence semantics. We match each test clip with its closest se-
mantics in the SVS. From Tab. 1, we see that this method al-
ready outperforms previous State-of-the-Art. As reference,
we provide the performance of a similar method which
matches clips in appearance space (Appearance matching).
The substantial gap between the two underscores the impor-
tance of our semantics-appearance similarity matching.
Unsupervised tessellation. The graph-based method for
considering temporal coherence, as presented in Sec. 4.3 is
able to provide a slight improvement in results in compari-
son to the local method (Tab. 1).
Supervised tessellation. The LSTM based model de-
scribed in Sec. 4.4, employing 2,000 LSTM units split be-

GT: SOMEONE serves SOMEONE and SOMEONE.
ST: Now at a restaurant a waitress serves drinks.

GT: Then reaches in her bag and takes out a framed photo
of a silver-haired woman.
ST: He spots a framed photo with SOMEONE in it.

GT: SOMEONE shifts his confused stare.
ST: He shifts his gaze then nods.
Figure 3. Qualitative video captioning results. Three caption
assignments from the test set of the LSMDC16 benchmark. The
Ground Truth captioning is provided along with the result of the
Supervised Tessellation (ST) method.

tween two layers.

This method achieved the overall best performance on
both CIDEr-D and BLEU-4, the metrics known to be most
correlated with human perception [44, 60], outperforming
previous state of the art with a gain of 23% on CIDEr-D.
Qualitative results are provided in Fig. 3.

5.2. Video Summarization

Video summarization performance is evaluated on the
SumMe [13] and TVSum [52] benchmarks. These bench-
marks consist of 25 and 50 raw user videos, each depicting
a certain event. The video frames are hand labeled with an
importance score ranging from 0 (redundant) and 1 (vital)
in SumMe and from 1 (redundant) and 5 (vital) in TVSum.
The videos are about 1-5 minutes in length and the task is
to produce a summary in the form of selected frames which
is up-to 15% of the given video’s length. Sample frames
are shown in Fig. 4. The evaluation metric is the average
f-measure of the predicted summary with the ground truth
annotations. We follow [14, 74] in evaluating with multiple
user annotations.

Similar to video annotation, our approach is to trans-
fer the semantics (represented here by frame importance
values) from the gallery to the tessellated video. Our
method operates without incorporating additional compu-
tational steps, such as optimizing the selected set using the
determinantal point process [28], commonly used for such

6

Figure 4. Sample video summarization results. Sample frames from six videos out of the SumMe benchmark. Each group of four frames
contains two frames (top rows) from short segments that were deemed important by the unsupervised tessellation method and two (bottom
rows) that were dropped out of our summaries.

SumMe TVSum

5.3. Temporal Action Detection

Method
Khosla et al. [22] † ‡
Zhao et al. [75] † ‡
Song et al. [52] †
Gygli et. al [14]
Long Short-Term Memory [74]
Summary Transfer [73]
Local Tessellation
Unsupervised Tessellation
Supervised Tessellation

–
–
–
39.7
39.8
40.9
33.8
41.4
37.2

36.0
46.0
50.0
–
54.7
–
60.9
64.1
63.4

Table 2. Video summarization results on the SumMe [13] and TV-
Sum [52] benchmarks. Shown are the average f-measures. Our
unsupervised tessellation method outperforms previous methods
by a substantial margin. † - unsupervised , ‡ - taken from [74]

applications [3, 11, 73].

Table 2 compares our performance with several recent
reports on the same benchmarks. We again provide results
for all three variants of our system. This time, the local and
the supervised tessellation methods are both outperformed
by previous work on SumMe but not on TVSum. Our unsu-
pervised tessellation outperforms other tessellation methods
as well as the state of the art on the summarization bench-
marks by substantial margins.

We believe that unsupervised tessellation worked bet-
ter than supervised because the available training examples
were much fewer than required for the more powerful but
data hungry LSTM. Speciﬁcally, for each benchmark we
used only the labels from the same dataset, without lever-
aging other summarization datasets for this purpose. Do-
ing so, using, e.g., the Open Video Project and YouTube
dataset [6], is left for future work.

We evaluate our method on the task of action detection,

using the THUMOS’14 [20] benchmark for this purpose.

This one of the most recent and, to our knowledge, most
challenging benchmark released for this task. THUMOS’14
consists of a training set of 13,320 temporally trimmed
videos from the action classes of the UCF 101 dataset [53],
a validation set of 1,010 temporally untrimmed videos with
temporal action annotations, a background set with 2,500
relevant videos guaranteed to not include any instance of
the 101 actions and ﬁnally a test set with 1,574 temporally
untrimmed videos. In the temporal action detection bench-
mark, for every action class out of a subset of 20 actions, the
task is to predict both the presence of the action in a given
video and its temporal interval, i.e., the start and end times
of its detected instances.

For each action, the detected intervals are compared
against ground-truth intervals using the Intersection over
Union (IoU) similarity measure. Denoting the predicted in-
tervals by Rp and the ground truth intervals by Rgt, the IoU
similarity is computed as IoU = Rp∩Rgt
Rp∪Rgt

.

A predicted action interval is considered as true posi-
tive, if its IoT measure is above a predeﬁned threshold and
false positive otherwise. Ground truth annotations with no
matching predictions are also counted as false positives.
The Average Precision (AP) for each of the 20 classes
is then computed and the mean Average Precision (mAP)
serves as an overall performance measure. The process re-
peats for different IoT thresholds ranging from 0.1 to 0.5.

Our approach to detection is to tessellate a given
untrimmed video with short atomic clips from the UCF
dataset [53]. With the resulting tessellation, we can de-
termine which action occurred at each time in the video.

7

Figure 5. Sample action detection results. Detection results for the ’Baseball pitch’ class. The predicted intervals by the supervised
tessellation method are shown in green, the ground truth in blue.

Method
Wang et al. [65]
Oneata et al. [35]
Heilbron et al. [2]
Escorcia et al. [9]
Richard and Gall [42]
Shou et al. [48]
Yeung et al. [69]
Yuan et al. [71]
Local tessellation
Unsupervised t.
Supervised t.

0.1
18.2
36.6
–
–
39.7
47.7
48.9
51.4
56.4
57.9
61.1

0.2
17.0
33.6
–
–
35.7
43.5
44.0
42.6
51.2
54.2
56.8

0.3
14.0
27.0
–
–
30.0
36.3
36.0
33.6
43.8
47.3
49.3

0.4
11.7
20.8
–
–
23.2
28.7
26.4
26.1
32.5
35.2
36.5

0.5
8.3
14.4
13.5
13.9
15.2
19.0
17.1
18.8
20.7
22.4
23.3

Table 3. Temporal Action Detection results on the THU-
MOS’14 [20] benchmark. Shown are the mAP of the various
methods for different IoT thresholds. Our proposed framework
outperforms previous State-of-the-Art methods by a large margin.
The supervised tessellation obtains the best results.

Detection results on one sample video are shown in Fig. 5.
Tab. 3 lists the results of the three variants of our frame-
work along with previous results presented on the bench-
mark. As evident from the table, our tessellation method
outperforms the state of the art by a large margin, where
the supervised tessellation achieves the best results among
the three variants of our method. Unsurprisingly, incorpo-
rating context in the tessellation dramatically improves per-
formance, as can be seen from the relatively inferior local
tessellation results.

5.4. Predicting Sounds from Video

Though we show our method to obtain state of the art
results on a number of tasks, at least in one case, a method
tailored for a speciﬁc video analysis task performs better
than our own. Speciﬁcally, we test the capability of our
method to predict sound from video using the Greatest Hits
dataset [37]. This dataset consists of 977 videos of humans
probing different environments with a drumstick: hitting,
scratching, and poking different objects in different scenes.
Each video, on average, contains 48 actions and lasts 35

Method

Full system of [37]
Appearance matching
Local tessellation
Unsupervised tessellation
Supervised tessellation

Loudness
r
Err
0.44
0.21
0.18
0.35
0.32
0.27
0.33
0.26
0.35
0.24

Centroid
r
Err
3.85
0.47
0.36
6.09
0.47
4.83
0.48
4.76
0.46
4.44

Table 4. Greatest Hits benchmark results. Shown are the MSE and
the correlation coefﬁcient for two different success criteria.

seconds. In [37], a CNN followed by an LSTM was used to
predict sounds for each video. Following their protocol, we
consider only the video segments centered on the audio am-
plitude peaks. We employ the published sound features that
are available for 15 frame intervals around each audio peak,
which we take to be our clip size. Each clip C is therefore
associated with the RNN-FV pooled VGG-19 CNN visual
representation, as presented in Sec. 3.1, and with a vector
a ∈ R1,890 of sound features that is simply the concatena-
tion of these 15 sound features.

Matching is performed in a SVS that is constructed from
the visual representation and the matching sound features.
We predict sound features for hit events by applying tessel-
lation and returning the sound feature vector a associated
with the selected reference clips.

There are two criteria that are used for evaluating the re-
sults: Loudness and Centroid. In both cases both the MSE
scores and correlations are reported. Loudness is taken to
be the maximum energy (L2 norm) of the compressed sub-
band envelopes over all timesteps. Centroid is measured by
taking the center of mass of the frequency channels for a
one-frame window around the center of the impact.

Our results are reported in Tab. 4. The importance of
the semantic space as can be observed from the gap be-
tween the appearance only matching to the Local Tessella-
tion method. Leveraging our supervised and unsupervised
tessellation methods improves the results even further. In
three out of four criteria the supervised tessellation seems
preferable to the unsupervised one in this benchmark.

8

6. Conclusions

We present a general approach to understanding and ana-
lyzing videos. Our design transfers per-clip video semantics
from reference, training videos to novel test videos. Three
alternative methods are proposed for this transfer: local tes-
sellation, which uses no context, unsupervised tessellation
which uses dynamic programming to apply temporal, se-
mantic coherency, and supervised tessellation which em-
ploys LSTM to predict future semantics. We show that
these methods, coupled with a recent video representation
technique, provide state of the art results on three very
different video analysis domains: video annotation, video
summarization, and action detection and near state of the
art on a fourth application, sound prediction from video.
Our method is unique in being ﬁrst to obtain state of the art
results on such different video understanding tasks, outper-
forming methods tailored for these applications.

Acknowledgments

This research is supported by the Intel Collaborative Re-

search Institute for Computational Intelligence (ICRI-CI).

References

[1] H. Aradhye, G. Toderici, and J. Yagnik. Video2text: Learn-
ing to annotate video content. In Int. Conf. on Data Mining
Workshops, pages 144–151. IEEE, 2009.

[2] F. Caba Heilbron, J. Carlos Niebles, and B. Ghanem. Fast
temporal activity proposals for efﬁcient detection of human
In Proceedings of the IEEE
actions in untrimmed videos.
Conference on Computer Vision and Pattern Recognition,
pages 1914–1923, 2016.

[3] W.-L. Chao, B. Gong, K. Grauman, and F. Sha. Large-

margin determinantal point processes. UAI, 2015.

[4] D. L. Chen and W. B. Dolan. Collecting highly parallel
data for paraphrase evaluation. In Proc. Annual Meeting of
the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 190–200. Association for Com-
putational Linguistics, 2011.

[5] W.-S. Chu, Y. Song,

Video co-
Jaimes.
summarization:
Video summarization by visual co-
occurrence. In Proc. Conf. Comput. Vision Pattern Recog-
nition, pages 3584–3592, 2015.

and A.

[6] S. E. F. De Avila, A. P. B. Lopes, A. da Luz, and A. de Albu-
querque Ara´ujo. Vsumm: A mechanism designed to produce
static video summaries and a novel evaluation method. Pat-
tern Recognition Letters, 32(1):56–68, 2011.

[7] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language. In In
Proceedings of the Ninth Workshop on Statistical Machine
Translation. Citeseer, 2014.

[8] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual

recognition and description. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 2625–2634, 2015.

[9] V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem.
Daps: Deep action proposals for action understanding.
In
European Conf. Comput. Vision, pages 768–784. Springer,
2016.

[10] B. G. Fabian Caba Heilbron, Victor Escorcia and J. C.
Niebles. Activitynet: A large-scale video benchmark for hu-
man activity understanding. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 961–970, 2015.

[11] B. Gong, W.-L. Chao, K. Grauman, and F. Sha. Diverse
sequential subset selection for supervised video summariza-
In Neural Inform. Process. Syst., pages 2069–2077,
tion.
2014.

[12] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar,
S. Venugopalan, R. Mooney, T. Darrell, and K. Saenko.
Youtube2text: Recognizing and describing arbitrary activi-
ties using semantic hierarchies and zero-shot recognition. In
Proc. Int. Conf. Comput. Vision, pages 2712–2719, 2013.
[13] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool.
In European Conf.

Creating summaries from user videos.
Comput. Vision, pages 505–520. Springer, 2014.

[14] M. Gygli, H. Grabner, and L. Van Gool. Video sum-
marization by learning submodular mixtures of objectives.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
3090–3098, 2015.

[15] T. Hassner. A critical review of action recognition bench-
marks. In Proc. Conf. Comput. Vision Pattern Recognition
Workshops, pages 245–250, 2013.

[16] T. Hassner and C. Liu. Dense Image Correspondences for

Computer Vision. Springer, 2015.

[17] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[18] H. Huang, Y. Lu, F. Zhang, and S. Sun. A multi-modal clus-
tering method for web videos. In Int. Conf. on Trustworthy
Computing and Services, pages 163–169. Springer, 2012.

[19] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. Trans. Pattern Anal.
Mach. Intell., 35(1):221–231, 2013.

[20] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014.

[21] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with con-
volutional neural networks. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 1725–1732, 2014.

[22] A. Khosla, R. Hamid, C.-J. Lin, and N. Sundaresan. Large-
scale video summarization using web-image priors. In Proc.
Conf. Comput. Vision Pattern Recognition, pages 2698–
2705, 2013.

[23] B. Klein, G. Lev, G. Sadeh, and L. Wolf. Associating neu-
ral word embeddings with deep image representations using
ﬁsher vectors. In Proc. Conf. Comput. Vision Pattern Recog-
nition, pages 4437–4446, 2015.

[24] O. Kliper-Gross, Y. Gurovich, T. Hassner, and L. Wolf. Mo-
tion interchange patterns for action recognition in uncon-

9

strained videos. In European Conf. Comput. Vision, pages
256–269. Springer, 2012.

[25] O. Kliper-Gross, T. Hassner, and L. Wolf. The action simi-
larity labeling challenge. Trans. Pattern Anal. Mach. Intell.,
34(3):615–621, 2012.

[26] N. Krishnamoorthy, G. Malkarnenkar, R. J. Mooney,
K. Saenko, and S. Guadarrama. Generating natural-language
In AAAI
video descriptions using text-mined knowledge.
Conf. on Artiﬁcial Intelligence, volume 1, page 2, 2013.
[27] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recognition.
In Proc. Int. Conf. Comput. Vision, pages 2556–2563. IEEE,
2011.

[28] A. Kulesza and B. Taskar. Determinantal point processes for

machine learning. arXiv preprint arXiv:1207.6083, 2012.

[29] I. Laptev. On space-time interest points. 64(2-3):107–123,

2005.

[30] G. Lev, G. Sadeh, B. Klein, and L. Wolf. RNN ﬁsher vectors
for action recognition and image annotation. arXiv preprint
arXiv:1512.03958, 2015.

[31] Y. Li, W. Li, V. Mahadevan, and N. Vasconcelos. Vlad3:
Encoding dynamics of deep features for action recognition.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
1951–1960, 2016.

[32] C.-Y. Lin and F. J. Och. Automatic evaluation of machine
translation quality using longest common subsequence and
skip-bigram statistics. In Proc. Annual Meeting on Associ-
ation for Computational Linguistics, page 605. Association
for Computational Linguistics, 2004.

[33] C. Liu, J. Yuen, and A. Torralba. SIFT ﬂow: Dense corre-
spondence across scenes and its applications. Trans. Pattern
Anal. Mach. Intell., 33(5):978–994, 2011.

[34] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013.

[35] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. 2014.

[36] P. Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, B. Shaw,
A. F. Smeaton, and G. Qu´enot. TRECVID 2012–an overview
of the goals, tasks, data, evaluation mechanisms and metrics.
In Proceedings of TRECVID, 2012.

[37] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adel-
son, and W. T. Freeman. Visually indicated sounds. In Proc.
Conf. Comput. Vision Pattern Recognition, pages 2405–
2413, 2016.

[38] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU:
a method for automatic evaluation of machine translation.
In Proc. annual meeting on association for computational
linguistics, pages 311–318. Association for Computational
Linguistics, 2002.

[39] F. Perronnin, J. S´anchez, and T. Mensink.

Improving the
In Euro-
ﬁsher kernel for large-scale image classiﬁcation.
pean Conf. Comput. Vision, pages 143–156. Springer, 2010.
[40] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid.
Category-speciﬁc video summarization. In European Conf.
Comput. Vision, pages 540–555. Springer, 2014.

[41] L. R. Rabiner. A tutorial on hidden Markov models and se-
lected applications in speech recognition. Proceedings of the
IEEE, 77(2):257–286, 1989.

[42] A. Richard and J. Gall. Temporal action detection using a
statistical language model. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3131–3140, 2016.

[43] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele. A
dataset for movie description. In Proc. Conf. Comput. Vision
Pattern Recognition, 2015.

[44] A. Rohrbach, A. Torabi, T. Maharaj, M. Rohrbach, C. Pal,
A. Courville, and B. Schiele. The large scale movie descrip-
tion and understanding challenge (LSMDC 2016), howpub-
lished = Available: http://tinyurl.com/zabh4et,
month = September, year = 2016.

[45] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, P. Chris,
L. Hugo, C. Aaron, and B. Schiele. Movie description. arXiv
preprint, 2016.

[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge. Int.
J. Comput. Vision, 115(3):211–252, 2015.

[47] R. Shetty and J. Laaksonen. Video captioning with recurrent
networks based on frame-and video-level features and vi-
sual content classiﬁcation. arXiv preprint arXiv:1512.02949,
2015.

[48] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
In Pro-
ization in untrimmed videos via multi-stage cnns.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1049–1058, 2016.

[49] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep ﬁsher net-
works for large-scale image classiﬁcation. In Neural Inform.
Process. Syst., pages 163–171, 2013.

[50] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In Neural Inform.
Process. Syst., pages 568–576, 2014.

[51] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[52] Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes. Tvsum:
Summarizing web videos using titles. In Proc. Conf. Com-
put. Vision Pattern Recognition, pages 5179–5187, 2015.
[53] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[54] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsu-
pervised learning of video representations using LSTMs. In
Int. Conf. Mach. Learning, volume 2, 2015.

[55] V. Sydorov, M. Sakurada, and C. H. Lampert. Deep ﬁsher
kernels-end to end learning of the ﬁsher kernel GMM param-
In Proc. Conf. Comput. Vision Pattern Recognition,
eters.
pages 1402–1409, 2014.

[56] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Con-
volutional learning of spatio-temporal features. In European
Conf. Comput. Vision, pages 140–153. Springer, 2010.
[57] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko,
and R. J. Mooney. Integrating language and vision to gen-

10

[74] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Video sum-
marization with long short-term memory. In European Conf.
Comput. Vision, 2016.

[75] B. Zhao and E. P. Xing. Quasi real-time summarization for
In Proc. Conf. Comput. Vision Pattern

consumer videos.
Recognition, pages 2513–2520, 2014.

erate natural language descriptions of videos in the wild. In
COLING, volume 2, page 9, 2014.

[58] A. Torabi, C. Pal, H. Larochelle, and A. Courville. Using
descriptive video services to create a large data source for
video annotation research. arXiv preprint, 2015.

[59] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In Proc. Int. Conf. Comput. Vision, pages 4489–4497,
2015.

[60] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
In Proc.
Consensus-based image description evaluation.
Conf. Comput. Vision Pattern Recognition, pages 4566–
4575, 2015.

[61] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney,
T. Darrell, and K. Saenko. Sequence to sequence-video to
In Proc. Conf. Comput. Vision Pattern Recognition,
text.
pages 4534–4542, 2015.

[62] H. D. Vinod. Canonical ridge and econometrics of joint pro-
duction. Journal of Econometrics, 4(2):147–166, May 1976.
[63] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: A neural image caption generator. In Proc. Conf. Com-
put. Vision Pattern Recognition, pages 3156–3164, 2015.
[64] H. Wang and C. Schmid. Action recognition with improved
trajectories. In Proc. Int. Conf. Comput. Vision, pages 3551–
3558, 2013.

[65] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features. THU-
MOS14 Action Recognition Challenge, 1:2, 2014.

[66] L. Wang, Y. Qiao, and X. Tang. Action recognition with
In Proc.
trajectory-pooled deep-convolutional descriptors.
Conf. Comput. Vision Pattern Recognition, pages 4305–
4314, 2015.

[67] S. Wei, Y. Zhao, Z. Zhu, and N. Liu. Multimodal fusion
for video search reranking. Trans. on Knowledge and Data
Engineering, 22(8):1191–1199, 2010.

[68] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle,
and A. Courville. Describing videos by exploiting temporal
structure. In Proc. Int. Conf. Comput. Vision, pages 4507–
4515, 2015.

[69] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2678–2687, 2016.
[70] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and
arXiv preprint

retrieval models with semantic attention.
arXiv:1610.02947, 2016.

[71] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal ac-
tion localization with pyramid of score distribution features.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
3093–3102, 2016.

[72] H. J. Zhang, J. Wu, D. Zhong, and S. W. Smoliar. An in-
tegrated system for content-based video retrieval and brows-
ing. Pattern recognition, 30(4):643–658, 1997.

[73] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Summary
transfer: Exemplar-based subset selection for video summa-
rizatio. In Proc. Conf. Comput. Vision Pattern Recognition,
2016.

11

Temporal Tessellation: A Uniﬁed Approach for Video Analysis

Dotan Kaufman1, Gil Levi1, Tal Hassner2,3, and Lior Wolf1,4

1The Blavatnik School of Computer Science , Tel Aviv University, Israel
2Information Sciences Institute , USC , CA, USA
3The Open University of Israel, Israel
4Facebook AI Research

7
1
0
2
 
r
p
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
5
9
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We present a general approach to video understanding,
inspired by semantic transfer techniques that have been suc-
cessfully used for 2D image analysis. Our method con-
siders a video to be a 1D sequence of clips, each one as-
sociated with its own semantics. The nature of these se-
mantics – natural language captions or other labels – de-
pends on the task at hand. A test video is processed by
forming correspondences between its clips and the clips of
reference videos with known semantics, following which,
reference semantics can be transferred to the test video.
We describe two matching methods, both designed to en-
sure that (a) reference clips appear similar to test clips and
(b), taken together, the semantics of the selected reference
clips is consistent and maintains temporal coherence. We
use our method for video captioning on the LSMDC’16
benchmark, video summarization on the SumMe and TV-
Sum benchmarks, Temporal Action Detection on the Thu-
mos2014 benchmark, and sound prediction on the Greatest
Hits benchmark. Our method not only surpasses the state
of the art, in four out of ﬁve benchmarks, but importantly, it
is the only single method we know of that was successfully
applied to such a diverse range of tasks.

1. Introduction

Despite decades of research, video understanding still
challenges computer vision. The reasons for this are many,
and include the hurdles of collecting, labeling and process-
ing video data, which is typically much larger yet less abun-
dant than images. Another reason is the inherent ambiguity
of actions in videos which often defy attempts to attach di-
chotomic labels to video sequences [25]

Rather than attempting to assign videos with single ac-
tion labels (in the same way that 2D images are assigned

Figure 1. Tessellation for temporal coherence. For video cap-
tioning, given a query video (top), we seek reference video clips
with similar semantics. Our tessellation ensures that the semantics
assigned to the test clip are not only the most relevant (the ﬁve
options for each clip) but also preserve temporal coherence (green
path). Ground truth captions are provided in blue.

object classes in, say, the ImageNet collection [46]) an in-
creasing number of efforts focus on other representations
for the semantics of videos. One popular approach as-
signs videos with natural language text annotations which
describe the events taking place in the video [4, 43]. Sys-
tems are then designed to automatically predict these anno-
tations. Others attach video sequences with numeric values
indicating what parts of the video are more interesting or
important [13]. Machine vision is then expected to deter-
mine the importance of each part of the video and summa-
rize videos by keeping only their most important parts.

Although impressive progress was made on these and
other video understanding problems, this progress was often
made disjointedly: separate specialized systems were uti-
lized that were tailored to obtain state of the art performance
on different video understanding problems. Still lacking is
a uniﬁed general approach to solving these different tasks.
Our approach is inspired by recent 2D dense correspon-
dence estimation methods (e.g., [16, 33]). These methods

1

were successfully shown to solve a variety of image un-
derstanding problems by transferring per-pixel semantics
from reference images to query images. This general ap-
proach was effectively applied to a variety of tasks, includ-
ing single view depth estimation, semantic segmentation
and more. We take an analogous approach, applying similar
techniques to 1D video sequences rather than 2D images.

Speciﬁcally, image based methods combine local, per-
pixel appearance similarity with global, spatial smoothness.
We instead combine local, per-region appearance similarity
with global semantics smoothness, or temporal coherence.
Fig. 1 offers an example of this, showing how temporal co-
herence improves the text captions assigned to a video.

Our contributions are as follows: (a) We describe a novel
method for matching test video clips to reference clips. Ref-
erences are assumed to be associated with semantics rep-
resenting the task at hand. Therefore, by this matching
we transfer semantics from reference to test videos. This
process seeks to match clips which share similar appear-
ances while maintaining semantic coherency between the
assigned reference clips. (b) We discuss two techniques for
maintaining temporal coherency: the ﬁrst uses unsupervised
learning for this purpose whereas the second is supervised.
Finally, (c), we show that our method is general by pre-
senting state of the art results on three recent and challeng-
ing video understanding tasks, previously addressed sepa-
rately: Video caption generation on the LSMDC’16 bench-
mark [45], video summarization on the SumMe [13] and
TVSum [52] benchmarks, and action detection on the THU-
In addition, we report results
MOS’14 benchmark [20].
comparable to the state of the art on the Greatest Hits bench-
mark [37] for sound prediction from video. Importantly, we
will publicly release our code and models.1

2. Related work

Video annotation. Signiﬁcant progress was made in the
relatively short time since work on video annotation / cap-
tion generation began. Early methods such as [1, 18, 36, 67]
attempted to cluster captions and videos and applied this
for video retrieval. Others [12, 26, 57] generated sentence
representations by ﬁrst identifying semantic video content
(e.g., verb, noun, etc.) using classiﬁers tailored for particu-
lar objects and events. They then produce template based
sentences. This approach, however, does not scale well,
since it requires substantial efforts to provide suitable train-
ing data for the classiﬁers, as well as limits the possible
sentences that the model can produce.

More recently, and following the success of image an-
notation systems based on deep networks such as [8, 63],
similar techniques were applied to videos [8, 54, 61, 68].
Whereas image based methods used convolutional neural

1See: github.com/dot27/temporal-tessellation

networks (CNN) for this purpose, application to video in-
volve temporal data, which led to the use of recurrent neural
networks (RNN), particularly short-term memory networks
(LSTM) [17]. We also use CNN and LSTM models but in
fundamentally different ways, as we later explain in Sec. 4.

Video summarization. This task involves selecting the
subset of a query video’s frames which represents its most
important content. Early methods developed for this pur-
pose relied on manually speciﬁed cues for determining
which parts of a video are important and should be retained.
A few such examples include [5, 40, 52, 72].

More recently,

the focus shifted towards supervised
learning methods [11, 13, 14, 73], which assume that train-
ing videos also provide manually speciﬁed labels indicat-
ing the importance of different video scenes. These meth-
ods sometimes use multiple individual-tailored decisions to
choose video portions for the summary [13, 14] and often
rely on the determinantal point process (DPP) in order to
increase the diversity of selected video subsets [3, 11, 73].
Unlike video description, LSTM based methods were
only considered for summarization very recently [74].
Their use of LSTM is also very different from ours.

Temporal action detection. Early work on video ac-
tion recognition relied on hand crafted space-time fea-
tures [24, 29, 64]. More recently, deep methods have been
proposed [19, 21, 56], many of which learn deep visual and
motion features [31, 50, 59, 66]. Along with the devel-
opment of stronger methods, larger and more challenging
benchmarks were proposed [15, 25, 27, 53]. Most datasets,
however, used trimmed, temporally segmented videos, i.e:
short clips which contain only a single action.

Recently, similar to the shift toward classiﬁcation com-
bined with localization in object recognition, some of the
focus shifted toward more challenging and realistic sce-
narios of classifying untrimmed videos [10, 20]. In these
datasets, a given video can be up to a few minutes in length,
different actions occur at different times in the video and
in some parts of the video no clear action occurs. These
datasets are also used for classiﬁcation, i.e. determining the
main action taking place in the video. A more challenging
task, however, is the combination of classiﬁcation with tem-
poral detection: determining which action, if any, is taking
place at each time interval in the video.

In order to tackle temporal action detection in untrimmed
videos, Yuan et al. [71] encode visual features at different
temporal resolutions followed by a classiﬁer to obtain clas-
siﬁcation scores at different time scales. Escorcia et al [9]
focus instead on a fast method for obtaining action pro-
posals from untrimmed videos, which later can be fed to
an action classiﬁer. Instead of using action classiﬁers, our
method relies on matching against a gallery of temporally
segmented action clips.

2

3. Preliminaries

Our approach assumes that test videos are partitioned
into clips. It then matches each test clip with a reference
(training) clip. Matching is performed with two goals in
mind. First, at the clip level, we select reference clips which
are visually similar to the input. Second, at the video level,
we select a sequence of clips which best preserves the tem-
poral semantic coherency. Taken in sequence, the order of
selected, reference semantics should adhere to the temporal
manner in which they appeared in the training videos.

Following this step, the semantics associated with se-
lected reference clips can be transferred to test clips. This
allows us to reason about the test video using information
from our reference. This approach is general, since it al-
lows for different types of semantics to be stored and trans-
ferred from reference, training videos to the test videos.
This can include, in particular, textual annotations, action
labels, manual annotations of interesting frames and others.
Thus, different semantics represent different video under-
standing problems which our method can be used to solve.

3.1. Encoding video content

We assume that training and test videos are partitioned
into sequences of clips. A clip C consists of a few consec-
utive frames Ii, i ∈ 1..n where n is the number of frames
in the clip. Our tessellation approach is agnostic to the par-
ticular method chosen to represent these clips. Of course,
The more robust and discriminative the representation, the
better we expect our results to be. We, therefore, chose the
following two step process, based on the recent state of the
art video representations of [30].

Step 1: Representing a single frame. Given a frame Ii
we use an off the shelf CNN to encode its appearance. We
found the VGG-19 CNN to be well suited for this pur-
pose. This network was recently proposed in [51] and used
to obtain state of the art results on the ImageNet, large
scale image recognition benchmark (ILSVRC) [46]. In their
work, [51] used the last layer of this network to predict Im-
ageNet class labels, represented as one-hot encodings. We
instead treat this network as a feature transform function
f : I (cid:55)→ a(cid:48) which for image (frame) I returns the 4, 096D
response vector from the penultimate layer of the network.
To provide robustness to local translations, we extract
these features by oversampling: I is cropped ten times at
different offsets around the center of the frame. These
cropped frames are normalized by subtracting the mean
value of each color channel and then fed to the network.
Finally, the ten 4, 096D response vectors returned by the
network are pooled into a single vector by element-wise
averaging. Principle component analysis (PCA) is further
used to reduce the dimensionality of these features to 500D,
giving us the ﬁnal, per frame representation a ∈ R500.

Step 2: Representing multiple frames. Once the frames
are encoded, we pool them to obtain a representation for
the entire clip. Pooling is performed by Recurrent Neural
Network Fisher Vector (RNN-FV) encoding [30].

Speciﬁcally, We use their RNN, trained to predict the
feature encoding of a future frame, ai, given the encodings
for its k preceding frames, (ai−k, ..., ai−1). This RNN was
trained on the training set from the Large Scale Movie De-
scription Challenge [45], containing roughly 100K videos.
We apply the RNN-FV to the representations produced for
all of the frames in the clip. The gradient of the last layer of
this RNN is then taken as a 100,500D representation for the
entire sequence of frames in C. We again use PCA for di-
mensionality reduction, this time mapping the features pro-
duced by the RNN-FV to 2,000D dimensions, resulting in
our pooled representation A ∈ R2,000. We refer to [30] for
more details about this process.

3.2. Encoding semantics

As previously mentioned, the nature of the semantics
associated with a video depends on the task at hand. For
tasks such as action detection and video summarization, for
which the supervision signal is of low dimension, the se-
mantic space of the labels has only a few bits of informa-
tion per segment and is not discriminative enough between
segments. In this case, we take the semantic space VS to
be the same as the appearance space VA and take both to
be the pooled representation A.

Textual semantics In video captioning, in which the text
data provides a rich source of information, our method
largely beneﬁts from having a separate semantic represen-
tation that is based on the label data.

We tested several representations for video semantics
and chose the recent Fisher Vector of a Hybrid Gaussian-
Laplacian Mixture Model (FV-HGLMM) [23], since it pro-
vided the best results in our initial cross-validation experi-
ments.

Brieﬂy, we assume a textual semantic representation, s
for a clip C, where s is a string containing natural language
words. We use word2vec [34] to map the sequence of words
in s to a vector of numbers, (s1, ..., sm), where m is the
number of words in s and can be different for different clips.
FV-HGLMM then maps this sequence of numbers to a vec-
tor S ∈ RM of ﬁxed dimensionality, M .

FV-HGLMM is based on the well-known Fisher Vectors
(FV) [39, 49, 55]. The standard Gaussian Mixture Models
(GMM) typically used to produce FV representations are
replaced here with a Hybrid Gaussian-Laplacian Mixture
Model which was shown in [23] to be effective for image
annotation. We refer to that paper for more details.

3

3.3. The joint semantics video space (SVS)

Clip representations and their associated semantics are
all mapped to the joint SVS. We aim to map the appearance
of each clip and its assigned semantics to two neighboring
points in the SVS. By doing so, given an appearance rep-
resentation for a query clip, we can search for potential se-
mantic assignments for this clip in our reference set using
standard Euclidean distance. This property will later be-
come important in Sec. 4.2.

In practice, all clip appearance representations A and
their associated semantic representations S are jointly
mapped to the SVS using regularized Canonical Correlation
Analysis (CCA) [62] where the CCA mapping is trained
using the ground truth semantics provided with each bench-
mark. In our experiments, the CCA regularization parame-
ter is ﬁxed to be a tenth of the largest eigenvalue of the cross
domain covariance matrix computed by CCA. For each clip,
CCA projects A and S (appearance and semantics, respec-
tively) to VA and VS.

4. Tessellation

We assume a data set of training (reference) clips, VA
j ,
and their associated semantics, VS
j , represented as de-
scribed in Sec. 3. Here, j ∈ 1..N indexes the entire data
set of N clips. Since these clips may come from different
videos, j does not necessarily reﬂect temporal order.

Given a test video, we process its clips following 3.1
and 3.3, obtaining a sequence of clip representations, UA
in
i
the SVS, where consecutive index values for i ∈ M , repre-
sent consecutive clips in a test video with M clips. Our goal
is to match each UA
i with a data set semantic representation
VS

ji while optimizing the following two requirements:

1. Semantics-appearance similarity. The representa-
tion for the test clip appearance is similar to the repre-
sentation of the selected semantics.

2. Temporal coherence. The selected semantics are or-
dered similar to their occurrences in the training set.

Drawing on the analogy to spatial correspondence estima-
tion methods such as SIFT ﬂow [33], the ﬁrst requirement is
a data term and the second is a smoothness term, albeit with
two important distinctions: First, the data term matches test
appearances to reference semantics directly, building on the
joint embedding of semantics and appearances in the SVS.
Second, we deﬁne the smoothness term in terms of associ-
ated semantics and not pixel coordinates.

4.1. Local Tessellation

Given the sequence of appearance representations U =
1 , ..., UA
M ) for the test sequence, we seek a correspond-
) (here,

(UA
ing set of reference semantics V = (VS
j1

, ..., VS
jM

again, j indexes the N clips in the reference set). The local
tessellation method employs only the semantics-appearance
similarity. In other words, we associate each test clip UA
i ,
with the following training clip:

V ∗
ji

= arg min
Vj

||UA

i − VS
j ||

(1)

4.2. Tessellation Distribution

We make the Markovian assumption that the semantics
assigned to input clip i, only depend on the appearance of
clip i and the semantics assigned to its preceding clip, i − 1.
This gives the standard factorization of the joint distribution
for the clip appearances and their selected semantics:

P (V, U) =P (VS
j1

)P (UA

1 |VS
j1

)×

(2)

M
(cid:89)

i=2

P (VS
ji

|VS

ji−1

)P (UA

i |VS
ji

).

We set the priors P (VS
) to be the uniform distribution.
j1
Due to our mapping of both appearances and semantics to
the joint SVS, we can deﬁne both posterior probabilities
simply using the L2-norm of these representations:

P (UA

i |VS
|VS

j ) ∝ exp (−||UA
) ∝ exp (−||VS
ji

i − VS
− VS

ji−1

j ||2)

||2)

ji−1

P (VS
ji

(3)

(4)

Ostensibly, We can now apply the standard Viterbi
method [41] to obtain a sequence V which maximizes this
probability.
In practice, we used a slightly modiﬁed ver-
sion of this method, and, when possible, a novel method de-
signed to better exploit our training data to predict database
matches. These are explained below.

4.3. Restricted Viterbi Method.

Given the test clip appearance representations U, the

Viterbi method provides an assignment V ∗ such that,

V ∗ = arg max

P (V, U).

V

(5)

i |VS

We found that in practice P (UA
j ) is a long-tail distri-
bution, with only a few dataset elements VS
j near enough to
any UA
i for their probability to be more than near-zero. We,
therefore, restrict the Viterbi method in two ways. First,
we consider only the r(cid:48) = 5 nearest neighboring database
semantics features. Second, we apply a threshold on the
probability of our data term, Eq. (3), and do not consider
semantics VS
j falling below this threshold, except for the
ﬁrst nearest neighbor. Therefore, the number of available
assignments for each clip is 1 ≤ r ≤ 5. This process is
illustrated in Figure 2 (left).

4

Figure 2. Our two non-local tessellations. Left: Tessellation by restricted Viterbi. For a query video (top), our method ﬁnds visually
similar videos and selects the clips that preserve temporal coherence using the Viterbi Method. The ground truth captions are shown in
blue, the closest caption is shown in pink. Note that our method does not always select clips with the closest captions but the ones that best
preserve temporal coherence. Right: Tessellation by predicting the dynamics of semantics. Given a query video (top) and a previous clip
selection, we use an LSTM to predict the most accurate semantics for the next clip.

Method
BUPT CIST AI lab∗
IIT Kanpur∗
Aalto University∗
Shetty and Laaksonen [47]
Yu et al [70]
S2VT [61]
Appearance Matching
Local Tessellation
Unsupervised Tessellation
Supervised Tessellation

CIDEr-D BLEU-4 BLEU-1 BLEU-2 BLEU-3 METEOR ROUGE
.047
.003
.001
.024
.049
.051
.026
.042
.043
.044

.075
.070
.033
.046
.070
.070
.046
.056
.055
.057

.152
.138
.069
.108
.149
.157
.110
.130
.137
.135

.151
.116
.007
.119
.157
.162
.118
.144
.146
.151

.072
.042
.037
.044
.082
.088
.042
.098
.102
.109

.005
.004
.002
.003
.007
.007
.003
.007
.007
.008

.013
.011
.005
.007
.017
.017
.008
.016
.016
.017

Table 1. Video annotation results on the LSMDC’16 challenge [45]. CIDEr-D and BLEU-4 values were found to be the most correlated
with human annotations in [44, 60]. Our results on these metrics far outperform others. * Denotes results which appear in the online
challenge result board, but were never published. They are included here as reference.

4.4. Predicting the Dynamics of Semantics

The Viterbi method of Sec. 4.3 is efﬁcient and requires
only unsupervised training. Its use of the smoothness term
of Eq. (3), however, results in potentially constant semantic
assignments, where for any ji, VS

ji can equal VS

ji−1.

In cases where reference clips are abundant and come
from continuous video sources, we provide a more effective
method of ensuring smoothness. This is done by supervised
learning of how the semantic labels associated with video
clips change over time, and by using that to predict the as-
signment VS

ji for UA
i .

Our process is illustrated in Fig. 2 (right). We train an
LSTM RNN [17] on the semantic and appearance represen-
tations of the training set video clips. We use this network
as a function:

g(VS

0 , VS

1 , ..., VS

1 , ..., UA

i−1, UA

i ) = VS
i ,

i−1, UA
VS

0 = 0,

which predicts the semantic representation VS
i for the clip
at time i given the semantic representation, VS
i−1, assigned
to the preceding clip and the appearance of the current clip,
UA
i . The labeled examples used to train g are taken from the
training set, following the processing described in Sec. 3.2
and 3.3 in order to produce 2,000D post-CCA vectors. Each
pair of previous ground truth semantics and current clip ap-
pearance in the training data provides one sample for train-
ing the LSTM. We employ two hidden layers, each with
1, 000 LSTM cells. The output, which predicts the seman-
tics of the next clip, is also 2,000D.

Given a test video, we begin by processing it as in
Sec. 4.3. In particular, for each of its clip representations
UA
i , we select r ≤ 5 nearest neighboring semantics from
the training set. At each time step i, we feed the clip and its
assigned semantics from the preceding clip at time i − 1 to
our LSTM predictor g. We thus obtain an estimate for the
semantics we expect to see at time i, ˆVS
i .
Of course, the predicted vector ˆVS
i cannot necessarily

(6)

5

be interpreted as a semantic label: not all points in the SVS
have semantic interpretations (in fact, many instead repre-
sent appearance information and more do not represent any-
thing at all). We thus choose a representation VS
ji out of the
r selected for this clip, such that || ˆVS
||2 is smallest.

i − VS
ji

5. Experiments

We apply our method to four separate video understand-
ing tasks: video annotation, video summarization, temporal
action detection, and sound prediction.
Importantly, pre-
vious work was separately tailored to each of these tasks;
we are unaware of any previously proposed single method
which reported results on such a diverse range of video un-
derstanding problems. Contrary to the others, our method
was applied to all of these tasks similarly.

5.1. Video Annotation

In our annotation experiments, we used the movie
annotation benchmark deﬁned by the 2016 Large
Scale Movie Description and Understanding Challenge
(LSMDC16) [45]. LSMDC16 presents a uniﬁed version
of
the recently published large-scale movie datasets,
M-VAD [58] and MPII-MD [43]. The joint dataset contains
188 movies, divided to short (4-20 seconds) video clips
with associated sentence descriptions. A total of 200
movies are available, from which 153, 12 and 17 are used
for training, validation, and testing, respectively (20 more
were used for blind tests, not performed here).

Table 1 present annotation results. We focus primarily
on the CIDEr-D [60] and the BLEU-4 [38] measures, since
they are the only ones that are known to be well correlated
with human perception [44, 60]. Other metrics are pro-
vided here for completeness. These measures are: BLEU1–
3 [38], METEOR [7], and ROUGE-L [32]. We compare
our method with several published and unpublished sys-
tems. The results include the following three variants of
our pipeline.
Local tessellation. Our baseline system uses per-clip near-
est neighbor matching in the SVS in order to choose refer-
ence semantics. We match each test clip with its closest se-
mantics in the SVS. From Tab. 1, we see that this method al-
ready outperforms previous State-of-the-Art. As reference,
we provide the performance of a similar method which
matches clips in appearance space (Appearance matching).
The substantial gap between the two underscores the impor-
tance of our semantics-appearance similarity matching.
Unsupervised tessellation. The graph-based method for
considering temporal coherence, as presented in Sec. 4.3 is
able to provide a slight improvement in results in compari-
son to the local method (Tab. 1).
Supervised tessellation. The LSTM based model de-
scribed in Sec. 4.4, employing 2,000 LSTM units split be-

GT: SOMEONE serves SOMEONE and SOMEONE.
ST: Now at a restaurant a waitress serves drinks.

GT: Then reaches in her bag and takes out a framed photo
of a silver-haired woman.
ST: He spots a framed photo with SOMEONE in it.

GT: SOMEONE shifts his confused stare.
ST: He shifts his gaze then nods.
Figure 3. Qualitative video captioning results. Three caption
assignments from the test set of the LSMDC16 benchmark. The
Ground Truth captioning is provided along with the result of the
Supervised Tessellation (ST) method.

tween two layers.

This method achieved the overall best performance on
both CIDEr-D and BLEU-4, the metrics known to be most
correlated with human perception [44, 60], outperforming
previous state of the art with a gain of 23% on CIDEr-D.
Qualitative results are provided in Fig. 3.

5.2. Video Summarization

Video summarization performance is evaluated on the
SumMe [13] and TVSum [52] benchmarks. These bench-
marks consist of 25 and 50 raw user videos, each depicting
a certain event. The video frames are hand labeled with an
importance score ranging from 0 (redundant) and 1 (vital)
in SumMe and from 1 (redundant) and 5 (vital) in TVSum.
The videos are about 1-5 minutes in length and the task is
to produce a summary in the form of selected frames which
is up-to 15% of the given video’s length. Sample frames
are shown in Fig. 4. The evaluation metric is the average
f-measure of the predicted summary with the ground truth
annotations. We follow [14, 74] in evaluating with multiple
user annotations.

Similar to video annotation, our approach is to trans-
fer the semantics (represented here by frame importance
values) from the gallery to the tessellated video. Our
method operates without incorporating additional compu-
tational steps, such as optimizing the selected set using the
determinantal point process [28], commonly used for such

6

Figure 4. Sample video summarization results. Sample frames from six videos out of the SumMe benchmark. Each group of four frames
contains two frames (top rows) from short segments that were deemed important by the unsupervised tessellation method and two (bottom
rows) that were dropped out of our summaries.

SumMe TVSum

5.3. Temporal Action Detection

Method
Khosla et al. [22] † ‡
Zhao et al. [75] † ‡
Song et al. [52] †
Gygli et. al [14]
Long Short-Term Memory [74]
Summary Transfer [73]
Local Tessellation
Unsupervised Tessellation
Supervised Tessellation

–
–
–
39.7
39.8
40.9
33.8
41.4
37.2

36.0
46.0
50.0
–
54.7
–
60.9
64.1
63.4

Table 2. Video summarization results on the SumMe [13] and TV-
Sum [52] benchmarks. Shown are the average f-measures. Our
unsupervised tessellation method outperforms previous methods
by a substantial margin. † - unsupervised , ‡ - taken from [74]

applications [3, 11, 73].

Table 2 compares our performance with several recent
reports on the same benchmarks. We again provide results
for all three variants of our system. This time, the local and
the supervised tessellation methods are both outperformed
by previous work on SumMe but not on TVSum. Our unsu-
pervised tessellation outperforms other tessellation methods
as well as the state of the art on the summarization bench-
marks by substantial margins.

We believe that unsupervised tessellation worked bet-
ter than supervised because the available training examples
were much fewer than required for the more powerful but
data hungry LSTM. Speciﬁcally, for each benchmark we
used only the labels from the same dataset, without lever-
aging other summarization datasets for this purpose. Do-
ing so, using, e.g., the Open Video Project and YouTube
dataset [6], is left for future work.

We evaluate our method on the task of action detection,

using the THUMOS’14 [20] benchmark for this purpose.

This one of the most recent and, to our knowledge, most
challenging benchmark released for this task. THUMOS’14
consists of a training set of 13,320 temporally trimmed
videos from the action classes of the UCF 101 dataset [53],
a validation set of 1,010 temporally untrimmed videos with
temporal action annotations, a background set with 2,500
relevant videos guaranteed to not include any instance of
the 101 actions and ﬁnally a test set with 1,574 temporally
untrimmed videos. In the temporal action detection bench-
mark, for every action class out of a subset of 20 actions, the
task is to predict both the presence of the action in a given
video and its temporal interval, i.e., the start and end times
of its detected instances.

For each action, the detected intervals are compared
against ground-truth intervals using the Intersection over
Union (IoU) similarity measure. Denoting the predicted in-
tervals by Rp and the ground truth intervals by Rgt, the IoU
similarity is computed as IoU = Rp∩Rgt
Rp∪Rgt

.

A predicted action interval is considered as true posi-
tive, if its IoT measure is above a predeﬁned threshold and
false positive otherwise. Ground truth annotations with no
matching predictions are also counted as false positives.
The Average Precision (AP) for each of the 20 classes
is then computed and the mean Average Precision (mAP)
serves as an overall performance measure. The process re-
peats for different IoT thresholds ranging from 0.1 to 0.5.

Our approach to detection is to tessellate a given
untrimmed video with short atomic clips from the UCF
dataset [53]. With the resulting tessellation, we can de-
termine which action occurred at each time in the video.

7

Figure 5. Sample action detection results. Detection results for the ’Baseball pitch’ class. The predicted intervals by the supervised
tessellation method are shown in green, the ground truth in blue.

Method
Wang et al. [65]
Oneata et al. [35]
Heilbron et al. [2]
Escorcia et al. [9]
Richard and Gall [42]
Shou et al. [48]
Yeung et al. [69]
Yuan et al. [71]
Local tessellation
Unsupervised t.
Supervised t.

0.1
18.2
36.6
–
–
39.7
47.7
48.9
51.4
56.4
57.9
61.1

0.2
17.0
33.6
–
–
35.7
43.5
44.0
42.6
51.2
54.2
56.8

0.3
14.0
27.0
–
–
30.0
36.3
36.0
33.6
43.8
47.3
49.3

0.4
11.7
20.8
–
–
23.2
28.7
26.4
26.1
32.5
35.2
36.5

0.5
8.3
14.4
13.5
13.9
15.2
19.0
17.1
18.8
20.7
22.4
23.3

Table 3. Temporal Action Detection results on the THU-
MOS’14 [20] benchmark. Shown are the mAP of the various
methods for different IoT thresholds. Our proposed framework
outperforms previous State-of-the-Art methods by a large margin.
The supervised tessellation obtains the best results.

Detection results on one sample video are shown in Fig. 5.
Tab. 3 lists the results of the three variants of our frame-
work along with previous results presented on the bench-
mark. As evident from the table, our tessellation method
outperforms the state of the art by a large margin, where
the supervised tessellation achieves the best results among
the three variants of our method. Unsurprisingly, incorpo-
rating context in the tessellation dramatically improves per-
formance, as can be seen from the relatively inferior local
tessellation results.

5.4. Predicting Sounds from Video

Though we show our method to obtain state of the art
results on a number of tasks, at least in one case, a method
tailored for a speciﬁc video analysis task performs better
than our own. Speciﬁcally, we test the capability of our
method to predict sound from video using the Greatest Hits
dataset [37]. This dataset consists of 977 videos of humans
probing different environments with a drumstick: hitting,
scratching, and poking different objects in different scenes.
Each video, on average, contains 48 actions and lasts 35

Method

Full system of [37]
Appearance matching
Local tessellation
Unsupervised tessellation
Supervised tessellation

Loudness
r
Err
0.44
0.21
0.18
0.35
0.32
0.27
0.33
0.26
0.35
0.24

Centroid
r
Err
3.85
0.47
0.36
6.09
0.47
4.83
0.48
4.76
0.46
4.44

Table 4. Greatest Hits benchmark results. Shown are the MSE and
the correlation coefﬁcient for two different success criteria.

seconds. In [37], a CNN followed by an LSTM was used to
predict sounds for each video. Following their protocol, we
consider only the video segments centered on the audio am-
plitude peaks. We employ the published sound features that
are available for 15 frame intervals around each audio peak,
which we take to be our clip size. Each clip C is therefore
associated with the RNN-FV pooled VGG-19 CNN visual
representation, as presented in Sec. 3.1, and with a vector
a ∈ R1,890 of sound features that is simply the concatena-
tion of these 15 sound features.

Matching is performed in a SVS that is constructed from
the visual representation and the matching sound features.
We predict sound features for hit events by applying tessel-
lation and returning the sound feature vector a associated
with the selected reference clips.

There are two criteria that are used for evaluating the re-
sults: Loudness and Centroid. In both cases both the MSE
scores and correlations are reported. Loudness is taken to
be the maximum energy (L2 norm) of the compressed sub-
band envelopes over all timesteps. Centroid is measured by
taking the center of mass of the frequency channels for a
one-frame window around the center of the impact.

Our results are reported in Tab. 4. The importance of
the semantic space as can be observed from the gap be-
tween the appearance only matching to the Local Tessella-
tion method. Leveraging our supervised and unsupervised
tessellation methods improves the results even further. In
three out of four criteria the supervised tessellation seems
preferable to the unsupervised one in this benchmark.

8

6. Conclusions

We present a general approach to understanding and ana-
lyzing videos. Our design transfers per-clip video semantics
from reference, training videos to novel test videos. Three
alternative methods are proposed for this transfer: local tes-
sellation, which uses no context, unsupervised tessellation
which uses dynamic programming to apply temporal, se-
mantic coherency, and supervised tessellation which em-
ploys LSTM to predict future semantics. We show that
these methods, coupled with a recent video representation
technique, provide state of the art results on three very
different video analysis domains: video annotation, video
summarization, and action detection and near state of the
art on a fourth application, sound prediction from video.
Our method is unique in being ﬁrst to obtain state of the art
results on such different video understanding tasks, outper-
forming methods tailored for these applications.

Acknowledgments

This research is supported by the Intel Collaborative Re-

search Institute for Computational Intelligence (ICRI-CI).

References

[1] H. Aradhye, G. Toderici, and J. Yagnik. Video2text: Learn-
ing to annotate video content. In Int. Conf. on Data Mining
Workshops, pages 144–151. IEEE, 2009.

[2] F. Caba Heilbron, J. Carlos Niebles, and B. Ghanem. Fast
temporal activity proposals for efﬁcient detection of human
In Proceedings of the IEEE
actions in untrimmed videos.
Conference on Computer Vision and Pattern Recognition,
pages 1914–1923, 2016.

[3] W.-L. Chao, B. Gong, K. Grauman, and F. Sha. Large-

margin determinantal point processes. UAI, 2015.

[4] D. L. Chen and W. B. Dolan. Collecting highly parallel
data for paraphrase evaluation. In Proc. Annual Meeting of
the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 190–200. Association for Com-
putational Linguistics, 2011.

[5] W.-S. Chu, Y. Song,

Video co-
Jaimes.
summarization:
Video summarization by visual co-
occurrence. In Proc. Conf. Comput. Vision Pattern Recog-
nition, pages 3584–3592, 2015.

and A.

[6] S. E. F. De Avila, A. P. B. Lopes, A. da Luz, and A. de Albu-
querque Ara´ujo. Vsumm: A mechanism designed to produce
static video summaries and a novel evaluation method. Pat-
tern Recognition Letters, 32(1):56–68, 2011.

[7] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language. In In
Proceedings of the Ninth Workshop on Statistical Machine
Translation. Citeseer, 2014.

[8] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual

recognition and description. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 2625–2634, 2015.

[9] V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem.
Daps: Deep action proposals for action understanding.
In
European Conf. Comput. Vision, pages 768–784. Springer,
2016.

[10] B. G. Fabian Caba Heilbron, Victor Escorcia and J. C.
Niebles. Activitynet: A large-scale video benchmark for hu-
man activity understanding. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 961–970, 2015.

[11] B. Gong, W.-L. Chao, K. Grauman, and F. Sha. Diverse
sequential subset selection for supervised video summariza-
In Neural Inform. Process. Syst., pages 2069–2077,
tion.
2014.

[12] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar,
S. Venugopalan, R. Mooney, T. Darrell, and K. Saenko.
Youtube2text: Recognizing and describing arbitrary activi-
ties using semantic hierarchies and zero-shot recognition. In
Proc. Int. Conf. Comput. Vision, pages 2712–2719, 2013.
[13] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool.
In European Conf.

Creating summaries from user videos.
Comput. Vision, pages 505–520. Springer, 2014.

[14] M. Gygli, H. Grabner, and L. Van Gool. Video sum-
marization by learning submodular mixtures of objectives.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
3090–3098, 2015.

[15] T. Hassner. A critical review of action recognition bench-
marks. In Proc. Conf. Comput. Vision Pattern Recognition
Workshops, pages 245–250, 2013.

[16] T. Hassner and C. Liu. Dense Image Correspondences for

Computer Vision. Springer, 2015.

[17] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[18] H. Huang, Y. Lu, F. Zhang, and S. Sun. A multi-modal clus-
tering method for web videos. In Int. Conf. on Trustworthy
Computing and Services, pages 163–169. Springer, 2012.

[19] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. Trans. Pattern Anal.
Mach. Intell., 35(1):221–231, 2013.

[20] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014.

[21] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with con-
volutional neural networks. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 1725–1732, 2014.

[22] A. Khosla, R. Hamid, C.-J. Lin, and N. Sundaresan. Large-
scale video summarization using web-image priors. In Proc.
Conf. Comput. Vision Pattern Recognition, pages 2698–
2705, 2013.

[23] B. Klein, G. Lev, G. Sadeh, and L. Wolf. Associating neu-
ral word embeddings with deep image representations using
ﬁsher vectors. In Proc. Conf. Comput. Vision Pattern Recog-
nition, pages 4437–4446, 2015.

[24] O. Kliper-Gross, Y. Gurovich, T. Hassner, and L. Wolf. Mo-
tion interchange patterns for action recognition in uncon-

9

strained videos. In European Conf. Comput. Vision, pages
256–269. Springer, 2012.

[25] O. Kliper-Gross, T. Hassner, and L. Wolf. The action simi-
larity labeling challenge. Trans. Pattern Anal. Mach. Intell.,
34(3):615–621, 2012.

[26] N. Krishnamoorthy, G. Malkarnenkar, R. J. Mooney,
K. Saenko, and S. Guadarrama. Generating natural-language
In AAAI
video descriptions using text-mined knowledge.
Conf. on Artiﬁcial Intelligence, volume 1, page 2, 2013.
[27] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recognition.
In Proc. Int. Conf. Comput. Vision, pages 2556–2563. IEEE,
2011.

[28] A. Kulesza and B. Taskar. Determinantal point processes for

machine learning. arXiv preprint arXiv:1207.6083, 2012.

[29] I. Laptev. On space-time interest points. 64(2-3):107–123,

2005.

[30] G. Lev, G. Sadeh, B. Klein, and L. Wolf. RNN ﬁsher vectors
for action recognition and image annotation. arXiv preprint
arXiv:1512.03958, 2015.

[31] Y. Li, W. Li, V. Mahadevan, and N. Vasconcelos. Vlad3:
Encoding dynamics of deep features for action recognition.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
1951–1960, 2016.

[32] C.-Y. Lin and F. J. Och. Automatic evaluation of machine
translation quality using longest common subsequence and
skip-bigram statistics. In Proc. Annual Meeting on Associ-
ation for Computational Linguistics, page 605. Association
for Computational Linguistics, 2004.

[33] C. Liu, J. Yuen, and A. Torralba. SIFT ﬂow: Dense corre-
spondence across scenes and its applications. Trans. Pattern
Anal. Mach. Intell., 33(5):978–994, 2011.

[34] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013.

[35] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. 2014.

[36] P. Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, B. Shaw,
A. F. Smeaton, and G. Qu´enot. TRECVID 2012–an overview
of the goals, tasks, data, evaluation mechanisms and metrics.
In Proceedings of TRECVID, 2012.

[37] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adel-
son, and W. T. Freeman. Visually indicated sounds. In Proc.
Conf. Comput. Vision Pattern Recognition, pages 2405–
2413, 2016.

[38] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU:
a method for automatic evaluation of machine translation.
In Proc. annual meeting on association for computational
linguistics, pages 311–318. Association for Computational
Linguistics, 2002.

[39] F. Perronnin, J. S´anchez, and T. Mensink.

Improving the
In Euro-
ﬁsher kernel for large-scale image classiﬁcation.
pean Conf. Comput. Vision, pages 143–156. Springer, 2010.
[40] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid.
Category-speciﬁc video summarization. In European Conf.
Comput. Vision, pages 540–555. Springer, 2014.

[41] L. R. Rabiner. A tutorial on hidden Markov models and se-
lected applications in speech recognition. Proceedings of the
IEEE, 77(2):257–286, 1989.

[42] A. Richard and J. Gall. Temporal action detection using a
statistical language model. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3131–3140, 2016.

[43] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele. A
dataset for movie description. In Proc. Conf. Comput. Vision
Pattern Recognition, 2015.

[44] A. Rohrbach, A. Torabi, T. Maharaj, M. Rohrbach, C. Pal,
A. Courville, and B. Schiele. The large scale movie descrip-
tion and understanding challenge (LSMDC 2016), howpub-
lished = Available: http://tinyurl.com/zabh4et,
month = September, year = 2016.

[45] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, P. Chris,
L. Hugo, C. Aaron, and B. Schiele. Movie description. arXiv
preprint, 2016.

[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge. Int.
J. Comput. Vision, 115(3):211–252, 2015.

[47] R. Shetty and J. Laaksonen. Video captioning with recurrent
networks based on frame-and video-level features and vi-
sual content classiﬁcation. arXiv preprint arXiv:1512.02949,
2015.

[48] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
In Pro-
ization in untrimmed videos via multi-stage cnns.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1049–1058, 2016.

[49] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep ﬁsher net-
works for large-scale image classiﬁcation. In Neural Inform.
Process. Syst., pages 163–171, 2013.

[50] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In Neural Inform.
Process. Syst., pages 568–576, 2014.

[51] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[52] Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes. Tvsum:
Summarizing web videos using titles. In Proc. Conf. Com-
put. Vision Pattern Recognition, pages 5179–5187, 2015.
[53] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[54] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsu-
pervised learning of video representations using LSTMs. In
Int. Conf. Mach. Learning, volume 2, 2015.

[55] V. Sydorov, M. Sakurada, and C. H. Lampert. Deep ﬁsher
kernels-end to end learning of the ﬁsher kernel GMM param-
In Proc. Conf. Comput. Vision Pattern Recognition,
eters.
pages 1402–1409, 2014.

[56] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Con-
volutional learning of spatio-temporal features. In European
Conf. Comput. Vision, pages 140–153. Springer, 2010.
[57] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko,
and R. J. Mooney. Integrating language and vision to gen-

10

[74] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Video sum-
marization with long short-term memory. In European Conf.
Comput. Vision, 2016.

[75] B. Zhao and E. P. Xing. Quasi real-time summarization for
In Proc. Conf. Comput. Vision Pattern

consumer videos.
Recognition, pages 2513–2520, 2014.

erate natural language descriptions of videos in the wild. In
COLING, volume 2, page 9, 2014.

[58] A. Torabi, C. Pal, H. Larochelle, and A. Courville. Using
descriptive video services to create a large data source for
video annotation research. arXiv preprint, 2015.

[59] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In Proc. Int. Conf. Comput. Vision, pages 4489–4497,
2015.

[60] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
In Proc.
Consensus-based image description evaluation.
Conf. Comput. Vision Pattern Recognition, pages 4566–
4575, 2015.

[61] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney,
T. Darrell, and K. Saenko. Sequence to sequence-video to
In Proc. Conf. Comput. Vision Pattern Recognition,
text.
pages 4534–4542, 2015.

[62] H. D. Vinod. Canonical ridge and econometrics of joint pro-
duction. Journal of Econometrics, 4(2):147–166, May 1976.
[63] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: A neural image caption generator. In Proc. Conf. Com-
put. Vision Pattern Recognition, pages 3156–3164, 2015.
[64] H. Wang and C. Schmid. Action recognition with improved
trajectories. In Proc. Int. Conf. Comput. Vision, pages 3551–
3558, 2013.

[65] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features. THU-
MOS14 Action Recognition Challenge, 1:2, 2014.

[66] L. Wang, Y. Qiao, and X. Tang. Action recognition with
In Proc.
trajectory-pooled deep-convolutional descriptors.
Conf. Comput. Vision Pattern Recognition, pages 4305–
4314, 2015.

[67] S. Wei, Y. Zhao, Z. Zhu, and N. Liu. Multimodal fusion
for video search reranking. Trans. on Knowledge and Data
Engineering, 22(8):1191–1199, 2010.

[68] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle,
and A. Courville. Describing videos by exploiting temporal
structure. In Proc. Int. Conf. Comput. Vision, pages 4507–
4515, 2015.

[69] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2678–2687, 2016.
[70] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and
arXiv preprint

retrieval models with semantic attention.
arXiv:1610.02947, 2016.

[71] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal ac-
tion localization with pyramid of score distribution features.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
3093–3102, 2016.

[72] H. J. Zhang, J. Wu, D. Zhong, and S. W. Smoliar. An in-
tegrated system for content-based video retrieval and brows-
ing. Pattern recognition, 30(4):643–658, 1997.

[73] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Summary
transfer: Exemplar-based subset selection for video summa-
rizatio. In Proc. Conf. Comput. Vision Pattern Recognition,
2016.

11

Temporal Tessellation: A Uniﬁed Approach for Video Analysis

Dotan Kaufman1, Gil Levi1, Tal Hassner2,3, and Lior Wolf1,4

1The Blavatnik School of Computer Science , Tel Aviv University, Israel
2Information Sciences Institute , USC , CA, USA
3The Open University of Israel, Israel
4Facebook AI Research

7
1
0
2
 
r
p
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
5
9
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We present a general approach to video understanding,
inspired by semantic transfer techniques that have been suc-
cessfully used for 2D image analysis. Our method con-
siders a video to be a 1D sequence of clips, each one as-
sociated with its own semantics. The nature of these se-
mantics – natural language captions or other labels – de-
pends on the task at hand. A test video is processed by
forming correspondences between its clips and the clips of
reference videos with known semantics, following which,
reference semantics can be transferred to the test video.
We describe two matching methods, both designed to en-
sure that (a) reference clips appear similar to test clips and
(b), taken together, the semantics of the selected reference
clips is consistent and maintains temporal coherence. We
use our method for video captioning on the LSMDC’16
benchmark, video summarization on the SumMe and TV-
Sum benchmarks, Temporal Action Detection on the Thu-
mos2014 benchmark, and sound prediction on the Greatest
Hits benchmark. Our method not only surpasses the state
of the art, in four out of ﬁve benchmarks, but importantly, it
is the only single method we know of that was successfully
applied to such a diverse range of tasks.

1. Introduction

Despite decades of research, video understanding still
challenges computer vision. The reasons for this are many,
and include the hurdles of collecting, labeling and process-
ing video data, which is typically much larger yet less abun-
dant than images. Another reason is the inherent ambiguity
of actions in videos which often defy attempts to attach di-
chotomic labels to video sequences [25]

Rather than attempting to assign videos with single ac-
tion labels (in the same way that 2D images are assigned

Figure 1. Tessellation for temporal coherence. For video cap-
tioning, given a query video (top), we seek reference video clips
with similar semantics. Our tessellation ensures that the semantics
assigned to the test clip are not only the most relevant (the ﬁve
options for each clip) but also preserve temporal coherence (green
path). Ground truth captions are provided in blue.

object classes in, say, the ImageNet collection [46]) an in-
creasing number of efforts focus on other representations
for the semantics of videos. One popular approach as-
signs videos with natural language text annotations which
describe the events taking place in the video [4, 43]. Sys-
tems are then designed to automatically predict these anno-
tations. Others attach video sequences with numeric values
indicating what parts of the video are more interesting or
important [13]. Machine vision is then expected to deter-
mine the importance of each part of the video and summa-
rize videos by keeping only their most important parts.

Although impressive progress was made on these and
other video understanding problems, this progress was often
made disjointedly: separate specialized systems were uti-
lized that were tailored to obtain state of the art performance
on different video understanding problems. Still lacking is
a uniﬁed general approach to solving these different tasks.
Our approach is inspired by recent 2D dense correspon-
dence estimation methods (e.g., [16, 33]). These methods

1

were successfully shown to solve a variety of image un-
derstanding problems by transferring per-pixel semantics
from reference images to query images. This general ap-
proach was effectively applied to a variety of tasks, includ-
ing single view depth estimation, semantic segmentation
and more. We take an analogous approach, applying similar
techniques to 1D video sequences rather than 2D images.

Speciﬁcally, image based methods combine local, per-
pixel appearance similarity with global, spatial smoothness.
We instead combine local, per-region appearance similarity
with global semantics smoothness, or temporal coherence.
Fig. 1 offers an example of this, showing how temporal co-
herence improves the text captions assigned to a video.

Our contributions are as follows: (a) We describe a novel
method for matching test video clips to reference clips. Ref-
erences are assumed to be associated with semantics rep-
resenting the task at hand. Therefore, by this matching
we transfer semantics from reference to test videos. This
process seeks to match clips which share similar appear-
ances while maintaining semantic coherency between the
assigned reference clips. (b) We discuss two techniques for
maintaining temporal coherency: the ﬁrst uses unsupervised
learning for this purpose whereas the second is supervised.
Finally, (c), we show that our method is general by pre-
senting state of the art results on three recent and challeng-
ing video understanding tasks, previously addressed sepa-
rately: Video caption generation on the LSMDC’16 bench-
mark [45], video summarization on the SumMe [13] and
TVSum [52] benchmarks, and action detection on the THU-
In addition, we report results
MOS’14 benchmark [20].
comparable to the state of the art on the Greatest Hits bench-
mark [37] for sound prediction from video. Importantly, we
will publicly release our code and models.1

2. Related work

Video annotation. Signiﬁcant progress was made in the
relatively short time since work on video annotation / cap-
tion generation began. Early methods such as [1, 18, 36, 67]
attempted to cluster captions and videos and applied this
for video retrieval. Others [12, 26, 57] generated sentence
representations by ﬁrst identifying semantic video content
(e.g., verb, noun, etc.) using classiﬁers tailored for particu-
lar objects and events. They then produce template based
sentences. This approach, however, does not scale well,
since it requires substantial efforts to provide suitable train-
ing data for the classiﬁers, as well as limits the possible
sentences that the model can produce.

More recently, and following the success of image an-
notation systems based on deep networks such as [8, 63],
similar techniques were applied to videos [8, 54, 61, 68].
Whereas image based methods used convolutional neural

1See: github.com/dot27/temporal-tessellation

networks (CNN) for this purpose, application to video in-
volve temporal data, which led to the use of recurrent neural
networks (RNN), particularly short-term memory networks
(LSTM) [17]. We also use CNN and LSTM models but in
fundamentally different ways, as we later explain in Sec. 4.

Video summarization. This task involves selecting the
subset of a query video’s frames which represents its most
important content. Early methods developed for this pur-
pose relied on manually speciﬁed cues for determining
which parts of a video are important and should be retained.
A few such examples include [5, 40, 52, 72].

More recently,

the focus shifted towards supervised
learning methods [11, 13, 14, 73], which assume that train-
ing videos also provide manually speciﬁed labels indicat-
ing the importance of different video scenes. These meth-
ods sometimes use multiple individual-tailored decisions to
choose video portions for the summary [13, 14] and often
rely on the determinantal point process (DPP) in order to
increase the diversity of selected video subsets [3, 11, 73].
Unlike video description, LSTM based methods were
only considered for summarization very recently [74].
Their use of LSTM is also very different from ours.

Temporal action detection. Early work on video ac-
tion recognition relied on hand crafted space-time fea-
tures [24, 29, 64]. More recently, deep methods have been
proposed [19, 21, 56], many of which learn deep visual and
motion features [31, 50, 59, 66]. Along with the devel-
opment of stronger methods, larger and more challenging
benchmarks were proposed [15, 25, 27, 53]. Most datasets,
however, used trimmed, temporally segmented videos, i.e:
short clips which contain only a single action.

Recently, similar to the shift toward classiﬁcation com-
bined with localization in object recognition, some of the
focus shifted toward more challenging and realistic sce-
narios of classifying untrimmed videos [10, 20]. In these
datasets, a given video can be up to a few minutes in length,
different actions occur at different times in the video and
in some parts of the video no clear action occurs. These
datasets are also used for classiﬁcation, i.e. determining the
main action taking place in the video. A more challenging
task, however, is the combination of classiﬁcation with tem-
poral detection: determining which action, if any, is taking
place at each time interval in the video.

In order to tackle temporal action detection in untrimmed
videos, Yuan et al. [71] encode visual features at different
temporal resolutions followed by a classiﬁer to obtain clas-
siﬁcation scores at different time scales. Escorcia et al [9]
focus instead on a fast method for obtaining action pro-
posals from untrimmed videos, which later can be fed to
an action classiﬁer. Instead of using action classiﬁers, our
method relies on matching against a gallery of temporally
segmented action clips.

2

3. Preliminaries

Our approach assumes that test videos are partitioned
into clips. It then matches each test clip with a reference
(training) clip. Matching is performed with two goals in
mind. First, at the clip level, we select reference clips which
are visually similar to the input. Second, at the video level,
we select a sequence of clips which best preserves the tem-
poral semantic coherency. Taken in sequence, the order of
selected, reference semantics should adhere to the temporal
manner in which they appeared in the training videos.

Following this step, the semantics associated with se-
lected reference clips can be transferred to test clips. This
allows us to reason about the test video using information
from our reference. This approach is general, since it al-
lows for different types of semantics to be stored and trans-
ferred from reference, training videos to the test videos.
This can include, in particular, textual annotations, action
labels, manual annotations of interesting frames and others.
Thus, different semantics represent different video under-
standing problems which our method can be used to solve.

3.1. Encoding video content

We assume that training and test videos are partitioned
into sequences of clips. A clip C consists of a few consec-
utive frames Ii, i ∈ 1..n where n is the number of frames
in the clip. Our tessellation approach is agnostic to the par-
ticular method chosen to represent these clips. Of course,
The more robust and discriminative the representation, the
better we expect our results to be. We, therefore, chose the
following two step process, based on the recent state of the
art video representations of [30].

Step 1: Representing a single frame. Given a frame Ii
we use an off the shelf CNN to encode its appearance. We
found the VGG-19 CNN to be well suited for this pur-
pose. This network was recently proposed in [51] and used
to obtain state of the art results on the ImageNet, large
scale image recognition benchmark (ILSVRC) [46]. In their
work, [51] used the last layer of this network to predict Im-
ageNet class labels, represented as one-hot encodings. We
instead treat this network as a feature transform function
f : I (cid:55)→ a(cid:48) which for image (frame) I returns the 4, 096D
response vector from the penultimate layer of the network.
To provide robustness to local translations, we extract
these features by oversampling: I is cropped ten times at
different offsets around the center of the frame. These
cropped frames are normalized by subtracting the mean
value of each color channel and then fed to the network.
Finally, the ten 4, 096D response vectors returned by the
network are pooled into a single vector by element-wise
averaging. Principle component analysis (PCA) is further
used to reduce the dimensionality of these features to 500D,
giving us the ﬁnal, per frame representation a ∈ R500.

Step 2: Representing multiple frames. Once the frames
are encoded, we pool them to obtain a representation for
the entire clip. Pooling is performed by Recurrent Neural
Network Fisher Vector (RNN-FV) encoding [30].

Speciﬁcally, We use their RNN, trained to predict the
feature encoding of a future frame, ai, given the encodings
for its k preceding frames, (ai−k, ..., ai−1). This RNN was
trained on the training set from the Large Scale Movie De-
scription Challenge [45], containing roughly 100K videos.
We apply the RNN-FV to the representations produced for
all of the frames in the clip. The gradient of the last layer of
this RNN is then taken as a 100,500D representation for the
entire sequence of frames in C. We again use PCA for di-
mensionality reduction, this time mapping the features pro-
duced by the RNN-FV to 2,000D dimensions, resulting in
our pooled representation A ∈ R2,000. We refer to [30] for
more details about this process.

3.2. Encoding semantics

As previously mentioned, the nature of the semantics
associated with a video depends on the task at hand. For
tasks such as action detection and video summarization, for
which the supervision signal is of low dimension, the se-
mantic space of the labels has only a few bits of informa-
tion per segment and is not discriminative enough between
segments. In this case, we take the semantic space VS to
be the same as the appearance space VA and take both to
be the pooled representation A.

Textual semantics In video captioning, in which the text
data provides a rich source of information, our method
largely beneﬁts from having a separate semantic represen-
tation that is based on the label data.

We tested several representations for video semantics
and chose the recent Fisher Vector of a Hybrid Gaussian-
Laplacian Mixture Model (FV-HGLMM) [23], since it pro-
vided the best results in our initial cross-validation experi-
ments.

Brieﬂy, we assume a textual semantic representation, s
for a clip C, where s is a string containing natural language
words. We use word2vec [34] to map the sequence of words
in s to a vector of numbers, (s1, ..., sm), where m is the
number of words in s and can be different for different clips.
FV-HGLMM then maps this sequence of numbers to a vec-
tor S ∈ RM of ﬁxed dimensionality, M .

FV-HGLMM is based on the well-known Fisher Vectors
(FV) [39, 49, 55]. The standard Gaussian Mixture Models
(GMM) typically used to produce FV representations are
replaced here with a Hybrid Gaussian-Laplacian Mixture
Model which was shown in [23] to be effective for image
annotation. We refer to that paper for more details.

3

3.3. The joint semantics video space (SVS)

Clip representations and their associated semantics are
all mapped to the joint SVS. We aim to map the appearance
of each clip and its assigned semantics to two neighboring
points in the SVS. By doing so, given an appearance rep-
resentation for a query clip, we can search for potential se-
mantic assignments for this clip in our reference set using
standard Euclidean distance. This property will later be-
come important in Sec. 4.2.

In practice, all clip appearance representations A and
their associated semantic representations S are jointly
mapped to the SVS using regularized Canonical Correlation
Analysis (CCA) [62] where the CCA mapping is trained
using the ground truth semantics provided with each bench-
mark. In our experiments, the CCA regularization parame-
ter is ﬁxed to be a tenth of the largest eigenvalue of the cross
domain covariance matrix computed by CCA. For each clip,
CCA projects A and S (appearance and semantics, respec-
tively) to VA and VS.

4. Tessellation

We assume a data set of training (reference) clips, VA
j ,
and their associated semantics, VS
j , represented as de-
scribed in Sec. 3. Here, j ∈ 1..N indexes the entire data
set of N clips. Since these clips may come from different
videos, j does not necessarily reﬂect temporal order.

Given a test video, we process its clips following 3.1
and 3.3, obtaining a sequence of clip representations, UA
in
i
the SVS, where consecutive index values for i ∈ M , repre-
sent consecutive clips in a test video with M clips. Our goal
is to match each UA
i with a data set semantic representation
VS

ji while optimizing the following two requirements:

1. Semantics-appearance similarity. The representa-
tion for the test clip appearance is similar to the repre-
sentation of the selected semantics.

2. Temporal coherence. The selected semantics are or-
dered similar to their occurrences in the training set.

Drawing on the analogy to spatial correspondence estima-
tion methods such as SIFT ﬂow [33], the ﬁrst requirement is
a data term and the second is a smoothness term, albeit with
two important distinctions: First, the data term matches test
appearances to reference semantics directly, building on the
joint embedding of semantics and appearances in the SVS.
Second, we deﬁne the smoothness term in terms of associ-
ated semantics and not pixel coordinates.

4.1. Local Tessellation

Given the sequence of appearance representations U =
1 , ..., UA
M ) for the test sequence, we seek a correspond-
) (here,

(UA
ing set of reference semantics V = (VS
j1

, ..., VS
jM

again, j indexes the N clips in the reference set). The local
tessellation method employs only the semantics-appearance
similarity. In other words, we associate each test clip UA
i ,
with the following training clip:

V ∗
ji

= arg min
Vj

||UA

i − VS
j ||

(1)

4.2. Tessellation Distribution

We make the Markovian assumption that the semantics
assigned to input clip i, only depend on the appearance of
clip i and the semantics assigned to its preceding clip, i − 1.
This gives the standard factorization of the joint distribution
for the clip appearances and their selected semantics:

P (V, U) =P (VS
j1

)P (UA

1 |VS
j1

)×

(2)

M
(cid:89)

i=2

P (VS
ji

|VS

ji−1

)P (UA

i |VS
ji

).

We set the priors P (VS
) to be the uniform distribution.
j1
Due to our mapping of both appearances and semantics to
the joint SVS, we can deﬁne both posterior probabilities
simply using the L2-norm of these representations:

P (UA

i |VS
|VS

j ) ∝ exp (−||UA
) ∝ exp (−||VS
ji

i − VS
− VS

ji−1

j ||2)

||2)

ji−1

P (VS
ji

(3)

(4)

Ostensibly, We can now apply the standard Viterbi
method [41] to obtain a sequence V which maximizes this
probability.
In practice, we used a slightly modiﬁed ver-
sion of this method, and, when possible, a novel method de-
signed to better exploit our training data to predict database
matches. These are explained below.

4.3. Restricted Viterbi Method.

Given the test clip appearance representations U, the

Viterbi method provides an assignment V ∗ such that,

V ∗ = arg max

P (V, U).

V

(5)

i |VS

We found that in practice P (UA
j ) is a long-tail distri-
bution, with only a few dataset elements VS
j near enough to
any UA
i for their probability to be more than near-zero. We,
therefore, restrict the Viterbi method in two ways. First,
we consider only the r(cid:48) = 5 nearest neighboring database
semantics features. Second, we apply a threshold on the
probability of our data term, Eq. (3), and do not consider
semantics VS
j falling below this threshold, except for the
ﬁrst nearest neighbor. Therefore, the number of available
assignments for each clip is 1 ≤ r ≤ 5. This process is
illustrated in Figure 2 (left).

4

Figure 2. Our two non-local tessellations. Left: Tessellation by restricted Viterbi. For a query video (top), our method ﬁnds visually
similar videos and selects the clips that preserve temporal coherence using the Viterbi Method. The ground truth captions are shown in
blue, the closest caption is shown in pink. Note that our method does not always select clips with the closest captions but the ones that best
preserve temporal coherence. Right: Tessellation by predicting the dynamics of semantics. Given a query video (top) and a previous clip
selection, we use an LSTM to predict the most accurate semantics for the next clip.

Method
BUPT CIST AI lab∗
IIT Kanpur∗
Aalto University∗
Shetty and Laaksonen [47]
Yu et al [70]
S2VT [61]
Appearance Matching
Local Tessellation
Unsupervised Tessellation
Supervised Tessellation

CIDEr-D BLEU-4 BLEU-1 BLEU-2 BLEU-3 METEOR ROUGE
.047
.003
.001
.024
.049
.051
.026
.042
.043
.044

.075
.070
.033
.046
.070
.070
.046
.056
.055
.057

.152
.138
.069
.108
.149
.157
.110
.130
.137
.135

.151
.116
.007
.119
.157
.162
.118
.144
.146
.151

.072
.042
.037
.044
.082
.088
.042
.098
.102
.109

.005
.004
.002
.003
.007
.007
.003
.007
.007
.008

.013
.011
.005
.007
.017
.017
.008
.016
.016
.017

Table 1. Video annotation results on the LSMDC’16 challenge [45]. CIDEr-D and BLEU-4 values were found to be the most correlated
with human annotations in [44, 60]. Our results on these metrics far outperform others. * Denotes results which appear in the online
challenge result board, but were never published. They are included here as reference.

4.4. Predicting the Dynamics of Semantics

The Viterbi method of Sec. 4.3 is efﬁcient and requires
only unsupervised training. Its use of the smoothness term
of Eq. (3), however, results in potentially constant semantic
assignments, where for any ji, VS

ji can equal VS

ji−1.

In cases where reference clips are abundant and come
from continuous video sources, we provide a more effective
method of ensuring smoothness. This is done by supervised
learning of how the semantic labels associated with video
clips change over time, and by using that to predict the as-
signment VS

ji for UA
i .

Our process is illustrated in Fig. 2 (right). We train an
LSTM RNN [17] on the semantic and appearance represen-
tations of the training set video clips. We use this network
as a function:

g(VS

0 , VS

1 , ..., VS

1 , ..., UA

i−1, UA

i ) = VS
i ,

i−1, UA
VS

0 = 0,

which predicts the semantic representation VS
i for the clip
at time i given the semantic representation, VS
i−1, assigned
to the preceding clip and the appearance of the current clip,
UA
i . The labeled examples used to train g are taken from the
training set, following the processing described in Sec. 3.2
and 3.3 in order to produce 2,000D post-CCA vectors. Each
pair of previous ground truth semantics and current clip ap-
pearance in the training data provides one sample for train-
ing the LSTM. We employ two hidden layers, each with
1, 000 LSTM cells. The output, which predicts the seman-
tics of the next clip, is also 2,000D.

Given a test video, we begin by processing it as in
Sec. 4.3. In particular, for each of its clip representations
UA
i , we select r ≤ 5 nearest neighboring semantics from
the training set. At each time step i, we feed the clip and its
assigned semantics from the preceding clip at time i − 1 to
our LSTM predictor g. We thus obtain an estimate for the
semantics we expect to see at time i, ˆVS
i .
Of course, the predicted vector ˆVS
i cannot necessarily

(6)

5

be interpreted as a semantic label: not all points in the SVS
have semantic interpretations (in fact, many instead repre-
sent appearance information and more do not represent any-
thing at all). We thus choose a representation VS
ji out of the
r selected for this clip, such that || ˆVS
||2 is smallest.

i − VS
ji

5. Experiments

We apply our method to four separate video understand-
ing tasks: video annotation, video summarization, temporal
action detection, and sound prediction.
Importantly, pre-
vious work was separately tailored to each of these tasks;
we are unaware of any previously proposed single method
which reported results on such a diverse range of video un-
derstanding problems. Contrary to the others, our method
was applied to all of these tasks similarly.

5.1. Video Annotation

In our annotation experiments, we used the movie
annotation benchmark deﬁned by the 2016 Large
Scale Movie Description and Understanding Challenge
(LSMDC16) [45]. LSMDC16 presents a uniﬁed version
of
the recently published large-scale movie datasets,
M-VAD [58] and MPII-MD [43]. The joint dataset contains
188 movies, divided to short (4-20 seconds) video clips
with associated sentence descriptions. A total of 200
movies are available, from which 153, 12 and 17 are used
for training, validation, and testing, respectively (20 more
were used for blind tests, not performed here).

Table 1 present annotation results. We focus primarily
on the CIDEr-D [60] and the BLEU-4 [38] measures, since
they are the only ones that are known to be well correlated
with human perception [44, 60]. Other metrics are pro-
vided here for completeness. These measures are: BLEU1–
3 [38], METEOR [7], and ROUGE-L [32]. We compare
our method with several published and unpublished sys-
tems. The results include the following three variants of
our pipeline.
Local tessellation. Our baseline system uses per-clip near-
est neighbor matching in the SVS in order to choose refer-
ence semantics. We match each test clip with its closest se-
mantics in the SVS. From Tab. 1, we see that this method al-
ready outperforms previous State-of-the-Art. As reference,
we provide the performance of a similar method which
matches clips in appearance space (Appearance matching).
The substantial gap between the two underscores the impor-
tance of our semantics-appearance similarity matching.
Unsupervised tessellation. The graph-based method for
considering temporal coherence, as presented in Sec. 4.3 is
able to provide a slight improvement in results in compari-
son to the local method (Tab. 1).
Supervised tessellation. The LSTM based model de-
scribed in Sec. 4.4, employing 2,000 LSTM units split be-

GT: SOMEONE serves SOMEONE and SOMEONE.
ST: Now at a restaurant a waitress serves drinks.

GT: Then reaches in her bag and takes out a framed photo
of a silver-haired woman.
ST: He spots a framed photo with SOMEONE in it.

GT: SOMEONE shifts his confused stare.
ST: He shifts his gaze then nods.
Figure 3. Qualitative video captioning results. Three caption
assignments from the test set of the LSMDC16 benchmark. The
Ground Truth captioning is provided along with the result of the
Supervised Tessellation (ST) method.

tween two layers.

This method achieved the overall best performance on
both CIDEr-D and BLEU-4, the metrics known to be most
correlated with human perception [44, 60], outperforming
previous state of the art with a gain of 23% on CIDEr-D.
Qualitative results are provided in Fig. 3.

5.2. Video Summarization

Video summarization performance is evaluated on the
SumMe [13] and TVSum [52] benchmarks. These bench-
marks consist of 25 and 50 raw user videos, each depicting
a certain event. The video frames are hand labeled with an
importance score ranging from 0 (redundant) and 1 (vital)
in SumMe and from 1 (redundant) and 5 (vital) in TVSum.
The videos are about 1-5 minutes in length and the task is
to produce a summary in the form of selected frames which
is up-to 15% of the given video’s length. Sample frames
are shown in Fig. 4. The evaluation metric is the average
f-measure of the predicted summary with the ground truth
annotations. We follow [14, 74] in evaluating with multiple
user annotations.

Similar to video annotation, our approach is to trans-
fer the semantics (represented here by frame importance
values) from the gallery to the tessellated video. Our
method operates without incorporating additional compu-
tational steps, such as optimizing the selected set using the
determinantal point process [28], commonly used for such

6

Figure 4. Sample video summarization results. Sample frames from six videos out of the SumMe benchmark. Each group of four frames
contains two frames (top rows) from short segments that were deemed important by the unsupervised tessellation method and two (bottom
rows) that were dropped out of our summaries.

SumMe TVSum

5.3. Temporal Action Detection

Method
Khosla et al. [22] † ‡
Zhao et al. [75] † ‡
Song et al. [52] †
Gygli et. al [14]
Long Short-Term Memory [74]
Summary Transfer [73]
Local Tessellation
Unsupervised Tessellation
Supervised Tessellation

–
–
–
39.7
39.8
40.9
33.8
41.4
37.2

36.0
46.0
50.0
–
54.7
–
60.9
64.1
63.4

Table 2. Video summarization results on the SumMe [13] and TV-
Sum [52] benchmarks. Shown are the average f-measures. Our
unsupervised tessellation method outperforms previous methods
by a substantial margin. † - unsupervised , ‡ - taken from [74]

applications [3, 11, 73].

Table 2 compares our performance with several recent
reports on the same benchmarks. We again provide results
for all three variants of our system. This time, the local and
the supervised tessellation methods are both outperformed
by previous work on SumMe but not on TVSum. Our unsu-
pervised tessellation outperforms other tessellation methods
as well as the state of the art on the summarization bench-
marks by substantial margins.

We believe that unsupervised tessellation worked bet-
ter than supervised because the available training examples
were much fewer than required for the more powerful but
data hungry LSTM. Speciﬁcally, for each benchmark we
used only the labels from the same dataset, without lever-
aging other summarization datasets for this purpose. Do-
ing so, using, e.g., the Open Video Project and YouTube
dataset [6], is left for future work.

We evaluate our method on the task of action detection,

using the THUMOS’14 [20] benchmark for this purpose.

This one of the most recent and, to our knowledge, most
challenging benchmark released for this task. THUMOS’14
consists of a training set of 13,320 temporally trimmed
videos from the action classes of the UCF 101 dataset [53],
a validation set of 1,010 temporally untrimmed videos with
temporal action annotations, a background set with 2,500
relevant videos guaranteed to not include any instance of
the 101 actions and ﬁnally a test set with 1,574 temporally
untrimmed videos. In the temporal action detection bench-
mark, for every action class out of a subset of 20 actions, the
task is to predict both the presence of the action in a given
video and its temporal interval, i.e., the start and end times
of its detected instances.

For each action, the detected intervals are compared
against ground-truth intervals using the Intersection over
Union (IoU) similarity measure. Denoting the predicted in-
tervals by Rp and the ground truth intervals by Rgt, the IoU
similarity is computed as IoU = Rp∩Rgt
Rp∪Rgt

.

A predicted action interval is considered as true posi-
tive, if its IoT measure is above a predeﬁned threshold and
false positive otherwise. Ground truth annotations with no
matching predictions are also counted as false positives.
The Average Precision (AP) for each of the 20 classes
is then computed and the mean Average Precision (mAP)
serves as an overall performance measure. The process re-
peats for different IoT thresholds ranging from 0.1 to 0.5.

Our approach to detection is to tessellate a given
untrimmed video with short atomic clips from the UCF
dataset [53]. With the resulting tessellation, we can de-
termine which action occurred at each time in the video.

7

Figure 5. Sample action detection results. Detection results for the ’Baseball pitch’ class. The predicted intervals by the supervised
tessellation method are shown in green, the ground truth in blue.

Method
Wang et al. [65]
Oneata et al. [35]
Heilbron et al. [2]
Escorcia et al. [9]
Richard and Gall [42]
Shou et al. [48]
Yeung et al. [69]
Yuan et al. [71]
Local tessellation
Unsupervised t.
Supervised t.

0.1
18.2
36.6
–
–
39.7
47.7
48.9
51.4
56.4
57.9
61.1

0.2
17.0
33.6
–
–
35.7
43.5
44.0
42.6
51.2
54.2
56.8

0.3
14.0
27.0
–
–
30.0
36.3
36.0
33.6
43.8
47.3
49.3

0.4
11.7
20.8
–
–
23.2
28.7
26.4
26.1
32.5
35.2
36.5

0.5
8.3
14.4
13.5
13.9
15.2
19.0
17.1
18.8
20.7
22.4
23.3

Table 3. Temporal Action Detection results on the THU-
MOS’14 [20] benchmark. Shown are the mAP of the various
methods for different IoT thresholds. Our proposed framework
outperforms previous State-of-the-Art methods by a large margin.
The supervised tessellation obtains the best results.

Detection results on one sample video are shown in Fig. 5.
Tab. 3 lists the results of the three variants of our frame-
work along with previous results presented on the bench-
mark. As evident from the table, our tessellation method
outperforms the state of the art by a large margin, where
the supervised tessellation achieves the best results among
the three variants of our method. Unsurprisingly, incorpo-
rating context in the tessellation dramatically improves per-
formance, as can be seen from the relatively inferior local
tessellation results.

5.4. Predicting Sounds from Video

Though we show our method to obtain state of the art
results on a number of tasks, at least in one case, a method
tailored for a speciﬁc video analysis task performs better
than our own. Speciﬁcally, we test the capability of our
method to predict sound from video using the Greatest Hits
dataset [37]. This dataset consists of 977 videos of humans
probing different environments with a drumstick: hitting,
scratching, and poking different objects in different scenes.
Each video, on average, contains 48 actions and lasts 35

Method

Full system of [37]
Appearance matching
Local tessellation
Unsupervised tessellation
Supervised tessellation

Loudness
r
Err
0.44
0.21
0.18
0.35
0.32
0.27
0.33
0.26
0.35
0.24

Centroid
r
Err
3.85
0.47
0.36
6.09
0.47
4.83
0.48
4.76
0.46
4.44

Table 4. Greatest Hits benchmark results. Shown are the MSE and
the correlation coefﬁcient for two different success criteria.

seconds. In [37], a CNN followed by an LSTM was used to
predict sounds for each video. Following their protocol, we
consider only the video segments centered on the audio am-
plitude peaks. We employ the published sound features that
are available for 15 frame intervals around each audio peak,
which we take to be our clip size. Each clip C is therefore
associated with the RNN-FV pooled VGG-19 CNN visual
representation, as presented in Sec. 3.1, and with a vector
a ∈ R1,890 of sound features that is simply the concatena-
tion of these 15 sound features.

Matching is performed in a SVS that is constructed from
the visual representation and the matching sound features.
We predict sound features for hit events by applying tessel-
lation and returning the sound feature vector a associated
with the selected reference clips.

There are two criteria that are used for evaluating the re-
sults: Loudness and Centroid. In both cases both the MSE
scores and correlations are reported. Loudness is taken to
be the maximum energy (L2 norm) of the compressed sub-
band envelopes over all timesteps. Centroid is measured by
taking the center of mass of the frequency channels for a
one-frame window around the center of the impact.

Our results are reported in Tab. 4. The importance of
the semantic space as can be observed from the gap be-
tween the appearance only matching to the Local Tessella-
tion method. Leveraging our supervised and unsupervised
tessellation methods improves the results even further. In
three out of four criteria the supervised tessellation seems
preferable to the unsupervised one in this benchmark.

8

6. Conclusions

We present a general approach to understanding and ana-
lyzing videos. Our design transfers per-clip video semantics
from reference, training videos to novel test videos. Three
alternative methods are proposed for this transfer: local tes-
sellation, which uses no context, unsupervised tessellation
which uses dynamic programming to apply temporal, se-
mantic coherency, and supervised tessellation which em-
ploys LSTM to predict future semantics. We show that
these methods, coupled with a recent video representation
technique, provide state of the art results on three very
different video analysis domains: video annotation, video
summarization, and action detection and near state of the
art on a fourth application, sound prediction from video.
Our method is unique in being ﬁrst to obtain state of the art
results on such different video understanding tasks, outper-
forming methods tailored for these applications.

Acknowledgments

This research is supported by the Intel Collaborative Re-

search Institute for Computational Intelligence (ICRI-CI).

References

[1] H. Aradhye, G. Toderici, and J. Yagnik. Video2text: Learn-
ing to annotate video content. In Int. Conf. on Data Mining
Workshops, pages 144–151. IEEE, 2009.

[2] F. Caba Heilbron, J. Carlos Niebles, and B. Ghanem. Fast
temporal activity proposals for efﬁcient detection of human
In Proceedings of the IEEE
actions in untrimmed videos.
Conference on Computer Vision and Pattern Recognition,
pages 1914–1923, 2016.

[3] W.-L. Chao, B. Gong, K. Grauman, and F. Sha. Large-

margin determinantal point processes. UAI, 2015.

[4] D. L. Chen and W. B. Dolan. Collecting highly parallel
data for paraphrase evaluation. In Proc. Annual Meeting of
the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 190–200. Association for Com-
putational Linguistics, 2011.

[5] W.-S. Chu, Y. Song,

Video co-
Jaimes.
summarization:
Video summarization by visual co-
occurrence. In Proc. Conf. Comput. Vision Pattern Recog-
nition, pages 3584–3592, 2015.

and A.

[6] S. E. F. De Avila, A. P. B. Lopes, A. da Luz, and A. de Albu-
querque Ara´ujo. Vsumm: A mechanism designed to produce
static video summaries and a novel evaluation method. Pat-
tern Recognition Letters, 32(1):56–68, 2011.

[7] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language. In In
Proceedings of the Ninth Workshop on Statistical Machine
Translation. Citeseer, 2014.

[8] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual

recognition and description. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 2625–2634, 2015.

[9] V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem.
Daps: Deep action proposals for action understanding.
In
European Conf. Comput. Vision, pages 768–784. Springer,
2016.

[10] B. G. Fabian Caba Heilbron, Victor Escorcia and J. C.
Niebles. Activitynet: A large-scale video benchmark for hu-
man activity understanding. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 961–970, 2015.

[11] B. Gong, W.-L. Chao, K. Grauman, and F. Sha. Diverse
sequential subset selection for supervised video summariza-
In Neural Inform. Process. Syst., pages 2069–2077,
tion.
2014.

[12] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar,
S. Venugopalan, R. Mooney, T. Darrell, and K. Saenko.
Youtube2text: Recognizing and describing arbitrary activi-
ties using semantic hierarchies and zero-shot recognition. In
Proc. Int. Conf. Comput. Vision, pages 2712–2719, 2013.
[13] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool.
In European Conf.

Creating summaries from user videos.
Comput. Vision, pages 505–520. Springer, 2014.

[14] M. Gygli, H. Grabner, and L. Van Gool. Video sum-
marization by learning submodular mixtures of objectives.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
3090–3098, 2015.

[15] T. Hassner. A critical review of action recognition bench-
marks. In Proc. Conf. Comput. Vision Pattern Recognition
Workshops, pages 245–250, 2013.

[16] T. Hassner and C. Liu. Dense Image Correspondences for

Computer Vision. Springer, 2015.

[17] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[18] H. Huang, Y. Lu, F. Zhang, and S. Sun. A multi-modal clus-
tering method for web videos. In Int. Conf. on Trustworthy
Computing and Services, pages 163–169. Springer, 2012.

[19] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. Trans. Pattern Anal.
Mach. Intell., 35(1):221–231, 2013.

[20] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014.

[21] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with con-
volutional neural networks. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 1725–1732, 2014.

[22] A. Khosla, R. Hamid, C.-J. Lin, and N. Sundaresan. Large-
scale video summarization using web-image priors. In Proc.
Conf. Comput. Vision Pattern Recognition, pages 2698–
2705, 2013.

[23] B. Klein, G. Lev, G. Sadeh, and L. Wolf. Associating neu-
ral word embeddings with deep image representations using
ﬁsher vectors. In Proc. Conf. Comput. Vision Pattern Recog-
nition, pages 4437–4446, 2015.

[24] O. Kliper-Gross, Y. Gurovich, T. Hassner, and L. Wolf. Mo-
tion interchange patterns for action recognition in uncon-

9

strained videos. In European Conf. Comput. Vision, pages
256–269. Springer, 2012.

[25] O. Kliper-Gross, T. Hassner, and L. Wolf. The action simi-
larity labeling challenge. Trans. Pattern Anal. Mach. Intell.,
34(3):615–621, 2012.

[26] N. Krishnamoorthy, G. Malkarnenkar, R. J. Mooney,
K. Saenko, and S. Guadarrama. Generating natural-language
In AAAI
video descriptions using text-mined knowledge.
Conf. on Artiﬁcial Intelligence, volume 1, page 2, 2013.
[27] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recognition.
In Proc. Int. Conf. Comput. Vision, pages 2556–2563. IEEE,
2011.

[28] A. Kulesza and B. Taskar. Determinantal point processes for

machine learning. arXiv preprint arXiv:1207.6083, 2012.

[29] I. Laptev. On space-time interest points. 64(2-3):107–123,

2005.

[30] G. Lev, G. Sadeh, B. Klein, and L. Wolf. RNN ﬁsher vectors
for action recognition and image annotation. arXiv preprint
arXiv:1512.03958, 2015.

[31] Y. Li, W. Li, V. Mahadevan, and N. Vasconcelos. Vlad3:
Encoding dynamics of deep features for action recognition.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
1951–1960, 2016.

[32] C.-Y. Lin and F. J. Och. Automatic evaluation of machine
translation quality using longest common subsequence and
skip-bigram statistics. In Proc. Annual Meeting on Associ-
ation for Computational Linguistics, page 605. Association
for Computational Linguistics, 2004.

[33] C. Liu, J. Yuen, and A. Torralba. SIFT ﬂow: Dense corre-
spondence across scenes and its applications. Trans. Pattern
Anal. Mach. Intell., 33(5):978–994, 2011.

[34] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013.

[35] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. 2014.

[36] P. Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, B. Shaw,
A. F. Smeaton, and G. Qu´enot. TRECVID 2012–an overview
of the goals, tasks, data, evaluation mechanisms and metrics.
In Proceedings of TRECVID, 2012.

[37] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adel-
son, and W. T. Freeman. Visually indicated sounds. In Proc.
Conf. Comput. Vision Pattern Recognition, pages 2405–
2413, 2016.

[38] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU:
a method for automatic evaluation of machine translation.
In Proc. annual meeting on association for computational
linguistics, pages 311–318. Association for Computational
Linguistics, 2002.

[39] F. Perronnin, J. S´anchez, and T. Mensink.

Improving the
In Euro-
ﬁsher kernel for large-scale image classiﬁcation.
pean Conf. Comput. Vision, pages 143–156. Springer, 2010.
[40] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid.
Category-speciﬁc video summarization. In European Conf.
Comput. Vision, pages 540–555. Springer, 2014.

[41] L. R. Rabiner. A tutorial on hidden Markov models and se-
lected applications in speech recognition. Proceedings of the
IEEE, 77(2):257–286, 1989.

[42] A. Richard and J. Gall. Temporal action detection using a
statistical language model. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3131–3140, 2016.

[43] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele. A
dataset for movie description. In Proc. Conf. Comput. Vision
Pattern Recognition, 2015.

[44] A. Rohrbach, A. Torabi, T. Maharaj, M. Rohrbach, C. Pal,
A. Courville, and B. Schiele. The large scale movie descrip-
tion and understanding challenge (LSMDC 2016), howpub-
lished = Available: http://tinyurl.com/zabh4et,
month = September, year = 2016.

[45] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, P. Chris,
L. Hugo, C. Aaron, and B. Schiele. Movie description. arXiv
preprint, 2016.

[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge. Int.
J. Comput. Vision, 115(3):211–252, 2015.

[47] R. Shetty and J. Laaksonen. Video captioning with recurrent
networks based on frame-and video-level features and vi-
sual content classiﬁcation. arXiv preprint arXiv:1512.02949,
2015.

[48] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
In Pro-
ization in untrimmed videos via multi-stage cnns.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1049–1058, 2016.

[49] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep ﬁsher net-
works for large-scale image classiﬁcation. In Neural Inform.
Process. Syst., pages 163–171, 2013.

[50] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In Neural Inform.
Process. Syst., pages 568–576, 2014.

[51] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[52] Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes. Tvsum:
Summarizing web videos using titles. In Proc. Conf. Com-
put. Vision Pattern Recognition, pages 5179–5187, 2015.
[53] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[54] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsu-
pervised learning of video representations using LSTMs. In
Int. Conf. Mach. Learning, volume 2, 2015.

[55] V. Sydorov, M. Sakurada, and C. H. Lampert. Deep ﬁsher
kernels-end to end learning of the ﬁsher kernel GMM param-
In Proc. Conf. Comput. Vision Pattern Recognition,
eters.
pages 1402–1409, 2014.

[56] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Con-
volutional learning of spatio-temporal features. In European
Conf. Comput. Vision, pages 140–153. Springer, 2010.
[57] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko,
and R. J. Mooney. Integrating language and vision to gen-

10

[74] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Video sum-
marization with long short-term memory. In European Conf.
Comput. Vision, 2016.

[75] B. Zhao and E. P. Xing. Quasi real-time summarization for
In Proc. Conf. Comput. Vision Pattern

consumer videos.
Recognition, pages 2513–2520, 2014.

erate natural language descriptions of videos in the wild. In
COLING, volume 2, page 9, 2014.

[58] A. Torabi, C. Pal, H. Larochelle, and A. Courville. Using
descriptive video services to create a large data source for
video annotation research. arXiv preprint, 2015.

[59] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In Proc. Int. Conf. Comput. Vision, pages 4489–4497,
2015.

[60] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
In Proc.
Consensus-based image description evaluation.
Conf. Comput. Vision Pattern Recognition, pages 4566–
4575, 2015.

[61] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney,
T. Darrell, and K. Saenko. Sequence to sequence-video to
In Proc. Conf. Comput. Vision Pattern Recognition,
text.
pages 4534–4542, 2015.

[62] H. D. Vinod. Canonical ridge and econometrics of joint pro-
duction. Journal of Econometrics, 4(2):147–166, May 1976.
[63] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: A neural image caption generator. In Proc. Conf. Com-
put. Vision Pattern Recognition, pages 3156–3164, 2015.
[64] H. Wang and C. Schmid. Action recognition with improved
trajectories. In Proc. Int. Conf. Comput. Vision, pages 3551–
3558, 2013.

[65] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features. THU-
MOS14 Action Recognition Challenge, 1:2, 2014.

[66] L. Wang, Y. Qiao, and X. Tang. Action recognition with
In Proc.
trajectory-pooled deep-convolutional descriptors.
Conf. Comput. Vision Pattern Recognition, pages 4305–
4314, 2015.

[67] S. Wei, Y. Zhao, Z. Zhu, and N. Liu. Multimodal fusion
for video search reranking. Trans. on Knowledge and Data
Engineering, 22(8):1191–1199, 2010.

[68] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle,
and A. Courville. Describing videos by exploiting temporal
structure. In Proc. Int. Conf. Comput. Vision, pages 4507–
4515, 2015.

[69] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2678–2687, 2016.
[70] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and
arXiv preprint

retrieval models with semantic attention.
arXiv:1610.02947, 2016.

[71] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal ac-
tion localization with pyramid of score distribution features.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
3093–3102, 2016.

[72] H. J. Zhang, J. Wu, D. Zhong, and S. W. Smoliar. An in-
tegrated system for content-based video retrieval and brows-
ing. Pattern recognition, 30(4):643–658, 1997.

[73] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Summary
transfer: Exemplar-based subset selection for video summa-
rizatio. In Proc. Conf. Comput. Vision Pattern Recognition,
2016.

11

Temporal Tessellation: A Uniﬁed Approach for Video Analysis

Dotan Kaufman1, Gil Levi1, Tal Hassner2,3, and Lior Wolf1,4

1The Blavatnik School of Computer Science , Tel Aviv University, Israel
2Information Sciences Institute , USC , CA, USA
3The Open University of Israel, Israel
4Facebook AI Research

7
1
0
2
 
r
p
A
 
4
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
5
9
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We present a general approach to video understanding,
inspired by semantic transfer techniques that have been suc-
cessfully used for 2D image analysis. Our method con-
siders a video to be a 1D sequence of clips, each one as-
sociated with its own semantics. The nature of these se-
mantics – natural language captions or other labels – de-
pends on the task at hand. A test video is processed by
forming correspondences between its clips and the clips of
reference videos with known semantics, following which,
reference semantics can be transferred to the test video.
We describe two matching methods, both designed to en-
sure that (a) reference clips appear similar to test clips and
(b), taken together, the semantics of the selected reference
clips is consistent and maintains temporal coherence. We
use our method for video captioning on the LSMDC’16
benchmark, video summarization on the SumMe and TV-
Sum benchmarks, Temporal Action Detection on the Thu-
mos2014 benchmark, and sound prediction on the Greatest
Hits benchmark. Our method not only surpasses the state
of the art, in four out of ﬁve benchmarks, but importantly, it
is the only single method we know of that was successfully
applied to such a diverse range of tasks.

1. Introduction

Despite decades of research, video understanding still
challenges computer vision. The reasons for this are many,
and include the hurdles of collecting, labeling and process-
ing video data, which is typically much larger yet less abun-
dant than images. Another reason is the inherent ambiguity
of actions in videos which often defy attempts to attach di-
chotomic labels to video sequences [25]

Rather than attempting to assign videos with single ac-
tion labels (in the same way that 2D images are assigned

Figure 1. Tessellation for temporal coherence. For video cap-
tioning, given a query video (top), we seek reference video clips
with similar semantics. Our tessellation ensures that the semantics
assigned to the test clip are not only the most relevant (the ﬁve
options for each clip) but also preserve temporal coherence (green
path). Ground truth captions are provided in blue.

object classes in, say, the ImageNet collection [46]) an in-
creasing number of efforts focus on other representations
for the semantics of videos. One popular approach as-
signs videos with natural language text annotations which
describe the events taking place in the video [4, 43]. Sys-
tems are then designed to automatically predict these anno-
tations. Others attach video sequences with numeric values
indicating what parts of the video are more interesting or
important [13]. Machine vision is then expected to deter-
mine the importance of each part of the video and summa-
rize videos by keeping only their most important parts.

Although impressive progress was made on these and
other video understanding problems, this progress was often
made disjointedly: separate specialized systems were uti-
lized that were tailored to obtain state of the art performance
on different video understanding problems. Still lacking is
a uniﬁed general approach to solving these different tasks.
Our approach is inspired by recent 2D dense correspon-
dence estimation methods (e.g., [16, 33]). These methods

1

were successfully shown to solve a variety of image un-
derstanding problems by transferring per-pixel semantics
from reference images to query images. This general ap-
proach was effectively applied to a variety of tasks, includ-
ing single view depth estimation, semantic segmentation
and more. We take an analogous approach, applying similar
techniques to 1D video sequences rather than 2D images.

Speciﬁcally, image based methods combine local, per-
pixel appearance similarity with global, spatial smoothness.
We instead combine local, per-region appearance similarity
with global semantics smoothness, or temporal coherence.
Fig. 1 offers an example of this, showing how temporal co-
herence improves the text captions assigned to a video.

Our contributions are as follows: (a) We describe a novel
method for matching test video clips to reference clips. Ref-
erences are assumed to be associated with semantics rep-
resenting the task at hand. Therefore, by this matching
we transfer semantics from reference to test videos. This
process seeks to match clips which share similar appear-
ances while maintaining semantic coherency between the
assigned reference clips. (b) We discuss two techniques for
maintaining temporal coherency: the ﬁrst uses unsupervised
learning for this purpose whereas the second is supervised.
Finally, (c), we show that our method is general by pre-
senting state of the art results on three recent and challeng-
ing video understanding tasks, previously addressed sepa-
rately: Video caption generation on the LSMDC’16 bench-
mark [45], video summarization on the SumMe [13] and
TVSum [52] benchmarks, and action detection on the THU-
In addition, we report results
MOS’14 benchmark [20].
comparable to the state of the art on the Greatest Hits bench-
mark [37] for sound prediction from video. Importantly, we
will publicly release our code and models.1

2. Related work

Video annotation. Signiﬁcant progress was made in the
relatively short time since work on video annotation / cap-
tion generation began. Early methods such as [1, 18, 36, 67]
attempted to cluster captions and videos and applied this
for video retrieval. Others [12, 26, 57] generated sentence
representations by ﬁrst identifying semantic video content
(e.g., verb, noun, etc.) using classiﬁers tailored for particu-
lar objects and events. They then produce template based
sentences. This approach, however, does not scale well,
since it requires substantial efforts to provide suitable train-
ing data for the classiﬁers, as well as limits the possible
sentences that the model can produce.

More recently, and following the success of image an-
notation systems based on deep networks such as [8, 63],
similar techniques were applied to videos [8, 54, 61, 68].
Whereas image based methods used convolutional neural

1See: github.com/dot27/temporal-tessellation

networks (CNN) for this purpose, application to video in-
volve temporal data, which led to the use of recurrent neural
networks (RNN), particularly short-term memory networks
(LSTM) [17]. We also use CNN and LSTM models but in
fundamentally different ways, as we later explain in Sec. 4.

Video summarization. This task involves selecting the
subset of a query video’s frames which represents its most
important content. Early methods developed for this pur-
pose relied on manually speciﬁed cues for determining
which parts of a video are important and should be retained.
A few such examples include [5, 40, 52, 72].

More recently,

the focus shifted towards supervised
learning methods [11, 13, 14, 73], which assume that train-
ing videos also provide manually speciﬁed labels indicat-
ing the importance of different video scenes. These meth-
ods sometimes use multiple individual-tailored decisions to
choose video portions for the summary [13, 14] and often
rely on the determinantal point process (DPP) in order to
increase the diversity of selected video subsets [3, 11, 73].
Unlike video description, LSTM based methods were
only considered for summarization very recently [74].
Their use of LSTM is also very different from ours.

Temporal action detection. Early work on video ac-
tion recognition relied on hand crafted space-time fea-
tures [24, 29, 64]. More recently, deep methods have been
proposed [19, 21, 56], many of which learn deep visual and
motion features [31, 50, 59, 66]. Along with the devel-
opment of stronger methods, larger and more challenging
benchmarks were proposed [15, 25, 27, 53]. Most datasets,
however, used trimmed, temporally segmented videos, i.e:
short clips which contain only a single action.

Recently, similar to the shift toward classiﬁcation com-
bined with localization in object recognition, some of the
focus shifted toward more challenging and realistic sce-
narios of classifying untrimmed videos [10, 20]. In these
datasets, a given video can be up to a few minutes in length,
different actions occur at different times in the video and
in some parts of the video no clear action occurs. These
datasets are also used for classiﬁcation, i.e. determining the
main action taking place in the video. A more challenging
task, however, is the combination of classiﬁcation with tem-
poral detection: determining which action, if any, is taking
place at each time interval in the video.

In order to tackle temporal action detection in untrimmed
videos, Yuan et al. [71] encode visual features at different
temporal resolutions followed by a classiﬁer to obtain clas-
siﬁcation scores at different time scales. Escorcia et al [9]
focus instead on a fast method for obtaining action pro-
posals from untrimmed videos, which later can be fed to
an action classiﬁer. Instead of using action classiﬁers, our
method relies on matching against a gallery of temporally
segmented action clips.

2

3. Preliminaries

Our approach assumes that test videos are partitioned
into clips. It then matches each test clip with a reference
(training) clip. Matching is performed with two goals in
mind. First, at the clip level, we select reference clips which
are visually similar to the input. Second, at the video level,
we select a sequence of clips which best preserves the tem-
poral semantic coherency. Taken in sequence, the order of
selected, reference semantics should adhere to the temporal
manner in which they appeared in the training videos.

Following this step, the semantics associated with se-
lected reference clips can be transferred to test clips. This
allows us to reason about the test video using information
from our reference. This approach is general, since it al-
lows for different types of semantics to be stored and trans-
ferred from reference, training videos to the test videos.
This can include, in particular, textual annotations, action
labels, manual annotations of interesting frames and others.
Thus, different semantics represent different video under-
standing problems which our method can be used to solve.

3.1. Encoding video content

We assume that training and test videos are partitioned
into sequences of clips. A clip C consists of a few consec-
utive frames Ii, i ∈ 1..n where n is the number of frames
in the clip. Our tessellation approach is agnostic to the par-
ticular method chosen to represent these clips. Of course,
The more robust and discriminative the representation, the
better we expect our results to be. We, therefore, chose the
following two step process, based on the recent state of the
art video representations of [30].

Step 1: Representing a single frame. Given a frame Ii
we use an off the shelf CNN to encode its appearance. We
found the VGG-19 CNN to be well suited for this pur-
pose. This network was recently proposed in [51] and used
to obtain state of the art results on the ImageNet, large
scale image recognition benchmark (ILSVRC) [46]. In their
work, [51] used the last layer of this network to predict Im-
ageNet class labels, represented as one-hot encodings. We
instead treat this network as a feature transform function
f : I (cid:55)→ a(cid:48) which for image (frame) I returns the 4, 096D
response vector from the penultimate layer of the network.
To provide robustness to local translations, we extract
these features by oversampling: I is cropped ten times at
different offsets around the center of the frame. These
cropped frames are normalized by subtracting the mean
value of each color channel and then fed to the network.
Finally, the ten 4, 096D response vectors returned by the
network are pooled into a single vector by element-wise
averaging. Principle component analysis (PCA) is further
used to reduce the dimensionality of these features to 500D,
giving us the ﬁnal, per frame representation a ∈ R500.

Step 2: Representing multiple frames. Once the frames
are encoded, we pool them to obtain a representation for
the entire clip. Pooling is performed by Recurrent Neural
Network Fisher Vector (RNN-FV) encoding [30].

Speciﬁcally, We use their RNN, trained to predict the
feature encoding of a future frame, ai, given the encodings
for its k preceding frames, (ai−k, ..., ai−1). This RNN was
trained on the training set from the Large Scale Movie De-
scription Challenge [45], containing roughly 100K videos.
We apply the RNN-FV to the representations produced for
all of the frames in the clip. The gradient of the last layer of
this RNN is then taken as a 100,500D representation for the
entire sequence of frames in C. We again use PCA for di-
mensionality reduction, this time mapping the features pro-
duced by the RNN-FV to 2,000D dimensions, resulting in
our pooled representation A ∈ R2,000. We refer to [30] for
more details about this process.

3.2. Encoding semantics

As previously mentioned, the nature of the semantics
associated with a video depends on the task at hand. For
tasks such as action detection and video summarization, for
which the supervision signal is of low dimension, the se-
mantic space of the labels has only a few bits of informa-
tion per segment and is not discriminative enough between
segments. In this case, we take the semantic space VS to
be the same as the appearance space VA and take both to
be the pooled representation A.

Textual semantics In video captioning, in which the text
data provides a rich source of information, our method
largely beneﬁts from having a separate semantic represen-
tation that is based on the label data.

We tested several representations for video semantics
and chose the recent Fisher Vector of a Hybrid Gaussian-
Laplacian Mixture Model (FV-HGLMM) [23], since it pro-
vided the best results in our initial cross-validation experi-
ments.

Brieﬂy, we assume a textual semantic representation, s
for a clip C, where s is a string containing natural language
words. We use word2vec [34] to map the sequence of words
in s to a vector of numbers, (s1, ..., sm), where m is the
number of words in s and can be different for different clips.
FV-HGLMM then maps this sequence of numbers to a vec-
tor S ∈ RM of ﬁxed dimensionality, M .

FV-HGLMM is based on the well-known Fisher Vectors
(FV) [39, 49, 55]. The standard Gaussian Mixture Models
(GMM) typically used to produce FV representations are
replaced here with a Hybrid Gaussian-Laplacian Mixture
Model which was shown in [23] to be effective for image
annotation. We refer to that paper for more details.

3

3.3. The joint semantics video space (SVS)

Clip representations and their associated semantics are
all mapped to the joint SVS. We aim to map the appearance
of each clip and its assigned semantics to two neighboring
points in the SVS. By doing so, given an appearance rep-
resentation for a query clip, we can search for potential se-
mantic assignments for this clip in our reference set using
standard Euclidean distance. This property will later be-
come important in Sec. 4.2.

In practice, all clip appearance representations A and
their associated semantic representations S are jointly
mapped to the SVS using regularized Canonical Correlation
Analysis (CCA) [62] where the CCA mapping is trained
using the ground truth semantics provided with each bench-
mark. In our experiments, the CCA regularization parame-
ter is ﬁxed to be a tenth of the largest eigenvalue of the cross
domain covariance matrix computed by CCA. For each clip,
CCA projects A and S (appearance and semantics, respec-
tively) to VA and VS.

4. Tessellation

We assume a data set of training (reference) clips, VA
j ,
and their associated semantics, VS
j , represented as de-
scribed in Sec. 3. Here, j ∈ 1..N indexes the entire data
set of N clips. Since these clips may come from different
videos, j does not necessarily reﬂect temporal order.

Given a test video, we process its clips following 3.1
and 3.3, obtaining a sequence of clip representations, UA
in
i
the SVS, where consecutive index values for i ∈ M , repre-
sent consecutive clips in a test video with M clips. Our goal
is to match each UA
i with a data set semantic representation
VS

ji while optimizing the following two requirements:

1. Semantics-appearance similarity. The representa-
tion for the test clip appearance is similar to the repre-
sentation of the selected semantics.

2. Temporal coherence. The selected semantics are or-
dered similar to their occurrences in the training set.

Drawing on the analogy to spatial correspondence estima-
tion methods such as SIFT ﬂow [33], the ﬁrst requirement is
a data term and the second is a smoothness term, albeit with
two important distinctions: First, the data term matches test
appearances to reference semantics directly, building on the
joint embedding of semantics and appearances in the SVS.
Second, we deﬁne the smoothness term in terms of associ-
ated semantics and not pixel coordinates.

4.1. Local Tessellation

Given the sequence of appearance representations U =
1 , ..., UA
M ) for the test sequence, we seek a correspond-
) (here,

(UA
ing set of reference semantics V = (VS
j1

, ..., VS
jM

again, j indexes the N clips in the reference set). The local
tessellation method employs only the semantics-appearance
similarity. In other words, we associate each test clip UA
i ,
with the following training clip:

V ∗
ji

= arg min
Vj

||UA

i − VS
j ||

(1)

4.2. Tessellation Distribution

We make the Markovian assumption that the semantics
assigned to input clip i, only depend on the appearance of
clip i and the semantics assigned to its preceding clip, i − 1.
This gives the standard factorization of the joint distribution
for the clip appearances and their selected semantics:

P (V, U) =P (VS
j1

)P (UA

1 |VS
j1

)×

(2)

M
(cid:89)

i=2

P (VS
ji

|VS

ji−1

)P (UA

i |VS
ji

).

We set the priors P (VS
) to be the uniform distribution.
j1
Due to our mapping of both appearances and semantics to
the joint SVS, we can deﬁne both posterior probabilities
simply using the L2-norm of these representations:

P (UA

i |VS
|VS

j ) ∝ exp (−||UA
) ∝ exp (−||VS
ji

i − VS
− VS

ji−1

j ||2)

||2)

ji−1

P (VS
ji

(3)

(4)

Ostensibly, We can now apply the standard Viterbi
method [41] to obtain a sequence V which maximizes this
probability.
In practice, we used a slightly modiﬁed ver-
sion of this method, and, when possible, a novel method de-
signed to better exploit our training data to predict database
matches. These are explained below.

4.3. Restricted Viterbi Method.

Given the test clip appearance representations U, the

Viterbi method provides an assignment V ∗ such that,

V ∗ = arg max

P (V, U).

V

(5)

i |VS

We found that in practice P (UA
j ) is a long-tail distri-
bution, with only a few dataset elements VS
j near enough to
any UA
i for their probability to be more than near-zero. We,
therefore, restrict the Viterbi method in two ways. First,
we consider only the r(cid:48) = 5 nearest neighboring database
semantics features. Second, we apply a threshold on the
probability of our data term, Eq. (3), and do not consider
semantics VS
j falling below this threshold, except for the
ﬁrst nearest neighbor. Therefore, the number of available
assignments for each clip is 1 ≤ r ≤ 5. This process is
illustrated in Figure 2 (left).

4

Figure 2. Our two non-local tessellations. Left: Tessellation by restricted Viterbi. For a query video (top), our method ﬁnds visually
similar videos and selects the clips that preserve temporal coherence using the Viterbi Method. The ground truth captions are shown in
blue, the closest caption is shown in pink. Note that our method does not always select clips with the closest captions but the ones that best
preserve temporal coherence. Right: Tessellation by predicting the dynamics of semantics. Given a query video (top) and a previous clip
selection, we use an LSTM to predict the most accurate semantics for the next clip.

Method
BUPT CIST AI lab∗
IIT Kanpur∗
Aalto University∗
Shetty and Laaksonen [47]
Yu et al [70]
S2VT [61]
Appearance Matching
Local Tessellation
Unsupervised Tessellation
Supervised Tessellation

CIDEr-D BLEU-4 BLEU-1 BLEU-2 BLEU-3 METEOR ROUGE
.047
.003
.001
.024
.049
.051
.026
.042
.043
.044

.075
.070
.033
.046
.070
.070
.046
.056
.055
.057

.152
.138
.069
.108
.149
.157
.110
.130
.137
.135

.151
.116
.007
.119
.157
.162
.118
.144
.146
.151

.072
.042
.037
.044
.082
.088
.042
.098
.102
.109

.005
.004
.002
.003
.007
.007
.003
.007
.007
.008

.013
.011
.005
.007
.017
.017
.008
.016
.016
.017

Table 1. Video annotation results on the LSMDC’16 challenge [45]. CIDEr-D and BLEU-4 values were found to be the most correlated
with human annotations in [44, 60]. Our results on these metrics far outperform others. * Denotes results which appear in the online
challenge result board, but were never published. They are included here as reference.

4.4. Predicting the Dynamics of Semantics

The Viterbi method of Sec. 4.3 is efﬁcient and requires
only unsupervised training. Its use of the smoothness term
of Eq. (3), however, results in potentially constant semantic
assignments, where for any ji, VS

ji can equal VS

ji−1.

In cases where reference clips are abundant and come
from continuous video sources, we provide a more effective
method of ensuring smoothness. This is done by supervised
learning of how the semantic labels associated with video
clips change over time, and by using that to predict the as-
signment VS

ji for UA
i .

Our process is illustrated in Fig. 2 (right). We train an
LSTM RNN [17] on the semantic and appearance represen-
tations of the training set video clips. We use this network
as a function:

g(VS

0 , VS

1 , ..., VS

1 , ..., UA

i−1, UA

i ) = VS
i ,

i−1, UA
VS

0 = 0,

which predicts the semantic representation VS
i for the clip
at time i given the semantic representation, VS
i−1, assigned
to the preceding clip and the appearance of the current clip,
UA
i . The labeled examples used to train g are taken from the
training set, following the processing described in Sec. 3.2
and 3.3 in order to produce 2,000D post-CCA vectors. Each
pair of previous ground truth semantics and current clip ap-
pearance in the training data provides one sample for train-
ing the LSTM. We employ two hidden layers, each with
1, 000 LSTM cells. The output, which predicts the seman-
tics of the next clip, is also 2,000D.

Given a test video, we begin by processing it as in
Sec. 4.3. In particular, for each of its clip representations
UA
i , we select r ≤ 5 nearest neighboring semantics from
the training set. At each time step i, we feed the clip and its
assigned semantics from the preceding clip at time i − 1 to
our LSTM predictor g. We thus obtain an estimate for the
semantics we expect to see at time i, ˆVS
i .
Of course, the predicted vector ˆVS
i cannot necessarily

(6)

5

be interpreted as a semantic label: not all points in the SVS
have semantic interpretations (in fact, many instead repre-
sent appearance information and more do not represent any-
thing at all). We thus choose a representation VS
ji out of the
r selected for this clip, such that || ˆVS
||2 is smallest.

i − VS
ji

5. Experiments

We apply our method to four separate video understand-
ing tasks: video annotation, video summarization, temporal
action detection, and sound prediction.
Importantly, pre-
vious work was separately tailored to each of these tasks;
we are unaware of any previously proposed single method
which reported results on such a diverse range of video un-
derstanding problems. Contrary to the others, our method
was applied to all of these tasks similarly.

5.1. Video Annotation

In our annotation experiments, we used the movie
annotation benchmark deﬁned by the 2016 Large
Scale Movie Description and Understanding Challenge
(LSMDC16) [45]. LSMDC16 presents a uniﬁed version
of
the recently published large-scale movie datasets,
M-VAD [58] and MPII-MD [43]. The joint dataset contains
188 movies, divided to short (4-20 seconds) video clips
with associated sentence descriptions. A total of 200
movies are available, from which 153, 12 and 17 are used
for training, validation, and testing, respectively (20 more
were used for blind tests, not performed here).

Table 1 present annotation results. We focus primarily
on the CIDEr-D [60] and the BLEU-4 [38] measures, since
they are the only ones that are known to be well correlated
with human perception [44, 60]. Other metrics are pro-
vided here for completeness. These measures are: BLEU1–
3 [38], METEOR [7], and ROUGE-L [32]. We compare
our method with several published and unpublished sys-
tems. The results include the following three variants of
our pipeline.
Local tessellation. Our baseline system uses per-clip near-
est neighbor matching in the SVS in order to choose refer-
ence semantics. We match each test clip with its closest se-
mantics in the SVS. From Tab. 1, we see that this method al-
ready outperforms previous State-of-the-Art. As reference,
we provide the performance of a similar method which
matches clips in appearance space (Appearance matching).
The substantial gap between the two underscores the impor-
tance of our semantics-appearance similarity matching.
Unsupervised tessellation. The graph-based method for
considering temporal coherence, as presented in Sec. 4.3 is
able to provide a slight improvement in results in compari-
son to the local method (Tab. 1).
Supervised tessellation. The LSTM based model de-
scribed in Sec. 4.4, employing 2,000 LSTM units split be-

GT: SOMEONE serves SOMEONE and SOMEONE.
ST: Now at a restaurant a waitress serves drinks.

GT: Then reaches in her bag and takes out a framed photo
of a silver-haired woman.
ST: He spots a framed photo with SOMEONE in it.

GT: SOMEONE shifts his confused stare.
ST: He shifts his gaze then nods.
Figure 3. Qualitative video captioning results. Three caption
assignments from the test set of the LSMDC16 benchmark. The
Ground Truth captioning is provided along with the result of the
Supervised Tessellation (ST) method.

tween two layers.

This method achieved the overall best performance on
both CIDEr-D and BLEU-4, the metrics known to be most
correlated with human perception [44, 60], outperforming
previous state of the art with a gain of 23% on CIDEr-D.
Qualitative results are provided in Fig. 3.

5.2. Video Summarization

Video summarization performance is evaluated on the
SumMe [13] and TVSum [52] benchmarks. These bench-
marks consist of 25 and 50 raw user videos, each depicting
a certain event. The video frames are hand labeled with an
importance score ranging from 0 (redundant) and 1 (vital)
in SumMe and from 1 (redundant) and 5 (vital) in TVSum.
The videos are about 1-5 minutes in length and the task is
to produce a summary in the form of selected frames which
is up-to 15% of the given video’s length. Sample frames
are shown in Fig. 4. The evaluation metric is the average
f-measure of the predicted summary with the ground truth
annotations. We follow [14, 74] in evaluating with multiple
user annotations.

Similar to video annotation, our approach is to trans-
fer the semantics (represented here by frame importance
values) from the gallery to the tessellated video. Our
method operates without incorporating additional compu-
tational steps, such as optimizing the selected set using the
determinantal point process [28], commonly used for such

6

Figure 4. Sample video summarization results. Sample frames from six videos out of the SumMe benchmark. Each group of four frames
contains two frames (top rows) from short segments that were deemed important by the unsupervised tessellation method and two (bottom
rows) that were dropped out of our summaries.

SumMe TVSum

5.3. Temporal Action Detection

Method
Khosla et al. [22] † ‡
Zhao et al. [75] † ‡
Song et al. [52] †
Gygli et. al [14]
Long Short-Term Memory [74]
Summary Transfer [73]
Local Tessellation
Unsupervised Tessellation
Supervised Tessellation

–
–
–
39.7
39.8
40.9
33.8
41.4
37.2

36.0
46.0
50.0
–
54.7
–
60.9
64.1
63.4

Table 2. Video summarization results on the SumMe [13] and TV-
Sum [52] benchmarks. Shown are the average f-measures. Our
unsupervised tessellation method outperforms previous methods
by a substantial margin. † - unsupervised , ‡ - taken from [74]

applications [3, 11, 73].

Table 2 compares our performance with several recent
reports on the same benchmarks. We again provide results
for all three variants of our system. This time, the local and
the supervised tessellation methods are both outperformed
by previous work on SumMe but not on TVSum. Our unsu-
pervised tessellation outperforms other tessellation methods
as well as the state of the art on the summarization bench-
marks by substantial margins.

We believe that unsupervised tessellation worked bet-
ter than supervised because the available training examples
were much fewer than required for the more powerful but
data hungry LSTM. Speciﬁcally, for each benchmark we
used only the labels from the same dataset, without lever-
aging other summarization datasets for this purpose. Do-
ing so, using, e.g., the Open Video Project and YouTube
dataset [6], is left for future work.

We evaluate our method on the task of action detection,

using the THUMOS’14 [20] benchmark for this purpose.

This one of the most recent and, to our knowledge, most
challenging benchmark released for this task. THUMOS’14
consists of a training set of 13,320 temporally trimmed
videos from the action classes of the UCF 101 dataset [53],
a validation set of 1,010 temporally untrimmed videos with
temporal action annotations, a background set with 2,500
relevant videos guaranteed to not include any instance of
the 101 actions and ﬁnally a test set with 1,574 temporally
untrimmed videos. In the temporal action detection bench-
mark, for every action class out of a subset of 20 actions, the
task is to predict both the presence of the action in a given
video and its temporal interval, i.e., the start and end times
of its detected instances.

For each action, the detected intervals are compared
against ground-truth intervals using the Intersection over
Union (IoU) similarity measure. Denoting the predicted in-
tervals by Rp and the ground truth intervals by Rgt, the IoU
similarity is computed as IoU = Rp∩Rgt
Rp∪Rgt

.

A predicted action interval is considered as true posi-
tive, if its IoT measure is above a predeﬁned threshold and
false positive otherwise. Ground truth annotations with no
matching predictions are also counted as false positives.
The Average Precision (AP) for each of the 20 classes
is then computed and the mean Average Precision (mAP)
serves as an overall performance measure. The process re-
peats for different IoT thresholds ranging from 0.1 to 0.5.

Our approach to detection is to tessellate a given
untrimmed video with short atomic clips from the UCF
dataset [53]. With the resulting tessellation, we can de-
termine which action occurred at each time in the video.

7

Figure 5. Sample action detection results. Detection results for the ’Baseball pitch’ class. The predicted intervals by the supervised
tessellation method are shown in green, the ground truth in blue.

Method
Wang et al. [65]
Oneata et al. [35]
Heilbron et al. [2]
Escorcia et al. [9]
Richard and Gall [42]
Shou et al. [48]
Yeung et al. [69]
Yuan et al. [71]
Local tessellation
Unsupervised t.
Supervised t.

0.1
18.2
36.6
–
–
39.7
47.7
48.9
51.4
56.4
57.9
61.1

0.2
17.0
33.6
–
–
35.7
43.5
44.0
42.6
51.2
54.2
56.8

0.3
14.0
27.0
–
–
30.0
36.3
36.0
33.6
43.8
47.3
49.3

0.4
11.7
20.8
–
–
23.2
28.7
26.4
26.1
32.5
35.2
36.5

0.5
8.3
14.4
13.5
13.9
15.2
19.0
17.1
18.8
20.7
22.4
23.3

Table 3. Temporal Action Detection results on the THU-
MOS’14 [20] benchmark. Shown are the mAP of the various
methods for different IoT thresholds. Our proposed framework
outperforms previous State-of-the-Art methods by a large margin.
The supervised tessellation obtains the best results.

Detection results on one sample video are shown in Fig. 5.
Tab. 3 lists the results of the three variants of our frame-
work along with previous results presented on the bench-
mark. As evident from the table, our tessellation method
outperforms the state of the art by a large margin, where
the supervised tessellation achieves the best results among
the three variants of our method. Unsurprisingly, incorpo-
rating context in the tessellation dramatically improves per-
formance, as can be seen from the relatively inferior local
tessellation results.

5.4. Predicting Sounds from Video

Though we show our method to obtain state of the art
results on a number of tasks, at least in one case, a method
tailored for a speciﬁc video analysis task performs better
than our own. Speciﬁcally, we test the capability of our
method to predict sound from video using the Greatest Hits
dataset [37]. This dataset consists of 977 videos of humans
probing different environments with a drumstick: hitting,
scratching, and poking different objects in different scenes.
Each video, on average, contains 48 actions and lasts 35

Method

Full system of [37]
Appearance matching
Local tessellation
Unsupervised tessellation
Supervised tessellation

Loudness
r
Err
0.44
0.21
0.18
0.35
0.32
0.27
0.33
0.26
0.35
0.24

Centroid
r
Err
3.85
0.47
0.36
6.09
0.47
4.83
0.48
4.76
0.46
4.44

Table 4. Greatest Hits benchmark results. Shown are the MSE and
the correlation coefﬁcient for two different success criteria.

seconds. In [37], a CNN followed by an LSTM was used to
predict sounds for each video. Following their protocol, we
consider only the video segments centered on the audio am-
plitude peaks. We employ the published sound features that
are available for 15 frame intervals around each audio peak,
which we take to be our clip size. Each clip C is therefore
associated with the RNN-FV pooled VGG-19 CNN visual
representation, as presented in Sec. 3.1, and with a vector
a ∈ R1,890 of sound features that is simply the concatena-
tion of these 15 sound features.

Matching is performed in a SVS that is constructed from
the visual representation and the matching sound features.
We predict sound features for hit events by applying tessel-
lation and returning the sound feature vector a associated
with the selected reference clips.

There are two criteria that are used for evaluating the re-
sults: Loudness and Centroid. In both cases both the MSE
scores and correlations are reported. Loudness is taken to
be the maximum energy (L2 norm) of the compressed sub-
band envelopes over all timesteps. Centroid is measured by
taking the center of mass of the frequency channels for a
one-frame window around the center of the impact.

Our results are reported in Tab. 4. The importance of
the semantic space as can be observed from the gap be-
tween the appearance only matching to the Local Tessella-
tion method. Leveraging our supervised and unsupervised
tessellation methods improves the results even further. In
three out of four criteria the supervised tessellation seems
preferable to the unsupervised one in this benchmark.

8

6. Conclusions

We present a general approach to understanding and ana-
lyzing videos. Our design transfers per-clip video semantics
from reference, training videos to novel test videos. Three
alternative methods are proposed for this transfer: local tes-
sellation, which uses no context, unsupervised tessellation
which uses dynamic programming to apply temporal, se-
mantic coherency, and supervised tessellation which em-
ploys LSTM to predict future semantics. We show that
these methods, coupled with a recent video representation
technique, provide state of the art results on three very
different video analysis domains: video annotation, video
summarization, and action detection and near state of the
art on a fourth application, sound prediction from video.
Our method is unique in being ﬁrst to obtain state of the art
results on such different video understanding tasks, outper-
forming methods tailored for these applications.

Acknowledgments

This research is supported by the Intel Collaborative Re-

search Institute for Computational Intelligence (ICRI-CI).

References

[1] H. Aradhye, G. Toderici, and J. Yagnik. Video2text: Learn-
ing to annotate video content. In Int. Conf. on Data Mining
Workshops, pages 144–151. IEEE, 2009.

[2] F. Caba Heilbron, J. Carlos Niebles, and B. Ghanem. Fast
temporal activity proposals for efﬁcient detection of human
In Proceedings of the IEEE
actions in untrimmed videos.
Conference on Computer Vision and Pattern Recognition,
pages 1914–1923, 2016.

[3] W.-L. Chao, B. Gong, K. Grauman, and F. Sha. Large-

margin determinantal point processes. UAI, 2015.

[4] D. L. Chen and W. B. Dolan. Collecting highly parallel
data for paraphrase evaluation. In Proc. Annual Meeting of
the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 190–200. Association for Com-
putational Linguistics, 2011.

[5] W.-S. Chu, Y. Song,

Video co-
Jaimes.
summarization:
Video summarization by visual co-
occurrence. In Proc. Conf. Comput. Vision Pattern Recog-
nition, pages 3584–3592, 2015.

and A.

[6] S. E. F. De Avila, A. P. B. Lopes, A. da Luz, and A. de Albu-
querque Ara´ujo. Vsumm: A mechanism designed to produce
static video summaries and a novel evaluation method. Pat-
tern Recognition Letters, 32(1):56–68, 2011.

[7] M. Denkowski and A. Lavie. Meteor universal: Language
speciﬁc translation evaluation for any target language. In In
Proceedings of the Ninth Workshop on Statistical Machine
Translation. Citeseer, 2014.

[8] J. Donahue, L. Anne Hendricks,

S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual

recognition and description. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 2625–2634, 2015.

[9] V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem.
Daps: Deep action proposals for action understanding.
In
European Conf. Comput. Vision, pages 768–784. Springer,
2016.

[10] B. G. Fabian Caba Heilbron, Victor Escorcia and J. C.
Niebles. Activitynet: A large-scale video benchmark for hu-
man activity understanding. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 961–970, 2015.

[11] B. Gong, W.-L. Chao, K. Grauman, and F. Sha. Diverse
sequential subset selection for supervised video summariza-
In Neural Inform. Process. Syst., pages 2069–2077,
tion.
2014.

[12] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar,
S. Venugopalan, R. Mooney, T. Darrell, and K. Saenko.
Youtube2text: Recognizing and describing arbitrary activi-
ties using semantic hierarchies and zero-shot recognition. In
Proc. Int. Conf. Comput. Vision, pages 2712–2719, 2013.
[13] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool.
In European Conf.

Creating summaries from user videos.
Comput. Vision, pages 505–520. Springer, 2014.

[14] M. Gygli, H. Grabner, and L. Van Gool. Video sum-
marization by learning submodular mixtures of objectives.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
3090–3098, 2015.

[15] T. Hassner. A critical review of action recognition bench-
marks. In Proc. Conf. Comput. Vision Pattern Recognition
Workshops, pages 245–250, 2013.

[16] T. Hassner and C. Liu. Dense Image Correspondences for

Computer Vision. Springer, 2015.

[17] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 9(8):1735–1780, 1997.

[18] H. Huang, Y. Lu, F. Zhang, and S. Sun. A multi-modal clus-
tering method for web videos. In Int. Conf. on Trustworthy
Computing and Services, pages 163–169. Springer, 2012.

[19] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural
networks for human action recognition. Trans. Pattern Anal.
Mach. Intell., 35(1):221–231, 2013.

[20] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014.

[21] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with con-
volutional neural networks. In Proc. Conf. Comput. Vision
Pattern Recognition, pages 1725–1732, 2014.

[22] A. Khosla, R. Hamid, C.-J. Lin, and N. Sundaresan. Large-
scale video summarization using web-image priors. In Proc.
Conf. Comput. Vision Pattern Recognition, pages 2698–
2705, 2013.

[23] B. Klein, G. Lev, G. Sadeh, and L. Wolf. Associating neu-
ral word embeddings with deep image representations using
ﬁsher vectors. In Proc. Conf. Comput. Vision Pattern Recog-
nition, pages 4437–4446, 2015.

[24] O. Kliper-Gross, Y. Gurovich, T. Hassner, and L. Wolf. Mo-
tion interchange patterns for action recognition in uncon-

9

strained videos. In European Conf. Comput. Vision, pages
256–269. Springer, 2012.

[25] O. Kliper-Gross, T. Hassner, and L. Wolf. The action simi-
larity labeling challenge. Trans. Pattern Anal. Mach. Intell.,
34(3):615–621, 2012.

[26] N. Krishnamoorthy, G. Malkarnenkar, R. J. Mooney,
K. Saenko, and S. Guadarrama. Generating natural-language
In AAAI
video descriptions using text-mined knowledge.
Conf. on Artiﬁcial Intelligence, volume 1, page 2, 2013.
[27] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
Hmdb: a large video database for human motion recognition.
In Proc. Int. Conf. Comput. Vision, pages 2556–2563. IEEE,
2011.

[28] A. Kulesza and B. Taskar. Determinantal point processes for

machine learning. arXiv preprint arXiv:1207.6083, 2012.

[29] I. Laptev. On space-time interest points. 64(2-3):107–123,

2005.

[30] G. Lev, G. Sadeh, B. Klein, and L. Wolf. RNN ﬁsher vectors
for action recognition and image annotation. arXiv preprint
arXiv:1512.03958, 2015.

[31] Y. Li, W. Li, V. Mahadevan, and N. Vasconcelos. Vlad3:
Encoding dynamics of deep features for action recognition.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
1951–1960, 2016.

[32] C.-Y. Lin and F. J. Och. Automatic evaluation of machine
translation quality using longest common subsequence and
skip-bigram statistics. In Proc. Annual Meeting on Associ-
ation for Computational Linguistics, page 605. Association
for Computational Linguistics, 2004.

[33] C. Liu, J. Yuen, and A. Torralba. SIFT ﬂow: Dense corre-
spondence across scenes and its applications. Trans. Pattern
Anal. Mach. Intell., 33(5):978–994, 2011.

[34] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013.

[35] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. 2014.

[36] P. Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, B. Shaw,
A. F. Smeaton, and G. Qu´enot. TRECVID 2012–an overview
of the goals, tasks, data, evaluation mechanisms and metrics.
In Proceedings of TRECVID, 2012.

[37] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adel-
son, and W. T. Freeman. Visually indicated sounds. In Proc.
Conf. Comput. Vision Pattern Recognition, pages 2405–
2413, 2016.

[38] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU:
a method for automatic evaluation of machine translation.
In Proc. annual meeting on association for computational
linguistics, pages 311–318. Association for Computational
Linguistics, 2002.

[39] F. Perronnin, J. S´anchez, and T. Mensink.

Improving the
In Euro-
ﬁsher kernel for large-scale image classiﬁcation.
pean Conf. Comput. Vision, pages 143–156. Springer, 2010.
[40] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid.
Category-speciﬁc video summarization. In European Conf.
Comput. Vision, pages 540–555. Springer, 2014.

[41] L. R. Rabiner. A tutorial on hidden Markov models and se-
lected applications in speech recognition. Proceedings of the
IEEE, 77(2):257–286, 1989.

[42] A. Richard and J. Gall. Temporal action detection using a
statistical language model. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
3131–3140, 2016.

[43] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele. A
dataset for movie description. In Proc. Conf. Comput. Vision
Pattern Recognition, 2015.

[44] A. Rohrbach, A. Torabi, T. Maharaj, M. Rohrbach, C. Pal,
A. Courville, and B. Schiele. The large scale movie descrip-
tion and understanding challenge (LSMDC 2016), howpub-
lished = Available: http://tinyurl.com/zabh4et,
month = September, year = 2016.

[45] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, P. Chris,
L. Hugo, C. Aaron, and B. Schiele. Movie description. arXiv
preprint, 2016.

[46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge. Int.
J. Comput. Vision, 115(3):211–252, 2015.

[47] R. Shetty and J. Laaksonen. Video captioning with recurrent
networks based on frame-and video-level features and vi-
sual content classiﬁcation. arXiv preprint arXiv:1512.02949,
2015.

[48] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
In Pro-
ization in untrimmed videos via multi-stage cnns.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1049–1058, 2016.

[49] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep ﬁsher net-
works for large-scale image classiﬁcation. In Neural Inform.
Process. Syst., pages 163–171, 2013.

[50] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. In Neural Inform.
Process. Syst., pages 568–576, 2014.

[51] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[52] Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes. Tvsum:
Summarizing web videos using titles. In Proc. Conf. Com-
put. Vision Pattern Recognition, pages 5179–5187, 2015.
[53] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[54] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsu-
pervised learning of video representations using LSTMs. In
Int. Conf. Mach. Learning, volume 2, 2015.

[55] V. Sydorov, M. Sakurada, and C. H. Lampert. Deep ﬁsher
kernels-end to end learning of the ﬁsher kernel GMM param-
In Proc. Conf. Comput. Vision Pattern Recognition,
eters.
pages 1402–1409, 2014.

[56] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Con-
volutional learning of spatio-temporal features. In European
Conf. Comput. Vision, pages 140–153. Springer, 2010.
[57] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko,
and R. J. Mooney. Integrating language and vision to gen-

10

[74] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Video sum-
marization with long short-term memory. In European Conf.
Comput. Vision, 2016.

[75] B. Zhao and E. P. Xing. Quasi real-time summarization for
In Proc. Conf. Comput. Vision Pattern

consumer videos.
Recognition, pages 2513–2520, 2014.

erate natural language descriptions of videos in the wild. In
COLING, volume 2, page 9, 2014.

[58] A. Torabi, C. Pal, H. Larochelle, and A. Courville. Using
descriptive video services to create a large data source for
video annotation research. arXiv preprint, 2015.

[59] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.
Learning spatiotemporal features with 3d convolutional net-
works. In Proc. Int. Conf. Comput. Vision, pages 4489–4497,
2015.

[60] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
In Proc.
Consensus-based image description evaluation.
Conf. Comput. Vision Pattern Recognition, pages 4566–
4575, 2015.

[61] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney,
T. Darrell, and K. Saenko. Sequence to sequence-video to
In Proc. Conf. Comput. Vision Pattern Recognition,
text.
pages 4534–4542, 2015.

[62] H. D. Vinod. Canonical ridge and econometrics of joint pro-
duction. Journal of Econometrics, 4(2):147–166, May 1976.
[63] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: A neural image caption generator. In Proc. Conf. Com-
put. Vision Pattern Recognition, pages 3156–3164, 2015.
[64] H. Wang and C. Schmid. Action recognition with improved
trajectories. In Proc. Int. Conf. Comput. Vision, pages 3551–
3558, 2013.

[65] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features. THU-
MOS14 Action Recognition Challenge, 1:2, 2014.

[66] L. Wang, Y. Qiao, and X. Tang. Action recognition with
In Proc.
trajectory-pooled deep-convolutional descriptors.
Conf. Comput. Vision Pattern Recognition, pages 4305–
4314, 2015.

[67] S. Wei, Y. Zhao, Z. Zhu, and N. Liu. Multimodal fusion
for video search reranking. Trans. on Knowledge and Data
Engineering, 22(8):1191–1199, 2010.

[68] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle,
and A. Courville. Describing videos by exploiting temporal
structure. In Proc. Int. Conf. Comput. Vision, pages 4507–
4515, 2015.

[69] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2678–2687, 2016.
[70] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and
arXiv preprint

retrieval models with semantic attention.
arXiv:1610.02947, 2016.

[71] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal ac-
tion localization with pyramid of score distribution features.
In Proc. Conf. Comput. Vision Pattern Recognition, pages
3093–3102, 2016.

[72] H. J. Zhang, J. Wu, D. Zhong, and S. W. Smoliar. An in-
tegrated system for content-based video retrieval and brows-
ing. Pattern recognition, 30(4):643–658, 1997.

[73] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman. Summary
transfer: Exemplar-based subset selection for video summa-
rizatio. In Proc. Conf. Comput. Vision Pattern Recognition,
2016.

11

