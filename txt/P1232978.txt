Using millions of emoji occurrences to learn any-domain representations
for detecting sentiment, emotion and sarcasm

Bjarke Felbo1, Alan Mislove2, Anders Søgaard3, Iyad Rahwan1, Sune Lehmann4

1Media Lab, Massachusetts Institute of Technology
2College of Computer and Information Science, Northeastern University
3Department of Computer Science, University of Copenhagen
4DTU Compute, Technical University of Denmark

ing the distant supervision to a more diverse set of
noisy labels enables the models to learn richer rep-
resentations of emotional content in text, thereby
obtaining better performance on benchmarks for
detecting sentiment, emotions and sarcasm. We
show that the learned representation of a single
pretrained model generalizes across 5 domains.

Table 1: Example sentences scored by our model.
For each text the top ﬁve most likely emojis are
shown with the model’s probability estimates.

Abstract

NLP tasks are often limited by scarcity of
manually annotated data.
In social me-
dia sentiment analysis and related tasks,
researchers have therefore used binarized
emoticons and speciﬁc hashtags as forms
of distant supervision. Our paper shows
that by extending the distant supervision
to a more diverse set of noisy labels, the
models can learn richer representations.
Through emoji prediction on a dataset of
1246 million tweets containing one of 64
common emojis we obtain state-of-the-
art performance on 8 benchmark datasets
within sentiment, emotion and sarcasm de-
tection using a single pretrained model.
Our analyses conﬁrm that the diversity of
our emotional labels yield a performance
improvement over previous distant super-
vision approaches.

1

Introduction

A variety of NLP tasks are limited by scarcity of
manually annotated data. Therefore, co-occurring
emotional expressions have been used for dis-
tant supervision in social media sentiment anal-
ysis and related tasks to make the models learn
useful text representations before modeling these
tasks directly. For instance, the state-of-the-art ap-
proaches within sentiment analysis of social me-
dia data use positive/negative emoticons for train-
ing their models (Deriu et al., 2016; Tang et al.,
2014). Similarly, hashtags such as #anger, #joy,
#happytweet, #ugh, #yuck and #fml have in pre-
vious research been mapped into emotional cate-
gories for emotion analysis (Mohammad, 2012).

Distant supervision on noisy labels often en-
ables a model to obtain better performance on the
In this paper, we show that extend-
target task.

Emojis are not always a direct labeling of emo-
tional content. For instance, a positive emoji may
serve to disambiguate an ambiguous sentence or to
complement an otherwise relatively negative text.
Kunneman et al. (2014) discuss a similar duality
in the use of emotional hashtags such as #nice and
#lame. Nevertheless, our work shows that emo-
jis can be used to classify the emotional content
of texts accurately in many cases. For instance,
our DeepMoji model captures varied usages of the
word ‘love’ as well as slang such as ‘this is the
shit’ being a positive statement (see Table 1). We
provide an online demo at deepmoji.mit.edu to al-
low others to explore the predictions of our model.

Contributions We show how millions of read-
ily available emoji occurrences on Twitter can be
used to pretrain models to learn a richer emotional

7
1
0
2
 
t
c
O
 
7
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
2
5
0
0
.
8
0
7
1
:
v
i
X
r
a

representation than traditionally obtained through
distant supervision. We transfer this knowledge to
the target tasks using a new layer-wise ﬁne-tuning
method, obtaining improvements over the state-
of-the-art within a range of tasks: emotion, sar-
casm and sentiment detection. We present multi-
ple analyses on the effect of pretraining, including
results that show that the diversity of our emoji set
is important for the transfer learning potential of
our model. Our pretrained DeepMoji model is re-
leased with the hope that other researchers can use
it for various NLP tasks1.

2 Related work

Using emotional expressions as noisy labels in
text to counter scarcity of labels is not a new
idea (Read, 2005; Go et al., 2009). Originally, bi-
narized emoticons were used as noisy labels, but
later also hashtags and emojis have been used.
To our knowledge, previous research has always
manually speciﬁed which emotional category each
emotional expression belong to. Prior work has
used theories of emotion such as Ekman’s six
basic emotions and Plutchik’s eight basic emo-
tions (Mohammad, 2012; Suttles and Ide, 2013).

Such manual categorization requires an under-
standing of the emotional content of each expres-
sion, which is difﬁcult and time-consuming for
sophisticated combinations of emotional content.
Moreover, any manual selection and categoriza-
tion is prone to misinterpretations and may omit
important details regarding usage. In contrast, our
approach requires no prior knowledge of the cor-
pus and can capture diverse usage of 64 types of
emojis (see Table 1 for examples and Figure 3 for
how the model implicitly groups emojis).

Another way of automatically interpreting the
emotional content of an emoji is to learn emoji
embeddings from the words describing the emoji-
semantics in ofﬁcial emoji tables (Eisner et al.,
2016). This approach, in our context, suffers from
two severe limitations: a) It requires emojis at test
time while there are many domains with limited
or no usage of emojis. b) The tables do not cap-
ture the dynamics of emoji usage, i.e., drift in an
emoji’s intended meaning over time.

Knowledge can be transferred from the emoji
dataset to the target task in many different ways.
In particular, multitask learning with simultaneous

1Available with preprocessing code, examples of usage,

benchmark datasets etc. at github.com/bfelbo/deepmoji

Figure 1: Illustration of the DeepMoji model with
T being text length and C the number of classes.

training on multiple datasets has shown promis-
ing results (Collobert and Weston, 2008). How-
ever, multitask learning requires access to the
emoji dataset whenever the classiﬁer needs to be
tuned for a new target task. Requiring access
to the dataset is problematic in terms of violat-
ing data access regulations. There are also is-
sues from a data storage perspective as the dataset
used for this research contains hundreds of mil-
lions of tweets (see Table 2). Instead we use trans-
fer learning (Bengio et al., 2012) as described in
§3.3, which does not require access to the original
dataset, but only the pretrained classiﬁer.

3 Method

3.1 Pretraining

In many cases, emojis serve as a proxy for the
emotional contents of a text. Therefore, pretrain-
ing on the classiﬁcation task of predicting which
emoji were initially part of a text can improve per-
formance on the target task (see §5.3 for an anal-
ysis of why our pretraining helps). Social media
contains large amounts of short texts with emojis
that can be utilized as noisy labels for pretraining.
Here, we use data from Twitter from January 1st
2013 to June 1st 2017, but any dataset with emoji
occurrences could be used.

Only English tweets without URL’s are used for
the pretraining dataset. Our hypothesis is that the
content obtained from the URL is likely to be im-
portant for understanding the emotional content of
the text in the tweet. Therefore, we expect emo-
jis associated with these tweets to be noiser labels

than for tweets without URLs, and the tweets with
URLs are thus removed.

Proper tokenization is important for generaliza-
tion. All tweets are tokenized on a word-by-word
basis. Words with 2 or more repeated characters
are shortened to the same token (e.g. ‘loool’ and
‘looooool’ are tokenized such that they are treated
the same). Similarly, we use a special token for all
URLs (only relevant for benchmark datasets), user
mentions (e.g. ‘@acl2017’ and ‘@emnlp2017’ are
thus treated the same) and numbers. To be in-
cluded in the training set the tweet must contain
at least 1 token that is not a punctuation symbol,
emoji or special token2.

Many tweets contain multiple repetitions of the
In the
same emoji or multiple different emojis.
training data, we address this in the following way.
For each unique emoji type, we save a separate
tweet for the pretraining with that emoji type as the
label. We only save a single tweet for the pretrain-
ing per unique emoji type regardless of the number
of emojis associated with the tweet. This data pre-
processing allows the pretraining task to capture
that multiple types of emotional content are asso-
ciated with the tweet while making our pretraining
task a single-label classiﬁcation instead of a more
complicated multi-label classiﬁcation.

To ensure that the pretraining encourages the
models to learn a rich understanding of emotional
content in text rather than only emotional content
associated with the most used emojis, we create
a balanced pretraining dataset. The pretraining
data is split into a training, validation and test set,
where the validation and test set is randomly sam-
pled in such a way that each emoji is equally repre-
sented. The remaining data is upsampled to create
a balanced training dataset.

3.2 Model

With the millions of emoji occurrences available,
we can train very expressive classiﬁers with lim-
ited risk of overﬁtting. We use a variant of the
Long Short-Term Memory (LSTM) model that has
been successful at many NLP tasks (Hochreiter
and Schmidhuber, 1997; Sutskever et al., 2014).
Our DeepMoji model uses an embedding layer of
256 dimensions to project each word into a vector
space. A hyperbolic tangent activation function is
used to enforce a constraint of each embedding di-
mension being within [−1, 1]. To capture the con-

2Details available at github.com/bfelbo/deepmoji

text of each word we use two bidirectional LSTM
layers with 1024 hidden units in each (512 in each
direction). Finally, an attention layer that take all
of these layers as input using skip-connections is
used (see Figure 1 for an illustration).

The attention mechanism lets the model decide
the importance of each word for the prediction task
by weighing them when constructing the represen-
tation of the text. For instance, a word such as
‘amazing’ is likely to be very informative of the
emotional meaning of a text and it should thus be
treated accordingly. We use a simple approach
inspired by (Bahdanau et al., 2014; Yang et al.,
2016) with a single parameter pr. input channel:

et = htwa

at =

exp(et)
i=1 exp(ei)

(cid:80)T

v =

aihi

T
(cid:88)

i=1

Here ht is the representation of the word at time
step t and wa is the weight matrix for the atten-
tion layer. The attention importance scores for
each time step, at, are obtained by multiplying the
representations with the weight matrix and then
normalizing to construct a probability distribution
over the words. Lastly, the representation vector
for the text, v, is found by a weighted summation
over all the time steps using the attention impor-
tance scores as weights. This representation vec-
tor obtained from the attention layer is a high-level
encoding of the entire text, which is used as input
to the ﬁnal Softmax layer for classiﬁcation. We
ﬁnd that adding the attention mechanism and skip-
connections improves the model’s capabilities for
transfer learning (see §5.2 for more details).

The only regularization used for the pretrain-
ing task is a L2 regularization of 1E−6 on the
embedding weights. For the ﬁnetuning additional
regularization is applied (see §4.2). Our model is
implemented using Theano (Theano Development
Team, 2016) and we make an easy-to-use version
available that uses Keras (Chollet et al., 2015).

3.3 Transfer learning

Our pretrained model can be ﬁne-tuned to the tar-
get task in multiple ways with some approaches
‘freezing’ layers by disabling parameters updates
to prevent overﬁtting. One common approach is

to use the network as a feature extractor (Don-
ahue et al., 2014), where all layers in the model are
frozen when ﬁne-tuning on the target task except
the last layer (hereafter referred to as the ‘last’ ap-
proach). Alternatively, another common approach
is to use the pretrained model as an initializa-
tion (Erhan et al., 2010), where the full model is
unfrozen (hereafter referred to as ‘full’).

We propose a new simple transfer learning ap-
proach, ‘chain-thaw’, that sequentially unfreezes
and ﬁne-tunes a single layer at a time. This ap-
proach increases accuracy on the target task at the
expense of extra computational power needed for
the ﬁne-tuning. By training each layer separately
the model is able to adjust the individual patterns
across the network with a reduced risk of overﬁt-
ting. The sequential ﬁne-tuning seems to have a
regularizing effect similar to what has been exam-
ined with layer-wise training in the context of un-
supervised learning (Erhan et al., 2010).

More speciﬁcally, the chain-thaw approach ﬁrst
ﬁne-tunes any new layers (often only a Softmax
layer) to the target task until convergence on a
validation set. Then the approach ﬁne-tunes each
layer individually starting from the ﬁrst layer in
the network. Lastly, the entire model is trained
with all layers. Each time the model converges
as measured on the validation set, the weights
are reloaded to the best setting, thereby prevent-
ing overﬁtting in a similar manner to early stop-
ping (Sj¨oberg and Ljung, 1995). This process is
illustrated in Figure 2. Note how only perform-
ing step a) in the ﬁgure is identical to the ‘last’
approach, where the existing network is used as
a feature extractor. Similarly, only doing step d)
is identical to the ‘full’ approach, where the pre-
trained weights are used as an initialization for a
fully trainable network. Although the chain-thaw
procedure may seem extensive it is easily imple-
mented with only a few lines of code. Similarly,
the additional time spent on ﬁne-tuning is limited
when the target task uses GPUs on small datasets
of manually annotated data as is often the case.

A beneﬁt of the chain-thaw approach is the abil-
ity to expand the vocabulary to new domains with
little risk of overﬁtting. For a given dataset up to
10000 new words from the training set are added
to the vocabulary. §5.3 contains analysis on the
added word coverage gained from this approach.

Figure 2: Illustration of the chain-thaw transfer
learning approach, where each layer is ﬁne-tuned
separately. Layers covered with a blue rectangle
are frozen. Step a) tunes any new layers, b) then
tunes the 1st layer and c) the next layer until all
layers have been ﬁne-tuned individually. Lastly,
in step d) all layers are ﬁne-tuned together.

Table 2: The number of tweets in the pretraining
dataset associated with each emoji in millions.

4 Experiments

4.1 Emoji prediction

We use a raw dataset of 56.6 billion tweets, which
is then ﬁltered to 1.2 billion relevant tweets (see
details in §3.1). In the pretraining dataset a copy
of a single tweet is stored once for each unique
emoji, resulting in a dataset consisting of 1.6 bil-
lion tweets. Table 2 shows the distribution of
tweets across different emoji types. To evaluate
performance on the pretraining task a validation
set and a test set both containing 640K tweets
(10K of each emoji type) are used. The remain-
ing tweets are used for the training set, which is
balanced using upsampling.

The performance of the DeepMoji model is
evaluated on the pretraining task with the results
shown in Table 3. Both top 1 and top 5 accuracy
is used for the evaluation as the emoji labels are
noisy with multiple emojis being potentially cor-
rect for any given sentence. For comparison we
also train a version of our DeepMoji model with
smaller LSTM layers and a bag-of-words classi-
ﬁer, fastText, that has recently shown competitive
results (Joulin et al., 2016). We use 256 dimen-

Table 3: Accuracy of classiﬁers on the emoji
prediction task. d refers to the dimensionality of
each LSTM layer. Parameters are in millions.

Params

Top 1

Top 5

−
Random
12.8
fasttext
15.5
DeepMoji (d = 512)
DeepMoji (d = 1024) 22.4

1.6%
7.8%
12.8% 36.2%
16.7% 43.3%
17.0% 43.8%

sions for this fastText classiﬁer, thereby making it
almost identical to only using the embedding layer
from the DeepMoji model. The difference in top
5 accuracy between the fastText classiﬁer (36.2%)
and the largest DeepMoji model (43.8%) under-
lines the difﬁculty of the emoji prediction task. As
the two classiﬁers only differ in that the DeepMoji
model has LSTM layers and an attention layer be-
tween the embedding and Softmax layer, this dif-
ference in accuracy demonstrates the importance
of capturing the context of each word.

4.2 Benchmarking

We benchmark our method on 3 different NLP
tasks using 8 datasets across 5 domains. To make
for a fair comparison, we compare DeepMoji
to other methods that also utilize external data
sources in addition to the benchmark dataset. An
averaged F1-measure across classes is used for
evaluation in emotion analysis and sarcasm detec-
tion as these consist of unbalanced datasets while
sentiment datasets are evaluated using accuracy.

An issue with many of the benchmark datasets
is data scarcity, which is particularly problem-
atic within emotion analysis. Many recent pa-
pers proposing new methods for emotion analysis
such as (Staiano and Guerini, 2014) only evaluate
performance on a single benchmark dataset, Se-
mEval 2007 Task 14, that contains 1250 observa-
tions. Recently, criticism has been raised concern-
ing the use of correlation with continuous ratings
as a measure (Buechel and Hahn, 2016), making
only the somewhat limited binary evaluation pos-
sible. We only evaluate the emotions {Fear, Joy,
Sadness} as the remaining emotions occur in less
than 5% of the observations.

To fully evaluate our method on emotion analy-
sis against the current methods we thus make use
of two other datasets: A dataset of emotions in
tweets related to the Olympic Games created by
that we convert to a single-label
Sintsova et al.

classiﬁcation task and a dataset of self-reported
emotional experiences created by a large group
of psychologists (Wallbott and Scherer, 1986).
See the supplementary material for details on the
datasets and the preprocessing. As these two
datasets do not have prior evaluations, we eval-
uate against a state-of-the-art approach, which
is based on a valence-arousal-dominance frame-
work (Buechel and Hahn, 2016). The scores ex-
tracted using this approach are mapped to the
classes in the datasets using a logistic regres-
sion with parameter optimization using cross-
validation. We release our preprocessing code and
hope that these 2 two datasets will be used for fu-
ture benchmarking within emotion analysis.

We evaluate sentiment analysis performance on
three benchmark datasets. These small datasets
are chosen to emphasize the importance of the
transfer learning ability of the evaluated models.
Two of the datasets are from SentiStrength (Thel-
wall et al., 2010), SS-Twitter and SS-Youtube,
and follow the relabeling described in (Saif et al.,
2013) to make the labels binary. The third dataset
is from SemEval 2016 Task4A (Nakov et al.,
2016). Due to tweets being deleted from Twitter,
the SemEval dataset suffers from data decay, mak-
ing it difﬁcult to compare results across papers. At
the time of writing, roughly 15% of the training
dataset for SemEval 2016 Task 4A was impossible
to obtain. We choose not to use review datasets for
sentiment benchmarking as these datasets contain
so many words pr. observation that even bag-of-
words classiﬁers and unsupervised approaches can
obtain a high accuracy (Joulin et al., 2016; Rad-
ford et al., 2017).

The current state of the art for sentiment analy-
sis on social media (and winner of SemEval 2016
Task 4A) uses an ensemble of convolutional neu-
ral networks that are pretrained on a private dataset
of tweets with emoticons, making it difﬁcult to
replicate (Deriu et al., 2016). Instead we pretrain
a model with the hyperparameters of the largest
model in their ensemble on the positive/negative
emoticon dataset from Go et al. (2009). Using
this pretraining as an initialization we ﬁnetune
the model on the target tasks using early stop-
ping on a validation set to determine the amount
of training. We also implemented the Sentiment-
Speciﬁc Word Embedding (SSWE) using the em-
beddings available on the authors’ website (Tang
et al., 2014), but found that it performed worse

Table 4: Description of benchmark datasets. Datasets without pre-existing training/test splits are split by
us (with splits publicly available). Data used for hyperparameter tuning is taken from the training set.

Study

Domain

Classes Ntrain Ntest

Identiﬁer

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

(Strapparava and Mihalcea, 2007)
(Sintsova et al., 2013)
(Wallbott and Scherer, 1986)

(Thelwall et al., 2012)
(Thelwall et al., 2012)
(Nakov et al., 2016)

(Walker et al., 2012)
(Oraby et al., 2016)

Task

Emotion
Emotion
Emotion

Headlines
Tweets
Experiences

Sentiment
Sentiment Video Comments
Sentiment

Tweets

Tweets

Sarcasm
Sarcasm

Debate Forums
Debate Forums

250
250
1000

1000
1000
7155

1000
1000

1000
709
6480

1113
1142
31986

995
2260

Table 5: Comparison across benchmark datasets. Reported values are averages across ﬁve runs. Varia-
tions refer to transfer learning approaches in §3.3 with ‘new’ being a model trained without pretraining.

Dataset

Measure

State of the art

DeepMoji
(new)

DeepMoji
(full)

DeepMoji
(last)

DeepMoji
(chain-thaw)

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

F1
F1
F1

Acc
Acc
Acc

F1
F1

.34 [Buechel]
.50 [Buechel]
.45 [Buechel]

.82 [Deriu]
.86 [Deriu]
.51 [Deriu]3

.63 [Joshi]
.72 [Joshi]

.21
.43
.32

.62
.75
.51

.67
.71

.31
.50
.42

.85
.88
.54

.65
.71

.37
.61
.57

.88
.93
.58

.69
.75

3
4
7

2
2
3

2
2

.36
.61
.56

.87
.92
.58

.68
.74

than the pretrained convolutional neural network.
These results are therefore excluded.

For sarcasm detection we use the sarcasm
dataset version 1 and 2 from the Internet Argu-
ment Corpus (Walker et al., 2012). Note that
results presented on these benchmarks in e.g.
Oraby et al. (2016) are not directly comparable
as only a subset of the data is available online.4
A state-of-the-art baseline is found by modeling
the embedding-based features from Joshi et al.
(2016) alongside unigrams, bigrams and trigrams
with an SVM. GoogleNews word2vec embed-
dings (Mikolov et al., 2013) are used for comput-
ing the embedding-based features. A hyperparam-
eter search for regularization parameters is carried
out using cross-validation. Note that the sarcasm
dataset version 2 contains both a quoted text and a
sarcastic response, but to keep the models identi-
cal across the datasets only the response is used.

For

training we

the Adam opti-
mizer (Kingma and Ba, 2015) with gradient

use

3The authors report a higher accuracy in their paper,
which is likely due to having a larger training dataset as they
were able to obtain it before data decay occurred.

4We contacted the authors, but were unable to obtain the

full dataset for neither version 1 or version 2.

clipping of the norm to 1. Learning rate is set to
1E−3 for training of all new layers and 1E−4
for ﬁnetuning any pretrained layers. To prevent
overﬁtting on the small datasets, 10% of the
channels across all words in the embedding layer
are dropped out during training. Unlike e.g. (Gal
and Ghahramani, 2016) we do not drop out entire
words in the input as some of our datasets contain
it could
observations with so few words that
change the meaning of the text.
In addition to
the embedding dropout, L2 regularization for the
embedding weights is used and 50% dropout is
applied to the penultimate layer.

Table 5 shows that the DeepMoji model out-
performs the state of the art across all benchmark
datasets and that our new ‘chain-thaw’ approach
consistently yields the highest performance for the
transfer learning, albeit often only slightly better
or equal to the ‘last’ approach. Results are aver-
aged across 5 runs to reduce the variance. We test
the statistical signiﬁcance of our results by com-
paring the performance of DeepMoji (chain-thaw)
the state of the art. Bootstrap testing with
vs.
10000 samples is used. Our results are statisti-
cally signiﬁcantly better than the state of the art

with p < 0.001 on every benchmark dataset.

useful for transfer learning.

Our model is able to out-perform the state-of-
the-art on datasets that originate from domains that
differ substantially from the tweets on which it
was pretrained. A key difference between the pre-
training dataset and the benchmark datasets is the
length of the observations. The average number of
tokens pr.
tweet in the pretraining dataset is 11,
whereas e.g. the board posts from the Internet Ar-
gument Corpus version 1 (Oraby et al., 2016) has
an average of 66 tokens with some observations
being much longer.

5 Model Analysis

5.1

Importance of emoji diversity

One of the major differences between this work
compared to previous papers using distant super-
vision is the diversity of the noisy labels used (see
§2). For instance, both Deriu et al. (2016) and
Tang et al. (2014) only used positive and negative
emoticons as noisy labels. Other instances of pre-
vious work have used slightly more nuanced sets
of noisy labels (see §2), but to our knowledge our
set of noisy labels is the most diverse yet. To an-
alyze the effect of using a diverse emoji set we
create a subset of our pretraining data containing
tweets with one of 8 emojis that are similar to
the positive/negative emoticons used by Tang et al.
(2014) and Hu et al. (2013) (the set of emoticons
and corresponding emojis are available in the sup-
plemental material). As the dataset based on this
reduced set of emojis contains 433M tweets, any
difference in performance on benchmark datasets
is likely linked to the diversity of labels rather than
differences in dataset sizes.

We train our DeepMoji model

to predict
whether the tweets contain a positive or negative
emoji and evaluate this pretrained model across
the benchmark datasets. We refer to the model
trained on the subset of emojis as DeepMoji-
PosNeg (as opposed to DeepMoji). To test the
emotional representations learned by the two pre-
trained models the ‘last’ transfer learning ap-
proach is used for the comparison, thereby only
allowing the models to map already learned fea-
tures to classes in the target dataset. Table 6 shows
that DeepMoji-PosNeg yields lower performance
compared to DeepMoji across all 8 benchmarks,
thereby showing that the diversity of our emoji
types encourage the model to learn a richer repre-
sentation of emotional content in text that is more

Table 6: Benchmarks using a smaller emoji set
(Pos/Neg emojis) or a classic architecture (stan-
dard LSTM). Results for DeepMoji from Table 5
are added for convenience. Evaluation metrics are
as in Table 5. Reported values are the averages
across ﬁve runs.

Pos/Neg
emojis

Standard
LSTM

DeepMoji

Dataset

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

.32
.55
.40

.86
.90
.56

.66
.72

.35
.57
.49

.86
.91
.57

.66
.73

.36
.61
.56

.87
.92
.58

.68
.74

Many of the emojis carry similar emotional
content, but have subtle differences in usage that
our model is able to capture. Through hierar-
chical clustering on the correlation matrix of the
DeepMoji model’s predictions on the test set we
can see that the model captures many similarities
that one would intuitively expect (see Figure 3).
For instance, the model groups emojis into overall
categories associated with e.g. negativity, positiv-
ity or love. Similarly, the model learns to differen-
tiate within these categories, mapping sad emojis
in one subcategory of negativity, annoyed in an-
other subcategory and angry in a third one.

5.2 Model architecture

Our DeepMoji model architecture as described
in §3.2 use an attention mechanism and skip-
connections to ease the transfer of the learned rep-
resentation to new domains and tasks. Here we
compare the DeepMoji model architecture to that
of a standard 2-layer LSTM, both compared using
the ‘last’ transfer learning approach. We use the
same regularization and training parameters.

As seen in Table 6 the DeepMoji model per-
forms better than a standard 2-layer LSTM across
all benchmark datasets. The two architectures per-
formed equally on the pretraining task, suggesting
that while the DeepMoji model architecture is in-
deed better for transfer learning, it may not neces-
sarily be better for single supervised classiﬁcation
task with ample available data.

A reasonable conjecture is that the improved
transfer learning performance is due to two fac-

Figure 3: Hierarchical clustering of the DeepMoji model’s predictions across categories on the test set.
The dendrogram shows how the model learns to group emojis into overall categories and subcategories
based on emotional content. The y-axis is the distance on the correlation matrix of the model’s predic-
tions measured using average linkage. More details are available in the supplementary material.

tors:
a) the attention mechanism with skip-
connections provide easy access to learned low-
level features for any time step, making it easy to
use this information if needed for a new task b)
the improved gradient-ﬂow from the output layer
to the early layers in the network due to skip-
connections (Graves, 2013) is important when ad-
justing parameters in early layers as part of trans-
fer learning to small datasets. Detailed analysis of
whether these factors actually explain why our ar-
chitecture outperform a standard 2-layer LSTM is
left for future work.

5.3 Analyzing the effect of pretraining

Performance on the target task beneﬁts strongly
from pretraining as shown in Table 5 by compar-
ing DeepMoji (new) to DeepMoji (chain-thaw).
In this section we experimentally decompose the
beneﬁt of pretraining into 2 effects: word coverage
and phrase coverage. These two effects help regu-
larize the model by preventing overﬁtting (see the
supplementary details for an visualization of the
effect of this regularization).

There are numerous ways to express a speciﬁc
sentiment, emotion or sarcastic comment. Conse-
quently, the test set may contain speciﬁc language
use not present in the training set. The pretraining
helps the target task models attend to low-support
evidence by having previously observed similar
usage in the pretraining dataset. We ﬁrst exam-
ine this effect by measuring the improvement in
word coverage on the test set when using the pre-
training with word coverage being deﬁned as the
% of words in the test dataset seen in the train-
ing/pretraining dataset (see Table 7). An impor-
tant reason why the ‘chain-thaw’ approach outper-
forms other transfer learning approaches is can be

used to tune the embedding layer with limited risk
of overﬁtting. Table 7 shows the increased word
coverage from adding new words to the vocabu-
lary as part of that tuning.

Note that word coverage can be a misleading
metric in this context as for many of these small
datasets a word will often occur only once in the
training set.
In contrast, all of the words in the
pretraining vocabulary are present in thousands (if
not millions) of observations in the emoji pretrain-
ing dataset thus making it possible for the model
to learn a good representation of the emotional
and semantic meaning. The added beneﬁt of pre-
training for learning word representations there-
fore likely extends beyond the differences seen in
Table 7.

Table 7: Word coverage on benchmark test sets
using only the vocabulary generated by ﬁnding
words in the training data (‘own’), the pretrain-
ing vocabulary (‘last’) or a combination of both
vocabularies (‘full / chain-thaw’).

Dataset

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

Own

41.9%
73.9%
85.4%

80.1%
79.6%
86.1%

88.7%
86.5%

Last

93.6%
90.3%
98.5%

97.1%
97.2%
96.6%

97.3%
97.2%

Full /
Chain-thaw

94.0%
96.0%
98.8%

97.2%
97.3%
97.0%

98.0%
98.0%

To examine the importance of capturing phrases
and the context of each word, we evaluate the ac-
curacy on the SS-Youtube dataset using a fastText
classiﬁer pretrained on the same emoji dataset as

our DeepMoji model. This fastText classiﬁer is al-
most identical to only using the embedding layer
from the DeepMoji model. We evaluate the rep-
resentations learned by ﬁne-tuning the models as
feature extractors (i.e. using the ‘last’ transfer
learning approach). The fastText model achieves
an accuracy of 63% as compared to 93% for our
DeepMoji model,
thereby emphasizing the im-
portance of phrase coverage. One concept that
the LSTM layers likely learn is negation, which
is known to be important for sentiment analy-
sis (Wiegand et al., 2010).

single MTurk rater.

Table 8: Comparison of agreement between clas-
siﬁers and the aggregate opinion of Amazon
Mechanical Turkers on sentiment prediction of
tweets.

Random
fastText
MTurk
DeepMoji

Agreement

50.1%
71.0%
76.1%
82.4%

5.4 Comparing with human-level agreement

6 Conclusion

To understand how well our DeepMoji classi-
ﬁer performs compared to humans, we created a
new dataset of random tweets annotated for senti-
ment. Each tweet was annotated by a minimum of
10 English-speaking Amazon Mechanical Turkers
(MTurk’s) living in USA. Tweets were rated on a
scale from 1 to 9 with a ‘Do not know’ option, and
guidelines regarding how to rate the tweets were
provided to the human raters. The tweets were
selected to contain only English text, no men-
tions and no URL’s to make it possible to rate
them without any additional contextual informa-
tion. Tweets where more than half of the eval-
uators chose ‘Do not know’ were removed (98
tweets).

For each tweet, we select a MTurk rating ran-
dom to be the ‘human evaluation’, and average
over the remaining nine MTurk ratings are av-
eraged to form the ground truth. The ‘senti-
ment label’ for a given tweet is thus deﬁned as
the overall consensus among raters (excluding the
randomly-selected ‘human evaluation’ rating). To
ensure that the label categories are clearly sep-
arated, we removed neutral tweets in the inter-
val [4.5, 5.5] (roughly 29% of the tweets). The
remaining dataset consists of 7 347 tweets. Of
these tweets, 5000 are used for training/validation
and the remaining are used as the test set. Our
DeepMoji model is trained using the chain-thaw
transfer learning approach.

Table 8 shows that the agreement of the random
MTurk rater is 76.1%, meaning that the randomly
selected rater will agree with the average of the
nine other MTurk-ratings of the tweet’s polarity
76.1% of the time. Our DeepMoji model achieves
82.4% agreement, which means it is better at cap-
turing the average human sentiment-rating than a

We have shown how the millions of texts on so-
cial media with emojis can be used for pretrain-
ing models, thereby allowing them to learn repre-
sentations of emotional content in texts. Through
comparison with an identical model pretrained on
a subset of emojis, we ﬁnd that the diversity of
our emoji set is important for the performance of
our method. We release our pretrained DeepMoji
model with the hope that other researchers will
ﬁnd good use of them for various emotion-related
NLP tasks5.

Acknowledgments

The authors would like to thank Janys Analytics
for generously allowing us to use their dataset of
human-rated tweets and the associated code to an-
alyze it. Furthermore, we would like to thank Max
Lever, who helped design the online demo, and
Han Thi Nguyen, who helped code the software
that is provided alongside the pretrained model.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In 3rd International
Conference on Learning Representations (ICLR).

Yoshua Bengio et al. 2012. Deep learning of repre-
sentations for unsupervised and transfer learning. In
29th International Conference on Machine learning
(ICML) – Workshop on Unsupervised and Transfer
Learning, volume 27, pages 17–36.

Sven Buechel and Udo Hahn. 2016. Emotion analy-
sis as a regression problem - dimensional models
and their implications on emotion representation and

5Available with preprocessing code, examples of usage,

benchmark datasets etc. at github.com/bfelbo/deepmoji

metrical evaluation. In 22nd European Conference
on Artiﬁcial Intelligence (ECAI).

Franc¸ois Chollet et al. 2015. Keras. https://

github.com/fchollet/keras.

Ronan Collobert and Jason Weston. 2008. A uni-
ﬁed architecture for natural language processing:
Deep neural networks with multitask learning.
In
25th International Conference on Machine learning
(ICML), pages 160–167.

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
relien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. Swisscheese at semeval-2016 task 4: Sen-
timent classiﬁcation using an ensemble of convo-
lutional neural networks with distant supervision.
Proceedings of SemEval, pages 1124–1128.

Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. Decaf: A deep convolutional activation fea-
In 31th Inter-
ture for generic visual recognition.
national Conference on Machine Learning (ICML),
volume 32, pages 647–655.

Ben Eisner, Tim Rockt¨aschel, Isabelle Augenstein,
Matko Boˇsnjak,
and Sebastian Riedel. 2016.
emoji2vec: Learning emoji representations from
their description. In 4th International Workshop on
Natural Language Processing for Social Media (So-
cialNLP).

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research (JMLR), 11:625–660.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In 30th Conference on Neural In-
formation Processing Systems (NIPS), pages 1019–
1027.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classiﬁcation using distant supervision.
CS224N Project Report, Stanford, 1(12).

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the 22nd interna-
tional conference on World Wide Web (WWW), pages
607–618. ACM.

Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak
Bhattacharyya, and Mark Carman. 2016. Are word

embedding-based features useful for sarcasm detec-
tion? In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efﬁcient text
classiﬁcation. arXiv preprint arXiv:1607.01759.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations
(ICLR).

FA Kunneman, CC Liebrecht, and APJ van den Bosch.
2014. The (un)predictability of emotional hashtags
in twitter. In 52th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL). Associa-
tion for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In 27th Conference on Neural Information Pro-
cessing Systems (NIPS), pages 3111–3119.

Saif Mohammad. 2012. #emotional tweets.

In The
First Joint Conference on Lexical and Computa-
tional Semantics (*SEM), pages 246–255. Associa-
tion for Computational Linguistics.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. Semeval-
2016 task 4: Sentiment analysis in twitter.
In
10th International Workshop on Semantic Evalua-
tion (SemEval), pages 1–18.

Shereen Oraby, Vrindavan Harrison, Lena Reed,
Ernesto Hernandez, Ellen Riloff, and Marilyn
Walker. 2016. Creating and characterizing a diverse
corpus of sarcasm in dialogue. In 17th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue (SIGDIAL), page 31.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classiﬁcation. In ACL student research work-
shop, pages 43–48. Association for Computational
Linguistics.

Hassan Saif, Miriam Fernandez, Yulan He, and Harith
Alani. 2013. Evaluation datasets for twitter senti-
ment analysis: a survey and a new dataset, the sts-
gold. In Workshop: Emotion and Sentiment in So-
cial and Expressive Media: approaches and per-
spectives from AI (ESSEM) at AI*IA Conference.

Valentina Sintsova, Claudiu-Cristian Musat, and Pearl
Fine-grained emotion recognition in
Pu. 2013.
In
olympic tweets based on human computation.
4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis
(WASSA).

In Workshop on Negation and Speculation in Natu-
ral Language Processing (NeSp-NLP), pages 60–68.
Association for Computational Linguistics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J Smola, and Eduard H Hovy. 2016. Hi-
erarchical attention networks for document classiﬁ-
cation. In HLT-NAACL.

A Supplemental Material

A.1 Preprocessing Emotion Datasets

In the Olympic Games dataset by Sintsova et al.
each tweet can be assigned multiple emotions out
of 20 possible emotions, making evaluation difﬁ-
cult. To counter this difﬁculty, we have chosen to
convert the labels to 4 classes of low/high valence
and low/high arousal based on the Geneva Emo-
tion Wheel that the study used. A tweet is deemed
as having emotions within the valence/arousal
class if the average evaluation by raters for that
class is 2.0 or higher, where ‘Low’ = 1, ‘Medium’
= 2 and ‘High’ = 3.

We also evaluate on the ISEAR databank (Wall-
bott and Scherer, 1986), which was created over
many years by a large group of psychologists that
interviewed respondents in 37 countries. Each ob-
servation in the dataset is a self-reported experi-
ence mapped to 1 of 7 possible emotions, making
for an interesting benchmark dataset.

A.2 Pretraining as Regularization

Jonas Sj¨oberg and Lennart Ljung. 1995. Overtraining,
regularization and searching for a minimum, with
application to neural networks. International Jour-
nal of Control, 62(6):1391–1407.

Jacopo Staiano and Marco Guerini. 2014.

De-
pechemood: A lexicon for emotion analysis from
In 52th Annual Meeting
crowd-annotated news.
of the Association for Computational Linguistics
(ACL). Association for Computational Linguistics.

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
In 4th Inter-
2007 task 14: Affective text.
national Workshop on Semantic Evaluations (Se-
mEval), pages 70–74. Association for Computa-
tional Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
In 28th Conference on Neural Information
works.
Processing Systems (NIPS), pages 3104–3112.

Jared Suttles and Nancy Ide. 2013. Distant supervi-
sion for emotion classiﬁcation with discrete binary
In International Conference on Intelligent
values.
Text Processing and Computational Linguistics (CI-
CLing), pages 121–136. Springer.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Learning sentiment-
Liu, and Bing Qin. 2014.
speciﬁc word embedding for twitter sentiment clas-
In 52th Annual Meeting of the Associ-
siﬁcation.
ation for Computational Linguistics (ACL), pages
1555–1565.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints, abs/1605.02688.

Mike Thelwall, Kevan Buckley, and Georgios Pal-
Sentiment strength detection for
toglou. 2012.
Journal of the American Society
the social web.
for Information Science and Technology (JASIST),
63(1):163–173.

Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
Journal of the
detection in short informal text.
American Society for Information Science and Tech-
nology, 61(12):2544–2558.

Marilyn A Walker, Jean E Fox Tree, Pranav Anand,
Rob Abbott, and Joseph King. 2012. A corpus for
In Interna-
research on deliberation and debate.
tional Conference on Language Resources and Eval-
uation (LREC), pages 812–817.

Harald G Wallbott and Klaus R Scherer. 1986. How
universal and speciﬁc is emotional experience? evi-
dence from 27 countries on ﬁve continents. Interna-
tional Social Science Council, 25(4):763–795.

Figure 4: Training statistics on the SS-Youtube
dataset with a pretrained model vs. a untrained
model. The architecture and all hyperparameters
are identical for the two models. All layers are
unfrozen.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A
survey on the role of negation in sentiment analysis.

Figure 4 shows an example of how the pretrain-
ing helps to regularize the target task model, which

otherwise quickly overﬁts. The chain-thaw trans-
fer learning approach further increases this reg-
ularization by ﬁne-tuning the model layer wise,
thereby adding additional regularization.

A.3 Emoticon to Emoji mapping

To analyze the effect of using a diverse emoji set
we create a subset of our pretraining data contain-
ing tweets with one of 8 emojis that are similar to
the positive/negative emoticons used by Tang et al.
(2014) and Hu et al. (2013). The positive emoti-
cons are :) : ) :-) :D =) and the negative emoticons
are :( : ( :-(. We ﬁnd the 8 similar emojis in our
dataset seen in Figure 5 as use these for creating
the reduced subset.

Figure 5: Emojis used for the experiment on the
importance of a diverse noisy label set.

A.4 Emoji Clustering

We compute the predictions of the DeepMoji
model on the pretraining test set containing 640K
tweets and compute the correlation matrix of the
predicted probabilities seen in Figure 7. Then we
use hierarchical clustering with average linkage
on the correlation matrix to generate the dendro-
gram seen in Figure 6. We visualized dendrograms
for various versions of our model and the over-
all structure is very stable with only a few emojis
changing places in the hierarchy.

Figure 6: Hierarchical clustering of the DeepMoji model’s predictions across categories on the test set.
The dendrogram shows how the model learns to group emojis into overall categories and subcategories
based on emotional content. The y-axis is the distance on the correlation matrix of the model’s predic-
tions measured using average linkage.

Figure 7: Correlation matrix of the model’s predictions on the pretraining test set.

Using millions of emoji occurrences to learn any-domain representations
for detecting sentiment, emotion and sarcasm

Bjarke Felbo1, Alan Mislove2, Anders Søgaard3, Iyad Rahwan1, Sune Lehmann4

1Media Lab, Massachusetts Institute of Technology
2College of Computer and Information Science, Northeastern University
3Department of Computer Science, University of Copenhagen
4DTU Compute, Technical University of Denmark

ing the distant supervision to a more diverse set of
noisy labels enables the models to learn richer rep-
resentations of emotional content in text, thereby
obtaining better performance on benchmarks for
detecting sentiment, emotions and sarcasm. We
show that the learned representation of a single
pretrained model generalizes across 5 domains.

Table 1: Example sentences scored by our model.
For each text the top ﬁve most likely emojis are
shown with the model’s probability estimates.

Abstract

NLP tasks are often limited by scarcity of
manually annotated data.
In social me-
dia sentiment analysis and related tasks,
researchers have therefore used binarized
emoticons and speciﬁc hashtags as forms
of distant supervision. Our paper shows
that by extending the distant supervision
to a more diverse set of noisy labels, the
models can learn richer representations.
Through emoji prediction on a dataset of
1246 million tweets containing one of 64
common emojis we obtain state-of-the-
art performance on 8 benchmark datasets
within sentiment, emotion and sarcasm de-
tection using a single pretrained model.
Our analyses conﬁrm that the diversity of
our emotional labels yield a performance
improvement over previous distant super-
vision approaches.

1

Introduction

A variety of NLP tasks are limited by scarcity of
manually annotated data. Therefore, co-occurring
emotional expressions have been used for dis-
tant supervision in social media sentiment anal-
ysis and related tasks to make the models learn
useful text representations before modeling these
tasks directly. For instance, the state-of-the-art ap-
proaches within sentiment analysis of social me-
dia data use positive/negative emoticons for train-
ing their models (Deriu et al., 2016; Tang et al.,
2014). Similarly, hashtags such as #anger, #joy,
#happytweet, #ugh, #yuck and #fml have in pre-
vious research been mapped into emotional cate-
gories for emotion analysis (Mohammad, 2012).

Distant supervision on noisy labels often en-
ables a model to obtain better performance on the
In this paper, we show that extend-
target task.

Emojis are not always a direct labeling of emo-
tional content. For instance, a positive emoji may
serve to disambiguate an ambiguous sentence or to
complement an otherwise relatively negative text.
Kunneman et al. (2014) discuss a similar duality
in the use of emotional hashtags such as #nice and
#lame. Nevertheless, our work shows that emo-
jis can be used to classify the emotional content
of texts accurately in many cases. For instance,
our DeepMoji model captures varied usages of the
word ‘love’ as well as slang such as ‘this is the
shit’ being a positive statement (see Table 1). We
provide an online demo at deepmoji.mit.edu to al-
low others to explore the predictions of our model.

Contributions We show how millions of read-
ily available emoji occurrences on Twitter can be
used to pretrain models to learn a richer emotional

7
1
0
2
 
t
c
O
 
7
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
2
5
0
0
.
8
0
7
1
:
v
i
X
r
a

representation than traditionally obtained through
distant supervision. We transfer this knowledge to
the target tasks using a new layer-wise ﬁne-tuning
method, obtaining improvements over the state-
of-the-art within a range of tasks: emotion, sar-
casm and sentiment detection. We present multi-
ple analyses on the effect of pretraining, including
results that show that the diversity of our emoji set
is important for the transfer learning potential of
our model. Our pretrained DeepMoji model is re-
leased with the hope that other researchers can use
it for various NLP tasks1.

2 Related work

Using emotional expressions as noisy labels in
text to counter scarcity of labels is not a new
idea (Read, 2005; Go et al., 2009). Originally, bi-
narized emoticons were used as noisy labels, but
later also hashtags and emojis have been used.
To our knowledge, previous research has always
manually speciﬁed which emotional category each
emotional expression belong to. Prior work has
used theories of emotion such as Ekman’s six
basic emotions and Plutchik’s eight basic emo-
tions (Mohammad, 2012; Suttles and Ide, 2013).

Such manual categorization requires an under-
standing of the emotional content of each expres-
sion, which is difﬁcult and time-consuming for
sophisticated combinations of emotional content.
Moreover, any manual selection and categoriza-
tion is prone to misinterpretations and may omit
important details regarding usage. In contrast, our
approach requires no prior knowledge of the cor-
pus and can capture diverse usage of 64 types of
emojis (see Table 1 for examples and Figure 3 for
how the model implicitly groups emojis).

Another way of automatically interpreting the
emotional content of an emoji is to learn emoji
embeddings from the words describing the emoji-
semantics in ofﬁcial emoji tables (Eisner et al.,
2016). This approach, in our context, suffers from
two severe limitations: a) It requires emojis at test
time while there are many domains with limited
or no usage of emojis. b) The tables do not cap-
ture the dynamics of emoji usage, i.e., drift in an
emoji’s intended meaning over time.

Knowledge can be transferred from the emoji
dataset to the target task in many different ways.
In particular, multitask learning with simultaneous

1Available with preprocessing code, examples of usage,

benchmark datasets etc. at github.com/bfelbo/deepmoji

Figure 1: Illustration of the DeepMoji model with
T being text length and C the number of classes.

training on multiple datasets has shown promis-
ing results (Collobert and Weston, 2008). How-
ever, multitask learning requires access to the
emoji dataset whenever the classiﬁer needs to be
tuned for a new target task. Requiring access
to the dataset is problematic in terms of violat-
ing data access regulations. There are also is-
sues from a data storage perspective as the dataset
used for this research contains hundreds of mil-
lions of tweets (see Table 2). Instead we use trans-
fer learning (Bengio et al., 2012) as described in
§3.3, which does not require access to the original
dataset, but only the pretrained classiﬁer.

3 Method

3.1 Pretraining

In many cases, emojis serve as a proxy for the
emotional contents of a text. Therefore, pretrain-
ing on the classiﬁcation task of predicting which
emoji were initially part of a text can improve per-
formance on the target task (see §5.3 for an anal-
ysis of why our pretraining helps). Social media
contains large amounts of short texts with emojis
that can be utilized as noisy labels for pretraining.
Here, we use data from Twitter from January 1st
2013 to June 1st 2017, but any dataset with emoji
occurrences could be used.

Only English tweets without URL’s are used for
the pretraining dataset. Our hypothesis is that the
content obtained from the URL is likely to be im-
portant for understanding the emotional content of
the text in the tweet. Therefore, we expect emo-
jis associated with these tweets to be noiser labels

than for tweets without URLs, and the tweets with
URLs are thus removed.

Proper tokenization is important for generaliza-
tion. All tweets are tokenized on a word-by-word
basis. Words with 2 or more repeated characters
are shortened to the same token (e.g. ‘loool’ and
‘looooool’ are tokenized such that they are treated
the same). Similarly, we use a special token for all
URLs (only relevant for benchmark datasets), user
mentions (e.g. ‘@acl2017’ and ‘@emnlp2017’ are
thus treated the same) and numbers. To be in-
cluded in the training set the tweet must contain
at least 1 token that is not a punctuation symbol,
emoji or special token2.

Many tweets contain multiple repetitions of the
In the
same emoji or multiple different emojis.
training data, we address this in the following way.
For each unique emoji type, we save a separate
tweet for the pretraining with that emoji type as the
label. We only save a single tweet for the pretrain-
ing per unique emoji type regardless of the number
of emojis associated with the tweet. This data pre-
processing allows the pretraining task to capture
that multiple types of emotional content are asso-
ciated with the tweet while making our pretraining
task a single-label classiﬁcation instead of a more
complicated multi-label classiﬁcation.

To ensure that the pretraining encourages the
models to learn a rich understanding of emotional
content in text rather than only emotional content
associated with the most used emojis, we create
a balanced pretraining dataset. The pretraining
data is split into a training, validation and test set,
where the validation and test set is randomly sam-
pled in such a way that each emoji is equally repre-
sented. The remaining data is upsampled to create
a balanced training dataset.

3.2 Model

With the millions of emoji occurrences available,
we can train very expressive classiﬁers with lim-
ited risk of overﬁtting. We use a variant of the
Long Short-Term Memory (LSTM) model that has
been successful at many NLP tasks (Hochreiter
and Schmidhuber, 1997; Sutskever et al., 2014).
Our DeepMoji model uses an embedding layer of
256 dimensions to project each word into a vector
space. A hyperbolic tangent activation function is
used to enforce a constraint of each embedding di-
mension being within [−1, 1]. To capture the con-

2Details available at github.com/bfelbo/deepmoji

text of each word we use two bidirectional LSTM
layers with 1024 hidden units in each (512 in each
direction). Finally, an attention layer that take all
of these layers as input using skip-connections is
used (see Figure 1 for an illustration).

The attention mechanism lets the model decide
the importance of each word for the prediction task
by weighing them when constructing the represen-
tation of the text. For instance, a word such as
‘amazing’ is likely to be very informative of the
emotional meaning of a text and it should thus be
treated accordingly. We use a simple approach
inspired by (Bahdanau et al., 2014; Yang et al.,
2016) with a single parameter pr. input channel:

et = htwa

at =

exp(et)
i=1 exp(ei)

(cid:80)T

v =

aihi

T
(cid:88)

i=1

Here ht is the representation of the word at time
step t and wa is the weight matrix for the atten-
tion layer. The attention importance scores for
each time step, at, are obtained by multiplying the
representations with the weight matrix and then
normalizing to construct a probability distribution
over the words. Lastly, the representation vector
for the text, v, is found by a weighted summation
over all the time steps using the attention impor-
tance scores as weights. This representation vec-
tor obtained from the attention layer is a high-level
encoding of the entire text, which is used as input
to the ﬁnal Softmax layer for classiﬁcation. We
ﬁnd that adding the attention mechanism and skip-
connections improves the model’s capabilities for
transfer learning (see §5.2 for more details).

The only regularization used for the pretrain-
ing task is a L2 regularization of 1E−6 on the
embedding weights. For the ﬁnetuning additional
regularization is applied (see §4.2). Our model is
implemented using Theano (Theano Development
Team, 2016) and we make an easy-to-use version
available that uses Keras (Chollet et al., 2015).

3.3 Transfer learning

Our pretrained model can be ﬁne-tuned to the tar-
get task in multiple ways with some approaches
‘freezing’ layers by disabling parameters updates
to prevent overﬁtting. One common approach is

to use the network as a feature extractor (Don-
ahue et al., 2014), where all layers in the model are
frozen when ﬁne-tuning on the target task except
the last layer (hereafter referred to as the ‘last’ ap-
proach). Alternatively, another common approach
is to use the pretrained model as an initializa-
tion (Erhan et al., 2010), where the full model is
unfrozen (hereafter referred to as ‘full’).

We propose a new simple transfer learning ap-
proach, ‘chain-thaw’, that sequentially unfreezes
and ﬁne-tunes a single layer at a time. This ap-
proach increases accuracy on the target task at the
expense of extra computational power needed for
the ﬁne-tuning. By training each layer separately
the model is able to adjust the individual patterns
across the network with a reduced risk of overﬁt-
ting. The sequential ﬁne-tuning seems to have a
regularizing effect similar to what has been exam-
ined with layer-wise training in the context of un-
supervised learning (Erhan et al., 2010).

More speciﬁcally, the chain-thaw approach ﬁrst
ﬁne-tunes any new layers (often only a Softmax
layer) to the target task until convergence on a
validation set. Then the approach ﬁne-tunes each
layer individually starting from the ﬁrst layer in
the network. Lastly, the entire model is trained
with all layers. Each time the model converges
as measured on the validation set, the weights
are reloaded to the best setting, thereby prevent-
ing overﬁtting in a similar manner to early stop-
ping (Sj¨oberg and Ljung, 1995). This process is
illustrated in Figure 2. Note how only perform-
ing step a) in the ﬁgure is identical to the ‘last’
approach, where the existing network is used as
a feature extractor. Similarly, only doing step d)
is identical to the ‘full’ approach, where the pre-
trained weights are used as an initialization for a
fully trainable network. Although the chain-thaw
procedure may seem extensive it is easily imple-
mented with only a few lines of code. Similarly,
the additional time spent on ﬁne-tuning is limited
when the target task uses GPUs on small datasets
of manually annotated data as is often the case.

A beneﬁt of the chain-thaw approach is the abil-
ity to expand the vocabulary to new domains with
little risk of overﬁtting. For a given dataset up to
10000 new words from the training set are added
to the vocabulary. §5.3 contains analysis on the
added word coverage gained from this approach.

Figure 2: Illustration of the chain-thaw transfer
learning approach, where each layer is ﬁne-tuned
separately. Layers covered with a blue rectangle
are frozen. Step a) tunes any new layers, b) then
tunes the 1st layer and c) the next layer until all
layers have been ﬁne-tuned individually. Lastly,
in step d) all layers are ﬁne-tuned together.

Table 2: The number of tweets in the pretraining
dataset associated with each emoji in millions.

4 Experiments

4.1 Emoji prediction

We use a raw dataset of 56.6 billion tweets, which
is then ﬁltered to 1.2 billion relevant tweets (see
details in §3.1). In the pretraining dataset a copy
of a single tweet is stored once for each unique
emoji, resulting in a dataset consisting of 1.6 bil-
lion tweets. Table 2 shows the distribution of
tweets across different emoji types. To evaluate
performance on the pretraining task a validation
set and a test set both containing 640K tweets
(10K of each emoji type) are used. The remain-
ing tweets are used for the training set, which is
balanced using upsampling.

The performance of the DeepMoji model is
evaluated on the pretraining task with the results
shown in Table 3. Both top 1 and top 5 accuracy
is used for the evaluation as the emoji labels are
noisy with multiple emojis being potentially cor-
rect for any given sentence. For comparison we
also train a version of our DeepMoji model with
smaller LSTM layers and a bag-of-words classi-
ﬁer, fastText, that has recently shown competitive
results (Joulin et al., 2016). We use 256 dimen-

Table 3: Accuracy of classiﬁers on the emoji
prediction task. d refers to the dimensionality of
each LSTM layer. Parameters are in millions.

Params

Top 1

Top 5

−
Random
12.8
fasttext
15.5
DeepMoji (d = 512)
DeepMoji (d = 1024) 22.4

1.6%
7.8%
12.8% 36.2%
16.7% 43.3%
17.0% 43.8%

sions for this fastText classiﬁer, thereby making it
almost identical to only using the embedding layer
from the DeepMoji model. The difference in top
5 accuracy between the fastText classiﬁer (36.2%)
and the largest DeepMoji model (43.8%) under-
lines the difﬁculty of the emoji prediction task. As
the two classiﬁers only differ in that the DeepMoji
model has LSTM layers and an attention layer be-
tween the embedding and Softmax layer, this dif-
ference in accuracy demonstrates the importance
of capturing the context of each word.

4.2 Benchmarking

We benchmark our method on 3 different NLP
tasks using 8 datasets across 5 domains. To make
for a fair comparison, we compare DeepMoji
to other methods that also utilize external data
sources in addition to the benchmark dataset. An
averaged F1-measure across classes is used for
evaluation in emotion analysis and sarcasm detec-
tion as these consist of unbalanced datasets while
sentiment datasets are evaluated using accuracy.

An issue with many of the benchmark datasets
is data scarcity, which is particularly problem-
atic within emotion analysis. Many recent pa-
pers proposing new methods for emotion analysis
such as (Staiano and Guerini, 2014) only evaluate
performance on a single benchmark dataset, Se-
mEval 2007 Task 14, that contains 1250 observa-
tions. Recently, criticism has been raised concern-
ing the use of correlation with continuous ratings
as a measure (Buechel and Hahn, 2016), making
only the somewhat limited binary evaluation pos-
sible. We only evaluate the emotions {Fear, Joy,
Sadness} as the remaining emotions occur in less
than 5% of the observations.

To fully evaluate our method on emotion analy-
sis against the current methods we thus make use
of two other datasets: A dataset of emotions in
tweets related to the Olympic Games created by
that we convert to a single-label
Sintsova et al.

classiﬁcation task and a dataset of self-reported
emotional experiences created by a large group
of psychologists (Wallbott and Scherer, 1986).
See the supplementary material for details on the
datasets and the preprocessing. As these two
datasets do not have prior evaluations, we eval-
uate against a state-of-the-art approach, which
is based on a valence-arousal-dominance frame-
work (Buechel and Hahn, 2016). The scores ex-
tracted using this approach are mapped to the
classes in the datasets using a logistic regres-
sion with parameter optimization using cross-
validation. We release our preprocessing code and
hope that these 2 two datasets will be used for fu-
ture benchmarking within emotion analysis.

We evaluate sentiment analysis performance on
three benchmark datasets. These small datasets
are chosen to emphasize the importance of the
transfer learning ability of the evaluated models.
Two of the datasets are from SentiStrength (Thel-
wall et al., 2010), SS-Twitter and SS-Youtube,
and follow the relabeling described in (Saif et al.,
2013) to make the labels binary. The third dataset
is from SemEval 2016 Task4A (Nakov et al.,
2016). Due to tweets being deleted from Twitter,
the SemEval dataset suffers from data decay, mak-
ing it difﬁcult to compare results across papers. At
the time of writing, roughly 15% of the training
dataset for SemEval 2016 Task 4A was impossible
to obtain. We choose not to use review datasets for
sentiment benchmarking as these datasets contain
so many words pr. observation that even bag-of-
words classiﬁers and unsupervised approaches can
obtain a high accuracy (Joulin et al., 2016; Rad-
ford et al., 2017).

The current state of the art for sentiment analy-
sis on social media (and winner of SemEval 2016
Task 4A) uses an ensemble of convolutional neu-
ral networks that are pretrained on a private dataset
of tweets with emoticons, making it difﬁcult to
replicate (Deriu et al., 2016). Instead we pretrain
a model with the hyperparameters of the largest
model in their ensemble on the positive/negative
emoticon dataset from Go et al. (2009). Using
this pretraining as an initialization we ﬁnetune
the model on the target tasks using early stop-
ping on a validation set to determine the amount
of training. We also implemented the Sentiment-
Speciﬁc Word Embedding (SSWE) using the em-
beddings available on the authors’ website (Tang
et al., 2014), but found that it performed worse

Table 4: Description of benchmark datasets. Datasets without pre-existing training/test splits are split by
us (with splits publicly available). Data used for hyperparameter tuning is taken from the training set.

Study

Domain

Classes Ntrain Ntest

Identiﬁer

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

(Strapparava and Mihalcea, 2007)
(Sintsova et al., 2013)
(Wallbott and Scherer, 1986)

(Thelwall et al., 2012)
(Thelwall et al., 2012)
(Nakov et al., 2016)

(Walker et al., 2012)
(Oraby et al., 2016)

Task

Emotion
Emotion
Emotion

Headlines
Tweets
Experiences

Sentiment
Sentiment Video Comments
Sentiment

Tweets

Tweets

Sarcasm
Sarcasm

Debate Forums
Debate Forums

250
250
1000

1000
1000
7155

1000
1000

1000
709
6480

1113
1142
31986

995
2260

Table 5: Comparison across benchmark datasets. Reported values are averages across ﬁve runs. Varia-
tions refer to transfer learning approaches in §3.3 with ‘new’ being a model trained without pretraining.

Dataset

Measure

State of the art

DeepMoji
(new)

DeepMoji
(full)

DeepMoji
(last)

DeepMoji
(chain-thaw)

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

F1
F1
F1

Acc
Acc
Acc

F1
F1

.34 [Buechel]
.50 [Buechel]
.45 [Buechel]

.82 [Deriu]
.86 [Deriu]
.51 [Deriu]3

.63 [Joshi]
.72 [Joshi]

.21
.43
.32

.62
.75
.51

.67
.71

.31
.50
.42

.85
.88
.54

.65
.71

.37
.61
.57

.88
.93
.58

.69
.75

3
4
7

2
2
3

2
2

.36
.61
.56

.87
.92
.58

.68
.74

than the pretrained convolutional neural network.
These results are therefore excluded.

For sarcasm detection we use the sarcasm
dataset version 1 and 2 from the Internet Argu-
ment Corpus (Walker et al., 2012). Note that
results presented on these benchmarks in e.g.
Oraby et al. (2016) are not directly comparable
as only a subset of the data is available online.4
A state-of-the-art baseline is found by modeling
the embedding-based features from Joshi et al.
(2016) alongside unigrams, bigrams and trigrams
with an SVM. GoogleNews word2vec embed-
dings (Mikolov et al., 2013) are used for comput-
ing the embedding-based features. A hyperparam-
eter search for regularization parameters is carried
out using cross-validation. Note that the sarcasm
dataset version 2 contains both a quoted text and a
sarcastic response, but to keep the models identi-
cal across the datasets only the response is used.

For

training we

the Adam opti-
mizer (Kingma and Ba, 2015) with gradient

use

3The authors report a higher accuracy in their paper,
which is likely due to having a larger training dataset as they
were able to obtain it before data decay occurred.

4We contacted the authors, but were unable to obtain the

full dataset for neither version 1 or version 2.

clipping of the norm to 1. Learning rate is set to
1E−3 for training of all new layers and 1E−4
for ﬁnetuning any pretrained layers. To prevent
overﬁtting on the small datasets, 10% of the
channels across all words in the embedding layer
are dropped out during training. Unlike e.g. (Gal
and Ghahramani, 2016) we do not drop out entire
words in the input as some of our datasets contain
it could
observations with so few words that
change the meaning of the text.
In addition to
the embedding dropout, L2 regularization for the
embedding weights is used and 50% dropout is
applied to the penultimate layer.

Table 5 shows that the DeepMoji model out-
performs the state of the art across all benchmark
datasets and that our new ‘chain-thaw’ approach
consistently yields the highest performance for the
transfer learning, albeit often only slightly better
or equal to the ‘last’ approach. Results are aver-
aged across 5 runs to reduce the variance. We test
the statistical signiﬁcance of our results by com-
paring the performance of DeepMoji (chain-thaw)
the state of the art. Bootstrap testing with
vs.
10000 samples is used. Our results are statisti-
cally signiﬁcantly better than the state of the art

with p < 0.001 on every benchmark dataset.

useful for transfer learning.

Our model is able to out-perform the state-of-
the-art on datasets that originate from domains that
differ substantially from the tweets on which it
was pretrained. A key difference between the pre-
training dataset and the benchmark datasets is the
length of the observations. The average number of
tokens pr.
tweet in the pretraining dataset is 11,
whereas e.g. the board posts from the Internet Ar-
gument Corpus version 1 (Oraby et al., 2016) has
an average of 66 tokens with some observations
being much longer.

5 Model Analysis

5.1

Importance of emoji diversity

One of the major differences between this work
compared to previous papers using distant super-
vision is the diversity of the noisy labels used (see
§2). For instance, both Deriu et al. (2016) and
Tang et al. (2014) only used positive and negative
emoticons as noisy labels. Other instances of pre-
vious work have used slightly more nuanced sets
of noisy labels (see §2), but to our knowledge our
set of noisy labels is the most diverse yet. To an-
alyze the effect of using a diverse emoji set we
create a subset of our pretraining data containing
tweets with one of 8 emojis that are similar to
the positive/negative emoticons used by Tang et al.
(2014) and Hu et al. (2013) (the set of emoticons
and corresponding emojis are available in the sup-
plemental material). As the dataset based on this
reduced set of emojis contains 433M tweets, any
difference in performance on benchmark datasets
is likely linked to the diversity of labels rather than
differences in dataset sizes.

We train our DeepMoji model

to predict
whether the tweets contain a positive or negative
emoji and evaluate this pretrained model across
the benchmark datasets. We refer to the model
trained on the subset of emojis as DeepMoji-
PosNeg (as opposed to DeepMoji). To test the
emotional representations learned by the two pre-
trained models the ‘last’ transfer learning ap-
proach is used for the comparison, thereby only
allowing the models to map already learned fea-
tures to classes in the target dataset. Table 6 shows
that DeepMoji-PosNeg yields lower performance
compared to DeepMoji across all 8 benchmarks,
thereby showing that the diversity of our emoji
types encourage the model to learn a richer repre-
sentation of emotional content in text that is more

Table 6: Benchmarks using a smaller emoji set
(Pos/Neg emojis) or a classic architecture (stan-
dard LSTM). Results for DeepMoji from Table 5
are added for convenience. Evaluation metrics are
as in Table 5. Reported values are the averages
across ﬁve runs.

Pos/Neg
emojis

Standard
LSTM

DeepMoji

Dataset

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

.32
.55
.40

.86
.90
.56

.66
.72

.35
.57
.49

.86
.91
.57

.66
.73

.36
.61
.56

.87
.92
.58

.68
.74

Many of the emojis carry similar emotional
content, but have subtle differences in usage that
our model is able to capture. Through hierar-
chical clustering on the correlation matrix of the
DeepMoji model’s predictions on the test set we
can see that the model captures many similarities
that one would intuitively expect (see Figure 3).
For instance, the model groups emojis into overall
categories associated with e.g. negativity, positiv-
ity or love. Similarly, the model learns to differen-
tiate within these categories, mapping sad emojis
in one subcategory of negativity, annoyed in an-
other subcategory and angry in a third one.

5.2 Model architecture

Our DeepMoji model architecture as described
in §3.2 use an attention mechanism and skip-
connections to ease the transfer of the learned rep-
resentation to new domains and tasks. Here we
compare the DeepMoji model architecture to that
of a standard 2-layer LSTM, both compared using
the ‘last’ transfer learning approach. We use the
same regularization and training parameters.

As seen in Table 6 the DeepMoji model per-
forms better than a standard 2-layer LSTM across
all benchmark datasets. The two architectures per-
formed equally on the pretraining task, suggesting
that while the DeepMoji model architecture is in-
deed better for transfer learning, it may not neces-
sarily be better for single supervised classiﬁcation
task with ample available data.

A reasonable conjecture is that the improved
transfer learning performance is due to two fac-

Figure 3: Hierarchical clustering of the DeepMoji model’s predictions across categories on the test set.
The dendrogram shows how the model learns to group emojis into overall categories and subcategories
based on emotional content. The y-axis is the distance on the correlation matrix of the model’s predic-
tions measured using average linkage. More details are available in the supplementary material.

tors:
a) the attention mechanism with skip-
connections provide easy access to learned low-
level features for any time step, making it easy to
use this information if needed for a new task b)
the improved gradient-ﬂow from the output layer
to the early layers in the network due to skip-
connections (Graves, 2013) is important when ad-
justing parameters in early layers as part of trans-
fer learning to small datasets. Detailed analysis of
whether these factors actually explain why our ar-
chitecture outperform a standard 2-layer LSTM is
left for future work.

5.3 Analyzing the effect of pretraining

Performance on the target task beneﬁts strongly
from pretraining as shown in Table 5 by compar-
ing DeepMoji (new) to DeepMoji (chain-thaw).
In this section we experimentally decompose the
beneﬁt of pretraining into 2 effects: word coverage
and phrase coverage. These two effects help regu-
larize the model by preventing overﬁtting (see the
supplementary details for an visualization of the
effect of this regularization).

There are numerous ways to express a speciﬁc
sentiment, emotion or sarcastic comment. Conse-
quently, the test set may contain speciﬁc language
use not present in the training set. The pretraining
helps the target task models attend to low-support
evidence by having previously observed similar
usage in the pretraining dataset. We ﬁrst exam-
ine this effect by measuring the improvement in
word coverage on the test set when using the pre-
training with word coverage being deﬁned as the
% of words in the test dataset seen in the train-
ing/pretraining dataset (see Table 7). An impor-
tant reason why the ‘chain-thaw’ approach outper-
forms other transfer learning approaches is can be

used to tune the embedding layer with limited risk
of overﬁtting. Table 7 shows the increased word
coverage from adding new words to the vocabu-
lary as part of that tuning.

Note that word coverage can be a misleading
metric in this context as for many of these small
datasets a word will often occur only once in the
training set.
In contrast, all of the words in the
pretraining vocabulary are present in thousands (if
not millions) of observations in the emoji pretrain-
ing dataset thus making it possible for the model
to learn a good representation of the emotional
and semantic meaning. The added beneﬁt of pre-
training for learning word representations there-
fore likely extends beyond the differences seen in
Table 7.

Table 7: Word coverage on benchmark test sets
using only the vocabulary generated by ﬁnding
words in the training data (‘own’), the pretrain-
ing vocabulary (‘last’) or a combination of both
vocabularies (‘full / chain-thaw’).

Dataset

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

Own

41.9%
73.9%
85.4%

80.1%
79.6%
86.1%

88.7%
86.5%

Last

93.6%
90.3%
98.5%

97.1%
97.2%
96.6%

97.3%
97.2%

Full /
Chain-thaw

94.0%
96.0%
98.8%

97.2%
97.3%
97.0%

98.0%
98.0%

To examine the importance of capturing phrases
and the context of each word, we evaluate the ac-
curacy on the SS-Youtube dataset using a fastText
classiﬁer pretrained on the same emoji dataset as

our DeepMoji model. This fastText classiﬁer is al-
most identical to only using the embedding layer
from the DeepMoji model. We evaluate the rep-
resentations learned by ﬁne-tuning the models as
feature extractors (i.e. using the ‘last’ transfer
learning approach). The fastText model achieves
an accuracy of 63% as compared to 93% for our
DeepMoji model,
thereby emphasizing the im-
portance of phrase coverage. One concept that
the LSTM layers likely learn is negation, which
is known to be important for sentiment analy-
sis (Wiegand et al., 2010).

single MTurk rater.

Table 8: Comparison of agreement between clas-
siﬁers and the aggregate opinion of Amazon
Mechanical Turkers on sentiment prediction of
tweets.

Random
fastText
MTurk
DeepMoji

Agreement

50.1%
71.0%
76.1%
82.4%

5.4 Comparing with human-level agreement

6 Conclusion

To understand how well our DeepMoji classi-
ﬁer performs compared to humans, we created a
new dataset of random tweets annotated for senti-
ment. Each tweet was annotated by a minimum of
10 English-speaking Amazon Mechanical Turkers
(MTurk’s) living in USA. Tweets were rated on a
scale from 1 to 9 with a ‘Do not know’ option, and
guidelines regarding how to rate the tweets were
provided to the human raters. The tweets were
selected to contain only English text, no men-
tions and no URL’s to make it possible to rate
them without any additional contextual informa-
tion. Tweets where more than half of the eval-
uators chose ‘Do not know’ were removed (98
tweets).

For each tweet, we select a MTurk rating ran-
dom to be the ‘human evaluation’, and average
over the remaining nine MTurk ratings are av-
eraged to form the ground truth. The ‘senti-
ment label’ for a given tweet is thus deﬁned as
the overall consensus among raters (excluding the
randomly-selected ‘human evaluation’ rating). To
ensure that the label categories are clearly sep-
arated, we removed neutral tweets in the inter-
val [4.5, 5.5] (roughly 29% of the tweets). The
remaining dataset consists of 7 347 tweets. Of
these tweets, 5000 are used for training/validation
and the remaining are used as the test set. Our
DeepMoji model is trained using the chain-thaw
transfer learning approach.

Table 8 shows that the agreement of the random
MTurk rater is 76.1%, meaning that the randomly
selected rater will agree with the average of the
nine other MTurk-ratings of the tweet’s polarity
76.1% of the time. Our DeepMoji model achieves
82.4% agreement, which means it is better at cap-
turing the average human sentiment-rating than a

We have shown how the millions of texts on so-
cial media with emojis can be used for pretrain-
ing models, thereby allowing them to learn repre-
sentations of emotional content in texts. Through
comparison with an identical model pretrained on
a subset of emojis, we ﬁnd that the diversity of
our emoji set is important for the performance of
our method. We release our pretrained DeepMoji
model with the hope that other researchers will
ﬁnd good use of them for various emotion-related
NLP tasks5.

Acknowledgments

The authors would like to thank Janys Analytics
for generously allowing us to use their dataset of
human-rated tweets and the associated code to an-
alyze it. Furthermore, we would like to thank Max
Lever, who helped design the online demo, and
Han Thi Nguyen, who helped code the software
that is provided alongside the pretrained model.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In 3rd International
Conference on Learning Representations (ICLR).

Yoshua Bengio et al. 2012. Deep learning of repre-
sentations for unsupervised and transfer learning. In
29th International Conference on Machine learning
(ICML) – Workshop on Unsupervised and Transfer
Learning, volume 27, pages 17–36.

Sven Buechel and Udo Hahn. 2016. Emotion analy-
sis as a regression problem - dimensional models
and their implications on emotion representation and

5Available with preprocessing code, examples of usage,

benchmark datasets etc. at github.com/bfelbo/deepmoji

metrical evaluation. In 22nd European Conference
on Artiﬁcial Intelligence (ECAI).

Franc¸ois Chollet et al. 2015. Keras. https://

github.com/fchollet/keras.

Ronan Collobert and Jason Weston. 2008. A uni-
ﬁed architecture for natural language processing:
Deep neural networks with multitask learning.
In
25th International Conference on Machine learning
(ICML), pages 160–167.

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
relien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. Swisscheese at semeval-2016 task 4: Sen-
timent classiﬁcation using an ensemble of convo-
lutional neural networks with distant supervision.
Proceedings of SemEval, pages 1124–1128.

Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. Decaf: A deep convolutional activation fea-
In 31th Inter-
ture for generic visual recognition.
national Conference on Machine Learning (ICML),
volume 32, pages 647–655.

Ben Eisner, Tim Rockt¨aschel, Isabelle Augenstein,
Matko Boˇsnjak,
and Sebastian Riedel. 2016.
emoji2vec: Learning emoji representations from
their description. In 4th International Workshop on
Natural Language Processing for Social Media (So-
cialNLP).

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research (JMLR), 11:625–660.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In 30th Conference on Neural In-
formation Processing Systems (NIPS), pages 1019–
1027.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classiﬁcation using distant supervision.
CS224N Project Report, Stanford, 1(12).

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the 22nd interna-
tional conference on World Wide Web (WWW), pages
607–618. ACM.

Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak
Bhattacharyya, and Mark Carman. 2016. Are word

embedding-based features useful for sarcasm detec-
tion? In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efﬁcient text
classiﬁcation. arXiv preprint arXiv:1607.01759.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations
(ICLR).

FA Kunneman, CC Liebrecht, and APJ van den Bosch.
2014. The (un)predictability of emotional hashtags
in twitter. In 52th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL). Associa-
tion for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In 27th Conference on Neural Information Pro-
cessing Systems (NIPS), pages 3111–3119.

Saif Mohammad. 2012. #emotional tweets.

In The
First Joint Conference on Lexical and Computa-
tional Semantics (*SEM), pages 246–255. Associa-
tion for Computational Linguistics.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. Semeval-
2016 task 4: Sentiment analysis in twitter.
In
10th International Workshop on Semantic Evalua-
tion (SemEval), pages 1–18.

Shereen Oraby, Vrindavan Harrison, Lena Reed,
Ernesto Hernandez, Ellen Riloff, and Marilyn
Walker. 2016. Creating and characterizing a diverse
corpus of sarcasm in dialogue. In 17th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue (SIGDIAL), page 31.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classiﬁcation. In ACL student research work-
shop, pages 43–48. Association for Computational
Linguistics.

Hassan Saif, Miriam Fernandez, Yulan He, and Harith
Alani. 2013. Evaluation datasets for twitter senti-
ment analysis: a survey and a new dataset, the sts-
gold. In Workshop: Emotion and Sentiment in So-
cial and Expressive Media: approaches and per-
spectives from AI (ESSEM) at AI*IA Conference.

Valentina Sintsova, Claudiu-Cristian Musat, and Pearl
Fine-grained emotion recognition in
Pu. 2013.
In
olympic tweets based on human computation.
4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis
(WASSA).

In Workshop on Negation and Speculation in Natu-
ral Language Processing (NeSp-NLP), pages 60–68.
Association for Computational Linguistics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J Smola, and Eduard H Hovy. 2016. Hi-
erarchical attention networks for document classiﬁ-
cation. In HLT-NAACL.

A Supplemental Material

A.1 Preprocessing Emotion Datasets

In the Olympic Games dataset by Sintsova et al.
each tweet can be assigned multiple emotions out
of 20 possible emotions, making evaluation difﬁ-
cult. To counter this difﬁculty, we have chosen to
convert the labels to 4 classes of low/high valence
and low/high arousal based on the Geneva Emo-
tion Wheel that the study used. A tweet is deemed
as having emotions within the valence/arousal
class if the average evaluation by raters for that
class is 2.0 or higher, where ‘Low’ = 1, ‘Medium’
= 2 and ‘High’ = 3.

We also evaluate on the ISEAR databank (Wall-
bott and Scherer, 1986), which was created over
many years by a large group of psychologists that
interviewed respondents in 37 countries. Each ob-
servation in the dataset is a self-reported experi-
ence mapped to 1 of 7 possible emotions, making
for an interesting benchmark dataset.

A.2 Pretraining as Regularization

Jonas Sj¨oberg and Lennart Ljung. 1995. Overtraining,
regularization and searching for a minimum, with
application to neural networks. International Jour-
nal of Control, 62(6):1391–1407.

Jacopo Staiano and Marco Guerini. 2014.

De-
pechemood: A lexicon for emotion analysis from
In 52th Annual Meeting
crowd-annotated news.
of the Association for Computational Linguistics
(ACL). Association for Computational Linguistics.

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
In 4th Inter-
2007 task 14: Affective text.
national Workshop on Semantic Evaluations (Se-
mEval), pages 70–74. Association for Computa-
tional Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
In 28th Conference on Neural Information
works.
Processing Systems (NIPS), pages 3104–3112.

Jared Suttles and Nancy Ide. 2013. Distant supervi-
sion for emotion classiﬁcation with discrete binary
In International Conference on Intelligent
values.
Text Processing and Computational Linguistics (CI-
CLing), pages 121–136. Springer.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Learning sentiment-
Liu, and Bing Qin. 2014.
speciﬁc word embedding for twitter sentiment clas-
In 52th Annual Meeting of the Associ-
siﬁcation.
ation for Computational Linguistics (ACL), pages
1555–1565.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints, abs/1605.02688.

Mike Thelwall, Kevan Buckley, and Georgios Pal-
Sentiment strength detection for
toglou. 2012.
Journal of the American Society
the social web.
for Information Science and Technology (JASIST),
63(1):163–173.

Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
Journal of the
detection in short informal text.
American Society for Information Science and Tech-
nology, 61(12):2544–2558.

Marilyn A Walker, Jean E Fox Tree, Pranav Anand,
Rob Abbott, and Joseph King. 2012. A corpus for
In Interna-
research on deliberation and debate.
tional Conference on Language Resources and Eval-
uation (LREC), pages 812–817.

Harald G Wallbott and Klaus R Scherer. 1986. How
universal and speciﬁc is emotional experience? evi-
dence from 27 countries on ﬁve continents. Interna-
tional Social Science Council, 25(4):763–795.

Figure 4: Training statistics on the SS-Youtube
dataset with a pretrained model vs. a untrained
model. The architecture and all hyperparameters
are identical for the two models. All layers are
unfrozen.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A
survey on the role of negation in sentiment analysis.

Figure 4 shows an example of how the pretrain-
ing helps to regularize the target task model, which

otherwise quickly overﬁts. The chain-thaw trans-
fer learning approach further increases this reg-
ularization by ﬁne-tuning the model layer wise,
thereby adding additional regularization.

A.3 Emoticon to Emoji mapping

To analyze the effect of using a diverse emoji set
we create a subset of our pretraining data contain-
ing tweets with one of 8 emojis that are similar to
the positive/negative emoticons used by Tang et al.
(2014) and Hu et al. (2013). The positive emoti-
cons are :) : ) :-) :D =) and the negative emoticons
are :( : ( :-(. We ﬁnd the 8 similar emojis in our
dataset seen in Figure 5 as use these for creating
the reduced subset.

Figure 5: Emojis used for the experiment on the
importance of a diverse noisy label set.

A.4 Emoji Clustering

We compute the predictions of the DeepMoji
model on the pretraining test set containing 640K
tweets and compute the correlation matrix of the
predicted probabilities seen in Figure 7. Then we
use hierarchical clustering with average linkage
on the correlation matrix to generate the dendro-
gram seen in Figure 6. We visualized dendrograms
for various versions of our model and the over-
all structure is very stable with only a few emojis
changing places in the hierarchy.

Figure 6: Hierarchical clustering of the DeepMoji model’s predictions across categories on the test set.
The dendrogram shows how the model learns to group emojis into overall categories and subcategories
based on emotional content. The y-axis is the distance on the correlation matrix of the model’s predic-
tions measured using average linkage.

Figure 7: Correlation matrix of the model’s predictions on the pretraining test set.

Using millions of emoji occurrences to learn any-domain representations
for detecting sentiment, emotion and sarcasm

Bjarke Felbo1, Alan Mislove2, Anders Søgaard3, Iyad Rahwan1, Sune Lehmann4

1Media Lab, Massachusetts Institute of Technology
2College of Computer and Information Science, Northeastern University
3Department of Computer Science, University of Copenhagen
4DTU Compute, Technical University of Denmark

ing the distant supervision to a more diverse set of
noisy labels enables the models to learn richer rep-
resentations of emotional content in text, thereby
obtaining better performance on benchmarks for
detecting sentiment, emotions and sarcasm. We
show that the learned representation of a single
pretrained model generalizes across 5 domains.

Table 1: Example sentences scored by our model.
For each text the top ﬁve most likely emojis are
shown with the model’s probability estimates.

Abstract

NLP tasks are often limited by scarcity of
manually annotated data.
In social me-
dia sentiment analysis and related tasks,
researchers have therefore used binarized
emoticons and speciﬁc hashtags as forms
of distant supervision. Our paper shows
that by extending the distant supervision
to a more diverse set of noisy labels, the
models can learn richer representations.
Through emoji prediction on a dataset of
1246 million tweets containing one of 64
common emojis we obtain state-of-the-
art performance on 8 benchmark datasets
within sentiment, emotion and sarcasm de-
tection using a single pretrained model.
Our analyses conﬁrm that the diversity of
our emotional labels yield a performance
improvement over previous distant super-
vision approaches.

1

Introduction

A variety of NLP tasks are limited by scarcity of
manually annotated data. Therefore, co-occurring
emotional expressions have been used for dis-
tant supervision in social media sentiment anal-
ysis and related tasks to make the models learn
useful text representations before modeling these
tasks directly. For instance, the state-of-the-art ap-
proaches within sentiment analysis of social me-
dia data use positive/negative emoticons for train-
ing their models (Deriu et al., 2016; Tang et al.,
2014). Similarly, hashtags such as #anger, #joy,
#happytweet, #ugh, #yuck and #fml have in pre-
vious research been mapped into emotional cate-
gories for emotion analysis (Mohammad, 2012).

Distant supervision on noisy labels often en-
ables a model to obtain better performance on the
In this paper, we show that extend-
target task.

Emojis are not always a direct labeling of emo-
tional content. For instance, a positive emoji may
serve to disambiguate an ambiguous sentence or to
complement an otherwise relatively negative text.
Kunneman et al. (2014) discuss a similar duality
in the use of emotional hashtags such as #nice and
#lame. Nevertheless, our work shows that emo-
jis can be used to classify the emotional content
of texts accurately in many cases. For instance,
our DeepMoji model captures varied usages of the
word ‘love’ as well as slang such as ‘this is the
shit’ being a positive statement (see Table 1). We
provide an online demo at deepmoji.mit.edu to al-
low others to explore the predictions of our model.

Contributions We show how millions of read-
ily available emoji occurrences on Twitter can be
used to pretrain models to learn a richer emotional

7
1
0
2
 
t
c
O
 
7
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
2
5
0
0
.
8
0
7
1
:
v
i
X
r
a

representation than traditionally obtained through
distant supervision. We transfer this knowledge to
the target tasks using a new layer-wise ﬁne-tuning
method, obtaining improvements over the state-
of-the-art within a range of tasks: emotion, sar-
casm and sentiment detection. We present multi-
ple analyses on the effect of pretraining, including
results that show that the diversity of our emoji set
is important for the transfer learning potential of
our model. Our pretrained DeepMoji model is re-
leased with the hope that other researchers can use
it for various NLP tasks1.

2 Related work

Using emotional expressions as noisy labels in
text to counter scarcity of labels is not a new
idea (Read, 2005; Go et al., 2009). Originally, bi-
narized emoticons were used as noisy labels, but
later also hashtags and emojis have been used.
To our knowledge, previous research has always
manually speciﬁed which emotional category each
emotional expression belong to. Prior work has
used theories of emotion such as Ekman’s six
basic emotions and Plutchik’s eight basic emo-
tions (Mohammad, 2012; Suttles and Ide, 2013).

Such manual categorization requires an under-
standing of the emotional content of each expres-
sion, which is difﬁcult and time-consuming for
sophisticated combinations of emotional content.
Moreover, any manual selection and categoriza-
tion is prone to misinterpretations and may omit
important details regarding usage. In contrast, our
approach requires no prior knowledge of the cor-
pus and can capture diverse usage of 64 types of
emojis (see Table 1 for examples and Figure 3 for
how the model implicitly groups emojis).

Another way of automatically interpreting the
emotional content of an emoji is to learn emoji
embeddings from the words describing the emoji-
semantics in ofﬁcial emoji tables (Eisner et al.,
2016). This approach, in our context, suffers from
two severe limitations: a) It requires emojis at test
time while there are many domains with limited
or no usage of emojis. b) The tables do not cap-
ture the dynamics of emoji usage, i.e., drift in an
emoji’s intended meaning over time.

Knowledge can be transferred from the emoji
dataset to the target task in many different ways.
In particular, multitask learning with simultaneous

1Available with preprocessing code, examples of usage,

benchmark datasets etc. at github.com/bfelbo/deepmoji

Figure 1: Illustration of the DeepMoji model with
T being text length and C the number of classes.

training on multiple datasets has shown promis-
ing results (Collobert and Weston, 2008). How-
ever, multitask learning requires access to the
emoji dataset whenever the classiﬁer needs to be
tuned for a new target task. Requiring access
to the dataset is problematic in terms of violat-
ing data access regulations. There are also is-
sues from a data storage perspective as the dataset
used for this research contains hundreds of mil-
lions of tweets (see Table 2). Instead we use trans-
fer learning (Bengio et al., 2012) as described in
§3.3, which does not require access to the original
dataset, but only the pretrained classiﬁer.

3 Method

3.1 Pretraining

In many cases, emojis serve as a proxy for the
emotional contents of a text. Therefore, pretrain-
ing on the classiﬁcation task of predicting which
emoji were initially part of a text can improve per-
formance on the target task (see §5.3 for an anal-
ysis of why our pretraining helps). Social media
contains large amounts of short texts with emojis
that can be utilized as noisy labels for pretraining.
Here, we use data from Twitter from January 1st
2013 to June 1st 2017, but any dataset with emoji
occurrences could be used.

Only English tweets without URL’s are used for
the pretraining dataset. Our hypothesis is that the
content obtained from the URL is likely to be im-
portant for understanding the emotional content of
the text in the tweet. Therefore, we expect emo-
jis associated with these tweets to be noiser labels

than for tweets without URLs, and the tweets with
URLs are thus removed.

Proper tokenization is important for generaliza-
tion. All tweets are tokenized on a word-by-word
basis. Words with 2 or more repeated characters
are shortened to the same token (e.g. ‘loool’ and
‘looooool’ are tokenized such that they are treated
the same). Similarly, we use a special token for all
URLs (only relevant for benchmark datasets), user
mentions (e.g. ‘@acl2017’ and ‘@emnlp2017’ are
thus treated the same) and numbers. To be in-
cluded in the training set the tweet must contain
at least 1 token that is not a punctuation symbol,
emoji or special token2.

Many tweets contain multiple repetitions of the
In the
same emoji or multiple different emojis.
training data, we address this in the following way.
For each unique emoji type, we save a separate
tweet for the pretraining with that emoji type as the
label. We only save a single tweet for the pretrain-
ing per unique emoji type regardless of the number
of emojis associated with the tweet. This data pre-
processing allows the pretraining task to capture
that multiple types of emotional content are asso-
ciated with the tweet while making our pretraining
task a single-label classiﬁcation instead of a more
complicated multi-label classiﬁcation.

To ensure that the pretraining encourages the
models to learn a rich understanding of emotional
content in text rather than only emotional content
associated with the most used emojis, we create
a balanced pretraining dataset. The pretraining
data is split into a training, validation and test set,
where the validation and test set is randomly sam-
pled in such a way that each emoji is equally repre-
sented. The remaining data is upsampled to create
a balanced training dataset.

3.2 Model

With the millions of emoji occurrences available,
we can train very expressive classiﬁers with lim-
ited risk of overﬁtting. We use a variant of the
Long Short-Term Memory (LSTM) model that has
been successful at many NLP tasks (Hochreiter
and Schmidhuber, 1997; Sutskever et al., 2014).
Our DeepMoji model uses an embedding layer of
256 dimensions to project each word into a vector
space. A hyperbolic tangent activation function is
used to enforce a constraint of each embedding di-
mension being within [−1, 1]. To capture the con-

2Details available at github.com/bfelbo/deepmoji

text of each word we use two bidirectional LSTM
layers with 1024 hidden units in each (512 in each
direction). Finally, an attention layer that take all
of these layers as input using skip-connections is
used (see Figure 1 for an illustration).

The attention mechanism lets the model decide
the importance of each word for the prediction task
by weighing them when constructing the represen-
tation of the text. For instance, a word such as
‘amazing’ is likely to be very informative of the
emotional meaning of a text and it should thus be
treated accordingly. We use a simple approach
inspired by (Bahdanau et al., 2014; Yang et al.,
2016) with a single parameter pr. input channel:

et = htwa

at =

exp(et)
i=1 exp(ei)

(cid:80)T

v =

aihi

T
(cid:88)

i=1

Here ht is the representation of the word at time
step t and wa is the weight matrix for the atten-
tion layer. The attention importance scores for
each time step, at, are obtained by multiplying the
representations with the weight matrix and then
normalizing to construct a probability distribution
over the words. Lastly, the representation vector
for the text, v, is found by a weighted summation
over all the time steps using the attention impor-
tance scores as weights. This representation vec-
tor obtained from the attention layer is a high-level
encoding of the entire text, which is used as input
to the ﬁnal Softmax layer for classiﬁcation. We
ﬁnd that adding the attention mechanism and skip-
connections improves the model’s capabilities for
transfer learning (see §5.2 for more details).

The only regularization used for the pretrain-
ing task is a L2 regularization of 1E−6 on the
embedding weights. For the ﬁnetuning additional
regularization is applied (see §4.2). Our model is
implemented using Theano (Theano Development
Team, 2016) and we make an easy-to-use version
available that uses Keras (Chollet et al., 2015).

3.3 Transfer learning

Our pretrained model can be ﬁne-tuned to the tar-
get task in multiple ways with some approaches
‘freezing’ layers by disabling parameters updates
to prevent overﬁtting. One common approach is

to use the network as a feature extractor (Don-
ahue et al., 2014), where all layers in the model are
frozen when ﬁne-tuning on the target task except
the last layer (hereafter referred to as the ‘last’ ap-
proach). Alternatively, another common approach
is to use the pretrained model as an initializa-
tion (Erhan et al., 2010), where the full model is
unfrozen (hereafter referred to as ‘full’).

We propose a new simple transfer learning ap-
proach, ‘chain-thaw’, that sequentially unfreezes
and ﬁne-tunes a single layer at a time. This ap-
proach increases accuracy on the target task at the
expense of extra computational power needed for
the ﬁne-tuning. By training each layer separately
the model is able to adjust the individual patterns
across the network with a reduced risk of overﬁt-
ting. The sequential ﬁne-tuning seems to have a
regularizing effect similar to what has been exam-
ined with layer-wise training in the context of un-
supervised learning (Erhan et al., 2010).

More speciﬁcally, the chain-thaw approach ﬁrst
ﬁne-tunes any new layers (often only a Softmax
layer) to the target task until convergence on a
validation set. Then the approach ﬁne-tunes each
layer individually starting from the ﬁrst layer in
the network. Lastly, the entire model is trained
with all layers. Each time the model converges
as measured on the validation set, the weights
are reloaded to the best setting, thereby prevent-
ing overﬁtting in a similar manner to early stop-
ping (Sj¨oberg and Ljung, 1995). This process is
illustrated in Figure 2. Note how only perform-
ing step a) in the ﬁgure is identical to the ‘last’
approach, where the existing network is used as
a feature extractor. Similarly, only doing step d)
is identical to the ‘full’ approach, where the pre-
trained weights are used as an initialization for a
fully trainable network. Although the chain-thaw
procedure may seem extensive it is easily imple-
mented with only a few lines of code. Similarly,
the additional time spent on ﬁne-tuning is limited
when the target task uses GPUs on small datasets
of manually annotated data as is often the case.

A beneﬁt of the chain-thaw approach is the abil-
ity to expand the vocabulary to new domains with
little risk of overﬁtting. For a given dataset up to
10000 new words from the training set are added
to the vocabulary. §5.3 contains analysis on the
added word coverage gained from this approach.

Figure 2: Illustration of the chain-thaw transfer
learning approach, where each layer is ﬁne-tuned
separately. Layers covered with a blue rectangle
are frozen. Step a) tunes any new layers, b) then
tunes the 1st layer and c) the next layer until all
layers have been ﬁne-tuned individually. Lastly,
in step d) all layers are ﬁne-tuned together.

Table 2: The number of tweets in the pretraining
dataset associated with each emoji in millions.

4 Experiments

4.1 Emoji prediction

We use a raw dataset of 56.6 billion tweets, which
is then ﬁltered to 1.2 billion relevant tweets (see
details in §3.1). In the pretraining dataset a copy
of a single tweet is stored once for each unique
emoji, resulting in a dataset consisting of 1.6 bil-
lion tweets. Table 2 shows the distribution of
tweets across different emoji types. To evaluate
performance on the pretraining task a validation
set and a test set both containing 640K tweets
(10K of each emoji type) are used. The remain-
ing tweets are used for the training set, which is
balanced using upsampling.

The performance of the DeepMoji model is
evaluated on the pretraining task with the results
shown in Table 3. Both top 1 and top 5 accuracy
is used for the evaluation as the emoji labels are
noisy with multiple emojis being potentially cor-
rect for any given sentence. For comparison we
also train a version of our DeepMoji model with
smaller LSTM layers and a bag-of-words classi-
ﬁer, fastText, that has recently shown competitive
results (Joulin et al., 2016). We use 256 dimen-

Table 3: Accuracy of classiﬁers on the emoji
prediction task. d refers to the dimensionality of
each LSTM layer. Parameters are in millions.

Params

Top 1

Top 5

−
Random
12.8
fasttext
15.5
DeepMoji (d = 512)
DeepMoji (d = 1024) 22.4

1.6%
7.8%
12.8% 36.2%
16.7% 43.3%
17.0% 43.8%

sions for this fastText classiﬁer, thereby making it
almost identical to only using the embedding layer
from the DeepMoji model. The difference in top
5 accuracy between the fastText classiﬁer (36.2%)
and the largest DeepMoji model (43.8%) under-
lines the difﬁculty of the emoji prediction task. As
the two classiﬁers only differ in that the DeepMoji
model has LSTM layers and an attention layer be-
tween the embedding and Softmax layer, this dif-
ference in accuracy demonstrates the importance
of capturing the context of each word.

4.2 Benchmarking

We benchmark our method on 3 different NLP
tasks using 8 datasets across 5 domains. To make
for a fair comparison, we compare DeepMoji
to other methods that also utilize external data
sources in addition to the benchmark dataset. An
averaged F1-measure across classes is used for
evaluation in emotion analysis and sarcasm detec-
tion as these consist of unbalanced datasets while
sentiment datasets are evaluated using accuracy.

An issue with many of the benchmark datasets
is data scarcity, which is particularly problem-
atic within emotion analysis. Many recent pa-
pers proposing new methods for emotion analysis
such as (Staiano and Guerini, 2014) only evaluate
performance on a single benchmark dataset, Se-
mEval 2007 Task 14, that contains 1250 observa-
tions. Recently, criticism has been raised concern-
ing the use of correlation with continuous ratings
as a measure (Buechel and Hahn, 2016), making
only the somewhat limited binary evaluation pos-
sible. We only evaluate the emotions {Fear, Joy,
Sadness} as the remaining emotions occur in less
than 5% of the observations.

To fully evaluate our method on emotion analy-
sis against the current methods we thus make use
of two other datasets: A dataset of emotions in
tweets related to the Olympic Games created by
that we convert to a single-label
Sintsova et al.

classiﬁcation task and a dataset of self-reported
emotional experiences created by a large group
of psychologists (Wallbott and Scherer, 1986).
See the supplementary material for details on the
datasets and the preprocessing. As these two
datasets do not have prior evaluations, we eval-
uate against a state-of-the-art approach, which
is based on a valence-arousal-dominance frame-
work (Buechel and Hahn, 2016). The scores ex-
tracted using this approach are mapped to the
classes in the datasets using a logistic regres-
sion with parameter optimization using cross-
validation. We release our preprocessing code and
hope that these 2 two datasets will be used for fu-
ture benchmarking within emotion analysis.

We evaluate sentiment analysis performance on
three benchmark datasets. These small datasets
are chosen to emphasize the importance of the
transfer learning ability of the evaluated models.
Two of the datasets are from SentiStrength (Thel-
wall et al., 2010), SS-Twitter and SS-Youtube,
and follow the relabeling described in (Saif et al.,
2013) to make the labels binary. The third dataset
is from SemEval 2016 Task4A (Nakov et al.,
2016). Due to tweets being deleted from Twitter,
the SemEval dataset suffers from data decay, mak-
ing it difﬁcult to compare results across papers. At
the time of writing, roughly 15% of the training
dataset for SemEval 2016 Task 4A was impossible
to obtain. We choose not to use review datasets for
sentiment benchmarking as these datasets contain
so many words pr. observation that even bag-of-
words classiﬁers and unsupervised approaches can
obtain a high accuracy (Joulin et al., 2016; Rad-
ford et al., 2017).

The current state of the art for sentiment analy-
sis on social media (and winner of SemEval 2016
Task 4A) uses an ensemble of convolutional neu-
ral networks that are pretrained on a private dataset
of tweets with emoticons, making it difﬁcult to
replicate (Deriu et al., 2016). Instead we pretrain
a model with the hyperparameters of the largest
model in their ensemble on the positive/negative
emoticon dataset from Go et al. (2009). Using
this pretraining as an initialization we ﬁnetune
the model on the target tasks using early stop-
ping on a validation set to determine the amount
of training. We also implemented the Sentiment-
Speciﬁc Word Embedding (SSWE) using the em-
beddings available on the authors’ website (Tang
et al., 2014), but found that it performed worse

Table 4: Description of benchmark datasets. Datasets without pre-existing training/test splits are split by
us (with splits publicly available). Data used for hyperparameter tuning is taken from the training set.

Study

Domain

Classes Ntrain Ntest

Identiﬁer

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

(Strapparava and Mihalcea, 2007)
(Sintsova et al., 2013)
(Wallbott and Scherer, 1986)

(Thelwall et al., 2012)
(Thelwall et al., 2012)
(Nakov et al., 2016)

(Walker et al., 2012)
(Oraby et al., 2016)

Task

Emotion
Emotion
Emotion

Headlines
Tweets
Experiences

Sentiment
Sentiment Video Comments
Sentiment

Tweets

Tweets

Sarcasm
Sarcasm

Debate Forums
Debate Forums

250
250
1000

1000
1000
7155

1000
1000

1000
709
6480

1113
1142
31986

995
2260

Table 5: Comparison across benchmark datasets. Reported values are averages across ﬁve runs. Varia-
tions refer to transfer learning approaches in §3.3 with ‘new’ being a model trained without pretraining.

Dataset

Measure

State of the art

DeepMoji
(new)

DeepMoji
(full)

DeepMoji
(last)

DeepMoji
(chain-thaw)

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

F1
F1
F1

Acc
Acc
Acc

F1
F1

.34 [Buechel]
.50 [Buechel]
.45 [Buechel]

.82 [Deriu]
.86 [Deriu]
.51 [Deriu]3

.63 [Joshi]
.72 [Joshi]

.21
.43
.32

.62
.75
.51

.67
.71

.31
.50
.42

.85
.88
.54

.65
.71

.37
.61
.57

.88
.93
.58

.69
.75

3
4
7

2
2
3

2
2

.36
.61
.56

.87
.92
.58

.68
.74

than the pretrained convolutional neural network.
These results are therefore excluded.

For sarcasm detection we use the sarcasm
dataset version 1 and 2 from the Internet Argu-
ment Corpus (Walker et al., 2012). Note that
results presented on these benchmarks in e.g.
Oraby et al. (2016) are not directly comparable
as only a subset of the data is available online.4
A state-of-the-art baseline is found by modeling
the embedding-based features from Joshi et al.
(2016) alongside unigrams, bigrams and trigrams
with an SVM. GoogleNews word2vec embed-
dings (Mikolov et al., 2013) are used for comput-
ing the embedding-based features. A hyperparam-
eter search for regularization parameters is carried
out using cross-validation. Note that the sarcasm
dataset version 2 contains both a quoted text and a
sarcastic response, but to keep the models identi-
cal across the datasets only the response is used.

For

training we

the Adam opti-
mizer (Kingma and Ba, 2015) with gradient

use

3The authors report a higher accuracy in their paper,
which is likely due to having a larger training dataset as they
were able to obtain it before data decay occurred.

4We contacted the authors, but were unable to obtain the

full dataset for neither version 1 or version 2.

clipping of the norm to 1. Learning rate is set to
1E−3 for training of all new layers and 1E−4
for ﬁnetuning any pretrained layers. To prevent
overﬁtting on the small datasets, 10% of the
channels across all words in the embedding layer
are dropped out during training. Unlike e.g. (Gal
and Ghahramani, 2016) we do not drop out entire
words in the input as some of our datasets contain
it could
observations with so few words that
change the meaning of the text.
In addition to
the embedding dropout, L2 regularization for the
embedding weights is used and 50% dropout is
applied to the penultimate layer.

Table 5 shows that the DeepMoji model out-
performs the state of the art across all benchmark
datasets and that our new ‘chain-thaw’ approach
consistently yields the highest performance for the
transfer learning, albeit often only slightly better
or equal to the ‘last’ approach. Results are aver-
aged across 5 runs to reduce the variance. We test
the statistical signiﬁcance of our results by com-
paring the performance of DeepMoji (chain-thaw)
the state of the art. Bootstrap testing with
vs.
10000 samples is used. Our results are statisti-
cally signiﬁcantly better than the state of the art

with p < 0.001 on every benchmark dataset.

useful for transfer learning.

Our model is able to out-perform the state-of-
the-art on datasets that originate from domains that
differ substantially from the tweets on which it
was pretrained. A key difference between the pre-
training dataset and the benchmark datasets is the
length of the observations. The average number of
tokens pr.
tweet in the pretraining dataset is 11,
whereas e.g. the board posts from the Internet Ar-
gument Corpus version 1 (Oraby et al., 2016) has
an average of 66 tokens with some observations
being much longer.

5 Model Analysis

5.1

Importance of emoji diversity

One of the major differences between this work
compared to previous papers using distant super-
vision is the diversity of the noisy labels used (see
§2). For instance, both Deriu et al. (2016) and
Tang et al. (2014) only used positive and negative
emoticons as noisy labels. Other instances of pre-
vious work have used slightly more nuanced sets
of noisy labels (see §2), but to our knowledge our
set of noisy labels is the most diverse yet. To an-
alyze the effect of using a diverse emoji set we
create a subset of our pretraining data containing
tweets with one of 8 emojis that are similar to
the positive/negative emoticons used by Tang et al.
(2014) and Hu et al. (2013) (the set of emoticons
and corresponding emojis are available in the sup-
plemental material). As the dataset based on this
reduced set of emojis contains 433M tweets, any
difference in performance on benchmark datasets
is likely linked to the diversity of labels rather than
differences in dataset sizes.

We train our DeepMoji model

to predict
whether the tweets contain a positive or negative
emoji and evaluate this pretrained model across
the benchmark datasets. We refer to the model
trained on the subset of emojis as DeepMoji-
PosNeg (as opposed to DeepMoji). To test the
emotional representations learned by the two pre-
trained models the ‘last’ transfer learning ap-
proach is used for the comparison, thereby only
allowing the models to map already learned fea-
tures to classes in the target dataset. Table 6 shows
that DeepMoji-PosNeg yields lower performance
compared to DeepMoji across all 8 benchmarks,
thereby showing that the diversity of our emoji
types encourage the model to learn a richer repre-
sentation of emotional content in text that is more

Table 6: Benchmarks using a smaller emoji set
(Pos/Neg emojis) or a classic architecture (stan-
dard LSTM). Results for DeepMoji from Table 5
are added for convenience. Evaluation metrics are
as in Table 5. Reported values are the averages
across ﬁve runs.

Pos/Neg
emojis

Standard
LSTM

DeepMoji

Dataset

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

.32
.55
.40

.86
.90
.56

.66
.72

.35
.57
.49

.86
.91
.57

.66
.73

.36
.61
.56

.87
.92
.58

.68
.74

Many of the emojis carry similar emotional
content, but have subtle differences in usage that
our model is able to capture. Through hierar-
chical clustering on the correlation matrix of the
DeepMoji model’s predictions on the test set we
can see that the model captures many similarities
that one would intuitively expect (see Figure 3).
For instance, the model groups emojis into overall
categories associated with e.g. negativity, positiv-
ity or love. Similarly, the model learns to differen-
tiate within these categories, mapping sad emojis
in one subcategory of negativity, annoyed in an-
other subcategory and angry in a third one.

5.2 Model architecture

Our DeepMoji model architecture as described
in §3.2 use an attention mechanism and skip-
connections to ease the transfer of the learned rep-
resentation to new domains and tasks. Here we
compare the DeepMoji model architecture to that
of a standard 2-layer LSTM, both compared using
the ‘last’ transfer learning approach. We use the
same regularization and training parameters.

As seen in Table 6 the DeepMoji model per-
forms better than a standard 2-layer LSTM across
all benchmark datasets. The two architectures per-
formed equally on the pretraining task, suggesting
that while the DeepMoji model architecture is in-
deed better for transfer learning, it may not neces-
sarily be better for single supervised classiﬁcation
task with ample available data.

A reasonable conjecture is that the improved
transfer learning performance is due to two fac-

Figure 3: Hierarchical clustering of the DeepMoji model’s predictions across categories on the test set.
The dendrogram shows how the model learns to group emojis into overall categories and subcategories
based on emotional content. The y-axis is the distance on the correlation matrix of the model’s predic-
tions measured using average linkage. More details are available in the supplementary material.

tors:
a) the attention mechanism with skip-
connections provide easy access to learned low-
level features for any time step, making it easy to
use this information if needed for a new task b)
the improved gradient-ﬂow from the output layer
to the early layers in the network due to skip-
connections (Graves, 2013) is important when ad-
justing parameters in early layers as part of trans-
fer learning to small datasets. Detailed analysis of
whether these factors actually explain why our ar-
chitecture outperform a standard 2-layer LSTM is
left for future work.

5.3 Analyzing the effect of pretraining

Performance on the target task beneﬁts strongly
from pretraining as shown in Table 5 by compar-
ing DeepMoji (new) to DeepMoji (chain-thaw).
In this section we experimentally decompose the
beneﬁt of pretraining into 2 effects: word coverage
and phrase coverage. These two effects help regu-
larize the model by preventing overﬁtting (see the
supplementary details for an visualization of the
effect of this regularization).

There are numerous ways to express a speciﬁc
sentiment, emotion or sarcastic comment. Conse-
quently, the test set may contain speciﬁc language
use not present in the training set. The pretraining
helps the target task models attend to low-support
evidence by having previously observed similar
usage in the pretraining dataset. We ﬁrst exam-
ine this effect by measuring the improvement in
word coverage on the test set when using the pre-
training with word coverage being deﬁned as the
% of words in the test dataset seen in the train-
ing/pretraining dataset (see Table 7). An impor-
tant reason why the ‘chain-thaw’ approach outper-
forms other transfer learning approaches is can be

used to tune the embedding layer with limited risk
of overﬁtting. Table 7 shows the increased word
coverage from adding new words to the vocabu-
lary as part of that tuning.

Note that word coverage can be a misleading
metric in this context as for many of these small
datasets a word will often occur only once in the
training set.
In contrast, all of the words in the
pretraining vocabulary are present in thousands (if
not millions) of observations in the emoji pretrain-
ing dataset thus making it possible for the model
to learn a good representation of the emotional
and semantic meaning. The added beneﬁt of pre-
training for learning word representations there-
fore likely extends beyond the differences seen in
Table 7.

Table 7: Word coverage on benchmark test sets
using only the vocabulary generated by ﬁnding
words in the training data (‘own’), the pretrain-
ing vocabulary (‘last’) or a combination of both
vocabularies (‘full / chain-thaw’).

Dataset

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

Own

41.9%
73.9%
85.4%

80.1%
79.6%
86.1%

88.7%
86.5%

Last

93.6%
90.3%
98.5%

97.1%
97.2%
96.6%

97.3%
97.2%

Full /
Chain-thaw

94.0%
96.0%
98.8%

97.2%
97.3%
97.0%

98.0%
98.0%

To examine the importance of capturing phrases
and the context of each word, we evaluate the ac-
curacy on the SS-Youtube dataset using a fastText
classiﬁer pretrained on the same emoji dataset as

our DeepMoji model. This fastText classiﬁer is al-
most identical to only using the embedding layer
from the DeepMoji model. We evaluate the rep-
resentations learned by ﬁne-tuning the models as
feature extractors (i.e. using the ‘last’ transfer
learning approach). The fastText model achieves
an accuracy of 63% as compared to 93% for our
DeepMoji model,
thereby emphasizing the im-
portance of phrase coverage. One concept that
the LSTM layers likely learn is negation, which
is known to be important for sentiment analy-
sis (Wiegand et al., 2010).

single MTurk rater.

Table 8: Comparison of agreement between clas-
siﬁers and the aggregate opinion of Amazon
Mechanical Turkers on sentiment prediction of
tweets.

Random
fastText
MTurk
DeepMoji

Agreement

50.1%
71.0%
76.1%
82.4%

5.4 Comparing with human-level agreement

6 Conclusion

To understand how well our DeepMoji classi-
ﬁer performs compared to humans, we created a
new dataset of random tweets annotated for senti-
ment. Each tweet was annotated by a minimum of
10 English-speaking Amazon Mechanical Turkers
(MTurk’s) living in USA. Tweets were rated on a
scale from 1 to 9 with a ‘Do not know’ option, and
guidelines regarding how to rate the tweets were
provided to the human raters. The tweets were
selected to contain only English text, no men-
tions and no URL’s to make it possible to rate
them without any additional contextual informa-
tion. Tweets where more than half of the eval-
uators chose ‘Do not know’ were removed (98
tweets).

For each tweet, we select a MTurk rating ran-
dom to be the ‘human evaluation’, and average
over the remaining nine MTurk ratings are av-
eraged to form the ground truth. The ‘senti-
ment label’ for a given tweet is thus deﬁned as
the overall consensus among raters (excluding the
randomly-selected ‘human evaluation’ rating). To
ensure that the label categories are clearly sep-
arated, we removed neutral tweets in the inter-
val [4.5, 5.5] (roughly 29% of the tweets). The
remaining dataset consists of 7 347 tweets. Of
these tweets, 5000 are used for training/validation
and the remaining are used as the test set. Our
DeepMoji model is trained using the chain-thaw
transfer learning approach.

Table 8 shows that the agreement of the random
MTurk rater is 76.1%, meaning that the randomly
selected rater will agree with the average of the
nine other MTurk-ratings of the tweet’s polarity
76.1% of the time. Our DeepMoji model achieves
82.4% agreement, which means it is better at cap-
turing the average human sentiment-rating than a

We have shown how the millions of texts on so-
cial media with emojis can be used for pretrain-
ing models, thereby allowing them to learn repre-
sentations of emotional content in texts. Through
comparison with an identical model pretrained on
a subset of emojis, we ﬁnd that the diversity of
our emoji set is important for the performance of
our method. We release our pretrained DeepMoji
model with the hope that other researchers will
ﬁnd good use of them for various emotion-related
NLP tasks5.

Acknowledgments

The authors would like to thank Janys Analytics
for generously allowing us to use their dataset of
human-rated tweets and the associated code to an-
alyze it. Furthermore, we would like to thank Max
Lever, who helped design the online demo, and
Han Thi Nguyen, who helped code the software
that is provided alongside the pretrained model.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In 3rd International
Conference on Learning Representations (ICLR).

Yoshua Bengio et al. 2012. Deep learning of repre-
sentations for unsupervised and transfer learning. In
29th International Conference on Machine learning
(ICML) – Workshop on Unsupervised and Transfer
Learning, volume 27, pages 17–36.

Sven Buechel and Udo Hahn. 2016. Emotion analy-
sis as a regression problem - dimensional models
and their implications on emotion representation and

5Available with preprocessing code, examples of usage,

benchmark datasets etc. at github.com/bfelbo/deepmoji

metrical evaluation. In 22nd European Conference
on Artiﬁcial Intelligence (ECAI).

Franc¸ois Chollet et al. 2015. Keras. https://

github.com/fchollet/keras.

Ronan Collobert and Jason Weston. 2008. A uni-
ﬁed architecture for natural language processing:
Deep neural networks with multitask learning.
In
25th International Conference on Machine learning
(ICML), pages 160–167.

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
relien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. Swisscheese at semeval-2016 task 4: Sen-
timent classiﬁcation using an ensemble of convo-
lutional neural networks with distant supervision.
Proceedings of SemEval, pages 1124–1128.

Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. Decaf: A deep convolutional activation fea-
In 31th Inter-
ture for generic visual recognition.
national Conference on Machine Learning (ICML),
volume 32, pages 647–655.

Ben Eisner, Tim Rockt¨aschel, Isabelle Augenstein,
Matko Boˇsnjak,
and Sebastian Riedel. 2016.
emoji2vec: Learning emoji representations from
their description. In 4th International Workshop on
Natural Language Processing for Social Media (So-
cialNLP).

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research (JMLR), 11:625–660.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In 30th Conference on Neural In-
formation Processing Systems (NIPS), pages 1019–
1027.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classiﬁcation using distant supervision.
CS224N Project Report, Stanford, 1(12).

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the 22nd interna-
tional conference on World Wide Web (WWW), pages
607–618. ACM.

Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak
Bhattacharyya, and Mark Carman. 2016. Are word

embedding-based features useful for sarcasm detec-
tion? In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efﬁcient text
classiﬁcation. arXiv preprint arXiv:1607.01759.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations
(ICLR).

FA Kunneman, CC Liebrecht, and APJ van den Bosch.
2014. The (un)predictability of emotional hashtags
in twitter. In 52th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL). Associa-
tion for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In 27th Conference on Neural Information Pro-
cessing Systems (NIPS), pages 3111–3119.

Saif Mohammad. 2012. #emotional tweets.

In The
First Joint Conference on Lexical and Computa-
tional Semantics (*SEM), pages 246–255. Associa-
tion for Computational Linguistics.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. Semeval-
2016 task 4: Sentiment analysis in twitter.
In
10th International Workshop on Semantic Evalua-
tion (SemEval), pages 1–18.

Shereen Oraby, Vrindavan Harrison, Lena Reed,
Ernesto Hernandez, Ellen Riloff, and Marilyn
Walker. 2016. Creating and characterizing a diverse
corpus of sarcasm in dialogue. In 17th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue (SIGDIAL), page 31.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classiﬁcation. In ACL student research work-
shop, pages 43–48. Association for Computational
Linguistics.

Hassan Saif, Miriam Fernandez, Yulan He, and Harith
Alani. 2013. Evaluation datasets for twitter senti-
ment analysis: a survey and a new dataset, the sts-
gold. In Workshop: Emotion and Sentiment in So-
cial and Expressive Media: approaches and per-
spectives from AI (ESSEM) at AI*IA Conference.

Valentina Sintsova, Claudiu-Cristian Musat, and Pearl
Fine-grained emotion recognition in
Pu. 2013.
In
olympic tweets based on human computation.
4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis
(WASSA).

In Workshop on Negation and Speculation in Natu-
ral Language Processing (NeSp-NLP), pages 60–68.
Association for Computational Linguistics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J Smola, and Eduard H Hovy. 2016. Hi-
erarchical attention networks for document classiﬁ-
cation. In HLT-NAACL.

A Supplemental Material

A.1 Preprocessing Emotion Datasets

In the Olympic Games dataset by Sintsova et al.
each tweet can be assigned multiple emotions out
of 20 possible emotions, making evaluation difﬁ-
cult. To counter this difﬁculty, we have chosen to
convert the labels to 4 classes of low/high valence
and low/high arousal based on the Geneva Emo-
tion Wheel that the study used. A tweet is deemed
as having emotions within the valence/arousal
class if the average evaluation by raters for that
class is 2.0 or higher, where ‘Low’ = 1, ‘Medium’
= 2 and ‘High’ = 3.

We also evaluate on the ISEAR databank (Wall-
bott and Scherer, 1986), which was created over
many years by a large group of psychologists that
interviewed respondents in 37 countries. Each ob-
servation in the dataset is a self-reported experi-
ence mapped to 1 of 7 possible emotions, making
for an interesting benchmark dataset.

A.2 Pretraining as Regularization

Jonas Sj¨oberg and Lennart Ljung. 1995. Overtraining,
regularization and searching for a minimum, with
application to neural networks. International Jour-
nal of Control, 62(6):1391–1407.

Jacopo Staiano and Marco Guerini. 2014.

De-
pechemood: A lexicon for emotion analysis from
In 52th Annual Meeting
crowd-annotated news.
of the Association for Computational Linguistics
(ACL). Association for Computational Linguistics.

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
In 4th Inter-
2007 task 14: Affective text.
national Workshop on Semantic Evaluations (Se-
mEval), pages 70–74. Association for Computa-
tional Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
In 28th Conference on Neural Information
works.
Processing Systems (NIPS), pages 3104–3112.

Jared Suttles and Nancy Ide. 2013. Distant supervi-
sion for emotion classiﬁcation with discrete binary
In International Conference on Intelligent
values.
Text Processing and Computational Linguistics (CI-
CLing), pages 121–136. Springer.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Learning sentiment-
Liu, and Bing Qin. 2014.
speciﬁc word embedding for twitter sentiment clas-
In 52th Annual Meeting of the Associ-
siﬁcation.
ation for Computational Linguistics (ACL), pages
1555–1565.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints, abs/1605.02688.

Mike Thelwall, Kevan Buckley, and Georgios Pal-
Sentiment strength detection for
toglou. 2012.
Journal of the American Society
the social web.
for Information Science and Technology (JASIST),
63(1):163–173.

Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
Journal of the
detection in short informal text.
American Society for Information Science and Tech-
nology, 61(12):2544–2558.

Marilyn A Walker, Jean E Fox Tree, Pranav Anand,
Rob Abbott, and Joseph King. 2012. A corpus for
In Interna-
research on deliberation and debate.
tional Conference on Language Resources and Eval-
uation (LREC), pages 812–817.

Harald G Wallbott and Klaus R Scherer. 1986. How
universal and speciﬁc is emotional experience? evi-
dence from 27 countries on ﬁve continents. Interna-
tional Social Science Council, 25(4):763–795.

Figure 4: Training statistics on the SS-Youtube
dataset with a pretrained model vs. a untrained
model. The architecture and all hyperparameters
are identical for the two models. All layers are
unfrozen.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A
survey on the role of negation in sentiment analysis.

Figure 4 shows an example of how the pretrain-
ing helps to regularize the target task model, which

otherwise quickly overﬁts. The chain-thaw trans-
fer learning approach further increases this reg-
ularization by ﬁne-tuning the model layer wise,
thereby adding additional regularization.

A.3 Emoticon to Emoji mapping

To analyze the effect of using a diverse emoji set
we create a subset of our pretraining data contain-
ing tweets with one of 8 emojis that are similar to
the positive/negative emoticons used by Tang et al.
(2014) and Hu et al. (2013). The positive emoti-
cons are :) : ) :-) :D =) and the negative emoticons
are :( : ( :-(. We ﬁnd the 8 similar emojis in our
dataset seen in Figure 5 as use these for creating
the reduced subset.

Figure 5: Emojis used for the experiment on the
importance of a diverse noisy label set.

A.4 Emoji Clustering

We compute the predictions of the DeepMoji
model on the pretraining test set containing 640K
tweets and compute the correlation matrix of the
predicted probabilities seen in Figure 7. Then we
use hierarchical clustering with average linkage
on the correlation matrix to generate the dendro-
gram seen in Figure 6. We visualized dendrograms
for various versions of our model and the over-
all structure is very stable with only a few emojis
changing places in the hierarchy.

Figure 6: Hierarchical clustering of the DeepMoji model’s predictions across categories on the test set.
The dendrogram shows how the model learns to group emojis into overall categories and subcategories
based on emotional content. The y-axis is the distance on the correlation matrix of the model’s predic-
tions measured using average linkage.

Figure 7: Correlation matrix of the model’s predictions on the pretraining test set.

Using millions of emoji occurrences to learn any-domain representations
for detecting sentiment, emotion and sarcasm

Bjarke Felbo1, Alan Mislove2, Anders Søgaard3, Iyad Rahwan1, Sune Lehmann4

1Media Lab, Massachusetts Institute of Technology
2College of Computer and Information Science, Northeastern University
3Department of Computer Science, University of Copenhagen
4DTU Compute, Technical University of Denmark

ing the distant supervision to a more diverse set of
noisy labels enables the models to learn richer rep-
resentations of emotional content in text, thereby
obtaining better performance on benchmarks for
detecting sentiment, emotions and sarcasm. We
show that the learned representation of a single
pretrained model generalizes across 5 domains.

Table 1: Example sentences scored by our model.
For each text the top ﬁve most likely emojis are
shown with the model’s probability estimates.

Abstract

NLP tasks are often limited by scarcity of
manually annotated data.
In social me-
dia sentiment analysis and related tasks,
researchers have therefore used binarized
emoticons and speciﬁc hashtags as forms
of distant supervision. Our paper shows
that by extending the distant supervision
to a more diverse set of noisy labels, the
models can learn richer representations.
Through emoji prediction on a dataset of
1246 million tweets containing one of 64
common emojis we obtain state-of-the-
art performance on 8 benchmark datasets
within sentiment, emotion and sarcasm de-
tection using a single pretrained model.
Our analyses conﬁrm that the diversity of
our emotional labels yield a performance
improvement over previous distant super-
vision approaches.

1

Introduction

A variety of NLP tasks are limited by scarcity of
manually annotated data. Therefore, co-occurring
emotional expressions have been used for dis-
tant supervision in social media sentiment anal-
ysis and related tasks to make the models learn
useful text representations before modeling these
tasks directly. For instance, the state-of-the-art ap-
proaches within sentiment analysis of social me-
dia data use positive/negative emoticons for train-
ing their models (Deriu et al., 2016; Tang et al.,
2014). Similarly, hashtags such as #anger, #joy,
#happytweet, #ugh, #yuck and #fml have in pre-
vious research been mapped into emotional cate-
gories for emotion analysis (Mohammad, 2012).

Distant supervision on noisy labels often en-
ables a model to obtain better performance on the
In this paper, we show that extend-
target task.

Emojis are not always a direct labeling of emo-
tional content. For instance, a positive emoji may
serve to disambiguate an ambiguous sentence or to
complement an otherwise relatively negative text.
Kunneman et al. (2014) discuss a similar duality
in the use of emotional hashtags such as #nice and
#lame. Nevertheless, our work shows that emo-
jis can be used to classify the emotional content
of texts accurately in many cases. For instance,
our DeepMoji model captures varied usages of the
word ‘love’ as well as slang such as ‘this is the
shit’ being a positive statement (see Table 1). We
provide an online demo at deepmoji.mit.edu to al-
low others to explore the predictions of our model.

Contributions We show how millions of read-
ily available emoji occurrences on Twitter can be
used to pretrain models to learn a richer emotional

7
1
0
2
 
t
c
O
 
7
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
2
5
0
0
.
8
0
7
1
:
v
i
X
r
a

representation than traditionally obtained through
distant supervision. We transfer this knowledge to
the target tasks using a new layer-wise ﬁne-tuning
method, obtaining improvements over the state-
of-the-art within a range of tasks: emotion, sar-
casm and sentiment detection. We present multi-
ple analyses on the effect of pretraining, including
results that show that the diversity of our emoji set
is important for the transfer learning potential of
our model. Our pretrained DeepMoji model is re-
leased with the hope that other researchers can use
it for various NLP tasks1.

2 Related work

Using emotional expressions as noisy labels in
text to counter scarcity of labels is not a new
idea (Read, 2005; Go et al., 2009). Originally, bi-
narized emoticons were used as noisy labels, but
later also hashtags and emojis have been used.
To our knowledge, previous research has always
manually speciﬁed which emotional category each
emotional expression belong to. Prior work has
used theories of emotion such as Ekman’s six
basic emotions and Plutchik’s eight basic emo-
tions (Mohammad, 2012; Suttles and Ide, 2013).

Such manual categorization requires an under-
standing of the emotional content of each expres-
sion, which is difﬁcult and time-consuming for
sophisticated combinations of emotional content.
Moreover, any manual selection and categoriza-
tion is prone to misinterpretations and may omit
important details regarding usage. In contrast, our
approach requires no prior knowledge of the cor-
pus and can capture diverse usage of 64 types of
emojis (see Table 1 for examples and Figure 3 for
how the model implicitly groups emojis).

Another way of automatically interpreting the
emotional content of an emoji is to learn emoji
embeddings from the words describing the emoji-
semantics in ofﬁcial emoji tables (Eisner et al.,
2016). This approach, in our context, suffers from
two severe limitations: a) It requires emojis at test
time while there are many domains with limited
or no usage of emojis. b) The tables do not cap-
ture the dynamics of emoji usage, i.e., drift in an
emoji’s intended meaning over time.

Knowledge can be transferred from the emoji
dataset to the target task in many different ways.
In particular, multitask learning with simultaneous

1Available with preprocessing code, examples of usage,

benchmark datasets etc. at github.com/bfelbo/deepmoji

Figure 1: Illustration of the DeepMoji model with
T being text length and C the number of classes.

training on multiple datasets has shown promis-
ing results (Collobert and Weston, 2008). How-
ever, multitask learning requires access to the
emoji dataset whenever the classiﬁer needs to be
tuned for a new target task. Requiring access
to the dataset is problematic in terms of violat-
ing data access regulations. There are also is-
sues from a data storage perspective as the dataset
used for this research contains hundreds of mil-
lions of tweets (see Table 2). Instead we use trans-
fer learning (Bengio et al., 2012) as described in
§3.3, which does not require access to the original
dataset, but only the pretrained classiﬁer.

3 Method

3.1 Pretraining

In many cases, emojis serve as a proxy for the
emotional contents of a text. Therefore, pretrain-
ing on the classiﬁcation task of predicting which
emoji were initially part of a text can improve per-
formance on the target task (see §5.3 for an anal-
ysis of why our pretraining helps). Social media
contains large amounts of short texts with emojis
that can be utilized as noisy labels for pretraining.
Here, we use data from Twitter from January 1st
2013 to June 1st 2017, but any dataset with emoji
occurrences could be used.

Only English tweets without URL’s are used for
the pretraining dataset. Our hypothesis is that the
content obtained from the URL is likely to be im-
portant for understanding the emotional content of
the text in the tweet. Therefore, we expect emo-
jis associated with these tweets to be noiser labels

than for tweets without URLs, and the tweets with
URLs are thus removed.

Proper tokenization is important for generaliza-
tion. All tweets are tokenized on a word-by-word
basis. Words with 2 or more repeated characters
are shortened to the same token (e.g. ‘loool’ and
‘looooool’ are tokenized such that they are treated
the same). Similarly, we use a special token for all
URLs (only relevant for benchmark datasets), user
mentions (e.g. ‘@acl2017’ and ‘@emnlp2017’ are
thus treated the same) and numbers. To be in-
cluded in the training set the tweet must contain
at least 1 token that is not a punctuation symbol,
emoji or special token2.

Many tweets contain multiple repetitions of the
In the
same emoji or multiple different emojis.
training data, we address this in the following way.
For each unique emoji type, we save a separate
tweet for the pretraining with that emoji type as the
label. We only save a single tweet for the pretrain-
ing per unique emoji type regardless of the number
of emojis associated with the tweet. This data pre-
processing allows the pretraining task to capture
that multiple types of emotional content are asso-
ciated with the tweet while making our pretraining
task a single-label classiﬁcation instead of a more
complicated multi-label classiﬁcation.

To ensure that the pretraining encourages the
models to learn a rich understanding of emotional
content in text rather than only emotional content
associated with the most used emojis, we create
a balanced pretraining dataset. The pretraining
data is split into a training, validation and test set,
where the validation and test set is randomly sam-
pled in such a way that each emoji is equally repre-
sented. The remaining data is upsampled to create
a balanced training dataset.

3.2 Model

With the millions of emoji occurrences available,
we can train very expressive classiﬁers with lim-
ited risk of overﬁtting. We use a variant of the
Long Short-Term Memory (LSTM) model that has
been successful at many NLP tasks (Hochreiter
and Schmidhuber, 1997; Sutskever et al., 2014).
Our DeepMoji model uses an embedding layer of
256 dimensions to project each word into a vector
space. A hyperbolic tangent activation function is
used to enforce a constraint of each embedding di-
mension being within [−1, 1]. To capture the con-

2Details available at github.com/bfelbo/deepmoji

text of each word we use two bidirectional LSTM
layers with 1024 hidden units in each (512 in each
direction). Finally, an attention layer that take all
of these layers as input using skip-connections is
used (see Figure 1 for an illustration).

The attention mechanism lets the model decide
the importance of each word for the prediction task
by weighing them when constructing the represen-
tation of the text. For instance, a word such as
‘amazing’ is likely to be very informative of the
emotional meaning of a text and it should thus be
treated accordingly. We use a simple approach
inspired by (Bahdanau et al., 2014; Yang et al.,
2016) with a single parameter pr. input channel:

et = htwa

at =

exp(et)
i=1 exp(ei)

(cid:80)T

v =

aihi

T
(cid:88)

i=1

Here ht is the representation of the word at time
step t and wa is the weight matrix for the atten-
tion layer. The attention importance scores for
each time step, at, are obtained by multiplying the
representations with the weight matrix and then
normalizing to construct a probability distribution
over the words. Lastly, the representation vector
for the text, v, is found by a weighted summation
over all the time steps using the attention impor-
tance scores as weights. This representation vec-
tor obtained from the attention layer is a high-level
encoding of the entire text, which is used as input
to the ﬁnal Softmax layer for classiﬁcation. We
ﬁnd that adding the attention mechanism and skip-
connections improves the model’s capabilities for
transfer learning (see §5.2 for more details).

The only regularization used for the pretrain-
ing task is a L2 regularization of 1E−6 on the
embedding weights. For the ﬁnetuning additional
regularization is applied (see §4.2). Our model is
implemented using Theano (Theano Development
Team, 2016) and we make an easy-to-use version
available that uses Keras (Chollet et al., 2015).

3.3 Transfer learning

Our pretrained model can be ﬁne-tuned to the tar-
get task in multiple ways with some approaches
‘freezing’ layers by disabling parameters updates
to prevent overﬁtting. One common approach is

to use the network as a feature extractor (Don-
ahue et al., 2014), where all layers in the model are
frozen when ﬁne-tuning on the target task except
the last layer (hereafter referred to as the ‘last’ ap-
proach). Alternatively, another common approach
is to use the pretrained model as an initializa-
tion (Erhan et al., 2010), where the full model is
unfrozen (hereafter referred to as ‘full’).

We propose a new simple transfer learning ap-
proach, ‘chain-thaw’, that sequentially unfreezes
and ﬁne-tunes a single layer at a time. This ap-
proach increases accuracy on the target task at the
expense of extra computational power needed for
the ﬁne-tuning. By training each layer separately
the model is able to adjust the individual patterns
across the network with a reduced risk of overﬁt-
ting. The sequential ﬁne-tuning seems to have a
regularizing effect similar to what has been exam-
ined with layer-wise training in the context of un-
supervised learning (Erhan et al., 2010).

More speciﬁcally, the chain-thaw approach ﬁrst
ﬁne-tunes any new layers (often only a Softmax
layer) to the target task until convergence on a
validation set. Then the approach ﬁne-tunes each
layer individually starting from the ﬁrst layer in
the network. Lastly, the entire model is trained
with all layers. Each time the model converges
as measured on the validation set, the weights
are reloaded to the best setting, thereby prevent-
ing overﬁtting in a similar manner to early stop-
ping (Sj¨oberg and Ljung, 1995). This process is
illustrated in Figure 2. Note how only perform-
ing step a) in the ﬁgure is identical to the ‘last’
approach, where the existing network is used as
a feature extractor. Similarly, only doing step d)
is identical to the ‘full’ approach, where the pre-
trained weights are used as an initialization for a
fully trainable network. Although the chain-thaw
procedure may seem extensive it is easily imple-
mented with only a few lines of code. Similarly,
the additional time spent on ﬁne-tuning is limited
when the target task uses GPUs on small datasets
of manually annotated data as is often the case.

A beneﬁt of the chain-thaw approach is the abil-
ity to expand the vocabulary to new domains with
little risk of overﬁtting. For a given dataset up to
10000 new words from the training set are added
to the vocabulary. §5.3 contains analysis on the
added word coverage gained from this approach.

Figure 2: Illustration of the chain-thaw transfer
learning approach, where each layer is ﬁne-tuned
separately. Layers covered with a blue rectangle
are frozen. Step a) tunes any new layers, b) then
tunes the 1st layer and c) the next layer until all
layers have been ﬁne-tuned individually. Lastly,
in step d) all layers are ﬁne-tuned together.

Table 2: The number of tweets in the pretraining
dataset associated with each emoji in millions.

4 Experiments

4.1 Emoji prediction

We use a raw dataset of 56.6 billion tweets, which
is then ﬁltered to 1.2 billion relevant tweets (see
details in §3.1). In the pretraining dataset a copy
of a single tweet is stored once for each unique
emoji, resulting in a dataset consisting of 1.6 bil-
lion tweets. Table 2 shows the distribution of
tweets across different emoji types. To evaluate
performance on the pretraining task a validation
set and a test set both containing 640K tweets
(10K of each emoji type) are used. The remain-
ing tweets are used for the training set, which is
balanced using upsampling.

The performance of the DeepMoji model is
evaluated on the pretraining task with the results
shown in Table 3. Both top 1 and top 5 accuracy
is used for the evaluation as the emoji labels are
noisy with multiple emojis being potentially cor-
rect for any given sentence. For comparison we
also train a version of our DeepMoji model with
smaller LSTM layers and a bag-of-words classi-
ﬁer, fastText, that has recently shown competitive
results (Joulin et al., 2016). We use 256 dimen-

Table 3: Accuracy of classiﬁers on the emoji
prediction task. d refers to the dimensionality of
each LSTM layer. Parameters are in millions.

Params

Top 1

Top 5

−
Random
12.8
fasttext
15.5
DeepMoji (d = 512)
DeepMoji (d = 1024) 22.4

1.6%
7.8%
12.8% 36.2%
16.7% 43.3%
17.0% 43.8%

sions for this fastText classiﬁer, thereby making it
almost identical to only using the embedding layer
from the DeepMoji model. The difference in top
5 accuracy between the fastText classiﬁer (36.2%)
and the largest DeepMoji model (43.8%) under-
lines the difﬁculty of the emoji prediction task. As
the two classiﬁers only differ in that the DeepMoji
model has LSTM layers and an attention layer be-
tween the embedding and Softmax layer, this dif-
ference in accuracy demonstrates the importance
of capturing the context of each word.

4.2 Benchmarking

We benchmark our method on 3 different NLP
tasks using 8 datasets across 5 domains. To make
for a fair comparison, we compare DeepMoji
to other methods that also utilize external data
sources in addition to the benchmark dataset. An
averaged F1-measure across classes is used for
evaluation in emotion analysis and sarcasm detec-
tion as these consist of unbalanced datasets while
sentiment datasets are evaluated using accuracy.

An issue with many of the benchmark datasets
is data scarcity, which is particularly problem-
atic within emotion analysis. Many recent pa-
pers proposing new methods for emotion analysis
such as (Staiano and Guerini, 2014) only evaluate
performance on a single benchmark dataset, Se-
mEval 2007 Task 14, that contains 1250 observa-
tions. Recently, criticism has been raised concern-
ing the use of correlation with continuous ratings
as a measure (Buechel and Hahn, 2016), making
only the somewhat limited binary evaluation pos-
sible. We only evaluate the emotions {Fear, Joy,
Sadness} as the remaining emotions occur in less
than 5% of the observations.

To fully evaluate our method on emotion analy-
sis against the current methods we thus make use
of two other datasets: A dataset of emotions in
tweets related to the Olympic Games created by
that we convert to a single-label
Sintsova et al.

classiﬁcation task and a dataset of self-reported
emotional experiences created by a large group
of psychologists (Wallbott and Scherer, 1986).
See the supplementary material for details on the
datasets and the preprocessing. As these two
datasets do not have prior evaluations, we eval-
uate against a state-of-the-art approach, which
is based on a valence-arousal-dominance frame-
work (Buechel and Hahn, 2016). The scores ex-
tracted using this approach are mapped to the
classes in the datasets using a logistic regres-
sion with parameter optimization using cross-
validation. We release our preprocessing code and
hope that these 2 two datasets will be used for fu-
ture benchmarking within emotion analysis.

We evaluate sentiment analysis performance on
three benchmark datasets. These small datasets
are chosen to emphasize the importance of the
transfer learning ability of the evaluated models.
Two of the datasets are from SentiStrength (Thel-
wall et al., 2010), SS-Twitter and SS-Youtube,
and follow the relabeling described in (Saif et al.,
2013) to make the labels binary. The third dataset
is from SemEval 2016 Task4A (Nakov et al.,
2016). Due to tweets being deleted from Twitter,
the SemEval dataset suffers from data decay, mak-
ing it difﬁcult to compare results across papers. At
the time of writing, roughly 15% of the training
dataset for SemEval 2016 Task 4A was impossible
to obtain. We choose not to use review datasets for
sentiment benchmarking as these datasets contain
so many words pr. observation that even bag-of-
words classiﬁers and unsupervised approaches can
obtain a high accuracy (Joulin et al., 2016; Rad-
ford et al., 2017).

The current state of the art for sentiment analy-
sis on social media (and winner of SemEval 2016
Task 4A) uses an ensemble of convolutional neu-
ral networks that are pretrained on a private dataset
of tweets with emoticons, making it difﬁcult to
replicate (Deriu et al., 2016). Instead we pretrain
a model with the hyperparameters of the largest
model in their ensemble on the positive/negative
emoticon dataset from Go et al. (2009). Using
this pretraining as an initialization we ﬁnetune
the model on the target tasks using early stop-
ping on a validation set to determine the amount
of training. We also implemented the Sentiment-
Speciﬁc Word Embedding (SSWE) using the em-
beddings available on the authors’ website (Tang
et al., 2014), but found that it performed worse

Table 4: Description of benchmark datasets. Datasets without pre-existing training/test splits are split by
us (with splits publicly available). Data used for hyperparameter tuning is taken from the training set.

Study

Domain

Classes Ntrain Ntest

Identiﬁer

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

(Strapparava and Mihalcea, 2007)
(Sintsova et al., 2013)
(Wallbott and Scherer, 1986)

(Thelwall et al., 2012)
(Thelwall et al., 2012)
(Nakov et al., 2016)

(Walker et al., 2012)
(Oraby et al., 2016)

Task

Emotion
Emotion
Emotion

Headlines
Tweets
Experiences

Sentiment
Sentiment Video Comments
Sentiment

Tweets

Tweets

Sarcasm
Sarcasm

Debate Forums
Debate Forums

250
250
1000

1000
1000
7155

1000
1000

1000
709
6480

1113
1142
31986

995
2260

Table 5: Comparison across benchmark datasets. Reported values are averages across ﬁve runs. Varia-
tions refer to transfer learning approaches in §3.3 with ‘new’ being a model trained without pretraining.

Dataset

Measure

State of the art

DeepMoji
(new)

DeepMoji
(full)

DeepMoji
(last)

DeepMoji
(chain-thaw)

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

F1
F1
F1

Acc
Acc
Acc

F1
F1

.34 [Buechel]
.50 [Buechel]
.45 [Buechel]

.82 [Deriu]
.86 [Deriu]
.51 [Deriu]3

.63 [Joshi]
.72 [Joshi]

.21
.43
.32

.62
.75
.51

.67
.71

.31
.50
.42

.85
.88
.54

.65
.71

.37
.61
.57

.88
.93
.58

.69
.75

3
4
7

2
2
3

2
2

.36
.61
.56

.87
.92
.58

.68
.74

than the pretrained convolutional neural network.
These results are therefore excluded.

For sarcasm detection we use the sarcasm
dataset version 1 and 2 from the Internet Argu-
ment Corpus (Walker et al., 2012). Note that
results presented on these benchmarks in e.g.
Oraby et al. (2016) are not directly comparable
as only a subset of the data is available online.4
A state-of-the-art baseline is found by modeling
the embedding-based features from Joshi et al.
(2016) alongside unigrams, bigrams and trigrams
with an SVM. GoogleNews word2vec embed-
dings (Mikolov et al., 2013) are used for comput-
ing the embedding-based features. A hyperparam-
eter search for regularization parameters is carried
out using cross-validation. Note that the sarcasm
dataset version 2 contains both a quoted text and a
sarcastic response, but to keep the models identi-
cal across the datasets only the response is used.

For

training we

the Adam opti-
mizer (Kingma and Ba, 2015) with gradient

use

3The authors report a higher accuracy in their paper,
which is likely due to having a larger training dataset as they
were able to obtain it before data decay occurred.

4We contacted the authors, but were unable to obtain the

full dataset for neither version 1 or version 2.

clipping of the norm to 1. Learning rate is set to
1E−3 for training of all new layers and 1E−4
for ﬁnetuning any pretrained layers. To prevent
overﬁtting on the small datasets, 10% of the
channels across all words in the embedding layer
are dropped out during training. Unlike e.g. (Gal
and Ghahramani, 2016) we do not drop out entire
words in the input as some of our datasets contain
it could
observations with so few words that
change the meaning of the text.
In addition to
the embedding dropout, L2 regularization for the
embedding weights is used and 50% dropout is
applied to the penultimate layer.

Table 5 shows that the DeepMoji model out-
performs the state of the art across all benchmark
datasets and that our new ‘chain-thaw’ approach
consistently yields the highest performance for the
transfer learning, albeit often only slightly better
or equal to the ‘last’ approach. Results are aver-
aged across 5 runs to reduce the variance. We test
the statistical signiﬁcance of our results by com-
paring the performance of DeepMoji (chain-thaw)
the state of the art. Bootstrap testing with
vs.
10000 samples is used. Our results are statisti-
cally signiﬁcantly better than the state of the art

with p < 0.001 on every benchmark dataset.

useful for transfer learning.

Our model is able to out-perform the state-of-
the-art on datasets that originate from domains that
differ substantially from the tweets on which it
was pretrained. A key difference between the pre-
training dataset and the benchmark datasets is the
length of the observations. The average number of
tokens pr.
tweet in the pretraining dataset is 11,
whereas e.g. the board posts from the Internet Ar-
gument Corpus version 1 (Oraby et al., 2016) has
an average of 66 tokens with some observations
being much longer.

5 Model Analysis

5.1

Importance of emoji diversity

One of the major differences between this work
compared to previous papers using distant super-
vision is the diversity of the noisy labels used (see
§2). For instance, both Deriu et al. (2016) and
Tang et al. (2014) only used positive and negative
emoticons as noisy labels. Other instances of pre-
vious work have used slightly more nuanced sets
of noisy labels (see §2), but to our knowledge our
set of noisy labels is the most diverse yet. To an-
alyze the effect of using a diverse emoji set we
create a subset of our pretraining data containing
tweets with one of 8 emojis that are similar to
the positive/negative emoticons used by Tang et al.
(2014) and Hu et al. (2013) (the set of emoticons
and corresponding emojis are available in the sup-
plemental material). As the dataset based on this
reduced set of emojis contains 433M tweets, any
difference in performance on benchmark datasets
is likely linked to the diversity of labels rather than
differences in dataset sizes.

We train our DeepMoji model

to predict
whether the tweets contain a positive or negative
emoji and evaluate this pretrained model across
the benchmark datasets. We refer to the model
trained on the subset of emojis as DeepMoji-
PosNeg (as opposed to DeepMoji). To test the
emotional representations learned by the two pre-
trained models the ‘last’ transfer learning ap-
proach is used for the comparison, thereby only
allowing the models to map already learned fea-
tures to classes in the target dataset. Table 6 shows
that DeepMoji-PosNeg yields lower performance
compared to DeepMoji across all 8 benchmarks,
thereby showing that the diversity of our emoji
types encourage the model to learn a richer repre-
sentation of emotional content in text that is more

Table 6: Benchmarks using a smaller emoji set
(Pos/Neg emojis) or a classic architecture (stan-
dard LSTM). Results for DeepMoji from Table 5
are added for convenience. Evaluation metrics are
as in Table 5. Reported values are the averages
across ﬁve runs.

Pos/Neg
emojis

Standard
LSTM

DeepMoji

Dataset

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

.32
.55
.40

.86
.90
.56

.66
.72

.35
.57
.49

.86
.91
.57

.66
.73

.36
.61
.56

.87
.92
.58

.68
.74

Many of the emojis carry similar emotional
content, but have subtle differences in usage that
our model is able to capture. Through hierar-
chical clustering on the correlation matrix of the
DeepMoji model’s predictions on the test set we
can see that the model captures many similarities
that one would intuitively expect (see Figure 3).
For instance, the model groups emojis into overall
categories associated with e.g. negativity, positiv-
ity or love. Similarly, the model learns to differen-
tiate within these categories, mapping sad emojis
in one subcategory of negativity, annoyed in an-
other subcategory and angry in a third one.

5.2 Model architecture

Our DeepMoji model architecture as described
in §3.2 use an attention mechanism and skip-
connections to ease the transfer of the learned rep-
resentation to new domains and tasks. Here we
compare the DeepMoji model architecture to that
of a standard 2-layer LSTM, both compared using
the ‘last’ transfer learning approach. We use the
same regularization and training parameters.

As seen in Table 6 the DeepMoji model per-
forms better than a standard 2-layer LSTM across
all benchmark datasets. The two architectures per-
formed equally on the pretraining task, suggesting
that while the DeepMoji model architecture is in-
deed better for transfer learning, it may not neces-
sarily be better for single supervised classiﬁcation
task with ample available data.

A reasonable conjecture is that the improved
transfer learning performance is due to two fac-

Figure 3: Hierarchical clustering of the DeepMoji model’s predictions across categories on the test set.
The dendrogram shows how the model learns to group emojis into overall categories and subcategories
based on emotional content. The y-axis is the distance on the correlation matrix of the model’s predic-
tions measured using average linkage. More details are available in the supplementary material.

tors:
a) the attention mechanism with skip-
connections provide easy access to learned low-
level features for any time step, making it easy to
use this information if needed for a new task b)
the improved gradient-ﬂow from the output layer
to the early layers in the network due to skip-
connections (Graves, 2013) is important when ad-
justing parameters in early layers as part of trans-
fer learning to small datasets. Detailed analysis of
whether these factors actually explain why our ar-
chitecture outperform a standard 2-layer LSTM is
left for future work.

5.3 Analyzing the effect of pretraining

Performance on the target task beneﬁts strongly
from pretraining as shown in Table 5 by compar-
ing DeepMoji (new) to DeepMoji (chain-thaw).
In this section we experimentally decompose the
beneﬁt of pretraining into 2 effects: word coverage
and phrase coverage. These two effects help regu-
larize the model by preventing overﬁtting (see the
supplementary details for an visualization of the
effect of this regularization).

There are numerous ways to express a speciﬁc
sentiment, emotion or sarcastic comment. Conse-
quently, the test set may contain speciﬁc language
use not present in the training set. The pretraining
helps the target task models attend to low-support
evidence by having previously observed similar
usage in the pretraining dataset. We ﬁrst exam-
ine this effect by measuring the improvement in
word coverage on the test set when using the pre-
training with word coverage being deﬁned as the
% of words in the test dataset seen in the train-
ing/pretraining dataset (see Table 7). An impor-
tant reason why the ‘chain-thaw’ approach outper-
forms other transfer learning approaches is can be

used to tune the embedding layer with limited risk
of overﬁtting. Table 7 shows the increased word
coverage from adding new words to the vocabu-
lary as part of that tuning.

Note that word coverage can be a misleading
metric in this context as for many of these small
datasets a word will often occur only once in the
training set.
In contrast, all of the words in the
pretraining vocabulary are present in thousands (if
not millions) of observations in the emoji pretrain-
ing dataset thus making it possible for the model
to learn a good representation of the emotional
and semantic meaning. The added beneﬁt of pre-
training for learning word representations there-
fore likely extends beyond the differences seen in
Table 7.

Table 7: Word coverage on benchmark test sets
using only the vocabulary generated by ﬁnding
words in the training data (‘own’), the pretrain-
ing vocabulary (‘last’) or a combination of both
vocabularies (‘full / chain-thaw’).

Dataset

SE0714
Olympic
PsychExp

SS-Twitter
SS-Youtube
SE1604

SCv1
SCv2-GEN

Own

41.9%
73.9%
85.4%

80.1%
79.6%
86.1%

88.7%
86.5%

Last

93.6%
90.3%
98.5%

97.1%
97.2%
96.6%

97.3%
97.2%

Full /
Chain-thaw

94.0%
96.0%
98.8%

97.2%
97.3%
97.0%

98.0%
98.0%

To examine the importance of capturing phrases
and the context of each word, we evaluate the ac-
curacy on the SS-Youtube dataset using a fastText
classiﬁer pretrained on the same emoji dataset as

our DeepMoji model. This fastText classiﬁer is al-
most identical to only using the embedding layer
from the DeepMoji model. We evaluate the rep-
resentations learned by ﬁne-tuning the models as
feature extractors (i.e. using the ‘last’ transfer
learning approach). The fastText model achieves
an accuracy of 63% as compared to 93% for our
DeepMoji model,
thereby emphasizing the im-
portance of phrase coverage. One concept that
the LSTM layers likely learn is negation, which
is known to be important for sentiment analy-
sis (Wiegand et al., 2010).

single MTurk rater.

Table 8: Comparison of agreement between clas-
siﬁers and the aggregate opinion of Amazon
Mechanical Turkers on sentiment prediction of
tweets.

Random
fastText
MTurk
DeepMoji

Agreement

50.1%
71.0%
76.1%
82.4%

5.4 Comparing with human-level agreement

6 Conclusion

To understand how well our DeepMoji classi-
ﬁer performs compared to humans, we created a
new dataset of random tweets annotated for senti-
ment. Each tweet was annotated by a minimum of
10 English-speaking Amazon Mechanical Turkers
(MTurk’s) living in USA. Tweets were rated on a
scale from 1 to 9 with a ‘Do not know’ option, and
guidelines regarding how to rate the tweets were
provided to the human raters. The tweets were
selected to contain only English text, no men-
tions and no URL’s to make it possible to rate
them without any additional contextual informa-
tion. Tweets where more than half of the eval-
uators chose ‘Do not know’ were removed (98
tweets).

For each tweet, we select a MTurk rating ran-
dom to be the ‘human evaluation’, and average
over the remaining nine MTurk ratings are av-
eraged to form the ground truth. The ‘senti-
ment label’ for a given tweet is thus deﬁned as
the overall consensus among raters (excluding the
randomly-selected ‘human evaluation’ rating). To
ensure that the label categories are clearly sep-
arated, we removed neutral tweets in the inter-
val [4.5, 5.5] (roughly 29% of the tweets). The
remaining dataset consists of 7 347 tweets. Of
these tweets, 5000 are used for training/validation
and the remaining are used as the test set. Our
DeepMoji model is trained using the chain-thaw
transfer learning approach.

Table 8 shows that the agreement of the random
MTurk rater is 76.1%, meaning that the randomly
selected rater will agree with the average of the
nine other MTurk-ratings of the tweet’s polarity
76.1% of the time. Our DeepMoji model achieves
82.4% agreement, which means it is better at cap-
turing the average human sentiment-rating than a

We have shown how the millions of texts on so-
cial media with emojis can be used for pretrain-
ing models, thereby allowing them to learn repre-
sentations of emotional content in texts. Through
comparison with an identical model pretrained on
a subset of emojis, we ﬁnd that the diversity of
our emoji set is important for the performance of
our method. We release our pretrained DeepMoji
model with the hope that other researchers will
ﬁnd good use of them for various emotion-related
NLP tasks5.

Acknowledgments

The authors would like to thank Janys Analytics
for generously allowing us to use their dataset of
human-rated tweets and the associated code to an-
alyze it. Furthermore, we would like to thank Max
Lever, who helped design the online demo, and
Han Thi Nguyen, who helped code the software
that is provided alongside the pretrained model.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In 3rd International
Conference on Learning Representations (ICLR).

Yoshua Bengio et al. 2012. Deep learning of repre-
sentations for unsupervised and transfer learning. In
29th International Conference on Machine learning
(ICML) – Workshop on Unsupervised and Transfer
Learning, volume 27, pages 17–36.

Sven Buechel and Udo Hahn. 2016. Emotion analy-
sis as a regression problem - dimensional models
and their implications on emotion representation and

5Available with preprocessing code, examples of usage,

benchmark datasets etc. at github.com/bfelbo/deepmoji

metrical evaluation. In 22nd European Conference
on Artiﬁcial Intelligence (ECAI).

Franc¸ois Chollet et al. 2015. Keras. https://

github.com/fchollet/keras.

Ronan Collobert and Jason Weston. 2008. A uni-
ﬁed architecture for natural language processing:
Deep neural networks with multitask learning.
In
25th International Conference on Machine learning
(ICML), pages 160–167.

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
relien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. Swisscheese at semeval-2016 task 4: Sen-
timent classiﬁcation using an ensemble of convo-
lutional neural networks with distant supervision.
Proceedings of SemEval, pages 1124–1128.

Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. Decaf: A deep convolutional activation fea-
In 31th Inter-
ture for generic visual recognition.
national Conference on Machine Learning (ICML),
volume 32, pages 647–655.

Ben Eisner, Tim Rockt¨aschel, Isabelle Augenstein,
Matko Boˇsnjak,
and Sebastian Riedel. 2016.
emoji2vec: Learning emoji representations from
their description. In 4th International Workshop on
Natural Language Processing for Social Media (So-
cialNLP).

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research (JMLR), 11:625–660.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In 30th Conference on Neural In-
formation Processing Systems (NIPS), pages 1019–
1027.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classiﬁcation using distant supervision.
CS224N Project Report, Stanford, 1(12).

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850.

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the 22nd interna-
tional conference on World Wide Web (WWW), pages
607–618. ACM.

Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak
Bhattacharyya, and Mark Carman. 2016. Are word

embedding-based features useful for sarcasm detec-
tion? In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efﬁcient text
classiﬁcation. arXiv preprint arXiv:1607.01759.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations
(ICLR).

FA Kunneman, CC Liebrecht, and APJ van den Bosch.
2014. The (un)predictability of emotional hashtags
in twitter. In 52th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL). Associa-
tion for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In 27th Conference on Neural Information Pro-
cessing Systems (NIPS), pages 3111–3119.

Saif Mohammad. 2012. #emotional tweets.

In The
First Joint Conference on Lexical and Computa-
tional Semantics (*SEM), pages 246–255. Associa-
tion for Computational Linguistics.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. Semeval-
2016 task 4: Sentiment analysis in twitter.
In
10th International Workshop on Semantic Evalua-
tion (SemEval), pages 1–18.

Shereen Oraby, Vrindavan Harrison, Lena Reed,
Ernesto Hernandez, Ellen Riloff, and Marilyn
Walker. 2016. Creating and characterizing a diverse
corpus of sarcasm in dialogue. In 17th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue (SIGDIAL), page 31.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classiﬁcation. In ACL student research work-
shop, pages 43–48. Association for Computational
Linguistics.

Hassan Saif, Miriam Fernandez, Yulan He, and Harith
Alani. 2013. Evaluation datasets for twitter senti-
ment analysis: a survey and a new dataset, the sts-
gold. In Workshop: Emotion and Sentiment in So-
cial and Expressive Media: approaches and per-
spectives from AI (ESSEM) at AI*IA Conference.

Valentina Sintsova, Claudiu-Cristian Musat, and Pearl
Fine-grained emotion recognition in
Pu. 2013.
In
olympic tweets based on human computation.
4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis
(WASSA).

In Workshop on Negation and Speculation in Natu-
ral Language Processing (NeSp-NLP), pages 60–68.
Association for Computational Linguistics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J Smola, and Eduard H Hovy. 2016. Hi-
erarchical attention networks for document classiﬁ-
cation. In HLT-NAACL.

A Supplemental Material

A.1 Preprocessing Emotion Datasets

In the Olympic Games dataset by Sintsova et al.
each tweet can be assigned multiple emotions out
of 20 possible emotions, making evaluation difﬁ-
cult. To counter this difﬁculty, we have chosen to
convert the labels to 4 classes of low/high valence
and low/high arousal based on the Geneva Emo-
tion Wheel that the study used. A tweet is deemed
as having emotions within the valence/arousal
class if the average evaluation by raters for that
class is 2.0 or higher, where ‘Low’ = 1, ‘Medium’
= 2 and ‘High’ = 3.

We also evaluate on the ISEAR databank (Wall-
bott and Scherer, 1986), which was created over
many years by a large group of psychologists that
interviewed respondents in 37 countries. Each ob-
servation in the dataset is a self-reported experi-
ence mapped to 1 of 7 possible emotions, making
for an interesting benchmark dataset.

A.2 Pretraining as Regularization

Jonas Sj¨oberg and Lennart Ljung. 1995. Overtraining,
regularization and searching for a minimum, with
application to neural networks. International Jour-
nal of Control, 62(6):1391–1407.

Jacopo Staiano and Marco Guerini. 2014.

De-
pechemood: A lexicon for emotion analysis from
In 52th Annual Meeting
crowd-annotated news.
of the Association for Computational Linguistics
(ACL). Association for Computational Linguistics.

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
In 4th Inter-
2007 task 14: Affective text.
national Workshop on Semantic Evaluations (Se-
mEval), pages 70–74. Association for Computa-
tional Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
In 28th Conference on Neural Information
works.
Processing Systems (NIPS), pages 3104–3112.

Jared Suttles and Nancy Ide. 2013. Distant supervi-
sion for emotion classiﬁcation with discrete binary
In International Conference on Intelligent
values.
Text Processing and Computational Linguistics (CI-
CLing), pages 121–136. Springer.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Learning sentiment-
Liu, and Bing Qin. 2014.
speciﬁc word embedding for twitter sentiment clas-
In 52th Annual Meeting of the Associ-
siﬁcation.
ation for Computational Linguistics (ACL), pages
1555–1565.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints, abs/1605.02688.

Mike Thelwall, Kevan Buckley, and Georgios Pal-
Sentiment strength detection for
toglou. 2012.
Journal of the American Society
the social web.
for Information Science and Technology (JASIST),
63(1):163–173.

Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
Journal of the
detection in short informal text.
American Society for Information Science and Tech-
nology, 61(12):2544–2558.

Marilyn A Walker, Jean E Fox Tree, Pranav Anand,
Rob Abbott, and Joseph King. 2012. A corpus for
In Interna-
research on deliberation and debate.
tional Conference on Language Resources and Eval-
uation (LREC), pages 812–817.

Harald G Wallbott and Klaus R Scherer. 1986. How
universal and speciﬁc is emotional experience? evi-
dence from 27 countries on ﬁve continents. Interna-
tional Social Science Council, 25(4):763–795.

Figure 4: Training statistics on the SS-Youtube
dataset with a pretrained model vs. a untrained
model. The architecture and all hyperparameters
are identical for the two models. All layers are
unfrozen.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A
survey on the role of negation in sentiment analysis.

Figure 4 shows an example of how the pretrain-
ing helps to regularize the target task model, which

otherwise quickly overﬁts. The chain-thaw trans-
fer learning approach further increases this reg-
ularization by ﬁne-tuning the model layer wise,
thereby adding additional regularization.

A.3 Emoticon to Emoji mapping

To analyze the effect of using a diverse emoji set
we create a subset of our pretraining data contain-
ing tweets with one of 8 emojis that are similar to
the positive/negative emoticons used by Tang et al.
(2014) and Hu et al. (2013). The positive emoti-
cons are :) : ) :-) :D =) and the negative emoticons
are :( : ( :-(. We ﬁnd the 8 similar emojis in our
dataset seen in Figure 5 as use these for creating
the reduced subset.

Figure 5: Emojis used for the experiment on the
importance of a diverse noisy label set.

A.4 Emoji Clustering

We compute the predictions of the DeepMoji
model on the pretraining test set containing 640K
tweets and compute the correlation matrix of the
predicted probabilities seen in Figure 7. Then we
use hierarchical clustering with average linkage
on the correlation matrix to generate the dendro-
gram seen in Figure 6. We visualized dendrograms
for various versions of our model and the over-
all structure is very stable with only a few emojis
changing places in the hierarchy.

Figure 6: Hierarchical clustering of the DeepMoji model’s predictions across categories on the test set.
The dendrogram shows how the model learns to group emojis into overall categories and subcategories
based on emotional content. The y-axis is the distance on the correlation matrix of the model’s predic-
tions measured using average linkage.

Figure 7: Correlation matrix of the model’s predictions on the pretraining test set.

