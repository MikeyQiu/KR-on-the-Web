Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

Clarice Poon * 1 Jingwei Liang * 1 Carola-Bibiane Sch¨onlieb 1

Abstract
In this paper, we present a local convergence anal-
ysis for a class of stochastic optimisation meth-
ods:
the proximal variance reduced stochastic
gradient methods, and mainly focus on SAGA
(Defazio et al., 2014) and Prox-SVRG (Xiao &
Zhang, 2014). Under the assumption that the
non-smooth component of the optimisation prob-
lem is partly smooth relative to a smooth mani-
fold, we present a uniﬁed framework for the local
convergence analysis of SAGA/Prox-SVRG: (i)
the sequences generated by the methods are able
to identify the smooth manifold in a ﬁnite num-
ber of iterations; (ii) then the sequence enters a
local linear convergence regime. Furthermore,
we discuss various possibilities for accelerating
these algorithms, including adapting to better lo-
cal parameters, and applying higher-order deter-
ministic/stochastic optimisation methods which
can achieve super-linear convergence. Several
concrete examples arising from machine learning
are considered to demonstrate the obtained result.

1. Introduction

1.1. Non-smooth Optimisation

Modern optimisation has become a core part of many ﬁelds,
such as machine learning and signal/image processing. In a
world of increasing data demands, there are two key driving
forces behind modern optimisation.

• Non-smooth regularisation. We are often faced with
models of high complexity, however, the solutions of
interest often lie on a manifold of low dimension which
is promoted by the non-smooth regularisers. There have
been several recent studies explaining how proximal
methods identify this low dimensional manifold and efﬁ-

*Equal contribution

1DAMTP, University of Cambridge,
Cambridge, United Kingdom. Correspondence to: Clarice
Poon <C.M.H.S.Poon@maths.cam.ac.uk>,
Jingwei Liang
<jl993@cam.ac.uk>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

ciently output solutions which take a certain structure;
see for instance (Liang et al., 2017) for the case of deter-
ministic proximal gradient methods.

• Stochastic methods. The past decades have seen an
exponential growth in the data sizes that we have to
handle, and stochastic methods have been popular due to
their low computational cost; see for instance (Schmidt
et al., 2017; Defazio et al., 2014; Xiao & Zhang, 2014)
and references therein.

The purpose of this paper is to show that proximal variance
reduced stochastic gradient methods allow to beneﬁt from
both efﬁcient structure enforcement and low computational
cost. In particular, we present a study of manifold identi-
ﬁcation and local acceleration properties of these methods
when applied to the following minimisation problem:

min
x∈Rn

Φ(x) def= R(x) + F (x),

(P)

where R(x) is a non-smooth structure imposing penalty
term, and

F (x) def= 1
m
where each fi is continuously differentiable.

i=1 fi(x),

(cid:80)m

We are interested in the problems where the value of m is
very large. A typical example of (P) is the LASSO problem,
which reads

min
x∈Rn

µ||x||1 +

1
m

(cid:80)m
i=1

1
2

||Kix − bi||2,

where µ > 0 is a positive trade-off parameter, Ki is the ith
row of a matrix K ∈ Rm×n, and bi is the ith element of
the vector b ∈ Rm. Throughout this paper, we consider the
following basic assumptions for problem (P):

(A.1) R : Rn → R ∪ {+∞} is proper, convex and lower

semi-continuous;

(A.2) F : Rn → R is continuously differentiable with
∇F being LF -Lipschitz continuous. For each in-
dex i = 1, · · · , m, fi is continuously differentiable
with Li-Lipschitz continuous gradient;

(A.3) Argmin(Φ) (cid:54)= ∅, that is the set of minimisers is

non-empty.

In addition to (A.2), deﬁne L def= maxi={1,··· ,m} Li, which
is the uniform Lipschitz continuity of functions fi. Note
that LF ≤ 1
m

i Li ≤ L holds.

(cid:80)

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

1.2. Deterministic Forward–Backward Splitting

A classical approach to solve (P) is the Forward–Backward
splitting (FBS) method (Lions & Mercier, 1979), which
is also known as proximal gradient descent method. The
standard FBS iteration reads

xk+1 = proxγkR

(cid:0)xk − γk∇F (xk)(cid:1), γk ∈ ]0, 2
LF

[,

(1)

where proxγR is the proximity operator of R deﬁned by

proxγR(·) def= min
x∈Rn

γR(x) + 1
2

||x − ·||2.

(2)

In general, the advantages of FBS can be summarised as:

√

• Known convergence rates.

• Robust convergence guarantees. Both sequence and
objective function are convergent for 0 < γ ≤ γk ≤
γ < 2/LF where γ, γ > 0 (Combettes & Wajs, 2005);
There holds ||xk −
xk−1|| = o(1/
k) (Liang et al., 2016), and Φ(xk) −
Φ(x(cid:63)) = o(1/k) (Molinari et al., 2018) where x(cid:63) ∈
Argmin(Φ). These rates can be improved to linear1 if
for instance Φ is strongly convex (Nesterov, 2004);
• Numerous acceleration techniques. Such as the iner-
tial schemes including inertial FBS (Liang et al., 2017),
FISTA (Beck & Teboulle, 2009) and Nesterov’s optimal
methods (Nesterov, 2004);

• Structure adaptivity. Several recent work (Liang et al.,
2014; 2017) studies the manifold identiﬁcation prop-
erties of FBS. It is shown in (Liang et al., 2017) that
within ﬁnite time, the FBS iterates xk all lie on the same
manifold as the optimal solution x(cid:63). Furthermore, upon
identifying this manifold, the FBS iterates can be proved
to converge linearly to the optimal solution x(cid:63).

However, despite these advantages, for problem (P), when
m is very large, computing ∇F (xk) could be very expen-
sive, which makes the deterministic FBS-type methods un-
suitable for solving large-scale problems.

1.3. Proximal Stochastic Gradient Descent

The most straightforward extension of stochastic gradient
descent to problem (P) is the proximal stochastic gradient
descent (Prox-SGD), which reads, for k = 0, 1, 2, 3, · · ·

(cid:36)

sample ik uniformly from {1, · · · , m}
(cid:0)xk − γk∇fik (xk)(cid:1).
xk+1 = proxγkR

(3)

Different from FBS, Prox-SGD only evaluates the gradient
of one sampled function fik at each step. However, to
ensure the convergence, the step-size γk of Prox-SGD has
to converge to 0 at a proper speed (e.g. γk = ks for s ∈
]1/2, 1]), leading to only O(1/
k) convergence rate for
Φ(xk) − Φ(x(cid:63)). When Φ is strongly convex, the rate for the
objective can only be improved to O(1/k).

√

1Linear convergence is also known as geometric or exponential

convergence.

Perturbed Forward–Backward Splitting An alterna-
tive perspective of treating Prox-SGD is as a perturbation of
the deterministic FBS scheme. More precisely, iteration (3)
can be written as the inexact FBS iteration with stochastic
approximation error for the gradient, for k = 0, 1, 2, 3, · · ·

(cid:36)

sample εk from a ﬁnite distribution Dk,
xk+1 = proxγkR

(cid:0)xk − γk(∇F (xk) + εk)(cid:1).

(4)

For most existing stochastic gradient methods, E[εk] = 0
and ||εk||2 is the variance of the stochastic gradient.

For Prox-SGD, the error εk takes the form

εSGD
k

def= ∇fik (xk) − ∇F (xk).

(5)

k

||2 is only bounded, and does
In general, the variance ||εSGD
not vanish to 0 as xk → x(cid:63). One consequence of this non-
vanishing variance is that, unlike FBS, in general manifold
identiﬁcation cannot occur. In Section 1 of the supplemen-
tary material, we give an example to illustrate this point.

1.4. Variance Reduced Stochastic Gradient Methods

To overcome the vanishing step-size and slow convergence
speed of Prox-SGD caused by non-vanishing variance, sev-
eral variance reduced schemes are proposed schemes (De-
fazio et al., 2014; Xiao & Zhang, 2014). The features of
variance reduced techniques can be summarised as:

• Same as Prox-SGD, in expectation, the stochastic gradi-
ent remains an unbiased estimation of the full gradient;
• Different from Prox-SGD, the variance ||εk||2 converges

to 0 when xk approaches the solution x(cid:63).

SAGA Algorithm (Defazio et al., 2014) The key idea of
SAGA for reducing the variance is utilising the gradient
history for the evaluation of the current gradient.

Given an initial point x0, deﬁne the individual gradient
def= ∇fi(x0), i = 1, · · · , m. Then, for k = 0, 1, 2, 3, · · ·
g0,i













sample ik uniformly from {1, · · · , m},
wk = xk − γk(∇fik (xk) − gk,ik + 1
m
xk+1 = proxγkR(wk),

(cid:40)

gk,i =

∇fi(xk)
gk−1,i

if i = ik,
o.w.

(cid:80)m

i=1 gk,i),

(6)

In the context of (4), the stochastic approximation error εk
of SAGA takes the form

εSAGA
k

def= ∇fik (xk)−gk,ik + 1
m

(cid:80)m

i=1 gk,i −∇F (xk). (7)

Prox-SVRG Algorithm (Xiao & Zhang, 2014) Com-
pared to SAGA, in stead of using the gradient history, Prox-

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

SVRG computes the full gradient of a given point, and uses
it for a certain number of iterations.

Let P be a positive integer, for (cid:96) = 0, 1, 2, · · ·

i=1 ∇fi(˜x(cid:96)), x(cid:96),0 = ˜x(cid:96),

(cid:80)m

˜g(cid:96) = 1
m
For p = 1, · · · , P






















sample ip uniformly from {1, · · · , m},
wk = x(cid:96),p−1 − γk(∇fip (x(cid:96),p−1) − ∇fip (˜x(cid:96)) + ˜g(cid:96)),
x(cid:96),p = proxγkR(wk).
Option I : ˜x(cid:96)+1 = x(cid:96),P ,
Option II : ˜x(cid:96)+1 = 1
P

p=1 x(cid:96),p.

(cid:80)P

Local Accelerations Another important implication of
manifold identiﬁcation is that the global non-smooth op-
timisation problem becomes locally C 2-smooth along the
manifold Mx(cid:63) , and moreover is locally strongly convex
if the restricted injectivity condition (RI) is satisﬁed. This
implies that locally we have many choices of acceleration to
choose, for instance we can turn to higher-order optimisation
methods, such as (quasi)-Newton methods or Riemannian
manifold based optimisation methods which can lead to
super linear convergence.

Lastly, for the numerical experiments considered in this pa-
per, the corresponding MATLAB source code to reproduce
the results is available online2.

(8)

1.6. Notations

In the context of (4), given x(cid:96),p, denote k = (cid:96)P + p, then
we have x(cid:96),p = xk and the stochastic approximation error
εk of Prox-SVRG reads

εSVRG
k

def= ∇fip (xk) − ∇fip (˜x(cid:96)) + ˜g(cid:96) − ∇F (xk).

(9)

1.5. Contributions

In this paper, we present a very general framework for
analysing the local convergence properties of variance re-
duced stochastic gradient. More precisely, this paper con-
sists of the following contributions.

Convergence of Sequence for SAGA/Prox-SVRG As-
suming only convexity, we prove the almost sure global
convergence of the sequences generated by SAGA (Theo-
rem 2.1) and Prox-SVRG with “Option I” (Theorem 2.2),
which is new to the literature. Moreover, for Prox-SVRG
with “Option I”, an O(1/k) ergodic convergence rate for
the objective function is proved.

Finite Time Manifold Identiﬁcation Let x(cid:63) be a global
minimiser of problem (P), and suppose that the sequence
{xk}k∈N generated by the perturbed FBS iteration (4) con-
verges to x(cid:63) almost surely. Then under the additional as-
sumptions that the non-smooth function R is partly smooth
at x(cid:63) relative to a C 2-smooth manifold Mx(cid:63) (Deﬁnition 3.1)
and a non-degeneracy condition (Eq. (ND)) holds at x(cid:63), in
Theorem 3.2 we prove a general ﬁnite time manifold iden-
tiﬁcation result for the perturbed FBS scheme (4). The
manifold identiﬁcation means that after a ﬁnite number of
iterations, say K, there holds xk ∈ Mx(cid:63) for all k ≥ K.
Specialising the result to SAGA and Prox-SVRG algorithms,
we prove the ﬁnite manifold identiﬁcation of these two al-
gorithms (Corollary 3.4).

Local Linear Convergence Building upon the manifold
identiﬁcation result, if moreover F is locally C 2-smooth
along Mx(cid:63) near x(cid:63) and a restricted injectivity condition
(Eq. (RI)) is satisﬁed by the Hessian ∇2F (x(cid:63)), we show
that locally SAGA and Prox-SVRG converge linearly.

Throughout the paper, N denotes the set of non-negative
integers and k ∈ N denotes the index. Rn is the Euclidean
space of dimension n. For a non-empty convex set Ω ⊂ Rn,
ri(Ω) and rbd(Ω) denote its relative interior and relative
boundary respectively.

Let R be proper, convex and lower semi-continuous, the
sub-differential is deﬁned by ∂R(x) def= (cid:8)g ∈ Rn|R(y) ≥
R(x) + (cid:104)g, y − x(cid:105), ∀y ∈ Rn(cid:9). A function R is α-strongly
convex for some α > 0 if R(x) − α

2 ||x||2 is convex.

2. Global Convergence of SAGA/Prox-SVRG

In this section, we prove the convergence of the sequence
generated by the SAGA and Prox-SVRG with “Option I”
without strong convexity, while in their respective original
work (Defazio et al., 2014; Xiao & Zhang, 2014), only the
convergence of the objective function value is provided.

We present ﬁrst the convergence result of the SAGA algo-
rithm, recall that L is the uniform Lipschitz continuity of all
element functions fi, i = 1, · · · , m.

Theorem 2.1 (Convergence of SAGA). For problem (P),
suppose that conditions (A.1)-(A.3) hold. Let {xk}k∈N be
the sequence generated by the SAGA algorithm (6) with
γk ≡ 1
3L , then there exists an x(cid:63) ∈ Argmin(Φ) such that
almost surely Φ(xk) → Φ(x(cid:63)), xk → x(cid:63) and εSAGA
k → 0.

Next we provide the convergence result of the Prox-
SVRG algorithm with “Option I”. Given (cid:96) ∈ N and
p ∈ {1, · · · , P }, let k = (cid:96)P + p, and denote x(cid:96),p = xk. For
(cid:96)=1 x(cid:96).
sequence {xk}k∈N, deﬁne a new point by ¯xk
Theorem 2.2 (Convergence of Prox-SVRG). For problem
(P), suppose that conditions (A.1)-(A.3) hold. Let {xk}k∈N
be the sequence generated by the Prox-SVRG algorithm (8)
with “Option I”. Then,

def= 1
k

(cid:80)k

2https://github.com/jliang993/Local-VRSGD

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

(i) If we ﬁx γk ≡ γ with γ ≤

4L(P +2) , then there exists
a minimiser x(cid:63) ∈ Argmin(Φ) such that xk → x(cid:63) and
k → 0 almost surely. Moreover, there holds for
εSVRG
each k = (cid:96)P , E[Φ(¯xk) − Φ(x(cid:63))] = O(1/k);

1

(ii) Suppose that R, F are moreover αR and αF strongly
convex respectively, then if 4Lγ(P + 1) < 1, there
holds E(cid:2)||˜x(cid:96) − x(cid:63)||2(cid:3) ≤ O(ρ(cid:96)
), where ρSVRG =
max{ 1−γαF
1+γαR

, 4Lγ(P + 1)}.

SVRG

Remark 2.3. To the best of our knowledge, the O(1/k)
ergodic convergence rate of {E[Φ(¯xk) − Φ(x(cid:63))]}k∈N is new
to the literature, which also holds for SVRG (Johnson &
Zhang, 2013).

3. Finite Time Manifold Identiﬁcation

From this section, we move to the local convergence proper-
ties of the SAGA/Prox-SVRG algorithms.

3.1. Partial Smoothness

Let Mx be a C 2-smooth Riemannian manifold of Rn
around a point x. Denote TMx (x) the tangent space of
Mx at a point x. Given a set S, par(S) def= R(S − S) is
the subspace parallel to the afﬁne hull of S, and (·)⊥ the
orthogonal complement.

Deﬁnition 3.1 (Partial smoothness (Lewis, 2003)). Let
R : Rn → R ∪ {+∞} be proper convex and lower semi-
continuous. Then R is said to be partly smooth at x relative
to a set Mx containing x if ∂R(x) (cid:54)= ∅, and moreover

Smoothness: Mx is a C 2-manifold around x, R re-
stricted to Mx is C 2 around x.
Sharpness: the tangent space TMx (x) coincides with
Tx
Continuity: the set-valued mapping ∂R is continuous at
x relative to Mx.

def= (par(∂R(x)))⊥.

The class of partly smooth functions at x relative to Mx is
denoted as PSFx(Mx). Many popular non-smooth penalty
functions are partly smooth, such as sparsity promoting
(cid:96)1-norm, group sparsity promoting (cid:96)1,2-norm, low rank pro-
moting nuclear norm, etc.; see Table 1 for more details.
We refer to (Liang et al., 2017) and references therein for
detailed properties of these partly smooth functions.

3.2. An Abstract Finite Time Manifold Identiﬁcation

Recall the perturbed FBS iteration (4). We have the follow-
ing abstract manifold identiﬁcation.

Theorem 3.2 (Abstract manifold identiﬁcation). For
problem (P), suppose that conditions (A.1)-(A.3) hold. For
the perturbed FBS iteration (4), suppose that:

(B.1) There exists γ > 0 such that lim inf k→+∞ γk ≥ γ;

(B.2) The error {εk}k∈N converges to 0 almost surely;
(B.3) There exists an x(cid:63) ∈ Argmin(Φ) such that

{xk}k∈N converges to x(cid:63) almost surely.

For the x(cid:63) in (B.3), suppose that R ∈ PSFx(cid:63) (Mx(cid:63) ), and
the following non-degeneracy condition holds

−∇F (x(cid:63)) ∈ ri(cid:0)∂R(x(cid:63))(cid:1).

(ND)

Then, there exists a K > 0 such that for all k ≥ K, we have
xk ∈ Mx(cid:63) almost surely.

Remark 3.3. In the deterministic setting, the ﬁnite manifold
identiﬁcation of (4), i.e. εk is deterministic approximation
error, is discussed in Section 3.3 of (Liang et al., 2017).

3.3. Manifold Identiﬁcation of SAGA/Prox-SVRG

Specialising Theorem 3.2 to the case of SAGA/Prox-SVRG
algorithms, we obtain the corollary below. For Prox-SVRG,
recall that x(cid:96),p is denoted as xk with k = (cid:96)P + p.

Corollary 3.4. For problem (P), suppose that conditions
(A.1)-(A.3) hold. Suppose that

• SAGA is applied under the conditions of Theorem 2.1;
• Prox-SVRG is ran under the conditions of Theorem 2.2.

Then there exists an x(cid:63) ∈ Argmin(Φ) such that the se-
quence {xk}k∈N generated by either algorithm converges
to x(cid:63) almost surely.

If moreover, R ∈ PSFx(cid:63) (Mx(cid:63) ), and the non-degeneracy
condition (ND) holds. Then, there exists a K > 0 such that
for all k ≥ K, xk ∈ Mx(cid:63) almost surely.

Remark 3.5. In (Lee & Wright, 2012; Duchi & Ruan,
2016), manifold identiﬁcation properties of the regularised
dual averaging algorithm (RDA) (Xiao, 2010) were re-
ported. The main difference between the present work and
those of (Lee & Wright, 2012; Duchi & Ruan, 2016) is
that, RDA is proposed for solving (P) with inﬁnite sum
problem, while in this work we mainly focus on the ﬁnite
sum case. We remark also that although RDA has manifold
identiﬁcation properties, the method does not exhibit linear
convergence under strong convexity, and hence, in contrast
to variance reduction methods, there can be no local acceler-
ation in the convergence rate upon manifold identiﬁcation.

4. Local Linear Convergence

Now we turn to the local linear convergence properties of
SAGA/Prox-SVRG algorithms. Throughout the section,
x(cid:63) ∈ Argmin(Φ) denotes a global minimiser (P), Mx(cid:63) is
a C 2-smooth manifold which contains x(cid:63), and Tx(cid:63) denotes
the tangent space of Mx(cid:63) at x(cid:63).

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

Table 1: Examples of partly smooth functions. For x ∈ Rn and some subset of indices b ⊂ {1, . . . , n}, xb is the restriction
of x to the entries indexed in b. DDIF stands for the ﬁnite differences operator, rank(z) denotes the rank of a matrix.

FUNCTION

(cid:96)1-NORM
(cid:96)1,2-NORM
(cid:96)∞-NORM
TV SEMI-NORM
NUCLEAR NORM

PARTIAL SMOOTH MANIFOLD

EXPRESSION

||x||1 = (cid:80)n
(cid:80)m

i=1 |xi|

i=1 ||xbi ||
maxi={1,...,n} |xi|
||x||TV = ||DDIF x||1 Mx = Tx = (cid:8)z ∈ Rn : IDDIF z ⊆ IDDIF x
||x||∗ = (cid:80)r

i=1 σ(x) Mx = (cid:8)z ∈ Rn1×n2 : rank(z) = rank(x) = r(cid:9), σ(x) SINGULAR VALUES OF x

Mx = Tx = (cid:8)z ∈ Rn : zIx ∈ Rsign(xIx )(cid:9), Ix = (cid:8)i : |xi| = ||x||∞

(cid:9), Ix = (cid:8)i : xi (cid:54)= 0(cid:9)
(cid:9), Ix = (cid:8)i : xbi (cid:54)= 0(cid:9)
(cid:9), IDDIF x = {i : (DDIF x)i (cid:54)= 0}

(cid:9)

Mx = Tx = (cid:8)z ∈ Rn : Iz ⊆ Ix
Mx = Tx = (cid:8)z ∈ Rn : Iz ⊆ Ix

4.1. Local Linear Convergence

Similar to (Liang et al., 2017) for the deterministic FBS-
type methods, the key assumption to establish local linear
convergence for SAGA/Prox-SVRG is a so-called restricted
injectivity condition, see below.
Restricted Injectivity Let F be locally C 2-smooth
around the minimiser x(cid:63), and moreover the following re-
stricted injectivity condition holds

ker(cid:0)∇2F (x(cid:63))(cid:1) ∩ Tx(cid:63) = {0}.

(RI)

Owing to Proposition 12 of (Liang et al., 2017), it can be
shown that under condition (RI), x(cid:63) actually is the unique
minimiser of problem (P), and Φ grows locally quadratic if
moreover R ∈ PSFx(cid:63) (Mx(cid:63) ).
Lemma 4.1 (Local quadratic growth (Liang et al.,
2017)). For problem (P), suppose that assumptions (A.1)-
(A.3) hold. Let x(cid:63) ∈ Argmin(Φ) be a global minimiser
such that conditions (ND) and (RI) are fulﬁlled and R ∈
PSFx(cid:63) (Mx(cid:63) ), then x(cid:63) is the unique minimiser of (P) and
there exist α > 0 and r > 0 such that

Φ(x) − Φ(x(cid:63)) ≥ α||x − x(cid:63)||2 : ∀x s.t. ||x − x(cid:63)|| ≤ r.

Remark 4.2. A similar result can be found in Theorem 5
of (Lee & Wright, 2012).

Lemma 4.1 implies that when a sequence convergent
stochastic method is applied to solve (P), eventually the
generated sequence {xk}k∈N will enter a local neighbour-
hood of the solution x(cid:63) ∈ Argmin(Φ) where the function
has the quadratic growth property. If moreover this method
is linearly convergent under strong convexity, then locally
it will also converge linearly under quadratic growth. As a
consequence, we have the following propositions.
Proposition 4.3 (Local linear convergence of SAGA).
For problem (P), suppose that conditions (A.1)-(A.3) hold,
and the SAGA algorithm (6) is applied with γk ≡ 1
3L . Then
xk converges to x(cid:63) ∈ Argmin(Φ) almost surely. If more-
over, R ∈ PSFx(cid:63) (Mx(cid:63) ), and conditions (ND)-(RI) are
satisﬁed, then there exists K > 0 such that for all k ≥ K,
E(cid:2)||xk − x(cid:63)||2(cid:3) = O(ρk−K
4m , α

where ρSAGA = 1 − min{ 1

3L }.

SAGA

),

The claim is a direct consequence of Theorem 1 of (Defazio
et al., 2014).

linear convergence of Prox-
Proposition 4.4 (Local
-SVRG). For problem (P), suppose that conditions (A.1)-
(A.3) hold, and the Prox-SVRG algorithm (8) is applied
such that Theorem 2.2 holds. Then xk converges to x(cid:63) ∈
Argmin(Φ) almost surely. If moreover, R ∈ PSFx(cid:63) (Mx(cid:63) ),
and conditions (ND)-(RI) are satisﬁed, then there exists
K > 0 such that for all k ≥ K,

E(cid:2)||˜x(cid:96) − x(cid:63)||2(cid:3) = O(ρ(cid:96)−K

),

SVRG

where ρSVRG = max{ 1−γαF
1+γαR
chosen such that ρSVRG < 1.

, 4Lγ(P + 1)} and γ, P are

The claim is a direct consequence of Theorem 2.2(ii).

4.2. Better Linear Rate Estimation?

In this part, we discuss brieﬂy the obtained linear rate esti-
mations of SAGA/Prox-SVRG (i.e. ρSAGA , ρSVRG ), in com-
parison to the one obtained in (Liang et al., 2017) for deter-
ministic FBS.

For the deterministic FBS algorithm, when R is locally
polyhedral around x(cid:63), Theorem 21 of (Liang et al., 2017)
implies that the local convergence rate of FBS is

ρFBS = 1 − γα.

While for ρSAGA , ρSVRG , their rate estimations both depend
on the number of functions m, which means that tightness3
of ρSAGA , ρSVRG could be much worse than ρFBS .

This naturally leads to the question of whether the rate
estimations for SAGA and Prox-SVRG can be improved.
Numerically, the rate estimation seems to be problem de-
pendent, that is for some problems ρFBS can be achieved;
see Example 6.1 and Figure 1. While for some problems,
the practical observation of SAGA/SVRG is much slower
than ρFBS; see Section 4 of the supplementary material for
details. Note that we are comparing the rate in terms of per
iteration, not gradient evaluation complexity.

3Tightness means how close is the rate estimation to the practi-

cal observation of the algorithms.

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

5. Beyond Local Convergence Analysis

5.3. Higher-order Acceleration

As already discussed, manifold identiﬁcation implies that,
the globally non-smooth problem minx∈Rn Φ(x) locally
becomes a C 2-smooth and possibly non-convex (e.g. nu-
clear norm) problem constrained on the identiﬁed manifold
minx∈Mx(cid:63) Φ(x). Such a transition to local C 2-smoothness,
provides various choices of acceleration. For instance, in
(Lee & Wright, 2012), the authors proposed a local version
of RDA, called RDA+, which achieves linear convergence.

The last acceleration strategy is the Riemannian manifold
based higher-order acceleration. Recently, various Rieman-
nian manifold based optimisation methods have been pro-
posed in the literature (Kressner et al., 2014; Ring & Wirth,
2012; Vandereycken, 2013; Boumal et al., 2014), particu-
larly for low-rank matrix recovery. However, an obvious
drawback of this class of methods is that the manifold should
be known a priori, which limits the their applications.

5.1. Better Local Lipschitz Continuity

If the dimension of the manifold Mx(cid:63) is much smaller than
that of the whole space Rn, then constrained to Mx(cid:63) , the
Lipschitz property of the smooth part would become much
better. For each i ∈ {1, · · · , m}, denote by LMx(cid:63) ,i the
Lipschitz constant of ∇fi along the manifold Mx(cid:63) , and let

LMx(cid:63)

def= max

i=1,··· ,m

LMx(cid:63) ,i.

In general, locally around x(cid:63), we have LMx(cid:63) ≤ L.
For SAGA/Prox-SVRG, and other stochastic methods which
have manifold identiﬁcation property, once the manifold is
identiﬁed, one can adapt their step-sizes to the local Lips-
chitz constants. Since step-size is crucial to the convergence
speed of these methods, the potential acceleration of such a
local adaptive strategy can be signiﬁcant. See Section 6.1
for numerical example, and the supplementary material on
how to compute or approximate LMx(cid:63) .

5.2. Lower Computational Complexity

Another important aspect of the manifold identiﬁcation prop-
erty is that one can naturally reduce the computational cost,
especially when Mx(cid:63) is of very low dimension.

Take R as the (cid:96)1-norm for example. Suppose that the so-
lution x(cid:63) of Φ is κ-sparse, i.e.
the number of non-zero
entries of x(cid:63) is κ. We have two stages of gradient evaluation
complexity for ∇fi(xk):

Before identiﬁcation: O(n),
After identiﬁcation: O(κ).

Now let R be the nuclear norm, and suppose F is quadratic.
Let the solutionx(cid:63) be of rank κ. We have two stages of gradi-
ent evaluation complexity for ∇fi(xk) (after identiﬁcation,
xk is stored in terms of its SVD decomposition):

Before identiﬁcation: O(n2),
After identiﬁcation: O(κn).

The manifold identiﬁcation of proximal methods implies
that one can ﬁrst use the proximal method to identify the
correct manifold, and then turn to the manifold based op-
timisation methods. The higher-order methods that can be
applied include Newton-type methods, when the restricted
injectivity condition (RI) is satisﬁed, and Riemannian geom-
etry based optimisation methods (Lemar´echal et al., 2000;
Miller & Malick, 2005; Smith, 1994; Boumal et al., 2014;
Vandereycken, 2013), for instance the non-linear conjugate
gradient method (Smith, 1994). Stochastic Riemannian
manifold based optimisation methods are also studied in the
literature, for instance in (Zhang et al., 2016), the authors
generalise SVRG to the manifold setting.

6. Numerical Experiments

We now consider several examples to verify the established
results. Three examples for R are considered, sparsity pro-
moting (cid:96)1-norm, group sparsity promoting (cid:96)1,2-norm and
low rank promoting nuclear norm.

As the main focus of this work is the theoretical properties
of SAGA and Prox-SVRG algorithms, the scale of the prob-
lems considered are not very large. In the supplementary
material, experiments on large scale real data are presented.

6.1. Local Linear Convergence

We consider the sparse logistic regression problem to demon-
strate the manifold identiﬁcation and local linear conver-
gence of SAGA/Prox-SVRG algorithms. Moreover in this
experiment, we provide only the rate estimation from the
FBS scheme, which is ρFBS = 1 − γα.
Example 6.1 (Sparse logistic regression). Let m > 0 and
(zi, yi) ∈ Rn × {±1}, i = 1, · · · , m be the training set.
The sparse logistic regression is to ﬁnd a linear decision
function which minimises the objective
µ||x||1 + 1
m

i=1 log (cid:0)1 + e−yif (zi;x,b)(cid:1),

min
(x,b)∈Rn×R

(cid:80)m

where f (z; x, b) = b + zT x.

For both cases, the reduction of computational cost depends
on the ratio of n/κ.

√

The setting of the experiment is: n = 256, m = 128, µ =
m and L = 1188. Notice that, the dimension of the
1/
problem is larger than the number of training points. The

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

(a) |supp(xk)|

(b) ||xk − x(cid:63)||

Figure 1: Finite manifold identiﬁcation and local linear convergence of SAGA and Prox-SVRG for solving the sparse logistic
regression problem in Example 6.1. (a) manifold identiﬁcation; (b) local linear convergence. ρFBS is the rate estimation
from FBS scheme, that is ρFBS = 1 − γα, where γ is the step-size and α is from Lemma 4.1.

parameters choices of SAGA and Prox-SVRG are:

SAGA : γ = 1
2L

;

Prox-SVRG : γ = 1
3L

, P = m.

Remark 6.2. The reason of choosing different step-sizes
for SAGA and Prox-SVRG is only to distinguish the red
and black plots in Figure 1. As for the considered synthetic
example, the performance of the two algorithms are almost
the same under same step-size.

The observations of the experiments are shown in Figure 1.
The observations of Prox-SVRG are for the inner loop se-
quence x(cid:96),p, which is denoted as xk by letting k = (cid:96)P + p.
The non-degeneracy condition (ND) and the restricted injec-
tivity condition (RI) are checked a posterior, which are all
satisﬁed for the tested example. The local quadratic growth
parameter α and the local Lipschitz constant LMx(cid:63) are

α = 0.0156 and LMx(cid:63) = 61.

Note that, locally the Lipschitz constant becomes about 19
times better.

Finite Manifold Identiﬁcation In Figure 1(a), we plot
the size of support of the sequence {xk}k∈N generated by
the two algorithms. The lines are sub-sampled, one out of
every m points.

The two algorithms are ran with the same initial point. It
can be observed that SAGA shows slightly faster manifold
identiﬁcation than Prox-SVRG, this is due the fact that the
step-size of SAGA (i.e. γ = 1
2L ) is larger than that of
Prox-SVRG (i.e. γ = 1
3L ). As mentioned in Remark 6.2,
the identiﬁcation speed of the two algorithms will be rather
similar if they are ran under the same choice of step-size.

Local Linear Convergence
In Figure 1(b), we demon-
strate the convergence rate of {||xk − x(cid:63)||}k∈N of the two
algorithms. The two solid lines are the practical observation
of {||xk − x(cid:63)||}k∈N generated by SAGA and Prox-SVRG,
the two dashed lines are the theoretical estimations using
ρFBS , and two dot-dashed lines are the practical observation
of the acceleration of SAGA/Prox-SVRG based on the local
Lipschitz continuity LMx(cid:63) . The lines are also sub-sampled,
one out of every m points.

For the considered problem, given the values of α and γ
above, we have that

SAGA : ρFBS = 0.999993, ρm
Prox-SVRG : ρFBS = 0.999995, ρm

FBS

FBS

= 0.99916;

= 0.99944.

For the considered problem setting, the spectral radius quite
matches the practical observations very well.

To conclude this part, we highlight the beneﬁts of adapting
to the local Lipschitz continuity of the problem. For both
SAGA and Prox-SVRG, their adaptive schemes (e.g. dot-
dashed lines) show 16 times faster performance compared
to the non-adaptive ones (e.g. solid lines). Such an accelera-
tion gain is on the same order of the difference between the
global Lipschitz and local Lipschitz constants, which is 19
times. More importantly, the computational cost of evaluat-
ing the local Lipschitz constant is almost negligible, which
makes the adaptive scheme more preferable in practice.

6.2. Local Higher-order Acceleration

We consider two problems of group sparse and low-rank
regression to demonstrate local higher-order acceleration.
Example 6.3 (Group sparse and low-rank regression).
Let xob ∈ Rn be either a group sparse vector or a low-
rank matrix (in a vectorised form), consider the following

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

(a) Group sparsity

(b) Low rank

Figure 2: Local higher-order acceleration after manifold identiﬁcation for Example 6.3. (a) Newton method is applied after
the manifold is identiﬁed by SAGA; (b) non-linear conjugate gradient is applied after manifold identiﬁcation. The black line
is the observation of SAGA algorithm, and the red line is the observation of the “SAGA+higher-order” scheme. The black
lines of the SAGA for both examples are not sub-sampled.

observation model b = Kxob + ω, where the entries of
K ∈ Rm×n are sampled from an i.i.d. zero-mean and unit-
variance Gaussian distribution, ω ∈ Rm is an additive error
with bounded (cid:96)2-norm.

Let µ > 0, and R be either the group sparsity promoting
(cid:96)1,2-norm or the low rank promoting nuclear norm. Con-
sider the problem to recover or approximate xob,

min
x∈Rn

µR(x) +

1
m

(cid:80)m
i=1

1
2

||Kix − bi||2
2,

where Ki, bi represent the ith row and entry of K and b,
respectively.

We have the following settings for the two examples of R:

(cid:96)1,2-norm: (m, n) = (256, 512), xob has 8 non-zero

blocks of block-size 4;
Nuclear norm: (m, n) = (2048, 4096), rank(xob) = 4.

We consider only the SAGA algorithm for this test, as the
main purpose is to highlight higher-order acceleration. For
the (cid:96)1,2-norm, Newton method is applied after the manifold
identiﬁcation, while for nuclear norm, a non-linear conju-
gate gradient method (Boumal et al., 2014) is applied after
manifold identiﬁcation.

The numerical results are shown in Figure 2. For (cid:96)1,2-
norm, the black line is the observation of the SAGA al-
gorithm with γ = 1
3L , the red line is the observation of the
“SAGA+Newton” hybrid scheme. It should be noted that the
lines are not sub-sampled.

For the hybrid scheme, SAGA is used for manifold identiﬁ-
cation, and Newton method is applied once the manifold is

identiﬁed. As observed, the quadratically convergent New-
ton method converges in only a few steps. For nuclear norm,
a non-linear conjugate gradient is applied when the manifold
is identiﬁed. Similar to the observation of the (cid:96)1,2-norm,
the super-linearly convergent non-linear conjugate gradient
shows superior performance than SAGA.

7. Conclusion

In this paper, we proposed a uniﬁed framework of local
convergence analysis for proximal variance reduced stochas-
tic gradient methods, and especially focused on the SAGA
and Prox-SVRG algorithms. Under partial smoothness, we
established that these schemes identify the partial smooth
manifold in ﬁnite time, and then converge locally linearly.
Moreover, we proposed several practical acceleration ap-
proaches which can greatly improve the convergence speed
of the algorithms.

Acknowledgements

The authors would like to thank F. Bach, J. Fadili and G.
Peyr´e for helpful discussions.

References

Beck, A. and Teboulle, M. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
Journal on Imaging Sciences, 2(1):183–202, 2009.

Boumal, N., Mishra, B., Absil, P.-A., Sepulchre, R., et al.
Manopt, a matlab toolbox for optimization on manifolds.
Journal of Machine Learning Research, 15(1):1455–1459,
2014.

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

Combettes, P. L. and Wajs, V. R. Signal recovery by proxi-
mal Forward–Backward splitting. Multiscale Modeling
& Simulation, 4(4):1168–1200, 2005.

Molinari, C., Liang, J., and Fadili, J. Convergence rates
of Forward–Douglas–Rachford splitting method. arXiv
preprint arXiv:1801.01088, 2018.

Nesterov, Y. Introductory lectures on convex optimization:

A basic course, volume 87. Springer, 2004.

Ring, W. and Wirth, B. Optimization methods on rieman-
nian manifolds and their application to shape space. SIAM
Journal on Optimization, 22(2):596–627, 2012.

Schmidt, M., Le Roux, N., and Bach, F. Minimizing ﬁnite
sums with the stochastic average gradient. Mathematical
Programming, 162(1-2):83–112, 2017.

Smith, S. T. Optimization techniques on Riemannian man-
ifolds. Fields institute communications, 3(3):113–135,
1994.

Vandereycken, B. Low-rank matrix completion by rieman-
nian optimization. SIAM Journal on Optimization, 23(2):
1214–1236, 2013.

Xiao, L. Dual averaging methods for regularized stochastic
learning and online optimization. Journal of Machine
Learning Research, 11(Oct):2543–2596, 2010.

Xiao, L. and Zhang, T. A proximal stochastic gradient
method with progressive variance reduction. SIAM Jour-
nal on Optimization, 24(4):2057–2075, 2014.

Zhang, H., Reddi, S. J., and Sra, S. Riemannian svrg: Fast
stochastic optimization on riemannian manifolds. In Ad-
vances in Neural Information Processing Systems, pp.
4592–4600, 2016.

Defazio, A., Bach, F., and Lacoste-Julien, S. Saga: A
fast incremental gradient method with support for non-
strongly convex composite objectives. In Advances in
Neural Information Processing Systems, pp. 1646–1654,
2014.

Duchi, J. and Ruan, F. Local asymptotics for some
stochastic optimization problems: Optimality, constraint
arXiv preprint
identiﬁcation, and dual averaging.
arXiv:1612.05612, 2016.

Johnson, R. and Zhang, T. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in neural information processing systems, pp. 315–323,
2013.

Kressner, D., Steinlechner, M., and Vandereycken, B. Low-
rank tensor completion by riemannian optimization. BIT
Numerical Mathematics, 54(2):447–468, 2014.

Lee, S. and Wright, S. J. Manifold identiﬁcation in dual
averaging for regularized stochastic online learning. Jour-
nal of Machine Learning Research, 13(Jun):1705–1744,
2012.

Lemar´echal, C., Oustry, F., and Sagastiz´abal, C. The U-
Lagrangian of a convex function. Trans. Amer. Math.
Soc., 352(2):711–729, 2000.

Lewis, A. S. Active sets, nonsmoothness, and sensitivity.
SIAM Journal on Optimization, 13(3):702–725, 2003.

Liang, J., Fadili, J., and Peyr´e, G. Local linear convergence
of Forward–Backward under partial smoothness. In Ad-
vances in Neural Information Processing Systems, pp.
1970–1978, 2014.

Liang, J., Fadili, J., and Peyr´e, G. Convergence rates with
inexact non-expansive operators. Mathematical Program-
ming, 159(1):403–434, September 2016.

Liang, J., Fadili, J., and Peyr´e, G. Activity identiﬁcation
and local linear convergence of Forward–Backward-type
methods. SIAM Journal on Optimization, 27(1):408–437,
2017.

Lions, P. L. and Mercier, B. Splitting algorithms for the sum
of two nonlinear operators. SIAM Journal on Numerical
Analysis, 16(6):964–979, 1979.

Miller, S. A. and Malick, J. Newton methods for nonsmooth
convex minimization: connections among-Lagrangian,
Riemannian Newton and SQP methods. Mathematical
programming, 104(2-3):609–633, 2005.

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

Clarice Poon * 1 Jingwei Liang * 1 Carola-Bibiane Sch¨onlieb 1

Abstract
In this paper, we present a local convergence anal-
ysis for a class of stochastic optimisation meth-
ods:
the proximal variance reduced stochastic
gradient methods, and mainly focus on SAGA
(Defazio et al., 2014) and Prox-SVRG (Xiao &
Zhang, 2014). Under the assumption that the
non-smooth component of the optimisation prob-
lem is partly smooth relative to a smooth mani-
fold, we present a uniﬁed framework for the local
convergence analysis of SAGA/Prox-SVRG: (i)
the sequences generated by the methods are able
to identify the smooth manifold in a ﬁnite num-
ber of iterations; (ii) then the sequence enters a
local linear convergence regime. Furthermore,
we discuss various possibilities for accelerating
these algorithms, including adapting to better lo-
cal parameters, and applying higher-order deter-
ministic/stochastic optimisation methods which
can achieve super-linear convergence. Several
concrete examples arising from machine learning
are considered to demonstrate the obtained result.

1. Introduction

1.1. Non-smooth Optimisation

Modern optimisation has become a core part of many ﬁelds,
such as machine learning and signal/image processing. In a
world of increasing data demands, there are two key driving
forces behind modern optimisation.

• Non-smooth regularisation. We are often faced with
models of high complexity, however, the solutions of
interest often lie on a manifold of low dimension which
is promoted by the non-smooth regularisers. There have
been several recent studies explaining how proximal
methods identify this low dimensional manifold and efﬁ-

*Equal contribution

1DAMTP, University of Cambridge,
Cambridge, United Kingdom. Correspondence to: Clarice
Poon <C.M.H.S.Poon@maths.cam.ac.uk>,
Jingwei Liang
<jl993@cam.ac.uk>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

ciently output solutions which take a certain structure;
see for instance (Liang et al., 2017) for the case of deter-
ministic proximal gradient methods.

• Stochastic methods. The past decades have seen an
exponential growth in the data sizes that we have to
handle, and stochastic methods have been popular due to
their low computational cost; see for instance (Schmidt
et al., 2017; Defazio et al., 2014; Xiao & Zhang, 2014)
and references therein.

The purpose of this paper is to show that proximal variance
reduced stochastic gradient methods allow to beneﬁt from
both efﬁcient structure enforcement and low computational
cost. In particular, we present a study of manifold identi-
ﬁcation and local acceleration properties of these methods
when applied to the following minimisation problem:

min
x∈Rn

Φ(x) def= R(x) + F (x),

(P)

where R(x) is a non-smooth structure imposing penalty
term, and

F (x) def= 1
m
where each fi is continuously differentiable.

i=1 fi(x),

(cid:80)m

We are interested in the problems where the value of m is
very large. A typical example of (P) is the LASSO problem,
which reads

min
x∈Rn

µ||x||1 +

1
m

(cid:80)m
i=1

1
2

||Kix − bi||2,

where µ > 0 is a positive trade-off parameter, Ki is the ith
row of a matrix K ∈ Rm×n, and bi is the ith element of
the vector b ∈ Rm. Throughout this paper, we consider the
following basic assumptions for problem (P):

(A.1) R : Rn → R ∪ {+∞} is proper, convex and lower

semi-continuous;

(A.2) F : Rn → R is continuously differentiable with
∇F being LF -Lipschitz continuous. For each in-
dex i = 1, · · · , m, fi is continuously differentiable
with Li-Lipschitz continuous gradient;

(A.3) Argmin(Φ) (cid:54)= ∅, that is the set of minimisers is

non-empty.

In addition to (A.2), deﬁne L def= maxi={1,··· ,m} Li, which
is the uniform Lipschitz continuity of functions fi. Note
that LF ≤ 1
m

i Li ≤ L holds.

(cid:80)

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

1.2. Deterministic Forward–Backward Splitting

A classical approach to solve (P) is the Forward–Backward
splitting (FBS) method (Lions & Mercier, 1979), which
is also known as proximal gradient descent method. The
standard FBS iteration reads

xk+1 = proxγkR

(cid:0)xk − γk∇F (xk)(cid:1), γk ∈ ]0, 2
LF

[,

(1)

where proxγR is the proximity operator of R deﬁned by

proxγR(·) def= min
x∈Rn

γR(x) + 1
2

||x − ·||2.

(2)

In general, the advantages of FBS can be summarised as:

√

• Known convergence rates.

• Robust convergence guarantees. Both sequence and
objective function are convergent for 0 < γ ≤ γk ≤
γ < 2/LF where γ, γ > 0 (Combettes & Wajs, 2005);
There holds ||xk −
xk−1|| = o(1/
k) (Liang et al., 2016), and Φ(xk) −
Φ(x(cid:63)) = o(1/k) (Molinari et al., 2018) where x(cid:63) ∈
Argmin(Φ). These rates can be improved to linear1 if
for instance Φ is strongly convex (Nesterov, 2004);
• Numerous acceleration techniques. Such as the iner-
tial schemes including inertial FBS (Liang et al., 2017),
FISTA (Beck & Teboulle, 2009) and Nesterov’s optimal
methods (Nesterov, 2004);

• Structure adaptivity. Several recent work (Liang et al.,
2014; 2017) studies the manifold identiﬁcation prop-
erties of FBS. It is shown in (Liang et al., 2017) that
within ﬁnite time, the FBS iterates xk all lie on the same
manifold as the optimal solution x(cid:63). Furthermore, upon
identifying this manifold, the FBS iterates can be proved
to converge linearly to the optimal solution x(cid:63).

However, despite these advantages, for problem (P), when
m is very large, computing ∇F (xk) could be very expen-
sive, which makes the deterministic FBS-type methods un-
suitable for solving large-scale problems.

1.3. Proximal Stochastic Gradient Descent

The most straightforward extension of stochastic gradient
descent to problem (P) is the proximal stochastic gradient
descent (Prox-SGD), which reads, for k = 0, 1, 2, 3, · · ·

(cid:36)

sample ik uniformly from {1, · · · , m}
(cid:0)xk − γk∇fik (xk)(cid:1).
xk+1 = proxγkR

(3)

Different from FBS, Prox-SGD only evaluates the gradient
of one sampled function fik at each step. However, to
ensure the convergence, the step-size γk of Prox-SGD has
to converge to 0 at a proper speed (e.g. γk = ks for s ∈
]1/2, 1]), leading to only O(1/
k) convergence rate for
Φ(xk) − Φ(x(cid:63)). When Φ is strongly convex, the rate for the
objective can only be improved to O(1/k).

√

1Linear convergence is also known as geometric or exponential

convergence.

Perturbed Forward–Backward Splitting An alterna-
tive perspective of treating Prox-SGD is as a perturbation of
the deterministic FBS scheme. More precisely, iteration (3)
can be written as the inexact FBS iteration with stochastic
approximation error for the gradient, for k = 0, 1, 2, 3, · · ·

(cid:36)

sample εk from a ﬁnite distribution Dk,
xk+1 = proxγkR

(cid:0)xk − γk(∇F (xk) + εk)(cid:1).

(4)

For most existing stochastic gradient methods, E[εk] = 0
and ||εk||2 is the variance of the stochastic gradient.

For Prox-SGD, the error εk takes the form

εSGD
k

def= ∇fik (xk) − ∇F (xk).

(5)

k

||2 is only bounded, and does
In general, the variance ||εSGD
not vanish to 0 as xk → x(cid:63). One consequence of this non-
vanishing variance is that, unlike FBS, in general manifold
identiﬁcation cannot occur. In Section 1 of the supplemen-
tary material, we give an example to illustrate this point.

1.4. Variance Reduced Stochastic Gradient Methods

To overcome the vanishing step-size and slow convergence
speed of Prox-SGD caused by non-vanishing variance, sev-
eral variance reduced schemes are proposed schemes (De-
fazio et al., 2014; Xiao & Zhang, 2014). The features of
variance reduced techniques can be summarised as:

• Same as Prox-SGD, in expectation, the stochastic gradi-
ent remains an unbiased estimation of the full gradient;
• Different from Prox-SGD, the variance ||εk||2 converges

to 0 when xk approaches the solution x(cid:63).

SAGA Algorithm (Defazio et al., 2014) The key idea of
SAGA for reducing the variance is utilising the gradient
history for the evaluation of the current gradient.

Given an initial point x0, deﬁne the individual gradient
def= ∇fi(x0), i = 1, · · · , m. Then, for k = 0, 1, 2, 3, · · ·
g0,i













sample ik uniformly from {1, · · · , m},
wk = xk − γk(∇fik (xk) − gk,ik + 1
m
xk+1 = proxγkR(wk),

(cid:40)

gk,i =

∇fi(xk)
gk−1,i

if i = ik,
o.w.

(cid:80)m

i=1 gk,i),

(6)

In the context of (4), the stochastic approximation error εk
of SAGA takes the form

εSAGA
k

def= ∇fik (xk)−gk,ik + 1
m

(cid:80)m

i=1 gk,i −∇F (xk). (7)

Prox-SVRG Algorithm (Xiao & Zhang, 2014) Com-
pared to SAGA, in stead of using the gradient history, Prox-

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

SVRG computes the full gradient of a given point, and uses
it for a certain number of iterations.

Let P be a positive integer, for (cid:96) = 0, 1, 2, · · ·

i=1 ∇fi(˜x(cid:96)), x(cid:96),0 = ˜x(cid:96),

(cid:80)m

˜g(cid:96) = 1
m
For p = 1, · · · , P






















sample ip uniformly from {1, · · · , m},
wk = x(cid:96),p−1 − γk(∇fip (x(cid:96),p−1) − ∇fip (˜x(cid:96)) + ˜g(cid:96)),
x(cid:96),p = proxγkR(wk).
Option I : ˜x(cid:96)+1 = x(cid:96),P ,
Option II : ˜x(cid:96)+1 = 1
P

p=1 x(cid:96),p.

(cid:80)P

Local Accelerations Another important implication of
manifold identiﬁcation is that the global non-smooth op-
timisation problem becomes locally C 2-smooth along the
manifold Mx(cid:63) , and moreover is locally strongly convex
if the restricted injectivity condition (RI) is satisﬁed. This
implies that locally we have many choices of acceleration to
choose, for instance we can turn to higher-order optimisation
methods, such as (quasi)-Newton methods or Riemannian
manifold based optimisation methods which can lead to
super linear convergence.

Lastly, for the numerical experiments considered in this pa-
per, the corresponding MATLAB source code to reproduce
the results is available online2.

(8)

1.6. Notations

In the context of (4), given x(cid:96),p, denote k = (cid:96)P + p, then
we have x(cid:96),p = xk and the stochastic approximation error
εk of Prox-SVRG reads

εSVRG
k

def= ∇fip (xk) − ∇fip (˜x(cid:96)) + ˜g(cid:96) − ∇F (xk).

(9)

1.5. Contributions

In this paper, we present a very general framework for
analysing the local convergence properties of variance re-
duced stochastic gradient. More precisely, this paper con-
sists of the following contributions.

Convergence of Sequence for SAGA/Prox-SVRG As-
suming only convexity, we prove the almost sure global
convergence of the sequences generated by SAGA (Theo-
rem 2.1) and Prox-SVRG with “Option I” (Theorem 2.2),
which is new to the literature. Moreover, for Prox-SVRG
with “Option I”, an O(1/k) ergodic convergence rate for
the objective function is proved.

Finite Time Manifold Identiﬁcation Let x(cid:63) be a global
minimiser of problem (P), and suppose that the sequence
{xk}k∈N generated by the perturbed FBS iteration (4) con-
verges to x(cid:63) almost surely. Then under the additional as-
sumptions that the non-smooth function R is partly smooth
at x(cid:63) relative to a C 2-smooth manifold Mx(cid:63) (Deﬁnition 3.1)
and a non-degeneracy condition (Eq. (ND)) holds at x(cid:63), in
Theorem 3.2 we prove a general ﬁnite time manifold iden-
tiﬁcation result for the perturbed FBS scheme (4). The
manifold identiﬁcation means that after a ﬁnite number of
iterations, say K, there holds xk ∈ Mx(cid:63) for all k ≥ K.
Specialising the result to SAGA and Prox-SVRG algorithms,
we prove the ﬁnite manifold identiﬁcation of these two al-
gorithms (Corollary 3.4).

Local Linear Convergence Building upon the manifold
identiﬁcation result, if moreover F is locally C 2-smooth
along Mx(cid:63) near x(cid:63) and a restricted injectivity condition
(Eq. (RI)) is satisﬁed by the Hessian ∇2F (x(cid:63)), we show
that locally SAGA and Prox-SVRG converge linearly.

Throughout the paper, N denotes the set of non-negative
integers and k ∈ N denotes the index. Rn is the Euclidean
space of dimension n. For a non-empty convex set Ω ⊂ Rn,
ri(Ω) and rbd(Ω) denote its relative interior and relative
boundary respectively.

Let R be proper, convex and lower semi-continuous, the
sub-differential is deﬁned by ∂R(x) def= (cid:8)g ∈ Rn|R(y) ≥
R(x) + (cid:104)g, y − x(cid:105), ∀y ∈ Rn(cid:9). A function R is α-strongly
convex for some α > 0 if R(x) − α

2 ||x||2 is convex.

2. Global Convergence of SAGA/Prox-SVRG

In this section, we prove the convergence of the sequence
generated by the SAGA and Prox-SVRG with “Option I”
without strong convexity, while in their respective original
work (Defazio et al., 2014; Xiao & Zhang, 2014), only the
convergence of the objective function value is provided.

We present ﬁrst the convergence result of the SAGA algo-
rithm, recall that L is the uniform Lipschitz continuity of all
element functions fi, i = 1, · · · , m.

Theorem 2.1 (Convergence of SAGA). For problem (P),
suppose that conditions (A.1)-(A.3) hold. Let {xk}k∈N be
the sequence generated by the SAGA algorithm (6) with
γk ≡ 1
3L , then there exists an x(cid:63) ∈ Argmin(Φ) such that
almost surely Φ(xk) → Φ(x(cid:63)), xk → x(cid:63) and εSAGA
k → 0.

Next we provide the convergence result of the Prox-
SVRG algorithm with “Option I”. Given (cid:96) ∈ N and
p ∈ {1, · · · , P }, let k = (cid:96)P + p, and denote x(cid:96),p = xk. For
(cid:96)=1 x(cid:96).
sequence {xk}k∈N, deﬁne a new point by ¯xk
Theorem 2.2 (Convergence of Prox-SVRG). For problem
(P), suppose that conditions (A.1)-(A.3) hold. Let {xk}k∈N
be the sequence generated by the Prox-SVRG algorithm (8)
with “Option I”. Then,

def= 1
k

(cid:80)k

2https://github.com/jliang993/Local-VRSGD

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

(i) If we ﬁx γk ≡ γ with γ ≤

4L(P +2) , then there exists
a minimiser x(cid:63) ∈ Argmin(Φ) such that xk → x(cid:63) and
k → 0 almost surely. Moreover, there holds for
εSVRG
each k = (cid:96)P , E[Φ(¯xk) − Φ(x(cid:63))] = O(1/k);

1

(ii) Suppose that R, F are moreover αR and αF strongly
convex respectively, then if 4Lγ(P + 1) < 1, there
holds E(cid:2)||˜x(cid:96) − x(cid:63)||2(cid:3) ≤ O(ρ(cid:96)
), where ρSVRG =
max{ 1−γαF
1+γαR

, 4Lγ(P + 1)}.

SVRG

Remark 2.3. To the best of our knowledge, the O(1/k)
ergodic convergence rate of {E[Φ(¯xk) − Φ(x(cid:63))]}k∈N is new
to the literature, which also holds for SVRG (Johnson &
Zhang, 2013).

3. Finite Time Manifold Identiﬁcation

From this section, we move to the local convergence proper-
ties of the SAGA/Prox-SVRG algorithms.

3.1. Partial Smoothness

Let Mx be a C 2-smooth Riemannian manifold of Rn
around a point x. Denote TMx (x) the tangent space of
Mx at a point x. Given a set S, par(S) def= R(S − S) is
the subspace parallel to the afﬁne hull of S, and (·)⊥ the
orthogonal complement.

Deﬁnition 3.1 (Partial smoothness (Lewis, 2003)). Let
R : Rn → R ∪ {+∞} be proper convex and lower semi-
continuous. Then R is said to be partly smooth at x relative
to a set Mx containing x if ∂R(x) (cid:54)= ∅, and moreover

Smoothness: Mx is a C 2-manifold around x, R re-
stricted to Mx is C 2 around x.
Sharpness: the tangent space TMx (x) coincides with
Tx
Continuity: the set-valued mapping ∂R is continuous at
x relative to Mx.

def= (par(∂R(x)))⊥.

The class of partly smooth functions at x relative to Mx is
denoted as PSFx(Mx). Many popular non-smooth penalty
functions are partly smooth, such as sparsity promoting
(cid:96)1-norm, group sparsity promoting (cid:96)1,2-norm, low rank pro-
moting nuclear norm, etc.; see Table 1 for more details.
We refer to (Liang et al., 2017) and references therein for
detailed properties of these partly smooth functions.

3.2. An Abstract Finite Time Manifold Identiﬁcation

Recall the perturbed FBS iteration (4). We have the follow-
ing abstract manifold identiﬁcation.

Theorem 3.2 (Abstract manifold identiﬁcation). For
problem (P), suppose that conditions (A.1)-(A.3) hold. For
the perturbed FBS iteration (4), suppose that:

(B.1) There exists γ > 0 such that lim inf k→+∞ γk ≥ γ;

(B.2) The error {εk}k∈N converges to 0 almost surely;
(B.3) There exists an x(cid:63) ∈ Argmin(Φ) such that

{xk}k∈N converges to x(cid:63) almost surely.

For the x(cid:63) in (B.3), suppose that R ∈ PSFx(cid:63) (Mx(cid:63) ), and
the following non-degeneracy condition holds

−∇F (x(cid:63)) ∈ ri(cid:0)∂R(x(cid:63))(cid:1).

(ND)

Then, there exists a K > 0 such that for all k ≥ K, we have
xk ∈ Mx(cid:63) almost surely.

Remark 3.3. In the deterministic setting, the ﬁnite manifold
identiﬁcation of (4), i.e. εk is deterministic approximation
error, is discussed in Section 3.3 of (Liang et al., 2017).

3.3. Manifold Identiﬁcation of SAGA/Prox-SVRG

Specialising Theorem 3.2 to the case of SAGA/Prox-SVRG
algorithms, we obtain the corollary below. For Prox-SVRG,
recall that x(cid:96),p is denoted as xk with k = (cid:96)P + p.

Corollary 3.4. For problem (P), suppose that conditions
(A.1)-(A.3) hold. Suppose that

• SAGA is applied under the conditions of Theorem 2.1;
• Prox-SVRG is ran under the conditions of Theorem 2.2.

Then there exists an x(cid:63) ∈ Argmin(Φ) such that the se-
quence {xk}k∈N generated by either algorithm converges
to x(cid:63) almost surely.

If moreover, R ∈ PSFx(cid:63) (Mx(cid:63) ), and the non-degeneracy
condition (ND) holds. Then, there exists a K > 0 such that
for all k ≥ K, xk ∈ Mx(cid:63) almost surely.

Remark 3.5. In (Lee & Wright, 2012; Duchi & Ruan,
2016), manifold identiﬁcation properties of the regularised
dual averaging algorithm (RDA) (Xiao, 2010) were re-
ported. The main difference between the present work and
those of (Lee & Wright, 2012; Duchi & Ruan, 2016) is
that, RDA is proposed for solving (P) with inﬁnite sum
problem, while in this work we mainly focus on the ﬁnite
sum case. We remark also that although RDA has manifold
identiﬁcation properties, the method does not exhibit linear
convergence under strong convexity, and hence, in contrast
to variance reduction methods, there can be no local acceler-
ation in the convergence rate upon manifold identiﬁcation.

4. Local Linear Convergence

Now we turn to the local linear convergence properties of
SAGA/Prox-SVRG algorithms. Throughout the section,
x(cid:63) ∈ Argmin(Φ) denotes a global minimiser (P), Mx(cid:63) is
a C 2-smooth manifold which contains x(cid:63), and Tx(cid:63) denotes
the tangent space of Mx(cid:63) at x(cid:63).

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

Table 1: Examples of partly smooth functions. For x ∈ Rn and some subset of indices b ⊂ {1, . . . , n}, xb is the restriction
of x to the entries indexed in b. DDIF stands for the ﬁnite differences operator, rank(z) denotes the rank of a matrix.

FUNCTION

(cid:96)1-NORM
(cid:96)1,2-NORM
(cid:96)∞-NORM
TV SEMI-NORM
NUCLEAR NORM

PARTIAL SMOOTH MANIFOLD

EXPRESSION

||x||1 = (cid:80)n
(cid:80)m

i=1 |xi|

i=1 ||xbi ||
maxi={1,...,n} |xi|
||x||TV = ||DDIF x||1 Mx = Tx = (cid:8)z ∈ Rn : IDDIF z ⊆ IDDIF x
||x||∗ = (cid:80)r

i=1 σ(x) Mx = (cid:8)z ∈ Rn1×n2 : rank(z) = rank(x) = r(cid:9), σ(x) SINGULAR VALUES OF x

Mx = Tx = (cid:8)z ∈ Rn : zIx ∈ Rsign(xIx )(cid:9), Ix = (cid:8)i : |xi| = ||x||∞

(cid:9), Ix = (cid:8)i : xi (cid:54)= 0(cid:9)
(cid:9), Ix = (cid:8)i : xbi (cid:54)= 0(cid:9)
(cid:9), IDDIF x = {i : (DDIF x)i (cid:54)= 0}

(cid:9)

Mx = Tx = (cid:8)z ∈ Rn : Iz ⊆ Ix
Mx = Tx = (cid:8)z ∈ Rn : Iz ⊆ Ix

4.1. Local Linear Convergence

Similar to (Liang et al., 2017) for the deterministic FBS-
type methods, the key assumption to establish local linear
convergence for SAGA/Prox-SVRG is a so-called restricted
injectivity condition, see below.
Restricted Injectivity Let F be locally C 2-smooth
around the minimiser x(cid:63), and moreover the following re-
stricted injectivity condition holds

ker(cid:0)∇2F (x(cid:63))(cid:1) ∩ Tx(cid:63) = {0}.

(RI)

Owing to Proposition 12 of (Liang et al., 2017), it can be
shown that under condition (RI), x(cid:63) actually is the unique
minimiser of problem (P), and Φ grows locally quadratic if
moreover R ∈ PSFx(cid:63) (Mx(cid:63) ).
Lemma 4.1 (Local quadratic growth (Liang et al.,
2017)). For problem (P), suppose that assumptions (A.1)-
(A.3) hold. Let x(cid:63) ∈ Argmin(Φ) be a global minimiser
such that conditions (ND) and (RI) are fulﬁlled and R ∈
PSFx(cid:63) (Mx(cid:63) ), then x(cid:63) is the unique minimiser of (P) and
there exist α > 0 and r > 0 such that

Φ(x) − Φ(x(cid:63)) ≥ α||x − x(cid:63)||2 : ∀x s.t. ||x − x(cid:63)|| ≤ r.

Remark 4.2. A similar result can be found in Theorem 5
of (Lee & Wright, 2012).

Lemma 4.1 implies that when a sequence convergent
stochastic method is applied to solve (P), eventually the
generated sequence {xk}k∈N will enter a local neighbour-
hood of the solution x(cid:63) ∈ Argmin(Φ) where the function
has the quadratic growth property. If moreover this method
is linearly convergent under strong convexity, then locally
it will also converge linearly under quadratic growth. As a
consequence, we have the following propositions.
Proposition 4.3 (Local linear convergence of SAGA).
For problem (P), suppose that conditions (A.1)-(A.3) hold,
and the SAGA algorithm (6) is applied with γk ≡ 1
3L . Then
xk converges to x(cid:63) ∈ Argmin(Φ) almost surely. If more-
over, R ∈ PSFx(cid:63) (Mx(cid:63) ), and conditions (ND)-(RI) are
satisﬁed, then there exists K > 0 such that for all k ≥ K,
E(cid:2)||xk − x(cid:63)||2(cid:3) = O(ρk−K
4m , α

where ρSAGA = 1 − min{ 1

3L }.

SAGA

),

The claim is a direct consequence of Theorem 1 of (Defazio
et al., 2014).

linear convergence of Prox-
Proposition 4.4 (Local
-SVRG). For problem (P), suppose that conditions (A.1)-
(A.3) hold, and the Prox-SVRG algorithm (8) is applied
such that Theorem 2.2 holds. Then xk converges to x(cid:63) ∈
Argmin(Φ) almost surely. If moreover, R ∈ PSFx(cid:63) (Mx(cid:63) ),
and conditions (ND)-(RI) are satisﬁed, then there exists
K > 0 such that for all k ≥ K,

E(cid:2)||˜x(cid:96) − x(cid:63)||2(cid:3) = O(ρ(cid:96)−K

),

SVRG

where ρSVRG = max{ 1−γαF
1+γαR
chosen such that ρSVRG < 1.

, 4Lγ(P + 1)} and γ, P are

The claim is a direct consequence of Theorem 2.2(ii).

4.2. Better Linear Rate Estimation?

In this part, we discuss brieﬂy the obtained linear rate esti-
mations of SAGA/Prox-SVRG (i.e. ρSAGA , ρSVRG ), in com-
parison to the one obtained in (Liang et al., 2017) for deter-
ministic FBS.

For the deterministic FBS algorithm, when R is locally
polyhedral around x(cid:63), Theorem 21 of (Liang et al., 2017)
implies that the local convergence rate of FBS is

ρFBS = 1 − γα.

While for ρSAGA , ρSVRG , their rate estimations both depend
on the number of functions m, which means that tightness3
of ρSAGA , ρSVRG could be much worse than ρFBS .

This naturally leads to the question of whether the rate
estimations for SAGA and Prox-SVRG can be improved.
Numerically, the rate estimation seems to be problem de-
pendent, that is for some problems ρFBS can be achieved;
see Example 6.1 and Figure 1. While for some problems,
the practical observation of SAGA/SVRG is much slower
than ρFBS; see Section 4 of the supplementary material for
details. Note that we are comparing the rate in terms of per
iteration, not gradient evaluation complexity.

3Tightness means how close is the rate estimation to the practi-

cal observation of the algorithms.

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

5. Beyond Local Convergence Analysis

5.3. Higher-order Acceleration

As already discussed, manifold identiﬁcation implies that,
the globally non-smooth problem minx∈Rn Φ(x) locally
becomes a C 2-smooth and possibly non-convex (e.g. nu-
clear norm) problem constrained on the identiﬁed manifold
minx∈Mx(cid:63) Φ(x). Such a transition to local C 2-smoothness,
provides various choices of acceleration. For instance, in
(Lee & Wright, 2012), the authors proposed a local version
of RDA, called RDA+, which achieves linear convergence.

The last acceleration strategy is the Riemannian manifold
based higher-order acceleration. Recently, various Rieman-
nian manifold based optimisation methods have been pro-
posed in the literature (Kressner et al., 2014; Ring & Wirth,
2012; Vandereycken, 2013; Boumal et al., 2014), particu-
larly for low-rank matrix recovery. However, an obvious
drawback of this class of methods is that the manifold should
be known a priori, which limits the their applications.

5.1. Better Local Lipschitz Continuity

If the dimension of the manifold Mx(cid:63) is much smaller than
that of the whole space Rn, then constrained to Mx(cid:63) , the
Lipschitz property of the smooth part would become much
better. For each i ∈ {1, · · · , m}, denote by LMx(cid:63) ,i the
Lipschitz constant of ∇fi along the manifold Mx(cid:63) , and let

LMx(cid:63)

def= max

i=1,··· ,m

LMx(cid:63) ,i.

In general, locally around x(cid:63), we have LMx(cid:63) ≤ L.
For SAGA/Prox-SVRG, and other stochastic methods which
have manifold identiﬁcation property, once the manifold is
identiﬁed, one can adapt their step-sizes to the local Lips-
chitz constants. Since step-size is crucial to the convergence
speed of these methods, the potential acceleration of such a
local adaptive strategy can be signiﬁcant. See Section 6.1
for numerical example, and the supplementary material on
how to compute or approximate LMx(cid:63) .

5.2. Lower Computational Complexity

Another important aspect of the manifold identiﬁcation prop-
erty is that one can naturally reduce the computational cost,
especially when Mx(cid:63) is of very low dimension.

Take R as the (cid:96)1-norm for example. Suppose that the so-
lution x(cid:63) of Φ is κ-sparse, i.e.
the number of non-zero
entries of x(cid:63) is κ. We have two stages of gradient evaluation
complexity for ∇fi(xk):

Before identiﬁcation: O(n),
After identiﬁcation: O(κ).

Now let R be the nuclear norm, and suppose F is quadratic.
Let the solutionx(cid:63) be of rank κ. We have two stages of gradi-
ent evaluation complexity for ∇fi(xk) (after identiﬁcation,
xk is stored in terms of its SVD decomposition):

Before identiﬁcation: O(n2),
After identiﬁcation: O(κn).

The manifold identiﬁcation of proximal methods implies
that one can ﬁrst use the proximal method to identify the
correct manifold, and then turn to the manifold based op-
timisation methods. The higher-order methods that can be
applied include Newton-type methods, when the restricted
injectivity condition (RI) is satisﬁed, and Riemannian geom-
etry based optimisation methods (Lemar´echal et al., 2000;
Miller & Malick, 2005; Smith, 1994; Boumal et al., 2014;
Vandereycken, 2013), for instance the non-linear conjugate
gradient method (Smith, 1994). Stochastic Riemannian
manifold based optimisation methods are also studied in the
literature, for instance in (Zhang et al., 2016), the authors
generalise SVRG to the manifold setting.

6. Numerical Experiments

We now consider several examples to verify the established
results. Three examples for R are considered, sparsity pro-
moting (cid:96)1-norm, group sparsity promoting (cid:96)1,2-norm and
low rank promoting nuclear norm.

As the main focus of this work is the theoretical properties
of SAGA and Prox-SVRG algorithms, the scale of the prob-
lems considered are not very large. In the supplementary
material, experiments on large scale real data are presented.

6.1. Local Linear Convergence

We consider the sparse logistic regression problem to demon-
strate the manifold identiﬁcation and local linear conver-
gence of SAGA/Prox-SVRG algorithms. Moreover in this
experiment, we provide only the rate estimation from the
FBS scheme, which is ρFBS = 1 − γα.
Example 6.1 (Sparse logistic regression). Let m > 0 and
(zi, yi) ∈ Rn × {±1}, i = 1, · · · , m be the training set.
The sparse logistic regression is to ﬁnd a linear decision
function which minimises the objective
µ||x||1 + 1
m

i=1 log (cid:0)1 + e−yif (zi;x,b)(cid:1),

min
(x,b)∈Rn×R

(cid:80)m

where f (z; x, b) = b + zT x.

For both cases, the reduction of computational cost depends
on the ratio of n/κ.

√

The setting of the experiment is: n = 256, m = 128, µ =
m and L = 1188. Notice that, the dimension of the
1/
problem is larger than the number of training points. The

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

(a) |supp(xk)|

(b) ||xk − x(cid:63)||

Figure 1: Finite manifold identiﬁcation and local linear convergence of SAGA and Prox-SVRG for solving the sparse logistic
regression problem in Example 6.1. (a) manifold identiﬁcation; (b) local linear convergence. ρFBS is the rate estimation
from FBS scheme, that is ρFBS = 1 − γα, where γ is the step-size and α is from Lemma 4.1.

parameters choices of SAGA and Prox-SVRG are:

SAGA : γ = 1
2L

;

Prox-SVRG : γ = 1
3L

, P = m.

Remark 6.2. The reason of choosing different step-sizes
for SAGA and Prox-SVRG is only to distinguish the red
and black plots in Figure 1. As for the considered synthetic
example, the performance of the two algorithms are almost
the same under same step-size.

The observations of the experiments are shown in Figure 1.
The observations of Prox-SVRG are for the inner loop se-
quence x(cid:96),p, which is denoted as xk by letting k = (cid:96)P + p.
The non-degeneracy condition (ND) and the restricted injec-
tivity condition (RI) are checked a posterior, which are all
satisﬁed for the tested example. The local quadratic growth
parameter α and the local Lipschitz constant LMx(cid:63) are

α = 0.0156 and LMx(cid:63) = 61.

Note that, locally the Lipschitz constant becomes about 19
times better.

Finite Manifold Identiﬁcation In Figure 1(a), we plot
the size of support of the sequence {xk}k∈N generated by
the two algorithms. The lines are sub-sampled, one out of
every m points.

The two algorithms are ran with the same initial point. It
can be observed that SAGA shows slightly faster manifold
identiﬁcation than Prox-SVRG, this is due the fact that the
step-size of SAGA (i.e. γ = 1
2L ) is larger than that of
Prox-SVRG (i.e. γ = 1
3L ). As mentioned in Remark 6.2,
the identiﬁcation speed of the two algorithms will be rather
similar if they are ran under the same choice of step-size.

Local Linear Convergence
In Figure 1(b), we demon-
strate the convergence rate of {||xk − x(cid:63)||}k∈N of the two
algorithms. The two solid lines are the practical observation
of {||xk − x(cid:63)||}k∈N generated by SAGA and Prox-SVRG,
the two dashed lines are the theoretical estimations using
ρFBS , and two dot-dashed lines are the practical observation
of the acceleration of SAGA/Prox-SVRG based on the local
Lipschitz continuity LMx(cid:63) . The lines are also sub-sampled,
one out of every m points.

For the considered problem, given the values of α and γ
above, we have that

SAGA : ρFBS = 0.999993, ρm
Prox-SVRG : ρFBS = 0.999995, ρm

FBS

FBS

= 0.99916;

= 0.99944.

For the considered problem setting, the spectral radius quite
matches the practical observations very well.

To conclude this part, we highlight the beneﬁts of adapting
to the local Lipschitz continuity of the problem. For both
SAGA and Prox-SVRG, their adaptive schemes (e.g. dot-
dashed lines) show 16 times faster performance compared
to the non-adaptive ones (e.g. solid lines). Such an accelera-
tion gain is on the same order of the difference between the
global Lipschitz and local Lipschitz constants, which is 19
times. More importantly, the computational cost of evaluat-
ing the local Lipschitz constant is almost negligible, which
makes the adaptive scheme more preferable in practice.

6.2. Local Higher-order Acceleration

We consider two problems of group sparse and low-rank
regression to demonstrate local higher-order acceleration.
Example 6.3 (Group sparse and low-rank regression).
Let xob ∈ Rn be either a group sparse vector or a low-
rank matrix (in a vectorised form), consider the following

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

(a) Group sparsity

(b) Low rank

Figure 2: Local higher-order acceleration after manifold identiﬁcation for Example 6.3. (a) Newton method is applied after
the manifold is identiﬁed by SAGA; (b) non-linear conjugate gradient is applied after manifold identiﬁcation. The black line
is the observation of SAGA algorithm, and the red line is the observation of the “SAGA+higher-order” scheme. The black
lines of the SAGA for both examples are not sub-sampled.

observation model b = Kxob + ω, where the entries of
K ∈ Rm×n are sampled from an i.i.d. zero-mean and unit-
variance Gaussian distribution, ω ∈ Rm is an additive error
with bounded (cid:96)2-norm.

Let µ > 0, and R be either the group sparsity promoting
(cid:96)1,2-norm or the low rank promoting nuclear norm. Con-
sider the problem to recover or approximate xob,

min
x∈Rn

µR(x) +

1
m

(cid:80)m
i=1

1
2

||Kix − bi||2
2,

where Ki, bi represent the ith row and entry of K and b,
respectively.

We have the following settings for the two examples of R:

(cid:96)1,2-norm: (m, n) = (256, 512), xob has 8 non-zero

blocks of block-size 4;
Nuclear norm: (m, n) = (2048, 4096), rank(xob) = 4.

We consider only the SAGA algorithm for this test, as the
main purpose is to highlight higher-order acceleration. For
the (cid:96)1,2-norm, Newton method is applied after the manifold
identiﬁcation, while for nuclear norm, a non-linear conju-
gate gradient method (Boumal et al., 2014) is applied after
manifold identiﬁcation.

The numerical results are shown in Figure 2. For (cid:96)1,2-
norm, the black line is the observation of the SAGA al-
gorithm with γ = 1
3L , the red line is the observation of the
“SAGA+Newton” hybrid scheme. It should be noted that the
lines are not sub-sampled.

For the hybrid scheme, SAGA is used for manifold identiﬁ-
cation, and Newton method is applied once the manifold is

identiﬁed. As observed, the quadratically convergent New-
ton method converges in only a few steps. For nuclear norm,
a non-linear conjugate gradient is applied when the manifold
is identiﬁed. Similar to the observation of the (cid:96)1,2-norm,
the super-linearly convergent non-linear conjugate gradient
shows superior performance than SAGA.

7. Conclusion

In this paper, we proposed a uniﬁed framework of local
convergence analysis for proximal variance reduced stochas-
tic gradient methods, and especially focused on the SAGA
and Prox-SVRG algorithms. Under partial smoothness, we
established that these schemes identify the partial smooth
manifold in ﬁnite time, and then converge locally linearly.
Moreover, we proposed several practical acceleration ap-
proaches which can greatly improve the convergence speed
of the algorithms.

Acknowledgements

The authors would like to thank F. Bach, J. Fadili and G.
Peyr´e for helpful discussions.

References

Beck, A. and Teboulle, M. A fast iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM
Journal on Imaging Sciences, 2(1):183–202, 2009.

Boumal, N., Mishra, B., Absil, P.-A., Sepulchre, R., et al.
Manopt, a matlab toolbox for optimization on manifolds.
Journal of Machine Learning Research, 15(1):1455–1459,
2014.

Local Convergence Properties of SAGA/Prox-SVRG and Acceleration

Combettes, P. L. and Wajs, V. R. Signal recovery by proxi-
mal Forward–Backward splitting. Multiscale Modeling
& Simulation, 4(4):1168–1200, 2005.

Molinari, C., Liang, J., and Fadili, J. Convergence rates
of Forward–Douglas–Rachford splitting method. arXiv
preprint arXiv:1801.01088, 2018.

Nesterov, Y. Introductory lectures on convex optimization:

A basic course, volume 87. Springer, 2004.

Ring, W. and Wirth, B. Optimization methods on rieman-
nian manifolds and their application to shape space. SIAM
Journal on Optimization, 22(2):596–627, 2012.

Schmidt, M., Le Roux, N., and Bach, F. Minimizing ﬁnite
sums with the stochastic average gradient. Mathematical
Programming, 162(1-2):83–112, 2017.

Smith, S. T. Optimization techniques on Riemannian man-
ifolds. Fields institute communications, 3(3):113–135,
1994.

Vandereycken, B. Low-rank matrix completion by rieman-
nian optimization. SIAM Journal on Optimization, 23(2):
1214–1236, 2013.

Xiao, L. Dual averaging methods for regularized stochastic
learning and online optimization. Journal of Machine
Learning Research, 11(Oct):2543–2596, 2010.

Xiao, L. and Zhang, T. A proximal stochastic gradient
method with progressive variance reduction. SIAM Jour-
nal on Optimization, 24(4):2057–2075, 2014.

Zhang, H., Reddi, S. J., and Sra, S. Riemannian svrg: Fast
stochastic optimization on riemannian manifolds. In Ad-
vances in Neural Information Processing Systems, pp.
4592–4600, 2016.

Defazio, A., Bach, F., and Lacoste-Julien, S. Saga: A
fast incremental gradient method with support for non-
strongly convex composite objectives. In Advances in
Neural Information Processing Systems, pp. 1646–1654,
2014.

Duchi, J. and Ruan, F. Local asymptotics for some
stochastic optimization problems: Optimality, constraint
arXiv preprint
identiﬁcation, and dual averaging.
arXiv:1612.05612, 2016.

Johnson, R. and Zhang, T. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in neural information processing systems, pp. 315–323,
2013.

Kressner, D., Steinlechner, M., and Vandereycken, B. Low-
rank tensor completion by riemannian optimization. BIT
Numerical Mathematics, 54(2):447–468, 2014.

Lee, S. and Wright, S. J. Manifold identiﬁcation in dual
averaging for regularized stochastic online learning. Jour-
nal of Machine Learning Research, 13(Jun):1705–1744,
2012.

Lemar´echal, C., Oustry, F., and Sagastiz´abal, C. The U-
Lagrangian of a convex function. Trans. Amer. Math.
Soc., 352(2):711–729, 2000.

Lewis, A. S. Active sets, nonsmoothness, and sensitivity.
SIAM Journal on Optimization, 13(3):702–725, 2003.

Liang, J., Fadili, J., and Peyr´e, G. Local linear convergence
of Forward–Backward under partial smoothness. In Ad-
vances in Neural Information Processing Systems, pp.
1970–1978, 2014.

Liang, J., Fadili, J., and Peyr´e, G. Convergence rates with
inexact non-expansive operators. Mathematical Program-
ming, 159(1):403–434, September 2016.

Liang, J., Fadili, J., and Peyr´e, G. Activity identiﬁcation
and local linear convergence of Forward–Backward-type
methods. SIAM Journal on Optimization, 27(1):408–437,
2017.

Lions, P. L. and Mercier, B. Splitting algorithms for the sum
of two nonlinear operators. SIAM Journal on Numerical
Analysis, 16(6):964–979, 1979.

Miller, S. A. and Malick, J. Newton methods for nonsmooth
convex minimization: connections among-Lagrangian,
Riemannian Newton and SQP methods. Mathematical
programming, 104(2-3):609–633, 2005.

