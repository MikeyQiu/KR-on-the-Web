Transfer learning and subword sampling for asymmetric-resource
one-to-many neural translation

Stig-Arne Gr¨onroos · Sami Virpioja · Mikko Kurimo

0
2
0
2
 
r
p
A
 
8
 
 
]
L
C
.
s
c
[
 
 
1
v
2
0
0
4
0
.
4
0
0
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract There are several approaches for improving neural machine translation for low-resource languages:
Monolingual data can be exploited via pretraining or data augmentation; Parallel corpora on related language
pairs can be used via parameter sharing or transfer learning in multilingual models; Subword segmentation and
regularization techniques can be applied to ensure high coverage of the vocabulary. We review these approaches
in the context of an asymmetric-resource one-to-many translation task, in which the pair of target languages are
related, with one being a very low-resource and the other a higher-resource language. We test various methods
on three artiﬁcially restricted translation tasks—English to Estonian (low-resource) and Finnish (high-resource),
English to Slovak and Czech, English to Danish and Swedish—and one real-world task, Norwegian to North
S´ami and Finnish. The experiments show positive eﬀects especially for scheduled multi-task learning, denoising
autoencoder, and subword sampling.

Keywords Low-resource languages · Multilingual machine translation · Transfer learning · Multi-task learning ·
Denoising sequence autoencoder · Subword segmentation

1 Introduction

Machine translation (MT) has become an important application for natural language processing (NLP), enabling
increased access to the wealth of digital information collected on-line, and new business opportunities in
multilingual markets. MT has made rapid advances following the adoption of deep neural networks in the last
decade, with variants of the sequence-to-sequence (seq2seq, Kalchbrenner and Blunsom, 2013; Sutskever et al.,
2014) architecture currently holding the state of the art in neural machine translation (NMT). However, the

Stig-Arne Gr¨onroos
Aalto University, Department of Signal Processing and Acoustics, Espoo, Finland
Tel.: +358-40-7398282
E-mail: stig-arne.gronroos@aalto.ﬁ
ORCiD: 0000-0002-3750-6924

Sami Virpioja
University of Helsinki, Department of Digital Humanities, Helsinki, Finland
and Utopia Analytics, Helsinki, Finland
E-mail: sami.virpioja@helsinki.ﬁ
ORCiD: 0000-0002-3568-150X

Mikko Kurimo
Aalto University, Department of Signal Processing and Acoustics, Espoo, Finland
E-mail: mikko.kurimo@aalto.ﬁ

2

Stig-Arne Gr¨onroos et al.

Low-resource multilingual NMT

Crosslingual
transfer

Exploiting
monolingual

Vocabulary
construction

Parameter
sharing

Parallel
transfer

Sequential
transfer

Pretraining

Dataset
augmentation

Subword
segmentation

Full
sharing

Partial
sharing

Scheduled
multi-task

Embeddings

LM fusion

Subnetwork
pretraining

Back-
translation

Auto-
encoder

Subword
regularization

Multilingual
segmentation

Fig. 1 Overview of techniques for improving low-resource multilingual NMT. Techniques highlighted with blue are used in this
work.

recent success has not applied to all languages equally. Current state-of-the-art methods require very large
amounts of data: Seq2seq methods have been shown to work well in large data scenarios, but are less eﬀective
for low-resource languages. The rapid digitalization of society has increased the availability of suitable parallel
training corpora, but the growth has not distributed evenly across languages.

The amount of data needed to reach acceptable quality can also vary based on language characteristics.
Rich, productive morphology leads to a combinatorial explosion in the number of word forms. Therefore, a
larger corpus is required to reach the same coverage of word forms. Often the two challenges coincide, with
morphologically complex languages that are also relatively low on resources.

Three distinct types of resources may be available for MT training: parallel data, monolingual data, and
data in related languages. In the low-resource translation setting, it is primarily the parallel data that is scarce.
Monolingual data is easier to acquire and typically more abundant. In addition, there may be related languages
with much more abundant resources.

In this work, we consider machine translation into a low-resource morphologically rich language by means of
transfer learning from a related high-resource target language, by exploiting available monolingual corpora,
and by exploring the methods and parameters for vocabulary construction. Figure 1 illustrates an overview
of the known techniques for low-resource multilingual NMT; most of them are considered in our experiments.
Our task is a one-to-many setting in multilingual neural machine translation (MNMT), as opposed to
many-to-one and many-to-many settings (Luong, 2016). As we consider target languages that have diﬀerent
amounts of training resources available, we call this an asymmetric-resource one-to-many translation task.
It has three major challenges:

Sparsity. Translating into a low-resource is challenging, especially in the case of a morphologically rich
language, due to a combination of small data and a large target vocabulary. The resulting data sparsity
makes it diﬃcult to estimate statistics for all but the most frequent items. Even though continuous-space
representations allow neural methods to generalize well, they learn poorly from low-count events. Methods like
subword segmentation (Virpioja et al., 2007; Sennrich et al., 2015) can reshape the frequency distribution of
the basic units to reduce sparsity. Suitable subwords are also beneﬁcial for exploiting transfer from related
high-resource languages (Gr¨onroos et al., 2018), and from monolingual data.

Data imbalance. In multilingual machine translation, it is very common to have an imbalance between the
languages in the training data. The data can vary in quantity, quality and appropriateness of domain. Typically
all three challenges aﬀect the low-resource languages: when data is hard to come by, even noisy and out-of-domain

Transfer learning and subword sampling for asymmetric one-to-many NMT

3

Table 1 Example from NMT system overﬁtted to the language modeling task.

Source (Estonian)
Overﬁt translation
Reference

Laktoosi puhul see nii ju ongi!
I’ve been thinking about it.
That’s the case with lactose!

data must be used. The data imbalance is typically addressed by oversampling the low-resource data. One way
to choose the oversampling weights is using a temperature-based approach to interpolate between sampling from
the true distribution and sampling uniformly (Arivazhagan et al., 2019). An alternative to oversampling the
data is to adapt the gradient scale or learning rate individually for each task (Chen et al., 2018).

Task imbalance. An NMT system is a conditional language model. The training signal for the language
model is much stronger than for the natural language understanding encoder and in particular the cross-lingually
aligning attention mechanism. High ﬂuency is a known property of NMT (Toral and S´anchez-Cartagena, 2017;
Koponen et al., 2019). When a vanilla NMT system is trained in a low-resource setting, it easily overﬁts the
decoder, degenerating into a fancy language model. When this occurs, the output resembles generated nonsense,
with possibly high ﬂuency but little relation to the source text. As an example, Table 1 shows an output from an
Estonian–English translation system trained from parallel data of only 18000 sentence pairs.

Given these challenges, our research questions include:

1. On cross-lingual transfer, is it better to use sequential (pretraining followed by ﬁne-tuning) or parallel (all

tasks at the same time) transfer, or something in between?

2. On exploiting monolingual data:

(a) Is it useful to have a target-language autoencoder in addition to the back-translation strategy, where

synthetic training data is generated by a target-to-source translation model?

(b) What kind of noise models are most useful for the autoencoder?

3. On vocabulary construction:

(a) What is a suitable granularity of subword segmentation for the low-resource task?
(b) Does it matter what data-driven segmentation method is used?
(c) Does subword regularization (sampling diﬀerent segmentations for the same word forms) help?

4. On available data and languages:

(a) How does the amount of the data available for the low-resource language aﬀect the translation quality?
(b) How important is language relatedness for the cross-lingual transfer?

As methodological contributions for NMT, we formulate a scheduled multi-task learning technique for
asymmetric-resource cross-lingual transfer, propose our recently introduced Morfessor EM+Prune method
(Gr¨onroos et al., 2020) for learning the subword vocabulary, and introduce a taboo sampling task for improving
the modeling of segmentation ambiguity. We include experiments using three diverse language families, with
Estonian, Slovak and Danish as simulated low-resource target languages. We also contribute a Norwegian bokm˚al
to North S´ami translation system, the ﬁrst NMT system for this target language, to the best of our knowledge.
In the next three sections, we will discuss the diﬀerent techniques for cross-lingual transfer, exploiting
monolingual data, and vocabulary construction. Then we will describe our experimental setup and discuss the
results for four diﬀerent groups of languages, and ﬁnally summarize our ﬁndings.

2 Cross-lingual transfer

Multilingual training allows exploiting cross-lingual transfer between related languages by training a single
model to translate between multiple language pairs. This is a form of multi-task learning (Caruana, 1997),
in which each language pair in the training data can be seen as a separate learning task (Luong et al., 2015).
The low-resource language is the main task, and at least one related high-resource language is used as an

Mixed ﬁne-tuning

Scheduled multi-task learning

Stig-Arne Gr¨onroos et al.

Task 1
Task 2

. . .

Pretraining

Fine-tuning

Phase 1

Phase N

time

4

Parallel transfer
Task 1
Task 2

Task 1
Task 2

Task 1
Task 2

Task 1
Task 2

Sequential transfer

Mixed pretraining

Pretraining

Fine-tuning

Pretraining

Fine-tuning

time

Fig. 2 Task mixing strategies for transfer learning.

time

time

auxiliary task. The cardinality of the multilingual translation has an eﬀect: cross-lingual transfer is easier in the
many-to-one setting compared to one-to-many (Arivazhagan et al., 2019). For a general survey on multilingual
translation, see (Dabre et al., 2020).

2.1 Sequential and parallel transfer

In transfer learning, knowledge gained while learning one task is transferred to another. The tasks can either
be trained sequentially or in parallel. Sequential transfer is a form of adaptation. In sequential transfer
learning the pretraining on a high-resource parent task is used to initialize and constrain the ﬁne-tuning
training on the low-resource child task. Zoph et al. (2016) applied sequential transfer learning to low-resource
neural machine translation. Sequential transfer carries the risk of catastrophic forgetting (McCloskey and Cohen,
1989; Goodfellow et al., 2013), in which the knowledge gained from the ﬁrst task fades away completely. Some
parameters can be frozen between the two training phases. This reduces the number of parameters trained from
the small data, which may delay overﬁtting.

When training tasks in parallel, called multi-task learning , catastrophic forgetting does not occur. If the
amount of data for diﬀerent tasks is highly asymmetrical, careful tuning of the task mixture weights is critical to
avoid overﬁtting on the small task.

It is also possible to combine sequential and parallel transfer. Figure 2 shows some possible ways of achieving
this by mixing the tasks. One strategy—mixed ﬁne-tuning —involves ﬁrst pretraining only on the large task,
and then ﬁne-tuning with a mixture of tasks. Chu et al. (2017) apply this strategy to domain adaptation. Kocmi
(2019) try the inverse setting—mixed pretraining —pretraining on a mixture of tasks and ﬁne-tuning only on
the child task.

(Kiperwasser and Ballesteros, 2018) propose generalizing these strategies into scheduled multi-task learn-
ing , in which training examples from diﬀerent tasks are selected according to a mixing distribution that changes
during training according to the task-mix schedule. They experiment with three schedules: constant, exponential
and sigmoidal. We propose a new partwise constant task-mix schedule suitable for an asymmetric-resource setting
with multiple auxiliary tasks. The task-mix schedule can have an arbitrary number of steps, any of which can be
mixing multiple tasks. All the other strategies can be recovered by using particular schedules with scheduled
multi-task learning.

Transfer learning and subword sampling for asymmetric one-to-many NMT

5

2.2 Parameter sharing

In neural networks, multilingual models are implemented through parameter sharing. It is possible to share all
neural network parameters, or select a subset for sharing allowing the remaining ones to be language-speciﬁc.
Parameter sharing can be either hard or soft. In hard parameter sharing the exact same parameter matrix is used
for several languages. In soft parameter sharing, each language has its own parameter matrix, but a dependence
is constructed between the corresponding parameters for diﬀerent languages.

The target language token Johnson et al. (2016) and language embedding (Lample and Conneau,
2019) approaches use hard sharing of all parameters. In the former, the model architecture is the same as in
a language-pair-speciﬁc model. The target language is indicated by a preprocessing step that prepends to the
input a special target language token, e.g. (cid:104)to fi(cid:105) to indicate that the target language is Finnish. The approach
can be scaled to more languages by increasing the capacity of the model, primarily by increasing the depth in
layers (Arivazhagan et al., 2019). The latter can be described as a factored representation, with the language
embedding factor marking the language of each word on the target side.

In contrast to full parameter sharing, it is also possible to divide the model parameters into shared and
language-speciﬁc subnetworks, e.g. sharing all parameters of the encoder, while letting each target language
have its own decoder. Parameter sharing can even be controlled on a more ﬁne-grained level (Sachan and Neubig,
2018). Shared attention (Firat et al., 2016) uses language-speciﬁc encoders and decoders with a shared attention,
while language-speciﬁc attention (Blackwood et al., 2018) does the opposite by sharing only the feedforward
sublayers of the decoder, while using language-speciﬁc parameters for the attention mechanisms.

The contextual parameter generator (Platanios et al., 2018) meta-learns a soft dependency between
parameters for diﬀerent tasks. It does this by using one neural network (the parameter generator) to generate
from some contextual variables the weights of another network (the model). Gu et al. (2018) apply meta-learning
to ﬁnd initializations that can very rapidly adapt to a new low-resource source language.

3 Exploiting monolingual data

While parallel data is the primary type of data used for training MT models, methods for eﬀectively exploiting
the more abundant monolingual data can greatly increase the number of available examples to learn from. Use of
monolingual data can be viewed as semi-supervised learning: both labeled (parallel) and unlabeled (monolingual)
data are used. There are two main approaches to exploiting monolingual data in MT: pretraining and dataset
augmentation.

In monolingual pretraining, some of the parameters of the ﬁnal translation model are pretrained on a task using
monolingual data, possibly using a diﬀerent loss than the one used during NMT training. There are several
ways to use pretraining: Pretrain word (or subword) embeddings for the encoder, decoder, or both. Pretrain a
separate language model for the target language, and combine it with the predictions of the translation model.
Or, ﬁnally, pretrain an entire subnetwork —encoder or decoder—of the translation model.

3.1 Pretraining

3.1.1 Embeddings

Both source and target embeddings can be pretrained on monolingual data from the source and target respectively.
As the embeddings are trained for e.g. a generic contextual prediction task, this is a form of transfer learning. The
pretrained embeddings can either be frozen or ﬁne-tuned, by respectively omitting or including them as trainable
parameters during the ﬁne-tuning phase. Thompson et al. (2018) investigate the eﬀects of freezing various
subnetwork parameters—including embeddings—on domain adaptation. In addition to using monolingual data,

6

Stig-Arne Gr¨onroos et al.

pretrained embeddings can contribute to cross-lingual transfer in the case of a shared multilingual embedding
space (Artetxe et al., 2018). The shared embedding spaces are typically on a word level.

3.1.2 Language model fusion

The predictions of a strong language model can be combined with the predictions of the translation model, either
using a separate rescoring step, or by combining the predictions during decoding, using model fusion. This
approach is used in statistical machine translation, where one or more target language models are combined
with a statistical translation model. The approach can also be applied in neural machine translation, through
shallow, deep (Gulcehre et al., 2015) or cold fusion (Sriram et al., 2017). As a neural machine translation system
is already a conditional language model, it may be preferable to ﬁnd a way to train the parameters of the NMT
system using the monolingual data.

3.1.3 Subnetwork pretraining

In subnetwork pretraining, the intent is to pretrain entire network components—the encoder or the decoder—with
knowledge about the structure of language. One way to achieve this using unlabeled data is to apply a language
modeling loss during pretraining. The loss function can either be the traditional next token prediction, or a
masked language model. Alternatively an autoencoder loss can be used.

Domhan and Hieber (2017) modify the NMT architecture by adding an auxiliary language model loss in the
internal layers of the decoder, before attending to the source. This loss allows the ﬁrst layers of the decoder to be
trained on monolingual data. They ﬁnd no beneﬁt of adding the language model loss unless additional monolingual
data is used. Adding monolingual data gives a beneﬁt, but does not outperform back-translation. Ramachandran
et al. (2017) pretrain the encoder and decoder with source and target language modeling tasks, respectively. To
prevent overﬁtting, they use task-mix ﬁne-tuning: the translation and language modeling objectives are trained
jointly (with equally weighted tasks). Skorokhodov et al. (2018) use both pretraining (on both source and target
side) and gated shallow fusion (on the target side) to transfer knowledge from pretrained language models. Some
of the experiments are performed on low-resource data going down to 10k sentence pairs.

3.2 Dataset augmentation

The best way to improve generalization is to train on more data. As natural training data is limited, a practical
way to acquire more is to generate additional synthetic data for augmentation. The main beneﬁt of dataset
augmentation is as regularization to prevent overﬁtting to non-robust properties of small data.

Simple ways to generate synthetic data include using a single dummy token on the source side (Sennrich
et al., 2016), and copying the target to source (Currey et al., 2017). The latter can be interpreted as a target-side
autoencoder task without noise. The largest factor in determining the eﬀectiveness of using synthetic data is how
much the synthetic data deviates from the true data distribution. To avoid confusing the encoder with synthetic
data from a diﬀerent distribution than the natural data, it may be beneﬁcial to use a special tag to identify the
synthetic data (Caswell et al., 2019).

3.2.1 Back-translation

Synthetic data can be self-generated by the model being trained, or a related model. In machine translation,
the best known example of synthetic data is back-translation (BT) (Sennrich et al., 2016). The process of
back-translation begins with the training of a preliminary MT model in the reverse direction, from target to
source. The target language monolingual data is translated using this model, producing a synthetic parallel data
set with the potentially noisy MT output on the source side. Because the quality of the translation system used

Transfer learning and subword sampling for asymmetric one-to-many NMT

7

for the back-translation aﬀects the noisiness of the synthetic data, the procedure can be improved by iterating
with alternating translation direction (Lample et al., 2018). Edunov et al. (2018) propose adding noise to the
back-translation output. The beneﬁt of noisy back-translation is further analyzed by Gra¸ca et al. (2019), who
recommend turning oﬀ label smoothing in the reverse model when combined with sampling decoding. As a
related strategy, Karakanta et al. (2018) convert parallel data from a high-resource language pair into synthetic
data for a related low-resource pair using transliteration. Zhang and Zong (2016) exploit monolingual data in
two ways: through self-learning by “forward-translating” the monolingual source data to create synthetic parallel
data, and by applying a reordering auxiliary task: the input is the natural source text, while the output is the
source text reordered using rules to match the target word order.

3.2.2 Subword regularization

Subword regularization is a technique proposed by Kudo (2018) for applying a probabilistic subword segmentation
model to generate more variability in the input text. Each time a word token is used during training, a new
segmentation is sampled for it. It can be seen as treating the subword segmentation as a latent variable. While
marginalizing over the latent variable exactly is intractable, the subword regularization procedure approximates
it through sampling.

3.2.3 Denoising sequence autoencoder

Back-translation is a slow method due to the additional training of the reverse translation model. A compu-
tationally cheaper way to turn monolingual data into synthetic source data is to use a denoising autoencoder
as an auxiliary task. Target language text, corrupted by a noise model, is fed in as a pseudo-source. Diﬀerent
noise models can be used, e.g. applying reordering, deletions, or substitutions to the input tokens. The desired
reconstruction output is the original noise-free target language text.

An autoencoder (Bourlard and Kamp, 1988) is a neural network that is trained to copy its input to its output.
It applies an encoder mapping from input to a hidden representation, i.e. code h = f (x), and decoder mapping
from code to a reconstruction of the input ˆx = g(h). To force the autoencoder to extract patterns in the data
instead of ﬁnding the trivial identity function ˆx = 1(1(x)), the capacity of the code must be restricted somehow.
In the under-complete autoencoder, the restriction is in the form of a bottleneck layer with small dimension.
For example, in the original sequence autoencoder (Dai and Le, 2015), the entire sequence is compressed into a
single vector.

In a modern sequence-to-sequence architecture, the attention mechanism ensures a very large bandwidth
between encoder and decoder. When used as an autoencoder, the network is thus highly over-complete. In this
case, the capacity of the code has to be controlled by regularization. Robustness to noise is used as the regularizer
in the denoising autoencoder (Vincent et al., 2008). Instead of feeding in the clean example x, a corrupted
copy of the input is sampled from a noise model C(˜x | x). The denoising autoencoder must then learn to reverse
the corruption to reconstruct the clean example. The use of noise as regularization is a successful technique
used e.g. in dropout (Srivastava et al., 2014), label smoothing (Szegedy et al., 2015), and SwitchOut (Wang
et al., 2018). Also multi-task learning acts as regularization by claiming some of the capacity of the model.
Belinkov and Bisk (2017) apply both natural and synthetic noises for NMT evaluation, ﬁnding that standard
character-based NMT models are not robust to these types of noise.

There are multiple ways of adding the autoencoder loss to the NMT training. The simplest one treats the
autoencoder task as if it was another language pair for multilingual training, and involves no changes to the
architecture. When using this type of autoencoder task on target language sentences, the task cardinality changes
into a many-to-one problem: the model must simultaneously learn a mapping from source to target and from
corrupted target to clean target. In both tasks the target language is the same. As the decoder is a conditional
language model, this task strengthens the modeling of the target language. When using source language sentences,
the model must simultaneously learn a one-to-many mapping from source to target and from corrupted source

8

Stig-Arne Gr¨onroos et al.

to clean source. Thus the decoder must learn to output both languages. The task may strengthen the encoder,
by increasing its robustness to noise, and by preventing the encoding from becoming too speciﬁc to the target
language. Luong et al. (2015) and Luong (2016) experiment with various auxiliary tasks, including this type of
autoencoder setup. They see a beneﬁt of using the autoencoder task, as long as it has a low enough weight in
the task mix. This setup is used also in our experiments.

There are also more complex NMT autoencoder setups. In dual learning , the autoencoder is built from
source-to-target and target-to-source translation models. Xia et al. (2016) combine source-to-target and target-
to-source translations in a closed loop which can be trained jointly, using two additional language modeling tasks
(for source and target respectively), and reinforcement learning with policy gradient. Cheng et al. (2016) use a
dual learning setup to exploit monolingual corpora in both source and target languages. Their loss consists of
four parts: translation likelihoods in both directions, source autoencoder and target autoencoder. Tu et al. (2016)
simplify the dual learning setup into an encoder–decoder–reconstructor network. The reconstructor attends
to the ﬁnal hidden states of the decoder and thus does not need a separate encoder. Their aim is to improve
adequacy by penalizing under-translation: the reconstructor is not able to generate any parts of the sentence
omitted by the decoder.

3.2.4 Noise models for text

To apply a denoising autoencoder to text, a suitable noise model for text is needed. In domains such as image and
speech, there are very intuitive noises, including rotating, scaling and mirroring for images, and reverberation,
time-scale stretching and pitch shifting for speech. As text is a sequence of discrete symbols, where even a
small change can have a drastic eﬀect on meaning, suitable noise models are less intuitive. It is not feasible to
guarantee the noise does not change the correct translation of the input.

Local reordering. Lample et al. (2017) perform a local reordering operation σ that they call slightly shuﬄing
the sentence. The reordering is achieved by adding to the index i of each token a random oﬀset drawn from the
uniform distribution from 0 to a maximum distance k. The tokens are then sorted according to the oﬀset indices.
This maintains the condition ∀i ∈ {1, n}, |σ(i) − i| ≤ k.

Token deletion. Randomly dropping tokens is perhaps the most commonly used noise. It is the central idea in
word dropout (Iyyer et al., 2015). In word dropout, each token is dropped according to a Bernoulli distribution
parameterized by a tunable dropout probability.

Token insertion. Randomly selected tokens can also be inserted into the sentence. The tokens can be sampled
from the entire vocabulary, or from a particular class of tokens. E.g. Vaibhav et al. (2019) insert three classes of
tokens: stop words, expletives and emoticons.

Token substitution. SwitchOut (Wang et al., 2018) applies random substitutions to tokens both in the source
and the target sentence. One beneﬁt of SwitchOut is that it can easily and eﬃciently be applied late in the data
processing pipeline, even to a numericalized and padded minibatch. Any noises that aﬀect the length of the
sequence are best applied before numericalization.

Word boundary noise. In a special case of token substitution, the substituted token is selected deterministically
as the token with a word boundary marker either added or removed. E.g. “kielinen” would be substituted by
“ kielinen” and vice versa. This might improve robustness to compounding mistakes such as “*suomen kielinen”
(Finnish speaker).

Transfer learning and subword sampling for asymmetric one-to-many NMT

9

Source

Target

Monolingual

Monolingual

Reorder

Drop

Segment

Segment

Segment

Segment

Taboo Segment

Target
language token

Target
language token

Target
language token

Length ﬁlter

Length ﬁlter

Length ﬁlter

Source

Target

Source

Target

Source

Target

(a)

(b)

(c)

Fig. 3 Transformations applied to data at training time. Steps with blue background are part of the stochastic noise model. Steps
with white background are the deterministic target language token preﬁxing and length ﬁltering. Length ﬁltering must be applied
after segmentation, which may make the sequence longer.

Taboo sampling. In addition to training the translation model, the idea of subword regularization (Kudo, 2018)
can be used in the autoencoder. Here, we propose taboo sampling as a special form of subword regularization for
monolingual data. The method takes a single word sequence as input, and outputs two diﬀerent segmentations
for it. The two segmentations consist of diﬀerent subwords, whenever possible. Only single character morphs are
allowed to be reused on the other side, to avoid failure if no alternative exists. E.g. “unreasonable” could be
segmented into “un ++reasonable” on the source side and “unreason ++able” on the target side. When converted
into numerical indices into the lexicon, these two representations are completely diﬀerent. The task aims to
teach the model to associate with each other the multiple ambiguous ways to segment a word, by using a
segmentation-invariant internal representation.

For each word, one segmentation is sampled in the usual way, after which another segmentation is sampled
using taboo sampling. During taboo sampling, all multi-character subwords used in the ﬁrst segmentation have
their emission probability temporarily set to zero. To avoid introducing a bias from having all the taboo sampled
segmentations on the same side, the sides are mixed by uniformly sampling a binary mask of the same length as
the sentence from the set of masks with half the bits 1. All words for which the mask bit is set have the source
and target segmentations swapped.

Proposed noise model combinations. Our proposed noise model combination is depicted in Figure 3. It consists of
three pipelines: The pipeline for parallel data (a) consists of only sampling segmentation. The primary pipeline
for monolingual data (b) is a concatenation of multiple noise models: local reordering, segmentation, and token
deletion. A secondary pipeline for monolingual data (c) uses taboo segmentation. In all cases the output consists
of a pair of source and target sequences.

Observe that the transformations are applied in the data loader at training time, not as an oﬀ-line preprocessing
stage. This allows the noise to be resampled for each parameter update, which is critical when training continues
for multiple epochs of a small dataset. As a minor downside, the NMT software needs to be modiﬁed to
accommodate the heavier data loader, while preprocessing generally requires no modiﬁcations to the software.

10

Stig-Arne Gr¨onroos et al.

4 Vocabulary construction

The vocabulary or lexicon of a translation model is the set of basic units or building blocks the text is decomposed
into. In phrase-based machine translation, the standard approach is to use a word lexicon. Segmentation into
subword units has been proposed mostly for morphologically rich languages, for which a word lexicon leads to
very high out-of-vocabulary (OOV) rates (Lee, 2004; Oﬂazer and El-Kahlout, 2007; Virpioja et al., 2007), and
character segmentation for closely related languages (Tiedemann, 2009). However, the change of paradigm to
neural machine translation has changed also the practice in vocabulary construction: With the exception of
unsupervised translation based on pretrained word embeddings (Artetxe et al., 2018; Yang et al., 2018), the
standard approach for models is segmentation into subword units (Sennrich et al., 2015). Some studies aim even
to the other extreme, characters (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016) or bytes (Costa-juss`a
et al., 2017).

A speciﬁc task in subword segmentation is the morphological surface segmentation. There the aim is to split
words into morphs, the surface forms of meaning-bearing sub-word units, morphemes. The concatenation of the
morphs is the word, for example

capability (cid:55)→ cap ++ abil ++ ity.

Unsupervised morphological segmentation, dating back to Harris (1955), was an active research topic in 2000s and
early 2010s (Goldsmith, 2001; Creutz and Lagus, 2007; Hammarstr¨om and Borin, 2011), and the methods have
been evaluated in various NLP applications (Kurimo et al., 2010; Virpioja et al., 2011). However, in applications
based on neural network models, such as NMT, the correspondence of the subwords to linguistic morphemes is
not of high importance, as the encoders are able to determine the meaning of the units in context. Therefore the
subword segmentation is typically tuned using other criteria, such as the size of subword lexicon or the frequency
distribution of the units. Desirable characteristics for a vocabulary to be used in multilingual NMT include:

1. high coverage of the training data, without imbalance between languages,
2. a tractable size for training, and
3. the right level of granularity for cross-lingual transfer.

Without a high coverage, some parts of the training data are impossible to represent using the vocabulary. The
unrepresentable parts may be replaced with a special “unknown” token. If the proportion of unknown tokens
increases, translation quality deteriorates. In a multilingual setting, a common approach is to use a shared
subword vocabulary between the multiple source or target languages. In this case, training the segmentation
model with a balanced data distribution is important to provide high coverage also for the less resourced
languages.

Vocabulary size aﬀects both the memory complexity via the number of network parameters and the
computational cost via the length of the sequences and the size of the softmax layer. When using large
vocabularies, e.g. words, the sequences are short, but vocabularies may grow intractably large, particularly for
morphologically complex languages. When using small vocabularies, e.g. characters, memory requirements are
low, but long sequences make training slow, particularly for recurrent networks.

The granularity of the segmentation aﬀects both coverage and size of the lexicon: ﬁner granularity typically
means better coverage and smaller lexicon size. However, within the reasonable limits set by the coverage and
size, it is much harder to determine the best possible level of granularity. Recent research (Cherry et al., 2018;
Kreutzer and Sokolov, 2018; Arivazhagan et al., 2019) indicates that smaller subwords are particularly useful for
cross-lingual transfer to low-resource languages in supervised settings. Exploiting similarity of related languages
by increasing the consistency of the segmentation between similar words of the source and target language
can also be useful (Gr¨onroos et al., 2018). In unsupervised NMT (Artetxe et al., 2018), cross-lingual transfer
requires basic units to be aligned between languages without use of parallel data. When starting with pretrained
embeddings, longer units are typically used, as they carry more meaning than short units. It is therefore an open
question how the optimal segmentation granularity varies with the amount of resources available.

Transfer learning and subword sampling for asymmetric one-to-many NMT

Next, we consider diﬀerent data-driven segmentation methods proposed for machine translation. This study
focuses on segmentation methods applying a unigram language model . In the unigram language model,
it is assumed that the morphs in a word occur independently of each other. Given the parameters θ of the
segmentation model, the probability of a sequence of morphs s decomposes into the product of the probabilities
of the morphs m of which it consists:

11

(1)

Pθ(s) =

Pθ(mi),

N
(cid:89)

i=1

4.1 Byte Pair Encoding

The most popular method for subword segmentation in the ﬁeld of NMT is currently the Byte Pair Encoding
(BPE) compression algorithm (Gage, 1994). The BPE algorithm iteratively replaces the most frequent pair of
bytes in the data with a single unused byte. In NMT, the algorithm is typically used on characters, and the
merging of characters is stopped when the given vocabulary size is reached (Sennrich et al., 2015). While BPE is
not a probabilistic model, the coding resembles unigram language models in that every subword mi is encoded
individually. As a bottom-up algorithm, BPE is reasonable to use in multilingual settings just by concatenating
the corpora before training; this approach is called joint segmentation (Sennrich et al., 2015). If the data is
balanced over the languages, the frequent words will be constructed in the early steps of the algorithm for all
languages.

4.2 SentencePiece

SentencePiece (Kudo, 2018; Kudo and Richardson, 2018) is another segmentation method proposed especially for
NMT. In contrast to BPE, it deﬁnes a proper statistical model for the unigram model in Equation 1, and tries
to ﬁnd the model parameters that maximize likelihood of the data given a constraint on the vocabulary size.

For training the model, SentencePiece applies the Expectation Maximization (EM) algorithm (Dempster
et al., 1977). The EM algorithm only updates the expected frequencies of the current units; it is not able to add
or remove subwords from the vocabulary. Thus to use EM for the segmentation problem, two other things are
needed: a seed lexicon and a pruning phase. The seed lexicon initializes the vocabulary with useful candidate
units, and pruning phase removes the least probable units from the model. Prior to SentencePiece, a similar
approach has been proposed by Varjokallio et al. (2013) for application in automatic speech recognition.

In SentencePiece, the seed lexicon is constructed from the most frequent substrings in the training data.
After initializing the seed lexicon, SentencePiece alternates between the EM phase and the pruning phases until
the desired vocabulary size is reached. In the pruning phase, the subwords are sorted by the reduction in the
likelihood function if the subword was removed. A certain proportion (e.g. 25%) of the multi-character subwords
are pruned at a time, followed by the next EM phase.

4.3 Morfessor EM+Prune

Morfessor is a family of generative models for unsupervised morphology induction (Creutz and Lagus, 2007).
Here, consider the Morfessor Baseline method (Creutz and Lagus, 2002; Virpioja et al., 2013) and its recent
Morfessor EM+Prune variant (Gr¨onroos et al., 2020).

12

4.3.1 Model and cost function

Stig-Arne Gr¨onroos et al.

Morfessor Baseline is applies the unigram language model (Equation 1). In contrast to SentencePiece, Morfessor
ﬁnds a point estimate for the model parameters ˆθ using Maximum a Posteriori (MAP) estimation. The MAP
estimate yields a two-part cost function, consisting of a prior (the lexicon cost) and likelihood (the corpus cost).
The Morfessor prior, inspired by the Minimum Description Length (MDL) principle (Rissanen, 1989), favors
lexicons containing fewer, shorter morphs.

For tuning the model, Kohonen et al. (2010) propose weighting the likelihood with a hyper-parameter α:

ˆθ = arg min

{− log

θ

prior
(cid:122) (cid:125)(cid:124) (cid:123)
P (θ) −α log

likelihood
(cid:123)
(cid:125)(cid:124)
(cid:122)
P (D | θ)}

(2)

This parameter controls the granularity of the segmentation. High values increase the weight of each emitted
morph in the corpus (less segmentation), and low values give a relatively larger weight to a small lexicon (more
segmentation).

Similar to SentencePiece, Morfessor can be used in subword regularization (Kudo, 2018). Alternative
segmentations can be sampled from the full data distribution using the forward-ﬁltering backward-sampling
algorithm (Scott, 2002) or approximatively from an n-best list.

4.3.2 Training algorithm

The original training algorithm of the Morfessor Baseline method, described in more detail by Creutz and Lagus
(2005) and Virpioja et al. (2013), is a local greedy search. The lexicon is initialized by whole words, and the
segmentation proceeds recursively top-down, ﬁnding an optimal segmentation into two parts for the current word
or subword unit. Our preliminary studies have indicated that this algorithm does not ﬁnd as good local optima
as the EM algorithm especially for the small lexicons useful in NMT. As a solution, we have developed a new
variant of the method called Morfessor EM+Prune (Gr¨onroos et al., 2020).1 It supports the MAP estimation and
MDL-based prior of the Baseline model, but implements a new training algorithm based on the EM algorithm
and lexicon pruning inspired by SentencePiece.

The training algorithm starts with a seed lexicon and alternates the EM and lexicon pruning steps similarly
to SentencePiece. The prior of the Morfessor model must be slightly modiﬁed for use with the EM algorithm,
but the standard prior is used during pruning. While SentencePiece aims for a predetermined lexicon size, in
Morfessor, the ﬁnal lexicon size is controlled by the hyper-parameter α (Equation 2). To reach a subword lexicon
of a predetermined size while using the prior, Morfessor EM+Prune implements an automatic tuning procedure.
When the estimated change in prior and likelihood are computed separately for each subword, the value of α
that gives exactly the desired size of lexicon after the pruning can be calculated.

In earlier work (Gr¨onroos et al., 2020), we have shown that the EM+Prune algorithm reduces search error
during training, resulting in models with lower costs for the optimization criterion. Moreover, lower costs lead
to improved accuracy when segmentation output is compared to linguistic morphological segmentation. In the
present study, we test it for the ﬁrst time in NMT.

5 Experiments

In the experiments, we study how to best exploit the additional monolingual and cross-lingual resources for
improving machine translation into low-resource morphologically rich languages. We compare various methods
for three major aspects aﬀecting the translation quality: using cross-lingual transfer, exploiting monolingual

1 Software available at https://github.com/Waino/morfessor-emprune.

Transfer learning and subword sampling for asymmetric one-to-many NMT

13

data and applying subword segmentation. The main focus lies on a noise model incorporating the subword
segmentation.

We target a one-to-many multilingual setting with related, morphologically rich languages on the target side.
The related languages include both high- and low-resource languages. This setting provides a good opportunity
for cross-lingual learning, as the amount of data is highly asymmetric. Our aim is not to achieve an interlingual
representation, so allowing the encoder to specialize for target languages is acceptable if it improves performance.

5.1 Data sets

We perform experiments on four tasks, each consisting of a language triple: source language (SRC), high-resource
target language (HRL) and low-resource target language (LRL). We only show SRC-LRL translation results, as
the goal is to improve this particular translation direction.

The four tasks (LRL in boldface) are:

1. English (eng) to Finnish (fin) and Estonian (est),
2. English to Czech (cze) and Slovak (slo),
3. English to Swedish (swe) and Danish (dan),
4. Norwegian bokm˚al (nob) to Finnish (fin) and North S´ami (sme).

In each task the two target languages are related. The target languages belong to three diﬀerent language
families: Germanic, Balto-Slavic and Uralic. All target languages are morphologically complex.

We use as parallel corpora Europarl (Koehn, 2005), and OpenSubtitles v2018 (Lison and Tiedemann, 2016),
when available. In addition, we use the eu, news, and subtitle domains of CzEng v1.7 (Bojar et al., 2016), and
the UiT freecorpus2. The corpora used for each language pair are shown in Table 2. The domains for the training
data are parliamentary debate, movie subtitles, news and web, with the exception of North S´ami which contains
a mix of many domains.

Our main source of monolingual data is WMT news text3. In addition, we use the following monolingual
corpora: skTenTen4 and Categorized News Corpus5 for Slovak, Riksdagens protokoll6 for Swedish, News 20127
for Danish, Aviskorpus8 for Norwegian, and Wikipedia9 for North S´ami.

Table 2 Parallel corpora.

Europarl OpenSubtitles Other parallel

eng
eng
eng
eng
eng
eng
nob
nob

cze
(cid:88)
slo
(cid:88)
fin
est
(cid:88)
swe (cid:88)
dan (cid:88)
fin
sme

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

CzEng

Rapid2016, Paracrawl

UiT freecorpus

2 https://victorio.uit.no/freecorpus/
3 http://www.statmt.org/wmt18/translation-task.html
4 http://hdl.handle.net/11858/00-097C-0000-0001-CCDB-0
5 Technical University of Kosice, 2014
6 https://spraakbanken.gu.se/eng/resource/rd-prot
7 http://hdl.handle.net/11022/0000-0000-2238-B
8 https://www.nb.no/sprakbanken/show?serial=oai%3Anb.no%3Asbr-4&lang=en
9 sewiki-20191201 dump

14

Stig-Arne Gr¨onroos et al.

For each of the low-resource languages, we select a subset of 18000 sentence pairs. For eng-est, we also
perform an experiment where the low-resource subset is repeatedly subsampled down to 3000 sentence pairs. To
avoid introducing a domain imbalance in the sampled subset, the pairs are sampled such that an equal number
of sentences are selected uniformly at random from each cleaned corpus. The training data sizes after cleaning
and subsampling are shown in Table 3.

Table 3 Data set sizes after cleaning.

Parallel

Monolingual

SRC HRL

LRL

SRC-HRL

SRC-LRL

BT

SRC

HRL

LRL

eng
eng
eng
nob

cze
fin
swe
fin

slo
est
dan
sme

24.7M
19.4M
11.5M
4.9M

(18k)
(18k)
(18k)
152k

1M 44.3M 13.6M 27.8M
3.6M
1M 44.3M
950k
181k

6.3M
44.3M 10.7M
6.3M
40.1M

750k
150k

As test sets we use the WMT newstest2018 (Bojar et al., 2018) for eng-est, the WMT test2011 extended
to Slovak by Galuˇsˇc´akov´a and Bojar (2012) for eng-slo. For eng-dan we use 2000 sentence pairs sampled
from the JRC-Acquis corpus (Steinberger et al., 2006). For nob-sme we use the Apertium story “Where is
James?”, a 48-sentence text with simple language, used as an initial development set for Apertium rule based
MT systems (Forcada et al., 2011).

5.2 Evaluation measures

When selecting the evaluation measures, the morphologically rich target languages must be taken into account.
Therefore, we use Character-F1 (Popovi´c, 2015) in addition to BLEU10 (Papineni et al., 2002). To evaluate the
performance of systems on rare words, we use word unigram F1 score computed over words occurring less than 5
times in the parallel training data (Sennrich et al., 2015).

5.3 Training details

Table 4 Speciﬁcations for the NMT system.

Encoder
Decoder
Hidden size
Filter size
Attention heads
Adam beta2
Warmup
Dropout weight

8 Transformer layers
8 Transformer layers
1024
4096
16
0.997
noam, 16000 steps
0.1

Label smoothing
Precision
Minibatch size
Gradient accumulation
Eﬀective minibatch size
Training time
Beam size
Heuristic penalties None

0.1
16-bit ﬂoating point
9200 subword tokens
4 minibatches
36800 subword tokens
100k steps
8

We use the Transformer NMT architecture (Vaswani et al., 2017). Model hyperparameters are shown in
Table 4. Training takes approximatively 96h on a single V100 GPU, with the data loader in a separate process.
When using scheduled multi-task learning, the mixing distribution is changed after 40k steps. In all experiments,

10 mteval-v13a.pl

Transfer learning and subword sampling for asymmetric one-to-many NMT

15

we apply full parameter sharing using a target language token. We tune our models towards the best product of
the three evaluation measures (charF1, BLEU, rare word F1) on a development set.

Back-translation was performed with essentially the same system, but with sources and targets swapped to

achieve a many-to-one conﬁguration. We mark the back-translation data as synthetic using a special token.

When using subword regularization or denoising autoencoder, the training data is not simply loaded from
disk, but new random segmentations and noises are sampled each time a training example is used. To alleviate
slowdown, we moved the dataloader and preprocessing pipeline into a separate process, which communicates the
numericalized and padded minibatches to the training process via a multiprocessing queue. Our data loader is
implemented as a fork of Open-NMT11 (Klein et al., 2017).

With multilingual training, autoencoders and back-translation, our setting involves a large number of diﬀerent
tasks. The tasks can be divided by language (HRL, LRL) and by type (translation, autoencoder). Nearly all
runs, with the exception of our vanilla baseline, use a mix of tasks in some or all phases.

5.4 Results

In this section, we present the results of ten experiments, each exploring a separate aspect of asymmetric-resource
one-to-many NMT. We have detailed results for English–Estonian, and verify the central ﬁndings on two
additional language triples. Finally, we present some results on the actual low-resource pair Norwegian–North
S´ami.

Unless otherwise stated, the compared models are trained using joint Morfessor EM+Prune segmentation
with 16k subword vocabulary, cross-lingual scheduled multi-task learning, autoencoder with full noise model,
and subword regularization for the translation task. Our initial results are using autoencoder tasks for all three
languages (SRC+HRL+LRL). Later some of the results were rerun with the better SRC+LRL conﬁguration,
which omits the high-resource target language autoencoder.

5.4.1 Subword segmentation

For subword segmentation, we compare Morfessor EM+Prune to SentencePiece on various vocabulary sizes.
The results are shown in Figure 4. There is no clear optimal vocabulary size: in particular for the Character
F1 measure the performance remains nearly constant. On the test set, Morfessor EM+Prune is +0.6 BLEU
better than SentencePiece. The diﬀerence is smaller than the +1.48 BLEU diﬀerence on the development set,
but consistent. The diﬀerence between Morfessor EM+Prune and SentencePiece is similar for the eng-dan and
eng-slo translation directions. In preliminary experiments BPE gave 0.65 BLEU worse results than EM+Prune
already without subword regularization. We decided against further experiments using BPE, as it is incompatible
with subword regularization.

5.4.2 Cross-lingual transfer

Table 5 shows the eﬀect of multilingual training, with and without the autoencoder task. The cross-lingual transfer
from the high-resource language yields the largest single improvement in our experiments. The multilingual
model without autoencoder performs between +10.26 and +12.7 BLEU better than the vanilla model using only
LRL parallel data. Adding an autoencoder loss results in a smaller gain, between +4.97 and +5.55 BLEU. The
gains are partly cumulative for an additional gain of +0.05 to +1.14 BLEU.

The results for the vanilla model use a smaller conﬁguration, with 4 encoder and 4 decoder layers, and batch
size reduced to 2048. For the vanilla model the small network performed better than the large one, but when
adding either multilingual training or autoencoder, the large network is superior.

11 Software available at https://github.com/Waino/OpenNMT-py/tree/dynamicdata

16

Stig-Arne Gr¨onroos et al.

Fig. 4 Varying the subword vocabulary. Multilingual models, with SRC+HRL+LRL autoencoder and full noise model, except for
BPE which are multilingual models without autoencoder or noise. Results on English→Estonian newsdev2018.

Table 5 Results for cross-lingual transfer. Abbreviations: ML for multilingual, BT for back-translation, AE for autoencoder.

Autoencoder

eng–est

eng–dan

eng–slo

Method

ML BT SRC HRL LRL chrF1 BLEU rare chrF1 BLEU rare chrF1 BLEU rare

(cid:88)
(cid:88)

Both
Only ML
Only AE
Neither (vanilla)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

51.71
50.09
42.65
29.46

14.04 34.79 50.06
12.90 33.20 49.57
8.19 21.59 42.26
6.22 31.95
2.64

13.92 54.58 50.19
13.13 54.21 49.83
7.60 44.48 38.97
2.63 30.40 23.76

14.02 69.94
13.97 68.79
6.25 62.51
1.27 36.80

5.4.3 Scheduled multi-task learning

Figure 5 shows the learning curves on the development set and Table 6 the evaluations on the test set for diﬀerent
conﬁgurations of transfer learning.

Multi-task without schedule is trained with a constant task mixing distribution. The result marked HRL
pretraining, LRL ﬁne-tuning uses a mix of HRL translation and autoencoder tasks for pretraining, and only
a single task—LRL translation—for ﬁne-tuning, and is thus fully sequential in terms of languages. It quickly
overﬁts in the ﬁne-tuning phase.

The models using scheduled multi-task learning combine sequential and parallel transfer. In 2-phase scheduled
multi-task, LRL tasks are not used in the pretraining phase, but a mix of tasks is used for ﬁne-tuning. It gives
a beneﬁt of +2.4 BLEU compared to the model ﬁne-tuning on only LRL tasks, and +1.77 BLEU compared
to training with a constant mixing distribution. The 3-phase scheduled multi-task adds a third phase training
mostly on LRL tasks. A small proportion of HRL translation is included to delay overﬁtting. The model again
overﬁts in the ﬁnal phase, but does reach a higher score before doing so. The 3-phase task mixing schedule is
shown in Figure 6.

Torrey and Shavlik (2009) describe three ways in which transfer learning can beneﬁt training: 1) higher
performance at the very beginning of learning, 2) steeper learning curve, and 3) higher asymptotic performance.
When pretraining the encoder and decoder on source and target autoencoder tasks respectively, we see the ﬁrst
of these, but not the other two: NMT training at ﬁrst improves faster than with random initialization, but
converges to a worse ﬁnal model. However, we have not tested pretraining on a next token prediction or masked
language modeling task.

Transfer learning and subword sampling for asymmetric one-to-many NMT

17

3-phase scheduled multi-task, SRC+HRL+LRL AE

SRC–HRL
SRC–LRL
SRC AE
HRL AE
LRL AE

92

0

5

3

0

67

22

0

0

11

20

70

0

0

10

Pretraining

Phase 2

time

Phase 3

Fig. 5 Learning curves on LRL English→Estonian develop-
ment set. Multilingual models, with SRC+HRL+LRL autoen-
coder and full noise model. Note that up to 40k training steps,
the model using scheduled multi-task learning has not seen
any LRL data.

Fig. 6 The task mix schedule used in the 3-phase scheduled
multi-task learning experiment. The 2-phase schedule is the
same, except it omits the third phase, continuing the second
phase until the end of training.

Table 6 Results for scheduled multi-task learning. English-Estonian.

Autoencoder

eng–est

Method

ML BT SRC HRL

LRL

chrF-1.0 BLEU

rare

(cid:88)
3-phase scheduled multi-task
(cid:88)
2-phase scheduled multi-task
(cid:88)
Multi-task w/o schedule
HRL pretraining, LRL ﬁne-tuning (cid:88)
(cid:88)
Subnetwork pretraining

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

51.71
51.42
48.62
48.15
47.74

13.94
13.75
11.98
11.35
11.17

33.96
33.83
29.16
29.88
26.93

5.4.4 Dataset augmentation – Subword regularization

Table 7 shows an improvement between +0.08 and +0.55 BLEU from using subword regularization as the only
noise model, without the use of an autoencoder.

Table 7 Results with subword regularization (SWR).

Autoencoder

eng–est

eng–dan

eng–slo

Method ML BT SRC HRL LRL chrF1 BLEU rare chrF1 BLEU rare chrF1 BLEU rare

(cid:88)
SWR
no SWR (cid:88)

50.09
49.77

12.90 33.20 49.57
12.57 31.14 49.27

13.13 54.21 49.83
13.05 53.66 49.27

13.97 68.79
13.42 69.07

18

Stig-Arne Gr¨onroos et al.

5.4.5 Dataset augmentation – Autoencoder

Table 8 shows an ablation experiment for the noise model. When compared against only using the subword
regularization, the additional noises give between +0.2 and +0.5 BLEU. All parts of the noise model are
individually ablated: the most important is local reordering, which when omitted causes a decrease of -0.36
BLEU. The full noise model includes subword regularization. When subword regularization is ablated, we turn it
entirely oﬀ, both for the parallel data and the autoencoder. Word boundary noise, taboo sampling, and insertions
are not included in our full noise model, as they did not show a beneﬁt on the development set. However, word
boundary noise gives +0.2 BLEU and taboo sampling +0.09 BLEU on the test set.

Table 8 Ablation results for noise model. Ordered by decreasing BLEU.

Autoencoder

eng–est

Method

ML BT SRC HRL

LRL

chrF-1.0 BLEU

rare

+ Word boundary noise (cid:88)
(cid:88)
+ Taboo sampling
(cid:88)
No drop
(cid:88)
Full noise
(cid:88)
+ Insertion
(cid:88)
Only switchout
(cid:88)
No SWR
(cid:88)
Only SWR
(cid:88)
No reorder

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

51.56
51.23
51.48
51.42
50.88
50.78
50.71
50.96
50.90

13.95
13.84
13.79
13.75
13.74
13.49
13.46
13.43
13.39

33.20
33.81
33.89
33.83
33.51
32.21
32.18
32.85
33.03

We also consider for which languages an autoencoder task should be added. Table 9 shows variants starting
from no autoencoder, adding autoencoders one by one ﬁrst for the low-resource target language, then for the
source language and ﬁnally for the high-resource target language. The best combination uses source and LRL,
with the SRC autoencoder giving a gain of +0.11 BLEU over only using the LRL. The HRL autoencoder is
detrimental, and leaving it out gives +0.29 BLEU.

Table 9 Autoencoder language tasks.

Autoencoder

eng–est

Method

ML BT SRC HRL

LRL

chrF-1.0 BLEU

rare

(cid:88)
SRC+LRL AE
(cid:88)
LRL AE
SRC+HRL+LRL AE (cid:88)
(cid:88)
No AE

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

51.71
51.41
51.42
50.09

14.04
13.93
13.75
12.90

34.79
33.57
33.83
33.20

5.4.6 Dataset augmentation – Back-translation

Table 10 shows the improvements gained using back-translated synthetic data. We weight the natural and
synthetic LRL data equally. Back-translation is generally eﬀective, giving a beneﬁt between +1.31 and +4.46
BLEU. When using back-translated data, the autoencoder task is less eﬀective, with small improvements to
Character F1 but inconsistent results for the other measures. Note that back-translation is not a silver bullet.
When using only back-translation, i.e. back-translating the data with a weak model trained only on the low-
resource parallel data, and then training a forward model augmented only by this low-quality back-translation
but not multilingual training or autoencoder, the performance is very low: only +2.87 BLEU better than the

Transfer learning and subword sampling for asymmetric one-to-many NMT

19

Fig. 7 Varying the amount of low-resource data. Multilingual models, with SRC+HRL+LRL autoencoder and full noise model.
Results on English→Estonian newstest2018.

vanilla model without back-translation. The high-quality back-translation together with multilingual training
gives an +12.7 BLEU increase over the vanilla back-translation.

Table 10 Results using back-translation. (cid:88)† indicates the use of a low-quality back-translation made with a non-multilingual
non-autoencoder vanilla BT model.

Autoencoder

eng–est

eng–dan

eng–slo

Method

ML BT SRC HRL LRL chrF1 BLEU rare chrF1 BLEU rare chrF1 BLEU rare

(cid:88) (cid:88) (cid:88)

Full BT
No AE, full BT (cid:88) (cid:88)
AE, no BT
Vanilla BT

(cid:88)

(cid:88)†

(cid:88)

(cid:88)

(cid:88)

56.45
56.33
51.71
36.12

18.05 41.13 51.27
18.15 40.85 51.20
14.04 34.79 50.06

14.80 56.63 52.80
15.00 57.39 52.65
13.92 54.58 50.19

16.87 70.97
16.63 70.82
14.02 69.94

5.51 13.25

5.4.7 Amount of low-resource language data

Figure 7 shows how the performance degrades when the low-resource parallel data is reduced. Each set is
subsampled from the previous larger set. All models use multilingual training with scheduled multi-task learning,
and SRC+HRL+LRL autoencoders. Down to 10k parallel sentences the performance stays reasonable, after
which it rapidly deteriorates.

Also plotted is a 10k sentence pair baseline by Kocmi and Bojar (2018), reaching 12.46 BLEU in a similar

setting on the same test set. Our result at 10k is 13.04 BLEU, or +0.68.

5.4.8 Relatedness of the target languages

Table 11 shows the results of using an unrelated but larger HRL (Czech). The results favor transfer from the
related HRL (Finnish), by +0.92 BLEU. The diﬀerence in favor of the related HRL is largest for the rare words.
Previously, Zoph et al. (2016) and Dabre et al. (2017) ﬁnd that related parent languages result in better
transfer. However, Kocmi and Bojar (2018) ﬁnd in the case of Estonian that a bigger parent (Czech) gave better

20

Stig-Arne Gr¨onroos et al.

Table 11 HRL language relatedness.

Method

BT SRC HRL

LRL

chrF-1.0 BLEU

rare

Autoencoder

eng–est

Within family
Cross family

ML

fin
cze

(cid:88)
(cid:88)

(cid:88)
(cid:88)

51.71
50.20

14.04
13.12

34.79
30.69

results than a more related parent (Finnish). Our results contradict Kocmi and Bojar (2018) and agree with the
prior literature.

5.4.9 Norwegian bokm˚al → Finnish + North S´ami

We apply the ﬁndings of the previous experiments to the low-resource pair Norwegian bokm˚al to North S´ami.
We use a larger task mix weight for the LRL task (40 SRC-HRL / 30 SRC-LRL / 30 BT) to account for the
larger LRL parallel data. Table 12 shows the results to be similar to the results of the other languages, with
beneﬁt from multilingual training, autoencoder task and back-translation.

Table 12 Results on Norwegian Bokm˚al–North S´ami Apertium story.

Autoencoder

nob–sme

Method

ML BT SRC HRL

LRL

chrF-1.0 BLEU

rare

ML, AE, BT (cid:88)
(cid:88)
ML, AE
Vanilla

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

57.27
54.86
45.97

24.40
21.07
15.64

35.62
21.54
21.05

5.5 Discussion

In our experiments for four asymmetric-resource one-to-many translation tasks, we ﬁnd that the largest gains
come from cross-lingual transfer (up to +12.7 BLEU), back-translation (up to +4.46 BLEU), and scheduled
multi-task learning (up to +2.4 BLEU). To sum up our ﬁndings related to the questions asked in the introduction:
On cross-lingual transfer, we ﬁnd that applying scheduled multi-task learning, in which the model is ﬁrst
pretrained only on high-resource tasks and ﬁne-tuned using a mix of tasks, is superior to both fully sequential
and fully parallel transfer. A second ﬁne-tuning phase only on the low-resource tasks is prone to overﬁtting.

On exploiting mono-lingual data, a low-resource target-language autoencoder is beneﬁcial, even when using
multi-lingual training, but inconclusive together with back-translation. A source-language autoencoder is also
helpful, to a lesser degree, but a high-resource target autoencoder is not. A noise model including subword
regularization, reordering, and deletion is beneﬁcial. The results for substitutions and the proposed taboo
sampling method are inconclusive.

On vocabulary construction, Morfessor EM+Prune is superior to SentencePiece in this translation setting,
for a gain of +0.6 BLEU. As the methods use the same training algorithm, it indicates that the prior used in
Morfessor is beneﬁcial in ﬁnding eﬃcient subword lexicons. The vocabulary size has less eﬀect (up to 0.5 BLEU
for sizes between 8k and 20k) on the results. Subword lexicon size has been considered an important parameter
to tune (Sennrich and Zhang, 2019; Salesky et al., 2020). Also our preliminary experiments of low-resource NMT
without subword regularization suggested a more substantial eﬀect for the lexicon size. It seems that the subword
sampling procedure (and perhaps the autoencoder task) lessens the impact of the subword vocabulary size.

Transfer learning and subword sampling for asymmetric one-to-many NMT

21

Regarding available data and languages, larger low-resource parallel data give better results, but diminishing
returns are already reached after 10k sentences. We ﬁnd language relatedness to be more important than parent
language size in highly asymmetrical transfer.

Among the translation tasks, we get the lowest scores in the English–Danish translation. While Danish has
the smallest LRL monolingual corpus, as the same order is observed also for the models not using monolingual
data, the reason must lie elsewhere, possibly in the diﬃculty of the JRC-Acquis corpus. The autoencoder
task has the largest beneﬁt for English–Estonian. In the Norwegian–North S´ami experiment the size of the
low-resource parallel data is an order of magnitude larger than in the other experiments, but the results
remain similar. Due to the small size of the test set, we include the entire translation output in Ancillary File
translated.apertium.story.txt.

Sennrich and Zhang (2019) ﬁnd that smaller models and batch sizes work better in low-resource settings.
We ﬁnd that large models are better whenever auxiliary multilingual or monolingual data is used. While in the
vanilla setting, the smaller model is better, it still falls far behind the models using additional data.

The three evaluation measures—BLEU, Character F1, and rare words F1—generally agree. Some exceptions
include ablation of the subword regularization and using SwitchOut as the sole noise model, which hurt in
particular the rare words more than BLEU. Turning oﬀ the autoencoder has the least eﬀect on rare words, even
giving a slight improvement for eng–dan when using back-translation.

Our results again underscore the need to gather parallel data for low-resource language pairs. This may be
possible to accomplish at reasonable cost, as 10000 sentence pairs already goes a long way. Monolingual corpora
of high quality and quantity are also of great importance as auxiliary data for MT.

6 Conclusion

When training a neural translation model for low-resource languages with limited parallel training data, it
is important to make use of eﬃcient methods for cross-lingual learning, data augmentation, and subword
segmentation. Our experiments in asymmetric-resourced one-to-many translation show that while the largest
individual improvements come from any cross-lingual transfer learning and augmenting the training data with
back-translation, considerable beneﬁts are gained also by less common approaches: scheduled multi-task learning,
subword regularization, and denoising autoencoder with multiple noise models. For this reason, we strongly
recommend that NMT frameworks should include a dataloader with the ability to (a) sample noisy minibatches
for training and (b) use a schedule for controlling the mixing of diﬀerent tasks. Subword sampling requires
a probabilistic segmentation model such as SentencePiece or Morfessor, making them preferable to the more
common BPE method. Both our data loader implementation for the Open-NMT system and the Morfessor
EM+Prune software are available with non-restrictive licenses.

Acknowledgements This study has been supported by the MeMAD project, funded by the European Union’s Horizon 2020
research and innovation programme (grant agreement No 780069), and the FoTran project, funded by the European Research
Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 771113).
Computer resources within the Aalto University School of Science “Science-IT” project were used.

References

Arivazhagan N, Bapna A, Firat O, Lepikhin D, Johnson M, Krikun M, Chen MX, Cao Y, Foster G, Cherry C,
Macherey W, Chen Z, Wu Y (2019) Massively multilingual neural machine translation in the wild: Findings
and challenges. CoRR 1907.05019

Artetxe M, Labaka G, Agirre E, Cho K (2018) Unsupervised neural machine translation. In: 6th International
Conference on Learning Representations, ICLR 2018, URL http://arxiv.org/abs/1710.11041, arXiv:
1710.11041

22

Stig-Arne Gr¨onroos et al.

Belinkov Y, Bisk Y (2017) Synthetic and natural noise both break neural machine translation. CoRR 1711.02173
Blackwood G, Ballesteros M, Ward T (2018) Multilingual neural machine translation with task-speciﬁc attention.

arXiv preprint arXiv:180603280

Bojar O, Duˇsek O, Kocmi T, Libovick´y J, Nov´ak M, Popel M, Sudarikov R, Variˇs D (2016) CzEng 1.6: Enlarged
Czech-English Parallel Corpus with Processing Tools Dockered. In: Sojka P, Hor´ak A, Kopeˇcek I, Pala K
(eds) Text, Speech, and Dialogue: 19th International Conference, TSD 2016, Masaryk University, Springer
International Publishing, Cham / Heidelberg / New York / Dordrecht / London, no. 9924 in Lecture Notes in
Artiﬁcial Intelligence, pp 231–238

Bojar O, Federmann C, Fishel M, Graham Y, Haddow B, Huck M, Koehn P, Monz C (2018) Findings of the 2018
conference on machine translation (wmt18). In: Proceedings of the Third Conference on Machine Translation,
Volume 2: Shared Task Papers, Association for Computational Linguistics, Belgium, Brussels, pp 272–307,
URL http://www.aclweb.org/anthology/W18-6401

Bourlard H, Kamp Y (1988) Auto-association by multilayer perceptrons and singular value decomposition.

Biological cybernetics 59(4-5):291–294

Caruana R (1997) Multi-task learning. PhD thesis
Caswell I, Chelba C, Grangier D (2019) Tagged back-translation. In: Proceedings of the Fourth Conference on

Machine Translation (Volume 1: Research Papers), pp 53–63

Chen Z, Badrinarayanan V, Lee CY, Rabinovich A (2018) Gradnorm: Gradient normalization for adaptive loss

balancing in deep multitask networks. In: International Conference on Machine Learning, pp 794–803

Cheng Y, Xu W, He Z, He W, Wu H, Sun M, Liu Y (2016) Semi-supervised learning for neural machine

translation. arXiv:160604596 [cs] URL http://arxiv.org/abs/1606.04596

Cherry C, Foster G, Bapna A, Firat O, Macherey W (2018) Revisiting character-based neural machine translation
with capacity and compression. In: Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, Association for Computational Linguistics, Brussels, Belgium, pp 4295–4305, DOI
10.18653/v1/D18-1461, URL https://www.aclweb.org/anthology/D18-1461

Chu C, Dabre R, Kurohashi S (2017) An empirical comparison of domain adaptation methods for neural
machine translation. In: Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), Association for Computational Linguistics, Vancouver, Canada, pp
385–391, DOI 10.18653/v1/P17-2061, URL https://www.aclweb.org/anthology/P17-2061

Chung J, Cho K, Bengio Y (2016) A character-level decoder without explicit segmentation for neural machine

translation. arXiv:160306147 [cs] URL http://arxiv.org/abs/1603.06147, arXiv: 1603.06147

Costa-juss`a MR, Fonollosa JAR (2016) Character-based neural machine translation. In: Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Association
for Computational Linguistics, Berlin, Germany, pp 357–361, DOI 10.18653/v1/P16-2058, URL https:
//www.aclweb.org/anthology/P16-2058

Costa-juss`a MR, Escolano C, Fonollosa JAR (2017) Byte-based neural machine translation. In: Proceedings
of the First Workshop on Subword and Character Level Models in NLP, Association for Computational
Linguistics, Copenhagen, Denmark, pp 154–158, DOI 10.18653/v1/W17-4123, URL https://www.aclweb.
org/anthology/W17-4123

Creutz M, Lagus K (2002) Unsupervised discovery of morphemes. In: ACL-02 Workshop on Morphological and
Phonological Learning, Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, MPL
’02, vol 6, pp 21–30, DOI 10.3115/1118647.1118650, URL http://portal.acm.org/citation.cfm?doid=
1118647.1118650

Creutz M, Lagus K (2005) Unsupervised morpheme segmentation and morphology induction from text corpora
using morfessor 1.0. Technical Report A81, Publications in Computer and Information Science, Publications
in Computer and Information Science, Helsinki University of Technology

Creutz M, Lagus K (2007) Unsupervised models for morpheme segmentation and morphology learning. ACM

Transactions on Speech and Language Processing 4(1)

Currey A, Miceli-Barone AV, Heaﬁeld K (2017) Copied monolingual data improves low-resource neural machine

Transfer learning and subword sampling for asymmetric one-to-many NMT

23

translation. In: Proceedings of the Second Conference on Machine Translation, pp 148–156

Dabre R, Nakagawa T, Kazawa H (2017) An empirical study of language relatedness for transfer learning in
neural machine translation. In: Proceedings of the 31st Paciﬁc Asia Conference on Language, Information and
Computation, pp 282–286

Dabre R, Chu C, Kunchukuttan A (2020) A comprehensive survey of multilingual neural machine translation.

Dai AM, Le QV (2015) Semi-supervised sequence learning. In: Advances in neural information processing systems,

2001.01115

pp 3079–3087

Dempster AP, Laird NM, Rubin DB (1977) Maximum likelihood from incomplete data via the EM algorithm.

Journal of the Royal Statistical Society, Series B (Methodological) 39(1):1–38

Domhan T, Hieber F (2017) Using target-side monolingual data for neural machine translation through multi-task
learning. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp
1500–1505

Edunov S, Ott M, Auli M, Grangier D (2018) Understanding back-translation at scale. In: Proceedings of the

2018 Conference on Empirical Methods in Natural Language Processing, pp 489–500

Firat O, Cho K, Bengio Y (2016) Multi-way, multilingual neural machine translation with a shared attention

mechanism. arXiv:160101073 [cs, stat] URL http://arxiv.org/abs/1601.01073, arXiv: 1601.01073

Forcada ML, Ginest´ı-Rosell M, Nordfalk J, O’Regan J, Ortiz-Rojas S, P´erez-Ortiz JA, S´anchez-Mart´ınez F,
Ram´ırez-S´anchez G, Tyers FM (2011) Apertium: a free/open-source platform for rule-based machine translation.
Machine translation 25(2):127–144

Gage P (1994) A new algorithm for data compression. The C Users Journal 12(2):23–38
Galuˇsˇc´akov´a P, Bojar O (2012) WMT 2011 testing set. URL http://hdl.handle.net/11858/
00-097C-0000-0006-AADA-9, LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Ap-
plied Linguistics ( ´UFAL), Faculty of Mathematics and Physics, Charles University

Goldsmith J (2001) Unsupervised learning of the morphology of a natural language. Computational Linguistics
27(2):153–198, DOI 10.1162/089120101750300490, URL http://www.mitpressjournals.org/doi/10.1162/
089120101750300490

Goodfellow IJ, Mirza M, Xiao D, Courville A, Bengio Y (2013) An empirical investigation of catastrophic

forgetting in gradient-based neural networks. arXiv preprint arXiv:13126211

Gra¸ca M, Kim Y, Schamper J, Khadivi S, Ney H (2019) Generalizing back-translation in neural machine

translation. arXiv preprint arXiv:190607286

Gr¨onroos SA, Virpioja S, Kurimo M (2018) Cognate-aware morphological segmentation for multilingual neural
translation. In: Proceedings of the Third Conference on Machine Translation, Association for Computational
Linguistics, Brussels, Belgium

Gr¨onroos SA, Virpioja S, Kurimo M (2020) Morfessor EM+Prune: Improved subword segmentation with
expectation maximization and pruning. In: Proceedings of the 12th Language Resources and Evaluation
Conference, ELRA, Marseilles, France, to appear

Gu J, Wang Y, Chen Y, Cho K, Li VOK (2018) Meta-learning for low-resource neural machine translation.

arXiv:180808437 [cs] URL http://arxiv.org/abs/1808.08437, arXiv: 1808.08437

Gulcehre C, Firat O, Xu K, Cho K, Barrault L, Lin HC, Bougares F, Schwenk H, Bengio Y (2015) On using
monolingual corpora in neural machine translation. arXiv:150303535 [cs] URL http://arxiv.org/abs/1503.
03535, arXiv: 1503.03535

Hammarstr¨om H, Borin L (2011) Unsupervised learning of morphology. Computational Linguistics 37(2):309–350,

DOI 10.1162/COLI a 00050, URL http://www.mitpressjournals.org/doi/10.1162/COLI_a_00050

Harris ZS (1955) From phoneme to morpheme. Language 31(2):190–222
Iyyer M, Manjunatha V, Boyd-Graber J, Daum´e III H (2015) Deep unordered composition rivals syntactic
methods for text classiﬁcation. In: Proceedings of the 53rd Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pp 1681–1691

24

Stig-Arne Gr¨onroos et al.

Johnson M, Schuster M, Le QV, Krikun M, Wu Y, Chen Z, Thorat N, Vi´egas F, Wattenberg M, Corrado
G, Hughes M, Dean J (2016) Google’s multilingual neural machine translation system: Enabling zero-shot
translation. arXiv:161104558 [cs] URL http://arxiv.org/abs/1611.04558, arXiv: 1611.04558

Kalchbrenner N, Blunsom P (2013) Recurrent continuous translation models. In: Proceedings of the 2013

Conference on Empirical Methods in Natural Language Processing, pp 1700–1709

Karakanta A, Dehdari J, van Genabith J (2018) Neural machine translation for low-resource languages without

parallel corpora. Machine Translation 32(1-2):167–189

Kiperwasser E, Ballesteros M (2018) Scheduled multi-task learning: From syntax to translation. Transactions of

the Association for Computational Linguistics 6:225–240

Klein G, Kim Y, Deng Y, Senellart J, Rush AM (2017) Opennmt: Open-source toolkit for neural machine
translation. In: Proc. ACL, DOI 10.18653/v1/P17-4012, URL http://arxiv.org/abs/1701.02810, arXiv:
1701.02810

Kocmi T (2019) Exploring beneﬁts of transfer learning in neural machine translation. PhD Thesis, Charles

University

Kocmi T, Bojar O (2018) Trivial transfer learning for low-resource neural machine translation. In: Proceedings

of the Third Conference on Machine Translation: Research Papers, pp 244–252

Koehn P (2005) Europarl: A parallel corpus for statistical machine translation. In: MT summit, vol 5, pp 79–86
Kohonen O, Virpioja S, Lagus K (2010) Semi-supervised learning of concatenative morphology. In: Proceedings
of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology,
Association for Computational Linguistics, Uppsala, Sweden, pp 78–86, URL http://www.aclweb.org/
anthology/W10-2210

Koponen M, Salmi L, Nikulin M (2019) A product and process analysis of post-editor corrections on neural,

statistical and rule-based machine translation output. Machine Translation 33(1-2):61–90

Kreutzer J, Sokolov A (2018) Learning to segment inputs for nmt favors character-level processing. arXiv preprint

arXiv:181001480

Kudo T (2018) Subword regularization: Improving neural network translation models with multiple subword

candidates. arXiv:180410959 [cs] URL http://arxiv.org/abs/1804.10959, arXiv: 1804.10959

Kudo T, Richardson J (2018) SentencePiece: A simple and language independent subword tokenizer and
detokenizer for neural text processing. In: Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations, Association for Computational Linguistics, Brussels, Belgium,
pp 66–71, DOI 10.18653/v1/D18-2012, URL https://www.aclweb.org/anthology/D18-2012

Kurimo M, Virpioja S, Turunen V, Lagus K (2010) Morpho challenge 2005-2010: Evaluations and results. In:
Heinz J, Cahill L, Wicentowski R (eds) Proceedings of the 11th Meeting of the ACL Special Interest Group
on Computational Morphology and Phonology, Association for Computational Linguistics, Uppsala, Sweden,
pp 87–95

Lample G, Conneau A (2019) Cross-lingual language model pretraining. arXiv preprint arXiv:190107291
Lample G, Conneau A, Denoyer L, Ranzato M (2017) Unsupervised machine translation using monolingual

corpora only. arXiv:171100043 [cs] URL http://arxiv.org/abs/1711.00043, arXiv: 1711.00043

Lample G, Ott M, Conneau A, Denoyer L, Ranzato M (2018) Phrase-based & neural unsupervised machine

translation. arXiv preprint arXiv:180407755

Lee YS (2004) Morphological analysis for statistical machine translation. In: Proceedings of HLT-NAACL 2004:

Short Papers, Association for Computational Linguistics, pp 57–60

Lison P, Tiedemann J (2016) OpenSubtitles2016: Extracting large parallel corpora from movie and tv subtitles.
In: Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016),
European Language Resources Association

Luong MT (2016) neural machine translation. PhD Thesis, Stanford University
Luong MT, Le QV, Sutskever I, Vinyals O, Kaiser L (2015) Multi-task sequence to sequence learning. In: ICLR,

URL http://arxiv.org/abs/1511.06114, arXiv: 1511.06114

McCloskey M, Cohen NJ (1989) Catastrophic interference in connectionist networks: The sequential learning

Transfer learning and subword sampling for asymmetric one-to-many NMT

25

problem. In: Psychology of learning and motivation, vol 24, Elsevier, pp 109–165

Oﬂazer K, El-Kahlout ID (2007) Exploring diﬀerent representational units in english-to-turkish statistical
machine translation. In: Proceedings of the second workshop on statistical machine translation, Association
for Computational Linguistics, pp 25–32, DOI 10.3115/1626355.1626359, URL http://portal.acm.org/
citation.cfm?doid=1626355.1626359

Papineni K, Roukos S, Ward T, Zhu WJ (2002) Bleu: a method for automatic evaluation of machine translation.
In: 40th annual meeting of the association for computational linguistics, Association for Computational
Linguistics, Philadelphia, PA, USA, pp 311–318, DOI 10.3115/1073083.1073135, URL http://portal.acm.
org/citation.cfm?doid=1073083.1073135

Platanios EA, Sachan M, Neubig G, Mitchell T (2018) Contextual parameter generation for universal neural
machine translation. arXiv:180808493 [cs, stat] URL http://arxiv.org/abs/1808.08493, arXiv: 1808.08493
Popovi´c M (2015) chrf: character n-gram f-score for automatic mt evaluation. In: WMT15, Association for
Computational Linguistics, pp 392–395, DOI 10.18653/v1/W15-3049, URL http://aclweb.org/anthology/
W15-3049

Ramachandran P, Liu PJ, Le Q (2017) Unsupervised pretraining for sequence to sequence learning. In: Proceedings

of the 2017 Conference on Empirical Methods in Natural Language Processing, pp 383–391

Rissanen J (1989) Stochastic Complexity in Statistical Inquiry, vol 15. World Scientiﬁc Series in Computer

Science, Singapore

arXiv preprint arXiv:180900252

Sachan DS, Neubig G (2018) Parameter sharing methods for multilingual self-attentional translation models.

Salesky E, Runge A, Coda A, Niehues J, Neubig G (2020) Optimizing segmentation granularity for neural

machine translation. Machine Translation pp 1–19

Scott SL (2002) Bayesian methods for hidden markov models: Recursive computing in the 21st century. Journal

of the American Statistical Association 97(457):337–351

Sennrich R, Zhang B (2019) Revisiting low-resource neural machine translation: A case study. In: Proceedings of

the 57th Annual Meeting of the Association for Computational Linguistics, pp 211–221

Sennrich R, Haddow B, Birch A (2015) Neural machine translation of rare words with subword units. In: ACL16,

URL http://arxiv.org/abs/1508.07909, arXiv: 1508.07909

Sennrich R, Haddow B, Birch A (2016) Improving neural machine translation models with monolingual data. In:
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp 86–96

Skorokhodov I, Rykachevskiy A, Emelyanenko D, Slotin S, Ponkratov A (2018) Semi-supervised neural machine
translation with language models. In: Proceedings of the AMTA 2018 Workshop on Technologies for MT of
Low Resource Languages (LoResMT 2018), pp 37–44

Sriram A, Jun H, Satheesh S, Coates A (2017) Cold fusion: Training seq2seq models together with language

models. arXiv preprint arXiv:170806426

Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014) Dropout: a simple way to prevent

neural networks from overﬁtting. The Journal of Machine Learning Research 15(1):1929–1958

Steinberger R, Pouliquen B, Widiger A, Ignat C, Erjavec T, Tuﬁ¸s D, Varga D (2006) The JRC-Acquis: A
multilingual aligned parallel corpus with 20+ languages. In: Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC’2006), Genoa, Italy

Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence learning with neural networks. In: Advances in

neural information processing systems, pp 3104–3112

Szegedy C, Vanhoucke V, Ioﬀe S, Shlens J, Wojna Z (2015) Rethinking the inception architecture for computer

vision. arXiv:151200567 [cs] URL http://arxiv.org/abs/1512.00567, arXiv: 1512.00567

Thompson B, Khayrallah H, Anastasopoulos A, McCarthy AD, Duh K, Marvin R, McNamee P, Gwinnup J,
Anderson T, Koehn P (2018) Freezing subnetworks to analyze domain adaptation in neural machine translation.
In: Proceedings of the Third Conference on Machine Translation: Research Papers, pp 124–132

Tiedemann J (2009) Character-based psmt for closely related languages. In: Proceedings of the 13th Conference

26

Stig-Arne Gr¨onroos et al.

of the European Association for Machine Translation (EAMT 2009), pp 12–19

Toral A, S´anchez-Cartagena VM (2017) A multifaceted evaluation of neural versus phrase-based machine
translation for 9 language directions. arXiv:170102901 [cs] URL http://arxiv.org/abs/1701.02901, arXiv:
1701.02901

Torrey L, Shavlik J (2009) Transfer learning. In: Olivas ES (ed) Handbook of Research on Machine Learning
Applications and Trends: Algorithms, Methods, and Techniques: Algorithms, Methods, and Techniques, IGI
Global, pp pages 242–264

Tu Z, Liu Y, Shang L, Liu X, Li H (2016) Neural machine translation with reconstruction. arXiv:161101874 [cs]

URL http://arxiv.org/abs/1611.01874, arXiv: 1611.01874

Vaibhav V, Singh S, Stewart C, Neubig G (2019) Improving robustness of machine translation with synthetic noise.
In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp 1916–1920

Varjokallio M, Kurimo M, Virpioja S (2013) Learning a subword vocabulary based on unigram likelihood. In:
Proc. 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), IEEE, Olomouc,
Czech Republic, pp 7–12, DOI 10.1109/ASRU.2013.6707697, URL http://ieeexplore.ieee.org/document/
6707697/

Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention is
all you need. In: Advances in Neural Information Processing Systems, pp 6000–6010, URL http://arxiv.
org/abs/1706.03762, arXiv: 1706.03762

Vincent P, Larochelle H, Bengio Y, Manzagol PA (2008) Extracting and composing robust features with denoising

autoencoders. In: Proceedings of the 25th international conference on Machine learning, pp 1096–1103

Virpioja S, V¨ayrynen JJ, Creutz M, Sadeniemi M (2007) Morphology-aware statistical machine translation based
on morphs induced in an unsupervised manner. In: Machine Translation Summit XI, Copenhagen, Denmark,
vol 2007, pp 491–498

Virpioja S, Turunen VT, Spiegler S, Kohonen O, Kurimo M (2011) Empirical comparison of evaluation
methods for unsupervised learning of morphology. Traitement Automatique des Langues 52(2):45–90, URL
http://www.atala.org/Empirical-Comparison-of-Evaluation

Virpioja S, Smit P, Gr¨onroos SA, Kurimo M (2013) Morfessor 2.0: Python implementation and extensions for
morfessor baseline. Report 25/2013 in Aalto University publication series SCIENCE + TECHNOLOGY,
Department of Signal Processing and Acoustics, Aalto University, Helsinki, Finland

Wang X, Pham H, Dai Z, Neubig G (2018) Switchout: an eﬃcient data augmentation algorithm for neural

machine translation. arXiv preprint arXiv:180807512

Xia Y, He D, Qin T, Wang L, Yu N, Liu TY, Ma WY (2016) Dual learning for machine translation.

arXiv:161100179 [cs] URL http://arxiv.org/abs/1611.00179, arXiv: 1611.00179

Yang Z, Chen W, Wang F, Xu B (2018) Unsupervised neural machine translation with weight sharing. In:
Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp 46–55

Zhang J, Zong C (2016) Exploiting source-side monolingual data in neural machine translation. In: Proceedings

of the 2016 Conference on Empirical Methods in Natural Language Processing, pp 1535–1545

Zoph B, Yuret D, May J, Knight K (2016) Transfer learning for low-resource neural machine translation. In:
Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp 1568–1575

