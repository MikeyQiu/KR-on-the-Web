7
1
0
2
 
p
e
S
 
1
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
7
0
4
4
0
.
2
0
7
1
:
v
i
X
r
a

Sequential Dirichlet Process Mixtures of
Multivariate Skew t-distributions for
Model-based Clustering of Flow Cytometry Data

Boris P. Hejblum1,2,∗ Chariﬀ Alkhassim1,2 Raphael Gottardo3,
Fran¸cois Caron4, and Rodolphe Thi´ebaut1,2

1Univ. Bordeaux, ISPED, Bordeaux Population Health Research Center,

Inserm U1219, Inria SISTM, 33000 Bordeaux, France
2Vaccine Research Institute (VRI), 94010 Cr´eteil, France
3Fred Hutchinson Cancer Research Center, Seattle, Washington, U.S.A.
4Department of Statistics, University of Oxford, Oxford, U.K.

∗boris.hejblum@u-bordeaux.fr

September 12, 2017

Abstract

Flow cytometry is a high-throughput technology used to quantify mul-
tiple surface and intracellular markers at the level of a single cell. This
enables to identify cell sub-types, and to determine their relative pro-
portions. Improvements of this technology allow to describe millions of
individual cells from a blood sample using multiple markers. This re-
sults in high-dimensional datasets, whose manual analysis is highly time-
consuming and poorly reproducible. While several methods have been
developed to perform automatic recognition of cell populations, most of
them treat and analyze each sample independently. However, in prac-
tice, individual samples are rarely independent (e.g. longitudinal studies).
Here, we propose to use a Bayesian nonparametric approach with Dirich-
let process mixture (DPM) of multivariate skew t-distributions to perform
model based clustering of ﬂow-cytometry data. DPM models directly
estimate the number of cell populations from the data, avoiding model
selection issues, and skew t-distributions provides robustness to outliers
and non-elliptical shape of cell populations. To accommodate repeated
measurements, we propose a sequential strategy relying on a parametric
approximation of the posterior. We illustrate the good performance of our
method on simulated data, on an experimental benchmark dataset, and
on new longitudinal data from the DALIA-1 trial which evaluates a ther-
apeutic vaccine against HIV. On the benchmark dataset, the sequential
strategy outperforms all other methods evaluated, and similarly, leads to
improved performance on the DALIA-1 data. We have made the method

1

available for the community in the R package NPflow.

Key words: Automatic gating; Bayesian Nonparametrics; Dirichlet pro-
cess; Flow cytometry; HIV; Mixture model; Skew t-distribution

1

Introduction

Flow cytometry is a high-throughput technology used to quantify multiple sur-
face and intracellular markers at the level of single cell. More speciﬁcally, cells
are stained with multiple ﬂuorescently-conjugated monoclonal antibodies di-
rected to cell surface receptors (such as CD4) or intracellular markers (such as
cytokines) to determine the type of cell, their diﬀerentiation and their func-
tionality. With the improvement of this technology leading currently to the
measurement of up to 18 at the same time (using 18 colors for Flow cytometry),
multi-parametric description of millions of individual cells can be generated.

Analysis of such data is generally performed manually. This results in anal-
yses that are:
i) poorly reproducible (Aghaeepour et al., 2013), ii) expensive
(highly time-consuming) and iii) as a result of ii), focused on speciﬁc cell pop-
ulations (i.e. speciﬁc combination of markers), ignoring other cell populations.
There has been an eﬀort in the recent years to oﬀer automated solutions to
overcome these limitations (Lo et al., 2008; Aghaeepour et al., 2013; Gondois-
Rey et al., 2016). Quite a lot of diﬀerent methodological approaches have been
proposed to perform automatic recognition of cell populations from ﬂow cy-
tometry data. Clustering methods related to the k-means were proposed, such
as L2kmeans (Aghaeepour et al., 2013), ﬂowMeans (Aghaeepour et al., 2011).
Model based clustering methods relying on ﬁnite mixture models such as ﬂow-
Cust/merge (Lo et al., 2008; Finak et al., 2009), FLAME (Pyne et al., 2009),
SWIFT (Naim et al., 2014) were also proposed, as well as dimension reduction
methods such as MM and MMPCA (Sug´ar and Sealfon, 2010), SamSPECTRAL
(Zare et al., 2010), FLOCK (Qian et al., 2010). All those approaches requires
the number of cell populations to be ﬁxed in advance, and resort to various crite-
ria to determine the number of cell populations. Finally, several authors (Chan
et al., 2008; Lin et al., 2013; Cron et al., 2013; Dundar et al., 2014), proposed
nonparametric Bayesian mixture models of Gaussian distributions, that directly
estimate the number of cell populations. All these methods, except those of Lin
et al. (2013), of Cron et al. (2013) and of Dundar et al. (2014), were evaluated
by Aghaeepour et al. (2013).

However, there is still room for improvement, especially in the estimation of
the suitable number of cell populations as well as in the identiﬁcation of rare cell
populations. In addition, most of those previous approaches have been proposed
for single sample analysis, except for Cron et al. (2013) who proposed to use hi-
erarchical Dirichlet process mixture (DPM) of Gaussian distribution models to
analyze multiple samples simultaneously. Yet in the case of repeated measure-
ments of ﬂow cytometry data, it can be useful to perform analysis as the samples

2

are acquired (samples are often collected across several time points in a popu-
lation of patients). In such a case, one would want to use previously acquired
sample as informative prior information in the analysis of a new sample. In this
paper, the proposed approach includes a strategy of sequential approximations
of the posterior distribution for multiple data samples, presented in Section 3.2.
Our approach oﬀers three advantages:
i) it quantiﬁes the uncertainty around
the posterior clustering estimate, ii) it can make use prior knowledge to inform
on the structure of the data, potentially building up on previous analyses, and
iii) it allows the analysis of multiple samples without requiring to process all
the data at once, alleviating both the computational burden and the necessity
for all data to be readily available before any analysis can be performed.

The automatic recognition of cell populations from ﬂow cytometry data is a
diﬃcult task which can be seen as an unsupervised clustering problem (Lo et al.,
2008). It is characterized by two big challenges. First, the total number of cell
populations to identify is unknown. Second, the empirical distributions of the
populations are heavily skewed, even when optimal transformation of the data
is applied (Lo et al., 2008; Pyne et al., 2009; Lo and Gottardo, 2012), and the
data generally present many outliers. To address all these points together, our
approach consider a Bayesian nonparametric model-based approach, where the
ﬂow cytometry data are assumed to be drawn from a DPM skew-t distributions.
First, this approach enables the number of cell populations to be inferred from
the data, and avoids the challenging problem of model selection. Second, it has
been demonstrated that the Gaussian assumption for the parametric shape of a
cell population ﬁts poorly ﬂow cytometry data (Mosmann et al., 2014). Indeed,
even after state-of-the-art transformation of raw cytometry data, such as the
biexponential transformation (Finak et al., 2010), cell population distributions
are typically skewed. Pyne et al. (2009) have showed the advantages of the skew
t-distribution (Azzalini and Capitanio, 2003) for modeling cell subpopulations
in ﬂow cytometry data. The skew t-distribution is a generalization of the skewed
normal distribution, with a heavier tail which makes it more robust to outliers.
Fr¨uhwirth-Schnatter and Pyne (2010) proposed a ﬁnite mixture model of skew
t-distributions. We extend this model to the inﬁnite mixture case in a Bayesian
nonparametric framework. Of interest, quantifying the uncertainty around the
estimated partition is straightforward in this Bayesian paradigm, from the pos-
terior distribution of the partition. While a skewed distribution could be ﬁt
either by a skew-t or a mixture of Gaussians, using the latter requires to sepa-
rate the estimation of the overall number of clusters from the skewness, while
the proposed approach jointly estimate those two and thus takes into account
the uncertainty associated with both. Furthermore, the use of a Bayesian frame-
work enables the use of informative priors. In the case of repeated measurements
for instance, we propose to sequentially estimate the posterior partition of ﬂow
cytometry using posterior information from time point t as prior information
for time point t + 1.

The proposed approach is applied to simulated data, to a benchmark clinical
dataset from Aghaeepour et al. (2013), and to an original experimental dataset
from a phase I HIV clinical trial DALIA-1. The method is implemented in the

3

R package NPﬂow, available on the CRAN at https://CRAN.R-project.org/
package=NPflow.

2 Statistical Model

2.1 Problem set-up

In this Section we ﬁrst consider that we have only one sample per subject. The
case of the sequential estimation of multiple datasets will be addressed in Sec-
tion 3.2. We consider that we have data yc ∈ Rd, c = 1, . . . , C corresponding to
the vector of ﬂuorescence intensities measured for the cell c. Typically, the ob-
servations yc have been transformed (to help visualization and gating) from the
raw measurements of ﬂuorescence through a biexponential or Box-Cox transfor-
mation (Finak et al., 2010). We assume that these observations are independent
and identically distributed (i.i.d.) from some unknown distribution F :

where F is a mixture of distributions:

yc|G i.i.d.∼ F for c = 1 . . . , C

F (y) =

fθ(y)G(dθ)

(cid:90)

Θ

with fθ(y) a known probability density function, parameterized by θ ∈ Θ, a set
of parameters, and deﬁning the shape of a cluster. G is the unknown mixing
distribution, which carries the weights and locations of the mixture components.
In a parametric approach, G = (cid:80)K
k=1 πkδθk where πk is the weight of the kth
mixture component. Maximum likelihood or Bayesian estimates of F can be
derived for such models (Biernacki et al., 2000). In a nonparametric perspective
(where the number of clusters is unknown) G is written as an inﬁnite sum
of atoms: G = (cid:80)+∞
k=1 πkδθk . The Dirichlet process is a conjugate prior for the
inﬁnite atomic discrete distribution, which makes it very useful for unsupervised
clustering approaches.

2.2 Dirichlet process mixture model

We assume that the random mixing distribution G is drawn from a Dirichlet
process (Ferguson, 1973):

G ∼ DP(α, G0)

where DP(α, G0) denotes the Dirichlet process of scale parameter α > 0 and base
probability distribution G0. A draw G ∼ DP(α, G0) is almost surely discrete
and takes the following form (Sethuraman, 1994):

(1)

(2)

(3)

(4)

G =

πkδθk

+∞
(cid:88)

k=1

4

where the θk are i.i.d. from the base distribution G0 and independent of the
weights, π = (πk)k=1,2,..., which are drawn from a so-called “stick-breaking”
distribution:

πk = βk

(1 − βj)

k−1
(cid:89)

j=1

i.i.d.∼ Beta(1, α) for k = 1, 2, . . . . We write π ∼ GEM(α) the Griﬃths-
with βk
Engen-McCloskey (GEM) distribution (Pitman, 2006). The model deﬁned by
Equations (1), (2) and (3) yields the following hierarchical model known as a
Dirichlet process mixture model (Lo, 1984; Escobar and West, 1995; Teh, 2010)
with a Gamma hyperprior on the concentration parameter α:

for k = 1, 2, . . .

for c = 1, 2, . . . , C

α|a, b ∼ Gamma(a, b)
π (cid:12)

(cid:12) α ∼ GEM(α)

θk

(cid:12)
(cid:12) G0 ∼ G0

(cid:12)
(cid:12) π ∼ Mult(π)

(cid:96)c

yc

(cid:12)
(cid:12) (cid:96)c, (θk) ∼ fθ(cid:96)c

(5a)

(5b)

(5c)

(5d)

(5e)

where (cid:96)c is an allocation variable indicating to which cluster is associated cell
c.

The base distribution G0 tunes the prior information we have about the
cluster locations. The parameter α tunes the prior distribution on the overall
number of clusters K that will be discovered within C data. In particular we
have E[K] = (cid:80)C−1
c=0

α
α+c .

2.3 Multivariate skew-t distribution

We now consider the choice of the parametric density fθ which is a skew-t
distribution.

2.3.1 Skew-normal distribution

Fr¨uhwirth-Schnatter and Pyne (2010) present a parametrization of the multi-
variate skew Normal distribution deﬁned by Azzalini and Valle (1996) which
leads to the following probability density function:

fSN (y; ξ, Ω, η) = 2φ(y − ξ; Ω)Φ(η(cid:48)ω−1(y − ξ))

(6)

5

(7)

(8)

(9)

(10)

(11)

with φ(·; Ω) the probability density function of the multivariate Normal distri-
bution with zero mean N (0, Ω) and Φ(·) the cumulative density function of the
standard univariate Normal distribution N (0, 1).

Fr¨uhwirth-Schnatter and Pyne (2010) propose a random-eﬀects model rep-
resentation of such a skew Normal distribution, with truncated normal random
eﬀects:

Y = ξ + ψZ + ε

with Z ∼ N[0;+∞[(0, 1) a truncated univariate standard Normal distribution and
ε ∼ N (0, Σ) a multivariate Normal distibution with zero mean. The original
parameters can be recovered from:

Ω = Σ + ψψ(cid:48), η =

1
1 − ψ(cid:48)Ω−1ψ

(cid:112)

ωΩ−1ψ

2.3.2 The skew t-distribution

Let X ∼ SN (0, Ω, η) and W ∼ Gamma( ν
representation:

2 , ν

2 ). If Y has the following stochastic

then it follows a multivariate skew t-distribution Y ∼ ST (ξ, Ω, η, ν) (Azzalini
and Capitanio, 2003). Equation (9) can be expressed as the following random
eﬀect model

Y = ξ +

1
√
W

X

Y = ξ + ψ

Z
√
W

+

(cid:15)
√
W

Following the same parametrization as Fr¨uhwirth-Schnatter and Pyne (2010),

we write the density of a multivariate skew t-distibution as:

fST (y; ξ, Ω, η, ν) =2fT (y; ξ, Ω, ν)

(cid:32)

× Tν+d

η(cid:48)ω−1(y − ξ)

(cid:115)

(cid:33)

ν + d
ν + Qy

with ω = (cid:112)Diag(Ω), Qy = (y − ξ)(cid:48)Ω−1(y − ξ), fT the multivariate Student
t-distribution probability density function, and Tν the cumulative distribution
function of the scalar standard Student t-distribution with ν degrees of freedom.
Figure 1 shows an example of such distributions, highlighting the skewness
of both the skew Normal and the skew t and the heavier tail of the skew t
distribution.

2.4 Dirichlet process mixture of skew t-distribution

Let G0 be the base distribution of a Dirichlet process in a DPM combining model
(5) with a random-eﬀects model representation (10) of the skew t-distribution.
G0 is the product of a structured Normal inverse Wishart (sN iW ) and of a prior

6

Figure 1: Density probability function of univariate skew Normal SN (ξ = 0, ψ =
10, σ = 1) and skew t ST (ξ = 0, ψ = 10, σ = 1, ν = 1.5) distributions

on ν, the degree of freedom of the skew-t: G0 = sN iW (ξ0, ψ0, B0, Λ0, λ0)P0,ν.
Our proposed model is fully written as follows:

α|a, b ∼ Gamma(a, b)
π (cid:12)

(cid:12) α ∼ GEM(α)

for k = 1, 2, . . .

for c = 1, 2, . . . , C

ξk, ψk, Σk, νk ∼ G0

(cid:12)
(cid:12) π ∼ Mult(π)

(cid:96)c

(cid:12)
(cid:12) (cid:96)c, (νk) ∼ Gamma

γc

(cid:12)
(cid:12) γc ∼ N[0,+∞[

sc

(cid:18)

(cid:17)

ν(cid:96)c
2
(cid:19)

(cid:16) ν(cid:96)c
2

,

(cid:18)

0,

1
γc

(cid:12)
(cid:12) (cid:96)c, γc, sc, (ξk, ψk, Σk) ∼ N

yc

ξ(cid:96)c + ψ(cid:96)csc,

Σ(cid:96)c

(cid:19)

1
γc

7

(12a)

(12b)

(12c)

(12d)

(12e)

(12f)

(12g)

2.5 Discussion on the model assumptions

In model (12), the base distribution parameter G0 conveys the prior information
on the cluster parametric shape. For the parameters ξk, ψk and Σk, we have
conditional conjugacy with the random-eﬀects model representation using joint
priors taking the form of a structured Normal-inverse-Wishart distribution. See
Appendix A for details. Fr¨uhwirth-Schnatter and Pyne (2010) pointed out that
the prior on Σk can have a big impact on the posterior number of clusters.
Indeed, setting the scale of the prior on Σk too small will result in an inﬂated
number of clusters in the posterior, whereas too large values tend to cluster all
the observations together. Adding a Wishart hyperprior on Σk, that carries on
conjugacy with the inverse-Wishart, enables us to reduce this impact of the prior
(Fr¨uhwirth-Schnatter and Pyne, 2010; Huang and Wand, 2013). Assuming prior
independence between each νk and also from the three parameters mentioned
above, we can use any of the three priors proposed in Ju´arez and Steel (2010)
for instance (such as an objective Jeﬀrey’s prior, see Appendix A).

3 Estimation

3.1 Posterior Estimation via Gibbs sampling

k}, {ν∗

k}, {ψ∗

k}, {Σ∗

For making inference on the model (12), MCMC methods can be used to
sample the partition {(cid:96)1:C} and the corresponding cluster parameters {θ∗
k} =
{{ξ∗
k}} from the marginal posterior distribution. Extending
results from Fr¨uhwirth-Schnatter and Pyne (2010) and Caron et al. (2014), it
is possible to implement an eﬃcient and valid partially collapsed Gibbs sam-
pler with a Metropolis-Hastings step (van Dyk and Park, 2008; van Dyk and
Jiao, 2015). The use of slice sampling (Neal, 2003; Kalli et al., 2011) enables
the straightforward parallelization of the latent allocation sampling (thanks to
conditional conjugacy) in such an MCMC algorithm (even in the skew-normal
and skew-t cases), which can lead to substantial computation speed up when
the number of observations C (cells) per sample increases. Each iteration of our
Gibbs sampler proceeds in the following order (details are provided Appendix
A):

1. Update the concentration parameter α given the previous partition {(cid:96)1:C}
using the data augmentation technique from Escobar and West (1995).

2. Update the mixing distribution G given α, {ξk}, {ψk}, {Σk} and the base

distribution G0 via slice sampling.

3. For c = 1, . . . , C update the individual skew parameter sc given {ξk},

{ψk}, {Σk} and the new (cid:96)c.

4. Update {ξk}, {ψk}, {Σk} given the base distribution G0, the updated
partition {(cid:96)1:C} and the updated individual skew parameters {s1:C}.

8

5. Finally jointly update the degrees of freedom and the individual scale
factors ({νk}, {γ1:C}) in an Metropolis-Hastings (M-H) within Gibbs step.
First an M-H step is performed to update the {νk} where the {γ1:C} are
integrated out, immediately followed by a Gibbs step to sample the {γ1:C}
from their full conditional distribution. This ensures that the reduced
conditioning performed in the M-H step does not change the stationary
distribution of the Markov chain (van Dyk and Jiao, 2015) – see Appendix
A.

3.2 Sequential Posterior Approximation

In ﬂow cytometry experiments it is common to actually have multiple datasets
y(i) (with i = 1, . . . , I) corresponding to multiple individuals, or repeated mea-
surements of the same individual. In such cases, it is of interest to use previous
time points or previous samples results as prior information, in order to leverage
all the information available to estimate the mixture. However, specifying prior
information to Dirichlet process mixture models is not straightforward (Kessler
et al., 2015). Here we propose to use the posterior MCMC draws obtained from
previous dataset y(i) as prior information to analyze the next dataset y(i+1).
To do so, ﬁrst let’s consider the hierarchical model using all observations from
both y(i) and y(i+1) at once :

α ∼ Gamma(a, b)

G|α ∼ DP (α, G0)
(cid:90)

y(i), y(i+1)|G i.i.d.∼

fθ(·)dG(θ)

Θ

(13a)

(13b)

(13c)

We are interested in estimating p(G|y(i), y(i+1)) ∝ p(G|y(i))p(y(i+1)|G). The
idea is to ﬁrst approximate p(G|y(i)) by a Dirichlet process through MCMC
draws from the model described in 2.1:

(cid:90)

p(G|y(i)) (cid:39)

DP (G; α, G1)Gamma(α; a1, b1)dα

(14)

where G1, a1, b1 are parameters to be estimated from the MCMC approxima-
tion of the true posterior:
i) (cid:98)a1 and (cid:98)b1 can be taken as MLE estimates from
the MCMC samples α(j) ; ii) (cid:99)G1 is a parametric approximation of the poste-
rior mixing distribution G1 (the true posterior is not suitable for being directly
plugged in as a base distribution parameter of another DP as it is nonpara-
metric). In the case of a skew t-distributions mixture model, we approximate
G1 with the following joint distribution: G1 (cid:39) (sN iW, P0,ν) where P0,ν is the
chosen prior for the skew t-distribution degrees of freedom. To estimate G1, we
estimate the Maximum a posteriori (MAP) from the posterior MCMC samples
(see Appendix B).

9

Now using this posterior parametric approximation, we have the same hier-

archical model as before but conditional on y(i):

α|y(i) ∼ Gamma( (cid:98)a1, (cid:98)b1)

G|α, y(i) ∼ DP (α, (cid:99)G1)

y(i+1)|G, y(i) i.i.d.∼

fθ(·)dG(θ)

(cid:90)

Θ

(15a)

(15b)

(15c)

Note that under this approximate posterior model, the cluster parameters
θ∗
k are i.i.d. from G1. Such an approach can be iterated a number of times,
if for instance several time points are observed, iteratively approximating the
successive posteriors. This approach allows to ﬁnally account for all the previous
information in the mixture model estimation.

3.3 Point estimate of the clustering

Getting a representation of the partition posterior distribution is diﬃcult (Medve-
dovic and Sivaganesan, 2002). One can use the maximum a posteriori, i.e. using
the point estimation form the MCMC sample that maximize the posterior den-
sity. However this ignores all the information about the uncertainty around the
partition gained through the Bayesian approach.

Another way is to rather consider a co-clustering posterior probability (or
similarity) matrix ζ on each pair (c, d) of observations. Such a matrix can be es-
timated by averaging the co-clustering matrices from all the explored partitions
in the posterior MCMC draws:

(cid:98)ζcd =

1
N

N
(cid:88)

i=1

δ(cid:96)(i)

c (cid:96)(i)
d

(16)

where N is the number of MCMC draws from the posterior and δkl = 1 if k = l,
0 otherwise. An optimal partition point estimate {(cid:98)(cid:96)1:C} can then be derived
in regard of this similarity matrix through stochastic search with the explored
partitions in the posterior MCMC draws (Dahl, 2006), by using a pairwise
coincidence loss function (Lau and Green, 2007) such as the one proposed by
Binder (1978, 1981) which optimizes the Rand index (Fritsch and Ickstadt,
2009):

{(cid:98)(cid:96)1:C} =

arg min
(cid:110)
{(cid:96)(1)

1:C },...,{(cid:96)(N )
1:C }

(cid:111)

{(cid:96)(i)

1:C }∈

C−1
(cid:88)

C
(cid:88)

(cid:16)

2

c=1

d=c+1

(cid:17)2

δ(cid:96)(i)

c (cid:96)(i)
d

− (cid:98)ζcd

(17)

The computational cost of this approach, though, is of the order O(N C 2) due
to the necessity of computing all the similarity matrices.

A diﬀerent optimal partition point estimate {˜(cid:96)1:C} can also be derived using
the F-measure as our loss function. The F-measure is widely used as a way

10

to summarize the accordance between 2 methods, one being considered as a
reference (gold-standard). It is the harmonic mean of the precision and recall:

F =

2P rRe
P r + Re

(18)

In order to use the F-measure to evaluate our clustering method, we rely on
the deﬁnition proposed in the online methods from Aghaeepour et al. (2013).
In this setting of unsupervised clustering, the precision P r is the number of
cells correctly assigned to a given cluster divided by the total number of cells
assigned to that cluster (also called Positive Predictive Value). The recall Re is
the number of cells correctly assigned to a given cluster divided by the number
of cells that should be assigned to this cluster according to the gold-standard.
Since in our problem the labels of the diﬀerent clusters are exchangeable, the
F-measure is computed for each combination of the reference clusters and the
predicted clusters. Let G = {g1, . . . , gm} be a set of m reference clusters and
H = {h1, . . . , hn} be set of n predicted clusters. For each combination pair (q, r)
of a reference cluster gq and a predicted cluster hr, the F-measure is computed
as follows:

P r(hr, gq) =

and P r(hr, gq) =

|gq ∩ hr|
|hr|

|gq ∩ hr|
|gq|

F(hr, gq) =

2P r(gq, hr)Re(gq, hr)
P r(gq, hr) + Re(gq, hr)

(19)

(20)

This F-measure is comprised in [0, 1], and the closer it is to 1 the better the
agreement is between the predicted cluster and the reference cluster. The total
F-measure for a predicted partition H given a gold-standard G is then deﬁne
as the weighted sum of the best matched F-measure:

Ftot(H, G) =

1
q=1 |gq|

(cid:80)m

m
(cid:88)

q=1

|gq| max

F(hr, gq)

r∈{1,...,n}

(21)

This total F-measure is again between 0 and 1, and the closer it is to 1 the better
the predicted partition agrees with the gold-standard. The optimal partition
point estimate in respects of this F-measure is then obtained with the partition
that maximizes its average F-measure over all the other explored partitions in
the posterior MCMC draws:

{˜(cid:96)1:C} =

arg max
(cid:110)
{(cid:96)(1)

1:C },...,{(cid:96)(N )
1:C }

(cid:111)

{(cid:96)(i)

1:C }∈

1
N

N
(cid:88)

j=1
j(cid:54)=i

Ftot

(cid:16)

{(cid:96)(i)

1:C}, {(cid:96)(j)

(cid:17)
1:C}

(22)

Note the F-measure is computed here only between sampled partitions, and a
gold-standard partition is unnecessary.

11

4 Simulations Study

4.1 Non informative prior

First, to assess the performances of the Dirichlet process mixture of skew t distri-
butions model in a simple clustering case, 100 simulations in 2-dimensions were
performed. In each simulation 2000 observations were drawn from 4 distinct
clusters representing respectively 50%, 30%, 15% and 5% of the data. After
10,000 MCMC iterations (9,000 iterations burnt and a thining of 5 gave 200
partitions sampled from the posterior; the chain was initialized with 30 clus-
ters), the resulting mean F-measure was 0.998 when comparing the partition
point estimate obtained from our approach with the true clustering of the simu-
lated data. In 97% of the cases, the partition point estimate had 4 clusters (i.e.
the true number of clusters in the simulated data), while it had 3 or 5 clusters
in the remaining 3%. Figure 2 shows an example of the partition point estimate
obtained for one of those simulation run.

Figure 2: Partition point estimate from one of the ﬁrst 100 2-dimensional sim-
ulations

12

4.2 Sequential posterior approximation plugged-in as in-

formative prior

To illustrate how the sequential posterior approximation strategy compares to
the standard non informative prior setting, we ran simulations where we con-
sidered two samples derived from the same inﬁnite mixture model. The ﬁrst
sample is simulated for a time t, and the second sample at t + 1. As all observa-
tions originate from the exact same distribution, regardless of the sample, the
hypothesis of the sequential posterior approximation strategy is satisﬁed. One
of the major gain observed is the time to convergence for the partition. Using
an informative prior derived from the sample at time t to estimate the parti-
tion of the sample from t + 1 makes it more than three time faster to converge
according to the Gelman-Rubin statistics.

In further simulations, we also investigated the performance of this sequen-
tial posterior approximation strategy. As opposed to using the standard non
informative prior, it shows substantial gains when the amount of information
brought by the prior is substantial compared to the amount available from the
data at t + 1 alone. As the amount of information available at t + 1 increases,
the gain from using this strategy can become less noticeable, as shown using
the F-measure in Figure 3. But even when as many observations are available
at t + 1 as at t, the accuracy for rare cell populations is still improved by us-
ing an informative prior. This is not necessary visible at the scale of the total
F-measure, because it is masked by the larger clusters. However, when com-
puting a limited F-measure, that only takes into account smaller clusters (see
Appendix C), the use of an informative prior in this sequential strategy seems to
always improves the clustering accuracy for smaller clusters (see Supplementary
Figure S1 in Appendix C).

Figure 3: Mean F-measure according to the number of observations available
at t + 1, while 1,000 observations are available at t, over 300 simulations

13

5 Application to real datasets

5.1 Benchmark Graft versus Host Disease dataset

The Graft versus Host Disease (GvHD) dataset is a public dataset that was
ﬁrst analysed (manually gated) in Brinkman et al. (2007), with the objec-
tive of identifying cellular signature that correlates or predict Graft versus
Host disease. The GvHD data were used as benchmark data in the Flow-
CAP challenge Aghaeepour et al. (2013). Flow cytometry data was collected
for 12 sample, and original manual gates are being regarded as the true cell
clustering (actually a consensus over eight manual operators, from eight dif-
ferent operators).
In order to try to mitigate further the well known repro-
ducibility issues with manual gating (Ge and Sealfon, 2012; Aghaeepour et al.,
2013), only the most concordant clusters between the 8 gatings (F-measure
above 0.8) were used for comparing with the automated results, as was done
in Aghaeepour et al. (2013). The data were downloaded from the FlowCAP
project website [http://flowcap.flowsite.org/] as part of the FlowCAP-I chal-
lenge [http://flowcap.flowsite.org/codeanddata/FlowCAP-I.zip]. Table 1 shows the
performance of our proposed approach NPﬂow on this dataset, in the context of
the other approaches reviewed by Aghaeepour et al. (2013). The F-measure is
computed for all samples available for a given dataset and the mean over all sam-
ples is reported, as well as bootstrap 95% Conﬁdence Intervals. No algorithm is
performing signiﬁcantly better than NPﬂow thus placing NPﬂow among the top
methods for automatic gating, and the sequential approach yields a F-measure
higher than any other method.

The GvHD benchmark data are not longitudinal data. However, the sequen-
tial posterior model can still improve the results by using each individual sam-
ple sequentially. Using this dataset, the mean F-measure reached 0.89 (0.85,
0.94) with the sequential approach, compared to a value of 0.85 (0.80, 0.90)
with the standard NPﬂow model (Table 1). The sequential strategy exhibits
the highest F-measure for the GvHD dataset, making it the best approach for
unsupervised automatic gating compared to competing methods evaluated in
Aghaeepour et al. (2013).

5.2 Original DALIA-1 data

We also applied our method to an original dataset from DALIA-1, a phase I
trial evaluating a therapeutic vaccine against HIV (L´evy et al., 2014). The
vaccine candidate was based on ex-vivo generated interferon-α dendritic cells
loaded with HIV-1 lipo-peptides, and activated with lipopolysaccharide. The
objectives of the trial were to evaluate the safety of the strategy and to evaluate
the immune response to the vaccine. For our purpose here, we are interested in
the 12 HIV positive patients who had their cellular populations quantiﬁed at 18
time-points during the trial. More speciﬁcally, we focused on two time points (at
week 24 and week 26 of the trial) immediately following antiretroviral treatment
(HAART) interruption which took place at week 24. Following this interruption,

14

Table 1: Mean F-measures across all the 12 samples from the GvHD benchmark
dataset

F-measure
Method
0.85 (0.80, 0.90)
NPﬂow
0.89 (0.85, 0.94)
NPﬂow-seq
0.81 (0.72, 0.88)
ADICyt
0.52 (0.46, 0.58)
CDP
0.85 (0.77, 0.91)
FLAME
0.84 (0.76, 0.90)
FLOCK
0.69 (0.55, 0.79)
ﬂowClust/Merge
0.88 (0.82, 0.93)
ﬂowMeans
0.85 (0.79, 0.91)
FlowVB
0.64 (0.57, 0.72)
L2kmeans
0.83 (0.74, 0.91)
MM
MMPCA
0.84 (0.74, 0.93)
SamSPECTRAL 0.87 (0.81, 0.93)
0.63 (0.56, 0.70)
SWIFT

All estimates except for our proposed NPﬂow approach are from Aghaeepour et al. (2013).
95% Conﬁdence Intervals are calculated on 10,000 bootstrap samples of the F -measures.

the increase of viral replication is associated with changes in cell populations
(Thi´ebaut et al., 2005; L´evy et al., 2012). Here we especially looked at the
eﬀector CD4+ T-cells, deﬁned as CD45RA+CD27- among the CD3+CD4+
cells (Larbi and Fulop, 2014), that are one of the ﬁrst cell populations to be
aﬀected during the viral rebound (L´evy et al., 2012). Since ﬂow-cytometry
measurements were repeated at each time points for each patients, we used
the sequential strategy at week 26, in the hope to use the information from
week 24 to better identify the eﬀector CD4+ T-cell population at the next time
point. Figure 4 illustrates the overall eﬃciency gain at week 26 from using
the sequential strategy. The average limited F-measure (compared to available
manual gating used as gold-standard) on those 12 samples is 0.5 for NPﬂow with
a non-informative prior, and increase to 0.59 with the sequential strategy. By
comparison, ﬂowMeans (the second best method on the GvHD dataset) gives
an average limited F-measure of 0.51 (see Appendix D for details). Figure 5
gives an example of a patient for which the sequential strategy was especially
In this case, the
improving the identiﬁcation of the eﬀector CD4+ T-cells.
percentage of eﬀector CD4+ T-cells was estimated at 31.7 by the manual gating,
at 7.5 by NPﬂow, and at 38.1 by the sequential strategy. Figure 6 shows the
general increase of eﬀector CD4+ T-cell proportions for every patients after
treatment interruption (see Appendix D for more details).

In addition to providing a point estimate of the partition, our method also
quantiﬁes the uncertainty around the posterior clustering through posterior co-
clustering probabilities. Figure 7 displays such a co-clustering posterior proba-

15

Figure 4: Limited F-measures for the eﬀector CD4+ T-cell population from the
DALIA-1 trial two weeks after HAART interruption for NPﬂow with or without
the sequential strategy, compared to manual gating.

Figure 5: CD3+CD4+ cells of patient 3 from the DALIA-1 trial two weeks
after HAART interruption (at week 26).

bilities matrix. Where we can clearly identify 4 core clusters, with some uncer-
tainty corresponding to marginal cells that are in between overlapping popula-
tions.

6 Discussion

We extend the classical Dirichlet process Gaussian mixture model to skew t-
distribution mixtures, based on Fr¨uhwirth-Schnatter and Pyne (2010) parametriza-
tion of such distributions. Such an approach is well suited for unsupervised
model based classiﬁcation of ﬂow cytometry data. Automatic gating of cell pop-
ulations is an open research problem and the proposed approach features two
important characteristics for this task: i) it avoids the diﬃcult issue of model
selection by estimating directly the number of components in the mixture ; ii) it

16

Figure 6: Evolution of the proportion of eﬀector CD4+ T-cells in the DALIA-1
trial following HAART interruption (from manual gating).

Figure 7: Heatmap of the posterior co-clustering probabilities for the
CD3+CD4+ cells of patient 3 at week 26 from DALIA-1.

uses skew and heavy tailed distributions in the form of skew t-distributions, of
which the gaussian is a particular case. Estimation of the posterior co-clustering

17

probabilities for each data pair allows to quantify the uncertainty about the
posterior partition, and an optimal point estimate of the clustering is provided
by minimizing a cost function in regards to the average posterior co-clustering
matrix. We have developed and implemented an eﬃcient collapsed Metropolis
within Gibbs sampler for estimating such models. One of the advantage of our
proposed sampler is the absence of label switching issue, as it uses directly the
partition of the data without having to deal with labels (Jasra et al., 2005). As
an indication of runtime, around 3,000 MCMC iterations can be run on average
for a real dataset of around 10,000 observations over 6 dimensions, using one
Intel R(cid:13) Xeon R(cid:13) x5675 processor in an hour. Besides, instead of using a partially
collapse Gibbs sampler algorithm, it could be of interest to also investigate the
use of sequential Monte-Carlo algorithms, especially for the sequential model-
ing strategy or other possible dynamic extensions of the model proposed here
(Caron et al., 2008, 2017).

We propose to use sequential parametric approximations of the posterior as
reﬁned informative priors in case of repeated measurement of ﬂow cytometry
data. The proposed sequential analysis strategy enables to analyze each sample
sequentially, as the data are acquired. It does not require to wait for the last
sample to perform the automatic gating nor to analyze all data at once, but it
still uses available prior knowledge. This contrasts with hierarchical extensions
of the Dirichlet Process Mixture Model such as those proposed by Cron et al.
(2013) or Dundar et al. (2014), where the complete dataset must be analyzed
at once. This sequential strategy allows one to analyze the samples as they are
acquired, which can be useful in clinical trials where there are often intermediate
analyses for instance. Moreover in large studies the size of the data can make
it challenging to analyze all samples at once, and such a sequential approach
then makes practical sense (Huang and Gelman, 2005). Futhermore, this use
of sequentially informed priors does not face the usual complications of cluster
matching arising when an algorithm is run on each sample separately (Cron
et al., 2013). In our simulation study this sequential posterior approximation
strategy improves the ﬁt of the model.
In addition, such a strategy exhibits
accelerated convergence and greater accuracy for small clusters, as long as the
diﬀerent samples are similar enough. Besides, the parametric prior can also
be speciﬁed to inform the model with expert knowledge, e.g. to favor a range
for the expected number of clusters. On real ﬂow-cytometry data we show
that the sequential strategy also improves the clustering performances. On the
benchmark GvHD dataset, it outperforms all other methods investigated in by
Aghaeepour et al. (2013). In the DALIA-1 trial, the sequential strategy allows a
better recovery of the eﬀector CD4+ T-cell population after a important pertur-
bation of this targeted population following HAART interruption among HIV
positive patients. It is worth noting however that in other cases, for instance if
the data distributions were too diﬀerent from samples to samples, the sequential
posterior model would not necessarily improve the clustering results, and could
even gave a diminished F -measure compared to the non sequential strategy.

Manual gating is still considered the gold-standard when evaluating an auto-
matic gating strategy on real ﬂow cytometry data. Yet one should keep in mind

18

that manual gating has reproducibility issues, often resulting in a partial and
subjective clustering (Ge and Sealfon, 2012; Welters et al., 2012; Aghaeepour
et al., 2013; Gondois-Rey et al., 2016). Therefore using manual gating as the
gold-standard might not be actually the best way to assess the performance of
automatic gating algorithms on real data, because of its inherent ﬂaws.

Mass cytometry is a technology very similar to ﬂow cytometry. Using ions
in place of colors, CyTOF is able to measure up to 40 cell markers at once,
generating even more data than ﬂow cytometry. Eﬃcient automated gating
method are therefore all the more needed in the context of CyTOF(Melchiotti
et al., 2017). The approach proposed here could be directly applied to such data.
More generally, we propose here a framework for Dirichlet process mixtures of
multivariate skew t-distributions modeling that is suitable for any kind of data
modeled as such a mixture, especially when the number of mixture component
is unknown. We provide an eﬃcient implementation of our method within the
R package NPflow that is available on CRAN at https://cran.r-project.
org/web/packages/NPflow.

Software

Software in the form of R code is available on the Comprehensive R Archive
Network as an R package tcgsaseq.

Acknowledgements

The authors are extremely grateful to Jean-Louis Palgen for his time and ef-
forts to manually gate the eﬀector CD4+ T-cells in the DALIA-1 trial at weeks
24 and 26, as well as to the DALIA-1 study group. The authors also thank
Nima Aghaeepour for his help in using supplementary data provided with his
publication (Aghaeepour et al., 2013). Part of this work has been supported
by the BNPSI ANR project no ANR-13-BS-03-0006-01. Boris P. Hejblum was
a recipient of a Ph.D. fellowship from the ´Ecole des Hautes ´Etudes en Sant´e
Publique (EHESP) Doctoral Network. Part of this work has been supported by
the IMI2 grant EBOVAC2. Computer time for this study was partly provided
by the computing facilities MCIA (M´esocentre de Calcul Intensif Aquitain) of
the Universit´e de Bordeaux and of the Universit´e de Pau et des Pays de l’Adour.

Conﬂict of Interest: None declared.

References

Aghaeepour, N., Finak, G., Hoos, H., Mosmann, T. R., Brinkman, R. R., Got-
tardo, R., and Scheuermann, R. H. (2013). Critical assessment of automated
ﬂow cytometry data analysis techniques. Nature methods 10, 228–238.

19

Aghaeepour, N., Nikolic, R., Hoos, H. H., and Brinkman, R. R. (2011). Rapid
cell population identiﬁcation in ﬂow cytometry data. Cytometry. Part A : the
journal of the International Society for Analytical Cytology 79, 6–13.

Azzalini, A. and Capitanio, A. (2003). Distributions generated by perturbation
of symmetry with emphasis on a multivariate skew t-distribution. Journal of
the Royal Statistical Society: Series B (Statistical Methodology) 65, 367–389.

Azzalini, A. and Valle, A. D. (1996). The multivariate skew-normal distribution.

Biometrika 83, 715–726.

Biernacki, C., Celeux, G., and Govaert, G. (2000). Assessing a mixture model
for clustering with the integrated completed likelihood. IEEE transactions on
pattern analysis and machine intelligence 22, 719–725.

Binder, D. A. (1978). Bayesian Cluster Analysis. Biometrika 65, 31–38.

Binder, D. A. (1981). Approximations to Bayesian Clustering Rules. Biometrika

68, 275–285.

Brinkman, R. R., Gasparetto, M., Lee, S.-J. J., Ribickas, A. J., Perkins, J.,
Janssen, W., Smiley, R., and Smith, C. (2007). High-content ﬂow cytometry
and temporal data analysis for deﬁning a cellular signature of graft-versus-
journal of the
host disease. Biology of blood and marrow transplantation :
American Society for Blood and Marrow Transplantation 13, 691–700.

Caron, F., Davy, M., Doucet, A., Duﬂos, E., and Vanheeghe, P. (2008). Bayesian
Inference for Linear Dynamic Models With Dirichlet Process Mixtures. IEEE
Transactions on Signal Processing 56, 71–84.

Caron, F., Neiswanger, W., Wood, F., Doucet, A., and Davy, M. (2017). Gener-
alized P´olya Urn for Time-Varying Pitman-Yor Processes. Journal of Machine
Learning Research 18, 1–32.

Caron, F., Teh, Y. W., and Murphy, T. B. (2014). Bayesian nonparametric
PlackettLuce models for the analysis of preferences for college degree pro-
grammes. The Annals of Applied Statistics 8, 1145–1181.

Chan, C., Feng, F., Ottinger, J., Foster, D., West, M., and Kepler, T. B. (2008).
Statistical mixture modeling for cell subtype identiﬁcation in ﬂow cytometry.
Cytometry. Part A : the journal of the International Society for Analytical
Cytology 73, 693–701.

Cron, A., Gouttefangeas, C., Frelinger, J., Lin, L., Singh, S. K., Britten, C. M.,
Welters, M. J. P., van der Burg, S. H., West, M., and Chan, C. (2013).
Hierarchical modeling for rare event detection and cell subset alignment across
ﬂow cytometry samples. PLoS computational biology 9, e1003130.

20

Dahl, D. B. (2006). Model-Based Clustering for Expression Data via a Dirichlet
Process Mixture Model. In Do, K.-A., M¨uller, P., and Vannucci, M., editors,
Bayesian Inference for Gene Expression and Proteomics, chapter 10, pages
201–218. Cambridge University Press, Cambridge.

Dundar, M., Akova, F., Yerebakan, H. Z., and Rajwa, B. (2014). A non-
parametric Bayesian model for joint cell clustering and cluster matching:
identiﬁcation of anomalous sample phenotypes with random eﬀects. BMC
Bioinformatics 15, 314.

Escobar, M. D. and West, M. (1995). Bayesian Density Estimation and Inference
Using Mixtures. Journal of the American Statistical Association 90, 577–588.

Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems.

The Annals of Statistics 1, 209–230.

Finak, G., Bashashati, A., Brinkman, R., and Gottardo, R. (2009). Merging
mixture components for cell population identiﬁcation in ﬂow cytometry. Ad-
vances in bioinformatics 2009, 247646.

Finak, G., Perez, J.-M., Weng, A., and Gottardo, R. (2010). Optimizing trans-
formations for automated, high throughput analysis of ﬂow cytometry data.
BMC Bioinformatics 11, 546.

Fraley, C. and Raftery, A. E. (2007). Bayesian Regularization for Normal Mix-
ture Estimation and Model-Based Clustering. Journal of Classiﬁcation 24,
155–181.

Fritsch, A. and Ickstadt, K. (2009). Improved criteria for clustering based on

the posterior similarity matrix. Bayesian Analysis 4, 367–392.

Fr¨uhwirth-Schnatter, S. and Pyne, S. (2010). Bayesian inference for ﬁnite mix-
tures of univariate and multivariate skew-normal and skew-t distributions.
Biostatistics 11, 317–36.

Ge, Y. and Sealfon, S. C. (2012). ﬂowPeaks: a fast unsupervised clustering for
ﬂow cytometry data via K-means and density peak ﬁnding. Bioinformatics
28, 2052–2058.

Gondois-Rey, F., Granjeaud, S., Rouillier, P., Rioualen, C., Bidaut, G., and
Olive, D. (2016). Multi-parametric cytometry from a complex cellular sample:
Improvements and limits of manual versus computational-based interactive
analyses. Cytometry Part A 89, 480–490.

Huang, A. and Wand, M. P. (2013). Simple Marginally Noninformative Prior

Distributions for Covariance Matrices. Bayesian Analysis 8, 439–452.

Huang, Z. and Gelman, A. (2005). Sampling for Bayesian Computation with

Large Datasets. SSRN Electronic Journal pages 1–21.

21

Jasra, A., Holmes, C. C., and Stephens, D. A. (2005). Markov Chain Monte
Carlo Methods and the Label Switching Problem in Bayesian Mixture Mod-
eling. Statistical Science 20, 50–67.

Ju´arez, M. A. and Steel, M. F. J. (2010). Model-Based Clustering of Non-
Gaussian Panel Data Based on Skew- t Distributions. Journal of Business &
Economic Statistics 28, 52–66.

Kalli, M., Griﬃn, J. E., and Walker, S. G. (2011). Slice sampling mixture

models. Statistics and Computing 21, 93–105.

Kessler, D. C., Hoﬀ, P. D., and Dunson, D. B. (2015). Marginally speciﬁed
priors for non-parametric bayesian estimation. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 77, 35–58.

Larbi, A. and Fulop, T. (2014). From ”truly na¨ıve” to ”exhausted senescent”
T cells: When markers predict functionality. Cytometry Part A 85, 25–35.

Lau, J. W. and Green, P. J. (2007). Bayesian Model-Based Clustering Proce-

dures. Journal of Computational and Graphical Statistics 16, 526–558.

L´evy, Y., Thi´ebaut, R., Gougeon, M.-L., Molina, J.-M., Weiss, L., Girard, P.-
M., Venet, A., Morlat, P., Poirier, B., Lascaux, A.-S., Boucherie, C., Sereni,
D., Rouzioux, C., Viard, J.-P., Lane, C., Delfraissy, J.-F., Sereti, I., Chˆene,
G., and ILIADE Study Group (2012). Eﬀect of intermittent interleukin-2
therapy on CD4+ T-cell counts following antiretroviral cessation in patients
with HIV. AIDS 26, 711–720.

L´evy, Y., Thi´ebaut, R., Montes, M., Lacabaratz, C., Sloan, L., King, B.,
P´erusat, S., Harrod, C., Cobb, A., Roberts, L. K., Surenaud, M., Boucherie,
C., Zurawski, S., Delaugerre, C., Richert, L., Chˆene, G., Banchereau, J.,
and Palucka, K. (2014). Dendritic cell-based therapeutic vaccine elicits poly-
functional HIV-speciﬁc T-cell immunity associated with control of viral load.
European journal of immunology 44, 2802–2810.

Lin, L., Chan, C., Hadrup, S. R., Froesig, T. M., Wang, Q., and West, M. (2013).
Hierarchical Bayesian mixture modelling for antigen-speciﬁc T-cell subtyping
in combinatorially encoded ﬂow cytometry studies. Statistical Applications in
Genetics and Molecular Biology 12, 309–331.

Lo, A. Y. (1984). On a class of Bayesian nonparametric estimates: I. Density

estimates. The Annals of Statistics 12, 351–357.

Lo, K., Brinkman, R. R., and Gottardo, R. (2008). Automated gating of ﬂow
cytometry data via robust model-based clustering. Cytometry. Part A : the
journal of the International Society for Analytical Cytology 73, 321–332.

Lo, K. and Gottardo, R. (2012). Flexible mixture modeling via the multivariate
t distribution with the Box-Cox transformation: An alternative to the skew-t
distribution. Statistics and Computing 22, 33–52.

22

Medvedovic, M. and Sivaganesan, S. (2002). Bayesian inﬁnite mixture model
based clustering of gene expression proﬁles. Bioinformatics 18, 1194–1206.

Melchiotti, R., Gracio, F., Kordasti, S., Todd, A. K., and de Rinaldis, E. (2017).
Cluster stability in the analysis of mass cytometry data. Cytometry Part A
91, 73–84.

Mosmann, T. R., Naim, I., Rebhahn, J., Datta, S., Cavenaugh, J. S., Weaver,
J. M., and Sharma, G. (2014). SWIFT-scalable clustering for automated iden-
tiﬁcation of rare cell populations in large, high-dimensional ﬂow cytometry
datasets, Part 2: Biological evaluation. Cytometry Part A 85, 422–433.

Naim, I., Datta, S., Rebhahn, J., Cavenaugh, J. S., Mosmann, T. R., and
Sharma, G. (2014). SWIFT-scalable clustering for automated identiﬁcation
of rare cell populations in large, high-dimensional ﬂow cytometry datasets,
Part 1: Algorithm design. Cytometry Part A 85, 408–421.

Neal, R. M. (2003). Slice sampling. The Annals of Statistics 31, 705–767.

Pitman, J. (2006). Combinatorial Stochastic Processes, volume 1875 of Lecture

Notes in Mathematics. Springer-Verlag, Berlin Heidelberg.

Pyne, S., Hu, X., Wang, K., Rossin, E., Lin, T.-I., Maier, L. M., Baecher-
Allan, C., McLachlan, G. J., Tamayo, P., Haﬂer, D. a., De Jager, P. L.,
and Mesirov, J. P. (2009). Automated high-dimensional ﬂow cytometric data
analysis. Proceedings of the National Academy of Sciences of the United States
of America 106, 8519–8524.

Qian, Y., Wei, C., Eun-Hyung Lee, F., Campbell, J., Halliley, J., Lee, J. a.,
Cai, J., Kong, Y. M., Sadat, E., Thomson, E., Dunn, P., Seegmiller, A. C.,
Karandikar, N. J., Tipton, C. M., Mosmann, T., Sanz, I., and Scheuermann,
R. H. (2010). Elucidation of seventeen human peripheral blood B-cell sub-
sets and quantiﬁcation of the tetanus response using a density-based method
for the automated identiﬁcation of cell populations in multidimensional ﬂow
cytometry data. Cytometry. Part B, Clinical cytometry 78 Suppl 1, S69–82.

Sethuraman, J. (1994). A constructive deﬁnition of Dirichlet priors. Statistica

Sinica 4, 639–650.

Sug´ar, I. P. and Sealfon, S. C. (2010). Misty Mountain clustering: application
to fast unsupervised ﬂow cytometry gating. BMC Bioinformatics 11, 502.

Teh, Y. W. (2010). Dirichlet Process. In Encyclopedia of Machine Learning,

pages 280–287. Springer US, Boston, MA.

Thi´ebaut, R., Pellegrin, I., Chˆene, G., Viallard, J. F., Fleury, H., Moreau, J. F.,
Pellegrin, J. L., and Blanco, P. (2005). Immunological markers after long-
term treatment interruption in chronically HIV-1 infected patients with CD4
cell count above 400 x 10(6) cells/l. AIDS 19, 53–61.

23

van Dyk, D. A. and Jiao, X. X. (2015). Metropolis-Hastings within Partially
Collapsed Gibbs Samplers. Journal of Computational and Graphical Statistics
24, 301–327.

van Dyk, D. A. and Park, T. (2008). Partially Collapsed Gibbs Samplers.

Journal of the American Statistical Association 103, 790–796.

Welters, M. J. P., Gouttefangeas, C., Ramwadhdoebe, T. H., Letsch, A., Ot-
tensmeier, C. H., Britten, C. M., and Van Der Burg, S. H. (2012). Har-
monization of the intracellular cytokine staining assay. Cancer Immunology,
Immunotherapy 61, 967–978.

West, M. (1992). Hyperparameter estimation in Dirichlet process mixture mod-

els. In ISDS discussion paper series, pages #92–03. Duke University.

Zare, H., Shooshtari, P., Gupta, A., and Brinkman, R. R. (2010). Data reduction
for spectral clustering to analyze high throughput ﬂow cytometry data. BMC
Bioinformatics 11, 403.

Appendix

A Gibbs samplers

• K is the number of diﬀerent unique values taken by c (i.e. the number of
clusters). This number of clusters K is not set and its value may change
at each iteration.

• (cid:96)c is the latent variable indicating which cluster the observation c belongs

to. {(cid:96)1:C} refers to a whole partition of the data.

• sc is the skew parameter for the observation c.

• γc is the scale parameter (skew t only) for the observation c.

A.1 Skew Normal distributions mixture

Our Gibbs sampler proceeds with each of the following updates in turn:

1. update concentration parameter α given {(cid:96)1:C} using the data augmenta-

tion technique from West (1992):
α ∝ p(α|{z1:C}, G0, {ξk}, {ψk}, {Σk}, {(cid:96)1:C}, {wk}, {s1:C}) ∝ p(α|{(cid:96)1:C})
(α, x|{(cid:96)1:C}) ∼ p(α)αK−1(α + C)xα(1 − x)C−1
(x|α, {(cid:96)1:C}) ∼ Beta(α + 1, C)
(α|x, {(cid:96)1:C}) ∼ πxGamma(a + K, b − log(x)) + (1 − πx)Gamma(a + K −
1, b − log(x)) with p(α) ∝ Gamma(a, b) and πx
1−πx

= a+k−1

C(b−log(x))

24

2. update G given α, {ξk}, {ψk}, {Σk} and G0 via slice sampling:

{wk}, {(cid:96)1:C} ∝ p ({wk}, {(cid:96)1:C}|{z1:C}, α, G0, {ξk}, {ψk}, {Σk}, {s1:C})

(a) sample the weights:

(w1, . . . , wK, w∗|{(cid:96)1:C}) ∼ Dirichlet(card({(cid:96)c = 1}), . . . , card({(cid:96)c =
K}), α)

k=1 wk < (1 − min(u1:C)):

(b) for c = 1, . . . , C: uc ∼ Unif([0, w(cid:96)c[)
(c) Set j = K. While (cid:80)j
• set j = j + 1
• sample πj ∼ Beta(1, α)
• set wj = w∗πj
• sample (ξj, ψj, Σj|G0) ∼ G0

k=K+1(1 − πk)

(cid:81)j−1

(d) for c = 1, . . . , C sample (cid:96)c given {ξk}, {ψk}, {Σk}, {wk} from:

p((cid:96)c = k) ∝ 1{wk>uc}fSN (zc, ξk, ψk, Σk)

3. for c = 1, . . . , C update sc given (cid:96)c , {ξk}, {ψk}, {Σk}:

p(sc|zc, α, G0, {ξk}, {ψk}, {Σk}, {(cid:96)1:C}, {wk}) ∝ p(sc|zc, {ξk}, {ψk}, {Σk}, (cid:96)c)
(sc|zc, {ξk}, {ψk}, {Σk}, (cid:96)c) ∼ N[0,+∞[(ac, Ac)
with Ac =

and ac = Acψ(cid:48)
(cid:96)c

(zc − ξ(cid:96)c)

Σ−1
(cid:96)c

1+ψ(cid:48)

(cid:96)c

1
Σ−1
(cid:96)c

ψ(cid:96)c

4. for k = 1, . . . , K update ξk, ψk and Σk given G0, {(cid:96)1:C} and {s1:C} from

p({ξk}, {ψk}, {Σk}|{z1:C}, α, G0, {(cid:96)1:C}, {wk}, {s1:C}):

(a) update Gk given {z1:C}, G0, {(cid:96)1:C} and {s1:C}:
0, bψ

0 , B0, Λ0, λ0) with b0 = (bξ

• G0 = sN iW (bξ

0

(cid:48) bψ
0

(cid:48))(cid:48) and B0 =

diag(Dξ

0, Dψ
0 )
• Gk = sN iW (bξ
k , Bk, Λk, λk) with bk = (bξ
• let X k be a matrix of dimension card({c|(cid:96)c = k}) × 2: X k =

k, bψ

(cid:48) bψ
k

(cid:48))(cid:48)

k

(1 sc|(cid:96)c=k)
• let Bk = (X (cid:48)
(cid:16)
• bk =

kX k + diag(D0)−1)−1
(cid:17)(cid:17)
bξ
0

bψ
0

(cid:16) 1
Dξ
0

1
Dψ
0

Bk

zc|(cid:96)c=k X k +
• λk = λ0 + card({c|(cid:96)c = k})

• Λk = Λ0+

εcε(cid:48)

c+

(bξ

k −bξ

0)(bξ

k −bξ

0)(cid:48)+

(bψ

k −bψ

0 )(bψ

k −

(cid:88)

c|(cid:96)c=k

1
Dξ
0

1
Dψ
0

bψ
0 )
with εc = zc − bξ
k − scbψ
k
(b) sample (ξk, ψk, Σk|Gk) ∼ Gk

• ((ξk, ψk)|Σk, {(cid:96)1:C}, {s1:C}, Gk) ∼ N2d
• (Σk|{(cid:96)1:C}, {s1:C}, Gk) ∼ W −1(λk, Λk)

(cid:16)

(bξ

k, bψ

k ), Bk ⊗ Σk

(cid:17)

25

A.2 Skew t-distributions mixture

Our Gibbs sampler for non parametric skew t-distributions mixture proceeds
with each of the following updates in turn:

1. update concentration parameter α given {(cid:96)1:C} using the data augmenta-

tion technique from West (1992):
α ∝ p(α|{z1:C}, G0, {ξk}, {ψk}, {Σk}, {νk}, {(cid:96)1:C}, {wk}, {s1:C}, {γ1:C}) ∝
p(α|{(cid:96)1:C})
(α, x|{(cid:96)1:C}) ∼ p(α)αK−1(α + C)xα(1 − x)C−1
(x|α, {(cid:96)1:C}) ∼ Beta(α + 1, C)
(α|x, {(cid:96)1:C}) ∼ πxGamma(a + K, b − log(x)) + (1 − πx)Gamma(a + K −
1, b − log(x)) with p(α) ∝ Gamma(a, b) and πx
1−πx

= a+k−1

C(b−log(x))

2. update G given α, {ξk}, {ψk}, {Σk}, {νk} and G0 via slice sampling:

{wk}, {(cid:96)1:C} ∝ p({wk}, {(cid:96)1:C}|{z1:C}, α, G0, {ξk}, {ψk}, {Σk}, {νk}, {s1:C}, {γ1:C})

(a) sample the weights:

(w1, . . . , wK, w∗|{(cid:96)1:C}) ∼ Dirichlet (card({(cid:96)1:C} = 1), . . . , card({(cid:96)1:C} = K), α)

k=1 wk < (1 − min(u1:C)):

(b) for c = 1, . . . , C: uc ∼ Unif([0, w(cid:96)c ])
(c) Set j = K. While (cid:80)j
• set j = j + 1
• sample πj ∼ Beta(1, α)
• set wj = w∗πj
• sample (ξj, ψj, Σj|G0) ∼ structured-Normal-invWishart(G0)
• sample νj ∼ p(νj)

k=K+1(1 − πk)

(cid:81)j−1

(d) K = j

(e) for c = 1, . . . , C sample (cid:96)c given {ξk}, {ψk}, {Σk}, {wk} from:

p((cid:96)c = k) ∝ 1{wk>uc}fSN (zc, ξk, ψk, Σk)

3. for c = 1, . . . , C update sc given (cid:96)c , {ξk}, {ψk}, {Σk}:

(sc|zc, {ξk}, {ψk}, {Σk}, (cid:96)c) ∼ N[0,+∞[(ac, Ac)
with Ac =

and ac = Acψ(cid:48)
(cid:96)c

Σ−1
(cid:96)c

1+ψ(cid:48)

(cid:96)c

1
Σ−1
(cid:96)c

ψ(cid:96)c

(zc − ξ(cid:96)c)

4. for k = 1, . . . , K update ξk, ψk and Σk given G0, {(cid:96)1:C} and {s1:C} from:

p({ξk}, {ψk}, {Σk}|{z1:C}, α, G0, {νk}, {(cid:96)1:C}, {wk}, {s1:C}, {γ1:C})
∝ p({ξk}, {ψk}, {Σk}|{(cid:96)1:C}, {s1:C}, G0):

(a) update the hyper parameters of the cluster distribution given {z1:C},

G0, {(cid:96)1:C} and {s1:C}:
0, bψ
• G0 = sN iW (bξ
0, Dψ
0 )
• Gk = sN iW (bξ

diag(Dξ

k, bψ

0 , B0, Λ0, λ0) with b0 = vec(bξ

0, bψ

0 ) and B0 =

k , Bk, Λk, λk) with bk = vec(bξ

k, bψ
k )

26

• let X k be a matrix of dimension card({c|(cid:96)c = k}) × 2: X k =

(1 sc|(cid:96)c=k)
• let Bk = (X (cid:48)
(cid:16)
• bk =

kX k + (B0)−1)−1
(cid:16) 1
1
Dψ
Dξ
0
0

bξ
0

zc|(cid:96)c=k X k +

(cid:17)(cid:17)

bψ
0

Bk

• λk = λ0 + card({(cid:96)c = k})
1
Dξ
0

• Λk = Λ0 +

εcε(cid:48)

c +

(cid:88)

c|(cid:96)c=k

bψ
0 )
with εc = zc − bξ

k − scbψ

k

(bξ

k −bξ

0)(bξ

k −bξ

0)(cid:48) +

(bψ

k −bψ

0 )(bψ

k −

1
Dψ
0

(b) sample (ξk, ψk, Σk|bk, Bk, Λk, λk) from a sN iW (bk, Bk, Λk, λk)
(cid:17)

(cid:16)

(bξ

k, bψ

k ), Bk ⊗ Σk

• ((ξk, ψk)|Σk, {(cid:96)1:C}, {s1:C}, Gk) ∼ N2d
• (Σk|{(cid:96)1:C}, {s1:C}, Gk) ∼ W −1(λk, Λk)

5. update the degrees of freedom {νk} and the scale factors {γ1:C} from the
random eﬀects representation given {ξk}, {ψk}, {Σk}, {s1:C} and {(cid:96)1:C},
sampling from:
p(νk, {γ1:C}|{ξk}, {ψk}, {Σk}, {s1:C}, {(cid:96)1:C})

(a) for k = 1, . . . , K update νk, given ξk, ψk, Σk, {s1:C} and {(cid:96)1:C},

integrating out the {γ1:C}, sampling from:
p(νk|{ξk}, {ψk}, {Σk}, {(cid:96)1:C}, {wk}, {s1:C}, α, {γ1:C})
∝ p(νk|{ξk}, {ψk}, {Σk}, {(cid:96)1:C}, {s1:C}, {γ1:C})
∝ p(νk|{ξk}, {ψk}, {Σk}, {(cid:96)1:C}, {s1:C}) (reducing conditioning on
the {γ1:C})
A Metropolis-Hastings step is required to sample from the above
distribution. We use a uniform log random-walk proposal as proposed
in Fr¨uhwirth-Schnatter and Pyne (2010):

log(νnew

k − 1) ∼ Unif([log(νk − 1) − cνk , log(νk − 1) + cνk ]

where cνk is a ﬁxed parameter of the algorithm (that can be tuned to
improve the acceptance rate of this MH step). Acceptance probability
for νnew
k

is as follow:

(cid:18)

min

1,

p(y|{ξk}, {ψk}, {Σk}, ν−k, νnew

, {(cid:96)1:C})p(νnew

k
p(y|{ξk}, {ψk}, {Σk}, {νk}, {(cid:96)1:C})p(νk)(ν−

k

)(νnew
k − 1)
k 1)

(cid:19)

(b) for c = 1, . . . , C update γc given {ξk}, {ψk}, {Σk}, {νk}, sc and (cid:96)c

sampling from:

(cid:32)

ν(cid:96)c + d + 1
2

,

ν(cid:96)c + z2

c + tr(ηcη
2

(cid:33)

(cid:48)

cΣ−1
(cid:96)c

)

p(γc|{ξk}, {ψk}, {Σk}, {νk}, sc, (cid:96)c) ∼ Gamma

with ηc = zc − ξ(cid:96)c − scψ(cid:96)c

27

A.2.1 MH within collapsed Gibbs

As an MH is used in the skew t sampler to sample {νk}, it is important to never
integrate out those {νk} in the previous steps of the Partially Collapsed Gibbs
sampler (van Dyk and Jiao, 2015). Otherwise, there is no guaranty that the
stationary distribution of the Markov chain remains unchanged (the correlation
structure of the {νk} with the other parameters is likely to not be estimated
properly). Besides, the reduced conditioning on the {γ1:C} does not change the
stationary distribution as those marginalized out {γ1:C} are sampled right after
the MH step from their full conditional distribution (van Dyk and Jiao, 2015).

B Parameter estimation for Normal inverse-Wishart
and structured Normal inverse-Wishart dis-
tributions

B.1 Maximum Likelihood Estimation

B.1.1 Maximum Likelihood estimators for Normal inverse-Wishart

Let observations (µi, Σi) follow a Normal inverse-Wishart distribution for i =
1 . . . n:

(µi, Σi) ∼ N iW (µ0, κ0, Λ0, λ0)

The likelihood is:

p ({µi}1:n, {Σi}1:n|µ0, κ0, Λ0, λ0) =

(2π)− d

2 |Σi|− λ0+d+1

2

n
(cid:89)

i=1

(cid:40)

(cid:20)

1
2

λ0
2

2− λ0d

2

|Λ0|
Γd( λ0
2 )

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
κ0

− 1
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Σi

exp

−

tr (cid:0)Λ0Σ−1

(cid:1) −

i

(µi − µ0)(cid:48)Σ−1

i

(µi − µ0)

κ0
2

(cid:21) (cid:41)

Taking the partial derivatives of the loglikelihood with respect of the four pa-
rameters µ0, κ0, Λ0, λ0 and setting each of them to zero gives the following
system:

µ0 =

(cid:48)
µ

iΣ−1
i

Σ−1
i

(cid:33)−1

(cid:32) n
(cid:88)

i=1

n
(cid:88)

i=1

1
nd

1
κ0

=

Λ0 = nλ0

n
(cid:88)

i=1
(cid:32) n
(cid:88)

i=1






(µi − µ0)(cid:48)Σ−1

i

(µi − µ0)

(cid:33)−1

Σ−1
i

28

0 = −

log (|Σi|) −

log(2) +

log (|Λ0|) −

nd
2

n
2

(cid:19)

n
2

(cid:122)d

(cid:18) λ0
2

1
2

n
(cid:88)

i=1

where (cid:122)d(x) =
tive of the logarithm of the d-dimensional Gamma function).

log(Γd(x)) is the d-dimensional digamma function (the deriva-

d
dx

NB: The above solution are obtained using the two following identities:

log(|X|) =

d
dX

X −1 and
Hence the MLE solutions verify:

tr(XA) = A(cid:48) if X is deﬁnite-positive

d
dX






(cid:99)µ0 =

n
(cid:88)

i=1

(cid:48)
µ

iΣ−1
i

Σ−1
i

(cid:32) n
(cid:88)

i=1

(cid:33)−1

(cid:99)κ0 = nd

(µi − (cid:99)µ0)(cid:48)Σ−1

i

(µi − (cid:99)µ0)

(cid:32) n
(cid:88)

i=1

(cid:32)

(cid:33)

(cid:99)λ0
2

(cid:122)d

(cid:99)Λ0 = n(cid:99)λ0

1
n

n
(cid:88)

i=1

Σ−1
i

(cid:33)−1

(cid:32) n
(cid:88)

i=1

= −

log (|Σi|) + d log

− log

(cid:33)−1

(cid:32)

(cid:33)

n(cid:99)λ0
2

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

Σ−1
i

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

under the constraint (cid:99)λ0 > d + 1 (in which case there should a unique solution
(cid:99)λ0).

B.1.2 Maximum Likelihood estimators for structured Normal inverse-

Wishart

Let observations (ξi, ψi, Σi) follow a structured Normal inverse-Wishart distri-
bution (sN iW ) for i = 1 . . . n:

(ξi, ψi, Σi) ∼ sN iW (ξ0, ψ0, B0, Λ0, λ0)

The likelihood is:

p ({ξi}1:n, {ψi}1:n, {Σi}1:n|µ0, B0, Λ0, λ0) =

(2π)− d

2 |Σi|− λ0+d+1

2

λ0
2

2− λ0d

2

|Λ0|
Γd( λ0
2 )

(cid:12)
(cid:12)B−1

0 ⊗ Σi

(cid:12)
(cid:12)

− 1
2

(cid:40)

n
(cid:89)

i=1

−

1
2

(cid:104)

exp

−

1
2

tr (cid:0)Λ0Σ−1

(cid:1)

i

(µi − µ0)(cid:48) (cid:0)B0 ⊗ Σ−1

i

(cid:1) (µi − µ0)

(cid:41)
(cid:105)

i ψ(cid:48)

i)(cid:48) and µ0 = (ξ(cid:48)

where µi = (ξ(cid:48)
Taking the partial derivatives of the loglikelihood with respect of the four pa-
rameters µ0, B0, Λ0, λ0 and setting each of them to zero gives the following

0 ψ(cid:48)

0)(cid:48)

29

system:

µ0 =

(cid:48)
µ

iΣ−1
i

Σ−1
i

(cid:33)−1

(cid:32) n
(cid:88)

i=1

n
(cid:88)

i=1

Λ0 = nλ0

(cid:33)−1

(cid:32) n
(cid:88)

i=1

Σ−1
i






0 =

n
(cid:88)

i=1

(cid:18) d
dB0

(cid:0)log (cid:0)(cid:12)

(cid:12)B0

−1 ⊗ Σi

(cid:1)(cid:1) +

(cid:12)
(cid:12)

(cid:0)(µi − µ0)(cid:48) (cid:0)B0 ⊗ Σ−1

i

(cid:1) (µi − µ0)(cid:1)

(cid:19)

d
dB0

0 = −

log (|Σi|) −

log(2) +

log (|Λ0|) −

nd
2

n
2

(cid:19)

n
2

(cid:122)d

(cid:18) λ0
2

1
2

n
(cid:88)

i=1

d
where (cid:122)d(x) =
dx
logarithm of the Gamma function).

log(Γd(x)) is the digamma function (the derivative of the

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:18) d
dB0

d
dB0

=

(cid:0)log (cid:0)(cid:12)

(cid:12)B0

−1 ⊗ Σi

(cid:1)(cid:1) +

(cid:12)
(cid:12)

(cid:0)(µi − µ0)(cid:48) (cid:0)B0 ⊗ Σ−1

i

(cid:1) (µi − µ0)(cid:1)

(cid:19)

(cid:16)

log

(cid:16)

|B0|−d |Σi|2(cid:17)(cid:17)

+

(cid:0)(µi − µ0)(cid:48) (cid:0)B0 ⊗ Σ−1

i

(cid:1) (µi − µ0)(cid:1)

d
dB0
n
(cid:88)

i=1

d
dB0

= − nd

(log (|B0|)) +

d
dB0

n
(cid:88)

i=1

d
dB0

(cid:0)tr (cid:0)(µi − µ0)(cid:48) (cid:0)B0 ⊗ Σ−1

i

(cid:1) (µi − µ0)(cid:1)(cid:1)

= − ndB0

−1 +

(cid:0) ξi − ξ0 ψi − ψ0

(cid:1)(cid:48) (cid:0)Σ−1

i

(cid:1) (cid:0) ξi − ξ0 ψi − ψ0

(cid:1)

= − ndB0

−1 +

(cid:19)

(cid:18) ξ(cid:48)
ψ(cid:48)

i − ξ(cid:48)
0
i − ψ(cid:48)
0

(cid:0)Σ−1

i

(cid:1) (cid:0) ξi − ξ0 ψi − ψ0

(cid:1)

n
(cid:88)

i=1
n
(cid:88)

i=1

So if the above expression is zero, we get:

(cid:99)B0 = nd

(cid:32) n
(cid:88)

i=1

(cid:19)

(cid:18) ξ(cid:48)
ψ(cid:48)

i − ξ(cid:48)
0
i − ψ(cid:48)
0

(cid:0)Σ−1

i

(cid:1) (cid:0) ξi − ξ0 ψi − ψ0

(cid:1)

(cid:33)−1

So MLE solution for sN iW are:

30






(cid:99)µ0 =

n
(cid:88)

i=1

(cid:48)
µ

iΣ−1
i

Σ−1
i

(cid:32) n
(cid:88)

i=1

(cid:33)−1

(cid:32) n
(cid:88)

(cid:32)

i=1

i − ξ(cid:48)
ξ(cid:48)
0
i − ψ(cid:48)
ψ(cid:48)
0

(cid:33)

(cid:1) (cid:16)

(cid:0)Σ−1

i

ξi − ξ0 ψi − ψ0

(cid:33)−1

(cid:17)

= −

log (|Σi|) + d log

− log

(cid:32)

(cid:33)

n(cid:99)λ0
2

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

Σ−1
i

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:99)B0 = nd

(cid:32)

(cid:33)

(cid:99)λ0
2

(cid:122)d

(cid:99)Λ0 = n(cid:99)λ0

1
n

n
(cid:88)

i=1

Σ−1
i

(cid:33)−1

(cid:32) n
(cid:88)

i=1

B.2 Expectation-Maximization algorithms (MLE & M AP )

B.2.1 MLE estimation via an E-M algorithm

The latent variables used in the EM algorithm for estimating a ﬁnite mixture
model over the MCMC draws for the parameters ξi, ψi and Σi are the allocation
variables (cid:96)i, with i = 1..N the number of (MCMC) observations. An (MCMC)
observation is then xi = (ξi, ψi, Σi). Let K be the number of components in
the mixture model:

p(xi|K, θ{1:K}) =

πkfθ(cid:96)i

(xi|(cid:96)i, θ{1:K})

for i = 1 . . . N

K
(cid:88)

k=1

where fθk is the parametric density function of a cluster: a sN iW density

function with parameters θk = (ξk, ψk, Bk, Λk, λk).
(cid:16)

At iteration t, the EM algorithm maximizes Q

θ{1:K}

(cid:17)

(cid:12)
(cid:12)θ(t−1)
(cid:12)

{1:K}

for θ{1:K}

31

with:
(cid:16)

Q

θ{1:K}

(cid:17)

(cid:12)
(cid:12)θ(t−1)
(cid:12)

{1:K}

(cid:104)

= E

log (cid:0)p(x{1:n}, (cid:96){1:N }|K, θ{1:K})(cid:1)(cid:12)

(cid:88)

(cid:16)

log

=

p(x{1:n}, (cid:96){1:N }|K, θ(t−1)
{1:K})

(cid:105)

(cid:12) θ(t−1)
(cid:17)

{1:K}

(cid:96){1:N }

K
(cid:88)

n
(cid:88)

k=1

i=1

K
(cid:88)

n
(cid:88)

(cid:34)

=

=

k=1

i=1
λkdr(t−1)
ik
2
r(t−1)
ik
2
r(t−1)
ik
2

−

−

−

r(t−1)
ik

log(πk) +

r(t−1)
ik

log (cid:0)p(xi|K, θ{1:K})(cid:1)

K
(cid:88)

n
(cid:88)

k=1

i=1

r(t−1)
ik

log(πk) −

λk + d + 1
2

r(t−1)
ik

log(|Σi|)

log(2) − r(t−1)

ik

log(Γd(

)) +

λk
2

λk

r(t−1)
ik
2

log(|Λk|)

log(|B−1

k ⊗ Σi|) −

tr(ΛkΣ−1

)

r(t−1)
ik
2

i
(cid:35)

(µi − µk)(cid:48)(Bk ⊗ Σ−1

i

)(µi − µk)

with r(t)

ik = p((cid:96)i = k|xi, θ(t)

{1:k}) =

(xi)

πkfθ(t)
j=1 πjfθ(t)

k

j

(cid:80)K

(xi)

1. Initialization

θ(0)
k

is initialized randomly (πk are initialized at 1/K)

2. E step at iteration t

Compute the membership weights r(t−1)
for each cluster k = 1 . . . K:

ik

for each observation i = 1 . . . N

r(t−1)
ik

= p

(cid:96)i = k

(cid:16)

(cid:12)
(cid:12)xi, θ(t−1)
(cid:12)

{1:k}

(cid:17)

=

(xi)

πkfθ(t−1)
j=1 πjfθ(t−1)

k

j

(cid:80)K

(xi)

3. M step at iteration t

Update the parameters:

• θ(t)

k are updated with their weighted Maximum Likelihood Estima-

32

tors for each k:


(cid:99)µk =

n
(cid:88)

i=1

r(t−1)
ik µ

(cid:48)

iΣ−1
i

r(t−1)
ik Σ−1

i

(cid:33)−1

(cid:32) n
(cid:88)

i=1

(cid:32)

(cid:99)Bk = Nkd

(cid:32) n
(cid:88)

i=1

r(t−1)
ik

(cid:48)

(cid:33)

ξ(cid:48)
i − (cid:99)ξk
ψ(cid:48)
i − (cid:99)ψk

(cid:48)

(cid:1) (cid:16)

(cid:0)Σ−1

i

ξi − (cid:99)ξk ψi − (cid:99)ψk

(cid:33)−1

(cid:17)

(cid:32)

(cid:33)

(cid:99)λk
2

(cid:122)d

= −

1
Nk

n
(cid:88)

i=1

r(t−1)
ik

log (|Σi|) + d log

(cid:32)

(cid:33)

Nk (cid:99)λk
2

− log

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

r(t−1)
ik Σ−1

i

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)




(cid:99)Λk = Nk (cid:99)λk

r(t−1)
ik Σ−1

i

(cid:32) n
(cid:88)

i=1

(cid:33)−1

• π(t)

k are updated with Nk/n, Nk = (cid:80)n

i=1 rik

4. Repeat 2. and 3. until convergence

Convergence is reached when the incomplete log-likelihood l(t) is unchanged
between two consecutive iterations t and t + 1 of the 2. and 3. steps:

l(t) = log

p(x{1:N }|K, θ(t)

(cid:16)

(cid:17)
{1:K})

=

n
(cid:88)

i=1

log

(cid:32) K
(cid:88)

k=1

(cid:33)

πkp(xi|K, θ(t)

{1:K})

B.2.2 M AP estimation via E-M algorithm

In order to avoid degenrate covariance matrices (for instance when K is set to too
many clusters in the EM algorithm), it can be useful to replace MLE estimation
with Maximum A Posteriori (M AP ) estimations (Fraley and Raftery, 2007).

To perform a M AP estimation instead of a MLE estimation as in section
B.2.1, the E-step of the algorithm is unchanged, but the M-step now maximizes
the following Q function:

(cid:16)

Q

θ{1:K}

(cid:17)

(cid:12)
(cid:12)θ(t−1)
(cid:12)

{1:K}

(cid:104)

= E

log (cid:0)p(θ{1:K})p(x{1:n}, (cid:96){1:n}|K, θ{1:K})(cid:1)(cid:12)

(cid:105)

(cid:12) θ(t−1)

{1:K}

(cid:88)

(cid:16)

(cid:16)

log

=

(cid:96){1:N }

p(x{1:n}, (cid:96){1:n}|K, θ(t−1)
{1:K})

+ log(p(θ{1:K}))

(cid:17)(cid:17)

= log(p(θ{1:K})) +

r(t−1)
ik

log(πk) +

r(t−1)
ik

log (cid:0)p(xi|K, θ{1:K})(cid:1)

K
(cid:88)

n
(cid:88)

k=1

i=1

K
(cid:88)

N
(cid:88)

k=1

i=1

We use the following priors :

• a Dirichlet prior over the cluster weigths π{1:K} with all parameters equal
to the same α ( if α = 1, then this is equivalent to a uniform prior over
the K − 1 simplex):

(π1, . . . , πK) ∼ Dir(α)

33

And for each k:

• a Normal-Wishart empirical bayes prior on (µk, Bk):

(µk, Bk) ∼ N W (m, κ0, C, 4)

µk|m, κ0, Bk, Σ{1:n} ∼N

m,

Bk ⊗



(cid:32)

1
κ0

(cid:33)−1


Σ−1
i

1
n

n
(cid:88)

i=1

Bk|C ∼W (C, 4)

with m = µ{1:n}, C = 100I 2 and L = (S(ξ) + S(ψ))/2 (where S(ξ) =
diag(var(ξ{1:n})) and S(ψ) = diag(var(ψ{1:n}))) and κ0 = 0.01 for in-
stance. The harmonic mean is used as an empirical bayes prior for the
bloc variance matrix.
One can also specify a vague prior on µk: µk ∼ U 2d
]−∞,+∞[ (which simpliﬁes
the ξ and ψ M AP estimators, as long as no cluster has an exactly null 0
contribution Nk)

• a Wishart priors on Λk:

Λk ∼ W (L, d + 2)
with L = (S(ξ) + S(ψ))/2 (where S(ξ) = diag(var(ξ{1:n})) and S(ψ) =
diag(var(ψ{1:n})))

• an Exponential prior on λk under the constraint that λk ≥ d + 1 :

λk − (d + 1) ∼ Exp(1)

Q(θ|θ(t−1)) =

−

log

Bk ⊗

(cid:32) n
(cid:88)

i=1

Σ−1
i



 −

(cid:33)−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

κ0
2n

(cid:32)

(cid:33)

(µk − m)(cid:48)

Bk ⊗

Σ−1
i

(µk − m)

n
(cid:88)

i=1
(cid:35)

+

log(|Bk|) −

tr (cid:0)C−1Bk

(cid:1) +

log(|Λk|) −

tr (cid:0)L−1Λk

(cid:1) − λk

1
2

1
2

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
2

(cid:34)

K
(cid:88)

k=1

1
2

1
2

K
(cid:88)

n
(cid:88)

(cid:34)

i=1
k=1
λkdr(t−1)
ik
2
r(t−1)
ik
2
r(t−1)
ik
2

+

−

−

−

r(t−1)
ik

log(πk) −

λk + d + 1
2

r(t−1)
ik

log(|Σi|)

log(2) − r(t−1)

ik

log(Γd(

)) +

λk
2

λk

r(t−1)
ik
2

log(|Λk|)

log(|Bk ⊗ Σi|) −

r(t−1)
ik
2

)

tr(ΛkΣ−1
i
(cid:35)
)(µi − µk)

(µi − µk)(cid:48)(Bk ⊗ Σ−1

i

+ constant

34

dQ(θ|θ(t−1))
dλk

= −

(cid:122)d

−

1
2

n
(cid:88)

i=1

r(t−1)
ik

log (|Σi|) +

(cid:32)

(cid:33)

Nkd
2

log

Nk (cid:99)λk
2

−

log

r(t−1)
ik Σ−1

i

− 1

Nk
2

Nk
2

(cid:32)

(cid:33)

(cid:99)λk
2
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

Nkd
2
(cid:18) ξ(cid:48)
ψ(cid:48)

−

κ0
2n

i=1
k − m(ξ) (cid:48)
k − m(ψ) (cid:48)
n
(cid:88)

i=1

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
r(t−1)
ik
2

(cid:19) n
(cid:88)

i=1

dQ(θ|θ(t−1))
dBk

d
2

=

Bk +

Bk −

n
(cid:88)

(cid:18) ξ(cid:48)
ψ(cid:48)

i − ξ(cid:48)
k
i − ψ(cid:48)
k

(cid:19)

(cid:0)Σ−1

i

(cid:1) (cid:0) ξi − ξk ψi − ψk

Σ−1
i

(cid:0) ξk − m(ξ) ψk − m(ψ) (cid:1) +

Bk −

C−1

1
2

(cid:1)

1
2

dQ(θ|θ(t−1))
dΛk

=

Nkλk
2

Λ−1

k −

1
2

r(t−1)
ik Σ−1

i +

Λ−1

k −

L−1

1
2

1
2

(cid:1) (cid:16)

(cid:0)Σ−1

i

M AP

ξi − (cid:99)ξk

ψi − (cid:99)ψk

M AP (cid:17)

r(t−1)
ik

log (|Σi|) − Nkd log

 + Nk log

rikΣ−1

i

+ 2

M AP







Nk (cid:99)λk
2

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The M AP estimators of θk|θ(t−1)

are thus:

k

M AP =

(cid:99)πk

Nk + α − 1
n + K(α − 1)

M AP =

(cid:99)µk

(cid:48)
m

Σ−1

i + r(t−1)

ik µ

(cid:48)

iΣ−1
i

Σ−1

i + r(t−1)

ik Σ−1

i

(cid:33)−1

n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

κ0
n



M AP

n
(cid:88)

i=1

(cid:48)



r(t−1)
ik

ξ(cid:48)
i − (cid:99)ξk
ψ(cid:48)
i − (cid:99)ψk
ξk − m(ξ) ψk − m(ψ) (cid:17)

M AP

(cid:48)





(cid:35)−1

(cid:34)

= (Nkd + d + 1)

C−1 +

M AP

(cid:99)Bk

(cid:32)

+

κ0
n

ξ(cid:48)
k − m(ξ) (cid:48)
ψ(cid:48)
k − m(ψ) (cid:48)


M AP

0 = Nk(cid:122)d



(cid:99)λk

2

(cid:33) n
(cid:88)

i=1

(cid:16)

Σ−1
i



 +

n
(cid:88)

i=1

M AP

(cid:16)

(cid:99)Λk

=

Nk (cid:99)λk

M AP

(cid:32)

(cid:17)

+ 1

L−1 +

(cid:33)−1

r(t−1)
ik Σ−1

i

n
(cid:88)

i=1






with Nk = (cid:80)n

i=1 r(t−1)

ik

1. Initialization

2. E step

θ(0)
k are initialized randomly (πk are initialized at 1/K)

Compute the membership weights r(t−1)
for each cluster k = 1 . . . K:

ik

for each observation i = 1 . . . N

35

3. M step

Update the parameters:

π(t)
k =

Nk + α − 1
n + K(α − 1)

• θk are updated with their MAP estimation for each k:






µ(t)

k =

m

(cid:48)

Σ−1

i + r(t−1)

ik µ

(cid:48)

iΣ−1
i

Σ−1

i + r(t−1)

ik Σ−1

i

(cid:33)−1

n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

κ0
n

(cid:32)

B(t)

k = (Nkd + d + 1)

C−1 +

r(t−1)
ik

(cid:34)

n
(cid:88)

i=1

ξ(cid:48)
i − ξ(cid:48)
0
i − ψ(cid:48)
ψ(cid:48)
0

(cid:33)

(cid:1) (cid:16)

(cid:0)Σ−1

i

ξi − ξ0 ψi − ψ0

(cid:17)

ξ(t)
k
ψ(t)
k
(cid:33)

(cid:32)

+

κ0
n

(cid:32)

λ(t)
k
2

(cid:122)d

= −

1
Nk

n
(cid:88)

i=1
(cid:32)

(cid:48) − m(ξ) (cid:48)
(cid:48) − m(ψ) (cid:48)

(cid:33) n
(cid:88)

i=1

(cid:16)

Σ−1
i

k − m(ξ) ψ(t)
ξ(t)

r(t−1)
ik

log (|Σi|) + d log

(cid:35)−1

k − m(ψ) (cid:17)
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− log

(cid:33)

Nkλ(t)
k
2

(cid:32)

n
(cid:88)

i=1

r(t−1)
ik Σ−1

i

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 = (Nkλ(t)
Λ(t)

k + 1)

L−1 +

r(t−1)
ik Σ−1

i

(cid:33)−1

n
(cid:88)

i=1

with Nk = (cid:80)n

i=1 r(t−1)

ik

4. Repeat 2. and 3. until convergence

Convergence is reached when the incomplete log-likelihood l(t) is unchanged
between two consecutive iterations t and t + 1 of the 2. and 3. steps:

l(t) = log

p(x{1:N }|K, θ(t)

(cid:16)

(cid:17)
{1:K})

=

n
(cid:88)

i=1

log

(cid:32) K
(cid:88)

k=1

(cid:33)

πkp(xi|K, θ(t)

{1:K})

C Limited F-measure

First let’s start from a reference partition G = {g1, . . . , gm} and an estimated
partition H = {h1, . . . , hn}. In order to compute a F-measure limited to the
clusters that have less than p observations, we need to deﬁne two subartition G(p)
and H (p) respectively. Let’s denote gq(cid:48) the clusters from the reference partition
(cid:12)
G that have less than p observations: {gq(cid:48)} = (cid:8)gq
(cid:12) |gq| < p(cid:9). Now let’s consider
all the estimated clusters that each contains at least one observation included
in this subpartition. This gives the estimated limited partition H (p) = {h(p)
r } =
(cid:12)
(cid:8)hr
(cid:12) ∃ c ∈ hr ∩ {gq(cid:48)}(cid:9). Finally, let’s consider the reference limited partition for
the observations included in H p: G(p) is the reference partition induced by
{(cid:96)c | c ∈ H (p)}. The limited F-measure is then deﬁned as follows:

36

Flim(H, G, p) = Ftot(H (p), G(p)) =

1
g∈G(p) |g|

(cid:80)

m
(cid:88)

g∈G(p)

|g| max
h∈H (p)

F(h, g)

Figure S1 displays the mean of this limited F-measure for several diﬀerent
limit maximum size for small clusters. Thus it seems that the use of an infor-
mative prior in the sequential strategy always improves the clustering accuracy
for small sized clusters.

Figure S1: Mean limited F-measure according to the limit size of rare popula-
tions, over 300 simulations

D ﬂowMeans applied to the DALIA-1 trial

Here we provide additional representation of the results from ﬂowMeans applied
to the DALIA-1 trial and compared to NPﬂow for the eﬀector CD4+ T-cell
population. Overall the results of ﬂowMeans are comparable to those of NPﬂow
without the sequential posterior approximation strategy, as can be seen from
Figures S2, S3 and S4, while the sequential strategy outperforms both.

37

Figure S2: Limited F-measures for the eﬀector T-cell population from the
DALIA-1 trial two weeks after HAART interruption for NPﬂow with or without
the sequential strategy and for ﬂowMeans, compared to manual gating.

Figure S3: Boxplots of the limited F-measures for the eﬀector CD4+ T-cell
population from the DALIA-1 trial two weeks after HAART interruption for
NPﬂow with or without the sequential strategy and for ﬂowMeans, compared
to manual gating.

38

Figure S4: Paired proportions of eﬀector CD4+ T-cells in the DALIA-1 trial
before and after HAART interruption from manual gating.

39

