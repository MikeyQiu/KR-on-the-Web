6
1
0
2
 
l
u
J
 
6
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
0
0
5
0
.
4
0
6
1
:
v
i
X
r
a

LSTM-CF: Unifying Context Modeling and
Fusion with LSTMs for RGB-D Scene Labeling

Zhen Li1, Yukang Gan2, Xiaodan Liang2, Yizhou Yu1,
Hui Cheng2, and Liang Lin2(cid:63)

1 Department of Computer Science, The University of Hong Kong, Hong Kong,
lizhen36@hku.hk,yizhouy@acm.org
2 School of Data and Computer Science, Sun Yat-sen University, Guangzhou,
ganyk@mail2.sysu.edu.cn,xdliang328@gmail.com
chengh9@mail.sysu.edu.cn,linliang@ieee.org

Abstract. Semantic labeling of RGB-D scenes is crucial to many in-
telligent applications including perceptual robotics. It generates pixel-
wise and ﬁne-grained label maps from simultaneously sensed photomet-
ric (RGB) and depth channels. This paper addresses this problem by i)
developing a novel Long Short-Term Memorized Context Fusion (LSTM-
CF) Model that captures and fuses contextual information from multiple
channels of photometric and depth data, and ii) incorporating this model
into deep convolutional neural networks (CNNs) for end-to-end training.
Speciﬁcally, contexts in photometric and depth channels are, respectively,
captured by stacking several convolutional layers and a long short-term
memory layer; the memory layer encodes both short-range and long-
range spatial dependencies in an image along the vertical direction. An-
other long short-term memorized fusion layer is set up to integrate the
contexts along the vertical direction from diﬀerent channels, and per-
form bi-directional propagation of the fused vertical contexts along the
horizontal direction to obtain true 2D global contexts. At last, the fused
contextual representation is concatenated with the convolutional features
extracted from the photometric channels in order to improve the accu-
racy of ﬁne-scale semantic labeling. Our proposed model has set a new
state of the art, i.e., 48.1% and 49.4% average class accuracy over 37
categories (2.2% and 5.4% improvement) on the large-scale SUNRGBD
dataset and the NYUDv2 dataset, respectively.

Keywords: RGB-D scene labeling, image context modeling, long short-
term memory, depth and photometric data fusion

1 Introduction

Scene labeling, also known as semantic scene segmentation, is one of the most
fundamental problems in computer vision. It refers to associating every pixel in

(cid:63) The corresponding author is Liang Lin. This work was support by Projects on Fac-
ulty/ Student Exchange and Collaboration Scheme between the Higher Education in
Hong Kong and the Mainland, Guangzhou Science and Technology Program under
grant 1563000439, and Fundamental Research Funds for the Central Universities.

2

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Fig. 1. An illustration of global context modeling and fusion for RGB-D images. Our
LSTM-CF model ﬁrst captures vertical contexts through a memory network layer en-
coding short- and long-range spatial dependencies along the vertical direction. After a
concatenation operation (denoted by “C”) over photometric and depth channels, our
model utilizes another memory network layer to fuse vertical contexts from all chan-
nels in a data-driven way and performs bi-directional propagation along the horizontal
direction to obtain true 2D global contexts. Best viewed in color.

an image with a semantic label, such as table, road and wall, as illustrated in
Fig. 1. High-quality scene labeling can be beneﬁcial to many intelligent tasks,
including robot task planning [1], pose estimation [2], plane segmentation [3],
context-based image retrieval [4], and automatic photo adjustment [5].

Previous work on scene labeling can be divided into two categories accord-
ing to their target scenes: indoor and outdoor scenes. Compared with outdoor
scene labeling [6,7,8], indoor scene labeling is more challenging due to a larger
set of semantic labels, more severe object occlusions, and more diverse object
appearances [9]. For example, indoor object classes, such as beds covered with
diﬀerent sheets and various appearances of curtains, are much harder to char-
acterize than outdoor classes, e.g., roads, buildings, and sky, through photo-
metric channels only. Recently, utilizing depth sensors to augment RGB data
have eﬀectively improved the performance of indoor scene labeling because the
depth channel complements photometric channels with structural information.
Nonetheless, two key issues remain open in the literature of RGB-D scene label-
ing.

(I) How to eﬀectively represent and fuse the coexisting depth and
photometric (RGB) data For data representation, a batch of sophisticated
hand-crafted features have been developed in previous methods. Such hand-
crafted features are somewhat ad hoc and less discriminative than those RGB-D
representations learned using convolutional neural networks (CNNs) [10,11,12,13,14].
However, in these CNN-related works, the fusion of depth and photometric data
has often been oversimpliﬁed. For instance, in [13,14], two independent CNNs are
leveraged to extract features from depth and photometric data separately, and
such features are simply concatenated before used for ﬁnal classiﬁcation. Over-
looking the strong correlation between depth and photometric channels could
inevitably harm semantic labeling.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

3

(II) How to capture global scene contexts during feature learning
Current CNN-based scene labeling approaches can only capture local contextual
information for every pixel due to their restricted receptive ﬁelds, resulting in
suboptimal labeling results. In particular, long-range dependencies sometimes
play a key role in distinguishing among diﬀerent objects having similar appear-
ances, e.g., labeling “ceiling” and “ﬂoor” in Fig. 1, according to the global scene
layout. To overcome this issue, graphical models, such as a conditional random
ﬁeld [9,11] or a mean-ﬁeld approximation [15], have been applied to improve
prediction results in a post-processing step. These methods, however, separate
context modeling from convolutional feature learning, which may give rise to
suboptimal results on complex scenes due to less discriminative feature repre-
sentation [16]. An alternative class of methods adopts cascaded recurrent neural
networks (RNNs) with gate structures, e.g., long short-term memory (LSTM)
networks, to explicitly strengthen context modeling [16,17,18]. In these methods,
the long- and short-range dependencies can be well memorized by sequentially
running the network over individual pixels.

To address the aforementioned challenges, this paper proposes a novel Long
Short-Term Memorized Context Fusion (LSTM-CF) model and demonstrates
its superiority in RGB-D scene labeling. Fig. 1 illustrates the brief idea of using
memory networks for context modeling and fusion of diﬀerent channels. Our
LSTM-CF model captures 2D dependencies within an image by exploiting the
cascaded bi-directional vertical and horizontal RNN models as introduced in
[19].

Our method constructs HHA images [13] for the depth channel through ge-
ometric encoding, and uses several convolutional layers for extracting features.
Inspired by [19], these convolutional layers are followed by a memorized context
layer to model both short-range and long-range spatial dependencies along the
vertical direction. For photometric channels, we generate convolutional features
using the Deeplab network [12], which is also followed by a memorized context
layer for context modeling along the vertical direction. Afterwards, a memorized
fusion layer is set up to integrate the contexts along the vertical direction from
both photometric and depth channels, and perform bi-directional propagation of
the fused vertical contexts along the horizontal direction to obtain true 2D global
contexts. Considering the features diﬀerences, e.g., signal frequency and other
characteristics (color/geometry) [20], our fusion layer facilitates deep integration
of contextual information from multiple channels in a data-driven manner rather
than simply concatenating diﬀerent feature vectors. Since photometric channels
usually contain ﬁner details in comparison to the depth channel [20], we further
enhance the network with cross-layer connections that append convolutional fea-
tures of the photometric channels to the fused global contexts before the ﬁnal
fully convolutional layer, which predicts pixel-wise semantic labels. Various lay-
ers in our LSTM-CF model are tightly integrated, and the entire network is
amenable to end-to-end training and testing.

In summary, this paper has the following contributions to the literature of

RGB-D scene labeling.

4

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

– It proposes a novel Long Short-Term Memorized Context Fusion (LSTM-
CF) Model, which is capable of capturing image contexts from a global
perspective and deeply fusing contextual information from multiple sources
(i.e., depth and photometric channels).

– It proposes to jointly optimize LSTM layers and convolutional layers for
achieving better performance in semantic scene labeling. Context modeling
and fusion are incorporated into the deep network architecture to enhance
the discriminative power of feature representation. This architecture can also
be extended to other similar tasks such as object/part parsing.

– It is demonstrated on the large-scale SUNRGBD benchmark (including 10355
images) and canonical NYUDv2 benchmark that our method outperforms
existing state-of-the-art methods. In addition, it is found that our scene la-
beling results can be leveraged to improve the groundtruth annotations of
newly captured 3943 RGB-D images in SUNRGBD dataset.

2 Related work

Scene Labeling: Scene labeling has caught researchers’ attention frequently
[6,11,12,16,17,18,21] in recent years. Instead of extracting features from over-
segmented images, recent methods usually utilize powerful CNN layers as the
feature extractor, taking advantage of fully convolutional networks (FCNs) [10]
and its variants [22] to obtain pixel-wise dense features. Another main challenge
for scene labeling is the fusion of local and global contexts, i.e., taking advantage
of global contexts to reﬁne local decisions. For instance, [6] exploits families of
segmentations or trees to generate segment candidates. [23] utilizes an inference
method based on graph cut to achieve image labeling. A pixel-wise conditional
random forest is used in [11,12] to directly optimize a deep CNN-driven cost
function. Most of the above models improve accuracy through carefully designed
processing on the predicted conﬁdence map instead of proposing more powerful
discriminative features, which usually results in suboptimal prediction results
[16]. The topological structure of recurrent neural networks (RNNs) is used to
model short- and long-range dependencies in [16,18]. In [17], a multi-directional
RNN is leveraged to extract local and global contexts without using a CNN,
which is well suited for low-resolution and relatively simple scene labeling prob-
lems. In contrast, our model can jointly optimize LSTM layers and convolutional
layers to explicitly improve discriminative feature learning for local and global
context modeling and fusion.

Scene Labeling in RGB-D images: With more and more convenient ac-
cess to aﬀordable depth sensors, scene labeling in RGB-D images [9,13,14,24,25,26]
enables a rapid progress of scene understanding. Various sophisticated hand-
crafted features are utilized in previous state-of-the-art methods. Speciﬁcally,
kernel descriptions based on traditional multi-channel features, such as color,
depth gradient, and surface normal, are used as photometric and depth features
[24]. A rich feature set containing various traditional features, e.g., SIFT, HOG,
LBP and plane orientation, are used as local appearance features and plane ap-
pearance features in [9]. HOG features of RGB images and HOG+HH (histogram

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

5

of height) features of depth images are extracted as representations in [25] for
training successive classiﬁers. In [27], proposed distance-from-wall features are
exploited to improve scene labeling performance. In addition, an unsupervised
joint feature learning and encoding model is proposed for scene labeling in [26].
However, due to the limited number of RGB-D images, deep learning for scene
labeling in RGB-D images was not as appealing as that for RGB images. The
release of the SUNRGBD dataset, which includes most of the previously popular
datasets, may have changed this situation [13,14].

Another main challenge imposed by scene labeling in RGB-D images is the
fusion of contextual representations of diﬀerent sources (i.e., depth and photo-
metric data). For instance, in [13,14], two independent CNNs are leveraged to
extract features from the depth and photometric data separately, which are then
simply concatenated for class prediction. Ignoring the strong correlation between
depth and photometric channels usually negatively aﬀects semantic labeling. In
contrast, instead of simply concatenating features from multiple sources, the
memorized fusion layer in our model facilitates the integration of contextual
information from diﬀerent sources in a data-driven manner,

RNN for Image Processing: Recurrent neural networks (RNNs) represent
a type of neural networks with loop connections [28]. They are designed to cap-
ture dependencies across a distance larger than the extent of local neighborhoods.
In previous work, RNN models have not been widely used partially due to the
diﬃculty to train such models, especially for sequential data with long-range de-
pendencies [29]. Fortunately, RNNs with gate and memory structures, e.g., long
short-term memory (LSTM) [30], can artiﬁcially learn to remember and forget
information by using speciﬁc gates to control the information ﬂow. Although
RNNs have an outstanding capability to capture short-range and long-range de-
pendencies, there exist problems for applying RNNs to image processing due to
the fact that, unlike data in natural language processing (NLP) tasks, images
do not have a natural sequential structure. Thus, diﬀerent strategies have been
proposed to overcome this problem. Speciﬁcally, in [19], cascaded bi-directional
vertical and horizonal RNN layers are designed for modeling 2D dependencies in
images. A multi-dimensional RNN with LSTM unit has been applied to hand-
writing [31]. A parallel multi-dimensional LSTM for image segmentation has
been proposed in [32]. In this paper, we propose an LSTM-CF model consisting
of memorized context layers and a memorized fusion layer to capture image con-
texts from a global perspective and fuse contextual representations from diﬀerent
sources.

3 LSTM-CF Model

As illustrated in Fig. 2, our end-to-end LSTM-CF model for RGB-D scene label-
ing consists of four components, layers for vertical depth context extraction, lay-
ers for vertical photometric context extraction, a memorized fusion layer for in-
corporating vertical photometric and depth contexts as true 2D global contexts,
and a ﬁnal layer for pixel-wise scene labeling given concatenated convolutional

6

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Fig. 2. Our LSTM-CF model for RGB-D scene labeling. The input consists of both
photometric and depth channels. Vertical contexts in photometric and depth channels
are computed in parallel using cascaded convolutional layers and a memorized context
layer. Vertical photometric (color) and depth contexts are fused and bi-directionally
propagated along the horizontal direction via another memorized fusion layer to ob-
tain true 2D global contexts. The fused global contexts and the ﬁnal convolutional
features of photometric channels are then concatenated together and fed into the ﬁ-
nal convolutional layer for pixel-wise scene labeling. “C” stands for the concatenation
operation.

features and global contexts. The inputs to our model include both photometric
and depth images. The path for extracting global contexts from the photometric
image consists of multiple convolutional layers and an extra memorized context
layer. On the other hand, the depth image is ﬁrst encoded as an HHA image,
which is fed into three convolutional layers [14] and an extra memorized context
layer for global depth context extraction. The other component, a memorized
fusion layer, is responsible for fusing previously extracted global RGB and depth
contexts in a data-driven manner. On top of the memorized fusion layer, the ﬁ-
nal convolutional feature of photometric channels and the fused global context
are concatenated together and fed into the ﬁnal fully convolutional layer, which
performs pixel-wise scene labeling with the softmax activation function.

3.1 Memorized Vertical Depth Context

Given a depth image, we use the HHA representation proposed in [13] to encode
geometric properties of the depth image in three channels, i.e., disparity, surface
normal and height. Diﬀerent from [13], the encoded HHA image in our pipeline is
fed into three randomly initialized convolutional layers (to obtain a feature map
with the same resolution as that in the RGB path) instead of layers taken from
the model pre-trained on the ILSVRC2012 dataset. This is because the color
distribution of HHA images is diﬀerent from that of natural images (see Fig.
2) according to [20]. One top of the third convolutional layer (i.e., HHAConv3),

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

7

there is an extra memorized context layer from Renet [19], which performs bi-
directional propagation of local contextual features from the convolutional layers
along the vertical direction. For better understanding, we denote the feature map
HHAConv3 as F = {fi,j}, where F ∈ Rw×h×c with w, h and c representing the
width, height and the number of channels. Since we perform pixel-wise scene
labeling, every patch in this Renet layer only contains a single pixel. Thus,
vertical memorized context layer (here we choose LSTM as recurrent unit) can
be formulated as

i,j = LSTM(hf
hf
i,j = LSTM(hb
hb

i,j−1, fi,j),
i,j+1, fi,j),

for j = 1, . . . , h

for j = h, . . . , 1,

(1)

(2)

where hf and hb stand for the hidden states of the forward and backward LSTM.
In the forward LSTM, the unit at pixel (i, j) takes hf
i,j−1 ∈ Rd and fi,j ∈ Rc as
input, and its output is calculated as follows according to [30]. The operations
in the backward LSTM can be deﬁned similarly.

i,j−1 + bi)
i,j−1 + bf )
i,j−1 + bo)

gatei = δ(Wif fi,j + Wihhf
gatef = δ(Wf f fi,j + Wf hhf
gateo = δ(Wof fi,j + Wohhf
gatec = tanh(Wcf fi,j + Wchhf
ci,j = gatef (cid:12) ci,j−1 + gatei (cid:12) gatec
hf
i,j = tanh(gateo (cid:12) ci,j)

i,j−1 + bc)

Finally, pixel-wise vertical depth contexts are collectively represented as a map,
Cdepth ∈ Rw×h×2d, where 2d is the total number of output channels from the
vertical memorized context layer.

(3)

3.2 Memorized Vertical Photometric Context

In the component for extracting global RGB contexts, we adapt the Deeplab ar-
chitecture proposed in [12]. Diﬀerent from existing Deeplab variants, we concate-
nate features at three diﬀerent scales to enrich the feature representation. This
is inspired by the network architecture in [33]. Speciﬁcally, since there exists hole
operations in Deeplab convolutional layers, feature maps from Conv2 2, Conv3 3
and Conv5 3 have suﬃcient initial resolutions. They can be further elevated to
the same resolution using interpolation. Corresponding pixel-wise features from
these three elevated feature maps are then concatenated together before being fed
into the subsequent memorized fusion layer, which again performs bi-directional
propagation to produce vertical photometric contexts. Here pixel-wise vertical
photometric contexts can also be represented as a map, CRGB ∈ Rw×h×2d, which
has the same dimensionalities as the map for vertical depth contexts.

8

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

3.3 Memorized Context Fusion

So far vertical depth and photometric contexts are computed independently
in parallel. Instead of simply concatenating these two types of contexts, the
memorized fusion layer, which performs horizontal bi-directional propagation
from Renet, is exploited for adaptively fusing vertical depth and RGB contexts
in a data-driven manner, and the output from this layer can be regarded as
the fused representation of both types of contexts. Such fusion can generate
more discriminative features through end-to-end training. The input and output
dimensions of the fusion layer are set to Rw×h×4d and Rw×h×2d, respectively.

Note that there are two separate memorized context layers in the photo-
metric and depth paths of our architecture. Since the memorized context layer
and the memorized fusion layer are two symmetric components of the original
Renet [19], a more natural and symmetric alternative would have a single memo-
rized context layer preceding the memorized fusion layer in our model (i.e., whole
structure of Renet including cascaded bi-directional vertical and horizonal mem-
orized layer) and let the memorized fusion layer incorporate the features from the
RGB and depth paths. Nonetheless, in our experiments, this alternative network
architecture gave rise to slightly worse performance.

3.4 Scene Labeling

Between photometric and depth images, photometric images contain more de-
tails and semantic information that can help scene labeling in comparison with
sparse and discontinuous depth images [14]. Nonetheless, depth images can pro-
vide auxiliary geometric information for improving scene labeling performance.
Thus, we design a cross-layer combination that integrates pixel-wise convolu-
tional features (i.e., Conv7 in Fig. 2) from the photometric image with fused
global contexts from the memorized fusion layer as the ﬁnal pixel-wise features,
which are fed into the last fully convolutional layer with softmax activation to
perform scene labeling at every pixel location.

4 Experimental Results

4.1 Experimental Setting

Datasets: We evaluate our proposed model for RGB-D scene labeling on three
public benchmarks, SUNRGBD, NYUDv2 and SUN3D. SUNRGBD [20] is the
largest dataset currently available, consisting of 10355 RGB-D images captured
from four diﬀerent depth sensors. It includes most previous datasets, such as
NYUDv2 depth [34], Berkeley B3DO [35], and SUN3D [36], as well as 3943
newly captured RGB-D images [20]. 5285 of these images are predeﬁned for
training and the remaining 5050 images constitute the testing set [14].
Implementation Details: In our experiments, a slightly modiﬁed Deeplab
pipeline [12] is adopted as the basic network in our RGB path for extracting
convolutional feature maps because of its high performance. It is initialized with

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

9

the publicly available VGG-16 model pre-trained on ImageNet. For the pur-
pose of pixel-wise scene labeling, this architecture transforms the last two fully
connected layers in the standard VGG-16 to convolutional layers with 1 × 1 ker-
nels. For the parallel depth path, three randomly initialized CNN layers with
max pooling are leveraged for depth feature extraction. In each path, on top
of the aforementioned convolutional network, a vertically bi-directional LSTM
layer implements the memorized context layer, and models both short-range
and long-range spatial dependencies. Then, another horizontally bi-directional
LSTM layer implements the memorized fusion layer, and is used to adaptively
integrate the global contexts from the two paths. In addition, there is a cross-
layer combination of ﬁnal convolutional features (i.e., Conv7) and the integrated
global representation from the horizontal LSTM layer.

Since the SUNRGBD dataset was collected by four diﬀerent depth sensors,
each input image is cropped to 426 × 426 (the smallest resolution of these four
sensors) [14]. During ﬁne-tuning, the learning rate for newly added layers, in-
cluding HHAConv1, HHAConv2, HHAConv3, the memorized context layers, the
memorized fusion layer and Conv8, is initialized to 10−2, and the learning rate
for those pre-trained layers of VGG-16 is initialized to 10−4. All weights in the
newly added convolutional layers are initialized using a Gaussian distribution
with a standard deviation equal to 0.01, and the weights in the LSTM layers are
randomly initialized with a uniform distribution over [−0.01, 0.01]. The number
of hidden memory cells in a memorized context layer or a memorized fusion layer
is set to 100, and the size of feature maps is 54 × 54. We train all the layers in
our deep network simultaneously using SGD with a momentum 0.9, the batch
size is set to one (due to limited GPU memory) and the weight decay is 0.0005.
The entire deep network is implemented on the publicly available platform Caﬀe
[37] and is trained on a single NVIDIA GeForce GTX TITAN X GPU with
12GB memory 1. It takes about 1 day to train our deep network. In the testing
stage, an RGB-D image takes 0.15s on average, which is signiﬁcantly faster than
pervious methods, i.e., the testing time in [9,24] is around 1.5s.

4.2 Results and Comparisons

According to [14,22], performance is evaluated by comparing class-wise Jaccard
Index, i.e., nii/ti, and average Jaccard Index, i.e., (1/ncl) (cid:80)
i nii/ti, where nij
is the number of pixels annotated as class i and predicted to be class j, ncl is
the number of diﬀerent classes, and ti = (cid:80)
j nij is the total number of pixels
annotated as class i [10].

SUNRGBD dataset [20]: The performance and comparison results on
SUNRGBD are shown in Table 1. Our proposed architecture can outperform
existing techniques: 2.2% higher than the performance reported in [22], 11.8%
higher than that in [24], 38% higher than that in [38] and 39.1% higher than that
in [20] in terms of 37-class average Jaccard Index. Improvements can be observed

1 LSTM-CF model is publicly available at: https://github.com/icemansina/LSTM-CF

10

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Table 1. Comparison of scene labeling results on SUNRGBD using class-wise and
average Jaccard Index. We compare our model with results reported in [20], [38], [24]
and previous state-of-the-art result in [22]. Boldface numbers mean best performance.

Wall
[20] 37.8
[20] 32.1
[20] 36.4
[38] 38.9
[38] 33.3
[38] 37.8
[24] 43.2
[22] 80.2
Ours 74.9

9.6
5.0
7.9
11.0
5.6
9.0

ﬂoor
45.0
42.6
45.8
47.2
43.8
48.3
78.6
90.9
82.3

chair sofa table door window bookshelf picture counter blinds
cabinet bed
9.4
16.9 12.8 18.5 6.1
21.8
17.4
0.8
21.5 4.1
12.5 3.4
6.4
2.9
12.8
19.9 11.6 19.3 6.0
23.3
15.4
9.6
17.2 13.4 20.4 6.8
21.5
18.8
0.9
12.9 3.8
22.3 3.9
6.3
3.0
13.1
20.8 12.1 20.9 6.8
23.6
17.2
23.1
42.5
26.2
33.2 40.6 34.3 33.2 43.6
31.2
64.8 76.0 58.6 62.6 47.7 66.4
58.9
43.1
67.7 55.5 57.8 45.6 52.8
62.1
47.3
paper towel shower box
ﬂoormat clothes ceiling books fridge tv
0.0
0.6
1.5
1.6
27.9
7.0
4.1
0.0
1.0
0.0
0.9
9.7
0.0
0.6
0.0
0.6
1.4
0.7
35.8
9.5
6.1
0.0
0.7
1.5
1.4
39.1
7.1
5.9
0.7
0.0
0.4
0.0
13.9
0.9
0.5
0.8
10.1 0.6
49.2
0.0
1.4
8.7
11.7
35.7
84.5
24.2 36.5 26.8 19.2 9.0
24.1
48.7 21.3 49.5 30.6 18.8 0.1
84.0
38.1
47.9 61.5 52.1 36.4 36.7 0
68.0

desk shelves curtain dresser pillow mirror
7.3
2.4
4.6
2.0
14.8
3.3
7.0
2.2
3.6
7.3
3.6
6.1
2.0
32.6
3.8
6.8
2.4
4.4
12.1 18.4
42.3
57.2
19.7 16.2
46.7
63.6
56.7
37.3 9.6
48.6
board person nightstand toilet sink
14.0
7.4
2.7
1.1
7.6
10.4
3.5
8.6
51.4
56.8
48.1

6.9
2.3
2.2
0.9
1.2
1.4
5.6
3.1
5.4
6.2
2.6
2.4
1.0
1.1
1.8
6.4
3.2
4.8
49.5
24.8
31.4
57.1 39.1
42.3
35.0
45.8 44.5
bathhub bag mean
8.3
0.9
0.6
5.3
0.4
0.0
9.0
0.6
1.1
9.3
1.1
0.9
6.0
0.5
0.0
10.1
1.3
1.1
18.6
47.0
36.3
24.1 45.9
45.1
23.6 48.1
65.6

4.3
2.0
4.4
6.9
3.6
7.8
59.1
67.0
63.4
lamp
0.9
0.7
0.9
1.3
0.8
1.2
44.2
48.8
58.0

8.9
2.3
12.0 15.2
12.3 14.8
2.6
14.9 16.8
64.1 53.0
73.0 66.2
68.8 67.9

1.2
0.3
1.4
1.3
0.6
1.6
27.0
24.4
28.4

1.1
2.6
1.7
1.5
1.5
1.8
25.0
42.9
36.4

0.0
0.3
0.7
0.0
0.3
0.8
35.7
17.9
72.6

2.2
1.7
5.2
2.6
2.2
6.2
31.8
33.8
39.4

1.0
15.3
1.7
1.2
10.1
1.0

1.9
0.1
0.2
2.2
0.3
0.2

1.2

[20] 0.0
[20] 0.0
[20] 0.0
[38] 0.0
[38] 0.0
[38] 0.0
[24] 5.6
[22] 0.1
Ours 0.0

in 15 class-wise Jaccard Indices. For a better understanding, we also show the
confusion matrix for this dataset in Fig. 3(a). It is worth mentioning that our
proposed architecture and most previous methods achieve zero accuracy on two
categories, i.e., ﬂoormat and shower, which mainly results from an imbalanced
data distribution instead of the capacity of our model.

NYUDv2 dataset: To further verify the eﬀectiveness of our architecture
and have more comparisons with existing state-of-the-art methods, we also con-
duct experiments on the NYUDv2 dataset. The results are presented in Table 2,
where the 13-class average Jaccard Index of our model is 20.3% higher than that
in [39]. Class frequencies and the confusion matrix are also shown in Table 2 and
Fig. 3(b) respectively. According to the reported results, our proposed architec-
ture gains 5.6% and 5.5% improvement in average Jaccard Index over [9] and
FCN-32s [10] respectively. Considering the listed class frequencies, our proposed
model signiﬁcantly outperforms existing methods on high frequency categories
and most low frequency categories, which primarily owes to the convolutional
features of the RGB image and the fused global contexts of the complete RGB-
D image. In terms of labeling categories with small and complex regions, e.g.,
pillows and chairs, our method also achieves a large improvement, which can be
veriﬁed in the following visual comparisons.

SUN3D dataset: Table 3 gives comparison results on the 1539 test images
in the SUN3D dataset. For fair comparison, the 12-class average Jaccard Index is
used in the comparison with the state-of-the-art results recently reported in [9].
Note that the 12-class accuracy of our network is calculated through the model
previously trained for 37 classes. Our model substantially outperforms the one
from [9] on large planar regions such as those labeled as ﬂoors and ceilings. This
also results from the incorporated convolutional features and the fused global
contexts. These comparison results further conﬁrm the power and generalization
capability of our LSTM-based model.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

11

(a) SUNRGBD

(b) NYUDv2

Fig. 3. Confusion matrix for SUNRGBD and NYUDv2. Class-wise Jaccard Index is
shown on the diagonal. Best viewed in color.

Table 2. Comparison of scene labeling on NYUDv2. We compare our proposed model
with existing state-of-the-art methods, i.e., [34], [24], [25], [26] and [9]. Class-wise Jac-
card Index and average Jaccard Index of 37 classes are presented. ‘Freq’ stands for
class frequency. Boldface numbers mean best performance.

Wall
Freq 21.4
[34] 60.7
[24] 60.0
[25] 67.4
[26] 61.4
[9]
65.7
Ours 79.6

2.1

2.7

ﬂoor
9.1
77.8
74.4
80.5
66.4
62.5
83.5

chair sofa table door window bookshelf picture counter blinds
cabinet bed
1.9
2.1
3.3
2.2
3.8
6.2
22.7
32.4 25.3 21.0 5.9
29.7
40.3
33.0
17.3
32.5 28.2 16.6 12.9 27.7
42.3
37.1
20.5
40.4 44.8 30.0 12.1 34.1
56.4
41.4
17.6
34.4 33.8 22.6 8.3
27.6
43.9
38.2
36.3
32.1
40.1
44.5 50.8 43.5 51.6 49.2
59.5
77.0 58.3 64.9 42.6 47.0 43.6
69.3
paper towel shower box
ﬂoormat clothes ceiling books fridge tv
0.3
0.4
0.4
0.4
0.5
0.6
0.1
5.7
3.6
12.7 0.1
1.4
3.3
1.9
18.6 11.7 12.6 5.4
1.1
14.5 14.4 14.1 19.8 6.0
0.8
6.1
2.6
2.9
36.3
17.6

desk shelves curtain dresser pillow mirror
1.0
1.1
1.7
2.1
4.7
3.3
40.6
35.7
10.1 6.1
26.5
32.4
10.1 1.6
44.7
38.7
5.1
2.7
33.6
27.7
48.0 45.2
55.8
41.4
74.5
33.6 13.1
74.6
board person nightstand toilet sink
0.3
0.3
0.0
0.2
12.9
28.2
60.6
93.9

1.0
0.8
0.9
4.4
18.9
13.3
17.9
19.7
7.0
14.6
31.3
21.6
12.5
10.7
16.8
50.5 46.1
55.3
56.5
48.0 47.7
bathhub bag mean
0.2
0.3
17.5
0.0
0.0
20.2
1.2
7.8
30.0
0.2
29.4
0.2
16.2
29.2
37.3 43.9
38.5
49.4
7.5
72.6

0.6
5.5
14.8
5.8
3.6
39.1 53.6 50.1 35.4 39.9 41.8
49.7 0.0

1.1
27.4
27.6
26.3
18.9
53.1
53.2
lamp
0.3
15.9
14.2
31.2
5.4
26.3
67.6

0.3
26.7 25.1
35.2 28.9
52.5 47.9
32
20.9
31.8 22.5
81.8 58.4

1.4
73.2
53.9
61.8
46.1
50.6
70.2

0.7
6.5
9.5
8.0
2.7
35.4
22.7

0.3
6.3
9.2
15.7
6.9
32.5
0

0.3
6.6
13.6
1.5
5
35.6
77.0

1.4
33.1
38.6
50.7
30.2
39.2
68.2

0.0 52.1 60.6 0

6.2

3.2

Freq 0.7
[34] 7.1
[24] 20.1
[25] 28.2
[26] 13.8
[9]
54.1
Ours 0.0

4.3 Ablation Study

To discover the vital elements in our proposed model, we conduct an ablation
study to remove or replace individual components in our deep network when
training and testing on the SUNRGBD dataset. Speciﬁcally, we have tested the
performance of our model without the RGB path, the depth path, multi-scale
RGB feature concatenation, the memorized context layers or the memorized
fusion layer. In addition, we also conduct an experiment with a model that does
not combine the ﬁnal convolutional features of photometric channels (i.e., Conv7
in Fig. 2) with the global contexts of the complete RGB-D image to ﬁgure out the
importance of diﬀerent components. The results are presented in Table 4. From
the given results, we ﬁnd that the ﬁnal convolutional features of the photometric
channels is the most vital information, i.e., the cross-layer combination is the
most eﬀective component as the performance drops to 15.2% without it, which
is consistent with previously mentioned properties of depth and photometric
data. In addition, multi-scale RGB feature concatenation before the memorized
context layer also plays a vital role as it directly aﬀects the vertical contexts in

12

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Table 3. Comparison of class-wise Jaccard Index and 12-class average Jaccard Index
on SUN3D.

wall ﬂoor bed chair table counter curtain ceiling tv toilet bathtub bag Mean
73 35

[9]
71 35
Ours 73 86 32 65

30
57

52
22

68
76

27
69

56 23
75 62

49
62

29 45.7
23 58.5

Table 4. Ablation Study

Model

Without RGB path, using Deeplab+Renet for depth path
Without depth path
Without multi-scale RGB feature concatenation
Without cross-layer integration of RGB convolutional features
Without memorized fusion layer
Without memorized context layers
Without any memorized (context or fusion) layers

Mean Accuracy
15.8%
43.7%
42.1%
15.2%
44.7%
45.7%
45.0%

the photometric channels and the performance drops to 42.1% without it. It is
obvious that performance would be inevitably harmed without the depth path.
Among the memorized layers, the memorized fusion layer is more important
than the memorized context layers in our pipeline as it accomplishes the fusion
of contexts in photometric and depth channels.

4.4 Visual Comparisons

SUNRGBD Dataset: We present visual results of RGB-D scene labeling in
Fig. 4. Here, we leverage super-pixel based averaging to smooth visual labeling
results as being done in [9]. The algorithm in [40] is used for performing super-
pixel segmentation. As can be observed in Fig. 4, our proposed deep network
produces accurate and semantically meaningful labeling results, especially for
large regions and high frequency labels. For instance, our model takes advantage
of global contexts when labeling ‘bed’ in Fig. 4(a), ‘wall’ in Fig. 4(e) and ‘mirror’
in Fig. 4(i). Our proposed model can precisely label almost all ‘chairs’ (a high
frequency label) by exploiting integrated photometric and depth information,
regardless of occlusions.

NYUDv2 Dataset: We also perform visual comparisons on the NYUDv2
benchmark, which has complicated indoor scenes and well-labeled ground truth.
We compare our scene labeling results with those publicly released labeling re-
sults from [25]. It is obvious that our results are clearly better than those from
[25] both visually and numerically (under the metric of average Jaccard Index)
even though scene labeling in [25] is based on sophisticated segmentation.

Label Reﬁnement: Surprisingly, our model can intelligently reﬁne certain
region annotations, which might have inaccuracies due to under-segmentation,
especially in the newly captured 3943 RGB-D images, as shown in Fig. 6. Specif-
ically, the cabinets in Fig. 6(a) were annotated as ‘background’, the pillows in

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

13

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

(q) legend of semantic labels

Fig. 4. Examples of semantic labeling results on the SUNRGBD dataset. The top row
shows the input RGB images, the bottom row shows scene labeling obtained with our
model and the middle row has the ground truth. Semantic labels and their correspond-
ing colors are shown at the bottom.

Fig. 6(g) as ‘bed’, and the tables in Fig. 6(n) as ‘wall’ by mistake. Our model
can eﬀectively deal with these diﬃcult regions. For example, the annotation of
the picture in Fig. 6(e) and that of the pillows in Fig. 6(g) have been corrected.
Thus, our model can be exploited to reﬁne certain annotations in the SUNRGBD
dataset, which is another contribution of our model.

5 Conclusions

In this paper, we have developed a novel Long Short-Term Memorized Con-
text Fusion (LSTM-CF) model that captures image contexts from a global per-
spective and deeply fuses contextual representations from multiple sources (i.e.,
depth and photometric data) for semantic scene labeling. In future, we will ex-
plore how to extend the memorized layers with an attention mechanism, and
reﬁne the performance of our model in boundary labeling.

14

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Fig. 5. Visual comparison of scene labeling results on the NYUDv2 dataset. The ﬁrst
and second rows show the input RGB images and their corresponding groundtruth
labeling. The third row shows the results from [25] and the last row shows the results
from our model.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

Fig. 6. Annotation reﬁnement on the SUNRGBD dataset. The top row shows the input
RGB images, the middle row shows the original annotations, and the bottom row shows
scene labeling results from our model.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

15

References

1. Wu, C., Lenz, I., Saxena, A.: Hierarchical semantic labeling for task-relevant rgb-d

perception. In: Robotics: Science and systems (RSS). (2014)

2. Hinterstoisser, S., Lepetit, V., Ilic, S., Holzer, S., Bradski, G., Konolige, K., Navab,
N.: Model based training, detection and pose estimation of texture-less 3d objects
In: Computer Vision–ACCV 2012. Springer (2012)
in heavily cluttered scenes.
548–562

3. Holz, D., Holzer, S., Rusu, R.B., Behnke, S.: Real-time plane segmentation using
rgb-d cameras. In: RoboCup 2011: robot soccer world cup XV. Springer (2011)
306–317

4. Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., Manning, C.D.: Generating se-
mantically precise scene graphs from textual descriptions for improved image re-
trieval. In: Proceedings of the Fourth Workshop on Vision and Language. (2015)
70–80

5. Yan, Z., Zhang, H., Wang, B., Paris, S., Yu, Y.: Automatic photo adjustment using

deep neural networks. ACM Transactions on Graphics 35(2) (2016)

6. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions
on 35(8) (2013) 1915–1929

7. Gould, S., Fulton, R., Koller, D.: Decomposing a scene into geometric and se-
mantically consistent regions. In: Computer Vision, 2009 IEEE 12th International
Conference on, IEEE (2009) 1–8

8. Tighe, J., Lazebnik, S.: Superparsing: scalable nonparametric image parsing with

superpixels. In: Computer Vision–ECCV 2010. Springer (2010) 352–365

9. Khan, S.H., Bennamoun, M., Sohel, F., Togneri, R., Naseem, I.: Integrating geo-
metrical context for semantic labeling of indoor scenes using rgbd images. Inter-
national Journal of Computer Vision (2015) 1–20

10. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3431–3440

11. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: Proceed-
ings of the IEEE International Conference on Computer Vision. (2015) 1529–1537
12. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. arXiv
preprint arXiv:1412.7062 (2014)

13. Gupta, S., Girshick, R., Arbel´aez, P., Malik, J.: Learning rich features from rgb-d
images for object detection and segmentation. In: Computer Vision–ECCV 2014.
Springer (2014) 345–360

14. Song, S., Xiao, J.: Deep sliding shapes for amodal 3d object detection in rgb-d

images. arXiv preprint arXiv:1511.02300 (2015)

15. Liu, Z., Li, X., Luo, P., Loy, C.C., Tang, X.: Semantic image segmentation via
deep parsing network. In: Proceedings of the IEEE International Conference on
Computer Vision. (2015) 1377–1385

16. Liang, X., Shen, X., Xiang, D., Feng, J., Lin, L., Yan, S.: Semantic object parsing
with local-global long short-term memory. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. (2016)

17. Byeon, W., Breuel, T.M., Raue, F., Liwicki, M.: Scene labeling with lstm recurrent
neural networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3547–3555

16

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

18. Pinheiro, P., Collobert, R.: Recurrent convolutional neural networks for scene
labeling. In: Proceedings of the 31st International Conference on Machine Learning
(ICML-14). (2014) 82–90

19. Visin, F., Kastner, K., Cho, K., Matteucci, M., Courville, A., Bengio, Y.: Renet:
A recurrent neural network based alternative to convolutional networks. arXiv
preprint arXiv:1505.00393 (2015)

20. Song, S., Lichtenberg, S.P., Xiao, J.: Sun rgb-d: A rgb-d scene understanding
benchmark suite. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. (2015) 567–576

21. Kumar, M.P., Koller, D.: Eﬃciently selecting regions for scene understanding. In:
Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
IEEE (2010) 3217–3224

22. Kendall, A., Badrinarayanan, V., Cipolla, R.: Bayesian segnet: Model uncertainty
in deep convolutional encoder-decoder architectures for scene understanding. arXiv
preprint arXiv:1511.02680 (2015)

23. Lempitsky, V., Vedaldi, A., Zisserman, A.: Pylon model for semantic segmentation.

In: Advances in neural information processing systems. (2011) 1485–1493
24. Ren, X., Bo, L., Fox, D.: Rgb-(d) scene labeling: Features and algorithms.

In:
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,
IEEE (2012) 2759–2766

25. Gupta, S., Arbel´aez, P., Girshick, R., Malik, J.: Indoor scene understanding with
rgb-d images: Bottom-up segmentation, object detection and semantic segmenta-
tion. International Journal of Computer Vision 112(2) (2015) 133–149

26. Wang, A., Lu, J., Cai, J., Wang, G., Cham, T.J.: Unsupervised joint feature learn-
ing and encoding for rgb-d scene labeling. Image Processing, IEEE Transactions
on 24(11) (2015) 4459–4473

27. Husain, F., Schulz, H., Dellen, B., Torras, C., Behnke, S.: Combining semantic and
geometric features for object class segmentation of indoor scenes. IEEE Robotics
and Automation Letters 2(1) (2017) 49–55

28. Schmidhuber, J.: A local learning algorithm for dynamic feedforward and recurrent

networks. Connection Science 1(4) (1989) 403–412

29. Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with gra-
dient descent is diﬃcult. Neural Networks, IEEE Transactions on 5(2) (1994)
157–166

30. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

9(8) (1997) 1735–1780

31. Graves, A., Schmidhuber, J.: Oﬄine handwriting recognition with multidimen-
sional recurrent neural networks. In: Advances in neural information processing
systems. (2009) 545–552

32. Stollenga, M.F., Byeon, W., Liwicki, M., Schmidhuber, J.:

Parallel multi-
dimensional lstm, with application to fast biomedical volumetric image segmen-
tation. In: Advances in Neural Information Processing Systems. (2015) 2980–2988
In: IEEE

33. Li, G., Yu, Y.: Deep contrast learning for salient object detection.

Conference on Computer Vision and Pattern Recognition (CVPR). (2016)

34. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and support
inference from rgbd images. In: Computer Vision–ECCV 2012. Springer (2012)
746–760

35. Janoch, A., Karayev, S., Jia, Y., Barron, J.T., Fritz, M., Saenko, K., Darrell, T.: A
category-level 3d object dataset: Putting the kinect to work. In: Consumer Depth
Cameras for Computer Vision. Springer (2013) 141–165

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

17

36. Xiao, J., Owens, A., Torralba, A.: Sun3d: A database of big spaces reconstructed
using sfm and object labels. In: Proceedings of the IEEE International Conference
on Computer Vision. (2013) 1625–1632

37. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

38. Liu, C., Yuen, J., Torralba, A.: Sift ﬂow: Dense correspondence across scenes and
its applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on
33(5) (2011) 978–994

39. Couprie, C., Farabet, C., Najman, L., LeCun, Y.: Toward real-time indoor semantic
segmentation using depth information. Journal of Machine Learning Research
(2014)

40. Felzenszwalb, P.F., Huttenlocher, D.P.: Eﬃcient graph-based image segmentation.

International Journal of Computer Vision 59(2) (2004) 167–181

6
1
0
2
 
l
u
J
 
6
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
0
0
5
0
.
4
0
6
1
:
v
i
X
r
a

LSTM-CF: Unifying Context Modeling and
Fusion with LSTMs for RGB-D Scene Labeling

Zhen Li1, Yukang Gan2, Xiaodan Liang2, Yizhou Yu1,
Hui Cheng2, and Liang Lin2(cid:63)

1 Department of Computer Science, The University of Hong Kong, Hong Kong,
lizhen36@hku.hk,yizhouy@acm.org
2 School of Data and Computer Science, Sun Yat-sen University, Guangzhou,
ganyk@mail2.sysu.edu.cn,xdliang328@gmail.com
chengh9@mail.sysu.edu.cn,linliang@ieee.org

Abstract. Semantic labeling of RGB-D scenes is crucial to many in-
telligent applications including perceptual robotics. It generates pixel-
wise and ﬁne-grained label maps from simultaneously sensed photomet-
ric (RGB) and depth channels. This paper addresses this problem by i)
developing a novel Long Short-Term Memorized Context Fusion (LSTM-
CF) Model that captures and fuses contextual information from multiple
channels of photometric and depth data, and ii) incorporating this model
into deep convolutional neural networks (CNNs) for end-to-end training.
Speciﬁcally, contexts in photometric and depth channels are, respectively,
captured by stacking several convolutional layers and a long short-term
memory layer; the memory layer encodes both short-range and long-
range spatial dependencies in an image along the vertical direction. An-
other long short-term memorized fusion layer is set up to integrate the
contexts along the vertical direction from diﬀerent channels, and per-
form bi-directional propagation of the fused vertical contexts along the
horizontal direction to obtain true 2D global contexts. At last, the fused
contextual representation is concatenated with the convolutional features
extracted from the photometric channels in order to improve the accu-
racy of ﬁne-scale semantic labeling. Our proposed model has set a new
state of the art, i.e., 48.1% and 49.4% average class accuracy over 37
categories (2.2% and 5.4% improvement) on the large-scale SUNRGBD
dataset and the NYUDv2 dataset, respectively.

Keywords: RGB-D scene labeling, image context modeling, long short-
term memory, depth and photometric data fusion

1 Introduction

Scene labeling, also known as semantic scene segmentation, is one of the most
fundamental problems in computer vision. It refers to associating every pixel in

(cid:63) The corresponding author is Liang Lin. This work was support by Projects on Fac-
ulty/ Student Exchange and Collaboration Scheme between the Higher Education in
Hong Kong and the Mainland, Guangzhou Science and Technology Program under
grant 1563000439, and Fundamental Research Funds for the Central Universities.

2

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Fig. 1. An illustration of global context modeling and fusion for RGB-D images. Our
LSTM-CF model ﬁrst captures vertical contexts through a memory network layer en-
coding short- and long-range spatial dependencies along the vertical direction. After a
concatenation operation (denoted by “C”) over photometric and depth channels, our
model utilizes another memory network layer to fuse vertical contexts from all chan-
nels in a data-driven way and performs bi-directional propagation along the horizontal
direction to obtain true 2D global contexts. Best viewed in color.

an image with a semantic label, such as table, road and wall, as illustrated in
Fig. 1. High-quality scene labeling can be beneﬁcial to many intelligent tasks,
including robot task planning [1], pose estimation [2], plane segmentation [3],
context-based image retrieval [4], and automatic photo adjustment [5].

Previous work on scene labeling can be divided into two categories accord-
ing to their target scenes: indoor and outdoor scenes. Compared with outdoor
scene labeling [6,7,8], indoor scene labeling is more challenging due to a larger
set of semantic labels, more severe object occlusions, and more diverse object
appearances [9]. For example, indoor object classes, such as beds covered with
diﬀerent sheets and various appearances of curtains, are much harder to char-
acterize than outdoor classes, e.g., roads, buildings, and sky, through photo-
metric channels only. Recently, utilizing depth sensors to augment RGB data
have eﬀectively improved the performance of indoor scene labeling because the
depth channel complements photometric channels with structural information.
Nonetheless, two key issues remain open in the literature of RGB-D scene label-
ing.

(I) How to eﬀectively represent and fuse the coexisting depth and
photometric (RGB) data For data representation, a batch of sophisticated
hand-crafted features have been developed in previous methods. Such hand-
crafted features are somewhat ad hoc and less discriminative than those RGB-D
representations learned using convolutional neural networks (CNNs) [10,11,12,13,14].
However, in these CNN-related works, the fusion of depth and photometric data
has often been oversimpliﬁed. For instance, in [13,14], two independent CNNs are
leveraged to extract features from depth and photometric data separately, and
such features are simply concatenated before used for ﬁnal classiﬁcation. Over-
looking the strong correlation between depth and photometric channels could
inevitably harm semantic labeling.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

3

(II) How to capture global scene contexts during feature learning
Current CNN-based scene labeling approaches can only capture local contextual
information for every pixel due to their restricted receptive ﬁelds, resulting in
suboptimal labeling results. In particular, long-range dependencies sometimes
play a key role in distinguishing among diﬀerent objects having similar appear-
ances, e.g., labeling “ceiling” and “ﬂoor” in Fig. 1, according to the global scene
layout. To overcome this issue, graphical models, such as a conditional random
ﬁeld [9,11] or a mean-ﬁeld approximation [15], have been applied to improve
prediction results in a post-processing step. These methods, however, separate
context modeling from convolutional feature learning, which may give rise to
suboptimal results on complex scenes due to less discriminative feature repre-
sentation [16]. An alternative class of methods adopts cascaded recurrent neural
networks (RNNs) with gate structures, e.g., long short-term memory (LSTM)
networks, to explicitly strengthen context modeling [16,17,18]. In these methods,
the long- and short-range dependencies can be well memorized by sequentially
running the network over individual pixels.

To address the aforementioned challenges, this paper proposes a novel Long
Short-Term Memorized Context Fusion (LSTM-CF) model and demonstrates
its superiority in RGB-D scene labeling. Fig. 1 illustrates the brief idea of using
memory networks for context modeling and fusion of diﬀerent channels. Our
LSTM-CF model captures 2D dependencies within an image by exploiting the
cascaded bi-directional vertical and horizontal RNN models as introduced in
[19].

Our method constructs HHA images [13] for the depth channel through ge-
ometric encoding, and uses several convolutional layers for extracting features.
Inspired by [19], these convolutional layers are followed by a memorized context
layer to model both short-range and long-range spatial dependencies along the
vertical direction. For photometric channels, we generate convolutional features
using the Deeplab network [12], which is also followed by a memorized context
layer for context modeling along the vertical direction. Afterwards, a memorized
fusion layer is set up to integrate the contexts along the vertical direction from
both photometric and depth channels, and perform bi-directional propagation of
the fused vertical contexts along the horizontal direction to obtain true 2D global
contexts. Considering the features diﬀerences, e.g., signal frequency and other
characteristics (color/geometry) [20], our fusion layer facilitates deep integration
of contextual information from multiple channels in a data-driven manner rather
than simply concatenating diﬀerent feature vectors. Since photometric channels
usually contain ﬁner details in comparison to the depth channel [20], we further
enhance the network with cross-layer connections that append convolutional fea-
tures of the photometric channels to the fused global contexts before the ﬁnal
fully convolutional layer, which predicts pixel-wise semantic labels. Various lay-
ers in our LSTM-CF model are tightly integrated, and the entire network is
amenable to end-to-end training and testing.

In summary, this paper has the following contributions to the literature of

RGB-D scene labeling.

4

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

– It proposes a novel Long Short-Term Memorized Context Fusion (LSTM-
CF) Model, which is capable of capturing image contexts from a global
perspective and deeply fusing contextual information from multiple sources
(i.e., depth and photometric channels).

– It proposes to jointly optimize LSTM layers and convolutional layers for
achieving better performance in semantic scene labeling. Context modeling
and fusion are incorporated into the deep network architecture to enhance
the discriminative power of feature representation. This architecture can also
be extended to other similar tasks such as object/part parsing.

– It is demonstrated on the large-scale SUNRGBD benchmark (including 10355
images) and canonical NYUDv2 benchmark that our method outperforms
existing state-of-the-art methods. In addition, it is found that our scene la-
beling results can be leveraged to improve the groundtruth annotations of
newly captured 3943 RGB-D images in SUNRGBD dataset.

2 Related work

Scene Labeling: Scene labeling has caught researchers’ attention frequently
[6,11,12,16,17,18,21] in recent years. Instead of extracting features from over-
segmented images, recent methods usually utilize powerful CNN layers as the
feature extractor, taking advantage of fully convolutional networks (FCNs) [10]
and its variants [22] to obtain pixel-wise dense features. Another main challenge
for scene labeling is the fusion of local and global contexts, i.e., taking advantage
of global contexts to reﬁne local decisions. For instance, [6] exploits families of
segmentations or trees to generate segment candidates. [23] utilizes an inference
method based on graph cut to achieve image labeling. A pixel-wise conditional
random forest is used in [11,12] to directly optimize a deep CNN-driven cost
function. Most of the above models improve accuracy through carefully designed
processing on the predicted conﬁdence map instead of proposing more powerful
discriminative features, which usually results in suboptimal prediction results
[16]. The topological structure of recurrent neural networks (RNNs) is used to
model short- and long-range dependencies in [16,18]. In [17], a multi-directional
RNN is leveraged to extract local and global contexts without using a CNN,
which is well suited for low-resolution and relatively simple scene labeling prob-
lems. In contrast, our model can jointly optimize LSTM layers and convolutional
layers to explicitly improve discriminative feature learning for local and global
context modeling and fusion.

Scene Labeling in RGB-D images: With more and more convenient ac-
cess to aﬀordable depth sensors, scene labeling in RGB-D images [9,13,14,24,25,26]
enables a rapid progress of scene understanding. Various sophisticated hand-
crafted features are utilized in previous state-of-the-art methods. Speciﬁcally,
kernel descriptions based on traditional multi-channel features, such as color,
depth gradient, and surface normal, are used as photometric and depth features
[24]. A rich feature set containing various traditional features, e.g., SIFT, HOG,
LBP and plane orientation, are used as local appearance features and plane ap-
pearance features in [9]. HOG features of RGB images and HOG+HH (histogram

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

5

of height) features of depth images are extracted as representations in [25] for
training successive classiﬁers. In [27], proposed distance-from-wall features are
exploited to improve scene labeling performance. In addition, an unsupervised
joint feature learning and encoding model is proposed for scene labeling in [26].
However, due to the limited number of RGB-D images, deep learning for scene
labeling in RGB-D images was not as appealing as that for RGB images. The
release of the SUNRGBD dataset, which includes most of the previously popular
datasets, may have changed this situation [13,14].

Another main challenge imposed by scene labeling in RGB-D images is the
fusion of contextual representations of diﬀerent sources (i.e., depth and photo-
metric data). For instance, in [13,14], two independent CNNs are leveraged to
extract features from the depth and photometric data separately, which are then
simply concatenated for class prediction. Ignoring the strong correlation between
depth and photometric channels usually negatively aﬀects semantic labeling. In
contrast, instead of simply concatenating features from multiple sources, the
memorized fusion layer in our model facilitates the integration of contextual
information from diﬀerent sources in a data-driven manner,

RNN for Image Processing: Recurrent neural networks (RNNs) represent
a type of neural networks with loop connections [28]. They are designed to cap-
ture dependencies across a distance larger than the extent of local neighborhoods.
In previous work, RNN models have not been widely used partially due to the
diﬃculty to train such models, especially for sequential data with long-range de-
pendencies [29]. Fortunately, RNNs with gate and memory structures, e.g., long
short-term memory (LSTM) [30], can artiﬁcially learn to remember and forget
information by using speciﬁc gates to control the information ﬂow. Although
RNNs have an outstanding capability to capture short-range and long-range de-
pendencies, there exist problems for applying RNNs to image processing due to
the fact that, unlike data in natural language processing (NLP) tasks, images
do not have a natural sequential structure. Thus, diﬀerent strategies have been
proposed to overcome this problem. Speciﬁcally, in [19], cascaded bi-directional
vertical and horizonal RNN layers are designed for modeling 2D dependencies in
images. A multi-dimensional RNN with LSTM unit has been applied to hand-
writing [31]. A parallel multi-dimensional LSTM for image segmentation has
been proposed in [32]. In this paper, we propose an LSTM-CF model consisting
of memorized context layers and a memorized fusion layer to capture image con-
texts from a global perspective and fuse contextual representations from diﬀerent
sources.

3 LSTM-CF Model

As illustrated in Fig. 2, our end-to-end LSTM-CF model for RGB-D scene label-
ing consists of four components, layers for vertical depth context extraction, lay-
ers for vertical photometric context extraction, a memorized fusion layer for in-
corporating vertical photometric and depth contexts as true 2D global contexts,
and a ﬁnal layer for pixel-wise scene labeling given concatenated convolutional

6

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Fig. 2. Our LSTM-CF model for RGB-D scene labeling. The input consists of both
photometric and depth channels. Vertical contexts in photometric and depth channels
are computed in parallel using cascaded convolutional layers and a memorized context
layer. Vertical photometric (color) and depth contexts are fused and bi-directionally
propagated along the horizontal direction via another memorized fusion layer to ob-
tain true 2D global contexts. The fused global contexts and the ﬁnal convolutional
features of photometric channels are then concatenated together and fed into the ﬁ-
nal convolutional layer for pixel-wise scene labeling. “C” stands for the concatenation
operation.

features and global contexts. The inputs to our model include both photometric
and depth images. The path for extracting global contexts from the photometric
image consists of multiple convolutional layers and an extra memorized context
layer. On the other hand, the depth image is ﬁrst encoded as an HHA image,
which is fed into three convolutional layers [14] and an extra memorized context
layer for global depth context extraction. The other component, a memorized
fusion layer, is responsible for fusing previously extracted global RGB and depth
contexts in a data-driven manner. On top of the memorized fusion layer, the ﬁ-
nal convolutional feature of photometric channels and the fused global context
are concatenated together and fed into the ﬁnal fully convolutional layer, which
performs pixel-wise scene labeling with the softmax activation function.

3.1 Memorized Vertical Depth Context

Given a depth image, we use the HHA representation proposed in [13] to encode
geometric properties of the depth image in three channels, i.e., disparity, surface
normal and height. Diﬀerent from [13], the encoded HHA image in our pipeline is
fed into three randomly initialized convolutional layers (to obtain a feature map
with the same resolution as that in the RGB path) instead of layers taken from
the model pre-trained on the ILSVRC2012 dataset. This is because the color
distribution of HHA images is diﬀerent from that of natural images (see Fig.
2) according to [20]. One top of the third convolutional layer (i.e., HHAConv3),

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

7

there is an extra memorized context layer from Renet [19], which performs bi-
directional propagation of local contextual features from the convolutional layers
along the vertical direction. For better understanding, we denote the feature map
HHAConv3 as F = {fi,j}, where F ∈ Rw×h×c with w, h and c representing the
width, height and the number of channels. Since we perform pixel-wise scene
labeling, every patch in this Renet layer only contains a single pixel. Thus,
vertical memorized context layer (here we choose LSTM as recurrent unit) can
be formulated as

i,j = LSTM(hf
hf
i,j = LSTM(hb
hb

i,j−1, fi,j),
i,j+1, fi,j),

for j = 1, . . . , h

for j = h, . . . , 1,

(1)

(2)

where hf and hb stand for the hidden states of the forward and backward LSTM.
In the forward LSTM, the unit at pixel (i, j) takes hf
i,j−1 ∈ Rd and fi,j ∈ Rc as
input, and its output is calculated as follows according to [30]. The operations
in the backward LSTM can be deﬁned similarly.

i,j−1 + bi)
i,j−1 + bf )
i,j−1 + bo)

gatei = δ(Wif fi,j + Wihhf
gatef = δ(Wf f fi,j + Wf hhf
gateo = δ(Wof fi,j + Wohhf
gatec = tanh(Wcf fi,j + Wchhf
ci,j = gatef (cid:12) ci,j−1 + gatei (cid:12) gatec
hf
i,j = tanh(gateo (cid:12) ci,j)

i,j−1 + bc)

Finally, pixel-wise vertical depth contexts are collectively represented as a map,
Cdepth ∈ Rw×h×2d, where 2d is the total number of output channels from the
vertical memorized context layer.

(3)

3.2 Memorized Vertical Photometric Context

In the component for extracting global RGB contexts, we adapt the Deeplab ar-
chitecture proposed in [12]. Diﬀerent from existing Deeplab variants, we concate-
nate features at three diﬀerent scales to enrich the feature representation. This
is inspired by the network architecture in [33]. Speciﬁcally, since there exists hole
operations in Deeplab convolutional layers, feature maps from Conv2 2, Conv3 3
and Conv5 3 have suﬃcient initial resolutions. They can be further elevated to
the same resolution using interpolation. Corresponding pixel-wise features from
these three elevated feature maps are then concatenated together before being fed
into the subsequent memorized fusion layer, which again performs bi-directional
propagation to produce vertical photometric contexts. Here pixel-wise vertical
photometric contexts can also be represented as a map, CRGB ∈ Rw×h×2d, which
has the same dimensionalities as the map for vertical depth contexts.

8

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

3.3 Memorized Context Fusion

So far vertical depth and photometric contexts are computed independently
in parallel. Instead of simply concatenating these two types of contexts, the
memorized fusion layer, which performs horizontal bi-directional propagation
from Renet, is exploited for adaptively fusing vertical depth and RGB contexts
in a data-driven manner, and the output from this layer can be regarded as
the fused representation of both types of contexts. Such fusion can generate
more discriminative features through end-to-end training. The input and output
dimensions of the fusion layer are set to Rw×h×4d and Rw×h×2d, respectively.

Note that there are two separate memorized context layers in the photo-
metric and depth paths of our architecture. Since the memorized context layer
and the memorized fusion layer are two symmetric components of the original
Renet [19], a more natural and symmetric alternative would have a single memo-
rized context layer preceding the memorized fusion layer in our model (i.e., whole
structure of Renet including cascaded bi-directional vertical and horizonal mem-
orized layer) and let the memorized fusion layer incorporate the features from the
RGB and depth paths. Nonetheless, in our experiments, this alternative network
architecture gave rise to slightly worse performance.

3.4 Scene Labeling

Between photometric and depth images, photometric images contain more de-
tails and semantic information that can help scene labeling in comparison with
sparse and discontinuous depth images [14]. Nonetheless, depth images can pro-
vide auxiliary geometric information for improving scene labeling performance.
Thus, we design a cross-layer combination that integrates pixel-wise convolu-
tional features (i.e., Conv7 in Fig. 2) from the photometric image with fused
global contexts from the memorized fusion layer as the ﬁnal pixel-wise features,
which are fed into the last fully convolutional layer with softmax activation to
perform scene labeling at every pixel location.

4 Experimental Results

4.1 Experimental Setting

Datasets: We evaluate our proposed model for RGB-D scene labeling on three
public benchmarks, SUNRGBD, NYUDv2 and SUN3D. SUNRGBD [20] is the
largest dataset currently available, consisting of 10355 RGB-D images captured
from four diﬀerent depth sensors. It includes most previous datasets, such as
NYUDv2 depth [34], Berkeley B3DO [35], and SUN3D [36], as well as 3943
newly captured RGB-D images [20]. 5285 of these images are predeﬁned for
training and the remaining 5050 images constitute the testing set [14].
Implementation Details: In our experiments, a slightly modiﬁed Deeplab
pipeline [12] is adopted as the basic network in our RGB path for extracting
convolutional feature maps because of its high performance. It is initialized with

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

9

the publicly available VGG-16 model pre-trained on ImageNet. For the pur-
pose of pixel-wise scene labeling, this architecture transforms the last two fully
connected layers in the standard VGG-16 to convolutional layers with 1 × 1 ker-
nels. For the parallel depth path, three randomly initialized CNN layers with
max pooling are leveraged for depth feature extraction. In each path, on top
of the aforementioned convolutional network, a vertically bi-directional LSTM
layer implements the memorized context layer, and models both short-range
and long-range spatial dependencies. Then, another horizontally bi-directional
LSTM layer implements the memorized fusion layer, and is used to adaptively
integrate the global contexts from the two paths. In addition, there is a cross-
layer combination of ﬁnal convolutional features (i.e., Conv7) and the integrated
global representation from the horizontal LSTM layer.

Since the SUNRGBD dataset was collected by four diﬀerent depth sensors,
each input image is cropped to 426 × 426 (the smallest resolution of these four
sensors) [14]. During ﬁne-tuning, the learning rate for newly added layers, in-
cluding HHAConv1, HHAConv2, HHAConv3, the memorized context layers, the
memorized fusion layer and Conv8, is initialized to 10−2, and the learning rate
for those pre-trained layers of VGG-16 is initialized to 10−4. All weights in the
newly added convolutional layers are initialized using a Gaussian distribution
with a standard deviation equal to 0.01, and the weights in the LSTM layers are
randomly initialized with a uniform distribution over [−0.01, 0.01]. The number
of hidden memory cells in a memorized context layer or a memorized fusion layer
is set to 100, and the size of feature maps is 54 × 54. We train all the layers in
our deep network simultaneously using SGD with a momentum 0.9, the batch
size is set to one (due to limited GPU memory) and the weight decay is 0.0005.
The entire deep network is implemented on the publicly available platform Caﬀe
[37] and is trained on a single NVIDIA GeForce GTX TITAN X GPU with
12GB memory 1. It takes about 1 day to train our deep network. In the testing
stage, an RGB-D image takes 0.15s on average, which is signiﬁcantly faster than
pervious methods, i.e., the testing time in [9,24] is around 1.5s.

4.2 Results and Comparisons

According to [14,22], performance is evaluated by comparing class-wise Jaccard
Index, i.e., nii/ti, and average Jaccard Index, i.e., (1/ncl) (cid:80)
i nii/ti, where nij
is the number of pixels annotated as class i and predicted to be class j, ncl is
the number of diﬀerent classes, and ti = (cid:80)
j nij is the total number of pixels
annotated as class i [10].

SUNRGBD dataset [20]: The performance and comparison results on
SUNRGBD are shown in Table 1. Our proposed architecture can outperform
existing techniques: 2.2% higher than the performance reported in [22], 11.8%
higher than that in [24], 38% higher than that in [38] and 39.1% higher than that
in [20] in terms of 37-class average Jaccard Index. Improvements can be observed

1 LSTM-CF model is publicly available at: https://github.com/icemansina/LSTM-CF

10

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Table 1. Comparison of scene labeling results on SUNRGBD using class-wise and
average Jaccard Index. We compare our model with results reported in [20], [38], [24]
and previous state-of-the-art result in [22]. Boldface numbers mean best performance.

Wall
[20] 37.8
[20] 32.1
[20] 36.4
[38] 38.9
[38] 33.3
[38] 37.8
[24] 43.2
[22] 80.2
Ours 74.9

9.6
5.0
7.9
11.0
5.6
9.0

ﬂoor
45.0
42.6
45.8
47.2
43.8
48.3
78.6
90.9
82.3

chair sofa table door window bookshelf picture counter blinds
cabinet bed
9.4
16.9 12.8 18.5 6.1
21.8
17.4
0.8
21.5 4.1
12.5 3.4
6.4
2.9
12.8
19.9 11.6 19.3 6.0
23.3
15.4
9.6
17.2 13.4 20.4 6.8
21.5
18.8
0.9
12.9 3.8
22.3 3.9
6.3
3.0
13.1
20.8 12.1 20.9 6.8
23.6
17.2
23.1
42.5
26.2
33.2 40.6 34.3 33.2 43.6
31.2
64.8 76.0 58.6 62.6 47.7 66.4
58.9
43.1
67.7 55.5 57.8 45.6 52.8
62.1
47.3
paper towel shower box
ﬂoormat clothes ceiling books fridge tv
0.0
0.6
1.5
1.6
27.9
7.0
4.1
0.0
1.0
0.0
0.9
9.7
0.0
0.6
0.0
0.6
1.4
0.7
35.8
9.5
6.1
0.0
0.7
1.5
1.4
39.1
7.1
5.9
0.7
0.0
0.4
0.0
13.9
0.9
0.5
0.8
10.1 0.6
49.2
0.0
1.4
8.7
11.7
35.7
84.5
24.2 36.5 26.8 19.2 9.0
24.1
48.7 21.3 49.5 30.6 18.8 0.1
84.0
38.1
47.9 61.5 52.1 36.4 36.7 0
68.0

desk shelves curtain dresser pillow mirror
7.3
2.4
4.6
2.0
14.8
3.3
7.0
2.2
3.6
7.3
3.6
6.1
2.0
32.6
3.8
6.8
2.4
4.4
12.1 18.4
42.3
57.2
19.7 16.2
46.7
63.6
56.7
37.3 9.6
48.6
board person nightstand toilet sink
14.0
7.4
2.7
1.1
7.6
10.4
3.5
8.6
51.4
56.8
48.1

6.9
2.3
2.2
0.9
1.2
1.4
5.6
3.1
5.4
6.2
2.6
2.4
1.0
1.1
1.8
6.4
3.2
4.8
49.5
24.8
31.4
57.1 39.1
42.3
35.0
45.8 44.5
bathhub bag mean
8.3
0.9
0.6
5.3
0.4
0.0
9.0
0.6
1.1
9.3
1.1
0.9
6.0
0.5
0.0
10.1
1.3
1.1
18.6
47.0
36.3
24.1 45.9
45.1
23.6 48.1
65.6

4.3
2.0
4.4
6.9
3.6
7.8
59.1
67.0
63.4
lamp
0.9
0.7
0.9
1.3
0.8
1.2
44.2
48.8
58.0

8.9
2.3
12.0 15.2
12.3 14.8
2.6
14.9 16.8
64.1 53.0
73.0 66.2
68.8 67.9

1.2
0.3
1.4
1.3
0.6
1.6
27.0
24.4
28.4

1.1
2.6
1.7
1.5
1.5
1.8
25.0
42.9
36.4

0.0
0.3
0.7
0.0
0.3
0.8
35.7
17.9
72.6

2.2
1.7
5.2
2.6
2.2
6.2
31.8
33.8
39.4

1.0
15.3
1.7
1.2
10.1
1.0

1.9
0.1
0.2
2.2
0.3
0.2

1.2

[20] 0.0
[20] 0.0
[20] 0.0
[38] 0.0
[38] 0.0
[38] 0.0
[24] 5.6
[22] 0.1
Ours 0.0

in 15 class-wise Jaccard Indices. For a better understanding, we also show the
confusion matrix for this dataset in Fig. 3(a). It is worth mentioning that our
proposed architecture and most previous methods achieve zero accuracy on two
categories, i.e., ﬂoormat and shower, which mainly results from an imbalanced
data distribution instead of the capacity of our model.

NYUDv2 dataset: To further verify the eﬀectiveness of our architecture
and have more comparisons with existing state-of-the-art methods, we also con-
duct experiments on the NYUDv2 dataset. The results are presented in Table 2,
where the 13-class average Jaccard Index of our model is 20.3% higher than that
in [39]. Class frequencies and the confusion matrix are also shown in Table 2 and
Fig. 3(b) respectively. According to the reported results, our proposed architec-
ture gains 5.6% and 5.5% improvement in average Jaccard Index over [9] and
FCN-32s [10] respectively. Considering the listed class frequencies, our proposed
model signiﬁcantly outperforms existing methods on high frequency categories
and most low frequency categories, which primarily owes to the convolutional
features of the RGB image and the fused global contexts of the complete RGB-
D image. In terms of labeling categories with small and complex regions, e.g.,
pillows and chairs, our method also achieves a large improvement, which can be
veriﬁed in the following visual comparisons.

SUN3D dataset: Table 3 gives comparison results on the 1539 test images
in the SUN3D dataset. For fair comparison, the 12-class average Jaccard Index is
used in the comparison with the state-of-the-art results recently reported in [9].
Note that the 12-class accuracy of our network is calculated through the model
previously trained for 37 classes. Our model substantially outperforms the one
from [9] on large planar regions such as those labeled as ﬂoors and ceilings. This
also results from the incorporated convolutional features and the fused global
contexts. These comparison results further conﬁrm the power and generalization
capability of our LSTM-based model.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

11

(a) SUNRGBD

(b) NYUDv2

Fig. 3. Confusion matrix for SUNRGBD and NYUDv2. Class-wise Jaccard Index is
shown on the diagonal. Best viewed in color.

Table 2. Comparison of scene labeling on NYUDv2. We compare our proposed model
with existing state-of-the-art methods, i.e., [34], [24], [25], [26] and [9]. Class-wise Jac-
card Index and average Jaccard Index of 37 classes are presented. ‘Freq’ stands for
class frequency. Boldface numbers mean best performance.

Wall
Freq 21.4
[34] 60.7
[24] 60.0
[25] 67.4
[26] 61.4
[9]
65.7
Ours 79.6

2.1

2.7

ﬂoor
9.1
77.8
74.4
80.5
66.4
62.5
83.5

chair sofa table door window bookshelf picture counter blinds
cabinet bed
1.9
2.1
3.3
2.2
3.8
6.2
22.7
32.4 25.3 21.0 5.9
29.7
40.3
33.0
17.3
32.5 28.2 16.6 12.9 27.7
42.3
37.1
20.5
40.4 44.8 30.0 12.1 34.1
56.4
41.4
17.6
34.4 33.8 22.6 8.3
27.6
43.9
38.2
36.3
32.1
40.1
44.5 50.8 43.5 51.6 49.2
59.5
77.0 58.3 64.9 42.6 47.0 43.6
69.3
paper towel shower box
ﬂoormat clothes ceiling books fridge tv
0.3
0.4
0.4
0.4
0.5
0.6
0.1
5.7
3.6
12.7 0.1
1.4
3.3
1.9
18.6 11.7 12.6 5.4
1.1
14.5 14.4 14.1 19.8 6.0
0.8
6.1
2.6
2.9
36.3
17.6

desk shelves curtain dresser pillow mirror
1.0
1.1
1.7
2.1
4.7
3.3
40.6
35.7
10.1 6.1
26.5
32.4
10.1 1.6
44.7
38.7
5.1
2.7
33.6
27.7
48.0 45.2
55.8
41.4
74.5
33.6 13.1
74.6
board person nightstand toilet sink
0.3
0.3
0.0
0.2
12.9
28.2
60.6
93.9

1.0
0.8
0.9
4.4
18.9
13.3
17.9
19.7
7.0
14.6
31.3
21.6
12.5
10.7
16.8
50.5 46.1
55.3
56.5
48.0 47.7
bathhub bag mean
0.2
0.3
17.5
0.0
0.0
20.2
1.2
7.8
30.0
0.2
29.4
0.2
16.2
29.2
37.3 43.9
38.5
49.4
7.5
72.6

0.6
5.5
14.8
5.8
3.6
39.1 53.6 50.1 35.4 39.9 41.8
49.7 0.0

1.1
27.4
27.6
26.3
18.9
53.1
53.2
lamp
0.3
15.9
14.2
31.2
5.4
26.3
67.6

0.3
26.7 25.1
35.2 28.9
52.5 47.9
32
20.9
31.8 22.5
81.8 58.4

1.4
73.2
53.9
61.8
46.1
50.6
70.2

0.7
6.5
9.5
8.0
2.7
35.4
22.7

0.3
6.6
13.6
1.5
5
35.6
77.0

0.3
6.3
9.2
15.7
6.9
32.5
0

1.4
33.1
38.6
50.7
30.2
39.2
68.2

0.0 52.1 60.6 0

6.2

3.2

Freq 0.7
[34] 7.1
[24] 20.1
[25] 28.2
[26] 13.8
[9]
54.1
Ours 0.0

4.3 Ablation Study

To discover the vital elements in our proposed model, we conduct an ablation
study to remove or replace individual components in our deep network when
training and testing on the SUNRGBD dataset. Speciﬁcally, we have tested the
performance of our model without the RGB path, the depth path, multi-scale
RGB feature concatenation, the memorized context layers or the memorized
fusion layer. In addition, we also conduct an experiment with a model that does
not combine the ﬁnal convolutional features of photometric channels (i.e., Conv7
in Fig. 2) with the global contexts of the complete RGB-D image to ﬁgure out the
importance of diﬀerent components. The results are presented in Table 4. From
the given results, we ﬁnd that the ﬁnal convolutional features of the photometric
channels is the most vital information, i.e., the cross-layer combination is the
most eﬀective component as the performance drops to 15.2% without it, which
is consistent with previously mentioned properties of depth and photometric
data. In addition, multi-scale RGB feature concatenation before the memorized
context layer also plays a vital role as it directly aﬀects the vertical contexts in

12

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Table 3. Comparison of class-wise Jaccard Index and 12-class average Jaccard Index
on SUN3D.

wall ﬂoor bed chair table counter curtain ceiling tv toilet bathtub bag Mean
73 35

[9]
71 35
Ours 73 86 32 65

30
57

52
22

68
76

27
69

56 23
75 62

49
62

29 45.7
23 58.5

Table 4. Ablation Study

Model

Without RGB path, using Deeplab+Renet for depth path
Without depth path
Without multi-scale RGB feature concatenation
Without cross-layer integration of RGB convolutional features
Without memorized fusion layer
Without memorized context layers
Without any memorized (context or fusion) layers

Mean Accuracy
15.8%
43.7%
42.1%
15.2%
44.7%
45.7%
45.0%

the photometric channels and the performance drops to 42.1% without it. It is
obvious that performance would be inevitably harmed without the depth path.
Among the memorized layers, the memorized fusion layer is more important
than the memorized context layers in our pipeline as it accomplishes the fusion
of contexts in photometric and depth channels.

4.4 Visual Comparisons

SUNRGBD Dataset: We present visual results of RGB-D scene labeling in
Fig. 4. Here, we leverage super-pixel based averaging to smooth visual labeling
results as being done in [9]. The algorithm in [40] is used for performing super-
pixel segmentation. As can be observed in Fig. 4, our proposed deep network
produces accurate and semantically meaningful labeling results, especially for
large regions and high frequency labels. For instance, our model takes advantage
of global contexts when labeling ‘bed’ in Fig. 4(a), ‘wall’ in Fig. 4(e) and ‘mirror’
in Fig. 4(i). Our proposed model can precisely label almost all ‘chairs’ (a high
frequency label) by exploiting integrated photometric and depth information,
regardless of occlusions.

NYUDv2 Dataset: We also perform visual comparisons on the NYUDv2
benchmark, which has complicated indoor scenes and well-labeled ground truth.
We compare our scene labeling results with those publicly released labeling re-
sults from [25]. It is obvious that our results are clearly better than those from
[25] both visually and numerically (under the metric of average Jaccard Index)
even though scene labeling in [25] is based on sophisticated segmentation.

Label Reﬁnement: Surprisingly, our model can intelligently reﬁne certain
region annotations, which might have inaccuracies due to under-segmentation,
especially in the newly captured 3943 RGB-D images, as shown in Fig. 6. Specif-
ically, the cabinets in Fig. 6(a) were annotated as ‘background’, the pillows in

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

13

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

(q) legend of semantic labels

Fig. 4. Examples of semantic labeling results on the SUNRGBD dataset. The top row
shows the input RGB images, the bottom row shows scene labeling obtained with our
model and the middle row has the ground truth. Semantic labels and their correspond-
ing colors are shown at the bottom.

Fig. 6(g) as ‘bed’, and the tables in Fig. 6(n) as ‘wall’ by mistake. Our model
can eﬀectively deal with these diﬃcult regions. For example, the annotation of
the picture in Fig. 6(e) and that of the pillows in Fig. 6(g) have been corrected.
Thus, our model can be exploited to reﬁne certain annotations in the SUNRGBD
dataset, which is another contribution of our model.

5 Conclusions

In this paper, we have developed a novel Long Short-Term Memorized Con-
text Fusion (LSTM-CF) model that captures image contexts from a global per-
spective and deeply fuses contextual representations from multiple sources (i.e.,
depth and photometric data) for semantic scene labeling. In future, we will ex-
plore how to extend the memorized layers with an attention mechanism, and
reﬁne the performance of our model in boundary labeling.

14

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Fig. 5. Visual comparison of scene labeling results on the NYUDv2 dataset. The ﬁrst
and second rows show the input RGB images and their corresponding groundtruth
labeling. The third row shows the results from [25] and the last row shows the results
from our model.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

Fig. 6. Annotation reﬁnement on the SUNRGBD dataset. The top row shows the input
RGB images, the middle row shows the original annotations, and the bottom row shows
scene labeling results from our model.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

15

References

1. Wu, C., Lenz, I., Saxena, A.: Hierarchical semantic labeling for task-relevant rgb-d

perception. In: Robotics: Science and systems (RSS). (2014)

2. Hinterstoisser, S., Lepetit, V., Ilic, S., Holzer, S., Bradski, G., Konolige, K., Navab,
N.: Model based training, detection and pose estimation of texture-less 3d objects
In: Computer Vision–ACCV 2012. Springer (2012)
in heavily cluttered scenes.
548–562

3. Holz, D., Holzer, S., Rusu, R.B., Behnke, S.: Real-time plane segmentation using
rgb-d cameras. In: RoboCup 2011: robot soccer world cup XV. Springer (2011)
306–317

4. Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., Manning, C.D.: Generating se-
mantically precise scene graphs from textual descriptions for improved image re-
trieval. In: Proceedings of the Fourth Workshop on Vision and Language. (2015)
70–80

5. Yan, Z., Zhang, H., Wang, B., Paris, S., Yu, Y.: Automatic photo adjustment using

deep neural networks. ACM Transactions on Graphics 35(2) (2016)

6. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions
on 35(8) (2013) 1915–1929

7. Gould, S., Fulton, R., Koller, D.: Decomposing a scene into geometric and se-
mantically consistent regions. In: Computer Vision, 2009 IEEE 12th International
Conference on, IEEE (2009) 1–8

8. Tighe, J., Lazebnik, S.: Superparsing: scalable nonparametric image parsing with

superpixels. In: Computer Vision–ECCV 2010. Springer (2010) 352–365

9. Khan, S.H., Bennamoun, M., Sohel, F., Togneri, R., Naseem, I.: Integrating geo-
metrical context for semantic labeling of indoor scenes using rgbd images. Inter-
national Journal of Computer Vision (2015) 1–20

10. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3431–3440

11. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: Proceed-
ings of the IEEE International Conference on Computer Vision. (2015) 1529–1537
12. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. arXiv
preprint arXiv:1412.7062 (2014)

13. Gupta, S., Girshick, R., Arbel´aez, P., Malik, J.: Learning rich features from rgb-d
images for object detection and segmentation. In: Computer Vision–ECCV 2014.
Springer (2014) 345–360

14. Song, S., Xiao, J.: Deep sliding shapes for amodal 3d object detection in rgb-d

images. arXiv preprint arXiv:1511.02300 (2015)

15. Liu, Z., Li, X., Luo, P., Loy, C.C., Tang, X.: Semantic image segmentation via
deep parsing network. In: Proceedings of the IEEE International Conference on
Computer Vision. (2015) 1377–1385

16. Liang, X., Shen, X., Xiang, D., Feng, J., Lin, L., Yan, S.: Semantic object parsing
with local-global long short-term memory. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. (2016)

17. Byeon, W., Breuel, T.M., Raue, F., Liwicki, M.: Scene labeling with lstm recurrent
neural networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3547–3555

16

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

18. Pinheiro, P., Collobert, R.: Recurrent convolutional neural networks for scene
labeling. In: Proceedings of the 31st International Conference on Machine Learning
(ICML-14). (2014) 82–90

19. Visin, F., Kastner, K., Cho, K., Matteucci, M., Courville, A., Bengio, Y.: Renet:
A recurrent neural network based alternative to convolutional networks. arXiv
preprint arXiv:1505.00393 (2015)

20. Song, S., Lichtenberg, S.P., Xiao, J.: Sun rgb-d: A rgb-d scene understanding
benchmark suite. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. (2015) 567–576

21. Kumar, M.P., Koller, D.: Eﬃciently selecting regions for scene understanding. In:
Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
IEEE (2010) 3217–3224

22. Kendall, A., Badrinarayanan, V., Cipolla, R.: Bayesian segnet: Model uncertainty
in deep convolutional encoder-decoder architectures for scene understanding. arXiv
preprint arXiv:1511.02680 (2015)

23. Lempitsky, V., Vedaldi, A., Zisserman, A.: Pylon model for semantic segmentation.

In: Advances in neural information processing systems. (2011) 1485–1493
24. Ren, X., Bo, L., Fox, D.: Rgb-(d) scene labeling: Features and algorithms.

In:
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,
IEEE (2012) 2759–2766

25. Gupta, S., Arbel´aez, P., Girshick, R., Malik, J.: Indoor scene understanding with
rgb-d images: Bottom-up segmentation, object detection and semantic segmenta-
tion. International Journal of Computer Vision 112(2) (2015) 133–149

26. Wang, A., Lu, J., Cai, J., Wang, G., Cham, T.J.: Unsupervised joint feature learn-
ing and encoding for rgb-d scene labeling. Image Processing, IEEE Transactions
on 24(11) (2015) 4459–4473

27. Husain, F., Schulz, H., Dellen, B., Torras, C., Behnke, S.: Combining semantic and
geometric features for object class segmentation of indoor scenes. IEEE Robotics
and Automation Letters 2(1) (2017) 49–55

28. Schmidhuber, J.: A local learning algorithm for dynamic feedforward and recurrent

networks. Connection Science 1(4) (1989) 403–412

29. Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with gra-
dient descent is diﬃcult. Neural Networks, IEEE Transactions on 5(2) (1994)
157–166

30. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

9(8) (1997) 1735–1780

31. Graves, A., Schmidhuber, J.: Oﬄine handwriting recognition with multidimen-
sional recurrent neural networks. In: Advances in neural information processing
systems. (2009) 545–552

32. Stollenga, M.F., Byeon, W., Liwicki, M., Schmidhuber, J.:

Parallel multi-
dimensional lstm, with application to fast biomedical volumetric image segmen-
tation. In: Advances in Neural Information Processing Systems. (2015) 2980–2988
In: IEEE

33. Li, G., Yu, Y.: Deep contrast learning for salient object detection.

Conference on Computer Vision and Pattern Recognition (CVPR). (2016)

34. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and support
inference from rgbd images. In: Computer Vision–ECCV 2012. Springer (2012)
746–760

35. Janoch, A., Karayev, S., Jia, Y., Barron, J.T., Fritz, M., Saenko, K., Darrell, T.: A
category-level 3d object dataset: Putting the kinect to work. In: Consumer Depth
Cameras for Computer Vision. Springer (2013) 141–165

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

17

36. Xiao, J., Owens, A., Torralba, A.: Sun3d: A database of big spaces reconstructed
using sfm and object labels. In: Proceedings of the IEEE International Conference
on Computer Vision. (2013) 1625–1632

37. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

38. Liu, C., Yuen, J., Torralba, A.: Sift ﬂow: Dense correspondence across scenes and
its applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on
33(5) (2011) 978–994

39. Couprie, C., Farabet, C., Najman, L., LeCun, Y.: Toward real-time indoor semantic
segmentation using depth information. Journal of Machine Learning Research
(2014)

40. Felzenszwalb, P.F., Huttenlocher, D.P.: Eﬃcient graph-based image segmentation.

International Journal of Computer Vision 59(2) (2004) 167–181

6
1
0
2
 
l
u
J
 
6
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
0
0
5
0
.
4
0
6
1
:
v
i
X
r
a

LSTM-CF: Unifying Context Modeling and
Fusion with LSTMs for RGB-D Scene Labeling

Zhen Li1, Yukang Gan2, Xiaodan Liang2, Yizhou Yu1,
Hui Cheng2, and Liang Lin2(cid:63)

1 Department of Computer Science, The University of Hong Kong, Hong Kong,
lizhen36@hku.hk,yizhouy@acm.org
2 School of Data and Computer Science, Sun Yat-sen University, Guangzhou,
ganyk@mail2.sysu.edu.cn,xdliang328@gmail.com
chengh9@mail.sysu.edu.cn,linliang@ieee.org

Abstract. Semantic labeling of RGB-D scenes is crucial to many in-
telligent applications including perceptual robotics. It generates pixel-
wise and ﬁne-grained label maps from simultaneously sensed photomet-
ric (RGB) and depth channels. This paper addresses this problem by i)
developing a novel Long Short-Term Memorized Context Fusion (LSTM-
CF) Model that captures and fuses contextual information from multiple
channels of photometric and depth data, and ii) incorporating this model
into deep convolutional neural networks (CNNs) for end-to-end training.
Speciﬁcally, contexts in photometric and depth channels are, respectively,
captured by stacking several convolutional layers and a long short-term
memory layer; the memory layer encodes both short-range and long-
range spatial dependencies in an image along the vertical direction. An-
other long short-term memorized fusion layer is set up to integrate the
contexts along the vertical direction from diﬀerent channels, and per-
form bi-directional propagation of the fused vertical contexts along the
horizontal direction to obtain true 2D global contexts. At last, the fused
contextual representation is concatenated with the convolutional features
extracted from the photometric channels in order to improve the accu-
racy of ﬁne-scale semantic labeling. Our proposed model has set a new
state of the art, i.e., 48.1% and 49.4% average class accuracy over 37
categories (2.2% and 5.4% improvement) on the large-scale SUNRGBD
dataset and the NYUDv2 dataset, respectively.

Keywords: RGB-D scene labeling, image context modeling, long short-
term memory, depth and photometric data fusion

1 Introduction

Scene labeling, also known as semantic scene segmentation, is one of the most
fundamental problems in computer vision. It refers to associating every pixel in

(cid:63) The corresponding author is Liang Lin. This work was support by Projects on Fac-
ulty/ Student Exchange and Collaboration Scheme between the Higher Education in
Hong Kong and the Mainland, Guangzhou Science and Technology Program under
grant 1563000439, and Fundamental Research Funds for the Central Universities.

2

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Fig. 1. An illustration of global context modeling and fusion for RGB-D images. Our
LSTM-CF model ﬁrst captures vertical contexts through a memory network layer en-
coding short- and long-range spatial dependencies along the vertical direction. After a
concatenation operation (denoted by “C”) over photometric and depth channels, our
model utilizes another memory network layer to fuse vertical contexts from all chan-
nels in a data-driven way and performs bi-directional propagation along the horizontal
direction to obtain true 2D global contexts. Best viewed in color.

an image with a semantic label, such as table, road and wall, as illustrated in
Fig. 1. High-quality scene labeling can be beneﬁcial to many intelligent tasks,
including robot task planning [1], pose estimation [2], plane segmentation [3],
context-based image retrieval [4], and automatic photo adjustment [5].

Previous work on scene labeling can be divided into two categories accord-
ing to their target scenes: indoor and outdoor scenes. Compared with outdoor
scene labeling [6,7,8], indoor scene labeling is more challenging due to a larger
set of semantic labels, more severe object occlusions, and more diverse object
appearances [9]. For example, indoor object classes, such as beds covered with
diﬀerent sheets and various appearances of curtains, are much harder to char-
acterize than outdoor classes, e.g., roads, buildings, and sky, through photo-
metric channels only. Recently, utilizing depth sensors to augment RGB data
have eﬀectively improved the performance of indoor scene labeling because the
depth channel complements photometric channels with structural information.
Nonetheless, two key issues remain open in the literature of RGB-D scene label-
ing.

(I) How to eﬀectively represent and fuse the coexisting depth and
photometric (RGB) data For data representation, a batch of sophisticated
hand-crafted features have been developed in previous methods. Such hand-
crafted features are somewhat ad hoc and less discriminative than those RGB-D
representations learned using convolutional neural networks (CNNs) [10,11,12,13,14].
However, in these CNN-related works, the fusion of depth and photometric data
has often been oversimpliﬁed. For instance, in [13,14], two independent CNNs are
leveraged to extract features from depth and photometric data separately, and
such features are simply concatenated before used for ﬁnal classiﬁcation. Over-
looking the strong correlation between depth and photometric channels could
inevitably harm semantic labeling.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

3

(II) How to capture global scene contexts during feature learning
Current CNN-based scene labeling approaches can only capture local contextual
information for every pixel due to their restricted receptive ﬁelds, resulting in
suboptimal labeling results. In particular, long-range dependencies sometimes
play a key role in distinguishing among diﬀerent objects having similar appear-
ances, e.g., labeling “ceiling” and “ﬂoor” in Fig. 1, according to the global scene
layout. To overcome this issue, graphical models, such as a conditional random
ﬁeld [9,11] or a mean-ﬁeld approximation [15], have been applied to improve
prediction results in a post-processing step. These methods, however, separate
context modeling from convolutional feature learning, which may give rise to
suboptimal results on complex scenes due to less discriminative feature repre-
sentation [16]. An alternative class of methods adopts cascaded recurrent neural
networks (RNNs) with gate structures, e.g., long short-term memory (LSTM)
networks, to explicitly strengthen context modeling [16,17,18]. In these methods,
the long- and short-range dependencies can be well memorized by sequentially
running the network over individual pixels.

To address the aforementioned challenges, this paper proposes a novel Long
Short-Term Memorized Context Fusion (LSTM-CF) model and demonstrates
its superiority in RGB-D scene labeling. Fig. 1 illustrates the brief idea of using
memory networks for context modeling and fusion of diﬀerent channels. Our
LSTM-CF model captures 2D dependencies within an image by exploiting the
cascaded bi-directional vertical and horizontal RNN models as introduced in
[19].

Our method constructs HHA images [13] for the depth channel through ge-
ometric encoding, and uses several convolutional layers for extracting features.
Inspired by [19], these convolutional layers are followed by a memorized context
layer to model both short-range and long-range spatial dependencies along the
vertical direction. For photometric channels, we generate convolutional features
using the Deeplab network [12], which is also followed by a memorized context
layer for context modeling along the vertical direction. Afterwards, a memorized
fusion layer is set up to integrate the contexts along the vertical direction from
both photometric and depth channels, and perform bi-directional propagation of
the fused vertical contexts along the horizontal direction to obtain true 2D global
contexts. Considering the features diﬀerences, e.g., signal frequency and other
characteristics (color/geometry) [20], our fusion layer facilitates deep integration
of contextual information from multiple channels in a data-driven manner rather
than simply concatenating diﬀerent feature vectors. Since photometric channels
usually contain ﬁner details in comparison to the depth channel [20], we further
enhance the network with cross-layer connections that append convolutional fea-
tures of the photometric channels to the fused global contexts before the ﬁnal
fully convolutional layer, which predicts pixel-wise semantic labels. Various lay-
ers in our LSTM-CF model are tightly integrated, and the entire network is
amenable to end-to-end training and testing.

In summary, this paper has the following contributions to the literature of

RGB-D scene labeling.

4

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

– It proposes a novel Long Short-Term Memorized Context Fusion (LSTM-
CF) Model, which is capable of capturing image contexts from a global
perspective and deeply fusing contextual information from multiple sources
(i.e., depth and photometric channels).

– It proposes to jointly optimize LSTM layers and convolutional layers for
achieving better performance in semantic scene labeling. Context modeling
and fusion are incorporated into the deep network architecture to enhance
the discriminative power of feature representation. This architecture can also
be extended to other similar tasks such as object/part parsing.

– It is demonstrated on the large-scale SUNRGBD benchmark (including 10355
images) and canonical NYUDv2 benchmark that our method outperforms
existing state-of-the-art methods. In addition, it is found that our scene la-
beling results can be leveraged to improve the groundtruth annotations of
newly captured 3943 RGB-D images in SUNRGBD dataset.

2 Related work

Scene Labeling: Scene labeling has caught researchers’ attention frequently
[6,11,12,16,17,18,21] in recent years. Instead of extracting features from over-
segmented images, recent methods usually utilize powerful CNN layers as the
feature extractor, taking advantage of fully convolutional networks (FCNs) [10]
and its variants [22] to obtain pixel-wise dense features. Another main challenge
for scene labeling is the fusion of local and global contexts, i.e., taking advantage
of global contexts to reﬁne local decisions. For instance, [6] exploits families of
segmentations or trees to generate segment candidates. [23] utilizes an inference
method based on graph cut to achieve image labeling. A pixel-wise conditional
random forest is used in [11,12] to directly optimize a deep CNN-driven cost
function. Most of the above models improve accuracy through carefully designed
processing on the predicted conﬁdence map instead of proposing more powerful
discriminative features, which usually results in suboptimal prediction results
[16]. The topological structure of recurrent neural networks (RNNs) is used to
model short- and long-range dependencies in [16,18]. In [17], a multi-directional
RNN is leveraged to extract local and global contexts without using a CNN,
which is well suited for low-resolution and relatively simple scene labeling prob-
lems. In contrast, our model can jointly optimize LSTM layers and convolutional
layers to explicitly improve discriminative feature learning for local and global
context modeling and fusion.

Scene Labeling in RGB-D images: With more and more convenient ac-
cess to aﬀordable depth sensors, scene labeling in RGB-D images [9,13,14,24,25,26]
enables a rapid progress of scene understanding. Various sophisticated hand-
crafted features are utilized in previous state-of-the-art methods. Speciﬁcally,
kernel descriptions based on traditional multi-channel features, such as color,
depth gradient, and surface normal, are used as photometric and depth features
[24]. A rich feature set containing various traditional features, e.g., SIFT, HOG,
LBP and plane orientation, are used as local appearance features and plane ap-
pearance features in [9]. HOG features of RGB images and HOG+HH (histogram

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

5

of height) features of depth images are extracted as representations in [25] for
training successive classiﬁers. In [27], proposed distance-from-wall features are
exploited to improve scene labeling performance. In addition, an unsupervised
joint feature learning and encoding model is proposed for scene labeling in [26].
However, due to the limited number of RGB-D images, deep learning for scene
labeling in RGB-D images was not as appealing as that for RGB images. The
release of the SUNRGBD dataset, which includes most of the previously popular
datasets, may have changed this situation [13,14].

Another main challenge imposed by scene labeling in RGB-D images is the
fusion of contextual representations of diﬀerent sources (i.e., depth and photo-
metric data). For instance, in [13,14], two independent CNNs are leveraged to
extract features from the depth and photometric data separately, which are then
simply concatenated for class prediction. Ignoring the strong correlation between
depth and photometric channels usually negatively aﬀects semantic labeling. In
contrast, instead of simply concatenating features from multiple sources, the
memorized fusion layer in our model facilitates the integration of contextual
information from diﬀerent sources in a data-driven manner,

RNN for Image Processing: Recurrent neural networks (RNNs) represent
a type of neural networks with loop connections [28]. They are designed to cap-
ture dependencies across a distance larger than the extent of local neighborhoods.
In previous work, RNN models have not been widely used partially due to the
diﬃculty to train such models, especially for sequential data with long-range de-
pendencies [29]. Fortunately, RNNs with gate and memory structures, e.g., long
short-term memory (LSTM) [30], can artiﬁcially learn to remember and forget
information by using speciﬁc gates to control the information ﬂow. Although
RNNs have an outstanding capability to capture short-range and long-range de-
pendencies, there exist problems for applying RNNs to image processing due to
the fact that, unlike data in natural language processing (NLP) tasks, images
do not have a natural sequential structure. Thus, diﬀerent strategies have been
proposed to overcome this problem. Speciﬁcally, in [19], cascaded bi-directional
vertical and horizonal RNN layers are designed for modeling 2D dependencies in
images. A multi-dimensional RNN with LSTM unit has been applied to hand-
writing [31]. A parallel multi-dimensional LSTM for image segmentation has
been proposed in [32]. In this paper, we propose an LSTM-CF model consisting
of memorized context layers and a memorized fusion layer to capture image con-
texts from a global perspective and fuse contextual representations from diﬀerent
sources.

3 LSTM-CF Model

As illustrated in Fig. 2, our end-to-end LSTM-CF model for RGB-D scene label-
ing consists of four components, layers for vertical depth context extraction, lay-
ers for vertical photometric context extraction, a memorized fusion layer for in-
corporating vertical photometric and depth contexts as true 2D global contexts,
and a ﬁnal layer for pixel-wise scene labeling given concatenated convolutional

6

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Fig. 2. Our LSTM-CF model for RGB-D scene labeling. The input consists of both
photometric and depth channels. Vertical contexts in photometric and depth channels
are computed in parallel using cascaded convolutional layers and a memorized context
layer. Vertical photometric (color) and depth contexts are fused and bi-directionally
propagated along the horizontal direction via another memorized fusion layer to ob-
tain true 2D global contexts. The fused global contexts and the ﬁnal convolutional
features of photometric channels are then concatenated together and fed into the ﬁ-
nal convolutional layer for pixel-wise scene labeling. “C” stands for the concatenation
operation.

features and global contexts. The inputs to our model include both photometric
and depth images. The path for extracting global contexts from the photometric
image consists of multiple convolutional layers and an extra memorized context
layer. On the other hand, the depth image is ﬁrst encoded as an HHA image,
which is fed into three convolutional layers [14] and an extra memorized context
layer for global depth context extraction. The other component, a memorized
fusion layer, is responsible for fusing previously extracted global RGB and depth
contexts in a data-driven manner. On top of the memorized fusion layer, the ﬁ-
nal convolutional feature of photometric channels and the fused global context
are concatenated together and fed into the ﬁnal fully convolutional layer, which
performs pixel-wise scene labeling with the softmax activation function.

3.1 Memorized Vertical Depth Context

Given a depth image, we use the HHA representation proposed in [13] to encode
geometric properties of the depth image in three channels, i.e., disparity, surface
normal and height. Diﬀerent from [13], the encoded HHA image in our pipeline is
fed into three randomly initialized convolutional layers (to obtain a feature map
with the same resolution as that in the RGB path) instead of layers taken from
the model pre-trained on the ILSVRC2012 dataset. This is because the color
distribution of HHA images is diﬀerent from that of natural images (see Fig.
2) according to [20]. One top of the third convolutional layer (i.e., HHAConv3),

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

7

there is an extra memorized context layer from Renet [19], which performs bi-
directional propagation of local contextual features from the convolutional layers
along the vertical direction. For better understanding, we denote the feature map
HHAConv3 as F = {fi,j}, where F ∈ Rw×h×c with w, h and c representing the
width, height and the number of channels. Since we perform pixel-wise scene
labeling, every patch in this Renet layer only contains a single pixel. Thus,
vertical memorized context layer (here we choose LSTM as recurrent unit) can
be formulated as

i,j = LSTM(hf
hf
i,j = LSTM(hb
hb

i,j−1, fi,j),
i,j+1, fi,j),

for j = 1, . . . , h

for j = h, . . . , 1,

(1)

(2)

where hf and hb stand for the hidden states of the forward and backward LSTM.
In the forward LSTM, the unit at pixel (i, j) takes hf
i,j−1 ∈ Rd and fi,j ∈ Rc as
input, and its output is calculated as follows according to [30]. The operations
in the backward LSTM can be deﬁned similarly.

i,j−1 + bi)
i,j−1 + bf )
i,j−1 + bo)

gatei = δ(Wif fi,j + Wihhf
gatef = δ(Wf f fi,j + Wf hhf
gateo = δ(Wof fi,j + Wohhf
gatec = tanh(Wcf fi,j + Wchhf
ci,j = gatef (cid:12) ci,j−1 + gatei (cid:12) gatec
hf
i,j = tanh(gateo (cid:12) ci,j)

i,j−1 + bc)

Finally, pixel-wise vertical depth contexts are collectively represented as a map,
Cdepth ∈ Rw×h×2d, where 2d is the total number of output channels from the
vertical memorized context layer.

(3)

3.2 Memorized Vertical Photometric Context

In the component for extracting global RGB contexts, we adapt the Deeplab ar-
chitecture proposed in [12]. Diﬀerent from existing Deeplab variants, we concate-
nate features at three diﬀerent scales to enrich the feature representation. This
is inspired by the network architecture in [33]. Speciﬁcally, since there exists hole
operations in Deeplab convolutional layers, feature maps from Conv2 2, Conv3 3
and Conv5 3 have suﬃcient initial resolutions. They can be further elevated to
the same resolution using interpolation. Corresponding pixel-wise features from
these three elevated feature maps are then concatenated together before being fed
into the subsequent memorized fusion layer, which again performs bi-directional
propagation to produce vertical photometric contexts. Here pixel-wise vertical
photometric contexts can also be represented as a map, CRGB ∈ Rw×h×2d, which
has the same dimensionalities as the map for vertical depth contexts.

8

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

3.3 Memorized Context Fusion

So far vertical depth and photometric contexts are computed independently
in parallel. Instead of simply concatenating these two types of contexts, the
memorized fusion layer, which performs horizontal bi-directional propagation
from Renet, is exploited for adaptively fusing vertical depth and RGB contexts
in a data-driven manner, and the output from this layer can be regarded as
the fused representation of both types of contexts. Such fusion can generate
more discriminative features through end-to-end training. The input and output
dimensions of the fusion layer are set to Rw×h×4d and Rw×h×2d, respectively.

Note that there are two separate memorized context layers in the photo-
metric and depth paths of our architecture. Since the memorized context layer
and the memorized fusion layer are two symmetric components of the original
Renet [19], a more natural and symmetric alternative would have a single memo-
rized context layer preceding the memorized fusion layer in our model (i.e., whole
structure of Renet including cascaded bi-directional vertical and horizonal mem-
orized layer) and let the memorized fusion layer incorporate the features from the
RGB and depth paths. Nonetheless, in our experiments, this alternative network
architecture gave rise to slightly worse performance.

3.4 Scene Labeling

Between photometric and depth images, photometric images contain more de-
tails and semantic information that can help scene labeling in comparison with
sparse and discontinuous depth images [14]. Nonetheless, depth images can pro-
vide auxiliary geometric information for improving scene labeling performance.
Thus, we design a cross-layer combination that integrates pixel-wise convolu-
tional features (i.e., Conv7 in Fig. 2) from the photometric image with fused
global contexts from the memorized fusion layer as the ﬁnal pixel-wise features,
which are fed into the last fully convolutional layer with softmax activation to
perform scene labeling at every pixel location.

4 Experimental Results

4.1 Experimental Setting

Datasets: We evaluate our proposed model for RGB-D scene labeling on three
public benchmarks, SUNRGBD, NYUDv2 and SUN3D. SUNRGBD [20] is the
largest dataset currently available, consisting of 10355 RGB-D images captured
from four diﬀerent depth sensors. It includes most previous datasets, such as
NYUDv2 depth [34], Berkeley B3DO [35], and SUN3D [36], as well as 3943
newly captured RGB-D images [20]. 5285 of these images are predeﬁned for
training and the remaining 5050 images constitute the testing set [14].
Implementation Details: In our experiments, a slightly modiﬁed Deeplab
pipeline [12] is adopted as the basic network in our RGB path for extracting
convolutional feature maps because of its high performance. It is initialized with

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

9

the publicly available VGG-16 model pre-trained on ImageNet. For the pur-
pose of pixel-wise scene labeling, this architecture transforms the last two fully
connected layers in the standard VGG-16 to convolutional layers with 1 × 1 ker-
nels. For the parallel depth path, three randomly initialized CNN layers with
max pooling are leveraged for depth feature extraction. In each path, on top
of the aforementioned convolutional network, a vertically bi-directional LSTM
layer implements the memorized context layer, and models both short-range
and long-range spatial dependencies. Then, another horizontally bi-directional
LSTM layer implements the memorized fusion layer, and is used to adaptively
integrate the global contexts from the two paths. In addition, there is a cross-
layer combination of ﬁnal convolutional features (i.e., Conv7) and the integrated
global representation from the horizontal LSTM layer.

Since the SUNRGBD dataset was collected by four diﬀerent depth sensors,
each input image is cropped to 426 × 426 (the smallest resolution of these four
sensors) [14]. During ﬁne-tuning, the learning rate for newly added layers, in-
cluding HHAConv1, HHAConv2, HHAConv3, the memorized context layers, the
memorized fusion layer and Conv8, is initialized to 10−2, and the learning rate
for those pre-trained layers of VGG-16 is initialized to 10−4. All weights in the
newly added convolutional layers are initialized using a Gaussian distribution
with a standard deviation equal to 0.01, and the weights in the LSTM layers are
randomly initialized with a uniform distribution over [−0.01, 0.01]. The number
of hidden memory cells in a memorized context layer or a memorized fusion layer
is set to 100, and the size of feature maps is 54 × 54. We train all the layers in
our deep network simultaneously using SGD with a momentum 0.9, the batch
size is set to one (due to limited GPU memory) and the weight decay is 0.0005.
The entire deep network is implemented on the publicly available platform Caﬀe
[37] and is trained on a single NVIDIA GeForce GTX TITAN X GPU with
12GB memory 1. It takes about 1 day to train our deep network. In the testing
stage, an RGB-D image takes 0.15s on average, which is signiﬁcantly faster than
pervious methods, i.e., the testing time in [9,24] is around 1.5s.

4.2 Results and Comparisons

According to [14,22], performance is evaluated by comparing class-wise Jaccard
Index, i.e., nii/ti, and average Jaccard Index, i.e., (1/ncl) (cid:80)
i nii/ti, where nij
is the number of pixels annotated as class i and predicted to be class j, ncl is
the number of diﬀerent classes, and ti = (cid:80)
j nij is the total number of pixels
annotated as class i [10].

SUNRGBD dataset [20]: The performance and comparison results on
SUNRGBD are shown in Table 1. Our proposed architecture can outperform
existing techniques: 2.2% higher than the performance reported in [22], 11.8%
higher than that in [24], 38% higher than that in [38] and 39.1% higher than that
in [20] in terms of 37-class average Jaccard Index. Improvements can be observed

1 LSTM-CF model is publicly available at: https://github.com/icemansina/LSTM-CF

10

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Table 1. Comparison of scene labeling results on SUNRGBD using class-wise and
average Jaccard Index. We compare our model with results reported in [20], [38], [24]
and previous state-of-the-art result in [22]. Boldface numbers mean best performance.

Wall
[20] 37.8
[20] 32.1
[20] 36.4
[38] 38.9
[38] 33.3
[38] 37.8
[24] 43.2
[22] 80.2
Ours 74.9

9.6
5.0
7.9
11.0
5.6
9.0

ﬂoor
45.0
42.6
45.8
47.2
43.8
48.3
78.6
90.9
82.3

chair sofa table door window bookshelf picture counter blinds
cabinet bed
9.4
16.9 12.8 18.5 6.1
21.8
17.4
0.8
21.5 4.1
12.5 3.4
6.4
2.9
12.8
19.9 11.6 19.3 6.0
23.3
15.4
9.6
17.2 13.4 20.4 6.8
21.5
18.8
0.9
12.9 3.8
22.3 3.9
6.3
3.0
13.1
20.8 12.1 20.9 6.8
23.6
17.2
23.1
42.5
26.2
33.2 40.6 34.3 33.2 43.6
31.2
64.8 76.0 58.6 62.6 47.7 66.4
58.9
43.1
67.7 55.5 57.8 45.6 52.8
62.1
47.3
paper towel shower box
ﬂoormat clothes ceiling books fridge tv
0.0
0.6
1.5
1.6
27.9
7.0
4.1
0.0
1.0
0.0
0.9
9.7
0.0
0.6
0.0
0.6
1.4
0.7
35.8
9.5
6.1
0.0
0.7
1.5
1.4
39.1
7.1
5.9
0.7
0.0
0.4
0.0
13.9
0.9
0.5
0.8
10.1 0.6
49.2
0.0
1.4
8.7
11.7
35.7
84.5
24.2 36.5 26.8 19.2 9.0
24.1
48.7 21.3 49.5 30.6 18.8 0.1
84.0
38.1
47.9 61.5 52.1 36.4 36.7 0
68.0

desk shelves curtain dresser pillow mirror
7.3
2.4
4.6
2.0
14.8
3.3
7.0
2.2
3.6
7.3
3.6
6.1
2.0
32.6
3.8
6.8
2.4
4.4
12.1 18.4
42.3
57.2
19.7 16.2
46.7
63.6
56.7
37.3 9.6
48.6
board person nightstand toilet sink
14.0
7.4
2.7
1.1
7.6
10.4
3.5
8.6
51.4
56.8
48.1

6.9
2.3
2.2
0.9
1.2
1.4
5.6
3.1
5.4
6.2
2.6
2.4
1.0
1.1
1.8
6.4
3.2
4.8
49.5
24.8
31.4
57.1 39.1
42.3
35.0
45.8 44.5
bathhub bag mean
8.3
0.9
0.6
5.3
0.4
0.0
9.0
0.6
1.1
9.3
1.1
0.9
6.0
0.5
0.0
10.1
1.3
1.1
18.6
47.0
36.3
24.1 45.9
45.1
23.6 48.1
65.6

4.3
2.0
4.4
6.9
3.6
7.8
59.1
67.0
63.4
lamp
0.9
0.7
0.9
1.3
0.8
1.2
44.2
48.8
58.0

8.9
2.3
12.0 15.2
12.3 14.8
2.6
14.9 16.8
64.1 53.0
73.0 66.2
68.8 67.9

1.2
0.3
1.4
1.3
0.6
1.6
27.0
24.4
28.4

0.0
0.3
0.7
0.0
0.3
0.8
35.7
17.9
72.6

1.1
2.6
1.7
1.5
1.5
1.8
25.0
42.9
36.4

2.2
1.7
5.2
2.6
2.2
6.2
31.8
33.8
39.4

1.0
15.3
1.7
1.2
10.1
1.0

1.9
0.1
0.2
2.2
0.3
0.2

1.2

[20] 0.0
[20] 0.0
[20] 0.0
[38] 0.0
[38] 0.0
[38] 0.0
[24] 5.6
[22] 0.1
Ours 0.0

in 15 class-wise Jaccard Indices. For a better understanding, we also show the
confusion matrix for this dataset in Fig. 3(a). It is worth mentioning that our
proposed architecture and most previous methods achieve zero accuracy on two
categories, i.e., ﬂoormat and shower, which mainly results from an imbalanced
data distribution instead of the capacity of our model.

NYUDv2 dataset: To further verify the eﬀectiveness of our architecture
and have more comparisons with existing state-of-the-art methods, we also con-
duct experiments on the NYUDv2 dataset. The results are presented in Table 2,
where the 13-class average Jaccard Index of our model is 20.3% higher than that
in [39]. Class frequencies and the confusion matrix are also shown in Table 2 and
Fig. 3(b) respectively. According to the reported results, our proposed architec-
ture gains 5.6% and 5.5% improvement in average Jaccard Index over [9] and
FCN-32s [10] respectively. Considering the listed class frequencies, our proposed
model signiﬁcantly outperforms existing methods on high frequency categories
and most low frequency categories, which primarily owes to the convolutional
features of the RGB image and the fused global contexts of the complete RGB-
D image. In terms of labeling categories with small and complex regions, e.g.,
pillows and chairs, our method also achieves a large improvement, which can be
veriﬁed in the following visual comparisons.

SUN3D dataset: Table 3 gives comparison results on the 1539 test images
in the SUN3D dataset. For fair comparison, the 12-class average Jaccard Index is
used in the comparison with the state-of-the-art results recently reported in [9].
Note that the 12-class accuracy of our network is calculated through the model
previously trained for 37 classes. Our model substantially outperforms the one
from [9] on large planar regions such as those labeled as ﬂoors and ceilings. This
also results from the incorporated convolutional features and the fused global
contexts. These comparison results further conﬁrm the power and generalization
capability of our LSTM-based model.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

11

(a) SUNRGBD

(b) NYUDv2

Fig. 3. Confusion matrix for SUNRGBD and NYUDv2. Class-wise Jaccard Index is
shown on the diagonal. Best viewed in color.

Table 2. Comparison of scene labeling on NYUDv2. We compare our proposed model
with existing state-of-the-art methods, i.e., [34], [24], [25], [26] and [9]. Class-wise Jac-
card Index and average Jaccard Index of 37 classes are presented. ‘Freq’ stands for
class frequency. Boldface numbers mean best performance.

Wall
Freq 21.4
[34] 60.7
[24] 60.0
[25] 67.4
[26] 61.4
[9]
65.7
Ours 79.6

2.1

2.7

ﬂoor
9.1
77.8
74.4
80.5
66.4
62.5
83.5

chair sofa table door window bookshelf picture counter blinds
cabinet bed
1.9
2.1
3.3
2.2
3.8
6.2
22.7
32.4 25.3 21.0 5.9
29.7
40.3
33.0
17.3
32.5 28.2 16.6 12.9 27.7
42.3
37.1
20.5
40.4 44.8 30.0 12.1 34.1
56.4
41.4
17.6
34.4 33.8 22.6 8.3
27.6
43.9
38.2
36.3
32.1
40.1
44.5 50.8 43.5 51.6 49.2
59.5
77.0 58.3 64.9 42.6 47.0 43.6
69.3
paper towel shower box
ﬂoormat clothes ceiling books fridge tv
0.3
0.4
0.4
0.4
0.5
0.6
0.1
5.7
3.6
12.7 0.1
1.4
3.3
1.9
18.6 11.7 12.6 5.4
1.1
14.5 14.4 14.1 19.8 6.0
0.8
6.1
2.6
2.9
36.3
17.6

desk shelves curtain dresser pillow mirror
1.0
1.1
1.7
2.1
4.7
3.3
40.6
35.7
10.1 6.1
26.5
32.4
10.1 1.6
44.7
38.7
5.1
2.7
33.6
27.7
48.0 45.2
55.8
41.4
74.5
33.6 13.1
74.6
board person nightstand toilet sink
0.3
0.3
0.0
0.2
12.9
28.2
60.6
93.9

1.0
0.8
0.9
4.4
18.9
13.3
17.9
19.7
7.0
14.6
31.3
21.6
12.5
10.7
16.8
50.5 46.1
55.3
56.5
48.0 47.7
bathhub bag mean
0.2
0.3
17.5
0.0
0.0
20.2
1.2
7.8
30.0
0.2
29.4
0.2
16.2
29.2
37.3 43.9
38.5
49.4
7.5
72.6

0.6
5.5
14.8
5.8
3.6
39.1 53.6 50.1 35.4 39.9 41.8
49.7 0.0

1.1
27.4
27.6
26.3
18.9
53.1
53.2
lamp
0.3
15.9
14.2
31.2
5.4
26.3
67.6

0.3
26.7 25.1
35.2 28.9
52.5 47.9
32
20.9
31.8 22.5
81.8 58.4

1.4
73.2
53.9
61.8
46.1
50.6
70.2

0.7
6.5
9.5
8.0
2.7
35.4
22.7

0.3
6.6
13.6
1.5
5
35.6
77.0

0.3
6.3
9.2
15.7
6.9
32.5
0

1.4
33.1
38.6
50.7
30.2
39.2
68.2

0.0 52.1 60.6 0

6.2

3.2

Freq 0.7
[34] 7.1
[24] 20.1
[25] 28.2
[26] 13.8
[9]
54.1
Ours 0.0

4.3 Ablation Study

To discover the vital elements in our proposed model, we conduct an ablation
study to remove or replace individual components in our deep network when
training and testing on the SUNRGBD dataset. Speciﬁcally, we have tested the
performance of our model without the RGB path, the depth path, multi-scale
RGB feature concatenation, the memorized context layers or the memorized
fusion layer. In addition, we also conduct an experiment with a model that does
not combine the ﬁnal convolutional features of photometric channels (i.e., Conv7
in Fig. 2) with the global contexts of the complete RGB-D image to ﬁgure out the
importance of diﬀerent components. The results are presented in Table 4. From
the given results, we ﬁnd that the ﬁnal convolutional features of the photometric
channels is the most vital information, i.e., the cross-layer combination is the
most eﬀective component as the performance drops to 15.2% without it, which
is consistent with previously mentioned properties of depth and photometric
data. In addition, multi-scale RGB feature concatenation before the memorized
context layer also plays a vital role as it directly aﬀects the vertical contexts in

12

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Table 3. Comparison of class-wise Jaccard Index and 12-class average Jaccard Index
on SUN3D.

wall ﬂoor bed chair table counter curtain ceiling tv toilet bathtub bag Mean
73 35

[9]
71 35
Ours 73 86 32 65

30
57

52
22

68
76

27
69

56 23
75 62

49
62

29 45.7
23 58.5

Table 4. Ablation Study

Model

Without RGB path, using Deeplab+Renet for depth path
Without depth path
Without multi-scale RGB feature concatenation
Without cross-layer integration of RGB convolutional features
Without memorized fusion layer
Without memorized context layers
Without any memorized (context or fusion) layers

Mean Accuracy
15.8%
43.7%
42.1%
15.2%
44.7%
45.7%
45.0%

the photometric channels and the performance drops to 42.1% without it. It is
obvious that performance would be inevitably harmed without the depth path.
Among the memorized layers, the memorized fusion layer is more important
than the memorized context layers in our pipeline as it accomplishes the fusion
of contexts in photometric and depth channels.

4.4 Visual Comparisons

SUNRGBD Dataset: We present visual results of RGB-D scene labeling in
Fig. 4. Here, we leverage super-pixel based averaging to smooth visual labeling
results as being done in [9]. The algorithm in [40] is used for performing super-
pixel segmentation. As can be observed in Fig. 4, our proposed deep network
produces accurate and semantically meaningful labeling results, especially for
large regions and high frequency labels. For instance, our model takes advantage
of global contexts when labeling ‘bed’ in Fig. 4(a), ‘wall’ in Fig. 4(e) and ‘mirror’
in Fig. 4(i). Our proposed model can precisely label almost all ‘chairs’ (a high
frequency label) by exploiting integrated photometric and depth information,
regardless of occlusions.

NYUDv2 Dataset: We also perform visual comparisons on the NYUDv2
benchmark, which has complicated indoor scenes and well-labeled ground truth.
We compare our scene labeling results with those publicly released labeling re-
sults from [25]. It is obvious that our results are clearly better than those from
[25] both visually and numerically (under the metric of average Jaccard Index)
even though scene labeling in [25] is based on sophisticated segmentation.

Label Reﬁnement: Surprisingly, our model can intelligently reﬁne certain
region annotations, which might have inaccuracies due to under-segmentation,
especially in the newly captured 3943 RGB-D images, as shown in Fig. 6. Specif-
ically, the cabinets in Fig. 6(a) were annotated as ‘background’, the pillows in

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

13

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

(q) legend of semantic labels

Fig. 4. Examples of semantic labeling results on the SUNRGBD dataset. The top row
shows the input RGB images, the bottom row shows scene labeling obtained with our
model and the middle row has the ground truth. Semantic labels and their correspond-
ing colors are shown at the bottom.

Fig. 6(g) as ‘bed’, and the tables in Fig. 6(n) as ‘wall’ by mistake. Our model
can eﬀectively deal with these diﬃcult regions. For example, the annotation of
the picture in Fig. 6(e) and that of the pillows in Fig. 6(g) have been corrected.
Thus, our model can be exploited to reﬁne certain annotations in the SUNRGBD
dataset, which is another contribution of our model.

5 Conclusions

In this paper, we have developed a novel Long Short-Term Memorized Con-
text Fusion (LSTM-CF) model that captures image contexts from a global per-
spective and deeply fuses contextual representations from multiple sources (i.e.,
depth and photometric data) for semantic scene labeling. In future, we will ex-
plore how to extend the memorized layers with an attention mechanism, and
reﬁne the performance of our model in boundary labeling.

14

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Fig. 5. Visual comparison of scene labeling results on the NYUDv2 dataset. The ﬁrst
and second rows show the input RGB images and their corresponding groundtruth
labeling. The third row shows the results from [25] and the last row shows the results
from our model.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

Fig. 6. Annotation reﬁnement on the SUNRGBD dataset. The top row shows the input
RGB images, the middle row shows the original annotations, and the bottom row shows
scene labeling results from our model.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

15

References

1. Wu, C., Lenz, I., Saxena, A.: Hierarchical semantic labeling for task-relevant rgb-d

perception. In: Robotics: Science and systems (RSS). (2014)

2. Hinterstoisser, S., Lepetit, V., Ilic, S., Holzer, S., Bradski, G., Konolige, K., Navab,
N.: Model based training, detection and pose estimation of texture-less 3d objects
In: Computer Vision–ACCV 2012. Springer (2012)
in heavily cluttered scenes.
548–562

3. Holz, D., Holzer, S., Rusu, R.B., Behnke, S.: Real-time plane segmentation using
rgb-d cameras. In: RoboCup 2011: robot soccer world cup XV. Springer (2011)
306–317

4. Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., Manning, C.D.: Generating se-
mantically precise scene graphs from textual descriptions for improved image re-
trieval. In: Proceedings of the Fourth Workshop on Vision and Language. (2015)
70–80

5. Yan, Z., Zhang, H., Wang, B., Paris, S., Yu, Y.: Automatic photo adjustment using

deep neural networks. ACM Transactions on Graphics 35(2) (2016)

6. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions
on 35(8) (2013) 1915–1929

7. Gould, S., Fulton, R., Koller, D.: Decomposing a scene into geometric and se-
mantically consistent regions. In: Computer Vision, 2009 IEEE 12th International
Conference on, IEEE (2009) 1–8

8. Tighe, J., Lazebnik, S.: Superparsing: scalable nonparametric image parsing with

superpixels. In: Computer Vision–ECCV 2010. Springer (2010) 352–365

9. Khan, S.H., Bennamoun, M., Sohel, F., Togneri, R., Naseem, I.: Integrating geo-
metrical context for semantic labeling of indoor scenes using rgbd images. Inter-
national Journal of Computer Vision (2015) 1–20

10. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3431–3440

11. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: Proceed-
ings of the IEEE International Conference on Computer Vision. (2015) 1529–1537
12. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. arXiv
preprint arXiv:1412.7062 (2014)

13. Gupta, S., Girshick, R., Arbel´aez, P., Malik, J.: Learning rich features from rgb-d
images for object detection and segmentation. In: Computer Vision–ECCV 2014.
Springer (2014) 345–360

14. Song, S., Xiao, J.: Deep sliding shapes for amodal 3d object detection in rgb-d

images. arXiv preprint arXiv:1511.02300 (2015)

15. Liu, Z., Li, X., Luo, P., Loy, C.C., Tang, X.: Semantic image segmentation via
deep parsing network. In: Proceedings of the IEEE International Conference on
Computer Vision. (2015) 1377–1385

16. Liang, X., Shen, X., Xiang, D., Feng, J., Lin, L., Yan, S.: Semantic object parsing
with local-global long short-term memory. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. (2016)

17. Byeon, W., Breuel, T.M., Raue, F., Liwicki, M.: Scene labeling with lstm recurrent
neural networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3547–3555

16

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

18. Pinheiro, P., Collobert, R.: Recurrent convolutional neural networks for scene
labeling. In: Proceedings of the 31st International Conference on Machine Learning
(ICML-14). (2014) 82–90

19. Visin, F., Kastner, K., Cho, K., Matteucci, M., Courville, A., Bengio, Y.: Renet:
A recurrent neural network based alternative to convolutional networks. arXiv
preprint arXiv:1505.00393 (2015)

20. Song, S., Lichtenberg, S.P., Xiao, J.: Sun rgb-d: A rgb-d scene understanding
benchmark suite. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. (2015) 567–576

21. Kumar, M.P., Koller, D.: Eﬃciently selecting regions for scene understanding. In:
Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
IEEE (2010) 3217–3224

22. Kendall, A., Badrinarayanan, V., Cipolla, R.: Bayesian segnet: Model uncertainty
in deep convolutional encoder-decoder architectures for scene understanding. arXiv
preprint arXiv:1511.02680 (2015)

23. Lempitsky, V., Vedaldi, A., Zisserman, A.: Pylon model for semantic segmentation.

In: Advances in neural information processing systems. (2011) 1485–1493
24. Ren, X., Bo, L., Fox, D.: Rgb-(d) scene labeling: Features and algorithms.

In:
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,
IEEE (2012) 2759–2766

25. Gupta, S., Arbel´aez, P., Girshick, R., Malik, J.: Indoor scene understanding with
rgb-d images: Bottom-up segmentation, object detection and semantic segmenta-
tion. International Journal of Computer Vision 112(2) (2015) 133–149

26. Wang, A., Lu, J., Cai, J., Wang, G., Cham, T.J.: Unsupervised joint feature learn-
ing and encoding for rgb-d scene labeling. Image Processing, IEEE Transactions
on 24(11) (2015) 4459–4473

27. Husain, F., Schulz, H., Dellen, B., Torras, C., Behnke, S.: Combining semantic and
geometric features for object class segmentation of indoor scenes. IEEE Robotics
and Automation Letters 2(1) (2017) 49–55

28. Schmidhuber, J.: A local learning algorithm for dynamic feedforward and recurrent

networks. Connection Science 1(4) (1989) 403–412

29. Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with gra-
dient descent is diﬃcult. Neural Networks, IEEE Transactions on 5(2) (1994)
157–166

30. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

9(8) (1997) 1735–1780

31. Graves, A., Schmidhuber, J.: Oﬄine handwriting recognition with multidimen-
sional recurrent neural networks. In: Advances in neural information processing
systems. (2009) 545–552

32. Stollenga, M.F., Byeon, W., Liwicki, M., Schmidhuber, J.:

Parallel multi-
dimensional lstm, with application to fast biomedical volumetric image segmen-
tation. In: Advances in Neural Information Processing Systems. (2015) 2980–2988
In: IEEE

33. Li, G., Yu, Y.: Deep contrast learning for salient object detection.

Conference on Computer Vision and Pattern Recognition (CVPR). (2016)

34. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and support
inference from rgbd images. In: Computer Vision–ECCV 2012. Springer (2012)
746–760

35. Janoch, A., Karayev, S., Jia, Y., Barron, J.T., Fritz, M., Saenko, K., Darrell, T.: A
category-level 3d object dataset: Putting the kinect to work. In: Consumer Depth
Cameras for Computer Vision. Springer (2013) 141–165

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

17

36. Xiao, J., Owens, A., Torralba, A.: Sun3d: A database of big spaces reconstructed
using sfm and object labels. In: Proceedings of the IEEE International Conference
on Computer Vision. (2013) 1625–1632

37. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

38. Liu, C., Yuen, J., Torralba, A.: Sift ﬂow: Dense correspondence across scenes and
its applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on
33(5) (2011) 978–994

39. Couprie, C., Farabet, C., Najman, L., LeCun, Y.: Toward real-time indoor semantic
segmentation using depth information. Journal of Machine Learning Research
(2014)

40. Felzenszwalb, P.F., Huttenlocher, D.P.: Eﬃcient graph-based image segmentation.

International Journal of Computer Vision 59(2) (2004) 167–181

6
1
0
2
 
l
u
J
 
6
2
 
 
]

V
C
.
s
c
[
 
 
3
v
0
0
0
5
0
.
4
0
6
1
:
v
i
X
r
a

LSTM-CF: Unifying Context Modeling and
Fusion with LSTMs for RGB-D Scene Labeling

Zhen Li1, Yukang Gan2, Xiaodan Liang2, Yizhou Yu1,
Hui Cheng2, and Liang Lin2(cid:63)

1 Department of Computer Science, The University of Hong Kong, Hong Kong,
lizhen36@hku.hk,yizhouy@acm.org
2 School of Data and Computer Science, Sun Yat-sen University, Guangzhou,
ganyk@mail2.sysu.edu.cn,xdliang328@gmail.com
chengh9@mail.sysu.edu.cn,linliang@ieee.org

Abstract. Semantic labeling of RGB-D scenes is crucial to many in-
telligent applications including perceptual robotics. It generates pixel-
wise and ﬁne-grained label maps from simultaneously sensed photomet-
ric (RGB) and depth channels. This paper addresses this problem by i)
developing a novel Long Short-Term Memorized Context Fusion (LSTM-
CF) Model that captures and fuses contextual information from multiple
channels of photometric and depth data, and ii) incorporating this model
into deep convolutional neural networks (CNNs) for end-to-end training.
Speciﬁcally, contexts in photometric and depth channels are, respectively,
captured by stacking several convolutional layers and a long short-term
memory layer; the memory layer encodes both short-range and long-
range spatial dependencies in an image along the vertical direction. An-
other long short-term memorized fusion layer is set up to integrate the
contexts along the vertical direction from diﬀerent channels, and per-
form bi-directional propagation of the fused vertical contexts along the
horizontal direction to obtain true 2D global contexts. At last, the fused
contextual representation is concatenated with the convolutional features
extracted from the photometric channels in order to improve the accu-
racy of ﬁne-scale semantic labeling. Our proposed model has set a new
state of the art, i.e., 48.1% and 49.4% average class accuracy over 37
categories (2.2% and 5.4% improvement) on the large-scale SUNRGBD
dataset and the NYUDv2 dataset, respectively.

Keywords: RGB-D scene labeling, image context modeling, long short-
term memory, depth and photometric data fusion

1 Introduction

Scene labeling, also known as semantic scene segmentation, is one of the most
fundamental problems in computer vision. It refers to associating every pixel in

(cid:63) The corresponding author is Liang Lin. This work was support by Projects on Fac-
ulty/ Student Exchange and Collaboration Scheme between the Higher Education in
Hong Kong and the Mainland, Guangzhou Science and Technology Program under
grant 1563000439, and Fundamental Research Funds for the Central Universities.

2

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Fig. 1. An illustration of global context modeling and fusion for RGB-D images. Our
LSTM-CF model ﬁrst captures vertical contexts through a memory network layer en-
coding short- and long-range spatial dependencies along the vertical direction. After a
concatenation operation (denoted by “C”) over photometric and depth channels, our
model utilizes another memory network layer to fuse vertical contexts from all chan-
nels in a data-driven way and performs bi-directional propagation along the horizontal
direction to obtain true 2D global contexts. Best viewed in color.

an image with a semantic label, such as table, road and wall, as illustrated in
Fig. 1. High-quality scene labeling can be beneﬁcial to many intelligent tasks,
including robot task planning [1], pose estimation [2], plane segmentation [3],
context-based image retrieval [4], and automatic photo adjustment [5].

Previous work on scene labeling can be divided into two categories accord-
ing to their target scenes: indoor and outdoor scenes. Compared with outdoor
scene labeling [6,7,8], indoor scene labeling is more challenging due to a larger
set of semantic labels, more severe object occlusions, and more diverse object
appearances [9]. For example, indoor object classes, such as beds covered with
diﬀerent sheets and various appearances of curtains, are much harder to char-
acterize than outdoor classes, e.g., roads, buildings, and sky, through photo-
metric channels only. Recently, utilizing depth sensors to augment RGB data
have eﬀectively improved the performance of indoor scene labeling because the
depth channel complements photometric channels with structural information.
Nonetheless, two key issues remain open in the literature of RGB-D scene label-
ing.

(I) How to eﬀectively represent and fuse the coexisting depth and
photometric (RGB) data For data representation, a batch of sophisticated
hand-crafted features have been developed in previous methods. Such hand-
crafted features are somewhat ad hoc and less discriminative than those RGB-D
representations learned using convolutional neural networks (CNNs) [10,11,12,13,14].
However, in these CNN-related works, the fusion of depth and photometric data
has often been oversimpliﬁed. For instance, in [13,14], two independent CNNs are
leveraged to extract features from depth and photometric data separately, and
such features are simply concatenated before used for ﬁnal classiﬁcation. Over-
looking the strong correlation between depth and photometric channels could
inevitably harm semantic labeling.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

3

(II) How to capture global scene contexts during feature learning
Current CNN-based scene labeling approaches can only capture local contextual
information for every pixel due to their restricted receptive ﬁelds, resulting in
suboptimal labeling results. In particular, long-range dependencies sometimes
play a key role in distinguishing among diﬀerent objects having similar appear-
ances, e.g., labeling “ceiling” and “ﬂoor” in Fig. 1, according to the global scene
layout. To overcome this issue, graphical models, such as a conditional random
ﬁeld [9,11] or a mean-ﬁeld approximation [15], have been applied to improve
prediction results in a post-processing step. These methods, however, separate
context modeling from convolutional feature learning, which may give rise to
suboptimal results on complex scenes due to less discriminative feature repre-
sentation [16]. An alternative class of methods adopts cascaded recurrent neural
networks (RNNs) with gate structures, e.g., long short-term memory (LSTM)
networks, to explicitly strengthen context modeling [16,17,18]. In these methods,
the long- and short-range dependencies can be well memorized by sequentially
running the network over individual pixels.

To address the aforementioned challenges, this paper proposes a novel Long
Short-Term Memorized Context Fusion (LSTM-CF) model and demonstrates
its superiority in RGB-D scene labeling. Fig. 1 illustrates the brief idea of using
memory networks for context modeling and fusion of diﬀerent channels. Our
LSTM-CF model captures 2D dependencies within an image by exploiting the
cascaded bi-directional vertical and horizontal RNN models as introduced in
[19].

Our method constructs HHA images [13] for the depth channel through ge-
ometric encoding, and uses several convolutional layers for extracting features.
Inspired by [19], these convolutional layers are followed by a memorized context
layer to model both short-range and long-range spatial dependencies along the
vertical direction. For photometric channels, we generate convolutional features
using the Deeplab network [12], which is also followed by a memorized context
layer for context modeling along the vertical direction. Afterwards, a memorized
fusion layer is set up to integrate the contexts along the vertical direction from
both photometric and depth channels, and perform bi-directional propagation of
the fused vertical contexts along the horizontal direction to obtain true 2D global
contexts. Considering the features diﬀerences, e.g., signal frequency and other
characteristics (color/geometry) [20], our fusion layer facilitates deep integration
of contextual information from multiple channels in a data-driven manner rather
than simply concatenating diﬀerent feature vectors. Since photometric channels
usually contain ﬁner details in comparison to the depth channel [20], we further
enhance the network with cross-layer connections that append convolutional fea-
tures of the photometric channels to the fused global contexts before the ﬁnal
fully convolutional layer, which predicts pixel-wise semantic labels. Various lay-
ers in our LSTM-CF model are tightly integrated, and the entire network is
amenable to end-to-end training and testing.

In summary, this paper has the following contributions to the literature of

RGB-D scene labeling.

4

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

– It proposes a novel Long Short-Term Memorized Context Fusion (LSTM-
CF) Model, which is capable of capturing image contexts from a global
perspective and deeply fusing contextual information from multiple sources
(i.e., depth and photometric channels).

– It proposes to jointly optimize LSTM layers and convolutional layers for
achieving better performance in semantic scene labeling. Context modeling
and fusion are incorporated into the deep network architecture to enhance
the discriminative power of feature representation. This architecture can also
be extended to other similar tasks such as object/part parsing.

– It is demonstrated on the large-scale SUNRGBD benchmark (including 10355
images) and canonical NYUDv2 benchmark that our method outperforms
existing state-of-the-art methods. In addition, it is found that our scene la-
beling results can be leveraged to improve the groundtruth annotations of
newly captured 3943 RGB-D images in SUNRGBD dataset.

2 Related work

Scene Labeling: Scene labeling has caught researchers’ attention frequently
[6,11,12,16,17,18,21] in recent years. Instead of extracting features from over-
segmented images, recent methods usually utilize powerful CNN layers as the
feature extractor, taking advantage of fully convolutional networks (FCNs) [10]
and its variants [22] to obtain pixel-wise dense features. Another main challenge
for scene labeling is the fusion of local and global contexts, i.e., taking advantage
of global contexts to reﬁne local decisions. For instance, [6] exploits families of
segmentations or trees to generate segment candidates. [23] utilizes an inference
method based on graph cut to achieve image labeling. A pixel-wise conditional
random forest is used in [11,12] to directly optimize a deep CNN-driven cost
function. Most of the above models improve accuracy through carefully designed
processing on the predicted conﬁdence map instead of proposing more powerful
discriminative features, which usually results in suboptimal prediction results
[16]. The topological structure of recurrent neural networks (RNNs) is used to
model short- and long-range dependencies in [16,18]. In [17], a multi-directional
RNN is leveraged to extract local and global contexts without using a CNN,
which is well suited for low-resolution and relatively simple scene labeling prob-
lems. In contrast, our model can jointly optimize LSTM layers and convolutional
layers to explicitly improve discriminative feature learning for local and global
context modeling and fusion.

Scene Labeling in RGB-D images: With more and more convenient ac-
cess to aﬀordable depth sensors, scene labeling in RGB-D images [9,13,14,24,25,26]
enables a rapid progress of scene understanding. Various sophisticated hand-
crafted features are utilized in previous state-of-the-art methods. Speciﬁcally,
kernel descriptions based on traditional multi-channel features, such as color,
depth gradient, and surface normal, are used as photometric and depth features
[24]. A rich feature set containing various traditional features, e.g., SIFT, HOG,
LBP and plane orientation, are used as local appearance features and plane ap-
pearance features in [9]. HOG features of RGB images and HOG+HH (histogram

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

5

of height) features of depth images are extracted as representations in [25] for
training successive classiﬁers. In [27], proposed distance-from-wall features are
exploited to improve scene labeling performance. In addition, an unsupervised
joint feature learning and encoding model is proposed for scene labeling in [26].
However, due to the limited number of RGB-D images, deep learning for scene
labeling in RGB-D images was not as appealing as that for RGB images. The
release of the SUNRGBD dataset, which includes most of the previously popular
datasets, may have changed this situation [13,14].

Another main challenge imposed by scene labeling in RGB-D images is the
fusion of contextual representations of diﬀerent sources (i.e., depth and photo-
metric data). For instance, in [13,14], two independent CNNs are leveraged to
extract features from the depth and photometric data separately, which are then
simply concatenated for class prediction. Ignoring the strong correlation between
depth and photometric channels usually negatively aﬀects semantic labeling. In
contrast, instead of simply concatenating features from multiple sources, the
memorized fusion layer in our model facilitates the integration of contextual
information from diﬀerent sources in a data-driven manner,

RNN for Image Processing: Recurrent neural networks (RNNs) represent
a type of neural networks with loop connections [28]. They are designed to cap-
ture dependencies across a distance larger than the extent of local neighborhoods.
In previous work, RNN models have not been widely used partially due to the
diﬃculty to train such models, especially for sequential data with long-range de-
pendencies [29]. Fortunately, RNNs with gate and memory structures, e.g., long
short-term memory (LSTM) [30], can artiﬁcially learn to remember and forget
information by using speciﬁc gates to control the information ﬂow. Although
RNNs have an outstanding capability to capture short-range and long-range de-
pendencies, there exist problems for applying RNNs to image processing due to
the fact that, unlike data in natural language processing (NLP) tasks, images
do not have a natural sequential structure. Thus, diﬀerent strategies have been
proposed to overcome this problem. Speciﬁcally, in [19], cascaded bi-directional
vertical and horizonal RNN layers are designed for modeling 2D dependencies in
images. A multi-dimensional RNN with LSTM unit has been applied to hand-
writing [31]. A parallel multi-dimensional LSTM for image segmentation has
been proposed in [32]. In this paper, we propose an LSTM-CF model consisting
of memorized context layers and a memorized fusion layer to capture image con-
texts from a global perspective and fuse contextual representations from diﬀerent
sources.

3 LSTM-CF Model

As illustrated in Fig. 2, our end-to-end LSTM-CF model for RGB-D scene label-
ing consists of four components, layers for vertical depth context extraction, lay-
ers for vertical photometric context extraction, a memorized fusion layer for in-
corporating vertical photometric and depth contexts as true 2D global contexts,
and a ﬁnal layer for pixel-wise scene labeling given concatenated convolutional

6

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Fig. 2. Our LSTM-CF model for RGB-D scene labeling. The input consists of both
photometric and depth channels. Vertical contexts in photometric and depth channels
are computed in parallel using cascaded convolutional layers and a memorized context
layer. Vertical photometric (color) and depth contexts are fused and bi-directionally
propagated along the horizontal direction via another memorized fusion layer to ob-
tain true 2D global contexts. The fused global contexts and the ﬁnal convolutional
features of photometric channels are then concatenated together and fed into the ﬁ-
nal convolutional layer for pixel-wise scene labeling. “C” stands for the concatenation
operation.

features and global contexts. The inputs to our model include both photometric
and depth images. The path for extracting global contexts from the photometric
image consists of multiple convolutional layers and an extra memorized context
layer. On the other hand, the depth image is ﬁrst encoded as an HHA image,
which is fed into three convolutional layers [14] and an extra memorized context
layer for global depth context extraction. The other component, a memorized
fusion layer, is responsible for fusing previously extracted global RGB and depth
contexts in a data-driven manner. On top of the memorized fusion layer, the ﬁ-
nal convolutional feature of photometric channels and the fused global context
are concatenated together and fed into the ﬁnal fully convolutional layer, which
performs pixel-wise scene labeling with the softmax activation function.

3.1 Memorized Vertical Depth Context

Given a depth image, we use the HHA representation proposed in [13] to encode
geometric properties of the depth image in three channels, i.e., disparity, surface
normal and height. Diﬀerent from [13], the encoded HHA image in our pipeline is
fed into three randomly initialized convolutional layers (to obtain a feature map
with the same resolution as that in the RGB path) instead of layers taken from
the model pre-trained on the ILSVRC2012 dataset. This is because the color
distribution of HHA images is diﬀerent from that of natural images (see Fig.
2) according to [20]. One top of the third convolutional layer (i.e., HHAConv3),

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

7

there is an extra memorized context layer from Renet [19], which performs bi-
directional propagation of local contextual features from the convolutional layers
along the vertical direction. For better understanding, we denote the feature map
HHAConv3 as F = {fi,j}, where F ∈ Rw×h×c with w, h and c representing the
width, height and the number of channels. Since we perform pixel-wise scene
labeling, every patch in this Renet layer only contains a single pixel. Thus,
vertical memorized context layer (here we choose LSTM as recurrent unit) can
be formulated as

i,j = LSTM(hf
hf
i,j = LSTM(hb
hb

i,j−1, fi,j),
i,j+1, fi,j),

for j = 1, . . . , h

for j = h, . . . , 1,

(1)

(2)

where hf and hb stand for the hidden states of the forward and backward LSTM.
In the forward LSTM, the unit at pixel (i, j) takes hf
i,j−1 ∈ Rd and fi,j ∈ Rc as
input, and its output is calculated as follows according to [30]. The operations
in the backward LSTM can be deﬁned similarly.

i,j−1 + bi)
i,j−1 + bf )
i,j−1 + bo)

gatei = δ(Wif fi,j + Wihhf
gatef = δ(Wf f fi,j + Wf hhf
gateo = δ(Wof fi,j + Wohhf
gatec = tanh(Wcf fi,j + Wchhf
ci,j = gatef (cid:12) ci,j−1 + gatei (cid:12) gatec
hf
i,j = tanh(gateo (cid:12) ci,j)

i,j−1 + bc)

Finally, pixel-wise vertical depth contexts are collectively represented as a map,
Cdepth ∈ Rw×h×2d, where 2d is the total number of output channels from the
vertical memorized context layer.

(3)

3.2 Memorized Vertical Photometric Context

In the component for extracting global RGB contexts, we adapt the Deeplab ar-
chitecture proposed in [12]. Diﬀerent from existing Deeplab variants, we concate-
nate features at three diﬀerent scales to enrich the feature representation. This
is inspired by the network architecture in [33]. Speciﬁcally, since there exists hole
operations in Deeplab convolutional layers, feature maps from Conv2 2, Conv3 3
and Conv5 3 have suﬃcient initial resolutions. They can be further elevated to
the same resolution using interpolation. Corresponding pixel-wise features from
these three elevated feature maps are then concatenated together before being fed
into the subsequent memorized fusion layer, which again performs bi-directional
propagation to produce vertical photometric contexts. Here pixel-wise vertical
photometric contexts can also be represented as a map, CRGB ∈ Rw×h×2d, which
has the same dimensionalities as the map for vertical depth contexts.

8

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

3.3 Memorized Context Fusion

So far vertical depth and photometric contexts are computed independently
in parallel. Instead of simply concatenating these two types of contexts, the
memorized fusion layer, which performs horizontal bi-directional propagation
from Renet, is exploited for adaptively fusing vertical depth and RGB contexts
in a data-driven manner, and the output from this layer can be regarded as
the fused representation of both types of contexts. Such fusion can generate
more discriminative features through end-to-end training. The input and output
dimensions of the fusion layer are set to Rw×h×4d and Rw×h×2d, respectively.

Note that there are two separate memorized context layers in the photo-
metric and depth paths of our architecture. Since the memorized context layer
and the memorized fusion layer are two symmetric components of the original
Renet [19], a more natural and symmetric alternative would have a single memo-
rized context layer preceding the memorized fusion layer in our model (i.e., whole
structure of Renet including cascaded bi-directional vertical and horizonal mem-
orized layer) and let the memorized fusion layer incorporate the features from the
RGB and depth paths. Nonetheless, in our experiments, this alternative network
architecture gave rise to slightly worse performance.

3.4 Scene Labeling

Between photometric and depth images, photometric images contain more de-
tails and semantic information that can help scene labeling in comparison with
sparse and discontinuous depth images [14]. Nonetheless, depth images can pro-
vide auxiliary geometric information for improving scene labeling performance.
Thus, we design a cross-layer combination that integrates pixel-wise convolu-
tional features (i.e., Conv7 in Fig. 2) from the photometric image with fused
global contexts from the memorized fusion layer as the ﬁnal pixel-wise features,
which are fed into the last fully convolutional layer with softmax activation to
perform scene labeling at every pixel location.

4 Experimental Results

4.1 Experimental Setting

Datasets: We evaluate our proposed model for RGB-D scene labeling on three
public benchmarks, SUNRGBD, NYUDv2 and SUN3D. SUNRGBD [20] is the
largest dataset currently available, consisting of 10355 RGB-D images captured
from four diﬀerent depth sensors. It includes most previous datasets, such as
NYUDv2 depth [34], Berkeley B3DO [35], and SUN3D [36], as well as 3943
newly captured RGB-D images [20]. 5285 of these images are predeﬁned for
training and the remaining 5050 images constitute the testing set [14].
Implementation Details: In our experiments, a slightly modiﬁed Deeplab
pipeline [12] is adopted as the basic network in our RGB path for extracting
convolutional feature maps because of its high performance. It is initialized with

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

9

the publicly available VGG-16 model pre-trained on ImageNet. For the pur-
pose of pixel-wise scene labeling, this architecture transforms the last two fully
connected layers in the standard VGG-16 to convolutional layers with 1 × 1 ker-
nels. For the parallel depth path, three randomly initialized CNN layers with
max pooling are leveraged for depth feature extraction. In each path, on top
of the aforementioned convolutional network, a vertically bi-directional LSTM
layer implements the memorized context layer, and models both short-range
and long-range spatial dependencies. Then, another horizontally bi-directional
LSTM layer implements the memorized fusion layer, and is used to adaptively
integrate the global contexts from the two paths. In addition, there is a cross-
layer combination of ﬁnal convolutional features (i.e., Conv7) and the integrated
global representation from the horizontal LSTM layer.

Since the SUNRGBD dataset was collected by four diﬀerent depth sensors,
each input image is cropped to 426 × 426 (the smallest resolution of these four
sensors) [14]. During ﬁne-tuning, the learning rate for newly added layers, in-
cluding HHAConv1, HHAConv2, HHAConv3, the memorized context layers, the
memorized fusion layer and Conv8, is initialized to 10−2, and the learning rate
for those pre-trained layers of VGG-16 is initialized to 10−4. All weights in the
newly added convolutional layers are initialized using a Gaussian distribution
with a standard deviation equal to 0.01, and the weights in the LSTM layers are
randomly initialized with a uniform distribution over [−0.01, 0.01]. The number
of hidden memory cells in a memorized context layer or a memorized fusion layer
is set to 100, and the size of feature maps is 54 × 54. We train all the layers in
our deep network simultaneously using SGD with a momentum 0.9, the batch
size is set to one (due to limited GPU memory) and the weight decay is 0.0005.
The entire deep network is implemented on the publicly available platform Caﬀe
[37] and is trained on a single NVIDIA GeForce GTX TITAN X GPU with
12GB memory 1. It takes about 1 day to train our deep network. In the testing
stage, an RGB-D image takes 0.15s on average, which is signiﬁcantly faster than
pervious methods, i.e., the testing time in [9,24] is around 1.5s.

4.2 Results and Comparisons

According to [14,22], performance is evaluated by comparing class-wise Jaccard
Index, i.e., nii/ti, and average Jaccard Index, i.e., (1/ncl) (cid:80)
i nii/ti, where nij
is the number of pixels annotated as class i and predicted to be class j, ncl is
the number of diﬀerent classes, and ti = (cid:80)
j nij is the total number of pixels
annotated as class i [10].

SUNRGBD dataset [20]: The performance and comparison results on
SUNRGBD are shown in Table 1. Our proposed architecture can outperform
existing techniques: 2.2% higher than the performance reported in [22], 11.8%
higher than that in [24], 38% higher than that in [38] and 39.1% higher than that
in [20] in terms of 37-class average Jaccard Index. Improvements can be observed

1 LSTM-CF model is publicly available at: https://github.com/icemansina/LSTM-CF

10

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Table 1. Comparison of scene labeling results on SUNRGBD using class-wise and
average Jaccard Index. We compare our model with results reported in [20], [38], [24]
and previous state-of-the-art result in [22]. Boldface numbers mean best performance.

Wall
[20] 37.8
[20] 32.1
[20] 36.4
[38] 38.9
[38] 33.3
[38] 37.8
[24] 43.2
[22] 80.2
Ours 74.9

9.6
5.0
7.9
11.0
5.6
9.0

ﬂoor
45.0
42.6
45.8
47.2
43.8
48.3
78.6
90.9
82.3

chair sofa table door window bookshelf picture counter blinds
cabinet bed
9.4
16.9 12.8 18.5 6.1
21.8
17.4
0.8
21.5 4.1
12.5 3.4
6.4
2.9
12.8
19.9 11.6 19.3 6.0
23.3
15.4
9.6
17.2 13.4 20.4 6.8
21.5
18.8
0.9
12.9 3.8
22.3 3.9
6.3
3.0
13.1
20.8 12.1 20.9 6.8
23.6
17.2
23.1
42.5
26.2
33.2 40.6 34.3 33.2 43.6
31.2
64.8 76.0 58.6 62.6 47.7 66.4
58.9
43.1
67.7 55.5 57.8 45.6 52.8
62.1
47.3
paper towel shower box
ﬂoormat clothes ceiling books fridge tv
0.0
0.6
1.5
1.6
27.9
7.0
4.1
0.0
1.0
0.0
0.9
9.7
0.0
0.6
0.0
0.6
1.4
0.7
35.8
9.5
6.1
0.0
0.7
1.5
1.4
39.1
7.1
5.9
0.7
0.0
0.4
0.0
13.9
0.9
0.5
0.8
10.1 0.6
49.2
0.0
1.4
8.7
11.7
35.7
84.5
24.2 36.5 26.8 19.2 9.0
24.1
48.7 21.3 49.5 30.6 18.8 0.1
84.0
38.1
47.9 61.5 52.1 36.4 36.7 0
68.0

desk shelves curtain dresser pillow mirror
7.3
2.4
4.6
2.0
14.8
3.3
7.0
2.2
3.6
7.3
3.6
6.1
2.0
32.6
3.8
6.8
2.4
4.4
12.1 18.4
42.3
57.2
19.7 16.2
46.7
63.6
56.7
37.3 9.6
48.6
board person nightstand toilet sink
14.0
7.4
2.7
1.1
7.6
10.4
3.5
8.6
51.4
56.8
48.1

6.9
2.3
2.2
0.9
1.2
1.4
5.6
3.1
5.4
6.2
2.6
2.4
1.0
1.1
1.8
6.4
3.2
4.8
49.5
24.8
31.4
57.1 39.1
42.3
35.0
45.8 44.5
bathhub bag mean
8.3
0.9
0.6
5.3
0.4
0.0
9.0
0.6
1.1
9.3
1.1
0.9
6.0
0.5
0.0
10.1
1.3
1.1
18.6
47.0
36.3
24.1 45.9
45.1
23.6 48.1
65.6

4.3
2.0
4.4
6.9
3.6
7.8
59.1
67.0
63.4
lamp
0.9
0.7
0.9
1.3
0.8
1.2
44.2
48.8
58.0

8.9
2.3
12.0 15.2
12.3 14.8
2.6
14.9 16.8
64.1 53.0
73.0 66.2
68.8 67.9

1.2
0.3
1.4
1.3
0.6
1.6
27.0
24.4
28.4

0.0
0.3
0.7
0.0
0.3
0.8
35.7
17.9
72.6

1.1
2.6
1.7
1.5
1.5
1.8
25.0
42.9
36.4

2.2
1.7
5.2
2.6
2.2
6.2
31.8
33.8
39.4

1.0
15.3
1.7
1.2
10.1
1.0

1.9
0.1
0.2
2.2
0.3
0.2

1.2

[20] 0.0
[20] 0.0
[20] 0.0
[38] 0.0
[38] 0.0
[38] 0.0
[24] 5.6
[22] 0.1
Ours 0.0

in 15 class-wise Jaccard Indices. For a better understanding, we also show the
confusion matrix for this dataset in Fig. 3(a). It is worth mentioning that our
proposed architecture and most previous methods achieve zero accuracy on two
categories, i.e., ﬂoormat and shower, which mainly results from an imbalanced
data distribution instead of the capacity of our model.

NYUDv2 dataset: To further verify the eﬀectiveness of our architecture
and have more comparisons with existing state-of-the-art methods, we also con-
duct experiments on the NYUDv2 dataset. The results are presented in Table 2,
where the 13-class average Jaccard Index of our model is 20.3% higher than that
in [39]. Class frequencies and the confusion matrix are also shown in Table 2 and
Fig. 3(b) respectively. According to the reported results, our proposed architec-
ture gains 5.6% and 5.5% improvement in average Jaccard Index over [9] and
FCN-32s [10] respectively. Considering the listed class frequencies, our proposed
model signiﬁcantly outperforms existing methods on high frequency categories
and most low frequency categories, which primarily owes to the convolutional
features of the RGB image and the fused global contexts of the complete RGB-
D image. In terms of labeling categories with small and complex regions, e.g.,
pillows and chairs, our method also achieves a large improvement, which can be
veriﬁed in the following visual comparisons.

SUN3D dataset: Table 3 gives comparison results on the 1539 test images
in the SUN3D dataset. For fair comparison, the 12-class average Jaccard Index is
used in the comparison with the state-of-the-art results recently reported in [9].
Note that the 12-class accuracy of our network is calculated through the model
previously trained for 37 classes. Our model substantially outperforms the one
from [9] on large planar regions such as those labeled as ﬂoors and ceilings. This
also results from the incorporated convolutional features and the fused global
contexts. These comparison results further conﬁrm the power and generalization
capability of our LSTM-based model.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

11

(a) SUNRGBD

(b) NYUDv2

Fig. 3. Confusion matrix for SUNRGBD and NYUDv2. Class-wise Jaccard Index is
shown on the diagonal. Best viewed in color.

Table 2. Comparison of scene labeling on NYUDv2. We compare our proposed model
with existing state-of-the-art methods, i.e., [34], [24], [25], [26] and [9]. Class-wise Jac-
card Index and average Jaccard Index of 37 classes are presented. ‘Freq’ stands for
class frequency. Boldface numbers mean best performance.

Wall
Freq 21.4
[34] 60.7
[24] 60.0
[25] 67.4
[26] 61.4
[9]
65.7
Ours 79.6

2.1

2.7

ﬂoor
9.1
77.8
74.4
80.5
66.4
62.5
83.5

chair sofa table door window bookshelf picture counter blinds
cabinet bed
1.9
2.1
3.3
2.2
3.8
6.2
22.7
32.4 25.3 21.0 5.9
29.7
40.3
33.0
17.3
32.5 28.2 16.6 12.9 27.7
42.3
37.1
20.5
40.4 44.8 30.0 12.1 34.1
56.4
41.4
17.6
34.4 33.8 22.6 8.3
27.6
43.9
38.2
36.3
32.1
40.1
44.5 50.8 43.5 51.6 49.2
59.5
77.0 58.3 64.9 42.6 47.0 43.6
69.3
paper towel shower box
ﬂoormat clothes ceiling books fridge tv
0.3
0.4
0.4
0.4
0.5
0.6
0.1
5.7
3.6
12.7 0.1
1.4
3.3
1.9
18.6 11.7 12.6 5.4
1.1
14.5 14.4 14.1 19.8 6.0
0.8
6.1
2.6
2.9
36.3
17.6

desk shelves curtain dresser pillow mirror
1.0
1.1
1.7
2.1
4.7
3.3
40.6
35.7
10.1 6.1
26.5
32.4
10.1 1.6
44.7
38.7
5.1
2.7
33.6
27.7
48.0 45.2
55.8
41.4
74.5
33.6 13.1
74.6
board person nightstand toilet sink
0.3
0.3
0.0
0.2
12.9
28.2
60.6
93.9

1.0
0.8
0.9
4.4
18.9
13.3
17.9
19.7
7.0
14.6
31.3
21.6
12.5
10.7
16.8
50.5 46.1
55.3
56.5
48.0 47.7
bathhub bag mean
0.2
0.3
17.5
0.0
0.0
20.2
1.2
7.8
30.0
0.2
29.4
0.2
16.2
29.2
37.3 43.9
38.5
49.4
7.5
72.6

0.6
5.5
14.8
5.8
3.6
39.1 53.6 50.1 35.4 39.9 41.8
49.7 0.0

1.1
27.4
27.6
26.3
18.9
53.1
53.2
lamp
0.3
15.9
14.2
31.2
5.4
26.3
67.6

0.3
26.7 25.1
35.2 28.9
52.5 47.9
32
20.9
31.8 22.5
81.8 58.4

1.4
73.2
53.9
61.8
46.1
50.6
70.2

0.7
6.5
9.5
8.0
2.7
35.4
22.7

0.3
6.6
13.6
1.5
5
35.6
77.0

0.3
6.3
9.2
15.7
6.9
32.5
0

1.4
33.1
38.6
50.7
30.2
39.2
68.2

0.0 52.1 60.6 0

6.2

3.2

Freq 0.7
[34] 7.1
[24] 20.1
[25] 28.2
[26] 13.8
[9]
54.1
Ours 0.0

4.3 Ablation Study

To discover the vital elements in our proposed model, we conduct an ablation
study to remove or replace individual components in our deep network when
training and testing on the SUNRGBD dataset. Speciﬁcally, we have tested the
performance of our model without the RGB path, the depth path, multi-scale
RGB feature concatenation, the memorized context layers or the memorized
fusion layer. In addition, we also conduct an experiment with a model that does
not combine the ﬁnal convolutional features of photometric channels (i.e., Conv7
in Fig. 2) with the global contexts of the complete RGB-D image to ﬁgure out the
importance of diﬀerent components. The results are presented in Table 4. From
the given results, we ﬁnd that the ﬁnal convolutional features of the photometric
channels is the most vital information, i.e., the cross-layer combination is the
most eﬀective component as the performance drops to 15.2% without it, which
is consistent with previously mentioned properties of depth and photometric
data. In addition, multi-scale RGB feature concatenation before the memorized
context layer also plays a vital role as it directly aﬀects the vertical contexts in

12

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

Table 3. Comparison of class-wise Jaccard Index and 12-class average Jaccard Index
on SUN3D.

wall ﬂoor bed chair table counter curtain ceiling tv toilet bathtub bag Mean
73 35

[9]
71 35
Ours 73 86 32 65

30
57

52
22

68
76

27
69

56 23
75 62

49
62

29 45.7
23 58.5

Table 4. Ablation Study

Model

Without RGB path, using Deeplab+Renet for depth path
Without depth path
Without multi-scale RGB feature concatenation
Without cross-layer integration of RGB convolutional features
Without memorized fusion layer
Without memorized context layers
Without any memorized (context or fusion) layers

Mean Accuracy
15.8%
43.7%
42.1%
15.2%
44.7%
45.7%
45.0%

the photometric channels and the performance drops to 42.1% without it. It is
obvious that performance would be inevitably harmed without the depth path.
Among the memorized layers, the memorized fusion layer is more important
than the memorized context layers in our pipeline as it accomplishes the fusion
of contexts in photometric and depth channels.

4.4 Visual Comparisons

SUNRGBD Dataset: We present visual results of RGB-D scene labeling in
Fig. 4. Here, we leverage super-pixel based averaging to smooth visual labeling
results as being done in [9]. The algorithm in [40] is used for performing super-
pixel segmentation. As can be observed in Fig. 4, our proposed deep network
produces accurate and semantically meaningful labeling results, especially for
large regions and high frequency labels. For instance, our model takes advantage
of global contexts when labeling ‘bed’ in Fig. 4(a), ‘wall’ in Fig. 4(e) and ‘mirror’
in Fig. 4(i). Our proposed model can precisely label almost all ‘chairs’ (a high
frequency label) by exploiting integrated photometric and depth information,
regardless of occlusions.

NYUDv2 Dataset: We also perform visual comparisons on the NYUDv2
benchmark, which has complicated indoor scenes and well-labeled ground truth.
We compare our scene labeling results with those publicly released labeling re-
sults from [25]. It is obvious that our results are clearly better than those from
[25] both visually and numerically (under the metric of average Jaccard Index)
even though scene labeling in [25] is based on sophisticated segmentation.

Label Reﬁnement: Surprisingly, our model can intelligently reﬁne certain
region annotations, which might have inaccuracies due to under-segmentation,
especially in the newly captured 3943 RGB-D images, as shown in Fig. 6. Specif-
ically, the cabinets in Fig. 6(a) were annotated as ‘background’, the pillows in

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

13

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

(q) legend of semantic labels

Fig. 4. Examples of semantic labeling results on the SUNRGBD dataset. The top row
shows the input RGB images, the bottom row shows scene labeling obtained with our
model and the middle row has the ground truth. Semantic labels and their correspond-
ing colors are shown at the bottom.

Fig. 6(g) as ‘bed’, and the tables in Fig. 6(n) as ‘wall’ by mistake. Our model
can eﬀectively deal with these diﬃcult regions. For example, the annotation of
the picture in Fig. 6(e) and that of the pillows in Fig. 6(g) have been corrected.
Thus, our model can be exploited to reﬁne certain annotations in the SUNRGBD
dataset, which is another contribution of our model.

5 Conclusions

In this paper, we have developed a novel Long Short-Term Memorized Con-
text Fusion (LSTM-CF) model that captures image contexts from a global per-
spective and deeply fuses contextual representations from multiple sources (i.e.,
depth and photometric data) for semantic scene labeling. In future, we will ex-
plore how to extend the memorized layers with an attention mechanism, and
reﬁne the performance of our model in boundary labeling.

14

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Fig. 5. Visual comparison of scene labeling results on the NYUDv2 dataset. The ﬁrst
and second rows show the input RGB images and their corresponding groundtruth
labeling. The third row shows the results from [25] and the last row shows the results
from our model.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

(p)

Fig. 6. Annotation reﬁnement on the SUNRGBD dataset. The top row shows the input
RGB images, the middle row shows the original annotations, and the bottom row shows
scene labeling results from our model.

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

15

References

1. Wu, C., Lenz, I., Saxena, A.: Hierarchical semantic labeling for task-relevant rgb-d

perception. In: Robotics: Science and systems (RSS). (2014)

2. Hinterstoisser, S., Lepetit, V., Ilic, S., Holzer, S., Bradski, G., Konolige, K., Navab,
N.: Model based training, detection and pose estimation of texture-less 3d objects
In: Computer Vision–ACCV 2012. Springer (2012)
in heavily cluttered scenes.
548–562

3. Holz, D., Holzer, S., Rusu, R.B., Behnke, S.: Real-time plane segmentation using
rgb-d cameras. In: RoboCup 2011: robot soccer world cup XV. Springer (2011)
306–317

4. Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., Manning, C.D.: Generating se-
mantically precise scene graphs from textual descriptions for improved image re-
trieval. In: Proceedings of the Fourth Workshop on Vision and Language. (2015)
70–80

5. Yan, Z., Zhang, H., Wang, B., Paris, S., Yu, Y.: Automatic photo adjustment using

deep neural networks. ACM Transactions on Graphics 35(2) (2016)

6. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions
on 35(8) (2013) 1915–1929

7. Gould, S., Fulton, R., Koller, D.: Decomposing a scene into geometric and se-
mantically consistent regions. In: Computer Vision, 2009 IEEE 12th International
Conference on, IEEE (2009) 1–8

8. Tighe, J., Lazebnik, S.: Superparsing: scalable nonparametric image parsing with

superpixels. In: Computer Vision–ECCV 2010. Springer (2010) 352–365

9. Khan, S.H., Bennamoun, M., Sohel, F., Togneri, R., Naseem, I.: Integrating geo-
metrical context for semantic labeling of indoor scenes using rgbd images. Inter-
national Journal of Computer Vision (2015) 1–20

10. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3431–3440

11. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: Proceed-
ings of the IEEE International Conference on Computer Vision. (2015) 1529–1537
12. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. arXiv
preprint arXiv:1412.7062 (2014)

13. Gupta, S., Girshick, R., Arbel´aez, P., Malik, J.: Learning rich features from rgb-d
images for object detection and segmentation. In: Computer Vision–ECCV 2014.
Springer (2014) 345–360

14. Song, S., Xiao, J.: Deep sliding shapes for amodal 3d object detection in rgb-d

images. arXiv preprint arXiv:1511.02300 (2015)

15. Liu, Z., Li, X., Luo, P., Loy, C.C., Tang, X.: Semantic image segmentation via
deep parsing network. In: Proceedings of the IEEE International Conference on
Computer Vision. (2015) 1377–1385

16. Liang, X., Shen, X., Xiang, D., Feng, J., Lin, L., Yan, S.: Semantic object parsing
with local-global long short-term memory. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. (2016)

17. Byeon, W., Breuel, T.M., Raue, F., Liwicki, M.: Scene labeling with lstm recurrent
neural networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3547–3555

16

Z. Li, Y. Gan, X. Liang, Y. Yu, H. Cheng, and L. Lin

18. Pinheiro, P., Collobert, R.: Recurrent convolutional neural networks for scene
labeling. In: Proceedings of the 31st International Conference on Machine Learning
(ICML-14). (2014) 82–90

19. Visin, F., Kastner, K., Cho, K., Matteucci, M., Courville, A., Bengio, Y.: Renet:
A recurrent neural network based alternative to convolutional networks. arXiv
preprint arXiv:1505.00393 (2015)

20. Song, S., Lichtenberg, S.P., Xiao, J.: Sun rgb-d: A rgb-d scene understanding
benchmark suite. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. (2015) 567–576

21. Kumar, M.P., Koller, D.: Eﬃciently selecting regions for scene understanding. In:
Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on,
IEEE (2010) 3217–3224

22. Kendall, A., Badrinarayanan, V., Cipolla, R.: Bayesian segnet: Model uncertainty
in deep convolutional encoder-decoder architectures for scene understanding. arXiv
preprint arXiv:1511.02680 (2015)

23. Lempitsky, V., Vedaldi, A., Zisserman, A.: Pylon model for semantic segmentation.

In: Advances in neural information processing systems. (2011) 1485–1493
24. Ren, X., Bo, L., Fox, D.: Rgb-(d) scene labeling: Features and algorithms.

In:
Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,
IEEE (2012) 2759–2766

25. Gupta, S., Arbel´aez, P., Girshick, R., Malik, J.: Indoor scene understanding with
rgb-d images: Bottom-up segmentation, object detection and semantic segmenta-
tion. International Journal of Computer Vision 112(2) (2015) 133–149

26. Wang, A., Lu, J., Cai, J., Wang, G., Cham, T.J.: Unsupervised joint feature learn-
ing and encoding for rgb-d scene labeling. Image Processing, IEEE Transactions
on 24(11) (2015) 4459–4473

27. Husain, F., Schulz, H., Dellen, B., Torras, C., Behnke, S.: Combining semantic and
geometric features for object class segmentation of indoor scenes. IEEE Robotics
and Automation Letters 2(1) (2017) 49–55

28. Schmidhuber, J.: A local learning algorithm for dynamic feedforward and recurrent

networks. Connection Science 1(4) (1989) 403–412

29. Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with gra-
dient descent is diﬃcult. Neural Networks, IEEE Transactions on 5(2) (1994)
157–166

30. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

9(8) (1997) 1735–1780

31. Graves, A., Schmidhuber, J.: Oﬄine handwriting recognition with multidimen-
sional recurrent neural networks. In: Advances in neural information processing
systems. (2009) 545–552

32. Stollenga, M.F., Byeon, W., Liwicki, M., Schmidhuber, J.:

Parallel multi-
dimensional lstm, with application to fast biomedical volumetric image segmen-
tation. In: Advances in Neural Information Processing Systems. (2015) 2980–2988
In: IEEE

33. Li, G., Yu, Y.: Deep contrast learning for salient object detection.

Conference on Computer Vision and Pattern Recognition (CVPR). (2016)

34. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and support
inference from rgbd images. In: Computer Vision–ECCV 2012. Springer (2012)
746–760

35. Janoch, A., Karayev, S., Jia, Y., Barron, J.T., Fritz, M., Saenko, K., Darrell, T.: A
category-level 3d object dataset: Putting the kinect to work. In: Consumer Depth
Cameras for Computer Vision. Springer (2013) 141–165

LSTM-CF: Unifying Context Modeling and Fusion with LSTMs

17

36. Xiao, J., Owens, A., Torralba, A.: Sun3d: A database of big spaces reconstructed
using sfm and object labels. In: Proceedings of the IEEE International Conference
on Computer Vision. (2013) 1625–1632

37. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

38. Liu, C., Yuen, J., Torralba, A.: Sift ﬂow: Dense correspondence across scenes and
its applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on
33(5) (2011) 978–994

39. Couprie, C., Farabet, C., Najman, L., LeCun, Y.: Toward real-time indoor semantic
segmentation using depth information. Journal of Machine Learning Research
(2014)

40. Felzenszwalb, P.F., Huttenlocher, D.P.: Eﬃcient graph-based image segmentation.

International Journal of Computer Vision 59(2) (2004) 167–181

