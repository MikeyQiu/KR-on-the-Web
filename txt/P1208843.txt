Unsupervised Statistical Machine Translation

Mikel Artetxe, Gorka Labaka, Eneko Agirre
IXA NLP Group
University of the Basque Country (UPV/EHU)
{mikel.artetxe,gorka.labaka,e.agirre}@ehu.eus

8
1
0
2
 
p
e
S
 
4
 
 
]
L
C
.
s
c
[
 
 
1
v
2
7
2
1
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

While modern machine translation has relied
on large parallel corpora, a recent line of work
has managed to train Neural Machine Trans-
lation (NMT) systems from monolingual cor-
pora only (Artetxe et al., 2018c; Lample et al.,
2018). Despite the potential of this approach
for low-resource settings, existing systems are
far behind their supervised counterparts, lim-
iting their practical interest.
In this paper,
we propose an alternative approach based on
phrase-based Statistical Machine Translation
(SMT) that signiﬁcantly closes the gap with
supervised systems. Our method proﬁts from
the modular architecture of SMT: we ﬁrst in-
duce a phrase table from monolingual cor-
pora through cross-lingual embedding map-
pings, combine it with an n-gram language
model, and ﬁne-tune hyperparameters through
an unsupervised MERT variant.
In addition,
iterative backtranslation improves results fur-
ther, yielding, for instance, 14.08 and 26.22
BLEU points in WMT 2014 English-German
and English-French, respectively, an improve-
ment of more than 7-10 BLEU points over pre-
vious unsupervised systems, and closing the
gap with supervised SMT (Moses trained on
Europarl) down to 2-5 BLEU points. Our
implementation is available at https://
github.com/artetxem/monoses.

1

Introduction

Neural Machine Translation (NMT) has recently
become the dominant paradigm in machine trans-
lation (Vaswani et al., 2017). In contrast to more
rigid Statistical Machine Translation (SMT) archi-
tectures (Koehn et al., 2003), NMT models are
trained end-to-end, exploit continuous representa-
tions that mitigate the sparsity problem, and over-
come the locality problem by making use of un-
constrained contexts. Thanks to this additional
ﬂexibility, NMT can more effectively exploit large

parallel corpora, although SMT is still superior
when the training corpus is not big enough (Koehn
and Knowles, 2017).

Somewhat paradoxically, while most machine
translation research has focused on resource-rich
settings where NMT has indeed superseded SMT,
a recent line of work has managed to train an NMT
system without any supervision, relying on mono-
lingual corpora alone (Artetxe et al., 2018c; Lam-
ple et al., 2018). Given the scarcity of parallel
corpora for most language pairs, including less-
resourced languages but also many combinations
of major languages, this research line opens excit-
ing opportunities to bring effective machine trans-
lation to many more scenarios. Nevertheless, ex-
isting solutions are still far behind their supervised
counterparts, greatly limiting their practical us-
ability. For instance, existing unsupervised NMT
systems obtain between 15-16 BLEU points in
WMT 2014 English-French translation, whereas
a state-of-the-art NMT system obtains around 41
(Artetxe et al., 2018c; Lample et al., 2018; Yang
et al., 2018).

In this paper, we explore whether the rigid and
modular nature of SMT is more suitable for these
unsupervised settings, and propose a novel un-
supervised SMT system that can be trained on
monolingual corpora alone.
For that purpose,
we present a natural extension of the skip-gram
model (Mikolov et al., 2013b) that simultaneously
learns word and phrase embeddings, which are
then mapped to a cross-lingual space through self-
learning (Artetxe et al., 2018b). We use the re-
sulting cross-lingual phrase embeddings to induce
a phrase table, and combine it with a language
model and a distance-based distortion model to
build a standard phrase-based SMT system. The
weights of this model are tuned in an unsupervised
manner through an iterative Minimum Error Rate
Training (MERT) variant, and the entire system

Figure 1: Architecture of our system, with references to sections.

is further improved through iterative backtransla-
tion. The architecture of the system is sketched
in Figure 1. Our experiments on WMT German-
English and French-English datasets show the ef-
fectiveness of our proposal, where we obtain im-
provements above 7-10 BLEU points over previ-
ous unsupervised NMT-based approaches, closing
the gap with supervised SMT (Moses trained on
Europarl) down to 2-5 points.

The remaining of this paper is structured as fol-
lows. Section 2 introduces phrase-based SMT.
Section 3 presents our unsupervised approach to
learn cross-lingual n-gram embeddings, which are
the basis of our proposal. Section 4 describes the
proposed unsupervised SMT system itself, while
Section 5 discusses its iterative reﬁnement through
backtranslation. Section 6 describes the experi-
ments run and the results obtained. Section 7 dis-
cusses the related work on the topic, and Section 8
concludes the paper.

2 Background: phrase-based SMT

While originally motivated as a noisy channel
model (Brown et al., 1990), phrase-based SMT
is now formulated as a log-linear combination
of several statistical models that score translation
candidates (Koehn et al., 2003). The parame-
ters of these scoring functions are estimated in-
dependently based on frequency counts, and their
weights are then tuned in a separate validation set.
At inference time, a decoder tries to ﬁnd the trans-
lation candidate with the highest score according
to the resulting combined model. The speciﬁc
scoring models found in a standard SMT system
are as follows:

• Phrase table. The phrase table is a collec-
tion of source language n-grams and a list of
their possible translations in the target lan-
guage along with different scores for each of

them. So as to translate longer sequences, the
decoder combines these partial n-gram trans-
lations, and ranks the resulting candidates ac-
cording to their corresponding scores and the
rest of scoring functions.
In order to build
the phrase table, SMT computes word align-
ments in both directions from a parallel cor-
pus, symmetrizes these alignments using dif-
ferent heuristics (Och and Ney, 2003), ex-
tracts the set of consistent phrase pairs, and
scores them based on frequency counts. For
that purpose, standard SMT uses 4 scores for
each phrase table entry: the direct and inverse
lexical weightings, which are derived from
word level alignments, and the direct and in-
verse phrase translation probabilities, which
are computed at the phrase level.

• Language model. The language model as-
signs a probability to a word sequence in the
target language. Traditional SMT uses n-
gram language models for that, which use
simple frequency counts over a large mono-
lingual corpus with back-off and smoothing.

• Reordering model. The reordering model
accounts for different word orders across lan-
guages, scoring translation candidates ac-
cording to the position of each translated
phrase in the target
Standard
SMT combines two such models: a distance
based distortion model that penalizes devia-
tion from a monotonic order, and a lexical re-
ordering model that incorporates phrase ori-
entation frequencies from a parallel corpus.

language.

• Word and phrase penalties. The word and
phrase penalties assign a ﬁxed score to every
generated word and phrase, and are useful to
control the length of the output text and the
preference for shorter or longer phrases.

Having trained all these different models, a tuning
process is applied to optimize their weights in the
resulting log-linear model, which typically max-
imizes some evaluation metric in a separate val-
idation parallel corpus. A common choice is to
optimize the BLEU score through Minimum Error
Rate Training (MERT) (Och, 2003).

3 Cross-lingual n-gram embeddings

Section 3.1 presents our proposed extension of
skip-gram to learn n-gram embeddings, while Sec-
tion 3.2 describes how we map them to a shared
space to obtain cross-lingual n-gram embeddings.

3.1 Learning n-gram embeddings

Negative sampling skip-gram takes word-context
pairs (w, c), and uses logistic regression to predict
whether the pair comes from the true distribution
as sampled from the training corpus, or it is one
of the k draws from a noise distribution (Mikolov
et al., 2013b):

log σ (w · c) +

EcN ∼PD [log σ (−w · cN )]

k
(cid:88)

i=1

In its basic formulation, both w and c corre-
spond to words that co-occur within a certain win-
dow in the training corpus. So as to learn em-
beddings for non-compositional phrases like New
York Times or Toronto Maple Leafs, Mikolov et al.
(2013b) propose to merge them into a single token
in a pre-processing step. For that purpose, they use
a scoring function based on their co-occurence fre-
quency in the training corpus, with a discounting
coefﬁcient δ that penalizes rare words, and itera-
tively merge those above a threshold:

score(wi, wj) =

count (wi, wj) − δ
count (wi) × count (wj)

However, we also need to learn representations
for compositional n-grams in our scenario, as there
is not always a 1:1 correspondence for n-grams
across languages even for compositional phrases.
For instance, the phrase he will come would typi-
cally be translated as vendrá into Spanish, so one
would need to represent the entire phrase as a sin-
gle unit to properly model this relation.

One option would be to merge all n-grams re-
gardless of their score, but this is not straightfor-
ward given their overlapping nature, which is fur-
ther accentuated when considering n-grams of dif-
ferent lengths. While we tried to randomly gen-
erate multiple consistent segmentations for each

sentence and train the embeddings over the result-
ing corpus, this worked poorly in our preliminary
experiments. We attribute this to the complex in-
teractions arising from the stochastic segmentation
the co-occurrence distribution changes rad-
(e.g.
ically, even for unigrams), severely accentuating
the sparsity problem, among other issues.

As an alternative approach, we propose a gen-
eralization of skip-gram that learns n-gram em-
beddings on-the-ﬂy, and has the desirable property
of unigram invariance: our proposed model learns
the exact same embeddings as the original skip-
gram for unigrams, while simultaneously learning
additional embeddings for longer n-grams. This
way, for each word-context pair (w, c) at distance
d within the given window, we update their corre-
sponding embeddings w and c with the usual neg-
ative sampling loss. In addition to that, we look
at all n-grams p of different lengths that are at the
same distance d, and for each pair (p, c), we up-
date the embedding p through negative sampling.
In order to enforce unigram invariance, the context
c and negative samples cN , which always corre-
spond to unigrams, are not updated for (p, c). This
allows to naturally learn n-gram embeddings ac-
cording to their co-occurrence patterns as modeled
by skip-gram, without introducing subtle interac-
tions that affect its fundamental behavior.

We implemented the above procedure as an ex-
tension of word2vec, and use it to train monolin-
gual n-gram embeddings with a window size of 5,
300 dimensions, 10 negative samples, 5 iterations
and subsampling disabled. So as to keep the model
size within a reasonable limit, we restrict the vo-
cabulary to the most frequent 200,000 unigrams,
400,000 bigrams and 400,000 trigrams.

3.2 Cross-lingual mapping

Cross-lingual mapping methods take indepen-
dently trained word embeddings in two languages,
and learn a linear transformation to map them
to a shared cross-lingual space (Mikolov et al.,
2013a; Artetxe et al., 2018a). Most mapping meth-
ods are supervised, and rely on a bilingual dic-
tionary, typically in the range of a few thousand
entries, although a recent line of work has man-
aged to achieve comparable results in a fully un-
supervised manner based on either self-learning
(Artetxe et al., 2017, 2018b) or adversarial train-
ing (Zhang et al., 2017a,b; Conneau et al., 2018).
In our case, we use the method of Artetxe et al.

(2018b) to map the n-gram embeddings to a shared
cross-lingual space using their open source im-
plementation VecMap1. Originally designed for
word embeddings, this method builds an initial
mapping by connecting the intra-lingual similarity
distribution of embeddings in different languages,
and iteratively improves this solution through self-
learning. The method applies a frequency-based
vocabulary cut-off, learning the mapping over the
20,000 most frequent words in each language. We
kept this cut-off to learn the mapping over the most
frequent 20,000 unigrams, and then apply the re-
sulting mapping to the entire embedding space, in-
cluding longer n-grams.

4 Unsupervised SMT

As discussed in Section 2, phrase-based SMT
follows a modular architecture that combines
several scoring functions through a log-linear
model. Among the scoring functions found in
standard SMT systems, the distortion model and
word/phrase penalties are parameterless, while the
language model is trained on monolingual cor-
pora, so they can all be directly integrated into our
unsupervised system. From the remaining mod-
els, typically trained on parallel corpora, we de-
cide to leave the lexical reordering out, as the dis-
tortion model already accounts for word reorder-
ing. As for the phrase table, we learn cross-lingual
n-gram embeddings as discussed in Section 3, and
use them to induce and score phrase translation
pairs as described next (Section 4.1). Finally, we
tune the weights of the resulting log-linear model
using an unsupervised procedure based on back-
translation (Section 4.2).

Unless otherwise speciﬁed, we use Moses2 with
default hyperparameters to implement these differ-
ent components of our system. We use KenML
(Heaﬁeld et al., 2013), bundled in Moses by de-
fault, to estimate our 5-gram language model with
modiﬁed Kneser-Ney smoothing, pruning n-grams
longer than 3 with a single occurrence.

4.1 Phrase table induction

Given the lack of a parallel corpus from which to
extract phrase translation pairs, every n-gram
in the target language could be taken as a poten-
tial translation candidate for each n-gram in the
source language. So as to keep the size of the

phrase table within a reasonable limit, we train
cross-lingual phrase embeddings as described in
Section 3, and limit the translation candidates for
each source phrase to its 100 nearest neighbors in
the target language.

In order to estimate their corresponding phrase
translation probabilities, we apply the softmax
function over the cosine similarities of their re-
spective embeddings. More concretely, given the
source language phrase ¯e and the translation can-
didate ¯f , their direct phrase translation probability
is computed as follows3:

φ( ¯f |¯e) =

cos(¯e, ¯f )/τ
¯f (cid:48) cos(¯e, ¯f (cid:48))/τ

(cid:80)

Note that, in the above formula, ¯f (cid:48) iterates across
all target language embeddings, and τ is a con-
stant temperature parameter that controls the con-
ﬁdence of the predictions. In order to tune it, we
induce a dictionary over the cross-lingual embed-
dings themselves with nearest neighbor retrieval,
and use maximum likelihood estimation over it.
However, inducing the dictionary in the same di-
rection as the probability predictions leads to a de-
generated solution (softmax approximates the hard
maximum underlying nearest neighbor as τ ap-
proaches 0), so we induce the dictionary in the
opposite direction and apply maximum likelihood
estimation over it:

min
τ

(cid:88)

¯f

log φ( ¯f | NN¯e( ¯f ))+

log φ(¯e| NN ¯f (¯e))

(cid:88)

¯e

So as to optimize τ , we use Adam with a learn-
ing rate of 0.0003 and a batch size of 200, imple-
mented in PyTorch.

In order to compute the lexical weightings, we
align each word in the target phrase with the one
in the source phrase most likely generating it,
and take the product of their respective translation
probabilities:

lex( ¯f |¯e) =

(cid:89)

max

(cid:18)

(cid:19)

(cid:15), max
j

w( ¯fi|¯ej)

i

The constant (cid:15) guarantees that each target lan-
guage word will get a minimum probability mass,
which is useful to model NULL alignments. In our
experiments, we set (cid:15) = 0.001, which we ﬁnd to

1https://github.com/artetxem/vecmap
2http://www.statmt.org/moses/

ﬁned analogously.

3The inverse phrase translation probability φ(¯e| ¯f ) is de-

Algorithm 1 Unsupervised tuning
Input: ms→t (source-to-target models)
Input: mt→s (target-to-source models)
Input: cs (source validation corpus)
Input: ct (target validation corpus)
Output: ws→t (source-to-target weights)
Output: wt→s (target-to-source weights)

1: wt→s ← DEFAULT_WEIGHTS
2: repeat
3:

bts ← TRANSLATE(mt→s, wt→s, ct)
ws→t ← MERT(ms→t, bts, ct)
btt ← TRANSLATE(ms→t, ws→t, cs)
wt→s ← MERT(mt→s, btt, cs)

4:

5:

6:
7: until convergence

work well in practice. Finally, the word transla-
tion probabilities w( ¯fi|¯ej) are computed using the
same formula deﬁned for phrase translation prob-
abilities (see above), with the difference that the
partition function goes over unigrams only.

4.2 Unsupervised tuning

As discussed in Section 2, standard SMT uses
MERT over a small parallel corpus to tune the
weights of the different scoring functions com-
bined through its log-linear model. Given that we
only have access to monolingual corpora in our
scenario, we propose to generate a synthetic paral-
lel corpus through backtranslation (Sennrich et al.,
2016) and apply MERT tuning over it, iteratively
repeating the process in both directions (see Al-
gorithm 1). For that purpose, we reserve a random
subset of 10,000 sentences from each monolingual
corpora, and run the proposed algorithm over them
for 10 iterations, which we ﬁnd to be enough for
convergence.

5

Iterative reﬁnement

The procedure described in Section 4 sufﬁces to
train an SMT system from monolingual corpora
which, as shown by our experiments in Section
6, already outperforms previous unsupervised sys-
tems. Nevertheless, our proposed method still
makes important simpliﬁcations that could com-
promise its potential performance: it does not use
any lexical reordering model, its phrase table is
limited by the underlying embedding vocabulary
(e.g.
it does not include phrases longer than tri-
grams, see Section 3.1), and the phrase translation
probabilities and lexical weightings are estimated
based on cross-lingual embeddings.

Algorithm 2 Iterative reﬁnement
Input: cs (source language corpus)
Input: ct (target language corpus)
Input/Output: mt→s (target-to-source models)
Input/Output: wt→s (target-to-source weights)
Output: ms→t (source-to-target models)
Output: ws→t (source-to-target weights)

1: trains, vals ← SPLIT(cs)
2: traint, valt ← SPLIT(ct)
3: repeat
4:

5:

6:

7:

8:

9:

10:

btts ← TRANSLATE(mt→s, wt→s, traint)
btvs ← TRANSLATE(mt→s, wt→s, valt)
ms→t ← TRAIN(btts, traint)
ws→t ← MERT(ms→t, btvs, valt)
bttt ← TRANSLATE(ms→t, ws→t, trains)
btvt ← TRANSLATE(ms→t, ws→t, vals)
mt→s ← TRAIN(bttt, trains)
wt→s ← MERT(mt→s, btvt, vals)

11:
12: until convergence

In order to overcome these limitations, we pro-
pose an iterative reﬁnement procedure based on
backtranslation (Sennrich et al., 2016). More con-
cretely, we generate a synthetic parallel corpus by
translating the monolingual corpus in one of the
languages with the initial system, and train and
tune a standard SMT system over it in the oppo-
site direction. Note that this new system does not
have any of the initial restrictions: the phrase table
is built and scored using standard word alignment
with an unconstrained vocabulary, and a lexical re-
ordering model is also learned. Having done that,
we use the resulting system to translate the mono-
lingual corpus in the other language, and train an-
other SMT system over it in the other direction. As
detailed in Algorithm 2, this process is repeated it-
eratively until some convergence criterion is met.
While this procedure would be expected to pro-
duce a more accurate model at each iteration, it
also happens to be very expensive computation-
ally.
In order to accelerate our experiments, we
use a random subset of 2 million sentences from
each monolingual corpus for training4, in addition
to the 10,000 separate sentences that are held out
as a validation set for MERT tuning, and perform
a ﬁxed number of 3 iterations of the above algo-
rithm. Moreover, we use FastAlign (Dyer et al.,
2013) instead of GIZA++ to make word alignment
faster. Other than that, training over the synthetic

4Note that we reuse the original language model, which is

trained in the full corpus.

WMT-14

WMT-16

FR-EN EN-FR DE-EN EN-DE

DE-EN EN-DE

Artetxe et al. (2018c)
Lample et al. (2018)
Yang et al. (2018)

Proposed system

15.56
14.31
15.58

25.87

15.13
15.05
16.97

26.22

10.21
-
-

17.43

6.55
-
-

14.08

-
13.33
14.62

23.05

-
9.64
10.86

18.23

Table 1: Results of the proposed method in comparison to existing unsupervised NMT systems (BLEU).

parallel corpus is done through standard Moses
tools with default settings.

6 Experiments and results

In order to make our experiments comparable to
previous work, we use the French-English and
German-English datasets from the WMT 2014
shared task. As discussed throughout the pa-
per, our system is trained on monolingual cor-
pora alone, so we take the concatenation of
all News Crawl monolingual corpora from 2007
to 2013 as our training data, which we tok-
enize and truecase using standard Moses tools.
The resulting corpus has 749 million tokens in
French, 1,606 million tokens in German, and
2,109 million tokens in English. Following com-
mon practice, the systems are evaluated in new-
stest2014 using tokenized BLEU scores as com-
puted by the multi-bleu.perl script
in-
In addition to that, we also
cluded in Moses.
report results in German-English newstest2016
(from WMT 2016), as this was used by some pre-
vious work in unsupervised NMT (Lample et al.,
2018; Yang et al., 2018)5. So as to be faithful to
our target scenario, we did not use any parallel
data in these language pairs, not even for devel-
opment purposes. Instead, we ran all our prelimi-
nary experiments on WMT Spanish-English data,
where we made all development decisions.

We present the results of our ﬁnal system in
comparison to other previous work in Section 6.1.
Section 6.2 then presents an ablation study of our
proposed method, where we analyze the contribu-
tion of its different components. Section 6.3 com-
pares the obtained results to those of different su-
pervised systems, analyzing the effect of some of
the inherent limitations of our method in a stan-

5Note that we use the same model trained in WMT 2014
for these experiments, so it is likely that our results could
be further improved by using the more extensive data from
WMT 2016.

dard phrase-based SMT system. Finally, Section
6.4 presents some translation examples from our
system.

6.1 Main results

We report the results obtained by our proposed
system in Table 1. As it can be seen, our system
obtains the best published results by a large mar-
gin, surpassing previous unsupervised NMT sys-
tems by around 10 BLEU points in French-English
(both directions), and more than 7 BLEU points in
German-English (both directions and datasets).

This way, while previous progress in the task
has been rather incremental (Yang et al., 2018),
our work represents an important step towards
high-quality unsupervised machine translation,
with improvements over 50% in all cases. This
suggests that, in contrast to previous NMT-based
approaches, phrase-based SMT may provide a
more suitable framework for unsupervised ma-
chine translation, which is in line with previ-
ous results in low-resource settings (Koehn and
Knowles, 2017).

6.2 Ablation analysis

We present ablation results of our proposed system
in Table 2. The ﬁrst row corresponds to the initial
system with our induced phrase table (Section 4.1)
and default weights as used by Moses, whereas
the second row uses our unsupervised MERT pro-
cedure to tune these weights (Section 4.2). The
remaining rows represent different iterations of
our reﬁnement procedure (Section 5), which uses
backtranslation to iteratively train a standard SMT
system from a synthetic parallel corpus.

The results show that the initial system with de-
fault weights (ﬁrst row) is already better than pre-
vious unsupervised NMT systems (Table 1) by a
substantial margin (2-6 BLEU points). Our un-
supervised tuning procedure further improves re-
sults, bringing an improvement of over 1 BLEU

WMT-14

WMT-16

FR-EN EN-FR DE-EN EN-DE

DE-EN EN-DE

Unsupervised SMT
+ unsupervised tuning
+ iterative reﬁnement (it1)
+ iterative reﬁnement (it2)
+ iterative reﬁnement (it3)

21.16
22.17
24.81
26.13
25.87

20.13
22.22
26.53
26.57
26.22

13.86
14.73
16.01
17.30
17.43

10.59
10.64
13.45
13.95
14.08

18.01
18.21
20.76
22.80
23.05

13.22
13.12
16.94
18.18
18.23

Table 2: Ablation results (BLEU). The last row corresponds to our full system. Refer to the text for more details.

WMT-14

WMT-16

FR-EN EN-FR DE-EN EN-DE

DE-EN EN-DE

Supervised

NMT (transformer)

-

WMT best

SMT (europarl)
+ w/o lexical reord.
+ constrained vocab.
+ unsup. tuning

41.8

35.8

30.82
30.33
30.10
29.46

26.22

-

29.0

20.83
20.37
19.91
17.75

17.43

28.4

20.6

16.60
16.34
16.32
15.45

14.08

35.0

30.61
30.54
30.04
29.32

25.87

-

40.2

26.38
25.99
25.66
23.35

23.05

-

34.2

22.12
22.20
21.53
19.86

18.23

Unsup.

Proposed system

Table 3: Results of the proposed method in comparison to supervised systems (BLEU). Transformer results re-
ported by Vaswani et al. (2017). SMT variants are incremental (e.g. 2nd includes 1st). Refer to the text for more
details.

point in both French-English directions, although
its contribution is somewhat weaker for German-
to-English (almost 1 BLEU point in WMT 2014
but only 0.2 in WMT 2016), and does not make
any difference for English-to-German.

The proposed iterative reﬁnement method has a
much stronger positive effect, with improvements
over 2.5 BLEU points in all cases, and up to 5
BLEU points in some. Most gains come in the
ﬁrst iteration, while the second iteration brings
weaker improvements and the algorithm seems to
converge in the third iteration, with marginal im-
provements for German-English and a small drop
in performance for French-English.

6.3 Comparison with supervised systems

So as to put our results into perspective, Table 3
comprises the results of different supervised meth-
ods in the same test sets. More concretely, we re-
port the results of the Transformer (Vaswani et al.,
2017), an NMT system based on self-attention that
is the current state-of-the-art in machine transla-
tion, along with the scores obtained by the best
performing system in each WMT shared task at

the time, and those of a standard phrase-based
SMT system trained on Europarl and tuned on
newstest2013 using Moses. We also report the ef-
fect of removing lexical reordering from the latter
as we do in our initial system (Section 4), restrict-
ing the vocabulary to the most frequent unigram,
bigram and trigrams as we do when training our
embeddings (Section 3), and using our unsuper-
vised tuning procedure over a subset of the mono-
lingual corpus (Section 4.2) instead of using stan-
dard MERT tuning over newstest2013.

Quite surprisingly, our proposed system, trained
exclusively on monolingual corpora, is relatively
close to a comparable phrase-based SMT sys-
tem trained on Europarl, with differences below
5 BLEU points in all cases and as little as 2.5 in
some. Note that both systems use the exact same
language model trained on News Crawl, making
them fully comparable in terms of the monolin-
gual corpora they have access to. While more
of a baseline than the state-of-the-art, note that
Moses+Europarl is widely used as a reference sys-
tem in machine translation. As such, we think
that our results are very encouraging, as they show

Source

Reference

Proposed system

D’autres révélations ont fait état de
documents divulgués par Snowden
selon lesquels la NSA avait intercepté
des données et des communications
émanant du téléphone portable de la
chancelière allemande Angela Merkel
et de ceux de 34 autres chefs d’État.

La NHTSA n’a pas pu examiner la let-
tre d’information aux propriétaires en
raison de l’arrêt de 16 jours des activ-
ités gouvernementales, ce qui a ralenti
la croissance des ventes de véhicules
en octobre.

Le M23 est né d’une mutinerie, en
avril 2012, d’anciens rebelles, essen-
tiellement tutsi, intégrés dans l’armée
en 2009 après un accord de paix.

revelations cited documents
Other
leaked by Snowden that
the NSA
monitored German Chancellor An-
gela Merkel’s cellphone and those of
up to 34 other world leaders.

NHTSA could not review the owner
notiﬁcation letter due to the 16-day
government shutdown, which tem-
pered auto sales growth in October.

Other disclosures have reported doc-
uments disclosed by Snowden sug-
gested the NSA had intercepted com-
munications and data from the mobile
phone of German Chancellor Angela
Merkel and those of 32 other heads of
state.

The NHTSA could not consider the
letter of information to owners be-
cause of halting 16-day government
activities, which slowed the growth in
vehicle sales in October.

The M23 was born of an April 2012
mutiny by former rebels, principally
Tutsis who were integrated into the
army in 2009 following a peace agree-
ment.

M23 began as a mutiny in April 2012,
former rebels, mainly Tutsi integrated
into the national army in 2009 after a
peace deal.

Tunks a déclaré au Sunday Telegraph
de Sydney que toute la famille était
«extrêmement préoccupée» du bien-
être de sa ﬁlle et voulait qu’elle rentre
en Australie.

Tunks told Sydney’s Sunday Tele-
graph the whole family was “ex-
tremely concerned” about his daugh-
ter’s welfare and wanted her back in
Australia.

Tunks told The Times of London from
Sydney that the whole family was “ex-
tremely concerned” of the welfare of
her daughter and wanted it to go in
Australia.

Table 4: Randomly chosen translation examples from French→English newstest2014.

that our fully unsupervised system is already quite
close to this competitive baseline.

In addition to that, the results for the constrained
variants of this SMT system justify some of the
simpliﬁcations required by our approach. In par-
ticular, removing lexical reordering and constrain-
ing the phrase table to the most frequent n-grams,
as we do for our initial system, has a relatively
small effect, with a drop of less than 1 BLEU point
in all cases, and as little as 0.28 in some. Replac-
ing standard MERT tuning with our unsupervised
variant does cause a considerable drop in perfor-
mance, although it is below 2.5 BLEU points even
in the worst case, and our unsupervised tuning
method is still better than using default weights as
reported in Table 2. This shows the importance of
tuning in SMT, suggesting that these results could
be further improved if one had access to a small
parallel corpus for tuning.

6.4 Qualitative results

Table 4 shows some of the translations produced
by the proposed system for French→English.
Note that these examples where randomly taken
from the test set, so they should be representative
of the general behavior of our approach.

While the examples reveal certain adequacy is-
sues (e.g. The Times of London from Sidney in-

stead of Sydney’s Sunday Telegraph), and the pro-
duced output is not perfectly grammatical (e.g.
go in Australia), our translations are overall quite
accurate and ﬂuent, and one could get a reason-
able understanding of the original text from them.
This suggests that unsupervised machine transla-
tion can indeed be a usable alternative in low re-
source settings.

7 Related work

Similar to our approach, statistical decipherment
also attempts to build machine translation sys-
tems from monolingual corpora. For that pur-
pose, existing methods treat the source language
as ciphertext, and model its generation through
a noisy channel model involving two steps:
the
generation of the original English sentence and
the probabilistic replacement of the words in it
(Ravi and Knight, 2011; Dou and Knight, 2012).
The English generative process is modeled us-
ing an n-gram language model, and the chan-
nel model parameters are estimated using either
expectation maximization or Bayesian inference.
Subsequent work has attempted to enrich these
models with additional information like syntactic
knowledge (Dou and Knight, 2013) and word em-
beddings (Dou et al., 2015). Nevertheless, these
systems work in a word-by-word basis and have

only been shown to work in limited settings, being
often evaluated in word-level translation. In con-
trast, our method builds a fully featured phrase-
based SMT system, and achieves competitive per-
formance in a standard machine translation task.

More recently, Artetxe et al. (2018c) and Lam-
ple et al. (2018) have managed to train a standard
attentional encoder-decoder NMT system from
monolingual corpora alone. For that purpose, they
use a shared encoder for both languages with pre-
trained cross-lingual embeddings, and train the
entire system using a combination of denoising,
backtranslation and, in the case of Lample et al.
(2018), adversarial training. This method was fur-
ther improved by Yang et al. (2018), who use a
separate encoder for each language, sharing only
a subset of their parameters, and incorporate two
generative adversarial networks. However, our re-
sults in Section 6.1 show that our SMT-based ap-
proach obtains substantially better results.

Our method is also connected to some previous
approaches to improve machine translation using
monolingual corpora. In particular, the generation
of a synthetic parallel corpus through backtransla-
tion (Sennrich et al., 2016), which is a key compo-
nent of our unsupervised tuning and iterative re-
ﬁnement procedures, has been previously used to
improve NMT. In addition, there have been sev-
eral proposals to extend the phrase table of SMT
systems by inducing translation candidates and/or
scores from monolingual corpora, using either sta-
tistical decipherment methods (Dou and Knight,
2012, 2013) or cross-lingual embeddings (Zhao
et al., 2015; Wang et al., 2016). While all these
methods exploit monolingual corpora to enhance
an existing machine translation system previously
trained on parallel corpora, our approach learns
a fully featured phrase-based SMT system from
monolingual corpora alone.

8 Conclusions and future work

In this paper, we propose a novel unsupervised
SMT system that can be trained on monolingual
corpora alone. For that purpose, we extend the
skip-gram model (Mikolov et al., 2013b) to si-
multaneously learn word and phrase embeddings,
and map them to a cross-lingual space adapting
previous unsupervised techniques (Artetxe et al.,
2018b). The resulting cross-lingual phrase embed-
dings are used to induce a phrase table, which cou-
pled with an n-gram language model and distance-

based distortion yields an unsupervised phrase-
based SMT system. We further improve results
tuning the weights with our unsupervised MERT
variant, and obtain additional improvements re-
training the entire system through iterative back-
translation. Our implementation is available as
an open source project at https://github.
com/artetxem/monoses.

Our experiments on standard WMT French-
English and German-English datasets conﬁrm the
effectiveness of our proposal, where we obtain im-
provements above 10 and 7 BLEU points over pre-
vious NMT-based approaches, respectively, clos-
ing the gap with supervised SMT (Moses trained
on Europarl) down to 2-5 points.

In the future, we would like to extend our ap-
proach to semi-supervised scenarios with small
parallel corpora, which we expect to be particu-
larly helpful for tuning purposes. Moreover, we
would like to try a hybrid approach with NMT,
using our unsupervised SMT system to generate
a synthetic parallel corpus and training an NMT
system over it through iterative backtranslation.

Acknowledgments

This research was partially supported by the
Spanish MINECO (TUNER TIN2015-65308-C5-
1-R, MUSTER PCIN-2015-226 and TADEEP
TIN2015-70214-P, cofunded by EU FEDER), the
UPV/EHU (excellence research group), and the
NVIDIA GPU grant program. Mikel Artetxe en-
joys a doctoral grant from the Spanish MECD.

References

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 451–462,
Vancouver, Canada. Association for Computational
Linguistics.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018a. Generalizing and improving bilingual word
embedding mappings with a multi-step framework
In Proceedings of the
of linear transformations.
Thirty-Second AAAI Conference on Artiﬁcial Intel-
ligence (AAAI-18), pages 5012–5019.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018b. A robust self-learning method for fully un-
supervised cross-lingual mappings of word embed-
dings. In Proceedings of the 56th Annual Meeting of

the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 789–798. Association
for Computational Linguistics.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018c. Unsupervised neural ma-
In Proceedings of the 6th Inter-
chine translation.
national Conference on Learning Representations
(ICLR 2018).

Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990. A
statistical approach to machine translation. Compu-
tational linguistics, 16(2):79–85.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Proceed-
ings of the 6th International Conference on Learning
Representations (ICLR 2018).

Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
266–275, Jeju Island, Korea. Association for Com-
putational Linguistics.

Qing Dou and Kevin Knight. 2013. Dependency-based
decipherment for resource-limited machine transla-
tion. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1668–1676, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.

Qing Dou, Ashish Vaswani, Kevin Knight, and Chris
Dyer. 2015. Unifying bayesian inference and vector
In Pro-
space models for improved decipherment.
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 836–
845, Beijing, China. Association for Computational
Linguistics.

Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
In Proceedings of the 2013
tion of ibm model 2.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644–648, Atlanta,
Georgia. Association for Computational Linguistics.

Kenneth Heaﬁeld,

Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modiﬁed
kneser-ney language model estimation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 690–696, Soﬁa, Bulgaria. Association
for Computational Linguistics.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
In Pro-
lenges for neural machine translation.
ceedings of the First Workshop on Neural Machine

Translation, pages 28–39, Vancouver. Association
for Computational Linguistics.

Statistical phrase-based translation.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018. Unsupervised
machine translation using monolingual corpora only.
In Proceedings of the 6th International Conference
on Learning Representations (ICLR 2018).

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Franz Josef Och. 2003. Minimum error rate train-
In Proceed-
ing in statistical machine translation.
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan. Association for Computational Linguis-
tics.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational linguistics, 29(1):19–51.

Sujith Ravi and Kevin Knight. 2011. Deciphering for-
In Proceedings of the 49th Annual
eign language.
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 12–
21, Portland, Oregon, USA. Association for Compu-
tational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation mod-
2016.
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Rui Wang, Hai Zhao, Sabine Ploux, Bao-Liang Lu, and
Masao Utiyama. 2016. A bilingual graph-based se-
mantic model for statistical machine translation. In
IJCAI, pages 2950–2956.

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu.
2018. Unsupervised neural machine translation with
weight sharing. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 46–55. As-
sociation for Computational Linguistics.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017a. Adversarial training for unsupervised
In Proceedings of the
bilingual lexicon induction.
55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1959–1970, Vancouver, Canada. Association
for Computational Linguistics.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Earth mover’s distance minimization
for unsupervised bilingual lexicon induction.
In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
1934–1945, Copenhagen, Denmark. Association for
Computational Linguistics.

Kai Zhao, Hany Hassan, and Michael Auli. 2015.
Learning translation models from monolingual con-
tinuous representations. In Proceedings of the 2015
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1527–1536, Denver,
Colorado. Association for Computational Linguis-
tics.

