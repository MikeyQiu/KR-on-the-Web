Precomputed Real-Time Texture Synthesis with
Markovian Generative Adversarial Networks

Chuan Li and Michael Wand

Institut for Informatik, University of Mainz, Germany

Abstract. This paper proposes Markovian Generative Adversarial Net-
works (MGANs), a method for training generative neural networks for
eﬃcient texture synthesis. While deep neural network approaches have
recently demonstrated remarkable results in terms of synthesis quality,
they still come at considerable computational costs (minutes of run-time
for low-res images). Our paper addresses this eﬃciency issue. Instead
of a numerical deconvolution in previous work, we precompute a feed-
forward, strided convolutional network that captures the feature statis-
tics of Markovian patches and is able to directly generate outputs of ar-
bitrary dimensions. Such network can directly decode brown noise to re-
alistic texture, or photos to artistic paintings. With adversarial training,
we obtain quality comparable to recent neural texture synthesis meth-
ods. As no optimization is required any longer at generation time, our
run-time performance (0.25M pixel images at 25Hz) surpasses previous
neural texture synthesizers by a signiﬁcant margin (at least 500 times
faster). We apply this idea to texture synthesis, style transfer, and video
stylization.

Keywords: Texture synthesis, Adversarial Generative Networks

6
1
0
2
 
r
p
A
 
5
1
 
 
]

V
C
.
s
c
[
 
 
1
v
2
8
3
4
0
.
4
0
6
1
:
v
i
X
r
a

1 Introduction

Image synthesis is a classical problem in computer graphics and vision [6,33].
The key challenges are to capture the structure of complex classes of images
in a concise, learnable model, and to ﬁnd eﬃcient algorithms for learning such
models and synthesizing new image data. Most traditional “texture synthesis”
methods address the complexity constraints using Markov random ﬁeld (MRF)
models that characterize images by statistics of local patches of pixels.

Recently, generative models based on deep neural networks have shown ex-
citing new perspectives for image synthesis [10,8,11]. Deep architectures capture
appearance variations in object classes beyond the abilities of pixel-level ap-
proaches. However, there are still strong limitations of how much structure can
be learned from limited training data. This currently leaves us with two main
classes of “deep” generative models: 1) full-image models that generate whole
images [10,3], and 2) Markovian models that also synthesize textures [8,21].

The ﬁrst class, full-image models, are often designed as specially trained
auto-encoders [16,11]. Results are impressive but limited to rather small images

2

Chuan Li and Michael Wand

(typically around 64×64 pixels) with limited ﬁdelity in details. The second class,
the deep Markovian models, capture the statistics of local patches only and
assemble them to high-resolution images. Consequently, the ﬁdelity of details is
good, but additional guidance is required if non-trivial global structure should
be reproduced [6,12,1,8,21]. Our paper addresses this second approach of deep
Markovian texture synthesis.

Previous neural methods of this type [8,21] are built upon a deconvolutional
framework [37,25]. This naturally provides blending of patches and permits
reusing the intricate, emergent multi-level feature representations of large, dis-
criminatively trained neural networks like the VGG network [30], repurposing
them for image synthesis. As a side note, we will later observe that this is actually
crucial for high-quality result (Figure 10). Gatys et al. [8] pioneer this approach
by modeling patch statistics with a global Gaussian models of the higher-level
feature vectors, and Li et al. [21] utilize dictionaries of extended local patches of
neural activation, trading-oﬀ ﬂexibility for visual realism.

Deep Markovian models are able to produce remarkable visual results, far be-
yond traditional pixel-level MRF methods. Unfortunately, the run-time costs of
the deconvolution approach are still very high, requiring iterative back-propagation
in order to estimate a pre-image (pixels) of the feature activations (higher net-
work layer). In the most expensive case of modeling MRFs of higher-level feature
patches [21], a high-end GPU needs several minutes to synthesize low-resolution
images (such as a 512-by-512 pixels image).

The objective of our paper is therefore to improve the eﬃciency of deep
Markovian texture synthesis. The key idea is to precompute the inversion of
the network by ﬁtting a strided1 convolutional network [31,29] to the inversion
process, which operates purely in a feed-forward fashion. Despite being trained
on patches of a ﬁxed size, the resulting network can generate continuous images
of arbitrary dimension without any additional optimization or blending, yielding
a high-quality texture synthesizer of a speciﬁc style and high performance2.

We train the convolutional network using adversarial training [29], which per-
mits maintaining image quality similar to the original, expensive optimization
approach. As result, we obtain signiﬁcant speed-up: Our GPU implementation
computes 512 × 512 images within 40ms (on an nVidia TitanX). The key limita-
tion, of course, is to precompute the feed-forward convolutional network for each
texture style. Nonetheless, this is still an attractive trade-oﬀ for many potential
applications, for example from the area of artistic image or video stylization. We
explore some of these applications in our experiments.

1 A strided convolutional network hence replaces pooling layers by subsampled convo-
lution ﬁlters that learn pooling during training (for example, two-fold mean pooling
is equivalent to blurred convolution kernels sampled at half resolution).

2 See supplementary material and code at: https://github.com/chuanli11/MGANs

Title Suppressed Due to Excessive Length

3

2 Related Work

Deconvolutional neural networks have been introduced to visualize deep features
and object classes. Zeiler et al. [37] back-project neural activations to pixels. Ma-
hendran et al. [23] reconstruct images from the neural encoding in intermediate
layers. Recently, eﬀort are made to improve the eﬃciency and accuracy of deep
visualization [36,26]. Mordvintsev et al. have raised wide attention by showing
how deconvolution of class-specifc activations can create hallucinogenic imagery
from discriminative networks [25]. The astonishing complexity of the obtained vi-
sual patterns has immediately spurred hope for new generative models: Gatys et
al. [8,7] drove deconvolution by global covariance statistics of feature vectors on
higher network layers, obtaining unprecedented results in artistic style transfer.
The statistical model has some limitations: Enforcing per-feature-vector statis-
tics permits a mixing of feature patterns that never appear in actual images
and limit plausibility of the learned texture. This can be partially addressed by
replacing point-wise feature statistics by statistics of spatial patches of feature
activations [21]. This permits photo-realistic synthesis in some cases, but also re-
duces invariance because the simplistic dictionary of patches introduces rigidity.
On the theory side, Xie et al. [34] have proved that a generative random ﬁeld
model can be derived from used discriminative networks, and show applications
to unguided texture synthesis.

Full image methods employ specially trained auto-encoders as generative
networks [16]. For example, the Generative Adversarial Networks (GANs) use
two networks, one as the discriminator and other as the generator, to iteratively
improve the model by playing a minimax game [10]. This model is extended
to work with a Laplacian pyramid [9], and with a conditional setting [3]. Very
recently, Radford et al. [29] propose a set of architectural reﬁnements3 that
stabilized the performance of this model, and show that the generators have
vector arithmetic properties. One important strength of adversarial networks is
that it oﬀers perceptual metrics [20,4] that allows auto-encoders to be training
more eﬃciently. These models can also be augmented semantic attributes [35],
image captions [24], 3D data [5,17], spatial/temporal status [11,13,27] etc.

In very recent, two concurrent work, Ulyanov et al. [32] and Johnson et
al. [14] propose fast implementations of Gatys et al’s approach. Both of their
methods employ precomputed decoders trained with a perceptual texture loss
and obtain signiﬁcant run-time beneﬁts (higher decoder complexity reduces their
speed-up a bit). The main conceptual diﬀerence in our paper is the use of Li et
al.’s [21] feature-patch statistics as opposed to learning Gaussian distributions of
individual feature vectors, which provides some beneﬁts in terms of reproducing
textures more faithfully.

3 strided convolution, ReLUs, batch normalization, removing fully connected layers

4

Chuan Li and Michael Wand

Fig. 1: Motivation: real world data does not always comply with a Gaussian
distribution (ﬁrst), but a complex nonlinear manifold (second). We adversarially
learn a mapping to project contextually related patches to that manifold.

3 Model

Let us ﬁrst conceptually motive our method. Statistics based methods [8,32]
match the distributions of source (input photo or noise signal) and target (tex-
ture) with a Gaussian model (Figure 1, ﬁrst). They do not further improve
the result once two distributions match. However, real world data does not al-
ways comply with a Gaussian distribution. For example, it can follow a compli-
cated non-linear manifold. Adversarial training [10] can recognize such manifold
with its discriminative network (Figure 1, second), and strengthen its generative
power with a projection on the manifold (Figure 1, third). We improve adver-
sarial training with contextually corresponding Markovian patches (Figure 1,
fourth). This allows the learning to focus on the mapping between diﬀerent de-
pictions of the same context, rather than the mixture of context and depictions.
Figure 2 visualizes our pipeline, which extends the patch-based synthesis
algorithm of Li et al. [21]. We ﬁrst replace their patch dictionary (including
the iterative nearest-neighbor search) with a continuous discriminative network
D (green blocks) that learns to distinguish actual feature patches (on VGG 19
layer Relu3 1, purple block) from inappropriately synthesized ones. A second
comparison (pipeline below D) with a VGG 19 encoding of the same image on
the higher, more abstract layer Relu5 1 can be optionally used for guidance. If we
run deconvolution on the VGG networks (from the discriminator and optionally
from the guidance content), we obtain deconvolutional image synthesizer, which
we call Markovian Deconvolutional Adversarial Networks (MDANs).

MDANs are still very slow; therefore, we aim for an additional generative
network G (blue blocks; a strided convolutional network). It takes a VGG 19
layer Relu4 1 encoding of an image and directly decodes it to pixels of the
synthesis image. During all of the training we do not change the VGG 19 network
(gray blocks), and only optimize D and G. Importantly, both D and G are
trained simultaneously to maximize the quality of G; D acts here as adversary
to G. We denote the overall architecture by Markovian Generative Adversarial
Networks (MGANs).

3.1 Markovian Deconvolutional Adversarial Networks (MDANs)

Our MDANs synthesize textures with a deconvolutional process that is driven
by adversarial training: a discriminative network D (green blocks in Figure 2)

Title Suppressed Due to Excessive Length

5

Fig. 2: Our model contains a generative network (blue blocks) and a discrimina-
tive network (green blocks). We apply the discriminative training on Markovian
neural patches (purple block as the input of the discriminative network.).

is trained to distinguish between “neural patches” sampled from the synthesis
image and sampled from the example image. We use regular sampling on layer
relu3 1 of VGG 19 output (purple block). It outputs a classiﬁcation score s = ±1
for each neural patch, indicating how “real” the patch is (with s = 1 being real).
For each patch sampled from the synthesized image, 1 − s is its texture loss to
minimize. The deconvolution process back-propagates this loss to pixels. Like
Radford et al. [29] we use batch normalization (BN) and leaky ReLU (LReLU)
to improve the training of D.

Formally, we denote the example texture image by xt ∈ Rwt×ht, and the
synthesized image by x ∈ Rw×h. We initialize x with random noise for un-guided
synthesis, or an content image xc ∈ Rw×h for guided synthesis. The deconvolutio
iteratively updates x so the following energy is minimized:

x = arg min

Et(Φ(x), Φ(xt)) + α1Ec(Φ(x), Φ(xc)) + α2Υ (x)

(1)

x

Here Et denotes the texture loss, in which Φ(x) is x’s feature map output from
layer relu3 1 of VGG 19. We sample patches from Φ(x), and compute Et as the
Hinge loss with their labels ﬁxed to one:

Et(Φ(x), Φ(xt)) =

max(0, 1 − 1 × si)

(2)

1
N

N
(cid:88)

i=1

Here si denotes the classiﬁcation score of i-th neural patch, and N is the total
number of sampled patches in Φ(x). The discriminative network is trained on
the ﬂy: Its parameters are randomly initialized, and then updated after each
deconvolution, so it becomes increasingly smarter as synthesis results improve.
The additional regularizer Υ (x) in Eq. 1 is a smoothness prior for pixels [23].
Using Et and Υ (x) can synthesize random textures (Figure 3). By minimizing
an additional content loss Ec, the network can generate an image that is contex-
tually related to a guidance image xc(Figure 4). This content loss is the Mean
Squared Error between two feature maps Φ(x) and Φ(xc). We set the weights
with α1 = 1 and α2 = 0.0001, and minimize Equation 1 using back-propagation

6

Chuan Li and Michael Wand

Fig. 3: Un-guided texture synthesis using MDANs. For each case the ﬁrst image
is the example texture, and the other two are the synthesis results. Image credits:
[34]’s “Ivy”, ﬂickr user erwin brevis’s “gell”, Katsushika Hokusai’s “The Great
Wave oﬀ Kanagawa”, Kandinsky’s “Composition VII”.

Fig. 4: Guided texture synthesis using MDANs. The reference textures are the
same as in Figure 3.

with ADAM [15] (learning rate 0.02, momentum 0.5). Notice each neural patch
receives its own output gradient through the back-propagation of D. In order
to have a coherent transition between adjacent patches, we blend their output
gradient like texture optimization [18] did.

3.2 Markovian Generative Adversarial Networks (MGANs)

MDANs require many iterations and a separate run for each output image. We
now train a variational auto-encoder (VAE) that decodes a feature map directly
to pixels. The target examples (textured photos) are obtained from the MDANs.
Our generator G (blue blocks in Figure 2) takes the layer relu4 1 of VGG 19
as the input, and decodes a picture through a ordinary convolution followed by
a cascade of fractional-strided convolutions (FS Conv). Although being trained
with ﬁxed size input, the generator naturally extends to arbitrary size images.
As Dosovitskiy et al. [4] point out, it is crucially important to ﬁnd a good
metric for training an auto-encoder: Using the Euclidean distance between the
synthesized image and the target image at the pixel level (Figure 5, pixel VAE)
yields an over-smoothed image. Comparing at the neural encoding level improves
results (Figure 5, neural VAE), and adversarial training improves the reproduc-
tion of the intended style further (Figure 5, MGANs).

Our approach is similar to classical Generative Adversarial Networks (GANs)
[10], with the key diﬀerence of not operating on full images, but neural patches

Title Suppressed Due to Excessive Length

7

Fig. 5: Our MGANs learn a mapping from VGG 19 encoding of the input photo
to the stylized example (MDANs). The reference style texture for MDANs is
Pablo Picasso’s “self portrait 1907”. We compare the results of MGANs to Pixel
VAE and Neural VAE in with both training and testing data.

from the same image. Doing so utilizes the contextual correspondence between
the patches, and makes learning easier and more eﬀective in contrast to learn-
ing the distribution of a object class [10] or a mapping between contextually
irrelevant data [32]. In additional we also replace the Sigmoid function and the
binary cross entropy criteria from [29] by a max margin criteria (Hinge loss).
This avoids the vanishing gradient problem when learning D. This is more prob-
lematic in our case than in Radfort et al.’s [29] because of less diversity in our
training data. Thus, the Sigmoid function can be easily saturated.

Figure 5 (MGANs) shows the results of a network that is trained to produce
paintings in the style of Picasso’s “Self-portrait 1907”. For training, we randomly
selected 75 faces photos from the CelebA data set [22], and in additional to it
25 non-celebrity photos from the public domain. We resize all photos so that the
maximum dimension is 384 pixels. We augmented the training data by generating
9 copies of each photo with diﬀerent rotations and scales. We regularly sample
subwindows of 128-by-128 croppings from them for batch processing. In total we
have 24,506 training examples, each is treated as a training image where neural
patches are sampled from its relu3 1 encoding as the input of D.

Figure 5 (top row, MGANs) shows the decoding result of our generative
network for a training photo. The bottom row shows the network generalizes well
to test data. Notice the MDANs image for the test image is never used in the
training. Nonetheless, direct decoding with G produces very good approximation
of it. The main diﬀerence between MDANs and MGANs is: MDANs preserve
the content of the input photo better and MGANs produce results that are
more stylized. This is because MGANs was trained with many images, hence
learned the most frequent features. Another noticeable diﬀerence is MDANs

8

Chuan Li and Michael Wand

Fig. 6: Intermediate decoding results during the training of MGANs. The refer-
ence style texture for MDANs is Pablo Picasso’s “self portrait 1907”.

Fig. 7: Visualizing the learned features in the generative networks. Image cred-
its: [34]’s “Ivy”, ﬂickr user erwin brevis’s “gell”, Katsushika Hokusai’s “The
Great Wave oﬀ Kanagawa”.

create more natural backgrounds (such as regions with ﬂat color), due to its
iterative reﬁnement. Despite such ﬂaws, the MGANs model produces comparable
results with a speed that is 25,000 times faster.

Figure 6 shows some intermediate results MGANs. It is clear that the decoder
gets better with more training. After 100 batches, the network is able to learn
the overall color, and where the regions of strong contrast are. After 300 batches
the network started to produce textures for brush strokes. After 1000 batches it
learns how to paint eyes. Further training is able to remove some of the ghosting
artifacts in the results. Notice the model generalizes well to testing data (right).

4 Experimental Analysis

We conduct empirical experiments with our model: we study parameter inﬂuence
(layers for classiﬁcation, patch size) and the complexity of the model (number
of layers in the network, number of channels in each layer). While there may not
be a universal optimal design for all textures, our study shed some light on how

Title Suppressed Due to Excessive Length

9

Fig. 8: Diﬀerent layers and patch sizes for training the discriminative netowrk.
Input image credit: “ropenet” from the project link of [19].

the model behaves for diﬀerent cases. For fair comparison, we scale the example
textures in this study to ﬁxed size (128-by-128 pixels), and demand the synthesis
output to be 256-by-256 pixels.

Visualizing decoder features: We visualize the learned ﬁlters of decoder
G in Figure 7. These features are directly decoded from a one-hot input vector.
Individual patches are similar to, but not very faithfully matching the exam-
ple textures (reconﬁrming the semi-distributed and non-linear nature of the en-
coding). Nonetheless, visual similarity of such artiﬁcial responses seems strong
enough for synthesizing new images.

Parameters: Next, we study the inﬂuence of changing the input layers for
the discriminative network. To do so we run unguided texture synthesis with
discriminator D taking layer relu2 1, relu3 1, and relu4 1 of VGG 19 as the
input. We use patch sizes of 16, 8 and 4 respectively for the three options, so
they have the same receptive ﬁeld of 32 image pixels (approximately; ignoring
padding). The ﬁrst three results in Fig. 8 shows the results of these three settings.
Lower layers (relu2 1 ) produce sharper appearances but at the cost of losing form
and structure of the texture. Higher layer (relu4 1 ) preserves coarse structure
better (such as regularity) but at the risk of being too rigid for guided scenarios.
Layer relu3 1 oﬀers a good balance between quality and ﬂexibility. We then show
the inﬂuence of patch size: We ﬁx the input layer of D to be relu3 1, and compare
patch size of 4 and 16 to with the default setting of 8. The last two results in
Fig. 8 shows that such change will also aﬀect the rigidity of the model: smaller
patches increase the ﬂexibility and larger patches preserve better structure.

Complexity: We now study the inﬂuence of 1) the number of layers in the
networks and 2) the number of channels in each layer. We ﬁrst vary the D by
removing the convolutional layer. Doing so reduces the depth of the network
and in consequence the synthesis quality (ﬁrst column, Fig. 9). Bringing this
convolutional layer back produces smoother synthesis (second column, Fig. 9).
However, in these examples the quality does not obviously improves with more
additional layers (third column, Fig. 9).

Testing the D with 4, 64, and 128 channels for the convolutional layer, we
observe in general that decreasing the number of channels leads to worse re-
sults (fourth column, Fig. 9), but there is no signiﬁcance diﬀerence between 64
channels and 128 channels (second column v.s. ﬁfth column). The complexity
requirements also depend on the actual texture. For example, the ivy texture is

10

Chuan Li and Michael Wand

Fig. 9: Diﬀerent depths for training the discriminative netowrk. The input tex-
tures are “ropenet” from the project link of [19], [34]’s “Ivy”, and Pablo Picasso’s
“self portrait 1907”.

a rather simple MRF, so the diﬀerence between 4 channel and 64 channel are
marginal, unlike in the other two cases.

Next, we ﬁx the discriminative network and vary the complexity of the gener-
ative network. We notice some quality loss when removing the ﬁrst convolutional
layer from the decoder, or reducing the number of channels for all layers, and
only very limited improvement from a more complex design. However the dif-
ference is not very signiﬁcant. This is likely because the networks are all driven
by the same discriminative network, and the reluctance of further improvement
indicates there are some non-trivial information from the deconvolutional pro-
cess that can not be recovered by a feed forward process. In particular, the
fractionally-strided convolutions does not model the nonlinear behaviour of the
max-pooling layer, hence often produces alias patterns. These become visible in
homogeneous, texture-less area. To avoid artifacts but encourage texture vari-
ability, we can optionally add Perlin noise [28] to the input image.

Initialization Usually, networks are initialized with random values. However
we found D has certain generalization ability. Thus, for transferring the same
texture to diﬀerent images with MDANs, a previously trained network can serve
as initialization. Figure 10 shows initialization with pre-trained discriminative
network (that has already transferred 50 face images) produces good result with
only 50 iterations. In comparison, random initialization does not produce com-
parable quality even after the ﬁrst 500 iterations. It is useful to initialize G with
an auto-encoder that directly decodes the input feature to the original input

Title Suppressed Due to Excessive Length

11

Fig. 10: Diﬀerent initializations of the discriminative networks. The reference
texture is Pablo Picasso’s “self portrait 1907”.

photo. Doing so essentially approximates the process of inverting VGG 19, and
let the whole adversarial network to be trained more stably.

The role of VGG: We also validate the importance of the pre-trained
VGG 19 network. As the last two pictures in Figure 10 show, training a discrim-
inative network from scratch (from pixel to class label [29]) yields signiﬁcantly
worse results. This has also been observed by Ulyanov et al. [32]. Our explana-
tion is that much of the statistical power of VGG 19 stems from building shared
feature cascades for a diverse set of images, thereby approaching human visual
perception more closely than a network trained with a limited example set.

5 Results

This section shows examples of our MGANs synthesis results. We train each
model with 100 randomly selected images from ImageNet, and a single example
texture. We ﬁrst produce 100 transferred images using the MDANs model, then
regularly sample 128-by-128 image croppings as training data for MGANs. In
total we have around 16k samples for each model. The training take as about 12
min per epoch. Each epoch min-batches through all samples in random order.
We train each texture for upto ﬁve epochs.

Figure 11 compares our synthesis results with previous methods. First, our
method has a very diﬀerent character in comparison to the methods that use
global statistics [32,8]: It transfers texture more coherently, such as the hair
of Lena was consistently mapped to dark textures. In contrast, the Gaussian
model
[32,8] failed to keep such consistency, and have diﬃculty in transferring
complicated image content. For example the eyes in [32]’s result and the entire
face in [8]’s result are not textured. Since these features do not ﬁt a Gaussian
distribution, they are diﬃcult to be constrained by a Gram matrix. The other
local patch based approach [21] produces the most coherent synthesis, due to the
use of non-parametric sampling. However, their method requires patch match-
ing so is signiﬁcantly slower (generate this 384-by-384 picture in 110 seconds).
Our method and Ulyanov et al. [32] run at the same level of speed; both bring
signiﬁcantly improvement of speed over Gatys et al. [8] (500 times faster) and
Li et al. [21] (5000 times faster).

12

Chuan Li and Michael Wand

Fig. 11: Comparisons with previous methods. See more examples in our supple-
mentary report. Results of Ulyanov et al. [32], Gatys et al. [8] and input images
are from [32].

Fig. 12: More comparisons with Texture Networks [32]. Results of [32] and input
images are from [32].

Figure 12 further discuss the diﬀerence between the Gaussian based method [32]

and our method4. In general [32] produces more faithful color distributions in
respect to the style image. It also texture background better (see the starry night
example) due to the learning of mapping from noise to Gaussian distribution. On
the other hand, our method produces more coherent texture transfer and does
not suﬀer the incapability of Gaussian model for more complex scenarios, such
as the facade in both examples. In comparison [32] produces either too much or
too little textures in such complex regions.

Figure 13 shows that unguided texture synthesis is possible by using the
trained model to decode noise input. In this case, Perlin noise5 images are

4 Since Ulyanov et al. [32] and Johnson et al. [14] are very similar approaches, in this
paper we only compare to one of them [32]. The main diﬀerences of [14] are: 1) using
a residual architecture instead of concatenating the outputs from diﬀerent layers; 2)
no additional noise in the decoding process.

5 We need to use “brown” noise with spectrum decaying to the higher frequencies
because ﬂat “white” noise creates an almost ﬂat response in the encoding of the
VGG network. Somer lower-frequency structure is required to trigger the feature
detectors in the discriminative network.

Title Suppressed Due to Excessive Length

13

Fig. 13: Generate random textures by decoding from Brown noise.

Fig. 14: Decoding a 1080-by-810 video. We achieved the speed of 8Hz. Input
video is credited to ﬂickr user macro antonio torres.

forwarded through VGG 19 to generate feature maps for the decoder. To our
surprise, the model that was trained with random ImageNet images is able to
decode such features maps to plausible textures. This again shows the general-
ization ability of our model. Last, Figure 13 shows our video decoding result.
As a feed-forward process our method is not only faster but also relatively more
temporally coherent than the deconvolutional methods.

Fig. 15: Left: speed comparison between our method and Ulyanov et al. [32].
Right: speed comparison (in log space) between our method and Gatys et al. [8],
Li et al. [21], and Ulyanov et al. [32]. The feed-forward methods (ours and [32])
are signiﬁcantly faster than Gatys et al. [8] (500 times speed up) and Li et al. [21]
(5000 times speed up).

Last but not the least, we provide details for the time/memory usage of our
method. The time measurement is based on a standard benchmark framework [2]
(Figure 15): Our speed is at the same level as the concurrent work by Ulyanov et
al. [32], who also use a feed-forward approach, perform signiﬁcantly faster than
previous deconvolution based approaches [8,21]. More precisely, both our method

14

Chuan Li and Michael Wand

and Ulyanov et al. [32] are able to decode 512-by-512 images at 25Hz (Figure 15,
left), while [32] leads the race by a very small margin. The time cost of both
methods scale linearly with the number of pixels in the image. For example,
our method cost 10 ms for a 256-by-256 image, 40 ms for a 512-by-512 image,
and 160 ms for a 1024-by-1024 image. Both methods show a very signiﬁcant
improvement in speed over previous deconvolutional methods such as Gatys et
al. [8] and Li et al. [21] (Figure 15 right): about 500 times faster than Gatys et
al. [8], and 5000 times faster than Li et al. [21]. In the meantime our method is
also faster than most traditional pixel based texture synthesizers (which rely on
expensive nearest-neighbor searching). A possible exceptions would be a GPU
implementation of “Patch Match” [1], which could run at comparable speed.
However, it provides the quality beneﬁts (better blending, invariance) of a deep-
neural-network method (as established in previous work [8,21]).

Memory-wise, our generative model takes 70 Mb memory for its parame-
ters(including the VGG network till layer Relu4 1). At runtime, the required
memory to decode a image linearly depends on the image’s size: for a 256-by-
256 picture it takes about 600 Mb, and for a 512-by-512 picture it requires about
2.5 Gb memory. Notice memory usage can be reduced by subdividing the input
photo into blocks and run the decoding in a scanline fashion. However, we do
not further explore the optimization of memory usage in this paper.

6 Limitation

Our current method works less well with non-texture data. For example, it failed
to transfer facial features between two diﬀerence face photos. This is because fa-
cial features can not be treated as textures, and need semantic understanding
(such as expression, pose, gender etc.). A possible solution is to couple our model
with the learning of object class [29] so the local statistics is better conditioned.
For synthesizing photo-realistic textures, Li et al. [21] often produces better re-
sults due to its non-parametric sampling that prohibits data distortion. However,
the rigidity of their model restricts its application domain. Our method works
better with deformable textures, and runs signiﬁcantly faster.

Our model has a very diﬀerent character compared to Gaussian based mod-
els [8,32]. By capturing a global feature distribution, these other methods are
able to better preserve the global “look and feels” of the example texture. In
contrast, our model may deviate from the example texture in, for example, the
global color distribution. However, such deviation may not always be bad when
the content image is expected to play a more important role.

Since our model learns the mapping between diﬀerent depictions of the same
content, it requires features highly invariant features. For this reason we use the
pre-trained VGG 19 network. This makes our method weaker in dealing with
highly stationary backgrounds (sky, out of focus region etc.) due to their weak
activation from VGG 19. We observed that in general statistics based meth-
ods [32,8] generate better textures for areas that has weak content, and our

Title Suppressed Due to Excessive Length

15

method works better for areas that consist of recognizable features. We believe
it is valuable future work to combine the strength of both methods.

Finally, we discuss the noticeable diﬀerence between the results of MDANs
and MGANs. The output of MGANs is often more consistent with the example
texture, this shows MGANs’ strength of learning from big data. MGANs has
weakness in ﬂat regions due to the lack of iterative optimization. More sophis-
ticated architectures such as the recurrent neural networks can bring in state
information that may improve the result.

7 Conclusion

The key insight of this paper is that adversarial generative networks can be
applied in a Markovian setting to learn the mapping between diﬀerent depictions
of the same content. We develop a fully generative model that is trained from
a single texture example and randomly selected images from ImageNet. Once
trained, our model can decode brown noise to realistic texture, or photos into
artworks. We show our model has certain advantages over the statistics based
methods [32,8] in preserving coherent texture for complex image content. Once
trained (which takes about an hour per example), synthesis is extremely fast
and oﬀers very attractive invariance for style transfer.

Our method is only one step in the direction of learning generative models
for images. An important avenue for future work would be to study the broader
framework in a big-data scenario to learn not only Markovian models but also
include coarse-scale structure models. This additional invariance to image lay-
out could, as a side eﬀect, open up ways to also use more training data for the
Markovian model, thus permitting more complex decoders with stronger gen-
eralization capability over larger classes. The ultimate goal would be a directly
decoding, generative image model of large classes of real-world images.

This work has been partially supported by the Intel Visual Computing Insti-
tute and the Center for Computational Science Mainz. We like to thank Bertil
Schmidt and Christian Hundt for providing additional computational resources.

Acknowledgments

References

1. Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: A ran-
domized correspondence algorithm for structural image editing. Siggrah pp. 24:1–
24:11 (2009) 2, 14

2. Chintala, S.: Easy benchmarking of all publicly accessible implementations of con-

vnets. https://github.com/soumith/convnet-benchmarks (2015) 13

3. Denton, E.L., Fergus, R., Szlam, A., Chintala, S.: Deep generative image models

using a laplacian pyramid of adversarial networks. In: NIPS (2015) 1, 3

16

Chuan Li and Michael Wand

4. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics
based on deep networks. CoRR abs/1602.02644 (2016), http://arxiv.org/abs/1602.
02644 3, 6

5. Dosovitskiy, A., Springenberg, J.T., Brox, T.: Learning to generate chairs with
convolutional neural networks. CoRR abs/1411.5928 (2014), http://arxiv.org/abs/
1411.5928 3

6. Efros, A.A., Freeman, W.T.: Image quilting for texture synthesis and transfer. In:

Siggraph. pp. 341–346 (2001) 1, 2

7. Gatys, L.A., Ecker, A.S., Bethge, M.: Texture synthesis and the controlled genera-
tion of natural stimuli using convolutional neural networks. In: NIPS (May 2015),
http://arxiv.org/abs/1505.07376 3

8. Gatys, L.A., Ecker, A.S., Bethge, M.: A neural algorithm of artistic style (2015),
arXiv preprint; http://arxiv.org/abs/1508.06576 1, 2, 3, 4, 11, 12, 13, 14, 15
9. Gauthier, J.: Conditional generative adversarial nets for convolutional face gener-
ation. http://www.foldl.me/2015/conditional-gans-face-generation/ (2015) 3
10. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. pp. 2672–2680
(2014) 1, 3, 4, 6, 7

11. Gregor, K., Danihelka, I., Graves, A., Wierstra, D.: DRAW: A recurrent neural
network for image generation. CoRR abs/1502.04623 (2015), http://arxiv.org/abs/
1502.04623 1, 3

12. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-

gies. In: Siggraph. pp. 327–340 (2001) 2

13. Im, D.J., Kim, C.D., Jiang, H., Memisevic, R.: Generating images with recur-
rent adversarial networks. CoRR abs/1602.05110 (2016), http://arxiv.org/abs/
1602.05110 3

14. Johnson, J., Alahi, A., Li, F.F.: Perceptual losses for real-time style transfer and
super-resolution. CoRR abs/1603.08155 (March, 2016), http://arxiv.org/abs/1603.
08155v1 3, 12

15. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR

abs/1412.6980 (2014), http://arxiv.org/abs/1412.6980 6

16. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. CoRR abs/1312.6114

(2013), http://arxiv.org/abs/1312.6114 1, 3

17. Kulkarni, T.D., Whitney, W., Kohli, P., Tenenbaum, J.B.: Deep convolutional in-
verse graphics network. CoRR abs/1503.03167 (2015), http://arxiv.org/abs/1503.
03167 3

18. Kwatra, V., Essa, I., Bobick, A., Kwatra, N.: Texture optimization for example-

based synthesis. Siggraph 24(3), 795–802 (2005) 6

19. Kwatra, V., Sch¨odl, A., Essa, I., Turk, G., Bobick, A.: Graphcut textures: Image
and video synthesis using graph cuts. ACM Trans. Graph. 22(3), 277–286 (Jul
2003) 9, 10

20. Larsen, A.B.L., Sønderby, S.K., Winther, O.: Autoencoding beyond pixels using
a learned similarity metric. CoRR abs/1512.09300 (2015), http://arxiv.org/abs/
1512.09300 3

21. Li, C., Wand, M.: Combining markov random ﬁelds and convolutional neural net-
works for image synthesis. CoRR abs/1601.04589 (2016), http://arxiv.org/abs/
1601.04589 1, 2, 3, 4, 11, 13, 14

22. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:

23. Mahendran, A., Vedaldi, A.: Understanding deep image representations by invert-

ICCV (2015) 7

ing them. In: CVPR (2015) 3, 5

Title Suppressed Due to Excessive Length

17

24. Mansimov, E., Parisotto, E., Ba, L.J., Salakhutdinov, R.: Generating images from
captions with attention. CoRR abs/1511.02793 (2015), http://arxiv.org/abs/1511.
02793 3

25. Mordvintsev, A., Olah, C., Tyka, M.:

into
inceptionism-going-deeper-into-neural.html (2015) 2, 3

networks.

neural

deeper
http://googleresearch.blogspot.com/2015/06/

Inceptionism: Going

26. Nguyen, A.M., Yosinski, J., Clune, J.: Multifaceted feature visualization: Uncover-
ing the diﬀerent types of features learned by each neuron in deep neural networks.
CoRR abs/1602.03616 (2016), http://arxiv.org/abs/1602.03616 3

27. Oord, A.V.D., Kalchbrenner, N., Kavukcuoglu, K.: Pixel recurrent neural networks.

CoRR abs/1601.06759 (2016), http://arxiv.org/abs/1601.06759 3

28. Perlin, K.: An image synthesizer. SIGGRAPH 19(3), 287–296 (1985) 10
29. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with
deep convolutional generative adversarial networks. CoRR abs/1511.06434 (2015),
http://arxiv.org/abs/1511.06434 2, 3, 5, 7, 11, 14

30. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. CoRR (2014), http://arxiv.org/abs/1409.1556 2

31. Springenberg, J., Dosovitskiy, A., Brox, T., Riedmiller, M.: Striving for sim-
plicity: The all convolutional net (2015), http://lmb.informatik.uni-freiburg.de/
Publications/2015/DB15a 2

32. Ulyanov, D., Lebedev, V., Vedaldi, A., Lempitsky, V.: Texture networks: Feed-
forward synthesis of textures and stylized images. CoRR abs/1603.03417 (March,
2016), http://arxiv.org/abs/1603.03417v1 3, 4, 7, 11, 12, 13, 14, 15

33. Wei, L.Y., Levoy, M.: Fast texture synthesis using tree-structured vector quanti-

zation. In: Siggraph. pp. 479–488 (2000) 1

34. Xie, J., Lu, Y., Zhu, S.C., Wu, Y.N.: A theory of generative convnet. CoRR

arXiv:1602.03264 (2016), http://arxiv.org/abs/1602.03264 3, 6, 8, 10

35. Yan, X., Yang, J., Sohn, K., Lee, H.: Attribute2image: Conditional image gener-
ation from visual attributes. CoRR abs/1512.00570 (2015), http://arxiv.org/abs/
1512.00570 3

36. Yosinski, J., Clune, J., Nguyen, A.M., Fuchs, T., Lipson, H.: Understanding neural
networks through deep visualization. CoRR abs/1506.06579 (2015), http://arxiv.
org/abs/1506.06579 3

37. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.

In: ECCV. pp. 818–833 (2014) 2, 3

