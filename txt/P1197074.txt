8
1
0
2
 
r
a

M
 
0
3
 
 
]
L
C
.
s
c
[
 
 
3
v
5
6
4
1
0
.
3
0
8
1
:
v
i
X
r
a

Query and Output: Generating Words by Querying Distributed Word
Representations for Paraphrase Generation

Shuming Ma1, Xu Sun1,2, Wei Li1, Sujian Li1, Wenjie Li3, Xuancheng Ren1
1MOE Key Lab of Computational Linguistics, School of EECS, Peking University
2Deep Learning Lab, Beijing Institute of Big Data Research, Peking University
3Department of Computing, The Hong Kong Polytechnic University
{shumingma, xusun, liweitj47, lisujian, renxc}@pku.edu.cn
cswjli@comp.polyu.edu.hk

Abstract

Most recent approaches use the sequence-
to-sequence model for paraphrase genera-
The existing sequence-to-sequence
tion.
model tends to memorize the words and the
patterns in the training dataset instead of learn-
ing the meaning of the words. Therefore,
the generated sentences are often grammati-
cally correct but semantically improper. In this
work, we introduce a novel model based on
the encoder-decoder framework, called Word
Embedding Attention Network (WEAN). Our
proposed model generates the words by query-
ing distributed word representations (i.e. neu-
ral word embeddings), hoping to capturing the
meaning of the according words. Following
previous work, we evaluate our model on two
paraphrase-oriented tasks, namely text sim-
pliﬁcation and short text abstractive summa-
rization. Experimental results show that our
model outperforms the sequence-to-sequence
baseline by the BLEU score of 6.3 and 5.5
on two English text simpliﬁcation datasets,
and the ROUGE-2 F1 score of 5.7 on a Chi-
nese summarization dataset. Moreover, our
model achieves state-of-the-art performances
on these three benchmark datasets.1

1 Introduction

Paraphrase is a restatement of the meaning of a
text using other words. Many natural language
generation tasks are paraphrase-orientated, such
as text simpliﬁcation and short text summariza-
tion. Text simpliﬁcation is to make the text easier
to read and understand, especially for poor read-
ers, while short text summarization is to generate a
brief sentence to describe the short texts (e.g. posts
on the social media). Most recent approaches use
sequence-to-sequence model for paraphrase gen-
eration (Prakash et al., 2016; Cao et al., 2017). It

1The

code

is

available

at

https://github.com/lancopku/WEAN

compresses the source text information into dense
vectors with the neural encoder, and the neural
decoder generates the target text using the com-
pressed vectors.

Although neural network models achieve suc-
cess in paraphrase generation, there are still two
major problems. One of the problem is that the ex-
isting sequence-to-sequence model tends to mem-
orize the words and the patterns in the training
dataset instead of the meaning of the words. The
main reason is that the word generator (i.e.
the
output layer of the decoder) does not model the
semantic information. The word generator, which
consists of a linear transformation and a softmax
operation, converts the Recurrent Neural Network
(RNN) output from a small dimension (e.g. 500)
to a much larger dimension (e.g. 50,000 words
in the vocabulary), where each dimension repre-
sents the score of each word. The latent assump-
tion of the word generator is that each word is in-
dependent and the score is irrelevant to each other.
Therefore, the scores of a word and its synonyms
may be of great difference, which means the word
generator learns the word itself rather than the re-
lationship between words.

The other problem is that the word generator
has a huge number of parameters. Suppose we
have a sequence-to-sequence model with a hid-
den size of 500 and a vocabulary size of 50,000.
The word generator has up to 25 million parame-
ters, which is even larger than other parts of the
encoder-decoder model in total. The huge size
of parameters will result in slow convergence, be-
cause there are a lot of parameters to be learned.
Moreover, under the distributed framework, the
more parameters a model has, the more bandwidth
and memory it consumes.

To tackle both of the problems, we propose a
novel model called Word Embedding Attention
Network (WEAN). The word generator of WEAN

is attention based, instead of the simple linear soft-
max operation. In our attention based word gen-
erator, the RNN output is a query, the candidate
words are the values, and the corresponding word
In order to predict
representations are the keys.
the word, the attention mechanism is used to se-
lect the value matching the query most, by means
of querying the keys. In this way, our model gen-
erates the words according to the distributed word
representations (i.e. neural word embeddings) in
a retrieval style rather than the traditional gener-
ative style. Our model is able to capture the se-
mantic meaning of a word by referring to its em-
bedding. Besides, the attention mechanism has
a much smaller number of parameters compared
with the linear transformation directly from the
RNN output space to the vocabulary space. The
reduction of the parameters can increase the con-
vergence rate and speed up the training process.
Moreover, the word embedding is updated from
three sources: the input of the encoder, the input
of the decoder, and the query of the output layer.

Following previous work (Cao et al., 2017), we
evaluate our model on two paraphrase-oriented
tasks, namely text simpliﬁcation and short text
abstractive summarization. Experimental results
show that our model outperforms the sequence-to-
sequence baseline by the BLEU score of 6.3 and
5.5 on two English text simpliﬁcation datasets, and
the ROUGE-2 F1 score of 5.7 on a Chinese sum-
marization dataset. Moreover, our model achieves
state-of-the-art performances on all of the bench-
mark datasets.

2 Proposed Model

We propose a novel model based on the encoder-
decoder framework, which generates the words
by querying distributed word representations with
the attention mechanism. In this section, we ﬁrst
present the overview of the model architecture.
Then, we explain the details of the word gener-
ation, especially the way to query word embed-
dings.

2.1 Overview

Word Embedding Attention Network is based on
the encoder-decoder framework, which consists of
two components: a source text encoder, and a tar-
get text decoder. Figure 1 is an illustration of our
model. Given the source texts, the encoder com-
presses the source texts into dense representation

vectors, and the decoder generates the paraphrased
texts. To predict a word, the decoder uses the hid-
den output to query the word embeddings. The
word embeddings assess all the candidate words,
and return the word whose embedding matches the
query most. The selected word is emitted as the
predicted token, and its embedding is then used as
the input of the LSTM at the next time step. After
the back propagation, the word embedding is up-
dated from three sources: the input of the encoder,
the input of the decoder, and the query of the out-
put layer. We show the details of our WEAN in
the following subsection.

2.2 Encoder and Decoder

The goal of the source text encoder is to pro-
vide a series of dense representation of complex
source texts for the decoder.
In our model, the
source text encoder is a Long Short-term Memory
Network (LSTM), which produces the dense rep-
resentation {h1, h2, ..., hN } from the source text
{x1, x2, ..., xN }:

The goal of the target text decoder is to generate
a series of paraphrased words from the dense rep-
resentation of source texts. Fisrt, the LSTM of the
decoder compute the dense representation of gen-
erated words st. Then, the dense representations
are fed into an attention layer (Bahdanau et al.,
2014) to generate the context vector ct, which cap-
tures context information of source texts. Atten-
tion vector ct is calculated by the weighted sum of
encoder hidden states:

ct =

αtihi

N

X
i=1

αti =

eg(st,hi)
N
j=1 eg(st,hj)

P

(1)

(2)

where g(st, hi) is an attentive score between the
decoder hidden state st and the encoder hidden
state hi.

In this way, ct and st respectively represent the
context information of source texts and the target
texts at the tth time step.

2.3 Word Generation by Querying Word

Embedding

For the current sequence-to-sequence model, the
word generator computes the distribution of output
words yt in a generative style:

p(yt) = sof tmax(W st)

(3)

{ hard,

}

Admission

very

hard

Key

...

(cid:1871)(cid:2869)

(cid:1871)(cid:3047)(cid:2879)(cid:2869)

(cid:1871)(cid:3047)

(cid:1855)(cid:3047)

Value

easy

...

happy

...

hard

...

competitive

Query

(cid:1860)(cid:2869)
Admission

(cid:1860)(cid:2870)
is

(cid:1860)(cid:2871)
extremely

(cid:1860)(cid:2872)
competitive

Figure 1: An overview of Word Embedding Attention Network.

where W ∈ Rk×V is a trainable parameter matrix,
k is hidden size, and V is the number of words in
the vocabulary. When the vocabulary is large, the
number of parameters will be huge.

Our model generates the words in a retrieval
style rather than the traditional generative style,
by querying the word embeddings. We denote the
combination of the source context vector ct and
the target context vector st as the query qt:

In implementation, we select the general attention
function as the relevance score function, based on
the performance on the validation sets. The key-
value pair with the highest score {wt, et} is se-
lected. At the test stage, the decoder generates the
key wt as the tth predicted word, and inputs the
value et to the LSTM unit at the t + 1th time step.
At the training stage, the scores are normalized as
the word probability distribution:

qt = tanh(Wc[st; ct])

(4)

p(yt) = sof tmax(f (qt, ei))

(6)

The candidate words wi and their corresponding
embeddings ei are paired as the key-value pairs
{wi, ei}(i = 1, 2, ..., n), where n is the number of
candidate words. We give the details of how to de-
termine the set of candidate words in Section 2.4.
Our model uses qt to query the key-value pairs
{wi, ei}(i = 1, 2, ..., n) by evaluating the rele-
vance between the query qt and each word vec-
tor ei with a score function f (qt, ei). The query
process can be regarded as the attentive selection
of the word embeddings. We borrow the attention
energy functions (Luong et al., 2015) as the rele-
vance score function f (qt, ei):

f (qt, ei) =

qT
t ei
qT
t Waei
vT tanh(Wqqt + Weei)

dot
general
concat

(5)






where Wq and We are two trainable parameter
matrices, and vT is a trainable parameter vector.

2.4 Selection of Candidate Key-value Pairs

As described in Section 2.3, the model generates
the words in a retrieval style, which selects a word
according to its embedding from a set of candidate
key-value pairs. We now give the details of how to
obtain the set of candidate key-value pairs. We
extract the vocabulary from the source text in the
training set, and select the n most frequent words
as the candidate words. We reuse the embeddings
of the decoder inputs as the values of the candi-
date words, which means that the decoder input
and the predicted output share the same vocabu-
lary and word embeddings. Besides, we do not use
any pretrained word embeddings in our model, so
that all of the parameters are learned from scratch.

2.5 Training

Although our generator is a retrieval style, WEAN
is as differentiable as the sequence-to-sequence
model. The objective of training is to minimize the

cross entropy between the predicted word proba-
bility distribution and the golden one-hot distribu-
tion:

L = −

ˆyi log p(yi)

(7)

X
i

We use Adam optimization method to train the
model, with the default hyper-parameters:
the
learning rate α = 0.001, and β1 = 0.9, β2 =
0.999, ǫ = 1e − 8.

3 Experiments

Following the previous work (Cao et al., 2017),
we test our model on the following two paraphrase
orientated tasks: text simpliﬁcation and short text
abstractive summarization.

3.1 Text Simpliﬁcation

3.1.1 Datasets
The datasets are both from the alignments be-
tween English Wikipedia website2 and Simple En-
glish Wikipedia website.3 The Simple English
Wikipedia is built for “the children and adults who
are learning the English language”, and the arti-
cles are composed with “easy words and short sen-
tences”. Therefore, Simple English Wikipedia is a
natural public simpliﬁed text corpus.

• Parallel Wikipedia Simpliﬁcation Corpus
(PWKP). PWKP (Zhu et al., 2010)
is a
widely used benchmark for evaluating text
simpliﬁcation systems. It consists of aligned
complex text from English WikiPedia (as
of Aug.
22nd, 2009) and simple text
from Simple Wikipedia (as of Aug. 17th,
2009). The dataset contains 108,016 sen-
tence pairs, with 25.01 words on average
per complex sentence and 20.87 words per
Following the previous
simple sentence.
work (Zhang and Lapata, 2017), we remove
the duplicate sentence pairs, and split the cor-
pus with 89,042 pairs for training, 205 pairs
for validation and 100 pairs for test.

• English Wikipedia and Simple English
Wikipedia (EW-SEW). EW-SEW is a pub-
licly available dataset provided by Hwang et
al. (2015). To build the corpus, they ﬁrst align
the complex-simple sentence pairs, score the
semantic similarity between the complex sen-
tence and the simple sentence, and classify

2http://en.wikipedia.org
3http://simple.wikipedia.org

each sentence pair as a good, good partial,
partial, or bad match. Following the previous
work (Nisioi et al., 2017), we discard the un-
classiﬁed matches, and use the good matches
and partial matches with a scaled threshold
greater than 0.45. The corpus contains about
150K good matches and 130K good partial
matches. We use this corpus as the train-
ing set, and the dataset provided by Xu et
al. (Xu et al., 2016) as the validation set and
the test set. The validation set consists of
2,000 sentence pairs, and the test set contains
359 sentence pairs. Besides, each complex
sentence is paired with 8 reference simpliﬁed
sentences provided by Amazon Mechanical
Turk workers.

3.1.2 Evaluation Metrics
Following the previous work (Nisioi et al., 2017;
Hu et al., 2015), we evaluate our model with dif-
ferent metrics on two tasks.

• Automatic evaluation. We use the BLEU
score (Papineni et al., 2002) as the automatic
evaluation metric. BLEU is a widely used
metric for machine translation and text sim-
pliﬁcation, which measures the agreement
between the model outputs and the gold ref-
erences. The references can be either single
or multiple.
In our experiments, the refer-
ences are single on PWKP, and multiple on
EW-SEW.

• Human evaluation. Human evaluation is es-
sential to evaluate the quality of the model
outputs. Following Nisioi et al. (2017) and
Zhang et al. (2017), we ask the human raters
to rate the simpliﬁed text in three dimensions:
Fluency, Adequacy and Simplicity. Fluency
assesses whether the outputs are grammati-
cally right and well formed. Adequacy rep-
resents the meaning preservation of the sim-
pliﬁed text. Both the scores of ﬂuency and
adequacy range from 1 to 5 (1 is very bad
and 5 is very good). Simplicity shows how
simpler the model outputs are than the source
text, which ranges from 1 to 5.

3.1.3 Settings
Our proposed model is based on the encoder-
decoder framework. The encoder is implemented
on LSTM, and the decoder is based on LSTM with
Luong style attention (Luong et al., 2015). We

PWKP
PBMT (Wubben et al., 2012)
Hybrid (Narayan and Gardent, 2014)
EncDecA (Zhang and Lapata, 2017)
DRESS (Zhang and Lapata, 2017)
DRESS-LS (Zhang and Lapata, 2017)
Seq2seq (our implementation)
WEAN (our proposal)

BLEU
46.31
53.94
47.93
34.53
36.32
48.26
54.54

Table 1: Automatic evaluation of our model and other
related systems on PWKP datasets. The results are re-
ported on the test sets.

EW-SEW
PBMT-R (Wubben et al., 2012)
Hybrid (Narayan and Gardent, 2014)
SBMT-SARI (Xu et al., 2016)
NTS (Nisioi et al., 2017)
NTS-w2v (Nisioi et al., 2017)
EncDecA (Zhang and Lapata, 2017)
DRESS (Zhang and Lapata, 2017)
DRESS-LS (Zhang and Lapata, 2017)
Seq2seq (our implementation)
WEAN (our proposal)

BLEU
67.79
48.97
73.62
84.70
87.50
88.85
77.18
80.12
88.97
94.45

PWKP
NTS-w2v
DRESS-LS
WEAN
Reference

Fluency Adequacy Simplicity All
3.46
3.58
3.67
3.60

3.54
3.68
3.77
3.76

3.38
3.50
3.58
3.44

3.47
3.55
3.66
3.60

EW-SEW Fluency Adequacy Simplicity All
3.22
PBMT-R
3.43
SBMT-SARI
3.50
NTS-w2v
3.56
DRESS-LS
3.61
WEAN
3.60
Reference

3.37
3.25
3.42
3.65
3.65
3.45

2.92
3.63
3.52
3.43
3.56
3.64

3.36
3.41
3.56
3.59
3.61
3.71

Table 3: Human evaluation of our model and other re-
lated systems on PWKP and EW-SEW datasets. The
results are reported on the test sets.

sentence simpliﬁcation models.

• EncDecA is a model based on the encoder-
implemented by

decoder with attention,
Zhang and Lapata (2017).

Table 2: Automatic evaluation of our model and other
related systems on EW-SEW datasets. The results are
reported on the test sets.

• PBMT-R (Wubben et al., 2012) is a phrase
based machine translation model which
reranks the outputs.

tune our hyper-parameter on the development set.
The model has two LSTM layers. The hidden size
of LSTM is 256, and the embedding size is 256.
We use Adam optimizer (Kingma and Ba, 2014)
to learn the parameters, and the batch size is set to
be 64. We set the dropout rate (Srivastava et al.,
2014) to be 0.4. All of the gradients are clipped
when the norm exceeds 5.

3.1.4 Baselines
We compare our model with several neural text
simpliﬁcation systems.

• Seq2seq is our

implementation of

the
sequence-to-sequence model with attention
mechanism, which is the most popular neu-
ral model for text generation.

• NTS and NTS-w2v (Nisioi et al., 2017) are
two sequence-to-sequence model with ex-
tra mechanism like prediction ranking, and
NTS-w2v uses a pretrain word2vec.

• DRESS and DRESS-LS (Zhang and Lapata,
2017) are two deep reinforcement learning

• Hybrid (Narayan and Gardent, 2014) is a hy-
brid approach which combines deep seman-
tics and mono-lingual machine translation.

• SBMT-SARI (Xu et al., 2016) is a syntax-
based machine translation model which is
trained on PPDB dataset (Ganitkevitch et al.,
2013) and tuned with SARI.

3.1.5 Results

We compare WEAN with state-of-the-art mod-
els for text simpliﬁcation. Table 1 and Table 2
summarize the results of the automatic evalua-
tion. On PWKP dataset, we compare WEAN with
PBMT, Hybrid, EncDecA, DRESS and DRESS-
LS. WEAN achieves a BLEU score of 54.54, out-
performing all of the previous systems. On EW-
SEW dataset, we compare WEAN with PBMT-R,
Hybrid, SBMT-SARI, and the neural models de-
scribed above. We do not ﬁnd any public release
code of PBMT-R and SBMT-SARI. Fortunately,
Xu et al. (2016) provides the predictions of PBMT-
R and SBMT-SARI on EW-SEW test set, so that
we can compare our model with these systems.

LCSTS

R-1 R-2 R-L
15.8
8.5
17.7
RNN-W(Hu et al., 2015)
RNN(Hu et al., 2015)
18.6
8.9
21.5
RNN-cont-W(Hu et al., 2015) 26.8 16.1 24.1
RNN-cont(Hu et al., 2015)
29.9 17.4 27.2
33.3 20.0 30.1
SRB(Ma et al., 2017)
CopyNet-W(Gu et al., 2016)
35.0 22.3 32.0
34.4 21.6 31.3
CopyNet(Gu et al., 2016)
RNN-dist(Chen et al., 2016)
35.2 22.6 32.5
37.0 24.2 34.2
DRGD(Li et al., 2017)
32.1 19.9 29.2
Seq2seq
37.8 25.6 35.2
WEAN

Table 4: ROUGE F1 score on the LCSTS test set. R-
1, R-2, and R-L denote ROUGE-1, ROUGE-2, and
ROUGE-L, respectively. The models with a sufﬁx of
‘W’ in the table are word-based, while the rest of mod-
els are character-based.

It shows that the neural models have better per-
formance in BLEU, and WEAN achieves the best
BLEU score with 94.45.

We perform the human evaluation of WEAN
and other related systems, and the results are
shown in Table 3. DRESS-LS is based on the rein-
forcement learning, and it encourages the ﬂuency,
simplicity and relevance of the outputs. There-
fore, it achieves a high score in our human eval-
uation. WEAN gains a even better score than
DRESS-LS. Besides, WEAN generates more ad-
equate and simpler outputs than the reference on
PWKP. The predictions of SBMT-SARI are the
most adequate among the compared systems on
EW-SEW. In general, WEAN outperforms all of
the other systems, considering the balance of ﬂu-
ency, adequate and simplicity. We conduct sig-
niﬁcance tests based on t-test. The signiﬁcance
tests suggest that WEAN has a very signiﬁcant
improvement over baseline, with p ≤ 0.001 over
DRESS-LS in all of the dimension on PWKP,
p ≤ 0.05 over DRESS-LS in the dimension of ﬂu-
ency, p ≤ 0.005 over NTS-w2v in the dimension
of simplicity and p ≤ 0.005 over DRESS-LS in
the dimension of all.

3.2 Large Scale Text Summarization

3.2.1 Dataset

Large Scale Chinese Social Media Short Text
Summarization Dataset (LCSTS): LCSTS is
constructed by Hu et al. (2015). The dataset con-
sists of more than 2,400,000 text-summary pairs,
constructed from a famous Chinese social media

website called Sina Weibo.4 It is split into three
parts, with 2,400,591 pairs in PART I, 10,666 pairs
in PART II and 1,106 pairs in PART III. All the
text-summary pairs in PART II and PART III are
manually annotated with relevant scores ranged
from 1 to 5. We only reserve pairs with scores
no less than 3, leaving 8,685 pairs in PART II
and 725 pairs in PART III. Following the previous
work (Hu et al., 2015), we use PART I as training
set, PART II as validation set, and PART III as test
set.

is

metric

3.2.2 Evaluation Metrics
Our
ROUGE
evaluation
score (Lin and Hovy, 2003), which is popu-
lar for summarization evaluation. The metrics
compare an automatically produced summary
against the reference summaries, by computing
overlapping lexical units, including unigram, bi-
gram, trigram, and longest common subsequence
(LCS). Following previous work (Rush et al.,
2015; Hu et al., 2015), we use ROUGE-1 (un-
igram), ROUGE-2 (bi-gram) and ROUGE-L
(LCS) as the evaluation metrics in the reported
experimental results.

3.2.3 Settings
The vocabularies are extracted from the training
sets, and the source contents and the summaries
share the same vocabularies. We tune the hyper-
parameters based on the ROUGE scores on the
In order to alleviate the risk of
validation sets.
word segmentation mistakes, we split the Chi-
nese sentences into characters. We prune the vo-
cabulary size to 4,000, which covers most of the
common characters. We set the word embed-
ding size and the hidden size to 512, the num-
ber of LSTM layers of the encoder is 2, and
the number of LSTM layers of the decoder is
1. The batch size is 64, and we do not use
dropout (Srivastava et al., 2014) on this dataset.
Following the previous work (Li et al., 2017), we
implement a beam search optimization, and set the
beam size to 5.

3.2.4 Baselines
We compare our model with the state-of-the-art
baselines.

• RNN and RNN-cont are two sequence-to-
sequence baseline with GRU encoder and de-
coder, provided by Hu et al. (2015).

4http://weibo.com

#Param PWKP EWSEW LCSTS
12.80M 12.80M 2.05M
Seq2seq
0.52M
0.13M
0.13M
WEAN

Table 5: The number of the parameters in the out-
put layer. The numbers of rest parameters between
Seq2seq and WEAN are the same.

• RNN-dist (Chen et al., 2016) is a distraction-
based neural model, which the attention
mechanism focuses on the different parts of
the source content.

• CopyNet (Gu et al., 2016) incorporates a
copy mechanism to allow part of the gener-
ated summary is copied from the source con-
tent.

• SRB (Ma et al., 2017)

is a sequence-to-
sequence based neural model with improving
the semantic relevance between the input text
and the output summary.

• DRGD (Li et al., 2017) is a deep recurrent
generative decoder model, combining the de-
coder with a variational autoencoder.

• Seq2seq is our

the
sequence-to-sequence model with the atten-
tion mechanism.

implementation of

3.2.5 Results

We report the ROUGE F1 score of our model
and the baseline models on the test sets. Ta-
ble 4 summarizes the comparison between our
model and the baselines. Our model achieves
the score of 37.8 ROUGE-1, 25.6 ROUGE-2, and
35.2 ROUGE-L, outperforming all of the previ-
ous models. First, we compare our model with
It shows that
the sequence-to-sequence model.
our model signiﬁcant outperforms the sequence-
to-sequence baseline with a large margin of 5.7
ROUGE-1, 5.7 ROUGE-2, and 6.0 ROUGE-
L. Then, we compare our model with other
related models.
is
DRGD (Li et al., 2017), which obtains the score
of 37.0 ROUGE-1, 24.2 ROUGE-2, and 34.2
ROUGE-L. Our model has a relative gain of 0.8
ROUGE-1, 1.4 ROUGE-2 and 1.0 ROUGE-L over
the state-of-the-art models.

The state-of-the-art model

Training Curve

 

U
E
L
B

50

40

30

20

10

 

0
0

WEAN
S2S

2

4

6

8

10 12 14 16

Epoch

Figure 2: The training curve of WEAN and Seq2seq on
the PWKP validation set.

4 Analysis and Discussion

4.1 Reducing Parameters

Our WEAN reduces a large number of the param-
eters in the output layer. To analyze the parame-
ter reduction, we compare our WEAN model with
the sequence-to-sequence model. Table 5 lists the
number of the parameters in the output layers of
two models. Both PWKP and EWSEWhave the
vocabulary size of 50000 words and the hidden
size of 256, resulting 50000 × 256 = 12, 800, 000
parameters. LCSTS has a vocabulary size of 4000
and the hidden size of 512, so the seq2seq has
4000 × 512 = 2, 048, 000 parameters in the out-
put layers. WEAN only has two parameter ma-
trices and one parameter vector at most in Equa-
tion 5, without regard to the vocabulary size.
It
has 256 × 256 × 2 + 256 = 131, 328 parameters
on PWKP and EWSEW, and 512×512×2+512 =
524, 800 parameters on LCSTS. Besides, WEAN
does not have any extra parameters in the other
part of the model.

4.2 Speeding up Convergence

Figure 2 shows the training curve of WEAN and
Seq2seq on the PWKP validation set. WEAN
achieve near the optimal score in only 2-3 epochs,
while Seq2seq takes more than 15 epochs to
achieve the optimal score. Therefore, WEAN
has much faster convergence rate, compared with
Seq2seq. With the much faster training speed,
WEAN does not suffer loss in BLEU, and even
improve the BLEU score.

Yoghurt oryogurt isadairyproduct produced bybacterial fermentation ofmilk.
Yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk.
. oryoghurtisadairy product producedbybacterial fermentation ofmilk.
Itismadebybacterial fermentation ofmilk.
Yoghurt oryogurt isadairyproduct producedbybacterial fermentation of.

Source
Reference
NTS
NTS-w2v
PBMT-R
SBMT-SARI Yogurtoryogurt isadairy product drawnupbybacterial fermentation ofmilk.
WEAN
Source

Yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk.
Depending on the context, another closely-related meaning of constituent is that of a
citizen residing in the area governed, represented, or otherwise served by a politician;
sometimes thisisrestricted tocitizens whoelected thepolitician.
Thewordconstituentcanalsobeusedtorefertoacitizenwholivesintheareathat
is governed, represented, or otherwise served by a politician; sometimes the word is
restricted tocitizens whoelected thepolitician.
Depending on the context, another closely-related meaning of constituent is that of a
citizen living in the area governed, represented, or otherwise served by a politician;
sometimes thisisrestricted tocitizens whoelected thepolitician.
Thisisrestricted tocitizens whoelected thepolitician.
Depending on the context and meaning of closely-related siemens-martin -rrb- is a
citizen living in the area, or otherwise, was governed by a 1924-1930 shurba; this is
restricted topeople whoelected it.

Reference

NTS

NTS-w2v
PBMT-R

SBMT-SARI In terms of the context, another closely-related sense of the component is that of a
citizen living in the area covered, makeup, or ifnot, served byapolicy; sometimes
thisislimitedtothepeople whoelected thepolicy.
Depending on the context, another closely-related meaning of constituent is that of a
citizenwholivesintheareagoverned,represented, orotherwiseservedbyapolitician;
sometimes thewordisrestricted tocitizens whoelected thepolitician.

WEAN

Table 6: Two examples of different text simpliﬁcation system outputs in EW-SEW dataset. Differences from the
source texts are shown in bold.

4.3 Case Study

and very close to the original meaning.

Table 6 shows two examples of different text sim-
pliﬁcation system outputs on EW-SEW. For the
ﬁrst example, NTS, NTS-w2v and PBMT-R miss
some essential constituents, so that the sentences
are incomplete and not ﬂuent. SBMT-SARI gen-
erates a ﬂuent sentence, but the output does not
preserve the original meaning. The predicted sen-
tence of WEAN is ﬂuent, simple, and the same
as the reference. For the second example, NTS-
w2v omits so many words that
it lacks a lot
of information. PBMT-R generates some irrele-
vant words, like ’siemens-martin’, ’-rrb-’, and ’-
shurba’, which hurts the ﬂuency and adequacy of
the generated sentence. SBMT-SARI is able to
generate a ﬂuent sentence, but the meaning is dif-
ferent from the source text, and even more difﬁ-
cult to understand. Compared with the statistic
model, WEAN generates a more ﬂuent sentence.
Besides, WEAN can capture the semantic mean-
ing of the word by querying the word embeddings,
so the generated sentence is semantically correct,

5 Related Work

Our work is related to the encoder-decoder
framework (Cho et al., 2014) and the attention
mechanism (Bahdanau et al., 2014).
Encoder-
decoder framework,
like sequence-to-sequence
model, has achieved success in machine trans-
lation (Sutskever et al., 2014; Jean et al., 2015;
Luong et al., 2015; Lin et al., 2018),
text sum-
marization
(Rush et al.,
2015; Chopra et al.,
2016; Nallapati et al., 2016; Wang et al., 2017;
Ma and Sun, 2017), and other natural language
processing tasks (Liu et al., 2017). There are
many other methods to improve neural attention
model (Jean et al., 2015; Luong et al., 2015).

Zhu et al.

(2010) constructs a wikipedia
dataset, and proposes a tree-based simpliﬁcation
model. Woodsend and Lapata (2011) introduces
a data-driven model based on quasi-synchronous
grammar, which captures structural mismatches
and complex rewrite operations. Wubben et al.

(2012) presents a method for text simpliﬁcation
using phrase based machine translation with re-
ranking the outputs. Kauchak (2013) proposes a
text simpliﬁcation corpus, and evaluates language
modeling for text simpliﬁcation on the proposed
corpus. Narayan and Gardent (2014) propose a
hybrid approach to sentence simpliﬁcation which
combines deep semantics and monolingual ma-
chine translation. Hwang et al. (2015) introduces
a parallel simpliﬁcation corpus by evaluating the
similarity between the source text and the simpli-
ﬁed text based on WordNet. Glavaˇs and ˇStajner
(2015) propose an unsupervised approach to lex-
ical simpliﬁcation that makes use of word vec-
tors and require only regular corpora. Xu et al.
(2016) design automatic metrics for text sim-
pliﬁcation. Recently, most works focus on the
neural sequence-to-sequence model. Nisioi et al.
(2017) present a sequence-to-sequence model, and
re-ranks the predictions with BLEU and SARI.
Zhang and Lapata (2017) propose a deep rein-
forcement learning model to improve the simplic-
ity, ﬂuency and adequacy of the simpliﬁed texts.
Cao et al. (2017) introduce a novel sequence-to-
sequence model to join copying and restricted gen-
eration for text simpliﬁcation.

Rush et al. (2015) ﬁrst used an attention-based
encoder to compress texts and a neural network
language decoder to generate summaries. Follow-
ing this work, recurrent encoder was introduced
to text summarization, and gained better perfor-
mance (Lopyrev, 2015; Chopra et al., 2016). To-
wards Chinese texts, Hu et al. (2015) built a large
corpus of Chinese short text summarization. To
deal with unknown word problem, Nallapati et al.
(2016) proposed a generator-pointer model so that
the decoder is able to generate words in source
texts. Gu et al. (2016) also solved this issue by
incorporating copying mechanism.

6 Conclusion

We propose a novel model based on the encoder-
decoder framework, which generates the words by
querying distributed word representations. Exper-
imental results show that our model outperforms
the sequence-to-sequence baseline by the BLEU
score of 6.3 and 5.5 on two English text simpliﬁ-
cation datasets, and the ROUGE-2 F1 score of 5.7
on a Chinese summarization dataset. Moreover,
our model achieves state-of-the-art performances
on these three benchmark datasets.

Acknowledgements

This work was supported in part by National Natu-
ral Science Foundation of China (No. 61673028),
National High Technology Research and Devel-
opment Program of China (863 Program, No.
2015AA015404), and the National Thousand
Young Talents Program. Xu Sun is the corre-
sponding author of this paper.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473.

Ziqiang Cao, Chuwei Luo, Wenjie Li, and Sujian Li.
2017.
Joint copying and restricted generation for
paraphrase. In Proceedings of the Thirty-First AAAI
Conference on Artiﬁcial Intelligence. pages 3152–
3158.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and
Hui Jiang. 2016. Distraction-based neural networks
for modeling documents. In Proceedings of the 25th
International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI 2015). AAAI, New York, NY.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.

Kyunghyun Cho, Bart van Merrienboer, C¸ aglar
G¨ulc¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
In Proceedings
for statistical machine translation.
of the 2014 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2014. pages
1724–1734.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. pages 93–
98.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013.
the paraphrase
database. In Human Language Technologies: Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, Proceedings.
pages 758–764.

PPDB:

Goran Glavaˇs and Sanja ˇStajner. 2015. Simplifying
lexical simpliﬁcation: Do we need simpliﬁed cor-
pora? In Proceedings of the 53rd Annual Meeting of

the Association for Computational Linguistics, ACL.
pages 63–68.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Incorporating copying mechanism in
Li. 2016.
In Proceedings of
sequence-to-sequence learning.
the 54th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2016.

Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LC-
STS: A large scale chinese short text summarization
dataset. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015. pages 1967–1972.

William Hwang, Hannaneh Hajishirzi, Mari Ostendorf,
and Wei Wu. 2015. Aligning sentences from stan-
dard wikipedia to simple wikipedia. In NAACL HLT
2015. pages 211–217.

S´ebastien Jean, KyungHyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2015.
pages 1–10.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
In Proceedings of the
neural machine translation.
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015. pages 1412–
1421.

Shuming Ma and Xu Sun. 2017. A semantic rele-
vance based neural network for text summarization
and text simpliﬁcation. CoRR abs/1710.02318.

Shuming Ma, Xu Sun, Jingjing Xu, Houfeng Wang,
Wenjie Li, and Qi Su. 2017.
Improving semantic
relevance for sequence-to-sequence learning of chi-
nese social media text summarization. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 2: Short
Papers. pages 635–640.

Ramesh Nallapati, Bowen Zhou, C´ıcero Nogueira dos
Santos, C¸ aglar G¨ulc¸ehre, and Bing Xiang. 2016.
Abstractive text summarization using sequence-to-
In Proceedings of the
sequence rnns and beyond.
20th SIGNLL Conference on Computational Natural
Language Learning, CoNLL 2016, Berlin, Germany,
August 11-12, 2016. pages 280–290.

David Kauchak. 2013.

Improving text simpliﬁcation
language modeling using unsimpliﬁed text data. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL. pages
1537–1546.

Shashi Narayan and Claire Gardent. 2014. Hybrid sim-
pliﬁcation using deep semantics and machine trans-
In Proceedings of the 52nd Annual Meet-
lation.
ing of the Association for Computational Linguis-
tics, ACL. pages 435–445.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR

A method for stochastic optimization.
abs/1412.6980.

Piji Li, Wai Lam, Lidong Bing, and Zihao Wang.
2017. Deep recurrent generative decoder for ab-
stractive text summarization. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2017, Copenhagen,
Denmark, September 9-11, 2017. pages 2091–2100.

Chin-Yew Lin and Eduard H. Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics, HLT-
NAACL 2003.

Junyang Lin, Shuming Ma, Qi Su, and Xu Sun.
2018. Decoding-history-based adaptive control of
attention for neural machine translation. CoRR
abs/1802.01812.

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,
and Zhifang Sui. 2017.
Table-to-text genera-
tion by structure-aware seq2seq learning. CoRR
abs/1711.09724.

Sergiu Nisioi, Sanja Stajner, Simone Paolo Ponzetto,
and Liviu P. Dinu. 2017. Exploring neural text sim-
pliﬁcation models. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL. pages 85–91.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics. pages 311–318.

Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek V.
Datla, Ashequl Qadir, Joey Liu, and Oladimeji Farri.
2016. Neural paraphrase generation with stacked
In COLING 2016, 26th
residual LSTM networks.
International Conference on Computational Lin-
guistics, Proceedings of the Conference: Techni-
cal Papers, December 11-16, 2016, Osaka, Japan.
pages 2923–2934.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
In Proceedings of the 2015
tence summarization.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015. pages 379–389.

Konstantin Lopyrev. 2015. Generating news head-
CoRR

lines with recurrent neural networks.
abs/1512.01712.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural

Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze
Chen, and Chris Callison-Burch. 2016. Optimizing
statistical machine translation for text simpliﬁcation.
TACL 4:401–415.

Xingxing Zhang and Mirella Lapata. 2017.

Sen-
tence simpliﬁcation with deep reinforcement learn-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September
9-11, 2017. pages 584–594.

Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simpliﬁcation. In COLING 2010. pages
1353–1361.

networks from overﬁtting.
Learning Research 15(1):1929–1958.

Journal of Machine

Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng
Wang. 2017a. meprop: Sparsiﬁed back propaga-
tion for accelerated deep learning with reduced over-
In Proceedings of the 34th International
ﬁtting.
Conference on Machine Learning, ICML 2017, Syd-
ney, NSW, Australia, 6-11 August 2017. pages 3299–
3308.

Xu Sun, Xuancheng Ren, Shuming Ma, Bingzhen Wei,
Wei Li, and Houfeng Wang. 2017b. Training simpli-
ﬁcation and model simpliﬁcation for deep learning:
A minimal effort back propagation method. CoRR
abs/1711.06528.

Xu Sun, Bingzhen Wei, Xuancheng Ren, and Shuming
Ma. 2017c. Label embedding network: Learning la-
bel representation for soft training of deep networks.
CoRR abs/1710.10393.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural Infor-
mation Processing Systems 2014. pages 3104–3112.

Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hi-
rao, and Masaaki Nagata. 2016. Neural headline
generation on abstract meaning representation.
In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2016, Austin, Texas, USA, November 1-4, 2016.
pages 1054–1059.

Kexiang Wang, Tianyu Liu, Zhifang Sui, and Baobao
Chang. 2017. Afﬁnity-preserving random walk for
multi-document summarization. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017. pages 210–
220.

Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing, EMNLP. pages 409–
420.

Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simpliﬁcation by mono-
In The 50th An-
lingual machine translation.
nual Meeting of the Association for Computational
Linguistics, Proceedings of the Conference. pages
1015–1024.

Jingjing Xu, Xu Sun, Xuancheng Ren, Junyang Lin,
Binzhen Wei, and Wei Li. 2018. Dp-gan: Diversity-
promoting generative adversarial network for gen-
CoRR
erating informative and diversiﬁed text.
abs/1802.01345.

8
1
0
2
 
r
a

M
 
0
3
 
 
]
L
C
.
s
c
[
 
 
3
v
5
6
4
1
0
.
3
0
8
1
:
v
i
X
r
a

Query and Output: Generating Words by Querying Distributed Word
Representations for Paraphrase Generation

Shuming Ma1, Xu Sun1,2, Wei Li1, Sujian Li1, Wenjie Li3, Xuancheng Ren1
1MOE Key Lab of Computational Linguistics, School of EECS, Peking University
2Deep Learning Lab, Beijing Institute of Big Data Research, Peking University
3Department of Computing, The Hong Kong Polytechnic University
{shumingma, xusun, liweitj47, lisujian, renxc}@pku.edu.cn
cswjli@comp.polyu.edu.hk

Abstract

Most recent approaches use the sequence-
to-sequence model for paraphrase genera-
The existing sequence-to-sequence
tion.
model tends to memorize the words and the
patterns in the training dataset instead of learn-
ing the meaning of the words. Therefore,
the generated sentences are often grammati-
cally correct but semantically improper. In this
work, we introduce a novel model based on
the encoder-decoder framework, called Word
Embedding Attention Network (WEAN). Our
proposed model generates the words by query-
ing distributed word representations (i.e. neu-
ral word embeddings), hoping to capturing the
meaning of the according words. Following
previous work, we evaluate our model on two
paraphrase-oriented tasks, namely text sim-
pliﬁcation and short text abstractive summa-
rization. Experimental results show that our
model outperforms the sequence-to-sequence
baseline by the BLEU score of 6.3 and 5.5
on two English text simpliﬁcation datasets,
and the ROUGE-2 F1 score of 5.7 on a Chi-
nese summarization dataset. Moreover, our
model achieves state-of-the-art performances
on these three benchmark datasets.1

1 Introduction

Paraphrase is a restatement of the meaning of a
text using other words. Many natural language
generation tasks are paraphrase-orientated, such
as text simpliﬁcation and short text summariza-
tion. Text simpliﬁcation is to make the text easier
to read and understand, especially for poor read-
ers, while short text summarization is to generate a
brief sentence to describe the short texts (e.g. posts
on the social media). Most recent approaches use
sequence-to-sequence model for paraphrase gen-
eration (Prakash et al., 2016; Cao et al., 2017). It

1The

code

is

available

at

https://github.com/lancopku/WEAN

compresses the source text information into dense
vectors with the neural encoder, and the neural
decoder generates the target text using the com-
pressed vectors.

Although neural network models achieve suc-
cess in paraphrase generation, there are still two
major problems. One of the problem is that the ex-
isting sequence-to-sequence model tends to mem-
orize the words and the patterns in the training
dataset instead of the meaning of the words. The
main reason is that the word generator (i.e.
the
output layer of the decoder) does not model the
semantic information. The word generator, which
consists of a linear transformation and a softmax
operation, converts the Recurrent Neural Network
(RNN) output from a small dimension (e.g. 500)
to a much larger dimension (e.g. 50,000 words
in the vocabulary), where each dimension repre-
sents the score of each word. The latent assump-
tion of the word generator is that each word is in-
dependent and the score is irrelevant to each other.
Therefore, the scores of a word and its synonyms
may be of great difference, which means the word
generator learns the word itself rather than the re-
lationship between words.

The other problem is that the word generator
has a huge number of parameters. Suppose we
have a sequence-to-sequence model with a hid-
den size of 500 and a vocabulary size of 50,000.
The word generator has up to 25 million parame-
ters, which is even larger than other parts of the
encoder-decoder model in total. The huge size
of parameters will result in slow convergence, be-
cause there are a lot of parameters to be learned.
Moreover, under the distributed framework, the
more parameters a model has, the more bandwidth
and memory it consumes.

To tackle both of the problems, we propose a
novel model called Word Embedding Attention
Network (WEAN). The word generator of WEAN

is attention based, instead of the simple linear soft-
max operation. In our attention based word gen-
erator, the RNN output is a query, the candidate
words are the values, and the corresponding word
In order to predict
representations are the keys.
the word, the attention mechanism is used to se-
lect the value matching the query most, by means
of querying the keys. In this way, our model gen-
erates the words according to the distributed word
representations (i.e. neural word embeddings) in
a retrieval style rather than the traditional gener-
ative style. Our model is able to capture the se-
mantic meaning of a word by referring to its em-
bedding. Besides, the attention mechanism has
a much smaller number of parameters compared
with the linear transformation directly from the
RNN output space to the vocabulary space. The
reduction of the parameters can increase the con-
vergence rate and speed up the training process.
Moreover, the word embedding is updated from
three sources: the input of the encoder, the input
of the decoder, and the query of the output layer.

Following previous work (Cao et al., 2017), we
evaluate our model on two paraphrase-oriented
tasks, namely text simpliﬁcation and short text
abstractive summarization. Experimental results
show that our model outperforms the sequence-to-
sequence baseline by the BLEU score of 6.3 and
5.5 on two English text simpliﬁcation datasets, and
the ROUGE-2 F1 score of 5.7 on a Chinese sum-
marization dataset. Moreover, our model achieves
state-of-the-art performances on all of the bench-
mark datasets.

2 Proposed Model

We propose a novel model based on the encoder-
decoder framework, which generates the words
by querying distributed word representations with
the attention mechanism. In this section, we ﬁrst
present the overview of the model architecture.
Then, we explain the details of the word gener-
ation, especially the way to query word embed-
dings.

2.1 Overview

Word Embedding Attention Network is based on
the encoder-decoder framework, which consists of
two components: a source text encoder, and a tar-
get text decoder. Figure 1 is an illustration of our
model. Given the source texts, the encoder com-
presses the source texts into dense representation

vectors, and the decoder generates the paraphrased
texts. To predict a word, the decoder uses the hid-
den output to query the word embeddings. The
word embeddings assess all the candidate words,
and return the word whose embedding matches the
query most. The selected word is emitted as the
predicted token, and its embedding is then used as
the input of the LSTM at the next time step. After
the back propagation, the word embedding is up-
dated from three sources: the input of the encoder,
the input of the decoder, and the query of the out-
put layer. We show the details of our WEAN in
the following subsection.

2.2 Encoder and Decoder

The goal of the source text encoder is to pro-
vide a series of dense representation of complex
source texts for the decoder.
In our model, the
source text encoder is a Long Short-term Memory
Network (LSTM), which produces the dense rep-
resentation {h1, h2, ..., hN } from the source text
{x1, x2, ..., xN }:

The goal of the target text decoder is to generate
a series of paraphrased words from the dense rep-
resentation of source texts. Fisrt, the LSTM of the
decoder compute the dense representation of gen-
erated words st. Then, the dense representations
are fed into an attention layer (Bahdanau et al.,
2014) to generate the context vector ct, which cap-
tures context information of source texts. Atten-
tion vector ct is calculated by the weighted sum of
encoder hidden states:

ct =

αtihi

N

X
i=1

αti =

eg(st,hi)
N
j=1 eg(st,hj)

P

(1)

(2)

where g(st, hi) is an attentive score between the
decoder hidden state st and the encoder hidden
state hi.

In this way, ct and st respectively represent the
context information of source texts and the target
texts at the tth time step.

2.3 Word Generation by Querying Word

Embedding

For the current sequence-to-sequence model, the
word generator computes the distribution of output
words yt in a generative style:

p(yt) = sof tmax(W st)

(3)

{ hard,

}

Admission

very

hard

Key

...

(cid:1871)(cid:2869)

(cid:1871)(cid:3047)(cid:2879)(cid:2869)

(cid:1871)(cid:3047)

(cid:1855)(cid:3047)

Value

easy

...

happy

...

hard

...

competitive

Query

(cid:1860)(cid:2869)
Admission

(cid:1860)(cid:2870)
is

(cid:1860)(cid:2871)
extremely

(cid:1860)(cid:2872)
competitive

Figure 1: An overview of Word Embedding Attention Network.

where W ∈ Rk×V is a trainable parameter matrix,
k is hidden size, and V is the number of words in
the vocabulary. When the vocabulary is large, the
number of parameters will be huge.

Our model generates the words in a retrieval
style rather than the traditional generative style,
by querying the word embeddings. We denote the
combination of the source context vector ct and
the target context vector st as the query qt:

In implementation, we select the general attention
function as the relevance score function, based on
the performance on the validation sets. The key-
value pair with the highest score {wt, et} is se-
lected. At the test stage, the decoder generates the
key wt as the tth predicted word, and inputs the
value et to the LSTM unit at the t + 1th time step.
At the training stage, the scores are normalized as
the word probability distribution:

qt = tanh(Wc[st; ct])

(4)

p(yt) = sof tmax(f (qt, ei))

(6)

The candidate words wi and their corresponding
embeddings ei are paired as the key-value pairs
{wi, ei}(i = 1, 2, ..., n), where n is the number of
candidate words. We give the details of how to de-
termine the set of candidate words in Section 2.4.
Our model uses qt to query the key-value pairs
{wi, ei}(i = 1, 2, ..., n) by evaluating the rele-
vance between the query qt and each word vec-
tor ei with a score function f (qt, ei). The query
process can be regarded as the attentive selection
of the word embeddings. We borrow the attention
energy functions (Luong et al., 2015) as the rele-
vance score function f (qt, ei):

f (qt, ei) =

qT
t ei
qT
t Waei
vT tanh(Wqqt + Weei)

dot
general
concat

(5)






where Wq and We are two trainable parameter
matrices, and vT is a trainable parameter vector.

2.4 Selection of Candidate Key-value Pairs

As described in Section 2.3, the model generates
the words in a retrieval style, which selects a word
according to its embedding from a set of candidate
key-value pairs. We now give the details of how to
obtain the set of candidate key-value pairs. We
extract the vocabulary from the source text in the
training set, and select the n most frequent words
as the candidate words. We reuse the embeddings
of the decoder inputs as the values of the candi-
date words, which means that the decoder input
and the predicted output share the same vocabu-
lary and word embeddings. Besides, we do not use
any pretrained word embeddings in our model, so
that all of the parameters are learned from scratch.

2.5 Training

Although our generator is a retrieval style, WEAN
is as differentiable as the sequence-to-sequence
model. The objective of training is to minimize the

cross entropy between the predicted word proba-
bility distribution and the golden one-hot distribu-
tion:

L = −

ˆyi log p(yi)

(7)

X
i

We use Adam optimization method to train the
model, with the default hyper-parameters:
the
learning rate α = 0.001, and β1 = 0.9, β2 =
0.999, ǫ = 1e − 8.

3 Experiments

Following the previous work (Cao et al., 2017),
we test our model on the following two paraphrase
orientated tasks: text simpliﬁcation and short text
abstractive summarization.

3.1 Text Simpliﬁcation

3.1.1 Datasets
The datasets are both from the alignments be-
tween English Wikipedia website2 and Simple En-
glish Wikipedia website.3 The Simple English
Wikipedia is built for “the children and adults who
are learning the English language”, and the arti-
cles are composed with “easy words and short sen-
tences”. Therefore, Simple English Wikipedia is a
natural public simpliﬁed text corpus.

• Parallel Wikipedia Simpliﬁcation Corpus
(PWKP). PWKP (Zhu et al., 2010)
is a
widely used benchmark for evaluating text
simpliﬁcation systems. It consists of aligned
complex text from English WikiPedia (as
of Aug.
22nd, 2009) and simple text
from Simple Wikipedia (as of Aug. 17th,
2009). The dataset contains 108,016 sen-
tence pairs, with 25.01 words on average
per complex sentence and 20.87 words per
Following the previous
simple sentence.
work (Zhang and Lapata, 2017), we remove
the duplicate sentence pairs, and split the cor-
pus with 89,042 pairs for training, 205 pairs
for validation and 100 pairs for test.

• English Wikipedia and Simple English
Wikipedia (EW-SEW). EW-SEW is a pub-
licly available dataset provided by Hwang et
al. (2015). To build the corpus, they ﬁrst align
the complex-simple sentence pairs, score the
semantic similarity between the complex sen-
tence and the simple sentence, and classify

2http://en.wikipedia.org
3http://simple.wikipedia.org

each sentence pair as a good, good partial,
partial, or bad match. Following the previous
work (Nisioi et al., 2017), we discard the un-
classiﬁed matches, and use the good matches
and partial matches with a scaled threshold
greater than 0.45. The corpus contains about
150K good matches and 130K good partial
matches. We use this corpus as the train-
ing set, and the dataset provided by Xu et
al. (Xu et al., 2016) as the validation set and
the test set. The validation set consists of
2,000 sentence pairs, and the test set contains
359 sentence pairs. Besides, each complex
sentence is paired with 8 reference simpliﬁed
sentences provided by Amazon Mechanical
Turk workers.

3.1.2 Evaluation Metrics
Following the previous work (Nisioi et al., 2017;
Hu et al., 2015), we evaluate our model with dif-
ferent metrics on two tasks.

• Automatic evaluation. We use the BLEU
score (Papineni et al., 2002) as the automatic
evaluation metric. BLEU is a widely used
metric for machine translation and text sim-
pliﬁcation, which measures the agreement
between the model outputs and the gold ref-
erences. The references can be either single
or multiple.
In our experiments, the refer-
ences are single on PWKP, and multiple on
EW-SEW.

• Human evaluation. Human evaluation is es-
sential to evaluate the quality of the model
outputs. Following Nisioi et al. (2017) and
Zhang et al. (2017), we ask the human raters
to rate the simpliﬁed text in three dimensions:
Fluency, Adequacy and Simplicity. Fluency
assesses whether the outputs are grammati-
cally right and well formed. Adequacy rep-
resents the meaning preservation of the sim-
pliﬁed text. Both the scores of ﬂuency and
adequacy range from 1 to 5 (1 is very bad
and 5 is very good). Simplicity shows how
simpler the model outputs are than the source
text, which ranges from 1 to 5.

3.1.3 Settings
Our proposed model is based on the encoder-
decoder framework. The encoder is implemented
on LSTM, and the decoder is based on LSTM with
Luong style attention (Luong et al., 2015). We

PWKP
PBMT (Wubben et al., 2012)
Hybrid (Narayan and Gardent, 2014)
EncDecA (Zhang and Lapata, 2017)
DRESS (Zhang and Lapata, 2017)
DRESS-LS (Zhang and Lapata, 2017)
Seq2seq (our implementation)
WEAN (our proposal)

BLEU
46.31
53.94
47.93
34.53
36.32
48.26
54.54

Table 1: Automatic evaluation of our model and other
related systems on PWKP datasets. The results are re-
ported on the test sets.

EW-SEW
PBMT-R (Wubben et al., 2012)
Hybrid (Narayan and Gardent, 2014)
SBMT-SARI (Xu et al., 2016)
NTS (Nisioi et al., 2017)
NTS-w2v (Nisioi et al., 2017)
EncDecA (Zhang and Lapata, 2017)
DRESS (Zhang and Lapata, 2017)
DRESS-LS (Zhang and Lapata, 2017)
Seq2seq (our implementation)
WEAN (our proposal)

BLEU
67.79
48.97
73.62
84.70
87.50
88.85
77.18
80.12
88.97
94.45

PWKP
NTS-w2v
DRESS-LS
WEAN
Reference

Fluency Adequacy Simplicity All
3.46
3.58
3.67
3.60

3.54
3.68
3.77
3.76

3.38
3.50
3.58
3.44

3.47
3.55
3.66
3.60

EW-SEW Fluency Adequacy Simplicity All
3.22
PBMT-R
3.43
SBMT-SARI
3.50
NTS-w2v
3.56
DRESS-LS
3.61
WEAN
3.60
Reference

3.37
3.25
3.42
3.65
3.65
3.45

2.92
3.63
3.52
3.43
3.56
3.64

3.36
3.41
3.56
3.59
3.61
3.71

Table 3: Human evaluation of our model and other re-
lated systems on PWKP and EW-SEW datasets. The
results are reported on the test sets.

sentence simpliﬁcation models.

• EncDecA is a model based on the encoder-
implemented by

decoder with attention,
Zhang and Lapata (2017).

Table 2: Automatic evaluation of our model and other
related systems on EW-SEW datasets. The results are
reported on the test sets.

• PBMT-R (Wubben et al., 2012) is a phrase
based machine translation model which
reranks the outputs.

tune our hyper-parameter on the development set.
The model has two LSTM layers. The hidden size
of LSTM is 256, and the embedding size is 256.
We use Adam optimizer (Kingma and Ba, 2014)
to learn the parameters, and the batch size is set to
be 64. We set the dropout rate (Srivastava et al.,
2014) to be 0.4. All of the gradients are clipped
when the norm exceeds 5.

3.1.4 Baselines
We compare our model with several neural text
simpliﬁcation systems.

• Seq2seq is our

implementation of

the
sequence-to-sequence model with attention
mechanism, which is the most popular neu-
ral model for text generation.

• NTS and NTS-w2v (Nisioi et al., 2017) are
two sequence-to-sequence model with ex-
tra mechanism like prediction ranking, and
NTS-w2v uses a pretrain word2vec.

• DRESS and DRESS-LS (Zhang and Lapata,
2017) are two deep reinforcement learning

• Hybrid (Narayan and Gardent, 2014) is a hy-
brid approach which combines deep seman-
tics and mono-lingual machine translation.

• SBMT-SARI (Xu et al., 2016) is a syntax-
based machine translation model which is
trained on PPDB dataset (Ganitkevitch et al.,
2013) and tuned with SARI.

3.1.5 Results

We compare WEAN with state-of-the-art mod-
els for text simpliﬁcation. Table 1 and Table 2
summarize the results of the automatic evalua-
tion. On PWKP dataset, we compare WEAN with
PBMT, Hybrid, EncDecA, DRESS and DRESS-
LS. WEAN achieves a BLEU score of 54.54, out-
performing all of the previous systems. On EW-
SEW dataset, we compare WEAN with PBMT-R,
Hybrid, SBMT-SARI, and the neural models de-
scribed above. We do not ﬁnd any public release
code of PBMT-R and SBMT-SARI. Fortunately,
Xu et al. (2016) provides the predictions of PBMT-
R and SBMT-SARI on EW-SEW test set, so that
we can compare our model with these systems.

LCSTS

R-1 R-2 R-L
15.8
8.5
17.7
RNN-W(Hu et al., 2015)
RNN(Hu et al., 2015)
18.6
8.9
21.5
RNN-cont-W(Hu et al., 2015) 26.8 16.1 24.1
RNN-cont(Hu et al., 2015)
29.9 17.4 27.2
33.3 20.0 30.1
SRB(Ma et al., 2017)
CopyNet-W(Gu et al., 2016)
35.0 22.3 32.0
34.4 21.6 31.3
CopyNet(Gu et al., 2016)
RNN-dist(Chen et al., 2016)
35.2 22.6 32.5
37.0 24.2 34.2
DRGD(Li et al., 2017)
32.1 19.9 29.2
Seq2seq
37.8 25.6 35.2
WEAN

Table 4: ROUGE F1 score on the LCSTS test set. R-
1, R-2, and R-L denote ROUGE-1, ROUGE-2, and
ROUGE-L, respectively. The models with a sufﬁx of
‘W’ in the table are word-based, while the rest of mod-
els are character-based.

It shows that the neural models have better per-
formance in BLEU, and WEAN achieves the best
BLEU score with 94.45.

We perform the human evaluation of WEAN
and other related systems, and the results are
shown in Table 3. DRESS-LS is based on the rein-
forcement learning, and it encourages the ﬂuency,
simplicity and relevance of the outputs. There-
fore, it achieves a high score in our human eval-
uation. WEAN gains a even better score than
DRESS-LS. Besides, WEAN generates more ad-
equate and simpler outputs than the reference on
PWKP. The predictions of SBMT-SARI are the
most adequate among the compared systems on
EW-SEW. In general, WEAN outperforms all of
the other systems, considering the balance of ﬂu-
ency, adequate and simplicity. We conduct sig-
niﬁcance tests based on t-test. The signiﬁcance
tests suggest that WEAN has a very signiﬁcant
improvement over baseline, with p ≤ 0.001 over
DRESS-LS in all of the dimension on PWKP,
p ≤ 0.05 over DRESS-LS in the dimension of ﬂu-
ency, p ≤ 0.005 over NTS-w2v in the dimension
of simplicity and p ≤ 0.005 over DRESS-LS in
the dimension of all.

3.2 Large Scale Text Summarization

3.2.1 Dataset

Large Scale Chinese Social Media Short Text
Summarization Dataset (LCSTS): LCSTS is
constructed by Hu et al. (2015). The dataset con-
sists of more than 2,400,000 text-summary pairs,
constructed from a famous Chinese social media

website called Sina Weibo.4 It is split into three
parts, with 2,400,591 pairs in PART I, 10,666 pairs
in PART II and 1,106 pairs in PART III. All the
text-summary pairs in PART II and PART III are
manually annotated with relevant scores ranged
from 1 to 5. We only reserve pairs with scores
no less than 3, leaving 8,685 pairs in PART II
and 725 pairs in PART III. Following the previous
work (Hu et al., 2015), we use PART I as training
set, PART II as validation set, and PART III as test
set.

is

metric

3.2.2 Evaluation Metrics
Our
ROUGE
evaluation
score (Lin and Hovy, 2003), which is popu-
lar for summarization evaluation. The metrics
compare an automatically produced summary
against the reference summaries, by computing
overlapping lexical units, including unigram, bi-
gram, trigram, and longest common subsequence
(LCS). Following previous work (Rush et al.,
2015; Hu et al., 2015), we use ROUGE-1 (un-
igram), ROUGE-2 (bi-gram) and ROUGE-L
(LCS) as the evaluation metrics in the reported
experimental results.

3.2.3 Settings
The vocabularies are extracted from the training
sets, and the source contents and the summaries
share the same vocabularies. We tune the hyper-
parameters based on the ROUGE scores on the
In order to alleviate the risk of
validation sets.
word segmentation mistakes, we split the Chi-
nese sentences into characters. We prune the vo-
cabulary size to 4,000, which covers most of the
common characters. We set the word embed-
ding size and the hidden size to 512, the num-
ber of LSTM layers of the encoder is 2, and
the number of LSTM layers of the decoder is
1. The batch size is 64, and we do not use
dropout (Srivastava et al., 2014) on this dataset.
Following the previous work (Li et al., 2017), we
implement a beam search optimization, and set the
beam size to 5.

3.2.4 Baselines
We compare our model with the state-of-the-art
baselines.

• RNN and RNN-cont are two sequence-to-
sequence baseline with GRU encoder and de-
coder, provided by Hu et al. (2015).

4http://weibo.com

#Param PWKP EWSEW LCSTS
12.80M 12.80M 2.05M
Seq2seq
0.52M
0.13M
0.13M
WEAN

Table 5: The number of the parameters in the out-
put layer. The numbers of rest parameters between
Seq2seq and WEAN are the same.

• RNN-dist (Chen et al., 2016) is a distraction-
based neural model, which the attention
mechanism focuses on the different parts of
the source content.

• CopyNet (Gu et al., 2016) incorporates a
copy mechanism to allow part of the gener-
ated summary is copied from the source con-
tent.

• SRB (Ma et al., 2017)

is a sequence-to-
sequence based neural model with improving
the semantic relevance between the input text
and the output summary.

• DRGD (Li et al., 2017) is a deep recurrent
generative decoder model, combining the de-
coder with a variational autoencoder.

• Seq2seq is our

the
sequence-to-sequence model with the atten-
tion mechanism.

implementation of

3.2.5 Results

We report the ROUGE F1 score of our model
and the baseline models on the test sets. Ta-
ble 4 summarizes the comparison between our
model and the baselines. Our model achieves
the score of 37.8 ROUGE-1, 25.6 ROUGE-2, and
35.2 ROUGE-L, outperforming all of the previ-
ous models. First, we compare our model with
It shows that
the sequence-to-sequence model.
our model signiﬁcant outperforms the sequence-
to-sequence baseline with a large margin of 5.7
ROUGE-1, 5.7 ROUGE-2, and 6.0 ROUGE-
L. Then, we compare our model with other
related models.
is
DRGD (Li et al., 2017), which obtains the score
of 37.0 ROUGE-1, 24.2 ROUGE-2, and 34.2
ROUGE-L. Our model has a relative gain of 0.8
ROUGE-1, 1.4 ROUGE-2 and 1.0 ROUGE-L over
the state-of-the-art models.

The state-of-the-art model

Training Curve

 

U
E
L
B

50

40

30

20

10

 

0
0

WEAN
S2S

2

4

6

8

10 12 14 16

Epoch

Figure 2: The training curve of WEAN and Seq2seq on
the PWKP validation set.

4 Analysis and Discussion

4.1 Reducing Parameters

Our WEAN reduces a large number of the param-
eters in the output layer. To analyze the parame-
ter reduction, we compare our WEAN model with
the sequence-to-sequence model. Table 5 lists the
number of the parameters in the output layers of
two models. Both PWKP and EWSEWhave the
vocabulary size of 50000 words and the hidden
size of 256, resulting 50000 × 256 = 12, 800, 000
parameters. LCSTS has a vocabulary size of 4000
and the hidden size of 512, so the seq2seq has
4000 × 512 = 2, 048, 000 parameters in the out-
put layers. WEAN only has two parameter ma-
trices and one parameter vector at most in Equa-
tion 5, without regard to the vocabulary size.
It
has 256 × 256 × 2 + 256 = 131, 328 parameters
on PWKP and EWSEW, and 512×512×2+512 =
524, 800 parameters on LCSTS. Besides, WEAN
does not have any extra parameters in the other
part of the model.

4.2 Speeding up Convergence

Figure 2 shows the training curve of WEAN and
Seq2seq on the PWKP validation set. WEAN
achieve near the optimal score in only 2-3 epochs,
while Seq2seq takes more than 15 epochs to
achieve the optimal score. Therefore, WEAN
has much faster convergence rate, compared with
Seq2seq. With the much faster training speed,
WEAN does not suffer loss in BLEU, and even
improve the BLEU score.

Yoghurt oryogurt isadairyproduct produced bybacterial fermentation ofmilk.
Yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk.
. oryoghurtisadairy product producedbybacterial fermentation ofmilk.
Itismadebybacterial fermentation ofmilk.
Yoghurt oryogurt isadairyproduct producedbybacterial fermentation of.

Source
Reference
NTS
NTS-w2v
PBMT-R
SBMT-SARI Yogurtoryogurt isadairy product drawnupbybacterial fermentation ofmilk.
WEAN
Source

Yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk.
Depending on the context, another closely-related meaning of constituent is that of a
citizen residing in the area governed, represented, or otherwise served by a politician;
sometimes thisisrestricted tocitizens whoelected thepolitician.
Thewordconstituentcanalsobeusedtorefertoacitizenwholivesintheareathat
is governed, represented, or otherwise served by a politician; sometimes the word is
restricted tocitizens whoelected thepolitician.
Depending on the context, another closely-related meaning of constituent is that of a
citizen living in the area governed, represented, or otherwise served by a politician;
sometimes thisisrestricted tocitizens whoelected thepolitician.
Thisisrestricted tocitizens whoelected thepolitician.
Depending on the context and meaning of closely-related siemens-martin -rrb- is a
citizen living in the area, or otherwise, was governed by a 1924-1930 shurba; this is
restricted topeople whoelected it.

Reference

NTS

NTS-w2v
PBMT-R

SBMT-SARI In terms of the context, another closely-related sense of the component is that of a
citizen living in the area covered, makeup, or ifnot, served byapolicy; sometimes
thisislimitedtothepeople whoelected thepolicy.
Depending on the context, another closely-related meaning of constituent is that of a
citizenwholivesintheareagoverned,represented, orotherwiseservedbyapolitician;
sometimes thewordisrestricted tocitizens whoelected thepolitician.

WEAN

Table 6: Two examples of different text simpliﬁcation system outputs in EW-SEW dataset. Differences from the
source texts are shown in bold.

4.3 Case Study

and very close to the original meaning.

Table 6 shows two examples of different text sim-
pliﬁcation system outputs on EW-SEW. For the
ﬁrst example, NTS, NTS-w2v and PBMT-R miss
some essential constituents, so that the sentences
are incomplete and not ﬂuent. SBMT-SARI gen-
erates a ﬂuent sentence, but the output does not
preserve the original meaning. The predicted sen-
tence of WEAN is ﬂuent, simple, and the same
as the reference. For the second example, NTS-
w2v omits so many words that
it lacks a lot
of information. PBMT-R generates some irrele-
vant words, like ’siemens-martin’, ’-rrb-’, and ’-
shurba’, which hurts the ﬂuency and adequacy of
the generated sentence. SBMT-SARI is able to
generate a ﬂuent sentence, but the meaning is dif-
ferent from the source text, and even more difﬁ-
cult to understand. Compared with the statistic
model, WEAN generates a more ﬂuent sentence.
Besides, WEAN can capture the semantic mean-
ing of the word by querying the word embeddings,
so the generated sentence is semantically correct,

5 Related Work

Our work is related to the encoder-decoder
framework (Cho et al., 2014) and the attention
mechanism (Bahdanau et al., 2014).
Encoder-
decoder framework,
like sequence-to-sequence
model, has achieved success in machine trans-
lation (Sutskever et al., 2014; Jean et al., 2015;
Luong et al., 2015; Lin et al., 2018),
text sum-
marization
(Rush et al.,
2015; Chopra et al.,
2016; Nallapati et al., 2016; Wang et al., 2017;
Ma and Sun, 2017), and other natural language
processing tasks (Liu et al., 2017). There are
many other methods to improve neural attention
model (Jean et al., 2015; Luong et al., 2015).

Zhu et al.

(2010) constructs a wikipedia
dataset, and proposes a tree-based simpliﬁcation
model. Woodsend and Lapata (2011) introduces
a data-driven model based on quasi-synchronous
grammar, which captures structural mismatches
and complex rewrite operations. Wubben et al.

(2012) presents a method for text simpliﬁcation
using phrase based machine translation with re-
ranking the outputs. Kauchak (2013) proposes a
text simpliﬁcation corpus, and evaluates language
modeling for text simpliﬁcation on the proposed
corpus. Narayan and Gardent (2014) propose a
hybrid approach to sentence simpliﬁcation which
combines deep semantics and monolingual ma-
chine translation. Hwang et al. (2015) introduces
a parallel simpliﬁcation corpus by evaluating the
similarity between the source text and the simpli-
ﬁed text based on WordNet. Glavaˇs and ˇStajner
(2015) propose an unsupervised approach to lex-
ical simpliﬁcation that makes use of word vec-
tors and require only regular corpora. Xu et al.
(2016) design automatic metrics for text sim-
pliﬁcation. Recently, most works focus on the
neural sequence-to-sequence model. Nisioi et al.
(2017) present a sequence-to-sequence model, and
re-ranks the predictions with BLEU and SARI.
Zhang and Lapata (2017) propose a deep rein-
forcement learning model to improve the simplic-
ity, ﬂuency and adequacy of the simpliﬁed texts.
Cao et al. (2017) introduce a novel sequence-to-
sequence model to join copying and restricted gen-
eration for text simpliﬁcation.

Rush et al. (2015) ﬁrst used an attention-based
encoder to compress texts and a neural network
language decoder to generate summaries. Follow-
ing this work, recurrent encoder was introduced
to text summarization, and gained better perfor-
mance (Lopyrev, 2015; Chopra et al., 2016). To-
wards Chinese texts, Hu et al. (2015) built a large
corpus of Chinese short text summarization. To
deal with unknown word problem, Nallapati et al.
(2016) proposed a generator-pointer model so that
the decoder is able to generate words in source
texts. Gu et al. (2016) also solved this issue by
incorporating copying mechanism.

6 Conclusion

We propose a novel model based on the encoder-
decoder framework, which generates the words by
querying distributed word representations. Exper-
imental results show that our model outperforms
the sequence-to-sequence baseline by the BLEU
score of 6.3 and 5.5 on two English text simpliﬁ-
cation datasets, and the ROUGE-2 F1 score of 5.7
on a Chinese summarization dataset. Moreover,
our model achieves state-of-the-art performances
on these three benchmark datasets.

Acknowledgements

This work was supported in part by National Natu-
ral Science Foundation of China (No. 61673028),
National High Technology Research and Devel-
opment Program of China (863 Program, No.
2015AA015404), and the National Thousand
Young Talents Program. Xu Sun is the corre-
sponding author of this paper.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473.

Ziqiang Cao, Chuwei Luo, Wenjie Li, and Sujian Li.
2017.
Joint copying and restricted generation for
paraphrase. In Proceedings of the Thirty-First AAAI
Conference on Artiﬁcial Intelligence. pages 3152–
3158.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and
Hui Jiang. 2016. Distraction-based neural networks
for modeling documents. In Proceedings of the 25th
International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI 2015). AAAI, New York, NY.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.

Kyunghyun Cho, Bart van Merrienboer, C¸ aglar
G¨ulc¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
In Proceedings
for statistical machine translation.
of the 2014 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2014. pages
1724–1734.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. pages 93–
98.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013.
the paraphrase
database. In Human Language Technologies: Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, Proceedings.
pages 758–764.

PPDB:

Goran Glavaˇs and Sanja ˇStajner. 2015. Simplifying
lexical simpliﬁcation: Do we need simpliﬁed cor-
pora? In Proceedings of the 53rd Annual Meeting of

the Association for Computational Linguistics, ACL.
pages 63–68.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Incorporating copying mechanism in
Li. 2016.
In Proceedings of
sequence-to-sequence learning.
the 54th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2016.

Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LC-
STS: A large scale chinese short text summarization
dataset. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015. pages 1967–1972.

William Hwang, Hannaneh Hajishirzi, Mari Ostendorf,
and Wei Wu. 2015. Aligning sentences from stan-
dard wikipedia to simple wikipedia. In NAACL HLT
2015. pages 211–217.

S´ebastien Jean, KyungHyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation.
In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2015.
pages 1–10.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
In Proceedings of the
neural machine translation.
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015. pages 1412–
1421.

Shuming Ma and Xu Sun. 2017. A semantic rele-
vance based neural network for text summarization
and text simpliﬁcation. CoRR abs/1710.02318.

Shuming Ma, Xu Sun, Jingjing Xu, Houfeng Wang,
Wenjie Li, and Qi Su. 2017.
Improving semantic
relevance for sequence-to-sequence learning of chi-
nese social media text summarization. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 2: Short
Papers. pages 635–640.

Ramesh Nallapati, Bowen Zhou, C´ıcero Nogueira dos
Santos, C¸ aglar G¨ulc¸ehre, and Bing Xiang. 2016.
Abstractive text summarization using sequence-to-
In Proceedings of the
sequence rnns and beyond.
20th SIGNLL Conference on Computational Natural
Language Learning, CoNLL 2016, Berlin, Germany,
August 11-12, 2016. pages 280–290.

David Kauchak. 2013.

Improving text simpliﬁcation
language modeling using unsimpliﬁed text data. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL. pages
1537–1546.

Shashi Narayan and Claire Gardent. 2014. Hybrid sim-
pliﬁcation using deep semantics and machine trans-
In Proceedings of the 52nd Annual Meet-
lation.
ing of the Association for Computational Linguis-
tics, ACL. pages 435–445.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR

A method for stochastic optimization.
abs/1412.6980.

Piji Li, Wai Lam, Lidong Bing, and Zihao Wang.
2017. Deep recurrent generative decoder for ab-
stractive text summarization. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2017, Copenhagen,
Denmark, September 9-11, 2017. pages 2091–2100.

Chin-Yew Lin and Eduard H. Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics, HLT-
NAACL 2003.

Junyang Lin, Shuming Ma, Qi Su, and Xu Sun.
2018. Decoding-history-based adaptive control of
attention for neural machine translation. CoRR
abs/1802.01812.

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,
and Zhifang Sui. 2017.
Table-to-text genera-
tion by structure-aware seq2seq learning. CoRR
abs/1711.09724.

Sergiu Nisioi, Sanja Stajner, Simone Paolo Ponzetto,
and Liviu P. Dinu. 2017. Exploring neural text sim-
pliﬁcation models. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL. pages 85–91.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics. pages 311–318.

Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek V.
Datla, Ashequl Qadir, Joey Liu, and Oladimeji Farri.
2016. Neural paraphrase generation with stacked
In COLING 2016, 26th
residual LSTM networks.
International Conference on Computational Lin-
guistics, Proceedings of the Conference: Techni-
cal Papers, December 11-16, 2016, Osaka, Japan.
pages 2923–2934.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
In Proceedings of the 2015
tence summarization.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015. pages 379–389.

Konstantin Lopyrev. 2015. Generating news head-
CoRR

lines with recurrent neural networks.
abs/1512.01712.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural

Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze
Chen, and Chris Callison-Burch. 2016. Optimizing
statistical machine translation for text simpliﬁcation.
TACL 4:401–415.

Xingxing Zhang and Mirella Lapata. 2017.

Sen-
tence simpliﬁcation with deep reinforcement learn-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September
9-11, 2017. pages 584–594.

Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simpliﬁcation. In COLING 2010. pages
1353–1361.

networks from overﬁtting.
Learning Research 15(1):1929–1958.

Journal of Machine

Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng
Wang. 2017a. meprop: Sparsiﬁed back propaga-
tion for accelerated deep learning with reduced over-
In Proceedings of the 34th International
ﬁtting.
Conference on Machine Learning, ICML 2017, Syd-
ney, NSW, Australia, 6-11 August 2017. pages 3299–
3308.

Xu Sun, Xuancheng Ren, Shuming Ma, Bingzhen Wei,
Wei Li, and Houfeng Wang. 2017b. Training simpli-
ﬁcation and model simpliﬁcation for deep learning:
A minimal effort back propagation method. CoRR
abs/1711.06528.

Xu Sun, Bingzhen Wei, Xuancheng Ren, and Shuming
Ma. 2017c. Label embedding network: Learning la-
bel representation for soft training of deep networks.
CoRR abs/1710.10393.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural Infor-
mation Processing Systems 2014. pages 3104–3112.

Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hi-
rao, and Masaaki Nagata. 2016. Neural headline
generation on abstract meaning representation.
In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2016, Austin, Texas, USA, November 1-4, 2016.
pages 1054–1059.

Kexiang Wang, Tianyu Liu, Zhifang Sui, and Baobao
Chang. 2017. Afﬁnity-preserving random walk for
multi-document summarization. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017. pages 210–
220.

Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing, EMNLP. pages 409–
420.

Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simpliﬁcation by mono-
In The 50th An-
lingual machine translation.
nual Meeting of the Association for Computational
Linguistics, Proceedings of the Conference. pages
1015–1024.

Jingjing Xu, Xu Sun, Xuancheng Ren, Junyang Lin,
Binzhen Wei, and Wei Li. 2018. Dp-gan: Diversity-
promoting generative adversarial network for gen-
CoRR
erating informative and diversiﬁed text.
abs/1802.01345.

