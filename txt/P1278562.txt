8
1
0
2
 
g
u
A
 
2
1

 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
3
2
4
6
0
.
0
1
5
1
:
v
i
X
r
a

Optimization as Estimation with Gaussian Processes in Bandit Settings

Zi Wang
MIT CSAIL

Bolei Zhou
MIT CSAIL

Stefanie Jegelka
MIT CSAIL

Abstract

there has been rising interest

Recently,
in
Bayesian optimization – the optimization of an
unknown function with assumptions usually ex-
pressed by a Gaussian Process (GP) prior. We
study an optimization strategy that directly uses
an estimate of the argmax of the function. This
strategy offers both practical and theoretical ad-
vantages: no tradeoff parameter needs to be se-
lected, and, moreover, we establish close connec-
tions to the popular GP-UCB and GP-PI strate-
gies. Our approach can be understood as auto-
matically and adaptively trading off exploration
and exploitation in GP-UCB and GP-PI. We il-
lustrate the effects of this adaptive tuning via
bounds on the regret as well as an extensive em-
pirical evaluation on robotics and vision tasks,
demonstrating the robustness of this strategy for
a range of performance criteria.

1 Introduction

The optimization of an unknown function that is expensive
to evaluate is an important problem in many areas of sci-
ence and engineering. Bayesian optimization uses proba-
bilistic methods to address this problem. In particular, an
increasingly popular direction has been to model smooth-
ness assumptions on the function via a Gaussian Process
(GP). The Bayesian approach provides a posterior distri-
bution of the unknown function, and thereby uncertainty
estimates that help decide where to evaluate the function
next, in search of a maximum. Recent successful appli-
cations of this Bayesian optimization framework include
the tuning of hyperparameters for complex models and al-
gorithms in machine learning, robotics, and computer vi-
sion [3, 6, 16, 19, 29, 33].

Despite progress on theory and applications of Bayesian

Appearing in Proceedings of the 19th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS) 2016, Cadiz,
Spain. JMLR: W&CP volume 51. Copyright 2016 by the authors.

optimization methods, the practitioner continues to face
many options: there is a menu of algorithms, and their rela-
tions and tradeoffs are only partially understood. Typically,
the points where the function is evaluated are selected se-
quentially; and the choice of the next point is based on ob-
served function values at the previous points. Popular al-
gorithms vary in their strategies to pick the next point: they
select the point that maximizes the probability of improve-
ment (GP-PI) [17]; the expected improvement (GP-EI) [22];
or an upper conﬁdence bound (GP-UCB) [30] on the max-
imum function value. Another alternative is entropy search
(ES) [12], which aims to minimize the uncertainty about
the location of the optimum of the function. Each algo-
rithm reduces the black-box function optimization problem
to a series of optimization problems of known acquisition
functions.

The motivations and analyses (if available) differ too: ob-
jectives include cumulative regret, where every evaluation
results in a reward or cost and the average of all function
evaluations is compared to the maximum value of the func-
tion; simple regret that takes into account only the best
value found so far [4]; the performance under a ﬁxed ﬁ-
nite budget [11]; or the uncertainty about the location of
the function maximizer [12]. Here, we focus on the estab-
lished objectives of cumulative regret in bandit games.

Notably, many of the above algorithms involve tuning a pa-
rameter to trade off exploration and exploitation, and this
can pose difﬁculties in practice [12]. Theoretical analyses
help in ﬁnding good parameter settings, but may be con-
servative in practice [30]. Computing the acquisition func-
tion and optimizing it to ﬁnd the next point can be com-
putationally very costly too. For example, the computation
to decide which next point to evaluate for entropy search
methods tends to be very expensive, while GP-PI, GP-EI
and GP-UCB are much cheaper.

In this paper, we study an intuitive strategy that offers a
compromise between a number of these approaches and, at
the same time, establishes connections between them that
help understand when theoretical results can be transferred.
Our strategy uses the Gaussian Process to obtain an esti-
mate of the argument that maximizes the unknown function
f . The next point to evaluate is determined by this estimate.
This point, it turns out, is not necessarily the same as the

Optimization as Estimation with Gaussian Processes in Bandit Settings

the argument with the highest upper conﬁdence bound.

This strategy has both practical and theoretical advantages.
On the theoretical side, we show connections to the popular
GP-UCB and GP-PI strategies, implying an intuitive and
provably correct way of setting the parameters in those im-
portant methods. Moreover, we establish bounds on the re-
gret of our estimation strategy. From a practical viewpoint,
our strategy obviates any costly parameter tuning. In fact,
we show that it corresponds to automatically and adap-
tively tuning the parameters of GP-UCB and GP-PI. Our
empirical evaluation includes problems from non-convex
optimization, robotics, and computer vision. The experi-
ments show that our strategy performs similarly to or even
better than the best competitors in terms of cumulative re-
gret. Although not designed to minimize simple regret di-
rectly, in practice our method also works well as measured
by simple regret, or by the number of steps to reach a ﬁxed
regret value. Together, these results suggest that our strat-
egy is easy to use and empirically performs well across a
spectrum of settings.

Related work. The practical beneﬁts of Bayesian opti-
mization have been shown in a number of applications [3,
6, 19, 29, 33, 34]. Different Bayesian optimization algo-
rithms differ in the selection criteria of the next point to
evaluate, i.e., the acquisition function. Popular criteria in-
clude the expected improvement (GP-EI) [22], the prob-
ability of improving over a given threshold (GP-PI) [17],
and GP-UCB [30], which is motivated by upper conﬁdence
bounds for multi-armed bandit problems [2, 1]. GP-EI,
GP-PI and GP-UCB have a parameter to select, and the
latter two are known to be sensitive to this choice. En-
tropy search (ES) [12] and the related predictive entropy
search (PES) [13] do not aim to minimize regret directly,
but to maximize the amount of information gained about
the optimal point. High-dimensional settings were con-
sidered in [7, 34]. Extensive empirical comparisons in-
clude [6, 29, 20]. Theoretical bounds on different forms
of regret were established for GP-UCB [30] and GP-EI [5].
Other theoretical studies focus on simple regret [4, 11] or
ﬁnite budgets [11]. In this work, in contrast, we are moti-
vated by practical considerations.

1.1 Background and Notation

)

∼

GP (0, k) be an unknown function we aim
Let f (
·
to optimize over a candidate set X. At time step t,
we select point xt and observe a possibly noisy func-
tion evaluation yt = f (xt) + ǫt, where ǫt are i.i.d.
(0, σ2). Given the observations Dt =
Gaussian noise
t
(xτ , yτ )
τ =1 up to time t, we obtain the posterior mean
{
}
and covariance of the function via the kernel matrix Kt =
[k(xi, xj)]xi,xj∈Dt
and kt(x) = [k(xi, x)]xi∈Dt [24]:
µt(x) = kt(x)T(Kt + σ2I)−1yt, and kt(x, x′) =
kt(x)T(Kt +σ2I)−1kt(x′). The posterior vari-
k(x, x′)

N

−

ance is given by σ2
note by Q(
·
tribution φ(
·

t (x) = kt(x, x). Furthermore, we de-
) the tail probability of the standard normal dis-
), and by Φ(
·

) its cumulative probability.

The Bayesian Optimization setting corresponds to a bandit
game where, in each round t, the player chooses a point xt
and then observes yt = f (xt)+ǫt. The regret for round t is
f (xt). The simple regret
deﬁned as ˜rt = maxx∈X f (x)
for any T rounds is rT = mint∈[1,T ] ˜rt, and the (average)
cumulative regret is RT = 1
T

T
t=1 ˜rt.

−

1.2 Existing methods for GP optimization

P

We focus on the following three approaches for compari-
son, since they are most widely used in bandit settings.

GP-UCB. Srinivas et al. [30] provide a detailed analysis
for using upper conﬁdence bounds [2] with GP bandits.
They propose the strategy xt = arg maxx∈X µt−1(x) +
π2t2/(6δ)))
λtσt−1(x) where λt = (2 log(
2 for ﬁnite
|
X. Their regret bound holds with probability 1

δ.

X

|

1

−

GP-EI. The GP-EI strategy [22] selects the point max-
imizing the expected improvement over a pre-speciﬁed
threshold θt [22]. For GPs, this improvement is given
in closed form as EI(x) = E[(f (x)
θt)+] =
[φ(γ(x))
γ(x)Q(γ(x))] σt−1(x), where γ(x) =
−
θt−µt−1(x)
. A popular choice for the threshold is θt =
σt−1(x)

−

maxτ ∈[1,t−1] yτ .

GP-PI. The third strategy maximizes the probability
Φ(γ(x)) of improving over
PI(x) = Pr[f (x) > θt] = 1
−
a threshold θt [17], i.e., xt = arg minx∈X γ(x). GP-PI is
sensitive to the choice of θt: as we will see in Section 2, θt
trades off exploration and exploitation, and setting θt too
low (e.g., θt = maxτ ∈[1,t−1] yτ ) can result in getting stuck
at a fairly suboptimal point. A popular choice in practice is
θt = maxτ ∈[1,t−1] yτ + ǫ, for a chosen constant ǫ.

2 Optimization as estimation

In this work, we study an alternative criterion that provides
an easy-to-use and tuning-free approach: we use the GP
to estimate the arg max of f . In Section 2.1, we will see
how, as a side effect, this criterion establishes connections
between the above criteria. Our strategy eventually leads to
tighter bounds than GP-UCB as shown in Section 3.

∈

Consider the posterior probability (in round t) that a ﬁxed
X is an arg max of f . We call this event Mx
x
and, for notational simplicity, omit the subscripts t
1
here. The event Mx is equivalent to the event that for all
x′
0. The dif-
∈
ference v(x′) between two Gaussian variables is Gaussian:
v(x′)
2k(x, x′)2).
(µ(x′)
−
∼ N
−
X is Cov(v(x′), v(x′′)) =
The covariance for any x′, x′′
k(x, x′′)2.
σ(x)2 + k(x′, x′′)2

X, we have v(x′) := f (x′)

µ(x), σ(x)2 + σ(x′)2

∈
k(x, x′)2

f (x)

−

−

≤

−

−

Zi Wang, Bolei Zhou, Stefanie Jegelka

The random variables
tive probability

v(x′)
}

{

x′∈X determine the cumula-

Pr[Mx

D] = Pr[

X, v(x′)

0

D].

(1)

|

x′
∀

∈

≤

|

This probability may be speciﬁed via limits as e.g. in [12,
App.A]. Moreover, due to the assumed smoothness of f ,
it is reasonable to work with a discrete approximation and
restrict the set of candidate points to be ﬁnite for now (we
discuss discretization further in Section 5). So the quantity
D] for
in Eqn. (1) is well-deﬁned. Since computing Pr[Mx
X
can be costly, we use a “mean-ﬁeld” approach and
large
|
approximate
x∈X by independent Gaussian random
variables with means µ(x) and variances σ(x)2 for all x
∈
X. Given be the maximum value m of f , the probability of
the event Mx

m, D amounts to

f (x)
}

{

|

|

|
m, D]

Pr[Mx

|

Q

≈

m

µ(x)

−
σ(x)

Φ

m

µ(x′)

−
σ(x′)

.

(cid:16)
(cid:16)
to evaluate
Our estimation strategy (EST) chooses
ˆm, D] next, which is the function in-
arg maxx∈X Pr[Mx
put that is most likely to achieve the highest function value.

(cid:17)

|

x′6=x
(cid:17) Y

Of course, the function maximum m may be unknown. In
this case, we use a plug-in estimate via the posterior expec-
tation of Y = maxx∈X f (x) given D [26]:

D]

ˆm = E[Y
|
∞
Pr[Y > y

=

0
Z

D]

|

−

Pr[Y <

y

D] dy.

−

|

If the noise in the observations is negligible, we can sim-
plify Eqn. (3) to be

ˆm = m0 +

∞

1

−

Φ

w

µ(x)

−
σ(x)

dw

(4)

m0

Z

(cid:16)

(cid:17)

x∈X
Y
where m0 = maxτ ∈[1,t−1] yτ is the current observed max-
imum value. Under conditions speciﬁed in Section 3, our
approximation with the independence assumption makes ˆm
an upper bound on m, which, as we will see, conservatively
emphasizes exploration a bit more. Other ways of setting
ˆm are discussed in Section 5.

2.1 Connections to GP-UCB and GP-PI

Next, we relate our strategy to GP-PI and GP-UCB: EST
turns out to be equivalent to adaptively tuning θt in GP-
PI and λt in GP-UCB. This observation reveals unifying
connections between GP-PI and GP-UCB and, in Section 3,
yields regret bounds for GP-PI with a certain choice of θt.
Lemma 2.1 characterizes the connection to GP-UCB:
Lemma 2.1.
the point selected by
EST is the same as the point selected by a variant of
. Conversely,
GP-UCB with λt = minx∈X
the candidate selected by GP-UCB is the same as the
candidate selected by a variant of EST with ˆmt =
maxx∈X µt−1(x) + λtσt−1(x).

ˆmt−µt−1(x)
σt−1(x)

In any round t,

Proof. We omit the subscripts t for simplicity. Let a be the
point selected by GP-UCB, and b selected by EST. Without
loss of generality, we assume a and b are unique. With
λ = minx∈X

, GP-UCB chooses to evaluate

ˆm−µ(x)
σ(x)

a = arg maxx∈X µ(x)+λσ(x) = arg minx∈X

ˆm

µ(x)

−
σ(x)

.

This is because

ˆm = max
x∈X

µ(x) + λσ(x) = µ(a) + λσ(a).

By deﬁnition of b, for all x

X, we have

Pr[Mb
|
Pr[Mx
|

ˆm, D]
ˆm, D] ≈

∈
Q( ˆm−µ(b)
σ(b)
Q( ˆm−µ(x)
σ(x)

)

)Φ( ˆm−µ(x)
σ(x)
)Φ( ˆm−µ(b)
σ(b)

) ≥

1.

The inequality holds if and only if ˆm−µ(b)
X, including a, and hence
all x

σ(b) ≤

ˆm−µ(x)
σ(x)

for

∈
ˆm

µ(b)
−
σ(b) ≤

ˆm

µ(a)

−
σ(a)

= λ = min
x∈X

ˆm

µ(x)

−
σ(x)

,

which, with uniqueness, implies that a = b and GP-UCB
and EST select the same point.

The other direction of the proof is similar and can be found
in the supplement.

Proposition 2.2. GP-PI is equivalent to EST when setting
θt = ˆmt in GP-PI.

(2)

(3)

As a corollary of Lemma 2.1 and Proposition 2.2, we obtain
a correspondence between GP-PI and GP-UCB.

Corollary 2.3. GP-UCB is equivalent to GP-PI if λt is
, and GP-PI corresponds to GP-
set to minx∈X
UCB if θt = maxx∈X µt−1(x) + λtσt−1(x).

θt−µt−1(x)
σt−1(x)

Algorithm 1 GP-UCB/PI/EST
1: t ← 1; D0 ← ∅
2: while stopping criterion not reached do
µt−1, Σt−1 ← GP-predict(X|Dt−1)
3:

ˆmt = 


maxx∈X µt−1(x) + λtσt−1(x)
max1≤τ <t yτ + ǫ
E[Y |Dt−1] (see Eqn. (3)/Eqn. (4)) EST

GP-UCB
GP-PI

4:

5:



ˆmt−µt−1(x)
σt−1(x)

xt ← arg minx∈X
yt ← f (xt) + ǫt, ǫt ∼ N (0, σ2)
Dt ← {xτ , yτ }t
τ =1
t ← t + 1

6:
7:
8:
9: end while

Proposition 2.2 suggests that we do not need to calcu-
ˆmt, Dt−1] directly when im-
late the probability Pr[Mx
|
plementing EST. Instead, we can reduce EST to GP-PI
with an automatically tuned target value θt. Algorithm 1
compares the pseudocode for all three methods. We use

Optimization as Estimation with Gaussian Processes in Bandit Settings

“GP-predict” to denote the update for the posterior mean
and covariance function for the GP as described in Sec-
tion 1.1. GP-UCB/PI/EST all share the same idea of reach-
ing a target value ( ˆmt in this case), and thereby trading
off exploration and exploitation. GP-UCB in [30] can
be interpreted as setting the target value to be a loose
upper bound maxx∈X µt−1(x) + λtσt−1(x) with λt =
1
2 , as a result of applying the union
(2 log(
|
bound over X1. GP-PI applies a ﬁxed upwards shift of ǫ
over the current maximum observation maxτ ∈[1,t−1] yτ . In
both cases, the exploration-exploitation tradeoff depends
on the parameter to be set. EST implicitly and automati-
cally balances the two by estimating the maximum. Viewed
as GP-UCB or GP-PI, it automatically sets the respective
parameter.

π2t2/6δ))

X

|

Note that this change by EST is not only intuitively reason-
able, but it also leads to vanishing regret, as will become
evident in the next section.

3 Regret Bounds

In this section, we analyze the regret of EST. We ﬁrst show
a bound on the cumulative regret both in expectation and
with high probability, with the assumption that our estima-
tion ˆmt is always an upper bound on the maximum of the
function. Then we interpret how this assumption is satis-
ﬁed via Eqn. (3) and Eqn. (4) under the condition speciﬁed
in Corollary 3.5.
Theorem 3.1. We assume ˆmt ≥
maxx∈X f (x),
∈
[1, T ], and restrict k(x, x′)
1. Let σ2 be the vari-
ance of the Gaussian noise in the observation, γT the
maximum information gain of the selected points, C =
2/ log(1 + σ−2), and t∗ = arg maxt νt where νt ,
. The cumulative expected regret sat-
minx∈X
T
E [˜rt|
Dt−1]
isﬁes
t=1
δ, it holds that
least 1
P
−
with ζT = (2 log( T
2 .

νt∗ √CT γT . With probability at
(νt∗ + ζT )√CT γT ,
t=1 ˜rt ≤

ˆmt−µt−1(x)
σt−1(x)

t
∀

≤

≤

T

1

2δ ))

P

The information gain γT after T rounds is the maxi-
mum mutual information that can be gained about f from
T measurements: γT = maxA⊆X,|A|≤T I(yA, fA) =
2 log det(I + σ−2KA). For the Gaussian
maxA⊆X,|A|≤T
kernel, γT = O((log T )d+1), and for the Mat´ern kernel,
γT = O(T d(d+1)/(2ξ+d(d+1)) log T ) where d is the dimen-
sion and ξ is the roughness parameter of the kernel [30,
Theorem 5].

1

The proof of Theorem 3.1 follows [30] and relies on the
following lemmas which are proved in the supplement.

1Since Pr[|f (x) − µ(x)| > λtσ(x)] ≤ e

, applying the
union bound results in Pr[|f (x) − µ(x)| > λtσ(x), ∀x ∈ X] ≤
−λ
2 . This means f (x) ≤ maxx∈X µ(x) + λtσ(x) with
|X|e
probability at least 1 − |X|e

[30, Lemma 5.1].

−λ
2

2
t

2
t

2
t

−λ
2

Lemma 3.2. Pick δ
T
t=1 π−1
where
that Pr[µt−1(xt)
P
[1, T ].
t

t ≤
−

∈

1
(0, 1) and set ζt = (2 log( πt
2 ,
∈
1, πt > 0. Then, for EST, it holds
δ, for all

ζtσt−1(xt)]

2δ ))

f (xt)

1

≤

≥

−

Lemma 3.2 is similar to but not exactly the same as [30,
Appendix A.1]: while they use a union bound over all of X,
here, we only need a union bound over the actually evalu-
T
ated points
t=1. This difference is due to the different
selection strategies.

xt}

{

f (xt)

Lemma 3.3. If µt−1(xt)
regret at time step t is upper bounded as ˜rt ≤
ζt)σt−1(xt) , where νt , minx∈X
maxx∈X f (x),
t
∀

ˆmt−µt−1(x)
σt−1(x)

[1, T ].

the
(νt +
, and ˆmt ≥

ζtσt−1(xt),

−

≤

∈

The proof of Theorem 3.1 now follows from Lemmas 3.2,
3.3, and Lemma 5.3 in [30].

Dt−1]

Proof. (Thm. 3.1) The expected regret of round t is
E[˜rt|
µt−1(xt) = νtσt−1(xt). Using
t∗ = arg maxt νt, we obtain that
νt∗

T
t=1 σt−1(xt).

E [˜rt|

ˆmt −

Dt−1]

T
t=1

≤

≤

P

2

−2

x

≤

≤

≤

k(xt, xt)

1 + ax for 0

To bound the sum of variances, we ﬁrst use that
P
(1 + a)x
1 and the
assumption σt−1(xt)
1 to obtain
log(1+σ
σ2
t−1(xt)
≤
implies that
≤
2
log(1+σ−2) γT . The Cauchy-Schwarz inequality leads to
T
2T γT
t=1 σ2
log(1+σ−2) .

. Lemma 5.3 in [30] now
log(1+σ−2) I(yT ; fT )

≤
t−1(xt))
σ
log(1+σ−2)
t−1(xt)

P
T
t=1 σt−1(xt)

t−1(xt)
Together, we have the ﬁnal regret bound
q
P

T
t=1 σ2

P

q

≤

≤

≤

≤

T

2

T

t=1
X

E [˜rt|

Dt−1]

νt∗

≤

s

2T
log(1 + σ−2)

γT .

Next we show a high probability bound. The condi-
tion of Lemma 3.3 holds with high probability because of
Lemma 3.2. Thus with probability at least 1
δ, the regret
for round t is bounded as follows,

−

˜rt ≤

(νt + ζt)σt−1(xt)

(νt∗ + ζT )σt−1(xt),

≤
2
2δ ), πt = π
6 , and t∗ = arg maxt νt.

2
t

where ζt = 2 log( πt
Therefore, with probability at least 1

δ,

−

T

t=1
X

˜rt ≤

(νt∗ + ζT )

s

2T γT
log(1 + σ−2)

.

Next we show that if we estimate ˆmt as described in Sec-
f (x)
tion 2 by assuming all the
x∈X are independent con-
}
ditioned on the current sampled data Dt, ˆmt can be guar-
anteed to be an upper bound on the function maximum m
given kt(x, x′)

X.

0,

{

x, x′
∀

∈

≥

Zi Wang, Bolei Zhou, Stefanie Jegelka

Lemma 3.4 (Slepian’s Comparison Lemma [28, 21]). Let
Rn be two multivariate Gaussian random vectors
u, v
with the same mean and variance, such that E[vivj]
E[supi∈[1,n] ui].
E[uiuj],

i, j. Then E[supi∈[1,n] vi]
∀

≥

≤

∈

Slepian’s Lemma implies a relation between our approxi-
mation ˆmt and m.
Corollary 3.5. Assume g
GP (µ, k) has posterior mean
X
µt(x) and posterior covariance kt(x, x′)
conditioned on Dt. Deﬁne a series of independent ran-
dom variables h(x) with equivalent mean µt(x) and poste-
rior variance kt(x, x),
E[supx∈X g(x)].

X. Then, E[supx∈X h(x)]

x, x′
∀

0,

≥

∼

≥

∈

∈

x

∀

Proof. By independence,

X

x, x′
∀
0 = E[h(x)h(x′)]
E[g(x)g(x′)]

−

∈
E[h(x)]E[h(x′)]
E[g(x)]E[g(x′)]

≤

−

Hence, Slepian’s Lemma implies that E[supx∈X h(x)]
E[supx∈X g(x)].

≥

X.
Corollary 3.5 assumes that kt(x, x′)
This depends on the choice of X and k. Notice that
kt(x, x′)
0 is only a sufﬁcient condition and, even if
the assumption fails, ˆmt is often still an upper bound on m
in practice (illustrations in the supplement).

x, x′
∀

0,

≥

≥

∈

In contrast, the results above are not necessarily true for
any arbitrary θt in GP-PI, an important distinction between
GP-PI and GP-EST.

Before evaluating the EST strategy empirically, we make
a few important observations. First, EST does not re-
quire manually setting a parameter that trades off explo-
ration and exploitation.
Instead, it corresponds to auto-
matically, adaptively setting the tradeoff parameters λt in
GP-UCB and θt in GP-PI. For EST, this means that if the
gap νt is large, then the method focuses more on explo-
ration, and if “good” function values (i.e., close to ˆmt) are
observed, then exploitation increases. If we write EST as
GP-PI, we see that by Eqn. (4), the estimated ˆmt always
ensures θt > maxτ ∈[1,T ] yτ , which is known to be advan-
tageous in practice [20]. These analogies likewise suggest
that θt = maxτ ∈[1,T ] yτ corresponds to a very small λt in
GP-UCB, and results in very little exploration, offering an
explanation for the known shortcomings of this θt.

4 Experiments

We test EST2 in three domains:
(1) synthetic black
box functions; (2) initialization tuning for trajectory op-
timization; and (3) parameter tuning for image classiﬁca-
tion. We compare the following methods: EST with a

2The code is available at https://github.com/zi-w/GP-EST.

Table 1: Minimum Regret r(t) and time to achieve this
minimum for functions sampled from 1-D (top) and 2-D
Gaussian Processes (bottom) with a limited budget of iter-
ations. ESTa and ESTn achieve lower regret values (rmin)
faster than other methods (Tmin). Here, ‘¯’ denotes the
mean and ‘ˆ’ the median.

RAND UCB

79.5
0.051
78.4
0.107

53
0.000
50.9
0.000

1-D GP, max 150 rounds
PI
EI
7
8
0.487
0.088
8.32
9.77
0.562
0.295
2-D GP, max 1000 rounds
EI
40.5
1.035
48.4
0.976

PI
45
1.290
59.8
1.26

RAND UCB
641.5
450.5
0.090
0.640
573.8
496.3
0.108
0.671

ESTa
26
0.000
26.1
0.024

ESTa
407.5
0.000
420.7
0.021

ESTn
23
0.000
21.9
0.043

ESTn
181
0.000
213.4
0.085

ˆTmin
ˆrmin
¯Tmin
¯rmin

ˆTmin
ˆrmin
¯Tmin
¯rmin

Laplace approximation, i.e., approximating the integrand in
Eqn. (4) by a truncated Gaussian (ESTa, details in supple-
ment); EST with numerical integration to evaluate Eqn. (4)
(ESTn); UCB; EI; PI; and random selection (RAND). We
omit the ’GP-’ preﬁx for simplicity.

For UCB, we follow [30] to set λt with δ = 0.01. For PI,
we use ǫ = 0.1. These parameters are coarsely tuned via
cross validation to ensure a small number of iterations for
achieving low regret. Additional experimental results and
details may be found in the supplement.

4.1 Synthetic Data

We sampled 200 functions from a 1-D GP and 100 func-
tions from a 2-D GP with known priors (Mat´ern kernel and
linear mean function). The maximum number of rounds
was 150 for 1-D and 1000 for 2-D. The ﬁrst samples were
the same for all the methods to minimize randomness ef-
fects. Table 1 shows the lowest simple regret achieved
(rmin) and the number of rounds needed to reach it (Tmin).
We measure the mean ( ¯Tmin and ¯rmin) and the median
( ˆTmin and ˆrmin). Figure 1(a) illustrates the average simple
regret and the standard deviation (scaled by 1/4). While
the progress of EI and PI quickly levels off, the other meth-
ods continue to reduce the regret, ESTn being the fastest.
Moreover, the standard deviation of ESTa and ESTn is
much lower than that of PI and EI. RAND is inferior both in
terms of Tmin and rmin. UCB ﬁnds a good point but takes
more than twice as many rounds as EST for doing so.

Figure 1(b) shows the cumulative regret RT . As for the
simple regret, we see that EI and PI focus too much on ex-
ploitation, stalling at a suboptimal point. EST converges
to lower values, and faster than UCB. For additional intu-
ition on cumulative regret, Figure 1(c) plots the cumulative

Optimization as Estimation with Gaussian Processes in Bandit Settings

1-D GP

1-D GP

3-D GP

rt

t

R

1.5

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

2.5

3

2

1

0.5

0
0

4.5

3.5

4

3

2

1

1.5

0.5

2.5

t

R

UCB:
UCB:
EST:
EST:

˜rt
rt
˜rt
rt

UCB:
UCB:
EST:
EST:

˜rt
rt
˜rt
rt

RAND
UCB
EI
PI
ESTa
ESTn

t
e
r
g
e
r

RAND
UCB
EI
PI
ESTa
ESTn

t
e
r
g
e
r

0.5
0

4.5

3.5

2.5

5

4

3

2

1

1.5

3.5

4

3

2

1

2.5

1.5

0.5

0
0
(c)

5

0

t

100

150

50

t

100

150

2-D GP

2-D GP

100

200

300

t
3-D Hartmann function

400

500

0
0

2.5

1.5

2

1

0.5

3.5

4

3

2.5

1.5

1

0.5

0
0
(a)

t
r

2

200

400

600

800

1000

200

400

600

800

1000

t

100

200

300

400

500

t

0
0
(b)

t

Figure 1: (a) Simple regrets for functions sampled from 1-D GP and 2-D GP over the number of rounds. ESTn quickly
minimizes regret. (b) Cumulative regrets for functions sampled from 1-D GP and 2-D GP. ESTn lowers the regret more
than PI and EI and faster than UCB. (c) Regrets ˜rt and simple regrets rt for a 3-D function sampled from GP and the 3-D
Hartmann function. Here, EST is ESTa. For EST, the regret in each round is usually less than that of UCB, explaining
UCB’s higher cumulative regret.

regret ˜rt and the simple regret rt for UCB and EST for a
function sampled from 3-D GP and a standard optimization
test function (the 3-D Hartmann function). UCB tends to
have higher cumulative regret than other methods because
it keeps exploring drastically even after having reached the
optimum of the function. This observation is in agreement
with the experimental results in [30] (they improved UCB’s
performance by scaling λt down by a factor of 5), indicat-
ing that the scheme of setting λt in UCB is not always ideal
in practice.

In summary, the results for synthetic functions suggest that
throughout, compared to other methods, EST ﬁnds better
function values within a smaller number of iterations.

4.2 Initialization Tuning for Trajectory Optimization

In online planning, robots must make a decision quickly,
within a possibly unknown budget (humans can stop the
“thinking period” of the robot any time, asking for a feasi-
ble and good decision to execute). We consider the prob-
lem of trajectory optimization, a non-convex optimization
problem that is commonly solved via sequential quadratic
programming (SQP) [27]. The employed solvers suffer
from sub-optimal local optima, and, in real-world scenar-
ios, even merely reaching a feasible solution can be chal-
lenging. Hence, we use Bayesian Optimization to tune the

Table 2: Lowest reward attained in 20 rounds on the air-
plane problem (illustrated in Figure 2). The results are av-
erages over 8 settings.

RAND
27.4429
3.3962
PI
28.0214
4.1489

UCB
29.2021
3.1247
ESTa
27.7587
4.2783

EI
29.1832
3.1377
ESTn
29.2071
3.1171

mean
std

mean
std

initialization for trajectory optimization. In this setting, x
is a trajectory initialization, and f (x) the score of the solu-
tion returned by the solver after starting it at x.

Our test example is the 2D airplane problem from [32],
illustrated in Figure 2. We used 8 conﬁgurations of the
starting point and ﬁxed the target. Our candidate set X of
initializations is a grid of the ﬁrst two dimensions of the
midpoint state of the trajectory (we do not optimize over
speed here). To solve the SQP, we used SNOPT [9]. Fig-
ure 2 shows the maximum rewards achieved up to round
t (standard deviations scaled by 0.1), and Table 2 displays
the ﬁnal rewards. ESTn achieves rewards on par with the
best competitors. Importantly, we observe that Bayesian
optimization achieves much better results than the standard

Zi Wang, Bolei Zhou, Stefanie Jegelka

Table 3: Classiﬁcation accuracy for visual classiﬁcation
on the test set after the model parameter is tuned. Tuning
achieves good improvements over the results in [37].

Caltech101 Caltech256

[37]
ESTa
ESTn

[37]
ESTa
ESTn

87.22
88.23
88.25
SUN397
42.61
47.13
47.21

67.23
69.39
69.39
Action40
54.92
57.60
57.58

Indoor67
56.79
60.02
60.08
Event8
94.42
94.91
94.86

random restarts, indicating a new successful application of
Bayesian optimization.

4.3 Parameter Tuning for Image Classiﬁcation

Our third set of experiments addresses Bayesian optimiza-
tion for efﬁciently tuning parameters in visual classiﬁca-
tion tasks. Here, x is the model parameter and y the accu-
racy on the validation set. Our six image datasets are stan-
dard benchmarks for object classiﬁcation (Caltech101 [8]
and Caltech256 [10]), scene classiﬁcation (Indoor67 [23]
and SUN397 [35]), and action/event classiﬁcation (Ac-
tion40 [36] and Event8 [18]). The number of images per
data set varies from 1,500 to 100,000. We use deep CNN
features pre-trained on ImageNet [14], the state of the art
on various visual classiﬁcation tasks [25].

Our experimental setup follows [37]. The data is split into
training, validation (20% of the original training set) and
test set following the standard settings of the datasets. We
train a linear SVM using the deep features, and tune its reg-
ularization parameter C via Bayesian optimization on the
validation set. After obtaining the parameter recommended
by each method, we train the classiﬁer on the whole train-
ing set, and then evaluate on the test set.

Figure 3 shows the maximum achieved accuracy on the val-
idation set during the iterations of Bayesian optimization
on the six datasets. While all methods improve the classi-
ﬁcation accuracy, ESTa does so faster than other methods.
Here too, PI and EI seem to explore too little. Table 3 dis-
plays the accuracy on the test set using the best parameter
found by ESTa and ESTn, indicating that the parameter
tuning via EST improved classiﬁcation accuracy. For ex-
ample, the tuning improves the accuracy on Action40 and
SUN397 by 3-4% over the results in [37].

5 Discussion
Next, we discuss a few further details and extensions.

Setting ˆm.
In Section 2, we discussed one way of set-
ting ˆm. There, we used an approximation with independent
variables. We focused on Equations (3) and (4) through-

≥

m that
out the paper since they yield upper bounds ˆm
preserve our theoretical results. Nevertheless, other possi-
bilities for setting ˆm are conceivable. For example, close
in spirit to Thompson and importance sampling, one may
σ(x) ). Furthermore,
sample ˆm from Pr[Y ] =
other search strategies can be used, such as guessing or
doubling, and prior knowledge about properties of f such
as the range can be taken into account.

x∈X Φ( Y −µ(x)

Q

Discretization.
In our approximations, we used dis-
cretizations of the input space. While adding a bit more
detail about this step here, we focus on the noiseless case,
described by Equation (4). Equation (3) can be analyzed
similarly for more general settings. For a Lipschitz con-
tinuous function, it is essentially sufﬁcient to estimate the
X on set W, which is a
probability of f (x)
x
ρ-covering of X.

y,

≤

∈

∀

|

k

x

x

−

| ≤

k ≤

f (x′)

We assume that f is Lipschitz continuous. Our analysis can
be adapted to the milder assumption that f is Lipschitz con-
tinuous with high probability. Let L be the Lipschitz con-
f (x)
Lρ,
stant of f . By assumption, we have
−
ρ. If X is a continuous set, we construct
x′
for all
X, inf x′∈W
its ρ-covering W such that
x′
ρ.
−
k
k ≤
X. Then,
Let EX(y) be the event that f (x)
x
Pr[EX(y)]
−
ρL)] The last step uses Lipschitz continuity to compute
ρL)] = 1. We can use this lower
Pr[EX\W(y)
|
bound to compute ˆm, so ˆm remains an upper bound on m.
Notably, none of the regret bounds relies on a discretiza-
tion of the space. Morever, once ˆm is chosen, the acqui-
sition function can be optimized with any search method,
including gradient descent.

y,
∀
ρL), EX\W(y)] = Pr[EW(y

Pr[EW(y

[EW(y

≤

−

≥

−

∈

∈

x

∀

High dimensions. Bayesian Optimization methods gen-
erally suffer in high dimensions. Common assumptions
are a low-dimensional or simpler underlying structure of f
[7, 34, 15]. Our approach can be combined with those
methods too, to be extended to higher dimensions.

Relation to entropy search.
EST is closely related to
entropy search (ES) methods [12, 13], but also differs in
a few important aspects. Like EST, ES methods approx-
imate the probability of a point x being the maximizer
arg maxx′∈X f (x′), and then choose where to evaluate
next by optimizing an acquisition function related to this
probability. However, instead of choosing the input that is
most likely to be the arg max, ES chooses where to evalu-
ate next by optimizing the expected change in the entropy
of Pr[Mx]. One reason is that ES does not aim to mini-
mize cumulative regret like many other bandit methods (in-
cluding EST). The cumulative regret penalizes all queried
points, and a method that minimizes the cumulative regret
needs to query enough supposedly good points. ES meth-
ods, in contrast, purely focus on exploration, since their
objective is to gather as much information as possible to

Optimization as Estimation with Gaussian Processes in Bandit Settings

10

9

8

7

6

5

4

3

2

1

0

Target

Trajectory 
initializations

30

29

28

27

26

25

d
r
a
w
e
R

24
0
(b)

Start

Obstacles

-1
(a)

-2

0

2

4

6

8

10

RAND
UCB
EI
PI
ESTa
ESTn

5

10
t

15

20

Figure 2: (a) Illustration of trajectory initializations. Trajectory initializations are passed to the non-convex optimization
solver to ensure a solution with as few collisions with the obstacles as possible. (b) Maximum rewards up to round t. EI
and ESTn perform relatively better than other methods.

Caltech101

Caltech256

Indoor67

0.865

0

10

30

40

0.66

0

20
t
SUN397

20
t
Action40

10

30

40

10

30

40

20
t
Event8

0.89

0.885

0.88

0.875

0.87

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.47

0.465

0.46

0.455

0.45

0.445

0.44

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.674

0.672

0.67

0.668

0.666

0.664

0.662

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.58

0.57

0.56

0.55

0.54

0.53

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

0.595

0.59

0.585

0.58

0.575

0.57

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.565

0

0.935

0.93

0.925

0.92

0.915

0.91

0.905

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.9

0

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

0.435

0

10

20
t

30

40

0.52

0

10

20
t

30

40

10

30

40

20
t

Figure 3: Maximum accuracy on the validation set over the iteration of the optimization. ESTa converges faster than other
methods. Experiments are repeated 5 times, the standard deviation is scaled by 1/4.

Zi Wang, Bolei Zhou, Stefanie Jegelka

estimate a ﬁnal value in the very end. Since the focus of
this work lies on cumulative regret, detailed empirical com-
parisons between EST and ES may be found in the supple-
ment.

6 Conclusion

In this paper, we studied a new Bayesian optimization
strategy derived from the viewpoint of the estimating the
arg max of an unknown function. We showed that this
strategy corresponds to adaptively setting the trade-off pa-
rameters λ and θ in GP-UCB and GP-PI, and established
bounds on the regret. Our experiments demonstrate that
this strategy is not only easy to use, but robustly performs
well by measure of different types of regret, on a variety of
real-world tasks from robotics and computer vision.

Acknowledgements. We thank Leslie Kaelbling and
Tom´as Lozano-P´erez for discussions, and Antonio Tor-
ralba for support with computational resources. We
gratefully acknowledge support
from NSF CAREER
award 1553284, NSF grants 1420927 and 1523767, from
ONR grant N00014-14-1-0486, and from ARO grant
W911NF1410433. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of our sponsors.

References

[1] P. Auer.

Using conﬁdence bounds for exploitation-
Journal of Machine Learning Re-

exploration tradeoffs.
search, 3:397–422, 2002.

[2] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analy-
sis of the multiarmed bandit problem. Machine learning, 47
(2-3):235–256, 2002.

[3] E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on
Bayesian optimization of expensive cost functions, with ap-
plication to active user modeling and hierarchical reinforce-
ment learning. Technical Report TR-2009-023, University
of British Columbia, 2009.

[4] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in
In Algorithmic Learning

multi-armed bandits problems.
Theory, pages 23–37. Springer, 2009.

[5] A. D. Bull. Convergence rates of efﬁcient global optimiza-
tion algorithms. Journal of Machine Learning Research, 12:
2879–2904, 2011.

[6] R. Calandra, A. Seyfarth, J. Peters, and M. P. Deisen-
roth. An experimental comparison of Bayesian optimiza-
tion for bipedal locomotion. In International Conference on
Robotics and Automation (ICRA), 2014.

[7] J. Djolonga, A. Krause, and V. Cevher. High-dimensional
Gaussian process bandits. In Advances in Neural Informa-
tion Processing Systems (NIPS), 2013.

[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: An incremental
Bayesian approach tested on 101 object categories. Com-
puter Vision and Image Understanding, 2007.

[9] P. E. Gill, W. Murray, and M. A. Saunders. SNOPT: An SQP
algorithm for large-scale constrained optimization. SIAM
Journal on optimization, 12(4):979–1006, 2002.

[10] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object cat-
egory dataset. Technical Report 7694, California Institute of
Technology, 2007.

[11] S. Gr¨unew¨alder, J.-Y. Audibert, M. Opper, and J. Shawe-
Taylor. Regret bounds for Gaussian process bandit prob-
lems. In International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), 2010.

[12] P. Hennig and C. J. Schuler. Entropy search for information-
efﬁcient global optimization. Journal of Machine Learning
Research, 13:1809–1837, 2012.

[13] J. M. Hern´andez-Lobato, M. W. Hoffman, and Z. Ghahra-
mani. Predictive entropy search for efﬁcient global opti-
mization of black-box functions. In Advances in Neural In-
formation Processing Systems (NIPS), 2014.

[14] Y.

Jia.

Caffe:

An

open

source

tional
http://caffe.berkeleyvision.org/, 2013.

architecture

feature

fast

for

convolu-
embedding.

[15] K. Kandasamy, J. Schneider, and B. Poczos. High di-
mensional Bayesian optimisation and bandits via additive
models. In International Conference on Machine Learning
(ICML), 2015.

[16] A. Krause and C. S. Ong. Contextual Gaussian process ban-
In Advances in Neural Information Pro-

dit optimization.
cessing Systems (NIPS), 2011.

[17] H. J. Kushner. A new method of locating the maximum point
of an arbitrary multipeak curve in the presence of noise.
Journal of Fluids Engineering, 86(1):97–106, 1964.

[18] L.-J. Li and L. Fei-Fei. What, where and who? classify-
ing events by scene and object recognition. In International
Conference on Computer Vision (ICCV), 2007.

[19] D. J. Lizotte, T. Wang, M. H. Bowling, and D. Schuurmans.
Automatic gait optimization with Gaussian process regres-
sion. In International Conference on Artiﬁcial Intelligence
(IJCAI), 2007.

[20] D. J. Lizotte, R. Greiner, and D. Schuurmans. An ex-
perimental methodology for response surface optimization
methods. Journal of Global Optimization, 53(4):699–736,
2012.

[21] P. Massart. Concentration Inequalities and Model Selection,

volume 6. Springer, 2007.

[22] J. Mo˘ckus. On Bayesian methods for seeking the ex-
tremum. In Optimization Techniques IFIP Technical Con-
ference, 1974.

[23] A. Quattoni and A. Torralba. Recognizing indoor scenes. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009.

[24] C. E. Rasmussen and C. K. Williams. Gaussian processes

for machine learning. The MIT Press, 2006.

[25] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. CNN features off-the-shelf: an astounding baseline for
recognition. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2014.

[26] A. M. Ross. Useful bounds on the expected maximum of
correlated normal variables. Technical report, Technical Re-
port 03W-004, ISE Dept., Lehigh Univ., Aug, 2003.

Optimization as Estimation with Gaussian Processes in Bandit Settings

[27] J. Schulman, J. Ho, A. Lee, I. Awwal, H. Bradlow, and
P. Abbeel. Finding locally optimal, collision-free trajecto-
ries with sequential convex optimization. In Robotics: Sci-
ence and Systems Conference (RSS), volume 9, pages 1–10,
2013.

[28] D. Slepian. The one-sided barrier problem for Gaussian
noise. Bell System Technical Journal, 41(2):463–501, 1962.

[29] J. Snoek, H. Larochelle, and R. P. Adams.

Practi-
cal Bayesian optimization of machine learning algorithms.
In Advances in Neural Information Processing Systems
(NIPS), 2012.

[30] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaus-
sian process optimization in the bandit setting: No regret
In International Conference on
and experimental design.
Machine Learning (ICML), 2010.

[31] R. Tedrake. Underactuated Robotics: Algorithms for Walk-
ing, Running, Swimming, Flying, and Manipulation (Course
Notes for MIT 6.832). Downloaded in Fall, 2014 from
http://people.csail.mit.edu/russt/underactuated/.

[32] R. Tedrake.
analysis
nonlinear
http://drake.mit.edu, 2014.

Drake:
for

toolbox

A planning,

control,

dynamical

and
systems.

[33] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown.
Auto-WEKA: combined selection and hyperparameter op-
In ACM SIGKDD
timization of classiﬁcation algorithms.
Conference on Knowledge Discovery and Data Mining
(KDD), 2013.

[34] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Fre-
itas. Bayesian optimization in high dimensions via random
embeddings. In International Conference on Artiﬁcial Intel-
ligence (IJCAI), 2013.

[35] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.
Sun database: large-scale scene recognition from abbey to
zoo. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2010.

[36] B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, and
L. Fei-Fei. Human action recognition by learning bases of
action attributes and parts. In International Conference on
Computer Vision (ICCV), 2011.

[37] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
In Advances in Neural Information Processing
database.
Systems (NIPS), 2014.

Zi Wang, Bolei Zhou, Stefanie Jegelka

Supplement

In this supplement, we provide proofs for all theorems and
lemmas in the main paper, more exhaustive experimental
results and details on the experiments.

A Proofs

A.1 Proofs from Section 2

In any round t,

Lemma 2.1.
the point selected by
EST is the same as the point selected by a variant of
GP-UCB with λt = minx∈X
. Conversely,
the candidate selected by GP-UCB is the same as the
candidate selected by a variant of EST with ˆmt =
maxx∈X µt−1(x) + λtσt−1(x).

ˆmt−µt−1(x)
σt−1(x)

Proof. We omit the subscripts t for simplicity. Let a be the
point selected by GP-UCB, and b selected by EST. Without
loss of generality, we assume a and b are unique. With
λ = minx∈X

, GP-UCB chooses to evaluate

ˆm−µ(x)
σ(x)

a = arg maxx∈X µ(x)+λσ(x) = arg minx∈X

ˆm

µ(x)

−
σ(x)

.

This is because

ˆm = max
x∈X

µ(x) + λσ(x) = µ(a) + λσ(a).

By deﬁnition of b, for all x

X, we have

Pr[Mb
Pr[Mx

ˆm, D]
ˆm, D] ≈

|
|

∈
Q( ˆm−µ(b)
σ(b)
Q( ˆm−µ(x)
σ(x)
Q( ˆm−µ(b)
σ(b)
Q( ˆm−µ(x)
σ(x)

)

)

x′6=b Φ( ˆm−µ(x′
σ(x′)
x′6=x Φ( ˆm−µ(x′)
σ(x′)
)

)

)

)
Q
)Φ( ˆm−µ(x)
Q
σ(x)
)Φ( ˆm−µ(b)
σ(b)

)

=

1.

≥
The inequality holds if and only if ˆm−µ(b)
X, including a, and hence
all x

σ(b) ≤

ˆm−µ(x)
σ(x)

for

∈
ˆm

µ(b)
−
σ(b) ≤

ˆm

µ(a)

−
σ(a)

= λ = min
x∈X

ˆm

µ(x)

−
σ(x)

,

which, with uniqueness, implies that a = b and GP-UCB
and EST select the same point.

For the other direction, we denote the candidate selected by
GP-UCB by

a = arg maxx∈X µ(x) + λσ(x).

The variant of EST with ˆm = maxx∈X µ(x) + λσ(x) se-
lects

b = arg maxx∈X Pr[Mx

ˆm, D].

|

We know that for all x

∈
µ(b) + ˆm−µ(x)
and hence ˆm
λσ(a), letting x = a implies that

≤

X, we have ˆm−µ(b)
σ(x) σ(b). Since ˆm = µ(a) +

ˆm−µ(x)
σ(x)

σ(b) ≤

ˆm = max
x∈X

µ(x) + λσ(x)

µ(b) + λσ(b).

≤

Hence, by uniqueness it must be that a = b and GP-UCB
and EST select the same candidate.

A.2 Proofs from Section 3

Lemma 3.2. Pick δ
T
t=1 π−1
where
that Pr[µt−1(xt)
P
[1, T ].
t

t ≤
−

∈

1
(0, 1) and set ζt = (2 log( πt
2 ,
∈
1, πt > 0. Then, for EST, it holds
δ, for all

ζtσt−1(xt)]

2δ ))

f (xt)

1

≤

≥

−

Proof. Let zt = µt−1(xt)−f (xt)

σt−1(xt)

(0, 1). It holds that

Pr[zt > ζt] =

∼ N

2

e−z

/2 dz

1
√2π
1
√2π

+∞

ζt

Z

+∞

ζt

Z
e−ζ

2
t /2

ζt

Z
t /2.

2

1
2

e−ζ

=

≤

=

e−(z−ζt)

2

/2−ζ

2

t /2−zζt dz

+∞

1
√2π

2

e−(z−ζt)

/2 dz

A union bound extends this bound to all rounds:

Pr[zt > ζt for some t

[1, T ]]

∈

T

≤

t=1
X

2

e−ζ

t /2.

1
2

1

T

2δ ))

With ζt = (2 log( πt
2 and
that with probability at least 1
ζtσt−1(xt) for all t
f (xt)
1
6 π2t2, or πt = T , in which case ζt = ζ = (2 log( T

t=1 π−1
δ, it holds that µt−1(xt)
−
[1, T ]. One may set πt =
1
2 .

t = 1, this implies

−
P
∈

2δ ))

≤

f (xt)

Lemma 3.3. If µt−1(xt)
regret at time step t is upper bounded as ˜rt ≤
ζt)σt−1(xt) , where νt , minx∈X
maxx∈X f (x),
t
∀

ˆmt−µt−1(x)
σt−1(x)

[1, T ].

the
(νt +
, and ˆmt ≥

ζtσt−1(xt),

≤

−

∈

Proof. At time step t

1, we have

≥
f (x)

˜rt = max
x∈X
ˆmt −
ˆmt −

≤

f (xt)

−
f (xt)
µt−1(xt) + ζtσt−1(xt)

≤
= (νt + ζt)σt−1(xt).

Optimization as Estimation with Gaussian Processes in Bandit Settings

B Experiments

B.1 Approximate m

In the paper, we estimate m to be

ˆm = m0 +

∞

1

−

Φ

w

µ(x)

−
σ(x)

dw

m0

Z

(cid:16)

(cid:17)

x∈W
Y
which involves an integration from the current maximum
m0 of the observed data to positive inﬁnity.
In fact the
factor inside the integration quickly approaches zero in
practice. We plot g(w) = 1
in
Figure 4, which looks like half of a Gaussian distribu-
tion. So instead of numerical integration (which can be
done efﬁciently), heuristically we can sample two values of
g(w) to ﬁt ˆg(w) = ae−(w−m0)
and do the integration
m0 ˆg(w) dw = √2πab analytically to be more efﬁcient.
This method is what we called ESTa in the paper, while the
R
numerical integration is called ESTn.

w−µ(x)
σ(x)

x∈W Φ

2
/2b

Q

−

(cid:17)

(cid:16)

∞

2

We notice that our estimation ˆm can serve as a tight upper
bound on the real value of the max of the function in prac-
tice. One example of is shown in Figure 4 with a 1-D GP
function. This example shows how PI, ESTa and ESTn
estimate m. Both ESTa and ESTn are upper bounds of the
true maximum of the function, and ESTn is actually very
tight. For PI, θ = arg max1≤τ <t yτ + ǫ is always a lower
bound of an ǫ shift over the true maximum of the function.

1

0.8

0.6

0.4

0.2

)

w
(
g

m

30

60

50

40

20

10

0

0

4

6

w

8

PI
ESTa
ESTn
max f

0

10

20

30

40

50

t

Figure 4: Top: g(w), w
of m.

∈

[m0, +

); Bottom: estimation

∞

B.2 Synthetic data

We show the examples of the functions we sampled from
GP in Figure 5. The covariance function of GP is an
isotropic Mat´ern kernel with parameters ℓ = 0.1, σf = 1.
The mean function is a linear function with a ﬁxed random
slope for different dimensions, and the constant is 1.

B.3

Initialization tuning for trajectory optimization

The 8 conﬁgurations of start state are [7 1 0 0], [7 0 0 0],
[1 0 0 0], [1 1 0 0], [2 0 0 0], [2 1 0 0], [3 0 0 0], [3 1 0 0],
where the ﬁrst two dimensions denote the position and the
last two dimension denote the speed. We only tune the ﬁrst
two dimension and keep the speed to be 0 for both direc-
tions. The target state is ﬁxed to be [5 9 0 0].

We can initialize the trajectory by setting the mid point
of trajectory to be any point falling on the grid of the
2, 12]). Then
space (both x axis and y axis have range [
use SNOPT to solve the trajectory optimization problem,
which involves an objective cost function (we take the neg-
ative cost to be a reward function to maximize), dynamics
constraints, and obstacle constraints etc. Details of trajec-
tory optimization are available in [32, 31].

−

We used the same settings of parameters for GP as in Sec-
tion B.2 for all the methods we tested and did kernel param-
eter ﬁtting every 5 rounds. The same strategy was used for
the image classiﬁcation experiments in the next section.

B.4 Parameter tuning for image classiﬁcation

We use the linear SVM in the liblinear package for all the
image classiﬁcation experiments. We extract the FC7 ac-
tivation from the imagenet reference network in the Caffe
deep learning package [14] as the visual feature. The re-
ported classiﬁcation accuracy is the accuracy averaged over
all the categories. ‘-c’ cost is the model parameter we tune
for the linear SVM.

In Caltech101 and Caltech256 experiment [8, 10], there are
8,677 images from 101 object categories in the Caltech101
and 29,780 images from 256 object categories. The train-
ing size is 30 images per category, and the rest are test im-
ages.

In SUN397 experiment [35], there are 108,754 images
from 397 scene categories. Images are randomly split into
training and test set. The training size is 50 images per
category, and the rest are test images.

Zi Wang, Bolei Zhou, Stefanie Jegelka

4

3

2

)
x
(
f

1

0

-1

-2

-2

)
x
(
f

4

2

0

-2

-4
2

-1

1

2

0
x

0

x(2)

-2

-2

-1

0

x(1)

2

1

Figure 5: Examples of a function sampled from 1-D GP
(top), and a function sampled from 2-D GP (bottom) with
isotropic Mat´ern kernel and linear mean function. We de-
liberately create many local optimums to make the problem
hard.

In MIT Indoor67 experiment [23], there are 15,620 images
from 67 indoor scene categories. Images are randomly split
into training set and test set. The training size is 100 images
per category, and the rest are test images.

In Stanford Action40 experiment [36], there are 9,532 im-
ages from 40 action categories. Images are randomly split
into training set and test set. The training size is 100 images
per category, and the rest are test images.

In UIUC Event8 experiment [18], there are 1,579 images
from 8 event categories.
Images are randomly split into
training set and test set. Training size is 70 images per
category, and the rest are test images.

We used features extracted from a convolutional neural net-
work (CNN) that was trained on images from ImageNet. It
has been found [37] that features from a CNN trained on
a set of images focused more on places than on objects,
the Places database, work better in some domains. So,
we repeated our experiments using the Places-CNN fea-
tures, and the results are shown in Figure 6 and Table 4.

All the methods help to improve the classiﬁcation accuracy
on the validation set. EST methods achieve good accuracy
on each validation set on par with the best competitors for
most of the datasets. And we also observe that for Cal-
tech101 and Event8, RAND and UCB converge faster and
achieve better accuracy than other methods. As we have
shown in Section 4.1, UCB and RAND perform worse than
other methods in terms of cumulative regret, because they
tend to explore too much. However, more exploration can
be helpful for some black-box functions that do not satisfy
our assumption that they are samples from GP. For exam-
ple, for discontinuous step functions, pure exploration can
be beneﬁcial for simple regret. One possible explanation
for the better results of RAND and UCB is that the black-
box functions we optimize here are possibly functions not
satisfying our assumption. The strong assumption on the
black-box function is also a major drawback of Bayesian
optimization,

B.5 Comparison to entropy search methods

Entropy search methods [12, 13] aim to minimize the
entropy of the probability for the event Mx (x =
arg maxx′∈X f (x′)). Although not suitable for minimiz-
ing cumulative regret, ES methods are intuitively ideal for
minimizing simple regret. We hence in this section com-
pare the empirical performance of entropy search (ES) [12]
and predictive entropy search (PES) [13] to that of the EST
methods (EST/GP-UCB/PI) and EI.

Since both ES and PES only support squared exponential
covariance function and zero mean function in their code
right now, and it requires signiﬁcant changes in their code
to accommodate other covariance functions, we created
synthetic functions that are different from the ones we used
in Section 4 in the paper. The new functions are sampled
from 1-D (80 functions) and 2-D GP (20 functions) with
squared exponential kernel (σf = 0.1 and l = 1) and 0
mean. Function examples are shown in Figure 9.

We show the results on these synthetic functions in Fig-
ure 7,8, and a standard optimization test function, Branin
Hoo function, in Figure 10. It is worth noting that ES meth-
ods make queries on the most informative points, which are
not necessarily the points with low regret. At each round,
ES methods make a “query” on the black-box function, and
then make a “guess” of the arg max of the function (but do
not test the “guess”). We plot the regret achieved by the
“guesses” made by ES methods. For the 1-D GP task, all
the methods behave similarly and achieve zero regret ex-
cept RAND. For the 2-D GP task, EI is the fastest method to
converge to zero regret, and in the end ESTn, PI,EI and ES
methods achieve similar results. For the test on Branin Hoo
function, PES achieves the lowest regret. ESTa converges
slightly faster than PES, but to a slightly higher regret.

We also compared the running time for all the methods in

Caltech101

Caltech256

Indoor67

0.64

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.635

0.63

0.625

0.62

0.615

0

0.582

0.58

0.578

0.576

0.574

0.572

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.57

0.568

0

s
g
n
i
t
t
e
S
t
i
d
n
a
B
n
i

s
e
s
s
e
c
o
r
P
n
a
i
s
s
u
a
G
h
t
i

w
n
o
i
t
a
m

i
t
s
E
s
a
n
o
i
t
a
z
i
m

i
t
p
O

0.46

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.45

0.44

0.43

0.42

0

0.48

0.47

0.46

0.45

0.44

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.43

0

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

0.705

0.7

0.695

0.69

0.685

0.68

0.675

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.67

0

0.92

0.915

0.91

0.905

0.9

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.895

0

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

20
t

Event8

20
t

10

30

40

10

30

40

10

30

40

20
t

SUN397

20
t

Action40

10

30

40

10

30

40

10

30

40

20
t

20
t

Figure 6: Maximum accuracy on the validation set over iteration of the optimization. Experiments are repeated 5 times. The visual features used here are Deep CNN
features pre-trained on the Places database.

Table 4: Classiﬁcation accuracy on the test set of the datasets after the model parameter is tuned by ESTa and ESTn. Tuning achieves good improvement over the results
in [37].

Imagenet-CNN feature
ESTa
ESTn
Places-CNN feature
ESTa
ESTn

87.22
88.23
88.25
65.18
66.94
66.95

Caltech101 Caltech256

67.23
69.39
69.39
45.59
47.51
47.43

Indoor67
56.79
60.02
60.08
68.24
70.27
70.27

SUN397 Action40 Event8
94.42
54.92
94.91
57.60
94.86
57.58
94.12
42.86
93.79
46.24
93.56
46.17

42.61
47.13
47.21
54.32
58.57
58.65

Z

i

W
a
n
g
,

B
o
l
e
i

Z
h
o
u

,

S
t
e
f
a
n
i
e
J
e
g
e
l

k
a

Optimization as Estimation with Gaussian Processes in Bandit Settings

t

r

1.5

3.5

2.5

0.5

-0.5

3

2

1

0

3

2

1

0

t

r

1.5

2.5

0.5

RAND
UCB
EI
PI
ES
PES
ESTn

RAND
UCB
EI
PI
ES
PES
ESTn

2

1

0

-1

-2

)
x
(
f

0

)
x
(
f

5

-5
2

t

r

25

50

45

40

35

30

20

15

10

5

0

20

40

60

100

120

140

80

t

-3

-2

-1

0
x

1

2

Figure 7: Simple regret for functions sampled from 1-D GP
with squared exponential kernel and 0 mean.

0

x(2)

-2

-2

0

x(1)

2

100

200

300

400

500

t

Figure 8: Simple regret for functions sampled from 2-D GP
with squared exponential kernel and 0 mean.

Figure 9: Examples of a function sampled from 1-D GP
(left), and a function sampled from 2-D GP (right) with
squared exponential kernel and 0 mean functions. These
functions can be easier than the ones in Figure 5 since they
have fewer local optima.

Table 5. 3 It is assumed in GP optimization that it is more
expensive to evaluate the blackbox function than comput-
ing the next query to evaluate using GP optimization tech-
niques. However, in practice, we still want the algorithm
to output the next query point as soon as possible. For ES
methods, it can be sometimes unacceptable to run them for
black-box functions that take minutes to complete a query.

3All of the methods were run with MATLAB (R2012b), on

Intel(R) Xeon(R) CPU E5645 @ 2.40GHz.

Table 5: Comparison on the running time (s) per iteration.

RAND UCB
0.075
0.0002
ESTn
ESTa
0.55
0.078

EI
0.079
ES
56

PI
0.076
PES
106

RAND
UCB
EI
PI
ES
PES
ESTa
ESTn

5

15

20

10
t

Simple regret for Branin Hoo function.
Figure 10:
UCB/EI/PI/ESTa/ESTn use the isotropic Mat´ern kernel
with parameters ℓ = 0.1, σf = 1; ES/PES use the isotropic
squared exponential kernel with ℓ = 0.1, σf = 1.

8
1
0
2
 
g
u
A
 
2
1

 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
3
2
4
6
0
.
0
1
5
1
:
v
i
X
r
a

Optimization as Estimation with Gaussian Processes in Bandit Settings

Zi Wang
MIT CSAIL

Bolei Zhou
MIT CSAIL

Stefanie Jegelka
MIT CSAIL

Abstract

there has been rising interest

Recently,
in
Bayesian optimization – the optimization of an
unknown function with assumptions usually ex-
pressed by a Gaussian Process (GP) prior. We
study an optimization strategy that directly uses
an estimate of the argmax of the function. This
strategy offers both practical and theoretical ad-
vantages: no tradeoff parameter needs to be se-
lected, and, moreover, we establish close connec-
tions to the popular GP-UCB and GP-PI strate-
gies. Our approach can be understood as auto-
matically and adaptively trading off exploration
and exploitation in GP-UCB and GP-PI. We il-
lustrate the effects of this adaptive tuning via
bounds on the regret as well as an extensive em-
pirical evaluation on robotics and vision tasks,
demonstrating the robustness of this strategy for
a range of performance criteria.

1 Introduction

The optimization of an unknown function that is expensive
to evaluate is an important problem in many areas of sci-
ence and engineering. Bayesian optimization uses proba-
bilistic methods to address this problem. In particular, an
increasingly popular direction has been to model smooth-
ness assumptions on the function via a Gaussian Process
(GP). The Bayesian approach provides a posterior distri-
bution of the unknown function, and thereby uncertainty
estimates that help decide where to evaluate the function
next, in search of a maximum. Recent successful appli-
cations of this Bayesian optimization framework include
the tuning of hyperparameters for complex models and al-
gorithms in machine learning, robotics, and computer vi-
sion [3, 6, 16, 19, 29, 33].

Despite progress on theory and applications of Bayesian

Appearing in Proceedings of the 19th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS) 2016, Cadiz,
Spain. JMLR: W&CP volume 51. Copyright 2016 by the authors.

optimization methods, the practitioner continues to face
many options: there is a menu of algorithms, and their rela-
tions and tradeoffs are only partially understood. Typically,
the points where the function is evaluated are selected se-
quentially; and the choice of the next point is based on ob-
served function values at the previous points. Popular al-
gorithms vary in their strategies to pick the next point: they
select the point that maximizes the probability of improve-
ment (GP-PI) [17]; the expected improvement (GP-EI) [22];
or an upper conﬁdence bound (GP-UCB) [30] on the max-
imum function value. Another alternative is entropy search
(ES) [12], which aims to minimize the uncertainty about
the location of the optimum of the function. Each algo-
rithm reduces the black-box function optimization problem
to a series of optimization problems of known acquisition
functions.

The motivations and analyses (if available) differ too: ob-
jectives include cumulative regret, where every evaluation
results in a reward or cost and the average of all function
evaluations is compared to the maximum value of the func-
tion; simple regret that takes into account only the best
value found so far [4]; the performance under a ﬁxed ﬁ-
nite budget [11]; or the uncertainty about the location of
the function maximizer [12]. Here, we focus on the estab-
lished objectives of cumulative regret in bandit games.

Notably, many of the above algorithms involve tuning a pa-
rameter to trade off exploration and exploitation, and this
can pose difﬁculties in practice [12]. Theoretical analyses
help in ﬁnding good parameter settings, but may be con-
servative in practice [30]. Computing the acquisition func-
tion and optimizing it to ﬁnd the next point can be com-
putationally very costly too. For example, the computation
to decide which next point to evaluate for entropy search
methods tends to be very expensive, while GP-PI, GP-EI
and GP-UCB are much cheaper.

In this paper, we study an intuitive strategy that offers a
compromise between a number of these approaches and, at
the same time, establishes connections between them that
help understand when theoretical results can be transferred.
Our strategy uses the Gaussian Process to obtain an esti-
mate of the argument that maximizes the unknown function
f . The next point to evaluate is determined by this estimate.
This point, it turns out, is not necessarily the same as the

Optimization as Estimation with Gaussian Processes in Bandit Settings

the argument with the highest upper conﬁdence bound.

This strategy has both practical and theoretical advantages.
On the theoretical side, we show connections to the popular
GP-UCB and GP-PI strategies, implying an intuitive and
provably correct way of setting the parameters in those im-
portant methods. Moreover, we establish bounds on the re-
gret of our estimation strategy. From a practical viewpoint,
our strategy obviates any costly parameter tuning. In fact,
we show that it corresponds to automatically and adap-
tively tuning the parameters of GP-UCB and GP-PI. Our
empirical evaluation includes problems from non-convex
optimization, robotics, and computer vision. The experi-
ments show that our strategy performs similarly to or even
better than the best competitors in terms of cumulative re-
gret. Although not designed to minimize simple regret di-
rectly, in practice our method also works well as measured
by simple regret, or by the number of steps to reach a ﬁxed
regret value. Together, these results suggest that our strat-
egy is easy to use and empirically performs well across a
spectrum of settings.

Related work. The practical beneﬁts of Bayesian opti-
mization have been shown in a number of applications [3,
6, 19, 29, 33, 34]. Different Bayesian optimization algo-
rithms differ in the selection criteria of the next point to
evaluate, i.e., the acquisition function. Popular criteria in-
clude the expected improvement (GP-EI) [22], the prob-
ability of improving over a given threshold (GP-PI) [17],
and GP-UCB [30], which is motivated by upper conﬁdence
bounds for multi-armed bandit problems [2, 1]. GP-EI,
GP-PI and GP-UCB have a parameter to select, and the
latter two are known to be sensitive to this choice. En-
tropy search (ES) [12] and the related predictive entropy
search (PES) [13] do not aim to minimize regret directly,
but to maximize the amount of information gained about
the optimal point. High-dimensional settings were con-
sidered in [7, 34]. Extensive empirical comparisons in-
clude [6, 29, 20]. Theoretical bounds on different forms
of regret were established for GP-UCB [30] and GP-EI [5].
Other theoretical studies focus on simple regret [4, 11] or
ﬁnite budgets [11]. In this work, in contrast, we are moti-
vated by practical considerations.

1.1 Background and Notation

)

∼

GP (0, k) be an unknown function we aim
Let f (
·
to optimize over a candidate set X. At time step t,
we select point xt and observe a possibly noisy func-
tion evaluation yt = f (xt) + ǫt, where ǫt are i.i.d.
(0, σ2). Given the observations Dt =
Gaussian noise
t
(xτ , yτ )
τ =1 up to time t, we obtain the posterior mean
{
}
and covariance of the function via the kernel matrix Kt =
[k(xi, xj)]xi,xj∈Dt
and kt(x) = [k(xi, x)]xi∈Dt [24]:
µt(x) = kt(x)T(Kt + σ2I)−1yt, and kt(x, x′) =
kt(x)T(Kt +σ2I)−1kt(x′). The posterior vari-
k(x, x′)

N

−

ance is given by σ2
note by Q(
·
tribution φ(
·

t (x) = kt(x, x). Furthermore, we de-
) the tail probability of the standard normal dis-
), and by Φ(
·

) its cumulative probability.

The Bayesian Optimization setting corresponds to a bandit
game where, in each round t, the player chooses a point xt
and then observes yt = f (xt)+ǫt. The regret for round t is
f (xt). The simple regret
deﬁned as ˜rt = maxx∈X f (x)
for any T rounds is rT = mint∈[1,T ] ˜rt, and the (average)
cumulative regret is RT = 1
T

T
t=1 ˜rt.

−

1.2 Existing methods for GP optimization

P

We focus on the following three approaches for compari-
son, since they are most widely used in bandit settings.

GP-UCB. Srinivas et al. [30] provide a detailed analysis
for using upper conﬁdence bounds [2] with GP bandits.
They propose the strategy xt = arg maxx∈X µt−1(x) +
π2t2/(6δ)))
λtσt−1(x) where λt = (2 log(
2 for ﬁnite
|
X. Their regret bound holds with probability 1

δ.

X

|

1

−

GP-EI. The GP-EI strategy [22] selects the point max-
imizing the expected improvement over a pre-speciﬁed
threshold θt [22]. For GPs, this improvement is given
in closed form as EI(x) = E[(f (x)
θt)+] =
[φ(γ(x))
γ(x)Q(γ(x))] σt−1(x), where γ(x) =
−
θt−µt−1(x)
. A popular choice for the threshold is θt =
σt−1(x)

−

maxτ ∈[1,t−1] yτ .

GP-PI. The third strategy maximizes the probability
Φ(γ(x)) of improving over
PI(x) = Pr[f (x) > θt] = 1
−
a threshold θt [17], i.e., xt = arg minx∈X γ(x). GP-PI is
sensitive to the choice of θt: as we will see in Section 2, θt
trades off exploration and exploitation, and setting θt too
low (e.g., θt = maxτ ∈[1,t−1] yτ ) can result in getting stuck
at a fairly suboptimal point. A popular choice in practice is
θt = maxτ ∈[1,t−1] yτ + ǫ, for a chosen constant ǫ.

2 Optimization as estimation

In this work, we study an alternative criterion that provides
an easy-to-use and tuning-free approach: we use the GP
to estimate the arg max of f . In Section 2.1, we will see
how, as a side effect, this criterion establishes connections
between the above criteria. Our strategy eventually leads to
tighter bounds than GP-UCB as shown in Section 3.

∈

Consider the posterior probability (in round t) that a ﬁxed
X is an arg max of f . We call this event Mx
x
and, for notational simplicity, omit the subscripts t
1
here. The event Mx is equivalent to the event that for all
x′
0. The dif-
∈
ference v(x′) between two Gaussian variables is Gaussian:
v(x′)
2k(x, x′)2).
(µ(x′)
−
∼ N
−
X is Cov(v(x′), v(x′′)) =
The covariance for any x′, x′′
k(x, x′′)2.
σ(x)2 + k(x′, x′′)2

X, we have v(x′) := f (x′)

µ(x), σ(x)2 + σ(x′)2

∈
k(x, x′)2

f (x)

−

−

≤

−

−

Zi Wang, Bolei Zhou, Stefanie Jegelka

The random variables
tive probability

v(x′)
}

{

x′∈X determine the cumula-

Pr[Mx

D] = Pr[

X, v(x′)

0

D].

(1)

|

x′
∀

∈

≤

|

This probability may be speciﬁed via limits as e.g. in [12,
App.A]. Moreover, due to the assumed smoothness of f ,
it is reasonable to work with a discrete approximation and
restrict the set of candidate points to be ﬁnite for now (we
discuss discretization further in Section 5). So the quantity
D] for
in Eqn. (1) is well-deﬁned. Since computing Pr[Mx
X
can be costly, we use a “mean-ﬁeld” approach and
large
|
approximate
x∈X by independent Gaussian random
variables with means µ(x) and variances σ(x)2 for all x
∈
X. Given be the maximum value m of f , the probability of
the event Mx

m, D amounts to

f (x)
}

{

|

|

|
m, D]

Pr[Mx

|

Q

≈

m

µ(x)

−
σ(x)

Φ

m

µ(x′)

−
σ(x′)

.

(cid:16)
(cid:16)
to evaluate
Our estimation strategy (EST) chooses
ˆm, D] next, which is the function in-
arg maxx∈X Pr[Mx
put that is most likely to achieve the highest function value.

(cid:17)

|

x′6=x
(cid:17) Y

Of course, the function maximum m may be unknown. In
this case, we use a plug-in estimate via the posterior expec-
tation of Y = maxx∈X f (x) given D [26]:

D]

ˆm = E[Y
|
∞
Pr[Y > y

=

0
Z

D]

|

−

Pr[Y <

y

D] dy.

−

|

If the noise in the observations is negligible, we can sim-
plify Eqn. (3) to be

ˆm = m0 +

∞

1

−

Φ

w

µ(x)

−
σ(x)

dw

(4)

m0

Z

(cid:16)

(cid:17)

x∈X
Y
where m0 = maxτ ∈[1,t−1] yτ is the current observed max-
imum value. Under conditions speciﬁed in Section 3, our
approximation with the independence assumption makes ˆm
an upper bound on m, which, as we will see, conservatively
emphasizes exploration a bit more. Other ways of setting
ˆm are discussed in Section 5.

2.1 Connections to GP-UCB and GP-PI

Next, we relate our strategy to GP-PI and GP-UCB: EST
turns out to be equivalent to adaptively tuning θt in GP-
PI and λt in GP-UCB. This observation reveals unifying
connections between GP-PI and GP-UCB and, in Section 3,
yields regret bounds for GP-PI with a certain choice of θt.
Lemma 2.1 characterizes the connection to GP-UCB:
Lemma 2.1.
the point selected by
EST is the same as the point selected by a variant of
. Conversely,
GP-UCB with λt = minx∈X
the candidate selected by GP-UCB is the same as the
candidate selected by a variant of EST with ˆmt =
maxx∈X µt−1(x) + λtσt−1(x).

ˆmt−µt−1(x)
σt−1(x)

In any round t,

Proof. We omit the subscripts t for simplicity. Let a be the
point selected by GP-UCB, and b selected by EST. Without
loss of generality, we assume a and b are unique. With
λ = minx∈X

, GP-UCB chooses to evaluate

ˆm−µ(x)
σ(x)

a = arg maxx∈X µ(x)+λσ(x) = arg minx∈X

ˆm

µ(x)

−
σ(x)

.

This is because

ˆm = max
x∈X

µ(x) + λσ(x) = µ(a) + λσ(a).

By deﬁnition of b, for all x

X, we have

Pr[Mb
|
Pr[Mx
|

ˆm, D]
ˆm, D] ≈

∈
Q( ˆm−µ(b)
σ(b)
Q( ˆm−µ(x)
σ(x)

)

)Φ( ˆm−µ(x)
σ(x)
)Φ( ˆm−µ(b)
σ(b)

) ≥

1.

The inequality holds if and only if ˆm−µ(b)
X, including a, and hence
all x

σ(b) ≤

ˆm−µ(x)
σ(x)

for

∈
ˆm

µ(b)
−
σ(b) ≤

ˆm

µ(a)

−
σ(a)

= λ = min
x∈X

ˆm

µ(x)

−
σ(x)

,

which, with uniqueness, implies that a = b and GP-UCB
and EST select the same point.

The other direction of the proof is similar and can be found
in the supplement.

Proposition 2.2. GP-PI is equivalent to EST when setting
θt = ˆmt in GP-PI.

(2)

(3)

As a corollary of Lemma 2.1 and Proposition 2.2, we obtain
a correspondence between GP-PI and GP-UCB.

Corollary 2.3. GP-UCB is equivalent to GP-PI if λt is
, and GP-PI corresponds to GP-
set to minx∈X
UCB if θt = maxx∈X µt−1(x) + λtσt−1(x).

θt−µt−1(x)
σt−1(x)

Algorithm 1 GP-UCB/PI/EST
1: t ← 1; D0 ← ∅
2: while stopping criterion not reached do
µt−1, Σt−1 ← GP-predict(X|Dt−1)
3:

ˆmt = 


maxx∈X µt−1(x) + λtσt−1(x)
max1≤τ <t yτ + ǫ
E[Y |Dt−1] (see Eqn. (3)/Eqn. (4)) EST

GP-UCB
GP-PI

4:

5:



ˆmt−µt−1(x)
σt−1(x)

xt ← arg minx∈X
yt ← f (xt) + ǫt, ǫt ∼ N (0, σ2)
Dt ← {xτ , yτ }t
τ =1
t ← t + 1

6:
7:
8:
9: end while

Proposition 2.2 suggests that we do not need to calcu-
ˆmt, Dt−1] directly when im-
late the probability Pr[Mx
|
plementing EST. Instead, we can reduce EST to GP-PI
with an automatically tuned target value θt. Algorithm 1
compares the pseudocode for all three methods. We use

Optimization as Estimation with Gaussian Processes in Bandit Settings

“GP-predict” to denote the update for the posterior mean
and covariance function for the GP as described in Sec-
tion 1.1. GP-UCB/PI/EST all share the same idea of reach-
ing a target value ( ˆmt in this case), and thereby trading
off exploration and exploitation. GP-UCB in [30] can
be interpreted as setting the target value to be a loose
upper bound maxx∈X µt−1(x) + λtσt−1(x) with λt =
1
2 , as a result of applying the union
(2 log(
|
bound over X1. GP-PI applies a ﬁxed upwards shift of ǫ
over the current maximum observation maxτ ∈[1,t−1] yτ . In
both cases, the exploration-exploitation tradeoff depends
on the parameter to be set. EST implicitly and automati-
cally balances the two by estimating the maximum. Viewed
as GP-UCB or GP-PI, it automatically sets the respective
parameter.

π2t2/6δ))

X

|

Note that this change by EST is not only intuitively reason-
able, but it also leads to vanishing regret, as will become
evident in the next section.

3 Regret Bounds

In this section, we analyze the regret of EST. We ﬁrst show
a bound on the cumulative regret both in expectation and
with high probability, with the assumption that our estima-
tion ˆmt is always an upper bound on the maximum of the
function. Then we interpret how this assumption is satis-
ﬁed via Eqn. (3) and Eqn. (4) under the condition speciﬁed
in Corollary 3.5.
Theorem 3.1. We assume ˆmt ≥
maxx∈X f (x),
∈
[1, T ], and restrict k(x, x′)
1. Let σ2 be the vari-
ance of the Gaussian noise in the observation, γT the
maximum information gain of the selected points, C =
2/ log(1 + σ−2), and t∗ = arg maxt νt where νt ,
. The cumulative expected regret sat-
minx∈X
T
E [˜rt|
Dt−1]
isﬁes
t=1
δ, it holds that
least 1
P
−
with ζT = (2 log( T
2 .

νt∗ √CT γT . With probability at
(νt∗ + ζT )√CT γT ,
t=1 ˜rt ≤

ˆmt−µt−1(x)
σt−1(x)

t
∀

≤

≤

T

1

2δ ))

P

The information gain γT after T rounds is the maxi-
mum mutual information that can be gained about f from
T measurements: γT = maxA⊆X,|A|≤T I(yA, fA) =
2 log det(I + σ−2KA). For the Gaussian
maxA⊆X,|A|≤T
kernel, γT = O((log T )d+1), and for the Mat´ern kernel,
γT = O(T d(d+1)/(2ξ+d(d+1)) log T ) where d is the dimen-
sion and ξ is the roughness parameter of the kernel [30,
Theorem 5].

1

The proof of Theorem 3.1 follows [30] and relies on the
following lemmas which are proved in the supplement.

1Since Pr[|f (x) − µ(x)| > λtσ(x)] ≤ e

, applying the
union bound results in Pr[|f (x) − µ(x)| > λtσ(x), ∀x ∈ X] ≤
−λ
2 . This means f (x) ≤ maxx∈X µ(x) + λtσ(x) with
|X|e
probability at least 1 − |X|e

[30, Lemma 5.1].

−λ
2

2
t

2
t

2
t

−λ
2

Lemma 3.2. Pick δ
T
t=1 π−1
where
that Pr[µt−1(xt)
P
[1, T ].
t

t ≤
−

∈

1
(0, 1) and set ζt = (2 log( πt
2 ,
∈
1, πt > 0. Then, for EST, it holds
δ, for all

ζtσt−1(xt)]

2δ ))

f (xt)

1

≤

≥

−

Lemma 3.2 is similar to but not exactly the same as [30,
Appendix A.1]: while they use a union bound over all of X,
here, we only need a union bound over the actually evalu-
T
ated points
t=1. This difference is due to the different
selection strategies.

xt}

{

f (xt)

Lemma 3.3. If µt−1(xt)
regret at time step t is upper bounded as ˜rt ≤
ζt)σt−1(xt) , where νt , minx∈X
maxx∈X f (x),
t
∀

ˆmt−µt−1(x)
σt−1(x)

[1, T ].

the
(νt +
, and ˆmt ≥

ζtσt−1(xt),

−

≤

∈

The proof of Theorem 3.1 now follows from Lemmas 3.2,
3.3, and Lemma 5.3 in [30].

Dt−1]

Proof. (Thm. 3.1) The expected regret of round t is
E[˜rt|
µt−1(xt) = νtσt−1(xt). Using
t∗ = arg maxt νt, we obtain that
νt∗

T
t=1 σt−1(xt).

E [˜rt|

ˆmt −

Dt−1]

T
t=1

≤

≤

P

2

−2

x

≤

≤

≤

k(xt, xt)

1 + ax for 0

To bound the sum of variances, we ﬁrst use that
P
(1 + a)x
1 and the
assumption σt−1(xt)
1 to obtain
log(1+σ
σ2
t−1(xt)
≤
implies that
≤
2
log(1+σ−2) γT . The Cauchy-Schwarz inequality leads to
T
2T γT
t=1 σ2
log(1+σ−2) .

. Lemma 5.3 in [30] now
log(1+σ−2) I(yT ; fT )

≤
t−1(xt))
σ
log(1+σ−2)
t−1(xt)

P
T
t=1 σt−1(xt)

t−1(xt)
Together, we have the ﬁnal regret bound
q
P

T
t=1 σ2

P

q

≤

≤

≤

≤

T

2

T

t=1
X

E [˜rt|

Dt−1]

νt∗

≤

s

2T
log(1 + σ−2)

γT .

Next we show a high probability bound. The condi-
tion of Lemma 3.3 holds with high probability because of
Lemma 3.2. Thus with probability at least 1
δ, the regret
for round t is bounded as follows,

−

˜rt ≤

(νt + ζt)σt−1(xt)

(νt∗ + ζT )σt−1(xt),

≤
2
2δ ), πt = π
6 , and t∗ = arg maxt νt.

2
t

where ζt = 2 log( πt
Therefore, with probability at least 1

δ,

−

T

t=1
X

˜rt ≤

(νt∗ + ζT )

s

2T γT
log(1 + σ−2)

.

Next we show that if we estimate ˆmt as described in Sec-
f (x)
tion 2 by assuming all the
x∈X are independent con-
}
ditioned on the current sampled data Dt, ˆmt can be guar-
anteed to be an upper bound on the function maximum m
given kt(x, x′)

X.

0,

{

x, x′
∀

∈

≥

Zi Wang, Bolei Zhou, Stefanie Jegelka

Lemma 3.4 (Slepian’s Comparison Lemma [28, 21]). Let
Rn be two multivariate Gaussian random vectors
u, v
with the same mean and variance, such that E[vivj]
E[supi∈[1,n] ui].
E[uiuj],

i, j. Then E[supi∈[1,n] vi]
∀

≥

≤

∈

Slepian’s Lemma implies a relation between our approxi-
mation ˆmt and m.
Corollary 3.5. Assume g
GP (µ, k) has posterior mean
X
µt(x) and posterior covariance kt(x, x′)
conditioned on Dt. Deﬁne a series of independent ran-
dom variables h(x) with equivalent mean µt(x) and poste-
rior variance kt(x, x),
E[supx∈X g(x)].

X. Then, E[supx∈X h(x)]

x, x′
∀

0,

≥

∼

≥

∈

∈

x

∀

Proof. By independence,

X

x, x′
∀
0 = E[h(x)h(x′)]
E[g(x)g(x′)]

−

∈
E[h(x)]E[h(x′)]
E[g(x)]E[g(x′)]

≤

−

Hence, Slepian’s Lemma implies that E[supx∈X h(x)]
E[supx∈X g(x)].

≥

X.
Corollary 3.5 assumes that kt(x, x′)
This depends on the choice of X and k. Notice that
kt(x, x′)
0 is only a sufﬁcient condition and, even if
the assumption fails, ˆmt is often still an upper bound on m
in practice (illustrations in the supplement).

x, x′
∀

0,

≥

≥

∈

In contrast, the results above are not necessarily true for
any arbitrary θt in GP-PI, an important distinction between
GP-PI and GP-EST.

Before evaluating the EST strategy empirically, we make
a few important observations. First, EST does not re-
quire manually setting a parameter that trades off explo-
ration and exploitation.
Instead, it corresponds to auto-
matically, adaptively setting the tradeoff parameters λt in
GP-UCB and θt in GP-PI. For EST, this means that if the
gap νt is large, then the method focuses more on explo-
ration, and if “good” function values (i.e., close to ˆmt) are
observed, then exploitation increases. If we write EST as
GP-PI, we see that by Eqn. (4), the estimated ˆmt always
ensures θt > maxτ ∈[1,T ] yτ , which is known to be advan-
tageous in practice [20]. These analogies likewise suggest
that θt = maxτ ∈[1,T ] yτ corresponds to a very small λt in
GP-UCB, and results in very little exploration, offering an
explanation for the known shortcomings of this θt.

4 Experiments

We test EST2 in three domains:
(1) synthetic black
box functions; (2) initialization tuning for trajectory op-
timization; and (3) parameter tuning for image classiﬁca-
tion. We compare the following methods: EST with a

2The code is available at https://github.com/zi-w/GP-EST.

Table 1: Minimum Regret r(t) and time to achieve this
minimum for functions sampled from 1-D (top) and 2-D
Gaussian Processes (bottom) with a limited budget of iter-
ations. ESTa and ESTn achieve lower regret values (rmin)
faster than other methods (Tmin). Here, ‘¯’ denotes the
mean and ‘ˆ’ the median.

RAND UCB

79.5
0.051
78.4
0.107

53
0.000
50.9
0.000

1-D GP, max 150 rounds
PI
EI
7
8
0.487
0.088
8.32
9.77
0.562
0.295
2-D GP, max 1000 rounds
EI
40.5
1.035
48.4
0.976

PI
45
1.290
59.8
1.26

RAND UCB
641.5
450.5
0.090
0.640
573.8
496.3
0.108
0.671

ESTa
26
0.000
26.1
0.024

ESTa
407.5
0.000
420.7
0.021

ESTn
23
0.000
21.9
0.043

ESTn
181
0.000
213.4
0.085

ˆTmin
ˆrmin
¯Tmin
¯rmin

ˆTmin
ˆrmin
¯Tmin
¯rmin

Laplace approximation, i.e., approximating the integrand in
Eqn. (4) by a truncated Gaussian (ESTa, details in supple-
ment); EST with numerical integration to evaluate Eqn. (4)
(ESTn); UCB; EI; PI; and random selection (RAND). We
omit the ’GP-’ preﬁx for simplicity.

For UCB, we follow [30] to set λt with δ = 0.01. For PI,
we use ǫ = 0.1. These parameters are coarsely tuned via
cross validation to ensure a small number of iterations for
achieving low regret. Additional experimental results and
details may be found in the supplement.

4.1 Synthetic Data

We sampled 200 functions from a 1-D GP and 100 func-
tions from a 2-D GP with known priors (Mat´ern kernel and
linear mean function). The maximum number of rounds
was 150 for 1-D and 1000 for 2-D. The ﬁrst samples were
the same for all the methods to minimize randomness ef-
fects. Table 1 shows the lowest simple regret achieved
(rmin) and the number of rounds needed to reach it (Tmin).
We measure the mean ( ¯Tmin and ¯rmin) and the median
( ˆTmin and ˆrmin). Figure 1(a) illustrates the average simple
regret and the standard deviation (scaled by 1/4). While
the progress of EI and PI quickly levels off, the other meth-
ods continue to reduce the regret, ESTn being the fastest.
Moreover, the standard deviation of ESTa and ESTn is
much lower than that of PI and EI. RAND is inferior both in
terms of Tmin and rmin. UCB ﬁnds a good point but takes
more than twice as many rounds as EST for doing so.

Figure 1(b) shows the cumulative regret RT . As for the
simple regret, we see that EI and PI focus too much on ex-
ploitation, stalling at a suboptimal point. EST converges
to lower values, and faster than UCB. For additional intu-
ition on cumulative regret, Figure 1(c) plots the cumulative

Optimization as Estimation with Gaussian Processes in Bandit Settings

1-D GP

1-D GP

3-D GP

rt

t

R

1.5

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

2.5

3

2

1

0.5

0
0

4.5

3.5

4

3

2

1

1.5

0.5

2.5

t

R

UCB:
UCB:
EST:
EST:

˜rt
rt
˜rt
rt

UCB:
UCB:
EST:
EST:

˜rt
rt
˜rt
rt

RAND
UCB
EI
PI
ESTa
ESTn

t
e
r
g
e
r

RAND
UCB
EI
PI
ESTa
ESTn

t
e
r
g
e
r

0.5
0

4.5

3.5

2.5

5

4

3

2

1

1.5

3.5

4

3

2

1

2.5

1.5

0.5

0
0
(c)

5

0

t

100

150

50

t

100

150

2-D GP

2-D GP

100

200

300

t
3-D Hartmann function

400

500

0
0

2.5

1.5

2

1

0.5

3.5

4

3

2.5

1.5

1

0.5

0
0
(a)

t
r

2

200

400

600

800

1000

200

400

600

800

1000

t

100

200

300

400

500

t

0
0
(b)

t

Figure 1: (a) Simple regrets for functions sampled from 1-D GP and 2-D GP over the number of rounds. ESTn quickly
minimizes regret. (b) Cumulative regrets for functions sampled from 1-D GP and 2-D GP. ESTn lowers the regret more
than PI and EI and faster than UCB. (c) Regrets ˜rt and simple regrets rt for a 3-D function sampled from GP and the 3-D
Hartmann function. Here, EST is ESTa. For EST, the regret in each round is usually less than that of UCB, explaining
UCB’s higher cumulative regret.

regret ˜rt and the simple regret rt for UCB and EST for a
function sampled from 3-D GP and a standard optimization
test function (the 3-D Hartmann function). UCB tends to
have higher cumulative regret than other methods because
it keeps exploring drastically even after having reached the
optimum of the function. This observation is in agreement
with the experimental results in [30] (they improved UCB’s
performance by scaling λt down by a factor of 5), indicat-
ing that the scheme of setting λt in UCB is not always ideal
in practice.

In summary, the results for synthetic functions suggest that
throughout, compared to other methods, EST ﬁnds better
function values within a smaller number of iterations.

4.2 Initialization Tuning for Trajectory Optimization

In online planning, robots must make a decision quickly,
within a possibly unknown budget (humans can stop the
“thinking period” of the robot any time, asking for a feasi-
ble and good decision to execute). We consider the prob-
lem of trajectory optimization, a non-convex optimization
problem that is commonly solved via sequential quadratic
programming (SQP) [27]. The employed solvers suffer
from sub-optimal local optima, and, in real-world scenar-
ios, even merely reaching a feasible solution can be chal-
lenging. Hence, we use Bayesian Optimization to tune the

Table 2: Lowest reward attained in 20 rounds on the air-
plane problem (illustrated in Figure 2). The results are av-
erages over 8 settings.

RAND
27.4429
3.3962
PI
28.0214
4.1489

UCB
29.2021
3.1247
ESTa
27.7587
4.2783

EI
29.1832
3.1377
ESTn
29.2071
3.1171

mean
std

mean
std

initialization for trajectory optimization. In this setting, x
is a trajectory initialization, and f (x) the score of the solu-
tion returned by the solver after starting it at x.

Our test example is the 2D airplane problem from [32],
illustrated in Figure 2. We used 8 conﬁgurations of the
starting point and ﬁxed the target. Our candidate set X of
initializations is a grid of the ﬁrst two dimensions of the
midpoint state of the trajectory (we do not optimize over
speed here). To solve the SQP, we used SNOPT [9]. Fig-
ure 2 shows the maximum rewards achieved up to round
t (standard deviations scaled by 0.1), and Table 2 displays
the ﬁnal rewards. ESTn achieves rewards on par with the
best competitors. Importantly, we observe that Bayesian
optimization achieves much better results than the standard

Zi Wang, Bolei Zhou, Stefanie Jegelka

Table 3: Classiﬁcation accuracy for visual classiﬁcation
on the test set after the model parameter is tuned. Tuning
achieves good improvements over the results in [37].

Caltech101 Caltech256

[37]
ESTa
ESTn

[37]
ESTa
ESTn

87.22
88.23
88.25
SUN397
42.61
47.13
47.21

67.23
69.39
69.39
Action40
54.92
57.60
57.58

Indoor67
56.79
60.02
60.08
Event8
94.42
94.91
94.86

random restarts, indicating a new successful application of
Bayesian optimization.

4.3 Parameter Tuning for Image Classiﬁcation

Our third set of experiments addresses Bayesian optimiza-
tion for efﬁciently tuning parameters in visual classiﬁca-
tion tasks. Here, x is the model parameter and y the accu-
racy on the validation set. Our six image datasets are stan-
dard benchmarks for object classiﬁcation (Caltech101 [8]
and Caltech256 [10]), scene classiﬁcation (Indoor67 [23]
and SUN397 [35]), and action/event classiﬁcation (Ac-
tion40 [36] and Event8 [18]). The number of images per
data set varies from 1,500 to 100,000. We use deep CNN
features pre-trained on ImageNet [14], the state of the art
on various visual classiﬁcation tasks [25].

Our experimental setup follows [37]. The data is split into
training, validation (20% of the original training set) and
test set following the standard settings of the datasets. We
train a linear SVM using the deep features, and tune its reg-
ularization parameter C via Bayesian optimization on the
validation set. After obtaining the parameter recommended
by each method, we train the classiﬁer on the whole train-
ing set, and then evaluate on the test set.

Figure 3 shows the maximum achieved accuracy on the val-
idation set during the iterations of Bayesian optimization
on the six datasets. While all methods improve the classi-
ﬁcation accuracy, ESTa does so faster than other methods.
Here too, PI and EI seem to explore too little. Table 3 dis-
plays the accuracy on the test set using the best parameter
found by ESTa and ESTn, indicating that the parameter
tuning via EST improved classiﬁcation accuracy. For ex-
ample, the tuning improves the accuracy on Action40 and
SUN397 by 3-4% over the results in [37].

5 Discussion
Next, we discuss a few further details and extensions.

Setting ˆm.
In Section 2, we discussed one way of set-
ting ˆm. There, we used an approximation with independent
variables. We focused on Equations (3) and (4) through-

≥

m that
out the paper since they yield upper bounds ˆm
preserve our theoretical results. Nevertheless, other possi-
bilities for setting ˆm are conceivable. For example, close
in spirit to Thompson and importance sampling, one may
σ(x) ). Furthermore,
sample ˆm from Pr[Y ] =
other search strategies can be used, such as guessing or
doubling, and prior knowledge about properties of f such
as the range can be taken into account.

x∈X Φ( Y −µ(x)

Q

Discretization.
In our approximations, we used dis-
cretizations of the input space. While adding a bit more
detail about this step here, we focus on the noiseless case,
described by Equation (4). Equation (3) can be analyzed
similarly for more general settings. For a Lipschitz con-
tinuous function, it is essentially sufﬁcient to estimate the
X on set W, which is a
probability of f (x)
x
ρ-covering of X.

y,

≤

∈

∀

|

k

x

x

−

| ≤

k ≤

f (x′)

We assume that f is Lipschitz continuous. Our analysis can
be adapted to the milder assumption that f is Lipschitz con-
tinuous with high probability. Let L be the Lipschitz con-
f (x)
Lρ,
stant of f . By assumption, we have
−
ρ. If X is a continuous set, we construct
x′
for all
X, inf x′∈W
its ρ-covering W such that
x′
ρ.
−
k
k ≤
X. Then,
Let EX(y) be the event that f (x)
x
Pr[EX(y)]
−
ρL)] The last step uses Lipschitz continuity to compute
ρL)] = 1. We can use this lower
Pr[EX\W(y)
|
bound to compute ˆm, so ˆm remains an upper bound on m.
Notably, none of the regret bounds relies on a discretiza-
tion of the space. Morever, once ˆm is chosen, the acqui-
sition function can be optimized with any search method,
including gradient descent.

y,
∀
ρL), EX\W(y)] = Pr[EW(y

Pr[EW(y

[EW(y

≤

−

≥

−

∈

∈

x

∀

High dimensions. Bayesian Optimization methods gen-
erally suffer in high dimensions. Common assumptions
are a low-dimensional or simpler underlying structure of f
[7, 34, 15]. Our approach can be combined with those
methods too, to be extended to higher dimensions.

Relation to entropy search.
EST is closely related to
entropy search (ES) methods [12, 13], but also differs in
a few important aspects. Like EST, ES methods approx-
imate the probability of a point x being the maximizer
arg maxx′∈X f (x′), and then choose where to evaluate
next by optimizing an acquisition function related to this
probability. However, instead of choosing the input that is
most likely to be the arg max, ES chooses where to evalu-
ate next by optimizing the expected change in the entropy
of Pr[Mx]. One reason is that ES does not aim to mini-
mize cumulative regret like many other bandit methods (in-
cluding EST). The cumulative regret penalizes all queried
points, and a method that minimizes the cumulative regret
needs to query enough supposedly good points. ES meth-
ods, in contrast, purely focus on exploration, since their
objective is to gather as much information as possible to

Optimization as Estimation with Gaussian Processes in Bandit Settings

10

9

8

7

6

5

4

3

2

1

0

Target

Trajectory 
initializations

30

29

28

27

26

25

d
r
a
w
e
R

24
0
(b)

Start

Obstacles

-1
(a)

-2

0

2

4

6

8

10

RAND
UCB
EI
PI
ESTa
ESTn

5

10
t

15

20

Figure 2: (a) Illustration of trajectory initializations. Trajectory initializations are passed to the non-convex optimization
solver to ensure a solution with as few collisions with the obstacles as possible. (b) Maximum rewards up to round t. EI
and ESTn perform relatively better than other methods.

Caltech101

Caltech256

Indoor67

0.865

0

10

30

40

0.66

0

20
t
SUN397

20
t
Action40

10

30

40

10

30

40

20
t
Event8

0.89

0.885

0.88

0.875

0.87

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.47

0.465

0.46

0.455

0.45

0.445

0.44

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.674

0.672

0.67

0.668

0.666

0.664

0.662

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.58

0.57

0.56

0.55

0.54

0.53

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

0.595

0.59

0.585

0.58

0.575

0.57

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.565

0

0.935

0.93

0.925

0.92

0.915

0.91

0.905

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.9

0

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

0.435

0

10

20
t

30

40

0.52

0

10

20
t

30

40

10

30

40

20
t

Figure 3: Maximum accuracy on the validation set over the iteration of the optimization. ESTa converges faster than other
methods. Experiments are repeated 5 times, the standard deviation is scaled by 1/4.

Zi Wang, Bolei Zhou, Stefanie Jegelka

estimate a ﬁnal value in the very end. Since the focus of
this work lies on cumulative regret, detailed empirical com-
parisons between EST and ES may be found in the supple-
ment.

6 Conclusion

In this paper, we studied a new Bayesian optimization
strategy derived from the viewpoint of the estimating the
arg max of an unknown function. We showed that this
strategy corresponds to adaptively setting the trade-off pa-
rameters λ and θ in GP-UCB and GP-PI, and established
bounds on the regret. Our experiments demonstrate that
this strategy is not only easy to use, but robustly performs
well by measure of different types of regret, on a variety of
real-world tasks from robotics and computer vision.

Acknowledgements. We thank Leslie Kaelbling and
Tom´as Lozano-P´erez for discussions, and Antonio Tor-
ralba for support with computational resources. We
gratefully acknowledge support
from NSF CAREER
award 1553284, NSF grants 1420927 and 1523767, from
ONR grant N00014-14-1-0486, and from ARO grant
W911NF1410433. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of our sponsors.

References

[1] P. Auer.

Using conﬁdence bounds for exploitation-
Journal of Machine Learning Re-

exploration tradeoffs.
search, 3:397–422, 2002.

[2] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analy-
sis of the multiarmed bandit problem. Machine learning, 47
(2-3):235–256, 2002.

[3] E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on
Bayesian optimization of expensive cost functions, with ap-
plication to active user modeling and hierarchical reinforce-
ment learning. Technical Report TR-2009-023, University
of British Columbia, 2009.

[4] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in
In Algorithmic Learning

multi-armed bandits problems.
Theory, pages 23–37. Springer, 2009.

[5] A. D. Bull. Convergence rates of efﬁcient global optimiza-
tion algorithms. Journal of Machine Learning Research, 12:
2879–2904, 2011.

[6] R. Calandra, A. Seyfarth, J. Peters, and M. P. Deisen-
roth. An experimental comparison of Bayesian optimiza-
tion for bipedal locomotion. In International Conference on
Robotics and Automation (ICRA), 2014.

[7] J. Djolonga, A. Krause, and V. Cevher. High-dimensional
Gaussian process bandits. In Advances in Neural Informa-
tion Processing Systems (NIPS), 2013.

[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: An incremental
Bayesian approach tested on 101 object categories. Com-
puter Vision and Image Understanding, 2007.

[9] P. E. Gill, W. Murray, and M. A. Saunders. SNOPT: An SQP
algorithm for large-scale constrained optimization. SIAM
Journal on optimization, 12(4):979–1006, 2002.

[10] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object cat-
egory dataset. Technical Report 7694, California Institute of
Technology, 2007.

[11] S. Gr¨unew¨alder, J.-Y. Audibert, M. Opper, and J. Shawe-
Taylor. Regret bounds for Gaussian process bandit prob-
lems. In International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), 2010.

[12] P. Hennig and C. J. Schuler. Entropy search for information-
efﬁcient global optimization. Journal of Machine Learning
Research, 13:1809–1837, 2012.

[13] J. M. Hern´andez-Lobato, M. W. Hoffman, and Z. Ghahra-
mani. Predictive entropy search for efﬁcient global opti-
mization of black-box functions. In Advances in Neural In-
formation Processing Systems (NIPS), 2014.

[14] Y.

Jia.

Caffe:

An

open

source

tional
http://caffe.berkeleyvision.org/, 2013.

architecture

feature

fast

for

convolu-
embedding.

[15] K. Kandasamy, J. Schneider, and B. Poczos. High di-
mensional Bayesian optimisation and bandits via additive
models. In International Conference on Machine Learning
(ICML), 2015.

[16] A. Krause and C. S. Ong. Contextual Gaussian process ban-
In Advances in Neural Information Pro-

dit optimization.
cessing Systems (NIPS), 2011.

[17] H. J. Kushner. A new method of locating the maximum point
of an arbitrary multipeak curve in the presence of noise.
Journal of Fluids Engineering, 86(1):97–106, 1964.

[18] L.-J. Li and L. Fei-Fei. What, where and who? classify-
ing events by scene and object recognition. In International
Conference on Computer Vision (ICCV), 2007.

[19] D. J. Lizotte, T. Wang, M. H. Bowling, and D. Schuurmans.
Automatic gait optimization with Gaussian process regres-
sion. In International Conference on Artiﬁcial Intelligence
(IJCAI), 2007.

[20] D. J. Lizotte, R. Greiner, and D. Schuurmans. An ex-
perimental methodology for response surface optimization
methods. Journal of Global Optimization, 53(4):699–736,
2012.

[21] P. Massart. Concentration Inequalities and Model Selection,

volume 6. Springer, 2007.

[22] J. Mo˘ckus. On Bayesian methods for seeking the ex-
tremum. In Optimization Techniques IFIP Technical Con-
ference, 1974.

[23] A. Quattoni and A. Torralba. Recognizing indoor scenes. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009.

[24] C. E. Rasmussen and C. K. Williams. Gaussian processes

for machine learning. The MIT Press, 2006.

[25] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. CNN features off-the-shelf: an astounding baseline for
recognition. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2014.

[26] A. M. Ross. Useful bounds on the expected maximum of
correlated normal variables. Technical report, Technical Re-
port 03W-004, ISE Dept., Lehigh Univ., Aug, 2003.

Optimization as Estimation with Gaussian Processes in Bandit Settings

[27] J. Schulman, J. Ho, A. Lee, I. Awwal, H. Bradlow, and
P. Abbeel. Finding locally optimal, collision-free trajecto-
ries with sequential convex optimization. In Robotics: Sci-
ence and Systems Conference (RSS), volume 9, pages 1–10,
2013.

[28] D. Slepian. The one-sided barrier problem for Gaussian
noise. Bell System Technical Journal, 41(2):463–501, 1962.

[29] J. Snoek, H. Larochelle, and R. P. Adams.

Practi-
cal Bayesian optimization of machine learning algorithms.
In Advances in Neural Information Processing Systems
(NIPS), 2012.

[30] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaus-
sian process optimization in the bandit setting: No regret
In International Conference on
and experimental design.
Machine Learning (ICML), 2010.

[31] R. Tedrake. Underactuated Robotics: Algorithms for Walk-
ing, Running, Swimming, Flying, and Manipulation (Course
Notes for MIT 6.832). Downloaded in Fall, 2014 from
http://people.csail.mit.edu/russt/underactuated/.

[32] R. Tedrake.
analysis
nonlinear
http://drake.mit.edu, 2014.

Drake:
for

toolbox

A planning,

control,

dynamical

and
systems.

[33] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown.
Auto-WEKA: combined selection and hyperparameter op-
In ACM SIGKDD
timization of classiﬁcation algorithms.
Conference on Knowledge Discovery and Data Mining
(KDD), 2013.

[34] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Fre-
itas. Bayesian optimization in high dimensions via random
embeddings. In International Conference on Artiﬁcial Intel-
ligence (IJCAI), 2013.

[35] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.
Sun database: large-scale scene recognition from abbey to
zoo. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2010.

[36] B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, and
L. Fei-Fei. Human action recognition by learning bases of
action attributes and parts. In International Conference on
Computer Vision (ICCV), 2011.

[37] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
In Advances in Neural Information Processing
database.
Systems (NIPS), 2014.

Zi Wang, Bolei Zhou, Stefanie Jegelka

Supplement

In this supplement, we provide proofs for all theorems and
lemmas in the main paper, more exhaustive experimental
results and details on the experiments.

A Proofs

A.1 Proofs from Section 2

In any round t,

Lemma 2.1.
the point selected by
EST is the same as the point selected by a variant of
GP-UCB with λt = minx∈X
. Conversely,
the candidate selected by GP-UCB is the same as the
candidate selected by a variant of EST with ˆmt =
maxx∈X µt−1(x) + λtσt−1(x).

ˆmt−µt−1(x)
σt−1(x)

Proof. We omit the subscripts t for simplicity. Let a be the
point selected by GP-UCB, and b selected by EST. Without
loss of generality, we assume a and b are unique. With
λ = minx∈X

, GP-UCB chooses to evaluate

ˆm−µ(x)
σ(x)

a = arg maxx∈X µ(x)+λσ(x) = arg minx∈X

ˆm

µ(x)

−
σ(x)

.

This is because

ˆm = max
x∈X

µ(x) + λσ(x) = µ(a) + λσ(a).

By deﬁnition of b, for all x

X, we have

Pr[Mb
Pr[Mx

ˆm, D]
ˆm, D] ≈

|
|

∈
Q( ˆm−µ(b)
σ(b)
Q( ˆm−µ(x)
σ(x)
Q( ˆm−µ(b)
σ(b)
Q( ˆm−µ(x)
σ(x)

)

)

x′6=b Φ( ˆm−µ(x′
σ(x′)
x′6=x Φ( ˆm−µ(x′)
σ(x′)
)

)

)

)
Q
)Φ( ˆm−µ(x)
Q
σ(x)
)Φ( ˆm−µ(b)
σ(b)

)

=

1.

≥
The inequality holds if and only if ˆm−µ(b)
X, including a, and hence
all x

σ(b) ≤

ˆm−µ(x)
σ(x)

for

∈
ˆm

µ(b)
−
σ(b) ≤

ˆm

µ(a)

−
σ(a)

= λ = min
x∈X

ˆm

µ(x)

−
σ(x)

,

which, with uniqueness, implies that a = b and GP-UCB
and EST select the same point.

For the other direction, we denote the candidate selected by
GP-UCB by

a = arg maxx∈X µ(x) + λσ(x).

The variant of EST with ˆm = maxx∈X µ(x) + λσ(x) se-
lects

b = arg maxx∈X Pr[Mx

ˆm, D].

|

We know that for all x

∈
µ(b) + ˆm−µ(x)
and hence ˆm
λσ(a), letting x = a implies that

≤

X, we have ˆm−µ(b)
σ(x) σ(b). Since ˆm = µ(a) +

ˆm−µ(x)
σ(x)

σ(b) ≤

ˆm = max
x∈X

µ(x) + λσ(x)

µ(b) + λσ(b).

≤

Hence, by uniqueness it must be that a = b and GP-UCB
and EST select the same candidate.

A.2 Proofs from Section 3

Lemma 3.2. Pick δ
T
t=1 π−1
where
that Pr[µt−1(xt)
P
[1, T ].
t

t ≤
−

∈

1
(0, 1) and set ζt = (2 log( πt
2 ,
∈
1, πt > 0. Then, for EST, it holds
δ, for all

ζtσt−1(xt)]

2δ ))

f (xt)

1

≤

≥

−

Proof. Let zt = µt−1(xt)−f (xt)

σt−1(xt)

(0, 1). It holds that

Pr[zt > ζt] =

∼ N

2

e−z

/2 dz

1
√2π
1
√2π

+∞

ζt

Z

+∞

ζt

Z
e−ζ

2
t /2

ζt

Z
t /2.

2

1
2

e−ζ

=

≤

=

e−(z−ζt)

2

/2−ζ

2

t /2−zζt dz

+∞

1
√2π

2

e−(z−ζt)

/2 dz

A union bound extends this bound to all rounds:

Pr[zt > ζt for some t

[1, T ]]

∈

T

≤

t=1
X

2

e−ζ

t /2.

1
2

1

T

2δ ))

With ζt = (2 log( πt
2 and
that with probability at least 1
ζtσt−1(xt) for all t
f (xt)
1
6 π2t2, or πt = T , in which case ζt = ζ = (2 log( T

t=1 π−1
δ, it holds that µt−1(xt)
−
[1, T ]. One may set πt =
1
2 .

t = 1, this implies

−
P
∈

2δ ))

≤

f (xt)

Lemma 3.3. If µt−1(xt)
regret at time step t is upper bounded as ˜rt ≤
ζt)σt−1(xt) , where νt , minx∈X
maxx∈X f (x),
t
∀

ˆmt−µt−1(x)
σt−1(x)

[1, T ].

the
(νt +
, and ˆmt ≥

ζtσt−1(xt),

≤

−

∈

Proof. At time step t

1, we have

≥
f (x)

˜rt = max
x∈X
ˆmt −
ˆmt −

≤

f (xt)

−
f (xt)
µt−1(xt) + ζtσt−1(xt)

≤
= (νt + ζt)σt−1(xt).

Optimization as Estimation with Gaussian Processes in Bandit Settings

B Experiments

B.1 Approximate m

In the paper, we estimate m to be

ˆm = m0 +

∞

1

−

Φ

w

µ(x)

−
σ(x)

dw

m0

Z

(cid:16)

(cid:17)

x∈W
Y
which involves an integration from the current maximum
m0 of the observed data to positive inﬁnity.
In fact the
factor inside the integration quickly approaches zero in
practice. We plot g(w) = 1
in
Figure 4, which looks like half of a Gaussian distribu-
tion. So instead of numerical integration (which can be
done efﬁciently), heuristically we can sample two values of
g(w) to ﬁt ˆg(w) = ae−(w−m0)
and do the integration
m0 ˆg(w) dw = √2πab analytically to be more efﬁcient.
This method is what we called ESTa in the paper, while the
R
numerical integration is called ESTn.

w−µ(x)
σ(x)

x∈W Φ

2
/2b

Q

−

(cid:17)

(cid:16)

∞

2

We notice that our estimation ˆm can serve as a tight upper
bound on the real value of the max of the function in prac-
tice. One example of is shown in Figure 4 with a 1-D GP
function. This example shows how PI, ESTa and ESTn
estimate m. Both ESTa and ESTn are upper bounds of the
true maximum of the function, and ESTn is actually very
tight. For PI, θ = arg max1≤τ <t yτ + ǫ is always a lower
bound of an ǫ shift over the true maximum of the function.

1

0.8

0.6

0.4

0.2

)

w
(
g

m

30

60

50

40

20

10

0

0

4

6

w

8

PI
ESTa
ESTn
max f

0

10

20

30

40

50

t

Figure 4: Top: g(w), w
of m.

∈

[m0, +

); Bottom: estimation

∞

B.2 Synthetic data

We show the examples of the functions we sampled from
GP in Figure 5. The covariance function of GP is an
isotropic Mat´ern kernel with parameters ℓ = 0.1, σf = 1.
The mean function is a linear function with a ﬁxed random
slope for different dimensions, and the constant is 1.

B.3

Initialization tuning for trajectory optimization

The 8 conﬁgurations of start state are [7 1 0 0], [7 0 0 0],
[1 0 0 0], [1 1 0 0], [2 0 0 0], [2 1 0 0], [3 0 0 0], [3 1 0 0],
where the ﬁrst two dimensions denote the position and the
last two dimension denote the speed. We only tune the ﬁrst
two dimension and keep the speed to be 0 for both direc-
tions. The target state is ﬁxed to be [5 9 0 0].

We can initialize the trajectory by setting the mid point
of trajectory to be any point falling on the grid of the
2, 12]). Then
space (both x axis and y axis have range [
use SNOPT to solve the trajectory optimization problem,
which involves an objective cost function (we take the neg-
ative cost to be a reward function to maximize), dynamics
constraints, and obstacle constraints etc. Details of trajec-
tory optimization are available in [32, 31].

−

We used the same settings of parameters for GP as in Sec-
tion B.2 for all the methods we tested and did kernel param-
eter ﬁtting every 5 rounds. The same strategy was used for
the image classiﬁcation experiments in the next section.

B.4 Parameter tuning for image classiﬁcation

We use the linear SVM in the liblinear package for all the
image classiﬁcation experiments. We extract the FC7 ac-
tivation from the imagenet reference network in the Caffe
deep learning package [14] as the visual feature. The re-
ported classiﬁcation accuracy is the accuracy averaged over
all the categories. ‘-c’ cost is the model parameter we tune
for the linear SVM.

In Caltech101 and Caltech256 experiment [8, 10], there are
8,677 images from 101 object categories in the Caltech101
and 29,780 images from 256 object categories. The train-
ing size is 30 images per category, and the rest are test im-
ages.

In SUN397 experiment [35], there are 108,754 images
from 397 scene categories. Images are randomly split into
training and test set. The training size is 50 images per
category, and the rest are test images.

Zi Wang, Bolei Zhou, Stefanie Jegelka

4

3

2

)
x
(
f

1

0

-1

-2

-2

)
x
(
f

4

2

0

-2

-4
2

-1

1

2

0
x

0

x(2)

-2

-2

-1

0

x(1)

2

1

Figure 5: Examples of a function sampled from 1-D GP
(top), and a function sampled from 2-D GP (bottom) with
isotropic Mat´ern kernel and linear mean function. We de-
liberately create many local optimums to make the problem
hard.

In MIT Indoor67 experiment [23], there are 15,620 images
from 67 indoor scene categories. Images are randomly split
into training set and test set. The training size is 100 images
per category, and the rest are test images.

In Stanford Action40 experiment [36], there are 9,532 im-
ages from 40 action categories. Images are randomly split
into training set and test set. The training size is 100 images
per category, and the rest are test images.

In UIUC Event8 experiment [18], there are 1,579 images
from 8 event categories.
Images are randomly split into
training set and test set. Training size is 70 images per
category, and the rest are test images.

We used features extracted from a convolutional neural net-
work (CNN) that was trained on images from ImageNet. It
has been found [37] that features from a CNN trained on
a set of images focused more on places than on objects,
the Places database, work better in some domains. So,
we repeated our experiments using the Places-CNN fea-
tures, and the results are shown in Figure 6 and Table 4.

All the methods help to improve the classiﬁcation accuracy
on the validation set. EST methods achieve good accuracy
on each validation set on par with the best competitors for
most of the datasets. And we also observe that for Cal-
tech101 and Event8, RAND and UCB converge faster and
achieve better accuracy than other methods. As we have
shown in Section 4.1, UCB and RAND perform worse than
other methods in terms of cumulative regret, because they
tend to explore too much. However, more exploration can
be helpful for some black-box functions that do not satisfy
our assumption that they are samples from GP. For exam-
ple, for discontinuous step functions, pure exploration can
be beneﬁcial for simple regret. One possible explanation
for the better results of RAND and UCB is that the black-
box functions we optimize here are possibly functions not
satisfying our assumption. The strong assumption on the
black-box function is also a major drawback of Bayesian
optimization,

B.5 Comparison to entropy search methods

Entropy search methods [12, 13] aim to minimize the
entropy of the probability for the event Mx (x =
arg maxx′∈X f (x′)). Although not suitable for minimiz-
ing cumulative regret, ES methods are intuitively ideal for
minimizing simple regret. We hence in this section com-
pare the empirical performance of entropy search (ES) [12]
and predictive entropy search (PES) [13] to that of the EST
methods (EST/GP-UCB/PI) and EI.

Since both ES and PES only support squared exponential
covariance function and zero mean function in their code
right now, and it requires signiﬁcant changes in their code
to accommodate other covariance functions, we created
synthetic functions that are different from the ones we used
in Section 4 in the paper. The new functions are sampled
from 1-D (80 functions) and 2-D GP (20 functions) with
squared exponential kernel (σf = 0.1 and l = 1) and 0
mean. Function examples are shown in Figure 9.

We show the results on these synthetic functions in Fig-
ure 7,8, and a standard optimization test function, Branin
Hoo function, in Figure 10. It is worth noting that ES meth-
ods make queries on the most informative points, which are
not necessarily the points with low regret. At each round,
ES methods make a “query” on the black-box function, and
then make a “guess” of the arg max of the function (but do
not test the “guess”). We plot the regret achieved by the
“guesses” made by ES methods. For the 1-D GP task, all
the methods behave similarly and achieve zero regret ex-
cept RAND. For the 2-D GP task, EI is the fastest method to
converge to zero regret, and in the end ESTn, PI,EI and ES
methods achieve similar results. For the test on Branin Hoo
function, PES achieves the lowest regret. ESTa converges
slightly faster than PES, but to a slightly higher regret.

We also compared the running time for all the methods in

Caltech101

Caltech256

Indoor67

0.64

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.635

0.63

0.625

0.62

0.615

0

0.582

0.58

0.578

0.576

0.574

0.572

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.57

0.568

0

s
g
n
i
t
t
e
S
t
i
d
n
a
B
n
i

s
e
s
s
e
c
o
r
P
n
a
i
s
s
u
a
G
h
t
i

w
n
o
i
t
a
m

i
t
s
E
s
a
n
o
i
t
a
z
i
m

i
t
p
O

0.46

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.45

0.44

0.43

0.42

0

0.48

0.47

0.46

0.45

0.44

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.43

0

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

0.705

0.7

0.695

0.69

0.685

0.68

0.675

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.67

0

0.92

0.915

0.91

0.905

0.9

t
e
s
 
l
a
v
 
n
o
 
y
c
a
r
u
c
c
a

0.895

0

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

RAND
UCB
EI
PI
ESTa
ESTn

20
t

Event8

20
t

10

30

40

10

30

40

10

30

40

20
t

SUN397

20
t

Action40

10

30

40

10

30

40

10

30

40

20
t

20
t

Figure 6: Maximum accuracy on the validation set over iteration of the optimization. Experiments are repeated 5 times. The visual features used here are Deep CNN
features pre-trained on the Places database.

Table 4: Classiﬁcation accuracy on the test set of the datasets after the model parameter is tuned by ESTa and ESTn. Tuning achieves good improvement over the results
in [37].

Imagenet-CNN feature
ESTa
ESTn
Places-CNN feature
ESTa
ESTn

87.22
88.23
88.25
65.18
66.94
66.95

Caltech101 Caltech256

67.23
69.39
69.39
45.59
47.51
47.43

Indoor67
56.79
60.02
60.08
68.24
70.27
70.27

SUN397 Action40 Event8
94.42
54.92
94.91
57.60
94.86
57.58
94.12
42.86
93.79
46.24
93.56
46.17

42.61
47.13
47.21
54.32
58.57
58.65

Z

i

W
a
n
g
,

B
o
l
e
i

Z
h
o
u

,

S
t
e
f
a
n
i
e
J
e
g
e
l

k
a

Optimization as Estimation with Gaussian Processes in Bandit Settings

t

r

1.5

3.5

2.5

0.5

-0.5

3

2

1

0

3

2

1

0

t

r

1.5

2.5

0.5

RAND
UCB
EI
PI
ES
PES
ESTn

RAND
UCB
EI
PI
ES
PES
ESTn

2

1

0

-1

-2

)
x
(
f

0

)
x
(
f

5

-5
2

t

r

25

50

45

40

35

30

20

15

10

5

0

20

40

60

100

120

140

80

t

-3

-2

-1

0
x

1

2

Figure 7: Simple regret for functions sampled from 1-D GP
with squared exponential kernel and 0 mean.

0

x(2)

-2

-2

0

x(1)

2

100

200

300

400

500

t

Figure 8: Simple regret for functions sampled from 2-D GP
with squared exponential kernel and 0 mean.

Figure 9: Examples of a function sampled from 1-D GP
(left), and a function sampled from 2-D GP (right) with
squared exponential kernel and 0 mean functions. These
functions can be easier than the ones in Figure 5 since they
have fewer local optima.

Table 5. 3 It is assumed in GP optimization that it is more
expensive to evaluate the blackbox function than comput-
ing the next query to evaluate using GP optimization tech-
niques. However, in practice, we still want the algorithm
to output the next query point as soon as possible. For ES
methods, it can be sometimes unacceptable to run them for
black-box functions that take minutes to complete a query.

3All of the methods were run with MATLAB (R2012b), on

Intel(R) Xeon(R) CPU E5645 @ 2.40GHz.

Table 5: Comparison on the running time (s) per iteration.

RAND UCB
0.075
0.0002
ESTn
ESTa
0.55
0.078

EI
0.079
ES
56

PI
0.076
PES
106

RAND
UCB
EI
PI
ES
PES
ESTa
ESTn

5

15

20

10
t

Simple regret for Branin Hoo function.
Figure 10:
UCB/EI/PI/ESTa/ESTn use the isotropic Mat´ern kernel
with parameters ℓ = 0.1, σf = 1; ES/PES use the isotropic
squared exponential kernel with ℓ = 0.1, σf = 1.

