A Dual Ascent Framework for Lagrangean Decomposition of Combinatorial
Problems

Paul Swoboda, Jan Kuske, Bogdan Savchynskyy

7
1
0
2
 
n
a
J
 
2
1
 
 
]
S
D
.
s
c
[
 
 
2
v
0
6
4
5
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

We propose a general dual ascent framework for La-
grangean decomposition of combinatorial problems. Al-
though methods of this type have shown their efﬁciency for a
number of problems, so far there was no general algorithm
applicable to multiple problem types. In this work, we pro-
pose such a general algorithm. It depends on several param-
eters, which can be used to optimize its performance in each
particular setting. We demonstrate efﬁcacy of our method on
graph matching and multicut problems, where it outperforms
state-of-the-art solvers including those based on subgradient
optimization and off-the-shelf linear programming solvers.

1. Introduction

Computer vision and machine learning give rise to a num-
ber of powerful computational models. It is typical that
inference in these models reduces to non-trivial combinato-
rial optimization problems. For some of the models, such
as conditional random ﬁelds (CRF), powerful specialized
solvers like [46, 47, 11, 50] were developed. In general, how-
ever, one has to resort to off-the-shelf integer linear program
(ILP) solvers like CPLEX [2] or Gurobi [35]. Although
these solvers have made a tremendous progress in the past
decade, the size of the problems they can tackle still remains
a limiting factor for many potential applications, as the run-
ning time scales super-linearly in the problem size. The goal
of this work is to partially ﬁll this gap between practical
requirements and existing computational methods.

It is an old observation that many important optimization
ILPs can be efﬁciently decomposed into easily solvable com-
binatorial sub-problems [31]. The convex relaxation, which
consists of these sub-problems coupled by linear constraints
is known as Lagrangean or dual decomposition [30, 48]. Al-
though this technique can be efﬁciently used in various sce-
narios to ﬁnd approximate solutions of combinatorial prob-
lems, it has a major drawback: In the most general setting
only slow (sub)gradient-based techniques [49, 55, 48, 40, 59]
can be used for optimization of the corresponding convex
relaxation.

In the area of conditional random ﬁelds, however, it

is well-known [39] that message passing or dual (block-
coordinate) ascent algorithms (like e.g. TRW-S [46]) signiﬁ-
cantly outperform (sub)gradient-based methods. Similar ob-
servations were made much earlier in [57] for a constrained
shortest path problem.

Although dual ascent algorithms were proposed for a
number of combinatorial problems (see the related work
overview below), there is no general framework, which
would (i) give a generalized view on the properties of such
algorithms and more importantly (ii) provide tools to eas-
ily construct such algorithms for new problems. Our work
provides such a framework.

Related Work Dual ascent algorithms optimize a dual
problem and guarantee monotonous improvement (non-
deterioration) of the dual objective. The most famous exam-
ples in computer vision are block-coordinate ascent (known
also as message passing) algorithms like TRW-S [46] or
MPLP [27] for maximum a posteriori inference in condi-
tional random ﬁelds [39].

To the best of our knowledge the ﬁrst dual ascent algo-
rithm addressing integer linear programs belongs to Bilde
and Krarup [10] (the corresponding technical report in Dan-
ish appeared 1967). In that work an uncapacitated facility
location problem was addressed. A similar problem (sim-
ple plant location) was addressed with an algorithm of the
In 1980 Fisher and Hochbaum [21]
same class in [29].
constructed a dual ascent-based algorithm for a problem of
database location in computer networks, which was used to
optimize the topology of Arpanet [1], predecessor of Internet.
The generalized linear assignment problem was addressed
by the same type of algorithms in [22]. The Authors con-
sidered a Lagrangean decomposition of this problem into
multiple knapsack problems, which were solved in each iter-
ation of the method. An improved version of this algorithm
was proposed in [33]. Efﬁcient dual ascent based solvers
were also proposed for the min-cost ﬂow in [24], for the set
covering and the set partitioning problems in [23] and the
resource-constrained minimum weighted arborescence prob-
lem in [34]. The work [32] describes basic principles for
constructing dual ascent algorithms. Although the authors
provide several examples, they do not go beyond that and

1

stick to the claim that these methods are structure dependent
and problem speciﬁc.

The work [17] suggests to use the max-product belief
propagation [71] to decomposable optimization problems.
However, their algorithm is neither monotone nor even con-
vergent in general.

In computer vision, dual block coordinate ascent algo-
rithms for Lagrangean decomposition of combinatorial prob-
lems were proposed for multiple targets tracking [7], graph
matching (quadratic assignment) problem [76] and inference
in conditional random ﬁelds [46, 47, 27, 72, 73, 60, 36, 54,
70]. From the latter, the TRW-S algorithm [46] is among the
most efﬁcient ones for pairwise conditional random ﬁelds
according to [39]. The SRMP algorithm [47] generalizes
TRW-S to conditional random ﬁelds of arbitrary order. In a
certain sense, our framework can be seen as a generalization
of SRMP to a broad class of combinatorial problems.

Contribution. We propose a new dual ascent based com-
putational framework for combinatorial optimization. To
this end we:
(i) Deﬁne the class of problems, called integer-relaxed
pairwise-separable linear programs (IRPS-LP), our frame-
work can be used for. Our deﬁnition captures Lagrangean
decompositions of many known discrete optimization prob-
lems (Section 2).
(ii) Give a general monotonically convergent message-
passing algorithm for solving IRPS-LP, which in particu-
lar subsumes several known solvers for conditional random
ﬁelds (Section 4).
(iii) Give a characterization of the ﬁxed points of our al-
gorithm, which subsumes such well-known ﬁxed point
characterizations as weak tree agreement [46] and arc-
consistency [72] (Section 5).

We demonstrate efﬁciency of our method by outperform-
ing state-of-the-art solvers for two famous special cases of
IRPS-LP, which are widely used in computer vision: the
multicut and the graph matching problems. (Section 6).

A C++-framework containing the above mentioned
solvers and the datasets used in experiments are available un-
der http://github.com/pawelswoboda/LP_MP.

We give all proofs in the supplementary material.

Notation. Undirected graphs will be denoted by G =
(cid:1) is the
(V, E), where V is a ﬁnite node set and E
⊆
edge set. The set of neighboring nodes of v
V w.r.t. graph
. The convex hull
G is denoted by
∈
Rn is denoted by conv(X). Disjoint union is
of a set X
⊂
denoted by ˙
.
∪

u : uv
{

G(v) :=

(cid:0)V
2

N

E

∈

}

2

2. Integer-Relaxed Pairwise-Separable Linear

Programs (IRPS-LP)

(cid:105)

i=1,...,k

0, 1

(cid:80)k

⊆ {

⊆ {

i=1(cid:104)

θi, xi

Combinatorial problems having an objective to min-
n of bi-
imize some cost θ(x) over a set X
0, 1
}
nary vectors often have a decomposable representation as
di being sets of
for Xi
min xi∈Xi
}
binary vectors, typically corresponding to subsets of the
coordinates of X. This decomposed problem is equiva-
lent to the original one under a set of linear constraints
A(i,j)xi = A(j,i)xj, which guarantee the mutual consis-
tency of the considered components. Replacing Xi by its
convex hull conv(Xi) and therefore switching to real-valued
vectors from binary ones one obtains a convex relaxation1 of
the problem, which reads:

k
(cid:88)
(cid:104)

i=1

min
ΛG
µ
∈

(cid:105)

θi, µi

, where ΛG is deﬁned as

(1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
1, . . . , k
{
(cid:19)
(cid:18)F
2

(cid:26)

ΛG :=

(µ1 . . . µk)

µi ∈ conv(Xi)
A(i,j)µi = A(j,i)µj

(cid:27)

i ∈ F
∀ij ∈ E

.

(2)

are called factors of the decompo-

Here F :=
}
sition and E
are called coupling constraints. The
undirected graph G = (F, E) is called factor graph. We
will use variable names µ whenever we want to emphasize
µi

conv(Xi) and x whenever xi

Xi, i

F.

⊆

∈

∈

E
Deﬁnition 1 (IRPS-LP). Assume that for each edge ij
the matrices of the coupling constraints A(i,j), A(j,i) are
K
di and A(i,j)xi
such that A(i,j) ∈ {
0, 1
×
}
}
N, analogously for A(j,i). The prob-
Xi for some K
xi
∈
∀
∈
lem minµ
is called an Integer-Relaxed
θi, µi
ΛG
F
(cid:104)
∈
Pairwise-Separable Linear Program, abbreviated by IRPS-
LP.

∈ {

0, 1

(cid:80)

i
∈

∈

K

(cid:105)

∈

In the following, we give several examples of IRPS-LP.
To distinguish between notation for the factor graph of IRPS-
LP, where we stick to bold letters (such as G, F, E) we
will use the straight font (such as G, V, E) for the graphs
occurring in the examples.
Example 1 (MAP-inference for CRF). A conditional ran-
dom ﬁeld is given by a graph G = (V, E), a discrete label
space X = (cid:81)
R and pairwise
V Xu, unary θu : Xu
u
→
∈
E. We also
V, uv
costs θuv : Xu
Xv
×
→
Xv. The associated maximum a poste-
denote Xuv := Xu
riori (MAP)-inference problem reads

R for u

×

∈

∈

(cid:88)

θu(xu) +

(cid:88)

min
X
x
∈

u

V

∈

uv

E

∈

θuv(xuv) ,

(3)

where xu and xuv denote the components corresponding to
E respectively. The well-known
node u

V and edge uv

∈

∈

1More precisely, this is a linear programming relaxation, since a convex

hull of a ﬁnite set can be represented in terms of linear inequalities

}

∈

∪

V and each edge uv

local polytope relaxation [72] can be seen as an IRPS-LP
by setting F = V
E, that is associating to each node
E a factor, and introducing two
v
∈
coupling constraints for each edge of the graphical model, i.e.
E =
E
. For the sake of notation
v, uv
}
{
we will assume that each label s
Xu is associated a unit
∈
, 0 . . . , 0) with dimensionality equal to
vector (0, . . . , 0, 1
(cid:124)(cid:123)(cid:122)(cid:125)
s

u, uv

: uv

,
}

{{

and 1 on the s-th position.
the total number of labels
Xu
|
Therefore, the notation conv(Xu) makes sense as a convex
hull of all such vectors. After denoting an N -dimensional
+ : (cid:80)N
RN
simplex as ∆N :=
the resulting
relaxation reads

i=1 µi = 1
}

µ
{

∈

∈

|

min
LG
µ
∈

θ, µ
(cid:105)
(cid:104)

:=

θu, µu
(cid:104)

(cid:105)

+

θuv, µuv
(cid:104)

(cid:105)

(4)

(cid:88)

u

V

∈

(cid:88)

uv

E

∈

in the overcomplete representation [69] and LG is deﬁned as

µu ∈ conv(Xu) :
µuv ∈ conv(Xuv) :
A(uv,u)µuv = A(u,uv)µu : (cid:80)
xv ∈Xv

µu ∈ ∆|Xu|, u ∈ V
µuv ∈ ∆|Xuv |, uv ∈ E

µuv(xu, xv) = µu(xu),

(5)

uv ∈ E, (xu, xv) ∈ Xuv,
u ∈ uv, xu ∈ Xu .

Here µu(xu) and µuv(xu, xv) denote those coordinates of
vectors µu and µuv, which correspond to the label xu and
the pair of labels (xu, xv) respectively.
Example 2 (Graph Matching). The graph matching problem,
also known as quadratic assignment [12] or feature match-
ing, can be seen as a MAP-inference problem for CRFs (as
in Example 1) equipped with additional constraints: The
label set of G belongs to a universe
V
, i.e. Xu
can be assigned at most once. The
and each label s
∈ L
overall problem reads

⊆ L ∀

L

∈

u

min
x

(cid:88)

u

V

∈

(cid:88)

uv

E

∈

θu(xu) +

θuv(xu, xv) s.t. xu

= xv

u

= v .

∀

(6)
Graph matching is a key step in many computer vision ap-
plications, among them tracking and image registration,
whose aim is to ﬁnd a one-to-one correspondence be-
tween image points. For this reason, a large number of
solvers have been proposed in the computer vision commu-
nity [17, 74, 76, 68, 51, 67, 61, 28, 77, 37, 52, 15]. Among
them two recent methods [68, 76] based on Lagrangean de-
composition show superior performance and provide lower
bounds for their solutions. The decomposition we describe
below, however, differs from those proposed in [68, 76].

Our IRPS-LP representation for graph matching consists
of two blocks: (i) the CRF itself (which further decomposes
into node- and edge-subproblems with variables (µu)u
V
and (ii) additional label-factors keeping track of nodes as-
signed the label s. We introduce these label-factors for each
. The set of possible conﬁgurations of this fac-
label s
∈ L
consists of those
tor Xs :=

V : s

Xu

#

u

∈

{

∈

∈

} ∪ {

}

3

∈

V which can be assigned the label s and an
nodes u
additional dummy node #. The dummy node # denotes
non-assignment of the label s and is necessary, as not every
label needs to be taken. As in Example 1, we associate a unit
binary vector with each element of the set Xs, and conv(Xs)
denotes the convex hull of such vectors. The set of factors
becomes F = V ˙
, with the set E =
:
,
{{
∪
}
}
E
of the factor-graph
uv
u, l
∈
edges. The resulting IRPS-LP formulation reads

∪L
: u
}

} ∪ {{

u, uv

v, uv

V, l

Xu

E ˙

∈

∈

{

}

θuv, µuv

+

(cid:105)

˜θs, ˜µs
(cid:104)

(cid:105)

(7)

(cid:88)

s

∈L

(cid:88)

min
µ,˜µ

θu, µu

+

(cid:105)

(cid:104)

u
V
∈
LG
µ
conv(Xs),
˜µs
µu(s) = ˜µs(u),

∈
∈

(cid:88)

(cid:104)
E

uv

∈

s
s

∈ L
∈

Xu .

0

≡

s
∀

Here we introduced (i) auxiliary variables ˜µs(u) for all vari-
ables µu(s) and (ii) auxiliary node costs ˜θs
,
∈ L
which may take other values in course of optimization. Fac-
tors associated with the vectors µu and µuv correspond to the
nodes and edges of the graph G (node- and edge-factors), as
in Example 1 and are coupled in the same way. Additionally,
factors associated with the vectors ˜µs ensure that the label s
can be taken at most once. These label-factors are coupled
with node-factors (last line in (7)).
Example 3 (Multicut). The multicut problem (also known
as correlation clustering) for an undirected weighted graph
G = (V, E) is to ﬁnd a partition (Π1, . . . , Πk), Πi
V,
k
V = ˙
i=1Πi of the graph vertexes, such that the total cost
∪
of edges connecting different components is minimized. The
number k of components is not ﬁxed but is determined by
the algorithm. See Fig. 1 for an illustration. Although the
problem has numerous applications in computer vision [3, 4,
5, 75] and beyond [6, 58, 13, 14], there is no scalable solver,
which could provide optimality bounds. Existing methods
are either efﬁcient primal heuristics [64, 56, 26, 18, 19, 8, 9]
or combinatorial branch-and-bound/branch-and-cut/column
generation algorithms, based on off-the-shelf LP solvers [41,
42, 45, 75]. Move-making algorithms do not provide lower
bounds, hence, one cannot judge their solution quality or
employ them in branch-and-bound procedures. Off-the-shelf
LP solvers on the other hand scale super-linearly, limiting
their application in large-scale problems.

⊆

Instead of directly optimizing over partitions (which has
many symmetries making optimization difﬁcult in a linear
programming setting), we follow [16] and formulate the
E denote the cost of
problem in the edge domain. Let θe, e
graph edges and let C be the set of all cycles of the graph G.
Each edge that belongs to different components is called a
cut edge. The multicut problem reads

∈

min
0,1
}
∈{

xE

|E|

(cid:88)

e

E

∈

θexe , s.t.

C

∀

e(cid:48)

∀

∈

C :

(cid:88)

e

C

e(cid:48)

∈

\{

}

xe

xe(cid:48) .

≥

(8)

Here xe = 1 signiﬁes a cut edge and the inequalities force
each cycle to have none or at least two cut edges. The formu-
lation (8) has exponentially many constraints. However, it
is well-known that it is sufﬁcient to consider only chordless
cycles [16] in place of the set C in (8). Moreover, the graph
can be triangulated by adding additional edges with zero
weights and therefore the set of chordless cycles reduces to
edge triples. Such triangulation is refered to as chordal com-
pletion in the literature [25]. The number of triples is cubic,
which is still too large for practical efﬁciency and therefore
violated constraints are typically added to the problem iter-
atively in a cutting plane manner [41, 42]. To simplify the
description, we will ignore this fact below and consider all
these cycles at once. Assuming a triangulated graph and
redeﬁning C as the set of all chordless cycles (triples) we
consider the following IRPS-LP relaxation of the multicut
problem 2:

(cid:88)

θeµe +

(cid:88)

(cid:88)

˜θe,c ˜µe,c ,

s.t.

(9)

e

c

c

e

E

∈

∈

C
∈
µe ∈ conv{{0, 1}} = [0, 1], e ∈ E
∀c ∈ C, e ∈ c :
˜µc := (˜µe,c)e∈c ∈ conv{{0, 1}3| ∀e(cid:48) ∈ c : (cid:80)

≡ conv{{0, 0, 0}, {0, 1, 1}, {1, 0, 1}, {1, 1, 0}, {1, 1, 1}}

µe = ˜µe,c

min
µ,˜µ






˜µe,c ≥ ˜µe(cid:48),c}

e∈c\{e(cid:48)}

(10)

}

{

∈

µ(cid:48)

0, 1

0, 1

conv

∈
{{

n : constraints on µ(cid:48)
}

For the sake of notation we shortened a feasible set def-
inition ˜µ
to
∈ {
n : constraints on ˜µ
. Here µe is the re-
˜µ
conv
}
}
laxed (potentially non-integer) variable corresponding to xe.
Variable ˜µe,c is a copy of µe, which corresponds to the cy-
cle c. Therefore, each µe gets as many copies ˜µe,c, as many
chordless cycles c contain the edge e. For each cycle the set
of binary vectors satisfying the cycle inequality is considered.
For a cycle with 3 edges this set can be written explicitly as
E we copy the corre-
in (10). Along with copies of µe, e
sponding cost θe and create auxiliary costs ˜θe,c
0 for each
cycle c containing the edge e. During optimization, the cost
θe will be redistributed between θe itself and its copies ˜θe,c,
C. The factors of the IRPS-LP are associated with each
c
edge (variable µe) and each chordless cycle (variable ˜µc).
Coupling constraints connect edge-factors with those cycle-
factors, which contain the corresponding edge (see the last
constraint in (10)). An in-depth discussion of message pass-
ing for the multicut problem with tighter relaxations can be
found in [66].

≡

∈

∈

3. Dual Problem and Admissible Messages

Since our technique can be seen as a dual ascent, we will
not optimize the primal problem (1) directly, but instead

2One can show that this relaxation coincides with the standard LP

relaxation for the multicut problem [16]

4

Figure 1. Illustration of
Example 3. A multicut
of a graph induced by
three connected compo-
nents Π1, Π2, Π3 (green).
Red dotted edges indicate
cut edges xe = 1.

maximize its dual lower bound.

Dual IPS-LP The Lagrangean dual to (1) w.r.t. the cou-
pling constraints reads

i

∈

∈

∈

F

∈

∈

j:ij

(11)

F minxi

maxφ D(φ) := (cid:80)
i := θi + (cid:80)
θφ
s.t.
φ(i,j) =

θφ
i , xi
Xi(cid:104)
(cid:105)
i
E A(cid:62)(i,j)φ(i,j) ∀
E
ij
φ(j,i) ∀
−
RK for A(i,j) ∈ {

di for some K

N.
Here φ(i,j) ∈
The function D(φ) is called lower bound and is concave in φ.
The modiﬁed primal costs θφ are called reparametrizations
of the potentials θ. We have duplicated the dual variables
φ(j,i) to symmetrize notation. In
by introducing φ(i,j) :=
practice, only one copy is stored and the other is computed on
the ﬂy. Note that in this doubled notation the reparametrized
node and edge potentials of the CRF from Example 1 read

K
0, 1
}

−

∈

×

u(xu) = θu(xu) + (cid:80)
θφ
θφ
uv(xu, xv) = θuv(xu, xv) + φuv,v(xv) + φuv,u(xu)
φu,uv =

E φu,uv(xu)

φuv,u

v : uv

∈

−

It is well-known for CRFs that cost of feasible solutions
are invariant under reparametrization. We generalize this to
the IRPS-LP-case.
Proposition 1. (cid:80)
θφ
i , µi
F
i
(cid:104)
∈
µ1, . . . , µk obey the coupling constraints.

, whenever
(cid:105)

θi, µi
(cid:104)

= (cid:80)

(cid:105)

∈

F

i

Admissible Messages While Proposition 1 guarantees
that the primal problem is invariant under reparametriza-
tions, the dual lower bound D(φ) is not. Our goal is to ﬁnd
φ such that D(φ) is maximal. By linear programming du-
ality, D(φ) will then be equal to the optimal value of the
primal (1).

First we will consider an elementary step of our future al-
gorithm and show that it is non-decreasing in the dual objec-
tive. This property will ensure the monotonicity of the whole
algorithm. Let θφ be any reparametrization of the problem
and D(φ) be the corresponding dual value. Let us consider
changing the reparametrization of a factor i by a vector ∆
with the only non-zero components ∆(i,j) and ∆(j,i) . This
will change reparametrization of the coupled factors j (such
∆(j,i). The lemma below
that ij
states properties of ∆(i,j) which are sufﬁcient to guarantee
improvement of the corresponding dual value D(φ + ∆):

E) due to ∆(i,j) =

−

∈

∆(i,j)(s)

, where ν := A(i,j)x∗i .

(12)

is a good choice. Although different δ

E be a pair
Lemma 1 (Monotonicity Condition). Let ij
of factors related by the coupling constraints and φ(i,j) be
and
a corresponding dual vector. Let x∗i ∈
∆(i,j) satisfy

argmin
Xi (cid:104)
xi

θφ
i , xi

∈

(cid:105)

∈

(cid:40)

0,
0,

≥
≤

ν(s) = 1
ν(s) = 0

Then x∗i ∈

argmin
Xi (cid:104)
xi

θφ+∆
i

, xi

(cid:105)

∈

implies D(φ)

D(φ + ∆).

≤

∈

(cid:75)

(cid:74)

}

∈

∈

u, uv

, where u

Example 4. Let us apply Lemma 1 to Example 1. Let ij
V is some node and
correspond to
{
E is any of its incident edges. Then x∗i corresponds
uv
Xu θu(s) and
to a locally optimal label x∗u ∈
arg mins
. Therefore we may assign ∆u,uv(s)
ν(s) =
s = x∗u
θu(s)]. This assures that
to any value from [0, θu(x∗u)
(20) is fulﬁlled and x∗u remains a locally optimal label after
reparametrization even if there are multiple optima in Xu.
Lemma 1 can be straightforwardly generalized to the
case, when more than two factors must be reparametrized
simultaneously. In terms of Example 1 this may correspond
to the situation when a graph node sends messages to several
incident edges at once:

−

F

i

a

be

Let

factor

} ⊆ N
:= θi + (cid:80)

G(i) be a subset of

and
2.
Deﬁnition
∈
its neigh-
J =
j1, . . . , jl
{
bors. Let θ∆
J A(cid:62)(i,j)∆(i,j), ∆(i,j)(=
∆(j,i))
i
∈
J and all other coordinates of ∆
satisﬁes (20) for all j
such
If there exists x∗i ∈
are zero.
argminxi
Xi(cid:104)
θ∆
, the dual vector ∆ is called
that x∗i ∈
i , xi
argminxi
Xi(cid:104)
(cid:105)
∈
admissible. The set of admissible vectors is denoted by
AD(θi, x∗i , J).

θi, xi

−

∈

(cid:105)

∈

j

Lemma 2. Let ∆

AD(θφ

i , x∗i , J) then D(φ)

D(φ+∆).

∈

≤

Procedure 1: Message-Passing Update Step.

F, neighboring factors

G(i), dual variables φ

J =

1 Input:Factor i
j1, . . . , jl
{
Compute x∗i ∈

∈
} ⊆ N
arg minxi

Choose δ

Rdi s.t. δ(s)

∈

θφ, xi

∈

Xi (cid:104)
(cid:105)
(cid:26) > 0, x∗i (s) = 1
< 0, x∗i (s) = 0

2 Maximize admissible messages to J:

∆∗(i,J) ∈

argmax
AD(θφ
i ,x∗

i ,J)(cid:104)

δ, θφ+∆
i

(cid:105)

∆

∈

3 Output: ∆∗(i,J).

(13)

(14)

(15)

5

Message-Passing Update Step To maximize D(φ), we
will iteratively visit all factors and adjust messages φ con-
nected to it, monotonically increasing the lower bound (11).
Such an elementary step is deﬁned by Procedure 1.

Procedure 1 is deﬁned up to the vector δ, which
δ(s) =

(see Proc. 1).

Usually,

(14)

satisﬁes
(cid:26) 1, x∗i (s) = 1
1, x∗i (s) = 0

−

may result in different efﬁciency of our framework, fulﬁll-
ment of (14) is sufﬁcient to prove its convergence properties.
The reparametrization adjustment problem (15) serves an
intuitive goal to move as much slack as possible from the
factor i to its neighbors J. For example, for the setting of
θφ
Example 4 its solution reads ∆u,uv(s) = θφ
u(s).
Depending on the selected δ it might correspond to maxi-
mization of the dual objective in the direction deﬁned by
admissible reparametrizations. Although maximization (15)
is not necessary to prove convergence of our method (as we
show below, only a feasible solution of (15) is required for
the proof), (i) it leads to faster convergence; (ii) for the case
of CRFs (as in Example 1) it makes our method equivalent to
well established techniques like TRW-S [46] and SRMP [47],
as shown in Section 4.1.

u(x∗u)

−

The following proposition states that the elementary up-
date step deﬁned by Procedure 1 can be performed efﬁciently.
That is, the size of the reparametrization adjustment prob-
lem (15) grows linearly with the size of the factor i and its
attached messages:

n

Proposition 2. Let conv(Xi) =
0, 1
A
}
∈ {
n1, . . . , n
J
|
) variables and O(m+n1 +. . .+n
. . .+n
J
|

with
{
m. Let the messages in problem (15) have size
×
. Then (15) is a linear program with O(n + n1 +
) constraints.

µi : Aiµi

J
|

≤

bi

}

|

|

|

4. Message Passing Algorithm

Now we combine message passing updates into Algo-
rithm 2. It visits every node of the factor graph and performs
the following two operations: (i) Receive Messages, when
messages are received from a subset of neighboring factors,
and (ii) Send Messages, when messages to some neighbor-
ing factors are computed and reweighted by ω. Distribution
of weights ω may inﬂuence the efﬁciency of Algorithm 2
just like it inﬂuences the efﬁciency of message passing for
CRFs (see [47]). We provide typical settings in Section 4.1.
Usually, factors are traversed in some given a-priori order
alternately in forward and backward direction, as done in
TRW-S [46] and SRMP [47]. We refer to [47] for a motiva-
tion for such a schedule of computations.

We will discuss parameters of Algorithm 2 (factor par-
, weights wJi) right after the theorem stating
}

titioning
monotonicity for any choice of parameters.

Ji

{

Theorem 1. Algorithm 2 monotonically increases the dual
lower bound (11).

Algorithm 2: One Iteration of Message-Passing

F in some order do

1 for i
2

∈

Receive Messages:
Choose a subset of connected factors
Jreceive
for j

G(i)

∈

⊆ N
Jreceive do
Compute ∆∗(j,
Set

{
φ = φ + ∆∗(j,

i
}

) with Procedure 1.

i

).
}

{

end

Send Messages:
Choose partition J1 ˙
∪
for J
J1, . . . , Jl
}
Compute ∆∗(i,J) with Procedure 1.

. . . ˙
∪
do

⊆ N

∈ {

Jl

G(i).

end
Choose weights ωJ1, . . . , ωJl ≥
ωJ1 + . . . + ωJl ≤
for J
J1, . . . , Jl
∈ {
Set φ = φ + ωJ ∆∗(i,J).

do

1.

}

0 such that

3

4

5

6

7

8

9

10

11

12

13

14

15

16

end

17
18 end

4.1. Parameter Selection for Algorithm 2

There are the following free parameters in Algorithm 2:
(i) The order of traversing factors of F; (ii) for each factor
the neighboring factors from which to receive messages
G(i)
Jreceive
of factors to send messages to and (iv) the associated weights
ωJ1, . . . , ωJl for messages.

G(i); (iii) the partition J1 ˙
∪

. . . ˙
∪

⊆ N

⊆ N

Jl

Although for any choice of these parameters Algorithm 2
monotonically increases the dual lower bound (as stated by
Theorem 1), its efﬁciency may signiﬁcantly depend on their
values. Below, we will describe the parameters for Exam-
ples 1-3, which we found the most efﬁcient empirically. Ad-
ditionally, in the supplement we discuss parameters, which
turn our algorithm into existing message passing solvers for
CRFs (as in Example 1).

Sending a message by some factor automatically implies
receiving this message by another, coupled factor. There-
fore, usually there is no need to go over all factors in Algo-
rithm 2. It is usually sufﬁcient to guarantee that all coupling
constraints are updated by Procedure 1. Formally, we can
always exclude processing some factors by setting Jreceive
and Ji, i = 1, . . . , l to the empty set. Instead, we will ex-
plicitly specify, which factors are processed in the loop of
Algorithm 2 in the examples below.

only node factors in Algorithm 2 is sufﬁcient. Below, we de-
scribe parameters, which turn Algorithm 2 into SRMP [47]
(which is up to details of implementation equivalent to TRW-
S [46] for pairwise CRFs). Other settings, given in the
supplement, may turn it to other popular message passing
techniques like MPLP [27] or min-sum diffusion [62].

∈

∈

∈

u and outgoing E−u edges for each node u

We order node factors and process them according to this
ordering. The ordering naturally deﬁnes the sets of incoming
E+
V. Here
E is incoming for u if v < u and outgoing if v > u.
uv
V receives messages from all incoming edges,
Each node u
u . The messages are send
which is Jreceive =
E in the partition in
to all outgoing edges, each edge uv
line 10 of Algorithm 2 is represented by a separate set. That
is, the partition reads ˙
. Weights are distributed
e
E−
∪e
}
u {
E−u .
uniformly and equal to we =
{
After each outer iteration, when all nodes were processed,
the ordering is reversed and the process repeats. We refer
to [47] for substantiation of these parameters.

G(u) = E+

E+
,
u
|
|

1
E−
u

|} }

, e

max

N

∈

∈

{|

∈

∈

Parameters for Example 2, Graph Matching. Addition-
ally to the node and edge factors, the corresponding IRPS-LP
has also label factors (7). To this end all node factors are
V re-
ordered, as in Example 1. Each node factor u
ceives messages from all incoming edge factors and label
factors Jreceive(u) = E+
Xu and sends them to all out-
going edges and label factors. The corresponding partition
reads ˙
Xu. The weights are distributed
f
E+
∪f
u {
∈N
. The label factors
uniformly with wf =
{|
are processed after all node factors were visited. Each label
factor receives messages from all connected node factors
V :
and send messages back as well: Jreceive(s) =
. We use the same single set for sending messages,
s
}
i.e. J1 = Jreceive. After each iteration we reverse the factor
order.

u ∪
˙
}
∪
1+max
{

E+
,
u
|
|

u
{

1
E−
u

|} }

Xu

G(u)

∈

∈

\

Parameters for Example 3, Multicut. Similarly to Ex-
ample 1, it is sufﬁcient to go only over all edge factors
in the loop of Algorithm 2, since each coupling constraint
contains exactly one cycle and one edge factor. Each edge
factor e receives messages from all coupled cycle factors
) and sends them to the
c
Jreceive =
}
∈
same factors. As in Example 1, each cycle factor forms a
trivial set in the partition in line 10 of Algorithm 2, the parti-
tion reads ˙
. Weights are distributed uniformly
c
c
{
}
∪
∈
. After each iteration the processing
with we =
c
c
|
∈
order of factors is reversed.

C:e
∈
1
C:e

c
G(
{

C : e

N

∈

∈

c

|

4.2. Obtaining Integer Solution

Parameters for Example 1, MAP-inference in CRFs.
Pairwise CRFs have the speciﬁc feature that node factors are
coupled with edge factors only. This implies that processing

Eventually we want to obtain a primal solution x
X
of (1), not a reparametrization θφ. We are not aware of any
rounding technique which would work equally well for all

∈

6

possible instances of IRPS-LP problem. According to our
experience, the most efﬁcient rounding is problem speciﬁc.
Below, we describe our choices for the Examples 1 – 3.

Rounding for Example 1 coincides with the one sug-
gested in [46]: Assume we have already computed a primal
integer solution x∗v for all v < u and we want to compute x∗u.
To this end, right before the message receiving step of Algo-
rithm 2 for i = u we assign

x∗u ∈

argmin
xu

θu(xu) +

θuv(xu, x∗v) .

(16)

(cid:88)

v<u:uv

E

∈

Rounding for Example 2 is the same except that we se-
lect the best label xu among those, which have not been
assigned yet, to satisfy uniqueness constraints:

x∗u ∈

argmin
xu:x∗
=xu
v(cid:54)

∀

v<u

θφ
u(xu) +

(cid:88)

θφ
uv(xu, x∗v) . (17)

v<u:uv

E

∈

Rounding for Example 3. We use
efﬁcient
Kernighan&Lin heuristic [43] as implemented in [44]. Costs
for the rounding are the reparametrized edge potentials.

the

5. Fixed Points and Comparison to Subgradi-

ent Method

Algorithm 2 does not necessarily converge to the op-
Instead, it may get stuck in suboptimal
timum of (1).
points, similar to those correspoding to the ”weak tree agree-
ment” [46] or ”arc consistency” [72] in CRFs from Exam-
ple 1. Below we characterise these ﬁxpoints precisely.

3

Deﬁnition
reparametrization θφ,
non-empty set Si
⊆
given. Deﬁne S = (cid:81)
i
marginally consistent for S on ij

(Marginal Consistency). Given

a
F a
let
F be
θφ, xi
argminxi
F Si. We call reparametrization θφ

for each factor i
, i

Xi(cid:104)
∈
E if

∈
∈

(cid:105)

∈

∈

A(i,j) (Si) = A(j,i) (Sj) .

(18)

If θφ is marginally consistent for S on all ij
marginally consistent for S.

∈

E, we call θφ

Note that marginal consistency is necessary, but not sufﬁ-
cient for optimality of the relaxation (1). This can be seen
in the case of CRFs (Example 1), where it exactly corre-
sponds to arc-consistency. The latter is only necessary, but
not sufﬁcient for optimality [72].

Comparison to Subgradient Method. Decomposi-
tion IRPS-LP and more general ones can be solved via
the subgradient method [48]. Similar to Algorithm 2, it
operates on dual variables φ and manipulates them by
visiting each factor sequentially. Contrary to Algorithm 2,
subgradient algorithms converge to the optimum. Moreover,
on a per-iterations basis, computing subgradients is cheaper
than using Algorithm 2, as only (13) needs to be computed,
while Algorithm 2 needs to solve (15) additionally. How-
ever, for MAP-inference, the study [39] has shown that
subgradient-based algorithms converge much slower than
message passing algorithms like TRWS [46]. In Section 6
we conﬁrm this for the graph matching problem as well.

The reason for this large empirical difference is that one
iteration of the subgradient algorithm only updates those co-
ordinates of dual variables φ that are affected by the current
(i.e. coordi-
minimal labeling x∗i ∈
argminxi
nates k : (A(cid:62)(i,j)x∗i )k = 1), while in Algorithm 2 all coor-
dinates of φ are taken into account. Also message passing
implicitly chooses the stepsize so as to achieve monotonical
convergence in Algorithm 1, while subgradient based algo-
rithms must rely on some stepsize rule that may either make
too large or too small changes to the dual variables φ.

θφ
i , xi

Xi (cid:104)

(cid:105)

∈

6. Experimental Evaluation

Our experiments’ goal is to illustrate applicability of the
proposed technique, they are not an exhaustive evaluation.
The presented algorithms are only basic variants, which can
be further improved and tuned to the considered problems.
Both issues are addressed in the specialized studies [65,
66] which are appended. Still, we show that the presented
basic variants are already able to surpass state-of-the-art
specialized solvers on challenging datasets. All experiments
were run on a computer with a 2.2 GHz i5-5200U CPU and
8 GB RAM.

6.1. Graph Matching

Solvers. We compare against two state-of-the-art algo-
rithms:
(i) the subgradient based dual decomposition
solver [68] abbreviated by DD and (ii) the recent “hungarian
belief propagation” message passing algorithm [76], abbrevi-
ated as HBP. While the authors of [76] have embedded their
solver in a branch-and-bound routine to produce exact solu-
tions, we have reimplemented their message passing com-
ponent but did not use branch and bound to make the com-
parison fair. Both algorithms DD and HBP outperformed
alternative solvers at the time of their publication, hence we
have not tested against [17, 74, 51, 67, 61, 28, 77, 37, 52, 15].
We call our solver AMP.

Theorem 2. If θφ is marginally consistent, the dual lower
bound D(φ) cannot be improved by Algorithm 2.

Datasets. We selected three challenging datasets. The ﬁrst
two are the standard benchmark datasets car and motor,

7

Figure 2. Runtime plots comparing averaged log(primal energy − dual lower bound) values on car, motor and worms graph matching
datasets. Both axes are logarithmic.

Figure 3. Runtime plots comparing averaged primal/dual values on the three knott-3d-{150|300|450} multicut datasets. Values are
averaged over all instances in the dataset. The x-axis are logarithmic. Continuous lines are dual lower bounds while corresponding dashed
lines show primal solutions obtained by rounding.

both used in [53], containing 30 pairs of cars and 20 pairs of
motorbikes with keypoints to be matched 1:1. The images
are taken from the VOC PASCAL 2007 challenge [20]. Costs
are computed from features as in [53]. Instances are densely
connected graphs with 20 – 60 nodes. The third one is the
novel worms datasets [38], containing 30 problem instances
coming from bioimaging. The problems are made of sparsely
connected graphs with up to 600 nodes and up to 1500 labels.
To our knowledge, the worms dataset contains the largest
graph matching instances ever considered in the literature.
For runtime plots showing averaged logarithmic primal/dual
gap over all instances of each dataset see Fig. 2.

Results. Our solver AMP consistently outperforms HBP
and DD w.r.t. primal/dual gap and anytime performance
Most markedly on the largest worms dataset, the subgra-
dient based algorithm DD struggles hard to decrease the
primal/dual gap, while AMP gives reasonable results.

6.2. Multicuts

Solvers. We compare against state-of-the-art multicut al-
gorithms implemented in the OpenGM [39] library, namely
(i) the branch-and-cut based solver MC-ILP [42] utilizing
the ILP solver CPLEX [2], (ii) the heuristic primal “fusion
move” algorithm CC-Fusion [8] with random hierarchical
clustering and random watershed proposal generator, de-
noted by the sufﬁxes -RHC and -RWS and (iii) the heuristic
primal “Cut, Glue & Cut” solver CGC [9]. Those solvers

were shown to outperform other multicut algorithms [8]. Al-
gorithm MC-ILP provides both upper and lower bounds,
while CC-Fusion and CGC are purely primal algorithms.
We call our message passing solver with cycle constraints
added in a cutting plane fashion MP-C.

Datasets. A source of large scale problems comes from
electron microscopy of brain tissue, for which we wish
to obtain neuron segmentation. We have selected three
datasets knott-3d-
of increasingly
{
large size [39], each consisting of 8 instances. Instances
have
5656, 36221, and
107060 edges respectively.

972, 5896 and 17074 nodes and

150|300|450
}

≤

≤

Results. For plots showing dual bounds and primal solu-
tion objectives over time see Figure 3. Our algorithm MP-C
combines advantages of LP-based techniques awith those of
primal heuristics: It delivers high dual lower bounds faster
than MC-ILP. Its has fast primal convergence speed and
delivers primal solutions comparable/superior to CGC’s and
CC-Fusion’s.

7. Acknowledgments

The authors would like to thank Vladimir Kolmogorov
for helpful discussions. This work is partially funded by the
European Research Council under the European Unions Sev-
enth Framework Programme (FP7/2007-2013)/ERC grant

8

agreement no 616160.

References

[1] https://en.wikipedia.org/wiki/ARPANET. 1
[2] IBM ILOG CPLEX Optimizer. http://www-01.ibm.
com/software/integration/optimization/
cplex-optimizer/. 1, 8

[3] A. Alush and J. Goldberger. Break and conquer: Efﬁcient cor-
relation clustering for image segmentation. In E. R. Hancock
and M. Pelillo, editors, SIMBAD, volume 7953 of Lecture
Notes in Computer Science, pages 134–147. Springer, 2013.
3

[4] B. Andres, J. H. Kappes, T. Beier, U. K¨othe, and F. A. Ham-
precht. Probabilistic image segmentation with closedness
constraints. In D. N. Metaxas, L. Quan, A. Sanfeliu, and
L. J. V. Gool, editors, ICCV, pages 2611–2618. IEEE Com-
puter Society, 2011. 3

[5] B. Andres, T. Kr¨oger, K. L. Briggman, W. Denk, N. Korogod,
G. Knott, U. K¨othe, and F. A. Hamprecht. Globally opti-
mal closed-surface segmentation for connectomics. In A. W.
Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, and C. Schmid,
editors, ECCV (3), volume 7574 of Lecture Notes in Com-
puter Science, pages 778–791. Springer, 2012. 3

[6] A. Arasu, C. R, and D. Suciu. Large-scale deduplication with
constraints using dedupalog. In Y. E. Ioannidis, D. L. Lee,
and R. T. Ng, editors, ICDE, pages 952–963. IEEE Computer
Society, 2009. 3

[7] C. Arora and A. Globerson. Higher order matching for con-
sistent multiple target tracking. In Proceedings of the IEEE
International Conference on Computer Vision, pages 177–
184, 2013. 2

[8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves
for correlation clustering. In CVPR, pages 3507–3516. IEEE
Computer Society, 2015. 3, 8

[9] T. Beier, T. Kr¨oger, J. H. Kappes, U. K¨othe, and F. A. Ham-
precht. Cut, glue & cut: A fast, approximate solver for
multicut partitioning. In CVPR. Proceedings, 2014. 3, 8
[10] O. Bilde and J. Krarup. Sharp lower bounds and efﬁcient
algorithms for the simple plant location problem. Annals of
Discrete Mathematics, 1:79–97, 1977. 1

[11] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy
minimization via graph cuts. IEEE Transactions on pattern
analysis and machine intelligence, 23(11):1222–1239, 2001.
1

[12] R. E. Burkard, E. C¸ ela, P. M. Pardalos, and L. S. Pitsoulis. The
Quadratic Assignment Problem, pages 1713–1809. Springer
US, Boston, MA, 1999. 3

[13] Y. Chen, S. Sanghavi, and H. Xu. Clustering sparse graphs.
In P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou,
and K. Q. Weinberger, editors, NIPS, pages 2213–2221, 2012.
3

[14] F. Chierichetti, N. Dalvi, and R. Kumar. Correlation clustering
in mapreduce. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data
Mining, KDD ’14, pages 641–650, New York, NY, USA,
2014. ACM. 3

[15] M. Cho, J. Lee, and K. M. Lee. Reweighted random walks for
graph matching. In K. Daniilidis, P. Maragos, and N. Paragios,
editors, ECCV (5), volume 6315 of Lecture Notes in Computer
Science, pages 492–505. Springer, 2010. 3, 7

[16] S. Chopra and M. R. Rao. The partition problem. Mathemati-

cal Programming, 59(1):87–115, 1993. 3, 4

[17] J. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using combi-
natorial optimization within max-product belief propagation.
In NIPS, pages 369–376. MIT Press, 2006. 2, 3, 7

[18] M. Elsner and E. Charniak. You talking to me? a corpus and
algorithm for conversation disentanglement. In K. McKeown,
J. D. Moore, S. Teufel, J. Allan, and S. Furui, editors, ACL,
pages 834–842. The Association for Computer Linguistics,
2008. 3

[19] M. Elsner and W. Schudy. Bounding and comparing methods
In Proceedings of
for correlation clustering beyond ILP.
the Workshop on Integer Linear Programming for Natural
Langauge Processing, ILP ’09, pages 19–27, Stroudsburg,
PA, USA, 2009. Association for Computational Linguistics.
3

[20] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and
A. Zisserman. The Pascal visual object classes (VOC) chal-
lenge. International Journal of Computer Vision, 88(2):303–
338, June 2010. 8

[21] M. L. Fisher and D. S. Hochbaum. Database location in
computer networks. Journal of the ACM (JACM), 27(4):718–
735, 1980. 1

[22] M. L. Fisher, R. Jaikumar, and L. N. Van Wassenhove. A
multiplier adjustment method for the generalized assignment
problem. Management Science, 32(9):1095–1103, 1986. 1

[23] M. L. Fisher and P. Kedia. Optimal solution of set cover-
ing/partitioning problems using dual heuristics. Management
science, 36(6):674–688, 1990. 1

[24] D. Gamarnik, D. Shah, and Y. Wei. Belief propagation for
min-cost network ﬂow: Convergence & correctness. In SODA,
pages 279–292. SIAM, 2010. 1

[25] M. R. Garey and D. S. Johnson. Computers and Intractability:
A Guide to the Theory of NP-Completeness. W. H. Freeman
& Co., New York, NY, USA, 1979. 4

[26] A. Gionis, H. Mannila, and P. Tsaparas. Clustering aggrega-
tion. ACM Trans. Knowl. Discov. Data, 1(1):4, 2007. 3
[27] A. Globerson and T. S. Jaakkola. Fixing max-product: Con-
vergent message passing algorithms for MAP LP-relaxations.
In NIPS, pages 553–560, 2007. 1, 2, 6, 13, 14

[28] S. Gold and A. Rangarajan. A graduated assignment algo-
rithm for graph matching. IEEE Trans. Pattern Anal. Mach.
Intell., 18(4):377–388, 1996. 3, 7

[29] M. Guignard. A Lagrangean dual ascent algorithm for simple
plant location problems. European Journal of Operational
Research, 35(2):193–200, 1988. 1

[30] M. Guignard and S. Kim. Lagrangean decomposition: A
model yielding stronger Lagrangean bounds. Mathematical
programming, 39(2):215–228, 1987. 1

[31] M. Guignard and S. Kim. Lagrangean decomposition for in-
teger programming: theory and applications. Revue franc¸aise
d’automatique, d’informatique et de recherche op´erationnelle.
Recherche op´erationnelle, 21(4):307–323, 1987. 1

9

[32] M. Guignard and M. B. Rosenwein. An application-oriented
guide for designing Lagrangean dual ascent algorithms. Euro-
pean Journal of Operational Research, 43(2):197–205, 1989.
1

[48] N. Komodakis, N. Paragios, and G. Tziritas. MRF energy
IEEE
minimization and beyond via dual decomposition.
Transactions on Pattern Analysis and Machine Intelligence,
33(3):531–552, March 2011. 1, 7

[33] M. Guignard and M. B. Rosenwein. Technical note-an im-
proved dual based algorithm for the generalized assignment
problem. Operations Research, 37(4):658–663, 1989. 1
[34] M. Guignard and M. B. Rosenwein. An application of La-
grangean decomposition to the resource-constrained mini-
mum weighted arborescence problem. Networks, 20(3):345–
359, 1990. 1

[35] Gurobi Optimization, Inc., 2015. http://www.gurobi.

com. 1

[36] J. Jancsary and G. Matz. Convergent decomposition solvers
for tree-reweighted free energies. In AISTATS 2011, 2011. 2
[37] B. Jiang, J. Tang, C. Ding, and B. Luo. A local sparse model
for matching problem. In Proceedings of the Twenty-Ninth
AAAI Conference on Artiﬁcial Intelligence, AAAI’15, pages
3790–3796. AAAI Press, 2015. 3, 7

[38] D. Kainmueller, F. Jug, C. Rother, and G. Myers. Active graph
matching for automatic joint segmentation and annotation of
C. elegans. In International Conference on Medical Image
Computing and Computer-Assisted Intervention, pages 81–88.
Springer, 2014. 8

[39] J. H. Kappes, B. Andres, F. A. Hamprecht, C. Schn¨orr,
S. Nowozin, D. Batra, S. Kim, B. X. Kausler, T. Kr¨oger,
J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother.
A comparative study of modern inference techniques for struc-
tured discrete energy minimization problems. International
Journal of Computer Vision, 115(2):155–184, 2015. 1, 2, 7, 8
[40] J. H. Kappes, B. Savchynskyy, and C. Schn¨orr. A bundle ap-
proach to efﬁcient MAP-inference by Lagrangian relaxation.
In Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on, pages 1688–1695. IEEE, 2012. 1
[41] J. H. Kappes, M. Speth, B. Andres, G. Reinelt, and C. Schn¨orr.
Globally optimal image partitioning by multicuts. In EMM-
CVPR. Springer, Springer, 2011. 3, 4

[42] J. H. Kappes, M. Speth, G. Reinelt, and C. Schn¨orr. Higher-
order segmentation via multicuts. CoRR, abs/1305.6387,
2013. 3, 4, 8

[43] B. Kernighan and S. Lin. An efﬁcient heuristic procedure
for partitioning graphs. The Bell Systems Technical Journal,
49(2), 1970. 7

[44] M. Keuper, E. Levinkov, N. Bonneel, G. Lavou´e, T. Brox,
and B. Andres. Efﬁcient decomposition of image and mesh
graphs by lifted multicuts. In ICCV, 2015. 7

[45] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order
correlation clustering for image segmentation. In J. Shawe-
Taylor, R. S. Zemel, P. L. Bartlett, F. C. N. Pereira, and K. Q.
Weinberger, editors, NIPS, pages 1530–1538, 2011. 3
[46] V. Kolmogorov. Convergent tree-reweighted message passing
for energy minimization. IEEE Trans. Pattern Anal. Mach.
Intell., 28(10):1568–1583, 2006. 1, 2, 5, 6, 7, 13, 14

[49] C. Lemar´echal. Lagrangian decomposition and nonsmooth
optimization: Bundle algorithm, prox iteration, augmented
Lagrangian. Nonsmooth Optimization: Methods and Applica-
tions, pages 201–216, 1992. 1

[50] V. Lempitsky, C. Rother, S. Roth, and A. Blake. Fusion moves
for Markov random ﬁeld optimization. IEEE transactions on
pattern analysis and machine intelligence, 32(8):1392–1405,
2010. 1

[51] M. Leordeanu and M. Hebert. A spectral technique for cor-
respondence problems using pairwise constraints. In ICCV,
pages 1482–1489. IEEE Computer Society, 2005. 3, 7
[52] M. Leordeanu, M. Hebert, and R. Sukthankar. An integer
projected ﬁxed point method for graph matching and MAP
inference. In Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I.
Williams, and A. Culotta, editors, NIPS, pages 1114–1122.
Curran Associates, Inc., 2009. 3, 7

[53] M. Leordeanu, R. Sukthankar, and M. Hebert. Unsupervised
learning for graph matching. International Journal of Com-
puter Vision, 96(1):28–45, 2012. 8

[54] T. Meltzer, A. Globerson, and Y. Weiss. Convergent message
passing algorithms - a unifying view. In UAI, pages 393–401.
AUAI Press, 2009. 2

[55] I. Necoara and J. A. Suykens. Application of a smoothing
technique to decomposition in convex optimization. IEEE
Transactions on Automatic Control, 53(11):2674–2679, 2008.
1

[56] V. Ng and C. Cardie. Improving machine learning approaches
to coreference resolution. Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics - ACL
’02, (July):104, 2001. 3

[57] C. Ribeiro and M. Minoux. Solving hard constrained short-
est path problems by Lagrangean relaxation and branch-and-
bound algorithms. Methods of Operations Research, 53:303–
316, 1986. 1

[58] E. Sadikov, J. Madhavan, L. Wang, and A. Halevy. Cluster-
ing query reﬁnements by user intent. In World Wide Web
Conference (WWW). ACM Press, April 2010. 3

[59] B. Savchynskyy, J. Kappes, S. Schmidt, and C. Schn¨orr. A
study of Nesterov’s scheme for Lagrangian decomposition
and MAP labeling. In Computer Vision and Pattern Recogni-
tion (CVPR), 2011 IEEE Conference on, pages 1817–1823.
IEEE, 2011. 1

[60] B. Savchynskyy, S. Schmidt, J. H. Kappes, and C. Schn¨orr.
Efﬁcient MRF energy minimization via adaptive diminishing
smoothing. In UAI, pages 746–755. AUAI Press, 2012. 2
[61] C. Schellewald and C. Schn¨orr. Probabilistic subgraph match-
In EMMCVPR, volume
ing based on convex relaxation.
3757 of Lecture Notes in Computer Science, pages 171–186.
Springer, 2005. 3, 7

[47] V. Kolmogorov. A new look at reweighted message passing.
IEEE Trans. Pattern Anal. Mach. Intell., 37(5):919–930, 2015.
1, 2, 5, 6, 13, 14

[62] M. I. Schlesinger and K. V. Antoniuk. Diffusion algorithms
and structural recognition optimization problems. Cybernetics
and Systems Analysis, 47(2):175–192, 2011. 6

10

[63] D. Sontag and T. S. Jaakkola. Tree block coordinate descent
for map in graphical models. In AISTATS, volume 5 of JMLR
Proceedings, pages 544–551, 2009. 14

[64] W. M. Soon, H. T. Ng, and D. C. Y. Lim. A machine learning
approach to coreference resolution of noun phrases. Compu-
tational Linguistics, 27(4):521–544, 2001. 3

[65] C. Swoboda, Paul Rother, A. A. Hassan, and Kainm˙Study of
Lagrangean decomposition and dual ascent solvers for graph
matching. 7

[66] P. Swoboda and B. Andres. A message passing algorithm for

the minimum cost multicut problem. 4, 7

[67] P. H. S. Torr. Solving Markov random ﬁelds using semi

deﬁnite programming. In In: AISTATS, 2003. 3, 7

[68] L. Torresani, V. Kolmogorov, and C. Rother. A dual decom-
position approach to feature correspondence. IEEE Trans.
Pattern Anal. Mach. Intell., 35(2):259–271, 2013. 3, 7
[69] M. J. Wainwright and M. I. Jordan. Graphical models, expo-
nential families, and variational inference. Foundations and
Trends in Machine Learning, 1(1-2):1–305, 2008. 3

[70] H. Wang and D. Koller. Subproblem-tree calibration: A
uniﬁed approach to max-product message passing. In 30th
International Conference on Machine Learning (ICML-13),
pages 190–198, 2013. 2, 14

[71] Y. Weiss and W. T. Freeman. On the optimality of solutions
of the max-product belief-propagation algorithm in arbitrary
graphs. IEEE Transactions on Information Theory, 47(2):736–
744, 2001. 2

[72] T. Werner. A linear programming approach to max-sum prob-
lem: A review. IEEE Trans. Pattern Analysis and Machine
Intelligence, 29(7):1165–1179, 2007. 2, 3, 7, 13, 14

[73] T. Werner. Revisiting the linear programming relaxation
approach to Gibbs energy minimization and weighted con-
straint satisfaction. IEEE Trans. Pattern Anal. Mach. Intell.,
32(8):1474–1488, 2010. 2

[74] J. Yarkony, C. C. Fowlkes, and A. T. Ihler. Covering trees
and lower-bounds on quadratic assignment. In CVPR, pages
887–894. IEEE Computer Society, 2010. 3, 7

[75] J. Yarkony, A. Ihler, and C. C. Fowlkes. Fast Planar Cor-
relation Clustering for Image Segmentation, pages 568–581.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. 3
[76] Z. Zhang, Q. Shi, J. McAuley, W. Wei, Y. Zhang, and
A. van den Hengel. Pairwise matching through max-weight
bipartite belief propagation. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2016. 2,
3, 7

[77] F. Zhou and F. D. la Torre. Factorized graph matching. In
CVPR, pages 127–134. IEEE Computer Society, 2012. 3, 7

11

8. Supplementary Material

Proof of Proposition 1

Proposition. (cid:80)
θφ
i , µi
θi, µi
F
i
(cid:104)
(cid:104)
µ1, . . . , µk obey the coupling constraints.

= (cid:80)

i
∈

(cid:105)

∈

F

, whenever
(cid:105)

Proof.
(cid:80)
i

F

∈

θ, µi
(cid:104)

(cid:105)

+

(cid:88)
E(cid:104)

ij
(cid:124)

∈

φ(i,j), A(i,j)µi

+

(cid:105)
(cid:123)(cid:122)
)
(
∗
) = 0 due to φ(i,j) =

(cid:80)
i

θφ, µi
F
(cid:104)
∈
φ(j,i), A(j,i)µj
(cid:104)

=

=

(cid:105)

(cid:105)
(cid:125)

φj,i) and

−

(cid:80)
θ, µi
i
A(i,j)µi = A(j,i)µj.

(cid:104)

(cid:105)

∈

F

,

where (
∗

Proof of Proposition 2

Proposition. Let conv(Xi) =
n
0, 1
×
{
}
n1, . . . , n
. . .+n
J
|

∈
m. Let the messages in problem (15) have size
. Then (15) is a linear program with O(n + n1 +
) constraints.

J
|
) variables and O(m+n1 +. . .+n

µi : Aiµi

with A

≤

bi

{

}

|

|

Proof. From LP-duality we
argminµi:Aµi
Aiµ∗i , y
bi
(cid:104)

c, µi

y
∃

≤
(cid:105)

iff

−

bi (cid:104)
≥
= 0. Hence, (15) can be rewritten as

(cid:105)

|

J
|
know that µ∗i
∈
0 : A(cid:62)i y = ci and

y

≥

0,∆(i,j1 ),...,∆(i,jl )

max

s.t.

δ, θφ+∆

(cid:104)

(cid:105)
Aiµ∗i , y
bi
(cid:104)
(cid:105)
A(cid:62)i y = θφ+∆

−

(cid:40)

= 0

∆(i,j)(s)

0,
0,
where νi := A(i,j)µ∗i

νi(s) = 0
νi(s) = 1

≤
≥

optimal for θφ and for θφ+∆ by construction. We need to
prove

(cid:88)
(cid:104)

J

j

∈

θφ
j , x∗j (cid:105)

+

θφ
i , x∗i (cid:105)
(cid:104)
(cid:88)

θφ
i +

≤ (cid:104)

j

J

∈

A(cid:62)(i,j)∆∗(i,j), x∗i (cid:105)

+

θφ
j −
(cid:104)

A(cid:62)(j,i)∆∗(i,j), x∗∗j (cid:105)

.

(cid:88)

j

J

∈

(21)

We shufﬂe all terms with variables ∆∗(i,j), j
side and all other terms to the left side.

∈

J to the right

θφ
i , x∗i −
(cid:104)
(cid:88)

≤ (cid:104)

j

∈

+

x∗i (cid:105)

(cid:88)

j

J

∈

x∗∗j (cid:105)

θφ
j , x∗j −
(cid:104)
(cid:88)
(cid:104)

j

J

∈

A(cid:62)(i,j)∆∗(i,j), x∗i (cid:105) −

A(cid:62)(j,i)∆∗(i,j), x∗∗j (cid:105)

(22)

All terms on the left side are smaller than zero due to the
choice of x∗j being minimizers w.r.t. θφ
j . Hence, it will be
enough to prove the above inequality when assuming the left
side to be zero. We rewrite the scalar products by transposing
A(cid:62)(i,j) and A(cid:62)(j,i).

(cid:88)

0

≤

j

J

∈

(cid:110)
∆∗(i,j), A(i,j)x∗i −
(cid:104)

Due to A(j,i)x∗∗j
∈ {
dim(φ(i,j)) by Deﬁnition 1 and ∆∗(i,j)
0, 1
{
}
A(i,j)x∗i

≶ 0, the result follows.

0, 1
}

(23)

(cid:111)
A(j,i)x∗∗j (cid:105)
dim(φ(i,j)) and A(i,j)x∗i ∈
≶ 0 whenever

Lemma. Let ∆

AD(θφ

i , x∗i , J) then D(φ)

D(φ + ∆).

∈

≤

Proof. Analoguous to the proof of Lemma 1.

(19)
θφ+∆ is a linear expression and µ∗ is constant during the
computation, hence (19) is a LP.

Proof of Theorem 1

Proof of Lemma 1 and Lemma 2

E be a pair of factors related by the
Lemma. Let ij
coupling constraints and φ(i,j) be a corresponding dual
θφ
vector. Let x∗i ∈
i , xi

and ∆(i,j) satisfy

argmin
Xi (cid:104)
xi

∈

(cid:105)

∈

ν(s) = 1
ν(s) = 0

(cid:40)

0,
0,

≥
≤

∆(i,j)(s)

, where ν := A(i,j)x∗i .

(20)

Then x∗i ∈

argmin
Xi (cid:104)
xi

θφ+∆
i

, xi

(cid:105)

∈

implies D(φ)

D(φ + ∆).

≤

θφ
j , xj

argminxj

Proof. Let x∗j ∈
be a solution of (13)
Xj (cid:104)
at which the dual lower bound (11) is attained before the
be
update and x∗∗j ∈
∈
an integral solution at which the dual lower bound is attained
after φ has been updated. Variable x∗i as chosen in (13) is

A(cid:62)(j,i)∆∗(i,j), xj

argminxj

Xj (cid:104)

θφ

−

(cid:105)

(cid:105)

∈

Theorem. Algorithm 2 monotonically increasis the dual
lower bound (11).

Proof. We prove that (i) the receiving messages and (ii) the
sending messages step improve (11).
(i) Directly apply Lemma 1. (ii) The difﬁculty here is that we
compute descent directions from the current dual variables
φ in parallel and then apply all of them simultaneously. By
Lemma 2, the send message step is non-decreasing when
called for each set J1, . . . , Jl in Algorithm 2. The dual lower
bound L(φ) is concave, hence we apply Jensen’s inequality
and note that ω1 + . . . + ωl

1 to obtain the result.

≤

Proof of Theorem 2

Theorem. If θφ is marginally consistent, the dual lower
bound D(φ) cannot be improved by Algorithm 2.

First, we need two technical lemmata.

12

Lemma 3. Let X
⊂ {
K
and Ax
x
0, 1
∀
}
∈ {
be given and deﬁne ν∗

be given such that ∆(s)

∈

n, A
0, 1
}
X.
:= Ax∗.
(cid:40)
0,
0,

∈ {
Let x∗
Let ∆
ν∗(s) = 1
ν∗(s) = 0

.

n

×
X
RK

K
0, 1
}
∈
∈

Then

(i) x∗
∈
argminx
X (cid:104)−
∈
whenever ν∗(s)

argminx
X (cid:104)−
∈
∈
, ν∗∗ = Ax∗∗ it holds that ∆(s) = 0
∆, Ax
(cid:105)
= ν∗∗(s).

and (ii) for x∗∗

≥
≤
∆, Ax
(cid:105)

Proof. Let x

X and deﬁne ν = Ax. Then

∈

∆, Ax
(cid:105)

(cid:104)−

=

(cid:88)

∆(s)

+

(cid:88)

∆(s)

−

s:ν(s)=1>0=ν∗(s)
(cid:124)

(cid:125)

(cid:123)(cid:122)
)
(
∗∗

−
s:ν∗(s)=1=ν(s)
(cid:124)

(cid:125)

(cid:88)

(cid:123)(cid:122)
)
(
∗

≥

s:ν∗(s)=1
(cid:124)
(cid:123)(cid:122)
∗∗∗

(

)

∆(s)

−

(cid:125)

=

∆, Ax∗

(24)

(cid:104)−

(cid:105)

)

) due to ∆(s)

(
∗ ∗ ∗
≥
0 due to ∆(s)

because (
∗
(
)
∗∗
(ii) is proven by observing that (
must also hold.

0 for ν∗(s) = 1 and
0 for ν∗(s) = 0. This proves (i) and
) = 0 and (
)
) = (
∗

∗ ∗ ∗

∗∗

≥

≥

≤

be two solu-
Lemma 4. Let x∗i , x∗∗i ∈
argminxi
tions to the i-th factor for the current reparametrization θφ.
If ∆ is admissible w.r.t. x∗i then ∆ is also admissible w.r.t.
x∗∗i

Xi(cid:104)
∈

(cid:105)

.

θφ, xi

x∗i

are

and

x∗∗i

both

optimal

is also optimal

Proof. As
θφ and x∗i
∆(i,j), A(j,i)x∗i (cid:105) ≤ (cid:104)
(cid:104)
(i) also
(cid:104)−
hence
holds,
argminxi
x∗∗i ∈
Xi(cid:104)
∈
plies that ∆(s) = 0 whenever ν∗(s)

to
to θφ+∆, we have
. By Lemma 3,
∆(i,j), A(j,i)x∗∗i (cid:105)
shows
. Second, Lemma 3, (ii) im-
(cid:105)
= ν∗∗(s). This proves

∆(i,j), A(j,i)x∗∗i (cid:105)
∆(i,j), A(j,i)x∗i (cid:105) ≤ (cid:104)−
equality must hold.

θφ+∆, xi

This

that ∆(i,j)(s)

, ν∗∗ := A(i,j)x∗∗i

.

(cid:40)

0,
0,

≥
≤

ν∗∗(s) = 1
ν∗∗(s) = 0

to show that

Proof of Theorem 2. It
for
is sufﬁcient
marginally consistent θφ for S, the update ∆ computed by
Algorithm 1 on an arbitrary factor i

F and some set J

⊂
G(i) has the following properties: (i) L(φ) = L(φ + ∆),
N
(ii) θφ+∆ is marginally consistent for S. For an easier proof,
. The general case can be
we only consider the case J =
}
proven analoguously.
(i) Let x∗i ∈
have to show that

Sj with A(i,j)x∗i = A(j,i)x∗j . We

Si, x∗j ∈

∈

{

j

θφ
i , xi

min
xi
∈

Xi(cid:104)

+ min
xj
∈

Xj(cid:104)

(cid:105)

θφ
j , xj

= min
xi

Xi(cid:104)

(cid:105)

∈

θφ+∆
i

, xi

i

∈

∈

∈

(cid:105)

.
(cid:105)

Xj (cid:104)

θφ, xj

As x∗j

θφ+∆, xj

argminxj

∆(i,j), A(j,i)xj

∈
Xj (cid:104)
Xj (cid:104)−

, since by Lemma 4 the
it remains to show

.
(cid:105)
∈
, it is sufﬁcient to prove that x∗j ∈
This follows from
θφ
i , x∗i (cid:105)
(cid:104)

Due to x∗i optimal to θφ+∆
update ∆ is admissible for x∗i ,
that x∗j
argminxj
argminxj
Lemma 3 (i). We conclude by noting
θφ+∆
θφ+∆
.
j
i
(cid:104)
(cid:105)
(cid:104)
computations
(ii)
Si
argminxi
⊆
θφ+∆
, xj
argminxj
j
all other factors stay the same: θφ+∆
.
}
Hence, θφ+∆ is marginally consistent for S after the
update.

θφ+∆
and
i
⊆
The reparametrizations of
k for k

show that

θφ
j xj
(cid:104)

+
The

in
, xi

, x∗i (cid:105)

= θφ

Xj (cid:104)

Xi(cid:104)

∈
.
(cid:105)

i, j

Sj

\{

(i)

xj

+

=

∈

F

(cid:105)

(cid:105)

∈

k

9. Special Cases: Graphical Model Solvers

We will show how Algorithm 2 subsumes known
message-passing algorithms MSD [72], TRWS [46],
SRMP [47] and MPLP [27] for MAP-inference with com-
mon graphical models, considered in Example 1.

all

seen,

Solver Primitives
be
Xi =
and conv(Xi) =
µ
{
dimensional simplex.

(13)

factors

in

and
(5)

(15). As
are

of

it
the

can
form

(1, 0, . . . , 0), (0, 1, 0, . . . , 0), . . . , (0, . . . , 0, 1)
{

}

0 :

1, µ
(cid:105)
(cid:104)

= 1
}

≥

is a dim( Xi)-

are

In all message passing algorithms [46, 47, 72, 27],
of Algo-
of
accom-
solutions
(15):

of
problem (13)

invokations

the
and

types

two

together with
optimization
Factor

Reparametrization
adjustment (15)

Optimization

(13)
{θφ

u(xu)}

min
xu∈Xu

there
rithm 1
panying
Alg. 1
input

i = u ∈ V
J = {uv}
uv ∈ E

i = uv ∈ E
J = {u}
u ∈ V

min{θφ
(xu, xv) ∈ Xu × Xv

u(xu, xv)}

∆∗

(u,uv)(xu) =
u(x(cid:48)
θφ

min
u∈Xu

u)

x(cid:48)

− θφ

u(xu)

∆∗

(uv,u)(xu) =
uv(x(cid:48)
θφ

uv)

min
uv ∈Xuv

x(cid:48)

− min
xv ∈Xv

{θuv(xu, xv)}

θφ+∆
j

+ min
(cid:105)
Xj(cid:104)
xj
∈
(25)

MAP-inference Solvers.
In Table 1 we state solvers
MSD [72], TRWS [46], SRMP [47] and MPLP [27] as spe-
, xj
cial cases of our framework. Factors are visited in the order
they are read in.

(cid:105)

13

Algorithm

MSD [72]

MPLP [27]

TRWS [46]
SRMP [47]

Current
factor
V
u
E
uv
V
u
E
uv

∈
∈
∈
∈

V

u

∈

u
uv

V
E

∈
∈

Jreceive

N

G(u)
∅
∅
u, v

}

{

J1 ˙
∪
uv

{

. . . ˙
∪

Jl

(u)

} ⊂ NG
—
—
,

v

u
}
}
{
forward pass:

{

ω

ω1, . . . = 1/

G(u)
|

|N

—
—
ω1 = 1/2 = ω2

uv : v

(u), v < u

uv

: v

(u), v > u ω1, . . . = 1/max(
{

v

∈N

G(u):v>u

v

,
{

}

∈N

G(u):v<u

)

{

{

∈ NG

∈ NG
∅

}

}

{

{

}

}

∈ NG
backward pass:

∈ NG
—

uv : v

(u), v > u

uv

: v

(u), v < u ω1, . . . = 1/max(
{

v

∈N

G(u):v>u
—

v

,
{

}

∈N

G(u):v<u

)

}

}

Table 1. [72, 46, 47, 27] as special cases of Algorithm 2.

Remark 1. We have only treated the case of unary θu, u
V
E here. MPLP [27] and
and pairwise potentials θuv, uv
SRMP [47] can be applied to higher order potentials as well,
which we do not treat here.SRMP [47] is a generalisation of
TRWS [46] to the higher-order case.

∈

∈

Remark 2. There are convergent message-passing algo-
rithms such that factors comprise trees [70, 63]. Their anal-
ysis is more difﬁcult, hence we omit it here.

Note that our framework generalizes upon [46, 47, 27,
72, 63, 70] in several ways: (i) Our factors need not be
simplices or trees. (ii) Our messages need not be marginal-
ization between unary/pairwise/triplet/. . . factors. (iii) We
can compute message updates on more than one coupling
constraint simultaneously, i.e. we may choose J1 ˙
Jl
∪
in Algorithm 2 to be different than singleton sets. (i) and (ii)
affect LP-modeling, (iii) affects computational efﬁciency:
By considering multiple messages at once in Procedure 1,
we may be able to make larger updates ∆∗, resulting in faster
convergence.

. . . ˙
∪

14

