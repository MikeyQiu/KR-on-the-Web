Embedding Multimodal Relational Data for Knowledge Base Completion

Pouya Pezeshkpour
University of California
Irvine, CA
pezeshkp@uci.edu

Liyan Chen
University of California
Irvine, CA
liyanc@uci.edu

Sameer Singh
University of California
Irvine, CA
sameer@uci.edu

8
1
0
2
 
p
e
S
 
7
 
 
]
I

A
.
s
c
[
 
 
2
v
1
4
3
1
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

images, and numerical values.

Representing entities and relations in an em-
bedding space is a well-studied approach for
machine learning on relational data. Exist-
ing approaches, however, primarily focus on
simple link structure between a ﬁnite set of
entities,
ignoring the variety of data types
that are often used in knowledge bases, such
In
as text,
this paper, we propose multimodal knowledge
base embeddings (MKBE) that use different
neural encoders for this variety of observed
data, and combine them with existing rela-
tional models to learn embeddings of the en-
tities and multimodal data.
Further, using
these learned embedings and different neural
decoders, we introduce a novel multimodal
imputation model to generate missing multi-
modal values, like text and images, from in-
formation in the knowledge base. We enrich
existing relational datasets to create two novel
benchmarks that contain additional informa-
tion such as textual descriptions and images
of the original entities. We demonstrate that
our models utilize this additional information
effectively to provide more accurate link pre-
diction, achieving state-of-the-art results with
a considerable gap of 5-7% over existing meth-
ods. Further, we evaluate the quality of our
generated multimodal values via a user study.
We have release the datasets and the open-
source implementation of our models at https:
//github.com/pouyapez/mkbe.

1

Introduction

Knowledge bases (KB) are an essential part of
many computational systems with applications in
search, structured data management, recommen-
dations, question answering, and information re-
trieval. However, KBs often suffer from incom-
pleteness, noise in their entries, and inefﬁcient in-
ference under uncertainty. To address these issues,
learning relational knowledge representations has

been a focus of active research (Bordes et al., 2011,
2013; Yang et al., 2015; Nickel et al., 2016; Trouil-
lon et al., 2016; Dettmers et al., 2018). These ap-
proaches represent relational triples, that consist
of a subject entity, relation, and an object entity,
by learning ﬁxed, low-dimensional representations
for each entity and relation from observations, en-
coding the uncertainty and inferring missing facts
accurately and efﬁciently. The subject and the ob-
ject entities come from a ﬁxed, enumerable set of
entities that appear in the knowledge base.

Knowledge bases in the real world, however,
contain a wide variety of data types beyond these
direct links. Apart from relations to a ﬁxed set
of entities, KBs often not only include numeri-
cal attributes (such as ages, dates, ﬁnancial, and
geoinformation), but also textual attributes (such
as names, descriptions, and titles/designations) and
images (proﬁle photos, ﬂags, posters, etc.). These
different types of data can play a crucial role as
extra pieces of evidence for knowledge base com-
pletion. For example the textual descriptions and
images might provide evidence for a person’s age,
profession, and designation. In the multimodal KB
shown in Figure 1 for example, the image can be
helpful in predicting of Carles Puyol’s occupation,
while the description contains his nationality. Incor-
porating this information into existing approaches
as entities, unfortunately, is challenging as they as-
sign each entity a distinct vector and predict miss-
ing links (or attributes) by enumerating over the
possible values, both of which are only possible
if the entities come from a small, enumerable set.
There is thus a crucial need for relational modeling
that goes beyond just the link-based view of KB
completion, by not only utilizing multimodal infor-
mation for better link prediction between existing
entities, but also being able to generate missing
multimodal values.

In this paper, we introduce multimodal knowl-

existing YAGO-10 and MovieLens-100k datasets
to include additional relations such as textual de-
scriptions, numerical attributes, and images of the
entities. We demonstrate that MKBE utilizes the
additional information effectively to provide gains
in link-prediction accuracy, achieving state-of-the-
art results on these datasets for both the DistMult
and the ConvE scoring functions. We evaluate the
quality of multimodal attributes generated by the
decoders via user studies that demonstrate their re-
alism and information content, along with present-
ing examples of such generated text and images.

2 Multimodal KB Completion

As described earlier, KBs often contain differ-
ent types of information about entities including
links, textual descriptions, categorical attributes,
numerical values, and images. In this section, we
brieﬂy introduce existing relational embedding ap-
proaches that focus on modeling the linked data
using distinct, dense vectors. We then describe
MKBE that extends these approaches to the multi-
modal setting, i.e., modeling the KB using all the
different information to predict the missing links
and impute the missing attributes.

2.1 Background on Link Prediction

Factual statements in a knowledge base are repre-
sented using a triple of subject, relation, and ob-
ject, (cid:104)s, r, o(cid:105), where s, o ∈ ξ, a set of entities, and
r ∈ R, a set of relations. Respectively, we con-
sider two goals for relational modeling, (1) to train
a machine learning model that can score the truth
value of any factual statement, and (2) to predict
missing links between the entities. In existing ap-
proaches, a scoring function ψ : ξ × R × ξ → R
(or sometimes, [0, 1]) is learned to evaluate whether
any given fact is true, as per the model. For pre-
dicting links between the entities, since the set ξ
is small enough to be enumerated, missing links
of the form (cid:104)s, r, ?(cid:105) are identiﬁed by enumerating
all the objects and scoring the triples using ψ (i.e.
assume the resulting entity comes from a known
set). For example, in Figure 1, the goal is to predict
that Carles Puyol plays for Barcelona.

Many of the recent advances in link prediction
use an embedding-based approach; each entity in ξ
and relation in R are assigned distinct, dense vec-
tors, which are then used by ψ to compute the score.
In DistMult (Yang et al., 2015), for example, each
entity i is mapped to a d-dimensional dense vector

Figure 1: Example of a Multimodal KB. Graph
representation of (a part of) a KB that consists of
regular links (in black) and multimodal ones (in
purple) that we support in this work.

edge base embeddings (MKBE) for modeling
knowledge bases that contain a variety of data
types, such as links, text, images, numerical, and
categorical values. We propose neural encoders and
decoders to replace initial layers of any embedding-
based relational model; we apply them to Dist-
Mult (Yang et al., 2015) and ConvE (Dettmers et al.,
2018) here. Speciﬁcally, instead of learning a dis-
tinct vector for each entity and using enumeration
to predict links, MKBE includes the following ex-
tensions: (1) introduce additional neural encoders
to embed multimodal evidence types that the rela-
tional model uses to predict links, and (2) introduce
neural decoders that use an entity’s embedding to
generate its multimodal attributes (like image and
text). For example, when the object of a triple is an
image, we encode it into a ﬁxed-length vector us-
ing a CNN, while textual objects are encoded using
RNN-based sequence encoders. The scoring mod-
ule remains identical to the underlying relational
model; given the vector representations of the sub-
ject, relation, and object of a triple, we produce a
score indicating the probability that the triple is cor-
rect using DistMult or ConvE. After learning the
KB representation, neural decoders use entity em-
beddings to generate missing multimodal attributes,
for example, generating the description of a person
from their structured information in the KB. This
uniﬁed framework allows for ﬂow of the informa-
tion across the different relation types (multimodal
or otherwise), providing a more accurate modeling
of relational data.

We provide an evaluation of our proposed ap-
proach on two relational KBs. Since we are intro-
ducing the multimodal KB completion setting, we
provide two benchmarks, created by extending the

(ei ∈ Rd) and each relation r to a diagonal matrix
Rr ∈ Rd×d, and consequently, the score for any
triple (cid:104)s, r, o(cid:105) is computed as ψ(s, r, o) = eT
s Rreo.
Along similar lines, ConvE (Dettmers et al., 2018)
uses vectors to represent the entities and the re-
lations, es, eo, rr ∈ Rd×1, then, after applying a
CNN layer on es and rr, combines it with eo to
score a triplet, i.e. the scoring function ψ(s, r, o) is
f (vec(f ([ ¯es; ¯rr ∗ w]))W )eo. Other relational em-
bedding approaches primarily vary in their design
of the scoring function (Bordes et al., 2013; Yang
et al., 2015; Nickel et al., 2016; Trouillon et al.,
2016), but share the shortcoming of assigning dis-
tinct vectors to every entity, and assuming that the
possible object entities can be enumerated. In this
work we focus on DistMult because of its sim-
plicity, popularity, and high accuracy, and ConvE
because of its state-of-the-art results.

2.2 Problem Setup

When faced with additional triples in form of mul-
timodal data, the setup of link prediction is slightly
different. Consider a set of all potential multimodal
objects, M, i.e. possible images, text, numerical,
and categorical values, and multimodal evidence
triples, (cid:104)s, r, o(cid:105), where s ∈ ξ, r ∈ R, and o ∈ M.
Our goals with incorporating multimodal informa-
tion into KB remain the same: we want to be able
to score the truth of any triple (cid:104)s, r, o(cid:105), where o is
from ξ (link data) or from M (multimodal data),
and to be able to predict missing value (cid:104)s, r, ?(cid:105) that
may be from ξ or M (depending on r). For the
example in Figure 1, in addition to predicting that
Carles Puyol plays for Barcelona from multimodal
evidence, we are also interested in generating an
image for Carles Puyol, if it is missing.

Existing approaches to this problem assume that
the subjects and the objects are from a ﬁxed set of
entities ξ, and thus are treated as indices into that
set, which fails for the multimodal setting primarily
for two reasons. First, learning distinct vectors for
each object entity does not apply to multimodal
values as they will ignore the actual content of the
multimodal attribute. For example, there will be
no way to generalize vectors learned during train-
ing to unseen values that might appear in the test;
this is not a problem for the standard setup due to
the assumption that all entities have been observed
during training. Second, in order to predict a miss-
ing multimodal value, (cid:104)s, r, ?(cid:105), enumeration is not
possible as the search space is potentially inﬁnite

(or at least intractable to search).

2.3 Multimodal KB Embeddings (MKBE)

To incorporate such multimodal objects into the
existing relational models like DistMult and ConvE,
we propose to learn embeddings for these types of
data as well. We utilize recent advances in deep
learning to construct encoders for these objects to
represent them, essentially providing an embedding
eo for any object value.

The overall goal remains the same: the model
needs to utilize all the observed subjects, objects,
and relations, across different data types, in order
to estimate whether any fact (cid:104)s, r, o(cid:105) holds. We
present an example of an instantiation of MKBE
for a knowledge base containing YAGO entities in
Figure 2a. For any triple (cid:104)s, r, o(cid:105), we embed the
subject (Carles Puyol) and the relation (such as
playsFor, wasBornOn, or playsFor) using a direct
lookup. For the object, depending on the domain
(indexed, string, numerical, or image, respectively),
we use approrpiate encoders to compute its embed-
ding eo. As in DistMult and ConvE, these embed-
dings are used to compute the score of the triple.

Via these neural encoders, the model can use
the information content of multimodal objects to
predict missing links where the objects are from
ξ, however, learning embeddings for objects in M
is not sufﬁcient to generate missing multimodal
values, i.e. (cid:104)s, r, ?(cid:105) where the object is in M. Con-
sequently, we introduce a set of neural decoders
D : ξ × R → M that use entity embeddings to
generate multimodal values. An outline of our
model for imputing missing values is depicted in
Figure 2b. We will describe these decoders in Sec-
tion 2.5.

2.4 Encoding Multimodal Data

Here we describe the encoders we use for mul-
timodal objects. A simple example of MKBE is
provided in Figure 2a. As it shows, we use different
encoder to embed each speciﬁc data type.

Structured Knowledge Consider a triplet of in-
formation in the form of (cid:104)s, r, o(cid:105). To represent
the subject entity s and the relation r as indepen-
dent embedding vectors (as in previous work), we
pass their one-hot encoding through a dense layer.
Furthermore, for the case that the object entity is
categorical, we embed it through a dense layer with
a recently introduced selu activation (Klambauer
et al., 2017), with the same number of nodes as the

Figure 2: Multimodal KB Embeddings (MKBE): (a) Proposed architecture that, given any entity and
its relations, uses domain-speciﬁc encoders to embed each object. The embeddings of entities, and the
relation are then used to score the truth value of the triple by the Scorer. (b) Architecture of the proposed
work for multimodal attributes recovery. Given an entity, we use its learned embeddings from (a) as the
context for attribute-speciﬁc decoders to generate the missing values.

embedding space dimension.

Numerical Objects in the form of real numbers
can provide a useful source of information and
are often quite readily available. We use a feed
forward layer, after standardizing the input, in order
to embed the numbers (in fact, we are projecting
them to a higher-dimensional space, from R →
Rd). It is worth noting that existing methods treat
numbers as distinct entities, e.g., learn independent
vectors for numbers 39 and 40, relying on data to
learn that these values are similar to each other.

Text Since text can be used to store a wide vari-
ety of different types of information, for example
names versus paragraph-long descriptions, we cre-
ate different encoders depending on the lengths of
the strings involved. For attributes that are fairly
short, such as names and titles, we use character-
based stacked, bidirectional GRUs to encode them,
similar to Verga et al. (2016), using the ﬁnal output
of the top layer as the representation of the string.
For strings that are much longer, such as detailed
descriptions of entities consisting of multiple sen-
tences, we treat them as a sequence of words, and
use a CNN over the word embeddings, similar to
Francis-Landau et al. (2016), in order to learn the
embedding of such values. These two encoders pro-
vide a ﬁxed length encoding that has been shown
to be an accurate semantic representation of strings
for multiple tasks (Dos Santos and Gatti, 2014).

Images Images can also provide useful evidence
for modeling entities. For example, we can ex-

tract person’s details such as gender, age, job, etc.,
from image of the person (Levi and Hassner, 2015),
or location information such as its approximate
coordinates, neighboring locations, and size from
map images (Weyand et al., 2016). A variety of
models have been used to compactly represent the
semantic information in the images, and have been
successfully applied to tasks such as image classi-
ﬁcation, captioning (Karpathy and Fei-Fei, 2015),
and question-answering (Yang et al., 2016). To
embed images such that the encoding represents
such semantic information, we use the last hidden
layer of VGG pretrained network on Imagenet (Si-
monyan and Zisserman, 2015), followed by com-
pact bilinear pooling (Gao et al., 2016), to obtain
the embedding of the images.
Training We follow the setup from Dettmers et al.
(2018) that consists of binary cross-entropy loss
without negative sampling for both ConvE and Dis-
Mult scoring. In particular, for a given subject-
relation pair (s, r), we use a binary label vector
ts,r over all entities, indicating whether (cid:104)s, r, o(cid:105) is
observed during training. Further, we denote the
model’s probability of truth for any triple (cid:104)s, r, o(cid:105)
by ps,r
o , computed using a sigmoid over ψ(s, r, o).
The binary cross-entropy loss is thus deﬁned as:
(cid:88)

(cid:88)

o ) + (1 − ts,r

o ) log(1 − ps,r

log(ps,r

o ).

ts,r
o

(s,r)

o

We use the same loss for multimodal triples as
well, except that the summation is restricted to the
objects of the same modality, i.e. for an entity s

and its text description, ts,r is a one-hot vector over
all descriptions observed during training.

2.5 Decoding Multimodal Data

Here we describe the decoders we use to generate
multimodal values for entities from their embed-
dings. The multimodal imputing model is shown
in Figure 2b, which uses different neural decoders
to generate missing attributes (more details are pro-
vided in supplementary materials).
Numerical and Categorical data To recover the
missing numerical and categorical data such as
dates, gender, and occupation, we use a simple
feed-forward network on the entity embedding to
predict the missing attributes. In other words, we
are asking the model, if the actual birth date of an
entity is not in the KB, what will be the most likely
date, given the rest of the relational information.
These decoders are trained with embeddings from
Section 2.4, with appropriate losses (RMSE for
numerical and cross-entropy for categories).
Text A number of methods have considered gen-
erative adversarial networks (GANs) to gener-
ate grammatical and linguistically coherent sen-
tences (Yu et al., 2017; Rajeswar et al., 2017; Guo
et al., 2017). In this work, we use the adversari-
ally regularized autoencoder (ARAE) (Zhao et al.,
2017) to train generators that decodes text from
continuous codes, however, instead of using the
random noise vector z, we condition the generator
on the entity embeddings.
Images Similar to text recovery, to ﬁnd the missing
images we use conditional GAN structure. Specif-
ically, we combine the BE-GAN (Berthelot et al.,
2017) structure with pix2pix-GAN (Isola et al.,
2017) model to generate high-quality images, con-
ditioning the generator on the entity embeddings in
the knowledge base representation.

3 Related Work

There is a rich literature on modeling knowledge
bases using low-dimensional representations, dif-
fering in the operator used to score the triples. In
particular, they use matrix and tensor multiplication
(Nickel et al., 2011; Yang et al., 2015; Socher et al.,
2013), Euclidean distance (Bordes et al., 2013;
Wang et al., 2014; Lin et al., 2015), circular corre-
lation (Nickel et al., 2016), or the Hermitian dot
product (Trouillon et al., 2016) as scoring function.
However, the objects for all of these approaches
are a ﬁxed set of entities, i.e., they only embed the

Table 1: Data Statistics of the two benchmark
datasets we are using. The numbers in bold are
our contributions to the datasets.

#Link Types
#Entities
#Link Triples
#Numerical Attributes
#Image Attributes
#Text Attributes

MovieLens YAGO-10

13
2,625
100,000
2,625
1,651
1,682

45
123,182
1,079,040
111,406
61,246
107,326

structured links between the entities. Here, we use
different types of information (text, numerical val-
ues, images, etc.) in the encoding component by
treating them as relational triples.

A number of methods utilize an extra type of
information as the observed features for entities,
by either merging, concatenating, or averaging
the entity and its features to compute its embed-
dings, such as numerical values (Garcia-Duran and
Niepert, 2017) (we use KBLN from this work to
compare it with our approach using only numer-
ical as extra attributes), images (Xie et al., 2017;
Oñoro-Rubio et al., 2017) (we use IKRL from the
ﬁrst work to compare it with our approach using
only images as extra attributes), text (McAuley and
Leskovec, 2013; Zhong et al., 2015; Toutanova
et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017),
and a combination of text and image (Sergieh et al.,
2018). Further, Verga et al. (2016) address the
multilingual relation extraction task to attain a uni-
versal schema by considering raw text with no
annotation as extra feature and using matrix fac-
torization to jointly embed KB and textual rela-
tions (Riedel et al., 2013). In addition to treating
the extra information as features, graph embedding
approaches (Schlichtkrull et al., 2017; Kipf and
Welling, 2016) consider observed attributes while
encoding to achieve more accurate embeddings.

The difference between MKBE and these men-
tioned approaches is three-fold: (1) we are the ﬁrst
to use different types of information in a uniﬁed
model, (2) we treat these different types of infor-
mation (numerical, text, image) as relational triples
of structured knowledge instead of predetermined
features, i.e., ﬁrst-class citizens of the KB, and not
auxiliary features, and (3) our model represents
uncertainty in them, supporting the missing values
and facilitating recovery of missing values.

Table 2: Rating Prediction in MovieLens. Re-
sults for models that use: rating information (R),
movie-attribute (M), user-attribute (U), movies’ ti-
tle text (T), and poster images (P).

Table 3: Link Prediction in YAGO-10. Results
shown for models using: structured information
(S), textual description of the entities (D), dates as
numerical information (N), and images (I). Pub-
lished refers to Dettmers et al. (2018).

Models

MRR Hits@1 Hits@2 RMSE

t
l
u
M

t
s
i
D

E
v
n
o
C

Ratings Only, R
R+M+U
R+M+U+T
R+M+U+P
R+M+U+T+P

0.62
0.646
0.650
0.652
0.644

Ratings Only, R 0.683
0.702
R+M+U
0.728
R+M+U+T
0.726
R+M+U+P
0.726
R+M+U+T+P

0.40
0.423
0.424
0.413
0.42

0.47
0.49
0.513
0.512
0.512

0.69
0.708
0.73
0.712
0.72

0.81
0.83
0.85
0.83
0.84

1.48
1.37
1.23
1.27
1.3

1.47
1.39
1.13
1.13
1.09

4 Evaluation Benchmarks

To evaluate the performance of our multimodal
relational embeddings approach, we provide two
new benchmarks by extending existing datasets.
Table 1 provides the statistics of these datasets.

MovieLens-100k dataset (Harper and Konstan,
2016) is a popular benchmark in recommenda-
tion systems to predict user ratings with contex-
tual features, containing around 1000 users on
1700 movies. MovieLens already contains rich
relational data about occupation, gender, zip code,
and age for users and genre, release date, and
the titles for movies. We augment this data with
movie posters collected from TMDB (https://
www.themoviedb.org/). We treat the 5-point rat-
ings as ﬁve different relations in KB triple format,
i.e., (cid:104)user, r = 5, movie(cid:105), and evaluate the rating
predictions as other relations are introduced.

YAGO-10 Even though MovieLens has a variety
of data types, it is still quite small, and is over a spe-
cialized domain. We also consider a second dataset
that is much more appropriate for knowledge graph
completion and is popular for link prediction, the
YAGO3-10 knowledge graph (Suchanek et al.,
2007; Nickel et al., 2012). This graph consists of
around 120,000 entities, such as people, locations,
and organizations, and 37 relations, such as kinship,
employment, and residency, and thus much closer
to the traditional information extraction goals. We
extend this dataset with the textual description (as
an additional relation) and the images associated
with each entity (for half of the entities), provided
by DBpedia (Lehmann et al., 2015). We also in-
clude additional relations such as wasBornOnDate
that have dates as values.

Models

MRR Hits@1 Hits@3 Hits@10

t
l
u
M

t
s
i
D

E
v
n
o
C

Published
Links only, S
S+D
S+N
S+I
S+D+N
S+D+N+I

Published
Links only, S
S+D
S+N
S+I
S+D+N
S+D+N+I

KBLN
IKRL

0.337
0.326
0.36
0.325
0.342
0.359
0.372

0.523
0.482
0.564
0.549
0.566
0.588
0.584

0.503
0.509

0.237
0.221
0.262
0.213
0.235
0.243
0.268

0.448
0.372
0.478
0.462
0.471
0.517
0.52

0.41
0.423

0.379
0.375
0.395
0.382
0.352
0.401
0.418

0.564
0.519
0.595
0.587
0.597
0.603
0.604

0.549
0.556

0.54
0.538
0.571
0.517
0.618
0.679
0.792

0.658
0.634
0.713
0.701
0.72
0.722
0.698

0.658
0.663

5 Experiment Results

In this section, we ﬁrst evaluate the ability of
MKBE to utilize the multimodal information by
comparing to DistMult and ConvE through a va-
riety of tasks. Then, by considering the recovery
of missing multimodal values (text, images, and
numerical) as the motivation, we examine the ca-
pability of our models in generation. Details of the
hyperparameters and model conﬁgurations is pro-
vided in the supplementary material, and the source
code and the datasets to reproduce the results is
available at https://github.com/pouyapez/mkbe.

5.1 Link Prediction

In this section, we evaluate the capability of MKBE
in the link prediction task. The goal is to calculate
MRR and Hits@ metric (ranking evaluations) of
recovering the missing entities from triples in the
test dataset, performed by ranking all the entities
and computing the rank of the correct entity. Simi-
lar to previous work, here we focus on providing
the results in a ﬁltered setting, that is we only rank
triples in the test data against the ones that never
appear in either train or test datasets.

MovieLens-100k We train the model using Rating
as the relation between users and movies. We use
a character-level GRU for the movie titles, a sep-
arate feed-forward network for age, zip code, and
release date, and ﬁnally, we use a VGG network on
the posters (for every other relation we use a dense

Table 4: Per-Relation Breakdown showing performance of each model on different relations.

Relation

Links Only

+Numbers

+Description

+Images

MRR Hits@1 MRR Hits@1 MRR Hits@1 MRR Hits@1

isAﬃliatedTo
playsFor
hasGender
isConnectedTo
isMarriedTo

0.524
0.528
0.798
0.482
0.365

0.401
0.413
0.596
0.367
0.207

0.551
0.554
0.799
0.497
0.387

0.467
0.471
0.599
0.379
0.221

0.572
0.574
0.813
0.492
0.404

0.481
0.486
0.627
0.384
0.296

0.569
0.566
0.842
0.484
0.413

0.478
0.476
0.683
0.372
0.326

layer). Table 2 shows the link (rating) prediction
evaluation on MovieLens when test data is consist-
ing only of rating triples. We calculate our metrics
by ranking the ﬁve relations that represent ratings
instead of object entities. We label models that use
ratings as R, movie-attributes as M, user-attributes
as U, movie titles as T, and posters as P. As shown,
the model R+M+U+T outperforms others with a
considerable gap demonstrating the importance of
incorporating extra information. Hits@1 for the
baseline is 40%, matching existing recommenda-
tion systems (Guimerà et al., 2012). From these
results, we see that the models beneﬁt more from
titles as compared to the posters.

YAGO-10 The result of link prediction on our
YAGO dataset is provided in Table 3. We label
models using structured information as S, entity-
description as D, numerical information as N, and
entity-image as I. We see that the model that en-
codes all type of information consistently performs
better than other models, indicating that the model
is effective in utilizing the extra information. On
the other hand, the model that uses only text per-
forms the second best, suggesting the entity de-
scriptions contain more information than others. It
is notable that model S is outperformed by all other
models, demonstrating the importance of using dif-
ferent data types for attaining higher accuracy. This
observation is consistent across both DistMult and
ConvE, and the results obtained on ConvE are the
new state-of-art for this dataset (as compared to
Dettmers et al. (2018)). Furthermore, we imple-
ment KBLN (Garcia-Duran and Niepert, 2017) and
IKRL (Xie et al., 2017) to compare them with our
S+N and S+I models. Our models outperform these
approaches, in part because both of these methods
require same multimodal attributes for both of the
subject and object in each triple.

Relation Breakdown We perform additional anal-
ysis on the YAGO dataset to gain a deeper under-
standing of the performance of our model using

Table 5: Predicting Numbers and Categories for
YAGO (dates) and MovieLens (genres), using mod-
els with access with different information.

Models Search Decoding

S+N
62.49
S+N+D 59.42
S+N+I
59.86
All Info 57.62

58.7
56.2
55.8
54.1

Models

Accuracy

R+M
R+M+U
R+M+U+T
R+M+U+P
All Info

71.82
71.98
73.01
73.77
75.89

(a) RMSE (years) in YAGO

(b) Genres in MovieLens

ConvE method. Table 4 compares our models on
some of the most frequent relations. As shown, the
model that includes textual description signiﬁcantly
beneﬁts isAﬃliatedTo, and playsFor relations, as
this information often appears in text. Moreover,
images are useful for hasGender and isMarriedTo,
while for the relation isConnectedTo, numerical
(dates) are more effective than images.

5.2

Imputing Multimodal Attributes

Here we present an evaluation on imputing multi-
modal attributes (text, image and numerical).

Numerical and Categorical Table 5a shows
performance of predicting missing numerical at-
tributes in the data, evaluated via holding out 10%
of the data. We only consider numerical values
(dates) that are more recent than 1000AD to fo-
cus on more relevant entities. In addition to the
neural decoder, we train a search-based decoder
as well by considering all 1017 choices in the in-
terval [1000, 2017], and for each triple in the test
data, ﬁnding the number that the model scores the
highest; we use this value to compute the RMSE.
As we can see, all info outperform other methods
on both datasets, demonstrating MKBE is able to
utilize different multimodal values for modeling
numerical information. Further, the neural decoder
performs better than the search-based one, showing
the importance of proper decoder, even for ﬁnite,
enumerable sets. Along the same line, Table 5b

Table 6: Evaluating Generated Titles for Movie-
Lens using movies embeddings conditioned on just
the ratings (R) and all the information. We present
the accuracy of the users in guessing whether the
generated title for a movie was real (yes/no), and
genre of the movie (4 choices).

Table 9: Generated Images for YAGO. We con-
sider athletes, and male and female celebrities, and
compare their reference images with corresponding
ones generated from all the information.

Reference

S+N+D+I

s
e
t
e
l
h
t
A

e
l
a
M

s
e
i
t
i
r
b
e
l
e
c

e
l
a
m
e
F

s
e
i
t
i
r
b
e
l
e
c

Models

Real vs Fake Genre

R
R+M+U+T+P
Reference

63
73
90

27.2
41.6
68

Table 7: Evaluating Generated Text and Images
for YAGO using entity embeddings conditioned on
just the links (S) or all information. We present
the accuracy of the users in guessing whether the
generated text/image for a person was real (yes/no),
gender of the person, age (<35, or ≥35), and occu-
pation (3 choices).

Models

Real Gender Age Occup.

. S
p
i
r
c
s
e
d

s S
e
g
a
m

i

S+N+D+I
Reference

S+N+D+I
Reference

57.1
59.2
67.8

60
67
96

72.1
77.2
83.2

67
77
1.0

59
63.4
69.5

53
53
83

71.4
78.6
90.4

43
52
82

shows genre prediction accuracy on 10% of held-
out MovieLens dataset. Again, the model that uses
all the information outperforms other methods.

MovieLens Titles For generating movie titles, we
randomly consider 200 of them as test, 100 as vali-
dation, and the remaining ones as training data. The
goal here is to generate titles for movies in the test
data using the previously mentioned GAN struc-
ture. To evaluate our results we conduct a human
experiment on Amazon Mechanical Turk (AMT)
asking participant two questions: (1) whether they
ﬁnd the movie title real, and (2) which of the four

Table 8: Generated Descriptions for "Carles
Puyol" (and the corresponding reference from the
DBpedia) by embeddings trained from just the links
(S) and all of the information (S+N+D+I).

Model

Generated Descriptions

Reference (cid:104)subject(cid:105) (born 13 April 1978) is a Spanish re-

tired professional footballer.

Only S

(cid:104)subject(cid:105) (born 25 January 1949) is a Georgian
football coach and former professional player.

S+N+D+I (cid:104)subject(cid:105) (born 22 April 1967) is an English

former football player.

genres is most appropriate for the given title. We
consider 30 movies each as reference titles, fake ti-
tles generated from only ratings as conditional data,
and fake titles conditioned on all the information.
Further, each question was asked for 3 participants,
and the results computed over the majority choice
are shown in Table 6. Fake titles generated with
all the information are more similar to reference
movie titles, demonstrating that the embeddings
that have access to more information effectively
generate higher-quality titles.

YAGO Descriptions The goal here is to generate
descriptive text for entities from their embeddings.
Since the original descriptions can be quite long,
we consider ﬁrst sentences that are less than 30 to-
kens, resulting in 96, 405 sentences. We randomly
consider 3000 of them as test, 3000 as validation,
and the remaining as training data for the decoder.
To evaluate the quality of the generated descrip-
tions, and whether they are appropriate for the en-
tity, we conduct a user study asking participants
if they can guess the realness of sentences and the
occupation (entertainer, sportsman, or politician),
gender, and age (above or below 35) of the subject
entity from the description. We provide 30 exam-
ples for each model asking each question from 3
participants and calculate the accuracy of the ma-
jority vote. The results presented in Table 7 show
that the models are fairly competent in informing
the users of the entity information, and further,

descriptions generated from embeddings that had
access to more information outperforms the model
with only structured data. Examples of generated
descriptions are provided in Table 8 (in addition
to screenshots of user study, more examples of
generated descriptions, and MovieLens titles are
provided in supplementary materials).

YAGO Images Here, we evaluate the quality of im-
ages generated from entity embeddings by humans
(31, 520, split into train/text). Similar to descrip-
tions, we conduct a study asking users to guess the
realness of images and the occupation, gender, and
age of the subject. We provide 30 examples for
each model asking each question from 3 partici-
pants, and use the majority choice.

The results in Table 7 indicate that the images
generated with embeddings based on all the infor-
mation are more accurate for gender and occupa-
tion. Guessing age from the images is difﬁcult
since the image on DBpedia may not correspond
to the age of the person, i.e. some of the older
celebrities had photos from their youth. Examples
of generated images are shown in Table 9.

6 Discussion and Limitations

An important concern regarding KB embedding
approaches is their scalability. While large KBs are
a problem for all embedding-based link prediction
techniques, MKBE is not signiﬁcantly worse than
existing ones because we treat multimodal infor-
mation as additional triples. Speciﬁcally, although
multimodal encoders/decoders are more expensive
to train than existing relational models, the cost is
still additive as we are effectively increasing the
size of the training dataset.

In addition to scalability, there are few other chal-
lenges when working with multimodal attributes.
Although multimodal evidence provides more in-
formation, it is not at all obvious which parts of this
additional data are informative for predicting the
relational structure of the KB, and the models are
prone to overﬁtting. MKBE builds upon the design
of neural encoders and decoders that have been
effective for speciﬁc modalities, and the results
demonstrate that it is able to utilize the information
effectively. However, there is still a need to further
study models that capture multimodal attributes in
a more efﬁcient and accurate manner.

Since our imputing multimodal attributes model
is based on GAN structure and the embeddings
learned from KB representation, the generated at-

tributes are directly limited by the power of GAN
models and the amount of information in the em-
bedding vectors. Although our generated attributes
convey several aspects of corresponding entities,
their quality is far from ideal due to the size of our
datasets (both of our image and text datasets are or-
der of magnitude smaller than common datasets in
the existing text/image genration literature) and the
amount of information captured by embedding vec-
tors (the knowledge graphs are sparse). In future,
we would like to (1) expand multimodal datasets
to have more attributes (use many more entities
from YAGO), and (2) instead of using learned em-
beddings to generate missing attributes, utilize the
knowledge graph directly for generation.

7 Conclusion

Motivated by the need to utilize multiple sources
of information, such as text and images, to achieve
more accurate link prediction, we present a novel
neural approach to multimodal relational learning.
We introduce MKBE, a link prediction model that
consists of (1) a compositional encoding compo-
nent to jointly learn the entity and multimodal em-
beddings to encode the information available for
each entity, and (2) adversarially trained decoding
component that use these entity embeddings to im-
pute missing multimodal values. We enrich two
existing datasets, YAGO-10 and MovieLens-100k,
with multimodal information to introduce bench-
marks. We show that MKBE, in comparison to
existing link predictors DistMult and ConvE, can
achieve higher accuracy on link prediction by utiliz-
ing the multimodal evidence. Further, we show that
MKBE effectively incorporates relational informa-
tion to generate high-quality multimodal attributes
like images and text. We have release the datasets
and the open-source implementation of our models
at https://github.com/pouyapez/mkbe.

Acknowledgements

We would like to thank Zhengli Zhao, Robert L.
Logan IV, Dheeru Dua, Casey Graff, and the anony-
mous reviewers for their detailed feedback and sug-
gestions. This work is supported in part by Allen
Institute for Artiﬁcial Intelligence (AI2) and in part
by NSF award #IIS-1817183. The views expressed
are those of the authors and do not reﬂect the ofﬁ-
cial policy or position of the funding agencies.

References

David Berthelot, Tom Schumm, and Luke Metz. 2017.
Began: Boundary equilibrium generative adversarial
networks. arXiv preprint arXiv:1703.10717.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran,
Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Antoine Bordes,

Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured em-
beddings of knowledge bases. In Conference on Ar-
tiﬁcial Intelligence (AAAI).

Thomas N Kipf and Max Welling. 2016. Variational
graph auto-encoders. NIPS Workshop on Bayesian
Deep Learning (NIPS-16 BDL).

Günter Klambauer, Thomas Unterthiner, Andreas
Mayr, and Sepp Hochreiter. 2017. Self-normalizing
neural networks. Advances in Neural Information
Processing Systems.

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
Dimitris Kontokostas, Pablo N Mendes, Sebastian
Hellmann, Mohamed Morsey, Patrick Van Kleef,
Sören Auer, et al. 2015. Dbpedia–a large-scale, mul-
tilingual knowledge base extracted from wikipedia.
Semantic Web, 6(2):167–195.

Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,
and Sebastian Riedel. 2018.
Convolutional 2d
knowledge graph embeddings. Conference on Ar-
tiﬁcial Intelligence (AAAI).

Gil Levi and Tal Hassner. 2015. Age and gender clas-
siﬁcation using convolutional neural networks.
In
IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 34–42.

Cícero Nogueira Dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In COLING, pages 69–78.

Matthew Francis-Landau, Greg Durrett, and Dan Klein.
2016. Capturing semantic similarity for entity link-
ing with convolutional neural networks. Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL).

Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor
Darrell. 2016. Compact bilinear pooling. In IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 317–326.

Alberto Garcia-Duran and Mathias Niepert. 2017.
Kblrn: End-to-end learning of knowledge base repre-
sentations with latent, relational, and numerical fea-
tures. Conference on Uncertainty in Artiﬁcial Intel-
ligence (UAI).

Roger Guimerà, Alejandro Llorente, Esteban Moro,
and Marta Sales-Pardo. 2012. Predicting human
preferences using the block structure of complex so-
cial networks. PloS one, 7(9):e44620.

Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong
Yu, and Jun Wang. 2017. Long text generation via
adversarial training with leaked information. Con-
ference on Artiﬁcial Intelligence (AAAI).

F Maxwell Harper and Joseph A Konstan. 2016.
The movielens datasets: History and context.
ACM Transactions on Interactive Intelligent Systems
(TiiS), 5(4):19.

Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with con-
Efros. 2017.
ditional adversarial networks. IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 3128–3137.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu,
and Xuan Zhu. 2015. Learning entity and relation
embeddings for knowledge graph completion.
In
Conference on Artiﬁcial Intelligence (AAAI), pages
2181–2187.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In ACM Conference on Rec-
ommender Systems, pages 165–172. ACM.

Maximilian Nickel, Lorenzo Rosasco, Tomaso A Pog-
gio, et al. 2016. Holographic embeddings of knowl-
edge graphs. In Conference on Artiﬁcial Intelligence
(AAAI), pages 1955–1961.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
In International
learning on multi-relational data.
Conference on Machine Learning (ICML), pages
809–816.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
In International Confer-
learning for linked data.
ence on World Wide Web, pages 271–280. ACM.

Daniel Oñoro-Rubio, Mathias Niepert, Alberto García-
Durán, Roberto González-Sánchez, and Roberto J
López-Sastre. 2017. Representation learning for
visual-relational knowledge graphs. arXiv preprint
arXiv:1709.02314.

Sai Rajeswar, Sandeep Subramanian, Francis Dutil,
Christopher Pal, and Aaron Courville. 2017. Adver-
sarial generation of natural language. Workshop on
Representation Learning for NLP.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT), pages 74–84.

Tobias Weyand, Ilya Kostrikov, and James Philbin.
2016. Planet-photo geolocation with convolutional
neural networks. In European Conference on Com-
puter Vision, pages 37–55. Springer.

Ruobing Xie, Zhiyuan Liu, Tat-seng Chua, Huanbo
Image-embodied
Luan, and Maosong Sun. 2017.
International
knowledge representation learning.
Joint Conference on Artiﬁcial Intelligence (IJCAI).

Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan,
and Maosong Sun. 2016. Representation learning
of knowledge graphs with entity descriptions.
In
Conference on Artiﬁcial Intelligence (AAAI), pages
2659–2665.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding entities and
relations for learning and inference in knowledge
International Conference on Learning Rep-
bases.
resentations (ICLR).

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2016. Stacked attention networks
for image question answering. In IEEE Conference
on Computer Vision and Pattern Recognition, pages
21–29.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In Conference on Artiﬁcial In-
telligence (AAAI), pages 2852–2858.

Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan-
der M Rush, Yann LeCun, et al. 2017. Adver-
arXiv preprint
sarially regularized autoencoders.
arXiv:1706.04223.

Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan,
and Zheng Chen. 2015. Aligning knowledge and
In Em-
text embeddings by entity descriptions.
pirical Methods in Natural Language Processing
(EMNLP), pages 267–272.

Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
Rianne van den Berg, Ivan Titov, and Max Welling.
2017. Modeling relational data with graph convo-
lutional networks. European Semantic Web Confer-
ence.

Hatem Mousselly Sergieh, Teresa Botschen,

Iryna
Gurevych, and Stefan Roth. 2018. A multimodal
translation-based approach for knowledge graph rep-
resentation learning. In Joint Conference on Lexical
and Computational Semantics, pages 225–234.

Karen Simonyan and Andrew Zisserman. 2015. Very
deep convolutional networks for large-scale image
recognition. International Conference on Learning
Representations (ICLR).

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
In
sor networks for knowledge base completion.
Advances in neural information processing systems,
pages 926–934.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
In International Conference on World Wide
edge.
Web, pages 697–706. ACM.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of
text and knowledge bases. In Empirical Methods in
Natural Language Processing (EMNLP), volume 15,
pages 1499–1509.

Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoi-
fung Poon, and Chris Quirk. 2016. Compositional
learning of embeddings for relation paths in knowl-
In Association for Computa-
edge base and text.
tional Linguistics (ACL).

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Com-
In In-
plex embeddings for simple link prediction.
ternational Conference on Machine Learning, pages
2071–2080.

Cunchao Tu, Han Liu, Zhiyuan Liu, and Maosong Sun.
2017. Cane: Context-aware network embedding for
In Annual Meeting of the As-
relation modeling.
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1722–1731.

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multilin-
gual relation extraction using compositional univer-
sal schema. Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL).

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
In Conference on Artiﬁcial
lating on hyperplanes.
Intelligence (AAAI), pages 1112–1119.

Embedding Multimodal Relational Data for Knowledge Base Completion

Pouya Pezeshkpour
University of California
Irvine, CA
pezeshkp@uci.edu

Liyan Chen
University of California
Irvine, CA
liyanc@uci.edu

Sameer Singh
University of California
Irvine, CA
sameer@uci.edu

8
1
0
2
 
p
e
S
 
7
 
 
]
I

A
.
s
c
[
 
 
2
v
1
4
3
1
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

images, and numerical values.

Representing entities and relations in an em-
bedding space is a well-studied approach for
machine learning on relational data. Exist-
ing approaches, however, primarily focus on
simple link structure between a ﬁnite set of
entities,
ignoring the variety of data types
that are often used in knowledge bases, such
In
as text,
this paper, we propose multimodal knowledge
base embeddings (MKBE) that use different
neural encoders for this variety of observed
data, and combine them with existing rela-
tional models to learn embeddings of the en-
tities and multimodal data.
Further, using
these learned embedings and different neural
decoders, we introduce a novel multimodal
imputation model to generate missing multi-
modal values, like text and images, from in-
formation in the knowledge base. We enrich
existing relational datasets to create two novel
benchmarks that contain additional informa-
tion such as textual descriptions and images
of the original entities. We demonstrate that
our models utilize this additional information
effectively to provide more accurate link pre-
diction, achieving state-of-the-art results with
a considerable gap of 5-7% over existing meth-
ods. Further, we evaluate the quality of our
generated multimodal values via a user study.
We have release the datasets and the open-
source implementation of our models at https:
//github.com/pouyapez/mkbe.

1

Introduction

Knowledge bases (KB) are an essential part of
many computational systems with applications in
search, structured data management, recommen-
dations, question answering, and information re-
trieval. However, KBs often suffer from incom-
pleteness, noise in their entries, and inefﬁcient in-
ference under uncertainty. To address these issues,
learning relational knowledge representations has

been a focus of active research (Bordes et al., 2011,
2013; Yang et al., 2015; Nickel et al., 2016; Trouil-
lon et al., 2016; Dettmers et al., 2018). These ap-
proaches represent relational triples, that consist
of a subject entity, relation, and an object entity,
by learning ﬁxed, low-dimensional representations
for each entity and relation from observations, en-
coding the uncertainty and inferring missing facts
accurately and efﬁciently. The subject and the ob-
ject entities come from a ﬁxed, enumerable set of
entities that appear in the knowledge base.

Knowledge bases in the real world, however,
contain a wide variety of data types beyond these
direct links. Apart from relations to a ﬁxed set
of entities, KBs often not only include numeri-
cal attributes (such as ages, dates, ﬁnancial, and
geoinformation), but also textual attributes (such
as names, descriptions, and titles/designations) and
images (proﬁle photos, ﬂags, posters, etc.). These
different types of data can play a crucial role as
extra pieces of evidence for knowledge base com-
pletion. For example the textual descriptions and
images might provide evidence for a person’s age,
profession, and designation. In the multimodal KB
shown in Figure 1 for example, the image can be
helpful in predicting of Carles Puyol’s occupation,
while the description contains his nationality. Incor-
porating this information into existing approaches
as entities, unfortunately, is challenging as they as-
sign each entity a distinct vector and predict miss-
ing links (or attributes) by enumerating over the
possible values, both of which are only possible
if the entities come from a small, enumerable set.
There is thus a crucial need for relational modeling
that goes beyond just the link-based view of KB
completion, by not only utilizing multimodal infor-
mation for better link prediction between existing
entities, but also being able to generate missing
multimodal values.

In this paper, we introduce multimodal knowl-

existing YAGO-10 and MovieLens-100k datasets
to include additional relations such as textual de-
scriptions, numerical attributes, and images of the
entities. We demonstrate that MKBE utilizes the
additional information effectively to provide gains
in link-prediction accuracy, achieving state-of-the-
art results on these datasets for both the DistMult
and the ConvE scoring functions. We evaluate the
quality of multimodal attributes generated by the
decoders via user studies that demonstrate their re-
alism and information content, along with present-
ing examples of such generated text and images.

2 Multimodal KB Completion

As described earlier, KBs often contain differ-
ent types of information about entities including
links, textual descriptions, categorical attributes,
numerical values, and images. In this section, we
brieﬂy introduce existing relational embedding ap-
proaches that focus on modeling the linked data
using distinct, dense vectors. We then describe
MKBE that extends these approaches to the multi-
modal setting, i.e., modeling the KB using all the
different information to predict the missing links
and impute the missing attributes.

2.1 Background on Link Prediction

Factual statements in a knowledge base are repre-
sented using a triple of subject, relation, and ob-
ject, (cid:104)s, r, o(cid:105), where s, o ∈ ξ, a set of entities, and
r ∈ R, a set of relations. Respectively, we con-
sider two goals for relational modeling, (1) to train
a machine learning model that can score the truth
value of any factual statement, and (2) to predict
missing links between the entities. In existing ap-
proaches, a scoring function ψ : ξ × R × ξ → R
(or sometimes, [0, 1]) is learned to evaluate whether
any given fact is true, as per the model. For pre-
dicting links between the entities, since the set ξ
is small enough to be enumerated, missing links
of the form (cid:104)s, r, ?(cid:105) are identiﬁed by enumerating
all the objects and scoring the triples using ψ (i.e.
assume the resulting entity comes from a known
set). For example, in Figure 1, the goal is to predict
that Carles Puyol plays for Barcelona.

Many of the recent advances in link prediction
use an embedding-based approach; each entity in ξ
and relation in R are assigned distinct, dense vec-
tors, which are then used by ψ to compute the score.
In DistMult (Yang et al., 2015), for example, each
entity i is mapped to a d-dimensional dense vector

Figure 1: Example of a Multimodal KB. Graph
representation of (a part of) a KB that consists of
regular links (in black) and multimodal ones (in
purple) that we support in this work.

edge base embeddings (MKBE) for modeling
knowledge bases that contain a variety of data
types, such as links, text, images, numerical, and
categorical values. We propose neural encoders and
decoders to replace initial layers of any embedding-
based relational model; we apply them to Dist-
Mult (Yang et al., 2015) and ConvE (Dettmers et al.,
2018) here. Speciﬁcally, instead of learning a dis-
tinct vector for each entity and using enumeration
to predict links, MKBE includes the following ex-
tensions: (1) introduce additional neural encoders
to embed multimodal evidence types that the rela-
tional model uses to predict links, and (2) introduce
neural decoders that use an entity’s embedding to
generate its multimodal attributes (like image and
text). For example, when the object of a triple is an
image, we encode it into a ﬁxed-length vector us-
ing a CNN, while textual objects are encoded using
RNN-based sequence encoders. The scoring mod-
ule remains identical to the underlying relational
model; given the vector representations of the sub-
ject, relation, and object of a triple, we produce a
score indicating the probability that the triple is cor-
rect using DistMult or ConvE. After learning the
KB representation, neural decoders use entity em-
beddings to generate missing multimodal attributes,
for example, generating the description of a person
from their structured information in the KB. This
uniﬁed framework allows for ﬂow of the informa-
tion across the different relation types (multimodal
or otherwise), providing a more accurate modeling
of relational data.

We provide an evaluation of our proposed ap-
proach on two relational KBs. Since we are intro-
ducing the multimodal KB completion setting, we
provide two benchmarks, created by extending the

(ei ∈ Rd) and each relation r to a diagonal matrix
Rr ∈ Rd×d, and consequently, the score for any
triple (cid:104)s, r, o(cid:105) is computed as ψ(s, r, o) = eT
s Rreo.
Along similar lines, ConvE (Dettmers et al., 2018)
uses vectors to represent the entities and the re-
lations, es, eo, rr ∈ Rd×1, then, after applying a
CNN layer on es and rr, combines it with eo to
score a triplet, i.e. the scoring function ψ(s, r, o) is
f (vec(f ([ ¯es; ¯rr ∗ w]))W )eo. Other relational em-
bedding approaches primarily vary in their design
of the scoring function (Bordes et al., 2013; Yang
et al., 2015; Nickel et al., 2016; Trouillon et al.,
2016), but share the shortcoming of assigning dis-
tinct vectors to every entity, and assuming that the
possible object entities can be enumerated. In this
work we focus on DistMult because of its sim-
plicity, popularity, and high accuracy, and ConvE
because of its state-of-the-art results.

2.2 Problem Setup

When faced with additional triples in form of mul-
timodal data, the setup of link prediction is slightly
different. Consider a set of all potential multimodal
objects, M, i.e. possible images, text, numerical,
and categorical values, and multimodal evidence
triples, (cid:104)s, r, o(cid:105), where s ∈ ξ, r ∈ R, and o ∈ M.
Our goals with incorporating multimodal informa-
tion into KB remain the same: we want to be able
to score the truth of any triple (cid:104)s, r, o(cid:105), where o is
from ξ (link data) or from M (multimodal data),
and to be able to predict missing value (cid:104)s, r, ?(cid:105) that
may be from ξ or M (depending on r). For the
example in Figure 1, in addition to predicting that
Carles Puyol plays for Barcelona from multimodal
evidence, we are also interested in generating an
image for Carles Puyol, if it is missing.

Existing approaches to this problem assume that
the subjects and the objects are from a ﬁxed set of
entities ξ, and thus are treated as indices into that
set, which fails for the multimodal setting primarily
for two reasons. First, learning distinct vectors for
each object entity does not apply to multimodal
values as they will ignore the actual content of the
multimodal attribute. For example, there will be
no way to generalize vectors learned during train-
ing to unseen values that might appear in the test;
this is not a problem for the standard setup due to
the assumption that all entities have been observed
during training. Second, in order to predict a miss-
ing multimodal value, (cid:104)s, r, ?(cid:105), enumeration is not
possible as the search space is potentially inﬁnite

(or at least intractable to search).

2.3 Multimodal KB Embeddings (MKBE)

To incorporate such multimodal objects into the
existing relational models like DistMult and ConvE,
we propose to learn embeddings for these types of
data as well. We utilize recent advances in deep
learning to construct encoders for these objects to
represent them, essentially providing an embedding
eo for any object value.

The overall goal remains the same: the model
needs to utilize all the observed subjects, objects,
and relations, across different data types, in order
to estimate whether any fact (cid:104)s, r, o(cid:105) holds. We
present an example of an instantiation of MKBE
for a knowledge base containing YAGO entities in
Figure 2a. For any triple (cid:104)s, r, o(cid:105), we embed the
subject (Carles Puyol) and the relation (such as
playsFor, wasBornOn, or playsFor) using a direct
lookup. For the object, depending on the domain
(indexed, string, numerical, or image, respectively),
we use approrpiate encoders to compute its embed-
ding eo. As in DistMult and ConvE, these embed-
dings are used to compute the score of the triple.

Via these neural encoders, the model can use
the information content of multimodal objects to
predict missing links where the objects are from
ξ, however, learning embeddings for objects in M
is not sufﬁcient to generate missing multimodal
values, i.e. (cid:104)s, r, ?(cid:105) where the object is in M. Con-
sequently, we introduce a set of neural decoders
D : ξ × R → M that use entity embeddings to
generate multimodal values. An outline of our
model for imputing missing values is depicted in
Figure 2b. We will describe these decoders in Sec-
tion 2.5.

2.4 Encoding Multimodal Data

Here we describe the encoders we use for mul-
timodal objects. A simple example of MKBE is
provided in Figure 2a. As it shows, we use different
encoder to embed each speciﬁc data type.

Structured Knowledge Consider a triplet of in-
formation in the form of (cid:104)s, r, o(cid:105). To represent
the subject entity s and the relation r as indepen-
dent embedding vectors (as in previous work), we
pass their one-hot encoding through a dense layer.
Furthermore, for the case that the object entity is
categorical, we embed it through a dense layer with
a recently introduced selu activation (Klambauer
et al., 2017), with the same number of nodes as the

Figure 2: Multimodal KB Embeddings (MKBE): (a) Proposed architecture that, given any entity and
its relations, uses domain-speciﬁc encoders to embed each object. The embeddings of entities, and the
relation are then used to score the truth value of the triple by the Scorer. (b) Architecture of the proposed
work for multimodal attributes recovery. Given an entity, we use its learned embeddings from (a) as the
context for attribute-speciﬁc decoders to generate the missing values.

embedding space dimension.

Numerical Objects in the form of real numbers
can provide a useful source of information and
are often quite readily available. We use a feed
forward layer, after standardizing the input, in order
to embed the numbers (in fact, we are projecting
them to a higher-dimensional space, from R →
Rd). It is worth noting that existing methods treat
numbers as distinct entities, e.g., learn independent
vectors for numbers 39 and 40, relying on data to
learn that these values are similar to each other.

Text Since text can be used to store a wide vari-
ety of different types of information, for example
names versus paragraph-long descriptions, we cre-
ate different encoders depending on the lengths of
the strings involved. For attributes that are fairly
short, such as names and titles, we use character-
based stacked, bidirectional GRUs to encode them,
similar to Verga et al. (2016), using the ﬁnal output
of the top layer as the representation of the string.
For strings that are much longer, such as detailed
descriptions of entities consisting of multiple sen-
tences, we treat them as a sequence of words, and
use a CNN over the word embeddings, similar to
Francis-Landau et al. (2016), in order to learn the
embedding of such values. These two encoders pro-
vide a ﬁxed length encoding that has been shown
to be an accurate semantic representation of strings
for multiple tasks (Dos Santos and Gatti, 2014).

Images Images can also provide useful evidence
for modeling entities. For example, we can ex-

tract person’s details such as gender, age, job, etc.,
from image of the person (Levi and Hassner, 2015),
or location information such as its approximate
coordinates, neighboring locations, and size from
map images (Weyand et al., 2016). A variety of
models have been used to compactly represent the
semantic information in the images, and have been
successfully applied to tasks such as image classi-
ﬁcation, captioning (Karpathy and Fei-Fei, 2015),
and question-answering (Yang et al., 2016). To
embed images such that the encoding represents
such semantic information, we use the last hidden
layer of VGG pretrained network on Imagenet (Si-
monyan and Zisserman, 2015), followed by com-
pact bilinear pooling (Gao et al., 2016), to obtain
the embedding of the images.
Training We follow the setup from Dettmers et al.
(2018) that consists of binary cross-entropy loss
without negative sampling for both ConvE and Dis-
Mult scoring. In particular, for a given subject-
relation pair (s, r), we use a binary label vector
ts,r over all entities, indicating whether (cid:104)s, r, o(cid:105) is
observed during training. Further, we denote the
model’s probability of truth for any triple (cid:104)s, r, o(cid:105)
by ps,r
o , computed using a sigmoid over ψ(s, r, o).
The binary cross-entropy loss is thus deﬁned as:
(cid:88)

(cid:88)

o ) + (1 − ts,r

o ) log(1 − ps,r

log(ps,r

o ).

ts,r
o

(s,r)

o

We use the same loss for multimodal triples as
well, except that the summation is restricted to the
objects of the same modality, i.e. for an entity s

and its text description, ts,r is a one-hot vector over
all descriptions observed during training.

2.5 Decoding Multimodal Data

Here we describe the decoders we use to generate
multimodal values for entities from their embed-
dings. The multimodal imputing model is shown
in Figure 2b, which uses different neural decoders
to generate missing attributes (more details are pro-
vided in supplementary materials).
Numerical and Categorical data To recover the
missing numerical and categorical data such as
dates, gender, and occupation, we use a simple
feed-forward network on the entity embedding to
predict the missing attributes. In other words, we
are asking the model, if the actual birth date of an
entity is not in the KB, what will be the most likely
date, given the rest of the relational information.
These decoders are trained with embeddings from
Section 2.4, with appropriate losses (RMSE for
numerical and cross-entropy for categories).
Text A number of methods have considered gen-
erative adversarial networks (GANs) to gener-
ate grammatical and linguistically coherent sen-
tences (Yu et al., 2017; Rajeswar et al., 2017; Guo
et al., 2017). In this work, we use the adversari-
ally regularized autoencoder (ARAE) (Zhao et al.,
2017) to train generators that decodes text from
continuous codes, however, instead of using the
random noise vector z, we condition the generator
on the entity embeddings.
Images Similar to text recovery, to ﬁnd the missing
images we use conditional GAN structure. Specif-
ically, we combine the BE-GAN (Berthelot et al.,
2017) structure with pix2pix-GAN (Isola et al.,
2017) model to generate high-quality images, con-
ditioning the generator on the entity embeddings in
the knowledge base representation.

3 Related Work

There is a rich literature on modeling knowledge
bases using low-dimensional representations, dif-
fering in the operator used to score the triples. In
particular, they use matrix and tensor multiplication
(Nickel et al., 2011; Yang et al., 2015; Socher et al.,
2013), Euclidean distance (Bordes et al., 2013;
Wang et al., 2014; Lin et al., 2015), circular corre-
lation (Nickel et al., 2016), or the Hermitian dot
product (Trouillon et al., 2016) as scoring function.
However, the objects for all of these approaches
are a ﬁxed set of entities, i.e., they only embed the

Table 1: Data Statistics of the two benchmark
datasets we are using. The numbers in bold are
our contributions to the datasets.

#Link Types
#Entities
#Link Triples
#Numerical Attributes
#Image Attributes
#Text Attributes

MovieLens YAGO-10

13
2,625
100,000
2,625
1,651
1,682

45
123,182
1,079,040
111,406
61,246
107,326

structured links between the entities. Here, we use
different types of information (text, numerical val-
ues, images, etc.) in the encoding component by
treating them as relational triples.

A number of methods utilize an extra type of
information as the observed features for entities,
by either merging, concatenating, or averaging
the entity and its features to compute its embed-
dings, such as numerical values (Garcia-Duran and
Niepert, 2017) (we use KBLN from this work to
compare it with our approach using only numer-
ical as extra attributes), images (Xie et al., 2017;
Oñoro-Rubio et al., 2017) (we use IKRL from the
ﬁrst work to compare it with our approach using
only images as extra attributes), text (McAuley and
Leskovec, 2013; Zhong et al., 2015; Toutanova
et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017),
and a combination of text and image (Sergieh et al.,
2018). Further, Verga et al. (2016) address the
multilingual relation extraction task to attain a uni-
versal schema by considering raw text with no
annotation as extra feature and using matrix fac-
torization to jointly embed KB and textual rela-
tions (Riedel et al., 2013). In addition to treating
the extra information as features, graph embedding
approaches (Schlichtkrull et al., 2017; Kipf and
Welling, 2016) consider observed attributes while
encoding to achieve more accurate embeddings.

The difference between MKBE and these men-
tioned approaches is three-fold: (1) we are the ﬁrst
to use different types of information in a uniﬁed
model, (2) we treat these different types of infor-
mation (numerical, text, image) as relational triples
of structured knowledge instead of predetermined
features, i.e., ﬁrst-class citizens of the KB, and not
auxiliary features, and (3) our model represents
uncertainty in them, supporting the missing values
and facilitating recovery of missing values.

Table 2: Rating Prediction in MovieLens. Re-
sults for models that use: rating information (R),
movie-attribute (M), user-attribute (U), movies’ ti-
tle text (T), and poster images (P).

Table 3: Link Prediction in YAGO-10. Results
shown for models using: structured information
(S), textual description of the entities (D), dates as
numerical information (N), and images (I). Pub-
lished refers to Dettmers et al. (2018).

Models

MRR Hits@1 Hits@2 RMSE

t
l
u
M

t
s
i
D

E
v
n
o
C

Ratings Only, R
R+M+U
R+M+U+T
R+M+U+P
R+M+U+T+P

0.62
0.646
0.650
0.652
0.644

Ratings Only, R 0.683
0.702
R+M+U
0.728
R+M+U+T
0.726
R+M+U+P
0.726
R+M+U+T+P

0.40
0.423
0.424
0.413
0.42

0.47
0.49
0.513
0.512
0.512

0.69
0.708
0.73
0.712
0.72

0.81
0.83
0.85
0.83
0.84

1.48
1.37
1.23
1.27
1.3

1.47
1.39
1.13
1.13
1.09

4 Evaluation Benchmarks

To evaluate the performance of our multimodal
relational embeddings approach, we provide two
new benchmarks by extending existing datasets.
Table 1 provides the statistics of these datasets.

MovieLens-100k dataset (Harper and Konstan,
2016) is a popular benchmark in recommenda-
tion systems to predict user ratings with contex-
tual features, containing around 1000 users on
1700 movies. MovieLens already contains rich
relational data about occupation, gender, zip code,
and age for users and genre, release date, and
the titles for movies. We augment this data with
movie posters collected from TMDB (https://
www.themoviedb.org/). We treat the 5-point rat-
ings as ﬁve different relations in KB triple format,
i.e., (cid:104)user, r = 5, movie(cid:105), and evaluate the rating
predictions as other relations are introduced.

YAGO-10 Even though MovieLens has a variety
of data types, it is still quite small, and is over a spe-
cialized domain. We also consider a second dataset
that is much more appropriate for knowledge graph
completion and is popular for link prediction, the
YAGO3-10 knowledge graph (Suchanek et al.,
2007; Nickel et al., 2012). This graph consists of
around 120,000 entities, such as people, locations,
and organizations, and 37 relations, such as kinship,
employment, and residency, and thus much closer
to the traditional information extraction goals. We
extend this dataset with the textual description (as
an additional relation) and the images associated
with each entity (for half of the entities), provided
by DBpedia (Lehmann et al., 2015). We also in-
clude additional relations such as wasBornOnDate
that have dates as values.

Models

MRR Hits@1 Hits@3 Hits@10

t
l
u
M

t
s
i
D

E
v
n
o
C

Published
Links only, S
S+D
S+N
S+I
S+D+N
S+D+N+I

Published
Links only, S
S+D
S+N
S+I
S+D+N
S+D+N+I

KBLN
IKRL

0.337
0.326
0.36
0.325
0.342
0.359
0.372

0.523
0.482
0.564
0.549
0.566
0.588
0.584

0.503
0.509

0.237
0.221
0.262
0.213
0.235
0.243
0.268

0.448
0.372
0.478
0.462
0.471
0.517
0.52

0.41
0.423

0.379
0.375
0.395
0.382
0.352
0.401
0.418

0.564
0.519
0.595
0.587
0.597
0.603
0.604

0.549
0.556

0.54
0.538
0.571
0.517
0.618
0.679
0.792

0.658
0.634
0.713
0.701
0.72
0.722
0.698

0.658
0.663

5 Experiment Results

In this section, we ﬁrst evaluate the ability of
MKBE to utilize the multimodal information by
comparing to DistMult and ConvE through a va-
riety of tasks. Then, by considering the recovery
of missing multimodal values (text, images, and
numerical) as the motivation, we examine the ca-
pability of our models in generation. Details of the
hyperparameters and model conﬁgurations is pro-
vided in the supplementary material, and the source
code and the datasets to reproduce the results is
available at https://github.com/pouyapez/mkbe.

5.1 Link Prediction

In this section, we evaluate the capability of MKBE
in the link prediction task. The goal is to calculate
MRR and Hits@ metric (ranking evaluations) of
recovering the missing entities from triples in the
test dataset, performed by ranking all the entities
and computing the rank of the correct entity. Simi-
lar to previous work, here we focus on providing
the results in a ﬁltered setting, that is we only rank
triples in the test data against the ones that never
appear in either train or test datasets.

MovieLens-100k We train the model using Rating
as the relation between users and movies. We use
a character-level GRU for the movie titles, a sep-
arate feed-forward network for age, zip code, and
release date, and ﬁnally, we use a VGG network on
the posters (for every other relation we use a dense

Table 4: Per-Relation Breakdown showing performance of each model on different relations.

Relation

Links Only

+Numbers

+Description

+Images

MRR Hits@1 MRR Hits@1 MRR Hits@1 MRR Hits@1

isAﬃliatedTo
playsFor
hasGender
isConnectedTo
isMarriedTo

0.524
0.528
0.798
0.482
0.365

0.401
0.413
0.596
0.367
0.207

0.551
0.554
0.799
0.497
0.387

0.467
0.471
0.599
0.379
0.221

0.572
0.574
0.813
0.492
0.404

0.481
0.486
0.627
0.384
0.296

0.569
0.566
0.842
0.484
0.413

0.478
0.476
0.683
0.372
0.326

layer). Table 2 shows the link (rating) prediction
evaluation on MovieLens when test data is consist-
ing only of rating triples. We calculate our metrics
by ranking the ﬁve relations that represent ratings
instead of object entities. We label models that use
ratings as R, movie-attributes as M, user-attributes
as U, movie titles as T, and posters as P. As shown,
the model R+M+U+T outperforms others with a
considerable gap demonstrating the importance of
incorporating extra information. Hits@1 for the
baseline is 40%, matching existing recommenda-
tion systems (Guimerà et al., 2012). From these
results, we see that the models beneﬁt more from
titles as compared to the posters.

YAGO-10 The result of link prediction on our
YAGO dataset is provided in Table 3. We label
models using structured information as S, entity-
description as D, numerical information as N, and
entity-image as I. We see that the model that en-
codes all type of information consistently performs
better than other models, indicating that the model
is effective in utilizing the extra information. On
the other hand, the model that uses only text per-
forms the second best, suggesting the entity de-
scriptions contain more information than others. It
is notable that model S is outperformed by all other
models, demonstrating the importance of using dif-
ferent data types for attaining higher accuracy. This
observation is consistent across both DistMult and
ConvE, and the results obtained on ConvE are the
new state-of-art for this dataset (as compared to
Dettmers et al. (2018)). Furthermore, we imple-
ment KBLN (Garcia-Duran and Niepert, 2017) and
IKRL (Xie et al., 2017) to compare them with our
S+N and S+I models. Our models outperform these
approaches, in part because both of these methods
require same multimodal attributes for both of the
subject and object in each triple.

Relation Breakdown We perform additional anal-
ysis on the YAGO dataset to gain a deeper under-
standing of the performance of our model using

Table 5: Predicting Numbers and Categories for
YAGO (dates) and MovieLens (genres), using mod-
els with access with different information.

Models Search Decoding

S+N
62.49
S+N+D 59.42
S+N+I
59.86
All Info 57.62

58.7
56.2
55.8
54.1

Models

Accuracy

R+M
R+M+U
R+M+U+T
R+M+U+P
All Info

71.82
71.98
73.01
73.77
75.89

(a) RMSE (years) in YAGO

(b) Genres in MovieLens

ConvE method. Table 4 compares our models on
some of the most frequent relations. As shown, the
model that includes textual description signiﬁcantly
beneﬁts isAﬃliatedTo, and playsFor relations, as
this information often appears in text. Moreover,
images are useful for hasGender and isMarriedTo,
while for the relation isConnectedTo, numerical
(dates) are more effective than images.

5.2

Imputing Multimodal Attributes

Here we present an evaluation on imputing multi-
modal attributes (text, image and numerical).

Numerical and Categorical Table 5a shows
performance of predicting missing numerical at-
tributes in the data, evaluated via holding out 10%
of the data. We only consider numerical values
(dates) that are more recent than 1000AD to fo-
cus on more relevant entities. In addition to the
neural decoder, we train a search-based decoder
as well by considering all 1017 choices in the in-
terval [1000, 2017], and for each triple in the test
data, ﬁnding the number that the model scores the
highest; we use this value to compute the RMSE.
As we can see, all info outperform other methods
on both datasets, demonstrating MKBE is able to
utilize different multimodal values for modeling
numerical information. Further, the neural decoder
performs better than the search-based one, showing
the importance of proper decoder, even for ﬁnite,
enumerable sets. Along the same line, Table 5b

Table 6: Evaluating Generated Titles for Movie-
Lens using movies embeddings conditioned on just
the ratings (R) and all the information. We present
the accuracy of the users in guessing whether the
generated title for a movie was real (yes/no), and
genre of the movie (4 choices).

Table 9: Generated Images for YAGO. We con-
sider athletes, and male and female celebrities, and
compare their reference images with corresponding
ones generated from all the information.

Reference

S+N+D+I

s
e
t
e
l
h
t
A

e
l
a
M

s
e
i
t
i
r
b
e
l
e
c

e
l
a
m
e
F

s
e
i
t
i
r
b
e
l
e
c

Models

Real vs Fake Genre

R
R+M+U+T+P
Reference

63
73
90

27.2
41.6
68

Table 7: Evaluating Generated Text and Images
for YAGO using entity embeddings conditioned on
just the links (S) or all information. We present
the accuracy of the users in guessing whether the
generated text/image for a person was real (yes/no),
gender of the person, age (<35, or ≥35), and occu-
pation (3 choices).

Models

Real Gender Age Occup.

. S
p
i
r
c
s
e
d

s S
e
g
a
m

i

S+N+D+I
Reference

S+N+D+I
Reference

57.1
59.2
67.8

60
67
96

72.1
77.2
83.2

67
77
1.0

59
63.4
69.5

53
53
83

71.4
78.6
90.4

43
52
82

shows genre prediction accuracy on 10% of held-
out MovieLens dataset. Again, the model that uses
all the information outperforms other methods.

MovieLens Titles For generating movie titles, we
randomly consider 200 of them as test, 100 as vali-
dation, and the remaining ones as training data. The
goal here is to generate titles for movies in the test
data using the previously mentioned GAN struc-
ture. To evaluate our results we conduct a human
experiment on Amazon Mechanical Turk (AMT)
asking participant two questions: (1) whether they
ﬁnd the movie title real, and (2) which of the four

Table 8: Generated Descriptions for "Carles
Puyol" (and the corresponding reference from the
DBpedia) by embeddings trained from just the links
(S) and all of the information (S+N+D+I).

Model

Generated Descriptions

Reference (cid:104)subject(cid:105) (born 13 April 1978) is a Spanish re-

tired professional footballer.

Only S

(cid:104)subject(cid:105) (born 25 January 1949) is a Georgian
football coach and former professional player.

S+N+D+I (cid:104)subject(cid:105) (born 22 April 1967) is an English

former football player.

genres is most appropriate for the given title. We
consider 30 movies each as reference titles, fake ti-
tles generated from only ratings as conditional data,
and fake titles conditioned on all the information.
Further, each question was asked for 3 participants,
and the results computed over the majority choice
are shown in Table 6. Fake titles generated with
all the information are more similar to reference
movie titles, demonstrating that the embeddings
that have access to more information effectively
generate higher-quality titles.

YAGO Descriptions The goal here is to generate
descriptive text for entities from their embeddings.
Since the original descriptions can be quite long,
we consider ﬁrst sentences that are less than 30 to-
kens, resulting in 96, 405 sentences. We randomly
consider 3000 of them as test, 3000 as validation,
and the remaining as training data for the decoder.
To evaluate the quality of the generated descrip-
tions, and whether they are appropriate for the en-
tity, we conduct a user study asking participants
if they can guess the realness of sentences and the
occupation (entertainer, sportsman, or politician),
gender, and age (above or below 35) of the subject
entity from the description. We provide 30 exam-
ples for each model asking each question from 3
participants and calculate the accuracy of the ma-
jority vote. The results presented in Table 7 show
that the models are fairly competent in informing
the users of the entity information, and further,

descriptions generated from embeddings that had
access to more information outperforms the model
with only structured data. Examples of generated
descriptions are provided in Table 8 (in addition
to screenshots of user study, more examples of
generated descriptions, and MovieLens titles are
provided in supplementary materials).

YAGO Images Here, we evaluate the quality of im-
ages generated from entity embeddings by humans
(31, 520, split into train/text). Similar to descrip-
tions, we conduct a study asking users to guess the
realness of images and the occupation, gender, and
age of the subject. We provide 30 examples for
each model asking each question from 3 partici-
pants, and use the majority choice.

The results in Table 7 indicate that the images
generated with embeddings based on all the infor-
mation are more accurate for gender and occupa-
tion. Guessing age from the images is difﬁcult
since the image on DBpedia may not correspond
to the age of the person, i.e. some of the older
celebrities had photos from their youth. Examples
of generated images are shown in Table 9.

6 Discussion and Limitations

An important concern regarding KB embedding
approaches is their scalability. While large KBs are
a problem for all embedding-based link prediction
techniques, MKBE is not signiﬁcantly worse than
existing ones because we treat multimodal infor-
mation as additional triples. Speciﬁcally, although
multimodal encoders/decoders are more expensive
to train than existing relational models, the cost is
still additive as we are effectively increasing the
size of the training dataset.

In addition to scalability, there are few other chal-
lenges when working with multimodal attributes.
Although multimodal evidence provides more in-
formation, it is not at all obvious which parts of this
additional data are informative for predicting the
relational structure of the KB, and the models are
prone to overﬁtting. MKBE builds upon the design
of neural encoders and decoders that have been
effective for speciﬁc modalities, and the results
demonstrate that it is able to utilize the information
effectively. However, there is still a need to further
study models that capture multimodal attributes in
a more efﬁcient and accurate manner.

Since our imputing multimodal attributes model
is based on GAN structure and the embeddings
learned from KB representation, the generated at-

tributes are directly limited by the power of GAN
models and the amount of information in the em-
bedding vectors. Although our generated attributes
convey several aspects of corresponding entities,
their quality is far from ideal due to the size of our
datasets (both of our image and text datasets are or-
der of magnitude smaller than common datasets in
the existing text/image genration literature) and the
amount of information captured by embedding vec-
tors (the knowledge graphs are sparse). In future,
we would like to (1) expand multimodal datasets
to have more attributes (use many more entities
from YAGO), and (2) instead of using learned em-
beddings to generate missing attributes, utilize the
knowledge graph directly for generation.

7 Conclusion

Motivated by the need to utilize multiple sources
of information, such as text and images, to achieve
more accurate link prediction, we present a novel
neural approach to multimodal relational learning.
We introduce MKBE, a link prediction model that
consists of (1) a compositional encoding compo-
nent to jointly learn the entity and multimodal em-
beddings to encode the information available for
each entity, and (2) adversarially trained decoding
component that use these entity embeddings to im-
pute missing multimodal values. We enrich two
existing datasets, YAGO-10 and MovieLens-100k,
with multimodal information to introduce bench-
marks. We show that MKBE, in comparison to
existing link predictors DistMult and ConvE, can
achieve higher accuracy on link prediction by utiliz-
ing the multimodal evidence. Further, we show that
MKBE effectively incorporates relational informa-
tion to generate high-quality multimodal attributes
like images and text. We have release the datasets
and the open-source implementation of our models
at https://github.com/pouyapez/mkbe.

Acknowledgements

We would like to thank Zhengli Zhao, Robert L.
Logan IV, Dheeru Dua, Casey Graff, and the anony-
mous reviewers for their detailed feedback and sug-
gestions. This work is supported in part by Allen
Institute for Artiﬁcial Intelligence (AI2) and in part
by NSF award #IIS-1817183. The views expressed
are those of the authors and do not reﬂect the ofﬁ-
cial policy or position of the funding agencies.

References

David Berthelot, Tom Schumm, and Luke Metz. 2017.
Began: Boundary equilibrium generative adversarial
networks. arXiv preprint arXiv:1703.10717.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran,
Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Antoine Bordes,

Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured em-
beddings of knowledge bases. In Conference on Ar-
tiﬁcial Intelligence (AAAI).

Thomas N Kipf and Max Welling. 2016. Variational
graph auto-encoders. NIPS Workshop on Bayesian
Deep Learning (NIPS-16 BDL).

Günter Klambauer, Thomas Unterthiner, Andreas
Mayr, and Sepp Hochreiter. 2017. Self-normalizing
neural networks. Advances in Neural Information
Processing Systems.

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
Dimitris Kontokostas, Pablo N Mendes, Sebastian
Hellmann, Mohamed Morsey, Patrick Van Kleef,
Sören Auer, et al. 2015. Dbpedia–a large-scale, mul-
tilingual knowledge base extracted from wikipedia.
Semantic Web, 6(2):167–195.

Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,
and Sebastian Riedel. 2018.
Convolutional 2d
knowledge graph embeddings. Conference on Ar-
tiﬁcial Intelligence (AAAI).

Gil Levi and Tal Hassner. 2015. Age and gender clas-
siﬁcation using convolutional neural networks.
In
IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 34–42.

Cícero Nogueira Dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In COLING, pages 69–78.

Matthew Francis-Landau, Greg Durrett, and Dan Klein.
2016. Capturing semantic similarity for entity link-
ing with convolutional neural networks. Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL).

Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor
Darrell. 2016. Compact bilinear pooling. In IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 317–326.

Alberto Garcia-Duran and Mathias Niepert. 2017.
Kblrn: End-to-end learning of knowledge base repre-
sentations with latent, relational, and numerical fea-
tures. Conference on Uncertainty in Artiﬁcial Intel-
ligence (UAI).

Roger Guimerà, Alejandro Llorente, Esteban Moro,
and Marta Sales-Pardo. 2012. Predicting human
preferences using the block structure of complex so-
cial networks. PloS one, 7(9):e44620.

Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong
Yu, and Jun Wang. 2017. Long text generation via
adversarial training with leaked information. Con-
ference on Artiﬁcial Intelligence (AAAI).

F Maxwell Harper and Joseph A Konstan. 2016.
The movielens datasets: History and context.
ACM Transactions on Interactive Intelligent Systems
(TiiS), 5(4):19.

Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Image-to-image translation with con-
Efros. 2017.
ditional adversarial networks. IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 3128–3137.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu,
and Xuan Zhu. 2015. Learning entity and relation
embeddings for knowledge graph completion.
In
Conference on Artiﬁcial Intelligence (AAAI), pages
2181–2187.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In ACM Conference on Rec-
ommender Systems, pages 165–172. ACM.

Maximilian Nickel, Lorenzo Rosasco, Tomaso A Pog-
gio, et al. 2016. Holographic embeddings of knowl-
edge graphs. In Conference on Artiﬁcial Intelligence
(AAAI), pages 1955–1961.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
In International
learning on multi-relational data.
Conference on Machine Learning (ICML), pages
809–816.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
In International Confer-
learning for linked data.
ence on World Wide Web, pages 271–280. ACM.

Daniel Oñoro-Rubio, Mathias Niepert, Alberto García-
Durán, Roberto González-Sánchez, and Roberto J
López-Sastre. 2017. Representation learning for
visual-relational knowledge graphs. arXiv preprint
arXiv:1709.02314.

Sai Rajeswar, Sandeep Subramanian, Francis Dutil,
Christopher Pal, and Aaron Courville. 2017. Adver-
sarial generation of natural language. Workshop on
Representation Learning for NLP.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT), pages 74–84.

Tobias Weyand, Ilya Kostrikov, and James Philbin.
2016. Planet-photo geolocation with convolutional
neural networks. In European Conference on Com-
puter Vision, pages 37–55. Springer.

Ruobing Xie, Zhiyuan Liu, Tat-seng Chua, Huanbo
Image-embodied
Luan, and Maosong Sun. 2017.
International
knowledge representation learning.
Joint Conference on Artiﬁcial Intelligence (IJCAI).

Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan,
and Maosong Sun. 2016. Representation learning
of knowledge graphs with entity descriptions.
In
Conference on Artiﬁcial Intelligence (AAAI), pages
2659–2665.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding entities and
relations for learning and inference in knowledge
International Conference on Learning Rep-
bases.
resentations (ICLR).

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2016. Stacked attention networks
for image question answering. In IEEE Conference
on Computer Vision and Pattern Recognition, pages
21–29.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In Conference on Artiﬁcial In-
telligence (AAAI), pages 2852–2858.

Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan-
der M Rush, Yann LeCun, et al. 2017. Adver-
arXiv preprint
sarially regularized autoencoders.
arXiv:1706.04223.

Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan,
and Zheng Chen. 2015. Aligning knowledge and
In Em-
text embeddings by entity descriptions.
pirical Methods in Natural Language Processing
(EMNLP), pages 267–272.

Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
Rianne van den Berg, Ivan Titov, and Max Welling.
2017. Modeling relational data with graph convo-
lutional networks. European Semantic Web Confer-
ence.

Hatem Mousselly Sergieh, Teresa Botschen,

Iryna
Gurevych, and Stefan Roth. 2018. A multimodal
translation-based approach for knowledge graph rep-
resentation learning. In Joint Conference on Lexical
and Computational Semantics, pages 225–234.

Karen Simonyan and Andrew Zisserman. 2015. Very
deep convolutional networks for large-scale image
recognition. International Conference on Learning
Representations (ICLR).

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
In
sor networks for knowledge base completion.
Advances in neural information processing systems,
pages 926–934.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
In International Conference on World Wide
edge.
Web, pages 697–706. ACM.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of
text and knowledge bases. In Empirical Methods in
Natural Language Processing (EMNLP), volume 15,
pages 1499–1509.

Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoi-
fung Poon, and Chris Quirk. 2016. Compositional
learning of embeddings for relation paths in knowl-
In Association for Computa-
edge base and text.
tional Linguistics (ACL).

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Com-
In In-
plex embeddings for simple link prediction.
ternational Conference on Machine Learning, pages
2071–2080.

Cunchao Tu, Han Liu, Zhiyuan Liu, and Maosong Sun.
2017. Cane: Context-aware network embedding for
In Annual Meeting of the As-
relation modeling.
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1722–1731.

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multilin-
gual relation extraction using compositional univer-
sal schema. Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL).

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
In Conference on Artiﬁcial
lating on hyperplanes.
Intelligence (AAAI), pages 1112–1119.

