8
1
0
2
 
c
e
D
 
3
 
 
]

G
L
.
s
c
[
 
 
5
v
4
4
1
5
0
.
1
1
7
1
:
v
i
X
r
a

Preventing Fairness Gerrymandering:
Auditing and Learning for Subgroup Fairness

Michael Kearns1, Seth Neel1, Aaron Roth1 and Zhiwei Steven Wu2

1University of Pennsylvania
2Microsoft Research-New York City

December 4, 2018

Abstract

The most prevalent notions of fairness in machine learning are statistical deﬁnitions: they
ﬁx a small collection of high-level, pre-deﬁned groups (such as race or gender), and then ask
for approximate parity of some statistic of the classiﬁer (like positive classiﬁcation rate or false
positive rate) across these groups. Constraints of this form are susceptible to (intentional or
inadvertent) fairness gerrymandering, in which a classiﬁer appears to be fair on each individual
group, but badly violates the fairness constraint on one or more structured subgroups deﬁned
over the protected attributes (such as certain combinations of protected attribute values). We
propose instead to demand statistical notions of fairness across exponentially (or inﬁnitely)
many subgroups, deﬁned by a structured class of functions over the protected attributes. This
interpolates between statistical deﬁnitions of fairness, and recently proposed individual no-
tions of fairness, but it raises several computational challenges. It is no longer clear how to
even check or audit a ﬁxed classiﬁer to see if it satisﬁes such a strong deﬁnition of fairness.
We prove that the computational problem of auditing subgroup fairness for both equality of
false positive rates and statistical parity is equivalent to the problem of weak agnostic learn-
ing — which means it is computationally hard in the worst case, even for simple structured
subclasses. However, it also suggests that common heuristics for learning can be applied to
successfully solve the auditing problem in practice.

We then derive two algorithms that provably converge to the best fair distribution over
classiﬁers in a given class, given access to oracles which can optimally solve the agnostic learn-
ing problem. The algorithms are based on a formulation of subgroup fairness as a two-player
zero-sum game between a Learner (the primal player) and an Auditor (the dual player). Both
algorithms compute an equilibrium of this game. We obtain our ﬁrst algorithm by simulat-
ing play of the game by having Learner play an instance of the no-regret Follow the Perturbed
Leader algorithm, and having Auditor play best response. This algorithm provably converges
to an approximate Nash equilibrium (and thus to an approximately optimal subgroup-fair dis-
tribution over classiﬁers) in a polynomial number of steps. We obtain our second algorithm
by simulating play of the game by having both players play Fictitious Play, which enjoys only
provably asymptotic convergence, but has the merit of simplicity and faster per-step compu-
tation. We implement the Fictitious Play version using linear regression as a heuristic oracle,
and show that we can eﬀectively both audit and learn fair classiﬁers on real datasets.

1 Introduction

As machine learning is being deployed in increasingly consequential domains (including policing
[Rudin, 2013], criminal sentencing [Barry-Jester et al., 2015], and lending [Koren, 2016]), the
problem of ensuring that learned models are fair has become urgent.

Approaches to fairness in machine learning can coarsely be divided into two kinds: statistical
and individual notions of fairness. Statistical notions typically ﬁx a small number of protected
demographic groups G (such as racial groups), and then ask for (approximate) parity of some
statistical measure across all of these groups. One popular statistical measure asks for equal-
ity of false positive or negative rates across all groups in G (this is also sometimes referred to as
an equal opportunity constraint [Hardt et al., 2016]). Another asks for equality of classiﬁcation
rates (also known as statistical parity). These statistical notions of fairness are the kinds of fair-
ness deﬁnitions most common in the literature (see e.g. Kamiran and Calders [2012], Hajian and
Domingo-Ferrer [2013], Kleinberg et al. [2017], Hardt et al. [2016], Friedler et al. [2016], Zafar
et al. [2017], Chouldechova [2017]).

One main attraction of statistical deﬁnitions of fairness is that they can in principle be ob-
tained and checked without making any assumptions about the underlying population, and hence
lead to more immediately actionable algorithmic approaches. On the other hand, individual no-
tions of fairness ask for the algorithm to satisfy some guarantee which binds at the individual,
rather than group, level. This often has the semantics that “individuals who are similar” should
be treated “similarly” [Dwork et al., 2012], or “less qualiﬁed individuals should not be favored
over more qualiﬁed individuals” [Joseph et al., 2016]. Individual notions of fairness have attrac-
tively strong semantics, but their main drawback is that achieving them seemingly requires more
assumptions to be made about the setting under consideration.

The semantics of statistical notions of fairness would be signiﬁcantly stronger if they were
deﬁned over a large number of subgroups, thus permitting a rich middle ground between fairness
only for a small number of coarse pre-deﬁned groups, and the strong assumptions needed for
fairness at the individual level. Consider the kind of fairness gerrymandering that can occur when
we only look for unfairness over a small number of pre-deﬁned groups:

Example 1.1. Imagine a setting with two binary features, corresponding to race (say black and white)
and gender (say male and female), both of which are distributed independently and uniformly at random
in a population. Consider a classiﬁer that labels an example positive if and only if it corresponds to a
black man, or a white woman. Then the classiﬁer will appear to be equitable when one considers either
protected attribute alone, in the sense that it labels both men and women as positive 50% of the time, and
labels both black and white individuals as positive 50% of the time. But if one looks at any conjunction
of the two attributes (such as black women), then it is apparent that the classiﬁer maximally violates the
statistical parity fairness constraint. Similarly, if examples have a binary label that is also distributed
uniformly at random, and independently from the features, the classiﬁer will satisfy equal opportunity
fairness with respect to either protected attribute alone, even though it maximally violates it with respect
to conjunctions of two attributes.

We remark that the issue raised by this toy example is not merely hypothetical. In our exper-
iments in Section 5, we show that similar violations of fairness on subgroups of the pre-deﬁned
groups can result from the application of standard machine learning methods applied to real
datasets. To avoid such problems, we would like to be able to satisfy a fairness constraint not
just for the small number of protected groups deﬁned by single protected attributes, but for a

1

combinatorially large or even inﬁnite collection of structured subgroups deﬁnable over protected
attributes.

In this paper, we consider the problem of auditing binary classiﬁers for equal opportunity and
statistical parity, and the problem of learning classiﬁers subject to these constraints, when the
number of protected groups is large. There are exponentially many ways of carving up a popu-
lation into subgroups, and we cannot necessarily identify a small number of these a priori as the
only ones we need to be concerned about. At the same time, we cannot insist on any notion of sta-
tistical fairness for every subgroup of the population: for example, any imperfect classiﬁer could
be accused of being unfair to the subgroup of individuals deﬁned ex-post as the set of individuals
it misclassiﬁed. This simply corresponds to “overﬁtting” a fairness constraint. We note that the
individual fairness deﬁnition of Joseph et al. [2016] (when restricted to the binary classiﬁcation
setting) can be viewed as asking for equalized false positive rates across the singleton subgroups,
containing just one individual each1 — but naturally, in order to achieve this strong deﬁnition of
fairness, Joseph et al. [2016] have to make structural assumptions about the form of the ground
truth. It is, however, sensible to ask for fairness for large structured subsets of individuals: so
long as these subsets have a bounded VC dimension, the statistical problem of learning and au-
diting fair classiﬁers is easy, so long as the dataset is suﬃciently large. This can be viewed as an
interpolation between equal opportunity fairness and the individual “weakly meritocratic” fair-
ness deﬁnition from Joseph et al. [2016], that does not require making any assumptions about the
ground truth. Our investigation focuses on the computational challenges, both in theory and in
practice.

1.1 Our Results

Brieﬂy, our contributions are:

• Formalization of the problem of auditing and learning classiﬁers for fairness with respect to

rich classes of subgroups G.

• Results proving (under certain assumptions) the computational equivalence of auditing G
and (weak) agnostic learning of G. While these results imply theoretical intractability of au-
diting for some natural classes G, they also suggest that practical machine learning heuristics
can be applied to the auditing problem.

• Provably convergent algorithms for learning classiﬁers that are fair with respect to G, based
on a formulation as a two-player zero-sum game between a Learner (the primal player) and
an Auditor (the dual player). We provide two diﬀerent algorithms, both of which are based
on solving for the equilibrium of this game. The ﬁrst provably converges in a polynomial
number of steps and is based on simulation of the game dynamics when the Learner uses
Follow the Perturbed Leader and the Auditor uses best response; the second is only guaranteed
to converge asymptotically but is computationally simpler, and involves both players using
Fictitious Play.

• An implementation and empirical evaluation of the Fictitious Play algorithm demonstrating

its eﬀectiveness on a real dataset in which subgroup fairness is a concern.

1It also asks for equalized false negative rates, and that the false positive rate is smaller than the true positive rate.

Here, the randomness in the “rates” is taken entirely over the randomness of the classiﬁer.

2

In more detail, we start by studying the computational challenge of simply checking whether
a given classiﬁer satisﬁes equal opportunity and statistical parity. Doing this in time linear in the
number of protected groups is simple: for each protected group, we need only estimate a single
expectation. However, when there are many diﬀerent protected attributes which can be combined
to deﬁne the protected groups, their number is combinatorially large2.

We model the problem by specifying a class of functions G deﬁned over a set of d protected
attributes. G deﬁnes a set of protected subgroups. Each function g ∈ G corresponds to the pro-
tected subgroup {x : gi(x) = 1}3. The ﬁrst result of this paper is that for both equal opportunity and
statistical parity, the computational problem of checking whether a classiﬁer or decision-making
algorithm D violates statistical fairness with respect to the set of protected groups G is equivalent
to the problem of agnostically learning G [Kearns et al., 1994], in a strong and distribution-speciﬁc
sense. This equivalence has two implications:

1. First, it allows us to import computational hardness results from the learning theory litera-
ture. Agnostic learning turns out to be computationally hard in the worst case, even for
extremely simple classes of functions G (like boolean conjunctions and linear threshold
functions). As a result, we can conclude that auditing a classiﬁer D for statistical fairness
violations with respect to a class G is also computationally hard. This means we should not
expect to ﬁnd a polynomial time algorithm that is always guaranteed to solve the auditing
problem.

2. However, in practice, various learning heuristics (like boosting, logistic regression, SVMs,
backpropagation for neural networks, etc.) are commonly used to learn accurate classiﬁers
which are known to be hard to learn in the worst case. The equivalence we show between
agnostic learning and auditing is distribution speciﬁc — that is, if on a particular data set, a
heuristic learning algorithm can solve the agnostic learning problem (on an appropriately
deﬁned subset of the data), it can be used also to solve the auditing problem on the same
data set.

These results appear in Section 3.

Next, we consider the problem of learning a classiﬁer that equalizes false positive or negative
rates across all (possibly inﬁnitely many) sub-groups, deﬁned by a class of functions G. As per the
reductions described above, this problem is computationally hard in the worst case.

However, under the assumption that we have an eﬃcient oracles which solves the agnostic
learning problem, we give and analyze algorithms for this problem based on a game-theoretic
formulation. We ﬁrst prove that the optimal fair classiﬁer can be found as the equilibrium of
a two-player, zero-sum game, in which the (pure) strategy space of the “Learner” player corre-
sponds to classiﬁers in H, and the (pure) strategy space of the “Auditor” player corresponds to

2For example, as discussed in a recent Propublica investigation [Angwin and Grassegger, 2017], Facebook policy
protects groups against hate speech if the group is deﬁnable as a conjunction of protected attributes. Under the Face-
book schema, “race” and “gender” are both protected attributes, and so the Facebook policy protects “black women” as
a distinct class, separately from black people and women. When there are d protected attributes, there are 2d protected
groups. As a statistical estimation problem, this is not a large obstacle — we can estimate 2d expectations to error ε so
long as our data set has size O(d/ε2), but there is now a computational problem.

3For example, in the case of Facebook’s policy, the protected attributes include “race, sex, gender identity, religious
aﬃliation, national origin, ethnicity, sexual orientation and serious disability/disease” [Angwin and Grassegger, 2017],
and G represents the class of boolean conjunctions. In other words, a group deﬁned by individuals having any subset
of values for the protected attributes is protected.

3

subgroups deﬁned by G. The best response problems for the two players correspond to agnostic
learning and auditing, respectively. We show that both problems can be solved with a single call
to a cost sensitive classiﬁcation oracle, which is equivalent to an agnostic learning oracle. We then
draw on extant theory for learning in games and no-regret algorithms to derive two diﬀerent al-
gorithms based on simulating game play in this formulation. In the ﬁrst, the Learner employs the
well-studied Follow the Perturbed Leader (FTPL) algorithm on an appropriate linearization of its
best-response problem, while the Auditor approximately best-responds to the distribution over
classiﬁers of the Learner at each step. Since FTPL has a no-regret guarantee, we obtain an algo-
rithm that provably converges in a polynomial number of steps.

While it enjoys strong provable guarantees, this ﬁrst algorithm is randomized (due to the
noise added by FTPL), and the best-response step for the Auditor is polynomial time but compu-
tationally expensive. We thus propose a second algorithm that is deterministic, simpler and faster
per step, based on both players adopting the Fictitious Play learning dynamic. This algorithm has
weaker theoretical guarantees: it has provable convergence only asymptotically, and not in a poly-
nomial number of steps — but is more practical and converges rapidly in practice. The derivation
of these algorithms (and their guarantees) appear in Section 4.

Finally, we implement the Fictitious Play algorithm and demonstrate its practicality by eﬃ-
ciently learning classiﬁers that approximately equalize false positive rates across any group de-
ﬁnable by a linear threshold function on 18 protected attributes in the “Communities and Crime”
dataset. We use simple, fast regression algorithms as heuristics to implement agnostic learn-
ing oracles, and (via our reduction from agnostic learning to auditing) auditing oracles. Our
results suggest that it is possible in practice to learn fair classiﬁers with respect to a large class
of subgroups that still achieve non-trivial error. Full details are contained in Section 5, and for a
substantially more comprehensive empirical investigation of our method we direct the interested
reader to Kearns et al. [2018].

1.2 Further Related Work

Independent of our work, H´ebert-Johnson et al. [2017] also consider a related and complementary
notion of fairness that they call “multicalibration”. In settings in which one wishes to train a real-
valued predictor, multicalibration can be considered the “calibration” analogue for the deﬁnitions
of subgroup fairness that we give for false positive rates, false negative rates, and classiﬁcation
rates. For a real-valued predictor, calibration informally requires that for every value v ∈ [0, 1]
predicted by an algorithm, the fraction of individuals who truly have a positive label in the subset
of individuals on which the algorithm predicted v should be approximately equal to v. Multi-
calibration asks for approximate calibration on every set deﬁned implicitly by some circuit in a
set G. H´ebert-Johnson et al. [2017] give an algorithmic result that is analogous to the one we give
for learning subgroup fair classiﬁers: a polynomial time algorithm for learning a multi-calibrated
predictor, given an agnostic learning algorithm for G. In addition to giving a polynomial-time
algorithm, we also give a practical variant of our algorithm (which is however only guaranteed to
converge in the limit) that we use to conduct empirical experiments on real data.

Thematically, the most closely related piece of prior work is Zhang and Neill [2016], who
also aim to audit classiﬁcation algorithms for discrimination in subgroups that have not been
pre-deﬁned. Our work diﬀers from theirs in a number of important ways. First, we audit the
algorithm for common measures of statistical unfairness, whereas Zhang and Neill [2016] design
a new measure compatible with their particular algorithmic technique. Second, we give a for-

4

mal analysis of our algorithm. Finally, we audit with respect to subgroups deﬁned by a class of
functions G, which we can take to have bounded VC dimension, which allows us to give formal
out-of-sample guarantees. Zhang and Neill [2016] attempt to audit with respect to all possible
sub-groups, which introduces a severe multiple-hypothesis testing problem, and risks overﬁtting.
Most importantly we give actionable algorithms for learning subgroup fair classiﬁers, whereas
Zhang and Neill [2016] restrict attention to auditing.

Technically, the most closely related piece of work (and from which we take inspiration for
our algorithm in Section 4) is Agarwal et al. [2017], who show that given access to an agnostic
learning oracle for a class H, there is an eﬃcient algorithm to ﬁnd the lowest-error distribution
over classiﬁers in H subject to equalizing false positive rates across polynomially many subgroups.
Their algorithm can be viewed as solving the same zero-sum game that we solve, but in which the
“subgroup” player plays gradient descent over his pure strategies, one for each sub-group. This
ceases to be an eﬃcient or practical algorithm when the number of subgroups is large, as is our
case. Our main insight is that an agnostic learning oracle is suﬃcient to have the both players
play “ﬁctitious play”, and that there is a transformation of the best response problem such that an
agnostic learning algorithm is enough to eﬃciently implement follow the perturbed leader.

There is also other work showing computational hardness for fair learning problems. Most no-
tably, Woodworth et al. [2017] show that ﬁnding a linear threshold classiﬁer that approximately
minimizes hinge loss subject to equalizing false positive rates across populations is computation-
ally hard (assuming that refuting a random k-XOR formula is hard). In contrast, we show that
even checking whether a classiﬁer satisﬁes a false positive rate constraint on a particular data set
is computationally hard (if the number of subgroups on which fairness is desired is too large to
enumerate).

2 Model and Preliminaries

(cid:48)), y), where x ∈ X denotes a vector of
We model each individual as being described by a tuple ((x, x
(cid:48) ∈ X (cid:48) denotes a vector of unprotected attributes, and y ∈ {0, 1} denotes a label.
protected attributes, x
Note that in our formulation, an auditing algorithm not only may not see the unprotected at-
(cid:48) may represent proprietary
tributes x
features or consumer data purchased by a credit scoring company.

(cid:48), it may not even be aware of their existence. For example, x

We will write X = (x, x

(cid:48)) to denote the joint feature vector. We assume that points (X, y) are
drawn i.i.d. from an unknown distribution P . Let D be a decision making algorithm, and let D(X)
denote the (possibly randomized) decision induced by D on individual (X, y). We restrict attention
in this paper to the case in which D makes a binary classiﬁcation decision: D(X) ∈ {0, 1}. Thus we
alternately refer to D as a classiﬁer. When auditing a ﬁxed classiﬁer D, it will be helpful to make
reference to the distribution over examples (X, y) together with their induced classiﬁcation D(X).
Let Paudit(D) denote the induced target joint distribution over the tuple (x, y, D(X)) that results from
(cid:48)) but
sampling (x, x
(cid:48). Note that the randomness here is over both the randomness of
not the unprotected attributes x
P , and the potential randomness of the classiﬁer D.

, y) ∼ P , and providing x, the true label y, and the classiﬁcation D(X) = D(x, x

(cid:48)

We will be concerned with learning and auditing classiﬁers D satisfying two common statis-
tical fairness constraints: equality of classiﬁcation rates (also known as statistical parity), and
equality of false positive rates (also known as equal opportunity). Auditing for equality of false
negative rates is symmetric and so we do not explicitly consider it. Each fairness constraint is

5

deﬁned with respect to a set of protected groups. We deﬁne sets of protected groups via a family
of indicator functions G for those groups, deﬁned over protected attributes. Each g : X → {0, 1} ∈ G
has the semantics that g(x) = 1 indicates that an individual with protected features x is in group
g.

Deﬁnition 2.1 (Statistical Parity (SP) Subgroup Fairness). Fix any classiﬁer D, distribution P , col-
lection of group indicators G, and parameter γ ∈ [0, 1]. For each g ∈ G, deﬁne

αSP (g, P ) = Pr
P

[g(x) = 1]

and, βSP (g, D, P ) = |SP(D) − SP(D, g)| ,

where SP(D) = PrP ,D[D(X) = 1] and SP(D, g) = PrP ,D[D(X) = 1|g(x) = 1] denote the overall acceptance
rate of D and the acceptance rate of D on group g respectively. We say that D satisﬁes γ-statistical
parity (SP) Fairness with respect to P and G if for every g ∈ G

We will sometimes refer to SP(D) as the SP base rate.

αSP (g, P ) βSP (g, D, P ) ≤ γ.

Remark 2.2. Note that our deﬁnition references two approximation parameters, both of which are im-
portant. We are allowed to ignore a group g if it (or its complement) represent only a small fraction
of the total probability mass. The parameter α governs how small a fraction of the population we are
allowed to ignore. Similarly, we do not require that the probability of a positive classiﬁcation in every
subgroup is exactly equal to the base rate, but instead allow deviations up to β. Both of these approxi-
mation parameters are necessary from a statistical estimation perspective. We control both of them with
a single parameter γ.

Deﬁnition 2.3 (False Positive (FP) Subgroup Fairness). Fix any classiﬁer D, distribution P , collection
of group indicators G, and parameter γ ∈ [0, 1]. For each g ∈ G, deﬁne

αFP (g, P ) = Pr
P

[g(x) = 1, y = 0]

and, βFP (g, D, P ) = |FP(D) − FP(D, g)|

where FP(D) = PrD,P [D(X) = 1 | y = 0] and FP(D, g) = PrD,P [D(X) = 1 | g(x) = 1, y = 0] denote the
overall false-positive rate of D and the false-positive rate of D on group g respectively.

We say D satisﬁes γ-False Positive (FP) Fairness with respect to P and G if for every g ∈ G

We will sometimes refer to FP(D) FP-base rate.

αFP (g, P ) βFP (g, D, P ) ≤ γ.

Remark 2.4. This deﬁnition is symmetric to the deﬁnition of statistical parity fairness, except that the
parameter α is now used to exclude any group g such that negative examples (y = 0) from g (or its
complement) have probability mass less than α. This is again necessary from a statistical estimation
perspective.

For either statistical parity and false positive fairness, if the algorithm D fails to satisfy the
γ-fairness condition, then we say that D is γ-unfair with respect to P and G. We call any subgroup
g which witnesses this unfairness an γ-unfair certiﬁcate for (D, P ).

An auditing algorithm for a notion of fairness is given sample access to Paudit(D) for some clas-
siﬁer D. It will either deem D to be fair with respect to P , or will else produce a certiﬁcate of
unfairness.

6

Deﬁnition 2.5 (Auditing Algorithm). Fix a notion of fairness (either statistical parity or false positive
(cid:48) ∈ (0, 1) such that
fairness), a collection of group indicators G over the protected features, and any δ, γ, γ
(cid:48))-auditing algorithm for G with respect to distribution P is an algorithm A such that
(cid:48) ≤ γ. A (γ, γ
γ
for any classiﬁer D, when given access the distribution Paudit(D), A runs in time poly(1/γ
, log(1/δ)),
and with probability 1 − δ, outputs a γ
-unfair certiﬁcate for D whenever D is γ-unfair with respect to
(cid:48)
P and G. If D is γ

-fair, A will output “fair”.

(cid:48)

(cid:48)

As we will show, our deﬁnition of auditing is closely related to weak agnostic learning.

Deﬁnition 2.6 (Weak Agnostic Learning [Kearns et al., 1994, Kalai et al., 2008]). Let Q be a dis-
(cid:48) ∈ (0, 1/2) such that ε ≥ ε
tribution over X × {0, 1} and let ε, ε
. We say that the function class G is
(cid:48))-weakly agnostically learnable under distribution Q if there exists an algorithm L such that
(ε, ε
, 1/δ), and with probability 1 − δ, outputs a
when given sample access to Q, L runs in time poly(1/ε
hypothesis h ∈ G such that

(cid:48)

(cid:48)

err(f , Q) ≤ 1/2 − ε =⇒ err(h, Q) ≤ 1/2 − ε

(cid:48)

.

min
f ∈G

where err(h, Q) = Pr(x,y)∼Q[h(x) (cid:44) y].

Cost-Sensitive Classiﬁcation.
In this paper, we will also give reductions to cost-sensitive classi-
ﬁcation (CSC) problems. Formally, an instance of a CSC problem for the class H is given by a set
of n tuples {(Xi, c0
i corresponds to the cost for predicting label (cid:96) on point Xi.
Given such an instance as input, a CSC oracle ﬁnds a hypothesis ˆh ∈ H that minimizes the total
cost across all points:

i=1 such that c(cid:96)

i , c1

i )}n

ˆh ∈ argmin

h∈H

n(cid:88)

i=1

[h(Xi)c1

i + (1 − h(Xi))c0
i ]

(1)

A crucial property of a CSC problem is that the solution is invariant to translations of the costs.

Claim 2.7. Let {(Xi, c0
a1, a2, . . . , an

i )}n
∈ R such that ˜c(cid:96)

i , c1

i=1 be a CSC instance, and {( ˜c0
i = c(cid:96)
i + ai for all i and (cid:96). Then

i , ˜c1

i )} be a set of new costs such that there exist

argmin
h∈H

n(cid:88)

i=1

[h(Xi)c1

i + (1 − h(Xi))c0

i ] = argmin

[h(Xi) ˜c1

i + (1 − h(Xi)) ˜c0
i ]

n(cid:88)

i=1

h∈H

Remark 2.8. We note that cost-sensitive classiﬁcation is polynomially equivalent to agnostic learn-
ing Zadrozny et al. [2003]. We give both deﬁnitions above because when describing our results for
auditing, we wish to directly appeal to known hardness results for weak agnostic learning, but it is more
convenient to describe our algorithms via oracles for cost-sensitive classiﬁcation.

Follow the Perturbed Leader. We will make use of the Follow the Perturbed Leader (FTPL) algo-
rithm as a no-regret learner for online linear optimization problems [Kalai and Vempala, 2005].
To formalize the algorithm, consider S ⊂ {0, 1}d to be a set of “actions” for a learner in an on-
line decision problem. The learner interacts with an adversary over T rounds, and in each round
t, the learner (randomly) chooses some action at ∈ S, and the adversary chooses a loss vector
(cid:96)t ∈ [−M, M]d. The learner incurs a loss of (cid:104)(cid:96)t, at(cid:105) at round t.

7

FTPL is a simple algorithm that in each round perturbs the cumulative loss vector over the pre-
s<t (cid:96)s, and chooses the action that minimizes loss with respect to the perturbed
vious rounds (cid:96) =
cumulative loss vector. We present the full algorithm in Algorithm 1, and its formal guarantee in
Theorem 2.9.

(cid:80)

U be the uniform distribution over [0, 1]d, and let a1 ∈ S be

Algorithm 1 Follow the Perturbed Leader (FTPL) Algorithm

Input: Loss bound M, action set S ∈ {0, 1}d
Initialize: Let η = (1/M)

, D

(cid:113)

1√

dT

arbitrary.
For t = 1, . . . , T :

Play action at; Observe loss vector (cid:96)t and suﬀer loss (cid:104)(cid:96)t, at(cid:105).
Update:

at+1 = argmin

η

(cid:104)(cid:96)s, a(cid:105) + (cid:104)ξt, a(cid:105)





(cid:88)

s≤t

a∈S





where ξt is drawn independently for each t from the distribution D

U .

Theorem 2.9 (Kalai and Vempala [2005]). For any sequence of loss vectors (cid:96)1, . . . , (cid:96)T , the FTPL algo-
rithm has regret





E

T(cid:88)



(cid:104)(cid:96)t, at(cid:105)

− min
a∈S

T(cid:88)

(cid:104)(cid:96)t, a(cid:105) ≤ 2d5/4M

T

√

t=1
where the randomness is taken over the perturbations ξt across rounds.

t=1

2.1 Generalization Error

In this section, we observe that the error rate of a classiﬁer D, as well as the degree to which it vio-
lates γ-fairness (for both statistical parity and false positive rates) can be accurately approximated
with the empirical estimates for these quantities on a dataset (drawn i.i.d. from the underlying
distribution P ) so long as the dataset is suﬃciently large. Once we establish this fact, since our
main interest is in the computational problem of auditing and learning, in the rest of the paper,
we assume that we have direct access to the underlying distribution (or equivalently, that the
empirical data deﬁnes the distribution of interest), and do not make further reference to sample
complexity or overﬁtting issues.

A standard VC dimension bound (see, e.g. Kearns and Vazirani [1994]) states:

Theorem 2.10. Fix a class of functions H. For any distribution P , let S ∼ P m be a dataset consisting
of m examples (Xi, yi) sampled i.i.d. from P . Then for any 0 < δ < 1, with probability 1 − δ, for every
h ∈ H, we have:

|err(h, P ) − err(h, S)| ≤ O

(cid:114)





VCDIM(H) log m + log(1/δ)
m





where err(h, S) = 1
m

(cid:80)m

i=1

1[h(Xi) (cid:44) yi].

8

The above theorem implies that so long as m ≥ ˜O(VCDIM(H)/ε2), then minimizing error over
the empirical sample S suﬃces to minimize error up to an additive ε term on the true distribution
P . Below, we give two analogous statements for fairness constraints:

Theorem 2.11 (SP Uniform Convergence). Fix a class of functions H and a class of group indicators
G. For any distribution P , let S ∼ P m be a dataset consisting of m examples (Xi, yi) sampled i.i.d. from
P . Then for any 0 < δ < 1, with probability 1 − δ, for every h ∈ H and g ∈ G

(cid:12)(cid:12)(cid:12)αSP (g, P

S) βSP (g, h, P

S) − αSP (g, P ) βSP (g, h, P )

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

where P

S denotes the empirical distribution over the realized sample S.

Similarly:

Theorem 2.12 (FP Uniform Convergence). Fix a class of functions H and a class of group indicators
G. For any distribution P , let S ∼ P m be a dataset consisting of m examples (Xi, yi) sampled i.i.d. from
P . Then for any 0 < δ < 1, with probability 1 − δ, for every h ∈ H and g ∈ G, we have:

(cid:12)(cid:12)(cid:12)αFP (g, P ) βFP (g, D, P ) − αFP (g, P ) βFP (g, D, P )

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

where P

S denotes the empirical distribution over the realized sample S.

These theorems together imply that for both SP and FP subgroup fairness, the degree to which
a group g violates the constraint of γ-fairness can be estimated up to error ε, so long as m ≥
˜O((VCDIM(H) + VCDIM(G))/ε2). The proofs can be found in Appendix B.









3 Equivalence of Auditing and Weak Agnostic Learning

In this section, we give a reduction from the problem of auditing both statistical parity and false
positive rate fairness, to the problem of agnostic learning, and vice versa. This has two implica-
tions. The main implication is that, from a worst-case analysis point of view, auditing is compu-
tationally hard in almost every case (since it inherits this pessimistic state of aﬀairs from agnostic
learning). However, worst-case hardness results in learning theory have not prevented the suc-
cessful practice of machine learning, and there are many heuristic algorithms that in real-world
cases successfully solve “hard” agnostic learning problems. Our reductions also imply that these
heuristics can be used successfully as auditing algorithms, and we exploit this in the development
of our algorithmic results and their experimental evaluation.

We make the following mild assumption on the class of group indicators G, to aid in our reduc-
tions. It is satisﬁed by most natural classes of functions, but is in any case essentially without loss
of generality (since learning negated functions can be simulated by learning the original function
class on a dataset with ﬂipped class labels).

Assumption 3.1. We assume the set of group indicators G satisﬁes closure under negation: for any
g ∈ G, we also have ¬g ∈ G.

Recalling that X = (x, x

(cid:48)) and the following notions will be useful for describing our results:

9

• SP(D) = PrP ,D[D(X) = 1] and FP(D) = PrD,P [D(X) = 1 | y = 0].
• αSP (g, P ) = PrP [g(x) = 1] and αFP (g, P ) = PrP [g(x) = 1, y = 0].
• βSP (g, D, P ) = |SP(D) − SP(D, g)| and βFP (g, D, P ) = |FP(D) − FP(D, g)|.

• P D: the marginal distribution on (x, D(X)).

• P D

y=0: the conditional distribution on (x, D(X)), conditioned on y = 0.

We will think about these as the target distributions for a learning problem: i.e. the problem of
learning to predict D(X) from only the protected features x. We will relate the ability to agnosti-
cally learn on these distributions, to the ability to audit D given access to the original distribution
Paudit(D).

3.1 Statistical Parity Fairness

We give our reduction ﬁrst for SP subgroup fairness. The reduction for FP subgroup fairness
will follow as a corollary, since auditing for FP subgroup fairness can be viewed as auditing for
statistical parity fairness on the subset of the data restricted to y = 0.

Theorem 3.2. Fix any distribution P , and any set of group indicators G. Then for any γ, ε > 0, the
following relationships hold:

• If there is a (γ/2, (γ/2 − ε)) auditing algorithm for G for all D such that SP(D) = 1/2, then the

class G is (γ, γ/2 − ε)-weakly agnostically learnable under P D.

• If G is (γ, γ − ε)-weakly agnostically learnable under distribution P D for all D such that SP(D) =

1/2, then there is a (γ, (γ − ε)/2) auditing algorithm for G for SP fairness under P .

We will prove Theorem 3.2 in two steps. First, we show that any unfair certiﬁcate f for D has

non-trivial error for predicting the decision made by D from the sensitive attributes.

Lemma 3.3. Suppose that the base rate SP(D) ≤ 1/2 and there exists a function f such that

αSP (g, P ) βSP (g, D, P ) = γ.

max{Pr[D(X) = f (x)], Pr[D(X) = ¬f (x)]} ≥ SP(D) + γ.

Proof. To simplify notations, let b = SP(D) denote the base rate, α = αSP and β = βSP . First, observe
that either Pr[D(X) = 1 | f (x) = 1] = b + β or Pr[D(X) = 1 | f (x) = 1] = b − β holds.

In the ﬁrst case, we know Pr[D(X) = 1 | f (x) = 0] < b, and so Pr[D(X) = 0 | f (x) = 0] > 1 − b. It

Then

follows that

Pr[D(X) = f (x)] = Pr[D(X) = f (x) = 1] + Pr[D(X) = f (x) = 0]

= Pr[D(X) = 1 | f (x) = 1] Pr[f (x) = 1] + Pr[D(X) = 0 | f (x) = 0] Pr[f (x) = 0]
> α(b + β) + (1 − α)(1 − b)
= (α − 1)b + (1 − α)(1 − b) + b + αβ
= (1 − α)(1 − 2b) + b + αβ.

10

In the second case, we have Pr[D(X) = 0 | f (x) = 1] = (1 − b) + β and Pr[D(X) = 1 | f (x) = 0] > b. We
can then bound

Pr[D(X) = f (x)] = Pr[D(X) = 1 | f (x) = 0] Pr[f (x) = 0] + Pr[D(X) = 0 | f (x) = 1] Pr[f (x) = 1]

> (1 − α)b + α(1 − b + β) = α(1 − 2b) + b + αβ.

In both cases, we have (1 − 2b) ≥ 0 by our assumption on the base rate. Since α ∈ [0, 1], we know
max{Pr[D(X) = f (x)], Pr[D(X) = ¬f (x)]} ≥ b + αβ = b + γ

which recovers our bound.

In the next step, we show that if there exists any function f that accurately predicts the de-
cisions made by the algorithm D, then either f or ¬f can serve as an unfairness certiﬁcate for
D.
Lemma 3.4. Suppose that the base rate SP(D) ≥ 1/2 and there exists a function f such that Pr[D(X) =
f (x)] ≥ SP(D) + γ for some value γ ∈ (0, 1/2). Then there exists a function g such that

αSP (g, P ) βSP (g, D, P ) ≥ γ/2,

where g ∈ {f , ¬f }.

Proof. Let b = SP(D). We can expand Pr[D(X) = f (x)] as follows:

Pr[D(X) = f (x)] = Pr[D(X) = f (x) = 1] + Pr[D(X) = f (x) = 0]

= Pr[D(X) = 1 | f (x) = 1] Pr[f (x) = 1] + Pr[D(X) = 0 | f (x) = 0] Pr[f (x) = 0]

This means

Pr[D(X) = f (x)] − b

= (Pr[D(X) = 1 | f (x) = 1] − b) Pr[f (x) = 1] + (Pr[D(X) = 0 | f (x) = 0] − b) Pr[f (x) = 0] ≥ γ
Suppose that (Pr[D(X) = 1 | f (x) = 1] − b) Pr[f (x) = 1] ≥ γ/2, then our claim holds with g = f . Sup-
pose not, then we must have

(Pr[D(X) = 0 | f (x) = 0] − b) Pr[f (x) = 0] = ((1 − b) − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ γ/2

Note that by our assumption b ≥ (1 − b). This means

(b − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ ((1 − b) − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ γ/2

which implies that our claim holds with g = ¬f .

Proof of Theorem 3.2. Suppose that the class G satisﬁes minf ∈G err(f , P D) ≤ 1/2−γ. Then by Lemma 3.4,
there exists some g ∈ G such that Pr[g(x) = 1]| Pr[D(X) = 1 | g(x) = 1] − SP(D)| ≥ γ/2. By the as-
(cid:48) ∈ G that is an
sumption of auditability, we can then use the auditing algorithm to ﬁnd a group g
(cid:48) predicts D with an
(γ/2 − ε)-unfair certiﬁcate of D. By Lemma 3.3, we know that either g
accuracy of at least 1/2 + (γ/2 − ε).

(cid:48) or ¬g

In the reverse direction, consider the auditing problem on the classiﬁer D. We can treat each
pair (x, D(X)) as a labelled example and learn a hypothesis in G that approximates the decisions
made by D. Suppose that D is γ-unfair. Then by Lemma 3.3, we know that there exists some g ∈ G
such that Pr[D(X) = g(x)] ≥ 1/2 + γ. Therefore, the weak agnostic learning algorithm from the
(cid:48)(x)] ≥ 1/2 + (γ − ε). By Lemma 3.4,
hypothesis of the theorem will return some g
we know g

(cid:48) is a (γ − ε)/2-unfair certiﬁcate for D.

(cid:48) with Pr[D(X) = g

(cid:48) or ¬g

11

3.2 False Positive Fairness

A corollary of the above reduction is an analogous equivalence between auditing for FP subgroup
fairness and agnostic learning. This is because a FP fairness constraint can be viewed as a statis-
tical parity fairness constraint on the subset of the data such that y = 0. Therefore, Theorem 3.2
implies the following:

Corollary 3.5. Fix any distribution P , and any set of group indicators G. The following two relation-
ships hold:

• If there is a (γ/2, (γ/2 − ε)) auditing algorithm for G for all D such that FP(D) = 1/2, then the

class G is (γ, γ/2 − ε)-weakly agnostically learnable under P D

y=0.

• If G is (γ, γ −ε)–weakly agnostically learnable under distribution P D

y=0 for all D such that FP(D) =
1/2, then there is a (γ, (γ − ε)/2) auditing algorithm for FP subgroup fairness for G under distri-
bution P .

3.3 Worst-Case Intractability of Auditing

While we shall see in subsequent sections that the equivalence given above has positive algo-
rithmic and experimental consequences, from a purely theoretical perspective the reduction of
agnostic learning to auditing has strong negative worst-case implications. More precisely, we can
import a long sequence of formal intractability results for agnostic learning to obtain:

Theorem 3.6. Under standard complexity-theoretic intractability assumptions, for G the classes of con-
junctions of boolean attributes, linear threshold functions, or bounded-degree polynomial threshold func-
tions, there exist distributions P such that the auditing problem cannot be solved in polynomial time, for
either statistical parity or false positive fairness.

The proof of this theorem follows from Theorem 3.2, Corollary 3.5, and the following negative
results from the learning theory literature. Feldman et al. [2012] show a strong negative result
for weak agnostic learning for conjunctions: given a distribution on labeled examples from the
hypercube such that there exists a monomial (or conjunction) consistent with (1−ε)-fraction of the
examples, it is NP-hard to ﬁnd a halfspace that is correct on (1/2 + ε)-fraction of the examples, for
arbitrary constant ε > 0. Diakonikolas et al. [2011] show that under the Unique Games Conjecture,
no polynomial-time algorithm can ﬁnd a degree-d polynomial threshold function (PTF) that is
consistent with (1/2 + ε) fraction of a given set of labeled examples, even if there exists a degree-d
PTF that is consistent with a (1 − ε) fraction of the examples. Diakonikolas et al. [2011] also show
that it is NP-Hard to ﬁnd a degree-2 PTF that is consistent with a (1/2 + ε) fraction of a given set
of labeled examples, even if there exists a halfspace (degree-1 PTF) that is consistent with a (1 − ε)
fraction of the examples.

While Theorem 3.6 shows that certain natural subgroup classes G yield intractable auditing
problems in the worst case, in the rest of the paper we demonstrate that eﬀective heuristics for
this problem on speciﬁc (non-worst case) distributions can be used to derive an eﬀective and
practical learning algorithm for subgroup fairness.

12

4 A Learning Algorithm Subject to Fairness Constraints G

In this section, we present an algorithm for training a (randomized) classiﬁer that satisﬁes false-
positive subgroup fairness simultaneously for all protected subgroups speciﬁed by a family of
group indicator functions G. All of our techniques also apply to a statistical parity or false negative
rate constraint.

Let S denote a set of n labeled examples {zi = (xi, x

i=1, and let P denote the empirical
distribution over this set of examples. Let H be a hypothesis class deﬁned over both the protected
and unprotected attributes, and let G be a collection of group indicators over the protected at-
tributes. We assume that H contains a constant classiﬁer (which implies that there is at least one
fair classiﬁer to be found, for any distribution).

i), yi)}n

Our goal will be to ﬁnd the distribution over classiﬁers from H that minimizes classiﬁcation
error subject to the fairness constraint over G. We will design an iterative algorithm that, when
given access to a CSC oracle, computes an optimal randomized classiﬁer in polynomial time.

(cid:48)

Let D denote a probability distribution over H. Consider the following Fair ERM (Empirical

Risk Minimization) problem:

such that ∀g ∈ G

E
h∼D

[err(h, P )]

min
D∈∆H
αFP (g, P ) βFP (g, D, P ) ≤ γ.

(2)

(3)

(cid:48)) (cid:44) y], and the quantities αFP and βFP are deﬁned in Deﬁnition 2.3.
where err(h, P ) = PrP [h(x, x
We will write OPT to denote the objective value at the optimum for the Fair ERM problem, that is
the minimum error achieved by a γ-fair distribution over the class H.

Observe that the optimization is feasible for any distribution P : the constant classiﬁers that
labels all points 1 or 0 satisfy all subgroup fairness constraints. At the moment, the number of
decision variables and constraints may be inﬁnite (if H and G are inﬁnite hypothesis classes), but
we will address this momentarily.

Assumption 4.1 (Cost-Sensitive Classiﬁcation Oracle). We assume our algorithm has access to the
cost-sensitive classication oracles CSC(H) and CSC(G) over the classes H and G.

Our main theoretical result is an computationally eﬃcient oracle-based algorithm for solving

the Fair ERM problem.

Theorem 4.2. Fix any ν, δ ∈ (0, 1). Then given an input of n data points and accuracy parameters ν, δ
and access to oracles CSC(H) and CSC(G), there exists an algorithm runs in polynomial time, and with
probability at least 1 − δ, output a randomized classiﬁer ˆD such that err( ˆD, P ) ≤ OPT +ν, and for any
g ∈ G, the fairness constraint violations satisﬁes

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ + O(ν).

Overview of our solution. We present our solution in steps:

• Step 1: Fair ERM as LP. First, we rewrite the Fair ERM problem as a linear program with
ﬁnitely many decision variables and constraints even when H and G are inﬁnite. To do this,
we take advantage of the fact that Sauer’s Lemma lets us bound the number of labellings
that any hypothesis class H of bounded VC dimension can induce on any ﬁxed dataset.

13

The LP has one variable for each of these possible labellings, rather than one variable for
each hypothesis. Moreover, again by Sauer’s Lemma, we have one constraint for each of the
ﬁnitely many possible subgroups induced by G on the ﬁxed dataset, rather than one for each
of the (possibly inﬁnitely many) subgroups deﬁnable over arbitrary datasets. This step is
important — it will guarantee that strong duality holds.

• Step 2: Formulation as Game. We then derive the partial Lagrangian of the LP, and note
that computing an approximately optimal solution to this LP is equivalent to ﬁnding an ap-
proximate minmax solution for a corresponding zero-sum game, in which the payoﬀ func-
tion U is the value of the Lagrangian. The pure strategies of the primal or “Learner” player
correspond to classiﬁers h ∈ H, and the pure strategies of the dual or “Auditor” player cor-
respond to subgroups g ∈ G. Intuitively, the Learner is trying to minimize the sum of the
prediction error and a fairness penalty term (given by the Lagrangian), and the Auditor is
trying to penalize the fairness violation of the Learner by ﬁrst identifying the subgroup with
the greatest fairness violation and putting all the weight on the dual variable corresponding
to this subgroup. In order to reason about convergence, we restrict the set of dual variables
to lie in a bounded set: C times the probability simplex. C is a parameter that we have
to set in the proof of our theorem to give the best theoretical guarantees — but it is also a
parameter that we will vary in the experimental section.

• Step 3: Best Responses as CSC. We observe that given a mixed strategy for the Auditor,
the best response problem of the Learner corresponds to a CSC problem. Similarly, given a
mixed strategy for the Learner, the best response problem of the Auditor corresponds to an
auditing problem (which can be represented as a CSC problem). Hence, if we have oracles
for solving CSC problems, we can compute best responses for both players, in response to
arbitrary mixed strategies of their opponents.

• Step 4: FTPL for No-Regret. Finally, we show that the ability to compute best responses
for each player is suﬃcient to implement dynamics known to converge quickly to equilib-
rium in zero-sum games. Our algorithm has the Learner play Follow the Perturbed Leader
(FTPL) Kalai and Vempala [2005], which is a no-regret algorithm, against an Auditor who at
every round best responds to the learner’s mixed strategy. By the seminal result of Freund
and Schapire [1996], the average plays of both players converge to an approximate equilib-
rium. In order to implement this in polynomial time, we need to represent the loss of the
learner as a low-dimensional linear optimization problem. To do so, we ﬁrst deﬁne an ap-
propriately translated CSC problem for any mixed strategy λ by the Auditor, and cast it as
a linear optimization problem.

4.1 Rewriting the Fair ERM Problem

To rewrite the Fair ERM problem, we note that even though both G and H can be inﬁnite sets,
the sets of possible labellings on the data set S induced by these classes are ﬁnite. More formally,
we will write G(S) and H(S) to denote the set of all labellings on S that are induced by G and H
respectively, that is

G(S) = {(g(x1), . . . , g(xn)) | g ∈ G}

and,

H(S) = {(h(X1), . . . , h(Xn)) | h ∈ H}

We can bound the cardinalities of G(S) and H(S) using Sauer’s Lemma.

14

Lemma 4.3 (Sauer’s Lemma (see e.g. Kearns and Vazirani [1994])). Let S be a data set of size n. Let
d1 = VCDIM(H) and d2 = VCDIM(G) be the VC-dimensions of the two classes. Then
(cid:17)

(cid:17)

|H(S)| ≤ O

(cid:16)
nd1

and

|G(S)| ≤ O

(cid:16)
nd2

.

Given this observation, we can then consider an equivalent optimization problem where the
distribution D is over the set of labellings in H(S), and the set of subgroups are deﬁned by the
labellings in G(S). We will view each g in G(S) as a Boolean function.

To simplify notations, we will deﬁne the following “fairness violation” functions for any g ∈ G

and any h ∈ H:

Φ+(h, g) ≡ αFP (g, P ) (FP(h) − FP(h, g)) − γ
Φ−(h, g) ≡ αFP (g, P ) (FP(h, g) − FP(h)) − γ

Moreover, for any distribution D over H, for any sign • ∈ {+, −}

Φ•(D, g) = E
h∼D

[Φ•(h, g)] .

Claim 4.4. For any g ∈ G, h ∈ H, and any ν > 0,

max{Φ+(D, g), Φ−(D, g)} ≤ ν

if and only if αFP (g, P ) βFP (g, D, P ) ≤ γ + ν.

Thus, we will focus on the following equivalent optimization problem.

such that for each g ∈ G(S) :

min
D∈∆H(S)

[err(h, P )]

E
h∼D
Φ+(D, g) ≤ 0
Φ−(D, g) ≤ 0

For each pair of constraints (7) and (8), corresponding to a group g ∈ G(S), we introduce a pair

of dual variables λ+

g and λ

−
g . The partial Lagrangian of the linear program is the following:

L(D, λ) = E
h∼D

[err(h, P )] +

(cid:88)

(cid:16)
λ+

g∈G(S)

−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:17)

By Sion’s minmax theorem [Sion, 1958], we have

min
D∈∆H(S)

max
λ∈R2|G(S)|
+

L(p, λ) = max
λ∈R2|G(S)|

+

min
D∈∆H(S)

L(p, λ) = OPT

where OPT denotes the optimal objective value in the fair ERM problem. Similarly, the distri-
L(D, λ) corresponds to an optimal feasible solution to the fair ERM linear
bution arg minD maxλ
program. Thus, ﬁnding an optimal solution for the fair ERM problem reduces to computing a
minmax solution for the Lagrangian. Our algorithms will both compute such a minmax solution
by iteratively optimizing over both the primal variables D and dual variables λ. In order to guar-
antee convergence in our optimization, we will restrict the dual space to the following bounded
set:

Λ = {λ ∈ R2|G(S)|

+

| (cid:107)λ(cid:107)

1

≤ C}.

15

(4)

(5)

(6)

(7)

(8)

where C will be a parameter of our algorithm. Since Λ is a compact and convex set, the minmax
condition continues to hold [Sion, 1958]:

min
D∈∆H(S)

max
λ∈Λ

L(D, λ) = max
λ∈Λ

min
D∈∆H(S)

L(D, λ)

(9)

If we knew an upper bound C on the (cid:96)1 norm of the optimal dual solution, then this restriction
on the dual solution would not change the minmax solution of the program. We do not in general
know such a bound. However, we can show that even though we restrict the dual variables to
lie in a bounded set, any approximate minmax solution to Equation (9) is also an approximately
optimal and approximately feasible solution to the original fair ERM problem.

Theorem 4.5. Let ( ˆD, ˆλ) be a ν-approximate minmax solution to the Λ-bounded Lagrangian problem
in the sense that

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν and, L( ˆD, ˆλ) ≥ max
λ∈Λ

L( ˆD, λ) − ν.

Then err( ˆD, P ) ≤ OPT +2ν and for any g ∈ G(S),

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C

.

4.2 Zero-Sum Game Formulation

To compute an approximate minmax solution, we will ﬁrst view Equation (9) as the following two
player zero-sum matrix game. The Learner (or the minimization player) has pure strategies cor-
responding to H, and the Auditor (or the maximization player) has pure strategies corresponding
to the set of vertices Λpure in Λ — more precisely, each vertex or pure strategy either is the all
zero vector or consists of a choice of a g ∈ G(S), along with the sign + or − that the corresponding
g-fairness constraint will have in the Lagrangian. More formally, we write

•
g = C | g ∈ G(S), • ∈ {±}} ∪ {0}
Λpure = {λ ∈ Λ with λ

Even though the number of pure strategies scales linearly with |G(S)|, our algorithm will never
need to actually represent such vectors explicitly. Note that any vector in Λ can be written as a
convex combination of the maximization player’s pure strategies, or in other words: as a mixed
strategy for the Auditor. For any pair of actions (h, λ) ∈ H × Λpure, the payoﬀ is deﬁned as
(cid:88)

U (h, λ) = err(h, P ) +

(cid:16)
λ+

−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:17)

.

Claim 4.6. Let D ∈ ∆H(S) and λ ∈ Λ such that (p, λ) is a ν-approximate minmax equilibrium in the
zero-sum game deﬁned above. Then (p, λ) is also a ν-approximate minmax solution for Equation (9).

Our problem reduces to ﬁnding an approximate equilibrium for this game. A key step in our
solution is the ability to compute best responses for both players in the game, which we now show
can be solved by the cost-sensitive classication (CSC) oracles.

g∈G(S)

16

Learner’s best response as CSC. Fix any mixed strategy (dual solution) λ ∈ Λ of the Auditor.
The Learner’s best response is given by:

err(h, P ) +

argmin
D∈∆H(S)

(cid:88)

g∈G(S)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

(10)

Note that it suﬃces for the Learner to optimize over deterministic classiﬁers h ∈ H, rather than
distributions over classiﬁers. This is because the Learner is solving a linear optimization problem
over the simplex, and so always has an optimal solution at a vertex (i.e. a single classiﬁer h ∈ H).
We can reduce this problem to one that can be solved with a single call to a CSC oracle.
In
particular, we can assign costs to each example (Xi, yi) as follows:

• if yi = 1, then c0

i = 0 and c1

i = − 1
n ;

• otherwise, c0

i = 0 and

c1
i =

1
n

+

1
n

(cid:88)

g∈G(S)

(λ+
g

−
g ) (Pr[g(x) = 1 | y = 0] − 1[g(xi) = 1])
− λ

(11)

Given a ﬁxed set of dual variables λ, we will write LC(λ) ∈ Rn to denote the vector of costs for
labelling each datapoint as 1. That is, LC(λ) is the vector such that for any i ∈ [n], LC(λ)i = c1
i .

Remark 4.7. Note that in deﬁning the costs above, we have translated them from their most natural
values so that the cost of labeling any example with 0 is 0. In doing so, we recall that by Claim 2.7,
the solution to a cost-sensitive classiﬁcation problem is invariant to translation. As we will see, this
will allow us to formulate the learner’s optimization problem as a low-dimensional linear optimization
problem, which will be important for an eﬃcient implementation of follow the perturbed leader. In
particular, if we ﬁnd a hypothesis that produces the n labels y = (y1, . . . , yn) for the n points in our
dataset, then the cost of this labelling in the CSC problem is by construction (cid:104)LC(λ), y(cid:105).

Auditor’s best response as CSC. Fix any mixed strategy (primal solution) p ∈ ∆H(S) of the
Learner. The Auditor’s best response is given by:
(cid:88)

(cid:88)

(cid:16)
λ+

−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:17)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

argmax
λ∈Λ

err(D, P ) +

g∈G(S)

= argmax
λ∈Λ

g∈G(S)

(12)
To ﬁnd the best response, consider the problem of computing ( ˆg, ˆ•) = argmax(g,•) Φ•(D, g).
There are two cases. In the ﬁrst case, p is a strictly feasible primal solution: that is Φˆ•(D, ˆg) < 0.
In this case, the solution to (12) sets λ = 0. Otherwise, if p is not strictly feasible, then by the
following Lemma 4.8 the best response is to set λˆ•

ˆg = C (and all other coordinates to 0).

Lemma 4.8. Fix any D ∈ ∆H(S) such that that maxg∈G(S)
(cid:48))
with one non-zero coordinate (λ

{Φ+(D, g), Φ−(D, g)} > 0. Let λ

(cid:48) ∈ Λ be vector

Then L(D, λ

(cid:48)) ≥ maxλ∈Λ L(D, λ).

•(cid:48)
g (cid:48) = C, where
, •(cid:48)

(g

(cid:48)

) = argmax

(g,•)∈G(S)×{±}

{Φ•(D, g)}

17

Therefore, it suﬃces to solve for argmax(g,•) Φ•(D, g). We proceed by solving argmaxg Φ+(D, g)
and argmaxg Φ−(D, g) separately: both problems can be reduced to a cost-sensitive classiﬁcation
problem. To solve for argmaxg Φ+(D, g) with a CSC oracle, we assign costs to each example (Xi, yi)
as follows:

• if yi = 1, then c0

i = 0 and c1

i = 0;

• otherwise, c0

i = 0 and

c1
i =

(cid:20)

−1
n

E
h∼D

[FP(h)] − E
h∼D

(cid:21)
[h(Xi)]

(13)

To solve for argmaxg Φ−(D, g) with a CSC oracle, we assign the same costs to each example

(Xi, yi), except when yi = 0, labeling “1” incurs a cost of

c1
i =

(cid:20)

−1
n

E
h∼D

[h(Xi)] − E
h∼D

(cid:21)
[FP(h)]

4.3 Solving the Game with No-Regret Dynamics

To compute an approximate equilibrium of the zero-sum game, we will simulate the following no-
regret dynamics between the Learner and the Auditor over rounds: over each of the T rounds, the
Learner plays a distribution over the hypothesis class according to a no-regret learning algorithm
(Follow the Perturbed Leader), and the Auditor plays an approximate best response against the
Learner’s distribution for that round. By the result of Freund and Schapire [1996], the average
plays of both players over time converge to an approximate equilibrium of the game, as long as
the Learner has low regret.

Theorem 4.9 (Freund and Schapire [1996]). Let D1, D2, . . . , DT ∈ ∆H(S) be a sequence of distribu-
tions played by the Learner, and let λ1, λ2, . . . , λT ∈ Λpure be the Auditor’s sequence of approximate best
responses against these distributions respectively. Let D = 1
t=1 λt be the two
T
players’ empirical distributions over their strategies. Suppose that the regret of the Learner satisﬁes

t=1 Dt and λ = 1
T

(cid:80)T

(cid:80)T

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105)− min
h∈H(S)

T(cid:88)

t=1

U (h, λt) ≤ γLT

and max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)]−

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) ≤ γAT .

Then (D, λ) is an (γL + γA)-approximate minimax equilibrium of the game.

Our Learner will play using the Follow the Perturbed Leader (FTPL), which gives a no-regret
guarantee. In order to implement FPTL, we will ﬁrst need to formulate the Learner’s best response
problem as a linear optimization problem over a low dimensional space. For each round t, let
t
s<t λs be the vector representing the sum of the actions played by the auditor over previous
λ
t
rounds, and recall that LC(λ
) is the cost vector given by our cost-sensitive classiﬁcation reduction.
t
is the following linear optimization problem
Then the Learner’s best response problem against λ

(cid:80)

=

t

(cid:104)LC(λ

), h(cid:105).

min
h∈H(S)

18

To run the FTPL algorithm, the Learner will optimize a “perturbed” version of the problem above.
In particular, the Learner will play a distribution Dt over the hypothesis class H(S) that is im-
plicitely deﬁned by the following sampling operation. To sample a hypothesis h from Dt, the
learner solves the following randomized optimization problem:

min
h∈H(S)

t
(cid:104)LC(λ

), h(cid:105) +

(cid:104)ξ, h(cid:105),

1
η

(14)

where η is a parameter and ξ is a noise vector drawn from the uniform distribution over [0, 1]n.
Note that while it is intractable to explicitly represent the distribution Dt (which has support size
scaling with |H(S)|), we can sample from Dt eﬃciently given access to a cost-sensitive classiﬁcation
oracle for H. By instantiating the standard regret bound of FTPL for online linear optimization
(Theorem 2.9), we get the following regret bound for the Learner.

Lemma 4.10. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm with η = n
, and λ1, . . . , λT be the
sequence of plays by the Auditor. Then

1√
nT

(1+C)

(cid:113)

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

T(cid:88)

t=1

U (h, λt) ≤ 2n1/4(1 + C)

T

√

Now we consider how the Auditor (approximately) best responds to the distribution Dt. The
main obstacle is that we do not have an explicit representation for Dt. Thus, our ﬁrst step is to
approximate Dt with an explicitly represented sparse distribution ˆDt. We do that by drawing m
i.i.d. samples from Dt, and taking the empirical distribution ˆDt over the sample. The Auditor
will best respond to this empirical distribution ˆDt. To show that any best response to ˆDt is also an
approximate best response to Dt, we will rely on the following uniform convergence lemma, which
bounds the diﬀerence in expected payoﬀ for any strategy of the auditor, when played against Dt
as compared to ˆDt.
Lemma 4.11. Fix any ξ, δ ∈ (0, 1) and any distribution D over H(S). Let h1, . . . , hm be m i.i.d. draws
from p, and ˆD be the empirical distribution over the realized sample. Then with probability at least 1 − δ
over the random draws of hj’s, the following holds,

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as long as m ≥ c0

C2(ln(1/δ)+d2 ln(n))
ξ2

for some absolute constant c0 and d2 = VCDIM(G).

Using Lemma 4.11, we can derive a regret bound for the Auditor in the no-regret dynamics.

Lemma 4.12. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm. For each Dt, let ˆDt be the empirical distri-
bution over m i.i.d. draws from Dt. Let λ1, . . . , λT be the Auditor’s best responses against ˆD1, . . . , ˆDT .
Then with probability 1 − δ,

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:114)

(cid:104)
U (h, λt)

(cid:105) ≤ T

c0C2(ln(T /δ) + d2 ln(n))
m

for some absolute constant c0 and d2 = VCDIM(G).

19

Finally, let D and λ be the average of the strategies played by the two players over the course
of the dynamics. Note that D is an average of many distributions with large support, and so D
itself has support size that is too large to represent explicitely. Thus, we will again approximate D
with a sparse distribution ˆD estimated from a sample drawn from D. Note that we can eﬃciently
sample from D given access to a CSC oracle. To sample, we ﬁrst uniformly randomly select a
round t ∈ [T ], and then use the CSC oracle to solve the sampling problem deﬁned in (14), with the
noise random variable ξ freshly sampled from its distribution. The full algorithm is described in
Algorithm 2 and we present the proof for Theorem 4.2 below.

Algorithm 2 FairNR: Fair No-Regret Dynamics

Input: distribution P over n labelled data points, CSC oracles CSC(H) and CSC(G), dual bound
C, and target accuracy parameter ν, δ
0
= 0, η = n
Initialize: Let C = 1/ν, λ

(cid:113)

,

(1+C)

1√
nT

m =

(ln(2T /δ)d2 ln(n)) C2c0T
n(1 + C)2 ln(2/δ)

√

and,

T =

√

4

n ln(2/δ)

ν4

For t = 1, . . . , T :

For s = 1, . . . m:

Sample from the Learner’s FTPL distribution:

Draw a random vector ξs uniformly at random from [0, 1]n
Use the oracle CSC(H) to compute h(s,t) = argminh∈H(S)

(cid:104)LC(λ

(t−1)

), h(cid:105) + 1
η

(cid:104)ξs, h(cid:105)

Let ˆDt be the empirical distribution over {hs,t}

Auditor best responds to ˆDt:

Use the oracle CSC(G) to compute λt = argmaxλ

E

h∼ ˆD [U (h, λ)]

t
Update: Let λ

=

(cid:80)

t(cid:48)≤t λt(cid:48)

Sample from the average distribution D =

(cid:80)T

t=1 Dt:

For s = 1, . . . m:

Draw a random number r ∈ [T ] and a random vector ξs uniformly at random from [0, 1]n
Use the oracle CSC(H) to compute h(r,t) = argminh∈H(S)

(cid:104)LC(λ

(cid:104)ξs, h(cid:105)

), h(cid:105) + 1
η

(r−1)

Let ˆD be the empirical distribution over {hr,t}

Output: ˆD as a randomized classiﬁer

Proof of Theorem 4.2. By Theorem 4.5, it suﬃces to show that with probability at least 1 − δ, ( ˆD, λ)
is a ν-approximate equilibrium in the zero-sum game. As a ﬁrst step, we will rely on Theorem 4.9
to show that (D, λ) forms an approximate equilibrium.

By Lemma 4.10, the regret of the sequence D1, . . . , DT is bounded by:

γL =


T(cid:88)


t=1

1
T

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

T(cid:88)

t=1

U (h, λt)





≤ 2n1/4(1 + C)
√
T

20

By Lemma 4.12, with probability 1 − δ/2, we have

(cid:114)

≤

γA

c0C2(ln(2T /δ) + d2 ln(n))
m

We will condition on this upper-bound event on γA for the rest of this proof, which is the case
except with probability δ/2. By Theorem 4.9, we know that the average plays (D, λ) form an
(γL + γA)-approximate equilibrium.

Finally, we need to bound the additional error for outputting the sparse approximation ˆD
instead of D. We can directly apply Lemma 4.11, which implies that except with probability δ/2,
the pair ( ˆD, λ) form a R-approximate equilibrium, with

R ≤ γA + γL +

(cid:112)

c0C2(ln(2/δ) + d2 ln(n))
m

√

Note that R ≤ ν as long as we have C = 1/ν,

m =

(ln(2T /δ)d2 ln(n)) C2c0T
n(1 + C)2 ln(2/δ)

√

and,

T =

√

4

n ln(2/δ)

ν4

This completes our proof.

5 Experimental Evaluation

We now describe an experimental evaluation of our proposed algorithmic framework on a dataset
in which fairness is a concern, due to the preponderance of racial and other sensitive features. For
far more detailed experiments on four real datasets investigating the convergence properties of
our algorithm, evaluating its accuracy vs. fairness tradeoﬀs, and comparing our approach to the
recent algorithm of Agarwal et al. [2017], we direct the reader to Kearns et al. [2018]. Python code
and an illustrative Jupyter notebook are provided here (https://github.com/algowatchpenn/GerryFair).
While the no-regret-based algorithm described in the last section enjoys provably polynomial
time convergence, for the experiments we instead implemented a simpler yet eﬀective algorithm
based on Fictitious Play dynamics. We ﬁrst describe and discuss this modiﬁed algorithm.

5.1 Solving the Game with Fictitious Play

Like the algorithm given in the last section, the algorithm we implemented works by simulating
a game dynamic that converges to Nash equilibrium in the zero-sum game that we derived, cor-
responding to the Fair ERM problem. Rather than using a no-regret dynamic, we instead use a
simple iterative procedure known as Fictitious Play [Brown, 1949]. Fictitious Play dynamics has
the beneﬁt of being more practical to implement: at each round, both players simply need to com-
pute a single best response to the empirical play of their opponents, and this optimization requires
only a single call to a CSC oracle. In contrast, the FTPL dynamic we gave in the previous section
requires making many calls to a CSC oracle per round — a computationally expensive process —
in order to ﬁnd a sparse approximation to the Learner’s mixed strategy at that round. Fictitious
Play also has the beneﬁt of being deterministic, unlike the randomized sampling required in the
FTPL no-regret dynamic, thus eliminating a source of experimental variance.

21

The disadvantage is that Fictitious Play is only known to converge to equilibrium in the
limit Robinson [1951], rather than in a polynomial number of rounds (though it is conjectured
to converge quickly under rather general circumstances; see Daskalakis and Pan [2014] for a re-
cent discussion). Nevertheless, this is the algorithm that we use in our experiments — and as we
will show, it performs well on real data, despite the fact that it has weaker theoretical guarantees
compared to the algorithm we presented in the last section.

Fictitious play proceeds in rounds, and in every round each player chooses a best response
to his opponent’s empirical history of play across previous rounds, by treating it as the mixed
strategy that randomizes uniformly over the empirical history. Pseudocode for the implemented
algorithm is given below.

Algorithm 3 FairFictPlay: Fair Fictitious Play

Input: distribution P over the labelled data points, CSC oracles CSC(H) and CSC(G) for the
classes H(S) and G(S) respectively, dual bound C, and number of rounds T
Initialize: set h0 to be some classiﬁer in H, set λ0 to be the zero vector. Let D and λ be the point
distributions that put all their mass on h0 and λ0 respectively.
For t = 1, . . . , T :

Compute the empirical play distributions:

Let D be the uniform distribution over the set of classiﬁers {h0, . . . , ht−1}
Let λ =

be the auditor’s empirical dual vector

(cid:80)
t

(cid:48)

(cid:48)

<t λt
t

Learner best responds: Use the oracle CSC(H) to compute ht = argminh∈H(S)
Auditor best responds: Use the oracle CSC(G) to compute λt = argmaxλ

(cid:104)LC(λ), h(cid:105)
h∼D [U (h, λ)]

E

Output: the ﬁnal empirical distribution D over classiﬁers

5.2 Description of Data

The dataset we use for our experimental valuation is known as the “Communities and Crime”
(C&C) dataset, available at the UC Irvine Data Repository4. Each record in this dataset describes
the aggregate demographic properties of a diﬀerent U.S. community; the data combines socio-
economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey,
and crime data from the 1995 FBI UCR. The total number of records is 1994, and the number of
features is 122. The variable to be predicted is the rate of violent crime in the community.

While there are larger and more recent datasets in which subgroup fairness is a potential con-
cern, there are properties of the C&C dataset that make it particularly appealing for the initial
experimental evaluation of our proposed algorithm. Foremost among these is the relatively high
number of sensitive or protected attributes, and the fact that they are real-valued (since they rep-
resent aggregates in a community rather than speciﬁc individuals). This means there is a very
large number of protected sub-groups that can be deﬁned over them. There are distinct con-
tinuous features measuring the percentage or per-capita representation of multiple racial groups
(including white, black, Hispanic, and Asian) in the community, each of which can vary indepen-
dently of the others. Similarly, there are continuous features measuring the average per capita
incomes of diﬀerent racial groups in the community, as well as features measuring the percentage
of each community’s police force that falls in each of the racial groups. Thus restricting to features

4http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime

22

capturing race statistics and a couple of related ones (such as the percentage of residents who do
not speak English well), we obtain an 18-dimensional space of real-valued protected attributes.
We note that the C&C dataset has numerous other features that arguably could or should be pro-
tected as well (such as gender features), which would raise the dimensionality of the protected
subgroups even further. 5

We convert the real-valued rate of violent crime in each community to a binary label indicating
whether the community is in the 70th percentile of that value, indicating that it is a relatively
high-crime community. Thus the strawman baseline that always predicts 0 (lower crime) has
error approximately 30% or 0.3 on this classiﬁcation problem. We chose the 70th percentile since
it seems most natural to predict the highest crime rates.

As in the theoretical sections of the paper, our main interest and emphasis is on the eﬀective-

ness of our proposed algorithm FairFictPlay on a given dataset, including:

• Whether the algorithm in fact converges, and does so in a feasible amount of computation.
Recall that formal convergence is only guaranteed under the assumption of oracles that do
not exist in practice, and even then is only guaranteed asymptotically.

• Whether the classiﬁer learned by the algorithm has nontrivial accuracy, as well as strong

subgroup fairness properties.

racy and subgroup fairness.

• Whether the algorithm and dataset permits nontrivial tuning of the trade-oﬀ between accu-

As discussed in Section 2.1, we note that all of these issues can be investigated entirely in-sample,
without concern for generalization performance. Thus for simplicity, despite the fact that our
algorithm enjoys all the usual generalization properties depending on the VC dimension of the
Learner’s hypothesis space and the Auditor’s subgroup space (see Theorems 2.12 and 2.11), we
report all results here on the full C&C dataset of 1994 points, treating it as the true distribution
of interest.

5.3 Algorithm Implementation

The main details in the implementation of FairFictPlay are the identiﬁcation of the model classes
for Learner and Auditor, the implementation of the cost sensitive classiﬁcation oracle and auditing
oracle, and the identiﬁcation of the protected features for Auditor. For our experiments, at each
round Learner chooses a linear threshold function over all 122 features. We implement the cost
sensitive classiﬁcation oracle via a two stage regression procedure. In particular, the inputs to
the cost sensitive classiﬁcation oracle are cost vectors c0, c1, where the ith element of ck is the cost
of predicting k on datapoint i. We train two linear regression models r0, r1 to predict c0 and c1
respectively, using all 122 features. Given a new point x, we predict the cost of classifying x as 0
and 1 using our regression models: these predictions are r0(x) and r1(x) respectively. Finally we
output the prediction ˆy corresponding to lower predicted cost: ˆy = argmini∈{0,1}ri(x).

Auditor’s model class consists of all linear threshold functions over just the 18 aforementioned
protected race-based attributes. As per the algorithm, at each iteration t Auditor attempts to ﬁnd
a subgroup on which the false positive rate is substantially diﬀerent than the base rate, given

5Ongoing experiments on other datasets where fairness is a concern will be reported on in a forthcoming experi-

mental paper.

23

the Learner’s randomized classiﬁer so far. We implement the auditing oracle by treating it as
a weighted regression problem in which the goal is ﬁnd a linear function (which will be taken
to deﬁne the subgroup) that on the negative examples, can predict the Learner’s probabilistic
classiﬁcation on each point. We use the same regression subroutine as Learner does, except that
Auditor only has access to the 18 sensitive features, rather than all 122.

Recall that in addition to the choices of protected attributes and model classes for Learner
and Auditor, FairFictPlay has a parameter C, which is a bound on the norm of the dual variables
for Auditor (the dual player). While the theory does not provide an explicit bound or guide for
choosing C, it needs to be large enough to permit the dual player to force the minmax value of the
game. For our experiments we chose C = 10, which despite being a relatively small value seems
to suﬃce for (approximate) convergence.

The other and more meaningful parameter of the algorithm is the bound γ in the Fair ERM
optimization problem implemented by the game, which controls the amount of unfairness per-
mitted. If on a given round the subgroup disparity found by the Auditor is greater than γ, the
Learner must react by adding a fairness penalty for this subgroup to its objective function; if it is
smaller than γ, the Learner can ignore it and continue to optimize its previous objective function.
Ideally, and as we shall see, varying γ allows us to trace out a menu of trade-oﬀs between accuracy
and fairness.

5.4 Results

Particularly in light of the gaps between the idealized theory and the actual implementation,
the most basic questions about FairFictPlay are whether it converges at all, and if so, whether
it converges to “interesting” models — that is, models with both nontrivial classiﬁcation error
(much better than the 30% or 0.3 baserate), and nontrivial subgroup fairness (much better than
ignoring fairness altogether). We shall see that at least for the C&C dataset, the answers to these
questions is strongly aﬃrmative.

(a)

(b)

Figure 1: Evolution of the error and unfairness of Learner’s classiﬁer across iterations, for varying
choices of γ. (a) Error εt of Learner’s model vs iteration t. (b) Unfairness γt of subgroup found by
Auditor vs. iteration t, as measured by Deﬁnition 2.3. See text for details.

24

We begin by examining the evolution of the error and unfairness of Learner’s model. In the
left panel of Figure 1 we show the error of the model found by Learner vs. iteration for values of
γ ranging from 0 to 0.029. Several comments are in order.

First, after an initial period in which there is a fair amount of oscillatory behavior, by 6000
iterations most of the curves have largely ﬂattened out, and by 8,000 iterations it appears most
but not all have reached approximate convergence. Second, while the top-to-bottom ordering of
these error curves is approximately aligned with decreasing γ — so larger γ generally results in
lower error, as expected — there are many violations of this for small t, and even a few at large
t. Third, and as we will examine more closely shortly, the converged values at large t do indeed
exhibit a range of errors.

In the right panel of Figure 1, we show the corresponding unfairness γt of the subgroup found
by the Auditor at each iteration t for the same runs and values of the parameter γ (indicated by
horizontal dashed lines), with the same color-coding as for the left panel. Now the ordering is
generally reversed — larger values of γ generally lead to higher γt curves, since the fairness con-
straint on the Learner is weaker. We again see a great deal of early oscillatory behavior, with most
γt curves then eventually settling at or near their corresponding input γ value, as Learner and
Auditor engage in a back-and-forth struggle for lower error for Learner and γ-subgroup fairness
for Auditor.

(a)

(b)

Figure 2:
(a) Pareto-optimal error-unfairness values, color coded by varying values of the input
parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same
range but are sampled more densely to get a smoother frontier. See text for details.

For any choice of the parameter γ, and each iteration t, the two panels of Figure 1 yield a pair
of realized values (cid:104)εt, γt
(cid:105) from the experiment, corresponding to a Learner model whose error is
εt, and for which the worst subgroup the Auditor was able to ﬁnd had unfairness γt. The set of all
(cid:105) pairs across all runs or γ values thus represents the diﬀerent trade-oﬀs between error and
(cid:104)εt, γt
unfairness found by our algorithm on the data. Most of these pairs are of course Pareto-dominated
by other pairs, so we are primarily interested in the undominated frontier.

In the left panel of Figure 2, for each value of γ we show the Pareto-optimal pairs, color-coded
for the value of γ. Each value of γ yields a set or cloud of undominated pairs that are usually fairly

25

close to each other, and as expected, as γ is increased, these clouds generally move leftwards and
upwards (lower error and higher unfairness).

We anticipate that the practical use of our algorithm would, as we have done, explore many
values of γ and then pick a model corresponding to a point on the aggregated Pareto frontier
across all γ, which represents the collection of all undominated models and the overall error-
unfairness trade-oﬀ. This aggregate frontier is shown in the right panel of Figure 2, and shows
a relatively smooth menu of options, ranging from error about 0.21 and no unfairness at one
extreme, to error about 0.12 and unfairness 0.025 at the other, and an appealing assortment of
intermediate trade-oﬀs. Of course, in a real application the selection of a particular point on
the frontier should be made in a domain-speciﬁc manner by the stakeholders or policymakers in
question.

Acknowledgements We thank Alekh Agarwal, Richard Berk, Miro Dud´ık, Akshay Krishna-
murthy, John Langford, Greg Ridgeway and Greg Yang for helpful discussions and suggestions.

References

Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, and John Langford. A reductions approach
to fair classiﬁcation. Fairness, Accountability, and Transparency in Machine Learning (FATML),
2017.

Julia Angwin and Hannes Grassegger. Facebooks secret censorship rules protect white men from

hate speech but not black children. Propublica, 2017.

Anna Maria Barry-Jester, Ben Casselman, and Dana Goldstein. The new science of sentencing.

The Marshall Project, August 8 2015. Retrieved 4/28/2016.

George W. Brown. Some notes on computation of games solutions, Jan 1949.

Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. arXiv preprint arXiv:1703.00056, 2017.

Constantinos Daskalakis and Qinxuan Pan. A counter-example to Karlin’s strong conjecture for
ﬁctitious play. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium
on, pages 11–20. IEEE, 2014.

Ilias Diakonikolas, Ryan O’Donnell, Rocco A. Servedio, and Yi Wu. Hardness results for agnosti-
cally learning low-degree polynomial threshold functions. In Proceedings of the Twenty-Second
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California,
USA, January 23-25, 2011, pages 1590–1606, 2011.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Confer-
ence, pages 214–226. ACM, 2012.

Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning of

monomials by halfspaces is hard. SIAM J. Comput., 41(6):1558–1590, 2012.

26

Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings
of the Ninth Annual Conference on Computational Learning Theory, COLT 1996, Desenzano del
Garda, Italy, June 28-July 1, 1996., pages 325–332, 1996.

Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im) possibility

of fairness. arXiv preprint arXiv:1609.07236, 2016.

Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination
prevention in data mining. IEEE transactions on knowledge and data engineering, 25(7):1445–
1459, 2013.

Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning.

Advances in Neural Information Processing Systems, 2016.

´Ursula H´ebert-Johnson, Michael P Kim, Omer Reingold, and Guy N Rothblum. Calibration for

the (computationally-identiﬁable) masses. arXiv preprint arXiv:1711.08513, 2017.

Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
In Advances in Neural Information Processing Systems, pages

Classic and contextual bandits.
325–333, 2016.

Adam Tauman Kalai and Santosh Vempala. Eﬃcient algorithms for online decision problems. J.

Comput. Syst. Sci., 71(3):291–307, 2005.

Adam Tauman Kalai, Yishay Mansour, and Elad Verbin. On agnostic boosting and parity learn-
ing. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British
Columbia, Canada, May 17-20, 2008, pages 629–638, 2008.

Faisal Kamiran and Toon Calders. Data preprocessing techniques for classiﬁcation without dis-

crimination. Knowledge and Information Systems, 33(1):1–33, 2012.

Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. An Empirical Study of Rich
Subgroup Fairness for Machine Learning. ArXiv e-prints, art. arXiv:1808.08166, August 2018.

Michael J Kearns and Umesh Virkumar Vazirani. An Introduction to Computational Learning The-

ory. MIT press, 1994.

Michael J Kearns, Robert E Schapire, and Linda M Sellie. Toward eﬃcient agnostic learning.

Machine Learning, 17(2-3):115–141, 1994.

Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-oﬀs in the fair deter-
mination of risk scores. In Proceedings of the 2017 ACM Conference on Innovations in Theoretical
Computer Science, Berkeley, CA, USA, 2017, 2017.

James Rufus Koren. What does that web search say about your credit? Los Angeles Times, July 16

2016. Retrieved 9/15/2016.

1951.

Julia Robinson. An iterative method of solving a game. Annals of Mathematics, pages 10–2307,

27

Cynthia Rudin. Predictive policing using machine learning to detect patterns of crime. Wired

Magazine, August 2013. Retrieved 4/28/2016.

Maurice Sion. On general minimax theorems. Paciﬁc J. Math., 8(1):171–176, 1958.

Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-

discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.

Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate ex-
ample weighting. In Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM
2003), 19-22 December 2003, Melbourne, Florida, USA, page 435, 2003.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate
mistreatment.
In Proceedings of the 26th International Conference on World Wide Web, pages
1171–1180. International World Wide Web Conferences Steering Committee, 2017.

Zhe Zhang and Daniel B Neill. Identifying signiﬁcant predictive bias in classiﬁers. arXiv preprint

arXiv:1611.08292, 2016.

A Chernoﬀ-Hoeﬀding Bound

We use the following concentration inequality.

Theorem A.1 (Real-vaued Additive Chernoﬀ-Hoeﬀding Bound). Let X1, X2, . . . , Xm be i.i.d. random
variables with E [Xi] = µ and a ≤ Xi

≤ b for all i. Then for every α > 0,
(cid:32) −2α2m
(cid:33)
(b − a)2

≤ 2 exp

i Xi
m

≥ α

− µ

(cid:80)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr

B Generalization Bounds

Proof of Theorems 2.11 and 2.12. We give a proof of Theorem 2.11. The proof of Theorem 2.12 is
identical, as false positive rates are just positive classiﬁcation rates on the subset of the data for
which y = 0.

Given a set of classiﬁers H and protected groups G, deﬁne the following function class:

FH,G = {fh,g (x) (cid:17) h(x) ∧ g(x) : h ∈ H, g ∈ G}

We can relate the VC-dimension of FH,G to the VC-dimension of H and G:
Claim B.1.

VCDIM(FH,G) ≤ ˜O(VCDIM(H) + VCDIM(G))
Proof. Let S be a set of size m shattered by FH,G. Let πFH,G (S) be the number of labelings of S
realized by elements of FH,G. By the deﬁnition of shattering, πFH,G (S) = 2m. Now for each labeling
of S by an element in FH,G, it is realized as (f ∧ g)(S) for some f ∈ F , g ∈ G. But (f ∧ g)(S) =
f (S)∧g(S), and so it can be realized as the conjunction of a labeling of S by an element of F and an

28

element of G. But since there are πF (S)πG(S) such pairs of labelings, this immediately implies that
πFH,G (S) ≤ πF (S)πG(S). Now by the Sauer-Shelah Lemma (see e.g. Kearns and Vazirani [1994]),
πF (S) = O(mVCDIM(H)), πG(S) = O(mVCDIM(G)). Thus πFH,G (S) = 2m ≤ O(mVCDIM(H)+VCDIM(G)), which
implies that m = ˜O(VCDIM(H) + VCDIM(G)), as desired.

This bound, together with a standard VC-Dimension based uniform convergence theorem (see

e.g. Kearns and Vazirani [1994]) implies that with probability 1 − δ, for every fh,g

∈ FH,G:

(cid:12)(cid:12)(cid:12)E
(X,y)∼P [fh,g (X)] − E

(X,y)∼P

S [fh,g (X)]

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

Note that the left hand side of the above inequality can be written as:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr
(X,y)∼P

[h(X) = 1|g(x) = 1] · Pr

(X,y)∼P

[g(x) = 1] − Pr
(X,y)∼P

[h(X) = 1|g(x) = 1] · Pr
(X,y)∼P

S

S

[g(x) = 1]





(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

This completes our proof.

C Missing Proofs in Section 4

Theorem 4.5. Let ( ˆD, ˆλ) be a ν-approximate minmax solution to the Λ-bounded Lagrangian problem
in the sense that

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν and, L( ˆD, ˆλ) ≥ max
λ∈Λ

L( ˆD, λ) − ν.

Then err( ˆD, P ) ≤ OPT +2ν and for any g ∈ G(S),

Proof of Theorem 4.5. Let D
problem. Since D

∗ is feasible, we know that L(D

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C
∗ be the optimal feasible solution for our constrained optimization
, P ).

, ˆλ) ≤ err(D

.

∗

∗

We will ﬁrst focus on the case where ˆD is not a feasible solution, that is

max
(g,•)∈G(S)×{±}

Φ•( ˆD, g) > 0

Let ( ˆg, ˆ•) ∈ argmax(g,•) Φ•( ˆD, g) and let λ
zero. By Lemma 4.8, we know that λ
minmax solution, we know that L( ˆD, ˆλ) ≥ L( ˆD, λ

(cid:48) ∈ argmaxλ∈Λ

(cid:48) ∈ Λ be a vector with (λ

(cid:48))ˆ•
ˆg = C and all other coordinates
L( ˆD, λ). By the deﬁnition of a ν-approximate

(cid:48)) − ν. This implies that

Note that L(D

, ˆλ) ≤ err(D

, P ), and so

∗

∗

L( ˆD, ˆλ) ≥ err( ˆD, P ) + C Φˆ•( ˆD, ˆg) − ν

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν ≤ L(D

, ˆλ) + ν

∗

29

(15)

(16)

Combining Equations (15) and (16), we get

err( ˆD, P ) + C Φˆ•( ˆD, ˆg) ≤ L( ˆD, ˆλ) + ν ≤ L(D

∗

, ˆλ) + 2ν ≤ err(D

, P ) + 2ν

∗

Note that C Φˆ•( ˆD, ˆg) ≥ 0, so we must have err( ˆD, P ) ≤ err(D
, P ) ∈ [0, 1], we know
since err( ˆD, P ), err(D

∗

∗

, P ) + 2ν = OPT +2ν. Furthermore,

which implies that maximum constraint violation satisﬁes Φˆ•( ˆD, ˆg) ≤ (1 + 2ν)/C. By applying
Claim 4.4, we get

C Φˆ•( ˆD, ˆg) ≤ 1 + 2ν,

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C

.

Now let us consider the case in which ˆD is a feasible solution for the optimization problem.

Then it follows that there is no constraint violation by ˆD and maxλ
∗

err( ˆD, P ) = max

L( ˆD, λ) ≤ L( ˆD, ˆλ) + ν ≤ min
D

λ

L(D, ˆλ) + 2ν ≤ L(D

, ˆλ) + 2ν ≤ err(D

, P ) + 2ν

∗

L( ˆD, λ) = err( ˆD, P ), and so

Therefore, the stated bounds hold for both cases.

Lemma 4.8. Fix any D ∈ ∆H(S) such that that maxg∈G(S)
(cid:48))
with one non-zero coordinate (λ

•(cid:48)
g (cid:48) = C, where

{Φ+(D, g), Φ−(D, g)} > 0. Let λ

(cid:48) ∈ Λ be vector

(cid:48)

, •(cid:48)

(g

) = argmax

(g,•)∈G(S)×{±}

{Φ•(D, g)}

Then L(D, λ

(cid:48)) ≥ maxλ∈Λ L(D, λ).

Proof of Lemma 4.8. Observe:

L(D, λ) = argmax

[err(h, P )] +

(cid:88)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

argmax
λ∈Λ

λ∈Λ

= argmax
λ∈Λ

E
h∼D
(cid:88)

g∈G

g∈G(S)
(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:16)
λ+

Note that this is a linear optimization problem over the non-negative orthant of a scaling of the (cid:96)1
ball, and so has a solution at a vertex, which corresponds to a single group g ∈ G(S). Thus, there
(cid:48))•
is always a best response λ
g that maximizes
Φ•(D, g).

(cid:48) that puts all the weight C on the coordinate (λ

Lemma 4.10. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm with η = n
, and λ1, . . . , λT be the
sequence of plays by the Auditor. Then

1√
nT

(1+C)

(cid:113)

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

U (h, λt) ≤ 2n1/4(1 + C)

T

√

T(cid:88)

t=1

30

Proof of Lemma 4.10. To instantiate the regret bound in Theorem 2.9, we just need to provide a
bound on the maximum absoluate value over the coordinates of the loss vector (the quantity M in
Theorem 2.9). For any λ ∈ Λ, the absolute value of the i-th coordinate of LC(λ) is bounded by:

(λ+
g

−
g ) (Pr[g(x) = 1 | y = 0] − 1) 1[g(xi) = 1]
− λ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(Pr[g(x) = 1 | y = 0]1g(xi) = 1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
n

+

(cid:88)

1
n
g∈G(S)


(cid:88)

≤ 1
n

+

≤ 1
n

+

1
n

1
n




g∈G(S)


(cid:88)

g∈G(S)

(cid:12)(cid:12)(cid:12)λ+

g

−
− λ
g



(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)λ+

g

(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)λ

−
g

max
g∈G(S)


(cid:12)(cid:12)(cid:12)

≤ 1 + C
n

Also note that the dimension of the optimization is the size of the dataset n. This means if we set
η = n

, the regret of the learner will then be bounded by 2n1/4(1 + C)

T .

(cid:113)

√

(1+C)

1√
nT

Lemma 4.11. Fix any ξ, δ ∈ (0, 1) and any distribution D over H(S). Let h1, . . . , hm be m i.i.d. draws
from p, and ˆD be the empirical distribution over the realized sample. Then with probability at least 1 − δ
over the random draws of hj’s, the following holds,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as long as m ≥ c0

C2(ln(1/δ)+d2 ln(n))
ξ2

for some absolute constant c0 and d2 = VCDIM(G).

Proof of Lemma 4.11. Recall that for any distribution D
deﬁned as

(cid:48) over H(S) the expected payoﬀ function is

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)] = E
h∼ ˆD

[err(h, P )] + E
h∼ ˆD

− E
h∼D

[err(h, P )] + E
h∼D

(cid:88)

(cid:16)
λ+

g Φ+(h, g) + λ


(cid:17)
−
g Φ−(h, g)


(cid:17)
−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:16)
λ+








g∈G(S)

(cid:88)

g∈G(S)



h∼D [err(h, P )] −

By the triangle inequality, it suﬃces to show that with probability (1 − δ), A = | E
E
h∼ ˆD [err(h, P )] | ≤ ξ/2 and for all λ ∈ Λ and g ∈ G(S),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

B =

E
h∼ ˆD





(cid:88)

g∈G(S)


(cid:17)
−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:16)
λ+



− E
h∼D





(cid:88)

g∈G(S)

(cid:16)
λ+

−
g Φ−(h, g)
g Φ+(h, g) + λ

≤ ξ/2


(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


The ﬁrst part follows directly from a simple application of the Chernoﬀ-Hoeﬀding bound

(Theorem A.1): with probability (1 − δ/2), A ≤ ξ/2, as long as m ≥ 2 ln(4/δ)/ξ2.
To bound the second part, we ﬁrst note that by H¨older’s inequality, we have

B ≤ (cid:107)λ(cid:107)

1 max
(g,•)∈G(S)×{±}

|Φ•(D, g) − Φ•( ˆD, g)|

31

Since for all λ ∈ Λ we have (cid:107)λ(cid:107)

≤ C, it suﬃces to show that with probability 1−δ/2, |Φ•(D, g)−

1

Φ•( ˆD, g)| ≤ ξ/(2C) holds for all • ∈ {−, +} and g ∈ G(S). Note that

We can rewrite the absolute value of ﬁrst term:

|Φ•(D, g) − Φ•( ˆD, g)| =

[FP(h)]

Pr[y = 0, g(x) = 1]

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[FP(h)] − E
h∼ ˆD

(cid:33)

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

E
h∼D

[FP(h)] − E
h∼ ˆD

(cid:33)
[FP(h)]

Pr[y = 0, g(x) = 1]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1 | y = 0]] − E
h∼ ˆD

E
h∼D

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0]]

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:33)
[Pr[h(X) = 1 | y = 0]]

Pr[g(x) = 1 | y = 0]

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:32)

+

(cid:32)

(cid:32)

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=

≤

where the last inequality follows from Pr[g(x) = 1 | y = 0] ≤ 1.

Note that E

h∼ ˆD [Pr[h(X) = 1, y = 0, g(x) = 1]] = 1
m

average of m i.i.d. random variables with expectation E
Chernoﬀ-Hoeﬀding bound (Theorem A.1), we have

(cid:80)m

j=1 Pr[hj(X) = 1, y = 0, g(x) = 1], which is an
h∼D [Pr[h(X) = 1, y = 0, g(x) = 1]]. By the

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

Pr

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD
(cid:16)− ξ2m
8C2

[Pr[h(X) = 1, y = 0]]

>

≤ 2 exp

(17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

ξ
4C

(cid:33)

(cid:32)

− ξ2m
8C2

In the following, we will let δ0 = 2 exp

(cid:17)
. Similarly, we also have for each g ∈ G(S),

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

>

(18)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

ξ
4C

≤ δ0

By taking the union bound over (17) and (18) over all choices of g ∈ G(S), we have with proba-

bility at least (1 − δ0(1 + |G(S)|)),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0]]

(19)

and,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

for all g ∈ G(S).

(20)

(cid:17)
Note that by Sauer’s lemma (Lemma 4.3), |G(S)| ≤ O
. Thus, there exists an absolute constant
C2(ln(1/δ)+d2 ln(n))
c0 such that m ≥ c0
implies that failure probability above δ0(1 + |G(S)|) ≤ δ/2. We
ξ2
will assume m satisiﬁes such a bound, and so the events of (19) and (20) hold with probaility at
least (1−δ/2). Then by the triangle inequality we have for all (g, •) ∈ G(S)×{±}, |Φ•(D, g)−Φ•( ˆD, g)| ≤
ξ/(2C), which implies that B ≤ ξ/2. This completes the proof.

(cid:16)
nd2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ ξ
4C

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ ξ
4C

32

Claim C.1. Suppose there are two distributions D and ˆD over H(S) such that

Let

Then

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ˆλ ∈ argmax

λ(cid:48)∈Λ

E
h∼ ˆD

(cid:2)U (h, λ

(cid:48)

)(cid:3)

max
λ

E
h∼D

[U (h, λ)] − ξ ≤ E
h∼D

(cid:105)
(cid:104)
U (h, ˆλ)

,

Lemma 4.12. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm. For each Dt, let ˆDt be the empirical distri-
bution over m i.i.d. draws from Dt. Let λ1, . . . , λT be the Auditor’s best responses against ˆD1, . . . , ˆDT .
Then with probability 1 − δ,

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:114)

(cid:104)
U (h, λt)

(cid:105) ≤ T

c0C2(ln(T /δ) + d2 ln(n))
m

for some absolute constant c0 and d2 = VCDIM(G).

Proof. Let γ t

A be deﬁned as

By instantiating Lemma 4.11 and applying union bound across all T steps, we know with proba-
bility at least 1 − δ, the following holds for all t ∈ [T ]:

γ t
A = max
λ∈Λ

E
h∼ ˆDt

[U (h, λ)] − E
h∼Dt

[U (h, λ)]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:114)

≤

γ t
A

c0C2(ln(T /δ) + d2 ln(n))
m

where c0 is the absolute constant in Lemma 4.11 and d2 = VCDIM(G).

Note that by Claim C.1, the Auditor is performing a γ t
round t. Then we can bound the Auditor’s regret as follows:

A-approximate best response at each

γA =





1
T

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

max
λ∈Λ

E
h∼Dt

[U (h, λ)] − E
h∼Dt

(cid:104)
U (h, λt)

(cid:105)(cid:19)


(cid:105)



(cid:18)

T(cid:88)

≤ 1
T
t=1
≤ max
γ t
A
T

It follows that with probability 1 − δ, we have

which completes the proof.

(cid:114)

≤

γA

c0C2(ln(T /δ) + d2 ln(n))
m

33

8
1
0
2
 
c
e
D
 
3
 
 
]

G
L
.
s
c
[
 
 
5
v
4
4
1
5
0
.
1
1
7
1
:
v
i
X
r
a

Preventing Fairness Gerrymandering:
Auditing and Learning for Subgroup Fairness

Michael Kearns1, Seth Neel1, Aaron Roth1 and Zhiwei Steven Wu2

1University of Pennsylvania
2Microsoft Research-New York City

December 4, 2018

Abstract

The most prevalent notions of fairness in machine learning are statistical deﬁnitions: they
ﬁx a small collection of high-level, pre-deﬁned groups (such as race or gender), and then ask
for approximate parity of some statistic of the classiﬁer (like positive classiﬁcation rate or false
positive rate) across these groups. Constraints of this form are susceptible to (intentional or
inadvertent) fairness gerrymandering, in which a classiﬁer appears to be fair on each individual
group, but badly violates the fairness constraint on one or more structured subgroups deﬁned
over the protected attributes (such as certain combinations of protected attribute values). We
propose instead to demand statistical notions of fairness across exponentially (or inﬁnitely)
many subgroups, deﬁned by a structured class of functions over the protected attributes. This
interpolates between statistical deﬁnitions of fairness, and recently proposed individual no-
tions of fairness, but it raises several computational challenges. It is no longer clear how to
even check or audit a ﬁxed classiﬁer to see if it satisﬁes such a strong deﬁnition of fairness.
We prove that the computational problem of auditing subgroup fairness for both equality of
false positive rates and statistical parity is equivalent to the problem of weak agnostic learn-
ing — which means it is computationally hard in the worst case, even for simple structured
subclasses. However, it also suggests that common heuristics for learning can be applied to
successfully solve the auditing problem in practice.

We then derive two algorithms that provably converge to the best fair distribution over
classiﬁers in a given class, given access to oracles which can optimally solve the agnostic learn-
ing problem. The algorithms are based on a formulation of subgroup fairness as a two-player
zero-sum game between a Learner (the primal player) and an Auditor (the dual player). Both
algorithms compute an equilibrium of this game. We obtain our ﬁrst algorithm by simulat-
ing play of the game by having Learner play an instance of the no-regret Follow the Perturbed
Leader algorithm, and having Auditor play best response. This algorithm provably converges
to an approximate Nash equilibrium (and thus to an approximately optimal subgroup-fair dis-
tribution over classiﬁers) in a polynomial number of steps. We obtain our second algorithm
by simulating play of the game by having both players play Fictitious Play, which enjoys only
provably asymptotic convergence, but has the merit of simplicity and faster per-step compu-
tation. We implement the Fictitious Play version using linear regression as a heuristic oracle,
and show that we can eﬀectively both audit and learn fair classiﬁers on real datasets.

1 Introduction

As machine learning is being deployed in increasingly consequential domains (including policing
[Rudin, 2013], criminal sentencing [Barry-Jester et al., 2015], and lending [Koren, 2016]), the
problem of ensuring that learned models are fair has become urgent.

Approaches to fairness in machine learning can coarsely be divided into two kinds: statistical
and individual notions of fairness. Statistical notions typically ﬁx a small number of protected
demographic groups G (such as racial groups), and then ask for (approximate) parity of some
statistical measure across all of these groups. One popular statistical measure asks for equal-
ity of false positive or negative rates across all groups in G (this is also sometimes referred to as
an equal opportunity constraint [Hardt et al., 2016]). Another asks for equality of classiﬁcation
rates (also known as statistical parity). These statistical notions of fairness are the kinds of fair-
ness deﬁnitions most common in the literature (see e.g. Kamiran and Calders [2012], Hajian and
Domingo-Ferrer [2013], Kleinberg et al. [2017], Hardt et al. [2016], Friedler et al. [2016], Zafar
et al. [2017], Chouldechova [2017]).

One main attraction of statistical deﬁnitions of fairness is that they can in principle be ob-
tained and checked without making any assumptions about the underlying population, and hence
lead to more immediately actionable algorithmic approaches. On the other hand, individual no-
tions of fairness ask for the algorithm to satisfy some guarantee which binds at the individual,
rather than group, level. This often has the semantics that “individuals who are similar” should
be treated “similarly” [Dwork et al., 2012], or “less qualiﬁed individuals should not be favored
over more qualiﬁed individuals” [Joseph et al., 2016]. Individual notions of fairness have attrac-
tively strong semantics, but their main drawback is that achieving them seemingly requires more
assumptions to be made about the setting under consideration.

The semantics of statistical notions of fairness would be signiﬁcantly stronger if they were
deﬁned over a large number of subgroups, thus permitting a rich middle ground between fairness
only for a small number of coarse pre-deﬁned groups, and the strong assumptions needed for
fairness at the individual level. Consider the kind of fairness gerrymandering that can occur when
we only look for unfairness over a small number of pre-deﬁned groups:

Example 1.1. Imagine a setting with two binary features, corresponding to race (say black and white)
and gender (say male and female), both of which are distributed independently and uniformly at random
in a population. Consider a classiﬁer that labels an example positive if and only if it corresponds to a
black man, or a white woman. Then the classiﬁer will appear to be equitable when one considers either
protected attribute alone, in the sense that it labels both men and women as positive 50% of the time, and
labels both black and white individuals as positive 50% of the time. But if one looks at any conjunction
of the two attributes (such as black women), then it is apparent that the classiﬁer maximally violates the
statistical parity fairness constraint. Similarly, if examples have a binary label that is also distributed
uniformly at random, and independently from the features, the classiﬁer will satisfy equal opportunity
fairness with respect to either protected attribute alone, even though it maximally violates it with respect
to conjunctions of two attributes.

We remark that the issue raised by this toy example is not merely hypothetical. In our exper-
iments in Section 5, we show that similar violations of fairness on subgroups of the pre-deﬁned
groups can result from the application of standard machine learning methods applied to real
datasets. To avoid such problems, we would like to be able to satisfy a fairness constraint not
just for the small number of protected groups deﬁned by single protected attributes, but for a

1

combinatorially large or even inﬁnite collection of structured subgroups deﬁnable over protected
attributes.

In this paper, we consider the problem of auditing binary classiﬁers for equal opportunity and
statistical parity, and the problem of learning classiﬁers subject to these constraints, when the
number of protected groups is large. There are exponentially many ways of carving up a popu-
lation into subgroups, and we cannot necessarily identify a small number of these a priori as the
only ones we need to be concerned about. At the same time, we cannot insist on any notion of sta-
tistical fairness for every subgroup of the population: for example, any imperfect classiﬁer could
be accused of being unfair to the subgroup of individuals deﬁned ex-post as the set of individuals
it misclassiﬁed. This simply corresponds to “overﬁtting” a fairness constraint. We note that the
individual fairness deﬁnition of Joseph et al. [2016] (when restricted to the binary classiﬁcation
setting) can be viewed as asking for equalized false positive rates across the singleton subgroups,
containing just one individual each1 — but naturally, in order to achieve this strong deﬁnition of
fairness, Joseph et al. [2016] have to make structural assumptions about the form of the ground
truth. It is, however, sensible to ask for fairness for large structured subsets of individuals: so
long as these subsets have a bounded VC dimension, the statistical problem of learning and au-
diting fair classiﬁers is easy, so long as the dataset is suﬃciently large. This can be viewed as an
interpolation between equal opportunity fairness and the individual “weakly meritocratic” fair-
ness deﬁnition from Joseph et al. [2016], that does not require making any assumptions about the
ground truth. Our investigation focuses on the computational challenges, both in theory and in
practice.

1.1 Our Results

Brieﬂy, our contributions are:

• Formalization of the problem of auditing and learning classiﬁers for fairness with respect to

rich classes of subgroups G.

• Results proving (under certain assumptions) the computational equivalence of auditing G
and (weak) agnostic learning of G. While these results imply theoretical intractability of au-
diting for some natural classes G, they also suggest that practical machine learning heuristics
can be applied to the auditing problem.

• Provably convergent algorithms for learning classiﬁers that are fair with respect to G, based
on a formulation as a two-player zero-sum game between a Learner (the primal player) and
an Auditor (the dual player). We provide two diﬀerent algorithms, both of which are based
on solving for the equilibrium of this game. The ﬁrst provably converges in a polynomial
number of steps and is based on simulation of the game dynamics when the Learner uses
Follow the Perturbed Leader and the Auditor uses best response; the second is only guaranteed
to converge asymptotically but is computationally simpler, and involves both players using
Fictitious Play.

• An implementation and empirical evaluation of the Fictitious Play algorithm demonstrating

its eﬀectiveness on a real dataset in which subgroup fairness is a concern.

1It also asks for equalized false negative rates, and that the false positive rate is smaller than the true positive rate.

Here, the randomness in the “rates” is taken entirely over the randomness of the classiﬁer.

2

In more detail, we start by studying the computational challenge of simply checking whether
a given classiﬁer satisﬁes equal opportunity and statistical parity. Doing this in time linear in the
number of protected groups is simple: for each protected group, we need only estimate a single
expectation. However, when there are many diﬀerent protected attributes which can be combined
to deﬁne the protected groups, their number is combinatorially large2.

We model the problem by specifying a class of functions G deﬁned over a set of d protected
attributes. G deﬁnes a set of protected subgroups. Each function g ∈ G corresponds to the pro-
tected subgroup {x : gi(x) = 1}3. The ﬁrst result of this paper is that for both equal opportunity and
statistical parity, the computational problem of checking whether a classiﬁer or decision-making
algorithm D violates statistical fairness with respect to the set of protected groups G is equivalent
to the problem of agnostically learning G [Kearns et al., 1994], in a strong and distribution-speciﬁc
sense. This equivalence has two implications:

1. First, it allows us to import computational hardness results from the learning theory litera-
ture. Agnostic learning turns out to be computationally hard in the worst case, even for
extremely simple classes of functions G (like boolean conjunctions and linear threshold
functions). As a result, we can conclude that auditing a classiﬁer D for statistical fairness
violations with respect to a class G is also computationally hard. This means we should not
expect to ﬁnd a polynomial time algorithm that is always guaranteed to solve the auditing
problem.

2. However, in practice, various learning heuristics (like boosting, logistic regression, SVMs,
backpropagation for neural networks, etc.) are commonly used to learn accurate classiﬁers
which are known to be hard to learn in the worst case. The equivalence we show between
agnostic learning and auditing is distribution speciﬁc — that is, if on a particular data set, a
heuristic learning algorithm can solve the agnostic learning problem (on an appropriately
deﬁned subset of the data), it can be used also to solve the auditing problem on the same
data set.

These results appear in Section 3.

Next, we consider the problem of learning a classiﬁer that equalizes false positive or negative
rates across all (possibly inﬁnitely many) sub-groups, deﬁned by a class of functions G. As per the
reductions described above, this problem is computationally hard in the worst case.

However, under the assumption that we have an eﬃcient oracles which solves the agnostic
learning problem, we give and analyze algorithms for this problem based on a game-theoretic
formulation. We ﬁrst prove that the optimal fair classiﬁer can be found as the equilibrium of
a two-player, zero-sum game, in which the (pure) strategy space of the “Learner” player corre-
sponds to classiﬁers in H, and the (pure) strategy space of the “Auditor” player corresponds to

2For example, as discussed in a recent Propublica investigation [Angwin and Grassegger, 2017], Facebook policy
protects groups against hate speech if the group is deﬁnable as a conjunction of protected attributes. Under the Face-
book schema, “race” and “gender” are both protected attributes, and so the Facebook policy protects “black women” as
a distinct class, separately from black people and women. When there are d protected attributes, there are 2d protected
groups. As a statistical estimation problem, this is not a large obstacle — we can estimate 2d expectations to error ε so
long as our data set has size O(d/ε2), but there is now a computational problem.

3For example, in the case of Facebook’s policy, the protected attributes include “race, sex, gender identity, religious
aﬃliation, national origin, ethnicity, sexual orientation and serious disability/disease” [Angwin and Grassegger, 2017],
and G represents the class of boolean conjunctions. In other words, a group deﬁned by individuals having any subset
of values for the protected attributes is protected.

3

subgroups deﬁned by G. The best response problems for the two players correspond to agnostic
learning and auditing, respectively. We show that both problems can be solved with a single call
to a cost sensitive classiﬁcation oracle, which is equivalent to an agnostic learning oracle. We then
draw on extant theory for learning in games and no-regret algorithms to derive two diﬀerent al-
gorithms based on simulating game play in this formulation. In the ﬁrst, the Learner employs the
well-studied Follow the Perturbed Leader (FTPL) algorithm on an appropriate linearization of its
best-response problem, while the Auditor approximately best-responds to the distribution over
classiﬁers of the Learner at each step. Since FTPL has a no-regret guarantee, we obtain an algo-
rithm that provably converges in a polynomial number of steps.

While it enjoys strong provable guarantees, this ﬁrst algorithm is randomized (due to the
noise added by FTPL), and the best-response step for the Auditor is polynomial time but compu-
tationally expensive. We thus propose a second algorithm that is deterministic, simpler and faster
per step, based on both players adopting the Fictitious Play learning dynamic. This algorithm has
weaker theoretical guarantees: it has provable convergence only asymptotically, and not in a poly-
nomial number of steps — but is more practical and converges rapidly in practice. The derivation
of these algorithms (and their guarantees) appear in Section 4.

Finally, we implement the Fictitious Play algorithm and demonstrate its practicality by eﬃ-
ciently learning classiﬁers that approximately equalize false positive rates across any group de-
ﬁnable by a linear threshold function on 18 protected attributes in the “Communities and Crime”
dataset. We use simple, fast regression algorithms as heuristics to implement agnostic learn-
ing oracles, and (via our reduction from agnostic learning to auditing) auditing oracles. Our
results suggest that it is possible in practice to learn fair classiﬁers with respect to a large class
of subgroups that still achieve non-trivial error. Full details are contained in Section 5, and for a
substantially more comprehensive empirical investigation of our method we direct the interested
reader to Kearns et al. [2018].

1.2 Further Related Work

Independent of our work, H´ebert-Johnson et al. [2017] also consider a related and complementary
notion of fairness that they call “multicalibration”. In settings in which one wishes to train a real-
valued predictor, multicalibration can be considered the “calibration” analogue for the deﬁnitions
of subgroup fairness that we give for false positive rates, false negative rates, and classiﬁcation
rates. For a real-valued predictor, calibration informally requires that for every value v ∈ [0, 1]
predicted by an algorithm, the fraction of individuals who truly have a positive label in the subset
of individuals on which the algorithm predicted v should be approximately equal to v. Multi-
calibration asks for approximate calibration on every set deﬁned implicitly by some circuit in a
set G. H´ebert-Johnson et al. [2017] give an algorithmic result that is analogous to the one we give
for learning subgroup fair classiﬁers: a polynomial time algorithm for learning a multi-calibrated
predictor, given an agnostic learning algorithm for G. In addition to giving a polynomial-time
algorithm, we also give a practical variant of our algorithm (which is however only guaranteed to
converge in the limit) that we use to conduct empirical experiments on real data.

Thematically, the most closely related piece of prior work is Zhang and Neill [2016], who
also aim to audit classiﬁcation algorithms for discrimination in subgroups that have not been
pre-deﬁned. Our work diﬀers from theirs in a number of important ways. First, we audit the
algorithm for common measures of statistical unfairness, whereas Zhang and Neill [2016] design
a new measure compatible with their particular algorithmic technique. Second, we give a for-

4

mal analysis of our algorithm. Finally, we audit with respect to subgroups deﬁned by a class of
functions G, which we can take to have bounded VC dimension, which allows us to give formal
out-of-sample guarantees. Zhang and Neill [2016] attempt to audit with respect to all possible
sub-groups, which introduces a severe multiple-hypothesis testing problem, and risks overﬁtting.
Most importantly we give actionable algorithms for learning subgroup fair classiﬁers, whereas
Zhang and Neill [2016] restrict attention to auditing.

Technically, the most closely related piece of work (and from which we take inspiration for
our algorithm in Section 4) is Agarwal et al. [2017], who show that given access to an agnostic
learning oracle for a class H, there is an eﬃcient algorithm to ﬁnd the lowest-error distribution
over classiﬁers in H subject to equalizing false positive rates across polynomially many subgroups.
Their algorithm can be viewed as solving the same zero-sum game that we solve, but in which the
“subgroup” player plays gradient descent over his pure strategies, one for each sub-group. This
ceases to be an eﬃcient or practical algorithm when the number of subgroups is large, as is our
case. Our main insight is that an agnostic learning oracle is suﬃcient to have the both players
play “ﬁctitious play”, and that there is a transformation of the best response problem such that an
agnostic learning algorithm is enough to eﬃciently implement follow the perturbed leader.

There is also other work showing computational hardness for fair learning problems. Most no-
tably, Woodworth et al. [2017] show that ﬁnding a linear threshold classiﬁer that approximately
minimizes hinge loss subject to equalizing false positive rates across populations is computation-
ally hard (assuming that refuting a random k-XOR formula is hard). In contrast, we show that
even checking whether a classiﬁer satisﬁes a false positive rate constraint on a particular data set
is computationally hard (if the number of subgroups on which fairness is desired is too large to
enumerate).

2 Model and Preliminaries

(cid:48)), y), where x ∈ X denotes a vector of
We model each individual as being described by a tuple ((x, x
(cid:48) ∈ X (cid:48) denotes a vector of unprotected attributes, and y ∈ {0, 1} denotes a label.
protected attributes, x
Note that in our formulation, an auditing algorithm not only may not see the unprotected at-
(cid:48) may represent proprietary
tributes x
features or consumer data purchased by a credit scoring company.

(cid:48), it may not even be aware of their existence. For example, x

We will write X = (x, x

(cid:48)) to denote the joint feature vector. We assume that points (X, y) are
drawn i.i.d. from an unknown distribution P . Let D be a decision making algorithm, and let D(X)
denote the (possibly randomized) decision induced by D on individual (X, y). We restrict attention
in this paper to the case in which D makes a binary classiﬁcation decision: D(X) ∈ {0, 1}. Thus we
alternately refer to D as a classiﬁer. When auditing a ﬁxed classiﬁer D, it will be helpful to make
reference to the distribution over examples (X, y) together with their induced classiﬁcation D(X).
Let Paudit(D) denote the induced target joint distribution over the tuple (x, y, D(X)) that results from
(cid:48)) but
sampling (x, x
(cid:48). Note that the randomness here is over both the randomness of
not the unprotected attributes x
P , and the potential randomness of the classiﬁer D.

, y) ∼ P , and providing x, the true label y, and the classiﬁcation D(X) = D(x, x

(cid:48)

We will be concerned with learning and auditing classiﬁers D satisfying two common statis-
tical fairness constraints: equality of classiﬁcation rates (also known as statistical parity), and
equality of false positive rates (also known as equal opportunity). Auditing for equality of false
negative rates is symmetric and so we do not explicitly consider it. Each fairness constraint is

5

deﬁned with respect to a set of protected groups. We deﬁne sets of protected groups via a family
of indicator functions G for those groups, deﬁned over protected attributes. Each g : X → {0, 1} ∈ G
has the semantics that g(x) = 1 indicates that an individual with protected features x is in group
g.

Deﬁnition 2.1 (Statistical Parity (SP) Subgroup Fairness). Fix any classiﬁer D, distribution P , col-
lection of group indicators G, and parameter γ ∈ [0, 1]. For each g ∈ G, deﬁne

αSP (g, P ) = Pr
P

[g(x) = 1]

and, βSP (g, D, P ) = |SP(D) − SP(D, g)| ,

where SP(D) = PrP ,D[D(X) = 1] and SP(D, g) = PrP ,D[D(X) = 1|g(x) = 1] denote the overall acceptance
rate of D and the acceptance rate of D on group g respectively. We say that D satisﬁes γ-statistical
parity (SP) Fairness with respect to P and G if for every g ∈ G

We will sometimes refer to SP(D) as the SP base rate.

αSP (g, P ) βSP (g, D, P ) ≤ γ.

Remark 2.2. Note that our deﬁnition references two approximation parameters, both of which are im-
portant. We are allowed to ignore a group g if it (or its complement) represent only a small fraction
of the total probability mass. The parameter α governs how small a fraction of the population we are
allowed to ignore. Similarly, we do not require that the probability of a positive classiﬁcation in every
subgroup is exactly equal to the base rate, but instead allow deviations up to β. Both of these approxi-
mation parameters are necessary from a statistical estimation perspective. We control both of them with
a single parameter γ.

Deﬁnition 2.3 (False Positive (FP) Subgroup Fairness). Fix any classiﬁer D, distribution P , collection
of group indicators G, and parameter γ ∈ [0, 1]. For each g ∈ G, deﬁne

αFP (g, P ) = Pr
P

[g(x) = 1, y = 0]

and, βFP (g, D, P ) = |FP(D) − FP(D, g)|

where FP(D) = PrD,P [D(X) = 1 | y = 0] and FP(D, g) = PrD,P [D(X) = 1 | g(x) = 1, y = 0] denote the
overall false-positive rate of D and the false-positive rate of D on group g respectively.

We say D satisﬁes γ-False Positive (FP) Fairness with respect to P and G if for every g ∈ G

We will sometimes refer to FP(D) FP-base rate.

αFP (g, P ) βFP (g, D, P ) ≤ γ.

Remark 2.4. This deﬁnition is symmetric to the deﬁnition of statistical parity fairness, except that the
parameter α is now used to exclude any group g such that negative examples (y = 0) from g (or its
complement) have probability mass less than α. This is again necessary from a statistical estimation
perspective.

For either statistical parity and false positive fairness, if the algorithm D fails to satisfy the
γ-fairness condition, then we say that D is γ-unfair with respect to P and G. We call any subgroup
g which witnesses this unfairness an γ-unfair certiﬁcate for (D, P ).

An auditing algorithm for a notion of fairness is given sample access to Paudit(D) for some clas-
siﬁer D. It will either deem D to be fair with respect to P , or will else produce a certiﬁcate of
unfairness.

6

Deﬁnition 2.5 (Auditing Algorithm). Fix a notion of fairness (either statistical parity or false positive
(cid:48) ∈ (0, 1) such that
fairness), a collection of group indicators G over the protected features, and any δ, γ, γ
(cid:48))-auditing algorithm for G with respect to distribution P is an algorithm A such that
(cid:48) ≤ γ. A (γ, γ
γ
for any classiﬁer D, when given access the distribution Paudit(D), A runs in time poly(1/γ
, log(1/δ)),
and with probability 1 − δ, outputs a γ
-unfair certiﬁcate for D whenever D is γ-unfair with respect to
(cid:48)
P and G. If D is γ

-fair, A will output “fair”.

(cid:48)

(cid:48)

As we will show, our deﬁnition of auditing is closely related to weak agnostic learning.

Deﬁnition 2.6 (Weak Agnostic Learning [Kearns et al., 1994, Kalai et al., 2008]). Let Q be a dis-
(cid:48) ∈ (0, 1/2) such that ε ≥ ε
tribution over X × {0, 1} and let ε, ε
. We say that the function class G is
(cid:48))-weakly agnostically learnable under distribution Q if there exists an algorithm L such that
(ε, ε
, 1/δ), and with probability 1 − δ, outputs a
when given sample access to Q, L runs in time poly(1/ε
hypothesis h ∈ G such that

(cid:48)

(cid:48)

err(f , Q) ≤ 1/2 − ε =⇒ err(h, Q) ≤ 1/2 − ε

(cid:48)

.

min
f ∈G

where err(h, Q) = Pr(x,y)∼Q[h(x) (cid:44) y].

Cost-Sensitive Classiﬁcation.
In this paper, we will also give reductions to cost-sensitive classi-
ﬁcation (CSC) problems. Formally, an instance of a CSC problem for the class H is given by a set
of n tuples {(Xi, c0
i corresponds to the cost for predicting label (cid:96) on point Xi.
Given such an instance as input, a CSC oracle ﬁnds a hypothesis ˆh ∈ H that minimizes the total
cost across all points:

i=1 such that c(cid:96)

i , c1

i )}n

ˆh ∈ argmin

h∈H

n(cid:88)

i=1

[h(Xi)c1

i + (1 − h(Xi))c0
i ]

(1)

A crucial property of a CSC problem is that the solution is invariant to translations of the costs.

Claim 2.7. Let {(Xi, c0
a1, a2, . . . , an

i )}n
∈ R such that ˜c(cid:96)

i , c1

i=1 be a CSC instance, and {( ˜c0
i = c(cid:96)
i + ai for all i and (cid:96). Then

i , ˜c1

i )} be a set of new costs such that there exist

argmin
h∈H

n(cid:88)

i=1

[h(Xi)c1

i + (1 − h(Xi))c0

i ] = argmin

[h(Xi) ˜c1

i + (1 − h(Xi)) ˜c0
i ]

n(cid:88)

i=1

h∈H

Remark 2.8. We note that cost-sensitive classiﬁcation is polynomially equivalent to agnostic learn-
ing Zadrozny et al. [2003]. We give both deﬁnitions above because when describing our results for
auditing, we wish to directly appeal to known hardness results for weak agnostic learning, but it is more
convenient to describe our algorithms via oracles for cost-sensitive classiﬁcation.

Follow the Perturbed Leader. We will make use of the Follow the Perturbed Leader (FTPL) algo-
rithm as a no-regret learner for online linear optimization problems [Kalai and Vempala, 2005].
To formalize the algorithm, consider S ⊂ {0, 1}d to be a set of “actions” for a learner in an on-
line decision problem. The learner interacts with an adversary over T rounds, and in each round
t, the learner (randomly) chooses some action at ∈ S, and the adversary chooses a loss vector
(cid:96)t ∈ [−M, M]d. The learner incurs a loss of (cid:104)(cid:96)t, at(cid:105) at round t.

7

FTPL is a simple algorithm that in each round perturbs the cumulative loss vector over the pre-
s<t (cid:96)s, and chooses the action that minimizes loss with respect to the perturbed
vious rounds (cid:96) =
cumulative loss vector. We present the full algorithm in Algorithm 1, and its formal guarantee in
Theorem 2.9.

(cid:80)

U be the uniform distribution over [0, 1]d, and let a1 ∈ S be

Algorithm 1 Follow the Perturbed Leader (FTPL) Algorithm

Input: Loss bound M, action set S ∈ {0, 1}d
Initialize: Let η = (1/M)

, D

(cid:113)

1√

dT

arbitrary.
For t = 1, . . . , T :

Play action at; Observe loss vector (cid:96)t and suﬀer loss (cid:104)(cid:96)t, at(cid:105).
Update:

at+1 = argmin

η

(cid:104)(cid:96)s, a(cid:105) + (cid:104)ξt, a(cid:105)





(cid:88)

s≤t

a∈S





where ξt is drawn independently for each t from the distribution D

U .

Theorem 2.9 (Kalai and Vempala [2005]). For any sequence of loss vectors (cid:96)1, . . . , (cid:96)T , the FTPL algo-
rithm has regret





E

T(cid:88)



(cid:104)(cid:96)t, at(cid:105)

− min
a∈S

T(cid:88)

(cid:104)(cid:96)t, a(cid:105) ≤ 2d5/4M

T

√

t=1
where the randomness is taken over the perturbations ξt across rounds.

t=1

2.1 Generalization Error

In this section, we observe that the error rate of a classiﬁer D, as well as the degree to which it vio-
lates γ-fairness (for both statistical parity and false positive rates) can be accurately approximated
with the empirical estimates for these quantities on a dataset (drawn i.i.d. from the underlying
distribution P ) so long as the dataset is suﬃciently large. Once we establish this fact, since our
main interest is in the computational problem of auditing and learning, in the rest of the paper,
we assume that we have direct access to the underlying distribution (or equivalently, that the
empirical data deﬁnes the distribution of interest), and do not make further reference to sample
complexity or overﬁtting issues.

A standard VC dimension bound (see, e.g. Kearns and Vazirani [1994]) states:

Theorem 2.10. Fix a class of functions H. For any distribution P , let S ∼ P m be a dataset consisting
of m examples (Xi, yi) sampled i.i.d. from P . Then for any 0 < δ < 1, with probability 1 − δ, for every
h ∈ H, we have:

|err(h, P ) − err(h, S)| ≤ O

(cid:114)





VCDIM(H) log m + log(1/δ)
m





where err(h, S) = 1
m

(cid:80)m

i=1

1[h(Xi) (cid:44) yi].

8

The above theorem implies that so long as m ≥ ˜O(VCDIM(H)/ε2), then minimizing error over
the empirical sample S suﬃces to minimize error up to an additive ε term on the true distribution
P . Below, we give two analogous statements for fairness constraints:

Theorem 2.11 (SP Uniform Convergence). Fix a class of functions H and a class of group indicators
G. For any distribution P , let S ∼ P m be a dataset consisting of m examples (Xi, yi) sampled i.i.d. from
P . Then for any 0 < δ < 1, with probability 1 − δ, for every h ∈ H and g ∈ G

(cid:12)(cid:12)(cid:12)αSP (g, P

S) βSP (g, h, P

S) − αSP (g, P ) βSP (g, h, P )

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

where P

S denotes the empirical distribution over the realized sample S.

Similarly:

Theorem 2.12 (FP Uniform Convergence). Fix a class of functions H and a class of group indicators
G. For any distribution P , let S ∼ P m be a dataset consisting of m examples (Xi, yi) sampled i.i.d. from
P . Then for any 0 < δ < 1, with probability 1 − δ, for every h ∈ H and g ∈ G, we have:

(cid:12)(cid:12)(cid:12)αFP (g, P ) βFP (g, D, P ) − αFP (g, P ) βFP (g, D, P )

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

where P

S denotes the empirical distribution over the realized sample S.

These theorems together imply that for both SP and FP subgroup fairness, the degree to which
a group g violates the constraint of γ-fairness can be estimated up to error ε, so long as m ≥
˜O((VCDIM(H) + VCDIM(G))/ε2). The proofs can be found in Appendix B.









3 Equivalence of Auditing and Weak Agnostic Learning

In this section, we give a reduction from the problem of auditing both statistical parity and false
positive rate fairness, to the problem of agnostic learning, and vice versa. This has two implica-
tions. The main implication is that, from a worst-case analysis point of view, auditing is compu-
tationally hard in almost every case (since it inherits this pessimistic state of aﬀairs from agnostic
learning). However, worst-case hardness results in learning theory have not prevented the suc-
cessful practice of machine learning, and there are many heuristic algorithms that in real-world
cases successfully solve “hard” agnostic learning problems. Our reductions also imply that these
heuristics can be used successfully as auditing algorithms, and we exploit this in the development
of our algorithmic results and their experimental evaluation.

We make the following mild assumption on the class of group indicators G, to aid in our reduc-
tions. It is satisﬁed by most natural classes of functions, but is in any case essentially without loss
of generality (since learning negated functions can be simulated by learning the original function
class on a dataset with ﬂipped class labels).

Assumption 3.1. We assume the set of group indicators G satisﬁes closure under negation: for any
g ∈ G, we also have ¬g ∈ G.

Recalling that X = (x, x

(cid:48)) and the following notions will be useful for describing our results:

9

• SP(D) = PrP ,D[D(X) = 1] and FP(D) = PrD,P [D(X) = 1 | y = 0].
• αSP (g, P ) = PrP [g(x) = 1] and αFP (g, P ) = PrP [g(x) = 1, y = 0].
• βSP (g, D, P ) = |SP(D) − SP(D, g)| and βFP (g, D, P ) = |FP(D) − FP(D, g)|.

• P D: the marginal distribution on (x, D(X)).

• P D

y=0: the conditional distribution on (x, D(X)), conditioned on y = 0.

We will think about these as the target distributions for a learning problem: i.e. the problem of
learning to predict D(X) from only the protected features x. We will relate the ability to agnosti-
cally learn on these distributions, to the ability to audit D given access to the original distribution
Paudit(D).

3.1 Statistical Parity Fairness

We give our reduction ﬁrst for SP subgroup fairness. The reduction for FP subgroup fairness
will follow as a corollary, since auditing for FP subgroup fairness can be viewed as auditing for
statistical parity fairness on the subset of the data restricted to y = 0.

Theorem 3.2. Fix any distribution P , and any set of group indicators G. Then for any γ, ε > 0, the
following relationships hold:

• If there is a (γ/2, (γ/2 − ε)) auditing algorithm for G for all D such that SP(D) = 1/2, then the

class G is (γ, γ/2 − ε)-weakly agnostically learnable under P D.

• If G is (γ, γ − ε)-weakly agnostically learnable under distribution P D for all D such that SP(D) =

1/2, then there is a (γ, (γ − ε)/2) auditing algorithm for G for SP fairness under P .

We will prove Theorem 3.2 in two steps. First, we show that any unfair certiﬁcate f for D has

non-trivial error for predicting the decision made by D from the sensitive attributes.

Lemma 3.3. Suppose that the base rate SP(D) ≤ 1/2 and there exists a function f such that

αSP (g, P ) βSP (g, D, P ) = γ.

max{Pr[D(X) = f (x)], Pr[D(X) = ¬f (x)]} ≥ SP(D) + γ.

Proof. To simplify notations, let b = SP(D) denote the base rate, α = αSP and β = βSP . First, observe
that either Pr[D(X) = 1 | f (x) = 1] = b + β or Pr[D(X) = 1 | f (x) = 1] = b − β holds.

In the ﬁrst case, we know Pr[D(X) = 1 | f (x) = 0] < b, and so Pr[D(X) = 0 | f (x) = 0] > 1 − b. It

Then

follows that

Pr[D(X) = f (x)] = Pr[D(X) = f (x) = 1] + Pr[D(X) = f (x) = 0]

= Pr[D(X) = 1 | f (x) = 1] Pr[f (x) = 1] + Pr[D(X) = 0 | f (x) = 0] Pr[f (x) = 0]
> α(b + β) + (1 − α)(1 − b)
= (α − 1)b + (1 − α)(1 − b) + b + αβ
= (1 − α)(1 − 2b) + b + αβ.

10

In the second case, we have Pr[D(X) = 0 | f (x) = 1] = (1 − b) + β and Pr[D(X) = 1 | f (x) = 0] > b. We
can then bound

Pr[D(X) = f (x)] = Pr[D(X) = 1 | f (x) = 0] Pr[f (x) = 0] + Pr[D(X) = 0 | f (x) = 1] Pr[f (x) = 1]

> (1 − α)b + α(1 − b + β) = α(1 − 2b) + b + αβ.

In both cases, we have (1 − 2b) ≥ 0 by our assumption on the base rate. Since α ∈ [0, 1], we know
max{Pr[D(X) = f (x)], Pr[D(X) = ¬f (x)]} ≥ b + αβ = b + γ

which recovers our bound.

In the next step, we show that if there exists any function f that accurately predicts the de-
cisions made by the algorithm D, then either f or ¬f can serve as an unfairness certiﬁcate for
D.
Lemma 3.4. Suppose that the base rate SP(D) ≥ 1/2 and there exists a function f such that Pr[D(X) =
f (x)] ≥ SP(D) + γ for some value γ ∈ (0, 1/2). Then there exists a function g such that

αSP (g, P ) βSP (g, D, P ) ≥ γ/2,

where g ∈ {f , ¬f }.

Proof. Let b = SP(D). We can expand Pr[D(X) = f (x)] as follows:

Pr[D(X) = f (x)] = Pr[D(X) = f (x) = 1] + Pr[D(X) = f (x) = 0]

= Pr[D(X) = 1 | f (x) = 1] Pr[f (x) = 1] + Pr[D(X) = 0 | f (x) = 0] Pr[f (x) = 0]

This means

Pr[D(X) = f (x)] − b

= (Pr[D(X) = 1 | f (x) = 1] − b) Pr[f (x) = 1] + (Pr[D(X) = 0 | f (x) = 0] − b) Pr[f (x) = 0] ≥ γ
Suppose that (Pr[D(X) = 1 | f (x) = 1] − b) Pr[f (x) = 1] ≥ γ/2, then our claim holds with g = f . Sup-
pose not, then we must have

(Pr[D(X) = 0 | f (x) = 0] − b) Pr[f (x) = 0] = ((1 − b) − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ γ/2

Note that by our assumption b ≥ (1 − b). This means

(b − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ ((1 − b) − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ γ/2

which implies that our claim holds with g = ¬f .

Proof of Theorem 3.2. Suppose that the class G satisﬁes minf ∈G err(f , P D) ≤ 1/2−γ. Then by Lemma 3.4,
there exists some g ∈ G such that Pr[g(x) = 1]| Pr[D(X) = 1 | g(x) = 1] − SP(D)| ≥ γ/2. By the as-
(cid:48) ∈ G that is an
sumption of auditability, we can then use the auditing algorithm to ﬁnd a group g
(cid:48) predicts D with an
(γ/2 − ε)-unfair certiﬁcate of D. By Lemma 3.3, we know that either g
accuracy of at least 1/2 + (γ/2 − ε).

(cid:48) or ¬g

In the reverse direction, consider the auditing problem on the classiﬁer D. We can treat each
pair (x, D(X)) as a labelled example and learn a hypothesis in G that approximates the decisions
made by D. Suppose that D is γ-unfair. Then by Lemma 3.3, we know that there exists some g ∈ G
such that Pr[D(X) = g(x)] ≥ 1/2 + γ. Therefore, the weak agnostic learning algorithm from the
(cid:48)(x)] ≥ 1/2 + (γ − ε). By Lemma 3.4,
hypothesis of the theorem will return some g
we know g

(cid:48) is a (γ − ε)/2-unfair certiﬁcate for D.

(cid:48) with Pr[D(X) = g

(cid:48) or ¬g

11

3.2 False Positive Fairness

A corollary of the above reduction is an analogous equivalence between auditing for FP subgroup
fairness and agnostic learning. This is because a FP fairness constraint can be viewed as a statis-
tical parity fairness constraint on the subset of the data such that y = 0. Therefore, Theorem 3.2
implies the following:

Corollary 3.5. Fix any distribution P , and any set of group indicators G. The following two relation-
ships hold:

• If there is a (γ/2, (γ/2 − ε)) auditing algorithm for G for all D such that FP(D) = 1/2, then the

class G is (γ, γ/2 − ε)-weakly agnostically learnable under P D

y=0.

• If G is (γ, γ −ε)–weakly agnostically learnable under distribution P D

y=0 for all D such that FP(D) =
1/2, then there is a (γ, (γ − ε)/2) auditing algorithm for FP subgroup fairness for G under distri-
bution P .

3.3 Worst-Case Intractability of Auditing

While we shall see in subsequent sections that the equivalence given above has positive algo-
rithmic and experimental consequences, from a purely theoretical perspective the reduction of
agnostic learning to auditing has strong negative worst-case implications. More precisely, we can
import a long sequence of formal intractability results for agnostic learning to obtain:

Theorem 3.6. Under standard complexity-theoretic intractability assumptions, for G the classes of con-
junctions of boolean attributes, linear threshold functions, or bounded-degree polynomial threshold func-
tions, there exist distributions P such that the auditing problem cannot be solved in polynomial time, for
either statistical parity or false positive fairness.

The proof of this theorem follows from Theorem 3.2, Corollary 3.5, and the following negative
results from the learning theory literature. Feldman et al. [2012] show a strong negative result
for weak agnostic learning for conjunctions: given a distribution on labeled examples from the
hypercube such that there exists a monomial (or conjunction) consistent with (1−ε)-fraction of the
examples, it is NP-hard to ﬁnd a halfspace that is correct on (1/2 + ε)-fraction of the examples, for
arbitrary constant ε > 0. Diakonikolas et al. [2011] show that under the Unique Games Conjecture,
no polynomial-time algorithm can ﬁnd a degree-d polynomial threshold function (PTF) that is
consistent with (1/2 + ε) fraction of a given set of labeled examples, even if there exists a degree-d
PTF that is consistent with a (1 − ε) fraction of the examples. Diakonikolas et al. [2011] also show
that it is NP-Hard to ﬁnd a degree-2 PTF that is consistent with a (1/2 + ε) fraction of a given set
of labeled examples, even if there exists a halfspace (degree-1 PTF) that is consistent with a (1 − ε)
fraction of the examples.

While Theorem 3.6 shows that certain natural subgroup classes G yield intractable auditing
problems in the worst case, in the rest of the paper we demonstrate that eﬀective heuristics for
this problem on speciﬁc (non-worst case) distributions can be used to derive an eﬀective and
practical learning algorithm for subgroup fairness.

12

4 A Learning Algorithm Subject to Fairness Constraints G

In this section, we present an algorithm for training a (randomized) classiﬁer that satisﬁes false-
positive subgroup fairness simultaneously for all protected subgroups speciﬁed by a family of
group indicator functions G. All of our techniques also apply to a statistical parity or false negative
rate constraint.

Let S denote a set of n labeled examples {zi = (xi, x

i=1, and let P denote the empirical
distribution over this set of examples. Let H be a hypothesis class deﬁned over both the protected
and unprotected attributes, and let G be a collection of group indicators over the protected at-
tributes. We assume that H contains a constant classiﬁer (which implies that there is at least one
fair classiﬁer to be found, for any distribution).

i), yi)}n

Our goal will be to ﬁnd the distribution over classiﬁers from H that minimizes classiﬁcation
error subject to the fairness constraint over G. We will design an iterative algorithm that, when
given access to a CSC oracle, computes an optimal randomized classiﬁer in polynomial time.

(cid:48)

Let D denote a probability distribution over H. Consider the following Fair ERM (Empirical

Risk Minimization) problem:

such that ∀g ∈ G

E
h∼D

[err(h, P )]

min
D∈∆H
αFP (g, P ) βFP (g, D, P ) ≤ γ.

(2)

(3)

(cid:48)) (cid:44) y], and the quantities αFP and βFP are deﬁned in Deﬁnition 2.3.
where err(h, P ) = PrP [h(x, x
We will write OPT to denote the objective value at the optimum for the Fair ERM problem, that is
the minimum error achieved by a γ-fair distribution over the class H.

Observe that the optimization is feasible for any distribution P : the constant classiﬁers that
labels all points 1 or 0 satisfy all subgroup fairness constraints. At the moment, the number of
decision variables and constraints may be inﬁnite (if H and G are inﬁnite hypothesis classes), but
we will address this momentarily.

Assumption 4.1 (Cost-Sensitive Classiﬁcation Oracle). We assume our algorithm has access to the
cost-sensitive classication oracles CSC(H) and CSC(G) over the classes H and G.

Our main theoretical result is an computationally eﬃcient oracle-based algorithm for solving

the Fair ERM problem.

Theorem 4.2. Fix any ν, δ ∈ (0, 1). Then given an input of n data points and accuracy parameters ν, δ
and access to oracles CSC(H) and CSC(G), there exists an algorithm runs in polynomial time, and with
probability at least 1 − δ, output a randomized classiﬁer ˆD such that err( ˆD, P ) ≤ OPT +ν, and for any
g ∈ G, the fairness constraint violations satisﬁes

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ + O(ν).

Overview of our solution. We present our solution in steps:

• Step 1: Fair ERM as LP. First, we rewrite the Fair ERM problem as a linear program with
ﬁnitely many decision variables and constraints even when H and G are inﬁnite. To do this,
we take advantage of the fact that Sauer’s Lemma lets us bound the number of labellings
that any hypothesis class H of bounded VC dimension can induce on any ﬁxed dataset.

13

The LP has one variable for each of these possible labellings, rather than one variable for
each hypothesis. Moreover, again by Sauer’s Lemma, we have one constraint for each of the
ﬁnitely many possible subgroups induced by G on the ﬁxed dataset, rather than one for each
of the (possibly inﬁnitely many) subgroups deﬁnable over arbitrary datasets. This step is
important — it will guarantee that strong duality holds.

• Step 2: Formulation as Game. We then derive the partial Lagrangian of the LP, and note
that computing an approximately optimal solution to this LP is equivalent to ﬁnding an ap-
proximate minmax solution for a corresponding zero-sum game, in which the payoﬀ func-
tion U is the value of the Lagrangian. The pure strategies of the primal or “Learner” player
correspond to classiﬁers h ∈ H, and the pure strategies of the dual or “Auditor” player cor-
respond to subgroups g ∈ G. Intuitively, the Learner is trying to minimize the sum of the
prediction error and a fairness penalty term (given by the Lagrangian), and the Auditor is
trying to penalize the fairness violation of the Learner by ﬁrst identifying the subgroup with
the greatest fairness violation and putting all the weight on the dual variable corresponding
to this subgroup. In order to reason about convergence, we restrict the set of dual variables
to lie in a bounded set: C times the probability simplex. C is a parameter that we have
to set in the proof of our theorem to give the best theoretical guarantees — but it is also a
parameter that we will vary in the experimental section.

• Step 3: Best Responses as CSC. We observe that given a mixed strategy for the Auditor,
the best response problem of the Learner corresponds to a CSC problem. Similarly, given a
mixed strategy for the Learner, the best response problem of the Auditor corresponds to an
auditing problem (which can be represented as a CSC problem). Hence, if we have oracles
for solving CSC problems, we can compute best responses for both players, in response to
arbitrary mixed strategies of their opponents.

• Step 4: FTPL for No-Regret. Finally, we show that the ability to compute best responses
for each player is suﬃcient to implement dynamics known to converge quickly to equilib-
rium in zero-sum games. Our algorithm has the Learner play Follow the Perturbed Leader
(FTPL) Kalai and Vempala [2005], which is a no-regret algorithm, against an Auditor who at
every round best responds to the learner’s mixed strategy. By the seminal result of Freund
and Schapire [1996], the average plays of both players converge to an approximate equilib-
rium. In order to implement this in polynomial time, we need to represent the loss of the
learner as a low-dimensional linear optimization problem. To do so, we ﬁrst deﬁne an ap-
propriately translated CSC problem for any mixed strategy λ by the Auditor, and cast it as
a linear optimization problem.

4.1 Rewriting the Fair ERM Problem

To rewrite the Fair ERM problem, we note that even though both G and H can be inﬁnite sets,
the sets of possible labellings on the data set S induced by these classes are ﬁnite. More formally,
we will write G(S) and H(S) to denote the set of all labellings on S that are induced by G and H
respectively, that is

G(S) = {(g(x1), . . . , g(xn)) | g ∈ G}

and,

H(S) = {(h(X1), . . . , h(Xn)) | h ∈ H}

We can bound the cardinalities of G(S) and H(S) using Sauer’s Lemma.

14

Lemma 4.3 (Sauer’s Lemma (see e.g. Kearns and Vazirani [1994])). Let S be a data set of size n. Let
d1 = VCDIM(H) and d2 = VCDIM(G) be the VC-dimensions of the two classes. Then
(cid:17)

(cid:17)

|H(S)| ≤ O

(cid:16)
nd1

and

|G(S)| ≤ O

(cid:16)
nd2

.

Given this observation, we can then consider an equivalent optimization problem where the
distribution D is over the set of labellings in H(S), and the set of subgroups are deﬁned by the
labellings in G(S). We will view each g in G(S) as a Boolean function.

To simplify notations, we will deﬁne the following “fairness violation” functions for any g ∈ G

and any h ∈ H:

Φ+(h, g) ≡ αFP (g, P ) (FP(h) − FP(h, g)) − γ
Φ−(h, g) ≡ αFP (g, P ) (FP(h, g) − FP(h)) − γ

Moreover, for any distribution D over H, for any sign • ∈ {+, −}

Φ•(D, g) = E
h∼D

[Φ•(h, g)] .

Claim 4.4. For any g ∈ G, h ∈ H, and any ν > 0,

max{Φ+(D, g), Φ−(D, g)} ≤ ν

if and only if αFP (g, P ) βFP (g, D, P ) ≤ γ + ν.

Thus, we will focus on the following equivalent optimization problem.

such that for each g ∈ G(S) :

min
D∈∆H(S)

[err(h, P )]

E
h∼D
Φ+(D, g) ≤ 0
Φ−(D, g) ≤ 0

For each pair of constraints (7) and (8), corresponding to a group g ∈ G(S), we introduce a pair

of dual variables λ+

g and λ

−
g . The partial Lagrangian of the linear program is the following:

L(D, λ) = E
h∼D

[err(h, P )] +

(cid:88)

(cid:16)
λ+

g∈G(S)

−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:17)

By Sion’s minmax theorem [Sion, 1958], we have

min
D∈∆H(S)

max
λ∈R2|G(S)|
+

L(p, λ) = max
λ∈R2|G(S)|

+

min
D∈∆H(S)

L(p, λ) = OPT

where OPT denotes the optimal objective value in the fair ERM problem. Similarly, the distri-
L(D, λ) corresponds to an optimal feasible solution to the fair ERM linear
bution arg minD maxλ
program. Thus, ﬁnding an optimal solution for the fair ERM problem reduces to computing a
minmax solution for the Lagrangian. Our algorithms will both compute such a minmax solution
by iteratively optimizing over both the primal variables D and dual variables λ. In order to guar-
antee convergence in our optimization, we will restrict the dual space to the following bounded
set:

Λ = {λ ∈ R2|G(S)|

+

| (cid:107)λ(cid:107)

1

≤ C}.

15

(4)

(5)

(6)

(7)

(8)

where C will be a parameter of our algorithm. Since Λ is a compact and convex set, the minmax
condition continues to hold [Sion, 1958]:

min
D∈∆H(S)

max
λ∈Λ

L(D, λ) = max
λ∈Λ

min
D∈∆H(S)

L(D, λ)

(9)

If we knew an upper bound C on the (cid:96)1 norm of the optimal dual solution, then this restriction
on the dual solution would not change the minmax solution of the program. We do not in general
know such a bound. However, we can show that even though we restrict the dual variables to
lie in a bounded set, any approximate minmax solution to Equation (9) is also an approximately
optimal and approximately feasible solution to the original fair ERM problem.

Theorem 4.5. Let ( ˆD, ˆλ) be a ν-approximate minmax solution to the Λ-bounded Lagrangian problem
in the sense that

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν and, L( ˆD, ˆλ) ≥ max
λ∈Λ

L( ˆD, λ) − ν.

Then err( ˆD, P ) ≤ OPT +2ν and for any g ∈ G(S),

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C

.

4.2 Zero-Sum Game Formulation

To compute an approximate minmax solution, we will ﬁrst view Equation (9) as the following two
player zero-sum matrix game. The Learner (or the minimization player) has pure strategies cor-
responding to H, and the Auditor (or the maximization player) has pure strategies corresponding
to the set of vertices Λpure in Λ — more precisely, each vertex or pure strategy either is the all
zero vector or consists of a choice of a g ∈ G(S), along with the sign + or − that the corresponding
g-fairness constraint will have in the Lagrangian. More formally, we write

•
g = C | g ∈ G(S), • ∈ {±}} ∪ {0}
Λpure = {λ ∈ Λ with λ

Even though the number of pure strategies scales linearly with |G(S)|, our algorithm will never
need to actually represent such vectors explicitly. Note that any vector in Λ can be written as a
convex combination of the maximization player’s pure strategies, or in other words: as a mixed
strategy for the Auditor. For any pair of actions (h, λ) ∈ H × Λpure, the payoﬀ is deﬁned as
(cid:88)

U (h, λ) = err(h, P ) +

(cid:16)
λ+

−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:17)

.

Claim 4.6. Let D ∈ ∆H(S) and λ ∈ Λ such that (p, λ) is a ν-approximate minmax equilibrium in the
zero-sum game deﬁned above. Then (p, λ) is also a ν-approximate minmax solution for Equation (9).

Our problem reduces to ﬁnding an approximate equilibrium for this game. A key step in our
solution is the ability to compute best responses for both players in the game, which we now show
can be solved by the cost-sensitive classication (CSC) oracles.

g∈G(S)

16

Learner’s best response as CSC. Fix any mixed strategy (dual solution) λ ∈ Λ of the Auditor.
The Learner’s best response is given by:

err(h, P ) +

argmin
D∈∆H(S)

(cid:88)

g∈G(S)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

(10)

Note that it suﬃces for the Learner to optimize over deterministic classiﬁers h ∈ H, rather than
distributions over classiﬁers. This is because the Learner is solving a linear optimization problem
over the simplex, and so always has an optimal solution at a vertex (i.e. a single classiﬁer h ∈ H).
We can reduce this problem to one that can be solved with a single call to a CSC oracle.
In
particular, we can assign costs to each example (Xi, yi) as follows:

• if yi = 1, then c0

i = 0 and c1

i = − 1
n ;

• otherwise, c0

i = 0 and

c1
i =

1
n

+

1
n

(cid:88)

g∈G(S)

(λ+
g

−
g ) (Pr[g(x) = 1 | y = 0] − 1[g(xi) = 1])
− λ

(11)

Given a ﬁxed set of dual variables λ, we will write LC(λ) ∈ Rn to denote the vector of costs for
labelling each datapoint as 1. That is, LC(λ) is the vector such that for any i ∈ [n], LC(λ)i = c1
i .

Remark 4.7. Note that in deﬁning the costs above, we have translated them from their most natural
values so that the cost of labeling any example with 0 is 0. In doing so, we recall that by Claim 2.7,
the solution to a cost-sensitive classiﬁcation problem is invariant to translation. As we will see, this
will allow us to formulate the learner’s optimization problem as a low-dimensional linear optimization
problem, which will be important for an eﬃcient implementation of follow the perturbed leader. In
particular, if we ﬁnd a hypothesis that produces the n labels y = (y1, . . . , yn) for the n points in our
dataset, then the cost of this labelling in the CSC problem is by construction (cid:104)LC(λ), y(cid:105).

Auditor’s best response as CSC. Fix any mixed strategy (primal solution) p ∈ ∆H(S) of the
Learner. The Auditor’s best response is given by:
(cid:88)

(cid:88)

(cid:16)
λ+

−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:17)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

argmax
λ∈Λ

err(D, P ) +

g∈G(S)

= argmax
λ∈Λ

g∈G(S)

(12)
To ﬁnd the best response, consider the problem of computing ( ˆg, ˆ•) = argmax(g,•) Φ•(D, g).
There are two cases. In the ﬁrst case, p is a strictly feasible primal solution: that is Φˆ•(D, ˆg) < 0.
In this case, the solution to (12) sets λ = 0. Otherwise, if p is not strictly feasible, then by the
following Lemma 4.8 the best response is to set λˆ•

ˆg = C (and all other coordinates to 0).

Lemma 4.8. Fix any D ∈ ∆H(S) such that that maxg∈G(S)
(cid:48))
with one non-zero coordinate (λ

{Φ+(D, g), Φ−(D, g)} > 0. Let λ

(cid:48) ∈ Λ be vector

Then L(D, λ

(cid:48)) ≥ maxλ∈Λ L(D, λ).

•(cid:48)
g (cid:48) = C, where
, •(cid:48)

(g

(cid:48)

) = argmax

(g,•)∈G(S)×{±}

{Φ•(D, g)}

17

Therefore, it suﬃces to solve for argmax(g,•) Φ•(D, g). We proceed by solving argmaxg Φ+(D, g)
and argmaxg Φ−(D, g) separately: both problems can be reduced to a cost-sensitive classiﬁcation
problem. To solve for argmaxg Φ+(D, g) with a CSC oracle, we assign costs to each example (Xi, yi)
as follows:

• if yi = 1, then c0

i = 0 and c1

i = 0;

• otherwise, c0

i = 0 and

c1
i =

(cid:20)

−1
n

E
h∼D

[FP(h)] − E
h∼D

(cid:21)
[h(Xi)]

(13)

To solve for argmaxg Φ−(D, g) with a CSC oracle, we assign the same costs to each example

(Xi, yi), except when yi = 0, labeling “1” incurs a cost of

c1
i =

(cid:20)

−1
n

E
h∼D

[h(Xi)] − E
h∼D

(cid:21)
[FP(h)]

4.3 Solving the Game with No-Regret Dynamics

To compute an approximate equilibrium of the zero-sum game, we will simulate the following no-
regret dynamics between the Learner and the Auditor over rounds: over each of the T rounds, the
Learner plays a distribution over the hypothesis class according to a no-regret learning algorithm
(Follow the Perturbed Leader), and the Auditor plays an approximate best response against the
Learner’s distribution for that round. By the result of Freund and Schapire [1996], the average
plays of both players over time converge to an approximate equilibrium of the game, as long as
the Learner has low regret.

Theorem 4.9 (Freund and Schapire [1996]). Let D1, D2, . . . , DT ∈ ∆H(S) be a sequence of distribu-
tions played by the Learner, and let λ1, λ2, . . . , λT ∈ Λpure be the Auditor’s sequence of approximate best
responses against these distributions respectively. Let D = 1
t=1 λt be the two
T
players’ empirical distributions over their strategies. Suppose that the regret of the Learner satisﬁes

t=1 Dt and λ = 1
T

(cid:80)T

(cid:80)T

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105)− min
h∈H(S)

T(cid:88)

t=1

U (h, λt) ≤ γLT

and max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)]−

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) ≤ γAT .

Then (D, λ) is an (γL + γA)-approximate minimax equilibrium of the game.

Our Learner will play using the Follow the Perturbed Leader (FTPL), which gives a no-regret
guarantee. In order to implement FPTL, we will ﬁrst need to formulate the Learner’s best response
problem as a linear optimization problem over a low dimensional space. For each round t, let
t
s<t λs be the vector representing the sum of the actions played by the auditor over previous
λ
t
rounds, and recall that LC(λ
) is the cost vector given by our cost-sensitive classiﬁcation reduction.
t
is the following linear optimization problem
Then the Learner’s best response problem against λ

(cid:80)

=

t

(cid:104)LC(λ

), h(cid:105).

min
h∈H(S)

18

To run the FTPL algorithm, the Learner will optimize a “perturbed” version of the problem above.
In particular, the Learner will play a distribution Dt over the hypothesis class H(S) that is im-
plicitely deﬁned by the following sampling operation. To sample a hypothesis h from Dt, the
learner solves the following randomized optimization problem:

min
h∈H(S)

t
(cid:104)LC(λ

), h(cid:105) +

(cid:104)ξ, h(cid:105),

1
η

(14)

where η is a parameter and ξ is a noise vector drawn from the uniform distribution over [0, 1]n.
Note that while it is intractable to explicitly represent the distribution Dt (which has support size
scaling with |H(S)|), we can sample from Dt eﬃciently given access to a cost-sensitive classiﬁcation
oracle for H. By instantiating the standard regret bound of FTPL for online linear optimization
(Theorem 2.9), we get the following regret bound for the Learner.

Lemma 4.10. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm with η = n
, and λ1, . . . , λT be the
sequence of plays by the Auditor. Then

1√
nT

(1+C)

(cid:113)

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

T(cid:88)

t=1

U (h, λt) ≤ 2n1/4(1 + C)

T

√

Now we consider how the Auditor (approximately) best responds to the distribution Dt. The
main obstacle is that we do not have an explicit representation for Dt. Thus, our ﬁrst step is to
approximate Dt with an explicitly represented sparse distribution ˆDt. We do that by drawing m
i.i.d. samples from Dt, and taking the empirical distribution ˆDt over the sample. The Auditor
will best respond to this empirical distribution ˆDt. To show that any best response to ˆDt is also an
approximate best response to Dt, we will rely on the following uniform convergence lemma, which
bounds the diﬀerence in expected payoﬀ for any strategy of the auditor, when played against Dt
as compared to ˆDt.
Lemma 4.11. Fix any ξ, δ ∈ (0, 1) and any distribution D over H(S). Let h1, . . . , hm be m i.i.d. draws
from p, and ˆD be the empirical distribution over the realized sample. Then with probability at least 1 − δ
over the random draws of hj’s, the following holds,

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as long as m ≥ c0

C2(ln(1/δ)+d2 ln(n))
ξ2

for some absolute constant c0 and d2 = VCDIM(G).

Using Lemma 4.11, we can derive a regret bound for the Auditor in the no-regret dynamics.

Lemma 4.12. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm. For each Dt, let ˆDt be the empirical distri-
bution over m i.i.d. draws from Dt. Let λ1, . . . , λT be the Auditor’s best responses against ˆD1, . . . , ˆDT .
Then with probability 1 − δ,

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:114)

(cid:104)
U (h, λt)

(cid:105) ≤ T

c0C2(ln(T /δ) + d2 ln(n))
m

for some absolute constant c0 and d2 = VCDIM(G).

19

Finally, let D and λ be the average of the strategies played by the two players over the course
of the dynamics. Note that D is an average of many distributions with large support, and so D
itself has support size that is too large to represent explicitely. Thus, we will again approximate D
with a sparse distribution ˆD estimated from a sample drawn from D. Note that we can eﬃciently
sample from D given access to a CSC oracle. To sample, we ﬁrst uniformly randomly select a
round t ∈ [T ], and then use the CSC oracle to solve the sampling problem deﬁned in (14), with the
noise random variable ξ freshly sampled from its distribution. The full algorithm is described in
Algorithm 2 and we present the proof for Theorem 4.2 below.

Algorithm 2 FairNR: Fair No-Regret Dynamics

Input: distribution P over n labelled data points, CSC oracles CSC(H) and CSC(G), dual bound
C, and target accuracy parameter ν, δ
0
= 0, η = n
Initialize: Let C = 1/ν, λ

(cid:113)

,

(1+C)

1√
nT

m =

(ln(2T /δ)d2 ln(n)) C2c0T
n(1 + C)2 ln(2/δ)

√

and,

T =

√

4

n ln(2/δ)

ν4

For t = 1, . . . , T :

For s = 1, . . . m:

Sample from the Learner’s FTPL distribution:

Draw a random vector ξs uniformly at random from [0, 1]n
Use the oracle CSC(H) to compute h(s,t) = argminh∈H(S)

(cid:104)LC(λ

(t−1)

), h(cid:105) + 1
η

(cid:104)ξs, h(cid:105)

Let ˆDt be the empirical distribution over {hs,t}

Auditor best responds to ˆDt:

Use the oracle CSC(G) to compute λt = argmaxλ

E

h∼ ˆD [U (h, λ)]

t
Update: Let λ

=

(cid:80)

t(cid:48)≤t λt(cid:48)

Sample from the average distribution D =

(cid:80)T

t=1 Dt:

For s = 1, . . . m:

Draw a random number r ∈ [T ] and a random vector ξs uniformly at random from [0, 1]n
Use the oracle CSC(H) to compute h(r,t) = argminh∈H(S)

(cid:104)LC(λ

(cid:104)ξs, h(cid:105)

), h(cid:105) + 1
η

(r−1)

Let ˆD be the empirical distribution over {hr,t}

Output: ˆD as a randomized classiﬁer

Proof of Theorem 4.2. By Theorem 4.5, it suﬃces to show that with probability at least 1 − δ, ( ˆD, λ)
is a ν-approximate equilibrium in the zero-sum game. As a ﬁrst step, we will rely on Theorem 4.9
to show that (D, λ) forms an approximate equilibrium.

By Lemma 4.10, the regret of the sequence D1, . . . , DT is bounded by:

γL =


T(cid:88)


t=1

1
T

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

T(cid:88)

t=1

U (h, λt)





≤ 2n1/4(1 + C)
√
T

20

By Lemma 4.12, with probability 1 − δ/2, we have

(cid:114)

≤

γA

c0C2(ln(2T /δ) + d2 ln(n))
m

We will condition on this upper-bound event on γA for the rest of this proof, which is the case
except with probability δ/2. By Theorem 4.9, we know that the average plays (D, λ) form an
(γL + γA)-approximate equilibrium.

Finally, we need to bound the additional error for outputting the sparse approximation ˆD
instead of D. We can directly apply Lemma 4.11, which implies that except with probability δ/2,
the pair ( ˆD, λ) form a R-approximate equilibrium, with

R ≤ γA + γL +

(cid:112)

c0C2(ln(2/δ) + d2 ln(n))
m

√

Note that R ≤ ν as long as we have C = 1/ν,

m =

(ln(2T /δ)d2 ln(n)) C2c0T
n(1 + C)2 ln(2/δ)

√

and,

T =

√

4

n ln(2/δ)

ν4

This completes our proof.

5 Experimental Evaluation

We now describe an experimental evaluation of our proposed algorithmic framework on a dataset
in which fairness is a concern, due to the preponderance of racial and other sensitive features. For
far more detailed experiments on four real datasets investigating the convergence properties of
our algorithm, evaluating its accuracy vs. fairness tradeoﬀs, and comparing our approach to the
recent algorithm of Agarwal et al. [2017], we direct the reader to Kearns et al. [2018]. Python code
and an illustrative Jupyter notebook are provided here (https://github.com/algowatchpenn/GerryFair).
While the no-regret-based algorithm described in the last section enjoys provably polynomial
time convergence, for the experiments we instead implemented a simpler yet eﬀective algorithm
based on Fictitious Play dynamics. We ﬁrst describe and discuss this modiﬁed algorithm.

5.1 Solving the Game with Fictitious Play

Like the algorithm given in the last section, the algorithm we implemented works by simulating
a game dynamic that converges to Nash equilibrium in the zero-sum game that we derived, cor-
responding to the Fair ERM problem. Rather than using a no-regret dynamic, we instead use a
simple iterative procedure known as Fictitious Play [Brown, 1949]. Fictitious Play dynamics has
the beneﬁt of being more practical to implement: at each round, both players simply need to com-
pute a single best response to the empirical play of their opponents, and this optimization requires
only a single call to a CSC oracle. In contrast, the FTPL dynamic we gave in the previous section
requires making many calls to a CSC oracle per round — a computationally expensive process —
in order to ﬁnd a sparse approximation to the Learner’s mixed strategy at that round. Fictitious
Play also has the beneﬁt of being deterministic, unlike the randomized sampling required in the
FTPL no-regret dynamic, thus eliminating a source of experimental variance.

21

The disadvantage is that Fictitious Play is only known to converge to equilibrium in the
limit Robinson [1951], rather than in a polynomial number of rounds (though it is conjectured
to converge quickly under rather general circumstances; see Daskalakis and Pan [2014] for a re-
cent discussion). Nevertheless, this is the algorithm that we use in our experiments — and as we
will show, it performs well on real data, despite the fact that it has weaker theoretical guarantees
compared to the algorithm we presented in the last section.

Fictitious play proceeds in rounds, and in every round each player chooses a best response
to his opponent’s empirical history of play across previous rounds, by treating it as the mixed
strategy that randomizes uniformly over the empirical history. Pseudocode for the implemented
algorithm is given below.

Algorithm 3 FairFictPlay: Fair Fictitious Play

Input: distribution P over the labelled data points, CSC oracles CSC(H) and CSC(G) for the
classes H(S) and G(S) respectively, dual bound C, and number of rounds T
Initialize: set h0 to be some classiﬁer in H, set λ0 to be the zero vector. Let D and λ be the point
distributions that put all their mass on h0 and λ0 respectively.
For t = 1, . . . , T :

Compute the empirical play distributions:

Let D be the uniform distribution over the set of classiﬁers {h0, . . . , ht−1}
Let λ =

be the auditor’s empirical dual vector

(cid:80)
t

(cid:48)

(cid:48)

<t λt
t

Learner best responds: Use the oracle CSC(H) to compute ht = argminh∈H(S)
Auditor best responds: Use the oracle CSC(G) to compute λt = argmaxλ

(cid:104)LC(λ), h(cid:105)
h∼D [U (h, λ)]

E

Output: the ﬁnal empirical distribution D over classiﬁers

5.2 Description of Data

The dataset we use for our experimental valuation is known as the “Communities and Crime”
(C&C) dataset, available at the UC Irvine Data Repository4. Each record in this dataset describes
the aggregate demographic properties of a diﬀerent U.S. community; the data combines socio-
economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey,
and crime data from the 1995 FBI UCR. The total number of records is 1994, and the number of
features is 122. The variable to be predicted is the rate of violent crime in the community.

While there are larger and more recent datasets in which subgroup fairness is a potential con-
cern, there are properties of the C&C dataset that make it particularly appealing for the initial
experimental evaluation of our proposed algorithm. Foremost among these is the relatively high
number of sensitive or protected attributes, and the fact that they are real-valued (since they rep-
resent aggregates in a community rather than speciﬁc individuals). This means there is a very
large number of protected sub-groups that can be deﬁned over them. There are distinct con-
tinuous features measuring the percentage or per-capita representation of multiple racial groups
(including white, black, Hispanic, and Asian) in the community, each of which can vary indepen-
dently of the others. Similarly, there are continuous features measuring the average per capita
incomes of diﬀerent racial groups in the community, as well as features measuring the percentage
of each community’s police force that falls in each of the racial groups. Thus restricting to features

4http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime

22

capturing race statistics and a couple of related ones (such as the percentage of residents who do
not speak English well), we obtain an 18-dimensional space of real-valued protected attributes.
We note that the C&C dataset has numerous other features that arguably could or should be pro-
tected as well (such as gender features), which would raise the dimensionality of the protected
subgroups even further. 5

We convert the real-valued rate of violent crime in each community to a binary label indicating
whether the community is in the 70th percentile of that value, indicating that it is a relatively
high-crime community. Thus the strawman baseline that always predicts 0 (lower crime) has
error approximately 30% or 0.3 on this classiﬁcation problem. We chose the 70th percentile since
it seems most natural to predict the highest crime rates.

As in the theoretical sections of the paper, our main interest and emphasis is on the eﬀective-

ness of our proposed algorithm FairFictPlay on a given dataset, including:

• Whether the algorithm in fact converges, and does so in a feasible amount of computation.
Recall that formal convergence is only guaranteed under the assumption of oracles that do
not exist in practice, and even then is only guaranteed asymptotically.

• Whether the classiﬁer learned by the algorithm has nontrivial accuracy, as well as strong

subgroup fairness properties.

racy and subgroup fairness.

• Whether the algorithm and dataset permits nontrivial tuning of the trade-oﬀ between accu-

As discussed in Section 2.1, we note that all of these issues can be investigated entirely in-sample,
without concern for generalization performance. Thus for simplicity, despite the fact that our
algorithm enjoys all the usual generalization properties depending on the VC dimension of the
Learner’s hypothesis space and the Auditor’s subgroup space (see Theorems 2.12 and 2.11), we
report all results here on the full C&C dataset of 1994 points, treating it as the true distribution
of interest.

5.3 Algorithm Implementation

The main details in the implementation of FairFictPlay are the identiﬁcation of the model classes
for Learner and Auditor, the implementation of the cost sensitive classiﬁcation oracle and auditing
oracle, and the identiﬁcation of the protected features for Auditor. For our experiments, at each
round Learner chooses a linear threshold function over all 122 features. We implement the cost
sensitive classiﬁcation oracle via a two stage regression procedure. In particular, the inputs to
the cost sensitive classiﬁcation oracle are cost vectors c0, c1, where the ith element of ck is the cost
of predicting k on datapoint i. We train two linear regression models r0, r1 to predict c0 and c1
respectively, using all 122 features. Given a new point x, we predict the cost of classifying x as 0
and 1 using our regression models: these predictions are r0(x) and r1(x) respectively. Finally we
output the prediction ˆy corresponding to lower predicted cost: ˆy = argmini∈{0,1}ri(x).

Auditor’s model class consists of all linear threshold functions over just the 18 aforementioned
protected race-based attributes. As per the algorithm, at each iteration t Auditor attempts to ﬁnd
a subgroup on which the false positive rate is substantially diﬀerent than the base rate, given

5Ongoing experiments on other datasets where fairness is a concern will be reported on in a forthcoming experi-

mental paper.

23

the Learner’s randomized classiﬁer so far. We implement the auditing oracle by treating it as
a weighted regression problem in which the goal is ﬁnd a linear function (which will be taken
to deﬁne the subgroup) that on the negative examples, can predict the Learner’s probabilistic
classiﬁcation on each point. We use the same regression subroutine as Learner does, except that
Auditor only has access to the 18 sensitive features, rather than all 122.

Recall that in addition to the choices of protected attributes and model classes for Learner
and Auditor, FairFictPlay has a parameter C, which is a bound on the norm of the dual variables
for Auditor (the dual player). While the theory does not provide an explicit bound or guide for
choosing C, it needs to be large enough to permit the dual player to force the minmax value of the
game. For our experiments we chose C = 10, which despite being a relatively small value seems
to suﬃce for (approximate) convergence.

The other and more meaningful parameter of the algorithm is the bound γ in the Fair ERM
optimization problem implemented by the game, which controls the amount of unfairness per-
mitted. If on a given round the subgroup disparity found by the Auditor is greater than γ, the
Learner must react by adding a fairness penalty for this subgroup to its objective function; if it is
smaller than γ, the Learner can ignore it and continue to optimize its previous objective function.
Ideally, and as we shall see, varying γ allows us to trace out a menu of trade-oﬀs between accuracy
and fairness.

5.4 Results

Particularly in light of the gaps between the idealized theory and the actual implementation,
the most basic questions about FairFictPlay are whether it converges at all, and if so, whether
it converges to “interesting” models — that is, models with both nontrivial classiﬁcation error
(much better than the 30% or 0.3 baserate), and nontrivial subgroup fairness (much better than
ignoring fairness altogether). We shall see that at least for the C&C dataset, the answers to these
questions is strongly aﬃrmative.

(a)

(b)

Figure 1: Evolution of the error and unfairness of Learner’s classiﬁer across iterations, for varying
choices of γ. (a) Error εt of Learner’s model vs iteration t. (b) Unfairness γt of subgroup found by
Auditor vs. iteration t, as measured by Deﬁnition 2.3. See text for details.

24

We begin by examining the evolution of the error and unfairness of Learner’s model. In the
left panel of Figure 1 we show the error of the model found by Learner vs. iteration for values of
γ ranging from 0 to 0.029. Several comments are in order.

First, after an initial period in which there is a fair amount of oscillatory behavior, by 6000
iterations most of the curves have largely ﬂattened out, and by 8,000 iterations it appears most
but not all have reached approximate convergence. Second, while the top-to-bottom ordering of
these error curves is approximately aligned with decreasing γ — so larger γ generally results in
lower error, as expected — there are many violations of this for small t, and even a few at large
t. Third, and as we will examine more closely shortly, the converged values at large t do indeed
exhibit a range of errors.

In the right panel of Figure 1, we show the corresponding unfairness γt of the subgroup found
by the Auditor at each iteration t for the same runs and values of the parameter γ (indicated by
horizontal dashed lines), with the same color-coding as for the left panel. Now the ordering is
generally reversed — larger values of γ generally lead to higher γt curves, since the fairness con-
straint on the Learner is weaker. We again see a great deal of early oscillatory behavior, with most
γt curves then eventually settling at or near their corresponding input γ value, as Learner and
Auditor engage in a back-and-forth struggle for lower error for Learner and γ-subgroup fairness
for Auditor.

(a)

(b)

Figure 2:
(a) Pareto-optimal error-unfairness values, color coded by varying values of the input
parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same
range but are sampled more densely to get a smoother frontier. See text for details.

For any choice of the parameter γ, and each iteration t, the two panels of Figure 1 yield a pair
of realized values (cid:104)εt, γt
(cid:105) from the experiment, corresponding to a Learner model whose error is
εt, and for which the worst subgroup the Auditor was able to ﬁnd had unfairness γt. The set of all
(cid:105) pairs across all runs or γ values thus represents the diﬀerent trade-oﬀs between error and
(cid:104)εt, γt
unfairness found by our algorithm on the data. Most of these pairs are of course Pareto-dominated
by other pairs, so we are primarily interested in the undominated frontier.

In the left panel of Figure 2, for each value of γ we show the Pareto-optimal pairs, color-coded
for the value of γ. Each value of γ yields a set or cloud of undominated pairs that are usually fairly

25

close to each other, and as expected, as γ is increased, these clouds generally move leftwards and
upwards (lower error and higher unfairness).

We anticipate that the practical use of our algorithm would, as we have done, explore many
values of γ and then pick a model corresponding to a point on the aggregated Pareto frontier
across all γ, which represents the collection of all undominated models and the overall error-
unfairness trade-oﬀ. This aggregate frontier is shown in the right panel of Figure 2, and shows
a relatively smooth menu of options, ranging from error about 0.21 and no unfairness at one
extreme, to error about 0.12 and unfairness 0.025 at the other, and an appealing assortment of
intermediate trade-oﬀs. Of course, in a real application the selection of a particular point on
the frontier should be made in a domain-speciﬁc manner by the stakeholders or policymakers in
question.

Acknowledgements We thank Alekh Agarwal, Richard Berk, Miro Dud´ık, Akshay Krishna-
murthy, John Langford, Greg Ridgeway and Greg Yang for helpful discussions and suggestions.

References

Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, and John Langford. A reductions approach
to fair classiﬁcation. Fairness, Accountability, and Transparency in Machine Learning (FATML),
2017.

Julia Angwin and Hannes Grassegger. Facebooks secret censorship rules protect white men from

hate speech but not black children. Propublica, 2017.

Anna Maria Barry-Jester, Ben Casselman, and Dana Goldstein. The new science of sentencing.

The Marshall Project, August 8 2015. Retrieved 4/28/2016.

George W. Brown. Some notes on computation of games solutions, Jan 1949.

Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. arXiv preprint arXiv:1703.00056, 2017.

Constantinos Daskalakis and Qinxuan Pan. A counter-example to Karlin’s strong conjecture for
ﬁctitious play. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium
on, pages 11–20. IEEE, 2014.

Ilias Diakonikolas, Ryan O’Donnell, Rocco A. Servedio, and Yi Wu. Hardness results for agnosti-
cally learning low-degree polynomial threshold functions. In Proceedings of the Twenty-Second
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California,
USA, January 23-25, 2011, pages 1590–1606, 2011.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Confer-
ence, pages 214–226. ACM, 2012.

Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning of

monomials by halfspaces is hard. SIAM J. Comput., 41(6):1558–1590, 2012.

26

Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings
of the Ninth Annual Conference on Computational Learning Theory, COLT 1996, Desenzano del
Garda, Italy, June 28-July 1, 1996., pages 325–332, 1996.

Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im) possibility

of fairness. arXiv preprint arXiv:1609.07236, 2016.

Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination
prevention in data mining. IEEE transactions on knowledge and data engineering, 25(7):1445–
1459, 2013.

Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning.

Advances in Neural Information Processing Systems, 2016.

´Ursula H´ebert-Johnson, Michael P Kim, Omer Reingold, and Guy N Rothblum. Calibration for

the (computationally-identiﬁable) masses. arXiv preprint arXiv:1711.08513, 2017.

Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
In Advances in Neural Information Processing Systems, pages

Classic and contextual bandits.
325–333, 2016.

Adam Tauman Kalai and Santosh Vempala. Eﬃcient algorithms for online decision problems. J.

Comput. Syst. Sci., 71(3):291–307, 2005.

Adam Tauman Kalai, Yishay Mansour, and Elad Verbin. On agnostic boosting and parity learn-
ing. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British
Columbia, Canada, May 17-20, 2008, pages 629–638, 2008.

Faisal Kamiran and Toon Calders. Data preprocessing techniques for classiﬁcation without dis-

crimination. Knowledge and Information Systems, 33(1):1–33, 2012.

Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. An Empirical Study of Rich
Subgroup Fairness for Machine Learning. ArXiv e-prints, art. arXiv:1808.08166, August 2018.

Michael J Kearns and Umesh Virkumar Vazirani. An Introduction to Computational Learning The-

ory. MIT press, 1994.

Michael J Kearns, Robert E Schapire, and Linda M Sellie. Toward eﬃcient agnostic learning.

Machine Learning, 17(2-3):115–141, 1994.

Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-oﬀs in the fair deter-
mination of risk scores. In Proceedings of the 2017 ACM Conference on Innovations in Theoretical
Computer Science, Berkeley, CA, USA, 2017, 2017.

James Rufus Koren. What does that web search say about your credit? Los Angeles Times, July 16

2016. Retrieved 9/15/2016.

1951.

Julia Robinson. An iterative method of solving a game. Annals of Mathematics, pages 10–2307,

27

Cynthia Rudin. Predictive policing using machine learning to detect patterns of crime. Wired

Magazine, August 2013. Retrieved 4/28/2016.

Maurice Sion. On general minimax theorems. Paciﬁc J. Math., 8(1):171–176, 1958.

Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-

discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.

Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate ex-
ample weighting. In Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM
2003), 19-22 December 2003, Melbourne, Florida, USA, page 435, 2003.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate
mistreatment.
In Proceedings of the 26th International Conference on World Wide Web, pages
1171–1180. International World Wide Web Conferences Steering Committee, 2017.

Zhe Zhang and Daniel B Neill. Identifying signiﬁcant predictive bias in classiﬁers. arXiv preprint

arXiv:1611.08292, 2016.

A Chernoﬀ-Hoeﬀding Bound

We use the following concentration inequality.

Theorem A.1 (Real-vaued Additive Chernoﬀ-Hoeﬀding Bound). Let X1, X2, . . . , Xm be i.i.d. random
variables with E [Xi] = µ and a ≤ Xi

≤ b for all i. Then for every α > 0,
(cid:32) −2α2m
(cid:33)
(b − a)2

≤ 2 exp

i Xi
m

≥ α

− µ

(cid:80)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr

B Generalization Bounds

Proof of Theorems 2.11 and 2.12. We give a proof of Theorem 2.11. The proof of Theorem 2.12 is
identical, as false positive rates are just positive classiﬁcation rates on the subset of the data for
which y = 0.

Given a set of classiﬁers H and protected groups G, deﬁne the following function class:

FH,G = {fh,g (x) (cid:17) h(x) ∧ g(x) : h ∈ H, g ∈ G}

We can relate the VC-dimension of FH,G to the VC-dimension of H and G:
Claim B.1.

VCDIM(FH,G) ≤ ˜O(VCDIM(H) + VCDIM(G))
Proof. Let S be a set of size m shattered by FH,G. Let πFH,G (S) be the number of labelings of S
realized by elements of FH,G. By the deﬁnition of shattering, πFH,G (S) = 2m. Now for each labeling
of S by an element in FH,G, it is realized as (f ∧ g)(S) for some f ∈ F , g ∈ G. But (f ∧ g)(S) =
f (S)∧g(S), and so it can be realized as the conjunction of a labeling of S by an element of F and an

28

element of G. But since there are πF (S)πG(S) such pairs of labelings, this immediately implies that
πFH,G (S) ≤ πF (S)πG(S). Now by the Sauer-Shelah Lemma (see e.g. Kearns and Vazirani [1994]),
πF (S) = O(mVCDIM(H)), πG(S) = O(mVCDIM(G)). Thus πFH,G (S) = 2m ≤ O(mVCDIM(H)+VCDIM(G)), which
implies that m = ˜O(VCDIM(H) + VCDIM(G)), as desired.

This bound, together with a standard VC-Dimension based uniform convergence theorem (see

e.g. Kearns and Vazirani [1994]) implies that with probability 1 − δ, for every fh,g

∈ FH,G:

(cid:12)(cid:12)(cid:12)E
(X,y)∼P [fh,g (X)] − E

(X,y)∼P

S [fh,g (X)]

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

Note that the left hand side of the above inequality can be written as:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr
(X,y)∼P

[h(X) = 1|g(x) = 1] · Pr

(X,y)∼P

[g(x) = 1] − Pr
(X,y)∼P

[h(X) = 1|g(x) = 1] · Pr
(X,y)∼P

S

S

[g(x) = 1]





(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

This completes our proof.

C Missing Proofs in Section 4

Theorem 4.5. Let ( ˆD, ˆλ) be a ν-approximate minmax solution to the Λ-bounded Lagrangian problem
in the sense that

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν and, L( ˆD, ˆλ) ≥ max
λ∈Λ

L( ˆD, λ) − ν.

Then err( ˆD, P ) ≤ OPT +2ν and for any g ∈ G(S),

Proof of Theorem 4.5. Let D
problem. Since D

∗ is feasible, we know that L(D

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C
∗ be the optimal feasible solution for our constrained optimization
, P ).

, ˆλ) ≤ err(D

.

∗

∗

We will ﬁrst focus on the case where ˆD is not a feasible solution, that is

max
(g,•)∈G(S)×{±}

Φ•( ˆD, g) > 0

Let ( ˆg, ˆ•) ∈ argmax(g,•) Φ•( ˆD, g) and let λ
zero. By Lemma 4.8, we know that λ
minmax solution, we know that L( ˆD, ˆλ) ≥ L( ˆD, λ

(cid:48) ∈ argmaxλ∈Λ

(cid:48) ∈ Λ be a vector with (λ

(cid:48))ˆ•
ˆg = C and all other coordinates
L( ˆD, λ). By the deﬁnition of a ν-approximate

(cid:48)) − ν. This implies that

Note that L(D

, ˆλ) ≤ err(D

, P ), and so

∗

∗

L( ˆD, ˆλ) ≥ err( ˆD, P ) + C Φˆ•( ˆD, ˆg) − ν

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν ≤ L(D

, ˆλ) + ν

∗

29

(15)

(16)

Combining Equations (15) and (16), we get

err( ˆD, P ) + C Φˆ•( ˆD, ˆg) ≤ L( ˆD, ˆλ) + ν ≤ L(D

∗

, ˆλ) + 2ν ≤ err(D

, P ) + 2ν

∗

Note that C Φˆ•( ˆD, ˆg) ≥ 0, so we must have err( ˆD, P ) ≤ err(D
, P ) ∈ [0, 1], we know
since err( ˆD, P ), err(D

∗

∗

, P ) + 2ν = OPT +2ν. Furthermore,

which implies that maximum constraint violation satisﬁes Φˆ•( ˆD, ˆg) ≤ (1 + 2ν)/C. By applying
Claim 4.4, we get

C Φˆ•( ˆD, ˆg) ≤ 1 + 2ν,

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C

.

Now let us consider the case in which ˆD is a feasible solution for the optimization problem.

Then it follows that there is no constraint violation by ˆD and maxλ
∗

err( ˆD, P ) = max

L( ˆD, λ) ≤ L( ˆD, ˆλ) + ν ≤ min
D

λ

L(D, ˆλ) + 2ν ≤ L(D

, ˆλ) + 2ν ≤ err(D

, P ) + 2ν

∗

L( ˆD, λ) = err( ˆD, P ), and so

Therefore, the stated bounds hold for both cases.

Lemma 4.8. Fix any D ∈ ∆H(S) such that that maxg∈G(S)
(cid:48))
with one non-zero coordinate (λ

•(cid:48)
g (cid:48) = C, where

{Φ+(D, g), Φ−(D, g)} > 0. Let λ

(cid:48) ∈ Λ be vector

(cid:48)

, •(cid:48)

(g

) = argmax

(g,•)∈G(S)×{±}

{Φ•(D, g)}

Then L(D, λ

(cid:48)) ≥ maxλ∈Λ L(D, λ).

Proof of Lemma 4.8. Observe:

L(D, λ) = argmax

[err(h, P )] +

(cid:88)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

argmax
λ∈Λ

λ∈Λ

= argmax
λ∈Λ

E
h∼D
(cid:88)

g∈G

g∈G(S)
(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:16)
λ+

Note that this is a linear optimization problem over the non-negative orthant of a scaling of the (cid:96)1
ball, and so has a solution at a vertex, which corresponds to a single group g ∈ G(S). Thus, there
(cid:48))•
is always a best response λ
g that maximizes
Φ•(D, g).

(cid:48) that puts all the weight C on the coordinate (λ

Lemma 4.10. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm with η = n
, and λ1, . . . , λT be the
sequence of plays by the Auditor. Then

1√
nT

(1+C)

(cid:113)

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

U (h, λt) ≤ 2n1/4(1 + C)

T

√

T(cid:88)

t=1

30

Proof of Lemma 4.10. To instantiate the regret bound in Theorem 2.9, we just need to provide a
bound on the maximum absoluate value over the coordinates of the loss vector (the quantity M in
Theorem 2.9). For any λ ∈ Λ, the absolute value of the i-th coordinate of LC(λ) is bounded by:

(λ+
g

−
g ) (Pr[g(x) = 1 | y = 0] − 1) 1[g(xi) = 1]
− λ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(Pr[g(x) = 1 | y = 0]1g(xi) = 1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
n

+

(cid:88)

1
n
g∈G(S)


(cid:88)

≤ 1
n

+

≤ 1
n

+

1
n

1
n




g∈G(S)


(cid:88)

g∈G(S)

(cid:12)(cid:12)(cid:12)λ+

g

−
− λ
g



(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)λ+

g

(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)λ

−
g

max
g∈G(S)


(cid:12)(cid:12)(cid:12)

≤ 1 + C
n

Also note that the dimension of the optimization is the size of the dataset n. This means if we set
η = n

, the regret of the learner will then be bounded by 2n1/4(1 + C)

T .

(cid:113)

√

(1+C)

1√
nT

Lemma 4.11. Fix any ξ, δ ∈ (0, 1) and any distribution D over H(S). Let h1, . . . , hm be m i.i.d. draws
from p, and ˆD be the empirical distribution over the realized sample. Then with probability at least 1 − δ
over the random draws of hj’s, the following holds,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as long as m ≥ c0

C2(ln(1/δ)+d2 ln(n))
ξ2

for some absolute constant c0 and d2 = VCDIM(G).

Proof of Lemma 4.11. Recall that for any distribution D
deﬁned as

(cid:48) over H(S) the expected payoﬀ function is

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)] = E
h∼ ˆD

[err(h, P )] + E
h∼ ˆD

− E
h∼D

[err(h, P )] + E
h∼D

(cid:88)

(cid:16)
λ+

g Φ+(h, g) + λ


(cid:17)
−
g Φ−(h, g)


(cid:17)
−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:16)
λ+








g∈G(S)

(cid:88)

g∈G(S)



h∼D [err(h, P )] −

By the triangle inequality, it suﬃces to show that with probability (1 − δ), A = | E
E
h∼ ˆD [err(h, P )] | ≤ ξ/2 and for all λ ∈ Λ and g ∈ G(S),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

B =

E
h∼ ˆD





(cid:88)

g∈G(S)


(cid:17)
−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:16)
λ+



− E
h∼D





(cid:88)

g∈G(S)

(cid:16)
λ+

−
g Φ−(h, g)
g Φ+(h, g) + λ

≤ ξ/2


(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


The ﬁrst part follows directly from a simple application of the Chernoﬀ-Hoeﬀding bound

(Theorem A.1): with probability (1 − δ/2), A ≤ ξ/2, as long as m ≥ 2 ln(4/δ)/ξ2.
To bound the second part, we ﬁrst note that by H¨older’s inequality, we have

B ≤ (cid:107)λ(cid:107)

1 max
(g,•)∈G(S)×{±}

|Φ•(D, g) − Φ•( ˆD, g)|

31

Since for all λ ∈ Λ we have (cid:107)λ(cid:107)

≤ C, it suﬃces to show that with probability 1−δ/2, |Φ•(D, g)−

1

Φ•( ˆD, g)| ≤ ξ/(2C) holds for all • ∈ {−, +} and g ∈ G(S). Note that

We can rewrite the absolute value of ﬁrst term:

|Φ•(D, g) − Φ•( ˆD, g)| =

[FP(h)]

Pr[y = 0, g(x) = 1]

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[FP(h)] − E
h∼ ˆD

(cid:33)

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

E
h∼D

[FP(h)] − E
h∼ ˆD

(cid:33)
[FP(h)]

Pr[y = 0, g(x) = 1]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1 | y = 0]] − E
h∼ ˆD

E
h∼D

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0]]

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:33)
[Pr[h(X) = 1 | y = 0]]

Pr[g(x) = 1 | y = 0]

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:32)

+

(cid:32)

(cid:32)

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=

≤

where the last inequality follows from Pr[g(x) = 1 | y = 0] ≤ 1.

Note that E

h∼ ˆD [Pr[h(X) = 1, y = 0, g(x) = 1]] = 1
m

average of m i.i.d. random variables with expectation E
Chernoﬀ-Hoeﬀding bound (Theorem A.1), we have

(cid:80)m

j=1 Pr[hj(X) = 1, y = 0, g(x) = 1], which is an
h∼D [Pr[h(X) = 1, y = 0, g(x) = 1]]. By the

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

Pr

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD
(cid:16)− ξ2m
8C2

[Pr[h(X) = 1, y = 0]]

>

≤ 2 exp

(17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

ξ
4C

(cid:33)

(cid:32)

− ξ2m
8C2

In the following, we will let δ0 = 2 exp

(cid:17)
. Similarly, we also have for each g ∈ G(S),

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

>

(18)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

ξ
4C

≤ δ0

By taking the union bound over (17) and (18) over all choices of g ∈ G(S), we have with proba-

bility at least (1 − δ0(1 + |G(S)|)),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0]]

(19)

and,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

for all g ∈ G(S).

(20)

(cid:17)
Note that by Sauer’s lemma (Lemma 4.3), |G(S)| ≤ O
. Thus, there exists an absolute constant
C2(ln(1/δ)+d2 ln(n))
c0 such that m ≥ c0
implies that failure probability above δ0(1 + |G(S)|) ≤ δ/2. We
ξ2
will assume m satisiﬁes such a bound, and so the events of (19) and (20) hold with probaility at
least (1−δ/2). Then by the triangle inequality we have for all (g, •) ∈ G(S)×{±}, |Φ•(D, g)−Φ•( ˆD, g)| ≤
ξ/(2C), which implies that B ≤ ξ/2. This completes the proof.

(cid:16)
nd2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ ξ
4C

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ ξ
4C

32

Claim C.1. Suppose there are two distributions D and ˆD over H(S) such that

Let

Then

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ˆλ ∈ argmax

λ(cid:48)∈Λ

E
h∼ ˆD

(cid:2)U (h, λ

(cid:48)

)(cid:3)

max
λ

E
h∼D

[U (h, λ)] − ξ ≤ E
h∼D

(cid:105)
(cid:104)
U (h, ˆλ)

,

Lemma 4.12. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm. For each Dt, let ˆDt be the empirical distri-
bution over m i.i.d. draws from Dt. Let λ1, . . . , λT be the Auditor’s best responses against ˆD1, . . . , ˆDT .
Then with probability 1 − δ,

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:114)

(cid:104)
U (h, λt)

(cid:105) ≤ T

c0C2(ln(T /δ) + d2 ln(n))
m

for some absolute constant c0 and d2 = VCDIM(G).

Proof. Let γ t

A be deﬁned as

By instantiating Lemma 4.11 and applying union bound across all T steps, we know with proba-
bility at least 1 − δ, the following holds for all t ∈ [T ]:

γ t
A = max
λ∈Λ

E
h∼ ˆDt

[U (h, λ)] − E
h∼Dt

[U (h, λ)]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:114)

≤

γ t
A

c0C2(ln(T /δ) + d2 ln(n))
m

where c0 is the absolute constant in Lemma 4.11 and d2 = VCDIM(G).

Note that by Claim C.1, the Auditor is performing a γ t
round t. Then we can bound the Auditor’s regret as follows:

A-approximate best response at each

γA =





1
T

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

max
λ∈Λ

E
h∼Dt

[U (h, λ)] − E
h∼Dt

(cid:104)
U (h, λt)

(cid:105)(cid:19)


(cid:105)



(cid:18)

T(cid:88)

≤ 1
T
t=1
≤ max
γ t
A
T

It follows that with probability 1 − δ, we have

which completes the proof.

(cid:114)

≤

γA

c0C2(ln(T /δ) + d2 ln(n))
m

33

8
1
0
2
 
c
e
D
 
3
 
 
]

G
L
.
s
c
[
 
 
5
v
4
4
1
5
0
.
1
1
7
1
:
v
i
X
r
a

Preventing Fairness Gerrymandering:
Auditing and Learning for Subgroup Fairness

Michael Kearns1, Seth Neel1, Aaron Roth1 and Zhiwei Steven Wu2

1University of Pennsylvania
2Microsoft Research-New York City

December 4, 2018

Abstract

The most prevalent notions of fairness in machine learning are statistical deﬁnitions: they
ﬁx a small collection of high-level, pre-deﬁned groups (such as race or gender), and then ask
for approximate parity of some statistic of the classiﬁer (like positive classiﬁcation rate or false
positive rate) across these groups. Constraints of this form are susceptible to (intentional or
inadvertent) fairness gerrymandering, in which a classiﬁer appears to be fair on each individual
group, but badly violates the fairness constraint on one or more structured subgroups deﬁned
over the protected attributes (such as certain combinations of protected attribute values). We
propose instead to demand statistical notions of fairness across exponentially (or inﬁnitely)
many subgroups, deﬁned by a structured class of functions over the protected attributes. This
interpolates between statistical deﬁnitions of fairness, and recently proposed individual no-
tions of fairness, but it raises several computational challenges. It is no longer clear how to
even check or audit a ﬁxed classiﬁer to see if it satisﬁes such a strong deﬁnition of fairness.
We prove that the computational problem of auditing subgroup fairness for both equality of
false positive rates and statistical parity is equivalent to the problem of weak agnostic learn-
ing — which means it is computationally hard in the worst case, even for simple structured
subclasses. However, it also suggests that common heuristics for learning can be applied to
successfully solve the auditing problem in practice.

We then derive two algorithms that provably converge to the best fair distribution over
classiﬁers in a given class, given access to oracles which can optimally solve the agnostic learn-
ing problem. The algorithms are based on a formulation of subgroup fairness as a two-player
zero-sum game between a Learner (the primal player) and an Auditor (the dual player). Both
algorithms compute an equilibrium of this game. We obtain our ﬁrst algorithm by simulat-
ing play of the game by having Learner play an instance of the no-regret Follow the Perturbed
Leader algorithm, and having Auditor play best response. This algorithm provably converges
to an approximate Nash equilibrium (and thus to an approximately optimal subgroup-fair dis-
tribution over classiﬁers) in a polynomial number of steps. We obtain our second algorithm
by simulating play of the game by having both players play Fictitious Play, which enjoys only
provably asymptotic convergence, but has the merit of simplicity and faster per-step compu-
tation. We implement the Fictitious Play version using linear regression as a heuristic oracle,
and show that we can eﬀectively both audit and learn fair classiﬁers on real datasets.

1 Introduction

As machine learning is being deployed in increasingly consequential domains (including policing
[Rudin, 2013], criminal sentencing [Barry-Jester et al., 2015], and lending [Koren, 2016]), the
problem of ensuring that learned models are fair has become urgent.

Approaches to fairness in machine learning can coarsely be divided into two kinds: statistical
and individual notions of fairness. Statistical notions typically ﬁx a small number of protected
demographic groups G (such as racial groups), and then ask for (approximate) parity of some
statistical measure across all of these groups. One popular statistical measure asks for equal-
ity of false positive or negative rates across all groups in G (this is also sometimes referred to as
an equal opportunity constraint [Hardt et al., 2016]). Another asks for equality of classiﬁcation
rates (also known as statistical parity). These statistical notions of fairness are the kinds of fair-
ness deﬁnitions most common in the literature (see e.g. Kamiran and Calders [2012], Hajian and
Domingo-Ferrer [2013], Kleinberg et al. [2017], Hardt et al. [2016], Friedler et al. [2016], Zafar
et al. [2017], Chouldechova [2017]).

One main attraction of statistical deﬁnitions of fairness is that they can in principle be ob-
tained and checked without making any assumptions about the underlying population, and hence
lead to more immediately actionable algorithmic approaches. On the other hand, individual no-
tions of fairness ask for the algorithm to satisfy some guarantee which binds at the individual,
rather than group, level. This often has the semantics that “individuals who are similar” should
be treated “similarly” [Dwork et al., 2012], or “less qualiﬁed individuals should not be favored
over more qualiﬁed individuals” [Joseph et al., 2016]. Individual notions of fairness have attrac-
tively strong semantics, but their main drawback is that achieving them seemingly requires more
assumptions to be made about the setting under consideration.

The semantics of statistical notions of fairness would be signiﬁcantly stronger if they were
deﬁned over a large number of subgroups, thus permitting a rich middle ground between fairness
only for a small number of coarse pre-deﬁned groups, and the strong assumptions needed for
fairness at the individual level. Consider the kind of fairness gerrymandering that can occur when
we only look for unfairness over a small number of pre-deﬁned groups:

Example 1.1. Imagine a setting with two binary features, corresponding to race (say black and white)
and gender (say male and female), both of which are distributed independently and uniformly at random
in a population. Consider a classiﬁer that labels an example positive if and only if it corresponds to a
black man, or a white woman. Then the classiﬁer will appear to be equitable when one considers either
protected attribute alone, in the sense that it labels both men and women as positive 50% of the time, and
labels both black and white individuals as positive 50% of the time. But if one looks at any conjunction
of the two attributes (such as black women), then it is apparent that the classiﬁer maximally violates the
statistical parity fairness constraint. Similarly, if examples have a binary label that is also distributed
uniformly at random, and independently from the features, the classiﬁer will satisfy equal opportunity
fairness with respect to either protected attribute alone, even though it maximally violates it with respect
to conjunctions of two attributes.

We remark that the issue raised by this toy example is not merely hypothetical. In our exper-
iments in Section 5, we show that similar violations of fairness on subgroups of the pre-deﬁned
groups can result from the application of standard machine learning methods applied to real
datasets. To avoid such problems, we would like to be able to satisfy a fairness constraint not
just for the small number of protected groups deﬁned by single protected attributes, but for a

1

combinatorially large or even inﬁnite collection of structured subgroups deﬁnable over protected
attributes.

In this paper, we consider the problem of auditing binary classiﬁers for equal opportunity and
statistical parity, and the problem of learning classiﬁers subject to these constraints, when the
number of protected groups is large. There are exponentially many ways of carving up a popu-
lation into subgroups, and we cannot necessarily identify a small number of these a priori as the
only ones we need to be concerned about. At the same time, we cannot insist on any notion of sta-
tistical fairness for every subgroup of the population: for example, any imperfect classiﬁer could
be accused of being unfair to the subgroup of individuals deﬁned ex-post as the set of individuals
it misclassiﬁed. This simply corresponds to “overﬁtting” a fairness constraint. We note that the
individual fairness deﬁnition of Joseph et al. [2016] (when restricted to the binary classiﬁcation
setting) can be viewed as asking for equalized false positive rates across the singleton subgroups,
containing just one individual each1 — but naturally, in order to achieve this strong deﬁnition of
fairness, Joseph et al. [2016] have to make structural assumptions about the form of the ground
truth. It is, however, sensible to ask for fairness for large structured subsets of individuals: so
long as these subsets have a bounded VC dimension, the statistical problem of learning and au-
diting fair classiﬁers is easy, so long as the dataset is suﬃciently large. This can be viewed as an
interpolation between equal opportunity fairness and the individual “weakly meritocratic” fair-
ness deﬁnition from Joseph et al. [2016], that does not require making any assumptions about the
ground truth. Our investigation focuses on the computational challenges, both in theory and in
practice.

1.1 Our Results

Brieﬂy, our contributions are:

• Formalization of the problem of auditing and learning classiﬁers for fairness with respect to

rich classes of subgroups G.

• Results proving (under certain assumptions) the computational equivalence of auditing G
and (weak) agnostic learning of G. While these results imply theoretical intractability of au-
diting for some natural classes G, they also suggest that practical machine learning heuristics
can be applied to the auditing problem.

• Provably convergent algorithms for learning classiﬁers that are fair with respect to G, based
on a formulation as a two-player zero-sum game between a Learner (the primal player) and
an Auditor (the dual player). We provide two diﬀerent algorithms, both of which are based
on solving for the equilibrium of this game. The ﬁrst provably converges in a polynomial
number of steps and is based on simulation of the game dynamics when the Learner uses
Follow the Perturbed Leader and the Auditor uses best response; the second is only guaranteed
to converge asymptotically but is computationally simpler, and involves both players using
Fictitious Play.

• An implementation and empirical evaluation of the Fictitious Play algorithm demonstrating

its eﬀectiveness on a real dataset in which subgroup fairness is a concern.

1It also asks for equalized false negative rates, and that the false positive rate is smaller than the true positive rate.

Here, the randomness in the “rates” is taken entirely over the randomness of the classiﬁer.

2

In more detail, we start by studying the computational challenge of simply checking whether
a given classiﬁer satisﬁes equal opportunity and statistical parity. Doing this in time linear in the
number of protected groups is simple: for each protected group, we need only estimate a single
expectation. However, when there are many diﬀerent protected attributes which can be combined
to deﬁne the protected groups, their number is combinatorially large2.

We model the problem by specifying a class of functions G deﬁned over a set of d protected
attributes. G deﬁnes a set of protected subgroups. Each function g ∈ G corresponds to the pro-
tected subgroup {x : gi(x) = 1}3. The ﬁrst result of this paper is that for both equal opportunity and
statistical parity, the computational problem of checking whether a classiﬁer or decision-making
algorithm D violates statistical fairness with respect to the set of protected groups G is equivalent
to the problem of agnostically learning G [Kearns et al., 1994], in a strong and distribution-speciﬁc
sense. This equivalence has two implications:

1. First, it allows us to import computational hardness results from the learning theory litera-
ture. Agnostic learning turns out to be computationally hard in the worst case, even for
extremely simple classes of functions G (like boolean conjunctions and linear threshold
functions). As a result, we can conclude that auditing a classiﬁer D for statistical fairness
violations with respect to a class G is also computationally hard. This means we should not
expect to ﬁnd a polynomial time algorithm that is always guaranteed to solve the auditing
problem.

2. However, in practice, various learning heuristics (like boosting, logistic regression, SVMs,
backpropagation for neural networks, etc.) are commonly used to learn accurate classiﬁers
which are known to be hard to learn in the worst case. The equivalence we show between
agnostic learning and auditing is distribution speciﬁc — that is, if on a particular data set, a
heuristic learning algorithm can solve the agnostic learning problem (on an appropriately
deﬁned subset of the data), it can be used also to solve the auditing problem on the same
data set.

These results appear in Section 3.

Next, we consider the problem of learning a classiﬁer that equalizes false positive or negative
rates across all (possibly inﬁnitely many) sub-groups, deﬁned by a class of functions G. As per the
reductions described above, this problem is computationally hard in the worst case.

However, under the assumption that we have an eﬃcient oracles which solves the agnostic
learning problem, we give and analyze algorithms for this problem based on a game-theoretic
formulation. We ﬁrst prove that the optimal fair classiﬁer can be found as the equilibrium of
a two-player, zero-sum game, in which the (pure) strategy space of the “Learner” player corre-
sponds to classiﬁers in H, and the (pure) strategy space of the “Auditor” player corresponds to

2For example, as discussed in a recent Propublica investigation [Angwin and Grassegger, 2017], Facebook policy
protects groups against hate speech if the group is deﬁnable as a conjunction of protected attributes. Under the Face-
book schema, “race” and “gender” are both protected attributes, and so the Facebook policy protects “black women” as
a distinct class, separately from black people and women. When there are d protected attributes, there are 2d protected
groups. As a statistical estimation problem, this is not a large obstacle — we can estimate 2d expectations to error ε so
long as our data set has size O(d/ε2), but there is now a computational problem.

3For example, in the case of Facebook’s policy, the protected attributes include “race, sex, gender identity, religious
aﬃliation, national origin, ethnicity, sexual orientation and serious disability/disease” [Angwin and Grassegger, 2017],
and G represents the class of boolean conjunctions. In other words, a group deﬁned by individuals having any subset
of values for the protected attributes is protected.

3

subgroups deﬁned by G. The best response problems for the two players correspond to agnostic
learning and auditing, respectively. We show that both problems can be solved with a single call
to a cost sensitive classiﬁcation oracle, which is equivalent to an agnostic learning oracle. We then
draw on extant theory for learning in games and no-regret algorithms to derive two diﬀerent al-
gorithms based on simulating game play in this formulation. In the ﬁrst, the Learner employs the
well-studied Follow the Perturbed Leader (FTPL) algorithm on an appropriate linearization of its
best-response problem, while the Auditor approximately best-responds to the distribution over
classiﬁers of the Learner at each step. Since FTPL has a no-regret guarantee, we obtain an algo-
rithm that provably converges in a polynomial number of steps.

While it enjoys strong provable guarantees, this ﬁrst algorithm is randomized (due to the
noise added by FTPL), and the best-response step for the Auditor is polynomial time but compu-
tationally expensive. We thus propose a second algorithm that is deterministic, simpler and faster
per step, based on both players adopting the Fictitious Play learning dynamic. This algorithm has
weaker theoretical guarantees: it has provable convergence only asymptotically, and not in a poly-
nomial number of steps — but is more practical and converges rapidly in practice. The derivation
of these algorithms (and their guarantees) appear in Section 4.

Finally, we implement the Fictitious Play algorithm and demonstrate its practicality by eﬃ-
ciently learning classiﬁers that approximately equalize false positive rates across any group de-
ﬁnable by a linear threshold function on 18 protected attributes in the “Communities and Crime”
dataset. We use simple, fast regression algorithms as heuristics to implement agnostic learn-
ing oracles, and (via our reduction from agnostic learning to auditing) auditing oracles. Our
results suggest that it is possible in practice to learn fair classiﬁers with respect to a large class
of subgroups that still achieve non-trivial error. Full details are contained in Section 5, and for a
substantially more comprehensive empirical investigation of our method we direct the interested
reader to Kearns et al. [2018].

1.2 Further Related Work

Independent of our work, H´ebert-Johnson et al. [2017] also consider a related and complementary
notion of fairness that they call “multicalibration”. In settings in which one wishes to train a real-
valued predictor, multicalibration can be considered the “calibration” analogue for the deﬁnitions
of subgroup fairness that we give for false positive rates, false negative rates, and classiﬁcation
rates. For a real-valued predictor, calibration informally requires that for every value v ∈ [0, 1]
predicted by an algorithm, the fraction of individuals who truly have a positive label in the subset
of individuals on which the algorithm predicted v should be approximately equal to v. Multi-
calibration asks for approximate calibration on every set deﬁned implicitly by some circuit in a
set G. H´ebert-Johnson et al. [2017] give an algorithmic result that is analogous to the one we give
for learning subgroup fair classiﬁers: a polynomial time algorithm for learning a multi-calibrated
predictor, given an agnostic learning algorithm for G. In addition to giving a polynomial-time
algorithm, we also give a practical variant of our algorithm (which is however only guaranteed to
converge in the limit) that we use to conduct empirical experiments on real data.

Thematically, the most closely related piece of prior work is Zhang and Neill [2016], who
also aim to audit classiﬁcation algorithms for discrimination in subgroups that have not been
pre-deﬁned. Our work diﬀers from theirs in a number of important ways. First, we audit the
algorithm for common measures of statistical unfairness, whereas Zhang and Neill [2016] design
a new measure compatible with their particular algorithmic technique. Second, we give a for-

4

mal analysis of our algorithm. Finally, we audit with respect to subgroups deﬁned by a class of
functions G, which we can take to have bounded VC dimension, which allows us to give formal
out-of-sample guarantees. Zhang and Neill [2016] attempt to audit with respect to all possible
sub-groups, which introduces a severe multiple-hypothesis testing problem, and risks overﬁtting.
Most importantly we give actionable algorithms for learning subgroup fair classiﬁers, whereas
Zhang and Neill [2016] restrict attention to auditing.

Technically, the most closely related piece of work (and from which we take inspiration for
our algorithm in Section 4) is Agarwal et al. [2017], who show that given access to an agnostic
learning oracle for a class H, there is an eﬃcient algorithm to ﬁnd the lowest-error distribution
over classiﬁers in H subject to equalizing false positive rates across polynomially many subgroups.
Their algorithm can be viewed as solving the same zero-sum game that we solve, but in which the
“subgroup” player plays gradient descent over his pure strategies, one for each sub-group. This
ceases to be an eﬃcient or practical algorithm when the number of subgroups is large, as is our
case. Our main insight is that an agnostic learning oracle is suﬃcient to have the both players
play “ﬁctitious play”, and that there is a transformation of the best response problem such that an
agnostic learning algorithm is enough to eﬃciently implement follow the perturbed leader.

There is also other work showing computational hardness for fair learning problems. Most no-
tably, Woodworth et al. [2017] show that ﬁnding a linear threshold classiﬁer that approximately
minimizes hinge loss subject to equalizing false positive rates across populations is computation-
ally hard (assuming that refuting a random k-XOR formula is hard). In contrast, we show that
even checking whether a classiﬁer satisﬁes a false positive rate constraint on a particular data set
is computationally hard (if the number of subgroups on which fairness is desired is too large to
enumerate).

2 Model and Preliminaries

(cid:48)), y), where x ∈ X denotes a vector of
We model each individual as being described by a tuple ((x, x
(cid:48) ∈ X (cid:48) denotes a vector of unprotected attributes, and y ∈ {0, 1} denotes a label.
protected attributes, x
Note that in our formulation, an auditing algorithm not only may not see the unprotected at-
(cid:48) may represent proprietary
tributes x
features or consumer data purchased by a credit scoring company.

(cid:48), it may not even be aware of their existence. For example, x

We will write X = (x, x

(cid:48)) to denote the joint feature vector. We assume that points (X, y) are
drawn i.i.d. from an unknown distribution P . Let D be a decision making algorithm, and let D(X)
denote the (possibly randomized) decision induced by D on individual (X, y). We restrict attention
in this paper to the case in which D makes a binary classiﬁcation decision: D(X) ∈ {0, 1}. Thus we
alternately refer to D as a classiﬁer. When auditing a ﬁxed classiﬁer D, it will be helpful to make
reference to the distribution over examples (X, y) together with their induced classiﬁcation D(X).
Let Paudit(D) denote the induced target joint distribution over the tuple (x, y, D(X)) that results from
(cid:48)) but
sampling (x, x
(cid:48). Note that the randomness here is over both the randomness of
not the unprotected attributes x
P , and the potential randomness of the classiﬁer D.

, y) ∼ P , and providing x, the true label y, and the classiﬁcation D(X) = D(x, x

(cid:48)

We will be concerned with learning and auditing classiﬁers D satisfying two common statis-
tical fairness constraints: equality of classiﬁcation rates (also known as statistical parity), and
equality of false positive rates (also known as equal opportunity). Auditing for equality of false
negative rates is symmetric and so we do not explicitly consider it. Each fairness constraint is

5

deﬁned with respect to a set of protected groups. We deﬁne sets of protected groups via a family
of indicator functions G for those groups, deﬁned over protected attributes. Each g : X → {0, 1} ∈ G
has the semantics that g(x) = 1 indicates that an individual with protected features x is in group
g.

Deﬁnition 2.1 (Statistical Parity (SP) Subgroup Fairness). Fix any classiﬁer D, distribution P , col-
lection of group indicators G, and parameter γ ∈ [0, 1]. For each g ∈ G, deﬁne

αSP (g, P ) = Pr
P

[g(x) = 1]

and, βSP (g, D, P ) = |SP(D) − SP(D, g)| ,

where SP(D) = PrP ,D[D(X) = 1] and SP(D, g) = PrP ,D[D(X) = 1|g(x) = 1] denote the overall acceptance
rate of D and the acceptance rate of D on group g respectively. We say that D satisﬁes γ-statistical
parity (SP) Fairness with respect to P and G if for every g ∈ G

We will sometimes refer to SP(D) as the SP base rate.

αSP (g, P ) βSP (g, D, P ) ≤ γ.

Remark 2.2. Note that our deﬁnition references two approximation parameters, both of which are im-
portant. We are allowed to ignore a group g if it (or its complement) represent only a small fraction
of the total probability mass. The parameter α governs how small a fraction of the population we are
allowed to ignore. Similarly, we do not require that the probability of a positive classiﬁcation in every
subgroup is exactly equal to the base rate, but instead allow deviations up to β. Both of these approxi-
mation parameters are necessary from a statistical estimation perspective. We control both of them with
a single parameter γ.

Deﬁnition 2.3 (False Positive (FP) Subgroup Fairness). Fix any classiﬁer D, distribution P , collection
of group indicators G, and parameter γ ∈ [0, 1]. For each g ∈ G, deﬁne

αFP (g, P ) = Pr
P

[g(x) = 1, y = 0]

and, βFP (g, D, P ) = |FP(D) − FP(D, g)|

where FP(D) = PrD,P [D(X) = 1 | y = 0] and FP(D, g) = PrD,P [D(X) = 1 | g(x) = 1, y = 0] denote the
overall false-positive rate of D and the false-positive rate of D on group g respectively.

We say D satisﬁes γ-False Positive (FP) Fairness with respect to P and G if for every g ∈ G

We will sometimes refer to FP(D) FP-base rate.

αFP (g, P ) βFP (g, D, P ) ≤ γ.

Remark 2.4. This deﬁnition is symmetric to the deﬁnition of statistical parity fairness, except that the
parameter α is now used to exclude any group g such that negative examples (y = 0) from g (or its
complement) have probability mass less than α. This is again necessary from a statistical estimation
perspective.

For either statistical parity and false positive fairness, if the algorithm D fails to satisfy the
γ-fairness condition, then we say that D is γ-unfair with respect to P and G. We call any subgroup
g which witnesses this unfairness an γ-unfair certiﬁcate for (D, P ).

An auditing algorithm for a notion of fairness is given sample access to Paudit(D) for some clas-
siﬁer D. It will either deem D to be fair with respect to P , or will else produce a certiﬁcate of
unfairness.

6

Deﬁnition 2.5 (Auditing Algorithm). Fix a notion of fairness (either statistical parity or false positive
(cid:48) ∈ (0, 1) such that
fairness), a collection of group indicators G over the protected features, and any δ, γ, γ
(cid:48))-auditing algorithm for G with respect to distribution P is an algorithm A such that
(cid:48) ≤ γ. A (γ, γ
γ
for any classiﬁer D, when given access the distribution Paudit(D), A runs in time poly(1/γ
, log(1/δ)),
and with probability 1 − δ, outputs a γ
-unfair certiﬁcate for D whenever D is γ-unfair with respect to
(cid:48)
P and G. If D is γ

-fair, A will output “fair”.

(cid:48)

(cid:48)

As we will show, our deﬁnition of auditing is closely related to weak agnostic learning.

Deﬁnition 2.6 (Weak Agnostic Learning [Kearns et al., 1994, Kalai et al., 2008]). Let Q be a dis-
(cid:48) ∈ (0, 1/2) such that ε ≥ ε
tribution over X × {0, 1} and let ε, ε
. We say that the function class G is
(cid:48))-weakly agnostically learnable under distribution Q if there exists an algorithm L such that
(ε, ε
, 1/δ), and with probability 1 − δ, outputs a
when given sample access to Q, L runs in time poly(1/ε
hypothesis h ∈ G such that

(cid:48)

(cid:48)

err(f , Q) ≤ 1/2 − ε =⇒ err(h, Q) ≤ 1/2 − ε

(cid:48)

.

min
f ∈G

where err(h, Q) = Pr(x,y)∼Q[h(x) (cid:44) y].

Cost-Sensitive Classiﬁcation.
In this paper, we will also give reductions to cost-sensitive classi-
ﬁcation (CSC) problems. Formally, an instance of a CSC problem for the class H is given by a set
of n tuples {(Xi, c0
i corresponds to the cost for predicting label (cid:96) on point Xi.
Given such an instance as input, a CSC oracle ﬁnds a hypothesis ˆh ∈ H that minimizes the total
cost across all points:

i=1 such that c(cid:96)

i , c1

i )}n

ˆh ∈ argmin

h∈H

n(cid:88)

i=1

[h(Xi)c1

i + (1 − h(Xi))c0
i ]

(1)

A crucial property of a CSC problem is that the solution is invariant to translations of the costs.

Claim 2.7. Let {(Xi, c0
a1, a2, . . . , an

i )}n
∈ R such that ˜c(cid:96)

i , c1

i=1 be a CSC instance, and {( ˜c0
i = c(cid:96)
i + ai for all i and (cid:96). Then

i , ˜c1

i )} be a set of new costs such that there exist

argmin
h∈H

n(cid:88)

i=1

[h(Xi)c1

i + (1 − h(Xi))c0

i ] = argmin

[h(Xi) ˜c1

i + (1 − h(Xi)) ˜c0
i ]

n(cid:88)

i=1

h∈H

Remark 2.8. We note that cost-sensitive classiﬁcation is polynomially equivalent to agnostic learn-
ing Zadrozny et al. [2003]. We give both deﬁnitions above because when describing our results for
auditing, we wish to directly appeal to known hardness results for weak agnostic learning, but it is more
convenient to describe our algorithms via oracles for cost-sensitive classiﬁcation.

Follow the Perturbed Leader. We will make use of the Follow the Perturbed Leader (FTPL) algo-
rithm as a no-regret learner for online linear optimization problems [Kalai and Vempala, 2005].
To formalize the algorithm, consider S ⊂ {0, 1}d to be a set of “actions” for a learner in an on-
line decision problem. The learner interacts with an adversary over T rounds, and in each round
t, the learner (randomly) chooses some action at ∈ S, and the adversary chooses a loss vector
(cid:96)t ∈ [−M, M]d. The learner incurs a loss of (cid:104)(cid:96)t, at(cid:105) at round t.

7

FTPL is a simple algorithm that in each round perturbs the cumulative loss vector over the pre-
s<t (cid:96)s, and chooses the action that minimizes loss with respect to the perturbed
vious rounds (cid:96) =
cumulative loss vector. We present the full algorithm in Algorithm 1, and its formal guarantee in
Theorem 2.9.

(cid:80)

U be the uniform distribution over [0, 1]d, and let a1 ∈ S be

Algorithm 1 Follow the Perturbed Leader (FTPL) Algorithm

Input: Loss bound M, action set S ∈ {0, 1}d
Initialize: Let η = (1/M)

, D

(cid:113)

1√

dT

arbitrary.
For t = 1, . . . , T :

Play action at; Observe loss vector (cid:96)t and suﬀer loss (cid:104)(cid:96)t, at(cid:105).
Update:

at+1 = argmin

η

(cid:104)(cid:96)s, a(cid:105) + (cid:104)ξt, a(cid:105)





(cid:88)

s≤t

a∈S





where ξt is drawn independently for each t from the distribution D

U .

Theorem 2.9 (Kalai and Vempala [2005]). For any sequence of loss vectors (cid:96)1, . . . , (cid:96)T , the FTPL algo-
rithm has regret





E

T(cid:88)



(cid:104)(cid:96)t, at(cid:105)

− min
a∈S

T(cid:88)

(cid:104)(cid:96)t, a(cid:105) ≤ 2d5/4M

T

√

t=1
where the randomness is taken over the perturbations ξt across rounds.

t=1

2.1 Generalization Error

In this section, we observe that the error rate of a classiﬁer D, as well as the degree to which it vio-
lates γ-fairness (for both statistical parity and false positive rates) can be accurately approximated
with the empirical estimates for these quantities on a dataset (drawn i.i.d. from the underlying
distribution P ) so long as the dataset is suﬃciently large. Once we establish this fact, since our
main interest is in the computational problem of auditing and learning, in the rest of the paper,
we assume that we have direct access to the underlying distribution (or equivalently, that the
empirical data deﬁnes the distribution of interest), and do not make further reference to sample
complexity or overﬁtting issues.

A standard VC dimension bound (see, e.g. Kearns and Vazirani [1994]) states:

Theorem 2.10. Fix a class of functions H. For any distribution P , let S ∼ P m be a dataset consisting
of m examples (Xi, yi) sampled i.i.d. from P . Then for any 0 < δ < 1, with probability 1 − δ, for every
h ∈ H, we have:

|err(h, P ) − err(h, S)| ≤ O

(cid:114)





VCDIM(H) log m + log(1/δ)
m





where err(h, S) = 1
m

(cid:80)m

i=1

1[h(Xi) (cid:44) yi].

8

The above theorem implies that so long as m ≥ ˜O(VCDIM(H)/ε2), then minimizing error over
the empirical sample S suﬃces to minimize error up to an additive ε term on the true distribution
P . Below, we give two analogous statements for fairness constraints:

Theorem 2.11 (SP Uniform Convergence). Fix a class of functions H and a class of group indicators
G. For any distribution P , let S ∼ P m be a dataset consisting of m examples (Xi, yi) sampled i.i.d. from
P . Then for any 0 < δ < 1, with probability 1 − δ, for every h ∈ H and g ∈ G

(cid:12)(cid:12)(cid:12)αSP (g, P

S) βSP (g, h, P

S) − αSP (g, P ) βSP (g, h, P )

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

where P

S denotes the empirical distribution over the realized sample S.

Similarly:

Theorem 2.12 (FP Uniform Convergence). Fix a class of functions H and a class of group indicators
G. For any distribution P , let S ∼ P m be a dataset consisting of m examples (Xi, yi) sampled i.i.d. from
P . Then for any 0 < δ < 1, with probability 1 − δ, for every h ∈ H and g ∈ G, we have:

(cid:12)(cid:12)(cid:12)αFP (g, P ) βFP (g, D, P ) − αFP (g, P ) βFP (g, D, P )

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

where P

S denotes the empirical distribution over the realized sample S.

These theorems together imply that for both SP and FP subgroup fairness, the degree to which
a group g violates the constraint of γ-fairness can be estimated up to error ε, so long as m ≥
˜O((VCDIM(H) + VCDIM(G))/ε2). The proofs can be found in Appendix B.









3 Equivalence of Auditing and Weak Agnostic Learning

In this section, we give a reduction from the problem of auditing both statistical parity and false
positive rate fairness, to the problem of agnostic learning, and vice versa. This has two implica-
tions. The main implication is that, from a worst-case analysis point of view, auditing is compu-
tationally hard in almost every case (since it inherits this pessimistic state of aﬀairs from agnostic
learning). However, worst-case hardness results in learning theory have not prevented the suc-
cessful practice of machine learning, and there are many heuristic algorithms that in real-world
cases successfully solve “hard” agnostic learning problems. Our reductions also imply that these
heuristics can be used successfully as auditing algorithms, and we exploit this in the development
of our algorithmic results and their experimental evaluation.

We make the following mild assumption on the class of group indicators G, to aid in our reduc-
tions. It is satisﬁed by most natural classes of functions, but is in any case essentially without loss
of generality (since learning negated functions can be simulated by learning the original function
class on a dataset with ﬂipped class labels).

Assumption 3.1. We assume the set of group indicators G satisﬁes closure under negation: for any
g ∈ G, we also have ¬g ∈ G.

Recalling that X = (x, x

(cid:48)) and the following notions will be useful for describing our results:

9

• SP(D) = PrP ,D[D(X) = 1] and FP(D) = PrD,P [D(X) = 1 | y = 0].
• αSP (g, P ) = PrP [g(x) = 1] and αFP (g, P ) = PrP [g(x) = 1, y = 0].
• βSP (g, D, P ) = |SP(D) − SP(D, g)| and βFP (g, D, P ) = |FP(D) − FP(D, g)|.

• P D: the marginal distribution on (x, D(X)).

• P D

y=0: the conditional distribution on (x, D(X)), conditioned on y = 0.

We will think about these as the target distributions for a learning problem: i.e. the problem of
learning to predict D(X) from only the protected features x. We will relate the ability to agnosti-
cally learn on these distributions, to the ability to audit D given access to the original distribution
Paudit(D).

3.1 Statistical Parity Fairness

We give our reduction ﬁrst for SP subgroup fairness. The reduction for FP subgroup fairness
will follow as a corollary, since auditing for FP subgroup fairness can be viewed as auditing for
statistical parity fairness on the subset of the data restricted to y = 0.

Theorem 3.2. Fix any distribution P , and any set of group indicators G. Then for any γ, ε > 0, the
following relationships hold:

• If there is a (γ/2, (γ/2 − ε)) auditing algorithm for G for all D such that SP(D) = 1/2, then the

class G is (γ, γ/2 − ε)-weakly agnostically learnable under P D.

• If G is (γ, γ − ε)-weakly agnostically learnable under distribution P D for all D such that SP(D) =

1/2, then there is a (γ, (γ − ε)/2) auditing algorithm for G for SP fairness under P .

We will prove Theorem 3.2 in two steps. First, we show that any unfair certiﬁcate f for D has

non-trivial error for predicting the decision made by D from the sensitive attributes.

Lemma 3.3. Suppose that the base rate SP(D) ≤ 1/2 and there exists a function f such that

αSP (g, P ) βSP (g, D, P ) = γ.

max{Pr[D(X) = f (x)], Pr[D(X) = ¬f (x)]} ≥ SP(D) + γ.

Proof. To simplify notations, let b = SP(D) denote the base rate, α = αSP and β = βSP . First, observe
that either Pr[D(X) = 1 | f (x) = 1] = b + β or Pr[D(X) = 1 | f (x) = 1] = b − β holds.

In the ﬁrst case, we know Pr[D(X) = 1 | f (x) = 0] < b, and so Pr[D(X) = 0 | f (x) = 0] > 1 − b. It

Then

follows that

Pr[D(X) = f (x)] = Pr[D(X) = f (x) = 1] + Pr[D(X) = f (x) = 0]

= Pr[D(X) = 1 | f (x) = 1] Pr[f (x) = 1] + Pr[D(X) = 0 | f (x) = 0] Pr[f (x) = 0]
> α(b + β) + (1 − α)(1 − b)
= (α − 1)b + (1 − α)(1 − b) + b + αβ
= (1 − α)(1 − 2b) + b + αβ.

10

In the second case, we have Pr[D(X) = 0 | f (x) = 1] = (1 − b) + β and Pr[D(X) = 1 | f (x) = 0] > b. We
can then bound

Pr[D(X) = f (x)] = Pr[D(X) = 1 | f (x) = 0] Pr[f (x) = 0] + Pr[D(X) = 0 | f (x) = 1] Pr[f (x) = 1]

> (1 − α)b + α(1 − b + β) = α(1 − 2b) + b + αβ.

In both cases, we have (1 − 2b) ≥ 0 by our assumption on the base rate. Since α ∈ [0, 1], we know
max{Pr[D(X) = f (x)], Pr[D(X) = ¬f (x)]} ≥ b + αβ = b + γ

which recovers our bound.

In the next step, we show that if there exists any function f that accurately predicts the de-
cisions made by the algorithm D, then either f or ¬f can serve as an unfairness certiﬁcate for
D.
Lemma 3.4. Suppose that the base rate SP(D) ≥ 1/2 and there exists a function f such that Pr[D(X) =
f (x)] ≥ SP(D) + γ for some value γ ∈ (0, 1/2). Then there exists a function g such that

αSP (g, P ) βSP (g, D, P ) ≥ γ/2,

where g ∈ {f , ¬f }.

Proof. Let b = SP(D). We can expand Pr[D(X) = f (x)] as follows:

Pr[D(X) = f (x)] = Pr[D(X) = f (x) = 1] + Pr[D(X) = f (x) = 0]

= Pr[D(X) = 1 | f (x) = 1] Pr[f (x) = 1] + Pr[D(X) = 0 | f (x) = 0] Pr[f (x) = 0]

This means

Pr[D(X) = f (x)] − b

= (Pr[D(X) = 1 | f (x) = 1] − b) Pr[f (x) = 1] + (Pr[D(X) = 0 | f (x) = 0] − b) Pr[f (x) = 0] ≥ γ
Suppose that (Pr[D(X) = 1 | f (x) = 1] − b) Pr[f (x) = 1] ≥ γ/2, then our claim holds with g = f . Sup-
pose not, then we must have

(Pr[D(X) = 0 | f (x) = 0] − b) Pr[f (x) = 0] = ((1 − b) − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ γ/2

Note that by our assumption b ≥ (1 − b). This means

(b − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ ((1 − b) − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ γ/2

which implies that our claim holds with g = ¬f .

Proof of Theorem 3.2. Suppose that the class G satisﬁes minf ∈G err(f , P D) ≤ 1/2−γ. Then by Lemma 3.4,
there exists some g ∈ G such that Pr[g(x) = 1]| Pr[D(X) = 1 | g(x) = 1] − SP(D)| ≥ γ/2. By the as-
(cid:48) ∈ G that is an
sumption of auditability, we can then use the auditing algorithm to ﬁnd a group g
(cid:48) predicts D with an
(γ/2 − ε)-unfair certiﬁcate of D. By Lemma 3.3, we know that either g
accuracy of at least 1/2 + (γ/2 − ε).

(cid:48) or ¬g

In the reverse direction, consider the auditing problem on the classiﬁer D. We can treat each
pair (x, D(X)) as a labelled example and learn a hypothesis in G that approximates the decisions
made by D. Suppose that D is γ-unfair. Then by Lemma 3.3, we know that there exists some g ∈ G
such that Pr[D(X) = g(x)] ≥ 1/2 + γ. Therefore, the weak agnostic learning algorithm from the
(cid:48)(x)] ≥ 1/2 + (γ − ε). By Lemma 3.4,
hypothesis of the theorem will return some g
we know g

(cid:48) is a (γ − ε)/2-unfair certiﬁcate for D.

(cid:48) with Pr[D(X) = g

(cid:48) or ¬g

11

3.2 False Positive Fairness

A corollary of the above reduction is an analogous equivalence between auditing for FP subgroup
fairness and agnostic learning. This is because a FP fairness constraint can be viewed as a statis-
tical parity fairness constraint on the subset of the data such that y = 0. Therefore, Theorem 3.2
implies the following:

Corollary 3.5. Fix any distribution P , and any set of group indicators G. The following two relation-
ships hold:

• If there is a (γ/2, (γ/2 − ε)) auditing algorithm for G for all D such that FP(D) = 1/2, then the

class G is (γ, γ/2 − ε)-weakly agnostically learnable under P D

y=0.

• If G is (γ, γ −ε)–weakly agnostically learnable under distribution P D

y=0 for all D such that FP(D) =
1/2, then there is a (γ, (γ − ε)/2) auditing algorithm for FP subgroup fairness for G under distri-
bution P .

3.3 Worst-Case Intractability of Auditing

While we shall see in subsequent sections that the equivalence given above has positive algo-
rithmic and experimental consequences, from a purely theoretical perspective the reduction of
agnostic learning to auditing has strong negative worst-case implications. More precisely, we can
import a long sequence of formal intractability results for agnostic learning to obtain:

Theorem 3.6. Under standard complexity-theoretic intractability assumptions, for G the classes of con-
junctions of boolean attributes, linear threshold functions, or bounded-degree polynomial threshold func-
tions, there exist distributions P such that the auditing problem cannot be solved in polynomial time, for
either statistical parity or false positive fairness.

The proof of this theorem follows from Theorem 3.2, Corollary 3.5, and the following negative
results from the learning theory literature. Feldman et al. [2012] show a strong negative result
for weak agnostic learning for conjunctions: given a distribution on labeled examples from the
hypercube such that there exists a monomial (or conjunction) consistent with (1−ε)-fraction of the
examples, it is NP-hard to ﬁnd a halfspace that is correct on (1/2 + ε)-fraction of the examples, for
arbitrary constant ε > 0. Diakonikolas et al. [2011] show that under the Unique Games Conjecture,
no polynomial-time algorithm can ﬁnd a degree-d polynomial threshold function (PTF) that is
consistent with (1/2 + ε) fraction of a given set of labeled examples, even if there exists a degree-d
PTF that is consistent with a (1 − ε) fraction of the examples. Diakonikolas et al. [2011] also show
that it is NP-Hard to ﬁnd a degree-2 PTF that is consistent with a (1/2 + ε) fraction of a given set
of labeled examples, even if there exists a halfspace (degree-1 PTF) that is consistent with a (1 − ε)
fraction of the examples.

While Theorem 3.6 shows that certain natural subgroup classes G yield intractable auditing
problems in the worst case, in the rest of the paper we demonstrate that eﬀective heuristics for
this problem on speciﬁc (non-worst case) distributions can be used to derive an eﬀective and
practical learning algorithm for subgroup fairness.

12

4 A Learning Algorithm Subject to Fairness Constraints G

In this section, we present an algorithm for training a (randomized) classiﬁer that satisﬁes false-
positive subgroup fairness simultaneously for all protected subgroups speciﬁed by a family of
group indicator functions G. All of our techniques also apply to a statistical parity or false negative
rate constraint.

Let S denote a set of n labeled examples {zi = (xi, x

i=1, and let P denote the empirical
distribution over this set of examples. Let H be a hypothesis class deﬁned over both the protected
and unprotected attributes, and let G be a collection of group indicators over the protected at-
tributes. We assume that H contains a constant classiﬁer (which implies that there is at least one
fair classiﬁer to be found, for any distribution).

i), yi)}n

Our goal will be to ﬁnd the distribution over classiﬁers from H that minimizes classiﬁcation
error subject to the fairness constraint over G. We will design an iterative algorithm that, when
given access to a CSC oracle, computes an optimal randomized classiﬁer in polynomial time.

(cid:48)

Let D denote a probability distribution over H. Consider the following Fair ERM (Empirical

Risk Minimization) problem:

such that ∀g ∈ G

E
h∼D

[err(h, P )]

min
D∈∆H
αFP (g, P ) βFP (g, D, P ) ≤ γ.

(2)

(3)

(cid:48)) (cid:44) y], and the quantities αFP and βFP are deﬁned in Deﬁnition 2.3.
where err(h, P ) = PrP [h(x, x
We will write OPT to denote the objective value at the optimum for the Fair ERM problem, that is
the minimum error achieved by a γ-fair distribution over the class H.

Observe that the optimization is feasible for any distribution P : the constant classiﬁers that
labels all points 1 or 0 satisfy all subgroup fairness constraints. At the moment, the number of
decision variables and constraints may be inﬁnite (if H and G are inﬁnite hypothesis classes), but
we will address this momentarily.

Assumption 4.1 (Cost-Sensitive Classiﬁcation Oracle). We assume our algorithm has access to the
cost-sensitive classication oracles CSC(H) and CSC(G) over the classes H and G.

Our main theoretical result is an computationally eﬃcient oracle-based algorithm for solving

the Fair ERM problem.

Theorem 4.2. Fix any ν, δ ∈ (0, 1). Then given an input of n data points and accuracy parameters ν, δ
and access to oracles CSC(H) and CSC(G), there exists an algorithm runs in polynomial time, and with
probability at least 1 − δ, output a randomized classiﬁer ˆD such that err( ˆD, P ) ≤ OPT +ν, and for any
g ∈ G, the fairness constraint violations satisﬁes

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ + O(ν).

Overview of our solution. We present our solution in steps:

• Step 1: Fair ERM as LP. First, we rewrite the Fair ERM problem as a linear program with
ﬁnitely many decision variables and constraints even when H and G are inﬁnite. To do this,
we take advantage of the fact that Sauer’s Lemma lets us bound the number of labellings
that any hypothesis class H of bounded VC dimension can induce on any ﬁxed dataset.

13

The LP has one variable for each of these possible labellings, rather than one variable for
each hypothesis. Moreover, again by Sauer’s Lemma, we have one constraint for each of the
ﬁnitely many possible subgroups induced by G on the ﬁxed dataset, rather than one for each
of the (possibly inﬁnitely many) subgroups deﬁnable over arbitrary datasets. This step is
important — it will guarantee that strong duality holds.

• Step 2: Formulation as Game. We then derive the partial Lagrangian of the LP, and note
that computing an approximately optimal solution to this LP is equivalent to ﬁnding an ap-
proximate minmax solution for a corresponding zero-sum game, in which the payoﬀ func-
tion U is the value of the Lagrangian. The pure strategies of the primal or “Learner” player
correspond to classiﬁers h ∈ H, and the pure strategies of the dual or “Auditor” player cor-
respond to subgroups g ∈ G. Intuitively, the Learner is trying to minimize the sum of the
prediction error and a fairness penalty term (given by the Lagrangian), and the Auditor is
trying to penalize the fairness violation of the Learner by ﬁrst identifying the subgroup with
the greatest fairness violation and putting all the weight on the dual variable corresponding
to this subgroup. In order to reason about convergence, we restrict the set of dual variables
to lie in a bounded set: C times the probability simplex. C is a parameter that we have
to set in the proof of our theorem to give the best theoretical guarantees — but it is also a
parameter that we will vary in the experimental section.

• Step 3: Best Responses as CSC. We observe that given a mixed strategy for the Auditor,
the best response problem of the Learner corresponds to a CSC problem. Similarly, given a
mixed strategy for the Learner, the best response problem of the Auditor corresponds to an
auditing problem (which can be represented as a CSC problem). Hence, if we have oracles
for solving CSC problems, we can compute best responses for both players, in response to
arbitrary mixed strategies of their opponents.

• Step 4: FTPL for No-Regret. Finally, we show that the ability to compute best responses
for each player is suﬃcient to implement dynamics known to converge quickly to equilib-
rium in zero-sum games. Our algorithm has the Learner play Follow the Perturbed Leader
(FTPL) Kalai and Vempala [2005], which is a no-regret algorithm, against an Auditor who at
every round best responds to the learner’s mixed strategy. By the seminal result of Freund
and Schapire [1996], the average plays of both players converge to an approximate equilib-
rium. In order to implement this in polynomial time, we need to represent the loss of the
learner as a low-dimensional linear optimization problem. To do so, we ﬁrst deﬁne an ap-
propriately translated CSC problem for any mixed strategy λ by the Auditor, and cast it as
a linear optimization problem.

4.1 Rewriting the Fair ERM Problem

To rewrite the Fair ERM problem, we note that even though both G and H can be inﬁnite sets,
the sets of possible labellings on the data set S induced by these classes are ﬁnite. More formally,
we will write G(S) and H(S) to denote the set of all labellings on S that are induced by G and H
respectively, that is

G(S) = {(g(x1), . . . , g(xn)) | g ∈ G}

and,

H(S) = {(h(X1), . . . , h(Xn)) | h ∈ H}

We can bound the cardinalities of G(S) and H(S) using Sauer’s Lemma.

14

Lemma 4.3 (Sauer’s Lemma (see e.g. Kearns and Vazirani [1994])). Let S be a data set of size n. Let
d1 = VCDIM(H) and d2 = VCDIM(G) be the VC-dimensions of the two classes. Then
(cid:17)

(cid:17)

|H(S)| ≤ O

(cid:16)
nd1

and

|G(S)| ≤ O

(cid:16)
nd2

.

Given this observation, we can then consider an equivalent optimization problem where the
distribution D is over the set of labellings in H(S), and the set of subgroups are deﬁned by the
labellings in G(S). We will view each g in G(S) as a Boolean function.

To simplify notations, we will deﬁne the following “fairness violation” functions for any g ∈ G

and any h ∈ H:

Φ+(h, g) ≡ αFP (g, P ) (FP(h) − FP(h, g)) − γ
Φ−(h, g) ≡ αFP (g, P ) (FP(h, g) − FP(h)) − γ

Moreover, for any distribution D over H, for any sign • ∈ {+, −}

Φ•(D, g) = E
h∼D

[Φ•(h, g)] .

Claim 4.4. For any g ∈ G, h ∈ H, and any ν > 0,

max{Φ+(D, g), Φ−(D, g)} ≤ ν

if and only if αFP (g, P ) βFP (g, D, P ) ≤ γ + ν.

Thus, we will focus on the following equivalent optimization problem.

such that for each g ∈ G(S) :

min
D∈∆H(S)

[err(h, P )]

E
h∼D
Φ+(D, g) ≤ 0
Φ−(D, g) ≤ 0

For each pair of constraints (7) and (8), corresponding to a group g ∈ G(S), we introduce a pair

of dual variables λ+

g and λ

−
g . The partial Lagrangian of the linear program is the following:

L(D, λ) = E
h∼D

[err(h, P )] +

(cid:88)

(cid:16)
λ+

g∈G(S)

−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:17)

By Sion’s minmax theorem [Sion, 1958], we have

min
D∈∆H(S)

max
λ∈R2|G(S)|
+

L(p, λ) = max
λ∈R2|G(S)|

+

min
D∈∆H(S)

L(p, λ) = OPT

where OPT denotes the optimal objective value in the fair ERM problem. Similarly, the distri-
L(D, λ) corresponds to an optimal feasible solution to the fair ERM linear
bution arg minD maxλ
program. Thus, ﬁnding an optimal solution for the fair ERM problem reduces to computing a
minmax solution for the Lagrangian. Our algorithms will both compute such a minmax solution
by iteratively optimizing over both the primal variables D and dual variables λ. In order to guar-
antee convergence in our optimization, we will restrict the dual space to the following bounded
set:

Λ = {λ ∈ R2|G(S)|

+

| (cid:107)λ(cid:107)

1

≤ C}.

15

(4)

(5)

(6)

(7)

(8)

where C will be a parameter of our algorithm. Since Λ is a compact and convex set, the minmax
condition continues to hold [Sion, 1958]:

min
D∈∆H(S)

max
λ∈Λ

L(D, λ) = max
λ∈Λ

min
D∈∆H(S)

L(D, λ)

(9)

If we knew an upper bound C on the (cid:96)1 norm of the optimal dual solution, then this restriction
on the dual solution would not change the minmax solution of the program. We do not in general
know such a bound. However, we can show that even though we restrict the dual variables to
lie in a bounded set, any approximate minmax solution to Equation (9) is also an approximately
optimal and approximately feasible solution to the original fair ERM problem.

Theorem 4.5. Let ( ˆD, ˆλ) be a ν-approximate minmax solution to the Λ-bounded Lagrangian problem
in the sense that

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν and, L( ˆD, ˆλ) ≥ max
λ∈Λ

L( ˆD, λ) − ν.

Then err( ˆD, P ) ≤ OPT +2ν and for any g ∈ G(S),

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C

.

4.2 Zero-Sum Game Formulation

To compute an approximate minmax solution, we will ﬁrst view Equation (9) as the following two
player zero-sum matrix game. The Learner (or the minimization player) has pure strategies cor-
responding to H, and the Auditor (or the maximization player) has pure strategies corresponding
to the set of vertices Λpure in Λ — more precisely, each vertex or pure strategy either is the all
zero vector or consists of a choice of a g ∈ G(S), along with the sign + or − that the corresponding
g-fairness constraint will have in the Lagrangian. More formally, we write

•
g = C | g ∈ G(S), • ∈ {±}} ∪ {0}
Λpure = {λ ∈ Λ with λ

Even though the number of pure strategies scales linearly with |G(S)|, our algorithm will never
need to actually represent such vectors explicitly. Note that any vector in Λ can be written as a
convex combination of the maximization player’s pure strategies, or in other words: as a mixed
strategy for the Auditor. For any pair of actions (h, λ) ∈ H × Λpure, the payoﬀ is deﬁned as
(cid:88)

U (h, λ) = err(h, P ) +

(cid:16)
λ+

−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:17)

.

Claim 4.6. Let D ∈ ∆H(S) and λ ∈ Λ such that (p, λ) is a ν-approximate minmax equilibrium in the
zero-sum game deﬁned above. Then (p, λ) is also a ν-approximate minmax solution for Equation (9).

Our problem reduces to ﬁnding an approximate equilibrium for this game. A key step in our
solution is the ability to compute best responses for both players in the game, which we now show
can be solved by the cost-sensitive classication (CSC) oracles.

g∈G(S)

16

Learner’s best response as CSC. Fix any mixed strategy (dual solution) λ ∈ Λ of the Auditor.
The Learner’s best response is given by:

err(h, P ) +

argmin
D∈∆H(S)

(cid:88)

g∈G(S)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

(10)

Note that it suﬃces for the Learner to optimize over deterministic classiﬁers h ∈ H, rather than
distributions over classiﬁers. This is because the Learner is solving a linear optimization problem
over the simplex, and so always has an optimal solution at a vertex (i.e. a single classiﬁer h ∈ H).
We can reduce this problem to one that can be solved with a single call to a CSC oracle.
In
particular, we can assign costs to each example (Xi, yi) as follows:

• if yi = 1, then c0

i = 0 and c1

i = − 1
n ;

• otherwise, c0

i = 0 and

c1
i =

1
n

+

1
n

(cid:88)

g∈G(S)

(λ+
g

−
g ) (Pr[g(x) = 1 | y = 0] − 1[g(xi) = 1])
− λ

(11)

Given a ﬁxed set of dual variables λ, we will write LC(λ) ∈ Rn to denote the vector of costs for
labelling each datapoint as 1. That is, LC(λ) is the vector such that for any i ∈ [n], LC(λ)i = c1
i .

Remark 4.7. Note that in deﬁning the costs above, we have translated them from their most natural
values so that the cost of labeling any example with 0 is 0. In doing so, we recall that by Claim 2.7,
the solution to a cost-sensitive classiﬁcation problem is invariant to translation. As we will see, this
will allow us to formulate the learner’s optimization problem as a low-dimensional linear optimization
problem, which will be important for an eﬃcient implementation of follow the perturbed leader. In
particular, if we ﬁnd a hypothesis that produces the n labels y = (y1, . . . , yn) for the n points in our
dataset, then the cost of this labelling in the CSC problem is by construction (cid:104)LC(λ), y(cid:105).

Auditor’s best response as CSC. Fix any mixed strategy (primal solution) p ∈ ∆H(S) of the
Learner. The Auditor’s best response is given by:
(cid:88)

(cid:88)

(cid:16)
λ+

−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:17)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

argmax
λ∈Λ

err(D, P ) +

g∈G(S)

= argmax
λ∈Λ

g∈G(S)

(12)
To ﬁnd the best response, consider the problem of computing ( ˆg, ˆ•) = argmax(g,•) Φ•(D, g).
There are two cases. In the ﬁrst case, p is a strictly feasible primal solution: that is Φˆ•(D, ˆg) < 0.
In this case, the solution to (12) sets λ = 0. Otherwise, if p is not strictly feasible, then by the
following Lemma 4.8 the best response is to set λˆ•

ˆg = C (and all other coordinates to 0).

Lemma 4.8. Fix any D ∈ ∆H(S) such that that maxg∈G(S)
(cid:48))
with one non-zero coordinate (λ

{Φ+(D, g), Φ−(D, g)} > 0. Let λ

(cid:48) ∈ Λ be vector

Then L(D, λ

(cid:48)) ≥ maxλ∈Λ L(D, λ).

•(cid:48)
g (cid:48) = C, where
, •(cid:48)

(g

(cid:48)

) = argmax

(g,•)∈G(S)×{±}

{Φ•(D, g)}

17

Therefore, it suﬃces to solve for argmax(g,•) Φ•(D, g). We proceed by solving argmaxg Φ+(D, g)
and argmaxg Φ−(D, g) separately: both problems can be reduced to a cost-sensitive classiﬁcation
problem. To solve for argmaxg Φ+(D, g) with a CSC oracle, we assign costs to each example (Xi, yi)
as follows:

• if yi = 1, then c0

i = 0 and c1

i = 0;

• otherwise, c0

i = 0 and

c1
i =

(cid:20)

−1
n

E
h∼D

[FP(h)] − E
h∼D

(cid:21)
[h(Xi)]

(13)

To solve for argmaxg Φ−(D, g) with a CSC oracle, we assign the same costs to each example

(Xi, yi), except when yi = 0, labeling “1” incurs a cost of

c1
i =

(cid:20)

−1
n

E
h∼D

[h(Xi)] − E
h∼D

(cid:21)
[FP(h)]

4.3 Solving the Game with No-Regret Dynamics

To compute an approximate equilibrium of the zero-sum game, we will simulate the following no-
regret dynamics between the Learner and the Auditor over rounds: over each of the T rounds, the
Learner plays a distribution over the hypothesis class according to a no-regret learning algorithm
(Follow the Perturbed Leader), and the Auditor plays an approximate best response against the
Learner’s distribution for that round. By the result of Freund and Schapire [1996], the average
plays of both players over time converge to an approximate equilibrium of the game, as long as
the Learner has low regret.

Theorem 4.9 (Freund and Schapire [1996]). Let D1, D2, . . . , DT ∈ ∆H(S) be a sequence of distribu-
tions played by the Learner, and let λ1, λ2, . . . , λT ∈ Λpure be the Auditor’s sequence of approximate best
responses against these distributions respectively. Let D = 1
t=1 λt be the two
T
players’ empirical distributions over their strategies. Suppose that the regret of the Learner satisﬁes

t=1 Dt and λ = 1
T

(cid:80)T

(cid:80)T

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105)− min
h∈H(S)

T(cid:88)

t=1

U (h, λt) ≤ γLT

and max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)]−

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) ≤ γAT .

Then (D, λ) is an (γL + γA)-approximate minimax equilibrium of the game.

Our Learner will play using the Follow the Perturbed Leader (FTPL), which gives a no-regret
guarantee. In order to implement FPTL, we will ﬁrst need to formulate the Learner’s best response
problem as a linear optimization problem over a low dimensional space. For each round t, let
t
s<t λs be the vector representing the sum of the actions played by the auditor over previous
λ
t
rounds, and recall that LC(λ
) is the cost vector given by our cost-sensitive classiﬁcation reduction.
t
is the following linear optimization problem
Then the Learner’s best response problem against λ

(cid:80)

=

t

(cid:104)LC(λ

), h(cid:105).

min
h∈H(S)

18

To run the FTPL algorithm, the Learner will optimize a “perturbed” version of the problem above.
In particular, the Learner will play a distribution Dt over the hypothesis class H(S) that is im-
plicitely deﬁned by the following sampling operation. To sample a hypothesis h from Dt, the
learner solves the following randomized optimization problem:

min
h∈H(S)

t
(cid:104)LC(λ

), h(cid:105) +

(cid:104)ξ, h(cid:105),

1
η

(14)

where η is a parameter and ξ is a noise vector drawn from the uniform distribution over [0, 1]n.
Note that while it is intractable to explicitly represent the distribution Dt (which has support size
scaling with |H(S)|), we can sample from Dt eﬃciently given access to a cost-sensitive classiﬁcation
oracle for H. By instantiating the standard regret bound of FTPL for online linear optimization
(Theorem 2.9), we get the following regret bound for the Learner.

Lemma 4.10. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm with η = n
, and λ1, . . . , λT be the
sequence of plays by the Auditor. Then

1√
nT

(1+C)

(cid:113)

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

T(cid:88)

t=1

U (h, λt) ≤ 2n1/4(1 + C)

T

√

Now we consider how the Auditor (approximately) best responds to the distribution Dt. The
main obstacle is that we do not have an explicit representation for Dt. Thus, our ﬁrst step is to
approximate Dt with an explicitly represented sparse distribution ˆDt. We do that by drawing m
i.i.d. samples from Dt, and taking the empirical distribution ˆDt over the sample. The Auditor
will best respond to this empirical distribution ˆDt. To show that any best response to ˆDt is also an
approximate best response to Dt, we will rely on the following uniform convergence lemma, which
bounds the diﬀerence in expected payoﬀ for any strategy of the auditor, when played against Dt
as compared to ˆDt.
Lemma 4.11. Fix any ξ, δ ∈ (0, 1) and any distribution D over H(S). Let h1, . . . , hm be m i.i.d. draws
from p, and ˆD be the empirical distribution over the realized sample. Then with probability at least 1 − δ
over the random draws of hj’s, the following holds,

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as long as m ≥ c0

C2(ln(1/δ)+d2 ln(n))
ξ2

for some absolute constant c0 and d2 = VCDIM(G).

Using Lemma 4.11, we can derive a regret bound for the Auditor in the no-regret dynamics.

Lemma 4.12. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm. For each Dt, let ˆDt be the empirical distri-
bution over m i.i.d. draws from Dt. Let λ1, . . . , λT be the Auditor’s best responses against ˆD1, . . . , ˆDT .
Then with probability 1 − δ,

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:114)

(cid:104)
U (h, λt)

(cid:105) ≤ T

c0C2(ln(T /δ) + d2 ln(n))
m

for some absolute constant c0 and d2 = VCDIM(G).

19

Finally, let D and λ be the average of the strategies played by the two players over the course
of the dynamics. Note that D is an average of many distributions with large support, and so D
itself has support size that is too large to represent explicitely. Thus, we will again approximate D
with a sparse distribution ˆD estimated from a sample drawn from D. Note that we can eﬃciently
sample from D given access to a CSC oracle. To sample, we ﬁrst uniformly randomly select a
round t ∈ [T ], and then use the CSC oracle to solve the sampling problem deﬁned in (14), with the
noise random variable ξ freshly sampled from its distribution. The full algorithm is described in
Algorithm 2 and we present the proof for Theorem 4.2 below.

Algorithm 2 FairNR: Fair No-Regret Dynamics

Input: distribution P over n labelled data points, CSC oracles CSC(H) and CSC(G), dual bound
C, and target accuracy parameter ν, δ
0
= 0, η = n
Initialize: Let C = 1/ν, λ

(cid:113)

,

(1+C)

1√
nT

m =

(ln(2T /δ)d2 ln(n)) C2c0T
n(1 + C)2 ln(2/δ)

√

and,

T =

√

4

n ln(2/δ)

ν4

For t = 1, . . . , T :

For s = 1, . . . m:

Sample from the Learner’s FTPL distribution:

Draw a random vector ξs uniformly at random from [0, 1]n
Use the oracle CSC(H) to compute h(s,t) = argminh∈H(S)

(cid:104)LC(λ

(t−1)

), h(cid:105) + 1
η

(cid:104)ξs, h(cid:105)

Let ˆDt be the empirical distribution over {hs,t}

Auditor best responds to ˆDt:

Use the oracle CSC(G) to compute λt = argmaxλ

E

h∼ ˆD [U (h, λ)]

t
Update: Let λ

=

(cid:80)

t(cid:48)≤t λt(cid:48)

Sample from the average distribution D =

(cid:80)T

t=1 Dt:

For s = 1, . . . m:

Draw a random number r ∈ [T ] and a random vector ξs uniformly at random from [0, 1]n
Use the oracle CSC(H) to compute h(r,t) = argminh∈H(S)

(cid:104)LC(λ

(cid:104)ξs, h(cid:105)

), h(cid:105) + 1
η

(r−1)

Let ˆD be the empirical distribution over {hr,t}

Output: ˆD as a randomized classiﬁer

Proof of Theorem 4.2. By Theorem 4.5, it suﬃces to show that with probability at least 1 − δ, ( ˆD, λ)
is a ν-approximate equilibrium in the zero-sum game. As a ﬁrst step, we will rely on Theorem 4.9
to show that (D, λ) forms an approximate equilibrium.

By Lemma 4.10, the regret of the sequence D1, . . . , DT is bounded by:

γL =


T(cid:88)


t=1

1
T

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

T(cid:88)

t=1

U (h, λt)





≤ 2n1/4(1 + C)
√
T

20

By Lemma 4.12, with probability 1 − δ/2, we have

(cid:114)

≤

γA

c0C2(ln(2T /δ) + d2 ln(n))
m

We will condition on this upper-bound event on γA for the rest of this proof, which is the case
except with probability δ/2. By Theorem 4.9, we know that the average plays (D, λ) form an
(γL + γA)-approximate equilibrium.

Finally, we need to bound the additional error for outputting the sparse approximation ˆD
instead of D. We can directly apply Lemma 4.11, which implies that except with probability δ/2,
the pair ( ˆD, λ) form a R-approximate equilibrium, with

R ≤ γA + γL +

(cid:112)

c0C2(ln(2/δ) + d2 ln(n))
m

√

Note that R ≤ ν as long as we have C = 1/ν,

m =

(ln(2T /δ)d2 ln(n)) C2c0T
n(1 + C)2 ln(2/δ)

√

and,

T =

√

4

n ln(2/δ)

ν4

This completes our proof.

5 Experimental Evaluation

We now describe an experimental evaluation of our proposed algorithmic framework on a dataset
in which fairness is a concern, due to the preponderance of racial and other sensitive features. For
far more detailed experiments on four real datasets investigating the convergence properties of
our algorithm, evaluating its accuracy vs. fairness tradeoﬀs, and comparing our approach to the
recent algorithm of Agarwal et al. [2017], we direct the reader to Kearns et al. [2018]. Python code
and an illustrative Jupyter notebook are provided here (https://github.com/algowatchpenn/GerryFair).
While the no-regret-based algorithm described in the last section enjoys provably polynomial
time convergence, for the experiments we instead implemented a simpler yet eﬀective algorithm
based on Fictitious Play dynamics. We ﬁrst describe and discuss this modiﬁed algorithm.

5.1 Solving the Game with Fictitious Play

Like the algorithm given in the last section, the algorithm we implemented works by simulating
a game dynamic that converges to Nash equilibrium in the zero-sum game that we derived, cor-
responding to the Fair ERM problem. Rather than using a no-regret dynamic, we instead use a
simple iterative procedure known as Fictitious Play [Brown, 1949]. Fictitious Play dynamics has
the beneﬁt of being more practical to implement: at each round, both players simply need to com-
pute a single best response to the empirical play of their opponents, and this optimization requires
only a single call to a CSC oracle. In contrast, the FTPL dynamic we gave in the previous section
requires making many calls to a CSC oracle per round — a computationally expensive process —
in order to ﬁnd a sparse approximation to the Learner’s mixed strategy at that round. Fictitious
Play also has the beneﬁt of being deterministic, unlike the randomized sampling required in the
FTPL no-regret dynamic, thus eliminating a source of experimental variance.

21

The disadvantage is that Fictitious Play is only known to converge to equilibrium in the
limit Robinson [1951], rather than in a polynomial number of rounds (though it is conjectured
to converge quickly under rather general circumstances; see Daskalakis and Pan [2014] for a re-
cent discussion). Nevertheless, this is the algorithm that we use in our experiments — and as we
will show, it performs well on real data, despite the fact that it has weaker theoretical guarantees
compared to the algorithm we presented in the last section.

Fictitious play proceeds in rounds, and in every round each player chooses a best response
to his opponent’s empirical history of play across previous rounds, by treating it as the mixed
strategy that randomizes uniformly over the empirical history. Pseudocode for the implemented
algorithm is given below.

Algorithm 3 FairFictPlay: Fair Fictitious Play

Input: distribution P over the labelled data points, CSC oracles CSC(H) and CSC(G) for the
classes H(S) and G(S) respectively, dual bound C, and number of rounds T
Initialize: set h0 to be some classiﬁer in H, set λ0 to be the zero vector. Let D and λ be the point
distributions that put all their mass on h0 and λ0 respectively.
For t = 1, . . . , T :

Compute the empirical play distributions:

Let D be the uniform distribution over the set of classiﬁers {h0, . . . , ht−1}
Let λ =

be the auditor’s empirical dual vector

(cid:80)
t

(cid:48)

(cid:48)

<t λt
t

Learner best responds: Use the oracle CSC(H) to compute ht = argminh∈H(S)
Auditor best responds: Use the oracle CSC(G) to compute λt = argmaxλ

(cid:104)LC(λ), h(cid:105)
h∼D [U (h, λ)]

E

Output: the ﬁnal empirical distribution D over classiﬁers

5.2 Description of Data

The dataset we use for our experimental valuation is known as the “Communities and Crime”
(C&C) dataset, available at the UC Irvine Data Repository4. Each record in this dataset describes
the aggregate demographic properties of a diﬀerent U.S. community; the data combines socio-
economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey,
and crime data from the 1995 FBI UCR. The total number of records is 1994, and the number of
features is 122. The variable to be predicted is the rate of violent crime in the community.

While there are larger and more recent datasets in which subgroup fairness is a potential con-
cern, there are properties of the C&C dataset that make it particularly appealing for the initial
experimental evaluation of our proposed algorithm. Foremost among these is the relatively high
number of sensitive or protected attributes, and the fact that they are real-valued (since they rep-
resent aggregates in a community rather than speciﬁc individuals). This means there is a very
large number of protected sub-groups that can be deﬁned over them. There are distinct con-
tinuous features measuring the percentage or per-capita representation of multiple racial groups
(including white, black, Hispanic, and Asian) in the community, each of which can vary indepen-
dently of the others. Similarly, there are continuous features measuring the average per capita
incomes of diﬀerent racial groups in the community, as well as features measuring the percentage
of each community’s police force that falls in each of the racial groups. Thus restricting to features

4http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime

22

capturing race statistics and a couple of related ones (such as the percentage of residents who do
not speak English well), we obtain an 18-dimensional space of real-valued protected attributes.
We note that the C&C dataset has numerous other features that arguably could or should be pro-
tected as well (such as gender features), which would raise the dimensionality of the protected
subgroups even further. 5

We convert the real-valued rate of violent crime in each community to a binary label indicating
whether the community is in the 70th percentile of that value, indicating that it is a relatively
high-crime community. Thus the strawman baseline that always predicts 0 (lower crime) has
error approximately 30% or 0.3 on this classiﬁcation problem. We chose the 70th percentile since
it seems most natural to predict the highest crime rates.

As in the theoretical sections of the paper, our main interest and emphasis is on the eﬀective-

ness of our proposed algorithm FairFictPlay on a given dataset, including:

• Whether the algorithm in fact converges, and does so in a feasible amount of computation.
Recall that formal convergence is only guaranteed under the assumption of oracles that do
not exist in practice, and even then is only guaranteed asymptotically.

• Whether the classiﬁer learned by the algorithm has nontrivial accuracy, as well as strong

subgroup fairness properties.

racy and subgroup fairness.

• Whether the algorithm and dataset permits nontrivial tuning of the trade-oﬀ between accu-

As discussed in Section 2.1, we note that all of these issues can be investigated entirely in-sample,
without concern for generalization performance. Thus for simplicity, despite the fact that our
algorithm enjoys all the usual generalization properties depending on the VC dimension of the
Learner’s hypothesis space and the Auditor’s subgroup space (see Theorems 2.12 and 2.11), we
report all results here on the full C&C dataset of 1994 points, treating it as the true distribution
of interest.

5.3 Algorithm Implementation

The main details in the implementation of FairFictPlay are the identiﬁcation of the model classes
for Learner and Auditor, the implementation of the cost sensitive classiﬁcation oracle and auditing
oracle, and the identiﬁcation of the protected features for Auditor. For our experiments, at each
round Learner chooses a linear threshold function over all 122 features. We implement the cost
sensitive classiﬁcation oracle via a two stage regression procedure. In particular, the inputs to
the cost sensitive classiﬁcation oracle are cost vectors c0, c1, where the ith element of ck is the cost
of predicting k on datapoint i. We train two linear regression models r0, r1 to predict c0 and c1
respectively, using all 122 features. Given a new point x, we predict the cost of classifying x as 0
and 1 using our regression models: these predictions are r0(x) and r1(x) respectively. Finally we
output the prediction ˆy corresponding to lower predicted cost: ˆy = argmini∈{0,1}ri(x).

Auditor’s model class consists of all linear threshold functions over just the 18 aforementioned
protected race-based attributes. As per the algorithm, at each iteration t Auditor attempts to ﬁnd
a subgroup on which the false positive rate is substantially diﬀerent than the base rate, given

5Ongoing experiments on other datasets where fairness is a concern will be reported on in a forthcoming experi-

mental paper.

23

the Learner’s randomized classiﬁer so far. We implement the auditing oracle by treating it as
a weighted regression problem in which the goal is ﬁnd a linear function (which will be taken
to deﬁne the subgroup) that on the negative examples, can predict the Learner’s probabilistic
classiﬁcation on each point. We use the same regression subroutine as Learner does, except that
Auditor only has access to the 18 sensitive features, rather than all 122.

Recall that in addition to the choices of protected attributes and model classes for Learner
and Auditor, FairFictPlay has a parameter C, which is a bound on the norm of the dual variables
for Auditor (the dual player). While the theory does not provide an explicit bound or guide for
choosing C, it needs to be large enough to permit the dual player to force the minmax value of the
game. For our experiments we chose C = 10, which despite being a relatively small value seems
to suﬃce for (approximate) convergence.

The other and more meaningful parameter of the algorithm is the bound γ in the Fair ERM
optimization problem implemented by the game, which controls the amount of unfairness per-
mitted. If on a given round the subgroup disparity found by the Auditor is greater than γ, the
Learner must react by adding a fairness penalty for this subgroup to its objective function; if it is
smaller than γ, the Learner can ignore it and continue to optimize its previous objective function.
Ideally, and as we shall see, varying γ allows us to trace out a menu of trade-oﬀs between accuracy
and fairness.

5.4 Results

Particularly in light of the gaps between the idealized theory and the actual implementation,
the most basic questions about FairFictPlay are whether it converges at all, and if so, whether
it converges to “interesting” models — that is, models with both nontrivial classiﬁcation error
(much better than the 30% or 0.3 baserate), and nontrivial subgroup fairness (much better than
ignoring fairness altogether). We shall see that at least for the C&C dataset, the answers to these
questions is strongly aﬃrmative.

(a)

(b)

Figure 1: Evolution of the error and unfairness of Learner’s classiﬁer across iterations, for varying
choices of γ. (a) Error εt of Learner’s model vs iteration t. (b) Unfairness γt of subgroup found by
Auditor vs. iteration t, as measured by Deﬁnition 2.3. See text for details.

24

We begin by examining the evolution of the error and unfairness of Learner’s model. In the
left panel of Figure 1 we show the error of the model found by Learner vs. iteration for values of
γ ranging from 0 to 0.029. Several comments are in order.

First, after an initial period in which there is a fair amount of oscillatory behavior, by 6000
iterations most of the curves have largely ﬂattened out, and by 8,000 iterations it appears most
but not all have reached approximate convergence. Second, while the top-to-bottom ordering of
these error curves is approximately aligned with decreasing γ — so larger γ generally results in
lower error, as expected — there are many violations of this for small t, and even a few at large
t. Third, and as we will examine more closely shortly, the converged values at large t do indeed
exhibit a range of errors.

In the right panel of Figure 1, we show the corresponding unfairness γt of the subgroup found
by the Auditor at each iteration t for the same runs and values of the parameter γ (indicated by
horizontal dashed lines), with the same color-coding as for the left panel. Now the ordering is
generally reversed — larger values of γ generally lead to higher γt curves, since the fairness con-
straint on the Learner is weaker. We again see a great deal of early oscillatory behavior, with most
γt curves then eventually settling at or near their corresponding input γ value, as Learner and
Auditor engage in a back-and-forth struggle for lower error for Learner and γ-subgroup fairness
for Auditor.

(a)

(b)

Figure 2:
(a) Pareto-optimal error-unfairness values, color coded by varying values of the input
parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same
range but are sampled more densely to get a smoother frontier. See text for details.

For any choice of the parameter γ, and each iteration t, the two panels of Figure 1 yield a pair
of realized values (cid:104)εt, γt
(cid:105) from the experiment, corresponding to a Learner model whose error is
εt, and for which the worst subgroup the Auditor was able to ﬁnd had unfairness γt. The set of all
(cid:105) pairs across all runs or γ values thus represents the diﬀerent trade-oﬀs between error and
(cid:104)εt, γt
unfairness found by our algorithm on the data. Most of these pairs are of course Pareto-dominated
by other pairs, so we are primarily interested in the undominated frontier.

In the left panel of Figure 2, for each value of γ we show the Pareto-optimal pairs, color-coded
for the value of γ. Each value of γ yields a set or cloud of undominated pairs that are usually fairly

25

close to each other, and as expected, as γ is increased, these clouds generally move leftwards and
upwards (lower error and higher unfairness).

We anticipate that the practical use of our algorithm would, as we have done, explore many
values of γ and then pick a model corresponding to a point on the aggregated Pareto frontier
across all γ, which represents the collection of all undominated models and the overall error-
unfairness trade-oﬀ. This aggregate frontier is shown in the right panel of Figure 2, and shows
a relatively smooth menu of options, ranging from error about 0.21 and no unfairness at one
extreme, to error about 0.12 and unfairness 0.025 at the other, and an appealing assortment of
intermediate trade-oﬀs. Of course, in a real application the selection of a particular point on
the frontier should be made in a domain-speciﬁc manner by the stakeholders or policymakers in
question.

Acknowledgements We thank Alekh Agarwal, Richard Berk, Miro Dud´ık, Akshay Krishna-
murthy, John Langford, Greg Ridgeway and Greg Yang for helpful discussions and suggestions.

References

Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, and John Langford. A reductions approach
to fair classiﬁcation. Fairness, Accountability, and Transparency in Machine Learning (FATML),
2017.

Julia Angwin and Hannes Grassegger. Facebooks secret censorship rules protect white men from

hate speech but not black children. Propublica, 2017.

Anna Maria Barry-Jester, Ben Casselman, and Dana Goldstein. The new science of sentencing.

The Marshall Project, August 8 2015. Retrieved 4/28/2016.

George W. Brown. Some notes on computation of games solutions, Jan 1949.

Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. arXiv preprint arXiv:1703.00056, 2017.

Constantinos Daskalakis and Qinxuan Pan. A counter-example to Karlin’s strong conjecture for
ﬁctitious play. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium
on, pages 11–20. IEEE, 2014.

Ilias Diakonikolas, Ryan O’Donnell, Rocco A. Servedio, and Yi Wu. Hardness results for agnosti-
cally learning low-degree polynomial threshold functions. In Proceedings of the Twenty-Second
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California,
USA, January 23-25, 2011, pages 1590–1606, 2011.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Confer-
ence, pages 214–226. ACM, 2012.

Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning of

monomials by halfspaces is hard. SIAM J. Comput., 41(6):1558–1590, 2012.

26

Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings
of the Ninth Annual Conference on Computational Learning Theory, COLT 1996, Desenzano del
Garda, Italy, June 28-July 1, 1996., pages 325–332, 1996.

Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im) possibility

of fairness. arXiv preprint arXiv:1609.07236, 2016.

Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination
prevention in data mining. IEEE transactions on knowledge and data engineering, 25(7):1445–
1459, 2013.

Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning.

Advances in Neural Information Processing Systems, 2016.

´Ursula H´ebert-Johnson, Michael P Kim, Omer Reingold, and Guy N Rothblum. Calibration for

the (computationally-identiﬁable) masses. arXiv preprint arXiv:1711.08513, 2017.

Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
In Advances in Neural Information Processing Systems, pages

Classic and contextual bandits.
325–333, 2016.

Adam Tauman Kalai and Santosh Vempala. Eﬃcient algorithms for online decision problems. J.

Comput. Syst. Sci., 71(3):291–307, 2005.

Adam Tauman Kalai, Yishay Mansour, and Elad Verbin. On agnostic boosting and parity learn-
ing. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British
Columbia, Canada, May 17-20, 2008, pages 629–638, 2008.

Faisal Kamiran and Toon Calders. Data preprocessing techniques for classiﬁcation without dis-

crimination. Knowledge and Information Systems, 33(1):1–33, 2012.

Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. An Empirical Study of Rich
Subgroup Fairness for Machine Learning. ArXiv e-prints, art. arXiv:1808.08166, August 2018.

Michael J Kearns and Umesh Virkumar Vazirani. An Introduction to Computational Learning The-

ory. MIT press, 1994.

Michael J Kearns, Robert E Schapire, and Linda M Sellie. Toward eﬃcient agnostic learning.

Machine Learning, 17(2-3):115–141, 1994.

Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-oﬀs in the fair deter-
mination of risk scores. In Proceedings of the 2017 ACM Conference on Innovations in Theoretical
Computer Science, Berkeley, CA, USA, 2017, 2017.

James Rufus Koren. What does that web search say about your credit? Los Angeles Times, July 16

2016. Retrieved 9/15/2016.

1951.

Julia Robinson. An iterative method of solving a game. Annals of Mathematics, pages 10–2307,

27

Cynthia Rudin. Predictive policing using machine learning to detect patterns of crime. Wired

Magazine, August 2013. Retrieved 4/28/2016.

Maurice Sion. On general minimax theorems. Paciﬁc J. Math., 8(1):171–176, 1958.

Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-

discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.

Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate ex-
ample weighting. In Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM
2003), 19-22 December 2003, Melbourne, Florida, USA, page 435, 2003.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate
mistreatment.
In Proceedings of the 26th International Conference on World Wide Web, pages
1171–1180. International World Wide Web Conferences Steering Committee, 2017.

Zhe Zhang and Daniel B Neill. Identifying signiﬁcant predictive bias in classiﬁers. arXiv preprint

arXiv:1611.08292, 2016.

A Chernoﬀ-Hoeﬀding Bound

We use the following concentration inequality.

Theorem A.1 (Real-vaued Additive Chernoﬀ-Hoeﬀding Bound). Let X1, X2, . . . , Xm be i.i.d. random
variables with E [Xi] = µ and a ≤ Xi

≤ b for all i. Then for every α > 0,
(cid:32) −2α2m
(cid:33)
(b − a)2

≤ 2 exp

i Xi
m

≥ α

− µ

(cid:80)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr

B Generalization Bounds

Proof of Theorems 2.11 and 2.12. We give a proof of Theorem 2.11. The proof of Theorem 2.12 is
identical, as false positive rates are just positive classiﬁcation rates on the subset of the data for
which y = 0.

Given a set of classiﬁers H and protected groups G, deﬁne the following function class:

FH,G = {fh,g (x) (cid:17) h(x) ∧ g(x) : h ∈ H, g ∈ G}

We can relate the VC-dimension of FH,G to the VC-dimension of H and G:
Claim B.1.

VCDIM(FH,G) ≤ ˜O(VCDIM(H) + VCDIM(G))
Proof. Let S be a set of size m shattered by FH,G. Let πFH,G (S) be the number of labelings of S
realized by elements of FH,G. By the deﬁnition of shattering, πFH,G (S) = 2m. Now for each labeling
of S by an element in FH,G, it is realized as (f ∧ g)(S) for some f ∈ F , g ∈ G. But (f ∧ g)(S) =
f (S)∧g(S), and so it can be realized as the conjunction of a labeling of S by an element of F and an

28

element of G. But since there are πF (S)πG(S) such pairs of labelings, this immediately implies that
πFH,G (S) ≤ πF (S)πG(S). Now by the Sauer-Shelah Lemma (see e.g. Kearns and Vazirani [1994]),
πF (S) = O(mVCDIM(H)), πG(S) = O(mVCDIM(G)). Thus πFH,G (S) = 2m ≤ O(mVCDIM(H)+VCDIM(G)), which
implies that m = ˜O(VCDIM(H) + VCDIM(G)), as desired.

This bound, together with a standard VC-Dimension based uniform convergence theorem (see

e.g. Kearns and Vazirani [1994]) implies that with probability 1 − δ, for every fh,g

∈ FH,G:

(cid:12)(cid:12)(cid:12)E
(X,y)∼P [fh,g (X)] − E

(X,y)∼P

S [fh,g (X)]

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

Note that the left hand side of the above inequality can be written as:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr
(X,y)∼P

[h(X) = 1|g(x) = 1] · Pr

(X,y)∼P

[g(x) = 1] − Pr
(X,y)∼P

[h(X) = 1|g(x) = 1] · Pr
(X,y)∼P

S

S

[g(x) = 1]





(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

This completes our proof.

C Missing Proofs in Section 4

Theorem 4.5. Let ( ˆD, ˆλ) be a ν-approximate minmax solution to the Λ-bounded Lagrangian problem
in the sense that

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν and, L( ˆD, ˆλ) ≥ max
λ∈Λ

L( ˆD, λ) − ν.

Then err( ˆD, P ) ≤ OPT +2ν and for any g ∈ G(S),

Proof of Theorem 4.5. Let D
problem. Since D

∗ is feasible, we know that L(D

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C
∗ be the optimal feasible solution for our constrained optimization
, P ).

, ˆλ) ≤ err(D

.

∗

∗

We will ﬁrst focus on the case where ˆD is not a feasible solution, that is

max
(g,•)∈G(S)×{±}

Φ•( ˆD, g) > 0

Let ( ˆg, ˆ•) ∈ argmax(g,•) Φ•( ˆD, g) and let λ
zero. By Lemma 4.8, we know that λ
minmax solution, we know that L( ˆD, ˆλ) ≥ L( ˆD, λ

(cid:48) ∈ argmaxλ∈Λ

(cid:48) ∈ Λ be a vector with (λ

(cid:48))ˆ•
ˆg = C and all other coordinates
L( ˆD, λ). By the deﬁnition of a ν-approximate

(cid:48)) − ν. This implies that

Note that L(D

, ˆλ) ≤ err(D

, P ), and so

∗

∗

L( ˆD, ˆλ) ≥ err( ˆD, P ) + C Φˆ•( ˆD, ˆg) − ν

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν ≤ L(D

, ˆλ) + ν

∗

29

(15)

(16)

Combining Equations (15) and (16), we get

err( ˆD, P ) + C Φˆ•( ˆD, ˆg) ≤ L( ˆD, ˆλ) + ν ≤ L(D

∗

, ˆλ) + 2ν ≤ err(D

, P ) + 2ν

∗

Note that C Φˆ•( ˆD, ˆg) ≥ 0, so we must have err( ˆD, P ) ≤ err(D
, P ) ∈ [0, 1], we know
since err( ˆD, P ), err(D

∗

∗

, P ) + 2ν = OPT +2ν. Furthermore,

which implies that maximum constraint violation satisﬁes Φˆ•( ˆD, ˆg) ≤ (1 + 2ν)/C. By applying
Claim 4.4, we get

C Φˆ•( ˆD, ˆg) ≤ 1 + 2ν,

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C

.

Now let us consider the case in which ˆD is a feasible solution for the optimization problem.

Then it follows that there is no constraint violation by ˆD and maxλ
∗

err( ˆD, P ) = max

L( ˆD, λ) ≤ L( ˆD, ˆλ) + ν ≤ min
D

λ

L(D, ˆλ) + 2ν ≤ L(D

, ˆλ) + 2ν ≤ err(D

, P ) + 2ν

∗

L( ˆD, λ) = err( ˆD, P ), and so

Therefore, the stated bounds hold for both cases.

Lemma 4.8. Fix any D ∈ ∆H(S) such that that maxg∈G(S)
(cid:48))
with one non-zero coordinate (λ

•(cid:48)
g (cid:48) = C, where

{Φ+(D, g), Φ−(D, g)} > 0. Let λ

(cid:48) ∈ Λ be vector

(cid:48)

, •(cid:48)

(g

) = argmax

(g,•)∈G(S)×{±}

{Φ•(D, g)}

Then L(D, λ

(cid:48)) ≥ maxλ∈Λ L(D, λ).

Proof of Lemma 4.8. Observe:

L(D, λ) = argmax

[err(h, P )] +

(cid:88)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

argmax
λ∈Λ

λ∈Λ

= argmax
λ∈Λ

E
h∼D
(cid:88)

g∈G

g∈G(S)
(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:16)
λ+

Note that this is a linear optimization problem over the non-negative orthant of a scaling of the (cid:96)1
ball, and so has a solution at a vertex, which corresponds to a single group g ∈ G(S). Thus, there
(cid:48))•
is always a best response λ
g that maximizes
Φ•(D, g).

(cid:48) that puts all the weight C on the coordinate (λ

Lemma 4.10. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm with η = n
, and λ1, . . . , λT be the
sequence of plays by the Auditor. Then

1√
nT

(1+C)

(cid:113)

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

U (h, λt) ≤ 2n1/4(1 + C)

T

√

T(cid:88)

t=1

30

Proof of Lemma 4.10. To instantiate the regret bound in Theorem 2.9, we just need to provide a
bound on the maximum absoluate value over the coordinates of the loss vector (the quantity M in
Theorem 2.9). For any λ ∈ Λ, the absolute value of the i-th coordinate of LC(λ) is bounded by:

(λ+
g

−
g ) (Pr[g(x) = 1 | y = 0] − 1) 1[g(xi) = 1]
− λ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(Pr[g(x) = 1 | y = 0]1g(xi) = 1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
n

+

(cid:88)

1
n
g∈G(S)


(cid:88)

≤ 1
n

+

≤ 1
n

+

1
n

1
n




g∈G(S)


(cid:88)

g∈G(S)

(cid:12)(cid:12)(cid:12)λ+

g

−
− λ
g



(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)λ+

g

(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)λ

−
g

max
g∈G(S)


(cid:12)(cid:12)(cid:12)

≤ 1 + C
n

Also note that the dimension of the optimization is the size of the dataset n. This means if we set
η = n

, the regret of the learner will then be bounded by 2n1/4(1 + C)

T .

(cid:113)

√

(1+C)

1√
nT

Lemma 4.11. Fix any ξ, δ ∈ (0, 1) and any distribution D over H(S). Let h1, . . . , hm be m i.i.d. draws
from p, and ˆD be the empirical distribution over the realized sample. Then with probability at least 1 − δ
over the random draws of hj’s, the following holds,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as long as m ≥ c0

C2(ln(1/δ)+d2 ln(n))
ξ2

for some absolute constant c0 and d2 = VCDIM(G).

Proof of Lemma 4.11. Recall that for any distribution D
deﬁned as

(cid:48) over H(S) the expected payoﬀ function is

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)] = E
h∼ ˆD

[err(h, P )] + E
h∼ ˆD

− E
h∼D

[err(h, P )] + E
h∼D

(cid:88)

(cid:16)
λ+

g Φ+(h, g) + λ


(cid:17)
−
g Φ−(h, g)


(cid:17)
−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:16)
λ+








g∈G(S)

(cid:88)

g∈G(S)



h∼D [err(h, P )] −

By the triangle inequality, it suﬃces to show that with probability (1 − δ), A = | E
E
h∼ ˆD [err(h, P )] | ≤ ξ/2 and for all λ ∈ Λ and g ∈ G(S),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

B =

E
h∼ ˆD





(cid:88)

g∈G(S)


(cid:17)
−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:16)
λ+



− E
h∼D





(cid:88)

g∈G(S)

(cid:16)
λ+

−
g Φ−(h, g)
g Φ+(h, g) + λ

≤ ξ/2


(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


The ﬁrst part follows directly from a simple application of the Chernoﬀ-Hoeﬀding bound

(Theorem A.1): with probability (1 − δ/2), A ≤ ξ/2, as long as m ≥ 2 ln(4/δ)/ξ2.
To bound the second part, we ﬁrst note that by H¨older’s inequality, we have

B ≤ (cid:107)λ(cid:107)

1 max
(g,•)∈G(S)×{±}

|Φ•(D, g) − Φ•( ˆD, g)|

31

Since for all λ ∈ Λ we have (cid:107)λ(cid:107)

≤ C, it suﬃces to show that with probability 1−δ/2, |Φ•(D, g)−

1

Φ•( ˆD, g)| ≤ ξ/(2C) holds for all • ∈ {−, +} and g ∈ G(S). Note that

We can rewrite the absolute value of ﬁrst term:

|Φ•(D, g) − Φ•( ˆD, g)| =

[FP(h)]

Pr[y = 0, g(x) = 1]

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[FP(h)] − E
h∼ ˆD

(cid:33)

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

E
h∼D

[FP(h)] − E
h∼ ˆD

(cid:33)
[FP(h)]

Pr[y = 0, g(x) = 1]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1 | y = 0]] − E
h∼ ˆD

E
h∼D

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0]]

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:33)
[Pr[h(X) = 1 | y = 0]]

Pr[g(x) = 1 | y = 0]

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:32)

+

(cid:32)

(cid:32)

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=

≤

where the last inequality follows from Pr[g(x) = 1 | y = 0] ≤ 1.

Note that E

h∼ ˆD [Pr[h(X) = 1, y = 0, g(x) = 1]] = 1
m

average of m i.i.d. random variables with expectation E
Chernoﬀ-Hoeﬀding bound (Theorem A.1), we have

(cid:80)m

j=1 Pr[hj(X) = 1, y = 0, g(x) = 1], which is an
h∼D [Pr[h(X) = 1, y = 0, g(x) = 1]]. By the

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

Pr

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD
(cid:16)− ξ2m
8C2

[Pr[h(X) = 1, y = 0]]

>

≤ 2 exp

(17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

ξ
4C

(cid:33)

(cid:32)

− ξ2m
8C2

In the following, we will let δ0 = 2 exp

(cid:17)
. Similarly, we also have for each g ∈ G(S),

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

>

(18)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

ξ
4C

≤ δ0

By taking the union bound over (17) and (18) over all choices of g ∈ G(S), we have with proba-

bility at least (1 − δ0(1 + |G(S)|)),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0]]

(19)

and,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

for all g ∈ G(S).

(20)

(cid:17)
Note that by Sauer’s lemma (Lemma 4.3), |G(S)| ≤ O
. Thus, there exists an absolute constant
C2(ln(1/δ)+d2 ln(n))
c0 such that m ≥ c0
implies that failure probability above δ0(1 + |G(S)|) ≤ δ/2. We
ξ2
will assume m satisiﬁes such a bound, and so the events of (19) and (20) hold with probaility at
least (1−δ/2). Then by the triangle inequality we have for all (g, •) ∈ G(S)×{±}, |Φ•(D, g)−Φ•( ˆD, g)| ≤
ξ/(2C), which implies that B ≤ ξ/2. This completes the proof.

(cid:16)
nd2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ ξ
4C

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ ξ
4C

32

Claim C.1. Suppose there are two distributions D and ˆD over H(S) such that

Let

Then

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ˆλ ∈ argmax

λ(cid:48)∈Λ

E
h∼ ˆD

(cid:2)U (h, λ

(cid:48)

)(cid:3)

max
λ

E
h∼D

[U (h, λ)] − ξ ≤ E
h∼D

(cid:105)
(cid:104)
U (h, ˆλ)

,

Lemma 4.12. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm. For each Dt, let ˆDt be the empirical distri-
bution over m i.i.d. draws from Dt. Let λ1, . . . , λT be the Auditor’s best responses against ˆD1, . . . , ˆDT .
Then with probability 1 − δ,

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:114)

(cid:104)
U (h, λt)

(cid:105) ≤ T

c0C2(ln(T /δ) + d2 ln(n))
m

for some absolute constant c0 and d2 = VCDIM(G).

Proof. Let γ t

A be deﬁned as

By instantiating Lemma 4.11 and applying union bound across all T steps, we know with proba-
bility at least 1 − δ, the following holds for all t ∈ [T ]:

γ t
A = max
λ∈Λ

E
h∼ ˆDt

[U (h, λ)] − E
h∼Dt

[U (h, λ)]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:114)

≤

γ t
A

c0C2(ln(T /δ) + d2 ln(n))
m

where c0 is the absolute constant in Lemma 4.11 and d2 = VCDIM(G).

Note that by Claim C.1, the Auditor is performing a γ t
round t. Then we can bound the Auditor’s regret as follows:

A-approximate best response at each

γA =





1
T

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

max
λ∈Λ

E
h∼Dt

[U (h, λ)] − E
h∼Dt

(cid:104)
U (h, λt)

(cid:105)(cid:19)


(cid:105)



(cid:18)

T(cid:88)

≤ 1
T
t=1
≤ max
γ t
A
T

It follows that with probability 1 − δ, we have

which completes the proof.

(cid:114)

≤

γA

c0C2(ln(T /δ) + d2 ln(n))
m

33

8
1
0
2
 
c
e
D
 
3
 
 
]

G
L
.
s
c
[
 
 
5
v
4
4
1
5
0
.
1
1
7
1
:
v
i
X
r
a

Preventing Fairness Gerrymandering:
Auditing and Learning for Subgroup Fairness

Michael Kearns1, Seth Neel1, Aaron Roth1 and Zhiwei Steven Wu2

1University of Pennsylvania
2Microsoft Research-New York City

December 4, 2018

Abstract

The most prevalent notions of fairness in machine learning are statistical deﬁnitions: they
ﬁx a small collection of high-level, pre-deﬁned groups (such as race or gender), and then ask
for approximate parity of some statistic of the classiﬁer (like positive classiﬁcation rate or false
positive rate) across these groups. Constraints of this form are susceptible to (intentional or
inadvertent) fairness gerrymandering, in which a classiﬁer appears to be fair on each individual
group, but badly violates the fairness constraint on one or more structured subgroups deﬁned
over the protected attributes (such as certain combinations of protected attribute values). We
propose instead to demand statistical notions of fairness across exponentially (or inﬁnitely)
many subgroups, deﬁned by a structured class of functions over the protected attributes. This
interpolates between statistical deﬁnitions of fairness, and recently proposed individual no-
tions of fairness, but it raises several computational challenges. It is no longer clear how to
even check or audit a ﬁxed classiﬁer to see if it satisﬁes such a strong deﬁnition of fairness.
We prove that the computational problem of auditing subgroup fairness for both equality of
false positive rates and statistical parity is equivalent to the problem of weak agnostic learn-
ing — which means it is computationally hard in the worst case, even for simple structured
subclasses. However, it also suggests that common heuristics for learning can be applied to
successfully solve the auditing problem in practice.

We then derive two algorithms that provably converge to the best fair distribution over
classiﬁers in a given class, given access to oracles which can optimally solve the agnostic learn-
ing problem. The algorithms are based on a formulation of subgroup fairness as a two-player
zero-sum game between a Learner (the primal player) and an Auditor (the dual player). Both
algorithms compute an equilibrium of this game. We obtain our ﬁrst algorithm by simulat-
ing play of the game by having Learner play an instance of the no-regret Follow the Perturbed
Leader algorithm, and having Auditor play best response. This algorithm provably converges
to an approximate Nash equilibrium (and thus to an approximately optimal subgroup-fair dis-
tribution over classiﬁers) in a polynomial number of steps. We obtain our second algorithm
by simulating play of the game by having both players play Fictitious Play, which enjoys only
provably asymptotic convergence, but has the merit of simplicity and faster per-step compu-
tation. We implement the Fictitious Play version using linear regression as a heuristic oracle,
and show that we can eﬀectively both audit and learn fair classiﬁers on real datasets.

1 Introduction

As machine learning is being deployed in increasingly consequential domains (including policing
[Rudin, 2013], criminal sentencing [Barry-Jester et al., 2015], and lending [Koren, 2016]), the
problem of ensuring that learned models are fair has become urgent.

Approaches to fairness in machine learning can coarsely be divided into two kinds: statistical
and individual notions of fairness. Statistical notions typically ﬁx a small number of protected
demographic groups G (such as racial groups), and then ask for (approximate) parity of some
statistical measure across all of these groups. One popular statistical measure asks for equal-
ity of false positive or negative rates across all groups in G (this is also sometimes referred to as
an equal opportunity constraint [Hardt et al., 2016]). Another asks for equality of classiﬁcation
rates (also known as statistical parity). These statistical notions of fairness are the kinds of fair-
ness deﬁnitions most common in the literature (see e.g. Kamiran and Calders [2012], Hajian and
Domingo-Ferrer [2013], Kleinberg et al. [2017], Hardt et al. [2016], Friedler et al. [2016], Zafar
et al. [2017], Chouldechova [2017]).

One main attraction of statistical deﬁnitions of fairness is that they can in principle be ob-
tained and checked without making any assumptions about the underlying population, and hence
lead to more immediately actionable algorithmic approaches. On the other hand, individual no-
tions of fairness ask for the algorithm to satisfy some guarantee which binds at the individual,
rather than group, level. This often has the semantics that “individuals who are similar” should
be treated “similarly” [Dwork et al., 2012], or “less qualiﬁed individuals should not be favored
over more qualiﬁed individuals” [Joseph et al., 2016]. Individual notions of fairness have attrac-
tively strong semantics, but their main drawback is that achieving them seemingly requires more
assumptions to be made about the setting under consideration.

The semantics of statistical notions of fairness would be signiﬁcantly stronger if they were
deﬁned over a large number of subgroups, thus permitting a rich middle ground between fairness
only for a small number of coarse pre-deﬁned groups, and the strong assumptions needed for
fairness at the individual level. Consider the kind of fairness gerrymandering that can occur when
we only look for unfairness over a small number of pre-deﬁned groups:

Example 1.1. Imagine a setting with two binary features, corresponding to race (say black and white)
and gender (say male and female), both of which are distributed independently and uniformly at random
in a population. Consider a classiﬁer that labels an example positive if and only if it corresponds to a
black man, or a white woman. Then the classiﬁer will appear to be equitable when one considers either
protected attribute alone, in the sense that it labels both men and women as positive 50% of the time, and
labels both black and white individuals as positive 50% of the time. But if one looks at any conjunction
of the two attributes (such as black women), then it is apparent that the classiﬁer maximally violates the
statistical parity fairness constraint. Similarly, if examples have a binary label that is also distributed
uniformly at random, and independently from the features, the classiﬁer will satisfy equal opportunity
fairness with respect to either protected attribute alone, even though it maximally violates it with respect
to conjunctions of two attributes.

We remark that the issue raised by this toy example is not merely hypothetical. In our exper-
iments in Section 5, we show that similar violations of fairness on subgroups of the pre-deﬁned
groups can result from the application of standard machine learning methods applied to real
datasets. To avoid such problems, we would like to be able to satisfy a fairness constraint not
just for the small number of protected groups deﬁned by single protected attributes, but for a

1

combinatorially large or even inﬁnite collection of structured subgroups deﬁnable over protected
attributes.

In this paper, we consider the problem of auditing binary classiﬁers for equal opportunity and
statistical parity, and the problem of learning classiﬁers subject to these constraints, when the
number of protected groups is large. There are exponentially many ways of carving up a popu-
lation into subgroups, and we cannot necessarily identify a small number of these a priori as the
only ones we need to be concerned about. At the same time, we cannot insist on any notion of sta-
tistical fairness for every subgroup of the population: for example, any imperfect classiﬁer could
be accused of being unfair to the subgroup of individuals deﬁned ex-post as the set of individuals
it misclassiﬁed. This simply corresponds to “overﬁtting” a fairness constraint. We note that the
individual fairness deﬁnition of Joseph et al. [2016] (when restricted to the binary classiﬁcation
setting) can be viewed as asking for equalized false positive rates across the singleton subgroups,
containing just one individual each1 — but naturally, in order to achieve this strong deﬁnition of
fairness, Joseph et al. [2016] have to make structural assumptions about the form of the ground
truth. It is, however, sensible to ask for fairness for large structured subsets of individuals: so
long as these subsets have a bounded VC dimension, the statistical problem of learning and au-
diting fair classiﬁers is easy, so long as the dataset is suﬃciently large. This can be viewed as an
interpolation between equal opportunity fairness and the individual “weakly meritocratic” fair-
ness deﬁnition from Joseph et al. [2016], that does not require making any assumptions about the
ground truth. Our investigation focuses on the computational challenges, both in theory and in
practice.

1.1 Our Results

Brieﬂy, our contributions are:

• Formalization of the problem of auditing and learning classiﬁers for fairness with respect to

rich classes of subgroups G.

• Results proving (under certain assumptions) the computational equivalence of auditing G
and (weak) agnostic learning of G. While these results imply theoretical intractability of au-
diting for some natural classes G, they also suggest that practical machine learning heuristics
can be applied to the auditing problem.

• Provably convergent algorithms for learning classiﬁers that are fair with respect to G, based
on a formulation as a two-player zero-sum game between a Learner (the primal player) and
an Auditor (the dual player). We provide two diﬀerent algorithms, both of which are based
on solving for the equilibrium of this game. The ﬁrst provably converges in a polynomial
number of steps and is based on simulation of the game dynamics when the Learner uses
Follow the Perturbed Leader and the Auditor uses best response; the second is only guaranteed
to converge asymptotically but is computationally simpler, and involves both players using
Fictitious Play.

• An implementation and empirical evaluation of the Fictitious Play algorithm demonstrating

its eﬀectiveness on a real dataset in which subgroup fairness is a concern.

1It also asks for equalized false negative rates, and that the false positive rate is smaller than the true positive rate.

Here, the randomness in the “rates” is taken entirely over the randomness of the classiﬁer.

2

In more detail, we start by studying the computational challenge of simply checking whether
a given classiﬁer satisﬁes equal opportunity and statistical parity. Doing this in time linear in the
number of protected groups is simple: for each protected group, we need only estimate a single
expectation. However, when there are many diﬀerent protected attributes which can be combined
to deﬁne the protected groups, their number is combinatorially large2.

We model the problem by specifying a class of functions G deﬁned over a set of d protected
attributes. G deﬁnes a set of protected subgroups. Each function g ∈ G corresponds to the pro-
tected subgroup {x : gi(x) = 1}3. The ﬁrst result of this paper is that for both equal opportunity and
statistical parity, the computational problem of checking whether a classiﬁer or decision-making
algorithm D violates statistical fairness with respect to the set of protected groups G is equivalent
to the problem of agnostically learning G [Kearns et al., 1994], in a strong and distribution-speciﬁc
sense. This equivalence has two implications:

1. First, it allows us to import computational hardness results from the learning theory litera-
ture. Agnostic learning turns out to be computationally hard in the worst case, even for
extremely simple classes of functions G (like boolean conjunctions and linear threshold
functions). As a result, we can conclude that auditing a classiﬁer D for statistical fairness
violations with respect to a class G is also computationally hard. This means we should not
expect to ﬁnd a polynomial time algorithm that is always guaranteed to solve the auditing
problem.

2. However, in practice, various learning heuristics (like boosting, logistic regression, SVMs,
backpropagation for neural networks, etc.) are commonly used to learn accurate classiﬁers
which are known to be hard to learn in the worst case. The equivalence we show between
agnostic learning and auditing is distribution speciﬁc — that is, if on a particular data set, a
heuristic learning algorithm can solve the agnostic learning problem (on an appropriately
deﬁned subset of the data), it can be used also to solve the auditing problem on the same
data set.

These results appear in Section 3.

Next, we consider the problem of learning a classiﬁer that equalizes false positive or negative
rates across all (possibly inﬁnitely many) sub-groups, deﬁned by a class of functions G. As per the
reductions described above, this problem is computationally hard in the worst case.

However, under the assumption that we have an eﬃcient oracles which solves the agnostic
learning problem, we give and analyze algorithms for this problem based on a game-theoretic
formulation. We ﬁrst prove that the optimal fair classiﬁer can be found as the equilibrium of
a two-player, zero-sum game, in which the (pure) strategy space of the “Learner” player corre-
sponds to classiﬁers in H, and the (pure) strategy space of the “Auditor” player corresponds to

2For example, as discussed in a recent Propublica investigation [Angwin and Grassegger, 2017], Facebook policy
protects groups against hate speech if the group is deﬁnable as a conjunction of protected attributes. Under the Face-
book schema, “race” and “gender” are both protected attributes, and so the Facebook policy protects “black women” as
a distinct class, separately from black people and women. When there are d protected attributes, there are 2d protected
groups. As a statistical estimation problem, this is not a large obstacle — we can estimate 2d expectations to error ε so
long as our data set has size O(d/ε2), but there is now a computational problem.

3For example, in the case of Facebook’s policy, the protected attributes include “race, sex, gender identity, religious
aﬃliation, national origin, ethnicity, sexual orientation and serious disability/disease” [Angwin and Grassegger, 2017],
and G represents the class of boolean conjunctions. In other words, a group deﬁned by individuals having any subset
of values for the protected attributes is protected.

3

subgroups deﬁned by G. The best response problems for the two players correspond to agnostic
learning and auditing, respectively. We show that both problems can be solved with a single call
to a cost sensitive classiﬁcation oracle, which is equivalent to an agnostic learning oracle. We then
draw on extant theory for learning in games and no-regret algorithms to derive two diﬀerent al-
gorithms based on simulating game play in this formulation. In the ﬁrst, the Learner employs the
well-studied Follow the Perturbed Leader (FTPL) algorithm on an appropriate linearization of its
best-response problem, while the Auditor approximately best-responds to the distribution over
classiﬁers of the Learner at each step. Since FTPL has a no-regret guarantee, we obtain an algo-
rithm that provably converges in a polynomial number of steps.

While it enjoys strong provable guarantees, this ﬁrst algorithm is randomized (due to the
noise added by FTPL), and the best-response step for the Auditor is polynomial time but compu-
tationally expensive. We thus propose a second algorithm that is deterministic, simpler and faster
per step, based on both players adopting the Fictitious Play learning dynamic. This algorithm has
weaker theoretical guarantees: it has provable convergence only asymptotically, and not in a poly-
nomial number of steps — but is more practical and converges rapidly in practice. The derivation
of these algorithms (and their guarantees) appear in Section 4.

Finally, we implement the Fictitious Play algorithm and demonstrate its practicality by eﬃ-
ciently learning classiﬁers that approximately equalize false positive rates across any group de-
ﬁnable by a linear threshold function on 18 protected attributes in the “Communities and Crime”
dataset. We use simple, fast regression algorithms as heuristics to implement agnostic learn-
ing oracles, and (via our reduction from agnostic learning to auditing) auditing oracles. Our
results suggest that it is possible in practice to learn fair classiﬁers with respect to a large class
of subgroups that still achieve non-trivial error. Full details are contained in Section 5, and for a
substantially more comprehensive empirical investigation of our method we direct the interested
reader to Kearns et al. [2018].

1.2 Further Related Work

Independent of our work, H´ebert-Johnson et al. [2017] also consider a related and complementary
notion of fairness that they call “multicalibration”. In settings in which one wishes to train a real-
valued predictor, multicalibration can be considered the “calibration” analogue for the deﬁnitions
of subgroup fairness that we give for false positive rates, false negative rates, and classiﬁcation
rates. For a real-valued predictor, calibration informally requires that for every value v ∈ [0, 1]
predicted by an algorithm, the fraction of individuals who truly have a positive label in the subset
of individuals on which the algorithm predicted v should be approximately equal to v. Multi-
calibration asks for approximate calibration on every set deﬁned implicitly by some circuit in a
set G. H´ebert-Johnson et al. [2017] give an algorithmic result that is analogous to the one we give
for learning subgroup fair classiﬁers: a polynomial time algorithm for learning a multi-calibrated
predictor, given an agnostic learning algorithm for G. In addition to giving a polynomial-time
algorithm, we also give a practical variant of our algorithm (which is however only guaranteed to
converge in the limit) that we use to conduct empirical experiments on real data.

Thematically, the most closely related piece of prior work is Zhang and Neill [2016], who
also aim to audit classiﬁcation algorithms for discrimination in subgroups that have not been
pre-deﬁned. Our work diﬀers from theirs in a number of important ways. First, we audit the
algorithm for common measures of statistical unfairness, whereas Zhang and Neill [2016] design
a new measure compatible with their particular algorithmic technique. Second, we give a for-

4

mal analysis of our algorithm. Finally, we audit with respect to subgroups deﬁned by a class of
functions G, which we can take to have bounded VC dimension, which allows us to give formal
out-of-sample guarantees. Zhang and Neill [2016] attempt to audit with respect to all possible
sub-groups, which introduces a severe multiple-hypothesis testing problem, and risks overﬁtting.
Most importantly we give actionable algorithms for learning subgroup fair classiﬁers, whereas
Zhang and Neill [2016] restrict attention to auditing.

Technically, the most closely related piece of work (and from which we take inspiration for
our algorithm in Section 4) is Agarwal et al. [2017], who show that given access to an agnostic
learning oracle for a class H, there is an eﬃcient algorithm to ﬁnd the lowest-error distribution
over classiﬁers in H subject to equalizing false positive rates across polynomially many subgroups.
Their algorithm can be viewed as solving the same zero-sum game that we solve, but in which the
“subgroup” player plays gradient descent over his pure strategies, one for each sub-group. This
ceases to be an eﬃcient or practical algorithm when the number of subgroups is large, as is our
case. Our main insight is that an agnostic learning oracle is suﬃcient to have the both players
play “ﬁctitious play”, and that there is a transformation of the best response problem such that an
agnostic learning algorithm is enough to eﬃciently implement follow the perturbed leader.

There is also other work showing computational hardness for fair learning problems. Most no-
tably, Woodworth et al. [2017] show that ﬁnding a linear threshold classiﬁer that approximately
minimizes hinge loss subject to equalizing false positive rates across populations is computation-
ally hard (assuming that refuting a random k-XOR formula is hard). In contrast, we show that
even checking whether a classiﬁer satisﬁes a false positive rate constraint on a particular data set
is computationally hard (if the number of subgroups on which fairness is desired is too large to
enumerate).

2 Model and Preliminaries

(cid:48)), y), where x ∈ X denotes a vector of
We model each individual as being described by a tuple ((x, x
(cid:48) ∈ X (cid:48) denotes a vector of unprotected attributes, and y ∈ {0, 1} denotes a label.
protected attributes, x
Note that in our formulation, an auditing algorithm not only may not see the unprotected at-
(cid:48) may represent proprietary
tributes x
features or consumer data purchased by a credit scoring company.

(cid:48), it may not even be aware of their existence. For example, x

We will write X = (x, x

(cid:48)) to denote the joint feature vector. We assume that points (X, y) are
drawn i.i.d. from an unknown distribution P . Let D be a decision making algorithm, and let D(X)
denote the (possibly randomized) decision induced by D on individual (X, y). We restrict attention
in this paper to the case in which D makes a binary classiﬁcation decision: D(X) ∈ {0, 1}. Thus we
alternately refer to D as a classiﬁer. When auditing a ﬁxed classiﬁer D, it will be helpful to make
reference to the distribution over examples (X, y) together with their induced classiﬁcation D(X).
Let Paudit(D) denote the induced target joint distribution over the tuple (x, y, D(X)) that results from
(cid:48)) but
sampling (x, x
(cid:48). Note that the randomness here is over both the randomness of
not the unprotected attributes x
P , and the potential randomness of the classiﬁer D.

, y) ∼ P , and providing x, the true label y, and the classiﬁcation D(X) = D(x, x

(cid:48)

We will be concerned with learning and auditing classiﬁers D satisfying two common statis-
tical fairness constraints: equality of classiﬁcation rates (also known as statistical parity), and
equality of false positive rates (also known as equal opportunity). Auditing for equality of false
negative rates is symmetric and so we do not explicitly consider it. Each fairness constraint is

5

deﬁned with respect to a set of protected groups. We deﬁne sets of protected groups via a family
of indicator functions G for those groups, deﬁned over protected attributes. Each g : X → {0, 1} ∈ G
has the semantics that g(x) = 1 indicates that an individual with protected features x is in group
g.

Deﬁnition 2.1 (Statistical Parity (SP) Subgroup Fairness). Fix any classiﬁer D, distribution P , col-
lection of group indicators G, and parameter γ ∈ [0, 1]. For each g ∈ G, deﬁne

αSP (g, P ) = Pr
P

[g(x) = 1]

and, βSP (g, D, P ) = |SP(D) − SP(D, g)| ,

where SP(D) = PrP ,D[D(X) = 1] and SP(D, g) = PrP ,D[D(X) = 1|g(x) = 1] denote the overall acceptance
rate of D and the acceptance rate of D on group g respectively. We say that D satisﬁes γ-statistical
parity (SP) Fairness with respect to P and G if for every g ∈ G

We will sometimes refer to SP(D) as the SP base rate.

αSP (g, P ) βSP (g, D, P ) ≤ γ.

Remark 2.2. Note that our deﬁnition references two approximation parameters, both of which are im-
portant. We are allowed to ignore a group g if it (or its complement) represent only a small fraction
of the total probability mass. The parameter α governs how small a fraction of the population we are
allowed to ignore. Similarly, we do not require that the probability of a positive classiﬁcation in every
subgroup is exactly equal to the base rate, but instead allow deviations up to β. Both of these approxi-
mation parameters are necessary from a statistical estimation perspective. We control both of them with
a single parameter γ.

Deﬁnition 2.3 (False Positive (FP) Subgroup Fairness). Fix any classiﬁer D, distribution P , collection
of group indicators G, and parameter γ ∈ [0, 1]. For each g ∈ G, deﬁne

αFP (g, P ) = Pr
P

[g(x) = 1, y = 0]

and, βFP (g, D, P ) = |FP(D) − FP(D, g)|

where FP(D) = PrD,P [D(X) = 1 | y = 0] and FP(D, g) = PrD,P [D(X) = 1 | g(x) = 1, y = 0] denote the
overall false-positive rate of D and the false-positive rate of D on group g respectively.

We say D satisﬁes γ-False Positive (FP) Fairness with respect to P and G if for every g ∈ G

We will sometimes refer to FP(D) FP-base rate.

αFP (g, P ) βFP (g, D, P ) ≤ γ.

Remark 2.4. This deﬁnition is symmetric to the deﬁnition of statistical parity fairness, except that the
parameter α is now used to exclude any group g such that negative examples (y = 0) from g (or its
complement) have probability mass less than α. This is again necessary from a statistical estimation
perspective.

For either statistical parity and false positive fairness, if the algorithm D fails to satisfy the
γ-fairness condition, then we say that D is γ-unfair with respect to P and G. We call any subgroup
g which witnesses this unfairness an γ-unfair certiﬁcate for (D, P ).

An auditing algorithm for a notion of fairness is given sample access to Paudit(D) for some clas-
siﬁer D. It will either deem D to be fair with respect to P , or will else produce a certiﬁcate of
unfairness.

6

Deﬁnition 2.5 (Auditing Algorithm). Fix a notion of fairness (either statistical parity or false positive
(cid:48) ∈ (0, 1) such that
fairness), a collection of group indicators G over the protected features, and any δ, γ, γ
(cid:48))-auditing algorithm for G with respect to distribution P is an algorithm A such that
(cid:48) ≤ γ. A (γ, γ
γ
for any classiﬁer D, when given access the distribution Paudit(D), A runs in time poly(1/γ
, log(1/δ)),
and with probability 1 − δ, outputs a γ
-unfair certiﬁcate for D whenever D is γ-unfair with respect to
(cid:48)
P and G. If D is γ

-fair, A will output “fair”.

(cid:48)

(cid:48)

As we will show, our deﬁnition of auditing is closely related to weak agnostic learning.

Deﬁnition 2.6 (Weak Agnostic Learning [Kearns et al., 1994, Kalai et al., 2008]). Let Q be a dis-
(cid:48) ∈ (0, 1/2) such that ε ≥ ε
tribution over X × {0, 1} and let ε, ε
. We say that the function class G is
(cid:48))-weakly agnostically learnable under distribution Q if there exists an algorithm L such that
(ε, ε
, 1/δ), and with probability 1 − δ, outputs a
when given sample access to Q, L runs in time poly(1/ε
hypothesis h ∈ G such that

(cid:48)

(cid:48)

err(f , Q) ≤ 1/2 − ε =⇒ err(h, Q) ≤ 1/2 − ε

(cid:48)

.

min
f ∈G

where err(h, Q) = Pr(x,y)∼Q[h(x) (cid:44) y].

Cost-Sensitive Classiﬁcation.
In this paper, we will also give reductions to cost-sensitive classi-
ﬁcation (CSC) problems. Formally, an instance of a CSC problem for the class H is given by a set
of n tuples {(Xi, c0
i corresponds to the cost for predicting label (cid:96) on point Xi.
Given such an instance as input, a CSC oracle ﬁnds a hypothesis ˆh ∈ H that minimizes the total
cost across all points:

i=1 such that c(cid:96)

i , c1

i )}n

ˆh ∈ argmin

h∈H

n(cid:88)

i=1

[h(Xi)c1

i + (1 − h(Xi))c0
i ]

(1)

A crucial property of a CSC problem is that the solution is invariant to translations of the costs.

Claim 2.7. Let {(Xi, c0
a1, a2, . . . , an

i )}n
∈ R such that ˜c(cid:96)

i , c1

i=1 be a CSC instance, and {( ˜c0
i = c(cid:96)
i + ai for all i and (cid:96). Then

i , ˜c1

i )} be a set of new costs such that there exist

argmin
h∈H

n(cid:88)

i=1

[h(Xi)c1

i + (1 − h(Xi))c0

i ] = argmin

[h(Xi) ˜c1

i + (1 − h(Xi)) ˜c0
i ]

n(cid:88)

i=1

h∈H

Remark 2.8. We note that cost-sensitive classiﬁcation is polynomially equivalent to agnostic learn-
ing Zadrozny et al. [2003]. We give both deﬁnitions above because when describing our results for
auditing, we wish to directly appeal to known hardness results for weak agnostic learning, but it is more
convenient to describe our algorithms via oracles for cost-sensitive classiﬁcation.

Follow the Perturbed Leader. We will make use of the Follow the Perturbed Leader (FTPL) algo-
rithm as a no-regret learner for online linear optimization problems [Kalai and Vempala, 2005].
To formalize the algorithm, consider S ⊂ {0, 1}d to be a set of “actions” for a learner in an on-
line decision problem. The learner interacts with an adversary over T rounds, and in each round
t, the learner (randomly) chooses some action at ∈ S, and the adversary chooses a loss vector
(cid:96)t ∈ [−M, M]d. The learner incurs a loss of (cid:104)(cid:96)t, at(cid:105) at round t.

7

FTPL is a simple algorithm that in each round perturbs the cumulative loss vector over the pre-
s<t (cid:96)s, and chooses the action that minimizes loss with respect to the perturbed
vious rounds (cid:96) =
cumulative loss vector. We present the full algorithm in Algorithm 1, and its formal guarantee in
Theorem 2.9.

(cid:80)

U be the uniform distribution over [0, 1]d, and let a1 ∈ S be

Algorithm 1 Follow the Perturbed Leader (FTPL) Algorithm

Input: Loss bound M, action set S ∈ {0, 1}d
Initialize: Let η = (1/M)

, D

(cid:113)

1√

dT

arbitrary.
For t = 1, . . . , T :

Play action at; Observe loss vector (cid:96)t and suﬀer loss (cid:104)(cid:96)t, at(cid:105).
Update:

at+1 = argmin

η

(cid:104)(cid:96)s, a(cid:105) + (cid:104)ξt, a(cid:105)





(cid:88)

s≤t

a∈S





where ξt is drawn independently for each t from the distribution D

U .

Theorem 2.9 (Kalai and Vempala [2005]). For any sequence of loss vectors (cid:96)1, . . . , (cid:96)T , the FTPL algo-
rithm has regret





E

T(cid:88)



(cid:104)(cid:96)t, at(cid:105)

− min
a∈S

T(cid:88)

(cid:104)(cid:96)t, a(cid:105) ≤ 2d5/4M

T

√

t=1
where the randomness is taken over the perturbations ξt across rounds.

t=1

2.1 Generalization Error

In this section, we observe that the error rate of a classiﬁer D, as well as the degree to which it vio-
lates γ-fairness (for both statistical parity and false positive rates) can be accurately approximated
with the empirical estimates for these quantities on a dataset (drawn i.i.d. from the underlying
distribution P ) so long as the dataset is suﬃciently large. Once we establish this fact, since our
main interest is in the computational problem of auditing and learning, in the rest of the paper,
we assume that we have direct access to the underlying distribution (or equivalently, that the
empirical data deﬁnes the distribution of interest), and do not make further reference to sample
complexity or overﬁtting issues.

A standard VC dimension bound (see, e.g. Kearns and Vazirani [1994]) states:

Theorem 2.10. Fix a class of functions H. For any distribution P , let S ∼ P m be a dataset consisting
of m examples (Xi, yi) sampled i.i.d. from P . Then for any 0 < δ < 1, with probability 1 − δ, for every
h ∈ H, we have:

|err(h, P ) − err(h, S)| ≤ O

(cid:114)





VCDIM(H) log m + log(1/δ)
m





where err(h, S) = 1
m

(cid:80)m

i=1

1[h(Xi) (cid:44) yi].

8

The above theorem implies that so long as m ≥ ˜O(VCDIM(H)/ε2), then minimizing error over
the empirical sample S suﬃces to minimize error up to an additive ε term on the true distribution
P . Below, we give two analogous statements for fairness constraints:

Theorem 2.11 (SP Uniform Convergence). Fix a class of functions H and a class of group indicators
G. For any distribution P , let S ∼ P m be a dataset consisting of m examples (Xi, yi) sampled i.i.d. from
P . Then for any 0 < δ < 1, with probability 1 − δ, for every h ∈ H and g ∈ G

(cid:12)(cid:12)(cid:12)αSP (g, P

S) βSP (g, h, P

S) − αSP (g, P ) βSP (g, h, P )

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

where P

S denotes the empirical distribution over the realized sample S.

Similarly:

Theorem 2.12 (FP Uniform Convergence). Fix a class of functions H and a class of group indicators
G. For any distribution P , let S ∼ P m be a dataset consisting of m examples (Xi, yi) sampled i.i.d. from
P . Then for any 0 < δ < 1, with probability 1 − δ, for every h ∈ H and g ∈ G, we have:

(cid:12)(cid:12)(cid:12)αFP (g, P ) βFP (g, D, P ) − αFP (g, P ) βFP (g, D, P )

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

where P

S denotes the empirical distribution over the realized sample S.

These theorems together imply that for both SP and FP subgroup fairness, the degree to which
a group g violates the constraint of γ-fairness can be estimated up to error ε, so long as m ≥
˜O((VCDIM(H) + VCDIM(G))/ε2). The proofs can be found in Appendix B.









3 Equivalence of Auditing and Weak Agnostic Learning

In this section, we give a reduction from the problem of auditing both statistical parity and false
positive rate fairness, to the problem of agnostic learning, and vice versa. This has two implica-
tions. The main implication is that, from a worst-case analysis point of view, auditing is compu-
tationally hard in almost every case (since it inherits this pessimistic state of aﬀairs from agnostic
learning). However, worst-case hardness results in learning theory have not prevented the suc-
cessful practice of machine learning, and there are many heuristic algorithms that in real-world
cases successfully solve “hard” agnostic learning problems. Our reductions also imply that these
heuristics can be used successfully as auditing algorithms, and we exploit this in the development
of our algorithmic results and their experimental evaluation.

We make the following mild assumption on the class of group indicators G, to aid in our reduc-
tions. It is satisﬁed by most natural classes of functions, but is in any case essentially without loss
of generality (since learning negated functions can be simulated by learning the original function
class on a dataset with ﬂipped class labels).

Assumption 3.1. We assume the set of group indicators G satisﬁes closure under negation: for any
g ∈ G, we also have ¬g ∈ G.

Recalling that X = (x, x

(cid:48)) and the following notions will be useful for describing our results:

9

• SP(D) = PrP ,D[D(X) = 1] and FP(D) = PrD,P [D(X) = 1 | y = 0].
• αSP (g, P ) = PrP [g(x) = 1] and αFP (g, P ) = PrP [g(x) = 1, y = 0].
• βSP (g, D, P ) = |SP(D) − SP(D, g)| and βFP (g, D, P ) = |FP(D) − FP(D, g)|.

• P D: the marginal distribution on (x, D(X)).

• P D

y=0: the conditional distribution on (x, D(X)), conditioned on y = 0.

We will think about these as the target distributions for a learning problem: i.e. the problem of
learning to predict D(X) from only the protected features x. We will relate the ability to agnosti-
cally learn on these distributions, to the ability to audit D given access to the original distribution
Paudit(D).

3.1 Statistical Parity Fairness

We give our reduction ﬁrst for SP subgroup fairness. The reduction for FP subgroup fairness
will follow as a corollary, since auditing for FP subgroup fairness can be viewed as auditing for
statistical parity fairness on the subset of the data restricted to y = 0.

Theorem 3.2. Fix any distribution P , and any set of group indicators G. Then for any γ, ε > 0, the
following relationships hold:

• If there is a (γ/2, (γ/2 − ε)) auditing algorithm for G for all D such that SP(D) = 1/2, then the

class G is (γ, γ/2 − ε)-weakly agnostically learnable under P D.

• If G is (γ, γ − ε)-weakly agnostically learnable under distribution P D for all D such that SP(D) =

1/2, then there is a (γ, (γ − ε)/2) auditing algorithm for G for SP fairness under P .

We will prove Theorem 3.2 in two steps. First, we show that any unfair certiﬁcate f for D has

non-trivial error for predicting the decision made by D from the sensitive attributes.

Lemma 3.3. Suppose that the base rate SP(D) ≤ 1/2 and there exists a function f such that

αSP (g, P ) βSP (g, D, P ) = γ.

max{Pr[D(X) = f (x)], Pr[D(X) = ¬f (x)]} ≥ SP(D) + γ.

Proof. To simplify notations, let b = SP(D) denote the base rate, α = αSP and β = βSP . First, observe
that either Pr[D(X) = 1 | f (x) = 1] = b + β or Pr[D(X) = 1 | f (x) = 1] = b − β holds.

In the ﬁrst case, we know Pr[D(X) = 1 | f (x) = 0] < b, and so Pr[D(X) = 0 | f (x) = 0] > 1 − b. It

Then

follows that

Pr[D(X) = f (x)] = Pr[D(X) = f (x) = 1] + Pr[D(X) = f (x) = 0]

= Pr[D(X) = 1 | f (x) = 1] Pr[f (x) = 1] + Pr[D(X) = 0 | f (x) = 0] Pr[f (x) = 0]
> α(b + β) + (1 − α)(1 − b)
= (α − 1)b + (1 − α)(1 − b) + b + αβ
= (1 − α)(1 − 2b) + b + αβ.

10

In the second case, we have Pr[D(X) = 0 | f (x) = 1] = (1 − b) + β and Pr[D(X) = 1 | f (x) = 0] > b. We
can then bound

Pr[D(X) = f (x)] = Pr[D(X) = 1 | f (x) = 0] Pr[f (x) = 0] + Pr[D(X) = 0 | f (x) = 1] Pr[f (x) = 1]

> (1 − α)b + α(1 − b + β) = α(1 − 2b) + b + αβ.

In both cases, we have (1 − 2b) ≥ 0 by our assumption on the base rate. Since α ∈ [0, 1], we know
max{Pr[D(X) = f (x)], Pr[D(X) = ¬f (x)]} ≥ b + αβ = b + γ

which recovers our bound.

In the next step, we show that if there exists any function f that accurately predicts the de-
cisions made by the algorithm D, then either f or ¬f can serve as an unfairness certiﬁcate for
D.
Lemma 3.4. Suppose that the base rate SP(D) ≥ 1/2 and there exists a function f such that Pr[D(X) =
f (x)] ≥ SP(D) + γ for some value γ ∈ (0, 1/2). Then there exists a function g such that

αSP (g, P ) βSP (g, D, P ) ≥ γ/2,

where g ∈ {f , ¬f }.

Proof. Let b = SP(D). We can expand Pr[D(X) = f (x)] as follows:

Pr[D(X) = f (x)] = Pr[D(X) = f (x) = 1] + Pr[D(X) = f (x) = 0]

= Pr[D(X) = 1 | f (x) = 1] Pr[f (x) = 1] + Pr[D(X) = 0 | f (x) = 0] Pr[f (x) = 0]

This means

Pr[D(X) = f (x)] − b

= (Pr[D(X) = 1 | f (x) = 1] − b) Pr[f (x) = 1] + (Pr[D(X) = 0 | f (x) = 0] − b) Pr[f (x) = 0] ≥ γ
Suppose that (Pr[D(X) = 1 | f (x) = 1] − b) Pr[f (x) = 1] ≥ γ/2, then our claim holds with g = f . Sup-
pose not, then we must have

(Pr[D(X) = 0 | f (x) = 0] − b) Pr[f (x) = 0] = ((1 − b) − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ γ/2

Note that by our assumption b ≥ (1 − b). This means

(b − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ ((1 − b) − Pr[D(X) = 1 | f (x) = 0]) Pr[f (x) = 0] ≥ γ/2

which implies that our claim holds with g = ¬f .

Proof of Theorem 3.2. Suppose that the class G satisﬁes minf ∈G err(f , P D) ≤ 1/2−γ. Then by Lemma 3.4,
there exists some g ∈ G such that Pr[g(x) = 1]| Pr[D(X) = 1 | g(x) = 1] − SP(D)| ≥ γ/2. By the as-
(cid:48) ∈ G that is an
sumption of auditability, we can then use the auditing algorithm to ﬁnd a group g
(cid:48) predicts D with an
(γ/2 − ε)-unfair certiﬁcate of D. By Lemma 3.3, we know that either g
accuracy of at least 1/2 + (γ/2 − ε).

(cid:48) or ¬g

In the reverse direction, consider the auditing problem on the classiﬁer D. We can treat each
pair (x, D(X)) as a labelled example and learn a hypothesis in G that approximates the decisions
made by D. Suppose that D is γ-unfair. Then by Lemma 3.3, we know that there exists some g ∈ G
such that Pr[D(X) = g(x)] ≥ 1/2 + γ. Therefore, the weak agnostic learning algorithm from the
(cid:48)(x)] ≥ 1/2 + (γ − ε). By Lemma 3.4,
hypothesis of the theorem will return some g
we know g

(cid:48) is a (γ − ε)/2-unfair certiﬁcate for D.

(cid:48) with Pr[D(X) = g

(cid:48) or ¬g

11

3.2 False Positive Fairness

A corollary of the above reduction is an analogous equivalence between auditing for FP subgroup
fairness and agnostic learning. This is because a FP fairness constraint can be viewed as a statis-
tical parity fairness constraint on the subset of the data such that y = 0. Therefore, Theorem 3.2
implies the following:

Corollary 3.5. Fix any distribution P , and any set of group indicators G. The following two relation-
ships hold:

• If there is a (γ/2, (γ/2 − ε)) auditing algorithm for G for all D such that FP(D) = 1/2, then the

class G is (γ, γ/2 − ε)-weakly agnostically learnable under P D

y=0.

• If G is (γ, γ −ε)–weakly agnostically learnable under distribution P D

y=0 for all D such that FP(D) =
1/2, then there is a (γ, (γ − ε)/2) auditing algorithm for FP subgroup fairness for G under distri-
bution P .

3.3 Worst-Case Intractability of Auditing

While we shall see in subsequent sections that the equivalence given above has positive algo-
rithmic and experimental consequences, from a purely theoretical perspective the reduction of
agnostic learning to auditing has strong negative worst-case implications. More precisely, we can
import a long sequence of formal intractability results for agnostic learning to obtain:

Theorem 3.6. Under standard complexity-theoretic intractability assumptions, for G the classes of con-
junctions of boolean attributes, linear threshold functions, or bounded-degree polynomial threshold func-
tions, there exist distributions P such that the auditing problem cannot be solved in polynomial time, for
either statistical parity or false positive fairness.

The proof of this theorem follows from Theorem 3.2, Corollary 3.5, and the following negative
results from the learning theory literature. Feldman et al. [2012] show a strong negative result
for weak agnostic learning for conjunctions: given a distribution on labeled examples from the
hypercube such that there exists a monomial (or conjunction) consistent with (1−ε)-fraction of the
examples, it is NP-hard to ﬁnd a halfspace that is correct on (1/2 + ε)-fraction of the examples, for
arbitrary constant ε > 0. Diakonikolas et al. [2011] show that under the Unique Games Conjecture,
no polynomial-time algorithm can ﬁnd a degree-d polynomial threshold function (PTF) that is
consistent with (1/2 + ε) fraction of a given set of labeled examples, even if there exists a degree-d
PTF that is consistent with a (1 − ε) fraction of the examples. Diakonikolas et al. [2011] also show
that it is NP-Hard to ﬁnd a degree-2 PTF that is consistent with a (1/2 + ε) fraction of a given set
of labeled examples, even if there exists a halfspace (degree-1 PTF) that is consistent with a (1 − ε)
fraction of the examples.

While Theorem 3.6 shows that certain natural subgroup classes G yield intractable auditing
problems in the worst case, in the rest of the paper we demonstrate that eﬀective heuristics for
this problem on speciﬁc (non-worst case) distributions can be used to derive an eﬀective and
practical learning algorithm for subgroup fairness.

12

4 A Learning Algorithm Subject to Fairness Constraints G

In this section, we present an algorithm for training a (randomized) classiﬁer that satisﬁes false-
positive subgroup fairness simultaneously for all protected subgroups speciﬁed by a family of
group indicator functions G. All of our techniques also apply to a statistical parity or false negative
rate constraint.

Let S denote a set of n labeled examples {zi = (xi, x

i=1, and let P denote the empirical
distribution over this set of examples. Let H be a hypothesis class deﬁned over both the protected
and unprotected attributes, and let G be a collection of group indicators over the protected at-
tributes. We assume that H contains a constant classiﬁer (which implies that there is at least one
fair classiﬁer to be found, for any distribution).

i), yi)}n

Our goal will be to ﬁnd the distribution over classiﬁers from H that minimizes classiﬁcation
error subject to the fairness constraint over G. We will design an iterative algorithm that, when
given access to a CSC oracle, computes an optimal randomized classiﬁer in polynomial time.

(cid:48)

Let D denote a probability distribution over H. Consider the following Fair ERM (Empirical

Risk Minimization) problem:

such that ∀g ∈ G

E
h∼D

[err(h, P )]

min
D∈∆H
αFP (g, P ) βFP (g, D, P ) ≤ γ.

(2)

(3)

(cid:48)) (cid:44) y], and the quantities αFP and βFP are deﬁned in Deﬁnition 2.3.
where err(h, P ) = PrP [h(x, x
We will write OPT to denote the objective value at the optimum for the Fair ERM problem, that is
the minimum error achieved by a γ-fair distribution over the class H.

Observe that the optimization is feasible for any distribution P : the constant classiﬁers that
labels all points 1 or 0 satisfy all subgroup fairness constraints. At the moment, the number of
decision variables and constraints may be inﬁnite (if H and G are inﬁnite hypothesis classes), but
we will address this momentarily.

Assumption 4.1 (Cost-Sensitive Classiﬁcation Oracle). We assume our algorithm has access to the
cost-sensitive classication oracles CSC(H) and CSC(G) over the classes H and G.

Our main theoretical result is an computationally eﬃcient oracle-based algorithm for solving

the Fair ERM problem.

Theorem 4.2. Fix any ν, δ ∈ (0, 1). Then given an input of n data points and accuracy parameters ν, δ
and access to oracles CSC(H) and CSC(G), there exists an algorithm runs in polynomial time, and with
probability at least 1 − δ, output a randomized classiﬁer ˆD such that err( ˆD, P ) ≤ OPT +ν, and for any
g ∈ G, the fairness constraint violations satisﬁes

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ + O(ν).

Overview of our solution. We present our solution in steps:

• Step 1: Fair ERM as LP. First, we rewrite the Fair ERM problem as a linear program with
ﬁnitely many decision variables and constraints even when H and G are inﬁnite. To do this,
we take advantage of the fact that Sauer’s Lemma lets us bound the number of labellings
that any hypothesis class H of bounded VC dimension can induce on any ﬁxed dataset.

13

The LP has one variable for each of these possible labellings, rather than one variable for
each hypothesis. Moreover, again by Sauer’s Lemma, we have one constraint for each of the
ﬁnitely many possible subgroups induced by G on the ﬁxed dataset, rather than one for each
of the (possibly inﬁnitely many) subgroups deﬁnable over arbitrary datasets. This step is
important — it will guarantee that strong duality holds.

• Step 2: Formulation as Game. We then derive the partial Lagrangian of the LP, and note
that computing an approximately optimal solution to this LP is equivalent to ﬁnding an ap-
proximate minmax solution for a corresponding zero-sum game, in which the payoﬀ func-
tion U is the value of the Lagrangian. The pure strategies of the primal or “Learner” player
correspond to classiﬁers h ∈ H, and the pure strategies of the dual or “Auditor” player cor-
respond to subgroups g ∈ G. Intuitively, the Learner is trying to minimize the sum of the
prediction error and a fairness penalty term (given by the Lagrangian), and the Auditor is
trying to penalize the fairness violation of the Learner by ﬁrst identifying the subgroup with
the greatest fairness violation and putting all the weight on the dual variable corresponding
to this subgroup. In order to reason about convergence, we restrict the set of dual variables
to lie in a bounded set: C times the probability simplex. C is a parameter that we have
to set in the proof of our theorem to give the best theoretical guarantees — but it is also a
parameter that we will vary in the experimental section.

• Step 3: Best Responses as CSC. We observe that given a mixed strategy for the Auditor,
the best response problem of the Learner corresponds to a CSC problem. Similarly, given a
mixed strategy for the Learner, the best response problem of the Auditor corresponds to an
auditing problem (which can be represented as a CSC problem). Hence, if we have oracles
for solving CSC problems, we can compute best responses for both players, in response to
arbitrary mixed strategies of their opponents.

• Step 4: FTPL for No-Regret. Finally, we show that the ability to compute best responses
for each player is suﬃcient to implement dynamics known to converge quickly to equilib-
rium in zero-sum games. Our algorithm has the Learner play Follow the Perturbed Leader
(FTPL) Kalai and Vempala [2005], which is a no-regret algorithm, against an Auditor who at
every round best responds to the learner’s mixed strategy. By the seminal result of Freund
and Schapire [1996], the average plays of both players converge to an approximate equilib-
rium. In order to implement this in polynomial time, we need to represent the loss of the
learner as a low-dimensional linear optimization problem. To do so, we ﬁrst deﬁne an ap-
propriately translated CSC problem for any mixed strategy λ by the Auditor, and cast it as
a linear optimization problem.

4.1 Rewriting the Fair ERM Problem

To rewrite the Fair ERM problem, we note that even though both G and H can be inﬁnite sets,
the sets of possible labellings on the data set S induced by these classes are ﬁnite. More formally,
we will write G(S) and H(S) to denote the set of all labellings on S that are induced by G and H
respectively, that is

G(S) = {(g(x1), . . . , g(xn)) | g ∈ G}

and,

H(S) = {(h(X1), . . . , h(Xn)) | h ∈ H}

We can bound the cardinalities of G(S) and H(S) using Sauer’s Lemma.

14

Lemma 4.3 (Sauer’s Lemma (see e.g. Kearns and Vazirani [1994])). Let S be a data set of size n. Let
d1 = VCDIM(H) and d2 = VCDIM(G) be the VC-dimensions of the two classes. Then
(cid:17)

(cid:17)

|H(S)| ≤ O

(cid:16)
nd1

and

|G(S)| ≤ O

(cid:16)
nd2

.

Given this observation, we can then consider an equivalent optimization problem where the
distribution D is over the set of labellings in H(S), and the set of subgroups are deﬁned by the
labellings in G(S). We will view each g in G(S) as a Boolean function.

To simplify notations, we will deﬁne the following “fairness violation” functions for any g ∈ G

and any h ∈ H:

Φ+(h, g) ≡ αFP (g, P ) (FP(h) − FP(h, g)) − γ
Φ−(h, g) ≡ αFP (g, P ) (FP(h, g) − FP(h)) − γ

Moreover, for any distribution D over H, for any sign • ∈ {+, −}

Φ•(D, g) = E
h∼D

[Φ•(h, g)] .

Claim 4.4. For any g ∈ G, h ∈ H, and any ν > 0,

max{Φ+(D, g), Φ−(D, g)} ≤ ν

if and only if αFP (g, P ) βFP (g, D, P ) ≤ γ + ν.

Thus, we will focus on the following equivalent optimization problem.

such that for each g ∈ G(S) :

min
D∈∆H(S)

[err(h, P )]

E
h∼D
Φ+(D, g) ≤ 0
Φ−(D, g) ≤ 0

For each pair of constraints (7) and (8), corresponding to a group g ∈ G(S), we introduce a pair

of dual variables λ+

g and λ

−
g . The partial Lagrangian of the linear program is the following:

L(D, λ) = E
h∼D

[err(h, P )] +

(cid:88)

(cid:16)
λ+

g∈G(S)

−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:17)

By Sion’s minmax theorem [Sion, 1958], we have

min
D∈∆H(S)

max
λ∈R2|G(S)|
+

L(p, λ) = max
λ∈R2|G(S)|

+

min
D∈∆H(S)

L(p, λ) = OPT

where OPT denotes the optimal objective value in the fair ERM problem. Similarly, the distri-
L(D, λ) corresponds to an optimal feasible solution to the fair ERM linear
bution arg minD maxλ
program. Thus, ﬁnding an optimal solution for the fair ERM problem reduces to computing a
minmax solution for the Lagrangian. Our algorithms will both compute such a minmax solution
by iteratively optimizing over both the primal variables D and dual variables λ. In order to guar-
antee convergence in our optimization, we will restrict the dual space to the following bounded
set:

Λ = {λ ∈ R2|G(S)|

+

| (cid:107)λ(cid:107)

1

≤ C}.

15

(4)

(5)

(6)

(7)

(8)

where C will be a parameter of our algorithm. Since Λ is a compact and convex set, the minmax
condition continues to hold [Sion, 1958]:

min
D∈∆H(S)

max
λ∈Λ

L(D, λ) = max
λ∈Λ

min
D∈∆H(S)

L(D, λ)

(9)

If we knew an upper bound C on the (cid:96)1 norm of the optimal dual solution, then this restriction
on the dual solution would not change the minmax solution of the program. We do not in general
know such a bound. However, we can show that even though we restrict the dual variables to
lie in a bounded set, any approximate minmax solution to Equation (9) is also an approximately
optimal and approximately feasible solution to the original fair ERM problem.

Theorem 4.5. Let ( ˆD, ˆλ) be a ν-approximate minmax solution to the Λ-bounded Lagrangian problem
in the sense that

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν and, L( ˆD, ˆλ) ≥ max
λ∈Λ

L( ˆD, λ) − ν.

Then err( ˆD, P ) ≤ OPT +2ν and for any g ∈ G(S),

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C

.

4.2 Zero-Sum Game Formulation

To compute an approximate minmax solution, we will ﬁrst view Equation (9) as the following two
player zero-sum matrix game. The Learner (or the minimization player) has pure strategies cor-
responding to H, and the Auditor (or the maximization player) has pure strategies corresponding
to the set of vertices Λpure in Λ — more precisely, each vertex or pure strategy either is the all
zero vector or consists of a choice of a g ∈ G(S), along with the sign + or − that the corresponding
g-fairness constraint will have in the Lagrangian. More formally, we write

•
g = C | g ∈ G(S), • ∈ {±}} ∪ {0}
Λpure = {λ ∈ Λ with λ

Even though the number of pure strategies scales linearly with |G(S)|, our algorithm will never
need to actually represent such vectors explicitly. Note that any vector in Λ can be written as a
convex combination of the maximization player’s pure strategies, or in other words: as a mixed
strategy for the Auditor. For any pair of actions (h, λ) ∈ H × Λpure, the payoﬀ is deﬁned as
(cid:88)

U (h, λ) = err(h, P ) +

(cid:16)
λ+

−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:17)

.

Claim 4.6. Let D ∈ ∆H(S) and λ ∈ Λ such that (p, λ) is a ν-approximate minmax equilibrium in the
zero-sum game deﬁned above. Then (p, λ) is also a ν-approximate minmax solution for Equation (9).

Our problem reduces to ﬁnding an approximate equilibrium for this game. A key step in our
solution is the ability to compute best responses for both players in the game, which we now show
can be solved by the cost-sensitive classication (CSC) oracles.

g∈G(S)

16

Learner’s best response as CSC. Fix any mixed strategy (dual solution) λ ∈ Λ of the Auditor.
The Learner’s best response is given by:

err(h, P ) +

argmin
D∈∆H(S)

(cid:88)

g∈G(S)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

(10)

Note that it suﬃces for the Learner to optimize over deterministic classiﬁers h ∈ H, rather than
distributions over classiﬁers. This is because the Learner is solving a linear optimization problem
over the simplex, and so always has an optimal solution at a vertex (i.e. a single classiﬁer h ∈ H).
We can reduce this problem to one that can be solved with a single call to a CSC oracle.
In
particular, we can assign costs to each example (Xi, yi) as follows:

• if yi = 1, then c0

i = 0 and c1

i = − 1
n ;

• otherwise, c0

i = 0 and

c1
i =

1
n

+

1
n

(cid:88)

g∈G(S)

(λ+
g

−
g ) (Pr[g(x) = 1 | y = 0] − 1[g(xi) = 1])
− λ

(11)

Given a ﬁxed set of dual variables λ, we will write LC(λ) ∈ Rn to denote the vector of costs for
labelling each datapoint as 1. That is, LC(λ) is the vector such that for any i ∈ [n], LC(λ)i = c1
i .

Remark 4.7. Note that in deﬁning the costs above, we have translated them from their most natural
values so that the cost of labeling any example with 0 is 0. In doing so, we recall that by Claim 2.7,
the solution to a cost-sensitive classiﬁcation problem is invariant to translation. As we will see, this
will allow us to formulate the learner’s optimization problem as a low-dimensional linear optimization
problem, which will be important for an eﬃcient implementation of follow the perturbed leader. In
particular, if we ﬁnd a hypothesis that produces the n labels y = (y1, . . . , yn) for the n points in our
dataset, then the cost of this labelling in the CSC problem is by construction (cid:104)LC(λ), y(cid:105).

Auditor’s best response as CSC. Fix any mixed strategy (primal solution) p ∈ ∆H(S) of the
Learner. The Auditor’s best response is given by:
(cid:88)

(cid:88)

(cid:16)
λ+

−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:17)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

argmax
λ∈Λ

err(D, P ) +

g∈G(S)

= argmax
λ∈Λ

g∈G(S)

(12)
To ﬁnd the best response, consider the problem of computing ( ˆg, ˆ•) = argmax(g,•) Φ•(D, g).
There are two cases. In the ﬁrst case, p is a strictly feasible primal solution: that is Φˆ•(D, ˆg) < 0.
In this case, the solution to (12) sets λ = 0. Otherwise, if p is not strictly feasible, then by the
following Lemma 4.8 the best response is to set λˆ•

ˆg = C (and all other coordinates to 0).

Lemma 4.8. Fix any D ∈ ∆H(S) such that that maxg∈G(S)
(cid:48))
with one non-zero coordinate (λ

{Φ+(D, g), Φ−(D, g)} > 0. Let λ

(cid:48) ∈ Λ be vector

Then L(D, λ

(cid:48)) ≥ maxλ∈Λ L(D, λ).

•(cid:48)
g (cid:48) = C, where
, •(cid:48)

(g

(cid:48)

) = argmax

(g,•)∈G(S)×{±}

{Φ•(D, g)}

17

Therefore, it suﬃces to solve for argmax(g,•) Φ•(D, g). We proceed by solving argmaxg Φ+(D, g)
and argmaxg Φ−(D, g) separately: both problems can be reduced to a cost-sensitive classiﬁcation
problem. To solve for argmaxg Φ+(D, g) with a CSC oracle, we assign costs to each example (Xi, yi)
as follows:

• if yi = 1, then c0

i = 0 and c1

i = 0;

• otherwise, c0

i = 0 and

c1
i =

(cid:20)

−1
n

E
h∼D

[FP(h)] − E
h∼D

(cid:21)
[h(Xi)]

(13)

To solve for argmaxg Φ−(D, g) with a CSC oracle, we assign the same costs to each example

(Xi, yi), except when yi = 0, labeling “1” incurs a cost of

c1
i =

(cid:20)

−1
n

E
h∼D

[h(Xi)] − E
h∼D

(cid:21)
[FP(h)]

4.3 Solving the Game with No-Regret Dynamics

To compute an approximate equilibrium of the zero-sum game, we will simulate the following no-
regret dynamics between the Learner and the Auditor over rounds: over each of the T rounds, the
Learner plays a distribution over the hypothesis class according to a no-regret learning algorithm
(Follow the Perturbed Leader), and the Auditor plays an approximate best response against the
Learner’s distribution for that round. By the result of Freund and Schapire [1996], the average
plays of both players over time converge to an approximate equilibrium of the game, as long as
the Learner has low regret.

Theorem 4.9 (Freund and Schapire [1996]). Let D1, D2, . . . , DT ∈ ∆H(S) be a sequence of distribu-
tions played by the Learner, and let λ1, λ2, . . . , λT ∈ Λpure be the Auditor’s sequence of approximate best
responses against these distributions respectively. Let D = 1
t=1 λt be the two
T
players’ empirical distributions over their strategies. Suppose that the regret of the Learner satisﬁes

t=1 Dt and λ = 1
T

(cid:80)T

(cid:80)T

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105)− min
h∈H(S)

T(cid:88)

t=1

U (h, λt) ≤ γLT

and max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)]−

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) ≤ γAT .

Then (D, λ) is an (γL + γA)-approximate minimax equilibrium of the game.

Our Learner will play using the Follow the Perturbed Leader (FTPL), which gives a no-regret
guarantee. In order to implement FPTL, we will ﬁrst need to formulate the Learner’s best response
problem as a linear optimization problem over a low dimensional space. For each round t, let
t
s<t λs be the vector representing the sum of the actions played by the auditor over previous
λ
t
rounds, and recall that LC(λ
) is the cost vector given by our cost-sensitive classiﬁcation reduction.
t
is the following linear optimization problem
Then the Learner’s best response problem against λ

(cid:80)

=

t

(cid:104)LC(λ

), h(cid:105).

min
h∈H(S)

18

To run the FTPL algorithm, the Learner will optimize a “perturbed” version of the problem above.
In particular, the Learner will play a distribution Dt over the hypothesis class H(S) that is im-
plicitely deﬁned by the following sampling operation. To sample a hypothesis h from Dt, the
learner solves the following randomized optimization problem:

min
h∈H(S)

t
(cid:104)LC(λ

), h(cid:105) +

(cid:104)ξ, h(cid:105),

1
η

(14)

where η is a parameter and ξ is a noise vector drawn from the uniform distribution over [0, 1]n.
Note that while it is intractable to explicitly represent the distribution Dt (which has support size
scaling with |H(S)|), we can sample from Dt eﬃciently given access to a cost-sensitive classiﬁcation
oracle for H. By instantiating the standard regret bound of FTPL for online linear optimization
(Theorem 2.9), we get the following regret bound for the Learner.

Lemma 4.10. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm with η = n
, and λ1, . . . , λT be the
sequence of plays by the Auditor. Then

1√
nT

(1+C)

(cid:113)

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

T(cid:88)

t=1

U (h, λt) ≤ 2n1/4(1 + C)

T

√

Now we consider how the Auditor (approximately) best responds to the distribution Dt. The
main obstacle is that we do not have an explicit representation for Dt. Thus, our ﬁrst step is to
approximate Dt with an explicitly represented sparse distribution ˆDt. We do that by drawing m
i.i.d. samples from Dt, and taking the empirical distribution ˆDt over the sample. The Auditor
will best respond to this empirical distribution ˆDt. To show that any best response to ˆDt is also an
approximate best response to Dt, we will rely on the following uniform convergence lemma, which
bounds the diﬀerence in expected payoﬀ for any strategy of the auditor, when played against Dt
as compared to ˆDt.
Lemma 4.11. Fix any ξ, δ ∈ (0, 1) and any distribution D over H(S). Let h1, . . . , hm be m i.i.d. draws
from p, and ˆD be the empirical distribution over the realized sample. Then with probability at least 1 − δ
over the random draws of hj’s, the following holds,

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as long as m ≥ c0

C2(ln(1/δ)+d2 ln(n))
ξ2

for some absolute constant c0 and d2 = VCDIM(G).

Using Lemma 4.11, we can derive a regret bound for the Auditor in the no-regret dynamics.

Lemma 4.12. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm. For each Dt, let ˆDt be the empirical distri-
bution over m i.i.d. draws from Dt. Let λ1, . . . , λT be the Auditor’s best responses against ˆD1, . . . , ˆDT .
Then with probability 1 − δ,

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:114)

(cid:104)
U (h, λt)

(cid:105) ≤ T

c0C2(ln(T /δ) + d2 ln(n))
m

for some absolute constant c0 and d2 = VCDIM(G).

19

Finally, let D and λ be the average of the strategies played by the two players over the course
of the dynamics. Note that D is an average of many distributions with large support, and so D
itself has support size that is too large to represent explicitely. Thus, we will again approximate D
with a sparse distribution ˆD estimated from a sample drawn from D. Note that we can eﬃciently
sample from D given access to a CSC oracle. To sample, we ﬁrst uniformly randomly select a
round t ∈ [T ], and then use the CSC oracle to solve the sampling problem deﬁned in (14), with the
noise random variable ξ freshly sampled from its distribution. The full algorithm is described in
Algorithm 2 and we present the proof for Theorem 4.2 below.

Algorithm 2 FairNR: Fair No-Regret Dynamics

Input: distribution P over n labelled data points, CSC oracles CSC(H) and CSC(G), dual bound
C, and target accuracy parameter ν, δ
0
= 0, η = n
Initialize: Let C = 1/ν, λ

(cid:113)

,

(1+C)

1√
nT

m =

(ln(2T /δ)d2 ln(n)) C2c0T
n(1 + C)2 ln(2/δ)

√

and,

T =

√

4

n ln(2/δ)

ν4

For t = 1, . . . , T :

For s = 1, . . . m:

Sample from the Learner’s FTPL distribution:

Draw a random vector ξs uniformly at random from [0, 1]n
Use the oracle CSC(H) to compute h(s,t) = argminh∈H(S)

(cid:104)LC(λ

(t−1)

), h(cid:105) + 1
η

(cid:104)ξs, h(cid:105)

Let ˆDt be the empirical distribution over {hs,t}

Auditor best responds to ˆDt:

Use the oracle CSC(G) to compute λt = argmaxλ

E

h∼ ˆD [U (h, λ)]

t
Update: Let λ

=

(cid:80)

t(cid:48)≤t λt(cid:48)

Sample from the average distribution D =

(cid:80)T

t=1 Dt:

For s = 1, . . . m:

Draw a random number r ∈ [T ] and a random vector ξs uniformly at random from [0, 1]n
Use the oracle CSC(H) to compute h(r,t) = argminh∈H(S)

(cid:104)LC(λ

(cid:104)ξs, h(cid:105)

), h(cid:105) + 1
η

(r−1)

Let ˆD be the empirical distribution over {hr,t}

Output: ˆD as a randomized classiﬁer

Proof of Theorem 4.2. By Theorem 4.5, it suﬃces to show that with probability at least 1 − δ, ( ˆD, λ)
is a ν-approximate equilibrium in the zero-sum game. As a ﬁrst step, we will rely on Theorem 4.9
to show that (D, λ) forms an approximate equilibrium.

By Lemma 4.10, the regret of the sequence D1, . . . , DT is bounded by:

γL =


T(cid:88)


t=1

1
T

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

T(cid:88)

t=1

U (h, λt)





≤ 2n1/4(1 + C)
√
T

20

By Lemma 4.12, with probability 1 − δ/2, we have

(cid:114)

≤

γA

c0C2(ln(2T /δ) + d2 ln(n))
m

We will condition on this upper-bound event on γA for the rest of this proof, which is the case
except with probability δ/2. By Theorem 4.9, we know that the average plays (D, λ) form an
(γL + γA)-approximate equilibrium.

Finally, we need to bound the additional error for outputting the sparse approximation ˆD
instead of D. We can directly apply Lemma 4.11, which implies that except with probability δ/2,
the pair ( ˆD, λ) form a R-approximate equilibrium, with

R ≤ γA + γL +

(cid:112)

c0C2(ln(2/δ) + d2 ln(n))
m

√

Note that R ≤ ν as long as we have C = 1/ν,

m =

(ln(2T /δ)d2 ln(n)) C2c0T
n(1 + C)2 ln(2/δ)

√

and,

T =

√

4

n ln(2/δ)

ν4

This completes our proof.

5 Experimental Evaluation

We now describe an experimental evaluation of our proposed algorithmic framework on a dataset
in which fairness is a concern, due to the preponderance of racial and other sensitive features. For
far more detailed experiments on four real datasets investigating the convergence properties of
our algorithm, evaluating its accuracy vs. fairness tradeoﬀs, and comparing our approach to the
recent algorithm of Agarwal et al. [2017], we direct the reader to Kearns et al. [2018]. Python code
and an illustrative Jupyter notebook are provided here (https://github.com/algowatchpenn/GerryFair).
While the no-regret-based algorithm described in the last section enjoys provably polynomial
time convergence, for the experiments we instead implemented a simpler yet eﬀective algorithm
based on Fictitious Play dynamics. We ﬁrst describe and discuss this modiﬁed algorithm.

5.1 Solving the Game with Fictitious Play

Like the algorithm given in the last section, the algorithm we implemented works by simulating
a game dynamic that converges to Nash equilibrium in the zero-sum game that we derived, cor-
responding to the Fair ERM problem. Rather than using a no-regret dynamic, we instead use a
simple iterative procedure known as Fictitious Play [Brown, 1949]. Fictitious Play dynamics has
the beneﬁt of being more practical to implement: at each round, both players simply need to com-
pute a single best response to the empirical play of their opponents, and this optimization requires
only a single call to a CSC oracle. In contrast, the FTPL dynamic we gave in the previous section
requires making many calls to a CSC oracle per round — a computationally expensive process —
in order to ﬁnd a sparse approximation to the Learner’s mixed strategy at that round. Fictitious
Play also has the beneﬁt of being deterministic, unlike the randomized sampling required in the
FTPL no-regret dynamic, thus eliminating a source of experimental variance.

21

The disadvantage is that Fictitious Play is only known to converge to equilibrium in the
limit Robinson [1951], rather than in a polynomial number of rounds (though it is conjectured
to converge quickly under rather general circumstances; see Daskalakis and Pan [2014] for a re-
cent discussion). Nevertheless, this is the algorithm that we use in our experiments — and as we
will show, it performs well on real data, despite the fact that it has weaker theoretical guarantees
compared to the algorithm we presented in the last section.

Fictitious play proceeds in rounds, and in every round each player chooses a best response
to his opponent’s empirical history of play across previous rounds, by treating it as the mixed
strategy that randomizes uniformly over the empirical history. Pseudocode for the implemented
algorithm is given below.

Algorithm 3 FairFictPlay: Fair Fictitious Play

Input: distribution P over the labelled data points, CSC oracles CSC(H) and CSC(G) for the
classes H(S) and G(S) respectively, dual bound C, and number of rounds T
Initialize: set h0 to be some classiﬁer in H, set λ0 to be the zero vector. Let D and λ be the point
distributions that put all their mass on h0 and λ0 respectively.
For t = 1, . . . , T :

Compute the empirical play distributions:

Let D be the uniform distribution over the set of classiﬁers {h0, . . . , ht−1}
Let λ =

be the auditor’s empirical dual vector

(cid:80)
t

(cid:48)

(cid:48)

<t λt
t

Learner best responds: Use the oracle CSC(H) to compute ht = argminh∈H(S)
Auditor best responds: Use the oracle CSC(G) to compute λt = argmaxλ

(cid:104)LC(λ), h(cid:105)
h∼D [U (h, λ)]

E

Output: the ﬁnal empirical distribution D over classiﬁers

5.2 Description of Data

The dataset we use for our experimental valuation is known as the “Communities and Crime”
(C&C) dataset, available at the UC Irvine Data Repository4. Each record in this dataset describes
the aggregate demographic properties of a diﬀerent U.S. community; the data combines socio-
economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey,
and crime data from the 1995 FBI UCR. The total number of records is 1994, and the number of
features is 122. The variable to be predicted is the rate of violent crime in the community.

While there are larger and more recent datasets in which subgroup fairness is a potential con-
cern, there are properties of the C&C dataset that make it particularly appealing for the initial
experimental evaluation of our proposed algorithm. Foremost among these is the relatively high
number of sensitive or protected attributes, and the fact that they are real-valued (since they rep-
resent aggregates in a community rather than speciﬁc individuals). This means there is a very
large number of protected sub-groups that can be deﬁned over them. There are distinct con-
tinuous features measuring the percentage or per-capita representation of multiple racial groups
(including white, black, Hispanic, and Asian) in the community, each of which can vary indepen-
dently of the others. Similarly, there are continuous features measuring the average per capita
incomes of diﬀerent racial groups in the community, as well as features measuring the percentage
of each community’s police force that falls in each of the racial groups. Thus restricting to features

4http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime

22

capturing race statistics and a couple of related ones (such as the percentage of residents who do
not speak English well), we obtain an 18-dimensional space of real-valued protected attributes.
We note that the C&C dataset has numerous other features that arguably could or should be pro-
tected as well (such as gender features), which would raise the dimensionality of the protected
subgroups even further. 5

We convert the real-valued rate of violent crime in each community to a binary label indicating
whether the community is in the 70th percentile of that value, indicating that it is a relatively
high-crime community. Thus the strawman baseline that always predicts 0 (lower crime) has
error approximately 30% or 0.3 on this classiﬁcation problem. We chose the 70th percentile since
it seems most natural to predict the highest crime rates.

As in the theoretical sections of the paper, our main interest and emphasis is on the eﬀective-

ness of our proposed algorithm FairFictPlay on a given dataset, including:

• Whether the algorithm in fact converges, and does so in a feasible amount of computation.
Recall that formal convergence is only guaranteed under the assumption of oracles that do
not exist in practice, and even then is only guaranteed asymptotically.

• Whether the classiﬁer learned by the algorithm has nontrivial accuracy, as well as strong

subgroup fairness properties.

racy and subgroup fairness.

• Whether the algorithm and dataset permits nontrivial tuning of the trade-oﬀ between accu-

As discussed in Section 2.1, we note that all of these issues can be investigated entirely in-sample,
without concern for generalization performance. Thus for simplicity, despite the fact that our
algorithm enjoys all the usual generalization properties depending on the VC dimension of the
Learner’s hypothesis space and the Auditor’s subgroup space (see Theorems 2.12 and 2.11), we
report all results here on the full C&C dataset of 1994 points, treating it as the true distribution
of interest.

5.3 Algorithm Implementation

The main details in the implementation of FairFictPlay are the identiﬁcation of the model classes
for Learner and Auditor, the implementation of the cost sensitive classiﬁcation oracle and auditing
oracle, and the identiﬁcation of the protected features for Auditor. For our experiments, at each
round Learner chooses a linear threshold function over all 122 features. We implement the cost
sensitive classiﬁcation oracle via a two stage regression procedure. In particular, the inputs to
the cost sensitive classiﬁcation oracle are cost vectors c0, c1, where the ith element of ck is the cost
of predicting k on datapoint i. We train two linear regression models r0, r1 to predict c0 and c1
respectively, using all 122 features. Given a new point x, we predict the cost of classifying x as 0
and 1 using our regression models: these predictions are r0(x) and r1(x) respectively. Finally we
output the prediction ˆy corresponding to lower predicted cost: ˆy = argmini∈{0,1}ri(x).

Auditor’s model class consists of all linear threshold functions over just the 18 aforementioned
protected race-based attributes. As per the algorithm, at each iteration t Auditor attempts to ﬁnd
a subgroup on which the false positive rate is substantially diﬀerent than the base rate, given

5Ongoing experiments on other datasets where fairness is a concern will be reported on in a forthcoming experi-

mental paper.

23

the Learner’s randomized classiﬁer so far. We implement the auditing oracle by treating it as
a weighted regression problem in which the goal is ﬁnd a linear function (which will be taken
to deﬁne the subgroup) that on the negative examples, can predict the Learner’s probabilistic
classiﬁcation on each point. We use the same regression subroutine as Learner does, except that
Auditor only has access to the 18 sensitive features, rather than all 122.

Recall that in addition to the choices of protected attributes and model classes for Learner
and Auditor, FairFictPlay has a parameter C, which is a bound on the norm of the dual variables
for Auditor (the dual player). While the theory does not provide an explicit bound or guide for
choosing C, it needs to be large enough to permit the dual player to force the minmax value of the
game. For our experiments we chose C = 10, which despite being a relatively small value seems
to suﬃce for (approximate) convergence.

The other and more meaningful parameter of the algorithm is the bound γ in the Fair ERM
optimization problem implemented by the game, which controls the amount of unfairness per-
mitted. If on a given round the subgroup disparity found by the Auditor is greater than γ, the
Learner must react by adding a fairness penalty for this subgroup to its objective function; if it is
smaller than γ, the Learner can ignore it and continue to optimize its previous objective function.
Ideally, and as we shall see, varying γ allows us to trace out a menu of trade-oﬀs between accuracy
and fairness.

5.4 Results

Particularly in light of the gaps between the idealized theory and the actual implementation,
the most basic questions about FairFictPlay are whether it converges at all, and if so, whether
it converges to “interesting” models — that is, models with both nontrivial classiﬁcation error
(much better than the 30% or 0.3 baserate), and nontrivial subgroup fairness (much better than
ignoring fairness altogether). We shall see that at least for the C&C dataset, the answers to these
questions is strongly aﬃrmative.

(a)

(b)

Figure 1: Evolution of the error and unfairness of Learner’s classiﬁer across iterations, for varying
choices of γ. (a) Error εt of Learner’s model vs iteration t. (b) Unfairness γt of subgroup found by
Auditor vs. iteration t, as measured by Deﬁnition 2.3. See text for details.

24

We begin by examining the evolution of the error and unfairness of Learner’s model. In the
left panel of Figure 1 we show the error of the model found by Learner vs. iteration for values of
γ ranging from 0 to 0.029. Several comments are in order.

First, after an initial period in which there is a fair amount of oscillatory behavior, by 6000
iterations most of the curves have largely ﬂattened out, and by 8,000 iterations it appears most
but not all have reached approximate convergence. Second, while the top-to-bottom ordering of
these error curves is approximately aligned with decreasing γ — so larger γ generally results in
lower error, as expected — there are many violations of this for small t, and even a few at large
t. Third, and as we will examine more closely shortly, the converged values at large t do indeed
exhibit a range of errors.

In the right panel of Figure 1, we show the corresponding unfairness γt of the subgroup found
by the Auditor at each iteration t for the same runs and values of the parameter γ (indicated by
horizontal dashed lines), with the same color-coding as for the left panel. Now the ordering is
generally reversed — larger values of γ generally lead to higher γt curves, since the fairness con-
straint on the Learner is weaker. We again see a great deal of early oscillatory behavior, with most
γt curves then eventually settling at or near their corresponding input γ value, as Learner and
Auditor engage in a back-and-forth struggle for lower error for Learner and γ-subgroup fairness
for Auditor.

(a)

(b)

Figure 2:
(a) Pareto-optimal error-unfairness values, color coded by varying values of the input
parameter γ. (b) Aggregate Pareto frontier across all values of γ. Here the γ values cover the same
range but are sampled more densely to get a smoother frontier. See text for details.

For any choice of the parameter γ, and each iteration t, the two panels of Figure 1 yield a pair
of realized values (cid:104)εt, γt
(cid:105) from the experiment, corresponding to a Learner model whose error is
εt, and for which the worst subgroup the Auditor was able to ﬁnd had unfairness γt. The set of all
(cid:105) pairs across all runs or γ values thus represents the diﬀerent trade-oﬀs between error and
(cid:104)εt, γt
unfairness found by our algorithm on the data. Most of these pairs are of course Pareto-dominated
by other pairs, so we are primarily interested in the undominated frontier.

In the left panel of Figure 2, for each value of γ we show the Pareto-optimal pairs, color-coded
for the value of γ. Each value of γ yields a set or cloud of undominated pairs that are usually fairly

25

close to each other, and as expected, as γ is increased, these clouds generally move leftwards and
upwards (lower error and higher unfairness).

We anticipate that the practical use of our algorithm would, as we have done, explore many
values of γ and then pick a model corresponding to a point on the aggregated Pareto frontier
across all γ, which represents the collection of all undominated models and the overall error-
unfairness trade-oﬀ. This aggregate frontier is shown in the right panel of Figure 2, and shows
a relatively smooth menu of options, ranging from error about 0.21 and no unfairness at one
extreme, to error about 0.12 and unfairness 0.025 at the other, and an appealing assortment of
intermediate trade-oﬀs. Of course, in a real application the selection of a particular point on
the frontier should be made in a domain-speciﬁc manner by the stakeholders or policymakers in
question.

Acknowledgements We thank Alekh Agarwal, Richard Berk, Miro Dud´ık, Akshay Krishna-
murthy, John Langford, Greg Ridgeway and Greg Yang for helpful discussions and suggestions.

References

Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, and John Langford. A reductions approach
to fair classiﬁcation. Fairness, Accountability, and Transparency in Machine Learning (FATML),
2017.

Julia Angwin and Hannes Grassegger. Facebooks secret censorship rules protect white men from

hate speech but not black children. Propublica, 2017.

Anna Maria Barry-Jester, Ben Casselman, and Dana Goldstein. The new science of sentencing.

The Marshall Project, August 8 2015. Retrieved 4/28/2016.

George W. Brown. Some notes on computation of games solutions, Jan 1949.

Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. arXiv preprint arXiv:1703.00056, 2017.

Constantinos Daskalakis and Qinxuan Pan. A counter-example to Karlin’s strong conjecture for
ﬁctitious play. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium
on, pages 11–20. IEEE, 2014.

Ilias Diakonikolas, Ryan O’Donnell, Rocco A. Servedio, and Yi Wu. Hardness results for agnosti-
cally learning low-degree polynomial threshold functions. In Proceedings of the Twenty-Second
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California,
USA, January 23-25, 2011, pages 1590–1606, 2011.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Confer-
ence, pages 214–226. ACM, 2012.

Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi Wu. Agnostic learning of

monomials by halfspaces is hard. SIAM J. Comput., 41(6):1558–1590, 2012.

26

Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings
of the Ninth Annual Conference on Computational Learning Theory, COLT 1996, Desenzano del
Garda, Italy, June 28-July 1, 1996., pages 325–332, 1996.

Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im) possibility

of fairness. arXiv preprint arXiv:1609.07236, 2016.

Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination
prevention in data mining. IEEE transactions on knowledge and data engineering, 25(7):1445–
1459, 2013.

Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning.

Advances in Neural Information Processing Systems, 2016.

´Ursula H´ebert-Johnson, Michael P Kim, Omer Reingold, and Guy N Rothblum. Calibration for

the (computationally-identiﬁable) masses. arXiv preprint arXiv:1711.08513, 2017.

Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
In Advances in Neural Information Processing Systems, pages

Classic and contextual bandits.
325–333, 2016.

Adam Tauman Kalai and Santosh Vempala. Eﬃcient algorithms for online decision problems. J.

Comput. Syst. Sci., 71(3):291–307, 2005.

Adam Tauman Kalai, Yishay Mansour, and Elad Verbin. On agnostic boosting and parity learn-
ing. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British
Columbia, Canada, May 17-20, 2008, pages 629–638, 2008.

Faisal Kamiran and Toon Calders. Data preprocessing techniques for classiﬁcation without dis-

crimination. Knowledge and Information Systems, 33(1):1–33, 2012.

Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. An Empirical Study of Rich
Subgroup Fairness for Machine Learning. ArXiv e-prints, art. arXiv:1808.08166, August 2018.

Michael J Kearns and Umesh Virkumar Vazirani. An Introduction to Computational Learning The-

ory. MIT press, 1994.

Michael J Kearns, Robert E Schapire, and Linda M Sellie. Toward eﬃcient agnostic learning.

Machine Learning, 17(2-3):115–141, 1994.

Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-oﬀs in the fair deter-
mination of risk scores. In Proceedings of the 2017 ACM Conference on Innovations in Theoretical
Computer Science, Berkeley, CA, USA, 2017, 2017.

James Rufus Koren. What does that web search say about your credit? Los Angeles Times, July 16

2016. Retrieved 9/15/2016.

1951.

Julia Robinson. An iterative method of solving a game. Annals of Mathematics, pages 10–2307,

27

Cynthia Rudin. Predictive policing using machine learning to detect patterns of crime. Wired

Magazine, August 2013. Retrieved 4/28/2016.

Maurice Sion. On general minimax theorems. Paciﬁc J. Math., 8(1):171–176, 1958.

Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-

discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.

Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate ex-
ample weighting. In Proceedings of the 3rd IEEE International Conference on Data Mining (ICDM
2003), 19-22 December 2003, Melbourne, Florida, USA, page 435, 2003.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate
mistreatment.
In Proceedings of the 26th International Conference on World Wide Web, pages
1171–1180. International World Wide Web Conferences Steering Committee, 2017.

Zhe Zhang and Daniel B Neill. Identifying signiﬁcant predictive bias in classiﬁers. arXiv preprint

arXiv:1611.08292, 2016.

A Chernoﬀ-Hoeﬀding Bound

We use the following concentration inequality.

Theorem A.1 (Real-vaued Additive Chernoﬀ-Hoeﬀding Bound). Let X1, X2, . . . , Xm be i.i.d. random
variables with E [Xi] = µ and a ≤ Xi

≤ b for all i. Then for every α > 0,
(cid:32) −2α2m
(cid:33)
(b − a)2

≤ 2 exp

i Xi
m

≥ α

− µ

(cid:80)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr

B Generalization Bounds

Proof of Theorems 2.11 and 2.12. We give a proof of Theorem 2.11. The proof of Theorem 2.12 is
identical, as false positive rates are just positive classiﬁcation rates on the subset of the data for
which y = 0.

Given a set of classiﬁers H and protected groups G, deﬁne the following function class:

FH,G = {fh,g (x) (cid:17) h(x) ∧ g(x) : h ∈ H, g ∈ G}

We can relate the VC-dimension of FH,G to the VC-dimension of H and G:
Claim B.1.

VCDIM(FH,G) ≤ ˜O(VCDIM(H) + VCDIM(G))
Proof. Let S be a set of size m shattered by FH,G. Let πFH,G (S) be the number of labelings of S
realized by elements of FH,G. By the deﬁnition of shattering, πFH,G (S) = 2m. Now for each labeling
of S by an element in FH,G, it is realized as (f ∧ g)(S) for some f ∈ F , g ∈ G. But (f ∧ g)(S) =
f (S)∧g(S), and so it can be realized as the conjunction of a labeling of S by an element of F and an

28

element of G. But since there are πF (S)πG(S) such pairs of labelings, this immediately implies that
πFH,G (S) ≤ πF (S)πG(S). Now by the Sauer-Shelah Lemma (see e.g. Kearns and Vazirani [1994]),
πF (S) = O(mVCDIM(H)), πG(S) = O(mVCDIM(G)). Thus πFH,G (S) = 2m ≤ O(mVCDIM(H)+VCDIM(G)), which
implies that m = ˜O(VCDIM(H) + VCDIM(G)), as desired.

This bound, together with a standard VC-Dimension based uniform convergence theorem (see

e.g. Kearns and Vazirani [1994]) implies that with probability 1 − δ, for every fh,g

∈ FH,G:

(cid:12)(cid:12)(cid:12)E
(X,y)∼P [fh,g (X)] − E

(X,y)∼P

S [fh,g (X)]

(cid:12)(cid:12)(cid:12) ≤ ˜O

(cid:114)





(VCDIM(H) + VCDIM(G)) log m + log(1/δ)
m

Note that the left hand side of the above inequality can be written as:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr
(X,y)∼P

[h(X) = 1|g(x) = 1] · Pr

(X,y)∼P

[g(x) = 1] − Pr
(X,y)∼P

[h(X) = 1|g(x) = 1] · Pr
(X,y)∼P

S

S

[g(x) = 1]





(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

This completes our proof.

C Missing Proofs in Section 4

Theorem 4.5. Let ( ˆD, ˆλ) be a ν-approximate minmax solution to the Λ-bounded Lagrangian problem
in the sense that

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν and, L( ˆD, ˆλ) ≥ max
λ∈Λ

L( ˆD, λ) − ν.

Then err( ˆD, P ) ≤ OPT +2ν and for any g ∈ G(S),

Proof of Theorem 4.5. Let D
problem. Since D

∗ is feasible, we know that L(D

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C
∗ be the optimal feasible solution for our constrained optimization
, P ).

, ˆλ) ≤ err(D

.

∗

∗

We will ﬁrst focus on the case where ˆD is not a feasible solution, that is

max
(g,•)∈G(S)×{±}

Φ•( ˆD, g) > 0

Let ( ˆg, ˆ•) ∈ argmax(g,•) Φ•( ˆD, g) and let λ
zero. By Lemma 4.8, we know that λ
minmax solution, we know that L( ˆD, ˆλ) ≥ L( ˆD, λ

(cid:48) ∈ argmaxλ∈Λ

(cid:48) ∈ Λ be a vector with (λ

(cid:48))ˆ•
ˆg = C and all other coordinates
L( ˆD, λ). By the deﬁnition of a ν-approximate

(cid:48)) − ν. This implies that

Note that L(D

, ˆλ) ≤ err(D

, P ), and so

∗

∗

L( ˆD, ˆλ) ≥ err( ˆD, P ) + C Φˆ•( ˆD, ˆg) − ν

L( ˆD, ˆλ) ≤ min
D∈∆H(S)

L(D, ˆλ) + ν ≤ L(D

, ˆλ) + ν

∗

29

(15)

(16)

Combining Equations (15) and (16), we get

err( ˆD, P ) + C Φˆ•( ˆD, ˆg) ≤ L( ˆD, ˆλ) + ν ≤ L(D

∗

, ˆλ) + 2ν ≤ err(D

, P ) + 2ν

∗

Note that C Φˆ•( ˆD, ˆg) ≥ 0, so we must have err( ˆD, P ) ≤ err(D
, P ) ∈ [0, 1], we know
since err( ˆD, P ), err(D

∗

∗

, P ) + 2ν = OPT +2ν. Furthermore,

which implies that maximum constraint violation satisﬁes Φˆ•( ˆD, ˆg) ≤ (1 + 2ν)/C. By applying
Claim 4.4, we get

C Φˆ•( ˆD, ˆg) ≤ 1 + 2ν,

αFP (g, P ) βFP (g, ˆD, P ) ≤ γ +

1 + 2ν
C

.

Now let us consider the case in which ˆD is a feasible solution for the optimization problem.

Then it follows that there is no constraint violation by ˆD and maxλ
∗

err( ˆD, P ) = max

L( ˆD, λ) ≤ L( ˆD, ˆλ) + ν ≤ min
D

λ

L(D, ˆλ) + 2ν ≤ L(D

, ˆλ) + 2ν ≤ err(D

, P ) + 2ν

∗

L( ˆD, λ) = err( ˆD, P ), and so

Therefore, the stated bounds hold for both cases.

Lemma 4.8. Fix any D ∈ ∆H(S) such that that maxg∈G(S)
(cid:48))
with one non-zero coordinate (λ

•(cid:48)
g (cid:48) = C, where

{Φ+(D, g), Φ−(D, g)} > 0. Let λ

(cid:48) ∈ Λ be vector

(cid:48)

, •(cid:48)

(g

) = argmax

(g,•)∈G(S)×{±}

{Φ•(D, g)}

Then L(D, λ

(cid:48)) ≥ maxλ∈Λ L(D, λ).

Proof of Lemma 4.8. Observe:

L(D, λ) = argmax

[err(h, P )] +

(cid:88)

(cid:16)
λ+

(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

argmax
λ∈Λ

λ∈Λ

= argmax
λ∈Λ

E
h∼D
(cid:88)

g∈G

g∈G(S)
(cid:17)
−
g Φ−(D, g)
g Φ+(D, g) + λ

(cid:16)
λ+

Note that this is a linear optimization problem over the non-negative orthant of a scaling of the (cid:96)1
ball, and so has a solution at a vertex, which corresponds to a single group g ∈ G(S). Thus, there
(cid:48))•
is always a best response λ
g that maximizes
Φ•(D, g).

(cid:48) that puts all the weight C on the coordinate (λ

Lemma 4.10. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm with η = n
, and λ1, . . . , λT be the
sequence of plays by the Auditor. Then

1√
nT

(1+C)

(cid:113)

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

(cid:105) − min
h∈H(S)

U (h, λt) ≤ 2n1/4(1 + C)

T

√

T(cid:88)

t=1

30

Proof of Lemma 4.10. To instantiate the regret bound in Theorem 2.9, we just need to provide a
bound on the maximum absoluate value over the coordinates of the loss vector (the quantity M in
Theorem 2.9). For any λ ∈ Λ, the absolute value of the i-th coordinate of LC(λ) is bounded by:

(λ+
g

−
g ) (Pr[g(x) = 1 | y = 0] − 1) 1[g(xi) = 1]
− λ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(Pr[g(x) = 1 | y = 0]1g(xi) = 1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
n

+

(cid:88)

1
n
g∈G(S)


(cid:88)

≤ 1
n

+

≤ 1
n

+

1
n

1
n




g∈G(S)


(cid:88)

g∈G(S)

(cid:12)(cid:12)(cid:12)λ+

g

−
− λ
g



(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)λ+

g

(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)λ

−
g

max
g∈G(S)


(cid:12)(cid:12)(cid:12)

≤ 1 + C
n

Also note that the dimension of the optimization is the size of the dataset n. This means if we set
η = n

, the regret of the learner will then be bounded by 2n1/4(1 + C)

T .

(cid:113)

√

(1+C)

1√
nT

Lemma 4.11. Fix any ξ, δ ∈ (0, 1) and any distribution D over H(S). Let h1, . . . , hm be m i.i.d. draws
from p, and ˆD be the empirical distribution over the realized sample. Then with probability at least 1 − δ
over the random draws of hj’s, the following holds,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

as long as m ≥ c0

C2(ln(1/δ)+d2 ln(n))
ξ2

for some absolute constant c0 and d2 = VCDIM(G).

Proof of Lemma 4.11. Recall that for any distribution D
deﬁned as

(cid:48) over H(S) the expected payoﬀ function is

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)] = E
h∼ ˆD

[err(h, P )] + E
h∼ ˆD

− E
h∼D

[err(h, P )] + E
h∼D

(cid:88)

(cid:16)
λ+

g Φ+(h, g) + λ


(cid:17)
−
g Φ−(h, g)


(cid:17)
−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:16)
λ+








g∈G(S)

(cid:88)

g∈G(S)



h∼D [err(h, P )] −

By the triangle inequality, it suﬃces to show that with probability (1 − δ), A = | E
E
h∼ ˆD [err(h, P )] | ≤ ξ/2 and for all λ ∈ Λ and g ∈ G(S),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

B =

E
h∼ ˆD





(cid:88)

g∈G(S)


(cid:17)
−
g Φ−(h, g)
g Φ+(h, g) + λ

(cid:16)
λ+



− E
h∼D





(cid:88)

g∈G(S)

(cid:16)
λ+

−
g Φ−(h, g)
g Φ+(h, g) + λ

≤ ξ/2


(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


The ﬁrst part follows directly from a simple application of the Chernoﬀ-Hoeﬀding bound

(Theorem A.1): with probability (1 − δ/2), A ≤ ξ/2, as long as m ≥ 2 ln(4/δ)/ξ2.
To bound the second part, we ﬁrst note that by H¨older’s inequality, we have

B ≤ (cid:107)λ(cid:107)

1 max
(g,•)∈G(S)×{±}

|Φ•(D, g) − Φ•( ˆD, g)|

31

Since for all λ ∈ Λ we have (cid:107)λ(cid:107)

≤ C, it suﬃces to show that with probability 1−δ/2, |Φ•(D, g)−

1

Φ•( ˆD, g)| ≤ ξ/(2C) holds for all • ∈ {−, +} and g ∈ G(S). Note that

We can rewrite the absolute value of ﬁrst term:

|Φ•(D, g) − Φ•( ˆD, g)| =

[FP(h)]

Pr[y = 0, g(x) = 1]

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[FP(h)] − E
h∼ ˆD

(cid:33)

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

E
h∼D

[FP(h)] − E
h∼ ˆD

(cid:33)
[FP(h)]

Pr[y = 0, g(x) = 1]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1 | y = 0]] − E
h∼ ˆD

E
h∼D

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0]]

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:33)
[Pr[h(X) = 1 | y = 0]]

Pr[g(x) = 1 | y = 0]

(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:32)

+

(cid:32)

(cid:32)

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=

≤

where the last inequality follows from Pr[g(x) = 1 | y = 0] ≤ 1.

Note that E

h∼ ˆD [Pr[h(X) = 1, y = 0, g(x) = 1]] = 1
m

average of m i.i.d. random variables with expectation E
Chernoﬀ-Hoeﬀding bound (Theorem A.1), we have

(cid:80)m

j=1 Pr[hj(X) = 1, y = 0, g(x) = 1], which is an
h∼D [Pr[h(X) = 1, y = 0, g(x) = 1]]. By the

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

Pr

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD
(cid:16)− ξ2m
8C2

[Pr[h(X) = 1, y = 0]]

>

≤ 2 exp

(17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

ξ
4C

(cid:33)

(cid:32)

− ξ2m
8C2

In the following, we will let δ0 = 2 exp

(cid:17)
. Similarly, we also have for each g ∈ G(S),

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Pr

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

>

(18)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

ξ
4C

≤ δ0

By taking the union bound over (17) and (18) over all choices of g ∈ G(S), we have with proba-

bility at least (1 − δ0(1 + |G(S)|)),

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1, y = 0]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0]]

(19)

and,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

E
h∼D

[Pr[h(X) = 1, y = 0, g(x) = 1]] − E
h∼ ˆD

[Pr[h(X) = 1, y = 0, g(x) = 1]]

for all g ∈ G(S).

(20)

(cid:17)
Note that by Sauer’s lemma (Lemma 4.3), |G(S)| ≤ O
. Thus, there exists an absolute constant
C2(ln(1/δ)+d2 ln(n))
c0 such that m ≥ c0
implies that failure probability above δ0(1 + |G(S)|) ≤ δ/2. We
ξ2
will assume m satisiﬁes such a bound, and so the events of (19) and (20) hold with probaility at
least (1−δ/2). Then by the triangle inequality we have for all (g, •) ∈ G(S)×{±}, |Φ•(D, g)−Φ•( ˆD, g)| ≤
ξ/(2C), which implies that B ≤ ξ/2. This completes the proof.

(cid:16)
nd2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ ξ
4C

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ ξ
4C

32

Claim C.1. Suppose there are two distributions D and ˆD over H(S) such that

Let

Then

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

max
λ∈Λ

E
h∼ ˆD

[U (h, λ)] − E
h∼D

[U (h, λ)]

≤ ξ.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ˆλ ∈ argmax

λ(cid:48)∈Λ

E
h∼ ˆD

(cid:2)U (h, λ

(cid:48)

)(cid:3)

max
λ

E
h∼D

[U (h, λ)] − ξ ≤ E
h∼D

(cid:105)
(cid:104)
U (h, ˆλ)

,

Lemma 4.12. Let T be the time horizon for the no-regret dynamics. Let D1, . . . , DT be the sequence of
distributions maintained by the Learner’s FTPL algorithm. For each Dt, let ˆDt be the empirical distri-
bution over m i.i.d. draws from Dt. Let λ1, . . . , λT be the Auditor’s best responses against ˆD1, . . . , ˆDT .
Then with probability 1 − δ,

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:114)

(cid:104)
U (h, λt)

(cid:105) ≤ T

c0C2(ln(T /δ) + d2 ln(n))
m

for some absolute constant c0 and d2 = VCDIM(G).

Proof. Let γ t

A be deﬁned as

By instantiating Lemma 4.11 and applying union bound across all T steps, we know with proba-
bility at least 1 − δ, the following holds for all t ∈ [T ]:

γ t
A = max
λ∈Λ

E
h∼ ˆDt

[U (h, λ)] − E
h∼Dt

[U (h, λ)]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:114)

≤

γ t
A

c0C2(ln(T /δ) + d2 ln(n))
m

where c0 is the absolute constant in Lemma 4.11 and d2 = VCDIM(G).

Note that by Claim C.1, the Auditor is performing a γ t
round t. Then we can bound the Auditor’s regret as follows:

A-approximate best response at each

γA =





1
T

max
λ∈Λ

T(cid:88)

t=1

E
h∼Dt

[U (h, λ)] −

T(cid:88)

t=1

E
h∼Dt

(cid:104)
U (h, λt)

max
λ∈Λ

E
h∼Dt

[U (h, λ)] − E
h∼Dt

(cid:104)
U (h, λt)

(cid:105)(cid:19)


(cid:105)



(cid:18)

T(cid:88)

≤ 1
T
t=1
≤ max
γ t
A
T

It follows that with probability 1 − δ, we have

which completes the proof.

(cid:114)

≤

γA

c0C2(ln(T /δ) + d2 ln(n))
m

33

