Adding vs. Averaging in Distributed Primal-Dual Optimization

5
1
0
2
 
l
u
J
 
3
 
 
]

G
L
.
s
c
[
 
 
2
v
8
0
5
3
0
.
2
0
5
1
:
v
i
X
r
a

Chenxin Ma∗
Industrial and Systems Engineering, Lehigh University, USA

Virginia Smith∗
University of California, Berkeley, USA

Martin Jaggi
ETH Z¨urich, Switzerland

Michael I. Jordan
University of California, Berkeley, USA

Peter Richt´arik
School of Mathematics, University of Edinburgh, UK

Martin Tak´aˇc
Industrial and Systems Engineering, Lehigh University, USA

∗Authors contributed equally.

CHM514@LEHIGH.EDU

VSMITH@BERKELEY.EDU

JAGGI@INF.ETHZ.CH

JORDAN@CS.BERKELEY.EDU

PETER.RICHTARIK@ED.AC.UK

TAKAC.MT@GMAIL.COM

Abstract

1. Introduction

Distributed optimization methods for large-scale
machine learning suffer from a communication
bottleneck. It is difﬁcult to reduce this bottleneck
while still efﬁciently and accurately aggregating
partial work from different machines. In this pa-
per, we present a novel generalization of the re-
cent communication-efﬁcient primal-dual frame-
work (COCOA) for distributed optimization. Our
framework, COCOA+, allows for additive com-
bination of local updates to the global parame-
ters at each iteration, whereas previous schemes
with convergence guarantees only allow conser-
vative averaging. We give stronger (primal-dual)
convergence rate guarantees for both COCOA as
well as our new variants, and generalize the the-
ory for both methods to cover non-smooth con-
vex loss functions. We provide an extensive ex-
perimental comparison that shows the markedly
improved performance of COCOA+ on several
real-world distributed datasets, especially when
scaling up the number of machines.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

With the wide availability of large datasets that exceed
the storage capacity of single machines, distributed opti-
mization methods for machine learning have become in-
creasingly important. Existing methods require signiﬁcant
communication between workers, frequently equaling the
amount of local computation (or reading of local data). As
a result, distributed machine learning suffers signiﬁcantly
from a communication bottleneck on real world systems,
where communication is typically several orders of magni-
tudes slower than reading data from main memory.

In this work we focus on optimization problems with em-
pirical loss minimization structure, i.e., objectives that are
a sum of the loss functions of each datapoint. This in-
cludes the most commonly used regularized variants of
linear regression and classiﬁcation methods.
For this
class of problems, the recently proposed COCOA approach
(Yang, 2013; Jaggi et al., 2014) develops a communication-
efﬁcient primal-dual scheme that targets the communica-
tion bottleneck, allowing more computation on data-local
subproblems native to each machine before communica-
tion. By appropriately choosing the amount of local com-
putation per round, this framework allows one to control
the trade-off between communication and local computa-
tion based on the systems hardware at hand.

However, the performance of COCOA (as well as related
primal SGD-based methods) is signiﬁcantly reduced by the

Adding vs. Averaging in Distributed Primal-Dual Optimization

need to average updates between all machines. As the
number of machines K grows, the updates get diluted and
slowed by 1/K, e.g., in the case where all machines ex-
cept one would have already reached the solutions of their
respective partial optimization tasks. On the other hand, if
the updates are instead added, the algorithms can diverge,
as we will observe in the practical experiments below.

To address both described issues, in this paper we develop
a novel generalization of the local COCOA subproblems
assigned to each worker, making the framework more pow-
erful in the following sense: Without extra computational
cost, the set of locally computed updates from the mod-
iﬁed subproblems (one from each machine) can be com-
bined more efﬁciently between machines. The proposed
COCOA+ updates can be aggressively added (hence the
‘+’-sufﬁx), which yields much faster convergence both in
practice and in theory. This difference is particularly sig-
niﬁcant as the number of machines K becomes large.

1.1. Contributions

Strong Scaling. To our knowledge, our framework is the
ﬁrst to exhibit favorable strong scaling for the class of prob-
lems considered, as the number of machines K increases
and the data size is kept ﬁxed. More precisely, while the
convergence rate of COCOA degrades as K is increased,
the stronger theoretical convergence rate here is – in the
worst case – independent of K. Our experiments in Section
7 conﬁrm the improved speed of convergence. Since the
number of communicated vectors is only one per round and
worker, this favorable scaling might be surprising. Indeed,
for existing methods, splitting data among more machines
generally increases communication requirements (Shamir
& Srebro, 2014), which can severely affect overall runtime.

Theoretical Analysis of Non-Smooth Losses. While the
existing analysis for COCOA in (Jaggi et al., 2014) only
covered smooth loss functions, here we extend the class
of functions where the rates apply, additionally covering,
e.g., Support Vector Machines and non-smooth regression
variants. We provide a primal-dual convergence rate for
both COCOA as well as our new method COCOA+ in the
case of general convex (L-Lipschitz) losses.

tight

Primal-Dual Convergence Rate. Furthermore, we addi-
tionally strengthen the rates by showing stronger primal-
dual convergence for both algorithmic frameworks, which
are almost
to their objective-only counterparts.
Primal-dual rates for COCOA had not previously been ana-
lyzed in the general convex case. Our primal-dual rates al-
low efﬁcient and practical certiﬁcates for the optimization
quality, e.g., for stopping criteria. The new rates apply to
both smooth and non-smooth losses, and for both COCOA
as well as the extended COCOA+.

Arbitrary Local Solvers. COCOA as well as COCOA+
allow the use of arbitrary local solvers on each machine.

Experimental Results. We provide a thorough experi-
mental comparison with competing algorithms using sev-
eral real-world distributed datasets. Our practical results
conﬁrm the strong scaling of COCOA+ as the number of
machines K grows, while competing methods, including
the original COCOA, slow down signiﬁcantly with larger
K. We implement all algorithms in Spark, and our code is
publicly available at: github.com/gingsmith/cocoa.

1.2. History and Related Work

While optimal algorithms for the serial (single machine)
case are already well researched and understood, the liter-
ature in the distributed setting is relatively sparse. In par-
ticular, details on optimal trade-offs between computation
and communication, as well as optimization or statistical
accuracy, are still widely unclear. For an overview over
this currently active research ﬁeld, we refer the reader to
(Balcan et al., 2012; Richt´arik & Tak´aˇc, 2013; Duchi et al.,
2013; Yang, 2013; Liu & Wright, 2014; Fercoq et al., 2014;
Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al.,
2014; Zhang & Lin, 2015; Qu & Richt´arik, 2014) and the
references therein. We provide a detailed comparison of
our proposed framework to the related work in Section 6.

2. Setup

We consider regularized empirical loss minimization prob-
lems of the following well-established form:
λ
2

P(w) :=

min
w∈Rd

i w) +

(cid:96)i(xT

(cid:107)w(cid:107)2

n
(cid:88)

1
n

(1)

(cid:40)

(cid:41)

i=1

i=1 ⊂ Rd represent the training data
Here the vectors {xi}n
examples, and the (cid:96)i(.) are arbitrary convex real-valued
loss functions (e.g., hinge loss), possibly depending on la-
bel information for the i-th datapoints. The constant λ > 0
is the regularization parameter.

The above class includes many standard problems of wide
interest in machine learning, statistics, and signal process-
ing, including support vector machines, regularized linear
and logistic regression, ordinal regression, and others.

Dual Problem, and Primal-Dual Certiﬁcates. The con-
jugate dual of (1) takes following form:

(cid:40)

max
α∈Rn

D(α) := −

(cid:96)∗
j (−αj) −

1
n

n
(cid:88)

j=1

2 (cid:41)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Aα
λn

(cid:13)
(cid:13)
(cid:13)
(cid:13)

λ
2

(2)

Here the data matrix A = [x1, x2, . . . , xn] ∈ Rd×n col-
lects all data-points as its columns, and (cid:96)∗
j is the conjugate
function to (cid:96)j. See, e.g., (Shalev-Shwartz & Zhang, 2013c)
for several concrete applications.

Adding vs. Averaging in Distributed Primal-Dual Optimization

It is possible to assign for any dual vector α ∈ Rn a corre-
sponding primal feasible point

w(α) = 1

λn Aα

The duality gap function is then given by:

G(α) := P(w(α)) − D(α)

(3)

(4)

By weak duality, every value D(α) at a dual candidate α
provides a lower bound on every primal value P(w). The
duality gap is therefore a certiﬁcate on the approxima-
tion quality: The distance to the unknown true optimum
P(w∗) must always lie within the duality gap, i.e., G(α) =
P(w) − D(α) ≥ P(w) − P(w∗) ≥ 0.

In large-scale machine learning settings like those consid-
ered here, the availability of such a computable measure of
approximation quality is a signiﬁcant beneﬁt during train-
ing time. Practitioners using classical primal-only methods
such as SGD have no means by which to accurately detect
if a model has been well trained, as P (w∗) is unknown.

Classes of Loss-Functions. To simplify presentation, we
assume that all loss functions (cid:96)i are non-negative, and
(cid:96)i(0) ≤ 1

(5)

∀i

Deﬁnition 1 (L-Lipschitz continuous loss). A function (cid:96)i :
R → R is L-Lipschitz continuous if ∀a, b ∈ R, we have

|(cid:96)i(a) − (cid:96)i(b)| ≤ L|a − b|
(6)
Deﬁnition 2 ((1/µ)-smooth loss). A function (cid:96)i : R → R
is (1/µ)-smooth if it is differentiable and its derivative is
(1/µ)-Lipschitz continuous, i.e., ∀a, b ∈ R, we have

|(cid:96)(cid:48)

i(a) − (cid:96)(cid:48)

i(b)| ≤

|a − b|

(7)

1
µ

3. The COCOA+ Algorithm Framework
In this section we present our novel COCOA+ frame-
work. COCOA+ inherits the many beneﬁts of CoCoA as
it remains a highly ﬂexible and scalable, communication-
efﬁcient framework for distributed optimization. COCOA+
differs algorithmically in that we modify the form of the lo-
cal subproblems (9) to allow for more aggressive additive
updates (as controlled by γ). We will see that these changes
allow for stronger convergence guarantees as well as im-
proved empirical performance. Proofs of all statements in
this section are given in the supplementary material.

Data Partitioning. We write {Pk}K
k=1 for the given par-
tition of the datapoints [n] := {1, 2, . . . , n} over the K
worker machines. We denote the size of each part by
nk = |Pk|. For any k ∈ [K] and α ∈ Rn we use the
notation α[k] ∈ Rn for the vector

(α[k])i :=

(cid:40)

0,
if i /∈ Pk,
αi, otherwise.

Local Subproblems in COCOA+. We can deﬁne a data-
local subproblem of the original dual optimization problem
(2), which can be solved on machine k and only requires
accessing data which is already available locally, i.e., data-
points with i ∈ Pk. More formally, each machine k is as-
signed the following local subproblem, depending only on
the previous shared primal vector w ∈ Rd, and the change
in the local dual variables αi with i ∈ Pk:

max
∆α[k]∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k])

(8)

where

1
n

(cid:88)

i∈Pk
λ
2

Gσ(cid:48)
k (∆α[k]; w, α[k]) := −

(cid:96)∗
i (−αi − (∆α[k])i)

−

1
K

λ
2

1
n

(cid:107)w(cid:107)2 −

wT A∆α[k] −

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

1
λn

A∆α[k]

(9)

(cid:13)
2
(cid:13)
(cid:13)

Interpretation. The above deﬁnition of the local objec-
tive functions Gσ(cid:48)
k are such that they closely approximate
the global dual objective D, as we vary the ‘local’ vari-
able ∆α[k], in the following precise sense:
Lemma 3. For any dual α, ∆α ∈ Rn, primal w = w(α)
and real values γ, σ(cid:48) satisfying (11), it holds that

(cid:16)

D

α + γ

K
(cid:88)

k=1

(cid:17)
∆α[k]
K
(cid:88)

+γ

≥ (1 − γ)D(α)

Gσ(cid:48)
k (∆α[k]; w, α[k])

(10)

k=1
The role of the parameter σ(cid:48) is to measure the difﬁculty of
the given data partition. For our purposes, we will see that
it must be chosen not smaller than

σ(cid:48) ≥ σ(cid:48)

min := γ max
α∈Rn

(cid:107)Aα(cid:107)2
k=1 (cid:107)Aα[k](cid:107)2

(cid:80)K

(11)

In the following lemma, we show that this parameter can
be upper-bounded by γK, which is trivial to calculate for
all values γ ∈ R. We show experimentally (Section 7) that
this safe upper bound for σ(cid:48) has a minimal effect on the
overall performance of the algorithm. Our main theorems
below show convergence rates dependent on γ ∈ [ 1
K , 1],
which we refer to as the aggregation parameter.
Lemma 4. The choice of σ(cid:48) := γK is valid for (11), i.e.,

γK ≥ σ(cid:48)

min

Notion of Approximation Quality of the Local Solver.
Assumption 1 (Θ-approximate solution). We assume that
there exists Θ ∈ [0, 1) such that ∀k ∈ [K], the local solver
at any outer iteration t produces a (possibly) randomized
approximate solution ∆α[k], which satisﬁes
[k]; w, α[k]) − Gσ(cid:48)

(12)

E(cid:2)Gσ(cid:48)
k (∆α∗
(cid:16)
Gσ(cid:48)
k (∆α∗

≤ Θ

k (∆α[k]; w, α[k])(cid:3)
(cid:17)

k (0; w, α[k])

,

[k]; w, α[k]) − Gσ(cid:48)

Adding vs. Averaging in Distributed Primal-Dual Optimization

where
∆α∗

[k] ∈ arg max
∆α∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k]) ∀k ∈ [K] (13)

We are now ready to describe the COCOA+ framework,
shown in Algorithm 1. The crucial difference compared
to the existing COCOA algorithm (Jaggi et al., 2014) is the
more general local subproblem, as deﬁned in (9), as well as
the aggregation parameter γ. These modiﬁcations allow the
option of directly adding updates to the global vector w.

Algorithm 1 COCOA+ Framework
1: Input: Datapoints A distributed according to parti-
tion {Pk}K
k=1. Aggregation parameter γ ∈ (0, 1],
subproblem parameter σ(cid:48) for the local subproblems
Gσ(cid:48)
k (∆α[k]; w, α[k]) for each k ∈ [K].
Starting point α(0) := 0 ∈ Rn, w(0) := 0 ∈ Rd.

2: for t = 0, 1, 2, . . . do
3:

for k ∈ {1, 2, . . . , K} in parallel over computers
do

call the local solver, computing a Θ-approximate
solution ∆α[k] of the local subproblem (9)
update α(t+1)
[k]
return ∆wk := 1

:= α(t)
λn A∆α[k]

[k] + γ ∆α[k]

end for
reduce w(t+1) := w(t) + γ (cid:80)K

k=1 ∆wk.

(14)

4:

5:

6:
7:
8:

9: end for

4. Convergence Guarantees

Before being able to state our main convergence results,
we introduce some useful quantities and the following
main lemma characterizing the effect of iterations of Al-
gorithm 1, for any chosen internal local solver.
Lemma 5. Let (cid:96)∗
i be strongly1 convex with convexity pa-
rameter µ ≥ 0 with respect to the norm (cid:107)·(cid:107), ∀i ∈ [n]. Then
for all iterations t of Algorithm 1 under Assumption 1, and
any s ∈ [0, 1], it holds that

E[D(α(t+1)) − D(α(t))] ≥

γ(1 − Θ)

sG(α(t)) −

(cid:16)

(15)
R(t)(cid:17)

,

σ(cid:48)
2λ

(cid:0) s
n

(cid:1)2

where

R(t) := − λµn(1−s)
+ (cid:80)K

(cid:107)u(t) − α(t)(cid:107)2
k=1(cid:107)A(u(t) − α(t))[k](cid:107)2,

σ(cid:48)s

(16)

for u(t) ∈ Rn with

− u(t)

i ∈ ∂(cid:96)i(w(α(t))T xi).

(17)

1Note that the case of weakly convex (cid:96)∗

i (.) is explicitly al-

lowed here as well, as the Lemma holds for the case µ = 0.

The following Lemma provides a uniform bound on R(t):
Lemma 6. If (cid:96)i are L-Lipschitz continuous for all i ∈ [n],
then

∀t : R(t) ≤ 4L2

σknk

,

K
(cid:88)

where

k=1
(cid:124)

(cid:125)

(cid:123)(cid:122)
=:σ
(cid:107)Aα[k](cid:107)2
(cid:107)α[k](cid:107)2 .

σk := max
α[k]∈Rn

(18)

(19)

Remark 7. If all data-points xi are normalized such that
(cid:107)xi(cid:107) ≤ 1 ∀i ∈ [n], then σk ≤ |Pk| = nk. Furthermore,
if we assume that the data partition is balanced, i.e., that
nk = n/K for all k, then σ ≤ n2/K. This can be used to
bound the constants R(t), above, as R(t) ≤ 4L2n2
K .

4.1. Primal-Dual Convergence for General Convex

Losses

The following theorem shows the convergence for non-
smooth loss functions, in terms of objective values as well
as primal-dual gap. The analysis in (Jaggi et al., 2014) only
covered the case of smooth loss functions.
Theorem 8. Consider Algorithm 1 with Assumption 1. Let
(cid:96)i(·) be L-Lipschitz continuous, and (cid:15)G > 0 be the de-
sired duality gap (and hence an upper-bound on primal
sub-optimality). Then after T iterations, where

(cid:108)
T ≥ T0 + max{

1
γ(1 − Θ)

(cid:109)
,

T0 ≥ t0 +

(cid:18)

2
γ(1 − Θ)
(cid:108)

4L2σσ(cid:48)
λn2(cid:15)Gγ(1 − Θ)
(cid:19)(cid:19)

(cid:18) 8L2σσ(cid:48)
λn2(cid:15)G

− 1

,

+

t0 ≥ max(0,

1

γ(1−Θ) log( 2λn2(D(α∗)−D(α(0)))

4L2σσ(cid:48)

(cid:109)
)

),

},

(20)

we have that the expected duality gap satisﬁes

E[P(w(α)) − D(α)] ≤ (cid:15)G,

at the averaged iterate

α := 1

T −T0

(cid:80)T −1

t=T0+1α(t).

(21)

The following corollary of the above theorem clariﬁes our
main result: The more aggressive adding of the partial up-
dates, as compared averaging, offers a very signiﬁcant im-
provement in terms of total iterations needed. While the
convergence in the ‘adding’ case becomes independent of
the number of machines K, the ‘averaging’ regime shows
the known degradation of the rate with growing K, which is
a major drawback of the original COCOA algorithm. This
important difference in the convergence speed is not a the-
oretical artifact but also conﬁrmed in our practical experi-
ments below for different K, as shown e.g. in Figure 2.

We further demonstrate below that by choosing γ and σ(cid:48)
accordingly, we can still recover the original COCOA al-
gorithm and its rate.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Corollary 9. Assume that all datapoints xi are bounded as
(cid:107)xi(cid:107) ≤ 1 and that the data partition is balanced, i.e. that
nk = n/K for all k. We consider two different possible
choices of the aggregation parameter γ:

• (COCOA Averaging, γ := 1

K ): In this case, σ(cid:48) := 1
is a valid choice which satisﬁes (11). Then using σ ≤
n2/K in light of Remark 7, we have that T iterations
are sufﬁcient for primal-dual accuracy (cid:15)G, with

T ≥ T0 + max{

T0 ≥ t0 +

(cid:18) 2K
1 − Θ
t0 ≥ max(0, (cid:6) K

(cid:108) K

(cid:109)
,

4L2
λ(cid:15)G(1 − Θ)
(cid:19)(cid:19)

},

− 1

,

+

1 − Θ
(cid:18) 8L2
λK(cid:15)G

1−Θ log( 2λ(D(α∗)−D(α(0)))

4KL2

)(cid:7))

Hence the more machines K, the more iterations are
needed (in the worst case).

• (COCOA+ Adding, γ := 1): In this case, the choice of
σ(cid:48) := K satisﬁes (11). Then using σ ≤ n2/K in light
of Remark 7, we have that T iterations are sufﬁcient
for primal-dual accuracy (cid:15)G, with

(cid:108)

1
1 − Θ
(cid:18) 8L2
λ(cid:15)G

T ≥ T0 + max{

(cid:18) 2

T0 ≥ t0 +

1 − Θ
t0 ≥ max(0, (cid:6) 1

(cid:109)
,

4L2
λ(cid:15)G(1 − Θ)

},

(cid:19)(cid:19)

− 1

,

+

1−Θ log( 2λn(D(α∗)−D(α(0)))

4KL2

)(cid:7))

This is signiﬁcantly better than the averaging case.

In practice, we usually have σ (cid:28) n2/K, and hence the
actual convergence rate can be much better than the proven
worst-case bound. Table 1 shows that the actual value of
σ is typically between one and two orders of magnitudes
smaller compared to our used upper-bound n2/K.

Table 1. The ratio of upper-bound n2
the parameter σ, for some real datasets.

K divided by the true value of

K

16

32

64

128

256

512

news
real-sim
rcv1

15.483
42.127
40.138

14.933
36.898
23.827

14.278
30.780
28.204

13.390
23.814
21.792

12.074
16.965
16.339

10.252
11.835
11.099

K

256

512

1024

2048

4096

8192

covtype

17.277

17.260

17.239

16.948

17.238

12.729

4.2. Primal-Dual Convergence for Smooth Losses

The following theorem shows the convergence for smooth
losses, in terms of the objective as well as primal-dual gap.

Theorem 10. Assume the loss functions functions (cid:96)i are
(1/µ)-smooth ∀i ∈ [n]. We deﬁne σmax = maxk∈[K] σk.
Then after T iterations of Algorithm 1, with

T ≥

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

log 1
(cid:15)D

,

it holds that

E[D(α∗) − D(α(T ))] ≤ (cid:15)D.

Furthermore, after T iterations with

T ≥

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

log

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

1
(cid:15)G

(cid:16)

(cid:17)

,

we have the expected duality gap

E[P(w(α(T ))) − D(α(T ))] ≤ (cid:15)G.

The following corollary is analogous to Corollary 9, but
for the case of smooth loses. It again shows that while the
COCOA variant degrades with the increase of the number
of machines K, the COCOA+ rate is independent of K.
Corollary 11. Assume that all datapoints xi are bounded
as (cid:107)xi(cid:107) ≤ 1 and that the data partition is balanced, i.e.,
that nk = n/K for all k. We again consider the same two
different possible choices of the aggregation parameter γ:
K ): In this case, σ(cid:48) :=
1 is a valid choice which satisﬁes (11). Then using
σmax ≤ nk = n/K in light of Remark 7, we have that
T iterations are sufﬁcient for suboptimality (cid:15)D, with

• (COCOA Averaging, γ := 1

T ≥ 1

1−Θ

λµK+1
λµ

log 1
(cid:15)D

Hence the more machines K, the more iterations are
needed (in the worst case).

• (COCOA+ Adding, γ := 1): In this case, the choice
of σ(cid:48) := K satisﬁes (11). Then using σmax ≤ nk =
n/K in light of Remark 7, we have that T iterations
are sufﬁcient for suboptimality (cid:15)D, with

T ≥ 1

1−Θ

λµ+1

λµ log 1
(cid:15)D

This is signiﬁcantly better than the averaging case.
Both rates hold analogously for the duality gap.

4.3. Comparison with Original COCOA

Remark 12. If we choose averaging (γ := 1
K ) for aggre-
gating the updates, together with σ(cid:48) := 1, then the result-
ing Algorithm 1 is identical to COCOA analyzed in (Jaggi
et al., 2014). However, they only provide convergence for
smooth loss functions (cid:96)i and have guarantees for dual sub-
optimality and not the duality gap. Formally, when σ(cid:48) = 1,
the subproblems (9) will differ from the original dual D(.)
only by an additive constant, which does not affect the local
optimization algorithms used within COCOA.

5. SDCA as an Example Local Solver

We have shown convergence rates for Algorithm 1, depend-
ing solely on the approximation quality Θ of the used local

Adding vs. Averaging in Distributed Primal-Dual Optimization

solver (Assumption 1). Any chosen local solver in each
round receives the local α variables as an input, as well as
a shared vector w (3)= w(α) being compatible with the last
state of all global α ∈ Rn variables.

As an illustrative example for a local solver, Algorithm 2
below summarizes randomized coordinate ascent (SDCA)
applied on the local subproblem (9). The following two
Theorems (13, 14) characterize the local convergence for
both smooth and non-smooth functions. In all the results
we will use rmax := maxi∈[n] (cid:107)xi(cid:107)2.

Algorithm 2 LOCALSDCA (w, α[k], k, H)
1: Input: α[k], w = w(α)
2: Data: Local {(xi, yi)}i∈Pk
3: Initialize: ∆α(0)
[k] := 0 ∈ Rn
4: for h = 0, 1, . . . , H − 1 do
5:

choose i ∈ Pk uniformly at random
Gσ(cid:48)
k (∆α(h)
δ∗
i := arg max

6:

[k] + δiei; w, α[k])

δi∈R
:= ∆α(h)

[k] + δ∗

i ei

7: ∆α(h+1)
[k]
8: end for
9: Output: ∆α(H)
[k]

Theorem 13. Assume the functions (cid:96)i are (1/µ)−smooth
for i ∈ [n]. Then Assumption 1 on the local approximation
quality Θ is satisﬁed for LOCALSDCA as given in Algo-
rithm 2, if we choose the number of inner iterations H as

H ≥ nk

σ(cid:48)rmax + λnµ
λnµ
Theorem 14. Assume the functions (cid:96)i are L-Lipschitz for
i ∈ [n]. Then Assumption 1 on the local approximation
quality Θ is satisﬁed for LOCALSDCA as given in Algo-
rithm 2, if we choose the number of inner iterations H as

1
Θ

(22)

log

.

H ≥ nk

(cid:18) 1 − Θ
Θ

+

σ(cid:48)rmax
2Θλn2

(cid:107)∆α∗
[k](cid:107)2
[k]; .) − Gσ(cid:48)

k (0; .)

(cid:19)

.

Gσ(cid:48)
k (∆α∗

(23)
Remark 15. Between the different regimes allowed in
COCOA+ (ranging between averaging and adding the up-
dates) the computational cost for obtaining the required
local approximation quality varies with the choice of σ(cid:48).
From the above worst-case upper bound, we note that the
cost can increase with σ(cid:48), as aggregation becomes more
aggressive. However, as we will see in the practical exper-
iments in Section 7 below, the additional cost is negligible
compared to the gain in speed from the different aggrega-
tion, when measured on real datasets.

6. Discussion and Related Work

SGD-based Algorithms. For the empirical loss mini-
mization problems of interest here, stochastic subgradient

descent (SGD) based methods are well-established. Sev-
eral distributed variants of SGD have been proposed, many
of which build on the idea of a parameter server (Niu et al.,
2011; Liu et al., 2014; Duchi et al., 2013). The downside of
this approach, even when carefully implemented, is that the
amount of required communication is equal to the amount
of data read locally (e.g., mini-batch SGD with a batch size
of 1 per worker). These variants are in practice not compet-
itive with the more communication-efﬁcient methods con-
sidered here, which allow more local updates per round.

One-Shot Communication Schemes. At the other ex-
treme, there are distributed methods using only a single
round of communication, such as (Zhang et al., 2013;
Zinkevich et al., 2010; Mann et al., 2009; McWilliams
et al., 2014). These require additional assumptions on the
partitioning of the data, and furthermore can not guarantee
convergence to the optimum solution for all regularizers, as
shown in, e.g., (Shamir et al., 2014). (Balcan et al., 2012)
shows additional relevant lower bounds on the minimum
number of communication rounds necessary for a given ap-
proximation quality for similar machine learning problems.

Mini-Batch Methods. Mini-batch methods are more
ﬂexible and lie within these two communication vs. com-
putation extremes. However, mini-batch versions of both
SGD and coordinate descent (CD) (Richt´arik & Tak´aˇc,
2013; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu
& Richt´arik, 2014; Qu et al., 2014) suffer from their con-
vergence rate degrading towards the rate of batch gradient
descent as the size of the mini-batch is increased. This fol-
lows because mini-batch updates are made based on the
outdated previous parameter vector w, in contrast to meth-
ods that allow immediate local updates like COCOA. Fur-
thermore, the aggregation parameter for mini-batch meth-
ods is harder to tune, as it can lie anywhere in the order of
mini-batch size. In the COCOA setting, the parameter lies
in the smaller range given by K. Our COCOA+ extension
avoids needing to tune this parameter entirely, by adding.

Methods Allowing Local Optimization. Developing
methods that allow for local optimization requires care-
fully devising data-local subproblems to be solved after
each communication round. (Shamir et al., 2014; Zhang
& Lin, 2015) have proposed distributed Newton-type algo-
rithms in this spirit. However, the subproblems must be
solved to high accuracy for convergence to hold, which is
often prohibitive as the size of the data on one machine is
still relatively large. In contrast, the COCOA framework
(Jaggi et al., 2014) allows using any local solver of weak
local approximation quality in each round. By making use
of the primal-dual structure in the line of work of (Yu et al.,
2012; Pechyony et al., 2011; Yang, 2013; Lee & Roth,
2015), the COCOA and COCOA+ frameworks also allow
more control over the aggregation of updates between ma-

Adding vs. Averaging in Distributed Primal-Dual Optimization

Figure 1. Duality gap vs. the number of communicated vectors, as well as duality gap vs. elapsed time in seconds for two datasets:
Covertype (left, K=4) and RCV1 (right, K=8). Both are shown on a log-log scale, and for three different values of regularization
(λ=1e-4; 1e-5; 1e-6). Each plot contains a comparison of COCOA (red) and COCOA+ (blue), for three different values of H, the
number of local iterations performed per round. For all plots, across all values of λ and H, we see that COCOA+ converges to the
optimal solution faster than COCOA, in terms of both the number of communications and the elapsed time.

chines. The practical variant DisDCA-p proposed in (Yang,
2013) allows additive updates but is restricted to SDCA
updates, and was proposed without convergence guaran-
tees. DisDCA-p can be recovered as a special case of the
COCOA+ framework when using SDCA as a local solver,
if nk = n/K and σ(cid:48) := K, see Appendix C. The theory
presented here also therefore covers that method.

ADMM. An alternative approach to distributed optimiza-
tion is to use the alternating direction method of multipli-
ers (ADMM), as used for distributed SVM training in, e.g.,
(Forero et al., 2010). This uses a penalty parameter balanc-
ing between the equality constraint w and the optimization
objective (Boyd et al., 2011). However, the known conver-
gence rates for ADMM are weaker than the more problem-
tailored methods mentioned previously, and the choice of
the penalty parameter is often unclear.

Batch Proximal Methods.
In spirit, for the special case
of adding (γ = 1), COCOA+ resembles a batch proximal
method, using the separable approximation (9) instead of
the original dual (2). Known batch proximal methods re-
quire high accuracy subproblem solutions, and don’t allow
arbitrary solvers of weak accuracy Θ such as we do here.

7. Numerical Experiments

We present experiments on several large real-world dis-
tributed datasets. We show that COCOA+ converges faster
in terms of total rounds as well as elapsed time as compared
to COCOA in all cases, despite varying: the dataset, values
of regularization, batch size, and cluster size (Section 7.2).
In Section 7.3 we demonstrate that this performance trans-
lates to orders of magnitude improvement in convergence
when scaling up the number of machines K, as compared
to COCOA as well as to several other state-of-the-art meth-
ods. Finally, in Section 7.4 we investigate the impact of the
local subproblem parameter σ(cid:48) in the COCOA+ framework.

Table 2. Datasets for Numerical Experiments.
n
522,911
400,000
677,399

Dataset
covertype
epsilon
RCV1

Sparsity
22.22%
100%
0.16%

d
54
2,000
47,236

7.1. Implementation Details

We implement all algorithms in Apache Spark (Zaharia
et al., 2012) and run them on m3.large Amazon EC2 in-
stances, applying each method to the binary hinge-loss sup-

Adding vs. Averaging in Distributed Primal-Dual Optimization

port vector machine. The analysis for this non-smooth loss
was not covered in (Jaggi et al., 2014) but has been captured
here, and thus is both theoretically and practically justiﬁed.
The used datasets are summarized in Table 2.

For illustration and ease of comparison, we here use SDCA
(Shalev-Shwartz & Zhang, 2013c) as the local solver for
both COCOA and COCOA+. Note that in this special case,
and if additionally σ(cid:48) := K, and if the partitioning nk =
n/K is balanced, once can show that the COCOA+ frame-
work reduces to the practical variant of DisDCA (Yang,
2013) (which had no convergence guarantees so far). We
include more details on the connection in Appendix C.

7.2. Comparison of COCOA+ and COCOA
We compare the COCOA+ and COCOA frameworks di-
rectly using two datasets (Covertype and RCV1) across var-
ious values of λ, the regularizer, in Figure 1. For each value
of λ we consider both methods with different values of H,
the number of local iterations performed before communi-
cating to the master. For all runs of COCOA+ we use the
safe upper bound of γK for σ(cid:48). In terms of both the to-
tal number of communications made and the elapsed time,
COCOA+ (shown in blue) converges to the optimal solu-
tion faster than COCOA (red). The discrepancy is larger
for greater values of λ, where the strongly convex regular-
izer has more of an impact and the problem difﬁculty is re-
duced. We also see a greater performance gap for smaller
values of H, where there is frequent communication be-
tween the machines and the master, and changes between
the algorithms therefore play a larger role.

7.3. Scaling the Number of Machines K
In Figure 2 we demonstrate the ability of COCOA+ to
scale with an increasing number of machines K. The
experiments conﬁrm the ability of strong scaling of the
new method, as predicted by our theory in Section 4,
in contrast to the competing methods. Unlike COCOA,
which becomes linearly slower when increasing the num-
ber of machines, the performance of COCOA+ improves
with additional machines, only starting to degrade slightly
once K=16 for the RCV1 dataset.

7.4. Impact of the Subproblem Parameter σ(cid:48)

Finally, in Figure 3, we consider the effect of the choice
of the subproblem parameter σ(cid:48) on convergence. We plot
both the number of communications and clock time on a
log-log scale for the RCV1 dataset with K=8 and H=1e4.
For γ = 1 (the most aggressive variant of COCOA+ in
which updates are added) we consider several different val-
ues of σ(cid:48), ranging from 1 to 8. The value σ(cid:48)=8 represents
the safe upper bound of γK. The optimal convergence oc-
curs around σ(cid:48)=4, and diverges for σ(cid:48) ≤ 2. Notably, we

Figure 2. The effect of increasing K on the time (s) to reach an
(cid:15)D-accurate solution. We see that COCOA+ converges twice as
fast as COCOA on 100 machines for the Epsilon dataset, and
nearly 7 times as quickly for the RCV1 dataset. Mini-batch SGD
converges an order of magnitude more slowly than both methods.

see that the easy to calculate upper bound of σ(cid:48) := γK (as
given by Lemma 4) has only slightly worse performance
than best possible subproblem parameter in our setting.

Figure 3. The effect of σ(cid:48) on convergence of COCOA+ for the
RCV1 dataset distributed across K=8 machines. Decreasing σ(cid:48)
improves performance in terms of communication and overall run
time until a certain point, after which the algorithm diverges. The
“safe” upper bound of σ(cid:48):=K=8 has only slightly worse perfor-
mance than the practically best “un-safe” value of σ(cid:48).

8. Conclusion
In conclusion, we present a novel framework COCOA+
that allows for fast and communication-efﬁcient additive
aggregation in distributed algorithms for primal-dual opti-
mization. We analyze the theoretical performance of this
method, giving strong primal-dual convergence rates with
outer iterations scaling independently of the number of ma-
chines. We extended our theory to allow for non-smooth
losses. Our experimental results show signiﬁcant speedups
over previous methods,
including the original COCOA
framework as well as other state-of-the-art methods.

Acknowledgments. We thank Ching-pei Lee and an
anonymous reviewer for several helpful insights and com-
ments.

Adding vs. Averaging in Distributed Primal-Dual Optimization

References

Balcan, M.-F., Blum, A., Fine, S., and Mansour, Y. Dis-
tributed Learning, Communication Complexity and Pri-
vacy. In COLT, pp. 26.1–26.22, 2012.

Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J.
Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3(1):1–122, 2011.

Duchi, J. C., Jordan, M. I., and McMahan, H. B. Estima-
tion, Optimization, and Parallelism when Data is Sparse.
In NIPS, 2013.

Fercoq, O. and Richt´arik, P. Accelerated, parallel and prox-

imal coordinate descent. arXiv:1312.5799, 2013.

Fercoq, O., Qu, Z., Richt´arik, P., and Tak´aˇc, M. Fast
distributed coordinate descent for non-strongly convex
losses. IEEE Workshop on Machine Learning for Signal
Processing, 2014.

Forero, P. A., Cano, A., and Giannakis, G. B. Consensus-
Based Distributed Support Vector Machines. JMLR, 11:
1663–1707, 2010.

Jaggi, M., Smith, V., Tak´aˇc, M., Terhorst, J., Krishnan, S.,
Hofmann, T., and Jordan, M. I. Communication-efﬁcient
distributed dual coordinate ascent. In NIPS, 2014.

Lee, C.-P. and Roth, D. Distributed Box-Constrained
Quadratic Optimization for Dual Linear SVM. In ICML,
2015.

Liu, J. and Wright, S. J. Asynchronous stochastic coor-
dinate descent: Parallelism and convergence properties.
arXiv:1403.3862, 2014.

Liu, J., Wright, S. J., R´e, C., Bittorf, V., and Sridhar,
S. An Asynchronous Parallel Stochastic Coordinate De-
scent Algorithm. In ICML, 2014.

Lu, Z. and Xiao, L. On the complexity analysis of random-
ized block-coordinate descent methods. arXiv preprint
arXiv:1305.4723, 2013.

Mann, G., McDonald, R., Mohri, M., Silberman, N., and
Walker, D. D. Efﬁcient Large-Scale Distributed Training
of Conditional Maximum Entropy Models. NIPS, 2009.

Mareˇcek, J., Richt´arik, P., and Tak´aˇc, M. Distributed block
coordinate descent for minimizing partially separable
functions. arXiv:1406.0238, 2014.

McWilliams, B., Heinze, C., Meinshausen, N., Krumme-
nacher, G., and Vanchinathan, H. P. LOCO: Distribut-
ing Ridge Regression with Random Projections. arXiv
stat.ML, June 2014.

Niu, F., Recht, B., R´e, C., and Wright, S. J. Hogwild!: A
Lock-Free Approach to Parallelizing Stochastic Gradient
Descent. In NIPS, 2011.

Pechyony, D., Shen, L., and Jones, R. Solving Large Scale
In
Linear SVM with Distributed Block Minimization.
NIPS Workshop on Big Learning, 2011.

Qu, Z. and Richt´arik, P.

Coordinate descent with
arbitrary sampling I: Algorithms and complexity.
arXiv:1412.8060, 2014.

Qu, Z., Richt´arik, P., and Zhang, T. Randomized dual coor-
dinate ascent with arbitrary sampling. arXiv:1411.5873,
2014.

Richt´arik, P. and Tak´aˇc, M. Distributed coordinate de-
scent method for learning with big data. arXiv preprint
arXiv:1310.2059, 2013.

Richt´arik, P. and Tak´aˇc, M.

Iteration complexity of ran-
domized block-coordinate descent methods for minimiz-
ing a composite function. Mathematical Programming,
144(1-2):1–38, April 2014.

Richt´arik, P. and Tak´aˇc, M. Parallel coordinate descent
methods for big data optimization. Mathematical Pro-
gramming, pp. 1–52, 2015.

Shalev-Shwartz, S. and Zhang, T. Accelerated mini-batch

stochastic dual coordinate ascent. In NIPS, 2013a.

Shalev-Shwartz, S. and Zhang, T. Accelerated proximal
stochastic dual coordinate ascent for regularized loss
minimization. arXiv:1309.2375, 2013b.

Shalev-Shwartz, S. and Zhang, T. Stochastic Dual Coor-
dinate Ascent Methods for Regularized Loss Minimiza-
tion. JMLR, 14:567–599, 2013c.

Shamir, O. and Srebro, N. Distributed Stochastic Optimiza-

tion and Learning . In Allerton, 2014.

Shamir, O., Srebro, N., and Zhang, T. Communication
efﬁcient distributed optimization using an approximate
newton-type method. In ICML, 2014.

Tappenden, R., Tak´aˇc, M., and Richt´arik, P. On the com-
plexity of parallel coordinate descent. Technical report,
2015. ERGO 15-001, University of Edinburgh.

Yang, T. Trading Computation for Communication: Dis-
In NIPS,

tributed Stochastic Dual Coordinate Ascent.
2013.

Yang, T., Zhu, S., Jin, R., and Lin, Y. On Theoretical Anal-
ysis of Distributed Stochastic Dual Coordinate Ascent.
arXiv:1312.1031, 2013.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Yu, H.-F., Hsieh, C.-J., Chang, K.-W., and Lin, C.-J. Large
Linear Classiﬁcation When Data Cannot Fit in Memory.
TKDD, 5(4):1–23, 2012.

Zaharia, M., Chowdhury, M., Das, T., Dave, A., McCauley,
M., Franklin, M. J., Shenker, S., and Stoica, I. Resilient
Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing. In NSDI, 2012.

Zhang, Y. and Lin, X. DiSCO: Distributed Optimization for
Self-Concordant Empirical Loss. In ICML, pp. 362–370,
2015.

Zhang, Y., Duchi,

J.
Communication-Efﬁcient Algorithms for Statistical Op-
timization. JMLR, 14:3321–3363, 2013.

and Wainwright, M.

J. C.,

Zinkevich, M. A., Weimer, M., Smola, A. J., and Li, L.
Parallelized Stochastic Gradient Descent. NIPS, 2010.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Appendix

A. Technical Lemmas

Lemma 16 (Lemma 21 in (Shalev-Shwartz & Zhang, 2013c)). Let (cid:96)i : R → R be an L-Lipschitz continuous. Then for
any real value a with |a| > L we have that (cid:96)∗

i (a) = ∞.

Lemma 17. Assuming the loss functions (cid:96)i are bounded by (cid:96)i(0) ≤ 1 for all i ∈ [n] (as we have assumed in (5) above),
then for the zero vector α(0) := 0 ∈ Rn, we have

D(α∗) − D(α(0)) = D(α∗) − D(0) ≤ 1.

(24)

Proof. For α := 0 ∈ Rn, we have w(α) = 1
in (2),

λn Aα = 0 ∈ Rd. Therefore, by deﬁnition of the dual objective D given

0 ≤ D(α∗) − D(α) ≤ P(w(α)) − D(α) = 0 − D(α)

(5),(2)

≤ 1.

B. Proofs

B.1. Proof of Lemma 3

Indeed, we have

D(α + γ

∆α[k]) = −

(cid:96)∗
i (−αi − γ(

∆α[k])i)

−

A(α + γ

∆α[k])

(25)

K
(cid:88)

k=1

1
n

n
(cid:88)

i=1

(cid:124)

K
(cid:88)

k=1

1
λn

λ
2

(cid:13)
(cid:13)
(cid:13)

(cid:124)

(cid:125)

K
(cid:88)

k=1

(cid:123)(cid:122)
B

(cid:13)
2
(cid:13)
(cid:13)

.

(cid:125)

Now, let us bound the terms A and B separately. We have

A = −

(cid:96)∗
i (−αi − γ(∆α[k])i)

= −

(cid:96)∗
i (−(1 − γ)αi − γ(α + ∆α[k])i)

(cid:33)

1
n

1
n

(cid:32)

K
(cid:88)

(cid:88)

k=1

K
(cid:88)

(cid:32)

i∈Pk

(cid:88)

k=1

i∈Pk

≥ −

(1 − γ)(cid:96)∗

i (−αi) + γ(cid:96)∗

i (−(α + ∆α[k])i)

.

(cid:123)(cid:122)
A

(cid:33)

1
n

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk
(cid:33)

Where the last inequality is due to Jensen’s inequality. Now we will bound B, using the safe separability measurement σ(cid:48)
as deﬁned in (11).

B =

A(α + γ

(cid:13)
(cid:13)
(cid:13)

1
λn

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∆α[k])
(cid:13)

=

(cid:13)
(cid:13)
(cid:13)w(α) + γ

1
λn

K
(cid:88)

k=1

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

= (cid:107)w(α)(cid:107)2 +

w(α)T A∆α[k] + γ

(cid:16) 1
λn

(cid:17)2

(cid:13)
(cid:13)
(cid:13)

γ

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

(11)
≤ (cid:107)w(α)(cid:107)2 +

w(α)T A∆α[k] + γ

(cid:107)A∆α[k](cid:107)2.

(cid:17)2

σ(cid:48)

(cid:16) 1
λn

K
(cid:88)

k=1

2γ

1
λn

K
(cid:88)

k=1

2γ

1
λn

K
(cid:88)

k=1

K
(cid:88)

k=1

− γ

(cid:107)w(α)(cid:107)2 − (1 − γ)

(cid:107)w(α)(cid:107)2 −

w(α)T A∆α[k] −

λ
2

γ

(cid:16) 1
λn

(cid:17)2

σ(cid:48)

K
(cid:88)

k=1

(cid:107)A∆α[k](cid:107)2

Adding vs. Averaging in Distributed Primal-Dual Optimization

Plugging A and B into (25) will give us

D(α + γ

∆α[k]) ≥ −

(1 − γ)(cid:96)∗

i (−αi) + γ(cid:96)∗

i (−(α + ∆α[k])i)

(cid:33)

K
(cid:88)

k=1

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk

1
n

λ
2

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk

= −

1
n

(cid:124)

+ γ

(cid:32)

K
(cid:88)

k=1

−

1
n

(cid:88)

i∈Pk

λ
2

(cid:33)

λ
2

K
(cid:88)

k=1

2γ

1
λn

(1 − γ)(cid:96)∗

i (−αi)

− (1 − γ)

(cid:107)w(α)(cid:107)2

(cid:123)(cid:122)
(1−γ)D(α)

(cid:125)

λ
2

1
K

λ
2

(9)=(1 − γ)D(α) + γ

Gσ(cid:48)
k (∆α[k]; w, α[k]).

K
(cid:88)

k=1

(cid:96)∗
i (−(α + ∆α[k])i) −

(cid:107)w(α)(cid:107)2 −

w(α)T A∆α[k] −

1
n

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α[k]

2(cid:33)
(cid:13)
(cid:13)
(cid:13)

B.2. Proof of Lemma 4

See (Richt´arik & Tak´aˇc, 2013).

B.3. Proof of Lemma 5

For sake of notation, we will write α instead of α(t), w instead of w(α(t)) and u instead of u(t).

Now, let us estimate the expected change of the dual objective. Using the deﬁnition of the dual update α(t+1) := α(t) +
γ (cid:80)

k ∆α[k] resulting in Algorithm 1, we have

(cid:104)
E(cid:2)D(α(t)) − D(α(t+1))(cid:3) = E

D(α) − D(α + γ

(by Lemma 3 on the local function Gσ(cid:48)

k (α; w, α[k]) approximating the global objective D(α))

(cid:104)
≤ E

D(α) − (1 − γ)D(α) − γ

Gσ(cid:48)
k (∆α(t)

(cid:105)
[k]; w, α[k])

(cid:104)
= γE
D(α) −

Gσ(cid:48)
k (∆α(t)

[k]; w, α[k])

(cid:105)

(cid:105)
∆α[k])

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:104)
= γE
D(α) −

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) +

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) −

(by the notion of quality (12) of the local solver, as in Assumption 1)

(cid:18)

≤ γ

D(α) −

K
(cid:88)

k=1

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) + Θ

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) −

Gσ(cid:48)
k (0; w, α[k])

K
(cid:88)

k=1

(cid:16) K
(cid:88)

k=1

Gσ(cid:48)
k (∆α(t)

(cid:105)
[k]; w, α[k])

K
(cid:88)

k=1

K
(cid:88)

k=1
(cid:124)

(cid:123)(cid:122)
D(α)

(cid:125)

(cid:17)(cid:19)

(26)

= γ(1 − Θ)

D(α) −

(cid:16)

(cid:124)

K
(cid:88)

k=1

Gσ(cid:48)
k (∆α∗

[k]; w, α[k])

(cid:17)

.

(cid:123)(cid:122)
C

(cid:125)

Adding vs. Averaging in Distributed Primal-Dual Optimization

Now, let us upper bound the C term (we will denote by ∆α∗ = (cid:80)K

k=1 ∆α∗

[k]):

((cid:96)∗

i (−αi − ∆α∗

i ) − (cid:96)∗

i (−αi)) +

w(α)T A∆α∗ +

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α∗
[k]

(cid:13)
2
(cid:13)
(cid:13)

((cid:96)∗

i (−αi − s(ui − αi)) − (cid:96)∗

i (−αi)) +

w(α)T As(u − α) +

(2),(9)
=

C

1
n

n
(cid:88)

i=1

n
(cid:88)

≤

1
n

i=1
Strong conv.
≤

1
n

1
n

µ
2

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

(cid:17)

+

1
n

s(cid:96)∗

i (−ui) + (1 − s)(cid:96)∗

i (−αi) −

(1 − s)s(ui − αi)2 − (cid:96)∗

i (−αi)

w(α)T As(u − α)

n
(cid:88)

(cid:16)

1
n

+

i=1

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

=

1
n

n
(cid:88)

(cid:16)

i=1

s(cid:96)∗

i (−ui) − s(cid:96)∗

i (−αi) −

(1 − s)s(ui − αi)2(cid:17)

+

µ
2

1
n

w(α)T As(u − α) +

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

.

The convex conjugate maximal property implies that

i (−ui) = −uiw(α)T xi − (cid:96)i(w(α)T xi).
(cid:96)∗

Moreover, from the deﬁnition of the primal and dual optimization problems (1), (2), we can write the duality gap as

G(α) := P(w(α)) − D(α)

(cid:0)(cid:96)i(xT

j w) + (cid:96)∗

i (−αi) + w(α)T xiαi

(cid:1) .

(1),(2)
=

1
n

n
(cid:88)

i=1


−suiw(α)T xi − s(cid:96)i(w(α)T xi) − s(cid:96)∗

i (−αi) −sw(α)T xiαi + sw(α)T xiαi
(cid:125)

(cid:124)

(cid:123)(cid:122)
0

−

(1 − s)s(ui − αi)2

Hence,

(27)
≤

C

1
n

n
(cid:88)

i=1

=

1
n

n
(cid:88)

i=1

1
n

1
n

+

w(α)T As(u − α) +

As(u − α)[k]

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

(cid:1) +

1
n

n
(cid:88)

(cid:16)

i=1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+

w(α)T As(u − α) +

As(u − α)[k]

(28)= −sG(α) −

(1 − s)s

(cid:107)u − α(cid:107)2 +

(cid:107)A(u − α)[k](cid:107)2.

µ
2

1
n

n
(cid:88)

i=1

σ(cid:48)
2λ

(

s
n

)2

K
(cid:88)

k=1

Now, the claimed improvement bound (15) follows by plugging (29) into (26).

µ
2

µ
2

(cid:0)−s(cid:96)i(w(α)T xi) − s(cid:96)∗

i (−αi) − sw(α)T xiαi

sw(α)T xi(αi − ui) −

(1 − s)s(ui − αi)2(cid:17)

(27)

(28)





(29)

B.4. Proof of Lemma 6

For general convex functions, the strong convexity parameter is µ = 0, and hence the deﬁnition of R(t) becomes

R(t) (16)=

(cid:107)A(u(t) − α(t))[k](cid:107)2

σk(cid:107)(u(t) − α(t))[k](cid:107)2 Lemma 16

≤

σk|Pk|4L2.

K
(cid:88)

k=1

(19)
≤

K
(cid:88)

k=1

K
(cid:88)

k=1

Adding vs. Averaging in Distributed Primal-Dual Optimization

B.5. Proof of Theorem 8

At ﬁrst let us estimate expected change of dual feasibility. By using the main Lemma 5, we have

E[D(α∗) − D(α(t+1))] = E[D(α∗) − D(α(t+1)) + D(α(t)) − D(α(t))]

(15)= D(α∗) − D(α(t)) − γ(1 − Θ)sG(α(t)) + γ(1 − Θ) σ(cid:48)
2λ (

)2R(t)

s
n

(4)= D(α∗) − D(α(t)) − γ(1 − Θ)s(P(w(α(t))) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

)2R(t)

≤ D(α∗) − D(α(t)) − γ(1 − Θ)s(D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

(18)
≤ (1 − γ(1 − Θ)s) (D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ.

s
n

s
n
)2R(t)

s
n

Using (30) recursively we have

E[D(α∗) − D(α(t))] = (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ

(1 − γ(1 − Θ)s)j

s
n

s
n

t−1
(cid:88)

j=0
1 − (1 − γ(1 − Θ)s)t
γ(1 − Θ)s

= (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ

≤ (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + s

4L2σσ(cid:48)
2λn2 .

Choice of s = 1 and t = t0 := max{0, (cid:100)

E[D(α∗) − D(α(t))] ≤ (1 − γ(1 − Θ))t0 (D(α∗) − D(α(0))) +

1

γ(1−Θ) log(2λn2(D(α∗) − D(α(0)))/(4L2σσ(cid:48)))(cid:101)} will lead to
4L2σσ(cid:48)
2λn2 ≤

4L2σσ(cid:48)
2λn2 =

4L2σσ(cid:48)
2λn2 +

4L2σσ(cid:48)
λn2

.

Now, we are going to show that

∀t ≥ t0 : E[D(α∗) − D(α(t))] ≤

λn2(1 + 1

4L2σσ(cid:48)
2 γ(1 − Θ)(t − t0))

.

Clearly, (32) implies that (33) holds for t = t0. Now imagine that it holds for any t ≥ t0 then we show that it also has to
hold for t + 1. Indeed, using

1

s =

1 + 1

2 γ(1 − Θ)(t − t0)

∈ [0, 1]

we obtain

E[D(α∗) − D(α(t+1))]

(30)
≤ (1 − γ(1 − Θ)s) (D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

(30)

(31)

(32)

(33)

(34)

)24L2σ

s
n
+ γ(1 − Θ) σ(cid:48)
2λ (

s
n

)24L2σ

(33)
≤ (1 − γ(1 − Θ)s)

(cid:18) 1 + 1

(cid:18) 1 + 1

(34)=

4L2σσ(cid:48)
λn2

=

4L2σσ(cid:48)
λn2

(cid:124)

λn2(1 + 1

4L2σσ(cid:48)
2 γ(1 − Θ)(t − t0))
2 γ(1 − Θ)(t − t0) − γ(1 − Θ) + γ(1 − Θ) 1
2 γ(1 − Θ)(t − t0))2
(cid:19)
2 γ(1 − Θ)

(1 + 1
2 γ(1 − Θ)(t − t0) − 1
(1 + 1

2 γ(1 − Θ)(t − t0))2

2

.

(cid:19)

(cid:125)

(cid:123)(cid:122)
D

(1 + 1

2 γ(1 − Θ)(t + 1 − t0))(1 + 1
(1 + 1

2 γ(1 − Θ)(t − t0))2

2 γ(1 − Θ)(t − 1 − t0))

(cid:123)(cid:122)
≤1

(cid:125)

Now, we will upperbound D as follows

D =

≤

1 + 1

2 γ(1 − Θ)(t + 1 − t0)

1 + 1

2 γ(1 − Θ)(t + 1 − t0)

(cid:124)

,

1

1

Adding vs. Averaging in Distributed Primal-Dual Optimization

where in the last inequality we have used the fact that geometric mean is less or equal to arithmetic mean.

If α is deﬁned as (21) then we obtain that

E[G(α)] = E

G

1
T −T0

α(t)

≤ 1

T −T0

E

(cid:34)

(cid:32) T −1
(cid:88)

t=T0

(cid:33)(cid:35)

(cid:35)

(cid:16)

α(t)(cid:17)

G

(cid:34) T −1
(cid:88)

t=T0

(15),(18)
≤

1
T −T0

E

(cid:34) T −1
(cid:88)

(cid:18)

1
γ(1 − Θ)s

(D(α(t+1)) − D(α(t))) + 4L2σσ(cid:48)s

2λn2

(cid:19)(cid:35)

=

≤

1
γ(1 − Θ)s
1
γ(1 − Θ)s

t=T0
1
T − T0
1
T − T0

(cid:104)
D(α(T )) − D(α(T0))

(cid:105)

E

+ 4L2σσ(cid:48)s
2λn2

(cid:105)
(cid:104)
D(α∗) − D(α(T0))

E

+ 4L2σσ(cid:48)s
2λn2

.

Now, if T ≥ (cid:100)

1

γ(1−Θ) (cid:101) + T0 such that T0 ≥ t0 we obtain
(cid:18)

E[G(α)]

(35),(33)
≤

1
γ(1 − Θ)s
(cid:18)

1
T − T0
1
γ(1 − Θ)s

4L2σσ(cid:48)
λn2

=

λn2(1 + 1
1
T − T0

1 + 1

4L2σσ(cid:48)

1

2 γ(1 − Θ)(T0 − t0))

2 γ(1 − Θ)(T0 − t0)

4L2σσ(cid:48)s
2λn2

(cid:19)

+

(cid:19)

s
2

+

.

s =

1
(T − T0)γ(1 − Θ)

∈ [0, 1]

E[G(α)]

(36),(37)
≤

(cid:18)

4L2σσ(cid:48)
λn2

1 + 1

2 γ(1 − Θ)(T0 − t0)

+

1
(T − T0)γ(1 − Θ)

1
2

(cid:19)

.

To have right hand side of (38) smaller then (cid:15)G it is sufﬁcient to choose T0 and T such that
(cid:18)

(cid:19)

1

1

Choosing

gives us

Hence of if

4L2σσ(cid:48)
λn2

1 + 1
(cid:18)

2 γ(1 − Θ)(T0 − t0)
1
1
2
(T − T0)γ(1 − Θ)

(cid:19)

4L2σσ(cid:48)
λn2

≤

≤

1
2

1
2

(cid:15)G,

(cid:15)G.

t0 +

2
γ(1 − Θ)

(cid:19)

− 1

(cid:18) 8L2σσ(cid:48)
λn2(cid:15)G
4L2σσ(cid:48)
λn2(cid:15)Gγ(1 − Θ)

≤ T0,

≤ T,

T0 +

then (39) and (40) are satisﬁed.

B.6. Proof of Theorem 10

If the function (cid:96)i(.) is (1/µ)-smooth then (cid:96)∗

i (.) is µ-strongly convex with respect to the (cid:107) · (cid:107) norm. From (16) we have

(cid:88)K

k=1

(cid:88)K

R(t) (16)= − λµn(1−s)

(cid:107)u(t) − α(t)(cid:107)2 +

σ(cid:48)s

(cid:107)A(u(t) − α(t))[k](cid:107)2

(19)
≤ − λµn(1−s)

σ(cid:48)s

(cid:107)u(t) − α(t)(cid:107)2 +

≤ − λµn(1−s)

(cid:16)

σ(cid:48)s
− λµn(1−s)

=

(cid:107)u(t) − α(t)(cid:107)2 + σmax
(cid:17)

σ(cid:48)s + σmax

(cid:107)u(t) − α(t)(cid:107)2.

σk(cid:107)u(t) − α(t)

[k](cid:107)2
(cid:107)u(t) − α(t)

k=1
(cid:88)K

[k](cid:107)2

k=1

(35)

(36)

(37)

(38)

(39)

(40)

(41)

Adding vs. Averaging in Distributed Primal-Dual Optimization

If we plug

λµn

s =

λµn + σmaxσ(cid:48) ∈ [0, 1]

(42)

into (41) we obtain that ∀t : R(t) ≤ 0. Putting the same s into (15) will give us

E[D(α(t+1)) − D(α(t))]

≥ γ(1 − Θ)

(15),(42)

λµn

λµn + σmaxσ(cid:48) G(α(t)) ≥ γ(1 − Θ)

λµn + σmaxσ(cid:48) D(α∗) − D(α(t)).

(43)

λµn

Using the fact that E[D(α(t+1)) − D(α(t))] = E[D(α(t+1)) − D(α∗)] + D(α∗) − D(α(t)) we have

E[D(α(t+1)) − D(α∗)] + D(α∗) − D(α(t))

(43)
≥ γ(1 − Θ)

λµn

λµn + σmaxσ(cid:48) D(α∗) − D(α(t))

which is equivalent with

E[D(α∗) − D(α(t+1))] ≤

1 − γ(1 − Θ)

D(α∗) − D(α(t)).

(44)

(cid:18)

(cid:19)

λµn
λµn + σmaxσ(cid:48)

Therefore if we denote by (cid:15)(t)

D = D(α∗) − D(α(t)) we have that

(cid:18)

E[(cid:15)(t)
D ]

(44)
≤

1 − γ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)t

(cid:18)

(cid:15)(0)
D

(24)
≤

1 − γ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)t

(cid:18)

≤ exp

−tγ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)

.

The right hand side will be smaller than some (cid:15)D if

t ≥

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

log

1
(cid:15)D

.

Moreover, to bound the duality gap, we have

γ(1 − Θ)

λµn

λµn + σmaxσ(cid:48) G(α(t))

(43)
≤ E[D(α(t+1)) − D(α(t))] ≤ E[D(α∗) − D(α(t))].

Therefore G(α(t)) ≤

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

(cid:15)(t)
D . Hence if (cid:15)D ≤ γ(1 − Θ)

λµn

λµn+σmaxσ(cid:48) (cid:15)G then G(α(t)) ≤ (cid:15)G. Therefore after

t ≥

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

(cid:18)

log

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

1
(cid:15)G

(cid:19)

iterations we have obtained a duality gap less than (cid:15)G.

B.7. Proof of Theorem 13

Because (cid:96)i are (1/µ)-smooth then functions (cid:96)∗
i are µ strongly convex with respect to the norm (cid:107) · (cid:107). The proof is based on
techniques developed in recent coordinate descent papers, including (Richt´arik & Tak´aˇc, 2014; 2013; Richt´arik & Tak´aˇc,
2015; Tappenden et al., 2015; Mareˇcek et al., 2014; Fercoq & Richt´arik, 2013; Lu & Xiao, 2013; Fercoq et al., 2014;
Qu & Richt´arik, 2014; Qu et al., 2014) (Efﬁcient accelerated variants were considered in (Fercoq & Richt´arik, 2013;
Shalev-Shwartz & Zhang, 2013a)).
First, let us deﬁne the function F (ζ) : Rnk → R as F (ζ) := −Gσ(cid:48)
ζiei; w, α[k]). This function can be written in
two parts F (ζ) = Φ(ζ)+f (ζ). The ﬁrst part denoted by Φ(ζ) = 1
(cid:96)∗
i (−αi −ζi) is strongly convex with convexity
n
parameter µ
n with respect to the standard Euclidean norm. In our application, we think of the ζ variable collecting the local
dual variables ∆α[k].

k ((cid:80)
(cid:80)

i∈Pk

i∈Pk

The second part we will denote by f (ζ) = 1
λ
K
to show that the gradient of f is coordinate-wise Lipschitz continuous with Lipschitz constant σ(cid:48)
standard Euclidean norm.

2 (cid:107)w(α)(cid:107)2 + 1

w(α)T xiζi + λ

2 σ(cid:48)

i∈Pk

n

1

(cid:80)

λ2n2 (cid:107) (cid:80)

i∈Pk

xiζi(cid:107)2. It is easy
λn2 rmax with respect to the

Adding vs. Averaging in Distributed Primal-Dual Optimization

Following the proof of Theorem 20 in (Richt´arik & Tak´aˇc, 2015), we obtain that

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h+1)

[k]

; w, α[k])] ≤

1 −

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

(cid:17)
[k] ; w, α[k])

(cid:32)

(cid:18)

=

1 −

(cid:16)

(cid:33)

1 + µnλ
σ(cid:48)rmax
µnλ
σ(cid:48)rmax
λnµ
σ(cid:48)rmax + λnµ

1
nk

1
nk

(cid:19) (cid:16)

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])

(cid:17)

.

Over all steps up to step h, this gives

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])] ≤

(cid:18)

1 −

1
nk

λnµ
σ(cid:48)rmax + λnµ

(cid:19)h (cid:16)

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (0; w, α[k])

(cid:17)

.

Therefore, choosing H as in the assumption of our Theorem, given in Equation (22), we are guaranteed that
(cid:16)

(cid:17)H

1 − 1
nk

λnµ
σ(cid:48)rmax+λnµ

≤ Θ, as desired.

B.8. Proof of Theorem 14

Similarly as in the proof of Theorem 13 we deﬁne a composite function F (ζ) = f (ζ) + Φ(ζ). However, in this case func-
tions (cid:96)∗
i are not guaranteed to be strongly convex. However, the ﬁrst part has still a coordinate-wise Lipschitz continuous
gradient with constant σ(cid:48)
λn2 rmax with respect to the standard Euclidean norm. Therefore from Theorem 3 in (Tappenden
et al., 2015) we have that

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])] ≤

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (0; w, α[k]) +

(cid:18)

nk
nk + h

1
2

σ(cid:48)rmax
λn2 (cid:107)∆α∗

[k](cid:107)2

(cid:19)

.

(45)

Now, choice of h = H from (23) is sufﬁcient to have the right hand side of (45) to be ≤ Θ(cid:0)Gσ(cid:48)
k (0; w, α[k])(cid:1).
Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) −

C. Relationship of DisDCA to COCOA+

We are indebted to Ching-pei Lee for showing the following relationship between the practical variant of DisDCA (Yang,
2013), and COCOA+ when SDCA is chosen as the local solver:

Considering the practical variant of DisDCA (DisDCA-p, see Figure 2 in (Yang, 2013)) using the scaling parameter scl =
K, the following holds:
K . If within the COCOA+
Lemma 18. Assume that the dataset is partitioned equally between workers, i.e. ∀k : nk = n
framework, SDCA is used as a local solver, and the subproblems are formulated using our shown “safe” (but pessimistic)
upper bound of σ(cid:48) = K, with aggregation parameter γ = 1 (adding), then the COCOA+ framework reduces exactly to the
DisDCA-p algorithm.

Proof. (Due to Ching-pei Lee, with some reformulations). As deﬁned in (9), the data-local subproblem solved by each
machine in COCOA+ is deﬁned as

max
∆α[k]∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k])

where

Gσ(cid:48)
k (∆α[k]; w, α[k]) := −

(cid:96)∗
i (−αi − (∆α[k])i) −

(cid:107)w(cid:107)2 −

wT A∆α[k] −

1
K

λ
2

1
n

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

.

1
n

(cid:88)

i∈Pk

We rewrite the local problem by scaling with n, and removing the constant regularizer term 1
K

λ

2 (cid:107)w(cid:107)2, i.e.

˜Gσ(cid:48)
k (∆α[k]; w) := −

i (−αj − (∆α[k])j) − wT A∆α[k] −
(cid:96)∗

σ(cid:48)
2λn

(cid:13)
(cid:13)
(cid:13)A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

.

(46)

(cid:88)

j∈Pk

Adding vs. Averaging in Distributed Primal-Dual Optimization

For the correspondence of interest, we now restrict to single coordinate updates in the local solver. In other words, the
local solver optimizes exactly one coordinate i ∈ Pk at a time. To relate the single coordinate update to the set of local
variables, we will use the notation

∆α[k] =: ∆αprev

[k] + δei ,

so that ∆αprev

[k] are the previous local variables, and ∆α[k] will be the updated ones.

From now on, we will consider the special case of COCOA+ when the quadratic upper bound parameter is chosen as the
“safe” value σ(cid:48) = K, combined with adding as the aggregation, i.e. γ = 1.
Now if the local solver within COCOA+ is chosen as LOCALSDCA, then one local step on the subproblem (9) will
calculate the following coordinate update. Recall that A = [x1, x2, . . . , xn] ∈ Rd×n.

δ(cid:63) := arg max

˜Gσ(cid:48)
k (∆α[k]; w)

δ∈R

which – because it is only affecting one single coordinate, employing (47) – can be expressed as

δ(cid:63) := arg max

−(cid:96)∗

i (−(αi + (∆αprev

[k] )i + δ)) − δxT

i w −

δxT

i A∆αprev

[k] −

δ2(cid:107)xi(cid:107)2
2

−(cid:96)∗

i (−(αi + (∆αprev

[k] )i + δ)) − δxT

i

δ2(cid:107)xi(cid:107)2
2

(49)

δ∈R

= arg max
δ∈R

K
λn

(cid:16)

w +

(cid:124)

K
λn

A∆αprev
[k]
(cid:123)(cid:122)
(cid:125)
=:ulocal

(cid:17)

−

K
2λn
K
2λn

From this formulation, it is apparent that single coordinate local solvers should maintain their locally updated version of
the current primal parameters, which we here denote as

(47)

(48)

(50)

ulocal = w +

K
λn

A∆αprev
[k] .

In the practical variant of DisDCA, the summarized local primal updates are ∆ulocal = 1
λnk
nk = n/K for K being the number of machines, this means the local ulocal update of DisDCA-p is

A∆α[k]. For the balanced case

∆α(cid:63)

i := arg max
∆αi∈R

−(cid:96)∗

i (−(αi + ∆αi)) − ∆αixT

i ulocal −

(∆αi)2(cid:107)xi(cid:107)2
2 .

(51)

K
2λn

It is not hard to show that during one outer round, the evolution of the local dual variables ∆α[k] is the same in both
methods, such that they will also have the same trajectory of ulocal. This requires some care if the same coordinate is
sampled more than once in a round, which can happen in LOCALSDCA within COCOA+ and also in DisDCA-p.

Discussion.

In the view of the above lemma, we will summarize the connection of the two methods as follows:

• COCOA/+ is Not an Algorithm. In contrast, it is a framework which allows to use any local solver to perform
approximate steps on the local subproblem. This additional level of abstraction (from the deﬁnition of such local
subproblems in (9)) is the ﬁrst to allow reusability of any fast/tuned and problem speciﬁc single machine solvers,
while decoupling this from the distributed algorithmic scheme, as presented in Algorithm 1.

Concerning the choice of local solver to be used within COCOA/+, SDCA is not the fastest known single machine
solver for most applications. Much recent research has shown improvements on SDCA (Shalev-Shwartz & Zhang,
2013c), such as accelerated variants (Shalev-Shwartz & Zhang, 2013b) and other approaches including variance re-
duction, methods incorporating second-order information, and importance sampling. In this light, we encourage the
user of the COCOA or COCOA+ framework to plug in the best and most recent solver available for their particular
local problem (within Algorithm 1), which is not necessarily SDCA. This choice should be made explicit especially
when comparing algorithms. Our presented convergence theory from Section 4 will still cover these choices, since it
only depends on the relative accuracy Θ of the chosen local solver.

Adding vs. Averaging in Distributed Primal-Dual Optimization

• COCOA+ is Theoretically Safe, while still Adaptive to the Data. The general deﬁnition of the local subproblems,
and therefore the treatment of the varying separable bound on the objective – quantiﬁed by σ(cid:48) – allows our framework
to adapt to the difﬁculty of the data partition and still give convergence results. The data-dependent measure σ(cid:48) is
fully decoupled from what the user of the framework prefers to employ as a local solver (see also the comment below
that COCOA is not a coordinate solver).
The safe upper bound σ(cid:48) = K is worst-case pessimistic, for the convergence theory to still hold in all cases, when the
updates are added. Using additional knowledge from the input data, better bounds and therefore better step-sizes can
be achieved in COCOA+. An example when σ(cid:48) can be safely chosen much smaller is when the data-matrix satisﬁes
strong row/column sparsity, see e.g. Lemma 1 in (Richt´arik & Tak´aˇc, 2013).

• Obtaining DisDCA-p as a Special Case. As shown in Lemma 18 above, we have that if in COCOA+, if SDCA is
used as the local solver and the pessimistic upper bound of σ(cid:48) = K is used and, moreover, the dataset is partitioned
K , then the COCOA+ framework reduces exactly to the DisDCA-p algorithm by (Yang, 2013).
equally, i.e. ∀k : nk = n
The correspondence breaks down if the subproblem parameter is chosen to a practically good value σ(cid:48) (cid:54)= K. Also, as
noted above, SDCA is often not the best local solver currently available. In our above experiments, SDCA was used
just for demonstration purposes and ease of comparison. Furthermore, the data partition might often be unbalanced
in practical applications.
While both DisDCA-p and COCOA are special cases of COCOA+, we note that DisDCA-p can not be recovered as a
special case of the original COCOA framework (Jaggi et al., 2014).

• COCOA/+ are Not Coordinate Methods. Despite the original name being motivated from this special case, COCOA
and COCOA+ are not coordinate methods. In fact, COCOA+ as presented here for the adding case (γ = 1) is much
more closely related to a batch method applied to the dual, using a block-separable proximal term, as following
from our new subproblem formulation (9), depending on σ(cid:48). See also the remark in Section 6. The framework here
(Algorithm 1) gives more generality, as the used local solver is not restricted to be a coordinate-wise one. In fact the
framework allows to translate recent and future improvements of single machine solvers directly to the distributed
setting, by employing them within Algorithm 1. DisDCA-p works very well for several applications, but is restricted
to using local coordinate ascent (SDCA) steps.

• Theoretical Convergence Results. While DisDCA-p (Yang, 2013) was proposed without theoretical justiﬁcation
(hence the nomenclature), the main contribution in the paper here – apart from the arbitrary local solvers – is the
convergence analysis for the framework. The theory proposed in (Yang et al., 2013) is given only for the setting of
orthogonal partitions, i.e., when σ(cid:48) = 1 and the problems become trivial to distribute given the orthogonality of data
between the workers.

The theoretical analysis here gives convergence rates applying for Algorithm 1 when using arbitrary local solvers,
and inherits the performance of the local solver. As a special case, we obtain the ﬁrst theoretical justiﬁcation and
convergence rates for original COCOA in the case of general convex objective, as well as for the special case of
DisDCA-p for both general convex and smooth convex objectives.

Adding vs. Averaging in Distributed Primal-Dual Optimization

5
1
0
2
 
l
u
J
 
3
 
 
]

G
L
.
s
c
[
 
 
2
v
8
0
5
3
0
.
2
0
5
1
:
v
i
X
r
a

Chenxin Ma∗
Industrial and Systems Engineering, Lehigh University, USA

Virginia Smith∗
University of California, Berkeley, USA

Martin Jaggi
ETH Z¨urich, Switzerland

Michael I. Jordan
University of California, Berkeley, USA

Peter Richt´arik
School of Mathematics, University of Edinburgh, UK

Martin Tak´aˇc
Industrial and Systems Engineering, Lehigh University, USA

∗Authors contributed equally.

CHM514@LEHIGH.EDU

VSMITH@BERKELEY.EDU

JAGGI@INF.ETHZ.CH

JORDAN@CS.BERKELEY.EDU

PETER.RICHTARIK@ED.AC.UK

TAKAC.MT@GMAIL.COM

Abstract

1. Introduction

Distributed optimization methods for large-scale
machine learning suffer from a communication
bottleneck. It is difﬁcult to reduce this bottleneck
while still efﬁciently and accurately aggregating
partial work from different machines. In this pa-
per, we present a novel generalization of the re-
cent communication-efﬁcient primal-dual frame-
work (COCOA) for distributed optimization. Our
framework, COCOA+, allows for additive com-
bination of local updates to the global parame-
ters at each iteration, whereas previous schemes
with convergence guarantees only allow conser-
vative averaging. We give stronger (primal-dual)
convergence rate guarantees for both COCOA as
well as our new variants, and generalize the the-
ory for both methods to cover non-smooth con-
vex loss functions. We provide an extensive ex-
perimental comparison that shows the markedly
improved performance of COCOA+ on several
real-world distributed datasets, especially when
scaling up the number of machines.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

With the wide availability of large datasets that exceed
the storage capacity of single machines, distributed opti-
mization methods for machine learning have become in-
creasingly important. Existing methods require signiﬁcant
communication between workers, frequently equaling the
amount of local computation (or reading of local data). As
a result, distributed machine learning suffers signiﬁcantly
from a communication bottleneck on real world systems,
where communication is typically several orders of magni-
tudes slower than reading data from main memory.

In this work we focus on optimization problems with em-
pirical loss minimization structure, i.e., objectives that are
a sum of the loss functions of each datapoint. This in-
cludes the most commonly used regularized variants of
linear regression and classiﬁcation methods.
For this
class of problems, the recently proposed COCOA approach
(Yang, 2013; Jaggi et al., 2014) develops a communication-
efﬁcient primal-dual scheme that targets the communica-
tion bottleneck, allowing more computation on data-local
subproblems native to each machine before communica-
tion. By appropriately choosing the amount of local com-
putation per round, this framework allows one to control
the trade-off between communication and local computa-
tion based on the systems hardware at hand.

However, the performance of COCOA (as well as related
primal SGD-based methods) is signiﬁcantly reduced by the

Adding vs. Averaging in Distributed Primal-Dual Optimization

need to average updates between all machines. As the
number of machines K grows, the updates get diluted and
slowed by 1/K, e.g., in the case where all machines ex-
cept one would have already reached the solutions of their
respective partial optimization tasks. On the other hand, if
the updates are instead added, the algorithms can diverge,
as we will observe in the practical experiments below.

To address both described issues, in this paper we develop
a novel generalization of the local COCOA subproblems
assigned to each worker, making the framework more pow-
erful in the following sense: Without extra computational
cost, the set of locally computed updates from the mod-
iﬁed subproblems (one from each machine) can be com-
bined more efﬁciently between machines. The proposed
COCOA+ updates can be aggressively added (hence the
‘+’-sufﬁx), which yields much faster convergence both in
practice and in theory. This difference is particularly sig-
niﬁcant as the number of machines K becomes large.

1.1. Contributions

Strong Scaling. To our knowledge, our framework is the
ﬁrst to exhibit favorable strong scaling for the class of prob-
lems considered, as the number of machines K increases
and the data size is kept ﬁxed. More precisely, while the
convergence rate of COCOA degrades as K is increased,
the stronger theoretical convergence rate here is – in the
worst case – independent of K. Our experiments in Section
7 conﬁrm the improved speed of convergence. Since the
number of communicated vectors is only one per round and
worker, this favorable scaling might be surprising. Indeed,
for existing methods, splitting data among more machines
generally increases communication requirements (Shamir
& Srebro, 2014), which can severely affect overall runtime.

Theoretical Analysis of Non-Smooth Losses. While the
existing analysis for COCOA in (Jaggi et al., 2014) only
covered smooth loss functions, here we extend the class
of functions where the rates apply, additionally covering,
e.g., Support Vector Machines and non-smooth regression
variants. We provide a primal-dual convergence rate for
both COCOA as well as our new method COCOA+ in the
case of general convex (L-Lipschitz) losses.

tight

Primal-Dual Convergence Rate. Furthermore, we addi-
tionally strengthen the rates by showing stronger primal-
dual convergence for both algorithmic frameworks, which
are almost
to their objective-only counterparts.
Primal-dual rates for COCOA had not previously been ana-
lyzed in the general convex case. Our primal-dual rates al-
low efﬁcient and practical certiﬁcates for the optimization
quality, e.g., for stopping criteria. The new rates apply to
both smooth and non-smooth losses, and for both COCOA
as well as the extended COCOA+.

Arbitrary Local Solvers. COCOA as well as COCOA+
allow the use of arbitrary local solvers on each machine.

Experimental Results. We provide a thorough experi-
mental comparison with competing algorithms using sev-
eral real-world distributed datasets. Our practical results
conﬁrm the strong scaling of COCOA+ as the number of
machines K grows, while competing methods, including
the original COCOA, slow down signiﬁcantly with larger
K. We implement all algorithms in Spark, and our code is
publicly available at: github.com/gingsmith/cocoa.

1.2. History and Related Work

While optimal algorithms for the serial (single machine)
case are already well researched and understood, the liter-
ature in the distributed setting is relatively sparse. In par-
ticular, details on optimal trade-offs between computation
and communication, as well as optimization or statistical
accuracy, are still widely unclear. For an overview over
this currently active research ﬁeld, we refer the reader to
(Balcan et al., 2012; Richt´arik & Tak´aˇc, 2013; Duchi et al.,
2013; Yang, 2013; Liu & Wright, 2014; Fercoq et al., 2014;
Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al.,
2014; Zhang & Lin, 2015; Qu & Richt´arik, 2014) and the
references therein. We provide a detailed comparison of
our proposed framework to the related work in Section 6.

2. Setup

We consider regularized empirical loss minimization prob-
lems of the following well-established form:
λ
2

P(w) :=

min
w∈Rd

i w) +

(cid:96)i(xT

(cid:107)w(cid:107)2

n
(cid:88)

1
n

(1)

(cid:40)

(cid:41)

i=1

i=1 ⊂ Rd represent the training data
Here the vectors {xi}n
examples, and the (cid:96)i(.) are arbitrary convex real-valued
loss functions (e.g., hinge loss), possibly depending on la-
bel information for the i-th datapoints. The constant λ > 0
is the regularization parameter.

The above class includes many standard problems of wide
interest in machine learning, statistics, and signal process-
ing, including support vector machines, regularized linear
and logistic regression, ordinal regression, and others.

Dual Problem, and Primal-Dual Certiﬁcates. The con-
jugate dual of (1) takes following form:

(cid:40)

max
α∈Rn

D(α) := −

(cid:96)∗
j (−αj) −

1
n

n
(cid:88)

j=1

2 (cid:41)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Aα
λn

(cid:13)
(cid:13)
(cid:13)
(cid:13)

λ
2

(2)

Here the data matrix A = [x1, x2, . . . , xn] ∈ Rd×n col-
lects all data-points as its columns, and (cid:96)∗
j is the conjugate
function to (cid:96)j. See, e.g., (Shalev-Shwartz & Zhang, 2013c)
for several concrete applications.

Adding vs. Averaging in Distributed Primal-Dual Optimization

It is possible to assign for any dual vector α ∈ Rn a corre-
sponding primal feasible point

w(α) = 1

λn Aα

The duality gap function is then given by:

G(α) := P(w(α)) − D(α)

(3)

(4)

By weak duality, every value D(α) at a dual candidate α
provides a lower bound on every primal value P(w). The
duality gap is therefore a certiﬁcate on the approxima-
tion quality: The distance to the unknown true optimum
P(w∗) must always lie within the duality gap, i.e., G(α) =
P(w) − D(α) ≥ P(w) − P(w∗) ≥ 0.

In large-scale machine learning settings like those consid-
ered here, the availability of such a computable measure of
approximation quality is a signiﬁcant beneﬁt during train-
ing time. Practitioners using classical primal-only methods
such as SGD have no means by which to accurately detect
if a model has been well trained, as P (w∗) is unknown.

Classes of Loss-Functions. To simplify presentation, we
assume that all loss functions (cid:96)i are non-negative, and
(cid:96)i(0) ≤ 1

(5)

∀i

Deﬁnition 1 (L-Lipschitz continuous loss). A function (cid:96)i :
R → R is L-Lipschitz continuous if ∀a, b ∈ R, we have

|(cid:96)i(a) − (cid:96)i(b)| ≤ L|a − b|
(6)
Deﬁnition 2 ((1/µ)-smooth loss). A function (cid:96)i : R → R
is (1/µ)-smooth if it is differentiable and its derivative is
(1/µ)-Lipschitz continuous, i.e., ∀a, b ∈ R, we have

|(cid:96)(cid:48)

i(a) − (cid:96)(cid:48)

i(b)| ≤

|a − b|

(7)

1
µ

3. The COCOA+ Algorithm Framework
In this section we present our novel COCOA+ frame-
work. COCOA+ inherits the many beneﬁts of CoCoA as
it remains a highly ﬂexible and scalable, communication-
efﬁcient framework for distributed optimization. COCOA+
differs algorithmically in that we modify the form of the lo-
cal subproblems (9) to allow for more aggressive additive
updates (as controlled by γ). We will see that these changes
allow for stronger convergence guarantees as well as im-
proved empirical performance. Proofs of all statements in
this section are given in the supplementary material.

Data Partitioning. We write {Pk}K
k=1 for the given par-
tition of the datapoints [n] := {1, 2, . . . , n} over the K
worker machines. We denote the size of each part by
nk = |Pk|. For any k ∈ [K] and α ∈ Rn we use the
notation α[k] ∈ Rn for the vector

(α[k])i :=

(cid:40)

0,
if i /∈ Pk,
αi, otherwise.

Local Subproblems in COCOA+. We can deﬁne a data-
local subproblem of the original dual optimization problem
(2), which can be solved on machine k and only requires
accessing data which is already available locally, i.e., data-
points with i ∈ Pk. More formally, each machine k is as-
signed the following local subproblem, depending only on
the previous shared primal vector w ∈ Rd, and the change
in the local dual variables αi with i ∈ Pk:

max
∆α[k]∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k])

(8)

where

1
n

(cid:88)

i∈Pk
λ
2

Gσ(cid:48)
k (∆α[k]; w, α[k]) := −

(cid:96)∗
i (−αi − (∆α[k])i)

−

1
K

λ
2

1
n

(cid:107)w(cid:107)2 −

wT A∆α[k] −

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

1
λn

A∆α[k]

(9)

(cid:13)
2
(cid:13)
(cid:13)

Interpretation. The above deﬁnition of the local objec-
tive functions Gσ(cid:48)
k are such that they closely approximate
the global dual objective D, as we vary the ‘local’ vari-
able ∆α[k], in the following precise sense:
Lemma 3. For any dual α, ∆α ∈ Rn, primal w = w(α)
and real values γ, σ(cid:48) satisfying (11), it holds that

(cid:16)

D

α + γ

K
(cid:88)

k=1

(cid:17)
∆α[k]
K
(cid:88)

+γ

≥ (1 − γ)D(α)

Gσ(cid:48)
k (∆α[k]; w, α[k])

(10)

k=1
The role of the parameter σ(cid:48) is to measure the difﬁculty of
the given data partition. For our purposes, we will see that
it must be chosen not smaller than

σ(cid:48) ≥ σ(cid:48)

min := γ max
α∈Rn

(cid:107)Aα(cid:107)2
k=1 (cid:107)Aα[k](cid:107)2

(cid:80)K

(11)

In the following lemma, we show that this parameter can
be upper-bounded by γK, which is trivial to calculate for
all values γ ∈ R. We show experimentally (Section 7) that
this safe upper bound for σ(cid:48) has a minimal effect on the
overall performance of the algorithm. Our main theorems
below show convergence rates dependent on γ ∈ [ 1
K , 1],
which we refer to as the aggregation parameter.
Lemma 4. The choice of σ(cid:48) := γK is valid for (11), i.e.,

γK ≥ σ(cid:48)

min

Notion of Approximation Quality of the Local Solver.
Assumption 1 (Θ-approximate solution). We assume that
there exists Θ ∈ [0, 1) such that ∀k ∈ [K], the local solver
at any outer iteration t produces a (possibly) randomized
approximate solution ∆α[k], which satisﬁes
[k]; w, α[k]) − Gσ(cid:48)

(12)

E(cid:2)Gσ(cid:48)
k (∆α∗
(cid:16)
Gσ(cid:48)
k (∆α∗

≤ Θ

k (∆α[k]; w, α[k])(cid:3)
(cid:17)

k (0; w, α[k])

,

[k]; w, α[k]) − Gσ(cid:48)

Adding vs. Averaging in Distributed Primal-Dual Optimization

where
∆α∗

[k] ∈ arg max
∆α∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k]) ∀k ∈ [K] (13)

We are now ready to describe the COCOA+ framework,
shown in Algorithm 1. The crucial difference compared
to the existing COCOA algorithm (Jaggi et al., 2014) is the
more general local subproblem, as deﬁned in (9), as well as
the aggregation parameter γ. These modiﬁcations allow the
option of directly adding updates to the global vector w.

Algorithm 1 COCOA+ Framework
1: Input: Datapoints A distributed according to parti-
tion {Pk}K
k=1. Aggregation parameter γ ∈ (0, 1],
subproblem parameter σ(cid:48) for the local subproblems
Gσ(cid:48)
k (∆α[k]; w, α[k]) for each k ∈ [K].
Starting point α(0) := 0 ∈ Rn, w(0) := 0 ∈ Rd.

2: for t = 0, 1, 2, . . . do
3:

for k ∈ {1, 2, . . . , K} in parallel over computers
do

call the local solver, computing a Θ-approximate
solution ∆α[k] of the local subproblem (9)
update α(t+1)
[k]
return ∆wk := 1

:= α(t)
λn A∆α[k]

[k] + γ ∆α[k]

end for
reduce w(t+1) := w(t) + γ (cid:80)K

k=1 ∆wk.

(14)

4:

5:

6:
7:
8:

9: end for

4. Convergence Guarantees

Before being able to state our main convergence results,
we introduce some useful quantities and the following
main lemma characterizing the effect of iterations of Al-
gorithm 1, for any chosen internal local solver.
Lemma 5. Let (cid:96)∗
i be strongly1 convex with convexity pa-
rameter µ ≥ 0 with respect to the norm (cid:107)·(cid:107), ∀i ∈ [n]. Then
for all iterations t of Algorithm 1 under Assumption 1, and
any s ∈ [0, 1], it holds that

E[D(α(t+1)) − D(α(t))] ≥

γ(1 − Θ)

sG(α(t)) −

(cid:16)

(15)
R(t)(cid:17)

,

σ(cid:48)
2λ

(cid:0) s
n

(cid:1)2

where

R(t) := − λµn(1−s)
+ (cid:80)K

(cid:107)u(t) − α(t)(cid:107)2
k=1(cid:107)A(u(t) − α(t))[k](cid:107)2,

σ(cid:48)s

(16)

for u(t) ∈ Rn with

− u(t)

i ∈ ∂(cid:96)i(w(α(t))T xi).

(17)

1Note that the case of weakly convex (cid:96)∗

i (.) is explicitly al-

lowed here as well, as the Lemma holds for the case µ = 0.

The following Lemma provides a uniform bound on R(t):
Lemma 6. If (cid:96)i are L-Lipschitz continuous for all i ∈ [n],
then

∀t : R(t) ≤ 4L2

σknk

,

K
(cid:88)

where

k=1
(cid:124)

(cid:125)

(cid:123)(cid:122)
=:σ
(cid:107)Aα[k](cid:107)2
(cid:107)α[k](cid:107)2 .

σk := max
α[k]∈Rn

(18)

(19)

Remark 7. If all data-points xi are normalized such that
(cid:107)xi(cid:107) ≤ 1 ∀i ∈ [n], then σk ≤ |Pk| = nk. Furthermore,
if we assume that the data partition is balanced, i.e., that
nk = n/K for all k, then σ ≤ n2/K. This can be used to
bound the constants R(t), above, as R(t) ≤ 4L2n2
K .

4.1. Primal-Dual Convergence for General Convex

Losses

The following theorem shows the convergence for non-
smooth loss functions, in terms of objective values as well
as primal-dual gap. The analysis in (Jaggi et al., 2014) only
covered the case of smooth loss functions.
Theorem 8. Consider Algorithm 1 with Assumption 1. Let
(cid:96)i(·) be L-Lipschitz continuous, and (cid:15)G > 0 be the de-
sired duality gap (and hence an upper-bound on primal
sub-optimality). Then after T iterations, where

(cid:108)
T ≥ T0 + max{

1
γ(1 − Θ)

(cid:109)
,

T0 ≥ t0 +

(cid:18)

2
γ(1 − Θ)
(cid:108)

4L2σσ(cid:48)
λn2(cid:15)Gγ(1 − Θ)
(cid:19)(cid:19)

(cid:18) 8L2σσ(cid:48)
λn2(cid:15)G

− 1

,

+

t0 ≥ max(0,

1

γ(1−Θ) log( 2λn2(D(α∗)−D(α(0)))

4L2σσ(cid:48)

(cid:109)
)

),

},

(20)

we have that the expected duality gap satisﬁes

E[P(w(α)) − D(α)] ≤ (cid:15)G,

at the averaged iterate

α := 1

T −T0

(cid:80)T −1

t=T0+1α(t).

(21)

The following corollary of the above theorem clariﬁes our
main result: The more aggressive adding of the partial up-
dates, as compared averaging, offers a very signiﬁcant im-
provement in terms of total iterations needed. While the
convergence in the ‘adding’ case becomes independent of
the number of machines K, the ‘averaging’ regime shows
the known degradation of the rate with growing K, which is
a major drawback of the original COCOA algorithm. This
important difference in the convergence speed is not a the-
oretical artifact but also conﬁrmed in our practical experi-
ments below for different K, as shown e.g. in Figure 2.

We further demonstrate below that by choosing γ and σ(cid:48)
accordingly, we can still recover the original COCOA al-
gorithm and its rate.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Corollary 9. Assume that all datapoints xi are bounded as
(cid:107)xi(cid:107) ≤ 1 and that the data partition is balanced, i.e. that
nk = n/K for all k. We consider two different possible
choices of the aggregation parameter γ:

• (COCOA Averaging, γ := 1

K ): In this case, σ(cid:48) := 1
is a valid choice which satisﬁes (11). Then using σ ≤
n2/K in light of Remark 7, we have that T iterations
are sufﬁcient for primal-dual accuracy (cid:15)G, with

T ≥ T0 + max{

T0 ≥ t0 +

(cid:18) 2K
1 − Θ
t0 ≥ max(0, (cid:6) K

(cid:108) K

(cid:109)
,

4L2
λ(cid:15)G(1 − Θ)
(cid:19)(cid:19)

},

− 1

,

+

1 − Θ
(cid:18) 8L2
λK(cid:15)G

1−Θ log( 2λ(D(α∗)−D(α(0)))

4KL2

)(cid:7))

Hence the more machines K, the more iterations are
needed (in the worst case).

• (COCOA+ Adding, γ := 1): In this case, the choice of
σ(cid:48) := K satisﬁes (11). Then using σ ≤ n2/K in light
of Remark 7, we have that T iterations are sufﬁcient
for primal-dual accuracy (cid:15)G, with

(cid:108)

1
1 − Θ
(cid:18) 8L2
λ(cid:15)G

T ≥ T0 + max{

(cid:18) 2

T0 ≥ t0 +

1 − Θ
t0 ≥ max(0, (cid:6) 1

(cid:109)
,

4L2
λ(cid:15)G(1 − Θ)

},

(cid:19)(cid:19)

− 1

,

+

1−Θ log( 2λn(D(α∗)−D(α(0)))

4KL2

)(cid:7))

This is signiﬁcantly better than the averaging case.

In practice, we usually have σ (cid:28) n2/K, and hence the
actual convergence rate can be much better than the proven
worst-case bound. Table 1 shows that the actual value of
σ is typically between one and two orders of magnitudes
smaller compared to our used upper-bound n2/K.

Table 1. The ratio of upper-bound n2
the parameter σ, for some real datasets.

K divided by the true value of

K

16

32

64

128

256

512

news
real-sim
rcv1

15.483
42.127
40.138

14.933
36.898
23.827

14.278
30.780
28.204

13.390
23.814
21.792

12.074
16.965
16.339

10.252
11.835
11.099

K

256

512

1024

2048

4096

8192

covtype

17.277

17.260

17.239

16.948

17.238

12.729

4.2. Primal-Dual Convergence for Smooth Losses

The following theorem shows the convergence for smooth
losses, in terms of the objective as well as primal-dual gap.

Theorem 10. Assume the loss functions functions (cid:96)i are
(1/µ)-smooth ∀i ∈ [n]. We deﬁne σmax = maxk∈[K] σk.
Then after T iterations of Algorithm 1, with

T ≥

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

log 1
(cid:15)D

,

it holds that

E[D(α∗) − D(α(T ))] ≤ (cid:15)D.

Furthermore, after T iterations with

T ≥

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

log

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

1
(cid:15)G

(cid:16)

(cid:17)

,

we have the expected duality gap

E[P(w(α(T ))) − D(α(T ))] ≤ (cid:15)G.

The following corollary is analogous to Corollary 9, but
for the case of smooth loses. It again shows that while the
COCOA variant degrades with the increase of the number
of machines K, the COCOA+ rate is independent of K.
Corollary 11. Assume that all datapoints xi are bounded
as (cid:107)xi(cid:107) ≤ 1 and that the data partition is balanced, i.e.,
that nk = n/K for all k. We again consider the same two
different possible choices of the aggregation parameter γ:
K ): In this case, σ(cid:48) :=
1 is a valid choice which satisﬁes (11). Then using
σmax ≤ nk = n/K in light of Remark 7, we have that
T iterations are sufﬁcient for suboptimality (cid:15)D, with

• (COCOA Averaging, γ := 1

T ≥ 1

1−Θ

λµK+1
λµ

log 1
(cid:15)D

Hence the more machines K, the more iterations are
needed (in the worst case).

• (COCOA+ Adding, γ := 1): In this case, the choice
of σ(cid:48) := K satisﬁes (11). Then using σmax ≤ nk =
n/K in light of Remark 7, we have that T iterations
are sufﬁcient for suboptimality (cid:15)D, with

T ≥ 1

1−Θ

λµ+1

λµ log 1
(cid:15)D

This is signiﬁcantly better than the averaging case.
Both rates hold analogously for the duality gap.

4.3. Comparison with Original COCOA

Remark 12. If we choose averaging (γ := 1
K ) for aggre-
gating the updates, together with σ(cid:48) := 1, then the result-
ing Algorithm 1 is identical to COCOA analyzed in (Jaggi
et al., 2014). However, they only provide convergence for
smooth loss functions (cid:96)i and have guarantees for dual sub-
optimality and not the duality gap. Formally, when σ(cid:48) = 1,
the subproblems (9) will differ from the original dual D(.)
only by an additive constant, which does not affect the local
optimization algorithms used within COCOA.

5. SDCA as an Example Local Solver

We have shown convergence rates for Algorithm 1, depend-
ing solely on the approximation quality Θ of the used local

Adding vs. Averaging in Distributed Primal-Dual Optimization

solver (Assumption 1). Any chosen local solver in each
round receives the local α variables as an input, as well as
a shared vector w (3)= w(α) being compatible with the last
state of all global α ∈ Rn variables.

As an illustrative example for a local solver, Algorithm 2
below summarizes randomized coordinate ascent (SDCA)
applied on the local subproblem (9). The following two
Theorems (13, 14) characterize the local convergence for
both smooth and non-smooth functions. In all the results
we will use rmax := maxi∈[n] (cid:107)xi(cid:107)2.

Algorithm 2 LOCALSDCA (w, α[k], k, H)
1: Input: α[k], w = w(α)
2: Data: Local {(xi, yi)}i∈Pk
3: Initialize: ∆α(0)
[k] := 0 ∈ Rn
4: for h = 0, 1, . . . , H − 1 do
5:

choose i ∈ Pk uniformly at random
Gσ(cid:48)
k (∆α(h)
δ∗
i := arg max

6:

[k] + δiei; w, α[k])

δi∈R
:= ∆α(h)

[k] + δ∗

i ei

7: ∆α(h+1)
[k]
8: end for
9: Output: ∆α(H)
[k]

Theorem 13. Assume the functions (cid:96)i are (1/µ)−smooth
for i ∈ [n]. Then Assumption 1 on the local approximation
quality Θ is satisﬁed for LOCALSDCA as given in Algo-
rithm 2, if we choose the number of inner iterations H as

H ≥ nk

σ(cid:48)rmax + λnµ
λnµ
Theorem 14. Assume the functions (cid:96)i are L-Lipschitz for
i ∈ [n]. Then Assumption 1 on the local approximation
quality Θ is satisﬁed for LOCALSDCA as given in Algo-
rithm 2, if we choose the number of inner iterations H as

1
Θ

(22)

log

.

H ≥ nk

(cid:18) 1 − Θ
Θ

+

σ(cid:48)rmax
2Θλn2

(cid:107)∆α∗
[k](cid:107)2
[k]; .) − Gσ(cid:48)

k (0; .)

(cid:19)

.

Gσ(cid:48)
k (∆α∗

(23)
Remark 15. Between the different regimes allowed in
COCOA+ (ranging between averaging and adding the up-
dates) the computational cost for obtaining the required
local approximation quality varies with the choice of σ(cid:48).
From the above worst-case upper bound, we note that the
cost can increase with σ(cid:48), as aggregation becomes more
aggressive. However, as we will see in the practical exper-
iments in Section 7 below, the additional cost is negligible
compared to the gain in speed from the different aggrega-
tion, when measured on real datasets.

6. Discussion and Related Work

SGD-based Algorithms. For the empirical loss mini-
mization problems of interest here, stochastic subgradient

descent (SGD) based methods are well-established. Sev-
eral distributed variants of SGD have been proposed, many
of which build on the idea of a parameter server (Niu et al.,
2011; Liu et al., 2014; Duchi et al., 2013). The downside of
this approach, even when carefully implemented, is that the
amount of required communication is equal to the amount
of data read locally (e.g., mini-batch SGD with a batch size
of 1 per worker). These variants are in practice not compet-
itive with the more communication-efﬁcient methods con-
sidered here, which allow more local updates per round.

One-Shot Communication Schemes. At the other ex-
treme, there are distributed methods using only a single
round of communication, such as (Zhang et al., 2013;
Zinkevich et al., 2010; Mann et al., 2009; McWilliams
et al., 2014). These require additional assumptions on the
partitioning of the data, and furthermore can not guarantee
convergence to the optimum solution for all regularizers, as
shown in, e.g., (Shamir et al., 2014). (Balcan et al., 2012)
shows additional relevant lower bounds on the minimum
number of communication rounds necessary for a given ap-
proximation quality for similar machine learning problems.

Mini-Batch Methods. Mini-batch methods are more
ﬂexible and lie within these two communication vs. com-
putation extremes. However, mini-batch versions of both
SGD and coordinate descent (CD) (Richt´arik & Tak´aˇc,
2013; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu
& Richt´arik, 2014; Qu et al., 2014) suffer from their con-
vergence rate degrading towards the rate of batch gradient
descent as the size of the mini-batch is increased. This fol-
lows because mini-batch updates are made based on the
outdated previous parameter vector w, in contrast to meth-
ods that allow immediate local updates like COCOA. Fur-
thermore, the aggregation parameter for mini-batch meth-
ods is harder to tune, as it can lie anywhere in the order of
mini-batch size. In the COCOA setting, the parameter lies
in the smaller range given by K. Our COCOA+ extension
avoids needing to tune this parameter entirely, by adding.

Methods Allowing Local Optimization. Developing
methods that allow for local optimization requires care-
fully devising data-local subproblems to be solved after
each communication round. (Shamir et al., 2014; Zhang
& Lin, 2015) have proposed distributed Newton-type algo-
rithms in this spirit. However, the subproblems must be
solved to high accuracy for convergence to hold, which is
often prohibitive as the size of the data on one machine is
still relatively large. In contrast, the COCOA framework
(Jaggi et al., 2014) allows using any local solver of weak
local approximation quality in each round. By making use
of the primal-dual structure in the line of work of (Yu et al.,
2012; Pechyony et al., 2011; Yang, 2013; Lee & Roth,
2015), the COCOA and COCOA+ frameworks also allow
more control over the aggregation of updates between ma-

Adding vs. Averaging in Distributed Primal-Dual Optimization

Figure 1. Duality gap vs. the number of communicated vectors, as well as duality gap vs. elapsed time in seconds for two datasets:
Covertype (left, K=4) and RCV1 (right, K=8). Both are shown on a log-log scale, and for three different values of regularization
(λ=1e-4; 1e-5; 1e-6). Each plot contains a comparison of COCOA (red) and COCOA+ (blue), for three different values of H, the
number of local iterations performed per round. For all plots, across all values of λ and H, we see that COCOA+ converges to the
optimal solution faster than COCOA, in terms of both the number of communications and the elapsed time.

chines. The practical variant DisDCA-p proposed in (Yang,
2013) allows additive updates but is restricted to SDCA
updates, and was proposed without convergence guaran-
tees. DisDCA-p can be recovered as a special case of the
COCOA+ framework when using SDCA as a local solver,
if nk = n/K and σ(cid:48) := K, see Appendix C. The theory
presented here also therefore covers that method.

ADMM. An alternative approach to distributed optimiza-
tion is to use the alternating direction method of multipli-
ers (ADMM), as used for distributed SVM training in, e.g.,
(Forero et al., 2010). This uses a penalty parameter balanc-
ing between the equality constraint w and the optimization
objective (Boyd et al., 2011). However, the known conver-
gence rates for ADMM are weaker than the more problem-
tailored methods mentioned previously, and the choice of
the penalty parameter is often unclear.

Batch Proximal Methods.
In spirit, for the special case
of adding (γ = 1), COCOA+ resembles a batch proximal
method, using the separable approximation (9) instead of
the original dual (2). Known batch proximal methods re-
quire high accuracy subproblem solutions, and don’t allow
arbitrary solvers of weak accuracy Θ such as we do here.

7. Numerical Experiments

We present experiments on several large real-world dis-
tributed datasets. We show that COCOA+ converges faster
in terms of total rounds as well as elapsed time as compared
to COCOA in all cases, despite varying: the dataset, values
of regularization, batch size, and cluster size (Section 7.2).
In Section 7.3 we demonstrate that this performance trans-
lates to orders of magnitude improvement in convergence
when scaling up the number of machines K, as compared
to COCOA as well as to several other state-of-the-art meth-
ods. Finally, in Section 7.4 we investigate the impact of the
local subproblem parameter σ(cid:48) in the COCOA+ framework.

Table 2. Datasets for Numerical Experiments.
n
522,911
400,000
677,399

Dataset
covertype
epsilon
RCV1

Sparsity
22.22%
100%
0.16%

d
54
2,000
47,236

7.1. Implementation Details

We implement all algorithms in Apache Spark (Zaharia
et al., 2012) and run them on m3.large Amazon EC2 in-
stances, applying each method to the binary hinge-loss sup-

Adding vs. Averaging in Distributed Primal-Dual Optimization

port vector machine. The analysis for this non-smooth loss
was not covered in (Jaggi et al., 2014) but has been captured
here, and thus is both theoretically and practically justiﬁed.
The used datasets are summarized in Table 2.

For illustration and ease of comparison, we here use SDCA
(Shalev-Shwartz & Zhang, 2013c) as the local solver for
both COCOA and COCOA+. Note that in this special case,
and if additionally σ(cid:48) := K, and if the partitioning nk =
n/K is balanced, once can show that the COCOA+ frame-
work reduces to the practical variant of DisDCA (Yang,
2013) (which had no convergence guarantees so far). We
include more details on the connection in Appendix C.

7.2. Comparison of COCOA+ and COCOA
We compare the COCOA+ and COCOA frameworks di-
rectly using two datasets (Covertype and RCV1) across var-
ious values of λ, the regularizer, in Figure 1. For each value
of λ we consider both methods with different values of H,
the number of local iterations performed before communi-
cating to the master. For all runs of COCOA+ we use the
safe upper bound of γK for σ(cid:48). In terms of both the to-
tal number of communications made and the elapsed time,
COCOA+ (shown in blue) converges to the optimal solu-
tion faster than COCOA (red). The discrepancy is larger
for greater values of λ, where the strongly convex regular-
izer has more of an impact and the problem difﬁculty is re-
duced. We also see a greater performance gap for smaller
values of H, where there is frequent communication be-
tween the machines and the master, and changes between
the algorithms therefore play a larger role.

7.3. Scaling the Number of Machines K
In Figure 2 we demonstrate the ability of COCOA+ to
scale with an increasing number of machines K. The
experiments conﬁrm the ability of strong scaling of the
new method, as predicted by our theory in Section 4,
in contrast to the competing methods. Unlike COCOA,
which becomes linearly slower when increasing the num-
ber of machines, the performance of COCOA+ improves
with additional machines, only starting to degrade slightly
once K=16 for the RCV1 dataset.

7.4. Impact of the Subproblem Parameter σ(cid:48)

Finally, in Figure 3, we consider the effect of the choice
of the subproblem parameter σ(cid:48) on convergence. We plot
both the number of communications and clock time on a
log-log scale for the RCV1 dataset with K=8 and H=1e4.
For γ = 1 (the most aggressive variant of COCOA+ in
which updates are added) we consider several different val-
ues of σ(cid:48), ranging from 1 to 8. The value σ(cid:48)=8 represents
the safe upper bound of γK. The optimal convergence oc-
curs around σ(cid:48)=4, and diverges for σ(cid:48) ≤ 2. Notably, we

Figure 2. The effect of increasing K on the time (s) to reach an
(cid:15)D-accurate solution. We see that COCOA+ converges twice as
fast as COCOA on 100 machines for the Epsilon dataset, and
nearly 7 times as quickly for the RCV1 dataset. Mini-batch SGD
converges an order of magnitude more slowly than both methods.

see that the easy to calculate upper bound of σ(cid:48) := γK (as
given by Lemma 4) has only slightly worse performance
than best possible subproblem parameter in our setting.

Figure 3. The effect of σ(cid:48) on convergence of COCOA+ for the
RCV1 dataset distributed across K=8 machines. Decreasing σ(cid:48)
improves performance in terms of communication and overall run
time until a certain point, after which the algorithm diverges. The
“safe” upper bound of σ(cid:48):=K=8 has only slightly worse perfor-
mance than the practically best “un-safe” value of σ(cid:48).

8. Conclusion
In conclusion, we present a novel framework COCOA+
that allows for fast and communication-efﬁcient additive
aggregation in distributed algorithms for primal-dual opti-
mization. We analyze the theoretical performance of this
method, giving strong primal-dual convergence rates with
outer iterations scaling independently of the number of ma-
chines. We extended our theory to allow for non-smooth
losses. Our experimental results show signiﬁcant speedups
over previous methods,
including the original COCOA
framework as well as other state-of-the-art methods.

Acknowledgments. We thank Ching-pei Lee and an
anonymous reviewer for several helpful insights and com-
ments.

Adding vs. Averaging in Distributed Primal-Dual Optimization

References

Balcan, M.-F., Blum, A., Fine, S., and Mansour, Y. Dis-
tributed Learning, Communication Complexity and Pri-
vacy. In COLT, pp. 26.1–26.22, 2012.

Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J.
Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3(1):1–122, 2011.

Duchi, J. C., Jordan, M. I., and McMahan, H. B. Estima-
tion, Optimization, and Parallelism when Data is Sparse.
In NIPS, 2013.

Fercoq, O. and Richt´arik, P. Accelerated, parallel and prox-

imal coordinate descent. arXiv:1312.5799, 2013.

Fercoq, O., Qu, Z., Richt´arik, P., and Tak´aˇc, M. Fast
distributed coordinate descent for non-strongly convex
losses. IEEE Workshop on Machine Learning for Signal
Processing, 2014.

Forero, P. A., Cano, A., and Giannakis, G. B. Consensus-
Based Distributed Support Vector Machines. JMLR, 11:
1663–1707, 2010.

Jaggi, M., Smith, V., Tak´aˇc, M., Terhorst, J., Krishnan, S.,
Hofmann, T., and Jordan, M. I. Communication-efﬁcient
distributed dual coordinate ascent. In NIPS, 2014.

Lee, C.-P. and Roth, D. Distributed Box-Constrained
Quadratic Optimization for Dual Linear SVM. In ICML,
2015.

Liu, J. and Wright, S. J. Asynchronous stochastic coor-
dinate descent: Parallelism and convergence properties.
arXiv:1403.3862, 2014.

Liu, J., Wright, S. J., R´e, C., Bittorf, V., and Sridhar,
S. An Asynchronous Parallel Stochastic Coordinate De-
scent Algorithm. In ICML, 2014.

Lu, Z. and Xiao, L. On the complexity analysis of random-
ized block-coordinate descent methods. arXiv preprint
arXiv:1305.4723, 2013.

Mann, G., McDonald, R., Mohri, M., Silberman, N., and
Walker, D. D. Efﬁcient Large-Scale Distributed Training
of Conditional Maximum Entropy Models. NIPS, 2009.

Mareˇcek, J., Richt´arik, P., and Tak´aˇc, M. Distributed block
coordinate descent for minimizing partially separable
functions. arXiv:1406.0238, 2014.

McWilliams, B., Heinze, C., Meinshausen, N., Krumme-
nacher, G., and Vanchinathan, H. P. LOCO: Distribut-
ing Ridge Regression with Random Projections. arXiv
stat.ML, June 2014.

Niu, F., Recht, B., R´e, C., and Wright, S. J. Hogwild!: A
Lock-Free Approach to Parallelizing Stochastic Gradient
Descent. In NIPS, 2011.

Pechyony, D., Shen, L., and Jones, R. Solving Large Scale
In
Linear SVM with Distributed Block Minimization.
NIPS Workshop on Big Learning, 2011.

Qu, Z. and Richt´arik, P.

Coordinate descent with
arbitrary sampling I: Algorithms and complexity.
arXiv:1412.8060, 2014.

Qu, Z., Richt´arik, P., and Zhang, T. Randomized dual coor-
dinate ascent with arbitrary sampling. arXiv:1411.5873,
2014.

Richt´arik, P. and Tak´aˇc, M. Distributed coordinate de-
scent method for learning with big data. arXiv preprint
arXiv:1310.2059, 2013.

Richt´arik, P. and Tak´aˇc, M.

Iteration complexity of ran-
domized block-coordinate descent methods for minimiz-
ing a composite function. Mathematical Programming,
144(1-2):1–38, April 2014.

Richt´arik, P. and Tak´aˇc, M. Parallel coordinate descent
methods for big data optimization. Mathematical Pro-
gramming, pp. 1–52, 2015.

Shalev-Shwartz, S. and Zhang, T. Accelerated mini-batch

stochastic dual coordinate ascent. In NIPS, 2013a.

Shalev-Shwartz, S. and Zhang, T. Accelerated proximal
stochastic dual coordinate ascent for regularized loss
minimization. arXiv:1309.2375, 2013b.

Shalev-Shwartz, S. and Zhang, T. Stochastic Dual Coor-
dinate Ascent Methods for Regularized Loss Minimiza-
tion. JMLR, 14:567–599, 2013c.

Shamir, O. and Srebro, N. Distributed Stochastic Optimiza-

tion and Learning . In Allerton, 2014.

Shamir, O., Srebro, N., and Zhang, T. Communication
efﬁcient distributed optimization using an approximate
newton-type method. In ICML, 2014.

Tappenden, R., Tak´aˇc, M., and Richt´arik, P. On the com-
plexity of parallel coordinate descent. Technical report,
2015. ERGO 15-001, University of Edinburgh.

Yang, T. Trading Computation for Communication: Dis-
In NIPS,

tributed Stochastic Dual Coordinate Ascent.
2013.

Yang, T., Zhu, S., Jin, R., and Lin, Y. On Theoretical Anal-
ysis of Distributed Stochastic Dual Coordinate Ascent.
arXiv:1312.1031, 2013.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Yu, H.-F., Hsieh, C.-J., Chang, K.-W., and Lin, C.-J. Large
Linear Classiﬁcation When Data Cannot Fit in Memory.
TKDD, 5(4):1–23, 2012.

Zaharia, M., Chowdhury, M., Das, T., Dave, A., McCauley,
M., Franklin, M. J., Shenker, S., and Stoica, I. Resilient
Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing. In NSDI, 2012.

Zhang, Y. and Lin, X. DiSCO: Distributed Optimization for
Self-Concordant Empirical Loss. In ICML, pp. 362–370,
2015.

Zhang, Y., Duchi,

J.
Communication-Efﬁcient Algorithms for Statistical Op-
timization. JMLR, 14:3321–3363, 2013.

and Wainwright, M.

J. C.,

Zinkevich, M. A., Weimer, M., Smola, A. J., and Li, L.
Parallelized Stochastic Gradient Descent. NIPS, 2010.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Appendix

A. Technical Lemmas

Lemma 16 (Lemma 21 in (Shalev-Shwartz & Zhang, 2013c)). Let (cid:96)i : R → R be an L-Lipschitz continuous. Then for
any real value a with |a| > L we have that (cid:96)∗

i (a) = ∞.

Lemma 17. Assuming the loss functions (cid:96)i are bounded by (cid:96)i(0) ≤ 1 for all i ∈ [n] (as we have assumed in (5) above),
then for the zero vector α(0) := 0 ∈ Rn, we have

D(α∗) − D(α(0)) = D(α∗) − D(0) ≤ 1.

(24)

Proof. For α := 0 ∈ Rn, we have w(α) = 1
in (2),

λn Aα = 0 ∈ Rd. Therefore, by deﬁnition of the dual objective D given

0 ≤ D(α∗) − D(α) ≤ P(w(α)) − D(α) = 0 − D(α)

(5),(2)

≤ 1.

B. Proofs

B.1. Proof of Lemma 3

Indeed, we have

D(α + γ

∆α[k]) = −

(cid:96)∗
i (−αi − γ(

∆α[k])i)

−

A(α + γ

∆α[k])

(25)

K
(cid:88)

k=1

1
n

n
(cid:88)

i=1

(cid:124)

K
(cid:88)

k=1

1
λn

λ
2

(cid:13)
(cid:13)
(cid:13)

(cid:124)

(cid:125)

K
(cid:88)

k=1

(cid:123)(cid:122)
B

(cid:13)
2
(cid:13)
(cid:13)

.

(cid:125)

Now, let us bound the terms A and B separately. We have

A = −

(cid:96)∗
i (−αi − γ(∆α[k])i)

= −

(cid:96)∗
i (−(1 − γ)αi − γ(α + ∆α[k])i)

(cid:33)

1
n

1
n

(cid:32)

K
(cid:88)

(cid:88)

k=1

K
(cid:88)

(cid:32)

i∈Pk

(cid:88)

k=1

i∈Pk

≥ −

(1 − γ)(cid:96)∗

i (−αi) + γ(cid:96)∗

i (−(α + ∆α[k])i)

.

(cid:123)(cid:122)
A

(cid:33)

1
n

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk
(cid:33)

Where the last inequality is due to Jensen’s inequality. Now we will bound B, using the safe separability measurement σ(cid:48)
as deﬁned in (11).

B =

A(α + γ

(cid:13)
(cid:13)
(cid:13)

1
λn

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∆α[k])
(cid:13)

=

(cid:13)
(cid:13)
(cid:13)w(α) + γ

1
λn

K
(cid:88)

k=1

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

= (cid:107)w(α)(cid:107)2 +

w(α)T A∆α[k] + γ

(cid:16) 1
λn

(cid:17)2

(cid:13)
(cid:13)
(cid:13)

γ

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

(11)
≤ (cid:107)w(α)(cid:107)2 +

w(α)T A∆α[k] + γ

(cid:107)A∆α[k](cid:107)2.

(cid:17)2

σ(cid:48)

(cid:16) 1
λn

K
(cid:88)

k=1

2γ

1
λn

K
(cid:88)

k=1

2γ

1
λn

K
(cid:88)

k=1

K
(cid:88)

k=1

− γ

(cid:107)w(α)(cid:107)2 − (1 − γ)

(cid:107)w(α)(cid:107)2 −

w(α)T A∆α[k] −

λ
2

γ

(cid:16) 1
λn

(cid:17)2

σ(cid:48)

K
(cid:88)

k=1

(cid:107)A∆α[k](cid:107)2

Adding vs. Averaging in Distributed Primal-Dual Optimization

Plugging A and B into (25) will give us

D(α + γ

∆α[k]) ≥ −

(1 − γ)(cid:96)∗

i (−αi) + γ(cid:96)∗

i (−(α + ∆α[k])i)

(cid:33)

K
(cid:88)

k=1

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk

1
n

λ
2

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk

= −

1
n

(cid:124)

+ γ

(cid:32)

K
(cid:88)

k=1

−

1
n

(cid:88)

i∈Pk

λ
2

(cid:33)

λ
2

K
(cid:88)

k=1

2γ

1
λn

(1 − γ)(cid:96)∗

i (−αi)

− (1 − γ)

(cid:107)w(α)(cid:107)2

(cid:123)(cid:122)
(1−γ)D(α)

(cid:125)

λ
2

1
K

λ
2

(9)=(1 − γ)D(α) + γ

Gσ(cid:48)
k (∆α[k]; w, α[k]).

K
(cid:88)

k=1

(cid:96)∗
i (−(α + ∆α[k])i) −

(cid:107)w(α)(cid:107)2 −

w(α)T A∆α[k] −

1
n

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α[k]

2(cid:33)
(cid:13)
(cid:13)
(cid:13)

B.2. Proof of Lemma 4

See (Richt´arik & Tak´aˇc, 2013).

B.3. Proof of Lemma 5

For sake of notation, we will write α instead of α(t), w instead of w(α(t)) and u instead of u(t).

Now, let us estimate the expected change of the dual objective. Using the deﬁnition of the dual update α(t+1) := α(t) +
γ (cid:80)

k ∆α[k] resulting in Algorithm 1, we have

(cid:104)
E(cid:2)D(α(t)) − D(α(t+1))(cid:3) = E

D(α) − D(α + γ

(by Lemma 3 on the local function Gσ(cid:48)

k (α; w, α[k]) approximating the global objective D(α))

(cid:104)
≤ E

D(α) − (1 − γ)D(α) − γ

Gσ(cid:48)
k (∆α(t)

(cid:105)
[k]; w, α[k])

(cid:104)
= γE
D(α) −

Gσ(cid:48)
k (∆α(t)

[k]; w, α[k])

(cid:105)

(cid:105)
∆α[k])

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:104)
= γE
D(α) −

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) +

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) −

(by the notion of quality (12) of the local solver, as in Assumption 1)

(cid:18)

≤ γ

D(α) −

K
(cid:88)

k=1

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) + Θ

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) −

Gσ(cid:48)
k (0; w, α[k])

K
(cid:88)

k=1

(cid:16) K
(cid:88)

k=1

Gσ(cid:48)
k (∆α(t)

(cid:105)
[k]; w, α[k])

K
(cid:88)

k=1

K
(cid:88)

k=1
(cid:124)

(cid:123)(cid:122)
D(α)

(cid:125)

(cid:17)(cid:19)

(26)

= γ(1 − Θ)

D(α) −

(cid:16)

(cid:124)

K
(cid:88)

k=1

Gσ(cid:48)
k (∆α∗

[k]; w, α[k])

(cid:17)

.

(cid:123)(cid:122)
C

(cid:125)

Adding vs. Averaging in Distributed Primal-Dual Optimization

Now, let us upper bound the C term (we will denote by ∆α∗ = (cid:80)K

k=1 ∆α∗

[k]):

((cid:96)∗

i (−αi − ∆α∗

i ) − (cid:96)∗

i (−αi)) +

w(α)T A∆α∗ +

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α∗
[k]

(cid:13)
2
(cid:13)
(cid:13)

((cid:96)∗

i (−αi − s(ui − αi)) − (cid:96)∗

i (−αi)) +

w(α)T As(u − α) +

(2),(9)
=

C

1
n

n
(cid:88)

i=1

n
(cid:88)

≤

1
n

i=1
Strong conv.
≤

1
n

1
n

µ
2

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

(cid:17)

+

1
n

s(cid:96)∗

i (−ui) + (1 − s)(cid:96)∗

i (−αi) −

(1 − s)s(ui − αi)2 − (cid:96)∗

i (−αi)

w(α)T As(u − α)

n
(cid:88)

(cid:16)

1
n

+

i=1

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

=

1
n

n
(cid:88)

(cid:16)

i=1

s(cid:96)∗

i (−ui) − s(cid:96)∗

i (−αi) −

(1 − s)s(ui − αi)2(cid:17)

+

µ
2

1
n

w(α)T As(u − α) +

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

.

The convex conjugate maximal property implies that

i (−ui) = −uiw(α)T xi − (cid:96)i(w(α)T xi).
(cid:96)∗

Moreover, from the deﬁnition of the primal and dual optimization problems (1), (2), we can write the duality gap as

G(α) := P(w(α)) − D(α)

(cid:0)(cid:96)i(xT

j w) + (cid:96)∗

i (−αi) + w(α)T xiαi

(cid:1) .

(1),(2)
=

1
n

n
(cid:88)

i=1


−suiw(α)T xi − s(cid:96)i(w(α)T xi) − s(cid:96)∗

i (−αi) −sw(α)T xiαi + sw(α)T xiαi
(cid:125)

(cid:124)

(cid:123)(cid:122)
0

−

(1 − s)s(ui − αi)2

Hence,

(27)
≤

C

1
n

n
(cid:88)

i=1

=

1
n

n
(cid:88)

i=1

1
n

1
n

+

w(α)T As(u − α) +

As(u − α)[k]

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

(cid:1) +

1
n

n
(cid:88)

(cid:16)

i=1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+

w(α)T As(u − α) +

As(u − α)[k]

(28)= −sG(α) −

(1 − s)s

(cid:107)u − α(cid:107)2 +

(cid:107)A(u − α)[k](cid:107)2.

µ
2

1
n

n
(cid:88)

i=1

σ(cid:48)
2λ

(

s
n

)2

K
(cid:88)

k=1

Now, the claimed improvement bound (15) follows by plugging (29) into (26).

µ
2

µ
2

(cid:0)−s(cid:96)i(w(α)T xi) − s(cid:96)∗

i (−αi) − sw(α)T xiαi

sw(α)T xi(αi − ui) −

(1 − s)s(ui − αi)2(cid:17)

(27)

(28)





(29)

B.4. Proof of Lemma 6

For general convex functions, the strong convexity parameter is µ = 0, and hence the deﬁnition of R(t) becomes

R(t) (16)=

(cid:107)A(u(t) − α(t))[k](cid:107)2

σk(cid:107)(u(t) − α(t))[k](cid:107)2 Lemma 16

≤

σk|Pk|4L2.

K
(cid:88)

k=1

(19)
≤

K
(cid:88)

k=1

K
(cid:88)

k=1

Adding vs. Averaging in Distributed Primal-Dual Optimization

B.5. Proof of Theorem 8

At ﬁrst let us estimate expected change of dual feasibility. By using the main Lemma 5, we have

E[D(α∗) − D(α(t+1))] = E[D(α∗) − D(α(t+1)) + D(α(t)) − D(α(t))]

(15)= D(α∗) − D(α(t)) − γ(1 − Θ)sG(α(t)) + γ(1 − Θ) σ(cid:48)
2λ (

)2R(t)

s
n

(4)= D(α∗) − D(α(t)) − γ(1 − Θ)s(P(w(α(t))) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

)2R(t)

≤ D(α∗) − D(α(t)) − γ(1 − Θ)s(D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

(18)
≤ (1 − γ(1 − Θ)s) (D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ.

s
n

s
n
)2R(t)

s
n

Using (30) recursively we have

E[D(α∗) − D(α(t))] = (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ

(1 − γ(1 − Θ)s)j

s
n

s
n

t−1
(cid:88)

j=0
1 − (1 − γ(1 − Θ)s)t
γ(1 − Θ)s

= (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ

≤ (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + s

4L2σσ(cid:48)
2λn2 .

Choice of s = 1 and t = t0 := max{0, (cid:100)

E[D(α∗) − D(α(t))] ≤ (1 − γ(1 − Θ))t0 (D(α∗) − D(α(0))) +

1

γ(1−Θ) log(2λn2(D(α∗) − D(α(0)))/(4L2σσ(cid:48)))(cid:101)} will lead to
4L2σσ(cid:48)
2λn2 ≤

4L2σσ(cid:48)
2λn2 =

4L2σσ(cid:48)
2λn2 +

4L2σσ(cid:48)
λn2

.

Now, we are going to show that

∀t ≥ t0 : E[D(α∗) − D(α(t))] ≤

λn2(1 + 1

4L2σσ(cid:48)
2 γ(1 − Θ)(t − t0))

.

Clearly, (32) implies that (33) holds for t = t0. Now imagine that it holds for any t ≥ t0 then we show that it also has to
hold for t + 1. Indeed, using

1

s =

1 + 1

2 γ(1 − Θ)(t − t0)

∈ [0, 1]

we obtain

E[D(α∗) − D(α(t+1))]

(30)
≤ (1 − γ(1 − Θ)s) (D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

(30)

(31)

(32)

(33)

(34)

)24L2σ

s
n
+ γ(1 − Θ) σ(cid:48)
2λ (

s
n

)24L2σ

(33)
≤ (1 − γ(1 − Θ)s)

(cid:18) 1 + 1

(cid:18) 1 + 1

(34)=

4L2σσ(cid:48)
λn2

=

4L2σσ(cid:48)
λn2

(cid:124)

λn2(1 + 1

4L2σσ(cid:48)
2 γ(1 − Θ)(t − t0))
2 γ(1 − Θ)(t − t0) − γ(1 − Θ) + γ(1 − Θ) 1
2 γ(1 − Θ)(t − t0))2
(cid:19)
2 γ(1 − Θ)

(1 + 1
2 γ(1 − Θ)(t − t0) − 1
(1 + 1

2 γ(1 − Θ)(t − t0))2

2

.

(cid:19)

(cid:125)

(cid:123)(cid:122)
D

(1 + 1

2 γ(1 − Θ)(t + 1 − t0))(1 + 1
(1 + 1

2 γ(1 − Θ)(t − t0))2

2 γ(1 − Θ)(t − 1 − t0))

(cid:123)(cid:122)
≤1

(cid:125)

Now, we will upperbound D as follows

D =

≤

1 + 1

2 γ(1 − Θ)(t + 1 − t0)

1 + 1

2 γ(1 − Θ)(t + 1 − t0)

(cid:124)

,

1

1

Adding vs. Averaging in Distributed Primal-Dual Optimization

where in the last inequality we have used the fact that geometric mean is less or equal to arithmetic mean.

If α is deﬁned as (21) then we obtain that

E[G(α)] = E

G

1
T −T0

α(t)

≤ 1

T −T0

E

(cid:34)

(cid:32) T −1
(cid:88)

t=T0

(cid:33)(cid:35)

(cid:35)

(cid:16)

α(t)(cid:17)

G

(cid:34) T −1
(cid:88)

t=T0

(15),(18)
≤

1
T −T0

E

(cid:34) T −1
(cid:88)

(cid:18)

1
γ(1 − Θ)s

(D(α(t+1)) − D(α(t))) + 4L2σσ(cid:48)s

2λn2

(cid:19)(cid:35)

=

≤

1
γ(1 − Θ)s
1
γ(1 − Θ)s

t=T0
1
T − T0
1
T − T0

(cid:104)
D(α(T )) − D(α(T0))

(cid:105)

E

+ 4L2σσ(cid:48)s
2λn2

(cid:105)
(cid:104)
D(α∗) − D(α(T0))

E

+ 4L2σσ(cid:48)s
2λn2

.

Now, if T ≥ (cid:100)

1

γ(1−Θ) (cid:101) + T0 such that T0 ≥ t0 we obtain
(cid:18)

E[G(α)]

(35),(33)
≤

1
γ(1 − Θ)s
(cid:18)

1
T − T0
1
γ(1 − Θ)s

4L2σσ(cid:48)
λn2

=

λn2(1 + 1
1
T − T0

1 + 1

4L2σσ(cid:48)

1

2 γ(1 − Θ)(T0 − t0))

2 γ(1 − Θ)(T0 − t0)

4L2σσ(cid:48)s
2λn2

(cid:19)

+

(cid:19)

s
2

+

.

s =

1
(T − T0)γ(1 − Θ)

∈ [0, 1]

E[G(α)]

(36),(37)
≤

(cid:18)

4L2σσ(cid:48)
λn2

1 + 1

2 γ(1 − Θ)(T0 − t0)

+

1
(T − T0)γ(1 − Θ)

1
2

(cid:19)

.

To have right hand side of (38) smaller then (cid:15)G it is sufﬁcient to choose T0 and T such that
(cid:18)

(cid:19)

1

1

Choosing

gives us

Hence of if

4L2σσ(cid:48)
λn2

1 + 1
(cid:18)

2 γ(1 − Θ)(T0 − t0)
1
1
2
(T − T0)γ(1 − Θ)

(cid:19)

4L2σσ(cid:48)
λn2

≤

≤

1
2

1
2

(cid:15)G,

(cid:15)G.

t0 +

2
γ(1 − Θ)

(cid:19)

− 1

(cid:18) 8L2σσ(cid:48)
λn2(cid:15)G
4L2σσ(cid:48)
λn2(cid:15)Gγ(1 − Θ)

≤ T0,

≤ T,

T0 +

then (39) and (40) are satisﬁed.

B.6. Proof of Theorem 10

If the function (cid:96)i(.) is (1/µ)-smooth then (cid:96)∗

i (.) is µ-strongly convex with respect to the (cid:107) · (cid:107) norm. From (16) we have

(cid:88)K

k=1

(cid:88)K

R(t) (16)= − λµn(1−s)

(cid:107)u(t) − α(t)(cid:107)2 +

σ(cid:48)s

(cid:107)A(u(t) − α(t))[k](cid:107)2

(19)
≤ − λµn(1−s)

σ(cid:48)s

(cid:107)u(t) − α(t)(cid:107)2 +

≤ − λµn(1−s)

(cid:16)

σ(cid:48)s
− λµn(1−s)

=

(cid:107)u(t) − α(t)(cid:107)2 + σmax
(cid:17)

σ(cid:48)s + σmax

(cid:107)u(t) − α(t)(cid:107)2.

σk(cid:107)u(t) − α(t)

[k](cid:107)2
(cid:107)u(t) − α(t)

k=1
(cid:88)K

[k](cid:107)2

k=1

(35)

(36)

(37)

(38)

(39)

(40)

(41)

Adding vs. Averaging in Distributed Primal-Dual Optimization

If we plug

λµn

s =

λµn + σmaxσ(cid:48) ∈ [0, 1]

(42)

into (41) we obtain that ∀t : R(t) ≤ 0. Putting the same s into (15) will give us

E[D(α(t+1)) − D(α(t))]

≥ γ(1 − Θ)

(15),(42)

λµn

λµn + σmaxσ(cid:48) G(α(t)) ≥ γ(1 − Θ)

λµn + σmaxσ(cid:48) D(α∗) − D(α(t)).

(43)

λµn

Using the fact that E[D(α(t+1)) − D(α(t))] = E[D(α(t+1)) − D(α∗)] + D(α∗) − D(α(t)) we have

E[D(α(t+1)) − D(α∗)] + D(α∗) − D(α(t))

(43)
≥ γ(1 − Θ)

λµn

λµn + σmaxσ(cid:48) D(α∗) − D(α(t))

which is equivalent with

E[D(α∗) − D(α(t+1))] ≤

1 − γ(1 − Θ)

D(α∗) − D(α(t)).

(44)

(cid:18)

(cid:19)

λµn
λµn + σmaxσ(cid:48)

Therefore if we denote by (cid:15)(t)

D = D(α∗) − D(α(t)) we have that

(cid:18)

E[(cid:15)(t)
D ]

(44)
≤

1 − γ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)t

(cid:18)

(cid:15)(0)
D

(24)
≤

1 − γ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)t

(cid:18)

≤ exp

−tγ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)

.

The right hand side will be smaller than some (cid:15)D if

t ≥

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

log

1
(cid:15)D

.

Moreover, to bound the duality gap, we have

γ(1 − Θ)

λµn

λµn + σmaxσ(cid:48) G(α(t))

(43)
≤ E[D(α(t+1)) − D(α(t))] ≤ E[D(α∗) − D(α(t))].

Therefore G(α(t)) ≤

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

(cid:15)(t)
D . Hence if (cid:15)D ≤ γ(1 − Θ)

λµn

λµn+σmaxσ(cid:48) (cid:15)G then G(α(t)) ≤ (cid:15)G. Therefore after

t ≥

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

(cid:18)

log

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

1
(cid:15)G

(cid:19)

iterations we have obtained a duality gap less than (cid:15)G.

B.7. Proof of Theorem 13

Because (cid:96)i are (1/µ)-smooth then functions (cid:96)∗
i are µ strongly convex with respect to the norm (cid:107) · (cid:107). The proof is based on
techniques developed in recent coordinate descent papers, including (Richt´arik & Tak´aˇc, 2014; 2013; Richt´arik & Tak´aˇc,
2015; Tappenden et al., 2015; Mareˇcek et al., 2014; Fercoq & Richt´arik, 2013; Lu & Xiao, 2013; Fercoq et al., 2014;
Qu & Richt´arik, 2014; Qu et al., 2014) (Efﬁcient accelerated variants were considered in (Fercoq & Richt´arik, 2013;
Shalev-Shwartz & Zhang, 2013a)).
First, let us deﬁne the function F (ζ) : Rnk → R as F (ζ) := −Gσ(cid:48)
ζiei; w, α[k]). This function can be written in
two parts F (ζ) = Φ(ζ)+f (ζ). The ﬁrst part denoted by Φ(ζ) = 1
(cid:96)∗
i (−αi −ζi) is strongly convex with convexity
n
parameter µ
n with respect to the standard Euclidean norm. In our application, we think of the ζ variable collecting the local
dual variables ∆α[k].

k ((cid:80)
(cid:80)

i∈Pk

i∈Pk

The second part we will denote by f (ζ) = 1
λ
K
to show that the gradient of f is coordinate-wise Lipschitz continuous with Lipschitz constant σ(cid:48)
standard Euclidean norm.

2 (cid:107)w(α)(cid:107)2 + 1

w(α)T xiζi + λ

2 σ(cid:48)

i∈Pk

n

1

(cid:80)

λ2n2 (cid:107) (cid:80)

i∈Pk

xiζi(cid:107)2. It is easy
λn2 rmax with respect to the

Adding vs. Averaging in Distributed Primal-Dual Optimization

Following the proof of Theorem 20 in (Richt´arik & Tak´aˇc, 2015), we obtain that

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h+1)

[k]

; w, α[k])] ≤

1 −

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

(cid:17)
[k] ; w, α[k])

(cid:32)

(cid:18)

=

1 −

(cid:16)

(cid:33)

1 + µnλ
σ(cid:48)rmax
µnλ
σ(cid:48)rmax
λnµ
σ(cid:48)rmax + λnµ

1
nk

1
nk

(cid:19) (cid:16)

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])

(cid:17)

.

Over all steps up to step h, this gives

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])] ≤

(cid:18)

1 −

1
nk

λnµ
σ(cid:48)rmax + λnµ

(cid:19)h (cid:16)

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (0; w, α[k])

(cid:17)

.

Therefore, choosing H as in the assumption of our Theorem, given in Equation (22), we are guaranteed that
(cid:16)

(cid:17)H

1 − 1
nk

λnµ
σ(cid:48)rmax+λnµ

≤ Θ, as desired.

B.8. Proof of Theorem 14

Similarly as in the proof of Theorem 13 we deﬁne a composite function F (ζ) = f (ζ) + Φ(ζ). However, in this case func-
tions (cid:96)∗
i are not guaranteed to be strongly convex. However, the ﬁrst part has still a coordinate-wise Lipschitz continuous
gradient with constant σ(cid:48)
λn2 rmax with respect to the standard Euclidean norm. Therefore from Theorem 3 in (Tappenden
et al., 2015) we have that

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])] ≤

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (0; w, α[k]) +

(cid:18)

nk
nk + h

1
2

σ(cid:48)rmax
λn2 (cid:107)∆α∗

[k](cid:107)2

(cid:19)

.

(45)

Now, choice of h = H from (23) is sufﬁcient to have the right hand side of (45) to be ≤ Θ(cid:0)Gσ(cid:48)
k (0; w, α[k])(cid:1).
Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) −

C. Relationship of DisDCA to COCOA+

We are indebted to Ching-pei Lee for showing the following relationship between the practical variant of DisDCA (Yang,
2013), and COCOA+ when SDCA is chosen as the local solver:

Considering the practical variant of DisDCA (DisDCA-p, see Figure 2 in (Yang, 2013)) using the scaling parameter scl =
K, the following holds:
K . If within the COCOA+
Lemma 18. Assume that the dataset is partitioned equally between workers, i.e. ∀k : nk = n
framework, SDCA is used as a local solver, and the subproblems are formulated using our shown “safe” (but pessimistic)
upper bound of σ(cid:48) = K, with aggregation parameter γ = 1 (adding), then the COCOA+ framework reduces exactly to the
DisDCA-p algorithm.

Proof. (Due to Ching-pei Lee, with some reformulations). As deﬁned in (9), the data-local subproblem solved by each
machine in COCOA+ is deﬁned as

max
∆α[k]∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k])

where

Gσ(cid:48)
k (∆α[k]; w, α[k]) := −

(cid:96)∗
i (−αi − (∆α[k])i) −

(cid:107)w(cid:107)2 −

wT A∆α[k] −

1
K

λ
2

1
n

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

.

1
n

(cid:88)

i∈Pk

We rewrite the local problem by scaling with n, and removing the constant regularizer term 1
K

λ

2 (cid:107)w(cid:107)2, i.e.

˜Gσ(cid:48)
k (∆α[k]; w) := −

i (−αj − (∆α[k])j) − wT A∆α[k] −
(cid:96)∗

σ(cid:48)
2λn

(cid:13)
(cid:13)
(cid:13)A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

.

(46)

(cid:88)

j∈Pk

Adding vs. Averaging in Distributed Primal-Dual Optimization

For the correspondence of interest, we now restrict to single coordinate updates in the local solver. In other words, the
local solver optimizes exactly one coordinate i ∈ Pk at a time. To relate the single coordinate update to the set of local
variables, we will use the notation

∆α[k] =: ∆αprev

[k] + δei ,

so that ∆αprev

[k] are the previous local variables, and ∆α[k] will be the updated ones.

From now on, we will consider the special case of COCOA+ when the quadratic upper bound parameter is chosen as the
“safe” value σ(cid:48) = K, combined with adding as the aggregation, i.e. γ = 1.
Now if the local solver within COCOA+ is chosen as LOCALSDCA, then one local step on the subproblem (9) will
calculate the following coordinate update. Recall that A = [x1, x2, . . . , xn] ∈ Rd×n.

δ(cid:63) := arg max

˜Gσ(cid:48)
k (∆α[k]; w)

δ∈R

which – because it is only affecting one single coordinate, employing (47) – can be expressed as

δ(cid:63) := arg max

−(cid:96)∗

i (−(αi + (∆αprev

[k] )i + δ)) − δxT

i w −

δxT

i A∆αprev

[k] −

δ2(cid:107)xi(cid:107)2
2

−(cid:96)∗

i (−(αi + (∆αprev

[k] )i + δ)) − δxT

i

δ2(cid:107)xi(cid:107)2
2

(49)

δ∈R

= arg max
δ∈R

K
λn

(cid:16)

w +

(cid:124)

K
λn

A∆αprev
[k]
(cid:123)(cid:122)
(cid:125)
=:ulocal

(cid:17)

−

K
2λn
K
2λn

From this formulation, it is apparent that single coordinate local solvers should maintain their locally updated version of
the current primal parameters, which we here denote as

(47)

(48)

(50)

ulocal = w +

K
λn

A∆αprev
[k] .

In the practical variant of DisDCA, the summarized local primal updates are ∆ulocal = 1
λnk
nk = n/K for K being the number of machines, this means the local ulocal update of DisDCA-p is

A∆α[k]. For the balanced case

∆α(cid:63)

i := arg max
∆αi∈R

−(cid:96)∗

i (−(αi + ∆αi)) − ∆αixT

i ulocal −

(∆αi)2(cid:107)xi(cid:107)2
2 .

(51)

K
2λn

It is not hard to show that during one outer round, the evolution of the local dual variables ∆α[k] is the same in both
methods, such that they will also have the same trajectory of ulocal. This requires some care if the same coordinate is
sampled more than once in a round, which can happen in LOCALSDCA within COCOA+ and also in DisDCA-p.

Discussion.

In the view of the above lemma, we will summarize the connection of the two methods as follows:

• COCOA/+ is Not an Algorithm. In contrast, it is a framework which allows to use any local solver to perform
approximate steps on the local subproblem. This additional level of abstraction (from the deﬁnition of such local
subproblems in (9)) is the ﬁrst to allow reusability of any fast/tuned and problem speciﬁc single machine solvers,
while decoupling this from the distributed algorithmic scheme, as presented in Algorithm 1.

Concerning the choice of local solver to be used within COCOA/+, SDCA is not the fastest known single machine
solver for most applications. Much recent research has shown improvements on SDCA (Shalev-Shwartz & Zhang,
2013c), such as accelerated variants (Shalev-Shwartz & Zhang, 2013b) and other approaches including variance re-
duction, methods incorporating second-order information, and importance sampling. In this light, we encourage the
user of the COCOA or COCOA+ framework to plug in the best and most recent solver available for their particular
local problem (within Algorithm 1), which is not necessarily SDCA. This choice should be made explicit especially
when comparing algorithms. Our presented convergence theory from Section 4 will still cover these choices, since it
only depends on the relative accuracy Θ of the chosen local solver.

Adding vs. Averaging in Distributed Primal-Dual Optimization

• COCOA+ is Theoretically Safe, while still Adaptive to the Data. The general deﬁnition of the local subproblems,
and therefore the treatment of the varying separable bound on the objective – quantiﬁed by σ(cid:48) – allows our framework
to adapt to the difﬁculty of the data partition and still give convergence results. The data-dependent measure σ(cid:48) is
fully decoupled from what the user of the framework prefers to employ as a local solver (see also the comment below
that COCOA is not a coordinate solver).
The safe upper bound σ(cid:48) = K is worst-case pessimistic, for the convergence theory to still hold in all cases, when the
updates are added. Using additional knowledge from the input data, better bounds and therefore better step-sizes can
be achieved in COCOA+. An example when σ(cid:48) can be safely chosen much smaller is when the data-matrix satisﬁes
strong row/column sparsity, see e.g. Lemma 1 in (Richt´arik & Tak´aˇc, 2013).

• Obtaining DisDCA-p as a Special Case. As shown in Lemma 18 above, we have that if in COCOA+, if SDCA is
used as the local solver and the pessimistic upper bound of σ(cid:48) = K is used and, moreover, the dataset is partitioned
K , then the COCOA+ framework reduces exactly to the DisDCA-p algorithm by (Yang, 2013).
equally, i.e. ∀k : nk = n
The correspondence breaks down if the subproblem parameter is chosen to a practically good value σ(cid:48) (cid:54)= K. Also, as
noted above, SDCA is often not the best local solver currently available. In our above experiments, SDCA was used
just for demonstration purposes and ease of comparison. Furthermore, the data partition might often be unbalanced
in practical applications.
While both DisDCA-p and COCOA are special cases of COCOA+, we note that DisDCA-p can not be recovered as a
special case of the original COCOA framework (Jaggi et al., 2014).

• COCOA/+ are Not Coordinate Methods. Despite the original name being motivated from this special case, COCOA
and COCOA+ are not coordinate methods. In fact, COCOA+ as presented here for the adding case (γ = 1) is much
more closely related to a batch method applied to the dual, using a block-separable proximal term, as following
from our new subproblem formulation (9), depending on σ(cid:48). See also the remark in Section 6. The framework here
(Algorithm 1) gives more generality, as the used local solver is not restricted to be a coordinate-wise one. In fact the
framework allows to translate recent and future improvements of single machine solvers directly to the distributed
setting, by employing them within Algorithm 1. DisDCA-p works very well for several applications, but is restricted
to using local coordinate ascent (SDCA) steps.

• Theoretical Convergence Results. While DisDCA-p (Yang, 2013) was proposed without theoretical justiﬁcation
(hence the nomenclature), the main contribution in the paper here – apart from the arbitrary local solvers – is the
convergence analysis for the framework. The theory proposed in (Yang et al., 2013) is given only for the setting of
orthogonal partitions, i.e., when σ(cid:48) = 1 and the problems become trivial to distribute given the orthogonality of data
between the workers.

The theoretical analysis here gives convergence rates applying for Algorithm 1 when using arbitrary local solvers,
and inherits the performance of the local solver. As a special case, we obtain the ﬁrst theoretical justiﬁcation and
convergence rates for original COCOA in the case of general convex objective, as well as for the special case of
DisDCA-p for both general convex and smooth convex objectives.

Adding vs. Averaging in Distributed Primal-Dual Optimization

5
1
0
2
 
l
u
J
 
3
 
 
]

G
L
.
s
c
[
 
 
2
v
8
0
5
3
0
.
2
0
5
1
:
v
i
X
r
a

Chenxin Ma∗
Industrial and Systems Engineering, Lehigh University, USA

Virginia Smith∗
University of California, Berkeley, USA

Martin Jaggi
ETH Z¨urich, Switzerland

Michael I. Jordan
University of California, Berkeley, USA

Peter Richt´arik
School of Mathematics, University of Edinburgh, UK

Martin Tak´aˇc
Industrial and Systems Engineering, Lehigh University, USA

∗Authors contributed equally.

CHM514@LEHIGH.EDU

VSMITH@BERKELEY.EDU

JAGGI@INF.ETHZ.CH

JORDAN@CS.BERKELEY.EDU

PETER.RICHTARIK@ED.AC.UK

TAKAC.MT@GMAIL.COM

Abstract

1. Introduction

Distributed optimization methods for large-scale
machine learning suffer from a communication
bottleneck. It is difﬁcult to reduce this bottleneck
while still efﬁciently and accurately aggregating
partial work from different machines. In this pa-
per, we present a novel generalization of the re-
cent communication-efﬁcient primal-dual frame-
work (COCOA) for distributed optimization. Our
framework, COCOA+, allows for additive com-
bination of local updates to the global parame-
ters at each iteration, whereas previous schemes
with convergence guarantees only allow conser-
vative averaging. We give stronger (primal-dual)
convergence rate guarantees for both COCOA as
well as our new variants, and generalize the the-
ory for both methods to cover non-smooth con-
vex loss functions. We provide an extensive ex-
perimental comparison that shows the markedly
improved performance of COCOA+ on several
real-world distributed datasets, especially when
scaling up the number of machines.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

With the wide availability of large datasets that exceed
the storage capacity of single machines, distributed opti-
mization methods for machine learning have become in-
creasingly important. Existing methods require signiﬁcant
communication between workers, frequently equaling the
amount of local computation (or reading of local data). As
a result, distributed machine learning suffers signiﬁcantly
from a communication bottleneck on real world systems,
where communication is typically several orders of magni-
tudes slower than reading data from main memory.

In this work we focus on optimization problems with em-
pirical loss minimization structure, i.e., objectives that are
a sum of the loss functions of each datapoint. This in-
cludes the most commonly used regularized variants of
linear regression and classiﬁcation methods.
For this
class of problems, the recently proposed COCOA approach
(Yang, 2013; Jaggi et al., 2014) develops a communication-
efﬁcient primal-dual scheme that targets the communica-
tion bottleneck, allowing more computation on data-local
subproblems native to each machine before communica-
tion. By appropriately choosing the amount of local com-
putation per round, this framework allows one to control
the trade-off between communication and local computa-
tion based on the systems hardware at hand.

However, the performance of COCOA (as well as related
primal SGD-based methods) is signiﬁcantly reduced by the

Adding vs. Averaging in Distributed Primal-Dual Optimization

need to average updates between all machines. As the
number of machines K grows, the updates get diluted and
slowed by 1/K, e.g., in the case where all machines ex-
cept one would have already reached the solutions of their
respective partial optimization tasks. On the other hand, if
the updates are instead added, the algorithms can diverge,
as we will observe in the practical experiments below.

To address both described issues, in this paper we develop
a novel generalization of the local COCOA subproblems
assigned to each worker, making the framework more pow-
erful in the following sense: Without extra computational
cost, the set of locally computed updates from the mod-
iﬁed subproblems (one from each machine) can be com-
bined more efﬁciently between machines. The proposed
COCOA+ updates can be aggressively added (hence the
‘+’-sufﬁx), which yields much faster convergence both in
practice and in theory. This difference is particularly sig-
niﬁcant as the number of machines K becomes large.

1.1. Contributions

Strong Scaling. To our knowledge, our framework is the
ﬁrst to exhibit favorable strong scaling for the class of prob-
lems considered, as the number of machines K increases
and the data size is kept ﬁxed. More precisely, while the
convergence rate of COCOA degrades as K is increased,
the stronger theoretical convergence rate here is – in the
worst case – independent of K. Our experiments in Section
7 conﬁrm the improved speed of convergence. Since the
number of communicated vectors is only one per round and
worker, this favorable scaling might be surprising. Indeed,
for existing methods, splitting data among more machines
generally increases communication requirements (Shamir
& Srebro, 2014), which can severely affect overall runtime.

Theoretical Analysis of Non-Smooth Losses. While the
existing analysis for COCOA in (Jaggi et al., 2014) only
covered smooth loss functions, here we extend the class
of functions where the rates apply, additionally covering,
e.g., Support Vector Machines and non-smooth regression
variants. We provide a primal-dual convergence rate for
both COCOA as well as our new method COCOA+ in the
case of general convex (L-Lipschitz) losses.

tight

Primal-Dual Convergence Rate. Furthermore, we addi-
tionally strengthen the rates by showing stronger primal-
dual convergence for both algorithmic frameworks, which
are almost
to their objective-only counterparts.
Primal-dual rates for COCOA had not previously been ana-
lyzed in the general convex case. Our primal-dual rates al-
low efﬁcient and practical certiﬁcates for the optimization
quality, e.g., for stopping criteria. The new rates apply to
both smooth and non-smooth losses, and for both COCOA
as well as the extended COCOA+.

Arbitrary Local Solvers. COCOA as well as COCOA+
allow the use of arbitrary local solvers on each machine.

Experimental Results. We provide a thorough experi-
mental comparison with competing algorithms using sev-
eral real-world distributed datasets. Our practical results
conﬁrm the strong scaling of COCOA+ as the number of
machines K grows, while competing methods, including
the original COCOA, slow down signiﬁcantly with larger
K. We implement all algorithms in Spark, and our code is
publicly available at: github.com/gingsmith/cocoa.

1.2. History and Related Work

While optimal algorithms for the serial (single machine)
case are already well researched and understood, the liter-
ature in the distributed setting is relatively sparse. In par-
ticular, details on optimal trade-offs between computation
and communication, as well as optimization or statistical
accuracy, are still widely unclear. For an overview over
this currently active research ﬁeld, we refer the reader to
(Balcan et al., 2012; Richt´arik & Tak´aˇc, 2013; Duchi et al.,
2013; Yang, 2013; Liu & Wright, 2014; Fercoq et al., 2014;
Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al.,
2014; Zhang & Lin, 2015; Qu & Richt´arik, 2014) and the
references therein. We provide a detailed comparison of
our proposed framework to the related work in Section 6.

2. Setup

We consider regularized empirical loss minimization prob-
lems of the following well-established form:
λ
2

P(w) :=

min
w∈Rd

i w) +

(cid:96)i(xT

(cid:107)w(cid:107)2

n
(cid:88)

1
n

(1)

(cid:40)

(cid:41)

i=1

i=1 ⊂ Rd represent the training data
Here the vectors {xi}n
examples, and the (cid:96)i(.) are arbitrary convex real-valued
loss functions (e.g., hinge loss), possibly depending on la-
bel information for the i-th datapoints. The constant λ > 0
is the regularization parameter.

The above class includes many standard problems of wide
interest in machine learning, statistics, and signal process-
ing, including support vector machines, regularized linear
and logistic regression, ordinal regression, and others.

Dual Problem, and Primal-Dual Certiﬁcates. The con-
jugate dual of (1) takes following form:

(cid:40)

max
α∈Rn

D(α) := −

(cid:96)∗
j (−αj) −

1
n

n
(cid:88)

j=1

2 (cid:41)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Aα
λn

(cid:13)
(cid:13)
(cid:13)
(cid:13)

λ
2

(2)

Here the data matrix A = [x1, x2, . . . , xn] ∈ Rd×n col-
lects all data-points as its columns, and (cid:96)∗
j is the conjugate
function to (cid:96)j. See, e.g., (Shalev-Shwartz & Zhang, 2013c)
for several concrete applications.

Adding vs. Averaging in Distributed Primal-Dual Optimization

It is possible to assign for any dual vector α ∈ Rn a corre-
sponding primal feasible point

w(α) = 1

λn Aα

The duality gap function is then given by:

G(α) := P(w(α)) − D(α)

(3)

(4)

By weak duality, every value D(α) at a dual candidate α
provides a lower bound on every primal value P(w). The
duality gap is therefore a certiﬁcate on the approxima-
tion quality: The distance to the unknown true optimum
P(w∗) must always lie within the duality gap, i.e., G(α) =
P(w) − D(α) ≥ P(w) − P(w∗) ≥ 0.

In large-scale machine learning settings like those consid-
ered here, the availability of such a computable measure of
approximation quality is a signiﬁcant beneﬁt during train-
ing time. Practitioners using classical primal-only methods
such as SGD have no means by which to accurately detect
if a model has been well trained, as P (w∗) is unknown.

Classes of Loss-Functions. To simplify presentation, we
assume that all loss functions (cid:96)i are non-negative, and
(cid:96)i(0) ≤ 1

(5)

∀i

Deﬁnition 1 (L-Lipschitz continuous loss). A function (cid:96)i :
R → R is L-Lipschitz continuous if ∀a, b ∈ R, we have

|(cid:96)i(a) − (cid:96)i(b)| ≤ L|a − b|
(6)
Deﬁnition 2 ((1/µ)-smooth loss). A function (cid:96)i : R → R
is (1/µ)-smooth if it is differentiable and its derivative is
(1/µ)-Lipschitz continuous, i.e., ∀a, b ∈ R, we have

|(cid:96)(cid:48)

i(a) − (cid:96)(cid:48)

i(b)| ≤

|a − b|

(7)

1
µ

3. The COCOA+ Algorithm Framework
In this section we present our novel COCOA+ frame-
work. COCOA+ inherits the many beneﬁts of CoCoA as
it remains a highly ﬂexible and scalable, communication-
efﬁcient framework for distributed optimization. COCOA+
differs algorithmically in that we modify the form of the lo-
cal subproblems (9) to allow for more aggressive additive
updates (as controlled by γ). We will see that these changes
allow for stronger convergence guarantees as well as im-
proved empirical performance. Proofs of all statements in
this section are given in the supplementary material.

Data Partitioning. We write {Pk}K
k=1 for the given par-
tition of the datapoints [n] := {1, 2, . . . , n} over the K
worker machines. We denote the size of each part by
nk = |Pk|. For any k ∈ [K] and α ∈ Rn we use the
notation α[k] ∈ Rn for the vector

(α[k])i :=

(cid:40)

0,
if i /∈ Pk,
αi, otherwise.

Local Subproblems in COCOA+. We can deﬁne a data-
local subproblem of the original dual optimization problem
(2), which can be solved on machine k and only requires
accessing data which is already available locally, i.e., data-
points with i ∈ Pk. More formally, each machine k is as-
signed the following local subproblem, depending only on
the previous shared primal vector w ∈ Rd, and the change
in the local dual variables αi with i ∈ Pk:

max
∆α[k]∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k])

(8)

where

1
n

(cid:88)

i∈Pk
λ
2

Gσ(cid:48)
k (∆α[k]; w, α[k]) := −

(cid:96)∗
i (−αi − (∆α[k])i)

−

1
K

λ
2

1
n

(cid:107)w(cid:107)2 −

wT A∆α[k] −

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

1
λn

A∆α[k]

(9)

(cid:13)
2
(cid:13)
(cid:13)

Interpretation. The above deﬁnition of the local objec-
tive functions Gσ(cid:48)
k are such that they closely approximate
the global dual objective D, as we vary the ‘local’ vari-
able ∆α[k], in the following precise sense:
Lemma 3. For any dual α, ∆α ∈ Rn, primal w = w(α)
and real values γ, σ(cid:48) satisfying (11), it holds that

(cid:16)

D

α + γ

K
(cid:88)

k=1

(cid:17)
∆α[k]
K
(cid:88)

+γ

≥ (1 − γ)D(α)

Gσ(cid:48)
k (∆α[k]; w, α[k])

(10)

k=1
The role of the parameter σ(cid:48) is to measure the difﬁculty of
the given data partition. For our purposes, we will see that
it must be chosen not smaller than

σ(cid:48) ≥ σ(cid:48)

min := γ max
α∈Rn

(cid:107)Aα(cid:107)2
k=1 (cid:107)Aα[k](cid:107)2

(cid:80)K

(11)

In the following lemma, we show that this parameter can
be upper-bounded by γK, which is trivial to calculate for
all values γ ∈ R. We show experimentally (Section 7) that
this safe upper bound for σ(cid:48) has a minimal effect on the
overall performance of the algorithm. Our main theorems
below show convergence rates dependent on γ ∈ [ 1
K , 1],
which we refer to as the aggregation parameter.
Lemma 4. The choice of σ(cid:48) := γK is valid for (11), i.e.,

γK ≥ σ(cid:48)

min

Notion of Approximation Quality of the Local Solver.
Assumption 1 (Θ-approximate solution). We assume that
there exists Θ ∈ [0, 1) such that ∀k ∈ [K], the local solver
at any outer iteration t produces a (possibly) randomized
approximate solution ∆α[k], which satisﬁes
[k]; w, α[k]) − Gσ(cid:48)

(12)

E(cid:2)Gσ(cid:48)
k (∆α∗
(cid:16)
Gσ(cid:48)
k (∆α∗

≤ Θ

k (∆α[k]; w, α[k])(cid:3)
(cid:17)

k (0; w, α[k])

,

[k]; w, α[k]) − Gσ(cid:48)

Adding vs. Averaging in Distributed Primal-Dual Optimization

where
∆α∗

[k] ∈ arg max
∆α∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k]) ∀k ∈ [K] (13)

We are now ready to describe the COCOA+ framework,
shown in Algorithm 1. The crucial difference compared
to the existing COCOA algorithm (Jaggi et al., 2014) is the
more general local subproblem, as deﬁned in (9), as well as
the aggregation parameter γ. These modiﬁcations allow the
option of directly adding updates to the global vector w.

Algorithm 1 COCOA+ Framework
1: Input: Datapoints A distributed according to parti-
tion {Pk}K
k=1. Aggregation parameter γ ∈ (0, 1],
subproblem parameter σ(cid:48) for the local subproblems
Gσ(cid:48)
k (∆α[k]; w, α[k]) for each k ∈ [K].
Starting point α(0) := 0 ∈ Rn, w(0) := 0 ∈ Rd.

2: for t = 0, 1, 2, . . . do
3:

for k ∈ {1, 2, . . . , K} in parallel over computers
do

call the local solver, computing a Θ-approximate
solution ∆α[k] of the local subproblem (9)
update α(t+1)
[k]
return ∆wk := 1

:= α(t)
λn A∆α[k]

[k] + γ ∆α[k]

end for
reduce w(t+1) := w(t) + γ (cid:80)K

k=1 ∆wk.

(14)

4:

5:

6:
7:
8:

9: end for

4. Convergence Guarantees

Before being able to state our main convergence results,
we introduce some useful quantities and the following
main lemma characterizing the effect of iterations of Al-
gorithm 1, for any chosen internal local solver.
Lemma 5. Let (cid:96)∗
i be strongly1 convex with convexity pa-
rameter µ ≥ 0 with respect to the norm (cid:107)·(cid:107), ∀i ∈ [n]. Then
for all iterations t of Algorithm 1 under Assumption 1, and
any s ∈ [0, 1], it holds that

E[D(α(t+1)) − D(α(t))] ≥

γ(1 − Θ)

sG(α(t)) −

(cid:16)

(15)
R(t)(cid:17)

,

σ(cid:48)
2λ

(cid:0) s
n

(cid:1)2

where

R(t) := − λµn(1−s)
+ (cid:80)K

(cid:107)u(t) − α(t)(cid:107)2
k=1(cid:107)A(u(t) − α(t))[k](cid:107)2,

σ(cid:48)s

(16)

for u(t) ∈ Rn with

− u(t)

i ∈ ∂(cid:96)i(w(α(t))T xi).

(17)

1Note that the case of weakly convex (cid:96)∗

i (.) is explicitly al-

lowed here as well, as the Lemma holds for the case µ = 0.

The following Lemma provides a uniform bound on R(t):
Lemma 6. If (cid:96)i are L-Lipschitz continuous for all i ∈ [n],
then

∀t : R(t) ≤ 4L2

σknk

,

K
(cid:88)

where

k=1
(cid:124)

(cid:125)

(cid:123)(cid:122)
=:σ
(cid:107)Aα[k](cid:107)2
(cid:107)α[k](cid:107)2 .

σk := max
α[k]∈Rn

(18)

(19)

Remark 7. If all data-points xi are normalized such that
(cid:107)xi(cid:107) ≤ 1 ∀i ∈ [n], then σk ≤ |Pk| = nk. Furthermore,
if we assume that the data partition is balanced, i.e., that
nk = n/K for all k, then σ ≤ n2/K. This can be used to
bound the constants R(t), above, as R(t) ≤ 4L2n2
K .

4.1. Primal-Dual Convergence for General Convex

Losses

The following theorem shows the convergence for non-
smooth loss functions, in terms of objective values as well
as primal-dual gap. The analysis in (Jaggi et al., 2014) only
covered the case of smooth loss functions.
Theorem 8. Consider Algorithm 1 with Assumption 1. Let
(cid:96)i(·) be L-Lipschitz continuous, and (cid:15)G > 0 be the de-
sired duality gap (and hence an upper-bound on primal
sub-optimality). Then after T iterations, where

(cid:108)
T ≥ T0 + max{

1
γ(1 − Θ)

(cid:109)
,

T0 ≥ t0 +

(cid:18)

2
γ(1 − Θ)
(cid:108)

4L2σσ(cid:48)
λn2(cid:15)Gγ(1 − Θ)
(cid:19)(cid:19)

(cid:18) 8L2σσ(cid:48)
λn2(cid:15)G

− 1

,

+

t0 ≥ max(0,

1

γ(1−Θ) log( 2λn2(D(α∗)−D(α(0)))

4L2σσ(cid:48)

(cid:109)
)

),

},

(20)

we have that the expected duality gap satisﬁes

E[P(w(α)) − D(α)] ≤ (cid:15)G,

at the averaged iterate

α := 1

T −T0

(cid:80)T −1

t=T0+1α(t).

(21)

The following corollary of the above theorem clariﬁes our
main result: The more aggressive adding of the partial up-
dates, as compared averaging, offers a very signiﬁcant im-
provement in terms of total iterations needed. While the
convergence in the ‘adding’ case becomes independent of
the number of machines K, the ‘averaging’ regime shows
the known degradation of the rate with growing K, which is
a major drawback of the original COCOA algorithm. This
important difference in the convergence speed is not a the-
oretical artifact but also conﬁrmed in our practical experi-
ments below for different K, as shown e.g. in Figure 2.

We further demonstrate below that by choosing γ and σ(cid:48)
accordingly, we can still recover the original COCOA al-
gorithm and its rate.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Corollary 9. Assume that all datapoints xi are bounded as
(cid:107)xi(cid:107) ≤ 1 and that the data partition is balanced, i.e. that
nk = n/K for all k. We consider two different possible
choices of the aggregation parameter γ:

• (COCOA Averaging, γ := 1

K ): In this case, σ(cid:48) := 1
is a valid choice which satisﬁes (11). Then using σ ≤
n2/K in light of Remark 7, we have that T iterations
are sufﬁcient for primal-dual accuracy (cid:15)G, with

T ≥ T0 + max{

T0 ≥ t0 +

(cid:18) 2K
1 − Θ
t0 ≥ max(0, (cid:6) K

(cid:108) K

(cid:109)
,

4L2
λ(cid:15)G(1 − Θ)
(cid:19)(cid:19)

},

− 1

,

+

1 − Θ
(cid:18) 8L2
λK(cid:15)G

1−Θ log( 2λ(D(α∗)−D(α(0)))

4KL2

)(cid:7))

Hence the more machines K, the more iterations are
needed (in the worst case).

• (COCOA+ Adding, γ := 1): In this case, the choice of
σ(cid:48) := K satisﬁes (11). Then using σ ≤ n2/K in light
of Remark 7, we have that T iterations are sufﬁcient
for primal-dual accuracy (cid:15)G, with

(cid:108)

1
1 − Θ
(cid:18) 8L2
λ(cid:15)G

T ≥ T0 + max{

(cid:18) 2

T0 ≥ t0 +

1 − Θ
t0 ≥ max(0, (cid:6) 1

(cid:109)
,

4L2
λ(cid:15)G(1 − Θ)

},

(cid:19)(cid:19)

− 1

,

+

1−Θ log( 2λn(D(α∗)−D(α(0)))

4KL2

)(cid:7))

This is signiﬁcantly better than the averaging case.

In practice, we usually have σ (cid:28) n2/K, and hence the
actual convergence rate can be much better than the proven
worst-case bound. Table 1 shows that the actual value of
σ is typically between one and two orders of magnitudes
smaller compared to our used upper-bound n2/K.

Table 1. The ratio of upper-bound n2
the parameter σ, for some real datasets.

K divided by the true value of

K

16

32

64

128

256

512

news
real-sim
rcv1

15.483
42.127
40.138

14.933
36.898
23.827

14.278
30.780
28.204

13.390
23.814
21.792

12.074
16.965
16.339

10.252
11.835
11.099

K

256

512

1024

2048

4096

8192

covtype

17.277

17.260

17.239

16.948

17.238

12.729

4.2. Primal-Dual Convergence for Smooth Losses

The following theorem shows the convergence for smooth
losses, in terms of the objective as well as primal-dual gap.

Theorem 10. Assume the loss functions functions (cid:96)i are
(1/µ)-smooth ∀i ∈ [n]. We deﬁne σmax = maxk∈[K] σk.
Then after T iterations of Algorithm 1, with

T ≥

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

log 1
(cid:15)D

,

it holds that

E[D(α∗) − D(α(T ))] ≤ (cid:15)D.

Furthermore, after T iterations with

T ≥

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

log

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

1
(cid:15)G

(cid:16)

(cid:17)

,

we have the expected duality gap

E[P(w(α(T ))) − D(α(T ))] ≤ (cid:15)G.

The following corollary is analogous to Corollary 9, but
for the case of smooth loses. It again shows that while the
COCOA variant degrades with the increase of the number
of machines K, the COCOA+ rate is independent of K.
Corollary 11. Assume that all datapoints xi are bounded
as (cid:107)xi(cid:107) ≤ 1 and that the data partition is balanced, i.e.,
that nk = n/K for all k. We again consider the same two
different possible choices of the aggregation parameter γ:
K ): In this case, σ(cid:48) :=
1 is a valid choice which satisﬁes (11). Then using
σmax ≤ nk = n/K in light of Remark 7, we have that
T iterations are sufﬁcient for suboptimality (cid:15)D, with

• (COCOA Averaging, γ := 1

T ≥ 1

1−Θ

λµK+1
λµ

log 1
(cid:15)D

Hence the more machines K, the more iterations are
needed (in the worst case).

• (COCOA+ Adding, γ := 1): In this case, the choice
of σ(cid:48) := K satisﬁes (11). Then using σmax ≤ nk =
n/K in light of Remark 7, we have that T iterations
are sufﬁcient for suboptimality (cid:15)D, with

T ≥ 1

1−Θ

λµ+1

λµ log 1
(cid:15)D

This is signiﬁcantly better than the averaging case.
Both rates hold analogously for the duality gap.

4.3. Comparison with Original COCOA

Remark 12. If we choose averaging (γ := 1
K ) for aggre-
gating the updates, together with σ(cid:48) := 1, then the result-
ing Algorithm 1 is identical to COCOA analyzed in (Jaggi
et al., 2014). However, they only provide convergence for
smooth loss functions (cid:96)i and have guarantees for dual sub-
optimality and not the duality gap. Formally, when σ(cid:48) = 1,
the subproblems (9) will differ from the original dual D(.)
only by an additive constant, which does not affect the local
optimization algorithms used within COCOA.

5. SDCA as an Example Local Solver

We have shown convergence rates for Algorithm 1, depend-
ing solely on the approximation quality Θ of the used local

Adding vs. Averaging in Distributed Primal-Dual Optimization

solver (Assumption 1). Any chosen local solver in each
round receives the local α variables as an input, as well as
a shared vector w (3)= w(α) being compatible with the last
state of all global α ∈ Rn variables.

As an illustrative example for a local solver, Algorithm 2
below summarizes randomized coordinate ascent (SDCA)
applied on the local subproblem (9). The following two
Theorems (13, 14) characterize the local convergence for
both smooth and non-smooth functions. In all the results
we will use rmax := maxi∈[n] (cid:107)xi(cid:107)2.

Algorithm 2 LOCALSDCA (w, α[k], k, H)
1: Input: α[k], w = w(α)
2: Data: Local {(xi, yi)}i∈Pk
3: Initialize: ∆α(0)
[k] := 0 ∈ Rn
4: for h = 0, 1, . . . , H − 1 do
5:

choose i ∈ Pk uniformly at random
Gσ(cid:48)
k (∆α(h)
δ∗
i := arg max

6:

[k] + δiei; w, α[k])

δi∈R
:= ∆α(h)

[k] + δ∗

i ei

7: ∆α(h+1)
[k]
8: end for
9: Output: ∆α(H)
[k]

Theorem 13. Assume the functions (cid:96)i are (1/µ)−smooth
for i ∈ [n]. Then Assumption 1 on the local approximation
quality Θ is satisﬁed for LOCALSDCA as given in Algo-
rithm 2, if we choose the number of inner iterations H as

H ≥ nk

σ(cid:48)rmax + λnµ
λnµ
Theorem 14. Assume the functions (cid:96)i are L-Lipschitz for
i ∈ [n]. Then Assumption 1 on the local approximation
quality Θ is satisﬁed for LOCALSDCA as given in Algo-
rithm 2, if we choose the number of inner iterations H as

1
Θ

(22)

log

.

H ≥ nk

(cid:18) 1 − Θ
Θ

+

σ(cid:48)rmax
2Θλn2

(cid:107)∆α∗
[k](cid:107)2
[k]; .) − Gσ(cid:48)

k (0; .)

(cid:19)

.

Gσ(cid:48)
k (∆α∗

(23)
Remark 15. Between the different regimes allowed in
COCOA+ (ranging between averaging and adding the up-
dates) the computational cost for obtaining the required
local approximation quality varies with the choice of σ(cid:48).
From the above worst-case upper bound, we note that the
cost can increase with σ(cid:48), as aggregation becomes more
aggressive. However, as we will see in the practical exper-
iments in Section 7 below, the additional cost is negligible
compared to the gain in speed from the different aggrega-
tion, when measured on real datasets.

6. Discussion and Related Work

SGD-based Algorithms. For the empirical loss mini-
mization problems of interest here, stochastic subgradient

descent (SGD) based methods are well-established. Sev-
eral distributed variants of SGD have been proposed, many
of which build on the idea of a parameter server (Niu et al.,
2011; Liu et al., 2014; Duchi et al., 2013). The downside of
this approach, even when carefully implemented, is that the
amount of required communication is equal to the amount
of data read locally (e.g., mini-batch SGD with a batch size
of 1 per worker). These variants are in practice not compet-
itive with the more communication-efﬁcient methods con-
sidered here, which allow more local updates per round.

One-Shot Communication Schemes. At the other ex-
treme, there are distributed methods using only a single
round of communication, such as (Zhang et al., 2013;
Zinkevich et al., 2010; Mann et al., 2009; McWilliams
et al., 2014). These require additional assumptions on the
partitioning of the data, and furthermore can not guarantee
convergence to the optimum solution for all regularizers, as
shown in, e.g., (Shamir et al., 2014). (Balcan et al., 2012)
shows additional relevant lower bounds on the minimum
number of communication rounds necessary for a given ap-
proximation quality for similar machine learning problems.

Mini-Batch Methods. Mini-batch methods are more
ﬂexible and lie within these two communication vs. com-
putation extremes. However, mini-batch versions of both
SGD and coordinate descent (CD) (Richt´arik & Tak´aˇc,
2013; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu
& Richt´arik, 2014; Qu et al., 2014) suffer from their con-
vergence rate degrading towards the rate of batch gradient
descent as the size of the mini-batch is increased. This fol-
lows because mini-batch updates are made based on the
outdated previous parameter vector w, in contrast to meth-
ods that allow immediate local updates like COCOA. Fur-
thermore, the aggregation parameter for mini-batch meth-
ods is harder to tune, as it can lie anywhere in the order of
mini-batch size. In the COCOA setting, the parameter lies
in the smaller range given by K. Our COCOA+ extension
avoids needing to tune this parameter entirely, by adding.

Methods Allowing Local Optimization. Developing
methods that allow for local optimization requires care-
fully devising data-local subproblems to be solved after
each communication round. (Shamir et al., 2014; Zhang
& Lin, 2015) have proposed distributed Newton-type algo-
rithms in this spirit. However, the subproblems must be
solved to high accuracy for convergence to hold, which is
often prohibitive as the size of the data on one machine is
still relatively large. In contrast, the COCOA framework
(Jaggi et al., 2014) allows using any local solver of weak
local approximation quality in each round. By making use
of the primal-dual structure in the line of work of (Yu et al.,
2012; Pechyony et al., 2011; Yang, 2013; Lee & Roth,
2015), the COCOA and COCOA+ frameworks also allow
more control over the aggregation of updates between ma-

Adding vs. Averaging in Distributed Primal-Dual Optimization

Figure 1. Duality gap vs. the number of communicated vectors, as well as duality gap vs. elapsed time in seconds for two datasets:
Covertype (left, K=4) and RCV1 (right, K=8). Both are shown on a log-log scale, and for three different values of regularization
(λ=1e-4; 1e-5; 1e-6). Each plot contains a comparison of COCOA (red) and COCOA+ (blue), for three different values of H, the
number of local iterations performed per round. For all plots, across all values of λ and H, we see that COCOA+ converges to the
optimal solution faster than COCOA, in terms of both the number of communications and the elapsed time.

chines. The practical variant DisDCA-p proposed in (Yang,
2013) allows additive updates but is restricted to SDCA
updates, and was proposed without convergence guaran-
tees. DisDCA-p can be recovered as a special case of the
COCOA+ framework when using SDCA as a local solver,
if nk = n/K and σ(cid:48) := K, see Appendix C. The theory
presented here also therefore covers that method.

ADMM. An alternative approach to distributed optimiza-
tion is to use the alternating direction method of multipli-
ers (ADMM), as used for distributed SVM training in, e.g.,
(Forero et al., 2010). This uses a penalty parameter balanc-
ing between the equality constraint w and the optimization
objective (Boyd et al., 2011). However, the known conver-
gence rates for ADMM are weaker than the more problem-
tailored methods mentioned previously, and the choice of
the penalty parameter is often unclear.

Batch Proximal Methods.
In spirit, for the special case
of adding (γ = 1), COCOA+ resembles a batch proximal
method, using the separable approximation (9) instead of
the original dual (2). Known batch proximal methods re-
quire high accuracy subproblem solutions, and don’t allow
arbitrary solvers of weak accuracy Θ such as we do here.

7. Numerical Experiments

We present experiments on several large real-world dis-
tributed datasets. We show that COCOA+ converges faster
in terms of total rounds as well as elapsed time as compared
to COCOA in all cases, despite varying: the dataset, values
of regularization, batch size, and cluster size (Section 7.2).
In Section 7.3 we demonstrate that this performance trans-
lates to orders of magnitude improvement in convergence
when scaling up the number of machines K, as compared
to COCOA as well as to several other state-of-the-art meth-
ods. Finally, in Section 7.4 we investigate the impact of the
local subproblem parameter σ(cid:48) in the COCOA+ framework.

Table 2. Datasets for Numerical Experiments.
n
522,911
400,000
677,399

Dataset
covertype
epsilon
RCV1

Sparsity
22.22%
100%
0.16%

d
54
2,000
47,236

7.1. Implementation Details

We implement all algorithms in Apache Spark (Zaharia
et al., 2012) and run them on m3.large Amazon EC2 in-
stances, applying each method to the binary hinge-loss sup-

Adding vs. Averaging in Distributed Primal-Dual Optimization

port vector machine. The analysis for this non-smooth loss
was not covered in (Jaggi et al., 2014) but has been captured
here, and thus is both theoretically and practically justiﬁed.
The used datasets are summarized in Table 2.

For illustration and ease of comparison, we here use SDCA
(Shalev-Shwartz & Zhang, 2013c) as the local solver for
both COCOA and COCOA+. Note that in this special case,
and if additionally σ(cid:48) := K, and if the partitioning nk =
n/K is balanced, once can show that the COCOA+ frame-
work reduces to the practical variant of DisDCA (Yang,
2013) (which had no convergence guarantees so far). We
include more details on the connection in Appendix C.

7.2. Comparison of COCOA+ and COCOA
We compare the COCOA+ and COCOA frameworks di-
rectly using two datasets (Covertype and RCV1) across var-
ious values of λ, the regularizer, in Figure 1. For each value
of λ we consider both methods with different values of H,
the number of local iterations performed before communi-
cating to the master. For all runs of COCOA+ we use the
safe upper bound of γK for σ(cid:48). In terms of both the to-
tal number of communications made and the elapsed time,
COCOA+ (shown in blue) converges to the optimal solu-
tion faster than COCOA (red). The discrepancy is larger
for greater values of λ, where the strongly convex regular-
izer has more of an impact and the problem difﬁculty is re-
duced. We also see a greater performance gap for smaller
values of H, where there is frequent communication be-
tween the machines and the master, and changes between
the algorithms therefore play a larger role.

7.3. Scaling the Number of Machines K
In Figure 2 we demonstrate the ability of COCOA+ to
scale with an increasing number of machines K. The
experiments conﬁrm the ability of strong scaling of the
new method, as predicted by our theory in Section 4,
in contrast to the competing methods. Unlike COCOA,
which becomes linearly slower when increasing the num-
ber of machines, the performance of COCOA+ improves
with additional machines, only starting to degrade slightly
once K=16 for the RCV1 dataset.

7.4. Impact of the Subproblem Parameter σ(cid:48)

Finally, in Figure 3, we consider the effect of the choice
of the subproblem parameter σ(cid:48) on convergence. We plot
both the number of communications and clock time on a
log-log scale for the RCV1 dataset with K=8 and H=1e4.
For γ = 1 (the most aggressive variant of COCOA+ in
which updates are added) we consider several different val-
ues of σ(cid:48), ranging from 1 to 8. The value σ(cid:48)=8 represents
the safe upper bound of γK. The optimal convergence oc-
curs around σ(cid:48)=4, and diverges for σ(cid:48) ≤ 2. Notably, we

Figure 2. The effect of increasing K on the time (s) to reach an
(cid:15)D-accurate solution. We see that COCOA+ converges twice as
fast as COCOA on 100 machines for the Epsilon dataset, and
nearly 7 times as quickly for the RCV1 dataset. Mini-batch SGD
converges an order of magnitude more slowly than both methods.

see that the easy to calculate upper bound of σ(cid:48) := γK (as
given by Lemma 4) has only slightly worse performance
than best possible subproblem parameter in our setting.

Figure 3. The effect of σ(cid:48) on convergence of COCOA+ for the
RCV1 dataset distributed across K=8 machines. Decreasing σ(cid:48)
improves performance in terms of communication and overall run
time until a certain point, after which the algorithm diverges. The
“safe” upper bound of σ(cid:48):=K=8 has only slightly worse perfor-
mance than the practically best “un-safe” value of σ(cid:48).

8. Conclusion
In conclusion, we present a novel framework COCOA+
that allows for fast and communication-efﬁcient additive
aggregation in distributed algorithms for primal-dual opti-
mization. We analyze the theoretical performance of this
method, giving strong primal-dual convergence rates with
outer iterations scaling independently of the number of ma-
chines. We extended our theory to allow for non-smooth
losses. Our experimental results show signiﬁcant speedups
over previous methods,
including the original COCOA
framework as well as other state-of-the-art methods.

Acknowledgments. We thank Ching-pei Lee and an
anonymous reviewer for several helpful insights and com-
ments.

Adding vs. Averaging in Distributed Primal-Dual Optimization

References

Balcan, M.-F., Blum, A., Fine, S., and Mansour, Y. Dis-
tributed Learning, Communication Complexity and Pri-
vacy. In COLT, pp. 26.1–26.22, 2012.

Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J.
Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3(1):1–122, 2011.

Duchi, J. C., Jordan, M. I., and McMahan, H. B. Estima-
tion, Optimization, and Parallelism when Data is Sparse.
In NIPS, 2013.

Fercoq, O. and Richt´arik, P. Accelerated, parallel and prox-

imal coordinate descent. arXiv:1312.5799, 2013.

Fercoq, O., Qu, Z., Richt´arik, P., and Tak´aˇc, M. Fast
distributed coordinate descent for non-strongly convex
losses. IEEE Workshop on Machine Learning for Signal
Processing, 2014.

Forero, P. A., Cano, A., and Giannakis, G. B. Consensus-
Based Distributed Support Vector Machines. JMLR, 11:
1663–1707, 2010.

Jaggi, M., Smith, V., Tak´aˇc, M., Terhorst, J., Krishnan, S.,
Hofmann, T., and Jordan, M. I. Communication-efﬁcient
distributed dual coordinate ascent. In NIPS, 2014.

Lee, C.-P. and Roth, D. Distributed Box-Constrained
Quadratic Optimization for Dual Linear SVM. In ICML,
2015.

Liu, J. and Wright, S. J. Asynchronous stochastic coor-
dinate descent: Parallelism and convergence properties.
arXiv:1403.3862, 2014.

Liu, J., Wright, S. J., R´e, C., Bittorf, V., and Sridhar,
S. An Asynchronous Parallel Stochastic Coordinate De-
scent Algorithm. In ICML, 2014.

Lu, Z. and Xiao, L. On the complexity analysis of random-
ized block-coordinate descent methods. arXiv preprint
arXiv:1305.4723, 2013.

Mann, G., McDonald, R., Mohri, M., Silberman, N., and
Walker, D. D. Efﬁcient Large-Scale Distributed Training
of Conditional Maximum Entropy Models. NIPS, 2009.

Mareˇcek, J., Richt´arik, P., and Tak´aˇc, M. Distributed block
coordinate descent for minimizing partially separable
functions. arXiv:1406.0238, 2014.

McWilliams, B., Heinze, C., Meinshausen, N., Krumme-
nacher, G., and Vanchinathan, H. P. LOCO: Distribut-
ing Ridge Regression with Random Projections. arXiv
stat.ML, June 2014.

Niu, F., Recht, B., R´e, C., and Wright, S. J. Hogwild!: A
Lock-Free Approach to Parallelizing Stochastic Gradient
Descent. In NIPS, 2011.

Pechyony, D., Shen, L., and Jones, R. Solving Large Scale
In
Linear SVM with Distributed Block Minimization.
NIPS Workshop on Big Learning, 2011.

Qu, Z. and Richt´arik, P.

Coordinate descent with
arbitrary sampling I: Algorithms and complexity.
arXiv:1412.8060, 2014.

Qu, Z., Richt´arik, P., and Zhang, T. Randomized dual coor-
dinate ascent with arbitrary sampling. arXiv:1411.5873,
2014.

Richt´arik, P. and Tak´aˇc, M. Distributed coordinate de-
scent method for learning with big data. arXiv preprint
arXiv:1310.2059, 2013.

Richt´arik, P. and Tak´aˇc, M.

Iteration complexity of ran-
domized block-coordinate descent methods for minimiz-
ing a composite function. Mathematical Programming,
144(1-2):1–38, April 2014.

Richt´arik, P. and Tak´aˇc, M. Parallel coordinate descent
methods for big data optimization. Mathematical Pro-
gramming, pp. 1–52, 2015.

Shalev-Shwartz, S. and Zhang, T. Accelerated mini-batch

stochastic dual coordinate ascent. In NIPS, 2013a.

Shalev-Shwartz, S. and Zhang, T. Accelerated proximal
stochastic dual coordinate ascent for regularized loss
minimization. arXiv:1309.2375, 2013b.

Shalev-Shwartz, S. and Zhang, T. Stochastic Dual Coor-
dinate Ascent Methods for Regularized Loss Minimiza-
tion. JMLR, 14:567–599, 2013c.

Shamir, O. and Srebro, N. Distributed Stochastic Optimiza-

tion and Learning . In Allerton, 2014.

Shamir, O., Srebro, N., and Zhang, T. Communication
efﬁcient distributed optimization using an approximate
newton-type method. In ICML, 2014.

Tappenden, R., Tak´aˇc, M., and Richt´arik, P. On the com-
plexity of parallel coordinate descent. Technical report,
2015. ERGO 15-001, University of Edinburgh.

Yang, T. Trading Computation for Communication: Dis-
In NIPS,

tributed Stochastic Dual Coordinate Ascent.
2013.

Yang, T., Zhu, S., Jin, R., and Lin, Y. On Theoretical Anal-
ysis of Distributed Stochastic Dual Coordinate Ascent.
arXiv:1312.1031, 2013.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Yu, H.-F., Hsieh, C.-J., Chang, K.-W., and Lin, C.-J. Large
Linear Classiﬁcation When Data Cannot Fit in Memory.
TKDD, 5(4):1–23, 2012.

Zaharia, M., Chowdhury, M., Das, T., Dave, A., McCauley,
M., Franklin, M. J., Shenker, S., and Stoica, I. Resilient
Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing. In NSDI, 2012.

Zhang, Y. and Lin, X. DiSCO: Distributed Optimization for
Self-Concordant Empirical Loss. In ICML, pp. 362–370,
2015.

Zhang, Y., Duchi,

J.
Communication-Efﬁcient Algorithms for Statistical Op-
timization. JMLR, 14:3321–3363, 2013.

and Wainwright, M.

J. C.,

Zinkevich, M. A., Weimer, M., Smola, A. J., and Li, L.
Parallelized Stochastic Gradient Descent. NIPS, 2010.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Appendix

A. Technical Lemmas

Lemma 16 (Lemma 21 in (Shalev-Shwartz & Zhang, 2013c)). Let (cid:96)i : R → R be an L-Lipschitz continuous. Then for
any real value a with |a| > L we have that (cid:96)∗

i (a) = ∞.

Lemma 17. Assuming the loss functions (cid:96)i are bounded by (cid:96)i(0) ≤ 1 for all i ∈ [n] (as we have assumed in (5) above),
then for the zero vector α(0) := 0 ∈ Rn, we have

D(α∗) − D(α(0)) = D(α∗) − D(0) ≤ 1.

(24)

Proof. For α := 0 ∈ Rn, we have w(α) = 1
in (2),

λn Aα = 0 ∈ Rd. Therefore, by deﬁnition of the dual objective D given

0 ≤ D(α∗) − D(α) ≤ P(w(α)) − D(α) = 0 − D(α)

(5),(2)

≤ 1.

B. Proofs

B.1. Proof of Lemma 3

Indeed, we have

D(α + γ

∆α[k]) = −

(cid:96)∗
i (−αi − γ(

∆α[k])i)

−

A(α + γ

∆α[k])

(25)

K
(cid:88)

k=1

1
n

n
(cid:88)

i=1

(cid:124)

K
(cid:88)

k=1

1
λn

λ
2

(cid:13)
(cid:13)
(cid:13)

(cid:124)

(cid:125)

K
(cid:88)

k=1

(cid:123)(cid:122)
B

(cid:13)
2
(cid:13)
(cid:13)

.

(cid:125)

Now, let us bound the terms A and B separately. We have

A = −

(cid:96)∗
i (−αi − γ(∆α[k])i)

= −

(cid:96)∗
i (−(1 − γ)αi − γ(α + ∆α[k])i)

(cid:33)

1
n

1
n

(cid:32)

K
(cid:88)

(cid:88)

k=1

K
(cid:88)

(cid:32)

i∈Pk

(cid:88)

k=1

i∈Pk

≥ −

(1 − γ)(cid:96)∗

i (−αi) + γ(cid:96)∗

i (−(α + ∆α[k])i)

.

(cid:123)(cid:122)
A

(cid:33)

1
n

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk
(cid:33)

Where the last inequality is due to Jensen’s inequality. Now we will bound B, using the safe separability measurement σ(cid:48)
as deﬁned in (11).

B =

A(α + γ

(cid:13)
(cid:13)
(cid:13)

1
λn

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∆α[k])
(cid:13)

=

(cid:13)
(cid:13)
(cid:13)w(α) + γ

1
λn

K
(cid:88)

k=1

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

= (cid:107)w(α)(cid:107)2 +

w(α)T A∆α[k] + γ

(cid:16) 1
λn

(cid:17)2

(cid:13)
(cid:13)
(cid:13)

γ

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

(11)
≤ (cid:107)w(α)(cid:107)2 +

w(α)T A∆α[k] + γ

(cid:107)A∆α[k](cid:107)2.

(cid:17)2

σ(cid:48)

(cid:16) 1
λn

K
(cid:88)

k=1

2γ

1
λn

K
(cid:88)

k=1

2γ

1
λn

K
(cid:88)

k=1

K
(cid:88)

k=1

− γ

(cid:107)w(α)(cid:107)2 − (1 − γ)

(cid:107)w(α)(cid:107)2 −

w(α)T A∆α[k] −

λ
2

γ

(cid:16) 1
λn

(cid:17)2

σ(cid:48)

K
(cid:88)

k=1

(cid:107)A∆α[k](cid:107)2

Adding vs. Averaging in Distributed Primal-Dual Optimization

Plugging A and B into (25) will give us

D(α + γ

∆α[k]) ≥ −

(1 − γ)(cid:96)∗

i (−αi) + γ(cid:96)∗

i (−(α + ∆α[k])i)

(cid:33)

K
(cid:88)

k=1

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk

1
n

λ
2

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk

= −

1
n

(cid:124)

+ γ

(cid:32)

K
(cid:88)

k=1

−

1
n

(cid:88)

i∈Pk

λ
2

(cid:33)

λ
2

K
(cid:88)

k=1

2γ

1
λn

(1 − γ)(cid:96)∗

i (−αi)

− (1 − γ)

(cid:107)w(α)(cid:107)2

(cid:123)(cid:122)
(1−γ)D(α)

(cid:125)

λ
2

1
K

λ
2

(9)=(1 − γ)D(α) + γ

Gσ(cid:48)
k (∆α[k]; w, α[k]).

K
(cid:88)

k=1

(cid:96)∗
i (−(α + ∆α[k])i) −

(cid:107)w(α)(cid:107)2 −

w(α)T A∆α[k] −

1
n

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α[k]

2(cid:33)
(cid:13)
(cid:13)
(cid:13)

B.2. Proof of Lemma 4

See (Richt´arik & Tak´aˇc, 2013).

B.3. Proof of Lemma 5

For sake of notation, we will write α instead of α(t), w instead of w(α(t)) and u instead of u(t).

Now, let us estimate the expected change of the dual objective. Using the deﬁnition of the dual update α(t+1) := α(t) +
γ (cid:80)

k ∆α[k] resulting in Algorithm 1, we have

(cid:104)
E(cid:2)D(α(t)) − D(α(t+1))(cid:3) = E

D(α) − D(α + γ

(by Lemma 3 on the local function Gσ(cid:48)

k (α; w, α[k]) approximating the global objective D(α))

(cid:104)
≤ E

D(α) − (1 − γ)D(α) − γ

Gσ(cid:48)
k (∆α(t)

(cid:105)
[k]; w, α[k])

(cid:104)
= γE
D(α) −

Gσ(cid:48)
k (∆α(t)

[k]; w, α[k])

(cid:105)

(cid:105)
∆α[k])

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:104)
= γE
D(α) −

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) +

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) −

(by the notion of quality (12) of the local solver, as in Assumption 1)

(cid:18)

≤ γ

D(α) −

K
(cid:88)

k=1

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) + Θ

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) −

Gσ(cid:48)
k (0; w, α[k])

K
(cid:88)

k=1

(cid:16) K
(cid:88)

k=1

Gσ(cid:48)
k (∆α(t)

(cid:105)
[k]; w, α[k])

K
(cid:88)

k=1

K
(cid:88)

k=1
(cid:124)

(cid:123)(cid:122)
D(α)

(cid:125)

(cid:17)(cid:19)

(26)

= γ(1 − Θ)

D(α) −

(cid:16)

(cid:124)

K
(cid:88)

k=1

Gσ(cid:48)
k (∆α∗

[k]; w, α[k])

(cid:17)

.

(cid:123)(cid:122)
C

(cid:125)

Adding vs. Averaging in Distributed Primal-Dual Optimization

Now, let us upper bound the C term (we will denote by ∆α∗ = (cid:80)K

k=1 ∆α∗

[k]):

((cid:96)∗

i (−αi − ∆α∗

i ) − (cid:96)∗

i (−αi)) +

w(α)T A∆α∗ +

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α∗
[k]

(cid:13)
2
(cid:13)
(cid:13)

((cid:96)∗

i (−αi − s(ui − αi)) − (cid:96)∗

i (−αi)) +

w(α)T As(u − α) +

(2),(9)
=

C

1
n

n
(cid:88)

i=1

n
(cid:88)

≤

1
n

i=1
Strong conv.
≤

1
n

1
n

µ
2

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

(cid:17)

+

1
n

s(cid:96)∗

i (−ui) + (1 − s)(cid:96)∗

i (−αi) −

(1 − s)s(ui − αi)2 − (cid:96)∗

i (−αi)

w(α)T As(u − α)

n
(cid:88)

(cid:16)

1
n

+

i=1

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

=

1
n

n
(cid:88)

(cid:16)

i=1

s(cid:96)∗

i (−ui) − s(cid:96)∗

i (−αi) −

(1 − s)s(ui − αi)2(cid:17)

+

µ
2

1
n

w(α)T As(u − α) +

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

.

The convex conjugate maximal property implies that

i (−ui) = −uiw(α)T xi − (cid:96)i(w(α)T xi).
(cid:96)∗

Moreover, from the deﬁnition of the primal and dual optimization problems (1), (2), we can write the duality gap as

G(α) := P(w(α)) − D(α)

(cid:0)(cid:96)i(xT

j w) + (cid:96)∗

i (−αi) + w(α)T xiαi

(cid:1) .

(1),(2)
=

1
n

n
(cid:88)

i=1


−suiw(α)T xi − s(cid:96)i(w(α)T xi) − s(cid:96)∗

i (−αi) −sw(α)T xiαi + sw(α)T xiαi
(cid:125)

(cid:124)

(cid:123)(cid:122)
0

−

(1 − s)s(ui − αi)2

Hence,

(27)
≤

C

1
n

n
(cid:88)

i=1

=

1
n

n
(cid:88)

i=1

1
n

1
n

+

w(α)T As(u − α) +

As(u − α)[k]

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

(cid:1) +

1
n

n
(cid:88)

(cid:16)

i=1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+

w(α)T As(u − α) +

As(u − α)[k]

(28)= −sG(α) −

(1 − s)s

(cid:107)u − α(cid:107)2 +

(cid:107)A(u − α)[k](cid:107)2.

µ
2

1
n

n
(cid:88)

i=1

σ(cid:48)
2λ

(

s
n

)2

K
(cid:88)

k=1

Now, the claimed improvement bound (15) follows by plugging (29) into (26).

µ
2

µ
2

(cid:0)−s(cid:96)i(w(α)T xi) − s(cid:96)∗

i (−αi) − sw(α)T xiαi

sw(α)T xi(αi − ui) −

(1 − s)s(ui − αi)2(cid:17)

(27)

(28)





(29)

B.4. Proof of Lemma 6

For general convex functions, the strong convexity parameter is µ = 0, and hence the deﬁnition of R(t) becomes

R(t) (16)=

(cid:107)A(u(t) − α(t))[k](cid:107)2

σk(cid:107)(u(t) − α(t))[k](cid:107)2 Lemma 16

≤

σk|Pk|4L2.

K
(cid:88)

k=1

(19)
≤

K
(cid:88)

k=1

K
(cid:88)

k=1

Adding vs. Averaging in Distributed Primal-Dual Optimization

B.5. Proof of Theorem 8

At ﬁrst let us estimate expected change of dual feasibility. By using the main Lemma 5, we have

E[D(α∗) − D(α(t+1))] = E[D(α∗) − D(α(t+1)) + D(α(t)) − D(α(t))]

(15)= D(α∗) − D(α(t)) − γ(1 − Θ)sG(α(t)) + γ(1 − Θ) σ(cid:48)
2λ (

)2R(t)

s
n

(4)= D(α∗) − D(α(t)) − γ(1 − Θ)s(P(w(α(t))) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

)2R(t)

≤ D(α∗) − D(α(t)) − γ(1 − Θ)s(D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

(18)
≤ (1 − γ(1 − Θ)s) (D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ.

s
n

s
n
)2R(t)

s
n

Using (30) recursively we have

E[D(α∗) − D(α(t))] = (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ

(1 − γ(1 − Θ)s)j

s
n

s
n

t−1
(cid:88)

j=0
1 − (1 − γ(1 − Θ)s)t
γ(1 − Θ)s

= (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ

≤ (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + s

4L2σσ(cid:48)
2λn2 .

Choice of s = 1 and t = t0 := max{0, (cid:100)

E[D(α∗) − D(α(t))] ≤ (1 − γ(1 − Θ))t0 (D(α∗) − D(α(0))) +

1

γ(1−Θ) log(2λn2(D(α∗) − D(α(0)))/(4L2σσ(cid:48)))(cid:101)} will lead to
4L2σσ(cid:48)
2λn2 ≤

4L2σσ(cid:48)
2λn2 =

4L2σσ(cid:48)
2λn2 +

4L2σσ(cid:48)
λn2

.

Now, we are going to show that

∀t ≥ t0 : E[D(α∗) − D(α(t))] ≤

λn2(1 + 1

4L2σσ(cid:48)
2 γ(1 − Θ)(t − t0))

.

Clearly, (32) implies that (33) holds for t = t0. Now imagine that it holds for any t ≥ t0 then we show that it also has to
hold for t + 1. Indeed, using

1

s =

1 + 1

2 γ(1 − Θ)(t − t0)

∈ [0, 1]

we obtain

E[D(α∗) − D(α(t+1))]

(30)
≤ (1 − γ(1 − Θ)s) (D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

(30)

(31)

(32)

(33)

(34)

)24L2σ

s
n
+ γ(1 − Θ) σ(cid:48)
2λ (

s
n

)24L2σ

(33)
≤ (1 − γ(1 − Θ)s)

(cid:18) 1 + 1

(cid:18) 1 + 1

(34)=

4L2σσ(cid:48)
λn2

=

4L2σσ(cid:48)
λn2

(cid:124)

λn2(1 + 1

4L2σσ(cid:48)
2 γ(1 − Θ)(t − t0))
2 γ(1 − Θ)(t − t0) − γ(1 − Θ) + γ(1 − Θ) 1
2 γ(1 − Θ)(t − t0))2
(cid:19)
2 γ(1 − Θ)

(1 + 1
2 γ(1 − Θ)(t − t0) − 1
(1 + 1

2 γ(1 − Θ)(t − t0))2

2

.

(cid:19)

(cid:125)

(cid:123)(cid:122)
D

(1 + 1

2 γ(1 − Θ)(t + 1 − t0))(1 + 1
(1 + 1

2 γ(1 − Θ)(t − t0))2

2 γ(1 − Θ)(t − 1 − t0))

(cid:123)(cid:122)
≤1

(cid:125)

Now, we will upperbound D as follows

D =

≤

1 + 1

2 γ(1 − Θ)(t + 1 − t0)

1 + 1

2 γ(1 − Θ)(t + 1 − t0)

(cid:124)

,

1

1

Adding vs. Averaging in Distributed Primal-Dual Optimization

where in the last inequality we have used the fact that geometric mean is less or equal to arithmetic mean.

If α is deﬁned as (21) then we obtain that

E[G(α)] = E

G

1
T −T0

α(t)

≤ 1

T −T0

E

(cid:34)

(cid:32) T −1
(cid:88)

t=T0

(cid:33)(cid:35)

(cid:35)

(cid:16)

α(t)(cid:17)

G

(cid:34) T −1
(cid:88)

t=T0

(15),(18)
≤

1
T −T0

E

(cid:34) T −1
(cid:88)

(cid:18)

1
γ(1 − Θ)s

(D(α(t+1)) − D(α(t))) + 4L2σσ(cid:48)s

2λn2

(cid:19)(cid:35)

=

≤

1
γ(1 − Θ)s
1
γ(1 − Θ)s

t=T0
1
T − T0
1
T − T0

(cid:104)
D(α(T )) − D(α(T0))

(cid:105)

E

+ 4L2σσ(cid:48)s
2λn2

(cid:105)
(cid:104)
D(α∗) − D(α(T0))

E

+ 4L2σσ(cid:48)s
2λn2

.

Now, if T ≥ (cid:100)

1

γ(1−Θ) (cid:101) + T0 such that T0 ≥ t0 we obtain
(cid:18)

E[G(α)]

(35),(33)
≤

1
γ(1 − Θ)s
(cid:18)

1
T − T0
1
γ(1 − Θ)s

4L2σσ(cid:48)
λn2

=

λn2(1 + 1
1
T − T0

1 + 1

4L2σσ(cid:48)

1

2 γ(1 − Θ)(T0 − t0))

2 γ(1 − Θ)(T0 − t0)

4L2σσ(cid:48)s
2λn2

(cid:19)

+

(cid:19)

s
2

+

.

s =

1
(T − T0)γ(1 − Θ)

∈ [0, 1]

E[G(α)]

(36),(37)
≤

(cid:18)

4L2σσ(cid:48)
λn2

1 + 1

2 γ(1 − Θ)(T0 − t0)

+

1
(T − T0)γ(1 − Θ)

1
2

(cid:19)

.

To have right hand side of (38) smaller then (cid:15)G it is sufﬁcient to choose T0 and T such that
(cid:18)

(cid:19)

1

1

Choosing

gives us

Hence of if

4L2σσ(cid:48)
λn2

1 + 1
(cid:18)

2 γ(1 − Θ)(T0 − t0)
1
1
2
(T − T0)γ(1 − Θ)

(cid:19)

4L2σσ(cid:48)
λn2

≤

≤

1
2

1
2

(cid:15)G,

(cid:15)G.

t0 +

2
γ(1 − Θ)

(cid:19)

− 1

(cid:18) 8L2σσ(cid:48)
λn2(cid:15)G
4L2σσ(cid:48)
λn2(cid:15)Gγ(1 − Θ)

≤ T0,

≤ T,

T0 +

then (39) and (40) are satisﬁed.

B.6. Proof of Theorem 10

If the function (cid:96)i(.) is (1/µ)-smooth then (cid:96)∗

i (.) is µ-strongly convex with respect to the (cid:107) · (cid:107) norm. From (16) we have

(cid:88)K

k=1

(cid:88)K

R(t) (16)= − λµn(1−s)

(cid:107)u(t) − α(t)(cid:107)2 +

σ(cid:48)s

(cid:107)A(u(t) − α(t))[k](cid:107)2

(19)
≤ − λµn(1−s)

σ(cid:48)s

(cid:107)u(t) − α(t)(cid:107)2 +

≤ − λµn(1−s)

(cid:16)

σ(cid:48)s
− λµn(1−s)

=

(cid:107)u(t) − α(t)(cid:107)2 + σmax
(cid:17)

σ(cid:48)s + σmax

(cid:107)u(t) − α(t)(cid:107)2.

σk(cid:107)u(t) − α(t)

[k](cid:107)2
(cid:107)u(t) − α(t)

k=1
(cid:88)K

[k](cid:107)2

k=1

(35)

(36)

(37)

(38)

(39)

(40)

(41)

Adding vs. Averaging in Distributed Primal-Dual Optimization

If we plug

λµn

s =

λµn + σmaxσ(cid:48) ∈ [0, 1]

(42)

into (41) we obtain that ∀t : R(t) ≤ 0. Putting the same s into (15) will give us

E[D(α(t+1)) − D(α(t))]

≥ γ(1 − Θ)

(15),(42)

λµn

λµn + σmaxσ(cid:48) G(α(t)) ≥ γ(1 − Θ)

λµn + σmaxσ(cid:48) D(α∗) − D(α(t)).

(43)

λµn

Using the fact that E[D(α(t+1)) − D(α(t))] = E[D(α(t+1)) − D(α∗)] + D(α∗) − D(α(t)) we have

E[D(α(t+1)) − D(α∗)] + D(α∗) − D(α(t))

(43)
≥ γ(1 − Θ)

λµn

λµn + σmaxσ(cid:48) D(α∗) − D(α(t))

which is equivalent with

E[D(α∗) − D(α(t+1))] ≤

1 − γ(1 − Θ)

D(α∗) − D(α(t)).

(44)

(cid:18)

(cid:19)

λµn
λµn + σmaxσ(cid:48)

Therefore if we denote by (cid:15)(t)

D = D(α∗) − D(α(t)) we have that

(cid:18)

E[(cid:15)(t)
D ]

(44)
≤

1 − γ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)t

(cid:18)

(cid:15)(0)
D

(24)
≤

1 − γ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)t

(cid:18)

≤ exp

−tγ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)

.

The right hand side will be smaller than some (cid:15)D if

t ≥

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

log

1
(cid:15)D

.

Moreover, to bound the duality gap, we have

γ(1 − Θ)

λµn

λµn + σmaxσ(cid:48) G(α(t))

(43)
≤ E[D(α(t+1)) − D(α(t))] ≤ E[D(α∗) − D(α(t))].

Therefore G(α(t)) ≤

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

(cid:15)(t)
D . Hence if (cid:15)D ≤ γ(1 − Θ)

λµn

λµn+σmaxσ(cid:48) (cid:15)G then G(α(t)) ≤ (cid:15)G. Therefore after

t ≥

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

(cid:18)

log

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

1
(cid:15)G

(cid:19)

iterations we have obtained a duality gap less than (cid:15)G.

B.7. Proof of Theorem 13

Because (cid:96)i are (1/µ)-smooth then functions (cid:96)∗
i are µ strongly convex with respect to the norm (cid:107) · (cid:107). The proof is based on
techniques developed in recent coordinate descent papers, including (Richt´arik & Tak´aˇc, 2014; 2013; Richt´arik & Tak´aˇc,
2015; Tappenden et al., 2015; Mareˇcek et al., 2014; Fercoq & Richt´arik, 2013; Lu & Xiao, 2013; Fercoq et al., 2014;
Qu & Richt´arik, 2014; Qu et al., 2014) (Efﬁcient accelerated variants were considered in (Fercoq & Richt´arik, 2013;
Shalev-Shwartz & Zhang, 2013a)).
First, let us deﬁne the function F (ζ) : Rnk → R as F (ζ) := −Gσ(cid:48)
ζiei; w, α[k]). This function can be written in
two parts F (ζ) = Φ(ζ)+f (ζ). The ﬁrst part denoted by Φ(ζ) = 1
(cid:96)∗
i (−αi −ζi) is strongly convex with convexity
n
parameter µ
n with respect to the standard Euclidean norm. In our application, we think of the ζ variable collecting the local
dual variables ∆α[k].

k ((cid:80)
(cid:80)

i∈Pk

i∈Pk

The second part we will denote by f (ζ) = 1
λ
K
to show that the gradient of f is coordinate-wise Lipschitz continuous with Lipschitz constant σ(cid:48)
standard Euclidean norm.

2 (cid:107)w(α)(cid:107)2 + 1

w(α)T xiζi + λ

2 σ(cid:48)

i∈Pk

n

1

(cid:80)

λ2n2 (cid:107) (cid:80)

i∈Pk

xiζi(cid:107)2. It is easy
λn2 rmax with respect to the

Adding vs. Averaging in Distributed Primal-Dual Optimization

Following the proof of Theorem 20 in (Richt´arik & Tak´aˇc, 2015), we obtain that

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h+1)

[k]

; w, α[k])] ≤

1 −

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

(cid:17)
[k] ; w, α[k])

(cid:32)

(cid:18)

=

1 −

(cid:16)

(cid:33)

1 + µnλ
σ(cid:48)rmax
µnλ
σ(cid:48)rmax
λnµ
σ(cid:48)rmax + λnµ

1
nk

1
nk

(cid:19) (cid:16)

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])

(cid:17)

.

Over all steps up to step h, this gives

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])] ≤

(cid:18)

1 −

1
nk

λnµ
σ(cid:48)rmax + λnµ

(cid:19)h (cid:16)

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (0; w, α[k])

(cid:17)

.

Therefore, choosing H as in the assumption of our Theorem, given in Equation (22), we are guaranteed that
(cid:16)

(cid:17)H

1 − 1
nk

λnµ
σ(cid:48)rmax+λnµ

≤ Θ, as desired.

B.8. Proof of Theorem 14

Similarly as in the proof of Theorem 13 we deﬁne a composite function F (ζ) = f (ζ) + Φ(ζ). However, in this case func-
tions (cid:96)∗
i are not guaranteed to be strongly convex. However, the ﬁrst part has still a coordinate-wise Lipschitz continuous
gradient with constant σ(cid:48)
λn2 rmax with respect to the standard Euclidean norm. Therefore from Theorem 3 in (Tappenden
et al., 2015) we have that

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])] ≤

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (0; w, α[k]) +

(cid:18)

nk
nk + h

1
2

σ(cid:48)rmax
λn2 (cid:107)∆α∗

[k](cid:107)2

(cid:19)

.

(45)

Now, choice of h = H from (23) is sufﬁcient to have the right hand side of (45) to be ≤ Θ(cid:0)Gσ(cid:48)
k (0; w, α[k])(cid:1).
Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) −

C. Relationship of DisDCA to COCOA+

We are indebted to Ching-pei Lee for showing the following relationship between the practical variant of DisDCA (Yang,
2013), and COCOA+ when SDCA is chosen as the local solver:

Considering the practical variant of DisDCA (DisDCA-p, see Figure 2 in (Yang, 2013)) using the scaling parameter scl =
K, the following holds:
K . If within the COCOA+
Lemma 18. Assume that the dataset is partitioned equally between workers, i.e. ∀k : nk = n
framework, SDCA is used as a local solver, and the subproblems are formulated using our shown “safe” (but pessimistic)
upper bound of σ(cid:48) = K, with aggregation parameter γ = 1 (adding), then the COCOA+ framework reduces exactly to the
DisDCA-p algorithm.

Proof. (Due to Ching-pei Lee, with some reformulations). As deﬁned in (9), the data-local subproblem solved by each
machine in COCOA+ is deﬁned as

max
∆α[k]∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k])

where

Gσ(cid:48)
k (∆α[k]; w, α[k]) := −

(cid:96)∗
i (−αi − (∆α[k])i) −

(cid:107)w(cid:107)2 −

wT A∆α[k] −

1
K

λ
2

1
n

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

.

1
n

(cid:88)

i∈Pk

We rewrite the local problem by scaling with n, and removing the constant regularizer term 1
K

λ

2 (cid:107)w(cid:107)2, i.e.

˜Gσ(cid:48)
k (∆α[k]; w) := −

i (−αj − (∆α[k])j) − wT A∆α[k] −
(cid:96)∗

σ(cid:48)
2λn

(cid:13)
(cid:13)
(cid:13)A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

.

(46)

(cid:88)

j∈Pk

Adding vs. Averaging in Distributed Primal-Dual Optimization

For the correspondence of interest, we now restrict to single coordinate updates in the local solver. In other words, the
local solver optimizes exactly one coordinate i ∈ Pk at a time. To relate the single coordinate update to the set of local
variables, we will use the notation

∆α[k] =: ∆αprev

[k] + δei ,

so that ∆αprev

[k] are the previous local variables, and ∆α[k] will be the updated ones.

From now on, we will consider the special case of COCOA+ when the quadratic upper bound parameter is chosen as the
“safe” value σ(cid:48) = K, combined with adding as the aggregation, i.e. γ = 1.
Now if the local solver within COCOA+ is chosen as LOCALSDCA, then one local step on the subproblem (9) will
calculate the following coordinate update. Recall that A = [x1, x2, . . . , xn] ∈ Rd×n.

δ(cid:63) := arg max

˜Gσ(cid:48)
k (∆α[k]; w)

δ∈R

which – because it is only affecting one single coordinate, employing (47) – can be expressed as

δ(cid:63) := arg max

−(cid:96)∗

i (−(αi + (∆αprev

[k] )i + δ)) − δxT

i w −

δxT

i A∆αprev

[k] −

δ2(cid:107)xi(cid:107)2
2

−(cid:96)∗

i (−(αi + (∆αprev

[k] )i + δ)) − δxT

i

δ2(cid:107)xi(cid:107)2
2

(49)

δ∈R

= arg max
δ∈R

K
λn

(cid:16)

w +

(cid:124)

K
λn

A∆αprev
[k]
(cid:123)(cid:122)
(cid:125)
=:ulocal

(cid:17)

−

K
2λn
K
2λn

From this formulation, it is apparent that single coordinate local solvers should maintain their locally updated version of
the current primal parameters, which we here denote as

(47)

(48)

(50)

ulocal = w +

K
λn

A∆αprev
[k] .

In the practical variant of DisDCA, the summarized local primal updates are ∆ulocal = 1
λnk
nk = n/K for K being the number of machines, this means the local ulocal update of DisDCA-p is

A∆α[k]. For the balanced case

∆α(cid:63)

i := arg max
∆αi∈R

−(cid:96)∗

i (−(αi + ∆αi)) − ∆αixT

i ulocal −

(∆αi)2(cid:107)xi(cid:107)2
2 .

(51)

K
2λn

It is not hard to show that during one outer round, the evolution of the local dual variables ∆α[k] is the same in both
methods, such that they will also have the same trajectory of ulocal. This requires some care if the same coordinate is
sampled more than once in a round, which can happen in LOCALSDCA within COCOA+ and also in DisDCA-p.

Discussion.

In the view of the above lemma, we will summarize the connection of the two methods as follows:

• COCOA/+ is Not an Algorithm. In contrast, it is a framework which allows to use any local solver to perform
approximate steps on the local subproblem. This additional level of abstraction (from the deﬁnition of such local
subproblems in (9)) is the ﬁrst to allow reusability of any fast/tuned and problem speciﬁc single machine solvers,
while decoupling this from the distributed algorithmic scheme, as presented in Algorithm 1.

Concerning the choice of local solver to be used within COCOA/+, SDCA is not the fastest known single machine
solver for most applications. Much recent research has shown improvements on SDCA (Shalev-Shwartz & Zhang,
2013c), such as accelerated variants (Shalev-Shwartz & Zhang, 2013b) and other approaches including variance re-
duction, methods incorporating second-order information, and importance sampling. In this light, we encourage the
user of the COCOA or COCOA+ framework to plug in the best and most recent solver available for their particular
local problem (within Algorithm 1), which is not necessarily SDCA. This choice should be made explicit especially
when comparing algorithms. Our presented convergence theory from Section 4 will still cover these choices, since it
only depends on the relative accuracy Θ of the chosen local solver.

Adding vs. Averaging in Distributed Primal-Dual Optimization

• COCOA+ is Theoretically Safe, while still Adaptive to the Data. The general deﬁnition of the local subproblems,
and therefore the treatment of the varying separable bound on the objective – quantiﬁed by σ(cid:48) – allows our framework
to adapt to the difﬁculty of the data partition and still give convergence results. The data-dependent measure σ(cid:48) is
fully decoupled from what the user of the framework prefers to employ as a local solver (see also the comment below
that COCOA is not a coordinate solver).
The safe upper bound σ(cid:48) = K is worst-case pessimistic, for the convergence theory to still hold in all cases, when the
updates are added. Using additional knowledge from the input data, better bounds and therefore better step-sizes can
be achieved in COCOA+. An example when σ(cid:48) can be safely chosen much smaller is when the data-matrix satisﬁes
strong row/column sparsity, see e.g. Lemma 1 in (Richt´arik & Tak´aˇc, 2013).

• Obtaining DisDCA-p as a Special Case. As shown in Lemma 18 above, we have that if in COCOA+, if SDCA is
used as the local solver and the pessimistic upper bound of σ(cid:48) = K is used and, moreover, the dataset is partitioned
K , then the COCOA+ framework reduces exactly to the DisDCA-p algorithm by (Yang, 2013).
equally, i.e. ∀k : nk = n
The correspondence breaks down if the subproblem parameter is chosen to a practically good value σ(cid:48) (cid:54)= K. Also, as
noted above, SDCA is often not the best local solver currently available. In our above experiments, SDCA was used
just for demonstration purposes and ease of comparison. Furthermore, the data partition might often be unbalanced
in practical applications.
While both DisDCA-p and COCOA are special cases of COCOA+, we note that DisDCA-p can not be recovered as a
special case of the original COCOA framework (Jaggi et al., 2014).

• COCOA/+ are Not Coordinate Methods. Despite the original name being motivated from this special case, COCOA
and COCOA+ are not coordinate methods. In fact, COCOA+ as presented here for the adding case (γ = 1) is much
more closely related to a batch method applied to the dual, using a block-separable proximal term, as following
from our new subproblem formulation (9), depending on σ(cid:48). See also the remark in Section 6. The framework here
(Algorithm 1) gives more generality, as the used local solver is not restricted to be a coordinate-wise one. In fact the
framework allows to translate recent and future improvements of single machine solvers directly to the distributed
setting, by employing them within Algorithm 1. DisDCA-p works very well for several applications, but is restricted
to using local coordinate ascent (SDCA) steps.

• Theoretical Convergence Results. While DisDCA-p (Yang, 2013) was proposed without theoretical justiﬁcation
(hence the nomenclature), the main contribution in the paper here – apart from the arbitrary local solvers – is the
convergence analysis for the framework. The theory proposed in (Yang et al., 2013) is given only for the setting of
orthogonal partitions, i.e., when σ(cid:48) = 1 and the problems become trivial to distribute given the orthogonality of data
between the workers.

The theoretical analysis here gives convergence rates applying for Algorithm 1 when using arbitrary local solvers,
and inherits the performance of the local solver. As a special case, we obtain the ﬁrst theoretical justiﬁcation and
convergence rates for original COCOA in the case of general convex objective, as well as for the special case of
DisDCA-p for both general convex and smooth convex objectives.

Adding vs. Averaging in Distributed Primal-Dual Optimization

5
1
0
2
 
l
u
J
 
3
 
 
]

G
L
.
s
c
[
 
 
2
v
8
0
5
3
0
.
2
0
5
1
:
v
i
X
r
a

Chenxin Ma∗
Industrial and Systems Engineering, Lehigh University, USA

Virginia Smith∗
University of California, Berkeley, USA

Martin Jaggi
ETH Z¨urich, Switzerland

Michael I. Jordan
University of California, Berkeley, USA

Peter Richt´arik
School of Mathematics, University of Edinburgh, UK

Martin Tak´aˇc
Industrial and Systems Engineering, Lehigh University, USA

∗Authors contributed equally.

CHM514@LEHIGH.EDU

VSMITH@BERKELEY.EDU

JAGGI@INF.ETHZ.CH

JORDAN@CS.BERKELEY.EDU

PETER.RICHTARIK@ED.AC.UK

TAKAC.MT@GMAIL.COM

Abstract

1. Introduction

Distributed optimization methods for large-scale
machine learning suffer from a communication
bottleneck. It is difﬁcult to reduce this bottleneck
while still efﬁciently and accurately aggregating
partial work from different machines. In this pa-
per, we present a novel generalization of the re-
cent communication-efﬁcient primal-dual frame-
work (COCOA) for distributed optimization. Our
framework, COCOA+, allows for additive com-
bination of local updates to the global parame-
ters at each iteration, whereas previous schemes
with convergence guarantees only allow conser-
vative averaging. We give stronger (primal-dual)
convergence rate guarantees for both COCOA as
well as our new variants, and generalize the the-
ory for both methods to cover non-smooth con-
vex loss functions. We provide an extensive ex-
perimental comparison that shows the markedly
improved performance of COCOA+ on several
real-world distributed datasets, especially when
scaling up the number of machines.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

With the wide availability of large datasets that exceed
the storage capacity of single machines, distributed opti-
mization methods for machine learning have become in-
creasingly important. Existing methods require signiﬁcant
communication between workers, frequently equaling the
amount of local computation (or reading of local data). As
a result, distributed machine learning suffers signiﬁcantly
from a communication bottleneck on real world systems,
where communication is typically several orders of magni-
tudes slower than reading data from main memory.

In this work we focus on optimization problems with em-
pirical loss minimization structure, i.e., objectives that are
a sum of the loss functions of each datapoint. This in-
cludes the most commonly used regularized variants of
linear regression and classiﬁcation methods.
For this
class of problems, the recently proposed COCOA approach
(Yang, 2013; Jaggi et al., 2014) develops a communication-
efﬁcient primal-dual scheme that targets the communica-
tion bottleneck, allowing more computation on data-local
subproblems native to each machine before communica-
tion. By appropriately choosing the amount of local com-
putation per round, this framework allows one to control
the trade-off between communication and local computa-
tion based on the systems hardware at hand.

However, the performance of COCOA (as well as related
primal SGD-based methods) is signiﬁcantly reduced by the

Adding vs. Averaging in Distributed Primal-Dual Optimization

need to average updates between all machines. As the
number of machines K grows, the updates get diluted and
slowed by 1/K, e.g., in the case where all machines ex-
cept one would have already reached the solutions of their
respective partial optimization tasks. On the other hand, if
the updates are instead added, the algorithms can diverge,
as we will observe in the practical experiments below.

To address both described issues, in this paper we develop
a novel generalization of the local COCOA subproblems
assigned to each worker, making the framework more pow-
erful in the following sense: Without extra computational
cost, the set of locally computed updates from the mod-
iﬁed subproblems (one from each machine) can be com-
bined more efﬁciently between machines. The proposed
COCOA+ updates can be aggressively added (hence the
‘+’-sufﬁx), which yields much faster convergence both in
practice and in theory. This difference is particularly sig-
niﬁcant as the number of machines K becomes large.

1.1. Contributions

Strong Scaling. To our knowledge, our framework is the
ﬁrst to exhibit favorable strong scaling for the class of prob-
lems considered, as the number of machines K increases
and the data size is kept ﬁxed. More precisely, while the
convergence rate of COCOA degrades as K is increased,
the stronger theoretical convergence rate here is – in the
worst case – independent of K. Our experiments in Section
7 conﬁrm the improved speed of convergence. Since the
number of communicated vectors is only one per round and
worker, this favorable scaling might be surprising. Indeed,
for existing methods, splitting data among more machines
generally increases communication requirements (Shamir
& Srebro, 2014), which can severely affect overall runtime.

Theoretical Analysis of Non-Smooth Losses. While the
existing analysis for COCOA in (Jaggi et al., 2014) only
covered smooth loss functions, here we extend the class
of functions where the rates apply, additionally covering,
e.g., Support Vector Machines and non-smooth regression
variants. We provide a primal-dual convergence rate for
both COCOA as well as our new method COCOA+ in the
case of general convex (L-Lipschitz) losses.

tight

Primal-Dual Convergence Rate. Furthermore, we addi-
tionally strengthen the rates by showing stronger primal-
dual convergence for both algorithmic frameworks, which
are almost
to their objective-only counterparts.
Primal-dual rates for COCOA had not previously been ana-
lyzed in the general convex case. Our primal-dual rates al-
low efﬁcient and practical certiﬁcates for the optimization
quality, e.g., for stopping criteria. The new rates apply to
both smooth and non-smooth losses, and for both COCOA
as well as the extended COCOA+.

Arbitrary Local Solvers. COCOA as well as COCOA+
allow the use of arbitrary local solvers on each machine.

Experimental Results. We provide a thorough experi-
mental comparison with competing algorithms using sev-
eral real-world distributed datasets. Our practical results
conﬁrm the strong scaling of COCOA+ as the number of
machines K grows, while competing methods, including
the original COCOA, slow down signiﬁcantly with larger
K. We implement all algorithms in Spark, and our code is
publicly available at: github.com/gingsmith/cocoa.

1.2. History and Related Work

While optimal algorithms for the serial (single machine)
case are already well researched and understood, the liter-
ature in the distributed setting is relatively sparse. In par-
ticular, details on optimal trade-offs between computation
and communication, as well as optimization or statistical
accuracy, are still widely unclear. For an overview over
this currently active research ﬁeld, we refer the reader to
(Balcan et al., 2012; Richt´arik & Tak´aˇc, 2013; Duchi et al.,
2013; Yang, 2013; Liu & Wright, 2014; Fercoq et al., 2014;
Jaggi et al., 2014; Shamir & Srebro, 2014; Shamir et al.,
2014; Zhang & Lin, 2015; Qu & Richt´arik, 2014) and the
references therein. We provide a detailed comparison of
our proposed framework to the related work in Section 6.

2. Setup

We consider regularized empirical loss minimization prob-
lems of the following well-established form:
λ
2

P(w) :=

min
w∈Rd

i w) +

(cid:96)i(xT

(cid:107)w(cid:107)2

n
(cid:88)

1
n

(1)

(cid:40)

(cid:41)

i=1

i=1 ⊂ Rd represent the training data
Here the vectors {xi}n
examples, and the (cid:96)i(.) are arbitrary convex real-valued
loss functions (e.g., hinge loss), possibly depending on la-
bel information for the i-th datapoints. The constant λ > 0
is the regularization parameter.

The above class includes many standard problems of wide
interest in machine learning, statistics, and signal process-
ing, including support vector machines, regularized linear
and logistic regression, ordinal regression, and others.

Dual Problem, and Primal-Dual Certiﬁcates. The con-
jugate dual of (1) takes following form:

(cid:40)

max
α∈Rn

D(α) := −

(cid:96)∗
j (−αj) −

1
n

n
(cid:88)

j=1

2 (cid:41)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Aα
λn

(cid:13)
(cid:13)
(cid:13)
(cid:13)

λ
2

(2)

Here the data matrix A = [x1, x2, . . . , xn] ∈ Rd×n col-
lects all data-points as its columns, and (cid:96)∗
j is the conjugate
function to (cid:96)j. See, e.g., (Shalev-Shwartz & Zhang, 2013c)
for several concrete applications.

Adding vs. Averaging in Distributed Primal-Dual Optimization

It is possible to assign for any dual vector α ∈ Rn a corre-
sponding primal feasible point

w(α) = 1

λn Aα

The duality gap function is then given by:

G(α) := P(w(α)) − D(α)

(3)

(4)

By weak duality, every value D(α) at a dual candidate α
provides a lower bound on every primal value P(w). The
duality gap is therefore a certiﬁcate on the approxima-
tion quality: The distance to the unknown true optimum
P(w∗) must always lie within the duality gap, i.e., G(α) =
P(w) − D(α) ≥ P(w) − P(w∗) ≥ 0.

In large-scale machine learning settings like those consid-
ered here, the availability of such a computable measure of
approximation quality is a signiﬁcant beneﬁt during train-
ing time. Practitioners using classical primal-only methods
such as SGD have no means by which to accurately detect
if a model has been well trained, as P (w∗) is unknown.

Classes of Loss-Functions. To simplify presentation, we
assume that all loss functions (cid:96)i are non-negative, and
(cid:96)i(0) ≤ 1

(5)

∀i

Deﬁnition 1 (L-Lipschitz continuous loss). A function (cid:96)i :
R → R is L-Lipschitz continuous if ∀a, b ∈ R, we have

|(cid:96)i(a) − (cid:96)i(b)| ≤ L|a − b|
(6)
Deﬁnition 2 ((1/µ)-smooth loss). A function (cid:96)i : R → R
is (1/µ)-smooth if it is differentiable and its derivative is
(1/µ)-Lipschitz continuous, i.e., ∀a, b ∈ R, we have

|(cid:96)(cid:48)

i(a) − (cid:96)(cid:48)

i(b)| ≤

|a − b|

(7)

1
µ

3. The COCOA+ Algorithm Framework
In this section we present our novel COCOA+ frame-
work. COCOA+ inherits the many beneﬁts of CoCoA as
it remains a highly ﬂexible and scalable, communication-
efﬁcient framework for distributed optimization. COCOA+
differs algorithmically in that we modify the form of the lo-
cal subproblems (9) to allow for more aggressive additive
updates (as controlled by γ). We will see that these changes
allow for stronger convergence guarantees as well as im-
proved empirical performance. Proofs of all statements in
this section are given in the supplementary material.

Data Partitioning. We write {Pk}K
k=1 for the given par-
tition of the datapoints [n] := {1, 2, . . . , n} over the K
worker machines. We denote the size of each part by
nk = |Pk|. For any k ∈ [K] and α ∈ Rn we use the
notation α[k] ∈ Rn for the vector

(α[k])i :=

(cid:40)

0,
if i /∈ Pk,
αi, otherwise.

Local Subproblems in COCOA+. We can deﬁne a data-
local subproblem of the original dual optimization problem
(2), which can be solved on machine k and only requires
accessing data which is already available locally, i.e., data-
points with i ∈ Pk. More formally, each machine k is as-
signed the following local subproblem, depending only on
the previous shared primal vector w ∈ Rd, and the change
in the local dual variables αi with i ∈ Pk:

max
∆α[k]∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k])

(8)

where

1
n

(cid:88)

i∈Pk
λ
2

Gσ(cid:48)
k (∆α[k]; w, α[k]) := −

(cid:96)∗
i (−αi − (∆α[k])i)

−

1
K

λ
2

1
n

(cid:107)w(cid:107)2 −

wT A∆α[k] −

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

1
λn

A∆α[k]

(9)

(cid:13)
2
(cid:13)
(cid:13)

Interpretation. The above deﬁnition of the local objec-
tive functions Gσ(cid:48)
k are such that they closely approximate
the global dual objective D, as we vary the ‘local’ vari-
able ∆α[k], in the following precise sense:
Lemma 3. For any dual α, ∆α ∈ Rn, primal w = w(α)
and real values γ, σ(cid:48) satisfying (11), it holds that

(cid:16)

D

α + γ

K
(cid:88)

k=1

(cid:17)
∆α[k]
K
(cid:88)

+γ

≥ (1 − γ)D(α)

Gσ(cid:48)
k (∆α[k]; w, α[k])

(10)

k=1
The role of the parameter σ(cid:48) is to measure the difﬁculty of
the given data partition. For our purposes, we will see that
it must be chosen not smaller than

σ(cid:48) ≥ σ(cid:48)

min := γ max
α∈Rn

(cid:107)Aα(cid:107)2
k=1 (cid:107)Aα[k](cid:107)2

(cid:80)K

(11)

In the following lemma, we show that this parameter can
be upper-bounded by γK, which is trivial to calculate for
all values γ ∈ R. We show experimentally (Section 7) that
this safe upper bound for σ(cid:48) has a minimal effect on the
overall performance of the algorithm. Our main theorems
below show convergence rates dependent on γ ∈ [ 1
K , 1],
which we refer to as the aggregation parameter.
Lemma 4. The choice of σ(cid:48) := γK is valid for (11), i.e.,

γK ≥ σ(cid:48)

min

Notion of Approximation Quality of the Local Solver.
Assumption 1 (Θ-approximate solution). We assume that
there exists Θ ∈ [0, 1) such that ∀k ∈ [K], the local solver
at any outer iteration t produces a (possibly) randomized
approximate solution ∆α[k], which satisﬁes
[k]; w, α[k]) − Gσ(cid:48)

(12)

E(cid:2)Gσ(cid:48)
k (∆α∗
(cid:16)
Gσ(cid:48)
k (∆α∗

≤ Θ

k (∆α[k]; w, α[k])(cid:3)
(cid:17)

k (0; w, α[k])

,

[k]; w, α[k]) − Gσ(cid:48)

Adding vs. Averaging in Distributed Primal-Dual Optimization

where
∆α∗

[k] ∈ arg max
∆α∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k]) ∀k ∈ [K] (13)

We are now ready to describe the COCOA+ framework,
shown in Algorithm 1. The crucial difference compared
to the existing COCOA algorithm (Jaggi et al., 2014) is the
more general local subproblem, as deﬁned in (9), as well as
the aggregation parameter γ. These modiﬁcations allow the
option of directly adding updates to the global vector w.

Algorithm 1 COCOA+ Framework
1: Input: Datapoints A distributed according to parti-
tion {Pk}K
k=1. Aggregation parameter γ ∈ (0, 1],
subproblem parameter σ(cid:48) for the local subproblems
Gσ(cid:48)
k (∆α[k]; w, α[k]) for each k ∈ [K].
Starting point α(0) := 0 ∈ Rn, w(0) := 0 ∈ Rd.

2: for t = 0, 1, 2, . . . do
3:

for k ∈ {1, 2, . . . , K} in parallel over computers
do

call the local solver, computing a Θ-approximate
solution ∆α[k] of the local subproblem (9)
update α(t+1)
[k]
return ∆wk := 1

:= α(t)
λn A∆α[k]

[k] + γ ∆α[k]

end for
reduce w(t+1) := w(t) + γ (cid:80)K

k=1 ∆wk.

(14)

4:

5:

6:
7:
8:

9: end for

4. Convergence Guarantees

Before being able to state our main convergence results,
we introduce some useful quantities and the following
main lemma characterizing the effect of iterations of Al-
gorithm 1, for any chosen internal local solver.
Lemma 5. Let (cid:96)∗
i be strongly1 convex with convexity pa-
rameter µ ≥ 0 with respect to the norm (cid:107)·(cid:107), ∀i ∈ [n]. Then
for all iterations t of Algorithm 1 under Assumption 1, and
any s ∈ [0, 1], it holds that

E[D(α(t+1)) − D(α(t))] ≥

γ(1 − Θ)

sG(α(t)) −

(cid:16)

(15)
R(t)(cid:17)

,

σ(cid:48)
2λ

(cid:0) s
n

(cid:1)2

where

R(t) := − λµn(1−s)
+ (cid:80)K

(cid:107)u(t) − α(t)(cid:107)2
k=1(cid:107)A(u(t) − α(t))[k](cid:107)2,

σ(cid:48)s

(16)

for u(t) ∈ Rn with

− u(t)

i ∈ ∂(cid:96)i(w(α(t))T xi).

(17)

1Note that the case of weakly convex (cid:96)∗

i (.) is explicitly al-

lowed here as well, as the Lemma holds for the case µ = 0.

The following Lemma provides a uniform bound on R(t):
Lemma 6. If (cid:96)i are L-Lipschitz continuous for all i ∈ [n],
then

∀t : R(t) ≤ 4L2

σknk

,

K
(cid:88)

where

k=1
(cid:124)

(cid:125)

(cid:123)(cid:122)
=:σ
(cid:107)Aα[k](cid:107)2
(cid:107)α[k](cid:107)2 .

σk := max
α[k]∈Rn

(18)

(19)

Remark 7. If all data-points xi are normalized such that
(cid:107)xi(cid:107) ≤ 1 ∀i ∈ [n], then σk ≤ |Pk| = nk. Furthermore,
if we assume that the data partition is balanced, i.e., that
nk = n/K for all k, then σ ≤ n2/K. This can be used to
bound the constants R(t), above, as R(t) ≤ 4L2n2
K .

4.1. Primal-Dual Convergence for General Convex

Losses

The following theorem shows the convergence for non-
smooth loss functions, in terms of objective values as well
as primal-dual gap. The analysis in (Jaggi et al., 2014) only
covered the case of smooth loss functions.
Theorem 8. Consider Algorithm 1 with Assumption 1. Let
(cid:96)i(·) be L-Lipschitz continuous, and (cid:15)G > 0 be the de-
sired duality gap (and hence an upper-bound on primal
sub-optimality). Then after T iterations, where

(cid:108)
T ≥ T0 + max{

1
γ(1 − Θ)

(cid:109)
,

T0 ≥ t0 +

(cid:18)

2
γ(1 − Θ)
(cid:108)

4L2σσ(cid:48)
λn2(cid:15)Gγ(1 − Θ)
(cid:19)(cid:19)

(cid:18) 8L2σσ(cid:48)
λn2(cid:15)G

− 1

,

+

t0 ≥ max(0,

1

γ(1−Θ) log( 2λn2(D(α∗)−D(α(0)))

4L2σσ(cid:48)

(cid:109)
)

),

},

(20)

we have that the expected duality gap satisﬁes

E[P(w(α)) − D(α)] ≤ (cid:15)G,

at the averaged iterate

α := 1

T −T0

(cid:80)T −1

t=T0+1α(t).

(21)

The following corollary of the above theorem clariﬁes our
main result: The more aggressive adding of the partial up-
dates, as compared averaging, offers a very signiﬁcant im-
provement in terms of total iterations needed. While the
convergence in the ‘adding’ case becomes independent of
the number of machines K, the ‘averaging’ regime shows
the known degradation of the rate with growing K, which is
a major drawback of the original COCOA algorithm. This
important difference in the convergence speed is not a the-
oretical artifact but also conﬁrmed in our practical experi-
ments below for different K, as shown e.g. in Figure 2.

We further demonstrate below that by choosing γ and σ(cid:48)
accordingly, we can still recover the original COCOA al-
gorithm and its rate.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Corollary 9. Assume that all datapoints xi are bounded as
(cid:107)xi(cid:107) ≤ 1 and that the data partition is balanced, i.e. that
nk = n/K for all k. We consider two different possible
choices of the aggregation parameter γ:

• (COCOA Averaging, γ := 1

K ): In this case, σ(cid:48) := 1
is a valid choice which satisﬁes (11). Then using σ ≤
n2/K in light of Remark 7, we have that T iterations
are sufﬁcient for primal-dual accuracy (cid:15)G, with

T ≥ T0 + max{

T0 ≥ t0 +

(cid:18) 2K
1 − Θ
t0 ≥ max(0, (cid:6) K

(cid:108) K

(cid:109)
,

4L2
λ(cid:15)G(1 − Θ)
(cid:19)(cid:19)

},

− 1

,

+

1 − Θ
(cid:18) 8L2
λK(cid:15)G

1−Θ log( 2λ(D(α∗)−D(α(0)))

4KL2

)(cid:7))

Hence the more machines K, the more iterations are
needed (in the worst case).

• (COCOA+ Adding, γ := 1): In this case, the choice of
σ(cid:48) := K satisﬁes (11). Then using σ ≤ n2/K in light
of Remark 7, we have that T iterations are sufﬁcient
for primal-dual accuracy (cid:15)G, with

(cid:108)

1
1 − Θ
(cid:18) 8L2
λ(cid:15)G

T ≥ T0 + max{

(cid:18) 2

T0 ≥ t0 +

1 − Θ
t0 ≥ max(0, (cid:6) 1

(cid:109)
,

4L2
λ(cid:15)G(1 − Θ)

},

(cid:19)(cid:19)

− 1

,

+

1−Θ log( 2λn(D(α∗)−D(α(0)))

4KL2

)(cid:7))

This is signiﬁcantly better than the averaging case.

In practice, we usually have σ (cid:28) n2/K, and hence the
actual convergence rate can be much better than the proven
worst-case bound. Table 1 shows that the actual value of
σ is typically between one and two orders of magnitudes
smaller compared to our used upper-bound n2/K.

Table 1. The ratio of upper-bound n2
the parameter σ, for some real datasets.

K divided by the true value of

K

16

32

64

128

256

512

news
real-sim
rcv1

15.483
42.127
40.138

14.933
36.898
23.827

14.278
30.780
28.204

13.390
23.814
21.792

12.074
16.965
16.339

10.252
11.835
11.099

K

256

512

1024

2048

4096

8192

covtype

17.277

17.260

17.239

16.948

17.238

12.729

4.2. Primal-Dual Convergence for Smooth Losses

The following theorem shows the convergence for smooth
losses, in terms of the objective as well as primal-dual gap.

Theorem 10. Assume the loss functions functions (cid:96)i are
(1/µ)-smooth ∀i ∈ [n]. We deﬁne σmax = maxk∈[K] σk.
Then after T iterations of Algorithm 1, with

T ≥

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

log 1
(cid:15)D

,

it holds that

E[D(α∗) − D(α(T ))] ≤ (cid:15)D.

Furthermore, after T iterations with

T ≥

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

log

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

1
(cid:15)G

(cid:16)

(cid:17)

,

we have the expected duality gap

E[P(w(α(T ))) − D(α(T ))] ≤ (cid:15)G.

The following corollary is analogous to Corollary 9, but
for the case of smooth loses. It again shows that while the
COCOA variant degrades with the increase of the number
of machines K, the COCOA+ rate is independent of K.
Corollary 11. Assume that all datapoints xi are bounded
as (cid:107)xi(cid:107) ≤ 1 and that the data partition is balanced, i.e.,
that nk = n/K for all k. We again consider the same two
different possible choices of the aggregation parameter γ:
K ): In this case, σ(cid:48) :=
1 is a valid choice which satisﬁes (11). Then using
σmax ≤ nk = n/K in light of Remark 7, we have that
T iterations are sufﬁcient for suboptimality (cid:15)D, with

• (COCOA Averaging, γ := 1

T ≥ 1

1−Θ

λµK+1
λµ

log 1
(cid:15)D

Hence the more machines K, the more iterations are
needed (in the worst case).

• (COCOA+ Adding, γ := 1): In this case, the choice
of σ(cid:48) := K satisﬁes (11). Then using σmax ≤ nk =
n/K in light of Remark 7, we have that T iterations
are sufﬁcient for suboptimality (cid:15)D, with

T ≥ 1

1−Θ

λµ+1

λµ log 1
(cid:15)D

This is signiﬁcantly better than the averaging case.
Both rates hold analogously for the duality gap.

4.3. Comparison with Original COCOA

Remark 12. If we choose averaging (γ := 1
K ) for aggre-
gating the updates, together with σ(cid:48) := 1, then the result-
ing Algorithm 1 is identical to COCOA analyzed in (Jaggi
et al., 2014). However, they only provide convergence for
smooth loss functions (cid:96)i and have guarantees for dual sub-
optimality and not the duality gap. Formally, when σ(cid:48) = 1,
the subproblems (9) will differ from the original dual D(.)
only by an additive constant, which does not affect the local
optimization algorithms used within COCOA.

5. SDCA as an Example Local Solver

We have shown convergence rates for Algorithm 1, depend-
ing solely on the approximation quality Θ of the used local

Adding vs. Averaging in Distributed Primal-Dual Optimization

solver (Assumption 1). Any chosen local solver in each
round receives the local α variables as an input, as well as
a shared vector w (3)= w(α) being compatible with the last
state of all global α ∈ Rn variables.

As an illustrative example for a local solver, Algorithm 2
below summarizes randomized coordinate ascent (SDCA)
applied on the local subproblem (9). The following two
Theorems (13, 14) characterize the local convergence for
both smooth and non-smooth functions. In all the results
we will use rmax := maxi∈[n] (cid:107)xi(cid:107)2.

Algorithm 2 LOCALSDCA (w, α[k], k, H)
1: Input: α[k], w = w(α)
2: Data: Local {(xi, yi)}i∈Pk
3: Initialize: ∆α(0)
[k] := 0 ∈ Rn
4: for h = 0, 1, . . . , H − 1 do
5:

choose i ∈ Pk uniformly at random
Gσ(cid:48)
k (∆α(h)
δ∗
i := arg max

6:

[k] + δiei; w, α[k])

δi∈R
:= ∆α(h)

[k] + δ∗

i ei

7: ∆α(h+1)
[k]
8: end for
9: Output: ∆α(H)
[k]

Theorem 13. Assume the functions (cid:96)i are (1/µ)−smooth
for i ∈ [n]. Then Assumption 1 on the local approximation
quality Θ is satisﬁed for LOCALSDCA as given in Algo-
rithm 2, if we choose the number of inner iterations H as

H ≥ nk

σ(cid:48)rmax + λnµ
λnµ
Theorem 14. Assume the functions (cid:96)i are L-Lipschitz for
i ∈ [n]. Then Assumption 1 on the local approximation
quality Θ is satisﬁed for LOCALSDCA as given in Algo-
rithm 2, if we choose the number of inner iterations H as

1
Θ

(22)

log

.

H ≥ nk

(cid:18) 1 − Θ
Θ

+

σ(cid:48)rmax
2Θλn2

(cid:107)∆α∗
[k](cid:107)2
[k]; .) − Gσ(cid:48)

k (0; .)

(cid:19)

.

Gσ(cid:48)
k (∆α∗

(23)
Remark 15. Between the different regimes allowed in
COCOA+ (ranging between averaging and adding the up-
dates) the computational cost for obtaining the required
local approximation quality varies with the choice of σ(cid:48).
From the above worst-case upper bound, we note that the
cost can increase with σ(cid:48), as aggregation becomes more
aggressive. However, as we will see in the practical exper-
iments in Section 7 below, the additional cost is negligible
compared to the gain in speed from the different aggrega-
tion, when measured on real datasets.

6. Discussion and Related Work

SGD-based Algorithms. For the empirical loss mini-
mization problems of interest here, stochastic subgradient

descent (SGD) based methods are well-established. Sev-
eral distributed variants of SGD have been proposed, many
of which build on the idea of a parameter server (Niu et al.,
2011; Liu et al., 2014; Duchi et al., 2013). The downside of
this approach, even when carefully implemented, is that the
amount of required communication is equal to the amount
of data read locally (e.g., mini-batch SGD with a batch size
of 1 per worker). These variants are in practice not compet-
itive with the more communication-efﬁcient methods con-
sidered here, which allow more local updates per round.

One-Shot Communication Schemes. At the other ex-
treme, there are distributed methods using only a single
round of communication, such as (Zhang et al., 2013;
Zinkevich et al., 2010; Mann et al., 2009; McWilliams
et al., 2014). These require additional assumptions on the
partitioning of the data, and furthermore can not guarantee
convergence to the optimum solution for all regularizers, as
shown in, e.g., (Shamir et al., 2014). (Balcan et al., 2012)
shows additional relevant lower bounds on the minimum
number of communication rounds necessary for a given ap-
proximation quality for similar machine learning problems.

Mini-Batch Methods. Mini-batch methods are more
ﬂexible and lie within these two communication vs. com-
putation extremes. However, mini-batch versions of both
SGD and coordinate descent (CD) (Richt´arik & Tak´aˇc,
2013; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu
& Richt´arik, 2014; Qu et al., 2014) suffer from their con-
vergence rate degrading towards the rate of batch gradient
descent as the size of the mini-batch is increased. This fol-
lows because mini-batch updates are made based on the
outdated previous parameter vector w, in contrast to meth-
ods that allow immediate local updates like COCOA. Fur-
thermore, the aggregation parameter for mini-batch meth-
ods is harder to tune, as it can lie anywhere in the order of
mini-batch size. In the COCOA setting, the parameter lies
in the smaller range given by K. Our COCOA+ extension
avoids needing to tune this parameter entirely, by adding.

Methods Allowing Local Optimization. Developing
methods that allow for local optimization requires care-
fully devising data-local subproblems to be solved after
each communication round. (Shamir et al., 2014; Zhang
& Lin, 2015) have proposed distributed Newton-type algo-
rithms in this spirit. However, the subproblems must be
solved to high accuracy for convergence to hold, which is
often prohibitive as the size of the data on one machine is
still relatively large. In contrast, the COCOA framework
(Jaggi et al., 2014) allows using any local solver of weak
local approximation quality in each round. By making use
of the primal-dual structure in the line of work of (Yu et al.,
2012; Pechyony et al., 2011; Yang, 2013; Lee & Roth,
2015), the COCOA and COCOA+ frameworks also allow
more control over the aggregation of updates between ma-

Adding vs. Averaging in Distributed Primal-Dual Optimization

Figure 1. Duality gap vs. the number of communicated vectors, as well as duality gap vs. elapsed time in seconds for two datasets:
Covertype (left, K=4) and RCV1 (right, K=8). Both are shown on a log-log scale, and for three different values of regularization
(λ=1e-4; 1e-5; 1e-6). Each plot contains a comparison of COCOA (red) and COCOA+ (blue), for three different values of H, the
number of local iterations performed per round. For all plots, across all values of λ and H, we see that COCOA+ converges to the
optimal solution faster than COCOA, in terms of both the number of communications and the elapsed time.

chines. The practical variant DisDCA-p proposed in (Yang,
2013) allows additive updates but is restricted to SDCA
updates, and was proposed without convergence guaran-
tees. DisDCA-p can be recovered as a special case of the
COCOA+ framework when using SDCA as a local solver,
if nk = n/K and σ(cid:48) := K, see Appendix C. The theory
presented here also therefore covers that method.

ADMM. An alternative approach to distributed optimiza-
tion is to use the alternating direction method of multipli-
ers (ADMM), as used for distributed SVM training in, e.g.,
(Forero et al., 2010). This uses a penalty parameter balanc-
ing between the equality constraint w and the optimization
objective (Boyd et al., 2011). However, the known conver-
gence rates for ADMM are weaker than the more problem-
tailored methods mentioned previously, and the choice of
the penalty parameter is often unclear.

Batch Proximal Methods.
In spirit, for the special case
of adding (γ = 1), COCOA+ resembles a batch proximal
method, using the separable approximation (9) instead of
the original dual (2). Known batch proximal methods re-
quire high accuracy subproblem solutions, and don’t allow
arbitrary solvers of weak accuracy Θ such as we do here.

7. Numerical Experiments

We present experiments on several large real-world dis-
tributed datasets. We show that COCOA+ converges faster
in terms of total rounds as well as elapsed time as compared
to COCOA in all cases, despite varying: the dataset, values
of regularization, batch size, and cluster size (Section 7.2).
In Section 7.3 we demonstrate that this performance trans-
lates to orders of magnitude improvement in convergence
when scaling up the number of machines K, as compared
to COCOA as well as to several other state-of-the-art meth-
ods. Finally, in Section 7.4 we investigate the impact of the
local subproblem parameter σ(cid:48) in the COCOA+ framework.

Table 2. Datasets for Numerical Experiments.
n
522,911
400,000
677,399

Dataset
covertype
epsilon
RCV1

Sparsity
22.22%
100%
0.16%

d
54
2,000
47,236

7.1. Implementation Details

We implement all algorithms in Apache Spark (Zaharia
et al., 2012) and run them on m3.large Amazon EC2 in-
stances, applying each method to the binary hinge-loss sup-

Adding vs. Averaging in Distributed Primal-Dual Optimization

port vector machine. The analysis for this non-smooth loss
was not covered in (Jaggi et al., 2014) but has been captured
here, and thus is both theoretically and practically justiﬁed.
The used datasets are summarized in Table 2.

For illustration and ease of comparison, we here use SDCA
(Shalev-Shwartz & Zhang, 2013c) as the local solver for
both COCOA and COCOA+. Note that in this special case,
and if additionally σ(cid:48) := K, and if the partitioning nk =
n/K is balanced, once can show that the COCOA+ frame-
work reduces to the practical variant of DisDCA (Yang,
2013) (which had no convergence guarantees so far). We
include more details on the connection in Appendix C.

7.2. Comparison of COCOA+ and COCOA
We compare the COCOA+ and COCOA frameworks di-
rectly using two datasets (Covertype and RCV1) across var-
ious values of λ, the regularizer, in Figure 1. For each value
of λ we consider both methods with different values of H,
the number of local iterations performed before communi-
cating to the master. For all runs of COCOA+ we use the
safe upper bound of γK for σ(cid:48). In terms of both the to-
tal number of communications made and the elapsed time,
COCOA+ (shown in blue) converges to the optimal solu-
tion faster than COCOA (red). The discrepancy is larger
for greater values of λ, where the strongly convex regular-
izer has more of an impact and the problem difﬁculty is re-
duced. We also see a greater performance gap for smaller
values of H, where there is frequent communication be-
tween the machines and the master, and changes between
the algorithms therefore play a larger role.

7.3. Scaling the Number of Machines K
In Figure 2 we demonstrate the ability of COCOA+ to
scale with an increasing number of machines K. The
experiments conﬁrm the ability of strong scaling of the
new method, as predicted by our theory in Section 4,
in contrast to the competing methods. Unlike COCOA,
which becomes linearly slower when increasing the num-
ber of machines, the performance of COCOA+ improves
with additional machines, only starting to degrade slightly
once K=16 for the RCV1 dataset.

7.4. Impact of the Subproblem Parameter σ(cid:48)

Finally, in Figure 3, we consider the effect of the choice
of the subproblem parameter σ(cid:48) on convergence. We plot
both the number of communications and clock time on a
log-log scale for the RCV1 dataset with K=8 and H=1e4.
For γ = 1 (the most aggressive variant of COCOA+ in
which updates are added) we consider several different val-
ues of σ(cid:48), ranging from 1 to 8. The value σ(cid:48)=8 represents
the safe upper bound of γK. The optimal convergence oc-
curs around σ(cid:48)=4, and diverges for σ(cid:48) ≤ 2. Notably, we

Figure 2. The effect of increasing K on the time (s) to reach an
(cid:15)D-accurate solution. We see that COCOA+ converges twice as
fast as COCOA on 100 machines for the Epsilon dataset, and
nearly 7 times as quickly for the RCV1 dataset. Mini-batch SGD
converges an order of magnitude more slowly than both methods.

see that the easy to calculate upper bound of σ(cid:48) := γK (as
given by Lemma 4) has only slightly worse performance
than best possible subproblem parameter in our setting.

Figure 3. The effect of σ(cid:48) on convergence of COCOA+ for the
RCV1 dataset distributed across K=8 machines. Decreasing σ(cid:48)
improves performance in terms of communication and overall run
time until a certain point, after which the algorithm diverges. The
“safe” upper bound of σ(cid:48):=K=8 has only slightly worse perfor-
mance than the practically best “un-safe” value of σ(cid:48).

8. Conclusion
In conclusion, we present a novel framework COCOA+
that allows for fast and communication-efﬁcient additive
aggregation in distributed algorithms for primal-dual opti-
mization. We analyze the theoretical performance of this
method, giving strong primal-dual convergence rates with
outer iterations scaling independently of the number of ma-
chines. We extended our theory to allow for non-smooth
losses. Our experimental results show signiﬁcant speedups
over previous methods,
including the original COCOA
framework as well as other state-of-the-art methods.

Acknowledgments. We thank Ching-pei Lee and an
anonymous reviewer for several helpful insights and com-
ments.

Adding vs. Averaging in Distributed Primal-Dual Optimization

References

Balcan, M.-F., Blum, A., Fine, S., and Mansour, Y. Dis-
tributed Learning, Communication Complexity and Pri-
vacy. In COLT, pp. 26.1–26.22, 2012.

Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J.
Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3(1):1–122, 2011.

Duchi, J. C., Jordan, M. I., and McMahan, H. B. Estima-
tion, Optimization, and Parallelism when Data is Sparse.
In NIPS, 2013.

Fercoq, O. and Richt´arik, P. Accelerated, parallel and prox-

imal coordinate descent. arXiv:1312.5799, 2013.

Fercoq, O., Qu, Z., Richt´arik, P., and Tak´aˇc, M. Fast
distributed coordinate descent for non-strongly convex
losses. IEEE Workshop on Machine Learning for Signal
Processing, 2014.

Forero, P. A., Cano, A., and Giannakis, G. B. Consensus-
Based Distributed Support Vector Machines. JMLR, 11:
1663–1707, 2010.

Jaggi, M., Smith, V., Tak´aˇc, M., Terhorst, J., Krishnan, S.,
Hofmann, T., and Jordan, M. I. Communication-efﬁcient
distributed dual coordinate ascent. In NIPS, 2014.

Lee, C.-P. and Roth, D. Distributed Box-Constrained
Quadratic Optimization for Dual Linear SVM. In ICML,
2015.

Liu, J. and Wright, S. J. Asynchronous stochastic coor-
dinate descent: Parallelism and convergence properties.
arXiv:1403.3862, 2014.

Liu, J., Wright, S. J., R´e, C., Bittorf, V., and Sridhar,
S. An Asynchronous Parallel Stochastic Coordinate De-
scent Algorithm. In ICML, 2014.

Lu, Z. and Xiao, L. On the complexity analysis of random-
ized block-coordinate descent methods. arXiv preprint
arXiv:1305.4723, 2013.

Mann, G., McDonald, R., Mohri, M., Silberman, N., and
Walker, D. D. Efﬁcient Large-Scale Distributed Training
of Conditional Maximum Entropy Models. NIPS, 2009.

Mareˇcek, J., Richt´arik, P., and Tak´aˇc, M. Distributed block
coordinate descent for minimizing partially separable
functions. arXiv:1406.0238, 2014.

McWilliams, B., Heinze, C., Meinshausen, N., Krumme-
nacher, G., and Vanchinathan, H. P. LOCO: Distribut-
ing Ridge Regression with Random Projections. arXiv
stat.ML, June 2014.

Niu, F., Recht, B., R´e, C., and Wright, S. J. Hogwild!: A
Lock-Free Approach to Parallelizing Stochastic Gradient
Descent. In NIPS, 2011.

Pechyony, D., Shen, L., and Jones, R. Solving Large Scale
In
Linear SVM with Distributed Block Minimization.
NIPS Workshop on Big Learning, 2011.

Qu, Z. and Richt´arik, P.

Coordinate descent with
arbitrary sampling I: Algorithms and complexity.
arXiv:1412.8060, 2014.

Qu, Z., Richt´arik, P., and Zhang, T. Randomized dual coor-
dinate ascent with arbitrary sampling. arXiv:1411.5873,
2014.

Richt´arik, P. and Tak´aˇc, M. Distributed coordinate de-
scent method for learning with big data. arXiv preprint
arXiv:1310.2059, 2013.

Richt´arik, P. and Tak´aˇc, M.

Iteration complexity of ran-
domized block-coordinate descent methods for minimiz-
ing a composite function. Mathematical Programming,
144(1-2):1–38, April 2014.

Richt´arik, P. and Tak´aˇc, M. Parallel coordinate descent
methods for big data optimization. Mathematical Pro-
gramming, pp. 1–52, 2015.

Shalev-Shwartz, S. and Zhang, T. Accelerated mini-batch

stochastic dual coordinate ascent. In NIPS, 2013a.

Shalev-Shwartz, S. and Zhang, T. Accelerated proximal
stochastic dual coordinate ascent for regularized loss
minimization. arXiv:1309.2375, 2013b.

Shalev-Shwartz, S. and Zhang, T. Stochastic Dual Coor-
dinate Ascent Methods for Regularized Loss Minimiza-
tion. JMLR, 14:567–599, 2013c.

Shamir, O. and Srebro, N. Distributed Stochastic Optimiza-

tion and Learning . In Allerton, 2014.

Shamir, O., Srebro, N., and Zhang, T. Communication
efﬁcient distributed optimization using an approximate
newton-type method. In ICML, 2014.

Tappenden, R., Tak´aˇc, M., and Richt´arik, P. On the com-
plexity of parallel coordinate descent. Technical report,
2015. ERGO 15-001, University of Edinburgh.

Yang, T. Trading Computation for Communication: Dis-
In NIPS,

tributed Stochastic Dual Coordinate Ascent.
2013.

Yang, T., Zhu, S., Jin, R., and Lin, Y. On Theoretical Anal-
ysis of Distributed Stochastic Dual Coordinate Ascent.
arXiv:1312.1031, 2013.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Yu, H.-F., Hsieh, C.-J., Chang, K.-W., and Lin, C.-J. Large
Linear Classiﬁcation When Data Cannot Fit in Memory.
TKDD, 5(4):1–23, 2012.

Zaharia, M., Chowdhury, M., Das, T., Dave, A., McCauley,
M., Franklin, M. J., Shenker, S., and Stoica, I. Resilient
Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing. In NSDI, 2012.

Zhang, Y. and Lin, X. DiSCO: Distributed Optimization for
Self-Concordant Empirical Loss. In ICML, pp. 362–370,
2015.

Zhang, Y., Duchi,

J.
Communication-Efﬁcient Algorithms for Statistical Op-
timization. JMLR, 14:3321–3363, 2013.

and Wainwright, M.

J. C.,

Zinkevich, M. A., Weimer, M., Smola, A. J., and Li, L.
Parallelized Stochastic Gradient Descent. NIPS, 2010.

Adding vs. Averaging in Distributed Primal-Dual Optimization

Appendix

A. Technical Lemmas

Lemma 16 (Lemma 21 in (Shalev-Shwartz & Zhang, 2013c)). Let (cid:96)i : R → R be an L-Lipschitz continuous. Then for
any real value a with |a| > L we have that (cid:96)∗

i (a) = ∞.

Lemma 17. Assuming the loss functions (cid:96)i are bounded by (cid:96)i(0) ≤ 1 for all i ∈ [n] (as we have assumed in (5) above),
then for the zero vector α(0) := 0 ∈ Rn, we have

D(α∗) − D(α(0)) = D(α∗) − D(0) ≤ 1.

(24)

Proof. For α := 0 ∈ Rn, we have w(α) = 1
in (2),

λn Aα = 0 ∈ Rd. Therefore, by deﬁnition of the dual objective D given

0 ≤ D(α∗) − D(α) ≤ P(w(α)) − D(α) = 0 − D(α)

(5),(2)

≤ 1.

B. Proofs

B.1. Proof of Lemma 3

Indeed, we have

D(α + γ

∆α[k]) = −

(cid:96)∗
i (−αi − γ(

∆α[k])i)

−

A(α + γ

∆α[k])

(25)

K
(cid:88)

k=1

1
n

n
(cid:88)

i=1

(cid:124)

K
(cid:88)

k=1

1
λn

λ
2

(cid:13)
(cid:13)
(cid:13)

(cid:124)

(cid:125)

K
(cid:88)

k=1

(cid:123)(cid:122)
B

(cid:13)
2
(cid:13)
(cid:13)

.

(cid:125)

Now, let us bound the terms A and B separately. We have

A = −

(cid:96)∗
i (−αi − γ(∆α[k])i)

= −

(cid:96)∗
i (−(1 − γ)αi − γ(α + ∆α[k])i)

(cid:33)

1
n

1
n

(cid:32)

K
(cid:88)

(cid:88)

k=1

K
(cid:88)

(cid:32)

i∈Pk

(cid:88)

k=1

i∈Pk

≥ −

(1 − γ)(cid:96)∗

i (−αi) + γ(cid:96)∗

i (−(α + ∆α[k])i)

.

(cid:123)(cid:122)
A

(cid:33)

1
n

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk
(cid:33)

Where the last inequality is due to Jensen’s inequality. Now we will bound B, using the safe separability measurement σ(cid:48)
as deﬁned in (11).

B =

A(α + γ

(cid:13)
(cid:13)
(cid:13)

1
λn

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
∆α[k])
(cid:13)

=

(cid:13)
(cid:13)
(cid:13)w(α) + γ

1
λn

K
(cid:88)

k=1

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

= (cid:107)w(α)(cid:107)2 +

w(α)T A∆α[k] + γ

(cid:16) 1
λn

(cid:17)2

(cid:13)
(cid:13)
(cid:13)

γ

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

(11)
≤ (cid:107)w(α)(cid:107)2 +

w(α)T A∆α[k] + γ

(cid:107)A∆α[k](cid:107)2.

(cid:17)2

σ(cid:48)

(cid:16) 1
λn

K
(cid:88)

k=1

2γ

1
λn

K
(cid:88)

k=1

2γ

1
λn

K
(cid:88)

k=1

K
(cid:88)

k=1

− γ

(cid:107)w(α)(cid:107)2 − (1 − γ)

(cid:107)w(α)(cid:107)2 −

w(α)T A∆α[k] −

λ
2

γ

(cid:16) 1
λn

(cid:17)2

σ(cid:48)

K
(cid:88)

k=1

(cid:107)A∆α[k](cid:107)2

Adding vs. Averaging in Distributed Primal-Dual Optimization

Plugging A and B into (25) will give us

D(α + γ

∆α[k]) ≥ −

(1 − γ)(cid:96)∗

i (−αi) + γ(cid:96)∗

i (−(α + ∆α[k])i)

(cid:33)

K
(cid:88)

k=1

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk

1
n

λ
2

(cid:32)

K
(cid:88)

(cid:88)

k=1

i∈Pk

= −

1
n

(cid:124)

+ γ

(cid:32)

K
(cid:88)

k=1

−

1
n

(cid:88)

i∈Pk

λ
2

(cid:33)

λ
2

K
(cid:88)

k=1

2γ

1
λn

(1 − γ)(cid:96)∗

i (−αi)

− (1 − γ)

(cid:107)w(α)(cid:107)2

(cid:123)(cid:122)
(1−γ)D(α)

(cid:125)

λ
2

1
K

λ
2

(9)=(1 − γ)D(α) + γ

Gσ(cid:48)
k (∆α[k]; w, α[k]).

K
(cid:88)

k=1

(cid:96)∗
i (−(α + ∆α[k])i) −

(cid:107)w(α)(cid:107)2 −

w(α)T A∆α[k] −

1
n

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α[k]

2(cid:33)
(cid:13)
(cid:13)
(cid:13)

B.2. Proof of Lemma 4

See (Richt´arik & Tak´aˇc, 2013).

B.3. Proof of Lemma 5

For sake of notation, we will write α instead of α(t), w instead of w(α(t)) and u instead of u(t).

Now, let us estimate the expected change of the dual objective. Using the deﬁnition of the dual update α(t+1) := α(t) +
γ (cid:80)

k ∆α[k] resulting in Algorithm 1, we have

(cid:104)
E(cid:2)D(α(t)) − D(α(t+1))(cid:3) = E

D(α) − D(α + γ

(by Lemma 3 on the local function Gσ(cid:48)

k (α; w, α[k]) approximating the global objective D(α))

(cid:104)
≤ E

D(α) − (1 − γ)D(α) − γ

Gσ(cid:48)
k (∆α(t)

(cid:105)
[k]; w, α[k])

(cid:104)
= γE
D(α) −

Gσ(cid:48)
k (∆α(t)

[k]; w, α[k])

(cid:105)

(cid:105)
∆α[k])

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:104)
= γE
D(α) −

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) +

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) −

(by the notion of quality (12) of the local solver, as in Assumption 1)

(cid:18)

≤ γ

D(α) −

K
(cid:88)

k=1

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) + Θ

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) −

Gσ(cid:48)
k (0; w, α[k])

K
(cid:88)

k=1

(cid:16) K
(cid:88)

k=1

Gσ(cid:48)
k (∆α(t)

(cid:105)
[k]; w, α[k])

K
(cid:88)

k=1

K
(cid:88)

k=1
(cid:124)

(cid:123)(cid:122)
D(α)

(cid:125)

(cid:17)(cid:19)

(26)

= γ(1 − Θ)

D(α) −

(cid:16)

(cid:124)

K
(cid:88)

k=1

Gσ(cid:48)
k (∆α∗

[k]; w, α[k])

(cid:17)

.

(cid:123)(cid:122)
C

(cid:125)

Adding vs. Averaging in Distributed Primal-Dual Optimization

Now, let us upper bound the C term (we will denote by ∆α∗ = (cid:80)K

k=1 ∆α∗

[k]):

((cid:96)∗

i (−αi − ∆α∗

i ) − (cid:96)∗

i (−αi)) +

w(α)T A∆α∗ +

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α∗
[k]

(cid:13)
2
(cid:13)
(cid:13)

((cid:96)∗

i (−αi − s(ui − αi)) − (cid:96)∗

i (−αi)) +

w(α)T As(u − α) +

(2),(9)
=

C

1
n

n
(cid:88)

i=1

n
(cid:88)

≤

1
n

i=1
Strong conv.
≤

1
n

1
n

µ
2

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

(cid:17)

+

1
n

s(cid:96)∗

i (−ui) + (1 − s)(cid:96)∗

i (−αi) −

(1 − s)s(ui − αi)2 − (cid:96)∗

i (−αi)

w(α)T As(u − α)

n
(cid:88)

(cid:16)

1
n

+

i=1

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

=

1
n

n
(cid:88)

(cid:16)

i=1

s(cid:96)∗

i (−ui) − s(cid:96)∗

i (−αi) −

(1 − s)s(ui − αi)2(cid:17)

+

µ
2

1
n

w(α)T As(u − α) +

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

As(u − α)[k]

(cid:13)
2
(cid:13)
(cid:13)

.

The convex conjugate maximal property implies that

i (−ui) = −uiw(α)T xi − (cid:96)i(w(α)T xi).
(cid:96)∗

Moreover, from the deﬁnition of the primal and dual optimization problems (1), (2), we can write the duality gap as

G(α) := P(w(α)) − D(α)

(cid:0)(cid:96)i(xT

j w) + (cid:96)∗

i (−αi) + w(α)T xiαi

(cid:1) .

(1),(2)
=

1
n

n
(cid:88)

i=1


−suiw(α)T xi − s(cid:96)i(w(α)T xi) − s(cid:96)∗

i (−αi) −sw(α)T xiαi + sw(α)T xiαi
(cid:125)

(cid:124)

(cid:123)(cid:122)
0

−

(1 − s)s(ui − αi)2

Hence,

(27)
≤

C

1
n

n
(cid:88)

i=1

=

1
n

n
(cid:88)

i=1

1
n

1
n

+

w(α)T As(u − α) +

As(u − α)[k]

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

K
(cid:88)

k=1

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

(cid:1) +

1
n

n
(cid:88)

(cid:16)

i=1

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+

w(α)T As(u − α) +

As(u − α)[k]

(28)= −sG(α) −

(1 − s)s

(cid:107)u − α(cid:107)2 +

(cid:107)A(u − α)[k](cid:107)2.

µ
2

1
n

n
(cid:88)

i=1

σ(cid:48)
2λ

(

s
n

)2

K
(cid:88)

k=1

Now, the claimed improvement bound (15) follows by plugging (29) into (26).

µ
2

µ
2

(cid:0)−s(cid:96)i(w(α)T xi) − s(cid:96)∗

i (−αi) − sw(α)T xiαi

sw(α)T xi(αi − ui) −

(1 − s)s(ui − αi)2(cid:17)

(27)

(28)





(29)

B.4. Proof of Lemma 6

For general convex functions, the strong convexity parameter is µ = 0, and hence the deﬁnition of R(t) becomes

R(t) (16)=

(cid:107)A(u(t) − α(t))[k](cid:107)2

σk(cid:107)(u(t) − α(t))[k](cid:107)2 Lemma 16

≤

σk|Pk|4L2.

K
(cid:88)

k=1

(19)
≤

K
(cid:88)

k=1

K
(cid:88)

k=1

Adding vs. Averaging in Distributed Primal-Dual Optimization

B.5. Proof of Theorem 8

At ﬁrst let us estimate expected change of dual feasibility. By using the main Lemma 5, we have

E[D(α∗) − D(α(t+1))] = E[D(α∗) − D(α(t+1)) + D(α(t)) − D(α(t))]

(15)= D(α∗) − D(α(t)) − γ(1 − Θ)sG(α(t)) + γ(1 − Θ) σ(cid:48)
2λ (

)2R(t)

s
n

(4)= D(α∗) − D(α(t)) − γ(1 − Θ)s(P(w(α(t))) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

)2R(t)

≤ D(α∗) − D(α(t)) − γ(1 − Θ)s(D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

(18)
≤ (1 − γ(1 − Θ)s) (D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ.

s
n

s
n
)2R(t)

s
n

Using (30) recursively we have

E[D(α∗) − D(α(t))] = (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ

(1 − γ(1 − Θ)s)j

s
n

s
n

t−1
(cid:88)

j=0
1 − (1 − γ(1 − Θ)s)t
γ(1 − Θ)s

= (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + γ(1 − Θ) σ(cid:48)
2λ (

)24L2σ

≤ (1 − γ(1 − Θ)s)t (D(α∗) − D(α(0))) + s

4L2σσ(cid:48)
2λn2 .

Choice of s = 1 and t = t0 := max{0, (cid:100)

E[D(α∗) − D(α(t))] ≤ (1 − γ(1 − Θ))t0 (D(α∗) − D(α(0))) +

1

γ(1−Θ) log(2λn2(D(α∗) − D(α(0)))/(4L2σσ(cid:48)))(cid:101)} will lead to
4L2σσ(cid:48)
2λn2 ≤

4L2σσ(cid:48)
2λn2 =

4L2σσ(cid:48)
2λn2 +

4L2σσ(cid:48)
λn2

.

Now, we are going to show that

∀t ≥ t0 : E[D(α∗) − D(α(t))] ≤

λn2(1 + 1

4L2σσ(cid:48)
2 γ(1 − Θ)(t − t0))

.

Clearly, (32) implies that (33) holds for t = t0. Now imagine that it holds for any t ≥ t0 then we show that it also has to
hold for t + 1. Indeed, using

1

s =

1 + 1

2 γ(1 − Θ)(t − t0)

∈ [0, 1]

we obtain

E[D(α∗) − D(α(t+1))]

(30)
≤ (1 − γ(1 − Θ)s) (D(α∗) − D(α(t))) + γ(1 − Θ) σ(cid:48)
2λ (

(30)

(31)

(32)

(33)

(34)

)24L2σ

s
n
+ γ(1 − Θ) σ(cid:48)
2λ (

s
n

)24L2σ

(33)
≤ (1 − γ(1 − Θ)s)

(cid:18) 1 + 1

(cid:18) 1 + 1

(34)=

4L2σσ(cid:48)
λn2

=

4L2σσ(cid:48)
λn2

(cid:124)

λn2(1 + 1

4L2σσ(cid:48)
2 γ(1 − Θ)(t − t0))
2 γ(1 − Θ)(t − t0) − γ(1 − Θ) + γ(1 − Θ) 1
2 γ(1 − Θ)(t − t0))2
(cid:19)
2 γ(1 − Θ)

(1 + 1
2 γ(1 − Θ)(t − t0) − 1
(1 + 1

2 γ(1 − Θ)(t − t0))2

2

.

(cid:19)

(cid:125)

(cid:123)(cid:122)
D

(1 + 1

2 γ(1 − Θ)(t + 1 − t0))(1 + 1
(1 + 1

2 γ(1 − Θ)(t − t0))2

2 γ(1 − Θ)(t − 1 − t0))

(cid:123)(cid:122)
≤1

(cid:125)

Now, we will upperbound D as follows

D =

≤

1 + 1

2 γ(1 − Θ)(t + 1 − t0)

1 + 1

2 γ(1 − Θ)(t + 1 − t0)

(cid:124)

,

1

1

Adding vs. Averaging in Distributed Primal-Dual Optimization

where in the last inequality we have used the fact that geometric mean is less or equal to arithmetic mean.

If α is deﬁned as (21) then we obtain that

E[G(α)] = E

G

1
T −T0

α(t)

≤ 1

T −T0

E

(cid:34)

(cid:32) T −1
(cid:88)

t=T0

(cid:33)(cid:35)

(cid:35)

(cid:16)

α(t)(cid:17)

G

(cid:34) T −1
(cid:88)

t=T0

(15),(18)
≤

1
T −T0

E

(cid:34) T −1
(cid:88)

(cid:18)

1
γ(1 − Θ)s

(D(α(t+1)) − D(α(t))) + 4L2σσ(cid:48)s

2λn2

(cid:19)(cid:35)

=

≤

1
γ(1 − Θ)s
1
γ(1 − Θ)s

t=T0
1
T − T0
1
T − T0

(cid:104)
D(α(T )) − D(α(T0))

(cid:105)

E

+ 4L2σσ(cid:48)s
2λn2

(cid:105)
(cid:104)
D(α∗) − D(α(T0))

E

+ 4L2σσ(cid:48)s
2λn2

.

Now, if T ≥ (cid:100)

1

γ(1−Θ) (cid:101) + T0 such that T0 ≥ t0 we obtain
(cid:18)

E[G(α)]

(35),(33)
≤

1
γ(1 − Θ)s
(cid:18)

1
T − T0
1
γ(1 − Θ)s

4L2σσ(cid:48)
λn2

=

λn2(1 + 1
1
T − T0

1 + 1

4L2σσ(cid:48)

1

2 γ(1 − Θ)(T0 − t0))

2 γ(1 − Θ)(T0 − t0)

4L2σσ(cid:48)s
2λn2

(cid:19)

+

(cid:19)

s
2

+

.

s =

1
(T − T0)γ(1 − Θ)

∈ [0, 1]

E[G(α)]

(36),(37)
≤

(cid:18)

4L2σσ(cid:48)
λn2

1 + 1

2 γ(1 − Θ)(T0 − t0)

+

1
(T − T0)γ(1 − Θ)

1
2

(cid:19)

.

To have right hand side of (38) smaller then (cid:15)G it is sufﬁcient to choose T0 and T such that
(cid:18)

(cid:19)

1

1

Choosing

gives us

Hence of if

4L2σσ(cid:48)
λn2

1 + 1
(cid:18)

2 γ(1 − Θ)(T0 − t0)
1
1
2
(T − T0)γ(1 − Θ)

(cid:19)

4L2σσ(cid:48)
λn2

≤

≤

1
2

1
2

(cid:15)G,

(cid:15)G.

t0 +

2
γ(1 − Θ)

(cid:19)

− 1

(cid:18) 8L2σσ(cid:48)
λn2(cid:15)G
4L2σσ(cid:48)
λn2(cid:15)Gγ(1 − Θ)

≤ T0,

≤ T,

T0 +

then (39) and (40) are satisﬁed.

B.6. Proof of Theorem 10

If the function (cid:96)i(.) is (1/µ)-smooth then (cid:96)∗

i (.) is µ-strongly convex with respect to the (cid:107) · (cid:107) norm. From (16) we have

(cid:88)K

k=1

(cid:88)K

R(t) (16)= − λµn(1−s)

(cid:107)u(t) − α(t)(cid:107)2 +

σ(cid:48)s

(cid:107)A(u(t) − α(t))[k](cid:107)2

(19)
≤ − λµn(1−s)

σ(cid:48)s

(cid:107)u(t) − α(t)(cid:107)2 +

≤ − λµn(1−s)

(cid:16)

σ(cid:48)s
− λµn(1−s)

=

(cid:107)u(t) − α(t)(cid:107)2 + σmax
(cid:17)

σ(cid:48)s + σmax

(cid:107)u(t) − α(t)(cid:107)2.

σk(cid:107)u(t) − α(t)

[k](cid:107)2
(cid:107)u(t) − α(t)

k=1
(cid:88)K

[k](cid:107)2

k=1

(35)

(36)

(37)

(38)

(39)

(40)

(41)

Adding vs. Averaging in Distributed Primal-Dual Optimization

If we plug

λµn

s =

λµn + σmaxσ(cid:48) ∈ [0, 1]

(42)

into (41) we obtain that ∀t : R(t) ≤ 0. Putting the same s into (15) will give us

E[D(α(t+1)) − D(α(t))]

≥ γ(1 − Θ)

(15),(42)

λµn

λµn + σmaxσ(cid:48) G(α(t)) ≥ γ(1 − Θ)

λµn + σmaxσ(cid:48) D(α∗) − D(α(t)).

(43)

λµn

Using the fact that E[D(α(t+1)) − D(α(t))] = E[D(α(t+1)) − D(α∗)] + D(α∗) − D(α(t)) we have

E[D(α(t+1)) − D(α∗)] + D(α∗) − D(α(t))

(43)
≥ γ(1 − Θ)

λµn

λµn + σmaxσ(cid:48) D(α∗) − D(α(t))

which is equivalent with

E[D(α∗) − D(α(t+1))] ≤

1 − γ(1 − Θ)

D(α∗) − D(α(t)).

(44)

(cid:18)

(cid:19)

λµn
λµn + σmaxσ(cid:48)

Therefore if we denote by (cid:15)(t)

D = D(α∗) − D(α(t)) we have that

(cid:18)

E[(cid:15)(t)
D ]

(44)
≤

1 − γ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)t

(cid:18)

(cid:15)(0)
D

(24)
≤

1 − γ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)t

(cid:18)

≤ exp

−tγ(1 − Θ)

λµn
λµn + σmaxσ(cid:48)

(cid:19)

.

The right hand side will be smaller than some (cid:15)D if

t ≥

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

log

1
(cid:15)D

.

Moreover, to bound the duality gap, we have

γ(1 − Θ)

λµn

λµn + σmaxσ(cid:48) G(α(t))

(43)
≤ E[D(α(t+1)) − D(α(t))] ≤ E[D(α∗) − D(α(t))].

Therefore G(α(t)) ≤

1
γ(1−Θ)

λµn+σmaxσ(cid:48)
λµn

(cid:15)(t)
D . Hence if (cid:15)D ≤ γ(1 − Θ)

λµn

λµn+σmaxσ(cid:48) (cid:15)G then G(α(t)) ≤ (cid:15)G. Therefore after

t ≥

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

(cid:18)

log

1
γ(1 − Θ)

λµn + σmaxσ(cid:48)
λµn

1
(cid:15)G

(cid:19)

iterations we have obtained a duality gap less than (cid:15)G.

B.7. Proof of Theorem 13

Because (cid:96)i are (1/µ)-smooth then functions (cid:96)∗
i are µ strongly convex with respect to the norm (cid:107) · (cid:107). The proof is based on
techniques developed in recent coordinate descent papers, including (Richt´arik & Tak´aˇc, 2014; 2013; Richt´arik & Tak´aˇc,
2015; Tappenden et al., 2015; Mareˇcek et al., 2014; Fercoq & Richt´arik, 2013; Lu & Xiao, 2013; Fercoq et al., 2014;
Qu & Richt´arik, 2014; Qu et al., 2014) (Efﬁcient accelerated variants were considered in (Fercoq & Richt´arik, 2013;
Shalev-Shwartz & Zhang, 2013a)).
First, let us deﬁne the function F (ζ) : Rnk → R as F (ζ) := −Gσ(cid:48)
ζiei; w, α[k]). This function can be written in
two parts F (ζ) = Φ(ζ)+f (ζ). The ﬁrst part denoted by Φ(ζ) = 1
(cid:96)∗
i (−αi −ζi) is strongly convex with convexity
n
parameter µ
n with respect to the standard Euclidean norm. In our application, we think of the ζ variable collecting the local
dual variables ∆α[k].

k ((cid:80)
(cid:80)

i∈Pk

i∈Pk

The second part we will denote by f (ζ) = 1
λ
K
to show that the gradient of f is coordinate-wise Lipschitz continuous with Lipschitz constant σ(cid:48)
standard Euclidean norm.

2 (cid:107)w(α)(cid:107)2 + 1

w(α)T xiζi + λ

2 σ(cid:48)

i∈Pk

n

1

(cid:80)

λ2n2 (cid:107) (cid:80)

i∈Pk

xiζi(cid:107)2. It is easy
λn2 rmax with respect to the

Adding vs. Averaging in Distributed Primal-Dual Optimization

Following the proof of Theorem 20 in (Richt´arik & Tak´aˇc, 2015), we obtain that

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h+1)

[k]

; w, α[k])] ≤

1 −

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

(cid:17)
[k] ; w, α[k])

(cid:32)

(cid:18)

=

1 −

(cid:16)

(cid:33)

1 + µnλ
σ(cid:48)rmax
µnλ
σ(cid:48)rmax
λnµ
σ(cid:48)rmax + λnµ

1
nk

1
nk

(cid:19) (cid:16)

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])

(cid:17)

.

Over all steps up to step h, this gives

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])] ≤

(cid:18)

1 −

1
nk

λnµ
σ(cid:48)rmax + λnµ

(cid:19)h (cid:16)

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (0; w, α[k])

(cid:17)

.

Therefore, choosing H as in the assumption of our Theorem, given in Equation (22), we are guaranteed that
(cid:16)

(cid:17)H

1 − 1
nk

λnµ
σ(cid:48)rmax+λnµ

≤ Θ, as desired.

B.8. Proof of Theorem 14

Similarly as in the proof of Theorem 13 we deﬁne a composite function F (ζ) = f (ζ) + Φ(ζ). However, in this case func-
tions (cid:96)∗
i are not guaranteed to be strongly convex. However, the ﬁrst part has still a coordinate-wise Lipschitz continuous
gradient with constant σ(cid:48)
λn2 rmax with respect to the standard Euclidean norm. Therefore from Theorem 3 in (Tappenden
et al., 2015) we have that

E[Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (∆α(h)

[k] ; w, α[k])] ≤

Gσ(cid:48)
k (∆α∗

[k]; w, α[k]) − Gσ(cid:48)

k (0; w, α[k]) +

(cid:18)

nk
nk + h

1
2

σ(cid:48)rmax
λn2 (cid:107)∆α∗

[k](cid:107)2

(cid:19)

.

(45)

Now, choice of h = H from (23) is sufﬁcient to have the right hand side of (45) to be ≤ Θ(cid:0)Gσ(cid:48)
k (0; w, α[k])(cid:1).
Gσ(cid:48)

k (∆α∗

[k]; w, α[k]) −

C. Relationship of DisDCA to COCOA+

We are indebted to Ching-pei Lee for showing the following relationship between the practical variant of DisDCA (Yang,
2013), and COCOA+ when SDCA is chosen as the local solver:

Considering the practical variant of DisDCA (DisDCA-p, see Figure 2 in (Yang, 2013)) using the scaling parameter scl =
K, the following holds:
K . If within the COCOA+
Lemma 18. Assume that the dataset is partitioned equally between workers, i.e. ∀k : nk = n
framework, SDCA is used as a local solver, and the subproblems are formulated using our shown “safe” (but pessimistic)
upper bound of σ(cid:48) = K, with aggregation parameter γ = 1 (adding), then the COCOA+ framework reduces exactly to the
DisDCA-p algorithm.

Proof. (Due to Ching-pei Lee, with some reformulations). As deﬁned in (9), the data-local subproblem solved by each
machine in COCOA+ is deﬁned as

max
∆α[k]∈Rn

Gσ(cid:48)
k (∆α[k]; w, α[k])

where

Gσ(cid:48)
k (∆α[k]; w, α[k]) := −

(cid:96)∗
i (−αi − (∆α[k])i) −

(cid:107)w(cid:107)2 −

wT A∆α[k] −

1
K

λ
2

1
n

σ(cid:48)(cid:13)
(cid:13)
(cid:13)

λ
2

1
λn

A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

.

1
n

(cid:88)

i∈Pk

We rewrite the local problem by scaling with n, and removing the constant regularizer term 1
K

λ

2 (cid:107)w(cid:107)2, i.e.

˜Gσ(cid:48)
k (∆α[k]; w) := −

i (−αj − (∆α[k])j) − wT A∆α[k] −
(cid:96)∗

σ(cid:48)
2λn

(cid:13)
(cid:13)
(cid:13)A∆α[k]

(cid:13)
2
(cid:13)
(cid:13)

.

(46)

(cid:88)

j∈Pk

Adding vs. Averaging in Distributed Primal-Dual Optimization

For the correspondence of interest, we now restrict to single coordinate updates in the local solver. In other words, the
local solver optimizes exactly one coordinate i ∈ Pk at a time. To relate the single coordinate update to the set of local
variables, we will use the notation

∆α[k] =: ∆αprev

[k] + δei ,

so that ∆αprev

[k] are the previous local variables, and ∆α[k] will be the updated ones.

From now on, we will consider the special case of COCOA+ when the quadratic upper bound parameter is chosen as the
“safe” value σ(cid:48) = K, combined with adding as the aggregation, i.e. γ = 1.
Now if the local solver within COCOA+ is chosen as LOCALSDCA, then one local step on the subproblem (9) will
calculate the following coordinate update. Recall that A = [x1, x2, . . . , xn] ∈ Rd×n.

δ(cid:63) := arg max

˜Gσ(cid:48)
k (∆α[k]; w)

δ∈R

which – because it is only affecting one single coordinate, employing (47) – can be expressed as

δ(cid:63) := arg max

−(cid:96)∗

i (−(αi + (∆αprev

[k] )i + δ)) − δxT

i w −

δxT

i A∆αprev

[k] −

δ2(cid:107)xi(cid:107)2
2

−(cid:96)∗

i (−(αi + (∆αprev

[k] )i + δ)) − δxT

i

δ2(cid:107)xi(cid:107)2
2

(49)

δ∈R

= arg max
δ∈R

K
λn

(cid:16)

w +

(cid:124)

K
λn

A∆αprev
[k]
(cid:123)(cid:122)
(cid:125)
=:ulocal

(cid:17)

−

K
2λn
K
2λn

From this formulation, it is apparent that single coordinate local solvers should maintain their locally updated version of
the current primal parameters, which we here denote as

(47)

(48)

(50)

ulocal = w +

K
λn

A∆αprev
[k] .

In the practical variant of DisDCA, the summarized local primal updates are ∆ulocal = 1
λnk
nk = n/K for K being the number of machines, this means the local ulocal update of DisDCA-p is

A∆α[k]. For the balanced case

∆α(cid:63)

i := arg max
∆αi∈R

−(cid:96)∗

i (−(αi + ∆αi)) − ∆αixT

i ulocal −

(∆αi)2(cid:107)xi(cid:107)2
2 .

(51)

K
2λn

It is not hard to show that during one outer round, the evolution of the local dual variables ∆α[k] is the same in both
methods, such that they will also have the same trajectory of ulocal. This requires some care if the same coordinate is
sampled more than once in a round, which can happen in LOCALSDCA within COCOA+ and also in DisDCA-p.

Discussion.

In the view of the above lemma, we will summarize the connection of the two methods as follows:

• COCOA/+ is Not an Algorithm. In contrast, it is a framework which allows to use any local solver to perform
approximate steps on the local subproblem. This additional level of abstraction (from the deﬁnition of such local
subproblems in (9)) is the ﬁrst to allow reusability of any fast/tuned and problem speciﬁc single machine solvers,
while decoupling this from the distributed algorithmic scheme, as presented in Algorithm 1.

Concerning the choice of local solver to be used within COCOA/+, SDCA is not the fastest known single machine
solver for most applications. Much recent research has shown improvements on SDCA (Shalev-Shwartz & Zhang,
2013c), such as accelerated variants (Shalev-Shwartz & Zhang, 2013b) and other approaches including variance re-
duction, methods incorporating second-order information, and importance sampling. In this light, we encourage the
user of the COCOA or COCOA+ framework to plug in the best and most recent solver available for their particular
local problem (within Algorithm 1), which is not necessarily SDCA. This choice should be made explicit especially
when comparing algorithms. Our presented convergence theory from Section 4 will still cover these choices, since it
only depends on the relative accuracy Θ of the chosen local solver.

Adding vs. Averaging in Distributed Primal-Dual Optimization

• COCOA+ is Theoretically Safe, while still Adaptive to the Data. The general deﬁnition of the local subproblems,
and therefore the treatment of the varying separable bound on the objective – quantiﬁed by σ(cid:48) – allows our framework
to adapt to the difﬁculty of the data partition and still give convergence results. The data-dependent measure σ(cid:48) is
fully decoupled from what the user of the framework prefers to employ as a local solver (see also the comment below
that COCOA is not a coordinate solver).
The safe upper bound σ(cid:48) = K is worst-case pessimistic, for the convergence theory to still hold in all cases, when the
updates are added. Using additional knowledge from the input data, better bounds and therefore better step-sizes can
be achieved in COCOA+. An example when σ(cid:48) can be safely chosen much smaller is when the data-matrix satisﬁes
strong row/column sparsity, see e.g. Lemma 1 in (Richt´arik & Tak´aˇc, 2013).

• Obtaining DisDCA-p as a Special Case. As shown in Lemma 18 above, we have that if in COCOA+, if SDCA is
used as the local solver and the pessimistic upper bound of σ(cid:48) = K is used and, moreover, the dataset is partitioned
K , then the COCOA+ framework reduces exactly to the DisDCA-p algorithm by (Yang, 2013).
equally, i.e. ∀k : nk = n
The correspondence breaks down if the subproblem parameter is chosen to a practically good value σ(cid:48) (cid:54)= K. Also, as
noted above, SDCA is often not the best local solver currently available. In our above experiments, SDCA was used
just for demonstration purposes and ease of comparison. Furthermore, the data partition might often be unbalanced
in practical applications.
While both DisDCA-p and COCOA are special cases of COCOA+, we note that DisDCA-p can not be recovered as a
special case of the original COCOA framework (Jaggi et al., 2014).

• COCOA/+ are Not Coordinate Methods. Despite the original name being motivated from this special case, COCOA
and COCOA+ are not coordinate methods. In fact, COCOA+ as presented here for the adding case (γ = 1) is much
more closely related to a batch method applied to the dual, using a block-separable proximal term, as following
from our new subproblem formulation (9), depending on σ(cid:48). See also the remark in Section 6. The framework here
(Algorithm 1) gives more generality, as the used local solver is not restricted to be a coordinate-wise one. In fact the
framework allows to translate recent and future improvements of single machine solvers directly to the distributed
setting, by employing them within Algorithm 1. DisDCA-p works very well for several applications, but is restricted
to using local coordinate ascent (SDCA) steps.

• Theoretical Convergence Results. While DisDCA-p (Yang, 2013) was proposed without theoretical justiﬁcation
(hence the nomenclature), the main contribution in the paper here – apart from the arbitrary local solvers – is the
convergence analysis for the framework. The theory proposed in (Yang et al., 2013) is given only for the setting of
orthogonal partitions, i.e., when σ(cid:48) = 1 and the problems become trivial to distribute given the orthogonality of data
between the workers.

The theoretical analysis here gives convergence rates applying for Algorithm 1 when using arbitrary local solvers,
and inherits the performance of the local solver. As a special case, we obtain the ﬁrst theoretical justiﬁcation and
convergence rates for original COCOA in the case of general convex objective, as well as for the special case of
DisDCA-p for both general convex and smooth convex objectives.

