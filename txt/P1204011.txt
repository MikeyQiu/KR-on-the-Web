9
1
0
2
 
n
u
J
 
4
 
 
]
L
C
.
s
c
[
 
 
2
v
4
4
2
7
0
.
8
0
8
1
:
v
i
X
r
a

Improving Matching Models with
Hierarchical Contextualized
Representations for Multi-turn Response
Selection

Chongyang Tao, Wei Wu, Can Xu, Yansong Feng, Rui
Yan, and Dongyan Zhao

Abstract—In this paper, we study context-response matching with pre-
trained contextualized representations for multi-turn response selection
in retrieval-based chatbots. Existing models, such as Cove and ELMo,
are trained with limited context (often a single sentence or paragraph),
and may not work well on multi-turn conversations, due to the hierarchical
nature, informal
language, and domain-speciﬁc words. To address
the challenges, we propose pre-training hierarchical contextualized
representations, including contextual word-level and sentence-level
representations, by learning a dialogue generation model from large-
scale conversations with a hierarchical encoder-decoder architecture.
Then the two levels of representations are blended into the input and
output layer of a matching model respectively. Experimental results on two
benchmark conversation datasets indicate that the proposed hierarchical
contextualized representations can bring signiﬁcantly and consistently
improvement to existing matching models for response selection.

Index Terms—Contextualized word vectors, deep neural network, match-
ing, multi-turn response selection, retrieval-based chatbot.

(cid:70)

1 INTRODUCTION

Human-machine conversation is one of the fundamental prob-
lems in natural language processing (NLP). While previous
research focuses on building task-oriented dialog systems [1]
that can fulﬁll speciﬁc tasks in vertical domains for people via
conversations; more recent attention is drawn to developing non-
task-oriented chatbots which can naturally and meaningfully
converse with humans on open domain topics [2]. Existing
approaches to building a chatbot include generation-based
methods [2], [3], [4], [5], [6] which synthesize a response
with natural language generation technologies, and retrieval-
based methods [7], [8] which select a response from a pool of
candidates. In this work, we study multi-turn response selection
for retrieval-based chatbots, because retrieval-based methods
can return ﬂuent and informative responses, and are the core
of many real products such as the social-bot XiaoIce from
Microsoft [9] and the E-commerce assistant AliMe Assist from
Alibaba Group [10].

A key step to multi-turn response selection is measuring the
matching degree between a conversational context consisting
of a sequence of utterances and a response candidate with
a matching model. Existing models, such as dual LSTM [7],
Multi-view [11] and sequential matching network (SMN) [8] are
deﬁned in neural architectures. Although these models vary in
structures, they are commonly built upon word embeddings
which are pre-trained on large-scale unlabeled text with al-
gorithms such as Word2Vec [12] and GloVe [13]. Indeed, the
pre-trained word embeddings are crucial to a matching model,

• Chongyang Tao, Yansong Feng, Rui Yan, and Dongyan Zhao are with
the Institute of Computer Science & Technology, Peking University,
Beijing, China, 100871. (e-mail: chongyangtao@pku.edu.cn; fengyan-
song@pku.edu.cn; ruiyan@pku.edu.cn; zhaody@pku.edu.cn)

• Wei Wu and Can Xu are with Microsoft Corporation, Beijing, China,

100871. (e-mail: wuwei@microsoft.com; caxu@microsoft.com)

1

as they carry useful syntactic and semantic information learned
from the unlabeled text to the matching task. On the other hand,
words appear in speciﬁc contexts (e.g., sentences), and the same
word could have different meanings in different contexts. The
widely used embeddings, however, represent words in a context-
independent way. As a result, contextual information of words
in the unlabeled text is lost in the matching task.

In this work, we study how to leverage pre-trained con-
textualized representations to improve matching models for
multi-turn response selection in retrieval-based chatbots. A
baseline method is integrating the state-of-the-art contextualized
word vectors such as CoVe [14] and ELMo [15] into matching
models. Although both CoVe and ELMo have proven effective
on various NLP tasks, they are never applied to conversation
tasks. Therefore, it is not clear if CoVe and ELMo are as effective
on the task of response selection as they are on other tasks
such as machine reading comprehension [16] and sentiment
analysis [17], etc. On the other hand, directly applying CoVe
or ELMo to conversation modeling might be problematic, as
there is a discrepancy between conversation data and the data
used to train the two models. First, conversation data is often
in a hierarchical structure, and thus there are both sentence-
level contexts and session-level contexts for a word. A sentence-
level context refers to an utterance that contains the word, and
represents a kind of local context, and a session-level context
means the entire conversation session (i.e., all utterances of
the conversation history in question) that contains the word,
and provides a global context for the word. CoVe and ELMo,
however, only encode local contextual information of words
when applied to a conversation task. Second, word use in
conversation data is different from that in the WMT data which
are used to train CoVe and ELMo. Words in conversations could
be informal (e.g., “srry”, which means “sorry” and is among the
top 10% high-frequency words in Ubuntu dialogues) or domain-
speciﬁc (e.g., “fstab”, which means a system conﬁguration ﬁle
on Unix and Unix-like computer systems, and is among the top
1% high-frequency words in Ubuntu dialogues), and thus rarely
appear in the WMT data. As a result, these words cannot be
accurately represented by the two models. The discrepancy on
data raises new challenges to leveraging contextualized word
vectors in matching for response selection.

To address the challenges, we propose pre-training con-
textualized representations with large-scale human-human
conversations. Speciﬁcally, we employ a hierarchical encoder-
decoder architecture, and learn a dialogue generation model
with the conversations. Local contextual information and global
contextual information for a word are naturally encoded by the
utterance-level recurrent neural network (RNN) and the context-
level RNN of the encoder of the generation model respectively.
Thus, we take the hidden states of the utterance-level RNN
and the context-level RNN as contextualized word-level and
sentence-level representations respectively, and name them
ECMo (embeddings from a conversation model) representations.
In matching, we integrate the word-level representation into the
input layer of a matching model where words are initialized,
and exploit the sentence-level representation in the output layer
of the matching model where a matching score is calculated. By
this means, we transfer the knowledge in large-scale unlabeled
human-human conversations to the learning of the matching
model.

We integrate CoVe, ELMo, and ECMo into Multi-view and
SMN, and conduct experiments on two benchmark datasets:
the Ubuntu Dialogue Corpus [7] and the Douban Conversation
Corpus [8]. Experimental results indicate that the performance of
matching models improves when they are combined with differ-

2

ent contextualized representations (CoVe, ELMo (ﬁne-tune) and
ECMO). On the other hand, we observe that ECMo brings more
signiﬁcant and consistent improvement to matching models
than CoVe and ELMo. On the Ubuntu data, the improvements to
Multi-view and SMN on R10@1 are 4.3% and 2.4% respectively;
and on the Douban data, the improvements to Multi-view and
SMN on MAP are 2.0% and 2.3% respectively.

Our contributions are three-fold:

1) We test CoVe and ELMo on benchmark datasets of response

selection;

2) We propose a new approach to pre-training hierarchical
contextualized representations that can well adapt to the
task of response selection;

3) We verify the effectiveness of the proposed model on the

benchmark datasets of response selection.

2 RELATED WORK

Existing methods on building a chatbot are either generation-
based or retrieval-based. The generation-based methods synthe-
size a response with the natural language generation technolo-
gies [2], [4]. Different from generation-based methods, retrieval-
based methods focus on designing a matching model of a human
input and a response candidate for response selection. Early
work along this line studies single-turn response selection where
the human input is set as a single message [18], [19]. Recently
more attention is paid to context-response matching for multi-
turn response selection. Representative methods include the dual
LSTM model [7], the deep learning to respond architecture [20],
the multi-view matching model [11], the sequential matching
network [8], and the deep attention matching network [21].
In this work, we study the problem of multi-turn response
selection for retrieval-based chatbots. Rather than designing
a sophisticated matching structure, we are interested in how
to leverage pre-trained contextualized word embeddings to
generally improve the performance of the existing matching
models. The contextualized embeddings are obtained from a
dialogue generation model learned with large-scale human-
human conversations apart from the training data of the
matching models.

Pre-trained word vectors have become a standard component
of most state-of-the-art models in NLP tasks. A common practice
is to learn a single context-independent word representation
from large-scale unlabeled data [12], [13], [22], and initialize
the task-speciﬁc models with the representation. Recently,
researchers begin to study pre-training context-dependent word
representations for downstream tasks [23], [24], [25]. For exam-
ple, McCann et al. [14] train an encoder-decoder model on large-
scale machine translation datasets, and treat the hidden states of
the encoder as contextualized representations of words. Peters
et al. [15] learn a multi-layer LSTM based language model on
large-scale monolingual data, and use hidden states of different
layers of the LSTM as contextualized word vectors. Very recently,
Devlin et al. [26] propose a bigger and more powerful pre-
trained model based on stacked self-attention. In this work,
we study how to pre-train contextualized word representations
for the task of multi-turn response selection which is never
explored before. In addition to the application of CoVe and
ELMo, we propose pre-training hierarchical contextualized word
representations by learning a dialogue generation model from
large-scale conversations with a hierarchical encoder-decoder.
Different from the existing models, the dialogue generation
model allows us to form two levels of contextualized word
representations that encode both local and global contextual
information for a word in conversations.

Fig. 1. The architecture of HED.

3 BACKGROUND: LEARNING A MATCHING MODEL FOR
RESPONSE SELECTION
Given a dataset D = {(yi, si, ri)}N
i=1 where si = {ui,1, . . . , ui,ni }
represents a conversational context with {ui,k}ni
k=1 as utterances;
ri is a response candidate; and yi ∈ {0, 1} denotes a label with
yi = 1 indicating ri a proper response for si and otherwise
yi = 0, the goal of the task of response selection is to learn a
matching model g(·, ·) from D. For any context-response pair
(s, r), g(s, r) gives a score that reﬂects the matching degree
between s and r, and thus allows one to rank a set of response
candidates according to the scores for response selection.

For any ui,k in si and ri in D, suppose that ui,k = (wi,k,1, . . . ,
wi,k,ni,k ) and ri = (vi,1, . . . , vi,ni ), where wi,k,j and vi,j denote
the j-th words of ui,k and ri respectively, a common practice
of learning of g(·, ·) is that wi,k,j and vi,j are ﬁrst initialized
with some pre-trained word vectors and then fed to an neural
architecture. With the word vectors either ﬁxed or optimized
together with other parameters of the neural architecture, g(·, ·)
is learnt by maximizing the following objective:

N
(cid:88)

i=1

(cid:2)yi log(g(ci, ri)) + (1 − yi) log(1 − g(ci, ri))(cid:3).

(1)

Existing work pre-trains word vectors with either Word2Vec
(e.g., SMN [8]) or GloVe (e.g., dual LSTM [7] and multi-view
[11]) which loses contextual information in the representations of
words. Therefore, inspired by the recent success of CoVe [14] and
ELMo [15] on downstream NLP tasks, we consider incorporating
contextualized word representations into the learning of g(·, ·).
On the other hand, contexts of a word in conversations are
usually in casual language, and sometimes with domain-speciﬁc
knowledge (e.g., the Ubuntu Dialogue Corpus [7]). The con-
texts are naturally in a hierarchical structure where utterances
and conversation sessions (i.e., sequences of utterances) that
contain the word provide contextual information from a local
and a global perspective respectively. The characteristics of
conversation data motivate us to learn new contextualized
representations of words that can well adapt to the task of
response selection.

4 ECMO: EMBEDDING FROM A CONVERSATION MODEL

Heading for contextualized word representations that can well
capture semantics and syntax of conversations, we propose
learning a dialogue generation model from large-scale human-
human conversations. We ﬁrst present the architecture of the
generation model, and then elaborate how to extract two levels
of contextualized representations from the model and exploit
them in matching.

4.1 Hierarchical Encoder-Decoder Model

In order to represent contextual information in both utterances
and the entire session of a conversation, we learn a hierarchical
encoder-decoder (HED) model for multi-turn dialogue genera-
tion. Figure 1 gives the architecture of HED. HED consists of a
two-level encoder and a decoder. The ﬁrst layer of the encoder
is an utterance-level encoder where HED reads utterances in
conversation history one by one, and represents the word
sequence of each utterance as a sequence of hidden vectors by
a bidirectional RNN with Gated Recurrent Units (biGRUs) [27].
The hidden vectors of each utterance are then processed by a
max pooling operation and transformed to an utterance vector.
The second layer of the encoder is a context-level encoder which
employs another GRU to transform the sequence of utterance
vectors to hidden vectors. The ﬁnal hidden vector of the context-
level encoder is fed to the decoder to generate the next turn.

Note that HED is in a similar structure with the model
proposed by [4]. The difference is that in HED, we represent each
utterance with a biGRU instead of a GRU in order to encode both
forward and backward contextual information in an utterance,
and employ a max pooling operation to generate an utterance
vector instead of only using the last hidden state, as suggested
by [28]. We select HED among many dialogue generation models
because the two-layer encoder naturally encodes local and global
contextual information in conversations, and the model balances
efﬁcacy and efﬁciency (to facilitate learning from large-scale
conversations) compared to more complicated architectures such
as VHRED [6].

Utterance-Level Encoder:: given a sequence of utter-
ances s = {u1, . . . , un}, we employ a biGRU to encode
each ui as hidden vectors {hi,k}Ti
k=1. Formally, suppose that
ui = {wi,k}Ti

k=1, then ∀k ∈ {1, . . . , Ti}, hi,k is given by

hi,k = [

−→
h i,k;

←−
h i,k],

(2)

−→
h i,k is the k-th hidden state of a forward GRU [27] and

where
←−
h i,k is the k-th hidden state of a backward GRU.

The output of the utterance-level encoder is a sequence of
i is the representation of ui

i=1 where vu

utterance vectors {vu
i }n
with the j-th element as

vu

i (j) = max(hi,1(j), . . . , hi,Ti (j)),

(3)

where hi,1(j) and hi,Ti (j) are the j-th elements of hi,1 and hi,Ti
respectively.

Context-Level encoder:: the context-level encoder takes
the output of the utterance-level encoder as input, and represents
the entire conversation session s as a sequence of hidden vectors
{hs

i=1, where hs

i }n

i is calculated by
i = GRUs(hs
hs

i−1, vu

i ),

Decoder:: the decoder of HED is an RNN language
model [29] which predicts the next utterance un+1 word by
n. Suppose that un+1 = {wn+1,k}Tn+1
word conditioned on hs
k=1 ,
then the generation probability of p(un+1|u1, . . . , un) is deﬁned
as

p(wn+1,1|hs

n) ·

p(wn+1,t|hs

n, {wn+1,k}t−1

k=1),

(5)

Tn+1
(cid:89)

where p(wn+1,t|hs

k=1) is given by

t=2
n, {wn+1,k}t−1
Iwn+1,t · softmax(hd

t , en+1,t−1).

t is the hidden state of the decoder at step t which is deﬁned

hd
as

(4)

(6)

(7)

3

where en+1,t−1 is the embedding of wn+1,t−1, Iwn+1,t is a one-
hot vector of wn+1,t, and softmax(·, ·) is a V -dimensional vector
(V is the vocabulary size) of which each element is the generation
probability of a word. We initialize the recurrent state of the
GRUd with a nonlinear transformation of hs
n.

Learning objective:: we estimate the parameters of HED
by maximizing the likelihood of a dataset D(cid:48) = {si}N (cid:48)
i=1 where
si is a conversation session. The source of D(cid:48) could be different
from that of D, and N (cid:48) could be much larger than N , as will
be seen in our experiments later. Thus, we can transfer the
knowledge in large scale unlabeled conversations to the learning
of a matching model. Suppose that si = (ui,1, . . . , ui,ni ), then the
learning of HED can be formulated as maximizing the following
objective:

N (cid:48)
(cid:88)

ni(cid:88)

i=1

j=2

log (cid:0)p(ui,j|ui,1, . . . , ui,j−1)(cid:1).

(8)

4.2 ECMo

ECMo are representations deﬁned by the hidden states of the
two-level encoder of HED. Given a word wi,k in an utterance
ui in a conversation session s, the word-level contextualized
representation of wi,k is deﬁned by

ECMolocal(wi,k) = hi,k,

where hi,k is given by Equation (2). The sentence-level contextu-
alized representation of wi,k is deﬁned by

(9)

(10)

ECMoglobal(wi,k) = hs
i ,

where hs

i is given by Equation (4).

4.3 Using ECMo for Matching Models

Given a pre-trained HED and a matching model, we incorporate
ECMo representations into both the input layer and the output
layer of the matching model by running the encoder of the HED
on a conversational context (i.e., a conversation session) and a
response candidate (treated as a special conversation session
with only one utterance) simultaneously. Existing matching
models share a common architecture at the input layers where
words are initialized with pre-trained vectors, and act in a
common manner at the output layers where a context-response
pair is transformed to a score, which allows us to add ECMo in
a uniﬁed way.

Formally, suppose that the input of a matching model g(·, ·)
is a conversational context s = {ui}n
i=1 with ui the i-th utterance
and a response candidate r, let ui = {wi,k}ni
k=1,
where word wi,k and word vk are initialized by pre-trained
context-independent representations eu
k respectively,
we then form a new representation for wi,k as

k=1 and r = {vk}nr

i,k and er

i,k = [eu
˜eu

i,k; ECMolocal(wi,k)],

Similarly, we form a new representation for vk as

˜er
k = [er

k; ECMolocal(vk)].

(11)

(12)

We then initialize the embedding of wi,k and vk at the input
layer of g(·, ·) with ˜eu
k respectively. At the output
layer of g(·, ·), in addition to g(s, r), we deﬁne a new matching
score based on the sentence-level contextualized representations,
which is deﬁned as

i,k and ˜er

t = GRUd(hd
hd

t−1, en+1,t−1),

g(cid:48)(s, r) = σ(˜es · W · ˜er + b),

(13)

where ˜es = ECMoglobal(wn,k), ˜er = ECMoglobal(vnr ), and W
and b are parameters, . We then re-deﬁne the matching model
g(s, r) as

˜g(s, r) = g(s, r) + g(cid:48)(s, r),

(14)

In learning of the matching model, one can either freeze the
parameters of the pre-trained HED or continue to optimize those
parameters with the cross entropy loss given by Equation (1).
We empirically compare the two strategies in our experiments.

5 EXPERIMENTS

We test CoVe, ELMo, and ECMo on two benchmark datasets for
multi-turn response selection.

5.1 Experiment Setup

Ubuntu Dialogue Corpus:: the Ubuntu Dialogue Cor-
pus [7] is an English dataset collected from chat logs of the
Ubuntu Forum. We use the version provided by [30] (i.e., Ubuntu
Dialogue Corpus v1). There are 1 million context-response pairs
for training, 0.5 million pairs for validation, and 0.5 million
pairs for the test. In the data, responses from humans are treated
as positive responses, and negative responses are randomly
sampled. In the training set, the ratio of positive responses and
negative responses is 1:1. In the validation set and the test set,
the ratios are 1:9. Following [7], we employ Rn@ks as evaluation
metrics.

Douban Conversation Corpus:: the Douban Conversation
Corpus [8] is a multi-turn Chinese conversation dataset crawled
from Douban group1. The dataset consists of 1 million context-
response pairs for training, 50 thousand pairs for validation, and
6, 670 pairs for the test. In the training set and the validation
set, the last turn of each conversation is regarded as a positive
response and the negative responses are randomly sampled.
The ratio of positive responses and negative responses is
1:1 in training and validation. In the test set, each context
has 10 response candidates retrieved from an index whose
appropriateness regarding to the context is judged by human
labelers. Following [8], we also employ Rn@ks, mean average
precision (MAP), mean reciprocal rank (MRR) and precision at
position 1 (P@1) as evaluation metrics.

5.2 HED Pre-training

The HED models for both datasets are trained using Adam [31]
with a mini-batch 40. The learning rate is set as 1e−3. The size of
the hidden vectors of the utterance-level RNN, the context-level
RNN, and the decoder RNN are 300. Since the utterance-level
RNN is bidirectional, the dimension of ECMolocal vectors is 600,
which is the same as CoVe. We set the maximum length of a
session as 10 and the maximum length of an utterance as 50.

For the Ubuntu data, we crawl 10 million multi-turn conver-
sations from Twitter, covering 2-month period from June 2016 to
July 2016. As pre-processing, we remove URLs, emotions, and
usernames, and transform each word to lower case. On average,
each conversation has 9.2 turns. The HED model is ﬁrst trained
on the Twitter data, and then is ﬁne-tuned on the training
set of the Ubuntu data. By this means, we encode both the
semantics in the casual conversations of Twitter and the domain
knowledge in the Ubuntu dialogues. In training, we initialize
word embeddings with 300-dimensional GloVe vectors [13]. The
vocabulary is constructed by merging the vocabulary of the
Ubuntu data (the size is 60k) and the vocabulary of the Twitter

1. https://www.douban.com/group

4

data (the size is 60k). The size of the ﬁnal vocabulary is 99, 394.
Words that are out of the vocabulary are randomly initialized
according to a zero-mean normal distribution. After the ﬁrst
stage of training, the HED model achieves a perplexity of 70.3
on a small validation set of the Twitter data (20k). The perplexity
of the ﬁnal HED model on the validation set of the Ubuntu data
is 59.4.

For the Douban data, we train the HED model on a public
dataset2. The dataset contains 5 million conversations crawled
from Sina Weibo3. On average, each conversation has 4.1
turns. Since conversations from Weibo cover a wide range of
topics which are similar to those in the Douban Conversation
Corpus, we only train the HED model on the Weibo data. Word
embeddings are initialized by running Word2Vec [12] on the
Weibo data. The ﬁnal model achieves a perplexity of 123.7 on
the validation set of the Douban data.

5.3 Matching Models

The following matching models are selected to test the effect of
pre-trained contextualized word representations:

Multi-view: the model proposed by [11] in which the
response vector interacts with a highly abstract context vector
obtained from both word view and utterance view. We select the
model as it is representative architecture for context-response
matching and is better than dual LSTM [7].

Sequential matching network (SMN): the model proposed
by [8] in which each utterance in a context interacts with a
response word-by-word at the beginning, and the interaction is
transformed to a matching vector by 2D CNNs. The matching
vectors are ﬁnally aggregated by an RNN as a matching score.
We select the model as it is a representative in the framework
of representation-matching-aggregation for context-response
matching.

For CoVe and ELMo, we use the models published at
https://github.com/salesforce/cove and https://github.com/
allenai/allennlp/blob/master/tutorials/how to/elmo.md re-
spectively, and follow the way in the existing papers to integrate
the contextualized vectors into the wording embedding layer
of the matching models. The dimensions of the vectors from
CoVe and ELMo are 600 and 1024 respectively. The combination
weights of ELMo are optimized together with the matching
models, as suggested in [15]. All parameters of the pre-trained
models are ﬁxed during the learning of the matching models.
Since HED is ﬁne-tuned on the Ubuntu data, to make a fair
comparison, we also ﬁne-tune ELMo on the Ubuntu data4.

To integrate the pre-trained contextualized vectors, we
implement Multi-view and SMN in the same setting as in [11]
and [8] respectively with PyTorch 0.3.1. Note that as indicated
in [8], [11], the context-independent word vectors (i.e., GloVe in
Multi-view and Word2Vec in SMN) are learned or ﬁne-tuned
with the training sets of the Ubuntu and the Douban data, and
the dimension of the vectors is 200.

5.4 Evaluation Results

Table 1 reports the evaluation results on the two datasets. We can
observe that the performance of Multi-view and SMN improves
on almost all metrics after they are combined with CoVe,
ELMo (ﬁne-tune), and ECMo, indicating that contextualized

2. Avalible

at
trainingdata05.zip

http://tcci.ccf.org.cn/conference/2018/dldoc/

3. www.weibo.com
4. We do not ﬁne-tune CoVe because CoVe needs paired data and thus

is difﬁcult to ﬁne-tune with the conversation data.

5

TABLE 1
Evaluation results on the two datasets. Numbers in bold mean that improvement to the original models brought by ECMo is statistically signiﬁcant
(t-test, p-value < 0.01 ). Numbers marked with ∗ mean that improvement to ELMo (ﬁne-tune) and CoVe is statistically signiﬁcant (t-test, p-value
< 0.01 ). We do not include the results of CoVe and ELMo enhanced models on the Douban data because the two models are not available for
Chinese data.

Multi-view
Multi-view + CoVe
Multi-view + ELMo
Multi-view + ELMo (ﬁne-tune)
Multi-view + ECMo
SMN
SMN + CoVe
SMN + ELMo
SMN + ELMo (ﬁne-tune)
SMN + ECMo

Ubuntu Corpus

R2@1 R10@1 R10@2 R10@5 MAP MRR
0.547
0.916
-
0.919
-
0.909
-
0.924
0.572
0.930
0.571
0.925
-
0.930
-
0.926
-
0.930
0.593
0.934

0.690
0.699
0.668
0.705
0.733∗
0.732
0.738
0.739
0.745
0.756∗

0.831
0.837
0.813
0.847
0.858∗
0.852
0.856
0.855
0.859
0.867∗

0.502
-
-
-
0.522
0.526
-
-
-
0.549

0.959
0.960
0.951
0.964
0.967
0.961
0.963
0.961
0.963
0.966

Douban Corpus
P@1
0.352
-
-
-
0.378
0.393
-
-
-
0.409

R10@1 R10@2 R10@5
0.728
0.353
0.205
-
-
-
-
-
-
-
-
-
0.761
0.391
0.224
0.729
0.387
0.236
-
-
-
-
-
-
-
-
-
0.774
0.416
0.247

TABLE 2
Evaluation results of model ablation.

Multi-view + ECMoL
Multi-view + ECMoG
Multi-view + ECMoL (twitter)
Multi-view + ECMoG (twitter)
Multi-view + ECMo (twitter)
Multi-view + ECMo (Ubuntu-only)
Multi-view + ECMo
SMN + ECMoL
SMN + ECMoG
SMN + ECMoL (twitter)
SMN + ECMoG (twitter)
SMN + ECMo (twitter)
SMN + ECMo (Ubuntu-only)
SMN + ECMo

R2@1
0.921
0.931
0.916
0.920
0.918
0.915
0.930
0.932
0.932
0.929
0.917
0.918
0.917
0.933

Ubuntu Corpus
R10@2
R10@1
0.837
0.698
0.858
0.729
0.829
0.686
0.839
0.704
0.838
0.699
0.826
0.686
0.858
0.733
0.862
0.747
0.864
0.750
0.859
0.743
0.841
0.718
0.844
0.719
0.839
0.717
0.866
0.755

R10@5 MAP MRR
0.554
0.510
0.961
0.562
0.515
0.967
-
-
0.959
-
-
0.960
-
-
0.960
-
-
0.956
0.572
0.522
0.967
0.584
0.540
0.966
0.582
0.537
0.964
-
-
0.962
-
-
0.951
-
-
0.953
-
-
0.948
0.593
0.549
0.975

Douban Corpus
P@1
0.358
0.373
-
-
-
-
0.378
0.399
0.396
-
-
-
-
0.409

R10@1 R10@2 R10@5
0.731
0.380
0.210
0.755
0.384
0.216
-
-
-
-
-
-
-
-
-
-
-
-
0.761
0.391
0.224
0.759
0.401
0.240
0.753
0.406
0.233
-
-
-
-
-
-
-
-
-
-
-
-
0.774
0.416
0.247

vectors are useful to the multi-turn response selection task.
Notably, ECMo brings more signiﬁcant and more consistent
improvement to matching models on both datasets, which
veriﬁes the effectiveness of the proposed method. With ECMo,
a simple Multi-view even performs better than SMN on the
Ubuntu data, although SMN is in a more complicated structure,
and thus is capable of capturing more semantic information in
conversational contexts.

Without ﬁne-tuning, the performance of Multi-view drops
with ELMo while the performance of SMN slightly increases.
The reason might be that Multi-view is in a relatively simple
structure and ELMo (w/o ﬁnetune) introduces high-dimensional
discrepant word representations and also brings too many
parameters.

5.5 Analysis

Model ablation: we investigate how different conﬁgurations of
ECMo affect the performance of matching through an ablation
study. We ﬁrst check how the word-level and the sentence-
level contextualized representations individually contribute
to the improvement on matching performance by leaving
only one type of representations, and denote the models as
model+ECMoL and model+ ECMoG respectively. Then, we
examine if ﬁne-tuning the HED model on the Ubuntu training
data after it is trained on the Twitter data matters a lot by
integrating the ECMo representations from the HED model
only trained on the Twitter data into Multi-view and SMN. The
models are denoted as model+ECMoL (twitter), model+ECMoG

(twitter), and model + ECMo (twitter) respectively. Finally, we
are also curious about if HED is already good enough when it
is only pre-trained with the Ubuntu training data. Models that
are combined with such ECMo representations are denoted as
model+ECMo (Ubuntu-only).

Table 2 reports the results. First, we ﬁnd that both ECMoL
and ECMoG are useful to matching. ECMoG generally brings
more improvement to matching than ECMoL for Multi-view
while the results reverse for SMN. This phenomenon is mainly
due to the different interaction operation of the two models.
In Multi-view, the response and context are interacted with
corresponding global utterance representations, while in SMN,
the interaction is performed word-by-word. Second, ﬁne-tuning
is necessary as the performance of models drops without it. Since
the Ubuntu dialogues have some domain-speciﬁc knowledge,
a HED model only trained on Twitter cannot fully capture
the contextual information in the Ubuntu data, resulting in
inaccurate contextualized word representations. We can see
that ECMoG and ECMoL shows different robustness to the
noise for Multi-view and SMN due to the different interaction
operation. Finally, although ﬁne-tuning is important, one cannot
only train a HED model on the domain-speciﬁc data, because
the size of the data is small and a lot of contextual information in
common conversation language will be lost. That explains why
model+ECMo (Ubuntu-only) is inferior to all other variants.

Does further optimization under the matching loss help?
So far, we ﬁx the pre-trained ECMo representations in matching.
Then it is interesting to know if we can obtain more improve-

TABLE 3
The results of the models with ﬁxed or continue-trained ECMo representations.

Multi-view + ECMo
Multi-view + ECMo (continue-train)
SMN + ECMo
SMN + ECMo (continue-train)

R2@1
0.930
0.929
0.933
0.935

Ubuntu Corpus
R10@2
R10@1
0.858
0.733
0.855
0.723
0.866
0.755
0.866
0.749

R10@5 MAP MRR
0.572
0.522
0.967
0.558
0.516
0.966
0.593
0.549
0.975
0.587
0.544
0.968

Douban Corpus
P@1
0.378
0.367
0.409
0.406

R10@1 R10@2 R10@5
0.761
0.391
0.224
0.739
0.372
0.222
0.774
0.416
0.247
0.765
0.414
0.246

6

ment when we continue to train the parameters of HED under
the cross entropy objective (i.e., Objective (1)) of matching.
Table 3 compares the two settings where models whose ECMo
is optimized under the matching objective are denoted as
model+ECMo (continue-train). We ﬁnd that “continue-train”
makes the performance of Multi-view and SMN drop on both
datasets. This phenomenon may stem from the fact that the
matching model is prone to over-ﬁtting with the additional
parameters coming from HED. On the other hand, ﬁne-tuning
with cross-entropy loss may damnify the original contextualized
representations.

6 CONCLUSION AND FUTURE WORK
We propose pre-training a dialogue generation model from
large-scale conversations with a hierarchical encoder-decoder
architecture, and extracting both local and global contextualized
word representations from the model to enhance matching mod-
els for multi-turn response selection. Experimental results on
two benchmark datasets indicate that the proposed method can
bring signiﬁcant and consistent improvement to the performance
of the existing matching models.

REFERENCES

[1]

S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatzmann, B. Thom-
son, and K. Yu, “The hidden information state model: A practical
framework for pomdp-based spoken dialogue management,” Com-
puter Speech & Language, vol. 24, no. 2, pp. 150–174, 2010.

[2] L. Shang, Z. Lu, and H. Li, “Neural responding machine for short-

text conversation,” in ACL, 2015, pp. 1577–1586.

[3] A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J.-Y.
Nie, J. Gao, and B. Dolan, “A neural network approach to context-
sensitive generation of conversational responses,” arXiv preprint
arXiv:1506.06714, 2015.
I. V. Serban, A. Sordoni, Y. Bengio, A. C. Courville, and J. Pineau,
“End-to-end dialogue systems using generative hierarchical neural
network models,” in AAAI, 2016, pp. 3776–3784.

[4]

[5] C. Xing, W. Wu, Y. Wu, J. Liu, Y. Huang, M. Zhou, and W.-Y.
Ma, “Topic aware neural response generation.” in AAAI, 2017, pp.
3351–3357.
I. V. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. C.
Courville, and Y. Bengio, “A hierarchical latent variable encoder-
decoder model for generating dialogues.” in AAAI, 2017, pp. 3295–
3301.

[6]

[7] R. Lowe, N. Pow, I. Serban, and J. Pineau, “The ubuntu dialogue
corpus: A large dataset for research in unstructured multi-turn
dialogue systems,” in Proceedings of the 16th Annual Meeting of the
Special Interest Group on Discourse and Dialogue, 2015, pp. 285–294.
[8] Y. Wu, W. Wu, C. Xing, M. Zhou, and Z. Li, “Sequential matching
network: A new architecture for multi-turn response selection in
retrieval-based chatbots,” in ACL, 2017, pp. 496–505.

[9] H.-Y. Shum, X. He, and D. Li, “From eliza to xiaoice: Challenges and
opportunities with social chatbots,” arXiv preprint arXiv:1801.01957,
2018.

[10] F.-L. Li, M. Qiu, H. Chen, X. Wang, X. Gao, J. Huang, J. Ren, Z. Zhao,
W. Zhao, L. Wang et al., “Alime assist: An intelligent assistant for
creating an innovative e-commerce experience,” in CIKM, 2017, pp.
2495–2498.

[11] X. Zhou, D. Dong, H. Wu, S. Zhao, R. Yan, D. Yu, X. Liu, and H. Tian,
“Multi-view response selection for human-computer conversation,”
in EMNLP, 2016, pp. 372–381.

[12] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their
compositionality,” in NIPS, 2013, pp. 3111–3119.

[13] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors

for word representation.” in EMNLP, vol. 14, 2014, pp. 1532–43.

[14] B. McCann, J. Bradbury, C. Xiong, and R. Socher, “Learned in
translation: Contextualized word vectors,” in NIPS, 2017, pp. 6297–
6308.

[15] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep contextualized word representations,”
in NAACL, 2018, pp. 2227–2237.

[16] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+
questions for machine comprehension of text,” in EMNLP, 2016, pp.
2383–2392.

[17] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,
and C. Potts, “Recursive deep models for semantic compositionality
over a sentiment treebank,” in EMNLP, 2013, pp. 1631–1642.
[18] H. Wang, Z. Lu, H. Li, and E. Chen, “A dataset for research on

short-text conversations.” in EMNLP, 2013, pp. 935–945.

[19] M. Wang, Z. Lu, H. Li, and Q. Liu, “Syntax-based deep matching

of short texts,” in IJCAI, 2015.

[20] R. Yan, Y. Song, and H. Wu, “Learning to respond with deep
neural networks for retrieval-based human-computer conversation
system,” in SIGIR, 2016, pp. 55–64.

[21] X. Zhou, L. Li, D. Dong, Y. Liu, Y. Chen, W. X. Zhao, D. Yu,
and H. Wu, “Multi-turn response selection for chatbots with deep
attention matching network,” in ACL, vol. 1, 2018, pp. 1118–1127.
[22] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word
vectors with subword information,” TACL, vol. 5, no. 1, pp. 135–146,
2017.

[23] O. Melamud, J. Goldberger, and I. Dagan, “context2vec: Learning
generic context embedding with bidirectional lstm,” in Proceedings
of The 20th SIGNLL Conference on Computational Natural Language
Learning, 2016, pp. 51–61.

[24] M. Peters, W. Ammar, C. Bhagavatula, and R. Power, “Semi-
supervised sequence tagging with bidirectional language models,”
in ACL, 2017, pp. 1756–1765.

[25] P. Ramachandran, P. Liu, and Q. Le, “Unsupervised pretraining for
sequence to sequence learning,” in EMNLP, 2017, pp. 383–391.
[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” in NAACL, 2018.

[27] K. Cho, B. van Merri¨enboer, D. Bahdanau, and Y. Bengio, “On
the properties of neural machine translation: Encoder–decoder
approaches,” Syntax, Semantics and Structure in Statistical Translation,
p. 103, 2014.

[28] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes,
“Supervised learning of universal sentence representations from
natural language inference data,” in EMNLP, 2017, pp. 670–680.

[29] T. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock `y, and S. Khudanpur,
“Recurrent neural network based language model.” in Interspeech,
vol. 2, 2010, p. 3.

[30] Z. Xu, B. Liu, B. Wang, C. Sun, and X. Wang, “Incorporating loose-
structured knowledge into lstm with recall gate for conversation
modeling,” in International Joint Conference on Neural Networks, 2017,
pp. 3506–3513.

[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-

tion,” in ICLR, 2015.

