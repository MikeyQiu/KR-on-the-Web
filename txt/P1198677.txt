9
1
0
2
 
v
o
N
 
9
1
 
 
]

V
C
.
s
c
[
 
 
1
v
1
5
2
8
0
.
1
1
9
1
:
v
i
X
r
a

General E(2) - Equivariant Steerable CNNs

Maurice Weiler∗
University of Amsterdam, QUVA Lab
m.weiler@uva.nl

Gabriele Cesa∗†
University of Amsterdam
cesa.gabriele@gmail.com

Abstract

The big empirical success of group equivariant networks has led in recent years to
the sprouting of a great variety of equivariant network architectures. A particular
focus has thereby been on rotation and reﬂection equivariant CNNs for planar
images. Here we give a general description of E(2)-equivariant convolutions in
the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields
constraints on the convolution kernels which depend on group representations
describing the transformation laws of feature spaces. We show that these constraints
for arbitrary group representations can be reduced to constraints under irreducible
representations. A general solution of the kernel space constraint is given for
arbitrary representations of the Euclidean group E(2) and its subgroups. We
implement a wide range of previously proposed and entirely new equivariant
network architectures and extensively compare their performances. E(2)-steerable
convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100
and STL-10 when used as a drop-in replacement for non-equivariant convolutions.

1

Introduction

The equivariance of neural networks under symmetry group actions has in the recent years proven
to be a fruitful prior in network design. By guaranteeing a desired transformation behavior of
convolutional features under transformations of the network input, equivariant networks achieve
improved generalization capabilities and sample complexities compared to their non-equivariant
counterparts. Due to their great practical relevance, a big pool of rotation- and reﬂection- equivariant
models for planar images has been proposed by now. Unfortunately, an empirical survey, reproducing
and comparing all these different approaches, is still missing.

An important step in this direction is given by the theory of Steerable CNNs [1, 2, 3, 4, 5] which
deﬁnes a very general notion of equivariant convolutions on homogeneous spaces. In particular,
steerable CNNs describe E(2)-equivariant (i.e. rotation- and reﬂection-equivariant) convolutions on
the image plane R2. The feature spaces of steerable CNNs are thereby deﬁned as spaces of feature
ﬁelds, characterized by a group representation which determines their transformation behavior under
transformations of the input. In order to preserve the speciﬁed transformation law of feature spaces,
the convolutional kernels are subject to a linear constraint, depending on the corresponding group
representations. While this constraint has been solved for speciﬁc groups and representations [1, 2],
no general solution strategy has been proposed so far. In this work we give a general strategy which
reduces the solution of the kernel space constraint under arbitrary representations to much simpler
constraints under single, irreducible representations.

Speciﬁcally for the Euclidean group E(2) and its subgroups, we give a general solution of this kernel
space constraint. As a result, we are able to implement a wide range of equivariant models, covering
regular GCNNs [6, 7, 8, 9, 10, 11], classical Steerable CNNs [1], Harmonic Networks [12], gated
Harmonic Networks [2], Vector Field Networks [13], Scattering Transforms [14, 15, 16, 17, 18] and
entirely new architectures, in one uniﬁed framework. In addition, we are able to build hybrid models,
mixing different ﬁeld types (representations) of these networks both over layers and within layers.

* Equal contribution, author ordering determined by random number generator.
† This research has been conducted during an internship at QUVA lab, University of Amsterdam.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

orthogonal
special orthogonal
cyclic
reﬂection
dihedral

order |G| G ≤ O(2)
O(2)
SO(2)
CN
({±1}, ∗) ∼= D1
DN

-
-
N
2
2N

∼= CN (cid:111)({±1}, ∗)

(R2, +) (cid:111) G

E(2) ∼= (R2, +) (cid:111) O(2)
SE(2) ∼= (R2, +) (cid:111) SO(2)

(R2, +) (cid:111) CN
(R2, +) (cid:111) ({±1}, ∗)
(R2, +) (cid:111) DN

Table 1: Overview over the different groups covered in our framework.

We further propose a group restriction operation, allowing for network architectures which are
decreasingly equivariant with depth. This is useful e.g. for natural images which show low level
features like edges in arbitrary orientations but carry a sense of preferred orientation globally. An
adaptive level of equivariance accounts for the resulting loss of symmetry in the hierarchy of features.

Since the theory of steerable CNNs does not give a preference for any choice of group representation
or equivariant nonlinearity, we run an extensive benchmark study, comparing different equivariance
groups, representations and nonlinearities. We do so on MNIST 12k, rotated MNIST SO(2) and
reﬂected and rotated MNIST O(2) to investigate the inﬂuence of the presence or absence of certain
symmetries in the dataset. A drop in replacement of our equivariant convolutional layers is shown to
yield signiﬁcant gains over non-equivariant baselines on CIFAR10, CIFAR100 and STL-10.

Beyond the applications presented in this paper, our contributions are of relevance for general
steerable CNNs on homogeneous spaces [3, 4] and gauge equivariant CNNs on manifolds [5] since
these models obey the same kind of kernel constraints. More speciﬁcally, 2-dimensional manifolds,
endowed with an orthogonal structure group O(2) (or subgroups thereof), necessitate exactly the
kernel constraints solved in this paper. Our results can therefore readily be transferred to e.g. spherical
CNNs [19, 5, 20, 21, 22, 23] or more general models of geometric deep learning [24, 25, 26, 27].

2 General E(2) - Equivariant Steerable CNNs

Convolutional neural networks process images by extracting a hierarchy of feature maps from a given
input signal. The convolutional weight sharing ensures the inference to be translation-equivariant
which means that a translated input signal results in a corresponding translation of the feature maps.
However, vanilla CNNs leave the transformation behavior of feature maps under more general
transformations, e.g. rotations and reﬂections, undeﬁned. In this work we devise a general framework
for convolutional networks which are equivariant under the Euclidean group E(2), that is, under
isometries of the plane R2. We work in the framework of steerable CNNs [1, 2, 3, 4, 5] which provides
a quite general theory for equivariant CNNs on homogeneous spaces, including Euclidean spaces Rd
as a speciﬁc instance. Sections 2.2 and 2.3 brieﬂy review the theory of Euclidean steerable CNNs as
described in [2]. The following subsections explain our main contributions: a decomposition of the
kernel space constraint into irreducible subspaces (2.4), their solution for E(2) and subgroups (2.5),
an overview on the group representations used to steer features, their admissible nonlinearities and
their use in related work (2.6), the group restriction operation (2.7) and implementation details (2.8).

Isometries of the Euclidean plane R2

2.1
The Euclidean group E(2) is the group of isometries of the plane R2, consisting of translations,
rotations and reﬂections. Characteristic patterns in images often occur at arbitrary positions and
in arbitrary orientations. The Euclidean group therefore models an important factor of variation of
image features. This is especially true for images without a preferred global orientation like satellite
imagery or biomedical images but often also applies to low level features of globally oriented images.
One can view the Euclidean group as being constructed from the translation group (R2, +) and the
orthogonal group O(2) = {O ∈ R2×2 | OT O = id2×2} via the semidirect product operation as
E(2) ∼= (R2, +) (cid:111) O(2). The orthogonal group thereby contains all operations leaving the origin
invariant, i.e. continuous rotations and reﬂections. In order to allow for different levels of equivariance
and to cover a wide spectrum of related work we consider subgroups of the Euclidean group of the
form (R2, +) (cid:111) G, deﬁned by subgroups G ≤ O(2). Speciﬁcally, G could be either the special
orthogonal group SO(2), the group ({±1}, ∗) of the reﬂections along a given axis, the cyclic groups
CN , the dihedral groups DN or the orthogonal group O(2) itself. While SO(2) describes continuous

2

rotations (without reﬂections), CN and DN contain N discrete rotations by angles multiple of 2π
N
and, in the case of DN , reﬂections. CN and DN are therefore discrete subgroups of order N and 2N ,
respectively. For an overview over the groups and their interrelations see Table 1.
Since the groups (R2, +) (cid:111) G are semidirect products, one can uniquely decompose any of their
elements into a product tg where t ∈ (R2, +) and g ∈ G [3] which we will do in the rest of the paper.

2.2 E(2) - steerable feature ﬁelds
Steerable CNNs deﬁne feature spaces as spaces of steerable feature ﬁelds f : R2 → Rc which
associate a c-dimensional feature vector f (x) ∈ Rc to each point x of a base space, in our case the
plane R2. In contrast to vanilla CNNs, the feature ﬁelds of steerable CNNs are associated with a
transformation law which speciﬁes their transformation under actions of E(2) (or subgroups) and
therefore endows features with a notion of orientation1. Formally, a feature vector f (x) encodes the
coefﬁcients of a coordinate independent geometric feature relative to a choice of reference frame or,
equivalently, image orientation (see Appendix A).

that

An important example are scalar feature ﬁelds
s : R2 → R, describing for instance gray-scale im-
ages or temperature and pressure ﬁelds. The
Euclidean group acts on scalar ﬁelds by mov-
ing each pixel
is,
to a new position,
(cid:55)→ s (cid:0)(tg)−1x(cid:1) = s (cid:0)g−1(x − t)(cid:1) for
s(x)
some tg ∈ (R2, +) (cid:111) G;
see Figure 1, left.
Vector ﬁelds v : R2 → R2, like optical ﬂow or gra-
dient
images, on the other hand transform as
v(x) (cid:55)→ g · v (cid:0)g−1(x − t)(cid:1). In contrast to the case
of scalar ﬁelds, each vector is therefore not only
moved to a new position but additionally changes its
orientation via the action of g ∈ G; see Figure 1, right.
The transformation law of a general feature ﬁeld f : R2 → Rc is fully characterized by its type ρ.
Here ρ : G (cid:55)→ GL(Rc) is a group representation, specifying how the c channels of each feature
vector f (x) mix under transformations. A representation satisﬁes ρ(g˜g) = ρ(g)ρ(˜g) and therefore
models the group multiplication g˜g as multiplication of c × c matrices ρ(g) and ρ(˜g); see Ap-
pendix B. More speciﬁcally, a ρ-ﬁeld transforms under the induced representation23 (cid:104)
(cid:105)
ρ
of (R2, +) (cid:111) G as

Figure 1: Transformation behavior of ρ-ﬁelds.

Ind(R2,+)(cid:111)G

vector ﬁeld ρ(g) = g

scalar ﬁeld ρ(g) = 1

G

f (x) (cid:55)→

(cid:16)(cid:104)

Ind(R2,+)(cid:111)G

G

(cid:105)
(tg) · f
ρ

(cid:17)

(x)

:= ρ(g) · f (cid:0)g−1(x − t)(cid:1) .

(1)

As in the examples above, it transforms feature ﬁelds by moving the feature vectors from g−1(x − t)
to a new position x and acting on them via ρ(g). We thus ﬁnd scalar ﬁelds to correspond to the trivial
representation ρ(g) = 1 ∀g ∈ G which reﬂects that the scalar values do not change when being
moved. Similarly, a vector ﬁeld corresponds to the standard representation ρ(g) = g of G.

In analogy to the feature spaces of vanilla CNNs comprising multiple channels, the feature spaces of
steerable CNNs consist of multiple feature ﬁelds fi : R2 → Rci, each of which is associated with its
own type ρi : G → GL(Rci). A stack f = (cid:76)
i fi of feature ﬁelds is then deﬁned to be concatenated
from the individual feature ﬁelds and transforms under the direct sum ρ = (cid:76)
i ρi of the individual
representations. Since the direct sum representation is block diagonal, the individual feature ﬁelds are
guaranteed to transform independently from each other. A common example for a stack of feature
ﬁelds are RGB images f: R2 → R3. Since the color channels transform independently under rotations
we identify them as three independent scalar ﬁelds. The stacked ﬁeld representation is thus given by
the direct sum (cid:76)3
i=1 1 = id3×3 of three trivial representations. While the input and output types of
steerable CNNs are given by the learning task, the user needs to specify the types ρi of intermediate
feature ﬁelds as hyperparameters, similar to the choice of channels for vanilla CNNs. We discuss
different choices of representations in Section 2.6 and investigate them empirically in Section 3.1.

1 Steerable feature ﬁelds can therefore be seen as ﬁelds of capsules [28].
2 Induced representations are the most general transformation laws compatible with convolutions [3, 4].
3 Note that this simple form of the induced representation is a special case for semidirect product groups.

3

2.3 E(2) - steerable convolutions

In order to preserve the transformation law of steerable feature spaces, each network layer is required
to be equivariant under the group actions. As proven for Euclidean groups4 in [2], the most general
equivariant linear map between steerable feature spaces, transforming under ρin and ρout, is given by
convolutions with G-steerable kernels5 k : R2 → Rcout×cin , satisfying a kernel constraint

k(gx) = ρout(g)k(x)ρin(g−1) ∀g ∈ G, x ∈ R2 .
Intuitively, this constraint determines the form of the kernel in transformed coordinates gx in terms
of the kernel in non-transformed coordinates x and thus its response to transformed input ﬁelds.
It ensures that the output feature ﬁelds transform as speciﬁed by Ind ρout when the input ﬁelds are
being transformed by Ind ρin; see Appendix D.1 for a proof.

(2)

Since the kernel constraint is linear, its solutions form a linear subspace of the vector space of
unconstrained kernels considered in conventional CNNs. It is thus sufﬁcient to solve for a basis of the
G-steerable kernel space in terms of which the equivariant convolutions can be parameterized. The
lower dimensionality of the restricted kernel space enhances the parameter efﬁciency of steerable
CNNs over conventional CNNs similarly to the increased parameter efﬁciency of CNNs over MLPs
by translational weight sharing.

2.4

Irrep decomposition of the kernel constraint

The kernel constraint (2) in principle needs to be solved individually for each pair of input and
output types ρin and ρout to be used in the network6. Here we show how the solution of the kernel
constraint for arbitrary representations can be reduced to much simpler constraints under irreducible
representations (irreps). Our approach relies on the fact that any representation of a ﬁnite or compact
group decomposes under a change of basis into a direct sum of irreps, each corresponding to an
invariant subspace of the representation space Rc on which ρ acts. Denoting the change of basis
by Q, this means that one can always write ρ = Q−1 (cid:2)(cid:76)
(cid:3) Q where ψi are the irreducible
representations of G and the index set I encodes the types and multiplicities of irreps present in ρ.
A decomposition can be found by exploiting basic results of character theory and linear algebra [29].

i∈I ψi

The decomposition of ρin and ρout in the kernel constraint (2) leads to
(cid:105)
(cid:105)
(g)
ψi(g)

k(gx) = Q−1
out

Qout k(x) Q−1
in

ψ−1
j

(cid:104)(cid:77)

(cid:104)(cid:77)

Qin

∀g ∈ G, x ∈ R2,

which, deﬁning a kernel relative to the irrep bases as κ := QoutkQ−1
(cid:105)
(cid:105)
(g)
ψi(g)

κ(gx) =

(cid:104)(cid:77)

(cid:104)(cid:77)

κ(x)

ψ−1
j

in , implies

∀g ∈ G, x ∈ R2.

i∈Iout

i∈Iout

j∈Iin

j∈Iin

The left and right multiplication with a direct sum of irreps reveals that the constraint decomposes
into independent constraints

κij(gx) = ψi(g) κij(x) ψ−1

(g)

j

∀g ∈ G, x ∈ R2 where i ∈ Iout, j ∈ Iin

(3)

on blocks κij in κ corresponding to invariant subspaces of the full space of equivariant kernels; see
Appendix E for a visualization. In order to solve for a basis of equivariant kernels satisfying the
original constraint (2), it is therefore sufﬁcient to solve the irrep constraints (3) to obtain bases for
each block, revert the change of basis and take the union over different blocks. Speciﬁcally, given
dij-dimensional bases(cid:8)κij
ijdij-dimensional basis

(cid:9) for the blocks κij of κ, we get a d = (cid:80)

(cid:91)

(cid:110)

i∈Iout

j∈Iin

Q−1

out κij

1 Qin, · · · , Q−1

out κij
dij

Qin

(cid:111)

(4)

1 , · · ·, κij
dij
(cid:91)
(cid:9) :=

(cid:8)k1, · · · , kd

of solutions of (2). Here κij denotes a block κij being ﬁlled at the corresponding location of a
matrix of the shape of κ with all other blocks being set to zero; see Appendix E. The completeness of
the basis found this way is guaranteed by construction if the bases for each block ij are complete.

4 Proofs for more general cases can be found in [3, 4].
5 As k : R2 → Rcout×cin returns a matrix of shape (cout, cin) for each position x ∈ R2, its discretized version

can be represented by a tensor of shape (cout, cin, X, Y ) as usually done in deep learning frameworks.

6 A numerical solution technique which is based on a Clebsch-Gordan decomposition of tensor products of
irreps has been proposed in [2]. While this technique would generalize to arbitrary representations it becomes
prohibitively expensive for larger representations as considered here; see Appendix G.

4

Note that while this approach shares some basic ideas with the solution strategy proposed in [2], it is
computationally more efﬁcient for large representations; see Appendix G. We want to emphasize
that this strategy for reducing the kernel constraint to irreducible representations is not restricted to
subgroups of O(2) but applies to steerable CNNs in general.

2.5 General solution of the kernel constraint for O(2) and subgroups
In order to build isometry-equivariant CNNs on R2 we need to solve the irrep constraints (3) for the
speciﬁc case of G being O(2) or one of its subgroups. For this purpose note that the action of G on
R2 is norm-preserving, that is, |g.x| = |x| ∀g ∈ G, x ∈ R2. The constraints (2) and (3) therefore
only restrict the angular parts of the kernels but leave their radial parts free. Since furthermore
all irreps of G correspond to one unique angular frequency (see Appendix F.2), it is convenient to
expand the kernel w.l.o.g. in terms of an (angular) Fourier series

κij
αβ

(cid:0)x(r, φ)(cid:1) = Aαβ,0(r) +

(cid:88)∞

(cid:104)

µ=1

Aαβ,µ(r) cos(µφ) + Bαβ,µ(r) sin(µφ)

(5)

(cid:105)

with real-valued, radially dependent coefﬁcients Aαβ,µ : R+ → R and Bαβ,µ : R+ → R for each
matrix entry κij
αβ of block κij. By inserting this expansion into the irrep constraints (3) and projecting
on individual harmonics we obtain constraints on the Fourier coefﬁcients, forcing most of them to be
zero. The vector spaces of G-steerable kernel blocks κij satisfying the irrep constraints (3) are then
parameterized in terms of the remaining Fourier coefﬁcients. The completeness of this basis follows
immediately from the completeness of the Fourier basis. Similar approaches have been followed in
simpler settings for the cases of CN in [7], SO(2) in [12] and SO(3) in [2].

The resulting bases for the angular parts of kernels for each pair of irreducible representations of
O(2) are shown in Table 2. It turns out that each basis element is harmonic and associated to one
unique angular frequency. Appendix F gives an explicit derivation and the resulting bases for all
possible pairs of irreps for all groups G ≤ O(2) following the strategy presented in this section. The
analytical solutions for SO(2), ({±1}, ∗), CN and DN are found in Tables 8, 10, 11 and 12. Since
these groups are subgroups of O(2), they enforce a weaker kernel constraint as compared to O(2).
As a result, the bases for G < O(2) are higher dimensional, i.e. they allow for a wider range of
kernels. A higher level of equivariance therefore leads simultaneously to a guaranteed behavior of the
inference process under transformations and on the other hand to an improved parameter efﬁciency.

ψi

ψj

trivial

sign-ﬂip

trivial
(cid:2)1(cid:3)

∅

sign-ﬂip

∅

(cid:2)1(cid:3)

(cid:34)

(cid:35) (cid:34)

(cid:35) (cid:34)

frequency
m ∈ N+

sin(mφ)
(cid:57)cos(mφ)

cos(mφ)
sin(mφ)

cos(cid:0)(m(cid:57)n)φ(cid:1) (cid:57)sin(cid:0)(m(cid:57)n)φ(cid:1)
cos(cid:0)(m(cid:57)n)φ(cid:1)
sin(cid:0)(m(cid:57)n)φ(cid:1)

frequency n ∈ N+
(cid:2) sin(nφ), (cid:57) cos(nφ)(cid:3)
sin(nφ)(cid:3)
(cid:2) cos(nφ),
(cid:35)
cos(cid:0)(m+n)φ(cid:1)
sin(cid:0)(m+n)φ(cid:1)
sin(cid:0)(m+n)φ(cid:1) (cid:57)cos(cid:0)(m+n)φ(cid:1)

(cid:34)
,

(cid:35)

Table 2: Bases for the angular parts of O(2)-steerable kernels satisfying the irrep constraint (3) for different pairs
of input ﬁeld irreps ψj and output ﬁeld irreps ψi.The different types of irreps are explained in Appendix F.2.

2.6 Group representations and nonlinearities

A question which so far has been left open is which ﬁeld types, i.e. which representations ρ of G,
should be used in practice. Considering only the convolution operation with G-steerable kernels for
the moment, it turns out that any change of basis P to an equivalent representation (cid:101)ρ := P −1ρP is
irrelevant. To see this, consider the irrep decomposition ρ = Q−1 (cid:2)(cid:76)
(cid:3) Q used in the solution
i∈I ψi
of the kernel constraint to obtain a basis {ki}d
i=1 of G-steerable kernels as deﬁned by Eq. (4). Any
equivalent representation will decompose into (cid:101)ρ = (cid:101)Q−1 (cid:2)(cid:76)
(cid:101)Q with (cid:101)Q = QP for some P and
therefore result in a kernel basis {P −1
i=1 which entirely negates changes of bases between
equivalent representations. It would therefore w.l.o.g. sufﬁce to consider direct sums of irreps
ρ = (cid:76)
i∈I ψi as representations only, reducing the question on which representations to choose to
the question on which types and multiplicities of irreps to use.

out kiPin}d

i∈I ψi

(cid:3)

In practice, however, convolution layers are interleaved with other operations which are sensitive to
speciﬁc choices of representations. In particular, nonlinearity layers are required to be equivariant
under the action of speciﬁc representations. The choice of group representations in steerable CNNs

5

therefore restricts the range of admissible nonlinearities, or, conversely, a choice of nonlinearity allows
only for certain representations. In the following we review prominent choices of representations
found in the literature in conjunction with their compatible nonlinearities.

All equivariant nonlinearities considered here act spatially localized, that is, on each feature vector
f (x) ∈ Rcin for all x ∈ R2 individually. They might produce different types of output ﬁelds
ρout : G → GL(Rcout), that is, σ : Rcin → Rcout, f (x) (cid:55)→ σ(f (x)). As proven in Appendix D.2,
it is sufﬁcient to require the equivariance of σ under the actions of ρin and ρout, i.e. σ ◦ ρin(g) =
ρout(g)◦σ ∀g ∈ G, for the nonlinearities to be equivariant under the action of induced representations
when being applied to a whole feature ﬁeld as σ(f )(x) := σ(f (x)).

(cid:12)f (x)(cid:12)

A general class of representations are unitary representations which preserve the norm of their
representation space, that is, they satisfy |ρunitary(g)f (x)| = (cid:12)
(cid:12) ∀ g ∈ G. As proven in
Appendix D.2.2, nonlinearities which solely act on the norm of feature vectors but preserve their
orientation are equivariant w.r.t. unitary representations. They can in general be decomposed in
σnorm : Rc → Rc, f (x) (cid:55)→ η(cid:0)|f (x)|(cid:1) f (x)
|f (x)| for some nonlinear function η : R≥0 → R≥0 acting
on the norm of feature vectors. Norm-ReLUs, deﬁned by η(|f (x)|) = ReLU(|f (x)| − b) where
b ∈ R+ is a learned bias, were used in [12, 2]. In [28], the authors consider squashing nonlinearities
η(|f (x)|) = |f (x)|2
|f (x)|2+1 . Gated nonlinearities were proposed in [2] as conditional version of norm
1
nonlinearities. They act by scaling the norm of a feature ﬁeld by learned sigmoid gates
1+e−s(x) ,
parameterized by a scalar feature ﬁeld s. All representations considered in this paper are unitary
such that their ﬁelds can be acted on by norm-nonlinearities. This applies speciﬁcally also to all
irreducible representations ψi of G ≤ O(2) which are discussed in detail in Section F.2.

A common choice of representations of ﬁnite groups like CN and DN are regular representations.
Their representation space R|G| has dimensionality equal to the order of the group, e.g. RN for CN
and R2N for DN . The action of the regular representation is deﬁned by assigning each axis eg of R|G|
to a group element g ∈ G and permuting the axes according to ρG
reg(˜g)eg := e˜gg. Since this action is
just permuting channels of ρG
reg-ﬁelds, it commutes with pointwise nonlinearities like ReLU; a proof
is given in Appendix D.2.3. While regular steerable CNNs were empirically found to perform very
well, they lead to high dimensional feature spaces with each individual ﬁeld consuming |G| channels.
Regular steerable CNNs were investigated for planar images in [6, 7, 8, 9, 10, 17, 18, 30], for spherical
CNNs in [19, 5] and for volumetric convolutions in [31, 32]. Further, the translation of feature maps
of conventional CNNs can be viewed as action of the regular representation of the translation group.

Closely related to regular representations are quotient representations. Instead of permuting |G|
channels indexed by G, they permute |G|/|H| channels indexed by cosets gH in the quotient
space G/H of a subgroup H ≤ G. Speciﬁcally, they act on axes egH of R|G|/|H| as deﬁned by
ρG/H
quot (˜g)egH := e˜ggH . This deﬁnition covers regular representations as a special case for the trivial
subgroup H = {e}. As permutation representations, quotient representations allow for pointwise
nonlinearities; see Appendix D.2.3. A beneﬁt of quotient representations over regular representation
is that they require by a factor of |H| less channels per feature ﬁeld. On the other hand, they enforce
more symmetries in the feature ﬁelds which result in a restriction of the G-steerable kernel basis; see
Appendix C for details and intuitive examples. Quotient representations were considered in [1, 11].

Both regular and quotient representations can be viewed as being induced from the trivial repre-
sentation of a subgroup H ≤ G, speciﬁcally ρG
H 1. More gen-
erally, any representation ˜ρ : H → GL(Rc) can be used to deﬁne an induced representations7
ρind = IndG
H ˜ρ : G → GL(Rc·|G:H|). Here |G : H| denotes the index of H in G which corresponds to
|G|/|H| if G and H are both ﬁnite. Admissible nonlinearities of induced representations can be con-
structed from valid nonlinearities of ˜ρ. For more information on inductions we refer to Appendix B.

quot = IndG

{e} 1 and ρG

reg = IndG

Regular and quotient ﬁelds can furthermore be acted on by nonlinear pooling operators. Via a
group pooling or projection operation max : Rc → R, f (x) → max(f (x)) the works [6, 7, 9,
32, 31] extract the maximum value of a regular or quotient ﬁeld. The invariance of the maximum
operation implies that the resulting features form scalar ﬁelds. Since group pooling operations

7 The induction from H to G considered here is conceptually equivalent to the induction (1) from G to
G (cid:111) (R2, +) but applies to representations acting on a single feature vector rather than on a full feature ﬁeld.

6

discard information on the feature orientations entirely, vector ﬁeld nonlinearities σvect : RN → R2
for regular representations of CN were proposed in [13]. Vector ﬁeld nonlinearities do not only
keep the maximum response max(f (x)) but also its index arg max(f (x)). This index corresponds
to a rotation angle θ = 2π
N arg max(f (x)) which is used to deﬁne a vector ﬁeld with elements
v(x) = max(f (x))(cos(θ), sin(θ))T . The equivariance of this operation is proven in D.2.4.
In general, any pair of feature ﬁelds f1 : R2 → Rc1 and f2 : R2 → Rc2 can be combined via
the tensor product operation f1 ⊗ f2. Given that the individual ﬁelds transform under arbitrary
representations ρ1 and ρ2, their product transforms under the tensor product representation ρ1 ⊗ρ2 :
G → GL(Rc1·c2). Since the tensor product is a nonlinear transformation by itself, no additional
nonlinearity needs to be applied. Tensor product nonlinearities have been discussed in [33, 2, 20, 34].

Any pair f1 and f2 of feature ﬁelds can furthermore be concatenated by taking their direct sum
f1 ⊕ f2 : R2 → Rc1+c2 which we used in Section 2.2 to deﬁne feature spaces comprising multiple
feature ﬁelds. The concatenated ﬁeld transforms according to the direct sum representation as
(cid:0)ρ1 ⊕ ρ2
(cid:1) := ρ1(g)f1 ⊕ ρ2(g)f2. Since each constituent fi of a concatenated ﬁeld
transforms independently it can be viewed as an individual feature and is equivariant under the action
of its corresponding nonlinearity σi; see Section D.2.1.

(cid:1)(g)(cid:0)f1 ⊕ f2

The theory of steerable CNNs does not prefer any of the here presented representations or nonlineari-
ties over each other. We are therefore extensively comparing different choices in Section 3.1.

2.7 Group restrictions and inductions

The key idea of equivariant networks is to exploit symmetries in the distribution of characteristic
patterns in signals. The level of symmetry present in data might thereby vary over different length
scales. For instance, natural images typically show small features like edges or intensity gradients in
arbitrary orientations and reﬂections. On a larger length scale, however, the rotational symmetry is
broken as manifested in visual patterns exclusively appearing upright but still in different reﬂections.
Each individual layer of a convolutional network should therefore be adapted to the symmetries
present in the length scale of its ﬁelds of view.

A loss of symmetry can be implemented by restricting the equivariance constraints at a certain depth
to a subgroup (R2, +) (cid:111) H ≤ (R2, +) (cid:111) G, where H ≤ G; e.g. from rotations and reﬂections
G = O(2) to mere reﬂections H = ({±1}, ∗) in the example above. This requires the feature ﬁelds
produced by a layer with a higher level of equivariance to be reinterpreted in the following layer as
ﬁelds transforming under a subgroup. Speciﬁcally, a ρ-ﬁeld, transforming under a representation
ρ : G → GL(Rc), needs to be reinterpreted as a ˜ρ-ﬁeld, where ˜ρ : H → GL(Rc) is a representation
of the subgroup H ≤ G. This is naturally achieved by deﬁning ˜ρ to be the restricted representation

˜ρ := ResG

H (ρ) : H → GL(Rc), h (cid:55)→ ρ(h) ,

(6)

deﬁned by restricting the domain of ρ to H. Since a subsequent H-steerable convolution layers can
map ﬁelds of arbitrary representations we can readily process the resulting ResG

H (ρ)-ﬁeld further.

Conversely, it is also possible that local patterns are aligned, while patterns which emerge on a larger
scale are more symmetrically distributed. This can be exploited by lifting a ρ ﬁeld, transforming
under H ≤ G, to an induced IndG
H ρ ﬁeld, transforming under G.
The effect of enforcing different levels of equivariance through group restrictions is investigated
empirically in Sections 3.2, 3.4, 3.5 and 3.6.

2.8

Implementation details

E(2)-steerable CNNs rely on convolutions with O(2)-steerable kernels. Our implementation therefore
involves 1) computing a basis of steerable kernels, 2) the expansion of a steerable kernel in terms of
this basis with learned expansion coefﬁcients and 3) running the actual convolution routine. Since the
kernel basis depends only on the chosen representations it is precomputed before training.

Given an input and output representation ρin and ρout of G ≤ O(2), we ﬁrst precompute a ba-
sis {k1, . . . kd} of G-steerable kernels satisfying Eq. (2). In order to solve the kernel constraint
we compute the types and multiplicities of irreps in the input and output representations using
character theory [29]. The change of basis can be obtained by solving the linear system of
equations ρ(g) = Q−1[(cid:76)
i∈I ψi(g)]Q ∀g ∈ G. For each pair ψi, ψj of irreps occurring in ρout and

7

1 , . . . , κij
ρin we retrieve the analytical solutions {κij
} listed in Appendix F.3. Together with the
dij
change of basis matrices Qin and Qout, they fully determine the angular parts of the basis {k1, . . . , kd}
of G-steerable kernels via Eq. (4). Since the kernel space constraint affects only the angular behavior
of the kernels we are free to choose any radial proﬁle. Following [7] and [2], we choose Gaussian
radial proﬁles exp (cid:0) 1
2σ2 (r (cid:57)R)2(cid:1) of width σ, centered at radii R = 1, . . . , (cid:98)s/2(cid:99).
In practice, we consider digitized signals on a pixel grid8 Z2. Correspondingly, we sample the
analytically found kernel basis {k1, . . . , kd} on a square grid of size s×s to obtain their numerical
representation of shape (d, cout, cin, s, s). In this process it is important to prevent aliasing effects.
Speciﬁcally, each basis kernel corresponds to one particular angular harmonic; see Table 2. When
being sampled with a too low rate, a basis kernel can appear as a lower harmonic and might therefore
introduce non-equivariant kernels to the sampled basis. For this reason, preventing aliasing is
necessary to guarantee (approximate) equivariance. In order to ensure a faithful discretization, note
that each Gaussian radial proﬁle deﬁnes a ring whose circumference, and thus angular sampling
rate, is proportional to its radius. It is therefore appropriate to bandlimit the kernel basis by a cutoff
frequency which is chosen in proportion to the rings’ radii. Since the basis kernels are harmonics of
speciﬁc angular frequencies this is easily implemented by discarding high frequency solutions.

γ ρout,γ and ρin = (cid:76)

In typical applications the feature spaces are deﬁned to be composed of multiple independent
feature ﬁelds. Since the corresponding representations are block diagonal, this implies that the actual
constraint (2) decomposes into multiple simpler constraints9 which we leverage in our implementation
to improve its computational efﬁciency. Assuming the output and input representations of a layer to
be given by ρout = (cid:76)
δ ρin,δ respectively, the constraint on the full kernel space
is equivalent to constraints on its blocks kγδ which map between the independent ﬁelds transforming
under ρin,δ and ρout,γ. Our implementation therefore computes a sampled basis (cid:8)kγδ
(cid:9) of
kγδ for each pair (ρin,δ, ρout,γ) of input and output representations individually.
At runtime, the convolution kernels are expanded by contracting the sampled kernel bases with learned
weights. Speciﬁcally, each basis (cid:8)kγδ
(cid:9), realized by a tensor of shape (dγδ, cout,γ, cin,δ, s, s),
is expanded into the corresponding block kγδ of the kernel by contracting it with a tensor of learned
parameters of shape (dγδ). This process is sped up further by batching together multiple occurrences
of the same pair of representations and thus block bases.

1 , . . . , kγδ
dγδ

1 , . . . , kγδ
dγδ

The resulting kernels are then used in a standard convolution routine. In practice we ﬁnd that the time
spent on the actual convolution of reasonably sized images outweighs the cost of the kernel expansion.
In evaluation mode the parameters are not updated such that the kernel needs to be expanded only
once and can then be reused. E(2)-steerable CNNs therefore have no computational overhead in
comparison to conventional CNNs at test time.

Our implementation is provided as a PyTorch extension which is available at https://github.com/
QUVA-Lab/e2cnn. The library provides equivariant versions of many neural network operations,
including G-steerable convolutions, nonlinearities, mappings to produce invariant features, spatial
pooling, batch normalization and dropout. Feature ﬁelds are represented by geometric tensors, which
are wrapping a torch.Tensor object and augment it, among other things, with their transformation
law under the action of a symmetry group. This allows for a dynamic type-checking which prevents
the user from applying operations to geometric tensors whose transformation law does not match
the transformation law expected by the operation. The user interface hides most complications on
group theory and solutions of the kernel space constraint and requires the user only to specify the
transformation laws of feature spaces. For instance, a C8-equivariant convolution operation, mapping
a RGB image, identiﬁed as three scalar ﬁelds, to ten regular feature ﬁelds, would be instantiated by:

1
2
3
4

r2_act = Rot2dOnR2(N=8)
feat_type_in = FieldType(r2_act, 3*[r2_act.trivial_repr])
feat_type_out = FieldType(r2_act, 10*[r2_act.regular_repr])
conv_op = R2Conv(feat_type_in, feat_type_out, kernel_size=5)

8 Note that this prevents equivariance from being exact for groups which are not symmetries of the grid.

Speciﬁcally, for Z2 only subgroups of D4 are exact symmetries which motivated their use in [6, 10, 1].

9 The same decomposition was used in a different context in Section 2.4.

8

Everything the user has to do is to specify that the group C8 acts on R2 by rotating it (line 1)
and to deﬁne the types ρin = (cid:76)3
reg of the input and output feature ﬁelds
(lines 2 and 3), which are subsequently passed to the constructor of the steerable convolution (line 4).

i=1 1 and ρout = (cid:76)10

i=1 ρC4

3 Experiments

Since the framework of general E(2)-equivariant steerable CNNs supports many choices of groups,
representations and nonlinearities, we ﬁrst run an extensive benchmark study over the space of
supported models in Section 3.1. As the performance of the models depends heavily on the level
of symmetry present in the data, we evaluate each model on three different versions of the MNIST
dataset: on untransformed digits, randomly rotated digits and simultaneously rotated and reﬂected
digits, corresponding to transformations in {e}, in SO(2) and O(2), respectively. Section 3.2 further
investigates the effect of the invariance of models to different global symmetries. We discuss how too
high levels of invariance can hurt the performance and demonstrate how it can be prevented via group
restrictions. The convergence rate of equivariant models is being explored in Section 3.3. In addition,
we present two models which improve upon the previous state of the art on MNIST rot in Section 3.4.

The insights from these benchmark experiments are then applied to build several models for classifying
CIFAR-10 and CIFAR-100 images in Section 3.5 and STL-10 images in Section 3.6. These datasets
consist of globally aligned, natural images which allows us to validate that exploiting local symmetries
is advantageous. In order to compare the beneﬁt from different local symmetries we restrict the
equivariance of our models to different subgroups at varying depths. All models are built by
replacing the non-equivariant convolutions of well established baseline models with our G-steerable
convolutions. Despite not tuning any hyperparameters, we ﬁnd that all of our models signiﬁcantly
outperform the baselines. This holds true even for settings in which a strong auto-augmentation
policy is being used.

All of our experiments are found in a dedicated repository at https://github.com/gabri95/
e2cnn_experiments.

3.1 Model benchmarking on transformed MNIST datasets

We ﬁrst perform a comprehensive benchmarking to compare the impact of the different design choices
covered in this work. Each model is evaluated on three different versions of the MNIST dataset, each
of which consists of 12000 training images and 50000 test images. As non-transformed version we
use the ofﬁcial MNIST 12k dataset. The ofﬁcial rotated MNIST dataset, often called MNIST rot,
differs from MNIST 12k by a random SO(2) rotation of each digit. In addition we build MNIST O(2)
whose digits are both rotated and reﬂected. These datasets allow us to study the beneﬁt from different
levels of G-steerability in the presence or absence of certain symmetries. In order to not disadvantage
models with lower levels of equivariance and since it would be done in real scenarios we train all
models using augmentation by the transformations present in the corresponding dataset.

Table 3 shows the test errors of 57 different models on the three MNIST variants. The ﬁrst four
columns state the equivariance groups, representations, nonlinearities and invariant maps which
distinguish the models. Column ﬁve cites related work which proposed the corresponding model
design. The statistics of each entry are averaged over (at least) 6 samples. All models in these
experiments are derived from the base architecture described in Table 13 in Appendix H. The actual
width of each model is adapted such that the number of parameters is approximately preserved.
Note that this results in different numbers of channels, depending on the parameter efﬁciency of the
corresponding models. All models apply some form of invariant mapping to scalar ﬁelds followed by
spatial pooling after the last convolutional layer such that the predictions are guaranteed to be invariant
under the equivariance group of the model. The number of invariant features passed to the fully
connected classiﬁer is approximately kept constant by adapting the width of the last convolutional
layer to the invariant mapping used. In the remainder of this subsection we will guide through the
results presented in Table 3. For more information on the training setup, see Appendix H.1.

Regular steerable CNNs: Due to their popularity we ﬁrst cover steerable CNNs whose features
transform under regular representations of CN and DN for varying orders N . Note that these models
correspond to group convolutional CNNs [6, 7]. For the dihedral models we choose a vertical
reﬂection axis. We use ELUs [35] as pointwise nonlinearities and perform group pooling (see
Section 2.6) as invariant map after the ﬁnal convolution. Overall, regular steerable CNNs perform

9

Figure 2: Test errors of CN and DN regular steerable CNNs for different orders N for all three MNIST variants.
Left: All equivariant models improve upon the non-equivariant CNN baseline on MNIST O(2). The error
decreases before saturating at around 8 to 12 orientations. Since the dataset contains reﬂected digits, the
DN -equivariant models perform better than their CN counterparts. Middle: Since the intraclass variability of
MNIST rot is reduced, the performances of the CN model and the baseline CNN improve on this dataset. In
contrast, the DN models are invariant to reﬂections such that they can’t distinguish between MNIST O(2) and
MNIST rot. For N = 1 this leads to a worse performance than that of the baseline. Restricted dihedral models,
denoted by DN |5CN , make use of the local reﬂectional symmetries but are not globally invariant. This makes
them perform even better than the CN models. Right: On MNIST 12k the globally invariant models CN and
DN don’t yield better results than the baseline, however, the restricted (i.e. non-invariant) models CN |5{e} and
DN |5{e} do. For more details see the main text.

very well. The reason for this is that feature vectors, transforming under regular representations, can
encode any function on the group.

Figure 2 summarizes the results for all regular steerable CNNs on all variants of MNIST (rows 2-10
and 19-27 in Table 3). For MNIST O(2) and MNIST rot the prediction accuracies improve with N
but start to saturate at approximately 8 to 12 rotations. On MNIST O(2) the DN models perform
consistently better than the CN models of the same order N . This is the case since the dihedral
models are guaranteed to generalize over reﬂections which are present in the dataset. All equivariant
models outperform the non-equivariant CNN baseline.

On MNIST rot, the accuracy of the CN -equivariant models improve signiﬁcantly in comparison to
their results on MNIST O(2) since the intra-class variability is reduced. In contrast, the test errors of
the DN -equivariant models is the same on both datasets. The reason for this result is the reﬂection
invariance of the DN models which implies that they can’t distinguish between reﬂected digits. For
N = 1 the dihedral model is purely reﬂection- but not rotation invariant and therefore performs even
worse than the CNN baseline. This issue is resolved by restricting the dihedral models after the
penultimate convolution to CN ≤ DN , such that the group pooling after the ﬁnal convolution results
in only CN -invariant features. This model, denoted in the ﬁgure by DN |5CN , achieves a slightly better
accuracy than the pure CN -equivariant model since it can leverage local reﬂectional symmetries10.
For MNIST 12k the non-restricted DN models perform again worse than the CN models since they
are insensitive to the chirality of the digits. In order to explain the non-monotonic trend of the curves

10 The group restricted models are not listed in Table 3 but are discussed in Section 3.2.

10

1 {e}
2 C1
3 C2
4 C3
5 C4
6 C6
7 C8
8 C12
9 C16
10 C20
11 C4
12 C8
13 C12
14 C16
15 C20

16

17

18

C16

19 D1
20 D2
21 D3
22 D4
23 D6
24 D8
25 D12
26 D16
27 D20

28 D16

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

group

representation

nonlinearity

invariant map

citation MNIST O(2) MNIST rot MNIST 12k

(conventional CNN)

ELU

-

-

5.53 ± 0.20

2.87 ± 0.09

0.91 ± 0.06

regular

ρreg

ELU

G-pooling

quotient

5ρreg ⊕2ρ
5ρreg ⊕2ρ
5ρreg ⊕2ρ
5ρreg ⊕2ρ
5ρreg ⊕2ρ

C8/C4
quot ⊕2ψ0
C12/C4
quot ⊕3ψ0
C16/C4
quot ⊕4ψ0
C20/C4
quot ⊕5ψ0

C4/C2
quot ⊕2ψ0
C8/C2
quot ⊕2ρ
C12/C2
quot ⊕2ρ
C16/C2
quot ⊕2ρ
C20/C2
quot ⊕2ρ
G-pool
−−−−→ ψ0
vector pool
−−−−−−→ ψ1
vector−−→
pool

conv−−→ 2ρreg

regular/scalar

ψ0
regular/vector ψ1
mixed vector

ρreg ⊕ψ1

conv−−→ ρreg
conv−−→ ρreg

ELU, G-pooling

vector ﬁeld

ρreg ⊕ψ1 ELU, vector ﬁeld

[6, 36]

2.02 ± 0.02

0.90 ± 0.03

0.93 ± 0.04

[13, 37]
-

2.12 ± 0.02
1.87 ± 0.03

1.07 ± 0.03
0.83 ± 0.02

0.78 ± 0.03
0.63 ± 0.02

conv−−→ ρreg

G-pool
−−−−→ ψ0,0

ELU, G-pooling

1.92 ± 0.03

1.88 ± 0.07

1.74 ± 0.04

regular

ρreg

ELU

G-pooling

ψ0,0
regular/scalar
(cid:76)1
irreps ≤ 1
(cid:76)3
irreps ≤ 3
(cid:76)5
irreps ≤ 5
(cid:76)7
irreps ≤ 7
C-irreps ≤ 1 (cid:76)1
C-irreps ≤ 3 (cid:76)3
C-irreps ≤ 5 (cid:76)5
C-irreps ≤ 7 (cid:76)7

i=0 ψi
i=0 ψi
i=0 ψi
i=0 ψi
i=0 ψC
i=0 ψC
i=0 ψC
i=0 ψC

i

i

i

i

SO(2)

irreps ≤ 3

(cid:76)3

i=0 ψi

ψ0,0
irreps = 0
ψ0,0 ⊕ ψ1,0 ⊕ 2ψ1,1
irreps ≤ 1
ψ0,0 ⊕ ψ1,0
irreps ≤ 3
ψ0,0 ⊕ ψ1,0
irreps ≤ 5
irreps ≤ 7
ψ0,0 ⊕ ψ1,0
Ind-irreps ≤ 1 Ind ψSO(2)
Ind-irreps ≤ 3 Ind ψSO(2)
Ind-irreps ≤ 5 Ind ψSO(2)
Ind-irreps ≤ 7 Ind ψSO(2)

(cid:76)3
i=1 2ψ1,i
(cid:76)5
i=1 2ψ1,i
(cid:76)7
i=1 2ψ1,i
⊕ Ind ψSO(2)
(cid:76)3
(cid:76)5
(cid:76)7

i=1 Ind ψSO(2)
i=1 Ind ψSO(2)
i=1 Ind ψSO(2)

0

0

1

0

0

i

i

i

O(2)

ELU, norm-ReLU

conv2triv

ELU, squash
ELU, norm-ReLU
ELU, shared norm-ReLU
shared norm-ReLU
ELU, gate
ELU, shared gate
ELU, gate
ELU, shared gate

ELU

norm

conv2triv

norm

-

ELU, norm-ReLU

O(2)-conv2triv

ELU, Ind norm-ReLU

Ind-conv2triv

[7, 9]
[7, 9]
-

5.19 ± 0.08
3.29 ± 0.07
2.87 ± 0.04
[6, 1, 7, 9, 10] 2.40 ± 0.05
2.08 ± 0.03
1.96 ± 0.04
1.95 ± 0.07
1.93 ± 0.04
1.95 ± 0.05

[8]
[7, 9]
[7]
[7, 9]
[7]

[1]
-
-
-
-

2.43 ± 0.05
2.03 ± 0.05
2.04 ± 0.04
2.00 ± 0.01
2.01 ± 0.05

-
-
-
[6, 1, 38]
[8]
-
-
-
-

-

-
-
-
-
[12]
[12]
-
-
-
-
-
-
-
-
-
-

-
-
-
-
-

-
-
-
-
-
-
-
-

3.40 ± 0.07
2.42 ± 0.07
2.17 ± 0.06
1.88 ± 0.04
1.77 ± 0.06
1.68 ± 0.06
1.66 ± 0.05
1.62 ± 0.04
1.64 ± 0.06

2.98 ± 0.04
3.02 ± 0.18
3.24 ± 0.05
3.30 ± 0.11
3.39 ± 0.10
3.48 ± 0.16
3.59 ± 0.08
3.64 ± 0.12
3.10 ± 0.09
3.23 ± 0.08
2.88 ± 0.11
3.61 ± 0.09
2.37 ± 0.06
2.33 ± 0.06
2.23 ± 0.09
2.20 ± 0.06

5.46 ± 0.46
3.31 ± 0.17
3.42 ± 0.03
3.59 ± 0.13
3.84 ± 0.25

2.72 ± 0.05
2.66 ± 0.07
2.71 ± 0.11
2.80 ± 0.12
2.39 ± 0.05
2.21 ± 0.09
2.13 ± 0.04
1.96 ± 0.06

2.48 ± 0.13
1.32 ± 0.02
1.19 ± 0.06
1.02 ± 0.03
0.89 ± 0.03
0.84 ± 0.02
0.80 ± 0.03
0.82 ± 0.02
0.83 ± 0.05

1.03 ± 0.05
0.84 ± 0.05
0.81 ± 0.02
0.86 ± 0.04
0.83 ± 0.03

3.44 ± 0.10
2.39 ± 0.04
2.15 ± 0.05
1.87 ± 0.04
1.77 ± 0.04
1.73 ± 0.03
1.65 ± 0.05
1.65 ± 0.02
1.62 ± 0.05

1.38 ± 0.09
1.38 ± 0.09
1.44 ± 0.10
1.51 ± 0.10
1.47 ± 0.06
1.51 ± 0.05
1.59 ± 0.05
1.61 ± 0.06
1.41 ± 0.04
1.38 ± 0.08
1.15 ± 0.06
1.57 ± 0.05
1.09 ± 0.03
1.11 ± 0.03
1.04 ± 0.04
1.01 ± 0.03

5.21 ± 0.29
3.37 ± 0.18
3.41 ± 0.10
3.78 ± 0.31
3.90 ± 0.18

2.70 ± 0.11
2.65 ± 0.12
2.84 ± 0.10
2.85 ± 0.06
2.38 ± 0.07
2.24 ± 0.06
2.09 ± 0.05
1.95 ± 0.05

0.82 ± 0.01
0.87 ± 0.04
0.80 ± 0.03
0.99 ± 0.03
0.84 ± 0.02
0.89 ± 0.03
0.89 ± 0.03
0.95 ± 0.04
0.94 ± 0.06

1.01 ± 0.03
0.91 ± 0.02
0.95 ± 0.02
0.98 ± 0.04
0.96 ± 0.04

0.98 ± 0.03
1.05 ± 0.03
0.94 ± 0.02
1.69 ± 0.03
1.00 ± 0.03
1.64 ± 0.02
1.67 ± 0.01
1.68 ± 0.04
1.69 ± 0.03

1.29 ± 0.05
1.27 ± 0.03
1.36 ± 0.04
1.40 ± 0.07
1.42 ± 0.04
1.53 ± 0.07
1.55 ± 0.06
1.62 ± 0.03
1.46 ± 0.05
1.33 ± 0.03
1.18 ± 0.03
1.88 ± 0.05
1.10 ± 0.02
1.12 ± 0.04
1.05 ± 0.06
1.03 ± 0.03

3.98 ± 0.04
3.05 ± 0.09
3.86 ± 0.09
4.17 ± 0.15
4.57 ± 0.27

2.39 ± 0.07
2.25 ± 0.06
2.39 ± 0.09
2.25 ± 0.08
2.28 ± 0.07
2.15 ± 0.03
2.05 ± 0.05
1.85 ± 0.07

irreps ≤ 3

ψ0,0 ⊕ ψ1,0

(cid:76)3

i=1 2ψ1,i

ELU, gate

Ind-irreps ≤ 3 Ind ψSO(2)

0

(cid:76)3

i=1 Ind ψSO(2)

i

ELU, Ind gate

O(2)-conv2triv
norm
Ind-conv2triv
Ind-norm

Table 3: Extensive comparison of G-steerable CNNs for different choices of groups G, representations, nonlinearities and ﬁnal G-invariant
maps on three transformed MNIST datasets. Multiplicities of representations are reported in relative terms; the actual multiplicities are
integer multiples with a depth dependent factor. All models apply a G-invariant map after the convolutions to guarantee an invariant
prediction. Citations give credit to the works which proposed the corresponding model design. For reference see Sections 2.6, 3.1, C and I.

of the CN and DN models, notice that some of the digits are approximately related by symmetry
transformations11. If these transformations happen to be part of the equivariance group w.r.t. which
the model is invariant the predictions are more likely to be confused. This is mostly the case for N
being a multiple of 2 or 4 or for large orders N , which include almost all orientations. Once again,
the restricted models, here DN |5{e} and CN |5{e}, show the best results since they exploit local
symmetries but preserve information on the global orientation. Since the restricted dihedral model
generalizes over local reﬂections, its performance is consistently better than that of the restricted
cyclic model.

Quotient representations: As an alternative to regular representations we experiment with some
mixtures of quotient representations of CN (rows 11-15). These models differ from the regular models
by enforcing more symmetries in the feature ﬁelds and thus kernels. The individual feature ﬁelds are
lower dimensional; however, by ﬁxing the number of parameters, the models use more different ﬁelds
which in this speciﬁc case leads to approximately the same number of channels and therefore compute
and memory requirements. We do not observe any signiﬁcant difference in performance between
regular and quotient representations. Appendix C gives more intuition on our speciﬁc choices of
quotient representations and which symmetries they enforce. Note that the space of possible quotient
representations and their multiplicities is very large and still needs to be investigated more thoroughly.

Group pooling and vector ﬁeld nonlinearities: For C16 we implement a group pooling network
(row 16) and a vector ﬁeld network (row 17). These models map regular feature ﬁelds, produced by
each convolutional layer, to scalar ﬁelds and vector ﬁelds, respectively; see Section 2.6. These pooling
operations compress the features in the regular ﬁelds, which can lead to lower memory and compute
requirements. However, since we ﬁx the number of parameters, the resulting models are ultimately
much wider than the corresponding regular steerable CNNs. Since the pooling operations lead to a
loss of information, both models perform worse than their purely regular counterpart on MNIST O(2)
and MNIST rot. Surprisingly, the group pooling network, whose features are orientation unaware,
performs better than the vector ﬁeld network. On MNIST 12k the group pooling network closes up
with the regular steerable CNN while the vector ﬁeld network achieves an even better result. We
further experiment with a model which applies vector ﬁeld nonlinearities to only half of the regular
ﬁelds and preserves the other half (row 18). This model is on par with the regular model on both
transformed MNIST versions but achieves the overall best result on MNIST 12k. Similar to the case
of C16, the group pooling network for D16 (row 28) performs worse than the corresponding regular
model, this time also on MNIST 12k.

SO(2) irrep models: The feature ﬁelds of all SO(2)-equivariant models which we consider are
deﬁned to transform under irreducible representations; see Appendix B and F.2. Note that this covers
scalar ﬁelds and vector ﬁelds which transform under ψSO(2)
, respectively. Overall these
models are not competitive compared to the regular steerable CNNs. This result is particularly impor-
tant for SE(3) ∼= (R3, +) (cid:111) SO(3)-equivariant CNNs whose feature ﬁelds are often transforming
under the irreps of SO(3) [39, 2, 33, 20, 34].

and ψSO(2)
1

0

The models in rows 29-32 are inspired by Harmonic Networks [12] and consist of irrep ﬁelds with the
same multiplicity up to a certain threshold. All models apply ELUs on scalar ﬁelds and norm-ReLUs
(see Section 2.6) on higher order ﬁelds. The projection to invariant features is done via a convolution
to scalar features (conv2triv) in the last convolutional layer. We ﬁnd that irrep ﬁelds up to order 1
and 3 perform equally well while higher thresholds yield worse results. The original implementation
of Harmonic Networks considered complex irreps of SO(2) which results in a lower dimensional
steerable kernel basis as discussed in Appendix F.5. We reimplemented these models and found that
their reduced kernel space leads to consistently worse results (rows 33-36).

For the model containing irreps up to order 3 we implemented some alternative variants. For instance,
the model in row 38 does not convolve to trivial features in the last layer but computes these by taking
the norms of all non-scalar ﬁelds. This does not lead to signiﬁcantly different results. Appendix I
discusses all variations in detail.

11 E.g. 9 and 6 (6 and 9) or 5 and 2 (2 and 5) are related by a rotations by π and might therefore be confused
(4 and 7) are related by a reﬂection and a rotation by

and

by all models C2k and D2k for k ∈ N. Similarly,
π/2 and might be confused by all models D4k.

12

restriction depth

MNIST rot

MNIST 12k

group

test error (%)

group

test error (%)

group

test error (%)

C16

{e}

0.82 ± 0.01

{e}

0.82 ± 0.01

(0)

1
2
3
4
5

0.82 ± 0.02

0.86 ± 0.05
0.82 ± 0.03
0.77 ± 0.03
0.79 ± 0.03
0.78 ± 0.04

1.65 ± 0.02

D16, C16

D16, {e}

C16, {e}

0.79 ± 0.03
0.74 ± 0.03
0.73 ± 0.03
0.72 ± 0.02
0.68 ± 0.04

1.68 ± 0.04

0.80 ± 0.03
0.77 ± 0.03
0.76 ± 0.03
0.77 ± 0.03
0.75 ± 0.02

0.95 ± 0.04

no restriction

D16

D16

C16

Table 4: Effect of the group restriction operation at different depths of the network on MNIST rot and MNIST 12k.
Before restriction, the models are equivariant to a larger symmetry group than the group of global symmetries of
the corresponding dataset. A restriction at larger depths leads to an improved accuracy. All restricted models
perform better than non-restricted, and hence globally invariant, models.

By far the best results are achieved by the models in rows 41-44, which replace the norm-ReLUs
with gated nonlinearities, see Section 2.6. This observation is in line with the results presented in [2],
where gated nonlinearities were proposed.

O(2) models: As for SO(2), we are investigating O(2)-equivariant models whose features trans-
form under irreps up to a certain order and apply norm-ReLUs (rows 46-49). In this case we choose
twice the multiplicity of 2-dimensional ﬁelds than scalar ﬁelds, which reﬂects the multiplicity of irreps
contained in the regular representation of O(2). Invariant predictions are computed by convolving in
equal proportion to ﬁelds which transform under trivial irreps ψO(2)
(see
0,0
Appendix F.2), followed by taking the absolute value of the latter (O(2)-conv2triv). We again ﬁnd
that higher irrep thresholds yield worse results, this time already starting from order 1. In particular,
these models perform worse than their SO(2)-equivariant counterparts even on MNIST O(2). This
suggests that the kernel constraint for this particular choice of representations is too restrictive.
If only scalar ﬁelds, corresponding to the trivial irrep ψO(2)
0,0 , are chosen, the kernel constraint becomes
k(gx) = k(x) ∀g ∈ O(2) and therefore allows for isotropic kernels only. This limits the expressivity
of the model so severely that it performs even worse than a conventional CNN on MNIST rot and
MNIST 12k while being on par for MNIST O(2), see row 45. Note that isotropic kernels correspond
to vanilla graph convolutional networks (cf. the results and discussion in [5]).

and sign-ﬂip irreps ψO(2)
1,0

k

SO(2) ψSO(2)

In order to improve the performance of O(2)-steerable CNNs, we propose to use representations
IndO(2)
, which are induced from the irreps of SO(2) (see Appendix B for more details on
induction). By the deﬁnition of induction, this leads to pairs of ﬁelds which transform according
to ψSO(2)
under rotations but permute under reﬂections. The multiplicity of the irreps of O(2)
k
contained in this induced representation coincides with the multiplicities chosen in the pure O(2)
irrep models. However, the change of basis, relating both representations, does not commute with the
nonlinearities, such that the networks behave differently. We apply Ind norm-ReLU nonlinearities to
the induced O(2) models which compute the norm of each of the permuting subﬁelds individually
but share the norm-ReLU parameters (the bias) to guarantee equivariance. In order to project to ﬁnal,
invariant features, we ﬁrst apply a convolution producing IndO(2)
ﬁelds (Ind-conv2triv).
Since these transform like the regular representation of ({±1}, ∗) ∼= O(2)/ SO(2), we can simply
apply G-pooling over the two reﬂections. The results, given in rows 50-53, show that these models
perform signiﬁcantly better than the O(2) irreps models and outperform the SO(2) irrep models on
MNIST O(2). More speciﬁc details on all induced O(2) model operations are given in Appendix I.

SO(2) ψSO(2)

0

We again build models which apply gated nonlinearities. As for SO(2), this leads to a greatly
improved performance of the pure irrep models, see rows 54-55. In addition we adapt the gated
nonlinearity to the induced irrep models (rows 56-57). Here we apply an independent gate to each of
the two permuting sub-ﬁelds (Ind gate). In order to be equivariant, the gates need to permute under
reﬂections as well, which is easily achieved by deriving them from IndO(2)
ﬁelds instead
of scalar ﬁelds. The gated induced irrep model achieves the best results among all O(2)-steerable
networks, however, it is still not competitive compared to the DN models with large N .

SO(2) ψSO(2)

0

13

Figure 3: Validation errors and losses during the training of a conventional CNN and CN -equivariant
models on MNIST rot. Networks with higher levels of equivariance converge signiﬁcantly faster.

3.2 MNIST group restriction experiments

All transformed MNIST datasets clearly show local rotational and reﬂectional symmetries but differ
in the level of symmetry present at the global scale. While the DN and O(2)-equivariant models in the
last section could exploit these local symmetries, their global invariance leads to a considerable loss of
information. On the other hand, models which are equivariant to the symmetries present at the global
scale of the dataset only are not able to generalize over all local symmetries. The proposed group
restriction operation allows for models which are locally equivariant but are globally invariant only to
the level of symmetry present in the data. Table 4 reports the results of models which are restricted
after different layers. Speciﬁcally, on MNIST rot the D16-equivariant model introduced in the last
section is restricted to C16, while we restrict the D16 and the C16 model to the trivial group {e}
on MNIST 12k. The overall trend is that a restriction at later stages of the model improves the
performance. All restricted models perform signiﬁcantly better than the invariant models. Figure 2
shows that this behavior is consistent for different orders N .

3.3 On the convergence of Steerable CNNs

In our experiments we ﬁnd that steerable CNNs converge signiﬁcantly faster than non-equivariant
CNNs. Figure 3 shows this behavior for the regular CN -steerable CNNs from Section 3.1 in
comparison to a conventional CNN, corresponding to N = 1, on MNIST rot. The rate of convergence
thereby increases with the order N and, as already observed in Figure 2, saturates at approximately
N = 8. All models share approximately the same number of parameters.

The faster convergence of equivariant networks is explained by the fact that they generalize over
G-transformed images by design which reduces the amount of intra-class variability which they
have to learn12. In contrast, a conventional CNN has to learn to classify all transformed versions of
each image explicitly which requires either an increased batch size or more training iterations. The
enhanced data efﬁciency of E(2)-steerable CNNs can therefore lead to a reduced training time.

3.4 Competitive MNIST rot experiments

As a ﬁnal experiment on MNIST rot we are replicating the regular C16 model from [7] which was
the previous SOTA. It is mostly similar to the models evaluated in the previous sections but is wider,
uses larger kernel sizes and adds additional fully connected layers; see Table 14 in the Appendix. As
reported in Table 5, our reimplementation matches the accuracy of the original model. Replacing
the regular feature ﬁelds with the quotient representations used in Section 3.1 leads to slightly better
results. We refer to Appendix C for more insights on the improved performance of the quotient model.
A further signiﬁcant improvement and a new state of the art is being achieved by a D16-equivariant
model, which is restricted to C16 after the penultimate layer.

12 Mathematically, G-steerable CNNs classify equivalence classes of images deﬁned by the equivalence
relation f ∼ f (cid:48) ⇔ ∃ tg ∈ (R2, +) (cid:111) G s.t. f (x) = f (cid:0)g−1(x − t)(cid:1). Instead, MLPs learn to classify each
image individually and conventional CNNs classify equivalence classes deﬁned by translations, i.e. above
equivalence classes for G = {e}. For more details see Section 2 of [7].

14

model

group

representation test error (%)

model

CIFAR-10 CIFAR-100

[6]
[6]
[12]
[40]
[13]
Ours
[7]
Ours
Ours D16|5C16

C4
C4
SO(2)
-
C17
C16
C16
C16

regular/scalar
regular
irreducible
-
regular/vector 1.09
regular
regular
quotient
regular

3.21 ± 0.0012
2.28 ± 0.0004
1.69
1.2

0.716 ± 0.028
0.714 ± 0.022
0.705 ± 0.025
0.682 ± 0.022

[41]

wrn28/10
wrn28/10 D1 D1 D1
wrn28/10* D8 D4 D1
wrn28/10 C8 C4 C1
wrn28/10 D8 D4 D1
wrn28/10 D8 D4 D4

3.87
3.36 ± 0.08
3.28 ± 0.10
3.20 ± 0.04
3.13 ± 0.17
2.91 ± 0.13

[42]

AA 2.6 ± 0.1
wrn28/10
wrn28/10* D8 D4 D1 AA 2.39 ± 0.11
wrn28/10 D8 D4 D1 AA 2.05 ± 0.03

18.80
17.97 ± 0.11
17.42 ± 0.33
16.47 ± 0.22
16.76 ± 0.40
16.22 ± 0.31

17.1 ± 0.3
15.55 ± 0.13
14.30 ± 0.09

Table 5: Final runs on MNIST rot

Table 6: Test errors on CIFAR (AA=autoaugment)

3.5 CIFAR experiments

The statistics of natural images are typically invariant under global translations and reﬂections but are
not under global rotations. Here we are investigating the practical beneﬁt of G-steerable convolutions
for such images by classifying CIFAR-10 and CIFAR-100. For this purpose we implement several
DN and CN -equivariant versions of WideResNet [41]. Different levels of equivariance, stated in
the model speciﬁcations in Table 6, are thereby used in the three main blocks of the network (i.e.
between pooling layers). Regular representations are used throughout the whole model except for the
last convolution which maps to a scalar ﬁeld to produce invariant predictions. For a fair comparison
we scale the width of all layers such that the number of parameters of the original wrn28/10 model
is approximately preserved. Note that, due to their enhanced parameter efﬁciency, our models
become wider than conventional CNNs. Since this implies a higher computational cost, we add an
equivariant model, marked by an additional *, which has about the same number of channels as the
non-equivariant wrn28/10. For rotation order N = 8 we are further using 5 × 5 kernels to mitigate
the discretization artifacts of steering 3 × 3 kernels by 45 degrees. All runs use the same training
procedure as reported in [41] and Appendix H.3. We want to emphasize in particular that we perform
no further hyperparameter tuning.

The results of the D1 D1 D1 model in Table 6 conﬁrm that incorporating the global symmetries of the
data already yields a signiﬁcant boost in accuracy. Interestingly, the C8 C4 C1 model, which is purely
rotation but not reﬂection-equivariant, achieves better results, which shows that it is worthwhile to
leverage local rotational symmetries. Both symmetries are respected simultaneously by the wrn28/10
D8 D4 D1 model. While this model performs better than the two previous ones on CIFAR-10, it
surprisingly yields slightly worse result on CIFAR-100. This might be due to the higher dimensionality
of its feature ﬁelds which, despite the model having more channels in total, leads to less independent
ﬁelds. The best results (without using auto augment) are obtained by the D8 D4 D4 model which
suggests that rotational symmetries are useful even on a larger scale. The small wrn28/10* D8 D4 D1
model shows a remarkable gain compared to the non-equivariant wrn28/10 baseline despite not
being computationally more expensive. To investigate whether equivariance is useful even when
a powerful data augmentation policy is available, we further rerun both D8 D4 D1 models with
AutoAugment (AA) [42]. As without AA, both the computationally cheap wrn28/10* model and the
wider wrn28/10 version outperform the wrn28/10 baseline by a large margin.

3.6 STL-10 experiments

In order to test whether the previous results general-
ize to natural images of higher resolution we run addi-
tional experiments on STL-10 [44]. While this dataset
was originally intended for semi-supervised learning
tasks, its 5000 training images are also being used for
supervised classiﬁcation in the low data regime [43].
We adapt the experiments in [43] by replacing the non-
equivariant convolutions of their wrn16/8 model, which
was the previous supervised SOTA, with DN -steerable
convolutions. As in the CIFAR experiments, all inter-
mediate features transform according to regular repre-
sentations. A ﬁnal, invariant prediction is generated
via a convolution to scalar ﬁelds. We are again using

15

model

group

#params test error (%)

wrn16/8 [43]
wrn16/8*
wrn16/8
wrn16/8*
wrn16/8

-
D1 D1 D1
D1 D1 D1
D8 D4 D1
D8 D4 D1

12.74±0.23
11M
11.05±0.45
5M
11.17±0.60
10M
4.2M 10.57±0.70
9.80±0.40
12M

Table 7: Test errors of different equivariant
models on the STL-10 dataset. Models with *
are not scaled to the same number of parameters
as the original model but preserve the number
of channels of the baseline.

steerable convolutions as a mere drop-in replacement, that is, we use the same training setting
and hyperparameters as in the original paper. The four adapted models, reported in Table 7, are
equivariant under either the action of D1 in all blocks or the actions of D8, D4 and D1 in the
respective blocks. For both choices we build a large model, whose width is scaled up to approx-
imately match the number of parameters of the baseline, and a small model, which preserves the
number of channels and thus compute and memory requirements, but is more parameter efﬁcient.
As expected, all models improve signiﬁcantly over
the baseline with larger models outperforming
smaller ones. However, due to their extended equiv-
ariance, the small D8 D4 D1 model performs better
than the large D1 D1 D1 model. In comparison to the
CIFAR experiments, rotational equivariance seems
to give a more signiﬁcant boost in accuracy. This is
expected since the higher resolution of 96 × 96 pixels
of the STL-10 images allows for more detailed local
patterns which occur in arbitrary orientations.

Figure 4 reports the results of a data ablation study
which investigates the performance of the D8 D4 D1
models for smaller training set sizes. The results
validate that the gains from incorporating equivari-
ance are consistent over all training sets. More infor-
mation on the exact training procedures is given in
Appendix H.4.

4 Conclusions

Figure 4: Data ablation study on STL-10.
The equivariant models yield signiﬁcantly im-
proved results on all dataset sizes.

A wide range of rotation- and reﬂection-equivariant models has been proposed in the recent years.
In this work we presented a general theory of E(2)-equivariant steerable CNNs which describes
many of these models in one uniﬁed framework. By analytically solving the kernel constraint for
any representation of O(2) or its subgroups we were able to reproduce and systematically compare
these models. We further proposed a group restriction operation which allows us to adapt the level of
equivariance to the symmetries present on the corresponding length scale. When using G-steerable
convolutions as drop in replacement for conventional convolution layers we obtained signiﬁcant
improvements on CIFAR-10, CIFAR-100 and STL-10 without additional hyperparameter tuning.
While the kernel expansion leads to a small overhead during train time, the ﬁnal kernels can be stored
such that during test time steerable CNNs are computationally not more expensive than conventional
CNNs of the same width. Due to the enhanced parameter efﬁciency of equivariant models it is a
common practice to adapt the model width to match the parameter cost of conventional CNNs. Our
results show that even non-scaled models outperform conventional CNNs in accuracy. Since steerable
CNNs converge faster than non-equivariant CNNs, they can even be cheaper to train.

We believe that equivariant CNNs will in the long term become the default choice for tasks like
biomedical imaging, where symmetries are present on a global scale. The impressive results on
natural images demonstrate the great potential of applying E(2)-steerable CNNs to more general
vision tasks which involve only local symmetries. Future research still needs to investigate the wide
range of design choices of steerable CNNs in more depth and collect evidence on whether our ﬁndings
generalize to different settings. We hope that our library13 will help equivariant CNNs to be adopted
by the community and facilitate further research.

Acknowledgments

We would like to thank Taco Cohen for fruitful discussions on an efﬁcient implementation and helpful feedback
on the paper and Daniel Worrall for elaborating on the real valued implementation of Harmonic Networks.

13The library is available at https://github.com/QUVA-Lab/e2cnn.

16

General E(2) - Equivariant Steerable CNNs
Appendix

A Local gauge equivariance of E(2)-steerable CNNs

local
coord. trafo

(passive)

active trafo

global
coord. trafo
(passive)

Figure 5: Different viewpoints on transformations of signals on R2.
Top Left: In our work we considered active rotations of the sig-
nal while keeping the coordinate frames ﬁxed. Bottom Left: The
equivalent, passive interpretation views the transformation as a
global rotation of reference frames (a global gauge transformation).
Right: Local gauge transformations rotate reference frames inde-
pendently from each other. E(2)-steerable CNNs are equivariant
w.r.t. both global and local gauge transformations.

The E(2)-equivariant steerable CNNs considered in this work were derived in the classical framework
of steerable CNNs on Euclidean spaces Rd (or more general homogeneous spaces) [1, 2, 3, 4].
This formulation considers active transformations of signals, in our case translations, rotations and
reﬂections of images. Speciﬁcally, an active transformation by a group element tg ∈ (R2, +) (cid:111) G
moves signal values from x to g−1(x − t); see Eq. (1) and Figure 5, top left. The proven equivariance
properties of the proposed E(2)-equivariant steerable CNNs guarantee the speciﬁed transformation
behavior of the feature spaces under such active transformations. However, our derivations so far
don’t prove any equivariance guarantees for local, independent rotations or reﬂections of small
patches in an image.

The appropriate framework for analyzing local transformations is given by Gauge Equivariant
In contrast to active transformations, Gauge Equivariant CNNs consider
Steerable CNNs [5].
passive gauge transformations; see Figure 5, right. Adapted to our speciﬁc setting, each feature
vector f (x) is being expressed relative to a local reference frame (or gauge) (cid:0)e1(x), e2(x)(cid:1) at
x ∈ R2. A gauge transformation formalizes a change of local reference frames by the action of
position dependent elements g(x) of the gauge group (or structure group), in our case rotations and
reﬂections in G ≤ O(2). Since gauge transformations act independently on each position, they model
independent transformation of local patches in an image. As derived in [5], the demand for local
gauge equivariance results in the same kernel constraint as in Eq. (2). This implies that our models
are automatically locally gauge equivariant14.

More generally, the kernel constraint (2) applies to arbitrary 2-dimensional Riemannian manifolds
M with structure groups G ≤ O(2). The presented solutions of the kernel space constraint therefore
describe spherical CNNs [19, 5, 20, 21, 22, 23] or convolutional networks on triangulated meshes
[24, 25, 26, 27] for different choices of structure groups and group representations.

14 Conversely, the equivariance under local gauge transformations g(x) ∈ O(2) implies the equivariance under
active isometries. In the case of the Euclidean space R2 these isometries are given by the Euclidean group E(2).

B A short primer on group representation theory

Linear group representations model abstract algebraic group elements via their action on some vector
space, that is, by representing them as linear transformations (matrices) on that space. Representation
theory forms the backbone of Steerable CNNs since it describes the transformation law of their
feature spaces. It is furthermore widely used to describe ﬁelds and their transformation behavior in
physics.
Formally, a linear representation ρ of a group G on a vector space (representation space) Rn is a
group homomorphism from G to the general linear group GL(Rn) (the group of invertible n × n
matrices), i.e. it is a map

ρ : G → GL(Rn)

such that ρ(g1g2) = ρ(g1)ρ(g2) ∀g1, g2 ∈ G .

The requirement to be a homomorphism, i.e. to satisfy ρ(g1g2) = ρ(g1)ρ(g2), ensures the compati-
bility of the matrix multiplication ρ(g1)ρ(g2) with the group composition g1g2 which is necessary for
a well deﬁned group action. Note that group representations do not need to model the group faithfully
(which would be the case for an isomorphism instead of a homomorphism).
A simple example is the trivial representation ρ : G → GL(R) which maps any group element to the
(cid:21)
(cid:20)cos (θ) (cid:57) sin (θ)
cos (θ)
sin (θ)
are an example of a representation of SO(2) (whose elements are identiﬁed by a rotation angle θ).

identity, i.e. ∀g ∈ G ρ(g) = 1. The 2-dimensional rotation matrices ψ(θ) =

Equivalent representations Two representations ρ and ρ(cid:48) on Rn are called equivalent iff they
are related by a change of basis Q ∈ GL(Rn), i.e. ρ(cid:48)(g) = Qρ(g)Q−1 for each g ∈ G. Equiv-
alent representations behave similarly since their composition is basis independent as seen by
ρ(cid:48)(g1)ρ(cid:48)(g2) = Qρ(g1)Q−1Qρ(g2)Q−1 = Qρ(g1)ρ(g2)Q−1.

Direct sums Two representations can be combined by taking their direct sum. Given representations
ρ1 : G → GL(Rn) and ρ2 : G → GL(Rm), their direct sum ρ1 ⊕ρ2 : G → GL(Rn+m) is deﬁned as

(ρ1 ⊕ ρ2)(g) =

(cid:20)ρ1(g)
0

(cid:21)
0
ρ2(g)

,

i.e. as the direct sum of the corresponding matrices. Its action is therefore given by the independent
actions of ρ1 and ρ2 on the orthogonal subspaces Rn and Rm in Rn+m. The direct sum admits an
obvious generalization to an arbitrary number of representations ρi:
(cid:77)
i

ρi(g) = ρ1(g) ⊕ ρ2(g) ⊕ . . .

Irreducible representations The action of a representation might leave a subspace of the represen-
tation space invariant. If this is the case there exists a change of basis to an equivalent representation
which is decomposed into the direct sum of two independent representations on the invariant subspace
and its orthogonal complement. A representation is called irreducible if no non-trivial invariant
subspace exists.
Any representation ρ : G → Rn of a compact group G can therefore be decomposed as

ρ(g) = Q

(cid:104)(cid:77)

(cid:105)
ψi(g)

Q−1

i∈I

where I is an index set specifying the irreducible representations ψi contained in ρ and Q is a change
of basis. In proofs it is therefore often sufﬁcient to consider irreducible representations which we use
in Section 2.4 to solve the kernel constraint.

Regular and quotient representations A commonly used representation in equivariant deep
learning is the regular representation. The regular representation of a ﬁnite group G acts on a vector
space R|G| by permuting its axes. Speciﬁcally, associating each axis eg of R|G| to an element g ∈ G,
the representation of an element ˜g ∈ G is a permutation matrix which maps eg to e˜gg. For instance,
the regular representation of the group C4 with elements {p π

2 |p = 0, . . . , 3} is instantiated by:

18

φ

0








1 0
0 1
0 0
0 0

0
0
1
0








0
0
0
1


0

1


0

0

π
2

0
0
1
0

0
0
0
1








1
0
0
0


0

0


1

0

π

0
0
0
1

1
0
0
0

3π
2

1
0
0
0

0
1
0
0








0
0
1
0








0
1
0
0


0

0


0

1

ρC4
reg (φ)

g vgeg in R|G| can be interpreted as a scalar function v : G → R, g (cid:55)→ vg on G.
g vgehg = (cid:80)
˜g vh−1 ˜ge˜g the regular representation corresponds to a left translation

A vector v = (cid:80)
Since ρ(h)v = (cid:80)
[ρ(h)v](g) = vh−1g of such functions.
Very similarly, the quotient representation ρG/H
quot of G w.r.t. a subgroup H acts on R|G|/|H| by
permuting its axes. Labeling the axes by the cosets gH in the quotient space G/H, it can be deﬁned
via its action ρG/H
quot (˜g)egH = e˜ggH . An intuitive explanation of quotient representations is given in
Appendix C.

Regular and trivial representations are two speciﬁc cases of quotient representations obtained by
choosing H = {e} or H = G, respectively. Vectors in the representation space R|G|/|H| can be
viewed as scalar functions on the quotient space G/H. The action of the quotient representations on
v then corresponds to a left translation of these functions on G/H.

Restricted representations Any representation ρ : G → GL(Rn) can be uniquely restricted to a
representation of a subgroup H of G by restricting its domain of deﬁnition:

ResG

H (ρ) : H → GL(Rn), h (cid:55)→ ρ(cid:12)

(cid:12)H (h)

Instead of restricting a representation from a group G to a subgroup
Induced Representations
H ≤ G, it is also possible to induce a representation of H to a representation of G. In order to keep
the presentation accessible we will ﬁrst only consider the case of ﬁnite groups G and H.
Let ρ : H → GL(Rn) be any representation of a subgroup H of G. The induced representation
IndG
H (ρ) is then deﬁned on the representation space Rn|G|/|H| which can be seen as one copy of
Rn for each of the |G|/|H| cosets gH in the quotient set G/H. For the deﬁnition of the induced
representation it is customary to view this space as the tensor product R|G|/|H| ⊗ Rn and to write
vectors in this space as15

w =

(cid:88)

gH

egH ⊗ wgH ∈ Rn |G|
|H| ,

where egH is a basis vector of R|G|/|H|, associated to the coset gH, and wgH is some vector in the
representation space Rn of ρ. Intuitively, IndG
H (ρ) acts on Rn|G|/|H| by i) permuting the |G|/|H|
subspaces associated to the cosets gH and ii) acting on each of these subspaces via ρ.

To formalize this intuition, note that any element g ∈ G can be identiﬁed by the coset gH to
which it belongs and an element h(g) ∈ H which speciﬁes its position within this coset. Hereby
h : G → H expresses g relative to an arbitrary representative16 R(gH) ∈ G of gH and is deﬁned as
h(g) := R(gH)−1g from which it immediately follows that g is decomposed relative to R as

g = R(gH)h(g) .

The action of an element ˜g ∈ G on a coset gH ∈ G/H is naturally given by ˜ggH ∈ G/H. This action
deﬁnes the aforementioned permutation of the n-dimensional subspaces in Rn|G|/|H| by sending egH
in Eq. (7) to e˜ggH . Each of the n-dimensional, translated subspaces ˜ggH, is in addition transformed

15The vector can equivalently be expressed as w = (cid:76)

gH wgH , however, we want to make the tensor product

basis explicit.

16 Formally, a representative for each coset is chosen by a map R : G/H → G such that it projects back to
the same coset, i.e. R(gH)H = gH. This map is therefore a section of the principal bundle G π→ G/H with
ﬁbers isomorphic to H and the projection given by π(g) := gH.

(7)

(8)

19

by the action of ρ(cid:0)h(˜gR(gH))(cid:1). This H-component h(˜gR(gH)) = R(˜ggH)−1˜gR(gH) of the
˜g action within the cosets accounts for the relative choice of representatives R(˜ggH) and R(gH).
Overall, the action of IndG

H (ρ(˜g)) is given by

(cid:104)

IndG

H ρ

(cid:105)
(˜g)

(cid:88)

egH ⊗ wgH :=

e˜ggH ⊗ ρ(cid:0)h(˜gR(gH))(cid:1) wgH ,

(9)

which can be visualized as:

gH
















...

wgH
...
...
...































IndG

H ρ(˜g) ·

=

(cid:88)

gH

...
...
...

(cid:111)

gH
















(cid:111)

˜ggH = ˜gR(gH)H

ρ(h(˜gR(gH)))wgH
...

Both quotient representations and regular representations can be viewed as being induced from
trivial representations of a subgroup. Speciﬁcally, let ρ{e}
: {e} → GL(R) = {(+1)} be the
triv
trivial representation of the the trivial subgroup. Then IndG
triv : G → GL(R|G|) is the regular
representation which permutes the cosets g{e} of G/{e} ∼= G which are in one to one relation to the
triv : H → GL(R) = {(+1)} being the trivial representation of an
group elements themself. For ρH
arbitrary subgroup H of G, the induced representation IndG
triv : G → GL(R|G|/|H|) permutes the
cosets gH of H and thus coincides with the quotient representation ρG/H
quot

{e} ρ{e}

H ρH

.

Note that a vector in R|G|/|H| ⊗ Rn is in one-to-one correspondence to a function f : G/H → Rn.
The induced representation can therefore equivalently be deﬁned as acting on the space of such
functions as17

[IndG

H ρ(˜g) · f ](gH) = ρ(h(˜gR(˜g−1gH)))f (˜g−1gH) .

(10)

This deﬁnition generalizes to non-ﬁnite groups where the quotient space G/H is not necessarily
ﬁnite anymore.
For the special case of semidirect product groups G = N (cid:111) H it is possible to choose representatives
of the cosets gH such that the elements h(˜gR(g(cid:48)H)) = h(˜g) become independent of the cosets [3].
This simpliﬁes the action of the induced representation to

[IndG

H ρ(˜g) · f ](gH) = ρ(h(˜g)) f (˜g−1gH)

(11)

which corresponds to Eq. (1) for the group G = E(2) = (R2, +) (cid:111) O(2), subgroup H = O(2) and
quotient space G/H = E(2)/ O(2) = R2.

C An intuition for quotient representation ﬁelds

The quotient representations of CN in rows 11-15 of Table 3 and in Table 5 are all of the form ρCN/CM
with CM ≤ CN . By the deﬁnition of quotient representations, given in Section 2.6, this implies
features which are invariant under the action of CM . For instance, ρCN/C2
-ﬁelds encode features
like lines, which are invariant under rotations by π. Similarly, ρCN/C4
rotations by π/2, and therefore describe features like a cross. The N/M channels of a ρCN/CM
-ﬁeld
respond to different orientations of these patterns, e.g. to + and × for the two channels of ρC8/C4
A few more examples are given by the 16/2 = 8 channels of ρC16/C2
, which respond to the patterns

quot
features are invariant under

quot

quot

quot

quot

.

quot

−, − ,

−

, − ,−,−,− and −,

17 The rhs. of Eq. (9) corresponds to [IndG

H ρ(˜g) · f ](˜ggH) = ρ(h(˜gR(gH)))f (gH).

20

respectively, or the 16/4 = 4 channels of ρC16/C4

quot

+, + ,

, which respond to
+ and + .

In principle, each of these patterns18 could be encoded by a regular feature ﬁeld of CN . A regular ﬁeld
of type ρCN
reg comprises N instead of N/M channels, which detect arbitrary patterns in N orientations,
for instance,

,(cid:111),(cid:111), (cid:111),
for N = 8. In the case of CM -symmetric patterns, e.g. crosses for M = 4, this becomes
,×,×, ×,

(cid:111) , (cid:111)and (cid:111)

× ,×and ×.

×,

×

(cid:111), (cid:111)

As evident from this example, the repetition after N/M orientations (here 8/4 = 2), introduces a
redundancy in the responses of the regular feature ﬁelds. A quotient representation ρCN/CM
addresses
this redundancy by a-priori assuming the CM symmetry to be present and storing only the N/M
non-redundant responses. If symmetric patterns are important for the learning task, a quotient
representation can therefore save computational, memory and parameter cost.

quot

In our experiments we mostly used quotients by C2 and C4 since we assumed the corresponding
symmetric patterns ( | and +) to be most frequent in MNIST. As hypothesized, our model, which
C16/C4
quot ⊕ 4ψ0 of C16, improves slightly upon a purely
uses the representations 5ρreg ⊕ 2ρ
regular model with the same number of parameters, see Table 5. By mixing regular, quotient and
trivial19 representations, our model keeps a certain level of expressiveness in its feature ﬁelds but
incorporates a-priori known symmetries and compresses the model.

C16/C2
quot ⊕ 2ρ

We want to emphasize that quotient representations are expected to severely harm the model perfor-
mance if the assumed symmetry does not actually exist in the data or is unimportant for the inference.
Since the space of possible quotient representations and their multiplicities is very large, it might be
necessary to apply some form of neural architecture search to ﬁnd beneﬁcial combinations. As a
default choice we recommend the user to work with regular representations.
Further, note that the intuition given above is speciﬁc for the case of quotient representations ρG/N
quot
where N (cid:69) G is a normal subgroup (which is always the case for CN ). Since normal subgroups
imply gN = N g ∀g ∈ G by deﬁnition, the action of the quotient representation by any element
n ∈ N is given by ρG/N
quot (n)egN = engN = enN g = eN g = egN , that it, it describes N -invariant
feature ﬁelds. The quotient representations ρG/H
quot for general, potentially non-normal subgroups
H ≤ G also imply certain symmetries in the feature ﬁelds but are not necessarily H-invariant.
For instance, the quotient representation ρDN /CN
is invariant under rotations since CN is a normal
subgroup of DN
is not invariant
since ({±1}, ∗) is not a normal subgroup of DN . In the latter case one has instead

∼= CN (cid:111)({±1}, ∗), while the quotient representation ρDN /({±1},∗)

quot

quot

ρDN /({±1},∗)
quot

(s)er({±1},∗) = esr({±1},∗) =

(cid:26)er({±1},∗)

er−1s({±1},∗) = er−1({±1},∗)

for s = +1
for s = −1

for all s ∈ ({±1}, ∗) and representatives r ∈ CN . The feature ﬁelds are therefore not invariant under
the action of ({±1}, ∗) but become reversed.

D Equivariance of E(2) - steerable CNNs

D.1 Equivariance of E(2) - steerable convolutions

Assume two feature ﬁelds fin : R2 → Rcin of type ρin and fout : R2 → Rcout of type ρout to be given.
Under actions of the Euclidean group these ﬁelds transform as
Ind(R2,+)(cid:111)G

(cid:0)g−1(x − t)(cid:1)

:= ρin(g) fin

fin(x) (cid:55)→

(gt)fin

(x)

ρin

(cid:16)(cid:104)

(cid:17)

(cid:105)

G

fout(x) (cid:55)→

(cid:16)(cid:104)

Ind(R2,+)(cid:111)G

G

(cid:105)

(cid:17)

ρout

(gt)fout

(x) := ρout(g)fout

(cid:0)g−1(x − t)(cid:1) .

18Or more generally, any possible pattern.
19Trivial representations ψ0 ∼= ρG/G
which are invariant to the full group G.

quot can themself be seen as an extreme case of quotient representations

21

Here we show that the G-steerability (2) of convolution kernels is sufﬁcient to guarantee the equivari-
ance of the mapping. We therefore deﬁne the convolution (or correlation) operation of a feature ﬁeld
with a G-steerable kernel k : R2 → Rcout×cin as usual by

fout(x) := (k ∗ fin) (x) =

k(y)fin(x + y) dy .

(cid:90)

R2

The convolution with a transformed input ﬁeld then gives

(cid:90)

(cid:90)

(cid:90)

R2

R2

R2

=

=

dy k(y)

(cid:16)(cid:104)

Ind(R2,+)(cid:111)G

G

(cid:105)

(cid:17)

ρin

(gt) fin

(x + y)

dy k(y)ρin(g)fin

(cid:0)g−1(x + y − t)(cid:1)

dy ρout(g)k(g−1y)ρin(g)−1 ρin(g)fin

(cid:0)g−1(x + y − t)(cid:1)

(cid:90)

= ρout(g)

R2
= ρout(g)fout

d˜y k(˜y)fin
(cid:0)g−1(x − t)(cid:1)

(cid:0)g−1(x − t) + ˜y(cid:1)

(cid:16)(cid:104)

=

Ind(R2,+)(cid:111)G

G

(cid:105)

(cid:17)

ρout

(gt)fout

(x) ,

i.e. it satisﬁes the desired equivariance condition

(cid:16)(cid:104)

k ∗

Ind(R2,+)(cid:111)G

G

(cid:105)

(cid:17)

ρin

(gt)fin

=

(cid:104)
Ind(R2,+)(cid:111)G

G

(cid:105)

ρout

(gt)(cid:0)k ∗ fin

(cid:1) .

We used the kernel steerability (2) in the second step to identify k(x) with ρout(g)k(g−1x)ρin(g−1). In
(cid:17)(cid:12)
(cid:12)
the third step we substituted ˜y = g−1y which does not affect the integral measure since
(cid:12) =
|det(g)| = 1 for an orthogonal transformation g ∈ G.

(cid:12)
(cid:12)
(cid:12)det

(cid:16) ∂y
∂ ˜y

A proof showing the G-steerability of the kernel to not only be sufﬁcient but necessary is given in [2].

D.2 Equivariance of spatially localized nonlinearities

We consider nonlinearities of the form

σ : Rcin → Rcout , f (x) (cid:55)→ σ(cid:0)f (x)(cid:1) ,

which act spatially localized on feature vectors f (x) ∈ Rcin. These localized nonlinearities are used
to deﬁne nonlinearities ¯σ acting on entire feature ﬁelds f : R2 → Rcin by mapping each feature
vector individually, that is,

¯σ : f (cid:55)→ ¯σ(f )

such that

¯σ(f )(x) := σ(cid:0)f (x)(cid:1) .

In order for ¯σ to be equivariant under the action of induced representations it is sufﬁcient to require

σ ◦ ρin(g) = ρout(g) ◦ σ

∀g ∈ G

since then

(cid:16)(cid:104)

¯σ

Ind(R2,+)(cid:111)G

G

ρin

(cid:105)

(cid:17)

(gt)f

(x) = σ (cid:0)ρin(g)f (g−1(x − t))(cid:1)
= ρout(g)σ (cid:0)f (g−1(x − t))(cid:1)
= ρout(g)¯σ(cid:0)f (cid:1)(g−1(x − t))

(cid:104)

=

Ind(R2,+)(cid:111)G

G

ρout

(cid:105)

¯σ(f )(x) .

22

D.2.1 Equivariance of individual subspace nonlinearities w.r.t. direct sum representations

(cid:16)(cid:77)

σi

◦

(cid:16)(cid:77)
i

The feature spaces of steerable CNNs comprise multiple feature ﬁelds fi : R2 → Rcin,i which are
concatenated into one big feature ﬁeld f : R2 → R(cid:80)
i cin,i deﬁned by f := (cid:76)
i fi. By deﬁnition f (x)
transforms under ρin = (cid:76)
i ρin,i if each fi(x) transforms under ρin,i. If σi : Rcin,i → Rcout,i is an
equivariant nonlinearity satisfying σi ◦ ρin,i(g) = ρout,i(g) ◦ σi for all g ∈ G, then
(cid:17)

(cid:17)

(cid:17)

(cid:17)

(cid:16)(cid:77)

(cid:16)(cid:77)

ρin,i(g)

=

ρout,i(g)

◦

σi

∀g ∈ G,

i

i σi : R(cid:80)

i
i.e. the concatenation of feature ﬁelds respects the equivariance of the individual nonlinearities. Here
we deﬁned (cid:76)
i cout,i as acting individually on the corresponding fi(x) in f (x),
that is, ((cid:76)
To proof this statement consider without loss of generality the case of two feature ﬁelds f1 and
f2 with corresponding representations ρ1 and ρ2 and equivariant nonlinearities σ1 and σ2. Then it
follows for all g ∈ G that

i (σi ◦ proji) (f (x)).

i fi(x)) = (cid:76)

i cin,i → R(cid:80)

i σi) ((cid:76)

i

(cid:0)σ1 ⊕ σ2

(cid:1)(cid:0)(ρ1,in ⊕ ρ2,in)(g)(cid:1)(cid:0)f1 ⊕ f2

(cid:1)

= σ1 (ρ1(g) f1) ⊕ σ2 (ρ2(g) f2)
= ρ1(g) σ1(f1) ⊕ ρ2(g) σ2(f2)
= (cid:0)(ρ1,out ⊕ ρ2,out)(g)(cid:1)(cid:0)σ1 ⊕ σ2

(cid:1)(cid:0)f1 ⊕ f2

(cid:1) .

The general case follows by induction.

D.2.2 Equivariance of norm nonlinearities w.r.t. unitary representations
(cid:12)ρiso(g)f (x)(cid:12)
We deﬁne unitary representations to preserve the norm of feature vectors, i.e. (cid:12)
(cid:12) =
(cid:12)
(cid:12)f (x)(cid:12)
(cid:0)f (x)(cid:1) := η(cid:0)|f (x)|(cid:1) f (x)
|f (x)| ,
where η : R≥0 → R is some nonlinearity acting on norm of a feature vector. Since norm nonlin-
earities preserve the orientation of feature vectors they are equivariant under the action of unitary
representations:

(cid:12) ∀g ∈ G. Norm nonlinearities are functions of the type σnorm

σnorm

(cid:0)ρiso(g)f (x)(cid:1) = η (cid:0)(cid:12)

(cid:12)ρiso(g)f (x)(cid:12)
(cid:12)

(cid:1) ρiso(g)f (x)
(cid:12)ρiso(g)f (x)(cid:12)
(cid:12)
(cid:12)

= η (cid:0)(cid:12)

(cid:1) ρiso(g)f (x)
(cid:12)f (x)(cid:12)
(cid:12)
(cid:12)f (x)(cid:12)
(cid:12)
(cid:12)
(cid:0)f (x)(cid:1) .

= ρiso(g)σnorm

D.2.3 Equivariance of pointwise nonlinearities w.r.t. regular and quotient representations

Quotient representations act on feature vectors by permuting their entries according to the group
composition as deﬁned by ρG/H
quot (˜g)egH := e˜ggH . The permutation of vector entries commutes
with pointwise nonlinearities σ : R → R which are being applied to each entry of a feature vector
individually:

σpt

(cid:0)ρG/H

quot (˜g) f (x)(cid:1) = σpt


ρG/H

quot (˜g)

(cid:88)

gH∈G/H

fgH (x) egH







= σpt



(cid:88)

gH∈G/H

fgH (x) e˜ggH





(cid:88)

=

(cid:0)fgH (x)(cid:1) e˜ggH

σpt

gH∈G/H

= ρG/H

quot (˜g)

(cid:88)

(cid:0)fgH (x)(cid:1) egH

σpt

gH∈G/H

= ρG/H

quot (˜g)σpt

(cid:0)f (x)(cid:1) .

23

Any pointwise nonlinearity is therefore equivariant under the action of quotient representations.
The same holds true for regular representations ρG
quot which are a special case of quotient
representations for the choice H = {e}

reg = ρG/{e}

D.2.4 Equivariance of vector ﬁeld nonlinearities w.r.t. regular and standard representations

Vector ﬁeld nonlinearities map an N -dimensional feature ﬁeld which is transforming under the regular
representation ρCN
reg of CN to a vector ﬁeld. As the elements of CN correspond to rotations by angles
θp ∈ (cid:8)p 2π
reg (˜θ)eθ := e˜θ+θ
and the feature vector as f (x) = (cid:80)
fθ(x)eθ. In this convention vector ﬁeld nonlinearities are
θ∈CN

(cid:9)N −1
p=0 we can write the action of the cyclic group in this speciﬁc case as ρCN

N

deﬁned as

σvec

(cid:0)f (x)(cid:1) := max (cid:0)f (x)(cid:1)

(cid:18)cos(arg max f (x))
sin(arg max f (x))

(cid:19)

.

The maximum operation max : RN → R thereby returns the maximal ﬁeld value which is invariant
under transformations of the regular input ﬁeld. Observe that

arg max

(cid:16)

(cid:17)
reg (˜θ)f (x)
ρCN

= arg max

reg (˜θ)
ρCN

fθ(x)eθ

(cid:33)

(cid:88)

θ∈CN

(cid:33)

(cid:32)

(cid:32)

(cid:88)

θ∈CN

= arg max

fθ(x)e˜θ+θ

= ˜θ + arg max (f (x))

such that

σvec

(cid:0)ρCN

reg (˜θ)f (x)(cid:1) = max (cid:0)f (x)(cid:1)

(cid:18)cos(˜θ + arg max f (x))
sin(˜θ + arg max f (x))

(cid:19)

= max (cid:0)f (x)(cid:1)

(cid:18)cos(˜θ) − sin(˜θ)
cos(˜θ)

sin(˜θ)

(cid:19) (cid:18)cos(arg max f (x))
sin(arg max f (x))

(cid:19)

(cid:18)cos(˜θ) − sin(˜θ)
cos(˜θ)

(cid:19)

transforms under the standard representation ρ(˜θ) =

sin(˜θ)
the resulting feature ﬁeld indeed transforms like a vector ﬁeld.
The original paper [13] used a different convention arg max : RN → {0, . . . , N (cid:57) 1}, return-
ing the integer index of the maximal vector entry. This leads to a corresponding rotation angle
(cid:0)f (x)(cid:1) =
θ = 2π
max (cid:0)f (x)(cid:1)

N arg max(f (x)) ∈ CN in terms of which the vector ﬁeld nonlinearity reads σvec

(cid:19)

of CN . This proofs that

(cid:18)cos(θ)
sin(θ)

D.2.5 Equivariance of nonlinearities w.r.t. induced representations

Consider a group H < G with two representations ρin : H → GL(Rcin) and ρout : H → GL(Rcout).
Suppose we are given an equivariant non-linearity σ : Rcin → Rcout with respect to the actions
of ρin and ρout, that is, ρout(h) ◦ σ = σ ◦ ρin(h) ∀h ∈ H. Then an induced non-linearity ˜σ,
equivariant w.r.t. the induced representations IndG
H ρout of G, can be deﬁned as applying
σ independently to each of the |G : H| different cin-dimensional subspaces of the representation
space which are being permuted by the action of IndG
H ρin, see Appendix B. The permutation of the
subspaces commutes with the individual action of the nonlinearity ˜σ on the subspaces, while the
non-linearity σ itself commutes with the transformation within the subspaces through ρ by assumption.

H ρin and IndG

24

Expressing the feature vector as f (x) = (cid:80)

IndG

H ρin(˜g) f (x)(cid:1) = ˜σ

˜σ(cid:0) IndG

gH egH ⊗ fgH (x) this is seen by:

H ρin(˜g)

(cid:88)

egH ⊗ fgH (x)

gH∈G/H



















= ˜σ

= ˜σ

gH∈G/H

(cid:88)

(cid:88)

gH∈G/H

(cid:88)

=

=

gH∈G/H
(cid:88)

gH∈G/H

IndG

H ρin(˜g) (egH ⊗ fgH (x))



e˜ggH ⊗ ρin(h(˜gr(gH))fgH (x)

e˜ggH ⊗ σ(cid:0)ρin(h(˜gr(gH))fgH (x)(cid:1)

e˜ggH ⊗ ρout(h(˜gr(gH))σ(cid:0)fgH (x)(cid:1)

= IndG

H ρout(˜g)

egH ⊗ σ (fgH (x))

(cid:88)

gH∈G/H

= IndG

H ρout(˜g) ˜σ(cid:0)f (x)(cid:1)

E Visualizations of the irrep kernel constraint

The irrep kernel constraint
(cid:104)(cid:77)

κ(gx) =

decomposes into independent constraints

(cid:105)

ψi(g)

κ(x)

(cid:104)(cid:77)

(cid:105)
(g)

ψ−1
j

j∈Iin

i∈Iout

∀g ∈ G, x ∈ R2

κij(gx) = ψi(g) κij(x) ψ−1

(g)

j

∀g ∈ G, x ∈ R2 where i ∈ Iout, j ∈ Iin ,

on invariant subspaces corresponding to blocks κij of κ. This is the case since the direct sums of
irreps on the right hand side are block diagonal:
















κi1j1(gx) κi1j2(gx) . . .

κi2j1(gx) κi2j2(gx) . . .
. . .

...

...





=





ψi1(g)





(cid:124)

κi1j1(x) κi1j2(x) . . .

κi2j1(x) κi2j2(x) . . .
. . .

...

...

ψi2(g)


·







. . .

(cid:125)

(cid:124)

ψ(cid:57)1
j1

(g)


·







(cid:125)

(cid:124)

ψ(cid:57)1
j2

(g)

. . .





(cid:125)

(cid:76)

(cid:123)(cid:122)
ψ(cid:57)1

j (g)

j∈Iin

(cid:123)(cid:122)
κ(gx)

(cid:125)

(cid:124)

(cid:76)

(cid:123)(cid:122)
i∈Iout

ψi(g)

(cid:123)(cid:122)
κ(x)

(cid:9) for the space of G-steerable kernels satisfying the independent constraints (3)

A basis (cid:8)κij
1 , · · · , κij
dij
on κij contributes to a part of the full basis
(cid:91)
(cid:8)k1, · · · , kd

(cid:9) :=

(cid:91)

of G-steerable kernels satisfying the original constraint (2). Here we deﬁned a zero-padded block

(cid:110)

Q−1

out κij

1 Qin, · · · , Q−1

out κij
dij

Qin

(cid:111)

.

(12)

i∈Iout

j∈Iin

F Solutions of the kernel constraints for irreducible representations

In this section we are deriving analytical solutions of the kernel constraints
κij(gx) = ψi(g) κij(x) ψ−1

∀g ∈ G, x ∈ R2

(g)

(13)

κij :=

0 κij







0

0

0

0







.

0

0

0

j

25

for irreducible representations ψi of O(2) and its subgroups. The linearity of the constraint implies
that the solution space of G-steerable kernels forms a linear subspace of the unrestricted kernel space
k ∈ L2(cid:0)R2(cid:1)cout×cin of square integrable functions k : R2 → Rcout×cin .
Section F.1 introduces the conventions, notation and basic properties used in the derivations. Since
our numerical implementation is on the real ﬁeld we are considering real-valued irreps. It is in general
possible to derive all solutions considering complex valued irreps of G ≤ O(2). While this approach
would simplify some steps it comes with an overhead of relating the ﬁnal results back to the real ﬁeld
which leads to further complications, see Appendix F.5. An overview over the real-valued irreps of
G ≤ O(2) and their properties is given in Section F.2.

We present the analytical solutions of the irrep kernel constraints for all possible pairs of irreps in
Section F.3. Speciﬁcally, the solutions for SO(2) are given in Table 8 while the solutions for O(2),
({±1}, ∗), CN and DN are given in Table 9, Table 10, Table 11 and Table 12 respectively.

Our derivation of the irrep kernel bases is motivated by the observation that the irreps of O(2) and
subgroups are harmonics, that is, they are associated to one particular angular frequency. This
suggests that the kernel constraint (13) decouples into simpler constraints on individual Fourier
modes. In the derivations, presented in Section F.4, we are therefore deﬁning the kernels in polar
coordinates x = x(r, φ) and expand them in terms of an orthogonal, angular, Fourier-like basis.
A projection on this orthogonal basis then yields constraints on the expansion coefﬁcients. Only
speciﬁc coefﬁcients are allowed to be non-zero; these coefﬁcients parameterize the complete space
of G-steerable kernels satisfying the irrep constraint (13). The completeness of the solution follows
from the completeness of the orthogonal basis.

We start with deriving the bases for the simplest cases SO(2) and ({±1}, ∗) in sections F.4.1 and F.4.2.
The G-steerable kernel basis for O(2) forms a subspace of the kernel basis for SO(2) such that it can
be easily derived from this solution by adding the additional constraint coming from the reﬂectional
symmetries in ({±1}, ∗) ∼= O(2)/ SO(2). This additional constraint is imposed in Section F.4.3.
Since CN is a subgroup of discrete rotations in SO(2) their derivation is mostly similar. However,
the discreteness of rotation angles leads to N systems of linear congruences modulo N in the ﬁnal
step. This system of equations is solved in Section F.4.4. Similar to how we derived the kernel basis
for O(2) from SO(2), we derive the basis for DN from CN by adding reﬂectional constraints from
({±1}, ∗) ∼= DN / CN in Section F.4.5.

F.1 Conventions, notation and basic properties

N

Throughout this section we denote rotations in SO(2) and CN by rθ with θ ∈ [0, 2π) and θ ∈
(cid:8)p 2π
(cid:9)N −1
p=0 respectively. Since O(2) ∼= SO(2) (cid:111) ({±1}, ∗) can be seen as a semidirect product of
rotations and reﬂections we decompose orthogonal group elements into a unique product g = rθs ∈
O(2) where s ∈ ({±1}, ∗) is a reﬂection and rθ ∈ SO(2). Similarly, we write g = rθs ∈ DN for the
dihedral group DN
The action of a rotation rθ on R2 in polar coordinates x(r, φ) is given by rθ.x(r, φ) = x(r, rθ.φ) =
x(r, φ + θ). An element g = rθs of O(2) or DN acts on R2 as g.x(r, φ) = x(r, rθs.φ) = x(r, sφ + θ)
where the symbol s denotes both group elements in ({±1}, ∗) and numbers in {±1}.

∼= CN (cid:111)({±1}, ∗), in this case with rθ ∈ CN .

We denote a 2×2 orthonormal matrix with positive determinant, i.e. rotation matrix for an angle θ,
by:

We deﬁne the orthonormal matrix with negative determinant corresponding to a reﬂection with respect
to the horizontal axis as:

and a general orthonormal matrix with negative determinant, i.e. reﬂection with respect to the axis
2θ, as:

(cid:20)cos (θ)
sin (θ)
sin (θ) (cid:57) cos (θ)

(cid:21)

=

(cid:20)cos (θ) (cid:57) sin (θ)
cos (θ)
sin (θ)

(cid:21) (cid:20)1 0
0 −1

(cid:21)

ψ(θ) =

(cid:20)cos (θ) (cid:57) sin (θ)
cos (θ)
sin (θ)

(cid:21)

ξ(s = (cid:57)1) =

(cid:21)
(cid:20)1 0
0 (cid:57)1

26

Hence, we can express any orthonormal matrix in the form:

(cid:20)cos (θ) (cid:57) sin (θ)
cos (θ)
sin (θ)

(cid:21) (cid:20)1
0

(cid:21)
0
s

= ψ(θ)ξ(s)

where ξ(s) =

and s ∈ ({±1}, ∗).

(cid:21)

(cid:20)1 0
0 s

Moreover, these properties will be useful later:

ψ(θ)ξ(s) = ξ(s)ψ(sθ)

ξ(s)−1 = ξ(s)T = ξ(s)

ψ(θ)−1 = ψ(θ)T = ψ(−θ)

ψ(θ1)ψ(θ2) = ψ(θ1 + θ2) = ψ(θ2)ψ(θ1)
(cid:21)
(cid:20)cos (θ)
sin (θ)
sin (θ) (cid:57) cos (θ)

tr(ψ(θ)ξ(−1)) = tr

= 0

tr(ψ(θ)) = tr

(cid:20)cos (θ) (cid:57) sin (θ)
cos (θ)
sin (θ)

(cid:21)

= 2 cos(θ)

w1 cos(α) + w2 sin(α) = w1 cos(β) + w2 sin(β) ∀w1, w2 ∈ R

⇔ ∃t ∈ Z s.t. α = β + 2tπ

(14)

(15)

(16)

(17)

(18)

(19)

(20)

F.2 Irreducible representations of G≤O(2)

Special Orthogonal group SO(2): SO(2) irreps would decompose into complex irreps of U(1)
on the complex ﬁeld but since we are implementing the theory with real-valued variables we will not
consider these. Except for the trivial representation ψ0, all the other irreps are 2-dimensional rotation
matrices with frequencies k ∈ N+.

– ψSO(2)
0

(rθ) = 1

– ψSO(2)
k

(rθ) =

(cid:20)cos (kθ) (cid:57) sin (kθ)
cos (kθ)
sin (kθ)

(cid:21)

= ψ(kθ),

k ∈ N+

Orthogonal group O(2): O(2) has two 1-dimensional irreps: the trivial representation ψ0,0 and a
representation ψ1,0 which assigns ±1 to reﬂections. The other representations are rotation matrices
precomposed by a reﬂection.

– ψO(2)
– ψO(2)

0,0 (rθs) = 1

1,0 (rθs) = s

– ψO(2)

1,k (rθs) =

(cid:20)cos (kθ) (cid:57) sin (kθ)
cos (kθ)
sin (kθ)

(cid:21) (cid:20)1
0

(cid:21)
0
s

= ψ(kθ)ξ(s),

k ∈ N+

Cyclic groups CN: The irreps of CN are identical to the irreps of SO(2) up to frequency (cid:98)N/2(cid:99).
Due to the discreteness of rotation angles, higher frequencies would be aliased.

– ψCN

0 (rθ) = 1

– ψCN

k (rθ) =

(cid:20)cos (kθ) (cid:57) sin (kθ)
cos (kθ)
sin (kθ)

(cid:21)

= ψ(kθ),

k ∈ {1, . . . , (cid:98) N −1

2 (cid:99)}

If N is even, there is an additional 1-dimensional irrep corresponding to frequency (cid:98) N

2 (cid:99) = N
2 :

27

– ψCN

N/2(rθ) = cos (cid:0) N

2 θ(cid:1) ∈ {±1} since θ ∈ {p 2π

N }N −1

p=0

Dihedral groups DN: Similarly, DN consists of irreps of O(2) up to frequency (cid:98)N/2(cid:99).

– ψDN
– ψDN

0,0 (rθs) = 1

1,0 (rθs) = s

– ψDN

1,k (rθs) =

(cid:20)cos (kθ) (cid:57) sin (kθ)
cos (kθ)
sin (kθ)

(cid:21) (cid:20)1
0

(cid:21)

0
s

= ψ(kθ)ξ(s),

k ∈ {1, . . . , (cid:98) N −1

2 (cid:99)}

If N is even, there are two 1-dimensional irreps:

– ψDN

– ψDN

0,N/2(rθs) = cos (cid:0) N
1,N/2(rθs) = s cos (cid:0) N

2 θ(cid:1) ∈ {±1}
2 θ(cid:1) ∈ {±1}

since θ ∈ {p 2π

since θ ∈ {p 2π

p=0

N }N −1
N }N −1

p=0

Reﬂection group ({±1}, ∗) ∼= D1
and C2. For this reason, it has the same irreps of these two groups:

∼= C2: The reﬂection group ({±1}, ∗) is isomorphic to D1

– ψ({±1},∗)
0
– ψ({±1},∗)
1

(s) = 1

(s) = s

28

F.3 Analytical solutions of the irrep kernel constraints

Special Orthogonal Group SO(2)

ψm

ψn

ψ0

ψ0
(cid:2)1(cid:3)

ψn, n ∈ N+
(cid:2) cos(nφ) sin(nφ)(cid:3), (cid:2) (cid:57) sin(nφ) cos(nφ)(cid:3)

ψm,
m ∈ N+

(cid:34)

(cid:34)

(cid:35)

(cid:34)

cos(mφ)
sin(mφ)

,

(cid:35)

(cid:34)

(cid:57)sin(mφ)
cos(mφ)

cos(cid:0)(m−n)φ(cid:1) (cid:57)sin(cid:0)(m−n)φ(cid:1)
cos(cid:0)(m−n)φ(cid:1)
sin(cid:0)(m−n)φ(cid:1)
cos(cid:0)(m+n)φ(cid:1)
sin(cid:0)(m+n)φ(cid:1)
sin(cid:0)(m+n)φ(cid:1) (cid:57)cos(cid:0)(m+n)φ(cid:1)

(cid:34)
(cid:35)
,

(cid:34)
(cid:35)
,

(cid:57) sin(cid:0)(m−n)φ(cid:1) (cid:57)cos(cid:0)(m−n)φ(cid:1)
cos(cid:0)(m−n)φ(cid:1) (cid:57)sin(cid:0)(m−n)φ(cid:1)
cos(cid:0)(m+n)φ(cid:1)
(cid:57) sin(cid:0)(m+n)φ(cid:1)
sin(cid:0)(m+n)φ(cid:1)
cos(cid:0)(m+n)φ(cid:1)

(cid:35)

,

(cid:35)

Table 8: Bases for the angular parts of SO(2)-steerable kernels satisfying the irrep constraint (3) for different
pairs of input ﬁeld irreps ψn and output ﬁeld irreps ψm. The different types of irreps are explained in F.2.

Orthogonal Group O(2)

ψi,m

ψj,n

ψ0,0

ψ1,0

ψ0,0
(cid:2)1(cid:3)

∅

ψ1,0

∅

(cid:2)1(cid:3)

ψ1,n, n ∈ N+
(cid:2) (cid:57) sin(nφ) cos(nφ)(cid:3)

(cid:2) cos(nφ)

sin(nφ)(cid:3)

ψ1,m,
m ∈ N+

(cid:34)(cid:57)sin(mφ)
cos(mφ)

(cid:35) (cid:34)

cos(mφ)

(cid:35) (cid:34)

sin(mφ)

(cid:35)
cos(cid:0)(m(cid:57)n)φ(cid:1) (cid:57)sin(cid:0)(m(cid:57)n)φ(cid:1)
,
cos(cid:0)(m(cid:57)n)φ(cid:1)
sin(cid:0)(m(cid:57)n)φ(cid:1)

(cid:35)
(cid:34)
sin(cid:0)(m+n)φ(cid:1)
cos(cid:0)(m+n)φ(cid:1)
sin(cid:0)(m+n)φ(cid:1) (cid:57)cos(cid:0)(m+n)φ(cid:1)

Table 9: Bases for the angular parts of O(2)-steerable kernels satisfying the irrep constraint (3) for different
pairs of input ﬁeld irreps ψj,n and output ﬁeld irreps ψi,m. The different types of irreps are explained in F.2.

Reﬂection group ({±1}, ∗)

ψi

ψj

ψ0

ψ1

ψ0
cos (cid:0)µ(φ − β)(cid:1)(cid:105)
sin (cid:0)µ(φ − β)(cid:1)(cid:105)

(cid:104)

(cid:104)

ψ1
sin (cid:0)µ(φ − β)(cid:1)(cid:105)
cos (cid:0)µ(φ − β)(cid:1)(cid:105)

(cid:104)

(cid:104)

Table 10: Bases for the angular parts of ({±1}, ∗)-steerable kernels satisfying the irrep constraint (3) for
different pairs of input ﬁeld irreps ψj and output ﬁeld irreps ψi for i, j ∈ {0, 1}. The different types of irreps
are explained in F.2. The group is assumed to act by reﬂecting over an axis deﬁned by the angle β. Note that the
bases shown here are a special case of the bases shown in Table 12 since ({±1}, ∗) ∼= D1.

29

ψN/2 (if N even)
(cid:1)N φ(cid:1) (cid:105)
cos (cid:0)(cid:0)ˆt+ 1
,
(cid:1)N φ(cid:1) (cid:105)
sin (cid:0)(cid:0)ˆt+ 1

2

2

(cid:104)

(cid:104)

(cid:2) cos(ˆtN φ)(cid:3),
(cid:2) sin(ˆtN φ)(cid:3)

ψm

ψn

ψ0

ψ0

ψN/2 (N even)

ψm,
m ∈ N+
1 ≤ m < N/2

(cid:2) cos(ˆtN φ)(cid:3),
(cid:2) sin(ˆtN φ)(cid:3)

(cid:104)

(cid:104)

(cid:34)

(cid:34)

,

2

2

cos (cid:0)(cid:0)ˆt+ 1
sin (cid:0)(cid:0)ˆt+ 1

(cid:1)N φ(cid:1) (cid:105)
(cid:1)N φ(cid:1) (cid:105)
(cid:35)
(cid:57)sin((m + tN )φ)
cos((m + tN )φ)
(cid:35)
cos((m + tN )φ)
sin((m + tN )φ)

,

Cyclic groups CN

ψn with n ∈ N+ and 1 ≤ n < N/2

(cid:2) (cid:57) sin((n + tN )φ) cos((n + tN )φ)(cid:3),
sin((n + tN )φ)(cid:3)
(cid:2)

cos((n + tN )φ)

(cid:104)

(cid:104)

(cid:57) sin (cid:0)(cid:0)n + (cid:0)t+ 1
cos (cid:0)(cid:0)n + (cid:0)t+ 1

2

(cid:1)N (cid:1)φ(cid:1)
(cid:1)N (cid:1)φ(cid:1)

2

cos (cid:0)(cid:0)n + (cid:0)t+ 1
sin (cid:0)(cid:0)n + (cid:0)t+ 1

2

2

(cid:1)N (cid:1)φ(cid:1) (cid:105)
,
(cid:1)N (cid:1)φ(cid:1) (cid:105)

(cid:34)

(cid:34)

2

(cid:57)sin (cid:0)(cid:0)m + (cid:0)t+ 1
cos (cid:0)(cid:0)m + (cid:0)t+ 1
cos (cid:0)(cid:0)m + (cid:0)t+ 1
sin (cid:0)(cid:0)m + (cid:0)t+ 1

2

2

2

,

(cid:35)
(cid:1)N (cid:1)φ(cid:1)
(cid:1)N (cid:1)φ(cid:1)
(cid:35)
(cid:1)N (cid:1)φ(cid:1)
(cid:1)N (cid:1)φ(cid:1)

(cid:34)

(cid:34)

(cid:35)
cos(cid:0)(m−n + tN)φ(cid:1) (cid:57)sin(cid:0)(m−n + tN)φ(cid:1)
,
cos(cid:0)(m−n + tN)φ(cid:1)
sin(cid:0)(m−n + tN)φ(cid:1)
(cid:35)
cos(cid:0)(m+n + tN)φ(cid:1)
sin(cid:0)(m+n + tN)φ(cid:1)
,
sin(cid:0)(m+n + tN)φ(cid:1) (cid:57)cos(cid:0)(m+n + tN)φ(cid:1)

(cid:34)

(cid:34)

(cid:35)
(cid:57) sin(cid:0)(m−n + tN)φ(cid:1) (cid:57)cos(cid:0)(m−n + tN)φ(cid:1)
,
cos(cid:0)(m−n + tN)φ(cid:1) (cid:57)sin(cid:0)(m−n + tN)φ(cid:1)
(cid:35)
(cid:57) sin(cid:0)(m+n + tN)φ(cid:1) (cid:57)cos(cid:0)(m+n + tN)φ(cid:1)
cos(cid:0)(m+n + tN)φ(cid:1) (cid:57)sin(cid:0)(m+n + tN)φ(cid:1)

Table 11: Bases for the angular parts of CN -steerable kernels for different pairs of input and output ﬁelds irreps ψn and ψm. The full basis is found
by instantiating these solutions for each t ∈ Z or ˆt ∈ N. The different types of irreps are explained in Appendix F.2.

3
0

Dihedral groups DN

ψi,m

ψj,n

ψ0,0

ψ1,0

ψ0,0

ψ1,0

(cid:2) cos(ˆtN φ)(cid:3)

(cid:2) sin(ˆtN φ)(cid:3)

(cid:2) sin(ˆtN φ)(cid:3)

(cid:2) cos(ˆtN φ)(cid:3)

ψ0,N/2 (N even)

ψ1,N/2 (N even)

(cid:104)
cos (cid:0)(cid:0)ˆt+ 1

(cid:104)

sin (cid:0)(cid:0)ˆt+ 1

(cid:1)N φ(cid:1) (cid:105)
(cid:1)N φ(cid:1) (cid:105)

2

2

(cid:104)

(cid:104)

sin (cid:0)(cid:0)ˆt+ 1

cos (cid:0)(cid:0)ˆt+ 1

(cid:1)N φ(cid:1) (cid:105)
(cid:1)N φ(cid:1) (cid:105)

2

2

cos (cid:0)(cid:0)ˆt+ 1

ψ0,N/2 (if N even)
(cid:1)N φ(cid:1) (cid:105)
(cid:104)
(cid:1)N φ(cid:1) (cid:105)

sin (cid:0)(cid:0)ˆt+ 1

(cid:104)

2

2

(cid:2) cos(ˆtN φ)(cid:3)

(cid:2) sin(ˆtN φ)(cid:3)

sin (cid:0)(cid:0)ˆt+ 1

ψ1,N/2 (if N even)
(cid:1)N φ(cid:1) (cid:105)
(cid:104)
(cid:1)N φ(cid:1) (cid:105)

cos (cid:0)(cid:0)ˆt+ 1

2

(cid:104)

2

(cid:2) sin(ˆtN φ)(cid:3)

(cid:2) cos(ˆtN φ)(cid:3)

ψ1,m,
m ∈ N+
1 ≤ m < N/2

(cid:34)(cid:57)sin((m + tN )φ)
cos((m + tN )φ)

(cid:35) (cid:34)

cos((m + tN )φ)

sin((m + tN )φ)

(cid:35) (cid:34)(cid:57)sin(cid:0)(cid:0)m + (cid:0)t+ 1
cos(cid:0)(cid:0)m + (cid:0)t+ 1

2

(cid:1)N (cid:1)φ(cid:1)
(cid:1)N (cid:1)φ(cid:1)

2

(cid:35) (cid:34)

cos(cid:0)(cid:0)m + (cid:0)t+ 1
sin(cid:0)(cid:0)m + (cid:0)t+ 1

2

(cid:35)
(cid:1)N (cid:1)φ(cid:1)
(cid:1)N (cid:1)φ(cid:1)

2

ψ1,n with n ∈ N+ and 1 ≤ n < N/2

(cid:2) (cid:57) sin((n + tN )φ) cos((n + tN )φ)(cid:3)

(cid:2) cos ((n + tN )φ)

sin ((n + tN )φ) (cid:3)

(cid:104)

(cid:104)

(cid:57) sin(cid:0)(cid:0)n + (cid:0)t+ 1

cos(cid:0)(cid:0)n + (cid:0)t+ 1

2

2

2

(cid:1)N (cid:1)φ(cid:1)

(cid:1)N (cid:1)φ(cid:1)

sin(cid:0)(cid:0)n + (cid:0)t+ 1

cos(cid:0)(cid:0)n + (cid:0)t+ 1

(cid:1)N (cid:1)φ(cid:1) (cid:105)
(cid:1)N (cid:1)φ(cid:1) (cid:105)
(cid:35)
cos(cid:0)(m−n + tN)φ(cid:1) (cid:57)sin(cid:0)(m−n + tN)φ(cid:1)
,
cos(cid:0)(m−n + tN)φ(cid:1)
sin(cid:0)(m−n + tN)φ(cid:1)
(cid:35)
cos(cid:0)(m+n + tN)φ(cid:1)
sin(cid:0)(m+n + tN)φ(cid:1)
sin(cid:0)(m+n + tN)φ(cid:1) (cid:57)cos(cid:0)(m+n + tN)φ(cid:1)

2

(cid:34)

(cid:34)

Table 12: Bases for the angular parts of DN -steerable kernels for different pairs of input and output ﬁelds irreps ψj,n and ψi,m. The full basis is
found by instantiating these solutions for each t ∈ Z or ˆt ∈ N. The different types of irreps are explained in Appendix F.2. The solutions here shown
are for a group action where the reﬂection is deﬁned around the horizontal axis (β = 0). For different axes β (cid:54)= 0 substitute φ with φ − β.

F.4 Derivations of the kernel constraints

Here we solve the kernel constraints for the irreducible representations of G ≤ O(2). Since the
irreps of G are either 1- or 2-dimensional, we distinguish between mappings between 2-dimensional
irreps, mappings from 2- to 1-dimensional and 1- to 2-dimensional irreps and mappings between
1-dimensional irreps. We are ﬁrst exclusively considering positive radial parts r > 0 in the following
sections. The constraint at the origin r = 0 requires some additional considerations which we
postpone to Section F.4.6.

F.4.1 Derivation for SO(2)

n

, where ψSO(2)

2-dimensional irreps:
We ﬁrst consider the case of 2-dimensional irreps both in the input and in output, that is, ρout = ψSO(2)
(cid:20)cos (kθ) (cid:57) sin (kθ)
and ρin = ψSO(2)
. This means that the kernel has the
cos (kθ)
sin (kθ)
form κij : R2 → R2×2. To reduce clutter we will from now on suppress the indices ij corresponding
to the input and output irreps in the input and output ﬁelds.
We expand each entry of the kernel κ in terms of an (angular) Fourier series20
(cid:20)sin(µφ)
0

(cid:20)cos(µφ)
0

+ B00,µ(r)

κ(r, φ) =

A00,µ(r)

(cid:21)
0
0

(cid:21)
0
0

(θ) =

∞
(cid:88)

(cid:21)

m

k

µ=0

+ A01,µ(r)

+ B01,µ(r)

(cid:20)0
0

(cid:20)

(cid:21)

cos(µφ)
0

(cid:21)
0
0
(cid:21)

0
cos(µφ)
(cid:20)0
0

0
cos(µφ)

+ A10,µ(r)

+ B10,µ(r)

+ A11,µ(r)

+ B11,µ(r)

(cid:20)0
0
(cid:20)

(cid:21)

sin(µφ)
0

(cid:21)
0
0
(cid:21)

0
sin(µφ)
(cid:20)0
0

0
sin(µφ)

and, for convenience, perform a change of basis to a different, non-sparse, orthogonal basis

κ(r, φ) =

w0,0,µ(r)

(cid:20)cos (µφ) (cid:57) sin (µφ)
cos (µφ)
sin (µφ)

(cid:21)

+ w0,1,µ(r)

∞
(cid:88)

µ=0

+ w1,0,µ(r)

+ w2,0,µ(r)

+ w3,0,µ(r)

(cid:21)

(cid:20)cos (µφ)
sin (µφ)
sin (µφ) (cid:57) cos (µφ)
(cid:21)
(cid:20)cos ((cid:57)µφ) (cid:57) sin ((cid:57)µφ)
cos ((cid:57)µφ)
sin ((cid:57)µφ)
(cid:21)
(cid:20)cos ((cid:57)µφ)
sin ((cid:57)µφ)
sin ((cid:57)µφ) (cid:57) cos ((cid:57)µφ)

+ w1,1,µ(r)

+ w2,1,µ(r)

+ w3,1,µ(r)

(cid:21)

2

2

2

2

2

(cid:1)
(cid:1)

(cid:20)cos (cid:0)µφ + π
sin (cid:0)µφ + π
(cid:20)cos (cid:0)µφ + π
sin (cid:0)µφ + π
(cid:20)cos (cid:0)(cid:57)µφ + π
sin (cid:0)(cid:57)µφ + π
(cid:20)cos (cid:0)(cid:57)µφ + π
sin (cid:0)(cid:57)µφ + π

(cid:1) (cid:57) sin (cid:0)µφ + π
cos (cid:0)µφ + π
(cid:1)
(cid:1)
sin (cid:0)µφ + π
(cid:1)
(cid:1) (cid:57) cos (cid:0)µφ + π
(cid:1)
(cid:1) (cid:57) sin (cid:0)(cid:57)µφ + π
cos (cid:0)(cid:57)µφ + π
(cid:1)
sin (cid:0)(cid:57)µφ + π
(cid:1)
(cid:1) (cid:57) cos (cid:0)(cid:57)µφ + π

2

2

2

2

2

2

2

2

2

(cid:21)

2

(cid:21)

(cid:1)
(cid:1)

(cid:21)
.

(cid:1)
(cid:1)

2

The last four matrices are equal to the ﬁrst four, except for their opposite frequency. Moreover, the
second matrices of each row are equal to the ﬁrst matrices, with a phase shift of π
2 added. Therefore,
we can as well write:

κ(r,φ) =

∞
(cid:88)

(cid:88)

µ=(cid:57)∞

γ∈{0, π

w0,γ,µ(r)
2 }

(cid:20)cos (µφ+γ) (cid:57) sin (µφ+γ)
cos (µφ+γ)
sin (µφ+γ)

(cid:21)

+w1,γ,µ(r)

(cid:21)
(cid:20)cos (µφ+γ)
sin (µφ+γ)
sin (µφ+γ) (cid:57) cos (µφ+γ)

Notice that the ﬁrst matrix evaluates to ψ(µφ + γ)ξ(1) = ψ(µφ + γ) while the second evaluates to
ψ(µφ + γ)ξ(−1). Hence, for s ∈ {±1} we can compactly write:

κ(r, φ) =

∞
(cid:88)

(cid:88)

(cid:88)

µ=−∞

γ∈{0, π

2 }

s∈{±1}

ws,γ,µ(r)ψ(µφ + γ)ξ(s)

20 For brevity, we suppress that frequency 0 is associated to only half the number of basis elements which

does not affect the validity of the derivation.

31

As already shown in Section 2.5, we can w.l.o.g. consider the kernels as being deﬁned only on the
angular component φ ∈ [0, 2π) = S1 by solving only for a speciﬁc radial component r. As a result,
we consider the basis

(cid:26)

bµ,γ,s(φ) = ψ(µφ + γ)ξ(s)

µ ∈ Z, γ ∈

, s ∈ ({±1}, ∗)

(21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:110)

0,

(cid:111)

π
2

(cid:27)

of the unrestricted kernel space which we will constrain in the following by demanding

κ(φ + θ) = ψSO(2)

(rθ)κ(φ)ψSO(2)

(rθ)−1 ∀φ, θ ∈ [0, 2π),

m

n

(22)

where we dropped the unrestricted radial part.

We solve for a basis of the subspace satisfying this constraint by projecting both sides on the basis
elements deﬁned above. The inner product on L2(cid:0)S1(cid:1)2×2
(cid:90)
1
1
4π
4π

is hereby deﬁned as
(cid:90)

dφ tr (cid:0)k1(φ)T k2(φ)(cid:1) ,

dφ(cid:104)k1(φ), k2(φ)(cid:105)F =

(cid:104)k1, k2(cid:105) =

where (cid:104)·, ·(cid:105)F denotes the Frobenius inner product between 2 matrices.

First consider the projection of the lhs of the kernel constraint (22) on a generic basis element
bµ(cid:48),γ(cid:48),s(cid:48)(φ) = ψ(µ(cid:48)φ + γ(cid:48))ξ(s(cid:48)). Deﬁning the operator Rθ by (Rθκ) (φ) := κ(φ + θ), the projection
gives:

(cid:104)bµ(cid:48),γ(cid:48),s(cid:48), Rθκ(cid:105) =

dφ tr (cid:0)bµ(cid:48),γ(cid:48),s(cid:48)(φ)T (Rθκ) (φ)(cid:1)

=

dφ tr (cid:0)bµ(cid:48),γ(cid:48),s(cid:48)(φ)T κ(φ + θ)(cid:1) .

By expanding the kernel in the linear combination of the basis we further obtain

(cid:32)

dφ tr

bµ(cid:48),γ(cid:48),s(cid:48)(φ)T

(cid:32)

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

ws,γ,µψ (µ(φ + θ) + γ) ξ(s)

,

which, observing that the trace, sums and integral commute, results in:

(cid:33)(cid:33)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

ws,γ,µ tr

dφ bµ(cid:48),γ(cid:48),s(cid:48)(φ)T ψ (µ(φ + θ) + γ) ξ(s)

ws,γ,µ tr

dφ (ψ(µ(cid:48)φ + γ(cid:48))ξ(s(cid:48)))T ψ (µ(φ + θ) + γ) ξ(s)

ws,γ,µ tr

dφ ξ(s(cid:48))T ψ(µ(cid:48)φ + γ(cid:48))T ψ (µ(φ + θ) + γ) ξ(s)

ws,γ,µ tr

dφ ξ(s(cid:48))ψ(−µ(cid:48)φ − γ(cid:48))ψ (µ(φ + θ) + γ) ξ(s)

(cid:18)

ws,γ,µ tr

ξ(s(cid:48))ψ(γ − γ(cid:48))

(cid:90)

(cid:18) 1
2π

(cid:19)
dφ ψ((µ − µ(cid:48))φ)

ψ(µθ)ξ(s)

(cid:19)

Using the properties in Eq. (15) and (16) then yields:

In the integral, each cell of the matrix ψ((µ − µ(cid:48))φ) contains either a sine or cosine. As a result, if
µ − µ(cid:48) (cid:54)= 0, all these integrals evaluate to 0. Otherwise, the cosines on the diagonal evaluate to 1,
while the sines integrate to 0. The whole integral evaluates to δµ,µ(cid:48) id2×2, such that

ws,γ,µ(cid:48) tr (ξ(s(cid:48))ψ(γ − γ(cid:48))ψ(µ(cid:48)θ)ξ(s)) ,

which, using the property in Eq. (14) leads to

ws,γ,µ(cid:48) tr (ψ(s(cid:48)(γ − γ(cid:48) + µ(cid:48)θ))ξ(s(cid:48) ∗ s)) .

1
4π
1
4π

(cid:90)

(cid:90)

(cid:90)

=

1
4π

=

=

=

=

=

1
4π

1
4π

1
4π

1
4π

1
2

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

=

=

1
2

1
2

(cid:88)

(cid:88)

γ

s

(cid:88)

(cid:88)

γ

s

(cid:18)(cid:90)

(cid:18)(cid:90)

(cid:18)(cid:90)

(cid:18)(cid:90)

32

Recall the propetries of the trace in Eq. (18), (19). If s(cid:48) ∗ s = −1, i.e. s(cid:48) (cid:54)= s, the matrix has a trace
of 0:

=

1
2

(cid:88)

(cid:88)

γ

s

ws,γ,µ(cid:48)δs(cid:48),s2 cos(s(cid:48)(γ − γ(cid:48) + µ(cid:48)θ))

Since cos(−α) = cos(α) and s(cid:48) ∈ {±1}:
1
2

(cid:88)

(cid:88)

=

ws,γ,µ(cid:48)δs(cid:48),s2 cos(γ − γ(cid:48) + µ(cid:48)θ)

s

γ
(cid:88)
ws(cid:48),γ,µ(cid:48) cos((γ − γ(cid:48)) + µ(cid:48)θ)

=

γ

Next consider the projection of the rhs of Eq. (22):
(rθ)κ(·)ψSO(2)
(cid:104)bµ(cid:48),γ(cid:48),s(cid:48), ψSO(2)
(cid:90)

(rθ)−1(cid:105)

m

n
(cid:16)

dφ tr

bµ(cid:48),γ(cid:48),s(cid:48)(φ)T ψSO(2)

(rθ)κ(φ)ψSO(2)

m

n

(rθ)−1(cid:17)

dφ tr (cid:0)bµ(cid:48),γ(cid:48),s(cid:48)(φ)T ψ(mθ)κ(φ)ψ(−nθ)(cid:1)

An expansion of the kernel in the linear combination of the basis yields:

dφ tr

bµ(cid:48),γ(cid:48),s(cid:48)(φ)T ψ(mθ)

ws,γ,µψ (µφ + γ) ξ(s)

ψ(−nθ)

(cid:32)

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

(cid:33)

(cid:33)

(cid:19)

(cid:18)(cid:90)

ws,γ,µ tr

dφ bµ(cid:48),γ(cid:48),s(cid:48)(φ)T ψ(mθ)ψ (µφ + γ) ξ(s)ψ(−nθ)

ws,γ,µ tr

(cid:18)(cid:90)

(cid:19)
dφ ξ(s(cid:48))ψ(−µ(cid:48)φ − γ(cid:48))ψ(mθ)ψ (µφ + γ) ξ(s)ψ(−nθ)

ws,γ,µ tr

(cid:18)
ξ(s(cid:48))ψ(γ (cid:57) γ(cid:48))ψ(mθ)

(cid:19)
dφ ψ((µ (cid:57) µ(cid:48))φ)

(cid:19)
ξ(s)ψ((cid:57)nθ)

(cid:90)

(cid:18) 1
2π

Again, the integral evaluates to δµ,µ(cid:48) id2×2:
(cid:88)

(cid:88)

(cid:88)

ws,γ,µδµ,µ(cid:48) tr (ξ(s(cid:48))ψ(γ − γ(cid:48))ψ(mθ)ξ(s)ψ(−nθ))

ws,γ,µ(cid:48) tr (ξ(s(cid:48))ψ(γ − γ(cid:48))ψ(mθ)ξ(s)ψ(−nθ))

ws,γ,µ(cid:48) tr (ψ(s(cid:48)(γ − γ(cid:48) + mθ − nsθ))ξ(s(cid:48) ∗ s))

For the same reason as before, the trace is not zero if and only if s(cid:48) = s:

ws,γ,µ(cid:48)δs(cid:48),s2 cos(s(cid:48)(γ − γ(cid:48) + mθ − nsθ))

s
Since cos(−α) = cos(α) and s(cid:48) ∈ {±1}:

γ

ws(cid:48),γ,µ(cid:48) cos(γ − γ(cid:48) + mθ − ns(cid:48)θ)

ws(cid:48),γ,µ(cid:48) cos((γ − γ(cid:48)) + (m − ns(cid:48))θ)

(cid:90)

(cid:90)

1
4π
1
4π

1
4π

1
4π

1
4π

(cid:32)

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

(cid:88)

(cid:88)

(cid:88)

µ

γ

s

s

µ
(cid:88)

γ
(cid:88)

γ
(cid:88)

s
(cid:88)

γ

s

(cid:88)

(cid:88)

=

=

=

=

=

=

=

=

=

=

=

=

1
2

1
2

1
2

1
2

1
2

(cid:88)

γ
(cid:88)

γ

Finally, we require the two projections to be equal for all rotations in SO(2), that is,

ws(cid:48),γ,µ(cid:48) cos((γ − γ(cid:48)) + µ(cid:48)θ) =

ws(cid:48),γ,µ(cid:48) cos((γ − γ(cid:48)) + (m − ns(cid:48))θ)

∀θ ∈ [0, 2π),

(cid:88)

γ

(cid:88)

γ

33

or, explicitly, with γ ∈ {0, π

2 } and cos(α + π

2 ) = − sin(α):

ws(cid:48),0,µ(cid:48) cos(µ(cid:48)θ − γ(cid:48))

− ws(cid:48), π
= ws(cid:48),0,µ(cid:48) cos((m − ns(cid:48))θ − γ(cid:48)) − ws(cid:48), π

2 ,µ(cid:48) sin(µ(cid:48)θ − γ(cid:48))
2 ,µ(cid:48) sin((m − ns(cid:48))θ − γ(cid:48))

∀θ ∈ [0, 2π)

Using the property in Eq. (20) then implies that for each θ in [0, 2π) there exists a t ∈ Z such that:

⇔

µ(cid:48)θ − γ(cid:48) = (m − ns(cid:48))θ − γ(cid:48) + 2tπ

⇔

(µ(cid:48) − (m − ns(cid:48)))θ = 2tπ

(23)
Since the constraint needs to hold for any θ ∈ [0, 2π) this results in the condition µ(cid:48) = m − sn(cid:48) on
the frequencies occurring in the SO(2)-steerable kernel basis. Both γ and s are left unrestricted such
that we end up with the four-dimensional basis

(cid:26)

KSO(2)

ψm←ψn

=

bµ,γ,s(φ) = ψ(cid:0)µφ + γ(cid:1)ξ(s)

µ = (m (cid:57) sn), γ ∈

(cid:110)

0,

(cid:111)

π
2

(cid:27)

, s ∈ {±1}

(24)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

for the angular parts of equivariant kernels for m, n > 0. This basis is explicitly written out in the
lower right cell of Table 8.

1-dimensional irreps:
For the case of 1-dimensional irreps in both the input and output, i.e. ρout = ρin = ψSO(2)
the kernel
has the form κij : R2 → R1×1. As a scalar function in L2(R2), it can be expressed by the Fourier
decomposition of its angular part:

0

κ(r, φ) = w0,0 +

wµ,γ(r) cos(µφ + γ)

∞
(cid:88)

(cid:88)

µ=1

γ∈{0, π

2 }

As before, we can w.l.o.g. drop the dependency on the radial part as it is not restricted by the
constraint. We are therefore considering the basis

(cid:40)

bµ,γ(φ) = cos(µφ + γ)

µ ∈ N, γ ∈

(cid:26){0}

if µ = 0

(cid:41)

{0, π/2} otherwise

(25)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

of angular kernels in L2(S1)1×1. The kernel constraint in Eq. (3) then requires

κ(φ + θ) = ψSO(2)

m

(rθ)κ(φ)ψSO(2)

(rθ)−1

n

⇔ κ(φ + θ) = κ(φ)

∀θ, φ ∈ [0, 2π)
∀θ, φ ∈ [0, 2π),

i.e. the kernel has to be invariant to rotations.

Again, we ﬁnd the space of all solutions by projecting both sides on the basis deﬁned above. Here, the
(cid:82) dφk1(φ)k2(φ)
projection of two kernels is deﬁned through the standard inner product (cid:104)k1, k2(cid:105) = 1
2π
on L2(S1).

We ﬁrst consider the projection of the lhs:

(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) =

dφ bµ(cid:48),γ(cid:48)(φ) (Rθκ) (φ)

dφ bµ(cid:48),γ(cid:48)(φ)κ(φ + θ)

As before we expand the kernel in the linear combination of the basis:

(cid:90)

(cid:90)

1
2π
1
2π

(cid:88)

µ,γ

(cid:88)

µ,γ

=

=

=

(cid:90)

(cid:90)

1
2π

1
2π

wµ,γ

dφ bµ(cid:48),γ(cid:48)(φ) cos(µφ + µθ + γ)

wµ,γ

dφ cos(µ(cid:48)φ + γ(cid:48)) cos(µφ + µθ + γ)

34

With cos(α) cos(β) = 1

=

(cid:90)

2 (cos(α − β) + cos(α + β)) this results in:
1
(cid:88)
2
+ cos(µ(cid:48)φ + γ(cid:48) + µφ + µθ + γ)(cid:1)

(cid:0) cos(µ(cid:48)φ + γ(cid:48) − µφ − µθ − γ)

1
2π

wµ,γ

dφ

µ,γ

dφ cos((µ(cid:48) − µ)φ + (γ(cid:48) − γ) − µθ)

dφ cos((µ(cid:48) + µ)φ + (γ(cid:48) + γ) + µθ)(cid:1)

wµ,γ

(δµ,µ(cid:48) cos((γ(cid:48) − γ) − µθ) + δµ,−µ(cid:48) cos((γ(cid:48) + γ) + µθ))

=

(cid:88)

µ,γ

wµ,γ

1
2

(cid:0) 1
2π

(cid:90)

(cid:90)

1
2π

+

1
2

=

(cid:88)

µ,γ

=

1
2

(cid:88)

γ

Since µ, µ(cid:48) ≥ 0 and µ = −µ(cid:48) imply µ = µ(cid:48) = 0 this simpliﬁes further to

wµ(cid:48),γ (cos((γ(cid:48) − γ) − µ(cid:48)θ) + δµ(cid:48),0 cos(γ(cid:48) + γ)) .

A projection of the rhs yields:

(cid:104)bµ(cid:48),γ(cid:48), κ(cid:105) =

dφ bµ(cid:48),γ(cid:48)(φ)κ(φ)

(cid:90)

1
2π
(cid:88)

µ,γ

(cid:88)

µ,γ
1
2

(cid:88)

γ

=

=

=

(cid:90)

(cid:90)

1
2π

1
2π

wµ,γ

dφ bµ(cid:48),γ(cid:48)(φ) cos(µφ + γ)

wµ,γ

dφ cos(µ(cid:48)φ + γ(cid:48)) cos(µφ + γ)

wµ(cid:48),γ (cos(γ(cid:48) − γ) + δµ(cid:48),0 cos((γ(cid:48) + γ)))

The projections are required to coincide for all rotations:

wµ(cid:48),γ (cos((γ(cid:48) −γ)−µ(cid:48)θ) + δµ(cid:48),0 cos((γ(cid:48) +γ))) =

(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) = (cid:104)bµ(cid:48),γ(cid:48), κ(cid:105)

∀θ ∈ [0, 2π)
wµ(cid:48),γ (cos(γ(cid:48) −γ)+δµ(cid:48),0 cos((γ(cid:48) +γ)))

(cid:88)

γ

(cid:88)

γ

∀θ ∈ [0, 2π)

We consider two cases:

• µ(cid:48) = 0 In this case, the basis in Eq.(25) is restricted to the single case γ(cid:48) = 0 (as γ(cid:48) = π

2 and µ(cid:48) = 0

together lead to a null basis element). Then:
(cid:88)

w0,γ (cos(−γ) + cos(γ)) =

w0,γ (cos(−γ) + cos(γ))

(cid:88)

γ

γ

2 } and cos(± π

As γ ∈ {0, π
⇔
⇔

2 ) = 0:
w0,0 (cos(0) + cos(0)) = w0,0 (cos(0) + cos(0))
w0,0 = w0,0

which is always true.

• µ(cid:48) > 0 Here:

(cid:88)

wµ(cid:48),γ cos((γ(cid:48) − γ) − µ(cid:48)θ) =

wµ(cid:48),γ cos(γ(cid:48) − γ)

(cid:88)

∀θ ∈ [0, 2π)

γ
⇔ wµ(cid:48),0 cos(γ(cid:48) (cid:57) µ(cid:48)θ) + wµ(cid:48), π
⇔
where Eq. (20) was used in the last step. From the last equation one can see that µ(cid:48) must be
zero. Since this contradicts the assumption that µ(cid:48) ≥ 0, no solution exists.

γ
sin(γ(cid:48) (cid:57) µ(cid:48)θ) = wµ(cid:48),0 cos(γ(cid:48)) + wµ(cid:48), π

−µ(cid:48)θ = 2tπ

2

2

sin(γ(cid:48)) ∀θ ∈ [0, 2π)

∀θ ∈ [0, 2π),

35

This results in a one dimensional basis of isotropic (rotation invariant) kernels

KSO(2)

ψm←ψn

= (cid:8)b0,0(φ) = 1(cid:9)

(26)

for m = n = 0, i.e. trivial representations. The basis is presented in the upper left cell of Table 8.

1 and 2-dimensional irreps:
Finally, consider the case of a 1-dimensional irrep in the input and a 2-dimensional irrep in the
output, that is, ρout = ψSO(2)
. The corresponding kernel κij : R2 → R2×1 can be
expanded in the following generalized Fourier series on L2(R2)2×1:

and ρin = ψSO(2)

m

0

κ(r, φ) =

A0,µ(r)

+ B0,µ(r)

∞
(cid:88)

µ=0

(cid:21)
(cid:20)cos(µφ)
0

(cid:20)

(cid:21)

0
cos(µφ)

+ A1,µ(r)

+ B1,µ(r)

(cid:21)

(cid:20)sin(µφ)
0

(cid:21)

(cid:20)

0
sin(µφ)

As before, we perform a change of basis to produce a non-sparse basis
(cid:21)
(cid:20)cos(µφ + γ)
sin(µφ + γ)

κ(r, φ) =

∞
(cid:88)

(cid:88)

.

µ=−∞

γ∈{0, π

wγ,µ(r)
2 }

Dropping the radial parts as usual, this corresponds to the complete basis :

(cid:26)

bµ,γ(φ) =

(cid:20)cos(µφ + γ)
sin(µφ + γ)

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

µ ∈ Z, γ ∈

(cid:111)(cid:27)

(cid:110)

0,

π
2

(27)

of angular kernels on L2(S1)2×1.

The constraint in Eq. (3) requires the kernel space to satisfy

κ(φ + θ) = ψSO(2)
⇔ κ(φ + θ) = ψSO(2)

m

m

(rθ)κ(φ)ψSO(2)
(rθ)κ(φ)

0

(rθ)−1

∀θ, φ ∈ [0, 2π)

∀θ, φ ∈ [0, 2π).

We again project both sides of this equation on the basis elements deﬁned above where the projection
on L2(S1)2×1 is deﬁned by (cid:104)k1, k2(cid:105) = 1
2π
Consider ﬁrst the projection of the lhs

(cid:82) dφ k1(φ)T k2(φ).

(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) =

dφ bµ(cid:48),γ(cid:48)(φ)T (Rθκ) (φ)

dφ bµ(cid:48),γ(cid:48)(φ)T κ(φ + θ) ,

which, after expanding the kernel in terms of the basis reads:

wµ,γ

dφ bµ(cid:48),γ(cid:48)(φ)T

(cid:21)
(cid:20)cos(µ(φ + θ) + γ)
sin(µ(φ + θ) + γ)

(cid:90)

(cid:90)

1
2π
1
2π

=

=

=

=

(cid:88)

µ,γ

(cid:88)

µ,γ

(cid:88)

µ,γ

wµ,γ

wµ,γ

(cid:90)

(cid:90)

(cid:90)

1
2π

1
2π

1
2π

dφ [cos(µ(cid:48)φ + γ(cid:48))

sin(µ(cid:48)φ + γ(cid:48))]

(cid:21)
(cid:20)cos(µ(φ + θ) + γ)
sin(µ(φ + θ) + γ)

dφ cos((µ(cid:48) − µ)φ + (γ(cid:48) − γ) − µθ).

As before, the integral is non-zero only if the frequency is 0, i.e. iff µ(cid:48) − µ = 0 and thus:

(cid:88)

=

γ

wµ(cid:48),γ cos((γ(cid:48) − γ) − µ(cid:48)θ)

36

For the rhs we obtain:

(cid:104)bµ(cid:48),γ(cid:48), ψSO(2)

m

dφ bµ(cid:48),γ(cid:48)(φ)T ψm(rθ)κ(φ)

wµ,γ

dφ bµ(cid:48),γ(cid:48)(φ)T ψm(rθ)

(cid:21)
(cid:20)cos(µφ + γ)
sin(µφ + γ)

wµ,γ

dφ [cos(µ(cid:48)φ + γ(cid:48))

sin(µ(cid:48)φ + γ(cid:48))] ψm(rθ)

(cid:20)cos(µφ + γ)
sin(µφ + γ)

(cid:21)

dφ cos(µ(cid:48)φ + γ(cid:48) − µφ − γ − mθ)

dφ cos((µ(cid:48) − µ)φ + γ(cid:48) − γ − mθ) .

(cid:90)

(cid:90)

(cid:90)

(cid:90)

1
2π

1
2π

1
2π

1
2π

wµ,γ

wµ,γ

=

(rθ)κ(·)(cid:105)
(cid:90)
1
2π
(cid:88)

=

µ,γ

(cid:88)

µ,γ

(cid:88)

µ,γ

(cid:88)

µ,γ

=

=

=

(cid:88)

=

γ

The integral is non-zero only if the frequency is 0, i.e. µ(cid:48) − µ = 0:

wµ(cid:48),γ cos(γ(cid:48) − γ − mθ)

Requiring the projections to be equal implies

(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) = (cid:104)bµ(cid:48),γ(cid:48), ψm(rθ)κ(·)(cid:105)
(cid:88)

wµ(cid:48),γ cos(γ(cid:48) − γ − µ(cid:48)θ) =

wµ(cid:48),γ cos(γ(cid:48) − γ − mθ)

∀θ ∈ [0, 2π)

∀θ ∈ [0, 2π)

γ

(cid:88)

γ

⇔ wµ(cid:48),0 cos(γ(cid:48) −µ(cid:48)θ) + wµ(cid:48), π

sin(γ(cid:48) −µ(cid:48)θ) = wµ(cid:48),0 cos(γ(cid:48) −mθ) + wµ(cid:48), π

sin(γ(cid:48) −mθ)

2

2

γ(cid:48) − µ(cid:48)θ = γ(cid:48) − mθ + 2tπ
µ(cid:48)θ = mθ + 2tπ

∀θ ∈ [0, 2π)

∀θ ∈ [0, 2π)

∀θ ∈ [0, 2π),

⇔

⇔

⇔

where we made use of Eq. (20) once again. It follows that µ(cid:48) = m, resulting in the two-dimensional
basis

(cid:26)

KSO(2)

ψm←ψn

=

bm,γ(φ) =

(cid:20)cos(mφ + γ)
sin(mφ + γ)

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:110)

γ ∈

0,

(cid:111)(cid:27)

π
2

(28)

of equivariant kernels for m > 0 and n = 0. This basis is explicitly given in the lower left cell of
Table 8.

2 and 1-dimensional irreps:
The case for 2-dimensional input and 1-dimensional output representations, i.e. ρin = ψSO(2)
ρout = ψSO(2)
m = 0 and n > 0 is therefore given by

and
, is identical to the previous one up to a transpose. The ﬁnal two-dimensional basis for

n

0

(cid:26)

KSO(2)

ψm←ψn

=

bn,γ(φ) = [cos(nφ + γ)

sin(nφ + γ)]

γ ∈

0,

(29)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:110)

(cid:111)(cid:27)

π
2

as shown in the upper right cell of Table 8.

37

F.4.2 Derivation for the reﬂection group

The action of the reﬂection group ({±1}, ∗) on R2 depends on a choice of reﬂection axis, which we
specify by an angle β. More precisely, the element s ∈ ({±1}, ∗) acts on x = (r, φ) ∈ R2 as

s.x(r, φ) := x(r, s.φ) := x(r, 2βδs,−1 + sφ) =

(cid:26)x(r, φ)

x(r, 2β − φ)

if s = 1
if s = (cid:57)1 .

The kernel constraint for the reﬂection group is therefore being made explicit by

κ(r, s.φ) = ρout(s)κ(r, φ)ρin(s)−1
⇔ κ(r, δs,−12β + sφ) = ρout(s)κ(r, φ)ρin(s)−1
⇔ κ(r, δs,−12β + sφ) = ρout(s)κ(r, φ)ρin(s)

∀s ∈ ({±1}, ∗), φ ∈ [0, 2π)

∀s ∈ ({±1}, ∗), φ ∈ [0, 2π)
∀s ∈ ({±1}, ∗), φ ∈ [0, 2π) ,

where we used the identity s−1 = s. For s = +1 the constraint is trivially true. We will thus in the
following consider the case s = −1, that is,

κ(r, 2β − φ) = ρout(−1)κ(r, φ)ρin(−1)

∀φ ∈ [0, 2π) .

In order to simplify this constraint further we deﬁne a transformed kernel κ(cid:48)(r, φ) := κ(r, φ + β)
which is oriented relative to the reﬂection axis. The transformed kernel is then required to satisfy

κ(cid:48)(r, β − φ) = ρout(−1)κ(cid:48)(r, φ − β)ρin(−1)

∀φ ∈ [0, 2π) ,

which, with the change of variables φ(cid:48) = φ − β , reduces to the constraint for equivariance under
reﬂections around the x-axis, i.e. the case for β = 0 :

κ(cid:48)(r, −φ(cid:48)) = ρout(−1)κ(cid:48)(r, φ(cid:48))ρin(−1)

∀φ(cid:48) ∈ [0, 2π) .

As a consequence we can retrieve kernels equivariant under reﬂections around the β-axis through

κ(r, φ) := κ(cid:48)(r, φ − β) .

We will therefore without loss of generality consider the case β = 0 only in the following.

1-dimensional irreps:

The reﬂection group ({±1}, ∗) has only two irreps, namely the trivial representation ψ({±1},∗)
and the sign-ﬂip representation ψ({±1},∗)
kernel of form κ : R2 → R1×1 exists. Note that we can write the irreps out as ψ({±1},∗)
particular ψ({±1},∗)

(s) = 1
(s) = s. Therefore only the 1-dimensional case with a
(s) = sf , in

(−1) = (−1)f .

1

0

f

f

Consider the output and input irreps ρout = ψ({±1},∗)
the usual 1-dimensional Fourier basis for scalar functions in L2(S1) as before:

and ρin = ψ({±1},∗)

j

i

(with i, j ∈ {0, 1}) and

(cid:40)

bµ,γ(φ) = cos(µφ + γ)

µ ∈ N, γ ∈

(cid:26){0}

if µ = 0

(cid:41)

{0, π/2} otherwise

(30)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Deﬁning the reﬂection operator S by its action (S κ) (φ) := κ(−φ), we require the projections of
both sides of the kernel constraint on the same basis element to be equal as usual. Speciﬁcally, for a
particular basis bµ(cid:48),γ(cid:48):

(cid:104)bµ(cid:48),γ(cid:48), S κ(cid:105) = (cid:104)bµ(cid:48),γ(cid:48), ψ({±1},∗)

(−1)κ(·)ψ({±1},∗)

(−1)(cid:105)

j

i

38

dφ cos(µ(cid:48)φ + γ(cid:48)) cos(−µφ + γ)

dφ

(cos((µ(cid:48) + µ)φ + (γ(cid:48) − γ)) + cos((µ(cid:48) − µ)φ + (γ(cid:48) + γ)))

1
2

The lhs implies

(cid:104)bµ(cid:48),γ(cid:48), S κ(cid:105) =

wµ,γ

dφ bµ(cid:48),γ(cid:48)(φ)bµ,γ(−φ)

(cid:88)

µ,γ

(cid:88)

µ,γ

(cid:88)

µ,γ
(cid:88)

γ

=

=

=

wµ,γ

wµ,γ

(cid:90)

(cid:90)

(cid:90)

1
2π

1
2π

1
2π

1
2

wµ(cid:48),γ

(cos(γ(cid:48) + γ) + δµ(cid:48),0 cos(γ(cid:48) − γ))

while the rhs leads to
(cid:104)bµ(cid:48),γ(cid:48), ψ({±1},∗)
m
(cid:88)

(−1)κ(·)ψ({±1},∗)
(cid:90)

(−1)(cid:105)

n
dφ bµ(cid:48),γ(cid:48)(φ)ψ({±1},∗)

i

wµ,γ

(−1)bµ,γ(φ)ψ({±1},∗)

(−1)

j

=

=

µ,γ

(cid:88)

µ,γ

(cid:90)

1
2π

1
2π

µ,γ

γ

(cid:90)

1
2π

1
2

wµ,γ

dφ cos(µ(cid:48)φ + γ(cid:48))(−1)i cos(µφ + γ)(−1)j

= (−1)i+j (cid:88)

wµ,γ

dφ cos(µ(cid:48)φ + γ(cid:48)) cos(µφ + γ)

= (−1)i+j (cid:88)

wµ(cid:48),γ

(cos(γ(cid:48) − γ) + δµ(cid:48),0 cos(γ(cid:48) + γ)) .

1
2

γ

1
2

γ

Now, we require both sides to be equal, that is,

(cid:88)

wµ(cid:48),γ

(cos(γ(cid:48) + γ) + δµ(cid:48),0 cos(γ(cid:48) (cid:57) γ)) = ((cid:57)1)i+j (cid:88)

wµ(cid:48),γ

(cos(γ(cid:48) (cid:57) γ) + δµ(cid:48),0 cos(γ(cid:48) + γ))

1
2

γ
and again consider two cases for µ(cid:48):

γ

• µ(cid:48) = 0 The basis in Eq.(30) is restricted to the single case γ(cid:48) = 0. Hence:
1
2

(cos(γ) + cos(−γ)) = (−1)i+j (cid:88)

w0,γ

w0,γ

(cid:88)

1
2

γ

(cos(−γ) + cos(γ))

As γ ∈ {0, π

2 } and cos(± π

2 ) = 0:

⇔

⇔

1
2

w0,0

(cos(0) + cos(0)) = (−1)i+jw0,0

(cos(−0) + cos(0))

w0,0 = (−1)i+jw0,0

Which is always true when i = j, while it enforces w0,0 = 0 when i (cid:54)= j.

• µ(cid:48) > 0 In this case we get:

wµ(cid:48),γ

1
2

(cid:88)

γ

cos(γ(cid:48) + γ) = (−1)i+j (cid:88)

wµ(cid:48),γ

cos(γ(cid:48) − γ)

⇔

(1 − (−1)i+j)wµ(cid:48),0 cos(γ(cid:48)) = (1 + (−1)i+j)wµ, π
If i + j ≡ 0 mod 2, the equation becomes sin(γ(cid:48)) = 0 and, so, γ(cid:48) = 0. Otherwise, it
becomes cos(γ(cid:48)) = 0, which means γ(cid:48) = π

2 . Shortly, γ(cid:48) = (i + j mod 2) π
2 .

sin(γ(cid:48))

2

As a result, only half of the basis for β = 0 is preserved:

K({±1},∗),β=0

ψi←ψj

=

(cid:26)

bµ,γ(φ) = cos(µφ + γ)

µ ∈ N, γ = (i + j mod 2)

, µ > 0 ∨ γ = 0

(cid:27)

(31)

1
2

π
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

39

The solution for a general reﬂection axis β is therefore given by

K({±1},∗),β
ψi←ψj

=

(cid:26)

(cid:12)
(cid:12)
bµ,γ(φ) = cos(µ(φ (cid:57) β) + γ)
(cid:12)
(cid:12)

µ ∈ N, γ = (i + j mod 2)

, µ > 0 ∨ γ = 0

π
2

(cid:27)

(32)

which is visualized in Table 10 for the different cases of irreps for i, j ∈ {0, 1}.

F.4.3 Derivation for O(2)

The orthogonal group O(2) is the semi-direct product between the rotation group SO(2) and the
reﬂection group ({±1}, ∗), i.e. O(2) ∼= SO(2) (cid:111) ({±1}, ∗). This justiﬁes a decomposition of the
constraint on O(2)-equivariant kernels as the union of the constraints for rotations and reﬂections.
Consequently, the space of O(2)-equivariant kernels is the intersection between the spaces of SO(2)-
and reﬂection-equivariant kernels.

Proof

Sufﬁciency:

Assume a rotation- and reﬂection-equivariant kernel, i.e. a kernel which for all r ∈ R+
φ ∈ [0, 2π) satisﬁes

0 and

κ(r, rθφ) =

(cid:16)

ResO(2)

SO(2) ρout

(cid:17)
(rθ) κ(r, φ)

(cid:16)

ResO(2)

SO(2) ρin

(cid:17)−1

(rθ)

∀ rθ ∈ SO(2)

ρout(rθ) κ(r, φ) ρ−1

in (rθ)

and

=

=

κ(r, sφ) =

(cid:16)
ResO(2)

({±1},∗) ρout

(cid:17)
(s) κ(r, φ)

(cid:16)

ResO(2)

({±1},∗) ρin

(cid:17)−1

(s)

∀ s ∈ ({±1}, ∗)

Then, for any g = rθs ∈ O(2), the kernel constraint becomes:

ρout(s) κ(r, φ) ρ−1

in (s) .

κ(r, gφ) = ρout(g) κ(r, φ) ρ−1

in (g)
⇔ κ(r, rθsφ) = ρout(rθs) κ(r, φ) ρ−1
in (rθs)
⇔ κ(r, rθsφ) = ρout(rθ)ρout(s) κ(r, φ) ρ−1

in (s)ρ−1

in (rθ) .

Applying reﬂection-equivariance this equation simpliﬁes to
⇔ κ(r, rθsφ) = ρout(rθ) κ(r, sφ) ρ−1

in (rθ) ,

which, applying rotation-equivariance yields

⇔ κ(r, rθsφ) = κ(r, rθsφ) .

Hence any kernel satisfying both SO(2) and reﬂection constraints is also O(2) equivariant.

Trivially, O(2) equivariance implies equivariance under SO(2) and reﬂections. Speciﬁcally, for
any r ∈ R+

0 and φ ∈ [0, 2π), the equation

κ(r, gφ) = ρout(g) κ(r, φ) ρ−1

in (g)

∀ g = rθs ∈ O(2)

Necessity:

implies

κ(r, rθφ) =

ρout(rθ) κ(r, φ) ρ−1

(cid:16)

=

ResO(2)

SO(2) ρout

(cid:17)
(rθ) κ(r, φ)

(cid:16)

in (rθ)
ResO(2)

SO(2) ρin

(cid:17)−1

(rθ)

∀ rθ ∈ SO(2)

40

and

κ(r, sφ) =

ρout(s) κ(r, φ) ρ−1

(cid:16)

=

ResO(2)

({±1},∗) ρout

(cid:17)
(s) κ(r, φ)

(cid:16)

in (s)
ResO(2)

({±1},∗) ρin

(cid:17)−1

(s)

∀ s ∈ ({±1}, ∗).

This observation allows us to derive the kernel space for O(2) by intersecting the previously derived
kernel space of SO(2) with the kernel space of the reﬂection group:
= (cid:8)κ | κ(r, g.φ) = ρout(g)κ(r, φ)ρin(g)−1 ∀ g ∈ O(2)(cid:9)

KO(2)

ρout←ρin

= (cid:8)κ | κ(r, rθ.φ) = ρout(rθ)κ(r, φ)ρin(rθ)−1 ∀ rθ ∈ SO(2)(cid:9)

∩ (cid:8)κ | κ(r, s.φ) = ρout(s)κ(r, φ)ρin(s)−1 ∀ s ∈ ({±1}, ∗)(cid:9)

As O(2) contains all rotations, it does also contain all reﬂection axes. Without loss of generality, we
deﬁne s ∈ O(2) as the reﬂection along the x-axis. A reﬂection along any other axis β is associated
with the group element r2βs ∈ O(2), i.e. the combination of a reﬂection with a rotation of 2β. As a
result, we consider the basis for reﬂection equivariant kernels derived for β = 0 in Eq. (31).

Therefore, to derive a basis associated to a pair of input and output representations ρin and ρout, we
restrict the representations to SO(2) and the reﬂection group, compute the two bases using the results
found in Appendix F.4.1 and in Appendix F.4.2, and, ﬁnally, take their intersection.

2-dimensional irreps:
The restriction of any 2-dimensional irrep ψO(2)
direct sum of the two 1-dimensional irreps of the reﬂection group, i.e. into the diagonal matrix
(cid:20)1 0
0 s

1,n of O(2) to the reﬂection group decomposes into the

({±1},∗) ψ1,n(s) =

(cid:34)
ψ({±1},∗)

⊕ ψ({±1},∗)
1

ψ({±1},∗)

ResO(2)

(s) =

(s)

=

(cid:16)

(cid:17)

(cid:35)

(cid:21)

0

0

0

.

0

ψ({±1},∗)

(s)

1

It follows that the restricted kernel space constraint decomposes into independent constraints on
each entry of the original kernel. Speciﬁcally, for output and input representations ρout = ψO(2)
1,m and
ρin = ψO(2)
(cid:32)

1,n , the constraint becomes

(cid:33)

(cid:33)

(cid:33)

(cid:32)

(cid:32)

ψ({±1},∗)

(s)−1

0

κ(s.x) =

ψ({±1},∗)

(s)

0

ψ({±1},∗)

(s)

1

(cid:124)

(cid:123)(cid:122)

ResO(2)

({±1},∗) ρout(s)

·

(cid:125)

(cid:124)

κ00 κ01

κ10 κ11
(cid:123)(cid:122)
κ(x)

·

(cid:125)

(cid:124)

ψ({±1},∗)

(s)−1

1

(cid:123)(cid:122)

ResO(2)

({±1},∗) ρin(s)

(cid:125)

We can therefore solve for a basis for each entry individually following Appendix F.4.2 to obtain the
complete basis

(cid:8)b00

µ,0 (φ) =

(cid:8)b10
µ, π
2

(φ) =

(cid:21) (cid:12)
(cid:20)cos(µφ) 0
(cid:12)
(cid:12)
0
0
(cid:12)
(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

0
sin(µφ) 0

(cid:20)

0

µ ∈ N (cid:9) ∪ (cid:8)b01
µ, π
2

(φ) =

µ ∈ N+(cid:9) ∪ (cid:8)b11

µ,0 (φ) =

(cid:20)0
0
(cid:20)0
0

sin(µφ)
0

0
cos(µφ)

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

µ ∈ N+(cid:9) ∪

µ ∈ N (cid:9).

Through the same change of basis applied in the ﬁrst paragraph of Appendix F.4.1, we get the
following equivalent basis for the same space:
(cid:26)

(cid:21) (cid:27)

bµ,s(φ) =

(cid:20)cos (µφ) (cid:57) sin (µφ)
cos (µφ)
sin (µφ)

(cid:21) (cid:20)1 0
0 s

= (cid:8)bµ,s(φ) = ψ(µφ)ξ(s)(cid:9)

µ∈Z,s∈{±1} .

µ∈Z,s∈{±1}

(33)

On the other hand, 2-dimensional O(2) representations restrict to the SO(2) irreps of the correspond-
ing frequency, i.e.

ResO(2)

SO(2) ρin = ResO(2)

SO(2) ψO(2)

1,n (rθ) = ψSO(2)

n

(rθ)

41

ResO(2)

SO(2) ρout = ResO(2)

SO(2) ψO(2)

1,m (rθ) = ψSO(2)

m

(rθ).

In Appendix F.4.1, a basis for SO(2)-equivariant kernels with respect to a ψSO(2)
input ﬁeld and
ψSO(2)
output ﬁeld was derived starting from the basis in Eq. (21). Notice that the basis of reﬂection-
m
equivariant kernels in Eq. (33) contains exactly half of the elements in Eq. (21), indexed by γ = 0. A
basis for O(2)-equivariant kernels can be found by repeating the derivations in Appendix F.4.1 for
SO(2)-equivariant kernels using only the subset in Eq. (33) of reﬂection-equivariant kernels. The
resulting two-dimensional O(2)-equivariant basis, which includes the SO(2)-equivariance conditions
(µ = m − sn) and the reﬂection-equivariance conditions (γ = 0), is given by

n

KO(2)

ψi,m←ψj,n

=

(cid:26)

bµ,0,s(φ) = ψ(µφ)ξ(s)

µ = m − sn, s ∈ {±1}

,

(34)

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where i = j = 1 and m, n > 0. See the bottom right cell in Table 9.

1-dimensional irreps:
O(2) has two 1-dimensional irreps ψO(2)
0,0
rotations and each of them corresponds to one of the two reﬂection group’s irreps, i.e.

and ψO(2)

1,0 (see Appendix F.2). Both are trivial under

and

and

Considering output and input representations ρout = ψO(2)

and ρin = ψO(2)

i,0

j,0 , it follows that:

ResO(2)

({±1},∗) ψO(2)

i,0

(s) = ψ({±1},∗)
i

(s)= si

ResO(2)

SO(2) ψO(2)

i,0

(rθ) = ψSO(2)

0

(rθ) = 1.

ResO(2)

ResO(2)

({±1},∗) ρin = ResO(2)
({±1},∗) ρout = ResO(2)
SO(2) ρin = ResO(2)
ResO(2)
SO(2) ρout = ResO(2)

({±1},∗) ψO(2)
({±1},∗) ψO(2)
SO(2) ψO(2)
SO(2) ψO(2)

j,0 = ψ({±1},∗)
i,0 = ψ({±1},∗)
i
= ψSO(2)
0
= ψSO(2)
0

ResO(2)

j,0

i,0

j

In order to solve the O(2) kernel constraint consider again the reﬂectional constraint and the SO(2)
constraint. Bases for reﬂection-equivariant kernels with above representations were derived in
Appendix F.4.2 and are shown in Eq. (31). These bases form a subset of the Fourier basis in Eq. (25)
which is being indexed by γ = (i + j mod 2) π
2 . On the other hand, the full Fourier basis was
restricted by the SO(2) constraint to satisfy µ = 0 and therefore γ = 0, see Eq. (26). Intersecting
both constraints therefore implies i = j, resulting in the O(2)-equivariant basis

KO(2)

ψi,m←ψj,n

=

(cid:26){b0,0(φ) = 1}
∅

if i = j,
else

(35)

for m, n = 0 which is shown in the top left cell in Table 9.

1 and 2-dimensional irreps:
Now we consider the 2-dimensional output representation ρout = ψO(2)
representation ρin = ψO(2)
j,0 .

1,m and the 1-dimensional input

42

Following the same strategy as before we ﬁnd the reﬂectional constraints for these representations to
be given by

κ(s.x) =

ψ({±1},∗)

(s)

0

(cid:32)

(cid:124)

ψ({±1},∗)

(s)

1

(cid:123)(cid:122)

ResO(2)

({±1},∗) ρout(s)

(cid:33)

(cid:32)

(cid:33)

κ00

·

(cid:125)

(cid:124)

κ10
(cid:123)(cid:122)
κ(x)

(cid:125)

(cid:16)

·

(s)−1

(cid:17)

,

j

ψ({±1},∗)
(cid:123)(cid:122)

(cid:124)
ResO(2)

(cid:125)
({±1},∗) ρin(s)

and therefore to decompose into two independent constraints on the entries κ00 and κ10. Solving for
a basis for each entry and taking their union as before we get21
(cid:20)cos(µφ + j π
2 )
0

(cid:20)
0
sin(µφ − j π
2 )

b00
µ (φ) =

b10
µ (φ) =

(cid:21) (cid:27)

(cid:21) (cid:27)

(cid:26)

(cid:26)

∪

,

µ∈N

µ∈N

which, through a change of basis, can be rewritten as

(cid:26)

bµ,j π

2

(φ) =

(cid:20)cos(µφ + j π
2 )
sin(µφ + j π
2 )

(cid:21) (cid:27)

.

µ∈Z

We intersect this basis with the basis of SO(2) equivariant kernels with respect to a ResO(2)
ψSO(2)
0
constraints, that is, γ = j π
one-dimensional basis for O(2)-equivariant kernels for n = 0, m > 0 and i = 1 as

SO(2) ρin =
output ﬁeld as derived in Appendix F.4.1. Both
2 for the reﬂection group and µ = m for SO(2) (see Eq. (27)), deﬁne the

SO(2) ρout = ψSO(2)

input ﬁeld and ResO(2)

m

KO(2)

ψi,m←ψj,n

=

bµ,j π

2

(φ) =

(cid:26)

(cid:20)cos(µφ + j π
2 )
sin(µφ + j π
2 )

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

µ = m

,

see the bottom left cell in Table 9.

(36)

(37)

2 and 1-dimensional irreps:
As already argued in the case for SO(2), the basis for 2-dimensional input representations ρin = ψO(2)
1,n
and 1-dimensional output representations ρout = ψO(2)
is identical to the previous basis up to a
transpose, i.e. it is given by

i,0

KO(2)

ψi,m←ψj,n

=

bµ,i π

2

(φ) = (cid:2)cos(µφ + i π
2 )

sin(µφ + i π

2 )(cid:3)

(cid:26)

(cid:27)

µ = n

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(38)

where j = 1, n > 0 and m = 0. This case is visualized in the top right cell of Table 9.

F.4.4 Derivation for CN

The derivations for CN coincide mostly with the derivations done for SO(2) with the difference
N | p = 0, . . . , N − 1(cid:9) only.
that the projected constraints need to hold for discrete angles θ ∈ (cid:8)p 2π
Furthermore, CN has one additional 1-dimensional irrep of frequencyN/2 if (and only if) N is even.

2-dimensional irreps:
During the derivation of the solutions for SO(2)’s 2-dimensional irreps in Appendix F.4.1, we
assumed continuous angles only in the very last step. The constraint in Eq. (23) therefore holds for

21Notice that for µ = 0 some of the elements of the set are zero and are therefore not part of the basis. We

omit this detail to reduce clutter.

43

CN as well. Speciﬁcally, it demands that for each θ ∈ {p 2π
such that:

N | p = 0, . . . , N − 1} there exists a t ∈ Z

⇔

⇔

(µ(cid:48) − (m − ns(cid:48)))θ = 2tπ

(µ(cid:48) − (m − ns(cid:48)))p

= 2tπ

(µ(cid:48) − (m − ns(cid:48)))p = tN

2π
N

The last result corresponds to a system of N linear congruence equations modulo N which require N
to divide (µ(cid:48) − (m − ns(cid:48)))p for each non-negative integer p smaller than N . Note that solutions of
the constraint for p = 1 already satisfy the constraints for p ∈ 2, . . . , N − 1 such that it is sufﬁcient
to consider

⇔

The resulting basis

(µ(cid:48) − (m − ns(cid:48)))1 = tN

µ(cid:48) = m − ns(cid:48) + tN .

KCN

ψm←ψn

=

(cid:26)

(cid:12)
(cid:12)
bµ,γ,s(φ) = ψ(µφ + γ)ξ(s)
(cid:12)
(cid:12)

µ = m (cid:57) sn + tN, γ ∈

s ∈ {±1}

(39)

(cid:110)

0,

(cid:111)

π
2

(cid:27)

t∈Z

for m, n > 0 thus coincides mostly with the basis 24 for SO(2) but contains solutions for aliased
frequencies, deﬁned by adding tN . The bottom right cell in Table 11 gives the explicit form of this
basis.

1-dimensional irreps:
The same trick could be applied to solve the remaining three cases. However, since CN has an
additional one dimensional irrep of frequency N/2 for even N it is convenient to rederive all
m and ρin = ψCN
cases. We therefore consider ρout = ψCN
n , where m, n ∈ {0, N/2}. Note that
m (θ), ψCN
ψCN
N | p = 0, . . . , N − 1}.

n (θ) ∈ {±1} for θ ∈ {p 2π

We use the same Fourier basis

(cid:40)

bµ,γ(φ) = cos(µφ + γ)

µ ∈ N, γ ∈

(cid:26){0}

if µ = 0

(cid:41)

{0, π/2} otherwise

(40)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

and the same projection operators as used for SO(2).

Since the lhs of the kernel constraint does not depend on the representations considered its projection
(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) is the same found for SO(2):

(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) =

wµ(cid:48),γ (cos((γ(cid:48) − γ) − µ(cid:48)θ) + δµ(cid:48),0 cos(γ(cid:48) + γ))

1
2

(cid:88)

γ

For the rhs we ﬁnd
(cid:104)bµ(cid:48),γ(cid:48), ψCN

n (rθ)−1(cid:105)

m (rθ)κ ψCN
1
2π

=

(cid:90)

dφ bµ(cid:48),γ(cid:48)(φ)ψCN

m (rθ)κ(φ)ψCN

n (rθ)−1 ,

which by expanding the kernel in the linear combination of the basis and writing the respresentations
out yields:

=

=

(cid:88)

µ,γ

(cid:88)

µ,γ

(cid:90)

(cid:90)

1
2π

1
2π

wµ,γ

dφ bµ(cid:48),γ(cid:48)(φ) cos(mθ)bµ,γ(φ) cos(nθ)−1

wµ,γ

dφ cos(µ(cid:48)φ + γ(cid:48)) cos(mθ) cos(µφ + γ) cos(nθ)−1

44

Since cos(nθ) ∈ {±1} the inverses can be dropped and terms can be collected via trigonometric
identities:

wµ,γ

(cid:90)

1
2π

dφ cos(µ(cid:48)φ + γ(cid:48)) cos(mθ) cos(µφ + γ) cos(nθ)

wµ,γ cos(mθ) cos(nθ)

dφ cos(µ(cid:48)φ + γ(cid:48)) cos(µφ + γ)

(cid:90)

(cid:90)

1
2π

1
4π

1
2

wµ,γ cos((±m ± n)θ)

(cid:17)
(cid:16)
cos((µ(cid:48) (cid:57) µ)φ + γ(cid:48) (cid:57) γ) + cos((µ(cid:48) + µ)φ + γ(cid:48) + γ)

dφ

wµ,γ cos((±m ± n)θ)

(δµ,µ(cid:48) cos(γ(cid:48) − γ) + δµ+µ(cid:48),0 cos(γ(cid:48) + γ))

wµ(cid:48),γ cos((±m ± n)θ) (cos(γ(cid:48) − γ) + δµ(cid:48),0 cos(γ(cid:48) + γ))

We require the projections to be equal for each θ = p 2π

N with p ∈ {0, . . . , N − 1}:

(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) =

(cid:68)
bµ(cid:48),γ(cid:48), ψCN

m (rθ)κ ψCN

n (rθ)−1(cid:69)

⇔

wµ(cid:48),γ (cos((γ(cid:48) − γ) − µ(cid:48)θ) + δµ(cid:48),0 cos(γ(cid:48) + γ)) =

wµ(cid:48),γ cos((±m ± n)θ)

cos(γ(cid:48) − γ) + δµ(cid:48),0 cos(γ(cid:48) + γ)

(cid:16)

(cid:17)

=

(cid:88)

µ(cid:48),γ

Again, we consider two cases for µ(cid:48):

• µ(cid:48) = 0 : The basis in Eq.(40) is restricted to the single case γ(cid:48) = 0.

w0,γ(cos(−γ)+cos(γ)) = cos((±m±n)θ)

w0,γ

(cid:16)

(cid:17)
cos(−γ)+cos(γ)

(cid:88)

γ

w0,02 cos(0) + w0, π

2

0 = cos((±m ± n)θ) (cid:0)w0,02 cos(0) + w0, π

0(cid:1)

2

w0,0 = cos((±m ± n)θ)w0,0

If cos((±m ± n)θ) (cid:54)= 1, the coefﬁcient w0,0 is forced to 0. Conversely:

(cid:88)

µ,γ

(cid:88)

µ,γ

(cid:88)

µ,γ
(cid:88)

=

=

=

=

=

µ,γ
1
2

(cid:88)

µ(cid:48),γ

(cid:88)

γ

⇔

⇔

⇔ ∃t ∈ Z s.t.

Using θ = p 2π
N :

⇔ ∃t ∈ Z s.t.

⇔ ∃t ∈ Z s.t.

cos((±m ± n)θ) = 1

(±m ± n)θ = 2tπ

(±m ± n)p

2π
N
(±m ± n)p = tN

= 2tπ

• µ(cid:48) > 0 :

(cid:88)

γ

(cid:88)

γ

wµ(cid:48),γ cos(γ(cid:48) − γ − µ(cid:48)θ) = cos((±m ± n)θ)

wµ(cid:48),γ cos(γ(cid:48) − γ)

(cid:88)

γ

⇔

wµ(cid:48),0 cos(γ(cid:48) − µ(cid:48)θ) + wµ(cid:48), π

2

sin(γ(cid:48) − µ(cid:48)θ) =
cos((±m ± n)θ) (cid:0)wµ(cid:48),0 cos(γ(cid:48)) + wµ(cid:48), π

sin(γ(cid:48))(cid:1)

2

Since (±m ± n)θ ∈ {(cid:57)π, 0, π} we have cos((±m ± n)θ) = ±1, therefore:

⇔ wµ(cid:48),0 cos(γ(cid:48) (cid:57) µ(cid:48)θ) + wµ(cid:48), π

sin(γ(cid:48) (cid:57) µ(cid:48)θ) =

2

= wµ(cid:48),0 cos(γ(cid:48) + (±m ± n)θ) + wµ(cid:48), π

sin(γ(cid:48) + (±m ± n)θ)

2

45

Using the property in Eq. (20):
⇔ ∃t ∈ Z s.t.
⇔ ∃t ∈ Z s.t.

γ(cid:48) − µ(cid:48)θ = γ(cid:48) + (±m ± n)θ + 2tπ
µ(cid:48)θ = (±m ± n)θ + 2tπ

Using θ = p 2π
N :

⇔ ∃t ∈ Z s.t.

⇔ ∃t ∈ Z s.t.
⇔ ∃t ∈ Z s.t.

µ(cid:48)p

= (±m ± n)p

2π
N
µ(cid:48)p = (±m ± n)p + tN

2π
N

+ 2tπ

(±m ± n + µ(cid:48))p = tN

In both cases µ(cid:48) = 0 and µ(cid:48) > 0 we thus ﬁnd the constraints

∀p ∈ {0, 1, . . . , N − 1} ∃t ∈ Z s.t.

(±m ± n + µ(cid:48))p = tN .

It is again sufﬁcient to consider the constraint for p = 1 which results in solutions with frequencies
µ(cid:48) = ±m ± n + tN . As (±m ± n) ∈ (cid:8)0, ± N
2 , ±N (cid:9), all valid solutions are captured by µ(cid:48) = (m + n
mod N ) + tN , resulting in the basis

KCN

ψm←ψn

=

(cid:26)

(cid:12)
(cid:12)
bµ,γ(φ) = cos(µφ+γ)
(cid:12)
(cid:12)

µ = (m+n mod N )+tN, γ ∈

0,

, µ (cid:54)= 0∨γ = 0

(cid:110)

(cid:111)

π
2

(cid:27)

t∈N

(41)

for n, m ∈ (cid:8)0, N

(cid:9). See the top left cells in Table 11.

2

1 and 2-dimensional irreps Next consider a 1-dimensional irrep ρin = ψCN
the input and a 2-dimensional irrep ρout = ψCN
the kernel constraint on the basis introduced in Eq. (27).

2 } in
m in the output. We derive the solutions by projecting

n with n ∈ {0, N

For the lhs the projection coincides with the result found for SO(2) as before:

(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) =

wµ(cid:48),γ cos((γ(cid:48) − γ) − µ(cid:48)θ)

(cid:88)

γ

An expansion and projection of rhs gives:

(cid:104)bµ(cid:48),γ(cid:48), ψCN

m (rθ)κ(·)ψCN
(cid:90)

n (rθ)−1(cid:105)

dφ bµ(cid:48),γ(cid:48)(φ)T ψCN

m (rθ)κ(φ)ψCN

n (rθ)−1

wµ,γ

dφ bµ(cid:48),γ(cid:48)(φ)T ψCN

m (rθ)

(cid:20)cos(µφ + γ)
sin(µφ + γ)

(cid:21)

n (rθ)−1
ψCN

wµ,γ

dφ [cos(µ(cid:48)φ + γ(cid:48))

sin(µ(cid:48)φ + γ(cid:48))] ψCN

m (rθ)

(cid:20)cos(µφ + γ)
sin(µφ + γ)

(cid:21)
n (rθ)(cid:57)1
ψCN

wµ,γ

dφ cos(µ(cid:48)φ + γ(cid:48) − µφ − γ − mθ)

ψCN
n (rθ)−1 .

(cid:19)

(cid:90)

1
2π

(cid:90)

1
2π
(cid:18) 1
2π

(cid:90)

The integral is non-zero only if the frequency is 0, i.e. iff µ(cid:48) = µ:

wµ(cid:48),γ cos(γ(cid:48) − γ − mθ)ψCN

n (rθ)−1

wµ(cid:48),γ cos(γ(cid:48) − γ − mθ) cos(±nθ)

46

=

=

=

=

1
2π
(cid:88)

µ,γ

(cid:88)

µ,γ

(cid:88)

µ,γ

=

=

(cid:88)

γ
(cid:88)

γ

⇔

⇔

⇔

⇔

⇔

⇔

⇔

⇔

(cid:27)

(cid:27)

(cid:26)

(cid:26)

p

p

2π
N
2π
N

2

sin(γ(cid:48) (cid:57)(m±n)θ)
2π
N

∀θ ∈

(cid:26)

p

(cid:27)

∀θ ∈

∀θ ∈

(cid:27)

(cid:27)

(cid:26)

(cid:26)

p

p

2π
N
2π
N

∀p ∈ {0, . . . , N (cid:57)1}

∀p ∈ {0, . . . , N (cid:57)1}

Since ±nθ = pπ for some p ∈ N one has sin(±nθ) = 0 which allows to add the following zero
summand and simplify:

wµ(cid:48),γ (cos(γ(cid:48) − γ − mθ) cos(±nθ) − sin(γ(cid:48) − γ − mθ) sin(±nθ))

=

=

(cid:88)

γ
(cid:88)

γ

wµ(cid:48),γ cos(γ(cid:48) − γ − (m ± n)θ)

Requiring the projections to be equal then yields:

(cid:104)bµ(cid:48),γ(cid:48), Rθκ(cid:105) = (cid:104)bµ(cid:48),γ(cid:48), ψCN

m (rθ)κ(·)ψCN

n (rθ)−1(cid:105)

∀θ ∈

(cid:88)

wµ(cid:48),γ cos(γ(cid:48) − γ − µ(cid:48)θ) =

wµ(cid:48),γ cos(γ(cid:48) − γ − (m ± n)θ) ∀θ ∈

(cid:88)

γ

γ
wµ(cid:48),0 cos(γ(cid:48) (cid:57)µ(cid:48)θ)+wµ(cid:48),π

2

sin(γ(cid:48) (cid:57)µ(cid:48)θ) = wµ(cid:48),0 cos(γ(cid:48) (cid:57)(m±n)θ)+wµ(cid:48),π

Using the property in Eq. (20), this requires that for each θ there exists a t ∈ Z such that:

Since θ = p 2π

N with p ∈ {0, . . . , N − 1} we ﬁnd that

γ(cid:48) − µ(cid:48)θ = γ(cid:48) − (m ± n)θ + 2tπ

µ(cid:48)θ = (m ± n)θ + 2tπ

µ(cid:48)p

2π
N

= (m ± n)p

2π
N
µ(cid:48)p = (m ± n)p + tN
µ(cid:48) = (m ± n) + tN

+ 2tπ

µ(cid:48) − (m ± n) = tN ,

which implies that N needs to divide µ(cid:48) − (m ± n). It follows that the condition holds also for any
other p. This gives the basis

(cid:26)

KCN

ψm←ψn

=

bµ,γ(φ) =

(cid:20)cos(µφ + γ)
sin(µφ + γ)

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

µ = (m ± n) + tN, γ ∈

0,

(42)

(cid:110)

(cid:111)(cid:27)

π
2

t∈Z

for m > 0 and n ∈ (cid:8)0, N

(cid:9); see the bottom left cells in Table 11.

2 and 1-dimensional irreps:
The basis for 2-dimensional input and 1-dimensional output representations, i.e. ρin = ψCN
ρout = ψCN

2 }, is identical to the previous one up to a transpose:

m with n > 0 and m ∈ {0, N

n and

(cid:26)

KCN

ψm←ψn

=

bµ,γ(φ) = [cos(µφ + γ) sin(µφ + γ)]

µ = (±m+n)+tN, γ ∈

0,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:110)

(cid:111)(cid:27)

π
2

t∈Z

(43)

for n > 0 and m ∈ (cid:8)0, N

(cid:9). See the top right cells in Table 11.

2

2

47

F.4.5 Derivation for DN

A solution for DN can easily be derived by repeating the process done for O(2) in Appendix F.4.3
but starting from the bases derived for CN in Appendix F.4.4 instead of those for SO(2).

In contrast to the case of O(2)-equivariant kernels, the choice of reﬂection axis β is not irrelevant
since DN does not act transitively on axes. More precisely, the action of DN deﬁnes equivalence
classes β ∼= β(cid:48) ⇔ ∃ 0 ≤ n < N : β = β(cid:48) + n 2π
N of axes which can be labeled by representatives
β ∈ [0, 2π
N ). For the same argument considered in Appendix F.4.2 we can without loss of generality
consider reﬂections along the axis β = 0 in our derivations and retrieve kernels κ(cid:48), equivariant to
reﬂections along a general axis β, as κ(cid:48)(r, φ) = κ(r, φ − β).

2-dimensional irreps:
For 2-dimensional input and output representations ρin = ψDN

1,n and ρout = ψDN

1,m, the ﬁnal basis is

KDN

ψi,m←ψj,n

(cid:26)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

bµ,0,s(φ) = ψ(µφ)ξ(s)

µ = m − sn + tN, s ∈ {±1}

(44)

(cid:27)

t∈Z

where i = j = 1 and m, n > 0. These solutions are written out explicitly in the bottom right of
Table 12.

1-dimensional irreps:
DN has 1-dimensional representations ρin = ψDN
we ﬁnd the bases

KDN

ψi,m←ψj,n

=

(cid:40)

(cid:12)
(cid:12)
bµ,γ(φ) = cos(µφ + γ)
(cid:12)
(cid:12)

j,n and ρout = ψDN

i,m for m, n ∈ {0, N

2 }. In these cases

µ = (m + n mod N ) + tN,

(45)

γ = (i + j mod 2)

, µ (cid:54)= 0 ∨ γ = 0

π
2

(cid:41)

t∈N

which are shown in the top left cells of Table 12.

1 and 2-dimensional irreps:
For 1-dimensional input and 2-dimensional output representations, that is, ρin = ψDN
with i = 1, m > 0 and n ∈ {0, N

2 }, the kernel basis is given by:

j,n and ρout = ψDN

1,m

KDN

ψi,m←ψj,n

=

bµ,γ(φ) =

(cid:26)

(cid:20)cos(µφ + γ)
sin(µφ + γ)

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)

µ = (m ± n) + tN, γ = j

(46)

(cid:27)

π
2

t∈Z

See the bottom left of Table 12.

2 and 1-dimensional irreps:
Similarly, for 2-dimensional input and 1-dimensional output representations ρin = ψDN
ψDN
i,m with j = 1, n > 0 and m ∈ {0, N
2 }, we ﬁnd:

1,n and ρout =

(cid:26)

KDN

ψi,m←ψj,n

=

bµ,γ(φ) = [cos(µφ + γ)

sin(µφ + γ)]

µ = (±m + n) + tN, γ = i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

π
2

t∈Z

(47)

Table 12 shows these solutions in its top right cells.

48

F.4.6 Kernel constraints at the origin

Our derivations rely on the fact that the kernel constraints restrict only the angular parts of the
unconstrained kernel space L2(R2)cout×cin which suggests an independent solution for each radius
r ∈ R+ ∪ {0}. Particular attention is required for kernels deﬁned at the origin, i.e. when r = 0.
The reason for this is that we are using polar coordinates (r, φ) which are ambiguous at the origin
where the angle is not deﬁned. In order to stay consistent with the solutions for r > 0 we still deﬁne
the kernel at the origin as an element of L2(S1)cout×cin. However, since the coordinates (0, φ) map
to the same point for all φ ∈ [0, 2π), we need to demand the kernels to be angularly constant, that
is, κ(φ) = κ(0). This additional constraint restricts the angular Fourier bases used in the previous
derivations to zero frequencies only. Apart from this, the kernel constraints are the same for r = 0
and r > 0 which implies that the G-steerable kernel bases at r = 0 are given by restricting the bases
derived in F.4.1, F.4.2, F.4.3, F.4.4 and F.4.5 to the elements indexed by frequencies µ = 0.

F.5 Complex valued representations and Harmonic Networks

Instead of considering real (irreducible) representations we could have derived all results using
complex representations, acting on complex feature maps. For the case of O(2) and DN this would
essentially not affect the derivations since their complex and real irreps are equivalent, that is, they
coincide up to a change of basis. Conversely, all complex irreps of SO(2) and CN are 1-dimensional
which simpliﬁes the derivations in complex space. However, the solution spaces of complex G-
steerable kernels need to be translated back to a real valued implementation. This translation has
some not immediately obvious pitfalls which can lead to an underparameterized implementation in
real space. In particular, Harmonic Networks [12] were derived with a complete solution in complex
space; however, their real valued implementation is using a G-steerable kernel space of half the
dimensionality as ours. We will in the following explain why this is the case.
In the complex ﬁeld, the irreps of SO(2) are given by ψC
k (θ) = eikθ ∈ C with frequencies k ∈ Z.
Notice that these complex irreps are indexed by positive and negative frequencies while their real
counterparts, deﬁned in Appendix F.2, only involve non-negative frequencies. As in [12] we consider
complex feature ﬁelds f C : R2 → C which are transforming according to complex irreps of SO(2).
A complex input ﬁeld f C
out : R2 → C
of type ψC

n is mapped to a complex output ﬁeld f C

in : R2 → C of type ψC

m via the cross-correlation

out = kC (cid:63) f C
f C
in .

(48)

with a complex ﬁlter kC : R2 → C. The (angular part of the) complete space of equivariant kernels
between f C

out was in [12] proven to be parameterized by

in and f C

kC(φ) = w ei(m−n)φ,

where w ∈ C is a complex weight which scales and phase-shifts the complex exponential. We want
to point out that an equivalent parametrization is given in terms of the real and imaginary parts wRe
and wIm of the weight w, i.e.

kC(φ) = wReei(m−n)φ + i wImei(m−n)φ

= wReei(m−n)φ + wImei((m−n)φ+π/2) .

(49)

The real valued implementation of Harmonic Networks models the complex feature ﬁelds f C of type
ψC
k (θ) by splitting them in two real valued channels f R := (f Re, f Im)T which contain their real and
imaginary part. The action of the complex irrep ψC
k (θ) is modeled accordingly by a rotation matrix of
the same, potentially negative22 frequency. A real valued implementation of the cross-correlation (48)
is built using a real kernel k : R2 → R2×2 as speciﬁed by
(cid:20)kRe −kIm
kRe
kIm

=

(cid:21)

(cid:21)

(cid:21)

(cid:63)

.

(cid:20)f Re
out
f Im
out

(cid:20)f Re
in
f Im
in

22This establishes an isomorphism between ψC

k (θ) and ψR

|k|(θ) depending on the sign of k.

49

The complex steerable kernel (49) is then given by
(cid:21)

(cid:20)cos ((m (cid:57) n)φ) (cid:57) sin ((m (cid:57) n)φ)
cos ((m (cid:57) n)φ)
sin ((m (cid:57) n)φ)

k(φ) = wRe

= wRe

ψ ((m − n)φ)

+ wIm

+ wIm

(cid:20)(cid:57) sin ((m (cid:57) n)φ) (cid:57) cos ((m (cid:57) n)φ)
cos ((m (cid:57) n)φ) (cid:57) sin ((m (cid:57) n)φ)

(cid:21)

(cid:16)

ψ

(m − n)φ +

(cid:17)

π
2

(50)

While this implementation models the complex Harmonic Networks faithfully in real space, it does
not utilize the complete SO(2)-steerable kernel space when the real feature ﬁelds are interpreted
as ﬁelds transforming under the real irreps ψR
k as done in our work. More speciﬁcally, the kernel
space used in (50) is only 2-dimensional while our basis (24) for the same case is 4-dimensional. The
additional solutions with frequency m + n are missing.

The lower dimensionality of the complex solution space can be understood by analyzing the relation-
ship between SO(2)’s real and complex irreps. On the complex ﬁeld, the real irreps become reducible
and decomposes into two 1-dimensional complex irreps with opposite frequencies:
(cid:20) 1 −i
1
−i

ψR
k (θ) =

(cid:21) (cid:20)eikθ
0

0
e−ikθ

(cid:20)1
i

i
1

(cid:21)

(cid:21) 1
√
2

1
√
2

Indeed, SO(2) has only half as many real irreps as complex ones since positive and negative
frequencies are conjugated to each other, i.e. they are equivalent up to a change of basis: ψR
k (θ) =
ξ(−1)ψR
−k(θ)ξ(−1). It follows that a real valued implementation of a complex ψC
k ﬁelds as a 2-
dimensional ψR
−k ﬁeld. The intertwiners between two real ﬁelds
of type ψR
m therefore do not only include the single complex intertwiner between complex
ﬁelds of type ψC
m, but four complex intertwiners between ﬁelds of type ψC
±m. The
real parts of these intertwiners correspond to our four dimensional solution space.

k ﬁelds implicitly adds a complex ψC

±n and ψC

n and ψR

n and ψC

In conclusion, [12] indeed found the complete solution on the complex ﬁeld. However, by imple-
menting the network on the real ﬁeld, negative frequencies are implicitly added to the feature ﬁelds
which allows for our larger basis (24) of steerable kernels to be used without adding an overhead.

G Alternative approaches to compute kernel bases and their complexities

The main challenge of building steerable CNNs is to ﬁnd the space of solutions of the kernel space
constraint in Eq. 2. Several recent works tackle this problem for the very speciﬁc case of features
which transform under irreducible representations of SE(3) ∼= (R3, +) (cid:111) SO(3). The strategy
followed in [39, 33, 20, 34] is based on well known analytical solutions and does not generalize to
arbitrary representations. In contrast, [2] present a numerical algorithm to solve the kernel space
constraint. While this algorithm was only applied to solve the constraints for irreps, it generalizes to
arbitrary representations. However, the computational complexity of the algorithm scales unfavorably
in comparison to the approach proposed in this work. We will in the following review the kernel
space solution algorithm of [2] for general representations and discuss its complexity in comparison
to our approach.

The algorithm proposed in [2] is considering the same kernel space constraint

k(gx) = ρout(g)k(x)ρ−1
as in this work. By vectorizing the kernel the constraint can be brought in the form
(cid:16)

in (g) ∀g ∈ G

vec(k) (gx) =

ρout ⊗ (cid:0)ρ−1

in

(cid:1)T (cid:17)

(g) vec(k) (x)

= (ρout ⊗ ρin) (g) vec(k) (x) ,

where the second step assumes the input representation to be unitary, that is, to satisfy ρ−1
in = ρT
in.
A Clebsch-Gordan decomposition, i.e. a decomposition of the tensor product representation into a
direct sum of irreps ψj of G, then yields23

vec(k) (gx) = Q−1 (cid:16)(cid:77)

(cid:17)

ψJ

(g) Q vec(k) (x)

J∈J

23For the irreps of SO(3) it is well known that J = {|j − l|, . . . , j + l} and |J | = 2 min(j, l) + 1.

50

Through a change of variables η(x) := Q vec(k)(x) this simpliﬁes to

which, in turn, decomposes into |J | independent constraints

(cid:16)(cid:77)

η(gx) =

(cid:17)

ψJ

(g)η(x)

J∈J

ηJ (gx) = ψJ (g)ηJ (x) .

Each of these constraints can be solved independently to ﬁnd a basis for each ηJ . The kernel
basis is then found by inverting the change of basis and the vectorization, i.e. by computing
k(x) = unvec(cid:0)Q−1η(x)(cid:1).

For the case that ρin = ψj and ρout = ψl are Wigner D-matrices, i.e. irreps of SO(3), the change of
basis Q is given by the Clebsch-Gordan coefﬁcients of SO(3). These well known solutions were
used in [39, 33, 20, 34] to build the basis of steerable kernels. Conversely, the authors of [2] solve for
the change of basis Q numerically. Given arbitrary unitary representations ρin and ρout the numerical
algorithm solves for the change of basis in

(cid:0)ρin ⊗ ρout

(cid:1)(g) = Q−1

(cid:33)

ψJ (g)

Q

(cid:32)

(cid:77)

J∈J

∀g ∈ G

⇔

0 = Q(cid:0)ρin ⊗ ρout

(cid:1)(g) −

ψJ (g)

Q

∀g ∈ G .

(cid:33)

(cid:32)

(cid:77)

J∈J

This linear constraint on Q, which is a speciﬁc instance of the Sylvester equation, can be solved by
vectorizing Q, i.e.

(cid:104)
I ⊗ (cid:0)ρin ⊗ ρout

(cid:1)(g) −

(cid:16)(cid:77)

(cid:17)

ψJ

J∈J

(cid:105)

(g) ⊗ I

vec(Q) = 0

∀g ∈ G ,

where I is the identity matrix on Rdim(ρin⊗ρout) = Rdim(ρin) dim(ρout) and vec(Q) ∈ Rdim(ρin)2 dim(ρout)2
.
In principle there is one Sylvester equation for each group element g ∈ G, however,
it
is sufﬁcient to consider the generators of G only, since the solutions found for the gener-
ators will automatically hold for all group elements. One can therefore stack the matrices
(cid:104)
I ⊗ (cid:0)ρin ⊗ ρout
for the generators of G into a bigger matrix and solve
for Q as the null space of this stacked matrix. The linearly independent solutions QJ in the null space
correspond to the Clebsch-Gordan coefﬁcients for J ∈ J .

(cid:1)(g) −

J∈J ψJ

(g) ⊗ I

(cid:16)(cid:76)

(cid:17)

(cid:105)

This approach does not rely on the analytical Clebsch-Gordan coefﬁcients, which are only known for
speciﬁc groups and representations, and therefore works for any choice of representations. However,
applying it naively to large representations can be extremely expensive. Speciﬁcally, computing
the null space to solve the (stacked) Sylvester equation for G generators of G via a SVD, as done
in [2], scales as O(cid:0) dim(ρin)6 dim(ρout)6G(cid:1). This is the case since the matrix which is multiplying
vec(Q) is of shape dim(ρin)2 dim(ρout)2G × dim(ρin)2 dim(ρout)2. Moreover, the change of basis
matrix Q itself has shape dim(ρin) dim(ρout) × dim(ρin) dim(ρout) which implies that the change
of variables24 from η to k has complexity O(cid:0) dim(ρin)2 dim(ρout)2(cid:1). In [2] the authors only use
irreducible representations which are relatively small such that the bad complexity of the algorithm is
negligible.

In comparison, the algorithm proposed in this work is based on an individual decomposition of the
representations ρin and ρout into irreps and leverages the analytically derived kernel space solutions
between the irreps of G ≤ O(2). The independent decomposition of the input and output represen-
tations leads to a complexity of only O(cid:0)(cid:0) dim(ρin)6 + dim(ρin)6(cid:1)G(cid:1). We further apply the input
and output changes of basis Qin and Qout independently to the irreps kernel solutions κij which
leads to a complexity of O(cid:0) dim(ρin) dim(ρout)2 + dim(ρout) dim(ρin)2(cid:1). The improved complexity
of our implementation makes working with large representations as used in this work, for instance
dim(ρD20

reg ) = 40, possible.

24No inversion from Q to Q−1 is necessary if the Sylvester equation is solved directly for Q−1.

51

H Additional information on the training setup

layer

output ﬁelds

layer

output ﬁelds

conv block 7 × 7 (pad 1)
conv block 5 × 5 (pad 2)
max pooling 2 × 2
conv block 5 × 5 (pad 2)
conv block 5 × 5 (pad 2)
max pooling 2 × 2
conv block 5 × 5 (pad 2)
conv block 5 × 5
invariant projection
global average pooling
fully connected
fully connected + softmax

16
24
24
32
32
32
48
64
64
64
64
10

Table 13: Basic model architecture from which
all models for the MNIST benchmarks in Tables 3
and 4 are being derived. Each convolution block
includes a convolution layer, batch-normalization
and a nonlinearity. The ﬁrst fully connected layer
is followed by batch-normalization and ELU. The
width of each layer is expressed as the number of
ﬁelds of a regular C16 model with approximately
the same number of parameters.

conv block 9 × 9
conv block 7 × 7 (pad 3)
max pooling 2 × 2
conv block 7 × 7 (pad 3)
conv block 7 × 7 (pad 3)
max pooling 2 × 2
conv block 7 × 7 (pad 3)
conv block 5 × 5
invariant projection
global average pooling
fully connected
fully connected
fully connected + softmax

24
32
32
36
36
36
64
96
96
96
96
96
10

Table 14: Model architecture for the ﬁnal MNIST-
rot experiments (replicated from [7]). Each fully
connected layer follows a dropout layer with p =
0.3; the ﬁrst two fully connected layers are fol-
lowed by batch normalization and ELU. The width
of each layer is expressed in terms of regular fea-
ture ﬁelds of a C16 model.

H.1 Benchmarking on transformed MNIST datasets

Each model reported in Sections 3.1, 3.2 and 3.3 is derived from the architecture reported in Table 13.
The width of each model’s layers is thereby scaled such that the total number of parameters is matched
and the relative width of layers coincides with that reported in Table13. Training is performed with
a batch size of 64 samples, using the Adam optimizer. The learning rate is initialized to 10−3 and
decayed exponentially by a factor of 0.8 per epoch, starting after a burn in phase of 10 epochs. We
train each model for 30 epochs and test the model which performed best on the validation set. A
weight decay of 10−7 is being used for all convolutional layers and the ﬁrst fully connected layer. In
all experiments, we build steerable bases with Gaussian radial proﬁles of width σ = 0.6 for all except
the outermost ring where we use σ = 0.4. We apply a strong bandlimiting policy which permits
frequencies up to 0, 2, 2 for radii 0, 1, 2 in a 5×5 kernel and up to 0, 2, 3, 2 for radii 0, 1, 2, 3 in a 7×7
kernel. The strong cutoff in the rings of maximal radius is motivated by our empirical observation
that these rings introduce a relatively high equivariance error for higher frequencies. This is the case
since the outermost ring ranges out of the sampled kernel support. During training, data augmentation
with continuous rotations and reﬂections is performed (if these are present in the dataset) to not
disadvantage non-equivariant models. In the models using group restriction, the restriction operation
is applied after the convolution layers but before batch normalization and non-linearities.

H.2 Competitive runs on MNIST rot

In Table 5 we report the performances of some of our best models. Our experiments are based on the
best performing, C16-equivariant model of [7] which deﬁned the state of the art on rotated MNIST
at the time of writing. We replicate their model architecture, summarized in Table 14, though our
models have a different frequency bandlimit and width σ for the Gaussian radial proﬁles as discussed
in the previous subsection. As before, the table reports the width of each layer in terms of number of
ﬁelds in the C16 regular model.

As commonly done, we train our ﬁnal models on the 10000 + 2000 training and validation samples.
Training is performed for 40 epochs with an initial learning rate 0.015, which is being decayed by a
factor of 0.8, starting after 15 epochs. As before, we use the Adam optimizer with a batch size of
64, this time using L1 and L2 regularization with a weight of 10−7. The fully connected layers are
additionally regularized using dropout with a probability of p = 0.3. We are again using train time
augmentation.

52

H.3 CIFAR experiments

The equivariant models used in the experiments on CIFAR-10 and CIFAR-100 are adapted from the
original WideResNet models by replacing conventional with G-steerable convolutions and scaling
the number of feature ﬁelds such that the total number of parameters is preserved. For blocks which
are equivariant under D8 or C8 we use 5 × 5 kernels instead of 3 × 3 kernels to allow for higher
frequencies. All models use regular feature ﬁelds in all but the ﬁnal convolution layer, which maps
to a scalar ﬁeld (conv2triv) to produce invariant predictions. We use a frequency cut-off of 3 times
the ring’s radius, e.g. 0, 3, 6 for rings of radii 0, 1, 2. These higher bandlimits in comparison to
the MNIST experiments are motivated by the fact that the corresponding bases introduce small
discretization errors, which is no problem for the classiﬁcation of natural images. In the contrary, this
leads to the models having a strong bias towards being equivariant, but might allow them to break
equivariance if necessary. The widths of the bases’ rings is chosen to be σ = 0.45 in all rings.

The training process is the same as used for WideResNets: we train for 200 epochs with a batch size
of 128. We optimize the model with SGD, using an initial learning rate of 0.1, momentum 0.9 and
a weight decay of 5 · 10−4. The learning rate is decayed by a factor of 0.2 every 60 epochs. We
perform a standard data augmentation with random crops, horizontal ﬂips and normalization. No
CutOut is done during the normal experiments but it is used in the AutoAugment policies.

H.4 STL-10 experiments

The models for our STL-10 experiments are adapted from [43]. However, according to an issue25 in
the authors’ GitHub repository, the publication states some model parameters and the training setup
wrongly. Our adaptations are therefore based on the setting reported on GitHub. Speciﬁcally, we
use patches of 60 × 60 pixels for cutout and the stride of the ﬁrst convolution layer in the ﬁrst block
is 2 instead of 1. Moreover, we normalize input features using CIFAR-10 statistics. Though these
statistics are very close to the statistics of STL-10, they might, as the authors of [43] suggest, cause
non-negligible changes in performance because of the small training set size of STL-10.

As before, regular feature ﬁelds are used throughout the whole model except for the last convolution
layer which maps to trivial ﬁelds. In the small model, which does not preserve the number of
parameters but the number of channels, we still scale up the number of output channels of the very
ﬁrst convolution layer (before the ﬁrst residual block). As the ﬁrst convolution layer originally
has 16 output channels and our model is initially equivariant to D8 (whose regular representation
spans 16 channels), the ﬁrst convolution layer would only be able to learn 1 single independent ﬁlter
(repeated 16 times, rotated and reﬂected). Hence, we increase the number of output channels of the
16 = 4) leading to 4 · 16 = 64 channels,
ﬁrst convolution layer by the square root of the group size (
i.e. 64/16 = 4 regular ﬁelds. We use a ring width of σ = 0.6 for the kernel basis except for the
outermost ring where we use σ = 0.4 and use a frequency cut-off factor of 3 for the rings’ radii, i.e.
cutoffs of 0, 3, 6, . . . .

√

We are again exactly replicating the training process as reported in the publication [43]. Only the
labeled subset of the training set is used, that is, the 100000 unlabeled training images are discarded.
Training is performed for 1000 epochs with a batch size of 128, using SGD with Nesterov momentum
of 0.9 and weight decay of 5 · 10−4. The learning rate is initialized to 0.1 and decayed by a factor of
5 at 300, 400, 600 and 800 epochs. During training, we perform data augmentation by zero-padding
with 12 pixels and randomly cropping patches of 96 × 96 pixels, mirroring them horizontally and
applying CutOut.

In the data ablation study, reported in Figure 4, we use the same models and training procedure as in
the main experiment on the full STL-10 dataset. For every single run, we generate new datasets by
mixing the original training, validation and test set and sample reduced datasets such that all classes
are balanced. The results are averaged over 4 runs on each of the considered training set sizes of
250, 500, 1000, 2000 or 4000. The validation and test sets contain 1000 and 8000 images, which are
resampled in each run as well.

25https://github.com/uoguelph-mlrg/Cutout/issues/2

53

I Additional information on the irrep models

SO(2) models We experiment with some variants (rows 37-44) of the Harmonic Network model in
row 30 of Table 3, varying in either the non-linearity or the invariant map applied. All of these models
are therefore to be analyzed relative to this baseline. First, we try to use squashing nonlinearities [45]
(row 37) instead of norm-ReLUs on each non-trivial irrep. This variant performs consistently worse
than the original model. In the baseline variant, we generate invariant features via a convolution to
scalar ﬁelds in the last layer (conv2triv). This, however, reduces the utilization of high frequency
irrep ﬁelds in the penultimate layer. The reason for this is that the kernel space for mappings from
high frequency- to scalar ﬁelds consists of kernels of a high angular frequency, which will be cut off
by our bandlimiting. To overcome this problem, we propose to instead compute the norms of all
non-trivial ﬁelds to produce invariant features. This enables us to use all irreps in the output of the last
convolutional layer. However, we ﬁnd that combining invariant norm mappings with norm-ReLUs
does not improve on the baseline model, see row 38. In row 39 we consider a variant which applies
norm-ReLUs on the direct sum of multiple non-trivial irrep ﬁelds, each with multiplicity 1, together
(shared norm-ReLU), while the scalar ﬁelds are still being acted on by ELUs. This is legitimate
since the direct sum of unitary representations is itself unitary. After the last convolutional layer,
the invariant projection preserves the trivial ﬁelds but computes the norm of each composed ﬁeld.
This model signiﬁcantly outperforms all previous variants on all datasets. The model in row 40
additionally merges the scalar ﬁelds to such combined ﬁelds instead of treating them independently.
This architecture performs signiﬁcantly worse than the previous variants.

We further explore four different variations which are applying gated nonlinearities (rows 41-44).
These models distinguish from each other by 1) their mapping to invariant features and 2) whether
the gate is being applied to each non-trivial ﬁeld independently or being shared between multiple
non-trivial ﬁelds. We ﬁnd that the second choice, i.e. sharing gates, does not signiﬁcantly affect the
performances (row 41 vs. 42 and 43 vs. 44). However, mapping to invariant features by taking the
norm of all non-trivial ﬁelds performs consistently better than applying conv2triv. Overall, gated
nonlinearities perform signiﬁcantly better than any other choice of nonlinearity on the tested SO(2)
irrep models.

O(2) models Here we will give more details on the O(2)-speciﬁc operations which we introduce to
improve the performance of the O(2)-equivariant models, reported in rows 45-57 of Table 3.

• O(2)-conv2triv: As invariant map of the O(2) irrep models in rows 46-49 and 54 we are designing
a last convolution layer which is mapping to an output representation ρout = ψO(2)
0,0 ⊕ ψO(2)
1,0 ,
that is, to scalar ﬁelds f0,0 and sign-ﬂip ﬁelds f1,0 in equal proportions. Since the latter
are not invariant under reﬂections, we are in addition taking their absolute value. The
resulting, invariant output features are then multiple ﬁelds f0,0 ⊕ |f1,0|. The motivation for
not convolving to trivial representations of O(2) directly via conv2triv is that the steerable
kernel space for mappings between irreps of O(2) does not allow for mapping between ψO(2)
0,0
and ψO(2)
1,0

(see Table 9), which would lead to dead neurons.

SO(2) ψSO(2)

The models in rows 50-53, 56 and 57 operate on IndO(2)
-ﬁelds whose representa-
tions are induced from the irreps of SO(2). Per deﬁnition, this representation acts on fea-
ture vectors f in Rdim(ψSO(2)
) ⊗ R| O(2):SO(2)|, which we treat in the following as functions
f : O(2)/ SO(2) → Rdim(ψSO(2)
). We further identify the coset s SO(2) in the quotient space
O(2)/ SO(2) by its representative R(s SO(2)) := s ∈ ({±1}, ∗) in the reﬂection group. Eq. 10 de-
ﬁnes the action of the induced representation on a feature vector by
(cid:16)(cid:2) IndO(2)

(s SO(2)) := ψSO(2)

(cid:3)(˜r˜s) f

(cid:17)

k

k

k

SO(2) ψSO(2)

k

(cid:0) h (cid:0)˜r˜sR((˜r˜s)−1s SO(2))(cid:1)(cid:1) f (cid:0)(˜r˜s)−1s SO(2)(cid:1)
(cid:0) h(˜rs)(cid:1) f (cid:0)˜ss SO(2)(cid:1)

k
= ψSO(2)
k

=

(cid:40) ψSO(2)
k
ψSO(2)
k

(cid:0)˜r) f (cid:0)˜ss SO(2)(cid:1)
(cid:0)˜r−1) f (cid:0)˜ss SO(2)(cid:1)

for s = +1

for s = −1 ,

54

where we used Eq. 8 to compute

h(˜rs) := R(cid:0)˜rs SO(2)(cid:1)−1

˜rs = s−1˜rs =

(cid:26)˜r

˜r−1

for s = +1
for s = −1 .

Intuitively, this action describes a permutation of the subﬁelds (indexed by s) via the reﬂection ˜s and a
rotation of the subﬁelds by ˜r and ˜r−1, respectively. Speciﬁcally, for k = 0, the induced representation
is for all ˜r instantiated by


(cid:20)1

0
(cid:20)0

1
that is, it coincides with the regular representation of the reﬂection group. Similarly, for k > 0, it is
for all ˜r given by the 4 × 4 matrices

SO(2) ψSO(2)

(cid:21)
0
1
(cid:21)
1
0

for ˜s = −1 ,

(cid:2) IndO(2)

for ˜s = +1

(cid:3)(˜r˜s) =

(51)

0

(cid:2) IndO(2)

SO(2) ψSO(2)

k>0

(cid:3)(˜r˜s) =




















ψSO(2)

k>0 (˜r)

0

0

0

ψSO(2)

k>0 (−˜r)

ψSO(2)

k>0 (˜r)

ψSO(2)

k>0 (−˜r)

0















for ˜s = +1

for ˜s = −1 .

We adapt the conv2triv and norm invariant maps, as well as the norm-ReLU and the gated nonlineari-
ties to operate on IndO(2)

SO(2)-ﬁelds as follows:

• Ind-conv2triv: Instead of applying O(2)-conv2triv to compute invariant features, we apply
convolutions to IndO(2)
-ﬁelds which are invariant under rotations but behave like
regular ({±1}, ∗)-ﬁelds under reﬂections. These ﬁelds are subsequently mapped to a scalar
ﬁeld via G-pooling, i.e. by taking the maximal response over the two subﬁelds.

SO(2) ψSO(2)

0

• Ind-norm: An alternative invariant map is deﬁned by computing the norms of the subﬁelds of

each ﬁnal IndO(2)

SO(2) ψSO(2)

k

-ﬁeld and applying G-pooling over the result.

• Ind norm-ReLU: It would be possible to apply a norm-ReLU to a IndO(2)

-ﬁeld for k > 0
as a whole, that is, to compute the norm of both subﬁelds together. Instead, we apply two
individual norm-ReLUs to the subﬁelds. Since the ﬁelds permute under reﬂections, we need
to choose the bias parameter of the two norm-ReLUs to be equal.
• Ind gate: Similarly, we could apply a single gate to each IndO(2)

-ﬁeld. However,
we apply an individual gate to each subﬁeld. In this case it is necessary that the gates
permute together with the IndO(2)
-ﬁelds to ensure equivariance. This is achieved by
computing the gates from IndO(2)

-ﬁelds, which contain two permuting scalar ﬁelds.

SO(2) ψSO(2)

SO(2) ψSO(2)

SO(2) ψSO(2)
SO(2) ψSO(2)

k

k

k

0

Empirically we ﬁnd that IndO(2)
SO(2) models perform much better than pure irrep models, despite both of
them being equivalent up to a change of basis. In particular, the induced representations decompose
for some change of basis matrices Q0 and Q>0 into:
IndO(2)

=

(cid:105)

0

SO(2) ψSO(2)
SO(2) ψSO(2)

IndO(2)

k>0 = Q>0

Q0
(cid:104)

(cid:104)
0,0 ⊕ ψO(2)
ψO(2)
1,k>0 ⊕ ψO(2)
ψO(2)

1,0

1,k>0

Q−1
0
(cid:105)

Q−1
>0

The difference between both bases is that the induced representations disentangle the action of
reﬂections into a permutation, while the direct sum of irreps is modeling reﬂections in each of its
sub-vectorﬁelds independently as an inversion of the vector direction and rotation orientation. Note
the analogy to the better performance of regular representations in comparison to a direct sum of the
respective irreps.

55

References

[1] Taco S. Cohen and Max Welling. Steerable CNNs. In International Conference on Learning

Representations (ICLR), 2017.

[2] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S. Cohen. 3D
steerable CNNs: Learning rotationally equivariant features in volumetric data. In Conference
on Neural Information Processing Systems (NeurIPS), 2018.

[3] Taco S. Cohen, Mario Geiger, and Maurice Weiler.

Intertwiners between induced repre-
sentations (with applications to the theory of equivariant neural networks). arXiv preprint
arXiv:1803.10743, 2018.

[4] Taco S. Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant CNNs on

homogeneous spaces. arXiv preprint arXiv:1811.02017, 2018.

[5] Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant
convolutional networks and the icosahedral CNN. In International Conference on Machine
Learning (ICML), 2019.

[6] Taco S. Cohen and Max Welling. Group equivariant convolutional networks. In International

Conference on Machine Learning (ICML), 2016.

[7] Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning steerable ﬁlters for rotation
equivariant CNNs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[8] Emiel Hoogeboom, Jorn W. T. Peters, Taco S. Cohen, and Max Welling. HexaConv.

In

International Conference on Learning Representations (ICLR), 2018.

[9] Erik J. Bekkers, Maxime W Lafarge, Mitko Veta, Koen A.J. Eppenhof, Josien P.W. Pluim, and
Remco Duits. Roto-translation covariant convolutional networks for medical image analysis. In
International Conference on Medical Image Computing and Computer-Assisted Intervention
(MICCAI), 2018.

[10] Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in
convolutional neural networks. In International Conference on Machine Learning (ICML),
2016.

[11] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution
in neural networks to the action of compact groups. In International Conference on Machine
Learning (ICML), 2018.

[12] Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow.
Harmonic networks: Deep translation and rotation equivariance. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

[13] Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector

ﬁeld networks. In International Conference on Computer Vision (ICCV), 2017.

[14] Laurent Sifre and Stéphane Mallat. Combined scattering for rotation invariant texture analysis.
In European Symposium on Artiﬁcial Neural Networks, Computational Intelligence and Machine
Learning (ESANN), volume 44, pages 68–81, 2012.

[15] Laurent Sifre and Stéphane Mallat. Rotation, scaling and deformation invariant scattering for
texture discrimination. Conference on Computer Vision and Pattern Recognition (CVPR), 2013.

[16] Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. IEEE transactions

on pattern analysis and machine intelligence, 35(8):1872–1886, 2013.

[17] Laurent Sifre and Stéphane Mallat. Rigid-motion scattering for texture classiﬁcation. arXiv

preprint arXiv:1403.1687, 2014.

[18] Edouard Oyallon and Stéphane Mallat. Deep roto-translation scattering for object classiﬁcation.

In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.

[19] Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. In International

Conference on Learning Representations (ICLR), 2018.

[20] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch–Gordan Nets: A Fully Fourier Space
Spherical Convolutional Neural Network. In Conference on Neural Information Processing
Systems (NeurIPS), 2018.

56

[21] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning
SO(3) equivariant representations with spherical CNNs. In European Conference on Computer
Vision (ECCV), 2018.

[22] Nathanaël Perraudin, Michaël Defferrard, Tomasz Kacprzak, and Raphael Sgier. DeepSphere:
Efﬁcient spherical Convolutional Neural Network with HEALPix sampling for cosmological
applications. arXiv:1810.12186 [astro-ph], 2018.

[23] Chiyu Jiang, Jingwei Huang, Karthik Kashinath, Prabhat, Philip Marcus, and Matthias Niessner.
Spherical CNNs on unstructured grids. In International Conference on Learning Representations
(ICLR), 2019.

[24] Adrien Poulenard and Maks Ovsjanikov. Multi-directional geodesic neural networks via

equivariant convolution. ACM Transactions on Graphics, 2018.

[25] Jonathan Masci, Davide Boscaini, Michael M. Bronstein, and Pierre Vandergheynst. Geodesic
In International Conference on

convolutional neural networks on Riemannian manifolds.
Computer Vision Workshop (ICCVW), 2015.

[26] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral Networks and Deep Locally Connected
Networks on Graphs. In International Conference on Learning Representations (ICLR), 2014.

[27] Davide Boscaini, Jonathan Masci, Simone Melzi, Michael M. Bronstein, Umberto Castellani,
and Pierre Vandergheynst. Learning class-speciﬁc descriptors for deformable shapes using
localized spectral convolutional networks. Computer Graphics Forum, 2015.

[28] Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Dynamic routing between capsules. In

Conference on Neural Information Processing Systems (NIPS), 2017.

[29] Jean-Pierre Serre. Linear representations of ﬁnite groups. 1977.

[30] Nichita Diaconu and Daniel Worrall. Learning to convolve: A generalized weight-tying

approach. In International Conference on Machine Learning (ICML), 2019.

[31] Marysia Winkels and Taco S. Cohen. 3D G-CNNs for pulmonary nodule detection.

In

Conference on Medical Imaging with Deep Learning (MIDL), 2018.

[32] Daniel E. Worrall and Gabriel J. Brostow. Cubenet: Equivariance to 3D rotation and translation.

In European Conference on Computer Vision (ECCV), pages 585–602, 2018.

[33] Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning

atomic potentials. arXiv preprint arXiv:1803.01588, 2018.

[34] Brandon Anderson, Truong-Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural

networks. arXiv preprint arXiv:1906.04015, 2019.

[35] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep net-
work learning by exponential linear units (ELUs). In International Conference on Learning
Representations (ICLR), 2016.

[36] Diego Marcos, Michele Volpi, and Devis Tuia. Learning rotation invariant convolutional ﬁlters

for texture classiﬁcation. In International Conference on Pattern Recognition (ICPR), 2016.

[37] Diego Marcos, Michele Volpi, Benjamin Kellenberger, and Devis Tuia. Land cover mapping
at very high resolution with rotation equivariant CNNs: Towards small yet accurate models.
ISPRS Journal of Photogrammetry and Remote Sensing, 145:96–107, 2018.

[38] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco S. Cohen, and Max Welling. Rota-
tion equivariant CNNs for digital pathology. In International Conference on Medical Image
Computing and Computer-Assisted Intervention (MICCAI), 2018.

[39] Nathaniel Thomas, Tess Smidt, Steven M. Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and
Patrick Riley. Tensor ﬁeld networks: Rotation- and translation-equivariant neural networks for
3d point clouds. arXiv preprint arXiv:1802.08219, 2018.

[40] Dmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, and Marc Pollefeys. Ti-pooling:
Transformation-invariant pooling for feature learning in convolutional neural networks. In
Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.

[41] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision

Conference (BMVC), 2016.

57

[42] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment:
Learning augmentation strategies from data. In Conference on Computer Vision and Pattern
Recognition (CVPR), 2019.

[43] Terrance DeVries and Graham W. Taylor. Improved regularization of convolutional neural

networks with cutout. arXiv preprint arXiv:1708.04552, 2017.

[44] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsu-
pervised feature learning. In International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2011.

[45] Geoffrey Hinton, Nicholas Frosst, and Sabour Sara. Matrix capsules with EM routing. In

International Conference on Learning Representations (ICLR), 2018.

58

