Dynamic Cell Imaging in PET with Optimal
Transport Regularization

Bernhard Schmitzer, Klaus P. Sch¨afers, and Benedikt Wirth

1

9
1
0
2
 
v
o
N
 
2
2
 
 
]

V
C
.
s
c
[
 
 
2
v
1
2
5
7
0
.
2
0
9
1
:
v
i
X
r
a

Abstract—We propose a novel dynamic image reconstruction
method from PET listmode data that could be particularly
suited to tracking single or small numbers of cells. In contrast
to conventional PET reconstruction our method combines the
information from all detected events not only to reconstruct
the dynamic evolution of the radionuclide distribution, but also
to improve the reconstruction at each single time point by
enforcing temporal consistency. This is achieved via optimal
transport regularization where in principle, among all possible
temporally evolving radionuclide distributions consistent with the
PET measurement, the one is chosen with least kinetic motion
energy. The reconstruction is found by convex optimization so
that there is no dependence on the initialization of the method.
We study its behaviour on simulated data of a human PET
system and demonstrate its robustness even in settings with very
low radioactivity. In contrast to previously reported cell tracking
algorithms, our technique is oblivious to the number of tracked
cells. Without any additional complexity one or multiple cells
can be reconstructed, and the model automatically determines
the number of particles. For instance, four radiolabelled cells
moving at a velocity of 3.1 mm/s and a PET recorded count rate
of 1.1 cps (for each cell) could be simultaneously tracked with a
tracking accuracy of 5.3 mm inside a simulated human body.

Index Terms—Molecular and cellular imaging, Nuclear imag-

ing, Tracking

I. INTRODUCTION

Immune and cell-based therapies have become of emerging
interest for eradicating or controlling intracellular pathogens
that are difﬁcult to treat with conventional therapies [1]. These
immunotherapies generally utilize the primary function of the
immune system to kill bacterial and viral pathogens or ﬁght
cancer cells. Molecular imaging has great potential to detect
and identify biomarkers allowing to treat only those patients
which most likely respond to a dedicated immunotherapy and,
most importantly, to monitor the treatment’s success [2].

Treatment planning and control need a quantitative measure
of the biomarker’s distribution within the patient’s body.
Positron emission tomography (PET) is a promising molecular
imaging technique as it provides quantitative whole-body data
at highest molecular sensitivity. The advent of new radiophar-
maceuticals based on F-18 or long-lived Zr-89 and Cu-64 (e.g.

This work was supported by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation) via Germany’s Excellence Strategy through
the Clusters of Excellence “Cells-in-Motion” (EXC 1003) and “Mathematics
M¨unster: Dynamics – Geometry – Structure” (EXC 2044) at the University of
M¨unster. B.W. was supported by the Alfried Krupp Prize for Young University
Teachers awarded by the Alfried Krupp von Bohlen und Halbach-Stiftung.

Bernhard Schmitzer is with the Department of Mathematics, Technical

University of Munich, Boltzmannstraße 3, 85748 Garching, Germany

Klaus Sch¨afers is with the European Institute for Molecular Imaging,

University of M¨unster, Waldeyerstraße 15, 48149 M¨unster, Germany.

Benedikt Wirth is with the Department of Mathematics and Computer
Science, University of M¨unster, Einsteinstraße 62, 48149 M¨unster, Germany.

[3]–[5]) led to the development of new immuno-PET imaging
approaches visualizing immune cell behaviour in-vivo.

Besides the use of PET for the visualization and quantitation
of biomarkers, a new strategy has been developed recently to
follow single cells over time using the excellent sensitivity
of PET systems in combination with mathematical modelling
techniques (see for instance [6] and references therein). In
these approaches, radio-labelled cells, directly injected or
indirectly labelled, are followed over time which is different
to standard PET techniques where usually a broad spectrum
marker like 18F-FDG distributes slowly within the body and
shows sites of its accumulation. Therefore, the PET informa-
tion by singular labelled cells is directly related to the process
of cellular motion rather than just showing the cell distribution
after long-term tracer accumulation. This application is of
great interest to understand the underlying biological processes
in immunotherapy. In addition, for the ﬁrst time, the possibility
of tracking the path of individual cells in the body opens
to better understand the role and
up which is of interest
function of circulating tumor cells in the development of
cancer metastases.

Note that conventional spatiotemporal PET-reconstruction
from listmode data (e. g. [7], [8]) rests upon a (temporally or
spatially) smooth parameterization of the radionuclide density
(e. g. via kernel methods or cubic splines) and typically also
a decoupling of space and time parameterization. Both these
features are incompatible with spatiotemporal radionuclide
densities induced by individually moving labelled cells (they
could only be utilized if the moving density is spatially smooth
and the motion is slow). Lee et al. instead proposed a novel
algorithm to reconstruct the trajectory of a moving cell directly
from PET list-mode data [6]. The trajectory is modelled
as a 3D B-spline function of time, and the mean squared
distance between the trajectory and the recorded coincidence
events is minimized via non-linear optimization. This approach
allows to track single sources in a small animal PET setup
with an accuracy below 3 mm provided that its activity (in
Bq) exceeds four times its velocity (in mm/s). Using the
same algorithm, Ouyang et al. demonstrated experimentally
the feasibility of tracking a moving point source in another
small animal PET system based on BGO detectors without
intrinsic background radiation [9]. Due to the reduced noise
level, the PET system could recognize moving sources with
an activity-to-velocity ratio > 3.45 decay/mm, outperforming
the LSO-based PET system. Langford et al. were able to track
multiple labelled yeast cells by following local maxima in a
discretized, smoothed back-projection of the PET signal, albeit
at signiﬁcantly higher activity-to-velocity ratios [10].

Our proposed method combines the information from all

detected events to reconstruct the dynamic evolution and si-
multaneously improve the reconstruction quality by enforcing
temporal consistency. Our approach combines the advantages
of [6], [9] and [10]. It can track multiple particles, the number
of which is automatically determined and does not affect the
complexity, at activity-to-velocity-ratios comparable with [6].
For single particles our functional is closely related to that of
[6]. We tested the new algorithm on simulated data of a human
PET system to evaluate a potential clinical use of PET based
cell tracking under realistic conditions.

II. MATHEMATICAL RECONSTRUCTION MODEL

A. Measurements and material distribution

The interior of the PET scanner (the measurement volume)
will be denoted by Ω ⊂ R3. Measurements are performed over
a time interval [0, T ) which we divide into M subintervals
(τi)M
i=1 of duration ∆T = T /M with τi = [(i − 1) ∆T, i ∆T ).
With j ∈ {1, . . . , N } we enumerate all scanner detector
pairs. The line of response (LOR) lj of pair j is a straight
line through Ω that connects the centers of the two detectors.
In an idealized setting any β-decay that led to the activation of
pair j is assumed to have happened on lj. More realistically,
due to a variety of effects, the β-decay either happened close
to the line or was caused by a random incidence or strong
scattering. The number of detected events in time interval τi
and detector pair j is denoted Ei,j, and we abbreviate

E = (Ei,j)i=1,...,M, j=1...,N .

Our goal will be to reconstruct from the measurement E
the corresponding radionuclide density ρt(x) at each position
x ∈ Ω and time t ∈ [0, T ]. We write this as

ρ = (ρt(x))t

[0,T ],x

∈

Ω .

∈

The expected number of decays in time interval τi and a spatial
domain S ⊂ Ω is given by
(cid:90)

(cid:90)

log 2
T 1

2

τi

S

ρt(x) dx dt

where T 1

denotes the radionuclide halﬂife.

2

B. Forward operator

Each event count Ei,j is Poisson distributed with mean Ki,j
which can be computed from ρ via the forward operator. In
our simpliﬁed model we distinguish three different outcomes
of a positron decay and subsequent photon emission:

a) Detection: The photons undergo at most minor scattering

before being registered by a pair of detectors.

b) Scattering: At least one of the photons undergoes sub-
stantial scattering which signiﬁcantly alters its direction.
c) No detection: The emitted photon pair is not detected,
e.g. due to absorption or imperfect scanner sensitivity.

i,j and As

The outcomes of the former two will be produced by linear
operators Ad
i,j; we set Ai,j = Ad
i,j and Ki,j =
Ai,jρ. The latter does not lead to any detected events and
thus does not contribute to Ki,j. Note that for each (i, j),
Ai,j is a linear function that takes the spatiotemporal material

i,j + As

2

distribution ρ as input and outputs the expected number of
detected events in time interval τi and detector pair j. We
now describe the two operators.
a) Detection: We set

Ad

i,jρ =

log 2
T 1
2

(cid:90)

(cid:90)

τi

Ω2

Hj(y) G(y, x) pd ρt(x) dx dy dt.

(1)

Hj(y) encodes the tomography geometry and gives the prob-
ability that a collinear unscattered photon pair emitted at y
activates the detector pair j. G(y, x) accounts for the positron
range and gives the probability that a positron emitted at x
annihilates at y. G can also be used to approximately model
minor scattering or slight non-collinearity of the photon pair.
We will just consider a spatially homogeneous Gaussian kernel
2π(cid:15))3 for some ﬁxed
G(y, x) = exp(−|y − x|2/(2(cid:15)2))/(
(cid:15) > 0. pd ∈ [0, 1] denotes the probability that a photon pair is
neither attenuated nor scattered substantially.

√

Without complicating the approach one could make pd
space-dependent and use Hj and G to encode spatially varying
material properties and scanner sensitivity, however, for sim-
plicity our simulations are performed for constant sensitivity.
b) Scattering: For simplicity we assume that scattering
changes the photon rays randomly such that the probability of
arriving at a detector pair j is homogeneous,

As

i,jρ =

1
N

log 2
T 1

2

(cid:90)

(cid:90)

τi

Ω

ps ρt(x) dx dt.

Similar as above, ps ∈ [0, 1] is the probability for strong
scattering. As with Ad
i,j, one can also consider a more
elaborate operator that accounts for spatially inhomogeneous
scattering. However, we will later see that for our data this
approximation is appropriate (cf. ﬁg. 2 and corresponding
text). With probability 1 − pd − ps > 0 photons remain
undetected, e.g. due to attenuation. Random coincidences can
be modelled by adding a constant background to Ki,j.

C. Optimal transport regularization

Our method aims at a regime of low radiation activity
with relatively few detected events. Reconstructing the particle
distribution ρ from the measurements E is therefore a classical
underdetermined and ill-posed inverse problem which we must
regularize by incorporating additional prior knowledge. If the
radionuclide distribution ρ were temporally almost constant,
then by taking long enough time intervals τi one would
detect enough decays within τi to reconstruct the sought ρ.
However, for quickly changing radionuclide distributions such
a framewise reconstruction separately for each time interval
τi immediately breaks down. Instead it becomes necessary to
combine information from multiple consecutive time intervals,
of course taking into account that the radionuclide distribution
changes between the time intervals.

As ρ describes the physical motion of radioactive particles
there will be some temporal consistency, and not all spa-
tiotemporal distributions ρ are equally likely. In our variational
reconstruction approach we model this by a penalty term that
prefers physically plausible distributions. For this we consider
the velocity vt(x) ∈ R3 of the radioactive material at each

position x and time t, and we will penalize too high velocities.
Mathematically, though, it is easier to work with the mass ﬂux
ωt(x) = ρt(x)vt(x) instead of the velocity, and we abbreviate

ω = (ωt(x))t

[0,T ],x

∈

Ω .

∈

The mass ﬂux satisﬁes the classical mass conservation

∂tρt(x) + divωt(x) = 0 .

(2)

2

Here we assume T (cid:28) T 1
so that the amount of radioactive
material does essentially not change over time; otherwise the
constraint could easily be complemented with an additional de-
cay term (leading to so-called unbalanced transport, cf. [11]).
In the end we will not only reconstruct ρ, but also ω.

To quantify the likelihood of a path (ρ, ω) we associate with

it its physical action

S(ρ, ω) =

(cid:82)
Ω

2
2

(cid:107)

ωt(cid:107)
ρt

dx dt

(cid:40)(cid:82) T
0
∞

if ρ ≥ 0 and (2) holds,
else.

(cid:107)

2
2

Ω

ωt(cid:107)
ρt

Note that ωt can be interpreted as physical momentum of the
motion of ρt, so (cid:82)
dx is essentially the kinetic energy
of all particles at time t. This action is the so-called Benamou–
Brenier functional for optimal transport, and the minimum
value that can be achieved for a transport of ρ0 to ρT is the
squared Wasserstein-2 distance between both measures [12].
We will below add β · S(ρ, ω) as a penalty term to our log-
likelihood functional where β ≥ 0 is a weighting parameter.
In essence, we assign a lower penalty to a path (ρ, ω) the less
the mass moves. This acts as a temporal regularization of the
particle trajectories (a spatial regularization could be added on
top, but since we do not want to impose any further structure
on the moving radiotracer distribution this is not done here).
Note that S is a convex function of (ρ, ω) and thus amenable
to global optimization (see [12] for details).

D. Penalized log-likelihood functional and unbiasing

Denoting by Pλ(k) = λke−

λ/k! the Poisson distribution
with mean λ and writing as before Ki,j = Ai,jρ,
the
conditional probability of having the measurement E if the
radioactive mass distribution is ρ can be calculated as

P (E|ρ) =

PKi,j (Ei,j) .

M
(cid:89)

N
(cid:89)

i=1

j=1

The corresponding negative log-likelihood is

− log P (E|ρ) =

[Ki,j − Ei,j log Ki,j] + const

M
(cid:88)

N
(cid:88)

i=1

j=1

where the terms log Ei,j! are subsumed into a constant.
Inserting Ki,j = Ai,jρ and adding the kinetic regularization
leads to a penalized maximum likelihood (ML) functional

J E[ρ, ω] =

[Ai,jρ − Ei,j log Ai,jρ] + β S(ρ, ω) (3)

M
(cid:88)

N
(cid:88)

i=1

j=1

to be minimized for ρ and ω.

Unfortunately, the ML estimate in this setting is strongly
biased towards declaring every detected photon pair as be-
ing unscattered. Essentially, this is due to As
i,jρ being very

3

small compared to Ad
i,jρ in the deﬁnition of Ai,j (indeed,
As
i,j distributes the intensity evenly over all detectors, while
Ad
i,j concentrates all the intensity on a few detectors, thus
producing much higher intensities there). This is a well-known
deﬁciency of the ML estimator. A simple and intuitive way to
compensate for this bias is by introducing a tuning parameter
p ≥ 0 that reweighs the contribution of the two forward
operators in that term which contains the detections Ei,j,

J E,p[ρ, ω] =

Ai,jρ − Ei,j log (cid:2)Ad

i,jρ + pAs

i,jρ(cid:3)

M
(cid:88)

N
(cid:88)

i=1

j=1

+ β S(ρ, ω) .

(4)

i,jρ > pAs

In the following we will assume that events Ei,j were most
likely caused by unscattered photons if Ad
i,jρ and by
scattering otherwise. This interpretation can be justiﬁed by a
more detailed derivation of the transition from (3) to (4). For
the present article the intuition will sufﬁce that the unscattered
contribution Ad
i,jρ dominates for p close to zero, whereas the
scattering contribution pAs
i,jρ dominates for very large p. By
choosing an intermediate p, just the right amount of events
will be interpreted as scattered.

Finally, recall that we will be working in the regime of
very low radiation activity which means that E is expected
to be sparse, i.e. only a small fraction of entries Ei,j will
be non-zero. (cid:80)M
j=1 Ai,jρ is the expected total number
i=1
of detected events (with and without scattering). Consequently,
there is a function r : Ω → R, related to the material properties
and the sensitivity of the PET scanner, such that

(cid:80)N

M
(cid:88)

N
(cid:88)

i=1

j=1

(cid:90) T

(cid:90)

0

Ω

Ai,jρ =

r(x) ρt(x) dx dt .

One can then rewrite (4) as

J E,p[ρ, ω] =

r(x) ρt(x) dx dt

(cid:90) T

(cid:90)

0

Ω

M
(cid:88)

N
(cid:88)

−

i=1

j=1

Ei,j log (cid:2)Ad

i,jρ + pAs

i,jρ(cid:3) + β S(ρ, ω),

(5)

where the sum now only runs over the sparse subset where
Ei,j > 0, and only those elements of the full tomography
operator must be computed. In our experiments the area where
the particles are moving is small compared to the dimensions
of the PET scanner. Thus we may assume that r is spatially
constant. Moreover, one can show that if (ρ, ω) minimize J E,p
for some r and β then (ρ, ω)/q will minimize J E,p for q · r
and q · β, for any q > 0. Hence, only the relative scaling of r
and β is relevant, and we may for simplicity set r(x) = 1/T .

E. Relation to single-cell tracking functional

At ﬁrst glance our functional looks very different from the
tracking functional for a single cell by Lee et al. in [6]. In
this section we illustrate that there is a close relation when our
functional is constrained to a single particle; in fact, the func-
tional by Lee et al. is a special case of our restricted functional
up to slight implementation and modelling variations. The

advantage of our new formulation is the straightforward and
easy applicability to tracking of multiple cells (with potentially
different unknown amounts of radioactivity).

Let t (cid:55)→ X(t) describe the trajectory of a single particle
in Ω; for simplicity assume that X is differentiable such
that X (cid:48)(t) denotes the particle velocity. The particle mass is
denoted by m ≥ 0. Then the corresponding time-dependent
particle distribution ρ and the material ﬂux ω are

ρt = m · δX(t),

ωt = X (cid:48)(t) · m · δX(t)

where δx denotes the Dirac delta function centered at x. Thus
(cid:90) T

S(ρ, ω) = m ·

(cid:107)X (cid:48)(t)(cid:107)2

2 dt ,

0
which corresponds to the kinetic energy of the particle inte-
grated over time. Likewise one computes

Ad

i,jρ = const · pd · m ·

Hj(y) G(y, X(t)) dy dt

(cid:90)

(cid:90)

τi

Ω

where we have subsumed some prefactors into a constant. If
we further assume that the time intervals τi are very small
(such that X is almost constant on τi) and that the spatial
detector resolution is very high (such that Hj is only non-
zero in a very small environment of the LOR lj, which is
much smaller than the width of the kernel G), then this can
be approximated by

Ad

i,jρ ≈ const · m ·

G(y, X(ti)) dy

(cid:90)

lj

where the constant contains additional new factors and ti is
the midpoint of the time interval τi. If we now choose G(y, x)
as a Gaussian kernel of width (cid:15) one obtains

i,jρ ≈ const · m · exp(− d2
Ad

i,j

2(cid:15)2 )

where di,j denotes the minimal distance between the point
X(ti) and the LOR lj.

Let us for a moment ignore the contribution of scattering,
i.e. we set p = 0. If the pair (ρ, ω) corresponding to a single
particle is inserted into (5) one obtains

J E,p[ρ, ω] ≈ m +

M
(cid:88)

N
(cid:88)

i=1

j=1

Ei,j

(cid:2) d2
2(cid:15)2 − log m(cid:3)

i,j

+ β m

(cid:107)X (cid:48)(t)(cid:107)2

2 dt + const .

(cid:90) T

0

0 (cid:107)X (cid:48)(t)(cid:107)2

For ﬁxed m the data term in this functional is (up to a factor)
the sum of the squared distances d2
i,j just as in [6]. The
regularization term (cid:82) T
2 dt leads to the preference of
short trajectories with low derivative. While the precise term
used in [6] is different (it penalizes quadratically the change
in the cubic B-spline coefﬁcients in a parametrization of X)
it is very similar in spirit and can in fact be interpreted as a
time discretization of (cid:82) T

0 (cid:107)X (cid:48)(t)(cid:107)2

2 dt.

Now consider scattering. One ﬁnds As

i,jρ = const · m, thus

− log(Ad

i,jρ + pAs

i,jρ) ≈ f (di,j) − log m + const

4

d2
with f (d) = − log(exp( −
2(cid:15)2 ) + ˜p) where ˜p ≥ 0 is the product
of p and some constant. For d/(cid:15) (cid:28) − log ˜p one has f (d) ≈
d2
2(cid:15)2 , while for (d/(cid:15))2 (cid:29) − log ˜p one ﬁnds f (d) ≈ − log ˜p.
Thus, the function g(d) = min{ d2
2(cid:15)2 , − log ˜p} can be viewed
as a simple approximation of f , and indeed the function g is
used in [6] to model scattering and random coincidences.

Summarizing, when restricted to a single particle our func-
tional
is indeed very similar to the one proposed in [6].
Analogously to the above computations one could now derive
a generalization of their functional for multiple particles
with trajectories given by functions Xk and masses mk for
k = 1, . . . , P . However, the resulting functional would be
severely non-convex in the coordinates Xk and thus difﬁcult
to optimize. The main novelty of our functional is thus not the
forward model or the regularization term, which are similar to
the ones used in [6], but indeed the formulation in terms of
time-dependent densities and ﬂux ﬁelds, which yields a convex
functional that is readily minimized and does not require to
manually choose the particle number P .

III. METHODS

A. Discretization and numerical optimization

The functions ρt and ωt are discretized by a regular grid of
voxels over Ω for time points t ∈ {0, ∆T, . . . , T }. Constraint
(2) and S(ρ, ω) are then discretized using ﬁnite differences on
a staggered grid as in [13]. The operators Ad
i,j are
discretized by identifying for each pair j of detectors which
voxels contribute how much. In particular the discretization of
the reconstruction method is completely independent from the
software used to generate the simulated data, thus preventing
the fundamental inverse crime [14]. Therefore, the obtained
results should indeed be meaningful.

i,j and As

Unless stated otherwise, in our experiments we set T =
120 s, Ω = [0, L] × [0, L] × [0, L/4] with L = 160 mm and
discretize [0, T ] × Ω with a regular Cartesian grid with 65 ×
64 × 64 × 16 points, resulting in a meshsize of ∆L = 2.5 mm.
We set the width (cid:15) of the Gaussian kernel G(y, x) from (1)
to (cid:15) = 5mm which is slightly larger than the value in the
simulations (cf. ﬁg. 2) to reduce discretization artifacts.

The functional J E,p to be minimized is jointly convex in
(ρ, ω) (indeed, the constraints ρ ≥ 0 and (2) are linear, and S
is known to be convex as well [12]). Thus, its minimization
can be performed via a convex optimization approach. The
optimization problem is then written in the form

F (K(ρ, ω)) + H(ρ, ω)

inf
(ρ,ω)

where K is a suitable matrix and F and H are convex lower-
semicontinuous functionals. To this optimization problem the
primal-dual implicit gradient descent and ascent from [15]
is applied. The precise form of F, H and K as well as the
algorithm are given in the supplementary materials.

B. Framewise reconstruction

To demonstrate the beneﬁt of temporal regularization we
implement framewise reconstruction as a reference, i. e. we set
β = 0 and assume ρ to be constant on each interval τi. The ﬂow

ﬁeld ω no longer appears in the functional. It is trivial to adapt
the numerical scheme accordingly. Apart from our scatter-
unbiasing the unregularized functional is then comparable to
standard Bayesian reconstruction methods for PET imaging.
We expect that for small ∆T the framewise method suffers
from a lack of signal in each independent time frame. As ∆T
increases, more events become available in each frame, but
there will be increasing inaccuracy due to particle motion.

C. Quantiﬁcation of reconstruction error

Using simulated measurements allows us to quantitatively
compare our reconstruction results to the ground truth. We
have to simultaneously measure errors in the localization of
multiple particles and their respective masses (since PET is
quantitative). Note that both cannot be separated from each
other, since incorrectly positioning some mass from x ∈ Ω
at y ∈ Ω can be interpreted both as an error in localizing
the mass from x or as an error in reconstructing the correct
amount of mass in x and y. In particular we cannot use the
squared Euclidean distance from [6] as it only applies to single
particles and cannot
take into account masses. A suitable
joint error quantiﬁcation for particle localization and mass
is achieved by the Wasserstein–Fisher–Rao (WFR) metric
dWFR,α(ρ, ˜ρ) between material distributions ρ and ˜ρ (see
supplementary material or [11] for a precise deﬁnition). It
measures how far on average the material in ρ has to be
transported to change ρ into material distribution ˜ρ, where
transport beyond distance ≈ α is replaced by just changing
the mass locally (assuming that rather the mass intensity was
wrong than its localization). In detail, for two Dirac masses
δx at x and δy at y one ﬁnds

WFR,α(δx, δy) = 16 α2 sin2 (cid:0) min (cid:8)
d2

x

y

(cid:107)4α , π

−

4

(cid:107)

(cid:9)(cid:1)

where α is a length scale parameter that balances the trade-off
between localization and mass errors. This can be interpreted
as a truncated Euclidean distance since d2
WFR,α(δx, δy) ≈
(cid:107)x − y(cid:107)2 for (cid:107)x − y(cid:107) (cid:28) α. Consequently, it provides a
natural generalization of the Euclidean distance to the setting
of multiple particles with different masses.

To quantify our reconstruction errors we pick α = 25 mm

and calculate

err2 =

WFR,α(N (ρt), N (ρgt
d2

t )) dt

1
T

(cid:90) T

0

for ρgt the ground truth, ρ our reconstruction, and N the
map that normalizes a non-negative density. We apply nor-
malization to avoid tedious calibration of the absolute scanner
sensitivity. err can roughly be interpreted as mean localization
error for the particles with a truncation for deviations (cid:29) α. It
is computed numerically using the code from [16].

Since α (cid:29) ∆L (where ∆L is the distance between two
neighbouring grid points) the error inﬂicted by misplacing all
particles by one discrete grid point is approximately

errdisc = ∆L = 2.5 mm.

(6)

This provides a scale for the expected discretization error.

5

Fig. 1. Coronal and transaxial view of the human torso phantom XCAT used
for Monte-Carlo simulation of single cell movement. The radio-labelled cell
(red dot) is moving along a circular trajectory (red circle) with radius 6 cm.

IV. EXPERIMENTAL VALIDATION

A. Monte Carlo Simulation

We work on simulated data. In this way we can tune the
problem difﬁculty, and reliable ground truth is available, al-
lowing a transparent evaluation of our reconstruction method.
We simulate list-mode data of a clinical PET scanner with
the Geant4 Application for Tomographic Emission (GATE)
Monte Carlo tool [17]. Within this simulation, the geometry
and detector concept of the Siemens Biograph mCT PET/CT
is implemented to mimic a state-of-the-art human PET system
with LSO detector material [18]. To simulate cells under
realistic conditions, body anatomy and attenuation is taken
from a 4D extended cardiac-torso (XCAT) phantom [19]. A
single cell is simulated as a point source (diameter 1 mm)
with radioactivity of 10 kBq 18F (T 1
= 6586 s). This cell is
moved in 120 steps along a circular trajectory with radius
60 mm in the x-y plane, see ﬁg. 1. At each step, a Monte-
Carlo simulation is performed using 1 s simulation time, and
the information of the detected coincidence events is stored
in a list-mode data format. Note that due to this discretization
of a continuous motion the simulated cell moves abruptly in
steps of π mm ≈ 3.14 mm which will cause a slight additional
error in our reconstructions (so we actually overestimate our
reconstruction error in the experiments).

2

Two different scenarios are simulated. In the ﬁrst scenario
(‘without scattering’), β+ particles are simulated with typical
energies of 18F followed by positron-electron annihilation
inside the human tissue and emission of two 511 keV gammas.
Scattered gamma events are rejected by setting an energy
window of 510–650 keV with no energy blurring. For a 10 kBq
source activity the rate of counts detected by the scanner
was 44 cps (counts per second), corresponding to a ratio of
3 cps/Bq. In the second scenario (‘with scat-
s = 4.4 · 10−
tering’), scattered events are partly allowed using an energy
window of 435–650 keV (energy blurring 11.6 % at reference
energy 511 keV), a range which is typically used in a clinical
setup. The intrinsic natural radiation from Lu-176 within the
LSO crystal was not considered in the GATE simulation.
Events were detected with a rate of 104 cps, yielding a ratio
2 cps/Bq. The scatter fraction in this scenario
of s = 1.0 · 10−
was about 18%. In both simulations a coincidence window of
4.1 ns is used.

To obtain more realistic experimentally feasible conditions
the activity is then subsampled to rates between 160 Bq and
800 Bq in the ﬁrst simulation scenario (corresponding to 0.7 to
3.6 cps for a single particle) and 500 Bq in the second scenario

6

Fig. 2. Cumulative distribution of distance between positron emission position
and line of response of the activated detector pair for all observed events with
and without photon scattering.

(5.2 cps/particle). Multiple particles are simulated by applying
a rotation in the x-y plane to the listmode data and then
combining the resulting events. This is admissible since the
scanner geometry is approximately symmetric around this axis.
Different subsamplings of the original data were used so that
events from different particles are statistically independent.

To obtain a notion of robustness of our reconstruction
method we generated ﬁve runs for each simulation scenario.
The reconstruction accuracies and scattering ratios reported in
the following are the obtained averages, and we visualize the
standard deviation among the ﬁve runs with errorbars.

B. Basic setup

Our primary test phantom consists of four cells moving one
after another on a circle in the x-y plane with a radius of
60 mm. We consider four experimental parameters: the amount
m of radionuclides within the cells, the cell velocity v, the
distance between subsequent cells and whether or not photon
scattering is included in the recorded events. The detection
rate (counts per second) is then given by A = s · m · log 2/T 1
.
We vary A by varying m. For values of s and T 1
and how m
is varied by subsampling the original data, see section IV-A.
Recall that in addition there are two major parameters in the
reconstruction model: β for kinetic regularization and the scat-
tering tuning parameter p. This setup allows to carefully test
the method at different levels of difﬁculty and to understand
how to choose the two reconstruction parameters properly.

2

2

Figure 2 shows the cumulative distributions of the distance
from the original positron emission to the line of response
of the activated detector pair for all detected events, for
simulated data with and without photon scattering. In absence
of scatter this distance is caused by the positron range and
photon non-collinearity, as modelled with the kernel G in Ad
i,j,
see (1). With scatter included, minor scattering with small
perturbations to the photon orientation is still modelled with G.
In events with strong scattering the true photon path will differ
substantially from the idealized line of response, and thus the
distance between positron emission and line may be much

Fig. 3. Visualization of projρ (projection of ρ into x-y plane) in [0, T ]
×
[0, L]2 (axis orientation according to (a)) for different detection rates A (of
all particles combined), with temporal regularization (a-d, β = 0.9 s/mm2)
and without (i.e. framewise, e-f). Red lines indicate ground truth particle
positions, dark shading indicates reconstructed particle distributions. In the
framewise reconstruction there are many spurious artifacts and a lack of tem-
poral smoothness. With temporal regularization the trajectories are accurately
reconstructed for detection rates as low as 4.3 cps. Even at 2.8 cps the particles
can be distinguished and their trajectories recognized (see also ﬁg. 4).

larger. This is modelled with the operator As
i,j, and the long
tail of the scattered distribution indicates that the assumption
of ‘uniform’ scattering is indeed a reasonable approximation.

C. Temporal regularization

We now study the inﬂuence of detection rate and temporal

regularization. For simplicity we start without scattering.

Exemplary visualizations of reconstructed trajectories with
and without temporal regularization are given in ﬁgs. 3 and 4.
As the particles in the phantom are restricted to the x-y plane,
for simplicity we visualize the projection of ρ into this plane
(or equivalently, the integration of ρ along the z axis).

Figure 5 shows the reconstruction error for different de-
tection rates A and values of β, where the detection rate is
simply varied by changing the amount of radioactive ma-
terial in the simulation. The optimal value of β is found
to lie between 0.4 s/mm2 and 1.0 s/mm2 and is essentially
independent of the rate A and number of time frames M .
For detection rates of A = 5.7 cps and above the optimal
reconstruction error is almost constant and almost at the level
of discretization artifacts (see (6)) across about one order of
magnitude for β. For lower rates the error increases more
quickly. The optimal reconstruction error is approximately
proportional to the inverse activity (cf. ﬁg. 7 (right)). For
A = (2.8, 4.3, 8.3) cps the lowest reconstruction errors are
approximately (8.0 ± 0.7, 5.3 ± 0.5, 3.6 ± 0.1) mm. For low
regularization β the error increases since the coupling between
the information contained in different time frames becomes
weaker (this effect is more pronounced with more time frames,

7

Fig. 4. 2D slices for some of the visualizations of ﬁg. 3 for various times.
Ground truth particle locations are indicated by red crosses. With temporal
regularization, at A = 5.6 cps the particles are reliably located. At A =
2.8 cps the particles are not as concentrated but can still be separated. Without
temporal regularization the reconstruction quality is substantially worse.

Fig. 5. Reconstruction error for different detection rate, temporal resolutions
M , and β for simulations without scattering. The phantom consists of four
particles moving one after another on a circle with radius 60 mm, with velocity
3.14 mm/s, distance 37 mm, discretized with M = 65 time frames (except for
the last data series). A is joint detection rate of all four particles. Values are
averages over ﬁve runs, errors are given by corresponding standard deviation.

M = 129, each of which then contains less information). As β
increases the error grows due to overregularization. Fluctuation
between different simulation runs is reasonably small (as the
errorbars indicate) showing that our method is robust.

For comparison, the reconstruction errors without kinetic
regularization (i.e. framewise reconstruction, section III-B) are
given in ﬁg. 6. The errors are higher than with temporal
regularization, and in particular they increase drastically with
decreasing detection rate and therefore also with increasing
number of time frames (which reduces the available infor-
mation per frame). Reducing the number of frames increases

Fig. 6. Reconstruction error for different detection rates and temporal
resolutions M , without temporal regularization. Other phantom parameters
are as in ﬁg. 5. Note y-axis is in log scale.

the available information per frame but the error eventually
increases due to ‘motion blur’. The reconstruction with tem-
poral regularization is more robust to little information per
frame due to the coupling between the frames.

2

2

2

→ q · T 1

For instance, if one rescales m → q · m, T 1

Parameter β balances the strength of temporal regularization
versus agreement with the measurements. Its optimal value
depends on the substance amount m, particle velocity v and
also the halﬂife T 1
(since our data term uses radioactivity, but
the regularization is based on material mass and thus still needs
to be scaled by the inverse halﬂife). The precise relation can
be deduced from the scaling behavior of the functional J E,p.
for
some q > 0 (which leaves the rate A and thus the measurement
E unchanged) and rescales β → β/q, then the minimizer of
J E,p is rescaled according to (ρ, ω) → (q ·ρ, q ·ω), i. e. the op-
timal reconstruction is rescaled by the same factor as the true
material amount. As a consequence the reconstruction quality
is invariant under this rescaling and thus if β is the optimal
parameter for (m, v, T 1
) then β/q is the optimal parameter for
). With additional similar arguments one can
(q · m, v, q · T 1
· v2) for
show that the optimal parameter β is given by C/(T 1
a suitable constant C. From ﬁg. 5 we deduce that the optimal
β for v = 3.14 mm/s and T 1
= 6586 s is approximately
0.9 s/mm2 and therefore C ≈ 600. Unless stated otherwise, in
all subsequent experiments we set β according to this formula.
In summary, one obtains good reconstructions for a large
interval of values of β, and the dependence of the optimal
value on the experimental parameters is well understood. Also,
as compared to the framewise reconstruction, the regularized
reconstruction quality is more robust at low detection rates.

2

2

2

2

D. Particle velocity and tracking efﬁciency

In [6] it was observed that particle velocity v and detection
rate A are not independent parameters with respect to the
reconstruction quality. In essence only their ratio v/A is impor-
tant which was called tracking efﬁciency in [6]. This is intuitive
as the tracking efﬁciency is the average distance between two
detected positron decays along the particle trajectory.

8

Fig. 7. Left: reconstruction error for different particle velocities, detection
rates, and β. Particle velocities v = q
·
8.3 cps. Other parameters as in ﬁg. 5. Right: tracking efﬁciency vs. tracking
accuracy for the data displayed in ﬁg. 5 for β = 0.9 s/mm2.

3.14 mm/s, detection rate A = q

·

Left: reconstruction error for different p for simulations with
Fig. 9.
scattering. β = 0.19 s/mm2, other parameters as in ﬁg. 8. Right: estimated
scattering ratios for same reconstructions.

Figure 8 (left) shows the reconstruction error for two values
of p. For low and medium values of β the behavior is very
similar to the no-scattering case (cf. ﬁg. 5). However, with
scattering the effect of overregularization is more dramatic
which is due to a new mechanism that becomes available with
scatter modelling: when the kinetic regularization is too strong,
the lowest functional values can be obtained by keeping the
reconstructed particles almost stationary and declaring almost
all detected events as scattering. Figure 8 (right) shows the
corresponding estimated scattering ratios, where we interpret
an event (i, j) as ‘scattered’ if pAs
i,jρ. For small and
medium values of β the ratio stays almost constant for each
p, but increases quickly to 1 in the overregularization regime.
For sufﬁciently low β (below overregularization) the scat-
tering ratios for p = 0.001 and p = 0.002 are roughly 0.2
and 0.8, respectively. From ﬁg. 2 we see that the former is
relatively accurate while the latter is substantially too high
and wrongfully discards many unscattered events. Despite this,
the reconstruction error for p = 0.002 is almost on par with
p = 0.001 since the remaining 20 % of undiscarded events
provide sufﬁcient information about the particle locations.

i,jρ ≥ Ad

2

Due to the stronger effect of overregularization the optimal
value for β is slightly reduced. Hence, for scattered data we
will set in the following β = C/(T 1

· v2) for C ≈ 130.

Figure 9 illustrates the inﬂuence of p on the reconstruc-
tion error and estimated scattering ratio in more detail. The
estimated scattering ratio increases monotonically with p.
For small p only the ‘most absurd’ events are labeled as
scattered. With p the ratio increases, and eventually almost
all events are considered scattered. In accordance, for small p
the reconstruction error is higher since some scattered events
that are not correctly recognized as such cause artifacts in the
reconstruction. For high p the error increases since too many
events are discarded as scattered which leaves less reliable
information for reconstruction. The minimum reconstruction
error is roughly obtained for that value of p which predicts
the correct scatter ratio. Our experiments show that the optimal
value for p does not depend signiﬁcantly on detection rate or
particle velocity, and it is thus relatively easy to calibrate. If

Fig. 8. Left: reconstruction error for different β, p (simulations with scatter).
Particle distance 47 mm, total detection rate (incl. scatter) 20.8 cps, other pa-
rameters as in ﬁg. 5. Right: estimated scattering ratio for same reconstructions.

This relation can also be observed for our reconstruction
functional J E,p. Let q > 0 and rescale v → q·v and m → q·m
which implies A → A · q and in particular that v/A remains
constant. Then, similar to above, if one rescales β → β/q2,
one can show that the minimizer of the functional transforms
as ρt → q · ρq t and in particular the reconstruction error does
not change. This is conﬁrmed numerically in ﬁg. 7 (left). As
predicted the curves look very similar.

The relation between tracking efﬁciency and tracking ac-
curacy for the data of ﬁg. 5 is shown in ﬁg. 7 (right). For a
detection rate of A = 4.3 cps for four particles the detection
rate per particle is Asingle = 1.1 cps which corresponds to
a tracking efﬁciency of 3.0 mm, at a tracking accuracy of
5.3 mm. In [6] comparable tracking efﬁciencies are reported at
somewhat better accuracies albeit only for a single particle and
a smaller scanner geometry providing higher image resolution.

E. Scattering

We now turn to data with scattering. We will use the full
forward operator involving the parameter p used for unbiasing.

9

Fig. 10. 2D slices for two of the reconstructions of ﬁg. 9 for various times,
analogous to ﬁg. 4. Ground truth particle locations are indicated by red
5 one can see spurious artifacts caused by the
crosses. For p = 3
10−
3 almost all artifacts vanish.
failure to identify scattered events. For p = 10−
Relative to ﬁg. 4 the color scale for the particle densities was magniﬁed as
otherwise the scattering artifacts would not be visible.

·

Fig. 12. Visualization of projρ analogous to ﬁg. 3 for the data sequence with
scattering from ﬁg. 11 for different particle distances ∆x.

to classical clinical PET reconstruction, our method performs
a dynamic rather than static imaging, resolving the temporal
changes. It is in particular designed for tracking individual
radiolabelled cells, but could without modiﬁcation also be
applied to the reconstruction of a temporally evolving dis-
tribution of injected PET tracers or PET reconstruction under
organ (cardiac or respiratory) motion.

In essence, for tracking individual cells via PET there are
two existing competing approaches, time binning with inde-
pendent reconstruction for each bin and path ﬁtting [6]. Time
binning pretends the radionuclide distribution was stationary
over short time intervals and performs a classical reconstruc-
tion over those time intervals. As a drawback, too large time
intervals cause substantial blurring due to the radionuclide
motion, while too small time intervals contain too few detected
decays to allow a reliable image reconstruction separately for
each time interval. In contrast, our novel method is essentially
continuous in time so that no blurring occurs, but at the same
time exploits temporal consistency so that the reconstruction
almost behaves as if the radionuclide distribution was indeed
stationary. The resulting strong superiority over time binning
was demonstrated quantitatively on simulated data.

Path ﬁtting on the other hand tries to ﬁt a particle trajectory
to the detected decay events. While for a single particle
this is readily doable, for multiple particles it introduces the
combinatorial problem of deciding which event belongs to
which particle. Our approach can be seen as a relaxation of
this method in the sense that in principle our method also
allows particles to split or to merge. As a consequence our
reconstruction boils down to a simple convex optimization.

The above advantages come at the price of two model
parameters that have to be adequately picked, however, the in-
ﬂuence of either parameter is well-understood. One parameter
determines the amount of detected events which are interpreted

Fig. 11. Reconstruction error for varying particle distance for different detec-
tion rates, with and without scattering, and optimal β. All other parameters
are as in ﬁg. 5 (without scattering) and ﬁg. 8 (with scattering).

pd and ps are known, the nuclide intensity can in principle be
reconstructed accurately, but regularization adds a systematic
bias towards lower masses. For the chosen optimal values of
β, the reconstructed mass was around 87% of the true value.
Reconstructions of data with scattering, with and without
proper detection of scattered events are visualized in ﬁg. 10.

F. Particle distance

In ﬁg. 11 the reconstruction errors for different detection
rates with and without scattering are investigated for varying
distances between the particles. For sufﬁciently large distances
the reconstruction error is low and approximately constant,
indicating that particles can be separated cleanly. As particles
come closer the reconstruction error increases. Eventually it
decreases again when the particles are essentially merged.
Visualizations of reconstructions for various particle distances
for the data of ﬁg. 11 with scattering are shown in ﬁg. 12.

V. CONCLUSION

We presented a mathematical model and algorithm for the
reconstruction of a temporally changing radionuclide distri-
bution within a patient from PET listmode data. As opposed

as having undergone scattering (or coming from background
noise) and are thus discarded. Since the amount of scattering
and noise is known for each setting (it depends on the PET
scanner, the radionuclide and the imaged object), this param-
eter can be easily tuned (once and for all for each setting)
by comparing the estimated scatter ratio to the expected one.
The other parameter represents the regularization strength, and
we have shown how to choose it optimally depending on
radionuclide halﬂife and expected particle velocity.

We tested our method on simulated data for imaging a
human in a standard clinical PET scanner, using four cells or
particles, each with 160-800 Bq activity and 3.1 mm/s velocity
(reconstructions of larger numbers of cells would exhibit the
same quantitative behaviour). For example, at 240 Bq per cell
(corresponding to 1.1 cps per cell), four cells could be tracked
simultaneously with an accuracy of 5.3 mm. The particle
trajectories can clearly be identiﬁed and separated up to a
distance comparable to the typical positron range (roughly two
to ﬁve times, depending on the activity).

The chosen cell test setting is highly challenging but not
completely unrealistic under the assumption that a single
cell can be labelled with an activity of ∼ 200 Bq per cell.
Although the activity is usually in the range of a few Bq
per cell for standard labelling protocols (e.g. [20], [21]), it
was already demonstrated that high activity levels in the range
of 50-70 Bq/cell could be achieved using F-18-FDG or Ga-
68 labelled cancer cells [10], [22]. PET isotopes with short
half-life would be preferable for tracking fast moving cells as
less molecules need to be attached to the cell providing high
activity. Conversely, long living isotopes, such as Zr-89 with
a half-life of 78.4 h, would allow to track cells over several
days albeit with limited activity per cell. Thus, the choice of
isotope and labelling efﬁciency will be crucial for tracking
accuracy. Furthermore, moving from a simulated clinical PET
scanner to a total-body human scanner, of which prototypes are
under evaluation, the sensitivity and thus detection rate could
be increased substantially (a 40-fold effective sensitivity was
predicted in [23]). This means that the radioactivity per cell
could be reduced accordingly or particles could be tracked at
higher speed, e.g. also inside large vessels such as the aorta.
Finally, reconstruction results could be further improved
by employing more a priori knowledge. Here we completely
ignored any anatomic information (which could come from
PET-MR or PET-CT systems), and rather than regularizing
the kinetic energy quantities such as the acceleration might
be biologically more relevant. Also, any time-of-ﬂight (TOF)
information of the coincidence events is ignored in our model
and its incorporation would require weaker regularization as
the reconstruction problem becomes less ill-posed and the
signal-to-noise characteristics are substantially improved (as
is typically observed in modern PET systems). Thus, the bias
introduced by our transport regularization will be reduced,
enhancing the tracking accuracy of our algorithm further.

REFERENCES

[1] T. Whiteside, S. Demaria, M. Rodriguez-ruiz, H. Zarour, and I. Melero,
“CCR FOCUS Emerging Opportunities and Challenges in Cancer Im-
munotherapy,” Clin. Cancer Res., no. 13, pp. 1845–1856, 2016.

10

[2] V. Ponomarev, “Advancing Immune and Cell-Based Therapies Through

Imaging,” Mol. Imaging Biol., no. March, pp. 379–384, 2017.

[3] M. F. Kircher, S. S. Gambhir, and J. Grimm, “Noninvasive cell-tracking
methods,” Nat. Rev. Clin. Oncol., vol. 8, no. 11, pp. 677–688, nov 2011.
[4] R. Tavar´e, H. Escuin-Ordinas, S. Mok, M. N. McCracken, K. A. Zettlitz,
F. B. Salazar, O. N. Witte, A. Ribas, and A. M. Wu, “An effective
immuno-PET imaging method to monitor CD8-dependent responses to
immunotherapy,” Cancer Res., vol. 76, no. 1, pp. 73–82, jan 2016.
[5] A. Bansal, M. K. Pandey, Y. E. Demirhan, J. J. Nesbitt, R. J. Crespo-
Diaz, A. Terzic, A. Behfar, and T. R. DeGrado, “Novel (89)Zr cell
labeling approach for PET-based cell trafﬁcking studies.” EJNMMI Res.,
vol. 5, p. 19, jan 2015.

[6] K. S. Lee, T. J. Kim, and G. Pratx, “Single-Cell Tracking With PET
Using a Novel Trajectory Reconstruction Algorithm,” IEEE Trans. Med.
Imaging, vol. 34, no. 4, pp. 994–1003, apr 2015.

[7] T. Nichols, J. Qi, E. Asma, and R. Leahy, “Spatiotemporal reconstruction
of list-mode pet data,” IEEE Trans. Med. Imaging, vol. 21, no. 4, pp.
396–404, apr 2002.

[8] G. Wang, “High temporal-resolution dynamic pet image reconstruction
using a new spatiotemporal kernel method,” IEEE Trans. Med. Imaging,
vol. 38, no. 3, pp. 664–674, mar 2019.

[9] Y. Ouyang, T. J. Kim, and G. Pratx, “Evaluation of a BGO-Based
PET System for Single-Cell Tracking Performance by Simulation and
Phantom Studies,” Mol. Imaging, vol. 15, p. 153601211664648, 2016.
[10] S. T. Langford, C. S. Wiggins, R. Santos, M. Hauser, J. M. Becker, and
A. E. Ruggles, “Three-dimensional spatiotemporal tracking of ﬂuorine-
18 radiolabeled yeast cells via positron emission particle tracking,”
PLOS ONE, vol. 12, no. 7, pp. 1–14, 2017.

[11] L. Chizat, G. Peyr´e, B. Schmitzer, and F.-X. Vialard, “Unbalanced
optimal transport: Dynamic and Kantorovich formulations,” J. Funct.
Anal., vol. 274, no. 11, pp. 3090–3123, 2018.

[12] J.-D. Benamou and Y. Brenier, “A computational ﬂuid mechanics
solution to the Monge-Kantorovich mass transfer problem,” Numerische
Mathematik, vol. 84, no. 3, pp. 375–393, 2000.

[13] N. Papadakis, G. Peyr´e, and E. Oudet, “Optimal transport with proximal
splitting,” SIAM J. Imaging Sci., vol. 7, no. 1, pp. 212–238, 2014.
[14] J. Kaipio and E. Somersalo, “Statistical inverse problems: Discretization,
model reduction and inverse crimes,” J. Comp. Appl. Math., vol. 198,
no. 2, pp. 493–504, 2007.

[15] A. Chambolle and T. Pock, “A ﬁrst-order primal-dual algorithm for
convex problems with applications to imaging,” J. Math. Imaging Vision,
vol. 40, no. 1, pp. 120–145, 2011.

[16] B. Schmitzer, “Stabilized sparse scaling algorithms for entropy regular-
ized transport problems,” to appear in SIAM J. Sc. Comp., 2016.
[17] S. Jan, G. Santin, D. Strul, S. Staelens, K. Assi´e, D. Autret, S. Avner,
R. Barbier, M. Bardi`es, P. M. Bloomﬁeld, D. Brasse, V. Breton,
P. Bruyndonckx, I. Buvat, A. F. Chatziioannou, Y. Choi, Y. H. Chung,
C. Comtat, L. Simon, T. Y. Song, J.-M. Vieira, D. Visvikis, R. Van
De Walle, E. Wie¨ers, and C. Morel, “GATE -Geant4 Application for
Tomographic Emission: a simulation toolkit for PET and SPECT,” Phys
Med Biol. Phys Med Biol, 2004.

[18] B. W. Jakoby, Y. Bercier, M. Conti, M. E. Casey, B. Bendriem, and
D. W. Townsend, “Physical and clinical performance of the mCT time-
of-ﬂight PET/CT scanner,” Phys. Med. Biol., 2011.

[19] W. Segars, G. Sturgeon, S. Mendonca, J. Grimes, and B. Tsui, “4D
XCAT phantom for multimodality imaging research,” Med. Phys., 2010.
[20] R. Meier, M. Piert, G. Piontek, M. Rudelius, R. A. Oostendorp,
R. Senekowitsch-Schmidtke, T. D. Henning, W. S. Wels, C. Uherek,
E. J. Rummeny et al., “Tracking of [18F] FDG-labeled natural killer cells
to HER2/neu-positive tumors,” Nuclear medicine and biology, vol. 35,
no. 5, pp. 579–588, 2008.

[21] L. Faivre, M. Chaussard, L. Vercellino, V. Vanneaux, B. Hosten,
K. Teixera, V. Parietti, P. Merlet, L. Sarda-Mantel et al., “18F-FDG
labelling of hematopoietic stem cells: Dynamic study of bone marrow
homing by PET-CT imaging and impact on cell functionality,” Current
research in translational medicine, vol. 64, no. 3, pp. 141–148, 2016.
[22] K. O. Jung, T. J. Kim, J. H. Yu, S. Rhee, W. Zhao, B. Ha, K. Red-Horse,
S. S. Gambhir, and G. Pratx, “CellGPS: Whole-body tracking of single
cells by positron emission tomography,” bioRxiv, p. 745224, jan 2019.
[23] S. Cherry, T. Jones, J. Karp, J. Qi, W. Moses, and R. Badawi, “Total-
body PET: maximizing sensitivity to create new opportunities for clinical
research and patient care,” J. Nucl. Med., vol. 59, no. 1, pp. 3–12, 2018.

SUPPLEMENTARY MATERIAL

Numerical optimization

We will perform numerical optimization of the discretized
problem with the primal-dual implicit gradient descent and
ascent from [15]. The (primal) variables of the problem are ρ
and ω, the mass and ﬂows on the time- and space-staggered
grids (cf. [13]) which we summarize as x = (ρ, ω)(cid:62). The
discrete problem can then be written as

Each of these proximal operators is separable into independent
operators per grid point or detector bin. The proximal operator
F ∗1 is described in [13], those for F ∗2 and F ∗3 can be deter-
mined by elementary calculations. The proximal operator of
H amounts to computing the projection onto solutions of the
continuity equation which involves the solution of the Poisson
equation on a Cartesian grid. This can be performed efﬁciently
with the FFT algorithm [13].

11

Wasserstein–Fisher–Rao metric

The WFR metric between two nonnegative densities µ, ν :

Ω → [0, ∞) is deﬁned via

d2
WFR,α(µ, ν) = inf

(cid:26)(cid:90) 1

(cid:90)

(cid:20) (cid:107)ωt(cid:107)2
ρt

2

+ α2 ζ 2
t
ρt

(cid:21)

dx dt

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ω
ρt : Ω → [0, ∞), ωt : Ω → R3, ζt : Ω → R for t ∈ [0, 1],
(cid:41)

0

(cid:19)
(cid:18)ρ0 = µ
ρ1 = ν

and ∂tρt + divωt = ζt

.

Above, the optimization variables are the temporally changing
material density ρt, the temporally changing vector ﬁeld ωt
describing a material transport, and ζt describing a temporal
change of material mass. For simplicity, in our notation we
treated ρt, ωt, and ζt as functions, but in the mathematically
rigorous deﬁnition the minimization is performed in the space
of Radon measures, i.e. ρ, ω, and ζ are all measures on [0, T ]×
Ω (for details see [11]).

By deﬁnition d2

WFR,α(µ, ν) is the minimum action to move
µ onto ν, where in contrast to S the action now is comple-
mented with a term for mass changes during transport. The
parameter α > 0 controls how much mass change is penalized
compared to transport. It can be interpreted as the length scale
across which a mass misplacement can be interpreted as a
localization error (in fact, it is known that mass transport only
happens up to a distance π · α in dWFR,α).

For two Dirac measures at x and y with masses mx and

my one obtains

d2
WFR,α(mx · δx, my · δy) =
4α2 (cid:0)mx + my − 2

√

mx my cos (cid:0) min{ (cid:107)

x

y

(cid:107)2α , π

−

2 }(cid:1)(cid:1) .

where K is a combination of linear operators given by

F (Kx) + H(x)

inf
x

K =







Qtime
0
id
A







.

0
Qspace
0
0

Here Qtime and Qspace are the interpolation operators from the
staggered grids to the centered grid [13] and A = (Ad
i,j +
p As
i,j)i,j is the discretized forward operator (restricted to bins
where Ei,j > 0). The functionals F and H are given by

F (ρc, ωc, ˜ρ, µ) = F1(ρc, ωc) + F2(˜ρ) + F3(µ),
β (cid:82) T
0
+∞
(cid:90)

F1(ρc, ωc) =

ωc,t(cid:107)
ρc,t

dx dt

(cid:82)
Ω

(cid:90) T

(cid:40)

2
2

(cid:107)

if ρc ≥ 0,
else,

F2(˜ρ) =

r ˜ρt dx dt,

F3(µ) = −

Ei,j log µi,j,

H(ρ, ω) =

if ∂tρt + divωt = 0,

0

Ω

M
(cid:88)

N
(cid:88)

i=1

j=1

(cid:40)
0
+∞ else.

Note that F and H are convex and lower-semicontinuous. The
dual variables corresponding to the arguments of F will be
denoted by y = (ϕ1,ρ, ϕ1,ω, ϕ2, ϕ3). Let τ, σ > 0 with τ ·
σ(cid:107)K(cid:107)2 < 1 where (cid:107)K(cid:107) denotes the operator norm of K.
Then, for initial primal and dual variables x(0) and y(0) the
algorithm is given for (cid:96) = 0, 1, . . . by

y((cid:96)+1) = ProxσF ∗
x((cid:96)+1) = Proxτ H

(cid:0)y((cid:96)) + σ K (2x((cid:96)) − x((cid:96)
(cid:0)x((cid:96)) − τ K (cid:62)y((cid:96)+1)(cid:1)

1))(cid:1),

−

1) = x(0). Here F ∗ denotes the
with the convention x(
Fenchel–Legendre conjugate of F and ProxσF ∗ denotes the
proximal operator of F ∗ with stepsize σ, deﬁned by

−

ProxσF ∗ (˜y) = argmin

y

(cid:8) 1
2 (cid:107)y − ˜y(cid:107)2 + σ · F ∗(y)(cid:9)

and analogously for H. The Fenchel–Legendre conjugate of
F decomposes into the conjugates of F1, F2 and F3,

F ∗(ϕ1,ρ, ϕ1,ω, ϕ2, ϕ3) = F ∗1 (ϕ1,ρ, ϕ1,ω) + F ∗2 (ϕ2) + F ∗3 (ϕ3),

and so does the proximal operator,

Proxσ F ∗ (ϕ1,ρ, ϕ1,ω, ϕ2, ϕ3) = (cid:0)Proxσ F ∗1

(ϕ1,ρ, ϕ1,ω),

Proxσ F ∗2

(ϕ2), Proxσ F ∗3

(ϕ3)(cid:1).

