Image Super-Resolution via Deep Recursive Residual Network

Ying Tai∗ 1, Jian Yang1, and Xiaoming Liu2

1Department of Computer Science and Engineering, Nanjing University of Science and Technology
2Department of Computer Science and Engineering, Michigan State University
{taiying, csjyang}@njust.edu.cn, liuxm@cse.msu.edu

Abstract

Recently, Convolutional Neural Network (CNN) based
models have achieved great success in Single Image Super-
Resolution (SISR). Owing to the strength of deep networks,
these CNN models learn an effective nonlinear mapping
from the low-resolution input image to the high-resolution
target image, at the cost of requiring enormous parameters.
This paper proposes a very deep CNN model (up to 52 con-
volutional layers) named Deep Recursive Residual Network
(DRRN) that strives for deep yet concise networks. Speciﬁ-
cally, residual learning is adopted, both in global and local
manners, to mitigate the difﬁculty of training very deep net-
works; recursive learning is used to control the model pa-
rameters while increasing the depth. Extensive benchmark
evaluation shows that DRRN signiﬁcantly outperforms state
of the art in SISR, while utilizing far fewer parameters.
Code is available at https://github.com/tyshiwo
/DRRN CVPR17.

1. Introduction

Single Image Super-Resolution (SISR) is a classic com-
puter vision problem, which aims to recover a high-
resolution (HR) image from a low-resolution (LR) image.
Since SISR restores the high-frequency information, it is
widely used in applications such as medical imaging [26],
satellite imaging [29], security and surveillance [37], where
high-frequency details are greatly desired.

In recent years, due to the powerful learning ability,
Deep Learning (DL) models, especially Convolutional Neu-
ral Networks (CNN), are widely used to address the ill-

∗This work was conducted when the ﬁrst author was a visiting scholar
at Michigan State University.
It was supported by the National Sci-
ence Fund of China under Grant Nos. 91420201, 61472187, 61502235,
61233011, 61373063 and 61602244, the 973 Program No.2014CB349303,
Program for Changjiang Scholars and Innovative Research Team in Uni-
versity, and partially sponsored by CCF-Tencent Open Research Fund.

DRCN

RED30

DRRN_B1U25

DRRN_B1U9

VDSR

CSCN

ESPCN

SRCNN

Figure 1. PSNR of recent CNN models for scale factor ×3 on
Set5 [1]. Red points are our models. △, ✩, and ◦ are models with
less than 5 layers, 20 layers, and more than 30 layers, respectively.
DRRN B1U9 means there is 1 recursive block, in which 9 residual
units are stacked. With the same depth but far fewer parameters,
DRRN B1U9 achieves better performance than the state-of-the-
art methods [13,14]. After increasing the depth without adding any
parameters, the 52-layer DRRN B1U25 further improves the per-
formance and signiﬁcantly outperforms VDSR [13], DRCN [14]
and RED30 [17] by 0.37, 0.21 and 0.21 dB respectively.

posed inverse problem of Super Resolution (SR), and have
demonstrated superiority over reconstruction-based meth-
ods [4, 35] or other learning paradigms [20, 22, 23, 31]. As
the pioneer CNN model for SR, Super-Resolution Convo-
lutional Neural Network (SRCNN) [2] predicts the nonlin-
ear LR-HR mapping via a fully convolutional network, and
signiﬁcantly outperforms classical non-DL methods. How-
ever, SRCNN does not consider any self similarity prop-
erty. To address this issue, the Deep Joint Super Resolution
(DJSR) jointly utilizes both the wealth of external examples
and the power of self examples unique to the input. Inspired
by the learning iterative shrinkage and thresholding algo-
rithm [5], Cascaded Sparse Coding Network (CSCN) [32]
is trained end-to-end to fully exploit the natural sparsity of
images. Shi et al. [25] observe that the prior models [2, 32]
increase LR image’s resolution via bicubic interpolation be-

13147

fore CNN learning, which increases the computational cost.
The Efﬁcient Sub-Pixel Convolutional neural Network (ES-
PCN) reduces the computational and memory complexity,
by increasing the resolution from LR to HR only at the end
of the network.

1, instead of 10−

One commonality among the above CNN models is that
their networks contain fewer than 5 layers, e.g., SRCNN [2]
uses 3 convolutional layers. Their deeper structures with 4
or 5 layers do not achieve better performance, which was
attributed to the difﬁculty of training deeper networks and
led to the observation that “the deeper the better” might not
be the case in SR. Inspired by the success of very deep net-
works [8, 27, 28] on ImageNet [21], Kim et al. [13, 14] pro-
pose two very deep convolutional networks for SR, both
stacking 20 convolutional layers, from the viewpoints of
training efﬁciency and storage, respectively. On the one
hand, to accelerate the convergence speed of very deep net-
works, the VDSR [13] is trained with a very high learning
4 in SRCNN) and the authors fur-
rate (10−
ther use residual learning and adjustable gradient clipping
to solve gradient explosion problem. On the other hand, to
control the model parameters, the Deeply-Recursive Con-
volutional Network (DRCN) [14] introduces a very deep re-
cursive layer via a chain structure with up to 16 recursions.
To mitigate the difﬁculty of training DRCN, the authors use
recursive-supervision and skip-connection, and adopt an en-
semble strategy to further improve the performance. Very
recently, Mao et al. [17] propose a 30-layer convolutional
auto-encoder network named RED30 for image restoration,
which uses symmetric skip connections to help training. All
of the three models learn the residual image between the in-
put Interpolated LR (ILR) image and the ground truth HR
image in the residual branch. The residual image is then
added to the ILR image from the identity branch to esti-
mate the HR image. The three models outperform the pre-
vious DL and non-DL methods by a large margin, which
demonstrates “the deeper the better” is still true in SR.

Despite achieving excellent performance, the very deep
networks require enormous parameters. Compared to the
compact models, large models demand more storage space
and are less applicable to mobile systems [6]. To address
this issue, we propose a novel Deep Recursive Residual
Network (DRRN) to effectively build a very deep network
structure, which achieves better performance, but with 2×,
6×, and 14× fewer parameters than VDSR, DRCN, and
RED30, respectively. In a nutshell, DRRN advances the SR
performance with a deeper yet concise network. Speciﬁ-
cally, DRRN has two major algorithmic novelties:
(1) Both global and local residual learning are introduced
in DRRN. In VDSR and DRCN, the residual image is es-
timated from the input and output of the networks, termed
as Global Residual Learning (GRL). Since the SR output is
vastly similar to the input, GRL is effective in easing the dif-

ﬁculty of training deep networks. Therefore, we also adopt
GRL in our identity branch. Further, very deep networks
could suffer from the performance degradation problem,
as observed in visual recognition [8] and image restora-
tion [17]. The reason may be a signiﬁcant amount of image
details are lost after so many layers. To address this issue,
we introduce an enhanced residual unit structure, termed
as multi-path mode Local Residual Learning (LRL), where
the identity branch not only carries rich image details to late
layers, but also helps gradient ﬂow. GRL and LRL mainly
differ in that LRL is performed in every few stacked lay-
ers, while GRL is performed between the input and output
images, i.e., DRRN has many LRLs and only 1 GRL.
(2) Recursive learning of residual units is proposed in
In DRCN [14], a
DRRN to keep our model compact.
deep recursive layer (up to 16 convolutional recursions)
is learned and the weights are shared in the 16 convolu-
tional recursions. Our DRRN has two major differences
compared to DRCN: (a) Unlike DRCN that shares weights
among convolutional layers, DRRN has a recursive block
consisting of several residual units, and the weight set is
shared among these residual units. (b) To address the van-
ishing/exploding gradients problem of very deep models,
DRCN supervises every recursion so that the supervision
on early recursions help backpropagation. DRRN is re-
lieved from this burden by designing a recursive block with
a multi-path structure. Our model can be easily trained even
with 52 convolutional layers. Last but not least, through re-
cursive learning, DRRN can improve accuracy by increas-
ing depth without adding any weight parameters.

To illustrate the effectiveness of the two strategies used
in DRRN, Fig. 1 shows the Peak Signal-to-Noise Ratio
(PSNR) performance of several recent CNN models for
SR [2, 13, 14, 17, 25, 32] versus the number of parameters,
denoted as k. Compared to the prior CNN models, DRRN
achieves the best performance with fewer parameters.

2. Related Work

Since Sec. 1 overviews DL-based SISR, this section fo-
cuses on three most related work to ours: ResNet [8],
VDSR [13] and DRCN [14]. Fig. 2 illustrates these models
via simpliﬁed network structures with only 6 convolutional
layers, where the activation functions, batch normalization
(BN) [11] and ReLU [19], are omitted for clarity.

2.1. ResNet

The main idea of ResNet [8] is to use a residual learn-
ing framework to ease the training of very deep networks.
Instead of hoping every few stacked layers directly ﬁt the
desired underlying mapping, the authors explicitly let these
layers ﬁt a residual mapping, which is assumed to be easier
for optimization. Denoting the input as x and the underly-
ing mapping as H(x), the residual mapping is deﬁned as

3148

Input

conv

Å

conv

conv

conv

conv

Å

conv

Output

Input

Input

Input

conv

conv

conv

conv

conv

conv

Å

Output

conv

conv

conv

conv

conv

conv

Output 1

Output 4

w1

w4

...

Å

Output

(c) DRCN

conv

conv

conv

Å

conv

conv

Å

conv

Å

Output

(a) ResNet

(b) VDSR

(d) DRRN (ours)

Figure 2. Simpliﬁed structures of (a) ResNet [8]. The green dashed box means a residual unit. (b) VDSR [13]. The purple line refers to a
global identity mapping. (c) DRCN [14]. The blue dashed box refers to a recursive layer, among which the convolutional layers (with light
green color) share the same weights. (d) DRRN. The red dashed box refers to a recursive block consisting of two residual units. In the
recursive block, the corresponding convolutional layers in the residual units (with light green or light red color) share the same weights. In
all four cases, the outputs with light blue color are supervised, and L is the element-wise addition.

Method
ResNet
VDSR
DRCN

DRRN

Key Strategies
Chain mode local residual learning
Global residual learning
Global residual learning + recursive learning (single weight layer) + multi-CNN ensemble
Multi-path mode local residual learning
+ global residual learning + recursive learning (multiple weight layers in the residual unit)

Mathematical Formulation
y = fRec(UU(UU−1(...(U1(f1(x)))...)))
y = fRec(fd−1(fd−2(...(f1(x))...))) + x
y = ΣT
2 (f1(x))) + x)
y = fRec(RB(RB−1(...(R1(x))...))) + x

t=1wt · (fRec(f (t)

Table 1. Strategies used in ResNet [8], VDSR [13], DRCN [14] and DRRN. U, d, T, and B are the numbers of residual units in ResNet,
convolutional layers in VDSR, recursions in DRCN, and recursive blocks in DRRN, respectively. x and y are input and output of networks.
f denotes function of convolutional layer. U denotes function of residual unit structure and R denotes function of our recursive block.

F(x) := H(x) − x, and a residual unit structure is thus:

ˆx = U (x) = σ(F(x, W ) + h(x)),

(1)

where ˆx is the output of the residual unit, h(x) is an iden-
tity mapping [8] : h(x) = x, W is a set of weights (the bi-
ases are omitted to simplify notations), function σ denotes
ReLU, F(x, W ) is the residual mapping to be learned, and
U denotes the function of the residual unit structure. For
a basic residual unit that stacks two convolutional layers,
F(x, W ) = W2σ(W1x). By stacking such structures to
construct a very deep 152-layer network, ResNet won the
ﬁrst place in the ILSVRC 2015 classiﬁcation competition.
Since the residual learning in ResNet is adopted in every
few stacked layers, this strategy is a form of local residual
learning, where residual units are stacked in the chain mode.

2.2. VDSR

Differing from ResNet that uses residual learning in ev-
ery few stacked layers, VDSR [13] introduces GRL, i.e.,
residual learning between the input ILR image and the out-
put HR image. There are three notes for VDSR: (1) Un-

like SRCNN [2] that only uses 3 layers, VDSR stacks 20
weight layers (3 × 3 for each layer) in the residual branch,
which leads to a much larger receptive ﬁeld (41 × 41 vs.
13 × 13). (2) GRL and adjustable gradient clipping enable
VDSR to converge very fast (∼4 hours on GPU Titan Z).
(3) By adopting scale augmentation, a single network of
VDSR is robust to images with different scales. Later, we
will show that VDSR actually is a special case of DRRN,
when there’s no residual unit in our recursive block.

2.3. DRCN

DRCN [14] is motivated by the observation that adding
more weight layers introduces more parameters, where the
model is likely to overﬁt and also becomes disk hungry. To
address these issues, the authors introduce a recursive layer
into the network, so that the model parameters do not in-
crease while more recursions are performed in the recursive
layer. DRCN consists of three parts: embedding net, infer-
ence net and reconstruction net, which are illustrated as the
ﬁrst, middle 4, and last convolutional layer(s) in Fig. 2(c),
respectively. The embedding net f1(x) represents a given

3149

H0

Hu-1

conv

conv

Å

Hu

BN

ReLU

weight

Figure 3. A close look at the u-th residual unit in DRRN. The black
dashed box represents the residual function F , which consists of
two “conv” layers and each stacked by BN-ReLU-weight layers.

image x as feature maps H0. The inference net f2(H0)
stacks T recursions (T = 16 in [14]) in a recursive layer,
with shared weights among these recursions. Finally, the
reconstruction net fRec(HT), where HT is the output of the
inference net, generates the intermediate HR image. How-
ever, since training such a deep network is difﬁcult, the au-
thors further propose two mitigations, recursive-supervision
and skip-connection. Speciﬁcally, for the t-th intermediate
recursion in the recursive layer, the output after reconstruc-
tion net is formulated as

yt = fRec(f (t)

2 (f1(x))) + x,

(2)

where x is skip-connection and basically a GRL. Each in-
termediate prediction yt is learned with supervision. Fi-
nally, an ensemble strategy is adopted and the output is the
weighted average of all predictions y = ΣT
t=1wt · yt, with
weights wt learned during training.

3. Deep Recursive Residual Network

In this section, we present the technical parts of our pro-
posed DRRN. Speciﬁcally, we adopt global residual learn-
ing in the identity branch and introduce recursive learn-
ing into the residual branch by constructing the recursive
block structure, in which several residual units are stacked.
Noted that in ResNet [8], different residual units use dif-
ferent inputs for the identity branch (green dashed boxes in
Fig. 2(a)). However, in our recursive block, a multi-path
structure is used and all the residual units share the same in-
put for the identity branch (green dashed boxes in Fig. 2(d)),
which further facilitates the learning [16]. We highlight the
differences of the network structures between DRRN and
the related models in Tab. 1. Now, we will gradually present
more details of our model, from the residual unit to the re-
cursive block and ﬁnally the whole network structure.

3.1. Residual Unit

In ResNet [8], the basic residual unit is formulated as
Eq. 1 and the activation functions (BN [11] and ReLU [19])
are performed after the weight layers. In contrast to such
a “post-activation” structure, He et al. [9] propose a “pre-
activation” structure, which performs the activation before
the weight layers. They claim that the pre-activation version

xb-1

xb-1

conv

conv

conv

Å

conv

conv

Å

xb

conv

conv

conv

Å

xb

xb-1

conv

conv

conv

Å

conv

conv

Å

conv

conv

Å

xb

(a) U=1

(b) U=2

(c) U=3

Figure 4. Structures of our recursive blocks. U means number of
residual units in the recursive block.

is much easier to train and generates better performance
than the post-activation version. Speciﬁcally, the residual
unit with pre-activation structure is formulated as

H u = F(H u

1, W u) + H u

1,

−

−

(3)

−

where u = 1, 2, ..., U, U is the number of residual units in a
recursive block, H u
1 and H u are the input and output of
the u-th residual unit, and F denotes the residual function.
Instead of directly using the above residual unit, we mod-
ify Eq. 3 so that the inputs to the identity branch and the
residual branch are different. As described in the beginning
of Sec. 3, the inputs to all of the identity branches of the
residual units in one recursive block are kept the same, i.e.,
H 0 in Fig. 3. As a result, there are multiple paths between
the input and output of our recursive block, as shown in
Fig. 4. The residual paths help to learn highly complex fea-
tures and the identity paths help gradient backpropagation
during training. Compared to the chain mode, this multi-
path mode facilitates the learning and is less prone to over-
ﬁtting [16]. Therefore, we formulate our residual unit as

H u = G(H u

1) = F(H u

1, W ) + H 0,

−

−

(4)

where G denotes the function of our residual unit, H 0 is the
result of the ﬁrst convolutional layer in the recursive block.
Since the residual unit is recursively learned, the weight set
W is shared among the residual units within a recursive
block, but different across different recursive blocks.

3.2. Recursive Block

We now introduce the details of our recursive block.
First, we illustrate the structure of our recursive blocks in
Fig. 4. Motivated by [16], we introduce a convolutional
layer at the beginning of the recursive block, and then sev-
eral residual units mentioned in Sec. 3.1 are stacked. We

3150

x

RB

RB

RB

RB

RB

RB

conv

Å

y

x3

conv

conv

conv

Å

conv

conv

Å

conv

conv

Å

x4

Figure 5. An example network structure of DRRN with B = 6 and
U = 3. Here, “RB” layer refers to a recursive block.

1 and xb
denote B as the number of recursive blocks, xb
(b = 1, 2, ..., B) as the input and output of the b-th recursive
block, and H 0
1
−
through the ﬁrst convolutional layer, whose function is fb.
According to Eq. 4, the result of u-th residual unit is

1) as the result after passing xb

b = fb(xb

−

−

H u

b = G(H u

−

1) = F(H u
b

1

, Wb) + H 0
b .

−

(5)

Thus, the output of the b-th recursive block xb is
xb = H U

1)) = G(G(...(G(fb(xb

b = G(U)(fb(xb

−

where U-fold operations of Gb are performed.

1)))...)),
(6)

−

3.3. Network Structure

Finally, we simply stack several recursive blocks, fol-
lowed by a convolutional layer reconstructing the residual
between the LR and HR images. The residual image is then
added to the global identity mapping from the input LR im-
age. The entire network structure of DRRN is illustrated in
Fig. 5. Actually, VDSR [13] can be viewed as a special case
of DRRN, i.e., when U = 0, DRRN becomes VDSR.

DRRN has two key parameters:

the recursive block
number B and the residual unit number U in each recur-
sive block. Given different B and U, we can learn DRRN
with different depths – the number of convolutional layers.
Speciﬁcally, the depth of DRRN d is calculated as:

d = (1 + 2 × U) × B + 1.

Denoting x and y to be the input and output of DRRN,

R to be the function of the b-th recursive block, we have

xb = Rb(xb

1) = G(U)(fb(xb

1)).

−

−

(7)

(8)

where fRec is a function for the last convolutional layer in
DRRN to reconstruct the residual. Tab. 1 lists the mathe-
matical formulations of ResNet, VDSR, DRCN and DRRN.
i=1, where N is the num-
ber of training patches and ˜x(i) is the ground truth HR patch
of the LR patch x(i), the loss function of DRRN is

Given a training set {x(i), ˜x(i)}N

L(Θ) =

k˜x(i) − D(x(i))k2,

(10)

1
2N

N

X
i=1

where Θ denotes the parameter set. The objective function
is optimized via the mini-batch stochastic gradient descent
(SGD) with backpropagation [15]. We implement DRRN
via Caffe [12].

4. Experiments
4.1. Datasets

By following [13, 23], we use a training dataset of 291
images, where 91 images are from Yang et al. [35] and other
200 images are from Berkeley Segmentation Dataset [18].
For testing, we utilize four widely used benchmark datasets,
Set5 [1], Set14 [36], BSD100 [18] and Urban100 [10],
which have 5, 14, 100 and 100 images respectively.

4.2. Implementation Details

Data augmentation is performed on the 291-image train-
ing dataset. Inspired by [30], the ﬂipped and rotated ver-
sions of the training images are considered. Speciﬁcally,
we rotate the original images by 90◦, 180◦, 270◦ and ﬂip
them horizontally. After that, for each original image, we
have 7 additional augmented versions. Besides, inspired by
VDSR [13], we also use scale augmentation to train our
model, and images with different scales (×2, ×3 and ×4)
are all included in the training set. Therefore, for all differ-
ent scales, we only need to train a single model.

Training images are split into 31 × 31 patches, with the
stride of 21, by considering both the training time and stor-
age complexities. We set the mini-batch size of SGD to 128,
4.
momentum parameter to 0.9, and weight decay to 10−
Every weight layer has 128 ﬁlters of the size 3 × 3.

For weight initialization, we use the same method as He
et al. [7], which is shown to be suitable for networks utiliz-
ing ReLU. The initial learning rate is set to 0.1 and then de-
creased to half every 10 epochs. Since a large learning rate
is used in our work, we adopt the adjustable gradient clip-
ping [13] to boost the convergence rate while suppressing
exploding gradients. Speciﬁcally, the gradients are clipped
to [− θ
γ ], where γ is the current learning rate and θ = 0.01
is the gradient clipping parameter. Training a DRRN of
d = 20 roughly takes 4 days with 2 Titan X GPUs.

γ , θ

When b = 1, we deﬁne x0 = x. Then, DRRN can be
formulated as

4.3. Study of B and U

y = D(x) = fRec(RB(RB

1(...(R1(x))...))) + x,

(9)

−

In this subsection, we explore various combinations of B
and U to construct different DRRN structures with different

3151

Set5

Set14

Dataset

Scale Bicubic
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

RFL [23]
36.54/0.9537
32.43/0.9057
30.14/0.8548
32.26/0.9040
29.05/0.8164
27.24/0.7451
31.16/0.8840
28.22/0.7806
26.75/0.7054
29.11/0.8904
25.86/0.7900
24.19/0.7096
Table 2. Benchmark results. Average PSNR/SSIMs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, BSD100 and Urban100. Red
color indicates the best performance of our methods and blue color indicates the best performance of previous methods.

DRRN B1U9 DRRN B1U25
37.66/0.9589
33.93/0.9234
31.58/0.8864
33.19/0.9133
29.94/0.8339
28.18/0.7701
32.01/0.8969
28.91/0.7992
27.35/0.7262
31.02/0.9164
27.38/0.8331
25.35/0.7576

VDSR [13]
37.53/0.9587
33.66/0.9213
31.35/0.8838
33.03/0.9124
29.77/0.8314
28.01/0.7674
31.90/0.8960
28.82/0.7976
27.29/0.7251
30.76/0.9140
27.14/0.8279
25.18/0.7524

DRCN [14]
37.63/0.9588
33.82/0.9226
31.53/0.8854
33.04/0.9118
29.76/0.8311
28.02/0.7670
31.85/0.8942
28.80/0.7963
27.23/0.7233
30.75/0.9133
27.15/0.8276
25.14/0.7510

SelfEx [10]
36.49/0.9537
32.58/0.9093
30.31/0.8619
32.22/0.9034
29.16/0.8196
27.40/0.7518
31.18/0.8855
28.29/0.7840
26.84/0.7106
29.54/0.8967
26.44/0.8088
24.79/0.7374

SRCNN [2]
36.66/0.9542
32.75/0.9090
30.48/0.8628
32.45/0.9067
29.30/0.8215
27.50/0.7513
31.36/0.8879
28.41/0.7863
26.90/0.7101
29.50/0.8946
26.24/0.7989
24.52/0.7221

37.74/0.9591
34.03/0.9244
31.68/0.8888
33.23/0.9136
29.96/0.8349
28.21/0.7720
32.05/0.8973
28.95/0.8004
27.38/0.7284
31.23/0.9188
27.53/0.8378
25.44/0.7638

33.66/0.9299
30.39/0.8682
28.42/0.8104
30.24/0.8688
27.55/0.7742
26.00/0.7027
29.56/0.8431
27.21/0.7385
25.96/0.6675
26.88/0.8403
24.46/0.7349
23.14/0.6577

Urban100

BSD100

more

params

less

Figure 6. PSNR of various DRRNs at B and U combinations. The
color of the point indicates the PSNR that corresponds to the bar
on the right and 4 depth contours (d = 50, 30, 20, 10) are also
plotted. The tests are conducted for scale factor ×3 on Set5.

depths, and see how the two parameters affect the perfor-
mance. In Fig. 6, we build a grid of B and U, and sample
several points in the grid with the depth ranging from 8 to 52
layers. The parameter number keeps the same when more
residual units are used in one recursive block, and linearly
increases when more recursive blocks are stacked.

First, to clearly show how a single parameter affects
DRRN, we ﬁx one parameter to 3 and change the other
from 1 to 4. Fig. 6 shows that increasing B or U results
in deeper models and achieves better performance, which
indicates deeper is still better. Despite different structures,
these models are comparable as long as their depths are sim-
ilar, e.g., B2U3 (d = 15, k = 784K) and B3U2 (d = 16,
k = 1, 182K) achieve 33.76 and 33.77 dB, respectively.

The structures mentioned above all use the recursive
learning strategy. Next, we test three very different struc-
tures to demonstrate the effectiveness of such a strategy.
Speciﬁcally, we ﬁx one parameter to 1 and change the other
to construct networks with d = 52. This results in two ex-
treme structures: B1U25 (k = 297K) and B17U1 (k =
7, 375K). For B1U25, only one recursive block is used, in
which 25 residual units are recursively learned. For B17U1,
17 recursive blocks are stacked, with no recursive learn-

ing. We also construct a normal structure B3U8 (d = 52,
k = 1, 182K). Fig. 6 shows that despite different struc-
tures, the three networks achieve comparable performance
(B17U1 34.03 dB, B3U8 34.04 dB and B1U25 34.03 dB)
and outperform the pervious shallow networks. Thanks to
the recursive learning strategy, B1U25 can achieve state-of-
the-art results using far fewer parameters.

4.4. Comparison with State of the Art Models

We now provide quantitative and qualitative compar-
isons. Considering both the performance and number of
parameters, we choose DRRN B1U25 (d = 52, k = 297K)
as our best model. For fair comparison, we also construct
a DRRN B1U9 (d = 20, k = 297K) structure, which has
the same depth as VDSR and DRCN, but fewer parameters.
Both the DL [2, 13, 14] and non-DL [10, 20, 23] methods in
recent years are used for benchmark. Experimental setting
is kept the same as those previous methods. Speciﬁcally, we
ﬁrst apply bicubic interpolation to the color components of
an image and all models are applied to its luminance com-
ponent only. Therefore, the input and output images are of
the same size. For fair comparison, similar to [2, 13, 14, 23],
we crop pixels near image boundary before evaluation, al-
though this is unnecessary for DRRN.

Tab. 2 summarizes quantitative results on the four test-
ing sets, by citing the results of prior methods from [13,14].
The two DRRN models outperforms all existing methods in
all datasets and scale factors, in both PSNR and Structural
SIMilarity (SSIM) 1. Especially on the recent difﬁcult Ur-
ban100 dataset [10], DRRN signiﬁcantly advances the state
of the art, with the improvement margin of 0.47, 0.38, and
0.26 dB on scale factor ×2, ×3 and ×4 respectively.

Further, we also use another metric: Information Fidelity
Criterion (IFC) [24] for comparison, which claims to have
the highest correlation with perceptual scores for SR evalu-
ation [34]. The results are presented in Tab. 3. Note that

1With two convolutional layers in the residual branch, DRRN achieves
state-of-the-art performance. More complex designs have the potential to
improve performance but are not the focus of this work.

3152

Dataset

Set5

Set14

Urban100

Scale
×2
×3
×4
×2
×3
×4
×2
×3
×4

Bicubic
6.083
3.580
2.329
6.105
3.473
2.237
6.245
3.620
2.361

SRCNN [2]
8.036
4.658
2.991
7.784
4.338
2.751
7.989
4.584
2.963

SelfEx [10]
7.811
4.748
3.166
7.591
4.371
2.893
7.937
4.843
3.314

RFL [23]
8.556
4.926
3.191
8.175
4.531
2.919
8.450
4.801
3.110

PSyCo [20]
8.642
5.083
3.379
8.280
4.660
3.055
8.589
5.031
3.351

VDSR [13]
8.569
5.221
3.547
8.178
4.730
3.133
8.645
5.194
3.496

DRRN B1U9
8.583
5.241
3.581
8.181
4.732
3.147
8.653
5.259
3.536

DRRN B1U25
8.671
5.397
3.703
8.320
4.878
3.252
8.917
5.456
3.676

Table 3. Benchmark results. Average IFCs for the scale factor ×2, ×3 and ×4 on datasets Set5, Set14 and Urban100. Red color indicates
the best performance of our methods and blue color indicates the best performance of previous methods.

Ground Truth

Bicubic

SRCNN

SelfEx

VDSR

DRRN_B1U9

DRRN_B1U25

(PSNR/SSIM/IFC)

(21.10/0.7046/3.134) (21.77/0.7540/3.761) (21.94/0.7608/3.669) (22.58/0.7942/4.341) (22.74/0.7999/4.365) (23.37/0.8158/4.713)

(PSNR/SSIM/IFC)

(22.55/0.7073/3.591) (23.51/0.7608/4.344) (23.42/0.7587/4.281) (23.99/0.7728/4.716) (24.23/0.7781/4.734) (24.41/0.7805/4.914)

(PSNR/SSIM/IFC)

(21.98/0.8126/1.920) (24.80/0.8928/2.666) (24.85/0.9076/2.941) (25.85/0.9289/3.406) (26.33/0.9365/3.557) (26.48/0.9415/3.822)

Figure 7. Qualitative comparison. (1) The ﬁrst row shows image “img059” (Urban100 with scale factor ×3). DRRN recovers sharp lines,
while others all give blurry results. (2) The second row shows image “253027” (BSD100 with scale factor ×3). DRRN accurately recovers
the pattern. (3) The last row shows image “ppt3” (Set14 with scale factor ×4). Texts in DRRN are sharp, while others are blurry.

the results of [2, 10, 20, 23] are cited from [20] 2, while
the results of VDSR come from our re-implementation.
Similar to DRRN, the VDSR re-implementation also uses
BN and ReLU as the activation functions, unlike the orig-
inal VDSR [13] that does not use BN. These results are

2Since PSyCo [20] does not present complete PSNR/SSIM perfor-

mance on the four benchmarks, we do not include it in Tab. 2.

faithful since our VDSR re-implementation achieves similar
benchmark performance as [13] reported in Tab. 2. Since
only Set5, Set14 and Urban100 are used in [20], we omit
BSD100 in this test. It is clear that DRRN still outperforms
all existing methods in all datasets and scale factors. Re-
garding the speed, our 20-layer B1U9 network takes 0.25
second to process a 288 × 288 image on a Titan X GPU.

3153

16 layers

22 layers

28 layers

Shallow

Deep

Very deep

Figure 8. PSNR for scale factor ×3 on Set5 using VDSR (blue),
DRRN NS (green), DRRN C (cyan) and DRRN (red).

Methods
Loc. Res. L
Recu. L
Multi-path
PSNR

×
×
×
33.86

VDSR

DRRN NS C

DRRN NS

DRRN C

DRRN

√

×
×
33.92

√

×
√
33.97

√
√

×
33.95

√
√
√
33.99

Table 4. Average PSNR when different DRRN components are
turned on or off, for scale factor ×3 on dataset Set5.

among

Qualitative

comparisons

SRCNN [2],
SelfEx [10], VDSR [13] and DRRN are illustrated in
Fig. 7. For SRCNN and SelfEx, we use their public codes.
For VDSR, we use our re-implementation. As we can see,
our method produces relatively sharper edges with respect
to patterns, while other methods may give blurry results.

4.5. Discussions

Since global residual learning has been well discussed
in [13], in this section, we mainly focus on local residual
learning (LRL), recursive learning and multi-path structure.
Local Residual Learning To demonstrate the effective-
ness of LRL, DRRN is compared with VDSR [13], which
has no LRL. For fair comparison, the depth and number of
parameters are kept the same for both methods. Speciﬁ-
cally, we evaluate three depths: 16(B3U2), 22(B3U3), and
28(B3U4) convolutional layers. Each convolutional layer
has 128 ﬁlters with the size 3 × 3. To keep the parameter
number the same, in this test we do not share the weight
set of the residual units in one recursive block, and denote
this DRRN structure as DRRN NS. Fig. 8 shows the PSNR
of both methods in different depths. We see that the LRL
strategy consistently improves VDSR at all depths.
Recursive Learning To contrast our recursive learning
strategy, the three DRRN NS versions are compared with
the three weight-shared versions (Fig. 8). Storage is an im-
portant factor to consider when building a deep model. The
recursive learning strategy can reduce the storage demand
and keep a concise model while increasing its depth. Inter-
estingly the weight-shared DRRN versions achieve compa-
rable or even better performance than DRRN NS versions

Figure 9. Comparing deep and shallow models proposed in recent
three years that report PSNR for scale factor ×3 on Set5 and Set14.

while using only a small fraction of parameters, which in-
dicates when limited training set (e.g., 291 images) is used,
the recursive learning is indeed effective under the same
structure, and less prone to overﬁtting [16].
Multi-Path Structure To demonstrate the effectiveness
of multi-path structure, we compare DRRN with the chain
structure, denoted as DRRN C. As shown in Fig. 8, with the
same depth and parameter number, the multi-path structures
achieve higher PSNR than the corresponding chain struc-
tures in all three cases. Further, Tab. 4 presents the compre-
hensive study on performance gains using network B3U4 as
the example. It shows how different technical parts improve
the performance compared to the baseline VDSR.
Deep vs. Shallow Finally, we give a comparison of the deep
and shallow SISR models, which are published in recent
three years (2014 to 2016) that report PSNR for scale factor
×3 on datasets Set5 and Set14. Shallow (non-DL) mod-
els include A+ [31], SelfEx [10], RFL [23], NBSRF [22],
PSyCo [20] and IA [30]. The deep models (d ≤ 8) in-
clude SRCNN [2], DJSR [33], CSCN [32], ESPCN [25]
and FSRCNN [3]. Very deep models (d ≥ 20) include
VDSR [13], DRCN [14], RED [17] and DRRN with d = 20
and 52. Fig. 9 shows that 1) very deep models signif-
icantly outperform the shallow models; 2) DRRN B1U9
(d = 20, k = 297K) already outperforms the state of the
arts with the same depth but fewer parameters; 3) a deeper
DRRN B1U25 (d = 52, k = 297K) further improves the
performance without adding any parameters.

5. Conclusions

In this paper, we propose Deep Recursive Residual Net-
work (DRRN) for single image super-resolution. In DRRN,
an enhanced residual unit structure is recursively learned in
a recursive block, and we stack several recursive blocks to
learn the residual image between the HR and LR images.
The residual image is then added to the input LR image
from a global identity branch to estimate the HR image.
Extensive benchmark experiments and analysis show that
DRRN is a deep, concise, and superior model for SISR.

3154

References

[1] C. M. Bevilacqua, A. Roumy, and M.-L. A. Morel. Low-
complexity single-image super-resolution based on nonneg-
ative neighbor embedding. In BMVC, 2012. 1, 5

[2] C. Dong, C. Loy, K. He, and X. Tang. Image super-resolution
IEEE Transactions on
using deep convolutional networks.
Pattern Analysis and Machine Intelligence, 38(2):295–307,
2016. 1, 2, 3, 6, 7, 8

[3] C. Dong, C. Loy, and X. Tang. Accelerating the super-
In ECCV, 2016.

resolution convolutional neural network.
8

[4] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a

single image. In ICCV, 2009. 1

[5] K. Gregor and Y. LeCun. Learning fast approximations of

sparse coding. In ICML, 2010. 1

[6] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In ICLR, 2016. 2

[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rec-
tiﬁers: Surpassing human-level performance on ImageNet
classiﬁcation. In ICCV, 2015. 5

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 2, 3, 4

[9] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. arXiv:1603.05027v2, 2016. 4
[10] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In CVPR, 2015.
5, 6, 7, 8

[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 2, 4

[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv:1408.5093,
2014. 5

[13] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In CVPR,
2016. 1, 2, 3, 5, 6, 7, 8

[14] J. Kim, J. K. Lee, and K. M. Lee. Deeply-recursive convolu-
tional network for image super-resolution. In CVPR, 2016.
1, 2, 3, 4, 6, 8

[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. In Proceed-
ings of the IEEE, 1998. 5

[16] M. Liang and X. Hu. Recurrent convolutional neural network

for object recognition. In CVPR, 2015. 4, 8

[17] X.-J. Mao, C. Shen, and Y.-B. Yang. Image restoration us-
ing very deep convolutional encoder-decoder networks with
symmetric skip connections. In NIPS, 2016. 1, 2, 8

[18] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-
cal statistics. In ICCV, 2001. 5

[19] V. Nair and G. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 2, 4

[20] E. Perez-Pellitero,

J. Ruiz-Hidalgo, and
B. Rosenhahn. PSyCo: Manifold span reduction for super
resolution. In CVPR, 2016. 1, 6, 7, 8

J. Salvador,

[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
and et al. ImageNet large scale visual recognition challenge.
arXiv:1409.0575, 2014. 2

[22] J. Salvador and E. Perez-Pellitero. Naive bayes super-

resolution forest. In ICCV, 2015. 1, 8

[23] S. Schulter, C. Leistner, and H. Bischof. Fast and accu-
rate image upscaling with super-resolution forests. In CVPR,
2015. 1, 5, 6, 7, 8

[24] H. Sheikh, A. Bovik, and G. de Veciana. An information
ﬁdelity criterion for image quality assessment using natu-
ral scene statistics. IEEE Transactions on image processing,
14(12):2117–2128, 2005. 6

[25] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In CVPR, 2016. 1, 2, 8
[26] W. Shi, J. Caballero, C. Ledig, X. Zhuang, W. Bai, K. Bha-
tia, A. Marvao, T. Dawes, D. ORegan, and D. Rueckert. Car-
diac image super-resolution with global correspondence us-
ing multi-atlas patchmatch. In MICCAI, 2013. 1

[27] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
2

[28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, and S. Reed. Going

deeper with convolutions. In CVPR, 2015. 2

[29] M. W. Thornton, P. M. Atkinson, and D. a. Holland. Sub-
pixel mapping of rural land cover objects from ﬁne spa-
tial resolution satellite sensor imagery using super-resolution
International Journal of Remote Sensing,
pixel-swapping.
27(3):473–491, 2006. 1

[30] R. Timofte, R. Rothe, and L. V. Gool. Seven ways to improve
In CVPR,

example-based single image super resolution.
2016. 5, 8

[31] R. Timofte, V. D. Smet, and L. V. Gool. A+: Adjusted an-
chored neighborhood regression for fast super-resolution. In
ACCV, 2014. 1, 8

[32] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep
networks for image super-resolution with sparse prior.
In
ICCV, 2015. 1, 2, 8

[33] Z. Wang, Y. Yang, Z. Wang, S. Chang, W. Han, J. Yang,
and T. Huang. Self-tuned deep super resolution. In CVPR
workshop, 2015. 8

[34] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-

resolution: A benchmark. In ECCV, 2014. 6

[35] J. Yang, J. Wright, T. Huang, and Y. Ma. Image superresolu-
tion via sparse representation. IEEE Transactions on image
processing, 19(11):2861–2873, 2010. 1, 5

[36] R. Zeyde, M. Elad, and M. Protter. On single image scale-
up using sparse-representations. Curves and Surfaces, pages
711–730, 2012. 5

[37] W. Zou and P. C. Yuen. Very low resolution face recog-
IEEE Transactions on image processing,

nition problem.
21(1):327–340, 2012. 1

3155

Image Super-Resolution via Deep Recursive Residual Network

Ying Tai∗ 1, Jian Yang1, and Xiaoming Liu2

1Department of Computer Science and Engineering, Nanjing University of Science and Technology
2Department of Computer Science and Engineering, Michigan State University
{taiying, csjyang}@njust.edu.cn, liuxm@cse.msu.edu

Abstract

Recently, Convolutional Neural Network (CNN) based
models have achieved great success in Single Image Super-
Resolution (SISR). Owing to the strength of deep networks,
these CNN models learn an effective nonlinear mapping
from the low-resolution input image to the high-resolution
target image, at the cost of requiring enormous parameters.
This paper proposes a very deep CNN model (up to 52 con-
volutional layers) named Deep Recursive Residual Network
(DRRN) that strives for deep yet concise networks. Speciﬁ-
cally, residual learning is adopted, both in global and local
manners, to mitigate the difﬁculty of training very deep net-
works; recursive learning is used to control the model pa-
rameters while increasing the depth. Extensive benchmark
evaluation shows that DRRN signiﬁcantly outperforms state
of the art in SISR, while utilizing far fewer parameters.
Code is available at https://github.com/tyshiwo
/DRRN CVPR17.

1. Introduction

Single Image Super-Resolution (SISR) is a classic com-
puter vision problem, which aims to recover a high-
resolution (HR) image from a low-resolution (LR) image.
Since SISR restores the high-frequency information, it is
widely used in applications such as medical imaging [26],
satellite imaging [29], security and surveillance [37], where
high-frequency details are greatly desired.

In recent years, due to the powerful learning ability,
Deep Learning (DL) models, especially Convolutional Neu-
ral Networks (CNN), are widely used to address the ill-

∗This work was conducted when the ﬁrst author was a visiting scholar
at Michigan State University.
It was supported by the National Sci-
ence Fund of China under Grant Nos. 91420201, 61472187, 61502235,
61233011, 61373063 and 61602244, the 973 Program No.2014CB349303,
Program for Changjiang Scholars and Innovative Research Team in Uni-
versity, and partially sponsored by CCF-Tencent Open Research Fund.

DRCN

RED30

DRRN_B1U25

DRRN_B1U9

VDSR

CSCN

ESPCN

SRCNN

Figure 1. PSNR of recent CNN models for scale factor ×3 on
Set5 [1]. Red points are our models. △, ✩, and ◦ are models with
less than 5 layers, 20 layers, and more than 30 layers, respectively.
DRRN B1U9 means there is 1 recursive block, in which 9 residual
units are stacked. With the same depth but far fewer parameters,
DRRN B1U9 achieves better performance than the state-of-the-
art methods [13,14]. After increasing the depth without adding any
parameters, the 52-layer DRRN B1U25 further improves the per-
formance and signiﬁcantly outperforms VDSR [13], DRCN [14]
and RED30 [17] by 0.37, 0.21 and 0.21 dB respectively.

posed inverse problem of Super Resolution (SR), and have
demonstrated superiority over reconstruction-based meth-
ods [4, 35] or other learning paradigms [20, 22, 23, 31]. As
the pioneer CNN model for SR, Super-Resolution Convo-
lutional Neural Network (SRCNN) [2] predicts the nonlin-
ear LR-HR mapping via a fully convolutional network, and
signiﬁcantly outperforms classical non-DL methods. How-
ever, SRCNN does not consider any self similarity prop-
erty. To address this issue, the Deep Joint Super Resolution
(DJSR) jointly utilizes both the wealth of external examples
and the power of self examples unique to the input. Inspired
by the learning iterative shrinkage and thresholding algo-
rithm [5], Cascaded Sparse Coding Network (CSCN) [32]
is trained end-to-end to fully exploit the natural sparsity of
images. Shi et al. [25] observe that the prior models [2, 32]
increase LR image’s resolution via bicubic interpolation be-

13147

fore CNN learning, which increases the computational cost.
The Efﬁcient Sub-Pixel Convolutional neural Network (ES-
PCN) reduces the computational and memory complexity,
by increasing the resolution from LR to HR only at the end
of the network.

1, instead of 10−

One commonality among the above CNN models is that
their networks contain fewer than 5 layers, e.g., SRCNN [2]
uses 3 convolutional layers. Their deeper structures with 4
or 5 layers do not achieve better performance, which was
attributed to the difﬁculty of training deeper networks and
led to the observation that “the deeper the better” might not
be the case in SR. Inspired by the success of very deep net-
works [8, 27, 28] on ImageNet [21], Kim et al. [13, 14] pro-
pose two very deep convolutional networks for SR, both
stacking 20 convolutional layers, from the viewpoints of
training efﬁciency and storage, respectively. On the one
hand, to accelerate the convergence speed of very deep net-
works, the VDSR [13] is trained with a very high learning
4 in SRCNN) and the authors fur-
rate (10−
ther use residual learning and adjustable gradient clipping
to solve gradient explosion problem. On the other hand, to
control the model parameters, the Deeply-Recursive Con-
volutional Network (DRCN) [14] introduces a very deep re-
cursive layer via a chain structure with up to 16 recursions.
To mitigate the difﬁculty of training DRCN, the authors use
recursive-supervision and skip-connection, and adopt an en-
semble strategy to further improve the performance. Very
recently, Mao et al. [17] propose a 30-layer convolutional
auto-encoder network named RED30 for image restoration,
which uses symmetric skip connections to help training. All
of the three models learn the residual image between the in-
put Interpolated LR (ILR) image and the ground truth HR
image in the residual branch. The residual image is then
added to the ILR image from the identity branch to esti-
mate the HR image. The three models outperform the pre-
vious DL and non-DL methods by a large margin, which
demonstrates “the deeper the better” is still true in SR.

Despite achieving excellent performance, the very deep
networks require enormous parameters. Compared to the
compact models, large models demand more storage space
and are less applicable to mobile systems [6]. To address
this issue, we propose a novel Deep Recursive Residual
Network (DRRN) to effectively build a very deep network
structure, which achieves better performance, but with 2×,
6×, and 14× fewer parameters than VDSR, DRCN, and
RED30, respectively. In a nutshell, DRRN advances the SR
performance with a deeper yet concise network. Speciﬁ-
cally, DRRN has two major algorithmic novelties:
(1) Both global and local residual learning are introduced
in DRRN. In VDSR and DRCN, the residual image is es-
timated from the input and output of the networks, termed
as Global Residual Learning (GRL). Since the SR output is
vastly similar to the input, GRL is effective in easing the dif-

ﬁculty of training deep networks. Therefore, we also adopt
GRL in our identity branch. Further, very deep networks
could suffer from the performance degradation problem,
as observed in visual recognition [8] and image restora-
tion [17]. The reason may be a signiﬁcant amount of image
details are lost after so many layers. To address this issue,
we introduce an enhanced residual unit structure, termed
as multi-path mode Local Residual Learning (LRL), where
the identity branch not only carries rich image details to late
layers, but also helps gradient ﬂow. GRL and LRL mainly
differ in that LRL is performed in every few stacked lay-
ers, while GRL is performed between the input and output
images, i.e., DRRN has many LRLs and only 1 GRL.
(2) Recursive learning of residual units is proposed in
In DRCN [14], a
DRRN to keep our model compact.
deep recursive layer (up to 16 convolutional recursions)
is learned and the weights are shared in the 16 convolu-
tional recursions. Our DRRN has two major differences
compared to DRCN: (a) Unlike DRCN that shares weights
among convolutional layers, DRRN has a recursive block
consisting of several residual units, and the weight set is
shared among these residual units. (b) To address the van-
ishing/exploding gradients problem of very deep models,
DRCN supervises every recursion so that the supervision
on early recursions help backpropagation. DRRN is re-
lieved from this burden by designing a recursive block with
a multi-path structure. Our model can be easily trained even
with 52 convolutional layers. Last but not least, through re-
cursive learning, DRRN can improve accuracy by increas-
ing depth without adding any weight parameters.

To illustrate the effectiveness of the two strategies used
in DRRN, Fig. 1 shows the Peak Signal-to-Noise Ratio
(PSNR) performance of several recent CNN models for
SR [2, 13, 14, 17, 25, 32] versus the number of parameters,
denoted as k. Compared to the prior CNN models, DRRN
achieves the best performance with fewer parameters.

2. Related Work

Since Sec. 1 overviews DL-based SISR, this section fo-
cuses on three most related work to ours: ResNet [8],
VDSR [13] and DRCN [14]. Fig. 2 illustrates these models
via simpliﬁed network structures with only 6 convolutional
layers, where the activation functions, batch normalization
(BN) [11] and ReLU [19], are omitted for clarity.

2.1. ResNet

The main idea of ResNet [8] is to use a residual learn-
ing framework to ease the training of very deep networks.
Instead of hoping every few stacked layers directly ﬁt the
desired underlying mapping, the authors explicitly let these
layers ﬁt a residual mapping, which is assumed to be easier
for optimization. Denoting the input as x and the underly-
ing mapping as H(x), the residual mapping is deﬁned as

3148

Input

conv

Å

conv

conv

conv

conv

Å

conv

Output

Input

Input

Input

conv

conv

conv

conv

conv

conv

Å

Output

conv

conv

conv

conv

conv

conv

Output 1

Output 4

w1

w4

...

Å

Output

(c) DRCN

conv

conv

conv

Å

conv

conv

Å

conv

Å

Output

(a) ResNet

(b) VDSR

(d) DRRN (ours)

Figure 2. Simpliﬁed structures of (a) ResNet [8]. The green dashed box means a residual unit. (b) VDSR [13]. The purple line refers to a
global identity mapping. (c) DRCN [14]. The blue dashed box refers to a recursive layer, among which the convolutional layers (with light
green color) share the same weights. (d) DRRN. The red dashed box refers to a recursive block consisting of two residual units. In the
recursive block, the corresponding convolutional layers in the residual units (with light green or light red color) share the same weights. In
all four cases, the outputs with light blue color are supervised, and L is the element-wise addition.

Method
ResNet
VDSR
DRCN

DRRN

Key Strategies
Chain mode local residual learning
Global residual learning
Global residual learning + recursive learning (single weight layer) + multi-CNN ensemble
Multi-path mode local residual learning
+ global residual learning + recursive learning (multiple weight layers in the residual unit)

Mathematical Formulation
y = fRec(UU(UU−1(...(U1(f1(x)))...)))
y = fRec(fd−1(fd−2(...(f1(x))...))) + x
y = ΣT
2 (f1(x))) + x)
y = fRec(RB(RB−1(...(R1(x))...))) + x

t=1wt · (fRec(f (t)

Table 1. Strategies used in ResNet [8], VDSR [13], DRCN [14] and DRRN. U, d, T, and B are the numbers of residual units in ResNet,
convolutional layers in VDSR, recursions in DRCN, and recursive blocks in DRRN, respectively. x and y are input and output of networks.
f denotes function of convolutional layer. U denotes function of residual unit structure and R denotes function of our recursive block.

F(x) := H(x) − x, and a residual unit structure is thus:

ˆx = U (x) = σ(F(x, W ) + h(x)),

(1)

where ˆx is the output of the residual unit, h(x) is an iden-
tity mapping [8] : h(x) = x, W is a set of weights (the bi-
ases are omitted to simplify notations), function σ denotes
ReLU, F(x, W ) is the residual mapping to be learned, and
U denotes the function of the residual unit structure. For
a basic residual unit that stacks two convolutional layers,
F(x, W ) = W2σ(W1x). By stacking such structures to
construct a very deep 152-layer network, ResNet won the
ﬁrst place in the ILSVRC 2015 classiﬁcation competition.
Since the residual learning in ResNet is adopted in every
few stacked layers, this strategy is a form of local residual
learning, where residual units are stacked in the chain mode.

2.2. VDSR

Differing from ResNet that uses residual learning in ev-
ery few stacked layers, VDSR [13] introduces GRL, i.e.,
residual learning between the input ILR image and the out-
put HR image. There are three notes for VDSR: (1) Un-

like SRCNN [2] that only uses 3 layers, VDSR stacks 20
weight layers (3 × 3 for each layer) in the residual branch,
which leads to a much larger receptive ﬁeld (41 × 41 vs.
13 × 13). (2) GRL and adjustable gradient clipping enable
VDSR to converge very fast (∼4 hours on GPU Titan Z).
(3) By adopting scale augmentation, a single network of
VDSR is robust to images with different scales. Later, we
will show that VDSR actually is a special case of DRRN,
when there’s no residual unit in our recursive block.

2.3. DRCN

DRCN [14] is motivated by the observation that adding
more weight layers introduces more parameters, where the
model is likely to overﬁt and also becomes disk hungry. To
address these issues, the authors introduce a recursive layer
into the network, so that the model parameters do not in-
crease while more recursions are performed in the recursive
layer. DRCN consists of three parts: embedding net, infer-
ence net and reconstruction net, which are illustrated as the
ﬁrst, middle 4, and last convolutional layer(s) in Fig. 2(c),
respectively. The embedding net f1(x) represents a given

3149

H0

Hu-1

conv

conv

Å

Hu

BN

ReLU

weight

Figure 3. A close look at the u-th residual unit in DRRN. The black
dashed box represents the residual function F , which consists of
two “conv” layers and each stacked by BN-ReLU-weight layers.

image x as feature maps H0. The inference net f2(H0)
stacks T recursions (T = 16 in [14]) in a recursive layer,
with shared weights among these recursions. Finally, the
reconstruction net fRec(HT), where HT is the output of the
inference net, generates the intermediate HR image. How-
ever, since training such a deep network is difﬁcult, the au-
thors further propose two mitigations, recursive-supervision
and skip-connection. Speciﬁcally, for the t-th intermediate
recursion in the recursive layer, the output after reconstruc-
tion net is formulated as

yt = fRec(f (t)

2 (f1(x))) + x,

(2)

where x is skip-connection and basically a GRL. Each in-
termediate prediction yt is learned with supervision. Fi-
nally, an ensemble strategy is adopted and the output is the
weighted average of all predictions y = ΣT
t=1wt · yt, with
weights wt learned during training.

3. Deep Recursive Residual Network

In this section, we present the technical parts of our pro-
posed DRRN. Speciﬁcally, we adopt global residual learn-
ing in the identity branch and introduce recursive learn-
ing into the residual branch by constructing the recursive
block structure, in which several residual units are stacked.
Noted that in ResNet [8], different residual units use dif-
ferent inputs for the identity branch (green dashed boxes in
Fig. 2(a)). However, in our recursive block, a multi-path
structure is used and all the residual units share the same in-
put for the identity branch (green dashed boxes in Fig. 2(d)),
which further facilitates the learning [16]. We highlight the
differences of the network structures between DRRN and
the related models in Tab. 1. Now, we will gradually present
more details of our model, from the residual unit to the re-
cursive block and ﬁnally the whole network structure.

3.1. Residual Unit

In ResNet [8], the basic residual unit is formulated as
Eq. 1 and the activation functions (BN [11] and ReLU [19])
are performed after the weight layers. In contrast to such
a “post-activation” structure, He et al. [9] propose a “pre-
activation” structure, which performs the activation before
the weight layers. They claim that the pre-activation version

xb-1

xb-1

conv

conv

conv

Å

conv

conv

Å

xb

conv

conv

conv

Å

xb

xb-1

conv

conv

conv

Å

conv

conv

Å

conv

conv

Å

xb

(a) U=1

(b) U=2

(c) U=3

Figure 4. Structures of our recursive blocks. U means number of
residual units in the recursive block.

is much easier to train and generates better performance
than the post-activation version. Speciﬁcally, the residual
unit with pre-activation structure is formulated as

H u = F(H u

1, W u) + H u

1,

−

−

(3)

−

where u = 1, 2, ..., U, U is the number of residual units in a
recursive block, H u
1 and H u are the input and output of
the u-th residual unit, and F denotes the residual function.
Instead of directly using the above residual unit, we mod-
ify Eq. 3 so that the inputs to the identity branch and the
residual branch are different. As described in the beginning
of Sec. 3, the inputs to all of the identity branches of the
residual units in one recursive block are kept the same, i.e.,
H 0 in Fig. 3. As a result, there are multiple paths between
the input and output of our recursive block, as shown in
Fig. 4. The residual paths help to learn highly complex fea-
tures and the identity paths help gradient backpropagation
during training. Compared to the chain mode, this multi-
path mode facilitates the learning and is less prone to over-
ﬁtting [16]. Therefore, we formulate our residual unit as

H u = G(H u

1) = F(H u

1, W ) + H 0,

−

−

(4)

where G denotes the function of our residual unit, H 0 is the
result of the ﬁrst convolutional layer in the recursive block.
Since the residual unit is recursively learned, the weight set
W is shared among the residual units within a recursive
block, but different across different recursive blocks.

3.2. Recursive Block

We now introduce the details of our recursive block.
First, we illustrate the structure of our recursive blocks in
Fig. 4. Motivated by [16], we introduce a convolutional
layer at the beginning of the recursive block, and then sev-
eral residual units mentioned in Sec. 3.1 are stacked. We

3150

x

RB

RB

RB

RB

RB

RB

conv

Å

y

x3

conv

conv

conv

Å

conv

conv

Å

conv

conv

Å

x4

Figure 5. An example network structure of DRRN with B = 6 and
U = 3. Here, “RB” layer refers to a recursive block.

1 and xb
denote B as the number of recursive blocks, xb
(b = 1, 2, ..., B) as the input and output of the b-th recursive
block, and H 0
1
−
through the ﬁrst convolutional layer, whose function is fb.
According to Eq. 4, the result of u-th residual unit is

1) as the result after passing xb

b = fb(xb

−

−

H u

b = G(H u

−

1) = F(H u
b

1

, Wb) + H 0
b .

−

(5)

Thus, the output of the b-th recursive block xb is
xb = H U

1)) = G(G(...(G(fb(xb

b = G(U)(fb(xb

−

where U-fold operations of Gb are performed.

1)))...)),
(6)

−

3.3. Network Structure

Finally, we simply stack several recursive blocks, fol-
lowed by a convolutional layer reconstructing the residual
between the LR and HR images. The residual image is then
added to the global identity mapping from the input LR im-
age. The entire network structure of DRRN is illustrated in
Fig. 5. Actually, VDSR [13] can be viewed as a special case
of DRRN, i.e., when U = 0, DRRN becomes VDSR.

DRRN has two key parameters:

the recursive block
number B and the residual unit number U in each recur-
sive block. Given different B and U, we can learn DRRN
with different depths – the number of convolutional layers.
Speciﬁcally, the depth of DRRN d is calculated as:

d = (1 + 2 × U) × B + 1.

Denoting x and y to be the input and output of DRRN,

R to be the function of the b-th recursive block, we have

xb = Rb(xb

1) = G(U)(fb(xb

1)).

−

−

(7)

(8)

where fRec is a function for the last convolutional layer in
DRRN to reconstruct the residual. Tab. 1 lists the mathe-
matical formulations of ResNet, VDSR, DRCN and DRRN.
i=1, where N is the num-
ber of training patches and ˜x(i) is the ground truth HR patch
of the LR patch x(i), the loss function of DRRN is

Given a training set {x(i), ˜x(i)}N

L(Θ) =

k˜x(i) − D(x(i))k2,

(10)

1
2N

N

X
i=1

where Θ denotes the parameter set. The objective function
is optimized via the mini-batch stochastic gradient descent
(SGD) with backpropagation [15]. We implement DRRN
via Caffe [12].

4. Experiments
4.1. Datasets

By following [13, 23], we use a training dataset of 291
images, where 91 images are from Yang et al. [35] and other
200 images are from Berkeley Segmentation Dataset [18].
For testing, we utilize four widely used benchmark datasets,
Set5 [1], Set14 [36], BSD100 [18] and Urban100 [10],
which have 5, 14, 100 and 100 images respectively.

4.2. Implementation Details

Data augmentation is performed on the 291-image train-
ing dataset. Inspired by [30], the ﬂipped and rotated ver-
sions of the training images are considered. Speciﬁcally,
we rotate the original images by 90◦, 180◦, 270◦ and ﬂip
them horizontally. After that, for each original image, we
have 7 additional augmented versions. Besides, inspired by
VDSR [13], we also use scale augmentation to train our
model, and images with different scales (×2, ×3 and ×4)
are all included in the training set. Therefore, for all differ-
ent scales, we only need to train a single model.

Training images are split into 31 × 31 patches, with the
stride of 21, by considering both the training time and stor-
age complexities. We set the mini-batch size of SGD to 128,
4.
momentum parameter to 0.9, and weight decay to 10−
Every weight layer has 128 ﬁlters of the size 3 × 3.

For weight initialization, we use the same method as He
et al. [7], which is shown to be suitable for networks utiliz-
ing ReLU. The initial learning rate is set to 0.1 and then de-
creased to half every 10 epochs. Since a large learning rate
is used in our work, we adopt the adjustable gradient clip-
ping [13] to boost the convergence rate while suppressing
exploding gradients. Speciﬁcally, the gradients are clipped
to [− θ
γ ], where γ is the current learning rate and θ = 0.01
is the gradient clipping parameter. Training a DRRN of
d = 20 roughly takes 4 days with 2 Titan X GPUs.

γ , θ

When b = 1, we deﬁne x0 = x. Then, DRRN can be
formulated as

4.3. Study of B and U

y = D(x) = fRec(RB(RB

1(...(R1(x))...))) + x,

(9)

−

In this subsection, we explore various combinations of B
and U to construct different DRRN structures with different

3151

Set5

Set14

Dataset

Scale Bicubic
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

RFL [23]
36.54/0.9537
32.43/0.9057
30.14/0.8548
32.26/0.9040
29.05/0.8164
27.24/0.7451
31.16/0.8840
28.22/0.7806
26.75/0.7054
29.11/0.8904
25.86/0.7900
24.19/0.7096
Table 2. Benchmark results. Average PSNR/SSIMs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, BSD100 and Urban100. Red
color indicates the best performance of our methods and blue color indicates the best performance of previous methods.

DRRN B1U9 DRRN B1U25
37.66/0.9589
33.93/0.9234
31.58/0.8864
33.19/0.9133
29.94/0.8339
28.18/0.7701
32.01/0.8969
28.91/0.7992
27.35/0.7262
31.02/0.9164
27.38/0.8331
25.35/0.7576

VDSR [13]
37.53/0.9587
33.66/0.9213
31.35/0.8838
33.03/0.9124
29.77/0.8314
28.01/0.7674
31.90/0.8960
28.82/0.7976
27.29/0.7251
30.76/0.9140
27.14/0.8279
25.18/0.7524

DRCN [14]
37.63/0.9588
33.82/0.9226
31.53/0.8854
33.04/0.9118
29.76/0.8311
28.02/0.7670
31.85/0.8942
28.80/0.7963
27.23/0.7233
30.75/0.9133
27.15/0.8276
25.14/0.7510

SelfEx [10]
36.49/0.9537
32.58/0.9093
30.31/0.8619
32.22/0.9034
29.16/0.8196
27.40/0.7518
31.18/0.8855
28.29/0.7840
26.84/0.7106
29.54/0.8967
26.44/0.8088
24.79/0.7374

SRCNN [2]
36.66/0.9542
32.75/0.9090
30.48/0.8628
32.45/0.9067
29.30/0.8215
27.50/0.7513
31.36/0.8879
28.41/0.7863
26.90/0.7101
29.50/0.8946
26.24/0.7989
24.52/0.7221

37.74/0.9591
34.03/0.9244
31.68/0.8888
33.23/0.9136
29.96/0.8349
28.21/0.7720
32.05/0.8973
28.95/0.8004
27.38/0.7284
31.23/0.9188
27.53/0.8378
25.44/0.7638

33.66/0.9299
30.39/0.8682
28.42/0.8104
30.24/0.8688
27.55/0.7742
26.00/0.7027
29.56/0.8431
27.21/0.7385
25.96/0.6675
26.88/0.8403
24.46/0.7349
23.14/0.6577

Urban100

BSD100

more

params

less

Figure 6. PSNR of various DRRNs at B and U combinations. The
color of the point indicates the PSNR that corresponds to the bar
on the right and 4 depth contours (d = 50, 30, 20, 10) are also
plotted. The tests are conducted for scale factor ×3 on Set5.

depths, and see how the two parameters affect the perfor-
mance. In Fig. 6, we build a grid of B and U, and sample
several points in the grid with the depth ranging from 8 to 52
layers. The parameter number keeps the same when more
residual units are used in one recursive block, and linearly
increases when more recursive blocks are stacked.

First, to clearly show how a single parameter affects
DRRN, we ﬁx one parameter to 3 and change the other
from 1 to 4. Fig. 6 shows that increasing B or U results
in deeper models and achieves better performance, which
indicates deeper is still better. Despite different structures,
these models are comparable as long as their depths are sim-
ilar, e.g., B2U3 (d = 15, k = 784K) and B3U2 (d = 16,
k = 1, 182K) achieve 33.76 and 33.77 dB, respectively.

The structures mentioned above all use the recursive
learning strategy. Next, we test three very different struc-
tures to demonstrate the effectiveness of such a strategy.
Speciﬁcally, we ﬁx one parameter to 1 and change the other
to construct networks with d = 52. This results in two ex-
treme structures: B1U25 (k = 297K) and B17U1 (k =
7, 375K). For B1U25, only one recursive block is used, in
which 25 residual units are recursively learned. For B17U1,
17 recursive blocks are stacked, with no recursive learn-

ing. We also construct a normal structure B3U8 (d = 52,
k = 1, 182K). Fig. 6 shows that despite different struc-
tures, the three networks achieve comparable performance
(B17U1 34.03 dB, B3U8 34.04 dB and B1U25 34.03 dB)
and outperform the pervious shallow networks. Thanks to
the recursive learning strategy, B1U25 can achieve state-of-
the-art results using far fewer parameters.

4.4. Comparison with State of the Art Models

We now provide quantitative and qualitative compar-
isons. Considering both the performance and number of
parameters, we choose DRRN B1U25 (d = 52, k = 297K)
as our best model. For fair comparison, we also construct
a DRRN B1U9 (d = 20, k = 297K) structure, which has
the same depth as VDSR and DRCN, but fewer parameters.
Both the DL [2, 13, 14] and non-DL [10, 20, 23] methods in
recent years are used for benchmark. Experimental setting
is kept the same as those previous methods. Speciﬁcally, we
ﬁrst apply bicubic interpolation to the color components of
an image and all models are applied to its luminance com-
ponent only. Therefore, the input and output images are of
the same size. For fair comparison, similar to [2, 13, 14, 23],
we crop pixels near image boundary before evaluation, al-
though this is unnecessary for DRRN.

Tab. 2 summarizes quantitative results on the four test-
ing sets, by citing the results of prior methods from [13,14].
The two DRRN models outperforms all existing methods in
all datasets and scale factors, in both PSNR and Structural
SIMilarity (SSIM) 1. Especially on the recent difﬁcult Ur-
ban100 dataset [10], DRRN signiﬁcantly advances the state
of the art, with the improvement margin of 0.47, 0.38, and
0.26 dB on scale factor ×2, ×3 and ×4 respectively.

Further, we also use another metric: Information Fidelity
Criterion (IFC) [24] for comparison, which claims to have
the highest correlation with perceptual scores for SR evalu-
ation [34]. The results are presented in Tab. 3. Note that

1With two convolutional layers in the residual branch, DRRN achieves
state-of-the-art performance. More complex designs have the potential to
improve performance but are not the focus of this work.

3152

Dataset

Set5

Set14

Urban100

Scale
×2
×3
×4
×2
×3
×4
×2
×3
×4

Bicubic
6.083
3.580
2.329
6.105
3.473
2.237
6.245
3.620
2.361

SRCNN [2]
8.036
4.658
2.991
7.784
4.338
2.751
7.989
4.584
2.963

SelfEx [10]
7.811
4.748
3.166
7.591
4.371
2.893
7.937
4.843
3.314

RFL [23]
8.556
4.926
3.191
8.175
4.531
2.919
8.450
4.801
3.110

PSyCo [20]
8.642
5.083
3.379
8.280
4.660
3.055
8.589
5.031
3.351

VDSR [13]
8.569
5.221
3.547
8.178
4.730
3.133
8.645
5.194
3.496

DRRN B1U9
8.583
5.241
3.581
8.181
4.732
3.147
8.653
5.259
3.536

DRRN B1U25
8.671
5.397
3.703
8.320
4.878
3.252
8.917
5.456
3.676

Table 3. Benchmark results. Average IFCs for the scale factor ×2, ×3 and ×4 on datasets Set5, Set14 and Urban100. Red color indicates
the best performance of our methods and blue color indicates the best performance of previous methods.

Ground Truth

Bicubic

SRCNN

SelfEx

VDSR

DRRN_B1U9

DRRN_B1U25

(PSNR/SSIM/IFC)

(21.10/0.7046/3.134) (21.77/0.7540/3.761) (21.94/0.7608/3.669) (22.58/0.7942/4.341) (22.74/0.7999/4.365) (23.37/0.8158/4.713)

(PSNR/SSIM/IFC)

(22.55/0.7073/3.591) (23.51/0.7608/4.344) (23.42/0.7587/4.281) (23.99/0.7728/4.716) (24.23/0.7781/4.734) (24.41/0.7805/4.914)

(PSNR/SSIM/IFC)

(21.98/0.8126/1.920) (24.80/0.8928/2.666) (24.85/0.9076/2.941) (25.85/0.9289/3.406) (26.33/0.9365/3.557) (26.48/0.9415/3.822)

Figure 7. Qualitative comparison. (1) The ﬁrst row shows image “img059” (Urban100 with scale factor ×3). DRRN recovers sharp lines,
while others all give blurry results. (2) The second row shows image “253027” (BSD100 with scale factor ×3). DRRN accurately recovers
the pattern. (3) The last row shows image “ppt3” (Set14 with scale factor ×4). Texts in DRRN are sharp, while others are blurry.

the results of [2, 10, 20, 23] are cited from [20] 2, while
the results of VDSR come from our re-implementation.
Similar to DRRN, the VDSR re-implementation also uses
BN and ReLU as the activation functions, unlike the orig-
inal VDSR [13] that does not use BN. These results are

2Since PSyCo [20] does not present complete PSNR/SSIM perfor-

mance on the four benchmarks, we do not include it in Tab. 2.

faithful since our VDSR re-implementation achieves similar
benchmark performance as [13] reported in Tab. 2. Since
only Set5, Set14 and Urban100 are used in [20], we omit
BSD100 in this test. It is clear that DRRN still outperforms
all existing methods in all datasets and scale factors. Re-
garding the speed, our 20-layer B1U9 network takes 0.25
second to process a 288 × 288 image on a Titan X GPU.

3153

16 layers

22 layers

28 layers

Shallow

Deep

Very deep

Figure 8. PSNR for scale factor ×3 on Set5 using VDSR (blue),
DRRN NS (green), DRRN C (cyan) and DRRN (red).

Methods
Loc. Res. L
Recu. L
Multi-path
PSNR

×
×
×
33.86

VDSR

DRRN NS C

DRRN NS

DRRN C

DRRN

√

×
×
33.92

√

×
√
33.97

√
√

×
33.95

√
√
√
33.99

Table 4. Average PSNR when different DRRN components are
turned on or off, for scale factor ×3 on dataset Set5.

among

Qualitative

comparisons

SRCNN [2],
SelfEx [10], VDSR [13] and DRRN are illustrated in
Fig. 7. For SRCNN and SelfEx, we use their public codes.
For VDSR, we use our re-implementation. As we can see,
our method produces relatively sharper edges with respect
to patterns, while other methods may give blurry results.

4.5. Discussions

Since global residual learning has been well discussed
in [13], in this section, we mainly focus on local residual
learning (LRL), recursive learning and multi-path structure.
Local Residual Learning To demonstrate the effective-
ness of LRL, DRRN is compared with VDSR [13], which
has no LRL. For fair comparison, the depth and number of
parameters are kept the same for both methods. Speciﬁ-
cally, we evaluate three depths: 16(B3U2), 22(B3U3), and
28(B3U4) convolutional layers. Each convolutional layer
has 128 ﬁlters with the size 3 × 3. To keep the parameter
number the same, in this test we do not share the weight
set of the residual units in one recursive block, and denote
this DRRN structure as DRRN NS. Fig. 8 shows the PSNR
of both methods in different depths. We see that the LRL
strategy consistently improves VDSR at all depths.
Recursive Learning To contrast our recursive learning
strategy, the three DRRN NS versions are compared with
the three weight-shared versions (Fig. 8). Storage is an im-
portant factor to consider when building a deep model. The
recursive learning strategy can reduce the storage demand
and keep a concise model while increasing its depth. Inter-
estingly the weight-shared DRRN versions achieve compa-
rable or even better performance than DRRN NS versions

Figure 9. Comparing deep and shallow models proposed in recent
three years that report PSNR for scale factor ×3 on Set5 and Set14.

while using only a small fraction of parameters, which in-
dicates when limited training set (e.g., 291 images) is used,
the recursive learning is indeed effective under the same
structure, and less prone to overﬁtting [16].
Multi-Path Structure To demonstrate the effectiveness
of multi-path structure, we compare DRRN with the chain
structure, denoted as DRRN C. As shown in Fig. 8, with the
same depth and parameter number, the multi-path structures
achieve higher PSNR than the corresponding chain struc-
tures in all three cases. Further, Tab. 4 presents the compre-
hensive study on performance gains using network B3U4 as
the example. It shows how different technical parts improve
the performance compared to the baseline VDSR.
Deep vs. Shallow Finally, we give a comparison of the deep
and shallow SISR models, which are published in recent
three years (2014 to 2016) that report PSNR for scale factor
×3 on datasets Set5 and Set14. Shallow (non-DL) mod-
els include A+ [31], SelfEx [10], RFL [23], NBSRF [22],
PSyCo [20] and IA [30]. The deep models (d ≤ 8) in-
clude SRCNN [2], DJSR [33], CSCN [32], ESPCN [25]
and FSRCNN [3]. Very deep models (d ≥ 20) include
VDSR [13], DRCN [14], RED [17] and DRRN with d = 20
and 52. Fig. 9 shows that 1) very deep models signif-
icantly outperform the shallow models; 2) DRRN B1U9
(d = 20, k = 297K) already outperforms the state of the
arts with the same depth but fewer parameters; 3) a deeper
DRRN B1U25 (d = 52, k = 297K) further improves the
performance without adding any parameters.

5. Conclusions

In this paper, we propose Deep Recursive Residual Net-
work (DRRN) for single image super-resolution. In DRRN,
an enhanced residual unit structure is recursively learned in
a recursive block, and we stack several recursive blocks to
learn the residual image between the HR and LR images.
The residual image is then added to the input LR image
from a global identity branch to estimate the HR image.
Extensive benchmark experiments and analysis show that
DRRN is a deep, concise, and superior model for SISR.

3154

References

[1] C. M. Bevilacqua, A. Roumy, and M.-L. A. Morel. Low-
complexity single-image super-resolution based on nonneg-
ative neighbor embedding. In BMVC, 2012. 1, 5

[2] C. Dong, C. Loy, K. He, and X. Tang. Image super-resolution
IEEE Transactions on
using deep convolutional networks.
Pattern Analysis and Machine Intelligence, 38(2):295–307,
2016. 1, 2, 3, 6, 7, 8

[3] C. Dong, C. Loy, and X. Tang. Accelerating the super-
In ECCV, 2016.

resolution convolutional neural network.
8

[4] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a

single image. In ICCV, 2009. 1

[5] K. Gregor and Y. LeCun. Learning fast approximations of

sparse coding. In ICML, 2010. 1

[6] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In ICLR, 2016. 2

[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rec-
tiﬁers: Surpassing human-level performance on ImageNet
classiﬁcation. In ICCV, 2015. 5

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 2, 3, 4

[9] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. arXiv:1603.05027v2, 2016. 4
[10] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In CVPR, 2015.
5, 6, 7, 8

[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 2, 4

[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv:1408.5093,
2014. 5

[13] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In CVPR,
2016. 1, 2, 3, 5, 6, 7, 8

[14] J. Kim, J. K. Lee, and K. M. Lee. Deeply-recursive convolu-
tional network for image super-resolution. In CVPR, 2016.
1, 2, 3, 4, 6, 8

[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. In Proceed-
ings of the IEEE, 1998. 5

[16] M. Liang and X. Hu. Recurrent convolutional neural network

for object recognition. In CVPR, 2015. 4, 8

[17] X.-J. Mao, C. Shen, and Y.-B. Yang. Image restoration us-
ing very deep convolutional encoder-decoder networks with
symmetric skip connections. In NIPS, 2016. 1, 2, 8

[18] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-
cal statistics. In ICCV, 2001. 5

[19] V. Nair and G. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 2, 4

[20] E. Perez-Pellitero,

J. Ruiz-Hidalgo, and
B. Rosenhahn. PSyCo: Manifold span reduction for super
resolution. In CVPR, 2016. 1, 6, 7, 8

J. Salvador,

[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
and et al. ImageNet large scale visual recognition challenge.
arXiv:1409.0575, 2014. 2

[22] J. Salvador and E. Perez-Pellitero. Naive bayes super-

resolution forest. In ICCV, 2015. 1, 8

[23] S. Schulter, C. Leistner, and H. Bischof. Fast and accu-
rate image upscaling with super-resolution forests. In CVPR,
2015. 1, 5, 6, 7, 8

[24] H. Sheikh, A. Bovik, and G. de Veciana. An information
ﬁdelity criterion for image quality assessment using natu-
ral scene statistics. IEEE Transactions on image processing,
14(12):2117–2128, 2005. 6

[25] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In CVPR, 2016. 1, 2, 8
[26] W. Shi, J. Caballero, C. Ledig, X. Zhuang, W. Bai, K. Bha-
tia, A. Marvao, T. Dawes, D. ORegan, and D. Rueckert. Car-
diac image super-resolution with global correspondence us-
ing multi-atlas patchmatch. In MICCAI, 2013. 1

[27] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
2

[28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, and S. Reed. Going

deeper with convolutions. In CVPR, 2015. 2

[29] M. W. Thornton, P. M. Atkinson, and D. a. Holland. Sub-
pixel mapping of rural land cover objects from ﬁne spa-
tial resolution satellite sensor imagery using super-resolution
International Journal of Remote Sensing,
pixel-swapping.
27(3):473–491, 2006. 1

[30] R. Timofte, R. Rothe, and L. V. Gool. Seven ways to improve
In CVPR,

example-based single image super resolution.
2016. 5, 8

[31] R. Timofte, V. D. Smet, and L. V. Gool. A+: Adjusted an-
chored neighborhood regression for fast super-resolution. In
ACCV, 2014. 1, 8

[32] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep
networks for image super-resolution with sparse prior.
In
ICCV, 2015. 1, 2, 8

[33] Z. Wang, Y. Yang, Z. Wang, S. Chang, W. Han, J. Yang,
and T. Huang. Self-tuned deep super resolution. In CVPR
workshop, 2015. 8

[34] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-

resolution: A benchmark. In ECCV, 2014. 6

[35] J. Yang, J. Wright, T. Huang, and Y. Ma. Image superresolu-
tion via sparse representation. IEEE Transactions on image
processing, 19(11):2861–2873, 2010. 1, 5

[36] R. Zeyde, M. Elad, and M. Protter. On single image scale-
up using sparse-representations. Curves and Surfaces, pages
711–730, 2012. 5

[37] W. Zou and P. C. Yuen. Very low resolution face recog-
IEEE Transactions on image processing,

nition problem.
21(1):327–340, 2012. 1

3155

Image Super-Resolution via Deep Recursive Residual Network

Ying Tai∗ 1, Jian Yang1, and Xiaoming Liu2

1Department of Computer Science and Engineering, Nanjing University of Science and Technology
2Department of Computer Science and Engineering, Michigan State University
{taiying, csjyang}@njust.edu.cn, liuxm@cse.msu.edu

Abstract

Recently, Convolutional Neural Network (CNN) based
models have achieved great success in Single Image Super-
Resolution (SISR). Owing to the strength of deep networks,
these CNN models learn an effective nonlinear mapping
from the low-resolution input image to the high-resolution
target image, at the cost of requiring enormous parameters.
This paper proposes a very deep CNN model (up to 52 con-
volutional layers) named Deep Recursive Residual Network
(DRRN) that strives for deep yet concise networks. Speciﬁ-
cally, residual learning is adopted, both in global and local
manners, to mitigate the difﬁculty of training very deep net-
works; recursive learning is used to control the model pa-
rameters while increasing the depth. Extensive benchmark
evaluation shows that DRRN signiﬁcantly outperforms state
of the art in SISR, while utilizing far fewer parameters.
Code is available at https://github.com/tyshiwo
/DRRN CVPR17.

1. Introduction

Single Image Super-Resolution (SISR) is a classic com-
puter vision problem, which aims to recover a high-
resolution (HR) image from a low-resolution (LR) image.
Since SISR restores the high-frequency information, it is
widely used in applications such as medical imaging [26],
satellite imaging [29], security and surveillance [37], where
high-frequency details are greatly desired.

In recent years, due to the powerful learning ability,
Deep Learning (DL) models, especially Convolutional Neu-
ral Networks (CNN), are widely used to address the ill-

∗This work was conducted when the ﬁrst author was a visiting scholar
at Michigan State University.
It was supported by the National Sci-
ence Fund of China under Grant Nos. 91420201, 61472187, 61502235,
61233011, 61373063 and 61602244, the 973 Program No.2014CB349303,
Program for Changjiang Scholars and Innovative Research Team in Uni-
versity, and partially sponsored by CCF-Tencent Open Research Fund.

DRCN

RED30

DRRN_B1U25

DRRN_B1U9

VDSR

CSCN

ESPCN

SRCNN

Figure 1. PSNR of recent CNN models for scale factor ×3 on
Set5 [1]. Red points are our models. △, ✩, and ◦ are models with
less than 5 layers, 20 layers, and more than 30 layers, respectively.
DRRN B1U9 means there is 1 recursive block, in which 9 residual
units are stacked. With the same depth but far fewer parameters,
DRRN B1U9 achieves better performance than the state-of-the-
art methods [13,14]. After increasing the depth without adding any
parameters, the 52-layer DRRN B1U25 further improves the per-
formance and signiﬁcantly outperforms VDSR [13], DRCN [14]
and RED30 [17] by 0.37, 0.21 and 0.21 dB respectively.

posed inverse problem of Super Resolution (SR), and have
demonstrated superiority over reconstruction-based meth-
ods [4, 35] or other learning paradigms [20, 22, 23, 31]. As
the pioneer CNN model for SR, Super-Resolution Convo-
lutional Neural Network (SRCNN) [2] predicts the nonlin-
ear LR-HR mapping via a fully convolutional network, and
signiﬁcantly outperforms classical non-DL methods. How-
ever, SRCNN does not consider any self similarity prop-
erty. To address this issue, the Deep Joint Super Resolution
(DJSR) jointly utilizes both the wealth of external examples
and the power of self examples unique to the input. Inspired
by the learning iterative shrinkage and thresholding algo-
rithm [5], Cascaded Sparse Coding Network (CSCN) [32]
is trained end-to-end to fully exploit the natural sparsity of
images. Shi et al. [25] observe that the prior models [2, 32]
increase LR image’s resolution via bicubic interpolation be-

13147

fore CNN learning, which increases the computational cost.
The Efﬁcient Sub-Pixel Convolutional neural Network (ES-
PCN) reduces the computational and memory complexity,
by increasing the resolution from LR to HR only at the end
of the network.

1, instead of 10−

One commonality among the above CNN models is that
their networks contain fewer than 5 layers, e.g., SRCNN [2]
uses 3 convolutional layers. Their deeper structures with 4
or 5 layers do not achieve better performance, which was
attributed to the difﬁculty of training deeper networks and
led to the observation that “the deeper the better” might not
be the case in SR. Inspired by the success of very deep net-
works [8, 27, 28] on ImageNet [21], Kim et al. [13, 14] pro-
pose two very deep convolutional networks for SR, both
stacking 20 convolutional layers, from the viewpoints of
training efﬁciency and storage, respectively. On the one
hand, to accelerate the convergence speed of very deep net-
works, the VDSR [13] is trained with a very high learning
4 in SRCNN) and the authors fur-
rate (10−
ther use residual learning and adjustable gradient clipping
to solve gradient explosion problem. On the other hand, to
control the model parameters, the Deeply-Recursive Con-
volutional Network (DRCN) [14] introduces a very deep re-
cursive layer via a chain structure with up to 16 recursions.
To mitigate the difﬁculty of training DRCN, the authors use
recursive-supervision and skip-connection, and adopt an en-
semble strategy to further improve the performance. Very
recently, Mao et al. [17] propose a 30-layer convolutional
auto-encoder network named RED30 for image restoration,
which uses symmetric skip connections to help training. All
of the three models learn the residual image between the in-
put Interpolated LR (ILR) image and the ground truth HR
image in the residual branch. The residual image is then
added to the ILR image from the identity branch to esti-
mate the HR image. The three models outperform the pre-
vious DL and non-DL methods by a large margin, which
demonstrates “the deeper the better” is still true in SR.

Despite achieving excellent performance, the very deep
networks require enormous parameters. Compared to the
compact models, large models demand more storage space
and are less applicable to mobile systems [6]. To address
this issue, we propose a novel Deep Recursive Residual
Network (DRRN) to effectively build a very deep network
structure, which achieves better performance, but with 2×,
6×, and 14× fewer parameters than VDSR, DRCN, and
RED30, respectively. In a nutshell, DRRN advances the SR
performance with a deeper yet concise network. Speciﬁ-
cally, DRRN has two major algorithmic novelties:
(1) Both global and local residual learning are introduced
in DRRN. In VDSR and DRCN, the residual image is es-
timated from the input and output of the networks, termed
as Global Residual Learning (GRL). Since the SR output is
vastly similar to the input, GRL is effective in easing the dif-

ﬁculty of training deep networks. Therefore, we also adopt
GRL in our identity branch. Further, very deep networks
could suffer from the performance degradation problem,
as observed in visual recognition [8] and image restora-
tion [17]. The reason may be a signiﬁcant amount of image
details are lost after so many layers. To address this issue,
we introduce an enhanced residual unit structure, termed
as multi-path mode Local Residual Learning (LRL), where
the identity branch not only carries rich image details to late
layers, but also helps gradient ﬂow. GRL and LRL mainly
differ in that LRL is performed in every few stacked lay-
ers, while GRL is performed between the input and output
images, i.e., DRRN has many LRLs and only 1 GRL.
(2) Recursive learning of residual units is proposed in
In DRCN [14], a
DRRN to keep our model compact.
deep recursive layer (up to 16 convolutional recursions)
is learned and the weights are shared in the 16 convolu-
tional recursions. Our DRRN has two major differences
compared to DRCN: (a) Unlike DRCN that shares weights
among convolutional layers, DRRN has a recursive block
consisting of several residual units, and the weight set is
shared among these residual units. (b) To address the van-
ishing/exploding gradients problem of very deep models,
DRCN supervises every recursion so that the supervision
on early recursions help backpropagation. DRRN is re-
lieved from this burden by designing a recursive block with
a multi-path structure. Our model can be easily trained even
with 52 convolutional layers. Last but not least, through re-
cursive learning, DRRN can improve accuracy by increas-
ing depth without adding any weight parameters.

To illustrate the effectiveness of the two strategies used
in DRRN, Fig. 1 shows the Peak Signal-to-Noise Ratio
(PSNR) performance of several recent CNN models for
SR [2, 13, 14, 17, 25, 32] versus the number of parameters,
denoted as k. Compared to the prior CNN models, DRRN
achieves the best performance with fewer parameters.

2. Related Work

Since Sec. 1 overviews DL-based SISR, this section fo-
cuses on three most related work to ours: ResNet [8],
VDSR [13] and DRCN [14]. Fig. 2 illustrates these models
via simpliﬁed network structures with only 6 convolutional
layers, where the activation functions, batch normalization
(BN) [11] and ReLU [19], are omitted for clarity.

2.1. ResNet

The main idea of ResNet [8] is to use a residual learn-
ing framework to ease the training of very deep networks.
Instead of hoping every few stacked layers directly ﬁt the
desired underlying mapping, the authors explicitly let these
layers ﬁt a residual mapping, which is assumed to be easier
for optimization. Denoting the input as x and the underly-
ing mapping as H(x), the residual mapping is deﬁned as

3148

Input

conv

Å

conv

conv

conv

conv

Å

conv

Output

Input

Input

Input

conv

conv

conv

conv

conv

conv

Å

Output

conv

conv

conv

conv

conv

conv

Output 1

Output 4

w1

w4

...

Å

Output

(c) DRCN

conv

conv

conv

Å

conv

conv

Å

conv

Å

Output

(a) ResNet

(b) VDSR

(d) DRRN (ours)

Figure 2. Simpliﬁed structures of (a) ResNet [8]. The green dashed box means a residual unit. (b) VDSR [13]. The purple line refers to a
global identity mapping. (c) DRCN [14]. The blue dashed box refers to a recursive layer, among which the convolutional layers (with light
green color) share the same weights. (d) DRRN. The red dashed box refers to a recursive block consisting of two residual units. In the
recursive block, the corresponding convolutional layers in the residual units (with light green or light red color) share the same weights. In
all four cases, the outputs with light blue color are supervised, and L is the element-wise addition.

Method
ResNet
VDSR
DRCN

DRRN

Key Strategies
Chain mode local residual learning
Global residual learning
Global residual learning + recursive learning (single weight layer) + multi-CNN ensemble
Multi-path mode local residual learning
+ global residual learning + recursive learning (multiple weight layers in the residual unit)

Mathematical Formulation
y = fRec(UU(UU−1(...(U1(f1(x)))...)))
y = fRec(fd−1(fd−2(...(f1(x))...))) + x
y = ΣT
2 (f1(x))) + x)
y = fRec(RB(RB−1(...(R1(x))...))) + x

t=1wt · (fRec(f (t)

Table 1. Strategies used in ResNet [8], VDSR [13], DRCN [14] and DRRN. U, d, T, and B are the numbers of residual units in ResNet,
convolutional layers in VDSR, recursions in DRCN, and recursive blocks in DRRN, respectively. x and y are input and output of networks.
f denotes function of convolutional layer. U denotes function of residual unit structure and R denotes function of our recursive block.

F(x) := H(x) − x, and a residual unit structure is thus:

ˆx = U (x) = σ(F(x, W ) + h(x)),

(1)

where ˆx is the output of the residual unit, h(x) is an iden-
tity mapping [8] : h(x) = x, W is a set of weights (the bi-
ases are omitted to simplify notations), function σ denotes
ReLU, F(x, W ) is the residual mapping to be learned, and
U denotes the function of the residual unit structure. For
a basic residual unit that stacks two convolutional layers,
F(x, W ) = W2σ(W1x). By stacking such structures to
construct a very deep 152-layer network, ResNet won the
ﬁrst place in the ILSVRC 2015 classiﬁcation competition.
Since the residual learning in ResNet is adopted in every
few stacked layers, this strategy is a form of local residual
learning, where residual units are stacked in the chain mode.

2.2. VDSR

Differing from ResNet that uses residual learning in ev-
ery few stacked layers, VDSR [13] introduces GRL, i.e.,
residual learning between the input ILR image and the out-
put HR image. There are three notes for VDSR: (1) Un-

like SRCNN [2] that only uses 3 layers, VDSR stacks 20
weight layers (3 × 3 for each layer) in the residual branch,
which leads to a much larger receptive ﬁeld (41 × 41 vs.
13 × 13). (2) GRL and adjustable gradient clipping enable
VDSR to converge very fast (∼4 hours on GPU Titan Z).
(3) By adopting scale augmentation, a single network of
VDSR is robust to images with different scales. Later, we
will show that VDSR actually is a special case of DRRN,
when there’s no residual unit in our recursive block.

2.3. DRCN

DRCN [14] is motivated by the observation that adding
more weight layers introduces more parameters, where the
model is likely to overﬁt and also becomes disk hungry. To
address these issues, the authors introduce a recursive layer
into the network, so that the model parameters do not in-
crease while more recursions are performed in the recursive
layer. DRCN consists of three parts: embedding net, infer-
ence net and reconstruction net, which are illustrated as the
ﬁrst, middle 4, and last convolutional layer(s) in Fig. 2(c),
respectively. The embedding net f1(x) represents a given

3149

H0

Hu-1

conv

conv

Å

Hu

BN

ReLU

weight

Figure 3. A close look at the u-th residual unit in DRRN. The black
dashed box represents the residual function F , which consists of
two “conv” layers and each stacked by BN-ReLU-weight layers.

image x as feature maps H0. The inference net f2(H0)
stacks T recursions (T = 16 in [14]) in a recursive layer,
with shared weights among these recursions. Finally, the
reconstruction net fRec(HT), where HT is the output of the
inference net, generates the intermediate HR image. How-
ever, since training such a deep network is difﬁcult, the au-
thors further propose two mitigations, recursive-supervision
and skip-connection. Speciﬁcally, for the t-th intermediate
recursion in the recursive layer, the output after reconstruc-
tion net is formulated as

yt = fRec(f (t)

2 (f1(x))) + x,

(2)

where x is skip-connection and basically a GRL. Each in-
termediate prediction yt is learned with supervision. Fi-
nally, an ensemble strategy is adopted and the output is the
weighted average of all predictions y = ΣT
t=1wt · yt, with
weights wt learned during training.

3. Deep Recursive Residual Network

In this section, we present the technical parts of our pro-
posed DRRN. Speciﬁcally, we adopt global residual learn-
ing in the identity branch and introduce recursive learn-
ing into the residual branch by constructing the recursive
block structure, in which several residual units are stacked.
Noted that in ResNet [8], different residual units use dif-
ferent inputs for the identity branch (green dashed boxes in
Fig. 2(a)). However, in our recursive block, a multi-path
structure is used and all the residual units share the same in-
put for the identity branch (green dashed boxes in Fig. 2(d)),
which further facilitates the learning [16]. We highlight the
differences of the network structures between DRRN and
the related models in Tab. 1. Now, we will gradually present
more details of our model, from the residual unit to the re-
cursive block and ﬁnally the whole network structure.

3.1. Residual Unit

In ResNet [8], the basic residual unit is formulated as
Eq. 1 and the activation functions (BN [11] and ReLU [19])
are performed after the weight layers. In contrast to such
a “post-activation” structure, He et al. [9] propose a “pre-
activation” structure, which performs the activation before
the weight layers. They claim that the pre-activation version

xb-1

xb-1

conv

conv

conv

Å

conv

conv

Å

xb

conv

conv

conv

Å

xb

xb-1

conv

conv

conv

Å

conv

conv

Å

conv

conv

Å

xb

(a) U=1

(b) U=2

(c) U=3

Figure 4. Structures of our recursive blocks. U means number of
residual units in the recursive block.

is much easier to train and generates better performance
than the post-activation version. Speciﬁcally, the residual
unit with pre-activation structure is formulated as

H u = F(H u

1, W u) + H u

1,

−

−

(3)

−

where u = 1, 2, ..., U, U is the number of residual units in a
recursive block, H u
1 and H u are the input and output of
the u-th residual unit, and F denotes the residual function.
Instead of directly using the above residual unit, we mod-
ify Eq. 3 so that the inputs to the identity branch and the
residual branch are different. As described in the beginning
of Sec. 3, the inputs to all of the identity branches of the
residual units in one recursive block are kept the same, i.e.,
H 0 in Fig. 3. As a result, there are multiple paths between
the input and output of our recursive block, as shown in
Fig. 4. The residual paths help to learn highly complex fea-
tures and the identity paths help gradient backpropagation
during training. Compared to the chain mode, this multi-
path mode facilitates the learning and is less prone to over-
ﬁtting [16]. Therefore, we formulate our residual unit as

H u = G(H u

1) = F(H u

1, W ) + H 0,

−

−

(4)

where G denotes the function of our residual unit, H 0 is the
result of the ﬁrst convolutional layer in the recursive block.
Since the residual unit is recursively learned, the weight set
W is shared among the residual units within a recursive
block, but different across different recursive blocks.

3.2. Recursive Block

We now introduce the details of our recursive block.
First, we illustrate the structure of our recursive blocks in
Fig. 4. Motivated by [16], we introduce a convolutional
layer at the beginning of the recursive block, and then sev-
eral residual units mentioned in Sec. 3.1 are stacked. We

3150

x

RB

RB

RB

RB

RB

RB

conv

Å

y

x3

conv

conv

conv

Å

conv

conv

Å

conv

conv

Å

x4

Figure 5. An example network structure of DRRN with B = 6 and
U = 3. Here, “RB” layer refers to a recursive block.

1 and xb
denote B as the number of recursive blocks, xb
(b = 1, 2, ..., B) as the input and output of the b-th recursive
block, and H 0
1
−
through the ﬁrst convolutional layer, whose function is fb.
According to Eq. 4, the result of u-th residual unit is

1) as the result after passing xb

b = fb(xb

−

−

H u

b = G(H u

−

1) = F(H u
b

1

, Wb) + H 0
b .

−

(5)

Thus, the output of the b-th recursive block xb is
xb = H U

1)) = G(G(...(G(fb(xb

b = G(U)(fb(xb

−

where U-fold operations of Gb are performed.

1)))...)),
(6)

−

3.3. Network Structure

Finally, we simply stack several recursive blocks, fol-
lowed by a convolutional layer reconstructing the residual
between the LR and HR images. The residual image is then
added to the global identity mapping from the input LR im-
age. The entire network structure of DRRN is illustrated in
Fig. 5. Actually, VDSR [13] can be viewed as a special case
of DRRN, i.e., when U = 0, DRRN becomes VDSR.

DRRN has two key parameters:

the recursive block
number B and the residual unit number U in each recur-
sive block. Given different B and U, we can learn DRRN
with different depths – the number of convolutional layers.
Speciﬁcally, the depth of DRRN d is calculated as:

d = (1 + 2 × U) × B + 1.

Denoting x and y to be the input and output of DRRN,

R to be the function of the b-th recursive block, we have

xb = Rb(xb

1) = G(U)(fb(xb

1)).

−

−

(7)

(8)

where fRec is a function for the last convolutional layer in
DRRN to reconstruct the residual. Tab. 1 lists the mathe-
matical formulations of ResNet, VDSR, DRCN and DRRN.
i=1, where N is the num-
ber of training patches and ˜x(i) is the ground truth HR patch
of the LR patch x(i), the loss function of DRRN is

Given a training set {x(i), ˜x(i)}N

L(Θ) =

k˜x(i) − D(x(i))k2,

(10)

1
2N

N

X
i=1

where Θ denotes the parameter set. The objective function
is optimized via the mini-batch stochastic gradient descent
(SGD) with backpropagation [15]. We implement DRRN
via Caffe [12].

4. Experiments
4.1. Datasets

By following [13, 23], we use a training dataset of 291
images, where 91 images are from Yang et al. [35] and other
200 images are from Berkeley Segmentation Dataset [18].
For testing, we utilize four widely used benchmark datasets,
Set5 [1], Set14 [36], BSD100 [18] and Urban100 [10],
which have 5, 14, 100 and 100 images respectively.

4.2. Implementation Details

Data augmentation is performed on the 291-image train-
ing dataset. Inspired by [30], the ﬂipped and rotated ver-
sions of the training images are considered. Speciﬁcally,
we rotate the original images by 90◦, 180◦, 270◦ and ﬂip
them horizontally. After that, for each original image, we
have 7 additional augmented versions. Besides, inspired by
VDSR [13], we also use scale augmentation to train our
model, and images with different scales (×2, ×3 and ×4)
are all included in the training set. Therefore, for all differ-
ent scales, we only need to train a single model.

Training images are split into 31 × 31 patches, with the
stride of 21, by considering both the training time and stor-
age complexities. We set the mini-batch size of SGD to 128,
4.
momentum parameter to 0.9, and weight decay to 10−
Every weight layer has 128 ﬁlters of the size 3 × 3.

For weight initialization, we use the same method as He
et al. [7], which is shown to be suitable for networks utiliz-
ing ReLU. The initial learning rate is set to 0.1 and then de-
creased to half every 10 epochs. Since a large learning rate
is used in our work, we adopt the adjustable gradient clip-
ping [13] to boost the convergence rate while suppressing
exploding gradients. Speciﬁcally, the gradients are clipped
to [− θ
γ ], where γ is the current learning rate and θ = 0.01
is the gradient clipping parameter. Training a DRRN of
d = 20 roughly takes 4 days with 2 Titan X GPUs.

γ , θ

When b = 1, we deﬁne x0 = x. Then, DRRN can be
formulated as

4.3. Study of B and U

y = D(x) = fRec(RB(RB

1(...(R1(x))...))) + x,

(9)

−

In this subsection, we explore various combinations of B
and U to construct different DRRN structures with different

3151

Set5

Set14

Dataset

Scale Bicubic
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

RFL [23]
36.54/0.9537
32.43/0.9057
30.14/0.8548
32.26/0.9040
29.05/0.8164
27.24/0.7451
31.16/0.8840
28.22/0.7806
26.75/0.7054
29.11/0.8904
25.86/0.7900
24.19/0.7096
Table 2. Benchmark results. Average PSNR/SSIMs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, BSD100 and Urban100. Red
color indicates the best performance of our methods and blue color indicates the best performance of previous methods.

DRRN B1U9 DRRN B1U25
37.66/0.9589
33.93/0.9234
31.58/0.8864
33.19/0.9133
29.94/0.8339
28.18/0.7701
32.01/0.8969
28.91/0.7992
27.35/0.7262
31.02/0.9164
27.38/0.8331
25.35/0.7576

VDSR [13]
37.53/0.9587
33.66/0.9213
31.35/0.8838
33.03/0.9124
29.77/0.8314
28.01/0.7674
31.90/0.8960
28.82/0.7976
27.29/0.7251
30.76/0.9140
27.14/0.8279
25.18/0.7524

DRCN [14]
37.63/0.9588
33.82/0.9226
31.53/0.8854
33.04/0.9118
29.76/0.8311
28.02/0.7670
31.85/0.8942
28.80/0.7963
27.23/0.7233
30.75/0.9133
27.15/0.8276
25.14/0.7510

SelfEx [10]
36.49/0.9537
32.58/0.9093
30.31/0.8619
32.22/0.9034
29.16/0.8196
27.40/0.7518
31.18/0.8855
28.29/0.7840
26.84/0.7106
29.54/0.8967
26.44/0.8088
24.79/0.7374

SRCNN [2]
36.66/0.9542
32.75/0.9090
30.48/0.8628
32.45/0.9067
29.30/0.8215
27.50/0.7513
31.36/0.8879
28.41/0.7863
26.90/0.7101
29.50/0.8946
26.24/0.7989
24.52/0.7221

37.74/0.9591
34.03/0.9244
31.68/0.8888
33.23/0.9136
29.96/0.8349
28.21/0.7720
32.05/0.8973
28.95/0.8004
27.38/0.7284
31.23/0.9188
27.53/0.8378
25.44/0.7638

33.66/0.9299
30.39/0.8682
28.42/0.8104
30.24/0.8688
27.55/0.7742
26.00/0.7027
29.56/0.8431
27.21/0.7385
25.96/0.6675
26.88/0.8403
24.46/0.7349
23.14/0.6577

Urban100

BSD100

more

params

less

Figure 6. PSNR of various DRRNs at B and U combinations. The
color of the point indicates the PSNR that corresponds to the bar
on the right and 4 depth contours (d = 50, 30, 20, 10) are also
plotted. The tests are conducted for scale factor ×3 on Set5.

depths, and see how the two parameters affect the perfor-
mance. In Fig. 6, we build a grid of B and U, and sample
several points in the grid with the depth ranging from 8 to 52
layers. The parameter number keeps the same when more
residual units are used in one recursive block, and linearly
increases when more recursive blocks are stacked.

First, to clearly show how a single parameter affects
DRRN, we ﬁx one parameter to 3 and change the other
from 1 to 4. Fig. 6 shows that increasing B or U results
in deeper models and achieves better performance, which
indicates deeper is still better. Despite different structures,
these models are comparable as long as their depths are sim-
ilar, e.g., B2U3 (d = 15, k = 784K) and B3U2 (d = 16,
k = 1, 182K) achieve 33.76 and 33.77 dB, respectively.

The structures mentioned above all use the recursive
learning strategy. Next, we test three very different struc-
tures to demonstrate the effectiveness of such a strategy.
Speciﬁcally, we ﬁx one parameter to 1 and change the other
to construct networks with d = 52. This results in two ex-
treme structures: B1U25 (k = 297K) and B17U1 (k =
7, 375K). For B1U25, only one recursive block is used, in
which 25 residual units are recursively learned. For B17U1,
17 recursive blocks are stacked, with no recursive learn-

ing. We also construct a normal structure B3U8 (d = 52,
k = 1, 182K). Fig. 6 shows that despite different struc-
tures, the three networks achieve comparable performance
(B17U1 34.03 dB, B3U8 34.04 dB and B1U25 34.03 dB)
and outperform the pervious shallow networks. Thanks to
the recursive learning strategy, B1U25 can achieve state-of-
the-art results using far fewer parameters.

4.4. Comparison with State of the Art Models

We now provide quantitative and qualitative compar-
isons. Considering both the performance and number of
parameters, we choose DRRN B1U25 (d = 52, k = 297K)
as our best model. For fair comparison, we also construct
a DRRN B1U9 (d = 20, k = 297K) structure, which has
the same depth as VDSR and DRCN, but fewer parameters.
Both the DL [2, 13, 14] and non-DL [10, 20, 23] methods in
recent years are used for benchmark. Experimental setting
is kept the same as those previous methods. Speciﬁcally, we
ﬁrst apply bicubic interpolation to the color components of
an image and all models are applied to its luminance com-
ponent only. Therefore, the input and output images are of
the same size. For fair comparison, similar to [2, 13, 14, 23],
we crop pixels near image boundary before evaluation, al-
though this is unnecessary for DRRN.

Tab. 2 summarizes quantitative results on the four test-
ing sets, by citing the results of prior methods from [13,14].
The two DRRN models outperforms all existing methods in
all datasets and scale factors, in both PSNR and Structural
SIMilarity (SSIM) 1. Especially on the recent difﬁcult Ur-
ban100 dataset [10], DRRN signiﬁcantly advances the state
of the art, with the improvement margin of 0.47, 0.38, and
0.26 dB on scale factor ×2, ×3 and ×4 respectively.

Further, we also use another metric: Information Fidelity
Criterion (IFC) [24] for comparison, which claims to have
the highest correlation with perceptual scores for SR evalu-
ation [34]. The results are presented in Tab. 3. Note that

1With two convolutional layers in the residual branch, DRRN achieves
state-of-the-art performance. More complex designs have the potential to
improve performance but are not the focus of this work.

3152

Dataset

Set5

Set14

Urban100

Scale
×2
×3
×4
×2
×3
×4
×2
×3
×4

Bicubic
6.083
3.580
2.329
6.105
3.473
2.237
6.245
3.620
2.361

SRCNN [2]
8.036
4.658
2.991
7.784
4.338
2.751
7.989
4.584
2.963

SelfEx [10]
7.811
4.748
3.166
7.591
4.371
2.893
7.937
4.843
3.314

RFL [23]
8.556
4.926
3.191
8.175
4.531
2.919
8.450
4.801
3.110

PSyCo [20]
8.642
5.083
3.379
8.280
4.660
3.055
8.589
5.031
3.351

VDSR [13]
8.569
5.221
3.547
8.178
4.730
3.133
8.645
5.194
3.496

DRRN B1U9
8.583
5.241
3.581
8.181
4.732
3.147
8.653
5.259
3.536

DRRN B1U25
8.671
5.397
3.703
8.320
4.878
3.252
8.917
5.456
3.676

Table 3. Benchmark results. Average IFCs for the scale factor ×2, ×3 and ×4 on datasets Set5, Set14 and Urban100. Red color indicates
the best performance of our methods and blue color indicates the best performance of previous methods.

Ground Truth

Bicubic

SRCNN

SelfEx

VDSR

DRRN_B1U9

DRRN_B1U25

(PSNR/SSIM/IFC)

(21.10/0.7046/3.134) (21.77/0.7540/3.761) (21.94/0.7608/3.669) (22.58/0.7942/4.341) (22.74/0.7999/4.365) (23.37/0.8158/4.713)

(PSNR/SSIM/IFC)

(22.55/0.7073/3.591) (23.51/0.7608/4.344) (23.42/0.7587/4.281) (23.99/0.7728/4.716) (24.23/0.7781/4.734) (24.41/0.7805/4.914)

(PSNR/SSIM/IFC)

(21.98/0.8126/1.920) (24.80/0.8928/2.666) (24.85/0.9076/2.941) (25.85/0.9289/3.406) (26.33/0.9365/3.557) (26.48/0.9415/3.822)

Figure 7. Qualitative comparison. (1) The ﬁrst row shows image “img059” (Urban100 with scale factor ×3). DRRN recovers sharp lines,
while others all give blurry results. (2) The second row shows image “253027” (BSD100 with scale factor ×3). DRRN accurately recovers
the pattern. (3) The last row shows image “ppt3” (Set14 with scale factor ×4). Texts in DRRN are sharp, while others are blurry.

the results of [2, 10, 20, 23] are cited from [20] 2, while
the results of VDSR come from our re-implementation.
Similar to DRRN, the VDSR re-implementation also uses
BN and ReLU as the activation functions, unlike the orig-
inal VDSR [13] that does not use BN. These results are

2Since PSyCo [20] does not present complete PSNR/SSIM perfor-

mance on the four benchmarks, we do not include it in Tab. 2.

faithful since our VDSR re-implementation achieves similar
benchmark performance as [13] reported in Tab. 2. Since
only Set5, Set14 and Urban100 are used in [20], we omit
BSD100 in this test. It is clear that DRRN still outperforms
all existing methods in all datasets and scale factors. Re-
garding the speed, our 20-layer B1U9 network takes 0.25
second to process a 288 × 288 image on a Titan X GPU.

3153

16 layers

22 layers

28 layers

Shallow

Deep

Very deep

Figure 8. PSNR for scale factor ×3 on Set5 using VDSR (blue),
DRRN NS (green), DRRN C (cyan) and DRRN (red).

Methods
Loc. Res. L
Recu. L
Multi-path
PSNR

×
×
×
33.86

VDSR

DRRN NS C

DRRN NS

DRRN C

DRRN

√

×
×
33.92

√

×
√
33.97

√
√

×
33.95

√
√
√
33.99

Table 4. Average PSNR when different DRRN components are
turned on or off, for scale factor ×3 on dataset Set5.

among

Qualitative

comparisons

SRCNN [2],
SelfEx [10], VDSR [13] and DRRN are illustrated in
Fig. 7. For SRCNN and SelfEx, we use their public codes.
For VDSR, we use our re-implementation. As we can see,
our method produces relatively sharper edges with respect
to patterns, while other methods may give blurry results.

4.5. Discussions

Since global residual learning has been well discussed
in [13], in this section, we mainly focus on local residual
learning (LRL), recursive learning and multi-path structure.
Local Residual Learning To demonstrate the effective-
ness of LRL, DRRN is compared with VDSR [13], which
has no LRL. For fair comparison, the depth and number of
parameters are kept the same for both methods. Speciﬁ-
cally, we evaluate three depths: 16(B3U2), 22(B3U3), and
28(B3U4) convolutional layers. Each convolutional layer
has 128 ﬁlters with the size 3 × 3. To keep the parameter
number the same, in this test we do not share the weight
set of the residual units in one recursive block, and denote
this DRRN structure as DRRN NS. Fig. 8 shows the PSNR
of both methods in different depths. We see that the LRL
strategy consistently improves VDSR at all depths.
Recursive Learning To contrast our recursive learning
strategy, the three DRRN NS versions are compared with
the three weight-shared versions (Fig. 8). Storage is an im-
portant factor to consider when building a deep model. The
recursive learning strategy can reduce the storage demand
and keep a concise model while increasing its depth. Inter-
estingly the weight-shared DRRN versions achieve compa-
rable or even better performance than DRRN NS versions

Figure 9. Comparing deep and shallow models proposed in recent
three years that report PSNR for scale factor ×3 on Set5 and Set14.

while using only a small fraction of parameters, which in-
dicates when limited training set (e.g., 291 images) is used,
the recursive learning is indeed effective under the same
structure, and less prone to overﬁtting [16].
Multi-Path Structure To demonstrate the effectiveness
of multi-path structure, we compare DRRN with the chain
structure, denoted as DRRN C. As shown in Fig. 8, with the
same depth and parameter number, the multi-path structures
achieve higher PSNR than the corresponding chain struc-
tures in all three cases. Further, Tab. 4 presents the compre-
hensive study on performance gains using network B3U4 as
the example. It shows how different technical parts improve
the performance compared to the baseline VDSR.
Deep vs. Shallow Finally, we give a comparison of the deep
and shallow SISR models, which are published in recent
three years (2014 to 2016) that report PSNR for scale factor
×3 on datasets Set5 and Set14. Shallow (non-DL) mod-
els include A+ [31], SelfEx [10], RFL [23], NBSRF [22],
PSyCo [20] and IA [30]. The deep models (d ≤ 8) in-
clude SRCNN [2], DJSR [33], CSCN [32], ESPCN [25]
and FSRCNN [3]. Very deep models (d ≥ 20) include
VDSR [13], DRCN [14], RED [17] and DRRN with d = 20
and 52. Fig. 9 shows that 1) very deep models signif-
icantly outperform the shallow models; 2) DRRN B1U9
(d = 20, k = 297K) already outperforms the state of the
arts with the same depth but fewer parameters; 3) a deeper
DRRN B1U25 (d = 52, k = 297K) further improves the
performance without adding any parameters.

5. Conclusions

In this paper, we propose Deep Recursive Residual Net-
work (DRRN) for single image super-resolution. In DRRN,
an enhanced residual unit structure is recursively learned in
a recursive block, and we stack several recursive blocks to
learn the residual image between the HR and LR images.
The residual image is then added to the input LR image
from a global identity branch to estimate the HR image.
Extensive benchmark experiments and analysis show that
DRRN is a deep, concise, and superior model for SISR.

3154

References

[1] C. M. Bevilacqua, A. Roumy, and M.-L. A. Morel. Low-
complexity single-image super-resolution based on nonneg-
ative neighbor embedding. In BMVC, 2012. 1, 5

[2] C. Dong, C. Loy, K. He, and X. Tang. Image super-resolution
IEEE Transactions on
using deep convolutional networks.
Pattern Analysis and Machine Intelligence, 38(2):295–307,
2016. 1, 2, 3, 6, 7, 8

[3] C. Dong, C. Loy, and X. Tang. Accelerating the super-
In ECCV, 2016.

resolution convolutional neural network.
8

[4] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a

single image. In ICCV, 2009. 1

[5] K. Gregor and Y. LeCun. Learning fast approximations of

sparse coding. In ICML, 2010. 1

[6] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In ICLR, 2016. 2

[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rec-
tiﬁers: Surpassing human-level performance on ImageNet
classiﬁcation. In ICCV, 2015. 5

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 2, 3, 4

[9] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. arXiv:1603.05027v2, 2016. 4
[10] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In CVPR, 2015.
5, 6, 7, 8

[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 2, 4

[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv:1408.5093,
2014. 5

[13] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In CVPR,
2016. 1, 2, 3, 5, 6, 7, 8

[14] J. Kim, J. K. Lee, and K. M. Lee. Deeply-recursive convolu-
tional network for image super-resolution. In CVPR, 2016.
1, 2, 3, 4, 6, 8

[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. In Proceed-
ings of the IEEE, 1998. 5

[16] M. Liang and X. Hu. Recurrent convolutional neural network

for object recognition. In CVPR, 2015. 4, 8

[17] X.-J. Mao, C. Shen, and Y.-B. Yang. Image restoration us-
ing very deep convolutional encoder-decoder networks with
symmetric skip connections. In NIPS, 2016. 1, 2, 8

[18] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-
cal statistics. In ICCV, 2001. 5

[19] V. Nair and G. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 2, 4

[20] E. Perez-Pellitero,

J. Ruiz-Hidalgo, and
B. Rosenhahn. PSyCo: Manifold span reduction for super
resolution. In CVPR, 2016. 1, 6, 7, 8

J. Salvador,

[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
and et al. ImageNet large scale visual recognition challenge.
arXiv:1409.0575, 2014. 2

[22] J. Salvador and E. Perez-Pellitero. Naive bayes super-

resolution forest. In ICCV, 2015. 1, 8

[23] S. Schulter, C. Leistner, and H. Bischof. Fast and accu-
rate image upscaling with super-resolution forests. In CVPR,
2015. 1, 5, 6, 7, 8

[24] H. Sheikh, A. Bovik, and G. de Veciana. An information
ﬁdelity criterion for image quality assessment using natu-
ral scene statistics. IEEE Transactions on image processing,
14(12):2117–2128, 2005. 6

[25] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In CVPR, 2016. 1, 2, 8
[26] W. Shi, J. Caballero, C. Ledig, X. Zhuang, W. Bai, K. Bha-
tia, A. Marvao, T. Dawes, D. ORegan, and D. Rueckert. Car-
diac image super-resolution with global correspondence us-
ing multi-atlas patchmatch. In MICCAI, 2013. 1

[27] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
2

[28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, and S. Reed. Going

deeper with convolutions. In CVPR, 2015. 2

[29] M. W. Thornton, P. M. Atkinson, and D. a. Holland. Sub-
pixel mapping of rural land cover objects from ﬁne spa-
tial resolution satellite sensor imagery using super-resolution
International Journal of Remote Sensing,
pixel-swapping.
27(3):473–491, 2006. 1

[30] R. Timofte, R. Rothe, and L. V. Gool. Seven ways to improve
In CVPR,

example-based single image super resolution.
2016. 5, 8

[31] R. Timofte, V. D. Smet, and L. V. Gool. A+: Adjusted an-
chored neighborhood regression for fast super-resolution. In
ACCV, 2014. 1, 8

[32] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep
networks for image super-resolution with sparse prior.
In
ICCV, 2015. 1, 2, 8

[33] Z. Wang, Y. Yang, Z. Wang, S. Chang, W. Han, J. Yang,
and T. Huang. Self-tuned deep super resolution. In CVPR
workshop, 2015. 8

[34] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-

resolution: A benchmark. In ECCV, 2014. 6

[35] J. Yang, J. Wright, T. Huang, and Y. Ma. Image superresolu-
tion via sparse representation. IEEE Transactions on image
processing, 19(11):2861–2873, 2010. 1, 5

[36] R. Zeyde, M. Elad, and M. Protter. On single image scale-
up using sparse-representations. Curves and Surfaces, pages
711–730, 2012. 5

[37] W. Zou and P. C. Yuen. Very low resolution face recog-
IEEE Transactions on image processing,

nition problem.
21(1):327–340, 2012. 1

3155

Image Super-Resolution via Deep Recursive Residual Network

Ying Tai∗ 1, Jian Yang1, and Xiaoming Liu2

1Department of Computer Science and Engineering, Nanjing University of Science and Technology
2Department of Computer Science and Engineering, Michigan State University
{taiying, csjyang}@njust.edu.cn, liuxm@cse.msu.edu

Abstract

Recently, Convolutional Neural Network (CNN) based
models have achieved great success in Single Image Super-
Resolution (SISR). Owing to the strength of deep networks,
these CNN models learn an effective nonlinear mapping
from the low-resolution input image to the high-resolution
target image, at the cost of requiring enormous parameters.
This paper proposes a very deep CNN model (up to 52 con-
volutional layers) named Deep Recursive Residual Network
(DRRN) that strives for deep yet concise networks. Speciﬁ-
cally, residual learning is adopted, both in global and local
manners, to mitigate the difﬁculty of training very deep net-
works; recursive learning is used to control the model pa-
rameters while increasing the depth. Extensive benchmark
evaluation shows that DRRN signiﬁcantly outperforms state
of the art in SISR, while utilizing far fewer parameters.
Code is available at https://github.com/tyshiwo
/DRRN CVPR17.

1. Introduction

Single Image Super-Resolution (SISR) is a classic com-
puter vision problem, which aims to recover a high-
resolution (HR) image from a low-resolution (LR) image.
Since SISR restores the high-frequency information, it is
widely used in applications such as medical imaging [26],
satellite imaging [29], security and surveillance [37], where
high-frequency details are greatly desired.

In recent years, due to the powerful learning ability,
Deep Learning (DL) models, especially Convolutional Neu-
ral Networks (CNN), are widely used to address the ill-

∗This work was conducted when the ﬁrst author was a visiting scholar
at Michigan State University.
It was supported by the National Sci-
ence Fund of China under Grant Nos. 91420201, 61472187, 61502235,
61233011, 61373063 and 61602244, the 973 Program No.2014CB349303,
Program for Changjiang Scholars and Innovative Research Team in Uni-
versity, and partially sponsored by CCF-Tencent Open Research Fund.

DRCN

RED30

DRRN_B1U25

DRRN_B1U9

VDSR

CSCN

ESPCN

SRCNN

Figure 1. PSNR of recent CNN models for scale factor ×3 on
Set5 [1]. Red points are our models. △, ✩, and ◦ are models with
less than 5 layers, 20 layers, and more than 30 layers, respectively.
DRRN B1U9 means there is 1 recursive block, in which 9 residual
units are stacked. With the same depth but far fewer parameters,
DRRN B1U9 achieves better performance than the state-of-the-
art methods [13,14]. After increasing the depth without adding any
parameters, the 52-layer DRRN B1U25 further improves the per-
formance and signiﬁcantly outperforms VDSR [13], DRCN [14]
and RED30 [17] by 0.37, 0.21 and 0.21 dB respectively.

posed inverse problem of Super Resolution (SR), and have
demonstrated superiority over reconstruction-based meth-
ods [4, 35] or other learning paradigms [20, 22, 23, 31]. As
the pioneer CNN model for SR, Super-Resolution Convo-
lutional Neural Network (SRCNN) [2] predicts the nonlin-
ear LR-HR mapping via a fully convolutional network, and
signiﬁcantly outperforms classical non-DL methods. How-
ever, SRCNN does not consider any self similarity prop-
erty. To address this issue, the Deep Joint Super Resolution
(DJSR) jointly utilizes both the wealth of external examples
and the power of self examples unique to the input. Inspired
by the learning iterative shrinkage and thresholding algo-
rithm [5], Cascaded Sparse Coding Network (CSCN) [32]
is trained end-to-end to fully exploit the natural sparsity of
images. Shi et al. [25] observe that the prior models [2, 32]
increase LR image’s resolution via bicubic interpolation be-

13147

fore CNN learning, which increases the computational cost.
The Efﬁcient Sub-Pixel Convolutional neural Network (ES-
PCN) reduces the computational and memory complexity,
by increasing the resolution from LR to HR only at the end
of the network.

1, instead of 10−

One commonality among the above CNN models is that
their networks contain fewer than 5 layers, e.g., SRCNN [2]
uses 3 convolutional layers. Their deeper structures with 4
or 5 layers do not achieve better performance, which was
attributed to the difﬁculty of training deeper networks and
led to the observation that “the deeper the better” might not
be the case in SR. Inspired by the success of very deep net-
works [8, 27, 28] on ImageNet [21], Kim et al. [13, 14] pro-
pose two very deep convolutional networks for SR, both
stacking 20 convolutional layers, from the viewpoints of
training efﬁciency and storage, respectively. On the one
hand, to accelerate the convergence speed of very deep net-
works, the VDSR [13] is trained with a very high learning
4 in SRCNN) and the authors fur-
rate (10−
ther use residual learning and adjustable gradient clipping
to solve gradient explosion problem. On the other hand, to
control the model parameters, the Deeply-Recursive Con-
volutional Network (DRCN) [14] introduces a very deep re-
cursive layer via a chain structure with up to 16 recursions.
To mitigate the difﬁculty of training DRCN, the authors use
recursive-supervision and skip-connection, and adopt an en-
semble strategy to further improve the performance. Very
recently, Mao et al. [17] propose a 30-layer convolutional
auto-encoder network named RED30 for image restoration,
which uses symmetric skip connections to help training. All
of the three models learn the residual image between the in-
put Interpolated LR (ILR) image and the ground truth HR
image in the residual branch. The residual image is then
added to the ILR image from the identity branch to esti-
mate the HR image. The three models outperform the pre-
vious DL and non-DL methods by a large margin, which
demonstrates “the deeper the better” is still true in SR.

Despite achieving excellent performance, the very deep
networks require enormous parameters. Compared to the
compact models, large models demand more storage space
and are less applicable to mobile systems [6]. To address
this issue, we propose a novel Deep Recursive Residual
Network (DRRN) to effectively build a very deep network
structure, which achieves better performance, but with 2×,
6×, and 14× fewer parameters than VDSR, DRCN, and
RED30, respectively. In a nutshell, DRRN advances the SR
performance with a deeper yet concise network. Speciﬁ-
cally, DRRN has two major algorithmic novelties:
(1) Both global and local residual learning are introduced
in DRRN. In VDSR and DRCN, the residual image is es-
timated from the input and output of the networks, termed
as Global Residual Learning (GRL). Since the SR output is
vastly similar to the input, GRL is effective in easing the dif-

ﬁculty of training deep networks. Therefore, we also adopt
GRL in our identity branch. Further, very deep networks
could suffer from the performance degradation problem,
as observed in visual recognition [8] and image restora-
tion [17]. The reason may be a signiﬁcant amount of image
details are lost after so many layers. To address this issue,
we introduce an enhanced residual unit structure, termed
as multi-path mode Local Residual Learning (LRL), where
the identity branch not only carries rich image details to late
layers, but also helps gradient ﬂow. GRL and LRL mainly
differ in that LRL is performed in every few stacked lay-
ers, while GRL is performed between the input and output
images, i.e., DRRN has many LRLs and only 1 GRL.
(2) Recursive learning of residual units is proposed in
In DRCN [14], a
DRRN to keep our model compact.
deep recursive layer (up to 16 convolutional recursions)
is learned and the weights are shared in the 16 convolu-
tional recursions. Our DRRN has two major differences
compared to DRCN: (a) Unlike DRCN that shares weights
among convolutional layers, DRRN has a recursive block
consisting of several residual units, and the weight set is
shared among these residual units. (b) To address the van-
ishing/exploding gradients problem of very deep models,
DRCN supervises every recursion so that the supervision
on early recursions help backpropagation. DRRN is re-
lieved from this burden by designing a recursive block with
a multi-path structure. Our model can be easily trained even
with 52 convolutional layers. Last but not least, through re-
cursive learning, DRRN can improve accuracy by increas-
ing depth without adding any weight parameters.

To illustrate the effectiveness of the two strategies used
in DRRN, Fig. 1 shows the Peak Signal-to-Noise Ratio
(PSNR) performance of several recent CNN models for
SR [2, 13, 14, 17, 25, 32] versus the number of parameters,
denoted as k. Compared to the prior CNN models, DRRN
achieves the best performance with fewer parameters.

2. Related Work

Since Sec. 1 overviews DL-based SISR, this section fo-
cuses on three most related work to ours: ResNet [8],
VDSR [13] and DRCN [14]. Fig. 2 illustrates these models
via simpliﬁed network structures with only 6 convolutional
layers, where the activation functions, batch normalization
(BN) [11] and ReLU [19], are omitted for clarity.

2.1. ResNet

The main idea of ResNet [8] is to use a residual learn-
ing framework to ease the training of very deep networks.
Instead of hoping every few stacked layers directly ﬁt the
desired underlying mapping, the authors explicitly let these
layers ﬁt a residual mapping, which is assumed to be easier
for optimization. Denoting the input as x and the underly-
ing mapping as H(x), the residual mapping is deﬁned as

3148

Input

conv

Å

conv

conv

conv

conv

Å

conv

Output

Input

Input

Input

conv

conv

conv

conv

conv

conv

Å

Output

conv

conv

conv

conv

conv

conv

Output 1

Output 4

w1

w4

...

Å

Output

(c) DRCN

conv

conv

conv

Å

conv

conv

Å

conv

Å

Output

(a) ResNet

(b) VDSR

(d) DRRN (ours)

Figure 2. Simpliﬁed structures of (a) ResNet [8]. The green dashed box means a residual unit. (b) VDSR [13]. The purple line refers to a
global identity mapping. (c) DRCN [14]. The blue dashed box refers to a recursive layer, among which the convolutional layers (with light
green color) share the same weights. (d) DRRN. The red dashed box refers to a recursive block consisting of two residual units. In the
recursive block, the corresponding convolutional layers in the residual units (with light green or light red color) share the same weights. In
all four cases, the outputs with light blue color are supervised, and L is the element-wise addition.

Method
ResNet
VDSR
DRCN

DRRN

Key Strategies
Chain mode local residual learning
Global residual learning
Global residual learning + recursive learning (single weight layer) + multi-CNN ensemble
Multi-path mode local residual learning
+ global residual learning + recursive learning (multiple weight layers in the residual unit)

Mathematical Formulation
y = fRec(UU(UU−1(...(U1(f1(x)))...)))
y = fRec(fd−1(fd−2(...(f1(x))...))) + x
y = ΣT
2 (f1(x))) + x)
y = fRec(RB(RB−1(...(R1(x))...))) + x

t=1wt · (fRec(f (t)

Table 1. Strategies used in ResNet [8], VDSR [13], DRCN [14] and DRRN. U, d, T, and B are the numbers of residual units in ResNet,
convolutional layers in VDSR, recursions in DRCN, and recursive blocks in DRRN, respectively. x and y are input and output of networks.
f denotes function of convolutional layer. U denotes function of residual unit structure and R denotes function of our recursive block.

F(x) := H(x) − x, and a residual unit structure is thus:

ˆx = U (x) = σ(F(x, W ) + h(x)),

(1)

where ˆx is the output of the residual unit, h(x) is an iden-
tity mapping [8] : h(x) = x, W is a set of weights (the bi-
ases are omitted to simplify notations), function σ denotes
ReLU, F(x, W ) is the residual mapping to be learned, and
U denotes the function of the residual unit structure. For
a basic residual unit that stacks two convolutional layers,
F(x, W ) = W2σ(W1x). By stacking such structures to
construct a very deep 152-layer network, ResNet won the
ﬁrst place in the ILSVRC 2015 classiﬁcation competition.
Since the residual learning in ResNet is adopted in every
few stacked layers, this strategy is a form of local residual
learning, where residual units are stacked in the chain mode.

2.2. VDSR

Differing from ResNet that uses residual learning in ev-
ery few stacked layers, VDSR [13] introduces GRL, i.e.,
residual learning between the input ILR image and the out-
put HR image. There are three notes for VDSR: (1) Un-

like SRCNN [2] that only uses 3 layers, VDSR stacks 20
weight layers (3 × 3 for each layer) in the residual branch,
which leads to a much larger receptive ﬁeld (41 × 41 vs.
13 × 13). (2) GRL and adjustable gradient clipping enable
VDSR to converge very fast (∼4 hours on GPU Titan Z).
(3) By adopting scale augmentation, a single network of
VDSR is robust to images with different scales. Later, we
will show that VDSR actually is a special case of DRRN,
when there’s no residual unit in our recursive block.

2.3. DRCN

DRCN [14] is motivated by the observation that adding
more weight layers introduces more parameters, where the
model is likely to overﬁt and also becomes disk hungry. To
address these issues, the authors introduce a recursive layer
into the network, so that the model parameters do not in-
crease while more recursions are performed in the recursive
layer. DRCN consists of three parts: embedding net, infer-
ence net and reconstruction net, which are illustrated as the
ﬁrst, middle 4, and last convolutional layer(s) in Fig. 2(c),
respectively. The embedding net f1(x) represents a given

3149

H0

Hu-1

conv

conv

Å

Hu

BN

ReLU

weight

Figure 3. A close look at the u-th residual unit in DRRN. The black
dashed box represents the residual function F , which consists of
two “conv” layers and each stacked by BN-ReLU-weight layers.

image x as feature maps H0. The inference net f2(H0)
stacks T recursions (T = 16 in [14]) in a recursive layer,
with shared weights among these recursions. Finally, the
reconstruction net fRec(HT), where HT is the output of the
inference net, generates the intermediate HR image. How-
ever, since training such a deep network is difﬁcult, the au-
thors further propose two mitigations, recursive-supervision
and skip-connection. Speciﬁcally, for the t-th intermediate
recursion in the recursive layer, the output after reconstruc-
tion net is formulated as

yt = fRec(f (t)

2 (f1(x))) + x,

(2)

where x is skip-connection and basically a GRL. Each in-
termediate prediction yt is learned with supervision. Fi-
nally, an ensemble strategy is adopted and the output is the
weighted average of all predictions y = ΣT
t=1wt · yt, with
weights wt learned during training.

3. Deep Recursive Residual Network

In this section, we present the technical parts of our pro-
posed DRRN. Speciﬁcally, we adopt global residual learn-
ing in the identity branch and introduce recursive learn-
ing into the residual branch by constructing the recursive
block structure, in which several residual units are stacked.
Noted that in ResNet [8], different residual units use dif-
ferent inputs for the identity branch (green dashed boxes in
Fig. 2(a)). However, in our recursive block, a multi-path
structure is used and all the residual units share the same in-
put for the identity branch (green dashed boxes in Fig. 2(d)),
which further facilitates the learning [16]. We highlight the
differences of the network structures between DRRN and
the related models in Tab. 1. Now, we will gradually present
more details of our model, from the residual unit to the re-
cursive block and ﬁnally the whole network structure.

3.1. Residual Unit

In ResNet [8], the basic residual unit is formulated as
Eq. 1 and the activation functions (BN [11] and ReLU [19])
are performed after the weight layers. In contrast to such
a “post-activation” structure, He et al. [9] propose a “pre-
activation” structure, which performs the activation before
the weight layers. They claim that the pre-activation version

xb-1

xb-1

conv

conv

conv

Å

conv

conv

Å

xb

conv

conv

conv

Å

xb

xb-1

conv

conv

conv

Å

conv

conv

Å

conv

conv

Å

xb

(a) U=1

(b) U=2

(c) U=3

Figure 4. Structures of our recursive blocks. U means number of
residual units in the recursive block.

is much easier to train and generates better performance
than the post-activation version. Speciﬁcally, the residual
unit with pre-activation structure is formulated as

H u = F(H u

1, W u) + H u

1,

−

−

(3)

−

where u = 1, 2, ..., U, U is the number of residual units in a
recursive block, H u
1 and H u are the input and output of
the u-th residual unit, and F denotes the residual function.
Instead of directly using the above residual unit, we mod-
ify Eq. 3 so that the inputs to the identity branch and the
residual branch are different. As described in the beginning
of Sec. 3, the inputs to all of the identity branches of the
residual units in one recursive block are kept the same, i.e.,
H 0 in Fig. 3. As a result, there are multiple paths between
the input and output of our recursive block, as shown in
Fig. 4. The residual paths help to learn highly complex fea-
tures and the identity paths help gradient backpropagation
during training. Compared to the chain mode, this multi-
path mode facilitates the learning and is less prone to over-
ﬁtting [16]. Therefore, we formulate our residual unit as

H u = G(H u

1) = F(H u

1, W ) + H 0,

−

−

(4)

where G denotes the function of our residual unit, H 0 is the
result of the ﬁrst convolutional layer in the recursive block.
Since the residual unit is recursively learned, the weight set
W is shared among the residual units within a recursive
block, but different across different recursive blocks.

3.2. Recursive Block

We now introduce the details of our recursive block.
First, we illustrate the structure of our recursive blocks in
Fig. 4. Motivated by [16], we introduce a convolutional
layer at the beginning of the recursive block, and then sev-
eral residual units mentioned in Sec. 3.1 are stacked. We

3150

x

RB

RB

RB

RB

RB

RB

conv

Å

y

x3

conv

conv

conv

Å

conv

conv

Å

conv

conv

Å

x4

Figure 5. An example network structure of DRRN with B = 6 and
U = 3. Here, “RB” layer refers to a recursive block.

1 and xb
denote B as the number of recursive blocks, xb
(b = 1, 2, ..., B) as the input and output of the b-th recursive
block, and H 0
1
−
through the ﬁrst convolutional layer, whose function is fb.
According to Eq. 4, the result of u-th residual unit is

1) as the result after passing xb

b = fb(xb

−

−

H u

b = G(H u

−

1) = F(H u
b

1

, Wb) + H 0
b .

−

(5)

Thus, the output of the b-th recursive block xb is
xb = H U

1)) = G(G(...(G(fb(xb

b = G(U)(fb(xb

−

where U-fold operations of Gb are performed.

1)))...)),
(6)

−

3.3. Network Structure

Finally, we simply stack several recursive blocks, fol-
lowed by a convolutional layer reconstructing the residual
between the LR and HR images. The residual image is then
added to the global identity mapping from the input LR im-
age. The entire network structure of DRRN is illustrated in
Fig. 5. Actually, VDSR [13] can be viewed as a special case
of DRRN, i.e., when U = 0, DRRN becomes VDSR.

DRRN has two key parameters:

the recursive block
number B and the residual unit number U in each recur-
sive block. Given different B and U, we can learn DRRN
with different depths – the number of convolutional layers.
Speciﬁcally, the depth of DRRN d is calculated as:

d = (1 + 2 × U) × B + 1.

Denoting x and y to be the input and output of DRRN,

R to be the function of the b-th recursive block, we have

xb = Rb(xb

1) = G(U)(fb(xb

1)).

−

−

(7)

(8)

where fRec is a function for the last convolutional layer in
DRRN to reconstruct the residual. Tab. 1 lists the mathe-
matical formulations of ResNet, VDSR, DRCN and DRRN.
i=1, where N is the num-
ber of training patches and ˜x(i) is the ground truth HR patch
of the LR patch x(i), the loss function of DRRN is

Given a training set {x(i), ˜x(i)}N

L(Θ) =

k˜x(i) − D(x(i))k2,

(10)

1
2N

N

X
i=1

where Θ denotes the parameter set. The objective function
is optimized via the mini-batch stochastic gradient descent
(SGD) with backpropagation [15]. We implement DRRN
via Caffe [12].

4. Experiments
4.1. Datasets

By following [13, 23], we use a training dataset of 291
images, where 91 images are from Yang et al. [35] and other
200 images are from Berkeley Segmentation Dataset [18].
For testing, we utilize four widely used benchmark datasets,
Set5 [1], Set14 [36], BSD100 [18] and Urban100 [10],
which have 5, 14, 100 and 100 images respectively.

4.2. Implementation Details

Data augmentation is performed on the 291-image train-
ing dataset. Inspired by [30], the ﬂipped and rotated ver-
sions of the training images are considered. Speciﬁcally,
we rotate the original images by 90◦, 180◦, 270◦ and ﬂip
them horizontally. After that, for each original image, we
have 7 additional augmented versions. Besides, inspired by
VDSR [13], we also use scale augmentation to train our
model, and images with different scales (×2, ×3 and ×4)
are all included in the training set. Therefore, for all differ-
ent scales, we only need to train a single model.

Training images are split into 31 × 31 patches, with the
stride of 21, by considering both the training time and stor-
age complexities. We set the mini-batch size of SGD to 128,
4.
momentum parameter to 0.9, and weight decay to 10−
Every weight layer has 128 ﬁlters of the size 3 × 3.

For weight initialization, we use the same method as He
et al. [7], which is shown to be suitable for networks utiliz-
ing ReLU. The initial learning rate is set to 0.1 and then de-
creased to half every 10 epochs. Since a large learning rate
is used in our work, we adopt the adjustable gradient clip-
ping [13] to boost the convergence rate while suppressing
exploding gradients. Speciﬁcally, the gradients are clipped
to [− θ
γ ], where γ is the current learning rate and θ = 0.01
is the gradient clipping parameter. Training a DRRN of
d = 20 roughly takes 4 days with 2 Titan X GPUs.

γ , θ

When b = 1, we deﬁne x0 = x. Then, DRRN can be
formulated as

4.3. Study of B and U

y = D(x) = fRec(RB(RB

1(...(R1(x))...))) + x,

(9)

−

In this subsection, we explore various combinations of B
and U to construct different DRRN structures with different

3151

Set5

Set14

Dataset

Scale Bicubic
×2
×3
×4
×2
×3
×4
×2
×3
×4
×2
×3
×4

RFL [23]
36.54/0.9537
32.43/0.9057
30.14/0.8548
32.26/0.9040
29.05/0.8164
27.24/0.7451
31.16/0.8840
28.22/0.7806
26.75/0.7054
29.11/0.8904
25.86/0.7900
24.19/0.7096
Table 2. Benchmark results. Average PSNR/SSIMs for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, BSD100 and Urban100. Red
color indicates the best performance of our methods and blue color indicates the best performance of previous methods.

DRRN B1U9 DRRN B1U25
37.66/0.9589
33.93/0.9234
31.58/0.8864
33.19/0.9133
29.94/0.8339
28.18/0.7701
32.01/0.8969
28.91/0.7992
27.35/0.7262
31.02/0.9164
27.38/0.8331
25.35/0.7576

VDSR [13]
37.53/0.9587
33.66/0.9213
31.35/0.8838
33.03/0.9124
29.77/0.8314
28.01/0.7674
31.90/0.8960
28.82/0.7976
27.29/0.7251
30.76/0.9140
27.14/0.8279
25.18/0.7524

DRCN [14]
37.63/0.9588
33.82/0.9226
31.53/0.8854
33.04/0.9118
29.76/0.8311
28.02/0.7670
31.85/0.8942
28.80/0.7963
27.23/0.7233
30.75/0.9133
27.15/0.8276
25.14/0.7510

SelfEx [10]
36.49/0.9537
32.58/0.9093
30.31/0.8619
32.22/0.9034
29.16/0.8196
27.40/0.7518
31.18/0.8855
28.29/0.7840
26.84/0.7106
29.54/0.8967
26.44/0.8088
24.79/0.7374

SRCNN [2]
36.66/0.9542
32.75/0.9090
30.48/0.8628
32.45/0.9067
29.30/0.8215
27.50/0.7513
31.36/0.8879
28.41/0.7863
26.90/0.7101
29.50/0.8946
26.24/0.7989
24.52/0.7221

37.74/0.9591
34.03/0.9244
31.68/0.8888
33.23/0.9136
29.96/0.8349
28.21/0.7720
32.05/0.8973
28.95/0.8004
27.38/0.7284
31.23/0.9188
27.53/0.8378
25.44/0.7638

33.66/0.9299
30.39/0.8682
28.42/0.8104
30.24/0.8688
27.55/0.7742
26.00/0.7027
29.56/0.8431
27.21/0.7385
25.96/0.6675
26.88/0.8403
24.46/0.7349
23.14/0.6577

Urban100

BSD100

more

params

less

Figure 6. PSNR of various DRRNs at B and U combinations. The
color of the point indicates the PSNR that corresponds to the bar
on the right and 4 depth contours (d = 50, 30, 20, 10) are also
plotted. The tests are conducted for scale factor ×3 on Set5.

depths, and see how the two parameters affect the perfor-
mance. In Fig. 6, we build a grid of B and U, and sample
several points in the grid with the depth ranging from 8 to 52
layers. The parameter number keeps the same when more
residual units are used in one recursive block, and linearly
increases when more recursive blocks are stacked.

First, to clearly show how a single parameter affects
DRRN, we ﬁx one parameter to 3 and change the other
from 1 to 4. Fig. 6 shows that increasing B or U results
in deeper models and achieves better performance, which
indicates deeper is still better. Despite different structures,
these models are comparable as long as their depths are sim-
ilar, e.g., B2U3 (d = 15, k = 784K) and B3U2 (d = 16,
k = 1, 182K) achieve 33.76 and 33.77 dB, respectively.

The structures mentioned above all use the recursive
learning strategy. Next, we test three very different struc-
tures to demonstrate the effectiveness of such a strategy.
Speciﬁcally, we ﬁx one parameter to 1 and change the other
to construct networks with d = 52. This results in two ex-
treme structures: B1U25 (k = 297K) and B17U1 (k =
7, 375K). For B1U25, only one recursive block is used, in
which 25 residual units are recursively learned. For B17U1,
17 recursive blocks are stacked, with no recursive learn-

ing. We also construct a normal structure B3U8 (d = 52,
k = 1, 182K). Fig. 6 shows that despite different struc-
tures, the three networks achieve comparable performance
(B17U1 34.03 dB, B3U8 34.04 dB and B1U25 34.03 dB)
and outperform the pervious shallow networks. Thanks to
the recursive learning strategy, B1U25 can achieve state-of-
the-art results using far fewer parameters.

4.4. Comparison with State of the Art Models

We now provide quantitative and qualitative compar-
isons. Considering both the performance and number of
parameters, we choose DRRN B1U25 (d = 52, k = 297K)
as our best model. For fair comparison, we also construct
a DRRN B1U9 (d = 20, k = 297K) structure, which has
the same depth as VDSR and DRCN, but fewer parameters.
Both the DL [2, 13, 14] and non-DL [10, 20, 23] methods in
recent years are used for benchmark. Experimental setting
is kept the same as those previous methods. Speciﬁcally, we
ﬁrst apply bicubic interpolation to the color components of
an image and all models are applied to its luminance com-
ponent only. Therefore, the input and output images are of
the same size. For fair comparison, similar to [2, 13, 14, 23],
we crop pixels near image boundary before evaluation, al-
though this is unnecessary for DRRN.

Tab. 2 summarizes quantitative results on the four test-
ing sets, by citing the results of prior methods from [13,14].
The two DRRN models outperforms all existing methods in
all datasets and scale factors, in both PSNR and Structural
SIMilarity (SSIM) 1. Especially on the recent difﬁcult Ur-
ban100 dataset [10], DRRN signiﬁcantly advances the state
of the art, with the improvement margin of 0.47, 0.38, and
0.26 dB on scale factor ×2, ×3 and ×4 respectively.

Further, we also use another metric: Information Fidelity
Criterion (IFC) [24] for comparison, which claims to have
the highest correlation with perceptual scores for SR evalu-
ation [34]. The results are presented in Tab. 3. Note that

1With two convolutional layers in the residual branch, DRRN achieves
state-of-the-art performance. More complex designs have the potential to
improve performance but are not the focus of this work.

3152

Dataset

Set5

Set14

Urban100

Scale
×2
×3
×4
×2
×3
×4
×2
×3
×4

Bicubic
6.083
3.580
2.329
6.105
3.473
2.237
6.245
3.620
2.361

SRCNN [2]
8.036
4.658
2.991
7.784
4.338
2.751
7.989
4.584
2.963

SelfEx [10]
7.811
4.748
3.166
7.591
4.371
2.893
7.937
4.843
3.314

RFL [23]
8.556
4.926
3.191
8.175
4.531
2.919
8.450
4.801
3.110

PSyCo [20]
8.642
5.083
3.379
8.280
4.660
3.055
8.589
5.031
3.351

VDSR [13]
8.569
5.221
3.547
8.178
4.730
3.133
8.645
5.194
3.496

DRRN B1U9
8.583
5.241
3.581
8.181
4.732
3.147
8.653
5.259
3.536

DRRN B1U25
8.671
5.397
3.703
8.320
4.878
3.252
8.917
5.456
3.676

Table 3. Benchmark results. Average IFCs for the scale factor ×2, ×3 and ×4 on datasets Set5, Set14 and Urban100. Red color indicates
the best performance of our methods and blue color indicates the best performance of previous methods.

Ground Truth

Bicubic

SRCNN

SelfEx

VDSR

DRRN_B1U9

DRRN_B1U25

(PSNR/SSIM/IFC)

(21.10/0.7046/3.134) (21.77/0.7540/3.761) (21.94/0.7608/3.669) (22.58/0.7942/4.341) (22.74/0.7999/4.365) (23.37/0.8158/4.713)

(PSNR/SSIM/IFC)

(22.55/0.7073/3.591) (23.51/0.7608/4.344) (23.42/0.7587/4.281) (23.99/0.7728/4.716) (24.23/0.7781/4.734) (24.41/0.7805/4.914)

(PSNR/SSIM/IFC)

(21.98/0.8126/1.920) (24.80/0.8928/2.666) (24.85/0.9076/2.941) (25.85/0.9289/3.406) (26.33/0.9365/3.557) (26.48/0.9415/3.822)

Figure 7. Qualitative comparison. (1) The ﬁrst row shows image “img059” (Urban100 with scale factor ×3). DRRN recovers sharp lines,
while others all give blurry results. (2) The second row shows image “253027” (BSD100 with scale factor ×3). DRRN accurately recovers
the pattern. (3) The last row shows image “ppt3” (Set14 with scale factor ×4). Texts in DRRN are sharp, while others are blurry.

the results of [2, 10, 20, 23] are cited from [20] 2, while
the results of VDSR come from our re-implementation.
Similar to DRRN, the VDSR re-implementation also uses
BN and ReLU as the activation functions, unlike the orig-
inal VDSR [13] that does not use BN. These results are

2Since PSyCo [20] does not present complete PSNR/SSIM perfor-

mance on the four benchmarks, we do not include it in Tab. 2.

faithful since our VDSR re-implementation achieves similar
benchmark performance as [13] reported in Tab. 2. Since
only Set5, Set14 and Urban100 are used in [20], we omit
BSD100 in this test. It is clear that DRRN still outperforms
all existing methods in all datasets and scale factors. Re-
garding the speed, our 20-layer B1U9 network takes 0.25
second to process a 288 × 288 image on a Titan X GPU.

3153

16 layers

22 layers

28 layers

Shallow

Deep

Very deep

Figure 8. PSNR for scale factor ×3 on Set5 using VDSR (blue),
DRRN NS (green), DRRN C (cyan) and DRRN (red).

Methods
Loc. Res. L
Recu. L
Multi-path
PSNR

×
×
×
33.86

VDSR

DRRN NS C

DRRN NS

DRRN C

DRRN

√

×
×
33.92

√

×
√
33.97

√
√

×
33.95

√
√
√
33.99

Table 4. Average PSNR when different DRRN components are
turned on or off, for scale factor ×3 on dataset Set5.

among

Qualitative

comparisons

SRCNN [2],
SelfEx [10], VDSR [13] and DRRN are illustrated in
Fig. 7. For SRCNN and SelfEx, we use their public codes.
For VDSR, we use our re-implementation. As we can see,
our method produces relatively sharper edges with respect
to patterns, while other methods may give blurry results.

4.5. Discussions

Since global residual learning has been well discussed
in [13], in this section, we mainly focus on local residual
learning (LRL), recursive learning and multi-path structure.
Local Residual Learning To demonstrate the effective-
ness of LRL, DRRN is compared with VDSR [13], which
has no LRL. For fair comparison, the depth and number of
parameters are kept the same for both methods. Speciﬁ-
cally, we evaluate three depths: 16(B3U2), 22(B3U3), and
28(B3U4) convolutional layers. Each convolutional layer
has 128 ﬁlters with the size 3 × 3. To keep the parameter
number the same, in this test we do not share the weight
set of the residual units in one recursive block, and denote
this DRRN structure as DRRN NS. Fig. 8 shows the PSNR
of both methods in different depths. We see that the LRL
strategy consistently improves VDSR at all depths.
Recursive Learning To contrast our recursive learning
strategy, the three DRRN NS versions are compared with
the three weight-shared versions (Fig. 8). Storage is an im-
portant factor to consider when building a deep model. The
recursive learning strategy can reduce the storage demand
and keep a concise model while increasing its depth. Inter-
estingly the weight-shared DRRN versions achieve compa-
rable or even better performance than DRRN NS versions

Figure 9. Comparing deep and shallow models proposed in recent
three years that report PSNR for scale factor ×3 on Set5 and Set14.

while using only a small fraction of parameters, which in-
dicates when limited training set (e.g., 291 images) is used,
the recursive learning is indeed effective under the same
structure, and less prone to overﬁtting [16].
Multi-Path Structure To demonstrate the effectiveness
of multi-path structure, we compare DRRN with the chain
structure, denoted as DRRN C. As shown in Fig. 8, with the
same depth and parameter number, the multi-path structures
achieve higher PSNR than the corresponding chain struc-
tures in all three cases. Further, Tab. 4 presents the compre-
hensive study on performance gains using network B3U4 as
the example. It shows how different technical parts improve
the performance compared to the baseline VDSR.
Deep vs. Shallow Finally, we give a comparison of the deep
and shallow SISR models, which are published in recent
three years (2014 to 2016) that report PSNR for scale factor
×3 on datasets Set5 and Set14. Shallow (non-DL) mod-
els include A+ [31], SelfEx [10], RFL [23], NBSRF [22],
PSyCo [20] and IA [30]. The deep models (d ≤ 8) in-
clude SRCNN [2], DJSR [33], CSCN [32], ESPCN [25]
and FSRCNN [3]. Very deep models (d ≥ 20) include
VDSR [13], DRCN [14], RED [17] and DRRN with d = 20
and 52. Fig. 9 shows that 1) very deep models signif-
icantly outperform the shallow models; 2) DRRN B1U9
(d = 20, k = 297K) already outperforms the state of the
arts with the same depth but fewer parameters; 3) a deeper
DRRN B1U25 (d = 52, k = 297K) further improves the
performance without adding any parameters.

5. Conclusions

In this paper, we propose Deep Recursive Residual Net-
work (DRRN) for single image super-resolution. In DRRN,
an enhanced residual unit structure is recursively learned in
a recursive block, and we stack several recursive blocks to
learn the residual image between the HR and LR images.
The residual image is then added to the input LR image
from a global identity branch to estimate the HR image.
Extensive benchmark experiments and analysis show that
DRRN is a deep, concise, and superior model for SISR.

3154

References

[1] C. M. Bevilacqua, A. Roumy, and M.-L. A. Morel. Low-
complexity single-image super-resolution based on nonneg-
ative neighbor embedding. In BMVC, 2012. 1, 5

[2] C. Dong, C. Loy, K. He, and X. Tang. Image super-resolution
IEEE Transactions on
using deep convolutional networks.
Pattern Analysis and Machine Intelligence, 38(2):295–307,
2016. 1, 2, 3, 6, 7, 8

[3] C. Dong, C. Loy, and X. Tang. Accelerating the super-
In ECCV, 2016.

resolution convolutional neural network.
8

[4] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a

single image. In ICCV, 2009. 1

[5] K. Gregor and Y. LeCun. Learning fast approximations of

sparse coding. In ICML, 2010. 1

[6] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In ICLR, 2016. 2

[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rec-
tiﬁers: Surpassing human-level performance on ImageNet
classiﬁcation. In ICCV, 2015. 5

[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 2, 3, 4

[9] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. arXiv:1603.05027v2, 2016. 4
[10] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-
resolution from transformed self-exemplars. In CVPR, 2015.
5, 6, 7, 8

[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, 2015. 2, 4

[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. arXiv:1408.5093,
2014. 5

[13] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In CVPR,
2016. 1, 2, 3, 5, 6, 7, 8

[14] J. Kim, J. K. Lee, and K. M. Lee. Deeply-recursive convolu-
tional network for image super-resolution. In CVPR, 2016.
1, 2, 3, 4, 6, 8

[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. In Proceed-
ings of the IEEE, 1998. 5

[16] M. Liang and X. Hu. Recurrent convolutional neural network

for object recognition. In CVPR, 2015. 4, 8

[17] X.-J. Mao, C. Shen, and Y.-B. Yang. Image restoration us-
ing very deep convolutional encoder-decoder networks with
symmetric skip connections. In NIPS, 2016. 1, 2, 8

[18] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-
cal statistics. In ICCV, 2001. 5

[19] V. Nair and G. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010. 2, 4

[20] E. Perez-Pellitero,

J. Ruiz-Hidalgo, and
B. Rosenhahn. PSyCo: Manifold span reduction for super
resolution. In CVPR, 2016. 1, 6, 7, 8

J. Salvador,

[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
and et al. ImageNet large scale visual recognition challenge.
arXiv:1409.0575, 2014. 2

[22] J. Salvador and E. Perez-Pellitero. Naive bayes super-

resolution forest. In ICCV, 2015. 1, 8

[23] S. Schulter, C. Leistner, and H. Bischof. Fast and accu-
rate image upscaling with super-resolution forests. In CVPR,
2015. 1, 5, 6, 7, 8

[24] H. Sheikh, A. Bovik, and G. de Veciana. An information
ﬁdelity criterion for image quality assessment using natu-
ral scene statistics. IEEE Transactions on image processing,
14(12):2117–2128, 2005. 6

[25] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In CVPR, 2016. 1, 2, 8
[26] W. Shi, J. Caballero, C. Ledig, X. Zhuang, W. Bai, K. Bha-
tia, A. Marvao, T. Dawes, D. ORegan, and D. Rueckert. Car-
diac image super-resolution with global correspondence us-
ing multi-atlas patchmatch. In MICCAI, 2013. 1

[27] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
2

[28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, and S. Reed. Going

deeper with convolutions. In CVPR, 2015. 2

[29] M. W. Thornton, P. M. Atkinson, and D. a. Holland. Sub-
pixel mapping of rural land cover objects from ﬁne spa-
tial resolution satellite sensor imagery using super-resolution
International Journal of Remote Sensing,
pixel-swapping.
27(3):473–491, 2006. 1

[30] R. Timofte, R. Rothe, and L. V. Gool. Seven ways to improve
In CVPR,

example-based single image super resolution.
2016. 5, 8

[31] R. Timofte, V. D. Smet, and L. V. Gool. A+: Adjusted an-
chored neighborhood regression for fast super-resolution. In
ACCV, 2014. 1, 8

[32] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep
networks for image super-resolution with sparse prior.
In
ICCV, 2015. 1, 2, 8

[33] Z. Wang, Y. Yang, Z. Wang, S. Chang, W. Han, J. Yang,
and T. Huang. Self-tuned deep super resolution. In CVPR
workshop, 2015. 8

[34] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-

resolution: A benchmark. In ECCV, 2014. 6

[35] J. Yang, J. Wright, T. Huang, and Y. Ma. Image superresolu-
tion via sparse representation. IEEE Transactions on image
processing, 19(11):2861–2873, 2010. 1, 5

[36] R. Zeyde, M. Elad, and M. Protter. On single image scale-
up using sparse-representations. Curves and Surfaces, pages
711–730, 2012. 5

[37] W. Zou and P. C. Yuen. Very low resolution face recog-
IEEE Transactions on image processing,

nition problem.
21(1):327–340, 2012. 1

3155

