7
1
0
2
 
p
e
S
 
7
 
 
]

G
L
.
s
c
[
 
 
5
v
5
1
5
2
0
.
6
0
7
1
:
v
i
X
r
a

Self-Normalizing Neural Networks

Günter Klambauer

Thomas Unterthiner

Andreas Mayr

Sepp Hochreiter
LIT AI Lab & Institute of Bioinformatics,
Johannes Kepler University Linz
A-4040 Linz, Austria
{klambauer,unterthiner,mayr,hochreit}@bioinf.jku.at

Abstract

Deep Learning has revolutionized vision via convolutional neural networks (CNNs)
and natural language processing via recurrent neural networks (RNNs). However,
success stories of Deep Learning with standard feed-forward neural networks
(FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot
exploit many levels of abstract representations. We introduce self-normalizing
neural networks (SNNs) to enable high-level abstract representations. While
batch normalization requires explicit normalization, neuron activations of SNNs
automatically converge towards zero mean and unit variance. The activation
function of SNNs are “scaled exponential linear units” (SELUs), which induce
self-normalizing properties. Using the Banach ﬁxed-point theorem, we prove that
activations close to zero mean and unit variance that are propagated through many
network layers will converge towards zero mean and unit variance — even under
the presence of noise and perturbations. This convergence property of SNNs allows
to (1) train deep networks with many layers, (2) employ strong regularization
schemes, and (3) to make learning highly robust. Furthermore, for activations
not close to unit variance, we prove an upper and lower bound on the variance,
thus, vanishing and exploding gradients are impossible. We compared SNNs on
(a) 121 tasks from the UCI machine learning repository, on (b) drug discovery
benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine
learning methods such as random forests and support vector machines. For FNNs
we considered (i) ReLU networks without normalization, (ii) batch normalization,
(iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi)
residual networks. SNNs signiﬁcantly outperformed all competing FNN methods
at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and
set a new record at an astronomy data set. The winning SNN architectures are often
very deep. Implementations are available at: github.com/bioinf-jku/SNNs.

Accepted for publication at NIPS 2017; please cite as:
Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017).
Self-Normalizing Neural Networks.
Processing Systems (NIPS).

In Advances in Neural Information

Introduction

Deep Learning has set new records at different benchmarks and led to various commercial applications
[25, 33]. Recurrent neural networks (RNNs) [18] achieved new levels at speech and natural language

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

processing, for example at the TIMIT benchmark [12] or at language translation [36], and are already
employed in mobile devices [31]. RNNs have won handwriting recognition challenges (Chinese and
Arabic handwriting) [33, 13, 6] and Kaggle challenges, such as the “Grasp-and Lift EEG” competition.
Their counterparts, convolutional neural networks (CNNs) [24] excel at vision and video tasks. CNNs
are on par with human dermatologists at the visual detection of skin cancer [9]. The visual processing
for self-driving cars is based on CNNs [19], as is the visual input to AlphaGo which has beaten one
of the best human GO players [34]. At vision challenges, CNNs are constantly winning, for example
at the large ImageNet competition [23, 16], but also almost all Kaggle vision challenges, such as the
“Diabetic Retinopathy” and the “Right Whale” challenges [8, 14].

However, looking at Kaggle challenges that are not related to vision or sequential tasks, gradient
boosting, random forests, or support vector machines (SVMs) are winning most of the competitions.
Deep Learning is notably absent, and for the few cases where FNNs won, they are shallow. For
example, the HIGGS challenge, the Merck Molecular Activity challenge, and the Tox21 Data
challenge were all won by FNNs with at most four hidden layers. Surprisingly, it is hard to ﬁnd
success stories with FNNs that have many hidden layers, though they would allow for different levels
of abstract representations of the input [3].

To robustly train very deep CNNs, batch normalization evolved into a standard to normalize neuron
activations to zero mean and unit variance [20]. Layer normalization [2] also ensures zero mean
and unit variance, while weight normalization [32] ensures zero mean and unit variance if in the
previous layer the activations have zero mean and unit variance. However, training with normalization
techniques is perturbed by stochastic gradient descent (SGD), stochastic regularization (like dropout),
and the estimation of the normalization parameters. Both RNNs and CNNs can stabilize learning
via weight sharing, therefore they are less prone to these perturbations. In contrast, FNNs trained
with normalization techniques suffer from these perturbations and have high variance in the training
error (see Figure 1). This high variance hinders learning and slows it down. Furthermore, strong
regularization, such as dropout, is not possible as it would further increase the variance which in turn
would lead to divergence of the learning process. We believe that this sensitivity to perturbations is
the reason that FNNs are less successful than RNNs and CNNs.

Self-normalizing neural networks (SNNs) are robust to perturbations and do not have high variance
in their training errors (see Figure 1). SNNs push neuron activations to zero mean and unit variance
thereby leading to the same effect as batch normalization, which enables to robustly learn many
layers. SNNs are based on scaled exponential linear units “SELUs” which induce self-normalizing
properties like variance stabilization which in turn avoids exploding and vanishing gradients.

Self-normalizing Neural Networks (SNNs)

Normalization and SNNs. For a neural network with activation function f , we consider two
consecutive layers that are connected by a weight matrix W . Since the input to a neural network
is a random variable, the activations x in the lower layer, the network inputs z = W x, and the
activations y = f (z) in the higher layer are random variables as well. We assume that all activations
xi of the lower layer have mean µ := E(xi) and variance ν := Var(xi). An activation y in the higher
layer has mean ˜µ := E(y) and variance ˜ν := Var(y). Here E(.) denotes the expectation and Var(.)
the variance of a random variable. A single activation y = f (z) has net input z = wT x. For n units
with activation xi, 1 (cid:54) i (cid:54) n in the lower layer, we deﬁne n times the mean of the weight vector
w ∈ Rn as ω := (cid:80)n
We consider the mapping g that maps mean and variance of the activations from one layer to mean
and variance of the activations in the next layer

i=1 wi and n times the second moment as τ := (cid:80)n

i=1 w2
i .

(cid:19)

(cid:18)µ
ν

(cid:55)→

(cid:19)

(cid:18)˜µ
˜ν

:

(cid:19)

(cid:18)˜µ
˜ν

= g

(cid:19)

(cid:18)µ
ν

.

(1)

Normalization techniques like batch, layer, or weight normalization ensure a mapping g that keeps
(µ, ν) and (˜µ, ˜ν) close to predeﬁned values, typically (0, 1).
Deﬁnition 1 (Self-normalizing neural net). A neural network is self-normalizing if it possesses a
mapping g : Ω (cid:55)→ Ω for each activation y that maps mean and variance from one layer to the next

2

Figure 1: The left panel and the right panel show the training error (y-axis) for feed-forward neural
networks (FNNs) with batch normalization (BatchNorm) and self-normalizing networks (SNN) across
update steps (x-axis) on the MNIST dataset the CIFAR10 dataset, respectively. We tested networks
with 8, 16, and 32 layers and learning rate 1e-5. FNNs with batch normalization exhibit high variance
due to perturbations. In contrast, SNNs do not suffer from high variance as they are more robust to
perturbations and learn faster.

and has a stable and attracting ﬁxed point depending on (ω, τ ) in Ω. Furthermore, the mean and
the variance remain in the domain Ω, that is g(Ω) ⊆ Ω, where Ω = {(µ, ν) | µ ∈ [µmin, µmax], ν ∈
[νmin, νmax]}. When iteratively applying the mapping g, each point within Ω converges to this ﬁxed
point.

Therefore, we consider activations of a neural network to be normalized, if both their mean and their
variance across samples are within predeﬁned intervals. If mean and variance of x are already within
these intervals, then also mean and variance of y remain in these intervals, i.e., the normalization is
transitive across layers. Within these intervals, the mean and variance both converge to a ﬁxed point
if the mapping g is applied iteratively.

Therefore, SNNs keep normalization of activations when propagating them through layers of the
network. The normalization effect is observed across layers of a network: in each layer the activations
are getting closer to the ﬁxed point. The normalization effect can also observed be for two ﬁxed
layers across learning steps: perturbations of lower layer activations or weights are damped in the
higher layer by drawing the activations towards the ﬁxed point. If for all y in the higher layer, ω and
τ of the corresponding weight vector are the same, then the ﬁxed points are also the same. In this
case we have a unique ﬁxed point for all activations y. Otherwise, in the more general case, ω and
τ differ for different y but the mean activations are drawn into [µmin, µmax] and the variances are
drawn into [νmin, νmax].

Constructing Self-Normalizing Neural Networks. We aim at constructing self-normalizing neu-
ral networks by adjusting the properties of the function g. Only two design choices are available for
the function g: (1) the activation function and (2) the initialization of the weights.

For the activation function, we propose “scaled exponential linear units” (SELUs) to render a FNN as
self-normalizing. The SELU activation function is given by

selu(x) = λ

(cid:26)x

if x > 0
αex − α if x (cid:54) 0

.

(2)

SELUs allow to construct a mapping g with properties that lead to SNNs. SNNs cannot be derived
with (scaled) rectiﬁed linear units (ReLUs), sigmoid units, tanh units, and leaky ReLUs. The
activation function is required to have (1) negative and positive values for controlling the mean, (2)
saturation regions (derivatives approaching zero) to dampen the variance if it is too large in the lower
layer, (3) a slope larger than one to increase the variance if it is too small in the lower layer, (4) a
continuous curve. The latter ensures a ﬁxed point, where variance damping is equalized by variance
increasing. We met these properties of the activation function by multiplying the exponential linear
unit (ELU) [7] with λ > 1 to ensure a slope larger than one for positive net inputs.

3

For the weight initialization, we propose ω = 0 and τ = 1 for all units in the higher layer. The
next paragraphs will show the advantages of this initialization. Of course, during learning these
assumptions on the weight vector will be violated. However, we can prove the self-normalizing
property even for weight vectors that are not normalized, therefore, the self-normalizing property can
be kept during learning and weight changes.

i=1 wi E(xi) = µ ω and Var(z) = Var((cid:80)n

Deriving the Mean and Variance Mapping Function g. We assume that the xi are independent
from each other but share the same mean µ and variance ν. Of course, the independence assumptions
is not fulﬁlled in general. We will elaborate on the independence assumption below. The network
input z in the higher layer is z = wT x for which we can infer the following moments E(z) =
(cid:80)n
i=1 wi xi) = ν τ , where we used the independence
of the xi. The net input z is a weighted sum of independent, but not necessarily identically distributed
variables xi, for which the central limit theorem (CLT) states that z approaches a normal distribution:
ντ ). According to the CLT, the larger n, the closer is z
z ∼ N (µω,
to a normal distribution. For Deep Learning, broad layers with hundreds of neurons xi are common.
Therefore the assumption that z is normally distributed is met well for most currently used neural
networks (see Figure A8). The function g maps the mean and variance of activations in the lower
layer to the mean ˜µ = E(y) and variance ˜ν = Var(y) of the activations y in the next layer:

ντ ) with density pN(z; µω,

√

√

(cid:19)

(cid:18)µ
ν

(cid:19)

(cid:18)˜µ
˜ν

:

(cid:55)→

g :

˜µ(µ, ω, ν, τ ) =

selu(z) pN(z; µω,

ντ ) dz

(3)

√

√

˜ν(µ, ω, ν, τ ) =

selu(z)2 pN(z; µω,

ντ ) dz − (˜µ)2 .

These integrals can be analytically computed and lead to following mappings of the moments:

˜µ =

λ

(µω) erf

(cid:18)

1
2

(cid:18) µω
√
√
2

(cid:19)

+

ντ
(cid:19)

√

(cid:18) µω + ντ
√
ντ
2
(cid:18)

α eµω+ ντ

2 erfc

− α erfc

˜ν =

(cid:18)

1
2

λ2

√

ντ e− (µω)2

2(ντ ) + µω

(cid:33)

(cid:19)

+

ντ
(cid:19)(cid:19)

(cid:114) 2
π
(cid:18)

(cid:0)(µω)2 + ντ (cid:1)

2 − erfc

+ α2

−2eµω+ ντ

2 erfc

+e2(µω+ντ ) erfc

(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

+ erfc

(cid:19)(cid:19)

(cid:114) 2
π

+

(µω)

ντ

(cid:19)

√

(cid:18) µω + ντ
√
2
ντ
(cid:33)
ντ e− (µω)2

2(ντ )

√

− (˜µ)2

(4)

(5)

(cid:90) ∞

−∞
(cid:90) ∞

−∞

(cid:18) µω
√
√
2
(cid:18) µω
√
√
2
ντ
(cid:18) µω
√
√
2

Stable and Attracting Fixed Point (0, 1) for Normalized Weights. We assume a normalized
weight vector w with ω = 0 and τ = 1. Given a ﬁxed point (µ, ν), we can solve equations Eq. (4)
and Eq. (5) for α and λ. We chose the ﬁxed point (µ, ν) = (0, 1), which is typical for activation
normalization. We obtain the ﬁxed point equations ˜µ = µ = 0 and ˜ν = ν = 1 that we solve for α
and λ and obtain the solutions α01 ≈ 1.6733 and λ01 ≈ 1.0507, where the subscript 01 indicates
that these are the parameters for ﬁxed point (0, 1). The analytical expressions for α01 and λ01 are
given in Eq. (14). We are interested whether the ﬁxed point (µ, ν) = (0, 1) is stable and attracting. If
the Jacobian of g has a norm smaller than 1 at the ﬁxed point, then g is a contraction mapping and the
ﬁxed point is stable. The (2x2)-Jacobian J (µ, ν) of g : (µ, ν) (cid:55)→ (˜µ, ˜ν) evaluated at the ﬁxed point
(0, 1) with α01 and λ01 is

J (µ, ν) =






∂ µnew(µ,ν)
∂µ

∂ µnew(µ,ν)
∂ν

∂ νnew(µ,ν)
∂µ

∂ νnew(µ,ν)
∂ν




 ,

J (0, 1) =

(cid:18)0.0
0.0

0.088834
0.782648

(cid:19)

.

(6)

The spectral norm of J (0, 1) (its largest singular value) is 0.7877 < 1. That means g is a contraction
mapping around the ﬁxed point (0, 1) (the mapping is depicted in Figure 2). Therefore, (0, 1) is a
stable ﬁxed point of the mapping g.

4

Figure 2: For ω = 0 and τ = 1, the mapping g of mean µ (x-axis) and variance ν (y-axis) to the
next layer’s mean ˜µ and variance ˜ν is depicted. Arrows show in which direction (µ, ν) is mapped by
g : (µ, ν) (cid:55)→ (˜µ, ˜ν). The ﬁxed point of the mapping g is (0, 1).

Stable and Attracting Fixed Points for Unnormalized Weights. A normalized weight vector w
cannot be ensured during learning. For SELU parameters α = α01 and λ = λ01, we show in the next
theorem that if (ω, τ ) is close to (0, 1), then g still has an attracting and stable ﬁxed point that is close
to (0, 1). Thus, in the general case there still exists a stable ﬁxed point which, however, depends
on (ω, τ ). If we restrict (µ, ν, ω, τ ) to certain intervals, then we can show that (µ, ν) is mapped to
the respective intervals. Next we present the central theorem of this paper, from which follows that
SELU networks are self-normalizing under mild conditions on the weights.
Theorem 1 (Stable and Attracting Fixed Points). We assume α = α01 and λ = λ01. We restrict the
range of the variables to the following intervals µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and
τ ∈ [0.95, 1.1], that deﬁne the functions’ domain Ω. For ω = 0 and τ = 1, the mapping Eq. (3) has
the stable ﬁxed point (µ, ν) = (0, 1), whereas for other ω and τ the mapping Eq. (3) has a stable
and attracting ﬁxed point depending on (ω, τ ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and
ν ∈ [0.80009, 1.48617]. All points within the (µ, ν)-domain converge when iteratively applying the
mapping Eq. (3) to this ﬁxed point.

Proof. We provide a proof sketch (see detailed proof in Appendix Section A3). With the Banach
ﬁxed point theorem we show that there exists a unique attracting and stable ﬁxed point. To this end,
we have to prove that a) g is a contraction mapping and b) that the mapping stays in the domain, that
is, g(Ω) ⊆ Ω. The spectral norm of the Jacobian of g can be obtained via an explicit formula for the
largest singular value for a 2 × 2 matrix. g is a contraction mapping if its spectral norm is smaller
than 1. We perform a computer-assisted proof to evaluate the largest singular value on a ﬁne grid and
ensure the precision of the computer evaluation by an error propagation analysis of the implemented
algorithms on the according hardware. Singular values between grid points are upper bounded by the
mean value theorem. To this end, we bound the derivatives of the formula for the largest singular
value with respect to ω, τ, µ, ν. Then we apply the mean value theorem to pairs of points, where one
is on the grid and the other is off the grid. This shows that for all values of ω, τ, µ, ν in the domain Ω,
the spectral norm of g is smaller than one. Therefore, g is a contraction mapping on the domain Ω.
Finally, we show that the mapping g stays in the domain Ω by deriving bounds on ˜µ and ˜ν. Hence,
the Banach ﬁxed-point theorem holds and there exists a unique ﬁxed point in Ω that is attained.

Consequently, feed-forward neural networks with many units in each layer and with the SELU
activation function are self-normalizing (see deﬁnition 1), which readily follows from Theorem 1. To
give an intuition, the main property of SELUs is that they damp the variance for negative net inputs
and increase the variance for positive net inputs. The variance damping is stronger if net inputs are
further away from zero while the variance increase is stronger if net inputs are close to zero. Thus, for
large variance of the activations in the lower layer the damping effect is dominant and the variance
decreases in the higher layer. Vice versa, for small variance the variance increase is dominant and the
variance increases in the higher layer.

However, we cannot guarantee that mean and variance remain in the domain Ω. Therefore, we next
treat the case where (µ, ν) are outside Ω. It is especially crucial to consider ν because this variable
has much stronger inﬂuence than µ. Mapping ν across layers to a high value corresponds to an

5

exploding gradient, since the Jacobian of the activation of high layers with respect to activations
in lower layers has large singular values. Analogously, mapping ν across layers to a low value
corresponds to an vanishing gradient. Bounding the mapping of ν from above and below would avoid
both exploding and vanishing gradients. Theorem 2 states that the variance of neuron activations of
SNNs is bounded from above, and therefore ensures that SNNs learn robustly and do not suffer from
exploding gradients.
Theorem 2 (Decreasing ν). For λ = λ01, α = α01 and the domain Ω+: −1 (cid:54) µ (cid:54) 1, −0.1 (cid:54) ω (cid:54)
0.1, 3 (cid:54) ν (cid:54) 16, and 0.8 (cid:54) τ (cid:54) 1.25, we have for the mapping of the variance ˜ν(µ, ω, ν, τ, λ, α)
given in Eq. (5): ˜ν(µ, ω, ν, τ, λ01, α01) < ν.

The proof can be found in the Appendix Section A3. Thus, when mapped across many layers, the
variance in the interval [3, 16] is mapped to a value below 3. Consequently, all ﬁxed points (µ, ν)
of the mapping g (Eq. (3)) have ν < 3. Analogously, Theorem 3 states that the variance of neuron
activations of SNNs is bounded from below, and therefore ensures that SNNs do not suffer from
vanishing gradients.
Theorem 3 (Increasing ν). We consider λ = λ01, α = α01 and the domain Ω−: −0.1 (cid:54) µ (cid:54) 0.1,
and −0.1 (cid:54) ω (cid:54) 0.1. For the domain 0.02 (cid:54) ν (cid:54) 0.16 and 0.8 (cid:54) τ (cid:54) 1.25 as well as for the
domain 0.02 (cid:54) ν (cid:54) 0.24 and 0.9 (cid:54) τ (cid:54) 1.25, the mapping of the variance ˜ν(µ, ω, ν, τ, λ, α) given
in Eq. (5) increases: ˜ν(µ, ω, ν, τ, λ01, α01) > ν.

The proof can be found in the Appendix Section A3. All ﬁxed points (µ, ν) of the mapping g (Eq. (3))
ensure for 0.8 (cid:54) τ that ˜ν > 0.16 and for 0.9 (cid:54) τ that ˜ν > 0.24. Consequently, the variance mapping
Eq. (5) ensures a lower bound on the variance ν. Therefore SELU networks control the variance of
the activations and push it into an interval, whereafter the mean and variance move toward the ﬁxed
point. Thus, SELU networks are steadily normalizing the variance and subsequently normalizing the
mean, too. In all experiments, we observed that self-normalizing neural networks push the mean and
variance of activations into the domain Ω .

i=1 w2

i=1 wi = 0 and τ = (cid:80)n

Initialization. Since SNNs have a ﬁxed point at zero mean and unit variance for normalized weights
ω = (cid:80)n
i = 1 (see above), we initialize SNNs such that these constraints
are fulﬁlled in expectation. We draw the weights from a Gaussian distribution with E(wi) = 0 and
variance Var(wi) = 1/n. Uniform and truncated Gaussian distributions with these moments led to
networks with similar behavior. The “MSRA initialization” is similar since it uses zero mean and
variance 2/n to initialize the weights [17]. The additional factor 2 counters the effect of rectiﬁed
linear units.

New Dropout Technique. Standard dropout randomly sets an activation x to zero with probability
1 − q for 0 < q (cid:54) 1. In order to preserve the mean, the activations are scaled by 1/q during
training. If x has mean E(x) = µ and variance Var(x) = ν, and the dropout variable d follows a
binomial distribution B(1, q), then the mean E(1/qdx) = µ is kept. Dropout ﬁts well to rectiﬁed
linear units, since zero is in the low variance region and corresponds to the default value. For scaled
exponential linear units, the default and low variance value is limx→−∞ selu(x) = −λα = α(cid:48).
Therefore, we propose “alpha dropout”, that randomly sets inputs to α(cid:48). The new mean and new
variance is E(xd + α(cid:48)(1 − d)) = qµ + (1 − q)α(cid:48), and Var(xd + α(cid:48)(1 − d)) = q((1 − q)(α(cid:48) −
µ)2 + ν). We aim at keeping mean and variance to their original values after “alpha dropout”, in
order to ensure the self-normalizing property even for “alpha dropout”. The afﬁne transformation
a(xd + α(cid:48)(1 − d)) + b allows to determine parameters a and b such that mean and variance are
kept to their values: E(a(xd + α(cid:48)(1 − d)) + b) = µ and Var(a(xd + α(cid:48)(1 − d)) + b) = ν . In
contrast to dropout, a and b will depend on µ and ν, however our SNNs converge to activations with
zero mean and unit variance. With µ = 0 and ν = 1, we obtain a = (cid:0)q + α(cid:48)2q(1 − q)(cid:1)−1/2
and
b = − (cid:0)q + α(cid:48)2q(1 − q)(cid:1)−1/2
((1 − q)α(cid:48)). The parameters a and b only depend on the dropout rate
1 − q and the most negative activation α(cid:48). Empirically, we found that dropout rates 1 − q = 0.05 or
0.10 lead to models with good performance. “Alpha-dropout” ﬁts well to scaled exponential linear
units by randomly setting activations to the negative saturation value.

6

Applicability of the central limit theorem and independence assumption.
In the derivative of
the mapping (Eq. (3)), we used the central limit theorem (CLT) to approximate the network inputs
z = (cid:80)n
i=1 wixi with a normal distribution. We justiﬁed normality because network inputs represent
a weighted sum of the inputs xi, where for Deep Learning n is typically large. The Berry-Esseen
theorem states that the convergence rate to normality is n−1/2 [22]. In the classical version of the CLT,
the random variables have to be independent and identically distributed, which typically does not
hold for neural networks. However, the Lyapunov CLT does not require the variable to be identically
distributed anymore. Furthermore, even under weak dependence, sums of random variables converge
in distribution to a Gaussian distribution [5].

Experiments

We compare SNNs to other deep networks at different benchmarks. Hyperparameters such as number
of layers (blocks), neurons per layer, learning rate, and dropout rate, are adjusted by grid-search for
each dataset on a separate validation set (see Section A4). We compare the following FNN methods:

• “MSRAinit”: FNNs without normalization and with ReLU activations and “Microsoft

weight initialization” [17].

• “BatchNorm”: FNNs with batch normalization [20].
• “LayerNorm”: FNNs with layer normalization [2].
• “WeightNorm”: FNNs with weight normalization [32].
• “Highway”: Highway networks [35].
• “ResNet”: Residual networks [16] adapted to FNNs using residual blocks with 2 or 3 layers

with rectangular or diavolo shape.

• “SNNs”: Self normalizing networks with SELUs with α = α01 and λ = λ01 and the

proposed dropout technique and initialization strategy.

121 UCI Machine Learning Repository datasets. The benchmark comprises 121 classiﬁcation
datasets from the UCI Machine Learning repository [10] from diverse application areas, such as
physics, geology, or biology. The size of the datasets ranges between 10 and 130, 000 data points
and the number of features from 4 to 250. In abovementioned work [10], there were methodological
mistakes [37] which we avoided here. Each compared FNN method was optimized with respect to
its architecture and hyperparameters on a validation set that was then removed from the subsequent
analysis. The selected hyperparameters served to evaluate the methods in terms of accuracy on the
pre-deﬁned test sets (details on the hyperparameter selection are given in Section A4). The accuracies
are reported in the Table A11. We ranked the methods by their accuracy for each prediction task and
compared their average ranks. SNNs signiﬁcantly outperform all competing networks in pairwise
comparisons (paired Wilcoxon test across datasets) as reported in Table 1 (left panel).

We further included 17 machine learning methods representing diverse method groups [10] in the
comparison and the grouped the data sets into “small” and “large” data sets (for details see Section A4).
On 75 small datasets with less than 1000 data points, random forests and SVMs outperform SNNs
and other FNNs. On 46 larger datasets with at least 1000 data points, SNNs show the highest
performance followed by SVMs and random forests (see right panel of Table 1, for complete results
see Tables A12 and A12). Overall, SNNs have outperformed state of the art machine learning methods
on UCI datasets with more than 1,000 data points.

Typically, hyperparameter selection chose SNN architectures that were much deeper than the selected
architectures of other FNNs, with an average depth of 10.8 layers, compared to average depths of 6.0
for BatchNorm, 3.8 WeightNorm, 7.0 LayerNorm, 5.9 Highway, and 7.1 for MSRAinit networks. For
ResNet, the average number of blocks was 6.35. SNNs with many more than 4 layers often provide
the best predictive accuracies across all neural networks.

Drug discovery: The Tox21 challenge dataset. The Tox21 challenge dataset comprises about
12,000 chemical compounds whose twelve toxic effects have to be predicted based on their chemical

7

Table 1: Left: Comparison of seven FNNs on 121 UCI tasks. We consider the average rank difference
to rank 4, which is the average rank of seven methods with random predictions. The ﬁrst column gives
the method, the second the average rank difference, and the last the p-value of a paired Wilcoxon test
whether the difference to the best performing method is signiﬁcant. SNNs signiﬁcantly outperform
all other methods. Right: Comparison of 24 machine learning methods (ML) on the UCI datasets
with more than 1000 data points. The ﬁrst column gives the method, the second the average rank
difference to rank 12.5, and the last the p-value of a paired Wilcoxon test whether the difference
to the best performing method is signiﬁcant. Methods that were signiﬁcantly worse than the best
method are marked with “*”. The full tables can be found in Table A11, Table A12 and Table A13.
SNNs outperform all competing methods.

FNN method comparison

ML method comparison

Method

avg. rank diff.

p-value Method

avg. rank diff.

p-value

SNN
MSRAinit
LayerNorm
Highway
ResNet
WeightNorm
BatchNorm

-0.756
-0.240*
-0.198*
0.021*
0.273*
0.397*
0.504*

SNN
SVM
2.7e-02
1.5e-02 RandomForest
1.9e-03 MSRAinit
5.4e-04 LayerNorm
7.8e-07 Highway
3.5e-06

. . .

-6.7
-6.4
-5.9
-5.4*
-5.3
-4.6*
. . .

5.8e-01
2.1e-01
4.5e-03
7.1e-02
1.7e-03
. . .

structure. We used the validation sets of the challenge winners for hyperparameter selection (see
Section A4) and the challenge test set for performance comparison. We repeated the whole evaluation
procedure 5 times to obtain error bars. The results in terms of average AUC are given in Table 2.
In 2015, the challenge organized by the US NIH was won by an ensemble of shallow ReLU FNNs
which achieved an AUC of 0.846 [28]. Besides FNNs, this ensemble also contained random forests
and SVMs. Single SNNs came close with an AUC of 0.845±0.003. The best performing SNNs have
8 layers, compared to the runner-ups ReLU networks with layer normalization with 2 and 3 layers.
Also batchnorm and weightnorm networks, typically perform best with shallow networks of 2 to 4
layers (Table 2). The deeper the networks, the larger the difference in performance between SNNs
and other methods (see columns 5–8 of Table 2). The best performing method is an SNN with 8
layers.

Table 2: Comparison of FNNs at the Tox21 challenge dataset in terms of AUC. The rows represent
different methods and the columns different network depth and for ResNets the number of residual
blocks (“na”: 32 blocks were omitted due to computational constraints). The deeper the networks,
the more prominent is the advantage of SNNs. The best networks are SNNs with 8 layers.

#layers / #blocks
6

4

method

2

3

8

16

32

83.7 ± 0.3
SNN
Batchnorm
80.0 ± 0.5
WeightNorm 83.7 ± 0.8
84.3 ± 0.3
LayerNorm
83.3 ± 0.9
Highway
82.7 ± 0.4
MSRAinit
82.2 ± 1.1
ResNet

84.4 ± 0.5
79.8 ± 1.6
82.9 ± 0.8
84.3 ± 0.5
83.0 ± 0.5
81.6 ± 0.9
80.0 ± 2.0

84.2 ± 0.4
77.2 ± 1.1
82.2 ± 0.9
84.0 ± 0.2
82.6 ± 0.9
81.1 ± 1.7
80.5 ± 1.2

83.9 ± 0.5
77.0 ± 1.7
82.5 ± 0.6
82.5 ± 0.8
82.4 ± 0.8
80.6 ± 0.6
81.2 ± 0.7

84.5 ± 0.2
75.0 ± 0.9
81.9 ± 1.2
80.9 ± 1.8
80.3 ± 1.4
80.9 ± 1.1
81.8 ± 0.6

83.5 ± 0.5
73.7 ± 2.0
78.1 ± 1.3
78.7 ± 2.3
80.3 ± 2.4
80.2 ± 1.1
81.2 ± 0.6

82.5 ± 0.7
76.0 ± 1.1
56.6 ± 2.6
78.8 ± 0.8
79.6 ± 0.8
80.4 ± 1.9
na

Astronomy: Prediction of pulsars in the HTRU2 dataset. Since a decade, machine learning
methods have been used to identify pulsars in radio wave signals [27]. Recently, the High Time
Resolution Universe Survey (HTRU2) dataset has been released with 1,639 real pulsars and 16,259
spurious signals. Currently, the highest AUC value of a 10-fold cross-validation is 0.976 which has
been achieved by Naive Bayes classiﬁers followed by decision tree C4.5 with 0.949 and SVMs with
0.929. We used eight features constructed by the PulsarFeatureLab as used previously [27]. We
assessed the performance of FNNs using 10-fold nested cross-validation, where the hyperparameters
were selected in the inner loop on a validation set (for details on the hyperparameter selection see

8

Section A4). Table 3 reports the results in terms of AUC. SNNs outperform all other methods and
have pushed the state-of-the-art to an AUC of 0.98.

Table 3: Comparison of FNNs and reference methods at HTRU2 in terms of AUC. The ﬁrst, fourth
and seventh column give the method, the second, ﬁfth and eight column the AUC averaged over 10
cross-validation folds, and the third and sixth column the p-value of a paired Wilcoxon test of the
AUCs against the best performing method across the 10 folds. FNNs achieve better results than Naive
Bayes (NB), C4.5, and SVM. SNNs exhibit the best performance and set a new record.

FNN methods

method

AUC

p-value method

FNN methods

AUC

ref. methods
p-value method AUC

0.9803 ± 0.010
SNN
MSRAinit
0.9791 ± 0.010
WeightNorm 0.9786* ± 0.010
0.9766* ± 0.009
Highway

3.5e-01

2.4e-02

9.8e-03

LayerNorm 0.9762* ± 0.011
BatchNorm 0.9760 ± 0.013
0.9753* ± 0.010
ResNet

1.4e-02

6.5e-02

6.8e-03

NB
C4.5
SVM

0.976
0.946
0.929

Conclusion

We have introduced self-normalizing neural networks for which we have proved that neuron ac-
tivations are pushed towards zero mean and unit variance when propagated through the network.
Additionally, for activations not close to unit variance, we have proved an upper and lower bound
on the variance mapping. Consequently, SNNs do not face vanishing and exploding gradient prob-
lems. Therefore, SNNs work well for architectures with many layers, allowed us to introduce a
novel regularization scheme, and learn very robustly. On 121 UCI benchmark datasets, SNNs have
outperformed other FNNs with and without normalization techniques, such as batch, layer, and
weight normalization, or specialized architectures, such as Highway or Residual networks. SNNs
also yielded the best results on drug discovery and astronomy tasks. The best performing SNN
architectures are typically very deep in contrast to other FNNs.

Acknowledgments

This work was supported by IWT research grant IWT150865 (Exaptation), H2020 project grant
671555 (ExCAPE), grant IWT135122 (ChemBioBridge), Zalando SE with Research Agreement
01/2016, Audi.JKU Deep Learning Center, Audi Electronic Venture GmbH, and the NVIDIA
Corporation.

The references are provided in Section A7.

References

Appendix

Contents

A1 Background

A2 Theorems

A2.1 Theorem 1: Stable and Attracting Fixed Points Close to (0,1) . . . . . . . . . . . .

A2.2 Theorem 2: Decreasing Variance from Above . . . . . . . . . . . . . . . . . . . .

9

11

12

12

12

A2.3 Theorem 3: Increasing Variance from Below . . . . . . . . . . . . . . . . . . . . .

12

A3 Proofs of the Theorems

A3.1 Proof of Theorem 1 .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A3.2 Proof of Theorem 2 .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A3.3 Proof of Theorem 3 .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

.

.

.

A3.4 Lemmata and Other Tools Required for the Proofs . . . . . . . . . . . . . . . . . .

A3.4.1 Lemmata for prooﬁng Theorem 1 (part 1): Jacobian norm smaller than one

A3.4.2 Lemmata for prooﬁng Theorem 1 (part 2): Mapping within domain . . . .

A3.4.3 Lemmata for prooﬁng Theorem 2: The variance is contracting . . . . . . .

A3.4.4 Lemmata for prooﬁng Theorem 3: The variance is expanding . . . . . . .

A3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1.

. .

A3.4.6 Intermediate Lemmata and Proofs . . . . . . . . . . . . . . . . . . . . . .

37

A4 Additional information on experiments

A4.1 121 UCI Machine Learning Repository data sets: Hyperparameters . . . . . . . . .

A4.2 121 UCI Machine Learning Repository data sets: detailed results . . . . . . . . . .

A4.3 Tox21 challenge data set: Hyperparameters

. . . . . . . . . . . . . . . . . . . . .

A4.4 HTRU2 data set: Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . .

A5 Other ﬁxed points

A6 Bounds determined by numerical methods

13

13

14

18

19

19

29

29

32

33

84

85

87

92

95

97

97

98

100

100

102

A7 References

List of ﬁgures

List of tables

Brief index

This appendix is organized as follows: the ﬁrst section sets the background, deﬁnitions, and for-
mulations. The main theorems are presented in the next section. The following section is devoted
to the proofs of these theorems. The next section reports additional results and details on the per-
formed computational experiments, such as hyperparameter selection. The last section shows that our
theoretical bounds can be conﬁrmed by numerical methods as a sanity check.

The proof of theorem 1 is based on the Banach’s ﬁxed point theorem for which we require (1) a
contraction mapping, which is proved in Subsection A3.4.1 and (2) that the mapping stays within its
domain, which is proved in Subsection A3.4.2 For part (1), the proof relies on the main Lemma 12,
which is a computer-assisted proof, and can be found in Subsection A3.4.1. The validity of the
computer-assisted proof is shown in Subsection A3.4.5 by error analysis and the precision of the
functions’ implementation. The last Subsection A3.4.6 compiles various lemmata with intermediate
results that support the proofs of the main lemmata and theorems.

10

A1 Background

We consider a neural network with activation function f and two consecutive layers that are
connected by weight matrix W . Since samples that serve as input to the neural network are chosen
according to a distribution, the activations x in the lower layer, the network inputs z = W x, and
activations y = f (z) in the higher layer are all random variables. We assume that all units xi in
the lower layer have mean activation µ := E(xi) and variance of the activation ν := Var(xi) and
a unit y in the higher layer has mean activation ˜µ := E(y) and variance ˜ν := Var(y). Here E(.)
denotes the expectation and Var(.) the variance of a random variable. For activation of unit y, we
have net input z = wT x and the scaled exponential linear unit (SELU) activation y = selu(z),
with

selu(x) = λ

(cid:26)x

if x > 0
αex − α if x (cid:54) 0

.

For n units xi, 1 (cid:54) i (cid:54) n in the lower layer and the weight vector w ∈ Rn, we deﬁne n times the
mean by ω := (cid:80)n
We deﬁne a mapping g from mean µ and variance ν of one layer to the mean ˜µ and variance ˜ν in the
next layer:

i=1 wi and n times the second moment by τ := (cid:80)n

i=1 w2
i .

g : (µ, ν) (cid:55)→ (˜µ, ˜ν) .

For neural networks with scaled exponential linear units, the mean is of the activations in the next
layer computed according to

˜µ =

λα(exp(z) − 1)pGauss(z; µω,

ντ )dz +

λzpGauss(z; µω,

ντ )dz ,

(9)

and the second moment of the activations in the next layer is computed according to

√

√

(cid:90) ∞

0

(cid:90) ∞

0

√

√

(cid:90) 0

−∞

˜ξ =

(cid:90) 0

−∞

λ2α2(exp(z) − 1)2pGauss(z; µω,

ντ )dz +

λ2z2pGauss(z; µω,

ντ )dz . (10)

Therefore, the expressions ˜µ and ˜ν have the following form:

˜µ(µ, ω, ν, τ, λ, α) =

λ

−(α + µω) erfc

(cid:18)

1
2

αeµω+ ντ

2 erfc

(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

+

√

(cid:114) 2
π

(cid:19)

(cid:18) µω
√
√
2

ντ

+

(cid:33)

ντ e− µ2 ω2

2ντ + 2µω

˜ν(µ, ω, ν, τ, λ, α) = ˜ξ(µ, ω, ν, τ, λ, α) − (˜µ(µ, ω, ν, τ, λ, α))2
(cid:19)

(cid:18)

(cid:18)

˜ξ(µ, ω, ν, τ, λ, α) =

(cid:0)(µω)2 + ντ (cid:1)

erf

(cid:18)

α2

−2eµω+ ντ

2 erfc

(cid:19)

+ e2(µω+ντ ) erfc

erfc

(cid:18) µω
√
√
2

ντ

(cid:19)(cid:19)

+

(cid:33)

√

ντ e− (µω)2

2(ντ )

√

λ2

1
2
(cid:18) µω + ντ
√
2
ντ
(cid:114) 2
π

(µω)

+ 1

(cid:18) µω
√
√
2
ντ
(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)

(cid:19)

+

+

We solve equations Eq. 4 and Eq. 5 for ﬁxed points ˜µ = µ and ˜ν = ν. For a normalized weight vector
with ω = 0 and τ = 1 and the ﬁxed point (µ, ν) = (0, 1), we can solve equations Eq. 4 and Eq. 5
for α and λ. We denote the solutions to ﬁxed point (µ, ν) = (0, 1) by α01 and λ01.

(7)

(8)

(11)

(12)

(13)

(14)

(cid:113) 2
π

α01 = −

(cid:16) 1√

(cid:17)

erfc

(cid:18)

λ01 =

1 − erfc

exp (cid:0) 1
2
(cid:19) √

(cid:1) − 1
(cid:19) √

e

2π

2
(cid:18) 1
√
2

≈ 1.67326

11

(cid:32)

(cid:16)√

(cid:17)
2

2 erfc

e2 + π erfc

(cid:19)2

(cid:18) 1
√
2

e − 2(2 + π) erfc

e + π + 2

(cid:19) √

(cid:18) 1
√
2

(cid:33)−1/2

λ01 ≈ 1.0507 .

The parameters α01 and λ01 ensure

˜µ(0, 0, 1, 1, λ01, α01) = 0
˜ν(0, 0, 1, 1, λ01, α01) = 1

Since we focus on the ﬁxed point (µ, ν) = (0, 1), we assume throughout the analysis that α =
α01 and λ = λ01. We consider the functions ˜µ(µ, ω, ν, τ, λ01, α01), ˜ν(µ, ω, ν, τ, λ01, α01), and
˜ξ(µ, ω, ν, τ, λ01, α01) on the domain Ω = {(µ, ω, ν, τ ) | µ ∈ [µmin, µmax] = [−0.1, 0.1], ω ∈
[ωmin, ωmax] = [−0.1, 0.1], ν ∈ [νmin, νmax] = [0.8, 1.5], τ ∈ [τmin, τmax] = [0.95, 1.1]}.

Figure 2 visualizes the mapping g for ω = 0 and τ = 1 and α01 and λ01 at few pre-selected points. It
can be seen that (0, 1) is an attracting ﬁxed point of the mapping g.

A2 Theorems

A2.1 Theorem 1: Stable and Attracting Fixed Points Close to (0,1)

Theorem 1 shows that the mapping g deﬁned by Eq. (4) and Eq. (5) exhibits a stable and attracting
ﬁxed point close to zero mean and unit variance. Theorem 1 establishes the self-normalizing property
of self-normalizing neural networks (SNNs). The stable and attracting ﬁxed point leads to robust
learning through many layers.
Theorem 1 (Stable and Attracting Fixed Points). We assume α = α01 and λ = λ01. We restrict
the range of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈
[0.95, 1.1]. For ω = 0 and τ = 1, the mapping Eq. (4) and Eq. (5) has the stable ﬁxed point (µ, ν) =
(0, 1). For other ω and τ the mapping Eq. (4) and Eq. (5) has a stable and attracting ﬁxed point
depending on (ω, τ ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617].
All points within the (µ, ν)-domain converge when iteratively applying the mapping Eq. (4) and
Eq. (5) to this ﬁxed point.

A2.2 Theorem 2: Decreasing Variance from Above

The next Theorem 2 states that the variance of unit activations does not explode through consecutive
layers of self-normalizing networks. Even more, a large variance of unit activations decreases when
propagated through the network. In particular this ensures that exploding gradients will never be
observed. In contrast to the domain in previous subsection, in which ν ∈ [0.8, 1.5], we now consider
a domain in which the variance of the inputs is higher ν ∈ [3, 16] and even the range of the mean is
increased µ ∈ [−1, 1]. We denote this new domain with the symbol Ω++ to indicate that the variance
lies above the variance of the original domain Ω. In Ω++, we can show that the variance ˜ν in the
next layer is always smaller then the original variance ν. Concretely, this theorem states that:
Theorem 2 (Decreasing ν). For λ = λ01, α = α01 and the domain Ω++: −1 (cid:54) µ (cid:54) 1, −0.1 (cid:54)
ω (cid:54) 0.1, 3 (cid:54) ν (cid:54) 16, and 0.8 (cid:54) τ (cid:54) 1.25 we have for the mapping of the variance ˜ν(µ, ω, ν, τ, λ, α)
given in Eq. (5)

The variance decreases in [3, 16] and all ﬁxed points (µ, ν) of mapping Eq. (5) and Eq. (4) have
ν < 3.

˜ν(µ, ω, ν, τ, λ01, α01) < ν .

(15)

A2.3 Theorem 3: Increasing Variance from Below

The next Theorem 3 states that the variance of unit activations does not vanish through consecutive
layers of self-normalizing networks. Even more, a small variance of unit activations increases when

12

propagated through the network. In particular this ensures that vanishing gradients will never be
observed. In contrast to the ﬁrst domain, in which ν ∈ [0.8, 1.5], we now consider two domains Ω−
1
2 in which the variance of the inputs is lower 0.05 (cid:54) ν (cid:54) 0.16 and 0.05 (cid:54) ν (cid:54) 0.24, and even
and Ω−
the parameter τ is different 0.9 (cid:54) τ (cid:54) 1.25 to the original Ω. We denote this new domain with the
symbol Ω−
to indicate that the variance lies below the variance of the original domain Ω. In Ω−
1
i
and Ω−
2 , we can show that the variance ˜ν in the next layer is always larger then the original variance
ν, which means that the variance does not vanish through consecutive layers of self-normalizing
networks. Concretely, this theorem states that:
Theorem 3 (Increasing ν). We consider λ = λ01, α = α01 and the two domains Ω−
1 =
{(µ, ω, ν, τ ) | − 0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1, 0.05 (cid:54) ν (cid:54) 0.16, 0.8 (cid:54) τ (cid:54) 1.25} and
Ω−

2 = {(µ, ω, ν, τ ) | − 0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1, 0.05 (cid:54) ν (cid:54) 0.24, 0.9 (cid:54) τ (cid:54) 1.25}.

The mapping of the variance ˜ν(µ, ω, ν, τ, λ, α) given in Eq. (5) increases

˜ν(µ, ω, ν, τ, λ01, α01) > ν

(16)

1 and Ω−

2 . All ﬁxed points (µ, ν) of mapping Eq. (5) and Eq. (4) ensure for 0.8 (cid:54) τ that
in both Ω−
˜ν > 0.16 and for 0.9 (cid:54) τ that ˜ν > 0.24. Consequently, the variance mapping Eq. (5) and Eq. (4)
ensures a lower bound on the variance ν.

A3 Proofs of the Theorems

A3.1 Proof of Theorem 1

We have to show that the mapping g deﬁned by Eq. (4) and Eq. (5) has a stable and attracting ﬁxed
point close to (0, 1). To proof this statement and Theorem 1, we apply the Banach ﬁxed point theorem
which requires (1) that g is a contraction mapping and (2) that g does not map outside the function’s
domain, concretely:

Theorem 4 (Banach Fixed Point Theorem). Let (X, d) be a non-empty complete metric space with a
contraction mapping f : X → X. Then f has a unique ﬁxed-point xf ∈ X with f (xf ) = xf . Every
xf .
sequence xn = f (xn−1) with starting element x0 ∈ X converges to the ﬁxed point: xn −−−−→
n→∞

Contraction mappings are functions that map two points such that their distance is decreasing:

Deﬁnition 2 (Contraction mapping). A function f : X → X on a metric space X with distance
d is a contraction mapping, if there is a 0 (cid:54) δ < 1, such that for all points u and v in X:
d(f (u), f (v)) (cid:54) δd(u, v).

To show that g is a contraction mapping in Ω with distance (cid:107).(cid:107)2, we use the Mean Value Theorem for
u, v ∈ Ω

(cid:107)g(u) − g(v)(cid:107)2 (cid:54) M (cid:107)u − v(cid:107)2,

(17)

in which M is an upper bound on the spectral norm the Jacobian H of g. The spectral norm is given
by the largest singular value of the Jacobian of g. If the largest singular value of the Jacobian is
smaller than 1, the mapping g of the mean and variance to the mean and variance in the next layer
is contracting. We show that the largest singular value is smaller than 1 by evaluating the function
for the singular value S(µ, ω, ν, τ, λ, α) on a grid. Then we use the Mean Value Theorem to bound
the deviation of the function S between grid points. To this end, we have to bound the gradient of S
with respect to (µ, ω, ν, τ ). If all function values plus gradient times the deltas (differences between
grid points and evaluated points) is still smaller than 1, then we have proofed that the function is
below 1 (Lemma 12). To show that the mapping does not map outside the function’s domain, we
derive bounds on the expressions for the mean and the variance (Lemma 13). Section A3.4.1 and
Section A3.4.2 are concerned with the contraction mapping and the image of the function domain of
g, respectively.

With the results that the largest singular value of the Jacobian is smaller than one (Lemma 12) and that
the mapping stays in the domain Ω (Lemma 13), we can prove Theorem 1. We ﬁrst recall Theorem 1:

13

Theorem (Stable and Attracting Fixed Points). We assume α = α01 and λ = λ01. We restrict
the range of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈
[0.95, 1.1]. For ω = 0 and τ = 1, the mapping Eq. (4) and Eq. (5) has the stable ﬁxed point (µ, ν) =
(0, 1). For other ω and τ the mapping Eq. (4) and Eq. (5) has a stable and attracting ﬁxed point
depending on (ω, τ ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617].
All points within the (µ, ν)-domain converge when iteratively applying the mapping Eq. (4) and
Eq. (5) to this ﬁxed point.

Proof. According to Lemma 12 the mapping g (Eq. (4) and Eq. (5)) is a contraction mapping in the
given domain, that is, it has a Lipschitz constant smaller than one. We showed that (µ, ν) = (0, 1) is
a ﬁxed point of the mapping for (ω, τ ) = (0, 1).

The domain is compact (bounded and closed), therefore it is a complete metric space. We further
have to make sure the mapping g does not map outside its domain Ω. According to Lemma 13, the
mapping maps into the domain µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617].

Now we can apply the Banach ﬁxed point theorem given in Theorem 4 from which the statement of
the theorem follows.

A3.2 Proof of Theorem 2

First we recall Theorem 2:
Theorem (Decreasing ν). For λ = λ01, α = α01 and the domain Ω++: −1 (cid:54) µ (cid:54) 1, −0.1 (cid:54) ω (cid:54)
0.1, 3 (cid:54) ν (cid:54) 16, and 0.8 (cid:54) τ (cid:54) 1.25 we have for the mapping of the variance ˜ν(µ, ω, ν, τ, λ, α)
given in Eq. (5)

The variance decreases in [3, 16] and all ﬁxed points (µ, ν) of mapping Eq. (5) and Eq. (4) have
ν < 3.

˜ν(µ, ω, ν, τ, λ01, α01) < ν .

(18)

Proof. We start to consider an even larger domain −1 (cid:54) µ (cid:54) 1, −0.1 (cid:54) ω (cid:54) 0.1, 1.5 (cid:54) ν (cid:54) 16,
and 0.8 (cid:54) τ (cid:54) 1.25. We prove facts for this domain and later restrict to 3 (cid:54) ν (cid:54) 16, i.e. Ω++. We
consider the function g of the difference between the second moment ˜ξ in the next layer and the
variance ν in the lower layer:

g(µ, ω, ν, τ, λ01, α01) = ˜ξ(µ, ω, ν, τ, λ01, α01) − ν .
If we can show that g(µ, ω, ν, τ, λ01, α01) < 0 for all (µ, ω, ν, τ ) ∈ Ω++, then we would obtain our
desired result ˜ν (cid:54) ˜ξ < ν. The derivative with respect to ν is according to Theorem 16:

(19)

∂
∂ν

∂
∂ν

g(µ, ω, ν, τ, λ01, α01) =

˜ξ(µ, ω, ν, τ, λ01, α01) − 1 < 0 .

(20)

Therefore g is strictly monotonically decreasing in ν. Since ˜ξ is a function in ντ (these variables only
appear as this product), we have for x = ντ

∂
∂ν

˜ξ =

∂
∂x

˜ξ

∂x
∂ν

=

∂
∂x

˜ξ τ

∂
∂τ

˜ξ =

∂
∂x

˜ξ

∂x
∂τ

=

∂
∂x

˜ξ ν .

and

Therefore

∂
∂τ

∂
∂τ

Therefore we have according to Theorem 16:

˜ξ(µ, ω, ν, τ, λ01, α01) =

˜ξ(µ, ω, ν, τ, λ01, α01) > 0 .

g(µ, ω, ν, τ, λ01, α01) =

˜ξ(µ, ω, ν, τ, λ01, α01) > 0 .

ν
τ

∂
∂ν

∂
∂τ

14

(21)

(22)

(23)

(24)

(25)

(29)

(30)

Consequently, g is strictly monotonically increasing in τ . Now we consider the derivative with respect
to µ and ω. We start with ∂
∂µ

˜ξ(µ, ω, ν, τ, λ, α), which is

˜ξ(µ, ω, ν, τ, λ, α) =

∂
∂µ

(cid:18)

λ2ω

α2 (cid:0)−eµω+ ντ

2 (cid:1) erfc

√

(cid:18) µω + ντ
√
2
ντ
(cid:19)

(cid:19)

+

(cid:18)

α2e2µω+2ντ erfc

+ µω

2 − erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)(cid:19)

(cid:18) µω
√
√
2

ντ

(cid:114) 2
π

+

√

ντ e− µ2ω2

2ντ

(cid:33)

.

We consider the sub-function

√

(cid:114) 2
π

ντ − α2

e

(cid:18)

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− e

erfc

(cid:19)(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

.

(26)

We set x = ντ and y = µω and obtain

√

(cid:114) 2
π

x − α2

(cid:18)
e

(cid:16) x+y
√
√
x
2

(cid:17)2

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(cid:16) 2x+y
√
√
x
2

(cid:17)2

− e

erfc

(cid:18) 2x + y
√
√
x
2

(cid:19)(cid:19)

.

(27)

The derivative to this sub-function with respect to y is

α2 (cid:16)

e

(2x+y)2
2x

(2x + y) erfc

(cid:16) 2x+y√

√

(cid:17)

(x+y)2
2x

− e

(x + y) erfc

(cid:17)(cid:17)

(cid:16) x+y√

√

2

x

=

(28)

√

2α2√

x





(2x+y)2
2x

e

(2x+y) erfc
√

√

2

x

(x+y)2
2x

e

−

(x+y) erfc

√

√

2

x

(cid:16) x+y
√
√
x
2

(cid:17)





> 0 .

2

x
x
(cid:16) 2x+y
√
√
x
2

(cid:17)

x

The inequality follows from Lemma 24, which states that zez2
erfc(z) is monotonically increasing in
z. Therefore the sub-function is increasing in y. The derivative to this sub-function with respect to x
is

1
√
πx2
2

(cid:18)

√

πα2

(2x+y)2
2x

e

(cid:0)4x2 − y2(cid:1) erfc

(x+y)2
2x

−e

(x − y)(x + y) erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2
√
2 (cid:0)α2 − 1(cid:1) x3/2.

−

(cid:19)(cid:19)

The sub-function is increasing in x, since the derivative is larger than zero:
√
(cid:16) 2x+y√

(cid:0)4x2 − y2(cid:1) erfc

(x − y)(x + y) erfc

πα2 (cid:16)

(2x+y)2
2x

− e

(cid:17)

e

√

2

x

(cid:17)(cid:17)

(cid:16) x+y√

√

2

x

−

√

2x3/2 (cid:0)α2 − 1(cid:1)

(cid:62)

√

πα2






(cid:32)

√

π

(2x−y)(2x+y)2
(cid:114)(cid:16) 2x+y

(cid:17)2

2x+y
√
x
2

√

+

√

√

2

x

+2

(cid:33) −

(cid:32)

√

(x−y)(x+y)2
(cid:114)(cid:16) x+y

(cid:17)2

+ 4
π

√

√

2

x

(cid:33)


 −

x+y
√
2

x

+

π

√
√
2

πx2



√

2x3/2 (cid:0)α2 − 1(cid:1)

=

√

πα2

(cid:18) (2x−y)(2x+y)2(

√

√

2

x)
(2x+y)2+4x

√

√

(cid:16)

π

2x+y+

(cid:17) −

(cid:18)

√

πα2

√

(cid:16)

(2x−y)(2x+y)2
√

π

2x+y+

(2x+y)2+4x

√

√

2

x)
(x+y)2+ 8x
π

(cid:17)

(cid:19)

√

−

√

√

(x−y)(x+y)2(
(cid:16)
π
√
2

πx2

x+y+

2x3/2 (cid:0)α2 − 1(cid:1)

=

(x−y)(x+y)2
√

(cid:16)

π

x+y+

(x+y)2+ 8x
π

(cid:17) −
√

√
√
2

πx3/2

(cid:19)

(cid:17)

− x (cid:0)α2 − 1(cid:1)

>

(cid:18) x + y
√
√
x
2

(x+y)2
2x
√
2

πx2

15

(cid:18)

√

πα2

√

(cid:16)

(2x−y)(2x+y)2
√

π

2x+y+

(2x+y)2+2(2x+y)+1

(x−y)(x+y)2

(x+y)2+0.878·2(x+y)+0.8782(cid:17)

(cid:19)

− x (cid:0)α2 − 1(cid:1)

=

(cid:17) −

√

√
√

π

(cid:16)
x+y+
√
2

πx3/2

(cid:18)

√

πα2

(2x−y)(2x+y)2
√

√

(cid:16)

π

2x+y+

(2x+y+1)2(cid:17) −
√
2

(cid:16)

√
π
√

√

πα2 (cid:16) (2x−y)(2x+y)2

π(2(2x+y)+1) − (x−y)(x+y)2
√
π(2(x+y)+0.878)
√
√
πx3/2
2
πα2 (cid:16) (2(x+y)+0.878)(2x−y)(2x+y)2

√

π

√

√

√

(x−y)(x+y)2

√

(x+y+0.878)2(cid:17)

x+y+

(cid:19)

− x (cid:0)α2 − 1(cid:1)

=

πx3/2
(cid:17)

− x (cid:0)α2 − 1(cid:1)

=

(cid:17)

− (x−y)(x+y)(2(2x+y)+1)2
√
√

√

π
πx3/2

+

(2(2x + y) + 1)(2(x + y) + 0.878)

2

πα2 (cid:0)−x (cid:0)α2 − 1(cid:1) (2(2x + y) + 1)(2(x + y) + 0.878)(cid:1)

(2(2x + y) + 1)(2(x + y) + 0.878)

√

√
2

πx3/2

=

8x3 + 12x2y + 4.14569x2 + 4xy2 − 6.76009xy − 1.58023x + 0.683154y2

√

√

>

(2(2x + y) + 1)(2(x + y) + 0.878)

2

πx3/2

8x3 − 0.1 · 12x2 + 4.14569x2 + 4 · (0.0)2x − 6.76009 · 0.1x − 1.58023x + 0.683154 · (0.0)2

(2(2x + y) + 1)(2(x + y) + 0.878)

√

√
2

πx3/2

=

8x2 + 2.94569x − 2.25624
√
(2(2x + y) + 1)(2(x + y) + 0.878)

2
8(x − 0.377966)(x + 0.746178)
2

(2(2x + y) + 1)(2(x + y) + 0.878)

√

√

√

π

x

=

√

√

π

x

> 0 .

We explain this chain of inequalities:

• First inequality: We applied Lemma 22 two times.

• Equalities factor out

x and reformulate.

√

√
2

• Second inequality part 1: we applied

0 < 2y =⇒ (2x + y)2 + 4x + 1 < (2x + y)2 + 2(2x + y) + 1 = (2x + y + 1)2 .

(31)

• Second inequality part 2: we show that for a = 1
10

π − (cid:0)a2 + 2a(x + y)(cid:1) (cid:62) 0. We have ∂

following holds:
π − 2a > 0 and
π − (cid:0)a2 + 2a(x + y)(cid:1) = −2a < 0. Therefore the minimum is at border for minimal x

(cid:16)(cid:113) 960+169π
π − (cid:0)a2 + 2a(x + y)(cid:1) = 8

− 13

∂x

(cid:17)

8x

8x

π

(cid:32)(cid:114) 960 + 169π

(cid:33)

− 13

(1.2 + 0.1) +

π

(cid:32)

1
10

(cid:32)(cid:114) 960 + 169π

(cid:33)(cid:33)2

− 13

 = 0 .

π

8x

∂
∂y
and maximal y:

8 · 1.2
π



−



2
10

Thus

(32)

(33)

8x
π

(cid:62) a2 + 2a(x + y) .

for a = 1
10

(cid:16)(cid:113) 960+169π

(cid:17)

− 13

> 0.878.

π

• Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and

(2(x + y) + 0.878).

• We set α = α01 and multiplied out. Thereafter we also factored out x in the numerator.

Finally a quadratic equations was solved.

16

The sub-function has its minimal value for minimal x = ντ = 1.5 · 0.8 = 1.2 and minimal
y = µω = −1 · 0.1 = −0.1. We further minimize the function
(cid:18)

(cid:19)(cid:19)

(cid:19)(cid:19)

(cid:18)

µ2ω2
2ντ

µωe

2 − erfc

> −0.1e

0.12
2·1.2

2 − erfc

.

(34)

(cid:18) 0.1
√
√
2

1.2

(cid:18) µω
√
√
2

ντ

We compute the minimum of the term in brackets of ∂
∂µ
(cid:18) µω
√
√
2

2 − erfc

µ2 ω2
2ντ

µωe

(cid:19)(cid:19)

ντ

+

(cid:18)

˜ξ(µ, ω, ν, τ, λ, α) in Eq. (25):

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:16) 1.2−0.1
√
1.2
2

√

(cid:17)2

erfc

(cid:18)

(cid:18)

−

−

(cid:18)

(cid:18)

α2
01

α2
01

e

e

(cid:18)

√

(cid:18) µω + ντ
√
2
ντ
(cid:18) 1.2 − 0.1
√
1.2
2
(cid:19)(cid:19)

√

0.12
2·1.2

0.1e

2 − erfc

(cid:18) 0.1
√
√
2

1.2

√

+

1.2

(cid:114) 2
π

(cid:19)

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− e

erfc

(cid:19)

(cid:16) 2·1.2−0.1
√
1.2
2

√

(cid:17)2

− e

erfc

√

(cid:114) 2
π

+

ντ >

(cid:19)(cid:19)(cid:19)

√

(cid:18) µω + 2ντ
√
ντ
2
(cid:18) 2 · 1.2 − 0.1
√
2

1.2

√

(cid:19)(cid:19)(cid:19)

−

= 0.212234 .

˜ξ(µ, ω, ν, τ, λ, α) has the sign
Therefore the term in brackets of Eq. (25) is larger than zero. Thus, ∂
∂µ
of ω. Since ˜ξ is a function in µω (these variables only appear as this product), we have for x = µω

and

∂
∂ν

˜ξ =

∂
∂x

˜ξ

∂x
∂µ

=

∂
∂x

˜ξ ω

∂
∂ω

˜ξ =

∂
∂x

˜ξ

∂x
∂ω

=

∂
∂x

˜ξ µ .

∂
∂ω
˜ξ has the sign of ω, ∂
∂µ

Since ∂
∂µ

˜ξ(µ, ω, ν, τ, λ01, α01) =

˜ξ(µ, ω, ν, τ, λ01, α01) .

˜ξ has the sign of µ. Therefore

∂
∂ω

g(µ, ω, ν, τ, λ01, α01) =

˜ξ(µ, ω, ν, τ, λ01, α01)

µ
ω

∂
∂µ

∂
∂ω

has the sign of µ.
We now divide the µ-domain into −1 (cid:54) µ (cid:54) 0 and 0 (cid:54) µ (cid:54) 1. Analogously we divide the ω-domain
into −0.1 (cid:54) ω (cid:54) 0 and 0 (cid:54) ω (cid:54) 0.1. In this domains g is strictly monotonically.

For all domains g is strictly monotonically decreasing in ν and strictly monotonically increasing in τ .
Note that we now consider the range 3 (cid:54) ν (cid:54) 16. For the maximal value of g we set ν = 3 (we set it
to 3!) and τ = 1.25.

We consider now all combination of these domains:

• −1 (cid:54) µ (cid:54) 0 and −0.1 (cid:54) ω (cid:54) 0:

g is decreasing in µ and decreasing in ω. We set µ = −1 and ω = −0.1.

g(−1, −0.1, 3, 1.25, λ01, α01) = −0.0180173 .

• −1 (cid:54) µ (cid:54) 0 and 0 (cid:54) ω (cid:54) 0.1:

g is increasing in µ and decreasing in ω. We set µ = 0 and ω = 0.

g(0, 0, 3, 1.25, λ01, α01) = −0.148532 .

• 0 (cid:54) µ (cid:54) 1 and −0.1 (cid:54) ω (cid:54) 0:

g is decreasing in µ and increasing in ω. We set µ = 0 and ω = 0.

g(0, 0, 3, 1.25, λ01, α01) = −0.148532 .

17

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

• 0 (cid:54) µ (cid:54) 1 and 0 (cid:54) ω (cid:54) 0.1:

g is increasing in µ and increasing in ω. We set µ = 1 and ω = 0.1.

g(1, 0.1, 3, 1.25, λ01, α01) = −0.0180173 .

(43)

Therefore the maximal value of g is −0.0180173.

A3.3 Proof of Theorem 3

First we recall Theorem 3:
Theorem (Increasing ν). We consider λ = λ01, α = α01 and the two domains Ω−
1 =
{(µ, ω, ν, τ ) | − 0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1, 0.05 (cid:54) ν (cid:54) 0.16, 0.8 (cid:54) τ (cid:54) 1.25} and
Ω−

2 = {(µ, ω, ν, τ ) | − 0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1, 0.05 (cid:54) ν (cid:54) 0.24, 0.9 (cid:54) τ (cid:54) 1.25} .

The mapping of the variance ˜ν(µ, ω, ν, τ, λ, α) given in Eq. (5) increases

(44)
2 . All ﬁxed points (µ, ν) of mapping Eq. (5) and Eq. (4) ensure for 0.8 (cid:54) τ that
in both Ω−
˜ν > 0.16 and for 0.9 (cid:54) τ that ˜ν > 0.24. Consequently, the variance mapping Eq. (5) and Eq. (4)
ensures a lower bound on the variance ν.

˜ν(µ, ω, ν, τ, λ01, α01) > ν

1 and Ω−

Proof. The mean value theorem states that there exists a t ∈ [0, 1] for which
˜ξ(µ, ω, ν, τ, λ01, α01) − ˜ξ(µ, ω, νmin, τ, λ01, α01) =
∂
∂ν

˜ξ(µ, ω, ν + t(νmin − ν), τ, λ01, α01) (ν − νmin) .

Therefore

˜ξ(µ, ω, ν, τ, λ01, α01) = ˜ξ(µ, ω, νmin, τ, λ01, α01) +
∂
∂ν

˜ξ(µ, ω, ν + t(νmin − ν), τ, λ01, α01) (ν − νmin) .

Therefore we are interested to bound the derivative of the ξ-mapping Eq. (13) with respect to ν:

˜ξ(µ, ω, ν, τ, λ01, α01) =
(cid:18)

(cid:18)

(cid:18)

∂
∂ν
1
λ2τ e− µ2ω2
2

2ντ

−

e

erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

+ 2

.

α2

(cid:19)

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− 2e

erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)(cid:19)(cid:19)

−

√

(cid:17)

(cid:16) µω√

The sub-term Eq. (308) enters the derivative Eq. (47) with a negative sign! According to Lemma 18,
the minimal value of sub-term Eq. (308) is obtained by the largest largest ν, by the smallest τ , and
+ 2 is multiplied by τ , which is
the largest y = µω = 0.01. Also the positive term erfc
minimized by using the smallest τ . Therefore we can use the smallest τ in whole formula Eq. (47) to
lower bound it.
First we consider the domain 0.05 (cid:54) ν (cid:54) 0.16 and 0.8 (cid:54) τ (cid:54) 1.25. The factor consisting of the
exponential in front of the brackets has its smallest value for e− 0.01·0.01
2·0.05·0.8 . Since erfc is monotonically
(cid:17)
decreasing we inserted the smallest argument via erfc
in order to obtain the maximal
negative contribution. Thus, applying Lemma 18, we obtain the lower bound on the derivative:
(cid:18) µω + ντ
λ2τ e− µ2ω2
√
ντ
2

(cid:18) µω + 2ντ
√
ντ
2

(cid:16) µω+ντ
√
e
ντ
2

(cid:16) µω+2ντ
√
ντ
2

0.01
0.05·0.8

(cid:19)(cid:19)(cid:19)

− 2e

erfc

erfc

α2

1
2

√

√

−

−

−

(cid:18)

(cid:18)

(cid:18)

(cid:19)

(cid:16)

(cid:17)2

(cid:17)2

ντ

2ντ

√

√

2

2

√

√

(45)

(46)

(47)

(48)

18

erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)

+ 2

>

1
2

0.8e− 0.01·0.01

2·0.05·0.8 λ2
01

(cid:16) 2·0.16·0.8+0.01

(cid:17)2

√

√

2

0.16·0.8

2e

erfc

(cid:18)

√

√

(cid:18)

(cid:18)
e

(cid:16) 0.16·0.8+0.01
0.16·0.8

−

α2
01
(cid:18) 2 · 0.16 · 0.8 + 0.01

2

(cid:17)2

erfc

(cid:19)(cid:19)(cid:19)

√

√
2

0.16 · 0.8

(cid:19)

−

√

(cid:18) 0.16 · 0.8 + 0.01
0.16 · 0.8
(cid:18)

√
2

− erfc

−

√

√

0.01
0.05 · 0.8

2

(cid:19)

(cid:19)

+ 2

) > 0.969231 .

For applying the mean value theorem, we require the smallest ˜ν(ν). We follow the proof of Lemma 8,
which shows that at the minimum y = µω must be maximal and x = ντ must be minimal. Thus, the
smallest ˜ξ(µ, ω, ν, τ, λ01, α01) is ˜ξ(0.01, 0.01, 0.05, 0.8, λ01, α01) = 0.0662727 for 0.05 (cid:54) ν and
0.8 (cid:54) τ .
Therefore the mean value theorem and the bound on (˜µ)2 (Lemma 43) provide
˜ν = ˜ξ(µ, ω, ν, τ, λ01, α01) − (˜µ(µ, ω, ν, τ, λ01, α01))2 >
0.0662727 + 0.969231(ν − 0.05) − 0.005 = 0.01281115 + 0.969231ν >
0.08006969 · 0.16 + 0.969231ν (cid:62) 1.049301ν > ν .

(49)

Next we consider the domain 0.05 (cid:54) ν (cid:54) 0.24 and 0.9 (cid:54) τ (cid:54) 1.25. The factor consisting of the
exponential in front of the brackets has its smallest value for e− 0.01·0.01
2·0.05·0.9 . Since erfc is monotonically
(cid:17)
decreasing we inserted the smallest argument via erfc
in order to obtain the maximal
negative contribution.

0.01
0.05·0.9

−

(cid:16)

√

√

2

Thus, applying Lemma 18, we obtain the lower bound on the derivative:

λ2τ e− µ2ω2

2ντ

(cid:18)

(cid:18)

(cid:18)

α2

−

(cid:16) µω+ντ
√
e
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− 2e

erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)(cid:19)(cid:19)

−

(50)

0.9e− 0.01·0.01

2·0.05·0.9 λ2
01

(cid:16) 2·0.24·0.9+0.01

(cid:17)2

√

√

2

0.24·0.9

2e

erfc

(cid:18)

√

√

(cid:18)

(cid:18)
e

(cid:16) 0.24·0.9+0.01
0.24·0.9

−

α2
01
(cid:18) 2 · 0.24 · 0.9 + 0.01

2

(cid:17)2

erfc

(cid:19)(cid:19)(cid:19)

√

√
2

0.24 · 0.9

(cid:19)

−

√

(cid:18) 0.24 · 0.9 + 0.01
0.24 · 0.9
(cid:18)

√
2

− erfc

−

√

√

0.01
0.05 · 0.9

2

(cid:19)

(cid:19)

+ 2

) > 0.976952 .

For applying the mean value theorem, we require the smallest ˜ν(ν). We follow the proof of Lemma 8,
which shows that at the minimum y = µω must be maximal and x = ντ must be minimal. Thus, the
smallest ˜ξ(µ, ω, ν, τ, λ01, α01) is ˜ξ(0.01, 0.01, 0.05, 0.9, λ01, α01) = 0.0738404 for 0.05 (cid:54) ν and
0.9 (cid:54) τ . Therefore the mean value theorem and the bound on (˜µ)2 (Lemma 43) gives

˜ν = ˜ξ(µ, ω, ν, τ, λ01, α01) − (˜µ(µ, ω, ν, τ, λ01, α01))2 >
0.0738404 + 0.976952(ν − 0.05) − 0.005 = 0.0199928 + 0.976952ν >
0.08330333 · 0.24 + 0.976952ν (cid:62) 1.060255ν > ν .

(51)

1
2

1
2

erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)

+ 2

>

A3.4 Lemmata and Other Tools Required for the Proofs

A3.4.1 Lemmata for prooﬁng Theorem 1 (part 1): Jacobian norm smaller than one

In this section, we show that the largest singular value of the Jacobian of the mapping g is smaller
than one. Therefore, g is a contraction mapping. This is even true in a larger domain than the original
Ω. We do not need to restrict τ ∈ [0.95, 1.1], but we can extend to τ ∈ [0.8, 1.25]. The range of the
other variables is unchanged such that we consider the following domain throughout this section:
µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].

19

In the following, we denote two Jacobians: (1) the Jacobian J of the
Jacobian of the mapping.
mapping h : (µ, ν) (cid:55)→ (˜µ, ˜ξ), and (2) the Jacobian H of the mapping g : (µ, ν) (cid:55)→ (˜µ, ˜ν) because the
inﬂuence of ˜µ on ˜ν is small, and many properties of the system can already be seen on J .

J =

H =

(cid:18) J11 J12
J21 J22
(cid:18) H11 H12
H21 H22

(cid:19)

=

(cid:32) ∂

∂µ ˜µ
˜ξ
∂
∂µ

(cid:33)

∂
∂ν ˜µ
˜ξ
∂
∂ν

(cid:19)

(cid:18)

=

J11

J21 − 2˜µJ11 J22 − 2˜µJ12

J12

(cid:19)

The deﬁnition of the entries of the Jacobian J is:

J11(µ, ω, ν, τ, λ, α) =

˜µ(µ, ω, ν, τ, λ, α) =

λω

αeµω+ ντ

2 erfc

− erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)

+ 2

J12(µ, ω, ν, τ, λ, α) =

˜µ(µ, ω, ν, τ, λ, α) =

λτ

αeµω+ ντ

2 erfc

− (α − 1)

(cid:114) 2
πντ

(cid:33)

e− µ2ω2

2ντ

√

∂
∂µ
(cid:18) µω + ντ
√
2
ντ
∂
∂ν
(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

(cid:19)

(cid:18)

(cid:32)

1
2

1
4

J21(µ, ω, ν, τ, λ, α) =

˜ξ(µ, ω, ν, τ, λ, α) =

(cid:18)

λ2ω

α2 (cid:0)−eµω+ ντ

∂
∂µ
2 (cid:1) erfc

(cid:18) µω + 2ντ
√
ντ
2

√

√

(cid:18) µω + ντ
√
2
ντ
(cid:19)

(cid:19)

+

(cid:18)

α2e2µω+2ντ erfc

+ µω

2 − erfc

J22(µ, ω, ν, τ, λ, α) =

(cid:18)

λ2τ

α2 (cid:0)−eµω+ ντ

1
2

∂
∂ν
2 (cid:1) erfc

˜ξ(µ, ω, ν, τ, λ, α) =
(cid:18) µω + ντ
√
2
ντ
(cid:19)

√

+

(cid:19)

2α2e2µω+2ντ erfc

(cid:18) µω + 2ντ
√
ντ
2

√

− erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)

+ 2

(cid:19)(cid:19)

(cid:18) µω
√
√
2

ντ

(cid:114) 2
π

+

(cid:33)

√

ντ e− µ2 ω2

2ντ

Proof sketch: Bounding the largest singular value of the Jacobian.
If the largest singular value
of the Jacobian is smaller than 1, then the spectral norm of the Jacobian is smaller than 1. Then the
mapping Eq. (4) and Eq. (5) of the mean and variance to the mean and variance in the next layer is
contracting.

We show that the largest singular value is smaller than 1 by evaluating the function S(µ, ω, ν, τ, λ, α)
on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between
grid points. Toward this end we have to bound the gradient of S with respect to (µ, ω, ν, τ ). If all
function values plus gradient times the deltas (differences between grid points and evaluated points)
is still smaller than 1, then we have proofed that the function is below 1.

The singular values of the 2 × 2 matrix

are

s1 =

s2 =

1
2
1
2

(cid:16)(cid:112)(a11 + a22)2 + (a21 − a12)2 + (cid:112)(a11 − a22)2 + (a12 + a21)2
(cid:17)
(cid:16)(cid:112)(a11 + a22)2 + (a21 − a12)2 − (cid:112)(a11 − a22)2 + (a12 + a21)2

(cid:17)

.

A =

(cid:18) a11 a12
a21 a22

(cid:19)

20

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

We used an explicit formula for the singular values [4]. We now set H11 = a11, H12 = a12, H21 =
a21, H22 = a22 to obtain a formula for the largest singular value of the Jacobian depending on
(µ, ω, ν, τ, λ, α). The formula for the largest singular value for the Jacobian is:

S(µ, ω, ν, τ, λ, α) =

(cid:16)(cid:112)(H11 + H22)2 + (H21 − H12)2 + (cid:112)(H11 − H22)2 + (H12 + H21)2

(cid:17)

=

(61)

1
2

(cid:16)(cid:112)(J11 + J22 − 2˜µJ12)2 + (J21 − 2˜µJ11 − J12)2 +

=
(cid:112)(J11 − J22 + 2˜µJ12)2 + (J12 + J21 − 2˜µJ11)2

(cid:17)

,

where J are deﬁned in Eq. (54) and we left out the dependencies on (µ, ω, ν, τ, λ, α) in order to keep
the notation uncluttered, e.g. we wrote J11 instead of J11(µ, ω, ν, τ, λ, α).

Bounds on the derivatives of the Jacobian entries.
In order to bound the gradient of the
singular value, we have to bound the derivatives of the Jacobian entries J11(µ, ω, ν, τ, λ, α),
J12(µ, ω, ν, τ, λ, α), J21(µ, ω, ν, τ, λ, α), and J22(µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ .
The values λ and α are ﬁxed to λ01 and α01. The 16 derivatives of the 4 Jacobian entries with respect
to the 4 variables are:

=

λω2e− µ2 ω2

2ντ

(µω+ντ )2
2ντ

erfc



αe

(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

−

(cid:113) 2

π (α − 1)
√
ντ





(62)



(cid:113) 2


−e− µ2ω2

2ντ



=

λ

π (α − 1)µω
ντ

√

− α(µω + 1)e

(µω+ντ )2
2ντ

erfc

(cid:18) µω + ντ
√
ντ
2

√



(cid:19)

 −

1
2

1
2

1
4

1
4

1
4

=

=

=

=

=

1
8
(cid:114) 2
π

=

1
8
(cid:114) 2
π

∂J11
∂µ

∂J11
∂ω

∂J11
∂ν

∂J11
∂τ

∂J12
∂µ

∂J12
∂ω

∂J12
∂ν

∂J12
∂τ

∂J21
∂µ

erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)

+ 2

(cid:32)

(cid:32)

(cid:32)

∂J11
∂ν

λτ ωe− µ2ω2

2ντ

(µω+ντ )2
2ντ

αe

erfc

λνωe− µ2ω2

2ντ

(µω+ντ )2
2ντ

αe

erfc

√

(cid:18) µω + ντ
√
2
ντ
(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

(cid:19)

+

+

(cid:114) 2
π
(cid:114) 2
π

(cid:18) (α − 1)µω
(ντ )3/2

−

α
√
ντ

(cid:18) (α − 1)µω
(ντ )3/2

−

α
√
ντ

(cid:19)(cid:33)

(cid:19)(cid:33)

λµτ e− µ2 ω2

2ντ

(µω+ντ )2
2ντ

αe

erfc

(cid:18)

λe− µ2 ω2

2ντ

ατ 2e

(µω+ντ )2
2ντ

erfc

(cid:19)

(cid:19)

√

(cid:18) µω + ντ
√
ντ
2
(cid:18) µω + ντ
√
ντ
2

√

+

+

√

+

(cid:18) (−1)(α − 1)µ2ω2
ν5/2
(cid:18)

√

τ

λe− µ2 ω2

2ντ

(µω+ντ )2
2ντ

2αe

erfc

τ (α + αµω − 1)
ν3/2
(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

(cid:114) 2
π

(cid:18) (α − 1)µω
(ντ )3/2

−

α
√
ντ

(cid:19)(cid:33)

(cid:19)(cid:33)

−

ατ 3/2
√
ν

+ αντ e

(µω+ντ )2
2ντ

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

+

(cid:18) (−1)(α − 1)µ2ω2
(ντ )3/2

+

−α + αµω + 1
ντ

√

(cid:19)(cid:33)

√

− α

ντ

(cid:18)

(cid:18)

(cid:19)

= λ2ω2

α2

−e− µ2ω2

2ντ

(µω+ντ )2
2ντ

e

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

+

21

∂J21
∂ω

∂J21
∂ν

∂J21
∂τ

1
2

1
2

2α2e

(µω+2ντ )2
2ντ

e− µ2ω2

2ντ erfc

(cid:18)

(cid:18)

= λ2

α2(µω + 1)

−e− µ2ω2

2ντ

(µω+ντ )2
2ντ

e

erfc

(cid:19)

√

(cid:18) µω + 2ντ
√
2
ντ
(cid:19)

− erfc

(cid:18) µω
√
√
ντ
2
(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

(cid:19)

+ 2

(cid:19)

+

α2(2µω + 1)e

(µω+2ντ )2
2ντ

e− µ2 ω2

2ντ erfc

(cid:18)

2µω

2 − erfc

(cid:18) µω
√
√
2
(cid:18)

ντ
(cid:18)

(cid:19)(cid:19)

(cid:114) 2
π

+

(cid:19)

(µω+ντ )2
2ντ

=

λ2τ ωe− µ2 ω2

2ντ

α2

−e

(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

+

(cid:33)

√

ντ e− µ2ω2

2ντ

+

(cid:19)

erfc

√

(cid:18) µω + ντ
√
2
ντ
π (−1) (cid:0)α2 − 1(cid:1)
ντ

√





(cid:113) 2

+

(cid:19)

erfc

√

(cid:18) µω + ντ
√
2
ντ
π (−1) (cid:0)α2 − 1(cid:1)
ντ

√





(cid:113) 2

+

(cid:19)

erfc

√

(cid:18) µω + ντ
√
2
ντ
π (−1) (cid:0)α2 − 1(cid:1)
ντ

√





(cid:113) 2

+

(cid:19)

√

(cid:18) µω + ντ
√
2
ντ
(cid:32) (cid:0)α2 − 1(cid:1) µω
(ντ )3/2
(cid:19)

−

(cid:18) µω + ντ
√
ντ
2

√

4α2e

(µω+2ντ )2
2ντ

erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)

+

=

λ2νωe− µ2ω2

2ντ

(cid:18)

(cid:18)

α2

−e

(cid:19)

(µω+ντ )2
2ντ

4α2e

(µω+2ντ )2
2ντ

erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)

+

∂J22
∂µ
∂J22
∂ω

=

=

∂J21
∂ν
1
λ2µτ e− µ2ω2
2

2ντ

(cid:18)

(cid:18)

α2

−e

(cid:19)

(µω+ντ )2
2ντ

4α2e

(µω+2ντ )2
2ντ

erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)

+

=

λ2τ 2e− µ2ω2

2ντ

(cid:18)

(cid:18)

α2

−e

(cid:19)

(µω+ντ )2
2ντ

erfc

∂J22
∂ν

1
4

8α2e

(µω+2ντ )2
2ντ

erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)

+

(cid:114) 2
π

∂J22
∂τ

=

(cid:18)

1
4

λ2

−2α2e− µ2 ω2
2ντ e

(µω+ντ )2
2ντ

erfc

(cid:19)

√

(cid:18) µω + ντ
√
ντ
2
(cid:18) µω + 2ντ
√
2
ντ
− 3α2√

ντ

√

(cid:33)(cid:33)

8α2ντ e

(µω+2ντ )2
2ντ

e− µ2ω2

2ντ erfc

(cid:114) 2
π

e− µ2 ω2

2ντ

(cid:32) (cid:0)α2 − 1(cid:1) µω
√

ντ

(cid:33)(cid:33)

−

3α2
√
ντ

α2ντ e− µ2ω2
2ντ e

(µω+ντ )2
2ντ

erfc

+ 4α2e

(µω+2ντ )2
2ντ

e− µ2 ω2

2ντ erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)

+

(cid:19)

(cid:18)

+ 2

2 − erfc

(cid:18) µω
√
√
2

ντ

(cid:19)(cid:19)

+

Lemma 5 (Bounds on the Derivatives). The following bounds on the absolute values of the deriva-
tives of the Jacobian entries J11(µ, ω, ν, τ, λ, α), J12(µ, ω, ν, τ, λ, α), J21(µ, ω, ν, τ, λ, α), and
J22(µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ hold:

< 0.0031049101995398316

(63)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J11
∂µ
∂J11
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 1.055872374194189

22

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J11
∂ν
∂J11
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J12
∂µ
∂J12
∂ω
∂J12
∂ν
∂J12
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J21
∂µ
∂J21
∂ω
∂J21
∂ν
∂J21
∂τ

∂J22
∂µ
∂J22
∂ω
∂J22
∂ν
∂J22
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 0.031242911235461816

< 0.03749149348255419

< 0.031242911235461816

< 0.031242911235461816

< 0.21232788238624354

< 0.2124377655377270

< 0.02220441024325437

< 1.146955401845684

< 0.14983446469110305

< 0.17980135762932363

< 0.14983446469110305

< 0.14983446469110305

< 1.805740052651535

< 2.396685907216327

Proof. See proof 39.

Bounds on the entries of the Jacobian.
Lemma 6 (Bound on J11). The absolute value of the function
(cid:17)
(cid:16) µω√
is bounded by |J11| (cid:54) 0.104497 in
J11 = 1
2
the domain −0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1, 0.8 (cid:54) ν (cid:54) 1.5, and 0.8 (cid:54) τ (cid:54) 1.25 for α = α01
and λ = λ01.

(cid:16) µω+ντ
√
√
ντ
2

αeµω+ ντ

− erfc

2 erfc

2 λω

+ 2

(cid:17)

(cid:16)

(cid:17)

ντ

√

Proof.

|J11| =

λω

αeµω+ ντ

2 erfc

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

+ 2 − erfc

(cid:18) µω
√
√
2

ντ

(cid:19)(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54) |

||λ||ω| (|α|0.587622 + 1.00584) (cid:54) 0.104497,

1
2

23

where we used that (a) J11 is strictly monotonically increasing in µω and |2 − erfc
(cid:16) 0.01+0.64
1.00584 and (b) Lemma 47 that |eµω+ ντ
√
0.64
2

(cid:16) µω+ντ
√
√
ντ
2

| (cid:54) e0.01+ 0.64

2 erfc

2 erfc

(cid:17)

√

(64)
(cid:17)

| (cid:54)

(cid:16) 0.01√
(cid:17)

√

2

ντ

= 0.587622

(cid:16)

Lemma 7 (Bound on J12). The absolute value of the function
(cid:16) µω+ντ
πντ e− µ2 ω2
is bounded by |J12| (cid:54) 0.194145
J12 = 1
√
√
ντ
2
in the domain −0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1, 0.8 (cid:54) ν (cid:54) 1.5, and 0.8 (cid:54) τ (cid:54) 1.25 for α = α01
and λ = λ01.

αeµω+ ντ

− (α − 1)

(cid:113) 2

2 erfc

4 λτ

(cid:17)

(cid:17)

2ντ

Proof.

|J12| (cid:54) 1
4

1
4
0.194035

|λ||τ |

αeµω+ ντ

2 erfc

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

− (α − 1)

(cid:114) 2
πντ

e− µ2ω2

2ντ

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

|λ||τ | |0.983247 − 0.392294| (cid:54)

(65)

For the ﬁrst term we have 0.434947 (cid:54) eµω+ ντ

2 erfc

(cid:54) 0.587622 after Lemma 47 and for

2ντ (cid:54) 0.997356, which can easily be seen by maximizing
the second term 0.582677 (cid:54)
or minimizing the arguments of the exponential or the square root function. The ﬁrst term scaled
by α is 0.727780 (cid:54) αeµω+ ντ
(cid:54) 0.983247 and the second term scaled by α − 1 is

(cid:17)

(cid:113) 2

πντ e− µ2ω2
(cid:16) µω+ντ
√
√
ντ
2

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

0.392294 (cid:54) (α − 1)
terms is at most 0.983247 − 0.392294 leading to the derived bound.

2ντ (cid:54) 0.671484. Therefore, the absolute difference between these

2 erfc
πντ e− µ2ω2

(cid:113) 2

Bounds on mean, variance and second moment. For deriving bounds on ˜µ, ˜ξ, and ˜ν, we need
the following lemma.
Lemma 8 (Derivatives of the Mapping). We assume α = α01 and λ = λ01. We restrict the range of
the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].

The derivative ∂

∂µ ˜µ(µ, ω, ν, τ, λ, α) has the sign of ω.

The derivative ∂

∂ν ˜µ(µ, ω, ν, τ, λ, α) is positive.

The derivative ∂
∂µ

˜ξ(µ, ω, ν, τ, λ, α) has the sign of ω.

The derivative ∂
∂ν

˜ξ(µ, ω, ν, τ, λ, α) is positive.

Proof. See 40.
Lemma 9 (Bounds on mean, variance and second moment). The expressions ˜µ, ˜ξ, and ˜ν for
α = α01 and λ = λ01 are bounded by −0.041160 < ˜µ < 0.087653, 0.703257 < ˜ξ < 1.643705
and 0.695574 < ˜ν < 1.636023 in the domain µ ∈ [−0.1, 0.1], ν ∈ [0.8, 15], ω ∈ [−0.1, 0.1],
τ ∈ [0.8, 1.25].

Proof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and
Eq. (5) with respect to ν and µ are either positive or have the sign of ω. Therefore with given sign of
ω the mappings are strict monotonic and the their maxima and minima are found at the borders. The
minimum of ˜µ is obtained at µω = −0.01 and its maximum at µω = 0.01 and σ and τ at minimal or
maximal values, respectively. It follows that
−0.041160 < ˜µ(−0.1, 0.1, 0.8, 0.8, λ01, α01) (cid:54)˜µ (cid:54) ˜µ(0.1, 0.1, 1.5, 1.25, λ01, α01) < 0.087653.
(66)

24

(67)

(68)

(69)

(70)

(71)

(72)

(73)

Similarly, the maximum and minimum of ˜ξ is obtained at the values mentioned above:

0.703257 < ˜ξ(−0.1, 0.1, 0.8, 0.8, λ01, α01) (cid:54) ˜ξ (cid:54) ˜ξ(0.1, 0.1, 1.5, 1.25, λ01, α01) < 1.643705.

Hence we obtain the following bounds on ˜ν:

0.703257 − ˜µ2 < ˜ξ − ˜µ2 < 1.643705 − ˜µ2
0.703257 − 0.007683 < ˜ν < 1.643705 − 0.007682
0.695574 < ˜ν < 1.636023.

Upper Bounds on the Largest Singular Value of the Jacobian.
Lemma 10 (Upper Bounds on Absolute Derivatives of Largest Singular Value). We set α = α01
and λ = λ01 and restrict the range of the variables to µ ∈ [µmin, µmax] = [−0.1, 0.1], ω ∈
[ωmin, ωmax] = [−0.1, 0.1], ν ∈ [νmin, νmax] = [0.8, 1.5], and τ ∈ [τmin, τmax] = [0.8, 1.25].

The absolute values of derivatives of the largest singular value S(µ, ω, ν, τ, λ, α) given in Eq. (61)
with respect to (µ, ω, ν, τ ) are bounded as follows:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂µ
∂S
∂ω
∂S
∂ν
∂S
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 0.32112 ,

< 2.63690 ,

< 2.28242 ,

< 2.98610 .

Proof. The Jacobian of our mapping Eq. (4) and Eq. (5) is deﬁned as

H =

(cid:18) H11 H12
H21 H22

(cid:19)

(cid:18)

=

J11

J12

(cid:19)

J21 − 2˜µJ11 J22 − 2˜µJ12

1
2

(cid:16)(cid:112)(H11 − H22)2 + (H12 + H21)2 + (cid:112)(H11 + H22)2 + (H12 − H21)2
(74)

(cid:17)

,

and has the largest singular value

S(µ, ω, ν, τ, λ, α) =

according to the formula of Blinn [4].

We obtain
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H11

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

H11 − H22
(cid:112)(H11 − H22)2 + (H12 + H21)2

+

H11 + H22
(cid:112)(H11 + H22)2 + (H21 − H12)2

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
2

1

1

(cid:113) (H12+H21)2

(H11−H22)2 + 1

(cid:113) (H21−H12)2

(H11+H22)2 + 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



 <

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 + 1
2

= 1

and analogously

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H12

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

H12 + H21
(cid:112)(H11 − H22)2 + (H12 + H21)2

−

H21 − H12
(cid:112)(H11 + H22)2 + (H21 − H12)2

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

<

(75)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 1

(76)

25

and

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H21

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

and

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H22

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

We have

H21 − H12
(cid:112)(H11 + H22)2 + (H21 − H12)2

+

H12 + H21
(cid:112)(H11 − H22)2 + (H12 + H21)2

H11 + H22
(cid:112)(H11 + H22)2 + (H21 − H12)2

−

H11 − H22
(cid:112)(H11 − H22)2 + (H12 + H21)2

∂S
∂µ
∂S
∂ω
∂S
∂ν
∂S
∂τ

=

=

=

=

∂S
∂H11
∂S
∂H11
∂S
∂H11
∂S
∂H11

∂H11
∂µ
∂H11
∂ω
∂H11
∂ν
∂H11
∂τ

+

+

+

+

∂S
∂H12
∂S
∂H12
∂S
∂H12
∂S
∂H12

∂H12
∂µ
∂H12
∂ω
∂H12
∂ν
∂H12
∂τ

+

+

+

+

∂S
∂H21
∂S
∂H21
∂S
∂H21
∂S
∂H21

∂H21
∂µ
∂H21
∂ω
∂H21
∂ν
∂H21
∂τ

+

+

+

+

∂S
∂H22
∂S
∂H22
∂S
∂H22
∂S
∂H22

∂H22
∂µ
∂H22
∂ω
∂H22
∂ν
∂H22
∂τ

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 1

(77)

< 1 .

(78)

(79)

(80)

(81)

(82)

(83)

from which follows using the bounds from Lemma 5:

+

(cid:54)

+

+

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(84)

∂H21
∂µ

∂S
∂H22

∂H12
∂µ
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H21
∂µ

(cid:12)
∂S
(cid:12)
(cid:12)
∂µ
(cid:12)
∂S
∂H11
∂H11
∂µ
∂J11
∂µ
∂J11
∂µ

Derivative of the singular value w.r.t. µ:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
0.0031049101995398316 + 0.031242911235461816 + 0.02220441024325437 + 0.14983446469110305+
2 · 0.104497 · 0.087653 + 2 · 0.1044972+
2 · 0.194035 · 0.087653 + 2 · 0.104497 · 0.194035 < 0.32112,

∂S
∂H12
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂J21 − 2˜µJ11
∂µ
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
∂H11
(cid:12)
(cid:12)
∂µ
(cid:12)
(cid:12)
∂H12
(cid:12)
(cid:12)
∂µ
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂µ
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂µ
(cid:12)

∂J22 − 2˜µJ12
∂µ
(cid:12)
∂J11
(cid:12)
(cid:12)
∂µ
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H22
∂µ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂J22
∂µ

∂S
∂H21
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|˜µ| + 2 |J12| |J11| (cid:54)

|˜µ| + 2 |J11|2 + 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂H22
∂µ

∂J21
∂µ

∂J12
∂µ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

+

(cid:54)

(cid:54)

+

+

+

+

+

+

+

where we used the results from the lemmata 5, 6, 7, and 9.

(cid:54)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Derivative of the singular value w.r.t. ω:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H12
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂J21 − 2˜µJ11
∂ω

(cid:12)
∂H11
(cid:12)
(cid:12)
∂ω
(cid:12)
(cid:12)
∂H12
(cid:12)
(cid:12)
∂ω
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂ω
(cid:12)

(cid:12)
∂S
(cid:12)
(cid:12)
∂ω
(cid:12)
∂S
∂H11
∂H11
∂ω
∂J11
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H22
∂ω
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H21
∂ω

∂H12
∂ω
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

+

+

+

+

+

+

∂S
∂H21
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J22 − 2˜µJ12
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

26

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂H21
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H22

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂H22
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

(85)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J12
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J22
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J11
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|˜µ| + 2 |J11|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂ ˜µ
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
∂J11
(cid:12)
(cid:12)
∂ω
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂ω
(cid:12)

2

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J21
∂ω
(cid:12)
∂ ˜µ
(cid:12)
(cid:12)
∂ω
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|˜µ| + 2 |J12|

(cid:54)

2.38392 + 2 · 1.055872374194189 · 0.087653 + 2 · 0.1044972 + 2 · 0.031242911235461816 · 0.087653
+ 2 · 0.194035 · 0.104497 < 2.63690 ,

where we used the results from the lemmata 5, 6, 7, and 9 and that ˜µ is symmetric for µ, ω.

∂J12
∂ν

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|˜µ| + 2 |J12|2 (cid:54)

(cid:54)

+

+

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂H12
∂ν

∂H21
∂ν

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H21
∂ν

(cid:12)
∂S
(cid:12)
(cid:12)
∂ν
(cid:12)
∂S
∂H11
∂H11
∂ν
∂J11
∂ν
∂J11
∂ν

Derivative of the singular value w.r.t. ν:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2.19916 + 2 · 0.031242911235461816 · 0.087653 + 2 · 0.104497 · 0.194035+
2 · 0.21232788238624354 · 0.087653 + 2 · 0.1940352 < 2.28242 ,

(cid:12)
(cid:12)
∂S
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H12
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H22
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂ν
(cid:12)
(cid:12)
(cid:12)
∂J21 − 2˜µJ11
(cid:12)
(cid:12)
∂ν
(cid:12)
(cid:12)
∂J22
(cid:12)
(cid:12)
∂ν
(cid:12)

(cid:12)
∂H11
(cid:12)
(cid:12)
∂ν
(cid:12)
(cid:12)
∂H12
(cid:12)
(cid:12)
∂ν
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂ν
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂ν
(cid:12)

∂J22 − 2˜µJ12
∂ν
(cid:12)
∂J11
(cid:12)
(cid:12)
∂ν
(cid:12)

∂S
∂H21
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|˜µ| + 2 |J11| |J12| + 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H22

∂H22
∂ν

∂J21
∂ν

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

+

+

+

(cid:54)

+

+

+

+

+

(cid:54)

(cid:54)

where we used the results from the lemmata 5, 6, 7, and 9.

Derivative of the singular value w.r.t. τ :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

(cid:12)
∂S
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
∂S
(cid:12)
(cid:12)
∂H11
(cid:12)
(cid:12)
∂H11
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
∂J11
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
∂J11
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂τ
(cid:12)

2

+

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H21
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
∂H11
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
∂H12
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
∂J12
(cid:12)
(cid:12)
∂τ
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

+

+

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂H12
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H21
∂τ

(cid:12)
(cid:12)
∂S
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H12
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂H22
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
(cid:12)
∂J21 − 2˜µJ11
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
∂J22
(cid:12)
(cid:12)
∂τ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J21
∂τ
(cid:12)
∂ ˜µ
(cid:12)
(cid:12)
∂τ
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:54)

|˜µ| + 2 |J12|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂H21
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S
∂H22

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂H22
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

∂J22 − 2˜µJ12
∂τ
(cid:12)
∂J11
(cid:12)
(cid:12)
∂τ
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 2

|˜µ| + 2 |J11|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂ ˜µ
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

2.82643 + 2 · 0.03749149348255419 · 0.087653 + 2 · 0.104497 · 0.194035+
2 · 0.2124377655377270 · 0.087653 + 2 · 0.1940352 < 2.98610 ,
where we used the results from the lemmata 5, 6, 7, and 9 and that ˜µ is symmetric for ν, τ .

(86)

(87)

(88)

(89)

Lemma 11 (Mean Value Theorem Bound on Deviation from Largest Singular Value). We set
α = α01 and λ = λ01 and restrict the range of the variables to µ ∈ [µmin, µmax] = [−0.1, 0.1],
ω ∈ [ωmin, ωmax] = [−0.1, 0.1], ν ∈ [νmin, νmax] = [0.8, 1.5], and τ ∈ [τmin, τmax] = [0.8, 1.25].

The distance of the singular value at S(µ, ω, ν, τ, λ01, α01) and that at S(µ + ∆µ, ω + ∆ω, ν +
∆ν, τ + ∆τ, λ01, α01) is bounded as follows:

|S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) − S(µ, ω, ν, τ, λ01, α01)| <

(90)

27

0.32112 |∆µ| + 2.63690 |∆ω| + 2.28242 |∆ν| + 2.98610 |∆τ | .

Proof. The mean value theorem states that a t ∈ [0, 1] exists for which

from which immediately follows that

(µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∆µ +

(µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∆ω +

S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) − S(µ, ω, ν, τ, λ01, α01) =
∂S
∂µ
∂S
∂ω
∂S
∂ν
∂S
∂τ

(µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∆ν +

(µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01) ∆τ

|S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) − S(µ, ω, ν, τ, λ01, α01)| (cid:54)
(cid:12)
∂S
(cid:12)
(cid:12)
∂µ
(cid:12)
(cid:12)
∂S
(cid:12)
(cid:12)
∂ω
(cid:12)
(cid:12)
∂S
(cid:12)
(cid:12)
∂ν
(cid:12)
(cid:12)
∂S
(cid:12)
(cid:12)
∂τ
(cid:12)

(cid:12)
(cid:12)
(µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ01, α01)
(cid:12)
(cid:12)

|∆ω| +

|∆µ| +

|∆ν| +

|∆τ | .

We now apply Lemma 10 which gives bounds on the derivatives, which immediately gives the
statement of the lemma.

Lemma 12 (Largest Singular Value Smaller Than One). We set α = α01 and λ = λ01 and restrict
the range of the variables to µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].

The the largest singular value of the Jacobian is smaller than 1:

S(µ, ω, ν, τ, λ01, α01) < 1 .

Therefore the mapping Eq. (4) and Eq. (5) is a contraction mapping.

Proof. We set ∆µ = 0.0068097371, ∆ω = 0.0008292885, ∆ν = 0.0009580840, and ∆τ =
0.0007323095.

According to Lemma 11 we have

|S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) − S(µ, ω, ν, τ, λ01, α01)| <
0.32112 · 0.0068097371 + 2.63690 · 0.0008292885+
2.28242 · 0.0009580840 + 2.98610 · 0.0007323095 < 0.008747 .

For a grid with grid length ∆µ = 0.0068097371, ∆ω = 0.0008292885, ∆ν = 0.0009580840, and
∆τ = 0.0007323095, we evaluated the function Eq. (61) for the largest singular value in the domain
µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25]. We did this using a computer.
According to Subsection A3.4.5 the precision if regarding error propagation and precision of the
implemented functions is larger than 10−13. We performed the evaluation on different operating
systems and different hardware architectures including CPUs and GPUs. In all cases the function
Eq. (61) for the largest singular value of the Jacobian is bounded by 0.9912524171058772.

We obtain from Eq. (94):

S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01) (cid:54) 0.9912524171058772 + 0.008747 < 1 .

(91)

(92)

(93)

(94)

(95)

28

A3.4.2 Lemmata for prooﬁng Theorem 1 (part 2): Mapping within domain

We further have to investigate whether the the mapping Eq. (4) and Eq. (5) maps into a predeﬁned
domains.
Lemma 13 (Mapping into the domain). The mapping Eq. (4) and Eq. (5) map for α = α01 and
λ = λ01 into the domain µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617] with ω ∈ [−0.1, 0.1]
and τ ∈ [0.95, 1.1].

Proof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and
Eq. (5) with respect to α = α01 and λ = λ01 are either positive or have the sign of ω. Therefore with
given sign of ω the mappings are strict monotonic and the their maxima and minima are found at the
borders. The minimum of ˜µ is obtained at µω = −0.01 and its maximum at µω = 0.01 and σ and τ
at their minimal and maximal values, respectively. It follows that:

−0.03106 < ˜µ(−0.1, 0.1, 0.8, 0.95, λ01, α01) (cid:54)˜µ (cid:54) ˜µ(0.1, 0.1, 1.5, 1.1, λ01, α01) < 0.06773,

(96)

and that ˜µ ∈ [−0.1, 0.1].
Similarly, the maximum and minimum of ˜ξ( is obtained at the values mentioned above:

0.80467 < ˜ξ(−0.1, 0.1, 0.8, 0.95, λ01, α01) (cid:54) ˜ξ (cid:54) ˜ξ(0.1, 0.1, 1.5, 1.1, λ01, α01) < 1.48617.

(97)

Since | ˜ξ − ˜ν| = |˜µ2| < 0.004597, we can conclude that 0.80009 < ˜ν < 1.48617 and the variance
remains in [0.8, 1.5].

Corollary 14. The image g(Ω(cid:48)) of the mapping g : (µ, ν) (cid:55)→ (˜µ, ˜ν) (Eq. (8)) and the domain
Ω(cid:48) = {(µ, ν)| − 0.1 (cid:54) µ (cid:54) 0.1, 0.8 (cid:54) µ (cid:54) 1.5} is a subset of Ω(cid:48):

g(Ω(cid:48)) ⊆ Ω(cid:48),

(98)

for all ω ∈ [−0.1, 0.1] and τ ∈ [0.95, 1.1].

Proof. Directly follows from Lemma 13.

A3.4.3 Lemmata for prooﬁng Theorem 2: The variance is contracting

Main Sub-Function. We consider the main sub-function of the derivate of second moment, J22
(Eq. (54)):

(cid:18)

∂
∂ν

1
2

˜ξ =

λ2τ

−α2eµω+ ντ

2 erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

+ 2α2e2µω+2ντ erfc

(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

− erfc

(cid:19)

(cid:19)

+ 2

(cid:18) µω
√
√
2
ντ
(99)

that depends on µω and ντ , therefore we set x = ντ and y = µω. Algebraic reformulations provide
the formula in the following form:

˜ξ =

λ2τ

α2

(cid:18)

(cid:18)

−e− y2

2x

(cid:19) (cid:18)
e

(x+y)2
2x

erfc

∂
∂ν

1
2

(cid:19)

(cid:18) y + x
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)(cid:19)

(cid:18) y + 2x
√
√
x
2

− erfc

(cid:19)

(cid:19)

+ 2

(cid:18) y
√
√
2
(100)

x

For λ = λ01 and α = α01, we consider the domain −1 (cid:54) µ (cid:54) 1, −0.1 (cid:54) ω (cid:54) 0.1, 1.5 (cid:54) ν (cid:54) 16,
and, 0.8 (cid:54) τ (cid:54) 1.25.
For x and y we obtain: 0.8 · 1.5 = 1.2 (cid:54) x (cid:54) 20 = 1.25 · 16 and 0.1 · (−1) = −0.1 (cid:54) y (cid:54) 0.1 =
0.1 · 1. In the following we assume to remain within this domain.

29

Figure A3: Left panel: Graphs of the main subfunction f (x, y) = e

(2x+y)2
2x

(cid:16) 2x+y√

(cid:17)

erfc

2e
treated in Lemma 15. The function is negative and monotonically increasing
with x independent of y. Right panel: Graphs of the main subfunction at minimal x = 1.2. The
graph shows that the function f (1.2, y) is strictly monotonically decreasing in y.

√

x

2

(x+y)2
2x

erfc

(cid:17)

(cid:16) x+y√

√

2

x

−

Lemma 15 (Main subfunction). For 1.2 (cid:54) x (cid:54) 20 and −0.1 (cid:54) y (cid:54) 0.1,

the function

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

(101)

is smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing
in y for the minimal x = 12/10 = 1.2.

Proof. See proof 44.

The graph of the subfunction in the speciﬁed domain is displayed in Figure A3.
Theorem 16 (Contraction ν-mapping). The mapping of the variance ˜ν(µ, ω, ν, τ, λ, α) given in
Eq. (5) is contracting for λ = λ01, α = α01 and the domain Ω+: −0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1,
1.5 (cid:54) ν (cid:54) 16, and 0.8 (cid:54) τ (cid:54) 1.25, that is,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂ν

(cid:12)
(cid:12)
˜ν(µ, ω, ν, τ, λ01, α01)
(cid:12)
(cid:12)

< 1 .

Proof. In this domain Ω+ we have the following three properties (see further below): ∂
∂ν
˜µ > 0, and ∂

∂ν ˜µ > 0. Therefore, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂ν

(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜ν

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂ν

˜ξ − 2˜µ

∂
∂ν

(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)

<

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂ν

(cid:12)
(cid:12)
˜ξ
(cid:12)
(cid:12)

< 1

• We ﬁrst proof that ∂
∂ν

˜ξ < 1 in an even larger domain that fully contains Ω+. According to

Eq. (54), the derivative of the mapping Eq. (5) with respect to the variance ν is

(102)

˜ξ < 1,

(103)

(104)

˜ξ(µ, ω, ν, τ, λ01, α01) =

(cid:18)

λ2τ

α2 (cid:0)−eµω+ ντ

2 (cid:1) erfc

∂
∂ν
1
2

2α2e2µω+2ντ erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)

+ 2

.

(cid:19)

+

√

(cid:18) µω + ντ
√
ντ
2
(cid:19)

− erfc

30

For λ = λ01, α = α01, −1 (cid:54) µ (cid:54) 1, −0.1 (cid:54) ω (cid:54) 0.1 1.5 (cid:54) ν (cid:54) 16, and 0.8 (cid:54) τ (cid:54) 1.25,
we ﬁrst show that the derivative is positive and then upper bound it.

According to Lemma 15, the expression
(cid:18) µω + ντ
√
ντ
2

(µω+ντ )2
2ντ

erfc

√

e

(cid:19)

(µω+2ντ )2
2ντ

− 2e

erfc

(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

is negative. This expression multiplied by positive factors is subtracted in the derivative
Eq. (104), therefore, the whole term is positive. The remaining term

2 − erfc

(cid:19)

(cid:18) µω
√
√
2

ντ

of the derivative Eq. (104) is also positive according to Lemma 21. All factors outside the
brackets in Eq. (104) are positive. Hence, the derivative Eq. (104) is positive.

(105)

(106)

(107)

1
2

1
2

1
2

1
2

1
2

1
2

The upper bound of the derivative is:

(cid:18)

λ2
01τ

α2
01

(cid:0)−eµω+ ντ

2 (cid:1) erfc

(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

+

2α2

01e2µω+2ντ erfc

√

(cid:18) µω + 2ντ
√
ντ
2
(cid:19) (cid:18)
e

(cid:18)

(cid:18)

λ2
01τ

α2
01

−e− µ2ω2

2ντ

(µω+ντ )2
2ντ

erfc

(µω+2ντ )2
2ντ

2e

erfc

(cid:18)

(cid:18)

1.25λ2
01

α2
01

(cid:19)(cid:19)

√

(cid:18) µω + 2ντ
√
2
ντ
−e− µ2 ω2

2ντ

(cid:19) (cid:18)
e

− erfc

(µω+ντ )2
2ντ

(cid:19)

− erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)

+ 2

=

(cid:19)

√

(cid:18) µω + ντ
√
ντ
2
(cid:19)
(cid:18) µω
√
√
2
ντ
(cid:18) µω + ντ
√
2
ντ
(cid:19)

erfc

√

−

(cid:19)

+ 2

(cid:54)

(cid:19)

−

(cid:19)

+ 2

(cid:54)

(µω+2ντ )2
2ντ

2e

erfc

(cid:18)

(cid:18)

1.25λ2
01

α2
01

√

(cid:18) µω + 2ντ
√
ντ
2
(cid:17)2
(cid:16) 1.2+0.1
√
e
1.2
2

√

(cid:19)(cid:19)

erfc

(cid:16) 2·1.2+0.1
√
1.2
2

√

(cid:17)2

2e

erfc

(cid:18)

(cid:18)

√

(cid:18) 2 · 1.2 + 0.1
√
2
1.2
(cid:16) 1.2+0.1
√
1.2
2

(cid:18)

√

(cid:17)2

e

(cid:17)2

√

erfc

−e0.0α2
01
(cid:18) 2 · 1.2 + 0.1
√
2
1.2
(cid:16) 1.2+0.1
√
1.2
2

−e0.0α2
01
(cid:18) 2 · 1.2 + 0.1
√
2

erfc

1.2

√

(cid:18)

(cid:17)2

e

√

(cid:17)2

1.25λ2
01

(cid:16) 2·1.2+0.1
√
1.2
2

√

2e

1.25λ2
01

(cid:16) 2·1.2+0.1
√
1.2
2

√

2e

0.995063 < 1 .

We explain the chain of inequalities:

ντ

− erfc

(cid:18) µω
√
√
2
(cid:19)
(cid:18) 1.2 + 0.1
√
1.2
2
(cid:19)(cid:19) (cid:18)
−e− µ2 ω2

√

2ντ

−

(cid:19)

erfc

(cid:19)(cid:19)

erfc

(cid:19)(cid:19)

−

ντ

−

− erfc

(cid:19)

√

(cid:18) 1.2 + 0.1
√
1.2
2
(cid:18) µω
√
√
2
(cid:18) 1.2 + 0.1
√
2
1.2
(cid:18) 0.1
√
√
2

√

(cid:19)

− erfc

1.2

(cid:19)

(cid:19)

+ 2

(cid:54)

(cid:19)

(cid:19)

+ 2

(cid:54)

− erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)

+ 2

(cid:54)

– First equality brings the expression into a shape where we can apply Lemma 15 for the

the function Eq. (101).

– First inequality: The overall factor τ is bounded by 1.25.
– Second inequality: We apply Lemma 15. According to Lemma 15 the function
Eq. (101) is negative. The largest contribution is to subtract the most negative value
of the function Eq. (101), that is, the minimum of function Eq. (101). According to
Lemma 15 the function Eq. (101) is strictly monotonically increasing in x and strictly
monotonically decreasing in y for x = 1.2. Therefore the function Eq. (101) has its
minimum at minimal x = ντ = 1.5 · 0.8 = 1.2 and maximal y = µω = 1.0 · 0.1 = 0.1.
We insert these values into the expression.

31

– Third inequality: We use for the whole expression the maximal factor e− µ2ω2

2ντ < 1 by

setting this factor to 1.

– Fourth inequality: erfc is strictly monotonically decreasing. Therefore we maximize its
argument to obtain the least value which is subtracted. We use the minimal x = ντ =
1.5 · 0.8 = 1.2 and the maximal y = µω = 1.0 · 0.1 = 0.1.

– Sixth inequality: evaluation of the terms.

• We now show that ˜µ > 0. The expression ˜µ(µ, ω, ν, τ ) (Eq. (4)) is strictly monoton-
ically increasing im µω and ντ . Therefore, the minimal value in Ω+ is obtained at
˜µ(0.01, 0.01, 1.5, 0.8) = 0.008293 > 0.

• Last we show that ∂

∂ν ˜µ > 0. The expression ∂

∂ν ˜µ(µ, ω, ν, τ ) = J12(µ, ω, ν, τ ) (Eq. (54))

can we reformulated as follows:

J12(µ, ω, ν, τ, λ, α) =

λτ e− µ2ω2

2ντ

(cid:16)√

(µω+ντ )2
2ντ

παe

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

−

√

(cid:17)

2(α−1)
√
ντ

(108)

is larger than
is larger than zero when the term
zero. This term obtains its minimal value at µω = 0.01 and ντ = 16 · 1.25, which can
easily be shown using the Abramowitz bounds (Lemma 22) and evaluates to 0.16, therefore
J12 > 0 in Ω+.

παe

erfc

−

√

(µω+ντ )2
2ντ

(cid:17)

√

2(α−1)
√
ντ

erfc
√
4

π
(cid:16) µω+ντ
√
√
ντ
2

A3.4.4 Lemmata for prooﬁng Theorem 3: The variance is expanding

Main Sub-Function From Below. We consider functions in µω and ντ , therefore we set x = µω
and y = ντ .
For λ = λ01 and α = α01, we consider the domain −0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1 0.00875 (cid:54)
ν (cid:54) 0.7, and 0.8 (cid:54) τ (cid:54) 1.25.
For x and y we obtain: 0.8 · 0.00875 = 0.007 (cid:54) x (cid:54) 0.875 = 1.25 · 0.7 and 0.1 · (−0.1) = −0.01 (cid:54)
y (cid:54) 0.01 = 0.1 · 0.1. In the following we assume to be within this domain.

In this domain, we consider the main sub-function of the derivate of second moment in the next layer,
J22 (Eq. (54)):
(cid:18)

(cid:19)

(cid:19)

∂
∂ν

1
2

˜ξ =

λ2τ

−α2eµω+ ντ

2 erfc

(cid:18) µω + ντ
√
ντ
2

√

+ 2α2e2µω+2ντ erfc

− erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)

(cid:19)

+ 2

(cid:18) µω
√
√
2
ντ
(109)

that depends on µω and ντ , therefore we set x = ντ and y = µω. Algebraic reformulations provide
the formula in the following form:

˜ξ =
(cid:18)

∂
∂ν
1
2
Lemma 17 (Main subfunction Below). For 0.007 (cid:54) x (cid:54) 0.875 and −0.01 (cid:54) y (cid:54) 0.01, the function

(cid:18) y + 2x
√
√
x
2

(cid:18) y + x
√
√
x
2

(cid:18) y
√
√
2

(cid:19) (cid:18)
e

−e− y2

(2x+y)2
2x

(x+y)2
2x

− erfc

(110)

− 2e

λ2τ

erfc

erfc

(cid:19)(cid:19)

α2

(cid:18)

(cid:19)

(cid:19)

x

2x

(cid:19)

+ 2

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

(111)

smaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in
y for the minimal x = 0.007 = 0.00875 · 0.8, x = 0.56 = 0.7 · 0.8, x = 0.128 = 0.16 · 0.8, and
x = 0.216 = 0.24 · 0.9 (lower bound of 0.9 on τ ).

32

Proof. See proof 45.
Lemma 18 (Monotone Derivative). For λ = λ01, α = α01 and the domain −0.1 (cid:54) µ (cid:54) 0.1,
−0.1 (cid:54) ω (cid:54) 0.1, 0.00875 (cid:54) ν (cid:54) 0.7, and 0.8 (cid:54) τ (cid:54) 1.25. We are interested of the derivative of

(cid:18)
e

τ

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2·ντ
√
ντ
2

√

(cid:17)2

− 2e

erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)(cid:19)

.

(112)

The derivative of the equation above with respect to

• ν is larger than zero;

• τ is smaller than zero for maximal ν = 0.7, ν = 0.16, and ν = 0.24 (with 0.9 (cid:54) τ );

• y = µω is larger than zero for ντ = 0.008750.8 = 0.007, ντ = 0.70.8 = 0.56, ντ =

0.160.8 = 0.128, and ντ = 0.24 · 0.9 = 0.216.

Proof. See proof 46.

A3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1.

Error Analysis. We investigate the error propagation for the singular value (Eq. (61)) if the function
arguments µ, ω, ν, τ suffer from numerical imprecisions up to (cid:15). To this end, we ﬁrst derive error
propagation rules based on the mean value theorem and then we apply these rules to the formula for
the singular value.
Lemma 19 (Mean value theorem). For a real-valued function f which is differentiable in the closed
interval [a, b], there exists t ∈ [0, 1] with

f (a) − f (b) = ∇f (a + t(b − a)) · (a − b) .

It follows that for computation with error ∆x, there exists a t ∈ [0, 1] with

|f (x + ∆x) − f (x)| (cid:54) (cid:107)∇f (x + t∆x)(cid:107) (cid:107)∆x(cid:107) .

Therefore the increase of the norm of the error after applying function f is bounded by the norm of
the gradient (cid:107)∇f (x + t∆x)(cid:107).

We now compute for the functions, that we consider their gradient and its 2-norm:

• addition:

f (x) = x1 + x2 and ∇f (x) = (1, 1), which gives (cid:107)∇f (x)(cid:107) =
We further know that

√

2.

|f (x + ∆x) − f (x)| = |x1 + x2 + ∆x1 + ∆x2 − x1 − x2| (cid:54) |∆x1| + |∆x2| .

Adding n terms gives:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

xi + ∆xi −

xi

(cid:54)

|∆xi| (cid:54) n |∆xi|max .

(116)

• subtraction:

f (x) = x1 − x2 and ∇f (x) = (1, −1), which gives (cid:107)∇f (x)(cid:107) =
We further know that

√

2.

|f (x + ∆x) − f (x)| = |x1 − x2 + ∆x1 − ∆x2 − x1 + x2| (cid:54) |∆x1| + |∆x2| .

Subtracting n terms gives:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

33

−(xi + ∆xi) +

xi

(cid:54)

|∆xi| (cid:54) n |∆xi|max .

(118)

(113)

(114)

(115)

(117)

• multiplication:

f (x) = x1x2 and ∇f (x) = (x2, x1), which gives (cid:107)∇f (x)(cid:107) = (cid:107)x(cid:107).
We further know that

|f (x + ∆x) − f (x)| = |x1 · x2 + ∆x1 · x2 + ∆x2 · x1 + ∆x1 · ∆xs − x1 · x2| (cid:54)

(119)

|∆x1| |x2| + |∆x2| |x1| + O(∆2) .

Multiplying n terms gives:

(xi + ∆xi) −

xi

=

+ O(∆2)

(cid:54)

(120)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:89)

i=1
n
(cid:89)

|xi|

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∆xi
xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

i=1

+ O(∆2) (cid:54) n

∆xi
xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)max

+ O(∆2) .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:89)

i=1

n
(cid:88)

xi

i=1
n
(cid:89)

i=1

∆xi
xi
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|xi|

n
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16) 1
x2

, − x1
x2
2

, which gives (cid:107)∇f (x)(cid:107) = (cid:107)x(cid:107)
x2
2

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x1 + ∆x1
x2 + ∆x2

−

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x1
x2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(x1 + ∆x1)x2 − x1(x2 + ∆x2)
(x2 + ∆x2)x2

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(121)

• division:

f (x) = x1
x2
We further know that

and ∇f (x) =

|f (x + ∆x) − f (x)| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∆x1 · x2 − ∆x2 · x1
x2
2 + ∆x2 · x2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∆x1
x2

−

∆x2 · x1
x2
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ O(∆2) .

x and f (cid:48)(x) = 1
√
2

x , which gives |f (cid:48)(x)| = 1

√

2

x .

• square root:
f (x) =

√

• exponential function:

• error function:

f (x) = exp(x) and f (cid:48)(x) = exp(x), which gives |f (cid:48)(x)| = exp(x).

f (x) = erf(x) and f (cid:48)(x) = 2√

π exp(−x2), which gives |f (cid:48)(x)| = 2√

π exp(−x2).

• complementary error function:

f (x) = erfc(x) and f (cid:48)(x) = − 2√

π exp(−x2), which gives |f (cid:48)(x)| = 2√

π exp(−x2).

Lemma 20. If the values µ, ω, ν, τ have a precision of (cid:15), the singular value (Eq. (61)) evaluated
with the formulas given in Eq. (54) and Eq. (61) has a precision better than 292(cid:15).

This means for a machine with a typical precision of 2−52 = 2.220446 · 10−16, we have the rounding
error (cid:15) ≈ 10−16, the evaluation of the singular value (Eq. (61)) with the formulas given in Eq. (54)
and Eq. (61) has a precision better than 10−13 > 292(cid:15).

Proof. We have the numerical precision (cid:15) of the parameters µ, ω, ν, τ ,
∆µ, ∆ω, ∆ν, ∆τ together with our domain Ω.

that we denote by

With the error propagation rules that we derived in Subsection A3.4.5, we can obtain bounds for the
numerical errors on the following simple expressions:

(122)

(cid:54) (6(cid:15) + 1.25 · 1.5(cid:15))/4 < 2(cid:15)

∆ (µω) (cid:54) ∆µ |ω| + ∆ω |µ| (cid:54) 0.2(cid:15)
∆ (ντ ) (cid:54) ∆ν |τ | + ∆τ |ν| (cid:54) 1.5(cid:15) + 1.5(cid:15) = 3(cid:15)
(cid:16) ντ
2

1
22
∆ (µω + ντ ) (cid:54) ∆ (µω) + ∆ (ντ ) = 3.2(cid:15)
(cid:16)

(cid:54) (∆(ντ )2 + ∆2 |ντ |)

∆

(cid:17)

(cid:17)

(cid:17)

∆

(cid:54) ∆ (µω) + ∆

µω +
∆ (cid:0)√

ντ
2
ντ (cid:1) (cid:54) ∆ (ντ )
√
ντ
2

(cid:16) ντ
2
(cid:54) 3(cid:15)
√
2

0.64

< 2.2(cid:15)

= 1.875(cid:15)

34

(cid:16)√

(cid:17)
2

∆

(cid:16)√

√
2

ντ

(cid:17)

∆

(cid:54) ∆2
(cid:54) 1
√
√
2
2
2
√
2∆ (cid:0)√
ντ (cid:1) + ντ ∆

(cid:54)

2

(cid:15)

(cid:16)√

(cid:17)

2

(cid:54)

√

2 · 1.875(cid:15) + 1.5 · 1.25 ·

(cid:15) < 3.5(cid:15)

1
√
2

2

∆

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:16)

(cid:54)

∆ (µω)

ντ + |µω| ∆

√

√
2

(cid:16)√

√
2

ντ

(cid:17)(cid:17)

1
√

(cid:0)√

2

ντ (cid:1)2

(cid:54)

∆

(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

(cid:16)

(cid:54)

(cid:16)

√

√

0.2(cid:15)

2

0.64 + 0.01 · 3.5(cid:15)

< 0.25(cid:15)

(cid:17)

1
2 · 0.64

∆ (µω + ντ )

ντ + |µω + ντ | ∆

√

√
2

(cid:16)

√

√

3.2(cid:15)

2

0.64 + 1.885 · 3.5(cid:15)

(cid:17)

1
2 · 0.64

< 8(cid:15).

(cid:16)√

√
2

ντ

(cid:17)(cid:17)

1
√
2

(cid:0)√

ντ (cid:1)2

(cid:54)

Using these bounds on the simple expressions, we can now calculate bounds on the numerical errors
of compound expressions:

(cid:54) 2
√
π

2
√
π
(cid:54) 2
√
π

0.25(cid:15) < 0.3(cid:15)

(cid:18)

∆

erfc

(cid:18) µω
√
√
2

ντ

(cid:19)(cid:19)

(cid:16) µω
√
√
2

ντ

(cid:17)2

e−

∆

(cid:19)

(cid:18) µω
√
√
2

ντ

<

(cid:18)

∆

erfc

(cid:18) µω + ντ
√
ντ
2

√

(cid:19)(cid:19)

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

e−

∆

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

<

∆ (cid:0)eµω+ ντ

2 (cid:1) ∆ (cid:0)eµω+ ντ

2 (cid:1) <

8(cid:15) < 10(cid:15)

2
√
π
2 (cid:1) (cid:54) (cid:0)eµω+ ντ

e0.94752.2(cid:15) < 5.7(cid:15)

(123)

(124)

(125)

(126)

Subsequently, we can use the above results to get bounds for the numerical errors on the Jacobian
entries (Eq. (54)), applying the rules from Subsection A3.4.5 again:

∆ (J11) = ∆

λω

αeµω+ ντ

2 erfc

(cid:18)

(cid:18) 1
2

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

− erfc

(cid:18) µω
√
√
2

ντ

(cid:19)

(cid:19)(cid:19)

+ 2

< 6(cid:15),

(127)

and we obtain ∆ (J12) < 78(cid:15), ∆ (J21) < 189(cid:15), ∆ (J22) < 405(cid:15) and ∆ (˜µ) < 52(cid:15). We also have
bounds on the absolute values on Jij and ˜µ (see Lemma 6, Lemma 7, and Lemma 9), therefore we
can propagate the error also through the function that calculates the singular value (Eq. (61)).

∆ (S(µ, ω, ν, τ, λ, α)) =

(128)

∆

(cid:18) 1
2

(cid:16)(cid:112)(J11 + J22 − 2˜µJ12)2 + (J21 − 2˜µJ11 − J12)2 +

(cid:112)(J11 − J22 + 2˜µJ12)2 + (J12 + J21 − 2˜µJ11)2

(cid:17)(cid:17)

< 292(cid:15).

Precision of Implementations. We will show that our computations are correct up to 3 ulps. For
our implementation in GNU C library and the hardware architectures that we used, the precision of
all mathematical functions that we used is at least one ulp. The term “ulp” (acronym for “unit in the
last place”) was coined by W. Kahan in 1960. It is the highest precision (up to some factor smaller 1),
which can be achieved for the given hardware and ﬂoating point representation.

Kahan deﬁned ulp as [21]:

35

“Ulp(x) is the gap between the two ﬁnite ﬂoating-point numbers nearest x, even if
x is one of them. (But ulp(NaN) is NaN.)”

Harrison deﬁned ulp as [15]:

“an ulp in x is the distance between the two closest straddling ﬂoating point
numbers a and b, i.e. those with a (cid:54) x (cid:54) b and a (cid:54)= b assuming an unbounded
exponent range.”

In the literature we ﬁnd also slightly different deﬁnitions [29].

According to [29] who refers to [11]:

“IEEE-754 mandates four standard rounding modes:”
“Round-to-nearest: r(x) is the ﬂoating-point value closest to x with the usual
distance; if two ﬂoating-point value are equally close to x, then r(x) is the one
whose least signiﬁcant bit is equal to zero.”
“IEEE-754 standardises 5 operations: addition (which we shall note ⊕ in order to
distinguish it from the operation over the reals), subtraction ((cid:9)), multiplication
(⊗), division ((cid:11)), and also square root.”
“IEEE-754 speciﬁes em exact rounding [Goldberg, 1991, §1.5]: the result of a
ﬂoating-point operation is the same as if the operation were performed on the real
numbers with the given inputs, then rounded according to the rules in the preceding
section. Thus, x ⊕ y is deﬁned as r(x + y), with x and y taken as elements of
R ∪ {−∞, +∞}; the same applies for the other operators.”

Consequently, the IEEE-754 standard guarantees that addition, subtraction, multiplication, division,
and squared root is precise up to one ulp.

We have to consider transcendental functions. First the is the exponential function, and then the
complementary error function erfc(x), which can be computed via the error function erf(x).

Intel states [29]:

“With the Intel486 processor and Intel 387 math coprocessor, the worst- case,
transcendental function error is typically 3 or 3.5 ulps, but is some- times as large
as 4.5 ulps.”

According
//man.openbsd.org/OpenBSD-current/man3/exp.3:

https://www.mirbsd.org/htman/i386/man3/exp.htm

to

and

http:

“exp(x), log(x), expm1(x) and log1p(x) are accurate to within an ulp”

which is the same for freebsd https://www.freebsd.org/cgi/man.cgi?query=exp&sektion=
3&apropos=0&manpath=freebsd:

“The values of exp(0), expm1(0), exp2(integer), and pow(integer, integer) are exact
provided that they are representable. Otherwise the error in these functions is
generally below one ulp.”

The same holds for “FDLIBM” http://www.netlib.org/fdlibm/readme:

“FDLIBM is intended to provide a reasonably portable (see assumptions below),
reference quality (below one ulp for major functions like sin,cos,exp,log) math
library (libm.a).”

In
http://www.gnu.org/software/libc/manual/html_node/
Errors-in-Math-Functions.html we ﬁnd that both exp and erf have an error of 1 ulp
while erfc has an error up to 3 ulps depending on the architecture. For the most common architectures
as used by us, however, the error of erfc is 1 ulp.

We implemented the function in the programming language C. We rely on the GNU C Library
[26]. According to the GNU C Library manual which can be obtained from http://www.gnu.org/

36

Figure A4: Graphs of the upper and lower bounds on erfc. The lower bound

√

upper bound

(cid:17) (green) and the function erfc(x) (blue) as treated in Lemma 22.

(cid:16)√

2e−x2
x2+ 4

√

π

π +x

2e−x2
√
x2+2+x)

π(

(red), the

software/libc/manual/pdf/libc.pdf, the errors of the math functions exp, erf, and erfc are
not larger than 3 ulps for all architectures [26, pp. 528]. For the architectures ix86, i386/i686/fpu,
and m68k/fpmu68k/m680x0/fpu that we used the error are at least one ulp [26, pp. 528].

A3.4.6

Intermediate Lemmata and Proofs

Since we focus on the ﬁxed point (µ, ν) = (0, 1), we assume for our whole analysis that α = α01
and λ = λ01. Furthermore, we restrict the range of the variables µ ∈ [µmin, µmax] = [−0.1, 0.1],
ω ∈ [ωmin, ωmax] = [−0.1, 0.1], ν ∈ [νmin, νmax] = [0.8, 1.5], and τ ∈ [τmin, τmax] = [0.8, 1.25].

For bounding different partial derivatives we need properties of different functions. We will bound a
the absolute value of a function by computing an upper bound on its maximum and a lower bound
on its minimum. These bounds are computed by upper or lower bounding terms. The bounds get
tighter if we can combine terms to a more complex function and bound this function. The following
lemmata give some properties of functions that we will use in bounding complex functions.

Throughout this work, we use the error function erf(x) := 1√
π
function erfc(x) = 1 − erf(x).
Lemma 21 (Basic functions). exp(x) is strictly monotonically increasing from 0 at −∞ to ∞ at ∞
and has positive curvature.

and the complementary error

(cid:82) x
−x e−t2

According to its deﬁnition erfc(x) is strictly monotonically decreasing from 2 at −∞ to 0 at ∞.

Next we introduce a bound on erfc:
Lemma 22 (Erfc bound from Abramowitz).

2e−x2
x2 + 2 + x(cid:1) < erfc(x) (cid:54)

√

π (cid:0)√

2e−x2

√

(cid:16)(cid:113)

π

x2 + 4

π + x

(cid:17) ,

(129)

for x > 0.

Proof. The statement follows immediately from [1] (page 298, formula 7.1.13).

These bounds are displayed in ﬁgure A4.

37

Figure A5: Graphs of the functions ex2
and Lemma 24, respectively.

erfc(x) (left) and xex2

erfc(x) (right) treated in Lemma 23

Lemma 23 (Function ex2
has positive curvature (positive 2nd order derivative), that is, the decreasing slowes down.

erfc(x) is strictly monotonically decreasing for x > 0 and

erfc(x)). ex2

A graph of the function is displayed in Figure A5.

Proof. The derivative of ex2

erfc(x) is

∂ex2

erfc(x)
∂x

= 2ex2

x erfc(x) −

2
√
π

.

Using Lemma 22, we get

∂ex2

erfc(x)
∂x

= 2ex2

x erfc(x) −

2
√
π

<

4x

√

(cid:16)(cid:113)

π

x2 + 4

π + x

(cid:17) −

2
√
π

=

erfc(x) is strictly monotonically decreasing for x > 0.

Thus ex2
The second order derivative of ex2

erfc(x) is

∂2ex2

erfc(x)

∂x2

= 4ex2

x2 erfc(x) + 2ex2

erfc(x) −

4x
√
π

.

Again using Lemma 22 (ﬁrst inequality), we get

(cid:32)

2

(cid:113) 4

2
πx2 +1+1
√
π

< 0

(130)

(cid:33)

− 1

(131)

(132)

(133)

(cid:18)

2

(cid:0)2x2 + 1(cid:1) ex2

erfc(x) −

(cid:19)

2x
√
π

>

=

√

4x
√
π

4 (cid:0)2x2 + 1(cid:1)
π (cid:0)√
x2 + 2 + x(cid:1) −
√
4 (cid:0)x2 −
x2 + 2x + 1(cid:1)
π (cid:0)√
x2 + 2 + x(cid:1) =
√
√
x4 + 2x2 + 1(cid:1)
4 (cid:0)x2 −
π (cid:0)√
√
x2 + 2 + x(cid:1)
√
4 (cid:0)x2 −
x4 + 2x2 + 1 + 1(cid:1)
π (cid:0)√
√
x2 + 2 + x(cid:1)

>

= 0

38

For the last inequality we added 1 in the numerator in the square root which is subtracted, that is,
making a larger negative term in the numerator.

erfc(x)). The function xex2

erfc(x) has the sign of x and is mono-

Lemma 24 (Properties of xex2
tonically increasing to 1√

π .

Proof. The derivative of xex2

erfc(x) is

This derivative is positive since

2ex2

x2 erfc(x) + ex2

erfc(x) −

2x
√
π

=

2ex2

x2 erfc(x) + ex2

erfc(x) −

2x
√
π

.

(134)

(135)

ex2 (cid:0)2x2 + 1(cid:1) erfc(x) −

2 (cid:0)x2 − x
π (cid:0)√
√

√
x2 + 2 + 1(cid:1)
x2 + 2 + x(cid:1) =

2x
√
π

>

√

2 (cid:0)2x2 + 1(cid:1)
π (cid:0)√
x2 + 2 + x(cid:1) −
√
2 (cid:0)x2 − x
x2 + 2 + 1(cid:1)
π (cid:0)√
x2 + 2 + x(cid:1) >
√
(cid:18)

(cid:113)

2 (cid:0)x2 −
√

√
π (cid:0)√

x4 + 2x2 + 1 + 1(cid:1)
x2 + 2 + x(cid:1)

2

=

x2 −
√

π (cid:0)√

(x2 + 1)2 + 1

x2 + 2 + x(cid:1)

x2 − x
√

(cid:19)

= 0 .

x2 + 2 + x(cid:1)(cid:1)

=

=

2x
√
π
(cid:16)

2

2 (cid:0)(cid:0)2x2 + 1(cid:1) − x (cid:0)√
π (cid:0)√
√
x2 + 1
x2 + 2 + x(cid:1)

(cid:113)
π (cid:0)√

(cid:17)
x2 + 2 + 1

x2 + 2 + x(cid:1)

=

We apply Lemma 22 to x erfc(x)ex2

and divide the terms of the lemma by x, which gives

2
(cid:16)(cid:113) 2

√

π

x2 + 1 + 1

(cid:17) < x erfc(x)ex2 (cid:54)

2
(cid:16)(cid:113) 4

√

π

πx2 + 1 + 1

(cid:17) .

(136)

For limx→∞ both the upper and the lower bound go to 1√

π .

Lemma 25 (Function µω). h11(µ, ω) = µω is monotonically increasing in µω. It has minimal value
t11 = −0.01 and maximal value T11 = 0.01.

Proof. Obvious.

Lemma 26 (Function ντ ). h22(ν, τ ) = ντ is monotonically increasing in ντ and is positive. It has
minimal value t22 = 0.64 and maximal value T22 = 1.875.

Proof. Obvious.
Lemma 27 (Function µω+ντ
ντ
ντ and µω. It has minimal value t1 = 0.5568 and maximal value T1 = 0.9734.

). h1(µ, ω, ν, τ ) = µω+ντ
ντ

√

√

√

√

2

2

is larger than zero and increasing in both

Proof. The derivative of the function µω+x√
√
x

2

with respect to x is

√

1
√
2

x

−

µω + x
√
2x3/2
2

=

2x − (µω + x)

√
2

2x3/2

=

x − µω
√
2x3/2
2

> 0 ,

(137)

since x > 0.8 · 0.8 and µω < 0.1 · 0.1.
Lemma 28 (Function µω+2ντ
ντ
ντ and µω. It has minimal value t2 = 1.1225 and maximal value T2 = 1.9417.

). h2(µ, ω, ν, τ ) = µω+2ντ
ντ

√

√

√

√

2

2

is larger than zero and increasing in both

Proof. The derivative of the function µω+2x√
√
x
√
√

2

=

−

2
x

µω + 2x
√
2x3/2
2

with respect to x is

4x − (µω + 2x)

√
2

2x3/2

=

2x − µω
√
2x3/2
2

> 0 .

(138)

39

Lemma 29 (Function
monotonically increasing in µω.
T3 = 0.0088388.

ντ

µω√
√
2

Proof. Obvious.

). h3(µ, ω, ν, τ ) = µω√
√

monotonically decreasing in ντ and
It has minimal value t3 = −0.0088388 and maximal value

ντ

2

(cid:16) µω√

(cid:17)2

(cid:16) µω√

(cid:17)2

has a minimum at 0 for µ = 0 or
Lemma 30 (Function
ω = 0 and has a maximum for the smallest ντ and largest |µω| and is larger or equal to zero. It has
minimal value t4 = 0 and maximal value T4 = 0.000078126.

). h4(µ, ω, ν, τ ) =

ντ

ντ

√

√

2

2

Proof. Obvious.

Lemma 31 (Function

√

2
π (α−1)
√
ντ

).

√

2
π (α−1)
√

ντ > 0 and decreasing in ντ .

Proof. Statements follow directly from elementary functions square root and division.
(cid:17)

(cid:17)

(cid:16) µω√

√

2

ντ

(cid:16) µω√

√

2

ντ

). 2 − erfc

> 0 and decreasing in ντ and

Lemma 32 (Function 2 − erfc
increasing in µω.

Proof. Statements follow directly from Lemma 21 and erfc.
(cid:113) 2
π

(ντ )3/2 − α√

(cid:16) (α−1)µω

(cid:17)

ντ

Lemma 33 (Function
(cid:113) 2
π

(ντ )3/2 − α√

(cid:16) (α−1)µω

(cid:17)

ντ

< 0 and increasing in both ντ and µω.

). For λ = λ01 and α = α01,

Proof. We consider the function

, which has the derivative with respect to x:

(cid:113) 2
π

(cid:16) (α−1)µω

x3/2 − α√

x

(cid:17)

(cid:114) 2
π

(cid:18) α

2x3/2

−

3(α − 1)µω
2x5/2

(cid:19)

.

This derivative is larger than zero, since

(cid:18)

(cid:114) 2
π

α
2(ντ )3/2

−

3(α − 1)µω
2(ντ )5/2

(cid:19)

>

(cid:16)

(cid:113) 2
π

(cid:17)

α − 3(α−1)µω

ντ
2(ντ )3/2

> 0 .

(140)

The last inequality follows from α − 3·0.1·0.1(α−1)

0.8·0.8

We next consider the function

, which has the derivative with respect to x:

> 0 for α = α01.
(cid:17)

(cid:113) 2
π

(cid:16) (α−1)x
(ντ )3/2 − α√
(cid:113) 2

ντ

π (α − 1)
(ντ )3/2

> 0 .

Lemma 34 (Function
(cid:113) 2
(cid:16) (−1)(α−1)µ2ω2
π
(ντ )3/2

(cid:113) 2
π

(cid:16) (−1)(α−1)µ2ω2
(ντ )3/2

+ −α+αµω+1
√

− α

ντ

+ −α+αµω+1
√
(cid:17)

ντ

√

√

(cid:17)

− α

ντ

). The function

ντ

< 0 is decreasing in ντ and increasing in µω.

Proof. We deﬁne the function

(cid:114) 2
π

(cid:18) (−1)(α − 1)µ2ω2
x3/2

+

−α + αµω + 1
√
x

(cid:19)

√

− α

x

which has as derivative with respect to x:
(cid:18) 3(α − 1)µ2ω2
2x5/2

(cid:114) 2
π

−

−α + αµω + 1
2x3/2

−

α
√

2

x

(cid:19)

=

40

(139)

(141)

(142)

(143)

√

1
2πx5/2

(cid:0)3(α − 1)µ2ω2 − x(−α + αµω + 1) − αx2(cid:1) .

The derivative of the term 3(α − 1)µ2ω2 − x(−α + αµω + 1) − αx2 with respect to x is −1 +
α − µωα − 2αx < 0, since 2αx > 1.6α. Therefore the term is maximized with the smallest value
for x, which is x = ντ = 0.8 · 0.8. For µω we use for each term the value which gives maximal
contribution. We obtain an upper bound for the term:
3(−0.1 · 0.1)2(α01 − 1) − (0.8 · 0.8)2α01 − 0.8 · 0.8((−0.1 · 0.1)α01 − α01 + 1) = −0.243569 .
(144)
Therefore the derivative with respect to x = ντ is smaller than zero and the original function is
decreasing in ντ

We now consider the derivative with respect to x = µω. The derivative with respect to x of the
function

(cid:114) 2
π

(cid:18)

√

−α

ντ −

(α − 1)x2
(ντ )3/2

+

−α + αx + 1
ντ

√

(cid:19)

(145)

is

(cid:113) 2

π (αντ − 2(α − 1)x)
(ντ )3/2
Since −2x(−1 + α) + ντ α > −2 · 0.01 · (−1 + α01) + 0.8 · 0.8α01 > 1.0574 > 0, the derivative
is larger than zero. Consequently, the original function is increasing in µω.

(146)

.

The maximal value is obtained with the minimal ντ = 0.8 · 0.8 and the maximal µω = 0.1 · 0.1. The
maximal value is

(cid:114) 2
π

(cid:18) 0.1 · 0.1α01 − α01 + 1

√

0.8 · 0.8

+

0.120.12(−1)(α01 − 1)
(0.8 · 0.8)3/2

√

(cid:19)

−

0.8 · 0.8α01

= −1.72296 .

Therefore the original function is smaller than zero.

Lemma 35 (Function
(cid:18) (α2−1)µω

(ντ )3/2 − 3α2

√

ντ

(cid:113) 2
π

(cid:113) 2
π
(cid:19)

(cid:18) (α2−1)µω

(ντ )3/2 − 3α2

√

ντ

(cid:19)

). For λ = λ01 and α = α01,

< 0 and increasing in both ντ and µω.

Proof. The derivative of the function
(cid:114) 2
π

(cid:32) (cid:0)α2 − 1(cid:1) µω
x3/2

−

3α2
√
x

(cid:33)

with respect to x is
(cid:114) 2
π

(cid:32)

3α2
2x3/2

−

3 (cid:0)α2 − 1(cid:1) µω
2x5/2

(cid:33)

=

3 (cid:0)α2x − (cid:0)α2 − 1(cid:1) µω(cid:1)
2πx5/2

√

> 0 ,

(149)

since α2x − µω(−1 + α2) > α2

010.8 · 0.8 − 0.1 · 0.1 · (−1 + α2

01) > 1.77387

The derivative of the function

with respect to x is

(147)

(148)

(150)

(151)

The maximal function value is obtained by maximal ντ = 1.5 · 1.25 and the maximal µω = 0.1 · 0.1.

(cid:113) 2
π

(cid:18) 0.1·0.1(α2

01−1)
(1.5·1.25)3/2 − 3α2

01
1.5·1.25

√

(cid:19)

= −4.88869. Therefore the function is

The maximal value is

negative.

(cid:114) 2
π

(cid:32) (cid:0)α2 − 1(cid:1) x
(ντ )3/2

−

3α2
√
ντ

(cid:33)

(cid:113) 2
(cid:0)α2 − 1(cid:1)
π
(ντ )3/2

> 0 .

41

Lemma

36

(cid:113) 2
π

(cid:18) (α2−1)µω

ντ − 3α2√

√

(Function
(cid:19)

ντ

< 0 is decreasing in ντ and increasing in µω.

(cid:113) 2
π

(cid:18) (α2−1)µω

ντ − 3α2√

√

(cid:19)

ντ

).

The

function

Proof. The derivative of the function
(cid:114) 2
π

(cid:32) (cid:0)α2 − 1(cid:1) µω
√

(cid:33)

− 3α2√

x

x

with respect to x is
(cid:114) 2
π

(cid:32)

(cid:0)α2 − 1(cid:1) µω
2x3/2

−

−

3α2
√
x
2

(cid:33)

=

− (cid:0)α2 − 1(cid:1) µω − 3α2x
2πx3/2

√

< 0 ,

(153)

since −3α2x − µω(−1 + α2) < −3α2

010.8 · 0.8 + 0.1 · 0.1(−1 + α2

01) < −5.35764.

The derivative of the function

with respect to x is

(cid:114) 2
π

(cid:32) (cid:0)α2 − 1(cid:1) x
√

ντ

(cid:33)

− 3α2√

ντ

(cid:113) 2
π

(cid:0)α2 − 1(cid:1)
√
ντ

> 0 .

The maximal function value is obtained for minimal ντ = 0.8 · 0.8 and the maximal µω = 0.1 ·

(cid:113) 2
π

(cid:18) 0.1·0.1(α2

01−1)

√

0.8·0.8

√

− 3

0.8 · 0.8α2
01

(cid:19)

= −5.34347. Thus, the function is

0.1. The value is

negative.

Lemma 37 (Function ντ e
erfc
increasing in ντ and decreasing in µω.

(µω+ντ )2
2ντ

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

). The function ντ e

(µω+ντ )2
2ντ

erfc

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

> 0 is

Proof. The derivative of the function

(µω+x)2
2x

xe

erfc

(cid:19)

(cid:18) µω + x
√
x
2

√

with respect to x is

(µω+x)2
2x

e

(cid:0)x(x + 2) − µ2ω2(cid:1) erfc

(cid:17)

(cid:16) µω+x√

√

2

x

This derivative is larger than zero, since

2x

2ντ

(µω+ντ )2
2ντ

e

(cid:0)ντ (ντ + 2) − µ2ω2(cid:1) erfc

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

0.4349 (cid:0)ντ (ντ + 2) − µ2ω2(cid:1)
2ντ
0.5 (cid:0)ντ (ντ + 2) − µ2ω2(cid:1)
2πντ
0.5 (cid:0)ντ (ντ + 2) − µ2ω2(cid:1) +

√

+

+

µω − ντ
√
√
ντ

2π

>

=

µω − ντ
√
√
2π
ντ
ντ (µω − ντ )

√

√

2πντ

=

42

+

µω − x
√
√
x
2π

.

+

µω − ντ
√
√
ντ

2π

>

(152)

(154)

(155)

(156)

(157)

(158)

−0.5µ2ω2 + µω

ντ + 0.5(ντ )2 − ντ

ντ + ντ

√

−0.5µ2ω2 + µω

ντ + (0.5ντ −

√

ντ )2 + 0.25(ντ )2

√

√

√

2πντ

√

2πντ

=

> 0 .

We explain this chain of inequalities:

• The ﬁrst inequality follows by applying Lemma 23 which says that e

is strictly monotonically decreasing. The minimal value that is larger than 0.4349 is taken
on at the maximal values ντ = 1.5 · 1.25 and µω = 0.1 · 0.1.

(µω+ντ )2
2ντ

erfc

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

• The second inequality uses 1

2 0.4349

2π = 0.545066 > 0.5.

√

• The equalities are just algebraic reformulations.

• The last inequality follows from −0.5µ2ω2 + µω

ντ + 0.25(ντ )2 > 0.25(0.8 · 0.8)2 −

√

0.5 · (0.1)2(0.1)2 − 0.1 · 0.1 ·

0.8 · 0.8 = 0.09435 > 0.

√

Therefore the function is increasing in ντ .
Decreasing in µω follows from decreasing of ex2
form the fact that erfc and the exponential function are positive and that ντ > 0.

erfc(x) according to Lemma 23. Positivity follows

Lemma 38 (Function ντ e
is increasing in ντ and decreasing in µω.

erfc

(µω+2ντ )2
2ντ

(cid:16) µω+2ντ
√
√
ντ
2

(cid:17)

). The function ντ e

(µω+2ντ )2
2ντ

erfc

(cid:16) µω+2ντ
√
√
ντ
2

(cid:17)

> 0

Proof. The derivative of the function

(µω+2x)2
2x

xe

erfc

(cid:19)

(cid:18) µω + 2x
√
2x
2

√

(159)

is

(µω+2x)2
4x

e

(cid:16)√

(µω+2x)2
4x

πe

(cid:0)2x(2x + 1) − µ2ω2(cid:1) erfc
√
2

πx

(cid:16) µω+2x
√
x
2

(cid:17)

√

+

x(µω − 2x)

(cid:17)

.

(160)

We only have to determine the sign of
2x) since all other factors are obviously larger than zero.

πe

√

(µω+2x)2
4x

(cid:0)2x(2x + 1) − µ2ω2(cid:1) erfc

(cid:16) µω+2x
√
x
2

(cid:17)

√

+

x(µω−

This derivative is larger than zero, since

√

πe

(µω+2ντ )2
4ντ

(cid:0)2ντ (2ντ + 1) − µ2ω2(cid:1) erfc

(cid:19)

(cid:18) µω + 2ντ
√
2

ντ

√

0.463979 (cid:0)2ντ (2ντ + 1) − µ2ω2(cid:1) +
− 0.463979µ2ω2 + µω
µω (cid:0)√

√

ντ − 0.463979µω(cid:1) + 0.85592(ντ )2 + (cid:0)ντ −

ντ (µω − 2ντ ) =

√

ντ (cid:1)2

ντ + 1.85592(ντ )2 + 0.927958ντ − 2ντ

ντ =

√

− 0.0720421ντ > 0 .

√

+

ντ (µω − 2ντ ) >

(161)

We explain this chain of inequalities:

follows

by

applying Lemma

23 which

says

that

• The

ﬁrst
(µω+2ντ )2
2ντ

inequality
(cid:16) µω+2ντ
√
√
ντ
2

(cid:17)

erfc

e
is strictly monotonically decreasing. The minimal value
that is larger than 0.261772 is taken on at the maximal values ντ = 1.5 · 1.25 and
µω = 0.1 · 0.1. 0.261772

π > 0.463979.

√

• The equalities are just algebraic reformulations.

43

• The last

inequality follows

0.0720421ντ > 0.85592 · (0.8 · 0.8)2 − 0.1 · 0.1 (cid:0)√
0.0720421 · 1.5 · 1.25 > 0.201766.

from µω (

√

ντ − 0.463979µω) + 0.85592(ντ )2 −
1.5 · 1.25 + 0.1 · 0.1 · 0.463979(cid:1) −

Therefore the function is increasing in ντ .
Decreasing in µω follows from decreasing of ex2
from the fact that erfc and the exponential function are positive and that ντ > 0.

erfc(x) according to Lemma 23. Positivity follows

Lemma 39 (Bounds on the Derivatives). The following bounds on the absolute values of the deriva-
tives of the Jacobian entries J11(µ, ω, ν, τ, λ, α), J12(µ, ω, ν, τ, λ, α), J21(µ, ω, ν, τ, λ, α), and
J22(µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ hold:

< 0.0031049101995398316

(162)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J11
∂µ
∂J11
∂ω
∂J11
∂ν
∂J11
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J12
∂µ
∂J12
∂ω
∂J12
∂ν
∂J12
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J21
∂µ
∂J21
∂ω
∂J21
∂ν
∂J21
∂τ

∂J22
∂µ
∂J22
∂ω
∂J22
∂ν
∂J22
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 1.055872374194189

< 0.031242911235461816

< 0.03749149348255419

< 0.031242911235461816

< 0.031242911235461816

< 0.21232788238624354

< 0.2124377655377270

< 0.02220441024325437

< 1.146955401845684

< 0.14983446469110305

< 0.17980135762932363

< 0.14983446469110305

< 0.14983446469110305

< 1.805740052651535

< 2.396685907216327

44

Proof. For each derivative we compute a lower and an upper bound and take the maximum of
the absolute value. A lower bound is determined by minimizing the single terms of the functions
that represents the derivative. An upper bound is determined by maximizing the single terms of
the functions that represent the derivative. Terms can be combined to larger terms for which the
maximum and the minimum must be known. We apply many previous lemmata which state properties
of functions representing single or combined terms. The more terms are combined, the tighter the
bounds can be made.

Next we go through all the derivatives, where we use Lemma 25, Lemma 26, Lemma 27, Lemma 28,
Lemma 29, Lemma 30, Lemma 21, and Lemma 23 without citing. Furthermore, we use the bounds
on the simple expressions t11,t22, ..., and T4 as deﬁned the aforementioned lemmata:

• ∂J11
∂µ

We use Lemma 31 and consider the expression αe
brackets. An upper bound on the maximum of is

(µω+ντ )2
2ντ

erfc

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

−

2
π (α−1)
√
ντ

in

√

α01et2

1 erfc(t1) −

= 0.591017 .

(163)

A lower bound on the minimum is

α01eT 2

1 erfc(T1) −

= 0.056318 .

(164)

(cid:113) 2

π (α01 − 1)
√
T22

(cid:113) 2

π (α01 − 1)
t22

√

Thus, an upper bound on the maximal absolute value is

1
2

λ01ω2

maxet4


α01et2

1 erfc(t1) −

(cid:113) 2



π (α01 − 1)
√
T22

 = 0.0031049101995398316 .

(165)

the

expression

− α(µω +

√

2
π (α−1)µω
ντ

√

• ∂J11
∂ω

We use Lemma 31 and consider
(cid:16) µω+ντ
√
√
ντ
2

in brackets.

(µω+ντ )2
2ντ

erfc

1)e

(cid:17)

An upper bound on the maximum is

(cid:113) 2

π (α01 − 1)T11
t22

√

A lower bound on the minimum is
(cid:113) 2

π (α01 − 1)t11
t22

√

− α01(t11 + 1)eT 2

1 erfc(T1) = −0.713808 .

(166)

− α01(T11 + 1)et2

1 erfc(t1) = −0.99987 .

(167)

This term is subtracted, and 2 − erfc(x) > 0, therefore we have to use the minimum and the
maximum for the argument of erfc.

Thus, an upper bound on the maximal absolute value is



(cid:113) 2


−et4



1
2

λ01

π (α01 − 1)t11
t22

√

− α01(T11 + 1)et2

1 erfc(t1)

 − erfc(T3) + 2

 =





(168)

1.055872374194189 .

45

• ∂J11
∂ν

We consider the term in brackets

(µω+ντ )2
2ντ

αe

erfc

(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

+

(cid:114) 2
π

(cid:18) (α − 1)µω
(ντ )3/2

−

α
√
ντ

(cid:19)

.

(169)

We apply Lemma 33 for the ﬁrst sub-term. An upper bound on the maximum is

α01et2

1 erfc(t1) +

(cid:32)

(cid:114) 2
π

(α01 − 1)T11
T 3/2
22

−

α01√
T22

A lower bound on the minimum is
(cid:114) 2
π

1 erfc(T1) +

α01eT 2

(cid:32)

(α01 − 1)t11
t3/2
22

−

α01√
t22

Thus, an upper bound on the maximal absolute value is

(cid:33)

(cid:33)

= 0.0104167 .

(170)

= −0.95153 .

(171)

−

λ01τmaxωmaxet4

α01eT 2

1 erfc(T1) +

(cid:32)

(cid:114) 2
π

(α01 − 1)t11
t3/2
22

−

α01√
t22

(cid:33)(cid:33)

= (172)

0.031242911235461816 .

• ∂J11
∂τ
We use the results of item ∂J11
bound on the maximal absolute value is

∂ν were the brackets are only differently scaled. Thus, an upper

−

λ01νmaxωmaxet4

α01eT 2

1 erfc(T1) +

0.03749149348255419 .

(cid:32)

(cid:114) 2
π

(α01 − 1)t11
t3/2
22

−

α01√
t22

(cid:33)(cid:33)

= (173)

• ∂J12
∂µ
Since ∂J12

∂µ = ∂J11

∂ν , an upper bound on the maximal absolute value is

−

λ01τmaxωmaxet4

α01eT 2

1 erfc(T1) +

(cid:32)

(cid:114) 2
π

(α01 − 1)t11
t3/2
22

−

α01√
t22

(cid:33)(cid:33)

= (174)

0.031242911235461816 .

• ∂J12
∂ω
We use the results of item ∂J11
bound on the maximal absolute value is

∂ν were the brackets are only differently scaled. Thus, an upper

−

λ01µmaxτmaxet4

α01eT 2

1 erfc(T1) +

0.031242911235461816 .

(cid:32)

(cid:114) 2
π

(α01 − 1)t11
t3/2
22

−

α01√
t22

(cid:33)(cid:33)

= (175)

(cid:32)

(cid:32)

(cid:32)

(cid:32)

1
4

1
4

1
4

1
4

maxet2

• ∂J12
∂ν
For the second term in brackets, we see that α01τ 2
α01τ 2
We now check different values for
(cid:114) 2
π

(cid:18) (−1)(α − 1)µ2ω2
ν5/2

1 erfc(t1) = 1.53644.

√

√

+

τ

mineT 2

1 erfc(T1) = 0.465793 and

τ (α + αµω − 1)
ν3/2

−

ατ 3/2
√
ν

(cid:19)

,

(176)

46

where we maximize or minimize all single terms.

A lower bound on the minimum of this expression is

(cid:32)

(cid:114) 2
π

(−1)(α01 − 1)µ2
ν5/2
τmin
min

√

maxω2

max

√

+

τmin(α01 + α01t11 − 1)
ν3/2
max

−

(cid:33)

α01τ 3/2
max
√
νmin

= (177)

− 1.83112 .

(cid:32)

(cid:114) 2
π

0.0802158 .

An upper bound on the maximum of this expression is

(−1)(α01 − 1)µ2
ν5/2
τmax
max

√

minω2

min

+

τmax(α01 + α01T11 − 1)
ν3/2
min

√

(cid:33)

−

α01τ 3/2
min
√
νmax

= (178)

An upper bound on the maximum is

(cid:32)

(cid:32)(cid:114) 2
π

(−1)(α01 − 1)µ2
ν5/2
τmax
max

√

minω2

min

−

α01τ 3/2
min
√
νmax

+

λ01et4

1
8
√

τmax(α01 + α01T11 − 1)
ν3/2
min

(cid:33)

+ α01τ 2

maxet2

1 erfc(t1)

= 0.212328 .

(cid:33)

A lower bound on the minimum is

(cid:16)

1
λ01et4
8
(cid:32)
(cid:114) 2
π

α01τ 2

mineT 2

1 erfc(T1) +

(−1)(α01 − 1)µ2
ν5/2
τmin
min

√

maxω2

max

− 0.179318 .

√

+

τmin(α01 + α01t11 − 1)
ν3/2
max

−

α01τ 3/2
max
√
νmin

(cid:33)(cid:33)

=

(179)

(180)

(181)

Thus, an upper bound on the maximal absolute value is

λ01et4

1
8
√

(cid:32)

(cid:32)(cid:114) 2
π

(−1)(α01 − 1)µ2
ν5/2
τmax
max

√

minω2

min

−

α01τ 3/2
min
√
νmax

+

(cid:33)

τmax(α01 + α01T11 − 1)
ν3/2
min

(cid:33)

+ α01τ 2

maxet2

1 erfc(t1)

= 0.21232788238624354 .

• ∂J12
∂τ

We use Lemma 34 to obtain an upper bound on the maximum of the expression of the
lemma:
(cid:114) 2
π

(cid:18) 0.12 · 0.12(−1)(α01 − 1)
(0.8 · 0.8)3/2

(0.1 · 0.1)α01 − α01 + 1
0.8 · 0.8

0.8 · 0.8α01 +

√

√

−

(cid:19)

= −1.72296 .

We use Lemma 34 to obtain an lower bound on the minimum of the expression of the lemma:
(cid:114) 2
π

(cid:18) 0.12 · 0.12(−1)(α01 − 1)
(1.5 · 1.25)3/2

(−0.1 · 0.1)α01 − α01 + 1
1.5 · 1.25

1.5 · 1.25α01 +

√

√

−

(cid:19)

Next we apply Lemma 37 for the expression ντ e
erfc
to obtain an upper bound on the maximum of this expression:
(cid:18) 1.5 · 1.25 − 0.1 · 0.1

(1.5·1.25−0.1·0.1)2
2·1.5·1.25

1.5 · 1.25e

√

√

α01 erfc

(cid:19)

2

1.5 · 1.25

(µω+ντ )2
2ντ

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

. We use Lemma 37

= 1.37381 .

(184)

(182)

= −2.2302 .

(183)

47

We use Lemma 37 to obtain an lower bound on the minimum of this expression:

0.8 · 0.8e

(0.8·0.8+0.1·0.1)2
2·0.8·0.8

α01 erfc

(cid:18) 0.8 · 0.8 + 0.1 · 0.1
√
2

0.8 · 0.8

√

(cid:19)

= 0.620462 .

(185)

Next we apply Lemma 23 for 2αe
sion is

(µω+ντ )2
2ντ

erfc

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

. An upper bound on this expres-

(0.8·0.8−0.1·0.1)2
20.8·0.8

2e

α01 erfc

(cid:18) 0.8 · 0.8 − 0.1 · 0.1
√
2

0.8 · 0.8

√

(cid:19)

= 1.96664 .

(186)

A lower bound on this expression is

(1.5·1.25+0.1·0.1)2
2·1.5·1.25

2e

α01 erfc

√

√

(cid:18) 1.5 · 1.25 + 0.1 · 0.1

(cid:19)

2

1.5 · 1.25

= 1.4556 .

(187)

The sum of the minimal values of the terms is −2.23019+0.62046+1.45560 = −0.154133.

The sum of the maximal values of the terms is −1.72295 + 1.37380 + 1.96664 = 1.61749.

Thus, an upper bound on the maximal absolute value is
(cid:18) t11 + T22
√
T22
2

(t11+T22 )2
2T22

α01T22e

λ01et4

erfc

1
8

√

(cid:18)

(cid:19)

+

(188)

(cid:32)

1 erfc(t1) +

(cid:114) 2
π
(cid:1)(cid:1) = 0.2124377655377270 .

(α01 − 1)T 2
11
t3/2
22

−

2α01et2
√

α01

t22

+

−α01 + α01T11 + 1
t22

√

−

An upper bound on the maximum is

• ∂J21
∂µ

(cid:16)

max

01ω2
λ2
0.0222044 .

01eT 2
α2

1 (cid:0)−e−T4(cid:1) erfc(T1) + 2α2

01et2

2et4 erfc(t2) − erfc(T3) + 2

= (189)

A upper bound on the absolute minimum is
1 (cid:0)−e−t4 (cid:1) erfc(t1) + 2α2

01et2
α2

max

(cid:16)

01ω2
λ2
0.00894889 .

Thus, an upper bound on the maximal absolute value is

01eT 2

2 eT4 erfc(T2) − erfc(t3) + 2

= (190)

1 (cid:0)−e−T4(cid:1) erfc(T1) + 2α2

01et2

2et4 erfc(t2) − erfc(T3) + 2

= (191)

(cid:17)

(cid:17)

(cid:17)

(cid:16)

max

01eT 2
α2

01ω2
λ2
0.02220441024325437 .

• ∂J21
∂ω

An upper bound on the maximum is

(cid:16)

λ2
01

01(2T11 + 1)et2
α2

2e−t4 erfc(t2) + 2T11(2 − erfc(T3)) +
(cid:114) 2
π

T22e−t4

1 (cid:0)−e−T4 (cid:1) erfc(T1) +

(cid:112)

(cid:33)

= 1.14696 .

01(t11 + 1)eT 2
α2

A lower bound on the minimum is

(cid:16)

λ2
01

01(T11 + 1)et2
α2

1 (cid:0)−e−t4 (cid:1) erfc(t1) +

01(2t11 + 1)eT 2
α2

2 e−T4 erfc(T2) + 2t11(2 − erfc(T3))+

(192)

(193)

48

(cid:33)

√

(cid:114) 2
π

t22e−T4

= −0.359403 .

Thus, an upper bound on the maximal absolute value is

(cid:16)

λ2
01

01(2T11 + 1)et2
α2

01(t11 + 1)eT 2
α2

2 e−t4 erfc(t2) + 2T11(2 − erfc(T3)) +
(cid:114) 2
π

T22e−t4

1 (cid:0)−e−T4(cid:1) erfc(T1) +

(cid:112)

(cid:33)

(194)

= 1.146955401845684 .

• ∂J21
∂ν

An upper bound on the maximum is

01τmaxωmaxe−t4
λ2

erfc(T1) + 4α2

01et2

2 erfc(t2) +


α2
01

(cid:16)

−eT 2

1

(cid:17)

0.149834 .

A lower bound on the minimum is

λ2
01τmaxωmaxe−t4

erfc(t1) + 4α2

01eT 2

2 erfc(T2) +


α2
01

(cid:16)

−et2

1

(cid:17)

− 0.0351035 .

Thus, an upper bound on the maximal absolute value is

01τmaxωmaxe−t4
λ2

erfc(T1) + 4α2

01et2

2 erfc(t2) +


α2
01

(cid:16)

−eT 2

1

(cid:17)

0.14983446469110305 .

• ∂J21
∂τ

An upper bound on the maximum is

01νmaxωmaxe−t4
λ2

erfc(T1) + 4α2

01et2

2 erfc(t2) +


α2
01

(cid:16)

−eT 2

1

(cid:17)

0.179801 .

A lower bound on the minimum is

01νmaxωmaxe−t4
λ2

erfc(t1) + 4α2

01eT 2

2 erfc(T2) +


α2
01

(cid:16)

−et2

1

(cid:17)

− 0.0421242 .

Thus, an upper bound on the maximal absolute value is

01νmaxωmaxe−t4
λ2

erfc(T1) + 4α2

01et2

2 erfc(t2) +


α2
01

(cid:16)

−eT 2

1

(cid:17)

1
2

1
2

1
2

1
2

1
2

1
2

(cid:113) 2

π (−1) (cid:0)α2
√
T22

01 − 1(cid:1)



 =

(195)

(cid:113) 2

π (−1) (cid:0)α2
√
t22

01 − 1(cid:1)



 =

(196)

(cid:113) 2

π (−1) (cid:0)α2
√
T22

01 − 1(cid:1)



 =

(197)

(cid:113) 2

π (−1) (cid:0)α2
√
T22

01 − 1(cid:1)



 =

(198)

(cid:113) 2

π (−1) (cid:0)α2
√
t22

01 − 1(cid:1)



 =

(199)

(cid:113) 2

π (−1) (cid:0)α2
√
T22

01 − 1(cid:1)



 =

(200)

0.17980135762932363 .

49

1
2

1
2

1
2

1
2

• ∂J22
∂µ
We use the fact that ∂J22

01τmaxωmaxe−t4
λ2

0.14983446469110305 .

• ∂J22
∂ω

∂µ = ∂J21

α2
01

−eT 2

(cid:16)

1

∂ν . Thus, an upper bound on the maximal absolute value is

01 − 1(cid:1)

(cid:113) 2

(cid:17)

erfc(T1) + 4α2

01et2

2 erfc(t2) +

 =

π (−1) (cid:0)α2
√
T22

(201)

An upper bound on the maximum is

01µmaxτmaxe−t4
λ2

erfc(T1) + 4α2

01et2

2 erfc(t2) +


α2
01

(cid:16)
−eT 2

1

(cid:17)

0.149834 .

A lower bound on the minimum is

01µmaxτmaxe−t4
λ2

erfc(t1) + 4α2

01eT 2

2 erfc(T2) +


α2
01

(cid:16)
−et2

1

(cid:17)

− 0.0351035 .

Thus, an upper bound on the maximal absolute value is

01µmaxτmaxe−t4
λ2

erfc(T1) + 4α2

01et2

2 erfc(t2) +


α2
01

(cid:16)
−eT 2

1

(cid:17)

(cid:113) 2

π (−1) (cid:0)α2
√
T22

01 − 1(cid:1)



 =

(202)

(cid:113) 2

π (−1) (cid:0)α2
√
t22

01 − 1(cid:1)



 =

(203)

(cid:113) 2

π (−1) (cid:0)α2
√
T22

01 − 1(cid:1)



 =

(204)

0.14983446469110305 .

• ∂J22
∂ν

We apply Lemma 35 to the expression

. Using Lemma 35, an

(cid:113) 2
π

(cid:18) (α2−1)µω

(ντ )3/2 − 3α2

√

ντ

(cid:19)

upper bound on the maximum is
(cid:16)
1
λ2
01τ 2
4
(cid:114) 2
π

maxe−t4
(cid:32) (cid:0)α2

α2
01
01 − 1(cid:1) T11
T 3/2
22

(cid:16)

(cid:17)

−eT 2

1

erfc(T1) + 8α2

01et2

2 erfc(t2) +

(205)

(cid:33)(cid:33)

−

3α2
01√
T22

= 1.19441 .

Using Lemma 35, a lower bound on the minimum is

(cid:16)

1
λ2
01τ 2
4
(cid:114) 2
π

maxe−t4
(cid:32) (cid:0)α2

α2
01
01 − 1(cid:1) t11
t3/2
22

(cid:16)

−et2

1

(cid:17)

−

3α2
01√
t22

erfc(t1) + 8α2
(cid:33)(cid:33)

= −1.80574 .

01eT 2

2 erfc(T2) +

(206)

Thus, an upper bound on the maximal absolute value is

(cid:16)

(cid:16)

−et2

1

(cid:17)

1
−
4
(cid:114) 2
π

01τ 2
λ2
(cid:32) (cid:0)α2

maxe−t4
α2
01
01 − 1(cid:1) t11
t3/2
22

−

3α2
01√
t22

50

erfc(t1) + 8α2
(cid:33)(cid:33)

01eT 2

2 erfc(T2) +

(207)

= 1.805740052651535 .

• ∂J22
∂τ

We apply Lemma 36 to the expression

We apply Lemma 37 to the expression ντ e
(cid:16) µω+2ντ
√
√
ντ
2

the expression ντ e

(µω+2ντ )2
2ντ

erfc

(cid:17)

.

(cid:18) (α2−1)µω

√

(cid:113) 2
π

(cid:19)
.

ντ − 3α2√
(cid:16) µω+ντ
√
√
ντ
2

erfc

ντ

(cid:17)

(µω+ντ )2
2ντ

. We apply Lemma 38 to

We combine the results of these lemmata to obtain an upper bound on the maximum:

(cid:18)

λ2
01

−α2

01t22e−T4e

(T11+t22)2
2t22

1
4

8α2

01T22e−t4e

(t11+2T22)2
2T22

erfc

√

erfc

(cid:18) T11 + t22
√
t22
2
(cid:18) t11 + 2T22
√
T22
2

√

−

(cid:19)

(cid:19)

+

01eT 2

1 e−T4 erfc(T1) + 4α2

01et2

2 e−t4 erfc(t2) + 2(2 − erfc(T3)) +

e−T4

(cid:32) (cid:0)α2

01 − 1(cid:1) T11
√
t22

(cid:33)(cid:33)

√

− 3α2
01

t22

= 2.39669 .

2α2
(cid:114) 2
π

(cid:18)

We combine the results of these lemmata to obtain an lower bound on the minimum:

λ2
01

8α2

01t22e−T4 e

(T11+2t22 )2
2t22

erfc

1
4

(cid:19)

(cid:18) T11 + 2t22
√
t22
2

√

+

01T22e−t4e
α2

(t11+T22)2
2T22

erfc

2α2

01et2

1e−t4 erfc(t1) + 4α2

2(2 − erfc(t3)) +

(cid:114) 2
π

e−t4

(cid:19)

−

√

(cid:18) t11 + T22
√
2
T22
01eT 2
2 e−T4 erfc(T2) +
(cid:32) (cid:0)α2
01 − 1(cid:1) t11
√
T22

Thus, an upper bound on the maximal absolute value is

(cid:18)

λ2
01

−α2

01t22e−T4e

(T11+t22)2
2t22

1
4

8α2

01T22e−t4e

(t11+2T22)2
2T22

erfc

√

erfc

(cid:18) T11 + t22
√
2
t22
(cid:18) t11 + 2T22
√
T22
2

√

−

(cid:19)

(cid:19)

+

(cid:33)(cid:33)

− 3α2
01

(cid:112)

T22

= −1.17154 .

(208)

(209)

(210)

01eT 2

1 e−T4 erfc(T1) + 4α2

01et2

2 e−t4 erfc(t2) + 2(2 − erfc(T3)) +

2α2
(cid:114) 2
π

e−T4

(cid:32) (cid:0)α2

01 − 1(cid:1) T11
√
t22

(cid:33)(cid:33)

√

− 3α2
01

t22

= 2.396685907216327 .

Lemma 40 (Derivatives of the Mapping). We assume α = α01 and λ = λ01. We restrict the range
of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].

The derivative ∂

∂µ ˜µ(µ, ω, ν, τ, λ, α) has the sign of ω.

The derivative ∂

∂ν ˜µ(µ, ω, ν, τ, λ, α) is positive.

The derivative ∂
∂µ

˜ξ(µ, ω, ν, τ, λ, α) has the sign of ω.

The derivative ∂
∂ν

˜ξ(µ, ω, ν, τ, λ, α) is positive.

Proof.

• ∂

∂µ ˜µ(µ, ω, ν, τ, λ, α)

(2 − erfc(x) > 0 according to Lemma 21 and ex2
to Lemma 23. Consequently, has ∂

∂µ ˜µ(µ, ω, ν, τ, λ, α) the sign of ω.

erfc(x) is also larger than zero according

51

• ∂

∂ν ˜µ(µ, ω, ν, τ, λ, α)
Lemma 23 says ex2
erfc(x) is decreasing in µω+ντ
ντ
in ντ since it is proportional to minus one over the squared root of ντ .

√

√

2

. The ﬁrst term (negative) is increasing

We obtain a lower bound by setting µω+ντ
ντ

√

term. The term in brackets is larger than e
(cid:113) 2

2
π0.8·0.8 (α01 − 1) = 0.056 Consequently, the function is larger than zero.

α01 erfc

1.5·1.25

√

2

√

= 1.5·1.25+0.1·0.1
√
2
2
(cid:16) 1.5·1.25+0.1·0.1
(cid:17)2

1.5·1.25

√

√

√

for the ex2
(cid:16) 1.5·1.25+0.1·0.1
√
1.5·1.25

erfc(x)
(cid:17)

−

• ∂
∂µ

˜ξ(µ, ω, ν, τ, λ, α)

We consider the sub-function
(cid:16) µω+ντ
√
ντ
2

√

√

ντ − α2

(cid:18)
e

(cid:17)2

(cid:114) 2
π

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− e

erfc

(cid:18) µω + 2ντ
√
ντ
2

√

(cid:19)(cid:19)

.

(211)

We set x = ντ and y = µω and obtain
(cid:18)

(cid:17)2

x − α2

(cid:16) x+y
√
√
x
2

e

√

(cid:114) 2
π

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(cid:16) 2x+y
√
√
x
2

(cid:17)2

− e

erfc

(cid:19)(cid:19)

(cid:18) 2x + y
√
√
x
2

.

(212)

The derivative of this sub-function with respect to y is

α2 (cid:16)

e

(2x+y)2
2x

(2x + y) erfc

(cid:16) 2x+y√

√

(cid:17)

(x+y)2
2x

− e

(x + y) erfc

(cid:17)(cid:17)

(cid:16) x+y√

√

2

x

√

2α2√

x





(2x+y)2
2x

e

(x+y) erfc
√

√

2

x

(x+y)2
2x

e

−

(x+y) erfc

√

√

2

x

(cid:16) x+y
√
√
x
2

(cid:17)





2

x
x
(cid:16) x+y
√
√
x
2

(cid:17)

x

=

(213)

> 0 .

The inequality follows from Lemma 24, which states that zez2
increasing in z. Therefore the sub-function is increasing in y.

erfc(z) is monotonically

The derivative of this sub-function with respect to x is
√

(cid:17)

πα2 (cid:16)

e

(2x+y)2
2x

(cid:0)4x2 − y2(cid:1) erfc

(cid:16) 2x+y√

√

2

x

− e

(x − y)(x + y) erfc

(cid:17)(cid:17)

(cid:16) x+y√

√

2

x

−

√

2 (cid:0)α2 − 1(cid:1) x3/2

The sub-function is increasing in x, since the derivative is larger than zero:
√
(cid:16) 2x+y√

(cid:0)4x2 − y2(cid:1) erfc

(x − y)(x + y) erfc

πα2 (cid:16)

(2x+y)2
2x

− e

(cid:17)

e

√

2

x

(cid:17)(cid:17)

(cid:16) x+y√

√

2

x

−

√

2x3/2 (cid:0)α2 − 1(cid:1)

(214)

.

(cid:62)

(x+y)2
2x
√

2

πx2

(x+y)2
2x
√

2

πx2

(215)



√

2x3/2 (cid:0)α2 − 1(cid:1)

=

√

πα2






(cid:32)

√

π

(2x−y)(2x+y)2
(cid:114)(cid:16) 2x+y

(cid:17)2

2x+y
√
x
2

√

+

√

√

2

x

+2

(cid:33) −

(x−y)(x+y)2
(cid:114)(cid:16) x+y

x+y
√
2

x

+

(cid:17)2

+ 4
π

√

√

2

x

(cid:33)


 −

√

π

(cid:32)

√
√

2

πx2

√

πα2

(cid:18) (2x−y)(2x+y)2(

√

√

2

x)
(2x+y)2+4x

√

√

(cid:16)

π

2x+y+

(cid:17) −

(cid:18)

√

πα2

√

(cid:16)

(2x−y)(2x+y)2
√

π

2x+y+

(2x+y)2+4x

√

√

2

x)
(x+y)2+ 8x
π

(cid:17)

(cid:19)

√

−

√

√

(x−y)(x+y)2(
(cid:16)
π
√
2

πx2

x+y+

2x3/2 (cid:0)α2 − 1(cid:1)

=

(x−y)(x+y)2
√

(cid:16)

π

x+y+

(x+y)2+ 8x
π

(cid:17) −
√

√
√
2

πx3/2

(cid:19)

(cid:17)

− x (cid:0)α2 − 1(cid:1)

>

52

(cid:18)

√

πα2

√

(cid:16)

(2x−y)(2x+y)2
√

π

2x+y+

(2x+y)2+2(2x+y)+1

(x−y)(x+y)2

(x+y)2+0.782·2(x+y)+0.7822(cid:17)

(cid:19)

− x (cid:0)α2 − 1(cid:1)

=

(cid:18)

√

πα2

(2x−y)(2x+y)2
√

√

(cid:16)

π

2x+y+

(2x+y+1)2(cid:17) −
√
π
√
√
2

√

πα2 (cid:16) (2x−y)(2x+y)2

π(2(2x+y)+1) − (x−y)(x+y)2
√
2

√
π(2(x+y)+0.782)
√
πx3/2
πα2 (cid:16) (2(x+y)+0.782)(2x−y)(2x+y)2

√

π

√

√

√

(cid:17) −

√

√
√

π

2

(cid:16)
x+y+
√

πx3/2

(x−y)(x+y)2

√

(cid:16)

x+y+

(x+y+0.782)2(cid:17)

πx3/2
(cid:17)

− x (cid:0)α2 − 1(cid:1)

=

(cid:17)

− (x−y)(x+y)(2(2x+y)+1)2
√
√
2

π
πx3/2

√

+

(2(2x + y) + 1)(2(x + y) + 0.782)

πα2 (cid:0)−x (cid:0)α2 − 1(cid:1) (2(2x + y) + 1)(2(x + y) + 0.782)(cid:1)

(2(2x + y) + 1)(2(x + y) + 0.782)

√

√
2

πx3/2

=

(cid:19)

− x (cid:0)α2 − 1(cid:1)

=

8x3 + (12y + 2.68657)x2 + (y(4y − 6.41452) − 1.40745)x + 1.22072y2

(2(2x + y) + 1)(2(x + y) + 0.782)

√

√
2

πx3/2

>

8x3 + (2.68657 − 120.01)x2 + (0.01(−6.41452 − 40.01) − 1.40745)x + 1.22072(0.0)2

(2(2x + y) + 1)(2(x + y) + 0.782)

√

√
2

πx3/2

=

√

8x2 + 2.56657x − 1.472
(2(2x + y) + 1)(2(x + y) + 0.782)
8x2 + 2.56657x − 1.472
(2(2x + y) + 1)(2(x + y) + 0.782)

√
2
8(x + 0.618374)(x − 0.297553)
√
2

(2(2x + y) + 1)(2(x + y) + 0.782)

√
2

√

√

√

π

x

√

π

x

√

π

x

=

=

> 0 .

We explain this chain of inequalities:

– First inequality: We applied Lemma 22 two times.
√
2
– Equalities factor out
– Second inequality part 1: we applied

x and reformulate.

√

0 < 2y =⇒ (2x + y)2 + 4x + 1 < (2x + y)2 + 2(2x + y) + 1 = (2x + y + 1)2 .
(216)

– Second inequality part 2: we show that for a = 1
20

following
π −(cid:0)a2 + 2a(x + y)(cid:1) (cid:62) 0. We have ∂
π −2a > 0
π − (cid:0)a2 + 2a(x + y)(cid:1) = −2a > 0. Therefore the minimum is at border for

(cid:16)(cid:113) 2048+169π
π −(cid:0)a2 + 2a(x + y)(cid:1) = 8

− 13

∂x

(cid:17)

8x

π

holds: 8x
and ∂
8x
∂y
minimal x and maximal y:


8 · 0.64
π

−



2
20

π

(cid:32)(cid:114) 2048 + 169π

(cid:33)

− 13

(0.64 + 0.01) +

(cid:32)

1
20

(cid:32)(cid:114) 2048 + 169π

(cid:33)(cid:33)2

− 13

 = 0 .

π

Thus

for a = 1
20

(cid:16)(cid:113) 2048+169π

π

− 13

> 0.782.

(cid:62) a2 + 2a(x + y) .

8x
π
(cid:17)

– Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1)

and (2(x + y) + 0.782).

– We set α = α01 and multiplied out. Thereafter we also factored out x in the numerator.

Finally a quadratic equations was solved.

(217)

(218)

53

The sub-function has its minimal value for minimal x and minimal y x = ντ = 0.8 · 0.8 =
0.64 and y = µω = −0.1 · 0.1 = −0.01. We further minimize the function
(cid:18) 0.01
√
2

(cid:18) µω
√
√
2

> −0.01e

2 − erfc

2 − erfc

(219)

µ2ω2
2ντ

0.012
20.64

0.64

µωe

(cid:19)(cid:19)

(cid:19)(cid:19)

ντ

√

(cid:18)

(cid:18)

.

We compute the minimum of the term in brackets of ∂
∂µ

˜ξ(µ, ω, ν, τ, λ, α):

(cid:18)

µ2ω2
2ντ

µωe

2 − erfc

(cid:18) µω
√
√
2

ντ

(cid:19)(cid:19)

+

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:16) 0.64−0.01
√
0.64
2

√

(cid:17)2

erfc

(cid:18)

(cid:18)

−

−

(cid:18)

(cid:18)

α2
01

α2
01

e

e

(cid:18)

− e

(cid:19)

√

(cid:18) µω + ντ
√
ντ
2
(cid:18) 0.64 − 0.01
√
0.64
2
(cid:19)(cid:19)

√

√

(cid:19)

0.01e

0.012
20.64

2 − erfc

(cid:18) 0.01
√
2

√

0.64

Therefore the term in brackets is larger than zero.

Thus, ∂
∂µ

˜ξ(µ, ω, ν, τ, λ, α) has the sign of ω.

(220)

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:16) 20.64−0.01
√
0.64
2

√

− e

+

√

√

(cid:19)(cid:19)(cid:19)

(cid:114) 2
(cid:18) µω + 2ντ
√
π
ντ
2
(cid:19)(cid:19)(cid:19)
(cid:18) 2 · 0.64 − 0.01
√
2

0.64

erfc

√

(cid:17)2

−

ντ >

+

0.64

= 0.0923765 .

(cid:114) 2
π

˜ξ(µ, ω, ν, τ, λ, α)

• ∂
∂ν
We look at the sub-term
(cid:16) 2x+y
√
√
x
2

2e

We obtain a chain of inequalities:

(cid:17)2

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

(cid:16) x+y
√
√
x
2

(cid:17)2

− e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

.

(221)

(222)

(cid:19)

(cid:18) 2x + y
√
√
x
2

2 · 2
(cid:114)(cid:16) 2x+y√

√

2

x

(cid:16) x+y
√
√
x
2

(cid:17)2

− e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

>

(cid:33) −

(cid:32)

√

π

(cid:17)2

+ 2

2
(cid:114)(cid:16) x+y√

√

2

x

(cid:17)2

+ 4
π

(cid:33) =

x+y√
√
x
2

+

(cid:19)

2

(2x+y)2+4x+2x+y

−

√

1

(x+y)2+ 8x

π +x+y

√

π

2

>

1

(2x+y)2+2(2x+y)+1+2x+y

(x+y)2+0.782·2(x+y)+0.7822+x+y

(cid:19)

=

(cid:16) 2x+y
√
√
x
2

(cid:17)2

2e

erfc

(cid:32)

√

π

2x+y√
√
x
2

+

√

√
2

2

x

(cid:18)

√

√

√
2

2

x

(cid:18)

√

√

√
2

2

x

(cid:16)

√

(cid:0)2

√
2
√

√

2
2(2x+y)+1 −

1
2(x+y)+0.782

√

π

x(cid:1) (2(2(x + y) + 0.782) − (2(2x + y) + 1))
π((2(x + y) + 0.782)(2(2x + y) + 1))
√

x(cid:1) (2y + 0.782 · 2 − 1)

(cid:0)2

√
2

=

> 0 .

π((2(x + y) + 0.782)(2(2x + y) + 1))

We explain this chain of inequalities:

– First inequality: We applied Lemma 22 two times.
√
2
– Equalities factor out
– Second inequality part 1: we applied

x and reformulate.

√

−

√

√

π

(cid:17)

=

54

0 < 2y =⇒ (2x + y)2 + 4x + 1 < (2x + y)2 + 2(2x + y) + 1 = (2x + y + 1)2 .
(223)

– Second inequality part 2: we show that for a = 1
20

following
π −(cid:0)a2 + 2a(x + y)(cid:1) (cid:62) 0. We have ∂
π −2a > 0
π − (cid:0)a2 + 2a(x + y)(cid:1) = −2a < 0. Therefore the minimum is at border for

(cid:16)(cid:113) 2048+169π
π −(cid:0)a2 + 2a(x + y)(cid:1) = 8

− 13

∂x

(cid:17)

8x

π

holds: 8x
and ∂
8x
∂y
minimal x and maximal y:


8 · 0.64
π

−



2
20

π

(cid:32)(cid:114) 2048 + 169π

(cid:33)

− 13

(0.64 + 0.01) +

(cid:32)

1
20

(cid:32)(cid:114) 2048 + 169π

(cid:33)(cid:33)2

− 13

 = 0 .

π

(224)

(225)

(228)

(229)

(230)

Thus

for a = 1
20

(cid:16)(cid:113) 2048+169π

π

− 13

> 0.782.

(cid:62) a2 + 2a(x + y) .

8x
π
(cid:17)

– Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1)

and (2(x + y) + 0.782).

We know that (2 − erfc(x) > 0 according to Lemma 21. For the sub-term we derived

(cid:16) 2x+y
√
√
x
2

(cid:17)2

2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

(cid:16) x+y
√
√
x
2

(cid:17)2

− e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

> 0 .

(226)

Consequently, both terms in the brackets of ∂
∂ν
˜ξ(µ, ω, ν, τ, λ, α) is larger than zero.
fore ∂
∂ν

˜ξ(µ, ω, ν, τ, λ, α) are larger than zero. There-

Lemma 41 (Mean at low variance). The mapping of the mean ˜µ (Eq. (4))

˜µ(µ, ω, ν, τ, λ, α) =

λ

−(α + µω) erfc

(227)

(cid:18)

1
2

(cid:19)

(cid:18) µω
√
√
2

ντ

+

(cid:33)

ντ e− µ2ω2

2ντ + 2µω

αeµω+ ντ

2 erfc

(cid:18) µω + ντ
√
ντ
2

√

(cid:19)

+

√

(cid:114) 2
π

in the domain −0.1 (cid:54) µ (cid:54) −0.1, −0.1 (cid:54) ω (cid:54) −0.1, and 0.02 (cid:54) ντ (cid:54) 0.5 is bounded by

and

|˜µ(µ, ω, ν, τ, λ01, α01)| < 0.289324

lim
ν→0

|˜µ(µ, ω, ν, τ, λ01, α01)| = λµω.

We can consider ˜µ with given µω as a function in x = ντ . We show the graph of this function at the
maximal µω = 0.01 in the interval x ∈ [0, 1] in Figure A6.

Proof. Since ˜µ is strictly monotonically increasing with µω
˜µ(µ, ω, ν, τ, λ, α) (cid:54)
˜µ(0.1, 0.1, ν, τ, λ, α) (cid:54)

λ

−(α + 0.01) erfc

+ αe0.01+ ντ

2 erfc

(cid:19)

(cid:18) 0.01
√
√
2
ντ
(cid:18) 0.02 + 0.01
√
0.02
2

√

(cid:19)

√

(cid:114) 2
π

(cid:19)

+

√

(cid:18) 0.01 + ντ
√
ντ
2
(cid:18) 0.01
√
2

√

0.02

− (α01 + 0.01) erfc

ντ e− 0.012

2ντ + 2 · 0.01

(cid:54)

(cid:33)

(cid:19)

+ e− 0.012

2·0.5

√

0.5

(cid:114) 2
π

(cid:33)

+ 0.01 · 2

(cid:32)

1
2

1
2

(cid:32)
e

λ01

0.05

2 +0.01α01 erfc

< 0.21857,

where we have used the monotonicity of the terms in ντ .

55

Figure A6: The graph of function ˜µ for low variances x = ντ for µω = 0.01, where x ∈ [0, 3], is
displayed in yellow. Lower and upper bounds based on the Abramowitz bounds (Lemma 22) are
displayed in green and blue, respectively.

Similarly, we can use the monotonicity of the terms in ντ to show that

˜µ(µ, ω, ν, τ, λ, α) (cid:62) ˜µ(0.1, −0.1, ν, τ, λ, α) > −0.289324,

(231)

such that |˜µ| < 0.289324 at low variances.

Furthermore, when (ντ ) → 0, the terms with the arguments of the complementary error functions
erfc and the exponential function go to inﬁnity, therefore these three terms converge to zero. Hence,
the remaining terms are only 2µω 1

2 λ.

Lemma 42 (Bounds on derivatives of ˜µ in Ω−). The derivatives of the function ˜µ(µ, ω, ν, τ, λ01, α01
(Eq. (4)) with respect to µ, ω, ν, τ in the domain Ω− = {µ, ω, ν, τ | − 0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54)
0.1, 0.05 (cid:54) ν (cid:54) 0.24, 0.8 (cid:54) τ (cid:54) 1.25} can be bounded as follows:

(232)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂µ
∂
∂ω
∂
∂ν
∂
∂τ

(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)

< 0.14

< 0.14

< 0.52

< 0.11.

Proof. The expression

∂
∂µ

˜µ = J11 =

λωe

1
2

(cid:18)

−(µω)2
2ντ

(µω)2
2ντ − e

(µω)2
2ντ erfc

2e

(cid:19)

(cid:18) µω
√
√
2

ντ

(µω+ντ )2
2ντ

+ αe

erfc

(cid:19)(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(233)

(µω)2
2ντ erfc

(cid:16) µω√

(cid:17)

(µω+ντ )2
2ντ

contains the terms e
which are monotonically de-
creasing in their arguments (Lemma 23). We can therefore obtain their minima and maximal at the
minimal and maximal arguments. Since the ﬁrst term has a negative sign in the expression, both
terms reach their maximal value at µω = −0.01, ν = 0.05, and τ = 0.8.

and e

erfc

ντ

√

2

(cid:16) µω+ντ
√
√
ντ
2

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂µ

(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)

(cid:54) 1
2

(cid:16)

(cid:12)
(cid:12)
(cid:12)

|λω|

2 − e0.03535532

erfc (0.0353553) + αe0.1060662

erfc (0.106066)

(cid:17)(cid:12)
(cid:12)
(cid:12) < 0.133

(234)

Since, ˜µ is symmetric in µ and ω, these bounds also hold for the derivate to ω.

56

(235)

(236)

(237)

Figure A7: The graph of the function h(x) = ˜µ2(0.1, −0.1, x, 1, λ01, α01) is displayed. It has a local
maximum at x = ντ ≈ 0.187342 and h(x) ≈ 0.00451457 in the domain x ∈ [0, 1].

We use the argumentation that the term with the error function is monotonically decreasing
(Lemma 23) again for the expression

˜µ = J12 =

∂
∂ν

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
4

1
4

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32)

λτ e− µ2 ω2

2ντ

(µω+ντ )2
2ντ

αe

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

− (α − 1)

(cid:114) 2
πντ

(cid:33)

(cid:54)

λτ

(|1.1072 − 2.68593|) < 0.52.

We have used that the term 1.1072 (cid:54) α01e
(cid:113) 2
πντ
4 λν(cid:12)
(cid:12) to (cid:12)
(cid:12) 1

(cid:16) µω+ντ
√
√
2
ντ
(cid:54) 2.68593. Since ˜µ is symmetric in ν and τ , we only have to chance
∂τ ˜µ(cid:12)
(cid:12) to obtain the estimate (cid:12)
(cid:12) ∂
(cid:12) < 0.11.

0.942286 (cid:54) (α − 1)
outermost term (cid:12)
4 λτ (cid:12)
(cid:12) 1

(cid:54) 1.49042 and the term

(µω+ντ )2
2ντ

erfc

(cid:17)

Lemma 43 (Tight bound on ˜µ2 in Ω−). The function ˜µ2(µ, ω, ν, τ, λ01, α01) (Eq. (4)) is bounded by

(cid:12)˜µ2(cid:12)
(cid:12)

(cid:12) < 0.005

in the domain Ω− = {µ, ω, ν, τ | − 0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1, 0.05 (cid:54) ν (cid:54) 0.24, 0.8 (cid:54) τ (cid:54)
1.25}.

We visualize the function ˜µ2 at its maximal µν = −0.01 and for x = ντ in the form h(x) =
˜µ2(0.1, −0.1, x, 1, λ01, α01) in Figure A7.

Proof. We use a similar strategy to the one we have used to show the bound on the singular value
(Lemmata 10, 11, and 12), where we evaluted the function on a grid and used bounds on the derivatives
together with the mean value theorem. Here we have

(cid:12)
(cid:12)˜µ2(µ, ω, ν, τ, λ01, α01) − ˜µ2(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ01, α01)(cid:12)
(cid:12)
(cid:12)
∂
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂µ
(cid:12)
(cid:12)

|∆ω| +

|∆µ| +

|∆ν| +

∂
∂ω

∂
∂ν

∂
∂τ

|∆τ |.

˜µ2

˜µ2

˜µ2

˜µ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12) (cid:54)

(238)

We use Lemma 42 and Lemma 41, to obtain

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂µ
∂
∂ω

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂µ
∂
∂ω

(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)

˜µ2

= 2 |˜µ|

< 2 · 0.289324 · 0.14 = 0.08101072

(239)

˜µ2

= 2 |˜µ|

< 2 · 0.289324 · 0.14 = 0.08101072

57

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂ν
∂
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂ν
∂
∂τ

(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜µ
(cid:12)
(cid:12)

˜µ2

= 2 |˜µ|

< 2 · 0.289324 · 0.52 = 0.30089696

˜µ2

= 2 |˜µ|

< 2 · 0.289324 · 0.11 = 0.06365128

We evaluated the function ˜µ2 in a grid G of Ω− with ∆µ = 0.001498041, ∆ω = 0.001498041,
∆ν = 0.0004033190, and ∆τ = 0.0019065994 using a computer and obtained the maximal value
maxG(˜µ)2 = 0.00451457, therefore the maximal value of ˜µ2 is bounded by

(˜µ)2 (cid:54)

max
(µ,ω,ν,τ )∈Ω−
0.00451457 + 0.001498041 · 0.08101072 + 0.001498041 · 0.08101072+
0.0004033190 · 0.30089696 + 0.0019065994 · 0.06365128 < 0.005.

(240)

(241)

Furthermore we used error propagation to estimate the numerical error on the function evaluation.
Using the error propagation rules derived in Subsection A3.4.5, we found that the numerical error is
smaller than 10−13 in the worst case.
Lemma 44 (Main subfunction). For 1.2 (cid:54) x (cid:54) 20 and −0.1 (cid:54) y (cid:54) 0.1,

the function

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing
in y for the minimal x = 12/10 = 1.2.

Proof. We ﬁrst consider the derivative of sub-function Eq. (101) with respect to x. The derivative of
the function

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

with respect to x is
√

(x+y)2
2x

(cid:16)
e

π

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

− 2e

(2x+y)2
2x

√
2

πx2

(cid:0)4x2 − y2(cid:1) erfc

(cid:16) 2x+y√

√

2

x

(cid:17)(cid:17)

√

√

+

2

x(3x − y)

=

(242)

(243)

(cid:17)(cid:17)

(cid:16) 2x+y√

√

2

x

+

(244)
√
2

√

x(3x − y)

=

√

(cid:16)
e

π

(x+y)2
2x

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

− 2e

(2x + y)(2x − y) erfc

(2x+y)2
2x
√

2

πx2

√



π



(x+y)2
2x

e

(x−y)(x+y) erfc

(cid:16) x+y
√
√
x
2

(cid:17)

−

(2x+y)2
2x

2e

√

√

2

x

(2x+y)(2x−y) erfc
√

√

2

x



(cid:16) 2x+y
√
√
x
2

(cid:17)

 + (3x − y)

√

√
2

2

√

πx2

x

.

We consider the numerator






√

π

(x+y)2
2x

e

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

−

(2x+y)2
2x

2e

√

√
2

x

(2x + y)(2x − y) erfc
√
2

√

x



(cid:17)

(cid:16) 2x+y√

√

2

x


 + (3x − y) .

(245)

For bounding this value, we use the approximation

ez2

erfc(z) ≈

√

2.911
√

π(2.911 − 1)z +

πz2 + 2.9112

.

(246)

58

from Ren and MacKenzie [30]. We start with an error analysis of this approximation. According
to Ren and MacKenzie [30] (Figure 1), the approximation error is positive in the range [0.7, 3.2].
This range contains all possible arguments of erfc that we consider. Numerically we maximized and
minimized the approximation error of the whole expression

E(x, y) =






(x+y)2
2x

e

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

−

(2x+y)2
2x

2e

(2x − y)(2x + y) erfc
√

√

2

x



(cid:17)

(cid:16) 2x+y√

√

2

x


 −

√

√

2

x









2.911(x − y)(x + y)

(cid:32) √

(cid:0)√

√

x(cid:1)

2

π(2.911−1)(x+y)
√

√

+

2

x

(cid:114)

(cid:16) x+y√

√

2

x

π

(cid:17)2

+ 2.9112

(cid:0)√

√
2

x(cid:1)

(cid:32) √

2 · 2.911(2x − y)(2x + y)
(cid:16) 2x+y√

π(2.911−1)(2x+y)
√

(cid:114)

√

√

+

π

(cid:17)2

2

x

+ 2.9112

2

x

(cid:33) −









(cid:33)

.

We numerically determined 0.0113556 (cid:54) E(x, y) (cid:54) 0.0169551 for 1.2 (cid:54) x (cid:54) 20 and −0.1 (cid:54)
y (cid:54) 0.1. We used different numerical optimization techniques like gradient based constraint BFGS
algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our
approximation is smaller than the function that we approximate. We subtract an additional safety gap
of 0.0131259 from our approximation to ensure that the inequality via the approximation holds true.
With this safety gap the inequality would hold true even for negative x, where the approximation
error becomes negative and the safety gap would compensate. Of course, the safety gap of 0.0131259
is not necessary for our analysis but may help or future investigations.

We have the sequences of inequalities using the approximation of Ren and MacKenzie [30]:

(3x − y) +

(x+y)2
2x

e

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

−

(2x+y)2
2x

2e

(2x − y)(2x + y) erfc
√
2

√

x

(cid:16) 2x+y√

√

2

x



(cid:17)




√

π (cid:62)

√

√

2

x

(247)

(248)

(3x − y) +

2.911(x − y)(x + y)

(cid:32)(cid:114)
π

(cid:17)2

(cid:16) x+y√

√

2

x

√
+ 2.9112 + (2.911−1)
√
√
2

π(x+y)
x

−

(cid:33)

(cid:0)√

√

x(cid:1)

2









(cid:33)

√

(cid:32)(cid:114)

(cid:0)√

√
2

x(cid:1)

2(2x − y)(2x + y)2.911
(cid:16) 2x+y√

(cid:17)2

√

+ 2.9112 + (2.911−1)
√
2

x

2

√
√

π

π(2x+y)
x

π − 0.0131259 =

(cid:0)√

√
2

x2.911(cid:1) (x − y)(x + y)

(3x − y) +

√
(cid:16)(cid:112)π(x + y)2 + 2 · 2.9112x + (2.911 − 1)(x + y)

π

(cid:17) (cid:0)√

√

x(cid:1)

2

−

2(2x − y)(2x + y) (cid:0)√

2

√

x2.911(cid:1)

(cid:0)√

√
2

√
x(cid:1) (cid:16)(cid:112)π(2x + y)2 + 2 · 2.9112x + (2.911 − 1)(2x + y)

π



√



(cid:17)

π − 0.0131259 =

59


















(3x − y) + 2.911

(x − y)(x + y)
(cid:113)

−

(2.911 − 1)(x + y) +

(x + y)2 + 2·2.9112x

π

2(2x − y)(2x + y)
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

π


 − 0.0131259 (cid:62)

(3x − y) + 2.911

−

(2.911 − 1)(x + y) +

+ (x + y)2 + 2·2.9112x

π

+ 2·2.9112y
π

(x − y)(x + y)

(cid:113)(cid:0) 2.9112
π


(cid:1)2

2(2x − y)(2x + y)
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

π

 − 0.0131259 =

(3x − y) + 2.911

(x − y)(x + y)

(2.911 − 1)(x + y) +

2(2x − y)(2x + y)
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

(cid:113)(cid:0)x + y + 2.9112


π

−

(cid:1)2

 − 0.0131259 =

















(3x − y) + 2.911

(x − y)(x + y)
2.911(x + y) + 2.9112

π

2(2x − y)(2x + y)
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

π



 − 0.0131259 =

(3x − y) +

(x − y)(x + y)
(x + y) + 2.911

π

−

2(2x − y)(2x + y)2.911
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

π

−2(2x − y)2.911

(x + y) +

(2x + y) +

(cid:19)

2.911
π

(x + y) +

(3x − y − 0.0131259)

(2.911 − 1)(2x + y) +

(2x + y)2 +

(cid:18)

(cid:18)

(cid:32)(cid:18)

(cid:16)

(cid:18)

(cid:19)

2.911
π
(cid:32)

(cid:19) (cid:32)

2.911
π

(x − y)(x + y)

(2.911 − 1)(2x + y) +

(2x + y)2 +

(x + y) +

(2.911 − 1)(2x + y) +

(2x + y)2 +

2 · 2.9112x
π

(cid:33)

+

(cid:114)

(cid:33)(cid:33)

2 · 2.9112x
π

2 · 2.9112x
π

(cid:33)(cid:33)−1

=

(249)

((x − y)(x + y) + (3x − y − 0.0131259)(x + y + 0.9266))

(cid:16)(cid:112)(2x + y)2 + 5.39467x + 3.822x + 1.911y

(cid:17)

−

5.822(2x − y)(x + y + 0.9266)(2x + y))
(cid:32)(cid:18)
(cid:19) (cid:32)

2.911
π

(x + y) +

(2.911 − 1)(2x + y) +

(2x + y)2 +

(cid:33)(cid:33)−1

22.9112x
π

> 0 .

We explain this sequence of inequalities:

• First inequality: The approximation of Ren and MacKenzie [30] and then subtracting a

safety gap (which would not be necessary for the current analysis).

• Equalities: The factor

x is factored out and canceled.

√

√
2

• Second inequality: adds a positive term in the ﬁrst root to obtain a binomial form. The term
containing the root is positive and the root is in the denominator, therefore the whole term
becomes smaller.

π

−

(cid:32)

(cid:114)

(cid:114)

(cid:114)

60

− 0.0131259 =

(3x − y) +

(x − y)(x + y)

(x + y) + 2.911

π

−

2(2x − y)(2x + y)2.911

(2.911 − 1)(2x + y) +

(cid:113)

(2x + y)2 + 2·2.9112x

π

− 0.0131259 =

• Equalities: solve for the term and factor out.

• Bringing all terms to the denominator (cid:0)(x + y) + 2.911

(cid:1)

(2.911 − 1)(2x + y) +

(cid:18)

π

(cid:113)

(2x + y)2 + 2·2.9112x

π

(cid:19)

.

• Equalities: Multiplying out and expanding terms.

• Last inequality > 0 is proofed in the following sequence of inequalities.

We look at the numerator of the last expression of Eq. (248), which we show to be positive in order to
show > 0 in Eq. (248). The numerator is

((x − y)(x + y) + (3x − y − 0.0131259)(x + y + 0.9266))

(cid:16)(cid:112)(2x + y)2 + 5.39467x + 3.822x + 1.911y

(cid:17)

−

5.822(2x − y)(x + y + 0.9266)(2x + y) =
− 5.822(2x − y)(x + y + 0.9266)(2x + y) + (3.822x + 1.911y)((x − y)(x + y)+
(3x − y − 0.0131259)(x + y + 0.9266)) + ((x − y)(x + y)+
(3x − y − 0.0131259)(x + y + 0.9266))(cid:112)(2x + y)2 + 5.39467x =
− 8.0x3 + (cid:0)4x2 + 2xy + 2.76667x − 2y2 − 0.939726y − 0.0121625(cid:1) (cid:112)(2x + y)2 + 5.39467x−
8.0x2y − 11.0044x2 + 2.0xy2 + 1.69548xy − 0.0464849x + 2.0y3 + 3.59885y2 − 0.0232425y =
− 8.0x3 + (cid:0)4x2 + 2xy + 2.76667x − 2y2 − 0.939726y − 0.0121625(cid:1) (cid:112)(2x + y)2 + 5.39467x−
8.0x2y − 11.0044x2 + 2.0xy2 + 1.69548xy − 0.0464849x + 2.0y3 + 3.59885y2 − 0.0232425y .

The factor in front of the root is positive. If the term, that does not contain the root, was positive, then
the whole expression would be positive and we would have proofed that the numerator is positive.
Therefore we consider the case that the term, that does not contain the root, is negative. The term that
contains the root must be larger than the other term in absolute values.
− (cid:0)−8.0x3 − 8.0x2y − 11.0044x2 + 2.xy2 + 1.69548xy − 0.0464849x + 2.y3 + 3.59885y2 − 0.0232425y(cid:1) <

(cid:0)4x2 + 2xy + 2.76667x − 2y2 − 0.939726y − 0.0121625(cid:1) (cid:112)(2x + y)2 + 5.39467x .
Therefore the squares of the root term have to be larger than the square of the other term to show > 0
in Eq. (248). Thus, we have the inequality:
(cid:0)−8.0x3 − 8.0x2y − 11.0044x2 + 2.xy2 + 1.69548xy − 0.0464849x + 2.y3 + 3.59885y2 − 0.0232425y(cid:1)2

<

(cid:0)4x2 + 2xy + 2.76667x − 2y2 − 0.939726y − 0.0121625(cid:1)2 (cid:0)(2x + y)2 + 5.39467x(cid:1) .

This is equivalent to
0 < (cid:0)4x2 + 2xy + 2.76667x − 2y2 − 0.939726y − 0.0121625(cid:1)2 (cid:0)(2x + y)2 + 5.39467x(cid:1) −

(cid:0)−8.0x3 − 8.0x2y − 11.0044x2 + 2.0xy2 + 1.69548xy − 0.0464849x + 2.0y3 + 3.59885y2 − 0.0232425y(cid:1)2
− 1.2227x5 + 40.1006x4y + 27.7897x4 + 41.0176x3y2 + 64.5799x3y + 39.4762x3 + 10.9422x2y3−
13.543x2y2 − 28.8455x2y − 0.364625x2 + 0.611352xy4 + 6.83183xy3 + 5.46393xy2+
0.121746xy + 0.000798008x − 10.6365y5 − 11.927y4 + 0.190151y3 − 0.000392287y2 .

=

We obtain the inequalities:
− 1.2227x5 + 40.1006x4y + 27.7897x4 + 41.0176x3y2 + 64.5799x3y + 39.4762x3 + 10.9422x2y3−

13.543x2y2 − 28.8455x2y − 0.364625x2 + 0.611352xy4 + 6.83183xy3 + 5.46393xy2+
0.121746xy + 0.000798008x − 10.6365y5 − 11.927y4 + 0.190151y3 − 0.000392287y2 =

(250)

(251)

(252)

(253)

(254)

61

− 1.2227x5 + 27.7897x4 + 41.0176x3y2 + 39.4762x3 − 13.543x2y2 − 0.364625x2+
y (cid:0)40.1006x4 + 64.5799x3 + 10.9422x2y2 − 28.8455x2 + 6.83183xy2 + 0.121746x −
10.6365y4 + 0.190151y2(cid:1) + 0.611352xy4 + 5.46393xy2 + 0.000798008x − 11.927y4 − 0.000392287y2 >
− 1.2227x5 + 27.7897x4 + 41.0176 · (0.0)2x3 + 39.4762x3 − 13.543 · (0.1)2x2 − 0.364625x2−
0.1 · (cid:0)40.1006x4 + 64.5799x3 + 10.9422 · (0.1)2x2 − 28.8455x2 + 6.83183 · (0.1)2x + 0.121746x +
10.6365 · (0.1)4 + 0.190151 · (0.1)2(cid:1) +
0.611352 · (0.0)4x + 5.46393 · (0.0)2x + 0.000798008x − 11.927 · (0.1)4 − 0.000392287 · (0.1)2 =
− 1.2227x5 + 23.7796x4 + (20 + 13.0182)x3 + 2.37355x2 − 0.0182084x − 0.000194074 (cid:62)
− 1.2227x5 + 24.7796x4 + 13.0182x3 + 2.37355x2 − 0.0182084x − 0.000194074 >
13.0182x3 + 2.37355x2 − 0.0182084x − 0.000194074 > 0 .

We used 24.7796 · (20)4 − 1.2227 · (20)5 = 52090.9 > 0 and x (cid:54) 20. We have proofed the last
inequality > 0 of Eq. (248).

Consequently the derivative is always positive independent of y, thus

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is strictly monotonically increasing in x.

(255)

(256)

(257)

(258)

(259)

The main subfunction is smaller than zero. Next we show that the sub-function Eq. (101) is
smaller than zero. We consider the limit:

(x+y)2
2x

erfc

lim
x→∞

e

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

= 0

The limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach
0 from below. Thus,

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is smaller than zero.

Behavior of the main subfunction with respect to y at minimal x. We now consider the deriva-
tive of sub-function Eq. (101) with respect to y. We proofed that sub-function Eq. (101) is strictly
monotonically increasing independent of y. In the proof of Theorem 16, we need the minimum of
sub-function Eq. (101). Therefore we are only interested in the derivative of sub-function Eq. (101)
with respect to y for the minimum x = 12/10 = 1.2

Consequently, we insert the minimum x = 12/10 = 1.2 into the sub-function Eq. (101). The main
terms become

x + y
√
√
x
2

=

y + 1.2
√
√
1.2
2

=

√

y
√

2

1.2

√
1.2
√
2

+

=

5y + 6
√
15
2

and

2x + y
√
√
x
2

=

y + 1.2 · 2
√
√
1.2
2

=

√

y
√
2

1.2

√

√

+

1.2

2 =

5y + 12
√
15
2

.

Sub-function Eq. (101) becomes:

(cid:32)

e

y

√

√

2

+

12
10

(cid:33)2

√
12
10√
2



erfc



√

(cid:113) 12
10
√

2



(cid:32)

 − 2e

y

√

√

2

12
10

y
(cid:113) 12
2
10

+

√

√
2

+

12
10

(cid:33)2



erfc



√

y
(cid:113) 12
10

2

√

(cid:114) 12
2
10

+



 .

(260)

62

The derivative of this function with respect to y is

√

(cid:16)

15π

60 (5y+6)2
e 1

(5y + 6) erfc

(cid:17)

(cid:16) 5y+6
√
15
2

− 2e 1
√
6

15π

60 (5y+12)2

(5y + 12) erfc

(cid:17)(cid:17)

(cid:16) 5y+12
√
15
2

+ 30

.

(261)

We again will use the approximation of Ren and MacKenzie [30]

ez2

erfc(z) =

√

2.911
√

π(2.911 − 1)z +

πz2 + 2.9112

.

(262)

Therefore we ﬁrst perform an error analysis. We estimated the maximum and minimum of

√

15π







√

√

(cid:18)
e

15π

2 · 2.911(5y + 12)
(cid:114)
(cid:16) 5y+12
√
15
2

+

π

(cid:17)2

π(2.911−1)(5y+12)
15

√

2

−

√

2.911(5y + 6)
(cid:114)

+ 2.9112

π(2.911−1)(5y+6)
15

√

2

+

1

60 (5y+6)2

(5y + 6) erfc

− 2e

1

60 (5y+12)2

(5y + 12) erfc

(cid:19)

(cid:18) 5y + 6
√
15
2







+ 30 +

(cid:16) 5y+6
√
15
2

π

(cid:17)2

+ 2.9112

(263)

(cid:19)(cid:19)

(cid:18) 5y + 12
√
15
2

+ 30 .

We obtained for the maximal absolute error the value 0.163052. We added an approximation error
of 0.2 to the approximation of the derivative. Since we want to show that the approximation upper
bounds the true expression, the addition of the approximation error is required here. We get a
sequence of inequalities:

√

(cid:18)
e

15π

1

60 (5y+6)2

(5y + 6) erfc

− 2e

1

60 (5y+12)2

(5y + 12) erfc

(cid:19)

(cid:18) 5y + 6
√
15
2

(cid:19)(cid:19)

(cid:18) 5y + 12
√
15
2

+ 30 (cid:54)

2.911(5y + 6)
(cid:114)

π(2.911−1)(5y+6)
15

√

2

+

(cid:16) 5y+6
√
15
2

π

(cid:17)2

+ 2.9112

−

√

2 · 2.911(5y + 12)
(cid:114)
(cid:16) 5y+12
√
15
2

+

π

(cid:17)2

π(2.911−1)(5y+12)
15

√

2

+ 2.9112

(264)







+

√

15π







√

30 + 0.2 =

(2.911 − 1)(5y + 6) +

(5y + 6)2 +

(2.911 − 1)(5y + 12) +

(5y + 12)2 +

(30 · 2.911)(5y + 6)

(cid:114)

−

(cid:17)2

√

(cid:16) 2

15·2.911
√
π

2(30 · 2.911)(5y + 12)

(cid:114)

+

(cid:17)2

√

(cid:16) 2

15·2.911
√
π

30 + 0.2 =





(0.2 + 30)


(2.911 − 1)(5y + 12) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 12)2 +

√

(cid:32)

2

(cid:33)2



15 · 2.911
π

√




(2.911 − 1)(5y + 6) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 6)2 +

√

(cid:32)

2

(cid:33)2

 −

15 · 2.911
π

√





2 · 30 · 2.911(5y + 12)


(2.911 − 1)(5y + 6) +

2.911 · 30(5y + 6)


(2.911 − 1)(5y + 12) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 6)2 +

(cid:32)

√
2

(cid:33)2

 +

15 · 2.911
π

√

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 12)2 +

(cid:32)

√
2

(cid:33)2








15 · 2.911
π

√

63





(2.911 − 1)(5y + 6) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 6)2 +

(cid:32)

√
2

(cid:33)2



15 · 2.911
π

√








(2.911 − 1)(5y + 12) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 12)2 +

(cid:32)

√
2



−1

(cid:33)2






15 · 2.911
π

√

< 0 .

We explain this sequence of inequalities.

• First inequality: The approximation of Ren and MacKenzie [30] and then adding the error

bound to ensure that the approximation is larger than the true value.

• First equality: The factor 2

15 and 2

π are factored out and canceled.

√

√

• Second equality: Bringing all terms to the denominator


(2.911 − 1)(5y + 6) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 6)2 +

√

(cid:32)

2

(cid:33)2



152.911
√
π

(265)






(2.911 − 1)(5y + 12) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 12)2 +

(cid:32)

√
2

(cid:33)2

 .

15 · 2.911
π

√

• Last inequality < 0 is proofed in the following sequence of inequalities.

We look at the numerator of the last term in Eq. (264). We have to proof that this numerator is smaller
than zero in order to proof the last inequality of Eq. (264). The numerator is



(0.2 + 30)


(2.911 − 1)(5y + 12) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 12)2 +

(cid:32)

√
2

(cid:33)2



15 · 2.911
π

√

(266)




(2.911 − 1)(5y + 6) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 6)2 +

√

(cid:32)

2

(cid:33)2

 −

15 · 2.911
π

√

2 · 30 · 2.911(5y + 12)


(2.911 − 1)(5y + 6) +





2.911 · 30(5y + 6)


(2.911 − 1)(5y + 12) +

We now compute upper bounds for this numerator:

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 6)2 +

(cid:32)

√
2

(cid:33)2

 +

15 · 2.911
π

√

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 12)2 +

(cid:32)

√
2

(cid:33)2

 .

15 .2.911
π

√



(0.2 + 30)


(2.911 − 1)(5y + 12) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 12)2 +

√

(cid:32)

2

(cid:33)2



15 · 2.911
π

√

(267)




(2.911 − 1)(5y + 6) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 6)2 +

(cid:32)

√
2

(cid:33)2

 −

15 · 2.911
π

√

64





2 · 30 · 2.911(5y + 12)


(2.911 − 1)(5y + 6) +

2.911 · 30(5y + 6)


(2.911 − 1)(5y + 12) +

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 6)2 +

(cid:32)

√
2

(cid:33)2

 +

15 · 2.911
π

√

(cid:118)
(cid:117)
(cid:117)
(cid:116)(5y + 12)2 +

(cid:32)

√
2

(cid:33)2

 =

15 · 2.911
π

√

− 1414.99y2 − 584.739(cid:112)(5y + 6)2 + 161.84y + 725.211(cid:112)(5y + 12)2 + 161.84y−
5093.97y − 1403.37(cid:112)(5y + 6)2 + 161.84 + 30.2(cid:112)(5y + 6)2 + 161.84(cid:112)(5y + 12)2 + 161.84+
870.253(cid:112)(5y + 12)2 + 161.84 − 4075.17 <
− 1414.99y2 − 584.739(cid:112)(5y + 6)2 + 161.84y + 725.211(cid:112)(5y + 12)2 + 161.84y−
5093.97y − 1403.37(cid:112)(6 + 5 · (−0.1))2 + 161.84 + 30.2(cid:112)(6 + 5 · 0.1)2 + 161.84(cid:112)(12 + 5 · 0.1)2 + 161.84+
870.253(cid:112)(12 + 5 · 0.1)2 + 161.84 − 4075.17 =
− 1414.99y2 − 584.739(cid:112)(5y + 6)2 + 161.84y + 725.211(cid:112)(5y + 12)2 + 161.84y − 5093.97y − 309.691 <
(cid:16)

(cid:17)
−584.739(cid:112)(5y + 6)2 + 161.84 + 725.211(cid:112)(5y + 12)2 + 161.84 − 5093.97

− 309.691 <
(cid:17)
725.211(cid:112)(12 + 5 · (−0.1))2 + 161.84 − 584.739(cid:112)(6 + 5 · 0.1)2 + 161.84 − 5093.97

− 309.691 =

− 0.1

(cid:16)

y

− 208.604 .

Consequently

For the ﬁrst inequality we choose y in the roots, so that positive terms maximally increase and
negative terms maximally decrease. The second inequality just removed the y2 term which is always
negative, therefore increased the expression. For the last inequality, the term in brackets is negative
for all settings of y. Therefore we make the brackets as negative as possible and make the whole term
positive by multiplying with y = −0.1.

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is strictly monotonically decreasing in y for the minimal x = 1.2.
Lemma 45 (Main subfunction below). For 0.007 (cid:54) x (cid:54) 0.875 and −0.01 (cid:54) y (cid:54) 0.01, the function

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

smaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in
y for the minimal x = 0.007 = 0.00875 · 0.8, x = 0.56 = 0.7 · 0.8, x = 0.128 = 0.16 · 0.8, and
x = 0.216 = 0.24 · 0.9 (lower bound of 0.9 on τ ).

Proof. We ﬁrst consider the derivative of sub-function Eq. (111) with respect to x. The derivative of
the function

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

with respect to x is

√

(cid:16)
e

π

(x+y)2
2x

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

− 2e

(cid:0)4x2 − y2(cid:1) erfc

(cid:16) 2x+y√

√

2

x

(cid:17)(cid:17)

√

√

+

2

x(3x − y)

(268)

(269)

(270)

√

(cid:16)
e

π

(x+y)2
2x

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

(2x+y)2
2x

− 2e

(2x + y)(2x − y) erfc

=

(cid:17)(cid:17)

(cid:16) 2x+y√

√

2

x

+

(271)
√
2

√

x(3x − y)

=

(2x+y)2
2x

√
2

πx2

√

2

πx2

65

√



π



(x+y)2
2x

e

(x−y)(x+y) erfc

(cid:16) x+y
√
√
x
2

(cid:17)

(2x+y)2
2x

2e

√

√

2

x

(2x+y)(2x−y) erfc
√

√

2

x



(cid:16) 2x+y
√
√
x
2

(cid:17)

 + (3x − y)

−

√

√

√

22

π

xx2

We consider the numerator






√

π

(x+y)2
2x

e

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

−

(2x+y)2
2x

2e

√

√
2

x

(2x + y)(2x − y) erfc
√
2

√

x



(cid:17)

(cid:16) 2x+y√

√

2

x

.


 + (3x − y) .

(272)

For bounding this value, we use the approximation

ez2

erfc(z) ≈

√

2.911
√

π(2.911 − 1)z +

πz2 + 2.9112

.

(273)

from Ren and MacKenzie [30]. We start with an error analysis of this approximation. According to
Ren and MacKenzie [30] (Figure 1), the approximation error is both positive and negative in the range
[0.175, 1.33]. This range contains all possible arguments of erfc that we consider in this subsection.
Numerically we maximized and minimized the approximation error of the whole expression
(cid:16) x+y√

(cid:16) 2x+y√

(2x+y)2
2x

(x+y)2
2x





(x − y)(x + y) erfc

2e

(cid:17)

(cid:17)

e

√

√

E(x, y) =




√

√

2

x

2

x

−

(2x − y)(2x + y) erfc
√

√

2

x

2

x


 −









2.911(x − y)(x + y)

(cid:32) √

(cid:0)√

√

x(cid:1)

2

π(2.911−1)(x+y)
√

√

+

2

x

(cid:114)

(cid:16) x+y√

√

2

x

π

(cid:17)2

+ 2.9112

(cid:0)√

√
2

x(cid:1)

(cid:32) √

2 · 2.911(2x − y)(2x + y)
(cid:16) 2x+y√

π(2.911−1)(2x+y)
√

(cid:114)

√

√

+

π

(cid:17)2

2

x

+ 2.9112

2

x

(cid:33) −









(cid:33)

.

We numerically determined −0.000228141 (cid:54) E(x, y) (cid:54) 0.00495688 for 0.08 (cid:54) x (cid:54) 0.875 and
−0.01 (cid:54) y (cid:54) 0.01. We used different numerical optimization techniques like gradient based
constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points.
Therefore our approximation is smaller than the function that we approximate.

We use an error gap of −0.0003 to countermand the error due to the approximation. We have the
sequences of inequalities using the approximation of Ren and MacKenzie [30]:

(3x − y) +

(x+y)2
2x

e

(x − y)(x + y) erfc

(cid:17)

(cid:16) x+y√

√

2

x

−

(2x+y)2
2x

2e

(2x − y)(2x + y) erfc
√
2

√

x

(cid:16) 2x+y√

√

2

x



(cid:17)




√

π (cid:62)

√

√

2

x

(274)

(275)

(3x − y) +

2.911(x − y)(x + y)

(cid:32)(cid:114)
π

(cid:17)2

(cid:16) x+y√

√

2

x

√
+ 2.9112 + (2.911−1)
√
√
2

π(x+y)
x

−

(cid:33)

(cid:0)√

√

x(cid:1)

2

66














(cid:32)(cid:114)

(cid:0)√

√
2

x(cid:1)

2(2x − y)(2x + y)2.911
(cid:16) 2x+y√

(cid:17)2

√

+ 2.9112 + (2.911−1)
√
2

x

2

√
√

π

π(2x+y)
x









(cid:33)

√

π − 0.0003 =

(3x − y) +

√
(cid:16)(cid:112)π(x + y)2 + 2 · 2.9112x + (2.911 − 1)(x + y)

π

(cid:17) (cid:0)√

√

x(cid:1)

2

−

(cid:0)√

√
2

x2.911(cid:1) (x − y)(x + y)





2(2x − y)(2x + y) (cid:0)√

2

√

x2.911(cid:1)

(cid:0)√

√
2

√
x(cid:1) (cid:16)(cid:112)π(2x + y)2 + 2 · 2.9112x + (2.911 − 1)(2x + y)

π

π − 0.0003 =



√



(cid:17)

−

(3x − y) + 2.911

(x − y)(x + y)
(cid:113)

(2.911 − 1)(x + y) +

(x + y)2 + 2·2.9112x

π

2(2x − y)(2x + y)
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

π


 − 0.0003 (cid:62)

(x − y)(x + y)

(cid:113)(cid:0) 2.9112
π


(cid:1)2

 − 0.0003 =

2(2x − y)(2x + y)
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

π

(3x − y) + 2.911

(x − y)(x + y)

(2.911 − 1)(x + y) +

2(2x − y)(2x + y)
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

(cid:113)(cid:0)x + y + 2.9112


π

−

(cid:1)2

 − 0.0003 =

















(3x − y) + 2.911

−

(2.911 − 1)(x + y) +

+ (x + y)2 + 2·2.9112x

π

+ 2·2.9112y
π



 − 0.0003 =

π

−

(cid:19)

2.911
π

(cid:32)

(cid:114)

(3x − y) + 2.911

(x − y)(x + y)
2.911(x + y) + 2.9112

π

2(2x − y)(2x + y)
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

π

(3x − y) +

(3x − y) +

(x − y)(x + y)
(x + y) + 2.911

(x − y)(x + y)
(x + y) + 2.911

π

π

−

−

2(2x − y)(2x + y)2.911
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

− 0.0003 =

2(2x − y)(2x + y)2.911
(cid:113)

(2.911 − 1)(2x + y) +

(2x + y)2 + 2·2.9112x

− 0.0003 =

π

π

(cid:18)

(cid:18)

(cid:32)(cid:18)

(cid:18)

(cid:19)

2.911
π
(cid:32)

(cid:19) (cid:32)

2.911
π

−2(2x − y)2.911

(x + y) +

(2x + y) +

(x + y) +

(3x − y − 0.0003)

(2.911 − 1)(2x + y) +

(2x + y)2 +

(cid:114)

2 · 2.9112x
π

(cid:33)

+

(x − y)(x + y)

(2.911 − 1)(2x + y) +

(2x + y)2 +

(x + y) +

(2.911 − 1)(2x + y) +

(2x + y)2 +

(cid:33)(cid:33)

2 · 2.9112x
π

2 · 2.9112x
π

(cid:33)(cid:33)−1

=

(cid:114)

67

(cid:16)

(cid:16)

−8x3 − 8x2y + 4x2(cid:112)(2x + y)2 + 5.39467x − 10.9554x2 + 2xy2 − 2y2(cid:112)(2x + y)2 + 5.39467x +
1.76901xy + 2xy(cid:112)(2x + y)2 + 5.39467x + 2.7795x(cid:112)(2x + y)2 + 5.39467x −
0.9269y(cid:112)(2x + y)2 + 5.39467x − 0.00027798(cid:112)(2x + y)2 + 5.39467x − 0.00106244x +
2y3 + 3.62336y2 − 0.00053122y(cid:1)
(cid:32)(cid:18)

(cid:33)(cid:33)−1

(cid:19) (cid:32)

(cid:114)

(x + y) +

(2.911 − 1)(2x + y) +

(2x + y)2 +

=

2 · 2.9112x
π

−8x3 + (cid:0)4x2 + 2xy + 2.7795x − 2y2 − 0.9269y − 0.00027798(cid:1) (cid:112)(2x + y)2 + 5.39467x −
8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x + 2y3 + 3.62336y2 − 0.00053122y(cid:1)
(cid:32)(cid:18)

(cid:33)(cid:33)−1

(cid:19) (cid:32)

(cid:114)

(x + y) +

(2.911 − 1)(2x + y) +

(2x + y)2 +

> 0 .

2 · 2.9112x
π

2.911
π

2.911
π

We explain this sequence of inequalities:

• First inequality: The approximation of Ren and MacKenzie [30] and then subtracting an

error gap of 0.0003.

• Equalities: The factor

x is factored out and canceled.

√

√
2

• Second inequality: adds a positive term in the ﬁrst root to obtain a binomial form. The term
containing the root is positive and the root is in the denominator, therefore the whole term
becomes smaller.

• Equalities: solve for the term and factor out.

• Bringing all terms to the denominator (cid:0)(x + y) + 2.911

(cid:1)

(2.911 − 1)(2x + y) +

(cid:18)

π

(cid:113)

(2x + y)2 + 2·2.9112x

π

(cid:19)

.

• Equalities: Multiplying out and expanding terms.

• Last inequality > 0 is proofed in the following sequence of inequalities.

We look at the numerator of the last expression of Eq. (275), which we show to be positive in order to
show > 0 in Eq. (275). The numerator is

− 8x3 + (cid:0)4x2 + 2xy + 2.7795x − 2y2 − 0.9269y − 0.00027798(cid:1) (cid:112)(2x + y)2 + 5.39467x −
(276)

8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x + 2y3 + 3.62336y2 − 0.00053122y .
The factor 4x2 + 2xy + 2.7795x − 2y2 − 0.9269y − 0.00027798 in front of the root is positive:

4x2 + 2xy + 2.7795x − 2y2 − 0.9269y − 0.00027798 >
−2y2 + 0.007 · 2y − 0.9269y + 4 · 0.0072 + 2.7795 · 0.007 − 0.00027798 =
−2y2 − 0.9129y + 2.77942 = −2(y + 1.42897)(y − 0.972523) > 0 .
If the term that does not contain the root would be positive, then everything is positive and we have
proofed the the numerator is positive. Therefore we consider the case that the term that does not
contain the root is negative. The term that contains the root must be larger than the other term in
absolute values.
− (cid:0)−8x3 − 8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x + 2y3 + 3.62336y2 − 0.00053122y(cid:1) <

(277)

(cid:0)4x2 + 2xy + 2.7795x − 2y2 − 0.9269y − 0.00027798(cid:1) (cid:112)(2x + y)2 + 5.39467x .
Therefore the squares of the root term have to be larger than the square of the other term to show > 0
in Eq. (275). Thus, we have the inequality:
(cid:0)−8x3 − 8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x + 2y3 + 3.62336y2 − 0.00053122y(cid:1)2

<

(278)

(279)

68

(cid:0)4x2 + 2xy + 2.7795x − 2y2 − 0.9269y − 0.00027798(cid:1)2 (cid:0)(2x + y)2 + 5.39467x(cid:1) .

This is equivalent to

0 < (cid:0)4x2 + 2xy + 2.7795x − 2y2 − 0.9269y − 0.00027798(cid:1)2 (cid:0)(2x + y)2 + 5.39467x(cid:1) −

(280)

(cid:0)−8x3 − 8x2y − 10.9554x2 + 2xy2 + 1.76901xy − 0.00106244x + 2y3 + 3.62336y2 − 0.00053122y(cid:1)2
x · 4.168614250 · 10−7 − y22.049216091 · 10−7 − 0.0279456x5+
43.0875x4y + 30.8113x4 + 43.1084x3y2 + 68.989x3y + 41.6357x3 + 10.7928x2y3 − 13.1726x2y2−
27.8148x2y − 0.00833715x2 + 0.0139728xy4 + 5.47537xy3+
4.65089xy2 + 0.00277916xy − 10.7858y5 − 12.2664y4 + 0.00436492y3 .

=

We obtain the inequalities:

(281)

x · 4.168614250 · 10−7 − y22.049216091 · 10−7 − 0.0279456x5+
43.0875x4y + 30.8113x4 + 43.1084x3y2 + 68.989x3y + 41.6357x3 + 10.7928x2y3−
13.1726x2y2 − 27.8148x2y − 0.00833715x2+
0.0139728xy4 + 5.47537xy3 + 4.65089xy2 + 0.00277916xy − 10.7858y5 − 12.2664y4 + 0.00436492y3 >
x · 4.168614250 · 10−7 − (0.01)22.049216091 · 10−7 − 0.0279456x5+
0.0 · 43.0875x4 + 30.8113x4 + 43.1084(0.0)2x3 + 0.0 · 68.989x3 + 41.6357x3+
10.7928(0.0)3x2 − 13.1726(0.01)2x2 − 27.8148(0.01)x2 − 0.00833715x2+
0.0139728(0.0)4x + 5.47537(0.0)3x + 4.65089(0.0)2x+
0.0 · 0.00277916x − 10.7858(0.01)5 − 12.2664(0.01)4 + 0.00436492(0.0)3 =
x · 4.168614250 · 10−7 − 1.237626189 · 10−7 − 0.0279456x5 + 30.8113x4 + 41.6357x3 − 0.287802x2 >

−

(cid:16) x

(cid:17)3

0.007

30.7869x4 + 0.160295x3 > 0 .

1.237626189 · 10−7 + 30.8113x4 − (0.875) · 0.0279456x4 + 41.6357x3 −

(0.287802x)x2
0.007

=

We used x (cid:62) 0.007 and x (cid:54) 0.875 (reducing the negative x4-term to a x3-term). We have proofed
the last inequality > 0 of Eq. (275).

Consequently the derivative is always positive independent of y, thus

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is strictly monotonically increasing in x.

Next we show that the sub-function Eq. (111) is smaller than zero. We consider the limit:

(x+y)2
2x

erfc

lim
x→∞

e

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

= 0

The limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach
0 from below. Thus,

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is smaller than zero.

We now consider the derivative of sub-function Eq. (111) with respect to y. We proofed that sub-
function Eq. (111) is strictly monotonically increasing independent of y. In the proof of Theorem 3,
we need the minimum of sub-function Eq. (111). First, we are interested in the derivative of sub-
function Eq. (111) with respect to y for the minimum x = 0.007 = 7/1000.

(282)

(283)

(284)

69

Consequently, we insert the minimum x = 0.007 = 7/1000 into the sub-function Eq. (111):

(cid:32)

e

√
y
2

√

+

7
1000

√

(cid:33)2

7
1000√
2



erfc



√

(cid:113) 7
1000
√

2



 −

y
(cid:113) 7
2
1000

+

(cid:32)

2e

√
y
2

√

7
1000

√

√

+

2

7
1000

(cid:33)2



erfc



√

y
(cid:113) 7
1000

2

√

(cid:114) 7
2

+

1000



 =

500y2

7 +y+ 7

e

2000 erfc

(cid:19)

(cid:18) 1000y + 7
√
20

35

(500y+7)2
3500

− 2e

erfc

(cid:19)

(cid:18) 500y + 7
√
35
10

.

The derivative of this function with respect to y is

(cid:18) 1000y
7

(cid:19)

+ 1

e

500y2

7 +y+ 7

2000 erfc

1
7
(cid:18)

(500y+7)2
3500

4e

(500y + 7) erfc

1 +

1000 · (−0.01)
7

(cid:19)

e−0.01+ 7

35

(cid:18) 1000y + 7
√
20
(cid:18) 500y + 7
√
35
10
2000 + 500·(−0.01)2

+ 20

(cid:19)

7

(cid:19)

−

(cid:114) 5
7π

erfc

>

(7+500·0.01)2
3500

1
7

4e

(7 + 500 · 0.01) erfc

(cid:18) 7 + 500 · 0.01
√
10

35

(cid:19)

−

(cid:18) 7 + 1000 + (−0.01)
√
20
(cid:114) 5
7π

+ 20

> 3.56 .

35

(cid:19)

For the ﬁrst inequality, we use Lemma 24. Lemma 24 says that the function xex2
erfc(x) has the sign
of x and is monotonically increasing to 1√
π . Consequently, we inserted the maximal y = 0.01 to
make the negative term more negative and the minimal y = −0.01 to make the positive term less
positive.

Consequently

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is strictly monotonically increasing in y for the minimal x = 0.007.

Next, we consider x = 0.7 · 0.8 = 0.56, which is the maximal ν = 0.7 and minimal τ = 0.8. We
insert the minimum x = 0.56 = 56/100 into the sub-function Eq. (111):

(cid:32)

e

√
y
2

√

+

56
100

√

56
100√
2

(cid:33)2



erfc



√

(cid:113) 56
100
√

2



 −

y
(cid:113) 56
2
100

+

(cid:32)

2e

√
y
2

√

56
100

√

√

+

2

56
100

(cid:33)2



erfc



√

y
(cid:113) 56
100

2

√

(cid:114) 56
100

2

+



 .

The derivative with respect to y is:

(cid:16) 5y
√
7
2

√

7

5

+

5e

(cid:17)2 (cid:16) 5y
√
7
2

(cid:17)

√

7
5

erfc

(cid:16) 5y
√
7
2

(cid:17)

√

7
5

+

−

(cid:16) 5y
√
7
2

+ 2

√

5

7

10e

(cid:17)2 (cid:16) 5y
√
7
2

√

(cid:17)

7

erfc

(cid:16) 5y
√
7
2

√

(cid:17)

7

+ 2

5

+
√

7

+ 2
√

5

7

(cid:16) √

7

5 − 0.01·5

√

2

7

(cid:17)2 (cid:16) √

5e

7

5 − 0.01·5
2
√

√

7

7

(cid:17)

erfc

(cid:16) √

7

5 − 0.01·5

√

2

7

(cid:17)

−

+

5
√
7π

>

70

(285)

(286)

(287)

(288)

(289)

√

(cid:16) 2

7

5 + 0.01·5

√

(cid:17)2 (cid:16) 2

√

(cid:17)

√

(cid:16) 2

(cid:17)

7

2

7

√

10e

5 + 0.01·5
2
√
7
For the ﬁrst inequality we applied Lemma 24 which states that the function xex2
erfc(x) is monotoni-
cally increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more
negative and the minimal y = −0.01 to make the positive term less positive.

5 + 0.01·5

> 0.00746 .

5
√
7π

erfc

+

√

7

2

7

7

Consequently

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is strictly monotonically increasing in y for x = 0.56.

Next, we consider x = 0.16 · 0.8 = 0.128, which is the minimal τ = 0.8. We insert the minimum
x = 0.128 = 128/1000 into the sub-function Eq. (111):

(cid:32)

e

√
y
2

√

+

128
1000

√

(cid:33)2

128
1000√
2



erfc



√

(cid:113) 128
1000
√

2



 −

y
(cid:113) 128
2
1000

+

(cid:32)

2e

√
y
2

√

128
1000

√

√
2

+

128
1000

(cid:33)2



erfc



√

y
(cid:113) 128
2
1000

√

(cid:114) 128
2
1000

+



 =

125y2

32 +y+ 8

e

125 erfc

(cid:19)

(cid:18) 125y + 16
√
20

10

(125y+32)2
4000

− 2e

erfc

(cid:19)

(cid:18) 125y + 32
√
20

10

.

The derivative with respect to y is:

(cid:18)

e

1
16

(cid:18)

1
16

125y2

32 +y+ 8

125 (125y + 16) erfc

(125y+32)2
4000

2e

(125y + 32) erfc

−

(cid:19)

(cid:18) 125y + 16
√
20
(cid:18) 125y + 32
√
20

10

10

(cid:19)

+ 20

(16 + 125(−0.01))e−0.01+ 8

125 + 125(−0.01)2

32

erfc

(32+1250.01)2
4000

2e

(32 + 1250.01) erfc

(cid:18) 32 + 1250.01
√
20

10

(cid:33)

>

(cid:114) 10
π
(cid:18) 16 + 125(−0.01)
√
20
(cid:114) 10
π

+ 20

10
(cid:33)

(cid:19)

(cid:19)

−

> 0.4468 .

For the ﬁrst inequality we applied Lemma 24 which states that the function xex2
erfc(x) is monotoni-
cally increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more
negative and the minimal y = −0.01 to make the positive term less positive.

Consequently

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

is strictly monotonically increasing in y for x = 0.128.

Next, we consider x = 0.24 · 0.9 = 0.216, which is the minimal τ = 0.9 (here we consider 0.9 as
lower bound for τ ). We insert the minimum x = 0.216 = 216/1000 into the sub-function Eq. (111):

(290)

(291)

(292)

(293)

(294)

(cid:32)

e

√
y
2

√

+

216
1000

√

(cid:33)2

216
1000√
2



erfc



√

(cid:113) 216
1000
√

2



 −

y
(cid:113) 216
2
1000

+

(cid:32)

2e

y

√

√

2

216
1000

√

√
2

+

216
1000

(cid:33)2



erfc



√

y
(cid:113) 216
2
1000

√

(cid:114) 216
2
1000

+



 =

71

(125y+27)2
6750

e

erfc

(cid:19)

(cid:18) 125y + 27
√
15

30

(125y+54)2
6750

− 2e

erfc

(cid:19)

(cid:18) 125y + 54
√
15

30

The derivative with respect to y is:

(125y+27)2
6750

(125y + 27) erfc

(125y+54)2
6750

2e

(125y + 54) erfc

(cid:19)

−

(cid:18) 125y + 27
√
15

30
(cid:19)
(cid:18) 125y + 54
√
15

30

(27 + 125(−0.01))e

(27+125(−0.01))2
6750

erfc

(cid:18)

e

1
27

(cid:18)

1
27

(54+1250.01)2
6750

2e

(54 + 1250.01) erfc

(cid:18) 54 + 1250.01
√
15

30

(cid:33)

>

+ 15

(cid:114) 30
π
(cid:18) 27 + 125(−0.01)
√
15
(cid:19)

30
(cid:114) 30
π

+ 15

(cid:19)

−

(cid:33)

) > 0.211288 .

(295)

For the ﬁrst inequality we applied Lemma 24 which states that the function xex2
erfc(x) is monotoni-
cally increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more
negative and the minimal y = −0.01 to make the positive term less positive.

Consequently

(x+y)2
2x

e

erfc

(cid:19)

(cid:18) x + y
√
√
x
2

(2x+y)2
2x

− 2e

erfc

(cid:19)

(cid:18) 2x + y
√
√
x
2

(296)

is strictly monotonically increasing in y for x = 0.216.

Lemma 46 (Monotone Derivative). For λ = λ01, α = α01 and the domain −0.1 (cid:54) µ (cid:54) 0.1,
−0.1 (cid:54) ω (cid:54) 0.1, 0.00875 (cid:54) ν (cid:54) 0.7, and 0.8 (cid:54) τ (cid:54) 1.25. We are interested of the derivative of

(cid:18)

τ

e

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2·ντ
√
ντ
2

√

(cid:17)2

− 2e

erfc

(cid:18) µω + 2 · ντ
√
2

ντ

√

(cid:19)(cid:19)

.

(297)

The derivative of the equation above with respect to

• ν is larger than zero;

• τ is smaller than zero for maximal ν = 0.7, ν = 0.16, and ν = 0.24 (with 0.9 (cid:54) τ );

• y = µω is larger than zero for ντ = 0.00875 · 0.8 = 0.007, ντ = 0.7 · 0.8 = 0.56,

ντ = 0.16 · 0.8 = 0.128, and ντ = 0.24 · 0.9 = 0.216.

Proof. We consider the domain: −0.1 (cid:54) µ (cid:54) 0.1, −0.1 (cid:54) ω (cid:54) 0.1, 0.00875 (cid:54) ν (cid:54) 0.7, and
0.8 (cid:54) τ (cid:54) 1.25.

We use Lemma 17 to determine the derivatives. Consequently, the derivative of
(cid:18) µω + 2ντ
(cid:18) µω + ντ
√
√
ντ
2
ντ
2

(cid:16) µω+2ντ
√
ντ
2

(cid:16) µω+ντ
√
ντ
2

(cid:18)
e

− 2e

erfc

erfc

√

√

(cid:19)

(cid:17)2

(cid:17)2

τ

√

√

(cid:19)(cid:19)

with respect to ν is larger than zero, which follows directly from Lemma 17 using the chain rule.

Consequently, the derivative of

(cid:18)
e

τ

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− 2e

erfc

(cid:19)(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

(298)

(299)

with respect to y = µω is larger than zero for ντ = 0.00875 · 0.8 = 0.007, ντ = 0.7 · 0.8 = 0.56,
ντ = 0.16 · 0.8 = 0.128, and ντ = 0.24 · 0.9 = 0.216, which also follows directly from Lemma 17.

We now consider the derivative with respect to τ , which is not trivial since τ is a factor of the whole
expression. The sub-expression should be maximized as it appears with negative sign in the mapping
for ν.

72

First, we consider the function for the largest ν = 0.7 and the largest y = µω = 0.01 for determining
the derivative with respect to τ .

The expression becomes



τ


e

(cid:33)2

(cid:32) 7τ
10
√

+ 1
√
100
7τ
2
10



erfc



10 + 1
7τ
100
√
(cid:113) 7τ
2
10



 − 2e

(cid:33)2

(cid:32) 2·7τ
10
√

+ 1
√
100
7τ
2
10



erfc










 .

10 + 1
2·7τ
100
√
(cid:113) 7τ
2
10

The derivative with respect to τ is
(cid:18)

(cid:18)√

(70τ +1)2
14000τ (700τ (7τ + 20) − 1) erfc

e

π

(140τ +1)2
14000τ

2e

(2800τ (7τ + 5) − 1) erfc

+ 20

35(210τ − 1)

τ

(cid:19)

√

(cid:19)

−

(cid:18) 70τ + 1
√
√
τ
(cid:19)(cid:19)

20

35

√

(cid:18) 140τ + 1
√
√
τ

35

20

√

(cid:0)14000

πτ (cid:1)−1

.

We are considering only the numerator and use again the approximation of Ren and MacKenzie [30].
The error analysis on the whole numerator gives an approximation error 97 < E < 186. Therefore
we add 200 to the numerator when we use the approximation Ren and MacKenzie [30]. We obtain
the inequalities:

(cid:18)

√

π

(70τ +1)2
14000τ (700τ (7τ + 20) − 1) erfc

e

(cid:19)

(cid:18) 70τ + 1
√
√
τ
(cid:19)(cid:19)

20
35
(cid:18) 140τ + 1
√
√
τ

20

35

−

√

(140τ +1)2
14000τ

2e

(2800τ (7τ + 5) − 1) erfc

+ 20

35(210τ − 1)

√

τ (cid:54)

(300)

(301)

(302)

2.911(700τ (7τ + 20) − 1)
(cid:114)

π(2.911−1)(70τ +1)
35

20

√

√

τ

+

(cid:16) 70τ +1
√
√
35

20

τ

π

(cid:17)2

+ 2.9112

−

√

2 · 2.911(2800τ (7τ + 5) − 1)
(cid:114)
(cid:17)2

(cid:16) 140τ +1
√
√
τ

35

20

π

π(2.911−1)(140τ +1)
35

√

√

τ

+ 2.9112







35(210τ − 1)

τ + 200 =

(700τ (7τ + 20) − 1) (cid:0)20 ·

√

π(2.911 − 1)(70τ + 1) +

(cid:113)(cid:0)20 · 2.911

2(2800τ (7τ + 5) − 1) (cid:0)20 ·

√

τ (cid:1)

√

35 · 2.911
√
τ (cid:1)2
√

35

τ (cid:1)

√

35 · 2.911
τ (cid:1)2

√

35 · 2.911

√

π(2.911 − 1)(140τ + 1) +

+ π(140τ + 1)2

+ π(70τ + 1)2

20

35(210τ − 1)

τ + 200

+
√

√

√







√

√

π

20
√

+ 20


√

π



√

√

(cid:16)

√

(cid:32)

(cid:16)

√

(cid:32)

√

π(2.911 − 1)(140τ + 1) +

20 ·

35 · 2.911

τ

+ π(140τ + 1)2

+

√

√

35

π(700τ (7τ + 20) − 1)

τ

2.911 · 20
(cid:32)

√

π(2.911 − 1)(140τ + 1) +

20 ·

35 · 2.911

τ

+ π(140τ + 1)2

−

(cid:113)(cid:0)20 ·
(cid:17)

=
(cid:32)

(cid:17)

√

(cid:114)(cid:16)

(cid:114)(cid:16)

√

√

√

−



 +

(cid:33)

(cid:33)

√

(cid:17)2

√

(cid:17)2

73

20

35(210τ − 1)

τ + 200

π(2.911 − 1)(70τ + 1) +

20 ·

35 · 2.911

τ

+ π(70τ + 1)2

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

√

√

τ

(cid:32)

√

(cid:32)(cid:32)

√

(cid:32)

√

√

π2 · 20 ·

35 · 2.911(2800τ (7τ + 5) − 1)

π(2.911 − 1)(70τ + 1) +

20 ·

35 · 2.911

τ

+ π(70τ + 1)2

(cid:114)(cid:16)

√

√

(cid:17)2

π(2.911 − 1)(70τ + 1) +

20

35 · 2.911 ·

τ

+ π(70τ + 1)2

(cid:114)(cid:16)

√

(cid:114)(cid:16)

√

√

(cid:17)2

√

(cid:17)2

π(2.911 − 1)(140τ + 1) +

20

35 · 2.911 ·

τ

+ π(140τ + 1)2

.

(cid:33)(cid:33)

(cid:33)

(cid:33)(cid:33)−1

√

After applying the approximation of Ren and MacKenzie [30] and adding 200, we ﬁrst factored out
20

τ . Then we brought all terms to the same denominator.

35

√

We now consider the numerator:

20

35(210τ − 1)

τ + 200

π(2.911 − 1)(70τ + 1) +

20 ·

35 · 2.911

τ

+ π(70τ + 1)2

√

(cid:32)

√

(cid:17)

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

(303)

(cid:16)

√

(cid:32)

√

2.911 · 20
(cid:32)

√

√

√

(cid:32)

√

π(2.911 − 1)(140τ + 1) +

20 ·

35 · 2.911

τ

+ π(140τ + 1)2

+

√

√

35

π(700τ (7τ + 20) − 1)

τ

(cid:114)(cid:16)

(cid:114)(cid:16)

√

√

√

√

(cid:17)2

√

(cid:17)2

π(2.911 − 1)(140τ + 1) +

20 ·

35 · 2.911

τ

+ π(140τ + 1)2

−

π2 · 20 ·

35 · 2.911(2800τ (7τ + 5) − 1)

τ

√

π(2.911 − 1)(70τ + 1) +

20 ·

35 · 2.911

τ

+ π(70τ + 1)2

=

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

(cid:33)

(cid:33)

√

− 1.70658 × 107(cid:112)π(70τ + 1)2 + 118635τ τ 3/2+
35(cid:112)π(70τ + 1)2 + 118635τ (cid:112)π(140τ + 1)2 + 118635τ τ 3/2 +
4200
8.60302 × 106(cid:112)π(140τ + 1)2 + 118635τ τ 3/2 − 2.89498 × 107τ 3/2 −
1.21486 × 107(cid:112)π(70τ + 1)2 + 118635τ τ 5/2 + 8.8828 × 106(cid:112)π(140τ + 1)2 + 118635τ τ 5/2 −
2.43651 × 107τ 5/2 − 1.46191 × 109τ 7/2 + 2.24868 × 107τ 2 + 94840.5(cid:112)π(70τ + 1)2 + 118635τ τ +
47420.2(cid:112)π(140τ + 1)2 + 118635τ τ + 481860τ + 710.354
820.213

τ +

√

√

√

τ (cid:112)π(70τ + 1)2 + 118635τ + 677.432(cid:112)π(70τ + 1)2 + 118635τ −
τ (cid:112)π(140τ + 1)2 + 118635τ −
τ (cid:112)π(70τ + 1)2 + 118635τ (cid:112)π(140τ + 1)2 + 118635τ +

1011.27
√
√

35

20
200(cid:112)π(70τ + 1)2 + 118635τ (cid:112)π(140τ + 1)2 + 118635τ +
677.432(cid:112)π(140τ + 1)2 + 118635τ + 2294.57 =
− 2.89498 × 107τ 3/2 − 2.43651 × 107τ 5/2 − 1.46191 × 109τ 7/2 +
(cid:16)

−1.70658 × 107τ 3/2 − 1.21486 × 107τ 5/2 + 94840.5τ + 820.213

√

τ + 677.432

(cid:17)

8.60302 × 106τ 3/2 + 8.8828 × 106τ 5/2 + 47420.2τ − 1011.27

τ + 677.432

√

(cid:17)

(cid:112)π(70τ + 1)2 + 118635τ +
(cid:16)

(cid:112)π(140τ + 1)2 + 118635τ +
(cid:16)
35τ 3/2 − 20

4200

35

√

√

√

τ + 200

(cid:17) (cid:112)π(70τ + 1)2 + 118635τ (cid:112)π(140τ + 1)2 + 118635τ +

74

2.24868 × 107τ 2 + 481860.τ + 710.354
− 2.89498 × 107τ 3/2 − 2.43651 × 107τ 5/2 − 1.46191 × 109τ 7/2+
(cid:16)

τ + 2294.57 (cid:54)

−1.70658 × 107τ 3/2 − 1.21486 × 107τ 5/2 + 820.213

√

√

1.25 + 1.25 · 94840.5 + 677.432

(cid:17)

(cid:112)π(70τ + 1)2 + 118635τ +
(cid:16)

8.60302 × 106τ 3/2 + 8.8828 × 106τ 5/2 − 1011.27

0.8 + 1.25 · 47420.2 + 677.432

√

(cid:17)

(cid:112)π(140τ + 1)2 + 118635τ +
(cid:16)
35τ 3/2 − 20

4200

35

√

√

√

(cid:17)

τ + 200

(cid:112)π(70τ + 1)2 + 118635τ (cid:112)π(140τ + 1)2 + 118635τ +
√
2.24868 × 107τ 2 + 710.354
− 2.89498 × 107τ 3/2 − 2.43651 × 107τ 5/2 − 1.46191 × 109τ 7/2+
(cid:16)

1.25 + 1.25 · 481860 + 2294.57 =

−1.70658 × 107τ 3/2 − 1.21486 × 107τ 5/2 + 120145.

8.60302 × 106τ 3/2 + 8.8828 × 106τ 5/2 + 59048.2
√

√

√

4200

35τ 3/2 − 20

35

τ + 200

(cid:17) (cid:112)π(70τ + 1)2 + 118635τ (cid:112)π(140τ + 1)2 + 118635τ +

(cid:17) (cid:112)π(70τ + 1)2 + 118635τ +
(cid:17) (cid:112)π(140τ + 1)2 + 118635τ +

2.24868 × 107τ 2 + 605413 =
− 2.89498 × 107τ 3/2 − 2.43651 × 107τ 5/2 − 1.46191 × 109τ 7/2+
(cid:16)

8.60302 × 106τ 3/2 + 8.8828 × 106τ 5/2 + 59048.2

−1.70658 × 107τ 3/2 − 1.21486 × 107τ 5/2 + 120145.

√

4200

35τ 3/2 − 20

35

τ + 200

√

√

(cid:17)

(cid:17) (cid:112)19600π(τ + 1.94093)(τ + 0.0000262866)+
(cid:17) (cid:112)4900π(τ + 7.73521)(τ + 0.0000263835)+

(cid:112)19600π(τ + 1.94093)(τ + 0.0000262866)(cid:112)4900π(τ + 7.73521)(τ + 0.0000263835)+
2.24868 × 107τ 2 + 605413 (cid:54)
− 2.89498 × 107τ 3/2 − 2.43651 × 107τ 5/2 − 1.46191 × 109τ 7/2+
(cid:16)

8.60302 × 106τ 3/2 + 8.8828 × 106τ 5/2 + 59048.2

(cid:17) (cid:112)19600π(τ + 1.94093)τ +

−1.70658 × 107τ 3/2 − 1.21486 × 107τ 5/2 + 120145.

(cid:17) (cid:112)4900π1.00003(τ + 7.73521)τ +

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

√

√

√

(cid:17) (cid:112)19600π1.00003(τ + 1.94093)τ

35

35τ 3/2 − 20

4200

τ + 200
(cid:112)4900π1.00003(τ + 7.73521)τ +
2.24868 × 107τ 2 + 605413 =
− 2.89498 × 107τ 3/2 − 2.43651 × 107τ 5/2 − 1.46191 × 109τ 7/2+
(cid:16)

−3.64296 × 106τ 3/2 + 7.65021 × 108τ 5/2 + 6.15772 × 106τ

(cid:17)

√

τ + 7.73521 + 2.24868 × 107τ 2+

√
τ + 1.94093
(cid:0)2.20425 × 109τ 3 + 2.13482 × 109τ 2 + 1.46527 × 107√
(cid:0)−1.5073 × 109τ 3 − 2.11738 × 109τ 2 + 1.49066 × 107√
√

√

(cid:16)

τ (cid:1) √
τ (cid:1) √

τ + 1.94093+
τ + 7.73521 + 605413 (cid:54)

1.25 + 7.73521

1.25 + 1.94093
1.25 + 1.94093 (cid:0)2.20425 × 109τ 3 + 2.13482 × 109τ 2 + 1.46527 × 107√
0.8 + 7.73521 (cid:0)−1.5073 × 109τ 3 − 2.11738 × 109τ 2 + 1.49066 × 107√

√

√

τ (cid:1) +
τ (cid:1) −

−3.64296 × 106τ 3/2 + 7.65021 × 108τ 5/2 + 6.15772 × 106τ

+

(cid:17)

75

2.89498 × 107τ 3/2 − 2.43651 × 107τ 5/2 − 1.46191 × 109τ 7/2 + 2.24868 × 107τ 2 + 605413 =
− 4.84561 × 107τ 3/2 + 4.07198 × 109τ 5/2 − 1.46191 × 109τ 7/2−
4.66103 × 108τ 3 − 2.34999 × 109τ 2+
3.29718 × 107τ + 6.97241 × 107√
605413τ 3/2
0.83/2

− 4.84561 × 107τ 3/2+

τ + 605413 (cid:54)

4.07198 × 109τ 5/2 − 1.46191 × 109τ 7/2−

4.66103 × 108τ 3 − 2.34999 × 109τ 2 +
τ 3/2 (cid:16)
4.07198 × 109τ + 7.64087 × 107(cid:1) (cid:54)

−4.66103 × 108τ 3/2 − 1.46191 × 109τ 2 − 2.34999 × 109√

τ +

3.29718 × 107√
√

τ τ

+

0.8

6.97241 × 107τ

0.8

√

τ

=

7.64087 × 107√
√
0.8

τ

−

−4.66103 × 108τ 3/2 − 1.46191 × 109τ 2 +

τ + 4.07198 × 109τ (cid:1) =

−1.46191 × 109τ 3/2 + 4.07198 × 109√

(cid:18)

τ 3/2
2.34999 × 109√
τ 2 (cid:16)
(cid:16)

−2.26457 × 109 + 4.07198 × 109

− 4.14199 × 107τ 2 < 0 .

τ − 4.66103 × 108τ − 2.26457 × 109(cid:17)
(cid:54)
0.8 − 4.66103 × 1080.8 − 1.46191 × 1090.83/2(cid:17)

√

τ 2 =

First we expanded the term (multiplied it out). The we put the terms multiplied by the same square
root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for τ for
some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign.
The next equality factors the terms under the squared root. We decreased the negative term by setting
τ = τ + 0.0000263835 under the root. We increased positive terms by setting τ + 0.000026286 =
1.00003τ and τ + 0.000026383 = 1.00003τ under the root for positive terms. The positive terms are
= 1.00003, thus τ + 0.000026286 < τ + 0.000026383 (cid:54) 1.00003τ .
increase, since 0.8+0.000026383
For the next inequality we decreased negative terms by inserting τ = 0.8 and increased positive terms
by inserting τ = 1.25. The next equality expands the terms. We use upper bound of 1.25 and lower
bound of 0.8 to obtain terms with corresponding exponents of τ .
For the last (cid:54)-sign we used the function

0.8

−1.46191 × 109τ 3/2 + 4.07198 × 109√

τ − 4.66103 × 108τ − 2.26457 × 109

(304)

The derivative of this function is

and the second order derivative is

−2.19286 × 109√

τ +

2.03599 × 109
√
τ

− 4.66103 × 108

(305)

−

1.01799 × 109
τ 3/2

−

1.09643 × 109
√
τ

< 0 .

The derivative at 0.8 is smaller than zero:

− 2.19286 × 109

0.8 − 4.66103 × 108 +

√

2.03599 × 109
0.8

√

=

− 1.51154 × 108 < 0 .

(306)

(307)

Since the second order derivative is negative, the derivative decreases with increasing τ . Therefore
the derivative is negative for all values of τ that we consider, that is, the function Eq. (304) is strictly
monotonically decreasing. The maximum of the function Eq. (304) is therefore at 0.8. We inserted
0.8 to obtain the maximum.

76

Consequently, the derivative of

(cid:18)
e

τ

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− 2e

erfc

(cid:19)(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

(308)

with respect to τ is smaller than zero for maximal ν = 0.7.

Next, we consider the function for the largest ν = 0.16 and the largest y = µω = 0.01 for determining
the derivative with respect to τ .

The expression becomes
(cid:33)2



(cid:32) 16τ
100
√

+ 1
√
100
16τ
2
100

τ


e



erfc



100 + 1
16τ
100
√
(cid:113) 16τ
2
100



 − e

(cid:33)2

(cid:32) 2 16τ
√
100
√
2

+ 1
100

16τ
100



erfc










 .

100 + 1
2 16τ
100
√
(cid:113) 16τ
100

2

The derivative with respect to τ is

(cid:18)√

(cid:18)

π

e

(16τ +1)2
3200τ

(128τ (2τ + 25) − 1) erfc

(32τ +1)2
3200τ

2e

(128τ (8τ + 25) − 1) erfc

√

(cid:0)3200

πτ (cid:1)−1

.

(cid:18) 16τ + 1
√
√
τ
2
(cid:19)(cid:19)

40
(cid:18) 32τ + 1
√
√
τ
2

40

(cid:19)

−

√

+ 40

2(48τ − 1)

τ

(cid:19)

√

We are considering only the numerator and use again the approximation of Ren and MacKenzie [30].
The error analysis on the whole numerator gives an approximation error 1.1 < E < 12. Therefore
we add 20 to the numerator when we use the approximation of Ren and MacKenzie [30]. We obtain
the inequalities:

√

(cid:18)

π

e

(16τ +1)2
3200τ

(128τ (2τ + 25) − 1) erfc

(cid:19)

−

40

(cid:18) 16τ + 1
√
√
2
τ
(cid:19)(cid:19)
(cid:18) 32τ + 1
√
√
τ

40

2

(32τ +1)2
3200τ

2e

(128τ (8τ + 25) − 1) erfc

√

+ 40

2(48τ − 1)

√

τ (cid:54)

(309)

(310)

(311)







√

√

π

40
√

+ 40


√

π



√

√

40
(cid:32)

(cid:32)

√

2.911(128τ (2τ + 25) − 1)
(cid:114)
(cid:17)2

(cid:16) 16τ +1
√
√
τ
40

2

π

π(2.911−1)(16τ +1)
2

40

√

√

τ

+

−

+ 2.9112

√

2 · 2.911(128τ (8τ + 25) − 1)
(cid:114)
(cid:17)2

(cid:16) 32τ +1
√
√
τ
40

2

π

π(2.911−1)(32τ +1)
2

√

√

τ

+ 2.9112

+
√







2(48τ − 1)

τ + 20 =

π(2.911 − 1)(16τ + 1) +

(128τ (2τ + 25) − 1) (cid:0)40
√

(cid:113)(cid:0)40
√
2(128τ (8τ + 25) − 1) (cid:0)40

√

√

τ (cid:1)

22.911
√

τ (cid:1)2

22.911
√

22.911
√
τ (cid:1)2

τ (cid:1)

−

+ π(16τ + 1)2



 +

(cid:113)(cid:0)40

√

22.911

+ π(32τ + 1)2

π(2.911 − 1)(32τ + 1) +
√
√

2(48τ − 1)

τ + 20 =

(cid:16)

√

√

(cid:32)

√

(cid:17)

40

2(48τ − 1)

τ + 20

π(2.911 − 1)(16τ + 1) +

40

22.911

τ

+ π(16τ + 1)2

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

π(2.911 − 1)(32τ + 1) +

40

22.911

τ

+ π(32τ + 1)2

+ +

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

77

√

√
2

2.911 · 40
(cid:32)

√

π(128τ (2τ + 25) − 1)

τ

√

π(2.911 − 1)(32τ + 1) +

40

22.911

τ

+ π(32τ + 1)2

−

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

√

π40
(cid:32)

√

22.911(128τ (8τ + 25) − 1)
(cid:114)(cid:16)

π(2.911 − 1)(16τ + 1) +

√

√

(cid:17)2

40

22.911

τ

+ π(16τ + 1)2

(cid:33)(cid:33)

π(2.911 − 1)(32τ + 1) +

40

22.911

τ

+ π(32τ + 1)2

(cid:114)(cid:16)

√

√

(cid:17)2

π(2.911 − 1)(32τ + 1) +

40

22.911

τ

+ π(32τ + 1)2

.

(cid:114)(cid:16)

√

√

(cid:17)2

√
2

√

τ

(cid:32)(cid:32)

√

(cid:32)

√

(cid:33)

(cid:33)(cid:33)−1

√

After applying the approximation of Ren and MacKenzie [30] and adding 20, we ﬁrst factored out
40

τ . Then we brought all terms to the same denominator.

√

2

We now consider the numerator:
√
(cid:16)

√

√

(cid:32)

(cid:17)

40

2(48τ − 1)

τ + 20

π(2.911 − 1)(16τ + 1) +

40

22.911

τ

+ π(16τ + 1)2

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

(312)

(cid:32)

√

2.911 · 40
(cid:32)

√

√

√
2
(cid:32)

√

π(2.911 − 1)(32τ + 1) +

40

22.911

τ

+ π(32τ + 1)2

+

(cid:114)(cid:16)

√

√

(cid:17)2

√

√
2

π(128τ (2τ + 25) − 1)

τ

π(2.911 − 1)(32τ + 1) +

40

22.911

τ

+ π(32τ + 1)2

−

√

√

√

√

(cid:114)(cid:16)

(cid:114)(cid:16)

√

(cid:17)2

√

(cid:17)2

π40

22.911(128τ (8τ + 25) − 1)

τ

(cid:33)

(cid:33)

(cid:33)

π(2.911 − 1)(16τ + 1) +

40

22.911

τ

+ π(16τ + 1)2

=

√

2(cid:112)π(16τ + 1)2 + 27116.5τ (cid:112)π(32τ + 1)2 + 27116.5τ τ 3/2+

− 1.86491 × 106(cid:112)π(16τ + 1)2 + 27116.5τ τ 3/2+
1920
940121(cid:112)π(32τ + 1)2 + 27116.5τ τ 3/2 − 3.16357 × 106τ 3/2−
303446(cid:112)π(16τ + 1)2 + 27116.5τ τ 5/2 + 221873(cid:112)π(32τ + 1)2 + 27116.5τ τ 5/2 − 608588τ 5/2−
8.34635 × 106τ 7/2 + 117482.τ 2 + 2167.78(cid:112)π(16τ + 1)2 + 27116.5τ τ +
1083.89(cid:112)π(32τ + 1)2 + 27116.5τ τ +
√
11013.9τ + 339.614
67.7432(cid:112)π(16τ + 1)2 + 27116.5τ − 483.478
40
20(cid:112)π(16τ + 1)2 + 27116.5τ (cid:112)π(32τ + 1)2 + 27116.5τ +
67.7432(cid:112)π(32τ + 1)2 + 27116.5τ + 229.457 =
− 3.16357 × 106τ 3/2 − 608588τ 5/2 − 8.34635 × 106τ 7/2+
(cid:16)

τ (cid:112)π(16τ + 1)2 + 27116.5τ (cid:112)π(32τ + 1)2 + 27116.5τ +

τ (cid:112)π(16τ + 1)2 + 27116.5τ +

τ (cid:112)π(32τ + 1)2 + 27116.5τ −

τ + 392.137

√
2

√

√

√

√

(cid:17)

−1.86491 × 106τ 3/2 − 303446τ 5/2 + 2167.78τ + 392.137

τ + 67.7432

(cid:112)π(16τ + 1)2 + 27116.5τ +
(cid:16)

940121τ 3/2 + 221873τ 5/2 + 1083.89τ − 483.478

τ + 67.7432

√

(cid:17)

78

(cid:112)π(32τ + 1)2 + 27116.5τ +
(cid:16)
2τ 3/2 − 40

√
2

1920

√

√

τ + 20

(cid:17) (cid:112)π(16τ + 1)2 + 27116.5τ (cid:112)π(32τ + 1)2 + 27116.5τ +
√

117482.τ 2 + 11013.9τ + 339.614
− 3.16357 × 106τ 3/2 − 608588τ 5/2 − 8.34635 × 106τ 7/2+
(cid:16)

τ + 229.457 (cid:54)

−1.86491 × 106τ 3/2 − 303446τ 5/2 + 392.137

√

1.25 + 1.252167.78 + 67.7432

(cid:17)

(cid:112)π(16τ + 1)2 + 27116.5τ +
(cid:16)

√

940121τ 3/2 + 221873τ 5/2 − 483.478

0.8 + 1.251083.89 + 67.7432

(cid:17)

(cid:112)π(32τ + 1)2 + 27116.5τ +
(cid:16)
2τ 3/2 − 40

1920

√

√

τ + 20

√
2
√

(cid:17) (cid:112)π(16τ + 1)2 + 27116.5τ (cid:112)π(32τ + 1)2 + 27116.5τ +

117482.τ 2 + 339.614
− 3.16357 × 106τ 3/2 − 608588τ 5/2 − 8.34635 × 106τ 7/2+
(cid:16)

1.25 + 1.2511013.9 + 229.457 =

−1.86491 × 106τ 3/2 − 303446τ 5/2 + 3215.89

(cid:17) (cid:112)π(16τ + 1)2 + 27116.5τ +

940121τ 3/2 + 221873τ 5/2 + 990.171
√

√

1920

2τ 3/2 − 40

√
2

τ + 20

(cid:17) (cid:112)π(16τ + 1)2 + 27116.5τ (cid:112)π(32τ + 1)2 + 27116.5τ +

(cid:17) (cid:112)π(32τ + 1)2 + 27116.5τ +

117482τ 2 + 14376.6 =
− 3.16357 × 106τ 3/2 − 608588τ 5/2 − 8.34635 × 106τ 7/2+
(cid:16)

940121τ 3/2 + 221873τ 5/2 + 990.171

(cid:17) (cid:112)1024π(τ + 8.49155)(τ + 0.000115004)+

(cid:17) (cid:112)256π(τ + 33.8415)(τ + 0.000115428)+

(cid:17) (cid:112)1024π(τ + 8.49155)(τ + 0.000115004)

(cid:16)

√

√

1920

τ + 20

2τ 3/2 − 40

−1.86491 × 106τ 3/2 − 303446τ 5/2 + 3215.89
√
2
(cid:112)256π(τ + 33.8415)(τ + 0.000115428)+
117482.τ 2 + 14376.6 (cid:54)
− 3.16357 × 106τ 3/2 − 608588τ 5/2 − 8.34635 × 106τ 7/2+
(cid:16)

940121τ 3/2 + 221873τ 5/2 + 990.171
√

√

1920

2τ 3/2 − 40

(cid:17) (cid:112)1024π1.00014(τ + 8.49155)τ +

√
2

τ + 20

(cid:17) (cid:112)256π1.00014(τ + 33.8415)τ (cid:112)1024π1.00014(τ + 8.49155)τ +

−1.86491 × 106τ 3/2 − 303446τ 5/2 + 3215.89

(cid:17) (cid:112)256π(τ + 33.8415)τ +

117482.τ 2 + 14376.6 =
− 3.16357 × 106τ 3/2 − 608588τ 5/2 − 8.34635 × 106τ 7/2+
(cid:16)

(cid:17) √

−91003τ 3/2 + 4.36814 × 106τ 5/2 + 32174.4τ
(cid:0)1.25852 × 107τ 3 + 5.33261 × 107τ 2 + 56165.1
(cid:0)−8.60549 × 106τ 3 − 5.28876 × 107τ 2 + 91200.4
√

√

√

(cid:16)

√

τ + 8.49155
τ (cid:1) √
τ (cid:1) √
√

τ + 8.49155+

τ + 33.8415 + 14376.6 (cid:54)
(cid:17)

1.25 + 33.8415

1.25 + 8.49155
1.25 + 8.49155 (cid:0)1.25852 × 107τ 3 + 5.33261 × 107τ 2 + 56165.1
0.8 + 33.8415 (cid:0)−8.60549 × 106τ 3 − 5.28876 × 107τ 2 + 91200.4

√

√

√

τ (cid:1) +
√
τ (cid:1) −

−91003τ 3/2 + 4.36814 × 106τ 5/2 + 32174.4τ

+

3.16357 × 106τ 3/2 − 608588τ 5/2 − 8.34635 × 106τ 7/2 + 117482.τ 2 + 14376.6 =

τ + 33.8415 + 117482.τ 2+

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

79

− 4.84613 × 106τ 3/2 + 8.01543 × 107τ 5/2 − 8.34635 × 106τ 7/2−
1.13691 × 107τ 3 − 1.44725 × 108τ 2+
τ + 14376.6 (cid:54)
594875.τ + 712078.
14376.6τ 3/2
0.83/2

− 4.84613 × 106τ 3/2+

√

8.01543 × 107τ 5/2 − 8.34635 × 106τ 7/2−

1.13691 × 107τ 3 − 1.44725 × 108τ 2 +

√

τ τ

594875.
√
0.8

+

712078.τ
0.8

√

τ

=

− 3.1311 · 106τ 3/2 − 1.44725 · 108τ 2 + 8.01543 · 107τ 5/2 − 1.13691 · 107τ 3−
8.34635 · 106τ 7/2 (cid:54)

− 3.1311 × 106τ 3/2 +

8.01543 × 107
τ

√

√

1.25τ 5/2

−

8.34635 × 106τ 7/2 − 1.13691 × 107τ 3 − 1.44725 × 108τ 2 =
− 3.1311 × 106τ 3/2 − 8.34635 × 106τ 7/2 − 1.13691 × 107τ 3 − 5.51094 × 107τ 22 < 0 .

First we expanded the term (multiplied it out). The we put the terms multiplied by the same square
root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for τ for
some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign.
The next equality factors the terms under the squared root. We decreased the negative term by setting
τ = τ + 0.00011542 under the root. We increased positive terms by setting τ + 0.00011542 =
1.00014τ and τ + 0.000115004 = 1.00014τ under the root for positive terms. The positive terms are
< 1.000142, thus τ + 0.000115004 < τ + 0.00011542 (cid:54) 1.00014τ .
increase, since 0.8+0.00011542
For the next inequality we decreased negative terms by inserting τ = 0.8 and increased positive terms
by inserting τ = 1.25. The next equality expands the terms. We use upper bound of 1.25 and lower
bound of 0.8 to obtain terms with corresponding exponents of τ .

0.8

Consequently, the derivative of

(cid:18)
e

τ

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− 2e

erfc

(cid:19)(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

(313)

with respect to τ is smaller than zero for maximal ν = 0.16.

Next, we consider the function for the largest ν = 0.24 and the largest y = µω = 0.01 for determining
the derivative with respect to τ . However we assume 0.9 (cid:54) τ , in order to restrict the domain of τ .

The expression becomes
(cid:33)2



(cid:32) 24τ
100
√

+ 1
√
100
24τ
2
100

τ


e



erfc



100 + 1
24τ
100
√
(cid:113) 24τ
2
100



 − e

(cid:33)2

(cid:32) 2 24τ
√
100
√
2

+ 1
100

24τ
100



erfc










 .

100 + 1
2 24τ
100
√
(cid:113) 24τ
100

2

The derivative with respect to τ is
(cid:18)

(cid:18)√

(24τ +1)2
4800τ

π

e

(192τ (3τ + 25) − 1) erfc

(48τ +1)2
4800τ

2e

(192τ (12τ + 25) − 1) erfc

√

(cid:19)

√

+ 40

3(72τ − 1)

τ

√

(cid:0)4800

πτ (cid:1)−1

.

(cid:19)

−

(cid:18) 24τ + 1
√
√
3
τ
(cid:19)(cid:19)

40
(cid:18) 48τ + 1
√
√
τ
3

40

We are considering only the numerator and use again the approximation of Ren and MacKenzie [30].
The error analysis on the whole numerator gives an approximation error 14 < E < 32. Therefore we
add 32 to the numerator when we use the approximation of Ren and MacKenzie [30]. We obtain the
inequalities:
(cid:18)
√

(cid:19)

(24τ +1)2
4800τ

π

e

(192τ (3τ + 25) − 1) erfc

(316)

(cid:18) 24τ + 1
√
√
τ
3

40

−

(314)

(315)

80

(48τ +1)2
4800τ

2e

(192τ (12τ + 25) − 1) erfc

(cid:18) 48τ + 1
√
√
τ
3

40

(cid:19)(cid:19)

√

+ 40

3(72τ − 1)

√

τ (cid:54)







√

√

π

2.911(192τ (3τ + 25) − 1)
(cid:114)
(cid:17)2

(cid:16) 24τ +1
√
√
τ
40

3

π

π(2.911−1)(24τ +1)
3

40

√

√

τ

+

−

+ 2.9112







+

2 · 2.911(192τ (12τ + 25) − 1)
(cid:114)

π(2.911−1)(48τ +1)
3

40

√

√

τ

+

(cid:16) 48τ +1
√
√
τ
40

3

π

(cid:17)2

+ 2.9112

√

3(72τ − 1)


τ + 32 =
(192τ (3τ + 25) − 1) (cid:0)40
√

√

π(2.911 − 1)(24τ + 1) +

(cid:113)(cid:0)40
√
2(192τ (12τ + 25) − 1) (cid:0)40

π



√

√

τ (cid:1)

32.911
√

τ (cid:1)2

32.911

−

+ π(24τ + 1)2

√

τ (cid:1)

(cid:113)(cid:0)40

√

32.911

32.911
√
τ (cid:1)2

+ π(48τ + 1)2



 +

π(2.911 − 1)(48τ + 1) +
√
√

3(72τ − 1)

τ + 32 =

(cid:16)

√

√

(cid:32)

√

(cid:17)

π(2.911 − 1)(48τ + 1) +

40

32.911

τ

+ π(48τ + 1)2

+

(cid:114)(cid:16)

√

√

(cid:17)2

√

√
3

2.911 · 40
(cid:32)

√

π(192τ (3τ + 25) − 1)

τ

√

π(2.911 − 1)(48τ + 1) +

40

32.911

τ

+ π(48τ + 1)2

−

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

(cid:33)

√

π40
(cid:32)

√

32.911(192τ (12τ + 25) − 1)
(cid:114)(cid:16)

π(2.911 − 1)(24τ + 1) +

√

√

(cid:17)2

40

32.911

τ

+ π(24τ + 1)2

(cid:33)(cid:33)

π(2.911 − 1)(24τ + 1) +

40

32.911

τ

+ π(24τ + 1)2

(cid:114)(cid:16)

√

√

(cid:17)2

π(2.911 − 1)(48τ + 1) +

40

32.911

τ

+ π(48τ + 1)2

.

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

(cid:33)(cid:33)−1

√

√

40

√

√

40
(cid:32)

(cid:32)

√

√
2

√

τ

(cid:32)(cid:32)

√

(cid:32)

√

40

3(72τ − 1)

τ + 32

π(2.911 − 1)(24τ + 1) +

40

32.911

τ

+ π(24τ + 1)2

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

√

After applying the approximation of Ren and MacKenzie [30] and adding 200, we ﬁrst factored out
40

τ . Then we brought all terms to the same denominator.

√

3

We now consider the numerator:

(cid:16)

√

√

(cid:32)

√

(cid:17)

40

3(72τ − 1)

τ + 32

π(2.911 − 1)(24τ + 1) +

40

32.911

τ

+ π(24τ + 1)2

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

(317)

(cid:32)

√

π(2.911 − 1)(48τ + 1) +

40

32.911

τ

+ π(48τ + 1)2

+

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

2.911 · 40

π(192τ (3τ + 25) − 1)

τ

√

√
3

√

81

(cid:32)

√

√
2
(cid:32)

√

π(2.911 − 1)(48τ + 1) +

40

32.911

τ

+ π(48τ + 1)2

−

(cid:114)(cid:16)

√

√

(cid:17)2

√

π40

32.911(192τ (12τ + 25) − 1)

τ

√

π(2.911 − 1)(24τ + 1) +

40

32.911

τ

+ π(24τ + 1)2

=

(cid:114)(cid:16)

√

√

(cid:17)2

(cid:33)

(cid:33)

√

− 3.42607 × 106(cid:112)π(24τ + 1)2 + 40674.8τ τ 3/2+
3(cid:112)π(24τ + 1)2 + 40674.8τ (cid:112)π(48τ + 1)2 + 40674.8τ τ 3/2+
2880
1.72711 × 106(cid:112)π(48τ + 1)2 + 40674.8τ τ 3/2 − 5.81185 × 106τ 3/2 −
836198(cid:112)π(24τ + 1)2 + 40674.8τ τ 5/2 + 611410(cid:112)π(48τ + 1)2 + 40674.8τ τ 5/2−
1.67707 × 106τ 5/2 −
3.44998 × 107τ 7/2 + 422935.τ 2 + 5202.68(cid:112)π(24τ + 1)2 + 40674.8τ τ +
2601.34(cid:112)π(48τ + 1)2 + 40674.8τ τ +
√
26433.4τ + 415.94
108.389(cid:112)π(24τ + 1)2 + 40674.8τ − 592.138
40
32(cid:112)π(24τ + 1)2 + 40674.8τ (cid:112)π(48τ + 1)2 + 40674.8τ +
108.389(cid:112)π(48τ + 1)2 + 40674.8τ + 367.131 =
− 5.81185 × 106τ 3/2 − 1.67707 × 106τ 5/2 − 3.44998 × 107τ 7/2+
(cid:16)

τ (cid:112)π(24τ + 1)2 + 40674.8τ (cid:112)π(48τ + 1)2 + 40674.8τ +

τ (cid:112)π(24τ + 1)2 + 40674.8τ +

τ (cid:112)π(48τ + 1)2 + 40674.8τ −

τ + 480.268

−3.42607 × 106τ 3/2 − 836198τ 5/2 + 5202.68τ + 480.268

τ + 108.389

√
3

√

√

√

√

(cid:17)

(cid:112)π(24τ + 1)2 + 40674.8τ +
(cid:16)

1.72711 × 106τ 3/2 + 611410τ 5/2 + 2601.34τ − 592.138

τ + 108.389

√

(cid:17)

(cid:112)π(48τ + 1)2 + 40674.8τ +
(cid:16)
3τ 3/2 − 40

√
3

2880

√

√

τ + 32

(cid:17) (cid:112)π(24τ + 1)2 + 40674.8τ (cid:112)π(48τ + 1)2 + 40674.8τ +
√

422935.τ 2 + 26433.4τ + 415.94
− 5.81185 × 106τ 3/2 − 1.67707 × 106τ 5/2 − 3.44998 × 107τ 7/2+
(cid:16)

τ + 367.131 (cid:54)

−3.42607 × 106τ 3/2 − 836198τ 5/2 + 480.268

√

1.25 + 1.255202.68 + 108.389

1.72711 × 106τ 3/2 + 611410τ 5/2 − 592.138

0.9 + 1.252601.34 + 108.389

√

(cid:17)

(cid:17)

(cid:112)π(24τ + 1)2 + 40674.8τ +
(cid:16)

(cid:112)π(48τ + 1)2 + 40674.8τ +
(cid:16)
3τ 3/2 − 40

√
3

2880

√

√

τ + 32

√

(cid:17) (cid:112)π(24τ + 1)2 + 40674.8τ (cid:112)π(48τ + 1)2 + 40674.8τ +

422935τ 2 + 415.94
− 5.81185 × 106τ 3/2 − 1.67707 × 106τ 5/2 − 3.44998 × 107τ 7/2+
(cid:16)

1.25 + 1.2526433.4 + 367.131 =

−3.42607 × 106τ 3/2 − 836198τ 5/2 + 7148.69

(cid:16)

(cid:16)

1.72711 × 106τ 3/2 + 611410τ 5/2 + 2798.31
√
3

3τ 3/2 − 40

τ + 32

2880

√

√

(cid:17) (cid:112)π(24τ + 1)2 + 40674.8τ +
(cid:17) (cid:112)π(48τ + 1)2 + 40674.8τ +
(cid:17) (cid:112)π(24τ + 1)2 + 40674.8τ (cid:112)π(48τ + 1)2 + 40674.8τ +

422935τ 2 + 33874 =

82

− 5.81185 × 106τ 3/2 − 1.67707 × 106τ 5/2 − 3.44998 × 107τ 7/2+
(cid:16)

1.72711 × 106τ 3/2 + 611410τ 5/2 + 2798.31

(cid:17) (cid:112)2304π(τ + 5.66103)(τ + 0.0000766694)+
(cid:17) (cid:112)576π(τ + 22.561)(τ + 0.0000769518)+

(cid:17) (cid:112)2304π(τ + 5.66103)(τ + 0.0000766694)

(cid:16)

√

√

2880

τ + 32

3τ 3/2 − 40

−3.42607 × 106τ 3/2 − 836198τ 5/2 + 7148.69
√
3
(cid:112)576π(τ + 22.561)(τ + 0.0000769518)+
422935τ 2 + 33874 (cid:54)
− 5.81185106τ 3/2 − 1.67707 × 106τ 5/2 − 3.44998 × 107τ 7/2+
(cid:16)

1.72711 × 106τ 3/2 + 611410τ 5/2 + 2798.31
√
3

(cid:17) (cid:112)2304π1.0001(τ + 5.66103)τ +
(cid:17) (cid:112)2304π1.0001(τ + 5.66103)τ (cid:112)576π1.0001(τ + 22.561)τ +

3τ 3/2 − 40

τ + 32

2880

√

√

(cid:16)

(cid:16)

(cid:16)

−3.42607 × 106τ 3/2 − 836198τ 5/2 + 7148.69

(cid:17)

(cid:112)576π(τ + 22.561)τ +
422935τ 2 + 33874. =
− 5.81185106τ 3/2 − 1.67707 × 106τ 5/2 − 3.44998 × 107τ 7/2+
(cid:16)

−250764.τ 3/2 + 1.8055 × 107τ 5/2 + 115823.τ

(cid:17)

√

√

τ + 22.561 + 422935.τ 2+

τ + 5.66103
(cid:0)5.20199 × 107τ 3 + 1.46946 × 108τ 2 + 238086.
(cid:0)−3.55709 × 107τ 3 − 1.45741 × 108τ 2 + 304097.
√

√

√

(cid:16)

τ (cid:1) √
τ (cid:1) √
√

τ + 5.66103+

τ + 22.561 + 33874. (cid:54)

(cid:17)

1.25 + 22.561

1.25 + 5.66103
1.25 + 5.66103 (cid:0)5.20199 × 107τ 3 + 1.46946 × 108τ 2 + 238086.
√
0.9 + 22.561 (cid:0)−3.55709 × 107τ 3 − 1.45741 × 108τ 2 + 304097.

√

√

√

τ (cid:1) +
τ (cid:1) −

−250764.τ 3/2 + 1.8055 × 107τ 5/2 + 115823.τ

+

5.81185106τ 3/2 − 1.67707 × 106τ 5/2 − 3.44998 × 107τ 7/2 + 422935.τ 2 + 33874. (cid:54)
33874.τ 3/2
0.93/2

− 9.02866 × 106τ 3/2 + 2.29933 × 108τ 5/2 − 3.44998 × 107τ 7/2−
√
1.48578 × 106√
√

3.5539 × 107τ 3 − 3.19193 × 108τ 2 +

2.09884 × 106τ

τ τ

+

=

τ

0.9

0.9

− 5.09079 × 106τ 3/2 + 2.29933 × 108τ 5/2−
3.44998 × 107τ 7/2 − 3.5539 × 107τ 3 − 3.19193 × 108τ 2 (cid:54)

√

− 5.09079 × 106τ 3/2 +

2.29933 × 108
τ
3.5539 × 107τ 3 − 3.19193 × 108τ 2 =
− 5.09079 × 106τ 3/2 − 3.44998 × 107τ 7/2 − 3.5539 × 107τ 3 − 6.21197 × 107τ 2 < 0 .

− 3.44998 × 107τ 7/2−

1.25τ 5/2

√

First we expanded the term (multiplied it out). The we put the terms multiplied by the same square
root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for
τ for some positive terms and value of 0.9 for negative terms. These terms are then expanded
at the =-sign. The next equality factors the terms under the squared root. We decreased the
negative term by setting τ = τ + 0.0000769518 under the root. We increased positive terms by
setting τ + 0.0000769518 = 1.0000962τ and τ + 0.0000766694 = 1.0000962τ under the root
for positive terms. The positive terms are increase, since 0.8+0.0000769518
< 1.0000962, thus
τ + 0.0000766694 < τ + 0.0000769518 (cid:54) 1.0000962τ . For the next inequality we decreased
negative terms by inserting τ = 0.9 and increased positive terms by inserting τ = 1.25. The next

0.8

83

equality expands the terms. We use upper bound of 1.25 and lower bound of 0.9 to obtain terms with
corresponding exponents of τ .

Consequently, the derivative of

(cid:18)
e

τ

(cid:16) µω+ντ
√
ντ
2

√

(cid:17)2

erfc

(cid:19)

(cid:18) µω + ντ
√
ντ
2

√

(cid:16) µω+2ντ
√
ντ
2

√

(cid:17)2

− 2e

erfc

(cid:19)(cid:19)

(cid:18) µω + 2ντ
√
ντ
2

√

(318)

with respect to τ is smaller than zero for maximal ν = 0.24 and the domain 0.9 (cid:54) τ (cid:54) 1.25.

Lemma 47. In the domain −0.01 (cid:54) y (cid:54) 0.01 and 0.64 (cid:54) x (cid:54) 1.875, the function f (x, y) =
e 1
2 (2y+x) erfc
has a global maximum at y = 0.64 and x = −0.01 and a global minimum at
y = 1.875 and x = 0.01.

(cid:16) x+y√

(cid:17)

2x

Proof. f (x, y) = e 1
with respect to x is negative:

2 (2y+x) erfc

(cid:16) x+y√

(cid:17)

2x

is strictly monotonically decreasing in x, since its derivative

(cid:16)√

e− y2

2x

πx3/2e

(x+y)2
2x

(cid:17)

(cid:16) x+y√

√

2

x

erfc
√
2

πx3/2

√

+

2(y − x)

(cid:17)

< 0

⇐⇒

√

πx3/2e

(x+y)2
2x

erfc

(cid:19)

√

+

2(y − x) < 0

√

πx3/2e

(x+y)2
2x

erfc

+

2(y − x) (cid:54)

(cid:18) x + y
√
√
x
2
√

(cid:19)

(cid:18) x + y
√
√
x
2
√

+ y

2 − x

√

2 (cid:54)

2x3/2
(cid:113) (x+y)2

x+y√
√
x
2

+

2x + 4

π

2 · 0.643/2

0.01+0.64
√
√
0.64

2

+

(cid:113) (0.01+0.64)2

2·0.64 + 4

π

√

√

+ 0.01

2 − 0.64

2 = −0.334658 < 0.

(319)

The two last inqualities come from applying Abramowitz bounds 22 and from the fact that the
2 does not change monotonicity in the domain and hence
expression

2 − x

+ y

√

√

2x3/2
(cid:113) (x+y)2

2x + 4

π

√

x+y
√
2

x

+

the maximum must be found at the border. For x = 0.64 that maximizes the function f (x, y) is
monotonically in y, because its derivative w.r.t. y at x = 0.64 is

(cid:16)

ey (cid:16)

1.37713 erfc(0.883883y + 0.565685) − 1.37349e−0.78125(y+0.64)2(cid:17)

< 0
1.37713 erfc(0.883883y + 0.565685) − 1.37349e−0.78125(y+0.64)2(cid:17)

⇐⇒
1.37713 erfc(0.883883y + 0.565685) − 1.37349e−0.78125(y+0.64)2(cid:17)
(cid:16)
1.37713 erfc(0.883883 · −0.01 + 0.565685) − 1.37349e−0.78125(0.01+0.64)2(cid:17)
(cid:16)
0.5935272325870631 − 0.987354705867739 < 0.

(cid:54)

< 0

=

(320)

Therefore, the values y = 0.64 and x = −0.01 give a global maximum of the function f (x, y) in the
domain −0.01 (cid:54) y (cid:54) 0.01 and 0.64 (cid:54) x (cid:54) 1.875 and the values y = 1.875 and x = 0.01 give the
global minimum.

A4 Additional information on experiments

In this section, we report the hyperparameters that were considered for each method and data set and
give details on the processing of the data sets.

84

A4.1

121 UCI Machine Learning Repository data sets: Hyperparameters

For the UCI data sets, the best hyperparameter setting was determined by a grid-search over all
hyperparameter combinations using 15% of the training data as validation set. The early stopping
parameter was determined on the smoothed learning curves of 100 epochs of the validation set.
Smoothing was done using moving averages of 10 consecutive values. We tested “rectangular”
and “conic” layers – rectangular layers have constant number of hidden units in each layer, conic
layers start with the given number of hidden units in the ﬁrst layer and then decrease the number
of hidden units to the size of the output layer according to the geometric progession. If multiple
hyperparameters provided identical performance on the validation set, we preferred settings with a
higher number of layers, lower learning rates and higher dropout rates. All methods had the chance
to adjust their hyperparameters to the data set at hand.

Table A4: Hyperparameters considered for self-normalizing networks in the UCI data sets.

Table A5: Hyperparameters considered for ReLU networks with MS initialization in the UCI data
sets.

Table A6: Hyperparameters considered for batch normalized networks in the UCI data sets.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Dropout rate
Layer form

{1024, 512, 256}
{2, 3, 4, 8, 16, 32}
{0.01, 0.1, 1}
{0.05, 0}
{rectangular, conic}

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Dropout rate
Layer form

{1024, 512, 256}
{2,3,4,8,16,32}
{0.01, 0.1, 1}
{0.5, 0}
{rectangular, conic}

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Layer form

{1024, 512, 256}
{2, 3, 4, 8, 16, 32}
{0.01, 0.1, 1}
{Batchnorm}
{rectangular, conic}

85

Table A7: Hyperparameters considered for weight normalized networks in the UCI data sets.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Layer form

{1024, 512, 256}
{2, 3, 4, 8, 16, 32}
{0.01, 0.1, 1}
{Weightnorm}
{rectangular, conic}

Table A8: Hyperparameters considered for layer normalized networks in the UCI data sets.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Layer form

{1024, 512, 256}
{2, 3, 4, 8, 16, 32}
{0.01, 0.1, 1}
{Layernorm}
{rectangular, conic}

Table A9: Hyperparameters considered for Highway networks in the UCI data sets.

Hyperparameter

Considered values

Number of hidden layers
Learning rate
Dropout rate

{2, 3, 4, 8, 16, 32}
{0.01, 0.1, 1}
{0, 0.5}

Table A10: Hyperparameters considered for Residual networks in the UCI data sets.

Hyperparameter

Considered values

Number of blocks
Number of neurons per blocks
Block form
Bottleneck
Learning rate

{2, 3, 4, 8, 16}
{1024, 512, 256}
{rectangular, diavolo}
{25%, 50%}
{0.01, 0.1, 1}

86

A4.2

121 UCI Machine Learning Repository data sets: detailed results

Methods compared. We used data sets and preprocessing scripts by Fernández-Delgado et al.
[10] for data preparation and deﬁning training and test sets. With several ﬂaws in the method
comparison[37] that we avoided, the authors compared 179 machine learning methods of 17 groups
in their experiments. The method groups were deﬁned by Fernández-Delgado et al. [10] as follows:
Support Vector Machines, RandomForest, Multivariate adaptive regression splines (MARS), Boosting,
Rule-based, logistic and multinomial regression, Discriminant Analysis (DA), Bagging, Nearest
Neighbour, DecisionTree, other Ensembles, Neural Networks, Bayesian, Other Methods, generalized
linear models (GLM), Partial least squares and principal component regression (PLSR), and Stacking.
However, many of methods assigned to those groups were merely different implementations of the
same method. Therefore, we selected one representative of each of the 17 groups for method compar-
ison. The representative method was chosen as the group’s method with the median performance
across all tasks. Finally, we included 17 other machine learning methods of Fernández-Delgado
et al. [10], and 6 FNNs, BatchNorm, WeightNorm, LayerNorm, Highway, Residual and MSRAinit
networks, and self-normalizing neural networks (SNNs) giving a total of 24 compared methods.

Results of FNN methods for all 121 data sets. The results of the compared FNN methods can be
found in Table A11.

Small and large data sets. We assigned each of the 121 UCI data sets into the group “large datasets”
or “small datasets” if the had more than 1,000 data points or less, respectively. We expected that
Deep Learning methods require large data sets to competitive to other machine learning methods.
This resulted in 75 small and 46 large data sets.

Results. The results of the method comparison are given in Tables A12 and A13 for small and large
data sets, respectively. On small data sets, SVMs performed best followed by RandomForest and
SNNs. On large data sets, SNNs are the best method followed by SVMs and Random Forest.

87

Table A11: Comparison of FNN methods on all 121 UCI data sets.. The table reports the accuracy
of FNN methods at each individual task of the 121 UCI data sets. The ﬁrst column gives the name
of the data set, the second the number of training data points N , the third the number of features
M and the consecutive columns the accuracy values of self-normalizing networks (SNNs), ReLU
networks without normalization and with MSRA initialization (MS), Highway networks (HW),
Residual Networks (ResNet), networks with batch normalization (BN), weight normalization (WN),
and layer normalization (LN).

dataset

N M SNN

MS

HW

ResNet BN

WN

LN

0.6284
1.0000
1.0000
0.8487
0.7300
0.6372
0.6800
0.9231
0.5000
0.8876
0.7754
0.6901
0.9714
0.9718
0.7347
0.4615
0.9861
0.8418
0.8964
0.8606
0.9900
0.6055
0.8269
0.9935
0.8831
0.5136
0.8430
0.7656
0.9121
0.8485
0.8333
0.9583
0.8958
0.8800
0.4583
0.6038
0.7237
0.4643
0.6053
0.8356
0.3871
0.2600
0.7692
0.5116
0.8529
0.6644

0.6427
1.0000
1.0000
0.8453
0.3600
0.6283
0.7200
0.9103
0.2500
0.8885
0.7968
0.7465
0.9771
0.9789
0.8367
0.6154
0.9560
0.8456
0.9171
0.5255
0.9900
0.5872
0.8462
0.9784
0.8599
0.5054
0.8547
0.7969
0.9780
0.6061
0.8690
0.8802
0.9010
0.8800
0.4375
0.6415
0.6447
0.7857
0.6316
0.7945
0.5806
0.4000
0.6667
0.5000
0.7794
0.6781

0.6466
1.0000
1.0000
0.8484
0.2600
0.6460
0.8000
0.9167
1.0000
0.8796
0.8021
0.7465
0.9714
0.9507
0.8163
0.4231
0.9282
0.8173
0.9021
0.8543
0.9912
0.5963
0.8077
0.9935
0.8716
0.5136
0.8430
0.7734
0.9231
0.8485
0.8214
0.8177
0.8750
0.8400
0.3750
0.6415
0.6842
0.7143
0.5658
0.8082
0.3226
0.2600
0.7692
0.5396
0.8088
0.6712

0.6303
1.0000
1.0000
0.8499
0.1200
0.5929
0.6400
0.9231
1.0000
0.8823
0.7647
0.7324
0.9829
0.9789
0.7755
0.4615
0.9606
0.7910
0.9096
0.8781
0.9862
0.5872
0.7115
0.9610
0.8729
0.4538
0.8721
0.7500
0.9341
0.8485
0.8214
0.8646
0.8750
0.6800
0.4167
0.5849
0.7368
0.7500
0.5789
0.8493
0.3871
0.2800
0.8718
0.5050
0.8529
0.5959

0.6351
1.0000
1.0000
0.8453
0.6500
0.6018
0.7200
0.9551
0.0000
0.8850
0.7594
0.6197
0.9657
0.9718
0.8367
0.5385
0.9769
0.8606
0.8945
0.7673
0.9912
0.5872
0.8269
0.9524
0.8833
0.4755
0.9070
0.7578
0.9451
0.7879
0.8452
0.9010
0.8906
0.6800
0.4167
0.6792
0.7500
0.5714
0.5658
0.7534
0.2581
0.2200
0.8462
0.4934
0.7059
0.6918

0.6178
0.9000
1.0000
0.8517
0.5000
0.5752
0.8000
0.9872
0.7500
0.8920
0.7112
0.6620
0.9714
0.9648
0.7959
0.5769
0.9907
0.8362
0.9021
0.8938
0.9875
0.5780
0.6731
0.9935
0.8856
0.4592
0.8547
0.7578
0.9451
0.8182
0.8571
0.9479
0.8802
0.8800
0.3542
0.6981
0.6842
0.8929
0.5789
0.8493
0.5161
0.2400
0.7436
0.5050
0.7941
0.6986

abalone
acute-inﬂammation
acute-nephritis
adult
annealing
arrhythmia
audiology-std
balance-scale
balloons
bank
blood
breast-cancer
breast-cancer-wisc
breast-cancer-wisc-diag
breast-cancer-wisc-prog
breast-tissue
car
cardiotocography-10clases
cardiotocography-3clases
chess-krvk
chess-krvkp
congressional-voting
conn-bench-sonar-mines-rocks
conn-bench-vowel-deterding
connect-4
contrac
credit-approval
cylinder-bands
dermatology
echocardiogram
ecoli
energy-y1
energy-y2
fertility
ﬂags
glass
haberman-survival
hayes-roth
heart-cleveland
heart-hungarian
heart-switzerland
heart-va
hepatitis
hill-valley
horse-colic
ilpd-indian-liver

4177
120
120
48842
898
452
196
625
16
4521
748
286
699
569
198
106
1728
2126
2126
28056
3196
435
208
990
67557
1473
690
512
366
131
336
768
768
100
194
214
306
160
303
294
123
200
155
1212
368
583

9
7
7
15
32
263
60
5
5
17
5
10
10
31
34
10
7
22
22
7
37
17
61
12
43
10
16
36
35
11
8
9
9
10
29
10
4
4
14
13
13
13
20
101
26
10

0.6657
1.0000
1.0000
0.8476
0.7600
0.6549
0.8000
0.9231
1.0000
0.8903
0.7701
0.7183
0.9714
0.9789
0.6735
0.7308
0.9838
0.8399
0.9153
0.8805
0.9912
0.6147
0.7885
0.9957
0.8807
0.5190
0.8430
0.7266
0.9231
0.8182
0.8929
0.9583
0.9063
0.9200
0.4583
0.7358
0.7368
0.6786
0.6184
0.7945
0.3548
0.3600
0.7692
0.5248
0.8088
0.6986

88

0.9090
0.9091
0.9189
0.7200
1.0000
0.9712
0.8667
0.8496
0.3750
0.7297
0.8629
0.8083
0.9250
0.7692
0.8482
0.6551
0.6343
0.7454
1.0000
0.8655
0.9945
0.9988
0.8196
0.9490
0.8728
0.9430
0.9666
0.9732
0.9708
0.9184
0.9714
0.7656
0.8462
0.7692
0.5217
0.8800
0.6538
0.6667
0.8125
0.6350
0.7900
0.7273
0.5000
0.9843
0.8654
0.9296
0.8723
0.9461
0.6183
0.6043
0.6802
0.7280

0.9024
0.9432
0.8378
0.7040
1.0000
0.8984
0.8222
0.9023
0.1250
0.7297
0.8673
0.7917
0.9270
0.6923
0.8833
0.5833
0.6389
0.5880
1.0000
0.8992
0.9915
1.0000
0.7176
0.9490
0.8289
0.9342
0.9644
0.9716
0.9656
0.8367
0.9671
0.7188
0.9231
0.6923
0.5652
0.8800
0.5385
0.6000
0.8375
0.6325
0.7900
0.5909
0.4512
0.9692
0.9423
0.9447
0.8617
0.9435
0.6022
0.8930
0.6802
0.7760

0.8919
0.9545
0.9730
0.7160
0.6667
0.9762
0.7111
0.8647
0.2500
0.6757
0.8723
0.7833
0.9254
0.7692
0.8557
0.7546
0.6273
0.5833
1.0000
0.8739
0.9964
0.9994
0.8000
0.9373
0.7719
0.8947
0.9627
0.9669
0.9605
0.9184
0.9708
0.7135
0.9231
0.8462
0.5652
0.8800
0.6538
0.7111
0.7975
0.5150
0.8000
0.7273
0.3902
0.9811
0.8654
0.9146
0.8670
0.9461
0.6667
0.7005
0.6395
0.7720

0.8481
0.9432
0.9189
0.6280
0.8333
0.9796
0.7444
0.8571
0.5000
0.7568
0.8713
0.8167
0.9262
0.7692
0.8519
0.9074
0.3287
0.5278
0.9990
0.8235
0.9982
0.9994
0.8078
0.9333
0.7456
0.8947
0.9716
0.9669
0.9613
0.8571
0.9734
0.7188
0.8846
0.7692
0.5652
0.8800
0.1154
0.6222
0.7600
0.2850
0.8200
0.5909
0.5122
0.9843
0.8654
0.9372
0.8883
0.9426
0.6344
0.2299
0.6802
0.7520

0.8938
0.9318
1.0000
0.6920
0.8333
0.9580
0.8000
0.8872
0.5000
0.7568
0.8690
0.8292
0.9272
0.6923
0.8494
0.5000
0.6644
0.5231
0.9995
0.8992
0.9927
0.9966
0.8078
0.9020
0.7939
0.9254
0.9638
0.9748
0.9730
0.8163
0.9620
0.6979
0.8077
0.6538
0.6522
0.8800
0.4615
0.6444
0.8175
0.6575
0.8175
0.5455
0.5000
0.9719
0.8846
0.9322
0.8537
0.9504
0.6398
0.4545
0.6860
0.7400

0.8838
0.9432
0.9730
0.6480
0.6667
0.9742
0.8333
0.8947
0.2500
0.7838
0.8620
0.8208
0.9313
0.4615
0.8607
0.7014
0.5162
0.6991
0.9995
0.8992
0.9951
0.9966
0.7686
0.9412
0.8202
0.8991
0.9755
0.9716
0.9708
0.8571
0.9657
0.6927
0.9231
0.7308
0.6087
0.8800
0.6538
0.6889
0.8425
0.6775
0.8350
0.7727
0.4512
0.9827
0.8846
0.9447
0.8484
0.9513
0.6720
0.5561
0.6279
0.7400

image-segmentation
ionosphere
iris
led-display
lenses
letter
libras
low-res-spect
lung-cancer
lymphography
magic
mammographic
miniboone
molec-biol-promoter
molec-biol-splice
monks-1
monks-2
monks-3
mushroom
musk-1
musk-2
nursery
oocytes_merluccius_nucleus_4d
oocytes_merluccius_states_2f
oocytes_trisopterus_nucleus_2f
oocytes_trisopterus_states_5b
optical
ozone
page-blocks
parkinsons
pendigits
pima
pittsburg-bridges-MATERIAL
pittsburg-bridges-REL-L
pittsburg-bridges-SPAN
pittsburg-bridges-T-OR-D
pittsburg-bridges-TYPE
planning
plant-margin
plant-shape
plant-texture
post-operative
primary-tumor
ringnorm
seeds
semeion
soybean
spambase
spect
spectf
statlog-australian-credit
statlog-german-credit

2310
351
150
1000
24
20000
360
531
32
148
19020
961
130064
106
3190
556
601
554
8124
476
6598
12960
1022
1022
912
912
5620
2536
5473
195
10992
768
106
103
92
102
105
182
1600
1600
1599
90
330
7400
210
1593
683
4601
265
267
690
1000

19
34
5
8
5
17
91
101
57
19
11
6
51
58
61
7
7
7
22
167
167
9
42
26
26
33
63
73
11
23
17
9
8
8
8
8
8
13
65
65
65
9
18
21
8
257
36
58
23
45
15
25

0.9114
0.8864
0.9730
0.7640
0.6667
0.9726
0.7889
0.8571
0.6250
0.9189
0.8692
0.8250
0.9307
0.8462
0.9009
0.7523
0.5926
0.6042
1.0000
0.8739
0.9891
0.9978
0.8235
0.9529
0.7982
0.9342
0.9711
0.9700
0.9583
0.8980
0.9706
0.7552
0.8846
0.6923
0.6957
0.8400
0.6538
0.6889
0.8125
0.7275
0.8125
0.7273
0.5244
0.9751
0.8846
0.9196
0.8511
0.9409
0.6398
0.4973
0.5988
0.7560

89

statlog-heart
statlog-image
statlog-landsat
statlog-shuttle
statlog-vehicle
steel-plates
synthetic-control
teaching
thyroid
tic-tac-toe
titanic
trains
twonorm
vertebral-column-2clases
vertebral-column-3clases
wall-following
waveform
waveform-noise
wine
wine-quality-red
wine-quality-white
yeast
zoo

270
2310
6435
58000
846
1941
600
151
7200
958
2201
10
7400
310
310
5456
5000
5000
178
1599
4898
1484
101

14
19
37
10
19
28
61
6
22
10
4

0.9254
0.9549
0.9100
0.9990
0.8009
0.7835
0.9867
0.5000
0.9816
0.9665
0.7836

30 NA
21
7
7
25
22
41
14
12
12
9
17

0.9805
0.8312
0.8312
0.9098
0.8480
0.8608
0.9773
0.6300
0.6373
0.6307
0.9200

0.8358
0.9757
0.9075
0.9983
0.8294
0.7567
0.9800
0.6053
0.9770
0.9833
0.7909
NA
0.9778
0.8701
0.8052
0.9076
0.8312
0.8328
0.9318
0.6250
0.6479
0.6173
1.0000

0.7761
0.9584
0.9110
0.9977
0.7962
0.7608
0.9867
0.5263
0.9708
0.9749
0.7927
NA
0.9708
0.8571
0.7922
0.9230
0.8320
0.8696
0.9091
0.5625
0.5564
0.6065
0.8800

0.8657
0.9584
0.9055
0.9992
0.7583
0.7629
0.9600
0.5526
0.9799
0.9623
0.7727
NA
0.9735
0.8312
0.7532
0.9223
0.8360
0.8584
0.9773
0.6150
0.6307
0.5499
1.0000

0.7910
0.9671
0.9040
0.9988
0.7583
0.7031
0.9733
0.5000
0.9778
0.9833
0.7800
0.5000
0.9757
0.8312
0.7792
0.9333
0.8360
0.8480
0.9773
0.5450
0.5335
0.4906
0.7200

0.8657
0.9515
0.8925
0.9988
0.8009
0.7856
0.9867
0.3158
0.9807
0.9707
0.7818
0.5000
0.9730
0.6623
0.7403
0.9274
0.8376
0.8640
0.9773
0.5575
0.5482
0.5876
0.9600

0.7910
0.9757
0.9040
0.9987
0.7915
0.7588
0.9733
0.6316
0.9752
0.9791
0.7891
1.0000
0.9724
0.8442
0.8312
0.9128
0.8448
0.8504
0.9773
0.6100
0.6544
0.6092
0.9600

Table A12: UCI comparison reporting the average rank of a method on 75 classiﬁcation task of the
UCI machine learning repository with less than 1000 data points. For each dataset, the 24 compared
methods, were ranked by their accuracy and the ranks were averaged across the tasks. The ﬁrst
column gives the method group, the second the method, the third the average rank , and the last the
p-value of a paired Wilcoxon test whether the difference to the best performing method is signiﬁcant.
SNNs are ranked third having been outperformed by Random Forests and SVMs.

methodGroup

method

avg. rank

p-value

LibSVM_weka
RRFglobal_caret
SNN
SimpleLogistic_weka
lvq_caret
gcvEarth_caret
MSRAinit
LayerNorm
Highway

SVM
RandomForest
SNN
LMR
NeuralNetworks
MARS
MSRAinit
LayerNorm
Highway
DiscriminantAnalysis mda_R
Boosting
Bagging
ResNet
BatchNorm
Rule-based
WeightNorm
DecisionTree
OtherEnsembles
Nearest Neighbour
OtherMethods
PLSR
Bayesian
GLM
Stacking

LogitBoost_weka
ctreeBag_R
ResNet
BatchNorm
JRip_caret
WeightNorm
rpart2_caret
Dagging_weka
NNge_weka
pam_caret
simpls_R
NaiveBayes_weka
bayesglm_caret
Stacking_weka

9.3
9.6
9.6
9.9
10.1
10.7
11.0
11.3
11.5
11.8
11.9
12.1
12.3
12.6
12.9
13.0
13.6
13.9
14.0
14.2
14.3
14.6
15.0
20.9

2.5e-01
3.8e-01
1.5e-01
1.0e-01
3.6e-02
4.0e-02
7.2e-02
8.9e-03
2.6e-03
2.4e-02
1.8e-03
3.5e-03
4.9e-04
1.7e-04
8.3e-05
7.0e-04
3.0e-05
7.7e-04
1.5e-04
4.6e-05
1.2e-04
1.6e-06
2.2e-12

90

Table A13: UCI comparison reporting the average rank of a method on 46 classiﬁcation task of the
UCI machine learning repository with more than 1000 data points. For each dataset, the 24 compared
methods, were ranked by their accuracy and the ranks were averaged across the tasks. The ﬁrst
column gives the method group, the second the method, the third the average rank , and the last the
p-value of a paired Wilcoxon test whether the difference to the best performing method is signiﬁcant.
SNNs are ranked ﬁrst having outperformed diverse machine learning methods and other FNNs.

methodGroup

method

avg. rank

p-value

SNN
LibSVM_weka
RRFglobal_caret
MSRAinit
LayerNorm
Highway
ResNet
WeightNorm
BatchNorm
gcvEarth_caret
LogitBoost_weka
SimpleLogistic_weka
JRip_caret
ctreeBag_R

SNN
SVM
RandomForest
MSRAinit
LayerNorm
Highway
ResNet
WeightNorm
BatchNorm
MARS
Boosting
LMR
Rule-based
Bagging
DiscriminantAnalysis mda_R
Nearest Neighbour
DecisionTree
OtherEnsembles
NeuralNetworks
Bayesian
OtherMethods
GLM
PLSR
Stacking

NNge_weka
rpart2_caret
Dagging_weka
lvq_caret
NaiveBayes_weka
pam_caret
bayesglm_caret
simpls_R
Stacking_weka

5.8
6.1
6.6
7.1
7.2
7.9
8.4
8.7
9.7
9.9
12.1
12.4
12.4
13.5
13.9
14.1
15.5
16.1
16.3
17.9
18.3
18.7
19.0
22.5

5.8e-01
2.1e-01
4.5e-03
7.1e-02
1.7e-03
1.7e-04
5.5e-04
1.8e-04
8.2e-05
2.2e-07
3.8e-09
9.0e-08
1.6e-05
1.4e-10
1.6e-10
2.3e-08
4.4e-12
1.6e-12
1.6e-12
2.8e-14
1.5e-11
3.4e-11
2.8e-14

91

A4.3 Tox21 challenge data set: Hyperparameters

For the Tox21 data set, the best hyperparameter setting was determined by a grid-search over all
hyperparameter combinations using the validation set deﬁned by the challenge winners [28]. The
hyperparameter space was chosen to be similar to the hyperparameters that were tested by Mayr et al.
[28]. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of
the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested
“rectangular” and “conic” layers – rectangular layers have constant number of hidden units in each
layer, conic layers start with the given number of hidden units in the ﬁrst layer and then decrease
the number of hidden units to the size of the output layer according to the geometric progession. All
methods had the chance to adjust their hyperparameters to the data set at hand.

Table A14: Hyperparameters considered for self-normalizing networks in the Tox21 data set.

Table A15: Hyperparameters considered for ReLU networks with MS initialization in the Tox21 data
set.

Table A16: Hyperparameters considered for batch normalized networks in the Tox21 data set.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Dropout rate
Layer form
L2 regularization parameter

{1024, 2048}
{2,3,4,6,8,16,32}
{0.01, 0.05, 0.1}
{0.05, 0.10}
{rectangular, conic}
{0.001,0.0001,0.00001}

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Dropout rate
Layer form
L2 regularization parameter

{1024, 2048}
{2,3,4,6,8,16,32}
{0.01, 0.05, 0.1}
{0.5, 0}
{rectangular, conic}
{0.001,0.0001,0.00001}

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Layer form
L2 regularization parameter

{1024, 2048}
{2, 3, 4, 6, 8, 16, 32}
{0.01, 0.05, 0.1}
{Batchnorm}
{rectangular, conic}
{0.001,0.0001,0.00001}

92

Table A17: Hyperparameters considered for weight normalized networks in the Tox21 data set.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Dropout rate
Layer form
L2 regularization parameter

{1024, 2048}
{2, 3, 4, 6, 8, 16, 32}
{0.01, 0.05, 0.1}
{Weightnorm}
{0, 0.5}
{rectangular, conic}
{0.001,0.0001,0.00001}

Table A18: Hyperparameters considered for layer normalized networks in the Tox21 data set.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Dropout rate
Layer form
L2 regularization parameter

{1024, 2048}
{2, 3, 4, 6, 8, 16, 32}
{0.01, 0.05, 0.1}
{Layernorm}
{0, 0.5}
{rectangular, conic}
{0.001,0.0001,0.00001}

Table A19: Hyperparameters considered for Highway networks in the Tox21 data set.

Hyperparameter

Considered values

Number of hidden layers
Learning rate
Dropout rate
L2 regularization parameter

{2, 3, 4, 6, 8, 16, 32}
{0.01, 0.05, 0.1}
{0, 0.5}
{0.001,0.0001,0.00001}

Table A20: Hyperparameters considered for Residual networks in the Tox21 data set.

Hyperparameter

Considered values

Number of blocks
Number of neurons per blocks
Block form
Bottleneck
Learning rate
L2 regularization parameter

{2, 3, 4, 6, 8, 16}
{1024, 2048}
{rectangular, diavolo}
{25%, 50%}
{0.01, 0.05, 0.1}
{0.001,0.0001,0.00001}

93

Figure A8: Distribution of network inputs of an SNN for the Tox21 data set. The plots show the
distribution of network inputs z of the second layer of a typical Tox21 network. The red curves
display a kernel density estimator of the network inputs and the black curve is the density of a
standard normal distribution. Left panel: At initialization time before learning. The distribution of
network inputs is close to a standard normal distribution. Right panel: After 40 epochs of learning.
The distributions of network inputs is close to a normal distribution.

Distribution of network inputs. We empirically checked the assumption that the distribution of
network inputs can well be approximated by a normal distribution. To this end, we investigated the
density of the network inputs before and during learning and found that these density are close to
normal distributions (see Figure A8).

94

A4.4 HTRU2 data set: Hyperparameters

For the HTRU2 data set, the best hyperparameter setting was determined by a grid-search over all
hyperparameter combinations using one of the 9 non-testing folds as validation fold in a nested
cross-validation procedure. Concretely, if M was the testing fold, we used M − 1 as validation fold,
and for M = 1 we used fold 10 for validation. The early stopping parameter was determined on the
smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving
averages of 10 consecutive values. We tested “rectangular” and “conic” layers – rectangular layers
have constant number of hidden units in each layer, conic layers start with the given number of hidden
units in the ﬁrst layer and then decrease the number of hidden units to the size of the output layer
according to the geometric progession. All methods had the chance to adjust their hyperparameters
to the data set at hand.

Table A21: Hyperparameters considered for self-normalizing networks on the HTRU2 data set.

Table A22: Hyperparameters considered for ReLU networks with Microsoft initialization on the
HTRU2 data set.

Table A23: Hyperparameters considered for BatchNorm networks on the HTRU2 data set.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Dropout rate
Layer form

{256, 512, 1024}
{2, 4, 8, 16, 32}
{0.1, 0.01, 1}
{ 0, 0.05}
{rectangular, conic}

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Dropout rate
Layer form

{256, 512, 1024}
{2, 4, 8, 16, 32}
{0.1, 0.01, 1}
{0, 0.5}
{rectangular, conic}

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Layer form

{256, 512, 1024}
{2, 4, 8, 16, 32}
{0.1, 0.01, 1}
{Batchnorm}
{rectangular, conic}

95

Table A24: Hyperparameters considered for WeightNorm networks on the HTRU2 data set.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Layer form

{256, 512, 1024}
{2, 4, 8, 16, 32}
{0.1, 0.01, 1}
{Weightnorm}
{rectangular, conic}

Table A25: Hyperparameters considered for LayerNorm networks on the HTRU2 data set.

Hyperparameter

Considered values

Number of hidden units
Number of hidden layers
Learning rate
Normalization
Layer form

{256, 512, 1024}
{2, 4, 8, 16, 32}
{0.1, 0.01, 1}
{Layernorm}
{rectangular, conic}

Table A26: Hyperparameters considered for Highway networks on the HTRU2 data set.

Hyperparameter

Considered values

Number of hidden layers
Learning rate
Dropout rate

{2, 4, 8, 16, 32}
{0.1, 0.01, 1}
{0, 0.5}

Table A27: Hyperparameters considered for Residual networks on the HTRU2 data set.

Hyperparameter

Considered values

Number of hidden units
Number of residual blocks
Learning rate
Block form
Bottleneck

{256, 512, 1024}
{2, 3, 4, 8, 16}
{0.1, 0.01, 1}
{rectangular, diavolo}
{0.25, 0.5}

96

A5 Other ﬁxed points

A similar analysis with corresponding function domains can be performed for other ﬁxed points, for
example for µ = ˜µ = 0 and ν = ˜ν = 2, which leads to a SELU activation function with parameters
α02 = 1.97126 and λ02 = 1.06071.

A6 Bounds determined by numerical methods

In this section we report bounds on previously discussed expressions as determined by numerical
methods (min and max have been computed).

0(µ=0.06,ω=0,ν=1.35,τ =1.12) <

< .00182415(µ=−0.1,ω=0.1,ν=1.47845,τ =0.883374)

(321)

0.905413(µ=0.1,ω=−0.1,ν=1.5,τ =1.25) <

< 1.04143(µ=0.1,ω=0.1,ν=0.8,τ =0.8)

−0.0151177(µ=−0.1,ω=0.1,ν=0.8,τ =1.25) <

< 0.0151177(µ=0.1,ω=−0.1,ν=0.8,τ =1.25)

−0.015194(µ=−0.1,ω=0.1,ν=0.8,τ =1.25) <

< 0.015194(µ=0.1,ω=−0.1,ν=0.8,τ =1.25)

−0.0151177(µ=−0.1,ω=0.1,ν=0.8,τ =1.25) <

< 0.0151177(µ=0.1,ω=−0.1,ν=0.8,τ =1.25)

−0.0151177(µ=0.1,ω=−0.1,ν=0.8,τ =1.25) <

< 0.0151177(µ=0.1,ω=−0.1,ν=0.8,τ =1.25)

−0.00785613(µ=0.1,ω=−0.1,ν=1.5,τ =1.25) <

< 0.0315805(µ=0.1,ω=0.1,ν=0.8,τ =0.8)

0.0799824(µ=0.1,ω=−0.1,ν=1.5,τ =1.25) <

< 0.110267(µ=−0.1,ω=0.1,ν=0.8,τ =0.8)

0(µ=0.06,ω=0,ν=1.35,τ =1.12) <

< 0.0174802(µ=0.1,ω=0.1,ν=0.8,τ =0.8)

0.0849308(µ=0.1,ω=−0.1,ν=0.8,τ =0.8) <

< 0.695766(µ=0.1,ω=0.1,ν=1.5,τ =1.25)

−0.0600823(µ=0.1,ω=−0.1,ν=0.8,τ =1.25) <

< 0.0600823(µ=−0.1,ω=0.1,ν=0.8,τ =1.25)

−0.0673083(µ=0.1,ω=−0.1,ν=1.5,τ =0.8) <

< 0.0673083(µ=−0.1,ω=0.1,ν=1.5,τ =0.8)

−0.0600823(µ=0.1,ω=−0.1,ν=0.8,τ =1.25) <

< 0.0600823(µ=−0.1,ω=0.1,ν=0.8,τ =1.25)

−0.0600823(µ=0.1,ω=−0.1,ν=0.8,τ =1.25) <

< 0.0600823(µ=−0.1,ω=0.1,ν=0.8,τ =1.25)

−0.276862(µ=−0.01,ω=−0.01,ν=0.8,τ =1.25) <

< −0.084813(µ=−0.1,ω=0.1,ν=1.5,τ =0.8)

0.562302(µ=0.1,ω=−0.1,ν=1.5,τ =1.25) <

< 0.664051(µ=0.1,ω=0.1,ν=0.8,τ =0.8)

< 0.00182415(0.0031049101995398316)

(322)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J11
∂µ
∂J11
∂ω
∂J11
∂ν

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 1.04143(1.055872374194189)

< 0.0151177(0.031242911235461816)

∂J11
∂µ

∂J11
∂ω
∂J11
∂ν
∂J11
∂τ
∂J12
∂µ
∂J12
∂ω
∂J12
∂ν
∂J12
∂τ
∂J21
∂µ
∂J21
∂ω
∂J21
∂ν
∂J21
∂τ
∂J22
∂µ
∂J22
∂ω
∂J22
∂ν
∂J22
∂τ

97

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂J11
∂τ
∂J12
∂µ
∂J12
∂ω
∂J12
∂ν
∂J12
∂τ
∂J21
∂µ
∂J21
∂ω
∂J21
∂ν
∂J21
∂τ
∂J22
∂µ
∂J22
∂ω
∂J22
∂ν
∂J22
∂τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 0.015194(0.03749149348255419)

< 0.0151177(0.031242911235461816)

< 0.0151177(0.031242911235461816)

< 0.0315805(0.21232788238624354)

< 0.110267(0.2124377655377270)

< 0.0174802(0.02220441024325437)

< 0.695766(1.146955401845684)

< 0.0600823(0.14983446469110305)

< 0.0673083(0.17980135762932363)

< 0.0600823(0.14983446469110305)

< 0.0600823(0.14983446469110305)

< 0.562302(1.805740052651535)

< 0.664051(2.396685907216327)

A7 References

[1] Abramowitz, M. and Stegun, I. (1964). Handbook of Mathematical Functions, volume 55 of

Applied Mathematics Series. National Bureau of Standards, 10th edition.

[2] Ba, J. L., Kiros, J. R., and Hinton, G. (2016). Layer normalization.

arXiv preprint

arXiv:1607.06450.

[3] Bengio, Y. (2013). Deep learning of representations: Looking forward. In Proceedings of the
First International Conference on Statistical Language and Speech Processing, pages 1–37, Berlin,
Heidelberg.

[4] Blinn, J. (1996). Consider the lowly 2×2 matrix. IEEE Computer Graphics and Applications,

[5] Bradley, R. C. (1981). Central limit theorems under weak dependence. Journal of Multivariate

pages 82–88.

Analysis, 11(1):1–16.

[6] Cire¸san, D. and Meier, U. (2015). Multi-column deep neural networks for ofﬂine handwritten
chinese character classiﬁcation. In 2015 International Joint Conference on Neural Networks
(IJCNN), pages 1–6. IEEE.

[7] Clevert, D.-A., Unterthiner, T., and Hochreiter, S. (2015). Fast and accurate deep network learning
by exponential linear units (ELUs). 5th International Conference on Learning Representations,
arXiv:1511.07289.

98

[8] Dugan, P., Clark, C., LeCun, Y., and Van Parijs, S. (2016). Phase 4: Dcl system using deep
learning approaches for land-based or ship-based real-time recognition and localization of marine
mammals-distributed processing and big data applications. arXiv preprint arXiv:1605.00982.

[9] Esteva, A., Kuprel, B., Novoa, R., Ko, J., Swetter, S., Blau, H., and Thrun, S. (2017).
Nature,

Dermatologist-level classiﬁcation of skin cancer with deep neural networks.
542(7639):115–118.

[10] Fernández-Delgado, M., Cernadas, E., Barro, S., and Amorim, D. (2014). Do we need hundreds
of classiﬁers to solve real world classiﬁcation problems. Journal of Machine Learning Research,
15(1):3133–3181.

[11] Goldberg, D. (1991). What every computer scientist should know about ﬂoating-point arithmetic.

ACM Comput. Surv., 223(1):5–48.

[12] Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent
neural networks. In IEEE International conference on acoustics, speech and signal processing
(ICASSP), pages 6645–6649.

[13] Graves, A. and Schmidhuber, J. (2009). Ofﬂine handwriting recognition with multidimensional
recurrent neural networks. In Advances in neural information processing systems, pages 545–552.

[14] Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan,
S., Widner, K., Madams, T., Cuadros, J., et al. (2016). Development and validation of a deep
learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA,
316(22):2402–2410.

[15] Harrison, J. (1999). A machine-checked theory of ﬂoating point arithmetic. In Bertot, Y.,
Dowek, G., Hirschowitz, A., Paulin, C., and Théry, L., editors, Theorem Proving in Higher Order
Logics: 12th International Conference, TPHOLs’99, volume 1690 of Lecture Notes in Computer
Science, pages 113–130. Springer-Verlag.

[16] He, K., Zhang, X., Ren, S., and Sun, J. (2015a). Deep residual learning for image recognition.

In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[17] He, K., Zhang, X., Ren, S., and Sun, J. (2015b). Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pages 1026–1034.

[18] Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation,

9(8):1735–1780.

[19] Huval, B., Wang, T., Tandon, S., et al. (2015). An empirical evaluation of deep learning on

highway driving. arXiv preprint arXiv:1504.01716.

[20] Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In Proceedings of The 32nd International Conference on
Machine Learning, pages 448–456.

[21] Kahan, W. (2004). A logarithm too clever by half. Technical report, University of California,

Berkeley.

[22] Korolev, V. and Shevtsova, I. (2012). An improvement of the Berry–Esseen inequality with
applications to Poisson and mixed Poisson random sums. Scandinavian Actuarial Journal,
2012(2):81–105.

[23] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105.

[24] LeCun, Y. and Bengio, Y. (1995). Convolutional networks for images, speech, and time series.

The handbook of brain theory and neural networks, 3361(10):1995.

[25] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521(7553):436–444.

99

[26] Loosemore, S., Stallman, R. M., McGrath, R., Oram, A., and Drepper, U. (2016). The GNU C
Library: Application Fundamentals. GNU Press, Free Software Foundation, 51 Franklin St, Fifth
Floor, Boston, MA 02110-1301, USA, 2.24 edition.

[27] Lyon, R., Stappers, B., Cooper, S., Brooke, J., and Knowles, J. (2016). Fifty years of pulsar
candidate selection: From simple ﬁlters to a new principled real-time classiﬁcation approach.
Monthly Notices of the Royal Astronomical Society, 459(1):1104–1123.

[28] Mayr, A., Klambauer, G., Unterthiner, T., and Hochreiter, S. (2016). DeepTox: Toxicity

prediction using deep learning. Frontiers in Environmental Science, 3:80.

[29] Muller, J.-M. (2005). On the deﬁnition of ulp(x). Technical Report Research report RR2005-09,

Laboratoire de l’Informatique du Parallélisme.

[30] Ren, C. and MacKenzie, A. R. (2007). Closed-form approximations to the error and comple-
mentary error functions and their applications in atmospheric science. Atmos. Sci. Let., pages
70–73.

[31] Sak, H., Senior, A., Rao, K., and Beaufays, F. (2015). Fast and accurate recurrent neural network

acoustic models for speech recognition. arXiv preprint arXiv:1507.06947.

[32] Salimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization
to accelerate training of deep neural networks. In Advances in Neural Information Processing
Systems, pages 901–909.

[33] Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks,

61:85–117.

[34] Silver, D., Huang, A., Maddison, C., et al. (2016). Mastering the game of Go with deep neural

networks and tree search. Nature, 529(7587):484–489.

[35] Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Training very deep networks. In

Advances in Neural Information Processing Systems, pages 2377–2385.

[36] Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural

networks. In Advances in Neural Information Processing Systems, pages 3104–3112.

[37] Wainberg, M., Alipanahi, B., and Frey, B. J. (2016). Are random forests truly the best classiﬁers?

Journal of Machine Learning Research, 17(110):1–5.

List of Figures

FNN and SNN trainin error curves . . . . . . . . . . . . . . . . . . . . . . . . . .

Visualization of the mapping g . . . .

. . . . . . .

. . .

. . . . . . . . . . . . . .

A3 Graph of the main subfunction of the derivative of the second moment

. . . . . . .

A4 Graph of the Abramowitz bound for the complementary error function. . . . . . . .
erfc(x) and xex2
A5 Graphs of the functions ex2
A6 The graph of function ˜µ for low variances . .
A7 Graph of the function h(x) = ˜µ2(0.1, −0.1, x, 1, λ01, α01) . . . .
A8 Distribution of network inputs in Tox21 SNNs.

. . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . .

. . . . . . . .

. . . . . . .

erfc(x).

. . . . .

. . .

. . .

. .

List of Tables

Comparison of seven FNNs on 121 UCI tasks . . . . . . . . . . . . . . . . . . . .

Comparison of FNNs at the Tox21 challenge dataset . . . . . . . . . . . . . . . . .

100

1

2

1

2

3

5

30

37

38

56

57

94

8

8

3

Comparison of FNNs and reference methods at HTRU2 . . . . . . . . . . . . . . .

A4 Hyperparameters considered for self-normalizing networks in the UCI data sets. . .

A5 Hyperparameters considered for ReLU networks in the UCI data sets.

. . . . . . .

A6 Hyperparameters considered for batch normalized networks in the UCI data sets.

.

A7 Hyperparameters considered for weight normalized networks in the UCI data sets. .

A8 Hyperparameters considered for layer normalized networks in the UCI data sets. . .

A9 Hyperparameters considered for Highway networks in the UCI data sets. . . . . . .

A10 Hyperparameters considered for Residual networks in the UCI data sets.

. . . . . .

A11 Comparison of FNN methods on all 121 UCI data sets.

. . . . . . . . . . . . . . .

A12 Method comparison on small UCI data sets

. . . . . . . . . . . . . . . . . . . . .

A13 Method comparison on large UCI data sets . . . . . . . . . . . . . . . . . . . . . . . 91

A14 Hyperparameters considered for self-normalizing networks in the Tox21 data set.

.

A15 Hyperparameters considered for ReLU networks in the Tox21 data set. . . . . . . .

A16 Hyperparameters considered for batch normalized networks in the Tox21 data set. .

A17 Hyperparameters considered for weight normalized networks in the Tox21 data set.

A18 Hyperparameters considered for layer normalized networks in the Tox21 data set.

.

A19 Hyperparameters considered for Highway networks in the Tox21 data set.

. . . . .

A20 Hyperparameters considered for Residual networks in the Tox21 data set.

. . . . .

A21 Hyperparameters considered for self-normalizing networks on the HTRU2 data set.

A22 Hyperparameters considered for ReLU networks on the HTRU2 data set. . . . . . .

A23 Hyperparameters considered for BatchNorm networks on the HTRU2 data set. . . .

A24 Hyperparameters considered for WeightNorm networks on the HTRU2 data set.

. .

A25 Hyperparameters considered for LayerNorm networks on the HTRU2 data set. . . .

A26 Hyperparameters considered for Highway networks on the HTRU2 data set.

. . . .

A27 Hyperparameters considered for Residual networks on the HTRU2 data set.

. . . .

9

85

85

85

86

86

86

86

88

90

92

92

92

93

93

93

93

95

95

95

96

96

96

96

101

Brief index

Abramowitz bounds, 37

Banach Fixed Point Theorem, 13
bounds

derivatives of Jacobian entries, 21
Jacobian entries, 23
mean and variance, 24
singular value, 25, 27

central limit theorem, 6
complementary error function

bounds, 37
deﬁnition, 37

computer-assisted proof, 33
contracting variance, 29

deﬁnition, 11
mapping in domain, 29

self-normalizing neural networks, 2
SELU

deﬁnition, 3
parameters, 4, 11

Theorem 1, 5, 12
proof, 13
proof sketch, 5

Theorem 2, 6, 12
proof, 14
Theorem 3, 6, 12
proof, 18

deﬁnitions, 2
domain

singular value, 19
Theorem 1, 12
Theorem 2, 12
Theorem 3, 13

dropout, 6

erf, 37
erfc, 37
error function

bounds, 37
deﬁnition, 37
properties, 39
expanding variance, 32
experiments, 7, 85
astronomy, 8
HTRU2, 8, 95

hyperparameters, 95

methods compared, 7
Tox21, 7, 92

hyperparameters, 8, 92

UCI, 7, 85

details, 85
hyperparameters, 85
results, 86

initialization, 6

Jacobian, 20

bounds, 23
deﬁnition, 20
derivatives, 21
entries, 20, 23
singular value, 21
singular value bound, 25

lemmata, 19

Jacobian bound, 19

mapping g, 2, 4

102

