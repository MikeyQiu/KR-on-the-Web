Multi-Cue Correlation Filters for Robust Visual Tracking

Ning Wang1, Wengang Zhou1, Qi Tian2, Richang Hong3, Meng Wang3, Houqiang Li1
1CAS Key Laboratory of GIPAS, University of Science and Technology of China
2University of Texas at San Antonio 3HeFei University of Technology
wn6149@mail.ustc.edu.cn, {zhwg,lihq}@ustc.edu.cn, qi.tian@utsa.edu, {hongrc,wangmeng}@hfut.edu.cn

Abstract

In recent years, many tracking algorithms achieve im-
pressive performance via fusing multiple types of features,
however, most of them fail to fully explore the context among
the adopted multiple features and the strength of them. In
this paper, we propose an efﬁcient multi-cue analysis frame-
work for robust visual tracking. By combining differen-
t types of features, our approach constructs multiple ex-
perts through Discriminative Correlation Filter (DCF) and
each of them tracks the target independently. With the pro-
posed robustness evaluation strategy, the suitable expert is
selected for tracking in each frame. Furthermore, the di-
vergence of multiple experts reveals the reliability of the
current tracking, which is quantiﬁed to update the experts
adaptively to keep them from corruption.

Through the proposed multi-cue analysis, our tracker
with standard DCF and deep features achieves outstand-
ing results on several challenging benchmarks: OTB-2013,
OTB-2015, Temple-Color and VOT 2016. On the other
hand, when evaluated with only simple hand-crafted fea-
tures, our method demonstrates comparable performance
amongst complex non-realtime trackers, but exhibits much
better efﬁciency, with a speed of 45 FPS on a CPU.

1. Introduction

Visual tracking is a fundamental task in computer vision
with a wide range of applications. It is challenging since
only the initial state of the target is available. Although
substantial progress has been made in the past decades
[52, 16, 35], there still remain many challenges [49].

In recent years, Discriminative Correlation Filter (DCF)
based tracking methods [4, 16] have gained much atten-
tion thanks to their impressive performance as well as high
speed. In DCF based trackers, the ﬁlter is trained through
minimizing a least-squares loss for all circular shifts of a
training sample. Since the correlation operation can be cal-
culated in Fourier domain, DCF shows the advantage of
Its performance is further
high computational efﬁciency.

(a) Biker

(b) Box

(c) Diving

(d) Shaking

(e) Singer2

(f) MotorRolling

MCCT

C-COT

DeepSRDCF

SRDCF

Figure 1. Comparison of the proposed algorithm (MCCT) with
the state-of-the-art trackers: SRDCF [12], DeepSRDCF [11] and
C-COT [13]. These trackers adopt various types of features and
perform differently in challenging scenes. Our method maintain-
s multiple cues for tracking and performs favorably against these
trackers. All the videos are from OTB-2015 [48].

enhanced by using multi-dimensional features [16, 26, 31],
part-based strategies [29, 28], adaptive scale [9, 26], long-
term framework [32, 17, 45], end-to-end learning [41, 40]
and improved ﬁlter training methods [20, 30, 33, 14]. Spe-
cially, the combination of DCF and features from deep con-
volutional neural networks (CNNs) [23, 39] has demonstrat-
ed state-of-the-art results. Recently, Ma et al. [31] propose
a HCF algorithm, which constructs multiple DCFs on low,
middle and high-level features to capture both spatial detail-
s and semantics. It predicts the target location using multi-
level DCF response maps in a coarse-to-ﬁne fashion.

Although feature-level fusion methods [26, 31, 38] have
been widely used or extended to boost the performance,
there still leaves room for improvement.
In HCF [31] or
other methods [56, 38] that follow such a fusion strategy,
the initial weight of high-level features is usually high such
that semantic features play the dominant role in general. It
is reasonable because of better effectiveness of high-level
features compared to shallow features. However, due to the
occasional misguidance of the semantic information, a tran-
sient drift or wrong prediction may be ampliﬁed by the in-
adequate online update. Therefore, the feature-level fusion

4844

approach sometimes still fails to fully explore the relation-
ship of multiple features. Furthermore, it is quite difﬁcult to
handle various challenging variations using a single model,
and relying on a certain feature-level fusion strategy limits
the model diversity to some extent.

To better illustrate the issues mentioned above, our pro-
posed algorithm is compared with three DCF based method-
s. In Figure 1, SRDCF [12], DeepSRDCF [11] and C-COT
[13] adopt different types of features, but none of them is
able to handle various challenging factors, even for the C-
COT algorithm with multiple features as well as the nov-
el continuous convolution operators. Since it is quite difﬁ-
cult to design a satisfying feature-level fusion method that
suits various challenging scenes, it is intuitive to design an
adaptive switch mechanism to achieve better performance,
which can ﬂexibly switch to the reliable tracker depend-
ing on what kind of challenging factors it is expert at han-
dling. In other words, the performance of a single tracker
can sometimes be unstable but the decision-level fusion of
the outputs from multiple trackers can enhance the robust-
ness effectively.

In this paper, a novel Multi-Cue Correlation ﬁlter based
Tracker (MCCT) is proposed. Different from the previous
DCF based methods [4, 16] that usually lack the diversity
of target appearance representations, our method maintains
multiple experts to learn the appearance models from dif-
ferent views. Here, a certain combination of features con-
structs an individual expert and provides a reliable cue (pre-
dicted target state) for tracking. The main contributions
(1) We pro-
of our work can be summarized as follows.
pose an algorithm that maintains multiple cues for track-
ing. Through checking the robustness scores of multiple
experts carefully, our method reﬁnes the tracking results
by choosing the reliable expert for tracking in each frame.
(2) By considering the divergence of multiple experts, we
present an adaptive update strategy which can discriminate
the unreliable samples (e.g., occlusion or sever deforma-
tion) effectively and alleviate the contamination of train-
(3) We implement two versions of the pro-
ing samples.
posed method to validate the generality of the framework.
The MCCT tracker with deep features demonstrates out-
standing performance on several challenging benchmarks
[47, 48, 27, 21]. The MCCT-H tracker with only two s-
tandard Hand-crafted features (HOG [7] and ColorNames
[46]) achieves comparable performance with many complex
deep-model based trackers, but operates about 45 frames
per second on a single CPU, exceeding most competitive
trackers by several times.

2. Related Work

In this section, we discuss two categories of trackers
closely related to our algorithm: correlation tracking and
ensemble tracking.

Correlation Tracking. Since Bolme et al.

[4] pro-
pose a tracker using minimum output sum of squared er-
ror (MOSSE) ﬁlter, the correlation ﬁlters have been widely
studied in visual tracking. Heriques et al. exploit the cir-
culant structure of the training patches [15] and propose to
train correlation ﬁlter in a kernel space with HOG features
[16]. Zhang et al. [54] incorporate context information in-
to ﬁlter learning. The SRDCF tracker [12] alleviates the
boundary effects by penalizing correlation ﬁlter coefﬁcients
depending on spatial location, and is enhanced by reduc-
ing the inﬂuence of corrupted samples [10]. Qi et al. [38]
propose a HDT algorithm that fuses several DCFs through
adaptive hedged method. Luca et al.
[2] propose a Sta-
ple tracker which combines DCF and color histogram based
model while running in excess of real-time. The CSR-DCF
algorithm [30] constructs DCF with channel and spatial reli-
ability. The recent C-COT [13] adopts a continuous-domain
formulation of the DCF, leading to the top performance on
several tracking benchmarks. The enhanced version of C-
COT is ECO [8], which improves both speed and perfor-
mance by introducing several efﬁcient strategies.

Different from the DCF based methods mentioned
above, our algorithm considers not only feature-level fusion
but also decision-level fusion to better explore the relation-
ship of multiple features, and adaptively selects the expert
that is suitable for a particular tracking task.

Ensemble Tracking. To enhance the tracking robust-
ness and obtain reliable results, the ensemble approach
treats the trackers as black boxes and takes only the bound-
ing boxes returned by them as input [43]. In [1], a dynam-
ic programming based trajectory optimization approach is
proposed to build a strong tracker. In [44], Wang et al. pro-
pose a factorial hidden Markov model for ensemble-based
tracking. The MEEM algorithm [51] exploits the relation-
ship between the current tracker and its historical snapshots
using entropy minimization, and Li et al.
[25] extend it
by introducing a discrete graph optimization framework. In
[19], a partition fusion framework is proposed to cluster re-
liable trackers for target state prediction.

Although promising results are obtained through fusing
multiple trackers, there still leave some limitations: (1) the
overall speed is limited by the lowest tracker in the en-
semble (e.g., sparse representation based methods [57, 18],
about 1 FPS), which restricts the real-time application. Spe-
cially, the fusion methods [19, 24] by analyzing the for-
ward and backward trajectories require each tracker to run
at least twice; (2) the trackers are just regarded as inde-
pendent black boxes and their fusion result in each frame
does not feedback to the trackers [1, 44, 19], which fails to
make full use of the reliable fusion outputs; (3) if the fu-
sion tracker number increases, the dynamic programming
based fusion methods [25, 1] still bring obvious computa-
tional burden (e.g., O(T N 2) for T frames and N trackers).

4845

Flowchart

Input 
Frame

ROI

Feature  Pool

Expert  Pool  (DCF)

Expert  Selection

Feature 
Extration

Multi-Expert
Prediction

...

Adaptive
Model
Update

Figure 2. A systematic ﬂowchart of the proposed tracking algorithm. First, different features of ROI are extracted and combined to train
multiple experts following DCF framework (Sec. 3.1 and 3.2). Then, each expert gives an individually predicted cue and the most reliable
expert is selected for the current tracking (Sec. 3.3). Finally, adaptive update helps keep the experts from corruption (Sec. 3.4).

Different from the above methods:

(1) Our approach
the experts in the DCF framework, and
constructs all
through the proposed ROI and training sample sharing s-
trategy (Sec. 3.3), the efﬁciency is greatly ensured; (2) the
reﬁned tracking results are fed back to the experts to boost
them further; (3) through a simple but effective robustness
evaluation strategy, our method selects the reliable expert
for tracking with a complexity of only O(T N ) for T frames
and N experts.

3. Method

Our algorithm is composed of several experts which pro-
vide different levels of cues for tracking. A preview of DCF
is introduced in Sec. 3.1. The component of the expert pool
is described in Sec. 3.2. How to switch to the suitable ex-
pert in each frame is elaborated in Sec. 3.3. Finally, Sec.
3.4 introduces the adaptive update. The framework of our
method is depicted in Figure 2.

3.1. Preview of DCF

A typical tracker based on DCF [4, 16] is trained using
an image patch x of size M × N , which is centered around
the target. All the circular shifts of the patch x(m, n) ∈
{0, 1, ...M − 1} × {0, 1, ...N − 1} are generated as training
samples with Gaussian function label y(m, n) in terms of
the shifted distance. The ﬁlter w is trained by minimizing
the following regression error:

min
w

kXw − yk2

2 + λkwk2
2,

where λ is a regularization parameter (λ ≥ 0) and X is the
data matrix by concatenating all the circular shifts. The ﬁl-
ter solution on the d-th (d ∈ {1, · · · , D}) channel is deﬁned
by

ˆw∗

d =

ˆy ⊙ ˆx∗
d
i ⊙ ˆxi + λ

,

D

i=1 ˆx∗
where ⊙ is the element-wise product, the hat symbol de-
notes the Discrete Fourier Transform (DFT) of a vector
(e.g., ˆx = F(x)) and ˆx∗ is the complex-conjugate of ˆx.
In the next frame, a Region of Interest (ROI) is cropped out
for tracking the target (e.g., a patch z with the same size of

P

(1)

(2)

x). The response map R of z is calculated in Eq. (3) and
the location of the target is identiﬁed by searching for the
maximum value of R.

R = F

−1(

ˆwd ⊙ ˆz∗

d).

(3)

D

d=1
X

To avoid the boundary effects during learning, we apply
Hann window to the signals [16]. Besides, inspired by [30,
5], color information is applied to the training sample in a
= X ⊙ C,
simple way to enhance its spatial reliability: X
where X represents the data matrix and C is the color mask
obtained by computing the histogram-based per-pixel score
map [37, 2] of ROI. The online update of the numerator ˆAd
and the denominator ˆBd of the ﬁlter ˆw∗

d is as follows,

′

ˆAt

d = (1 − η) ˆAt−1

d + η ˆy ⊙ ˆx∗t
d ,

ˆBt

d = (1 − η) ˆBt−1

d + η

ˆx∗t
i ⊙ ˆxt
i,

(4)

D

i=1
X

ˆw∗t

d =

ˆAt
d
d + λ

,

ˆBt

where η is the learning rate and t is the index of the current
frame. As for the target scale estimation, we follow the
DSST tracker [9].

3.2. Feature Pool and Expert Pool

A variety of features can be adopted in DCF and differ-
ent features have their own strength. Hand-crafted features
are typically used to capture low-level details while deep
features are semantic-aware. As discussed in HCF [31], the
DCF constructed by each single layer of VGG-19 [39] is
not accurate enough, so HCF performs coarse-to-ﬁne search
on several DCF response maps from different layers.
In
our MCCT tracker, HOG [7] is used as low-level features.
Then, we remove the fully-connected layers and extract the
outputs of the conv4-4 and conv5-4 convolutional layers of
VGG-19 as middle-level and high-level features, respective-
ly. Thus, the feature pool consists of three types of features:
{Low, M iddle, High} and they are optionally combined
into C 1
3 = 7 experts. As for the coarse-to-ﬁne

3 + C 3

3 + C 2

4846

Table 1. The component of the expert ensemble. Our MCCT
and MCCT-H trackers both consist of seven experts. Each expert
adopts different features and tracks the target via a different view.

Expert Pool
Expert I
Expert II
Expert III
Expert IV
Expert V
Expert VI
Expert VII

Tracker MCCT
Feature Type
Low (HOG)
Middle (conv4-4 of VGG-19)
High (conv5-4 of VGG-19)
Middle, Low
High, Low
High, Middle
High, Middle, Low

Tracker MCCT-H
Feature Type
HOG1
HOG2
ColorNames
HOG1, ColorNames
HOG2, ColorNames
HOG1, HOG2
HOG1, HOG2, ColorNames

Expert I

Expert II

Expert III

Expert IV

…

…

…

…

…

…

…

…

weighting parameters of different level DCF response maps,
we follow the settings in HCF [31]. Although some experts
(e.g., Expert I, II and III) with single type of features may be
less robust compared to Expert VII, they provide the diver-
sity of tracking results, which is crucial in ensemble-based
tracking [43].

Our fast variant, the MCCT-H tracker only utilizes stan-
dard hand-crafted features (HOG [7] and ColorNames [46])
to construct experts. Since HOG and ColorNames are both
low-level features, we do not conduct coarse-to-ﬁne fusion
and just simply concatenate them to construct DCF. Col-
orNames provides an 11 dimensional color representation
and HOG feature is 31-dim. To obtain more experts, we
take the average grey value over all pixels in a patch as a
1-dim feature and concatenate it to the HOG feature into a
32-dim vector, which is further evenly decomposed into two
16-dim features, denoted as HOG1 and HOG2, respective-
ly. The detailed information of the experts in MCCT and
MCCT-H is shown in Table 1.

3.3. Multi Cue Correlation Tracker

The tracking process of our proposed framework can be
illustrated by Figure 3, where the multiple experts track the
target in parallel and the nodes denote the generated cues
(bounding boxes) of the experts. In each frame, the evalua-
tion between different hypothesis nodes reveals the consis-
tency degree between the experts, which is denoted as pair-
wise evaluation. Besides, each expert has its own trajec-
tory continuity and smoothness. Thus, given a hypothesis
node, its robustness degree can be evaluated by both pair-
evaluation and self-evaluation. After evaluating the overall
reliability of each node, the expert with the highest robust-
ness score is selected and its tracking result is taken for the
current frame.

In the following, we elaborate the formulation of pair-

evaluation, self-evaluation and expert selection strategy.

Expert Pair-Evaluation. Most experts in the ensemble
are capable of tracking the target stably, and the cues pro-
duced by a good expert should be consistent with the cues
from other experts as much as possible.

Let E1, · · · , E7 denote Expert I, · · · , Expert VII, respec-
tively. In the t-th frame, the bounding box of Expert i is

Frame t

Frame t+1

Frame t+2

Expert Pair-Evaluation

Expert Self-Evaluation

Figure 3. Graph illustration of the multi-expert framework. The
node in the graph denotes the predicted bounding box. The ro-
bustness score of each expert is evaluated by both pair-evaluation
and self-evaluation. For clarity, only four experts are displayed.

denoted as Bt
. Through regarding all the experts as black
Ei
boxes, the bounding box Bt
only contains the target state
Ei
(e.g., location and scale) without any context information,
which reduces the computational and memory burden ef-
fectively.

First, we compute overlap ratios of the bounding boxes
(Ei,Ej ) of Expert

from different experts. The overlap ratio Ot
i and Expert j at frame t is calculated as follows,

Ot

(Ei,Ej ) =

Area(Bt
Ei
Area(Bt
Ei

Bt
Bt

Ej )
Ej )

.

T

(5)

To reduce the gap between low and high overlap ratios, we
adopt a nonlinear gaussian function to Ot
(Ei,Ej ) as follows,
which gathers the expert scores.

S

O

′t
(Ei,Ej ) = exp

−

1 − Ot

(Ei,Ej )

2

.

(6)

(cid:17)

(cid:16)

K
j=1 O

The mean value M t

(cid:18)
(cid:19)
′t
Ei = 1
(Ei,Ej ) of overlap ra-
K
tios reveals the trajectory consistency between expert i and
others, where K denotes the number of experts (in our ex-
periment, K = 7). In general, the pair-wise comparison s-
cores between two experts should be temporal stable. Thus,
the ﬂuctuation extent of overlap ratios in a short period ∆t
(e.g., 5 frames) reveals the stability of overlap evaluation
between Ei and other experts, which is given by Eq. (7).

P

V t
Ei =

K

1
K

v
u
j=1 (cid:16)
X
u
t
′t−∆t+1:t
(Ei,Ej ) = 1
∆t

where O
τ ∈ [t − ∆t + 1, t].

O

′t
(Ei,Ej ) − O

′t−∆t+1:t
(Ei,Ej )

(7)

2

,

(cid:17)

τ O

′τ
(Ei,Ej ) and the time index

P

Then, to avoid performance ﬂuctuation of the experts, we
further take temporal stability into account and introduce an
increasing sequence W = {ρ0, ρ1, · · · , ρ∆t−1}, (ρ > 1) to
give more weight to the recent scores. After considering the
temporal context, the average weighted mean and standard

4847

#8

Selected Cue:
Expert VII

#60

Selected Cue:
Expert V

#75

Selected Cue:
Expert II

#135

Selected Cue:
Expert IV

Expert I

Expert V

Expert II

Expert VI

Expert III

Expert VII

Expert IV

Figure 4. Expert selection process of our MCCT tracker in MotorRolling sequence. The bottom plots show the normalized robustness
scores of different experts. In top ﬁgures, the expert with the highest robustness score is selected for tracking. All the experts share the
same ROI (the largest yellow box) and are updated using the same selected results.

′t
τ Wτ M τ
Ei = 1
variance are calculated through: M
Ei
N
and V
, respectively, where Wτ denotes
the (τ − t + ∆t)-th element in sequence W and N is the
normalization factor deﬁned by N =

′t
Ei = 1
N

τ Wτ V τ
Ei

P

P

τ Wτ .

Finally, the pair-wise expert robustness score of Expert i

at frame t is deﬁned as follows,

P

′t
M
Ei
′t
Ei + ξ

,

V

Rt

pair(Ei) =

(8)

where ξ is a small constant that avoids the inﬁnite pair-
evaluation score for a zero denominator. A larger Rt
pair(Ei)
means better consistency with other experts and higher sta-
bility of the target state prediction.

Expert Self-Evaluation. The trajectory smoothness de-
gree of each expert indicates the reliability of its track-
ing results to some extent. The Euclidean distance mea-
suring the shift between the previous bounding box Bt−1
Ei
is computed by Dt
and the current bounding box Bt
Ei =
Ei
kc(Bt−1
Ei )k, where c(Bt
Ei ) − c(Bt
Ei ) is the center of the
bounding box Bt
In frame t, the trajectory ﬂuctuation
.
Ei
degree of Expert i is given by Eq. (9).

St

Ei = exp

−

1
2σ2
Ei

(Dt

Ei )2

,

(cid:19)

Ei)].

(cid:18)
where σEi is the average length of the width W (Bt
height H(Bt
i.e., σEi = 1

Ei) and
Ei) of the bounding box provided by Expert i,
2 [W (Bt

Ei) + H(Bt

Similar to the pair-evaluation, we collect the previous
movement information to consider the temporal stability.
Finally, the self-evaluation score is deﬁned by Rt
self (Ei) =
1
self (Ei) means the better re-
N
liability of the tracking trajectory.

. The higher Rt

τ Wτ Sτ
Ei

P
Expert Selection. The ﬁnal robustness score Rt(Ei)
of the Expert i in t-th frame is a linear combination of it-
s pair-evaluation score Rt
pair(Ei) and self-evaluation score

Rt

self (Ei):
Rt(Ei) = µ · Rt

pair(Ei) + (1 − µ) · Rt

self (Ei),

(10)

where µ is the parameter to trade off the pair-evaluation and
self-evaluation weights. In each frame, as shown in Figure
4, the expert with the highest robustness score is selected
for the current tracking.

Sharing Strategy. In the tracking process, all the expert-
s are updated using the same selected samples and share the
same searching areas (ROI). This sharing strategy has two
advantages: (1) It alleviates the drift and tracking failures
of the weak experts effectively by sharing the reﬁned re-
sults for target position prediction and model update; (2) It
ensures the efﬁciency of our framework greatly. The main
computational burden of DCF lies in the feature extraction
process, especially the deep features. Although multiple ex-
perts (K = 7) are maintained in our method, the sharing s-
trategy makes the feature extraction process only be carried
out twice in each frame (instead of 7 × 2 = 14 times), one
for the searching patch (ROI) and the other for the train-
ing patch (used for model update), which is the same as the
standard DCF and independent of the expert number.

Due to the training sample sharing strategy used in our
framework, the selected tracking results should be carefully
checked to avoid the corruption of the experts. The peak-
to-sidelobe ratio (PSR) is widely used in DCF to quantify
the reliability of the tracked samples [4]. PSR is deﬁned as
P = (Rmax − m)/σ, where Rmax is the maximum conﬁ-
dence, m and σ are the mean and standard deviation of the
response, respectively. We compute the average PSR of d-
ifferent features: P t
3 (P t
L) to evaluate
the t-th tracking result, where P t
L denote the PSR
values of the High, Middle and Low-level response map-
s, respectively. However, in some cases when the unreliable

H + P t
H , P t

mean = 1

M + P t

M , P t

(9)

3.4. Adaptive Expert Update

4848

tracked results have similar features with the target, the PSR
value may fail to make a difference.

mean = 1
K

In our experiment, we observe that when occlusion or se-
vere deformation occurs, the average robustness score of the
K
i=1 Rt(Ei) decreases signiﬁcant-
experts Rt
ly, which can be regarded as the divergence between mul-
tiple experts when facing an unreliable sample. Through
considering the average PSR score as well as average ex-
pert robustness score together, a combined reliability score
St = P t
mean is proposed, which can discover unre-
liable samples more effectively and better evaluate the qual-
ity of the current tracking.

mean · Rt

P

Since the DCF learns both target and background in-
formation, it is not reasonable to simply discard unreli-
able samples. When the current reliability score St is
signiﬁcantly lower than the past average reliability score:
t
i=1 Si, the learning rate η in Eq. (4) is de-
S1:t
mean = 1
t
creased by Eq. (11).

P

η =

C
C · [St/(α · S1:t

mean)]β

(

if St > α · S1:t
otherwise,

mean,

(11)

where C is the learning rate of the standard DCF, α is the re-
liability threshold, and β is the power exponent of the power
function. The designed power function penalizes samples
with low reliability scores severely to protect the experts
from corruption.

4. Experiments

4.1. Experimental Setup

Implementation Details: In our experiment, we follow
the parameters in standard DCF method [16] to construct
experts. The parameter ρ in the weigh sequence W is set to
1.1. The weighting factor µ in Eq. (10) is set to 0.1. As for
the adaptive update, α and β in Eq. (11) are set to 0.7 and
3 for MCCT tracker, and 0.6 and 3 for MCCT-H tracker,
respectively. More details can be found in the source code1.
We use the same setting of parameters for all the exper-
iments. Our trackers are implemented in MATLAB 2017a
on a computer with an Intel I7-4790K 4.00GHz CPU and
16GB RAM. The MatConvNet toolbox [42] is used for ex-
tracting the deep features from VGG-19 [39]. Our MCCT
tracker runs at about 1.5 FPS on CPU. The GPU version of
MCCT tracker runs at about 8 FPS, which is carried out on a
GeForce GTX 1080Ti GPU. The speed of MCCT-H tracker
is about 45 FPS on CPU.

Evaluation Benchmarks and Metrics: Our method is
evaluated on three benchmark datasets by a no-reset evalua-
tion protocol: OTB-2013 [47], OTB-2015 [48] and Temple-
Color [27]. All the tracking methods are evaluated by the
overlap precision (OP) at an overlap threshold 0.5. For the

1https://github.com/594422814/MCCT

Table 2. Effectiveness study of the proposed MCCT-H (top) and
MCCT (bottom) trackers. The DP (@20px) and AUC scores are
reported on the OTB-2013 [47] and OTB-2015 [48] datasets (D-
P/AUC) corresponding to the OPE. Expert VII (E7) is the best
individual expert (best baseline) in MCCT and MCCT-H.

E7 in MCCT-H MCCT-H-NU MCCT-H-PSR MCCT-H (Final)
Best Baseline
78.4 / 60.8
79.2 / 60.5

Sec. 3.3 + PSR
83.5 / 64.6
82.7 / 63.1

Sec. 3.3 + 3.4
85.6 / 66.4
84.1 / 64.2

Sec. 3.3
83.5 / 64.4
83.3 / 63.3

E7 in MCCT
Best Baseline
89.7 / 69.0
87.1 / 66.5

MCCT-NU
Sec. 3.3
91.9 / 70.9
88.3 / 67.6

MCCT-PSR
Sec. 3.3 + PSR
90.9 / 70.5
88.1 / 67.4

MCCT (Final)
Sec. 3.3 + 3.4
92.8 / 71.4
91.4 / 69.5

OTB-2013
OTB-2015

OTB-2013
OTB-2015

better performance measure, we also use the average dis-
tance precision (DP) plots and overlap success plots over
these datasets using one-pass evaluation (OPE) [47, 48].

Finally, we evaluate the performance of the proposed
trackers on the VOT2016 [21, 22] benchmark, which con-
sists of 60 challenging sequences and provides an evalua-
tion toolkit which will re-initialize the tracker to the correct
position to continue tracking when tracking failure occurs.
In VOT2016, the expected average overlap (EAO) is used
for ranking trackers, which combines the raw values of per-
frame accuracies and failures in a principled manner [21].

4.2. Framework Effectiveness Study

To evaluate the effectiveness of the proposed methods,
we compare the MCCT and MCCT-H trackers with their
individual experts. By only fusing multiple experts, we ob-
tain the MCCT-NU (MCCT-H-NU) without adaptive update
(only Sec. 3.3). The MCCT-PSR (MCCTH-PSR) repre-
sents the tracker adopts the fusion method and uses PSR
measurement for adaptive update (Sec. 3.3 + PSR). MCCT
(MCCT-H) is our ﬁnal algorithm which combines the ex-
pert selection mechanism and the proposed adaptive update
strategy (Sec. 3.3 + Sec. 3.4).

From the results in Table 2, we can observe that our ﬁ-
nal methods outperform their corresponding best baseline
Expert VII (the best individual expert) obviously, which il-
lustrates the effectiveness of the proposed framework. On
the OTB-2013 and OTB-2015 datasets, our ﬁnal MCCT-H
tracker outperforms its best baseline with a gain of about
6% (from 4.9%-7.2%) in DP and 4% (from 3.7%-5.6%) in
AUC. For tracker MCCT, it should be noted that its Expert
VII has already achieved a sufﬁcient performance level, but
our framework still boosts its performance further (about
4% DP and 3% AUC on challenging OTB-2015). Besides,
for the baseline MCCT-NU with high performance, the tra-
ditional PSR method for reliability estimation [4] does not
have obvious impact, but our adaptive update strategy by
considering the divergence of multiple experts improves the
performance further.

4849

Table 3. A comparison of our methods using overlap precision (OP) (%) at an overlap threshold 0.5 with recent state-of-the-art trackers on
the OTB-2013 [47], OTB-2015 [48] and Temple-Color [27] datasets. The average speed (i.e., frames per second, FPS) is evaluated on the
OTB-2013 dataset. The speed labeled with ∗ represents the corresponding tracker utilizes GPU in the experiment. The ﬁrst and second
highest values are highlighted by red and blue.

When
Where
OTB-2013
OTB-2015
Temple-Color
Speed (FPS)

LCT
2015

KCF
2015

HCF
DSST MEEM SAMF
2014
2015
2014
2014
BMVC ECCV ECCVW TPAMI CVPR ICCV
72.9
65.4
56.9
∗
10.8

62.6
55.3
45.9
243.7

81.4
70.1
55.3
26.1

66.8
61.6
47.0
26.3

71.4
66.4
56.8
18.3

70.3
62.5
60.6
19.2

2015
ICCV
78.1
72.6
61.2
5.6

2015
ICCVW
77.8
76.4
65.2
<1

HDT
2016

SCT
2016
2016
CVPR CVPR CVPR ECCV
77.8
74.2
73.0
63.0
63.5
55.7
∗
86.0
41.5

ECO MCCT-H MCCT
Staple SiamFc SRDCFdecon MDNet C-COT ACFN CSR-DCF ADNet MCPF
2017
2017
2016
2017
CVPR CVPR CVPR
87.0
84.6
80.6
84.1
77.5
77.2
73.5
66.9
-
∗
∗
15.0
0.6
2.9

2016
2017
ECCV CVPR
71.8
82.0
68.1
81.5
-
70.2
15.0
0.3

2017
CVPR
74.0
68.3
57.7
14.2

2016
CVPR
89.6
82.9
-
1.0

2016
CVPR
79.8
75.8
65.6
2.9

72.0
65.0
56.8
∗
10.5

90.7
86.4
74.4
∗
7.8

81.8
78.9
67.4
44.8

76.0
70.8
62.5
67.2

∗

∗

∗

SRDCF DeepSRDCDF

4.3. State of the art Comparison

We evaluate the proposed approach with 20 recent state-
of-the-art trackers including DSST [9], MEEM [51], SAMF
[26], KCF [16], LCT [32], HCF [31], SRDCF [12], Deep-
SRDCF [11], SCT [5], HDT [38], Staple [2], SiamFc [3],
SRDCFdecon [10], MDNet [35], C-COT [13], ACFN [6],
CSR-DCF [30], ADNet [50], MCPF [56], ECO [8]. A com-
parison with these state-of-the-art trackers using OP met-
ric on OTB-2013, OTB-2015 and Temple-Color datasets is
shown in Table 3.

Evaluation on OTB-2013. On the OTB-2013 [47]
benchmark, our proposed MCCT method achieves the best
OP of 90.7% (Table 3) and the highest area-under-curve
(AUC) score of 71.4% (Figure 5). The MDNet method
also exhibits excellent results and performs slightly better
than ours on DP metric (Figure 5), which is mainly due to
the effectiveness of the multi-domain network trained using
various similar tracking videos. Compared with other DCF
based methods, the proposed MCCT method outperform-
s the recent ECO, C-COT and MCPF on various metrics,
including on metrics OP, DP and AUC.

Evaluation on OTB-2015. On the OTB-2015 [48]
dataset, our proposed MCCT tracker achieves the best re-
sults on OP and DP with scores of 86.4% and 91.4% (Table
3 and Figure 6), outperforming the second best method by
2.3% and 1.3%. Among all the trackers, only ECO slightly
outperforms our method on AUC score. Our MCCT track-
er performs much better than HCF, HDT and MCPF, which
are also based on correlation ﬁlters with multiple types of
features.

The proposed MCCT-H method which only incorpo-
rates simple hand-crafted features achieves impressive per-
formance and operates at about 45 FPS on a single CPU
(Table 3).
It signiﬁcantly outperforms the DCF based
trackers with the same features (e.g., SAMF, SRDCF and
CSR-DCF) in both performance and speed. Furthermore,
MCCT-H achieves comparable results with the recent com-
plex trackers using deep features (e.g., ADNet, MCPF and
DeepSRDCF) but runs much faster than them. Compared
with other frame-wise adaption trackers (e.g., STC [5] and
ACFN [6]) that use attention mechanism for tracker con-
struction, our framework maintains several experts in par-
allel for decision-level selection and achieves better perfor-

1

0.8

0.6

0.4

0.2

t

e
a
r
 
s
s
e
c
c
u
S

0

0

1

0.8

0.6

0.4

0.2

e
t
a
r
 
s
s
e
c
c
u
S

0

0

1

0.8

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0.6

0.4

0.2

0

0

1

0.8

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0.6

0.4

0.2

0

0

Precision plots of OPE

Success plots of OPE

MDNet [93.2]
MCCT(Ours) [92.8]
ECO [91.3]
MCPF [90.3]
HCF [89.2]
C-COT [89.1]
ADNet [87.4]
HDT [87.0]
MCCTH(Ours) [85.6]
SRDCFdecon [85.3]

MCCT(Ours) [71.4]
ECO [71.2]
MDNet [71.2]
MCPF [68.2]
C-COT [68.0]
MCCT-H(Ours) [66.4]
SRDCFdecon [65.6]
ADNet [64.9]
DeepSRDCF [64.3]
LCT [64.0]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 5. Precision and success plots on the OTB-2013 [47] dataset
with 50 videos. Only the top 10 trackers are displayed for clarity.
In the legend, the DP at a threshold of 20 pixels and area-under-
curve (AUC) are reported in the left and right ﬁgures, respectively.

Precision plots of OPE

Success plots of OPE

MCCT(Ours) [91.4]
ECO [90.1]
C-COT [89.4]
MDNet [89.1]
MCPF [86.6]
ADNet [84.4]
DeepSRDCF [84.2]
MCCT-H(Ours) [84.1]
HCF [83.9]
HDT [83.6]

ECO [69.6]
MCCT(Ours) [69.5]
C-COT [68.1]
MDNet [67.6]
DeepSRDCF [64.2]
MCCT-H(Ours) [64.2]
ADNet [63.6]
MCPF [63.5]
SRDCFdecon [63.4]
SRDCF [60.9]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 6. Precision and success plots on the OTB-2015 [48] dataset
with 100 videos. In the legend, the DP (@20px) and AUC scores
are reported in the left and right ﬁgures, respectively.

mance (Table 3, Figure 5 and 6).

Evaluation on Temple-Color. For further evaluation,
we compare the proposed MCCT-H and MCCT methods on
the Temple-Color dataset [27] with the trackers mentioned
in Sec. 4.3 excluding MDNet, ADNet and ACFN, which all
need many external tracking videos for network training. In
contrast, our approach is free of such necessity.

In Table 3 and Figure 7, the proposed MCCT-H track-
er still achieves outstanding performance amongst real-time
trackers and outperforms some complex non-realtime track-
ers. Our MCCT method achieves the OP, DP and AUC s-
cores of (74.4%, 79.7%, 59.6%), while ECO and C-COT
yield (73.5%, 79.7%, 60.7%) and (70.2%, 78.1%, 58.3%),
respectively. Overall, our MCCT tracker shows compara-
ble results compared to the recent performance leader E-
CO and outperforms other state-of-the-art methods (e.g., C-
COT) on various metrics.

4850

Precision plots of OPE

Success plots of OPE

Expected overlap scores for baseline

1

0.8

0.6

0.4

0.2

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0

0

MCCT(Ours) [79.7]
ECO [79.7]
C-COT [78.1]
MCPF [77.4]
DeepSRDCF [73.8]
SRDCFdecon [72.7]
MCCT-H(Ours) [72.0]
MEEM [69.9]
SiamFc [69.2]
HCF [68.9]

1

0.8

0.6

0.4

0.2

t

e
a
r
 
s
s
e
c
c
u
S

0

0

ECO [60.7]
MCCT(Ours) [59.6]
C-COT [58.3]
MCPF [55.3]
MCCT-H(Ours) [55.1]
DeepSRDCF [54.5]
SRDCFdecon [54.3]
Staple [50.9]
SiamFc [50.5]
MEEM [50.3]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 7. Precision and success plots on the Temple-Color [27]
dataset with 128 color videos. In the legend, the DP (@20px) and
AUC scores are reported in the left and right ﬁgures, respectively.

Evaluation on VOT2016. Figure 8 shows the rank-
ing results in terms of expected average overlap (EAO) in
VOT2016 [21], from which we can observe that our MCC-
T tracker outperforms the top performer (C-COT [13]) by a
considerable margin. For presentation clarity, we only show
some top ranked and baseline trackers for comparison. For
more information, please refer to [21].

In Table 4, we list the detailed results of our approach-
es and the top ranked methods in VOT2016 (e.g., C-COT
[13], TCNN [34] and SSAT). Besides, two recently pro-
posed methods: CSR-DCF [30] and ECO [8] are also put
into comparison which do not participate in VOT2016. A-
mong all the compared methods, the proposed MCCT track-
er demonstrates advances in both accuracy and robustness.
As a result, our MCCT tracker provides the best EAO score
of 0.393, achieving a relative gain of 18.7% compared to
the VOT2016 top performer C-COT.

Discussion: (1) About Performance. It should be noted
that the ECO, C-COT and DeepSRDCF methods all take S-
RDCF as baseline, which can alleviate boundary effects ef-
fectively and performs much better than the standard DCF
[4, 16]. Besides, ECO and C-COT adopt the novel contin-
uous operator to better fuse the feature maps. However, all
the experts in our MCCT-H and MCCT methods just take
the simple DCF as baseline and all the strategies mentioned
above and other novel techniques [14, 33] can also be inte-
grated into our framework to further boost the performance.
(2) About Efﬁciency. Our tracker achieves almost the same
speed with expert VII because the same number of DCFs
and features are utilized (i.e., sharing strategy), but achieves
much better performance than it. Our general framework
provides a promising alternative for the multi-feature based
DCF trackers with ignorable impact on efﬁciency. Further-
more, we believe that other tracking algorithms [53, 36, 55]
with multiple types of features can also beneﬁt from our
multi-cue analysis framework.

5. Conclusion

In this paper, we propose a multi-cue analysis frame-
work for robust visual tracking, which considers not only

p
a
l
r
e
v
o

 

d
e
t
c
e
p
x
e

 

e
g
a
r
e
v
A

0.5

0.4

0.3

0.2

0.1

0

13

10

4

1

7
Order

TCNN
SRDCF

MCCT
EBT

MCCT-H
CCOT
DeepSRDCF MDNet

SSAT
HCF

MLDF
DSST

Staple
KCF

Figure 8. Expected Average Overlap (EAO) graph with trackers
ranked from right to left evaluated on VOT2016 [21]. Our pro-
posed MCCT tracker outperforms the top performer (C-COT [13])
by a considerable margin.

Table 4. The accuracy, robustness (failure rate) and EAO of state-
of-the-art methods on the VOT2016 [21]. The proposed MCCT
tracker achieves superior results compared to the top ranked meth-
ods in the challenge and recently proposed state-of-the-art algo-
rithms (ECO [8] and CSR-DCF [30]). The ﬁrst and second highest
values are highlighted by red and blue.

MCCT-H MLDF SSAT TCNN C-COT CSR-DCF ECO MCCT

Accuracy
Failure Rate
EAO

0.57
1.24
0.305

0.57
0.54
0.48
0.96
1.04
0.83
0.311 0.321 0.325

0.52
0.85
0.331

0.51
0.85
0.338

0.58
0.54
0.73
0.72
0.374 0.393

feature-level fusion but also decision-level fusion to fully
explore the strength of multiple features. Our framework
maintains multiple experts to track the target via different
views and selects the reliable outputs to reﬁne the tracking
results. Moreover, the proposed method evaluates the unre-
liable samples through considering the divergence of mul-
tiple experts and updates them adaptively. Through exten-
sive experiments on several challenging datasets, we show
that after adopting our simple yet effective multi-cue analy-
sis framework, without sophisticated models, only standard
Discriminative Correlation Filter (DCF) with deep or hand-
crafted features is able to perform favorably against state-
of-the-art methods in both accuracy and efﬁciency.

Acknowledgement. This work was supported in part
to Dr. Houqiang Li by 973 Pro-gram under contract No.
2015CB351803 and NSFC under contract No. 61390514,
in part to Dr. Wengang Zhou by NSFC under contract No.
61472378 and No. 61632019, the Fundamental Research
Funds for the Central Universities, and Young Elite Scien-
tists Sponsorship Program By CAST (2016QNRC001), and
in part to Dr. Qi Tian by ARO grant W911NF-15-1-0290
and Faculty Research Gift Awards by NEC Laboratories of
America and Blippar. This work was supported in part by
National Science Foundation of China (NSFC) 61429201.

4851

References

[1] C. Bailer, A. Pagani, and D. Stricker. A superior tracking ap-
proach: Building a strong tracker through fusion. In ECCV,
2014. 2

[2] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and
P. Torr. Staple: Complementary learners for real-time track-
ing. In CVPR, 2016. 2, 3, 7

[3] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and
P. H. Torr. Fully-convolutional siamese networks for object
tracking. In ECCV, 2016. 7

[4] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
CVPR, 2010. 1, 2, 3, 5, 6, 8

[5] J. Choi, H. Jin Chang, J. Jeong, Y. Demiris, and J. Y-
oung Choi. Visual tracking using attention-modulated dis-
integration and integration. In CVPR, 2016. 3, 7

[6] J. Choi, H. Jin Chang, S. Yun, T. Fischer, Y. Demiris, and
J. Young Choi. Attentional correlation ﬁlter network for
adaptive visual tracking. In CVPR, 2017. 7

[7] N. Dalal and B. Triggs. Histograms of oriented gradients for

human detection. In CVPR, 2005. 2, 3, 4

[8] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg.
Eco: Efﬁcient convolution operators for tracking. In CVPR,
2017. 2, 7, 8

[9] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accurate
scale estimation for robust visual tracking. In BMVC, 2014.
1, 3, 7

[10] M. Danelljan, G. H¨ager, F. S. Khan, and M. Felsberg. Adap-
tive decontamination of the training set: A uniﬁed formula-
tion for discriminative visual tracking. In CVPR, 2016. 2,
7

[11] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Fels-
berg. Convolutional features for correlation ﬁlter based vi-
sual tracking. In ICCV Workshop, 2015. 1, 2, 7

[12] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg.
Learning spatially regularized correlation ﬁlters for visual
tracking. In ICCV, 2015. 1, 2, 7

[13] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. In ECCV, 2016. 1, 2, 7, 8

[14] H. K. Galoogahi, A. Fagg, and S. Lucey.

Learning
background-aware correlation ﬁlters for visual tracking. In
ICCV, 2017. 1, 8

[15] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In ECCV, 2012. 2

[16] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
speed tracking with kernelized correlation ﬁlters. TPAMI,
37(3):583–596, 2015. 1, 2, 3, 6, 7, 8

[17] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychol-
In CVPR, 2015.
ogy inspired approach to object tracking.
1

[18] X. Jia, H. Lu, and M.-H. Yang. Visual tracking via adaptive
structural local sparse appearance model. In CVPR, 2012. 2
[19] O. Khalid, J. C. SanMiguel, and A. Cavallaro. Multi-tracker

partition fusion. TCSVT, 27(7):1527–1539, 2017. 2

[20] H. Kiani Galoogahi, T. Sim, and S. Lucey. Correlation ﬁlters

with limited boundaries. In CVPR, 2015. 1

[21] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, L. Cehovin,
G. Fern´andez, T. Vojir, Hager, and et al. The visual objec-
In ECCV Workshop,
t tracking vot2016 challenge results.
2016. 2, 6, 8

[22] M. Kristan, J. Matas, A. Leonardis, T. Voj´ıˇr, R. Pﬂugfelder,
G. Fernandez, G. Nebehay, F. Porikli, and L. ˇCehovin. A
novel performance evaluation methodology for single-target
trackers. TPAMI, 38(11):2137–2155, 2016. 6
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1

Imagenet
In

[24] D. Y. Lee, J. Y. Sim, and C. S. Kim. Multihypothesis tra-
jectory analysis for robust visual tracking. In CVPR, 2015.
2

[25] J. Li, C. Deng, R. Y. D. Xu, D. Tao, and B. Zhao. Robust
object tracking with discrete graph based multiple experts.
TIP, 26(6):2736–2750, 2017. 2

[26] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. In ECCV Workshop, 2014.
1, 7

[27] P. Liang, E. Blasch, and H. Ling. Encoding color informa-
tion for visual tracking: algorithms and benchmark. TIP,
24(12):5630–5644, 2015. 2, 6, 7, 8

[28] S. Liu, T. Zhang, X. Cao, and C. Xu. Structural correlation

ﬁlter for robust visual tracking. In CVPR, 2016. 1

[29] T. Liu, G. Wang, and Q. Yang. Real-time part-based visual

tracking via adaptive correlation ﬁlters. In CVPR, 2015. 1

[30] A. Lukezic, T. Vojir, L. Cehovin Zajc, J. Matas, and M. Kris-
tan. Discriminative correlation ﬁlter with channel and spatial
reliability. In CVPR, 2017. 1, 2, 3, 7, 8

[31] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In ICCV, 2015. 1,
3, 4, 7

[32] C. Ma, X. Yang, C. Zhang, and M.-H. Yang. Long-term

correlation tracking. In CVPR, 2015. 1, 7

[33] M. Mueller, N. Smith, and B. Ghanem. Context-aware cor-

relation ﬁlter tracking. In CVPR, 2017. 1, 8

[34] H. Nam, M. Baek, and B. Han. Modeling and propagating
cnns in a tree structure for visual tracking. arXiv preprint
arXiv:1608.07242, 2016. 8

[35] H. Nam and B. Han. Learning multi-domain convolutional
neural networks for visual tracking. In CVPR, 2016. 1, 7
[36] J. Ning, J. Yang, S. Jiang, L. Zhang, and M.-H. Yang. Object
tracking via dual linear structured svm and explicit feature
map. In CVPR, 2016. 8

[37] H. Possegger, T. Mauthner, and H. Bischof.

In defense of

color-based model-free tracking. In CVPR, 2015. 3

[38] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-H.
Yang. Hedged deep tracking. In CVPR, 2016. 1, 2, 7
[39] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 1, 3, 6

[40] Y. Song, C. Ma, L. Gong, J. Zhang, R. Lau, and M.-H. Yang.
Crest: Convolutional residual learning for visual tracking. In
ICCV, 2017. 1

4852

[41] J. Valmadre, L. Bertinetto, J. F. Henriques, A. Vedaldi, and
P. H. Torr. End-to-end representation learning for correlation
ﬁlter based tracking. In CVPR, 2017. 1

[42] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural

networks for matlab. In ACM MM, 2014. 6

[43] N. Wang, J. Shi, D. Y. Yeung, and J. Jia. Understanding and
diagnosing visual tracking systems. In ICCV, 2015. 2, 4
[44] N. Wang and D. Y. Yeung. Ensemble-based tracking: Aggre-
gating crowdsourced structured time series data. In ICML,
2014. 2

[45] N. Wang, W. Zhou, and H. Li. Reliable re-detection for long-

term tracking. TCSVT, 2018. 1

[46] J. V. D. Weijer, C. Schmid, J. Verbeek, and D. Larlus.
TIP,
Learning color names for real-world applications.
18(7):1512–1523, 2009. 2, 4

[47] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A

benchmark. In CVPR, 2013. 2, 6, 7

[48] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.

TPAMI, 37(9):1834–1848, 2015. 1, 2, 6, 7

[49] A. Yilmaz, O. Javed, and M. Shah. Object tracking: A sur-

vey. ACM Computing Surveys, 38(4):13, 2006. 1

[50] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Young Choi. Action-
decision networks for visual tracking with deep reinforce-
ment learning. In CVPR, 2017. 7

[51] J. Zhang, S. Ma, and S. Sclaroff. Meem: robust tracking
via multiple experts using entropy minimization. In ECCV,
2014. 2, 7

[52] K. Zhang, L. Zhang, and M.-H. Yang. Real-time compres-

sive tracking. In ECCV, 2012. 1

[53] K. Zhang, L. Zhang, and M.-H. Yang. Real-time objec-
t tracking via online discriminative feature selection. TIP,
22(12):4664–4677, 2013. 8

[54] K. Zhang, L. Zhang, M.-H. Yang, and D. Zhang. Fast vi-
sual tracking via dense spatio-temporal context learning. In
ECCV, 2013. 2

[55] T. Zhang, A. Bibi, and B. Ghanem.

In defense of sparse

tracking: Circulant sparse tracker. In CVPR, 2016. 8
[56] T. Zhang, C. Xu, and M.-H. Yang. Multi-task correlation
particle ﬁlter for robust object tracking. In CVPR, 2017. 1, 7
[57] W. Zhong, H. Lu, and M.-H. Yang. Robust object tracking

via sparsity-based collaborative model. In CVPR, 2012. 2

4853

Multi-Cue Correlation Filters for Robust Visual Tracking

Ning Wang1, Wengang Zhou1, Qi Tian2, Richang Hong3, Meng Wang3, Houqiang Li1
1CAS Key Laboratory of GIPAS, University of Science and Technology of China
2University of Texas at San Antonio 3HeFei University of Technology
wn6149@mail.ustc.edu.cn, {zhwg,lihq}@ustc.edu.cn, qi.tian@utsa.edu, {hongrc,wangmeng}@hfut.edu.cn

Abstract

In recent years, many tracking algorithms achieve im-
pressive performance via fusing multiple types of features,
however, most of them fail to fully explore the context among
the adopted multiple features and the strength of them. In
this paper, we propose an efﬁcient multi-cue analysis frame-
work for robust visual tracking. By combining differen-
t types of features, our approach constructs multiple ex-
perts through Discriminative Correlation Filter (DCF) and
each of them tracks the target independently. With the pro-
posed robustness evaluation strategy, the suitable expert is
selected for tracking in each frame. Furthermore, the di-
vergence of multiple experts reveals the reliability of the
current tracking, which is quantiﬁed to update the experts
adaptively to keep them from corruption.

Through the proposed multi-cue analysis, our tracker
with standard DCF and deep features achieves outstand-
ing results on several challenging benchmarks: OTB-2013,
OTB-2015, Temple-Color and VOT 2016. On the other
hand, when evaluated with only simple hand-crafted fea-
tures, our method demonstrates comparable performance
amongst complex non-realtime trackers, but exhibits much
better efﬁciency, with a speed of 45 FPS on a CPU.

1. Introduction

Visual tracking is a fundamental task in computer vision
with a wide range of applications. It is challenging since
only the initial state of the target is available. Although
substantial progress has been made in the past decades
[52, 16, 35], there still remain many challenges [49].

In recent years, Discriminative Correlation Filter (DCF)
based tracking methods [4, 16] have gained much atten-
tion thanks to their impressive performance as well as high
speed. In DCF based trackers, the ﬁlter is trained through
minimizing a least-squares loss for all circular shifts of a
training sample. Since the correlation operation can be cal-
culated in Fourier domain, DCF shows the advantage of
Its performance is further
high computational efﬁciency.

(a) Biker

(b) Box

(c) Diving

(d) Shaking

(e) Singer2

(f) MotorRolling

MCCT

C-COT

DeepSRDCF

SRDCF

Figure 1. Comparison of the proposed algorithm (MCCT) with
the state-of-the-art trackers: SRDCF [12], DeepSRDCF [11] and
C-COT [13]. These trackers adopt various types of features and
perform differently in challenging scenes. Our method maintain-
s multiple cues for tracking and performs favorably against these
trackers. All the videos are from OTB-2015 [48].

enhanced by using multi-dimensional features [16, 26, 31],
part-based strategies [29, 28], adaptive scale [9, 26], long-
term framework [32, 17, 45], end-to-end learning [41, 40]
and improved ﬁlter training methods [20, 30, 33, 14]. Spe-
cially, the combination of DCF and features from deep con-
volutional neural networks (CNNs) [23, 39] has demonstrat-
ed state-of-the-art results. Recently, Ma et al. [31] propose
a HCF algorithm, which constructs multiple DCFs on low,
middle and high-level features to capture both spatial detail-
s and semantics. It predicts the target location using multi-
level DCF response maps in a coarse-to-ﬁne fashion.

Although feature-level fusion methods [26, 31, 38] have
been widely used or extended to boost the performance,
there still leaves room for improvement.
In HCF [31] or
other methods [56, 38] that follow such a fusion strategy,
the initial weight of high-level features is usually high such
that semantic features play the dominant role in general. It
is reasonable because of better effectiveness of high-level
features compared to shallow features. However, due to the
occasional misguidance of the semantic information, a tran-
sient drift or wrong prediction may be ampliﬁed by the in-
adequate online update. Therefore, the feature-level fusion

4844

approach sometimes still fails to fully explore the relation-
ship of multiple features. Furthermore, it is quite difﬁcult to
handle various challenging variations using a single model,
and relying on a certain feature-level fusion strategy limits
the model diversity to some extent.

To better illustrate the issues mentioned above, our pro-
posed algorithm is compared with three DCF based method-
s. In Figure 1, SRDCF [12], DeepSRDCF [11] and C-COT
[13] adopt different types of features, but none of them is
able to handle various challenging factors, even for the C-
COT algorithm with multiple features as well as the nov-
el continuous convolution operators. Since it is quite difﬁ-
cult to design a satisfying feature-level fusion method that
suits various challenging scenes, it is intuitive to design an
adaptive switch mechanism to achieve better performance,
which can ﬂexibly switch to the reliable tracker depend-
ing on what kind of challenging factors it is expert at han-
dling. In other words, the performance of a single tracker
can sometimes be unstable but the decision-level fusion of
the outputs from multiple trackers can enhance the robust-
ness effectively.

In this paper, a novel Multi-Cue Correlation ﬁlter based
Tracker (MCCT) is proposed. Different from the previous
DCF based methods [4, 16] that usually lack the diversity
of target appearance representations, our method maintains
multiple experts to learn the appearance models from dif-
ferent views. Here, a certain combination of features con-
structs an individual expert and provides a reliable cue (pre-
dicted target state) for tracking. The main contributions
(1) We pro-
of our work can be summarized as follows.
pose an algorithm that maintains multiple cues for track-
ing. Through checking the robustness scores of multiple
experts carefully, our method reﬁnes the tracking results
by choosing the reliable expert for tracking in each frame.
(2) By considering the divergence of multiple experts, we
present an adaptive update strategy which can discriminate
the unreliable samples (e.g., occlusion or sever deforma-
tion) effectively and alleviate the contamination of train-
(3) We implement two versions of the pro-
ing samples.
posed method to validate the generality of the framework.
The MCCT tracker with deep features demonstrates out-
standing performance on several challenging benchmarks
[47, 48, 27, 21]. The MCCT-H tracker with only two s-
tandard Hand-crafted features (HOG [7] and ColorNames
[46]) achieves comparable performance with many complex
deep-model based trackers, but operates about 45 frames
per second on a single CPU, exceeding most competitive
trackers by several times.

2. Related Work

In this section, we discuss two categories of trackers
closely related to our algorithm: correlation tracking and
ensemble tracking.

Correlation Tracking. Since Bolme et al.

[4] pro-
pose a tracker using minimum output sum of squared er-
ror (MOSSE) ﬁlter, the correlation ﬁlters have been widely
studied in visual tracking. Heriques et al. exploit the cir-
culant structure of the training patches [15] and propose to
train correlation ﬁlter in a kernel space with HOG features
[16]. Zhang et al. [54] incorporate context information in-
to ﬁlter learning. The SRDCF tracker [12] alleviates the
boundary effects by penalizing correlation ﬁlter coefﬁcients
depending on spatial location, and is enhanced by reduc-
ing the inﬂuence of corrupted samples [10]. Qi et al. [38]
propose a HDT algorithm that fuses several DCFs through
adaptive hedged method. Luca et al.
[2] propose a Sta-
ple tracker which combines DCF and color histogram based
model while running in excess of real-time. The CSR-DCF
algorithm [30] constructs DCF with channel and spatial reli-
ability. The recent C-COT [13] adopts a continuous-domain
formulation of the DCF, leading to the top performance on
several tracking benchmarks. The enhanced version of C-
COT is ECO [8], which improves both speed and perfor-
mance by introducing several efﬁcient strategies.

Different from the DCF based methods mentioned
above, our algorithm considers not only feature-level fusion
but also decision-level fusion to better explore the relation-
ship of multiple features, and adaptively selects the expert
that is suitable for a particular tracking task.

Ensemble Tracking. To enhance the tracking robust-
ness and obtain reliable results, the ensemble approach
treats the trackers as black boxes and takes only the bound-
ing boxes returned by them as input [43]. In [1], a dynam-
ic programming based trajectory optimization approach is
proposed to build a strong tracker. In [44], Wang et al. pro-
pose a factorial hidden Markov model for ensemble-based
tracking. The MEEM algorithm [51] exploits the relation-
ship between the current tracker and its historical snapshots
using entropy minimization, and Li et al.
[25] extend it
by introducing a discrete graph optimization framework. In
[19], a partition fusion framework is proposed to cluster re-
liable trackers for target state prediction.

Although promising results are obtained through fusing
multiple trackers, there still leave some limitations: (1) the
overall speed is limited by the lowest tracker in the en-
semble (e.g., sparse representation based methods [57, 18],
about 1 FPS), which restricts the real-time application. Spe-
cially, the fusion methods [19, 24] by analyzing the for-
ward and backward trajectories require each tracker to run
at least twice; (2) the trackers are just regarded as inde-
pendent black boxes and their fusion result in each frame
does not feedback to the trackers [1, 44, 19], which fails to
make full use of the reliable fusion outputs; (3) if the fu-
sion tracker number increases, the dynamic programming
based fusion methods [25, 1] still bring obvious computa-
tional burden (e.g., O(T N 2) for T frames and N trackers).

4845

Flowchart

Input 
Frame

ROI

Feature  Pool

Expert  Pool  (DCF)

Expert  Selection

Feature 
Extration

Multi-Expert
Prediction

...

Adaptive
Model
Update

Figure 2. A systematic ﬂowchart of the proposed tracking algorithm. First, different features of ROI are extracted and combined to train
multiple experts following DCF framework (Sec. 3.1 and 3.2). Then, each expert gives an individually predicted cue and the most reliable
expert is selected for the current tracking (Sec. 3.3). Finally, adaptive update helps keep the experts from corruption (Sec. 3.4).

Different from the above methods:

(1) Our approach
the experts in the DCF framework, and
constructs all
through the proposed ROI and training sample sharing s-
trategy (Sec. 3.3), the efﬁciency is greatly ensured; (2) the
reﬁned tracking results are fed back to the experts to boost
them further; (3) through a simple but effective robustness
evaluation strategy, our method selects the reliable expert
for tracking with a complexity of only O(T N ) for T frames
and N experts.

3. Method

Our algorithm is composed of several experts which pro-
vide different levels of cues for tracking. A preview of DCF
is introduced in Sec. 3.1. The component of the expert pool
is described in Sec. 3.2. How to switch to the suitable ex-
pert in each frame is elaborated in Sec. 3.3. Finally, Sec.
3.4 introduces the adaptive update. The framework of our
method is depicted in Figure 2.

3.1. Preview of DCF

A typical tracker based on DCF [4, 16] is trained using
an image patch x of size M × N , which is centered around
the target. All the circular shifts of the patch x(m, n) ∈
{0, 1, ...M − 1} × {0, 1, ...N − 1} are generated as training
samples with Gaussian function label y(m, n) in terms of
the shifted distance. The ﬁlter w is trained by minimizing
the following regression error:

min
w

kXw − yk2

2 + λkwk2
2,

where λ is a regularization parameter (λ ≥ 0) and X is the
data matrix by concatenating all the circular shifts. The ﬁl-
ter solution on the d-th (d ∈ {1, · · · , D}) channel is deﬁned
by

ˆw∗

d =

ˆy ⊙ ˆx∗
d
i ⊙ ˆxi + λ

,

D

i=1 ˆx∗
where ⊙ is the element-wise product, the hat symbol de-
notes the Discrete Fourier Transform (DFT) of a vector
(e.g., ˆx = F(x)) and ˆx∗ is the complex-conjugate of ˆx.
In the next frame, a Region of Interest (ROI) is cropped out
for tracking the target (e.g., a patch z with the same size of

P

(1)

(2)

x). The response map R of z is calculated in Eq. (3) and
the location of the target is identiﬁed by searching for the
maximum value of R.

R = F

−1(

ˆwd ⊙ ˆz∗

d).

(3)

D

d=1
X

To avoid the boundary effects during learning, we apply
Hann window to the signals [16]. Besides, inspired by [30,
5], color information is applied to the training sample in a
= X ⊙ C,
simple way to enhance its spatial reliability: X
where X represents the data matrix and C is the color mask
obtained by computing the histogram-based per-pixel score
map [37, 2] of ROI. The online update of the numerator ˆAd
and the denominator ˆBd of the ﬁlter ˆw∗

d is as follows,

′

ˆAt

d = (1 − η) ˆAt−1

d + η ˆy ⊙ ˆx∗t
d ,

ˆBt

d = (1 − η) ˆBt−1

d + η

ˆx∗t
i ⊙ ˆxt
i,

(4)

D

i=1
X

ˆw∗t

d =

ˆAt
d
d + λ

,

ˆBt

where η is the learning rate and t is the index of the current
frame. As for the target scale estimation, we follow the
DSST tracker [9].

3.2. Feature Pool and Expert Pool

A variety of features can be adopted in DCF and differ-
ent features have their own strength. Hand-crafted features
are typically used to capture low-level details while deep
features are semantic-aware. As discussed in HCF [31], the
DCF constructed by each single layer of VGG-19 [39] is
not accurate enough, so HCF performs coarse-to-ﬁne search
on several DCF response maps from different layers.
In
our MCCT tracker, HOG [7] is used as low-level features.
Then, we remove the fully-connected layers and extract the
outputs of the conv4-4 and conv5-4 convolutional layers of
VGG-19 as middle-level and high-level features, respective-
ly. Thus, the feature pool consists of three types of features:
{Low, M iddle, High} and they are optionally combined
into C 1
3 = 7 experts. As for the coarse-to-ﬁne

3 + C 2

3 + C 3

4846

Table 1. The component of the expert ensemble. Our MCCT
and MCCT-H trackers both consist of seven experts. Each expert
adopts different features and tracks the target via a different view.

Expert Pool
Expert I
Expert II
Expert III
Expert IV
Expert V
Expert VI
Expert VII

Tracker MCCT
Feature Type
Low (HOG)
Middle (conv4-4 of VGG-19)
High (conv5-4 of VGG-19)
Middle, Low
High, Low
High, Middle
High, Middle, Low

Tracker MCCT-H
Feature Type
HOG1
HOG2
ColorNames
HOG1, ColorNames
HOG2, ColorNames
HOG1, HOG2
HOG1, HOG2, ColorNames

Expert I

Expert II

Expert III

Expert IV

…

…

…

…

…

…

…

…

weighting parameters of different level DCF response maps,
we follow the settings in HCF [31]. Although some experts
(e.g., Expert I, II and III) with single type of features may be
less robust compared to Expert VII, they provide the diver-
sity of tracking results, which is crucial in ensemble-based
tracking [43].

Our fast variant, the MCCT-H tracker only utilizes stan-
dard hand-crafted features (HOG [7] and ColorNames [46])
to construct experts. Since HOG and ColorNames are both
low-level features, we do not conduct coarse-to-ﬁne fusion
and just simply concatenate them to construct DCF. Col-
orNames provides an 11 dimensional color representation
and HOG feature is 31-dim. To obtain more experts, we
take the average grey value over all pixels in a patch as a
1-dim feature and concatenate it to the HOG feature into a
32-dim vector, which is further evenly decomposed into two
16-dim features, denoted as HOG1 and HOG2, respective-
ly. The detailed information of the experts in MCCT and
MCCT-H is shown in Table 1.

3.3. Multi Cue Correlation Tracker

The tracking process of our proposed framework can be
illustrated by Figure 3, where the multiple experts track the
target in parallel and the nodes denote the generated cues
(bounding boxes) of the experts. In each frame, the evalua-
tion between different hypothesis nodes reveals the consis-
tency degree between the experts, which is denoted as pair-
wise evaluation. Besides, each expert has its own trajec-
tory continuity and smoothness. Thus, given a hypothesis
node, its robustness degree can be evaluated by both pair-
evaluation and self-evaluation. After evaluating the overall
reliability of each node, the expert with the highest robust-
ness score is selected and its tracking result is taken for the
current frame.

In the following, we elaborate the formulation of pair-

evaluation, self-evaluation and expert selection strategy.

Expert Pair-Evaluation. Most experts in the ensemble
are capable of tracking the target stably, and the cues pro-
duced by a good expert should be consistent with the cues
from other experts as much as possible.

Let E1, · · · , E7 denote Expert I, · · · , Expert VII, respec-
tively. In the t-th frame, the bounding box of Expert i is

Frame t

Frame t+1

Frame t+2

Expert Pair-Evaluation

Expert Self-Evaluation

Figure 3. Graph illustration of the multi-expert framework. The
node in the graph denotes the predicted bounding box. The ro-
bustness score of each expert is evaluated by both pair-evaluation
and self-evaluation. For clarity, only four experts are displayed.

denoted as Bt
. Through regarding all the experts as black
Ei
boxes, the bounding box Bt
only contains the target state
Ei
(e.g., location and scale) without any context information,
which reduces the computational and memory burden ef-
fectively.

First, we compute overlap ratios of the bounding boxes
(Ei,Ej ) of Expert

from different experts. The overlap ratio Ot
i and Expert j at frame t is calculated as follows,

Ot

(Ei,Ej ) =

Area(Bt
Ei
Area(Bt
Ei

Bt
Bt

Ej )
Ej )

.

T

(5)

To reduce the gap between low and high overlap ratios, we
adopt a nonlinear gaussian function to Ot
(Ei,Ej ) as follows,
which gathers the expert scores.

S

O

′t
(Ei,Ej ) = exp

−

1 − Ot

(Ei,Ej )

2

.

(6)

(cid:17)

(cid:16)

K
j=1 O

The mean value M t

(cid:18)
(cid:19)
′t
Ei = 1
(Ei,Ej ) of overlap ra-
K
tios reveals the trajectory consistency between expert i and
others, where K denotes the number of experts (in our ex-
periment, K = 7). In general, the pair-wise comparison s-
cores between two experts should be temporal stable. Thus,
the ﬂuctuation extent of overlap ratios in a short period ∆t
(e.g., 5 frames) reveals the stability of overlap evaluation
between Ei and other experts, which is given by Eq. (7).

P

V t
Ei =

K

1
K

v
u
j=1 (cid:16)
X
u
t
′t−∆t+1:t
(Ei,Ej ) = 1
∆t

where O
τ ∈ [t − ∆t + 1, t].

O

′t
(Ei,Ej ) − O

′t−∆t+1:t
(Ei,Ej )

(7)

2

,

(cid:17)

τ O

′τ
(Ei,Ej ) and the time index

P

Then, to avoid performance ﬂuctuation of the experts, we
further take temporal stability into account and introduce an
increasing sequence W = {ρ0, ρ1, · · · , ρ∆t−1}, (ρ > 1) to
give more weight to the recent scores. After considering the
temporal context, the average weighted mean and standard

4847

#8

Selected Cue:
Expert VII

#60

Selected Cue:
Expert V

#75

Selected Cue:
Expert II

#135

Selected Cue:
Expert IV

Expert I

Expert V

Expert II

Expert VI

Expert III

Expert VII

Expert IV

Figure 4. Expert selection process of our MCCT tracker in MotorRolling sequence. The bottom plots show the normalized robustness
scores of different experts. In top ﬁgures, the expert with the highest robustness score is selected for tracking. All the experts share the
same ROI (the largest yellow box) and are updated using the same selected results.

′t
τ Wτ M τ
Ei = 1
variance are calculated through: M
Ei
N
and V
, respectively, where Wτ denotes
the (τ − t + ∆t)-th element in sequence W and N is the
normalization factor deﬁned by N =

′t
Ei = 1
N

τ Wτ V τ
Ei

P

P

τ Wτ .

Finally, the pair-wise expert robustness score of Expert i

at frame t is deﬁned as follows,

P

′t
M
Ei
′t
Ei + ξ

,

V

Rt

pair(Ei) =

(8)

where ξ is a small constant that avoids the inﬁnite pair-
evaluation score for a zero denominator. A larger Rt
pair(Ei)
means better consistency with other experts and higher sta-
bility of the target state prediction.

Expert Self-Evaluation. The trajectory smoothness de-
gree of each expert indicates the reliability of its track-
ing results to some extent. The Euclidean distance mea-
suring the shift between the previous bounding box Bt−1
Ei
is computed by Dt
and the current bounding box Bt
Ei =
Ei
kc(Bt−1
Ei )k, where c(Bt
Ei ) − c(Bt
Ei ) is the center of the
bounding box Bt
In frame t, the trajectory ﬂuctuation
.
Ei
degree of Expert i is given by Eq. (9).

St

Ei = exp

−

1
2σ2
Ei

(Dt

Ei )2

,

(cid:19)

Ei)].

(cid:18)
where σEi is the average length of the width W (Bt
height H(Bt
i.e., σEi = 1

Ei) and
Ei) of the bounding box provided by Expert i,
2 [W (Bt

Ei) + H(Bt

Similar to the pair-evaluation, we collect the previous
movement information to consider the temporal stability.
Finally, the self-evaluation score is deﬁned by Rt
self (Ei) =
1
self (Ei) means the better re-
N
liability of the tracking trajectory.

. The higher Rt

τ Wτ Sτ
Ei

P
Expert Selection. The ﬁnal robustness score Rt(Ei)
of the Expert i in t-th frame is a linear combination of it-
s pair-evaluation score Rt
pair(Ei) and self-evaluation score

Rt

self (Ei):
Rt(Ei) = µ · Rt

pair(Ei) + (1 − µ) · Rt

self (Ei),

(10)

where µ is the parameter to trade off the pair-evaluation and
self-evaluation weights. In each frame, as shown in Figure
4, the expert with the highest robustness score is selected
for the current tracking.

Sharing Strategy. In the tracking process, all the expert-
s are updated using the same selected samples and share the
same searching areas (ROI). This sharing strategy has two
advantages: (1) It alleviates the drift and tracking failures
of the weak experts effectively by sharing the reﬁned re-
sults for target position prediction and model update; (2) It
ensures the efﬁciency of our framework greatly. The main
computational burden of DCF lies in the feature extraction
process, especially the deep features. Although multiple ex-
perts (K = 7) are maintained in our method, the sharing s-
trategy makes the feature extraction process only be carried
out twice in each frame (instead of 7 × 2 = 14 times), one
for the searching patch (ROI) and the other for the train-
ing patch (used for model update), which is the same as the
standard DCF and independent of the expert number.

Due to the training sample sharing strategy used in our
framework, the selected tracking results should be carefully
checked to avoid the corruption of the experts. The peak-
to-sidelobe ratio (PSR) is widely used in DCF to quantify
the reliability of the tracked samples [4]. PSR is deﬁned as
P = (Rmax − m)/σ, where Rmax is the maximum conﬁ-
dence, m and σ are the mean and standard deviation of the
response, respectively. We compute the average PSR of d-
ifferent features: P t
3 (P t
L) to evaluate
the t-th tracking result, where P t
L denote the PSR
values of the High, Middle and Low-level response map-
s, respectively. However, in some cases when the unreliable

H + P t
H , P t

mean = 1

M + P t

M , P t

(9)

3.4. Adaptive Expert Update

4848

tracked results have similar features with the target, the PSR
value may fail to make a difference.

mean = 1
K

In our experiment, we observe that when occlusion or se-
vere deformation occurs, the average robustness score of the
K
i=1 Rt(Ei) decreases signiﬁcant-
experts Rt
ly, which can be regarded as the divergence between mul-
tiple experts when facing an unreliable sample. Through
considering the average PSR score as well as average ex-
pert robustness score together, a combined reliability score
St = P t
mean is proposed, which can discover unre-
liable samples more effectively and better evaluate the qual-
ity of the current tracking.

mean · Rt

P

Since the DCF learns both target and background in-
formation, it is not reasonable to simply discard unreli-
able samples. When the current reliability score St is
signiﬁcantly lower than the past average reliability score:
t
i=1 Si, the learning rate η in Eq. (4) is de-
S1:t
mean = 1
t
creased by Eq. (11).

P

η =

C
C · [St/(α · S1:t

mean)]β

(

if St > α · S1:t
otherwise,

mean,

(11)

where C is the learning rate of the standard DCF, α is the re-
liability threshold, and β is the power exponent of the power
function. The designed power function penalizes samples
with low reliability scores severely to protect the experts
from corruption.

4. Experiments

4.1. Experimental Setup

Implementation Details: In our experiment, we follow
the parameters in standard DCF method [16] to construct
experts. The parameter ρ in the weigh sequence W is set to
1.1. The weighting factor µ in Eq. (10) is set to 0.1. As for
the adaptive update, α and β in Eq. (11) are set to 0.7 and
3 for MCCT tracker, and 0.6 and 3 for MCCT-H tracker,
respectively. More details can be found in the source code1.
We use the same setting of parameters for all the exper-
iments. Our trackers are implemented in MATLAB 2017a
on a computer with an Intel I7-4790K 4.00GHz CPU and
16GB RAM. The MatConvNet toolbox [42] is used for ex-
tracting the deep features from VGG-19 [39]. Our MCCT
tracker runs at about 1.5 FPS on CPU. The GPU version of
MCCT tracker runs at about 8 FPS, which is carried out on a
GeForce GTX 1080Ti GPU. The speed of MCCT-H tracker
is about 45 FPS on CPU.

Evaluation Benchmarks and Metrics: Our method is
evaluated on three benchmark datasets by a no-reset evalua-
tion protocol: OTB-2013 [47], OTB-2015 [48] and Temple-
Color [27]. All the tracking methods are evaluated by the
overlap precision (OP) at an overlap threshold 0.5. For the

1https://github.com/594422814/MCCT

Table 2. Effectiveness study of the proposed MCCT-H (top) and
MCCT (bottom) trackers. The DP (@20px) and AUC scores are
reported on the OTB-2013 [47] and OTB-2015 [48] datasets (D-
P/AUC) corresponding to the OPE. Expert VII (E7) is the best
individual expert (best baseline) in MCCT and MCCT-H.

E7 in MCCT-H MCCT-H-NU MCCT-H-PSR MCCT-H (Final)
Best Baseline
78.4 / 60.8
79.2 / 60.5

Sec. 3.3 + PSR
83.5 / 64.6
82.7 / 63.1

Sec. 3.3 + 3.4
85.6 / 66.4
84.1 / 64.2

Sec. 3.3
83.5 / 64.4
83.3 / 63.3

E7 in MCCT
Best Baseline
89.7 / 69.0
87.1 / 66.5

MCCT-NU
Sec. 3.3
91.9 / 70.9
88.3 / 67.6

MCCT-PSR
Sec. 3.3 + PSR
90.9 / 70.5
88.1 / 67.4

MCCT (Final)
Sec. 3.3 + 3.4
92.8 / 71.4
91.4 / 69.5

OTB-2013
OTB-2015

OTB-2013
OTB-2015

better performance measure, we also use the average dis-
tance precision (DP) plots and overlap success plots over
these datasets using one-pass evaluation (OPE) [47, 48].

Finally, we evaluate the performance of the proposed
trackers on the VOT2016 [21, 22] benchmark, which con-
sists of 60 challenging sequences and provides an evalua-
tion toolkit which will re-initialize the tracker to the correct
position to continue tracking when tracking failure occurs.
In VOT2016, the expected average overlap (EAO) is used
for ranking trackers, which combines the raw values of per-
frame accuracies and failures in a principled manner [21].

4.2. Framework Effectiveness Study

To evaluate the effectiveness of the proposed methods,
we compare the MCCT and MCCT-H trackers with their
individual experts. By only fusing multiple experts, we ob-
tain the MCCT-NU (MCCT-H-NU) without adaptive update
(only Sec. 3.3). The MCCT-PSR (MCCTH-PSR) repre-
sents the tracker adopts the fusion method and uses PSR
measurement for adaptive update (Sec. 3.3 + PSR). MCCT
(MCCT-H) is our ﬁnal algorithm which combines the ex-
pert selection mechanism and the proposed adaptive update
strategy (Sec. 3.3 + Sec. 3.4).

From the results in Table 2, we can observe that our ﬁ-
nal methods outperform their corresponding best baseline
Expert VII (the best individual expert) obviously, which il-
lustrates the effectiveness of the proposed framework. On
the OTB-2013 and OTB-2015 datasets, our ﬁnal MCCT-H
tracker outperforms its best baseline with a gain of about
6% (from 4.9%-7.2%) in DP and 4% (from 3.7%-5.6%) in
AUC. For tracker MCCT, it should be noted that its Expert
VII has already achieved a sufﬁcient performance level, but
our framework still boosts its performance further (about
4% DP and 3% AUC on challenging OTB-2015). Besides,
for the baseline MCCT-NU with high performance, the tra-
ditional PSR method for reliability estimation [4] does not
have obvious impact, but our adaptive update strategy by
considering the divergence of multiple experts improves the
performance further.

4849

Table 3. A comparison of our methods using overlap precision (OP) (%) at an overlap threshold 0.5 with recent state-of-the-art trackers on
the OTB-2013 [47], OTB-2015 [48] and Temple-Color [27] datasets. The average speed (i.e., frames per second, FPS) is evaluated on the
OTB-2013 dataset. The speed labeled with ∗ represents the corresponding tracker utilizes GPU in the experiment. The ﬁrst and second
highest values are highlighted by red and blue.

When
Where
OTB-2013
OTB-2015
Temple-Color
Speed (FPS)

LCT
2015

KCF
2015

HCF
DSST MEEM SAMF
2014
2015
2014
2014
BMVC ECCV ECCVW TPAMI CVPR ICCV
72.9
65.4
56.9
∗
10.8

62.6
55.3
45.9
243.7

81.4
70.1
55.3
26.1

66.8
61.6
47.0
26.3

71.4
66.4
56.8
18.3

70.3
62.5
60.6
19.2

2015
ICCV
78.1
72.6
61.2
5.6

2015
ICCVW
77.8
76.4
65.2
<1

HDT
2016

SCT
2016
2016
CVPR CVPR CVPR ECCV
77.8
74.2
73.0
63.0
63.5
55.7
∗
86.0
41.5

ECO MCCT-H MCCT
Staple SiamFc SRDCFdecon MDNet C-COT ACFN CSR-DCF ADNet MCPF
2017
2017
2016
2017
CVPR CVPR CVPR
87.0
84.6
80.6
84.1
77.5
77.2
73.5
66.9
-
∗
∗
15.0
0.6
2.9

2016
2017
ECCV CVPR
71.8
82.0
68.1
81.5
-
70.2
15.0
0.3

2016
CVPR
79.8
75.8
65.6
2.9

2017
CVPR
74.0
68.3
57.7
14.2

2016
CVPR
89.6
82.9
-
1.0

72.0
65.0
56.8
∗
10.5

90.7
86.4
74.4
∗
7.8

81.8
78.9
67.4
44.8

76.0
70.8
62.5
67.2

∗

∗

∗

SRDCF DeepSRDCDF

4.3. State of the art Comparison

We evaluate the proposed approach with 20 recent state-
of-the-art trackers including DSST [9], MEEM [51], SAMF
[26], KCF [16], LCT [32], HCF [31], SRDCF [12], Deep-
SRDCF [11], SCT [5], HDT [38], Staple [2], SiamFc [3],
SRDCFdecon [10], MDNet [35], C-COT [13], ACFN [6],
CSR-DCF [30], ADNet [50], MCPF [56], ECO [8]. A com-
parison with these state-of-the-art trackers using OP met-
ric on OTB-2013, OTB-2015 and Temple-Color datasets is
shown in Table 3.

Evaluation on OTB-2013. On the OTB-2013 [47]
benchmark, our proposed MCCT method achieves the best
OP of 90.7% (Table 3) and the highest area-under-curve
(AUC) score of 71.4% (Figure 5). The MDNet method
also exhibits excellent results and performs slightly better
than ours on DP metric (Figure 5), which is mainly due to
the effectiveness of the multi-domain network trained using
various similar tracking videos. Compared with other DCF
based methods, the proposed MCCT method outperform-
s the recent ECO, C-COT and MCPF on various metrics,
including on metrics OP, DP and AUC.

Evaluation on OTB-2015. On the OTB-2015 [48]
dataset, our proposed MCCT tracker achieves the best re-
sults on OP and DP with scores of 86.4% and 91.4% (Table
3 and Figure 6), outperforming the second best method by
2.3% and 1.3%. Among all the trackers, only ECO slightly
outperforms our method on AUC score. Our MCCT track-
er performs much better than HCF, HDT and MCPF, which
are also based on correlation ﬁlters with multiple types of
features.

The proposed MCCT-H method which only incorpo-
rates simple hand-crafted features achieves impressive per-
formance and operates at about 45 FPS on a single CPU
(Table 3).
It signiﬁcantly outperforms the DCF based
trackers with the same features (e.g., SAMF, SRDCF and
CSR-DCF) in both performance and speed. Furthermore,
MCCT-H achieves comparable results with the recent com-
plex trackers using deep features (e.g., ADNet, MCPF and
DeepSRDCF) but runs much faster than them. Compared
with other frame-wise adaption trackers (e.g., STC [5] and
ACFN [6]) that use attention mechanism for tracker con-
struction, our framework maintains several experts in par-
allel for decision-level selection and achieves better perfor-

1

0.8

0.6

0.4

0.2

t

e
a
r
 
s
s
e
c
c
u
S

0

0

1

0.8

0.6

0.4

0.2

e
t
a
r
 
s
s
e
c
c
u
S

0

0

1

0.8

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0.6

0.4

0.2

0

0

1

0.8

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0.6

0.4

0.2

0

0

Precision plots of OPE

Success plots of OPE

MDNet [93.2]
MCCT(Ours) [92.8]
ECO [91.3]
MCPF [90.3]
HCF [89.2]
C-COT [89.1]
ADNet [87.4]
HDT [87.0]
MCCTH(Ours) [85.6]
SRDCFdecon [85.3]

MCCT(Ours) [71.4]
ECO [71.2]
MDNet [71.2]
MCPF [68.2]
C-COT [68.0]
MCCT-H(Ours) [66.4]
SRDCFdecon [65.6]
ADNet [64.9]
DeepSRDCF [64.3]
LCT [64.0]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 5. Precision and success plots on the OTB-2013 [47] dataset
with 50 videos. Only the top 10 trackers are displayed for clarity.
In the legend, the DP at a threshold of 20 pixels and area-under-
curve (AUC) are reported in the left and right ﬁgures, respectively.

Precision plots of OPE

Success plots of OPE

MCCT(Ours) [91.4]
ECO [90.1]
C-COT [89.4]
MDNet [89.1]
MCPF [86.6]
ADNet [84.4]
DeepSRDCF [84.2]
MCCT-H(Ours) [84.1]
HCF [83.9]
HDT [83.6]

ECO [69.6]
MCCT(Ours) [69.5]
C-COT [68.1]
MDNet [67.6]
DeepSRDCF [64.2]
MCCT-H(Ours) [64.2]
ADNet [63.6]
MCPF [63.5]
SRDCFdecon [63.4]
SRDCF [60.9]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 6. Precision and success plots on the OTB-2015 [48] dataset
with 100 videos. In the legend, the DP (@20px) and AUC scores
are reported in the left and right ﬁgures, respectively.

mance (Table 3, Figure 5 and 6).

Evaluation on Temple-Color. For further evaluation,
we compare the proposed MCCT-H and MCCT methods on
the Temple-Color dataset [27] with the trackers mentioned
in Sec. 4.3 excluding MDNet, ADNet and ACFN, which all
need many external tracking videos for network training. In
contrast, our approach is free of such necessity.

In Table 3 and Figure 7, the proposed MCCT-H track-
er still achieves outstanding performance amongst real-time
trackers and outperforms some complex non-realtime track-
ers. Our MCCT method achieves the OP, DP and AUC s-
cores of (74.4%, 79.7%, 59.6%), while ECO and C-COT
yield (73.5%, 79.7%, 60.7%) and (70.2%, 78.1%, 58.3%),
respectively. Overall, our MCCT tracker shows compara-
ble results compared to the recent performance leader E-
CO and outperforms other state-of-the-art methods (e.g., C-
COT) on various metrics.

4850

Precision plots of OPE

Success plots of OPE

Expected overlap scores for baseline

1

0.8

0.6

0.4

0.2

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0

0

MCCT(Ours) [79.7]
ECO [79.7]
C-COT [78.1]
MCPF [77.4]
DeepSRDCF [73.8]
SRDCFdecon [72.7]
MCCT-H(Ours) [72.0]
MEEM [69.9]
SiamFc [69.2]
HCF [68.9]

1

0.8

0.6

0.4

0.2

t

e
a
r
 
s
s
e
c
c
u
S

0

0

ECO [60.7]
MCCT(Ours) [59.6]
C-COT [58.3]
MCPF [55.3]
MCCT-H(Ours) [55.1]
DeepSRDCF [54.5]
SRDCFdecon [54.3]
Staple [50.9]
SiamFc [50.5]
MEEM [50.3]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 7. Precision and success plots on the Temple-Color [27]
dataset with 128 color videos. In the legend, the DP (@20px) and
AUC scores are reported in the left and right ﬁgures, respectively.

Evaluation on VOT2016. Figure 8 shows the rank-
ing results in terms of expected average overlap (EAO) in
VOT2016 [21], from which we can observe that our MCC-
T tracker outperforms the top performer (C-COT [13]) by a
considerable margin. For presentation clarity, we only show
some top ranked and baseline trackers for comparison. For
more information, please refer to [21].

In Table 4, we list the detailed results of our approach-
es and the top ranked methods in VOT2016 (e.g., C-COT
[13], TCNN [34] and SSAT). Besides, two recently pro-
posed methods: CSR-DCF [30] and ECO [8] are also put
into comparison which do not participate in VOT2016. A-
mong all the compared methods, the proposed MCCT track-
er demonstrates advances in both accuracy and robustness.
As a result, our MCCT tracker provides the best EAO score
of 0.393, achieving a relative gain of 18.7% compared to
the VOT2016 top performer C-COT.

Discussion: (1) About Performance. It should be noted
that the ECO, C-COT and DeepSRDCF methods all take S-
RDCF as baseline, which can alleviate boundary effects ef-
fectively and performs much better than the standard DCF
[4, 16]. Besides, ECO and C-COT adopt the novel contin-
uous operator to better fuse the feature maps. However, all
the experts in our MCCT-H and MCCT methods just take
the simple DCF as baseline and all the strategies mentioned
above and other novel techniques [14, 33] can also be inte-
grated into our framework to further boost the performance.
(2) About Efﬁciency. Our tracker achieves almost the same
speed with expert VII because the same number of DCFs
and features are utilized (i.e., sharing strategy), but achieves
much better performance than it. Our general framework
provides a promising alternative for the multi-feature based
DCF trackers with ignorable impact on efﬁciency. Further-
more, we believe that other tracking algorithms [53, 36, 55]
with multiple types of features can also beneﬁt from our
multi-cue analysis framework.

5. Conclusion

In this paper, we propose a multi-cue analysis frame-
work for robust visual tracking, which considers not only

p
a
l
r
e
v
o

 

d
e
t
c
e
p
x
e

 

e
g
a
r
e
v
A

0.5

0.4

0.3

0.2

0.1

0

13

10

4

1

7
Order

TCNN
SRDCF

MCCT
EBT

MCCT-H
CCOT
DeepSRDCF MDNet

SSAT
HCF

MLDF
DSST

Staple
KCF

Figure 8. Expected Average Overlap (EAO) graph with trackers
ranked from right to left evaluated on VOT2016 [21]. Our pro-
posed MCCT tracker outperforms the top performer (C-COT [13])
by a considerable margin.

Table 4. The accuracy, robustness (failure rate) and EAO of state-
of-the-art methods on the VOT2016 [21]. The proposed MCCT
tracker achieves superior results compared to the top ranked meth-
ods in the challenge and recently proposed state-of-the-art algo-
rithms (ECO [8] and CSR-DCF [30]). The ﬁrst and second highest
values are highlighted by red and blue.

MCCT-H MLDF SSAT TCNN C-COT CSR-DCF ECO MCCT

Accuracy
Failure Rate
EAO

0.57
1.24
0.305

0.57
0.54
0.48
0.96
1.04
0.83
0.311 0.321 0.325

0.52
0.85
0.331

0.51
0.85
0.338

0.58
0.54
0.73
0.72
0.374 0.393

feature-level fusion but also decision-level fusion to fully
explore the strength of multiple features. Our framework
maintains multiple experts to track the target via different
views and selects the reliable outputs to reﬁne the tracking
results. Moreover, the proposed method evaluates the unre-
liable samples through considering the divergence of mul-
tiple experts and updates them adaptively. Through exten-
sive experiments on several challenging datasets, we show
that after adopting our simple yet effective multi-cue analy-
sis framework, without sophisticated models, only standard
Discriminative Correlation Filter (DCF) with deep or hand-
crafted features is able to perform favorably against state-
of-the-art methods in both accuracy and efﬁciency.

Acknowledgement. This work was supported in part
to Dr. Houqiang Li by 973 Pro-gram under contract No.
2015CB351803 and NSFC under contract No. 61390514,
in part to Dr. Wengang Zhou by NSFC under contract No.
61472378 and No. 61632019, the Fundamental Research
Funds for the Central Universities, and Young Elite Scien-
tists Sponsorship Program By CAST (2016QNRC001), and
in part to Dr. Qi Tian by ARO grant W911NF-15-1-0290
and Faculty Research Gift Awards by NEC Laboratories of
America and Blippar. This work was supported in part by
National Science Foundation of China (NSFC) 61429201.

4851

References

[1] C. Bailer, A. Pagani, and D. Stricker. A superior tracking ap-
proach: Building a strong tracker through fusion. In ECCV,
2014. 2

[2] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and
P. Torr. Staple: Complementary learners for real-time track-
ing. In CVPR, 2016. 2, 3, 7

[3] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and
P. H. Torr. Fully-convolutional siamese networks for object
tracking. In ECCV, 2016. 7

[4] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
CVPR, 2010. 1, 2, 3, 5, 6, 8

[5] J. Choi, H. Jin Chang, J. Jeong, Y. Demiris, and J. Y-
oung Choi. Visual tracking using attention-modulated dis-
integration and integration. In CVPR, 2016. 3, 7

[6] J. Choi, H. Jin Chang, S. Yun, T. Fischer, Y. Demiris, and
J. Young Choi. Attentional correlation ﬁlter network for
adaptive visual tracking. In CVPR, 2017. 7

[7] N. Dalal and B. Triggs. Histograms of oriented gradients for

human detection. In CVPR, 2005. 2, 3, 4

[8] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg.
Eco: Efﬁcient convolution operators for tracking. In CVPR,
2017. 2, 7, 8

[9] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accurate
scale estimation for robust visual tracking. In BMVC, 2014.
1, 3, 7

[10] M. Danelljan, G. H¨ager, F. S. Khan, and M. Felsberg. Adap-
tive decontamination of the training set: A uniﬁed formula-
tion for discriminative visual tracking. In CVPR, 2016. 2,
7

[11] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Fels-
berg. Convolutional features for correlation ﬁlter based vi-
sual tracking. In ICCV Workshop, 2015. 1, 2, 7

[12] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg.
Learning spatially regularized correlation ﬁlters for visual
tracking. In ICCV, 2015. 1, 2, 7

[13] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. In ECCV, 2016. 1, 2, 7, 8

[14] H. K. Galoogahi, A. Fagg, and S. Lucey.

Learning
background-aware correlation ﬁlters for visual tracking. In
ICCV, 2017. 1, 8

[15] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In ECCV, 2012. 2

[16] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
speed tracking with kernelized correlation ﬁlters. TPAMI,
37(3):583–596, 2015. 1, 2, 3, 6, 7, 8

[17] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychol-
In CVPR, 2015.
ogy inspired approach to object tracking.
1

[18] X. Jia, H. Lu, and M.-H. Yang. Visual tracking via adaptive
structural local sparse appearance model. In CVPR, 2012. 2
[19] O. Khalid, J. C. SanMiguel, and A. Cavallaro. Multi-tracker

partition fusion. TCSVT, 27(7):1527–1539, 2017. 2

[20] H. Kiani Galoogahi, T. Sim, and S. Lucey. Correlation ﬁlters

with limited boundaries. In CVPR, 2015. 1

[21] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, L. Cehovin,
G. Fern´andez, T. Vojir, Hager, and et al. The visual objec-
In ECCV Workshop,
t tracking vot2016 challenge results.
2016. 2, 6, 8

[22] M. Kristan, J. Matas, A. Leonardis, T. Voj´ıˇr, R. Pﬂugfelder,
G. Fernandez, G. Nebehay, F. Porikli, and L. ˇCehovin. A
novel performance evaluation methodology for single-target
trackers. TPAMI, 38(11):2137–2155, 2016. 6
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1

Imagenet
In

[24] D. Y. Lee, J. Y. Sim, and C. S. Kim. Multihypothesis tra-
jectory analysis for robust visual tracking. In CVPR, 2015.
2

[25] J. Li, C. Deng, R. Y. D. Xu, D. Tao, and B. Zhao. Robust
object tracking with discrete graph based multiple experts.
TIP, 26(6):2736–2750, 2017. 2

[26] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. In ECCV Workshop, 2014.
1, 7

[27] P. Liang, E. Blasch, and H. Ling. Encoding color informa-
tion for visual tracking: algorithms and benchmark. TIP,
24(12):5630–5644, 2015. 2, 6, 7, 8

[28] S. Liu, T. Zhang, X. Cao, and C. Xu. Structural correlation

ﬁlter for robust visual tracking. In CVPR, 2016. 1

[29] T. Liu, G. Wang, and Q. Yang. Real-time part-based visual

tracking via adaptive correlation ﬁlters. In CVPR, 2015. 1

[30] A. Lukezic, T. Vojir, L. Cehovin Zajc, J. Matas, and M. Kris-
tan. Discriminative correlation ﬁlter with channel and spatial
reliability. In CVPR, 2017. 1, 2, 3, 7, 8

[31] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In ICCV, 2015. 1,
3, 4, 7

[32] C. Ma, X. Yang, C. Zhang, and M.-H. Yang. Long-term

correlation tracking. In CVPR, 2015. 1, 7

[33] M. Mueller, N. Smith, and B. Ghanem. Context-aware cor-

relation ﬁlter tracking. In CVPR, 2017. 1, 8

[34] H. Nam, M. Baek, and B. Han. Modeling and propagating
cnns in a tree structure for visual tracking. arXiv preprint
arXiv:1608.07242, 2016. 8

[35] H. Nam and B. Han. Learning multi-domain convolutional
neural networks for visual tracking. In CVPR, 2016. 1, 7
[36] J. Ning, J. Yang, S. Jiang, L. Zhang, and M.-H. Yang. Object
tracking via dual linear structured svm and explicit feature
map. In CVPR, 2016. 8

[37] H. Possegger, T. Mauthner, and H. Bischof.

In defense of

color-based model-free tracking. In CVPR, 2015. 3

[38] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-H.
Yang. Hedged deep tracking. In CVPR, 2016. 1, 2, 7
[39] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 1, 3, 6

[40] Y. Song, C. Ma, L. Gong, J. Zhang, R. Lau, and M.-H. Yang.
Crest: Convolutional residual learning for visual tracking. In
ICCV, 2017. 1

4852

[41] J. Valmadre, L. Bertinetto, J. F. Henriques, A. Vedaldi, and
P. H. Torr. End-to-end representation learning for correlation
ﬁlter based tracking. In CVPR, 2017. 1

[42] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural

networks for matlab. In ACM MM, 2014. 6

[43] N. Wang, J. Shi, D. Y. Yeung, and J. Jia. Understanding and
diagnosing visual tracking systems. In ICCV, 2015. 2, 4
[44] N. Wang and D. Y. Yeung. Ensemble-based tracking: Aggre-
gating crowdsourced structured time series data. In ICML,
2014. 2

[45] N. Wang, W. Zhou, and H. Li. Reliable re-detection for long-

term tracking. TCSVT, 2018. 1

[46] J. V. D. Weijer, C. Schmid, J. Verbeek, and D. Larlus.
TIP,
Learning color names for real-world applications.
18(7):1512–1523, 2009. 2, 4

[47] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A

benchmark. In CVPR, 2013. 2, 6, 7

[48] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.

TPAMI, 37(9):1834–1848, 2015. 1, 2, 6, 7

[49] A. Yilmaz, O. Javed, and M. Shah. Object tracking: A sur-

vey. ACM Computing Surveys, 38(4):13, 2006. 1

[50] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Young Choi. Action-
decision networks for visual tracking with deep reinforce-
ment learning. In CVPR, 2017. 7

[51] J. Zhang, S. Ma, and S. Sclaroff. Meem: robust tracking
via multiple experts using entropy minimization. In ECCV,
2014. 2, 7

[52] K. Zhang, L. Zhang, and M.-H. Yang. Real-time compres-

sive tracking. In ECCV, 2012. 1

[53] K. Zhang, L. Zhang, and M.-H. Yang. Real-time objec-
t tracking via online discriminative feature selection. TIP,
22(12):4664–4677, 2013. 8

[54] K. Zhang, L. Zhang, M.-H. Yang, and D. Zhang. Fast vi-
sual tracking via dense spatio-temporal context learning. In
ECCV, 2013. 2

[55] T. Zhang, A. Bibi, and B. Ghanem.

In defense of sparse

tracking: Circulant sparse tracker. In CVPR, 2016. 8
[56] T. Zhang, C. Xu, and M.-H. Yang. Multi-task correlation
particle ﬁlter for robust object tracking. In CVPR, 2017. 1, 7
[57] W. Zhong, H. Lu, and M.-H. Yang. Robust object tracking

via sparsity-based collaborative model. In CVPR, 2012. 2

4853

Multi-Cue Correlation Filters for Robust Visual Tracking

Ning Wang1, Wengang Zhou1, Qi Tian2, Richang Hong3, Meng Wang3, Houqiang Li1
1CAS Key Laboratory of GIPAS, University of Science and Technology of China
2University of Texas at San Antonio 3HeFei University of Technology
wn6149@mail.ustc.edu.cn, {zhwg,lihq}@ustc.edu.cn, qi.tian@utsa.edu, {hongrc,wangmeng}@hfut.edu.cn

Abstract

In recent years, many tracking algorithms achieve im-
pressive performance via fusing multiple types of features,
however, most of them fail to fully explore the context among
the adopted multiple features and the strength of them. In
this paper, we propose an efﬁcient multi-cue analysis frame-
work for robust visual tracking. By combining differen-
t types of features, our approach constructs multiple ex-
perts through Discriminative Correlation Filter (DCF) and
each of them tracks the target independently. With the pro-
posed robustness evaluation strategy, the suitable expert is
selected for tracking in each frame. Furthermore, the di-
vergence of multiple experts reveals the reliability of the
current tracking, which is quantiﬁed to update the experts
adaptively to keep them from corruption.

Through the proposed multi-cue analysis, our tracker
with standard DCF and deep features achieves outstand-
ing results on several challenging benchmarks: OTB-2013,
OTB-2015, Temple-Color and VOT 2016. On the other
hand, when evaluated with only simple hand-crafted fea-
tures, our method demonstrates comparable performance
amongst complex non-realtime trackers, but exhibits much
better efﬁciency, with a speed of 45 FPS on a CPU.

1. Introduction

Visual tracking is a fundamental task in computer vision
with a wide range of applications. It is challenging since
only the initial state of the target is available. Although
substantial progress has been made in the past decades
[52, 16, 35], there still remain many challenges [49].

In recent years, Discriminative Correlation Filter (DCF)
based tracking methods [4, 16] have gained much atten-
tion thanks to their impressive performance as well as high
speed. In DCF based trackers, the ﬁlter is trained through
minimizing a least-squares loss for all circular shifts of a
training sample. Since the correlation operation can be cal-
culated in Fourier domain, DCF shows the advantage of
Its performance is further
high computational efﬁciency.

(a) Biker

(b) Box

(c) Diving

(d) Shaking

(e) Singer2

(f) MotorRolling

MCCT

C-COT

DeepSRDCF

SRDCF

Figure 1. Comparison of the proposed algorithm (MCCT) with
the state-of-the-art trackers: SRDCF [12], DeepSRDCF [11] and
C-COT [13]. These trackers adopt various types of features and
perform differently in challenging scenes. Our method maintain-
s multiple cues for tracking and performs favorably against these
trackers. All the videos are from OTB-2015 [48].

enhanced by using multi-dimensional features [16, 26, 31],
part-based strategies [29, 28], adaptive scale [9, 26], long-
term framework [32, 17, 45], end-to-end learning [41, 40]
and improved ﬁlter training methods [20, 30, 33, 14]. Spe-
cially, the combination of DCF and features from deep con-
volutional neural networks (CNNs) [23, 39] has demonstrat-
ed state-of-the-art results. Recently, Ma et al. [31] propose
a HCF algorithm, which constructs multiple DCFs on low,
middle and high-level features to capture both spatial detail-
s and semantics. It predicts the target location using multi-
level DCF response maps in a coarse-to-ﬁne fashion.

Although feature-level fusion methods [26, 31, 38] have
been widely used or extended to boost the performance,
there still leaves room for improvement.
In HCF [31] or
other methods [56, 38] that follow such a fusion strategy,
the initial weight of high-level features is usually high such
that semantic features play the dominant role in general. It
is reasonable because of better effectiveness of high-level
features compared to shallow features. However, due to the
occasional misguidance of the semantic information, a tran-
sient drift or wrong prediction may be ampliﬁed by the in-
adequate online update. Therefore, the feature-level fusion

4844

approach sometimes still fails to fully explore the relation-
ship of multiple features. Furthermore, it is quite difﬁcult to
handle various challenging variations using a single model,
and relying on a certain feature-level fusion strategy limits
the model diversity to some extent.

To better illustrate the issues mentioned above, our pro-
posed algorithm is compared with three DCF based method-
s. In Figure 1, SRDCF [12], DeepSRDCF [11] and C-COT
[13] adopt different types of features, but none of them is
able to handle various challenging factors, even for the C-
COT algorithm with multiple features as well as the nov-
el continuous convolution operators. Since it is quite difﬁ-
cult to design a satisfying feature-level fusion method that
suits various challenging scenes, it is intuitive to design an
adaptive switch mechanism to achieve better performance,
which can ﬂexibly switch to the reliable tracker depend-
ing on what kind of challenging factors it is expert at han-
dling. In other words, the performance of a single tracker
can sometimes be unstable but the decision-level fusion of
the outputs from multiple trackers can enhance the robust-
ness effectively.

In this paper, a novel Multi-Cue Correlation ﬁlter based
Tracker (MCCT) is proposed. Different from the previous
DCF based methods [4, 16] that usually lack the diversity
of target appearance representations, our method maintains
multiple experts to learn the appearance models from dif-
ferent views. Here, a certain combination of features con-
structs an individual expert and provides a reliable cue (pre-
dicted target state) for tracking. The main contributions
(1) We pro-
of our work can be summarized as follows.
pose an algorithm that maintains multiple cues for track-
ing. Through checking the robustness scores of multiple
experts carefully, our method reﬁnes the tracking results
by choosing the reliable expert for tracking in each frame.
(2) By considering the divergence of multiple experts, we
present an adaptive update strategy which can discriminate
the unreliable samples (e.g., occlusion or sever deforma-
tion) effectively and alleviate the contamination of train-
(3) We implement two versions of the pro-
ing samples.
posed method to validate the generality of the framework.
The MCCT tracker with deep features demonstrates out-
standing performance on several challenging benchmarks
[47, 48, 27, 21]. The MCCT-H tracker with only two s-
tandard Hand-crafted features (HOG [7] and ColorNames
[46]) achieves comparable performance with many complex
deep-model based trackers, but operates about 45 frames
per second on a single CPU, exceeding most competitive
trackers by several times.

2. Related Work

In this section, we discuss two categories of trackers
closely related to our algorithm: correlation tracking and
ensemble tracking.

Correlation Tracking. Since Bolme et al.

[4] pro-
pose a tracker using minimum output sum of squared er-
ror (MOSSE) ﬁlter, the correlation ﬁlters have been widely
studied in visual tracking. Heriques et al. exploit the cir-
culant structure of the training patches [15] and propose to
train correlation ﬁlter in a kernel space with HOG features
[16]. Zhang et al. [54] incorporate context information in-
to ﬁlter learning. The SRDCF tracker [12] alleviates the
boundary effects by penalizing correlation ﬁlter coefﬁcients
depending on spatial location, and is enhanced by reduc-
ing the inﬂuence of corrupted samples [10]. Qi et al. [38]
propose a HDT algorithm that fuses several DCFs through
adaptive hedged method. Luca et al.
[2] propose a Sta-
ple tracker which combines DCF and color histogram based
model while running in excess of real-time. The CSR-DCF
algorithm [30] constructs DCF with channel and spatial reli-
ability. The recent C-COT [13] adopts a continuous-domain
formulation of the DCF, leading to the top performance on
several tracking benchmarks. The enhanced version of C-
COT is ECO [8], which improves both speed and perfor-
mance by introducing several efﬁcient strategies.

Different from the DCF based methods mentioned
above, our algorithm considers not only feature-level fusion
but also decision-level fusion to better explore the relation-
ship of multiple features, and adaptively selects the expert
that is suitable for a particular tracking task.

Ensemble Tracking. To enhance the tracking robust-
ness and obtain reliable results, the ensemble approach
treats the trackers as black boxes and takes only the bound-
ing boxes returned by them as input [43]. In [1], a dynam-
ic programming based trajectory optimization approach is
proposed to build a strong tracker. In [44], Wang et al. pro-
pose a factorial hidden Markov model for ensemble-based
tracking. The MEEM algorithm [51] exploits the relation-
ship between the current tracker and its historical snapshots
using entropy minimization, and Li et al.
[25] extend it
by introducing a discrete graph optimization framework. In
[19], a partition fusion framework is proposed to cluster re-
liable trackers for target state prediction.

Although promising results are obtained through fusing
multiple trackers, there still leave some limitations: (1) the
overall speed is limited by the lowest tracker in the en-
semble (e.g., sparse representation based methods [57, 18],
about 1 FPS), which restricts the real-time application. Spe-
cially, the fusion methods [19, 24] by analyzing the for-
ward and backward trajectories require each tracker to run
at least twice; (2) the trackers are just regarded as inde-
pendent black boxes and their fusion result in each frame
does not feedback to the trackers [1, 44, 19], which fails to
make full use of the reliable fusion outputs; (3) if the fu-
sion tracker number increases, the dynamic programming
based fusion methods [25, 1] still bring obvious computa-
tional burden (e.g., O(T N 2) for T frames and N trackers).

4845

Flowchart

Input 
Frame

ROI

Feature  Pool

Expert  Pool  (DCF)

Expert  Selection

Feature 
Extration

Multi-Expert
Prediction

...

Adaptive
Model
Update

Figure 2. A systematic ﬂowchart of the proposed tracking algorithm. First, different features of ROI are extracted and combined to train
multiple experts following DCF framework (Sec. 3.1 and 3.2). Then, each expert gives an individually predicted cue and the most reliable
expert is selected for the current tracking (Sec. 3.3). Finally, adaptive update helps keep the experts from corruption (Sec. 3.4).

Different from the above methods:

(1) Our approach
the experts in the DCF framework, and
constructs all
through the proposed ROI and training sample sharing s-
trategy (Sec. 3.3), the efﬁciency is greatly ensured; (2) the
reﬁned tracking results are fed back to the experts to boost
them further; (3) through a simple but effective robustness
evaluation strategy, our method selects the reliable expert
for tracking with a complexity of only O(T N ) for T frames
and N experts.

3. Method

Our algorithm is composed of several experts which pro-
vide different levels of cues for tracking. A preview of DCF
is introduced in Sec. 3.1. The component of the expert pool
is described in Sec. 3.2. How to switch to the suitable ex-
pert in each frame is elaborated in Sec. 3.3. Finally, Sec.
3.4 introduces the adaptive update. The framework of our
method is depicted in Figure 2.

3.1. Preview of DCF

A typical tracker based on DCF [4, 16] is trained using
an image patch x of size M × N , which is centered around
the target. All the circular shifts of the patch x(m, n) ∈
{0, 1, ...M − 1} × {0, 1, ...N − 1} are generated as training
samples with Gaussian function label y(m, n) in terms of
the shifted distance. The ﬁlter w is trained by minimizing
the following regression error:

min
w

kXw − yk2

2 + λkwk2
2,

where λ is a regularization parameter (λ ≥ 0) and X is the
data matrix by concatenating all the circular shifts. The ﬁl-
ter solution on the d-th (d ∈ {1, · · · , D}) channel is deﬁned
by

ˆw∗

d =

ˆy ⊙ ˆx∗
d
i ⊙ ˆxi + λ

,

D

i=1 ˆx∗
where ⊙ is the element-wise product, the hat symbol de-
notes the Discrete Fourier Transform (DFT) of a vector
(e.g., ˆx = F(x)) and ˆx∗ is the complex-conjugate of ˆx.
In the next frame, a Region of Interest (ROI) is cropped out
for tracking the target (e.g., a patch z with the same size of

P

(1)

(2)

x). The response map R of z is calculated in Eq. (3) and
the location of the target is identiﬁed by searching for the
maximum value of R.

R = F

−1(

ˆwd ⊙ ˆz∗

d).

(3)

D

d=1
X

To avoid the boundary effects during learning, we apply
Hann window to the signals [16]. Besides, inspired by [30,
5], color information is applied to the training sample in a
= X ⊙ C,
simple way to enhance its spatial reliability: X
where X represents the data matrix and C is the color mask
obtained by computing the histogram-based per-pixel score
map [37, 2] of ROI. The online update of the numerator ˆAd
and the denominator ˆBd of the ﬁlter ˆw∗

d is as follows,

′

ˆAt

d = (1 − η) ˆAt−1

d + η ˆy ⊙ ˆx∗t
d ,

ˆBt

d = (1 − η) ˆBt−1

d + η

ˆx∗t
i ⊙ ˆxt
i,

(4)

D

i=1
X

ˆw∗t

d =

ˆAt
d
d + λ

,

ˆBt

where η is the learning rate and t is the index of the current
frame. As for the target scale estimation, we follow the
DSST tracker [9].

3.2. Feature Pool and Expert Pool

A variety of features can be adopted in DCF and differ-
ent features have their own strength. Hand-crafted features
are typically used to capture low-level details while deep
features are semantic-aware. As discussed in HCF [31], the
DCF constructed by each single layer of VGG-19 [39] is
not accurate enough, so HCF performs coarse-to-ﬁne search
on several DCF response maps from different layers.
In
our MCCT tracker, HOG [7] is used as low-level features.
Then, we remove the fully-connected layers and extract the
outputs of the conv4-4 and conv5-4 convolutional layers of
VGG-19 as middle-level and high-level features, respective-
ly. Thus, the feature pool consists of three types of features:
{Low, M iddle, High} and they are optionally combined
into C 1
3 = 7 experts. As for the coarse-to-ﬁne

3 + C 3

3 + C 2

4846

Table 1. The component of the expert ensemble. Our MCCT
and MCCT-H trackers both consist of seven experts. Each expert
adopts different features and tracks the target via a different view.

Expert Pool
Expert I
Expert II
Expert III
Expert IV
Expert V
Expert VI
Expert VII

Tracker MCCT
Feature Type
Low (HOG)
Middle (conv4-4 of VGG-19)
High (conv5-4 of VGG-19)
Middle, Low
High, Low
High, Middle
High, Middle, Low

Tracker MCCT-H
Feature Type
HOG1
HOG2
ColorNames
HOG1, ColorNames
HOG2, ColorNames
HOG1, HOG2
HOG1, HOG2, ColorNames

Expert I

Expert II

Expert III

Expert IV

…

…

…

…

…

…

…

…

weighting parameters of different level DCF response maps,
we follow the settings in HCF [31]. Although some experts
(e.g., Expert I, II and III) with single type of features may be
less robust compared to Expert VII, they provide the diver-
sity of tracking results, which is crucial in ensemble-based
tracking [43].

Our fast variant, the MCCT-H tracker only utilizes stan-
dard hand-crafted features (HOG [7] and ColorNames [46])
to construct experts. Since HOG and ColorNames are both
low-level features, we do not conduct coarse-to-ﬁne fusion
and just simply concatenate them to construct DCF. Col-
orNames provides an 11 dimensional color representation
and HOG feature is 31-dim. To obtain more experts, we
take the average grey value over all pixels in a patch as a
1-dim feature and concatenate it to the HOG feature into a
32-dim vector, which is further evenly decomposed into two
16-dim features, denoted as HOG1 and HOG2, respective-
ly. The detailed information of the experts in MCCT and
MCCT-H is shown in Table 1.

3.3. Multi Cue Correlation Tracker

The tracking process of our proposed framework can be
illustrated by Figure 3, where the multiple experts track the
target in parallel and the nodes denote the generated cues
(bounding boxes) of the experts. In each frame, the evalua-
tion between different hypothesis nodes reveals the consis-
tency degree between the experts, which is denoted as pair-
wise evaluation. Besides, each expert has its own trajec-
tory continuity and smoothness. Thus, given a hypothesis
node, its robustness degree can be evaluated by both pair-
evaluation and self-evaluation. After evaluating the overall
reliability of each node, the expert with the highest robust-
ness score is selected and its tracking result is taken for the
current frame.

In the following, we elaborate the formulation of pair-

evaluation, self-evaluation and expert selection strategy.

Expert Pair-Evaluation. Most experts in the ensemble
are capable of tracking the target stably, and the cues pro-
duced by a good expert should be consistent with the cues
from other experts as much as possible.

Let E1, · · · , E7 denote Expert I, · · · , Expert VII, respec-
tively. In the t-th frame, the bounding box of Expert i is

Frame t

Frame t+1

Frame t+2

Expert Pair-Evaluation

Expert Self-Evaluation

Figure 3. Graph illustration of the multi-expert framework. The
node in the graph denotes the predicted bounding box. The ro-
bustness score of each expert is evaluated by both pair-evaluation
and self-evaluation. For clarity, only four experts are displayed.

denoted as Bt
. Through regarding all the experts as black
Ei
boxes, the bounding box Bt
only contains the target state
Ei
(e.g., location and scale) without any context information,
which reduces the computational and memory burden ef-
fectively.

First, we compute overlap ratios of the bounding boxes
(Ei,Ej ) of Expert

from different experts. The overlap ratio Ot
i and Expert j at frame t is calculated as follows,

Ot

(Ei,Ej ) =

Area(Bt
Ei
Area(Bt
Ei

Bt
Bt

Ej )
Ej )

.

T

(5)

To reduce the gap between low and high overlap ratios, we
adopt a nonlinear gaussian function to Ot
(Ei,Ej ) as follows,
which gathers the expert scores.

S

O

′t
(Ei,Ej ) = exp

−

1 − Ot

(Ei,Ej )

2

.

(6)

(cid:17)

(cid:16)

K
j=1 O

The mean value M t

(cid:18)
(cid:19)
′t
Ei = 1
(Ei,Ej ) of overlap ra-
K
tios reveals the trajectory consistency between expert i and
others, where K denotes the number of experts (in our ex-
periment, K = 7). In general, the pair-wise comparison s-
cores between two experts should be temporal stable. Thus,
the ﬂuctuation extent of overlap ratios in a short period ∆t
(e.g., 5 frames) reveals the stability of overlap evaluation
between Ei and other experts, which is given by Eq. (7).

P

V t
Ei =

K

1
K

v
u
j=1 (cid:16)
X
u
t
′t−∆t+1:t
(Ei,Ej ) = 1
∆t

where O
τ ∈ [t − ∆t + 1, t].

O

′t
(Ei,Ej ) − O

′t−∆t+1:t
(Ei,Ej )

(7)

2

,

(cid:17)

τ O

′τ
(Ei,Ej ) and the time index

P

Then, to avoid performance ﬂuctuation of the experts, we
further take temporal stability into account and introduce an
increasing sequence W = {ρ0, ρ1, · · · , ρ∆t−1}, (ρ > 1) to
give more weight to the recent scores. After considering the
temporal context, the average weighted mean and standard

4847

#8

Selected Cue:
Expert VII

#60

Selected Cue:
Expert V

#75

Selected Cue:
Expert II

#135

Selected Cue:
Expert IV

Expert I

Expert V

Expert II

Expert VI

Expert III

Expert VII

Expert IV

Figure 4. Expert selection process of our MCCT tracker in MotorRolling sequence. The bottom plots show the normalized robustness
scores of different experts. In top ﬁgures, the expert with the highest robustness score is selected for tracking. All the experts share the
same ROI (the largest yellow box) and are updated using the same selected results.

′t
τ Wτ M τ
Ei = 1
variance are calculated through: M
Ei
N
and V
, respectively, where Wτ denotes
the (τ − t + ∆t)-th element in sequence W and N is the
normalization factor deﬁned by N =

′t
Ei = 1
N

τ Wτ V τ
Ei

P

P

τ Wτ .

Finally, the pair-wise expert robustness score of Expert i

at frame t is deﬁned as follows,

P

′t
M
Ei
′t
Ei + ξ

,

V

Rt

pair(Ei) =

(8)

where ξ is a small constant that avoids the inﬁnite pair-
evaluation score for a zero denominator. A larger Rt
pair(Ei)
means better consistency with other experts and higher sta-
bility of the target state prediction.

Expert Self-Evaluation. The trajectory smoothness de-
gree of each expert indicates the reliability of its track-
ing results to some extent. The Euclidean distance mea-
suring the shift between the previous bounding box Bt−1
Ei
is computed by Dt
and the current bounding box Bt
Ei =
Ei
kc(Bt−1
Ei )k, where c(Bt
Ei ) − c(Bt
Ei ) is the center of the
bounding box Bt
In frame t, the trajectory ﬂuctuation
.
Ei
degree of Expert i is given by Eq. (9).

St

Ei = exp

−

1
2σ2
Ei

(Dt

Ei )2

,

(cid:19)

Ei)].

(cid:18)
where σEi is the average length of the width W (Bt
height H(Bt
i.e., σEi = 1

Ei) and
Ei) of the bounding box provided by Expert i,
2 [W (Bt

Ei) + H(Bt

Similar to the pair-evaluation, we collect the previous
movement information to consider the temporal stability.
Finally, the self-evaluation score is deﬁned by Rt
self (Ei) =
1
self (Ei) means the better re-
N
liability of the tracking trajectory.

. The higher Rt

τ Wτ Sτ
Ei

P
Expert Selection. The ﬁnal robustness score Rt(Ei)
of the Expert i in t-th frame is a linear combination of it-
s pair-evaluation score Rt
pair(Ei) and self-evaluation score

Rt

self (Ei):
Rt(Ei) = µ · Rt

pair(Ei) + (1 − µ) · Rt

self (Ei),

(10)

where µ is the parameter to trade off the pair-evaluation and
self-evaluation weights. In each frame, as shown in Figure
4, the expert with the highest robustness score is selected
for the current tracking.

Sharing Strategy. In the tracking process, all the expert-
s are updated using the same selected samples and share the
same searching areas (ROI). This sharing strategy has two
advantages: (1) It alleviates the drift and tracking failures
of the weak experts effectively by sharing the reﬁned re-
sults for target position prediction and model update; (2) It
ensures the efﬁciency of our framework greatly. The main
computational burden of DCF lies in the feature extraction
process, especially the deep features. Although multiple ex-
perts (K = 7) are maintained in our method, the sharing s-
trategy makes the feature extraction process only be carried
out twice in each frame (instead of 7 × 2 = 14 times), one
for the searching patch (ROI) and the other for the train-
ing patch (used for model update), which is the same as the
standard DCF and independent of the expert number.

Due to the training sample sharing strategy used in our
framework, the selected tracking results should be carefully
checked to avoid the corruption of the experts. The peak-
to-sidelobe ratio (PSR) is widely used in DCF to quantify
the reliability of the tracked samples [4]. PSR is deﬁned as
P = (Rmax − m)/σ, where Rmax is the maximum conﬁ-
dence, m and σ are the mean and standard deviation of the
response, respectively. We compute the average PSR of d-
ifferent features: P t
3 (P t
L) to evaluate
the t-th tracking result, where P t
L denote the PSR
values of the High, Middle and Low-level response map-
s, respectively. However, in some cases when the unreliable

H + P t
H , P t

mean = 1

M + P t

M , P t

(9)

3.4. Adaptive Expert Update

4848

tracked results have similar features with the target, the PSR
value may fail to make a difference.

mean = 1
K

In our experiment, we observe that when occlusion or se-
vere deformation occurs, the average robustness score of the
K
i=1 Rt(Ei) decreases signiﬁcant-
experts Rt
ly, which can be regarded as the divergence between mul-
tiple experts when facing an unreliable sample. Through
considering the average PSR score as well as average ex-
pert robustness score together, a combined reliability score
St = P t
mean is proposed, which can discover unre-
liable samples more effectively and better evaluate the qual-
ity of the current tracking.

mean · Rt

P

Since the DCF learns both target and background in-
formation, it is not reasonable to simply discard unreli-
able samples. When the current reliability score St is
signiﬁcantly lower than the past average reliability score:
t
i=1 Si, the learning rate η in Eq. (4) is de-
S1:t
mean = 1
t
creased by Eq. (11).

P

η =

C
C · [St/(α · S1:t

mean)]β

(

if St > α · S1:t
otherwise,

mean,

(11)

where C is the learning rate of the standard DCF, α is the re-
liability threshold, and β is the power exponent of the power
function. The designed power function penalizes samples
with low reliability scores severely to protect the experts
from corruption.

4. Experiments

4.1. Experimental Setup

Implementation Details: In our experiment, we follow
the parameters in standard DCF method [16] to construct
experts. The parameter ρ in the weigh sequence W is set to
1.1. The weighting factor µ in Eq. (10) is set to 0.1. As for
the adaptive update, α and β in Eq. (11) are set to 0.7 and
3 for MCCT tracker, and 0.6 and 3 for MCCT-H tracker,
respectively. More details can be found in the source code1.
We use the same setting of parameters for all the exper-
iments. Our trackers are implemented in MATLAB 2017a
on a computer with an Intel I7-4790K 4.00GHz CPU and
16GB RAM. The MatConvNet toolbox [42] is used for ex-
tracting the deep features from VGG-19 [39]. Our MCCT
tracker runs at about 1.5 FPS on CPU. The GPU version of
MCCT tracker runs at about 8 FPS, which is carried out on a
GeForce GTX 1080Ti GPU. The speed of MCCT-H tracker
is about 45 FPS on CPU.

Evaluation Benchmarks and Metrics: Our method is
evaluated on three benchmark datasets by a no-reset evalua-
tion protocol: OTB-2013 [47], OTB-2015 [48] and Temple-
Color [27]. All the tracking methods are evaluated by the
overlap precision (OP) at an overlap threshold 0.5. For the

1https://github.com/594422814/MCCT

Table 2. Effectiveness study of the proposed MCCT-H (top) and
MCCT (bottom) trackers. The DP (@20px) and AUC scores are
reported on the OTB-2013 [47] and OTB-2015 [48] datasets (D-
P/AUC) corresponding to the OPE. Expert VII (E7) is the best
individual expert (best baseline) in MCCT and MCCT-H.

E7 in MCCT-H MCCT-H-NU MCCT-H-PSR MCCT-H (Final)
Best Baseline
78.4 / 60.8
79.2 / 60.5

Sec. 3.3 + PSR
83.5 / 64.6
82.7 / 63.1

Sec. 3.3 + 3.4
85.6 / 66.4
84.1 / 64.2

Sec. 3.3
83.5 / 64.4
83.3 / 63.3

E7 in MCCT
Best Baseline
89.7 / 69.0
87.1 / 66.5

MCCT-NU
Sec. 3.3
91.9 / 70.9
88.3 / 67.6

MCCT-PSR
Sec. 3.3 + PSR
90.9 / 70.5
88.1 / 67.4

MCCT (Final)
Sec. 3.3 + 3.4
92.8 / 71.4
91.4 / 69.5

OTB-2013
OTB-2015

OTB-2013
OTB-2015

better performance measure, we also use the average dis-
tance precision (DP) plots and overlap success plots over
these datasets using one-pass evaluation (OPE) [47, 48].

Finally, we evaluate the performance of the proposed
trackers on the VOT2016 [21, 22] benchmark, which con-
sists of 60 challenging sequences and provides an evalua-
tion toolkit which will re-initialize the tracker to the correct
position to continue tracking when tracking failure occurs.
In VOT2016, the expected average overlap (EAO) is used
for ranking trackers, which combines the raw values of per-
frame accuracies and failures in a principled manner [21].

4.2. Framework Effectiveness Study

To evaluate the effectiveness of the proposed methods,
we compare the MCCT and MCCT-H trackers with their
individual experts. By only fusing multiple experts, we ob-
tain the MCCT-NU (MCCT-H-NU) without adaptive update
(only Sec. 3.3). The MCCT-PSR (MCCTH-PSR) repre-
sents the tracker adopts the fusion method and uses PSR
measurement for adaptive update (Sec. 3.3 + PSR). MCCT
(MCCT-H) is our ﬁnal algorithm which combines the ex-
pert selection mechanism and the proposed adaptive update
strategy (Sec. 3.3 + Sec. 3.4).

From the results in Table 2, we can observe that our ﬁ-
nal methods outperform their corresponding best baseline
Expert VII (the best individual expert) obviously, which il-
lustrates the effectiveness of the proposed framework. On
the OTB-2013 and OTB-2015 datasets, our ﬁnal MCCT-H
tracker outperforms its best baseline with a gain of about
6% (from 4.9%-7.2%) in DP and 4% (from 3.7%-5.6%) in
AUC. For tracker MCCT, it should be noted that its Expert
VII has already achieved a sufﬁcient performance level, but
our framework still boosts its performance further (about
4% DP and 3% AUC on challenging OTB-2015). Besides,
for the baseline MCCT-NU with high performance, the tra-
ditional PSR method for reliability estimation [4] does not
have obvious impact, but our adaptive update strategy by
considering the divergence of multiple experts improves the
performance further.

4849

Table 3. A comparison of our methods using overlap precision (OP) (%) at an overlap threshold 0.5 with recent state-of-the-art trackers on
the OTB-2013 [47], OTB-2015 [48] and Temple-Color [27] datasets. The average speed (i.e., frames per second, FPS) is evaluated on the
OTB-2013 dataset. The speed labeled with ∗ represents the corresponding tracker utilizes GPU in the experiment. The ﬁrst and second
highest values are highlighted by red and blue.

When
Where
OTB-2013
OTB-2015
Temple-Color
Speed (FPS)

LCT
2015

KCF
2015

HCF
DSST MEEM SAMF
2014
2015
2014
2014
BMVC ECCV ECCVW TPAMI CVPR ICCV
72.9
65.4
56.9
∗
10.8

62.6
55.3
45.9
243.7

81.4
70.1
55.3
26.1

66.8
61.6
47.0
26.3

71.4
66.4
56.8
18.3

70.3
62.5
60.6
19.2

2015
ICCV
78.1
72.6
61.2
5.6

2015
ICCVW
77.8
76.4
65.2
<1

HDT
2016

SCT
2016
2016
CVPR CVPR CVPR ECCV
77.8
74.2
73.0
63.0
63.5
55.7
∗
86.0
41.5

ECO MCCT-H MCCT
Staple SiamFc SRDCFdecon MDNet C-COT ACFN CSR-DCF ADNet MCPF
2017
2017
2016
2017
CVPR CVPR CVPR
87.0
84.6
80.6
84.1
77.5
77.2
73.5
66.9
-
∗
∗
15.0
0.6
2.9

2016
2017
ECCV CVPR
71.8
82.0
68.1
81.5
-
70.2
15.0
0.3

2017
CVPR
74.0
68.3
57.7
14.2

2016
CVPR
79.8
75.8
65.6
2.9

2016
CVPR
89.6
82.9
-
1.0

72.0
65.0
56.8
∗
10.5

90.7
86.4
74.4
∗
7.8

81.8
78.9
67.4
44.8

76.0
70.8
62.5
67.2

∗

∗

∗

SRDCF DeepSRDCDF

4.3. State of the art Comparison

We evaluate the proposed approach with 20 recent state-
of-the-art trackers including DSST [9], MEEM [51], SAMF
[26], KCF [16], LCT [32], HCF [31], SRDCF [12], Deep-
SRDCF [11], SCT [5], HDT [38], Staple [2], SiamFc [3],
SRDCFdecon [10], MDNet [35], C-COT [13], ACFN [6],
CSR-DCF [30], ADNet [50], MCPF [56], ECO [8]. A com-
parison with these state-of-the-art trackers using OP met-
ric on OTB-2013, OTB-2015 and Temple-Color datasets is
shown in Table 3.

Evaluation on OTB-2013. On the OTB-2013 [47]
benchmark, our proposed MCCT method achieves the best
OP of 90.7% (Table 3) and the highest area-under-curve
(AUC) score of 71.4% (Figure 5). The MDNet method
also exhibits excellent results and performs slightly better
than ours on DP metric (Figure 5), which is mainly due to
the effectiveness of the multi-domain network trained using
various similar tracking videos. Compared with other DCF
based methods, the proposed MCCT method outperform-
s the recent ECO, C-COT and MCPF on various metrics,
including on metrics OP, DP and AUC.

Evaluation on OTB-2015. On the OTB-2015 [48]
dataset, our proposed MCCT tracker achieves the best re-
sults on OP and DP with scores of 86.4% and 91.4% (Table
3 and Figure 6), outperforming the second best method by
2.3% and 1.3%. Among all the trackers, only ECO slightly
outperforms our method on AUC score. Our MCCT track-
er performs much better than HCF, HDT and MCPF, which
are also based on correlation ﬁlters with multiple types of
features.

The proposed MCCT-H method which only incorpo-
rates simple hand-crafted features achieves impressive per-
formance and operates at about 45 FPS on a single CPU
(Table 3).
It signiﬁcantly outperforms the DCF based
trackers with the same features (e.g., SAMF, SRDCF and
CSR-DCF) in both performance and speed. Furthermore,
MCCT-H achieves comparable results with the recent com-
plex trackers using deep features (e.g., ADNet, MCPF and
DeepSRDCF) but runs much faster than them. Compared
with other frame-wise adaption trackers (e.g., STC [5] and
ACFN [6]) that use attention mechanism for tracker con-
struction, our framework maintains several experts in par-
allel for decision-level selection and achieves better perfor-

1

0.8

0.6

0.4

0.2

t

e
a
r
 
s
s
e
c
c
u
S

0

0

1

0.8

0.6

0.4

0.2

e
t
a
r
 
s
s
e
c
c
u
S

0

0

1

0.8

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0.6

0.4

0.2

0

0

1

0.8

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0.6

0.4

0.2

0

0

Precision plots of OPE

Success plots of OPE

MDNet [93.2]
MCCT(Ours) [92.8]
ECO [91.3]
MCPF [90.3]
HCF [89.2]
C-COT [89.1]
ADNet [87.4]
HDT [87.0]
MCCTH(Ours) [85.6]
SRDCFdecon [85.3]

MCCT(Ours) [71.4]
ECO [71.2]
MDNet [71.2]
MCPF [68.2]
C-COT [68.0]
MCCT-H(Ours) [66.4]
SRDCFdecon [65.6]
ADNet [64.9]
DeepSRDCF [64.3]
LCT [64.0]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 5. Precision and success plots on the OTB-2013 [47] dataset
with 50 videos. Only the top 10 trackers are displayed for clarity.
In the legend, the DP at a threshold of 20 pixels and area-under-
curve (AUC) are reported in the left and right ﬁgures, respectively.

Precision plots of OPE

Success plots of OPE

MCCT(Ours) [91.4]
ECO [90.1]
C-COT [89.4]
MDNet [89.1]
MCPF [86.6]
ADNet [84.4]
DeepSRDCF [84.2]
MCCT-H(Ours) [84.1]
HCF [83.9]
HDT [83.6]

ECO [69.6]
MCCT(Ours) [69.5]
C-COT [68.1]
MDNet [67.6]
DeepSRDCF [64.2]
MCCT-H(Ours) [64.2]
ADNet [63.6]
MCPF [63.5]
SRDCFdecon [63.4]
SRDCF [60.9]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 6. Precision and success plots on the OTB-2015 [48] dataset
with 100 videos. In the legend, the DP (@20px) and AUC scores
are reported in the left and right ﬁgures, respectively.

mance (Table 3, Figure 5 and 6).

Evaluation on Temple-Color. For further evaluation,
we compare the proposed MCCT-H and MCCT methods on
the Temple-Color dataset [27] with the trackers mentioned
in Sec. 4.3 excluding MDNet, ADNet and ACFN, which all
need many external tracking videos for network training. In
contrast, our approach is free of such necessity.

In Table 3 and Figure 7, the proposed MCCT-H track-
er still achieves outstanding performance amongst real-time
trackers and outperforms some complex non-realtime track-
ers. Our MCCT method achieves the OP, DP and AUC s-
cores of (74.4%, 79.7%, 59.6%), while ECO and C-COT
yield (73.5%, 79.7%, 60.7%) and (70.2%, 78.1%, 58.3%),
respectively. Overall, our MCCT tracker shows compara-
ble results compared to the recent performance leader E-
CO and outperforms other state-of-the-art methods (e.g., C-
COT) on various metrics.

4850

Precision plots of OPE

Success plots of OPE

Expected overlap scores for baseline

1

0.8

0.6

0.4

0.2

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0

0

MCCT(Ours) [79.7]
ECO [79.7]
C-COT [78.1]
MCPF [77.4]
DeepSRDCF [73.8]
SRDCFdecon [72.7]
MCCT-H(Ours) [72.0]
MEEM [69.9]
SiamFc [69.2]
HCF [68.9]

1

0.8

0.6

0.4

0.2

t

e
a
r
 
s
s
e
c
c
u
S

0

0

ECO [60.7]
MCCT(Ours) [59.6]
C-COT [58.3]
MCPF [55.3]
MCCT-H(Ours) [55.1]
DeepSRDCF [54.5]
SRDCFdecon [54.3]
Staple [50.9]
SiamFc [50.5]
MEEM [50.3]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 7. Precision and success plots on the Temple-Color [27]
dataset with 128 color videos. In the legend, the DP (@20px) and
AUC scores are reported in the left and right ﬁgures, respectively.

Evaluation on VOT2016. Figure 8 shows the rank-
ing results in terms of expected average overlap (EAO) in
VOT2016 [21], from which we can observe that our MCC-
T tracker outperforms the top performer (C-COT [13]) by a
considerable margin. For presentation clarity, we only show
some top ranked and baseline trackers for comparison. For
more information, please refer to [21].

In Table 4, we list the detailed results of our approach-
es and the top ranked methods in VOT2016 (e.g., C-COT
[13], TCNN [34] and SSAT). Besides, two recently pro-
posed methods: CSR-DCF [30] and ECO [8] are also put
into comparison which do not participate in VOT2016. A-
mong all the compared methods, the proposed MCCT track-
er demonstrates advances in both accuracy and robustness.
As a result, our MCCT tracker provides the best EAO score
of 0.393, achieving a relative gain of 18.7% compared to
the VOT2016 top performer C-COT.

Discussion: (1) About Performance. It should be noted
that the ECO, C-COT and DeepSRDCF methods all take S-
RDCF as baseline, which can alleviate boundary effects ef-
fectively and performs much better than the standard DCF
[4, 16]. Besides, ECO and C-COT adopt the novel contin-
uous operator to better fuse the feature maps. However, all
the experts in our MCCT-H and MCCT methods just take
the simple DCF as baseline and all the strategies mentioned
above and other novel techniques [14, 33] can also be inte-
grated into our framework to further boost the performance.
(2) About Efﬁciency. Our tracker achieves almost the same
speed with expert VII because the same number of DCFs
and features are utilized (i.e., sharing strategy), but achieves
much better performance than it. Our general framework
provides a promising alternative for the multi-feature based
DCF trackers with ignorable impact on efﬁciency. Further-
more, we believe that other tracking algorithms [53, 36, 55]
with multiple types of features can also beneﬁt from our
multi-cue analysis framework.

5. Conclusion

In this paper, we propose a multi-cue analysis frame-
work for robust visual tracking, which considers not only

p
a
l
r
e
v
o

 

d
e
t
c
e
p
x
e

 

e
g
a
r
e
v
A

0.5

0.4

0.3

0.2

0.1

0

13

10

4

1

7
Order

TCNN
SRDCF

MCCT
EBT

MCCT-H
CCOT
DeepSRDCF MDNet

SSAT
HCF

MLDF
DSST

Staple
KCF

Figure 8. Expected Average Overlap (EAO) graph with trackers
ranked from right to left evaluated on VOT2016 [21]. Our pro-
posed MCCT tracker outperforms the top performer (C-COT [13])
by a considerable margin.

Table 4. The accuracy, robustness (failure rate) and EAO of state-
of-the-art methods on the VOT2016 [21]. The proposed MCCT
tracker achieves superior results compared to the top ranked meth-
ods in the challenge and recently proposed state-of-the-art algo-
rithms (ECO [8] and CSR-DCF [30]). The ﬁrst and second highest
values are highlighted by red and blue.

MCCT-H MLDF SSAT TCNN C-COT CSR-DCF ECO MCCT

Accuracy
Failure Rate
EAO

0.57
1.24
0.305

0.57
0.54
0.48
0.96
1.04
0.83
0.311 0.321 0.325

0.52
0.85
0.331

0.51
0.85
0.338

0.58
0.54
0.73
0.72
0.374 0.393

feature-level fusion but also decision-level fusion to fully
explore the strength of multiple features. Our framework
maintains multiple experts to track the target via different
views and selects the reliable outputs to reﬁne the tracking
results. Moreover, the proposed method evaluates the unre-
liable samples through considering the divergence of mul-
tiple experts and updates them adaptively. Through exten-
sive experiments on several challenging datasets, we show
that after adopting our simple yet effective multi-cue analy-
sis framework, without sophisticated models, only standard
Discriminative Correlation Filter (DCF) with deep or hand-
crafted features is able to perform favorably against state-
of-the-art methods in both accuracy and efﬁciency.

Acknowledgement. This work was supported in part
to Dr. Houqiang Li by 973 Pro-gram under contract No.
2015CB351803 and NSFC under contract No. 61390514,
in part to Dr. Wengang Zhou by NSFC under contract No.
61472378 and No. 61632019, the Fundamental Research
Funds for the Central Universities, and Young Elite Scien-
tists Sponsorship Program By CAST (2016QNRC001), and
in part to Dr. Qi Tian by ARO grant W911NF-15-1-0290
and Faculty Research Gift Awards by NEC Laboratories of
America and Blippar. This work was supported in part by
National Science Foundation of China (NSFC) 61429201.

4851

References

[1] C. Bailer, A. Pagani, and D. Stricker. A superior tracking ap-
proach: Building a strong tracker through fusion. In ECCV,
2014. 2

[2] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and
P. Torr. Staple: Complementary learners for real-time track-
ing. In CVPR, 2016. 2, 3, 7

[3] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and
P. H. Torr. Fully-convolutional siamese networks for object
tracking. In ECCV, 2016. 7

[4] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
CVPR, 2010. 1, 2, 3, 5, 6, 8

[5] J. Choi, H. Jin Chang, J. Jeong, Y. Demiris, and J. Y-
oung Choi. Visual tracking using attention-modulated dis-
integration and integration. In CVPR, 2016. 3, 7

[6] J. Choi, H. Jin Chang, S. Yun, T. Fischer, Y. Demiris, and
J. Young Choi. Attentional correlation ﬁlter network for
adaptive visual tracking. In CVPR, 2017. 7

[7] N. Dalal and B. Triggs. Histograms of oriented gradients for

human detection. In CVPR, 2005. 2, 3, 4

[8] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg.
Eco: Efﬁcient convolution operators for tracking. In CVPR,
2017. 2, 7, 8

[9] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accurate
scale estimation for robust visual tracking. In BMVC, 2014.
1, 3, 7

[10] M. Danelljan, G. H¨ager, F. S. Khan, and M. Felsberg. Adap-
tive decontamination of the training set: A uniﬁed formula-
tion for discriminative visual tracking. In CVPR, 2016. 2,
7

[11] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Fels-
berg. Convolutional features for correlation ﬁlter based vi-
sual tracking. In ICCV Workshop, 2015. 1, 2, 7

[12] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg.
Learning spatially regularized correlation ﬁlters for visual
tracking. In ICCV, 2015. 1, 2, 7

[13] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. In ECCV, 2016. 1, 2, 7, 8

[14] H. K. Galoogahi, A. Fagg, and S. Lucey.

Learning
background-aware correlation ﬁlters for visual tracking. In
ICCV, 2017. 1, 8

[15] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In ECCV, 2012. 2

[16] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
speed tracking with kernelized correlation ﬁlters. TPAMI,
37(3):583–596, 2015. 1, 2, 3, 6, 7, 8

[17] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychol-
In CVPR, 2015.
ogy inspired approach to object tracking.
1

[18] X. Jia, H. Lu, and M.-H. Yang. Visual tracking via adaptive
structural local sparse appearance model. In CVPR, 2012. 2
[19] O. Khalid, J. C. SanMiguel, and A. Cavallaro. Multi-tracker

partition fusion. TCSVT, 27(7):1527–1539, 2017. 2

[20] H. Kiani Galoogahi, T. Sim, and S. Lucey. Correlation ﬁlters

with limited boundaries. In CVPR, 2015. 1

[21] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, L. Cehovin,
G. Fern´andez, T. Vojir, Hager, and et al. The visual objec-
In ECCV Workshop,
t tracking vot2016 challenge results.
2016. 2, 6, 8

[22] M. Kristan, J. Matas, A. Leonardis, T. Voj´ıˇr, R. Pﬂugfelder,
G. Fernandez, G. Nebehay, F. Porikli, and L. ˇCehovin. A
novel performance evaluation methodology for single-target
trackers. TPAMI, 38(11):2137–2155, 2016. 6
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1

Imagenet
In

[24] D. Y. Lee, J. Y. Sim, and C. S. Kim. Multihypothesis tra-
jectory analysis for robust visual tracking. In CVPR, 2015.
2

[25] J. Li, C. Deng, R. Y. D. Xu, D. Tao, and B. Zhao. Robust
object tracking with discrete graph based multiple experts.
TIP, 26(6):2736–2750, 2017. 2

[26] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. In ECCV Workshop, 2014.
1, 7

[27] P. Liang, E. Blasch, and H. Ling. Encoding color informa-
tion for visual tracking: algorithms and benchmark. TIP,
24(12):5630–5644, 2015. 2, 6, 7, 8

[28] S. Liu, T. Zhang, X. Cao, and C. Xu. Structural correlation

ﬁlter for robust visual tracking. In CVPR, 2016. 1

[29] T. Liu, G. Wang, and Q. Yang. Real-time part-based visual

tracking via adaptive correlation ﬁlters. In CVPR, 2015. 1

[30] A. Lukezic, T. Vojir, L. Cehovin Zajc, J. Matas, and M. Kris-
tan. Discriminative correlation ﬁlter with channel and spatial
reliability. In CVPR, 2017. 1, 2, 3, 7, 8

[31] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In ICCV, 2015. 1,
3, 4, 7

[32] C. Ma, X. Yang, C. Zhang, and M.-H. Yang. Long-term

correlation tracking. In CVPR, 2015. 1, 7

[33] M. Mueller, N. Smith, and B. Ghanem. Context-aware cor-

relation ﬁlter tracking. In CVPR, 2017. 1, 8

[34] H. Nam, M. Baek, and B. Han. Modeling and propagating
cnns in a tree structure for visual tracking. arXiv preprint
arXiv:1608.07242, 2016. 8

[35] H. Nam and B. Han. Learning multi-domain convolutional
neural networks for visual tracking. In CVPR, 2016. 1, 7
[36] J. Ning, J. Yang, S. Jiang, L. Zhang, and M.-H. Yang. Object
tracking via dual linear structured svm and explicit feature
map. In CVPR, 2016. 8

[37] H. Possegger, T. Mauthner, and H. Bischof.

In defense of

color-based model-free tracking. In CVPR, 2015. 3

[38] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-H.
Yang. Hedged deep tracking. In CVPR, 2016. 1, 2, 7
[39] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 1, 3, 6

[40] Y. Song, C. Ma, L. Gong, J. Zhang, R. Lau, and M.-H. Yang.
Crest: Convolutional residual learning for visual tracking. In
ICCV, 2017. 1

4852

[41] J. Valmadre, L. Bertinetto, J. F. Henriques, A. Vedaldi, and
P. H. Torr. End-to-end representation learning for correlation
ﬁlter based tracking. In CVPR, 2017. 1

[42] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural

networks for matlab. In ACM MM, 2014. 6

[43] N. Wang, J. Shi, D. Y. Yeung, and J. Jia. Understanding and
diagnosing visual tracking systems. In ICCV, 2015. 2, 4
[44] N. Wang and D. Y. Yeung. Ensemble-based tracking: Aggre-
gating crowdsourced structured time series data. In ICML,
2014. 2

[45] N. Wang, W. Zhou, and H. Li. Reliable re-detection for long-

term tracking. TCSVT, 2018. 1

[46] J. V. D. Weijer, C. Schmid, J. Verbeek, and D. Larlus.
TIP,
Learning color names for real-world applications.
18(7):1512–1523, 2009. 2, 4

[47] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A

benchmark. In CVPR, 2013. 2, 6, 7

[48] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.

TPAMI, 37(9):1834–1848, 2015. 1, 2, 6, 7

[49] A. Yilmaz, O. Javed, and M. Shah. Object tracking: A sur-

vey. ACM Computing Surveys, 38(4):13, 2006. 1

[50] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Young Choi. Action-
decision networks for visual tracking with deep reinforce-
ment learning. In CVPR, 2017. 7

[51] J. Zhang, S. Ma, and S. Sclaroff. Meem: robust tracking
via multiple experts using entropy minimization. In ECCV,
2014. 2, 7

[52] K. Zhang, L. Zhang, and M.-H. Yang. Real-time compres-

sive tracking. In ECCV, 2012. 1

[53] K. Zhang, L. Zhang, and M.-H. Yang. Real-time objec-
t tracking via online discriminative feature selection. TIP,
22(12):4664–4677, 2013. 8

[54] K. Zhang, L. Zhang, M.-H. Yang, and D. Zhang. Fast vi-
sual tracking via dense spatio-temporal context learning. In
ECCV, 2013. 2

[55] T. Zhang, A. Bibi, and B. Ghanem.

In defense of sparse

tracking: Circulant sparse tracker. In CVPR, 2016. 8
[56] T. Zhang, C. Xu, and M.-H. Yang. Multi-task correlation
particle ﬁlter for robust object tracking. In CVPR, 2017. 1, 7
[57] W. Zhong, H. Lu, and M.-H. Yang. Robust object tracking

via sparsity-based collaborative model. In CVPR, 2012. 2

4853

Multi-Cue Correlation Filters for Robust Visual Tracking

Ning Wang1, Wengang Zhou1, Qi Tian2, Richang Hong3, Meng Wang3, Houqiang Li1
1CAS Key Laboratory of GIPAS, University of Science and Technology of China
2University of Texas at San Antonio 3HeFei University of Technology
wn6149@mail.ustc.edu.cn, {zhwg,lihq}@ustc.edu.cn, qi.tian@utsa.edu, {hongrc,wangmeng}@hfut.edu.cn

Abstract

In recent years, many tracking algorithms achieve im-
pressive performance via fusing multiple types of features,
however, most of them fail to fully explore the context among
the adopted multiple features and the strength of them. In
this paper, we propose an efﬁcient multi-cue analysis frame-
work for robust visual tracking. By combining differen-
t types of features, our approach constructs multiple ex-
perts through Discriminative Correlation Filter (DCF) and
each of them tracks the target independently. With the pro-
posed robustness evaluation strategy, the suitable expert is
selected for tracking in each frame. Furthermore, the di-
vergence of multiple experts reveals the reliability of the
current tracking, which is quantiﬁed to update the experts
adaptively to keep them from corruption.

Through the proposed multi-cue analysis, our tracker
with standard DCF and deep features achieves outstand-
ing results on several challenging benchmarks: OTB-2013,
OTB-2015, Temple-Color and VOT 2016. On the other
hand, when evaluated with only simple hand-crafted fea-
tures, our method demonstrates comparable performance
amongst complex non-realtime trackers, but exhibits much
better efﬁciency, with a speed of 45 FPS on a CPU.

1. Introduction

Visual tracking is a fundamental task in computer vision
with a wide range of applications. It is challenging since
only the initial state of the target is available. Although
substantial progress has been made in the past decades
[52, 16, 35], there still remain many challenges [49].

In recent years, Discriminative Correlation Filter (DCF)
based tracking methods [4, 16] have gained much atten-
tion thanks to their impressive performance as well as high
speed. In DCF based trackers, the ﬁlter is trained through
minimizing a least-squares loss for all circular shifts of a
training sample. Since the correlation operation can be cal-
culated in Fourier domain, DCF shows the advantage of
Its performance is further
high computational efﬁciency.

(a) Biker

(b) Box

(c) Diving

(d) Shaking

(e) Singer2

(f) MotorRolling

MCCT

C-COT

DeepSRDCF

SRDCF

Figure 1. Comparison of the proposed algorithm (MCCT) with
the state-of-the-art trackers: SRDCF [12], DeepSRDCF [11] and
C-COT [13]. These trackers adopt various types of features and
perform differently in challenging scenes. Our method maintain-
s multiple cues for tracking and performs favorably against these
trackers. All the videos are from OTB-2015 [48].

enhanced by using multi-dimensional features [16, 26, 31],
part-based strategies [29, 28], adaptive scale [9, 26], long-
term framework [32, 17, 45], end-to-end learning [41, 40]
and improved ﬁlter training methods [20, 30, 33, 14]. Spe-
cially, the combination of DCF and features from deep con-
volutional neural networks (CNNs) [23, 39] has demonstrat-
ed state-of-the-art results. Recently, Ma et al. [31] propose
a HCF algorithm, which constructs multiple DCFs on low,
middle and high-level features to capture both spatial detail-
s and semantics. It predicts the target location using multi-
level DCF response maps in a coarse-to-ﬁne fashion.

Although feature-level fusion methods [26, 31, 38] have
been widely used or extended to boost the performance,
there still leaves room for improvement.
In HCF [31] or
other methods [56, 38] that follow such a fusion strategy,
the initial weight of high-level features is usually high such
that semantic features play the dominant role in general. It
is reasonable because of better effectiveness of high-level
features compared to shallow features. However, due to the
occasional misguidance of the semantic information, a tran-
sient drift or wrong prediction may be ampliﬁed by the in-
adequate online update. Therefore, the feature-level fusion

4844

approach sometimes still fails to fully explore the relation-
ship of multiple features. Furthermore, it is quite difﬁcult to
handle various challenging variations using a single model,
and relying on a certain feature-level fusion strategy limits
the model diversity to some extent.

To better illustrate the issues mentioned above, our pro-
posed algorithm is compared with three DCF based method-
s. In Figure 1, SRDCF [12], DeepSRDCF [11] and C-COT
[13] adopt different types of features, but none of them is
able to handle various challenging factors, even for the C-
COT algorithm with multiple features as well as the nov-
el continuous convolution operators. Since it is quite difﬁ-
cult to design a satisfying feature-level fusion method that
suits various challenging scenes, it is intuitive to design an
adaptive switch mechanism to achieve better performance,
which can ﬂexibly switch to the reliable tracker depend-
ing on what kind of challenging factors it is expert at han-
dling. In other words, the performance of a single tracker
can sometimes be unstable but the decision-level fusion of
the outputs from multiple trackers can enhance the robust-
ness effectively.

In this paper, a novel Multi-Cue Correlation ﬁlter based
Tracker (MCCT) is proposed. Different from the previous
DCF based methods [4, 16] that usually lack the diversity
of target appearance representations, our method maintains
multiple experts to learn the appearance models from dif-
ferent views. Here, a certain combination of features con-
structs an individual expert and provides a reliable cue (pre-
dicted target state) for tracking. The main contributions
(1) We pro-
of our work can be summarized as follows.
pose an algorithm that maintains multiple cues for track-
ing. Through checking the robustness scores of multiple
experts carefully, our method reﬁnes the tracking results
by choosing the reliable expert for tracking in each frame.
(2) By considering the divergence of multiple experts, we
present an adaptive update strategy which can discriminate
the unreliable samples (e.g., occlusion or sever deforma-
tion) effectively and alleviate the contamination of train-
(3) We implement two versions of the pro-
ing samples.
posed method to validate the generality of the framework.
The MCCT tracker with deep features demonstrates out-
standing performance on several challenging benchmarks
[47, 48, 27, 21]. The MCCT-H tracker with only two s-
tandard Hand-crafted features (HOG [7] and ColorNames
[46]) achieves comparable performance with many complex
deep-model based trackers, but operates about 45 frames
per second on a single CPU, exceeding most competitive
trackers by several times.

2. Related Work

In this section, we discuss two categories of trackers
closely related to our algorithm: correlation tracking and
ensemble tracking.

Correlation Tracking. Since Bolme et al.

[4] pro-
pose a tracker using minimum output sum of squared er-
ror (MOSSE) ﬁlter, the correlation ﬁlters have been widely
studied in visual tracking. Heriques et al. exploit the cir-
culant structure of the training patches [15] and propose to
train correlation ﬁlter in a kernel space with HOG features
[16]. Zhang et al. [54] incorporate context information in-
to ﬁlter learning. The SRDCF tracker [12] alleviates the
boundary effects by penalizing correlation ﬁlter coefﬁcients
depending on spatial location, and is enhanced by reduc-
ing the inﬂuence of corrupted samples [10]. Qi et al. [38]
propose a HDT algorithm that fuses several DCFs through
adaptive hedged method. Luca et al.
[2] propose a Sta-
ple tracker which combines DCF and color histogram based
model while running in excess of real-time. The CSR-DCF
algorithm [30] constructs DCF with channel and spatial reli-
ability. The recent C-COT [13] adopts a continuous-domain
formulation of the DCF, leading to the top performance on
several tracking benchmarks. The enhanced version of C-
COT is ECO [8], which improves both speed and perfor-
mance by introducing several efﬁcient strategies.

Different from the DCF based methods mentioned
above, our algorithm considers not only feature-level fusion
but also decision-level fusion to better explore the relation-
ship of multiple features, and adaptively selects the expert
that is suitable for a particular tracking task.

Ensemble Tracking. To enhance the tracking robust-
ness and obtain reliable results, the ensemble approach
treats the trackers as black boxes and takes only the bound-
ing boxes returned by them as input [43]. In [1], a dynam-
ic programming based trajectory optimization approach is
proposed to build a strong tracker. In [44], Wang et al. pro-
pose a factorial hidden Markov model for ensemble-based
tracking. The MEEM algorithm [51] exploits the relation-
ship between the current tracker and its historical snapshots
using entropy minimization, and Li et al.
[25] extend it
by introducing a discrete graph optimization framework. In
[19], a partition fusion framework is proposed to cluster re-
liable trackers for target state prediction.

Although promising results are obtained through fusing
multiple trackers, there still leave some limitations: (1) the
overall speed is limited by the lowest tracker in the en-
semble (e.g., sparse representation based methods [57, 18],
about 1 FPS), which restricts the real-time application. Spe-
cially, the fusion methods [19, 24] by analyzing the for-
ward and backward trajectories require each tracker to run
at least twice; (2) the trackers are just regarded as inde-
pendent black boxes and their fusion result in each frame
does not feedback to the trackers [1, 44, 19], which fails to
make full use of the reliable fusion outputs; (3) if the fu-
sion tracker number increases, the dynamic programming
based fusion methods [25, 1] still bring obvious computa-
tional burden (e.g., O(T N 2) for T frames and N trackers).

4845

Flowchart

Input 
Frame

ROI

Feature  Pool

Expert  Pool  (DCF)

Expert  Selection

Feature 
Extration

Multi-Expert
Prediction

...

Adaptive
Model
Update

Figure 2. A systematic ﬂowchart of the proposed tracking algorithm. First, different features of ROI are extracted and combined to train
multiple experts following DCF framework (Sec. 3.1 and 3.2). Then, each expert gives an individually predicted cue and the most reliable
expert is selected for the current tracking (Sec. 3.3). Finally, adaptive update helps keep the experts from corruption (Sec. 3.4).

Different from the above methods:

(1) Our approach
the experts in the DCF framework, and
constructs all
through the proposed ROI and training sample sharing s-
trategy (Sec. 3.3), the efﬁciency is greatly ensured; (2) the
reﬁned tracking results are fed back to the experts to boost
them further; (3) through a simple but effective robustness
evaluation strategy, our method selects the reliable expert
for tracking with a complexity of only O(T N ) for T frames
and N experts.

3. Method

Our algorithm is composed of several experts which pro-
vide different levels of cues for tracking. A preview of DCF
is introduced in Sec. 3.1. The component of the expert pool
is described in Sec. 3.2. How to switch to the suitable ex-
pert in each frame is elaborated in Sec. 3.3. Finally, Sec.
3.4 introduces the adaptive update. The framework of our
method is depicted in Figure 2.

3.1. Preview of DCF

A typical tracker based on DCF [4, 16] is trained using
an image patch x of size M × N , which is centered around
the target. All the circular shifts of the patch x(m, n) ∈
{0, 1, ...M − 1} × {0, 1, ...N − 1} are generated as training
samples with Gaussian function label y(m, n) in terms of
the shifted distance. The ﬁlter w is trained by minimizing
the following regression error:

min
w

kXw − yk2

2 + λkwk2
2,

where λ is a regularization parameter (λ ≥ 0) and X is the
data matrix by concatenating all the circular shifts. The ﬁl-
ter solution on the d-th (d ∈ {1, · · · , D}) channel is deﬁned
by

ˆw∗

d =

ˆy ⊙ ˆx∗
d
i ⊙ ˆxi + λ

,

D

i=1 ˆx∗
where ⊙ is the element-wise product, the hat symbol de-
notes the Discrete Fourier Transform (DFT) of a vector
(e.g., ˆx = F(x)) and ˆx∗ is the complex-conjugate of ˆx.
In the next frame, a Region of Interest (ROI) is cropped out
for tracking the target (e.g., a patch z with the same size of

P

(1)

(2)

x). The response map R of z is calculated in Eq. (3) and
the location of the target is identiﬁed by searching for the
maximum value of R.

R = F

−1(

ˆwd ⊙ ˆz∗

d).

(3)

D

d=1
X

To avoid the boundary effects during learning, we apply
Hann window to the signals [16]. Besides, inspired by [30,
5], color information is applied to the training sample in a
= X ⊙ C,
simple way to enhance its spatial reliability: X
where X represents the data matrix and C is the color mask
obtained by computing the histogram-based per-pixel score
map [37, 2] of ROI. The online update of the numerator ˆAd
and the denominator ˆBd of the ﬁlter ˆw∗

d is as follows,

′

ˆAt

d = (1 − η) ˆAt−1

d + η ˆy ⊙ ˆx∗t
d ,

ˆBt

d = (1 − η) ˆBt−1

d + η

ˆx∗t
i ⊙ ˆxt
i,

(4)

D

i=1
X

ˆw∗t

d =

ˆAt
d
d + λ

,

ˆBt

where η is the learning rate and t is the index of the current
frame. As for the target scale estimation, we follow the
DSST tracker [9].

3.2. Feature Pool and Expert Pool

A variety of features can be adopted in DCF and differ-
ent features have their own strength. Hand-crafted features
are typically used to capture low-level details while deep
features are semantic-aware. As discussed in HCF [31], the
DCF constructed by each single layer of VGG-19 [39] is
not accurate enough, so HCF performs coarse-to-ﬁne search
on several DCF response maps from different layers.
In
our MCCT tracker, HOG [7] is used as low-level features.
Then, we remove the fully-connected layers and extract the
outputs of the conv4-4 and conv5-4 convolutional layers of
VGG-19 as middle-level and high-level features, respective-
ly. Thus, the feature pool consists of three types of features:
{Low, M iddle, High} and they are optionally combined
into C 1
3 = 7 experts. As for the coarse-to-ﬁne

3 + C 3

3 + C 2

4846

Table 1. The component of the expert ensemble. Our MCCT
and MCCT-H trackers both consist of seven experts. Each expert
adopts different features and tracks the target via a different view.

Expert Pool
Expert I
Expert II
Expert III
Expert IV
Expert V
Expert VI
Expert VII

Tracker MCCT
Feature Type
Low (HOG)
Middle (conv4-4 of VGG-19)
High (conv5-4 of VGG-19)
Middle, Low
High, Low
High, Middle
High, Middle, Low

Tracker MCCT-H
Feature Type
HOG1
HOG2
ColorNames
HOG1, ColorNames
HOG2, ColorNames
HOG1, HOG2
HOG1, HOG2, ColorNames

Expert I

Expert II

Expert III

Expert IV

…

…

…

…

…

…

…

…

weighting parameters of different level DCF response maps,
we follow the settings in HCF [31]. Although some experts
(e.g., Expert I, II and III) with single type of features may be
less robust compared to Expert VII, they provide the diver-
sity of tracking results, which is crucial in ensemble-based
tracking [43].

Our fast variant, the MCCT-H tracker only utilizes stan-
dard hand-crafted features (HOG [7] and ColorNames [46])
to construct experts. Since HOG and ColorNames are both
low-level features, we do not conduct coarse-to-ﬁne fusion
and just simply concatenate them to construct DCF. Col-
orNames provides an 11 dimensional color representation
and HOG feature is 31-dim. To obtain more experts, we
take the average grey value over all pixels in a patch as a
1-dim feature and concatenate it to the HOG feature into a
32-dim vector, which is further evenly decomposed into two
16-dim features, denoted as HOG1 and HOG2, respective-
ly. The detailed information of the experts in MCCT and
MCCT-H is shown in Table 1.

3.3. Multi Cue Correlation Tracker

The tracking process of our proposed framework can be
illustrated by Figure 3, where the multiple experts track the
target in parallel and the nodes denote the generated cues
(bounding boxes) of the experts. In each frame, the evalua-
tion between different hypothesis nodes reveals the consis-
tency degree between the experts, which is denoted as pair-
wise evaluation. Besides, each expert has its own trajec-
tory continuity and smoothness. Thus, given a hypothesis
node, its robustness degree can be evaluated by both pair-
evaluation and self-evaluation. After evaluating the overall
reliability of each node, the expert with the highest robust-
ness score is selected and its tracking result is taken for the
current frame.

In the following, we elaborate the formulation of pair-

evaluation, self-evaluation and expert selection strategy.

Expert Pair-Evaluation. Most experts in the ensemble
are capable of tracking the target stably, and the cues pro-
duced by a good expert should be consistent with the cues
from other experts as much as possible.

Let E1, · · · , E7 denote Expert I, · · · , Expert VII, respec-
tively. In the t-th frame, the bounding box of Expert i is

Frame t

Frame t+1

Frame t+2

Expert Pair-Evaluation

Expert Self-Evaluation

Figure 3. Graph illustration of the multi-expert framework. The
node in the graph denotes the predicted bounding box. The ro-
bustness score of each expert is evaluated by both pair-evaluation
and self-evaluation. For clarity, only four experts are displayed.

denoted as Bt
. Through regarding all the experts as black
Ei
boxes, the bounding box Bt
only contains the target state
Ei
(e.g., location and scale) without any context information,
which reduces the computational and memory burden ef-
fectively.

First, we compute overlap ratios of the bounding boxes
(Ei,Ej ) of Expert

from different experts. The overlap ratio Ot
i and Expert j at frame t is calculated as follows,

Ot

(Ei,Ej ) =

Area(Bt
Ei
Area(Bt
Ei

Bt
Bt

Ej )
Ej )

.

T

(5)

To reduce the gap between low and high overlap ratios, we
adopt a nonlinear gaussian function to Ot
(Ei,Ej ) as follows,
which gathers the expert scores.

S

O

′t
(Ei,Ej ) = exp

−

1 − Ot

(Ei,Ej )

2

.

(6)

(cid:17)

(cid:16)

K
j=1 O

The mean value M t

(cid:18)
(cid:19)
′t
Ei = 1
(Ei,Ej ) of overlap ra-
K
tios reveals the trajectory consistency between expert i and
others, where K denotes the number of experts (in our ex-
periment, K = 7). In general, the pair-wise comparison s-
cores between two experts should be temporal stable. Thus,
the ﬂuctuation extent of overlap ratios in a short period ∆t
(e.g., 5 frames) reveals the stability of overlap evaluation
between Ei and other experts, which is given by Eq. (7).

P

V t
Ei =

K

1
K

v
u
j=1 (cid:16)
X
u
t
′t−∆t+1:t
(Ei,Ej ) = 1
∆t

where O
τ ∈ [t − ∆t + 1, t].

O

′t
(Ei,Ej ) − O

′t−∆t+1:t
(Ei,Ej )

(7)

2

,

(cid:17)

τ O

′τ
(Ei,Ej ) and the time index

P

Then, to avoid performance ﬂuctuation of the experts, we
further take temporal stability into account and introduce an
increasing sequence W = {ρ0, ρ1, · · · , ρ∆t−1}, (ρ > 1) to
give more weight to the recent scores. After considering the
temporal context, the average weighted mean and standard

4847

#8

Selected Cue:
Expert VII

#60

Selected Cue:
Expert V

#75

Selected Cue:
Expert II

#135

Selected Cue:
Expert IV

Expert I

Expert V

Expert II

Expert VI

Expert III

Expert VII

Expert IV

Figure 4. Expert selection process of our MCCT tracker in MotorRolling sequence. The bottom plots show the normalized robustness
scores of different experts. In top ﬁgures, the expert with the highest robustness score is selected for tracking. All the experts share the
same ROI (the largest yellow box) and are updated using the same selected results.

′t
τ Wτ M τ
Ei = 1
variance are calculated through: M
Ei
N
and V
, respectively, where Wτ denotes
the (τ − t + ∆t)-th element in sequence W and N is the
normalization factor deﬁned by N =

′t
Ei = 1
N

τ Wτ V τ
Ei

P

P

τ Wτ .

Finally, the pair-wise expert robustness score of Expert i

at frame t is deﬁned as follows,

P

′t
M
Ei
′t
Ei + ξ

,

V

Rt

pair(Ei) =

(8)

where ξ is a small constant that avoids the inﬁnite pair-
evaluation score for a zero denominator. A larger Rt
pair(Ei)
means better consistency with other experts and higher sta-
bility of the target state prediction.

Expert Self-Evaluation. The trajectory smoothness de-
gree of each expert indicates the reliability of its track-
ing results to some extent. The Euclidean distance mea-
suring the shift between the previous bounding box Bt−1
Ei
is computed by Dt
and the current bounding box Bt
Ei =
Ei
kc(Bt−1
Ei )k, where c(Bt
Ei ) − c(Bt
Ei ) is the center of the
bounding box Bt
In frame t, the trajectory ﬂuctuation
.
Ei
degree of Expert i is given by Eq. (9).

St

Ei = exp

−

1
2σ2
Ei

(Dt

Ei )2

,

(cid:19)

Ei)].

(cid:18)
where σEi is the average length of the width W (Bt
height H(Bt
i.e., σEi = 1

Ei) and
Ei) of the bounding box provided by Expert i,
2 [W (Bt

Ei) + H(Bt

Similar to the pair-evaluation, we collect the previous
movement information to consider the temporal stability.
Finally, the self-evaluation score is deﬁned by Rt
self (Ei) =
1
self (Ei) means the better re-
N
liability of the tracking trajectory.

. The higher Rt

τ Wτ Sτ
Ei

P
Expert Selection. The ﬁnal robustness score Rt(Ei)
of the Expert i in t-th frame is a linear combination of it-
s pair-evaluation score Rt
pair(Ei) and self-evaluation score

Rt

self (Ei):
Rt(Ei) = µ · Rt

pair(Ei) + (1 − µ) · Rt

self (Ei),

(10)

where µ is the parameter to trade off the pair-evaluation and
self-evaluation weights. In each frame, as shown in Figure
4, the expert with the highest robustness score is selected
for the current tracking.

Sharing Strategy. In the tracking process, all the expert-
s are updated using the same selected samples and share the
same searching areas (ROI). This sharing strategy has two
advantages: (1) It alleviates the drift and tracking failures
of the weak experts effectively by sharing the reﬁned re-
sults for target position prediction and model update; (2) It
ensures the efﬁciency of our framework greatly. The main
computational burden of DCF lies in the feature extraction
process, especially the deep features. Although multiple ex-
perts (K = 7) are maintained in our method, the sharing s-
trategy makes the feature extraction process only be carried
out twice in each frame (instead of 7 × 2 = 14 times), one
for the searching patch (ROI) and the other for the train-
ing patch (used for model update), which is the same as the
standard DCF and independent of the expert number.

Due to the training sample sharing strategy used in our
framework, the selected tracking results should be carefully
checked to avoid the corruption of the experts. The peak-
to-sidelobe ratio (PSR) is widely used in DCF to quantify
the reliability of the tracked samples [4]. PSR is deﬁned as
P = (Rmax − m)/σ, where Rmax is the maximum conﬁ-
dence, m and σ are the mean and standard deviation of the
response, respectively. We compute the average PSR of d-
ifferent features: P t
3 (P t
L) to evaluate
the t-th tracking result, where P t
L denote the PSR
values of the High, Middle and Low-level response map-
s, respectively. However, in some cases when the unreliable

H + P t
H , P t

mean = 1

M + P t

M , P t

(9)

3.4. Adaptive Expert Update

4848

tracked results have similar features with the target, the PSR
value may fail to make a difference.

mean = 1
K

In our experiment, we observe that when occlusion or se-
vere deformation occurs, the average robustness score of the
K
i=1 Rt(Ei) decreases signiﬁcant-
experts Rt
ly, which can be regarded as the divergence between mul-
tiple experts when facing an unreliable sample. Through
considering the average PSR score as well as average ex-
pert robustness score together, a combined reliability score
St = P t
mean is proposed, which can discover unre-
liable samples more effectively and better evaluate the qual-
ity of the current tracking.

mean · Rt

P

Since the DCF learns both target and background in-
formation, it is not reasonable to simply discard unreli-
able samples. When the current reliability score St is
signiﬁcantly lower than the past average reliability score:
t
i=1 Si, the learning rate η in Eq. (4) is de-
S1:t
mean = 1
t
creased by Eq. (11).

P

η =

C
C · [St/(α · S1:t

mean)]β

(

if St > α · S1:t
otherwise,

mean,

(11)

where C is the learning rate of the standard DCF, α is the re-
liability threshold, and β is the power exponent of the power
function. The designed power function penalizes samples
with low reliability scores severely to protect the experts
from corruption.

4. Experiments

4.1. Experimental Setup

Implementation Details: In our experiment, we follow
the parameters in standard DCF method [16] to construct
experts. The parameter ρ in the weigh sequence W is set to
1.1. The weighting factor µ in Eq. (10) is set to 0.1. As for
the adaptive update, α and β in Eq. (11) are set to 0.7 and
3 for MCCT tracker, and 0.6 and 3 for MCCT-H tracker,
respectively. More details can be found in the source code1.
We use the same setting of parameters for all the exper-
iments. Our trackers are implemented in MATLAB 2017a
on a computer with an Intel I7-4790K 4.00GHz CPU and
16GB RAM. The MatConvNet toolbox [42] is used for ex-
tracting the deep features from VGG-19 [39]. Our MCCT
tracker runs at about 1.5 FPS on CPU. The GPU version of
MCCT tracker runs at about 8 FPS, which is carried out on a
GeForce GTX 1080Ti GPU. The speed of MCCT-H tracker
is about 45 FPS on CPU.

Evaluation Benchmarks and Metrics: Our method is
evaluated on three benchmark datasets by a no-reset evalua-
tion protocol: OTB-2013 [47], OTB-2015 [48] and Temple-
Color [27]. All the tracking methods are evaluated by the
overlap precision (OP) at an overlap threshold 0.5. For the

1https://github.com/594422814/MCCT

Table 2. Effectiveness study of the proposed MCCT-H (top) and
MCCT (bottom) trackers. The DP (@20px) and AUC scores are
reported on the OTB-2013 [47] and OTB-2015 [48] datasets (D-
P/AUC) corresponding to the OPE. Expert VII (E7) is the best
individual expert (best baseline) in MCCT and MCCT-H.

E7 in MCCT-H MCCT-H-NU MCCT-H-PSR MCCT-H (Final)
Best Baseline
78.4 / 60.8
79.2 / 60.5

Sec. 3.3 + PSR
83.5 / 64.6
82.7 / 63.1

Sec. 3.3 + 3.4
85.6 / 66.4
84.1 / 64.2

Sec. 3.3
83.5 / 64.4
83.3 / 63.3

E7 in MCCT
Best Baseline
89.7 / 69.0
87.1 / 66.5

MCCT-NU
Sec. 3.3
91.9 / 70.9
88.3 / 67.6

MCCT-PSR
Sec. 3.3 + PSR
90.9 / 70.5
88.1 / 67.4

MCCT (Final)
Sec. 3.3 + 3.4
92.8 / 71.4
91.4 / 69.5

OTB-2013
OTB-2015

OTB-2013
OTB-2015

better performance measure, we also use the average dis-
tance precision (DP) plots and overlap success plots over
these datasets using one-pass evaluation (OPE) [47, 48].

Finally, we evaluate the performance of the proposed
trackers on the VOT2016 [21, 22] benchmark, which con-
sists of 60 challenging sequences and provides an evalua-
tion toolkit which will re-initialize the tracker to the correct
position to continue tracking when tracking failure occurs.
In VOT2016, the expected average overlap (EAO) is used
for ranking trackers, which combines the raw values of per-
frame accuracies and failures in a principled manner [21].

4.2. Framework Effectiveness Study

To evaluate the effectiveness of the proposed methods,
we compare the MCCT and MCCT-H trackers with their
individual experts. By only fusing multiple experts, we ob-
tain the MCCT-NU (MCCT-H-NU) without adaptive update
(only Sec. 3.3). The MCCT-PSR (MCCTH-PSR) repre-
sents the tracker adopts the fusion method and uses PSR
measurement for adaptive update (Sec. 3.3 + PSR). MCCT
(MCCT-H) is our ﬁnal algorithm which combines the ex-
pert selection mechanism and the proposed adaptive update
strategy (Sec. 3.3 + Sec. 3.4).

From the results in Table 2, we can observe that our ﬁ-
nal methods outperform their corresponding best baseline
Expert VII (the best individual expert) obviously, which il-
lustrates the effectiveness of the proposed framework. On
the OTB-2013 and OTB-2015 datasets, our ﬁnal MCCT-H
tracker outperforms its best baseline with a gain of about
6% (from 4.9%-7.2%) in DP and 4% (from 3.7%-5.6%) in
AUC. For tracker MCCT, it should be noted that its Expert
VII has already achieved a sufﬁcient performance level, but
our framework still boosts its performance further (about
4% DP and 3% AUC on challenging OTB-2015). Besides,
for the baseline MCCT-NU with high performance, the tra-
ditional PSR method for reliability estimation [4] does not
have obvious impact, but our adaptive update strategy by
considering the divergence of multiple experts improves the
performance further.

4849

Table 3. A comparison of our methods using overlap precision (OP) (%) at an overlap threshold 0.5 with recent state-of-the-art trackers on
the OTB-2013 [47], OTB-2015 [48] and Temple-Color [27] datasets. The average speed (i.e., frames per second, FPS) is evaluated on the
OTB-2013 dataset. The speed labeled with ∗ represents the corresponding tracker utilizes GPU in the experiment. The ﬁrst and second
highest values are highlighted by red and blue.

When
Where
OTB-2013
OTB-2015
Temple-Color
Speed (FPS)

LCT
2015

KCF
2015

HCF
DSST MEEM SAMF
2014
2015
2014
2014
BMVC ECCV ECCVW TPAMI CVPR ICCV
72.9
65.4
56.9
∗
10.8

62.6
55.3
45.9
243.7

81.4
70.1
55.3
26.1

66.8
61.6
47.0
26.3

70.3
62.5
60.6
19.2

71.4
66.4
56.8
18.3

2015
ICCV
78.1
72.6
61.2
5.6

2015
ICCVW
77.8
76.4
65.2
<1

HDT
2016

SCT
2016
2016
CVPR CVPR CVPR ECCV
77.8
74.2
73.0
63.0
63.5
55.7
∗
86.0
41.5

ECO MCCT-H MCCT
Staple SiamFc SRDCFdecon MDNet C-COT ACFN CSR-DCF ADNet MCPF
2017
2017
2016
2017
CVPR CVPR CVPR
87.0
84.6
80.6
84.1
77.5
77.2
73.5
66.9
-
∗
∗
15.0
0.6
2.9

2016
2017
ECCV CVPR
71.8
82.0
68.1
81.5
-
70.2
15.0
0.3

2017
CVPR
74.0
68.3
57.7
14.2

2016
CVPR
89.6
82.9
-
1.0

2016
CVPR
79.8
75.8
65.6
2.9

72.0
65.0
56.8
∗
10.5

90.7
86.4
74.4
∗
7.8

81.8
78.9
67.4
44.8

76.0
70.8
62.5
67.2

∗

∗

∗

SRDCF DeepSRDCDF

4.3. State of the art Comparison

We evaluate the proposed approach with 20 recent state-
of-the-art trackers including DSST [9], MEEM [51], SAMF
[26], KCF [16], LCT [32], HCF [31], SRDCF [12], Deep-
SRDCF [11], SCT [5], HDT [38], Staple [2], SiamFc [3],
SRDCFdecon [10], MDNet [35], C-COT [13], ACFN [6],
CSR-DCF [30], ADNet [50], MCPF [56], ECO [8]. A com-
parison with these state-of-the-art trackers using OP met-
ric on OTB-2013, OTB-2015 and Temple-Color datasets is
shown in Table 3.

Evaluation on OTB-2013. On the OTB-2013 [47]
benchmark, our proposed MCCT method achieves the best
OP of 90.7% (Table 3) and the highest area-under-curve
(AUC) score of 71.4% (Figure 5). The MDNet method
also exhibits excellent results and performs slightly better
than ours on DP metric (Figure 5), which is mainly due to
the effectiveness of the multi-domain network trained using
various similar tracking videos. Compared with other DCF
based methods, the proposed MCCT method outperform-
s the recent ECO, C-COT and MCPF on various metrics,
including on metrics OP, DP and AUC.

Evaluation on OTB-2015. On the OTB-2015 [48]
dataset, our proposed MCCT tracker achieves the best re-
sults on OP and DP with scores of 86.4% and 91.4% (Table
3 and Figure 6), outperforming the second best method by
2.3% and 1.3%. Among all the trackers, only ECO slightly
outperforms our method on AUC score. Our MCCT track-
er performs much better than HCF, HDT and MCPF, which
are also based on correlation ﬁlters with multiple types of
features.

The proposed MCCT-H method which only incorpo-
rates simple hand-crafted features achieves impressive per-
formance and operates at about 45 FPS on a single CPU
(Table 3).
It signiﬁcantly outperforms the DCF based
trackers with the same features (e.g., SAMF, SRDCF and
CSR-DCF) in both performance and speed. Furthermore,
MCCT-H achieves comparable results with the recent com-
plex trackers using deep features (e.g., ADNet, MCPF and
DeepSRDCF) but runs much faster than them. Compared
with other frame-wise adaption trackers (e.g., STC [5] and
ACFN [6]) that use attention mechanism for tracker con-
struction, our framework maintains several experts in par-
allel for decision-level selection and achieves better perfor-

1

0.8

0.6

0.4

0.2

t

e
a
r
 
s
s
e
c
c
u
S

0

0

1

0.8

0.6

0.4

0.2

e
t
a
r
 
s
s
e
c
c
u
S

0

0

1

0.8

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0.6

0.4

0.2

0

0

1

0.8

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0.6

0.4

0.2

0

0

Precision plots of OPE

Success plots of OPE

MDNet [93.2]
MCCT(Ours) [92.8]
ECO [91.3]
MCPF [90.3]
HCF [89.2]
C-COT [89.1]
ADNet [87.4]
HDT [87.0]
MCCTH(Ours) [85.6]
SRDCFdecon [85.3]

MCCT(Ours) [71.4]
ECO [71.2]
MDNet [71.2]
MCPF [68.2]
C-COT [68.0]
MCCT-H(Ours) [66.4]
SRDCFdecon [65.6]
ADNet [64.9]
DeepSRDCF [64.3]
LCT [64.0]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 5. Precision and success plots on the OTB-2013 [47] dataset
with 50 videos. Only the top 10 trackers are displayed for clarity.
In the legend, the DP at a threshold of 20 pixels and area-under-
curve (AUC) are reported in the left and right ﬁgures, respectively.

Precision plots of OPE

Success plots of OPE

MCCT(Ours) [91.4]
ECO [90.1]
C-COT [89.4]
MDNet [89.1]
MCPF [86.6]
ADNet [84.4]
DeepSRDCF [84.2]
MCCT-H(Ours) [84.1]
HCF [83.9]
HDT [83.6]

ECO [69.6]
MCCT(Ours) [69.5]
C-COT [68.1]
MDNet [67.6]
DeepSRDCF [64.2]
MCCT-H(Ours) [64.2]
ADNet [63.6]
MCPF [63.5]
SRDCFdecon [63.4]
SRDCF [60.9]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 6. Precision and success plots on the OTB-2015 [48] dataset
with 100 videos. In the legend, the DP (@20px) and AUC scores
are reported in the left and right ﬁgures, respectively.

mance (Table 3, Figure 5 and 6).

Evaluation on Temple-Color. For further evaluation,
we compare the proposed MCCT-H and MCCT methods on
the Temple-Color dataset [27] with the trackers mentioned
in Sec. 4.3 excluding MDNet, ADNet and ACFN, which all
need many external tracking videos for network training. In
contrast, our approach is free of such necessity.

In Table 3 and Figure 7, the proposed MCCT-H track-
er still achieves outstanding performance amongst real-time
trackers and outperforms some complex non-realtime track-
ers. Our MCCT method achieves the OP, DP and AUC s-
cores of (74.4%, 79.7%, 59.6%), while ECO and C-COT
yield (73.5%, 79.7%, 60.7%) and (70.2%, 78.1%, 58.3%),
respectively. Overall, our MCCT tracker shows compara-
ble results compared to the recent performance leader E-
CO and outperforms other state-of-the-art methods (e.g., C-
COT) on various metrics.

4850

Precision plots of OPE

Success plots of OPE

Expected overlap scores for baseline

1

0.8

0.6

0.4

0.2

e
t
a
r
 
n
o
s
c
e
r
P

i

i

0

0

MCCT(Ours) [79.7]
ECO [79.7]
C-COT [78.1]
MCPF [77.4]
DeepSRDCF [73.8]
SRDCFdecon [72.7]
MCCT-H(Ours) [72.0]
MEEM [69.9]
SiamFc [69.2]
HCF [68.9]

1

0.8

0.6

0.4

0.2

t

e
a
r
 
s
s
e
c
c
u
S

0

0

ECO [60.7]
MCCT(Ours) [59.6]
C-COT [58.3]
MCPF [55.3]
MCCT-H(Ours) [55.1]
DeepSRDCF [54.5]
SRDCFdecon [54.3]
Staple [50.9]
SiamFc [50.5]
MEEM [50.3]

10

20

30

40

50

0.2

0.4

0.6

0.8

1

Location error threshold (pixels)

Overlap threshold

Figure 7. Precision and success plots on the Temple-Color [27]
dataset with 128 color videos. In the legend, the DP (@20px) and
AUC scores are reported in the left and right ﬁgures, respectively.

Evaluation on VOT2016. Figure 8 shows the rank-
ing results in terms of expected average overlap (EAO) in
VOT2016 [21], from which we can observe that our MCC-
T tracker outperforms the top performer (C-COT [13]) by a
considerable margin. For presentation clarity, we only show
some top ranked and baseline trackers for comparison. For
more information, please refer to [21].

In Table 4, we list the detailed results of our approach-
es and the top ranked methods in VOT2016 (e.g., C-COT
[13], TCNN [34] and SSAT). Besides, two recently pro-
posed methods: CSR-DCF [30] and ECO [8] are also put
into comparison which do not participate in VOT2016. A-
mong all the compared methods, the proposed MCCT track-
er demonstrates advances in both accuracy and robustness.
As a result, our MCCT tracker provides the best EAO score
of 0.393, achieving a relative gain of 18.7% compared to
the VOT2016 top performer C-COT.

Discussion: (1) About Performance. It should be noted
that the ECO, C-COT and DeepSRDCF methods all take S-
RDCF as baseline, which can alleviate boundary effects ef-
fectively and performs much better than the standard DCF
[4, 16]. Besides, ECO and C-COT adopt the novel contin-
uous operator to better fuse the feature maps. However, all
the experts in our MCCT-H and MCCT methods just take
the simple DCF as baseline and all the strategies mentioned
above and other novel techniques [14, 33] can also be inte-
grated into our framework to further boost the performance.
(2) About Efﬁciency. Our tracker achieves almost the same
speed with expert VII because the same number of DCFs
and features are utilized (i.e., sharing strategy), but achieves
much better performance than it. Our general framework
provides a promising alternative for the multi-feature based
DCF trackers with ignorable impact on efﬁciency. Further-
more, we believe that other tracking algorithms [53, 36, 55]
with multiple types of features can also beneﬁt from our
multi-cue analysis framework.

5. Conclusion

In this paper, we propose a multi-cue analysis frame-
work for robust visual tracking, which considers not only

p
a
l
r
e
v
o

 

d
e
t
c
e
p
x
e

 

e
g
a
r
e
v
A

0.5

0.4

0.3

0.2

0.1

0

13

10

4

1

7
Order

TCNN
SRDCF

MCCT
EBT

MCCT-H
CCOT
DeepSRDCF MDNet

SSAT
HCF

MLDF
DSST

Staple
KCF

Figure 8. Expected Average Overlap (EAO) graph with trackers
ranked from right to left evaluated on VOT2016 [21]. Our pro-
posed MCCT tracker outperforms the top performer (C-COT [13])
by a considerable margin.

Table 4. The accuracy, robustness (failure rate) and EAO of state-
of-the-art methods on the VOT2016 [21]. The proposed MCCT
tracker achieves superior results compared to the top ranked meth-
ods in the challenge and recently proposed state-of-the-art algo-
rithms (ECO [8] and CSR-DCF [30]). The ﬁrst and second highest
values are highlighted by red and blue.

MCCT-H MLDF SSAT TCNN C-COT CSR-DCF ECO MCCT

Accuracy
Failure Rate
EAO

0.57
1.24
0.305

0.57
0.54
0.48
0.96
1.04
0.83
0.311 0.321 0.325

0.52
0.85
0.331

0.51
0.85
0.338

0.58
0.54
0.73
0.72
0.374 0.393

feature-level fusion but also decision-level fusion to fully
explore the strength of multiple features. Our framework
maintains multiple experts to track the target via different
views and selects the reliable outputs to reﬁne the tracking
results. Moreover, the proposed method evaluates the unre-
liable samples through considering the divergence of mul-
tiple experts and updates them adaptively. Through exten-
sive experiments on several challenging datasets, we show
that after adopting our simple yet effective multi-cue analy-
sis framework, without sophisticated models, only standard
Discriminative Correlation Filter (DCF) with deep or hand-
crafted features is able to perform favorably against state-
of-the-art methods in both accuracy and efﬁciency.

Acknowledgement. This work was supported in part
to Dr. Houqiang Li by 973 Pro-gram under contract No.
2015CB351803 and NSFC under contract No. 61390514,
in part to Dr. Wengang Zhou by NSFC under contract No.
61472378 and No. 61632019, the Fundamental Research
Funds for the Central Universities, and Young Elite Scien-
tists Sponsorship Program By CAST (2016QNRC001), and
in part to Dr. Qi Tian by ARO grant W911NF-15-1-0290
and Faculty Research Gift Awards by NEC Laboratories of
America and Blippar. This work was supported in part by
National Science Foundation of China (NSFC) 61429201.

4851

References

[1] C. Bailer, A. Pagani, and D. Stricker. A superior tracking ap-
proach: Building a strong tracker through fusion. In ECCV,
2014. 2

[2] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and
P. Torr. Staple: Complementary learners for real-time track-
ing. In CVPR, 2016. 2, 3, 7

[3] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and
P. H. Torr. Fully-convolutional siamese networks for object
tracking. In ECCV, 2016. 7

[4] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
CVPR, 2010. 1, 2, 3, 5, 6, 8

[5] J. Choi, H. Jin Chang, J. Jeong, Y. Demiris, and J. Y-
oung Choi. Visual tracking using attention-modulated dis-
integration and integration. In CVPR, 2016. 3, 7

[6] J. Choi, H. Jin Chang, S. Yun, T. Fischer, Y. Demiris, and
J. Young Choi. Attentional correlation ﬁlter network for
adaptive visual tracking. In CVPR, 2017. 7

[7] N. Dalal and B. Triggs. Histograms of oriented gradients for

human detection. In CVPR, 2005. 2, 3, 4

[8] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg.
Eco: Efﬁcient convolution operators for tracking. In CVPR,
2017. 2, 7, 8

[9] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accurate
scale estimation for robust visual tracking. In BMVC, 2014.
1, 3, 7

[10] M. Danelljan, G. H¨ager, F. S. Khan, and M. Felsberg. Adap-
tive decontamination of the training set: A uniﬁed formula-
tion for discriminative visual tracking. In CVPR, 2016. 2,
7

[11] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Fels-
berg. Convolutional features for correlation ﬁlter based vi-
sual tracking. In ICCV Workshop, 2015. 1, 2, 7

[12] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg.
Learning spatially regularized correlation ﬁlters for visual
tracking. In ICCV, 2015. 1, 2, 7

[13] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. In ECCV, 2016. 1, 2, 7, 8

[14] H. K. Galoogahi, A. Fagg, and S. Lucey.

Learning
background-aware correlation ﬁlters for visual tracking. In
ICCV, 2017. 1, 8

[15] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In ECCV, 2012. 2

[16] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
speed tracking with kernelized correlation ﬁlters. TPAMI,
37(3):583–596, 2015. 1, 2, 3, 6, 7, 8

[17] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychol-
In CVPR, 2015.
ogy inspired approach to object tracking.
1

[18] X. Jia, H. Lu, and M.-H. Yang. Visual tracking via adaptive
structural local sparse appearance model. In CVPR, 2012. 2
[19] O. Khalid, J. C. SanMiguel, and A. Cavallaro. Multi-tracker

partition fusion. TCSVT, 27(7):1527–1539, 2017. 2

[20] H. Kiani Galoogahi, T. Sim, and S. Lucey. Correlation ﬁlters

with limited boundaries. In CVPR, 2015. 1

[21] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, L. Cehovin,
G. Fern´andez, T. Vojir, Hager, and et al. The visual objec-
In ECCV Workshop,
t tracking vot2016 challenge results.
2016. 2, 6, 8

[22] M. Kristan, J. Matas, A. Leonardis, T. Voj´ıˇr, R. Pﬂugfelder,
G. Fernandez, G. Nebehay, F. Porikli, and L. ˇCehovin. A
novel performance evaluation methodology for single-target
trackers. TPAMI, 38(11):2137–2155, 2016. 6
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 1

Imagenet
In

[24] D. Y. Lee, J. Y. Sim, and C. S. Kim. Multihypothesis tra-
jectory analysis for robust visual tracking. In CVPR, 2015.
2

[25] J. Li, C. Deng, R. Y. D. Xu, D. Tao, and B. Zhao. Robust
object tracking with discrete graph based multiple experts.
TIP, 26(6):2736–2750, 2017. 2

[26] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. In ECCV Workshop, 2014.
1, 7

[27] P. Liang, E. Blasch, and H. Ling. Encoding color informa-
tion for visual tracking: algorithms and benchmark. TIP,
24(12):5630–5644, 2015. 2, 6, 7, 8

[28] S. Liu, T. Zhang, X. Cao, and C. Xu. Structural correlation

ﬁlter for robust visual tracking. In CVPR, 2016. 1

[29] T. Liu, G. Wang, and Q. Yang. Real-time part-based visual

tracking via adaptive correlation ﬁlters. In CVPR, 2015. 1

[30] A. Lukezic, T. Vojir, L. Cehovin Zajc, J. Matas, and M. Kris-
tan. Discriminative correlation ﬁlter with channel and spatial
reliability. In CVPR, 2017. 1, 2, 3, 7, 8

[31] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In ICCV, 2015. 1,
3, 4, 7

[32] C. Ma, X. Yang, C. Zhang, and M.-H. Yang. Long-term

correlation tracking. In CVPR, 2015. 1, 7

[33] M. Mueller, N. Smith, and B. Ghanem. Context-aware cor-

relation ﬁlter tracking. In CVPR, 2017. 1, 8

[34] H. Nam, M. Baek, and B. Han. Modeling and propagating
cnns in a tree structure for visual tracking. arXiv preprint
arXiv:1608.07242, 2016. 8

[35] H. Nam and B. Han. Learning multi-domain convolutional
neural networks for visual tracking. In CVPR, 2016. 1, 7
[36] J. Ning, J. Yang, S. Jiang, L. Zhang, and M.-H. Yang. Object
tracking via dual linear structured svm and explicit feature
map. In CVPR, 2016. 8

[37] H. Possegger, T. Mauthner, and H. Bischof.

In defense of

color-based model-free tracking. In CVPR, 2015. 3

[38] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-H.
Yang. Hedged deep tracking. In CVPR, 2016. 1, 2, 7
[39] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 1, 3, 6

[40] Y. Song, C. Ma, L. Gong, J. Zhang, R. Lau, and M.-H. Yang.
Crest: Convolutional residual learning for visual tracking. In
ICCV, 2017. 1

4852

[41] J. Valmadre, L. Bertinetto, J. F. Henriques, A. Vedaldi, and
P. H. Torr. End-to-end representation learning for correlation
ﬁlter based tracking. In CVPR, 2017. 1

[42] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural

networks for matlab. In ACM MM, 2014. 6

[43] N. Wang, J. Shi, D. Y. Yeung, and J. Jia. Understanding and
diagnosing visual tracking systems. In ICCV, 2015. 2, 4
[44] N. Wang and D. Y. Yeung. Ensemble-based tracking: Aggre-
gating crowdsourced structured time series data. In ICML,
2014. 2

[45] N. Wang, W. Zhou, and H. Li. Reliable re-detection for long-

term tracking. TCSVT, 2018. 1

[46] J. V. D. Weijer, C. Schmid, J. Verbeek, and D. Larlus.
TIP,
Learning color names for real-world applications.
18(7):1512–1523, 2009. 2, 4

[47] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A

benchmark. In CVPR, 2013. 2, 6, 7

[48] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.

TPAMI, 37(9):1834–1848, 2015. 1, 2, 6, 7

[49] A. Yilmaz, O. Javed, and M. Shah. Object tracking: A sur-

vey. ACM Computing Surveys, 38(4):13, 2006. 1

[50] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Young Choi. Action-
decision networks for visual tracking with deep reinforce-
ment learning. In CVPR, 2017. 7

[51] J. Zhang, S. Ma, and S. Sclaroff. Meem: robust tracking
via multiple experts using entropy minimization. In ECCV,
2014. 2, 7

[52] K. Zhang, L. Zhang, and M.-H. Yang. Real-time compres-

sive tracking. In ECCV, 2012. 1

[53] K. Zhang, L. Zhang, and M.-H. Yang. Real-time objec-
t tracking via online discriminative feature selection. TIP,
22(12):4664–4677, 2013. 8

[54] K. Zhang, L. Zhang, M.-H. Yang, and D. Zhang. Fast vi-
sual tracking via dense spatio-temporal context learning. In
ECCV, 2013. 2

[55] T. Zhang, A. Bibi, and B. Ghanem.

In defense of sparse

tracking: Circulant sparse tracker. In CVPR, 2016. 8
[56] T. Zhang, C. Xu, and M.-H. Yang. Multi-task correlation
particle ﬁlter for robust object tracking. In CVPR, 2017. 1, 7
[57] W. Zhong, H. Lu, and M.-H. Yang. Robust object tracking

via sparsity-based collaborative model. In CVPR, 2012. 2

4853

