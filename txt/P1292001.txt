Type-Sensitive Knowledge Base Inference
Without Explicit Type Supervision

Prachi Jain*1 and Pankaj Kumar*1 and Mausam1 and Soumen Chakrabarti2

1Indian Institute of Technology, Delhi
{p6.jain, k97.pankaj}@gmail.com, mausam@cse.iitd.ac.in
2Indian Institute of Technology, Bombay
soumen@cse.iitb.ac.in

Abstract

State-of-the-art knowledge base comple-
tion (KBC) models predict a score for ev-
ery known or unknown fact via a latent
factorization over entity and relation em-
beddings. We observe that when they
fail,
they often make entity predictions
that are incompatible with the type re-
quired by the relation.
In response, we
enhance each base factorization with two
type-compatibility terms between entity-
relation pairs, and combine the signals in
a novel manner. Without explicit super-
vision from a type catalog, our proposed
modiﬁcation obtains up to 7% MRR gains
over base models, and new state-of-the-art
results on several datasets. Further analy-
sis reveals that our models better represent
the latent types of entities and their embed-
dings also predict supervised types better
than the embeddings learned by baseline
models.

1

Introduction

Knowledge bases (KBs) store facts in the form of
relations (r) between subject entity (s) and object
entity (o), e.g., (cid:104)Obama, born-in, Hawaii(cid:105). Since
KBs are typically incomplete (Bollacker et al.,
2008), the task of KB Completion (KBC) attempts
to infer new tuples from a given KB. Neural ap-
proaches to KBC, e.g., Complex (Trouillon et al.,
2016) and DistMult (Yang et al., 2015), calculate
the score f (s, r, o) of a tuple (s, r, o) via a latent
factorization over entity and relation embeddings,
and use these scores to predict the validity of an
unseen tuple.

A model is evaluated over queries of the form
(cid:104)s∗, r∗, ?(cid:105). It ranks all entities o in the descend-

ing order of tuple scores f (s∗, r∗, o), and credit is
assigned based on the rank of gold entity o∗. Our
preliminary analysis of DistMult (DM) and Com-
plex (CX) reveals that they make frequent errors
by ranking entities that are not compatible with
types expected as arguments of r∗ high. In 19.5%
of predictions made by DM on FB15K, the top
prediction has a type different from what is ex-
pected (see Table 1 for illustrative examples).

In response, we propose a modiﬁcation to
base models (DM, Complex) by explicitly mod-
eling type compatibility. Our modiﬁed func-
tion f (cid:48)(s, r, o) is the product of three terms:
the original tuple score f (s, r, o), subject type-
compatibility between r and s, and object type-
compatibility between r and o. Our type-sensitive
models, TypeDM and TypeComplex, do not ex-
pect any additional type-speciﬁc supervision —
they induce all embeddings using only the origi-
nal KB.

Experiments over three datasets show that all
typed models outperform base models by signif-
icant margins, obtaining new state-of-the-art re-
sults in several cases. We perform additional anal-
yses to assess if the learned embeddings indeed
capture the type information well. We ﬁnd that
embeddings from typed models can predict known
symbolic types better than base models.

Finally, we note that an older model called E
(Riedel et al., 2013) can be seen as modeling
type compatibilities. Moreover, previous work
has explored additive combinations of DM and E
(Garcia-Duran et al., 2015b; Toutanova and Chen,
2015). We directly compare against these mod-
els and ﬁnd that, our proposal outperforms both E,
DM and their linear combinations.

We contribute open-source implementations1 of
all models and experiments discussed in this paper

*Equal contribution.

1https://github.com/dair-iitd/KBI

for further research.

2 Background and Related Work

We are given an incomplete KB with entities E
and relations R. The KB also contains T =
{(cid:104)s, r, o(cid:105)}, a set of known valid tuples, each with
subject and object entities s, o ∈ E, and relation
r ∈ R. Our goal is to predict the validity of any
tuple not present in T . Popular top performing
models for this task are Complex and DM.

(cid:17)

(cid:16)(cid:80)D

d=1 asdbrda(cid:63)
od

In Complex, each entity e (resp., relation r)
is represented as a complex vector aaae ∈ CD
(resp., bbbr ∈ CD). Tuple score fCX(s, r, o) =
(cid:60)
, where (cid:60)(z) is real part of
z, and z(cid:63) is complex conjugate of z. Holographic
embeddings (Nickel et al., 2016) are algebraically
In DM, each entity e
equivalent to Complex.
is represented as a vector aaae ∈ RD, each rela-
tion r as a vector bbbr ∈ RD, and the tuple score
fDM(s, r, o) = (cid:104)aaas, bbbr, aaao(cid:105) = (cid:80)D
d=1 asdbrdaod.
Earlier, Riedel et al. (2013) proposed a differ-
ent model called E: relation r is represented by
two vectors vvvr, wwwr ∈ RD, and the tuple score
fE(s, r, o) = aaas · vvvr + aaao · wwwr. E may be regarded
as a relation prediction model that depends purely
on type compatibility checking.

Observe that, in (cid:104)aaas, bbbr, aaao(cid:105), bbbr mediates a di-
rect compatibility between s and o for relation r,
whereas, in aaas·vvvr+aaao·wwwr, we are scoring how well
s can serve as subject and o as object of the rela-
tion r. Thus, in the second case, aaae is expected to
encode the type(s) of entity e, where, by ‘type’, we
loosely mean “information that helps decide if e
can participate in a relation r, as subject or object.”
Heuristic ﬁltering of the entities that do not match
the desired type at test time has been known to im-
prove accuracy (Toutanova et al., 2015; Krompaß
et al., 2015). Our typed models formalize this
within the embeddings and allow for discovery
of latent types without additional data. Krompaß
et al. (2015) also use heuristic typing of entities
for generating negative samples while training the
model. Our experiment ﬁnds that this approach is
not very competitive against our typed models.

3 TypeDM and TypeComplex

Representation: We start with DM as the base
model; the Complex case is identical. The ﬁrst
key modiﬁcation (see Figure 1) is that each entity
e is now represented by two vectors: uuue ∈ RK to
encode type information, and aaae ∈ RD(cid:48)
to encode

information. Typically, K (cid:28) D(cid:48). The second,
concomitant modiﬁcation is that each relation r is
now associated with three vectors: bbbr ∈ RD(cid:48)
as
before, and also vvvr, wwwr ∈ RK. vvvr and wwwr encode
the expected types for subject and object entities.
An ideal way to train type embeddings would
be to provide canonical type signatures for each
relation and entity. Unfortunately, these aspects
of realistic KBs are themselves incomplete (Nee-
lakantan and Chang, 2015; Murty et al., 2018).
Our models train all embeddings using T only and
don’t rely on any explicit type supervision.

DM uses (E + R)D model weights for a KB
with R relations and E entities, whereas TypeDM
uses E(D(cid:48) +K)+R(D(cid:48) +2K). To make compar-
isons fair, we set D(cid:48) and K so that the total number
of model weights (real or complex) are about the
same for base and typed models.

vvvr

bbbr

wwwr

uuus

aaas

uuuo

aaao

Cvvv

Cwww

f

f (cid:48)

Figure 1: TypeDM and TypeComplex.

Prediction: DM’s base prediction score for tu-
ple (s, r, o) is (cid:104)aaas, bbbr, aaao(cid:105). We apply a (sigmoid)
nonlinearity:

f (s, r, o) = σ((cid:104)aaas, bbbr, aaao(cid:105)),
(1)
and then combine with two additional terms that
measure type compatibility between the subject
and the relation, and the object and the relation:
f (cid:48)(s, r, o) = f (s, r, o) Cvvv(s, r) Cwww(o, r),
(2)
where Cxxx(e, r) is a function that measures the
compatibility between the type embedding of e for
a given argument slot of r:

Cxxx(e, r) = σ(xxxr · uuue)
(3)
If each of the three terms in Equation 2 is inter-
preted as a probability, f (cid:48)(s, r, o) corresponds to a
simple logical AND of the three conditions.

We want f (cid:48)(s, r, o) to be almost 1 for positive
instances (tuples known to be in the KG) and close
to 0 for negative instances (tuples not in the KG).
For a negative instance, one or more of the three
terms may be near zero. There is no guidance to
the learner on which term to drive down.

Gold Object o
Jewism (religion)

Relation r
follows-religion
headquarter-located-in El lay (location)
born-in-location

Subject s
Howard Leslie Shore
Spyglass Entertainment
Les Fradkin
Eugene Alden Hackman studied
Chief Phillips (ﬁlm)
Presidential Medal of Freedom (award)
Table 1: Samples of top two DM predictions (having inconsistent types) on FB15K. TypeDM predicts
entities of the correct type in top positions in the corresponding examples.

Prediction 2
21 Jump Street (ﬁlm)
Contraband (ﬁlm)
New York (location)
Louie De palma (person)
Rural Journalism (education) Loudon Snowden Wainwright III (person) The Bourne Legacy (ﬁlm)
Yankee land (location)

Prediction 1
Walk Hard (ﬁlm)
The Real World (tv)
Federico Fellini (person)

Akira Isida (person)

released-in-region

Contrastive Sampling: Training data consist of
positive gold tuples (s, r, o) and negative tuples,
which are obtained by perturbing each positive tu-
ple by replacing either s or o with a randomly
sampled s(cid:48) or o(cid:48). This offers the learning algo-
rithm positive and negative instances. The models
are trained such that observed tuples have higher
scores than unobserved ones.
Loss Functions: We implement two common
loss objectives. The log-likelihood loss ﬁrst com-
putes the probability of predicting a response o for
a query (s, r, ?) as follows:

Pr(o|s, r) =

exp(βf (cid:48)(s, r, o))
o(cid:48) exp(βf (cid:48)(s, r, o(cid:48)))

(cid:80)

(4)

Because f (cid:48) ∈ [0, 1] for typed models, we scale
it with a hyper-parameter β > 0 (a form of in-
verse temperature) to allow Pr(o|s, r) to take val-
ues over the full range [0, 1] in loss minimization.
The sum over o(cid:48) in the denominator is sampled
based on contrastive sampling, so the left hand
side is not a formal probability (exactly as in DM).
A similar term is added for Pr(s|r, o). The log-
likelihood loss minimizes:

(cid:88)

(cid:16)

−

(cid:104)s,r,o(cid:105)∈P

log P r(o|s, r; θ)

+ log P r(s|o, r; θ)

(5)

(cid:17)

The summation is over P which is the set of all
positive facts. Following Trouillon et al. (2016),
we also implement the logistic loss

(cid:88)

(cid:104)
1 + e−Ysrof (cid:48)(s,r,o)(cid:105)

log

(6)

(cid:104)s,r,o(cid:105)∈T

Here Ysro is 1 if the fact (s, r, o) is true and
−1 otherwise. Also, T is the set of all positive
facts along with the negative samples. With logis-
tic loss, model weights θ are L2-regularized and
gradient norm is clipped at 1.

4 Experiments

Datasets: We evaluate on three standard data
sets, FB15K, FB15K-237, and YAGO3-10 (Bor-

des et al., 2013; Toutanova et al., 2015; Dettmers
et al., 2017). We retain the exact train, dev and
test folds used in previous works. TypeDM and
TypeComplex are competitive on the WN18 data
set (Bordes et al., 2013), but we omit those results,
as WN18 has 18 very generic relations (e.g., hy-
ponym, hypernym, antonym, meronym), which do
not give enough evidence for inducing types.

Model

Embedding Number of
dimensions parameters
3,528,200
3,393,700
3,259,200
3,268,459
6,518,400
6,201,739

E
DM+E
DM
TypeDM
Complex
TypeComplex

200
100+100
200
180+19
200
180+19
Table 2: Sizes were approximately balanced be-
tween base and typed models (FB15K).

Metrics: As is common, we regard test instances
(s, r, ?) as a task of ranking o, with gold o∗ known.
We report MRR (Mean Reciprocal Rank) and the
fraction of queries where o∗ is recalled within
rank 1 and rank 10 (HITS). The ﬁltered evaluation
(Garcia-Duran et al., 2015a) removes valid train
or test tuples ranking above (s, r, o∗) for scoring
purposes.
Hyperparameters: We run AdaGrad for up to
1000 epochs for all losses, with early stopping on
the dev fold to prevent overﬁtting. All the mod-
els generally converge after 300-400 epochs, ex-
cept TypeDM that exhausts 1000 epochs. E, DM,
DM+E and Complex use 200 dimensional vectors.
All except E perform best with logistic loss and
20 negative samples (obtained by randomly cor-
rupting s and r) per positive fact. This is deter-
mined by doing a hyperparameter search on a set
{10, 20, 50, 100, 200, 400}.

For typed models we ﬁrst perform hyperparam-
eter search for size of type embeddings (K) such
that total entity embedding size remains 200. We
get the best results at K = 20, from among val-
ues in {10, 20, 30, 50, 80, 100, 120}. This hyper-
parameter search is done for the TypeDM model
(which is faster to train than TypeComplex) on
FB15k dataset, and the selected split is used for

Model
E
DM+E
DM
TypeDM
Complex
TypeComplex

FB15K

FB15K237
MRR HITS@1 HITS@10 MRR HITS@1 HITS@10 MRR HITS@1 HITS@10
23.40
60.84
67.47
75.01
70.50
75.44

17.39
49.53
56.52
66.07
61.00
66.32

35.29
79.70
84.86
87.92
86.09
88.51

21.30
38.15
37.21
38.70
37.58
38.93

14.51
28.06
27.43
29.30
26.97
29.57

36.38
58.02
56.12
57.36
55.98
57.50

7.87
52.48
55.31
58.16
54.86
58.65

6.22
38.72
46.80
51.36
46.90
51.62

10.00
77.40
70.76
70.08
69.08
70.42

YAGO3-10

Table 3: KBC performance for base, typed, and related formulations. Typed models outperform their
base models across all datasets.

all the typed models. To balance total model sizes
(Table 2), we choose K = 19 dimensions for
uuue, vvvr, wwwr and 180 dimensions for aaae, bbbr

2.

Typed models and E perform best with 400 neg-
ative samples per positive tuple while using log-
likelihood loss (robust to a larger number of neg-
ative facts as opposed to logistic loss, which falls
for class imbalance). FB15K and YAGO3-10 use
L2 regularization coefﬁcient of 2.0, and it is 5.0
for FB15K-237. Note that the L2 regularization
penalty is applied to only those entities and rela-
tions that are a part of that batch update, as pro-
posed by Trouillon et al. (2016). β is set to 20.0 for
the typed models, and 1.0 for other models if they
use the log-likelihood loss. Entity embeddings are
unit normalized at the end of every epoch, for the
type models. Also, we ﬁnd that in TypeDM scal-
ing the embeddings of the base model to unit norm
performs better than using L2 regularization.

Results: Table 3 shows that TypeDM and Type-
Complex dominate across all data sets. E by it-
self is understandably weak, and DM+E does not
lift it much. Each typed model improves upon the
corresponding base model on all measures, under-
scoring the value of type compatibility scores.3 To
the best of our knowledge, the results of our typed
models are competitive with various reported re-
sults for models of similar sizes that do not use any
additional information, e.g., soft rules (Guo et al.,
2018), or textual corpora (Toutanova et al., 2015).
We also compare against the heuristic genera-

2Notice that a typed model has a slightly higher number
of parameters for relation embeddings, because it needs to
maintain two type embeddings of size K, over and above bbbr.
Using K = 19 reduced and brought the total number of pa-
rameters closer to that of the base model, for a fair direct
comparison. The model performance did not differ by much
when using either of the options (i.e., K = 19 or 20).

3For direct comparisons with published work, we choose
200 and 400 parameters per entity for DM and Complex re-
spectively (Complex model has two 200 dimensional embed-
dings per entity). DM and TypeDM, on increasing the di-
mensionality to 400, yield MRR scores of 69.79 and 78.91,
respectively, for FB15K.

tion of type-sensitive negative samples (Krompaß
et al., 2015). For this experiment, we train a Com-
plex model using this heuristically generated nega-
tive set, and use standard evaluation, as in all other
models. We ﬁnd that all the models reported in Ta-
ble 3 outperform this approach.

(a)

(c)

(b)

(d)

represent-
Projection of vectors
Figure 2:
ing entities belonging to frequent KB types-
{people,
location,
ﬁlm,
b: TypeDM,aaae;
a: TypeDM,uuue;
sports}:
c: TypeComplex,uuue; d: DM,aaae.

organisation,

5 Analysis of Typed Embeddings

We perform two further analyses to assess whether
the embeddings produced by typed models indeed
capture type information better. For these exper-
iments, we try to correlate (and predict) known
symbolic types of an entity using the unsupervised
embeddings produced by the models. We take a
ﬁne catalog of most frequent 90 freebase types
over the 14,951 entities in the FB15k dataset (Xie
et al., 2016). We exclude /common/topic as
it occurs with most entities. On an average each
entity has 12 associated types.

1. Clustering Entity/Type Embeddings:
For
this experiment we subselect entities in FB15k that

Method

Size

H

C

Embed
-ding
uuue
aaae
Both
aaae
uuue
aaae
Both
aaae
uuue
aaae
Both
aaae

TypeDM
19
TypeDM
180
TypeDM
199
DM
200
TypeComplex
19
TypeComplex
180x2
TypeComplex
379
Complex
200x2
DM+E
19
DM+E
180
DM+E
199
E
200
Table 4: Interpretation of embeddings wrt super-
vised types: cluster homogeneity H, completeness
C, and type prediction F1 score.

66.29
59.67
66.29
48.12
62.97
48.57
63.09
47.20
2.05
47.24
47.26
37.62

66.72
57.89
66.75
51.40
65.90
50.76
66.03
51.56
0.48
49.62
49.66
39.83

Type
F1
81.77
75.96
82.57
81.34
82.70
74.75
84.14
81.58
74.66
82.72
82.68
74.23

belong to one of the 5 types (people, location,
organization, ﬁlm, and sports) from the freebase
dataset. These cover 84.88% of FB15K entities.
We plot the FB15K entities e using the PCA pro-
jection of uuue and aaae in Figure 2, color-coding their
types. We observe that uuue separates the type clus-
ters better than aaae, suggesting that uuue vectors in-
deed collect type information. We also perform
k-means clustering of uuue and aaae embeddings of
these entities, as available from different models.
We report cluster homogeneity and completeness
scores (Rosenberg and Hirschberg, 2007) in Ta-
ble 4. Typed models yield superior clusters.
2. Prediction of Symbolic Types: We train a
single-layer network that inputs embeddings from
various models and predicts a set of symbolic
types from the KB. This tells us the extent to
which the embeddings capture KB type informa-
tion (that was not provided explicitly during train-
ing). Table 4 reports average macro F1 score (5-
fold cross validation). Embeddings from TypeDM
and TypeComplex are generally better predictors
than embeddings learned by Complex, DM and E.
uuue ∈ R19 is often better than aaae ∈ R180 or more,
for typed models. DM+E with 199 model weights
narrowly beats TypeDM with 19 weights, but re-
call that it has poorer KBC scores.

6 Conclusion and Future Work

We propose an unsupervised typing gadget, which
enhances top-of-the-line base models for KBC
(DistMult, Complex) with two type-compatibility
functions, one between r and s and another be-
tween r and o. Without explicit supervision from
any type catalog, our typed variants (with simi-
lar number of parameters as base models) substan-

tially outperform base models, obtaining up to 7%
MRR improvements and over 10% improvements
in the correctness of the top result. To conﬁrm that
our models capture type information better, we
correlate the embeddings learned without type su-
pervision with existing type catalogs. We ﬁnd that
our embeddings indeed separate and predict types
better.
In future work, combining type-sensitive
embeddings with a focus on less frequent relations
(Xie et al., 2017), more frequent entities (Dettmers
et al., 2017), or side information such as inference
rules (Guo et al., 2018; Jain and Mausam, 2016) or
textual corpora (Toutanova et al., 2015) may fur-
ther increase KBC accuracy. It may also be of in-
terest to integrate the typing approach here with
the combinations of tensor and matrix factoriza-
tion models for KBC (Jain et al., 2018).

Acknowledgements

This work is supported by Google language un-
derstanding and knowledge discovery focused re-
search grants, a Bloomberg award, an IBM SUR
award, and a Visvesvaraya faculty award by Govt.
It is also supported
of India to the third author.
by a TCS Fellowship to the ﬁrst author. We thank
Microsoft Azure sponsorships, IIT Delhi HPC fa-
cility and the support of nVidia Corporation for
computational resources.

References

Praveen

Kurt Bollacker, Colin Evans,

tosh, Tim Sturge,
Freebase:
database
In
http://ids.snu.ac.kr/w/images/9/98/sc17.pdf.

Pari-
and Jamie Taylor. 2008.
graph
knowledge.
1247–1250.

structuring
SIGMOD Conference.

collaboratively

human
pages

created

for

a

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
In NIPS Conference. pages 2787–
relational data.
2795. http://papers.nips.cc/paper/5071-translating-
embeddings-for-modeling-multi-relational-data.pdf.

Tim Dettmers, Pasquale Minervini, Pontus Stene-
torp, and Sebastian Riedel. 2017. Convolutional
2d knowledge graph embeddings. arXiv preprint
arXiv:1707.01476 .

Alberto Garcia-Duran, Antoine Bordes, and Nico-
las Usunier. 2015a. Composing relationships with
In EMNLP Conference. pages 286–
translations.
290. http://www.aclweb.org/anthology/D15-1034.

Alberto Garcia-Duran, Antoine Bordes, Nicolas
Usunier, and Yves Grandvalet. 2015b. Combin-

ing two and three-way embeddings models for link
arXiv preprint
prediction in knowledge bases.
arXiv:1506.00999 https://arxiv.org/pdf/1506.00999.

Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and
Li Guo. 2018. Knowledge graph embedding with
In Proceedings
iterative guidance from soft rules.
of the Thirty-Second AAAI Conference on Artiﬁcial
Intelligence.

Prachi Jain and Mausam. 2016. Knowledge-guided lin-
guistic rewrites for inference rule veriﬁcation.
In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
pages 86–92.

Prachi Jain, Shikhar Murty, Mausam, and Soumen
Chakrabarti. 2018. Mitigating the effect of out-of-
vocabulary entity pairs in matrix factorization for kb
inference. In Proceedings of the Twenty-Sixth Inter-
national Joint Conference on Artiﬁcial Intelligence,
IJCAI-18.

Denis Krompaß, Stephan Baier, and Volker Tresp.
2015. Type-constrained representation learning in
In International Semantic Web
knowledge graphs.
Conference. Springer, pages 640–655.

Shikhar Murty, Patrik Verga, Luke Vilnis,

Irena
Radovanovic, and Andrew McCallum. 2018. Hier-
archical losses and new resources for ﬁne-grained
In Proceedings of the
entity typing and linking.
56th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.

Arvind Neelakantan and Ming-Wei Chang. 2015. In-
ferring missing entity type instances for knowledge
In
base completion: New dataset and methods.
NAACL .

Sebastian Riedel, Limin Yao, Andrew McCallum,
Relation ex-
and Benjamin M Marlin. 2013.
traction with matrix factorization and universal
In NAACL Conference. pages 74–
schemas.
84. http://www.anthology.aclweb.org/N/N13/N13-
1008.pdf.

Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
In EMNLP Conference.
ter evaluation measure.
http://aclweb.org/anthology/D/D07/D07-1043.pdf.

Kristina Toutanova and Danqi Chen. 2015.

Ob-
served versus latent features for knowledge base
the
and text
3rd Workshop on Continuous Vector Space Mod-
els and their Compositionality. pages 57–66.
http://www.aclweb.org/anthology/W15-4007.

In Proceedings of

inference.

Kristina Toutanova, Danqi Chen, Patrick Pan-
and
tel, Hoifung Poon, Pallavi Choudhury,
Michael Gamon. 2015.
Representing text for
joint embedding of text and knowledge bases.
In EMNLP Conference.
1499–1509.
https://www.aclweb.org/anthology/D/D15/D15-
1174.pdf.

pages

Th´eo Trouillon, Johannes Welbl, Sebastian Riedel, ´Eric
Gaussier, and Guillaume Bouchard. 2016. Complex
In ICML.
embeddings for simple link prediction.
pages 2071–2080. http://arxiv.org/abs/1606.06357.

Qizhe Xie, Xuezhe Ma, Zihang Dai,

uard Hovy. 2017.
edge transfer model
arXiv
pletion.
https://arxiv.org/pdf/1704.05908.pdf.

and Ed-
An interpretable knowl-
for knowledge base com-
arXiv:1704.05908
preprint

Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016.
Representation learning of knowledge graphs with
hierarchical types. In IJCAI. pages 2965–2971.

Maximilian Nickel, Lorenzo Rosasco, Tomaso A Pog-
gio, et al. 2016. Holographic embeddings of knowl-
In AAAI Conference. pages 1955–
edge graphs.
1961. https://arxiv.org/abs/1510.04935.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding entities and
relations for learning and inference in knowledge

bases. In ICLR.

Type-Sensitive Knowledge Base Inference
Without Explicit Type Supervision

Prachi Jain*1 and Pankaj Kumar*1 and Mausam1 and Soumen Chakrabarti2

1Indian Institute of Technology, Delhi
{p6.jain, k97.pankaj}@gmail.com, mausam@cse.iitd.ac.in
2Indian Institute of Technology, Bombay
soumen@cse.iitb.ac.in

Abstract

State-of-the-art knowledge base comple-
tion (KBC) models predict a score for ev-
ery known or unknown fact via a latent
factorization over entity and relation em-
beddings. We observe that when they
fail,
they often make entity predictions
that are incompatible with the type re-
quired by the relation.
In response, we
enhance each base factorization with two
type-compatibility terms between entity-
relation pairs, and combine the signals in
a novel manner. Without explicit super-
vision from a type catalog, our proposed
modiﬁcation obtains up to 7% MRR gains
over base models, and new state-of-the-art
results on several datasets. Further analy-
sis reveals that our models better represent
the latent types of entities and their embed-
dings also predict supervised types better
than the embeddings learned by baseline
models.

1

Introduction

Knowledge bases (KBs) store facts in the form of
relations (r) between subject entity (s) and object
entity (o), e.g., (cid:104)Obama, born-in, Hawaii(cid:105). Since
KBs are typically incomplete (Bollacker et al.,
2008), the task of KB Completion (KBC) attempts
to infer new tuples from a given KB. Neural ap-
proaches to KBC, e.g., Complex (Trouillon et al.,
2016) and DistMult (Yang et al., 2015), calculate
the score f (s, r, o) of a tuple (s, r, o) via a latent
factorization over entity and relation embeddings,
and use these scores to predict the validity of an
unseen tuple.

A model is evaluated over queries of the form
(cid:104)s∗, r∗, ?(cid:105). It ranks all entities o in the descend-

ing order of tuple scores f (s∗, r∗, o), and credit is
assigned based on the rank of gold entity o∗. Our
preliminary analysis of DistMult (DM) and Com-
plex (CX) reveals that they make frequent errors
by ranking entities that are not compatible with
types expected as arguments of r∗ high. In 19.5%
of predictions made by DM on FB15K, the top
prediction has a type different from what is ex-
pected (see Table 1 for illustrative examples).

In response, we propose a modiﬁcation to
base models (DM, Complex) by explicitly mod-
eling type compatibility. Our modiﬁed func-
tion f (cid:48)(s, r, o) is the product of three terms:
the original tuple score f (s, r, o), subject type-
compatibility between r and s, and object type-
compatibility between r and o. Our type-sensitive
models, TypeDM and TypeComplex, do not ex-
pect any additional type-speciﬁc supervision —
they induce all embeddings using only the origi-
nal KB.

Experiments over three datasets show that all
typed models outperform base models by signif-
icant margins, obtaining new state-of-the-art re-
sults in several cases. We perform additional anal-
yses to assess if the learned embeddings indeed
capture the type information well. We ﬁnd that
embeddings from typed models can predict known
symbolic types better than base models.

Finally, we note that an older model called E
(Riedel et al., 2013) can be seen as modeling
type compatibilities. Moreover, previous work
has explored additive combinations of DM and E
(Garcia-Duran et al., 2015b; Toutanova and Chen,
2015). We directly compare against these mod-
els and ﬁnd that, our proposal outperforms both E,
DM and their linear combinations.

We contribute open-source implementations1 of
all models and experiments discussed in this paper

*Equal contribution.

1https://github.com/dair-iitd/KBI

for further research.

2 Background and Related Work

We are given an incomplete KB with entities E
and relations R. The KB also contains T =
{(cid:104)s, r, o(cid:105)}, a set of known valid tuples, each with
subject and object entities s, o ∈ E, and relation
r ∈ R. Our goal is to predict the validity of any
tuple not present in T . Popular top performing
models for this task are Complex and DM.

(cid:17)

(cid:16)(cid:80)D

d=1 asdbrda(cid:63)
od

In Complex, each entity e (resp., relation r)
is represented as a complex vector aaae ∈ CD
(resp., bbbr ∈ CD). Tuple score fCX(s, r, o) =
(cid:60)
, where (cid:60)(z) is real part of
z, and z(cid:63) is complex conjugate of z. Holographic
embeddings (Nickel et al., 2016) are algebraically
In DM, each entity e
equivalent to Complex.
is represented as a vector aaae ∈ RD, each rela-
tion r as a vector bbbr ∈ RD, and the tuple score
fDM(s, r, o) = (cid:104)aaas, bbbr, aaao(cid:105) = (cid:80)D
d=1 asdbrdaod.
Earlier, Riedel et al. (2013) proposed a differ-
ent model called E: relation r is represented by
two vectors vvvr, wwwr ∈ RD, and the tuple score
fE(s, r, o) = aaas · vvvr + aaao · wwwr. E may be regarded
as a relation prediction model that depends purely
on type compatibility checking.

Observe that, in (cid:104)aaas, bbbr, aaao(cid:105), bbbr mediates a di-
rect compatibility between s and o for relation r,
whereas, in aaas·vvvr+aaao·wwwr, we are scoring how well
s can serve as subject and o as object of the rela-
tion r. Thus, in the second case, aaae is expected to
encode the type(s) of entity e, where, by ‘type’, we
loosely mean “information that helps decide if e
can participate in a relation r, as subject or object.”
Heuristic ﬁltering of the entities that do not match
the desired type at test time has been known to im-
prove accuracy (Toutanova et al., 2015; Krompaß
et al., 2015). Our typed models formalize this
within the embeddings and allow for discovery
of latent types without additional data. Krompaß
et al. (2015) also use heuristic typing of entities
for generating negative samples while training the
model. Our experiment ﬁnds that this approach is
not very competitive against our typed models.

3 TypeDM and TypeComplex

Representation: We start with DM as the base
model; the Complex case is identical. The ﬁrst
key modiﬁcation (see Figure 1) is that each entity
e is now represented by two vectors: uuue ∈ RK to
encode type information, and aaae ∈ RD(cid:48)
to encode

information. Typically, K (cid:28) D(cid:48). The second,
concomitant modiﬁcation is that each relation r is
now associated with three vectors: bbbr ∈ RD(cid:48)
as
before, and also vvvr, wwwr ∈ RK. vvvr and wwwr encode
the expected types for subject and object entities.
An ideal way to train type embeddings would
be to provide canonical type signatures for each
relation and entity. Unfortunately, these aspects
of realistic KBs are themselves incomplete (Nee-
lakantan and Chang, 2015; Murty et al., 2018).
Our models train all embeddings using T only and
don’t rely on any explicit type supervision.

DM uses (E + R)D model weights for a KB
with R relations and E entities, whereas TypeDM
uses E(D(cid:48) +K)+R(D(cid:48) +2K). To make compar-
isons fair, we set D(cid:48) and K so that the total number
of model weights (real or complex) are about the
same for base and typed models.

vvvr

bbbr

wwwr

uuus

aaas

uuuo

aaao

Cvvv

Cwww

f

f (cid:48)

Figure 1: TypeDM and TypeComplex.

Prediction: DM’s base prediction score for tu-
ple (s, r, o) is (cid:104)aaas, bbbr, aaao(cid:105). We apply a (sigmoid)
nonlinearity:

f (s, r, o) = σ((cid:104)aaas, bbbr, aaao(cid:105)),
(1)
and then combine with two additional terms that
measure type compatibility between the subject
and the relation, and the object and the relation:
f (cid:48)(s, r, o) = f (s, r, o) Cvvv(s, r) Cwww(o, r),
(2)
where Cxxx(e, r) is a function that measures the
compatibility between the type embedding of e for
a given argument slot of r:

Cxxx(e, r) = σ(xxxr · uuue)
(3)
If each of the three terms in Equation 2 is inter-
preted as a probability, f (cid:48)(s, r, o) corresponds to a
simple logical AND of the three conditions.

We want f (cid:48)(s, r, o) to be almost 1 for positive
instances (tuples known to be in the KG) and close
to 0 for negative instances (tuples not in the KG).
For a negative instance, one or more of the three
terms may be near zero. There is no guidance to
the learner on which term to drive down.

Gold Object o
Jewism (religion)

Relation r
follows-religion
headquarter-located-in El lay (location)
born-in-location

Subject s
Howard Leslie Shore
Spyglass Entertainment
Les Fradkin
Eugene Alden Hackman studied
Chief Phillips (ﬁlm)
Presidential Medal of Freedom (award)
Table 1: Samples of top two DM predictions (having inconsistent types) on FB15K. TypeDM predicts
entities of the correct type in top positions in the corresponding examples.

Prediction 2
21 Jump Street (ﬁlm)
Contraband (ﬁlm)
New York (location)
Louie De palma (person)
Rural Journalism (education) Loudon Snowden Wainwright III (person) The Bourne Legacy (ﬁlm)
Yankee land (location)

Prediction 1
Walk Hard (ﬁlm)
The Real World (tv)
Federico Fellini (person)

Akira Isida (person)

released-in-region

Contrastive Sampling: Training data consist of
positive gold tuples (s, r, o) and negative tuples,
which are obtained by perturbing each positive tu-
ple by replacing either s or o with a randomly
sampled s(cid:48) or o(cid:48). This offers the learning algo-
rithm positive and negative instances. The models
are trained such that observed tuples have higher
scores than unobserved ones.
Loss Functions: We implement two common
loss objectives. The log-likelihood loss ﬁrst com-
putes the probability of predicting a response o for
a query (s, r, ?) as follows:

Pr(o|s, r) =

exp(βf (cid:48)(s, r, o))
o(cid:48) exp(βf (cid:48)(s, r, o(cid:48)))

(cid:80)

(4)

Because f (cid:48) ∈ [0, 1] for typed models, we scale
it with a hyper-parameter β > 0 (a form of in-
verse temperature) to allow Pr(o|s, r) to take val-
ues over the full range [0, 1] in loss minimization.
The sum over o(cid:48) in the denominator is sampled
based on contrastive sampling, so the left hand
side is not a formal probability (exactly as in DM).
A similar term is added for Pr(s|r, o). The log-
likelihood loss minimizes:

(cid:88)

(cid:16)

−

(cid:104)s,r,o(cid:105)∈P

log P r(o|s, r; θ)

+ log P r(s|o, r; θ)

(5)

(cid:17)

The summation is over P which is the set of all
positive facts. Following Trouillon et al. (2016),
we also implement the logistic loss

(cid:88)

(cid:104)
1 + e−Ysrof (cid:48)(s,r,o)(cid:105)

log

(6)

(cid:104)s,r,o(cid:105)∈T

Here Ysro is 1 if the fact (s, r, o) is true and
−1 otherwise. Also, T is the set of all positive
facts along with the negative samples. With logis-
tic loss, model weights θ are L2-regularized and
gradient norm is clipped at 1.

4 Experiments

Datasets: We evaluate on three standard data
sets, FB15K, FB15K-237, and YAGO3-10 (Bor-

des et al., 2013; Toutanova et al., 2015; Dettmers
et al., 2017). We retain the exact train, dev and
test folds used in previous works. TypeDM and
TypeComplex are competitive on the WN18 data
set (Bordes et al., 2013), but we omit those results,
as WN18 has 18 very generic relations (e.g., hy-
ponym, hypernym, antonym, meronym), which do
not give enough evidence for inducing types.

Model

Embedding Number of
dimensions parameters
3,528,200
3,393,700
3,259,200
3,268,459
6,518,400
6,201,739

E
DM+E
DM
TypeDM
Complex
TypeComplex

200
100+100
200
180+19
200
180+19
Table 2: Sizes were approximately balanced be-
tween base and typed models (FB15K).

Metrics: As is common, we regard test instances
(s, r, ?) as a task of ranking o, with gold o∗ known.
We report MRR (Mean Reciprocal Rank) and the
fraction of queries where o∗ is recalled within
rank 1 and rank 10 (HITS). The ﬁltered evaluation
(Garcia-Duran et al., 2015a) removes valid train
or test tuples ranking above (s, r, o∗) for scoring
purposes.
Hyperparameters: We run AdaGrad for up to
1000 epochs for all losses, with early stopping on
the dev fold to prevent overﬁtting. All the mod-
els generally converge after 300-400 epochs, ex-
cept TypeDM that exhausts 1000 epochs. E, DM,
DM+E and Complex use 200 dimensional vectors.
All except E perform best with logistic loss and
20 negative samples (obtained by randomly cor-
rupting s and r) per positive fact. This is deter-
mined by doing a hyperparameter search on a set
{10, 20, 50, 100, 200, 400}.

For typed models we ﬁrst perform hyperparam-
eter search for size of type embeddings (K) such
that total entity embedding size remains 200. We
get the best results at K = 20, from among val-
ues in {10, 20, 30, 50, 80, 100, 120}. This hyper-
parameter search is done for the TypeDM model
(which is faster to train than TypeComplex) on
FB15k dataset, and the selected split is used for

Model
E
DM+E
DM
TypeDM
Complex
TypeComplex

FB15K

FB15K237
MRR HITS@1 HITS@10 MRR HITS@1 HITS@10 MRR HITS@1 HITS@10
23.40
60.84
67.47
75.01
70.50
75.44

17.39
49.53
56.52
66.07
61.00
66.32

35.29
79.70
84.86
87.92
86.09
88.51

21.30
38.15
37.21
38.70
37.58
38.93

14.51
28.06
27.43
29.30
26.97
29.57

36.38
58.02
56.12
57.36
55.98
57.50

7.87
52.48
55.31
58.16
54.86
58.65

6.22
38.72
46.80
51.36
46.90
51.62

10.00
77.40
70.76
70.08
69.08
70.42

YAGO3-10

Table 3: KBC performance for base, typed, and related formulations. Typed models outperform their
base models across all datasets.

all the typed models. To balance total model sizes
(Table 2), we choose K = 19 dimensions for
uuue, vvvr, wwwr and 180 dimensions for aaae, bbbr

2.

Typed models and E perform best with 400 neg-
ative samples per positive tuple while using log-
likelihood loss (robust to a larger number of neg-
ative facts as opposed to logistic loss, which falls
for class imbalance). FB15K and YAGO3-10 use
L2 regularization coefﬁcient of 2.0, and it is 5.0
for FB15K-237. Note that the L2 regularization
penalty is applied to only those entities and rela-
tions that are a part of that batch update, as pro-
posed by Trouillon et al. (2016). β is set to 20.0 for
the typed models, and 1.0 for other models if they
use the log-likelihood loss. Entity embeddings are
unit normalized at the end of every epoch, for the
type models. Also, we ﬁnd that in TypeDM scal-
ing the embeddings of the base model to unit norm
performs better than using L2 regularization.

Results: Table 3 shows that TypeDM and Type-
Complex dominate across all data sets. E by it-
self is understandably weak, and DM+E does not
lift it much. Each typed model improves upon the
corresponding base model on all measures, under-
scoring the value of type compatibility scores.3 To
the best of our knowledge, the results of our typed
models are competitive with various reported re-
sults for models of similar sizes that do not use any
additional information, e.g., soft rules (Guo et al.,
2018), or textual corpora (Toutanova et al., 2015).
We also compare against the heuristic genera-

2Notice that a typed model has a slightly higher number
of parameters for relation embeddings, because it needs to
maintain two type embeddings of size K, over and above bbbr.
Using K = 19 reduced and brought the total number of pa-
rameters closer to that of the base model, for a fair direct
comparison. The model performance did not differ by much
when using either of the options (i.e., K = 19 or 20).

3For direct comparisons with published work, we choose
200 and 400 parameters per entity for DM and Complex re-
spectively (Complex model has two 200 dimensional embed-
dings per entity). DM and TypeDM, on increasing the di-
mensionality to 400, yield MRR scores of 69.79 and 78.91,
respectively, for FB15K.

tion of type-sensitive negative samples (Krompaß
et al., 2015). For this experiment, we train a Com-
plex model using this heuristically generated nega-
tive set, and use standard evaluation, as in all other
models. We ﬁnd that all the models reported in Ta-
ble 3 outperform this approach.

(a)

(c)

(b)

(d)

represent-
Projection of vectors
Figure 2:
ing entities belonging to frequent KB types-
{people,
location,
ﬁlm,
b: TypeDM,aaae;
a: TypeDM,uuue;
sports}:
c: TypeComplex,uuue; d: DM,aaae.

organisation,

5 Analysis of Typed Embeddings

We perform two further analyses to assess whether
the embeddings produced by typed models indeed
capture type information better. For these exper-
iments, we try to correlate (and predict) known
symbolic types of an entity using the unsupervised
embeddings produced by the models. We take a
ﬁne catalog of most frequent 90 freebase types
over the 14,951 entities in the FB15k dataset (Xie
et al., 2016). We exclude /common/topic as
it occurs with most entities. On an average each
entity has 12 associated types.

1. Clustering Entity/Type Embeddings:
For
this experiment we subselect entities in FB15k that

Method

Size

H

C

Embed
-ding
uuue
aaae
Both
aaae
uuue
aaae
Both
aaae
uuue
aaae
Both
aaae

TypeDM
19
TypeDM
180
TypeDM
199
DM
200
TypeComplex
19
TypeComplex
180x2
TypeComplex
379
Complex
200x2
DM+E
19
DM+E
180
DM+E
199
E
200
Table 4: Interpretation of embeddings wrt super-
vised types: cluster homogeneity H, completeness
C, and type prediction F1 score.

66.72
57.89
66.75
51.40
65.90
50.76
66.03
51.56
0.48
49.62
49.66
39.83

66.29
59.67
66.29
48.12
62.97
48.57
63.09
47.20
2.05
47.24
47.26
37.62

Type
F1
81.77
75.96
82.57
81.34
82.70
74.75
84.14
81.58
74.66
82.72
82.68
74.23

belong to one of the 5 types (people, location,
organization, ﬁlm, and sports) from the freebase
dataset. These cover 84.88% of FB15K entities.
We plot the FB15K entities e using the PCA pro-
jection of uuue and aaae in Figure 2, color-coding their
types. We observe that uuue separates the type clus-
ters better than aaae, suggesting that uuue vectors in-
deed collect type information. We also perform
k-means clustering of uuue and aaae embeddings of
these entities, as available from different models.
We report cluster homogeneity and completeness
scores (Rosenberg and Hirschberg, 2007) in Ta-
ble 4. Typed models yield superior clusters.
2. Prediction of Symbolic Types: We train a
single-layer network that inputs embeddings from
various models and predicts a set of symbolic
types from the KB. This tells us the extent to
which the embeddings capture KB type informa-
tion (that was not provided explicitly during train-
ing). Table 4 reports average macro F1 score (5-
fold cross validation). Embeddings from TypeDM
and TypeComplex are generally better predictors
than embeddings learned by Complex, DM and E.
uuue ∈ R19 is often better than aaae ∈ R180 or more,
for typed models. DM+E with 199 model weights
narrowly beats TypeDM with 19 weights, but re-
call that it has poorer KBC scores.

6 Conclusion and Future Work

We propose an unsupervised typing gadget, which
enhances top-of-the-line base models for KBC
(DistMult, Complex) with two type-compatibility
functions, one between r and s and another be-
tween r and o. Without explicit supervision from
any type catalog, our typed variants (with simi-
lar number of parameters as base models) substan-

tially outperform base models, obtaining up to 7%
MRR improvements and over 10% improvements
in the correctness of the top result. To conﬁrm that
our models capture type information better, we
correlate the embeddings learned without type su-
pervision with existing type catalogs. We ﬁnd that
our embeddings indeed separate and predict types
better.
In future work, combining type-sensitive
embeddings with a focus on less frequent relations
(Xie et al., 2017), more frequent entities (Dettmers
et al., 2017), or side information such as inference
rules (Guo et al., 2018; Jain and Mausam, 2016) or
textual corpora (Toutanova et al., 2015) may fur-
ther increase KBC accuracy. It may also be of in-
terest to integrate the typing approach here with
the combinations of tensor and matrix factoriza-
tion models for KBC (Jain et al., 2018).

Acknowledgements

This work is supported by Google language un-
derstanding and knowledge discovery focused re-
search grants, a Bloomberg award, an IBM SUR
award, and a Visvesvaraya faculty award by Govt.
It is also supported
of India to the third author.
by a TCS Fellowship to the ﬁrst author. We thank
Microsoft Azure sponsorships, IIT Delhi HPC fa-
cility and the support of nVidia Corporation for
computational resources.

References

Praveen

Kurt Bollacker, Colin Evans,

tosh, Tim Sturge,
Freebase:
database
In
http://ids.snu.ac.kr/w/images/9/98/sc17.pdf.

Pari-
and Jamie Taylor. 2008.
graph
knowledge.
1247–1250.

structuring
SIGMOD Conference.

collaboratively

human
pages

created

for

a

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
In NIPS Conference. pages 2787–
relational data.
2795. http://papers.nips.cc/paper/5071-translating-
embeddings-for-modeling-multi-relational-data.pdf.

Tim Dettmers, Pasquale Minervini, Pontus Stene-
torp, and Sebastian Riedel. 2017. Convolutional
2d knowledge graph embeddings. arXiv preprint
arXiv:1707.01476 .

Alberto Garcia-Duran, Antoine Bordes, and Nico-
las Usunier. 2015a. Composing relationships with
In EMNLP Conference. pages 286–
translations.
290. http://www.aclweb.org/anthology/D15-1034.

Alberto Garcia-Duran, Antoine Bordes, Nicolas
Usunier, and Yves Grandvalet. 2015b. Combin-

ing two and three-way embeddings models for link
arXiv preprint
prediction in knowledge bases.
arXiv:1506.00999 https://arxiv.org/pdf/1506.00999.

Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and
Li Guo. 2018. Knowledge graph embedding with
In Proceedings
iterative guidance from soft rules.
of the Thirty-Second AAAI Conference on Artiﬁcial
Intelligence.

Prachi Jain and Mausam. 2016. Knowledge-guided lin-
guistic rewrites for inference rule veriﬁcation.
In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
pages 86–92.

Prachi Jain, Shikhar Murty, Mausam, and Soumen
Chakrabarti. 2018. Mitigating the effect of out-of-
vocabulary entity pairs in matrix factorization for kb
inference. In Proceedings of the Twenty-Sixth Inter-
national Joint Conference on Artiﬁcial Intelligence,
IJCAI-18.

Denis Krompaß, Stephan Baier, and Volker Tresp.
2015. Type-constrained representation learning in
In International Semantic Web
knowledge graphs.
Conference. Springer, pages 640–655.

Shikhar Murty, Patrik Verga, Luke Vilnis,

Irena
Radovanovic, and Andrew McCallum. 2018. Hier-
archical losses and new resources for ﬁne-grained
In Proceedings of the
entity typing and linking.
56th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.

Arvind Neelakantan and Ming-Wei Chang. 2015. In-
ferring missing entity type instances for knowledge
In
base completion: New dataset and methods.
NAACL .

Sebastian Riedel, Limin Yao, Andrew McCallum,
Relation ex-
and Benjamin M Marlin. 2013.
traction with matrix factorization and universal
In NAACL Conference. pages 74–
schemas.
84. http://www.anthology.aclweb.org/N/N13/N13-
1008.pdf.

Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
In EMNLP Conference.
ter evaluation measure.
http://aclweb.org/anthology/D/D07/D07-1043.pdf.

Kristina Toutanova and Danqi Chen. 2015.

Ob-
served versus latent features for knowledge base
the
and text
3rd Workshop on Continuous Vector Space Mod-
els and their Compositionality. pages 57–66.
http://www.aclweb.org/anthology/W15-4007.

In Proceedings of

inference.

Kristina Toutanova, Danqi Chen, Patrick Pan-
and
tel, Hoifung Poon, Pallavi Choudhury,
Michael Gamon. 2015.
Representing text for
joint embedding of text and knowledge bases.
In EMNLP Conference.
1499–1509.
https://www.aclweb.org/anthology/D/D15/D15-
1174.pdf.

pages

Th´eo Trouillon, Johannes Welbl, Sebastian Riedel, ´Eric
Gaussier, and Guillaume Bouchard. 2016. Complex
In ICML.
embeddings for simple link prediction.
pages 2071–2080. http://arxiv.org/abs/1606.06357.

Qizhe Xie, Xuezhe Ma, Zihang Dai,

uard Hovy. 2017.
edge transfer model
arXiv
pletion.
https://arxiv.org/pdf/1704.05908.pdf.

and Ed-
An interpretable knowl-
for knowledge base com-
arXiv:1704.05908
preprint

Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016.
Representation learning of knowledge graphs with
hierarchical types. In IJCAI. pages 2965–2971.

Maximilian Nickel, Lorenzo Rosasco, Tomaso A Pog-
gio, et al. 2016. Holographic embeddings of knowl-
In AAAI Conference. pages 1955–
edge graphs.
1961. https://arxiv.org/abs/1510.04935.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding entities and
relations for learning and inference in knowledge

bases. In ICLR.

