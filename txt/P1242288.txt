Multitask Parsing Across Semantic Representations

Daniel Hershcovich1,2

Ari Rappoport2

Omri Abend2
1The Edmond and Lily Safra Center for Brain Sciences
2School of Computer Science and Engineering
Hebrew University of Jerusalem
{danielh,oabend,arir}@cs.huji.ac.il

8
1
0
2
 
y
a
M
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
7
8
2
0
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

The ability to consolidate information of
different types is at the core of intelli-
gence, and has tremendous practical value
in allowing learning for one task to beneﬁt
from generalizations learned for others. In
this paper we tackle the challenging task of
improving semantic parsing performance,
taking UCCA parsing as a test case, and
AMR, SDP and Universal Dependencies
(UD) parsing as auxiliary tasks. We ex-
periment on three languages, using a uni-
form transition-based system and learning
architecture for all parsing tasks. Despite
notable conceptual, formal and domain
differences, we show that multitask learn-
ing signiﬁcantly improves UCCA parsing
in both in-domain and out-of-domain set-
tings. Our code is publicly available.1

1 Introduction

Semantic parsing has arguably yet to reach its
full potential in terms of its contribution to down-
stream linguistic tasks, partially due to the limited
amount of semantically annotated training data.
This shortage is more pronounced in languages
other than English, and less researched domains.

Indeed, recent work in semantic parsing has
targeted, among others, Abstract Meaning Rep-
resentation (AMR; Banarescu et al., 2013), bilex-
ical Semantic Dependencies (SDP; Oepen et al.,
2016) and Universal Conceptual Cognitive An-
notation (UCCA; Abend and Rappoport, 2013).
While these schemes are formally different and
focus on different distinctions, much of their se-
mantic content is shared (Abend and Rappoport,
2017).

1http://github.com/danielhers/tupa

Multitask learning (MTL; Caruana, 1997) al-
lows exploiting the overlap between tasks to ef-
fectively extend the training data, and has greatly
advanced with neural networks and representation
learning (see §2). We build on these ideas and pro-
pose a general transition-based DAG parser, able
to parse UCCA, AMR, SDP and UD (Nivre et al.,
2016). We train the parser using MTL to obtain
signiﬁcant improvements on UCCA parsing over
single-task training in (1) in-domain and (2) out-
of-domain settings in English; (3) an in-domain
setting in German; and (4) an in-domain setting in
French, where training data is scarce.

The novelty of this work is in proposing a gen-
eral parsing and learning architecture, able to ac-
commodate such widely different parsing tasks,
and in leveraging it to show beneﬁts from learn-
ing them jointly.

2 Related Work

MTL has been used over the years for NLP tasks
with varying degrees of similarity, examples in-
cluding joint classiﬁcation of different arguments
in semantic role labeling (Toutanova et al., 2005),
and joint parsing and named entity recognition
(Finkel and Manning, 2009). Similar ideas, of pa-
rameter sharing across models trained with differ-
ent datasets, can be found in studies of domain
adaptation (Blitzer et al., 2006; Daume III, 2007;
Ziser and Reichart, 2017). For parsing, domain
adaptation has been applied successfully in parser
combination and co-training (McClosky et al.,
2010; Baucom et al., 2013).

in

tackling

formally

been
similar

Neural MTL has mostly

effec-
tive
tasks
(Søgaard and Goldberg, 2016), including multilin-
gual syntactic dependency parsing (Ammar et al.,
2016; Guo et al., 2016), as well as multilingual
(Duong et al., 2017), and cross-domain semantic

parsing
2017).

(Herzig and Berant, 2017; Fan et al.,

for

when

jointly

parsing,

Sharing parameters with a low-level

task
transition-based
has shown great beneﬁt
training
syntactic
with POS tagging (Bohnet and Nivre, 2012;
Zhang and Weiss, 2016), and with lexical anal-
ysis (Constant and Nivre, 2016; More, 2016).
re-
Recent work has achieved state-of-the-art
sults in multiple NLP tasks by jointly learning
the tasks forming the NLP standard pipeline
using a single neural model
(Collobert et al.,
2011; Hashimoto et al., 2017), thereby avoiding
cascading errors, common in pipelines.

Much effort has been devoted to joint learn-
ing of syntactic and semantic parsing, including
two CoNLL shared tasks (Surdeanu et al., 2008;
Hajiˇc et al., 2009). Despite their conceptual and
practical appeal, such joint models rarely outper-
form the pipeline approach (Llu´ıs and M`arquez,
2008; Henderson et al., 2013; Lewis et al., 2015;
Swayamdipta et al., 2016, 2017).

Peng et al. (2017a) performed MTL for SDP in
a closely related setting to ours. They tackled
three tasks, annotated over the same text and shar-
ing the same formal structures (bilexical DAGs),
with considerable edge overlap, but differing in
target representations (see §3). For all tasks, they
reported an increase of 0.5-1 labeled F1 points.
Recently, Peng et al. (2018) applied a similar ap-
proach to joint frame-semantic parsing and seman-
tic dependency parsing, using disjoint datasets,
and reported further improvements.

3 Tackled Parsing Tasks

In this section, we outline the parsing tasks we
address. We focus on representations that pro-
duce full-sentence analyses, i.e., produce a graph
covering all (content) words in the text, or the
lexical concepts they evoke. This contrasts with
“shallow” semantic parsing, primarily semantic
role labeling (SRL; Gildea and Jurafsky, 2002;
Palmer et al., 2005), which targets argument struc-
ture phenomena using ﬂat structures. We consider
four formalisms: UCCA, AMR, SDP and Univer-
sal Dependencies. Figure 1 presents one sentence
annotated in each scheme.

Universal Conceptual Cognitive Annotation.
UCCA (Abend and Rappoport, 2013) is a seman-
tic representation whose main design principles
are ease of annotation, cross-linguistic applicabil-

(a) UCCA

move-01

A
R
G
0

n
a
m
e

o
p
1

person

name

”John”

(b) AMR

(c) DM

(d) UD

LA

H

LR

After

LA

L

H

P

,U

A

graduation

John

A

P
moved

t i m e

A R G 0

after

o
p
1

graduate-01

A

R
to

C

Paris

ARG2

city

name

n
a
m
e

o
p
1

”Paris”

ARG1

top

ARG2

ARG2

ARG1

ARG1

ARG2

After

graduation

,

John moved

to

Paris

case

root

obl

punct

nsubj

obl

case

After

graduation

,

John moved

to

Paris

Figure 1: Example graph for each task. Figure 1a presents
a UCCA graph. The dashed edge is remote, while the blue
node and its outgoing edges represent inter-Scene linkage.
Pre-terminal nodes and edges are omitted for brevity. Fig-
ure 1b presents an AMR graph. Text tokens are not part of
the graph, and must be matched to concepts and constants by
alignment. Variables are represented by their concepts. Fig-
ure 1c presents a DM semantic dependency graph, containing
multiple roots: “After”, “moved” and “to”, of which “moved”
is marked as top. Punctuation tokens are excluded from SDP
graphs. Figure 1d presents a UD tree. Edge labels express
syntactic relations.

ity, and a modular architecture. UCCA represents
the semantics of linguistic utterances as directed
acyclic graphs (DAGs), where terminal (childless)
nodes correspond to the text tokens, and non-
terminal nodes to semantic units that participate in
some super-ordinate relation. Edges are labeled,
indicating the role of a child in the relation the par-
ent represents. Nodes and edges belong to one of
several layers, each corresponding to a “module”
of semantic distinctions. UCCA’s foundational
layer (the only layer for which annotated data ex-

ists) mostly covers predicate-argument structure,
semantic heads and inter-Scene relations.

UCCA distinguishes primary edges, corre-
sponding to explicit relations, from remote edges
(appear dashed in Figure 1a) that allow for a unit
to participate in several super-ordinate relations.
Primary edges form a tree in each layer, whereas
remote edges enable reentrancy, forming a DAG.

Abstract Meaning Representation. AMR
(Banarescu et al., 2013) is a semantic represen-
tation that encodes information about named
semantic roles,
entities,
word sense and co-reference. AMRs are rooted
directed graphs, in which both nodes and edges
are labeled. Most AMRs are DAGs, although
cycles are permitted.

argument

structure,

AMR differs from the other schemes we con-
sider in that it does not anchor its graphs in the
words of the sentence (Figure 1b). Instead, AMR
graphs connect variables, concepts (from a pre-
deﬁned set) and constants (which may be strings
or numbers). Still, most AMR nodes are alignable
to text tokens, a tendency used by AMR parsers,
which align a subset of the graph nodes to a subset
of the text tokens (concept identiﬁcation). In this
work, we use pre-aligned AMR graphs.

Pust et al.,

Despite the brief period since its inception,
AMR has been targeted by a number of works,
notably in two SemEval shared tasks (May, 2016;
May and Priyadarshi, 2017). To tackle its variety
of distinctions and unrestricted graph structure,
AMR parsers often use specialized methods.
Graph-based parsers construct AMRs by iden-
tifying concepts and scoring edges between
them, either in a pipeline fashion (Flanigan et al.,
2014; Artzi et al.,
2015;
2015;
Foland and Martin, 2017), or jointly (Zhou et al.,
2016). Another line of work trains machine trans-
lation models to convert strings into linearized
AMRs (Barzdins and Gosko, 2016; Peng et al.,
2017b; Konstas et al., 2017; Buys and Blunsom,
2017b). Transition-based AMR parsers either use
dependency trees as pre-processing, then mapping
them into AMRs (Wang et al., 2015a,b, 2016;
Goodman et al., 2016), or use a transition system
tailored to AMR parsing (Damonte et al., 2017;
Ballesteros and Al-Onaizan, 2017). We differ
from the above approaches in addressing AMR
parsing using the same general DAG parser used
for other schemes.

Semantic Dependency Parsing. SDP uses a set
of related representations, targeted in two recent
SemEval shared tasks (Oepen et al., 2014, 2015),
and extended by Oepen et al. (2016). They cor-
respond to four semantic representation schemes,
referred to as DM, PAS, PSD and CCD, represent-
ing predicate-argument relations between content
words in a sentence. All are based on semantic for-
malisms converted into bilexical dependencies—
directed graphs whose nodes are text
tokens.
Edges are labeled, encoding semantic relations be-
tween the tokens. Non-content tokens, such as
punctuation, are left out of the analysis (see Fig-
ure 1c). Graphs containing cycles have been re-
moved from the SDP datasets.

from DeepBank

We use one of the representations from the
SemEval shared tasks: DM (DELPH-IN MRS),
(Flickinger et al.,
converted
2012), a corpus of hand-corrected parses from
LinGO ERG (Copestake and Flickinger, 2000),
an HPSG (Pollard and Sag, 1994) using Minimal
Recursion Semantics (Copestake et al., 2005).

Universal Dependencies. UD (Nivre et al.,
2016, 2017) has quickly become the dominant
dependency scheme for syntactic annotation in
many languages, aiming for cross-linguistically
consistent and coarse-grained treebank annota-
tion. Formally, UD uses bilexical trees, with edge
labels representing syntactic relations between
words.

We use UD as an auxiliary task, inspired by pre-
vious work on joint syntactic and semantic parsing
(see §2).
In order to reach comparable analyses
cross-linguistically, UD often ends up in annota-
tion that is similar to the common practice in se-
mantic treebanks, such as linking content words to
content words wherever possible. Using UD fur-
ther allows conducting experiments on languages
other than English, for which AMR and SDP an-
notated data is not available (§7).

In addition to basic UD trees, we use the en-
hanced++ UD graphs available for English, which
are generated by the Stanford CoreNLP convert-
ers (Schuster and Manning, 2016).2 These include
additional and augmented relations between con-
tent words, partially overlapping with the notion
of remote edges in UCCA: in the case of control
verbs, for example, a direct relation is added in
enhanced++ UD between the subordinated verb

2http://github.com/stanfordnlp/CoreNLP

and its controller, which is similar to the seman-
tic schemes’ treatment of this construction.

Parser state

4 General Transition-based DAG Parser

All schemes considered in this work exhibit reen-
trancy and discontinuity (or non-projectivity), to
In addition, UCCA and AMR
varying degrees.
To parse these
contain non-terminal nodes.
graphs, we extend TUPA (Hershcovich et al.,
2017), a transition-based parser originally devel-
oped for UCCA, as it supports all these structural
properties. TUPA’s transition system can yield any
labeled DAG whose terminals are anchored in the
text tokens. To support parsing into AMR, which
uses graphs that are not anchored in the tokens,
we take advantage of existing alignments of the
graphs with the text tokens during training (§5).

First used for projective syntactic dependency
tree parsing (Nivre, 2003),
transition-based
parsers have since been generalized to parse into
many other graph families, such as (discontinu-
ous) constituency trees (e.g., Zhang and Clark,
and DAGs
2009; Maier and Lichte, 2016),
(e.g., Sagae and Tsujii, 2008; Du et al., 2015).
Transition-based parsers apply transitions incre-
mentally to an internal state deﬁned by a buffer
B of remaining tokens and nodes, a stack S
of unresolved nodes, and a labeled graph G of
constructed nodes and edges. When a terminal
state is reached, the graph G is the ﬁnal output.
the
A classiﬁer is used at each step to select
next transition, based on features that encode the
current state.

4.1 TUPA’s Transition Set

Given a sequence of tokens w1, . . . , wn, we pre-
dict a rooted graph G whose terminals are the to-
kens. Parsing starts with the root node on the
stack, and the input tokens in the buffer.

The TUPA transition set includes the standard
SHIFT and REDUCE operations, NODEX for cre-
ating a new non-terminal node and an X-labeled
edge, LEFT-EDGEX and RIGHT-EDGEX to create
a new primary X-labeled edge, LEFT-REMOTEX
and RIGHT-REMOTEX to create a new remote
X-labeled edge, SWAP to handle discontinuous
nodes, and FINISH to mark the state as terminal.

Although UCCA contains nodes without any
text tokens as descendants (called implicit units),
these nodes are infrequent and only cover 0.5% of
non-terminal nodes. For this reason we follow pre-

S

B
John moved to Paris .

G

,

L
After

H

P
graduation

Classiﬁer

transition

softmax

MLP

BiLSTM

Embeddings

After

graduation

. . .

to

Paris

Figure 2:
Hershcovich et al. (2017).
BiLTSM architecture.

Illustration of the TUPA model, adapted from
Top: parser state. Bottom:

vious work (Hershcovich et al., 2017) and discard
implicit units from the training and evaluation, and
so do not include transitions for creating them.

In AMR, implicit units are considerably more
common, as any unaligned concept with no
aligned descendents is implicit (about 6% of
the nodes).
Implicit AMR nodes usually re-
sult from alignment errors, or from abstract con-
cepts which have no explicit realization in the text
(Buys and Blunsom, 2017a). We ignore implicit
nodes when training on AMR as well. TUPA also
does not support node labels, which are ubiqui-
tous in AMR but absent in UCCA structures (only
edges are labeled in UCCA). We therefore only
produce edge labels and not node labels when
training on AMR.

4.2 Transition Classiﬁer

To predict the next transition at each step, we use
a BiLSTM with embeddings as inputs, followed
by an MLP and a softmax layer for classiﬁcation
(Kiperwasser and Goldberg, 2016). The model is
Inference is performed
illustrated in Figure 2.
greedily, and training is done with an oracle that
yields the set of all optimal transitions at a given
state (those that lead to a state from which the gold
graph is still reachable). Out of this set, the ac-
tual transition performed in training is the one with
the highest score given by the classiﬁer, which is

trained to maximize the sum of log-likelihoods of
all optimal transitions at each step.

After

L

P

H

H

A

,U

A

P

graduation

John

moved

After

o
p

graduation

ti m e

A R G 0

(a) UCCA

moved

A
R
G
0

n
a
m
e

John

(b) AMR

r o o t

A

R

G1

root

t
o
p

1

G

R

A

A

R

to

A

R

G2

C

Paris

n
a
m
e

Paris

d
a
e
h

A

R

G

2

d
a
e
h

A

R

G

2

h

e

a

d

ARG2

Afterggraduation ,

movedg

tog

Parisg

G1
R
A
g John

(c) DM

b l

o

obl

punct

h

e

a

d

n
s
u
b
j

ase
c

h

h

e

e

a

a

d

d

ase
c

Afterg

graduation

,g

Johng movedgtog

Parisg

(d) UD

Figure 3: Graphs from Figure 1, after conversion to the uni-
ﬁed DAG format (with pre-terminals omitted: each terminal
drawn in place of its parent). Figure 3a presents a converted
UCCA graph. Linkage nodes and edges are removed, but
the original graph is otherwise preserved. Figure 3b presents
a converted AMR graph, with text tokens added according
to the alignments. Numeric sufﬁxes of op relations are re-
moved, and names collapsed. Figure 3c presents a converted
SDP graph (in the DM representation), with intermediate
non-terminal head nodes introduced. In case of reentrancy,
an arbitrary reentrant edge is marked as remote. Figure 3d
presents a converted UD graph. As in SDP, intermediate non-
terminals and head edges are introduced. While converted
UD graphs form trees, enhanced++ UD graphs may not.

Features. We use the original TUPA features,
representing the words, POS tags, syntactic depen-
dency relations, and previously predicted edge la-
bels for nodes in speciﬁc locations in the parser

state. In addition, for each token we use embed-
dings representing the one-character preﬁx, three-
character sufﬁx, shape (capturing orthographic
features, e.g., “Xxxx”), and named entity type,3
all provided by spaCy (Honnibal and Montani,
2018).4 To the learned word vectors, we concate-
nate the 250K most frequent word vectors from
fastText (Bojanowski et al., 2017),5 pre-trained
over Wikipedia and updated during training.

Constraints. As each annotation scheme has
different constraints on the allowed graph struc-
tures, we apply these constraints separately for
each task. During training and parsing, the rele-
vant constraint set rules out some of the transitions
according to the parser state. Some constraints
are task-speciﬁc, others are generic. For exam-
ple, in UCCA, a terminal may only have one par-
ent. In AMR, a concept corresponding to a Prop-
Bank frame may only have the core arguments de-
ﬁned for the frame as children. An example of
a generic constraint is that stack nodes that have
been swapped should not be swapped again.6

5 Uniﬁed DAG Format

To apply our parser to the four target tasks (§3), we
convert them into a uniﬁed DAG format, which is
inclusive enough to allow representing any of the
schemes with very little loss of information.7

The format consists of a rooted DAG, where the
tokens are the terminal nodes. As in the UCCA
format, edges are labeled (but not nodes), and are
divided into primary and remote edges, where the
primary edges form a tree (all nodes have at most
one primary parent, and the root has none). Re-
mote edges enable reentrancy, and thus together
with primary edges form a DAG. Figure 3 shows
examples for converted graphs. Converting UCCA
into the uniﬁed format consists simply of remov-
ing linkage nodes and edges (see Figure 3a), which
were also discarded by Hershcovich et al. (2017).

3See Supplementary Material for a full listing of features.
4http://spacy.io
5http://fasttext.cc
6 To implement this constraint, we deﬁne a swap index for
each node, assigned when the node is created. At initializa-
tion, only the root node and terminals exist. We assign the
root a swap index of 0, and for each terminal, its position in
the text (starting at 1). Whenever a node is created as a result
of a NODE transition, its swap index is the arithmetic mean
of the swap indices of the stack top and buffer head.

7See Supplementary Material for more conversion details.

Parser state

Classiﬁer

. . .

transition

softmax

Task-speciﬁc MLP

Task-speciﬁc BiLSTM

Shared BiLSTM

Shared embeddings

After

graduation

. . .

to

Paris

Figure 4: MTL model. Token representations are computed
both by a task-speciﬁc and a shared BiLSTM. Their outputs
are concatenated with the parser state embedding, identical to
Figure 2, and fed into the task-speciﬁc MLP for selecting the
next transition. Shared parameters are shown in blue.

Converting bilexical dependencies. To convert
DM and UD into the uniﬁed DAG format, we add
a pre-terminal for each token, and attach the pre-
terminals according to the original dependency
edges: traversing the tree from the root down, for
each head token we create a non-terminal parent
with the edge label head, and add the node’s de-
pendents as children of the created non-terminal
node (see Figures 3c and 3d). Since DM allows
multiple roots, we form a single root node, whose
children are the original roots. The added edges
are labeled root, where top nodes are labeled top
instead. In case of reentrancy, an arbitrary parent
is marked as primary, and the rest as remote (de-
noted as dashed edges in Figure 3).

Converting AMR.
In the conversion from
AMR, node labels are dropped. Since alignments
are not part of the AMR graph (see Figure 3b), we
use automatic alignments (see §7), and attach each
node with an edge to each of its aligned terminals.
Named entities in AMR are represented as a
subgraph, whose name-labeled root has a child for
each token in the name (see the two name nodes in
Figure 1b). We collapse this subgraph into a single
node whose children are the name tokens.

6 Multitask Transition-based Parsing

Now that the same model can be applied to dif-
ferent tasks, we can train it in a multitask setting.
The fairly small training set available for UCCA
(see §7) makes MTL particularly appealing, and

we focus on it in this paper, treating AMR, DM
and UD parsing as auxiliary tasks.

of

the

2016;

Plank,

Søgaard and Goldberg,

Following previous work, we share only
(Klerke et al.,
parameters
some
2016;
2016;
Bollmann and Søgaard,
2016;
Braud et al., 2016; Mart´ınez Alonso and Plank,
2017; Peng et al., 2017a, 2018),
leaving task-
speciﬁc sub-networks as well. Concretely, we
keep the BiLSTM used by TUPA for the main
task (UCCA parsing), add a BiLSTM that
is
shared across all tasks, and replicate the MLP
(feedforward sub-network) for each task. The
BiLSTM outputs (concatenated for the main task)
are fed into the task-speciﬁc MLP (see Figure 4).
Feature embeddings are shared across tasks.

Unlabeled parsing for auxiliary tasks. To sim-
plify the auxiliary tasks and facilitate generaliza-
tion (Bingel and Søgaard, 2017), we perform un-
labeled parsing for AMR, DM and UD, while still
predicting edge labels in UCCA parsing. To sup-
port unlabeled parsing, we simply remove all la-
bels from the EDGE, REMOTE and NODE tran-
sitions output by the oracle. This results in a
much smaller number of transitions the classiﬁer
has to select from (no more than 10, as opposed
to 45 in labeled UCCA parsing), allowing us to
use no BiLSTMs and fewer dimensions and layers
for task-speciﬁc MLPs of auxiliary tasks (see §7).
This limited capacity forces the network to use the
shared parameters for all tasks, increasing gener-
alization (Mart´ınez Alonso and Plank, 2017).

7 Experimental Setup

We here detail a range of experiments to assess
the value of MTL to UCCA parsing, training the
parser in single-task and multitask settings, and
evaluating its performance on the UCCA test sets
in both in-domain and out-of-domain settings.

Data. For UCCA, we use v1.2 of the English
Wikipedia corpus (Wiki; Abend and Rappoport,
2013), with the standard train/dev/test split (see
Table 1), and the Twenty Thousand Leagues Un-
der the Sea corpora (20K; Sulem et al., 2015), an-
notated in English, French and German.8 For En-
glish and French we use 20K v1.0, a small par-
allel corpus comprising the ﬁrst ﬁve chapters of
the book. As in previous work (Hershcovich et al.,

8http://github.com/huji-nlp/ucca-corpora

English

French

German

# tokens
dev

train

test

# sentences
train dev test

# tokens
train dev test

# sentences
train dev test

# tokens
dev

test

# sentences
train dev test

train

128444 14676 15313 4268 454 503
12339

506 10047 1558 1324 413 67 67 79894 10059 42366 3429 561 2164

UCCA
Wiki
20K
AMR 648950
DM 765025
UD
458277

36521
33964
17062

Table 1: Number of tokens and sentences in the training, development and test sets we use for each corpus and language.

899163

32347

268145

13814

2017), we use the English part only as an out-of-
domain test set. We train and test on the French
part using the standard split, as well as the Ger-
man corpus (v0.9), which is a pre-release and still
contains a considerable amount of noisy annota-
tion. Tuning is performed on the respective devel-
opment sets.

to the dataset

For AMR, we use LDC2017T10,

identi-
targeted in SemEval 2017
cal
(May and Priyadarshi, 2017).9
For SDP, we
use the DM representation from the SDP 2016
dataset (Oepen et al., 2016).10
For Universal
Dependencies, we use all English, French and
German treebanks from UD v2.1 (Nivre et al.,
2017).11 We use the enhanced++ UD representa-
tion (Schuster and Manning, 2016) in our English
experiments, henceforth referred to as UD++. We
use only the AMR, DM and UD training sets from
standard splits.

While UCCA is annotated over Wikipedia and
over a literary corpus, the domains for AMR, DM
and UD are blogs, news, emails, reviews, and
Q&A. This domain difference between training
and test is particularly challenging (see §9). Un-
fortunately, none of the other schemes have avail-
able annotation over Wikipedia text.

Settings. We explore the following settings: (1)
in-domain setting in English, training and test-
ing on Wiki; (2) out-of-domain setting in English,
training on Wiki and testing on 20K; (3) French in-
domain setting, where available training dataset is
small, training and testing on 20K; (4) German in-
domain setting on 20K, with somewhat noisy an-
notation. For MTL experiments, we use unlabeled
AMR, DM and UD++ parsing as auxiliary tasks in
English, and unlabeled UD parsing in French and
German.12 We also report baseline results training

9http://catalog.ldc.upenn.edu/LDC2017T10
10http://sdp.delph-in.net/osdp-12.tgz
11http://hdl.handle.net/11234/1-2515
12We did not use AMR, DM or UD++ in French and Ger-

man, as these are only available in English.

only the UCCA training sets.

Training. We create a uniﬁed corpus for each
shufﬂing all sentences from relevant
setting,
datasets together, but using only the UCCA devel-
opment set F1 score as the early stopping criterion.
In each training epoch, we use the same number of
examples from each task—the UCCA training set
size. Since training sets differ in size, we sample
this many sentences from each one. The model is
implemented using DyNet (Neubig et al., 2017).13

Multitask

Single Main Aux Shared

Hyperparameter
Pre-trained word dim.
Learned word dim.
POS tag dim.
Dependency relation dim.
Named entity dim.
Punctuation dim.
Action dim.
Edge label dim.
MLP layers
MLP dimensions
BiLSTM layers
BiLSTM dimensions

300
200
20
10
3
1
3
20
2
50
2
500

300
200
20
10
3
1
3

2
300

1
50

20
2
50
2
300

Table 2: Hyperparameter settings. Middle column shows hy-
perparameters used for the single-task architecture, described
in §4.2, and right column for the multitask architecture, de-
scribed in §6. Main refers to parameters speciﬁc to the main
task—UCCA parsing (task-speciﬁc MLP and BiLSTM, and
edge label embedding), Aux to parameters speciﬁc to each
auxiliary task (task-speciﬁc MLP, but no edge label embed-
ding since the tasks are unlabeled), and Shared to parameters
shared among all tasks (shared BiLSTM and embeddings).

Hyperparameters. We initialize embeddings
(Srivastava et al.,
randomly. We use dropout
2014) between MLP layers, and recurrent dropout
(Gal and Ghahramani, 2016) between BiLSTM
layers, both with p = 0.4. We also use word
(α = 0.2), tag (α = 0.2) and dependency relation
(α = 0.5) dropout (Kiperwasser and Goldberg,
2016).14
In addition, we use a novel form of

13http://dynet.io
14In training, the embedding for a feature value w is re-
α
#(w)+α ,

placed with a zero vector with a probability of
where #(w) is the number of occurrences of w observed.

Primary
LR

LF

LP

Remote

LP

LR LF

73.5
73.6
73.7

72.7
72.9
72.8

English (in-domain)
74.4
HAR17
Single
74.4
74.7
AMR
75.7⋆ 73.9⋆ 74.8⋆ 54.9
DM
75⋆
UD++
75.6⋆ 73.9⋆ 74.7⋆ 49.9
AMR + DM
AMR + UD++ 74.9
47.1
DM + UD++
All

75.9⋆ 73.9⋆ 74.9⋆ 48
75.6⋆ 73.1

51.6 49.4
47.4
53
51.5
50
48.7⋆ 51.1 49.9
53.9
53
52.7 50.8
51.4
53
50
48.5
54.8 51.2
53.2 52

74.4⋆ 50.9

74.1⋆ 49

73.2

72.7

73.8

Table 3: Labeled precision, recall and F1 (in %) for primary
and remote edges, on the Wiki test set. ⋆ indicates signiﬁ-
cantly better than Single. HAR17: Hershcovich et al. (2017).

dropout, node dropout: with a probability of 0.1
at each step, all features associated with a single
node in the parser state are replaced with zero vec-
tors. For optimization we use a minibatch size of
100, decaying all weights by 10−5 at each update,
and train with stochastic gradient descent for N
epochs with a learning rate of 0.1, followed by
AMSGrad (Sashank J. Reddi, 2018) for N epochs
with α = 0.001, β1 = 0.9 and β2 = 0.999.
We use N = 50 for English and German, and
N = 400 for French. We found this training strat-
egy better than using only one of the optimization
methods, similar to ﬁndings by Keskar and Socher
(2017). We select the epoch with the best aver-
age labeled F1 score on the UCCA development
set. Other hyperparameter settings are listed in Ta-
ble 2.

Evaluation. We evaluate on UCCA using la-
recall and F1 on primary
beled precision,
and remote edges,
following previous work
(Hershcovich et al., 2017). Edges in predicted and
gold graphs are matched by terminal yield and la-
bel. Signiﬁcance testing of improvements over
the single-task model is done by the bootstrap test
(Berg-Kirkpatrick et al., 2012), with p < 0.05.

8 Results

Table 3 presents our results on the English in-
domain Wiki test set. MTL with all auxiliary tasks
and their combinations improves the primary F1
In most set-
score over the single task baseline.
tings the improvement is statistically signiﬁcant.
Using all auxiliary tasks contributed less than just
DM and UD++, the combination of which yielded
the best scores yet in in-domain UCCA parsing,
with 74.9% F1 on primary edges. Remote F1 is
improved in some settings, but due to the rela-

Primary
LR

LF

LP

Remote

LP LR

LF

41.4 22

68.5
69
69.5

69.8⋆ 69.7

38.6 18.8
68.6
68.7
41.2 19.8
69
69
69.5
42.9 20.2
69.5
70.7⋆ 70.7⋆ 70.7⋆ 42.7 18.6
69.6
70.7⋆ 70.2⋆ 70.5⋆ 45.8 19.4
45.1 21.8
70.8⋆ 70.3⋆ 70.6⋆ 41.6 21.6
71.2⋆ 70.9⋆ 71⋆

English (out-of-domain)
HAR17
Single
AMR
DM
UD++
AMR + DM
AMR + UD++ 70.2⋆ 69.9⋆ 70⋆
DM + UD++
All
French (in-domain)
68.2
Single
UD
70.3
German (in-domain)
73.3
Single
27.1
73.7⋆ 72.6⋆ 73.2⋆ 61.8 24.9⋆ 35.5⋆
UD

25.3
26.7
27.5
25.9
28.7
27.3
29.4
28.4
29.6

67.6
9.4
26
70.1⋆ 43.8 13.2

57.1 17.7

13.9
20.3

45.1 22

67
70⋆

71.7

72.5

Table 4: Labeled precision, recall and F1 (in %) for primary
and remote edges, on the 20K test sets. ⋆ indicates signiﬁ-
cantly better than Single. HAR17: Hershcovich et al. (2017).

tively small number of remote edges (about 2%
of all edges), none of the differences is signiﬁcant.
Note that our baseline single-task model (Single)
is slightly better than the current state-of-the-art
(HAR17; Hershcovich et al., 2017), due to the in-
corporation of additional features (see §4.2).

Table 4 presents our experimental results on the
20K corpora in the three languages. For English
improvements from using MTL
out-of-domain,
are even more marked. Moreover, the improve-
ment is largely additive: the best model, using all
three auxiliary tasks (All), yields an error reduc-
tion of 2.9%. Again, the single-task baseline is
slightly better than HAR17.

The contribution of MTL is also apparent in
French and German in-domain parsing: 3.7% er-
ror reduction in French (having less than 10%
as much UCCA training data as English) and
1% in German, where the training set is com-
parable in size to the English one, but is noisier
(see §7). The best MTL models are signiﬁcantly
better than single-task models, demonstrating that
even a small training set for the main task may
sufﬁce, given enough auxiliary training data (as in
French).

9 Discussion

success

Quantifying the similarity between tasks.
in
Task similarity is
2017;
MTL
In our case,
Mart´ınez Alonso and Plank, 2017).
the main and auxiliary tasks are annotated on
different corpora from different domains (§7), and

an important
(Bingel and Søgaard,

factor

Wiki
20K
AMR
DM

20K
1.047

AMR
0.895
0.949

DM
0.913
0.971
0.757

UD
0.843
0.904
0.469
0.754

Table 5: L1 distance between dataset word distributions,
quantifying domain differences in English (low is similar).

Primary
UR
15.6
49.2
84.6

UP
53.8
65
82.7

UF
24.2
56
83.6

Remote
UR
5.5
65.9
12.7

UP
7.3
7.4
12.5

UF
6.3
13.3
12.6

AMR
DM
UD++

Table 6: Unlabeled F1 scores between the representations of
the same English sentences (from PTB WSJ), converted to
the uniﬁed DAG format, and annotated UCCA graphs.

the target representations vary both in form and in
content.

To quantify the domain differences, we follow
Plank and van Noord (2011) and measure the L1
distance between word distributions in the English
training sets and 20K test set (Table 5). All aux-
iliary training sets are more similar to 20K than
Wiki is, which may contribute to the beneﬁts ob-
served on the English 20K test set.

As a measure of the formal similarity of the dif-
ferent schemes to UCCA, we use unlabeled F1
score evaluation on both primary and remote edges
(ignoring edge labels). To this end, we annotated
100 English sentences from Section 02 of the Penn
Treebank Wall Street Journal (PTB WSJ). Anno-
tation was carried out by a single expert UCCA
annotator, and is publicly available.15 These sen-
tences had already been annotated by the AMR,
DM and PTB schemes,16 and we convert their an-
notation to the uniﬁed DAG format.

Unlabeled F1 scores between the UCCA graphs
and those converted from AMR, DM and UD++
are presented in Table 6. UD++ is highly over-
lapping with UCCA, while DM less so, and AMR
even less (cf. Figure 3).

Comparing the average improvements resulting
from adding each of the tasks as auxiliary (see §8),
we ﬁnd AMR the least beneﬁcial, UD++ second,
and DM the most beneﬁcial, in both in-domain
and out-of-domain settings. This trend is weakly
correlated with the formal similarity between the
tasks (as expressed in Table 6), but weakly neg-
atively correlated with the word distribution simi-

15http://github.com/danielhers/wsj
16We convert
the PTB format

to UD++ v1 using
Stanford CoreNLP, and then to UD v2 using Udapi:
http://github.com/udapi/udapi-python.

larity scores (Table 5). We conclude that other fac-
tors should be taken into account to fully explain
this effect, and propose to address this in future
work through controlled experiments, where cor-
pora of the same domain are annotated with the
various formalisms and used as training data for
MTL.

AMR, SDP and UD parsing. Evaluating the
full MTL model (All) on the unlabeled auxil-
iary tasks yielded 64.7% unlabeled Smatch F1
(Cai and Knight, 2013) on the AMR develop-
ment set, when using oracle concept identiﬁca-
tion (since the auxiliary model does not predict
node labels), 27.2% unlabeled F1 on the DM de-
velopment set, and 4.9% UAS on the UD devel-
opment set. These poor results reﬂect the fact
that model selection was based on the score on
the UCCA development set, and that the model
parameters dedicated to auxiliary tasks were very
limited (to encourage using the shared param-
eters). However, preliminary experiments us-
ing our approach produced promising results on
each of the tasks’ respective English develop-
ment sets, when treated as a single task: 67.1%
labeled Smatch F1 on AMR (adding a transi-
tion for implicit nodes and classiﬁer for node la-
bels), 79.1% labeled F1 on DM, and 80.1% LAS
F1 on UD. For comparison, the best results on
these datasets are 70.7%, 91.2% and 82.2%, re-
spectively (Foland and Martin, 2017; Peng et al.,
2018; Dozat et al., 2017).

10 Conclusion

We demonstrate that semantic parsers can leverage
a range of semantically and syntactically anno-
tated data, to improve their performance. Our ex-
periments show that MTL improves UCCA pars-
ing, using AMR, DM and UD parsing as auxil-
iaries. We propose a uniﬁed DAG representation,
construct protocols for converting these schemes
into the uniﬁed format, and generalize a transition-
based DAG parser to support all these tasks, allow-
ing it to be jointly trained on them.

While we focus on UCCA in this work, our
parser is capable of parsing any scheme that can
be represented in the uniﬁed DAG format, and pre-
liminary results on AMR, DM and UD are promis-
ing (see §9). Future work will investigate whether
a single algorithm and architecture can be com-
petitive on all of these parsing tasks, an important
step towards a joint many-task model for semantic

parsing.

Acknowledgments

This work was supported by the Israel Science
Foundation (grant no. 929/17), by the HUJI Cy-
ber Security Research Center in conjunction with
the Israel National Cyber Bureau in the Prime
Minister’s Ofﬁce, and by the Intel Collaborative
Research Institute for Computational Intelligence
(ICRI-CI). The ﬁrst author was supported by a fel-
lowship from the Edmond and Lily Safra Center
for Brain Sciences. We thank Roi Reichart, Rotem
Dror and the anonymous reviewers for their help-
ful comments.

References

Omri Abend and Ari Rappoport. 2013. Universal Con-
ceptual Cognitive Annotation (UCCA). In Proc. of
ACL, pages 228–238.

Omri Abend and Ari Rappoport. 2017. The state of
the art in semantic representation. In Proc. of ACL,
pages 77–89.

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah Smith. 2016. Many lan-
guages, one parser. TACL, 4:431–444.

Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
Broad-coverage CCG semantic parsing with AMR.
In Proc. of EMNLP, pages 1699–1710.

Miguel Ballesteros and Yaser Al-Onaizan. 2017. AMR
In Proc. of EMNLP,

parsing using stack-LSTMs.
pages 1269–1275.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin
Knight, Martha Palmer, and Nathan Schneider.
2013. Abstract Meaning Representation for sem-
In Proc. of the Linguistic Annotation
banking.
Workshop.

Guntis Barzdins and Didzis Gosko. 2016. RIGA at
SemEval-2016 task 8: Impact of Smatch extensions
and character-level neural translation on AMR pars-
In Proc. of SemEval, pages 1143–
ing accuracy.
1147.

Eric Baucom, Levi King, and Sandra K¨ubler. 2013.
Domain adaptation for parsing. In Proc. of RANLP,
pages 56–64.

Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statistical
In Proc. of EMNLP-CoNLL,
signiﬁcance in NLP.
pages 995–1005.

Joachim Bingel and Anders Søgaard. 2017. Identify-
ing beneﬁcial task relations for multi-task learning
in deep neural networks. In Proc. of EACL, pages
164–169.

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120–128.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proc.
of EMNLP-CoNLL, pages 1455–1465.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL, 5:135–146.

Marcel Bollmann and Anders Søgaard. 2016.

Im-
proving historical spelling normalization with bi-
directional lstms and multi-task learning. In Proc.
of COLING, pages 131–139.

Chlo´e Braud, Barbara Plank, and Anders Søgaard.
2016. Multi-view and multi-task training of RST
In Proc. of COLING, pages
discourse parsers.
1903–1913.

Jan Buys and Phil Blunsom. 2017a.

Oxford at
SemEval-2017 task 9: Neural AMR parsing with
pointer-augmented attention. In Proc. of SemEval,
pages 914–919.

Jan Buys and Phil Blunsom. 2017b. Robust incremen-
tal neural semantic graph parsing. In Proc. of ACL,
pages 1215–1226.

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Proc.
of ACL, pages 748–752.

Rich Caruana. 1997. Multitask Learning. Machine

Learning, 28(1):41–75.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537.

Matthieu Constant and Joakim Nivre. 2016.

A
transition-based system for joint lexical and syntac-
tic analysis. In Proc. of ACL, pages 161–171.

Ann Copestake and Dan Flickinger. 2000.

An
open source grammar development environment and
broad-coverage English grammar using HPSG.
In
Proc. of LREC, pages 591–600.

Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3(2):281–332.

Marco Damonte, Shay B. Cohen, and Giorgio Satta.
2017. An incremental parser for Abstract Meaning
Representation. In Proc. of EACL.

Hal Daume III. 2007. Frustratingly easy domain adap-

tation. In Proc. of ACL, pages 256–263.

Timothy Dozat, Peng Qi, and Christopher D. Manning.
2017. Stanford’s graph-based neural dependency
In Proc. of
parser at the conll 2017 shared task.
CoNLL, pages 20–30.

Yantao Du, Fan Zhang, Xun Zhang, Weiwei Sun, and
Xiaojun Wan. 2015. Peking: Building semantic de-
pendency graphs with a hybrid parser. In Proc. of
SemEval, pages 927–931.

Long Duong, Hadi Afshar, Dominique Estival, Glen
Pink, Philip Cohen, and Mark Johnson. 2017. Mul-
tilingual semantic parsing and code-switching.
In
Proc. of CoNLL, pages 379–389.

Xing Fan, Emilio Monti, Lambert Mathias, and Markus
Dreyer. 2017. Transfer learning for neural seman-
tic parsing. In Proc. of Workshop on Representation
Learning for NLP, pages 48–56.

Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc.
of NAACL-HLT, pages 326–334.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the Abstract Meaning
Representation. In Proc. of ACL, pages 1426–1436.

Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank: A dynamically annotated treebank of the
Wall Street Journal. In Proc. of Workshop on Tree-
banks and Linguistic Theories, pages 85–96.

Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proc. of CoNLL,
pages 1–18.

Kazuma Hashimoto, caiming xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple NLP
tasks. In Proc. of EMNLP, pages 1923–1933.

James Henderson, Paola Merlo,

Ivan Titov, and
Gabriele Musillo. 2013. Multilingual joint pars-
ing of syntactic and semantic dependencies with a
latent variable model. Computational Linguistics,
39(4):949–998.

Daniel Hershcovich, Omri Abend, and Ari Rappoport.
2017. A transition-based directed acyclic graph
In Proc. of ACL, pages 1127–
parser for UCCA.
1138.

Jonathan Herzig and Jonathan Berant. 2017. Neural
semantic parsing over multiple knowledge-bases. In
Proc. of ACL, pages 623–628.

Matthew Honnibal and Ines Montani. 2018. spaCy 2:
Natural language understanding with Bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Nitish Shirish Keskar and Richard Socher. 2017. Im-
proving generalization performance by switching
from Adam to SGD. CoRR, abs/1712.07628.

William Foland and James H. Martin. 2017. Abstract
Meaning Representation parsing using LSTM recur-
rent neural networks. In Proc. of ACL, pages 463–
472.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. TACL, 4:313–
327.

Yarin Gal and Zoubin Ghahramani. 2016. A Theoreti-
cally Grounded Application of Dropout in Recurrent
Neural Networks. In D D Lee, M Sugiyama, U V
Luxburg, I Guyon, and R Garnett, editors, Advances
in Neural Information Processing Systems 29, pages
1019–1027. Curran Associates, Inc.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3).

James Goodman, Andreas Vlachos, and Jason Narad-
owsky. 2016. Noise reduction and targeted explo-
ration in imitation learning for Abstract Meaning
Representation parsing. In Proc. of ACL, pages 1–
11.

Sigrid Klerke, Yoav Goldberg, and Anders Søgaard.
2016.
Improving sentence compression by learn-
ing to predict gaze. In Proc. of NAACL-HLT, pages
1528–1533.

Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin
Choi, and Luke Zettlemoyer. 2017. Neural AMR:
Sequence-to-sequence models for parsing and gen-
eration. In Proc. of ACL, pages 146–157.

Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015.
Joint A* CCG parsing and semantic role labelling.
In Proc. of EMNLP, pages 1444–1454.

Xavier Llu´ıs and Llu´ıs M`arquez. 2008. A joint model
for parsing syntactic and semantic dependencies. In
Proc. of CoNLL, pages 188–192.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2016. Exploiting multi-typed treebanks for
CoRR,
parsing with deep multi-task learning.
abs/1606.01161.

Wolfgang Maier and Timm Lichte. 2016. Discontinu-
ous parsing with continuous trees. In Proc. of Work-
shop on Discontinuous Structures in NLP, pages 47–
57.

H´ector Mart´ınez Alonso and Barbara Plank. 2017.
When is multitask learning effective? Semantic se-
quence prediction under varying data conditions. In
Proc. of EACL, pages 44–53.

Jonathan May. 2016. SemEval-2016 task 8: Meaning
representation parsing. In Proc. of SemEval, pages
1063–1073.

Jonathan May and Jay Priyadarshi. 2017. SemEval-
2017 task 9: Abstract Meaning Representation pars-
ing and generation. In Proc. of SemEval, pages 536–
545.

David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Proc. of NAACL-HLT, pages 28–36.

Amir More. 2016. Joint morpho-syntactic processing
of morphologically rich languages in a transition-
based framework. Master’s thesis, The Interdisci-
plinary Center, Herzliya.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. DyNet:
CoRR,
The dynamic neural network toolkit.
abs/1701.03980.

Joakim Nivre. 2003. An efﬁcient algorithm for projec-
tive dependency parsing. In Proc. of IWPT, pages
149–160.

Joakim Nivre,

ˇZeljko Agi´c, Lars Ahrenberg, Lene
Antonsen, Maria Jesus Aranzabe, Masayuki Asa-
hara, Luma Ateyah, Mohammed Attia, Aitz-
iber Atutxa, Liesbeth Augustinus, Elena Bad-
maeva, Miguel Ballesteros, Esha Banerjee, Sebas-
tian Bank, Verginica Barbu Mititelu, John Bauer,
Kepa Bengoetxea, Riyaz Ahmad Bhat, Eckhard
Bick, Victoria Bobicev, Carl B¨orstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Aljoscha Bur-
chardt, Marie Candito, Gauthier Caron, G¨uls¸en
Cebirolu Eryiit, Giuseppe G. A. Celano, Savas
Cetin, Fabricio Chalub, Jinho Choi, Silvie Cinkov´a,
C¸ ar C¸ ¨oltekin, Miriam Connor, Elizabeth David-
son, Marie-Catherine de Marneffe, Valeria de Paiva,
Arantza Diaz de Ilarraza, Peter Dirix, Kaja Do-
brovoljc, Timothy Dozat, Kira Droganova, Puneet
Dwivedi, Marhaba Eli, Ali Elkahky, Tomaˇz Erjavec,
Rich´ard Farkas, Hector Fernandez Alcalde, Jennifer
Foster, Cl´audia Freitas, Katar´ına Gajdoˇsov´a, Daniel
Galbraith, Marcos Garcia, Moa G¨ardenfors, Kim
Gerdes, Filip Ginter, Iakes Goenaga, Koldo Go-
jenola, Memduh G¨okrmak, Yoav Goldberg, Xavier
G´omez Guinovart, Berta Gonz´ales Saavedra, Ma-
tias Grioni, Normunds Gr¯uz¯itis, Bruno Guillaume,
Nizar Habash, Jan Hajiˇc, Jan Hajiˇc jr., Linh H`a M,
Kim Harris, Dag Haug, Barbora Hladk´a, Jaroslava

Hlav´aˇcov´a, Florinel Hociung, Petter Hohle, Radu
Ion, Elena Irimia, Tom´aˇs Jel´ınek, Anders Jo-
hannsen, Fredrik Jørgensen, H¨uner Kas¸kara, Hi-
roshi Kanayama, Jenna Kanerva, Tolga Kayade-
len, V´aclava Kettnerov´a,
Jesse Kirchner, Na-
talia Kotsyba, Simon Krek, Veronika Laippala,
Lorenzo Lambertino, Tatiana Lando, John Lee,
Phng Lˆe Hng, Alessandro Lenci, Saran Lertpra-
dit, Herman Leung, Cheuk Ying Li, Josie Li, Key-
ing Li, Nikola Ljubeˇsi´c, Olga Loginova, Olga Lya-
shevskaya, Teresa Lynn, Vivien Macketanz, Aibek
Makazhanov, Michael Mandl, Christopher Man-
ning, C˘at˘alina M˘ar˘anduc, David Mareˇcek, Katrin
Marheinecke, H´ector Mart´ınez Alonso, Andr´e Mar-
tins, Jan Maˇsek, Yuji Matsumoto, Ryan McDon-
ald, Gustavo Mendonc¸a, Niko Miekka, Anna Mis-
sil¨a, C˘at˘alin Mititelu, Yusuke Miyao, Simonetta
Montemagni, Amir More, Laura Moreno Romero,
Shinsuke Mori, Bohdan Moskalevskyi, Kadri
Muischnek, Kaili M¨u¨urisep, Pinkey Nainwani,
Anna Nedoluzhko, Gunta Neˇspore-B¯erzkalne, Lng
Nguyn Th, Huyn Nguyn Th Minh, Vitaly Niko-
laev, Hanna Nurmi, Stina Ojala, Petya Osen-
ova, Robert ¨Ostling, Lilja Øvrelid, Elena Pascual,
Marco Passarotti, Cenel-Augusto Perez, Guy Per-
rier, Slav Petrov, Jussi Piitulainen, Emily Pitler,
Barbara Plank, Martin Popel, Lauma Pretkalnia,
Prokopis Prokopidis, Tiina Puolakainen, Sampo
Pyysalo, Alexandre Rademaker, Loganathan Ra-
masamy, Taraka Rama, Vinit Ravishankar, Livy
Real, Siva Reddy, Georg Rehm, Larissa Rinaldi,
Laura Rituma, Mykhailo Romanenko, Rudolf Rosa,
Davide Rovati, Benoˆıt Sagot, Shadi Saleh, Tanja
Samardˇzi´c, Manuela Sanguinetti, Baiba Saul¯ite, Se-
bastian Schuster, Djam´e Seddah, Wolfgang Seeker,
Mojgan Seraji, Mo Shen, Atsuko Shimada, Dmitry
Sichinava, Natalia Silveira, Maria Simi, Radu
Simionescu, Katalin Simk´o, M´aria ˇSimkov´a, Kiril
Simov, Aaron Smith, Antonio Stella, Milan Straka,
Jana Strnadov´a, Alane Suhr, Umut Sulubacak,
Zsolt Sz´ant´o, Dima Taji, Takaaki Tanaka, Trond
Trosterud, Anna Trukhina, Reut Tsarfaty, Francis
Tyers, Sumire Uematsu, Zdeˇnka Ureˇsov´a, Larraitz
Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van
Niekerk, Gertjan van Noord, Viktor Varga, Eric
Villemonte de la Clergerie, Veronika Vincze, Lars
Wallin, Jonathan North Washington, Mats Wir´en,
Tak-sum Wong, Zhuoran Yu, Zdenˇek ˇZabokrtsk´y,
Amir Zeldes, Daniel Zeman, and Hanzhi Zhu. 2017.
Universal dependencies 2.1.
LINDAT/CLARIN
digital library at the Institute of Formal and Ap-
plied Linguistics ( ´UFAL), Faculty of Mathematics
and Physics, Charles University.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proc. of LREC.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinkova, Dan Flickinger,
Jan Hajic, Angelina Ivanova, and Zdenka Uresova.

2016. Towards comparability of linguistic graph
banks for semantic parsing. In Proc. of LREC.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinkov´a, Dan Flickinger, Jan
Hajiˇc, and Zdeˇnka Ureˇsov´a. 2015. SemEval 2015
task 18: Broad-coverage semantic dependency pars-
ing. In Proc. of SemEval, pages 915–926.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajiˇc, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 task
8: Broad-coverage semantic dependency parsing. In
Proc. of SemEval, pages 63–72.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, 31(1).

Hao Peng, Sam Thomson, and Noah A. Smith. 2017a.
Deep multitask learning for semantic dependency
parsing. In Proc. of ACL, pages 2037–2048.

Hao Peng, Sam Thomson, Swabha Swayamdipta, and
Noah A. Smith. 2018. Learning joint semantic
parsers from disjoint data. In Proc. of NAACL-HLT.

Xiaochang Peng, Chuan Wang, Daniel Gildea, and Ni-
anwen Xue. 2017b. Addressing the data sparsity is-
sue in neural AMR parsing. In Proc. of EACL, pages
366–375.

Barbara Plank. 2016. Keystroke dynamics as signal
for shallow syntactic parsing. In Proc. of COLING,
pages 609–619.

Barbara Plank and Gertjan van Noord. 2011. Effective
measures of domain similarity for parsing. In Proc.
of ACL-HLT, pages 1566–1576.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Re-
search, 15:1929–1958.

Elior Sulem, Omri Abend, and Ari Rappoport. 2015.
Conceptual annotations preserve structure across
translations: A French-English case study. In Proc.
of S2MT, pages 11–22.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008.
The
CoNLL 2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proc. of CoNLL,
pages 159–177.

Swabha Swayamdipta, Miguel Ballesteros, Chris Dyer,
and Noah A. Smith. 2016. Greedy, joint syntactic-
In Proc. of
semantic parsing with stack LSTMs.
CoNLL, pages 187–197.

Swabha Swayamdipta, Sam Thomson, Chris Dyer, and
Noah A. Smith. 2017. Frame-semantic parsing with
softmax-margin segmental rnns and a syntactic scaf-
fold. CoRR, abs/1706.09528.

Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proc. of ACL, pages 589–596.

Chuan Wang, Sameer Pradhan, Xiaoman Pan, Heng Ji,
and Nianwen Xue. 2016. CAMR at SemEval-2016
task 8: An extended transition-based AMR parser.
In Proc. of SemEval, pages 1173–1178.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015a. Boosting transition-based AMR parsing
with reﬁned actions and auxiliary analyzers.
In
Proc. of ACL, pages 857–862.

Carl Pollard and Ivan A Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015b. A transition-based algorithm for AMR pars-
ing. In Proc. of NAACL, pages 366–375.

Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel
Marcu, and Jonathan May. 2015. Parsing English
into Abstract Meaning Representation using syntax-
In Proc. of EMNLP,
based machine translation.
pages 1143–1154.

Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce de-
pendency DAG parsing. In Proc. of COLING, pages
753–760.

Sanjiv Kumar Sashank J. Reddi, Satyen Kale. 2018.
On the convergence of Adam and beyond. ICLR.

Sebastian Schuster and Christopher D. Manning. 2016.
Enhanced English Universal Dependencies: An im-
proved representation for natural language under-
standing tasks. In Proc. of LREC. ELRA.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proc. of ACL, pages 231–235.

Yuan Zhang and David Weiss. 2016.

Stack-
propagation: Improved representation learning for
syntax. In Proc. of ACL, pages 1557–1566.

Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese treebank using a global dis-
In Proc. of IWPT, pages 162–
criminative model.
171.

Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang
Qu, Ran Li, and Yanhui Gu. 2016. AMR pars-
In Proc. of
ing with an incremental joint model.
EMNLP, pages 680–689.

Yftah Ziser and Roi Reichart. 2017. Neural structural
correspondence learning for domain adaptation. In
Proc. of CoNLL, pages 400–410.

Multitask Parsing Across Semantic Representations

Daniel Hershcovich1,2

Ari Rappoport2

Omri Abend2
1The Edmond and Lily Safra Center for Brain Sciences
2School of Computer Science and Engineering
Hebrew University of Jerusalem
{danielh,oabend,arir}@cs.huji.ac.il

8
1
0
2
 
y
a
M
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
7
8
2
0
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

The ability to consolidate information of
different types is at the core of intelli-
gence, and has tremendous practical value
in allowing learning for one task to beneﬁt
from generalizations learned for others. In
this paper we tackle the challenging task of
improving semantic parsing performance,
taking UCCA parsing as a test case, and
AMR, SDP and Universal Dependencies
(UD) parsing as auxiliary tasks. We ex-
periment on three languages, using a uni-
form transition-based system and learning
architecture for all parsing tasks. Despite
notable conceptual, formal and domain
differences, we show that multitask learn-
ing signiﬁcantly improves UCCA parsing
in both in-domain and out-of-domain set-
tings. Our code is publicly available.1

1 Introduction

Semantic parsing has arguably yet to reach its
full potential in terms of its contribution to down-
stream linguistic tasks, partially due to the limited
amount of semantically annotated training data.
This shortage is more pronounced in languages
other than English, and less researched domains.

Indeed, recent work in semantic parsing has
targeted, among others, Abstract Meaning Rep-
resentation (AMR; Banarescu et al., 2013), bilex-
ical Semantic Dependencies (SDP; Oepen et al.,
2016) and Universal Conceptual Cognitive An-
notation (UCCA; Abend and Rappoport, 2013).
While these schemes are formally different and
focus on different distinctions, much of their se-
mantic content is shared (Abend and Rappoport,
2017).

1http://github.com/danielhers/tupa

Multitask learning (MTL; Caruana, 1997) al-
lows exploiting the overlap between tasks to ef-
fectively extend the training data, and has greatly
advanced with neural networks and representation
learning (see §2). We build on these ideas and pro-
pose a general transition-based DAG parser, able
to parse UCCA, AMR, SDP and UD (Nivre et al.,
2016). We train the parser using MTL to obtain
signiﬁcant improvements on UCCA parsing over
single-task training in (1) in-domain and (2) out-
of-domain settings in English; (3) an in-domain
setting in German; and (4) an in-domain setting in
French, where training data is scarce.

The novelty of this work is in proposing a gen-
eral parsing and learning architecture, able to ac-
commodate such widely different parsing tasks,
and in leveraging it to show beneﬁts from learn-
ing them jointly.

2 Related Work

MTL has been used over the years for NLP tasks
with varying degrees of similarity, examples in-
cluding joint classiﬁcation of different arguments
in semantic role labeling (Toutanova et al., 2005),
and joint parsing and named entity recognition
(Finkel and Manning, 2009). Similar ideas, of pa-
rameter sharing across models trained with differ-
ent datasets, can be found in studies of domain
adaptation (Blitzer et al., 2006; Daume III, 2007;
Ziser and Reichart, 2017). For parsing, domain
adaptation has been applied successfully in parser
combination and co-training (McClosky et al.,
2010; Baucom et al., 2013).

in

tackling

formally

been
similar

Neural MTL has mostly

effec-
tive
tasks
(Søgaard and Goldberg, 2016), including multilin-
gual syntactic dependency parsing (Ammar et al.,
2016; Guo et al., 2016), as well as multilingual
(Duong et al., 2017), and cross-domain semantic

parsing
2017).

(Herzig and Berant, 2017; Fan et al.,

for

when

jointly

parsing,

Sharing parameters with a low-level

task
transition-based
has shown great beneﬁt
training
syntactic
with POS tagging (Bohnet and Nivre, 2012;
Zhang and Weiss, 2016), and with lexical anal-
ysis (Constant and Nivre, 2016; More, 2016).
re-
Recent work has achieved state-of-the-art
sults in multiple NLP tasks by jointly learning
the tasks forming the NLP standard pipeline
using a single neural model
(Collobert et al.,
2011; Hashimoto et al., 2017), thereby avoiding
cascading errors, common in pipelines.

Much effort has been devoted to joint learn-
ing of syntactic and semantic parsing, including
two CoNLL shared tasks (Surdeanu et al., 2008;
Hajiˇc et al., 2009). Despite their conceptual and
practical appeal, such joint models rarely outper-
form the pipeline approach (Llu´ıs and M`arquez,
2008; Henderson et al., 2013; Lewis et al., 2015;
Swayamdipta et al., 2016, 2017).

Peng et al. (2017a) performed MTL for SDP in
a closely related setting to ours. They tackled
three tasks, annotated over the same text and shar-
ing the same formal structures (bilexical DAGs),
with considerable edge overlap, but differing in
target representations (see §3). For all tasks, they
reported an increase of 0.5-1 labeled F1 points.
Recently, Peng et al. (2018) applied a similar ap-
proach to joint frame-semantic parsing and seman-
tic dependency parsing, using disjoint datasets,
and reported further improvements.

3 Tackled Parsing Tasks

In this section, we outline the parsing tasks we
address. We focus on representations that pro-
duce full-sentence analyses, i.e., produce a graph
covering all (content) words in the text, or the
lexical concepts they evoke. This contrasts with
“shallow” semantic parsing, primarily semantic
role labeling (SRL; Gildea and Jurafsky, 2002;
Palmer et al., 2005), which targets argument struc-
ture phenomena using ﬂat structures. We consider
four formalisms: UCCA, AMR, SDP and Univer-
sal Dependencies. Figure 1 presents one sentence
annotated in each scheme.

Universal Conceptual Cognitive Annotation.
UCCA (Abend and Rappoport, 2013) is a seman-
tic representation whose main design principles
are ease of annotation, cross-linguistic applicabil-

(a) UCCA

move-01

A
R
G
0

n
a
m
e

o
p
1

person

name

”John”

(b) AMR

(c) DM

(d) UD

LA

H

LR

After

LA

L

H

P

,U

A

graduation

John

A

P
moved

t i m e

A R G 0

after

o
p
1

graduate-01

A

R
to

C

Paris

ARG2

city

name

n
a
m
e

o
p
1

”Paris”

ARG1

top

ARG2

ARG2

ARG1

ARG1

ARG2

After

graduation

,

John moved

to

Paris

case

root

obl

punct

nsubj

obl

case

After

graduation

,

John moved

to

Paris

Figure 1: Example graph for each task. Figure 1a presents
a UCCA graph. The dashed edge is remote, while the blue
node and its outgoing edges represent inter-Scene linkage.
Pre-terminal nodes and edges are omitted for brevity. Fig-
ure 1b presents an AMR graph. Text tokens are not part of
the graph, and must be matched to concepts and constants by
alignment. Variables are represented by their concepts. Fig-
ure 1c presents a DM semantic dependency graph, containing
multiple roots: “After”, “moved” and “to”, of which “moved”
is marked as top. Punctuation tokens are excluded from SDP
graphs. Figure 1d presents a UD tree. Edge labels express
syntactic relations.

ity, and a modular architecture. UCCA represents
the semantics of linguistic utterances as directed
acyclic graphs (DAGs), where terminal (childless)
nodes correspond to the text tokens, and non-
terminal nodes to semantic units that participate in
some super-ordinate relation. Edges are labeled,
indicating the role of a child in the relation the par-
ent represents. Nodes and edges belong to one of
several layers, each corresponding to a “module”
of semantic distinctions. UCCA’s foundational
layer (the only layer for which annotated data ex-

ists) mostly covers predicate-argument structure,
semantic heads and inter-Scene relations.

UCCA distinguishes primary edges, corre-
sponding to explicit relations, from remote edges
(appear dashed in Figure 1a) that allow for a unit
to participate in several super-ordinate relations.
Primary edges form a tree in each layer, whereas
remote edges enable reentrancy, forming a DAG.

Abstract Meaning Representation. AMR
(Banarescu et al., 2013) is a semantic represen-
tation that encodes information about named
semantic roles,
entities,
word sense and co-reference. AMRs are rooted
directed graphs, in which both nodes and edges
are labeled. Most AMRs are DAGs, although
cycles are permitted.

argument

structure,

AMR differs from the other schemes we con-
sider in that it does not anchor its graphs in the
words of the sentence (Figure 1b). Instead, AMR
graphs connect variables, concepts (from a pre-
deﬁned set) and constants (which may be strings
or numbers). Still, most AMR nodes are alignable
to text tokens, a tendency used by AMR parsers,
which align a subset of the graph nodes to a subset
of the text tokens (concept identiﬁcation). In this
work, we use pre-aligned AMR graphs.

Pust et al.,

Despite the brief period since its inception,
AMR has been targeted by a number of works,
notably in two SemEval shared tasks (May, 2016;
May and Priyadarshi, 2017). To tackle its variety
of distinctions and unrestricted graph structure,
AMR parsers often use specialized methods.
Graph-based parsers construct AMRs by iden-
tifying concepts and scoring edges between
them, either in a pipeline fashion (Flanigan et al.,
2014; Artzi et al.,
2015;
2015;
Foland and Martin, 2017), or jointly (Zhou et al.,
2016). Another line of work trains machine trans-
lation models to convert strings into linearized
AMRs (Barzdins and Gosko, 2016; Peng et al.,
2017b; Konstas et al., 2017; Buys and Blunsom,
2017b). Transition-based AMR parsers either use
dependency trees as pre-processing, then mapping
them into AMRs (Wang et al., 2015a,b, 2016;
Goodman et al., 2016), or use a transition system
tailored to AMR parsing (Damonte et al., 2017;
Ballesteros and Al-Onaizan, 2017). We differ
from the above approaches in addressing AMR
parsing using the same general DAG parser used
for other schemes.

Semantic Dependency Parsing. SDP uses a set
of related representations, targeted in two recent
SemEval shared tasks (Oepen et al., 2014, 2015),
and extended by Oepen et al. (2016). They cor-
respond to four semantic representation schemes,
referred to as DM, PAS, PSD and CCD, represent-
ing predicate-argument relations between content
words in a sentence. All are based on semantic for-
malisms converted into bilexical dependencies—
directed graphs whose nodes are text
tokens.
Edges are labeled, encoding semantic relations be-
tween the tokens. Non-content tokens, such as
punctuation, are left out of the analysis (see Fig-
ure 1c). Graphs containing cycles have been re-
moved from the SDP datasets.

from DeepBank

We use one of the representations from the
SemEval shared tasks: DM (DELPH-IN MRS),
(Flickinger et al.,
converted
2012), a corpus of hand-corrected parses from
LinGO ERG (Copestake and Flickinger, 2000),
an HPSG (Pollard and Sag, 1994) using Minimal
Recursion Semantics (Copestake et al., 2005).

Universal Dependencies. UD (Nivre et al.,
2016, 2017) has quickly become the dominant
dependency scheme for syntactic annotation in
many languages, aiming for cross-linguistically
consistent and coarse-grained treebank annota-
tion. Formally, UD uses bilexical trees, with edge
labels representing syntactic relations between
words.

We use UD as an auxiliary task, inspired by pre-
vious work on joint syntactic and semantic parsing
(see §2).
In order to reach comparable analyses
cross-linguistically, UD often ends up in annota-
tion that is similar to the common practice in se-
mantic treebanks, such as linking content words to
content words wherever possible. Using UD fur-
ther allows conducting experiments on languages
other than English, for which AMR and SDP an-
notated data is not available (§7).

In addition to basic UD trees, we use the en-
hanced++ UD graphs available for English, which
are generated by the Stanford CoreNLP convert-
ers (Schuster and Manning, 2016).2 These include
additional and augmented relations between con-
tent words, partially overlapping with the notion
of remote edges in UCCA: in the case of control
verbs, for example, a direct relation is added in
enhanced++ UD between the subordinated verb

2http://github.com/stanfordnlp/CoreNLP

and its controller, which is similar to the seman-
tic schemes’ treatment of this construction.

Parser state

4 General Transition-based DAG Parser

All schemes considered in this work exhibit reen-
trancy and discontinuity (or non-projectivity), to
In addition, UCCA and AMR
varying degrees.
To parse these
contain non-terminal nodes.
graphs, we extend TUPA (Hershcovich et al.,
2017), a transition-based parser originally devel-
oped for UCCA, as it supports all these structural
properties. TUPA’s transition system can yield any
labeled DAG whose terminals are anchored in the
text tokens. To support parsing into AMR, which
uses graphs that are not anchored in the tokens,
we take advantage of existing alignments of the
graphs with the text tokens during training (§5).

First used for projective syntactic dependency
tree parsing (Nivre, 2003),
transition-based
parsers have since been generalized to parse into
many other graph families, such as (discontinu-
ous) constituency trees (e.g., Zhang and Clark,
and DAGs
2009; Maier and Lichte, 2016),
(e.g., Sagae and Tsujii, 2008; Du et al., 2015).
Transition-based parsers apply transitions incre-
mentally to an internal state deﬁned by a buffer
B of remaining tokens and nodes, a stack S
of unresolved nodes, and a labeled graph G of
constructed nodes and edges. When a terminal
state is reached, the graph G is the ﬁnal output.
the
A classiﬁer is used at each step to select
next transition, based on features that encode the
current state.

4.1 TUPA’s Transition Set

Given a sequence of tokens w1, . . . , wn, we pre-
dict a rooted graph G whose terminals are the to-
kens. Parsing starts with the root node on the
stack, and the input tokens in the buffer.

The TUPA transition set includes the standard
SHIFT and REDUCE operations, NODEX for cre-
ating a new non-terminal node and an X-labeled
edge, LEFT-EDGEX and RIGHT-EDGEX to create
a new primary X-labeled edge, LEFT-REMOTEX
and RIGHT-REMOTEX to create a new remote
X-labeled edge, SWAP to handle discontinuous
nodes, and FINISH to mark the state as terminal.

Although UCCA contains nodes without any
text tokens as descendants (called implicit units),
these nodes are infrequent and only cover 0.5% of
non-terminal nodes. For this reason we follow pre-

S

B
John moved to Paris .

G

,

L
After

H

P
graduation

Classiﬁer

transition

softmax

MLP

BiLSTM

Embeddings

After

graduation

. . .

to

Paris

Figure 2:
Hershcovich et al. (2017).
BiLTSM architecture.

Illustration of the TUPA model, adapted from
Top: parser state. Bottom:

vious work (Hershcovich et al., 2017) and discard
implicit units from the training and evaluation, and
so do not include transitions for creating them.

In AMR, implicit units are considerably more
common, as any unaligned concept with no
aligned descendents is implicit (about 6% of
the nodes).
Implicit AMR nodes usually re-
sult from alignment errors, or from abstract con-
cepts which have no explicit realization in the text
(Buys and Blunsom, 2017a). We ignore implicit
nodes when training on AMR as well. TUPA also
does not support node labels, which are ubiqui-
tous in AMR but absent in UCCA structures (only
edges are labeled in UCCA). We therefore only
produce edge labels and not node labels when
training on AMR.

4.2 Transition Classiﬁer

To predict the next transition at each step, we use
a BiLSTM with embeddings as inputs, followed
by an MLP and a softmax layer for classiﬁcation
(Kiperwasser and Goldberg, 2016). The model is
Inference is performed
illustrated in Figure 2.
greedily, and training is done with an oracle that
yields the set of all optimal transitions at a given
state (those that lead to a state from which the gold
graph is still reachable). Out of this set, the ac-
tual transition performed in training is the one with
the highest score given by the classiﬁer, which is

trained to maximize the sum of log-likelihoods of
all optimal transitions at each step.

After

L

P

H

H

A

,U

A

P

graduation

John

moved

After

o
p

graduation

ti m e

A R G 0

(a) UCCA

moved

A
R
G
0

n
a
m
e

John

(b) AMR

r o o t

A

R

G1

root

t
o
p

1

G

R

A

A

R

to

A

R

G2

C

Paris

n
a
m
e

Paris

d
a
e
h

A

R

G

2

d
a
e
h

A

R

G

2

h

e

a

d

ARG2

Afterggraduation ,

movedg

tog

Parisg

G1
R
A
g John

(c) DM

b l

o

obl

punct

h

e

a

d

n
s
u
b
j

ase
c

h

h

e

e

a

a

d

d

ase
c

Afterg

graduation

,g

Johng movedgtog

Parisg

(d) UD

Figure 3: Graphs from Figure 1, after conversion to the uni-
ﬁed DAG format (with pre-terminals omitted: each terminal
drawn in place of its parent). Figure 3a presents a converted
UCCA graph. Linkage nodes and edges are removed, but
the original graph is otherwise preserved. Figure 3b presents
a converted AMR graph, with text tokens added according
to the alignments. Numeric sufﬁxes of op relations are re-
moved, and names collapsed. Figure 3c presents a converted
SDP graph (in the DM representation), with intermediate
non-terminal head nodes introduced. In case of reentrancy,
an arbitrary reentrant edge is marked as remote. Figure 3d
presents a converted UD graph. As in SDP, intermediate non-
terminals and head edges are introduced. While converted
UD graphs form trees, enhanced++ UD graphs may not.

Features. We use the original TUPA features,
representing the words, POS tags, syntactic depen-
dency relations, and previously predicted edge la-
bels for nodes in speciﬁc locations in the parser

state. In addition, for each token we use embed-
dings representing the one-character preﬁx, three-
character sufﬁx, shape (capturing orthographic
features, e.g., “Xxxx”), and named entity type,3
all provided by spaCy (Honnibal and Montani,
2018).4 To the learned word vectors, we concate-
nate the 250K most frequent word vectors from
fastText (Bojanowski et al., 2017),5 pre-trained
over Wikipedia and updated during training.

Constraints. As each annotation scheme has
different constraints on the allowed graph struc-
tures, we apply these constraints separately for
each task. During training and parsing, the rele-
vant constraint set rules out some of the transitions
according to the parser state. Some constraints
are task-speciﬁc, others are generic. For exam-
ple, in UCCA, a terminal may only have one par-
ent. In AMR, a concept corresponding to a Prop-
Bank frame may only have the core arguments de-
ﬁned for the frame as children. An example of
a generic constraint is that stack nodes that have
been swapped should not be swapped again.6

5 Uniﬁed DAG Format

To apply our parser to the four target tasks (§3), we
convert them into a uniﬁed DAG format, which is
inclusive enough to allow representing any of the
schemes with very little loss of information.7

The format consists of a rooted DAG, where the
tokens are the terminal nodes. As in the UCCA
format, edges are labeled (but not nodes), and are
divided into primary and remote edges, where the
primary edges form a tree (all nodes have at most
one primary parent, and the root has none). Re-
mote edges enable reentrancy, and thus together
with primary edges form a DAG. Figure 3 shows
examples for converted graphs. Converting UCCA
into the uniﬁed format consists simply of remov-
ing linkage nodes and edges (see Figure 3a), which
were also discarded by Hershcovich et al. (2017).

3See Supplementary Material for a full listing of features.
4http://spacy.io
5http://fasttext.cc
6 To implement this constraint, we deﬁne a swap index for
each node, assigned when the node is created. At initializa-
tion, only the root node and terminals exist. We assign the
root a swap index of 0, and for each terminal, its position in
the text (starting at 1). Whenever a node is created as a result
of a NODE transition, its swap index is the arithmetic mean
of the swap indices of the stack top and buffer head.

7See Supplementary Material for more conversion details.

Parser state

Classiﬁer

. . .

transition

softmax

Task-speciﬁc MLP

Task-speciﬁc BiLSTM

Shared BiLSTM

Shared embeddings

After

graduation

. . .

to

Paris

Figure 4: MTL model. Token representations are computed
both by a task-speciﬁc and a shared BiLSTM. Their outputs
are concatenated with the parser state embedding, identical to
Figure 2, and fed into the task-speciﬁc MLP for selecting the
next transition. Shared parameters are shown in blue.

Converting bilexical dependencies. To convert
DM and UD into the uniﬁed DAG format, we add
a pre-terminal for each token, and attach the pre-
terminals according to the original dependency
edges: traversing the tree from the root down, for
each head token we create a non-terminal parent
with the edge label head, and add the node’s de-
pendents as children of the created non-terminal
node (see Figures 3c and 3d). Since DM allows
multiple roots, we form a single root node, whose
children are the original roots. The added edges
are labeled root, where top nodes are labeled top
instead. In case of reentrancy, an arbitrary parent
is marked as primary, and the rest as remote (de-
noted as dashed edges in Figure 3).

Converting AMR.
In the conversion from
AMR, node labels are dropped. Since alignments
are not part of the AMR graph (see Figure 3b), we
use automatic alignments (see §7), and attach each
node with an edge to each of its aligned terminals.
Named entities in AMR are represented as a
subgraph, whose name-labeled root has a child for
each token in the name (see the two name nodes in
Figure 1b). We collapse this subgraph into a single
node whose children are the name tokens.

6 Multitask Transition-based Parsing

Now that the same model can be applied to dif-
ferent tasks, we can train it in a multitask setting.
The fairly small training set available for UCCA
(see §7) makes MTL particularly appealing, and

we focus on it in this paper, treating AMR, DM
and UD parsing as auxiliary tasks.

of

the

2016;

Plank,

Søgaard and Goldberg,

Following previous work, we share only
(Klerke et al.,
parameters
some
2016;
2016;
Bollmann and Søgaard,
2016;
Braud et al., 2016; Mart´ınez Alonso and Plank,
2017; Peng et al., 2017a, 2018),
leaving task-
speciﬁc sub-networks as well. Concretely, we
keep the BiLSTM used by TUPA for the main
task (UCCA parsing), add a BiLSTM that
is
shared across all tasks, and replicate the MLP
(feedforward sub-network) for each task. The
BiLSTM outputs (concatenated for the main task)
are fed into the task-speciﬁc MLP (see Figure 4).
Feature embeddings are shared across tasks.

Unlabeled parsing for auxiliary tasks. To sim-
plify the auxiliary tasks and facilitate generaliza-
tion (Bingel and Søgaard, 2017), we perform un-
labeled parsing for AMR, DM and UD, while still
predicting edge labels in UCCA parsing. To sup-
port unlabeled parsing, we simply remove all la-
bels from the EDGE, REMOTE and NODE tran-
sitions output by the oracle. This results in a
much smaller number of transitions the classiﬁer
has to select from (no more than 10, as opposed
to 45 in labeled UCCA parsing), allowing us to
use no BiLSTMs and fewer dimensions and layers
for task-speciﬁc MLPs of auxiliary tasks (see §7).
This limited capacity forces the network to use the
shared parameters for all tasks, increasing gener-
alization (Mart´ınez Alonso and Plank, 2017).

7 Experimental Setup

We here detail a range of experiments to assess
the value of MTL to UCCA parsing, training the
parser in single-task and multitask settings, and
evaluating its performance on the UCCA test sets
in both in-domain and out-of-domain settings.

Data. For UCCA, we use v1.2 of the English
Wikipedia corpus (Wiki; Abend and Rappoport,
2013), with the standard train/dev/test split (see
Table 1), and the Twenty Thousand Leagues Un-
der the Sea corpora (20K; Sulem et al., 2015), an-
notated in English, French and German.8 For En-
glish and French we use 20K v1.0, a small par-
allel corpus comprising the ﬁrst ﬁve chapters of
the book. As in previous work (Hershcovich et al.,

8http://github.com/huji-nlp/ucca-corpora

English

French

German

# tokens
dev

train

test

# sentences
train dev test

# tokens
train dev test

# sentences
train dev test

# tokens
dev

test

# sentences
train dev test

train

128444 14676 15313 4268 454 503
12339

506 10047 1558 1324 413 67 67 79894 10059 42366 3429 561 2164

UCCA
Wiki
20K
AMR 648950
DM 765025
UD
458277

36521
33964
17062

Table 1: Number of tokens and sentences in the training, development and test sets we use for each corpus and language.

899163

32347

268145

13814

2017), we use the English part only as an out-of-
domain test set. We train and test on the French
part using the standard split, as well as the Ger-
man corpus (v0.9), which is a pre-release and still
contains a considerable amount of noisy annota-
tion. Tuning is performed on the respective devel-
opment sets.

to the dataset

For AMR, we use LDC2017T10,

identi-
targeted in SemEval 2017
cal
(May and Priyadarshi, 2017).9
For SDP, we
use the DM representation from the SDP 2016
dataset (Oepen et al., 2016).10
For Universal
Dependencies, we use all English, French and
German treebanks from UD v2.1 (Nivre et al.,
2017).11 We use the enhanced++ UD representa-
tion (Schuster and Manning, 2016) in our English
experiments, henceforth referred to as UD++. We
use only the AMR, DM and UD training sets from
standard splits.

While UCCA is annotated over Wikipedia and
over a literary corpus, the domains for AMR, DM
and UD are blogs, news, emails, reviews, and
Q&A. This domain difference between training
and test is particularly challenging (see §9). Un-
fortunately, none of the other schemes have avail-
able annotation over Wikipedia text.

Settings. We explore the following settings: (1)
in-domain setting in English, training and test-
ing on Wiki; (2) out-of-domain setting in English,
training on Wiki and testing on 20K; (3) French in-
domain setting, where available training dataset is
small, training and testing on 20K; (4) German in-
domain setting on 20K, with somewhat noisy an-
notation. For MTL experiments, we use unlabeled
AMR, DM and UD++ parsing as auxiliary tasks in
English, and unlabeled UD parsing in French and
German.12 We also report baseline results training

9http://catalog.ldc.upenn.edu/LDC2017T10
10http://sdp.delph-in.net/osdp-12.tgz
11http://hdl.handle.net/11234/1-2515
12We did not use AMR, DM or UD++ in French and Ger-

man, as these are only available in English.

only the UCCA training sets.

Training. We create a uniﬁed corpus for each
shufﬂing all sentences from relevant
setting,
datasets together, but using only the UCCA devel-
opment set F1 score as the early stopping criterion.
In each training epoch, we use the same number of
examples from each task—the UCCA training set
size. Since training sets differ in size, we sample
this many sentences from each one. The model is
implemented using DyNet (Neubig et al., 2017).13

Multitask

Single Main Aux Shared

Hyperparameter
Pre-trained word dim.
Learned word dim.
POS tag dim.
Dependency relation dim.
Named entity dim.
Punctuation dim.
Action dim.
Edge label dim.
MLP layers
MLP dimensions
BiLSTM layers
BiLSTM dimensions

300
200
20
10
3
1
3
20
2
50
2
500

300
200
20
10
3
1
3

2
300

1
50

20
2
50
2
300

Table 2: Hyperparameter settings. Middle column shows hy-
perparameters used for the single-task architecture, described
in §4.2, and right column for the multitask architecture, de-
scribed in §6. Main refers to parameters speciﬁc to the main
task—UCCA parsing (task-speciﬁc MLP and BiLSTM, and
edge label embedding), Aux to parameters speciﬁc to each
auxiliary task (task-speciﬁc MLP, but no edge label embed-
ding since the tasks are unlabeled), and Shared to parameters
shared among all tasks (shared BiLSTM and embeddings).

Hyperparameters. We initialize embeddings
(Srivastava et al.,
randomly. We use dropout
2014) between MLP layers, and recurrent dropout
(Gal and Ghahramani, 2016) between BiLSTM
layers, both with p = 0.4. We also use word
(α = 0.2), tag (α = 0.2) and dependency relation
(α = 0.5) dropout (Kiperwasser and Goldberg,
2016).14
In addition, we use a novel form of

13http://dynet.io
14In training, the embedding for a feature value w is re-
α
#(w)+α ,

placed with a zero vector with a probability of
where #(w) is the number of occurrences of w observed.

Primary
LR

LF

LP

Remote

LP

LR LF

73.5
73.6
73.7

72.7
72.9
72.8

English (in-domain)
74.4
HAR17
Single
74.4
74.7
AMR
75.7⋆ 73.9⋆ 74.8⋆ 54.9
DM
75⋆
UD++
75.6⋆ 73.9⋆ 74.7⋆ 49.9
AMR + DM
AMR + UD++ 74.9
47.1
DM + UD++
All

75.9⋆ 73.9⋆ 74.9⋆ 48
75.6⋆ 73.1

51.6 49.4
47.4
53
51.5
50
48.7⋆ 51.1 49.9
53.9
53
52.7 50.8
51.4
53
50
48.5
54.8 51.2
53.2 52

74.4⋆ 50.9

74.1⋆ 49

73.2

72.7

73.8

Table 3: Labeled precision, recall and F1 (in %) for primary
and remote edges, on the Wiki test set. ⋆ indicates signiﬁ-
cantly better than Single. HAR17: Hershcovich et al. (2017).

dropout, node dropout: with a probability of 0.1
at each step, all features associated with a single
node in the parser state are replaced with zero vec-
tors. For optimization we use a minibatch size of
100, decaying all weights by 10−5 at each update,
and train with stochastic gradient descent for N
epochs with a learning rate of 0.1, followed by
AMSGrad (Sashank J. Reddi, 2018) for N epochs
with α = 0.001, β1 = 0.9 and β2 = 0.999.
We use N = 50 for English and German, and
N = 400 for French. We found this training strat-
egy better than using only one of the optimization
methods, similar to ﬁndings by Keskar and Socher
(2017). We select the epoch with the best aver-
age labeled F1 score on the UCCA development
set. Other hyperparameter settings are listed in Ta-
ble 2.

Evaluation. We evaluate on UCCA using la-
recall and F1 on primary
beled precision,
and remote edges,
following previous work
(Hershcovich et al., 2017). Edges in predicted and
gold graphs are matched by terminal yield and la-
bel. Signiﬁcance testing of improvements over
the single-task model is done by the bootstrap test
(Berg-Kirkpatrick et al., 2012), with p < 0.05.

8 Results

Table 3 presents our results on the English in-
domain Wiki test set. MTL with all auxiliary tasks
and their combinations improves the primary F1
In most set-
score over the single task baseline.
tings the improvement is statistically signiﬁcant.
Using all auxiliary tasks contributed less than just
DM and UD++, the combination of which yielded
the best scores yet in in-domain UCCA parsing,
with 74.9% F1 on primary edges. Remote F1 is
improved in some settings, but due to the rela-

Primary
LR

LF

LP

Remote

LP LR

LF

41.4 22

68.5
69
69.5

69.8⋆ 69.7

38.6 18.8
68.6
68.7
41.2 19.8
69
69
69.5
42.9 20.2
69.5
70.7⋆ 70.7⋆ 70.7⋆ 42.7 18.6
69.6
70.7⋆ 70.2⋆ 70.5⋆ 45.8 19.4
45.1 21.8
70.8⋆ 70.3⋆ 70.6⋆ 41.6 21.6
71.2⋆ 70.9⋆ 71⋆

English (out-of-domain)
HAR17
Single
AMR
DM
UD++
AMR + DM
AMR + UD++ 70.2⋆ 69.9⋆ 70⋆
DM + UD++
All
French (in-domain)
68.2
Single
UD
70.3
German (in-domain)
73.3
Single
27.1
73.7⋆ 72.6⋆ 73.2⋆ 61.8 24.9⋆ 35.5⋆
UD

25.3
26.7
27.5
25.9
28.7
27.3
29.4
28.4
29.6

67.6
9.4
26
70.1⋆ 43.8 13.2

57.1 17.7

13.9
20.3

45.1 22

67
70⋆

71.7

72.5

Table 4: Labeled precision, recall and F1 (in %) for primary
and remote edges, on the 20K test sets. ⋆ indicates signiﬁ-
cantly better than Single. HAR17: Hershcovich et al. (2017).

tively small number of remote edges (about 2%
of all edges), none of the differences is signiﬁcant.
Note that our baseline single-task model (Single)
is slightly better than the current state-of-the-art
(HAR17; Hershcovich et al., 2017), due to the in-
corporation of additional features (see §4.2).

Table 4 presents our experimental results on the
20K corpora in the three languages. For English
improvements from using MTL
out-of-domain,
are even more marked. Moreover, the improve-
ment is largely additive: the best model, using all
three auxiliary tasks (All), yields an error reduc-
tion of 2.9%. Again, the single-task baseline is
slightly better than HAR17.

The contribution of MTL is also apparent in
French and German in-domain parsing: 3.7% er-
ror reduction in French (having less than 10%
as much UCCA training data as English) and
1% in German, where the training set is com-
parable in size to the English one, but is noisier
(see §7). The best MTL models are signiﬁcantly
better than single-task models, demonstrating that
even a small training set for the main task may
sufﬁce, given enough auxiliary training data (as in
French).

9 Discussion

success

Quantifying the similarity between tasks.
in
Task similarity is
2017;
MTL
In our case,
Mart´ınez Alonso and Plank, 2017).
the main and auxiliary tasks are annotated on
different corpora from different domains (§7), and

an important
(Bingel and Søgaard,

factor

Wiki
20K
AMR
DM

20K
1.047

AMR
0.895
0.949

DM
0.913
0.971
0.757

UD
0.843
0.904
0.469
0.754

Table 5: L1 distance between dataset word distributions,
quantifying domain differences in English (low is similar).

Primary
UR
15.6
49.2
84.6

UP
53.8
65
82.7

UF
24.2
56
83.6

Remote
UR
5.5
65.9
12.7

UP
7.3
7.4
12.5

UF
6.3
13.3
12.6

AMR
DM
UD++

Table 6: Unlabeled F1 scores between the representations of
the same English sentences (from PTB WSJ), converted to
the uniﬁed DAG format, and annotated UCCA graphs.

the target representations vary both in form and in
content.

To quantify the domain differences, we follow
Plank and van Noord (2011) and measure the L1
distance between word distributions in the English
training sets and 20K test set (Table 5). All aux-
iliary training sets are more similar to 20K than
Wiki is, which may contribute to the beneﬁts ob-
served on the English 20K test set.

As a measure of the formal similarity of the dif-
ferent schemes to UCCA, we use unlabeled F1
score evaluation on both primary and remote edges
(ignoring edge labels). To this end, we annotated
100 English sentences from Section 02 of the Penn
Treebank Wall Street Journal (PTB WSJ). Anno-
tation was carried out by a single expert UCCA
annotator, and is publicly available.15 These sen-
tences had already been annotated by the AMR,
DM and PTB schemes,16 and we convert their an-
notation to the uniﬁed DAG format.

Unlabeled F1 scores between the UCCA graphs
and those converted from AMR, DM and UD++
are presented in Table 6. UD++ is highly over-
lapping with UCCA, while DM less so, and AMR
even less (cf. Figure 3).

Comparing the average improvements resulting
from adding each of the tasks as auxiliary (see §8),
we ﬁnd AMR the least beneﬁcial, UD++ second,
and DM the most beneﬁcial, in both in-domain
and out-of-domain settings. This trend is weakly
correlated with the formal similarity between the
tasks (as expressed in Table 6), but weakly neg-
atively correlated with the word distribution simi-

15http://github.com/danielhers/wsj
16We convert
the PTB format

to UD++ v1 using
Stanford CoreNLP, and then to UD v2 using Udapi:
http://github.com/udapi/udapi-python.

larity scores (Table 5). We conclude that other fac-
tors should be taken into account to fully explain
this effect, and propose to address this in future
work through controlled experiments, where cor-
pora of the same domain are annotated with the
various formalisms and used as training data for
MTL.

AMR, SDP and UD parsing. Evaluating the
full MTL model (All) on the unlabeled auxil-
iary tasks yielded 64.7% unlabeled Smatch F1
(Cai and Knight, 2013) on the AMR develop-
ment set, when using oracle concept identiﬁca-
tion (since the auxiliary model does not predict
node labels), 27.2% unlabeled F1 on the DM de-
velopment set, and 4.9% UAS on the UD devel-
opment set. These poor results reﬂect the fact
that model selection was based on the score on
the UCCA development set, and that the model
parameters dedicated to auxiliary tasks were very
limited (to encourage using the shared param-
eters). However, preliminary experiments us-
ing our approach produced promising results on
each of the tasks’ respective English develop-
ment sets, when treated as a single task: 67.1%
labeled Smatch F1 on AMR (adding a transi-
tion for implicit nodes and classiﬁer for node la-
bels), 79.1% labeled F1 on DM, and 80.1% LAS
F1 on UD. For comparison, the best results on
these datasets are 70.7%, 91.2% and 82.2%, re-
spectively (Foland and Martin, 2017; Peng et al.,
2018; Dozat et al., 2017).

10 Conclusion

We demonstrate that semantic parsers can leverage
a range of semantically and syntactically anno-
tated data, to improve their performance. Our ex-
periments show that MTL improves UCCA pars-
ing, using AMR, DM and UD parsing as auxil-
iaries. We propose a uniﬁed DAG representation,
construct protocols for converting these schemes
into the uniﬁed format, and generalize a transition-
based DAG parser to support all these tasks, allow-
ing it to be jointly trained on them.

While we focus on UCCA in this work, our
parser is capable of parsing any scheme that can
be represented in the uniﬁed DAG format, and pre-
liminary results on AMR, DM and UD are promis-
ing (see §9). Future work will investigate whether
a single algorithm and architecture can be com-
petitive on all of these parsing tasks, an important
step towards a joint many-task model for semantic

parsing.

Acknowledgments

This work was supported by the Israel Science
Foundation (grant no. 929/17), by the HUJI Cy-
ber Security Research Center in conjunction with
the Israel National Cyber Bureau in the Prime
Minister’s Ofﬁce, and by the Intel Collaborative
Research Institute for Computational Intelligence
(ICRI-CI). The ﬁrst author was supported by a fel-
lowship from the Edmond and Lily Safra Center
for Brain Sciences. We thank Roi Reichart, Rotem
Dror and the anonymous reviewers for their help-
ful comments.

References

Omri Abend and Ari Rappoport. 2013. Universal Con-
ceptual Cognitive Annotation (UCCA). In Proc. of
ACL, pages 228–238.

Omri Abend and Ari Rappoport. 2017. The state of
the art in semantic representation. In Proc. of ACL,
pages 77–89.

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah Smith. 2016. Many lan-
guages, one parser. TACL, 4:431–444.

Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
Broad-coverage CCG semantic parsing with AMR.
In Proc. of EMNLP, pages 1699–1710.

Miguel Ballesteros and Yaser Al-Onaizan. 2017. AMR
In Proc. of EMNLP,

parsing using stack-LSTMs.
pages 1269–1275.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin
Knight, Martha Palmer, and Nathan Schneider.
2013. Abstract Meaning Representation for sem-
In Proc. of the Linguistic Annotation
banking.
Workshop.

Guntis Barzdins and Didzis Gosko. 2016. RIGA at
SemEval-2016 task 8: Impact of Smatch extensions
and character-level neural translation on AMR pars-
In Proc. of SemEval, pages 1143–
ing accuracy.
1147.

Eric Baucom, Levi King, and Sandra K¨ubler. 2013.
Domain adaptation for parsing. In Proc. of RANLP,
pages 56–64.

Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statistical
In Proc. of EMNLP-CoNLL,
signiﬁcance in NLP.
pages 995–1005.

Joachim Bingel and Anders Søgaard. 2017. Identify-
ing beneﬁcial task relations for multi-task learning
in deep neural networks. In Proc. of EACL, pages
164–169.

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120–128.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proc.
of EMNLP-CoNLL, pages 1455–1465.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL, 5:135–146.

Marcel Bollmann and Anders Søgaard. 2016.

Im-
proving historical spelling normalization with bi-
directional lstms and multi-task learning. In Proc.
of COLING, pages 131–139.

Chlo´e Braud, Barbara Plank, and Anders Søgaard.
2016. Multi-view and multi-task training of RST
In Proc. of COLING, pages
discourse parsers.
1903–1913.

Jan Buys and Phil Blunsom. 2017a.

Oxford at
SemEval-2017 task 9: Neural AMR parsing with
pointer-augmented attention. In Proc. of SemEval,
pages 914–919.

Jan Buys and Phil Blunsom. 2017b. Robust incremen-
tal neural semantic graph parsing. In Proc. of ACL,
pages 1215–1226.

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Proc.
of ACL, pages 748–752.

Rich Caruana. 1997. Multitask Learning. Machine

Learning, 28(1):41–75.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537.

Matthieu Constant and Joakim Nivre. 2016.

A
transition-based system for joint lexical and syntac-
tic analysis. In Proc. of ACL, pages 161–171.

Ann Copestake and Dan Flickinger. 2000.

An
open source grammar development environment and
broad-coverage English grammar using HPSG.
In
Proc. of LREC, pages 591–600.

Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3(2):281–332.

Marco Damonte, Shay B. Cohen, and Giorgio Satta.
2017. An incremental parser for Abstract Meaning
Representation. In Proc. of EACL.

Hal Daume III. 2007. Frustratingly easy domain adap-

tation. In Proc. of ACL, pages 256–263.

Timothy Dozat, Peng Qi, and Christopher D. Manning.
2017. Stanford’s graph-based neural dependency
In Proc. of
parser at the conll 2017 shared task.
CoNLL, pages 20–30.

Yantao Du, Fan Zhang, Xun Zhang, Weiwei Sun, and
Xiaojun Wan. 2015. Peking: Building semantic de-
pendency graphs with a hybrid parser. In Proc. of
SemEval, pages 927–931.

Long Duong, Hadi Afshar, Dominique Estival, Glen
Pink, Philip Cohen, and Mark Johnson. 2017. Mul-
tilingual semantic parsing and code-switching.
In
Proc. of CoNLL, pages 379–389.

Xing Fan, Emilio Monti, Lambert Mathias, and Markus
Dreyer. 2017. Transfer learning for neural seman-
tic parsing. In Proc. of Workshop on Representation
Learning for NLP, pages 48–56.

Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc.
of NAACL-HLT, pages 326–334.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the Abstract Meaning
Representation. In Proc. of ACL, pages 1426–1436.

Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank: A dynamically annotated treebank of the
Wall Street Journal. In Proc. of Workshop on Tree-
banks and Linguistic Theories, pages 85–96.

Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proc. of CoNLL,
pages 1–18.

Kazuma Hashimoto, caiming xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple NLP
tasks. In Proc. of EMNLP, pages 1923–1933.

James Henderson, Paola Merlo,

Ivan Titov, and
Gabriele Musillo. 2013. Multilingual joint pars-
ing of syntactic and semantic dependencies with a
latent variable model. Computational Linguistics,
39(4):949–998.

Daniel Hershcovich, Omri Abend, and Ari Rappoport.
2017. A transition-based directed acyclic graph
In Proc. of ACL, pages 1127–
parser for UCCA.
1138.

Jonathan Herzig and Jonathan Berant. 2017. Neural
semantic parsing over multiple knowledge-bases. In
Proc. of ACL, pages 623–628.

Matthew Honnibal and Ines Montani. 2018. spaCy 2:
Natural language understanding with Bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Nitish Shirish Keskar and Richard Socher. 2017. Im-
proving generalization performance by switching
from Adam to SGD. CoRR, abs/1712.07628.

William Foland and James H. Martin. 2017. Abstract
Meaning Representation parsing using LSTM recur-
rent neural networks. In Proc. of ACL, pages 463–
472.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. TACL, 4:313–
327.

Yarin Gal and Zoubin Ghahramani. 2016. A Theoreti-
cally Grounded Application of Dropout in Recurrent
Neural Networks. In D D Lee, M Sugiyama, U V
Luxburg, I Guyon, and R Garnett, editors, Advances
in Neural Information Processing Systems 29, pages
1019–1027. Curran Associates, Inc.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3).

James Goodman, Andreas Vlachos, and Jason Narad-
owsky. 2016. Noise reduction and targeted explo-
ration in imitation learning for Abstract Meaning
Representation parsing. In Proc. of ACL, pages 1–
11.

Sigrid Klerke, Yoav Goldberg, and Anders Søgaard.
2016.
Improving sentence compression by learn-
ing to predict gaze. In Proc. of NAACL-HLT, pages
1528–1533.

Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin
Choi, and Luke Zettlemoyer. 2017. Neural AMR:
Sequence-to-sequence models for parsing and gen-
eration. In Proc. of ACL, pages 146–157.

Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015.
Joint A* CCG parsing and semantic role labelling.
In Proc. of EMNLP, pages 1444–1454.

Xavier Llu´ıs and Llu´ıs M`arquez. 2008. A joint model
for parsing syntactic and semantic dependencies. In
Proc. of CoNLL, pages 188–192.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2016. Exploiting multi-typed treebanks for
CoRR,
parsing with deep multi-task learning.
abs/1606.01161.

Wolfgang Maier and Timm Lichte. 2016. Discontinu-
ous parsing with continuous trees. In Proc. of Work-
shop on Discontinuous Structures in NLP, pages 47–
57.

H´ector Mart´ınez Alonso and Barbara Plank. 2017.
When is multitask learning effective? Semantic se-
quence prediction under varying data conditions. In
Proc. of EACL, pages 44–53.

Jonathan May. 2016. SemEval-2016 task 8: Meaning
representation parsing. In Proc. of SemEval, pages
1063–1073.

Jonathan May and Jay Priyadarshi. 2017. SemEval-
2017 task 9: Abstract Meaning Representation pars-
ing and generation. In Proc. of SemEval, pages 536–
545.

David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Proc. of NAACL-HLT, pages 28–36.

Amir More. 2016. Joint morpho-syntactic processing
of morphologically rich languages in a transition-
based framework. Master’s thesis, The Interdisci-
plinary Center, Herzliya.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. DyNet:
CoRR,
The dynamic neural network toolkit.
abs/1701.03980.

Joakim Nivre. 2003. An efﬁcient algorithm for projec-
tive dependency parsing. In Proc. of IWPT, pages
149–160.

Joakim Nivre,

ˇZeljko Agi´c, Lars Ahrenberg, Lene
Antonsen, Maria Jesus Aranzabe, Masayuki Asa-
hara, Luma Ateyah, Mohammed Attia, Aitz-
iber Atutxa, Liesbeth Augustinus, Elena Bad-
maeva, Miguel Ballesteros, Esha Banerjee, Sebas-
tian Bank, Verginica Barbu Mititelu, John Bauer,
Kepa Bengoetxea, Riyaz Ahmad Bhat, Eckhard
Bick, Victoria Bobicev, Carl B¨orstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Aljoscha Bur-
chardt, Marie Candito, Gauthier Caron, G¨uls¸en
Cebirolu Eryiit, Giuseppe G. A. Celano, Savas
Cetin, Fabricio Chalub, Jinho Choi, Silvie Cinkov´a,
C¸ ar C¸ ¨oltekin, Miriam Connor, Elizabeth David-
son, Marie-Catherine de Marneffe, Valeria de Paiva,
Arantza Diaz de Ilarraza, Peter Dirix, Kaja Do-
brovoljc, Timothy Dozat, Kira Droganova, Puneet
Dwivedi, Marhaba Eli, Ali Elkahky, Tomaˇz Erjavec,
Rich´ard Farkas, Hector Fernandez Alcalde, Jennifer
Foster, Cl´audia Freitas, Katar´ına Gajdoˇsov´a, Daniel
Galbraith, Marcos Garcia, Moa G¨ardenfors, Kim
Gerdes, Filip Ginter, Iakes Goenaga, Koldo Go-
jenola, Memduh G¨okrmak, Yoav Goldberg, Xavier
G´omez Guinovart, Berta Gonz´ales Saavedra, Ma-
tias Grioni, Normunds Gr¯uz¯itis, Bruno Guillaume,
Nizar Habash, Jan Hajiˇc, Jan Hajiˇc jr., Linh H`a M,
Kim Harris, Dag Haug, Barbora Hladk´a, Jaroslava

Hlav´aˇcov´a, Florinel Hociung, Petter Hohle, Radu
Ion, Elena Irimia, Tom´aˇs Jel´ınek, Anders Jo-
hannsen, Fredrik Jørgensen, H¨uner Kas¸kara, Hi-
roshi Kanayama, Jenna Kanerva, Tolga Kayade-
len, V´aclava Kettnerov´a,
Jesse Kirchner, Na-
talia Kotsyba, Simon Krek, Veronika Laippala,
Lorenzo Lambertino, Tatiana Lando, John Lee,
Phng Lˆe Hng, Alessandro Lenci, Saran Lertpra-
dit, Herman Leung, Cheuk Ying Li, Josie Li, Key-
ing Li, Nikola Ljubeˇsi´c, Olga Loginova, Olga Lya-
shevskaya, Teresa Lynn, Vivien Macketanz, Aibek
Makazhanov, Michael Mandl, Christopher Man-
ning, C˘at˘alina M˘ar˘anduc, David Mareˇcek, Katrin
Marheinecke, H´ector Mart´ınez Alonso, Andr´e Mar-
tins, Jan Maˇsek, Yuji Matsumoto, Ryan McDon-
ald, Gustavo Mendonc¸a, Niko Miekka, Anna Mis-
sil¨a, C˘at˘alin Mititelu, Yusuke Miyao, Simonetta
Montemagni, Amir More, Laura Moreno Romero,
Shinsuke Mori, Bohdan Moskalevskyi, Kadri
Muischnek, Kaili M¨u¨urisep, Pinkey Nainwani,
Anna Nedoluzhko, Gunta Neˇspore-B¯erzkalne, Lng
Nguyn Th, Huyn Nguyn Th Minh, Vitaly Niko-
laev, Hanna Nurmi, Stina Ojala, Petya Osen-
ova, Robert ¨Ostling, Lilja Øvrelid, Elena Pascual,
Marco Passarotti, Cenel-Augusto Perez, Guy Per-
rier, Slav Petrov, Jussi Piitulainen, Emily Pitler,
Barbara Plank, Martin Popel, Lauma Pretkalnia,
Prokopis Prokopidis, Tiina Puolakainen, Sampo
Pyysalo, Alexandre Rademaker, Loganathan Ra-
masamy, Taraka Rama, Vinit Ravishankar, Livy
Real, Siva Reddy, Georg Rehm, Larissa Rinaldi,
Laura Rituma, Mykhailo Romanenko, Rudolf Rosa,
Davide Rovati, Benoˆıt Sagot, Shadi Saleh, Tanja
Samardˇzi´c, Manuela Sanguinetti, Baiba Saul¯ite, Se-
bastian Schuster, Djam´e Seddah, Wolfgang Seeker,
Mojgan Seraji, Mo Shen, Atsuko Shimada, Dmitry
Sichinava, Natalia Silveira, Maria Simi, Radu
Simionescu, Katalin Simk´o, M´aria ˇSimkov´a, Kiril
Simov, Aaron Smith, Antonio Stella, Milan Straka,
Jana Strnadov´a, Alane Suhr, Umut Sulubacak,
Zsolt Sz´ant´o, Dima Taji, Takaaki Tanaka, Trond
Trosterud, Anna Trukhina, Reut Tsarfaty, Francis
Tyers, Sumire Uematsu, Zdeˇnka Ureˇsov´a, Larraitz
Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van
Niekerk, Gertjan van Noord, Viktor Varga, Eric
Villemonte de la Clergerie, Veronika Vincze, Lars
Wallin, Jonathan North Washington, Mats Wir´en,
Tak-sum Wong, Zhuoran Yu, Zdenˇek ˇZabokrtsk´y,
Amir Zeldes, Daniel Zeman, and Hanzhi Zhu. 2017.
Universal dependencies 2.1.
LINDAT/CLARIN
digital library at the Institute of Formal and Ap-
plied Linguistics ( ´UFAL), Faculty of Mathematics
and Physics, Charles University.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proc. of LREC.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinkova, Dan Flickinger,
Jan Hajic, Angelina Ivanova, and Zdenka Uresova.

2016. Towards comparability of linguistic graph
banks for semantic parsing. In Proc. of LREC.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinkov´a, Dan Flickinger, Jan
Hajiˇc, and Zdeˇnka Ureˇsov´a. 2015. SemEval 2015
task 18: Broad-coverage semantic dependency pars-
ing. In Proc. of SemEval, pages 915–926.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajiˇc, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 task
8: Broad-coverage semantic dependency parsing. In
Proc. of SemEval, pages 63–72.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, 31(1).

Hao Peng, Sam Thomson, and Noah A. Smith. 2017a.
Deep multitask learning for semantic dependency
parsing. In Proc. of ACL, pages 2037–2048.

Hao Peng, Sam Thomson, Swabha Swayamdipta, and
Noah A. Smith. 2018. Learning joint semantic
parsers from disjoint data. In Proc. of NAACL-HLT.

Xiaochang Peng, Chuan Wang, Daniel Gildea, and Ni-
anwen Xue. 2017b. Addressing the data sparsity is-
sue in neural AMR parsing. In Proc. of EACL, pages
366–375.

Barbara Plank. 2016. Keystroke dynamics as signal
for shallow syntactic parsing. In Proc. of COLING,
pages 609–619.

Barbara Plank and Gertjan van Noord. 2011. Effective
measures of domain similarity for parsing. In Proc.
of ACL-HLT, pages 1566–1576.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Re-
search, 15:1929–1958.

Elior Sulem, Omri Abend, and Ari Rappoport. 2015.
Conceptual annotations preserve structure across
translations: A French-English case study. In Proc.
of S2MT, pages 11–22.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008.
The
CoNLL 2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proc. of CoNLL,
pages 159–177.

Swabha Swayamdipta, Miguel Ballesteros, Chris Dyer,
and Noah A. Smith. 2016. Greedy, joint syntactic-
In Proc. of
semantic parsing with stack LSTMs.
CoNLL, pages 187–197.

Swabha Swayamdipta, Sam Thomson, Chris Dyer, and
Noah A. Smith. 2017. Frame-semantic parsing with
softmax-margin segmental rnns and a syntactic scaf-
fold. CoRR, abs/1706.09528.

Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proc. of ACL, pages 589–596.

Chuan Wang, Sameer Pradhan, Xiaoman Pan, Heng Ji,
and Nianwen Xue. 2016. CAMR at SemEval-2016
task 8: An extended transition-based AMR parser.
In Proc. of SemEval, pages 1173–1178.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015a. Boosting transition-based AMR parsing
with reﬁned actions and auxiliary analyzers.
In
Proc. of ACL, pages 857–862.

Carl Pollard and Ivan A Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015b. A transition-based algorithm for AMR pars-
ing. In Proc. of NAACL, pages 366–375.

Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel
Marcu, and Jonathan May. 2015. Parsing English
into Abstract Meaning Representation using syntax-
In Proc. of EMNLP,
based machine translation.
pages 1143–1154.

Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce de-
pendency DAG parsing. In Proc. of COLING, pages
753–760.

Sanjiv Kumar Sashank J. Reddi, Satyen Kale. 2018.
On the convergence of Adam and beyond. ICLR.

Sebastian Schuster and Christopher D. Manning. 2016.
Enhanced English Universal Dependencies: An im-
proved representation for natural language under-
standing tasks. In Proc. of LREC. ELRA.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proc. of ACL, pages 231–235.

Yuan Zhang and David Weiss. 2016.

Stack-
propagation: Improved representation learning for
syntax. In Proc. of ACL, pages 1557–1566.

Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese treebank using a global dis-
In Proc. of IWPT, pages 162–
criminative model.
171.

Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang
Qu, Ran Li, and Yanhui Gu. 2016. AMR pars-
In Proc. of
ing with an incremental joint model.
EMNLP, pages 680–689.

Yftah Ziser and Roi Reichart. 2017. Neural structural
correspondence learning for domain adaptation. In
Proc. of CoNLL, pages 400–410.

