8
1
0
2
 
l
u
J
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
7
6
4
0
.
1
1
7
1
:
v
i
X
r
a

Model Criticism in Latent Space

Sohan Seth∗

Iain Murray†

Christopher K. I. Williams‡§

Abstract

Model criticism is usually carried out by assessing if replicated data generated under the ﬁtted
model looks similar to the observed data, see e.g. Gelman, Carlin, Stern, and Rubin (2004, p. 165). This
paper presents a method for latent variable models by pulling back the data into the space of latent
variables, and carrying out model criticism in that space. Making use of a model’s structure enables
a more direct assessment of the assumptions made in the prior and likelihood. We demonstrate the
method with examples of model criticism in latent space applied to factor analysis, linear dynamical
systems and Gaussian processes.

1 Introduction

Model criticism is the process of assessing the goodness of ﬁt between some data and a statistical
model of that data1. While model criticism uses goodness-of-ﬁt tests to judge aspects of the model, its
general objective is to identify deﬁciencies in the model that can lead to model extension to address these
deﬁciencies. The extended model(s) can again be subjected to criticism, and the process continues until
a satisfactory model is found (O’Hagan, 2003). Model criticism is contrasted with model comparison in
that model criticism assesses a single model, while model comparison deals with at least two models to
decide which model is a better ﬁt. Model comparison can be applied to compare the original and the
extended model after model criticism and extension (O’Hagan, 2003, p. 2).

Bayesian modelling has become an indispensable tool in statistical learning, and it is being widely
used to model complex signals, e.g., by Ratmann et al. (2009). With its growing popularity, there is need
for model criticism in this framework. Most work on model criticism makes use of the idea that “if
the model ﬁts, then replicated data generated under the model should look similar to observed data”
(Gelman et al., 2004, p. 165). In contrast, in this paper we focus on a less well explored idea that for latent
variable models, we can probabilistically pull back the data into the space of the latent variables, and
carry out model criticism in that space. We can summarize this principle as that if the model ﬁts, then
posterior inferences should match the prior assumptions.

To elaborate, consider a model with observed variables X and unobserved variables U with joint
distribution P(X, U | γ) where γ are known parameters. In general U may contain latent variables Z,
parameters Θ, and hyperparameters λ2. Given a sample xobs from the marginal distribution P(X | γ), and
a single posterior sample u∗ from the conditional distribution P(U | xobs, γ), the joint sample (xobs, u∗)
is a draw from the distribution P(X, U | γ). This property can be used to check the ﬁt of the model in
the latent space by checking if u∗ is a sample from the marginal distribution P(U | γ). Testing a single
sample against a distribution, however, is not an effective approach. But, in many widely-used models,

∗School of Informatics, University of Edinburgh, UK; seth@inf.ed.ac.uk
†School of Informatics, University of Edinburgh, UK; i.murray@ed.ac.uk
‡School of Informatics, University of Edinburgh, UK; and the Alan Turing Institute, London, UK; ckiw@inf.ed.ac.uk
§SS and CW gratefully acknowledge the UK Engineering and Physical Sciences Research Council (EP/K03197X/1) for funding
this work. The work of CW is supported by EPSRC grant EP/N510129/1 to the Alan Turing Institute. Code used in the paper is
available at https://github.com/sohanseth/mcls.

1Following O’Hagan (2003, p423) we prefer the term model criticism over model validation and model checking, as if “all models are
strictly wrong” it is impossible to validate a model, and model criticism has a more active tone of looking to discover problems,
compared to model checking, which may seem a more passive activity that does not expect to uncover any problems.

2For example, in the context of the Bayesian matrix factorization (Salakhutdinov and Mnih, 2008), X is the observed data matrix,
U = {Z, Θ, λ} is the matrix of latent factors Z, the loading matrix Θ, precision hyperparameters λ, and γ denotes the parameters
of the hyperpriors.

1

Figure 1: a) A probabilistic model with observed variables X, unobserved variables U, and known
parameters γ, b) Given the observed data xobs from P(X | γ) and a posterior sample u∗ from P(U | xobs, γ),
(xobs, u∗) is a joint sample from P(X, U | γ), and therefore, u∗ is a sample from P(U | γ) and xobs is
a sample from P(X | u∗, γ). c) If the prior (or part of it) factorizes into identical distributions, e.g.,
P(U | γ) = ∏2
2} is independent and identical sample from
Pu(· | γ). d) A factor analysis model showing observed variables X = {xi}n
i=1, unobserved variables
U = {{zi}n
12, . . .} is a sample from
P(Z | τz), and {θ∗

i=1, Θ, τ, τθ}, and known parameters γ = {α, β, τz}. We test if {z∗

k=1 Pu(Uk | γ), then posterior sample {u∗

12, . . .} is a sample from P(θ | τ∗
θ ).

11, θ∗

11, z∗

1, u∗

groups of unknown variables are independently and identically distributed under the prior. These
related variables are easily aggregated together, giving a simple test of the prior assumptions. Figure 1
summarizes the overall approach, which is justiﬁed in §3.

In comparison to model criticism in the observation space, comparing u∗ with prior P(U | γ), provides
an additional tool for model criticism which does not require crafting an appropriate discrepancy
measure, generating replicate observations, and approximating the null distribution. This approach also
does not suffer from the “double use” of data (see discussion in §2). These points have also been made
by Yuan and Johnson (2012), but were applied to a relatively small scale hierarchical linear model. We
develop the use of model criticism in latent space for large scale and complex models, yielding new
insights and developments. Speciﬁcally, we apply this approach to the criticism of linear dynamical
systems, factor analysis and Gaussian processes, and discuss its connection to the observation space
based approach.

The structure of the rest of the paper is as follows: in §2 we describe the methods of model criticism in
observation space. §3 provides details of the argument for model criticism in latent space and describes
related work, and §4 shows results from applying the method to the three examples. Table 1 describes
the notations followed in the paper, and Table 2 shows the distributions used in the paper.

Style

Explanation

Upper case italics
Lower case italics
Lower case bold
Upper case bold
P(X)
P(X | y)
p(x)
p(x | y)
· ∼ ·
·∗
·obs
·rep
0
I

Random variable or a group of random variables
Realization of a random variable
Vectors, realization or random variable
Matrices, realization or random variable
Distribution of random variable X
Conditional distribution of random variable X given Y = y
Probability density function of r. v. X, abbreviation for pX (x)
Conditional density function of r. v. X given Y = y, abbreviation for pX | Y (x | y)
Distributed as
A posterior sample
Observed data
Replicate data
Zero vector
Identity matrix

Example

i=1, u∗
i=1, P(z), u = (u1, . . . , uK )(cid:62)
i=1, P(Z), U = [u1, . . . , uK]

X, Z, U = {U1, . . . , UK }
{xi}n
{xi}n
{Xi}n
X ∼ P(X)
X ∼ P(X | y)
p(y) = (cid:82) p(x, y)dx
p(y) = (cid:82) p(x | y)p(y)dx
Xrep ∼ P(X), X ∼ N (0, 1)
u∗, z∗
xobs, Xobs ∼ P(X)
xrep, Xrep ∼ P(X)
x ∼ N (0, I)
x ∼ N (0, I)

Table 1: Description of notation used in the paper.

2

Name

Normal

Notation

N (x | µ, τ

−1)

Multivariate normal N (x | µ, τ

−1)

Double exponential

L(x | µ, τ)

Gamma

Dirichlet

Gamma(x | α, β)

Dir(x | α)

τ1/2

Density
(2π)1/2 exp (cid:0)−(τ(x − µ)2)/2(cid:1)
det(τ)1/2
(2π)d/2 exp (cid:0)−((x − µ)(cid:62)τ(x − µ))/2(cid:1)
τ
2 exp(−τ|x − µ|)
βα
Γ(α) xα−1 exp(−βx)
i=1xαi −1

where xi ∈ (0, 1) and ∑K

∏K

1
B(α)

i

i=1 xi = 1.

Table 2: List of distributions used in the paper.

2 Model Criticism in Observation Space

A general approach of model criticism is to evaluate if replicated data generated under the (ﬁtted)
model looks similar to observed data. Consider that we are modelling observed data xobs with a latent
variable model parameterized by U, i.e., we have deﬁned the likelihood p(x | u) and (optionally) a prior
distribution P(U) over potential parameter values. The principle of model criticism in the observation
space is to assess if xobs is a reasonable observation under the proposed model. For example, given the
maximum likelihood estimator (or another point estimate) ˆu of the parameters, one standard approach is to
ﬁnd the plug-in p-value (Bayarri and Berger, 2000)

pplug-in = Pr(D(Xrep, ˆu) > D(xobs, ˆu)).

Here D is called a discrepancy function and it resembles a test statistic in hypothesis testing, i.e., a larger
value rejects the null hypothesis or indicates incompatibility of data and model, and Xrep is a replicate
observation generated under the ﬁtted model, i.e., Xrep ∼ P(X | ˆu).

If the p-value is low, then it implies that the probability of generating a more extreme dataset than the
observed data is small, or in other words, the observed data itself is considered extreme relative to the
model, and thus, the model does not adequately describe the dataset. In summary, a low p-value rejects
the hypothesis that the data is being adequately modelled. The p-value is usually estimated via an empirical
average by generating multiple replicates xrep

, r = 1, . . . , R, and evaluating

ˆpplug-in =

1(D(xrep

r

, ˆu) > D(xobs, ˆu)),

r

1
R

∑
r

(1)

(2)

where xrep

r

is a sample from P(X | ˆu).

An alternative to point estimation is to consider a Bayesian treatment of the problem where one
can integrate out the contribution of the parameters. The test statistic can be averaged under either the
prior distribution or the posterior distribution. The prior-predictive distribution is deﬁned to have the
density p(xrep | γ) = (cid:82) p(xrep | u) p(u | γ) du where γ parameterizes the prior distribution over U. One
can generate replicate observations from this distribution, and compute the prior predictive p-value (Box,
1980)

pprior = Pr(D(Xrep, U) > D(xobs, U)) ≈

1(D(xrep

r

, ur) > D(xobs, ur)) = ˆpprior,

(3)

where (xrep, u)r is a sample from P(X, U | γ). This approach is not reasonable when the prior distribution
is improper (cannot be integrated) or uninformative. Additionally, even if the prior distribution is
informative, one might not generate enough samples to represent the data distribution well when the
parameter space is large. However, notice that one does not need to ﬁt the model to criticise it.

On the other hand, one can use the posterior distribution P(U | xobs), and sample from the posterior-
predictive distribution with density p(xrep | xobs) = (cid:82) p(xrep | u) p(u | xobs) du. The posterior predictive
p-value (Rubin, 1984) is then computed as:

ppost = Pr(D(Xrep, U) > D(xobs, U) | xobs) ≈

1(D(xrep

r

, ur) > D(xobs, ur)) = ˆppost,

(4)

1
R

∑
r

1
R

∑
r

3

where (xrep, u)r is a sample from P(Xrep, U | xobs), i.e., by generating samples ur from the posterior
distribution instead3. The support of the posterior is usually more concentrated than prior, and the
posterior distribution may be well-deﬁned even if the prior distribution is improper.

The posterior predictive p-value has been criticised for “double use” of data, once for computing
the posterior distribution P(Xrep | xobs) and once for computing the discrepancy measure D(xobs, U)
(Bayarri and Berger, 2000). This means that ppost does not have a uniform distribution under the null
hypothesis, whereas pprior is a valid p-value. pplug-in is subject to the same criticism as ppost since the
MLE uses the observed data as well (Bayarri and Berger, 2000). Lloyd and Ghahramani (2015, §7) view
the different p-values as arising from “different null hypotheses and interpretations of the word ‘model’ ”.
They argued that the posterior predictive and plug-in p-values are most useful for highly ﬂexible models,
as the aim is to assess the ﬁtted model rather than the whole space of models. Lloyd and Ghahramani
(2015) also point out that “it may be more appropriate to hold out data and attempt to falsify the null
hypothesis that future data will be generated by the plug-in or posterior distribution”, which is also in
line with the discussion in (O’Hagan, 2003, §2.1). Further examples of posterior predictive checking can
be found in (Belin and Rubin, 1995; Gopalan et al., 2015).

In all of the model criticism described above, a key quantity is the discrepancy function D used to
compare the data and predictive simulations. We agree with Belin and Rubin (1995, p. 753) who wrote of
the importance of identifying discrepancy functions “that would not automatically be well ﬁt by the
assumed model”, and that “there is no unique method of Bayesian model monitoring, as there are an
unlimited number of non-sufﬁcient statistics that could be studied”.

Lloyd and Ghahramani (2015) suggest the Maximum Mean Discrepancy (MMD) as a measure of
discrepancy between the observed data and replicates. The motivation of using this approach is to
maximize the discrepancy over a class of discrepancy functions rather than choosing only one, i.e.,

MMD = sup
f ∈F

(E

Xobs f (Xobs) − E

Xrep f (Xrep))

where F is a set of functions. The function that maximizes the discrepancy is known as the witness
function. When F is a reproducing kernel Hilbert space (RKHS) the witness function can be derived in
closed form as

ˆf (·) =

1
|xobs|

|xobs|
∑
i=1

κ(·, xobs

) −

i

1
|xrep|

|xrep|
∑
j=1

κ(·, xrep

),

j

(5)

(6)

where κ is the kernel of the RKHS. This estimation does not work well in high dimensions, and therefore,
the authors suggests reducing the dimensionality of the observation space before applying this statistic
(Lloyd and Ghahramani, 2015, p. 4).

3 Model Criticism in Latent Space

Recall we have a model P(X, U | γ), with observed variables X, unobserved variables U, and known
parameters γ. In general U may contain latent variables Z, parameters Θ, and hyperparameters λ. Our
procedure depends on the following two key observations:

1. If xobs is drawn from the above model, then a sample u∗ from P(U | xobs, γ) is a sample from the
prior distribution P(U | γ). To see why this is true, observe that a natural way to sample from
the joint P(U, X | γ) is to generate a sample u from P(U | γ), and then generate a sample x from
P(X | u, γ) in that order. However, it is also valid to draw samples from the joint by ﬁrst sampling
x from P(X | γ) and then sampling u from P(U | x, γ). Thus we have

Statement 1. If xobs is a sample from P(X | γ), then a sample u∗ from P(U | xobs, γ) will be a draw from
P(U | γ).

3Note that the p-value ppost(u) = P(D(Xrep, u) > (xobs, u)) might be available in closed form depending on the choice of D

(Gelman et al., 1996, Eq. (8–9)). Then ppost = 1
R

∑r ppost(ur) where ur are posterior samples.

4

It is important to clarify what Statement 1 is not saying. It is not saying that repeated draws from
P(U | xobs, γ) will explore the full prior distribution P(U | γ), but only that it is a valid way to draw
one sample from it if xobs is a draw from the model4. However, testing how well a single draw
from a given distribution ﬁts that distribution is difﬁcult. This brings us to our second observation.

1, . . . , u∗

1, . . . , u∗

2. If U is a collection of variables, i.e., U = (U1, . . . , UK), and the prior distribution of U decomposes
into independent draws from the same distribution, e.g., P(U | γ) = ∏K
k=1 Pu(Uk | γ) then it is
possible to aggregate these variables together, i.e., instead of testing if (u∗
K) is a sample from
P(U | γ), one can test if {u∗
K} is independent and identical draws from the distribution
Pu(· | γ).
In other words, rather than testing one sample against a known high dimensional
distribution, one can test if the collection of K samples are independent and identical draws from a
known lower-dimensional distribution Pu. Thus, we deﬁne aggregation as pooling variables with the
same prior distribution together, and an aggregated posterior sample (APS) is deﬁned as a set of posterior
samples that have been aggregated for comparison with a speciﬁc reference distribution. The above
can be generalized to the situation where U = (U1, . . . , UK, θ) is a collection of variables and
k=1 Pu(Uk | θ)P(θ | γ). Then {u∗
parameters such that P(U | γ) = ∏K
K} can be aggregated and
tested against Pu(· | θ∗)5. Aggregation can be also extended to the case where U consists of groups
1 , . . . , Ug
of variables (U1, . . . , UG) where aggregation is performed within each group Ug = (Ug
)
Kg
by pooling {ug∗
} and comparing against pug (· | u−g∗) where U−g denotes all groups except
g. We provide more concrete examples of aggregation in §3.1 and Table 3.

1 , . . . , ug∗

1, . . . , u∗

Kg

We refer to this approach as aggregated posterior checking (APC). We summarize this approach in
Algorithm 1. Ideas equivalent to Statement 1 and the aggregation of posterior samples can also be found
in Yuan and Johnson (2012) 6, but were applied to the case where U contains only model parameters,
and for hierarchical linear models. See §3.2 for more details on related work.

Algorithm 1 Aggregated posterior check
Require: Observed data xobs
Require: Bayesian model P(X | U, γ)P(U | γ) with latent variables U
1: Generate a posterior sample u∗ from P(U | xobs, γ)
(cid:46) See Table 3
2: Generate aggregated posterior sample(s)
3: Compare aggregated posterior sample(s) with corresponding reference distribution(s) with appropri-

ate test

4: return p-value of the test(s)

So far, we have addressed the idea of assessing deviations from the prior distribution and aggregation
in the latent space. However, the same idea can be applied to the observation space as well, i.e., to
the likelihood by testing if xobs is a sample from P(X | u∗, γ). Although it is true that a discrepancy in
the choice of likelihood should be reﬂected in the posterior sample, assessing the discrepancy in the
likelihood directly provides better understanding and easier resolution of the discrepancy. Notice that,
although we make use of xobs, our approach is not equivalent to model criticism in the observation space
since we do not compare the observed data xobs with replicate observations xrep
, but only investigate
the relation between the latent space u∗ and observation space xobs. Both methods, however, require
generating posterior samples ur (for model criticism in the latent space we use r = 1).

r

3.1 Application to different models

We discuss below the application of model criticism in latent space to factor analysis, linear dynamical
systems and Gaussian process regression. These situations are then demonstrated on real data in §4.

4Throughout this paper, we assume that the prior distribution is proper, so the respective posterior distribution is well-deﬁned,

and that any MCMC sampler has converged, i.e., the posterior sample is well-behaved.

5Alternatively, U and θ can be combined to deﬁne a pivotal quantity s whose distribution does not depend on θ (Yuan and

Johnson, 2012), and s(u∗, θ∗) can be tested against that distribution.

6We had independently derived the key results. We thank an anonymous referee for pointing out the work of Yuan and Johnson

(2012).

5

p(zi) p(xi | zi, Θ, τ).

n
∏
i=1
−1I) and θ | λ ∼ N (0, τθ

(7)

Factor analysis model Consider a factor analysis model with hyperparameters λ = {τθ, τ}, parameters
(loading matrix) Θ, latent variables (factors) z and data x. Grouping Z = {zi}n
i=1 and similarly for X7,

p(λ, Θ, Z, X) = p(λ) p(Θ | λ) p(Z | λ) p(X | Z, Θ, λ) = p(τθ) p(τ)p(Θ | τθ)

−1I)
Figure 1d illustrates this model. In Gaussian factor analysis, z ∼ N (0, τz
(See Table 2). There is an identiﬁability issue in the factor analysis model between Θ and z, which is
resolved by ﬁxing the scale of one of the two. In Eq. (7) the dependence of z on λ is taken to be null, i.e.,
−1I) and
τz = 1. (In the case of example §4.1 we ﬁx the scale of Θ instead.) Also, p(x | z, Θ, τ) = N (Θz, τ
τ, τθ | α, β ∼ Gamma(α, β). Thus the ﬁxed parameters γ = {τz, α, β}.

If Xobs is drawn from the above model, then a sample λ∗, Θ∗, Z∗ from P(λ, Θ, Z | Xobs) is a sam-
ple from the prior P(λ)P(Θ | λ) P(Z | λ). In factor analysis, Z decomposes into independent draws
from P(z | τz), and therefore, one can pool the posterior samples z∗
i to assess deviations from P(z | τz).
Moreover, each zi usually decomposes into independent draws over the different latent dimensions as
∏k p(zik | τz), one can pool the z∗
ik to assess deviations from p(z | τz). Similarly, if the prior over the factor
loadings matrix Θ decomposes as p(Θ | τθ) = ∏kj p(θkj | τθ) then one can pool the θ∗
kjs, and compare
with p(θ | τ∗
θ ). One can also go beyond the marginal z or the full vector z, and assess a subset of the
vector such as bivariate interactions (see §4.1).

Linear dynamical system One can extend the idea of aggregation beyond factor analysis models. For
example, Statement 1 holds for general latent variable models with repeated structure. Take, for example,
a linear dynamical system model with a latent Markov chain, so that

p(X, Z | U) = p(z1)p(x1 | z1, U)

p(zt | zt−1, U)p(xt | zt, U)

(8)

T
∏
t=2

where U consists of the system and observation matrices A, B, and precisions, Q, R. Then according to
Statement 1 a sample (Z∗, u∗) drawn from P(Z, U | Xobs) should be distributed according to the prior
over (Z, U). Although the zt’s are not independent (due to the Markov chain), we can consider model
criticism for p(zt | zt−1). For example for a system model parameterized as zt | zt−1 ∼ Azt−1 + (cid:101)t with
(cid:101)t ∼ N (0, Q−1), violations of the model will show up as deviations of the (cid:101)∗
t ’s from N (0, Q∗ −1) (see §4.2).
Similarly, for an observation model parameterized as xt | zt ∼ Bzt + ψt with ψt ∼ N (0, R−1), violations
of the model may also show up as deviations of the ψ∗

t ’s from N (0, R∗ −1). See §4.2.

Gaussian process regression A Gaussian process probabilistic model is deﬁned as:

ϑ, ζ, τ ∼ p(ϑ) p(ζ) p(τ),
f (x) ∼ GP (m(x | ϑ), κ(x, x(cid:48) | ζ)),

yi ∼ N ( f (xi), τ

−1) ∀i = 1, . . . , n,

where m(x | ϑ) is the mean function parameterized by ϑ, κ(x, x(cid:48) | ζ) is the covariance function (or ker-
nel) parameterized by ζ, and τ is the observation noise precision, see e.g., Rasmussen and Williams (2006).
Given observations {(xi, yi)}n
−1δ(xi, xj). Alternatively, considering the eigendecomposition K = UΛU(cid:62)
and Kij = κ(xi, xj | ζ) + τ
where U = [u1, . . . , un] is the matrix of the eigenvectors uis and Λ is the diagonal matrix of the corre-
sponding eigenvalues, i.e., Λ

i=1, y ∼ N (m, K), where y = (y(x1), . . . , y(xn))(cid:62), m = (m(x1 | ϑ), . . . , m(xn | ϑ))(cid:62)

ii = λi,

c = U(cid:62)(y − m) ∼ N (0, Λ).

This implies that, according to the model, the projections ci of the signal y on the eigenvector ui are
independent samples from N (0, λi). Thus, the normalized projections

7We have omitted the ﬁxed parameters γ for simplicity.

z = Λ−1/2U(cid:62)(y − m) ∼ N (0, I)

6

(9a)

(9b)

(9c)

(10)

(11)

should be independent samples from N (0, 1) distribution. One can thus test the normality of the z’s
to assess the goodness of ﬁt. However, note that if the ith eigenvalue of K is much smaller than the
−1, then this zi is dominated by the white noise contribution. Thus we only include z’s
noise variance τ
corresponding to eigenvalues with λi > 2τ

−1 to assess the ﬁt of the GP model8. See §4.3.

3.2 Related Work

Cook et al. (2006) consider the situation with (in our notation) a prior p(u) on the parameters of the
model, and data p(x | u). They then assume that speciﬁc parameters u0 are drawn from the prior, then
data xobs drawn from P(X | u0). They then consider samples u1, u2, . . . , uL drawn from P(U | xobs), and
comment (in the caption of their Figure 1, translating to our notation) that “(xobs, u(cid:96)) should look like
a draw from P(X, U) for (cid:96) = 0, 1, . . . , L”. They then use the ‘reverse’ of Statement 1 to validate the
correctness of posterior samples generated by a statistical software, by comparing u0 with u1, u2, . . . , uL.
Their recommended method for this is to calculate posterior quantiles for each scalar parameter; if the
software is working correctly then the posterior quantiles are uniformly distributed. Although they
share with us the observation that (xobs, u(cid:96)) should look like a draw from P(X, U), this is used to answer
a totally different question. Also, they do not discuss the inclusion of latent variables in the model.

Johnson (2007) and later Yuan and Johnson (2012) also consider a model with parameters U and
data drawn from P(X | U). Their interest is in the use of pivotal quantity d(x, u) that has a known
and invariant sampling distribution when data xobs are generated from a model with data-generating
parameters u0. Then Yuan and Johnson (2012) show that if the d(X, u0) is a pivotal quantity distributed
according to F, then d(X, u(cid:96)) is also distributed according to F, if u(cid:96) is drawn from the posterior on
U given xobs. The result of Yuan and Johnson (2012) extends earlier work by Johnson (2007) to the
case where d(x, u) does not depend on the data x—for example this situation can arise in a Bayesian
hierarchical linear regression model, when considering the second level where parameters for individual
units are generated from a hyperprior.

Regression diagnostics is a well-explored example of model criticism. Existing approaches assess
certain statistical assumptions made during modelling, e.g., if the residuals follow a normal distribution
with zero mean, (e.g., using a Q-Q plot (Wilk and Gnanadesikan, 1968)), if the residuals are homoscedas-
tic, (e.g., using the Breusch–Pagan (Breusch and Pagan, 1979) or White test (White, 1980)) or if the
successive residual terms are uncorrelated (e.g., using the Durbin–Watson test (Durbin and Watson,
1950)). Regression diagnostics can be seen as a special case of model criticism in the latent space since
residuals are representatives of errors, which are latent variables of the model. However, our methods
are also applicable to more complex models.

Meulders et al. (1998) consider a factor analysis model for binary data, using (in our notation)
Beta(2, 2) priors on Z and Θ. They carry out posterior sampling using block Gibbs sampling for Z and
Θ and compare histograms of these variables against the prior. Discrepancies between the prior and
histograms of the sampled aggregated posterior led to model extension, expanding the model to use a
mixture of two beta distributions for the parameters. However, the authors do not explain the basis for
carrying out this check (cf. Statement 1).

Buccigrossi and Simoncelli (1999) consider the posterior distribution of wavelet coefﬁcients (analo-
gous to z in the factor analysis model) in response to image patches. By considering the distribution of
a bivariate aggregated posterior, they show that this is not equal to the product of the marginals, but
exhibits variance correlations. (This is shown by introducing a “bowtie plot” showing the conditional
histogram of z2 given z1.) This work is a nice example of how the failure of a diagnostic test can give rise
to an extended model (see §4.1).

O’Hagan (2003, §3) considered model criticism tools that can be applied at each node of a graphical
model (and of course latent variables can be considered as such). O’Hagan (§3.1 2003) discussed the idea
of residual testing at different levels of a hierarchical model as well as a generic probabilistic model. He
suggested checking if a node in a probabilistic model is misbehaving by comparing the posterior samples
at that node to prior distribution. O’Hagan (§3.2 2003) also emphasized that conﬂict can arise between
the different sources of information about a variable at a particular node, arising from contributions from
each neighbouring node in the graph. However, he did not suggest using the aggregated posterior to
assess goodness of ﬁt, but considered the posterior at each node separately.

8The factor of 2 on the RHS of the inequality is included because the λi’s are shifted by τ

−1 by deﬁnition.

7

Tang et al. (2012) introduce the concept of the “aggregated posterior” as applied to deep mixtures of
factor analysers (MFA) model. Consider the situation as above but where Θ is estimated by maximum
likelihood, so it is the posterior over Z that is of interest. Thus p(X, Z | Θ) = ∏n
i=1 p(zi) p(xi | zi, Θ).
Under this model we also have that p(z) = (cid:82) pΘ(z | x) pΘ(x)dx where the Θ subscript denotes that
both pΘ(z | x) and pΘ(x) correspond to distributions under the model. Tang et al. (2012, p3) deﬁne the
aggregated posterior as “the empirical average over the data of the posteriors over the factors”, i.e.,

˜p(z) =

pΘ(z | xobs

),

i

1
n

n
∑
i=1

(12)

where the integral wrt pΘ(x) has been replaced by the empirical average over samples. If the data
distribution p(x) is equal to the model distribution pΘ(x) then ˜p(z) should agree with p(z). However,
differences between p(x) and pΘ(x) will manifest as differences between the two respective distributions
in the latent space.

) for i = 1, . . . , n to p(z). This is a valid approach since if {xobs

In practice, however, one does not explicitly construct the aggregated posterior Eq. (12) since it is
only asymptotically equal to the prior. Instead Tang et al. (2012) compare a collection of n samples z∗
i
from pΘ(z | xobs
n } follow the
i
distribution pΘ(x), then {z∗
n} follow the distribution p(z) as we show in Statement 1. Additionally,
as Θ is not known in practice, Tang et al. (2012) replace Θ with maximum likelihood estimate ˆΘ in the
deﬁnition of aggregated posterior Eq. (12). In Statement 1 we extend this idea to a Bayesian setting where
Θ and λ are not ﬁxed parameters but latent variables themselves.

1, . . . , z∗

, . . . , xobs

1

Tang et al. (2012) started with a simple mixture of factor analysers (MFA), and observed that the
aggregated posterior for a latent component often doesn’t match the N (0, I) prior. By replacing the
prior for a component with another MFA model, they constructed a deep MFA model. The idea of
the aggregated posterior (although not the name) can be traced back e.g. to Hinton et al. (2006), where
in deep belief nets the idea was that the posterior distribution of the latents of a restricted Boltzmann
machine (RBM) could be modelled by another RBM.

4 Examples

In this section, we provide three examples of model criticism and extension in the latent space. First, we
explore a factor analysis model in the context of image compression (§4.1). The objective of this example
is to show how the model can be criticised in the latent space as well as in the observation space. Our
analysis leads to changing the latent distribution from a single Gaussian to a scale mixture of Gaussians,
which captures both the marginal and the joint structure of the latent space, and improves the model in
the observation space as well.

Next, we explore a linear dynamical system model (§4.2) in the context of modelling time series. We
show that model criticism in latent space allows us to interrogate not only the standard “innovations”
(deﬁned in (20)), but also the latent residuals (deﬁned in (19)).

Finally, we explore a Gaussian process model (§4.3) in the context of modelling time series. The
objective of this example is to show when model criticism in the latent space can be a natural choice
whereas model criticism in the observation space can be difﬁcult. Our analysis leads to changing the
covariance function from squared exponential to a combination of periodic and squared exponential
kernels.

We implemented all models (except the Gaussian process model) in JAGS (Plummer, 2003), keeping a
single sample in the MCMC run after discarding a burn-in of 1000 samples (10,000 samples for §4.2). Note
that for model criticism in the latent space, we need only a single sample. We summarize the aggregation
process and corresponding reference distributions used in this section in Table 3.

4.1

Image patch data

The Berkeley Segmentation Database (Martin et al., 2001) consists of 200 training images. Following
Zoran and Weiss (2012), we convert the images to greyscale, extract 8 × 8 randomly located patches,
and remove DC components from all image patches9. We extract 50,000 image patches, and ﬁt different

9https://people.csail.mit.edu/danielzoran/NIPSGMM.zip

8

Model

xobs

U

APS

reference distribution

MF (13)

{xobs

i }n

i=1

{zi}n
{zi}n

i=1, Θ, b, τ, τz for (14)-(15)
i=1, Θ, b, τ, π, τ for (16)

{z∗
k1 i, z∗

ki}n,K
k2 i)}n,K

{(z∗

i=1,k=1, and

i=1, k1 ,k2 =1,k1 (cid:54)=k2

LDS
(17), (18)

{(x, y,

cos(ν), sin(ν))obs

t }n

t=1

t=1, {zt}n

t=1, A(1), . . . ,
{st}n
A(S), Q(1), . . . , Q(S), B, R

GP (9)

{(xi, yi)obs}n

i=1

{z∗

i }n

i=1 from (21)

σ2
f , l, τ for (22)
σ2
f , p, lp, ld for (22) and (23)
σ2
f , f , lp, ld, σ2

f s, ls, σ2

f l , ll

for (22) (large and small) and (23)

t=2 from (19) and
t=2 from (20) ∀ j, k

{ ˜z∗
{ ˜x∗
kt, ˜z∗
kt, ˜x∗

kt}n
jt}n
k(t+1))}n−1
k(t+1))}n−1

{( ˜z∗
{( ˜x∗

t=2 from (19) and
t=2 from (20) ∀ j, k

N (0, τ∗
z

−1), and
−1I2) for (14)
N (0, τ∗
z
L(0, τ∗
z ), and
z ) L(z2; 0, τ∗
mN (0, τ∗
m

L(z1; 0, τ∗
∑m π∗

z ) for (15)

−1), and
−1I2) for (16)

∑m π∗

mN (0, τ∗
m
N (0, 1)
N (0, 1)
N (0, I2)
N (0, I2)

N (0, 1)

Table 3: The table summarizes observed data xobs, unknown variables U, aggregated posterior sample(s)
(APS), and corresponding reference distribution(s) (as elaborated in Algorithm 1) for three models
discussed in §4, and different scenarios within each model.

matrix factorization models of the form:

b ∼ N (0, I), τ ∼ Gamma(α, β), θjk ∼ N (0, 1)
zi ∼ LatentDist, xi ∼ N (Θzi + b, τ

−1I)

(13a)

(13b)

with K = 16 latent dimensions (> 82% explained variance in PCA). Previously Zoran and Weiss (2012)
used full covariance zero-mean Gaussians (τ = 0, K = 64, b = 0). We set α = β = 0.001.

We start by assuming a Gaussian distribution for the latent model

τz ∼ Gamma(α, β), z ∼ N (0, τz

−1),

(14, Gaussian)

and generate a sample (Z∗, Θ∗, b∗, τ∗, τ∗
z ) from the posterior. To criticise the model, we aggregate the
, z∗
ki} ∀k, i, and bivariate samples {(z∗
univariate posterior samples {z∗
)} ∀k, i1 (cid:54)= i2. If the observed
ki2
ki1
−1)
data follows the model, then the distributions of the corresponding APSs are univariate normal N (0, τ∗
z
−1I2) respectively. We observe that neither of the APSs follow the expected
and bivariate normal N (0, τ∗
z
prior distribution. Also, the marginal distribution is more concentrated around zero than the expected
distribution, whereas the joint distribution shows heteroscedasticity (Figure 2, left column) which is
inconsistent with the factorized bivariate normal prior.

An alternative latent variable model for the factor analysis model is (See Table 2)

z ∼ L(0, τz).

(15, Laplace)

We use the same aggregation strategy as before, and compare the empirical distributions with the uni-
variate distribution L(0, τ∗
z ) respectively. We observe
similar characteristics in the aggregated posterior as before (Figure 2, middle column).

z ) and bivariate distribution L(z1; 0, τ∗

z ) L(z2; 0, τ∗

To accommodate these observations, we allow a scale mixture of Gaussian distributions as used by

Wainwright and Simoncelli (2000) with 8 components for the latent variable

π ∼ Dir(1), τm ∼ Gamma(α, β), z ∼

πmN (0, τm

−1I).

(16, Scale Mixture of Gaussians)

1 , . . . , π∗

We generate sample (π∗
8 , τ∗
8 ) as well. We assess the same aggregated distributions as
−1I2) respectively. We observe that
before and compare them with ∑m π∗
the empirical marginal distribution follows the mixture distribution well, although a KS test rejects
the hypothesis that the aggregated posterior follows the mixture distribution. Additionally, the joint
distribution captures the heteroscedasticity in the latent space (Figure 2, right column).

−1), and ∑m π∗

mN (0, τ∗
m

mN (0, τ∗
m

1 , . . . , τ∗

We also show the eigenvectors of the corresponding loading matrix for each of the three cases (Figure
2 bottom row). We show the eigenvectors rather than the loading matrix themselves since for the

8
∑
m=1

9

(a) Gaussian (14)

(b) Laplace (15)

(c) Scale Mixture of Gaussians (16)

Figure 2: Aggregated posterior [agg. post] and associated prior at the latent node for (left to right) the
Gaussian, Laplace and Scale mixture of Gaussian models of image patches. Top: ECDF of aggregated
posterior samples (16 × 50, 000 samples) and CDF of respective prior. Middle: conditional mean and
standard deviation of the bivariate aggregated posterior samples (16 × 15 × 50, 000 ÷ 2 samples, over
100 bins) and respective prior distribution. Bottom: eigenvectors of respective loading matrices.

Gaussian and Gaussian scale mixture, the columns of the loading matrix may not correspond to any
particular pattern due to rotational invariance. We observe that all three loading matrices span a similar
space.

The matrix factorization model can be criticised in the observation space with established image
statistics as a discrepancy measure. This, however, requires generating replicate data of the same size
as the observed data, which in this case is computationally extensive since Xobs ∈ R64×50,000. To avoid
generating multiple replicates, i.e., matrices Xrep
r ∈ R64×50,000, we only generate a single replicate for
each latent distribution choice and compare them the observed data.

For all three cases, i.e., Gaussian, Laplace, and Scale Mixture of Gaussians, we generate latent samples
zrep
z (and τ, π∗ for Scale Mixture of Gaussians). We use the rest of the ﬁtted
from the ﬁtted parameters τ∗
i
parameters, i.e., Θ∗, τ∗, and b∗ to generate samples xrep
. We generate 50,000, 8 × 8 replicate
image patches, and compare the observed and replicate data in terms of the distribution of raw pixel
values. We show the results in Figure 3. We observe that the distribution of the image pixel values in
the replicate data follows the observed data more closely for Scale Mixture of Gaussians than the other
latent distributions. However, it is not a perfect ﬁt, and that tells us that this model can improved further;
potentially by increasing K, and varying the noise characteristics such as using a full diagonal covariance.

from zrep

i

i

10

Figure 3: Empirical cumulative distribution functions of raw pixel values on observed and replicate
data for varying different distributions.

4.2 Honey bee data

The honey bee data consists of measurements of (x, y) coordinate and head angle (ν) of 6 honey bees. The
measurements are usually translated into a 4-dimensional multivariate time series (x, y, cos(ν), sin(ν)),
and modelled using a switching linear dynamical system to capture three distinct dynamical regimes,
namely, left turn, right turn and waggle (Oh et al., 2008). We follow this strategy, and model each time
series by a switching linear dynamical system (SLDS) (Fox et al., 2009) as follows:

s1 = 1, z1 ∼ N (0, I)
st ∼ Cat(π(st−1))
zt ∼ A(st)zt−1 + (cid:101)t, (cid:101)t ∼ N (0, Q(st) −1)
xt ∼ Bzt + ψt, ψt ∼ N (0, R−1)

∀t = 2, . . . , n

∀t = 2, . . . , n

∀t = 1, . . . , n

where st can be in one of {1, . . . , S} states. We assume that Q(·) (for each state) and R are diagonal
matrices with Gamma(α, β) prior over nonzero entries, entries of A(·) (for each state) and B originate
from Gaussian distribution, and π(·) (for each state) follow a Dirichlet distribution, i.e.,

−1), b·· ∼ N (0, τB

(·)
·· ∼ N (0, τA
a
τA ∼ Gamma(α, β), τB ∼ Gamma(α, β)
π(·) ∼ Dir(1).

−1)

We group s = {si}n

i=1 and Z = {zi}n

i=1. We set α = β = 0.001.

We ﬁt two models with S = 1 (standard linear dynamical system), and S = 3, both with a 4 dimen-
sional latent space. We generate a posterior sample (s∗, Z∗, A(1)∗, . . . , A(S)∗, Q(1)∗, . . . , Q(S)∗, B∗, R∗), and
aggregate the standardized latent residuals

˜zt = (Q(s∗

t )∗)0.5(z∗

t − A(s∗

t )∗z∗

t−1) ∀ t = 2, . . . , n,

and observation residuals (or innovations)

˜xt = (R∗)0.5(xobs

t − B∗z∗

t ) ∀ t = 2, . . . , n.

11

(17a)

(17b)

(17c)

(17d)

(18a)

(18b)

(18c)

(19)

(20)

Figure 4: Left: ECDF of aggregated posterior samples [agg. post.] (4 × n samples) for S = 1 and S = 3,
and CDF of prior N (0, 1) at the latent (top) and observation node (bottom). Right: segmentation of
honey bee sequence 6 as observed in s∗
n. Black markers indicate true change points. p-values
correspond to KS test.

1, . . . , s∗

For an LDS the standard approach to model criticism is to check that the innovations sequence is
zero-mean and white (see, e.g. (Candy, 1986, §5.1)), although this is usually carried out for known or
point-estimates of the parameters, not in a Bayesian setting. We use this check (extended to the SLDS
case) below, but also consider the latent residuals.

First, we focus on marginal structures ˜zkt and ˜xjt by pooling k = 1, 2, 3, 4 and j = 1, 2, 3, 4 together,
rather than the 4-dimensional vectors themselves as shown in Figure 4 (left). We expect that the APSs
would deviate from normality more (lower p-value) when S = 1, compared to S = 3, and we observe
this to be true for all honey bee sequences except 2. For sequences 4–6, the latent segmentations of the
SLDS in terms of (s∗
n) agree with the ground truth well; we present the 6th sequence in Figure 4
(right). For sequences 1–3, we observe that the segmentations are rather poor, similar to the results in
(Fox et al., 2009, §5).

1, . . . , s∗

Next, we focus on the joint structures in the temporal domain by pooling, ( ˜xj1t, ˜xj2t) ∀j1, j2 = 1, 2, 3, 4
and j1 (cid:54)= j2, ( ˜zkt, ˜zk(t+1)) ∀k = 1, 2, 3, 4, and ( ˜xjt, ˜xj(t+1)) ∀j = 1, 2, 3, 4. We expect that this APSs would
deviate from the reference distribution N (0, I2) more for S = 1 than for S = 3. We compute the
correlation coefﬁcients, and observe the respective p-values for S = 1 and S = 3 (Rahman, 1968).
We observe that for S = 1, the models are rejected either in the latent domain or in the observation
domain except for sequence 3, while for S = 3, the models are rejected either in the latent domain or the
observation domain for sequences 2 and 3 only. In other words, for sequences 1, 4, 5 and 6, the model
improves for S = 3, whereas for sequence 2 it fails to improve, and for sequence 3 it degrades for S = 3.
These observations can again be attributed to the poor segmentation for sequences 1-3. We show the
corresponding aggregated posteriors in the latent and observation space for sequence 6 in Figure 5. We
observe that the residuals in both latent and observation space display correlations for S = 1 while these
is reduced considerably for S = 3.

12

Figure 5: Scatterplot of bivariate aggregated posterior samples (607 × 4 samples) for sequence 6. The
black line shows the best linear ﬁt, whereas the dotted line shows the expected ﬁt in the absence of
(serial) correlation. The values in each plot are the correlation (r) and respective p-value (p).

4.3 Carbon emission data

The CO2 emission dataset10 comprises monthly average atmospheric carbon concentration yi (in parts
per million) between 1958 and 2017 (707 measurements after removing missing values). Rasmussen and
Williams (2006, §5.4.3) show that this time series can be modelled well by a combination of 4 standard
covariance functions involving 10 hyperparameters (and an additional parameter to model the additive
white noise). Each covariance function is introduced to model a speciﬁc aspect of the signal, e.g., a
squared exponential kernel to model the long term trend, a decaying periodic kernel to model the
seasonal variation, a rational quadratic kernel to model the short term irregularities, and another squared
exponential kernel to model the residual correlated noise. We show below how model criticism and
extension can be used to justify the use of covariance functions representing similar aspects of the data.
Following Rasmussen11 we use the measurements up to year 2004 (543) as training data and the
rest (164) as testing data12. We remove the mean of the training data before modelling, and use a zero
−1 to keep
mean function, i.e., m(x) = 0 or m = 0. We use Gamma(α, β) (See Table 2) priors over ζ and τ
them positive. We use the GPstuff toolbox (Vanhatalo et al., 2013) to generate MCMC samples, and
initialize the sampler at the maximum likelihood (ML) solution obtained using GPML toolbox13. We
set the parameters α and β such that the mean of the prior distribution is at the ML solution, and the
variance is equal to the mean. We generate a posterior sample (ϑ∗, ζ∗, τ∗} and aggregate the standardized
projections

z∗ = Λ∗ −1/2U∗(cid:62)(y − m∗)

where U∗ and Λ∗ are the eigenvectors and eigenvalues of kernel matrix K∗ such that K∗
τ∗ −1δ(xi, xj), and m∗ = 0 by design.

We ﬁrst model the time series with the squared exponential or Gaussian kernel,

ij = κ(xi, xj | ζ∗) +

(21)

(22)

κse(x, x(cid:48) | ζ) = σ2

f exp

(cid:18)

−

(x − x(cid:48))2
2l2

(cid:19)

f is the signal variance, i.e., ζ = {σ2

where l is the length scale and σ2
f , l}. We obtain ζML = (188, 0.30) and
ζ∗ = (197, 0.29). We present the ﬁtted data along with unstandardized and standardized projections in
Figure 614. The ﬁgure shows that the Gaussian kernel fails to model the time series as the prediction
quickly falls to the mean of the training signal and KS-test p-value = 4 × 10−10. We observe that most
of the signal strength (ci’s) is concentrated at lower frequencies (corresponding to large eigenvalues

10ftp://ftp.cmdl.noaa.gov/ccg/co2/trends/co2_mm_mlo.txt
11http://learning.eng.cam.ac.uk/carl/mauna/
12We do not use testing data for model criticism but to show the goodness of ﬁt visually.
13http://www.gaussianprocess.org/gpml/code/matlab/doc/
14It is also possible to model this time series with a large length scale, i.e., ζ = (1958, 31) but this has lower marginal likelihood

exp(−1198) as opposed to exp(−753).

13

(a) SE

(b) Periodic

(c) Peri. + SE(s) + SE(l)

Figure 6: Latent values of the Gaussian process model for CO2 emission dataset. Top: Original and ﬁtted
signal. Training and testing sets are separated by a gray line. Middle: Unnormalized projections c∗
i ’s
for i = 1, . . . , n, and the respective 95% conﬁdence interval ±1.96λ1/2
. We only show values for which
i > 2τ∗ −1. y-axis has been transformed by sgn(y)|y|0.3 to show small values. Bottom: ECDFs of aggregated
λ∗
posterior samples [agg. post.] of the normalized projections zi’s and CDF of prior distribution N (0, 1).
p-values correspond to KS-test.

i

λi∈{1,5}). The respective eigenvectors correspond to an upward trend. Also, a relatively high strength is
observed at eigenvalues 92–93. The respective eigenvectors correspond to sinusoids of frequency ∼1
year (see Figure 7a) which indicates a potential need of a periodic covariance function to model this data.
To tackle this, we use the decaying periodic function to model the time series, (Rasmussen and

Williams, 2006, §5.4.3)

κpe(x, x(cid:48) | ζ) = σ2

f exp

(cid:32)

−

2 sin2(π(x − x(cid:48))/p)
l2
p

(cid:33)

(cid:32)

exp

−

(cid:33)

(x − x(cid:48))2
2l2
d

(23)

where p is the period of the covariance function. Therefore, ζ = (σ2

f , p, lp, ld). We obtain ζML =

14

(a) SE

(b) Periodic

Figure 7: Weighted eigenfunctions of carbon emission dataset. The kinks in the plot appear due to the
short length scale.

(283, 1, 5.13, 5.86), and ζ∗ = (385, 1, 4.88, 6.09). We observe that this provides a better ﬁt than squared
exponential kernel (p-value 0.08). Although the KS-test fails to reject the ﬁtted model (perhaps due to
lack of samples), we observe that the signal strengths (ci’s) still deviate from their expected values. In
particular, the second, fourth and sixth projections show relatively high values compared to third, ﬁfth
and seventh. The signal ∑i∈{2,4,6} ciui corresponds to an upward trend, which corroborates the need to
model the trend further. See Figure 7b. Note that although the CO2 data is a time-series, the analysis
of the c-samples (see Eq. 10) does not depend on this, and can also be used where the input-space is
multi-dimensional.

f , p, lp, ld, σ2

To accommodate the upward trend, we introduce a squared exponential kernel with a relatively
large length scale. However, to avoid modelling small scale variations with the same kernel, we use
combination of two squared exponential kernels with two different length scales. Therefore, ζ =
(σ2
f l, ll) where the last four parameters belong to the two squared exponential kernels
with small (s) and large (l) length scales. We obtain ζML = (4.37, 1, 1.78, 74.60, 0.81, 0.92, 4132, 27.14), and
ζ∗ = (2.25, 1, 1.24, 73.88, 0.32, 0.66, 4095, 32.25). We observe that this improves the ﬁt even further, both in
terms of the testing data (visually) and in terms of unstandardized projections. The KS-test uses more
samples, and still fails to reject the model (p-value 0.55).

f s, ls, σ2

Model criticism of Gaussian processes in the observation space has been discussed by Lloyd and
Ghahramani (2015). However, their approach is different from the standard posterior predictive check
since the authors use hold-out data rather than using the observed data twice. Although this approach
shows if the response on hold-out data is different for the ﬁtted model, it does not necessarily point out
how the model can be extended.

One could generate a replicate sample from yrep ∼ p(· | ζ∗, Xobs), and compare yrep and yobs as for
a posterior predictive check. However, note that in this case yrep will be an independent draw from the
GP with parameters ζ∗ and input locations Xobs, hence it could look very different from yobs—this is
why Lloyd and Ghahramani (2015) make use of held-out data. Also, it would be difﬁcult to come up
with a suitable discrepancy function in this case. One could consider the χ2 discrepancy15, i.e., y(cid:62)K−1y.
However, this quantity is ﬁtted when sampling the kernel parameters ζ, and is also (as discussed above)
dominated by the noise for small eigenvalues of K. Other discrepancy measures could be investigated,
but exploring these alternatives is beyond the scope of this paper.

5 Discussion

Model criticism explores the discrepancies between a statistical model P(X, U) and observed data xobs.
This is often achieved by generating replicate observations Xrep ∼ P(X | xobs) from the ﬁtted model,

15Inspired by Gelman et al. (1996, Eq. (8))

15

and investigating which aspects D(X, U) of the replicated observations do not match the observed
data. Instead here we have focused on pulling the effect of the data back into the latent space, and
investigating if the posterior sample u∗ ∼ P(U | xobs) follows the prior distribution P(U), as it should do
by Statement 1 if the data were generated by the model. This is tested by aggregating related variables
with the same prior distribution and comparing them with the associated prior.

It should be noted that model criticism is not used to judge if a model is right or wrong. On the
contrary, it is widely accepted that all models are wrong but some are useful (Box and Draper, 1987, p.
424). Model criticism aims at understanding the limitations of the model with the hope that a better
model can be found, e.g., since all models are basically simpliﬁcations of a more complex process, model
criticism inspects if the simpliﬁcation is meaningful, or if the statistical assumptions made are reasonable.
Following this principle, we have discussed four examples of model criticism in latent space. We have
shown that by analysing the distribution of the aggregated posterior, a model can be extended so that
the aggregated posteriors follow the respective prior distributions better.

We thank the anonymous referees for pointing out the work of Johnson (2007) and Yuan and Johnson
(2012), and for comments which helped to improve the paper.

Acknowledgements

References

M. J. Bayarri and James O. Berger. p-values for Composite Null Models. Journal of the American Statistical

Association, 95(452):1127–1142, December 2000. 3, 4

T. R. Belin and D. B. Rubin. The Analysis of Repeated-Measures Data on Schizophrenic Reaction Times

using Mixture Models. Statistics in Medicine, 14(8):747–768, 1995. 4

G. E. P. Box and N. R. Draper. Empirical Model-Building and Response Surfaces. Wiley, 1987. 16

G. E.P Box. Sampling and Bayes’ Inference in Scientiﬁc Modelling and Robustness. Journal of the Royal

Statistical Society, 143(4):383–430, 1980. 3

T. S. Breusch and A. R. Pagan. A simple test for heteroscedasticity and random coefﬁcient variation.

Econometrica, 47(5):1287–1294, 1979. 7

R. P. Buccigrossi and E. P. Simoncelli. Image Compression via Joint Statistical Characterization in the

Wavelet Domain. IEEE Transactions on Signal Processing., 8(12):1688–1701, 1999. 7

J. V. Candy. Signal Processing: The Model Based Approach. McGraw-Hill, 1986. 12

S. R. Cook, A. Gelman, and D. B. Rubin. Validation of software for bayesian models using posterior

quantiles. Journal of Computational and Graphical Statistics, 15(3):675–692, 2006. 7

J. Durbin and G. S. Watson. Testing for serial correlation in least squares regression: I. Biometrika, 37(3/4):

409–428, 1950. 7

E. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky. Nonparametric Bayesian Learning of Switching
Linear Dynamical Systems. In Advances in Neural Information Processing Systems 21, pages 457–464.
2009. 11, 12

A. Gelman, X. Meng, and H. Stern. Posterior predictive assessment of model ﬁtness via realized

discrepancies. Statistica Sinica, pages 733–807, 1996. 4, 15

A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman and Hall, London,

2004. Second edition. 1

P. Gopalan, J. M. Hofman, and D. M. Blei. Scalable recommendation with hierarchical poisson factor-
ization. In Proceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence, UAI, pages
326–335, 2015. 4

16

G. E. Hinton, S. Osindero, and Y. W. Teh. A Fast Learning Algorithm for Deep Belief Nets. Neural

Computation, 18:1527–1554, 2006. 8

doi: 10.1214/07-BA229. 7, 16

V. E. Johnson. Bayesian model assessment using pivotal quantities. Bayesian Anal., 2(4):719–733, 12 2007.

J. R Lloyd and Z. Ghahramani. Statistical Model Criticism Using Kernel Two Sample Tests. In Advances

in Neural Information Processing Systems, 2015. 4, 15

D. Martin, C. Fowlkes, D. Tal, and J. Malik. A Database of Human Segmented Natural Images and its
Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics. In Proceedings
of 8th International Conference on Computer Vision, volume 2, pages 416–423, 2001. 8

M. Meulders, A. Gelman, I. Van Mechelen, and P. De Boeck. Generalizing the Probability Matrix
Decomposition Model: an Example of Bayesian Model Checking and Model Expansion. In Hox, J. J.
and de Leeuw, E. D., editor, Assumptions, Robustness and Estimation Methods in Multivariate Modeling.
TT-Publikaties, Amsterdam, 1998. 7

S. M. Oh, J. M. Rehg, T. Balch, and F. Dellaert. Learning and Inferring Motion Patterns using Parametric
Segmental Switching Linear Dynamic Systems. International Journal of Computer Vision, 77(1):103–124,
2008. 11

A. O’Hagan. HSSS Model Criticism. In P. J. Green, N. L. Hjort, and S. Richardson, editors, Highly

Structured Stochastic Systems, pages 422–444. Oxford University Press, 2003. 1, 4, 7

M. Plummer. JAGS: A Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling. In
Proceedings of the 3rd International Workshop on Distributed Statistical Computing (DSC 2003), 2003. 8

N. A. Rahman. A Course in Theoretical Statistics. Charles Grifﬁn and Company, 1968. 12

C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.

ISBN 026218253X. 6, 13, 14

O. Ratmann, C. Andrieu, C. Wiuf, and S. Richardson. Model criticism based on likelihood-free inference,
with an application to protein network evolution. Proceedings of the National Academy of Sciences, 106
(26):10576–10581, 2009. ISSN 0027-8424. doi: 10.1073/pnas.0807882106. 1

D. B. Rubin. Bayesianly Justiﬁable and Relevant Frequency Calculations for the Applied Statistician.

Annals of Statistics, 12:1151–1172, 1984. 3

R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte

Carlo. In Proceedings of the International Conference on Machine Learning, volume 25, 2008. 1

Y. Tang, R. Salakhutdinov, and G. E. Hinton. Deep Mixtures of Factor Analysers. In Proceedings of the

29th International Conference on Machine Learning, 2012. 7, 8

J. Vanhatalo, J. Riihim¨aki, J. Hartikainen, P. Jyl¨anki, V. Tolvanen, and A. Vehtari. GPstuff: Bayesian
modeling with Gaussian processes. J. Mach. Learn. Res., 14(1):1175–1179, April 2013. ISSN 1532-4435.
13

M J Wainwright and E P Simoncelli. Scale Mixtures of Gaussians and the Statistics of Natural Images. In

Advances in Neural Information Processing Systems, volume 12, pages 855–861, 2000. 9

H. White. A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedas-

ticity. Econometrica, 48(4):817–838, 1980. 7

M. B. Wilk and R. Gnanadesikan. Probability plotting methods for the analysis of data. Biometrika, 55(1):

1–17, 1968. 7

(1):156 – 164, 2012. 2, 5, 7, 16

Y. Yuan and V. E. Johnson. Goodness-of-ﬁt diagnostics for Bayesian hierarchical models. Biometrics, 68

D. Zoran and Y. Weiss. Natural Images, Gaussian Mixtures and Dead Leaves. In F. Pereira, C. J. C.
Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25,
pages 1736–1744. Curran Associates, Inc., 2012. 8, 9

17

8
1
0
2
 
l
u
J
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
4
7
6
4
0
.
1
1
7
1
:
v
i
X
r
a

Model Criticism in Latent Space

Sohan Seth∗

Iain Murray†

Christopher K. I. Williams‡§

Abstract

Model criticism is usually carried out by assessing if replicated data generated under the ﬁtted
model looks similar to the observed data, see e.g. Gelman, Carlin, Stern, and Rubin (2004, p. 165). This
paper presents a method for latent variable models by pulling back the data into the space of latent
variables, and carrying out model criticism in that space. Making use of a model’s structure enables
a more direct assessment of the assumptions made in the prior and likelihood. We demonstrate the
method with examples of model criticism in latent space applied to factor analysis, linear dynamical
systems and Gaussian processes.

1 Introduction

Model criticism is the process of assessing the goodness of ﬁt between some data and a statistical
model of that data1. While model criticism uses goodness-of-ﬁt tests to judge aspects of the model, its
general objective is to identify deﬁciencies in the model that can lead to model extension to address these
deﬁciencies. The extended model(s) can again be subjected to criticism, and the process continues until
a satisfactory model is found (O’Hagan, 2003). Model criticism is contrasted with model comparison in
that model criticism assesses a single model, while model comparison deals with at least two models to
decide which model is a better ﬁt. Model comparison can be applied to compare the original and the
extended model after model criticism and extension (O’Hagan, 2003, p. 2).

Bayesian modelling has become an indispensable tool in statistical learning, and it is being widely
used to model complex signals, e.g., by Ratmann et al. (2009). With its growing popularity, there is need
for model criticism in this framework. Most work on model criticism makes use of the idea that “if
the model ﬁts, then replicated data generated under the model should look similar to observed data”
(Gelman et al., 2004, p. 165). In contrast, in this paper we focus on a less well explored idea that for latent
variable models, we can probabilistically pull back the data into the space of the latent variables, and
carry out model criticism in that space. We can summarize this principle as that if the model ﬁts, then
posterior inferences should match the prior assumptions.

To elaborate, consider a model with observed variables X and unobserved variables U with joint
distribution P(X, U | γ) where γ are known parameters. In general U may contain latent variables Z,
parameters Θ, and hyperparameters λ2. Given a sample xobs from the marginal distribution P(X | γ), and
a single posterior sample u∗ from the conditional distribution P(U | xobs, γ), the joint sample (xobs, u∗)
is a draw from the distribution P(X, U | γ). This property can be used to check the ﬁt of the model in
the latent space by checking if u∗ is a sample from the marginal distribution P(U | γ). Testing a single
sample against a distribution, however, is not an effective approach. But, in many widely-used models,

∗School of Informatics, University of Edinburgh, UK; seth@inf.ed.ac.uk
†School of Informatics, University of Edinburgh, UK; i.murray@ed.ac.uk
‡School of Informatics, University of Edinburgh, UK; and the Alan Turing Institute, London, UK; ckiw@inf.ed.ac.uk
§SS and CW gratefully acknowledge the UK Engineering and Physical Sciences Research Council (EP/K03197X/1) for funding
this work. The work of CW is supported by EPSRC grant EP/N510129/1 to the Alan Turing Institute. Code used in the paper is
available at https://github.com/sohanseth/mcls.

1Following O’Hagan (2003, p423) we prefer the term model criticism over model validation and model checking, as if “all models are
strictly wrong” it is impossible to validate a model, and model criticism has a more active tone of looking to discover problems,
compared to model checking, which may seem a more passive activity that does not expect to uncover any problems.

2For example, in the context of the Bayesian matrix factorization (Salakhutdinov and Mnih, 2008), X is the observed data matrix,
U = {Z, Θ, λ} is the matrix of latent factors Z, the loading matrix Θ, precision hyperparameters λ, and γ denotes the parameters
of the hyperpriors.

1

Figure 1: a) A probabilistic model with observed variables X, unobserved variables U, and known
parameters γ, b) Given the observed data xobs from P(X | γ) and a posterior sample u∗ from P(U | xobs, γ),
(xobs, u∗) is a joint sample from P(X, U | γ), and therefore, u∗ is a sample from P(U | γ) and xobs is
a sample from P(X | u∗, γ). c) If the prior (or part of it) factorizes into identical distributions, e.g.,
P(U | γ) = ∏2
2} is independent and identical sample from
Pu(· | γ). d) A factor analysis model showing observed variables X = {xi}n
i=1, unobserved variables
U = {{zi}n
12, . . .} is a sample from
P(Z | τz), and {θ∗

i=1, Θ, τ, τθ}, and known parameters γ = {α, β, τz}. We test if {z∗

k=1 Pu(Uk | γ), then posterior sample {u∗

12, . . .} is a sample from P(θ | τ∗
θ ).

11, θ∗

11, z∗

1, u∗

groups of unknown variables are independently and identically distributed under the prior. These
related variables are easily aggregated together, giving a simple test of the prior assumptions. Figure 1
summarizes the overall approach, which is justiﬁed in §3.

In comparison to model criticism in the observation space, comparing u∗ with prior P(U | γ), provides
an additional tool for model criticism which does not require crafting an appropriate discrepancy
measure, generating replicate observations, and approximating the null distribution. This approach also
does not suffer from the “double use” of data (see discussion in §2). These points have also been made
by Yuan and Johnson (2012), but were applied to a relatively small scale hierarchical linear model. We
develop the use of model criticism in latent space for large scale and complex models, yielding new
insights and developments. Speciﬁcally, we apply this approach to the criticism of linear dynamical
systems, factor analysis and Gaussian processes, and discuss its connection to the observation space
based approach.

The structure of the rest of the paper is as follows: in §2 we describe the methods of model criticism in
observation space. §3 provides details of the argument for model criticism in latent space and describes
related work, and §4 shows results from applying the method to the three examples. Table 1 describes
the notations followed in the paper, and Table 2 shows the distributions used in the paper.

Style

Explanation

Upper case italics
Lower case italics
Lower case bold
Upper case bold
P(X)
P(X | y)
p(x)
p(x | y)
· ∼ ·
·∗
·obs
·rep
0
I

Random variable or a group of random variables
Realization of a random variable
Vectors, realization or random variable
Matrices, realization or random variable
Distribution of random variable X
Conditional distribution of random variable X given Y = y
Probability density function of r. v. X, abbreviation for pX (x)
Conditional density function of r. v. X given Y = y, abbreviation for pX | Y (x | y)
Distributed as
A posterior sample
Observed data
Replicate data
Zero vector
Identity matrix

Example

i=1, u∗
i=1, P(z), u = (u1, . . . , uK )(cid:62)
i=1, P(Z), U = [u1, . . . , uK]

X, Z, U = {U1, . . . , UK }
{xi}n
{xi}n
{Xi}n
X ∼ P(X)
X ∼ P(X | y)
p(y) = (cid:82) p(x, y)dx
p(y) = (cid:82) p(x | y)p(y)dx
Xrep ∼ P(X), X ∼ N (0, 1)
u∗, z∗
xobs, Xobs ∼ P(X)
xrep, Xrep ∼ P(X)
x ∼ N (0, I)
x ∼ N (0, I)

Table 1: Description of notation used in the paper.

2

Name

Normal

Notation

N (x | µ, τ

−1)

Multivariate normal N (x | µ, τ

−1)

Double exponential

L(x | µ, τ)

Gamma

Dirichlet

Gamma(x | α, β)

Dir(x | α)

τ1/2

Density
(2π)1/2 exp (cid:0)−(τ(x − µ)2)/2(cid:1)
det(τ)1/2
(2π)d/2 exp (cid:0)−((x − µ)(cid:62)τ(x − µ))/2(cid:1)
τ
2 exp(−τ|x − µ|)
βα
Γ(α) xα−1 exp(−βx)
i=1xαi −1

where xi ∈ (0, 1) and ∑K

∏K

1
B(α)

i

i=1 xi = 1.

Table 2: List of distributions used in the paper.

2 Model Criticism in Observation Space

A general approach of model criticism is to evaluate if replicated data generated under the (ﬁtted)
model looks similar to observed data. Consider that we are modelling observed data xobs with a latent
variable model parameterized by U, i.e., we have deﬁned the likelihood p(x | u) and (optionally) a prior
distribution P(U) over potential parameter values. The principle of model criticism in the observation
space is to assess if xobs is a reasonable observation under the proposed model. For example, given the
maximum likelihood estimator (or another point estimate) ˆu of the parameters, one standard approach is to
ﬁnd the plug-in p-value (Bayarri and Berger, 2000)

pplug-in = Pr(D(Xrep, ˆu) > D(xobs, ˆu)).

Here D is called a discrepancy function and it resembles a test statistic in hypothesis testing, i.e., a larger
value rejects the null hypothesis or indicates incompatibility of data and model, and Xrep is a replicate
observation generated under the ﬁtted model, i.e., Xrep ∼ P(X | ˆu).

If the p-value is low, then it implies that the probability of generating a more extreme dataset than the
observed data is small, or in other words, the observed data itself is considered extreme relative to the
model, and thus, the model does not adequately describe the dataset. In summary, a low p-value rejects
the hypothesis that the data is being adequately modelled. The p-value is usually estimated via an empirical
average by generating multiple replicates xrep

, r = 1, . . . , R, and evaluating

ˆpplug-in =

1(D(xrep

r

, ˆu) > D(xobs, ˆu)),

r

1
R

∑
r

(1)

(2)

where xrep

r

is a sample from P(X | ˆu).

An alternative to point estimation is to consider a Bayesian treatment of the problem where one
can integrate out the contribution of the parameters. The test statistic can be averaged under either the
prior distribution or the posterior distribution. The prior-predictive distribution is deﬁned to have the
density p(xrep | γ) = (cid:82) p(xrep | u) p(u | γ) du where γ parameterizes the prior distribution over U. One
can generate replicate observations from this distribution, and compute the prior predictive p-value (Box,
1980)

pprior = Pr(D(Xrep, U) > D(xobs, U)) ≈

1(D(xrep

r

, ur) > D(xobs, ur)) = ˆpprior,

(3)

where (xrep, u)r is a sample from P(X, U | γ). This approach is not reasonable when the prior distribution
is improper (cannot be integrated) or uninformative. Additionally, even if the prior distribution is
informative, one might not generate enough samples to represent the data distribution well when the
parameter space is large. However, notice that one does not need to ﬁt the model to criticise it.

On the other hand, one can use the posterior distribution P(U | xobs), and sample from the posterior-
predictive distribution with density p(xrep | xobs) = (cid:82) p(xrep | u) p(u | xobs) du. The posterior predictive
p-value (Rubin, 1984) is then computed as:

ppost = Pr(D(Xrep, U) > D(xobs, U) | xobs) ≈

1(D(xrep

r

, ur) > D(xobs, ur)) = ˆppost,

(4)

1
R

∑
r

1
R

∑
r

3

where (xrep, u)r is a sample from P(Xrep, U | xobs), i.e., by generating samples ur from the posterior
distribution instead3. The support of the posterior is usually more concentrated than prior, and the
posterior distribution may be well-deﬁned even if the prior distribution is improper.

The posterior predictive p-value has been criticised for “double use” of data, once for computing
the posterior distribution P(Xrep | xobs) and once for computing the discrepancy measure D(xobs, U)
(Bayarri and Berger, 2000). This means that ppost does not have a uniform distribution under the null
hypothesis, whereas pprior is a valid p-value. pplug-in is subject to the same criticism as ppost since the
MLE uses the observed data as well (Bayarri and Berger, 2000). Lloyd and Ghahramani (2015, §7) view
the different p-values as arising from “different null hypotheses and interpretations of the word ‘model’ ”.
They argued that the posterior predictive and plug-in p-values are most useful for highly ﬂexible models,
as the aim is to assess the ﬁtted model rather than the whole space of models. Lloyd and Ghahramani
(2015) also point out that “it may be more appropriate to hold out data and attempt to falsify the null
hypothesis that future data will be generated by the plug-in or posterior distribution”, which is also in
line with the discussion in (O’Hagan, 2003, §2.1). Further examples of posterior predictive checking can
be found in (Belin and Rubin, 1995; Gopalan et al., 2015).

In all of the model criticism described above, a key quantity is the discrepancy function D used to
compare the data and predictive simulations. We agree with Belin and Rubin (1995, p. 753) who wrote of
the importance of identifying discrepancy functions “that would not automatically be well ﬁt by the
assumed model”, and that “there is no unique method of Bayesian model monitoring, as there are an
unlimited number of non-sufﬁcient statistics that could be studied”.

Lloyd and Ghahramani (2015) suggest the Maximum Mean Discrepancy (MMD) as a measure of
discrepancy between the observed data and replicates. The motivation of using this approach is to
maximize the discrepancy over a class of discrepancy functions rather than choosing only one, i.e.,

MMD = sup
f ∈F

(E

Xobs f (Xobs) − E

Xrep f (Xrep))

where F is a set of functions. The function that maximizes the discrepancy is known as the witness
function. When F is a reproducing kernel Hilbert space (RKHS) the witness function can be derived in
closed form as

ˆf (·) =

1
|xobs|

|xobs|
∑
i=1

κ(·, xobs

) −

i

1
|xrep|

|xrep|
∑
j=1

κ(·, xrep

),

j

(5)

(6)

where κ is the kernel of the RKHS. This estimation does not work well in high dimensions, and therefore,
the authors suggests reducing the dimensionality of the observation space before applying this statistic
(Lloyd and Ghahramani, 2015, p. 4).

3 Model Criticism in Latent Space

Recall we have a model P(X, U | γ), with observed variables X, unobserved variables U, and known
parameters γ. In general U may contain latent variables Z, parameters Θ, and hyperparameters λ. Our
procedure depends on the following two key observations:

1. If xobs is drawn from the above model, then a sample u∗ from P(U | xobs, γ) is a sample from the
prior distribution P(U | γ). To see why this is true, observe that a natural way to sample from
the joint P(U, X | γ) is to generate a sample u from P(U | γ), and then generate a sample x from
P(X | u, γ) in that order. However, it is also valid to draw samples from the joint by ﬁrst sampling
x from P(X | γ) and then sampling u from P(U | x, γ). Thus we have

Statement 1. If xobs is a sample from P(X | γ), then a sample u∗ from P(U | xobs, γ) will be a draw from
P(U | γ).

3Note that the p-value ppost(u) = P(D(Xrep, u) > (xobs, u)) might be available in closed form depending on the choice of D

(Gelman et al., 1996, Eq. (8–9)). Then ppost = 1
R

∑r ppost(ur) where ur are posterior samples.

4

It is important to clarify what Statement 1 is not saying. It is not saying that repeated draws from
P(U | xobs, γ) will explore the full prior distribution P(U | γ), but only that it is a valid way to draw
one sample from it if xobs is a draw from the model4. However, testing how well a single draw
from a given distribution ﬁts that distribution is difﬁcult. This brings us to our second observation.

1, . . . , u∗

1, . . . , u∗

2. If U is a collection of variables, i.e., U = (U1, . . . , UK), and the prior distribution of U decomposes
into independent draws from the same distribution, e.g., P(U | γ) = ∏K
k=1 Pu(Uk | γ) then it is
possible to aggregate these variables together, i.e., instead of testing if (u∗
K) is a sample from
P(U | γ), one can test if {u∗
K} is independent and identical draws from the distribution
Pu(· | γ).
In other words, rather than testing one sample against a known high dimensional
distribution, one can test if the collection of K samples are independent and identical draws from a
known lower-dimensional distribution Pu. Thus, we deﬁne aggregation as pooling variables with the
same prior distribution together, and an aggregated posterior sample (APS) is deﬁned as a set of posterior
samples that have been aggregated for comparison with a speciﬁc reference distribution. The above
can be generalized to the situation where U = (U1, . . . , UK, θ) is a collection of variables and
k=1 Pu(Uk | θ)P(θ | γ). Then {u∗
parameters such that P(U | γ) = ∏K
K} can be aggregated and
tested against Pu(· | θ∗)5. Aggregation can be also extended to the case where U consists of groups
1 , . . . , Ug
of variables (U1, . . . , UG) where aggregation is performed within each group Ug = (Ug
)
Kg
by pooling {ug∗
} and comparing against pug (· | u−g∗) where U−g denotes all groups except
g. We provide more concrete examples of aggregation in §3.1 and Table 3.

1 , . . . , ug∗

1, . . . , u∗

Kg

We refer to this approach as aggregated posterior checking (APC). We summarize this approach in
Algorithm 1. Ideas equivalent to Statement 1 and the aggregation of posterior samples can also be found
in Yuan and Johnson (2012) 6, but were applied to the case where U contains only model parameters,
and for hierarchical linear models. See §3.2 for more details on related work.

Algorithm 1 Aggregated posterior check
Require: Observed data xobs
Require: Bayesian model P(X | U, γ)P(U | γ) with latent variables U
1: Generate a posterior sample u∗ from P(U | xobs, γ)
(cid:46) See Table 3
2: Generate aggregated posterior sample(s)
3: Compare aggregated posterior sample(s) with corresponding reference distribution(s) with appropri-

ate test

4: return p-value of the test(s)

So far, we have addressed the idea of assessing deviations from the prior distribution and aggregation
in the latent space. However, the same idea can be applied to the observation space as well, i.e., to
the likelihood by testing if xobs is a sample from P(X | u∗, γ). Although it is true that a discrepancy in
the choice of likelihood should be reﬂected in the posterior sample, assessing the discrepancy in the
likelihood directly provides better understanding and easier resolution of the discrepancy. Notice that,
although we make use of xobs, our approach is not equivalent to model criticism in the observation space
since we do not compare the observed data xobs with replicate observations xrep
, but only investigate
the relation between the latent space u∗ and observation space xobs. Both methods, however, require
generating posterior samples ur (for model criticism in the latent space we use r = 1).

r

3.1 Application to different models

We discuss below the application of model criticism in latent space to factor analysis, linear dynamical
systems and Gaussian process regression. These situations are then demonstrated on real data in §4.

4Throughout this paper, we assume that the prior distribution is proper, so the respective posterior distribution is well-deﬁned,

and that any MCMC sampler has converged, i.e., the posterior sample is well-behaved.

5Alternatively, U and θ can be combined to deﬁne a pivotal quantity s whose distribution does not depend on θ (Yuan and

Johnson, 2012), and s(u∗, θ∗) can be tested against that distribution.

6We had independently derived the key results. We thank an anonymous referee for pointing out the work of Yuan and Johnson

(2012).

5

p(zi) p(xi | zi, Θ, τ).

n
∏
i=1
−1I) and θ | λ ∼ N (0, τθ

(7)

Factor analysis model Consider a factor analysis model with hyperparameters λ = {τθ, τ}, parameters
(loading matrix) Θ, latent variables (factors) z and data x. Grouping Z = {zi}n
i=1 and similarly for X7,

p(λ, Θ, Z, X) = p(λ) p(Θ | λ) p(Z | λ) p(X | Z, Θ, λ) = p(τθ) p(τ)p(Θ | τθ)

−1I)
Figure 1d illustrates this model. In Gaussian factor analysis, z ∼ N (0, τz
(See Table 2). There is an identiﬁability issue in the factor analysis model between Θ and z, which is
resolved by ﬁxing the scale of one of the two. In Eq. (7) the dependence of z on λ is taken to be null, i.e.,
−1I) and
τz = 1. (In the case of example §4.1 we ﬁx the scale of Θ instead.) Also, p(x | z, Θ, τ) = N (Θz, τ
τ, τθ | α, β ∼ Gamma(α, β). Thus the ﬁxed parameters γ = {τz, α, β}.

If Xobs is drawn from the above model, then a sample λ∗, Θ∗, Z∗ from P(λ, Θ, Z | Xobs) is a sam-
ple from the prior P(λ)P(Θ | λ) P(Z | λ). In factor analysis, Z decomposes into independent draws
from P(z | τz), and therefore, one can pool the posterior samples z∗
i to assess deviations from P(z | τz).
Moreover, each zi usually decomposes into independent draws over the different latent dimensions as
∏k p(zik | τz), one can pool the z∗
ik to assess deviations from p(z | τz). Similarly, if the prior over the factor
loadings matrix Θ decomposes as p(Θ | τθ) = ∏kj p(θkj | τθ) then one can pool the θ∗
kjs, and compare
with p(θ | τ∗
θ ). One can also go beyond the marginal z or the full vector z, and assess a subset of the
vector such as bivariate interactions (see §4.1).

Linear dynamical system One can extend the idea of aggregation beyond factor analysis models. For
example, Statement 1 holds for general latent variable models with repeated structure. Take, for example,
a linear dynamical system model with a latent Markov chain, so that

p(X, Z | U) = p(z1)p(x1 | z1, U)

p(zt | zt−1, U)p(xt | zt, U)

(8)

T
∏
t=2

where U consists of the system and observation matrices A, B, and precisions, Q, R. Then according to
Statement 1 a sample (Z∗, u∗) drawn from P(Z, U | Xobs) should be distributed according to the prior
over (Z, U). Although the zt’s are not independent (due to the Markov chain), we can consider model
criticism for p(zt | zt−1). For example for a system model parameterized as zt | zt−1 ∼ Azt−1 + (cid:101)t with
(cid:101)t ∼ N (0, Q−1), violations of the model will show up as deviations of the (cid:101)∗
t ’s from N (0, Q∗ −1) (see §4.2).
Similarly, for an observation model parameterized as xt | zt ∼ Bzt + ψt with ψt ∼ N (0, R−1), violations
of the model may also show up as deviations of the ψ∗

t ’s from N (0, R∗ −1). See §4.2.

Gaussian process regression A Gaussian process probabilistic model is deﬁned as:

ϑ, ζ, τ ∼ p(ϑ) p(ζ) p(τ),
f (x) ∼ GP (m(x | ϑ), κ(x, x(cid:48) | ζ)),

yi ∼ N ( f (xi), τ

−1) ∀i = 1, . . . , n,

where m(x | ϑ) is the mean function parameterized by ϑ, κ(x, x(cid:48) | ζ) is the covariance function (or ker-
nel) parameterized by ζ, and τ is the observation noise precision, see e.g., Rasmussen and Williams (2006).
Given observations {(xi, yi)}n
−1δ(xi, xj). Alternatively, considering the eigendecomposition K = UΛU(cid:62)
and Kij = κ(xi, xj | ζ) + τ
where U = [u1, . . . , un] is the matrix of the eigenvectors uis and Λ is the diagonal matrix of the corre-
sponding eigenvalues, i.e., Λ

i=1, y ∼ N (m, K), where y = (y(x1), . . . , y(xn))(cid:62), m = (m(x1 | ϑ), . . . , m(xn | ϑ))(cid:62)

ii = λi,

c = U(cid:62)(y − m) ∼ N (0, Λ).

This implies that, according to the model, the projections ci of the signal y on the eigenvector ui are
independent samples from N (0, λi). Thus, the normalized projections

7We have omitted the ﬁxed parameters γ for simplicity.

z = Λ−1/2U(cid:62)(y − m) ∼ N (0, I)

6

(9a)

(9b)

(9c)

(10)

(11)

should be independent samples from N (0, 1) distribution. One can thus test the normality of the z’s
to assess the goodness of ﬁt. However, note that if the ith eigenvalue of K is much smaller than the
−1, then this zi is dominated by the white noise contribution. Thus we only include z’s
noise variance τ
corresponding to eigenvalues with λi > 2τ

−1 to assess the ﬁt of the GP model8. See §4.3.

3.2 Related Work

Cook et al. (2006) consider the situation with (in our notation) a prior p(u) on the parameters of the
model, and data p(x | u). They then assume that speciﬁc parameters u0 are drawn from the prior, then
data xobs drawn from P(X | u0). They then consider samples u1, u2, . . . , uL drawn from P(U | xobs), and
comment (in the caption of their Figure 1, translating to our notation) that “(xobs, u(cid:96)) should look like
a draw from P(X, U) for (cid:96) = 0, 1, . . . , L”. They then use the ‘reverse’ of Statement 1 to validate the
correctness of posterior samples generated by a statistical software, by comparing u0 with u1, u2, . . . , uL.
Their recommended method for this is to calculate posterior quantiles for each scalar parameter; if the
software is working correctly then the posterior quantiles are uniformly distributed. Although they
share with us the observation that (xobs, u(cid:96)) should look like a draw from P(X, U), this is used to answer
a totally different question. Also, they do not discuss the inclusion of latent variables in the model.

Johnson (2007) and later Yuan and Johnson (2012) also consider a model with parameters U and
data drawn from P(X | U). Their interest is in the use of pivotal quantity d(x, u) that has a known
and invariant sampling distribution when data xobs are generated from a model with data-generating
parameters u0. Then Yuan and Johnson (2012) show that if the d(X, u0) is a pivotal quantity distributed
according to F, then d(X, u(cid:96)) is also distributed according to F, if u(cid:96) is drawn from the posterior on
U given xobs. The result of Yuan and Johnson (2012) extends earlier work by Johnson (2007) to the
case where d(x, u) does not depend on the data x—for example this situation can arise in a Bayesian
hierarchical linear regression model, when considering the second level where parameters for individual
units are generated from a hyperprior.

Regression diagnostics is a well-explored example of model criticism. Existing approaches assess
certain statistical assumptions made during modelling, e.g., if the residuals follow a normal distribution
with zero mean, (e.g., using a Q-Q plot (Wilk and Gnanadesikan, 1968)), if the residuals are homoscedas-
tic, (e.g., using the Breusch–Pagan (Breusch and Pagan, 1979) or White test (White, 1980)) or if the
successive residual terms are uncorrelated (e.g., using the Durbin–Watson test (Durbin and Watson,
1950)). Regression diagnostics can be seen as a special case of model criticism in the latent space since
residuals are representatives of errors, which are latent variables of the model. However, our methods
are also applicable to more complex models.

Meulders et al. (1998) consider a factor analysis model for binary data, using (in our notation)
Beta(2, 2) priors on Z and Θ. They carry out posterior sampling using block Gibbs sampling for Z and
Θ and compare histograms of these variables against the prior. Discrepancies between the prior and
histograms of the sampled aggregated posterior led to model extension, expanding the model to use a
mixture of two beta distributions for the parameters. However, the authors do not explain the basis for
carrying out this check (cf. Statement 1).

Buccigrossi and Simoncelli (1999) consider the posterior distribution of wavelet coefﬁcients (analo-
gous to z in the factor analysis model) in response to image patches. By considering the distribution of
a bivariate aggregated posterior, they show that this is not equal to the product of the marginals, but
exhibits variance correlations. (This is shown by introducing a “bowtie plot” showing the conditional
histogram of z2 given z1.) This work is a nice example of how the failure of a diagnostic test can give rise
to an extended model (see §4.1).

O’Hagan (2003, §3) considered model criticism tools that can be applied at each node of a graphical
model (and of course latent variables can be considered as such). O’Hagan (§3.1 2003) discussed the idea
of residual testing at different levels of a hierarchical model as well as a generic probabilistic model. He
suggested checking if a node in a probabilistic model is misbehaving by comparing the posterior samples
at that node to prior distribution. O’Hagan (§3.2 2003) also emphasized that conﬂict can arise between
the different sources of information about a variable at a particular node, arising from contributions from
each neighbouring node in the graph. However, he did not suggest using the aggregated posterior to
assess goodness of ﬁt, but considered the posterior at each node separately.

8The factor of 2 on the RHS of the inequality is included because the λi’s are shifted by τ

−1 by deﬁnition.

7

Tang et al. (2012) introduce the concept of the “aggregated posterior” as applied to deep mixtures of
factor analysers (MFA) model. Consider the situation as above but where Θ is estimated by maximum
likelihood, so it is the posterior over Z that is of interest. Thus p(X, Z | Θ) = ∏n
i=1 p(zi) p(xi | zi, Θ).
Under this model we also have that p(z) = (cid:82) pΘ(z | x) pΘ(x)dx where the Θ subscript denotes that
both pΘ(z | x) and pΘ(x) correspond to distributions under the model. Tang et al. (2012, p3) deﬁne the
aggregated posterior as “the empirical average over the data of the posteriors over the factors”, i.e.,

˜p(z) =

pΘ(z | xobs

),

i

1
n

n
∑
i=1

(12)

where the integral wrt pΘ(x) has been replaced by the empirical average over samples. If the data
distribution p(x) is equal to the model distribution pΘ(x) then ˜p(z) should agree with p(z). However,
differences between p(x) and pΘ(x) will manifest as differences between the two respective distributions
in the latent space.

) for i = 1, . . . , n to p(z). This is a valid approach since if {xobs

In practice, however, one does not explicitly construct the aggregated posterior Eq. (12) since it is
only asymptotically equal to the prior. Instead Tang et al. (2012) compare a collection of n samples z∗
i
from pΘ(z | xobs
n } follow the
i
distribution pΘ(x), then {z∗
n} follow the distribution p(z) as we show in Statement 1. Additionally,
as Θ is not known in practice, Tang et al. (2012) replace Θ with maximum likelihood estimate ˆΘ in the
deﬁnition of aggregated posterior Eq. (12). In Statement 1 we extend this idea to a Bayesian setting where
Θ and λ are not ﬁxed parameters but latent variables themselves.

1, . . . , z∗

, . . . , xobs

1

Tang et al. (2012) started with a simple mixture of factor analysers (MFA), and observed that the
aggregated posterior for a latent component often doesn’t match the N (0, I) prior. By replacing the
prior for a component with another MFA model, they constructed a deep MFA model. The idea of
the aggregated posterior (although not the name) can be traced back e.g. to Hinton et al. (2006), where
in deep belief nets the idea was that the posterior distribution of the latents of a restricted Boltzmann
machine (RBM) could be modelled by another RBM.

4 Examples

In this section, we provide three examples of model criticism and extension in the latent space. First, we
explore a factor analysis model in the context of image compression (§4.1). The objective of this example
is to show how the model can be criticised in the latent space as well as in the observation space. Our
analysis leads to changing the latent distribution from a single Gaussian to a scale mixture of Gaussians,
which captures both the marginal and the joint structure of the latent space, and improves the model in
the observation space as well.

Next, we explore a linear dynamical system model (§4.2) in the context of modelling time series. We
show that model criticism in latent space allows us to interrogate not only the standard “innovations”
(deﬁned in (20)), but also the latent residuals (deﬁned in (19)).

Finally, we explore a Gaussian process model (§4.3) in the context of modelling time series. The
objective of this example is to show when model criticism in the latent space can be a natural choice
whereas model criticism in the observation space can be difﬁcult. Our analysis leads to changing the
covariance function from squared exponential to a combination of periodic and squared exponential
kernels.

We implemented all models (except the Gaussian process model) in JAGS (Plummer, 2003), keeping a
single sample in the MCMC run after discarding a burn-in of 1000 samples (10,000 samples for §4.2). Note
that for model criticism in the latent space, we need only a single sample. We summarize the aggregation
process and corresponding reference distributions used in this section in Table 3.

4.1

Image patch data

The Berkeley Segmentation Database (Martin et al., 2001) consists of 200 training images. Following
Zoran and Weiss (2012), we convert the images to greyscale, extract 8 × 8 randomly located patches,
and remove DC components from all image patches9. We extract 50,000 image patches, and ﬁt different

9https://people.csail.mit.edu/danielzoran/NIPSGMM.zip

8

Model

xobs

U

APS

reference distribution

MF (13)

{xobs

i }n

i=1

{zi}n
{zi}n

i=1, Θ, b, τ, τz for (14)-(15)
i=1, Θ, b, τ, π, τ for (16)

{z∗
k1 i, z∗

ki}n,K
k2 i)}n,K

{(z∗

i=1,k=1, and

i=1, k1 ,k2 =1,k1 (cid:54)=k2

LDS
(17), (18)

{(x, y,

cos(ν), sin(ν))obs

t }n

t=1

t=1, {zt}n

t=1, A(1), . . . ,
{st}n
A(S), Q(1), . . . , Q(S), B, R

GP (9)

{(xi, yi)obs}n

i=1

{z∗

i }n

i=1 from (21)

σ2
f , l, τ for (22)
σ2
f , p, lp, ld for (22) and (23)
σ2
f , f , lp, ld, σ2

f s, ls, σ2

f l , ll

for (22) (large and small) and (23)

t=2 from (19) and
t=2 from (20) ∀ j, k

{ ˜z∗
{ ˜x∗
kt, ˜z∗
kt, ˜x∗

kt}n
jt}n
k(t+1))}n−1
k(t+1))}n−1

{( ˜z∗
{( ˜x∗

t=2 from (19) and
t=2 from (20) ∀ j, k

N (0, τ∗
z

−1), and
−1I2) for (14)
N (0, τ∗
z
L(0, τ∗
z ), and
z ) L(z2; 0, τ∗
mN (0, τ∗
m

L(z1; 0, τ∗
∑m π∗

z ) for (15)

−1), and
−1I2) for (16)

∑m π∗

mN (0, τ∗
m
N (0, 1)
N (0, 1)
N (0, I2)
N (0, I2)

N (0, 1)

Table 3: The table summarizes observed data xobs, unknown variables U, aggregated posterior sample(s)
(APS), and corresponding reference distribution(s) (as elaborated in Algorithm 1) for three models
discussed in §4, and different scenarios within each model.

matrix factorization models of the form:

b ∼ N (0, I), τ ∼ Gamma(α, β), θjk ∼ N (0, 1)
zi ∼ LatentDist, xi ∼ N (Θzi + b, τ

−1I)

(13a)

(13b)

with K = 16 latent dimensions (> 82% explained variance in PCA). Previously Zoran and Weiss (2012)
used full covariance zero-mean Gaussians (τ = 0, K = 64, b = 0). We set α = β = 0.001.

We start by assuming a Gaussian distribution for the latent model

τz ∼ Gamma(α, β), z ∼ N (0, τz

−1),

(14, Gaussian)

and generate a sample (Z∗, Θ∗, b∗, τ∗, τ∗
z ) from the posterior. To criticise the model, we aggregate the
, z∗
ki} ∀k, i, and bivariate samples {(z∗
univariate posterior samples {z∗
)} ∀k, i1 (cid:54)= i2. If the observed
ki2
ki1
−1)
data follows the model, then the distributions of the corresponding APSs are univariate normal N (0, τ∗
z
−1I2) respectively. We observe that neither of the APSs follow the expected
and bivariate normal N (0, τ∗
z
prior distribution. Also, the marginal distribution is more concentrated around zero than the expected
distribution, whereas the joint distribution shows heteroscedasticity (Figure 2, left column) which is
inconsistent with the factorized bivariate normal prior.

An alternative latent variable model for the factor analysis model is (See Table 2)

z ∼ L(0, τz).

(15, Laplace)

We use the same aggregation strategy as before, and compare the empirical distributions with the uni-
variate distribution L(0, τ∗
z ) respectively. We observe
similar characteristics in the aggregated posterior as before (Figure 2, middle column).

z ) and bivariate distribution L(z1; 0, τ∗

z ) L(z2; 0, τ∗

To accommodate these observations, we allow a scale mixture of Gaussian distributions as used by

Wainwright and Simoncelli (2000) with 8 components for the latent variable

π ∼ Dir(1), τm ∼ Gamma(α, β), z ∼

πmN (0, τm

−1I).

(16, Scale Mixture of Gaussians)

1 , . . . , π∗

We generate sample (π∗
8 , τ∗
8 ) as well. We assess the same aggregated distributions as
−1I2) respectively. We observe that
before and compare them with ∑m π∗
the empirical marginal distribution follows the mixture distribution well, although a KS test rejects
the hypothesis that the aggregated posterior follows the mixture distribution. Additionally, the joint
distribution captures the heteroscedasticity in the latent space (Figure 2, right column).

−1), and ∑m π∗

mN (0, τ∗
m

mN (0, τ∗
m

1 , . . . , τ∗

We also show the eigenvectors of the corresponding loading matrix for each of the three cases (Figure
2 bottom row). We show the eigenvectors rather than the loading matrix themselves since for the

8
∑
m=1

9

(a) Gaussian (14)

(b) Laplace (15)

(c) Scale Mixture of Gaussians (16)

Figure 2: Aggregated posterior [agg. post] and associated prior at the latent node for (left to right) the
Gaussian, Laplace and Scale mixture of Gaussian models of image patches. Top: ECDF of aggregated
posterior samples (16 × 50, 000 samples) and CDF of respective prior. Middle: conditional mean and
standard deviation of the bivariate aggregated posterior samples (16 × 15 × 50, 000 ÷ 2 samples, over
100 bins) and respective prior distribution. Bottom: eigenvectors of respective loading matrices.

Gaussian and Gaussian scale mixture, the columns of the loading matrix may not correspond to any
particular pattern due to rotational invariance. We observe that all three loading matrices span a similar
space.

The matrix factorization model can be criticised in the observation space with established image
statistics as a discrepancy measure. This, however, requires generating replicate data of the same size
as the observed data, which in this case is computationally extensive since Xobs ∈ R64×50,000. To avoid
generating multiple replicates, i.e., matrices Xrep
r ∈ R64×50,000, we only generate a single replicate for
each latent distribution choice and compare them the observed data.

For all three cases, i.e., Gaussian, Laplace, and Scale Mixture of Gaussians, we generate latent samples
zrep
z (and τ, π∗ for Scale Mixture of Gaussians). We use the rest of the ﬁtted
from the ﬁtted parameters τ∗
i
parameters, i.e., Θ∗, τ∗, and b∗ to generate samples xrep
. We generate 50,000, 8 × 8 replicate
image patches, and compare the observed and replicate data in terms of the distribution of raw pixel
values. We show the results in Figure 3. We observe that the distribution of the image pixel values in
the replicate data follows the observed data more closely for Scale Mixture of Gaussians than the other
latent distributions. However, it is not a perfect ﬁt, and that tells us that this model can improved further;
potentially by increasing K, and varying the noise characteristics such as using a full diagonal covariance.

from zrep

i

i

10

Figure 3: Empirical cumulative distribution functions of raw pixel values on observed and replicate
data for varying different distributions.

4.2 Honey bee data

The honey bee data consists of measurements of (x, y) coordinate and head angle (ν) of 6 honey bees. The
measurements are usually translated into a 4-dimensional multivariate time series (x, y, cos(ν), sin(ν)),
and modelled using a switching linear dynamical system to capture three distinct dynamical regimes,
namely, left turn, right turn and waggle (Oh et al., 2008). We follow this strategy, and model each time
series by a switching linear dynamical system (SLDS) (Fox et al., 2009) as follows:

s1 = 1, z1 ∼ N (0, I)
st ∼ Cat(π(st−1))
zt ∼ A(st)zt−1 + (cid:101)t, (cid:101)t ∼ N (0, Q(st) −1)
xt ∼ Bzt + ψt, ψt ∼ N (0, R−1)

∀t = 2, . . . , n

∀t = 2, . . . , n

∀t = 1, . . . , n

where st can be in one of {1, . . . , S} states. We assume that Q(·) (for each state) and R are diagonal
matrices with Gamma(α, β) prior over nonzero entries, entries of A(·) (for each state) and B originate
from Gaussian distribution, and π(·) (for each state) follow a Dirichlet distribution, i.e.,

−1), b·· ∼ N (0, τB

(·)
·· ∼ N (0, τA
a
τA ∼ Gamma(α, β), τB ∼ Gamma(α, β)
π(·) ∼ Dir(1).

−1)

We group s = {si}n

i=1 and Z = {zi}n

i=1. We set α = β = 0.001.

We ﬁt two models with S = 1 (standard linear dynamical system), and S = 3, both with a 4 dimen-
sional latent space. We generate a posterior sample (s∗, Z∗, A(1)∗, . . . , A(S)∗, Q(1)∗, . . . , Q(S)∗, B∗, R∗), and
aggregate the standardized latent residuals

˜zt = (Q(s∗

t )∗)0.5(z∗

t − A(s∗

t )∗z∗

t−1) ∀ t = 2, . . . , n,

and observation residuals (or innovations)

˜xt = (R∗)0.5(xobs

t − B∗z∗

t ) ∀ t = 2, . . . , n.

11

(17a)

(17b)

(17c)

(17d)

(18a)

(18b)

(18c)

(19)

(20)

Figure 4: Left: ECDF of aggregated posterior samples [agg. post.] (4 × n samples) for S = 1 and S = 3,
and CDF of prior N (0, 1) at the latent (top) and observation node (bottom). Right: segmentation of
honey bee sequence 6 as observed in s∗
n. Black markers indicate true change points. p-values
correspond to KS test.

1, . . . , s∗

For an LDS the standard approach to model criticism is to check that the innovations sequence is
zero-mean and white (see, e.g. (Candy, 1986, §5.1)), although this is usually carried out for known or
point-estimates of the parameters, not in a Bayesian setting. We use this check (extended to the SLDS
case) below, but also consider the latent residuals.

First, we focus on marginal structures ˜zkt and ˜xjt by pooling k = 1, 2, 3, 4 and j = 1, 2, 3, 4 together,
rather than the 4-dimensional vectors themselves as shown in Figure 4 (left). We expect that the APSs
would deviate from normality more (lower p-value) when S = 1, compared to S = 3, and we observe
this to be true for all honey bee sequences except 2. For sequences 4–6, the latent segmentations of the
SLDS in terms of (s∗
n) agree with the ground truth well; we present the 6th sequence in Figure 4
(right). For sequences 1–3, we observe that the segmentations are rather poor, similar to the results in
(Fox et al., 2009, §5).

1, . . . , s∗

Next, we focus on the joint structures in the temporal domain by pooling, ( ˜xj1t, ˜xj2t) ∀j1, j2 = 1, 2, 3, 4
and j1 (cid:54)= j2, ( ˜zkt, ˜zk(t+1)) ∀k = 1, 2, 3, 4, and ( ˜xjt, ˜xj(t+1)) ∀j = 1, 2, 3, 4. We expect that this APSs would
deviate from the reference distribution N (0, I2) more for S = 1 than for S = 3. We compute the
correlation coefﬁcients, and observe the respective p-values for S = 1 and S = 3 (Rahman, 1968).
We observe that for S = 1, the models are rejected either in the latent domain or in the observation
domain except for sequence 3, while for S = 3, the models are rejected either in the latent domain or the
observation domain for sequences 2 and 3 only. In other words, for sequences 1, 4, 5 and 6, the model
improves for S = 3, whereas for sequence 2 it fails to improve, and for sequence 3 it degrades for S = 3.
These observations can again be attributed to the poor segmentation for sequences 1-3. We show the
corresponding aggregated posteriors in the latent and observation space for sequence 6 in Figure 5. We
observe that the residuals in both latent and observation space display correlations for S = 1 while these
is reduced considerably for S = 3.

12

Figure 5: Scatterplot of bivariate aggregated posterior samples (607 × 4 samples) for sequence 6. The
black line shows the best linear ﬁt, whereas the dotted line shows the expected ﬁt in the absence of
(serial) correlation. The values in each plot are the correlation (r) and respective p-value (p).

4.3 Carbon emission data

The CO2 emission dataset10 comprises monthly average atmospheric carbon concentration yi (in parts
per million) between 1958 and 2017 (707 measurements after removing missing values). Rasmussen and
Williams (2006, §5.4.3) show that this time series can be modelled well by a combination of 4 standard
covariance functions involving 10 hyperparameters (and an additional parameter to model the additive
white noise). Each covariance function is introduced to model a speciﬁc aspect of the signal, e.g., a
squared exponential kernel to model the long term trend, a decaying periodic kernel to model the
seasonal variation, a rational quadratic kernel to model the short term irregularities, and another squared
exponential kernel to model the residual correlated noise. We show below how model criticism and
extension can be used to justify the use of covariance functions representing similar aspects of the data.
Following Rasmussen11 we use the measurements up to year 2004 (543) as training data and the
rest (164) as testing data12. We remove the mean of the training data before modelling, and use a zero
−1 to keep
mean function, i.e., m(x) = 0 or m = 0. We use Gamma(α, β) (See Table 2) priors over ζ and τ
them positive. We use the GPstuff toolbox (Vanhatalo et al., 2013) to generate MCMC samples, and
initialize the sampler at the maximum likelihood (ML) solution obtained using GPML toolbox13. We
set the parameters α and β such that the mean of the prior distribution is at the ML solution, and the
variance is equal to the mean. We generate a posterior sample (ϑ∗, ζ∗, τ∗} and aggregate the standardized
projections

z∗ = Λ∗ −1/2U∗(cid:62)(y − m∗)

where U∗ and Λ∗ are the eigenvectors and eigenvalues of kernel matrix K∗ such that K∗
τ∗ −1δ(xi, xj), and m∗ = 0 by design.

We ﬁrst model the time series with the squared exponential or Gaussian kernel,

ij = κ(xi, xj | ζ∗) +

(21)

(22)

κse(x, x(cid:48) | ζ) = σ2

f exp

(cid:18)

−

(x − x(cid:48))2
2l2

(cid:19)

f is the signal variance, i.e., ζ = {σ2

where l is the length scale and σ2
f , l}. We obtain ζML = (188, 0.30) and
ζ∗ = (197, 0.29). We present the ﬁtted data along with unstandardized and standardized projections in
Figure 614. The ﬁgure shows that the Gaussian kernel fails to model the time series as the prediction
quickly falls to the mean of the training signal and KS-test p-value = 4 × 10−10. We observe that most
of the signal strength (ci’s) is concentrated at lower frequencies (corresponding to large eigenvalues

10ftp://ftp.cmdl.noaa.gov/ccg/co2/trends/co2_mm_mlo.txt
11http://learning.eng.cam.ac.uk/carl/mauna/
12We do not use testing data for model criticism but to show the goodness of ﬁt visually.
13http://www.gaussianprocess.org/gpml/code/matlab/doc/
14It is also possible to model this time series with a large length scale, i.e., ζ = (1958, 31) but this has lower marginal likelihood

exp(−1198) as opposed to exp(−753).

13

(a) SE

(b) Periodic

(c) Peri. + SE(s) + SE(l)

Figure 6: Latent values of the Gaussian process model for CO2 emission dataset. Top: Original and ﬁtted
signal. Training and testing sets are separated by a gray line. Middle: Unnormalized projections c∗
i ’s
for i = 1, . . . , n, and the respective 95% conﬁdence interval ±1.96λ1/2
. We only show values for which
i > 2τ∗ −1. y-axis has been transformed by sgn(y)|y|0.3 to show small values. Bottom: ECDFs of aggregated
λ∗
posterior samples [agg. post.] of the normalized projections zi’s and CDF of prior distribution N (0, 1).
p-values correspond to KS-test.

i

λi∈{1,5}). The respective eigenvectors correspond to an upward trend. Also, a relatively high strength is
observed at eigenvalues 92–93. The respective eigenvectors correspond to sinusoids of frequency ∼1
year (see Figure 7a) which indicates a potential need of a periodic covariance function to model this data.
To tackle this, we use the decaying periodic function to model the time series, (Rasmussen and

Williams, 2006, §5.4.3)

κpe(x, x(cid:48) | ζ) = σ2

f exp

(cid:32)

−

2 sin2(π(x − x(cid:48))/p)
l2
p

(cid:33)

(cid:32)

exp

−

(cid:33)

(x − x(cid:48))2
2l2
d

(23)

where p is the period of the covariance function. Therefore, ζ = (σ2

f , p, lp, ld). We obtain ζML =

14

(a) SE

(b) Periodic

Figure 7: Weighted eigenfunctions of carbon emission dataset. The kinks in the plot appear due to the
short length scale.

(283, 1, 5.13, 5.86), and ζ∗ = (385, 1, 4.88, 6.09). We observe that this provides a better ﬁt than squared
exponential kernel (p-value 0.08). Although the KS-test fails to reject the ﬁtted model (perhaps due to
lack of samples), we observe that the signal strengths (ci’s) still deviate from their expected values. In
particular, the second, fourth and sixth projections show relatively high values compared to third, ﬁfth
and seventh. The signal ∑i∈{2,4,6} ciui corresponds to an upward trend, which corroborates the need to
model the trend further. See Figure 7b. Note that although the CO2 data is a time-series, the analysis
of the c-samples (see Eq. 10) does not depend on this, and can also be used where the input-space is
multi-dimensional.

f , p, lp, ld, σ2

To accommodate the upward trend, we introduce a squared exponential kernel with a relatively
large length scale. However, to avoid modelling small scale variations with the same kernel, we use
combination of two squared exponential kernels with two different length scales. Therefore, ζ =
(σ2
f l, ll) where the last four parameters belong to the two squared exponential kernels
with small (s) and large (l) length scales. We obtain ζML = (4.37, 1, 1.78, 74.60, 0.81, 0.92, 4132, 27.14), and
ζ∗ = (2.25, 1, 1.24, 73.88, 0.32, 0.66, 4095, 32.25). We observe that this improves the ﬁt even further, both in
terms of the testing data (visually) and in terms of unstandardized projections. The KS-test uses more
samples, and still fails to reject the model (p-value 0.55).

f s, ls, σ2

Model criticism of Gaussian processes in the observation space has been discussed by Lloyd and
Ghahramani (2015). However, their approach is different from the standard posterior predictive check
since the authors use hold-out data rather than using the observed data twice. Although this approach
shows if the response on hold-out data is different for the ﬁtted model, it does not necessarily point out
how the model can be extended.

One could generate a replicate sample from yrep ∼ p(· | ζ∗, Xobs), and compare yrep and yobs as for
a posterior predictive check. However, note that in this case yrep will be an independent draw from the
GP with parameters ζ∗ and input locations Xobs, hence it could look very different from yobs—this is
why Lloyd and Ghahramani (2015) make use of held-out data. Also, it would be difﬁcult to come up
with a suitable discrepancy function in this case. One could consider the χ2 discrepancy15, i.e., y(cid:62)K−1y.
However, this quantity is ﬁtted when sampling the kernel parameters ζ, and is also (as discussed above)
dominated by the noise for small eigenvalues of K. Other discrepancy measures could be investigated,
but exploring these alternatives is beyond the scope of this paper.

5 Discussion

Model criticism explores the discrepancies between a statistical model P(X, U) and observed data xobs.
This is often achieved by generating replicate observations Xrep ∼ P(X | xobs) from the ﬁtted model,

15Inspired by Gelman et al. (1996, Eq. (8))

15

and investigating which aspects D(X, U) of the replicated observations do not match the observed
data. Instead here we have focused on pulling the effect of the data back into the latent space, and
investigating if the posterior sample u∗ ∼ P(U | xobs) follows the prior distribution P(U), as it should do
by Statement 1 if the data were generated by the model. This is tested by aggregating related variables
with the same prior distribution and comparing them with the associated prior.

It should be noted that model criticism is not used to judge if a model is right or wrong. On the
contrary, it is widely accepted that all models are wrong but some are useful (Box and Draper, 1987, p.
424). Model criticism aims at understanding the limitations of the model with the hope that a better
model can be found, e.g., since all models are basically simpliﬁcations of a more complex process, model
criticism inspects if the simpliﬁcation is meaningful, or if the statistical assumptions made are reasonable.
Following this principle, we have discussed four examples of model criticism in latent space. We have
shown that by analysing the distribution of the aggregated posterior, a model can be extended so that
the aggregated posteriors follow the respective prior distributions better.

We thank the anonymous referees for pointing out the work of Johnson (2007) and Yuan and Johnson
(2012), and for comments which helped to improve the paper.

Acknowledgements

References

M. J. Bayarri and James O. Berger. p-values for Composite Null Models. Journal of the American Statistical

Association, 95(452):1127–1142, December 2000. 3, 4

T. R. Belin and D. B. Rubin. The Analysis of Repeated-Measures Data on Schizophrenic Reaction Times

using Mixture Models. Statistics in Medicine, 14(8):747–768, 1995. 4

G. E. P. Box and N. R. Draper. Empirical Model-Building and Response Surfaces. Wiley, 1987. 16

G. E.P Box. Sampling and Bayes’ Inference in Scientiﬁc Modelling and Robustness. Journal of the Royal

Statistical Society, 143(4):383–430, 1980. 3

T. S. Breusch and A. R. Pagan. A simple test for heteroscedasticity and random coefﬁcient variation.

Econometrica, 47(5):1287–1294, 1979. 7

R. P. Buccigrossi and E. P. Simoncelli. Image Compression via Joint Statistical Characterization in the

Wavelet Domain. IEEE Transactions on Signal Processing., 8(12):1688–1701, 1999. 7

J. V. Candy. Signal Processing: The Model Based Approach. McGraw-Hill, 1986. 12

S. R. Cook, A. Gelman, and D. B. Rubin. Validation of software for bayesian models using posterior

quantiles. Journal of Computational and Graphical Statistics, 15(3):675–692, 2006. 7

J. Durbin and G. S. Watson. Testing for serial correlation in least squares regression: I. Biometrika, 37(3/4):

409–428, 1950. 7

E. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky. Nonparametric Bayesian Learning of Switching
Linear Dynamical Systems. In Advances in Neural Information Processing Systems 21, pages 457–464.
2009. 11, 12

A. Gelman, X. Meng, and H. Stern. Posterior predictive assessment of model ﬁtness via realized

discrepancies. Statistica Sinica, pages 733–807, 1996. 4, 15

A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman and Hall, London,

2004. Second edition. 1

P. Gopalan, J. M. Hofman, and D. M. Blei. Scalable recommendation with hierarchical poisson factor-
ization. In Proceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence, UAI, pages
326–335, 2015. 4

16

G. E. Hinton, S. Osindero, and Y. W. Teh. A Fast Learning Algorithm for Deep Belief Nets. Neural

Computation, 18:1527–1554, 2006. 8

doi: 10.1214/07-BA229. 7, 16

V. E. Johnson. Bayesian model assessment using pivotal quantities. Bayesian Anal., 2(4):719–733, 12 2007.

J. R Lloyd and Z. Ghahramani. Statistical Model Criticism Using Kernel Two Sample Tests. In Advances

in Neural Information Processing Systems, 2015. 4, 15

D. Martin, C. Fowlkes, D. Tal, and J. Malik. A Database of Human Segmented Natural Images and its
Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics. In Proceedings
of 8th International Conference on Computer Vision, volume 2, pages 416–423, 2001. 8

M. Meulders, A. Gelman, I. Van Mechelen, and P. De Boeck. Generalizing the Probability Matrix
Decomposition Model: an Example of Bayesian Model Checking and Model Expansion. In Hox, J. J.
and de Leeuw, E. D., editor, Assumptions, Robustness and Estimation Methods in Multivariate Modeling.
TT-Publikaties, Amsterdam, 1998. 7

S. M. Oh, J. M. Rehg, T. Balch, and F. Dellaert. Learning and Inferring Motion Patterns using Parametric
Segmental Switching Linear Dynamic Systems. International Journal of Computer Vision, 77(1):103–124,
2008. 11

A. O’Hagan. HSSS Model Criticism. In P. J. Green, N. L. Hjort, and S. Richardson, editors, Highly

Structured Stochastic Systems, pages 422–444. Oxford University Press, 2003. 1, 4, 7

M. Plummer. JAGS: A Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling. In
Proceedings of the 3rd International Workshop on Distributed Statistical Computing (DSC 2003), 2003. 8

N. A. Rahman. A Course in Theoretical Statistics. Charles Grifﬁn and Company, 1968. 12

C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.

ISBN 026218253X. 6, 13, 14

O. Ratmann, C. Andrieu, C. Wiuf, and S. Richardson. Model criticism based on likelihood-free inference,
with an application to protein network evolution. Proceedings of the National Academy of Sciences, 106
(26):10576–10581, 2009. ISSN 0027-8424. doi: 10.1073/pnas.0807882106. 1

D. B. Rubin. Bayesianly Justiﬁable and Relevant Frequency Calculations for the Applied Statistician.

Annals of Statistics, 12:1151–1172, 1984. 3

R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte

Carlo. In Proceedings of the International Conference on Machine Learning, volume 25, 2008. 1

Y. Tang, R. Salakhutdinov, and G. E. Hinton. Deep Mixtures of Factor Analysers. In Proceedings of the

29th International Conference on Machine Learning, 2012. 7, 8

J. Vanhatalo, J. Riihim¨aki, J. Hartikainen, P. Jyl¨anki, V. Tolvanen, and A. Vehtari. GPstuff: Bayesian
modeling with Gaussian processes. J. Mach. Learn. Res., 14(1):1175–1179, April 2013. ISSN 1532-4435.
13

M J Wainwright and E P Simoncelli. Scale Mixtures of Gaussians and the Statistics of Natural Images. In

Advances in Neural Information Processing Systems, volume 12, pages 855–861, 2000. 9

H. White. A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedas-

ticity. Econometrica, 48(4):817–838, 1980. 7

M. B. Wilk and R. Gnanadesikan. Probability plotting methods for the analysis of data. Biometrika, 55(1):

1–17, 1968. 7

(1):156 – 164, 2012. 2, 5, 7, 16

Y. Yuan and V. E. Johnson. Goodness-of-ﬁt diagnostics for Bayesian hierarchical models. Biometrics, 68

D. Zoran and Y. Weiss. Natural Images, Gaussian Mixtures and Dead Leaves. In F. Pereira, C. J. C.
Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25,
pages 1736–1744. Curran Associates, Inc., 2012. 8, 9

17

