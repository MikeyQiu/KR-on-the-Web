Graph Representation Ensemble Learning

Palash Goyal∗
University of Southern California
palashgo@usc.edu

Di Huang∗
University of Southern California
dh 599@usc.edu

Sujit Rokka Chhetri∗
University of California-Irvine
schhetri@uci.edu

Arquimedes Canedo*
Siemens Corporate Technology
arquimedes.canedo@siemens.com

Jaya Shree
University of Southern California
shree@usc.edu

Evan Patterson
Stanford University
epatters@stanford.edu

9
1
0
2
 
p
e
S
 
2
1
 
 
]
I
S
.
s
c
[
 
 
2
v
1
1
8
2
0
.
9
0
9
1
:
v
i
X
r
a

Representation learning on graphs has been gaining atten-
tion due to its wide applicability in predicting missing links,
and classifying and recommending nodes. Most embedding
methods aim to preserve certain properties of the original
graph in the low dimensional space. However, real world
graphs have a combination of several properties which are
difﬁcult to characterize and capture by a single approach. In
this work, we introduce the problem of graph representation
ensemble learning and provide a ﬁrst of its kind framework
to aggregate multiple graph embedding methods efﬁciently.
We provide analysis of our framework and analyze – theo-
retically and empirically – the dependence between state-of-
the-art embedding methods. We test our models on the node
classiﬁcation task on four real world graphs and show that
proposed ensemble approaches can outperform the state-of-
the-art methods by up to 8% on macro-F1. We further show
that the approach is even more beneﬁcial for underrepre-
sented classes providing an improvement of up to 12%.

Introduction
Graphs are used to represent data in various scientiﬁc ﬁelds
including social sciences, biology and physics (Gehrke,
Ginsparg, and Kleinberg 2003; Freeman 2000; Theocharidis
et al. 2009; Goyal, Sapienza, and Ferrara 2018). Such rep-
resentation allows researchers to gain insights about their
problem. The most common tasks on graphs are link predic-
tion, node classiﬁcation and visualization. For example, link
prediction in the social domain is used to determine friend-
ships between people. Node classiﬁcation in the biology do-
main is used to identify genes of proteins. Similarly, visu-
alization is used to identify communities and structure of
a graph. Recently, signiﬁcant amount of work has been de-
voted to learning low dimensional representation of nodes in
the graphs to allow the use of machine learning techniques to
perform the tasks on graphs. Graph representation learning
techniques embed each node in the network in a low dimen-
sional space, and map link prediction and node classiﬁcation
in the network space to a nearest neighbor search and vec-
tor classiﬁcation in the embedding space (Goyal and Fer-
rara 2018). Several of these techniques have showed state-
of-the-art performance on graph tasks (Grover and Leskovec

*These authors contributed equally to this work.

Copyright © 2019, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All
rights reserved.

Figure 1: Example motivating the need of ensemble learn-
ing. The graph represents interactions in classrooms (in red)
and family trees of students (in purple). Such complex com-
bination of community and structure requires combination
of multiple embedding methods for accuracy.

2016a; Ou et al. 2016a).

State-of-the-art techniques in graph representation learn-
ing deﬁne some characteristics of the graphs they aim to cap-
ture and deﬁne an objective function to learn these features
in the low-dimensional embedding. For example, HOPE (Ou
et al. 2016a) preserves higher order proximity between
nodes using the singular value decomposition of the sim-
ilarity matrix. Similarly, node2vec (Grover and Leskovec
2016a) captures the similarity of nodes using random walks
on the graph. However, real world graphs do not follow a
simple structure and can be layered with several categories
of properties with complex interactions between them. It has
been shown that no single method outperforms other meth-
ods on all network tasks and data sets (Goyal and Ferrara
2018). We further illustrate this by the example in Figure 1
with a social network from two classrooms (represented by
the pink color). We also show the family links of individ-
ual students in the classroom and represent family members
outside the classroom (represented by the the blue color).
Here, we consider the task of multi-label node classiﬁca-
tion with the classes classroom and role in family. This net-
work is complex and has both community and structural
properties. Methods such as HOPE (Ou et al. 2016a) which
preserve community can effectively classify the nodes into
classrooms but perform poorly on family links which follow

structure. On the other hand, structure preserving methods
can classify the role of an individual student in the family but
puts nodes in the same classroom into separate categories.

In this work, we introduce graph representation ensemble
learning. Given a graph and a list of methods capturing vari-
ous properties of the graph, we aim to learn a representation
of nodes which can combine embeddings from each method
such that it outperforms each of the constituent method in
terms of prediction performance. Ensemble methods have
been very successful in the ﬁeld of machine learning. Meth-
ods such as AdaBoost (R¨atsch, Onoda, and M¨uller 2001) and
Random Forest (Liaw, Wiener, and others 2002) have shown
to be much more accurate than the individual classiﬁers that
compose them. It has been shown that combining even the
simplest but diverse classiﬁers can yield high performance.
However, to the best of our knowledge, no work has focused
on ensemble learning on graph representation learning.

Here, we formally introduce ensemble learning on graph
representation methods and provide a framework for it. We
ﬁrst provide a motivation example to show that a single em-
bedding approach is not enough for accurate predictions on
a graph task and combining methods can yield improvement
in performance. We then formalize the problem and deﬁne
a method to measure correlations of embeddings obtained
from various approaches. Then, we provide an upper bound
on the correlation assuming certain properties of the graph.
The upper bound is used to establish the utility of our frame-
work. We focus our experiments on the task of node classiﬁ-
cation. We compare our method with the state-of-the-art em-
bedding methods and show its performance on 4 real world
networks including collaboration networks, social networks
and biology networks. Our experiments show that the pro-
posed ensemble approaches outperform the state-of-the-art
methods by 8% on macro-F1. We further show that the ap-
proach is even more beneﬁcial for underrepresented classes
and get an improvement of 12%.

Overall, our paper makes the following contributions:

1. We introduce ensemble learning in the ﬁeld of graph rep-

resentation learning.

2. We propose a framework for ensemble learning given a

variety of graph embedding methods.

3. We provide a theoretical analysis of the proposed frame-
work and show its utility theoretically and empirically.

4. We demonstrate that combining multiple diverse methods
through ensemble achieves state-of-the-art accuracy.
5. We publish a library, GraphEnsembleLearning 1, imple-
menting the framework for graph ensemble learning.

Related Work
Methods for graph representation learning (aka graph em-
bedding) typically vary in properties preserved by the ap-
proach and the objective function used to capture these prop-
erties. Based on the properties, embedding methods can
be divided into two broad categories: (i) community pre-
serving, and (ii) structure preserving. Community preserv-

1https://github.com/dihuang0220/GraphEnsembleLearning

ing approaches aim to capture the distances in the orig-
inal graph in the embedding space. Within this category,
methods vary on the level of distances captured. For exam-
ple, Graph Factorization (Ahmed et al. 2013a) and Lapla-
cian Eigenmaps (Belkin and Niyogi 2001) preserve shorter
distances (i.e., low order proximity) in the graph, whereas
more recent methods such as Higher Order Proximity Em-
bedding (HOPE) (Ou et al. 2016a) and GraRep (Cao, Lu,
and Xu 2016) capture longer distances (i.e., high order prox-
imity). Structure preserving methods aim to understand the
structural similarity between nodes and capture role of each
node. node2vec (Grover and Leskovec 2016a) uses a mixture
of breadth ﬁrst and depth ﬁrst search for this. Deep learn-
ing methods such as Structural Deep Network Embedding
(SDNE) (Wang, Cui, and Zhu 2016a) and Deep Network
Graph Representation (DNGR) (Cao, Lu, and Xu 2016) use
deep autoencoders to preserve distance and structure.

Based on the objective function, embedding methods can
be broadly divided into two categories: (i) matrix factoriza-
tion, and (ii) deep learning methods. Matrix factorization
techniques represent graph as a similarity matrix and de-
compose it to get the embedding. Graph Factorization and
HOPE use adjacency matrix and higher order proximity ma-
trix for this. Deep learning methods, on the other hand, use
multiple non-linear layers to capture the underlying mani-
fold of the interactions between nodes. SDNE, DNGR and
VGAE (Kipf and Welling 2016b) are examples of these
methods. Some other recent approaches use graph convo-
lutional networks to learn graph structure (Kipf and Welling
2016a; Bruna et al. 2013; Henaff, Bruna, and LeCun 2015).

In machine learning, ensemble approaches (Zhou 2012)
are algorithms which combine the outputs of a set of classi-
ﬁers. It has been shown that ensemble of classiﬁers are more
accurate than any of its individual members if the classi-
ﬁers are accurate and diverse (Hansen and Salamon 1990).
There are several ways individual classiﬁers can be com-
bined. Broadly, they can be divided into four categories: (i)
Bayesian voting, (ii) random selection of training examples,
(iii) random selection of input features, and (iv) random se-
lection of output labels. Bayesian voting methods combine
the predictions from the classiﬁers weighted by their con-
ﬁdence. On the other hand, methods such as Random For-
est (Liaw, Wiener, and others 2002) and Adaboost (R¨atsch,
Onoda, and M¨uller 2001) divide the training data into mul-
tiple subsets, train classiﬁers on each individual subset, and
combine the output. The third category of approaches di-
vide the input set of features available to the learning algo-
rithm (Opitz 1999). Finally, for data with a large number of
output labels, some methods divide the set of output labels
and learn individual classiﬁers to learn their corresponding
label subset (Ricci and Aha 1997).

In this work, we extend the concept of ensemble learning
to graph representation learning and get insights into the cor-
relations between various graph embedding methods. Based
on this, we propose ensemble methods for them and show
the improvement in performance on node classiﬁcation task.

Motivating Example
This section presents a motivational case study to highlight
the effectiveness of the proposed graph representation en-
semble learning on a synthetic dataset. We present the anal-
ysis by utilizing four synthetic graphs: (a) Barabasi-Albert,
(b) Random Geometry (c) Stochastic Block Model, and (d)
Watts Strogatz graph (see Figure 2). Each of these graphs ex-
hibits a speciﬁc structural property. We use a spring layout
to further elucidate the difference in the structural properties
of the four different synthetic graphs. The Barabasi-Albert
graph makes new connections through preferential attach-
ment using the degree of the existing nodes. Watts Strogatz
graph generates a ring of n graphs with the addition of edges
of each nodes with its k neighbors. Stochastic Block Model
creates community clusters by preserving the community
structure. The Random Geometry graph generates n nodes
and add m edges by utilizing the spatial proximity among
the nodes as a measure.

Figure 2: Four synthetic graph with different graph proper-
ties (with node color representing different degrees) initially
drawn using the spring layout.

We have generated each of the synthetic graphs with 100
nodes each. As mentioned earlier, different embedding algo-
rithms such as Graph Factorization, Laplacian Eigenmaps,
High Order Proximity Preserving, Structural Deep Network
Embedding and Node2vec capture various characteristics of
the graphs. Hence, a single embedding algorithm may not
be able to capture the entire complex interaction. To test this
hypothesis we have created two node labels for the synthetic
graph. The ﬁrst label is based on the degree of the graph,
whereas the second label is based on the closeness centrality
measure (Freeman 1978) of the graph. The centrality values
are binned and the respective bins are used as node labels.

To simulate the interaction between different synthetic
graphs, we have randomly selected node pairs (equal to 40%
of the total number of nodes) and added edges between them
(with a probability threshold of 0.3). The addition of the
edges are shown in Figure 5.

The result of the node classiﬁcation for the degree labels
of the merged synthetic graph is shown in Table 1. The em-

(a) Original layout showing added edges

(b) New spring layout

Figure 3: Synthetic graphs merged together (different colors
represents the updated node degree).

Methods
gf
lap
hope
sdne
node2vec
sdne, node2vec
hope,gf,lap

Dimensions
128
32
128
64
128

Macro-F1
0.127
0.055
0.157
0.177
0.128

128,64,32,64,64

0.183(3.4%)

Table 1: Ensemble methods on motivating example with de-
gree labels.

.

bedding obtained from the state-of-the-art methods and the
ensemble approach is utilized to predict the degree labels. It
can be seen that compared to the state-of-the-art algorithms,
the ensemble based approach is able to achieve 3.4% im-
provement in macro F1 score. Although not signiﬁcant, it is
still able to improve the classiﬁcation accuracy.

Methods
gf
lap
hope
sdne
node2vec
sdne, node2vec
gf,hope,lap

Dimensions
64
32
64
128
128

Macro-F1
0.108
0.064
0.090
0.191
0.142

128,64,64,32,128

0.215(12.6%)

Table 2: Ensemble methods on motivating example with
centrality labels.

The classiﬁcation accuracy results for classifying the cen-
trality measures are shown in Table 2. For this label, it can be
observed that the ensemble based method is able to achieve
12.6% improvement in macro F1-score. Both the macro F1-
score proves that the ensemble based approach are able to
utilize the best characteristic of different graph embedding
algorithm’s ability to capture the structure of the network.

Graph Representation Ensemble Learning

In this section, we deﬁne the notations and provide the graph
ensemble problem statement. We then explain multiple vari-
ations of deep learning models capable of capturing tempo-
ral patterns in dynamic graphs. Finally, we design the loss
functions and optimization approach.

Notations

We deﬁne a directed graph as G = (V, E), where V is the
vertex set and E is the directed edge set. The adjacency ma-
trix is denoted as A. We deﬁne the embedding matrix from
a method m as X m. The embedding matrix can be used to
reconstruct the distance between all pairwise nodes in the
graph. We denote this as Sm, in which Sm
j,.(cid:107).

i,j = (cid:107)X m

i,. − X m

Problem Statement

In this paper, we introduce the problem of ensemble learn-
ing on graph representation learning. We deﬁne it as follows:
Given a set of embedding methods {m1, . . . , mk} with cor-
responding embeddings for a graph G as {X m1, . . . , X mk }
and errors {(cid:15)1, . . . , (cid:15)k} on a graph task T , a graph ensem-
ble learning approach aims to learn an embedding X m with
error (cid:15) such that (cid:15) < min((cid:15)1, . . . , (cid:15)k).

Measuring Graph Embedding Diversity

Different graph embedding techniques vary in the types of
properties of the graphs preserved by them and the model
deﬁned. Broadly, embedding techniques can be divided into:
(i) structure preserving, and (ii) community preserving mod-
els, deﬁned as follows:

Deﬁnition 1. (Community Preserving Models) It aims to
embed nodes with lower distance between them closer in
the embedding space.

Deﬁnition 2. (Structure Preserving Models) It aims to em-
bed structurally similar nodes closer in the embedding space.
As ensemble accuracy of a combination of methods de-
pends on the diversity of the input methods (Dietterich and
others 2002), we now establish bounds on the diversity of
embedding models. Graph embedding of a graph G is a ma-
trix X ∈ Rn×d where n is the number of nodes and d is
the dimension of the embedding. Thus, we require a diver-
sity measure which can quantify diversity between matrices.
Pearson correlation (Benesty et al. 2009) is a popular met-
ric traditionally used to measure diversity of two uni-variate
random variables. It can be generalized to multivariate case
and deﬁned as RV coefﬁcient (Robert and Escouﬁer 1976).
As RV Coefﬁcient measures linear dependence between
the variables and embedding methods can be non-linear in
construction, we can use a distance based metric to capture
such non-linearity between embeddings:

Deﬁnition 3. (Sz´ekely, Rizzo, and others 2009) (Distance
Covariance): Suppose that X and Y are matrices of centered
random vectors (column vectors). Let the n × n distance
matrices (aj,k) and (bj, k) containing all pairwise distances,
aj,k = (cid:107)Xj − Xk(cid:107) and bj,k = (cid:107)Yj − Yk(cid:107). We compute the
doubly centered distance matrices (Aj,k) and (Bj,k), where
Aj,k = aj,k − aj,. − a.,k + a.,. and Bj,k = bj,k − bj,. − b.,k +
b.,.. The distance covariance is deﬁned as follows:

dCov2(X, Y ) =

Aj,kBj,k.

1
n2

n
(cid:88)

n
(cid:88)

j=1

k=1

Deﬁnition 4. (Sz´ekely et al. 2007) (Distance Correlation):
The distance correlation between random variables X and
Y is given as follows:

dCor(X, Y ) =

dCov(X, Y )
(cid:112)dCov(X, X)dCov(Y, Y )

Based on this, we obtain the following bound:

Theorem 1. Consider two embedding methods m1 and m2
with corresponding embeddings for a graph G = (V, E)
as X m1 and X m2, where |V | = n. Let G have a set V1
of structurally similar nodes with |V1| = n1 and a set
V2 = V \ V1 with nodes in multiple communities. If m1 is a
purely structural preserving method and m2 preserves both
structural and community properties, then distance correla-
tion between the the embeddings has the following bound:

dCor(X m1 , X m2 ) < 1 −

n1
n

.

Proof. Let Sm1 and Sm2 denote the pairwise distance ma-
trices for methods m1 and m2, and S(cid:48)m1 and S(cid:48)m2 denote
their doubly centered versions. We now have,

dCov(X m1 , X m2 ) =

1
n2

(cid:88)

v,w∈V

v,w S(cid:48)m2
S(cid:48)m1
v,w

dCov(X m1 , X m1 ) =

1
n2

(cid:88)

(S(cid:48)m1

v,w )2

v,w∈V

(1)

(2)

We can divide the ﬁrst summation (eqn. 1) into four parts:

dCov(X m1 , X m2 ) =

(cid:88)

v,w∈V1
(cid:88)

1
n2

1
n2

+

v∈V2,w∈V1

v,w S(cid:48)m2
S(cid:48)m1

v,w +

1
n2

(cid:88)

v,w S(cid:48)m2
S(cid:48)m1
v,w

v,w S(cid:48)m2
S(cid:48)m1

v,w +

v,w S(cid:48)m2
S(cid:48)m1
v,w

v∈V1,w∈V2
1
n2

(cid:88)

v,w∈V2

As m2 preserves structural similarity, the distance be-
tween each pair of nodes in set V1 will be 0 yielding the ﬁrst
term of above equation 0. Also, since V1 and V2 do not have
a speciﬁed relation, the embedding distances by m1 and m2
will be randomly distributed and uncorrelated. Thus, the sec-
ond and third terms become 0. We can get similar results for
second summation (eqn. 2) as well. From this, we get

dCor(X m1 , X m2 ) =

(cid:80)

S(cid:48)m1
v,w S(cid:48)m2
v,w
(cid:113)(cid:80)

v,w∈V2
(S(cid:48)m1

v,w )2

(cid:113)(cid:80)

v,w∈V2

(S(cid:48)m2

v,w )2

v,w∈V2

As correlation between two variables is bounded by 1,

from the above we get

dCor(X m1 , X m2 ) ≤

(n − n1)2
n2
Also, n1 < n and thus n2

n2 < n1

1

n . We thus get

dCor(X m1 , X m2 ) < 1 −

n1
n

.

= (1 −

)2 = 1 +

n1
n

n2
1
n2 −

2n1
n

Corollary 1. For a graph G with s sets of structurally sim-
ilar nodes {V1 . . . Vk} with |Vi| = ni and embedding meth-
ods m1 and m2 preserving purely structural and both struc-
tural and community properties respectively, the distance
correlation bound is:

dCor(X m1 , X m2 ) < 1 −

s
(cid:88)

i=1

ni
n

.

Proof. The summation in Theorem 1, eqn. 2, can be broken
down into 2s parts and the rest follows as above.

Measuring Label Prediction Diversity
We have now established the upper bound on correlation be-
tween the embeddings. We also know the following about
predictions using Logistic Regression:
Theorem 2. Consider two sets of feature spaces for data
D represented as X ∈ Rn×d1 and Y ∈ Rn×d2 with labels
for individual data points as Z ∈ Rn. If logistic regression
models trained on (X, Z) and (Y, Z) obtain accuracy of aX
and aY respectively, then we have the following bound for
the model trained on (X (cid:107) Y, Z), where (cid:107) denotes concate-
nation operation:

aX(cid:107)Y ≥ max(aX , aY )
Proof. Without loss of generality, assume that aX > aY .
As logistic regression is an additive model, setting weights
of the model corresponding to Y would yield the accuracy
of the concatenated model aX .

From the above theorem, we note that adding embeddings
of method m2 on m1 would not decrease the performance.
Further, the equality in Theorem 2 is realized when Y is a
linear scaling of X or distances in Y are exactly correlated
with X. But from Theorem 1 we have an upper bound on
the correlation between the embeddings. Thus, we can get
aX(cid:107)Y > max(aX , aY ). Tighter bounds are left as a future
work.

Runtime Optimization Techniques
Given a set of k embedding methods {m1 . . . mk} with op-
timal hyperparameters {(cid:104)1 . . . (cid:104)k} and the maximum time
complexity from the methods as T per unit dimension, a
naive implementation of ﬁnding the optimal combination
of methods would take a time complexity of O(2k × T ×
d), where d is the embedding dimensionality. To optimize
this, we do an approximation by greedily adding the next
method’s embedding to the current set of embeddings. This
yields a time complexity of O(k × T × d).

Algorithm
Algorithm 1 provides the pseudo-code for the framework.
Given an input graph G, we split the graph nodes into train-
ing, validation and test. We then use the validation set to
get an accuracy score for each embedding method. Based
on this, we greedily add the next best embedding approach
to evaluate the performance of the ensemble of methods. Fi-
nally, we report the performance on a held-out test set. In the
experiments below the above step is performed 5 times and
the average is reported.

Algorithm 1: graphensemble
Function graphensemble (Graph G, Embedding
methods M = {m1, .., mK})

train, val, test ← splitNodeIndexes(G);
for i = 1 . . . K do

X mi ← getEmbedding(G, mi);
training(model, X mi[train]);
ai ← evaluate(model, X mi[val]);
sortedmethods ← Sort M based on a;
ensembleset ← set();
aprev ← 0;
for m ∈ sortedmethods do
ensembleset.add(m);
X ← concat(X mi ∀ mi ∈ ensembleset);
training(model, X[train]);
ae ← evaluate(model, X[val]);
if ae < aprev then

ensembleset.remove(m);

aprev ← ae;

X ← concat(X mi ∀ mi ∈ ensembleset);
training(model, X[train]);
atest ← evaluate(model, X[test]);
return atest

Dataset
PPI
BlogCatalog
Citeseer
Wikipedia

Nodes
3,890
10,312
3,312
4,777

Edges
38,839
333,983
4,660
92,512

Classes
50
39
6
40

Table 3: Statistics of benchmark datasets in the experiment

Experiments
In this section, we establish the Graph Ensemble approach
against ﬁve state-of-the-art baseline embedding methods to
evaluate their multi-label node classiﬁcation performance on
four benchmark datasets. In addition, we yield insights into
the correlation of graph embedding obtained by the different
methods.

Datasets
As Table 3 shows, we use four benchmark real-life graphs
for node classiﬁcation task in our experiment. For each
dataset, we derive the largest weakly connected component
from the original graph.

• Protein-Protein Interactions

(PPI)(Breitkreutz et al.
2008): This is a network of biological interactions be-
tween proteins in humans. This network has 3,890 nodes
and 38,739 edges.

• BlogCatalog(Tang and Liu 2009): This is a network of
social relationships of the bloggers listed on the Blog-
Catalog website. The labels represent blogger interests in-
ferred through the metadata provided by the bloggers. The
network has 10,312 nodes, 333,983 edges and 39 different
labels.

• Citeseer(Lu and Getoor 2003): This dataset consists of
3312 scientiﬁc publications classiﬁed into one of six
classes. The citation network consists of 4732 links.

• Wikipedia(Mahoney 2011): This is a cooccurrence net-
work of words appearing in the ﬁrst million bytes of the
Wikipedia dump. The labels represent the Part-of-Speech
(POS) tags inferred using the Stanford POS-Tagger. The
network has 4,777 nodes, 184,812 edges, and 40 different
labels.

Baseline Graph Embedding Methods

We compare our Graph Ensemble method with the following
ﬁve baseline graph embedding models.

• Graph Factorization (GF)(Ahmed et al. 2013b): It factor-

izes the adjacency matrix with regularization.

• Laplacian Eigenmaps (LAP)(Belkin and Niyogi 2002): It
preserves local information by projecting points into a
low-dimensional space using eigen-vectors of the graph.
• High Order Proximity Preserving(HOPE)(Ou et al.
2016b): It factorizes the higher order similarity matrix be-
tween nodes using generalized SVD.

• Structural Deep Network Embedding(SDNE)(Wang, Cui,
and Zhu 2016b): This uses deep auto-encoders to preserve
the ﬁrst and second order network proximities by using
non-linear functions to obtain the embedding.

• node2vec(Grover and Leskovec 2016b): It is an embed-
ding technique that uses random walks on graphs to obtain
node representations which preserves higher order prox-
imity between nodes.

Graph Ensemble Approach

Our graph representation ensemble learning mechanism
leverages a bag of single embedding methods and achieves
an optimal embedding combination for graph feature learn-
ing. First, we run single graph embedding methods on the
original graph to get the best embedding at each dimension.
Then, we use the greedy approximated search to add em-
bedding generated by other methods iteratively to the em-
bedding given by the best single method. In the end, we feed
the ensemble concatenation embedding and baseline method
embedding to the downstream multi-label node classiﬁca-
tion task. At each experiment round, we split the nodes of
a graph into training data (50%), validation data (20%) and
test data (30%). Using training data is intended to ﬁnd the
best hyperparamter for single methods. We choose the op-
timal ensemble embedding combination based on the val-
idation data. And we report the performance of our graph
ensemble methods and ﬁve baseline methods on test data.

Hyperparameter Search In order to get the best embed-
ding for each single graph embedding model, we employ a
best hyperparamter search on the training dataset. Among
three embedding dimensions 32, 64 and 128, we select the
best hyperparameter set respectively at each dimension. Ex-
cept for LAP which does not contain hyperparamters, we
use grid search on a range of hyperparameter sets for the

other four methods. For GF, we search parameters includ-
ing learning rate from {1e-3, 1e-2, 1e-1} and regulariza-
tion from {1e-1, 1, 10}. For HOPE, we select a decaying
factor from {1e-4, 1e-3, 1e-2, 1e-1} and similarity func-
tion from Katz Index, PageRank, Common Neighbours and
Adamic-Adar. For SDNE, we ﬁx the autoencoder structure
500, 1000, 300 nodes in each layer, and set ﬁrst loss function
parameter α to 1e-5 and penalty β to 10. We select two reg-
ularization factors and xeta from {1e-3, 1e-2} respectively.
As for Node2vec, we set walk length to 80, number of walks
to 10, context size to 10. We select return p and in-and-out q
from {0.25, 0.5, 1, 2, 4} respectively.

Ensemble Combination Search After obtaining the best
hyperparameter set for each method at each dimension, we
evaluate their performance on multi-label node classiﬁcation
task with validation dataset and select the optimal ensem-
ble combination. First, we choose the best method which
has best performance on the training data. We test its per-
formance on validation data under best setting in respect to
three dimensions 32, 64 and 128, and then select its best di-
mension based on Macro F1 score. Secondly, we append the
embedding of the second best method at three dimensions
separately to the best embedding so far and repeat the eval-
uation process. If the performance improves, we keep the
second embedding at the chosen dimension. Otherwise we
abandon this method and continue the appending process.
In the end, we will obtain the best combination iteratively
via such greedy approximation.

Embedding Correlation
The distance correlations between the embeddings obtained
by different embedding methods is presented in Figure 4. We
observe that the correlation between the embeddings varies
signiﬁcantly with the underlying data set. For PPI and Cite-
seer, we see that all methods are weakly correlated. This
strengthens our claim in Theorem 1 that embedding meth-
ods preserve different properties and if the underlying graph
is complex, then the embeddings will be diverse. For the
Wikipedia dataset we observe that Graph Factorization and
Laplacian Eigenmaps have a very high correlation. As they
both capture ﬁrst order proximity, the correlation may be be-
cause the ﬁrst order correlations in the Wikipedia dataset
may have a simple pattern easily visible to both these ap-
proaches. We also observe that SDNE which preserves ﬁrst
and second order proximity in a non-linear way also has
high correlations with GF and Lap further strengthening our
claim.

Multi-label Node Classiﬁcation
In the multi-label node classiﬁcation task, we are given a
graph as well as labels of a proportion of nodes as training
data. And we aim to predict the unknown labels for the rest
of nodes in the test data. Each node in the graph has one or
multiple labels. To evaluate the graph ensemble embedding
and baseline methods embedding, we utilize the same One-
Vs-the-Rest multi-label strategy and Logistic Regression by
default setting to build classiﬁers. To ensure the robustness
of our proposed graph ensemble methods and stability of

Figure 4: Distance correlations of embedding methods on real networks (dimensions set to 128).

the experiments, we repeat the whole process for 5 rounds
and report the average results. We use Macro F1 and Micro
F1 as evaluation metrics. Micro F1 has similar performance
like Macro F1 thus it is not reported in the paper. We care
more about the minority class prediction and Macro F1 is
preferably considered.

We summarize multi-label classiﬁcation results in Ta-
ble 4. Overall, we observe that the ensemble of methods out-
performs individual methods signiﬁcantly with the excep-
tion of Citeseer. node2vec gives highest accuracy for all data
sets except Wikipedia for which HOPE outperforms other
methods. Another key observation is that the optimal em-
bedding dimensionality for a method in an ensemble may be
different than the individual optimal. This can be attributed
to the interplay of embeddings when concatenated together
and the amount of information shared between them.

Minority Class

(a) Node Classiﬁcation on Citeseer

(b) Node Classiﬁcation on Wikipedia

Figure 5: Node classiﬁcation results on two Citeseer and
Wikipedia Dataset. Y-axis is F1 score on each class. The red
spots represent the graph ensemble learning and the black
spots represent the best single graph embedding method’s
performance

As the Figure 5 indicates, the F1 score of our graph en-
semble methods on smaller classes are higher than the best
individual methods. Our graph ensemble strategy combines
the captured features derived by all single methods and gen-
erate a comprehensive graph embedding, which is able to
improve the performance on less represented classes. In
Wikipedia, we observe that for really small classes, none of
the individual methods perform well and give close to 0 F1.
However, the combination ensemble is able to perform well
and gives F1 ranging from 0.2 to 1.0. Similarly in Citeseer,

we see an improvement of about 50% for less represented
labels.

Dataset

PPI

BlogCatalog

Citeseer

Wikipedia

Method
gf
lap
hope
sdne
node2vec
node2vec,hope,gf,lap
gf
lap
hope
sdne
node2vec
node2vec,sdne,lap,gf
gf
lap
hope
sdne
node2vec
node2vec,sdne,hope,gf,lap
gf
lap
hope
sdne
node2vec
hope,node2vec,sdne,gf,lap

Dimensions
128
128
128
32
128
64,32,128,64
128
128
128
128
128
128,32,128,128
128
32
128
128
128
128,128,128,32,32
64
128
128
128
64
128,64,128,128,64

Macro-F1
0.118
0.077
0.144
0.159
0.179
0.192 (7.3%)
0.044
0.047
0.137
0.212
0.225
0.243 (8.0%)
0.442
0.388
0.517
0.513
0.671
0.673 (0.3%)
0.042
0.034
0.172
0.032
0.110
0.181 (5.2%)

Table 4: Macro F1 score of Graph Ensemble methods and
ﬁve baseline graph embedding methods. The scores with un-
derline show the best performance of single method on dif-
ferent datasets. The percentage inside parentheses indicates
the performance gain of Graph Ensemble method over the
best single method.

Conclusion
In this paper, we proposed a Graph Representation Ensem-
ble Learning framework which can create an ensemble of
graph embedding approaches outperforming each individ-
ual method. We provided theoretical analysis of the frame-
work and established the upper bound on the correlations
between graph embedding techniques. Further, we com-
pared our method with state-of-the-art embedding methods
and showed improvement on four real world networks. We
also showed that the model is even more useful for under-
represented classes. There are several research directions
for future work: (1) tighter ensemble bound to get a bet-
ter understanding of the framework, (2) information theo-
retic approaches which can take into account the mutual in-
formation between embeddings, and (3) dynamic ensembles

which can create ensemble learning for evolving graphs.

References
[Ahmed et al. 2013a] Ahmed, A.;
Shervashidze, N.;
Narayanamurthy, S.; Josifovski, V.; and Smola, A. J. 2013a.
Distributed large-scale natural graph factorization.
In
Proceedings of the 22nd international conference on World
Wide Web, 37–48. ACM.
[Ahmed et al. 2013b] Ahmed, A.;
Shervashidze, N.;
Narayanamurthy, S.; Josifovski, V.; and Smola, A. J. 2013b.
In
Distributed large-scale natural graph factorization.
Proceedings of the 22nd international conference on World
Wide Web, 37–48. ACM.
[Belkin and Niyogi 2001] Belkin, M., and Niyogi, P. 2001.
Laplacian eigenmaps and spectral techniques for embedding
and clustering. In NIPS, volume 14, 585–591.
[Belkin and Niyogi 2002] Belkin, M., and Niyogi, P. 2002.
Laplacian eigenmaps and spectral techniques for embedding
and clustering. In Advances in neural information process-
ing systems, 585–591.
[Benesty et al. 2009] Benesty, J.; Chen, J.; Huang, Y.; and
Cohen, I. 2009. Pearson correlation coefﬁcient. In Noise
reduction in speech processing. Springer. 1–4.
[Breitkreutz et al. 2008] Breitkreutz, B.-J.; Stark, C.; Reguly,
T.; Boucher, L.; Breitkreutz, A.; Livstone, M.; Oughtred, R.;
Lackner, D. H.; B¨ahler, J.; Wood, V.; et al. 2008. The biogrid
interaction database: 2008 update. Nucleic acids research
36(suppl 1):D637–D640.
[Bruna et al. 2013] Bruna, J.; Zaremba, W.; Szlam, A.; and
LeCun, Y. 2013. Spectral networks and locally connected
networks on graphs. arXiv preprint arXiv:1312.6203.
[Cao, Lu, and Xu 2016] Cao, S.; Lu, W.; and Xu, Q. 2016.
Deep neural networks for learning graph representations. In
Proceedings of the Thirtieth AAAI Conference on Artiﬁcial
Intelligence, 1145–1152. AAAI Press.
[Dietterich and others 2002] Dietterich, T. G., et al. 2002.
Ensemble learning. The handbook of brain theory and neu-
ral networks 2:110–125.
[Freeman 1978] Freeman, L. C. 1978. Centrality in so-
Social networks
cial networks conceptual clariﬁcation.
1(3):215–239.
[Freeman 2000] Freeman, L. C. 2000. Visualizing social
networks. Journal of social structure 1(1):4.
[Gehrke, Ginsparg, and Kleinberg 2003] Gehrke,
Ginsparg, P.; and Kleinberg, J.
the 2003 kdd cup. ACM SIGKDD Explorations 5(2).
[Goyal and Ferrara 2018] Goyal, P., and Ferrara, E. 2018.
Graph embedding techniques, applications, and perfor-
mance: A survey. Knowledge-Based Systems.
[Goyal, Sapienza, and Ferrara 2018] Goyal, P.; Sapienza, A.;
and Ferrara, E. 2018. Recommending teammates with deep
neural networks. In Proceedings of the 29th on Hypertext
and Social Media, 57–61. ACM.
[Grover and Leskovec 2016a] Grover, A., and Leskovec, J.
2016a. node2vec: Scalable feature learning for networks.
In Proceedings of the 22nd International Conference on
Knowledge Discovery and Data Mining, 855–864. ACM.

J.;
2003. Overview of

[Grover and Leskovec 2016b] Grover, A., and Leskovec, J.
2016b. node2vec: Scalable feature learning for networks. In
Proceedings of the 22nd ACM SIGKDD international con-
ference on Knowledge discovery and data mining, 855–864.
ACM.
[Hansen and Salamon 1990] Hansen, L. K., and Salamon, P.
IEEE Transactions on
1990. Neural network ensembles.
Pattern Analysis & Machine Intelligence (10):993–1001.
[Henaff, Bruna, and LeCun 2015] Henaff, M.; Bruna, J.; and
LeCun, Y. 2015. Deep convolutional networks on graph-
structured data. arXiv preprint arXiv:1506.05163.
[Kipf and Welling 2016a] Kipf, T. N., and Welling, M.
2016a. Semi-supervised classiﬁcation with graph convolu-
tional networks. arXiv preprint arXiv:1609.02907.
[Kipf and Welling 2016b] Kipf, T. N., and Welling, M.
2016b. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308.
[Liaw, Wiener, and others 2002] Liaw, A.; Wiener, M.; et al.
2002. Classiﬁcation and regression by randomforest. R news
2(3):18–22.
[Lu and Getoor 2003] Lu, Q., and Getoor, L. 2003. Link-
based classiﬁcation. In ICML, volume 3, 496–503.
[Mahoney 2011] Mahoney, M. 2011. Large text compression
benchmark. URL: http://www. mattmahoney. net/text/text.
html.
[Opitz 1999] Opitz, D. W. 1999. Feature selection for en-
sembles. AAAI/IAAI 379:384.
[Ou et al. 2016a] Ou, M.; Cui, P.; Pei, J.; Zhang, Z.; and Zhu,
W. 2016a. Asymmetric transitivity preserving graph embed-
ding. In Proc. of ACM SIGKDD, 1105–1114.
[Ou et al. 2016b] Ou, M.; Cui, P.; Pei, J.; Zhang, Z.; and Zhu,
W. 2016b. Asymmetric transitivity preserving graph embed-
In Proceedings of the 22nd ACM SIGKDD interna-
ding.
tional conference on Knowledge discovery and data mining,
1105–1114. ACM.
[R¨atsch, Onoda, and M¨uller 2001] R¨atsch, G.; Onoda, T.;
and M¨uller, K.-R. 2001. Soft margins for adaboost. Ma-
chine learning 42(3):287–320.
[Ricci and Aha 1997] Ricci, F., and Aha, D. W. 1997. Ex-
tending local learners with error-correcting output codes.
Naval Center for Applied Research in Artiﬁcial Intelligence,
Washington, DC.
[Robert and Escouﬁer 1976] Robert, P., and Escouﬁer, Y.
1976. A unifying tool for linear multivariate statistical meth-
ods: the rv-coefﬁcient. Journal of the Royal Statistical Soci-
ety: Series C (Applied Statistics) 25(3):257–265.
[Sz´ekely et al. 2007] Sz´ekely, G. J.; Rizzo, M. L.; Bakirov,
N. K.; et al. 2007. Measuring and testing dependence by
correlation of distances. The annals of statistics 35(6):2769–
2794.
[Sz´ekely, Rizzo, and others 2009] Sz´ekely, G.
J.; Rizzo,
M. L.; et al. 2009. Brownian distance covariance. The
annals of applied statistics 3(4):1236–1265.
[Tang and Liu 2009] Tang, L., and Liu, H. 2009. Relational
learning via latent social dimensions. In Proceedings of the
15th international conference on Knowledge discovery and
data mining, 817–826. ACM.

[Theocharidis et al. 2009] Theocharidis, A.; Van Dongen, S.;
Enright, A.; and Freeman, T. 2009. Network visualiza-
tion and analysis of gene expression data using biolayout
express3d. Nature protocols 4:1535–1550.
[Wang, Cui, and Zhu 2016a] Wang, D.; Cui, P.; and Zhu, W.
2016a. Structural deep network embedding. In Proceedings
of the 22nd International Conference on Knowledge Discov-
ery and Data Mining, 1225–1234. ACM.
[Wang, Cui, and Zhu 2016b] Wang, D.; Cui, P.; and Zhu, W.
In Proceed-
2016b. Structural deep network embedding.
ings of the 22nd ACM SIGKDD international conference on
Knowledge discovery and data mining, 1225–1234. ACM.
[Zhou 2012] Zhou, Z.-H. 2012. Ensemble methods: founda-
tions and algorithms. Chapman and Hall/CRC.

