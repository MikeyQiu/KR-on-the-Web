8
1
0
2
 
t
c
O
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
5
3
5
7
0
.
2
0
8
1
:
v
i
X
r
a

BRUNO: A Deep Recurrent Model for Exchangeable
Data

Iryna Korshunova ♥
Ghent University
iryna.korshunova@ugent.be

Jonas Degrave ♥ †
Ghent University
jonas.degrave@ugent.be

Ferenc Huszár
Twitter
fhuszar@twitter.com

Yarin Gal
University of Oxford
yarin@cs.ox.ac.uk

Arthur Gretton ♠
Gatsby Unit, UCL
arthur.gretton@gmail.com

Joni Dambre ♠
Ghent University
joni.dambre@ugent.be

Abstract

We present a novel model architecture which leverages deep learning tools to per-
form exact Bayesian inference on sets of high dimensional, complex observations.
Our model is provably exchangeable, meaning that the joint distribution over obser-
vations is invariant under permutation: this property lies at the heart of Bayesian
inference. The model does not require variational approximations to train, and new
samples can be generated conditional on previous samples, with cost linear in the
size of the conditioning set. The advantages of our architecture are demonstrated
on learning tasks that require generalisation from short observed sequences while
modelling sequence variability, such as conditional image generation, few-shot
learning, and anomaly detection.

1

Introduction

We address the problem of modelling unordered sets of objects that have some characteristic in
common. Set modelling has been a recent focus in machine learning, both due to relevant application
domains and to efﬁciency gains when dealing with groups of objects [7, 20, 23, 26]. The relevant
concept in statistics is the notion of an exchangeable sequence of random variables – a sequence where
any re-ordering of the elements is equally likely. To fulﬁl this deﬁnition, subsequent observations must
behave like previous ones, which implies that we can make predictions about the future. This property
allows the formulation of some machine learning problems in terms of modelling exchangeable data.
For instance, one can think of few-shot concept learning as learning to complete short exchangeable
sequences [12]. A related example comes from the generative image modelling ﬁeld, where we
might want to generate images that are in some ways similar to the ones from a given set. At present,
however, there are few ﬂexible and provably exchangeable deep generative models to solve this
problem.

Formally, a ﬁnite or inﬁnite sequence of random variables x1, x2, x3, . . . is said to be exchangeable
if for all n and all permutations π

p(x1, . . . , xn) = p

xπ(1), . . . , xπ(n)

,

(1)

i. e. the joint probability remains the same under any permutation of the sequence. If random variables
(cid:0)
in the sequence are independent and identically distributed (i. i. d.), then it is easy to see that the
sequence is exchangeable. The converse is false: exchangeable random variables can be correlated.
One example of an exchangeable but non-i. i. d. sequence is a sequence of variables x1, . . . , xn, which

(cid:1)

♥♠Equal contribution †Now at DeepMind.
32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

jointly have a multivariate normal distribution
all the dimensions [1]: Σii = 1 and Σij,i(cid:54)=j = ρ, with 0
The concept of exchangeability is intimately related to Bayesian statistics. De Finetti’s theorem
states that every exchangeable process (inﬁnite sequence of random variables) is a mixture of i. i. d.
processes:

Nn(0, Σ) with the same variance and covariance for

ρ < 1.

≤

p(x1, . . . , xn) =

p(θ)

θ)dθ,

p(xi|

(cid:90)

n

i=1
(cid:89)

(2)

where θ is some parameter (ﬁnite or inﬁnite dimensional) conditioned on which, the random variables
are i. i. d. [1]. In our previous Gaussian example, one can prove that x1, . . . , xn are i. i. d. with
xi ∼ N
∼ N
In terms of predictive distributions p(xn|

x1:n−1), the stochastic process in Eq. 2 can be written as

ρ) conditioned on θ

(0, ρ).

(θ, 1

−

p(xn|

x1:n−1) =

p(xn|
by conditioning both sides on x1:n−1. Eq. 3 is exactly the posterior predictive distribution, where
we marginalise the likelihood of xn given θ with respect to the posterior distribution of θ. From this
follows one possible interpretation of the de Finetti’s theorem: learning to ﬁt an exchangeable model
to sequences of data is implicitly the same as learning to reason about the hidden variables behind the
data.

x1:n−1)dθ,
|

θ)p(θ

(cid:90)

(3)

One strategy for deﬁning models of exchangeable sequences is through explicit Bayesian modelling:
θ) and calculates the posterior in Eq. 2 directly. Here,
one deﬁnes a prior p(θ), a likelihood p(xi|
x1:n−1).
the key difﬁculty is the intractability of the posterior and the predictive distribution p(xn|
Both of these expressions require integrating over the parameter θ, so we might end up having to
use approximations. This could violate the exchangeability property and make explicit Bayesian
modelling difﬁcult.

On the other hand, we do not have to explicitly represent the posterior to ensure exchangeability.
One could deﬁne a predictive distribution p(xn|
x1:n−1) directly, and as long as the process is
exchangeable, it is consistent with Bayesian reasoning. The key difﬁculty here is deﬁning an easy-to-
x1:n−1) which satisﬁes exchangeability. For example, it is not clear how to train or
calculate p(xn|
modify an ordinary recurrent neural network (RNN) to model exchangeable data. In our opinion, the
main challenge is to ensure that a hidden state contains information about all previous inputs x1:n
regardless of sequence length.

In this paper, we propose a novel architecture which combines features of the approaches above,
which we will refer to as BRUNO: Bayesian RecUrrent Neural mOdel. Our model is provably
exchangeable, and makes use of deep features learned from observations so as to model complex data
types such as images. To achieve this, we construct a bijective mapping between random variables
, and explicitly deﬁne an exchangeable model
xi ∈ X
z1:n−1) without explicitly
for the sequences z1, z2, z3, . . . , where we know an analytic form of p(zn|
computing the integral in Eq. 3.

in the observation space and features zi ∈ Z

Using BRUNO, we are able to generate samples conditioned on the input sequence by sampling
x1:n−1). The latter is also tractable to evaluate, i. e. has linear complexity in the
directly from p(xn|
number of data points. In respect of model training, evaluating the predictive distribution requires a
single pass through the neural network that implements
mapping. The model can be learned
straightforwardly, since p(xn|
The paper is structured as follows. In Section 2 we will look at two methods selected to highlight
the relation of our work with previous approaches to modelling exchangeable data. Section 3 will
describe BRUNO, along with necessary background information. In Section 4, we will use our model
for conditional image generation, few-shot learning, set expansion and set anomaly detection. Our
code is available at github.com/IraKorshunova/bruno.

x1:n−1) is differentiable with respect to the model parameters.

X (cid:55)→ Z

2 Related work

Bayesian sets [8] aim to model exchangeable sequences of binary random variables by analytically
computing the integrals in Eq. 2, 3. This is made possible by using a Bernoulli distribution for the

2

likelihood and a beta distribution for the prior. To apply this method to other types of data, e.g. images,
one needs to engineer a set of binary features [9]. In that case, there is usually no one-to-one mapping
between the input space
: in consequence, it is not possible to draw
x1:n−1). Unlike Bayesian sets, our approach does have a bijective transformation,
samples from p(xn|
which guarantees that inference in

Z
is equivalent to inference in space

and the features space

X

.

Z

X

The neural statistician [7] is an extension of a variational autoencoder model [10, 17] applied to
datasets. In addition to learning an approximate inference network over the latent variable zi for
every xi in the set, approximate inference is also implemented over a latent variable c – a context
that is global to the dataset. The architecture for the inference network q(c
x1, . . . , xn) maps every
xi into a feature vector and applies a mean pooling operation across these representations. The
resulting vector is then used to produce parameters of a Gaussian distribution over c. Mean pooling
x1, . . . , xn) invariant under permutations of the inputs. In addition to the inference
makes q(c
|
c) which assumes
networks, the neural statistician also has a generative component p(x1, . . . , xn|
that xi’s are independent given c. Here, it is easy to see that c plays the role of θ from Eq. 2. In
the neural statistician, it is intractable to compute p(x1, . . . , xn), so its variational lower bound
is used instead. In our model, we perform an implicit inference over θ and can exactly compute
predictive distributions and the marginal likelihood. Despite these differences, both neural statistician
and BRUNO can be applied in similar settings, namely few-shot learning and conditional image
generation, albeit with some restrictions, as we will see in Section 4.

|

3 Method

We begin this section with an overview of the mathematical tools needed to construct our model: ﬁrst
the Student-t process [19]; and then the Real NVP – a deep, stably invertible and learnable neural
network architecture for density estimation [6]. We next propose BRUNO, wherein we combine an
exchangeable Student-t process with the Real NVP, and derive recurrent equations for the predictive
distribution such that our model can be trained as an RNN. Our model is illustrated in Figure 1.

Figure 1: A schematic of the BRUNO model. It depicts how Bayesian thinking can lead to an
RNN-like computational graph in which Real NVP is a bijective feature extractor and the recurrence
is represented by Bayesian updates of an exchangeable Student-t process.

3.1 Student-t processes

The Student-t process (
representable density [19]. The more commonly used Gaussian processes (
limiting case of

) is the most general elliptically symmetric process with an analytically
P s) can be seen as
s.

s. In what follows, we provide the background and deﬁnition of

T P

G

Let us assume that z = (z1, . . . zn)
∈
M V Tn(ν, µ, K) with degrees of freedom ν
n

n covariance matrix K. Its density is given by

∈

Rn follows a multivariate Student-t distribution
Rn and a positive deﬁnite
R+ \

[0, 2], mean µ

∈

T P

T P

×

p(z) =

((ν

−

Γ( ν+n
2 )
2)π)n/2Γ(ν/2) |

K

−1/2
|

(z

µ)T K−1(z

µ)

1 +

−

−

ν

2

−

− ν+n
2

.

(cid:19)

(4)

(cid:18)

3

For our problem, we are interested in computing a conditional distribution. Suppose we can partition
z into two consecutive parts za ∈

Rnb , such that

za
zb

(cid:20)

(cid:21)

∼

Rna and zb ∈
ν,
M V Tn

(cid:32)

(cid:20)
za) is given by

µa
µb

,

Kaa Kab
Kba Kbb

(cid:21)

(cid:20)

.
(cid:21) (cid:33)

Then conditional distribution p(zb|

ν + βa −
ν + na −

2
2

˜Kbb

,

(cid:17)

za) = M V Tnb

ν + na, ˜µb,

p(zb|
˜µb = KbaK−1
βa = (za −
˜Kbb = Kbb −

(cid:16)
aa (za −
µa)T K−1
KbaK−1

µa) + µb

aa (za −
aa Kab.
In the general case, when one needs to invert the covariance matrix, the complexity of computing
a). These computations become infeasible for large datasets, which is a known
za) is
p(zb|
O
bottleneck for
P s [15]. In Section 3.3, we will show that exchangeable processes do not
P s and
G
have this issue.

µa)

(n3

T

G

P s. It
The parameter ν, representing the degrees of freedom, has a large impact on the behaviour of
controls how heavy-tailed the t-distribution is: as ν increases, the tails get lighter and the t-distribution
gets closer to the Gaussian. From Eq. 6, we can see that as ν or na tends to inﬁnity, the predictive
distribution tends to the one from a
P would give less certain
predictions than its corresponding

P . Thus, for small ν and na, a

T

T

G
P .

T

A second feature of the
P is the scaling of the predictive variance with a βa coefﬁcient, which
explicitly depends on the values of the conditioning observations. From Eq. 6, the value of βa is
precisely the Hotelling statistic for the vector za, and has a χ2
na distribution with mean na in the
event that za ∼ Nna (µa, Kaa). Looking at the weight (ν+βa−2)/(ν+na−2), we see that the variance
za) is increased over the Gaussian default when βa > na, and is reduced otherwise. In other
of p(zb|
words, when the samples are dispersed more than they would be under the Gaussian distribution, the
predictive uncertainty is increased compared with the Gaussian case. It is helpful in understanding
these two properties to recall that the multivariate Student-t distribution can be thought of as a
Gaussian distribution with an inverse Wishart prior on the covariance [19].

3.2 Real NVP

X
with

is transformed into a desired probability distribution in space

Real NVP [6] is a member of the normalising ﬂows family of models, where some density in the
input space
through a sequence
of invertible mappings [16]. Speciﬁcally, Real NVP proposes a design for a bijective function
= RD such that (a) the inverse is easy to evaluate, i.e. the cost
f :
of computing x = f −1(z) is the same as for the forward mapping, and (b) computing the Jacobian
determinant takes linear time in the number of dimensions D. Additionally, Real NVP assumes a
simple distribution for z, e.g. an isotropic Gaussian, so one can use a change of variables formula to
evaluate p(x):

= RD and

X (cid:55)→ Z

Z

Z

X

The main building block of Real NVP is a coupling layer. It implements a mapping
transforms half of its inputs while copying the other half directly to the output:

X (cid:55)→ Y

p(x) = p(z)

det

(cid:32)

∂f (x)

.

∂x (cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

y1:d = x1:d
yd+1:D = xd+1:D

(cid:26)

(cid:12)

exp(s(x1:d)) + t(x1:d),

where
convolutional neural networks.

(cid:12)

is an elementwise product, s (scale) and t (translation) are arbitrarily complex functions, e.g.

One can show that the coupling layer is a bijective, easily invertible mapping with a triangular
Jacobian and composition of such layers preserves these properties. To obtain a highly nonlinear
mapping f (x), one needs to stack coupling layers
while alternating the
dimensions that are being copied to the output.

X (cid:55)→ Y1 (cid:55)→ Y2 · · · (cid:55)→ Z

(5)

(6)

(7)

that

(8)

4

To make good use of modelling densities, the Real NVP has to treat its inputs as instances of a
continuous random variable [21]. To do so, integer pixel values in x are dequantised by adding
[0, 256)D are then rescaled to a [0, 1) interval and
[0, 1)D. The values x + u
uniform noise u
transformed with an elementwise function: f (x) = logit(α + (1

2α)x) with some small α.

∈

∈

−

3.3 BRUNO: the exchangeable sequence model

We now combine Bayesian and deep learning tools from the previous sections and present our model
for exchangeable sequences whose schematic is given in Figure 1.

Assume we are given an exchangeable sequence x1, . . . , xn, where every element is a D-dimensional
vector: xi = (x1
i ). We apply a Real NVP transformation to every xi, which results in an
RD. The proof that the latter
exchangeable sequence in the latent space: z1, . . . , zn, where zi ∈
sequence is exchangeable is given in Appendix A.

i , . . . xD

We make the following assumptions about the latents:

zd

A1: dimensions

D
d=1 p(zd)
}d=1,...,D are independent, so p(z) =
A2: for every dimension d, we assume the following: (zd
1 , . . . zd
n)
(cid:81)
parameters:

∼

{

M V Tn(νd, µd1, Kd), with

• degrees of freedom νd
• mean µd1 is a 1
• n

R+ \

∈

[0, 2]

×

n dimensional vector of ones multiplied by the scalar µd
ij,i(cid:54)=j = ρd where 0
ii = vd and Kd

n covariance matrix Kd with Kd

ρd < vd to
make sure that Kd is a positive-deﬁnite matrix that complies with covariance properties of
exchangeable sequences [1].

≤

×

∈

R

The exchangeable structure of the covariance matrix and having the same mean for every n, guarantees
that the sequence zd
2 . . . zd
n is exchangeable. Because the covariance matrix is simple, we can
derive recurrent updates for the parameters of p(zd
zd
1:n). Using the recurrence is a lot more
efﬁcient compared to the closed-form expressions in Eq. 6 since we want to compute the predictive
distribution for every step n.

1 , zd

n+1|

We start from a prior Student-t distribution for p(z1) with parameters µ1 = µ , v1 = v, ν1 = ν,
β1 = 0. Here, we will drop the dimension index d to simplify the notation. A detailed derivation of
the following results is given in Appendix B. To compute the degrees of freedom, mean and variance
of p(zn+1|

z1:n) for every n, we begin with the recurrent relations

νn+1 = νn + 1, µn+1 = (1

dn)µn + dnzn,

vn+1 = (1

dn)vn + dn(v

ρ

G

−

1 , . . . zd
n)

v+ρ(n−1) . Note that the

−
P recursions simply use the latter two equations, i.e. if we
where dn =
were to assume that (zd
P s, however, we also need to compute β
– a data-dependent term that scales the covariance matrix as in Eq. 6. To update β, we introduce
recurrent expressions for the auxiliary variables:
˜zi = zi −
an =

∼ Nn(µd1, Kd). For

bn =

2)

−

−

T

ρ

,

ρ),

(9)

µ
v + ρ(n
ρ)(v + ρ(n

(v

−
ρ)(v + ρ(n

1))

1))

(v

−
βn+1 = βn + (an −

−
bn)˜z2

n + bn(

˜zi)2

bn−1(

−

−

n

i=1
(cid:88)

−
˜zi)2.

n−1

i=1
(cid:88)

From these equations, we see that computational complexity of making predictions in exchangeable
(n3)

P s scales linearly with the number of observations, i.e.

(n) instead of a general

P s or

G
case where one needs to compute an inverse covariance matrix.

T

O

O

So far, we have constructed an exchangeable Student-t process in the latent space
with a bijective Real NVP mapping, we get an exchangeable process in space
not have an explicit analytic form of the transitions in
evaluate the predictive distribution via the change of variables formula in Eq. 7.

. By coupling it
Z
. Although we do
, we still can sample from this process and

X

X

5

3.4 Training

=

N −1

x1:n) allows us to use a training
Having an easy-to-evaluate autoregressive distribution p(xn+1|
scheme that is common for RNNs, i.e. maximise the likelihood of the next element in the sequence
at every step. Thus, our objective function for a single sequence of ﬁxed length N can be writ-
x1:n), which is equivalent to maximising the joint log-likelihood
ten as
log p(x1, . . . , xN ). While we do have a closed-form expression for the latter, we chose not to use
it during training in order to minimize the difference between the implementation of training and
testing phases. Note that at test time, dealing with the joint log-likelihood would be inconvenient or
even impossible due to high memory costs when N gets large, which again motivates the use of a
recurrent formulation.

n=0 log p(xn+1|

(cid:80)

L

During training, we update the weights of the Real NVP model and also learn the parameters of
the prior Student-t distribution. For the latter, we have three trainable parameters per dimension:
degrees of freedom νd, variance vd and covariance ρd. The mean µd is ﬁxed to 0 for every d and is
not updated during training.

4 Experiments

In this section, we will consider a few problems that ﬁt naturally into the framework of modeling
exchangeable data. We chose to work with sequences of images, so the results are easy to analyse; yet
BRUNO does not make any image-speciﬁc assumptions, and our conclusions can generalise to other
types of data. Speciﬁcally, for non-image data, one can use a general-purpose Real NVP coupling
layer as proposed by Papamakarios et al. [14]. In contrast to the original Real NVP model, which uses
convolutional architecture for scaling and translation functions in Eq. 8, a general implementation
has s and t composed from fully connected layers. We experimented with both convolutional and
non-convolutional architectures, the details of which are given in Appendix C.

In our experiments, the models are trained on image sequences of length 20. We form each sequence
by uniformly sampling a class and then selecting 20 random images from that class. This scheme
implies that a model is trained to implicitly infer a class label that is global to a sequence. In what
follows, we will see how this property can be used in a few tasks.

4.1 Conditional image generation

We ﬁrst consider a problem of generating samples conditionally on a set of images, which reduces to
sampling from a predictive distribution. This is different from a general Bayesian approach, where
one needs to infer the posterior over some meaningful latent variable and then ‘decode’ it.
To draw samples from p(xn+1|
z1:n) and then compute the
inverse Real NVP mapping: x = f −1(z). Since we assumed that dimensions of z are independent,
we can sample each zd from a univariate Student-t distribution. To do so, we modiﬁed Bailey’s polar
t-distribution generation method [2] to be computationally efﬁcient for GPU. Its algorithm is given in
Appendix D.

x1:n), we ﬁrst sample z

p(zn+1|

∼

In Figure 3, we show samples from the prior distribution p(x1) and conditional samples from a
x1:n) at steps n = 1, . . . , 20. Here, we used a convolutional Real NVP
predictive distribution p(xn+1|
model as a part of BRUNO. The model was trained on Omniglot [12] same-class image sequences of
length 20 and we used the train-test split and preprocessing as deﬁned by Vinyals et al. [24]. Namely,
we resized the images to 28
28 pixels and augmented the dataset with rotations by multiples of 90
degrees yielding 4,800 and 1,692 classes for training and testing respectively.

×

To better understand how BRUNO behaves, we test it on special types of input sequences that were
not seen during training. In Appendix E, we give an example where the same image is used throughout
the sequence. In that case, the variability of the samples reduces as the models gets more of the
same input. This property does not hold for the neural statistician model [7], discussed in Section 2.
As mentioned earlier, the neural statistician computes the approximate posterior q(c
x1, . . . , xn)
cmean). This scheme does not
and then uses its mean to sample x from a conditional model p(x
|
account for the variability in the inputs as a consequence of applying mean pooling over the features
of x1, . . . , xn when computing q(c
x1, . . . , xn). Thus, when all xi’s are the same, it would still
|
sample different instances from the class speciﬁed by xi. Given the code provided by the authors of

|

6

Figure 2: Samples generated conditionally on the sequence of the unseen Omniglot character class.
An input sequence is shown in the top row and samples in the bottom 4 rows. Every column of the
bottom subplot contains 4 samples from the predictive distribution conditioned on the input images
up to and including that column. That is, the 1st column shows samples from the prior p(x) when no
input image is given; the 2nd column shows samples from p(x
x1) where x1 is the 1st input image
|
in the top row and so on.

the neural statistician and following an email exchange, we could not reproduce the results from their
paper, so we refrained from making any direct comparisons.

More generated samples from convolutional and non-convolutional architectures trained on
MNIST [13], Fashion-MNIST [25] and CIFAR-10 [11] are given in the appendix. For a couple of
these models, we analyse the parameters of the learnt latent distributions (see Appendix F).

4.2 Few-shot learning

Previously, we saw that BRUNO can generate images of the unseen classes even after being
conditioned on a couple of examples. In this section, we will see how one can use its conditional
probabilities not only for generation, but also for a few-shot classiﬁcation.

We evaluate the few-shot learning accuracy of the model from Section 4.1 on the unseen Omniglot
characters from the 1,692 testing classes following the n-shot and k-way classiﬁcation setup proposed
by Vinyals et al. [24]. For every test case, we randomly draw a test image xn+1 and a sequence of n
images from the target class. At the same time, we draw n images for every of the k
1 random
1:n ) for each class i = 1 . . . k in
decoy classes. To classify an image xn+1, we compute p(xn+1|
the batch. An image is classiﬁed correctly when the conditional probability is highest for the target
class compared to the decoy classes. This evaluation is performed 20 times for each of the test classes
and the average classiﬁcation accuracy is reported in Table 1.

xC=i

−

For comparison, we considered three models from Vinyals et al. [24]: (a) k-nearest neighbours
(k-NN), where matching is done on raw pixels (Pixels), (b) k-NN with matching on discriminative
features from a state-of-the-art classiﬁer (Baseline Classiﬁer), and (c) Matching networks.

We observe that BRUNO model from Section 4.1 outperforms the baseline classiﬁer, despite having
been trained on relatively long sequences with a generative objective, i.e. maximising the likelihood
of the input images. Yet, it cannot compete with matching networks – a model tailored for a few-shot
learning and trained in a discriminative way on short sequences such that its test-time protocol exactly
matches the training time protocol. One can argue, however, that a comparison between models
trained generatively and discriminatively is not fair. Generative modelling is a more general, harder
problem to solve than discrimination, so a generatively trained model may waste a lot of statistical
power on modelling aspects of the data which are irrelevant for the classiﬁcation task. To verify our
intuition, we ﬁne-tuned BRUNO with a discriminative objective, i.e. maximising the likelihood of
correct labels in n-shot, k-way classiﬁcation episodes formed from the training examples of Omniglot.
While we could sample a different n and k for every training episode like in matching networks,
we found it sufﬁcient to ﬁx n and k during training. Namely, we chose the setting with n = 1 and
k = 20. From Table 1, we see that this additional discriminative training makes BRUNO competitive
with state-of-the-art models across all n-shot and k-way tasks.

As an extension to the few-shot learning task, we showed that BRUNO could also be used for online
set anomaly detection. These experiments can be found in Appendix H.

7

Table 1: Classiﬁcation accuracy for a few-shot learning task on the Omniglot dataset.

Model

PIXELS [24]
BASELINE CLASSIFIER [24]
MATCHING NETS [24]

5-way

1-shot 5-shot

20-way
1-shot 5-shot

41.7% 63.2% 26.7% 42.6%
80.0% 95.0% 69.5% 89.1%
98.1% 98.9% 93.8% 98.5%

BRUNO
BRUNO (discriminative ﬁne-tuning)

86.3% 95.6% 69.2% 87.7%
97.1% 99.4% 91.3% 97.8%

4.3

-based models

GP

T P

-based models can be easier compared to

In practice, we noticed that training
-based models as
they are more robust to anomalous training inputs and are less sensitive to the choise of hyperparame-
-based models
ters. Under certain conditions, we were not able to obtain convergent training with
GP
s; an example is given in Appendix G. However, we found a
which was not the case when using
few heuristics that make for a successful training such that
-based models perform equally
well in terms of test likelihoods, sample quality and few-shot classiﬁcation results. For instance, it
was crucial to use weight normalisation with a data-dependent initialisation of parameters of the Real
NVP [18]. As a result, one can opt for using
s due to their simpler implementation. Nevertheless,
a Student-t process remains a strictly richer model class for the latent space with negligible additional
computational costs.

T P

T P

and

GP

GP

GP

5 Discussion and conclusion

In this paper, we introduced BRUNO, a new technique combining deep learning and Student-t or
Gaussian processes for modelling exchangeable data. With this architecture, we may carry out implicit
Bayesian inference, avoiding the need to compute posteriors and eliminating the high computational
cost or approximation errors often associated with explicit Bayesian inference.

Based on our experiments, BRUNO shows promise for applications such as conditional image
generation, few-shot concept learning, few-shot classiﬁcation and online anomaly detection. The
probabilistic construction makes the BRUNO approach particularly useful and versatile in transfer
learning and multi-task situations. To demonstrate this, we showed that BRUNO trained in a
generative way achieves good performance in a downstream few-shot classiﬁcation task without any
task-speciﬁc retraining. Though, the performance can be signiﬁcantly improved with discriminative
ﬁne-tuning.

Training BRUNO is a form of meta-learning or learning-to-learn: it learns to perform Bayesian
inference on various sets of data. Just as encoding translational invariance in convolutional neural
networks seems to be the key to success in vision applications, we believe that the notion of
exchangeability is equally central to data-efﬁcient meta-learning. In this sense, architectures like
BRUNO and Deep Sets [26] can be seen as the most natural starting point for these applications.

As a consequence of exchangeability-by-design, BRUNO is endowed with a hidden state which
integrates information about all inputs regardless of sequence length. This desired property for
meta-learning is usually difﬁcult to ensure in general RNNs as they do not automatically generalise to
longer sequences than they were trained on and are sensitive to the ordering of inputs. Based on this
observation, the most promising applications for BRUNO may fall in the many-shot meta-learning
regime, where larger sets of data are available in each episode. Such problems naturally arise in
privacy-preserving on-device machine learning, or federated meta-learning [3], which is a potential
future application area for BRUNO.

8

We would like to thank Lucas Theis for his conceptual contributions to BRUNO, Conrado Miranda
and Frederic Godin for their helpful comments on the paper, Wittawat Jitkrittum for useful discussions,
and Lionel Pigou for setting up the hardware.

Acknowledgements

References

[1] Aldous, D., Hennequin, P., Ibragimov, I., and Jacod, J. (1985). Ecole d’Ete de Probabilites de

Saint-Flour XIII, 1983. Lecture Notes in Mathematics. Springer Berlin Heidelberg.

[2] Bailey, R. W. (1994). Polar generation of random variates with the t-distribution. Math. Comp.,

62(206):779–781.

[3] Chen, F., Dong, Z., Li, Z., and He, X. (2018). Federated meta-learning for recommendation.

arXiv preprint arXiv:1802.07876.

[4] Clevert, D., Unterthiner, T., and Hochreiter, S. (2016). Fast and accurate deep network learning by
exponential linear units (ELUs). In Proceedings of the 4th International Conference on Learning
Representations.

[5] Dinh, L., Krueger, D., and Bengio, Y. (2014). NICE: non-linear independent components

estimation. arXiv preprint, abs/1410.8516.

[6] Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). Density estimation using Real NVP. In

Proceedings of the 5th International Conference on Learning Representations.

[7] Edwards, H. and Storkey, A. (2017). Towards a neural statistician. In Proceedings of the 5th

International Conference on Learning Representations.

[8] Ghahramani, Z. and Heller, K. A. (2006). Bayesian sets. In Weiss, Y., Schölkopf, B., and Platt,
J. C., editors, Advances in Neural Information Processing Systems 18, pages 435–442. MIT Press.

[9] Heller, K. A. and Ghahramani, Z. (2006). A simple bayesian framework for content-based image
retrieval. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
pages 2110–2117.

[10] Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the

2nd International Conference on Learning Representations.

[11] Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.

[12] Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning

through probabilistic program induction. Science.

[13] LeCun, Y., Cortes, C., and Burges, C. J. (1998). The MNIST database of handwritten digits.

[14] Papamakarios, G., Murray, I., and Pavlakou, T. (2017). Masked autoregressive ﬂow for density

estimation. In Advances in Neural Information Processing Systems 30, pages 2335–2344.

[15] Rasmussen, C. E. and Williams, C. K. I. (2005). Gaussian Processes for Machine Learning

(Adaptive Computation and Machine Learning). The MIT Press.

[16] Rezende, D. and Mohamed, S. (2015). Variational inference with normalizing ﬂows.

In
Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings
of Machine Learning Research, pages 1530–1538.

[17] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and
In Proceedings of the 31st International

approximate inference in deep generative models.
Conference on Machine Learning, pages 1278–1286.

[18] Salimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In Proceedings of the 30th International Conference
on Neural Information Processing Systems.

9

[19] Shah, A., Wilson, A. G., and Ghahramani, Z. (2014). Student-t processes as alternatives to
gaussian processes. In Proceedings of the 17th International Conference on Artiﬁcial Intelligence
and Statistics, pages 877–885.

[20] Szabo, Z., Sriperumbudur, B., Poczos, B., and Gretton, A. (2016). Learning theory for distribu-

tion regression. Journal of Machine Learning Research, 17(152).

[21] Theis, L., van den Oord, A., and Bethge, M. (2016). A note on the evaluation of generative

models. In Proceedings of the 4th International Conference on Learning Representations.

[22] Tieleman, T. and Hinton, G. (2012). Lecture 6.5 - RmsProp: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.

[23] Vinyals, O., Bengio, S., and Kudlur, M. (2016a). Order matters: Sequence to sequence for sets.

In Proceedings of the 4th International Conference on Learning Representations.

[24] Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. (2016b). Matching
networks for one shot learning. In Advances in Neural Information Processing Systems 29, pages
3630–3638.

[25] Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for

benchmarking machine learning algorithms. arXiv preprint, abs/1708.07747.

[26] Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J.
(2017). Deep sets. In Advances in Neural Information Processing Systems 30, pages 3394–3404.

10

A Proofs

Lemma 1

Lemma 2

Given an exchangeable sequence (x1, x2, . . . , xn) of random variables xi ∈ X
mapping f :
Proof. Consider a vector function g : Rn
(x1, . . . , xn)

, the sequence (f (x1), f (x2), . . . , f (xn)) is exchangeable.
Rn such that
(z1 = f (x1), . . . , zn = f (xn)). A change of variable formula gives:

X (cid:55)→ Z

(cid:55)→

(cid:55)→

and a bijective

p(x1, x2, . . . , xn) = p(z1, z2, . . . , zn)

det J

,

∂f (xi)
where det J =
∂xi
of (x1, x2, . . . , xn) and the
|
p(z1, z2, . . . , zn). This proves that (z1, z2, . . . , zn) is exchangeable. (cid:3)

|
is the determinant of the Jacobian of g. Since both the joint probability
are invariant to the permutation of sequence entries, so must be
det J
|

n
i=1

(cid:81)

|

Given two exchangeable sequence x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) of ran-
i, j, the concatenated sequence x(cid:95)y =
dom variables, where xi is independent from yj for
∀
((x1, y1), (x2, y2), . . . , (xn, yn)) is exchangeable as well.
Proof. For any permutation π, as both sequences x and y are exchangeable we have:

p(x1, x2, . . . , xn)p(y1, y2, . . . , yn) = p(xπ(1), xπ(2), . . . , xπ(n))p(yπ(1), yπ(2), . . . , yπ(n)).

Independence between elements in x and y allows to write it as a joint distribution:

p((x1, y1), (x2, y2) . . . , (xn, yn)) = p((xπ(1), yπ(1)), (xπ(2), yπ(2)), . . . , (xπ(n), yπ(n))),

and thus the sequence x(cid:95)y is exchangeable. (cid:3)
This Lemma justiﬁes our construction with D independent exchangeable processes in the latent space
as given in A1 from Section 3.3.

B Derivation of recurrent Bayesian updates for exchangeable Student-t and

Gaussian processes

×

We assume that x = (x1, x2, . . . xn)
∈
M V Tn(ν, µ, K) with degrees of freedom ν
n

n covariance matrix K. Its density is given by:

∈

Rn follows a multivariate Student-t distribution
Rn and a positive deﬁnite
R+ \

[0, 2], mean µ

∈

p(x) =

((ν

−

Γ( ν+n
2 )
2)π)n/2Γ(ν/2) |

K

−1/2
|

(cid:18)

(x

µ)T K−1(x

µ)

1 +

−

−

ν

2

−

− ν+n
2

.

(cid:19)

(10)

Note that this parameterization of the multivariate t-distribution as deﬁned by Shah et al. [19] is
slightly different from the commonly used one. We used this parametrization as it makes the formulas
simpler.
If we partition x into two consecutive parts xa ∈

Rnb :

(cid:21)
the conditional distribution p(xb|

(cid:20)

xa
xb

M V Tn

ν,

∼

(cid:32)

(cid:20)
xa) is given by:

Rna and xb ∈
µa
µb

Kaa Kab
Kba Kbb

,

(cid:21)

(cid:20)

,
(cid:21) (cid:33)

p(xb|

xa) = M V Tnb (ν + na, ˜µb,

ν + βa −
ν + na −

2
2

˜Kbb),

(11)

where

µa) + µb

˜µb = KbaK−1
βa = (xa −
˜Kbb = Kbb −

aa (xa −
µa)T K−1
KbaK−1

aa (xa −
aa Kab.

µa)

11

Derivation of this result is given in the appendix of [19]. Let us now simplify these equations for the
case of exchangeable sequences with the following covariance structure:

K = 

v
ρ
ρ v
...
...
ρ ρ





· · ·
· · ·
. . .

· · ·

ρ
ρ
...
v



.





In our problem, we are interested in doing one-step predictions, i.e. computing a univariate density
x1:n) with parameters νn+1, µn+1, vn+1. Therefore, in Eq. 11 we can take: nb = 1, na = n,
p(xn+1|
R, Kaa = K1:n,1:n, Kab = K1:n,n+1, Kba = Kn+1,1:n and
xa = x1:n ∈
Kbb = Kn+1,n+1 = v.
Computing the parameters of the predictive distribution requires the inverse of Kaa, which we can
ﬁnd using the Sherman-Morrison formula:

Rn, xb = xn+1 ∈

K−1

aa = (A + uvT )−1 = A−1

A−1uvT A−1
1 + vT A−1u

,

−

with

with

After a few steps, the inverse of Kaa is:

v

ρ

−
0
...
0

v

ρ

0

−
...
0

· · ·
· · ·
. . .

· · ·

0
0
...
−

v



,

ρ





A = 





ρ
ρ
u = 
...
ρ











1
1
, v = 
...

1





.





K−1

aa = 

an
bn
...
bn





bn
an
...
bn

· · ·
· · ·
. . .

· · ·

bn
bn
...
an







an =

bn =

v + ρ(n
ρ)(v + ρ(n

−

2)

ρ

−
ρ)(v + ρ(n

−

−

(v

(v

−

−

,

.

1))

1))

Note that entries of K−1

aa explicitly depend on n.

Equations for the mean and variance of the predictive distribution require the following term:

KbaK−1

aa = (ρ ρ

ρ) K−1

aa =

· · ·

ρ
v + ρ(n

(cid:110)

,

1)

1:n

−

(cid:111)

which is a 1

n vector.

×

With this in mind, it is easy to derive the following recurrence:

Finally, let us derive recurrent equations for βn+1 = (xa −

aa (xa −

µa).

dn =

ρ
v + ρ(n

1)

µn+1 = (1

vn+1 = (1

−

−
dn)µn + dnxn

−
dn)vn + dn(ρ

v).

−
µa)T K −1

12

(cid:88)i(cid:54)=n

n−1

i=1
(cid:88)

= (an ˜x1 + bn

˜xi, an ˜x2 + bn

˜xi, . . . , an ˜xn + bn

˜xi)T (˜x1, ˜x2, . . . ˜xn)

Let ˜x = xa −

µa, then:

βn+1 = ˜xT K −1
aa ˜x
n

n

(cid:88)i(cid:54)=1
n
˜x2
i + bn(

n

(cid:88)i(cid:54)=2
˜xi)2.

= (an −

bn)

i=1
(cid:88)
Similarly, βn from p(xn|

i=1
(cid:88)

x1:n−1) is:

βn = (an−1 −

bn−1)

˜x2
i + bn−1(

˜xi)2

n−1

i=1
(cid:88)

βn+1 = (an −

bn)(

= (an −

bn)

βn −

n−1

n

i + ˜x2
˜x2

n) + bn(

˜xi)2

n−1
i=1 ˜xi)2
bn−1

i=1
(cid:88)
+ (an −

i=1
(cid:88)
bn−1(
an−1 −
(cid:80)
= 1, so βn+1 can be written recursively as:

n + bn(

bn)˜x2

n

i=1
(cid:88)

˜xi)2.

sn+1 = sn + ˜xn
βn+1 = βn + (an −

bn)˜x2

n + bn(s2

s2
n),

n+1 −

It is easy to show that

an−bn
an−1−bn−1

with s1 = 0.

C Implementation details

For simple datasets, such as MNIST, we found it tolerable to use models that rely upon a general
implementation of the Real NVP coupling layer similarly to Papamakarios et al. [14]. Namely,
when scaling and translation functions s and t are fully-connected neural networks. In our model,
networks s and t share the parameters in the ﬁrst two dense layers with 1024 hidden units and ELU
nonlinearity [4]. Their output layers are different: s ends with a dense layer with tanh and t ends
with a dense layer without a nonlinearity. We stacked 6 coupling layers with alternating the indices
of the transformed dimensions between odd and even as described by Dinh et al. [5]. For the ﬁrst
layer, which implements a logit transformation of the inputs, namely f (x) = logit(α + (1
2α)x),
we used α = 10−6. The logit transformation ensures that when taking the inverse mapping during
sample generation, the outputs always lie within ( −α

−

1−2α , 1−α

1−2α ).

In Omniglot, Fashion MNIST and CIFAR-10 experiments, we built upon a Real NVP model originally
designed for CIFAR-10 by Dinh et al. [6]: a multi-scale architecture with deep convolutional residual
networks in the coupling layers. Our main difference was the use of coupling layers with fully-
connected s and t networks (as described above) placed on top of the original convolutional Real
NVP model. We found that adding these layers allowed for a faster convergence and improved results.
This is likely due to a better mixing of the information before the output of the Real NVP gets into
the Student-t layer. We also found that using weight normalisation [18] within every s and t function
was crucial for successful training of large models.

The model parameters were optimized using RMSProp [22] with a decaying learning rate starting
from 10−3. Trainable parameters of a
were updated with a 10x smaller learning rate and
were initialized as following: νd = 1000, vd = 1., ρd = 0.1 for every dimension d. The mean µd
was ﬁxed at 0. For the Omniglot model, we used a batch size of 32, sequence length of 20 and trained
for 200K iterations. The other models were trained for a smaller number of iterations, i.e. ranging
from 50K to 100K updates.

T P

GP

or

13

D Sampling from a Student-t distribution

Algorithm 1 Efﬁcient sampling on GPU from a univariate t-distribution with mean µ, variance v and
degrees of freedom ν

function sample(µ, v, ν)

(0, 1)
← U
min(a, b)
max(a, b)
2πc
r
cos(α)

a, b
c
r
α
t

←
←
←
←

ν−2
v
σ
(cid:112)
ν
return µ + σt
(cid:113)

←

(cid:0)
end function

(cid:1)

(ν/r2)(r−4/ν

1)

−

E Sample analysis

In Figure 3, which includes Figure 2 from the main text, we want to illustrate how sample variability
depends on the variance of the inputs. From these examples, we see that in the case of a repeated
input image, samples get more coherent as the number of conditioning inputs grows. It also shows
that BRUNO does not merely generate samples according to the inferred class label.

While Omngilot is limited to 20 images per class, we can experiment with longer sequences using
CIFAR-10 or MNIST. In Figure 4 and Figure 5, we show samples from the models trained on those
datasets. In Figure 6, we also show more samples from the prior distribution p(x).

Figure 3: Samples generated conditionally on images from an unseen Omniglot character class. Left:
input sequence of 20 images from one class. Right: the same image is used as an input at every step.

x1:n) for every n = 480, . . . , 500. Left: input sequence
Figure 4: CIFAR-10 samples from p(x
|
(given in the top row of each subplot) is composed of random same-class test images. Right: same
image is given as input at every step. In both cases, input images come from the test set of CIFAR-10
and the model was trained on all of the classes.

14

x1:n) for every n = 480, . . . , 500. Left: input sequence (given
Figure 5: MNIST samples from p(x
|
in the top row of each subplot) is composed of random same-class test images. Right: same image is
given as input at every step. In both cases, input images come from the test set of MNIST and the
model was trained only on even digits, so it did not see digit ‘1’ during training.

Figure 6: Samples from the prior for the models trained on Omniglot, CIFAR-10, Fashion MNIST
and MNIST (only trained on even digits).

F Parameter analysis

After training a model, we observed that a majority of the processes in the latent space have low
correlations ρd/vd, and thus their predictive distributions remain close to the prior. Figure 7 plots
the number of dimensions where correlations exceed a certain value on the x-axis. For instance,
MNIST model has 8 dimensions where the correlation is higher than 0.1. While we have not veriﬁed
it experimentally, it is reasonable to expect those dimensions to capture information about visual
features of the digits.

Figure 7: Number of dimensions where ρd/vd > (cid:15) plotted on a double logarithmic scale. Left:
Omniglot model. Middle: CIFAR-10 model Right: Non-convolutional version of BRUNO trained on
MNIST.

T P

-based models, degrees of freedom νd for every process in the latent space were intialized to
For
1000, which makes a
. After training, most of the dimensions retain fairly high
degrees of freedom, but some can have small ν’s. One can notice from Figure 8 that dimensions with
high correlation tend to have smaller degrees of freedom.

close to a

T P

GP

15

Figure 8: Correlation ρd/vd versus degrees of freedom νd for every d. Degrees of freedom on the
x-axis are plotted on a logarithmic scale. Left: Omniglot model. Middle: CIFAR-10 model Right:
Non-convolutional version of BRUNO trained on MNIST.

We noticed that exchangeable
rameters even when
case.

T P

s can behave differently for certain settings of hyperpa-
s have high degrees of freedom. Figure 9 gives one example when this is the

s and

T P

GP

Figure 9: A toy example which illustrates how degrees of freedom ν affect the behaviour of a
T P
compared to a
. Here, we generate one sequence of 100 observations from an exchangeable
GP
multivariate normal disribution with parameters µ = 0., v = 0.1, ρ = 0.05 and evaluate predictive
probabilities under an exchangeable
models with parameters µ = 0., v = 1., ρ = 0.01
and different ν for

T P
s in the left and the right plots.

and

GP

T P

G Training of

and

-based models

T P

GP
on top, we found that these two versions of
When jointly optimizing Real NVP with a
s the convergence was
BRUNO occasionally behave differently during training. Namely, with
harder to achive. We could pinpoint a few determining factors: (a) the use of weightnorm [18] in the
Real NVP layers, (b) an intialisation of the covariance parameters, and (c) presence of outliers in the
training data. In Figure 10, we give examples of learning curves when BRUNO with
s tends not
to work well. Here, we use a convolutional architecture and train on Fashion MNIST. To simulate
outliers, every 100 iterations we feed a training batch where the last image of every sequence in the
batch is completely white.

or a

T P

GP

GP

GP

We would like to note that there are many settings where both versions of BRUNO diverge or
they both work well, and that the results of this partial ablation study are not sufﬁcient to draws
general conclusions. However, we can speculate that when extending BRUNO to new problems, it is
reasonable to start from a
-based model with weightnorm, small initial covariances, and small
learning rates. However, when ﬁnding a good set of hyperparameters is difﬁcult, it might be worth
trying the

GP
-based BRUNO.

T P

16

and

T P

GP
(0.1, 0.95) for every dimension. Here, the

Figure 10: Negative log-likelihood of
-based BRUNO on the training batches, smoothed
using a moving average over 10 points. Left: not using weightnorm, initial covariances are sampled
from
P -based model diverged after a few hundred
iterations. Adding weighnorm ﬁxes this problem. Middle: using weightnorm, covariances are
initialised to 0.1, learning rate is 0.002 (two times the default one). In this case, the learning rate is
-based model suffers from it more. Right: using weightnorm,
too high for both models, but the
covariances are initialised to 0.95.

GP

U

G

H Set anomaly detection

Online anomaly detection for exchangeable data is one of the application where we can use BRUNO.
This problem is closely related to the task of content-based image retrieval, where we need to rank
an image x on how well it ﬁts with the sequence x1:n [9]. For the ranking, we use the probabilistic
score proposed in Bayesian sets [8]:

score(x) =

x1:n)

p(x
|
p(x)

.

(12)

When we care exclusively about comparing ratios of conditional densities of xn+1 under different
sequences x1:n, we can compare densities in the latent space
instead. This is because the Jacobian
from the change of variable formula does not depend on the sequence we condition on.

Z

For the following experiment, we trained a small convolutional version of BRUNO only on even
MNIST digits (30,508 training images). In Figure 11, we give typical examples of how the score
evolves as the model gets more data points and how it behaves in the presence of inputs that do not
conform with the majority of the sequence. This preliminary experiment shows that our model can
detect anomalies in a stream of incoming data.

Figure 11: Evolution of the score as the model sees more images from an input sequence. Identiﬁed
outliers are marked with vertical lines and plotted on the right in the order from top to bottom. Note
that the model was trained only on images of even digits. Left: a sequence of digit ‘1’ images with
one image of ‘7’ correctly identiﬁed as an outlier. Right: a sequence of digit ‘9’ with one image of
digit ‘5’.

17

I Model samples

Figure 12: Samples from a model trained on Omniglot. Conditioning images come from character
classes that were not used during training, so when n is small, the problem is equivalent to a few-shot
generation.

18

Figure 13: Samples from a model trained on CIFAR-10. The model was trained on the set with 10
classes. Conditioning images in the top row of each subplot come from the test set.

Figure 14: Samples from a convolutional BRUNO model trained on Fashion MNIST. The model was
trained on the set with 10 classes. Conditioning images in the top row of each subplot come from the
test set.

19

Figure 15: Samples from a non-convolutional model trained on MNIST. The model was trained on
the set with 10 classes. Conditioning images in the top row of each subplot come from the test set.

20

8
1
0
2
 
t
c
O
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
5
3
5
7
0
.
2
0
8
1
:
v
i
X
r
a

BRUNO: A Deep Recurrent Model for Exchangeable
Data

Iryna Korshunova ♥
Ghent University
iryna.korshunova@ugent.be

Jonas Degrave ♥ †
Ghent University
jonas.degrave@ugent.be

Ferenc Huszár
Twitter
fhuszar@twitter.com

Yarin Gal
University of Oxford
yarin@cs.ox.ac.uk

Arthur Gretton ♠
Gatsby Unit, UCL
arthur.gretton@gmail.com

Joni Dambre ♠
Ghent University
joni.dambre@ugent.be

Abstract

We present a novel model architecture which leverages deep learning tools to per-
form exact Bayesian inference on sets of high dimensional, complex observations.
Our model is provably exchangeable, meaning that the joint distribution over obser-
vations is invariant under permutation: this property lies at the heart of Bayesian
inference. The model does not require variational approximations to train, and new
samples can be generated conditional on previous samples, with cost linear in the
size of the conditioning set. The advantages of our architecture are demonstrated
on learning tasks that require generalisation from short observed sequences while
modelling sequence variability, such as conditional image generation, few-shot
learning, and anomaly detection.

1

Introduction

We address the problem of modelling unordered sets of objects that have some characteristic in
common. Set modelling has been a recent focus in machine learning, both due to relevant application
domains and to efﬁciency gains when dealing with groups of objects [7, 20, 23, 26]. The relevant
concept in statistics is the notion of an exchangeable sequence of random variables – a sequence where
any re-ordering of the elements is equally likely. To fulﬁl this deﬁnition, subsequent observations must
behave like previous ones, which implies that we can make predictions about the future. This property
allows the formulation of some machine learning problems in terms of modelling exchangeable data.
For instance, one can think of few-shot concept learning as learning to complete short exchangeable
sequences [12]. A related example comes from the generative image modelling ﬁeld, where we
might want to generate images that are in some ways similar to the ones from a given set. At present,
however, there are few ﬂexible and provably exchangeable deep generative models to solve this
problem.

Formally, a ﬁnite or inﬁnite sequence of random variables x1, x2, x3, . . . is said to be exchangeable
if for all n and all permutations π

p(x1, . . . , xn) = p

xπ(1), . . . , xπ(n)

,

(1)

i. e. the joint probability remains the same under any permutation of the sequence. If random variables
(cid:0)
in the sequence are independent and identically distributed (i. i. d.), then it is easy to see that the
sequence is exchangeable. The converse is false: exchangeable random variables can be correlated.
One example of an exchangeable but non-i. i. d. sequence is a sequence of variables x1, . . . , xn, which

(cid:1)

♥♠Equal contribution †Now at DeepMind.
32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

jointly have a multivariate normal distribution
all the dimensions [1]: Σii = 1 and Σij,i(cid:54)=j = ρ, with 0
The concept of exchangeability is intimately related to Bayesian statistics. De Finetti’s theorem
states that every exchangeable process (inﬁnite sequence of random variables) is a mixture of i. i. d.
processes:

Nn(0, Σ) with the same variance and covariance for

ρ < 1.

≤

p(x1, . . . , xn) =

p(θ)

θ)dθ,

p(xi|

(cid:90)

n

i=1
(cid:89)

(2)

where θ is some parameter (ﬁnite or inﬁnite dimensional) conditioned on which, the random variables
are i. i. d. [1]. In our previous Gaussian example, one can prove that x1, . . . , xn are i. i. d. with
xi ∼ N
∼ N
In terms of predictive distributions p(xn|

x1:n−1), the stochastic process in Eq. 2 can be written as

ρ) conditioned on θ

(0, ρ).

(θ, 1

−

p(xn|

x1:n−1) =

p(xn|
by conditioning both sides on x1:n−1. Eq. 3 is exactly the posterior predictive distribution, where
we marginalise the likelihood of xn given θ with respect to the posterior distribution of θ. From this
follows one possible interpretation of the de Finetti’s theorem: learning to ﬁt an exchangeable model
to sequences of data is implicitly the same as learning to reason about the hidden variables behind the
data.

x1:n−1)dθ,
|

θ)p(θ

(cid:90)

(3)

One strategy for deﬁning models of exchangeable sequences is through explicit Bayesian modelling:
θ) and calculates the posterior in Eq. 2 directly. Here,
one deﬁnes a prior p(θ), a likelihood p(xi|
x1:n−1).
the key difﬁculty is the intractability of the posterior and the predictive distribution p(xn|
Both of these expressions require integrating over the parameter θ, so we might end up having to
use approximations. This could violate the exchangeability property and make explicit Bayesian
modelling difﬁcult.

On the other hand, we do not have to explicitly represent the posterior to ensure exchangeability.
One could deﬁne a predictive distribution p(xn|
x1:n−1) directly, and as long as the process is
exchangeable, it is consistent with Bayesian reasoning. The key difﬁculty here is deﬁning an easy-to-
x1:n−1) which satisﬁes exchangeability. For example, it is not clear how to train or
calculate p(xn|
modify an ordinary recurrent neural network (RNN) to model exchangeable data. In our opinion, the
main challenge is to ensure that a hidden state contains information about all previous inputs x1:n
regardless of sequence length.

In this paper, we propose a novel architecture which combines features of the approaches above,
which we will refer to as BRUNO: Bayesian RecUrrent Neural mOdel. Our model is provably
exchangeable, and makes use of deep features learned from observations so as to model complex data
types such as images. To achieve this, we construct a bijective mapping between random variables
, and explicitly deﬁne an exchangeable model
xi ∈ X
z1:n−1) without explicitly
for the sequences z1, z2, z3, . . . , where we know an analytic form of p(zn|
computing the integral in Eq. 3.

in the observation space and features zi ∈ Z

Using BRUNO, we are able to generate samples conditioned on the input sequence by sampling
x1:n−1). The latter is also tractable to evaluate, i. e. has linear complexity in the
directly from p(xn|
number of data points. In respect of model training, evaluating the predictive distribution requires a
single pass through the neural network that implements
mapping. The model can be learned
straightforwardly, since p(xn|
The paper is structured as follows. In Section 2 we will look at two methods selected to highlight
the relation of our work with previous approaches to modelling exchangeable data. Section 3 will
describe BRUNO, along with necessary background information. In Section 4, we will use our model
for conditional image generation, few-shot learning, set expansion and set anomaly detection. Our
code is available at github.com/IraKorshunova/bruno.

x1:n−1) is differentiable with respect to the model parameters.

X (cid:55)→ Z

2 Related work

Bayesian sets [8] aim to model exchangeable sequences of binary random variables by analytically
computing the integrals in Eq. 2, 3. This is made possible by using a Bernoulli distribution for the

2

likelihood and a beta distribution for the prior. To apply this method to other types of data, e.g. images,
one needs to engineer a set of binary features [9]. In that case, there is usually no one-to-one mapping
between the input space
: in consequence, it is not possible to draw
x1:n−1). Unlike Bayesian sets, our approach does have a bijective transformation,
samples from p(xn|
which guarantees that inference in

Z
is equivalent to inference in space

and the features space

X

.

Z

X

The neural statistician [7] is an extension of a variational autoencoder model [10, 17] applied to
datasets. In addition to learning an approximate inference network over the latent variable zi for
every xi in the set, approximate inference is also implemented over a latent variable c – a context
that is global to the dataset. The architecture for the inference network q(c
x1, . . . , xn) maps every
xi into a feature vector and applies a mean pooling operation across these representations. The
resulting vector is then used to produce parameters of a Gaussian distribution over c. Mean pooling
x1, . . . , xn) invariant under permutations of the inputs. In addition to the inference
makes q(c
|
c) which assumes
networks, the neural statistician also has a generative component p(x1, . . . , xn|
that xi’s are independent given c. Here, it is easy to see that c plays the role of θ from Eq. 2. In
the neural statistician, it is intractable to compute p(x1, . . . , xn), so its variational lower bound
is used instead. In our model, we perform an implicit inference over θ and can exactly compute
predictive distributions and the marginal likelihood. Despite these differences, both neural statistician
and BRUNO can be applied in similar settings, namely few-shot learning and conditional image
generation, albeit with some restrictions, as we will see in Section 4.

|

3 Method

We begin this section with an overview of the mathematical tools needed to construct our model: ﬁrst
the Student-t process [19]; and then the Real NVP – a deep, stably invertible and learnable neural
network architecture for density estimation [6]. We next propose BRUNO, wherein we combine an
exchangeable Student-t process with the Real NVP, and derive recurrent equations for the predictive
distribution such that our model can be trained as an RNN. Our model is illustrated in Figure 1.

Figure 1: A schematic of the BRUNO model. It depicts how Bayesian thinking can lead to an
RNN-like computational graph in which Real NVP is a bijective feature extractor and the recurrence
is represented by Bayesian updates of an exchangeable Student-t process.

3.1 Student-t processes

The Student-t process (
representable density [19]. The more commonly used Gaussian processes (
limiting case of

) is the most general elliptically symmetric process with an analytically
P s) can be seen as
s.

s. In what follows, we provide the background and deﬁnition of

T P

G

Let us assume that z = (z1, . . . zn)
∈
M V Tn(ν, µ, K) with degrees of freedom ν
n

n covariance matrix K. Its density is given by

∈

Rn follows a multivariate Student-t distribution
Rn and a positive deﬁnite
R+ \

[0, 2], mean µ

∈

T P

T P

×

p(z) =

((ν

−

Γ( ν+n
2 )
2)π)n/2Γ(ν/2) |

K

−1/2
|

(z

µ)T K−1(z

µ)

1 +

−

−

ν

2

−

− ν+n
2

.

(cid:19)

(4)

(cid:18)

3

For our problem, we are interested in computing a conditional distribution. Suppose we can partition
z into two consecutive parts za ∈

Rnb , such that

za
zb

(cid:20)

(cid:21)

∼

Rna and zb ∈
ν,
M V Tn

(cid:32)

(cid:20)
za) is given by

µa
µb

,

Kaa Kab
Kba Kbb

(cid:21)

(cid:20)

.
(cid:21) (cid:33)

Then conditional distribution p(zb|

ν + βa −
ν + na −

2
2

˜Kbb

,

(cid:17)

za) = M V Tnb

ν + na, ˜µb,

p(zb|
˜µb = KbaK−1
βa = (za −
˜Kbb = Kbb −

(cid:16)
aa (za −
µa)T K−1
KbaK−1

µa) + µb

aa (za −
aa Kab.
In the general case, when one needs to invert the covariance matrix, the complexity of computing
a). These computations become infeasible for large datasets, which is a known
za) is
p(zb|
O
bottleneck for
P s [15]. In Section 3.3, we will show that exchangeable processes do not
P s and
G
have this issue.

µa)

(n3

T

G

P s. It
The parameter ν, representing the degrees of freedom, has a large impact on the behaviour of
controls how heavy-tailed the t-distribution is: as ν increases, the tails get lighter and the t-distribution
gets closer to the Gaussian. From Eq. 6, we can see that as ν or na tends to inﬁnity, the predictive
distribution tends to the one from a
P would give less certain
predictions than its corresponding

P . Thus, for small ν and na, a

T

T

G
P .

T

A second feature of the
P is the scaling of the predictive variance with a βa coefﬁcient, which
explicitly depends on the values of the conditioning observations. From Eq. 6, the value of βa is
precisely the Hotelling statistic for the vector za, and has a χ2
na distribution with mean na in the
event that za ∼ Nna (µa, Kaa). Looking at the weight (ν+βa−2)/(ν+na−2), we see that the variance
za) is increased over the Gaussian default when βa > na, and is reduced otherwise. In other
of p(zb|
words, when the samples are dispersed more than they would be under the Gaussian distribution, the
predictive uncertainty is increased compared with the Gaussian case. It is helpful in understanding
these two properties to recall that the multivariate Student-t distribution can be thought of as a
Gaussian distribution with an inverse Wishart prior on the covariance [19].

3.2 Real NVP

X
with

is transformed into a desired probability distribution in space

Real NVP [6] is a member of the normalising ﬂows family of models, where some density in the
input space
through a sequence
of invertible mappings [16]. Speciﬁcally, Real NVP proposes a design for a bijective function
= RD such that (a) the inverse is easy to evaluate, i.e. the cost
f :
of computing x = f −1(z) is the same as for the forward mapping, and (b) computing the Jacobian
determinant takes linear time in the number of dimensions D. Additionally, Real NVP assumes a
simple distribution for z, e.g. an isotropic Gaussian, so one can use a change of variables formula to
evaluate p(x):

= RD and

X (cid:55)→ Z

Z

Z

X

The main building block of Real NVP is a coupling layer. It implements a mapping
transforms half of its inputs while copying the other half directly to the output:

X (cid:55)→ Y

p(x) = p(z)

det

(cid:32)

∂f (x)

.

∂x (cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

y1:d = x1:d
yd+1:D = xd+1:D

(cid:26)

(cid:12)

exp(s(x1:d)) + t(x1:d),

where
convolutional neural networks.

(cid:12)

is an elementwise product, s (scale) and t (translation) are arbitrarily complex functions, e.g.

One can show that the coupling layer is a bijective, easily invertible mapping with a triangular
Jacobian and composition of such layers preserves these properties. To obtain a highly nonlinear
mapping f (x), one needs to stack coupling layers
while alternating the
dimensions that are being copied to the output.

X (cid:55)→ Y1 (cid:55)→ Y2 · · · (cid:55)→ Z

(5)

(6)

(7)

that

(8)

4

To make good use of modelling densities, the Real NVP has to treat its inputs as instances of a
continuous random variable [21]. To do so, integer pixel values in x are dequantised by adding
[0, 256)D are then rescaled to a [0, 1) interval and
[0, 1)D. The values x + u
uniform noise u
transformed with an elementwise function: f (x) = logit(α + (1

2α)x) with some small α.

∈

∈

−

3.3 BRUNO: the exchangeable sequence model

We now combine Bayesian and deep learning tools from the previous sections and present our model
for exchangeable sequences whose schematic is given in Figure 1.

Assume we are given an exchangeable sequence x1, . . . , xn, where every element is a D-dimensional
vector: xi = (x1
i ). We apply a Real NVP transformation to every xi, which results in an
RD. The proof that the latter
exchangeable sequence in the latent space: z1, . . . , zn, where zi ∈
sequence is exchangeable is given in Appendix A.

i , . . . xD

We make the following assumptions about the latents:

zd

A1: dimensions

D
d=1 p(zd)
}d=1,...,D are independent, so p(z) =
A2: for every dimension d, we assume the following: (zd
1 , . . . zd
n)
(cid:81)
parameters:

∼

{

M V Tn(νd, µd1, Kd), with

• degrees of freedom νd
• mean µd1 is a 1
• n

×

R+ \

∈

[0, 2]

n dimensional vector of ones multiplied by the scalar µd
ij,i(cid:54)=j = ρd where 0
ii = vd and Kd

n covariance matrix Kd with Kd

ρd < vd to
make sure that Kd is a positive-deﬁnite matrix that complies with covariance properties of
exchangeable sequences [1].

≤

×

∈

R

The exchangeable structure of the covariance matrix and having the same mean for every n, guarantees
that the sequence zd
2 . . . zd
n is exchangeable. Because the covariance matrix is simple, we can
derive recurrent updates for the parameters of p(zd
zd
1:n). Using the recurrence is a lot more
efﬁcient compared to the closed-form expressions in Eq. 6 since we want to compute the predictive
distribution for every step n.

1 , zd

n+1|

We start from a prior Student-t distribution for p(z1) with parameters µ1 = µ , v1 = v, ν1 = ν,
β1 = 0. Here, we will drop the dimension index d to simplify the notation. A detailed derivation of
the following results is given in Appendix B. To compute the degrees of freedom, mean and variance
of p(zn+1|

z1:n) for every n, we begin with the recurrent relations

νn+1 = νn + 1, µn+1 = (1

dn)µn + dnzn,

vn+1 = (1

dn)vn + dn(v

ρ

G

−

1 , . . . zd
n)

v+ρ(n−1) . Note that the

−
P recursions simply use the latter two equations, i.e. if we
where dn =
were to assume that (zd
P s, however, we also need to compute β
– a data-dependent term that scales the covariance matrix as in Eq. 6. To update β, we introduce
recurrent expressions for the auxiliary variables:
˜zi = zi −
an =

∼ Nn(µd1, Kd). For

bn =

2)

−

−

T

ρ

,

ρ),

(9)

µ
v + ρ(n
ρ)(v + ρ(n

(v

−
ρ)(v + ρ(n

1))

1))

(v

−
βn+1 = βn + (an −

−
bn)˜z2

n + bn(

˜zi)2

bn−1(

−

−

n

i=1
(cid:88)

−
˜zi)2.

n−1

i=1
(cid:88)

From these equations, we see that computational complexity of making predictions in exchangeable
(n3)

P s scales linearly with the number of observations, i.e.

(n) instead of a general

P s or

G
case where one needs to compute an inverse covariance matrix.

T

O

O

So far, we have constructed an exchangeable Student-t process in the latent space
with a bijective Real NVP mapping, we get an exchangeable process in space
not have an explicit analytic form of the transitions in
evaluate the predictive distribution via the change of variables formula in Eq. 7.

. By coupling it
Z
. Although we do
, we still can sample from this process and

X

X

5

3.4 Training

=

N −1

x1:n) allows us to use a training
Having an easy-to-evaluate autoregressive distribution p(xn+1|
scheme that is common for RNNs, i.e. maximise the likelihood of the next element in the sequence
at every step. Thus, our objective function for a single sequence of ﬁxed length N can be writ-
x1:n), which is equivalent to maximising the joint log-likelihood
ten as
log p(x1, . . . , xN ). While we do have a closed-form expression for the latter, we chose not to use
it during training in order to minimize the difference between the implementation of training and
testing phases. Note that at test time, dealing with the joint log-likelihood would be inconvenient or
even impossible due to high memory costs when N gets large, which again motivates the use of a
recurrent formulation.

n=0 log p(xn+1|

(cid:80)

L

During training, we update the weights of the Real NVP model and also learn the parameters of
the prior Student-t distribution. For the latter, we have three trainable parameters per dimension:
degrees of freedom νd, variance vd and covariance ρd. The mean µd is ﬁxed to 0 for every d and is
not updated during training.

4 Experiments

In this section, we will consider a few problems that ﬁt naturally into the framework of modeling
exchangeable data. We chose to work with sequences of images, so the results are easy to analyse; yet
BRUNO does not make any image-speciﬁc assumptions, and our conclusions can generalise to other
types of data. Speciﬁcally, for non-image data, one can use a general-purpose Real NVP coupling
layer as proposed by Papamakarios et al. [14]. In contrast to the original Real NVP model, which uses
convolutional architecture for scaling and translation functions in Eq. 8, a general implementation
has s and t composed from fully connected layers. We experimented with both convolutional and
non-convolutional architectures, the details of which are given in Appendix C.

In our experiments, the models are trained on image sequences of length 20. We form each sequence
by uniformly sampling a class and then selecting 20 random images from that class. This scheme
implies that a model is trained to implicitly infer a class label that is global to a sequence. In what
follows, we will see how this property can be used in a few tasks.

4.1 Conditional image generation

We ﬁrst consider a problem of generating samples conditionally on a set of images, which reduces to
sampling from a predictive distribution. This is different from a general Bayesian approach, where
one needs to infer the posterior over some meaningful latent variable and then ‘decode’ it.
To draw samples from p(xn+1|
z1:n) and then compute the
inverse Real NVP mapping: x = f −1(z). Since we assumed that dimensions of z are independent,
we can sample each zd from a univariate Student-t distribution. To do so, we modiﬁed Bailey’s polar
t-distribution generation method [2] to be computationally efﬁcient for GPU. Its algorithm is given in
Appendix D.

x1:n), we ﬁrst sample z

p(zn+1|

∼

In Figure 3, we show samples from the prior distribution p(x1) and conditional samples from a
x1:n) at steps n = 1, . . . , 20. Here, we used a convolutional Real NVP
predictive distribution p(xn+1|
model as a part of BRUNO. The model was trained on Omniglot [12] same-class image sequences of
length 20 and we used the train-test split and preprocessing as deﬁned by Vinyals et al. [24]. Namely,
we resized the images to 28
28 pixels and augmented the dataset with rotations by multiples of 90
degrees yielding 4,800 and 1,692 classes for training and testing respectively.

×

To better understand how BRUNO behaves, we test it on special types of input sequences that were
not seen during training. In Appendix E, we give an example where the same image is used throughout
the sequence. In that case, the variability of the samples reduces as the models gets more of the
same input. This property does not hold for the neural statistician model [7], discussed in Section 2.
As mentioned earlier, the neural statistician computes the approximate posterior q(c
x1, . . . , xn)
cmean). This scheme does not
and then uses its mean to sample x from a conditional model p(x
|
account for the variability in the inputs as a consequence of applying mean pooling over the features
of x1, . . . , xn when computing q(c
x1, . . . , xn). Thus, when all xi’s are the same, it would still
|
sample different instances from the class speciﬁed by xi. Given the code provided by the authors of

|

6

Figure 2: Samples generated conditionally on the sequence of the unseen Omniglot character class.
An input sequence is shown in the top row and samples in the bottom 4 rows. Every column of the
bottom subplot contains 4 samples from the predictive distribution conditioned on the input images
up to and including that column. That is, the 1st column shows samples from the prior p(x) when no
input image is given; the 2nd column shows samples from p(x
x1) where x1 is the 1st input image
|
in the top row and so on.

the neural statistician and following an email exchange, we could not reproduce the results from their
paper, so we refrained from making any direct comparisons.

More generated samples from convolutional and non-convolutional architectures trained on
MNIST [13], Fashion-MNIST [25] and CIFAR-10 [11] are given in the appendix. For a couple of
these models, we analyse the parameters of the learnt latent distributions (see Appendix F).

4.2 Few-shot learning

Previously, we saw that BRUNO can generate images of the unseen classes even after being
conditioned on a couple of examples. In this section, we will see how one can use its conditional
probabilities not only for generation, but also for a few-shot classiﬁcation.

We evaluate the few-shot learning accuracy of the model from Section 4.1 on the unseen Omniglot
characters from the 1,692 testing classes following the n-shot and k-way classiﬁcation setup proposed
by Vinyals et al. [24]. For every test case, we randomly draw a test image xn+1 and a sequence of n
images from the target class. At the same time, we draw n images for every of the k
1 random
1:n ) for each class i = 1 . . . k in
decoy classes. To classify an image xn+1, we compute p(xn+1|
the batch. An image is classiﬁed correctly when the conditional probability is highest for the target
class compared to the decoy classes. This evaluation is performed 20 times for each of the test classes
and the average classiﬁcation accuracy is reported in Table 1.

xC=i

−

For comparison, we considered three models from Vinyals et al. [24]: (a) k-nearest neighbours
(k-NN), where matching is done on raw pixels (Pixels), (b) k-NN with matching on discriminative
features from a state-of-the-art classiﬁer (Baseline Classiﬁer), and (c) Matching networks.

We observe that BRUNO model from Section 4.1 outperforms the baseline classiﬁer, despite having
been trained on relatively long sequences with a generative objective, i.e. maximising the likelihood
of the input images. Yet, it cannot compete with matching networks – a model tailored for a few-shot
learning and trained in a discriminative way on short sequences such that its test-time protocol exactly
matches the training time protocol. One can argue, however, that a comparison between models
trained generatively and discriminatively is not fair. Generative modelling is a more general, harder
problem to solve than discrimination, so a generatively trained model may waste a lot of statistical
power on modelling aspects of the data which are irrelevant for the classiﬁcation task. To verify our
intuition, we ﬁne-tuned BRUNO with a discriminative objective, i.e. maximising the likelihood of
correct labels in n-shot, k-way classiﬁcation episodes formed from the training examples of Omniglot.
While we could sample a different n and k for every training episode like in matching networks,
we found it sufﬁcient to ﬁx n and k during training. Namely, we chose the setting with n = 1 and
k = 20. From Table 1, we see that this additional discriminative training makes BRUNO competitive
with state-of-the-art models across all n-shot and k-way tasks.

As an extension to the few-shot learning task, we showed that BRUNO could also be used for online
set anomaly detection. These experiments can be found in Appendix H.

7

Table 1: Classiﬁcation accuracy for a few-shot learning task on the Omniglot dataset.

Model

PIXELS [24]
BASELINE CLASSIFIER [24]
MATCHING NETS [24]

5-way

1-shot 5-shot

20-way
1-shot 5-shot

41.7% 63.2% 26.7% 42.6%
80.0% 95.0% 69.5% 89.1%
98.1% 98.9% 93.8% 98.5%

BRUNO
BRUNO (discriminative ﬁne-tuning)

86.3% 95.6% 69.2% 87.7%
97.1% 99.4% 91.3% 97.8%

4.3

-based models

GP

T P

-based models can be easier compared to

In practice, we noticed that training
-based models as
they are more robust to anomalous training inputs and are less sensitive to the choise of hyperparame-
-based models
ters. Under certain conditions, we were not able to obtain convergent training with
GP
s; an example is given in Appendix G. However, we found a
which was not the case when using
few heuristics that make for a successful training such that
-based models perform equally
well in terms of test likelihoods, sample quality and few-shot classiﬁcation results. For instance, it
was crucial to use weight normalisation with a data-dependent initialisation of parameters of the Real
NVP [18]. As a result, one can opt for using
s due to their simpler implementation. Nevertheless,
a Student-t process remains a strictly richer model class for the latent space with negligible additional
computational costs.

T P

T P

and

GP

GP

GP

5 Discussion and conclusion

In this paper, we introduced BRUNO, a new technique combining deep learning and Student-t or
Gaussian processes for modelling exchangeable data. With this architecture, we may carry out implicit
Bayesian inference, avoiding the need to compute posteriors and eliminating the high computational
cost or approximation errors often associated with explicit Bayesian inference.

Based on our experiments, BRUNO shows promise for applications such as conditional image
generation, few-shot concept learning, few-shot classiﬁcation and online anomaly detection. The
probabilistic construction makes the BRUNO approach particularly useful and versatile in transfer
learning and multi-task situations. To demonstrate this, we showed that BRUNO trained in a
generative way achieves good performance in a downstream few-shot classiﬁcation task without any
task-speciﬁc retraining. Though, the performance can be signiﬁcantly improved with discriminative
ﬁne-tuning.

Training BRUNO is a form of meta-learning or learning-to-learn: it learns to perform Bayesian
inference on various sets of data. Just as encoding translational invariance in convolutional neural
networks seems to be the key to success in vision applications, we believe that the notion of
exchangeability is equally central to data-efﬁcient meta-learning. In this sense, architectures like
BRUNO and Deep Sets [26] can be seen as the most natural starting point for these applications.

As a consequence of exchangeability-by-design, BRUNO is endowed with a hidden state which
integrates information about all inputs regardless of sequence length. This desired property for
meta-learning is usually difﬁcult to ensure in general RNNs as they do not automatically generalise to
longer sequences than they were trained on and are sensitive to the ordering of inputs. Based on this
observation, the most promising applications for BRUNO may fall in the many-shot meta-learning
regime, where larger sets of data are available in each episode. Such problems naturally arise in
privacy-preserving on-device machine learning, or federated meta-learning [3], which is a potential
future application area for BRUNO.

8

We would like to thank Lucas Theis for his conceptual contributions to BRUNO, Conrado Miranda
and Frederic Godin for their helpful comments on the paper, Wittawat Jitkrittum for useful discussions,
and Lionel Pigou for setting up the hardware.

Acknowledgements

References

[1] Aldous, D., Hennequin, P., Ibragimov, I., and Jacod, J. (1985). Ecole d’Ete de Probabilites de

Saint-Flour XIII, 1983. Lecture Notes in Mathematics. Springer Berlin Heidelberg.

[2] Bailey, R. W. (1994). Polar generation of random variates with the t-distribution. Math. Comp.,

62(206):779–781.

[3] Chen, F., Dong, Z., Li, Z., and He, X. (2018). Federated meta-learning for recommendation.

arXiv preprint arXiv:1802.07876.

[4] Clevert, D., Unterthiner, T., and Hochreiter, S. (2016). Fast and accurate deep network learning by
exponential linear units (ELUs). In Proceedings of the 4th International Conference on Learning
Representations.

[5] Dinh, L., Krueger, D., and Bengio, Y. (2014). NICE: non-linear independent components

estimation. arXiv preprint, abs/1410.8516.

[6] Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). Density estimation using Real NVP. In

Proceedings of the 5th International Conference on Learning Representations.

[7] Edwards, H. and Storkey, A. (2017). Towards a neural statistician. In Proceedings of the 5th

International Conference on Learning Representations.

[8] Ghahramani, Z. and Heller, K. A. (2006). Bayesian sets. In Weiss, Y., Schölkopf, B., and Platt,
J. C., editors, Advances in Neural Information Processing Systems 18, pages 435–442. MIT Press.

[9] Heller, K. A. and Ghahramani, Z. (2006). A simple bayesian framework for content-based image
retrieval. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
pages 2110–2117.

[10] Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the

2nd International Conference on Learning Representations.

[11] Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.

[12] Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning

through probabilistic program induction. Science.

[13] LeCun, Y., Cortes, C., and Burges, C. J. (1998). The MNIST database of handwritten digits.

[14] Papamakarios, G., Murray, I., and Pavlakou, T. (2017). Masked autoregressive ﬂow for density

estimation. In Advances in Neural Information Processing Systems 30, pages 2335–2344.

[15] Rasmussen, C. E. and Williams, C. K. I. (2005). Gaussian Processes for Machine Learning

(Adaptive Computation and Machine Learning). The MIT Press.

[16] Rezende, D. and Mohamed, S. (2015). Variational inference with normalizing ﬂows.

In
Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings
of Machine Learning Research, pages 1530–1538.

[17] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and
In Proceedings of the 31st International

approximate inference in deep generative models.
Conference on Machine Learning, pages 1278–1286.

[18] Salimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In Proceedings of the 30th International Conference
on Neural Information Processing Systems.

9

[19] Shah, A., Wilson, A. G., and Ghahramani, Z. (2014). Student-t processes as alternatives to
gaussian processes. In Proceedings of the 17th International Conference on Artiﬁcial Intelligence
and Statistics, pages 877–885.

[20] Szabo, Z., Sriperumbudur, B., Poczos, B., and Gretton, A. (2016). Learning theory for distribu-

tion regression. Journal of Machine Learning Research, 17(152).

[21] Theis, L., van den Oord, A., and Bethge, M. (2016). A note on the evaluation of generative

models. In Proceedings of the 4th International Conference on Learning Representations.

[22] Tieleman, T. and Hinton, G. (2012). Lecture 6.5 - RmsProp: Divide the gradient by a running

average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.

[23] Vinyals, O., Bengio, S., and Kudlur, M. (2016a). Order matters: Sequence to sequence for sets.

In Proceedings of the 4th International Conference on Learning Representations.

[24] Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. (2016b). Matching
networks for one shot learning. In Advances in Neural Information Processing Systems 29, pages
3630–3638.

[25] Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for

benchmarking machine learning algorithms. arXiv preprint, abs/1708.07747.

[26] Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J.
(2017). Deep sets. In Advances in Neural Information Processing Systems 30, pages 3394–3404.

10

A Proofs

Lemma 1

Lemma 2

Given an exchangeable sequence (x1, x2, . . . , xn) of random variables xi ∈ X
mapping f :
Proof. Consider a vector function g : Rn
(x1, . . . , xn)

, the sequence (f (x1), f (x2), . . . , f (xn)) is exchangeable.
Rn such that
(z1 = f (x1), . . . , zn = f (xn)). A change of variable formula gives:

X (cid:55)→ Z

(cid:55)→

(cid:55)→

and a bijective

p(x1, x2, . . . , xn) = p(z1, z2, . . . , zn)

det J

,

∂f (xi)
where det J =
∂xi
of (x1, x2, . . . , xn) and the
|
p(z1, z2, . . . , zn). This proves that (z1, z2, . . . , zn) is exchangeable. (cid:3)

|
is the determinant of the Jacobian of g. Since both the joint probability
are invariant to the permutation of sequence entries, so must be
det J
|

n
i=1

(cid:81)

|

Given two exchangeable sequence x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) of ran-
i, j, the concatenated sequence x(cid:95)y =
dom variables, where xi is independent from yj for
∀
((x1, y1), (x2, y2), . . . , (xn, yn)) is exchangeable as well.
Proof. For any permutation π, as both sequences x and y are exchangeable we have:

p(x1, x2, . . . , xn)p(y1, y2, . . . , yn) = p(xπ(1), xπ(2), . . . , xπ(n))p(yπ(1), yπ(2), . . . , yπ(n)).

Independence between elements in x and y allows to write it as a joint distribution:

p((x1, y1), (x2, y2) . . . , (xn, yn)) = p((xπ(1), yπ(1)), (xπ(2), yπ(2)), . . . , (xπ(n), yπ(n))),

and thus the sequence x(cid:95)y is exchangeable. (cid:3)
This Lemma justiﬁes our construction with D independent exchangeable processes in the latent space
as given in A1 from Section 3.3.

B Derivation of recurrent Bayesian updates for exchangeable Student-t and

Gaussian processes

×

We assume that x = (x1, x2, . . . xn)
∈
M V Tn(ν, µ, K) with degrees of freedom ν
n

n covariance matrix K. Its density is given by:

∈

Rn follows a multivariate Student-t distribution
Rn and a positive deﬁnite
R+ \

[0, 2], mean µ

∈

p(x) =

((ν

−

Γ( ν+n
2 )
2)π)n/2Γ(ν/2) |

K

−1/2
|

(cid:18)

(x

µ)T K−1(x

µ)

1 +

−

−

ν

2

−

− ν+n
2

.

(cid:19)

(10)

Note that this parameterization of the multivariate t-distribution as deﬁned by Shah et al. [19] is
slightly different from the commonly used one. We used this parametrization as it makes the formulas
simpler.
If we partition x into two consecutive parts xa ∈

Rnb :

(cid:21)
the conditional distribution p(xb|

(cid:20)

xa
xb

M V Tn

ν,

∼

(cid:32)

(cid:20)
xa) is given by:

Rna and xb ∈
µa
µb

Kaa Kab
Kba Kbb

,

(cid:21)

(cid:20)

,
(cid:21) (cid:33)

p(xb|

xa) = M V Tnb (ν + na, ˜µb,

ν + βa −
ν + na −

2
2

˜Kbb),

(11)

where

µa) + µb

˜µb = KbaK−1
βa = (xa −
˜Kbb = Kbb −

aa (xa −
µa)T K−1
KbaK−1

aa (xa −
aa Kab.

µa)

11

Derivation of this result is given in the appendix of [19]. Let us now simplify these equations for the
case of exchangeable sequences with the following covariance structure:

K = 

v
ρ
ρ v
...
...
ρ ρ





· · ·
· · ·
. . .

· · ·

ρ
ρ
...
v



.





In our problem, we are interested in doing one-step predictions, i.e. computing a univariate density
x1:n) with parameters νn+1, µn+1, vn+1. Therefore, in Eq. 11 we can take: nb = 1, na = n,
p(xn+1|
R, Kaa = K1:n,1:n, Kab = K1:n,n+1, Kba = Kn+1,1:n and
xa = x1:n ∈
Kbb = Kn+1,n+1 = v.
Computing the parameters of the predictive distribution requires the inverse of Kaa, which we can
ﬁnd using the Sherman-Morrison formula:

Rn, xb = xn+1 ∈

K−1

aa = (A + uvT )−1 = A−1

A−1uvT A−1
1 + vT A−1u

,

−

with

with

After a few steps, the inverse of Kaa is:

v

ρ

−
0
...
0

v

ρ

0

−
...
0

· · ·
· · ·
. . .

· · ·

0
0
...
−

v



,

ρ





A = 





ρ
ρ
u = 
...
ρ











1
1
, v = 
...

1





.





K−1

aa = 

an
bn
...
bn





bn
an
...
bn

· · ·
· · ·
. . .

· · ·

bn
bn
...
an







an =

bn =

v + ρ(n
ρ)(v + ρ(n

−

2)

ρ

−
ρ)(v + ρ(n

−

−

(v

(v

−

−

,

.

1))

1))

Note that entries of K−1

aa explicitly depend on n.

Equations for the mean and variance of the predictive distribution require the following term:

KbaK−1

aa = (ρ ρ

ρ) K−1

aa =

· · ·

ρ
v + ρ(n

(cid:110)

,

1)

1:n

−

(cid:111)

which is a 1

n vector.

×

With this in mind, it is easy to derive the following recurrence:

Finally, let us derive recurrent equations for βn+1 = (xa −

aa (xa −

µa).

dn =

ρ
v + ρ(n

1)

µn+1 = (1

vn+1 = (1

−

−
dn)µn + dnxn

−
dn)vn + dn(ρ

v).

−
µa)T K −1

12

(cid:88)i(cid:54)=n

n−1

i=1
(cid:88)

= (an ˜x1 + bn

˜xi, an ˜x2 + bn

˜xi, . . . , an ˜xn + bn

˜xi)T (˜x1, ˜x2, . . . ˜xn)

Let ˜x = xa −

µa, then:

βn+1 = ˜xT K −1
aa ˜x
n

n

(cid:88)i(cid:54)=1
n
˜x2
i + bn(

n

(cid:88)i(cid:54)=2
˜xi)2.

= (an −

bn)

i=1
(cid:88)
Similarly, βn from p(xn|

i=1
(cid:88)

x1:n−1) is:

βn = (an−1 −

bn−1)

˜x2
i + bn−1(

˜xi)2

n−1

i=1
(cid:88)

βn+1 = (an −

bn)(

= (an −

bn)

βn −

n−1

n

i + ˜x2
˜x2

n) + bn(

˜xi)2

n−1
i=1 ˜xi)2
bn−1

i=1
(cid:88)
+ (an −

i=1
(cid:88)
bn−1(
an−1 −
(cid:80)
= 1, so βn+1 can be written recursively as:

n + bn(

bn)˜x2

n

i=1
(cid:88)

˜xi)2.

sn+1 = sn + ˜xn
βn+1 = βn + (an −

bn)˜x2

n + bn(s2

s2
n),

n+1 −

It is easy to show that

an−bn
an−1−bn−1

with s1 = 0.

C Implementation details

For simple datasets, such as MNIST, we found it tolerable to use models that rely upon a general
implementation of the Real NVP coupling layer similarly to Papamakarios et al. [14]. Namely,
when scaling and translation functions s and t are fully-connected neural networks. In our model,
networks s and t share the parameters in the ﬁrst two dense layers with 1024 hidden units and ELU
nonlinearity [4]. Their output layers are different: s ends with a dense layer with tanh and t ends
with a dense layer without a nonlinearity. We stacked 6 coupling layers with alternating the indices
of the transformed dimensions between odd and even as described by Dinh et al. [5]. For the ﬁrst
layer, which implements a logit transformation of the inputs, namely f (x) = logit(α + (1
2α)x),
we used α = 10−6. The logit transformation ensures that when taking the inverse mapping during
sample generation, the outputs always lie within ( −α

−

1−2α , 1−α

1−2α ).

In Omniglot, Fashion MNIST and CIFAR-10 experiments, we built upon a Real NVP model originally
designed for CIFAR-10 by Dinh et al. [6]: a multi-scale architecture with deep convolutional residual
networks in the coupling layers. Our main difference was the use of coupling layers with fully-
connected s and t networks (as described above) placed on top of the original convolutional Real
NVP model. We found that adding these layers allowed for a faster convergence and improved results.
This is likely due to a better mixing of the information before the output of the Real NVP gets into
the Student-t layer. We also found that using weight normalisation [18] within every s and t function
was crucial for successful training of large models.

The model parameters were optimized using RMSProp [22] with a decaying learning rate starting
from 10−3. Trainable parameters of a
were updated with a 10x smaller learning rate and
were initialized as following: νd = 1000, vd = 1., ρd = 0.1 for every dimension d. The mean µd
was ﬁxed at 0. For the Omniglot model, we used a batch size of 32, sequence length of 20 and trained
for 200K iterations. The other models were trained for a smaller number of iterations, i.e. ranging
from 50K to 100K updates.

T P

GP

or

13

D Sampling from a Student-t distribution

Algorithm 1 Efﬁcient sampling on GPU from a univariate t-distribution with mean µ, variance v and
degrees of freedom ν

function sample(µ, v, ν)

(0, 1)
← U
min(a, b)
max(a, b)
2πc
r
cos(α)

a, b
c
r
α
t

←
←
←
←

ν−2
v
σ
(cid:112)
ν
return µ + σt
(cid:113)

←

(cid:0)
end function

(cid:1)

(ν/r2)(r−4/ν

1)

−

E Sample analysis

In Figure 3, which includes Figure 2 from the main text, we want to illustrate how sample variability
depends on the variance of the inputs. From these examples, we see that in the case of a repeated
input image, samples get more coherent as the number of conditioning inputs grows. It also shows
that BRUNO does not merely generate samples according to the inferred class label.

While Omngilot is limited to 20 images per class, we can experiment with longer sequences using
CIFAR-10 or MNIST. In Figure 4 and Figure 5, we show samples from the models trained on those
datasets. In Figure 6, we also show more samples from the prior distribution p(x).

Figure 3: Samples generated conditionally on images from an unseen Omniglot character class. Left:
input sequence of 20 images from one class. Right: the same image is used as an input at every step.

x1:n) for every n = 480, . . . , 500. Left: input sequence
Figure 4: CIFAR-10 samples from p(x
|
(given in the top row of each subplot) is composed of random same-class test images. Right: same
image is given as input at every step. In both cases, input images come from the test set of CIFAR-10
and the model was trained on all of the classes.

14

x1:n) for every n = 480, . . . , 500. Left: input sequence (given
Figure 5: MNIST samples from p(x
|
in the top row of each subplot) is composed of random same-class test images. Right: same image is
given as input at every step. In both cases, input images come from the test set of MNIST and the
model was trained only on even digits, so it did not see digit ‘1’ during training.

Figure 6: Samples from the prior for the models trained on Omniglot, CIFAR-10, Fashion MNIST
and MNIST (only trained on even digits).

F Parameter analysis

After training a model, we observed that a majority of the processes in the latent space have low
correlations ρd/vd, and thus their predictive distributions remain close to the prior. Figure 7 plots
the number of dimensions where correlations exceed a certain value on the x-axis. For instance,
MNIST model has 8 dimensions where the correlation is higher than 0.1. While we have not veriﬁed
it experimentally, it is reasonable to expect those dimensions to capture information about visual
features of the digits.

Figure 7: Number of dimensions where ρd/vd > (cid:15) plotted on a double logarithmic scale. Left:
Omniglot model. Middle: CIFAR-10 model Right: Non-convolutional version of BRUNO trained on
MNIST.

T P

-based models, degrees of freedom νd for every process in the latent space were intialized to
For
1000, which makes a
. After training, most of the dimensions retain fairly high
degrees of freedom, but some can have small ν’s. One can notice from Figure 8 that dimensions with
high correlation tend to have smaller degrees of freedom.

close to a

T P

GP

15

Figure 8: Correlation ρd/vd versus degrees of freedom νd for every d. Degrees of freedom on the
x-axis are plotted on a logarithmic scale. Left: Omniglot model. Middle: CIFAR-10 model Right:
Non-convolutional version of BRUNO trained on MNIST.

We noticed that exchangeable
rameters even when
case.

T P

s can behave differently for certain settings of hyperpa-
s have high degrees of freedom. Figure 9 gives one example when this is the

s and

T P

GP

Figure 9: A toy example which illustrates how degrees of freedom ν affect the behaviour of a
T P
compared to a
. Here, we generate one sequence of 100 observations from an exchangeable
GP
multivariate normal disribution with parameters µ = 0., v = 0.1, ρ = 0.05 and evaluate predictive
probabilities under an exchangeable
models with parameters µ = 0., v = 1., ρ = 0.01
and different ν for

T P
s in the left and the right plots.

and

GP

T P

G Training of

and

-based models

T P

GP
on top, we found that these two versions of
When jointly optimizing Real NVP with a
s the convergence was
BRUNO occasionally behave differently during training. Namely, with
harder to achive. We could pinpoint a few determining factors: (a) the use of weightnorm [18] in the
Real NVP layers, (b) an intialisation of the covariance parameters, and (c) presence of outliers in the
training data. In Figure 10, we give examples of learning curves when BRUNO with
s tends not
to work well. Here, we use a convolutional architecture and train on Fashion MNIST. To simulate
outliers, every 100 iterations we feed a training batch where the last image of every sequence in the
batch is completely white.

or a

T P

GP

GP

GP

We would like to note that there are many settings where both versions of BRUNO diverge or
they both work well, and that the results of this partial ablation study are not sufﬁcient to draws
general conclusions. However, we can speculate that when extending BRUNO to new problems, it is
reasonable to start from a
-based model with weightnorm, small initial covariances, and small
learning rates. However, when ﬁnding a good set of hyperparameters is difﬁcult, it might be worth
trying the

GP
-based BRUNO.

T P

16

and

T P

GP
(0.1, 0.95) for every dimension. Here, the

Figure 10: Negative log-likelihood of
-based BRUNO on the training batches, smoothed
using a moving average over 10 points. Left: not using weightnorm, initial covariances are sampled
from
P -based model diverged after a few hundred
iterations. Adding weighnorm ﬁxes this problem. Middle: using weightnorm, covariances are
initialised to 0.1, learning rate is 0.002 (two times the default one). In this case, the learning rate is
-based model suffers from it more. Right: using weightnorm,
too high for both models, but the
covariances are initialised to 0.95.

GP

U

G

H Set anomaly detection

Online anomaly detection for exchangeable data is one of the application where we can use BRUNO.
This problem is closely related to the task of content-based image retrieval, where we need to rank
an image x on how well it ﬁts with the sequence x1:n [9]. For the ranking, we use the probabilistic
score proposed in Bayesian sets [8]:

score(x) =

x1:n)

p(x
|
p(x)

.

(12)

When we care exclusively about comparing ratios of conditional densities of xn+1 under different
sequences x1:n, we can compare densities in the latent space
instead. This is because the Jacobian
from the change of variable formula does not depend on the sequence we condition on.

Z

For the following experiment, we trained a small convolutional version of BRUNO only on even
MNIST digits (30,508 training images). In Figure 11, we give typical examples of how the score
evolves as the model gets more data points and how it behaves in the presence of inputs that do not
conform with the majority of the sequence. This preliminary experiment shows that our model can
detect anomalies in a stream of incoming data.

Figure 11: Evolution of the score as the model sees more images from an input sequence. Identiﬁed
outliers are marked with vertical lines and plotted on the right in the order from top to bottom. Note
that the model was trained only on images of even digits. Left: a sequence of digit ‘1’ images with
one image of ‘7’ correctly identiﬁed as an outlier. Right: a sequence of digit ‘9’ with one image of
digit ‘5’.

17

I Model samples

Figure 12: Samples from a model trained on Omniglot. Conditioning images come from character
classes that were not used during training, so when n is small, the problem is equivalent to a few-shot
generation.

18

Figure 13: Samples from a model trained on CIFAR-10. The model was trained on the set with 10
classes. Conditioning images in the top row of each subplot come from the test set.

Figure 14: Samples from a convolutional BRUNO model trained on Fashion MNIST. The model was
trained on the set with 10 classes. Conditioning images in the top row of each subplot come from the
test set.

19

Figure 15: Samples from a non-convolutional model trained on MNIST. The model was trained on
the set with 10 classes. Conditioning images in the top row of each subplot come from the test set.

20

