Interpretable Structure Induction Via Sparse Attention

Ben Peters† Vlad Niculae† and Andr´e F.T. Martins†‡
†Instituto de Telecomunicac¸ ˜oes, Lisbon, Portugal
‡Unbabel, Lisbon, Portugal
benzurdopeters@gmail.com, vlad@vene.ro, andre.martins@unbabel.com.

1

Introduction

Neural network methods are experiencing wide
adoption in NLP, thanks to their empirical per-
formance on many tasks. Modern neural ar-
chitectures go way beyond simple feedforward
and recurrent models: they are complex pipelines
that perform soft, differentiable computation in-
Inspired by pioneering
stead of discrete logic.
work by, e.g. Kohonen et al. (1981); Das et al.
(1992); Schmidhuber (1992), such modern dif-
ferentiable architectures include neural memories
(Sukhbaatar et al., 2015) and attention mecha-
nisms (Bahdanau et al., 2015). The price of such
soft computing is the introduction of dense depen-
dencies, which make it hard to disentangle the pat-
terns that trigger a prediction. Our recent work on
sparse and structured latent computation (Mar-
tins and Astudillo, 2016; Niculae and Blondel,
2017; Niculae et al., 2018; Malaviya et al., 2018)
presents a promising avenue for enhancing inter-
pretability of such neural pipelines. Through this
extended abstract, we aim to discuss and explore
the potential and impact of our methods.

The principle of parsimony suggests that sim-
pler explanations are more plausible and inter-
pretable. Our perspective is similar to prior
work on regularizing model weights (Hastie et al.,
2015), but with a twist: instead of model sparsity
that tells us which “static” groups of variables are
relevant for a task, we now have a “dynamic” form
of sparsity that tells us, for a particular input ob-
ject, where we should attend to produce a decision.

• sparsity:

shrinking probabilities to zero to
prune entire parts of the input when explaining
a prediction (Martins and Astudillo, 2016);

• constraints: constraining probabilities within
lower and upper bounds,
to prevent words
from receiving too much or too little attention
(Malaviya et al., 2018);

• structure:

learning latent structure predictors
(e.g. aligners or parsers), to induce a compact
representation as a small, interpretable set of
global structures (Niculae et al., 2018).

2 Attention Mechanisms

The key background for our work is the concept
of attention. Attention mechanisms and mem-
ory networks are able to “point” to relevant items
(e.g. words or pixels) that determine the ﬁnal pre-
diction, approximating a discrete choice (argmax)
with a soft, differentiable one (softmax). Let H =
[h1, . . . , hL] ∈ RD×L be a matrix whose columns
are vectors encoding the L different choices (for
example, words in a sentence). An attention mech-
anism maps a H and a control state s to a proba-
bility distribution p ∈ △L over the L choices.1
This can be split into (i) generating scores for
each choice, e.g., zi = v⊤tanh(Whi + Us)
for i ∈ {1, . . . , L} and (ii) mapping the scores
to a probability distribution. Common attention
uses (Bahdanau et al., 2015; Luong et al., 2015)
p = softmax(z), i.e., pi = exp(zi)/Pj exp(zj).
Since softmax is strictly positive, this leads to
dense probability distributions. However, putting
nonzero weight on every choice is not ideal for in-
terpretability (Fig. 1, center); instead, we explore
sparse selection, identifying a small set of choices
responsible for a prediction. Niculae and Blondel
(2017) proposed the general family

ΠΩ(z) = argmax

z⊤p − Ω(p),

(1)

p∈△L

• regularization:

injecting prior assumptions,
such as that neighbouring words should be fused
together (Niculae and Blondel, 2017);

recovering softmax for Ω(p) = − Pj pj log pj.
= {p ∈ RL

1We denote by △

L
i=1 pi = 1, pi ≥

| P

L

0, ∀i} the (L − 1)-dimensional probability simplex.

Figure 1: Attention weights for a sequence-to-sequence sentence compression instance. Traditional softmax attention (middle)
yields dense weights, which are less interpretable than the sparse weights from sparsemax (right) or fusedmax (left); the latter
further enhances interpretability by clustering probabilities of adjacent words. Image courtesy of Niculae and Blondel (2017).

Sparse attention. Martins and Astudillo (2016)
proposed sparsemax, which replaces softmax with
a Euclidean projection, remaining differentiable
while also yielding sparse probabilities. This can
2 in Eqn 1. The
be obtained by setting Ω =
resulting probabilities are substantially more inter-
pretable, as the contribution of irrelevant words is
now shrunk to exactly 0 (Fig. 1, right).

1
2 k · k2

Regularized attention. Parsimony goes beyond
sparsity: prior assumptions may encourage se-
lecting groups or clusters with equal proba-
bility.
Niculae and Blondel (2017) propose
two linguistically-motivated regularized attention
mechanisms: fusedmax, which tends to group ad-
jacent words together, and oscarmax, which may
cluster non-adjacent words, suitable for languages
with ﬂexible word order. Such mechanisms can
select interpretable segments (Fig. 1, left).

Constrained attention. Some forms of parsi-
mony must be strictly enforced using constraints,
rather than simply encouraged via regulariza-
tion. One such constraint is to add an upper
bound to the cumulative attention an input vari-
able may receive. This can be done using con-
strained softmax (Martins and Kreutzer, 2017)
or its sparse analogue, constrained sparsemax
(Malaviya et al., 2018). Constraining attention
weights can be interpreted as specifying the fertil-
ity (Brown et al., 1993) of the alignments between
the source and target, in machine translation.

a
police
ofﬁcer
watches
a
situation
closely
.

(a) sequence

(b) matching

Figure 2: Structured alignment on SNLI (Niculae et al.,
2018). The premise is on the y-axis, the hypothesis on the
x-axis. Sequential alignment encourages monotonic align-
ments, matching induces a single symmetrical alignment.

Allowing hidden layers to output structured repre-
sentations can be valuable for modelling perspec-
tive but also for interpretability: discrete structures
provide organized representations, in contrast to
unstructured vectors of neuron activations.

SparseMAP (Niculae et al., 2018) allows han-
dling discrete structures within end-to-end differ-
entiable neural networks, able to automatically se-
lect only a few global structures. On natural lan-
guage inference, for a word-to-word alignment
joint attention mechanism, SparseMAP can induce
structured alignments as illustrated in Fig. 2.

3 Structured Attention

4 Conclusion

In this section, we consider combinatorial repre-
sentations. Across application domains, but es-
pecially in NLP, many objects of interest can be
represented by such structures: syntactic and de-
pendency trees, sequential labellings, alignments.

Building upon the principle of parsimony, we pro-
pose sparse, regularized, constrained and struc-
tured hidden layers. We seek to discuss the poten-
tials of these strategies with an expert community
on black-box interpretability.

Acknowledgments

This work was supported by the European Re-
search Council (ERC StG DeepSPIN 758969) and
by the Fundac¸ ˜ao para a Ciˆencia e Tecnologia
through contract UID/EEA/50008/2013.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. of ICLR.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311.

Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1992.
Learning context-free grammars: Capabilities and
limitations of a recurrent neural network with an ex-
ternal stack memory. In Proceedings of The Four-
teenth Annual Conference of Cognitive Science So-
ciety. Indiana University, page 14.

Trevor Hastie, Robert Tibshirani, and Martin Wain-
Statistical Learning with Spar-
wright. 2015.
sity: The Lasso and Generalizations. Chapman &
Hall/CRC.

Teuvo Kohonen, Erkki Oja, and Pekka Lehtio. 1981.
Storage and processing of information in distributed
associative memory systems. Parallel Models of As-
sociative Memory, pages 129–167.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proc. of EMNLP.

Chaitanya Malaviya, Pedro Ferreira, and Andr´e F. T.
Martins. 2018. Sparse and constrained attention for
neural machine translation. In Proc. of ACL.

Andr´e F. T. Martins and Ram´on Astudillo. 2016. From
softmax to sparsemax: A sparse model of attention
and multi-label classiﬁcation. In Proc. of ICML.

Andr´e FT Martins and Julia Kreutzer. 2017. Learn-
ing what’s easy: Fully differentiable neural easy-ﬁrst
taggers. In Proc. of EMNLP, pages 349–362.

Vlad Niculae and Mathieu Blondel. 2017. A regular-
ized framework for sparse and structured neural at-
tention. In Proc. of NIPS.

Vlad Niculae, Andr´e F. T. Martins, Mathieu Blondel,
and Claire Cardie. 2018. SparseMAP: Differen-
tiable sparse structured inference. In Proc. of ICML.

J¨urgen Schmidhuber. 1992. Learning to control fast-
weight memories: An alternative to dynamic recur-
rent networks. Neural Computation, 4(1):131–139.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Proc. of NIPS.

