Omnivore: An Optimizer for Multi-device
Deep Learning on CPUs and GPUs

Stefan Hadjis
Dept. of Computer Science
Stanford University
Email: shadjis@stanford.edu

Ce Zhang
Dept. of Computer Science
ETH Zurich
Email: ce.zhang@inf.ethz.ch

Ioannis Mitliagkas, Dan Iter, Christopher R´e
Dept. of Computer Science
Stanford University
Email: {imit, daniter, chrismre}@stanford.edu

6
1
0
2
 
t
c
O
 
9
1
 
 
]

C
D
.
s
c
[
 
 
4
v
7
8
4
4
0
.
6
0
6
1
:
v
i
X
r
a

Abstract—We study the factors affecting training time in
multi-device deep learning systems. Given a speciﬁcation of a
convolutional neural network, our goal is to minimize the time
to train this model on a cluster of commodity CPUs and GPUs.
We ﬁrst focus on the single-node setting and show that by using
standard batching and data-parallel techniques, throughput can
be improved by at least 5.5× over state-of-the-art systems on
CPUs. This ensures an end-to-end training speed directly propor-
tional to the throughput of a device regardless of its underlying
hardware, allowing each node in the cluster to be treated as a
black box. Our second contribution is a theoretical and empirical
study of the tradeoffs affecting end-to-end training time in a
multiple-device setting. We identify the degree of asynchronous
parallelization as a key factor affecting both hardware and
statistical efﬁciency. We see that asynchrony can be viewed as
introducing a momentum term. Our results imply that tuning
momentum is critical in asynchronous parallel conﬁgurations,
and suggest that published results that have not been fully tuned
might report suboptimal performance for some conﬁgurations.
For our third contribution, we use our novel understanding of
the interaction between system and optimization dynamics to
provide an efﬁcient hyperparameter optimizer. Our optimizer
involves a predictive model for the total time to convergence
and selects an allocation of resources to minimize that time.
We demonstrate that the most popular distributed deep learning
systems fall within our tradeoff space, but do not optimize within
the space. By doing this optimization, our prototype runs 1.9×
to 12× faster than the fastest state-of-the-art systems.

I. INTRODUCTION

In recent years, deep learning has provided signiﬁcant
improvements in quality for a number of data-driven appli-
cations [1]–[4]. An important aspect of deep learning is that
quality improves with the amount of data we can process;
therefore, advances in system efﬁciency and scalability directly
improve quality. This has led to an arms race of distributed
deep learning systems both in industry (e.g., Google’s Dist-
Belief [3], Microsoft’s Adam [5]) and in academia [6]–[9].

Despite this proliferation of deep learning systems, there
have been few studies of deep learning from a data-systems
perspective. Each of these systems makes a set of design
decisions that may not work for other tasks or hardware
settings. In our experience working with multiple Ph.D.-
level users of these systems—including experts in pathology,
radiology, computer vision, and the energy sector—it is often
very difﬁcult even for advanced users to make these design
decisions themselves. It is not uncommon for a suboptimal

design choice to result in an end-to-end runtime that is an
order of magnitude slower than what is possible. Moreover,
most systems provide no way of automatically selecting an
optimal conﬁguration, placing this burden on the user. This
also contributes to two debates about deep learning systems.

Debate 1: CPU vs. GPU. There has been a long debate about
CPUs vs. GPUs for deep learning. GPUs are popular for CNN
systems because of the high throughput they provide, but they
contain smaller off-chip memories. Microsoft’s Adam argues
that CPUs can deliver more cost-effective performance [5]. For
users who cannot control their data-center hardware, Amazon’s
EC2 provides GPUs but Google Compute does not.

Debate 2: Synchronous vs. Asynchronous Training. An-
other debate is about the synchronization strategies to use in
multi-device deep learning. For example, Google’s latest Ten-
sorFlow paper [10] comes out in support of fully synchronous
methods, citing recent papers [11], [12]. Other systems, such
as DistBelief [3], Project Adam [5], H2O [13], and recent
theoretical efforts [14] focus on asynchronous training and
argue that it is more scalable than fully synchronous strategies.

In this paper, we perform a ﬁrst study of the design space for
deep learning systems. We identify the key tradeoffs for single-
device and multi-device systems, providing insights into the
above debates. We ﬁnd that the CPU implementation of many
state-of-the-art systems misses textbook batching optimiza-
tions, which can make the CPU implementation at least 5.5×
faster. We also see that when the momentum parameter [15] is
tuned, there is no penalty for asynchrony—except in a short,
cold-start training period. This previously unknown connection
between asynchrony and momentum was surprising even to
leading groups in the area. Recent work does not tune momen-
tum [11], [12] and reports that asynchronous conﬁgurations
are slower. We see on different systems, including TensorFlow,
that tuning changes this outcome: asynchronous conﬁgurations
are faster when we tune. These theoretical and experimental
insights help us understand the tradeoff space better and build
an automatic optimizer to choose the optimal conﬁguration. As
a result, our prototype system, Omnivore, can be 1.9× to 12×
faster than the fastest competitor systems. Our results have
attracted interest from major companies. In collaboration with
a major chip manufacturer, we are integrating our optimizers
on new platforms of much larger scale.

Overview of Technical Contributions

To conduct our study, we develop a prototype distributed

system called Omnivore.1 We make three contributions.

Scope of Our Study: We focus on perhaps the most
popular deep learning models, convolutional neural networks
(CNNs), which are state-of-the-art for a wide range of applica-
tions (e.g., image processing, video analysis, drug discovery).
Our study answers the following question: “Given a cluster
(e.g., X machines, Y GPUs, Z CPUs, etc.), how do I train my
CNN as quickly as possible?”. We assume that the following
are given: (i) a deep learning model (network architecture),
(ii) a dataset for training this model, and (iii) a set of
computational resources (a number of devices on machines,
their throughput, and the network speed).2 We then study
how to minimize the total training time. We build a complete
prototype capable of training the most popular deep learning
models. This allows us to hone in on two major choices: (i)
how to use hardware on each node and (ii) the degree to
which asynchrony can be tolerated. Our work demystiﬁes these
factors by identifying the key tradeoffs that underlie all design
decisions, providing theoretical guidance about asynchrony,
and quantifying the impact of those tradeoffs experimentally.
Contribution 1: Single-Device Optimizations: We show
that it is possible to achieve throughput proportional to the
maximum FLOPS of both CPUs and GPUs. This is not trivial;
while state-of-the-art systems achieve GPU speeds propor-
tional to the device throughput, existing CPU implementations
can be sped up signiﬁcantly compared to what is reported
in the literature. Our study builds on two key optimizations
we reported in a workshop [16]. We use batching and data-
parallel optimizations—not employed in other systems—to
achieve end-to-end speedups of more than 5.5× over state-of-
the-art systems on commodity CPUs. Such optimizations are
not always possible on the GPU, but by selecting this strategy
for the CPU, we now achieve speeds proportional to the peak
throughput. This allows us to build a simpler optimizer by
modeling CPUs and GPUs as black boxes.

Contribution 2: Multi-device Tradeoffs: Our second con-
tribution is an empirical study of factors affecting training
time for multi-device deep learning training. We analyze the
decisions made on existing systems and ﬁnd that while they are
diverse, the strategies of the most popular systems fall within
a tradeoff space deﬁned by two fundamental dimensions [3],
[5]–[9]: (i) the server architecture, or how the layers of a
CNN map to devices; and (ii) the execution strategy, or how
batches of data are mapped to machines for processing. We
develop a simple framework that allows us to model each of
these approaches. Devices are organized in compute groups.
Each compute group is responsible for a single batch of
data per iteration. Inside a group, the computation occurs
in standard, synchronous steps. Across groups, computation
happens asynchronously.

1 https://github.com/HazyResearch/Omnivore
2 We only do basic network optimization, and we assume that machines are connected

by a uniform and fast topology, e.g., if they were housed on the same rack.

We study the impact of asynchrony on the end-to-end
performance of training deep learning systems. Not surpris-
ingly, the more asynchrony there is, the faster the system is
for each iteration, which we call hardware efﬁciency. This
happens because there is less coordination between workers.
The challenge is to understand how asynchrony affects the
number of iterations to converge, which we call statistical
efﬁciency. We provide novel understanding of the factors that
affect statistical efﬁciency. Empirically, we observe that the
optimal value for the momentum parameter decreases as we
Indeed, our
increase the number of asynchronous workers.
theory in [17] shows that asynchronous-parallel training can be
viewed as a synchronous update but with an increased, implicit
momentum term. Furthermore, if the optimal momentum value
for a problem is above the implicit momentum, then there is no
penalty for running asynchronously, as long as the momentum
is properly tuned. Although momentum is explicitly (i.e.,
algorithmically) introduced in almost every system [15], we
are the ﬁrst to realize this connection between asynchrony and
momentum. Here, we validate this experimentally: we see that,
by properly tuning momentum, asynchronous methods can be
at least 1.5–2.5× faster than using the standard momentum
value of 0.9, which is used in most existing work. This
understanding of statistical efﬁciency, along with an analytical
hardware efﬁciency model, forms a novel system tradeoff
space.

Contribution 3: Simple Automatic Optimizer: Based on
the intuition behind our
our theory and empirical study,
optimizer is very simple: pick the highest degree of asynchrony
such that
the implicit momentum induced by asynchrony
is below the optimal momentum. Given a ﬁxed number of
compute groups (which control the degree of asynchrony), we
grid-search the parameters for learning rate and momentum by
measuring the statistical and hardware efﬁciency for minutes
(less than 10% of the time to train a network). If the best-
the optimizer chooses this
found momentum is non-zero,
conﬁguration. If it is zero, we assume that there could be
a better setting with fewer compute groups. Our optimizer is
able to choose a near-optimal point in the tradeoff space, and
we demonstrate that our system achieves end-to-end speedups
of 1.9× to 12× on popular CNN workloads compared to state-
of-the-art tools that choose suboptimal tradeoff points. We
compare our simple optimizer with a state-of-the-art Bayesian
optimization approach [18]. Both approaches are able to reach
the same ﬁnal accuracy (within 1%), but the Bayesian strategy
takes almost 6× as long. We can also apply our optimizer to
other deep learning systems. In some cases, this prevents those
other tools from diverging, while in other cases, it speeds them
up by 7×.

Outline: We present background in Section II. Section III
and Section IV introduce the tradeoff space related to single-
machine and multi-device settings, respectively. Section V
describes the optimizer for making decisions in this tradeoff
space. We validate our results in Section VI, discuss related
work in Section VII, and conclude in Section VIII.

II. BACKGROUND

A. Convolutional Neural Networks (CNNs)

A convolutional neural network (CNN, [2]) consists of
layers L1, L2, . . . , LP . Each layer is an operator which takes
as input a 3D data tensor D ∈ Rn×n×din and transforms
to a resulting 3D data tensor R ∈ Rm×m×dout,
it
i.e.
LF W
(Dp) = Rp. F W indicates the layer running in the
p
“forward” direction to transform D into R. Layers have a
second operation, backward or BW , described later. Often,
D1, the input to the ﬁrst layer L1, is an image I ∈ Rn×n×3,
where 3 represents the RGB color channels.

For layers after L1 (the input layer), the input tensor Dp
comes from the output of a prior layer (usually Dp = Rp−1),
such that the CNN layers are cascaded to deﬁne a composite
operation (boldface highlights inputs and outputs)

RP = LF W

P

◦ LF W

P −1 ◦ ... ◦ LF W

2

◦ LF W
1

(I)

(1)

The ﬁnal result RP is the CNN’s prediction for image I.
For example, if the task is image classiﬁcation with 1000
categories, the tensor RP ∈ R1×1×1000 is a vector containing
the probability of each category. This prediction is then
compared to C, the true classiﬁcation for I, using a loss
function (cid:96)(RP , C) that evaluates the quality of the prediction.
A lower loss indicates a better prediction.

Many types of layers exist in a CNN. Some layers perform
a pre-deﬁned transformation such as downsampling while
other layers contain a model, W, and perform an operation
parameterized by the model. Models are also known as weights
or parameters. The models of all layers constitute the entire
set of weights or parameters of the CNN, i.e.,

W = WCNN = {WL1 , . . . , WLP }.

B. Stochastic Gradient Descent

The goal of CNN training is to optimize the model W in
order to minimize the loss function (cid:96)(RP , C), also denoted as
(cid:96)(W, I, C) to make the fact that RP is a function of W and I
explicit. Low loss is correlated with high prediction accuracy
and in this work we refer to both. The most popular training
algorithm for CNNs is an iterative technique called stochastic
gradient descent (SGD). Each SGD iteration consists of a
forward and backward pass.

The input to each SGD iteration is an image-label tuple
(I, C) as described above. The forward pass calculates the
prediction RP of I using equation (1), and then the prediction
error compared to C is used to calculate the gradient (or
derivative) of (cid:96) with respect to RP . We denote this gradient
as ∇RP (cid:96). Now the cascade of equation (1) runs in reverse by
applying each layer in the “backward” direction:

LBW
1

◦ LBW
2

◦ . . . ◦ LBW

P −1 ◦ LBW

P

(∇RP(cid:96))

(2)

equation (2) implements the chain rule of calculus. The BW
operation of layer p takes as input a data gradient ∇Rp (cid:96) and
outputs a data gradient ∇Dp (cid:96). Internally, it also updates that
layer’s model WLp by (i) calculating a gradient of the loss
with respect to the model, ∇WLp (cid:96), and (ii) using an SGD

Fig. 1. Abstracting a CNN into two phases.

update on the model. SGD repeats for many, often millions
of iterations, until the loss is sufﬁciently low, i.e. the model
is sufﬁciently optimized. The initial model W (0) is randomly
initialized. The SGD update at step t takes the form

W (t) ← W (t−1) + V (t),

(3)

where the new step, V (t), consists of a scaled version of the
previous step, V (t−1), plus a gradient calculated with (I, C):
(cid:16)

(cid:17)

(cid:104)

V (t) ← µV (t−1) −η

∇W (cid:96)

W (t−1), I, C

+ λW (t−1)(cid:105)

(4)

In Section IV we introduce the notion of asynchronous up-
dates. The main change in equation (4) under asynchrony,
is the use of an older model, W (s), when evaluating the
gradient ∇W (cid:96). This gradient (speciﬁcally, its negative) is the
direction to “step” within the parameter space each SGD
iteration. The learning rate, η, is the scale factor applied to the
magnitude of this gradient, i.e. the size of the step. λ dictates
the amount of regularization, which is an input to the training
problem (part of the CNN model to train). µ is the amount
of explicit momentum we add to the update. Momentum is
used to “accelerate” learning in the directions common to each
gradient by keeping a history of past gradients and adding this
history to the gradient of the current step, with past gradients
decayed exponentially. Commonly, in order to produce more
stable gradients, each SGD iteration does not process a single
tuple (I, C), but a batch of b tuples, e.g., 256, in which case D
and R become 4D. The gradients from each tuple are summed
to produce a single, combined gradient for that iteration.

Selecting the right values for hyperparameters (η, µ, b) is
critical for performance. We will describe a simple optimizer
to pick these parameters.

C. CNN Computation

Of all the CNN layers, two layers are the most compu-
tationally intensive, convolutional (conv) and fully-connected
(FC) layers. A convolutional layer performs many independent
convolutions over an image, i.e., several sliding window oper-
ations; an FC layer performs a dense matrix multiplication. In
many popular implementations, the bottleneck in both layers
is a matrix multiply implemented as a call to either a BLAS
or cuBLAS library. For our study, their data properties are
more important, and we refer to Chetlur et al. [19] for a more
detailed description of their use in machine learning.

that convolution layers repeat

Figure 1 illustrates a state-of-the-art CNN, in which all
convolutional layers always appear before all fully-connected
layers. In this work we introduce an abstraction which sepa-
rates a CNN into two phases, each consisting of a number of
consecutive layers: ﬁrst the convolution phase (conv), whose
layers have large data tensors D and R (e.g., 100MB-1GB)
and small models W (e.g., 5-50 MB), followed by the fully-
connected phase (FC), whose layers have small D and R (e.g.,
1-10MB) and large W (e.g., 30-300 MB). The reduction in
data size is in part due to pooling layers in the convolution
phase which perform down-sampling. The increase in model
size is due to the fact
the
same weights for the sliding window. Note that our two-phase
categorization also applies to modern, non-linear CNNs [20].
The computation for both the conv and FC phases is usually
compute-bound although the conv phase contains signiﬁcantly
more computation (e.g., in AlexNet conv is 1.6 TFLOPs and
FC is 80 GFLOPs, or 95% of the computation is convolution).
Within a machine, each layer’s computation can be mapped
to CPUs, GPUs, or a combination of both. In addition, this
computation can be parallelized either using data parallelism
(partitioning the data batch and replicating the model, which
works well for the conv layers) or model parallelism (par-
titioning the model and replicating the data batch, which
works well for the FC layers). In distributed settings, the layer
computations are mapped across machines. We will show later
this mapping is always done at the coarser granularity of entire
phases (conv or FC) because it reduces network delays due to
the distinct model and data sizes of the two phases. We study
the choices of mapping and parallelization techniques for both
single and multiple devices in Section III and IV.

D. Problem Deﬁnition

We study systems tradeoffs to build an optimizer for the
most widely used networks/algorithms. We focus on SGD
due to its popularity, although our optimizer applies to other
algorithms as well. More precisely, we are given as input
the following: (i) a CNN architecture {L1, ..., LP }, including
regularization, (ii) a dataset D consisting of data batches,
(iii) a device graph G in which vertices are hardware devices
(speciﬁed by their throughput) and edges are communication
speeds between devices. Our goal is to design an optimizer
which creates a plan for physical mapping and execution
strategy in order to train as quickly as possible.

A plan for physical mapping maps the computation (both
FW and BW) of each layer to vertices (e.g., GPUs or CPU
cores) of the device graph. A plan for the execution strategy
maps data batches to devices in order to parallelize SGD in
the multi-device case. The key decision here is the degree of
asynchrony in execution. Section III and Section IV study how
to do physical mapping within a device and across devices,
respectively. Section IV also studies the impact of asynchrony.
If the cluster is homogeneous, we do not need the explicit
device graph – instead, a few parameters, such as the number
of devices and the throughput of each device, are enough to
specify the cluster for the optimizer.

Fig. 2. The computation performed by a convolutional layer which processes
bp images at a time in parallel, where 1 ≤ bp ≤ batch size. The convolutions
can be implemented either directly (top) or using the lowering/GEMM method
(bottom).

III. SINGLE-DEVICE TRADEOFF

that

is proportional

We ﬁrst study the systems tradeoffs within a single device.
We show that for each device (GPU or CPU) we can achieve
throughput
to its peak FLOPS. This
enables the distributed optimizer to treat each device as a
black box in Section IV. This is not a trivial property for deep
learning: many existing systems [21]–[23] use either CPUs
or GPUs, but they often report that GPU implementations
are an order of magnitude faster than CPU even when the
devices offer similar FLOPS. Therefore the challenge is to
utilize the FLOPS on the CPU. We study the key kernel in
CNN implementations, which is compute bound. We introduce
a data batching technique which trades off memory footprint
for compute time and demonstrate that this tradeoff gives
a more than 5× CPU speedup over existing systems. With
this optimization now both CPUs and GPUs give throughput
proportional to the FLOPS offered by the device.

A. Convolutional Layer Computation

As reported in the literature and conﬁrmed by our ex-
periments, the most computationally intensive layers in the
CNN are the convolutional layers. Together, all convolutional
layers in a CNN often consume between 70-90% of total
execution time. Recall
layer contains
a model, which we will also call its kernel and denote as
K. A convolutional layer accepts as input a 4D data tensor
D ∈ Rn×n×din×b, where recall b is the batch size. K is also
a 4D tensor, K ∈ Rk×k×din×dout. The output is a 4D data
tensor R ∈ Rm×m×dout×b, where:

that a convolutional

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Rx,y,z,w =

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48),wKx(cid:48),y(cid:48),d(cid:48),z

(5)
Like most HPC kernels a straightforward implementation is
suboptimal, and many optimized implementations of this con-
volution kernel exist [19], [21], [24]. A popular implementa-
tion is to perform equation (5) as a dense matrix multiplication

Device Type
(EC2 Instance)
1× CPU Xeon E5-2666
(c4.4xlarge)
2× CPU Xeon E5-2666
(c4.8xlarge)
1× GPU Grid K520
(g2.2xlarge)
Dual-GPU Grid K520
(g2.8xlarge)

Device % Peak

Caffe Omnivore

% Peak % Peak
SGEMM

GFLOPS

742

18%

1,670

1,229

2,458

8%

53%

26%

56%

40%

54%

52%

81%

71%

99%

99%

Fig. 3. Across several CPUs and GPUs we obtain throughput on convolution
layers that is ∼ 50% of the device peak (shown for AlexNet, total F W +BW
time for all conv layers). We also show large GEMM as a reference of what
the device can achieve.

(also known as GEMM, general matrix multiply), which [19]
demonstrates to be versatile and fast for a range of size
parameters, as GEMM kernels are highly optimized.

the data and model

In order for equation (5) to be carried out as a matrix mul-
tiplication an initial reformatting and replication step called
lowering is required to put
into the
correct format. Figure 2 shows the three logical steps in the
lowering process: (i) lowering, which transforms 3D tensors
D and K into 2D matrices ˆD and ˆK; (ii) matrix multiply,
which multiplies ˆD ˆK to get the result ˆR; and (iii) lifting,
which transforms ˆR back to a tensor representation of R. The
lifting step is fast (reformatting), but the lowering step requires
replication of the data, sometimes by a factor of 1 or 2 orders
of magnitude. This blowup in the data size demands more
off-chip memory and more computation in step (ii).

B. Batching and Data Parallelism

The design tradeoff between CPUs and GPUs arises as
a result of this increase in data size. Assume that we are
given a ﬁxed batch size of b images (e.g., b = 256). GPUs
cannot ﬁt an entire batch of lowered data into off-chip memory,
therefore many CNN implementations on GPUs perform low-
ering/GEMM serially on one or few images at a time until all
b have been processed. On the CPU however, off-chip memory
is larger which allows lowering/GEMM to be performed on
all b images at once. This leads to signiﬁcant CPU speedups
compared with state-of-the-art tools which do not explore this
tradeoff but use the same implementation suited to the GPU
for the CPU. In particular, as Figure 3 shows, this allows us
to view a CPU or GPU as simply a device producing FLOPS;
for reference, we also include the FLOPS delivered by the
most popular CNN framework Caffe [21] and the call of an
optimized single-precision matrix multiply (SGEMM) for that
hardware. The throughput obtained by Omnivore on all devices
in Figure 3 also matches the expected range for CNNs (on
GPUs) reported by the manufacturer [19].

To achieve this throughput on the CPU, we use a simple
convolution batching technique in which we ﬁrst lower all
b images in a batch before performing any GEMM. After
this lowering, we perform a single GEMM for all b images,
rather than b smaller GEMMs. This consumes b× the memory
because the ˆD matrix is b× larger than when lowering images
one by one. However, it has two speed beneﬁts: (i) one large
GEMM is faster than b smaller GEMMs because CPU caches

Fig. 4. The impact of batch size (bp) and number of threads on the GEMM
kernel (total batch size b = 256, 8 physical cores). Shown for the GEMM in
the largest convolutional layer of AlexNet.

and vector instructions are fully utilized, and (ii) lowering all
images in the batch at once enables data parallelism to be used
during lowering. Speciﬁcally for (ii), given n CPU cores, a
batch is split into n partitions, with b/n images per partition.
A separate thread then performs lowering and GEMM on each
partition, i.e. each thread performs convolution on a subset of
the batch. This data parallelism can be applied to other layers.
Generalizing this implementation tradeoff, bp images can
be processed in parallel by the convolution layer, where
1 ≤ bp ≤ b. Increasing bp increases the memory footprint but
decreases the overall execution time. Figure 4 shows batching
experiments for a CPU GEMM kernel. All points in each graph
perform GEMM on 256 images (i.e., b = 256), but the number
of total GEMM calls depends on bp (e.g., if bp = 256, there
is one large GEMM). We therefore advocate the strategy of
selecting bp as large as possible (up to b) such that ˆD ﬁts into
the off-chip memory. This can be predicted because memory
usage increases linearly with bp as seen in Figure 4 (c). For a
range of modern CPUs that we used in our experiments, the
optimal bp value is always b.

While this tradeoff is simple, it enables FLOPS proportional
scheduling which allows us to abstract away the details of
devices in our distributed implementation.

IV. MULTI-DEVICE TRADEOFF

In this section, we study the distributed setting. Given a
CNN, an input set of data and a set of devices, our goal is to
map each layer of the CNN and a subset of data to each device.
Many distributed CNN systems [3], [5]–[9] can be mapped to
points within our tradeoff space.

We show that a key tradeoff is the degree of asynchrony. We
build models predicting how hardware and statistical efﬁciency
are affected by asynchronous execution. By decoupling hard-
ware efﬁciency and statistical efﬁciency and creating separate
models, the optimizer can ﬁnd a balance and minimize the
total time to convergence. Speciﬁcally, we argue for an analytic
model for hardware efﬁciency and we are able to give a new
theoretical characterization of statistical efﬁciency.

A. Setup and Assumptions

We separate each layer into compute servers, responsible
for computation, and a model server responsible for handling
reads from and writes to the model. These “servers” are
virtual: many servers may be mapped to the same device
or an individual server may be mapped to several devices.

serialized into a chain, hence we view the input as a list of
layers (without loss of generality).

Execution: Given a list of layers, the main computational
loop is to move through the list forward calling the forward
operation and in reverse order calling the backward operation
at each step. As we have decomposed each layer into a model
server and compute servers and further mapped compute
servers to compute groups over several devices, it is the re-
sponsibility of our execution engine to make sure that all data
is on each device when needed. We use standard techniques
to hide latency of device copies, e.g., double buffering.

B. Hardware Efﬁciency Model

The goal of this section is to create a predictive model
HE(S) for hardware efﬁciency. The goal is to characterize
the impact of the amount of staleness S in the system (or
equivalently the number of compute groups, g). We derive a
simple analytic model which reasons about the bottlenecks.
An execution strategy partitions the N conv devices into g
compute groups. Again for concreteness, assume there are k
devices per group (k = N/g). Let tconv(k) be a function
that returns the time that a group of size k needs to compute
the convolution phase. We make the assumption that FC only
operates sequentially3. Note that the number of requests to the
FC phase is a function of the number of groups, and let tf c
be the time that the FC phase needs to serve one group.

Given g, tconv(k) and tf c, our goal is to create a hardware
efﬁciency model which predicts the time per iteration. There
are two cases depending on which phase is the bottleneck.
(1) When the FC phase is saturated, i.e., it starts to serve the
next request immediately after the previous request ﬁnishes,

Time per iterationsaturated fc = tf c;
(2) When the FC phase is not saturated, each conv group
becomes the bottleneck. In this case,

Time per iterationsaturated conv = (tconv(k) + tf c)/g
which is the total time for a single iteration divided by the
number of parallel groups. Thus, our predicted model for
iteration time, HE(g), is:

HE(g) = max{tf c, (tconv(k) + tf c)/g}

Given tconv(k), tf c and the number of groups, g, the model
can now predict what the mode of saturation will be and
therefore the time per iteration.

Obtaining the Parameters: The parameters above can
be measured with high accuracy and low variance. tf c can
be measured by running an iteration on a single device, but
tconv(k), though still directly measurable, requires measure-
ments for each k. Instead, tconv(k) can be calculated from (i)
the throughput of each node; (ii) the network speed; and (iii) a
measurement of tconv(1) (which only needs to be measured for
a single k, k = 1, and on a single device). Figure 5(b) shows
that our hardware efﬁciency is accurate; detailed evaluation
in the appendix.

3 For simplicity, we assume different groups (batches) cannot be executed in parallel

on the FC server but our model can be extended to more general cases.

Fig. 5.
(a) Default physical mapping used by Omnivore. (b) Predicted and
measured iteration time as the number of devices (machines) per group
changes (32 c4.4xlarge machine, AlexNet)

While the separation of compute and model servers is present
in all systems, only project Adam [5] described mapping
both compute and model servers to the same device (or set
of devices for parallelism), i.e. merging the servers, which
they did for FC layers. Figure 5 (a) shows this mapping. For
concreteness, suppose there are N + 1 devices: one device
handles the FC layers, and the remaining N devices handle
the conv layers (motivated by 90 − 95% of the computation
being in the convolutions). To simplify our exposition we refer
to this reference architecture throughout Section IV. Section V
demonstrates the beneﬁts of this architecture empirically both
in terms of hardware and statistical efﬁciency.

Compute Groups: The input data to a layer is divided
into batches (whose size is determined by the system). The
main computational operation for each layer is to (i) read the
model; (ii) compute an operation on the data (the forward
or the backward pass) for the given a batch of data; and (iii)
update the model. We assign each compute server to a compute
group. Within a compute group, many devices speed up the
computation of an individual batch of data, e.g., all devices
compute a single gradient for a batch. Across compute groups,
different devices process distinct batches.

Asynchrony and Staleness:

In most systems, compute
groups communicate asynchronously [3], [5], [25]: the models
locking or barriers, and so forward
are updated without
and backward passes may be computed with a stale model.
Intuitively, the lack of coordination allows one to make better
use of the hardware (better hardware efﬁciency) but the use of
stale information may reduce statistical efﬁciency. If there are
g = S + 1 compute groups, we call S the staleness parameter.
This is justiﬁed as the computation within each group in step
(ii) above is very regular for CNNs (standard deviation of
runtime is less than 6% of mean), hence these groups execute
in a nearly round-robin fashion.

Throughout this section, we make two simplifying assump-
tions: First, we focus our description on the two phases
described in Section II, i.e., we abstract networks into con-
volutional layers (conv) which have a large amount of data
but a small model, and fully-connected layers (FC) which
have small data but a large model. Practically,
these two
layers are the main bottlenecks in current networks. Their
differing characteristics give rise to different points of the
tradeoff space. Second, many popular networks are simply a
single chain, not a DAG (e.g., AlexNet), and DAGs can be

C. Statistical Efﬁciency Model

We now characterize the effect of staleness on statistical
efﬁciency. While working on Omnivore, we realized that the
momentum value that yields the fastest convergence decreases
as we increase the number of asynchronous workers. Using the
right value can signiﬁcantly reduce the number of iterations
to reach a target loss. This motivated us to mathematically
analyze this phenomenon in our companion theory paper [17].
The result
is surprisingly elegant: under a simple model,
asynchrony introduces an extra momentum term in the SGD
update. This comes in agreement with our experimental ﬁnd-
ings in this paper. Here, we recap the essential result of
our theory and validate it experimentally on different systems.
Importantly, our result is predictive. We use it in this work to
complement our experimental ﬁndings on the importance of
momentum tuning and to design the ﬁrst asynchrony-aware
optimizer for deep learning systems in Section V.

We make the following assumptions, which are not neces-

sary but are helpful to capture the essence of our result.
(A0) The batch for each step is drawn uniformly at random
with replacement. This is a standard assumption of SGD.
(A1) Variations in the time to process a step are due to unmod-
eled system behavior. Also, variations are independent of
the speciﬁc batch drawn. This is justiﬁed by the fact that,
for all batches, all computation involves dense operations.
takes to process a step is exponentially
distributed and independent from other steps. This is
a simplifying but standard assumption from queuing
theory [26]. A more general (and complex) version of
Theorem 1 below holds without this assumption.

(A2) The time it

Theorem 1: Consider g asynchronous groups and set explicit
momentum to zero, i.e. µ = 0 in the update of equation (4).
Under the above assumptions, the expected update becomes

EV (t+1) =

1 −

EV (t) −

E∇W (cid:96)(W (t)).

(6)

(cid:18)

(cid:19)

1
g

η
g

In which (cid:96)(W ) denotes the expectation of (cid:96)(W, I, C) over the
random draw of possible batches (I, C).

In plain English, asynchrony increases momentum–there is
implicit momentum of 1−1/g. This not only matches the stan-
dard form of momentum used by deep learning practitioners in
equation (4), but also can predict measured system behavior.
Figure 6 shows the predicted and actual measured momentum
for two datasets: As long as the asynchrony-induced implicit
momentum is less than the optimal total momentum, we can
algorithmically compensate with explicit momentum. When
however, implicit momentum exceeds the optimal total mo-
mentum, we start incurring statistical inefﬁciency. We use this
intuition as the basis for our optimizer.

Cold-start: We observe a phenomenon similar to burn-
in [27] in Gibbs samplers. The model needs a few iterations
to set
the appropriate scale of the model parameters. On
Imagenet-1000, we ﬁnd that 5 passes over the dataset (< 15%
of total execution) sufﬁce to “warm up” the model. As a result,
the optimizer will start by running synchronously and then
switches to asynchronous (Section V).

Fig. 6. Predicted and measured momentum moduli: (Left) Predicted, Theo-
rem 1 (Middle) Measured, CIFAR (Right) Measured, ImageNet

V. DISTRIBUTED OPTIMIZER

This section uses the models and tradeoff space charac-
terization of the previous two sections to create (i) a plan
for physical mapping which maps each server to a machine,
and (ii) a plan for the execution strategy which deﬁnes the
number of compute groups by allocating data batches to each
server. As in previous sections we assume a ﬁxed number of
machines. We ﬁrst discuss the process of physical mapping
and then describe our optimizer. We conclude with theoretical
and empirical justiﬁcation for the optimizer, and in Section VI
compare it to state-of-the-art, Bayesian approaches.

A. Physical Mapping

We observe that in all CNNs the architecture of Figure 5 (a)
works best, and describe other physical maps in the appendix.
Omnivore maps the FC compute and model servers to the
same machine, an approach we call merged FC. Merging the
FC compute and model servers to the same devices was shown
in [5] to reduce communication overhead in a CPU cluster
(better hardware efﬁciency), however it was not known that (1)
this also beneﬁts hardware efﬁciency for a GPU cluster, and (2)
this technique also beneﬁts statistical efﬁciency by eliminating
staleness in the FC model. These are both observations we
make. The remaining machines are used for the conv compute
servers. The conv model servers are mapped to one of the
conv compute machines. These optimizations are critical: on
a cluster of 33 EC2 c4.4xlarge machines, not merging the
FC servers incurs an additional hardware efﬁciency penalty of
1.2× due to increased communication as well as a statistical
efﬁciency penalty of 2.5× because of staleness in the FC
model. The key tradeoff is therefore the number of conv
compute groups, which we describe next.

B. Optimizer

There are multiple interdependent factors that have impact
on the performance of the training procedure: (1) the number
of compute groups; (2) the momentum; and (3) the learning
rate. The optimal setting of these parameters might also change
during training, so our optimizer runs periodically in epochs
(e.g., every hour). Algorithm 1 shows the end-to-end optimizer
that runs after the initial cold-start period.

In each of the epochs, the key issue is to set the number of
compute groups, g. We perform a grid search over both the
learning rate and the momentum starting at a particular value
of g. This search determines the optimal explicit momentum
for that g by selecting the conﬁguration with the lowest ﬁnal

Algorithm 1 Automatic Optimizer for the Tradeoff
Input: Time budget T and possible choices of (1) # compute groups

CG, (2) momentum M, and (3) learning rate H.

Output: Trained model W .

(µ, η) ← gridSearch(M, H|W, g)
while µ = 0 and g > 1 do

1: g = CG
2: while not reaching the termination criteria do
3:
4:
5:
6:
7:
8:
9: end while
10: return W .

end while
W ← train(g, µ, η, W ) for T minutes

g ← g/2
(µ, η) ← gridSearch(M, H|W, g)

loss. The key intuition is: set the highest amount of asynchrony,
g, such that this explicit momentum is non-zero. The reasoning
is that, when the optimal explicit momentum is 0, the implicit
momentum is likely higher than the optimal value, and a
cause of statistical inefﬁciency (c.f. Figure 6). In this case, we
reduce the amount of asynchrony by reducing g. We provide an
initial value for g by leveraging the hardware efﬁciency model.
In particular, we start with the smallest number of compute
groups that saturate the FC server. This can be determined
analytically or through measurements during the cold start
phase. Having selected a (g, η, µ), we run for the rest of the
epoch. At the end of one hour, the epoch ends, the model is
checkpointed (written to disk), and the optimizer repeats.

Importance of Compute Groups: We demonstrate that
using the right number of compute groups has an impact on
performance. Fixing the total number of machines, we try
different numbers of compute groups on CPU-L (Figure 9) for
the Imagenet 1000-class dataset, and AlexNet CNN. We grid-
search a number of values for the learning rate and momentum
and report the best result achieved in each case. Figure 7
reports (a) the time per iteration (hardware efﬁciency), (b)
the number of iterations to reach a speciﬁc training loss
(statistical efﬁciency), and (c) their product, which is the total
time required to reach the ﬁnal loss. Note that the hardware
efﬁciency curve in (a) is the same as in Figure 5 (b).

We see in Figure 7 (c) that g = 32 (fully asynchronous) is
3.7× faster than g = 1 (synchronous) as measured by wall-
clock time to ﬁnal loss. This is due to its 6.7× faster iteration
time in (a), although it requires 1.8× more iterations as shown
in (b). This matches the theory’s prediction:
increasing g
causes the optimal explicit momentum, µ∗, to decrease. We
noted in Figure 6 (right) that µ∗ drops to 0 at g = 32,
and consequently there is a penalty in statistical efﬁciency.
Running the optimizer of Algorithm 1 selects g = 4, which
is near-optimal: 5.3× faster than sync and 1.4× faster than
async. We repeat the same experiment for CIFAR and ﬁnd the
results are similar. In all cases, the optimal number of groups
is more than 2× faster compared to sync, and that Algorithm 1
always picks a near-optimal point strictly better than sync.

Fig. 7. Hardware efﬁciency, statistical efﬁciency, and total time for various
execution strategies.

VI. EXPERIMENTS

We evaluate the runtime performance of our system. We
show that, Omnivore outperforms state-of-the-art
tools by
1.9× to 12× on a range of training tasks. Our result holds (i)
across a diverse range of hardware, including both CPUs and
GPUs, (ii) in both single device and multiple device/machine
settings. Moreover, Omnivore contains an automatic optimizer
and therefore does not require users to input hyperparameter
values.

Our experiments also validate that our optimizer is up to

6× faster compared to state-of-the-art Bayesian optimizers.

A. Experiment Setup

Datasets and Models: We validate the performance of
Omnivore on a diverse range of datasets and models, as shown
in Figure 8. The largest corpus we use is ImageNet [28], which
contains 1.3M images. ImageNet is the de facto benchmark
for deep learning systems [2], [3], [5]. Training a model
on ImageNet can take tens of hours—even on the latest
7 TFLOPS Titan X GPU, NVIDIA reports that it took three
days to train with a single GPU.4 For some of our experiments,
which require running many conﬁgurations to convergence,
we used a reduced version, ImageNet8, containing the ﬁrst 8
classes. We train the standard CaffeNet5 on both data sets. We
also use smaller, but standard, datasets CIFAR and MNIST,
which are for object recognition and handwriting recognition,
respectively. We train the networks in Caffe’s tutorials for both.
Metrics: Our main metric of performance is the wall-
clock time required to achieve a given training accuracy. As
in Section V, we deﬁne statistical efﬁciency as the number of
iterations needed to achieve that training accuracy.

Competitor Systems and Experiment Settings: We com-
pare Omnivore against a range of existing tools using both
a single-machine and multiple machines. Single machine:
we compare Omnivore to Caffe [21] and Google’s Tensor-
Flow [22], the two most popular CNN systems, using Caffe’s
reference (CaffeNet) model. Multiple machines: we compare
Omnivore to MXNet [6] and SINGA [7], two popular dis-
tributed deep learning systems.6 Both MXNet and SINGA
support multiple execution strategies, and we consider all
of these strategies in our experiments. We set up and tune

4https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/
5https://github.com/BVLC/caffe/tree/master/models/bvlc reference caffenet
6 We also surveyed a range of distributed training systems including SparkNet, DL4J,
and others. We found that MXNet is the fastest system on our datasets/tasks. At the time
of writing, the ofﬁcial version of Caffe can only run on a single machine, and TensorFlow
does not contain a distributed example for AlexNet/CaffeNet.

Data Set
ImageNet
ImageNet8
CIFAR
MNIST
Shakespeare

# Images

Image Size
1.3 M 256×256×3
256×256×3
10 K
32×32×3
60 K
28×28×1
60 K
25×1×128
162 K

# Classes
1000
8
10
10
128

Fig. 8.
Statistics of Data Sets. All data sets are image-related except
Shakespeare, a natural language corpus for text synthesization used in our
RNN experiments.

Name
1xCPU
2xCPU
1xGPU
4xGPU
CPU-S
CPU-L
GPU-S

Machines
1 × c4.4xlarge
1 × c4.8xlarge
1 × g2.2xlarge
1 × g2.8xlarge
9 × c4.4xlarge
33 × c4.4xlarge
9 × g2.8xlarge

TFLOPS
0.74
1.67
1.23
4.89
6.68
24.51
44.24

Network
-
-
-
-
1 Gbits
1 Gbits
10 Gbits

$/hour
$0.84
$1.68
$0.65
$2.60
$7.56
$27.72
$23.40

Fig. 9. Summary of Machines and Clusters. TFLOPS are the total TFLOPS
that the given machine/cluster can provide. We are unable to open 33 4xGPU
machines due to limited EC2 availability and therefore there is no GPU-L.

both systems according to their tutorials. On data sets where
SINGA performs strictly worse than MXNet, we omit its result
from the ﬁgure. To demonstrate the merits of momentum
tuning and compute groups on other platforms, we also
implemented these features in TensorFlow in the appendix.

Omnivore is implemented in C++. Caffe, TensorFlow,
MXNet, and SINGA all implement their core components
in C++. We compile all systems with g++ 4.8.2, and use
OpenBLAS 0.2.15, cuda 7.5, as well as both cuDNN v3 and
v4 for competitor systems and report the faster result.

B. Performance Evaluation

We validate that Omnivore has faster execution to the
same quality as existing systems in training deep learning
models. More precisely, Omnivore achieves the same training
accuracy/loss faster, as measured by wall-clock time. We ﬁrst
present our main end-to-end performance results, comparing
Omnivore to the state-of-the-art. Then, we validate our contri-
butions by showing results (i) for a single machine comparing
to Caffe and TensorFlow and (ii) in the distributed setting
comparing to MXNet and SINGA. We evaluate the impact of
our tradeoff space and optimizer in Section VI-C.

1) End-to-end Performance: For large datasets, Omnivore
is faster than state-of-the-art tools. We validate this on Ima-
geNet. We compare Omnivore with MXNet and SINGA. We
train the standard CaffeNet on all systems using both CPU-
L and GPU-S clusters. We time out each run after 8 hours
and report the training accuracy at a given time. We tune all
competitor systems following the ofﬁcial performance tuning
guideline7.8 For Omnivore, we use its automatic optimizer
that does not require any hyperparameters. Because SINGA
is always slower than MXNet in our experiments, we omit it
from this ﬁgure. but discuss it in Section VI-B3.

7https://github.com/dmlc/mxnet/tree/db6f6a9418e5696b04be741a78a47ae877bb5505/

example/image-classiﬁcation and previous work [2], [29]

8 The time for this tuning of other tools exceeds Omnivore’s automatic

tuning time for the cold start phase so we omit these initial tuning times.

Fig. 10. End-to-end Performance of Omnivore and MXNet on ImageNet. We
omit SINGA as it performs worse than MXNet.

System

Caffe
TensorFlow
Omnivore

Machines

1xCPU
1.03×
1×
3.90×

2xCPU
1×
1.05×
5.36×

1xGPU
1.11×
1×
1.04×

4xGPU
1×
1.26×
3.34×

Fig. 11. End-to-end single machine performance across different machines
on CaffeNet. Times are normalized as the speedup over the slowest system
on the same machine (the larger the better). We run cuDNNv3, cuDNNv4
and no cuDNN for Caffe and TensorFlow and report the fastest one.

Figure 10 shows the result. We report sync and async for
MXNet as their documentation suggests trying both. Omnivore
reaches the same accuracy up to 11× faster than MXNet on
the GPU cluster and 12× faster on the CPU cluster. Compared
to sync, Omnivore is 4.5× and 1.9× faster respectively.
This includes the 10% overhead of Omnivore’s optimizer
during the run. The optimizer reduces momentum or learning
rate each time it runs. In the remainder of this section we
conduct detailed analysis of Omnivore, MXNet, and SINGA
to understand this improvement. As we will see, Omnivore’s
optimizer, which searches within the larger tradeoff space, is
the key reason for our system’s performance.

2) Single Machine Experiments: We validate our claim that
Omnivore’s performance is FLOPS-proportional, which means
it scales with available FLOPS, regardless of the number or
type of devices available. We ﬁrst explain the protocol of
our experiments. Then we report our results and discuss the
importance of FLOPS-proportional system performance.

Protocol: We compare Omnivore against Caffe and Ten-
sorﬂow on a single machine. We train CaffeNet on ImageNet
on hardware described in Figure 9. We measure the time each
system needs to ﬁnish 40 iterations of training (following
10 iterations of warm-up), using the standard batch size in
CaffeNet (256 images). Our single-machine optimizations only
affect hardware efﬁciency; the number of iterations needed for
convergence does not change. Hence, we can use time per
iteration as a surrogate for performance.

Results: Figure 11 shows the results. We normalize all
execution times by the slowest system in each column and
report the resulting speedups. We see that on a single CPU,
Omnivore is 3.9× faster than both Caffe and TensorFlow; on a
single GPU, all systems show similar speed. This is consistent
with our observation in Section III-B: TensorFlow and Caffe
use the same strategy for both CPU and GPU, which is optimal
for GPU but suboptimal for CPU. One interesting consequence
of this speedup result is that, although Caffe on 1xCPU is

that a fully synchronous strategy will be fast in terms of
hardware efﬁciency while having the best statistical efﬁciency.
Figure 12(a) shows the results. All systems reach accuracy
60% within 2 hours, and Omnivore reaches 60% the fastest.
Omnivore is 2.3× faster than MXNet (388 seconds vs. 907
seconds). At 3000 seconds, Omnivore already achieves an
accuracy > 99%, while MXNet achieves the same accuracy
after 7000 seconds. The speed up here is also 2.3×. As
expected, both systems chose a fully synchronous strategy, so
statistical efﬁciency is not the cause of this performance gap.
The 2.3× speed up is due to our CPU-based optimization
(Section III) and merging FC servers (Section V-A).10

(Small GPU Cluster: GPU-S) GPU-S is a small GPU
cluster that contains 9 4xGPU machines (36 GPUs). Because
each node is signiﬁcantly (7×) faster than 1xCPU, we expect
the optimal strategy to be more asynchronous, and thus,
statistical efﬁciency to come into play. Figure 12(b) shows the
result.11 Similar to the CPU-S cluster, Omnivore outperforms
MXNet: it reaches 99% accuracy 4.8× faster. MXNet only
supports completely synchronous or asynchronous execution,
and its optimal run uses the completely synchronous strategy.
On the other hand, Omnivore’s optimizer chooses to run with
two compute groups. Had Omnivore chosen the same strategy
as MXNet, it would be 1.7× slower than Omnivore’s actual
choice due to a different choice of the synchronization strategy.
The remainder of the 4.8× gap is due to the physical mapping
(merging FC) used by Omnivore, and this improves both
hardware and statistical efﬁciency: while originally described
as a mechanism to reduce network communication [5], we ﬁnd
that merged FC also improves statistical efﬁciency by reducing
staleness in the FC model.

(Large CPU Cluster: CPU-L) CPU-L is a CPU cluster
with 33 1xCPU machines. Because the number of machines
is large, we expect the synchronization across all machines
would incur a large penalty in terms of hardware efﬁciency—
thus, we expect the optimal strategy to be more asynchronous.
Figure 12(c) shows the result. We see that Omnivore is 3.2×
faster than MXNet to reach 99% accuracy. The best MXNet
strategy was to train completely synchronously; Omnivore’s
optimizer now chose four compute groups. Had Omnivore
it would incur 5×
chosen the same strategy as MXNet,
overhead for hardware efﬁciency but only gain a 2× beneﬁt
for statistical efﬁciency. Also, had Omnivore simply chosen a
fully asynchronous conﬁguration, it would be 3× slower. This
shows the importance of choosing the right number of groups
to balance statistical and hardware efﬁciency.

Impact of Optimizer: In the experiments above, we use
grid search to ﬁnd the optimal strategy for both MXNet and
SINGA. On the other hand, Omnivore relies on the optimizer
to automatically choose the best strategy. Had we not used
the grid search for MXNet and relied on default parameters
the performance gap would be 20× on ImageNet8. This

10 SINGA does not converge to 99% in 2 hours and Omnivore is 11× faster than

Fig. 12. Comparison of Omnivore with MXNet and SINGA on ImageNet8.
We omit the SINGA for CPU-L because it performs worse than MXNet.

7× slower than on 1xGPU, Omnivore is only 1.8× slower
on 1xCPU, which we will see matches the FLOPS ratio of
these devices. Omnivore’s FLOPS-scaling extends to multiple
devices, and the gap with other systems increases for more
CPU sockets (2xCPU) or GPU cards (4xGPU).9

FLOPS Proportionality: The training performance of
CPUs is commonly believed to be an order of magnitude
slower than GPU performance. Literature often reports this,
and we showed that it is the case for Caffe and TensorFlow
on 1xCPU and 1xGPU. We validate that Omnivore delivers
performance proportional to the FLOPS that a device can
provide. As shown in Figure 9, 1xGPU provides 1.7× more
FLOPS than 1xCPU, and Omnivore has a 1.8× gap between
1xCPU and 1xGPU. In other words, regardless of the type of
device, Omnivore performs proportionally to the number of
FLOPS available. We also observe that proportionality holds
for all machines in Table 9. FLOPS-proportionality means that,
using both CPUs and GPUs on the same machine, we should
be able to construct an even faster system. We validate this
by using both CPUs and GPUs on 4xGPU, whose CPU and
GPUs provide 0.67 TFLOPS and 4.89 TFLOPS, respectively.
By using data parallelism across the CPU and a single GPU,
Omnivore achieves an 18% speedup over just using the GPU.
3) Distributed Experiments: We conduct experiments to
understand our end-to-end improvement across three different
clusters described in Figure 9. As we will show, our tradeoff
characterization leads to the performance gains of Omnivore.
We ﬁrst describe the settings and the performance metric
used in the experiments. Then we discuss the optimizer’s
contribution and analyze its decisions across different clusters.
Protocol: We tune Omnivore, MXNet, and SINGA and
run them to convergence under multiple settings. Thus, we use
ImageNet8 that contains the ﬁrst eight classes of ImageNet.
This allows us to grid search all parameters in MXNet and
SINGA, including synchronization strategies and learning rate,
and pick the best run. We do not include the time of our
optimizer, which takes signiﬁcantly less time compared with
the grid search we did for MXNet and SINGA. We run all
systems for 2 hours and measure the training accuracy at a
given time. Figure 12 shows the results.

Results:

(Small CPU Cluster: CPU-S) CPU-S is a
small CPU cluster that contains 9 1xCPU machines. Because
each machine is slow and the network is fast, we expect

9We also run experiments on a 4-socket, 56-core Haswell CPU machine, and Omnivore

11 As of the time of writting, SINGA does not support GPUs and is omitted from

is 13× faster than Caffe.

SINGA to reach 60% accuracy.

Figure 12(b).

is compared to MXNet’s completely asynchronous strategy,
which is recommended in their performance tuning guideline
for networks like AlexNet.

Comparison across Clusters: Omnivore’s optimizer
makes different choices on different clusters. It is interesting
to compare them. As we can see, given the same amount of
machines (CPU-S vs. GPU-S), as devices get faster, Omnivore
tends to choose more asynchronous strategies. Intuitively,
the faster compute nodes get, the easier for the network to
get congested. The fully synchronous approach incurs higher
penalty in that case. On the other hand, given the same
speed of each compute node (CPU-S vs. CPU-L), when the
number of machines gets larger, Omnivore also tends to choose
a strategy that is between a fully synchronous and a fully
asynchronous strategy: (1) when the staleness gets very large,
even a properly tuned, fully asynchronous strategy incurs a
penalty in terms of statistical efﬁciency, and (2) when the
number of machines that need to be synchronized gets larger,
a fully synchronous strategy incurs a penalty in terms of
hardware efﬁciency. Omnivore’s optimizer makes it possible
for us to be robust across different devices and cluster sizes.

C. Tradeoff and Optimizer of Omnivore

We validate the hypothesis that (1) the tradeoffs studied
in this paper and (2) the automatic optimizer have a signiﬁ-
cant impact on the performance of Omnivore. We study the
importance of compute groups, as well as compute-group-
speciﬁc momentum tuning. We also study the effectiveness
of Omnivore’s automatic optimizer for this tradeoff space by
comparing it against a standard Bayesian optimizer.

1) The Tradeoff Space: In this section we demonstrate that
the various dimensions of the tradeoff space have a signiﬁcant
impact on performance. Throughout this work we already
illustrated some of these tradeoffs, so here we only summarize
them and leave the detailed discussion for the appendix. We
see that tuning the learning rate is necessary for convergence
and the physical mapping has both HE and SE beneﬁts. We
also showed that using the optimal number of compute groups
can yield 6.7× speedups compared to fully synchronous and
1.8× compared to fully asynchronous execution. This holds on
a range of datasets and clusters. We also implement compute
groups within TensorFlow and demonstrate the tradeoffs for
the Inception-v3 network in the appendix. We now focus
on the importance of properly tuned momentum, which as
we showed in Section IV-C is a function of the level of
asynchrony.

Importance of Momentum Tuning: We validate that the
correct value of momentum depends on the number of groups.
We expect that properly tuned momentum would outperform
a momentum tuned agnostically to the number of compute
groups. Therefore, we compare different methods of momen-
tum tuning on the optimal number groups for ImageNet8
on CPU-L, which was 4 (recall Figure 12(c)). We ﬁx that
number of groups and (i) set momentum to 0.9 (as reported in
AlexNet [2]); (ii) use the momentum tuned for a synchronous
system; (iii) tune the momentum using Omnivore’s optimizer

Fig. 13. Lesion study of momentum. Default momentum = 0.9, which is also
the optimal momentum for the fully synchronous strategy.

for 4 compute groups. As we see in Figure 13, tuning for the
right amount of asynchrony is important: if Omnivore did not
tune momentum, it would be 1.5× slower. Further experiments
show that tuning momentum can yield speedups of 2×. On
TensorFlow, we observe similar speedups: When momentum
is set to 0.9, synchronous training is faster. When, however,
we perform momentum tuning, the asynchronous conﬁguration
wins with its performance relative to sync improved by a factor
of 2.4× This both veriﬁes our expectations for this experiment
and provides further support for our theory in Section IV-C.
Discussion: Other Models: We ﬁnd that these tradeoffs
are impactful when applied to other models. We ﬁnd that for
Recurrent Neural Network models and LSTM models (e.g.,
the same choices affect performance—for example,
[30]),
choosing a completely synchronous or asynchronous conﬁgu-
ration can be up to 2× slower than the optimal conﬁguration.
This could imply speedups for applications in which RNNs
are widely used, such as handwriting, speech recognition, and
general sequence or time-series data.

2) Optimizer: We validate that our optimizer outperforms
state-of-the-art Bayesian optimization algorithms. We compare
our optimizer with the optimizer proposed by Snoek et al. [18].
We measure both the number of conﬁgurations and the total
number of epochs that the Bayesian optimizer needs to achieve
an accuracy within 1% of the highest accuracy Omnivore
achieves. In our experiments, the Bayesian optimizer never
discovers a conﬁguration which outperforms the conﬁguration
Omnivore obtains by grid search. We found that the Bayesian
optimizer takes on average 12 runs to ﬁnd a near-optimal
strategy, which on average is 6× more epochs than just
running that strategy. Because of this search overhead it was
not feasible to use the full ImageNet 1000 dataset so we used
ImageNet8 (whereas recall from Figure 10 that Omnivore had
an overhead of only 10% on ImageNet 1000). We used the
GPU-S cluster. Typically Bayesian optimizers can amortize
this cost by running in parallel, but here that is not possible
as the parameters depend on the hardware conﬁguration and
so the optimizer needs complete access to the entire cluster.

VII. RELATED WORK
Single Node. Optimizing CNN performance has become a
well-studied problem in recent years. Popular libraries include
Caffe [21], cuDNN [19], TensorFlow [22], Theano [23], and
Torch. To compute convolutions, many of these frameworks
use lowering, an idea proposed by Chellapilla et al. [31] that
takes advantage of highly-optimized BLAS libraries. Our work

follows from this line of research and demonstrates how to
optimize lowering for CPUs in order to build a system which
is robust to different types of hardware.

Distributed Deep Learning. Distributed systems for Deep
Learning popular, with SINGA [7], MXNET [6], FireCaffe [9],
SparkNet [8], DL4J, DistBelief [3], and Project Adam [5]
selecting different execution strategies and other optimizations.
Our study shows a combined tradeoff space on the union of all
these techniques. We did this by decoupling the hardware and
statistical efﬁciency for each technique and optimizing them
separately. Our work is the ﬁrst to provide a theoretical char-
acterization for statistical efﬁciency and to show that hyper-
parameters need to be tuned to compensate for asynchrony.

The idea of decoupling the number of iterations and time
per iteration and analyzing each separately is not new for
distributed CNN systems. MXNet reported hardware efﬁciency
and statistical efﬁciency separately. SINGA went deeper into
the tradeoff, identifying the compute group size as a tunable
parameter. They advocate combining both synchronous and
asynchronous training and offer a ﬂexible training architecture
which enables trading off the convergence rate with the time
per iteration in order to to minimize training time. However,
while SINGA identiﬁes this tradeoff and provides experimen-
tal evidence of its importance similar to the curves we showed,
the user still needs to manually choose a conﬁguration.

SparkNet also separated the time per iteration and number
of iterations by building models of each. They did not explore
the same tradeoff of machines per group, but rather a similar
tradeoff related to staleness. Because SparkNet uses a MapRe-
duce framework they implement model averaging. Within this
technique they explore, in isolation, how the staleness (their
τ parameter) impacts the number of iterations to convergence
and the time per iteration. Their hardware efﬁciency model was
measured (both network speed and compute time) and their
statistical efﬁciency model was also empirical (they varied
staleness and measured the statistical efﬁciency penalty).

[5] T. Chilimbi et al., “Project adam: Building an efﬁcient and scalable deep

learning training system,” in OSDI, 2014.

[6] T. Chen et al., “Mxnet: A ﬂexible and efﬁcient machine learning library

for heterogeneous distributed systems,” arXiv, 2015.

[7] W. Wang et al., “SINGA: A distributed system for deep learning,” NUS

Tech Report, Tech. Rep., 2015.

[8] P. Moritz et al., “SparkNet: Training deep networks in Spark,” arXiv,

2015.

[9] F. N. Iandola et al., “FireCaffe: near-linear acceleration of deep neural

network training on compute clusters,” arXiv, 2015.

[10] Abadi et al., “Tensorﬂow: A system for large-scale machine learning,”

arXiv preprint arXiv:1605.08695, 2016.

[11] H. Cui et al., “Geeps: Scalable deep learning on distributed gpus with
a gpu-specialized parameter server,” in Proc. of the Eleventh European
Conference on Computer Systems. ACM, 2016, p. 4.

[12] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz, “Revisiting distributed

synchronous sgd,” arXiv preprint arXiv:1604.00981v2, 2016.

[13] A. Candel, V. Parmar, E. LeDell, and A. Arora, “Deep learning with

h2o,” 2015.

[14] X. Lian et al., “Asynchronous parallel stochastic gradient for nonconvex

optimization,” in NIPS, 2015.

[15] I. Sutskever et al., “On the importance of initialization and momentum

in deep learning,” in ICML, 2013.

[16] S. Hadjis et al., “Caffe con Troll: Shallow ideas to speed up deep

learning,” in DanaC, 2015.

[17] I. Mitliagkas et al., “Asynchrony begets momentum, with an application

to deep learning,” arXiv:1605.09774, 2016.

[18] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimiza-

tion of machine learning algorithms,” in NIPS, 2012.

[19] S. Chetlur et al., “cuDNN: Efﬁcient Primitives for Deep Learning,”

[20] K. He et al., “Deep residual learning for image recognition,” in CVPR,

[21] Y. Jia et al., “Caffe: Convolutional architecture for fast feature embed-

ArXiv, 2014.

2016.

ding,” ArXiv, 2014.

[22] Abadi et al., “TensorFlow: Large-scale machine learning on heteroge-

neous distributed systems,” arXiv, 2015.

[23] J. Bergstra et al., “Theano: a CPU and GPU math expression compiler,”

VIII. CONCLUSIONS

in SciPy, 2010.

We described the ﬁrst explicit study of the tradeoff space
for deep learning systems, a popular, high-value type of
industrially deployed learning systems. We identiﬁed critical
issues in how one maps layers to devices and are the ﬁrst to
systematically study widely used techniques like asynchrony.
We designed a new optimizer and showed that it has excellent
end-to-end performance and is independent of our particular
implementation substructure. We are collaborating with a
major chip manufacturer to apply asynchrony-aware tuning
and compute groups onto new platforms of much larger scale.

[24] N. Vasilache et al., “Fast Convolutional Nets With fbfft: A GPU

Performance Evaluation,” ArXiv, 2014.

[25] F. Niu et al., “Hogwild!: A lock-free approach to parallelizing stochastic

gradient descent,” in NIPS, 2011.

[26] D. Gross, Fundamentals of queueing theory. John Wiley & Sons, 2008.

[27] A. E. Raftery, S. Lewis et al., “How many iterations in the gibbs

sampler,” Bayesian statistics, 1992.

[28] J. Deng et al., “ImageNet: A large-scale hierarchical image database,”

in CVPR, 2009.

[29] L. Bottou, “Stochastic gradient descent tricks.” in Neural Networks:

Tricks of the Trade (2nd ed.). Springer, 2012.

REFERENCES

[30] A. Graves, “Generating sequences with recurrent neural networks,”

[1] L. Deng and D. Yu, “Deep learning: Methods and applications,” FTSP,

arXiv, 2013.

[2] A. Krizhevsky et al., “ImageNet classiﬁcation with deep convolutional

for document processing.” ICFHR, 2006.

neural networks,” in NIPS, 2012.

[3] J. Dean et al., “Large scale distributed deep networks,” in NIPS, 2012.
[4] X. Zhang and Y. LeCun, “Text Understanding from Scratch,” ArXiv,

[32] “Inception in TensorFlow,” https://github.com/tensorﬂow/models/tree/

51238b1b5219a37ba145915efa764cca870e0d9f/inception.

[31] K. Chellapilla et al., “High performance convolutional neural networks

2014.

2015.

IX. ACKNOWLEDGEMENTS

The authors would like to thank Chris Aberger and the
rest of the Hazy Group for their feedback and help, as
well as HyoukJoong Lee, Nadathur Rajagopalan Satish, Peter
Bailis and Benjamin Recht for their thoughtful comments. We
would like to thank Intel, Toshiba and the Moore Foundation
for support along with DARPA through MEMEX (FA8750-
14-2-0240), SIMPLEX (N66001-15-C-4043), and XDATA
(FA8750-12-2-0335) programs, and the Ofﬁce of Naval Re-
search (N000141210041 and N000141310129). Any opinions,
ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reﬂect the views of DARPA, ONR, or the U.S. government.

APPENDIX

The appendix sections mirror the section structure of the

main paper, carrying corresponding supplementary material.

Appendix A includes supplementary information for the
Introduction, most importantly more discussion on the CPU
vs GPU debate.

Appendix B includes a discussion of CNN trends.
Appendix C includes a full

tradeoff analysis on lower-
ing strategies—ﬁrst reported in a workshop paper [16]—that
shows that the strategy we use in this paper works best for
most CNN kernels. It also includes a more comprehensive
treatment of batching and data parallelism for the GPU.
Finally,
it shows that FLOPS-proportionality holds for all
machines in Table 9.

Appendix D includes a full discussion of the tradeoff space,
including terminology, a survey (Appendix D-C) of many
diverse distributed CNN systems [3], [5]–[9] and shows that
they can be mapped to points within our trade-off space.
Appendix D-D includes a proof for the hardware efﬁciency
model we use, describes how to measure necessary quantities
from the system, and shows that it works across a range of
datasets.

Appendix E gives more details on our distributed optimizer:
Appendix E-A includes discussion and experiments on select-
ing the batch size; Appendix E-B describes other physical
mappings we studied and related analysis; Appendix E-D
discusses in more detail the cold-start period of optimization.
Appendix F includes the full setup of our distributed ex-
periments: Appendix F-B contains full details for our small
cluster experiments; Appendix F-C contains a full trade-off
space analysis for ImageNet and CIFAR10; Appendix F-C4
shows that tuning momentum can yield speedups of 2×; Ap-
pendices F-C4 and F-D contain full details on our larger cluster
experiments and discuss the impact of optimizing the hyper-
parameters of competitor systems; Appendix F-E includes
details on the end-to-end experiment on ImageNet1000, as
well as the choices of our optimizer. Appendix F-F describes
preliminary experiments on RNNs; Appendix F-G compares
our optimizer to standard schedules and; Appendix F-H com-
pares to Bayesian hyper-parameter optimization.

TABLE I
TRADEOFF SPACE WHEN DESIGNING DISTRIBUTED DEEP LEARNING
SYSTEMS.

Tradeoff
Single Node Hardware
Type of Parallelism

Batch size

Batch allocation
within node

Batch allocation
across nodes

Combining Model
Replicas

Server Architecture

Network Architecture

Hyperparameter
Optimization

Some Examples
CPUs, GPUs, both together
Model, Data
Large (few accurate updates),
Small (many parallel updates)
1 CPU or GPU per batch,
many CPUs or GPUs per batch
1 batch across all machines,
1 parallel batch per machine,
groups of machines per batch
Atomic gradient updates,
Model averaging, Ensembles,
Race conditions (Hogwild!)
Separate parameter/compute,
Merged parameter/compute
Model Size (impacts
communication),
Model Depth (impacts memory
/ batch size)

Grid/Random search, Bayesian,
Plateau (e.g., ResNet), Decay
Schedule (e.g., Inception-v4)

Optimization Algorithm SGD, Adagrad, momentum

Appendix G, includes our TensorFlow results. We show that
on Inception-v3, momentum tuning can be result-changing:
when momentum is set to 0.9, synchronous training is faster.
When, however, we perform momentum tuning, the asyn-
chronous conﬁguration wins with its performance relative to
sync improved by a factor of 2.4×. We also study the effect
of compute groups.

Appendix H gives a total cost of ownership analysis.
Appendix I includes supplementary discussion from the

Conclusions section.

APPENDIX A
APPENDIX FOR INTRODUCTION (SECTION I)

Tradeoff Table: While many distributed deep learning
systems exist, each of these makes design decisions suited
for a particular type of (1) compute cluster, (2) deep learning
model and (3) dataset, although these same decisions may
not work for other problem settings or hardware resources.
This is because deep learning is known to be complex both
from a computational and machine learning perspective, and
designing a highly efﬁcient and scalable deep learning engine
involves a number of interrelated design decisions. Table I
shows a number of factors to consider when designing dis-
tributed deep learning systems. Given a ﬁxed number of
machines, a number of tradeoffs exist from how to use
hardware on each node, to how to allocate batches of data to
machines, to the type of deep learning model to use in order to
minimize communication. In addition, these decisions impact
one another – for instance the batch size inﬂuences the number
of machines which can be used to effectively parallelize within
a batch and therefore inﬂuences the total number of parallel
gradients being computed to update the model. Our work,
which is a study, demystiﬁes these factors by identifying

the key tradeoffs which underlie all design decisions and
quantifying the impact of those tradeoffs experimentally.

Contribution 1: Single-Node Optimization: Even focusing
on just a single node, there has been a long debate about
CPUs vs GPUs for deep learning. GPUs are popular for CNN
systems because of the high throughput they provide. Modern
GPUs offer between 1.2 TFLOPS (NVIDIA GRID K520, per
GPU) and 8 TFLOPS (NVIDIA Titan Z). However, GPUs
contain smaller off-chip memories than CPUs, and GPUs are
connected to host memory by a slow PCI-e interconnect.
On the other hand, Microsoft’s Project Adam argues that
CPUs can deliver more cost-effective performance [5].12 This
debate is only going to get more interesting, as modern
GPUs offer high-speed interconnect with host memory13 while
Intel’s current Haswell CPU can achieve 1.4 TFLOPS on a
single chip.14. Moreover, SIMD parallelism has doubled in
each of the last four Intel CPU generations and is likely to
continue.15 Our work is the ﬁrst to conduct a systematic study
to understand the relative performance of CPUs and GPUs for
deep learning.

APPENDIX B
APPENDIX FOR BACKGROUND (SECTION II)

A. CNN Computation

AlexNet FLOPS: We approximate the FLOPs (# ﬂoating
point operations) in AlexNet by the sum of all the GEMM
operations with batch size 256. Speciﬁcally, we add 1 GEMM
in the forward pass for each Conv and FC layer, plus two
GEMMs in the backward pass for each Conv and FC layer
(although Conv1 backward has only 1 GEMM because no
gradient is needed with respect to the data).

Terminology: This section introduces model and data
parallelism as two techniques to parallelize CNNs. For a
full description of these concepts, see the Terminology in
Appendix D-B.

CNN Trends: This section viewed CNNs as two phases,
Conv and FC. Recent CNNs, e.g., Residual Networks
(ResNets) and Inception Networks, can also be categorized
into this partitioning. For instance early Inception variants
contained multiple FC layers at different parts of the network,
but from a computational point of view these are all considered
to be part of the FC phase.

In particular, CNNs have undergone a number of changes

in the past few years16. We summarize a few here:

• Multiple FC layers replaced with average pooling, leaving
a single fully-connected layer (for the softmax). This
leads to a reduction in the overall model size (e.g., 60
million parameters for AlexNet compared to 4 million
for GoogleNet)

12http://www.wired.com/2014/07/microsoft-adam/
13http://nvidianews.nvidia.com/news/nvidia-launches-world-s-ﬁrst-high-speed-gpu-

interconnect-helping-pave-the-way-to-exascale-computing
14Xeon E5-2698 v3, http://ark.intel.com/products/81060
15SIMD scales linearly in power and area (whereas frequency scaling is cubic) http:

//parasol.tamu.edu/lcpc2014/keynote-tian.pdf.

16http://cs231n.github.io/convolutional-networks/#case

• Increase in network depth (e.g., AlexNet with 5 conv
layers, compared to ResNets17 with > 150). This in-
creases memory requirements of networks, and makes
multi-device training necessary.

As we will see,

the optimizer presented in this paper
considers the impacts of each of these points when making
decisions for physical mapping and execution strategy.

B. Problem Deﬁnition

Scope of Work: Our work does not focus on improving
machine learning techniques but rather studies systems trade-
offs in order to build an optimizer that is robust to the most
widely used networks/algorithms. Our study uses the SGD
algorithm due to its popularity, although our optimizer applies
to other algorithms as well. Similarly we do not modify the
CNN architecture but assume that this is provided by the user.
Terminology: The physical mapping maps the layer
computation to vertices in G. Vertices in G may contain
other vertices, e.g., GPUs or CPU cores within a machine.
Section III ﬁrst studies how to map the CNN to hardware
within a machine, and concludes that with proper optimization
only the throughput of each vertex matters, not its underlying
hardware. Section IV then studies how to map the CNN to all
of G, i.e. across machines.

APPENDIX C
APPENDIX FOR SINGLE-NODE TRADEOFFS (SECTION III)
This section describes the optimizer’s physical plan (how to
map the CNN to hardware) at the level of a single machine.
Given a machine containing devices of various throughput
(CPUs, GPUs), our goal is to run the CNN computation as
quickly as possible. We do this by identifying two tradeoffs.
Our ﬁrst tradeoff introduces a data batching technique which
trades off memory footprint for compute time. We demonstrate
that this tradeoff gives a > 5× CPU speedup over existing sys-
tems. With this optimization now both CPUs and GPUs give
throughput proportional to the FLOPS offered by the device,
and our second tradeoff partitions the CNN computation across
both the CPU and GPU to combine their FLOPS – a known
HPC trick, but one which has not been applied to CNNs.

A. Convolutional Layer Computation

A 3D convolution consumes a pair of order 3 tensors–the
data D ∈ Rn×n×din and the kernel K ∈ Rk×k×din . For
example, if D is a color image with 3 (RGB) color channels,
din = 3. In AlexNet [2], n ∈ [13, 227], k ∈ [3, 11], and
din ∈ [3, 384], The output is a 2D matrix R ∈ Rm×m where
m = n − k + 1 and each element Rx,y is deﬁned as:

Rx,y =

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48)Kx(cid:48),y(cid:48),d(cid:48)

(7)

The kernel also supports parameters called padding and stride,
which affect the size of m. For details on stride and padding
see http://cs231n.github.io/convolutional-networks/#conv.

17https://github.com/KaimingHe/deep-residual-networks

This is the standard image 3D discrete convolution. A
convolution layer in the CNN contains a number of kernels
{Kj}, not just one, where we call dout = |Kj| the number of
output channels. These kernels {Kj} constitute the model of
the convolutional layer, and the reason for computing multiple
kernels rather than just 1 in the convolutional layer is to a
more powerful machine learning model. The convolutional
layer takes as input the 3D data tensor D and performs dout
3D convolutions, one per {Kj}, such that the output of the
convolutional layer is now not a 2D matrix R but a 3D tensor
R ∈ Rm×m×dout . Similarly the model of the CNN can be
viewed as a 4D tensor K ∈ Rk×k×din×dout.

Finally, recall that often rather than process a single data
example the CNN processes a batch of b examples simul-
taneously. The motivation for doing this is that gradient
computations during learning are less noisy. Therefore in the
most general case, the input D to a convolutional layer is not
1 but b 3D data tensors, or equivalently a 4D data tensor D ∈
Rn×n×din×b. The model is unchanged, but the convolutional
layer now performs the dout 3D convolutions on each example
in the batch, i.e. the batched convolution layer performs b·dout
3D convolutions. The output of the convolutional layer is
therefore also a 4D tensor, R ∈ Rm×m×dout×b.

To summarize, a convolutional layer accepts as input a 4D
data tensor D ∈ Rn×n×d×b, performs a total of b·dout 3D dis-
crete convolutions using D and its model K ∈ Rk×k×din×dout,
and outputs a 4D output data tensor R ∈ Rm×m×dout×b. The
full formula is:

Rx,y,z,w =

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48),wKx(cid:48),y(cid:48),d(cid:48),z

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Many implementations of this convolutional

layer exist.
Like most other HPC kernels, a straightforward implemen-
tation of this operation is suboptimal. Optimized implementa-
tions include directly computing the convolutions, as in cuda-
convnet2,18, computing the convolution as a discrete Fourier
transform as in [24], or implementing the convolution as a
matrix multiply, as in Caffe [21] or cuDNN [19].

While the studies in these papers conclude that different
strategies perform best for different kernel sizes, cuDNN [19]
demonstrates that the third technique of performing convolu-
tion as a matrix multiplication is versatile and fast for a range
of size parameters, as matrix multiplication kernels are often
highly optimized.

In order for the convolution to be carried out as a matrix
multiplication, an initial reformatting phase called lowering is
required to put the data and kernel into the correct format,
which we discuss in the next subsection of this appendix.

1) Convolution by Lowering and GEMM: Lowering fol-
lowed by a general dense matrix multiplication (GEMM) is a
popular way to implement the convolution operation. Figure 2
shows the three logical steps in the lowering process: (1)
lowering, which transforms 4D tensors D and K into 2D
matrices ˆD and ˆK; (2) matrix multiply (GEMM), in which

18https://code.google.com/p/cuda-convnet2/

we multiply ˆD ˆK to get the result ˆR; and (3) lifting, which
transforms ˆR back to a tensor representation of R.

Lowering Phase in which we construct the matrix
ˆD and ˆK. A value of D will appear more than once
in the lowered matrices.
Multiply Phase (GEMM) in which we multiply ˆD
and ˆK to create ˆR = ˆD ˆK.
Lifting Phase in which we map ˆR back to R.
Three techniques exist to perform this process, each corre-
sponding to a different way to group the sum in Equation 7.
Each of these techniques requires replicating the data or the
kernel in order to allow the convolution to be implemented
as a GEMM, but the amount of replication and the size of
the GEMM depend on whether the replication happens in the
lowering phase, lifting phase, or partly in each. The tradeoff is
studied in detail by [16], which concludes that the best choice
is determined entirely by the ratio din/dout (the ratio of the
number of input channels to the number of output channels
of the convolution), and that for modern CNNs these ratios
suggest that the data replication should be done during the
lowering phase. Therefore in the lowering used by this work,
there are two components to convolution: lowering (which
requires replication of data), and the GEMM. The lifting
does not require any memory copies or computation in the
optimal technique. Note also that in this technique, the kernel
does not require any replication, only the data. CNNs are
continuously evolving however, and so it is possible that future
CNN architectures will beneﬁt from other lowering strategies.
For a full study of the lowering tradeoff, refer to [16].

The amount of data replication required by the lowering in
this work is m2k2/n2, where m < n and m depends on the
stride and padding of the convolution. The replication can be
on the order of 1 to 2 orders of magnitude (i.e. 10 − 100×
more data). In turn, this blowup in the data size requires more
memory and computation in step 2 (GEMM). The beneﬁt
however is that a sparse computation has become dense,
which is important for hardware implementations because the
direct computation of the 3D convolution is usually memory
bandwidth-bound (due to the small convolution window size,
k ∈ [3, 11]). A GEMM implementation on the other hand,
while performing more computation as a result of lowering,
receives hardware acceleration which eclipses the increase in
data size.

B. Batching and Data Parallelism

1) Batching Analysis: Batching is the implementation
tradeoff that arises between the CPU and GPU as a result
of available off-chip memory. It concerns how many images
of the batch to process in parallel by the convolution layer.
Recall that bp images are processed in parallel, where where
1 ≤ bp ≤ b. b is the batch size, i.e. the total number of
images that need to be processed. The value of bp (how much
to batch the convolution computation) is determined by how
many lowered images can be stored in off-chip memory.

Modern GPU cards cannot store entire batches of lowered
data into off-chip memory and implementations of CNNs on

GPUs perform lowering and GEMM serially on one or few
images at a time until all b have been processed, i.e. bp =
1. On the CPU, off-chip memory is larger which allows for
batching techniques that perform lowering and GEMM on all
images in parallel and therefore allow CPU caches and vector
instructions to be fully utilized. This tradeoff is continuing
to evolve however, as newer GPU cards contain more off-
chip memory (e.g., 12 GB in the Titan X), and also use new
implementations which perform lowering and GEMM without
having to materialize the intermediate lowered representation,
as described by [19]. Therefore we believe that this tradeoff
will grow in importance.

For example bp = 1 in Caffe [21]: in order to process b
images in a batch, Caffe performs convolution on one image
at a time, i.e. lowering/GEMM is done serially for each image
in the batch (lower one image, run GEMM, lower the next
image, etc.) This has the smallest possible memory footprint
as it only maintains the lowered matrix of a single image in
memory, and is the preferred strategy for devices with limited
memory. Figure 4(c) showed that the memory footprint for the
convolution is directly proportional to bp.

Computationally however, Figure 4 (b) showed that bp = 1
suffers from lower hardware utilization. This ﬁgure was run
for the GEMM in the Conv2 layer of Alexnet (although we
observed similar trends for other Conv layers). Speciﬁcally
the matrix ˆD is “B” in the GEMM operation A × B, and
increasing bp increased the number of columns in ˆD.

In Figure 4 (b) we ﬁx the number of threads to 8, vary
bp, and plot the speedup normalized to bp = 1. Increasing
bp reduces the number of total GEMM calls, and this gives
a 2× overall speedup for bp = 256 compared to bp = 1.
This is because a small bp means that ˆD becomes thinner.
For thinner matrices, possible partition sizes of the underlying
GEMM algorithm are smaller and so the kernel cannot run
optimally, for example the L2 and L3 caches cannot be ﬁlled
during blocking optimizations. As a result bp = 1 is more
likely memory-bandwidth-bound than higher batch sizes (and
this phenomenon is likely more severe when the GEMM kernel
is executed with multiple threads.) Also note that Figure 4
was run on a CPU machine with 8 physical cores. Regarding
Figure 4 (a), the reason that 16 threads was slightly slower
than 8 is that we hit the memory bandwidth bottleneck.

For modern CPUs, memory is large and so b = bp. We use

b = bp for the remaining CPU experiments in this section.

2) Data Parallelism in non-GEMM Kernels: On the CPU,
recall that the batching technique above makes a single matrix
that
is b times (full batch size) larger than it would be
for a single image, and then performs a single GEMM on
this large matrix. A related strategy is to split a batch into
multiple partitions, and process each partition in parallel using
a separate thread.

For GEMM, processing an entire batch of size b with n
threads is equivalent to partitioning the batch into p partitions
of size b/p with n/p threads used in each GEMM. For
example, batching b images and performing a single GEMM
with 8 threads is equivalent to creating 8 matrices, each with

Fig. 14. The impact of data parallelism on the end-to-end execution time of
CaffeNet, run with 256 images per mini-batch on an Amazon EC2 c4.4xlarge
instance.

b/8 images, and performing 8 parallel GEMM kernels with 1
thread each. These are equivalent as this is exactly how BLAS
parallelizes GEMM: by partitioning partition columns of B in
A × B and allocating 1 thread per partition.

While partitioning and then performing the GEMM kernel
is the same as simply performing the GEMM kernel, this is
not true for other kernels which are not multi-threaded For
non-GEMM kernels such as lowering, or other layers such
as pooling, the second technique of partitioning the batch
and processing each partition using a separate thread gives
signiﬁcant speedups (this is simply data parallelism) across all
cores. For example, it can be used to lower images in parallel
using all cores by assigning a subset of the images in the batch
to each core.

Figure 14 shows the impact of data parallelism on a full
end-to-end CaffeNet on the EC2 c4.4xlarge instance with 8
physical cores (CPU only). The batch size used is 256 images
and the horizontal axis represents into how many parallel
partitions we partitioned these 256 images for each layer of the
CNN. I.e. the horizontal axis is the number of threads used
for data parallelism. Note that the GEMM kernel is always
parallelized (OpenBLAS was used) and uses the maximum
number of threads (16).

“None” indicates the default Caffe implementation. For all
layers, each image is processed serially. For example, one
image is lowered, then the convolution GEMM is performed,
and then the next image is lowered, etc. The only multi-
threaded kernels are the BLAS GEMM kernels used in the
convolution and FC.

“1” is identical to the Caffe implementation except that
lowering is ﬁrst done for all images (serially, i.e. one image
lowered at a time), and then a single, large GEMM is done
for the convolution. All other layers are the same.

For all other number of parallel partitions p, the 256 images
were equally split into p partitions. For example if p = 2, two
partitions of size 128 images each were created. Then, threads
processed each partition in parallel, one thread per partition.
For example if p = 2, lowering was done with two threads,
each lowering 128 images. Following the lowering, a GEMM

was performed for each partition. The total number of threads
used for these GEMM kernels was always 16, i.e. the GEMM
is performed on each partition with 16/p threads per GEMM.
Figure 14 (None vs 1) shows that batching the GEMM
kernels (one large GEMM as opposed to 256 smaller ones)
saves ∼ 2.2s of convolution time. Then, data parallelism
provides another ∼ 10 s of reduction. The ﬁnal time is then
4s, where ∼ 3s is spend in convolution layers. Therefore
the batching of the GEMM made the convolution roughly
∼ 2× faster (in the optimized execution), and the remaining
speedups were due to data parallelism.

Finally, studying more closely the speedups from data
parallelism, the time reduction from 14 seconds to 4 seconds
from data parallelism was roughly 80% due to speeding up
the lowering, and 20% due to speeding up the other layers.
I.e. the ﬁnal iteration time would be ∼ 6s if data parallelism
was only used for the Conv layers. The remaining 20% is
for data parallelism in pooling, normalization, dropout and
ReLU layers. Fully-Connected layers are simply a GEMM
which always uses 16 threads, so data parallelism and model
parallelism do not apply on the CPU, although as we also saw
previously this GEMM can be made slightly faster by using 8
threads instead for the FC layers (because there are 8 physical
cores).

Overall, batching combined with data parallelism gives
more than a 4× speedup end-to-end when running Caffe’s
AlexNet on 8 physical cores. Importantly,
this end-to-end
speed is now proportional to the peak throughput of the CPU.

In summary, we show two sources of speedup on the CPU.
First, by batching the lowering and GEMM, we perform a
single GEMM which is b× larger, as opposed to b smaller
GEMMs, which as described above has better hardware uti-
lization on the CPU. Second, we apply the batch partition (data
parallel) technique above to parallelize non-GEMM kernels
such as lowering. These optimizations are possible for the CPU
because it has more memory to store the lowered matrices.
As a result the CPU performance is proportional to the device
FLOPS, which allows partitioning computation across both the
CPU and GPU proportional to the device throughput.

C. Device Throughput

FLOPS Experiments: Figure 3 showed throughput for the
CNN when using Caffe, Omnivore and also for reference a
single-precision GEMM kernel. For the CPU the GEMM ex-
periment used OpenBLAS and matrices of size 16384×16384.
For the GPU GEMM we used a GEMM kernel from NVIDIA’s
CUDA examples. For Caffe and Omnivore we focus on only
the Convolution layers, speciﬁcally the time to run forwards
and backwards on all 5 layers of CaffeNet.

FLOPS calculations: The c4.4xlarge instance contains
a single-socket Haswell CPU with 8 physical cores. The
c4.4xlarge instance CPU FLOPS are calculated as: 8 physical
cores × 2.9 GHz ×32 = 0.742 TFLOPs, where 32 is the
single-precision Haswell instructions per cycle (8-ﬂoat SIMD
× 2 FMA per cycle, and FMA is fused-multiply-add).

Each g2.2xlarge instance provides a single Grid K520 GPU,
i.e. 1536 cores ×800 MHz = 1.23 TFLOPS (the Grid K520
contains a total of 3072 cores, 1536/GPU).

The c4.8xlarge instance contains a dual-socket Haswell
CPU with 18 physical cores. The FLOPS are calculated as:
18 physical cores × 2.9 GHz ×32 = 1.670 TFLOPs.

D. FLOPS-Proportional Scheduling

Given that both CPU and GPU speeds are now proportional
to the device FLOPS, we next consider whether the CPU and
GPU can be used simultaneously to process each layer. We
do this using data parallelism (batch is partitioned, model
is replicated) for all layers in the convolution phase, which
is compute-bound and has small data. The tradeoff is what
fraction of the batch to give to each device. We select the
simple but optimal choice that a device should process a
fraction p of the input where p is the proportion of total
FLOPS which that device contributes. e.g., if a CPU provides 1
TFLOPS and a GPU 4 TFLOPS, 1/5 of the batch is processed
by the CPU for an ideal speedup of 20% over the GPU alone.

E. Single-Node Experiments

Omnivore matches Caffe’s output on each layer. It accepts
the same input ﬁles as Caffe and produces the same outputs.
Our experiments compare against Caffe and use the CaffeNet19
CNN, which is Caffe’s implementation of the popular AlexNet
(the default architecture for benchmarking), as well as the
ImageNet dataset.

Both systems take as input the same network conﬁguration
ﬁles that Caffe provides. We remove grouping for convolution
layers because the full AlexNet ﬁts in the memory of a
single K520 (g2.2xlarge) GPU. We use the same external
libraries for both Caffe and Omnivore: GCC-4.8, CUDA-7.5,
and OpenBLAS. For Caffe we report both cuDNN v3 and v4.
We built the same timer into Caffe and Omnivore (measur-
ing wall-clock time, clock_gettime in C). We run for 50
iterations and omit the ﬁrst 10 in case there are disk or other
delays in the ﬁrst few iterations not representative of steady-
state execution. Beyond these ﬁrst 10 iterations we noticed
that all iterations were consistent in terms of time for both
tools and had a coefﬁcient of variation less than 5%.

We also ran Tensorﬂow using the same protocol as above

(40 iterations, burn-in of 10, identical network).

1) End-to-end Performance: Figure 15 shows the results
of running Omnivore and Caffe on various EC2 instances.
Given that Omnivore and Caffe generate the same outputs, we
concentrate on throughput. We ran both Omnivore and Caffe
on each EC2 instance for 50 iterations, and counted the last
40. On the c4.4xlarge CPU instance Omnivore outperforms
Caffe by nearly 4× due to batching and data parallelism. On
the g2.2xlarge GPU instance Omnivore and Caffe achieve the
same speed (within 5%). Note that the c4.4xlarge instance
offers 60% of the FLOPS of the g2.2xlarge instance, and
the ratio of Omnivore speeds on these instances is 59%, i.e.

19https://github.com/BVLC/caffe/tree/master/models/bvlc reference caffenet

×16 physical cores ×2.6 GHz). The ratio of CPU:GPU
FLOPS is therefore 1:2, i.e. we should partition roughly 1/3 of
the data on the CPU and 2/3 on the GPU. Since a batch size
is 256 images, we rounded 67% on the GPU to 75%, such
that the CPU processes 64 images, because this partition is
better suited to hardware (although we see only a 5% speedup
compared to using the exact ratio). This partitioning gives a
18% speedup over just using the GPU.

For parallelization across 4 GPUs, we use data parallelism
for all layers (each GPU given 1/4 of the batch and a model
replica) except for the FC layers, which use model parallelism
(each GPU given 1/4 of the model and a replica of the batch).
We ran Caffe on 4 GPUs with cuDNN v4, cuDNN v3 and
no cuDNN, and found that Caffe was fastest but neither gave
a speedup compared to 1 GPU.

state-of-the-art

Therefore while CNN parallelization is challenging even
for
systems, we’ve shown that FLOPS-
proportional partitioning is possible across a range of hardware
devices. We now extend this technique to multiple machines,
where the added challenge of network delay motivates re-
thinking the SGD algorithm.

Fig. 15. End-to-end performance comparison across EC2 machines on
CaffeNet. All numbers are normalized as the speedup over running Caffe’s
GPU version on g2.2xlarge instance ($0.65/hour).

APPENDIX D
APPENDIX FOR DISTRIBUTED TRADEOFFS (SECTION IV)

Omnivore delivers speed proportional to the FLOPS. On the
two-socket c4.8xlarge CPU instance Omnivore is now 5.5×
faster than Caffe. Caffe does not speed up given the additional
cores, but Omnivore does. The speedup is not linear because
the extra cores are distributed across sockets and not all layers
are compute-bound. However, these results show that given
similar device throughput, CPU CNN performance is not an
order of magnitude slower than GPU performance as the
literature often reports and as is the case in Caffe. Lastly we
compared Omnivore to Caffe on a 4-socket, 56-core Haswell
(non-EC2) CPU machine, and Omnivore is 13× faster than
Caffe.

FLOPS calculations: See Appendix C-C for the FLOPS

calculations in Figure 15.

2) CPU/GPU Hybrid and Multi-GPU: Figure 15 also
shows that using the CPU in a GPU instance can accelerate
purely GPU training. The g2.8xlarge instance’s CPU provides
0.67 TFLOPS, and by using data parallelism across the CPU
and a single GPU we achieve an 18% speedup in Omnivore
end-to-end over just using the GPU. This is faster than all
other single GPU results.

Finally, we also apply this data parallel partitioning across
multiple GPUs. We ran both Omnivore and Caffe using the 4
GPUs on the g2.8xlarge instance and show that while Caffe
actually slows down compared to the 1 GPU case, Omnivore
becomes 3.1× faster.

For CPU + GPU, each g2.8xlarge GPU provides 1.23
TFLOPS as shown above, and the CPU provides 665.6
TFLOPS (Sandy/Ivy bridge, i.e. 16 SP instructions per cycle

Having studied the tradeoffs for a single machine,

this
section now studies the distributed setting. The goal of this
section is to build an optimizer that creates (1) a physical plan
P (A, G) which maps the CNN architecture A to machines in
the device graph G, and (2) an execution plan E(G, D) which
parallelizes SGD by allocating data batches from the dataset
D to each machine. This section begins by describing why
these two are the most important tasks for the optimizer. While
many distributed CNN systems exist [3], [5]–[9] and each
describes their own distribution techniques, upon analyzing
these strategies we discover that,
they all
describe either (1) or (2) above. Given these two fundamental
design dimensions, we then arrange existing strategies into
a tradeoff space and restate our optimizer’s goal precisely
within this space. The remainder of the section then quantiﬁes
the impact of these tradeoffs to allow optimization within the
space.

though diverse,

A. Distributed CNN Tradeoff Space

This section describes popular techniques for distributed

CNN training and reﬁnes them into a tradeoff space.

1) Distributed Stochastic Gradient Descent: Recall

that
SGD iteration i (1) reads a batch of data Bi, (2) uses the
current model Mi to compute the model gradient ∇M (Mi),
and (3) subtracts ∇M (Mi) from Mi to obtain Mi+1. Iteration
i + 1 reads a new batch Bi+1 and the algorithm repeats until
convergence.

Figure 16 (a) shows a common distributed implementation
of SGD in which the entire CNN model is stored on a server
called a model server or parameter server. There are also
a number of compute servers which each perform the CNN

Fig. 16. SGD server architectures.

Fig. 17. A graphical illustration of (a) one worker (S=0) and (b) two workers
(S=1).

calculation (Equations 1 and 2). Each iteration, every compute
server reads M over the network from the parameter server,
as well as a batch of data B from a local database. Each
compute server calculates a gradient ∇M which is sent back
to the parameter server and used to update the model. In
this example, compute servers operate in parallel and do not
synchronize or communicate with one another.

In Figure 16 (a), each server physically maps to a single
machine (node). Generally, multiple servers can map to a
single machine or a single server to multiple machines. For
example, parameter and compute servers can map to the same
node to reduce network communication. Figure 16 (b) shows
a more complex server architecture for SGD in which rather
than two types of servers (compute and model), there are now
4 types: (1) conv compute, (2) conv model, (3) FC compute,
and (4) FC model. 1 and 2 are for layers in the conv phase
of the CNN while 3 and 4 are for layers in the FC phase.
In Figure 16 (b), the FC compute and model servers map
physically to the same machine, i.e. the computation of the
FC phase happens on the same machine as where the FC
model is stored. There are many beneﬁts to Figure 16 (b) vs.
(a): Recall that FC has small data, large model whereas conv
has large data, small model. This server architecture has the
beneﬁt of only needing to communicate the conv model (and
its gradients) and the FC data (and its gradients) across the
network. Second, computation is ofﬂoaded from the compute
machines to the FC model server, which otherwise is majorly
idle. These beneﬁts both improve hardware efﬁciency, and
were described in [5]. However, yet another beneﬁt is that
by having only a single FC compute machine, the FC model
does not experience any staleness – a term we deﬁne next.
This improves statistical efﬁciency.

2) Staleness: Staleness is a metric used to quantify the
statistical efﬁciency (# iterations to convergence). Figure 17
shows staleness graphically for the simple server architecture
of Figure 16 (a). In Figure 17 (a) there is a single worker,
and so that worker always computes the gradient using the
current model. In this diagram, we assume that once a worker
sends the updates back to the model server, it is immediately
sent a new model, i.e. in this diagram, write/read is an atomic
operation for each worker (the write to the model and read

from the model happen together).

In Figure 17 (b), there are now two concurrent workers.
Assume for now that (1) these workers update the model in
round-robin order, (2) write/read is again atomic, and (3) the
server architecture is again as in Figure 16 (a). We notice
that now each worker computes the gradient using a copy
of the model which is stale by 1 iteration, e.g., updating
model 2 to produce model 3, but doing so using a gradient
update which was calculated using model 1. The reason for
this staleness is that while worker 0 is computing its next
update, worker 1 updates the model. This staleness is bad
for statistical efﬁciency (i.e. more iterations are required to
converge) because now each gradient used to update the model
no longer points in the direction of steepest descent for the
model it is applied to, but rather the direction of steepest
direction for a model from an earlier iteration.

Precisely, we deﬁne staleness as follows: given N workers,
the staleness is the number of updates to the model between
a worker’s read from the model and subsequent write back to
the model. Because the updates are round-robin, this staleness
is the same for all workers, and is equal to S = N − 1.
e.g., if there are 100 parallel workers, S = 99. Intuitively,
the staleness is just equal to the number of parallel workers
sending gradient updates (minus one, although this can often
be ignored because the number of workers is large).

The three assumptions above are useful to give a precise

deﬁnition but are not necessary in practice.

Assumption 1:: In practice the workers do not proceed in
round-robin order due to inherent variance across machines,
but we observe empirically that for dense models the updates
are nearly round-robin. This is because dense model compu-
tations like those used in deep learning have roughly constant
time per iteration (this is not true for sparse models).

Assumption 2:: Writes and reads do not need to be
atomic, and in fact this can be beneﬁcial for statistical ef-
ﬁciency. Rather than have a conv compute server request an
entire updated model as soon as it publishes all of its gradients
to the conv model server, it may instead publish gradients
layer-by-layer in a backwards fashion during the backward
pass of iteration i, and then lazily request the updated model
in the forwards pass of the next iteration i + 1. For example,

conv compute worker processes half
the data of a single
batch, using the same model replica, and produces half of
the ﬁnal gradient. A barrier is then introduced in which the
gradients are summed, and a single, ﬁnal gradient is applied to
the model. Figure 18 (b) seemingly solves both problems: all
the hardware is being utilized (good for hardware efﬁciency),
and S = 0 (good for statistical efﬁciency). However the
price we pay is in hardware efﬁciency, speciﬁcally the cost
of synchronization across the machines. Indeed, we will show
that the hardware efﬁciency of Figure 18 (b) is poorer than
that of Figure 18 (a).

We deﬁne a compute group as a group of machines working
together to process a single batch of data. A compute group
is characterized by processing a single data batch at a time,
with all machines in the group using the same model replica,
to the model server. The
and returning a single gradient
compute groups in Figure 18 are shown with black dotted
lines. Figure 18 (a) has 2 compute groups, and since there are
2 machines, the compute group size is 1 machine. Figure 18
(b) has 1 compute group of size 2. Note that this allows us
to simplify our deﬁnition of staleness: because the number of
parallel gradients being computed in the system is equal to the
number of compute groups, the staleness is just equal to the
number of compute groups (minus one).

Generally, if we have N machines used as conv compute
servers, Figure 18 (a) and (b) show two extreme cases.
Figure 18 (a) is the extreme case of 1 machine per com-
pute group, and N groups. This technique is often called
asynchronous SGD, or “async” for short. In async, workers
do not communicate and each worker updates the model
independently. Every worker computes a separate gradient
using a separate batch and separate model replica, and then
sends these gradients to the parameter server in order to
update the model. Figure 18 (b) is the other extreme case
of 1 group, and all N machines in that single compute group.
This technique is often called synchronous SGD, or “sync”.
In sync, all machines work synchronously and in parallel on
a single batch of data and using a single model replica to
compute a single gradient. In this case the gradients over
all workers are aggregated each iteration (or batch) before
updating the model. An intermediate conﬁguration could also
exist, for example one which has 4 groups each of size N/4.
There could even be groups of different sizes if different
machines have different throughput (some GPUs, some CPUs,
etc.). Notice that because the compute group in Figure 18 (b)
parallelizes the conv phase, it uses data parallelism, i.e. the
batch is partitioned across the machines in the group.

4) Precise Problem Deﬁnition: This section has described
two key tradeoffs: (1) the server architecture, concerned with
physically mapping servers to machines, and (2) the execution
strategy (the number of compute groups vs. their size), con-
cerned with mapping batches to servers. We can now restate
the goals of the optimizer from II-D in terms of our tradeoff
space. Given (1) A, the CNN architecture, (2) D, the dataset,
and (3) G, the device graph, our optimizer transforms A into
S, an abstracted network of logical server types (Convmodel,

Fig. 18. Two different execution strategies given a ﬁxed machine budget.

AlexNet may update the model with gradients from conv5,
conv4, conv3, conv2, and conv1, and then begin its next
forwards pass and request conv1, conv2, conv3, conv4 and
conv5. These requests can happen asynchronously from the
computation to hide latency and overlap the network delays
with the computation. As a result the write/read for conv1 may
be almost atomic, but there would be some delay between
the write/read for conv5. This delay in fact reduces staleness
slightly because it reduces the number of intermediate writes
by other workers between a worker’s read and subsequent
write (intuitively, in the extreme case, if the delay was very
large, then every worker would write before any of them
read the new model. Then this is just equal to mini-batch,
except with a larger batch size, although that would of course
make each iteration slower, i.e. harm hardware efﬁciency). In
practice we observed a roughly 20% reduction in the number
of iterations to converge by requesting models in this lazy
fashion (which does not harm hardware efﬁciency).

Assumption 3:: Staleness also applies to the server ar-
chitecture of Figure 16 (b), which recall reduces network
communication by merging the FC model and FC compute
servers, i.e. mapping them to the same machine which does
both the gradient computation and model updates for the FC
phase. This merged FC server processes only one batch at a
time, and thus produces only one FC model gradient at a time.
This means that the staleness for the FC model is 0 which is
good for statistical efﬁciency. The conv compute servers on
the other hand still calculate the updates to the conv model
in parallel (once they receive their data gradients from the FC
machine), therefore the merged architecture in Figure 16 (b)
still contains staleness, but only for the conv model.

3) Compute Groups: The ﬁnal concept common to all
systems is the compute group. Consider the example of two
conv compute machines in Figure 18 (a). This conﬁguration
has a conv staleness of S = 1, as there are 2 workers
independently updating the model. However, it is not necessary
to introduce staleness in order for both compute machines to
be utilized, and Figure 18 (b) shows a second conﬁguration
in which data parallelism is used across the machines: each

of the model replica and 1 replica of the data batch. This is
useful for parallelizing fully-connected layers, which contain
small data but large models: each device receives 1/N of the
model and a replica of the data, and uses the data to compute
a gradient for that portion of the model. These gradients in
total combine to a single gradient with respect to that entire
model, using the data batch.

Note that logically, a single gradient is produced by all
devices together and that single gradient is used to update the
model and produce a new model (physically the updates may
occur locally on each device communication, i.e. the device
updates its portion of the model).

Data Parallelism: A single replica of the data batch and
N (identical) replicas of the model are created. Each device is
given 1/N of the data replica and 1 replica of the model. This
is useful for parallelizing convolutional layers, which contain
small models but large data: each device receives 1/N of the
data and a replica of the model, and together they calculate
a single gradient with respect to that entire model using the
data batch.

Note that, as mentioned above, here the model is always the
same for each device, i.e. the model is replicated and a single
gradient is produced. That gradient is then used to update the
model and the model is then re-broadcast to each device. So
while model replicas exist, they are identical model replicas.
(Note: model replica is a logical term and does not always
correspond to a physical replica. In particular, when two
devices share memory, e.g., 2 CPU threads running on 2 cores,
no physical replica is necessary as the same model in memory
can be read by both.)

2) Asynchronous Batches: The above deﬁnitions apply to
a single data batch in the system (although it may have been
replicated for the purpose of parallelization). Consequently,
there was always one gradient being computed at once, and
so each model replica was identical.

This deﬁnition focuses on the asynchronous case mentioned
previously, i.e. NB parallel batches in the system being used
to compute NB parallel gradients. Each of these NB batches
of data in the system is different. Each batch is allocated to
a compute group (a group of devices), and each batch (i.e.
each group of devices) is given a replica of the model. For
example if NB = N/4, then there will be 4 devices assigned
to each batch, and one model replica assigned to each of these
N/4 batches (i.e. each group of 4 devices). Each compute
group of 4 devices will then compute a gradient given that
model and that batch, and the devices in the group may do
so using either model or data parallelism (i.e. choosing to
create additional model or data replicas within the group, as
described above, e.g., if the group uses data parallelism it will
create 4 additional model replicas, but each of these models
will be identical).

Note that here the number of batches in the system, the
number of compute groups, and the number of parallel gradi-
ents being computed by the system are all the same number.
Within a compute group (group of devices), additional model
replicas will be the same. However across compute groups, the

Fig. 19. An Overview of our Mapping.

Convcompute, FCmodel, and FCcompute), and creates (1) a physical
plan P (S, G) mapping each server in S to machines in G (note
that we also refer to this distributed portion of the physical plan
as the server architecture), and (2) an execution plan E(S, D)
which deﬁnes the number of compute groups by allocating
batches of D to each server in S. Both of these choices impact
hardware and statistical efﬁciency, and our next task is to
quantify this impact. First, we present terminology and then
ﬁnish this section by describing where existing systems fall
within this tradeoff space.

Our mapping from CNN layers to devices is shown in

Figure 19.

We are given: (1) a network, which can be viewed as a
labeled directed acyclic graph in which each node is a type
of layer (e.g., convolution, fully connected, max pooling) and
edges indicate dataﬂow dependencies, and (2) a set of devices
grouped into machines. Our goal is to devise a mapping from
the network to the machines.

B. Terminology

In this work we interchangeably uses the terms node and
machine to refer to a single box, connected to other nodes
or machines over a network. We also interchangeably use the
terms device and (when describing a device graph) vertex to
refer to a discrete unit of compute hardware (e.g., a GPU or
CPU core. When discussing a cluster, a device can also be a
machine in that cluster.). Finally, we also use the terms group
and compute group interchangeably, as deﬁned in Section II.
Existing work also often contains a lot of terminology,
which we summarize here. Consider for these examples the
simple case of N identical compute devices (e.g., N machines,
N GPUs, N CPU cores, etc.)

1) Synchronous Batches: These ﬁrst two deﬁnitions apply
to a single batch of data, i.e. there are not multiple parallel
batches in the system used to compute asynchronous gradients,
but rather a single batch used to compute a single gradient.
As a result, this gradient is applied to the model at once to
produce a new model, i.e. there only ever a single model in
the system (although it may be replicated, in which case all
models are identical). This is also known as the synchronous
case above.

Model Parallelism: A single replica of the model and N
replicas of the data batch are created. Each device is given 1/N

model replicas may be out of sync, because of asynchronous
gradient computations. This is discussed in the ﬁnal deﬁnition.
To make all the deﬁnitions above concrete, consider the
example of 2 devices, e.g., 2 GPUs. There are 3 possible
scenarios: (a) 2 parallel asynchronous batches (“async”), (b) 1
batch with data parallelism across the 2 devices (“sync”), or
(c) 1 batch replicated twice with model parallelism across the
2 devices (also sync).

3) Combining Model Replicas: If there are G asynchronous
compute groups, each using a separate data batch and produc-
ing a separate gradient each iteration, then each group will
also have a separate version of the model as a result of the
asynchronous gradient computations.

then broadcast

Parameter server is a technique in which these separate
compute groups will not perform their model updates locally,
but rather each publish their gradients to a global parameter
server which will
to a
group upon receiving the update for that group. In this way
the groups always have models which are almost the same
(they will still be slightly out of sync because gradients are
published asynchronously and so models are returned to the
groups asynchronously as well). DistBelief, Adam, SINGA,
MXNet use this technique, and it is the focus of our study.

the updated model

Note that if the parameter server waits for each parallel
group to publish a gradient and then broadcasts the model
to all groups (i.e.
this is no longer
introduces a barrier),
asynchronous, and is now equivalent to the synchronous case
above (data parallelism) where the batch size is N times larger
than it is per individual compute group.

Finally, the updates to the server from parallel groups can
happen with or without race conditions. The case of race
conditions is known as Hogwild!.

Model Averaging is a technique in which there is also
a global parameter server, but model updates happen locally
within each group. Then periodically, every τ iterations, the
groups will publish not their gradients but their entire models
to the parameter server. The parameter server will average
the models (reduce step) and then broadcast them to each
group (map step). This averaging does not have theoreti-
cal guarantees because neural networks are non-convex, but
works in practice. This technique works well for map/reduce
frameworks, e.g., Spark/Hadoop, and is used by SparkNet and
DL4J. Here the models are more different than they are in the
parameter server case.

The choice of the τ parameter is similar to the tradeoff
of multiple groups of varying size, except that here staleness
comes not from multiple asynchronous workers updating a
single model, but multiple workers with a separate model
combining models. In the case where τ = 1, this is identical
to the synchronous case of parameter server (all machines in
a single group, i.e. all computing a single batch/gradient).

Ensembles are used by AlexNet. Here each group trains an
entirely separate model to convergence and then predictions
of these models are combined (e.g., through voting). The
gradients or models themselves are never combined.

TABLE II
POINTS IN THE DISTRIBUTED CNN TRADEOFF SPACE CHOSEN BY
POPULAR SYSTEMS. G IS THE NUMBER OF COMPUTE GROUPS, N IS THE
NUMBER OF MACHINES.

1<g<N

Model
Avg.

Merge
FC

sync
(g=1)
•

•
•
•

Tool

DistBelief [3]
Adam [5]
FireCaffe [9]
MXNet [6]
SINGA [7]
SparkNet [8]
DL4J

async
(g=N)
•
•

•
•

•
•

•

•

•
•

In this study we focus on the parameter server approach,
which is the most widely used by distributed CNN systems.

C. Existing Systems

Using the terminology above, we discuss design decisions
made by CNN systems in Table II. In our review of the
literature, these are the tradeoffs which we identiﬁed as most
impactful to minimizing convergence time.

Execution Strategy:

In terms of execution strategies,
Microsoft’s Project Adam [5] reports that the async strategy
is effective and uses one or a few (e.g., 4) machines per
compute group. Moreover, they report that using a technique
called Hogwild! [25], which introduces race conditions, is
an effective source of additional hardware speedups. On the
other end of the spectrum, FireCaffe [9] implements the sync
strategy and notices that high scalability can be achieved by
having all machines work synchronously and in parallel on a
single, large batch of data, and by reducing communication
using reduction trees. MXNet [6] implements both the sync
and async strategies, and allows the user to select which to
use. They also call the sync strategy the Bulk Synchronous
Parallel (BSP) protocol. Finally, Google DistBelief [3] and
Apache SINGA [7] implement both sync and async as well
as the compromise of multiple, larger compute groups. They
also call sync Sandblaster, and cases of more than one group
Downpour (e.g., Downpour with group size of 1 machine is
equivalent to the async). SINGA calls the intermediate case
of multiple, larger compute groups hybrid (SINGA also has
hybrid parallelism, which is different. That is a combination
of model and data parallelism.)

All these systems implement the Parameter Server tech-
nique, described above in the Terminology section (Ap-
pendix D-B). It is the most widely used technique by dis-
tributed CNN systems and the technique we focus on in
this study. Another technique known as model averaging,
which works well within a map/reduce framework, is used
by SparkNet [8] and DL4J (http://deeplearning4j.org/). Model
averaging is also described above. The key difference between
model averaging and parameter server is the way in which
model replicas are combined.

Physical Map, Modern Networks: The second point in
the distributed tradeoff space is the server architecture (how
servers map to hardware), speciﬁcally whether the FC compute
and FC model servers are mapped to the same physical
machine (or machines, for multi-machine model parallelism).

This is a technique introduced by Microsoft’s Project
Adam [5] to avoid communicating the large FC model and
its gradient. The method was reported for older networks
with large, fully-connected layers (AlexNet, VGG), however
it also is useful for modern networks (Inception, ResNets).
In traditional networks, the fully-connected layers contained
the majority of the model parameters [2] (> 90%). Newer
networks instead use average pooling and have only a single
FC layer (for softmax regression) [20]. Therefore newer net-
works contain fewer parameters in the FC phase, and fewer
parameters overall, however this single FC layer can still be
very large (e.g., when predicting among 22,000 classes on
ImageNet 22k) and still beneﬁts from reduced FC communi-
cation (because the number of FC weights will always be less
than the number of inputs to the FC phase). Therefore while
newer networks contain only a single FC layer, the merged
FC optimization of Project Adam is still relevant, and while
a characteristic of newer networks is that their overall model
size is smaller due to the elimination of multiple FC layers,
ultimately this does not translate to reduced communication
overall because the cost of communicating the FC layers has
been eliminated in prior work.

is eliminated (i.e.

In addition, as we show, the beneﬁt of merging the FC
servers is not only improving HE due to reduced network
communication, as [5] noted, but also improving SE because
staleness in the FC model
the device
or devices which compute the FC model gradient updates
also store that subset of the model). Moreover there is no
consequence of merging the FC servers for small FC models
because little computation in the FC phase means it is less
likely for the FC to saturate (become the bottleneck), but
merging still provides the beneﬁt of (1) improving SE, (2)
reducing communication and (3) ofﬂoading computation to the
parameter server machines.

As our goal is to be a complete study, we study both
cases (many large FC layers, few small FC layers) in order
to build a system which is robust to any application. For
example future CNN architectures may employ multiple FC
layers again to support transfer learning tasks, or may need
to predict among many object classes (e.g., hundreds of
thousands or millions), further increasing the communication
bottleneck for the FC. In addition, FC layers are also used
for RNNs and other architectures.

This is the subset of the tradeoff which we study in this
work, summarized in Table II. Our goal is to ﬁnd best point in
the tradeoff space given the model, data, and hardware speciﬁ-
cations from the user. Speciﬁcally we do not change the neural
network architecture, but assume that this model is given to
us. I.e. we do not focus on machine learning algorithms or
techniques in this work, but rather study systems tradeoffs
which exist for the most widely used networks/algorithms.
We focus on the SGD algorithm for learning, as it has been
and continues to be used along with momentum by annual
ImageNet winners [2], [20]. Other algorithms exist for training
deep learning models and can also be parallelized, for example

Fig. 20. Hardware efﬁciency penalty for various numbers of compute groups.
The number of machines is ﬁxed to 32. Each machine is an EC2 c4.4xlarge
(CPU) machine.

Google’s deep learning system uses Adagrad [3]. Microsoft on
the other hand uses SGD [5]. Because the systems tradeoffs
we study in this work are orthogonal to the choice of update
algorithm, in this work we focus on SGD, although the same
tradeoff applies to other algorithms as well.

D. Hardware Efﬁciency Model

The goal of this section is to create a predictive model for
how the hardware efﬁciency (HE) varies with the amount of
staleness S in the system, given a ﬁxed number of machines
and batch size. Recall that the staleness S is equal to the
number of compute groups (minus one). This is because a
compute group is characterized by processing a unique data
batch and returning a unique gradient to the model server, so
the number of parallel gradients being computed is equal to
the number of compute groups. S = 0 is the case of 1 compute
group, also called the synchronous case.

Figure 20 shows a plot of staleness vs. hardware efﬁciency
penalty for three datasets. The standard networks from Caffe’s
tutorials are used for each dataset. The hardware efﬁciency
penalty, or PHE, is deﬁned as the ratio of the time per iteration
relative to the time per iteration for this synchronous case,

PHE(S) =

HE(S)
HE(0)

the smallest

A higher PHE is worse (more time per iteration). PHE
decreases (iterations become faster) as the number of compute
groups increases. This is because, if we ﬁx the number of
time per iteration occurs
machines to be N ,
when there is no synchronization, i.e. when the compute group
size is 1 and there are N compute groups (asynchronous
case). In this case S = N − 1. As the compute group sizes
increase, and hence the number of groups decreases (because
the number of machines is ﬁxed to N ), PHE will increase due
to synchronization delays within the groups. When the number
of compute groups is 1, that group contains all N machines
and requires the most synchronization. In this case S = 0, and
this has the highest penalty PHE.

The hardware efﬁciency penalty is dimensionless. It is the
ratio of hardware efﬁciencies (time per iter / time per iter).
Because PHE is normalized to the synchronous (S = 0) case,
PHE ≤ 1. Note also that the hardware efﬁciency penalty is
only comparable across different staleness values if the number
of machines is ﬁxed.

The hardware efﬁciency penalty can be predicted analyti-
cally. However, as we will see in a later section, the cold-start
phase of the optimizer performs a short adaptive grid search
across different staleness values. Because a few iterations are
run for various staleness values, the execution time for these
iterations can be used to provide a precise hardware efﬁciency
model (deep learning layer computations are dense, not sparse,
so there is little variation in iteration time). Nevertheless
understanding the hardware efﬁciency precisely is important:
1) to understand the execution bottlenecks and either hard-
code or allow the optimizer to make physical mapping
choices

2) because it may be too time-consuming to obtain static
information for every staleness value of interest, and
3) because our work is a study meant to inform future

systems which may not use a static optimizer

Figure 20 was run on 33 EC2 CPU machines. The server
architecture shown in Figure 18 was used, i.e. one machine
contains the merged FC compute and FC model servers, and
the other 32 machines contain Conv Compute servers. The
Conv Model server is mapped to one of the Conv Compute
machines. We make two observations, which are true for all
datasets in Figure 20:

Observation 1: As the number of groups decreases (and
hence as their sizes increases), the hardware efﬁciency be-
comes poorer. There are two reasons for this: (1) machine
variance, which causes synchronization delays, and (2) net-
work congestion, because the convolution model needs to be
sent simultaneously to all machines in the group (and gradients
need to be send back simultaneously). Our analysis below
shows that while machine variance exists, it is insigniﬁcant
compared to increased network congestion.

Observation 2: As the number of groups increases, the
speedup is not linear (it saturates). This is because the FC
phase processes only a single batch at once, or equivalently
because the FC compute and FC model server map to the same
physical machine (or machines, as the FC compute / model
server may use multi-machine model parallelism). Recall that
this has beneﬁts for both hardware efﬁciency (by reducing
network communication) and statistical efﬁciency (by reducing
the staleness of the FC model). However it means that only
one gradient computation (batch) is processed by the FC at a
time, and so it may become a computational bottleneck.

Many optimizations exist for both of these observations.

They are presented after our derivation of the model.

1) Full Derivation of Analytic Model: Formally, let there be
N + 1 machines. 1 machine is allocated to the FC phase, and
N machines (e.g., 32) to the conv phase. An execution strategy
partitions the N conv machines into g compute groups. Each
group contains k machines, and the k machines in a group
compute the conv phase with data-parallelism. Therefore, there
will be g = N/k compute groups sharing the single FC server
machine. In addition, let tconv(k) be a function that returns the
time that a group of size k needs to compute the convolution
phase (forwards and backwards for only the conv phase), and
tf c be the time that an FC server needs to serve one group

Fig. 21. Gantt chart illustrating the Hardware Efﬁciency model for the server
architecture of Figure 18. Case 1 is FC saturation (top), Case 2 is Conv
Saturation (bottom), and the boundary between the two is shown in the middle.

(forwards and backwards for only the FC phase. Note that tf c
is independent of k, the number of machines used to perform
convolution on each batch). We also deﬁne that tf c includes
the network time to transfer the data from the conv phase to
the fc phase and the data gradients from the fc phase back to
the conv phase, although we observe that this network time
is often small compared to the computation time of tf c. Note
that tf c is independent of k, the number of machines used
to perform convolution on each batch. Finally, assume for
now as we did above that different groups (batches) cannot be
executed in parallel on the FC server (that case is described
later).

Given k, g, tconv(k) and tf c, our goal is to create a hardware
efﬁciency model which predicts the time per iteration or,
equivalently, which given a time interval T predicts how many
batches will be processed by the system. Because each batch
must be processed by the FC server, this is equivalent to
determining the number of requests that the FC server can
process from the g convolution groups in time T . There are
two cases, also illustrated in Figure 21.

Case 1: Saturated FC Server The ﬁrst case is when
FC server is saturated, i.e. it starts to serve the next request
immediately after the previous request ﬁnishes. In this case,
the hardware efﬁciency is straightforward. The server will
serve T /tf c requests in time T, or equivalently,

Time per iterationsaturated fc = tf c

Case 2: Saturated Conv Server When the FC Server is
not saturated, each conv server becomes the bottleneck. In this
case, the FC server serves T g/(tconv(k)+tf c) requests in time
T, or equivalently,

Time per iterationsaturated conv = (tconv(k) + tf c)/g

which is the total time for a single iteration divided by the
number of parallel groups. This is because the groups all are
computed in parallel, with the exception of the FC server
which is serial, but the FC server is never saturated so it can
also be seen as being part of each parallel group. Refer to
Figure 21 for an illustration of this case. To understand this
case, note that:

1) When each conv server is fast (tconv(k) is small), the

FC server serves more requests in time T

2) When the FC server is fast (tf c is small), the FC server

serves more requests in time T

3) When the number of concurrent group is large (g is
large), the FC server serves more requests in time T
Determining Saturation Finally, the model needs to predict
when the FC server will saturate. This occurs at the boundary
of the times above, speciﬁcally the FC server saturates (case
1) when:

tconv(k) + tf c < gtf c

Intuitively, if the combined FC time to process all groups
(gtf c) exceeds
iteration
(tconv(k) + tf c), then the FC server will always be saturated.
Note that:

the time for a single group’s

1) When each conv server is fast (tconv(k) is small), it is

easier to saturate the FC server

2) When the FC server is fast (tf c is small), it is harder to

saturate the FC server.

3) When the number of concurrent group is large (g is

large), it is easier to saturate the FC server.

We now have an expression for the time per iteration in both
cases as well as a condition to decide which case applies.
Given the following:

• Two of: N , g or k (the third can be calculated from the

• tconv(k), the time to complete the convolution portion of
the network (forwards and backwards) given the group
size, and

• tf c, the time to complete forwards and backwards on the

other 2),

FC phase,

the model can predict the mode of saturation and therefore the
time per iteration.

tconv(k) can be calculated given the throughput of each

node and the network speed. It has two components:
tconv,compute(k) and tconv,network(k).

Let us deﬁne tconv,compute(1) = Tc,c, i.e. Tc,c is the time
it takes for a single machine (k = 1) to compute the forward
and backward pass of the convolution phase. Similarly, let us
deﬁne tconv,network(1) = Tn,c, i.e. Tn,c is the time needed
for a single copy of all the conv phase’s models (forwards
pass) and model gradients (backwards pass) to be passed over
the network. We will describe how to determine these two
quantities later.

The computation time for the convolution phase for k > 1
is then tconv,compute(k) = Tc,c/k, because recall that a single
compute group performs computation on a single batch of data

(data parallelism), i.e. the amount of data per group is always
the same per iteration (e.g., b images) and so if there are k
machines in a group, each will process b/k images. We assume
a linear speedup.

On the other hand, the time for the network communication
increases with k. This is because of increased network com-
munication from the conv model server, i.e. the model needs
to be sent to k workers simultaneously and gradients will be
received from k workers simultaneously (all requests are made
at almost the same time, because the workers in the group are
synchronous). The network time for the convolution phase for
k > 1 is then tconv,network(k) = Tn,c ∗ k. Here, we assume a
linear slowdown.

So while the compute time decreases with k, the network
time increases with k. We assume that both of these are linear.
Empirically we notice that the convolution computation does
not scale exactly linearly with k: on 8 c4.4xlarge machines in
a single group, the forward pass of the convolution becomes
7.2× faster and the backwards pass 6.6× faster. Similarly, we
observe that the network slowdown is usually linear but can
be super-linear, which we attribute to thrashing.

Given tconv,compute(k) and tconv,network(k), we can

naively approximate

tconv(k) = tconv,compute(k) + tconv,network(k)

However, these two can be done in parallel, i.e. while one layer
is computing its forwards pass, the model for the next layer
is being sent over the network. This does not entirely overlap
because the ﬁrst layer needs to complete its backwards pass
before requesting the model for its next forwards pass, but we
can approximate the total convolution phase time as:

tconv(k) = max(tconv,compute(k), tconv,network(k))

it

Finally,
is necessary to obtain Tc,c, Tn,c and tf c. We
measured these because they only need to be measured once
(not for each k), but they can be calculated using the node
throughput and network throughput: Tc,c and tf c can be
approximated by counting the total number of operation from
each GEMM operation in the conv and fc phases and assuming
that BLAS achieves the device peak. Tn,c can be approximated
by counting the total number of bytes in the conv models
and assuming the peak network throughput is achieved. These
assumptions are justiﬁed because the matrices and models are
large.

Also note that measurements of these quantities are accurate
for all iterations because deep learning computations are dense
and so there is little variation in the computation time across
iterations, as shown in Figure 22. Note that there is a standard
deviation of than 6% for tconv(1) and tf c, and a standard
deviation of 8% for the total iteration time. For CIFAR-10,
the standard deviation for total iteration time was 1.5%. We
also observed similar variances on a GPU cluster.

Using measurements of Tc,c, Tn,c and tf c, Figure 5 shows
that the analytic model of hardware efﬁciency is accurate.
When the FC server is saturated (right side of the graph), the
model is almost exact. When the FC server is not saturated

FC compute and FC model servers are mapped to the same
physical machine. Also recall that this has beneﬁts for both
hardware efﬁciency (by reducing network communication) as
well as statistical efﬁciency (by reducing the staleness of
the FC model). However it means that
there is only one
FC compute server, and so it may become a computational
bottleneck. This is seen in the top Gantt chart of Figure 21
in which there are blank spaces which indicate un-utilized
machines.

A simple way to ﬁx this is to make the FC machine faster,
for example if there is limited access to GPU hardware, it it
best to use them on this machine (indeed, we see in Section VI
that the GPU cluster does not saturate FC). Another technique
is to use multiple machines for the FC phase, for example
using model parallelism across multiple machines such that
each machine stores a portion of the FC model (this still has
a staleness of 0 for the FC phase).

Another solution is to remove this physical mapping, i.e.
rather than have a single FC compute server mapped to the
FC model server, to have a separate FC compute server per
Conv compute server. This removes the bottleneck of a single
FC compute server, although it also sacriﬁces the beneﬁts
mentioned above. Section VI demonstrates the consequences
of this decision experimentally.

Physical Mapping Details: In addition to mapping the
FC compute and model servers to the same physical machine,
note that the conv model server does minimal computation and
can also be mapped physically to the same machine as the FC
compute/model server or one of the conv compute machines.
The primary concern with this server is network congestion,
so it makes more sense to map to one of the conv compute
server’s machines.

In addition to multiple servers physically mapping to a
single machine, it is also possible for a server to physically
map to many machines, for example using multiple machines
in a model server to implement a reduction tree as in FireCaffe,
or mapping 4 FC Compute servers to a machine that contains
4 GPUs, etc. Another example is merging the FC compute
and FC model server and mapping them to the same physical
hardware as in Figure 16 (b), but where that hardware is
not a single machine as shown in Figure 16 (b) but multiple
machines e.g., using model parallelism.

Finally, a common technique is to “pipeline” the servers by
mapping multiple conv compute servers to the same physical
machine. For instance in the synchronous case (1 group of N
machines), during the FC computation all N machines are idle
(because they are waiting for the FC to return data gradients
before they can begin the backwards pass of the conv phase).
During this idle time those machines can be processing a
different batch, i.e. N = 32, but there are two groups of
size 32. Note that in this example, this pipelining increases
staleness from 0 to 1. Using this pipelining, now the time per
iteration in conv saturation becomes:

Time per iterationsaturated conv = tconv(k)/g

i.e. the FC time is completely hidden.

Fig. 22. Variance of HE times. These are on a cluster of 9 CPU machines,
8 Conv compute groups, 1 machine per group

(left side of the graph), the slowdown and speedups are not
exactly linear, and we under-estimate the time per iteration.

While this analytic model may seem speciﬁc to CNNs, it
extends to any deep learning model because its derivation
relies only on queuing theory, not any speciﬁc properties of
CNNs.

2) Further Optimizations: A primary goal for understand-
ing the hardware efﬁciency above is to determine possible
optimizations.

Saturated Conv Server: We showed above that as the
group size (k) increases, there is no longer FC saturation,
because

tconv(k) + tf c > gtf c

and recall

tconv(k) = max(tconv,compute(k), tconv,network(k))

Speciﬁcally, in Figure 5 the case of a single group (left side
of the graph) is so much slower than FC saturation (right side
of the graph) because tconv,network becomes very large, i.e.
the time it takes to send the conv model to all 32 machines in
the group is signiﬁcantly greater than the computation time,
which is small due to data parallelism across 32 machines.

In particular, note that a single, large group is slower than
many small groups not because of synchronization delays
due to machine variance exists, but because of increased
network congestion, although some variance does exist in the
computation time across machines.

Microsoft’s Adam [5] discusses techniques to mitigate both
of these problems, from not requiring each worker in a group
to ﬁnish processing all of its images to adding multiple NIC
cards on the parameter server machines. FireCaffe [9] uses
the technique of reduction tress for their parameter servers to
reduce communication overhead, which allows them to reduce
this network congestion and scale to many machines in the
synchronous case using a larger batch size.

Saturated FC Server: The optimizations above improve
the hardware efﬁciency for the synchronous case, or generally
for small g and large k. On the right side of Figure 5
(small k, large g), the FC server becomes saturated and no
further speedups are possible. Recall that this is because the

We see that as long as η∗ increases with the batch size,
there is little penalty for larger batch sizes. This is because
larger batch sizes provide a truer gradient each iteration and
permit a larger η before divergence, therefore while a larger
batch consumes more of the dataset, the progress made by
each step is greater. η∗ cannot scale inﬁnitely however, and
plateaus beyond η∗ = 0.0032. As a result, larger batch sizes
make no more progress per iteration than smaller batch sizes,
but consume much more of the dataset each iteration. This is
catastrophic for performance (it can take 30× more epochs
to converge) as computation is effectively “wasted”, which is
why neural networks have been trained with SGD rather than
batch gradient descent since the early days. This also greatly
exceeds the staleness cost incurred of “splitting” a large batch
into smaller, asynchronous batches (which we show is nearly
ﬂat), which is why asynchrony is necessary for systems to
scale to very large clusters.

This suggests that the optimizer needs to tune batch size,
however for imagenet-8 and other datasets we observe that this
performance penalty is negligible around 256 (speciﬁcally we
use 256 for Imagenet, 128 for CIFAR-10 and 64 for MNIST,
based on published results for these datasets). In principle the
optimizer could tune b as well, but we observe that unless b is
too large the penalty is small so we don’t study this in more
detail.

B. Physical Mapping

As discussed in the text, for the physical map which maps
servers to machines, we map the FC compute and model
servers to the same machine (i.e. “merge” the FC servers,
which as [5] argues reduces communication overhead because
it avoids communicating the large FC model) and use one
compute group for the FC phase. The rest of the machines are
used for the conv compute servers. The conv model servers
are mapped to one of the conv compute machines.

Empirically we show in Appendix F-C4 that this mapping
is best for both hardware and statistical efﬁciency: on a
cluster of 33 EC2 c4.4xlarge machines, not merging the FC
servers incurs an additional hardware efﬁciency penalty of
1.2× due to increased communication as well as a statistical
efﬁciency penalty of 2.5× because of staleness in the FC
model. Our current optimizer therefore always chooses this
server architecture, although Appendix D-D (for Section IV-B)
described other scenarios in which these penalties are justiﬁed
to eliminate FC saturation, as well as additional optimizations
within this server architecture (such as reduction trees or multi-
machine model parallelism for the FC phase).

C. Optimizer Details

Fig. 23. Using a batch size that is too large greatly increases the # epochs to
converge (Imagenet 8-class). This slowdown begins when the optimal learning
rate no longer scales with the batch size.

E. Statistical Efﬁciency Model

Because asynchrony can be viewed as increasing implicit
momentum, asynchrony can be made equivalent
to syn-
chronous execution by properly reducing the explicit momen-
tum in order for the total explicit + implicit momentum to
stay the same as the optimal momentum of the synchronous
case. This is true as long as the implicit momentum is less
than the optimal momentum of the synchronous case. This
is a key discovery because it means that staleness can exist
in the system without incurring a statistical penalty, which
is advantageous for hardware efﬁciency. Also, making the
momentum stay the same (rather than just ignoring this result
and letting there be extra momentum) is important because a
total momentum that is too high will diverge, which we show
experimentally in Appendix E. This theory also successfully
predicts measured system behavior: Figure 6 shows the pre-
dicted and actual measured momentum for several popular
deep learning problems. In both cases, upon reducing the
explicit momentum to compensate for implicit momentum,
we observe no SE penalty. Moreover, the momentum increase
due to asynchrony closely matches the theoretical curve for
both datasets. Our study is the ﬁrst to identify a relationship
between hyper-parameters and asynchrony, and next
these
results are used to design the optimizer in Section V.

APPENDIX E
APPENDIX FOR DISTRIBUTED OPTIMIZER (SECTION V)

This section describes how to use the models from the
previous two sections to choose (1) a physical mapping which
maps each server to a machine, and (2) an execution strategy
which deﬁnes the number of compute groups by allocating
data batches to each server. As in previous sections we assume
that the number of machines are ﬁxed.

A. Selecting the Batch Size

We ﬁrst study the batch size in Figure 23, which uses the
imagenet-8 dataset with S = 0 and momentum µ = 0.9. The
x axis varies the batch size and the y axis plots the # passes
over the dataset (or epochs) until convergence. For each batch
size we used an oracle to ﬁnd the optimal learning rate, η∗.
η > η∗ diverged.

For each epoch, Algorithm 1 performs an adaptive grid
search over both the learning rate and the momentum starting
at the current value of g. Speciﬁcally, we run each learning
rate and momentum (see below) for one minute and select the
conﬁguration with lowest loss after 1 minute of execution. If
after 1 minute all these conﬁgurations have the same loss, we
continue to run another minute until there is a clear winner

1.8× more iterations (SE) to reach (cid:96)F in (b). This matches
the theory’s prediction: increasing g decreases the explicit
momentum µ∗, which falls to 0 at g = 32 (see Figure 6 (right))
, and consequently there is a penalty in SE. The optimizer of
Algorithm 1 would therefore select g = 16, which is near-
optimal.

However, our optimizer additionally employs an optimiza-
tion to leverage the HE model from Section IV-B: because
the FC server saturates at g = 4 (determined analytically
or through measurements during the cold start phase), the
optimizer will “short-circuit” Algorithm 1 to begin at g = 4
instead of g = 32, and ends up selecting g = 4, which is 5.3×
faster than sync.

Figure 24 shows the accuracy vs. time (and for reference
accuracy vs. iter, i.e. statistical efﬁciency) for each conﬁgura-
tion (# groups) in Figure 7. Recall that the optimizer selected
4 groups because with proper momentum tuning the statistical
efﬁciency was nearly the same for all curves, but 4 or more
groups had the best hardware efﬁciency. Figures 7 and 24 use
ImageNet 1000 class (1 hour of training) as described above
with 33 EC2 c4.4xlarge machines.

D. Cold Start Phase

The model is trained synchronously before beginning asyn-
chronous execution in Algorithm 1. This is needed in order
to set the appropriate scale for the weights. However fully
synchronous execution may be slow, and just as an optimiza-
tion to Algorithm 1 was to run asynchronously only up to
FC saturation, similarly this section focuses on accelerating
training during the cold-start phase. In particular, a fully
synchronous execution may signiﬁcantly increase the duration
of the cold-start phase due to poor hardware efﬁciency, and
so a cold-start run with slight asynchrony may more quickly
terminate the cold start phase. Therefore, tuning the number
of compute groups is also important for the cold-start phase.
Cold Start Grid Search: To do this, as in Algorithm 1,
we grid search hyper-parameters for each number of groups
(1, 2, 4, . . . , N , for N machines). For each we also grid search
learning rate and momentum. We use a standard adaptive
grid search algorithm for its simplicity. For each staleness,
the algorithm searches for optimal settings of momentum and
learning rate by running each conﬁguration of parameters for
1 minute, and selecting the parameters with the lowest loss
after 1 minute.

The search happens as follows, and is similar to the search in
the steady-state execution of Algorithm 1. We start with S = 0,
ﬁx the momentum to 0.9, and run 1 minute for each learning
rate η ∈ {0.1, 0.01, 0.001, 0.0001, 0.00001}. We search from
lowest to highest and stop early if a learning rate produces a
ﬁnal loss worse than the previous learning rate (or if a learning
rate causes divergence). We select the learning rate which has
the lowest loss after 1 minute. Call this η∗
sync. Therefore for
S = 0, the optimal conﬁguration (µ∗, η∗) = (0.9, η∗
sync). We
do not tune momentum for sync because 0.9 is standard [2],
because this saves optimizer time, and because there is no
implicitly induced momentum due to asynchrony for S = 0.

Fig. 24. Accuracy vs. iteration and Accuracy vs. Time for each execution
strategy shown in Figure 7

in terms of loss (we determine this using a threshold of 5%
from the loss of the past 50 iterations, although a statistical
test can be used as well). We then run this best (µ∗, η∗) for
an hour and then rerun the optimizer.

One could use more sophisticated parameter search routines,

but this took less than 10% of the execution time.

We search the learning rate as follows. Let the learning rate
and momentum used in the previous 1 hour epoch (i.e. the
result of the grid search from that epoch) be (µ∗
last).
For the current epoch, we then search η ∈ {η∗
last/10},
and µ ∈ {0.0, 0.3, 0.6, 0.9}. As an optimization to prune the
search space, we do not search µ > µ∗
last, as
we notice empirically that as the run progresses, the optimal
total momentum decreases.

last when η = η∗

last, η∗

last, η∗

If the optimal µ∗ = 0.0, we try µ∗ = 0.1 and µ∗ = 0.2 as
well, and if the lowest loss is still achieved at µ∗ = 0.0, we
decrease g and repeat the search.

(µ∗, η∗) in the initial (cold-start) phase are selected using a

similar procedure, described in Appendix E-D.

1) Empirical Validation: We now justify the theoretical
results of Section IV-C experimentally. We use a cluster of 33
EC2 CPU machines (CPU-L in Figure 9), the Imagenet 1000-
class dataset, and the AlexNet CNN. We run each execution
strategy from sync (g = 1 conv compute group) to async
(g = 32), each for a single epoch (1 hour). Speciﬁcally, we
plot 6 strategies: g = {1, 2, 4, 8, 16, 32}, where the number of
conv compute machines is ﬁxed to 32.

Each strategy starts from the same checkpoint (i.e. same
loss), but achieves a different ﬁnal loss by the end of the
epoch. We select the lowest ﬁnal training loss achieved by
all strategies, (cid:96)F , and plot three measures in Figure 7: (a) the
time per iteration (hardware efﬁciency, HE), (b) the number of
iterations to reach (cid:96)F (statistical efﬁciency, SE), and (c) their
product, which is the total time to reach (cid:96)F . For completeness,
each strategy uses an oracle to exhaustively ﬁnd its optimal
explicit momentum, µ∗ (within 0.3) and optimal learning rate
η∗ (within an order of magnitude. For all strategies η∗ = 0.01
was optimal). The HE curve in (a) is the same as in Figure 5
(b).

We see in (c) that g = 32 (fully asynchronous) is 3.7×
faster to reach (cid:96)F than g = 1 (synchronous). This is due
to its faster iteration time (HE) in (a), although it requires

For the remaining S after S = 0, we perform the following
iteration: We increase the number of groups to the next highest
power of 2 (after sync, this is g = 2, then g = 4, g = 8, etc, i.e.
S = 1, 3, 7, . . .) Let the optimal conﬁguration from the previ-
ous S be (µ∗
last). For the current S, we run a grid search
for (µ, η)|µ ∈ {0.0, 0.3, 0.6, 0.9}, η ∈ {η∗
last/10}. I.e.,
η∗ deﬁnes the search range for the next S. In addition, µ∗
reduces the search space for the next S: we do not search a
higher momentum than µ∗

last while searching η = η∗

last, η∗

last, η∗

last.

We notice empirically that

there is not a large impact
of running a ﬁner grid for momentum (although this can
be done by adding a second-phase of search which ﬁxes
η∗ and searches µ around µ∗). Running multiple random
seeds (network weight initializations) can also be used to
ﬁnd a good starting point for the SGD algorithm (this is a
known technique). Tuning parameters is not a novel idea in
machine learning, but unlike existing work our problem is
more sophisticated as we are coupling tuning hyper-parameters
and execution strategies (staleness). Our work is the ﬁrst to
show that hyper-parameters and execution strategies need to
be tuned jointly to avoid divergence as staleness increases.

Once we obtain (µ∗, η∗) for each S, we then run each S
for one minute at a time until there is a clear winner in terms
of loss (we determine this using a threshold of 5% from the
loss of the past 50 iterations, although a statistical test can be
used as well). We then run this best S with its (µ∗, η∗) for an
hour (the cold-start period).

Parameter Search Experiments: The remainder of this
section describes experiments motivating the pruning above,
in particular why a larger staleness does not need to try larger
learning rates or larger momentum values at the same learning
rate. There are a number of insights which allow us to prune
the search space for the cold-start phase and reduce the total
search time. We discovered that as staleness increases, the
optimal learning rate and momentum parameters when S = 0
cause divergence (loss goes to inﬁnity) for larger staleness
values, e.g., S = 31. This makes sense given our theoretical
foundation from the steady-state optimizer: staleness induces
implicit momentum, hence if explicit momentum is not de-
creased as S increases, total momentum can be > 1 and cause
divergence. As S increases, we note that one or both of η and
µ need to be reduced otherwise SGD will diverge (loss goes
to inﬁnity).

Table III shows the optimal parameters for the same datasets
and networks used in Figure 20, at different staleness values.
Here the optimal parameter settings are deﬁned as the param-
eter settings with which the training converges in the fewest
number of iterations. We say that a model has converged once
the training accuracy reaches 99.5%. Recall that a staleness
value of S corresponds to N = S + 1 parallel groups updating
the model asynchronously. In these experiments, the staleness
of the fully-connected models was zero, and so only the
conv model had staleness. Note that for imagenet 8-class
there are only 10400 examples and batch size is 256 so 128
parallel workers was not possible (there is insufﬁcient data).
Note that these small datasets all converge in under an hour

TABLE III
(COLD START) OPTIMAL MOMENTUM AND LEARNING RATE DURING THE
COLD START FOR VARIOUS DATASETS/NETWORKS AND VARIOUS
AMOUNTS OF STALENESS IN THE CONV MODEL

Dataset
(Network)

Staleness

Optimal
Momentum

MNIST
(LeNet)

CIFAR-10
(Krizhevsky)

ImageNet-8
(CaffeNet)

0
31
127
0
31
127
0
31

0.6
0.0
0.8
0.9
0.7
0.1
0.6
0.0

Optimal
Learning
Rate
0.1
0.1
0.01
0.001
0.0001
0.0001
0.01
0.01

and therefore consist entirely of the cold-start phase in our
implementation. While the cold-start phase of these datasets
is less than an hour (e.g., MNIST converges in seconds), and
therefore Algorithm 1 could be run part-way during execution
to select a better strategy for the remainder of the execution
(e.g., asynchronous), the overhead of re-running Algorithm 1
is not justiﬁed for these small datasets, i.e. it is faster to treat
the entire run as the cold-start phase. Therefore we use these
smaller datasets to study the cold-start phase.

The table shows that, with a ﬁxed batch size, as staleness in-
creases the optimal momentum and/or learning rate decreases,
and in some cases not decreasing these parameters and reusing
the parameters for S = 0 causes divergence. Also, we see that
decreasing the learning rate means momentum can increase
again. Intuitively this is because momentum can be viewed as
increasing the learning rate (larger SGD steps), and so if the
learning rate is decreased too much, momentum increases to
compensate for this decrease. Our grid search searches orders
of magnitude for the learning rate, following from previous
work [2], but decreasing the learning rate by a smaller factor
may avoid the need for momentum to increase and provide
faster overall convergence. We leave this exploration to future
work.

APPENDIX F
APPENDIX FOR DISTRIBUTED EXPERIMENTS (SECTION VI)
A. Single-Node Experiments

See Appendix C-E.

B. Small Cluster Experiments

This section provides additional details of the experimental
setup. For the end-to-end experiment on ImageNet 1000, see
Appendix F-E.

We ran all systems to 99% accuracy (which we deﬁne as
convergence) with a timeout of 2 hours. For each system we
used the same CPU and GPU external libraries as discussed
in Appendix C-E.

We further sped up other tools by applying our optimizer
to the extent that no code change was required. MXNet offers
both the sync and async strategy. Given our observation that
async requires tuning parameters, to ensure training did not
diverge we ran each strategy of MXNet with 4 orders of

magnitude of the learning rate for 10 minutes each. We then
selected the best strategy (as the static optimizer would) and
ran it until convergence or timeout. For SINGA we followed
the same procedure and tried all available conﬁgurations.
SINGA supports not only sync (1 group) and async (1 machine
per group) strategies, but also intermediate group sizes. We ran
SINGA with 1, 2, 4, and 8 machines per group, and also 4
orders of magnitude for the learning rate η in each case. All
runs were also for 10 minutes, and then as with MXNet the
best one was run to convergence.

For Omnivore we ran our optimizer, which merged the FC
compute and model servers to one machine and used the other
8 machines as conv compute machines. As with SINGA, the
optimizer searched statically among 1, 2, 4 and 8 machines
per group, but for 1 minute per execution setting. Overall the
optimizer ran for less time than the tuning we did to ensure
no divergence for MXNet and SINGA.

We followed the tutorials for each system and also ensured
that all three systems used identical networks and parameters,
including weight initializations and data preprocessing.

The network we use is CaffeNet, which is Caffe’s version
of Alexnet20. AlexNet is the standard network for ImageNet
and Caffe is the most widely used CNN framework, so this
ensures reproducibility. The weight initializations, batch size,
regularization, and other hyperparameters are the same as
CaffeNet, with a few minor differences:

Unlike Caffe and SINGA, MXNet does not easily support
learning rate and weight decay multipliers, or different ini-
tializations for each model and bias. For consistency across
tools, we therefore just made all 3 tools use the same weight
initialization scheme, which is Gaussian with mean 0 and
standard deviation 0.01, and no multipliers.

MXNet and SINGA do not support the grouping in AlexNet
(which was done in 2012 to save GPU memory), so this is
disabled from CaffeNet (and also is not important anymore as
GPUs have more memory)

No random data preprocessing was used (crop, mirror),
and we show convergence on the training set, not a test or
validation set. We do this because these are machine learning
concerns/optimizations and our focus is the system. We do
subtract the image mean to avoid divergence.

Similarly, we disable the learning rate schedule in all tools
and use a constant learning rate. We do this because we only
train on a subset of ImageNet and to reduce the search space
of the parameter conﬁgurations.

The following subsections describe in detail the individual
settings used for each system to ensure fairness in our com-
parison.

1) Detailed Settings for Both Systems: For both systems
we built in a wall-clock timer to ensure accurate timing. We
created and shufﬂed the dataset using the tools provided by
the systems: MXNet required shufﬂing beforehand, SINGA
provided an im2rec utility. Those tools also were used to
calculate the image mean: MXNet automatically generated the

20https://github.com/BVLC/caffe/blob/master/models/bvlc reference caffenet

mean ﬁle when it ran and SINGA as part of im2rec. Because
we focus on the training set we removed any validation set
from the tools to ensure no time was spent on that. We used
the provided AlexNet examples for each system and changed
them only as above to ensure identical settings across all three
systems (e.g., weight initializations, L2 regularization, etc.)
Accuracy was reported instead of loss for all tools to ensure
consistency. The MXNet examples do not report loss so we
used their accuracy eval_metric. Moreover MXNet’s acc
metric is by default on the entire epoch while SINGA’s is
since last time printing, so we averaged the logs to ensure
consistency across all three tools.

2) Detailed MXNET Settings and Results: We removed all
machine learning optimizations from both tools except those
described above. For MXNet this meant removing gradient
clipping. Because we ensure the parameters are used for all
tools, including the batch size (256 for CaffeNet), this meant
that for MXNet’s dist_sync strategy on 8 machines, a batch
size of 32 was used, and for 32 machines, a batch size of 8
was used (other tools partition the batch size for the sync
strategy, i.e. they partition b images by sending b/N to each
sync worker, but MXNet uses that batch per worker, i.e. they
use a batch size of b × N ). We ﬁxed the random seed to 1 so
the initialization is always the same.

We created a single data ﬁle and ensured that each worker
read it from different location (using ImageRecordIter as
in the AlexNet example). The timer was added as modiﬁed ver-
sion of the speedometer callback (using time.time(),
which is wall-clock time in python).

We used a cluster of 9 machines in these experiments
because MXNet’s AWS documentation instructs to “Prepare a
host ﬁle with all slaves’s private IPs”. Therefore in order to test
parallelism across 8 machines, we opened 9 EC2 machines,
ran MXNet from the master (root) machine, and placed the
other 8 machines in the hosts ﬁle.

On the cluster of 9 c4.4xlarge machines we ran the 4 orders
of magnitude learning rate for each execution strategy and
noticed after 10 minutes that the best was learning rate 0.01
and sync, so we ran until 99% convergence. We needed 4
orders of magnitude to ensure that the optimal setting was
never on the “boundary” of the interval, i.e. the optimal η we
report was superior to an order of magnitude higher η and
lower η. For async no parameter setting had high accuracy
after 10 minutes: The best sync was 60% in 10 min and the
best async got to 20% in 10 min, in spite of better hardware
efﬁciency for async (72 s per epoch async, 120 s per epoch
sync). The best async was with learning rate 0.0001.

On the cluster of 9 g2.8xlarge machines (again following
MXNet’s documentation, 1 parameter server machine and 8
compute machines), we tried both cuDNN v3 and v4 and
found no speed difference. Again we searched learning rate
and found that 0.01, sync was best.

On the c4.4xlargs machines we ensured each worker was
using all the cores, and on the g2.8xlarge machines that all 4
GPUs on each worker were utilized (using nvidia-smi).

3) Detailed SINGA Settings and Results: For SINGA, the
timer built into TrainOneBatch. It also uses wall-clock
time (clock_gettime, same as Omnivore). Again we use
the default AlexNet example with only small changes to make
all weight initializations the same (as in MXNet), and to
remove learning rate / weight decay multipliers (since not
supported easily in MXNet).

Tuning parallelism within groups: we tried tuning

partition_dim for each layer. Speciﬁcally we ﬁrst used
the default, i.e. partition_dim commented out (as in their
AlexNet example). We then uncommented those recommended
partition_dim settings in the example (i.e. dim 0 or batch
parallelism for convolution, and dim 1 or model parallelism
for FC) and found no difference, so we left
partition_dim commented out as in the default AlexNet
SINGA example.

Tuning parallelism across groups: To ensure different data
for each worker we tried specifying a random_skip in the
data section but this made no difference. Documentation v0.1.0
suggested using random_skip but in v0.2.0 (which we used)
it was deprecated, so as with partition_dim we left this
out and used the default AlexNet SINGA example settings.

Next, we had to select for each machine number of workers
per process. For each machine, we tried 1 process of 8, 16,
and 32 threads on a single machine. The number of physical
cores is 8 on the c4.4xlarge, and virtual cores is 16 (nproc
= 16). 16 was fastest so we used 16 workers per process.

As with MXNet we followed the documentation for SINGA
and also included 8 machines in the hostﬁle. We ran 4 orders of
magnitude for the learning rate as described above and found
that 0.0001 and 4 groups of 2 machines each was best after 10
minutes. The result was noisy however, and looked similar to
the distributed results in the “SINGA: Putting Deep Learning
in the Hands of Multimedia Users” paper. We then ran the
best conﬁguration for but it did not converge in 3 hours (got
to 70-80%).

SINGA GPU distributed did not work at the time of this

study so it is not included.

4) Detailed Omnivore Settings and Results: Omnivore was
run using the same network and parameters as the systems
above. The optimizer was run as described above in Ap-
pendix E. Each conﬁguration was searched by the optimizer
for 1 minute and search time was reduced by pruning the space
across staleness values. The second search phase was skipped
(momentum granularity was 0.3). We ﬁxed the random seed to
1 so the initialization is always the same. The overall optimizer
time was less than the search time to avoid divergence in other
tools.

On the CPU cluster, the strategy chosen was the same
as with MXNet,
i.e. sync with η = 0.01. Momentum is
untuned for both Omnivore and MXNet, i.e. 0.9, because our
contribution is tuning momentum to compensate for staleness
and sync has no staleness. Since the parameters and staleness
are the same as MXNet, as expected Omnivore achieves the
same statistical efﬁciency. However, it is 2.3× faster in terms
of hardware efﬁciency, for an overall end-to-end convergence

Fig. 25. Recall Figure 7 (replicated here for convenience and also showing
momentum values from Figure 6 (right))

speedup of 2.3×. On the GPU cluster, the optimizer chose
2 groups of 4 m/g, and was 5.0× faster to converge. The
following section studies the beneﬁts of the optimizer in more
detail, and also examines how the tradeoffs change on a large
cluster which has more options for execution strategies: for
example, the extreme strategies of sync or async may not be
sufﬁcient for a larger cluster. This may prevent MXNet, which
only supports these strategies, from scaling to a larger cluster.

C. Detailed Tradeoff Space Analysis

This section analyzes the speedups observed in the previ-
ous section to understand the contribution of each tradeoff
selection that the optimizer made. These tradeoffs include (1)
execution strategy (number of groups), (2) optimizing hyper-
parameters to compensate for staleness, and (3) physical plan
(server to machine allocation).

1) Penalty Deﬁnition: Consider again Figure 7, which
we’ve replicated here for convenience in Figure 25 and also
shown momentum (note that when the optimal explicit mo-
mentum is 0, there is an associated SE penalty). Recall this
ﬁgure showed the tradeoff for compute groups on the CPU-L
cluster for an epoch of ImageNet 1000. The HE and SE plots
were multiplied to produce the right-most plot of total time to
reach a ﬁnal loss.

The vertical axis of the SE ﬁgure in Figure 25 shows what
we call the SE penalty, PSE(S), which is deﬁned as the ratio
of the # iterations needed to converge relative to the case of
no staleness (S = 0),

PSE(S) =

SE(S)
SE(0)

(8)

PSE is dimensionless, because it is the ratio of statistical
efﬁciencies (#iter / #iter). The penalty is 1 when the staleness
is 0, and should be higher for all S > 0. A higher PSE is
worse (more iterations to converge).

Recall also that we deﬁned hardware efﬁciency penalty
(PHE). This is shown in the middle graph of Figure 25.
Since the statistical efﬁciency penalty is deﬁned as the ratio
of the # iterations to convergence with respect to S = 0,
for consistency PHE(S) is also normalized with respect to
S = 0. S = 0 is the case of 1 compute group, also called the
synchronous case. The hardware efﬁciency penalty is deﬁned

Fig. 26. Cold start vs Steady state for Imagenet 1000 on GPU-S

Fig. 27. Cold start vs Steady state for cifar on GPU-S

as the ratio of the time per iteration relative to the time per
iteration for this synchronous case,

PHE(S) =

HE(S)
HE(0)

(9)

As with PSE, a higher PHE is worse (more time per itera-
tion). Whereas PSE(S) increased with staleness, for hardware
efﬁciency this trend is reversed: PHE decreases (iterations
become faster) as the number of compute groups increases.

Note in these ﬁgures, the staleness is 0 for the FC model, so
staleness on the horizontal axis refers only to the conv models
(i.e. the number of conv compute groups).

Finally, recall that the product of hardware and statistical
efﬁciency is the total time to convergence. Since the horizontal
axis (staleness, i.e. # groups) is the same on both the SE
and HE plots, these plots can be multiplied, and the resulting
vertical axis is the total penalty, deﬁned as the ratio of the
total time to convergence (normalized to sync, i.e. S = 0):

PTotal(S) = PSE(S) · PHE(S) =

SE(S) · HE(S)
SE(0) · HE(0)

(10)

We use ﬁgures of this format throughout this section to

quantify the beneﬁt of the choice of compute groups.

2) End-to-End Imagenet 1000: Recall that Figure 25 was
run for a 1-hour optimizer epoch of Imagenet 1000, on the
CPU-L cluster, as discussed in Appendix E-C. Figure 26
(bottom 2 sets of ﬁgures) shows the same experiment on the
GPU-S cluster. Notice again that SE is ﬂat, i.e. maximum
asynchrony is optimal.

Fig. 28.
c4.4xlarge CPU machines.

Imagenet 8-class and CIFAR-10 tradeoff on a cluster of 9 EC2

These ﬁgures show steady-state execution, hence the SE
curves show no penalty (nearly ﬂat). The same curves but for
the cold-start epoch of the GPU-S cluster are shown in the top
of Figure 26.

We also validate this for the small cifar dataset in Figure 27.
Here for exposition we reduced the epoch size to 2 minutes
(otherwise the cold start would converge after only a few
minutes).

3) Small Clusters: The optimizer’s choice of execution
strategy for the small cluster experiments (Figure 12(a) and
(b)) is shown in Figure 28 (CPU-S) and Figure 29 (GPU-S).
In addition to the imagenet-8 dataset we also include CIFAR-

Fig. 29.
machines (36 GPUs total).

Imagenet 8-class tradeoff on a cluster of 9 EC2 g2.8xlarge GPU

Fig. 30. Hardware efﬁciency, statistical efﬁciency, and total time tradeoff
curve (ImageNet 8-class, 33 machines)

every example 21.

10 to show the optimizer is robust across datasets. We see
that for these small clusters, choosing the execution strategy
incorrectly incurs a penalty of roughly 1.5×.

Note that Figure 28 and Figure 29 have the same statistical
efﬁciency curves but different hardware efﬁciency curves.
This is because the difference in throughput between the
GPU machines and CPU machines exceeds the difference in
network speed between these clusters so there is a higher
penalty for the sync case on the GPU cluster. Also, neither
of these 9 machine clusters reach FC saturation.

Next we consider the CPU-L cluster.
4) Large Cluster: The tradeoff for CPU-L (Figure 12(c))
is shown in Figure 30. 4 groups (8 machines per group) was
the optimal point, and that the optimizer chose this execution
strategy. The detailed tradeoff space for CPU-L is analyzed
in Figure 31. Each curve, from the bottom up, represents a
selection made by the optimizer. We’ve isolated each selection
to observe their relative impact.

Avoiding Divergence: First, consider the red line, which
represents the default point chosen by many systems: asyn-
chronous with a large number of machines. Indeed, statistical
efﬁciency is often ignored by other systems and so by default,
the conﬁguration with the best hardware efﬁciency (fastest it-
eration time) is erroneously selected. However if the published
AlexNet hyper-parameters [2] (which are optimal for the sync
case) are naively used in the async case, there is divergence.
Thus, our tuning approach is critical.

The green curve shows that if only the learning rate η is
tuned, divergence can be avoided. Tuning η is also common
practice, although prior work does not do so explicitly to
compensate for staleness as we advocate. In the green curve
momentum is not been tuned, as many systems always use
a momentum of 0.9 (as mentioned in [2]). For example, at
the time of this study MXNet hard-codes this momentum into

Fig. 31. Tradeoff curve showing the impact of each dimension of the
optimizer.

Also, the green curve does not merge the FC compute
and model servers by physically mapping them to the same
machine. Instead, this curve represents the architecture shown
in Figure 16 (a), i.e. there is an FC compute server for each
CONV compute server, and each of these server pairs is
mapped to a separate machine. Of the 33 machines therefore,
one machine contains the CONV and FC model servers,
while each of the other 32 contains a CONV compute and
FC compute server. This conﬁguration represents the strategy
chosen by MXNet, so we report their async curve as the green
line because their system is optimized for this case (note that
doing this disadvantages our ﬁnal speedup ﬁgure, i.e. if we
had used Omnivore’s implementation of this tradeoff point our
optimizer’s speedup would be > 20×). The remaining curves
have their hardware and statistical efﬁciencies normalized to
those of this green curve.

Device Mapping: We now examine our choice of merging
the FC compute and model servers to the same machine (the
33rd machine), as Section V described. The other systems
do not support this merging so we take “unmerged” as the
baseline, and use the 33rd machine for the conv and FC model
servers as MXNet and SINGA’s documentation suggests. The
remaining curves will have their hardware and statistical
efﬁciencies normalized to those of this curve.

The turquoise curve merges the FC servers. We see that
this gives a 1.18× improvement to hardware efﬁciency (due
to reduced communication) and a 2.55× improvement
to
statistical efﬁciency (due to no staleness in the FC model).
Overall, this is 3.01× faster to converge than the baseline.

In the turquoise curve, note that the hardware efﬁciency
improvement is only 1.18× for the merged FC. As discussed in
Section IV-B, this is because while communication is reduced
by merging these servers, on the large CPU cluster the FC
is reached at 4 groups, and not mapping
saturation point

21https://github.com/dmlc/mxnet/blob/52ea0f0cbbf5eaaf38a2341e57afd6829f88a86d/

example/image-classiﬁcation/train model.py#L77

the FC servers to the same physical machine as described
above eliminates the saturation (but requires more network
communication and incurs a statistical efﬁciency penalty).

Parameter Tuning for Staleness: Divergence can always
be avoided by tuning η alone, and indeed most systems
always use a momentum of 0.9 (see comment above) which is
standard for the sync case [2]. However, we show in the purple
curve that at larger staleness values, additionally tuning the
momentum µ permits using a higher η. This does not change
hardware efﬁciency, but now gives an overall 5.85× speedup
over the baseline due to improved statistical efﬁciency.

Execution Strategy: Finally, the blue line represents the
actual choice made by the optimizer. In addition to the selec-
tions above, recall that the optimizer did not choose 32 groups
but 4 groups (Figure 30), which further improves the statistical
efﬁciency to give an overall speedup of > 20× compared to
standard choices traditionally made by deep learning systems.
Note that changing the number of groups to 4 did not hurt
hardware efﬁciency because the FC server is already saturated
(see Section IV-B).

D. Scalability to a Larger Cluster

The previous section (Appendix F-C) showed that as a result
of the optimizer’s tradeoffs Omnivore is able to scale to 32
machines. In this ﬁnal experiment we compare Omnivore to
the best competitor from the small clusters, MXNet, on this
larger cluster. We use the same dataset and network as the
small cluster experiments, and deﬁne convergence the same
way (99% accuracy).

We attemped to open a cluster of 33 g2.8xlarge instances
but continuously ran into EC2 errors related to not enough
machines available (InsufﬁcientInstanceCapacity).

As in Section VI-B, we repeat the same procedure to apply
our optimizer to MXNet, i.e. we run each conﬁguration for
10 minutes, select the best execution strategy and learning
rate, and run that to convergence. The best strategy was once
again sync with η = 0.01. Speciﬁcally, as in the previous
experiments, we followed MXNet’s documentation and used
the EC2 master machine as the root, and put the other 32
workers in the hostﬁle. We ran MXNet for 10 minutes with
both sync/async and 4 orders of magnitude learning rate as
described above. This time all 4 orders of magnitude for η
were needed because for sync with 32 machines, after 10
minutes 0.001 and 0.01 were almost the same, although 0.001
was better by ∼ 5% points. Since this differed from the
optimal η from the 8 machine case, which was 0.01 (i.e.
statistical efﬁciency changed for the larger cluster), to be sure
we ran MXNet with each η to convergence and noticed that
in fact 0.01 was signiﬁcantly faster to converge in the end, as
was true on the smaller clusters. Similarly, for async after 10
minutes the best η was 0.00001, although this was close to
0.0001, therefore once again we ran both to convergence and
noticed that 0.0001 was faster to converge for MXNet. We did
not do this extended parameter tuning for Omnivore, and only
ran the 1 minute static runs described above, to make sure that

we were getting the best possible performance from MXNet
for our comparison.

For Omnivore the optimizer was used as described in the
previous section. The best result of MXNet and Omnivore is
shown in Figure 12(c). We see that Omnivore is 3.2× faster
than MXNet to converge now (it was 2.3× faster on the 9 CPU
cluster). In addition, we see that compared to Figure 12(a),
Omnivore sped up on the larger cluster but MXNet did not.
Therefore not only does the optimizer give speedups by not
relying solely on the sync strategy and by merging the FC
servers, but also enables scalability to more machines.

If we do not apply our optimizer to MXNet, Omnivore now
converges 20× faster. This is compared to MXNet’s async
strategy, which has poor statistical efﬁciency for 32 machines.
This 20× corresponds exactly to the speedup in the previous
section because the green curve in that section is MXNet
using the async strategy (i.e. they are the same point in the
tradeoff, see the discussion in Appendix F-C4) By applying
our optimizer to MXNet we select the sync strategy instead,
which lowers the gap with Omnivore to 3× on this cluster.
Therefore this section shows not only that the optimizer gives
speedups of more than an order of magnitude, but that it is
versatile and can be applied to existing tools.

E. End-To-End Experiments

The end-to-end result

is in Figure 10. We trained the
standard CaffeNet (same setup as in Appendix F-B) using
ImageNet-1000 on both systems using both the CPU-L and
GPU-S clusters. We time out each run after 8 hours and report
the training accuracy vs. time.

According to MXNet’s ofﬁcial performance tuning guide-
line 22, they recommend trying the sync strategy, but also
state that “if the model size is quite large or you use a
large number of machines, you may want to use dist async”.
Immediately above, they describe large as “models with size
>> 100MB such as AlexNet and VGG.” Because we are
training AlexNet and use up to 33 machines, which may be
considered large, then according to these instructions async
could be the best choice. Because they do not provide an
automatic mechanism to make this decision we followed this
advice and tried both strategies, as we did above in section
F.2. This required tuning the learning rate for each strategy.
We used the optimal learning rate obtained for each strategy
on ImageNet-8, as recommended by [29] which states that “the
best way to determine the correct learning rates is to perform
experiments using a small but representative sample of the
training set”. In addition, MXNet does not provide a learning
rate schedule for their AlexNet example (as of the writing
of this study) so we use the standard learning rate schedule
of [2] which decreased the learning rate by 10× when training
plateaued.

For Omnivore, we ran the optimizer end-to-end. We ensure
a 10% overhead by running the optimizer and then training for
10× the optimizer time before rerunning the optimizer. Each

22https://github.com/dmlc/mxnet/tree/db6f6a9418e5696b04be741a78a47ae877bb5505/

example/image-classiﬁcation

TABLE V
GRID SEARCH PARAMETERS FROM FIGURE 10 (B) ON CPU-L

Fig. 32. Recurrent Neural Network using 9 EC2 c4.4xlarge CPU machines.

TABLE IV
GRID SEARCH PARAMETERS FROM FIGURE 10 (A) ON GPU-S

Phase
cold
phase 1
phase 2

µ
0.6
0.6
0.6

η
0.01
0.001
0.001

Phase
cold
phase 1
phase 2

µ
0.6
0.6
0.3

η
0.01
0.001
0.001

g
4
8
8

g
2
4
4

time the optimizer runs, it searches momentum, µ and learning
rate, η, either reducing one, reducing both, or keeping them
the same. The grid search results for each phase in Figure 10
(a) and (b) are shown in Table IV and Table V. On the GPU-S
cluster, the ﬁrst time the optimizer ran µ remained at 0.6 but η
decreased from 0.01 to 0.001. The second time, the parameters
did not change. On the CPU-L cluster, the ﬁrst optimizer run
made the same choices: µ also began at 0.6, and remained 0.6,
while η decreased from 0.01 to 0.001. Following the second
optimizer run, µ reduced from 0.6 to 0.3, and η remained
unchanged. Note that the cold-start (ﬁrst) phase of the CPU-
L run in Figure 10 shows a very large slope, and then after
the optimizer run this slope decreases. In ongoing work we
are exploring a slope-aware optimizer which would not rerun
the optimizer at this point but continue the execution. With
this change we expect the gap against competitor systems to
increase signiﬁcantly.

F. Preliminary RNN/LSTM Result

To understand if our tradeoff applies more broadly, we
implemented the Recurrent Neural Network model and LSTM
proposed by Graves [30]. Following the same protocol as
Figure 28 and using the CPU-S cluster, we see in Figure 32
that the tradeoff between statistical efﬁciency and hardware
efﬁciency is comparable, and choosing a completely syn-
chronous or asynchronous conﬁguration can be up to 2×
slower than the optimal conﬁguration. 23

G. Comparison to Standard Schedules

We have shown that tuning is critical for good performance.
We next validate the hypothesis that Omnivore’s optimizer out-
performs standard tuning and parameter scheduling methods.
To validate this, we run Omnivore on the full ImageNet using
the standard CaffeNet. We run two versions of Omnivore: (1)
Omnivore (Default Schedule), which uses CaffeNet’s default
learning rate schedule that decreases the learning rate by 10×
every 100,000 iterations; and (2) Omnivore, which uses the
standard Omnivore optimizer. To be fair, both versions use the
same grid search strategies to select the optimal learning rate,
momentum, and number of compute groups at the beginning.

23 We also see a similar tradeoff in the LSTM variant proposed by Graves [30].

Fig. 33. Comparison of Omnivore’s optimizer to CaffeNet’s default learning
rate schedule on Full ImageNet with AlexNet.

In addition, we run Omnivore for 10× the optimizer time
before it re-optimizes the parameters.

Figure 33 shows the training loss vs. wall-clock time. The
two plateaus shown in Omnivore correspond to the times
Omnivore re-optimizes the parameters. The losses of both
Omnivore (Default Schedule) and Omnivore decrease over
time. However, after the ﬁrst parameter re-tuning, Omnivore’s
loss starts to decrease more rapidly. Finishing at 36K seconds,
Omnivore is 1.5× faster to achieve the same loss as Omni-
vore (Default Schedule). Omnivore does not require the user
to specify the number of iterations to run before re-optimize
the parameters.

H. Comparison to Bayesian Optimizer

We compare our simple optimizer with the state-of-the-art
Bayesian optimization approach that explores the parameter
space of CNNs. The results are shown in Figure 34. We follow
Snoek et al. [18] to model the search space as (η, µ, S, N )
where N is the number of epochs to run. We use the same
search space for (η, µ, S) as in our optimizer and measure
both the number of conﬁgurations and the total number of
epochs that the Bayesian optimizer needs to run before ﬁnding
a run that achieves an accuracy within 1% of Omnivore’s best
accuracy.

Our procedure is as follows. We ﬁrst run Omnivore to obtain
a run which reaches 99% convergence using the same dataset
and cluster as in Figure 12 (b). This took 80 epochs and 680
seconds. We then give the Bayesian optimizer N = 80 and
it tries to ﬁt η, µ and S in order to reach the lowest loss
(highest accuracy) by a timeout of 1000 seconds. It searches
N in the range 1, . . . , 80, S in the range 1, 2, 4, 8, µ in
the range 0.0, 0.3, 0.6, 0.9, and learning rates in the range

Fig. 35. Hardware and statistical penalty without tuning (momentum 0.9)

Fig. 34. Bayesian optimizer run on Imagenet-8 using the GPU-S cluster.

0.1, 0.01, 0.001, 0.0001, 0.00001, i.e. the same as Omnivore
searches.

It took the Bayesian optimizer on average 12 runs before
ﬁnding a strategy which achieves accuracy within 1% of
Omnivore’s run. On average this takes 6× more epochs
than just training that strategy to convergence, which makes
the Bayesian approach infeasible to run on Imagenet 1000
(whereas Omnivore’s optimizer incurred only a 10% over-
head).

Compared with our optimizer, one difference is that we
are using the ﬁrst minute’s execution as a proxy for a longer
run, while on the other hand, Snoek et al. have the number
of epochs to run as a parameter to explore and do not
share information across runs. It is of course possible to use
Bayesian optimization to guide our grid search for the ﬁrst
minute, however, it is future work to integrate this heuristic
into the Bayesian optimization framework in a principled way.

APPENDIX G
TENSORFLOW EXPERIMENTS

In this section we see that

tuning momentum can sig-
niﬁcantly improve the performance on platforms other than
Omnivore by running experiments on TensorFlow [22]. We use
TensorFlow r0.9 starting with the fully synchronous and fully
asynchronous implementations of Inception-v3 for ImageNet
[12] found in [32]. Synchronous training has each worker send
its gradients to the parameter server. The parameter server
aggregates all the gradients, applies them to update the weights
and then sends the new weights to all the workers. In the
asynchronous conﬁguration, each worker sends its gradients
to the parameter server. The gradients are immediately applied
to the weights and the new model is returned to the worker.
On 32 GPU workers, asynchronous training reaches the
same loss as its synchronous counterpart 1.5× faster, when
properly tuned. In contrast, when momentum is ﬁxed to 0.9
for both, synchronous training is faster. We also implement
compute groups on top of the code from [12] and report the
tradeoff ﬁgures for a 32 worker conﬁguration. We discuss the

Fig. 36. Hardware and statistical penalty with tuning

experimental setup and discuss results on momentum tuning
and compute groups.

A. Experimental Setup

We deployed 8 g2.8xlarge instances on AWS, each equipped
with 4 GPUs. We allocate 1 GPU per worker node, meaning
there are 4 workers per instance and a total of 32 worker
nodes. There is a single parameter server that runs on one of
the instances. Each worker uses a batch size of 32 images,
as suggested in [32].24 Notice that the suggested setup uses a
per-worker batch-size as opposed to the per-group batch-size
we used on Omnivore. We used the SGD with momentum
optimizer for Inception-v3 as opposed to RMSProp with
momentum used in [12].

We perform our experiments after a warm start: we train
the Inception-v3 network synchronously on ImageNet, until
it reaches 50% training accuracy and take a snapshot. This
snapshot is used as the starting point for all measured runs.
We run each set of parameter values for 1 hour. We grid seach
momentum values in {0.0, 0.3, 0.6, 0.9} and learning rates in
{0.005, 0.01, 0.05}. We experimented with other values and
found these to capture the range of optimal learning rates. We
measure the loss achieved by each conﬁguration after 1 hour
of execution.

B. Results

We measure the time and number of iterations it takes to
reach a target loss and then perform hardware and software
efﬁciency analysis. Figure 35 shows the normalized statistical
penalty, hardware penalty and wall clock time of training to
the target loss for both the synchronous and asynchronous
conﬁgurations, starting from the same snapshot. In this ﬁgure,

24 This is also the largest batch size that can ﬁt on a single GPU of the

g2.8xlarge instances.

same number of iterations. As Figure 11 showed, Omnivore’s
speed on the c4.4xlarge instance is 0.57× the speed of the
g2.2xlarge instance. This ratio closely matches the FLOPS
ratio 0.7/1.2. Therefore we observe that running on a CPU
instance is 2.1× more expensive than a GPU instance, due to
the difference in the FLOPS/dollar ratio for these instances.
This suggests that on cloud services such as Google Compute
which do not have GPU instances, CPU-based deep learning
is a viable and cost-effective option when using Omnivore.
Moreover, organizations that can amortize the cost of CPUs
in more ways than GPUs may ﬁnd them to be a cheaper
alternative.

Distributed: In the distributed setting we consider again
the case of 9 machines and compare Omnivore running
on the GPU cluster (g2.8xlarge, $2.60 per machine-hr, 4.8
TFLOPS per machine) and CPU cluster (c4.4xlarge, $0.838
per machine-hr, 0.7 TFLOPS per machine). The difference in
peak FLOPS between these clusters is 6.8×, and the speedup
to convergence obtained by Omnivore on the GPU cluster
compared to the CPU cluster is 5×–note it
is not quite
6.8× because network speed does not scale with the node
throughput. If we consider only hardware efﬁciency (since
statistical efﬁciency is unrelated to the underlying hardware),
the GPU cluster is 5.6× faster than the CPU cluster, which
is nearly the FLOPS ratio. As in the single-machine case
therefore it is only the FLOPS/dollar ratio which matters. The
GPU cluster is more cost-effective, now by a factor of 1.8×.
These results show that CPU deep learning is not sig-
niﬁcantly different from GPU in terms of consumer cost,
and it will be exciting to see how these trends change for
future CPUs which have increased SIMD parallelism as well
as newer GPUs which optimize for lower power. As SIMD
processor bandwidth has been doubling in each generation, it
seems that CPU training may indeed catch GPUs relatively
soon.

A. Distributed Calculation

The ratio of peak FLOPS of the GPU cluster / CPU cluster
is 4.9/0.74 = 6.6. Considering the optimal points chosen by
our optimizer, the CPU / GPU time to convergence is 5×.
If statistical efﬁciency is ignored, and we compare only the
speeds of the async cases on each cluster, the ratio is now
34s/iter for the CPU cluster and 6 s/iter for the GPU, or 5.6×,
which almost matches the ratio in device FLOPS. Given that
GPU cluster is $2.6/$0.838 = 3.1×more expensive, the GPU
cluster is 1.8× cheaper per iteration which matches closely
with the FLOPS/dollar ratio.

APPENDIX I
APPENDIX FOR CONCLUSIONS (SECTION VIII)

Our study ﬁrst demonstrated that on a single machine we
could achieve CPU speeds proportional to the device FLOPS,
showing end-to-end speedups of more than 5.5× on EC2 CPU
instances over state-of-the-art tools. With this improved CPU
speed we showed that CNN computations are compute-bound.
This allows the underlying hardware in a machine to be treated

Fig. 37. Hardware efﬁciency, statistical efﬁciency and wall clock time when
tuning momentum and learning rate.

the momentum parameter is set to the standard 0.9 value (the
value used in [12] is not reported in the paper, but available
code [32] suggests it was 0.9). The learning rate is tuned
using the grid described above. The statistical penalty is high
for asynchronous training, which results in longer wall clock
time to reach the same loss compared to synchronous training.
As expected, the hardware penalty is lower for asynchronous
training. Figure 36 shows the results of the same experiments,
but using a grid search to tune momentum and learning rate.
The statistical penalty for asynchronous training relative to
synchronous is now improved by a factor of about 2.4×.
Tuning in this case is result-changing. The asynchronous
conﬁguration reaches the same loss in less time compared to
synchronous training.

C. Compute Groups

We implemented compute groups in TensorFlow r0.9, used
the same experimental setup as before and report results on
Inception-v3. Our goal is to understand the tradeoffs on this
different platform. Figure 37 reports the performance curves
for this setup. As in some of our Omnivore experiments,
the statistical efﬁciency remains nearly ﬂat when tuning.
This allows us to take advantage of the better hardware
efﬁciency of asynchronous settings. We see that, in this case,
32 nodes are not enough for the limits of asynchrony to start
showing. We expect that given a larger number of nodes,
the optimal conﬁguration will not be fully asynchronous, but
rather some intermediate compute groups setting. This result,
though under a different setup, contradicts the—reported but
not demonstrated—claim that hybrid conﬁgurations do not
perform better than fully synchronous training in TensorFlow.
We attribute this to the fact that experiments in [12] do not
involve any momentum tuning.

APPENDIX H
APPENDIX STUDYING TOTAL COST OF OWNERSHIP (TCO)

Our study showed that CNN training is compute-bound
regardless of the compute device used. Given that we can
now train CNNs on CPUs proportional to the CPU FLOPS,
this opens new questions in total cost of ownership (TCO) for
running CNN systems. We discuss those trends and changes.
Single Node: We compare the price of running Omnivore
on a GPU instance (g2.2xlarge, $0.65/hr, 1.2 TFLOPS) and
a CPU instance (c4.4xlarge, $0.838/hr, 0.7 TFLOPS) for the

as a black-box, and we are 2.7× faster than other systems on 4
GPUs and also 15% faster on a single GPU by using the weak
CPU alongside the EC2 instance’s GPU. More generally, we
show that each device or node in a cluster can be treated as a
black-box that is characterized only by the throughput which
it provides and is irrelevant to the type of hardware on that
node (e.g., CPUs or GPUs).

Our second contribution was an empirical study of the
factors affecting time to convergence for distributed deep
learning training, and a novel, theoretical characterization of
asynchrony which demonstrates that by tuning algorithmic
(explicit) momentum in SGD there is no statistical penalty
associated with asynchronous execution. We justiﬁed this
empirically. We deﬁned a tradeoff space and demonstrated
that the execution strategy and server architecture were key in
reducing the total time to convergence. We further showed that
all existing distributed deep learning systems fall somewhere
along this tradeoff space, but do not optimize within the space.
Finally, we studied each of these tradeoffs by decoupling
their impact on hardware and statistical efﬁciency. This made
it possible to study these factors in isolation and build an
optimizer which optimizes within the tradeoff space. We
showed both theoretically and empirically the need to jointly
tune hyper-parameters with execution strategies in order to
avoid slower convergence or divergence. We show that our
optimizer provides a > 20× reduction in time to convergence
compared to other systems which select sub-optimal points in
the space, and we also show that our optimizer is versatile by
applying it to existing tools. In doing so, we close the gap
between our system to 3× faster than other systems, in some
cases also preventing divergence in those other tools.

Omnivore: An Optimizer for Multi-device
Deep Learning on CPUs and GPUs

Stefan Hadjis
Dept. of Computer Science
Stanford University
Email: shadjis@stanford.edu

Ce Zhang
Dept. of Computer Science
ETH Zurich
Email: ce.zhang@inf.ethz.ch

Ioannis Mitliagkas, Dan Iter, Christopher R´e
Dept. of Computer Science
Stanford University
Email: {imit, daniter, chrismre}@stanford.edu

6
1
0
2
 
t
c
O
 
9
1
 
 
]

C
D
.
s
c
[
 
 
4
v
7
8
4
4
0
.
6
0
6
1
:
v
i
X
r
a

Abstract—We study the factors affecting training time in
multi-device deep learning systems. Given a speciﬁcation of a
convolutional neural network, our goal is to minimize the time
to train this model on a cluster of commodity CPUs and GPUs.
We ﬁrst focus on the single-node setting and show that by using
standard batching and data-parallel techniques, throughput can
be improved by at least 5.5× over state-of-the-art systems on
CPUs. This ensures an end-to-end training speed directly propor-
tional to the throughput of a device regardless of its underlying
hardware, allowing each node in the cluster to be treated as a
black box. Our second contribution is a theoretical and empirical
study of the tradeoffs affecting end-to-end training time in a
multiple-device setting. We identify the degree of asynchronous
parallelization as a key factor affecting both hardware and
statistical efﬁciency. We see that asynchrony can be viewed as
introducing a momentum term. Our results imply that tuning
momentum is critical in asynchronous parallel conﬁgurations,
and suggest that published results that have not been fully tuned
might report suboptimal performance for some conﬁgurations.
For our third contribution, we use our novel understanding of
the interaction between system and optimization dynamics to
provide an efﬁcient hyperparameter optimizer. Our optimizer
involves a predictive model for the total time to convergence
and selects an allocation of resources to minimize that time.
We demonstrate that the most popular distributed deep learning
systems fall within our tradeoff space, but do not optimize within
the space. By doing this optimization, our prototype runs 1.9×
to 12× faster than the fastest state-of-the-art systems.

I. INTRODUCTION

In recent years, deep learning has provided signiﬁcant
improvements in quality for a number of data-driven appli-
cations [1]–[4]. An important aspect of deep learning is that
quality improves with the amount of data we can process;
therefore, advances in system efﬁciency and scalability directly
improve quality. This has led to an arms race of distributed
deep learning systems both in industry (e.g., Google’s Dist-
Belief [3], Microsoft’s Adam [5]) and in academia [6]–[9].

Despite this proliferation of deep learning systems, there
have been few studies of deep learning from a data-systems
perspective. Each of these systems makes a set of design
decisions that may not work for other tasks or hardware
settings. In our experience working with multiple Ph.D.-
level users of these systems—including experts in pathology,
radiology, computer vision, and the energy sector—it is often
very difﬁcult even for advanced users to make these design
decisions themselves. It is not uncommon for a suboptimal

design choice to result in an end-to-end runtime that is an
order of magnitude slower than what is possible. Moreover,
most systems provide no way of automatically selecting an
optimal conﬁguration, placing this burden on the user. This
also contributes to two debates about deep learning systems.

Debate 1: CPU vs. GPU. There has been a long debate about
CPUs vs. GPUs for deep learning. GPUs are popular for CNN
systems because of the high throughput they provide, but they
contain smaller off-chip memories. Microsoft’s Adam argues
that CPUs can deliver more cost-effective performance [5]. For
users who cannot control their data-center hardware, Amazon’s
EC2 provides GPUs but Google Compute does not.

Debate 2: Synchronous vs. Asynchronous Training. An-
other debate is about the synchronization strategies to use in
multi-device deep learning. For example, Google’s latest Ten-
sorFlow paper [10] comes out in support of fully synchronous
methods, citing recent papers [11], [12]. Other systems, such
as DistBelief [3], Project Adam [5], H2O [13], and recent
theoretical efforts [14] focus on asynchronous training and
argue that it is more scalable than fully synchronous strategies.

In this paper, we perform a ﬁrst study of the design space for
deep learning systems. We identify the key tradeoffs for single-
device and multi-device systems, providing insights into the
above debates. We ﬁnd that the CPU implementation of many
state-of-the-art systems misses textbook batching optimiza-
tions, which can make the CPU implementation at least 5.5×
faster. We also see that when the momentum parameter [15] is
tuned, there is no penalty for asynchrony—except in a short,
cold-start training period. This previously unknown connection
between asynchrony and momentum was surprising even to
leading groups in the area. Recent work does not tune momen-
tum [11], [12] and reports that asynchronous conﬁgurations
are slower. We see on different systems, including TensorFlow,
that tuning changes this outcome: asynchronous conﬁgurations
are faster when we tune. These theoretical and experimental
insights help us understand the tradeoff space better and build
an automatic optimizer to choose the optimal conﬁguration. As
a result, our prototype system, Omnivore, can be 1.9× to 12×
faster than the fastest competitor systems. Our results have
attracted interest from major companies. In collaboration with
a major chip manufacturer, we are integrating our optimizers
on new platforms of much larger scale.

Overview of Technical Contributions

To conduct our study, we develop a prototype distributed

system called Omnivore.1 We make three contributions.

Scope of Our Study: We focus on perhaps the most
popular deep learning models, convolutional neural networks
(CNNs), which are state-of-the-art for a wide range of applica-
tions (e.g., image processing, video analysis, drug discovery).
Our study answers the following question: “Given a cluster
(e.g., X machines, Y GPUs, Z CPUs, etc.), how do I train my
CNN as quickly as possible?”. We assume that the following
are given: (i) a deep learning model (network architecture),
(ii) a dataset for training this model, and (iii) a set of
computational resources (a number of devices on machines,
their throughput, and the network speed).2 We then study
how to minimize the total training time. We build a complete
prototype capable of training the most popular deep learning
models. This allows us to hone in on two major choices: (i)
how to use hardware on each node and (ii) the degree to
which asynchrony can be tolerated. Our work demystiﬁes these
factors by identifying the key tradeoffs that underlie all design
decisions, providing theoretical guidance about asynchrony,
and quantifying the impact of those tradeoffs experimentally.
Contribution 1: Single-Device Optimizations: We show
that it is possible to achieve throughput proportional to the
maximum FLOPS of both CPUs and GPUs. This is not trivial;
while state-of-the-art systems achieve GPU speeds propor-
tional to the device throughput, existing CPU implementations
can be sped up signiﬁcantly compared to what is reported
in the literature. Our study builds on two key optimizations
we reported in a workshop [16]. We use batching and data-
parallel optimizations—not employed in other systems—to
achieve end-to-end speedups of more than 5.5× over state-of-
the-art systems on commodity CPUs. Such optimizations are
not always possible on the GPU, but by selecting this strategy
for the CPU, we now achieve speeds proportional to the peak
throughput. This allows us to build a simpler optimizer by
modeling CPUs and GPUs as black boxes.

Contribution 2: Multi-device Tradeoffs: Our second con-
tribution is an empirical study of factors affecting training
time for multi-device deep learning training. We analyze the
decisions made on existing systems and ﬁnd that while they are
diverse, the strategies of the most popular systems fall within
a tradeoff space deﬁned by two fundamental dimensions [3],
[5]–[9]: (i) the server architecture, or how the layers of a
CNN map to devices; and (ii) the execution strategy, or how
batches of data are mapped to machines for processing. We
develop a simple framework that allows us to model each of
these approaches. Devices are organized in compute groups.
Each compute group is responsible for a single batch of
data per iteration. Inside a group, the computation occurs
in standard, synchronous steps. Across groups, computation
happens asynchronously.

1 https://github.com/HazyResearch/Omnivore
2 We only do basic network optimization, and we assume that machines are connected

by a uniform and fast topology, e.g., if they were housed on the same rack.

We study the impact of asynchrony on the end-to-end
performance of training deep learning systems. Not surpris-
ingly, the more asynchrony there is, the faster the system is
for each iteration, which we call hardware efﬁciency. This
happens because there is less coordination between workers.
The challenge is to understand how asynchrony affects the
number of iterations to converge, which we call statistical
efﬁciency. We provide novel understanding of the factors that
affect statistical efﬁciency. Empirically, we observe that the
optimal value for the momentum parameter decreases as we
Indeed, our
increase the number of asynchronous workers.
theory in [17] shows that asynchronous-parallel training can be
viewed as a synchronous update but with an increased, implicit
momentum term. Furthermore, if the optimal momentum value
for a problem is above the implicit momentum, then there is no
penalty for running asynchronously, as long as the momentum
is properly tuned. Although momentum is explicitly (i.e.,
algorithmically) introduced in almost every system [15], we
are the ﬁrst to realize this connection between asynchrony and
momentum. Here, we validate this experimentally: we see that,
by properly tuning momentum, asynchronous methods can be
at least 1.5–2.5× faster than using the standard momentum
value of 0.9, which is used in most existing work. This
understanding of statistical efﬁciency, along with an analytical
hardware efﬁciency model, forms a novel system tradeoff
space.

Contribution 3: Simple Automatic Optimizer: Based on
the intuition behind our
our theory and empirical study,
optimizer is very simple: pick the highest degree of asynchrony
such that
the implicit momentum induced by asynchrony
is below the optimal momentum. Given a ﬁxed number of
compute groups (which control the degree of asynchrony), we
grid-search the parameters for learning rate and momentum by
measuring the statistical and hardware efﬁciency for minutes
(less than 10% of the time to train a network). If the best-
the optimizer chooses this
found momentum is non-zero,
conﬁguration. If it is zero, we assume that there could be
a better setting with fewer compute groups. Our optimizer is
able to choose a near-optimal point in the tradeoff space, and
we demonstrate that our system achieves end-to-end speedups
of 1.9× to 12× on popular CNN workloads compared to state-
of-the-art tools that choose suboptimal tradeoff points. We
compare our simple optimizer with a state-of-the-art Bayesian
optimization approach [18]. Both approaches are able to reach
the same ﬁnal accuracy (within 1%), but the Bayesian strategy
takes almost 6× as long. We can also apply our optimizer to
other deep learning systems. In some cases, this prevents those
other tools from diverging, while in other cases, it speeds them
up by 7×.

Outline: We present background in Section II. Section III
and Section IV introduce the tradeoff space related to single-
machine and multi-device settings, respectively. Section V
describes the optimizer for making decisions in this tradeoff
space. We validate our results in Section VI, discuss related
work in Section VII, and conclude in Section VIII.

II. BACKGROUND

A. Convolutional Neural Networks (CNNs)

A convolutional neural network (CNN, [2]) consists of
layers L1, L2, . . . , LP . Each layer is an operator which takes
as input a 3D data tensor D ∈ Rn×n×din and transforms
to a resulting 3D data tensor R ∈ Rm×m×dout,
it
i.e.
LF W
(Dp) = Rp. F W indicates the layer running in the
p
“forward” direction to transform D into R. Layers have a
second operation, backward or BW , described later. Often,
D1, the input to the ﬁrst layer L1, is an image I ∈ Rn×n×3,
where 3 represents the RGB color channels.

For layers after L1 (the input layer), the input tensor Dp
comes from the output of a prior layer (usually Dp = Rp−1),
such that the CNN layers are cascaded to deﬁne a composite
operation (boldface highlights inputs and outputs)

RP = LF W

P

◦ LF W

P −1 ◦ ... ◦ LF W

2

◦ LF W
1

(I)

(1)

The ﬁnal result RP is the CNN’s prediction for image I.
For example, if the task is image classiﬁcation with 1000
categories, the tensor RP ∈ R1×1×1000 is a vector containing
the probability of each category. This prediction is then
compared to C, the true classiﬁcation for I, using a loss
function (cid:96)(RP , C) that evaluates the quality of the prediction.
A lower loss indicates a better prediction.

Many types of layers exist in a CNN. Some layers perform
a pre-deﬁned transformation such as downsampling while
other layers contain a model, W, and perform an operation
parameterized by the model. Models are also known as weights
or parameters. The models of all layers constitute the entire
set of weights or parameters of the CNN, i.e.,

W = WCNN = {WL1 , . . . , WLP }.

B. Stochastic Gradient Descent

The goal of CNN training is to optimize the model W in
order to minimize the loss function (cid:96)(RP , C), also denoted as
(cid:96)(W, I, C) to make the fact that RP is a function of W and I
explicit. Low loss is correlated with high prediction accuracy
and in this work we refer to both. The most popular training
algorithm for CNNs is an iterative technique called stochastic
gradient descent (SGD). Each SGD iteration consists of a
forward and backward pass.

The input to each SGD iteration is an image-label tuple
(I, C) as described above. The forward pass calculates the
prediction RP of I using equation (1), and then the prediction
error compared to C is used to calculate the gradient (or
derivative) of (cid:96) with respect to RP . We denote this gradient
as ∇RP (cid:96). Now the cascade of equation (1) runs in reverse by
applying each layer in the “backward” direction:

LBW
1

◦ LBW
2

◦ . . . ◦ LBW

P −1 ◦ LBW

P

(∇RP(cid:96))

(2)

equation (2) implements the chain rule of calculus. The BW
operation of layer p takes as input a data gradient ∇Rp (cid:96) and
outputs a data gradient ∇Dp (cid:96). Internally, it also updates that
layer’s model WLp by (i) calculating a gradient of the loss
with respect to the model, ∇WLp (cid:96), and (ii) using an SGD

Fig. 1. Abstracting a CNN into two phases.

update on the model. SGD repeats for many, often millions
of iterations, until the loss is sufﬁciently low, i.e. the model
is sufﬁciently optimized. The initial model W (0) is randomly
initialized. The SGD update at step t takes the form

W (t) ← W (t−1) + V (t),

(3)

where the new step, V (t), consists of a scaled version of the
previous step, V (t−1), plus a gradient calculated with (I, C):
(cid:16)

(cid:17)

(cid:104)

V (t) ← µV (t−1) −η

∇W (cid:96)

W (t−1), I, C

+ λW (t−1)(cid:105)

(4)

In Section IV we introduce the notion of asynchronous up-
dates. The main change in equation (4) under asynchrony,
is the use of an older model, W (s), when evaluating the
gradient ∇W (cid:96). This gradient (speciﬁcally, its negative) is the
direction to “step” within the parameter space each SGD
iteration. The learning rate, η, is the scale factor applied to the
magnitude of this gradient, i.e. the size of the step. λ dictates
the amount of regularization, which is an input to the training
problem (part of the CNN model to train). µ is the amount
of explicit momentum we add to the update. Momentum is
used to “accelerate” learning in the directions common to each
gradient by keeping a history of past gradients and adding this
history to the gradient of the current step, with past gradients
decayed exponentially. Commonly, in order to produce more
stable gradients, each SGD iteration does not process a single
tuple (I, C), but a batch of b tuples, e.g., 256, in which case D
and R become 4D. The gradients from each tuple are summed
to produce a single, combined gradient for that iteration.

Selecting the right values for hyperparameters (η, µ, b) is
critical for performance. We will describe a simple optimizer
to pick these parameters.

C. CNN Computation

Of all the CNN layers, two layers are the most compu-
tationally intensive, convolutional (conv) and fully-connected
(FC) layers. A convolutional layer performs many independent
convolutions over an image, i.e., several sliding window oper-
ations; an FC layer performs a dense matrix multiplication. In
many popular implementations, the bottleneck in both layers
is a matrix multiply implemented as a call to either a BLAS
or cuBLAS library. For our study, their data properties are
more important, and we refer to Chetlur et al. [19] for a more
detailed description of their use in machine learning.

that convolution layers repeat

Figure 1 illustrates a state-of-the-art CNN, in which all
convolutional layers always appear before all fully-connected
layers. In this work we introduce an abstraction which sepa-
rates a CNN into two phases, each consisting of a number of
consecutive layers: ﬁrst the convolution phase (conv), whose
layers have large data tensors D and R (e.g., 100MB-1GB)
and small models W (e.g., 5-50 MB), followed by the fully-
connected phase (FC), whose layers have small D and R (e.g.,
1-10MB) and large W (e.g., 30-300 MB). The reduction in
data size is in part due to pooling layers in the convolution
phase which perform down-sampling. The increase in model
size is due to the fact
the
same weights for the sliding window. Note that our two-phase
categorization also applies to modern, non-linear CNNs [20].
The computation for both the conv and FC phases is usually
compute-bound although the conv phase contains signiﬁcantly
more computation (e.g., in AlexNet conv is 1.6 TFLOPs and
FC is 80 GFLOPs, or 95% of the computation is convolution).
Within a machine, each layer’s computation can be mapped
to CPUs, GPUs, or a combination of both. In addition, this
computation can be parallelized either using data parallelism
(partitioning the data batch and replicating the model, which
works well for the conv layers) or model parallelism (par-
titioning the model and replicating the data batch, which
works well for the FC layers). In distributed settings, the layer
computations are mapped across machines. We will show later
this mapping is always done at the coarser granularity of entire
phases (conv or FC) because it reduces network delays due to
the distinct model and data sizes of the two phases. We study
the choices of mapping and parallelization techniques for both
single and multiple devices in Section III and IV.

D. Problem Deﬁnition

We study systems tradeoffs to build an optimizer for the
most widely used networks/algorithms. We focus on SGD
due to its popularity, although our optimizer applies to other
algorithms as well. More precisely, we are given as input
the following: (i) a CNN architecture {L1, ..., LP }, including
regularization, (ii) a dataset D consisting of data batches,
(iii) a device graph G in which vertices are hardware devices
(speciﬁed by their throughput) and edges are communication
speeds between devices. Our goal is to design an optimizer
which creates a plan for physical mapping and execution
strategy in order to train as quickly as possible.

A plan for physical mapping maps the computation (both
FW and BW) of each layer to vertices (e.g., GPUs or CPU
cores) of the device graph. A plan for the execution strategy
maps data batches to devices in order to parallelize SGD in
the multi-device case. The key decision here is the degree of
asynchrony in execution. Section III and Section IV study how
to do physical mapping within a device and across devices,
respectively. Section IV also studies the impact of asynchrony.
If the cluster is homogeneous, we do not need the explicit
device graph – instead, a few parameters, such as the number
of devices and the throughput of each device, are enough to
specify the cluster for the optimizer.

Fig. 2. The computation performed by a convolutional layer which processes
bp images at a time in parallel, where 1 ≤ bp ≤ batch size. The convolutions
can be implemented either directly (top) or using the lowering/GEMM method
(bottom).

III. SINGLE-DEVICE TRADEOFF

that

is proportional

We ﬁrst study the systems tradeoffs within a single device.
We show that for each device (GPU or CPU) we can achieve
throughput
to its peak FLOPS. This
enables the distributed optimizer to treat each device as a
black box in Section IV. This is not a trivial property for deep
learning: many existing systems [21]–[23] use either CPUs
or GPUs, but they often report that GPU implementations
are an order of magnitude faster than CPU even when the
devices offer similar FLOPS. Therefore the challenge is to
utilize the FLOPS on the CPU. We study the key kernel in
CNN implementations, which is compute bound. We introduce
a data batching technique which trades off memory footprint
for compute time and demonstrate that this tradeoff gives
a more than 5× CPU speedup over existing systems. With
this optimization now both CPUs and GPUs give throughput
proportional to the FLOPS offered by the device.

A. Convolutional Layer Computation

As reported in the literature and conﬁrmed by our ex-
periments, the most computationally intensive layers in the
CNN are the convolutional layers. Together, all convolutional
layers in a CNN often consume between 70-90% of total
execution time. Recall
layer contains
a model, which we will also call its kernel and denote as
K. A convolutional layer accepts as input a 4D data tensor
D ∈ Rn×n×din×b, where recall b is the batch size. K is also
a 4D tensor, K ∈ Rk×k×din×dout. The output is a 4D data
tensor R ∈ Rm×m×dout×b, where:

that a convolutional

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Rx,y,z,w =

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48),wKx(cid:48),y(cid:48),d(cid:48),z

(5)
Like most HPC kernels a straightforward implementation is
suboptimal, and many optimized implementations of this con-
volution kernel exist [19], [21], [24]. A popular implementa-
tion is to perform equation (5) as a dense matrix multiplication

Device Type
(EC2 Instance)
1× CPU Xeon E5-2666
(c4.4xlarge)
2× CPU Xeon E5-2666
(c4.8xlarge)
1× GPU Grid K520
(g2.2xlarge)
Dual-GPU Grid K520
(g2.8xlarge)

Device % Peak

Caffe Omnivore

% Peak % Peak
SGEMM

GFLOPS

742

18%

1,670

1,229

2,458

8%

53%

26%

56%

40%

54%

52%

81%

71%

99%

99%

Fig. 3. Across several CPUs and GPUs we obtain throughput on convolution
layers that is ∼ 50% of the device peak (shown for AlexNet, total F W +BW
time for all conv layers). We also show large GEMM as a reference of what
the device can achieve.

(also known as GEMM, general matrix multiply), which [19]
demonstrates to be versatile and fast for a range of size
parameters, as GEMM kernels are highly optimized.

the data and model

In order for equation (5) to be carried out as a matrix mul-
tiplication an initial reformatting and replication step called
lowering is required to put
into the
correct format. Figure 2 shows the three logical steps in the
lowering process: (i) lowering, which transforms 3D tensors
D and K into 2D matrices ˆD and ˆK; (ii) matrix multiply,
which multiplies ˆD ˆK to get the result ˆR; and (iii) lifting,
which transforms ˆR back to a tensor representation of R. The
lifting step is fast (reformatting), but the lowering step requires
replication of the data, sometimes by a factor of 1 or 2 orders
of magnitude. This blowup in the data size demands more
off-chip memory and more computation in step (ii).

B. Batching and Data Parallelism

The design tradeoff between CPUs and GPUs arises as
a result of this increase in data size. Assume that we are
given a ﬁxed batch size of b images (e.g., b = 256). GPUs
cannot ﬁt an entire batch of lowered data into off-chip memory,
therefore many CNN implementations on GPUs perform low-
ering/GEMM serially on one or few images at a time until all
b have been processed. On the CPU however, off-chip memory
is larger which allows lowering/GEMM to be performed on
all b images at once. This leads to signiﬁcant CPU speedups
compared with state-of-the-art tools which do not explore this
tradeoff but use the same implementation suited to the GPU
for the CPU. In particular, as Figure 3 shows, this allows us
to view a CPU or GPU as simply a device producing FLOPS;
for reference, we also include the FLOPS delivered by the
most popular CNN framework Caffe [21] and the call of an
optimized single-precision matrix multiply (SGEMM) for that
hardware. The throughput obtained by Omnivore on all devices
in Figure 3 also matches the expected range for CNNs (on
GPUs) reported by the manufacturer [19].

To achieve this throughput on the CPU, we use a simple
convolution batching technique in which we ﬁrst lower all
b images in a batch before performing any GEMM. After
this lowering, we perform a single GEMM for all b images,
rather than b smaller GEMMs. This consumes b× the memory
because the ˆD matrix is b× larger than when lowering images
one by one. However, it has two speed beneﬁts: (i) one large
GEMM is faster than b smaller GEMMs because CPU caches

Fig. 4. The impact of batch size (bp) and number of threads on the GEMM
kernel (total batch size b = 256, 8 physical cores). Shown for the GEMM in
the largest convolutional layer of AlexNet.

and vector instructions are fully utilized, and (ii) lowering all
images in the batch at once enables data parallelism to be used
during lowering. Speciﬁcally for (ii), given n CPU cores, a
batch is split into n partitions, with b/n images per partition.
A separate thread then performs lowering and GEMM on each
partition, i.e. each thread performs convolution on a subset of
the batch. This data parallelism can be applied to other layers.
Generalizing this implementation tradeoff, bp images can
be processed in parallel by the convolution layer, where
1 ≤ bp ≤ b. Increasing bp increases the memory footprint but
decreases the overall execution time. Figure 4 shows batching
experiments for a CPU GEMM kernel. All points in each graph
perform GEMM on 256 images (i.e., b = 256), but the number
of total GEMM calls depends on bp (e.g., if bp = 256, there
is one large GEMM). We therefore advocate the strategy of
selecting bp as large as possible (up to b) such that ˆD ﬁts into
the off-chip memory. This can be predicted because memory
usage increases linearly with bp as seen in Figure 4 (c). For a
range of modern CPUs that we used in our experiments, the
optimal bp value is always b.

While this tradeoff is simple, it enables FLOPS proportional
scheduling which allows us to abstract away the details of
devices in our distributed implementation.

IV. MULTI-DEVICE TRADEOFF

In this section, we study the distributed setting. Given a
CNN, an input set of data and a set of devices, our goal is to
map each layer of the CNN and a subset of data to each device.
Many distributed CNN systems [3], [5]–[9] can be mapped to
points within our tradeoff space.

We show that a key tradeoff is the degree of asynchrony. We
build models predicting how hardware and statistical efﬁciency
are affected by asynchronous execution. By decoupling hard-
ware efﬁciency and statistical efﬁciency and creating separate
models, the optimizer can ﬁnd a balance and minimize the
total time to convergence. Speciﬁcally, we argue for an analytic
model for hardware efﬁciency and we are able to give a new
theoretical characterization of statistical efﬁciency.

A. Setup and Assumptions

We separate each layer into compute servers, responsible
for computation, and a model server responsible for handling
reads from and writes to the model. These “servers” are
virtual: many servers may be mapped to the same device
or an individual server may be mapped to several devices.

serialized into a chain, hence we view the input as a list of
layers (without loss of generality).

Execution: Given a list of layers, the main computational
loop is to move through the list forward calling the forward
operation and in reverse order calling the backward operation
at each step. As we have decomposed each layer into a model
server and compute servers and further mapped compute
servers to compute groups over several devices, it is the re-
sponsibility of our execution engine to make sure that all data
is on each device when needed. We use standard techniques
to hide latency of device copies, e.g., double buffering.

B. Hardware Efﬁciency Model

The goal of this section is to create a predictive model
HE(S) for hardware efﬁciency. The goal is to characterize
the impact of the amount of staleness S in the system (or
equivalently the number of compute groups, g). We derive a
simple analytic model which reasons about the bottlenecks.
An execution strategy partitions the N conv devices into g
compute groups. Again for concreteness, assume there are k
devices per group (k = N/g). Let tconv(k) be a function
that returns the time that a group of size k needs to compute
the convolution phase. We make the assumption that FC only
operates sequentially3. Note that the number of requests to the
FC phase is a function of the number of groups, and let tf c
be the time that the FC phase needs to serve one group.

Given g, tconv(k) and tf c, our goal is to create a hardware
efﬁciency model which predicts the time per iteration. There
are two cases depending on which phase is the bottleneck.
(1) When the FC phase is saturated, i.e., it starts to serve the
next request immediately after the previous request ﬁnishes,

Time per iterationsaturated fc = tf c;
(2) When the FC phase is not saturated, each conv group
becomes the bottleneck. In this case,

Time per iterationsaturated conv = (tconv(k) + tf c)/g
which is the total time for a single iteration divided by the
number of parallel groups. Thus, our predicted model for
iteration time, HE(g), is:

HE(g) = max{tf c, (tconv(k) + tf c)/g}

Given tconv(k), tf c and the number of groups, g, the model
can now predict what the mode of saturation will be and
therefore the time per iteration.

Obtaining the Parameters: The parameters above can
be measured with high accuracy and low variance. tf c can
be measured by running an iteration on a single device, but
tconv(k), though still directly measurable, requires measure-
ments for each k. Instead, tconv(k) can be calculated from (i)
the throughput of each node; (ii) the network speed; and (iii) a
measurement of tconv(1) (which only needs to be measured for
a single k, k = 1, and on a single device). Figure 5(b) shows
that our hardware efﬁciency is accurate; detailed evaluation
in the appendix.

3 For simplicity, we assume different groups (batches) cannot be executed in parallel

on the FC server but our model can be extended to more general cases.

Fig. 5.
(a) Default physical mapping used by Omnivore. (b) Predicted and
measured iteration time as the number of devices (machines) per group
changes (32 c4.4xlarge machine, AlexNet)

While the separation of compute and model servers is present
in all systems, only project Adam [5] described mapping
both compute and model servers to the same device (or set
of devices for parallelism), i.e. merging the servers, which
they did for FC layers. Figure 5 (a) shows this mapping. For
concreteness, suppose there are N + 1 devices: one device
handles the FC layers, and the remaining N devices handle
the conv layers (motivated by 90 − 95% of the computation
being in the convolutions). To simplify our exposition we refer
to this reference architecture throughout Section IV. Section V
demonstrates the beneﬁts of this architecture empirically both
in terms of hardware and statistical efﬁciency.

Compute Groups: The input data to a layer is divided
into batches (whose size is determined by the system). The
main computational operation for each layer is to (i) read the
model; (ii) compute an operation on the data (the forward
or the backward pass) for the given a batch of data; and (iii)
update the model. We assign each compute server to a compute
group. Within a compute group, many devices speed up the
computation of an individual batch of data, e.g., all devices
compute a single gradient for a batch. Across compute groups,
different devices process distinct batches.

Asynchrony and Staleness:

In most systems, compute
groups communicate asynchronously [3], [5], [25]: the models
locking or barriers, and so forward
are updated without
and backward passes may be computed with a stale model.
Intuitively, the lack of coordination allows one to make better
use of the hardware (better hardware efﬁciency) but the use of
stale information may reduce statistical efﬁciency. If there are
g = S + 1 compute groups, we call S the staleness parameter.
This is justiﬁed as the computation within each group in step
(ii) above is very regular for CNNs (standard deviation of
runtime is less than 6% of mean), hence these groups execute
in a nearly round-robin fashion.

Throughout this section, we make two simplifying assump-
tions: First, we focus our description on the two phases
described in Section II, i.e., we abstract networks into con-
volutional layers (conv) which have a large amount of data
but a small model, and fully-connected layers (FC) which
have small data but a large model. Practically,
these two
layers are the main bottlenecks in current networks. Their
differing characteristics give rise to different points of the
tradeoff space. Second, many popular networks are simply a
single chain, not a DAG (e.g., AlexNet), and DAGs can be

C. Statistical Efﬁciency Model

We now characterize the effect of staleness on statistical
efﬁciency. While working on Omnivore, we realized that the
momentum value that yields the fastest convergence decreases
as we increase the number of asynchronous workers. Using the
right value can signiﬁcantly reduce the number of iterations
to reach a target loss. This motivated us to mathematically
analyze this phenomenon in our companion theory paper [17].
The result
is surprisingly elegant: under a simple model,
asynchrony introduces an extra momentum term in the SGD
update. This comes in agreement with our experimental ﬁnd-
ings in this paper. Here, we recap the essential result of
our theory and validate it experimentally on different systems.
Importantly, our result is predictive. We use it in this work to
complement our experimental ﬁndings on the importance of
momentum tuning and to design the ﬁrst asynchrony-aware
optimizer for deep learning systems in Section V.

We make the following assumptions, which are not neces-

sary but are helpful to capture the essence of our result.
(A0) The batch for each step is drawn uniformly at random
with replacement. This is a standard assumption of SGD.
(A1) Variations in the time to process a step are due to unmod-
eled system behavior. Also, variations are independent of
the speciﬁc batch drawn. This is justiﬁed by the fact that,
for all batches, all computation involves dense operations.
takes to process a step is exponentially
distributed and independent from other steps. This is
a simplifying but standard assumption from queuing
theory [26]. A more general (and complex) version of
Theorem 1 below holds without this assumption.

(A2) The time it

Theorem 1: Consider g asynchronous groups and set explicit
momentum to zero, i.e. µ = 0 in the update of equation (4).
Under the above assumptions, the expected update becomes

EV (t+1) =

1 −

EV (t) −

E∇W (cid:96)(W (t)).

(6)

(cid:18)

(cid:19)

1
g

η
g

In which (cid:96)(W ) denotes the expectation of (cid:96)(W, I, C) over the
random draw of possible batches (I, C).

In plain English, asynchrony increases momentum–there is
implicit momentum of 1−1/g. This not only matches the stan-
dard form of momentum used by deep learning practitioners in
equation (4), but also can predict measured system behavior.
Figure 6 shows the predicted and actual measured momentum
for two datasets: As long as the asynchrony-induced implicit
momentum is less than the optimal total momentum, we can
algorithmically compensate with explicit momentum. When
however, implicit momentum exceeds the optimal total mo-
mentum, we start incurring statistical inefﬁciency. We use this
intuition as the basis for our optimizer.

Cold-start: We observe a phenomenon similar to burn-
in [27] in Gibbs samplers. The model needs a few iterations
to set
the appropriate scale of the model parameters. On
Imagenet-1000, we ﬁnd that 5 passes over the dataset (< 15%
of total execution) sufﬁce to “warm up” the model. As a result,
the optimizer will start by running synchronously and then
switches to asynchronous (Section V).

Fig. 6. Predicted and measured momentum moduli: (Left) Predicted, Theo-
rem 1 (Middle) Measured, CIFAR (Right) Measured, ImageNet

V. DISTRIBUTED OPTIMIZER

This section uses the models and tradeoff space charac-
terization of the previous two sections to create (i) a plan
for physical mapping which maps each server to a machine,
and (ii) a plan for the execution strategy which deﬁnes the
number of compute groups by allocating data batches to each
server. As in previous sections we assume a ﬁxed number of
machines. We ﬁrst discuss the process of physical mapping
and then describe our optimizer. We conclude with theoretical
and empirical justiﬁcation for the optimizer, and in Section VI
compare it to state-of-the-art, Bayesian approaches.

A. Physical Mapping

We observe that in all CNNs the architecture of Figure 5 (a)
works best, and describe other physical maps in the appendix.
Omnivore maps the FC compute and model servers to the
same machine, an approach we call merged FC. Merging the
FC compute and model servers to the same devices was shown
in [5] to reduce communication overhead in a CPU cluster
(better hardware efﬁciency), however it was not known that (1)
this also beneﬁts hardware efﬁciency for a GPU cluster, and (2)
this technique also beneﬁts statistical efﬁciency by eliminating
staleness in the FC model. These are both observations we
make. The remaining machines are used for the conv compute
servers. The conv model servers are mapped to one of the
conv compute machines. These optimizations are critical: on
a cluster of 33 EC2 c4.4xlarge machines, not merging the
FC servers incurs an additional hardware efﬁciency penalty of
1.2× due to increased communication as well as a statistical
efﬁciency penalty of 2.5× because of staleness in the FC
model. The key tradeoff is therefore the number of conv
compute groups, which we describe next.

B. Optimizer

There are multiple interdependent factors that have impact
on the performance of the training procedure: (1) the number
of compute groups; (2) the momentum; and (3) the learning
rate. The optimal setting of these parameters might also change
during training, so our optimizer runs periodically in epochs
(e.g., every hour). Algorithm 1 shows the end-to-end optimizer
that runs after the initial cold-start period.

In each of the epochs, the key issue is to set the number of
compute groups, g. We perform a grid search over both the
learning rate and the momentum starting at a particular value
of g. This search determines the optimal explicit momentum
for that g by selecting the conﬁguration with the lowest ﬁnal

Algorithm 1 Automatic Optimizer for the Tradeoff
Input: Time budget T and possible choices of (1) # compute groups

CG, (2) momentum M, and (3) learning rate H.

Output: Trained model W .

(µ, η) ← gridSearch(M, H|W, g)
while µ = 0 and g > 1 do

1: g = CG
2: while not reaching the termination criteria do
3:
4:
5:
6:
7:
8:
9: end while
10: return W .

end while
W ← train(g, µ, η, W ) for T minutes

g ← g/2
(µ, η) ← gridSearch(M, H|W, g)

loss. The key intuition is: set the highest amount of asynchrony,
g, such that this explicit momentum is non-zero. The reasoning
is that, when the optimal explicit momentum is 0, the implicit
momentum is likely higher than the optimal value, and a
cause of statistical inefﬁciency (c.f. Figure 6). In this case, we
reduce the amount of asynchrony by reducing g. We provide an
initial value for g by leveraging the hardware efﬁciency model.
In particular, we start with the smallest number of compute
groups that saturate the FC server. This can be determined
analytically or through measurements during the cold start
phase. Having selected a (g, η, µ), we run for the rest of the
epoch. At the end of one hour, the epoch ends, the model is
checkpointed (written to disk), and the optimizer repeats.

Importance of Compute Groups: We demonstrate that
using the right number of compute groups has an impact on
performance. Fixing the total number of machines, we try
different numbers of compute groups on CPU-L (Figure 9) for
the Imagenet 1000-class dataset, and AlexNet CNN. We grid-
search a number of values for the learning rate and momentum
and report the best result achieved in each case. Figure 7
reports (a) the time per iteration (hardware efﬁciency), (b)
the number of iterations to reach a speciﬁc training loss
(statistical efﬁciency), and (c) their product, which is the total
time required to reach the ﬁnal loss. Note that the hardware
efﬁciency curve in (a) is the same as in Figure 5 (b).

We see in Figure 7 (c) that g = 32 (fully asynchronous) is
3.7× faster than g = 1 (synchronous) as measured by wall-
clock time to ﬁnal loss. This is due to its 6.7× faster iteration
time in (a), although it requires 1.8× more iterations as shown
in (b). This matches the theory’s prediction:
increasing g
causes the optimal explicit momentum, µ∗, to decrease. We
noted in Figure 6 (right) that µ∗ drops to 0 at g = 32,
and consequently there is a penalty in statistical efﬁciency.
Running the optimizer of Algorithm 1 selects g = 4, which
is near-optimal: 5.3× faster than sync and 1.4× faster than
async. We repeat the same experiment for CIFAR and ﬁnd the
results are similar. In all cases, the optimal number of groups
is more than 2× faster compared to sync, and that Algorithm 1
always picks a near-optimal point strictly better than sync.

Fig. 7. Hardware efﬁciency, statistical efﬁciency, and total time for various
execution strategies.

VI. EXPERIMENTS

We evaluate the runtime performance of our system. We
show that, Omnivore outperforms state-of-the-art
tools by
1.9× to 12× on a range of training tasks. Our result holds (i)
across a diverse range of hardware, including both CPUs and
GPUs, (ii) in both single device and multiple device/machine
settings. Moreover, Omnivore contains an automatic optimizer
and therefore does not require users to input hyperparameter
values.

Our experiments also validate that our optimizer is up to

6× faster compared to state-of-the-art Bayesian optimizers.

A. Experiment Setup

Datasets and Models: We validate the performance of
Omnivore on a diverse range of datasets and models, as shown
in Figure 8. The largest corpus we use is ImageNet [28], which
contains 1.3M images. ImageNet is the de facto benchmark
for deep learning systems [2], [3], [5]. Training a model
on ImageNet can take tens of hours—even on the latest
7 TFLOPS Titan X GPU, NVIDIA reports that it took three
days to train with a single GPU.4 For some of our experiments,
which require running many conﬁgurations to convergence,
we used a reduced version, ImageNet8, containing the ﬁrst 8
classes. We train the standard CaffeNet5 on both data sets. We
also use smaller, but standard, datasets CIFAR and MNIST,
which are for object recognition and handwriting recognition,
respectively. We train the networks in Caffe’s tutorials for both.
Metrics: Our main metric of performance is the wall-
clock time required to achieve a given training accuracy. As
in Section V, we deﬁne statistical efﬁciency as the number of
iterations needed to achieve that training accuracy.

Competitor Systems and Experiment Settings: We com-
pare Omnivore against a range of existing tools using both
a single-machine and multiple machines. Single machine:
we compare Omnivore to Caffe [21] and Google’s Tensor-
Flow [22], the two most popular CNN systems, using Caffe’s
reference (CaffeNet) model. Multiple machines: we compare
Omnivore to MXNet [6] and SINGA [7], two popular dis-
tributed deep learning systems.6 Both MXNet and SINGA
support multiple execution strategies, and we consider all
of these strategies in our experiments. We set up and tune

4https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/
5https://github.com/BVLC/caffe/tree/master/models/bvlc reference caffenet
6 We also surveyed a range of distributed training systems including SparkNet, DL4J,
and others. We found that MXNet is the fastest system on our datasets/tasks. At the time
of writing, the ofﬁcial version of Caffe can only run on a single machine, and TensorFlow
does not contain a distributed example for AlexNet/CaffeNet.

Data Set
ImageNet
ImageNet8
CIFAR
MNIST
Shakespeare

# Images

Image Size
1.3 M 256×256×3
256×256×3
10 K
32×32×3
60 K
28×28×1
60 K
25×1×128
162 K

# Classes
1000
8
10
10
128

Fig. 8.
Statistics of Data Sets. All data sets are image-related except
Shakespeare, a natural language corpus for text synthesization used in our
RNN experiments.

Name
1xCPU
2xCPU
1xGPU
4xGPU
CPU-S
CPU-L
GPU-S

Machines
1 × c4.4xlarge
1 × c4.8xlarge
1 × g2.2xlarge
1 × g2.8xlarge
9 × c4.4xlarge
33 × c4.4xlarge
9 × g2.8xlarge

TFLOPS
0.74
1.67
1.23
4.89
6.68
24.51
44.24

Network
-
-
-
-
1 Gbits
1 Gbits
10 Gbits

$/hour
$0.84
$1.68
$0.65
$2.60
$7.56
$27.72
$23.40

Fig. 9. Summary of Machines and Clusters. TFLOPS are the total TFLOPS
that the given machine/cluster can provide. We are unable to open 33 4xGPU
machines due to limited EC2 availability and therefore there is no GPU-L.

both systems according to their tutorials. On data sets where
SINGA performs strictly worse than MXNet, we omit its result
from the ﬁgure. To demonstrate the merits of momentum
tuning and compute groups on other platforms, we also
implemented these features in TensorFlow in the appendix.

Omnivore is implemented in C++. Caffe, TensorFlow,
MXNet, and SINGA all implement their core components
in C++. We compile all systems with g++ 4.8.2, and use
OpenBLAS 0.2.15, cuda 7.5, as well as both cuDNN v3 and
v4 for competitor systems and report the faster result.

B. Performance Evaluation

We validate that Omnivore has faster execution to the
same quality as existing systems in training deep learning
models. More precisely, Omnivore achieves the same training
accuracy/loss faster, as measured by wall-clock time. We ﬁrst
present our main end-to-end performance results, comparing
Omnivore to the state-of-the-art. Then, we validate our contri-
butions by showing results (i) for a single machine comparing
to Caffe and TensorFlow and (ii) in the distributed setting
comparing to MXNet and SINGA. We evaluate the impact of
our tradeoff space and optimizer in Section VI-C.

1) End-to-end Performance: For large datasets, Omnivore
is faster than state-of-the-art tools. We validate this on Ima-
geNet. We compare Omnivore with MXNet and SINGA. We
train the standard CaffeNet on all systems using both CPU-
L and GPU-S clusters. We time out each run after 8 hours
and report the training accuracy at a given time. We tune all
competitor systems following the ofﬁcial performance tuning
guideline7.8 For Omnivore, we use its automatic optimizer
that does not require any hyperparameters. Because SINGA
is always slower than MXNet in our experiments, we omit it
from this ﬁgure. but discuss it in Section VI-B3.

7https://github.com/dmlc/mxnet/tree/db6f6a9418e5696b04be741a78a47ae877bb5505/

example/image-classiﬁcation and previous work [2], [29]

8 The time for this tuning of other tools exceeds Omnivore’s automatic

tuning time for the cold start phase so we omit these initial tuning times.

Fig. 10. End-to-end Performance of Omnivore and MXNet on ImageNet. We
omit SINGA as it performs worse than MXNet.

System

Caffe
TensorFlow
Omnivore

Machines

1xCPU
1.03×
1×
3.90×

2xCPU
1×
1.05×
5.36×

1xGPU
1.11×
1×
1.04×

4xGPU
1×
1.26×
3.34×

Fig. 11. End-to-end single machine performance across different machines
on CaffeNet. Times are normalized as the speedup over the slowest system
on the same machine (the larger the better). We run cuDNNv3, cuDNNv4
and no cuDNN for Caffe and TensorFlow and report the fastest one.

Figure 10 shows the result. We report sync and async for
MXNet as their documentation suggests trying both. Omnivore
reaches the same accuracy up to 11× faster than MXNet on
the GPU cluster and 12× faster on the CPU cluster. Compared
to sync, Omnivore is 4.5× and 1.9× faster respectively.
This includes the 10% overhead of Omnivore’s optimizer
during the run. The optimizer reduces momentum or learning
rate each time it runs. In the remainder of this section we
conduct detailed analysis of Omnivore, MXNet, and SINGA
to understand this improvement. As we will see, Omnivore’s
optimizer, which searches within the larger tradeoff space, is
the key reason for our system’s performance.

2) Single Machine Experiments: We validate our claim that
Omnivore’s performance is FLOPS-proportional, which means
it scales with available FLOPS, regardless of the number or
type of devices available. We ﬁrst explain the protocol of
our experiments. Then we report our results and discuss the
importance of FLOPS-proportional system performance.

Protocol: We compare Omnivore against Caffe and Ten-
sorﬂow on a single machine. We train CaffeNet on ImageNet
on hardware described in Figure 9. We measure the time each
system needs to ﬁnish 40 iterations of training (following
10 iterations of warm-up), using the standard batch size in
CaffeNet (256 images). Our single-machine optimizations only
affect hardware efﬁciency; the number of iterations needed for
convergence does not change. Hence, we can use time per
iteration as a surrogate for performance.

Results: Figure 11 shows the results. We normalize all
execution times by the slowest system in each column and
report the resulting speedups. We see that on a single CPU,
Omnivore is 3.9× faster than both Caffe and TensorFlow; on a
single GPU, all systems show similar speed. This is consistent
with our observation in Section III-B: TensorFlow and Caffe
use the same strategy for both CPU and GPU, which is optimal
for GPU but suboptimal for CPU. One interesting consequence
of this speedup result is that, although Caffe on 1xCPU is

that a fully synchronous strategy will be fast in terms of
hardware efﬁciency while having the best statistical efﬁciency.
Figure 12(a) shows the results. All systems reach accuracy
60% within 2 hours, and Omnivore reaches 60% the fastest.
Omnivore is 2.3× faster than MXNet (388 seconds vs. 907
seconds). At 3000 seconds, Omnivore already achieves an
accuracy > 99%, while MXNet achieves the same accuracy
after 7000 seconds. The speed up here is also 2.3×. As
expected, both systems chose a fully synchronous strategy, so
statistical efﬁciency is not the cause of this performance gap.
The 2.3× speed up is due to our CPU-based optimization
(Section III) and merging FC servers (Section V-A).10

(Small GPU Cluster: GPU-S) GPU-S is a small GPU
cluster that contains 9 4xGPU machines (36 GPUs). Because
each node is signiﬁcantly (7×) faster than 1xCPU, we expect
the optimal strategy to be more asynchronous, and thus,
statistical efﬁciency to come into play. Figure 12(b) shows the
result.11 Similar to the CPU-S cluster, Omnivore outperforms
MXNet: it reaches 99% accuracy 4.8× faster. MXNet only
supports completely synchronous or asynchronous execution,
and its optimal run uses the completely synchronous strategy.
On the other hand, Omnivore’s optimizer chooses to run with
two compute groups. Had Omnivore chosen the same strategy
as MXNet, it would be 1.7× slower than Omnivore’s actual
choice due to a different choice of the synchronization strategy.
The remainder of the 4.8× gap is due to the physical mapping
(merging FC) used by Omnivore, and this improves both
hardware and statistical efﬁciency: while originally described
as a mechanism to reduce network communication [5], we ﬁnd
that merged FC also improves statistical efﬁciency by reducing
staleness in the FC model.

(Large CPU Cluster: CPU-L) CPU-L is a CPU cluster
with 33 1xCPU machines. Because the number of machines
is large, we expect the synchronization across all machines
would incur a large penalty in terms of hardware efﬁciency—
thus, we expect the optimal strategy to be more asynchronous.
Figure 12(c) shows the result. We see that Omnivore is 3.2×
faster than MXNet to reach 99% accuracy. The best MXNet
strategy was to train completely synchronously; Omnivore’s
optimizer now chose four compute groups. Had Omnivore
it would incur 5×
chosen the same strategy as MXNet,
overhead for hardware efﬁciency but only gain a 2× beneﬁt
for statistical efﬁciency. Also, had Omnivore simply chosen a
fully asynchronous conﬁguration, it would be 3× slower. This
shows the importance of choosing the right number of groups
to balance statistical and hardware efﬁciency.

Impact of Optimizer: In the experiments above, we use
grid search to ﬁnd the optimal strategy for both MXNet and
SINGA. On the other hand, Omnivore relies on the optimizer
to automatically choose the best strategy. Had we not used
the grid search for MXNet and relied on default parameters
the performance gap would be 20× on ImageNet8. This

10 SINGA does not converge to 99% in 2 hours and Omnivore is 11× faster than

Fig. 12. Comparison of Omnivore with MXNet and SINGA on ImageNet8.
We omit the SINGA for CPU-L because it performs worse than MXNet.

7× slower than on 1xGPU, Omnivore is only 1.8× slower
on 1xCPU, which we will see matches the FLOPS ratio of
these devices. Omnivore’s FLOPS-scaling extends to multiple
devices, and the gap with other systems increases for more
CPU sockets (2xCPU) or GPU cards (4xGPU).9

FLOPS Proportionality: The training performance of
CPUs is commonly believed to be an order of magnitude
slower than GPU performance. Literature often reports this,
and we showed that it is the case for Caffe and TensorFlow
on 1xCPU and 1xGPU. We validate that Omnivore delivers
performance proportional to the FLOPS that a device can
provide. As shown in Figure 9, 1xGPU provides 1.7× more
FLOPS than 1xCPU, and Omnivore has a 1.8× gap between
1xCPU and 1xGPU. In other words, regardless of the type of
device, Omnivore performs proportionally to the number of
FLOPS available. We also observe that proportionality holds
for all machines in Table 9. FLOPS-proportionality means that,
using both CPUs and GPUs on the same machine, we should
be able to construct an even faster system. We validate this
by using both CPUs and GPUs on 4xGPU, whose CPU and
GPUs provide 0.67 TFLOPS and 4.89 TFLOPS, respectively.
By using data parallelism across the CPU and a single GPU,
Omnivore achieves an 18% speedup over just using the GPU.
3) Distributed Experiments: We conduct experiments to
understand our end-to-end improvement across three different
clusters described in Figure 9. As we will show, our tradeoff
characterization leads to the performance gains of Omnivore.
We ﬁrst describe the settings and the performance metric
used in the experiments. Then we discuss the optimizer’s
contribution and analyze its decisions across different clusters.
Protocol: We tune Omnivore, MXNet, and SINGA and
run them to convergence under multiple settings. Thus, we use
ImageNet8 that contains the ﬁrst eight classes of ImageNet.
This allows us to grid search all parameters in MXNet and
SINGA, including synchronization strategies and learning rate,
and pick the best run. We do not include the time of our
optimizer, which takes signiﬁcantly less time compared with
the grid search we did for MXNet and SINGA. We run all
systems for 2 hours and measure the training accuracy at a
given time. Figure 12 shows the results.

Results:

(Small CPU Cluster: CPU-S) CPU-S is a
small CPU cluster that contains 9 1xCPU machines. Because
each machine is slow and the network is fast, we expect

9We also run experiments on a 4-socket, 56-core Haswell CPU machine, and Omnivore

11 As of the time of writting, SINGA does not support GPUs and is omitted from

is 13× faster than Caffe.

SINGA to reach 60% accuracy.

Figure 12(b).

is compared to MXNet’s completely asynchronous strategy,
which is recommended in their performance tuning guideline
for networks like AlexNet.

Comparison across Clusters: Omnivore’s optimizer
makes different choices on different clusters. It is interesting
to compare them. As we can see, given the same amount of
machines (CPU-S vs. GPU-S), as devices get faster, Omnivore
tends to choose more asynchronous strategies. Intuitively,
the faster compute nodes get, the easier for the network to
get congested. The fully synchronous approach incurs higher
penalty in that case. On the other hand, given the same
speed of each compute node (CPU-S vs. CPU-L), when the
number of machines gets larger, Omnivore also tends to choose
a strategy that is between a fully synchronous and a fully
asynchronous strategy: (1) when the staleness gets very large,
even a properly tuned, fully asynchronous strategy incurs a
penalty in terms of statistical efﬁciency, and (2) when the
number of machines that need to be synchronized gets larger,
a fully synchronous strategy incurs a penalty in terms of
hardware efﬁciency. Omnivore’s optimizer makes it possible
for us to be robust across different devices and cluster sizes.

C. Tradeoff and Optimizer of Omnivore

We validate the hypothesis that (1) the tradeoffs studied
in this paper and (2) the automatic optimizer have a signiﬁ-
cant impact on the performance of Omnivore. We study the
importance of compute groups, as well as compute-group-
speciﬁc momentum tuning. We also study the effectiveness
of Omnivore’s automatic optimizer for this tradeoff space by
comparing it against a standard Bayesian optimizer.

1) The Tradeoff Space: In this section we demonstrate that
the various dimensions of the tradeoff space have a signiﬁcant
impact on performance. Throughout this work we already
illustrated some of these tradeoffs, so here we only summarize
them and leave the detailed discussion for the appendix. We
see that tuning the learning rate is necessary for convergence
and the physical mapping has both HE and SE beneﬁts. We
also showed that using the optimal number of compute groups
can yield 6.7× speedups compared to fully synchronous and
1.8× compared to fully asynchronous execution. This holds on
a range of datasets and clusters. We also implement compute
groups within TensorFlow and demonstrate the tradeoffs for
the Inception-v3 network in the appendix. We now focus
on the importance of properly tuned momentum, which as
we showed in Section IV-C is a function of the level of
asynchrony.

Importance of Momentum Tuning: We validate that the
correct value of momentum depends on the number of groups.
We expect that properly tuned momentum would outperform
a momentum tuned agnostically to the number of compute
groups. Therefore, we compare different methods of momen-
tum tuning on the optimal number groups for ImageNet8
on CPU-L, which was 4 (recall Figure 12(c)). We ﬁx that
number of groups and (i) set momentum to 0.9 (as reported in
AlexNet [2]); (ii) use the momentum tuned for a synchronous
system; (iii) tune the momentum using Omnivore’s optimizer

Fig. 13. Lesion study of momentum. Default momentum = 0.9, which is also
the optimal momentum for the fully synchronous strategy.

for 4 compute groups. As we see in Figure 13, tuning for the
right amount of asynchrony is important: if Omnivore did not
tune momentum, it would be 1.5× slower. Further experiments
show that tuning momentum can yield speedups of 2×. On
TensorFlow, we observe similar speedups: When momentum
is set to 0.9, synchronous training is faster. When, however,
we perform momentum tuning, the asynchronous conﬁguration
wins with its performance relative to sync improved by a factor
of 2.4× This both veriﬁes our expectations for this experiment
and provides further support for our theory in Section IV-C.
Discussion: Other Models: We ﬁnd that these tradeoffs
are impactful when applied to other models. We ﬁnd that for
Recurrent Neural Network models and LSTM models (e.g.,
the same choices affect performance—for example,
[30]),
choosing a completely synchronous or asynchronous conﬁgu-
ration can be up to 2× slower than the optimal conﬁguration.
This could imply speedups for applications in which RNNs
are widely used, such as handwriting, speech recognition, and
general sequence or time-series data.

2) Optimizer: We validate that our optimizer outperforms
state-of-the-art Bayesian optimization algorithms. We compare
our optimizer with the optimizer proposed by Snoek et al. [18].
We measure both the number of conﬁgurations and the total
number of epochs that the Bayesian optimizer needs to achieve
an accuracy within 1% of the highest accuracy Omnivore
achieves. In our experiments, the Bayesian optimizer never
discovers a conﬁguration which outperforms the conﬁguration
Omnivore obtains by grid search. We found that the Bayesian
optimizer takes on average 12 runs to ﬁnd a near-optimal
strategy, which on average is 6× more epochs than just
running that strategy. Because of this search overhead it was
not feasible to use the full ImageNet 1000 dataset so we used
ImageNet8 (whereas recall from Figure 10 that Omnivore had
an overhead of only 10% on ImageNet 1000). We used the
GPU-S cluster. Typically Bayesian optimizers can amortize
this cost by running in parallel, but here that is not possible
as the parameters depend on the hardware conﬁguration and
so the optimizer needs complete access to the entire cluster.

VII. RELATED WORK
Single Node. Optimizing CNN performance has become a
well-studied problem in recent years. Popular libraries include
Caffe [21], cuDNN [19], TensorFlow [22], Theano [23], and
Torch. To compute convolutions, many of these frameworks
use lowering, an idea proposed by Chellapilla et al. [31] that
takes advantage of highly-optimized BLAS libraries. Our work

follows from this line of research and demonstrates how to
optimize lowering for CPUs in order to build a system which
is robust to different types of hardware.

Distributed Deep Learning. Distributed systems for Deep
Learning popular, with SINGA [7], MXNET [6], FireCaffe [9],
SparkNet [8], DL4J, DistBelief [3], and Project Adam [5]
selecting different execution strategies and other optimizations.
Our study shows a combined tradeoff space on the union of all
these techniques. We did this by decoupling the hardware and
statistical efﬁciency for each technique and optimizing them
separately. Our work is the ﬁrst to provide a theoretical char-
acterization for statistical efﬁciency and to show that hyper-
parameters need to be tuned to compensate for asynchrony.

The idea of decoupling the number of iterations and time
per iteration and analyzing each separately is not new for
distributed CNN systems. MXNet reported hardware efﬁciency
and statistical efﬁciency separately. SINGA went deeper into
the tradeoff, identifying the compute group size as a tunable
parameter. They advocate combining both synchronous and
asynchronous training and offer a ﬂexible training architecture
which enables trading off the convergence rate with the time
per iteration in order to to minimize training time. However,
while SINGA identiﬁes this tradeoff and provides experimen-
tal evidence of its importance similar to the curves we showed,
the user still needs to manually choose a conﬁguration.

SparkNet also separated the time per iteration and number
of iterations by building models of each. They did not explore
the same tradeoff of machines per group, but rather a similar
tradeoff related to staleness. Because SparkNet uses a MapRe-
duce framework they implement model averaging. Within this
technique they explore, in isolation, how the staleness (their
τ parameter) impacts the number of iterations to convergence
and the time per iteration. Their hardware efﬁciency model was
measured (both network speed and compute time) and their
statistical efﬁciency model was also empirical (they varied
staleness and measured the statistical efﬁciency penalty).

[5] T. Chilimbi et al., “Project adam: Building an efﬁcient and scalable deep

learning training system,” in OSDI, 2014.

[6] T. Chen et al., “Mxnet: A ﬂexible and efﬁcient machine learning library

for heterogeneous distributed systems,” arXiv, 2015.

[7] W. Wang et al., “SINGA: A distributed system for deep learning,” NUS

Tech Report, Tech. Rep., 2015.

[8] P. Moritz et al., “SparkNet: Training deep networks in Spark,” arXiv,

2015.

[9] F. N. Iandola et al., “FireCaffe: near-linear acceleration of deep neural

network training on compute clusters,” arXiv, 2015.

[10] Abadi et al., “Tensorﬂow: A system for large-scale machine learning,”

arXiv preprint arXiv:1605.08695, 2016.

[11] H. Cui et al., “Geeps: Scalable deep learning on distributed gpus with
a gpu-specialized parameter server,” in Proc. of the Eleventh European
Conference on Computer Systems. ACM, 2016, p. 4.

[12] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz, “Revisiting distributed

synchronous sgd,” arXiv preprint arXiv:1604.00981v2, 2016.

[13] A. Candel, V. Parmar, E. LeDell, and A. Arora, “Deep learning with

h2o,” 2015.

[14] X. Lian et al., “Asynchronous parallel stochastic gradient for nonconvex

optimization,” in NIPS, 2015.

[15] I. Sutskever et al., “On the importance of initialization and momentum

in deep learning,” in ICML, 2013.

[16] S. Hadjis et al., “Caffe con Troll: Shallow ideas to speed up deep

learning,” in DanaC, 2015.

[17] I. Mitliagkas et al., “Asynchrony begets momentum, with an application

to deep learning,” arXiv:1605.09774, 2016.

[18] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimiza-

tion of machine learning algorithms,” in NIPS, 2012.

[19] S. Chetlur et al., “cuDNN: Efﬁcient Primitives for Deep Learning,”

[20] K. He et al., “Deep residual learning for image recognition,” in CVPR,

[21] Y. Jia et al., “Caffe: Convolutional architecture for fast feature embed-

ArXiv, 2014.

2016.

ding,” ArXiv, 2014.

[22] Abadi et al., “TensorFlow: Large-scale machine learning on heteroge-

neous distributed systems,” arXiv, 2015.

[23] J. Bergstra et al., “Theano: a CPU and GPU math expression compiler,”

VIII. CONCLUSIONS

in SciPy, 2010.

We described the ﬁrst explicit study of the tradeoff space
for deep learning systems, a popular, high-value type of
industrially deployed learning systems. We identiﬁed critical
issues in how one maps layers to devices and are the ﬁrst to
systematically study widely used techniques like asynchrony.
We designed a new optimizer and showed that it has excellent
end-to-end performance and is independent of our particular
implementation substructure. We are collaborating with a
major chip manufacturer to apply asynchrony-aware tuning
and compute groups onto new platforms of much larger scale.

[24] N. Vasilache et al., “Fast Convolutional Nets With fbfft: A GPU

Performance Evaluation,” ArXiv, 2014.

[25] F. Niu et al., “Hogwild!: A lock-free approach to parallelizing stochastic

gradient descent,” in NIPS, 2011.

[26] D. Gross, Fundamentals of queueing theory. John Wiley & Sons, 2008.

[27] A. E. Raftery, S. Lewis et al., “How many iterations in the gibbs

sampler,” Bayesian statistics, 1992.

[28] J. Deng et al., “ImageNet: A large-scale hierarchical image database,”

in CVPR, 2009.

[29] L. Bottou, “Stochastic gradient descent tricks.” in Neural Networks:

Tricks of the Trade (2nd ed.). Springer, 2012.

REFERENCES

[30] A. Graves, “Generating sequences with recurrent neural networks,”

[1] L. Deng and D. Yu, “Deep learning: Methods and applications,” FTSP,

arXiv, 2013.

[2] A. Krizhevsky et al., “ImageNet classiﬁcation with deep convolutional

for document processing.” ICFHR, 2006.

neural networks,” in NIPS, 2012.

[3] J. Dean et al., “Large scale distributed deep networks,” in NIPS, 2012.
[4] X. Zhang and Y. LeCun, “Text Understanding from Scratch,” ArXiv,

[32] “Inception in TensorFlow,” https://github.com/tensorﬂow/models/tree/

51238b1b5219a37ba145915efa764cca870e0d9f/inception.

[31] K. Chellapilla et al., “High performance convolutional neural networks

2014.

2015.

IX. ACKNOWLEDGEMENTS

The authors would like to thank Chris Aberger and the
rest of the Hazy Group for their feedback and help, as
well as HyoukJoong Lee, Nadathur Rajagopalan Satish, Peter
Bailis and Benjamin Recht for their thoughtful comments. We
would like to thank Intel, Toshiba and the Moore Foundation
for support along with DARPA through MEMEX (FA8750-
14-2-0240), SIMPLEX (N66001-15-C-4043), and XDATA
(FA8750-12-2-0335) programs, and the Ofﬁce of Naval Re-
search (N000141210041 and N000141310129). Any opinions,
ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reﬂect the views of DARPA, ONR, or the U.S. government.

APPENDIX

The appendix sections mirror the section structure of the

main paper, carrying corresponding supplementary material.

Appendix A includes supplementary information for the
Introduction, most importantly more discussion on the CPU
vs GPU debate.

Appendix B includes a discussion of CNN trends.
Appendix C includes a full

tradeoff analysis on lower-
ing strategies—ﬁrst reported in a workshop paper [16]—that
shows that the strategy we use in this paper works best for
most CNN kernels. It also includes a more comprehensive
treatment of batching and data parallelism for the GPU.
Finally,
it shows that FLOPS-proportionality holds for all
machines in Table 9.

Appendix D includes a full discussion of the tradeoff space,
including terminology, a survey (Appendix D-C) of many
diverse distributed CNN systems [3], [5]–[9] and shows that
they can be mapped to points within our trade-off space.
Appendix D-D includes a proof for the hardware efﬁciency
model we use, describes how to measure necessary quantities
from the system, and shows that it works across a range of
datasets.

Appendix E gives more details on our distributed optimizer:
Appendix E-A includes discussion and experiments on select-
ing the batch size; Appendix E-B describes other physical
mappings we studied and related analysis; Appendix E-D
discusses in more detail the cold-start period of optimization.
Appendix F includes the full setup of our distributed ex-
periments: Appendix F-B contains full details for our small
cluster experiments; Appendix F-C contains a full trade-off
space analysis for ImageNet and CIFAR10; Appendix F-C4
shows that tuning momentum can yield speedups of 2×; Ap-
pendices F-C4 and F-D contain full details on our larger cluster
experiments and discuss the impact of optimizing the hyper-
parameters of competitor systems; Appendix F-E includes
details on the end-to-end experiment on ImageNet1000, as
well as the choices of our optimizer. Appendix F-F describes
preliminary experiments on RNNs; Appendix F-G compares
our optimizer to standard schedules and; Appendix F-H com-
pares to Bayesian hyper-parameter optimization.

TABLE I
TRADEOFF SPACE WHEN DESIGNING DISTRIBUTED DEEP LEARNING
SYSTEMS.

Tradeoff
Single Node Hardware
Type of Parallelism

Batch size

Batch allocation
within node

Batch allocation
across nodes

Combining Model
Replicas

Server Architecture

Network Architecture

Hyperparameter
Optimization

Some Examples
CPUs, GPUs, both together
Model, Data
Large (few accurate updates),
Small (many parallel updates)
1 CPU or GPU per batch,
many CPUs or GPUs per batch
1 batch across all machines,
1 parallel batch per machine,
groups of machines per batch
Atomic gradient updates,
Model averaging, Ensembles,
Race conditions (Hogwild!)
Separate parameter/compute,
Merged parameter/compute
Model Size (impacts
communication),
Model Depth (impacts memory
/ batch size)

Grid/Random search, Bayesian,
Plateau (e.g., ResNet), Decay
Schedule (e.g., Inception-v4)

Optimization Algorithm SGD, Adagrad, momentum

Appendix G, includes our TensorFlow results. We show that
on Inception-v3, momentum tuning can be result-changing:
when momentum is set to 0.9, synchronous training is faster.
When, however, we perform momentum tuning, the asyn-
chronous conﬁguration wins with its performance relative to
sync improved by a factor of 2.4×. We also study the effect
of compute groups.

Appendix H gives a total cost of ownership analysis.
Appendix I includes supplementary discussion from the

Conclusions section.

APPENDIX A
APPENDIX FOR INTRODUCTION (SECTION I)

Tradeoff Table: While many distributed deep learning
systems exist, each of these makes design decisions suited
for a particular type of (1) compute cluster, (2) deep learning
model and (3) dataset, although these same decisions may
not work for other problem settings or hardware resources.
This is because deep learning is known to be complex both
from a computational and machine learning perspective, and
designing a highly efﬁcient and scalable deep learning engine
involves a number of interrelated design decisions. Table I
shows a number of factors to consider when designing dis-
tributed deep learning systems. Given a ﬁxed number of
machines, a number of tradeoffs exist from how to use
hardware on each node, to how to allocate batches of data to
machines, to the type of deep learning model to use in order to
minimize communication. In addition, these decisions impact
one another – for instance the batch size inﬂuences the number
of machines which can be used to effectively parallelize within
a batch and therefore inﬂuences the total number of parallel
gradients being computed to update the model. Our work,
which is a study, demystiﬁes these factors by identifying

the key tradeoffs which underlie all design decisions and
quantifying the impact of those tradeoffs experimentally.

Contribution 1: Single-Node Optimization: Even focusing
on just a single node, there has been a long debate about
CPUs vs GPUs for deep learning. GPUs are popular for CNN
systems because of the high throughput they provide. Modern
GPUs offer between 1.2 TFLOPS (NVIDIA GRID K520, per
GPU) and 8 TFLOPS (NVIDIA Titan Z). However, GPUs
contain smaller off-chip memories than CPUs, and GPUs are
connected to host memory by a slow PCI-e interconnect.
On the other hand, Microsoft’s Project Adam argues that
CPUs can deliver more cost-effective performance [5].12 This
debate is only going to get more interesting, as modern
GPUs offer high-speed interconnect with host memory13 while
Intel’s current Haswell CPU can achieve 1.4 TFLOPS on a
single chip.14. Moreover, SIMD parallelism has doubled in
each of the last four Intel CPU generations and is likely to
continue.15 Our work is the ﬁrst to conduct a systematic study
to understand the relative performance of CPUs and GPUs for
deep learning.

APPENDIX B
APPENDIX FOR BACKGROUND (SECTION II)

A. CNN Computation

AlexNet FLOPS: We approximate the FLOPs (# ﬂoating
point operations) in AlexNet by the sum of all the GEMM
operations with batch size 256. Speciﬁcally, we add 1 GEMM
in the forward pass for each Conv and FC layer, plus two
GEMMs in the backward pass for each Conv and FC layer
(although Conv1 backward has only 1 GEMM because no
gradient is needed with respect to the data).

Terminology: This section introduces model and data
parallelism as two techniques to parallelize CNNs. For a
full description of these concepts, see the Terminology in
Appendix D-B.

CNN Trends: This section viewed CNNs as two phases,
Conv and FC. Recent CNNs, e.g., Residual Networks
(ResNets) and Inception Networks, can also be categorized
into this partitioning. For instance early Inception variants
contained multiple FC layers at different parts of the network,
but from a computational point of view these are all considered
to be part of the FC phase.

In particular, CNNs have undergone a number of changes

in the past few years16. We summarize a few here:

• Multiple FC layers replaced with average pooling, leaving
a single fully-connected layer (for the softmax). This
leads to a reduction in the overall model size (e.g., 60
million parameters for AlexNet compared to 4 million
for GoogleNet)

12http://www.wired.com/2014/07/microsoft-adam/
13http://nvidianews.nvidia.com/news/nvidia-launches-world-s-ﬁrst-high-speed-gpu-

interconnect-helping-pave-the-way-to-exascale-computing
14Xeon E5-2698 v3, http://ark.intel.com/products/81060
15SIMD scales linearly in power and area (whereas frequency scaling is cubic) http:

//parasol.tamu.edu/lcpc2014/keynote-tian.pdf.

16http://cs231n.github.io/convolutional-networks/#case

• Increase in network depth (e.g., AlexNet with 5 conv
layers, compared to ResNets17 with > 150). This in-
creases memory requirements of networks, and makes
multi-device training necessary.

As we will see,

the optimizer presented in this paper
considers the impacts of each of these points when making
decisions for physical mapping and execution strategy.

B. Problem Deﬁnition

Scope of Work: Our work does not focus on improving
machine learning techniques but rather studies systems trade-
offs in order to build an optimizer that is robust to the most
widely used networks/algorithms. Our study uses the SGD
algorithm due to its popularity, although our optimizer applies
to other algorithms as well. Similarly we do not modify the
CNN architecture but assume that this is provided by the user.
Terminology: The physical mapping maps the layer
computation to vertices in G. Vertices in G may contain
other vertices, e.g., GPUs or CPU cores within a machine.
Section III ﬁrst studies how to map the CNN to hardware
within a machine, and concludes that with proper optimization
only the throughput of each vertex matters, not its underlying
hardware. Section IV then studies how to map the CNN to all
of G, i.e. across machines.

APPENDIX C
APPENDIX FOR SINGLE-NODE TRADEOFFS (SECTION III)
This section describes the optimizer’s physical plan (how to
map the CNN to hardware) at the level of a single machine.
Given a machine containing devices of various throughput
(CPUs, GPUs), our goal is to run the CNN computation as
quickly as possible. We do this by identifying two tradeoffs.
Our ﬁrst tradeoff introduces a data batching technique which
trades off memory footprint for compute time. We demonstrate
that this tradeoff gives a > 5× CPU speedup over existing sys-
tems. With this optimization now both CPUs and GPUs give
throughput proportional to the FLOPS offered by the device,
and our second tradeoff partitions the CNN computation across
both the CPU and GPU to combine their FLOPS – a known
HPC trick, but one which has not been applied to CNNs.

A. Convolutional Layer Computation

A 3D convolution consumes a pair of order 3 tensors–the
data D ∈ Rn×n×din and the kernel K ∈ Rk×k×din . For
example, if D is a color image with 3 (RGB) color channels,
din = 3. In AlexNet [2], n ∈ [13, 227], k ∈ [3, 11], and
din ∈ [3, 384], The output is a 2D matrix R ∈ Rm×m where
m = n − k + 1 and each element Rx,y is deﬁned as:

Rx,y =

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48)Kx(cid:48),y(cid:48),d(cid:48)

(7)

The kernel also supports parameters called padding and stride,
which affect the size of m. For details on stride and padding
see http://cs231n.github.io/convolutional-networks/#conv.

17https://github.com/KaimingHe/deep-residual-networks

This is the standard image 3D discrete convolution. A
convolution layer in the CNN contains a number of kernels
{Kj}, not just one, where we call dout = |Kj| the number of
output channels. These kernels {Kj} constitute the model of
the convolutional layer, and the reason for computing multiple
kernels rather than just 1 in the convolutional layer is to a
more powerful machine learning model. The convolutional
layer takes as input the 3D data tensor D and performs dout
3D convolutions, one per {Kj}, such that the output of the
convolutional layer is now not a 2D matrix R but a 3D tensor
R ∈ Rm×m×dout . Similarly the model of the CNN can be
viewed as a 4D tensor K ∈ Rk×k×din×dout.

Finally, recall that often rather than process a single data
example the CNN processes a batch of b examples simul-
taneously. The motivation for doing this is that gradient
computations during learning are less noisy. Therefore in the
most general case, the input D to a convolutional layer is not
1 but b 3D data tensors, or equivalently a 4D data tensor D ∈
Rn×n×din×b. The model is unchanged, but the convolutional
layer now performs the dout 3D convolutions on each example
in the batch, i.e. the batched convolution layer performs b·dout
3D convolutions. The output of the convolutional layer is
therefore also a 4D tensor, R ∈ Rm×m×dout×b.

To summarize, a convolutional layer accepts as input a 4D
data tensor D ∈ Rn×n×d×b, performs a total of b·dout 3D dis-
crete convolutions using D and its model K ∈ Rk×k×din×dout,
and outputs a 4D output data tensor R ∈ Rm×m×dout×b. The
full formula is:

Rx,y,z,w =

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48),wKx(cid:48),y(cid:48),d(cid:48),z

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Many implementations of this convolutional

layer exist.
Like most other HPC kernels, a straightforward implemen-
tation of this operation is suboptimal. Optimized implementa-
tions include directly computing the convolutions, as in cuda-
convnet2,18, computing the convolution as a discrete Fourier
transform as in [24], or implementing the convolution as a
matrix multiply, as in Caffe [21] or cuDNN [19].

While the studies in these papers conclude that different
strategies perform best for different kernel sizes, cuDNN [19]
demonstrates that the third technique of performing convolu-
tion as a matrix multiplication is versatile and fast for a range
of size parameters, as matrix multiplication kernels are often
highly optimized.

In order for the convolution to be carried out as a matrix
multiplication, an initial reformatting phase called lowering is
required to put the data and kernel into the correct format,
which we discuss in the next subsection of this appendix.

1) Convolution by Lowering and GEMM: Lowering fol-
lowed by a general dense matrix multiplication (GEMM) is a
popular way to implement the convolution operation. Figure 2
shows the three logical steps in the lowering process: (1)
lowering, which transforms 4D tensors D and K into 2D
matrices ˆD and ˆK; (2) matrix multiply (GEMM), in which

18https://code.google.com/p/cuda-convnet2/

we multiply ˆD ˆK to get the result ˆR; and (3) lifting, which
transforms ˆR back to a tensor representation of R.

Lowering Phase in which we construct the matrix
ˆD and ˆK. A value of D will appear more than once
in the lowered matrices.
Multiply Phase (GEMM) in which we multiply ˆD
and ˆK to create ˆR = ˆD ˆK.
Lifting Phase in which we map ˆR back to R.
Three techniques exist to perform this process, each corre-
sponding to a different way to group the sum in Equation 7.
Each of these techniques requires replicating the data or the
kernel in order to allow the convolution to be implemented
as a GEMM, but the amount of replication and the size of
the GEMM depend on whether the replication happens in the
lowering phase, lifting phase, or partly in each. The tradeoff is
studied in detail by [16], which concludes that the best choice
is determined entirely by the ratio din/dout (the ratio of the
number of input channels to the number of output channels
of the convolution), and that for modern CNNs these ratios
suggest that the data replication should be done during the
lowering phase. Therefore in the lowering used by this work,
there are two components to convolution: lowering (which
requires replication of data), and the GEMM. The lifting
does not require any memory copies or computation in the
optimal technique. Note also that in this technique, the kernel
does not require any replication, only the data. CNNs are
continuously evolving however, and so it is possible that future
CNN architectures will beneﬁt from other lowering strategies.
For a full study of the lowering tradeoff, refer to [16].

The amount of data replication required by the lowering in
this work is m2k2/n2, where m < n and m depends on the
stride and padding of the convolution. The replication can be
on the order of 1 to 2 orders of magnitude (i.e. 10 − 100×
more data). In turn, this blowup in the data size requires more
memory and computation in step 2 (GEMM). The beneﬁt
however is that a sparse computation has become dense,
which is important for hardware implementations because the
direct computation of the 3D convolution is usually memory
bandwidth-bound (due to the small convolution window size,
k ∈ [3, 11]). A GEMM implementation on the other hand,
while performing more computation as a result of lowering,
receives hardware acceleration which eclipses the increase in
data size.

B. Batching and Data Parallelism

1) Batching Analysis: Batching is the implementation
tradeoff that arises between the CPU and GPU as a result
of available off-chip memory. It concerns how many images
of the batch to process in parallel by the convolution layer.
Recall that bp images are processed in parallel, where where
1 ≤ bp ≤ b. b is the batch size, i.e. the total number of
images that need to be processed. The value of bp (how much
to batch the convolution computation) is determined by how
many lowered images can be stored in off-chip memory.

Modern GPU cards cannot store entire batches of lowered
data into off-chip memory and implementations of CNNs on

GPUs perform lowering and GEMM serially on one or few
images at a time until all b have been processed, i.e. bp =
1. On the CPU, off-chip memory is larger which allows for
batching techniques that perform lowering and GEMM on all
images in parallel and therefore allow CPU caches and vector
instructions to be fully utilized. This tradeoff is continuing
to evolve however, as newer GPU cards contain more off-
chip memory (e.g., 12 GB in the Titan X), and also use new
implementations which perform lowering and GEMM without
having to materialize the intermediate lowered representation,
as described by [19]. Therefore we believe that this tradeoff
will grow in importance.

For example bp = 1 in Caffe [21]: in order to process b
images in a batch, Caffe performs convolution on one image
at a time, i.e. lowering/GEMM is done serially for each image
in the batch (lower one image, run GEMM, lower the next
image, etc.) This has the smallest possible memory footprint
as it only maintains the lowered matrix of a single image in
memory, and is the preferred strategy for devices with limited
memory. Figure 4(c) showed that the memory footprint for the
convolution is directly proportional to bp.

Computationally however, Figure 4 (b) showed that bp = 1
suffers from lower hardware utilization. This ﬁgure was run
for the GEMM in the Conv2 layer of Alexnet (although we
observed similar trends for other Conv layers). Speciﬁcally
the matrix ˆD is “B” in the GEMM operation A × B, and
increasing bp increased the number of columns in ˆD.

In Figure 4 (b) we ﬁx the number of threads to 8, vary
bp, and plot the speedup normalized to bp = 1. Increasing
bp reduces the number of total GEMM calls, and this gives
a 2× overall speedup for bp = 256 compared to bp = 1.
This is because a small bp means that ˆD becomes thinner.
For thinner matrices, possible partition sizes of the underlying
GEMM algorithm are smaller and so the kernel cannot run
optimally, for example the L2 and L3 caches cannot be ﬁlled
during blocking optimizations. As a result bp = 1 is more
likely memory-bandwidth-bound than higher batch sizes (and
this phenomenon is likely more severe when the GEMM kernel
is executed with multiple threads.) Also note that Figure 4
was run on a CPU machine with 8 physical cores. Regarding
Figure 4 (a), the reason that 16 threads was slightly slower
than 8 is that we hit the memory bandwidth bottleneck.

For modern CPUs, memory is large and so b = bp. We use

b = bp for the remaining CPU experiments in this section.

2) Data Parallelism in non-GEMM Kernels: On the CPU,
recall that the batching technique above makes a single matrix
that
is b times (full batch size) larger than it would be
for a single image, and then performs a single GEMM on
this large matrix. A related strategy is to split a batch into
multiple partitions, and process each partition in parallel using
a separate thread.

For GEMM, processing an entire batch of size b with n
threads is equivalent to partitioning the batch into p partitions
of size b/p with n/p threads used in each GEMM. For
example, batching b images and performing a single GEMM
with 8 threads is equivalent to creating 8 matrices, each with

Fig. 14. The impact of data parallelism on the end-to-end execution time of
CaffeNet, run with 256 images per mini-batch on an Amazon EC2 c4.4xlarge
instance.

b/8 images, and performing 8 parallel GEMM kernels with 1
thread each. These are equivalent as this is exactly how BLAS
parallelizes GEMM: by partitioning partition columns of B in
A × B and allocating 1 thread per partition.

While partitioning and then performing the GEMM kernel
is the same as simply performing the GEMM kernel, this is
not true for other kernels which are not multi-threaded For
non-GEMM kernels such as lowering, or other layers such
as pooling, the second technique of partitioning the batch
and processing each partition using a separate thread gives
signiﬁcant speedups (this is simply data parallelism) across all
cores. For example, it can be used to lower images in parallel
using all cores by assigning a subset of the images in the batch
to each core.

Figure 14 shows the impact of data parallelism on a full
end-to-end CaffeNet on the EC2 c4.4xlarge instance with 8
physical cores (CPU only). The batch size used is 256 images
and the horizontal axis represents into how many parallel
partitions we partitioned these 256 images for each layer of the
CNN. I.e. the horizontal axis is the number of threads used
for data parallelism. Note that the GEMM kernel is always
parallelized (OpenBLAS was used) and uses the maximum
number of threads (16).

“None” indicates the default Caffe implementation. For all
layers, each image is processed serially. For example, one
image is lowered, then the convolution GEMM is performed,
and then the next image is lowered, etc. The only multi-
threaded kernels are the BLAS GEMM kernels used in the
convolution and FC.

“1” is identical to the Caffe implementation except that
lowering is ﬁrst done for all images (serially, i.e. one image
lowered at a time), and then a single, large GEMM is done
for the convolution. All other layers are the same.

For all other number of parallel partitions p, the 256 images
were equally split into p partitions. For example if p = 2, two
partitions of size 128 images each were created. Then, threads
processed each partition in parallel, one thread per partition.
For example if p = 2, lowering was done with two threads,
each lowering 128 images. Following the lowering, a GEMM

was performed for each partition. The total number of threads
used for these GEMM kernels was always 16, i.e. the GEMM
is performed on each partition with 16/p threads per GEMM.
Figure 14 (None vs 1) shows that batching the GEMM
kernels (one large GEMM as opposed to 256 smaller ones)
saves ∼ 2.2s of convolution time. Then, data parallelism
provides another ∼ 10 s of reduction. The ﬁnal time is then
4s, where ∼ 3s is spend in convolution layers. Therefore
the batching of the GEMM made the convolution roughly
∼ 2× faster (in the optimized execution), and the remaining
speedups were due to data parallelism.

Finally, studying more closely the speedups from data
parallelism, the time reduction from 14 seconds to 4 seconds
from data parallelism was roughly 80% due to speeding up
the lowering, and 20% due to speeding up the other layers.
I.e. the ﬁnal iteration time would be ∼ 6s if data parallelism
was only used for the Conv layers. The remaining 20% is
for data parallelism in pooling, normalization, dropout and
ReLU layers. Fully-Connected layers are simply a GEMM
which always uses 16 threads, so data parallelism and model
parallelism do not apply on the CPU, although as we also saw
previously this GEMM can be made slightly faster by using 8
threads instead for the FC layers (because there are 8 physical
cores).

Overall, batching combined with data parallelism gives
more than a 4× speedup end-to-end when running Caffe’s
AlexNet on 8 physical cores. Importantly,
this end-to-end
speed is now proportional to the peak throughput of the CPU.

In summary, we show two sources of speedup on the CPU.
First, by batching the lowering and GEMM, we perform a
single GEMM which is b× larger, as opposed to b smaller
GEMMs, which as described above has better hardware uti-
lization on the CPU. Second, we apply the batch partition (data
parallel) technique above to parallelize non-GEMM kernels
such as lowering. These optimizations are possible for the CPU
because it has more memory to store the lowered matrices.
As a result the CPU performance is proportional to the device
FLOPS, which allows partitioning computation across both the
CPU and GPU proportional to the device throughput.

C. Device Throughput

FLOPS Experiments: Figure 3 showed throughput for the
CNN when using Caffe, Omnivore and also for reference a
single-precision GEMM kernel. For the CPU the GEMM ex-
periment used OpenBLAS and matrices of size 16384×16384.
For the GPU GEMM we used a GEMM kernel from NVIDIA’s
CUDA examples. For Caffe and Omnivore we focus on only
the Convolution layers, speciﬁcally the time to run forwards
and backwards on all 5 layers of CaffeNet.

FLOPS calculations: The c4.4xlarge instance contains
a single-socket Haswell CPU with 8 physical cores. The
c4.4xlarge instance CPU FLOPS are calculated as: 8 physical
cores × 2.9 GHz ×32 = 0.742 TFLOPs, where 32 is the
single-precision Haswell instructions per cycle (8-ﬂoat SIMD
× 2 FMA per cycle, and FMA is fused-multiply-add).

Each g2.2xlarge instance provides a single Grid K520 GPU,
i.e. 1536 cores ×800 MHz = 1.23 TFLOPS (the Grid K520
contains a total of 3072 cores, 1536/GPU).

The c4.8xlarge instance contains a dual-socket Haswell
CPU with 18 physical cores. The FLOPS are calculated as:
18 physical cores × 2.9 GHz ×32 = 1.670 TFLOPs.

D. FLOPS-Proportional Scheduling

Given that both CPU and GPU speeds are now proportional
to the device FLOPS, we next consider whether the CPU and
GPU can be used simultaneously to process each layer. We
do this using data parallelism (batch is partitioned, model
is replicated) for all layers in the convolution phase, which
is compute-bound and has small data. The tradeoff is what
fraction of the batch to give to each device. We select the
simple but optimal choice that a device should process a
fraction p of the input where p is the proportion of total
FLOPS which that device contributes. e.g., if a CPU provides 1
TFLOPS and a GPU 4 TFLOPS, 1/5 of the batch is processed
by the CPU for an ideal speedup of 20% over the GPU alone.

E. Single-Node Experiments

Omnivore matches Caffe’s output on each layer. It accepts
the same input ﬁles as Caffe and produces the same outputs.
Our experiments compare against Caffe and use the CaffeNet19
CNN, which is Caffe’s implementation of the popular AlexNet
(the default architecture for benchmarking), as well as the
ImageNet dataset.

Both systems take as input the same network conﬁguration
ﬁles that Caffe provides. We remove grouping for convolution
layers because the full AlexNet ﬁts in the memory of a
single K520 (g2.2xlarge) GPU. We use the same external
libraries for both Caffe and Omnivore: GCC-4.8, CUDA-7.5,
and OpenBLAS. For Caffe we report both cuDNN v3 and v4.
We built the same timer into Caffe and Omnivore (measur-
ing wall-clock time, clock_gettime in C). We run for 50
iterations and omit the ﬁrst 10 in case there are disk or other
delays in the ﬁrst few iterations not representative of steady-
state execution. Beyond these ﬁrst 10 iterations we noticed
that all iterations were consistent in terms of time for both
tools and had a coefﬁcient of variation less than 5%.

We also ran Tensorﬂow using the same protocol as above

(40 iterations, burn-in of 10, identical network).

1) End-to-end Performance: Figure 15 shows the results
of running Omnivore and Caffe on various EC2 instances.
Given that Omnivore and Caffe generate the same outputs, we
concentrate on throughput. We ran both Omnivore and Caffe
on each EC2 instance for 50 iterations, and counted the last
40. On the c4.4xlarge CPU instance Omnivore outperforms
Caffe by nearly 4× due to batching and data parallelism. On
the g2.2xlarge GPU instance Omnivore and Caffe achieve the
same speed (within 5%). Note that the c4.4xlarge instance
offers 60% of the FLOPS of the g2.2xlarge instance, and
the ratio of Omnivore speeds on these instances is 59%, i.e.

19https://github.com/BVLC/caffe/tree/master/models/bvlc reference caffenet

×16 physical cores ×2.6 GHz). The ratio of CPU:GPU
FLOPS is therefore 1:2, i.e. we should partition roughly 1/3 of
the data on the CPU and 2/3 on the GPU. Since a batch size
is 256 images, we rounded 67% on the GPU to 75%, such
that the CPU processes 64 images, because this partition is
better suited to hardware (although we see only a 5% speedup
compared to using the exact ratio). This partitioning gives a
18% speedup over just using the GPU.

For parallelization across 4 GPUs, we use data parallelism
for all layers (each GPU given 1/4 of the batch and a model
replica) except for the FC layers, which use model parallelism
(each GPU given 1/4 of the model and a replica of the batch).
We ran Caffe on 4 GPUs with cuDNN v4, cuDNN v3 and
no cuDNN, and found that Caffe was fastest but neither gave
a speedup compared to 1 GPU.

state-of-the-art

Therefore while CNN parallelization is challenging even
for
systems, we’ve shown that FLOPS-
proportional partitioning is possible across a range of hardware
devices. We now extend this technique to multiple machines,
where the added challenge of network delay motivates re-
thinking the SGD algorithm.

Fig. 15. End-to-end performance comparison across EC2 machines on
CaffeNet. All numbers are normalized as the speedup over running Caffe’s
GPU version on g2.2xlarge instance ($0.65/hour).

APPENDIX D
APPENDIX FOR DISTRIBUTED TRADEOFFS (SECTION IV)

Omnivore delivers speed proportional to the FLOPS. On the
two-socket c4.8xlarge CPU instance Omnivore is now 5.5×
faster than Caffe. Caffe does not speed up given the additional
cores, but Omnivore does. The speedup is not linear because
the extra cores are distributed across sockets and not all layers
are compute-bound. However, these results show that given
similar device throughput, CPU CNN performance is not an
order of magnitude slower than GPU performance as the
literature often reports and as is the case in Caffe. Lastly we
compared Omnivore to Caffe on a 4-socket, 56-core Haswell
(non-EC2) CPU machine, and Omnivore is 13× faster than
Caffe.

FLOPS calculations: See Appendix C-C for the FLOPS

calculations in Figure 15.

2) CPU/GPU Hybrid and Multi-GPU: Figure 15 also
shows that using the CPU in a GPU instance can accelerate
purely GPU training. The g2.8xlarge instance’s CPU provides
0.67 TFLOPS, and by using data parallelism across the CPU
and a single GPU we achieve an 18% speedup in Omnivore
end-to-end over just using the GPU. This is faster than all
other single GPU results.

Finally, we also apply this data parallel partitioning across
multiple GPUs. We ran both Omnivore and Caffe using the 4
GPUs on the g2.8xlarge instance and show that while Caffe
actually slows down compared to the 1 GPU case, Omnivore
becomes 3.1× faster.

For CPU + GPU, each g2.8xlarge GPU provides 1.23
TFLOPS as shown above, and the CPU provides 665.6
TFLOPS (Sandy/Ivy bridge, i.e. 16 SP instructions per cycle

Having studied the tradeoffs for a single machine,

this
section now studies the distributed setting. The goal of this
section is to build an optimizer that creates (1) a physical plan
P (A, G) which maps the CNN architecture A to machines in
the device graph G, and (2) an execution plan E(G, D) which
parallelizes SGD by allocating data batches from the dataset
D to each machine. This section begins by describing why
these two are the most important tasks for the optimizer. While
many distributed CNN systems exist [3], [5]–[9] and each
describes their own distribution techniques, upon analyzing
these strategies we discover that,
they all
describe either (1) or (2) above. Given these two fundamental
design dimensions, we then arrange existing strategies into
a tradeoff space and restate our optimizer’s goal precisely
within this space. The remainder of the section then quantiﬁes
the impact of these tradeoffs to allow optimization within the
space.

though diverse,

A. Distributed CNN Tradeoff Space

This section describes popular techniques for distributed

CNN training and reﬁnes them into a tradeoff space.

1) Distributed Stochastic Gradient Descent: Recall

that
SGD iteration i (1) reads a batch of data Bi, (2) uses the
current model Mi to compute the model gradient ∇M (Mi),
and (3) subtracts ∇M (Mi) from Mi to obtain Mi+1. Iteration
i + 1 reads a new batch Bi+1 and the algorithm repeats until
convergence.

Figure 16 (a) shows a common distributed implementation
of SGD in which the entire CNN model is stored on a server
called a model server or parameter server. There are also
a number of compute servers which each perform the CNN

Fig. 16. SGD server architectures.

Fig. 17. A graphical illustration of (a) one worker (S=0) and (b) two workers
(S=1).

calculation (Equations 1 and 2). Each iteration, every compute
server reads M over the network from the parameter server,
as well as a batch of data B from a local database. Each
compute server calculates a gradient ∇M which is sent back
to the parameter server and used to update the model. In
this example, compute servers operate in parallel and do not
synchronize or communicate with one another.

In Figure 16 (a), each server physically maps to a single
machine (node). Generally, multiple servers can map to a
single machine or a single server to multiple machines. For
example, parameter and compute servers can map to the same
node to reduce network communication. Figure 16 (b) shows
a more complex server architecture for SGD in which rather
than two types of servers (compute and model), there are now
4 types: (1) conv compute, (2) conv model, (3) FC compute,
and (4) FC model. 1 and 2 are for layers in the conv phase
of the CNN while 3 and 4 are for layers in the FC phase.
In Figure 16 (b), the FC compute and model servers map
physically to the same machine, i.e. the computation of the
FC phase happens on the same machine as where the FC
model is stored. There are many beneﬁts to Figure 16 (b) vs.
(a): Recall that FC has small data, large model whereas conv
has large data, small model. This server architecture has the
beneﬁt of only needing to communicate the conv model (and
its gradients) and the FC data (and its gradients) across the
network. Second, computation is ofﬂoaded from the compute
machines to the FC model server, which otherwise is majorly
idle. These beneﬁts both improve hardware efﬁciency, and
were described in [5]. However, yet another beneﬁt is that
by having only a single FC compute machine, the FC model
does not experience any staleness – a term we deﬁne next.
This improves statistical efﬁciency.

2) Staleness: Staleness is a metric used to quantify the
statistical efﬁciency (# iterations to convergence). Figure 17
shows staleness graphically for the simple server architecture
of Figure 16 (a). In Figure 17 (a) there is a single worker,
and so that worker always computes the gradient using the
current model. In this diagram, we assume that once a worker
sends the updates back to the model server, it is immediately
sent a new model, i.e. in this diagram, write/read is an atomic
operation for each worker (the write to the model and read

from the model happen together).

In Figure 17 (b), there are now two concurrent workers.
Assume for now that (1) these workers update the model in
round-robin order, (2) write/read is again atomic, and (3) the
server architecture is again as in Figure 16 (a). We notice
that now each worker computes the gradient using a copy
of the model which is stale by 1 iteration, e.g., updating
model 2 to produce model 3, but doing so using a gradient
update which was calculated using model 1. The reason for
this staleness is that while worker 0 is computing its next
update, worker 1 updates the model. This staleness is bad
for statistical efﬁciency (i.e. more iterations are required to
converge) because now each gradient used to update the model
no longer points in the direction of steepest descent for the
model it is applied to, but rather the direction of steepest
direction for a model from an earlier iteration.

Precisely, we deﬁne staleness as follows: given N workers,
the staleness is the number of updates to the model between
a worker’s read from the model and subsequent write back to
the model. Because the updates are round-robin, this staleness
is the same for all workers, and is equal to S = N − 1.
e.g., if there are 100 parallel workers, S = 99. Intuitively,
the staleness is just equal to the number of parallel workers
sending gradient updates (minus one, although this can often
be ignored because the number of workers is large).

The three assumptions above are useful to give a precise

deﬁnition but are not necessary in practice.

Assumption 1:: In practice the workers do not proceed in
round-robin order due to inherent variance across machines,
but we observe empirically that for dense models the updates
are nearly round-robin. This is because dense model compu-
tations like those used in deep learning have roughly constant
time per iteration (this is not true for sparse models).

Assumption 2:: Writes and reads do not need to be
atomic, and in fact this can be beneﬁcial for statistical ef-
ﬁciency. Rather than have a conv compute server request an
entire updated model as soon as it publishes all of its gradients
to the conv model server, it may instead publish gradients
layer-by-layer in a backwards fashion during the backward
pass of iteration i, and then lazily request the updated model
in the forwards pass of the next iteration i + 1. For example,

conv compute worker processes half
the data of a single
batch, using the same model replica, and produces half of
the ﬁnal gradient. A barrier is then introduced in which the
gradients are summed, and a single, ﬁnal gradient is applied to
the model. Figure 18 (b) seemingly solves both problems: all
the hardware is being utilized (good for hardware efﬁciency),
and S = 0 (good for statistical efﬁciency). However the
price we pay is in hardware efﬁciency, speciﬁcally the cost
of synchronization across the machines. Indeed, we will show
that the hardware efﬁciency of Figure 18 (b) is poorer than
that of Figure 18 (a).

We deﬁne a compute group as a group of machines working
together to process a single batch of data. A compute group
is characterized by processing a single data batch at a time,
with all machines in the group using the same model replica,
to the model server. The
and returning a single gradient
compute groups in Figure 18 are shown with black dotted
lines. Figure 18 (a) has 2 compute groups, and since there are
2 machines, the compute group size is 1 machine. Figure 18
(b) has 1 compute group of size 2. Note that this allows us
to simplify our deﬁnition of staleness: because the number of
parallel gradients being computed in the system is equal to the
number of compute groups, the staleness is just equal to the
number of compute groups (minus one).

Generally, if we have N machines used as conv compute
servers, Figure 18 (a) and (b) show two extreme cases.
Figure 18 (a) is the extreme case of 1 machine per com-
pute group, and N groups. This technique is often called
asynchronous SGD, or “async” for short. In async, workers
do not communicate and each worker updates the model
independently. Every worker computes a separate gradient
using a separate batch and separate model replica, and then
sends these gradients to the parameter server in order to
update the model. Figure 18 (b) is the other extreme case
of 1 group, and all N machines in that single compute group.
This technique is often called synchronous SGD, or “sync”.
In sync, all machines work synchronously and in parallel on
a single batch of data and using a single model replica to
compute a single gradient. In this case the gradients over
all workers are aggregated each iteration (or batch) before
updating the model. An intermediate conﬁguration could also
exist, for example one which has 4 groups each of size N/4.
There could even be groups of different sizes if different
machines have different throughput (some GPUs, some CPUs,
etc.). Notice that because the compute group in Figure 18 (b)
parallelizes the conv phase, it uses data parallelism, i.e. the
batch is partitioned across the machines in the group.

4) Precise Problem Deﬁnition: This section has described
two key tradeoffs: (1) the server architecture, concerned with
physically mapping servers to machines, and (2) the execution
strategy (the number of compute groups vs. their size), con-
cerned with mapping batches to servers. We can now restate
the goals of the optimizer from II-D in terms of our tradeoff
space. Given (1) A, the CNN architecture, (2) D, the dataset,
and (3) G, the device graph, our optimizer transforms A into
S, an abstracted network of logical server types (Convmodel,

Fig. 18. Two different execution strategies given a ﬁxed machine budget.

AlexNet may update the model with gradients from conv5,
conv4, conv3, conv2, and conv1, and then begin its next
forwards pass and request conv1, conv2, conv3, conv4 and
conv5. These requests can happen asynchronously from the
computation to hide latency and overlap the network delays
with the computation. As a result the write/read for conv1 may
be almost atomic, but there would be some delay between
the write/read for conv5. This delay in fact reduces staleness
slightly because it reduces the number of intermediate writes
by other workers between a worker’s read and subsequent
write (intuitively, in the extreme case, if the delay was very
large, then every worker would write before any of them
read the new model. Then this is just equal to mini-batch,
except with a larger batch size, although that would of course
make each iteration slower, i.e. harm hardware efﬁciency). In
practice we observed a roughly 20% reduction in the number
of iterations to converge by requesting models in this lazy
fashion (which does not harm hardware efﬁciency).

Assumption 3:: Staleness also applies to the server ar-
chitecture of Figure 16 (b), which recall reduces network
communication by merging the FC model and FC compute
servers, i.e. mapping them to the same machine which does
both the gradient computation and model updates for the FC
phase. This merged FC server processes only one batch at a
time, and thus produces only one FC model gradient at a time.
This means that the staleness for the FC model is 0 which is
good for statistical efﬁciency. The conv compute servers on
the other hand still calculate the updates to the conv model
in parallel (once they receive their data gradients from the FC
machine), therefore the merged architecture in Figure 16 (b)
still contains staleness, but only for the conv model.

3) Compute Groups: The ﬁnal concept common to all
systems is the compute group. Consider the example of two
conv compute machines in Figure 18 (a). This conﬁguration
has a conv staleness of S = 1, as there are 2 workers
independently updating the model. However, it is not necessary
to introduce staleness in order for both compute machines to
be utilized, and Figure 18 (b) shows a second conﬁguration
in which data parallelism is used across the machines: each

of the model replica and 1 replica of the data batch. This is
useful for parallelizing fully-connected layers, which contain
small data but large models: each device receives 1/N of the
model and a replica of the data, and uses the data to compute
a gradient for that portion of the model. These gradients in
total combine to a single gradient with respect to that entire
model, using the data batch.

Note that logically, a single gradient is produced by all
devices together and that single gradient is used to update the
model and produce a new model (physically the updates may
occur locally on each device communication, i.e. the device
updates its portion of the model).

Data Parallelism: A single replica of the data batch and
N (identical) replicas of the model are created. Each device is
given 1/N of the data replica and 1 replica of the model. This
is useful for parallelizing convolutional layers, which contain
small models but large data: each device receives 1/N of the
data and a replica of the model, and together they calculate
a single gradient with respect to that entire model using the
data batch.

Note that, as mentioned above, here the model is always the
same for each device, i.e. the model is replicated and a single
gradient is produced. That gradient is then used to update the
model and the model is then re-broadcast to each device. So
while model replicas exist, they are identical model replicas.
(Note: model replica is a logical term and does not always
correspond to a physical replica. In particular, when two
devices share memory, e.g., 2 CPU threads running on 2 cores,
no physical replica is necessary as the same model in memory
can be read by both.)

2) Asynchronous Batches: The above deﬁnitions apply to
a single data batch in the system (although it may have been
replicated for the purpose of parallelization). Consequently,
there was always one gradient being computed at once, and
so each model replica was identical.

This deﬁnition focuses on the asynchronous case mentioned
previously, i.e. NB parallel batches in the system being used
to compute NB parallel gradients. Each of these NB batches
of data in the system is different. Each batch is allocated to
a compute group (a group of devices), and each batch (i.e.
each group of devices) is given a replica of the model. For
example if NB = N/4, then there will be 4 devices assigned
to each batch, and one model replica assigned to each of these
N/4 batches (i.e. each group of 4 devices). Each compute
group of 4 devices will then compute a gradient given that
model and that batch, and the devices in the group may do
so using either model or data parallelism (i.e. choosing to
create additional model or data replicas within the group, as
described above, e.g., if the group uses data parallelism it will
create 4 additional model replicas, but each of these models
will be identical).

Note that here the number of batches in the system, the
number of compute groups, and the number of parallel gradi-
ents being computed by the system are all the same number.
Within a compute group (group of devices), additional model
replicas will be the same. However across compute groups, the

Fig. 19. An Overview of our Mapping.

Convcompute, FCmodel, and FCcompute), and creates (1) a physical
plan P (S, G) mapping each server in S to machines in G (note
that we also refer to this distributed portion of the physical plan
as the server architecture), and (2) an execution plan E(S, D)
which deﬁnes the number of compute groups by allocating
batches of D to each server in S. Both of these choices impact
hardware and statistical efﬁciency, and our next task is to
quantify this impact. First, we present terminology and then
ﬁnish this section by describing where existing systems fall
within this tradeoff space.

Our mapping from CNN layers to devices is shown in

Figure 19.

We are given: (1) a network, which can be viewed as a
labeled directed acyclic graph in which each node is a type
of layer (e.g., convolution, fully connected, max pooling) and
edges indicate dataﬂow dependencies, and (2) a set of devices
grouped into machines. Our goal is to devise a mapping from
the network to the machines.

B. Terminology

In this work we interchangeably uses the terms node and
machine to refer to a single box, connected to other nodes
or machines over a network. We also interchangeably use the
terms device and (when describing a device graph) vertex to
refer to a discrete unit of compute hardware (e.g., a GPU or
CPU core. When discussing a cluster, a device can also be a
machine in that cluster.). Finally, we also use the terms group
and compute group interchangeably, as deﬁned in Section II.
Existing work also often contains a lot of terminology,
which we summarize here. Consider for these examples the
simple case of N identical compute devices (e.g., N machines,
N GPUs, N CPU cores, etc.)

1) Synchronous Batches: These ﬁrst two deﬁnitions apply
to a single batch of data, i.e. there are not multiple parallel
batches in the system used to compute asynchronous gradients,
but rather a single batch used to compute a single gradient.
As a result, this gradient is applied to the model at once to
produce a new model, i.e. there only ever a single model in
the system (although it may be replicated, in which case all
models are identical). This is also known as the synchronous
case above.

Model Parallelism: A single replica of the model and N
replicas of the data batch are created. Each device is given 1/N

model replicas may be out of sync, because of asynchronous
gradient computations. This is discussed in the ﬁnal deﬁnition.
To make all the deﬁnitions above concrete, consider the
example of 2 devices, e.g., 2 GPUs. There are 3 possible
scenarios: (a) 2 parallel asynchronous batches (“async”), (b) 1
batch with data parallelism across the 2 devices (“sync”), or
(c) 1 batch replicated twice with model parallelism across the
2 devices (also sync).

3) Combining Model Replicas: If there are G asynchronous
compute groups, each using a separate data batch and produc-
ing a separate gradient each iteration, then each group will
also have a separate version of the model as a result of the
asynchronous gradient computations.

then broadcast

Parameter server is a technique in which these separate
compute groups will not perform their model updates locally,
but rather each publish their gradients to a global parameter
server which will
to a
group upon receiving the update for that group. In this way
the groups always have models which are almost the same
(they will still be slightly out of sync because gradients are
published asynchronously and so models are returned to the
groups asynchronously as well). DistBelief, Adam, SINGA,
MXNet use this technique, and it is the focus of our study.

the updated model

Note that if the parameter server waits for each parallel
group to publish a gradient and then broadcasts the model
to all groups (i.e.
this is no longer
introduces a barrier),
asynchronous, and is now equivalent to the synchronous case
above (data parallelism) where the batch size is N times larger
than it is per individual compute group.

Finally, the updates to the server from parallel groups can
happen with or without race conditions. The case of race
conditions is known as Hogwild!.

Model Averaging is a technique in which there is also
a global parameter server, but model updates happen locally
within each group. Then periodically, every τ iterations, the
groups will publish not their gradients but their entire models
to the parameter server. The parameter server will average
the models (reduce step) and then broadcast them to each
group (map step). This averaging does not have theoreti-
cal guarantees because neural networks are non-convex, but
works in practice. This technique works well for map/reduce
frameworks, e.g., Spark/Hadoop, and is used by SparkNet and
DL4J. Here the models are more different than they are in the
parameter server case.

The choice of the τ parameter is similar to the tradeoff
of multiple groups of varying size, except that here staleness
comes not from multiple asynchronous workers updating a
single model, but multiple workers with a separate model
combining models. In the case where τ = 1, this is identical
to the synchronous case of parameter server (all machines in
a single group, i.e. all computing a single batch/gradient).

Ensembles are used by AlexNet. Here each group trains an
entirely separate model to convergence and then predictions
of these models are combined (e.g., through voting). The
gradients or models themselves are never combined.

TABLE II
POINTS IN THE DISTRIBUTED CNN TRADEOFF SPACE CHOSEN BY
POPULAR SYSTEMS. G IS THE NUMBER OF COMPUTE GROUPS, N IS THE
NUMBER OF MACHINES.

1<g<N

Model
Avg.

Merge
FC

sync
(g=1)
•

•
•
•

Tool

DistBelief [3]
Adam [5]
FireCaffe [9]
MXNet [6]
SINGA [7]
SparkNet [8]
DL4J

async
(g=N)
•
•

•
•

•
•

•

•

•
•

In this study we focus on the parameter server approach,
which is the most widely used by distributed CNN systems.

C. Existing Systems

Using the terminology above, we discuss design decisions
made by CNN systems in Table II. In our review of the
literature, these are the tradeoffs which we identiﬁed as most
impactful to minimizing convergence time.

Execution Strategy:

In terms of execution strategies,
Microsoft’s Project Adam [5] reports that the async strategy
is effective and uses one or a few (e.g., 4) machines per
compute group. Moreover, they report that using a technique
called Hogwild! [25], which introduces race conditions, is
an effective source of additional hardware speedups. On the
other end of the spectrum, FireCaffe [9] implements the sync
strategy and notices that high scalability can be achieved by
having all machines work synchronously and in parallel on a
single, large batch of data, and by reducing communication
using reduction trees. MXNet [6] implements both the sync
and async strategies, and allows the user to select which to
use. They also call the sync strategy the Bulk Synchronous
Parallel (BSP) protocol. Finally, Google DistBelief [3] and
Apache SINGA [7] implement both sync and async as well
as the compromise of multiple, larger compute groups. They
also call sync Sandblaster, and cases of more than one group
Downpour (e.g., Downpour with group size of 1 machine is
equivalent to the async). SINGA calls the intermediate case
of multiple, larger compute groups hybrid (SINGA also has
hybrid parallelism, which is different. That is a combination
of model and data parallelism.)

All these systems implement the Parameter Server tech-
nique, described above in the Terminology section (Ap-
pendix D-B). It is the most widely used technique by dis-
tributed CNN systems and the technique we focus on in
this study. Another technique known as model averaging,
which works well within a map/reduce framework, is used
by SparkNet [8] and DL4J (http://deeplearning4j.org/). Model
averaging is also described above. The key difference between
model averaging and parameter server is the way in which
model replicas are combined.

Physical Map, Modern Networks: The second point in
the distributed tradeoff space is the server architecture (how
servers map to hardware), speciﬁcally whether the FC compute
and FC model servers are mapped to the same physical
machine (or machines, for multi-machine model parallelism).

This is a technique introduced by Microsoft’s Project
Adam [5] to avoid communicating the large FC model and
its gradient. The method was reported for older networks
with large, fully-connected layers (AlexNet, VGG), however
it also is useful for modern networks (Inception, ResNets).
In traditional networks, the fully-connected layers contained
the majority of the model parameters [2] (> 90%). Newer
networks instead use average pooling and have only a single
FC layer (for softmax regression) [20]. Therefore newer net-
works contain fewer parameters in the FC phase, and fewer
parameters overall, however this single FC layer can still be
very large (e.g., when predicting among 22,000 classes on
ImageNet 22k) and still beneﬁts from reduced FC communi-
cation (because the number of FC weights will always be less
than the number of inputs to the FC phase). Therefore while
newer networks contain only a single FC layer, the merged
FC optimization of Project Adam is still relevant, and while
a characteristic of newer networks is that their overall model
size is smaller due to the elimination of multiple FC layers,
ultimately this does not translate to reduced communication
overall because the cost of communicating the FC layers has
been eliminated in prior work.

is eliminated (i.e.

In addition, as we show, the beneﬁt of merging the FC
servers is not only improving HE due to reduced network
communication, as [5] noted, but also improving SE because
staleness in the FC model
the device
or devices which compute the FC model gradient updates
also store that subset of the model). Moreover there is no
consequence of merging the FC servers for small FC models
because little computation in the FC phase means it is less
likely for the FC to saturate (become the bottleneck), but
merging still provides the beneﬁt of (1) improving SE, (2)
reducing communication and (3) ofﬂoading computation to the
parameter server machines.

As our goal is to be a complete study, we study both
cases (many large FC layers, few small FC layers) in order
to build a system which is robust to any application. For
example future CNN architectures may employ multiple FC
layers again to support transfer learning tasks, or may need
to predict among many object classes (e.g., hundreds of
thousands or millions), further increasing the communication
bottleneck for the FC. In addition, FC layers are also used
for RNNs and other architectures.

This is the subset of the tradeoff which we study in this
work, summarized in Table II. Our goal is to ﬁnd best point in
the tradeoff space given the model, data, and hardware speciﬁ-
cations from the user. Speciﬁcally we do not change the neural
network architecture, but assume that this model is given to
us. I.e. we do not focus on machine learning algorithms or
techniques in this work, but rather study systems tradeoffs
which exist for the most widely used networks/algorithms.
We focus on the SGD algorithm for learning, as it has been
and continues to be used along with momentum by annual
ImageNet winners [2], [20]. Other algorithms exist for training
deep learning models and can also be parallelized, for example

Fig. 20. Hardware efﬁciency penalty for various numbers of compute groups.
The number of machines is ﬁxed to 32. Each machine is an EC2 c4.4xlarge
(CPU) machine.

Google’s deep learning system uses Adagrad [3]. Microsoft on
the other hand uses SGD [5]. Because the systems tradeoffs
we study in this work are orthogonal to the choice of update
algorithm, in this work we focus on SGD, although the same
tradeoff applies to other algorithms as well.

D. Hardware Efﬁciency Model

The goal of this section is to create a predictive model for
how the hardware efﬁciency (HE) varies with the amount of
staleness S in the system, given a ﬁxed number of machines
and batch size. Recall that the staleness S is equal to the
number of compute groups (minus one). This is because a
compute group is characterized by processing a unique data
batch and returning a unique gradient to the model server, so
the number of parallel gradients being computed is equal to
the number of compute groups. S = 0 is the case of 1 compute
group, also called the synchronous case.

Figure 20 shows a plot of staleness vs. hardware efﬁciency
penalty for three datasets. The standard networks from Caffe’s
tutorials are used for each dataset. The hardware efﬁciency
penalty, or PHE, is deﬁned as the ratio of the time per iteration
relative to the time per iteration for this synchronous case,

PHE(S) =

HE(S)
HE(0)

the smallest

A higher PHE is worse (more time per iteration). PHE
decreases (iterations become faster) as the number of compute
groups increases. This is because, if we ﬁx the number of
time per iteration occurs
machines to be N ,
when there is no synchronization, i.e. when the compute group
size is 1 and there are N compute groups (asynchronous
case). In this case S = N − 1. As the compute group sizes
increase, and hence the number of groups decreases (because
the number of machines is ﬁxed to N ), PHE will increase due
to synchronization delays within the groups. When the number
of compute groups is 1, that group contains all N machines
and requires the most synchronization. In this case S = 0, and
this has the highest penalty PHE.

The hardware efﬁciency penalty is dimensionless. It is the
ratio of hardware efﬁciencies (time per iter / time per iter).
Because PHE is normalized to the synchronous (S = 0) case,
PHE ≤ 1. Note also that the hardware efﬁciency penalty is
only comparable across different staleness values if the number
of machines is ﬁxed.

The hardware efﬁciency penalty can be predicted analyti-
cally. However, as we will see in a later section, the cold-start
phase of the optimizer performs a short adaptive grid search
across different staleness values. Because a few iterations are
run for various staleness values, the execution time for these
iterations can be used to provide a precise hardware efﬁciency
model (deep learning layer computations are dense, not sparse,
so there is little variation in iteration time). Nevertheless
understanding the hardware efﬁciency precisely is important:
1) to understand the execution bottlenecks and either hard-
code or allow the optimizer to make physical mapping
choices

2) because it may be too time-consuming to obtain static
information for every staleness value of interest, and
3) because our work is a study meant to inform future

systems which may not use a static optimizer

Figure 20 was run on 33 EC2 CPU machines. The server
architecture shown in Figure 18 was used, i.e. one machine
contains the merged FC compute and FC model servers, and
the other 32 machines contain Conv Compute servers. The
Conv Model server is mapped to one of the Conv Compute
machines. We make two observations, which are true for all
datasets in Figure 20:

Observation 1: As the number of groups decreases (and
hence as their sizes increases), the hardware efﬁciency be-
comes poorer. There are two reasons for this: (1) machine
variance, which causes synchronization delays, and (2) net-
work congestion, because the convolution model needs to be
sent simultaneously to all machines in the group (and gradients
need to be send back simultaneously). Our analysis below
shows that while machine variance exists, it is insigniﬁcant
compared to increased network congestion.

Observation 2: As the number of groups increases, the
speedup is not linear (it saturates). This is because the FC
phase processes only a single batch at once, or equivalently
because the FC compute and FC model server map to the same
physical machine (or machines, as the FC compute / model
server may use multi-machine model parallelism). Recall that
this has beneﬁts for both hardware efﬁciency (by reducing
network communication) and statistical efﬁciency (by reducing
the staleness of the FC model). However it means that only
one gradient computation (batch) is processed by the FC at a
time, and so it may become a computational bottleneck.

Many optimizations exist for both of these observations.

They are presented after our derivation of the model.

1) Full Derivation of Analytic Model: Formally, let there be
N + 1 machines. 1 machine is allocated to the FC phase, and
N machines (e.g., 32) to the conv phase. An execution strategy
partitions the N conv machines into g compute groups. Each
group contains k machines, and the k machines in a group
compute the conv phase with data-parallelism. Therefore, there
will be g = N/k compute groups sharing the single FC server
machine. In addition, let tconv(k) be a function that returns the
time that a group of size k needs to compute the convolution
phase (forwards and backwards for only the conv phase), and
tf c be the time that an FC server needs to serve one group

Fig. 21. Gantt chart illustrating the Hardware Efﬁciency model for the server
architecture of Figure 18. Case 1 is FC saturation (top), Case 2 is Conv
Saturation (bottom), and the boundary between the two is shown in the middle.

(forwards and backwards for only the FC phase. Note that tf c
is independent of k, the number of machines used to perform
convolution on each batch). We also deﬁne that tf c includes
the network time to transfer the data from the conv phase to
the fc phase and the data gradients from the fc phase back to
the conv phase, although we observe that this network time
is often small compared to the computation time of tf c. Note
that tf c is independent of k, the number of machines used
to perform convolution on each batch. Finally, assume for
now as we did above that different groups (batches) cannot be
executed in parallel on the FC server (that case is described
later).

Given k, g, tconv(k) and tf c, our goal is to create a hardware
efﬁciency model which predicts the time per iteration or,
equivalently, which given a time interval T predicts how many
batches will be processed by the system. Because each batch
must be processed by the FC server, this is equivalent to
determining the number of requests that the FC server can
process from the g convolution groups in time T . There are
two cases, also illustrated in Figure 21.

Case 1: Saturated FC Server The ﬁrst case is when
FC server is saturated, i.e. it starts to serve the next request
immediately after the previous request ﬁnishes. In this case,
the hardware efﬁciency is straightforward. The server will
serve T /tf c requests in time T, or equivalently,

Time per iterationsaturated fc = tf c

Case 2: Saturated Conv Server When the FC Server is
not saturated, each conv server becomes the bottleneck. In this
case, the FC server serves T g/(tconv(k)+tf c) requests in time
T, or equivalently,

Time per iterationsaturated conv = (tconv(k) + tf c)/g

which is the total time for a single iteration divided by the
number of parallel groups. This is because the groups all are
computed in parallel, with the exception of the FC server
which is serial, but the FC server is never saturated so it can
also be seen as being part of each parallel group. Refer to
Figure 21 for an illustration of this case. To understand this
case, note that:

1) When each conv server is fast (tconv(k) is small), the

FC server serves more requests in time T

2) When the FC server is fast (tf c is small), the FC server

serves more requests in time T

3) When the number of concurrent group is large (g is
large), the FC server serves more requests in time T
Determining Saturation Finally, the model needs to predict
when the FC server will saturate. This occurs at the boundary
of the times above, speciﬁcally the FC server saturates (case
1) when:

tconv(k) + tf c < gtf c

Intuitively, if the combined FC time to process all groups
(gtf c) exceeds
iteration
(tconv(k) + tf c), then the FC server will always be saturated.
Note that:

the time for a single group’s

1) When each conv server is fast (tconv(k) is small), it is

easier to saturate the FC server

2) When the FC server is fast (tf c is small), it is harder to

saturate the FC server.

3) When the number of concurrent group is large (g is

large), it is easier to saturate the FC server.

We now have an expression for the time per iteration in both
cases as well as a condition to decide which case applies.
Given the following:

• Two of: N , g or k (the third can be calculated from the

• tconv(k), the time to complete the convolution portion of
the network (forwards and backwards) given the group
size, and

• tf c, the time to complete forwards and backwards on the

other 2),

FC phase,

the model can predict the mode of saturation and therefore the
time per iteration.

tconv(k) can be calculated given the throughput of each

node and the network speed. It has two components:
tconv,compute(k) and tconv,network(k).

Let us deﬁne tconv,compute(1) = Tc,c, i.e. Tc,c is the time
it takes for a single machine (k = 1) to compute the forward
and backward pass of the convolution phase. Similarly, let us
deﬁne tconv,network(1) = Tn,c, i.e. Tn,c is the time needed
for a single copy of all the conv phase’s models (forwards
pass) and model gradients (backwards pass) to be passed over
the network. We will describe how to determine these two
quantities later.

The computation time for the convolution phase for k > 1
is then tconv,compute(k) = Tc,c/k, because recall that a single
compute group performs computation on a single batch of data

(data parallelism), i.e. the amount of data per group is always
the same per iteration (e.g., b images) and so if there are k
machines in a group, each will process b/k images. We assume
a linear speedup.

On the other hand, the time for the network communication
increases with k. This is because of increased network com-
munication from the conv model server, i.e. the model needs
to be sent to k workers simultaneously and gradients will be
received from k workers simultaneously (all requests are made
at almost the same time, because the workers in the group are
synchronous). The network time for the convolution phase for
k > 1 is then tconv,network(k) = Tn,c ∗ k. Here, we assume a
linear slowdown.

So while the compute time decreases with k, the network
time increases with k. We assume that both of these are linear.
Empirically we notice that the convolution computation does
not scale exactly linearly with k: on 8 c4.4xlarge machines in
a single group, the forward pass of the convolution becomes
7.2× faster and the backwards pass 6.6× faster. Similarly, we
observe that the network slowdown is usually linear but can
be super-linear, which we attribute to thrashing.

Given tconv,compute(k) and tconv,network(k), we can

naively approximate

tconv(k) = tconv,compute(k) + tconv,network(k)

However, these two can be done in parallel, i.e. while one layer
is computing its forwards pass, the model for the next layer
is being sent over the network. This does not entirely overlap
because the ﬁrst layer needs to complete its backwards pass
before requesting the model for its next forwards pass, but we
can approximate the total convolution phase time as:

tconv(k) = max(tconv,compute(k), tconv,network(k))

it

Finally,
is necessary to obtain Tc,c, Tn,c and tf c. We
measured these because they only need to be measured once
(not for each k), but they can be calculated using the node
throughput and network throughput: Tc,c and tf c can be
approximated by counting the total number of operation from
each GEMM operation in the conv and fc phases and assuming
that BLAS achieves the device peak. Tn,c can be approximated
by counting the total number of bytes in the conv models
and assuming the peak network throughput is achieved. These
assumptions are justiﬁed because the matrices and models are
large.

Also note that measurements of these quantities are accurate
for all iterations because deep learning computations are dense
and so there is little variation in the computation time across
iterations, as shown in Figure 22. Note that there is a standard
deviation of than 6% for tconv(1) and tf c, and a standard
deviation of 8% for the total iteration time. For CIFAR-10,
the standard deviation for total iteration time was 1.5%. We
also observed similar variances on a GPU cluster.

Using measurements of Tc,c, Tn,c and tf c, Figure 5 shows
that the analytic model of hardware efﬁciency is accurate.
When the FC server is saturated (right side of the graph), the
model is almost exact. When the FC server is not saturated

FC compute and FC model servers are mapped to the same
physical machine. Also recall that this has beneﬁts for both
hardware efﬁciency (by reducing network communication) as
well as statistical efﬁciency (by reducing the staleness of
the FC model). However it means that
there is only one
FC compute server, and so it may become a computational
bottleneck. This is seen in the top Gantt chart of Figure 21
in which there are blank spaces which indicate un-utilized
machines.

A simple way to ﬁx this is to make the FC machine faster,
for example if there is limited access to GPU hardware, it it
best to use them on this machine (indeed, we see in Section VI
that the GPU cluster does not saturate FC). Another technique
is to use multiple machines for the FC phase, for example
using model parallelism across multiple machines such that
each machine stores a portion of the FC model (this still has
a staleness of 0 for the FC phase).

Another solution is to remove this physical mapping, i.e.
rather than have a single FC compute server mapped to the
FC model server, to have a separate FC compute server per
Conv compute server. This removes the bottleneck of a single
FC compute server, although it also sacriﬁces the beneﬁts
mentioned above. Section VI demonstrates the consequences
of this decision experimentally.

Physical Mapping Details: In addition to mapping the
FC compute and model servers to the same physical machine,
note that the conv model server does minimal computation and
can also be mapped physically to the same machine as the FC
compute/model server or one of the conv compute machines.
The primary concern with this server is network congestion,
so it makes more sense to map to one of the conv compute
server’s machines.

In addition to multiple servers physically mapping to a
single machine, it is also possible for a server to physically
map to many machines, for example using multiple machines
in a model server to implement a reduction tree as in FireCaffe,
or mapping 4 FC Compute servers to a machine that contains
4 GPUs, etc. Another example is merging the FC compute
and FC model server and mapping them to the same physical
hardware as in Figure 16 (b), but where that hardware is
not a single machine as shown in Figure 16 (b) but multiple
machines e.g., using model parallelism.

Finally, a common technique is to “pipeline” the servers by
mapping multiple conv compute servers to the same physical
machine. For instance in the synchronous case (1 group of N
machines), during the FC computation all N machines are idle
(because they are waiting for the FC to return data gradients
before they can begin the backwards pass of the conv phase).
During this idle time those machines can be processing a
different batch, i.e. N = 32, but there are two groups of
size 32. Note that in this example, this pipelining increases
staleness from 0 to 1. Using this pipelining, now the time per
iteration in conv saturation becomes:

Time per iterationsaturated conv = tconv(k)/g

i.e. the FC time is completely hidden.

Fig. 22. Variance of HE times. These are on a cluster of 9 CPU machines,
8 Conv compute groups, 1 machine per group

(left side of the graph), the slowdown and speedups are not
exactly linear, and we under-estimate the time per iteration.

While this analytic model may seem speciﬁc to CNNs, it
extends to any deep learning model because its derivation
relies only on queuing theory, not any speciﬁc properties of
CNNs.

2) Further Optimizations: A primary goal for understand-
ing the hardware efﬁciency above is to determine possible
optimizations.

Saturated Conv Server: We showed above that as the
group size (k) increases, there is no longer FC saturation,
because

tconv(k) + tf c > gtf c

and recall

tconv(k) = max(tconv,compute(k), tconv,network(k))

Speciﬁcally, in Figure 5 the case of a single group (left side
of the graph) is so much slower than FC saturation (right side
of the graph) because tconv,network becomes very large, i.e.
the time it takes to send the conv model to all 32 machines in
the group is signiﬁcantly greater than the computation time,
which is small due to data parallelism across 32 machines.

In particular, note that a single, large group is slower than
many small groups not because of synchronization delays
due to machine variance exists, but because of increased
network congestion, although some variance does exist in the
computation time across machines.

Microsoft’s Adam [5] discusses techniques to mitigate both
of these problems, from not requiring each worker in a group
to ﬁnish processing all of its images to adding multiple NIC
cards on the parameter server machines. FireCaffe [9] uses
the technique of reduction tress for their parameter servers to
reduce communication overhead, which allows them to reduce
this network congestion and scale to many machines in the
synchronous case using a larger batch size.

Saturated FC Server: The optimizations above improve
the hardware efﬁciency for the synchronous case, or generally
for small g and large k. On the right side of Figure 5
(small k, large g), the FC server becomes saturated and no
further speedups are possible. Recall that this is because the

We see that as long as η∗ increases with the batch size,
there is little penalty for larger batch sizes. This is because
larger batch sizes provide a truer gradient each iteration and
permit a larger η before divergence, therefore while a larger
batch consumes more of the dataset, the progress made by
each step is greater. η∗ cannot scale inﬁnitely however, and
plateaus beyond η∗ = 0.0032. As a result, larger batch sizes
make no more progress per iteration than smaller batch sizes,
but consume much more of the dataset each iteration. This is
catastrophic for performance (it can take 30× more epochs
to converge) as computation is effectively “wasted”, which is
why neural networks have been trained with SGD rather than
batch gradient descent since the early days. This also greatly
exceeds the staleness cost incurred of “splitting” a large batch
into smaller, asynchronous batches (which we show is nearly
ﬂat), which is why asynchrony is necessary for systems to
scale to very large clusters.

This suggests that the optimizer needs to tune batch size,
however for imagenet-8 and other datasets we observe that this
performance penalty is negligible around 256 (speciﬁcally we
use 256 for Imagenet, 128 for CIFAR-10 and 64 for MNIST,
based on published results for these datasets). In principle the
optimizer could tune b as well, but we observe that unless b is
too large the penalty is small so we don’t study this in more
detail.

B. Physical Mapping

As discussed in the text, for the physical map which maps
servers to machines, we map the FC compute and model
servers to the same machine (i.e. “merge” the FC servers,
which as [5] argues reduces communication overhead because
it avoids communicating the large FC model) and use one
compute group for the FC phase. The rest of the machines are
used for the conv compute servers. The conv model servers
are mapped to one of the conv compute machines.

Empirically we show in Appendix F-C4 that this mapping
is best for both hardware and statistical efﬁciency: on a
cluster of 33 EC2 c4.4xlarge machines, not merging the FC
servers incurs an additional hardware efﬁciency penalty of
1.2× due to increased communication as well as a statistical
efﬁciency penalty of 2.5× because of staleness in the FC
model. Our current optimizer therefore always chooses this
server architecture, although Appendix D-D (for Section IV-B)
described other scenarios in which these penalties are justiﬁed
to eliminate FC saturation, as well as additional optimizations
within this server architecture (such as reduction trees or multi-
machine model parallelism for the FC phase).

C. Optimizer Details

Fig. 23. Using a batch size that is too large greatly increases the # epochs to
converge (Imagenet 8-class). This slowdown begins when the optimal learning
rate no longer scales with the batch size.

E. Statistical Efﬁciency Model

Because asynchrony can be viewed as increasing implicit
momentum, asynchrony can be made equivalent
to syn-
chronous execution by properly reducing the explicit momen-
tum in order for the total explicit + implicit momentum to
stay the same as the optimal momentum of the synchronous
case. This is true as long as the implicit momentum is less
than the optimal momentum of the synchronous case. This
is a key discovery because it means that staleness can exist
in the system without incurring a statistical penalty, which
is advantageous for hardware efﬁciency. Also, making the
momentum stay the same (rather than just ignoring this result
and letting there be extra momentum) is important because a
total momentum that is too high will diverge, which we show
experimentally in Appendix E. This theory also successfully
predicts measured system behavior: Figure 6 shows the pre-
dicted and actual measured momentum for several popular
deep learning problems. In both cases, upon reducing the
explicit momentum to compensate for implicit momentum,
we observe no SE penalty. Moreover, the momentum increase
due to asynchrony closely matches the theoretical curve for
both datasets. Our study is the ﬁrst to identify a relationship
between hyper-parameters and asynchrony, and next
these
results are used to design the optimizer in Section V.

APPENDIX E
APPENDIX FOR DISTRIBUTED OPTIMIZER (SECTION V)

This section describes how to use the models from the
previous two sections to choose (1) a physical mapping which
maps each server to a machine, and (2) an execution strategy
which deﬁnes the number of compute groups by allocating
data batches to each server. As in previous sections we assume
that the number of machines are ﬁxed.

A. Selecting the Batch Size

We ﬁrst study the batch size in Figure 23, which uses the
imagenet-8 dataset with S = 0 and momentum µ = 0.9. The
x axis varies the batch size and the y axis plots the # passes
over the dataset (or epochs) until convergence. For each batch
size we used an oracle to ﬁnd the optimal learning rate, η∗.
η > η∗ diverged.

For each epoch, Algorithm 1 performs an adaptive grid
search over both the learning rate and the momentum starting
at the current value of g. Speciﬁcally, we run each learning
rate and momentum (see below) for one minute and select the
conﬁguration with lowest loss after 1 minute of execution. If
after 1 minute all these conﬁgurations have the same loss, we
continue to run another minute until there is a clear winner

1.8× more iterations (SE) to reach (cid:96)F in (b). This matches
the theory’s prediction: increasing g decreases the explicit
momentum µ∗, which falls to 0 at g = 32 (see Figure 6 (right))
, and consequently there is a penalty in SE. The optimizer of
Algorithm 1 would therefore select g = 16, which is near-
optimal.

However, our optimizer additionally employs an optimiza-
tion to leverage the HE model from Section IV-B: because
the FC server saturates at g = 4 (determined analytically
or through measurements during the cold start phase), the
optimizer will “short-circuit” Algorithm 1 to begin at g = 4
instead of g = 32, and ends up selecting g = 4, which is 5.3×
faster than sync.

Figure 24 shows the accuracy vs. time (and for reference
accuracy vs. iter, i.e. statistical efﬁciency) for each conﬁgura-
tion (# groups) in Figure 7. Recall that the optimizer selected
4 groups because with proper momentum tuning the statistical
efﬁciency was nearly the same for all curves, but 4 or more
groups had the best hardware efﬁciency. Figures 7 and 24 use
ImageNet 1000 class (1 hour of training) as described above
with 33 EC2 c4.4xlarge machines.

D. Cold Start Phase

The model is trained synchronously before beginning asyn-
chronous execution in Algorithm 1. This is needed in order
to set the appropriate scale for the weights. However fully
synchronous execution may be slow, and just as an optimiza-
tion to Algorithm 1 was to run asynchronously only up to
FC saturation, similarly this section focuses on accelerating
training during the cold-start phase. In particular, a fully
synchronous execution may signiﬁcantly increase the duration
of the cold-start phase due to poor hardware efﬁciency, and
so a cold-start run with slight asynchrony may more quickly
terminate the cold start phase. Therefore, tuning the number
of compute groups is also important for the cold-start phase.
Cold Start Grid Search: To do this, as in Algorithm 1,
we grid search hyper-parameters for each number of groups
(1, 2, 4, . . . , N , for N machines). For each we also grid search
learning rate and momentum. We use a standard adaptive
grid search algorithm for its simplicity. For each staleness,
the algorithm searches for optimal settings of momentum and
learning rate by running each conﬁguration of parameters for
1 minute, and selecting the parameters with the lowest loss
after 1 minute.

The search happens as follows, and is similar to the search in
the steady-state execution of Algorithm 1. We start with S = 0,
ﬁx the momentum to 0.9, and run 1 minute for each learning
rate η ∈ {0.1, 0.01, 0.001, 0.0001, 0.00001}. We search from
lowest to highest and stop early if a learning rate produces a
ﬁnal loss worse than the previous learning rate (or if a learning
rate causes divergence). We select the learning rate which has
the lowest loss after 1 minute. Call this η∗
sync. Therefore for
S = 0, the optimal conﬁguration (µ∗, η∗) = (0.9, η∗
sync). We
do not tune momentum for sync because 0.9 is standard [2],
because this saves optimizer time, and because there is no
implicitly induced momentum due to asynchrony for S = 0.

Fig. 24. Accuracy vs. iteration and Accuracy vs. Time for each execution
strategy shown in Figure 7

in terms of loss (we determine this using a threshold of 5%
from the loss of the past 50 iterations, although a statistical
test can be used as well). We then run this best (µ∗, η∗) for
an hour and then rerun the optimizer.

One could use more sophisticated parameter search routines,

but this took less than 10% of the execution time.

We search the learning rate as follows. Let the learning rate
and momentum used in the previous 1 hour epoch (i.e. the
result of the grid search from that epoch) be (µ∗
last).
For the current epoch, we then search η ∈ {η∗
last/10},
and µ ∈ {0.0, 0.3, 0.6, 0.9}. As an optimization to prune the
search space, we do not search µ > µ∗
last, as
we notice empirically that as the run progresses, the optimal
total momentum decreases.

last when η = η∗

last, η∗

last, η∗

If the optimal µ∗ = 0.0, we try µ∗ = 0.1 and µ∗ = 0.2 as
well, and if the lowest loss is still achieved at µ∗ = 0.0, we
decrease g and repeat the search.

(µ∗, η∗) in the initial (cold-start) phase are selected using a

similar procedure, described in Appendix E-D.

1) Empirical Validation: We now justify the theoretical
results of Section IV-C experimentally. We use a cluster of 33
EC2 CPU machines (CPU-L in Figure 9), the Imagenet 1000-
class dataset, and the AlexNet CNN. We run each execution
strategy from sync (g = 1 conv compute group) to async
(g = 32), each for a single epoch (1 hour). Speciﬁcally, we
plot 6 strategies: g = {1, 2, 4, 8, 16, 32}, where the number of
conv compute machines is ﬁxed to 32.

Each strategy starts from the same checkpoint (i.e. same
loss), but achieves a different ﬁnal loss by the end of the
epoch. We select the lowest ﬁnal training loss achieved by
all strategies, (cid:96)F , and plot three measures in Figure 7: (a) the
time per iteration (hardware efﬁciency, HE), (b) the number of
iterations to reach (cid:96)F (statistical efﬁciency, SE), and (c) their
product, which is the total time to reach (cid:96)F . For completeness,
each strategy uses an oracle to exhaustively ﬁnd its optimal
explicit momentum, µ∗ (within 0.3) and optimal learning rate
η∗ (within an order of magnitude. For all strategies η∗ = 0.01
was optimal). The HE curve in (a) is the same as in Figure 5
(b).

We see in (c) that g = 32 (fully asynchronous) is 3.7×
faster to reach (cid:96)F than g = 1 (synchronous). This is due
to its faster iteration time (HE) in (a), although it requires

For the remaining S after S = 0, we perform the following
iteration: We increase the number of groups to the next highest
power of 2 (after sync, this is g = 2, then g = 4, g = 8, etc, i.e.
S = 1, 3, 7, . . .) Let the optimal conﬁguration from the previ-
ous S be (µ∗
last). For the current S, we run a grid search
for (µ, η)|µ ∈ {0.0, 0.3, 0.6, 0.9}, η ∈ {η∗
last/10}. I.e.,
η∗ deﬁnes the search range for the next S. In addition, µ∗
reduces the search space for the next S: we do not search a
higher momentum than µ∗

last while searching η = η∗

last, η∗

last, η∗

last.

We notice empirically that

there is not a large impact
of running a ﬁner grid for momentum (although this can
be done by adding a second-phase of search which ﬁxes
η∗ and searches µ around µ∗). Running multiple random
seeds (network weight initializations) can also be used to
ﬁnd a good starting point for the SGD algorithm (this is a
known technique). Tuning parameters is not a novel idea in
machine learning, but unlike existing work our problem is
more sophisticated as we are coupling tuning hyper-parameters
and execution strategies (staleness). Our work is the ﬁrst to
show that hyper-parameters and execution strategies need to
be tuned jointly to avoid divergence as staleness increases.

Once we obtain (µ∗, η∗) for each S, we then run each S
for one minute at a time until there is a clear winner in terms
of loss (we determine this using a threshold of 5% from the
loss of the past 50 iterations, although a statistical test can be
used as well). We then run this best S with its (µ∗, η∗) for an
hour (the cold-start period).

Parameter Search Experiments: The remainder of this
section describes experiments motivating the pruning above,
in particular why a larger staleness does not need to try larger
learning rates or larger momentum values at the same learning
rate. There are a number of insights which allow us to prune
the search space for the cold-start phase and reduce the total
search time. We discovered that as staleness increases, the
optimal learning rate and momentum parameters when S = 0
cause divergence (loss goes to inﬁnity) for larger staleness
values, e.g., S = 31. This makes sense given our theoretical
foundation from the steady-state optimizer: staleness induces
implicit momentum, hence if explicit momentum is not de-
creased as S increases, total momentum can be > 1 and cause
divergence. As S increases, we note that one or both of η and
µ need to be reduced otherwise SGD will diverge (loss goes
to inﬁnity).

Table III shows the optimal parameters for the same datasets
and networks used in Figure 20, at different staleness values.
Here the optimal parameter settings are deﬁned as the param-
eter settings with which the training converges in the fewest
number of iterations. We say that a model has converged once
the training accuracy reaches 99.5%. Recall that a staleness
value of S corresponds to N = S + 1 parallel groups updating
the model asynchronously. In these experiments, the staleness
of the fully-connected models was zero, and so only the
conv model had staleness. Note that for imagenet 8-class
there are only 10400 examples and batch size is 256 so 128
parallel workers was not possible (there is insufﬁcient data).
Note that these small datasets all converge in under an hour

TABLE III
(COLD START) OPTIMAL MOMENTUM AND LEARNING RATE DURING THE
COLD START FOR VARIOUS DATASETS/NETWORKS AND VARIOUS
AMOUNTS OF STALENESS IN THE CONV MODEL

Dataset
(Network)

Staleness

Optimal
Momentum

MNIST
(LeNet)

CIFAR-10
(Krizhevsky)

ImageNet-8
(CaffeNet)

0
31
127
0
31
127
0
31

0.6
0.0
0.8
0.9
0.7
0.1
0.6
0.0

Optimal
Learning
Rate
0.1
0.1
0.01
0.001
0.0001
0.0001
0.01
0.01

and therefore consist entirely of the cold-start phase in our
implementation. While the cold-start phase of these datasets
is less than an hour (e.g., MNIST converges in seconds), and
therefore Algorithm 1 could be run part-way during execution
to select a better strategy for the remainder of the execution
(e.g., asynchronous), the overhead of re-running Algorithm 1
is not justiﬁed for these small datasets, i.e. it is faster to treat
the entire run as the cold-start phase. Therefore we use these
smaller datasets to study the cold-start phase.

The table shows that, with a ﬁxed batch size, as staleness in-
creases the optimal momentum and/or learning rate decreases,
and in some cases not decreasing these parameters and reusing
the parameters for S = 0 causes divergence. Also, we see that
decreasing the learning rate means momentum can increase
again. Intuitively this is because momentum can be viewed as
increasing the learning rate (larger SGD steps), and so if the
learning rate is decreased too much, momentum increases to
compensate for this decrease. Our grid search searches orders
of magnitude for the learning rate, following from previous
work [2], but decreasing the learning rate by a smaller factor
may avoid the need for momentum to increase and provide
faster overall convergence. We leave this exploration to future
work.

APPENDIX F
APPENDIX FOR DISTRIBUTED EXPERIMENTS (SECTION VI)
A. Single-Node Experiments

See Appendix C-E.

B. Small Cluster Experiments

This section provides additional details of the experimental
setup. For the end-to-end experiment on ImageNet 1000, see
Appendix F-E.

We ran all systems to 99% accuracy (which we deﬁne as
convergence) with a timeout of 2 hours. For each system we
used the same CPU and GPU external libraries as discussed
in Appendix C-E.

We further sped up other tools by applying our optimizer
to the extent that no code change was required. MXNet offers
both the sync and async strategy. Given our observation that
async requires tuning parameters, to ensure training did not
diverge we ran each strategy of MXNet with 4 orders of

magnitude of the learning rate for 10 minutes each. We then
selected the best strategy (as the static optimizer would) and
ran it until convergence or timeout. For SINGA we followed
the same procedure and tried all available conﬁgurations.
SINGA supports not only sync (1 group) and async (1 machine
per group) strategies, but also intermediate group sizes. We ran
SINGA with 1, 2, 4, and 8 machines per group, and also 4
orders of magnitude for the learning rate η in each case. All
runs were also for 10 minutes, and then as with MXNet the
best one was run to convergence.

For Omnivore we ran our optimizer, which merged the FC
compute and model servers to one machine and used the other
8 machines as conv compute machines. As with SINGA, the
optimizer searched statically among 1, 2, 4 and 8 machines
per group, but for 1 minute per execution setting. Overall the
optimizer ran for less time than the tuning we did to ensure
no divergence for MXNet and SINGA.

We followed the tutorials for each system and also ensured
that all three systems used identical networks and parameters,
including weight initializations and data preprocessing.

The network we use is CaffeNet, which is Caffe’s version
of Alexnet20. AlexNet is the standard network for ImageNet
and Caffe is the most widely used CNN framework, so this
ensures reproducibility. The weight initializations, batch size,
regularization, and other hyperparameters are the same as
CaffeNet, with a few minor differences:

Unlike Caffe and SINGA, MXNet does not easily support
learning rate and weight decay multipliers, or different ini-
tializations for each model and bias. For consistency across
tools, we therefore just made all 3 tools use the same weight
initialization scheme, which is Gaussian with mean 0 and
standard deviation 0.01, and no multipliers.

MXNet and SINGA do not support the grouping in AlexNet
(which was done in 2012 to save GPU memory), so this is
disabled from CaffeNet (and also is not important anymore as
GPUs have more memory)

No random data preprocessing was used (crop, mirror),
and we show convergence on the training set, not a test or
validation set. We do this because these are machine learning
concerns/optimizations and our focus is the system. We do
subtract the image mean to avoid divergence.

Similarly, we disable the learning rate schedule in all tools
and use a constant learning rate. We do this because we only
train on a subset of ImageNet and to reduce the search space
of the parameter conﬁgurations.

The following subsections describe in detail the individual
settings used for each system to ensure fairness in our com-
parison.

1) Detailed Settings for Both Systems: For both systems
we built in a wall-clock timer to ensure accurate timing. We
created and shufﬂed the dataset using the tools provided by
the systems: MXNet required shufﬂing beforehand, SINGA
provided an im2rec utility. Those tools also were used to
calculate the image mean: MXNet automatically generated the

20https://github.com/BVLC/caffe/blob/master/models/bvlc reference caffenet

mean ﬁle when it ran and SINGA as part of im2rec. Because
we focus on the training set we removed any validation set
from the tools to ensure no time was spent on that. We used
the provided AlexNet examples for each system and changed
them only as above to ensure identical settings across all three
systems (e.g., weight initializations, L2 regularization, etc.)
Accuracy was reported instead of loss for all tools to ensure
consistency. The MXNet examples do not report loss so we
used their accuracy eval_metric. Moreover MXNet’s acc
metric is by default on the entire epoch while SINGA’s is
since last time printing, so we averaged the logs to ensure
consistency across all three tools.

2) Detailed MXNET Settings and Results: We removed all
machine learning optimizations from both tools except those
described above. For MXNet this meant removing gradient
clipping. Because we ensure the parameters are used for all
tools, including the batch size (256 for CaffeNet), this meant
that for MXNet’s dist_sync strategy on 8 machines, a batch
size of 32 was used, and for 32 machines, a batch size of 8
was used (other tools partition the batch size for the sync
strategy, i.e. they partition b images by sending b/N to each
sync worker, but MXNet uses that batch per worker, i.e. they
use a batch size of b × N ). We ﬁxed the random seed to 1 so
the initialization is always the same.

We created a single data ﬁle and ensured that each worker
read it from different location (using ImageRecordIter as
in the AlexNet example). The timer was added as modiﬁed ver-
sion of the speedometer callback (using time.time(),
which is wall-clock time in python).

We used a cluster of 9 machines in these experiments
because MXNet’s AWS documentation instructs to “Prepare a
host ﬁle with all slaves’s private IPs”. Therefore in order to test
parallelism across 8 machines, we opened 9 EC2 machines,
ran MXNet from the master (root) machine, and placed the
other 8 machines in the hosts ﬁle.

On the cluster of 9 c4.4xlarge machines we ran the 4 orders
of magnitude learning rate for each execution strategy and
noticed after 10 minutes that the best was learning rate 0.01
and sync, so we ran until 99% convergence. We needed 4
orders of magnitude to ensure that the optimal setting was
never on the “boundary” of the interval, i.e. the optimal η we
report was superior to an order of magnitude higher η and
lower η. For async no parameter setting had high accuracy
after 10 minutes: The best sync was 60% in 10 min and the
best async got to 20% in 10 min, in spite of better hardware
efﬁciency for async (72 s per epoch async, 120 s per epoch
sync). The best async was with learning rate 0.0001.

On the cluster of 9 g2.8xlarge machines (again following
MXNet’s documentation, 1 parameter server machine and 8
compute machines), we tried both cuDNN v3 and v4 and
found no speed difference. Again we searched learning rate
and found that 0.01, sync was best.

On the c4.4xlargs machines we ensured each worker was
using all the cores, and on the g2.8xlarge machines that all 4
GPUs on each worker were utilized (using nvidia-smi).

3) Detailed SINGA Settings and Results: For SINGA, the
timer built into TrainOneBatch. It also uses wall-clock
time (clock_gettime, same as Omnivore). Again we use
the default AlexNet example with only small changes to make
all weight initializations the same (as in MXNet), and to
remove learning rate / weight decay multipliers (since not
supported easily in MXNet).

Tuning parallelism within groups: we tried tuning

partition_dim for each layer. Speciﬁcally we ﬁrst used
the default, i.e. partition_dim commented out (as in their
AlexNet example). We then uncommented those recommended
partition_dim settings in the example (i.e. dim 0 or batch
parallelism for convolution, and dim 1 or model parallelism
for FC) and found no difference, so we left
partition_dim commented out as in the default AlexNet
SINGA example.

Tuning parallelism across groups: To ensure different data
for each worker we tried specifying a random_skip in the
data section but this made no difference. Documentation v0.1.0
suggested using random_skip but in v0.2.0 (which we used)
it was deprecated, so as with partition_dim we left this
out and used the default AlexNet SINGA example settings.

Next, we had to select for each machine number of workers
per process. For each machine, we tried 1 process of 8, 16,
and 32 threads on a single machine. The number of physical
cores is 8 on the c4.4xlarge, and virtual cores is 16 (nproc
= 16). 16 was fastest so we used 16 workers per process.

As with MXNet we followed the documentation for SINGA
and also included 8 machines in the hostﬁle. We ran 4 orders of
magnitude for the learning rate as described above and found
that 0.0001 and 4 groups of 2 machines each was best after 10
minutes. The result was noisy however, and looked similar to
the distributed results in the “SINGA: Putting Deep Learning
in the Hands of Multimedia Users” paper. We then ran the
best conﬁguration for but it did not converge in 3 hours (got
to 70-80%).

SINGA GPU distributed did not work at the time of this

study so it is not included.

4) Detailed Omnivore Settings and Results: Omnivore was
run using the same network and parameters as the systems
above. The optimizer was run as described above in Ap-
pendix E. Each conﬁguration was searched by the optimizer
for 1 minute and search time was reduced by pruning the space
across staleness values. The second search phase was skipped
(momentum granularity was 0.3). We ﬁxed the random seed to
1 so the initialization is always the same. The overall optimizer
time was less than the search time to avoid divergence in other
tools.

On the CPU cluster, the strategy chosen was the same
as with MXNet,
i.e. sync with η = 0.01. Momentum is
untuned for both Omnivore and MXNet, i.e. 0.9, because our
contribution is tuning momentum to compensate for staleness
and sync has no staleness. Since the parameters and staleness
are the same as MXNet, as expected Omnivore achieves the
same statistical efﬁciency. However, it is 2.3× faster in terms
of hardware efﬁciency, for an overall end-to-end convergence

Fig. 25. Recall Figure 7 (replicated here for convenience and also showing
momentum values from Figure 6 (right))

speedup of 2.3×. On the GPU cluster, the optimizer chose
2 groups of 4 m/g, and was 5.0× faster to converge. The
following section studies the beneﬁts of the optimizer in more
detail, and also examines how the tradeoffs change on a large
cluster which has more options for execution strategies: for
example, the extreme strategies of sync or async may not be
sufﬁcient for a larger cluster. This may prevent MXNet, which
only supports these strategies, from scaling to a larger cluster.

C. Detailed Tradeoff Space Analysis

This section analyzes the speedups observed in the previ-
ous section to understand the contribution of each tradeoff
selection that the optimizer made. These tradeoffs include (1)
execution strategy (number of groups), (2) optimizing hyper-
parameters to compensate for staleness, and (3) physical plan
(server to machine allocation).

1) Penalty Deﬁnition: Consider again Figure 7, which
we’ve replicated here for convenience in Figure 25 and also
shown momentum (note that when the optimal explicit mo-
mentum is 0, there is an associated SE penalty). Recall this
ﬁgure showed the tradeoff for compute groups on the CPU-L
cluster for an epoch of ImageNet 1000. The HE and SE plots
were multiplied to produce the right-most plot of total time to
reach a ﬁnal loss.

The vertical axis of the SE ﬁgure in Figure 25 shows what
we call the SE penalty, PSE(S), which is deﬁned as the ratio
of the # iterations needed to converge relative to the case of
no staleness (S = 0),

PSE(S) =

SE(S)
SE(0)

(8)

PSE is dimensionless, because it is the ratio of statistical
efﬁciencies (#iter / #iter). The penalty is 1 when the staleness
is 0, and should be higher for all S > 0. A higher PSE is
worse (more iterations to converge).

Recall also that we deﬁned hardware efﬁciency penalty
(PHE). This is shown in the middle graph of Figure 25.
Since the statistical efﬁciency penalty is deﬁned as the ratio
of the # iterations to convergence with respect to S = 0,
for consistency PHE(S) is also normalized with respect to
S = 0. S = 0 is the case of 1 compute group, also called the
synchronous case. The hardware efﬁciency penalty is deﬁned

Fig. 26. Cold start vs Steady state for Imagenet 1000 on GPU-S

Fig. 27. Cold start vs Steady state for cifar on GPU-S

as the ratio of the time per iteration relative to the time per
iteration for this synchronous case,

PHE(S) =

HE(S)
HE(0)

(9)

As with PSE, a higher PHE is worse (more time per itera-
tion). Whereas PSE(S) increased with staleness, for hardware
efﬁciency this trend is reversed: PHE decreases (iterations
become faster) as the number of compute groups increases.

Note in these ﬁgures, the staleness is 0 for the FC model, so
staleness on the horizontal axis refers only to the conv models
(i.e. the number of conv compute groups).

Finally, recall that the product of hardware and statistical
efﬁciency is the total time to convergence. Since the horizontal
axis (staleness, i.e. # groups) is the same on both the SE
and HE plots, these plots can be multiplied, and the resulting
vertical axis is the total penalty, deﬁned as the ratio of the
total time to convergence (normalized to sync, i.e. S = 0):

PTotal(S) = PSE(S) · PHE(S) =

SE(S) · HE(S)
SE(0) · HE(0)

(10)

We use ﬁgures of this format throughout this section to

quantify the beneﬁt of the choice of compute groups.

2) End-to-End Imagenet 1000: Recall that Figure 25 was
run for a 1-hour optimizer epoch of Imagenet 1000, on the
CPU-L cluster, as discussed in Appendix E-C. Figure 26
(bottom 2 sets of ﬁgures) shows the same experiment on the
GPU-S cluster. Notice again that SE is ﬂat, i.e. maximum
asynchrony is optimal.

Fig. 28.
c4.4xlarge CPU machines.

Imagenet 8-class and CIFAR-10 tradeoff on a cluster of 9 EC2

These ﬁgures show steady-state execution, hence the SE
curves show no penalty (nearly ﬂat). The same curves but for
the cold-start epoch of the GPU-S cluster are shown in the top
of Figure 26.

We also validate this for the small cifar dataset in Figure 27.
Here for exposition we reduced the epoch size to 2 minutes
(otherwise the cold start would converge after only a few
minutes).

3) Small Clusters: The optimizer’s choice of execution
strategy for the small cluster experiments (Figure 12(a) and
(b)) is shown in Figure 28 (CPU-S) and Figure 29 (GPU-S).
In addition to the imagenet-8 dataset we also include CIFAR-

Fig. 29.
machines (36 GPUs total).

Imagenet 8-class tradeoff on a cluster of 9 EC2 g2.8xlarge GPU

Fig. 30. Hardware efﬁciency, statistical efﬁciency, and total time tradeoff
curve (ImageNet 8-class, 33 machines)

every example 21.

10 to show the optimizer is robust across datasets. We see
that for these small clusters, choosing the execution strategy
incorrectly incurs a penalty of roughly 1.5×.

Note that Figure 28 and Figure 29 have the same statistical
efﬁciency curves but different hardware efﬁciency curves.
This is because the difference in throughput between the
GPU machines and CPU machines exceeds the difference in
network speed between these clusters so there is a higher
penalty for the sync case on the GPU cluster. Also, neither
of these 9 machine clusters reach FC saturation.

Next we consider the CPU-L cluster.
4) Large Cluster: The tradeoff for CPU-L (Figure 12(c))
is shown in Figure 30. 4 groups (8 machines per group) was
the optimal point, and that the optimizer chose this execution
strategy. The detailed tradeoff space for CPU-L is analyzed
in Figure 31. Each curve, from the bottom up, represents a
selection made by the optimizer. We’ve isolated each selection
to observe their relative impact.

Avoiding Divergence: First, consider the red line, which
represents the default point chosen by many systems: asyn-
chronous with a large number of machines. Indeed, statistical
efﬁciency is often ignored by other systems and so by default,
the conﬁguration with the best hardware efﬁciency (fastest it-
eration time) is erroneously selected. However if the published
AlexNet hyper-parameters [2] (which are optimal for the sync
case) are naively used in the async case, there is divergence.
Thus, our tuning approach is critical.

The green curve shows that if only the learning rate η is
tuned, divergence can be avoided. Tuning η is also common
practice, although prior work does not do so explicitly to
compensate for staleness as we advocate. In the green curve
momentum is not been tuned, as many systems always use
a momentum of 0.9 (as mentioned in [2]). For example, at
the time of this study MXNet hard-codes this momentum into

Fig. 31. Tradeoff curve showing the impact of each dimension of the
optimizer.

Also, the green curve does not merge the FC compute
and model servers by physically mapping them to the same
machine. Instead, this curve represents the architecture shown
in Figure 16 (a), i.e. there is an FC compute server for each
CONV compute server, and each of these server pairs is
mapped to a separate machine. Of the 33 machines therefore,
one machine contains the CONV and FC model servers,
while each of the other 32 contains a CONV compute and
FC compute server. This conﬁguration represents the strategy
chosen by MXNet, so we report their async curve as the green
line because their system is optimized for this case (note that
doing this disadvantages our ﬁnal speedup ﬁgure, i.e. if we
had used Omnivore’s implementation of this tradeoff point our
optimizer’s speedup would be > 20×). The remaining curves
have their hardware and statistical efﬁciencies normalized to
those of this green curve.

Device Mapping: We now examine our choice of merging
the FC compute and model servers to the same machine (the
33rd machine), as Section V described. The other systems
do not support this merging so we take “unmerged” as the
baseline, and use the 33rd machine for the conv and FC model
servers as MXNet and SINGA’s documentation suggests. The
remaining curves will have their hardware and statistical
efﬁciencies normalized to those of this curve.

The turquoise curve merges the FC servers. We see that
this gives a 1.18× improvement to hardware efﬁciency (due
to reduced communication) and a 2.55× improvement
to
statistical efﬁciency (due to no staleness in the FC model).
Overall, this is 3.01× faster to converge than the baseline.

In the turquoise curve, note that the hardware efﬁciency
improvement is only 1.18× for the merged FC. As discussed in
Section IV-B, this is because while communication is reduced
by merging these servers, on the large CPU cluster the FC
is reached at 4 groups, and not mapping
saturation point

21https://github.com/dmlc/mxnet/blob/52ea0f0cbbf5eaaf38a2341e57afd6829f88a86d/

example/image-classiﬁcation/train model.py#L77

the FC servers to the same physical machine as described
above eliminates the saturation (but requires more network
communication and incurs a statistical efﬁciency penalty).

Parameter Tuning for Staleness: Divergence can always
be avoided by tuning η alone, and indeed most systems
always use a momentum of 0.9 (see comment above) which is
standard for the sync case [2]. However, we show in the purple
curve that at larger staleness values, additionally tuning the
momentum µ permits using a higher η. This does not change
hardware efﬁciency, but now gives an overall 5.85× speedup
over the baseline due to improved statistical efﬁciency.

Execution Strategy: Finally, the blue line represents the
actual choice made by the optimizer. In addition to the selec-
tions above, recall that the optimizer did not choose 32 groups
but 4 groups (Figure 30), which further improves the statistical
efﬁciency to give an overall speedup of > 20× compared to
standard choices traditionally made by deep learning systems.
Note that changing the number of groups to 4 did not hurt
hardware efﬁciency because the FC server is already saturated
(see Section IV-B).

D. Scalability to a Larger Cluster

The previous section (Appendix F-C) showed that as a result
of the optimizer’s tradeoffs Omnivore is able to scale to 32
machines. In this ﬁnal experiment we compare Omnivore to
the best competitor from the small clusters, MXNet, on this
larger cluster. We use the same dataset and network as the
small cluster experiments, and deﬁne convergence the same
way (99% accuracy).

We attemped to open a cluster of 33 g2.8xlarge instances
but continuously ran into EC2 errors related to not enough
machines available (InsufﬁcientInstanceCapacity).

As in Section VI-B, we repeat the same procedure to apply
our optimizer to MXNet, i.e. we run each conﬁguration for
10 minutes, select the best execution strategy and learning
rate, and run that to convergence. The best strategy was once
again sync with η = 0.01. Speciﬁcally, as in the previous
experiments, we followed MXNet’s documentation and used
the EC2 master machine as the root, and put the other 32
workers in the hostﬁle. We ran MXNet for 10 minutes with
both sync/async and 4 orders of magnitude learning rate as
described above. This time all 4 orders of magnitude for η
were needed because for sync with 32 machines, after 10
minutes 0.001 and 0.01 were almost the same, although 0.001
was better by ∼ 5% points. Since this differed from the
optimal η from the 8 machine case, which was 0.01 (i.e.
statistical efﬁciency changed for the larger cluster), to be sure
we ran MXNet with each η to convergence and noticed that
in fact 0.01 was signiﬁcantly faster to converge in the end, as
was true on the smaller clusters. Similarly, for async after 10
minutes the best η was 0.00001, although this was close to
0.0001, therefore once again we ran both to convergence and
noticed that 0.0001 was faster to converge for MXNet. We did
not do this extended parameter tuning for Omnivore, and only
ran the 1 minute static runs described above, to make sure that

we were getting the best possible performance from MXNet
for our comparison.

For Omnivore the optimizer was used as described in the
previous section. The best result of MXNet and Omnivore is
shown in Figure 12(c). We see that Omnivore is 3.2× faster
than MXNet to converge now (it was 2.3× faster on the 9 CPU
cluster). In addition, we see that compared to Figure 12(a),
Omnivore sped up on the larger cluster but MXNet did not.
Therefore not only does the optimizer give speedups by not
relying solely on the sync strategy and by merging the FC
servers, but also enables scalability to more machines.

If we do not apply our optimizer to MXNet, Omnivore now
converges 20× faster. This is compared to MXNet’s async
strategy, which has poor statistical efﬁciency for 32 machines.
This 20× corresponds exactly to the speedup in the previous
section because the green curve in that section is MXNet
using the async strategy (i.e. they are the same point in the
tradeoff, see the discussion in Appendix F-C4) By applying
our optimizer to MXNet we select the sync strategy instead,
which lowers the gap with Omnivore to 3× on this cluster.
Therefore this section shows not only that the optimizer gives
speedups of more than an order of magnitude, but that it is
versatile and can be applied to existing tools.

E. End-To-End Experiments

The end-to-end result

is in Figure 10. We trained the
standard CaffeNet (same setup as in Appendix F-B) using
ImageNet-1000 on both systems using both the CPU-L and
GPU-S clusters. We time out each run after 8 hours and report
the training accuracy vs. time.

According to MXNet’s ofﬁcial performance tuning guide-
line 22, they recommend trying the sync strategy, but also
state that “if the model size is quite large or you use a
large number of machines, you may want to use dist async”.
Immediately above, they describe large as “models with size
>> 100MB such as AlexNet and VGG.” Because we are
training AlexNet and use up to 33 machines, which may be
considered large, then according to these instructions async
could be the best choice. Because they do not provide an
automatic mechanism to make this decision we followed this
advice and tried both strategies, as we did above in section
F.2. This required tuning the learning rate for each strategy.
We used the optimal learning rate obtained for each strategy
on ImageNet-8, as recommended by [29] which states that “the
best way to determine the correct learning rates is to perform
experiments using a small but representative sample of the
training set”. In addition, MXNet does not provide a learning
rate schedule for their AlexNet example (as of the writing
of this study) so we use the standard learning rate schedule
of [2] which decreased the learning rate by 10× when training
plateaued.

For Omnivore, we ran the optimizer end-to-end. We ensure
a 10% overhead by running the optimizer and then training for
10× the optimizer time before rerunning the optimizer. Each

22https://github.com/dmlc/mxnet/tree/db6f6a9418e5696b04be741a78a47ae877bb5505/

example/image-classiﬁcation

TABLE V
GRID SEARCH PARAMETERS FROM FIGURE 10 (B) ON CPU-L

Fig. 32. Recurrent Neural Network using 9 EC2 c4.4xlarge CPU machines.

TABLE IV
GRID SEARCH PARAMETERS FROM FIGURE 10 (A) ON GPU-S

Phase
cold
phase 1
phase 2

µ
0.6
0.6
0.6

η
0.01
0.001
0.001

Phase
cold
phase 1
phase 2

µ
0.6
0.6
0.3

η
0.01
0.001
0.001

g
4
8
8

g
2
4
4

time the optimizer runs, it searches momentum, µ and learning
rate, η, either reducing one, reducing both, or keeping them
the same. The grid search results for each phase in Figure 10
(a) and (b) are shown in Table IV and Table V. On the GPU-S
cluster, the ﬁrst time the optimizer ran µ remained at 0.6 but η
decreased from 0.01 to 0.001. The second time, the parameters
did not change. On the CPU-L cluster, the ﬁrst optimizer run
made the same choices: µ also began at 0.6, and remained 0.6,
while η decreased from 0.01 to 0.001. Following the second
optimizer run, µ reduced from 0.6 to 0.3, and η remained
unchanged. Note that the cold-start (ﬁrst) phase of the CPU-
L run in Figure 10 shows a very large slope, and then after
the optimizer run this slope decreases. In ongoing work we
are exploring a slope-aware optimizer which would not rerun
the optimizer at this point but continue the execution. With
this change we expect the gap against competitor systems to
increase signiﬁcantly.

F. Preliminary RNN/LSTM Result

To understand if our tradeoff applies more broadly, we
implemented the Recurrent Neural Network model and LSTM
proposed by Graves [30]. Following the same protocol as
Figure 28 and using the CPU-S cluster, we see in Figure 32
that the tradeoff between statistical efﬁciency and hardware
efﬁciency is comparable, and choosing a completely syn-
chronous or asynchronous conﬁguration can be up to 2×
slower than the optimal conﬁguration. 23

G. Comparison to Standard Schedules

We have shown that tuning is critical for good performance.
We next validate the hypothesis that Omnivore’s optimizer out-
performs standard tuning and parameter scheduling methods.
To validate this, we run Omnivore on the full ImageNet using
the standard CaffeNet. We run two versions of Omnivore: (1)
Omnivore (Default Schedule), which uses CaffeNet’s default
learning rate schedule that decreases the learning rate by 10×
every 100,000 iterations; and (2) Omnivore, which uses the
standard Omnivore optimizer. To be fair, both versions use the
same grid search strategies to select the optimal learning rate,
momentum, and number of compute groups at the beginning.

23 We also see a similar tradeoff in the LSTM variant proposed by Graves [30].

Fig. 33. Comparison of Omnivore’s optimizer to CaffeNet’s default learning
rate schedule on Full ImageNet with AlexNet.

In addition, we run Omnivore for 10× the optimizer time
before it re-optimizes the parameters.

Figure 33 shows the training loss vs. wall-clock time. The
two plateaus shown in Omnivore correspond to the times
Omnivore re-optimizes the parameters. The losses of both
Omnivore (Default Schedule) and Omnivore decrease over
time. However, after the ﬁrst parameter re-tuning, Omnivore’s
loss starts to decrease more rapidly. Finishing at 36K seconds,
Omnivore is 1.5× faster to achieve the same loss as Omni-
vore (Default Schedule). Omnivore does not require the user
to specify the number of iterations to run before re-optimize
the parameters.

H. Comparison to Bayesian Optimizer

We compare our simple optimizer with the state-of-the-art
Bayesian optimization approach that explores the parameter
space of CNNs. The results are shown in Figure 34. We follow
Snoek et al. [18] to model the search space as (η, µ, S, N )
where N is the number of epochs to run. We use the same
search space for (η, µ, S) as in our optimizer and measure
both the number of conﬁgurations and the total number of
epochs that the Bayesian optimizer needs to run before ﬁnding
a run that achieves an accuracy within 1% of Omnivore’s best
accuracy.

Our procedure is as follows. We ﬁrst run Omnivore to obtain
a run which reaches 99% convergence using the same dataset
and cluster as in Figure 12 (b). This took 80 epochs and 680
seconds. We then give the Bayesian optimizer N = 80 and
it tries to ﬁt η, µ and S in order to reach the lowest loss
(highest accuracy) by a timeout of 1000 seconds. It searches
N in the range 1, . . . , 80, S in the range 1, 2, 4, 8, µ in
the range 0.0, 0.3, 0.6, 0.9, and learning rates in the range

Fig. 35. Hardware and statistical penalty without tuning (momentum 0.9)

Fig. 34. Bayesian optimizer run on Imagenet-8 using the GPU-S cluster.

0.1, 0.01, 0.001, 0.0001, 0.00001, i.e. the same as Omnivore
searches.

It took the Bayesian optimizer on average 12 runs before
ﬁnding a strategy which achieves accuracy within 1% of
Omnivore’s run. On average this takes 6× more epochs
than just training that strategy to convergence, which makes
the Bayesian approach infeasible to run on Imagenet 1000
(whereas Omnivore’s optimizer incurred only a 10% over-
head).

Compared with our optimizer, one difference is that we
are using the ﬁrst minute’s execution as a proxy for a longer
run, while on the other hand, Snoek et al. have the number
of epochs to run as a parameter to explore and do not
share information across runs. It is of course possible to use
Bayesian optimization to guide our grid search for the ﬁrst
minute, however, it is future work to integrate this heuristic
into the Bayesian optimization framework in a principled way.

APPENDIX G
TENSORFLOW EXPERIMENTS

In this section we see that

tuning momentum can sig-
niﬁcantly improve the performance on platforms other than
Omnivore by running experiments on TensorFlow [22]. We use
TensorFlow r0.9 starting with the fully synchronous and fully
asynchronous implementations of Inception-v3 for ImageNet
[12] found in [32]. Synchronous training has each worker send
its gradients to the parameter server. The parameter server
aggregates all the gradients, applies them to update the weights
and then sends the new weights to all the workers. In the
asynchronous conﬁguration, each worker sends its gradients
to the parameter server. The gradients are immediately applied
to the weights and the new model is returned to the worker.
On 32 GPU workers, asynchronous training reaches the
same loss as its synchronous counterpart 1.5× faster, when
properly tuned. In contrast, when momentum is ﬁxed to 0.9
for both, synchronous training is faster. We also implement
compute groups on top of the code from [12] and report the
tradeoff ﬁgures for a 32 worker conﬁguration. We discuss the

Fig. 36. Hardware and statistical penalty with tuning

experimental setup and discuss results on momentum tuning
and compute groups.

A. Experimental Setup

We deployed 8 g2.8xlarge instances on AWS, each equipped
with 4 GPUs. We allocate 1 GPU per worker node, meaning
there are 4 workers per instance and a total of 32 worker
nodes. There is a single parameter server that runs on one of
the instances. Each worker uses a batch size of 32 images,
as suggested in [32].24 Notice that the suggested setup uses a
per-worker batch-size as opposed to the per-group batch-size
we used on Omnivore. We used the SGD with momentum
optimizer for Inception-v3 as opposed to RMSProp with
momentum used in [12].

We perform our experiments after a warm start: we train
the Inception-v3 network synchronously on ImageNet, until
it reaches 50% training accuracy and take a snapshot. This
snapshot is used as the starting point for all measured runs.
We run each set of parameter values for 1 hour. We grid seach
momentum values in {0.0, 0.3, 0.6, 0.9} and learning rates in
{0.005, 0.01, 0.05}. We experimented with other values and
found these to capture the range of optimal learning rates. We
measure the loss achieved by each conﬁguration after 1 hour
of execution.

B. Results

We measure the time and number of iterations it takes to
reach a target loss and then perform hardware and software
efﬁciency analysis. Figure 35 shows the normalized statistical
penalty, hardware penalty and wall clock time of training to
the target loss for both the synchronous and asynchronous
conﬁgurations, starting from the same snapshot. In this ﬁgure,

24 This is also the largest batch size that can ﬁt on a single GPU of the

g2.8xlarge instances.

same number of iterations. As Figure 11 showed, Omnivore’s
speed on the c4.4xlarge instance is 0.57× the speed of the
g2.2xlarge instance. This ratio closely matches the FLOPS
ratio 0.7/1.2. Therefore we observe that running on a CPU
instance is 2.1× more expensive than a GPU instance, due to
the difference in the FLOPS/dollar ratio for these instances.
This suggests that on cloud services such as Google Compute
which do not have GPU instances, CPU-based deep learning
is a viable and cost-effective option when using Omnivore.
Moreover, organizations that can amortize the cost of CPUs
in more ways than GPUs may ﬁnd them to be a cheaper
alternative.

Distributed: In the distributed setting we consider again
the case of 9 machines and compare Omnivore running
on the GPU cluster (g2.8xlarge, $2.60 per machine-hr, 4.8
TFLOPS per machine) and CPU cluster (c4.4xlarge, $0.838
per machine-hr, 0.7 TFLOPS per machine). The difference in
peak FLOPS between these clusters is 6.8×, and the speedup
to convergence obtained by Omnivore on the GPU cluster
compared to the CPU cluster is 5×–note it
is not quite
6.8× because network speed does not scale with the node
throughput. If we consider only hardware efﬁciency (since
statistical efﬁciency is unrelated to the underlying hardware),
the GPU cluster is 5.6× faster than the CPU cluster, which
is nearly the FLOPS ratio. As in the single-machine case
therefore it is only the FLOPS/dollar ratio which matters. The
GPU cluster is more cost-effective, now by a factor of 1.8×.
These results show that CPU deep learning is not sig-
niﬁcantly different from GPU in terms of consumer cost,
and it will be exciting to see how these trends change for
future CPUs which have increased SIMD parallelism as well
as newer GPUs which optimize for lower power. As SIMD
processor bandwidth has been doubling in each generation, it
seems that CPU training may indeed catch GPUs relatively
soon.

A. Distributed Calculation

The ratio of peak FLOPS of the GPU cluster / CPU cluster
is 4.9/0.74 = 6.6. Considering the optimal points chosen by
our optimizer, the CPU / GPU time to convergence is 5×.
If statistical efﬁciency is ignored, and we compare only the
speeds of the async cases on each cluster, the ratio is now
34s/iter for the CPU cluster and 6 s/iter for the GPU, or 5.6×,
which almost matches the ratio in device FLOPS. Given that
GPU cluster is $2.6/$0.838 = 3.1×more expensive, the GPU
cluster is 1.8× cheaper per iteration which matches closely
with the FLOPS/dollar ratio.

APPENDIX I
APPENDIX FOR CONCLUSIONS (SECTION VIII)

Our study ﬁrst demonstrated that on a single machine we
could achieve CPU speeds proportional to the device FLOPS,
showing end-to-end speedups of more than 5.5× on EC2 CPU
instances over state-of-the-art tools. With this improved CPU
speed we showed that CNN computations are compute-bound.
This allows the underlying hardware in a machine to be treated

Fig. 37. Hardware efﬁciency, statistical efﬁciency and wall clock time when
tuning momentum and learning rate.

the momentum parameter is set to the standard 0.9 value (the
value used in [12] is not reported in the paper, but available
code [32] suggests it was 0.9). The learning rate is tuned
using the grid described above. The statistical penalty is high
for asynchronous training, which results in longer wall clock
time to reach the same loss compared to synchronous training.
As expected, the hardware penalty is lower for asynchronous
training. Figure 36 shows the results of the same experiments,
but using a grid search to tune momentum and learning rate.
The statistical penalty for asynchronous training relative to
synchronous is now improved by a factor of about 2.4×.
Tuning in this case is result-changing. The asynchronous
conﬁguration reaches the same loss in less time compared to
synchronous training.

C. Compute Groups

We implemented compute groups in TensorFlow r0.9, used
the same experimental setup as before and report results on
Inception-v3. Our goal is to understand the tradeoffs on this
different platform. Figure 37 reports the performance curves
for this setup. As in some of our Omnivore experiments,
the statistical efﬁciency remains nearly ﬂat when tuning.
This allows us to take advantage of the better hardware
efﬁciency of asynchronous settings. We see that, in this case,
32 nodes are not enough for the limits of asynchrony to start
showing. We expect that given a larger number of nodes,
the optimal conﬁguration will not be fully asynchronous, but
rather some intermediate compute groups setting. This result,
though under a different setup, contradicts the—reported but
not demonstrated—claim that hybrid conﬁgurations do not
perform better than fully synchronous training in TensorFlow.
We attribute this to the fact that experiments in [12] do not
involve any momentum tuning.

APPENDIX H
APPENDIX STUDYING TOTAL COST OF OWNERSHIP (TCO)

Our study showed that CNN training is compute-bound
regardless of the compute device used. Given that we can
now train CNNs on CPUs proportional to the CPU FLOPS,
this opens new questions in total cost of ownership (TCO) for
running CNN systems. We discuss those trends and changes.
Single Node: We compare the price of running Omnivore
on a GPU instance (g2.2xlarge, $0.65/hr, 1.2 TFLOPS) and
a CPU instance (c4.4xlarge, $0.838/hr, 0.7 TFLOPS) for the

as a black-box, and we are 2.7× faster than other systems on 4
GPUs and also 15% faster on a single GPU by using the weak
CPU alongside the EC2 instance’s GPU. More generally, we
show that each device or node in a cluster can be treated as a
black-box that is characterized only by the throughput which
it provides and is irrelevant to the type of hardware on that
node (e.g., CPUs or GPUs).

Our second contribution was an empirical study of the
factors affecting time to convergence for distributed deep
learning training, and a novel, theoretical characterization of
asynchrony which demonstrates that by tuning algorithmic
(explicit) momentum in SGD there is no statistical penalty
associated with asynchronous execution. We justiﬁed this
empirically. We deﬁned a tradeoff space and demonstrated
that the execution strategy and server architecture were key in
reducing the total time to convergence. We further showed that
all existing distributed deep learning systems fall somewhere
along this tradeoff space, but do not optimize within the space.
Finally, we studied each of these tradeoffs by decoupling
their impact on hardware and statistical efﬁciency. This made
it possible to study these factors in isolation and build an
optimizer which optimizes within the tradeoff space. We
showed both theoretically and empirically the need to jointly
tune hyper-parameters with execution strategies in order to
avoid slower convergence or divergence. We show that our
optimizer provides a > 20× reduction in time to convergence
compared to other systems which select sub-optimal points in
the space, and we also show that our optimizer is versatile by
applying it to existing tools. In doing so, we close the gap
between our system to 3× faster than other systems, in some
cases also preventing divergence in those other tools.

Omnivore: An Optimizer for Multi-device
Deep Learning on CPUs and GPUs

Stefan Hadjis
Dept. of Computer Science
Stanford University
Email: shadjis@stanford.edu

Ce Zhang
Dept. of Computer Science
ETH Zurich
Email: ce.zhang@inf.ethz.ch

Ioannis Mitliagkas, Dan Iter, Christopher R´e
Dept. of Computer Science
Stanford University
Email: {imit, daniter, chrismre}@stanford.edu

6
1
0
2
 
t
c
O
 
9
1
 
 
]

C
D
.
s
c
[
 
 
4
v
7
8
4
4
0
.
6
0
6
1
:
v
i
X
r
a

Abstract—We study the factors affecting training time in
multi-device deep learning systems. Given a speciﬁcation of a
convolutional neural network, our goal is to minimize the time
to train this model on a cluster of commodity CPUs and GPUs.
We ﬁrst focus on the single-node setting and show that by using
standard batching and data-parallel techniques, throughput can
be improved by at least 5.5× over state-of-the-art systems on
CPUs. This ensures an end-to-end training speed directly propor-
tional to the throughput of a device regardless of its underlying
hardware, allowing each node in the cluster to be treated as a
black box. Our second contribution is a theoretical and empirical
study of the tradeoffs affecting end-to-end training time in a
multiple-device setting. We identify the degree of asynchronous
parallelization as a key factor affecting both hardware and
statistical efﬁciency. We see that asynchrony can be viewed as
introducing a momentum term. Our results imply that tuning
momentum is critical in asynchronous parallel conﬁgurations,
and suggest that published results that have not been fully tuned
might report suboptimal performance for some conﬁgurations.
For our third contribution, we use our novel understanding of
the interaction between system and optimization dynamics to
provide an efﬁcient hyperparameter optimizer. Our optimizer
involves a predictive model for the total time to convergence
and selects an allocation of resources to minimize that time.
We demonstrate that the most popular distributed deep learning
systems fall within our tradeoff space, but do not optimize within
the space. By doing this optimization, our prototype runs 1.9×
to 12× faster than the fastest state-of-the-art systems.

I. INTRODUCTION

In recent years, deep learning has provided signiﬁcant
improvements in quality for a number of data-driven appli-
cations [1]–[4]. An important aspect of deep learning is that
quality improves with the amount of data we can process;
therefore, advances in system efﬁciency and scalability directly
improve quality. This has led to an arms race of distributed
deep learning systems both in industry (e.g., Google’s Dist-
Belief [3], Microsoft’s Adam [5]) and in academia [6]–[9].

Despite this proliferation of deep learning systems, there
have been few studies of deep learning from a data-systems
perspective. Each of these systems makes a set of design
decisions that may not work for other tasks or hardware
settings. In our experience working with multiple Ph.D.-
level users of these systems—including experts in pathology,
radiology, computer vision, and the energy sector—it is often
very difﬁcult even for advanced users to make these design
decisions themselves. It is not uncommon for a suboptimal

design choice to result in an end-to-end runtime that is an
order of magnitude slower than what is possible. Moreover,
most systems provide no way of automatically selecting an
optimal conﬁguration, placing this burden on the user. This
also contributes to two debates about deep learning systems.

Debate 1: CPU vs. GPU. There has been a long debate about
CPUs vs. GPUs for deep learning. GPUs are popular for CNN
systems because of the high throughput they provide, but they
contain smaller off-chip memories. Microsoft’s Adam argues
that CPUs can deliver more cost-effective performance [5]. For
users who cannot control their data-center hardware, Amazon’s
EC2 provides GPUs but Google Compute does not.

Debate 2: Synchronous vs. Asynchronous Training. An-
other debate is about the synchronization strategies to use in
multi-device deep learning. For example, Google’s latest Ten-
sorFlow paper [10] comes out in support of fully synchronous
methods, citing recent papers [11], [12]. Other systems, such
as DistBelief [3], Project Adam [5], H2O [13], and recent
theoretical efforts [14] focus on asynchronous training and
argue that it is more scalable than fully synchronous strategies.

In this paper, we perform a ﬁrst study of the design space for
deep learning systems. We identify the key tradeoffs for single-
device and multi-device systems, providing insights into the
above debates. We ﬁnd that the CPU implementation of many
state-of-the-art systems misses textbook batching optimiza-
tions, which can make the CPU implementation at least 5.5×
faster. We also see that when the momentum parameter [15] is
tuned, there is no penalty for asynchrony—except in a short,
cold-start training period. This previously unknown connection
between asynchrony and momentum was surprising even to
leading groups in the area. Recent work does not tune momen-
tum [11], [12] and reports that asynchronous conﬁgurations
are slower. We see on different systems, including TensorFlow,
that tuning changes this outcome: asynchronous conﬁgurations
are faster when we tune. These theoretical and experimental
insights help us understand the tradeoff space better and build
an automatic optimizer to choose the optimal conﬁguration. As
a result, our prototype system, Omnivore, can be 1.9× to 12×
faster than the fastest competitor systems. Our results have
attracted interest from major companies. In collaboration with
a major chip manufacturer, we are integrating our optimizers
on new platforms of much larger scale.

Overview of Technical Contributions

To conduct our study, we develop a prototype distributed

system called Omnivore.1 We make three contributions.

Scope of Our Study: We focus on perhaps the most
popular deep learning models, convolutional neural networks
(CNNs), which are state-of-the-art for a wide range of applica-
tions (e.g., image processing, video analysis, drug discovery).
Our study answers the following question: “Given a cluster
(e.g., X machines, Y GPUs, Z CPUs, etc.), how do I train my
CNN as quickly as possible?”. We assume that the following
are given: (i) a deep learning model (network architecture),
(ii) a dataset for training this model, and (iii) a set of
computational resources (a number of devices on machines,
their throughput, and the network speed).2 We then study
how to minimize the total training time. We build a complete
prototype capable of training the most popular deep learning
models. This allows us to hone in on two major choices: (i)
how to use hardware on each node and (ii) the degree to
which asynchrony can be tolerated. Our work demystiﬁes these
factors by identifying the key tradeoffs that underlie all design
decisions, providing theoretical guidance about asynchrony,
and quantifying the impact of those tradeoffs experimentally.
Contribution 1: Single-Device Optimizations: We show
that it is possible to achieve throughput proportional to the
maximum FLOPS of both CPUs and GPUs. This is not trivial;
while state-of-the-art systems achieve GPU speeds propor-
tional to the device throughput, existing CPU implementations
can be sped up signiﬁcantly compared to what is reported
in the literature. Our study builds on two key optimizations
we reported in a workshop [16]. We use batching and data-
parallel optimizations—not employed in other systems—to
achieve end-to-end speedups of more than 5.5× over state-of-
the-art systems on commodity CPUs. Such optimizations are
not always possible on the GPU, but by selecting this strategy
for the CPU, we now achieve speeds proportional to the peak
throughput. This allows us to build a simpler optimizer by
modeling CPUs and GPUs as black boxes.

Contribution 2: Multi-device Tradeoffs: Our second con-
tribution is an empirical study of factors affecting training
time for multi-device deep learning training. We analyze the
decisions made on existing systems and ﬁnd that while they are
diverse, the strategies of the most popular systems fall within
a tradeoff space deﬁned by two fundamental dimensions [3],
[5]–[9]: (i) the server architecture, or how the layers of a
CNN map to devices; and (ii) the execution strategy, or how
batches of data are mapped to machines for processing. We
develop a simple framework that allows us to model each of
these approaches. Devices are organized in compute groups.
Each compute group is responsible for a single batch of
data per iteration. Inside a group, the computation occurs
in standard, synchronous steps. Across groups, computation
happens asynchronously.

1 https://github.com/HazyResearch/Omnivore
2 We only do basic network optimization, and we assume that machines are connected

by a uniform and fast topology, e.g., if they were housed on the same rack.

We study the impact of asynchrony on the end-to-end
performance of training deep learning systems. Not surpris-
ingly, the more asynchrony there is, the faster the system is
for each iteration, which we call hardware efﬁciency. This
happens because there is less coordination between workers.
The challenge is to understand how asynchrony affects the
number of iterations to converge, which we call statistical
efﬁciency. We provide novel understanding of the factors that
affect statistical efﬁciency. Empirically, we observe that the
optimal value for the momentum parameter decreases as we
Indeed, our
increase the number of asynchronous workers.
theory in [17] shows that asynchronous-parallel training can be
viewed as a synchronous update but with an increased, implicit
momentum term. Furthermore, if the optimal momentum value
for a problem is above the implicit momentum, then there is no
penalty for running asynchronously, as long as the momentum
is properly tuned. Although momentum is explicitly (i.e.,
algorithmically) introduced in almost every system [15], we
are the ﬁrst to realize this connection between asynchrony and
momentum. Here, we validate this experimentally: we see that,
by properly tuning momentum, asynchronous methods can be
at least 1.5–2.5× faster than using the standard momentum
value of 0.9, which is used in most existing work. This
understanding of statistical efﬁciency, along with an analytical
hardware efﬁciency model, forms a novel system tradeoff
space.

Contribution 3: Simple Automatic Optimizer: Based on
the intuition behind our
our theory and empirical study,
optimizer is very simple: pick the highest degree of asynchrony
such that
the implicit momentum induced by asynchrony
is below the optimal momentum. Given a ﬁxed number of
compute groups (which control the degree of asynchrony), we
grid-search the parameters for learning rate and momentum by
measuring the statistical and hardware efﬁciency for minutes
(less than 10% of the time to train a network). If the best-
the optimizer chooses this
found momentum is non-zero,
conﬁguration. If it is zero, we assume that there could be
a better setting with fewer compute groups. Our optimizer is
able to choose a near-optimal point in the tradeoff space, and
we demonstrate that our system achieves end-to-end speedups
of 1.9× to 12× on popular CNN workloads compared to state-
of-the-art tools that choose suboptimal tradeoff points. We
compare our simple optimizer with a state-of-the-art Bayesian
optimization approach [18]. Both approaches are able to reach
the same ﬁnal accuracy (within 1%), but the Bayesian strategy
takes almost 6× as long. We can also apply our optimizer to
other deep learning systems. In some cases, this prevents those
other tools from diverging, while in other cases, it speeds them
up by 7×.

Outline: We present background in Section II. Section III
and Section IV introduce the tradeoff space related to single-
machine and multi-device settings, respectively. Section V
describes the optimizer for making decisions in this tradeoff
space. We validate our results in Section VI, discuss related
work in Section VII, and conclude in Section VIII.

II. BACKGROUND

A. Convolutional Neural Networks (CNNs)

A convolutional neural network (CNN, [2]) consists of
layers L1, L2, . . . , LP . Each layer is an operator which takes
as input a 3D data tensor D ∈ Rn×n×din and transforms
to a resulting 3D data tensor R ∈ Rm×m×dout,
it
i.e.
LF W
(Dp) = Rp. F W indicates the layer running in the
p
“forward” direction to transform D into R. Layers have a
second operation, backward or BW , described later. Often,
D1, the input to the ﬁrst layer L1, is an image I ∈ Rn×n×3,
where 3 represents the RGB color channels.

For layers after L1 (the input layer), the input tensor Dp
comes from the output of a prior layer (usually Dp = Rp−1),
such that the CNN layers are cascaded to deﬁne a composite
operation (boldface highlights inputs and outputs)

RP = LF W

P

◦ LF W

P −1 ◦ ... ◦ LF W

2

◦ LF W
1

(I)

(1)

The ﬁnal result RP is the CNN’s prediction for image I.
For example, if the task is image classiﬁcation with 1000
categories, the tensor RP ∈ R1×1×1000 is a vector containing
the probability of each category. This prediction is then
compared to C, the true classiﬁcation for I, using a loss
function (cid:96)(RP , C) that evaluates the quality of the prediction.
A lower loss indicates a better prediction.

Many types of layers exist in a CNN. Some layers perform
a pre-deﬁned transformation such as downsampling while
other layers contain a model, W, and perform an operation
parameterized by the model. Models are also known as weights
or parameters. The models of all layers constitute the entire
set of weights or parameters of the CNN, i.e.,

W = WCNN = {WL1 , . . . , WLP }.

B. Stochastic Gradient Descent

The goal of CNN training is to optimize the model W in
order to minimize the loss function (cid:96)(RP , C), also denoted as
(cid:96)(W, I, C) to make the fact that RP is a function of W and I
explicit. Low loss is correlated with high prediction accuracy
and in this work we refer to both. The most popular training
algorithm for CNNs is an iterative technique called stochastic
gradient descent (SGD). Each SGD iteration consists of a
forward and backward pass.

The input to each SGD iteration is an image-label tuple
(I, C) as described above. The forward pass calculates the
prediction RP of I using equation (1), and then the prediction
error compared to C is used to calculate the gradient (or
derivative) of (cid:96) with respect to RP . We denote this gradient
as ∇RP (cid:96). Now the cascade of equation (1) runs in reverse by
applying each layer in the “backward” direction:

LBW
1

◦ LBW
2

◦ . . . ◦ LBW

P −1 ◦ LBW

P

(∇RP(cid:96))

(2)

equation (2) implements the chain rule of calculus. The BW
operation of layer p takes as input a data gradient ∇Rp (cid:96) and
outputs a data gradient ∇Dp (cid:96). Internally, it also updates that
layer’s model WLp by (i) calculating a gradient of the loss
with respect to the model, ∇WLp (cid:96), and (ii) using an SGD

Fig. 1. Abstracting a CNN into two phases.

update on the model. SGD repeats for many, often millions
of iterations, until the loss is sufﬁciently low, i.e. the model
is sufﬁciently optimized. The initial model W (0) is randomly
initialized. The SGD update at step t takes the form

W (t) ← W (t−1) + V (t),

(3)

where the new step, V (t), consists of a scaled version of the
previous step, V (t−1), plus a gradient calculated with (I, C):
(cid:16)

(cid:17)

(cid:104)

V (t) ← µV (t−1) −η

∇W (cid:96)

W (t−1), I, C

+ λW (t−1)(cid:105)

(4)

In Section IV we introduce the notion of asynchronous up-
dates. The main change in equation (4) under asynchrony,
is the use of an older model, W (s), when evaluating the
gradient ∇W (cid:96). This gradient (speciﬁcally, its negative) is the
direction to “step” within the parameter space each SGD
iteration. The learning rate, η, is the scale factor applied to the
magnitude of this gradient, i.e. the size of the step. λ dictates
the amount of regularization, which is an input to the training
problem (part of the CNN model to train). µ is the amount
of explicit momentum we add to the update. Momentum is
used to “accelerate” learning in the directions common to each
gradient by keeping a history of past gradients and adding this
history to the gradient of the current step, with past gradients
decayed exponentially. Commonly, in order to produce more
stable gradients, each SGD iteration does not process a single
tuple (I, C), but a batch of b tuples, e.g., 256, in which case D
and R become 4D. The gradients from each tuple are summed
to produce a single, combined gradient for that iteration.

Selecting the right values for hyperparameters (η, µ, b) is
critical for performance. We will describe a simple optimizer
to pick these parameters.

C. CNN Computation

Of all the CNN layers, two layers are the most compu-
tationally intensive, convolutional (conv) and fully-connected
(FC) layers. A convolutional layer performs many independent
convolutions over an image, i.e., several sliding window oper-
ations; an FC layer performs a dense matrix multiplication. In
many popular implementations, the bottleneck in both layers
is a matrix multiply implemented as a call to either a BLAS
or cuBLAS library. For our study, their data properties are
more important, and we refer to Chetlur et al. [19] for a more
detailed description of their use in machine learning.

that convolution layers repeat

Figure 1 illustrates a state-of-the-art CNN, in which all
convolutional layers always appear before all fully-connected
layers. In this work we introduce an abstraction which sepa-
rates a CNN into two phases, each consisting of a number of
consecutive layers: ﬁrst the convolution phase (conv), whose
layers have large data tensors D and R (e.g., 100MB-1GB)
and small models W (e.g., 5-50 MB), followed by the fully-
connected phase (FC), whose layers have small D and R (e.g.,
1-10MB) and large W (e.g., 30-300 MB). The reduction in
data size is in part due to pooling layers in the convolution
phase which perform down-sampling. The increase in model
size is due to the fact
the
same weights for the sliding window. Note that our two-phase
categorization also applies to modern, non-linear CNNs [20].
The computation for both the conv and FC phases is usually
compute-bound although the conv phase contains signiﬁcantly
more computation (e.g., in AlexNet conv is 1.6 TFLOPs and
FC is 80 GFLOPs, or 95% of the computation is convolution).
Within a machine, each layer’s computation can be mapped
to CPUs, GPUs, or a combination of both. In addition, this
computation can be parallelized either using data parallelism
(partitioning the data batch and replicating the model, which
works well for the conv layers) or model parallelism (par-
titioning the model and replicating the data batch, which
works well for the FC layers). In distributed settings, the layer
computations are mapped across machines. We will show later
this mapping is always done at the coarser granularity of entire
phases (conv or FC) because it reduces network delays due to
the distinct model and data sizes of the two phases. We study
the choices of mapping and parallelization techniques for both
single and multiple devices in Section III and IV.

D. Problem Deﬁnition

We study systems tradeoffs to build an optimizer for the
most widely used networks/algorithms. We focus on SGD
due to its popularity, although our optimizer applies to other
algorithms as well. More precisely, we are given as input
the following: (i) a CNN architecture {L1, ..., LP }, including
regularization, (ii) a dataset D consisting of data batches,
(iii) a device graph G in which vertices are hardware devices
(speciﬁed by their throughput) and edges are communication
speeds between devices. Our goal is to design an optimizer
which creates a plan for physical mapping and execution
strategy in order to train as quickly as possible.

A plan for physical mapping maps the computation (both
FW and BW) of each layer to vertices (e.g., GPUs or CPU
cores) of the device graph. A plan for the execution strategy
maps data batches to devices in order to parallelize SGD in
the multi-device case. The key decision here is the degree of
asynchrony in execution. Section III and Section IV study how
to do physical mapping within a device and across devices,
respectively. Section IV also studies the impact of asynchrony.
If the cluster is homogeneous, we do not need the explicit
device graph – instead, a few parameters, such as the number
of devices and the throughput of each device, are enough to
specify the cluster for the optimizer.

Fig. 2. The computation performed by a convolutional layer which processes
bp images at a time in parallel, where 1 ≤ bp ≤ batch size. The convolutions
can be implemented either directly (top) or using the lowering/GEMM method
(bottom).

III. SINGLE-DEVICE TRADEOFF

that

is proportional

We ﬁrst study the systems tradeoffs within a single device.
We show that for each device (GPU or CPU) we can achieve
throughput
to its peak FLOPS. This
enables the distributed optimizer to treat each device as a
black box in Section IV. This is not a trivial property for deep
learning: many existing systems [21]–[23] use either CPUs
or GPUs, but they often report that GPU implementations
are an order of magnitude faster than CPU even when the
devices offer similar FLOPS. Therefore the challenge is to
utilize the FLOPS on the CPU. We study the key kernel in
CNN implementations, which is compute bound. We introduce
a data batching technique which trades off memory footprint
for compute time and demonstrate that this tradeoff gives
a more than 5× CPU speedup over existing systems. With
this optimization now both CPUs and GPUs give throughput
proportional to the FLOPS offered by the device.

A. Convolutional Layer Computation

As reported in the literature and conﬁrmed by our ex-
periments, the most computationally intensive layers in the
CNN are the convolutional layers. Together, all convolutional
layers in a CNN often consume between 70-90% of total
execution time. Recall
layer contains
a model, which we will also call its kernel and denote as
K. A convolutional layer accepts as input a 4D data tensor
D ∈ Rn×n×din×b, where recall b is the batch size. K is also
a 4D tensor, K ∈ Rk×k×din×dout. The output is a 4D data
tensor R ∈ Rm×m×dout×b, where:

that a convolutional

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Rx,y,z,w =

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48),wKx(cid:48),y(cid:48),d(cid:48),z

(5)
Like most HPC kernels a straightforward implementation is
suboptimal, and many optimized implementations of this con-
volution kernel exist [19], [21], [24]. A popular implementa-
tion is to perform equation (5) as a dense matrix multiplication

Device Type
(EC2 Instance)
1× CPU Xeon E5-2666
(c4.4xlarge)
2× CPU Xeon E5-2666
(c4.8xlarge)
1× GPU Grid K520
(g2.2xlarge)
Dual-GPU Grid K520
(g2.8xlarge)

Device % Peak

Caffe Omnivore

% Peak % Peak
SGEMM

GFLOPS

742

18%

1,670

1,229

2,458

8%

53%

26%

56%

40%

54%

52%

81%

71%

99%

99%

Fig. 3. Across several CPUs and GPUs we obtain throughput on convolution
layers that is ∼ 50% of the device peak (shown for AlexNet, total F W +BW
time for all conv layers). We also show large GEMM as a reference of what
the device can achieve.

(also known as GEMM, general matrix multiply), which [19]
demonstrates to be versatile and fast for a range of size
parameters, as GEMM kernels are highly optimized.

the data and model

In order for equation (5) to be carried out as a matrix mul-
tiplication an initial reformatting and replication step called
lowering is required to put
into the
correct format. Figure 2 shows the three logical steps in the
lowering process: (i) lowering, which transforms 3D tensors
D and K into 2D matrices ˆD and ˆK; (ii) matrix multiply,
which multiplies ˆD ˆK to get the result ˆR; and (iii) lifting,
which transforms ˆR back to a tensor representation of R. The
lifting step is fast (reformatting), but the lowering step requires
replication of the data, sometimes by a factor of 1 or 2 orders
of magnitude. This blowup in the data size demands more
off-chip memory and more computation in step (ii).

B. Batching and Data Parallelism

The design tradeoff between CPUs and GPUs arises as
a result of this increase in data size. Assume that we are
given a ﬁxed batch size of b images (e.g., b = 256). GPUs
cannot ﬁt an entire batch of lowered data into off-chip memory,
therefore many CNN implementations on GPUs perform low-
ering/GEMM serially on one or few images at a time until all
b have been processed. On the CPU however, off-chip memory
is larger which allows lowering/GEMM to be performed on
all b images at once. This leads to signiﬁcant CPU speedups
compared with state-of-the-art tools which do not explore this
tradeoff but use the same implementation suited to the GPU
for the CPU. In particular, as Figure 3 shows, this allows us
to view a CPU or GPU as simply a device producing FLOPS;
for reference, we also include the FLOPS delivered by the
most popular CNN framework Caffe [21] and the call of an
optimized single-precision matrix multiply (SGEMM) for that
hardware. The throughput obtained by Omnivore on all devices
in Figure 3 also matches the expected range for CNNs (on
GPUs) reported by the manufacturer [19].

To achieve this throughput on the CPU, we use a simple
convolution batching technique in which we ﬁrst lower all
b images in a batch before performing any GEMM. After
this lowering, we perform a single GEMM for all b images,
rather than b smaller GEMMs. This consumes b× the memory
because the ˆD matrix is b× larger than when lowering images
one by one. However, it has two speed beneﬁts: (i) one large
GEMM is faster than b smaller GEMMs because CPU caches

Fig. 4. The impact of batch size (bp) and number of threads on the GEMM
kernel (total batch size b = 256, 8 physical cores). Shown for the GEMM in
the largest convolutional layer of AlexNet.

and vector instructions are fully utilized, and (ii) lowering all
images in the batch at once enables data parallelism to be used
during lowering. Speciﬁcally for (ii), given n CPU cores, a
batch is split into n partitions, with b/n images per partition.
A separate thread then performs lowering and GEMM on each
partition, i.e. each thread performs convolution on a subset of
the batch. This data parallelism can be applied to other layers.
Generalizing this implementation tradeoff, bp images can
be processed in parallel by the convolution layer, where
1 ≤ bp ≤ b. Increasing bp increases the memory footprint but
decreases the overall execution time. Figure 4 shows batching
experiments for a CPU GEMM kernel. All points in each graph
perform GEMM on 256 images (i.e., b = 256), but the number
of total GEMM calls depends on bp (e.g., if bp = 256, there
is one large GEMM). We therefore advocate the strategy of
selecting bp as large as possible (up to b) such that ˆD ﬁts into
the off-chip memory. This can be predicted because memory
usage increases linearly with bp as seen in Figure 4 (c). For a
range of modern CPUs that we used in our experiments, the
optimal bp value is always b.

While this tradeoff is simple, it enables FLOPS proportional
scheduling which allows us to abstract away the details of
devices in our distributed implementation.

IV. MULTI-DEVICE TRADEOFF

In this section, we study the distributed setting. Given a
CNN, an input set of data and a set of devices, our goal is to
map each layer of the CNN and a subset of data to each device.
Many distributed CNN systems [3], [5]–[9] can be mapped to
points within our tradeoff space.

We show that a key tradeoff is the degree of asynchrony. We
build models predicting how hardware and statistical efﬁciency
are affected by asynchronous execution. By decoupling hard-
ware efﬁciency and statistical efﬁciency and creating separate
models, the optimizer can ﬁnd a balance and minimize the
total time to convergence. Speciﬁcally, we argue for an analytic
model for hardware efﬁciency and we are able to give a new
theoretical characterization of statistical efﬁciency.

A. Setup and Assumptions

We separate each layer into compute servers, responsible
for computation, and a model server responsible for handling
reads from and writes to the model. These “servers” are
virtual: many servers may be mapped to the same device
or an individual server may be mapped to several devices.

serialized into a chain, hence we view the input as a list of
layers (without loss of generality).

Execution: Given a list of layers, the main computational
loop is to move through the list forward calling the forward
operation and in reverse order calling the backward operation
at each step. As we have decomposed each layer into a model
server and compute servers and further mapped compute
servers to compute groups over several devices, it is the re-
sponsibility of our execution engine to make sure that all data
is on each device when needed. We use standard techniques
to hide latency of device copies, e.g., double buffering.

B. Hardware Efﬁciency Model

The goal of this section is to create a predictive model
HE(S) for hardware efﬁciency. The goal is to characterize
the impact of the amount of staleness S in the system (or
equivalently the number of compute groups, g). We derive a
simple analytic model which reasons about the bottlenecks.
An execution strategy partitions the N conv devices into g
compute groups. Again for concreteness, assume there are k
devices per group (k = N/g). Let tconv(k) be a function
that returns the time that a group of size k needs to compute
the convolution phase. We make the assumption that FC only
operates sequentially3. Note that the number of requests to the
FC phase is a function of the number of groups, and let tf c
be the time that the FC phase needs to serve one group.

Given g, tconv(k) and tf c, our goal is to create a hardware
efﬁciency model which predicts the time per iteration. There
are two cases depending on which phase is the bottleneck.
(1) When the FC phase is saturated, i.e., it starts to serve the
next request immediately after the previous request ﬁnishes,

Time per iterationsaturated fc = tf c;
(2) When the FC phase is not saturated, each conv group
becomes the bottleneck. In this case,

Time per iterationsaturated conv = (tconv(k) + tf c)/g
which is the total time for a single iteration divided by the
number of parallel groups. Thus, our predicted model for
iteration time, HE(g), is:

HE(g) = max{tf c, (tconv(k) + tf c)/g}

Given tconv(k), tf c and the number of groups, g, the model
can now predict what the mode of saturation will be and
therefore the time per iteration.

Obtaining the Parameters: The parameters above can
be measured with high accuracy and low variance. tf c can
be measured by running an iteration on a single device, but
tconv(k), though still directly measurable, requires measure-
ments for each k. Instead, tconv(k) can be calculated from (i)
the throughput of each node; (ii) the network speed; and (iii) a
measurement of tconv(1) (which only needs to be measured for
a single k, k = 1, and on a single device). Figure 5(b) shows
that our hardware efﬁciency is accurate; detailed evaluation
in the appendix.

3 For simplicity, we assume different groups (batches) cannot be executed in parallel

on the FC server but our model can be extended to more general cases.

Fig. 5.
(a) Default physical mapping used by Omnivore. (b) Predicted and
measured iteration time as the number of devices (machines) per group
changes (32 c4.4xlarge machine, AlexNet)

While the separation of compute and model servers is present
in all systems, only project Adam [5] described mapping
both compute and model servers to the same device (or set
of devices for parallelism), i.e. merging the servers, which
they did for FC layers. Figure 5 (a) shows this mapping. For
concreteness, suppose there are N + 1 devices: one device
handles the FC layers, and the remaining N devices handle
the conv layers (motivated by 90 − 95% of the computation
being in the convolutions). To simplify our exposition we refer
to this reference architecture throughout Section IV. Section V
demonstrates the beneﬁts of this architecture empirically both
in terms of hardware and statistical efﬁciency.

Compute Groups: The input data to a layer is divided
into batches (whose size is determined by the system). The
main computational operation for each layer is to (i) read the
model; (ii) compute an operation on the data (the forward
or the backward pass) for the given a batch of data; and (iii)
update the model. We assign each compute server to a compute
group. Within a compute group, many devices speed up the
computation of an individual batch of data, e.g., all devices
compute a single gradient for a batch. Across compute groups,
different devices process distinct batches.

Asynchrony and Staleness:

In most systems, compute
groups communicate asynchronously [3], [5], [25]: the models
locking or barriers, and so forward
are updated without
and backward passes may be computed with a stale model.
Intuitively, the lack of coordination allows one to make better
use of the hardware (better hardware efﬁciency) but the use of
stale information may reduce statistical efﬁciency. If there are
g = S + 1 compute groups, we call S the staleness parameter.
This is justiﬁed as the computation within each group in step
(ii) above is very regular for CNNs (standard deviation of
runtime is less than 6% of mean), hence these groups execute
in a nearly round-robin fashion.

Throughout this section, we make two simplifying assump-
tions: First, we focus our description on the two phases
described in Section II, i.e., we abstract networks into con-
volutional layers (conv) which have a large amount of data
but a small model, and fully-connected layers (FC) which
have small data but a large model. Practically,
these two
layers are the main bottlenecks in current networks. Their
differing characteristics give rise to different points of the
tradeoff space. Second, many popular networks are simply a
single chain, not a DAG (e.g., AlexNet), and DAGs can be

C. Statistical Efﬁciency Model

We now characterize the effect of staleness on statistical
efﬁciency. While working on Omnivore, we realized that the
momentum value that yields the fastest convergence decreases
as we increase the number of asynchronous workers. Using the
right value can signiﬁcantly reduce the number of iterations
to reach a target loss. This motivated us to mathematically
analyze this phenomenon in our companion theory paper [17].
The result
is surprisingly elegant: under a simple model,
asynchrony introduces an extra momentum term in the SGD
update. This comes in agreement with our experimental ﬁnd-
ings in this paper. Here, we recap the essential result of
our theory and validate it experimentally on different systems.
Importantly, our result is predictive. We use it in this work to
complement our experimental ﬁndings on the importance of
momentum tuning and to design the ﬁrst asynchrony-aware
optimizer for deep learning systems in Section V.

We make the following assumptions, which are not neces-

sary but are helpful to capture the essence of our result.
(A0) The batch for each step is drawn uniformly at random
with replacement. This is a standard assumption of SGD.
(A1) Variations in the time to process a step are due to unmod-
eled system behavior. Also, variations are independent of
the speciﬁc batch drawn. This is justiﬁed by the fact that,
for all batches, all computation involves dense operations.
takes to process a step is exponentially
distributed and independent from other steps. This is
a simplifying but standard assumption from queuing
theory [26]. A more general (and complex) version of
Theorem 1 below holds without this assumption.

(A2) The time it

Theorem 1: Consider g asynchronous groups and set explicit
momentum to zero, i.e. µ = 0 in the update of equation (4).
Under the above assumptions, the expected update becomes

EV (t+1) =

1 −

EV (t) −

E∇W (cid:96)(W (t)).

(6)

(cid:18)

(cid:19)

1
g

η
g

In which (cid:96)(W ) denotes the expectation of (cid:96)(W, I, C) over the
random draw of possible batches (I, C).

In plain English, asynchrony increases momentum–there is
implicit momentum of 1−1/g. This not only matches the stan-
dard form of momentum used by deep learning practitioners in
equation (4), but also can predict measured system behavior.
Figure 6 shows the predicted and actual measured momentum
for two datasets: As long as the asynchrony-induced implicit
momentum is less than the optimal total momentum, we can
algorithmically compensate with explicit momentum. When
however, implicit momentum exceeds the optimal total mo-
mentum, we start incurring statistical inefﬁciency. We use this
intuition as the basis for our optimizer.

Cold-start: We observe a phenomenon similar to burn-
in [27] in Gibbs samplers. The model needs a few iterations
to set
the appropriate scale of the model parameters. On
Imagenet-1000, we ﬁnd that 5 passes over the dataset (< 15%
of total execution) sufﬁce to “warm up” the model. As a result,
the optimizer will start by running synchronously and then
switches to asynchronous (Section V).

Fig. 6. Predicted and measured momentum moduli: (Left) Predicted, Theo-
rem 1 (Middle) Measured, CIFAR (Right) Measured, ImageNet

V. DISTRIBUTED OPTIMIZER

This section uses the models and tradeoff space charac-
terization of the previous two sections to create (i) a plan
for physical mapping which maps each server to a machine,
and (ii) a plan for the execution strategy which deﬁnes the
number of compute groups by allocating data batches to each
server. As in previous sections we assume a ﬁxed number of
machines. We ﬁrst discuss the process of physical mapping
and then describe our optimizer. We conclude with theoretical
and empirical justiﬁcation for the optimizer, and in Section VI
compare it to state-of-the-art, Bayesian approaches.

A. Physical Mapping

We observe that in all CNNs the architecture of Figure 5 (a)
works best, and describe other physical maps in the appendix.
Omnivore maps the FC compute and model servers to the
same machine, an approach we call merged FC. Merging the
FC compute and model servers to the same devices was shown
in [5] to reduce communication overhead in a CPU cluster
(better hardware efﬁciency), however it was not known that (1)
this also beneﬁts hardware efﬁciency for a GPU cluster, and (2)
this technique also beneﬁts statistical efﬁciency by eliminating
staleness in the FC model. These are both observations we
make. The remaining machines are used for the conv compute
servers. The conv model servers are mapped to one of the
conv compute machines. These optimizations are critical: on
a cluster of 33 EC2 c4.4xlarge machines, not merging the
FC servers incurs an additional hardware efﬁciency penalty of
1.2× due to increased communication as well as a statistical
efﬁciency penalty of 2.5× because of staleness in the FC
model. The key tradeoff is therefore the number of conv
compute groups, which we describe next.

B. Optimizer

There are multiple interdependent factors that have impact
on the performance of the training procedure: (1) the number
of compute groups; (2) the momentum; and (3) the learning
rate. The optimal setting of these parameters might also change
during training, so our optimizer runs periodically in epochs
(e.g., every hour). Algorithm 1 shows the end-to-end optimizer
that runs after the initial cold-start period.

In each of the epochs, the key issue is to set the number of
compute groups, g. We perform a grid search over both the
learning rate and the momentum starting at a particular value
of g. This search determines the optimal explicit momentum
for that g by selecting the conﬁguration with the lowest ﬁnal

Algorithm 1 Automatic Optimizer for the Tradeoff
Input: Time budget T and possible choices of (1) # compute groups

CG, (2) momentum M, and (3) learning rate H.

Output: Trained model W .

(µ, η) ← gridSearch(M, H|W, g)
while µ = 0 and g > 1 do

1: g = CG
2: while not reaching the termination criteria do
3:
4:
5:
6:
7:
8:
9: end while
10: return W .

end while
W ← train(g, µ, η, W ) for T minutes

g ← g/2
(µ, η) ← gridSearch(M, H|W, g)

loss. The key intuition is: set the highest amount of asynchrony,
g, such that this explicit momentum is non-zero. The reasoning
is that, when the optimal explicit momentum is 0, the implicit
momentum is likely higher than the optimal value, and a
cause of statistical inefﬁciency (c.f. Figure 6). In this case, we
reduce the amount of asynchrony by reducing g. We provide an
initial value for g by leveraging the hardware efﬁciency model.
In particular, we start with the smallest number of compute
groups that saturate the FC server. This can be determined
analytically or through measurements during the cold start
phase. Having selected a (g, η, µ), we run for the rest of the
epoch. At the end of one hour, the epoch ends, the model is
checkpointed (written to disk), and the optimizer repeats.

Importance of Compute Groups: We demonstrate that
using the right number of compute groups has an impact on
performance. Fixing the total number of machines, we try
different numbers of compute groups on CPU-L (Figure 9) for
the Imagenet 1000-class dataset, and AlexNet CNN. We grid-
search a number of values for the learning rate and momentum
and report the best result achieved in each case. Figure 7
reports (a) the time per iteration (hardware efﬁciency), (b)
the number of iterations to reach a speciﬁc training loss
(statistical efﬁciency), and (c) their product, which is the total
time required to reach the ﬁnal loss. Note that the hardware
efﬁciency curve in (a) is the same as in Figure 5 (b).

We see in Figure 7 (c) that g = 32 (fully asynchronous) is
3.7× faster than g = 1 (synchronous) as measured by wall-
clock time to ﬁnal loss. This is due to its 6.7× faster iteration
time in (a), although it requires 1.8× more iterations as shown
in (b). This matches the theory’s prediction:
increasing g
causes the optimal explicit momentum, µ∗, to decrease. We
noted in Figure 6 (right) that µ∗ drops to 0 at g = 32,
and consequently there is a penalty in statistical efﬁciency.
Running the optimizer of Algorithm 1 selects g = 4, which
is near-optimal: 5.3× faster than sync and 1.4× faster than
async. We repeat the same experiment for CIFAR and ﬁnd the
results are similar. In all cases, the optimal number of groups
is more than 2× faster compared to sync, and that Algorithm 1
always picks a near-optimal point strictly better than sync.

Fig. 7. Hardware efﬁciency, statistical efﬁciency, and total time for various
execution strategies.

VI. EXPERIMENTS

We evaluate the runtime performance of our system. We
show that, Omnivore outperforms state-of-the-art
tools by
1.9× to 12× on a range of training tasks. Our result holds (i)
across a diverse range of hardware, including both CPUs and
GPUs, (ii) in both single device and multiple device/machine
settings. Moreover, Omnivore contains an automatic optimizer
and therefore does not require users to input hyperparameter
values.

Our experiments also validate that our optimizer is up to

6× faster compared to state-of-the-art Bayesian optimizers.

A. Experiment Setup

Datasets and Models: We validate the performance of
Omnivore on a diverse range of datasets and models, as shown
in Figure 8. The largest corpus we use is ImageNet [28], which
contains 1.3M images. ImageNet is the de facto benchmark
for deep learning systems [2], [3], [5]. Training a model
on ImageNet can take tens of hours—even on the latest
7 TFLOPS Titan X GPU, NVIDIA reports that it took three
days to train with a single GPU.4 For some of our experiments,
which require running many conﬁgurations to convergence,
we used a reduced version, ImageNet8, containing the ﬁrst 8
classes. We train the standard CaffeNet5 on both data sets. We
also use smaller, but standard, datasets CIFAR and MNIST,
which are for object recognition and handwriting recognition,
respectively. We train the networks in Caffe’s tutorials for both.
Metrics: Our main metric of performance is the wall-
clock time required to achieve a given training accuracy. As
in Section V, we deﬁne statistical efﬁciency as the number of
iterations needed to achieve that training accuracy.

Competitor Systems and Experiment Settings: We com-
pare Omnivore against a range of existing tools using both
a single-machine and multiple machines. Single machine:
we compare Omnivore to Caffe [21] and Google’s Tensor-
Flow [22], the two most popular CNN systems, using Caffe’s
reference (CaffeNet) model. Multiple machines: we compare
Omnivore to MXNet [6] and SINGA [7], two popular dis-
tributed deep learning systems.6 Both MXNet and SINGA
support multiple execution strategies, and we consider all
of these strategies in our experiments. We set up and tune

4https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/
5https://github.com/BVLC/caffe/tree/master/models/bvlc reference caffenet
6 We also surveyed a range of distributed training systems including SparkNet, DL4J,
and others. We found that MXNet is the fastest system on our datasets/tasks. At the time
of writing, the ofﬁcial version of Caffe can only run on a single machine, and TensorFlow
does not contain a distributed example for AlexNet/CaffeNet.

Data Set
ImageNet
ImageNet8
CIFAR
MNIST
Shakespeare

# Images

Image Size
1.3 M 256×256×3
256×256×3
10 K
32×32×3
60 K
28×28×1
60 K
25×1×128
162 K

# Classes
1000
8
10
10
128

Fig. 8.
Statistics of Data Sets. All data sets are image-related except
Shakespeare, a natural language corpus for text synthesization used in our
RNN experiments.

Name
1xCPU
2xCPU
1xGPU
4xGPU
CPU-S
CPU-L
GPU-S

Machines
1 × c4.4xlarge
1 × c4.8xlarge
1 × g2.2xlarge
1 × g2.8xlarge
9 × c4.4xlarge
33 × c4.4xlarge
9 × g2.8xlarge

TFLOPS
0.74
1.67
1.23
4.89
6.68
24.51
44.24

Network
-
-
-
-
1 Gbits
1 Gbits
10 Gbits

$/hour
$0.84
$1.68
$0.65
$2.60
$7.56
$27.72
$23.40

Fig. 9. Summary of Machines and Clusters. TFLOPS are the total TFLOPS
that the given machine/cluster can provide. We are unable to open 33 4xGPU
machines due to limited EC2 availability and therefore there is no GPU-L.

both systems according to their tutorials. On data sets where
SINGA performs strictly worse than MXNet, we omit its result
from the ﬁgure. To demonstrate the merits of momentum
tuning and compute groups on other platforms, we also
implemented these features in TensorFlow in the appendix.

Omnivore is implemented in C++. Caffe, TensorFlow,
MXNet, and SINGA all implement their core components
in C++. We compile all systems with g++ 4.8.2, and use
OpenBLAS 0.2.15, cuda 7.5, as well as both cuDNN v3 and
v4 for competitor systems and report the faster result.

B. Performance Evaluation

We validate that Omnivore has faster execution to the
same quality as existing systems in training deep learning
models. More precisely, Omnivore achieves the same training
accuracy/loss faster, as measured by wall-clock time. We ﬁrst
present our main end-to-end performance results, comparing
Omnivore to the state-of-the-art. Then, we validate our contri-
butions by showing results (i) for a single machine comparing
to Caffe and TensorFlow and (ii) in the distributed setting
comparing to MXNet and SINGA. We evaluate the impact of
our tradeoff space and optimizer in Section VI-C.

1) End-to-end Performance: For large datasets, Omnivore
is faster than state-of-the-art tools. We validate this on Ima-
geNet. We compare Omnivore with MXNet and SINGA. We
train the standard CaffeNet on all systems using both CPU-
L and GPU-S clusters. We time out each run after 8 hours
and report the training accuracy at a given time. We tune all
competitor systems following the ofﬁcial performance tuning
guideline7.8 For Omnivore, we use its automatic optimizer
that does not require any hyperparameters. Because SINGA
is always slower than MXNet in our experiments, we omit it
from this ﬁgure. but discuss it in Section VI-B3.

7https://github.com/dmlc/mxnet/tree/db6f6a9418e5696b04be741a78a47ae877bb5505/

example/image-classiﬁcation and previous work [2], [29]

8 The time for this tuning of other tools exceeds Omnivore’s automatic

tuning time for the cold start phase so we omit these initial tuning times.

Fig. 10. End-to-end Performance of Omnivore and MXNet on ImageNet. We
omit SINGA as it performs worse than MXNet.

System

Caffe
TensorFlow
Omnivore

Machines

1xCPU
1.03×
1×
3.90×

2xCPU
1×
1.05×
5.36×

1xGPU
1.11×
1×
1.04×

4xGPU
1×
1.26×
3.34×

Fig. 11. End-to-end single machine performance across different machines
on CaffeNet. Times are normalized as the speedup over the slowest system
on the same machine (the larger the better). We run cuDNNv3, cuDNNv4
and no cuDNN for Caffe and TensorFlow and report the fastest one.

Figure 10 shows the result. We report sync and async for
MXNet as their documentation suggests trying both. Omnivore
reaches the same accuracy up to 11× faster than MXNet on
the GPU cluster and 12× faster on the CPU cluster. Compared
to sync, Omnivore is 4.5× and 1.9× faster respectively.
This includes the 10% overhead of Omnivore’s optimizer
during the run. The optimizer reduces momentum or learning
rate each time it runs. In the remainder of this section we
conduct detailed analysis of Omnivore, MXNet, and SINGA
to understand this improvement. As we will see, Omnivore’s
optimizer, which searches within the larger tradeoff space, is
the key reason for our system’s performance.

2) Single Machine Experiments: We validate our claim that
Omnivore’s performance is FLOPS-proportional, which means
it scales with available FLOPS, regardless of the number or
type of devices available. We ﬁrst explain the protocol of
our experiments. Then we report our results and discuss the
importance of FLOPS-proportional system performance.

Protocol: We compare Omnivore against Caffe and Ten-
sorﬂow on a single machine. We train CaffeNet on ImageNet
on hardware described in Figure 9. We measure the time each
system needs to ﬁnish 40 iterations of training (following
10 iterations of warm-up), using the standard batch size in
CaffeNet (256 images). Our single-machine optimizations only
affect hardware efﬁciency; the number of iterations needed for
convergence does not change. Hence, we can use time per
iteration as a surrogate for performance.

Results: Figure 11 shows the results. We normalize all
execution times by the slowest system in each column and
report the resulting speedups. We see that on a single CPU,
Omnivore is 3.9× faster than both Caffe and TensorFlow; on a
single GPU, all systems show similar speed. This is consistent
with our observation in Section III-B: TensorFlow and Caffe
use the same strategy for both CPU and GPU, which is optimal
for GPU but suboptimal for CPU. One interesting consequence
of this speedup result is that, although Caffe on 1xCPU is

that a fully synchronous strategy will be fast in terms of
hardware efﬁciency while having the best statistical efﬁciency.
Figure 12(a) shows the results. All systems reach accuracy
60% within 2 hours, and Omnivore reaches 60% the fastest.
Omnivore is 2.3× faster than MXNet (388 seconds vs. 907
seconds). At 3000 seconds, Omnivore already achieves an
accuracy > 99%, while MXNet achieves the same accuracy
after 7000 seconds. The speed up here is also 2.3×. As
expected, both systems chose a fully synchronous strategy, so
statistical efﬁciency is not the cause of this performance gap.
The 2.3× speed up is due to our CPU-based optimization
(Section III) and merging FC servers (Section V-A).10

(Small GPU Cluster: GPU-S) GPU-S is a small GPU
cluster that contains 9 4xGPU machines (36 GPUs). Because
each node is signiﬁcantly (7×) faster than 1xCPU, we expect
the optimal strategy to be more asynchronous, and thus,
statistical efﬁciency to come into play. Figure 12(b) shows the
result.11 Similar to the CPU-S cluster, Omnivore outperforms
MXNet: it reaches 99% accuracy 4.8× faster. MXNet only
supports completely synchronous or asynchronous execution,
and its optimal run uses the completely synchronous strategy.
On the other hand, Omnivore’s optimizer chooses to run with
two compute groups. Had Omnivore chosen the same strategy
as MXNet, it would be 1.7× slower than Omnivore’s actual
choice due to a different choice of the synchronization strategy.
The remainder of the 4.8× gap is due to the physical mapping
(merging FC) used by Omnivore, and this improves both
hardware and statistical efﬁciency: while originally described
as a mechanism to reduce network communication [5], we ﬁnd
that merged FC also improves statistical efﬁciency by reducing
staleness in the FC model.

(Large CPU Cluster: CPU-L) CPU-L is a CPU cluster
with 33 1xCPU machines. Because the number of machines
is large, we expect the synchronization across all machines
would incur a large penalty in terms of hardware efﬁciency—
thus, we expect the optimal strategy to be more asynchronous.
Figure 12(c) shows the result. We see that Omnivore is 3.2×
faster than MXNet to reach 99% accuracy. The best MXNet
strategy was to train completely synchronously; Omnivore’s
optimizer now chose four compute groups. Had Omnivore
it would incur 5×
chosen the same strategy as MXNet,
overhead for hardware efﬁciency but only gain a 2× beneﬁt
for statistical efﬁciency. Also, had Omnivore simply chosen a
fully asynchronous conﬁguration, it would be 3× slower. This
shows the importance of choosing the right number of groups
to balance statistical and hardware efﬁciency.

Impact of Optimizer: In the experiments above, we use
grid search to ﬁnd the optimal strategy for both MXNet and
SINGA. On the other hand, Omnivore relies on the optimizer
to automatically choose the best strategy. Had we not used
the grid search for MXNet and relied on default parameters
the performance gap would be 20× on ImageNet8. This

10 SINGA does not converge to 99% in 2 hours and Omnivore is 11× faster than

Fig. 12. Comparison of Omnivore with MXNet and SINGA on ImageNet8.
We omit the SINGA for CPU-L because it performs worse than MXNet.

7× slower than on 1xGPU, Omnivore is only 1.8× slower
on 1xCPU, which we will see matches the FLOPS ratio of
these devices. Omnivore’s FLOPS-scaling extends to multiple
devices, and the gap with other systems increases for more
CPU sockets (2xCPU) or GPU cards (4xGPU).9

FLOPS Proportionality: The training performance of
CPUs is commonly believed to be an order of magnitude
slower than GPU performance. Literature often reports this,
and we showed that it is the case for Caffe and TensorFlow
on 1xCPU and 1xGPU. We validate that Omnivore delivers
performance proportional to the FLOPS that a device can
provide. As shown in Figure 9, 1xGPU provides 1.7× more
FLOPS than 1xCPU, and Omnivore has a 1.8× gap between
1xCPU and 1xGPU. In other words, regardless of the type of
device, Omnivore performs proportionally to the number of
FLOPS available. We also observe that proportionality holds
for all machines in Table 9. FLOPS-proportionality means that,
using both CPUs and GPUs on the same machine, we should
be able to construct an even faster system. We validate this
by using both CPUs and GPUs on 4xGPU, whose CPU and
GPUs provide 0.67 TFLOPS and 4.89 TFLOPS, respectively.
By using data parallelism across the CPU and a single GPU,
Omnivore achieves an 18% speedup over just using the GPU.
3) Distributed Experiments: We conduct experiments to
understand our end-to-end improvement across three different
clusters described in Figure 9. As we will show, our tradeoff
characterization leads to the performance gains of Omnivore.
We ﬁrst describe the settings and the performance metric
used in the experiments. Then we discuss the optimizer’s
contribution and analyze its decisions across different clusters.
Protocol: We tune Omnivore, MXNet, and SINGA and
run them to convergence under multiple settings. Thus, we use
ImageNet8 that contains the ﬁrst eight classes of ImageNet.
This allows us to grid search all parameters in MXNet and
SINGA, including synchronization strategies and learning rate,
and pick the best run. We do not include the time of our
optimizer, which takes signiﬁcantly less time compared with
the grid search we did for MXNet and SINGA. We run all
systems for 2 hours and measure the training accuracy at a
given time. Figure 12 shows the results.

Results:

(Small CPU Cluster: CPU-S) CPU-S is a
small CPU cluster that contains 9 1xCPU machines. Because
each machine is slow and the network is fast, we expect

9We also run experiments on a 4-socket, 56-core Haswell CPU machine, and Omnivore

11 As of the time of writting, SINGA does not support GPUs and is omitted from

is 13× faster than Caffe.

SINGA to reach 60% accuracy.

Figure 12(b).

is compared to MXNet’s completely asynchronous strategy,
which is recommended in their performance tuning guideline
for networks like AlexNet.

Comparison across Clusters: Omnivore’s optimizer
makes different choices on different clusters. It is interesting
to compare them. As we can see, given the same amount of
machines (CPU-S vs. GPU-S), as devices get faster, Omnivore
tends to choose more asynchronous strategies. Intuitively,
the faster compute nodes get, the easier for the network to
get congested. The fully synchronous approach incurs higher
penalty in that case. On the other hand, given the same
speed of each compute node (CPU-S vs. CPU-L), when the
number of machines gets larger, Omnivore also tends to choose
a strategy that is between a fully synchronous and a fully
asynchronous strategy: (1) when the staleness gets very large,
even a properly tuned, fully asynchronous strategy incurs a
penalty in terms of statistical efﬁciency, and (2) when the
number of machines that need to be synchronized gets larger,
a fully synchronous strategy incurs a penalty in terms of
hardware efﬁciency. Omnivore’s optimizer makes it possible
for us to be robust across different devices and cluster sizes.

C. Tradeoff and Optimizer of Omnivore

We validate the hypothesis that (1) the tradeoffs studied
in this paper and (2) the automatic optimizer have a signiﬁ-
cant impact on the performance of Omnivore. We study the
importance of compute groups, as well as compute-group-
speciﬁc momentum tuning. We also study the effectiveness
of Omnivore’s automatic optimizer for this tradeoff space by
comparing it against a standard Bayesian optimizer.

1) The Tradeoff Space: In this section we demonstrate that
the various dimensions of the tradeoff space have a signiﬁcant
impact on performance. Throughout this work we already
illustrated some of these tradeoffs, so here we only summarize
them and leave the detailed discussion for the appendix. We
see that tuning the learning rate is necessary for convergence
and the physical mapping has both HE and SE beneﬁts. We
also showed that using the optimal number of compute groups
can yield 6.7× speedups compared to fully synchronous and
1.8× compared to fully asynchronous execution. This holds on
a range of datasets and clusters. We also implement compute
groups within TensorFlow and demonstrate the tradeoffs for
the Inception-v3 network in the appendix. We now focus
on the importance of properly tuned momentum, which as
we showed in Section IV-C is a function of the level of
asynchrony.

Importance of Momentum Tuning: We validate that the
correct value of momentum depends on the number of groups.
We expect that properly tuned momentum would outperform
a momentum tuned agnostically to the number of compute
groups. Therefore, we compare different methods of momen-
tum tuning on the optimal number groups for ImageNet8
on CPU-L, which was 4 (recall Figure 12(c)). We ﬁx that
number of groups and (i) set momentum to 0.9 (as reported in
AlexNet [2]); (ii) use the momentum tuned for a synchronous
system; (iii) tune the momentum using Omnivore’s optimizer

Fig. 13. Lesion study of momentum. Default momentum = 0.9, which is also
the optimal momentum for the fully synchronous strategy.

for 4 compute groups. As we see in Figure 13, tuning for the
right amount of asynchrony is important: if Omnivore did not
tune momentum, it would be 1.5× slower. Further experiments
show that tuning momentum can yield speedups of 2×. On
TensorFlow, we observe similar speedups: When momentum
is set to 0.9, synchronous training is faster. When, however,
we perform momentum tuning, the asynchronous conﬁguration
wins with its performance relative to sync improved by a factor
of 2.4× This both veriﬁes our expectations for this experiment
and provides further support for our theory in Section IV-C.
Discussion: Other Models: We ﬁnd that these tradeoffs
are impactful when applied to other models. We ﬁnd that for
Recurrent Neural Network models and LSTM models (e.g.,
the same choices affect performance—for example,
[30]),
choosing a completely synchronous or asynchronous conﬁgu-
ration can be up to 2× slower than the optimal conﬁguration.
This could imply speedups for applications in which RNNs
are widely used, such as handwriting, speech recognition, and
general sequence or time-series data.

2) Optimizer: We validate that our optimizer outperforms
state-of-the-art Bayesian optimization algorithms. We compare
our optimizer with the optimizer proposed by Snoek et al. [18].
We measure both the number of conﬁgurations and the total
number of epochs that the Bayesian optimizer needs to achieve
an accuracy within 1% of the highest accuracy Omnivore
achieves. In our experiments, the Bayesian optimizer never
discovers a conﬁguration which outperforms the conﬁguration
Omnivore obtains by grid search. We found that the Bayesian
optimizer takes on average 12 runs to ﬁnd a near-optimal
strategy, which on average is 6× more epochs than just
running that strategy. Because of this search overhead it was
not feasible to use the full ImageNet 1000 dataset so we used
ImageNet8 (whereas recall from Figure 10 that Omnivore had
an overhead of only 10% on ImageNet 1000). We used the
GPU-S cluster. Typically Bayesian optimizers can amortize
this cost by running in parallel, but here that is not possible
as the parameters depend on the hardware conﬁguration and
so the optimizer needs complete access to the entire cluster.

VII. RELATED WORK
Single Node. Optimizing CNN performance has become a
well-studied problem in recent years. Popular libraries include
Caffe [21], cuDNN [19], TensorFlow [22], Theano [23], and
Torch. To compute convolutions, many of these frameworks
use lowering, an idea proposed by Chellapilla et al. [31] that
takes advantage of highly-optimized BLAS libraries. Our work

follows from this line of research and demonstrates how to
optimize lowering for CPUs in order to build a system which
is robust to different types of hardware.

Distributed Deep Learning. Distributed systems for Deep
Learning popular, with SINGA [7], MXNET [6], FireCaffe [9],
SparkNet [8], DL4J, DistBelief [3], and Project Adam [5]
selecting different execution strategies and other optimizations.
Our study shows a combined tradeoff space on the union of all
these techniques. We did this by decoupling the hardware and
statistical efﬁciency for each technique and optimizing them
separately. Our work is the ﬁrst to provide a theoretical char-
acterization for statistical efﬁciency and to show that hyper-
parameters need to be tuned to compensate for asynchrony.

The idea of decoupling the number of iterations and time
per iteration and analyzing each separately is not new for
distributed CNN systems. MXNet reported hardware efﬁciency
and statistical efﬁciency separately. SINGA went deeper into
the tradeoff, identifying the compute group size as a tunable
parameter. They advocate combining both synchronous and
asynchronous training and offer a ﬂexible training architecture
which enables trading off the convergence rate with the time
per iteration in order to to minimize training time. However,
while SINGA identiﬁes this tradeoff and provides experimen-
tal evidence of its importance similar to the curves we showed,
the user still needs to manually choose a conﬁguration.

SparkNet also separated the time per iteration and number
of iterations by building models of each. They did not explore
the same tradeoff of machines per group, but rather a similar
tradeoff related to staleness. Because SparkNet uses a MapRe-
duce framework they implement model averaging. Within this
technique they explore, in isolation, how the staleness (their
τ parameter) impacts the number of iterations to convergence
and the time per iteration. Their hardware efﬁciency model was
measured (both network speed and compute time) and their
statistical efﬁciency model was also empirical (they varied
staleness and measured the statistical efﬁciency penalty).

[5] T. Chilimbi et al., “Project adam: Building an efﬁcient and scalable deep

learning training system,” in OSDI, 2014.

[6] T. Chen et al., “Mxnet: A ﬂexible and efﬁcient machine learning library

for heterogeneous distributed systems,” arXiv, 2015.

[7] W. Wang et al., “SINGA: A distributed system for deep learning,” NUS

Tech Report, Tech. Rep., 2015.

[8] P. Moritz et al., “SparkNet: Training deep networks in Spark,” arXiv,

2015.

[9] F. N. Iandola et al., “FireCaffe: near-linear acceleration of deep neural

network training on compute clusters,” arXiv, 2015.

[10] Abadi et al., “Tensorﬂow: A system for large-scale machine learning,”

arXiv preprint arXiv:1605.08695, 2016.

[11] H. Cui et al., “Geeps: Scalable deep learning on distributed gpus with
a gpu-specialized parameter server,” in Proc. of the Eleventh European
Conference on Computer Systems. ACM, 2016, p. 4.

[12] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz, “Revisiting distributed

synchronous sgd,” arXiv preprint arXiv:1604.00981v2, 2016.

[13] A. Candel, V. Parmar, E. LeDell, and A. Arora, “Deep learning with

h2o,” 2015.

[14] X. Lian et al., “Asynchronous parallel stochastic gradient for nonconvex

optimization,” in NIPS, 2015.

[15] I. Sutskever et al., “On the importance of initialization and momentum

in deep learning,” in ICML, 2013.

[16] S. Hadjis et al., “Caffe con Troll: Shallow ideas to speed up deep

learning,” in DanaC, 2015.

[17] I. Mitliagkas et al., “Asynchrony begets momentum, with an application

to deep learning,” arXiv:1605.09774, 2016.

[18] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimiza-

tion of machine learning algorithms,” in NIPS, 2012.

[19] S. Chetlur et al., “cuDNN: Efﬁcient Primitives for Deep Learning,”

[20] K. He et al., “Deep residual learning for image recognition,” in CVPR,

[21] Y. Jia et al., “Caffe: Convolutional architecture for fast feature embed-

ArXiv, 2014.

2016.

ding,” ArXiv, 2014.

[22] Abadi et al., “TensorFlow: Large-scale machine learning on heteroge-

neous distributed systems,” arXiv, 2015.

[23] J. Bergstra et al., “Theano: a CPU and GPU math expression compiler,”

VIII. CONCLUSIONS

in SciPy, 2010.

We described the ﬁrst explicit study of the tradeoff space
for deep learning systems, a popular, high-value type of
industrially deployed learning systems. We identiﬁed critical
issues in how one maps layers to devices and are the ﬁrst to
systematically study widely used techniques like asynchrony.
We designed a new optimizer and showed that it has excellent
end-to-end performance and is independent of our particular
implementation substructure. We are collaborating with a
major chip manufacturer to apply asynchrony-aware tuning
and compute groups onto new platforms of much larger scale.

[24] N. Vasilache et al., “Fast Convolutional Nets With fbfft: A GPU

Performance Evaluation,” ArXiv, 2014.

[25] F. Niu et al., “Hogwild!: A lock-free approach to parallelizing stochastic

gradient descent,” in NIPS, 2011.

[26] D. Gross, Fundamentals of queueing theory. John Wiley & Sons, 2008.

[27] A. E. Raftery, S. Lewis et al., “How many iterations in the gibbs

sampler,” Bayesian statistics, 1992.

[28] J. Deng et al., “ImageNet: A large-scale hierarchical image database,”

in CVPR, 2009.

[29] L. Bottou, “Stochastic gradient descent tricks.” in Neural Networks:

Tricks of the Trade (2nd ed.). Springer, 2012.

REFERENCES

[30] A. Graves, “Generating sequences with recurrent neural networks,”

[1] L. Deng and D. Yu, “Deep learning: Methods and applications,” FTSP,

arXiv, 2013.

[2] A. Krizhevsky et al., “ImageNet classiﬁcation with deep convolutional

for document processing.” ICFHR, 2006.

neural networks,” in NIPS, 2012.

[3] J. Dean et al., “Large scale distributed deep networks,” in NIPS, 2012.
[4] X. Zhang and Y. LeCun, “Text Understanding from Scratch,” ArXiv,

[32] “Inception in TensorFlow,” https://github.com/tensorﬂow/models/tree/

51238b1b5219a37ba145915efa764cca870e0d9f/inception.

[31] K. Chellapilla et al., “High performance convolutional neural networks

2014.

2015.

IX. ACKNOWLEDGEMENTS

The authors would like to thank Chris Aberger and the
rest of the Hazy Group for their feedback and help, as
well as HyoukJoong Lee, Nadathur Rajagopalan Satish, Peter
Bailis and Benjamin Recht for their thoughtful comments. We
would like to thank Intel, Toshiba and the Moore Foundation
for support along with DARPA through MEMEX (FA8750-
14-2-0240), SIMPLEX (N66001-15-C-4043), and XDATA
(FA8750-12-2-0335) programs, and the Ofﬁce of Naval Re-
search (N000141210041 and N000141310129). Any opinions,
ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reﬂect the views of DARPA, ONR, or the U.S. government.

APPENDIX

The appendix sections mirror the section structure of the

main paper, carrying corresponding supplementary material.

Appendix A includes supplementary information for the
Introduction, most importantly more discussion on the CPU
vs GPU debate.

Appendix B includes a discussion of CNN trends.
Appendix C includes a full

tradeoff analysis on lower-
ing strategies—ﬁrst reported in a workshop paper [16]—that
shows that the strategy we use in this paper works best for
most CNN kernels. It also includes a more comprehensive
treatment of batching and data parallelism for the GPU.
Finally,
it shows that FLOPS-proportionality holds for all
machines in Table 9.

Appendix D includes a full discussion of the tradeoff space,
including terminology, a survey (Appendix D-C) of many
diverse distributed CNN systems [3], [5]–[9] and shows that
they can be mapped to points within our trade-off space.
Appendix D-D includes a proof for the hardware efﬁciency
model we use, describes how to measure necessary quantities
from the system, and shows that it works across a range of
datasets.

Appendix E gives more details on our distributed optimizer:
Appendix E-A includes discussion and experiments on select-
ing the batch size; Appendix E-B describes other physical
mappings we studied and related analysis; Appendix E-D
discusses in more detail the cold-start period of optimization.
Appendix F includes the full setup of our distributed ex-
periments: Appendix F-B contains full details for our small
cluster experiments; Appendix F-C contains a full trade-off
space analysis for ImageNet and CIFAR10; Appendix F-C4
shows that tuning momentum can yield speedups of 2×; Ap-
pendices F-C4 and F-D contain full details on our larger cluster
experiments and discuss the impact of optimizing the hyper-
parameters of competitor systems; Appendix F-E includes
details on the end-to-end experiment on ImageNet1000, as
well as the choices of our optimizer. Appendix F-F describes
preliminary experiments on RNNs; Appendix F-G compares
our optimizer to standard schedules and; Appendix F-H com-
pares to Bayesian hyper-parameter optimization.

TABLE I
TRADEOFF SPACE WHEN DESIGNING DISTRIBUTED DEEP LEARNING
SYSTEMS.

Tradeoff
Single Node Hardware
Type of Parallelism

Batch size

Batch allocation
within node

Batch allocation
across nodes

Combining Model
Replicas

Server Architecture

Network Architecture

Hyperparameter
Optimization

Some Examples
CPUs, GPUs, both together
Model, Data
Large (few accurate updates),
Small (many parallel updates)
1 CPU or GPU per batch,
many CPUs or GPUs per batch
1 batch across all machines,
1 parallel batch per machine,
groups of machines per batch
Atomic gradient updates,
Model averaging, Ensembles,
Race conditions (Hogwild!)
Separate parameter/compute,
Merged parameter/compute
Model Size (impacts
communication),
Model Depth (impacts memory
/ batch size)

Grid/Random search, Bayesian,
Plateau (e.g., ResNet), Decay
Schedule (e.g., Inception-v4)

Optimization Algorithm SGD, Adagrad, momentum

Appendix G, includes our TensorFlow results. We show that
on Inception-v3, momentum tuning can be result-changing:
when momentum is set to 0.9, synchronous training is faster.
When, however, we perform momentum tuning, the asyn-
chronous conﬁguration wins with its performance relative to
sync improved by a factor of 2.4×. We also study the effect
of compute groups.

Appendix H gives a total cost of ownership analysis.
Appendix I includes supplementary discussion from the

Conclusions section.

APPENDIX A
APPENDIX FOR INTRODUCTION (SECTION I)

Tradeoff Table: While many distributed deep learning
systems exist, each of these makes design decisions suited
for a particular type of (1) compute cluster, (2) deep learning
model and (3) dataset, although these same decisions may
not work for other problem settings or hardware resources.
This is because deep learning is known to be complex both
from a computational and machine learning perspective, and
designing a highly efﬁcient and scalable deep learning engine
involves a number of interrelated design decisions. Table I
shows a number of factors to consider when designing dis-
tributed deep learning systems. Given a ﬁxed number of
machines, a number of tradeoffs exist from how to use
hardware on each node, to how to allocate batches of data to
machines, to the type of deep learning model to use in order to
minimize communication. In addition, these decisions impact
one another – for instance the batch size inﬂuences the number
of machines which can be used to effectively parallelize within
a batch and therefore inﬂuences the total number of parallel
gradients being computed to update the model. Our work,
which is a study, demystiﬁes these factors by identifying

the key tradeoffs which underlie all design decisions and
quantifying the impact of those tradeoffs experimentally.

Contribution 1: Single-Node Optimization: Even focusing
on just a single node, there has been a long debate about
CPUs vs GPUs for deep learning. GPUs are popular for CNN
systems because of the high throughput they provide. Modern
GPUs offer between 1.2 TFLOPS (NVIDIA GRID K520, per
GPU) and 8 TFLOPS (NVIDIA Titan Z). However, GPUs
contain smaller off-chip memories than CPUs, and GPUs are
connected to host memory by a slow PCI-e interconnect.
On the other hand, Microsoft’s Project Adam argues that
CPUs can deliver more cost-effective performance [5].12 This
debate is only going to get more interesting, as modern
GPUs offer high-speed interconnect with host memory13 while
Intel’s current Haswell CPU can achieve 1.4 TFLOPS on a
single chip.14. Moreover, SIMD parallelism has doubled in
each of the last four Intel CPU generations and is likely to
continue.15 Our work is the ﬁrst to conduct a systematic study
to understand the relative performance of CPUs and GPUs for
deep learning.

APPENDIX B
APPENDIX FOR BACKGROUND (SECTION II)

A. CNN Computation

AlexNet FLOPS: We approximate the FLOPs (# ﬂoating
point operations) in AlexNet by the sum of all the GEMM
operations with batch size 256. Speciﬁcally, we add 1 GEMM
in the forward pass for each Conv and FC layer, plus two
GEMMs in the backward pass for each Conv and FC layer
(although Conv1 backward has only 1 GEMM because no
gradient is needed with respect to the data).

Terminology: This section introduces model and data
parallelism as two techniques to parallelize CNNs. For a
full description of these concepts, see the Terminology in
Appendix D-B.

CNN Trends: This section viewed CNNs as two phases,
Conv and FC. Recent CNNs, e.g., Residual Networks
(ResNets) and Inception Networks, can also be categorized
into this partitioning. For instance early Inception variants
contained multiple FC layers at different parts of the network,
but from a computational point of view these are all considered
to be part of the FC phase.

In particular, CNNs have undergone a number of changes

in the past few years16. We summarize a few here:

• Multiple FC layers replaced with average pooling, leaving
a single fully-connected layer (for the softmax). This
leads to a reduction in the overall model size (e.g., 60
million parameters for AlexNet compared to 4 million
for GoogleNet)

12http://www.wired.com/2014/07/microsoft-adam/
13http://nvidianews.nvidia.com/news/nvidia-launches-world-s-ﬁrst-high-speed-gpu-

interconnect-helping-pave-the-way-to-exascale-computing
14Xeon E5-2698 v3, http://ark.intel.com/products/81060
15SIMD scales linearly in power and area (whereas frequency scaling is cubic) http:

//parasol.tamu.edu/lcpc2014/keynote-tian.pdf.

16http://cs231n.github.io/convolutional-networks/#case

• Increase in network depth (e.g., AlexNet with 5 conv
layers, compared to ResNets17 with > 150). This in-
creases memory requirements of networks, and makes
multi-device training necessary.

As we will see,

the optimizer presented in this paper
considers the impacts of each of these points when making
decisions for physical mapping and execution strategy.

B. Problem Deﬁnition

Scope of Work: Our work does not focus on improving
machine learning techniques but rather studies systems trade-
offs in order to build an optimizer that is robust to the most
widely used networks/algorithms. Our study uses the SGD
algorithm due to its popularity, although our optimizer applies
to other algorithms as well. Similarly we do not modify the
CNN architecture but assume that this is provided by the user.
Terminology: The physical mapping maps the layer
computation to vertices in G. Vertices in G may contain
other vertices, e.g., GPUs or CPU cores within a machine.
Section III ﬁrst studies how to map the CNN to hardware
within a machine, and concludes that with proper optimization
only the throughput of each vertex matters, not its underlying
hardware. Section IV then studies how to map the CNN to all
of G, i.e. across machines.

APPENDIX C
APPENDIX FOR SINGLE-NODE TRADEOFFS (SECTION III)
This section describes the optimizer’s physical plan (how to
map the CNN to hardware) at the level of a single machine.
Given a machine containing devices of various throughput
(CPUs, GPUs), our goal is to run the CNN computation as
quickly as possible. We do this by identifying two tradeoffs.
Our ﬁrst tradeoff introduces a data batching technique which
trades off memory footprint for compute time. We demonstrate
that this tradeoff gives a > 5× CPU speedup over existing sys-
tems. With this optimization now both CPUs and GPUs give
throughput proportional to the FLOPS offered by the device,
and our second tradeoff partitions the CNN computation across
both the CPU and GPU to combine their FLOPS – a known
HPC trick, but one which has not been applied to CNNs.

A. Convolutional Layer Computation

A 3D convolution consumes a pair of order 3 tensors–the
data D ∈ Rn×n×din and the kernel K ∈ Rk×k×din . For
example, if D is a color image with 3 (RGB) color channels,
din = 3. In AlexNet [2], n ∈ [13, 227], k ∈ [3, 11], and
din ∈ [3, 384], The output is a 2D matrix R ∈ Rm×m where
m = n − k + 1 and each element Rx,y is deﬁned as:

Rx,y =

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48)Kx(cid:48),y(cid:48),d(cid:48)

(7)

The kernel also supports parameters called padding and stride,
which affect the size of m. For details on stride and padding
see http://cs231n.github.io/convolutional-networks/#conv.

17https://github.com/KaimingHe/deep-residual-networks

This is the standard image 3D discrete convolution. A
convolution layer in the CNN contains a number of kernels
{Kj}, not just one, where we call dout = |Kj| the number of
output channels. These kernels {Kj} constitute the model of
the convolutional layer, and the reason for computing multiple
kernels rather than just 1 in the convolutional layer is to a
more powerful machine learning model. The convolutional
layer takes as input the 3D data tensor D and performs dout
3D convolutions, one per {Kj}, such that the output of the
convolutional layer is now not a 2D matrix R but a 3D tensor
R ∈ Rm×m×dout . Similarly the model of the CNN can be
viewed as a 4D tensor K ∈ Rk×k×din×dout.

Finally, recall that often rather than process a single data
example the CNN processes a batch of b examples simul-
taneously. The motivation for doing this is that gradient
computations during learning are less noisy. Therefore in the
most general case, the input D to a convolutional layer is not
1 but b 3D data tensors, or equivalently a 4D data tensor D ∈
Rn×n×din×b. The model is unchanged, but the convolutional
layer now performs the dout 3D convolutions on each example
in the batch, i.e. the batched convolution layer performs b·dout
3D convolutions. The output of the convolutional layer is
therefore also a 4D tensor, R ∈ Rm×m×dout×b.

To summarize, a convolutional layer accepts as input a 4D
data tensor D ∈ Rn×n×d×b, performs a total of b·dout 3D dis-
crete convolutions using D and its model K ∈ Rk×k×din×dout,
and outputs a 4D output data tensor R ∈ Rm×m×dout×b. The
full formula is:

Rx,y,z,w =

Dx− k

2 +x(cid:48),y− k

2 +y(cid:48),d(cid:48),wKx(cid:48),y(cid:48),d(cid:48),z

din−1
(cid:88)

k−1
(cid:88)

k−1
(cid:88)

d(cid:48)=0

x(cid:48)=0

y(cid:48)=0

Many implementations of this convolutional

layer exist.
Like most other HPC kernels, a straightforward implemen-
tation of this operation is suboptimal. Optimized implementa-
tions include directly computing the convolutions, as in cuda-
convnet2,18, computing the convolution as a discrete Fourier
transform as in [24], or implementing the convolution as a
matrix multiply, as in Caffe [21] or cuDNN [19].

While the studies in these papers conclude that different
strategies perform best for different kernel sizes, cuDNN [19]
demonstrates that the third technique of performing convolu-
tion as a matrix multiplication is versatile and fast for a range
of size parameters, as matrix multiplication kernels are often
highly optimized.

In order for the convolution to be carried out as a matrix
multiplication, an initial reformatting phase called lowering is
required to put the data and kernel into the correct format,
which we discuss in the next subsection of this appendix.

1) Convolution by Lowering and GEMM: Lowering fol-
lowed by a general dense matrix multiplication (GEMM) is a
popular way to implement the convolution operation. Figure 2
shows the three logical steps in the lowering process: (1)
lowering, which transforms 4D tensors D and K into 2D
matrices ˆD and ˆK; (2) matrix multiply (GEMM), in which

18https://code.google.com/p/cuda-convnet2/

we multiply ˆD ˆK to get the result ˆR; and (3) lifting, which
transforms ˆR back to a tensor representation of R.

Lowering Phase in which we construct the matrix
ˆD and ˆK. A value of D will appear more than once
in the lowered matrices.
Multiply Phase (GEMM) in which we multiply ˆD
and ˆK to create ˆR = ˆD ˆK.
Lifting Phase in which we map ˆR back to R.
Three techniques exist to perform this process, each corre-
sponding to a different way to group the sum in Equation 7.
Each of these techniques requires replicating the data or the
kernel in order to allow the convolution to be implemented
as a GEMM, but the amount of replication and the size of
the GEMM depend on whether the replication happens in the
lowering phase, lifting phase, or partly in each. The tradeoff is
studied in detail by [16], which concludes that the best choice
is determined entirely by the ratio din/dout (the ratio of the
number of input channels to the number of output channels
of the convolution), and that for modern CNNs these ratios
suggest that the data replication should be done during the
lowering phase. Therefore in the lowering used by this work,
there are two components to convolution: lowering (which
requires replication of data), and the GEMM. The lifting
does not require any memory copies or computation in the
optimal technique. Note also that in this technique, the kernel
does not require any replication, only the data. CNNs are
continuously evolving however, and so it is possible that future
CNN architectures will beneﬁt from other lowering strategies.
For a full study of the lowering tradeoff, refer to [16].

The amount of data replication required by the lowering in
this work is m2k2/n2, where m < n and m depends on the
stride and padding of the convolution. The replication can be
on the order of 1 to 2 orders of magnitude (i.e. 10 − 100×
more data). In turn, this blowup in the data size requires more
memory and computation in step 2 (GEMM). The beneﬁt
however is that a sparse computation has become dense,
which is important for hardware implementations because the
direct computation of the 3D convolution is usually memory
bandwidth-bound (due to the small convolution window size,
k ∈ [3, 11]). A GEMM implementation on the other hand,
while performing more computation as a result of lowering,
receives hardware acceleration which eclipses the increase in
data size.

B. Batching and Data Parallelism

1) Batching Analysis: Batching is the implementation
tradeoff that arises between the CPU and GPU as a result
of available off-chip memory. It concerns how many images
of the batch to process in parallel by the convolution layer.
Recall that bp images are processed in parallel, where where
1 ≤ bp ≤ b. b is the batch size, i.e. the total number of
images that need to be processed. The value of bp (how much
to batch the convolution computation) is determined by how
many lowered images can be stored in off-chip memory.

Modern GPU cards cannot store entire batches of lowered
data into off-chip memory and implementations of CNNs on

GPUs perform lowering and GEMM serially on one or few
images at a time until all b have been processed, i.e. bp =
1. On the CPU, off-chip memory is larger which allows for
batching techniques that perform lowering and GEMM on all
images in parallel and therefore allow CPU caches and vector
instructions to be fully utilized. This tradeoff is continuing
to evolve however, as newer GPU cards contain more off-
chip memory (e.g., 12 GB in the Titan X), and also use new
implementations which perform lowering and GEMM without
having to materialize the intermediate lowered representation,
as described by [19]. Therefore we believe that this tradeoff
will grow in importance.

For example bp = 1 in Caffe [21]: in order to process b
images in a batch, Caffe performs convolution on one image
at a time, i.e. lowering/GEMM is done serially for each image
in the batch (lower one image, run GEMM, lower the next
image, etc.) This has the smallest possible memory footprint
as it only maintains the lowered matrix of a single image in
memory, and is the preferred strategy for devices with limited
memory. Figure 4(c) showed that the memory footprint for the
convolution is directly proportional to bp.

Computationally however, Figure 4 (b) showed that bp = 1
suffers from lower hardware utilization. This ﬁgure was run
for the GEMM in the Conv2 layer of Alexnet (although we
observed similar trends for other Conv layers). Speciﬁcally
the matrix ˆD is “B” in the GEMM operation A × B, and
increasing bp increased the number of columns in ˆD.

In Figure 4 (b) we ﬁx the number of threads to 8, vary
bp, and plot the speedup normalized to bp = 1. Increasing
bp reduces the number of total GEMM calls, and this gives
a 2× overall speedup for bp = 256 compared to bp = 1.
This is because a small bp means that ˆD becomes thinner.
For thinner matrices, possible partition sizes of the underlying
GEMM algorithm are smaller and so the kernel cannot run
optimally, for example the L2 and L3 caches cannot be ﬁlled
during blocking optimizations. As a result bp = 1 is more
likely memory-bandwidth-bound than higher batch sizes (and
this phenomenon is likely more severe when the GEMM kernel
is executed with multiple threads.) Also note that Figure 4
was run on a CPU machine with 8 physical cores. Regarding
Figure 4 (a), the reason that 16 threads was slightly slower
than 8 is that we hit the memory bandwidth bottleneck.

For modern CPUs, memory is large and so b = bp. We use

b = bp for the remaining CPU experiments in this section.

2) Data Parallelism in non-GEMM Kernels: On the CPU,
recall that the batching technique above makes a single matrix
that
is b times (full batch size) larger than it would be
for a single image, and then performs a single GEMM on
this large matrix. A related strategy is to split a batch into
multiple partitions, and process each partition in parallel using
a separate thread.

For GEMM, processing an entire batch of size b with n
threads is equivalent to partitioning the batch into p partitions
of size b/p with n/p threads used in each GEMM. For
example, batching b images and performing a single GEMM
with 8 threads is equivalent to creating 8 matrices, each with

Fig. 14. The impact of data parallelism on the end-to-end execution time of
CaffeNet, run with 256 images per mini-batch on an Amazon EC2 c4.4xlarge
instance.

b/8 images, and performing 8 parallel GEMM kernels with 1
thread each. These are equivalent as this is exactly how BLAS
parallelizes GEMM: by partitioning partition columns of B in
A × B and allocating 1 thread per partition.

While partitioning and then performing the GEMM kernel
is the same as simply performing the GEMM kernel, this is
not true for other kernels which are not multi-threaded For
non-GEMM kernels such as lowering, or other layers such
as pooling, the second technique of partitioning the batch
and processing each partition using a separate thread gives
signiﬁcant speedups (this is simply data parallelism) across all
cores. For example, it can be used to lower images in parallel
using all cores by assigning a subset of the images in the batch
to each core.

Figure 14 shows the impact of data parallelism on a full
end-to-end CaffeNet on the EC2 c4.4xlarge instance with 8
physical cores (CPU only). The batch size used is 256 images
and the horizontal axis represents into how many parallel
partitions we partitioned these 256 images for each layer of the
CNN. I.e. the horizontal axis is the number of threads used
for data parallelism. Note that the GEMM kernel is always
parallelized (OpenBLAS was used) and uses the maximum
number of threads (16).

“None” indicates the default Caffe implementation. For all
layers, each image is processed serially. For example, one
image is lowered, then the convolution GEMM is performed,
and then the next image is lowered, etc. The only multi-
threaded kernels are the BLAS GEMM kernels used in the
convolution and FC.

“1” is identical to the Caffe implementation except that
lowering is ﬁrst done for all images (serially, i.e. one image
lowered at a time), and then a single, large GEMM is done
for the convolution. All other layers are the same.

For all other number of parallel partitions p, the 256 images
were equally split into p partitions. For example if p = 2, two
partitions of size 128 images each were created. Then, threads
processed each partition in parallel, one thread per partition.
For example if p = 2, lowering was done with two threads,
each lowering 128 images. Following the lowering, a GEMM

was performed for each partition. The total number of threads
used for these GEMM kernels was always 16, i.e. the GEMM
is performed on each partition with 16/p threads per GEMM.
Figure 14 (None vs 1) shows that batching the GEMM
kernels (one large GEMM as opposed to 256 smaller ones)
saves ∼ 2.2s of convolution time. Then, data parallelism
provides another ∼ 10 s of reduction. The ﬁnal time is then
4s, where ∼ 3s is spend in convolution layers. Therefore
the batching of the GEMM made the convolution roughly
∼ 2× faster (in the optimized execution), and the remaining
speedups were due to data parallelism.

Finally, studying more closely the speedups from data
parallelism, the time reduction from 14 seconds to 4 seconds
from data parallelism was roughly 80% due to speeding up
the lowering, and 20% due to speeding up the other layers.
I.e. the ﬁnal iteration time would be ∼ 6s if data parallelism
was only used for the Conv layers. The remaining 20% is
for data parallelism in pooling, normalization, dropout and
ReLU layers. Fully-Connected layers are simply a GEMM
which always uses 16 threads, so data parallelism and model
parallelism do not apply on the CPU, although as we also saw
previously this GEMM can be made slightly faster by using 8
threads instead for the FC layers (because there are 8 physical
cores).

Overall, batching combined with data parallelism gives
more than a 4× speedup end-to-end when running Caffe’s
AlexNet on 8 physical cores. Importantly,
this end-to-end
speed is now proportional to the peak throughput of the CPU.

In summary, we show two sources of speedup on the CPU.
First, by batching the lowering and GEMM, we perform a
single GEMM which is b× larger, as opposed to b smaller
GEMMs, which as described above has better hardware uti-
lization on the CPU. Second, we apply the batch partition (data
parallel) technique above to parallelize non-GEMM kernels
such as lowering. These optimizations are possible for the CPU
because it has more memory to store the lowered matrices.
As a result the CPU performance is proportional to the device
FLOPS, which allows partitioning computation across both the
CPU and GPU proportional to the device throughput.

C. Device Throughput

FLOPS Experiments: Figure 3 showed throughput for the
CNN when using Caffe, Omnivore and also for reference a
single-precision GEMM kernel. For the CPU the GEMM ex-
periment used OpenBLAS and matrices of size 16384×16384.
For the GPU GEMM we used a GEMM kernel from NVIDIA’s
CUDA examples. For Caffe and Omnivore we focus on only
the Convolution layers, speciﬁcally the time to run forwards
and backwards on all 5 layers of CaffeNet.

FLOPS calculations: The c4.4xlarge instance contains
a single-socket Haswell CPU with 8 physical cores. The
c4.4xlarge instance CPU FLOPS are calculated as: 8 physical
cores × 2.9 GHz ×32 = 0.742 TFLOPs, where 32 is the
single-precision Haswell instructions per cycle (8-ﬂoat SIMD
× 2 FMA per cycle, and FMA is fused-multiply-add).

Each g2.2xlarge instance provides a single Grid K520 GPU,
i.e. 1536 cores ×800 MHz = 1.23 TFLOPS (the Grid K520
contains a total of 3072 cores, 1536/GPU).

The c4.8xlarge instance contains a dual-socket Haswell
CPU with 18 physical cores. The FLOPS are calculated as:
18 physical cores × 2.9 GHz ×32 = 1.670 TFLOPs.

D. FLOPS-Proportional Scheduling

Given that both CPU and GPU speeds are now proportional
to the device FLOPS, we next consider whether the CPU and
GPU can be used simultaneously to process each layer. We
do this using data parallelism (batch is partitioned, model
is replicated) for all layers in the convolution phase, which
is compute-bound and has small data. The tradeoff is what
fraction of the batch to give to each device. We select the
simple but optimal choice that a device should process a
fraction p of the input where p is the proportion of total
FLOPS which that device contributes. e.g., if a CPU provides 1
TFLOPS and a GPU 4 TFLOPS, 1/5 of the batch is processed
by the CPU for an ideal speedup of 20% over the GPU alone.

E. Single-Node Experiments

Omnivore matches Caffe’s output on each layer. It accepts
the same input ﬁles as Caffe and produces the same outputs.
Our experiments compare against Caffe and use the CaffeNet19
CNN, which is Caffe’s implementation of the popular AlexNet
(the default architecture for benchmarking), as well as the
ImageNet dataset.

Both systems take as input the same network conﬁguration
ﬁles that Caffe provides. We remove grouping for convolution
layers because the full AlexNet ﬁts in the memory of a
single K520 (g2.2xlarge) GPU. We use the same external
libraries for both Caffe and Omnivore: GCC-4.8, CUDA-7.5,
and OpenBLAS. For Caffe we report both cuDNN v3 and v4.
We built the same timer into Caffe and Omnivore (measur-
ing wall-clock time, clock_gettime in C). We run for 50
iterations and omit the ﬁrst 10 in case there are disk or other
delays in the ﬁrst few iterations not representative of steady-
state execution. Beyond these ﬁrst 10 iterations we noticed
that all iterations were consistent in terms of time for both
tools and had a coefﬁcient of variation less than 5%.

We also ran Tensorﬂow using the same protocol as above

(40 iterations, burn-in of 10, identical network).

1) End-to-end Performance: Figure 15 shows the results
of running Omnivore and Caffe on various EC2 instances.
Given that Omnivore and Caffe generate the same outputs, we
concentrate on throughput. We ran both Omnivore and Caffe
on each EC2 instance for 50 iterations, and counted the last
40. On the c4.4xlarge CPU instance Omnivore outperforms
Caffe by nearly 4× due to batching and data parallelism. On
the g2.2xlarge GPU instance Omnivore and Caffe achieve the
same speed (within 5%). Note that the c4.4xlarge instance
offers 60% of the FLOPS of the g2.2xlarge instance, and
the ratio of Omnivore speeds on these instances is 59%, i.e.

19https://github.com/BVLC/caffe/tree/master/models/bvlc reference caffenet

×16 physical cores ×2.6 GHz). The ratio of CPU:GPU
FLOPS is therefore 1:2, i.e. we should partition roughly 1/3 of
the data on the CPU and 2/3 on the GPU. Since a batch size
is 256 images, we rounded 67% on the GPU to 75%, such
that the CPU processes 64 images, because this partition is
better suited to hardware (although we see only a 5% speedup
compared to using the exact ratio). This partitioning gives a
18% speedup over just using the GPU.

For parallelization across 4 GPUs, we use data parallelism
for all layers (each GPU given 1/4 of the batch and a model
replica) except for the FC layers, which use model parallelism
(each GPU given 1/4 of the model and a replica of the batch).
We ran Caffe on 4 GPUs with cuDNN v4, cuDNN v3 and
no cuDNN, and found that Caffe was fastest but neither gave
a speedup compared to 1 GPU.

state-of-the-art

Therefore while CNN parallelization is challenging even
for
systems, we’ve shown that FLOPS-
proportional partitioning is possible across a range of hardware
devices. We now extend this technique to multiple machines,
where the added challenge of network delay motivates re-
thinking the SGD algorithm.

Fig. 15. End-to-end performance comparison across EC2 machines on
CaffeNet. All numbers are normalized as the speedup over running Caffe’s
GPU version on g2.2xlarge instance ($0.65/hour).

APPENDIX D
APPENDIX FOR DISTRIBUTED TRADEOFFS (SECTION IV)

Omnivore delivers speed proportional to the FLOPS. On the
two-socket c4.8xlarge CPU instance Omnivore is now 5.5×
faster than Caffe. Caffe does not speed up given the additional
cores, but Omnivore does. The speedup is not linear because
the extra cores are distributed across sockets and not all layers
are compute-bound. However, these results show that given
similar device throughput, CPU CNN performance is not an
order of magnitude slower than GPU performance as the
literature often reports and as is the case in Caffe. Lastly we
compared Omnivore to Caffe on a 4-socket, 56-core Haswell
(non-EC2) CPU machine, and Omnivore is 13× faster than
Caffe.

FLOPS calculations: See Appendix C-C for the FLOPS

calculations in Figure 15.

2) CPU/GPU Hybrid and Multi-GPU: Figure 15 also
shows that using the CPU in a GPU instance can accelerate
purely GPU training. The g2.8xlarge instance’s CPU provides
0.67 TFLOPS, and by using data parallelism across the CPU
and a single GPU we achieve an 18% speedup in Omnivore
end-to-end over just using the GPU. This is faster than all
other single GPU results.

Finally, we also apply this data parallel partitioning across
multiple GPUs. We ran both Omnivore and Caffe using the 4
GPUs on the g2.8xlarge instance and show that while Caffe
actually slows down compared to the 1 GPU case, Omnivore
becomes 3.1× faster.

For CPU + GPU, each g2.8xlarge GPU provides 1.23
TFLOPS as shown above, and the CPU provides 665.6
TFLOPS (Sandy/Ivy bridge, i.e. 16 SP instructions per cycle

Having studied the tradeoffs for a single machine,

this
section now studies the distributed setting. The goal of this
section is to build an optimizer that creates (1) a physical plan
P (A, G) which maps the CNN architecture A to machines in
the device graph G, and (2) an execution plan E(G, D) which
parallelizes SGD by allocating data batches from the dataset
D to each machine. This section begins by describing why
these two are the most important tasks for the optimizer. While
many distributed CNN systems exist [3], [5]–[9] and each
describes their own distribution techniques, upon analyzing
these strategies we discover that,
they all
describe either (1) or (2) above. Given these two fundamental
design dimensions, we then arrange existing strategies into
a tradeoff space and restate our optimizer’s goal precisely
within this space. The remainder of the section then quantiﬁes
the impact of these tradeoffs to allow optimization within the
space.

though diverse,

A. Distributed CNN Tradeoff Space

This section describes popular techniques for distributed

CNN training and reﬁnes them into a tradeoff space.

1) Distributed Stochastic Gradient Descent: Recall

that
SGD iteration i (1) reads a batch of data Bi, (2) uses the
current model Mi to compute the model gradient ∇M (Mi),
and (3) subtracts ∇M (Mi) from Mi to obtain Mi+1. Iteration
i + 1 reads a new batch Bi+1 and the algorithm repeats until
convergence.

Figure 16 (a) shows a common distributed implementation
of SGD in which the entire CNN model is stored on a server
called a model server or parameter server. There are also
a number of compute servers which each perform the CNN

Fig. 16. SGD server architectures.

Fig. 17. A graphical illustration of (a) one worker (S=0) and (b) two workers
(S=1).

calculation (Equations 1 and 2). Each iteration, every compute
server reads M over the network from the parameter server,
as well as a batch of data B from a local database. Each
compute server calculates a gradient ∇M which is sent back
to the parameter server and used to update the model. In
this example, compute servers operate in parallel and do not
synchronize or communicate with one another.

In Figure 16 (a), each server physically maps to a single
machine (node). Generally, multiple servers can map to a
single machine or a single server to multiple machines. For
example, parameter and compute servers can map to the same
node to reduce network communication. Figure 16 (b) shows
a more complex server architecture for SGD in which rather
than two types of servers (compute and model), there are now
4 types: (1) conv compute, (2) conv model, (3) FC compute,
and (4) FC model. 1 and 2 are for layers in the conv phase
of the CNN while 3 and 4 are for layers in the FC phase.
In Figure 16 (b), the FC compute and model servers map
physically to the same machine, i.e. the computation of the
FC phase happens on the same machine as where the FC
model is stored. There are many beneﬁts to Figure 16 (b) vs.
(a): Recall that FC has small data, large model whereas conv
has large data, small model. This server architecture has the
beneﬁt of only needing to communicate the conv model (and
its gradients) and the FC data (and its gradients) across the
network. Second, computation is ofﬂoaded from the compute
machines to the FC model server, which otherwise is majorly
idle. These beneﬁts both improve hardware efﬁciency, and
were described in [5]. However, yet another beneﬁt is that
by having only a single FC compute machine, the FC model
does not experience any staleness – a term we deﬁne next.
This improves statistical efﬁciency.

2) Staleness: Staleness is a metric used to quantify the
statistical efﬁciency (# iterations to convergence). Figure 17
shows staleness graphically for the simple server architecture
of Figure 16 (a). In Figure 17 (a) there is a single worker,
and so that worker always computes the gradient using the
current model. In this diagram, we assume that once a worker
sends the updates back to the model server, it is immediately
sent a new model, i.e. in this diagram, write/read is an atomic
operation for each worker (the write to the model and read

from the model happen together).

In Figure 17 (b), there are now two concurrent workers.
Assume for now that (1) these workers update the model in
round-robin order, (2) write/read is again atomic, and (3) the
server architecture is again as in Figure 16 (a). We notice
that now each worker computes the gradient using a copy
of the model which is stale by 1 iteration, e.g., updating
model 2 to produce model 3, but doing so using a gradient
update which was calculated using model 1. The reason for
this staleness is that while worker 0 is computing its next
update, worker 1 updates the model. This staleness is bad
for statistical efﬁciency (i.e. more iterations are required to
converge) because now each gradient used to update the model
no longer points in the direction of steepest descent for the
model it is applied to, but rather the direction of steepest
direction for a model from an earlier iteration.

Precisely, we deﬁne staleness as follows: given N workers,
the staleness is the number of updates to the model between
a worker’s read from the model and subsequent write back to
the model. Because the updates are round-robin, this staleness
is the same for all workers, and is equal to S = N − 1.
e.g., if there are 100 parallel workers, S = 99. Intuitively,
the staleness is just equal to the number of parallel workers
sending gradient updates (minus one, although this can often
be ignored because the number of workers is large).

The three assumptions above are useful to give a precise

deﬁnition but are not necessary in practice.

Assumption 1:: In practice the workers do not proceed in
round-robin order due to inherent variance across machines,
but we observe empirically that for dense models the updates
are nearly round-robin. This is because dense model compu-
tations like those used in deep learning have roughly constant
time per iteration (this is not true for sparse models).

Assumption 2:: Writes and reads do not need to be
atomic, and in fact this can be beneﬁcial for statistical ef-
ﬁciency. Rather than have a conv compute server request an
entire updated model as soon as it publishes all of its gradients
to the conv model server, it may instead publish gradients
layer-by-layer in a backwards fashion during the backward
pass of iteration i, and then lazily request the updated model
in the forwards pass of the next iteration i + 1. For example,

conv compute worker processes half
the data of a single
batch, using the same model replica, and produces half of
the ﬁnal gradient. A barrier is then introduced in which the
gradients are summed, and a single, ﬁnal gradient is applied to
the model. Figure 18 (b) seemingly solves both problems: all
the hardware is being utilized (good for hardware efﬁciency),
and S = 0 (good for statistical efﬁciency). However the
price we pay is in hardware efﬁciency, speciﬁcally the cost
of synchronization across the machines. Indeed, we will show
that the hardware efﬁciency of Figure 18 (b) is poorer than
that of Figure 18 (a).

We deﬁne a compute group as a group of machines working
together to process a single batch of data. A compute group
is characterized by processing a single data batch at a time,
with all machines in the group using the same model replica,
to the model server. The
and returning a single gradient
compute groups in Figure 18 are shown with black dotted
lines. Figure 18 (a) has 2 compute groups, and since there are
2 machines, the compute group size is 1 machine. Figure 18
(b) has 1 compute group of size 2. Note that this allows us
to simplify our deﬁnition of staleness: because the number of
parallel gradients being computed in the system is equal to the
number of compute groups, the staleness is just equal to the
number of compute groups (minus one).

Generally, if we have N machines used as conv compute
servers, Figure 18 (a) and (b) show two extreme cases.
Figure 18 (a) is the extreme case of 1 machine per com-
pute group, and N groups. This technique is often called
asynchronous SGD, or “async” for short. In async, workers
do not communicate and each worker updates the model
independently. Every worker computes a separate gradient
using a separate batch and separate model replica, and then
sends these gradients to the parameter server in order to
update the model. Figure 18 (b) is the other extreme case
of 1 group, and all N machines in that single compute group.
This technique is often called synchronous SGD, or “sync”.
In sync, all machines work synchronously and in parallel on
a single batch of data and using a single model replica to
compute a single gradient. In this case the gradients over
all workers are aggregated each iteration (or batch) before
updating the model. An intermediate conﬁguration could also
exist, for example one which has 4 groups each of size N/4.
There could even be groups of different sizes if different
machines have different throughput (some GPUs, some CPUs,
etc.). Notice that because the compute group in Figure 18 (b)
parallelizes the conv phase, it uses data parallelism, i.e. the
batch is partitioned across the machines in the group.

4) Precise Problem Deﬁnition: This section has described
two key tradeoffs: (1) the server architecture, concerned with
physically mapping servers to machines, and (2) the execution
strategy (the number of compute groups vs. their size), con-
cerned with mapping batches to servers. We can now restate
the goals of the optimizer from II-D in terms of our tradeoff
space. Given (1) A, the CNN architecture, (2) D, the dataset,
and (3) G, the device graph, our optimizer transforms A into
S, an abstracted network of logical server types (Convmodel,

Fig. 18. Two different execution strategies given a ﬁxed machine budget.

AlexNet may update the model with gradients from conv5,
conv4, conv3, conv2, and conv1, and then begin its next
forwards pass and request conv1, conv2, conv3, conv4 and
conv5. These requests can happen asynchronously from the
computation to hide latency and overlap the network delays
with the computation. As a result the write/read for conv1 may
be almost atomic, but there would be some delay between
the write/read for conv5. This delay in fact reduces staleness
slightly because it reduces the number of intermediate writes
by other workers between a worker’s read and subsequent
write (intuitively, in the extreme case, if the delay was very
large, then every worker would write before any of them
read the new model. Then this is just equal to mini-batch,
except with a larger batch size, although that would of course
make each iteration slower, i.e. harm hardware efﬁciency). In
practice we observed a roughly 20% reduction in the number
of iterations to converge by requesting models in this lazy
fashion (which does not harm hardware efﬁciency).

Assumption 3:: Staleness also applies to the server ar-
chitecture of Figure 16 (b), which recall reduces network
communication by merging the FC model and FC compute
servers, i.e. mapping them to the same machine which does
both the gradient computation and model updates for the FC
phase. This merged FC server processes only one batch at a
time, and thus produces only one FC model gradient at a time.
This means that the staleness for the FC model is 0 which is
good for statistical efﬁciency. The conv compute servers on
the other hand still calculate the updates to the conv model
in parallel (once they receive their data gradients from the FC
machine), therefore the merged architecture in Figure 16 (b)
still contains staleness, but only for the conv model.

3) Compute Groups: The ﬁnal concept common to all
systems is the compute group. Consider the example of two
conv compute machines in Figure 18 (a). This conﬁguration
has a conv staleness of S = 1, as there are 2 workers
independently updating the model. However, it is not necessary
to introduce staleness in order for both compute machines to
be utilized, and Figure 18 (b) shows a second conﬁguration
in which data parallelism is used across the machines: each

of the model replica and 1 replica of the data batch. This is
useful for parallelizing fully-connected layers, which contain
small data but large models: each device receives 1/N of the
model and a replica of the data, and uses the data to compute
a gradient for that portion of the model. These gradients in
total combine to a single gradient with respect to that entire
model, using the data batch.

Note that logically, a single gradient is produced by all
devices together and that single gradient is used to update the
model and produce a new model (physically the updates may
occur locally on each device communication, i.e. the device
updates its portion of the model).

Data Parallelism: A single replica of the data batch and
N (identical) replicas of the model are created. Each device is
given 1/N of the data replica and 1 replica of the model. This
is useful for parallelizing convolutional layers, which contain
small models but large data: each device receives 1/N of the
data and a replica of the model, and together they calculate
a single gradient with respect to that entire model using the
data batch.

Note that, as mentioned above, here the model is always the
same for each device, i.e. the model is replicated and a single
gradient is produced. That gradient is then used to update the
model and the model is then re-broadcast to each device. So
while model replicas exist, they are identical model replicas.
(Note: model replica is a logical term and does not always
correspond to a physical replica. In particular, when two
devices share memory, e.g., 2 CPU threads running on 2 cores,
no physical replica is necessary as the same model in memory
can be read by both.)

2) Asynchronous Batches: The above deﬁnitions apply to
a single data batch in the system (although it may have been
replicated for the purpose of parallelization). Consequently,
there was always one gradient being computed at once, and
so each model replica was identical.

This deﬁnition focuses on the asynchronous case mentioned
previously, i.e. NB parallel batches in the system being used
to compute NB parallel gradients. Each of these NB batches
of data in the system is different. Each batch is allocated to
a compute group (a group of devices), and each batch (i.e.
each group of devices) is given a replica of the model. For
example if NB = N/4, then there will be 4 devices assigned
to each batch, and one model replica assigned to each of these
N/4 batches (i.e. each group of 4 devices). Each compute
group of 4 devices will then compute a gradient given that
model and that batch, and the devices in the group may do
so using either model or data parallelism (i.e. choosing to
create additional model or data replicas within the group, as
described above, e.g., if the group uses data parallelism it will
create 4 additional model replicas, but each of these models
will be identical).

Note that here the number of batches in the system, the
number of compute groups, and the number of parallel gradi-
ents being computed by the system are all the same number.
Within a compute group (group of devices), additional model
replicas will be the same. However across compute groups, the

Fig. 19. An Overview of our Mapping.

Convcompute, FCmodel, and FCcompute), and creates (1) a physical
plan P (S, G) mapping each server in S to machines in G (note
that we also refer to this distributed portion of the physical plan
as the server architecture), and (2) an execution plan E(S, D)
which deﬁnes the number of compute groups by allocating
batches of D to each server in S. Both of these choices impact
hardware and statistical efﬁciency, and our next task is to
quantify this impact. First, we present terminology and then
ﬁnish this section by describing where existing systems fall
within this tradeoff space.

Our mapping from CNN layers to devices is shown in

Figure 19.

We are given: (1) a network, which can be viewed as a
labeled directed acyclic graph in which each node is a type
of layer (e.g., convolution, fully connected, max pooling) and
edges indicate dataﬂow dependencies, and (2) a set of devices
grouped into machines. Our goal is to devise a mapping from
the network to the machines.

B. Terminology

In this work we interchangeably uses the terms node and
machine to refer to a single box, connected to other nodes
or machines over a network. We also interchangeably use the
terms device and (when describing a device graph) vertex to
refer to a discrete unit of compute hardware (e.g., a GPU or
CPU core. When discussing a cluster, a device can also be a
machine in that cluster.). Finally, we also use the terms group
and compute group interchangeably, as deﬁned in Section II.
Existing work also often contains a lot of terminology,
which we summarize here. Consider for these examples the
simple case of N identical compute devices (e.g., N machines,
N GPUs, N CPU cores, etc.)

1) Synchronous Batches: These ﬁrst two deﬁnitions apply
to a single batch of data, i.e. there are not multiple parallel
batches in the system used to compute asynchronous gradients,
but rather a single batch used to compute a single gradient.
As a result, this gradient is applied to the model at once to
produce a new model, i.e. there only ever a single model in
the system (although it may be replicated, in which case all
models are identical). This is also known as the synchronous
case above.

Model Parallelism: A single replica of the model and N
replicas of the data batch are created. Each device is given 1/N

model replicas may be out of sync, because of asynchronous
gradient computations. This is discussed in the ﬁnal deﬁnition.
To make all the deﬁnitions above concrete, consider the
example of 2 devices, e.g., 2 GPUs. There are 3 possible
scenarios: (a) 2 parallel asynchronous batches (“async”), (b) 1
batch with data parallelism across the 2 devices (“sync”), or
(c) 1 batch replicated twice with model parallelism across the
2 devices (also sync).

3) Combining Model Replicas: If there are G asynchronous
compute groups, each using a separate data batch and produc-
ing a separate gradient each iteration, then each group will
also have a separate version of the model as a result of the
asynchronous gradient computations.

then broadcast

Parameter server is a technique in which these separate
compute groups will not perform their model updates locally,
but rather each publish their gradients to a global parameter
server which will
to a
group upon receiving the update for that group. In this way
the groups always have models which are almost the same
(they will still be slightly out of sync because gradients are
published asynchronously and so models are returned to the
groups asynchronously as well). DistBelief, Adam, SINGA,
MXNet use this technique, and it is the focus of our study.

the updated model

Note that if the parameter server waits for each parallel
group to publish a gradient and then broadcasts the model
to all groups (i.e.
this is no longer
introduces a barrier),
asynchronous, and is now equivalent to the synchronous case
above (data parallelism) where the batch size is N times larger
than it is per individual compute group.

Finally, the updates to the server from parallel groups can
happen with or without race conditions. The case of race
conditions is known as Hogwild!.

Model Averaging is a technique in which there is also
a global parameter server, but model updates happen locally
within each group. Then periodically, every τ iterations, the
groups will publish not their gradients but their entire models
to the parameter server. The parameter server will average
the models (reduce step) and then broadcast them to each
group (map step). This averaging does not have theoreti-
cal guarantees because neural networks are non-convex, but
works in practice. This technique works well for map/reduce
frameworks, e.g., Spark/Hadoop, and is used by SparkNet and
DL4J. Here the models are more different than they are in the
parameter server case.

The choice of the τ parameter is similar to the tradeoff
of multiple groups of varying size, except that here staleness
comes not from multiple asynchronous workers updating a
single model, but multiple workers with a separate model
combining models. In the case where τ = 1, this is identical
to the synchronous case of parameter server (all machines in
a single group, i.e. all computing a single batch/gradient).

Ensembles are used by AlexNet. Here each group trains an
entirely separate model to convergence and then predictions
of these models are combined (e.g., through voting). The
gradients or models themselves are never combined.

TABLE II
POINTS IN THE DISTRIBUTED CNN TRADEOFF SPACE CHOSEN BY
POPULAR SYSTEMS. G IS THE NUMBER OF COMPUTE GROUPS, N IS THE
NUMBER OF MACHINES.

1<g<N

Model
Avg.

Merge
FC

sync
(g=1)
•

•
•
•

Tool

DistBelief [3]
Adam [5]
FireCaffe [9]
MXNet [6]
SINGA [7]
SparkNet [8]
DL4J

async
(g=N)
•
•

•
•

•
•

•

•

•
•

In this study we focus on the parameter server approach,
which is the most widely used by distributed CNN systems.

C. Existing Systems

Using the terminology above, we discuss design decisions
made by CNN systems in Table II. In our review of the
literature, these are the tradeoffs which we identiﬁed as most
impactful to minimizing convergence time.

Execution Strategy:

In terms of execution strategies,
Microsoft’s Project Adam [5] reports that the async strategy
is effective and uses one or a few (e.g., 4) machines per
compute group. Moreover, they report that using a technique
called Hogwild! [25], which introduces race conditions, is
an effective source of additional hardware speedups. On the
other end of the spectrum, FireCaffe [9] implements the sync
strategy and notices that high scalability can be achieved by
having all machines work synchronously and in parallel on a
single, large batch of data, and by reducing communication
using reduction trees. MXNet [6] implements both the sync
and async strategies, and allows the user to select which to
use. They also call the sync strategy the Bulk Synchronous
Parallel (BSP) protocol. Finally, Google DistBelief [3] and
Apache SINGA [7] implement both sync and async as well
as the compromise of multiple, larger compute groups. They
also call sync Sandblaster, and cases of more than one group
Downpour (e.g., Downpour with group size of 1 machine is
equivalent to the async). SINGA calls the intermediate case
of multiple, larger compute groups hybrid (SINGA also has
hybrid parallelism, which is different. That is a combination
of model and data parallelism.)

All these systems implement the Parameter Server tech-
nique, described above in the Terminology section (Ap-
pendix D-B). It is the most widely used technique by dis-
tributed CNN systems and the technique we focus on in
this study. Another technique known as model averaging,
which works well within a map/reduce framework, is used
by SparkNet [8] and DL4J (http://deeplearning4j.org/). Model
averaging is also described above. The key difference between
model averaging and parameter server is the way in which
model replicas are combined.

Physical Map, Modern Networks: The second point in
the distributed tradeoff space is the server architecture (how
servers map to hardware), speciﬁcally whether the FC compute
and FC model servers are mapped to the same physical
machine (or machines, for multi-machine model parallelism).

This is a technique introduced by Microsoft’s Project
Adam [5] to avoid communicating the large FC model and
its gradient. The method was reported for older networks
with large, fully-connected layers (AlexNet, VGG), however
it also is useful for modern networks (Inception, ResNets).
In traditional networks, the fully-connected layers contained
the majority of the model parameters [2] (> 90%). Newer
networks instead use average pooling and have only a single
FC layer (for softmax regression) [20]. Therefore newer net-
works contain fewer parameters in the FC phase, and fewer
parameters overall, however this single FC layer can still be
very large (e.g., when predicting among 22,000 classes on
ImageNet 22k) and still beneﬁts from reduced FC communi-
cation (because the number of FC weights will always be less
than the number of inputs to the FC phase). Therefore while
newer networks contain only a single FC layer, the merged
FC optimization of Project Adam is still relevant, and while
a characteristic of newer networks is that their overall model
size is smaller due to the elimination of multiple FC layers,
ultimately this does not translate to reduced communication
overall because the cost of communicating the FC layers has
been eliminated in prior work.

is eliminated (i.e.

In addition, as we show, the beneﬁt of merging the FC
servers is not only improving HE due to reduced network
communication, as [5] noted, but also improving SE because
staleness in the FC model
the device
or devices which compute the FC model gradient updates
also store that subset of the model). Moreover there is no
consequence of merging the FC servers for small FC models
because little computation in the FC phase means it is less
likely for the FC to saturate (become the bottleneck), but
merging still provides the beneﬁt of (1) improving SE, (2)
reducing communication and (3) ofﬂoading computation to the
parameter server machines.

As our goal is to be a complete study, we study both
cases (many large FC layers, few small FC layers) in order
to build a system which is robust to any application. For
example future CNN architectures may employ multiple FC
layers again to support transfer learning tasks, or may need
to predict among many object classes (e.g., hundreds of
thousands or millions), further increasing the communication
bottleneck for the FC. In addition, FC layers are also used
for RNNs and other architectures.

This is the subset of the tradeoff which we study in this
work, summarized in Table II. Our goal is to ﬁnd best point in
the tradeoff space given the model, data, and hardware speciﬁ-
cations from the user. Speciﬁcally we do not change the neural
network architecture, but assume that this model is given to
us. I.e. we do not focus on machine learning algorithms or
techniques in this work, but rather study systems tradeoffs
which exist for the most widely used networks/algorithms.
We focus on the SGD algorithm for learning, as it has been
and continues to be used along with momentum by annual
ImageNet winners [2], [20]. Other algorithms exist for training
deep learning models and can also be parallelized, for example

Fig. 20. Hardware efﬁciency penalty for various numbers of compute groups.
The number of machines is ﬁxed to 32. Each machine is an EC2 c4.4xlarge
(CPU) machine.

Google’s deep learning system uses Adagrad [3]. Microsoft on
the other hand uses SGD [5]. Because the systems tradeoffs
we study in this work are orthogonal to the choice of update
algorithm, in this work we focus on SGD, although the same
tradeoff applies to other algorithms as well.

D. Hardware Efﬁciency Model

The goal of this section is to create a predictive model for
how the hardware efﬁciency (HE) varies with the amount of
staleness S in the system, given a ﬁxed number of machines
and batch size. Recall that the staleness S is equal to the
number of compute groups (minus one). This is because a
compute group is characterized by processing a unique data
batch and returning a unique gradient to the model server, so
the number of parallel gradients being computed is equal to
the number of compute groups. S = 0 is the case of 1 compute
group, also called the synchronous case.

Figure 20 shows a plot of staleness vs. hardware efﬁciency
penalty for three datasets. The standard networks from Caffe’s
tutorials are used for each dataset. The hardware efﬁciency
penalty, or PHE, is deﬁned as the ratio of the time per iteration
relative to the time per iteration for this synchronous case,

PHE(S) =

HE(S)
HE(0)

the smallest

A higher PHE is worse (more time per iteration). PHE
decreases (iterations become faster) as the number of compute
groups increases. This is because, if we ﬁx the number of
time per iteration occurs
machines to be N ,
when there is no synchronization, i.e. when the compute group
size is 1 and there are N compute groups (asynchronous
case). In this case S = N − 1. As the compute group sizes
increase, and hence the number of groups decreases (because
the number of machines is ﬁxed to N ), PHE will increase due
to synchronization delays within the groups. When the number
of compute groups is 1, that group contains all N machines
and requires the most synchronization. In this case S = 0, and
this has the highest penalty PHE.

The hardware efﬁciency penalty is dimensionless. It is the
ratio of hardware efﬁciencies (time per iter / time per iter).
Because PHE is normalized to the synchronous (S = 0) case,
PHE ≤ 1. Note also that the hardware efﬁciency penalty is
only comparable across different staleness values if the number
of machines is ﬁxed.

The hardware efﬁciency penalty can be predicted analyti-
cally. However, as we will see in a later section, the cold-start
phase of the optimizer performs a short adaptive grid search
across different staleness values. Because a few iterations are
run for various staleness values, the execution time for these
iterations can be used to provide a precise hardware efﬁciency
model (deep learning layer computations are dense, not sparse,
so there is little variation in iteration time). Nevertheless
understanding the hardware efﬁciency precisely is important:
1) to understand the execution bottlenecks and either hard-
code or allow the optimizer to make physical mapping
choices

2) because it may be too time-consuming to obtain static
information for every staleness value of interest, and
3) because our work is a study meant to inform future

systems which may not use a static optimizer

Figure 20 was run on 33 EC2 CPU machines. The server
architecture shown in Figure 18 was used, i.e. one machine
contains the merged FC compute and FC model servers, and
the other 32 machines contain Conv Compute servers. The
Conv Model server is mapped to one of the Conv Compute
machines. We make two observations, which are true for all
datasets in Figure 20:

Observation 1: As the number of groups decreases (and
hence as their sizes increases), the hardware efﬁciency be-
comes poorer. There are two reasons for this: (1) machine
variance, which causes synchronization delays, and (2) net-
work congestion, because the convolution model needs to be
sent simultaneously to all machines in the group (and gradients
need to be send back simultaneously). Our analysis below
shows that while machine variance exists, it is insigniﬁcant
compared to increased network congestion.

Observation 2: As the number of groups increases, the
speedup is not linear (it saturates). This is because the FC
phase processes only a single batch at once, or equivalently
because the FC compute and FC model server map to the same
physical machine (or machines, as the FC compute / model
server may use multi-machine model parallelism). Recall that
this has beneﬁts for both hardware efﬁciency (by reducing
network communication) and statistical efﬁciency (by reducing
the staleness of the FC model). However it means that only
one gradient computation (batch) is processed by the FC at a
time, and so it may become a computational bottleneck.

Many optimizations exist for both of these observations.

They are presented after our derivation of the model.

1) Full Derivation of Analytic Model: Formally, let there be
N + 1 machines. 1 machine is allocated to the FC phase, and
N machines (e.g., 32) to the conv phase. An execution strategy
partitions the N conv machines into g compute groups. Each
group contains k machines, and the k machines in a group
compute the conv phase with data-parallelism. Therefore, there
will be g = N/k compute groups sharing the single FC server
machine. In addition, let tconv(k) be a function that returns the
time that a group of size k needs to compute the convolution
phase (forwards and backwards for only the conv phase), and
tf c be the time that an FC server needs to serve one group

Fig. 21. Gantt chart illustrating the Hardware Efﬁciency model for the server
architecture of Figure 18. Case 1 is FC saturation (top), Case 2 is Conv
Saturation (bottom), and the boundary between the two is shown in the middle.

(forwards and backwards for only the FC phase. Note that tf c
is independent of k, the number of machines used to perform
convolution on each batch). We also deﬁne that tf c includes
the network time to transfer the data from the conv phase to
the fc phase and the data gradients from the fc phase back to
the conv phase, although we observe that this network time
is often small compared to the computation time of tf c. Note
that tf c is independent of k, the number of machines used
to perform convolution on each batch. Finally, assume for
now as we did above that different groups (batches) cannot be
executed in parallel on the FC server (that case is described
later).

Given k, g, tconv(k) and tf c, our goal is to create a hardware
efﬁciency model which predicts the time per iteration or,
equivalently, which given a time interval T predicts how many
batches will be processed by the system. Because each batch
must be processed by the FC server, this is equivalent to
determining the number of requests that the FC server can
process from the g convolution groups in time T . There are
two cases, also illustrated in Figure 21.

Case 1: Saturated FC Server The ﬁrst case is when
FC server is saturated, i.e. it starts to serve the next request
immediately after the previous request ﬁnishes. In this case,
the hardware efﬁciency is straightforward. The server will
serve T /tf c requests in time T, or equivalently,

Time per iterationsaturated fc = tf c

Case 2: Saturated Conv Server When the FC Server is
not saturated, each conv server becomes the bottleneck. In this
case, the FC server serves T g/(tconv(k)+tf c) requests in time
T, or equivalently,

Time per iterationsaturated conv = (tconv(k) + tf c)/g

which is the total time for a single iteration divided by the
number of parallel groups. This is because the groups all are
computed in parallel, with the exception of the FC server
which is serial, but the FC server is never saturated so it can
also be seen as being part of each parallel group. Refer to
Figure 21 for an illustration of this case. To understand this
case, note that:

1) When each conv server is fast (tconv(k) is small), the

FC server serves more requests in time T

2) When the FC server is fast (tf c is small), the FC server

serves more requests in time T

3) When the number of concurrent group is large (g is
large), the FC server serves more requests in time T
Determining Saturation Finally, the model needs to predict
when the FC server will saturate. This occurs at the boundary
of the times above, speciﬁcally the FC server saturates (case
1) when:

tconv(k) + tf c < gtf c

Intuitively, if the combined FC time to process all groups
(gtf c) exceeds
iteration
(tconv(k) + tf c), then the FC server will always be saturated.
Note that:

the time for a single group’s

1) When each conv server is fast (tconv(k) is small), it is

easier to saturate the FC server

2) When the FC server is fast (tf c is small), it is harder to

saturate the FC server.

3) When the number of concurrent group is large (g is

large), it is easier to saturate the FC server.

We now have an expression for the time per iteration in both
cases as well as a condition to decide which case applies.
Given the following:

• Two of: N , g or k (the third can be calculated from the

• tconv(k), the time to complete the convolution portion of
the network (forwards and backwards) given the group
size, and

• tf c, the time to complete forwards and backwards on the

other 2),

FC phase,

the model can predict the mode of saturation and therefore the
time per iteration.

tconv(k) can be calculated given the throughput of each

node and the network speed. It has two components:
tconv,compute(k) and tconv,network(k).

Let us deﬁne tconv,compute(1) = Tc,c, i.e. Tc,c is the time
it takes for a single machine (k = 1) to compute the forward
and backward pass of the convolution phase. Similarly, let us
deﬁne tconv,network(1) = Tn,c, i.e. Tn,c is the time needed
for a single copy of all the conv phase’s models (forwards
pass) and model gradients (backwards pass) to be passed over
the network. We will describe how to determine these two
quantities later.

The computation time for the convolution phase for k > 1
is then tconv,compute(k) = Tc,c/k, because recall that a single
compute group performs computation on a single batch of data

(data parallelism), i.e. the amount of data per group is always
the same per iteration (e.g., b images) and so if there are k
machines in a group, each will process b/k images. We assume
a linear speedup.

On the other hand, the time for the network communication
increases with k. This is because of increased network com-
munication from the conv model server, i.e. the model needs
to be sent to k workers simultaneously and gradients will be
received from k workers simultaneously (all requests are made
at almost the same time, because the workers in the group are
synchronous). The network time for the convolution phase for
k > 1 is then tconv,network(k) = Tn,c ∗ k. Here, we assume a
linear slowdown.

So while the compute time decreases with k, the network
time increases with k. We assume that both of these are linear.
Empirically we notice that the convolution computation does
not scale exactly linearly with k: on 8 c4.4xlarge machines in
a single group, the forward pass of the convolution becomes
7.2× faster and the backwards pass 6.6× faster. Similarly, we
observe that the network slowdown is usually linear but can
be super-linear, which we attribute to thrashing.

Given tconv,compute(k) and tconv,network(k), we can

naively approximate

tconv(k) = tconv,compute(k) + tconv,network(k)

However, these two can be done in parallel, i.e. while one layer
is computing its forwards pass, the model for the next layer
is being sent over the network. This does not entirely overlap
because the ﬁrst layer needs to complete its backwards pass
before requesting the model for its next forwards pass, but we
can approximate the total convolution phase time as:

tconv(k) = max(tconv,compute(k), tconv,network(k))

it

Finally,
is necessary to obtain Tc,c, Tn,c and tf c. We
measured these because they only need to be measured once
(not for each k), but they can be calculated using the node
throughput and network throughput: Tc,c and tf c can be
approximated by counting the total number of operation from
each GEMM operation in the conv and fc phases and assuming
that BLAS achieves the device peak. Tn,c can be approximated
by counting the total number of bytes in the conv models
and assuming the peak network throughput is achieved. These
assumptions are justiﬁed because the matrices and models are
large.

Also note that measurements of these quantities are accurate
for all iterations because deep learning computations are dense
and so there is little variation in the computation time across
iterations, as shown in Figure 22. Note that there is a standard
deviation of than 6% for tconv(1) and tf c, and a standard
deviation of 8% for the total iteration time. For CIFAR-10,
the standard deviation for total iteration time was 1.5%. We
also observed similar variances on a GPU cluster.

Using measurements of Tc,c, Tn,c and tf c, Figure 5 shows
that the analytic model of hardware efﬁciency is accurate.
When the FC server is saturated (right side of the graph), the
model is almost exact. When the FC server is not saturated

FC compute and FC model servers are mapped to the same
physical machine. Also recall that this has beneﬁts for both
hardware efﬁciency (by reducing network communication) as
well as statistical efﬁciency (by reducing the staleness of
the FC model). However it means that
there is only one
FC compute server, and so it may become a computational
bottleneck. This is seen in the top Gantt chart of Figure 21
in which there are blank spaces which indicate un-utilized
machines.

A simple way to ﬁx this is to make the FC machine faster,
for example if there is limited access to GPU hardware, it it
best to use them on this machine (indeed, we see in Section VI
that the GPU cluster does not saturate FC). Another technique
is to use multiple machines for the FC phase, for example
using model parallelism across multiple machines such that
each machine stores a portion of the FC model (this still has
a staleness of 0 for the FC phase).

Another solution is to remove this physical mapping, i.e.
rather than have a single FC compute server mapped to the
FC model server, to have a separate FC compute server per
Conv compute server. This removes the bottleneck of a single
FC compute server, although it also sacriﬁces the beneﬁts
mentioned above. Section VI demonstrates the consequences
of this decision experimentally.

Physical Mapping Details: In addition to mapping the
FC compute and model servers to the same physical machine,
note that the conv model server does minimal computation and
can also be mapped physically to the same machine as the FC
compute/model server or one of the conv compute machines.
The primary concern with this server is network congestion,
so it makes more sense to map to one of the conv compute
server’s machines.

In addition to multiple servers physically mapping to a
single machine, it is also possible for a server to physically
map to many machines, for example using multiple machines
in a model server to implement a reduction tree as in FireCaffe,
or mapping 4 FC Compute servers to a machine that contains
4 GPUs, etc. Another example is merging the FC compute
and FC model server and mapping them to the same physical
hardware as in Figure 16 (b), but where that hardware is
not a single machine as shown in Figure 16 (b) but multiple
machines e.g., using model parallelism.

Finally, a common technique is to “pipeline” the servers by
mapping multiple conv compute servers to the same physical
machine. For instance in the synchronous case (1 group of N
machines), during the FC computation all N machines are idle
(because they are waiting for the FC to return data gradients
before they can begin the backwards pass of the conv phase).
During this idle time those machines can be processing a
different batch, i.e. N = 32, but there are two groups of
size 32. Note that in this example, this pipelining increases
staleness from 0 to 1. Using this pipelining, now the time per
iteration in conv saturation becomes:

Time per iterationsaturated conv = tconv(k)/g

i.e. the FC time is completely hidden.

Fig. 22. Variance of HE times. These are on a cluster of 9 CPU machines,
8 Conv compute groups, 1 machine per group

(left side of the graph), the slowdown and speedups are not
exactly linear, and we under-estimate the time per iteration.

While this analytic model may seem speciﬁc to CNNs, it
extends to any deep learning model because its derivation
relies only on queuing theory, not any speciﬁc properties of
CNNs.

2) Further Optimizations: A primary goal for understand-
ing the hardware efﬁciency above is to determine possible
optimizations.

Saturated Conv Server: We showed above that as the
group size (k) increases, there is no longer FC saturation,
because

tconv(k) + tf c > gtf c

and recall

tconv(k) = max(tconv,compute(k), tconv,network(k))

Speciﬁcally, in Figure 5 the case of a single group (left side
of the graph) is so much slower than FC saturation (right side
of the graph) because tconv,network becomes very large, i.e.
the time it takes to send the conv model to all 32 machines in
the group is signiﬁcantly greater than the computation time,
which is small due to data parallelism across 32 machines.

In particular, note that a single, large group is slower than
many small groups not because of synchronization delays
due to machine variance exists, but because of increased
network congestion, although some variance does exist in the
computation time across machines.

Microsoft’s Adam [5] discusses techniques to mitigate both
of these problems, from not requiring each worker in a group
to ﬁnish processing all of its images to adding multiple NIC
cards on the parameter server machines. FireCaffe [9] uses
the technique of reduction tress for their parameter servers to
reduce communication overhead, which allows them to reduce
this network congestion and scale to many machines in the
synchronous case using a larger batch size.

Saturated FC Server: The optimizations above improve
the hardware efﬁciency for the synchronous case, or generally
for small g and large k. On the right side of Figure 5
(small k, large g), the FC server becomes saturated and no
further speedups are possible. Recall that this is because the

We see that as long as η∗ increases with the batch size,
there is little penalty for larger batch sizes. This is because
larger batch sizes provide a truer gradient each iteration and
permit a larger η before divergence, therefore while a larger
batch consumes more of the dataset, the progress made by
each step is greater. η∗ cannot scale inﬁnitely however, and
plateaus beyond η∗ = 0.0032. As a result, larger batch sizes
make no more progress per iteration than smaller batch sizes,
but consume much more of the dataset each iteration. This is
catastrophic for performance (it can take 30× more epochs
to converge) as computation is effectively “wasted”, which is
why neural networks have been trained with SGD rather than
batch gradient descent since the early days. This also greatly
exceeds the staleness cost incurred of “splitting” a large batch
into smaller, asynchronous batches (which we show is nearly
ﬂat), which is why asynchrony is necessary for systems to
scale to very large clusters.

This suggests that the optimizer needs to tune batch size,
however for imagenet-8 and other datasets we observe that this
performance penalty is negligible around 256 (speciﬁcally we
use 256 for Imagenet, 128 for CIFAR-10 and 64 for MNIST,
based on published results for these datasets). In principle the
optimizer could tune b as well, but we observe that unless b is
too large the penalty is small so we don’t study this in more
detail.

B. Physical Mapping

As discussed in the text, for the physical map which maps
servers to machines, we map the FC compute and model
servers to the same machine (i.e. “merge” the FC servers,
which as [5] argues reduces communication overhead because
it avoids communicating the large FC model) and use one
compute group for the FC phase. The rest of the machines are
used for the conv compute servers. The conv model servers
are mapped to one of the conv compute machines.

Empirically we show in Appendix F-C4 that this mapping
is best for both hardware and statistical efﬁciency: on a
cluster of 33 EC2 c4.4xlarge machines, not merging the FC
servers incurs an additional hardware efﬁciency penalty of
1.2× due to increased communication as well as a statistical
efﬁciency penalty of 2.5× because of staleness in the FC
model. Our current optimizer therefore always chooses this
server architecture, although Appendix D-D (for Section IV-B)
described other scenarios in which these penalties are justiﬁed
to eliminate FC saturation, as well as additional optimizations
within this server architecture (such as reduction trees or multi-
machine model parallelism for the FC phase).

C. Optimizer Details

Fig. 23. Using a batch size that is too large greatly increases the # epochs to
converge (Imagenet 8-class). This slowdown begins when the optimal learning
rate no longer scales with the batch size.

E. Statistical Efﬁciency Model

Because asynchrony can be viewed as increasing implicit
momentum, asynchrony can be made equivalent
to syn-
chronous execution by properly reducing the explicit momen-
tum in order for the total explicit + implicit momentum to
stay the same as the optimal momentum of the synchronous
case. This is true as long as the implicit momentum is less
than the optimal momentum of the synchronous case. This
is a key discovery because it means that staleness can exist
in the system without incurring a statistical penalty, which
is advantageous for hardware efﬁciency. Also, making the
momentum stay the same (rather than just ignoring this result
and letting there be extra momentum) is important because a
total momentum that is too high will diverge, which we show
experimentally in Appendix E. This theory also successfully
predicts measured system behavior: Figure 6 shows the pre-
dicted and actual measured momentum for several popular
deep learning problems. In both cases, upon reducing the
explicit momentum to compensate for implicit momentum,
we observe no SE penalty. Moreover, the momentum increase
due to asynchrony closely matches the theoretical curve for
both datasets. Our study is the ﬁrst to identify a relationship
between hyper-parameters and asynchrony, and next
these
results are used to design the optimizer in Section V.

APPENDIX E
APPENDIX FOR DISTRIBUTED OPTIMIZER (SECTION V)

This section describes how to use the models from the
previous two sections to choose (1) a physical mapping which
maps each server to a machine, and (2) an execution strategy
which deﬁnes the number of compute groups by allocating
data batches to each server. As in previous sections we assume
that the number of machines are ﬁxed.

A. Selecting the Batch Size

We ﬁrst study the batch size in Figure 23, which uses the
imagenet-8 dataset with S = 0 and momentum µ = 0.9. The
x axis varies the batch size and the y axis plots the # passes
over the dataset (or epochs) until convergence. For each batch
size we used an oracle to ﬁnd the optimal learning rate, η∗.
η > η∗ diverged.

For each epoch, Algorithm 1 performs an adaptive grid
search over both the learning rate and the momentum starting
at the current value of g. Speciﬁcally, we run each learning
rate and momentum (see below) for one minute and select the
conﬁguration with lowest loss after 1 minute of execution. If
after 1 minute all these conﬁgurations have the same loss, we
continue to run another minute until there is a clear winner

1.8× more iterations (SE) to reach (cid:96)F in (b). This matches
the theory’s prediction: increasing g decreases the explicit
momentum µ∗, which falls to 0 at g = 32 (see Figure 6 (right))
, and consequently there is a penalty in SE. The optimizer of
Algorithm 1 would therefore select g = 16, which is near-
optimal.

However, our optimizer additionally employs an optimiza-
tion to leverage the HE model from Section IV-B: because
the FC server saturates at g = 4 (determined analytically
or through measurements during the cold start phase), the
optimizer will “short-circuit” Algorithm 1 to begin at g = 4
instead of g = 32, and ends up selecting g = 4, which is 5.3×
faster than sync.

Figure 24 shows the accuracy vs. time (and for reference
accuracy vs. iter, i.e. statistical efﬁciency) for each conﬁgura-
tion (# groups) in Figure 7. Recall that the optimizer selected
4 groups because with proper momentum tuning the statistical
efﬁciency was nearly the same for all curves, but 4 or more
groups had the best hardware efﬁciency. Figures 7 and 24 use
ImageNet 1000 class (1 hour of training) as described above
with 33 EC2 c4.4xlarge machines.

D. Cold Start Phase

The model is trained synchronously before beginning asyn-
chronous execution in Algorithm 1. This is needed in order
to set the appropriate scale for the weights. However fully
synchronous execution may be slow, and just as an optimiza-
tion to Algorithm 1 was to run asynchronously only up to
FC saturation, similarly this section focuses on accelerating
training during the cold-start phase. In particular, a fully
synchronous execution may signiﬁcantly increase the duration
of the cold-start phase due to poor hardware efﬁciency, and
so a cold-start run with slight asynchrony may more quickly
terminate the cold start phase. Therefore, tuning the number
of compute groups is also important for the cold-start phase.
Cold Start Grid Search: To do this, as in Algorithm 1,
we grid search hyper-parameters for each number of groups
(1, 2, 4, . . . , N , for N machines). For each we also grid search
learning rate and momentum. We use a standard adaptive
grid search algorithm for its simplicity. For each staleness,
the algorithm searches for optimal settings of momentum and
learning rate by running each conﬁguration of parameters for
1 minute, and selecting the parameters with the lowest loss
after 1 minute.

The search happens as follows, and is similar to the search in
the steady-state execution of Algorithm 1. We start with S = 0,
ﬁx the momentum to 0.9, and run 1 minute for each learning
rate η ∈ {0.1, 0.01, 0.001, 0.0001, 0.00001}. We search from
lowest to highest and stop early if a learning rate produces a
ﬁnal loss worse than the previous learning rate (or if a learning
rate causes divergence). We select the learning rate which has
the lowest loss after 1 minute. Call this η∗
sync. Therefore for
S = 0, the optimal conﬁguration (µ∗, η∗) = (0.9, η∗
sync). We
do not tune momentum for sync because 0.9 is standard [2],
because this saves optimizer time, and because there is no
implicitly induced momentum due to asynchrony for S = 0.

Fig. 24. Accuracy vs. iteration and Accuracy vs. Time for each execution
strategy shown in Figure 7

in terms of loss (we determine this using a threshold of 5%
from the loss of the past 50 iterations, although a statistical
test can be used as well). We then run this best (µ∗, η∗) for
an hour and then rerun the optimizer.

One could use more sophisticated parameter search routines,

but this took less than 10% of the execution time.

We search the learning rate as follows. Let the learning rate
and momentum used in the previous 1 hour epoch (i.e. the
result of the grid search from that epoch) be (µ∗
last).
For the current epoch, we then search η ∈ {η∗
last/10},
and µ ∈ {0.0, 0.3, 0.6, 0.9}. As an optimization to prune the
search space, we do not search µ > µ∗
last, as
we notice empirically that as the run progresses, the optimal
total momentum decreases.

last when η = η∗

last, η∗

last, η∗

If the optimal µ∗ = 0.0, we try µ∗ = 0.1 and µ∗ = 0.2 as
well, and if the lowest loss is still achieved at µ∗ = 0.0, we
decrease g and repeat the search.

(µ∗, η∗) in the initial (cold-start) phase are selected using a

similar procedure, described in Appendix E-D.

1) Empirical Validation: We now justify the theoretical
results of Section IV-C experimentally. We use a cluster of 33
EC2 CPU machines (CPU-L in Figure 9), the Imagenet 1000-
class dataset, and the AlexNet CNN. We run each execution
strategy from sync (g = 1 conv compute group) to async
(g = 32), each for a single epoch (1 hour). Speciﬁcally, we
plot 6 strategies: g = {1, 2, 4, 8, 16, 32}, where the number of
conv compute machines is ﬁxed to 32.

Each strategy starts from the same checkpoint (i.e. same
loss), but achieves a different ﬁnal loss by the end of the
epoch. We select the lowest ﬁnal training loss achieved by
all strategies, (cid:96)F , and plot three measures in Figure 7: (a) the
time per iteration (hardware efﬁciency, HE), (b) the number of
iterations to reach (cid:96)F (statistical efﬁciency, SE), and (c) their
product, which is the total time to reach (cid:96)F . For completeness,
each strategy uses an oracle to exhaustively ﬁnd its optimal
explicit momentum, µ∗ (within 0.3) and optimal learning rate
η∗ (within an order of magnitude. For all strategies η∗ = 0.01
was optimal). The HE curve in (a) is the same as in Figure 5
(b).

We see in (c) that g = 32 (fully asynchronous) is 3.7×
faster to reach (cid:96)F than g = 1 (synchronous). This is due
to its faster iteration time (HE) in (a), although it requires

For the remaining S after S = 0, we perform the following
iteration: We increase the number of groups to the next highest
power of 2 (after sync, this is g = 2, then g = 4, g = 8, etc, i.e.
S = 1, 3, 7, . . .) Let the optimal conﬁguration from the previ-
ous S be (µ∗
last). For the current S, we run a grid search
for (µ, η)|µ ∈ {0.0, 0.3, 0.6, 0.9}, η ∈ {η∗
last/10}. I.e.,
η∗ deﬁnes the search range for the next S. In addition, µ∗
reduces the search space for the next S: we do not search a
higher momentum than µ∗

last while searching η = η∗

last, η∗

last, η∗

last.

We notice empirically that

there is not a large impact
of running a ﬁner grid for momentum (although this can
be done by adding a second-phase of search which ﬁxes
η∗ and searches µ around µ∗). Running multiple random
seeds (network weight initializations) can also be used to
ﬁnd a good starting point for the SGD algorithm (this is a
known technique). Tuning parameters is not a novel idea in
machine learning, but unlike existing work our problem is
more sophisticated as we are coupling tuning hyper-parameters
and execution strategies (staleness). Our work is the ﬁrst to
show that hyper-parameters and execution strategies need to
be tuned jointly to avoid divergence as staleness increases.

Once we obtain (µ∗, η∗) for each S, we then run each S
for one minute at a time until there is a clear winner in terms
of loss (we determine this using a threshold of 5% from the
loss of the past 50 iterations, although a statistical test can be
used as well). We then run this best S with its (µ∗, η∗) for an
hour (the cold-start period).

Parameter Search Experiments: The remainder of this
section describes experiments motivating the pruning above,
in particular why a larger staleness does not need to try larger
learning rates or larger momentum values at the same learning
rate. There are a number of insights which allow us to prune
the search space for the cold-start phase and reduce the total
search time. We discovered that as staleness increases, the
optimal learning rate and momentum parameters when S = 0
cause divergence (loss goes to inﬁnity) for larger staleness
values, e.g., S = 31. This makes sense given our theoretical
foundation from the steady-state optimizer: staleness induces
implicit momentum, hence if explicit momentum is not de-
creased as S increases, total momentum can be > 1 and cause
divergence. As S increases, we note that one or both of η and
µ need to be reduced otherwise SGD will diverge (loss goes
to inﬁnity).

Table III shows the optimal parameters for the same datasets
and networks used in Figure 20, at different staleness values.
Here the optimal parameter settings are deﬁned as the param-
eter settings with which the training converges in the fewest
number of iterations. We say that a model has converged once
the training accuracy reaches 99.5%. Recall that a staleness
value of S corresponds to N = S + 1 parallel groups updating
the model asynchronously. In these experiments, the staleness
of the fully-connected models was zero, and so only the
conv model had staleness. Note that for imagenet 8-class
there are only 10400 examples and batch size is 256 so 128
parallel workers was not possible (there is insufﬁcient data).
Note that these small datasets all converge in under an hour

TABLE III
(COLD START) OPTIMAL MOMENTUM AND LEARNING RATE DURING THE
COLD START FOR VARIOUS DATASETS/NETWORKS AND VARIOUS
AMOUNTS OF STALENESS IN THE CONV MODEL

Dataset
(Network)

Staleness

Optimal
Momentum

MNIST
(LeNet)

CIFAR-10
(Krizhevsky)

ImageNet-8
(CaffeNet)

0
31
127
0
31
127
0
31

0.6
0.0
0.8
0.9
0.7
0.1
0.6
0.0

Optimal
Learning
Rate
0.1
0.1
0.01
0.001
0.0001
0.0001
0.01
0.01

and therefore consist entirely of the cold-start phase in our
implementation. While the cold-start phase of these datasets
is less than an hour (e.g., MNIST converges in seconds), and
therefore Algorithm 1 could be run part-way during execution
to select a better strategy for the remainder of the execution
(e.g., asynchronous), the overhead of re-running Algorithm 1
is not justiﬁed for these small datasets, i.e. it is faster to treat
the entire run as the cold-start phase. Therefore we use these
smaller datasets to study the cold-start phase.

The table shows that, with a ﬁxed batch size, as staleness in-
creases the optimal momentum and/or learning rate decreases,
and in some cases not decreasing these parameters and reusing
the parameters for S = 0 causes divergence. Also, we see that
decreasing the learning rate means momentum can increase
again. Intuitively this is because momentum can be viewed as
increasing the learning rate (larger SGD steps), and so if the
learning rate is decreased too much, momentum increases to
compensate for this decrease. Our grid search searches orders
of magnitude for the learning rate, following from previous
work [2], but decreasing the learning rate by a smaller factor
may avoid the need for momentum to increase and provide
faster overall convergence. We leave this exploration to future
work.

APPENDIX F
APPENDIX FOR DISTRIBUTED EXPERIMENTS (SECTION VI)
A. Single-Node Experiments

See Appendix C-E.

B. Small Cluster Experiments

This section provides additional details of the experimental
setup. For the end-to-end experiment on ImageNet 1000, see
Appendix F-E.

We ran all systems to 99% accuracy (which we deﬁne as
convergence) with a timeout of 2 hours. For each system we
used the same CPU and GPU external libraries as discussed
in Appendix C-E.

We further sped up other tools by applying our optimizer
to the extent that no code change was required. MXNet offers
both the sync and async strategy. Given our observation that
async requires tuning parameters, to ensure training did not
diverge we ran each strategy of MXNet with 4 orders of

magnitude of the learning rate for 10 minutes each. We then
selected the best strategy (as the static optimizer would) and
ran it until convergence or timeout. For SINGA we followed
the same procedure and tried all available conﬁgurations.
SINGA supports not only sync (1 group) and async (1 machine
per group) strategies, but also intermediate group sizes. We ran
SINGA with 1, 2, 4, and 8 machines per group, and also 4
orders of magnitude for the learning rate η in each case. All
runs were also for 10 minutes, and then as with MXNet the
best one was run to convergence.

For Omnivore we ran our optimizer, which merged the FC
compute and model servers to one machine and used the other
8 machines as conv compute machines. As with SINGA, the
optimizer searched statically among 1, 2, 4 and 8 machines
per group, but for 1 minute per execution setting. Overall the
optimizer ran for less time than the tuning we did to ensure
no divergence for MXNet and SINGA.

We followed the tutorials for each system and also ensured
that all three systems used identical networks and parameters,
including weight initializations and data preprocessing.

The network we use is CaffeNet, which is Caffe’s version
of Alexnet20. AlexNet is the standard network for ImageNet
and Caffe is the most widely used CNN framework, so this
ensures reproducibility. The weight initializations, batch size,
regularization, and other hyperparameters are the same as
CaffeNet, with a few minor differences:

Unlike Caffe and SINGA, MXNet does not easily support
learning rate and weight decay multipliers, or different ini-
tializations for each model and bias. For consistency across
tools, we therefore just made all 3 tools use the same weight
initialization scheme, which is Gaussian with mean 0 and
standard deviation 0.01, and no multipliers.

MXNet and SINGA do not support the grouping in AlexNet
(which was done in 2012 to save GPU memory), so this is
disabled from CaffeNet (and also is not important anymore as
GPUs have more memory)

No random data preprocessing was used (crop, mirror),
and we show convergence on the training set, not a test or
validation set. We do this because these are machine learning
concerns/optimizations and our focus is the system. We do
subtract the image mean to avoid divergence.

Similarly, we disable the learning rate schedule in all tools
and use a constant learning rate. We do this because we only
train on a subset of ImageNet and to reduce the search space
of the parameter conﬁgurations.

The following subsections describe in detail the individual
settings used for each system to ensure fairness in our com-
parison.

1) Detailed Settings for Both Systems: For both systems
we built in a wall-clock timer to ensure accurate timing. We
created and shufﬂed the dataset using the tools provided by
the systems: MXNet required shufﬂing beforehand, SINGA
provided an im2rec utility. Those tools also were used to
calculate the image mean: MXNet automatically generated the

20https://github.com/BVLC/caffe/blob/master/models/bvlc reference caffenet

mean ﬁle when it ran and SINGA as part of im2rec. Because
we focus on the training set we removed any validation set
from the tools to ensure no time was spent on that. We used
the provided AlexNet examples for each system and changed
them only as above to ensure identical settings across all three
systems (e.g., weight initializations, L2 regularization, etc.)
Accuracy was reported instead of loss for all tools to ensure
consistency. The MXNet examples do not report loss so we
used their accuracy eval_metric. Moreover MXNet’s acc
metric is by default on the entire epoch while SINGA’s is
since last time printing, so we averaged the logs to ensure
consistency across all three tools.

2) Detailed MXNET Settings and Results: We removed all
machine learning optimizations from both tools except those
described above. For MXNet this meant removing gradient
clipping. Because we ensure the parameters are used for all
tools, including the batch size (256 for CaffeNet), this meant
that for MXNet’s dist_sync strategy on 8 machines, a batch
size of 32 was used, and for 32 machines, a batch size of 8
was used (other tools partition the batch size for the sync
strategy, i.e. they partition b images by sending b/N to each
sync worker, but MXNet uses that batch per worker, i.e. they
use a batch size of b × N ). We ﬁxed the random seed to 1 so
the initialization is always the same.

We created a single data ﬁle and ensured that each worker
read it from different location (using ImageRecordIter as
in the AlexNet example). The timer was added as modiﬁed ver-
sion of the speedometer callback (using time.time(),
which is wall-clock time in python).

We used a cluster of 9 machines in these experiments
because MXNet’s AWS documentation instructs to “Prepare a
host ﬁle with all slaves’s private IPs”. Therefore in order to test
parallelism across 8 machines, we opened 9 EC2 machines,
ran MXNet from the master (root) machine, and placed the
other 8 machines in the hosts ﬁle.

On the cluster of 9 c4.4xlarge machines we ran the 4 orders
of magnitude learning rate for each execution strategy and
noticed after 10 minutes that the best was learning rate 0.01
and sync, so we ran until 99% convergence. We needed 4
orders of magnitude to ensure that the optimal setting was
never on the “boundary” of the interval, i.e. the optimal η we
report was superior to an order of magnitude higher η and
lower η. For async no parameter setting had high accuracy
after 10 minutes: The best sync was 60% in 10 min and the
best async got to 20% in 10 min, in spite of better hardware
efﬁciency for async (72 s per epoch async, 120 s per epoch
sync). The best async was with learning rate 0.0001.

On the cluster of 9 g2.8xlarge machines (again following
MXNet’s documentation, 1 parameter server machine and 8
compute machines), we tried both cuDNN v3 and v4 and
found no speed difference. Again we searched learning rate
and found that 0.01, sync was best.

On the c4.4xlargs machines we ensured each worker was
using all the cores, and on the g2.8xlarge machines that all 4
GPUs on each worker were utilized (using nvidia-smi).

3) Detailed SINGA Settings and Results: For SINGA, the
timer built into TrainOneBatch. It also uses wall-clock
time (clock_gettime, same as Omnivore). Again we use
the default AlexNet example with only small changes to make
all weight initializations the same (as in MXNet), and to
remove learning rate / weight decay multipliers (since not
supported easily in MXNet).

Tuning parallelism within groups: we tried tuning

partition_dim for each layer. Speciﬁcally we ﬁrst used
the default, i.e. partition_dim commented out (as in their
AlexNet example). We then uncommented those recommended
partition_dim settings in the example (i.e. dim 0 or batch
parallelism for convolution, and dim 1 or model parallelism
for FC) and found no difference, so we left
partition_dim commented out as in the default AlexNet
SINGA example.

Tuning parallelism across groups: To ensure different data
for each worker we tried specifying a random_skip in the
data section but this made no difference. Documentation v0.1.0
suggested using random_skip but in v0.2.0 (which we used)
it was deprecated, so as with partition_dim we left this
out and used the default AlexNet SINGA example settings.

Next, we had to select for each machine number of workers
per process. For each machine, we tried 1 process of 8, 16,
and 32 threads on a single machine. The number of physical
cores is 8 on the c4.4xlarge, and virtual cores is 16 (nproc
= 16). 16 was fastest so we used 16 workers per process.

As with MXNet we followed the documentation for SINGA
and also included 8 machines in the hostﬁle. We ran 4 orders of
magnitude for the learning rate as described above and found
that 0.0001 and 4 groups of 2 machines each was best after 10
minutes. The result was noisy however, and looked similar to
the distributed results in the “SINGA: Putting Deep Learning
in the Hands of Multimedia Users” paper. We then ran the
best conﬁguration for but it did not converge in 3 hours (got
to 70-80%).

SINGA GPU distributed did not work at the time of this

study so it is not included.

4) Detailed Omnivore Settings and Results: Omnivore was
run using the same network and parameters as the systems
above. The optimizer was run as described above in Ap-
pendix E. Each conﬁguration was searched by the optimizer
for 1 minute and search time was reduced by pruning the space
across staleness values. The second search phase was skipped
(momentum granularity was 0.3). We ﬁxed the random seed to
1 so the initialization is always the same. The overall optimizer
time was less than the search time to avoid divergence in other
tools.

On the CPU cluster, the strategy chosen was the same
as with MXNet,
i.e. sync with η = 0.01. Momentum is
untuned for both Omnivore and MXNet, i.e. 0.9, because our
contribution is tuning momentum to compensate for staleness
and sync has no staleness. Since the parameters and staleness
are the same as MXNet, as expected Omnivore achieves the
same statistical efﬁciency. However, it is 2.3× faster in terms
of hardware efﬁciency, for an overall end-to-end convergence

Fig. 25. Recall Figure 7 (replicated here for convenience and also showing
momentum values from Figure 6 (right))

speedup of 2.3×. On the GPU cluster, the optimizer chose
2 groups of 4 m/g, and was 5.0× faster to converge. The
following section studies the beneﬁts of the optimizer in more
detail, and also examines how the tradeoffs change on a large
cluster which has more options for execution strategies: for
example, the extreme strategies of sync or async may not be
sufﬁcient for a larger cluster. This may prevent MXNet, which
only supports these strategies, from scaling to a larger cluster.

C. Detailed Tradeoff Space Analysis

This section analyzes the speedups observed in the previ-
ous section to understand the contribution of each tradeoff
selection that the optimizer made. These tradeoffs include (1)
execution strategy (number of groups), (2) optimizing hyper-
parameters to compensate for staleness, and (3) physical plan
(server to machine allocation).

1) Penalty Deﬁnition: Consider again Figure 7, which
we’ve replicated here for convenience in Figure 25 and also
shown momentum (note that when the optimal explicit mo-
mentum is 0, there is an associated SE penalty). Recall this
ﬁgure showed the tradeoff for compute groups on the CPU-L
cluster for an epoch of ImageNet 1000. The HE and SE plots
were multiplied to produce the right-most plot of total time to
reach a ﬁnal loss.

The vertical axis of the SE ﬁgure in Figure 25 shows what
we call the SE penalty, PSE(S), which is deﬁned as the ratio
of the # iterations needed to converge relative to the case of
no staleness (S = 0),

PSE(S) =

SE(S)
SE(0)

(8)

PSE is dimensionless, because it is the ratio of statistical
efﬁciencies (#iter / #iter). The penalty is 1 when the staleness
is 0, and should be higher for all S > 0. A higher PSE is
worse (more iterations to converge).

Recall also that we deﬁned hardware efﬁciency penalty
(PHE). This is shown in the middle graph of Figure 25.
Since the statistical efﬁciency penalty is deﬁned as the ratio
of the # iterations to convergence with respect to S = 0,
for consistency PHE(S) is also normalized with respect to
S = 0. S = 0 is the case of 1 compute group, also called the
synchronous case. The hardware efﬁciency penalty is deﬁned

Fig. 26. Cold start vs Steady state for Imagenet 1000 on GPU-S

Fig. 27. Cold start vs Steady state for cifar on GPU-S

as the ratio of the time per iteration relative to the time per
iteration for this synchronous case,

PHE(S) =

HE(S)
HE(0)

(9)

As with PSE, a higher PHE is worse (more time per itera-
tion). Whereas PSE(S) increased with staleness, for hardware
efﬁciency this trend is reversed: PHE decreases (iterations
become faster) as the number of compute groups increases.

Note in these ﬁgures, the staleness is 0 for the FC model, so
staleness on the horizontal axis refers only to the conv models
(i.e. the number of conv compute groups).

Finally, recall that the product of hardware and statistical
efﬁciency is the total time to convergence. Since the horizontal
axis (staleness, i.e. # groups) is the same on both the SE
and HE plots, these plots can be multiplied, and the resulting
vertical axis is the total penalty, deﬁned as the ratio of the
total time to convergence (normalized to sync, i.e. S = 0):

PTotal(S) = PSE(S) · PHE(S) =

SE(S) · HE(S)
SE(0) · HE(0)

(10)

We use ﬁgures of this format throughout this section to

quantify the beneﬁt of the choice of compute groups.

2) End-to-End Imagenet 1000: Recall that Figure 25 was
run for a 1-hour optimizer epoch of Imagenet 1000, on the
CPU-L cluster, as discussed in Appendix E-C. Figure 26
(bottom 2 sets of ﬁgures) shows the same experiment on the
GPU-S cluster. Notice again that SE is ﬂat, i.e. maximum
asynchrony is optimal.

Fig. 28.
c4.4xlarge CPU machines.

Imagenet 8-class and CIFAR-10 tradeoff on a cluster of 9 EC2

These ﬁgures show steady-state execution, hence the SE
curves show no penalty (nearly ﬂat). The same curves but for
the cold-start epoch of the GPU-S cluster are shown in the top
of Figure 26.

We also validate this for the small cifar dataset in Figure 27.
Here for exposition we reduced the epoch size to 2 minutes
(otherwise the cold start would converge after only a few
minutes).

3) Small Clusters: The optimizer’s choice of execution
strategy for the small cluster experiments (Figure 12(a) and
(b)) is shown in Figure 28 (CPU-S) and Figure 29 (GPU-S).
In addition to the imagenet-8 dataset we also include CIFAR-

Fig. 29.
machines (36 GPUs total).

Imagenet 8-class tradeoff on a cluster of 9 EC2 g2.8xlarge GPU

Fig. 30. Hardware efﬁciency, statistical efﬁciency, and total time tradeoff
curve (ImageNet 8-class, 33 machines)

every example 21.

10 to show the optimizer is robust across datasets. We see
that for these small clusters, choosing the execution strategy
incorrectly incurs a penalty of roughly 1.5×.

Note that Figure 28 and Figure 29 have the same statistical
efﬁciency curves but different hardware efﬁciency curves.
This is because the difference in throughput between the
GPU machines and CPU machines exceeds the difference in
network speed between these clusters so there is a higher
penalty for the sync case on the GPU cluster. Also, neither
of these 9 machine clusters reach FC saturation.

Next we consider the CPU-L cluster.
4) Large Cluster: The tradeoff for CPU-L (Figure 12(c))
is shown in Figure 30. 4 groups (8 machines per group) was
the optimal point, and that the optimizer chose this execution
strategy. The detailed tradeoff space for CPU-L is analyzed
in Figure 31. Each curve, from the bottom up, represents a
selection made by the optimizer. We’ve isolated each selection
to observe their relative impact.

Avoiding Divergence: First, consider the red line, which
represents the default point chosen by many systems: asyn-
chronous with a large number of machines. Indeed, statistical
efﬁciency is often ignored by other systems and so by default,
the conﬁguration with the best hardware efﬁciency (fastest it-
eration time) is erroneously selected. However if the published
AlexNet hyper-parameters [2] (which are optimal for the sync
case) are naively used in the async case, there is divergence.
Thus, our tuning approach is critical.

The green curve shows that if only the learning rate η is
tuned, divergence can be avoided. Tuning η is also common
practice, although prior work does not do so explicitly to
compensate for staleness as we advocate. In the green curve
momentum is not been tuned, as many systems always use
a momentum of 0.9 (as mentioned in [2]). For example, at
the time of this study MXNet hard-codes this momentum into

Fig. 31. Tradeoff curve showing the impact of each dimension of the
optimizer.

Also, the green curve does not merge the FC compute
and model servers by physically mapping them to the same
machine. Instead, this curve represents the architecture shown
in Figure 16 (a), i.e. there is an FC compute server for each
CONV compute server, and each of these server pairs is
mapped to a separate machine. Of the 33 machines therefore,
one machine contains the CONV and FC model servers,
while each of the other 32 contains a CONV compute and
FC compute server. This conﬁguration represents the strategy
chosen by MXNet, so we report their async curve as the green
line because their system is optimized for this case (note that
doing this disadvantages our ﬁnal speedup ﬁgure, i.e. if we
had used Omnivore’s implementation of this tradeoff point our
optimizer’s speedup would be > 20×). The remaining curves
have their hardware and statistical efﬁciencies normalized to
those of this green curve.

Device Mapping: We now examine our choice of merging
the FC compute and model servers to the same machine (the
33rd machine), as Section V described. The other systems
do not support this merging so we take “unmerged” as the
baseline, and use the 33rd machine for the conv and FC model
servers as MXNet and SINGA’s documentation suggests. The
remaining curves will have their hardware and statistical
efﬁciencies normalized to those of this curve.

The turquoise curve merges the FC servers. We see that
this gives a 1.18× improvement to hardware efﬁciency (due
to reduced communication) and a 2.55× improvement
to
statistical efﬁciency (due to no staleness in the FC model).
Overall, this is 3.01× faster to converge than the baseline.

In the turquoise curve, note that the hardware efﬁciency
improvement is only 1.18× for the merged FC. As discussed in
Section IV-B, this is because while communication is reduced
by merging these servers, on the large CPU cluster the FC
is reached at 4 groups, and not mapping
saturation point

21https://github.com/dmlc/mxnet/blob/52ea0f0cbbf5eaaf38a2341e57afd6829f88a86d/

example/image-classiﬁcation/train model.py#L77

the FC servers to the same physical machine as described
above eliminates the saturation (but requires more network
communication and incurs a statistical efﬁciency penalty).

Parameter Tuning for Staleness: Divergence can always
be avoided by tuning η alone, and indeed most systems
always use a momentum of 0.9 (see comment above) which is
standard for the sync case [2]. However, we show in the purple
curve that at larger staleness values, additionally tuning the
momentum µ permits using a higher η. This does not change
hardware efﬁciency, but now gives an overall 5.85× speedup
over the baseline due to improved statistical efﬁciency.

Execution Strategy: Finally, the blue line represents the
actual choice made by the optimizer. In addition to the selec-
tions above, recall that the optimizer did not choose 32 groups
but 4 groups (Figure 30), which further improves the statistical
efﬁciency to give an overall speedup of > 20× compared to
standard choices traditionally made by deep learning systems.
Note that changing the number of groups to 4 did not hurt
hardware efﬁciency because the FC server is already saturated
(see Section IV-B).

D. Scalability to a Larger Cluster

The previous section (Appendix F-C) showed that as a result
of the optimizer’s tradeoffs Omnivore is able to scale to 32
machines. In this ﬁnal experiment we compare Omnivore to
the best competitor from the small clusters, MXNet, on this
larger cluster. We use the same dataset and network as the
small cluster experiments, and deﬁne convergence the same
way (99% accuracy).

We attemped to open a cluster of 33 g2.8xlarge instances
but continuously ran into EC2 errors related to not enough
machines available (InsufﬁcientInstanceCapacity).

As in Section VI-B, we repeat the same procedure to apply
our optimizer to MXNet, i.e. we run each conﬁguration for
10 minutes, select the best execution strategy and learning
rate, and run that to convergence. The best strategy was once
again sync with η = 0.01. Speciﬁcally, as in the previous
experiments, we followed MXNet’s documentation and used
the EC2 master machine as the root, and put the other 32
workers in the hostﬁle. We ran MXNet for 10 minutes with
both sync/async and 4 orders of magnitude learning rate as
described above. This time all 4 orders of magnitude for η
were needed because for sync with 32 machines, after 10
minutes 0.001 and 0.01 were almost the same, although 0.001
was better by ∼ 5% points. Since this differed from the
optimal η from the 8 machine case, which was 0.01 (i.e.
statistical efﬁciency changed for the larger cluster), to be sure
we ran MXNet with each η to convergence and noticed that
in fact 0.01 was signiﬁcantly faster to converge in the end, as
was true on the smaller clusters. Similarly, for async after 10
minutes the best η was 0.00001, although this was close to
0.0001, therefore once again we ran both to convergence and
noticed that 0.0001 was faster to converge for MXNet. We did
not do this extended parameter tuning for Omnivore, and only
ran the 1 minute static runs described above, to make sure that

we were getting the best possible performance from MXNet
for our comparison.

For Omnivore the optimizer was used as described in the
previous section. The best result of MXNet and Omnivore is
shown in Figure 12(c). We see that Omnivore is 3.2× faster
than MXNet to converge now (it was 2.3× faster on the 9 CPU
cluster). In addition, we see that compared to Figure 12(a),
Omnivore sped up on the larger cluster but MXNet did not.
Therefore not only does the optimizer give speedups by not
relying solely on the sync strategy and by merging the FC
servers, but also enables scalability to more machines.

If we do not apply our optimizer to MXNet, Omnivore now
converges 20× faster. This is compared to MXNet’s async
strategy, which has poor statistical efﬁciency for 32 machines.
This 20× corresponds exactly to the speedup in the previous
section because the green curve in that section is MXNet
using the async strategy (i.e. they are the same point in the
tradeoff, see the discussion in Appendix F-C4) By applying
our optimizer to MXNet we select the sync strategy instead,
which lowers the gap with Omnivore to 3× on this cluster.
Therefore this section shows not only that the optimizer gives
speedups of more than an order of magnitude, but that it is
versatile and can be applied to existing tools.

E. End-To-End Experiments

The end-to-end result

is in Figure 10. We trained the
standard CaffeNet (same setup as in Appendix F-B) using
ImageNet-1000 on both systems using both the CPU-L and
GPU-S clusters. We time out each run after 8 hours and report
the training accuracy vs. time.

According to MXNet’s ofﬁcial performance tuning guide-
line 22, they recommend trying the sync strategy, but also
state that “if the model size is quite large or you use a
large number of machines, you may want to use dist async”.
Immediately above, they describe large as “models with size
>> 100MB such as AlexNet and VGG.” Because we are
training AlexNet and use up to 33 machines, which may be
considered large, then according to these instructions async
could be the best choice. Because they do not provide an
automatic mechanism to make this decision we followed this
advice and tried both strategies, as we did above in section
F.2. This required tuning the learning rate for each strategy.
We used the optimal learning rate obtained for each strategy
on ImageNet-8, as recommended by [29] which states that “the
best way to determine the correct learning rates is to perform
experiments using a small but representative sample of the
training set”. In addition, MXNet does not provide a learning
rate schedule for their AlexNet example (as of the writing
of this study) so we use the standard learning rate schedule
of [2] which decreased the learning rate by 10× when training
plateaued.

For Omnivore, we ran the optimizer end-to-end. We ensure
a 10% overhead by running the optimizer and then training for
10× the optimizer time before rerunning the optimizer. Each

22https://github.com/dmlc/mxnet/tree/db6f6a9418e5696b04be741a78a47ae877bb5505/

example/image-classiﬁcation

TABLE V
GRID SEARCH PARAMETERS FROM FIGURE 10 (B) ON CPU-L

Fig. 32. Recurrent Neural Network using 9 EC2 c4.4xlarge CPU machines.

TABLE IV
GRID SEARCH PARAMETERS FROM FIGURE 10 (A) ON GPU-S

Phase
cold
phase 1
phase 2

µ
0.6
0.6
0.6

η
0.01
0.001
0.001

Phase
cold
phase 1
phase 2

µ
0.6
0.6
0.3

η
0.01
0.001
0.001

g
4
8
8

g
2
4
4

time the optimizer runs, it searches momentum, µ and learning
rate, η, either reducing one, reducing both, or keeping them
the same. The grid search results for each phase in Figure 10
(a) and (b) are shown in Table IV and Table V. On the GPU-S
cluster, the ﬁrst time the optimizer ran µ remained at 0.6 but η
decreased from 0.01 to 0.001. The second time, the parameters
did not change. On the CPU-L cluster, the ﬁrst optimizer run
made the same choices: µ also began at 0.6, and remained 0.6,
while η decreased from 0.01 to 0.001. Following the second
optimizer run, µ reduced from 0.6 to 0.3, and η remained
unchanged. Note that the cold-start (ﬁrst) phase of the CPU-
L run in Figure 10 shows a very large slope, and then after
the optimizer run this slope decreases. In ongoing work we
are exploring a slope-aware optimizer which would not rerun
the optimizer at this point but continue the execution. With
this change we expect the gap against competitor systems to
increase signiﬁcantly.

F. Preliminary RNN/LSTM Result

To understand if our tradeoff applies more broadly, we
implemented the Recurrent Neural Network model and LSTM
proposed by Graves [30]. Following the same protocol as
Figure 28 and using the CPU-S cluster, we see in Figure 32
that the tradeoff between statistical efﬁciency and hardware
efﬁciency is comparable, and choosing a completely syn-
chronous or asynchronous conﬁguration can be up to 2×
slower than the optimal conﬁguration. 23

G. Comparison to Standard Schedules

We have shown that tuning is critical for good performance.
We next validate the hypothesis that Omnivore’s optimizer out-
performs standard tuning and parameter scheduling methods.
To validate this, we run Omnivore on the full ImageNet using
the standard CaffeNet. We run two versions of Omnivore: (1)
Omnivore (Default Schedule), which uses CaffeNet’s default
learning rate schedule that decreases the learning rate by 10×
every 100,000 iterations; and (2) Omnivore, which uses the
standard Omnivore optimizer. To be fair, both versions use the
same grid search strategies to select the optimal learning rate,
momentum, and number of compute groups at the beginning.

23 We also see a similar tradeoff in the LSTM variant proposed by Graves [30].

Fig. 33. Comparison of Omnivore’s optimizer to CaffeNet’s default learning
rate schedule on Full ImageNet with AlexNet.

In addition, we run Omnivore for 10× the optimizer time
before it re-optimizes the parameters.

Figure 33 shows the training loss vs. wall-clock time. The
two plateaus shown in Omnivore correspond to the times
Omnivore re-optimizes the parameters. The losses of both
Omnivore (Default Schedule) and Omnivore decrease over
time. However, after the ﬁrst parameter re-tuning, Omnivore’s
loss starts to decrease more rapidly. Finishing at 36K seconds,
Omnivore is 1.5× faster to achieve the same loss as Omni-
vore (Default Schedule). Omnivore does not require the user
to specify the number of iterations to run before re-optimize
the parameters.

H. Comparison to Bayesian Optimizer

We compare our simple optimizer with the state-of-the-art
Bayesian optimization approach that explores the parameter
space of CNNs. The results are shown in Figure 34. We follow
Snoek et al. [18] to model the search space as (η, µ, S, N )
where N is the number of epochs to run. We use the same
search space for (η, µ, S) as in our optimizer and measure
both the number of conﬁgurations and the total number of
epochs that the Bayesian optimizer needs to run before ﬁnding
a run that achieves an accuracy within 1% of Omnivore’s best
accuracy.

Our procedure is as follows. We ﬁrst run Omnivore to obtain
a run which reaches 99% convergence using the same dataset
and cluster as in Figure 12 (b). This took 80 epochs and 680
seconds. We then give the Bayesian optimizer N = 80 and
it tries to ﬁt η, µ and S in order to reach the lowest loss
(highest accuracy) by a timeout of 1000 seconds. It searches
N in the range 1, . . . , 80, S in the range 1, 2, 4, 8, µ in
the range 0.0, 0.3, 0.6, 0.9, and learning rates in the range

Fig. 35. Hardware and statistical penalty without tuning (momentum 0.9)

Fig. 34. Bayesian optimizer run on Imagenet-8 using the GPU-S cluster.

0.1, 0.01, 0.001, 0.0001, 0.00001, i.e. the same as Omnivore
searches.

It took the Bayesian optimizer on average 12 runs before
ﬁnding a strategy which achieves accuracy within 1% of
Omnivore’s run. On average this takes 6× more epochs
than just training that strategy to convergence, which makes
the Bayesian approach infeasible to run on Imagenet 1000
(whereas Omnivore’s optimizer incurred only a 10% over-
head).

Compared with our optimizer, one difference is that we
are using the ﬁrst minute’s execution as a proxy for a longer
run, while on the other hand, Snoek et al. have the number
of epochs to run as a parameter to explore and do not
share information across runs. It is of course possible to use
Bayesian optimization to guide our grid search for the ﬁrst
minute, however, it is future work to integrate this heuristic
into the Bayesian optimization framework in a principled way.

APPENDIX G
TENSORFLOW EXPERIMENTS

In this section we see that

tuning momentum can sig-
niﬁcantly improve the performance on platforms other than
Omnivore by running experiments on TensorFlow [22]. We use
TensorFlow r0.9 starting with the fully synchronous and fully
asynchronous implementations of Inception-v3 for ImageNet
[12] found in [32]. Synchronous training has each worker send
its gradients to the parameter server. The parameter server
aggregates all the gradients, applies them to update the weights
and then sends the new weights to all the workers. In the
asynchronous conﬁguration, each worker sends its gradients
to the parameter server. The gradients are immediately applied
to the weights and the new model is returned to the worker.
On 32 GPU workers, asynchronous training reaches the
same loss as its synchronous counterpart 1.5× faster, when
properly tuned. In contrast, when momentum is ﬁxed to 0.9
for both, synchronous training is faster. We also implement
compute groups on top of the code from [12] and report the
tradeoff ﬁgures for a 32 worker conﬁguration. We discuss the

Fig. 36. Hardware and statistical penalty with tuning

experimental setup and discuss results on momentum tuning
and compute groups.

A. Experimental Setup

We deployed 8 g2.8xlarge instances on AWS, each equipped
with 4 GPUs. We allocate 1 GPU per worker node, meaning
there are 4 workers per instance and a total of 32 worker
nodes. There is a single parameter server that runs on one of
the instances. Each worker uses a batch size of 32 images,
as suggested in [32].24 Notice that the suggested setup uses a
per-worker batch-size as opposed to the per-group batch-size
we used on Omnivore. We used the SGD with momentum
optimizer for Inception-v3 as opposed to RMSProp with
momentum used in [12].

We perform our experiments after a warm start: we train
the Inception-v3 network synchronously on ImageNet, until
it reaches 50% training accuracy and take a snapshot. This
snapshot is used as the starting point for all measured runs.
We run each set of parameter values for 1 hour. We grid seach
momentum values in {0.0, 0.3, 0.6, 0.9} and learning rates in
{0.005, 0.01, 0.05}. We experimented with other values and
found these to capture the range of optimal learning rates. We
measure the loss achieved by each conﬁguration after 1 hour
of execution.

B. Results

We measure the time and number of iterations it takes to
reach a target loss and then perform hardware and software
efﬁciency analysis. Figure 35 shows the normalized statistical
penalty, hardware penalty and wall clock time of training to
the target loss for both the synchronous and asynchronous
conﬁgurations, starting from the same snapshot. In this ﬁgure,

24 This is also the largest batch size that can ﬁt on a single GPU of the

g2.8xlarge instances.

same number of iterations. As Figure 11 showed, Omnivore’s
speed on the c4.4xlarge instance is 0.57× the speed of the
g2.2xlarge instance. This ratio closely matches the FLOPS
ratio 0.7/1.2. Therefore we observe that running on a CPU
instance is 2.1× more expensive than a GPU instance, due to
the difference in the FLOPS/dollar ratio for these instances.
This suggests that on cloud services such as Google Compute
which do not have GPU instances, CPU-based deep learning
is a viable and cost-effective option when using Omnivore.
Moreover, organizations that can amortize the cost of CPUs
in more ways than GPUs may ﬁnd them to be a cheaper
alternative.

Distributed: In the distributed setting we consider again
the case of 9 machines and compare Omnivore running
on the GPU cluster (g2.8xlarge, $2.60 per machine-hr, 4.8
TFLOPS per machine) and CPU cluster (c4.4xlarge, $0.838
per machine-hr, 0.7 TFLOPS per machine). The difference in
peak FLOPS between these clusters is 6.8×, and the speedup
to convergence obtained by Omnivore on the GPU cluster
compared to the CPU cluster is 5×–note it
is not quite
6.8× because network speed does not scale with the node
throughput. If we consider only hardware efﬁciency (since
statistical efﬁciency is unrelated to the underlying hardware),
the GPU cluster is 5.6× faster than the CPU cluster, which
is nearly the FLOPS ratio. As in the single-machine case
therefore it is only the FLOPS/dollar ratio which matters. The
GPU cluster is more cost-effective, now by a factor of 1.8×.
These results show that CPU deep learning is not sig-
niﬁcantly different from GPU in terms of consumer cost,
and it will be exciting to see how these trends change for
future CPUs which have increased SIMD parallelism as well
as newer GPUs which optimize for lower power. As SIMD
processor bandwidth has been doubling in each generation, it
seems that CPU training may indeed catch GPUs relatively
soon.

A. Distributed Calculation

The ratio of peak FLOPS of the GPU cluster / CPU cluster
is 4.9/0.74 = 6.6. Considering the optimal points chosen by
our optimizer, the CPU / GPU time to convergence is 5×.
If statistical efﬁciency is ignored, and we compare only the
speeds of the async cases on each cluster, the ratio is now
34s/iter for the CPU cluster and 6 s/iter for the GPU, or 5.6×,
which almost matches the ratio in device FLOPS. Given that
GPU cluster is $2.6/$0.838 = 3.1×more expensive, the GPU
cluster is 1.8× cheaper per iteration which matches closely
with the FLOPS/dollar ratio.

APPENDIX I
APPENDIX FOR CONCLUSIONS (SECTION VIII)

Our study ﬁrst demonstrated that on a single machine we
could achieve CPU speeds proportional to the device FLOPS,
showing end-to-end speedups of more than 5.5× on EC2 CPU
instances over state-of-the-art tools. With this improved CPU
speed we showed that CNN computations are compute-bound.
This allows the underlying hardware in a machine to be treated

Fig. 37. Hardware efﬁciency, statistical efﬁciency and wall clock time when
tuning momentum and learning rate.

the momentum parameter is set to the standard 0.9 value (the
value used in [12] is not reported in the paper, but available
code [32] suggests it was 0.9). The learning rate is tuned
using the grid described above. The statistical penalty is high
for asynchronous training, which results in longer wall clock
time to reach the same loss compared to synchronous training.
As expected, the hardware penalty is lower for asynchronous
training. Figure 36 shows the results of the same experiments,
but using a grid search to tune momentum and learning rate.
The statistical penalty for asynchronous training relative to
synchronous is now improved by a factor of about 2.4×.
Tuning in this case is result-changing. The asynchronous
conﬁguration reaches the same loss in less time compared to
synchronous training.

C. Compute Groups

We implemented compute groups in TensorFlow r0.9, used
the same experimental setup as before and report results on
Inception-v3. Our goal is to understand the tradeoffs on this
different platform. Figure 37 reports the performance curves
for this setup. As in some of our Omnivore experiments,
the statistical efﬁciency remains nearly ﬂat when tuning.
This allows us to take advantage of the better hardware
efﬁciency of asynchronous settings. We see that, in this case,
32 nodes are not enough for the limits of asynchrony to start
showing. We expect that given a larger number of nodes,
the optimal conﬁguration will not be fully asynchronous, but
rather some intermediate compute groups setting. This result,
though under a different setup, contradicts the—reported but
not demonstrated—claim that hybrid conﬁgurations do not
perform better than fully synchronous training in TensorFlow.
We attribute this to the fact that experiments in [12] do not
involve any momentum tuning.

APPENDIX H
APPENDIX STUDYING TOTAL COST OF OWNERSHIP (TCO)

Our study showed that CNN training is compute-bound
regardless of the compute device used. Given that we can
now train CNNs on CPUs proportional to the CPU FLOPS,
this opens new questions in total cost of ownership (TCO) for
running CNN systems. We discuss those trends and changes.
Single Node: We compare the price of running Omnivore
on a GPU instance (g2.2xlarge, $0.65/hr, 1.2 TFLOPS) and
a CPU instance (c4.4xlarge, $0.838/hr, 0.7 TFLOPS) for the

as a black-box, and we are 2.7× faster than other systems on 4
GPUs and also 15% faster on a single GPU by using the weak
CPU alongside the EC2 instance’s GPU. More generally, we
show that each device or node in a cluster can be treated as a
black-box that is characterized only by the throughput which
it provides and is irrelevant to the type of hardware on that
node (e.g., CPUs or GPUs).

Our second contribution was an empirical study of the
factors affecting time to convergence for distributed deep
learning training, and a novel, theoretical characterization of
asynchrony which demonstrates that by tuning algorithmic
(explicit) momentum in SGD there is no statistical penalty
associated with asynchronous execution. We justiﬁed this
empirically. We deﬁned a tradeoff space and demonstrated
that the execution strategy and server architecture were key in
reducing the total time to convergence. We further showed that
all existing distributed deep learning systems fall somewhere
along this tradeoff space, but do not optimize within the space.
Finally, we studied each of these tradeoffs by decoupling
their impact on hardware and statistical efﬁciency. This made
it possible to study these factors in isolation and build an
optimizer which optimizes within the tradeoff space. We
showed both theoretically and empirically the need to jointly
tune hyper-parameters with execution strategies in order to
avoid slower convergence or divergence. We show that our
optimizer provides a > 20× reduction in time to convergence
compared to other systems which select sub-optimal points in
the space, and we also show that our optimizer is versatile by
applying it to existing tools. In doing so, we close the gap
between our system to 3× faster than other systems, in some
cases also preventing divergence in those other tools.

