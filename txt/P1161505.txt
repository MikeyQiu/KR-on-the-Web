0
2
0
2
 
b
e
F
 
8
 
 
]

V
C
.
s
c
[
 
 
1
v
9
1
2
3
0
.
2
0
0
2
:
v
i
X
r
a

EXOCENTRIC TO EGOCENTRIC IMAGE GENERATION VIA PARALLEL GENERATIVE
ADVERSARIAL NETWORK

Gaowen Liu1, Hao Tang1,2, Hugo Latapie3, Yan Yan1∗

1Department of Computer Science, Texas State University, USA
2DISI, University of Trento, Italy
3Chief Technology & Architecture Ofﬁce, Cisco, USA

ABSTRACT

problem in literature.

Cross-view image generation has been recently proposed to
generate images of one view from another dramatically dif-
ferent view. In this paper, we investigate exocentric (third-
person) view to egocentric (ﬁrst-person) view image genera-
tion. This is a challenging task since egocentric view some-
times is remarkably different from exocentric view. Thus,
transforming the appearances across the two views is a non-
trivial task. To this end, we propose a novel Parallel Gener-
ative Adversarial Network (P-GAN) with a novel cross-cycle
loss to learn the shared information for generating egocentric
images from exocentric view. We also incorporate a novel
contextual feature loss in the learning procedure to capture
the contextual information in images. Extensive experiments
on the Exo-Ego datasets [1] show that our model outperforms
the state-of-the-art approaches.

Index Terms— Egocentric, Exocentric, Cross-View Im-

age Generation, Parallel GANs

1. INTRODUCTION
Wearable cameras, also known as ﬁrst-person cameras, nowa-
days are widely used in our daily lives since the appearance of
low price but high quality wearable products such as GoPro
cameras. Meanwhile, egocentric (ﬁrst-person) vision is also
becoming a critical research topic in the ﬁeld. As we know,
egocentric view have some unique properties other than ex-
ocentric (third-person) view. Traditional exocentric cameras
usually give a wide and global view of the high-level appear-
ances happened in a video. However, egocentric cameras can
capture the objects and people at a much ﬁner level of gran-
ularity. In the early egocentric vision studies, researchers [2]
found that people perform different activities or interacting
with objects from a ﬁrst-person egocentric perspective and
seamlessly transfer knowledge between egocentric and exo-
centric perspective. Therefore, analyzing the relationship be-
tween egocentric and exocentric perspectives is an extremely
useful and interesting topic for image and video understand-
ing. However, there is few research to address this important

∗Corresponding author.

Recently, Generative Adversarial Networks (GANs) [3]
have been shown effectively in image generation tasks. Isola
et al. [4] propose Pix2Pix adversarial learning framework on
paired image generation, which is a supervised model and
uses a conditional GAN framework to learn a translation func-
tion from input to output image domain. Zhu et al. [5] intro-
duce CycleGAN which develops cycle-consistency constraint
to deal with unpaired image generation. However, these ex-
isting works consider an application scenario in which the
objects and the scenes have a large degree of overlapping
in appearance and view. Recently, some works investigate
cross-view image generation problems to generate a novel
scene which is drastically different from a given scene image.
This is a more challenging task since different views share lit-
tle overlap information. To tackle this problem, Regmi and
Borji [6] propose X-Fork and X-Seq GAN-based architec-
ture using an extra semantic segmentation map to facilitate
the generation. Moreover, Tang et al. [7] propose a multi-
channel attention selection module within a GAN framework
for cross-view image generation. However, these methods are
not able to generate satisfactory results due to the drastically
differences between exocentric and egocentric views.

To bridge egocentric and exocentric analaysis, in this pa-
per we propose a novel Parallel GAN (P-GAN) to generate
exocentric images from egocentric view. P-GAN framework
is able to automatically learn the shared information between
two parallel generation tasks via a novel cross-cycle loss and
hard-sharing of network layers. We also utilize a novel con-
textual loss in our objective function to capture texture infor-
mation over the entire images. To the best of our knowledge,
we are the ﬁrst to attempt to incorporate a parallel genera-
tive network for exocentric to egocentric image translation.
The proposed P-GAN is related to CoGAN [8] and Dual-
GAN [9]. However, CoGAN and DualGAN have limited
ability in generating image pairs with dramatically different
viewpoints. As shown in Fig. 1, our architecture is designed
in a bi-directional parallel fashion to discover the shared in-
formation between egocentric and exocentric images. Two
parallel GANs are trained simultaneously with hard-sharing

Fig. 1: The pipeline of our P-GAN model. It consists of two parallel generators (G1, G2) and two discriminators (D1, D2).
The total loss contains pairs of L1 loss, contextual loss and adversarial loss.

of certain layers.

In summary, our contributions can be highlighted as fol-
lows. (i) A novel P-GAN is proposed to learn the shared in-
formation between different views simultaneously via a novel
cross-cycle loss. (ii) A novel contextual feature loss is incor-
porated in the training to capture the contextual information.
(iii) Experiments on Exo-Ego dataset show the effectiveness
of our hard-sharing of network layers in multi-directional par-
allel generative models.

2. PARALLEL GANS

2.1. Network Architecture
Cross-view exocentric to egocentric image synthesis is a chal-
lenging task, because these two views have little overlapping
in image appearance. Most existing works on cross-view im-
age synthesis are based on GANs. A traditional GAN consists
of a generative model and a discriminative model. The objec-
tive of the generative model is to synthesize images resem-
bling real images, while the objective of the discriminative
model is to distinguish real images from synthesized ones.
Both the generative and discriminative models are realized
as multi-layer perceptrons. Since there will be some shared
high-level concept information in a pair of corresponding im-
ages between exocentric and egocentric views, we propose
a P-GAN with two GANs in parallel which is able to learn
the shared high-level semantic information among different
views. Fig. 1 shows our framework which contains two gen-
erators and two discriminators. A set number of layers from
two generators are shared across P-GAN. We force the ﬁrst
three layers of two generators to have the identical structure
and share the weights, and the rest layers are task-speciﬁc.
The experiments show that sharing three layers of generators
yield the best performance.

Particularly, we employ U-Net [10] as the architecture of

our generators G1 and G2. We impose skip connection strat-
egy from down-sampling path to up-sampling path to avoid
vanishing gradient problem. To learn the shared information
between exocentric and egocentric view, we perform hard-
sharing in the ﬁrst three layers of down-sampling path. We
adopt PatchGAN [4] for the discriminator D1 and D2. The
feature maps for contextual loss are extracted by the VGG-19
network pretrained on ImageNet.

2.2. Overall Optimization Objective
The training objective can be decomposed into four main
components which are contextual loss, adversarial loss, cross-
cycle loss and reconstruction loss.
Contextual loss. Different from the commonly used L1 loss
function which compares pixels at the same spatial coordi-
nates between the generated image and the target image, we
incorporate contextual loss in our P-GAN learning frame-
work. The key idea is to measure similarity between images
at the high-level feature space.

ego={I (cid:48)

Given a generated fake image I (cid:48)

ego and a real image Iego in
egocentric view, we obtain a list of VGG-19 [11] features as
Iego={Ii} and I (cid:48)
ego),
ψ means VGG-19 feature. i, j are i-th and j-th layer in the
network ψ. The similarity between the generated image I (cid:48)
ego
and the real image Iego in egocentric view can be deﬁned as
follows,

j}, where Ii=ψi(Iego), I (cid:48)

j=ψj(I (cid:48)

SIi,I (cid:48)

j

= exp

1 −

(cid:18)

1 − dij
minkdik + ζ

(cid:19)

/h

where dij is the cosine distance between Iego and I (cid:48)
ego. We
deﬁne ζ=1e−5, h=0.5 in our experiments. The similarity can
be normalized as,

(1)

(2)

¯Sij =

j

SIi,I (cid:48)
k SIi,I (cid:48)

k

(cid:80)

Fig. 2: Results generated by different methods on Side2Ego dataset. These samples were randomly selected for visualization
purposes. Columns from left to right are: Input, Pix2pix [4], CycleGAN [5], P-GAN (ours), X-Fork [6], X-Seq [6], Selection-
GAN [7], P-GAN + Segmentation map (ours), Ground Truth.

Then the contextual loss is formulated as follows,

Lcont(Ii, I (cid:48)

j) =

1

(cid:88)

max ¯Sij

(3)

max(| Iego |, | I (cid:48)

ego |)

j

where | · | denotes the numbers of feature maps.
Cross-cycle loss. As shown in Fig. 1, we employ U-Net [10]
as our generators G1 and G2. Each U-Net contains a down-
sampling encoder EN which is a feature contracting path,
and an up-sampling decoder DE which is a feature expand-
ing path. Inspired by the U-net properties, we design a novel
cross-cycle loss as follows,
LX (G1, G2) =E

[(cid:107)Iexo − DE2(EN1(Iexo))(cid:107)1] +

Iexo,I (cid:48)

exo

λ1E

ego

Iego,I (cid:48)

[(cid:107)Iego − DE1(EN2(Iego))(cid:107)1]
(4)
Adversarial loss. Recent works [3, 12, 13, 14, 15] have
shown that one can learn a mapping function by tuning a gen-
erator and a discriminator in an adversarial way. Assuming
we target to learn a mapping G : Iexo→Iego from input exo-
centric image Iexo to output egocentric image Iego. The gen-
erator G is trained to produce outputs to fool the discriminator
D. The adversarial loss can be expressed as,
LGAN1(G1, D1) =EIexo,Iego [log D1(Iexo, Iego)] +

E

Iexo,I (cid:48)

ego

[log(1 − D1(Iexo, G1(Iexo)))]
(5)

LGAN2 (G2, D2) =EIego,Iexo [log D2(Iego, Iexo)] +

E

Iego,I (cid:48)

exo

[log(1 − D2(Iego, G2(Iego)))]
(6)

The adversarial loss is the sum of Eq. (5) and Eq. (6).
LGAN = LGAN1(G1, D1) + λ2LGAN2 (G2, D2)
(7)
Reconstruction loss. The task of the generator is to recon-
struct an image as close as the target image. We use L1 dis-
tance in the reconstruction loss,
Lre(G1, G2) =E

[(cid:107)Iego − DE1(EN1(Iexo))(cid:107)1] +

Iexo,I (cid:48)

ego

λ3E

Iego,I (cid:48)

exo

[(cid:107)Iexo − DE2(EN2(Iego))(cid:107)1]
(8)

Overall loss. The total optimization loss is a weighted sum
of the above losses. Generators G1, G2 and discriminators
D1, D2 are trained in an end-to-end fashion to optimize the
following objective function,

L = LGAN + λ4LX + λ5Lre + λ6Lcont

(9)

where λi’s are the regularization parameters.

3. EXPERIMENTAL RESULTS
Datasets. To explore the effectiveness of our proposed P-
GAN model, we compare our model with the state-of-the-art
methods on Exo-Ego dataset [1] which contains two different
viewpoint subsets (Side2Ego and Top2Ego). This dataset is
challenging due to two reasons. First, it contains dramatically
different indoor and outdoor scenes. Second, the dataset is
collected simultaneously by an exocentric camera (side and
top view) and an egocentric body-worn wearable camera. It
includes a huge amount of blurred images for egocentric view.
For Side2Ego subset, there are 26,764 pairs of images for
training and 13,788 pairs for testing. For Top2Ego subset,
there are 28,408 pairs for training and 14,064 pairs for test-
ing. All images are in high-resolution 1280×720 pixels.
Experimental Setup. We compare our P-GAN with both
single-view image generation methods [4, 5] and cross-view
image generation methods [6, 7]. We adopt the same experi-
mental setup as in [4, 6, 7]. All images are scaled to 256×256.
We enable image ﬂipping and random crops for data augmen-
tation. To compute contextual loss, we follow [16] and use the
VGG-19 network to extract image feature maps pretrained on
ImageNet. We train 35 epochs with the batch size of 4. In our
experiments, we set λ1=10, λ2=10, λ3=100, λ4=10, λ5=1,
λ6=1 in Eq. (4), (7), (8) and (9), respectively. The state-
of-the-art cross-view generation methods, i.e., X-Fork [6], X-
Seq [6] and SelectionGAN [7] utilize segmentation map to fa-
cilitate target view image generation. To compare with these
cross-view methods, we adopt ReﬁneNet [17, 18] to gener-
ate segmentation maps on Side2Ego and Top2Ego subsets as

Fig. 3: Results generated by different methods on Top2Ego dataset. These samples were randomly selected for visualization
purposes. Columns from left to right are: Input, Pix2pix [4], CycleGAN [5], P-GAN (ours), X-Fork [6], X-Seq [6], Selection-
GAN [7], P-GAN + Segmentation map (ours), Ground Truth.
Table 1: SSIM, PSNR, Sharpness Difference (SD), KL score (KL) and Accuracy of different single-view image generation
methods. For these metrics except KL score, higher is better.

Dataset

Method

SSIM

PSNR

SD

KL

Top2Ego

18.1002
18.5678
18.6043
19.8664
19.7533
20.6521
Table 2: SSIM, PSNR, Sharpness Difference (SD), KL score (KL) and Accuracy of different cross-view image generation
methods. For these metrics except KL score, higher is better.

Pix2pix[4]
CycleGAN [5]
Ours
Pix2pix [4]
CycleGAN [5]
Ours

62.74 ± 1.78
52.09 ± 1.69
31.46 ± 1.74
75.27 ± 2.01
62.41 ± 2.41
13.92 ± 1.53

15.0532
15.5486
17.0236
16.0716
15.9678
17.9951

0.2514
0.2806
0.3098
0.3946
0.4017
0.4908

Side2Ego

Top-1
Accuracy (%)
1.22
1.24
2.10
0.99
5.90
1.81
5.18
3.20
7.60
4.18
30.80
16.21

Top-5
Accuracy (%)
4.35
4.21
2.72
5.37
9.17
5.74
13.30
8.41
21.45
15.62
46.51
27.57

Dataset

Method

SSIM

PSNR

SD

KL

Top2Ego

Side2Ego

X-Fork [6]
X-Seq [6]
SelectionGAN [7]
Ours
X-Fork [6]
X-Seq [6]
SelectionGAN [7]
Ours

0.2952
0.3522
0.5047
0.5287
0.4499
0.4763
0.5128
0.5205

15.8849
16.9439
22.0244
22.2891
17.0743
17.1462
18.3021
19.4521

18.7349
19.2733
19.1976
19.2389
20.4443
20.7468
20.9426
20.9684

63.96±1.74
54.91 ± 1.81
10.07 ± 1.29
12.07 ± 1.69
51.20 ± 1.94
45.10 ± 1.95
7.26 ± 1.27
25.25 ± 1.88

Top-1
Accuracy (%)
1.22
0.8
1.77
1.07
16.55
8.85
29.67
9.76
9.76
4.49
12.70
6.51
37.49
20.84
39.08
20.96

Top-5
Accuracy (%)
4.08
3.16
6.94
4.29
33.90
24.32
51.79
24.80
19.44
11.63
19.36
11.97
65.22
42.51
66.00
42.58

in [6, 7]. The generated segmentation maps are used as the
conditional input of G1 and G2. The proposed P-GAN is im-
plemented using PyTorch.
Evaluation Metrics. We apply metrics such as top-k pre-
diction accuracy and KL score for evaluations as in [7, 6].
We also employ pixel-level similarity metrics, i.e., Structural-
Similarity (SSIM), Peak Signal-to-Noise Ratio (PSNR) and
Sharpness Difference (SD). These metrics evaluate the gener-
ated images in a high-level feature space.
Quantitative Results. The quantitative results are presented
in Tables 1 and 2. We observe that our P-GAN achieves better
results than state-of-the-art methods in most cases. Compared
with single-view image generation methods, our P-GAN out-
performs Pix2pix [4] and CycleGAN [5]. On the other hand,
we also achieve better results than other cross-view image
generation methods in most metrics while incorporating se-
mantic segmentation map as in the SelectionGAN [7].
Qualitative Results. Qualitative results are shown in Fig. 2
and Fig. 3. The results conﬁrm that the proposed P-GAN net-
work has the ability to transfer the image representations from

exocentric to egocentric view, i.e., objects are in the correct
positions for generated egocentric images. Results show that
egocentric images generated by P-GAN are visually much
better compared with other baselines.

4. CONCLUSIONS

In this paper, we introduce a novel P-GAN which is able to
learn shared information between cross-view images via a
novel cross-cycle loss for a challenging exocentric to egocen-
tric view image generation task. Moreover, we incorporate
a novel contextual feature loss to capture the contextual in-
formation in images. Experimental results demonstrate that
the hard-sharing of network layers in multi-directional paral-
lel generative models can be used to increase the performance
of cross-view image generation.
ACKNOWLEDGEMENT. This research was partially sup-
ported by a gift donation from Cisco Inc. and NSF NeTS-
1909185 and NSF CSR-1908658. This article solely reﬂects
the opinions and conclusions of its authors and not the fund-
ing agents.

[15] Hao Tang, Dan Xu, Gaowen Liu, Wei Wang, Nicu Sebe,
and Yan Yan, “Cycle in cycle generative adversarial net-
works for keypoint-guided image generation,” in ACM
MM, 2019. 3

[16] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor,
“The contextual loss for image transformation with non-
aligned data,” in ECCV, 2018. 3

[17] Guosheng Lin, Fayao Liu, Anton Milan, Chunhua Shen,
and Ian Reid, “Reﬁnenet: Multi-path reﬁnement net-
works for dense prediction,” IEEE TPAMI, 2019. 3

[18] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian
“ReﬁneNet: Multi-path reﬁnement networks
Reid,
for high-resolution semantic segmentation,” in CVPR,
2017. 3

5. REFERENCES

[1] Mohamed Elfeki, Krishna Regmi, Shervin Ardeshir, and
Ali Borji, “From third person to ﬁrst person: Dataset
in CVPR,
and baselines for synthesis and retrieval,”
2019. 1, 3

[2] Takeo Kanade and Martial Hebert,

“First-person vi-
sion,” Proceedings of the IEEE, vol. 100, no. 8, pp.
2442–2453, 2012. 1

[3] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio, “Generative adversarial
nets,” in NeurIPS, 2014. 1, 3

[4] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros, “Image-to-image translation with conditional ad-
versarial networks,” in CVPR, 2017. 1, 2, 3, 4

[5] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros,
“Unpaired image-to-image translation using
cycle-consistent adversarial networkss,” in ICCV, 2017.
1, 3, 4

[6] Krishna Regmi and Ali Borji, “Cross-view image syn-
thesis using conditional gans,” in CVPR, 2018. 1, 3,
4

[7] Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J.
Corso, and Yan Yan, “Multi-channel attention selection
gan with cascaded semantic guidancefor cross-view im-
age translation,” in CVPR, 2019. 1, 3, 4

[8] Ming-Yu Liu and Oncel Tuzel, “Coupled generative ad-

versarial networks,” in NeurIPS, 2016. 1

[9] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong, “Du-
algan: Unsupervised dual learning for image-to-image
translation,” in ICCV, 2017. 1

[10] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in CVPR, 2015. 2, 3

[11] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” in ICLR, 2015. 2

[12] Tim Salimans, Ian Goodfellow, Wojciech Zaremba,
Vicki Cheung, Alec Radford, and Xi Chen, “Improved
techniques for training gans,” in NeurIPS, 2016. 3

[13] Shaohui Liu, Yi Wei, Jiwen Lu, and Jie Zhou, “An im-
proved evaluation framework for generative adversarial
networks,” arXiv preprint arXiv:1803.07474, 2018. 3

[14] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio,
and Wenjie Li, “Mode regularized generative adversar-
ial networks,” in ICLR, 2017. 3

