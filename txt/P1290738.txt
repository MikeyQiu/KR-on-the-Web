9
1
0
2
 
y
a
M
 
1
1
 
 
]

V
C
.
s
c
[
 
 
1
v
2
3
4
4
0
.
5
0
9
1
:
v
i
X
r
a

JOINT LEARNING OF SELF-REPRESENTATION AND INDICATOR FOR MULTI-VIEW
IMAGE CLUSTERING

Songsong Wu1,5, Zhiqiang Lu1, Hao Tang2, Yan Yan3, Songhao Zhu1, Xiao-Yuan Jing4, Zuoyong Li5

1Nanjing University of Posts and Telecommunications 2University of Trento
3Texas State University 4 Wuhan University
5 Industrial Robot Application of Fujian University Engineering Research Center Minjiang University

ABSTRACT

Multi-view subspace clustering aims to divide a set of multi-
source data into several groups according to their underlying
subspace structure. Although the spectral clustering based
methods achieve promotion in multi-view clustering, their
utility is limited by the separate learning manner in which
afﬁnity matrix construction and cluster indicator estimation
In this paper, we propose to jointly learn the
are isolated.
self-representation, continue and discrete cluster indicators
in an uniﬁed model. Our model can explore the subspace
structure of each view and fusion them to facilitate cluster-
ing simultaneously. Experimental results on two benchmark
datasets demonstrate that our method outperforms other ex-
isting competitive multi-view clustering methods.

Index Terms— Multi-view Clustering, Subspace Cluster-

ing, Self-representation Learning

Subspace learning based MVC clustering methods rely on
afﬁnity matrix learning followed by cluster indicator predic-
tion. However, these two procedures are accomplished sep-
arately, resulting that they are hardly optimal for clustering.
Besides, the desired discrete clustering indicator is approxi-
mated by a continuous matrix for the convenience in model
solving. This incurs the loss in cluster information, as re-
ported by [22, 23] for single view clustering.

To address the above issues, we proposed a joint learning
model for MVC in this paper. It can jointly learn subspace
representations, continuous labels, and discrete labels from
the multi-view data.
In solving the optimization problem,
continuous clustering label is only used as intermediate prod-
uct. The main contributions of our work is to combine sub-
space learning, continuous label learning, and discrete label
learning to obtained multi-view afﬁnity matrices constantly
updated with the inherent interactions between three steps.

1. INTRODUCTION

In real-world applications, images always own multiple views
as they are represented by multiple ways. For instance, three
typical views are pixel intensity, LBP feature, and Gabor co-
efﬁcients. Unsupervised clustering of images based on their
multiple views has attracted increasing attention recently [1,
2, 3]. Compared with single-view clustering (SVC) [4, 5, 6,
7, 8, 9, 10] that performs clustering relying on a single type
of feature, multi-view clustering (MVC) exploits the comple-
mentary information among views to facilitate clustering.

Multi-view clustering can be divided into three categories,
co-training MVC [11, 12, 13], multiple kernel learning MVC
[14, 15], and subspace learning MVC [4, 16, 17, 18, 19, 20,
21]. Co-training based methods pursue the optimal cluster-
ing in each view while minimize the disagreement among
them simultaneously. Multiple kernel learning based meth-
ods claims that the distinct information from each view can
be integrated with a linear combination of multiple kernels
to promote clustering performance. Subspace learning based
methods model data views as distinct low-dimensional sub-
spaces. In this paper, we focus on subspace MVC because of
it’s simplicity in computation and stability in performance.

2. RELATED WORK

SPC [22] selects the most informative to perform standard
spectral clustering algorithm. SSC [6] aims to ﬁnd a sparse
representation from the data, then the ﬁnal clustering result
can be obtained by spectral clustering. S3C [9] executes clus-
tering on each view and selects the best performance. Min-
Dis [11] aims to minimize the disagreement of two-view by
generating a bipartite graph and uses spectral clustering to ob-
tain the clustering results. ConvexReg SPC [24] learns a com-
mon representation for all views, and executes the standard
spectral clustering to get clustering results. RMSC [25] is ro-
bust multi-view spectral clustering method and uses a Markov
chain to cluster. Di-MSC [16] adopts the HSIC criterion norm
to obtain the diverse representations of different views with
considering the complementary information, then uses spec-
tral clustering to generate the clustering result. LT-MSC [17]
obtains the ﬁnal clustering result by capturing high-dimension
information of multi-view data. ECMSC [20] considers com-
plementary information with exclusivity term and guarantees
the consistency with a common indicator and uses the spectral
clustering algorithm to obtain clustering results. CSMSC [21]

3. THE PROPOSED METHOD

s.t. Zii = 0

tries to explore a common representation and speciﬁc repre-
sentation from multi-view data, and the clustering result can
be obtained by spectral clustering.

Beyond the above methods estimate discrete cluster indi-
cators with continuous indicators, [26, 23] learn discrete label
and then use discrete cluster labels directly for clustering in
single-view. s for as we have known, the issue of discrete and
continuous cluster indicator has not been touched in the case
of multi-view clustering. We propose to simultaneously learn
subspace representation, continuous indicator and discrete in-
dicator for multi-view clustering, rather than straightly extend
[26, 23] from single view clustering to multiple view cluster-
ing.

3.1. Formulation

Given sample set Xv ∈ Rdv×N ,v ∈ [1, 2, ...V ] with V views,
where N is the number of samples, and dv is the sample di-
mension of view v. The self-representation of samples for a
single view is written as

Xv = XvZv + Ev,

s.t. Zv(i,i) = 0

(1)

where Zv ∈ Rn×n is self-representation matrix and Ev ∈
Rdv×n is error term.

Under the principle of spectral clustering, the discrete in-

dicator is obtained by

arg min

F

tr(F T LvF ),

s.t. F ∈ Idx

(2)

where Idx means that F = [f1, f2, · · · fn] ∈ RK×N is
the discrete cluster indicator matrix with each column fi ∈
{0, 1}K×1 contains only one element 1 indicating the cluster
the sample is assigned to. The L in (2) is graph Laplacian
matrix with the afﬁnity matrix Wv = |Zv|+|Zv|T
. In practice,
the following model is solved instead as model 2 is NP-hard

2

arg min

P

tr(P T LvP ),

s.t. P T P = I

(3)

where P ∈ RK×N is the relaxed continuous indicator, and
the orthogonal constraint is to prevent from trivial solutions.
Take formula (1)(2)(3) into consideration, we have the

following model for multi-view subspace clustering

arg min
Zv,Ev,F,P,Q

V
(cid:88)

(cid:40) (cid:107)Xv − XvZv − Ev(cid:107)2

F + λ1(cid:107)Ev(cid:107)1
+λ2T r(P T LvP ) + λ3(cid:107)F − P Q(cid:107)2
F
s.t. Zv(i, i) = 0, P T P = I, QT Q = I, F ∈ Idx

v=1

(cid:41)

(4)
where λ1,λ2 and λ3 are positive hyper-parameters. The ﬁrst
and second term are associated with the self-representation in
each view. The third term is related to continuous indicator

seeking so as to avoid NP-hard issue in computation. The
fourth term build a linear relation between the continuous in-
dicator the desired discrete indicator with matrix Q ∈ Rc×c
to ensure these two indicators are consistent with each other.

3.2. Optimization

We solve model (4) by alternative optimization strategy, i.e.
optimizing one variable by ﬁxing the previous values of other
variables. To facilitate the narrative, we omit the subscript of
view tentatively.
Update Z. When E,F , P , and Q are ﬁxed, we have

arg min
Z

||X − XZ − E||2

F + λ2T r(P T LP )

(5)

By deﬁning Yij = (cid:107)Pi − Pj(cid:107)2
2 and using the deﬁnition of L,
we solve (5) by seek a optimal column of Z at one time based
on the following optimal model

arg min
Zi

||Zi − Vi||2

2 +

|Zi|T Yi,

s.t. Zii = 0

(6)

λ2
2

where Zi, Yi and Xi are the i-th column of Z,Y , and X re-
spectively, and Vi = KT Xi
i )−E.
i Xi
So far, we can get closed form solution based on (6) for that
if j = i, we have Zji = 0, and if j (cid:54)= i, we have

with K = X−(XZ−XiZ T

X T

Zji = sign(Vji)(|Vji| − λ2Yji
Vji − λ2Yji
Vji + λ2Yji
0,

)+
if Vji > + λ2Yji
if Vji < − λ2Yji
otherwise






,
,

4

4

4

4

4

(7)

Update E. When Z,F ,P , and Q are ﬁxed, we seek the op-
timal E using the same column-wise strategy as Z. Speciﬁ-
cally, for a single column Ei of E, we have

arg min
Ei

||(X − XZ)i − Ei||2

F + λ1||Ei||1

(8)

Where (X − XZ)i is the i-th column in the matrix. The
solution is provided by

Eji = sign(X − XZ)ji(|(X − XZ)ji| −



λ1
2
2 , if (X − XZ)ji > λ1
2 , if (X − XZ)ji < λ1

(X − XZ)ji − λ1
(X − XZ)ji + λ1
0, otherwise



2

2

)

+

(9)

Update P. When Z, F, E, and Q are ﬁxed, the model becomes

arg min
P

λ2T r(P T LP )+λ3||F −P Q||2

F , s.t.P T P =I (10)

The model can be efﬁciently solved by the algorithm pro-
posed by [27].

Algorithm 1 The proposed MVC method
Input: V view image sets {X =v}V

v=1, positive parameters

λ1,λ2,λ3.
Initialize Z1, Z2, · · · , ZV , P, Q with random values, and
set F, E = 0
repeat

1.Update Zv with (7)
2.Update Ev with (9)
3.Update P with (10).
4.Update Q with (12)
5.Update F with (14)
until Stopping criterion

Compute afﬁnity matrix by by W =

V
(cid:80)
v=1

|Zv|+|Zv|T
2

Perform spectral clustering using afﬁnity matrix W .

Output: Clustering result.

Update Q. When F, Z, E and P are ﬁxed, the model becomes

arg min
Q

λ3||F −P Q||2
F ,

s.t. QT Q = I

(11)

It is a orthogonal Procrustes problem [28], whose solution is
given as

Q = U V T

(12)

where U and V are the left and right singular value matrices
of F T P .
Update F. When Z, E, P and Q are ﬁxed, the model becomes

arg max
F

λ3T r(F T P Q),

s.t. F ∈ Idx

(13)

whose optimal solution can be obtained as follows

Fij =

(cid:40) 1, if j = arg max
0, otherwise

k

(P Q)ik

(14)

In summary, the proposed method can be expressed in Algo-
rithm 1.

4. EXPERIMENT

4.1. Experimental Settings

Dataset Description. We adopted two face image datasets
in the experiments which are used widely in image cluster-
ing [17, 18, 25]. In our experiment, each image sample has
three views: intensity, LBP [28], and Gabor [29]. Statistics
of the datasets are summarized in Table 1. (i) ORL is divided
into 40 different themes, each containing 10 images. All im-
ages are in a uniform black background and are All images
are taken from the top of the front. For some of these themes,
the images are taken at different times. In light conditions,
facial expressions (such as open eyes, closed eyes, laugh, and

non-laugh), facial details (glasses) and other aspects are dif-
ferent. (ii) Yale is consisted of 165 gray-scale images of 15
individuals, each individual has 11 images, including: center
light, glasses, happy, left light, no glasses, normal, right light,
sad, sleepy, surprised and wink.
Comparison Algorithm. We compare our method with
three single-view methods (SPC [22], SSC [6], S3C [9]) and
seven multi-view methods (Min-Dis [11], ConvexReg SPC
[24], RMSC [25], Di-MSC [16], LT-MSC [17], ECMSC
[20], CSMSC [21]). The optimal parameters of the pro-
posed method are set empirically based on grid searching.
Speciﬁcally, for ORL dataset,
the parameters are set as:
λ1=0.002,λ2=0.5, λ3=0.1 and for Yale dataset, they are set
as: λ1=0.003, λ2=0.7, λ3=0.2. The parameter values of
other methods are set according to the corresponding papers.
Evaluation Metrics. In our experiment, we use six metrics to
evaluate the clustering performance, i.e., Normalized Mutual
Information (NMI), Accuracy (ACC), Adjusted Rand Index
(ARI), F-score, Precision and Recall. ACC and NMI have
been adopted to evaluate clustering result [30, 31]. Precision,
F-score, ARI and Recall have been widely used in [32] to
measure the clustering quality. These six metrics evaluate the
different performance of clustering. For all metrics, higher
values indicate better cluster quality.
Performance Comparison. We repeat the random experi-
ments for 30 rounds and report the mean and standard devia-
tion as ﬁnal results. The clustering results on ORL and Yale
are provided in Table 1 and Table 2 respectively.

From Table 1, we observe that compared with Di-MSC,
we achieve improvements around 0.3%, 4.8%, 2.9%, 2.8%,
4.0%, and 1.2% in terms of NMI, ACC, ARI, F, P, and Re.
Compared with ECMSC, we achieve improvements around
3.2%, 2.1%, 1.4%, 2.0%, and 0.9% in terms of ACC, ARI,
F, P, and Re, because the afﬁnity matrix is constantly up-
dated due to add learning about discrete labels. Compared
with latest method (CSMSC), our experimental results im-
prove around 1.8%, 0.4%, 0.4%, and 6.4% in terms of ACC,
ARI, F, Re. The reason is that we consider the relationship
between subspace learning and spectral clustering. But our
results are lower than CSMSC in NMI and P, it seems that we
did not consider the complementary information of multiple
views.

Table 2 shows that the multi-view clustering results on
Yale dataset. The method LT-MSC has a signiﬁcant improve-
ment.
It seems that low-rank tensor is suitable for images
clustering. Di-MSC considers the diversity representations
of different views. Compared with Di-MSC, we achieve im-
provements around 5.5%, 8.3%, 9.5%, 8.0%, 7.3%, 7.5%
in terms of NMI, ACC, ARI, F, P, Re, respectively. The
major reason is that we put subspace representation and
spectral clustering into an optimization model. Compared
with ECMSC, we achieve improvements around 0.9%, 2.1%,
3.0%, 3.7%, 3.2%, 0.8% in six metrics, respectively. The
reason is that our method does not use continuous label to up-

single
single
single
Multiple
Multiple
Multiple
Multiple
Multiple
Multiple
Multiple
Proposed

single
single
single
Multiple
Multiple
Multiple
Multiple
Multiple
Multiple
Multiple
Proposed

Table 1: Clustering performances on ORL dataset (meanstandard deviation).

Method
SPCbest
SSCbest
S3Cbest
Min-Dis
RMSC
ConReg
LTMSC
DiMSC
ECMSC
CSMSC
Ours

Method
SPCbest
SSCbest
S3Cbest
Min-Dis
RMSC
ConReg
LTMSC
DiMSC
ECMSC
CSMSC
Ours

Table 2: Clustering performances on ORL dataset (meanstandard deviation).

NMI
0.884(0.002)
0.893(0.007)
0.902(0.012)
0.876(0.002)
0.872(0.012)
0.883(0.003)
0.930(0.002)
0.940(0.003)
0.947(0.009)
0.942(0.005)
0.943(0.005)

NMI
0.654(0.009)
0.671(0.011)
0.678(0.013)
0.645(0.005)
0.684(0.033)
0.673(0.023)
0.765(0.008)
0.727(0.010)
0.773(0.010)
0.784(0.001)
0.782(0.005)

ACC
0.726(0.025)
0.765(0.008)
0.784(0.009)
0.748(0.051)
0.723(0.025)
0.734(0.031)
0.795(0.007)
0.838(0.001)
0.854(0.011)
0.868(0.012)
0.886(0.016)

ACC
0.616(0.030)
0.627(0.000)
0.634(0.016)
0.615(0.043)
0.642(0.036)
0.611(0.035)
0.741(0.002)
0.709(0.003)
0.771(0.014)
0.752(0.001)
0.792(0.026)

ARI
0.655(0.005)
0.694(0.013)
0.705(0.019)
0.654(0.004)
0.644(0.029)
0.668(0.032)
0.750(0.003)
0.802(0.000)
0.810(0.012)
0.827(0.002)
0.831(0.019)

ARI
0.440(0.011)
0.475(0.004)
0.471(0.005)
0.433(0.006)
0.485(0.046)
0.466(0.032)
0.570(0.004)
0.535(0.001)
0.590(0.014)
0.615(0.005)
0.620(0.008)

F
0.664(0.005)
0.682(0.012)
0.698(0.018)
0.663(0.004)
0.654(0.028)
0.676(0.035)
0.768(0.007)
0.807(0.003)
0.821(0.015)
0.831(0.001)
0.835(0.019)

F
0.475(0.011)
0.517(0.007)
0.508(0.012)
0.470(0.006)
0.517(0.043)
0.501(0.030)
0.598(0.006)
0.564(0.002)
0.617(0.012)
0.640(0.004)
0.644(0.007)

P
0.610(0.006)
0.673(0.007)
0.688(0.012)
0.615(0.004)
0.607(0.033)
0.628(0.041)
0.766(0.009)
0.764(0.012)
0.783(0.008)
0.860(0.002)
0.804(0.023)

P
0.457(0.011)
0.509(0.003)
0.512(0.005)
0.446(0.005)
0.500(0.043)
0.476(0.032)
0.569(0.004)
0.543(0.001)
0.584(0.013)
0.673(0.002)
0.616(0.009)

Re
0.728(0.005)
0.764(0.005)
0.791(0.011)
0.718(0.003)
0.709(0.027)
0.731(0.030)
0.837(0.004)
0.856(0.004)
0.859(0.012)
0.804(0.003)
0.868(0.018)

Re
0.495(0.010)
0.547(0.004)
0.568(0.025)
0.496(0.006)
0.535(0.044)
0.532(0.029)
0.629(0.005)
0.586(0.003)
0.653(0.013)
0.610(0.006)
0.661(0.006)

(a) ORL

(b) Yale

Fig. 2: Convergence curve of our method on ORL and Yale.

can see simultaneously learn subspace representations of each
view, continuous labels, and discrete labels will be beneﬁcial
for multi-view subspace clustering. As shown in Fig. 2, “ob-
jvalue” represents the value of the objective function; “steps”
is the number of iterations. We show the convergence of our
method on ORL and Yale datasets. We can see that our algo-
rithm converge quickly on both datasets.

5. CONCLUSIONS

We propose a novel multi-view image clustering approach by
jointly learning the self-representation of images and cluster
indicators. The two processes promote each other to achieve
complementary information confusion for multi-view cluster-
ing. The experimental results of multi-view face images clus-
tering demonstate that our method outperforms other existing
competitive multi-view clustering algorithms.

6. ACKNOWLEDGEMENT

This work was supported in part by Industrial Robot Applica-
tion of Fujian University Engineering Research Center, Min-
jiang University. (Grant No. MJUKF-IRA201806).

Fig. 1: Afﬁnity matrix visualization on ORL (top) and Yale
(bottom). From left to right: The afﬁnity matrix without joint
learning (corresponding to λ2 = 0, λ3 = 0) and the afﬁnity
matrix of our method.

date the subspace representation, instead simultaneously uses
discrete label and continuous label to update the subspace
representation. Compared with latest method (CSMSC),
our experimental results improve around 4.0%, 0.5%, 0.4%,
5.0% in terms of ACC, ARI, F, and Re. CSMSC does not
put subspace learning and spectral clustering method in an
optimization model.
Visualization and Convergence Analysis. To validate the
effectiveness of our method, we show the visualization of
afﬁnity matrices on Yale and ORL dataset. As observed in
Fig.1, we can clearly see that the afﬁnity matrix of our method
presents multiple block structures. Each block structure rep-
resents a subspace. From the two ﬁgures on the right, we

[21] Shirui Luo, Changqing Zhang, Wei Zhang, and Xiaochun Cao,
“Consistent and speciﬁc multi-view subspace clustering,” in
AAAI, 2018.

[22] Andrew Y Ng, Michael I Jordan, and Yair Weiss, “On spectral
clustering: Analysis and an algorithm,” in NIPS, 2002.
[23] Zhao Kang, Chong Peng, Qiang Cheng, and Zenglin Xu, “Uni-

ﬁed spectral clustering with optimal graph,” in AAAI, 2018.

[24] Maxwell D Collins, Ji Liu, Jia Xu, Lopamudra Mukherjee, and
Vikas Singh, “Spectral clustering with a convex regularizer on
millions of images,” in ECCV, 2014.

[25] Rongkai Xia, Yan Pan, Lei Du, and Jian Yin, “Robust multi-
view spectral clustering via low-rank and sparse decomposi-
tion.,” in AAAI, 2014.

[26] Yang Yang, Fumin Shen, Zi Huang, and Heng Tao Shen, “A
uniﬁed framework for discrete spectral clustering,” in IJCAI,
2016.

[27] Zaiwen Wen and Wotao Yin, “A feasible method for optimiza-
tion with orthogonality constraints,” Mathematical Program-
ming, vol. 142, no. 1-2, pp. 397–434, 2013.

[28] Timo Ojala, Matti Pietikainen, and Topi Maenpaa, “Multires-
olution gray-scale and rotation invariant texture classiﬁcation
IEEE Transactions on Pattern
with local binary patterns,”
Analysis and Machine Intelligence, vol. 24, no. 7, pp. 971–987,
2002.

[29] Martin Lades, Jan C Vorbruggen, Joachim Buhmann, J¨org
Lange, Christoph Von Der Malsburg, Rolf P Wurtz, and Wolf-
gang Konen, “Distortion invariant object recognition in the
dynamic link architecture,” IEEE Transactions on Computers,
, no. 3, pp. 300–311, 1993.

[30] Xiaochun Cao, Changqing Zhang, Chengju Zhou, Huazhu Fu,
and Hassan Foroosh, “Constrained multi-view video face clus-
tering,” IEEE Transactions on Image Processing, vol. 24, no.
11, pp. 4381–4393, 2015.

[31] Xiaojie Guo, “Robust subspace segmentation by simultane-
ously learning data representations and their afﬁnity matrix.,”
in IJCAI, 2015.

[32] Qian Zhao, Deyu Meng, Zongben Xu, Wangmeng Zuo, and Lei
Zhang, “Robust principal component analysis with complex
noise,” in ICML, 2014.

7. REFERENCES

[1] Changqing Zhang, Huazhu Fu, Qinghua Hu, Pengfei Zhu,
and Xiaochun Cao, “Flexible multi-view dimensionality co-
reduction,” IEEE Transactions on Image Processing, vol. 26,
no. 2, pp. 648–659, 2017.

[2] Ying Cui, Xiaoli Z Fern, and Jennifer G Dy, “Non-redundant
multi-view clustering via orthogonalization,” in ICDM, 2007.
[3] Hua Wang, Feiping Nie, and Heng Huang, “Multi-view clus-
tering and feature learning via structured sparsity,” in ICML,
2013.

[4] Kamalika Chaudhuri, Sham M Kakade, Karen Livescu, and
Karthik Sridharan, “Multi-view clustering via canonical corre-
lation analysis,” in ICML, 2009.

[5] Hao Tang, Hong Liu, Wei Xiao, and Nicu Sebe, “Fast and
robust dynamic hand gesture recognition via key frames ex-
traction and feature fusion,” Neurocomputing, vol. 331, pp.
424–433, 2019.

[6] Ehsan Elhamifar and Ren´e Vidal, “Sparse subspace cluster-

ing,” in CVPR, 2009.

[7] Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang
Huang, and Shuicheng Yan, “Robust and efﬁcient subspace
segmentation via least squares regression,” in ECCV, 2012.
[8] Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong
Yu, and Yi Ma, “Robust recovery of subspace structures by
low-rank representation,” IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, vol. 35, no. 1, pp. 171–184,
2013.

[9] Chun-Guang Li and Rene Vidal, “Structured sparse subspace
in CVPR,

clustering: A uniﬁed optimization framework,”
2015.

[10] Hao Tang and Hong Liu, “A novel feature matching strategy

for large scale image retrieval,” in IJCAI, 2016.

[11] Virginia R De Sa, “Spectral clustering with two views,” in
ICML workshop on learning with multiple views, 2005.
[12] Abhishek Kumar and Hal Daum´e, “A co-training approach for

multi-view spectral clustering,” in ICML, 2011.
[13] Abhishek Kumar, Piyush Rai, and Hal Daume,

“Co-

regularized multi-view spectral clustering,” in NIPS, 2011.
[14] Mehmet G¨onen and Ethem Alpaydın, “Multiple kernel learn-
ing algorithms,” Journal of Machine Learning Research, vol.
12, no. Jul, pp. 2211–2268, 2011.

[15] Grigorios Tzortzis and Aristidis Likas,

“Kernel-based

weighted multi-view clustering,” in ICDM, 2012.

[16] Xiaochun Cao, Changqing Zhang, Huazhu Fu, Si Liu, and Hua
Zhang, “Diversity-induced multi-view subspace clustering,” in
CVPR, 2015.

[17] Changqing Zhang, Huazhu Fu, Si Liu, Guangcan Liu, and Xi-
“Low-rank tensor constrained multiview sub-

aochun Cao,
space clustering,” in ICCV, 2015.

[18] Hongchang Gao, Feiping Nie, Xuelong Li, and Heng Huang,

“Multi-view subspace clustering,” in ICCV, 2015.

[19] Yuan Xie, Dacheng Tao, Wensheng Zhang, and Lei Zhang,
“Multi-view subspace clustering via relaxed l1-norm of tensor
multi-rank,” arXiv preprint arXiv:1610.07126, 2016.

[20] Xiaobo Wang, Xiaojie Guo, Zhen Lei, Changqing Zhang, and
“Exclusivity-consistency regularized multi-view

Stan Z Li,
subspace clustering,” in CVPR, 2017.

