Depth CNNs for RGB-D scene recognition: learning from scratch better than
transferring from RGB-CNNs

Xinhang Song1,2, Luis Herranz1, Shuqiang Jiang1,2
1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),
Institute of Computing Technology, CAS, Beijing, 100190, China
2University of Chinese Academy of Sciences, Beijing, China
{xinhang.song, luis.herranz, shuqiang.jiang}@vipl.ict.ac.cn

8
1
0
2
 
n
a
J
 
1
2
 
 
]

V
C
.
s
c
[
 
 
1
v
7
9
7
6
0
.
1
0
8
1
:
v
i
X
r
a

Abstract

Scene recognition with RGB images has been extensively
studied and has reached very remarkable recognition levels,
thanks to convolutional neural networks (CNN) and large
scene datasets. In contrast, current RGB-D scene data is much
more limited, so often leverages RGB large datasets, by trans-
ferring pretrained RGB CNN models and ﬁne-tuning with the
target RGB-D dataset. However, we show that this approach
has the limitation of hardly reaching bottom layers, which is
key to learn modality-speciﬁc features. In contrast, we focus
on the bottom layers, and propose an alternative strategy to
learn depth features combining local weakly supervised train-
ing from patches followed by global ﬁne tuning with images.
This strategy is capable of learning very discriminative depth-
speciﬁc features with limited depth images, without resorting
to Places-CNN. In addition we propose a modiﬁed CNN ar-
chitecture to further match the complexity of the model and
the amount of data available. For RGB-D scene recognition,
depth and RGB features are combined by projecting them
in a common space and further leaning a multilayer classi-
ﬁer, which is jointly optimized in an end-to-end network. Our
framework achieves state-of-the-art accuracy on NYU2 and
SUN RGB-D in both depth only and combined RGB-D data.

Introduction
Success in visual recognition mainly depends on the fea-
ture representing the input data. Scene recognition in par-
ticular has beneﬁted from recent developments in the
ﬁeld. Most notably, massive image datasets (ImageNet and
Places(Zhou et al. 2014)) provide the necessary amount
of data to train complex convolutional neural networks
(CNNs)(Krizhevsky, Sutskever, and Hinton 2012; Simonyan
and Zisserman 2015), with millions of parameters, without
falling into overﬁtting. The features extracted from models
pretrained in those datasets have shown to be generic and
powerful enough to obtain state-of-the-art performance in
smaller datasets (e.g. MIT indoor 67(Quattoni and Torralba
2009), SUN397(Xiao et al. 2010)), just using an SVM(Don-
ahue et al. 2014) or ﬁne-tuning, outperforming earlier hand-
crafted paradigms (e.g. SIFT, HOG, bag-of-words).

Low cost depth sensors can capture depth information in
addition to RGB data. Depth can provide valuable informa-
tion to model object boundaries and understand the global
layout of objects in the scene. Thus, RGB-D models can im-
prove recognition over mere RGB models. However, in con-

trast to RGB data, which can be crowdsourced by crawling
the web, RGB-D data needs to be captured with a specialized
and relatively complex setup(Silberman and Fergus 2011;
Song, Lichtenberg, and Xiao 2015). For this reason, RGB-
D datasets are orders of magnitude smaller than the largest
RGB datasets, also with much fewer categories. This pre-
vents from training deep CNNs properly, and handcrafted
features are still a better choice for this modality.

However, the recent SUN RGB-D dataset(Song, Lichten-
berg, and Xiao 2015) is signiﬁcantly larger than previous
RGB-D scene datasets (e.g. NYU2(Silberman et al. 2012)).
While still not large enough to train from scratch deep CNNs
of comparable size to RGB counterparts (10335 RGB-D im-
ages compared with 2.5 million RGB images in Places),
at least provides enough data for ﬁne tuning deep models
(e.g. AlexNet-CNN on Places) without signiﬁcant overﬁt-
ting. This approach typically exploit the HHA encoding for
depth data(Gupta et al. 2014a), since it also a three channel
representation (horizontal disparity, height above ground,
and angle with the direction of gravity, see Figure 1 top).
Fine tuning is typically used when the target dataset has lim-
ited data, but there is another large dataset that covers a simi-
lar domain which can be exploited ﬁrst to train a deep model.
Thus, transferring RGB features and ﬁne tuning with depth
(HHA) data is the common practice to learn deep represen-
tations for depth data(Song, Lichtenberg, and Xiao 2015;
Wang et al. 2016; Zhu, Weibel, and Lu 2016; Gupta, Hoff-
man, and Malik 2016). However, although HHA images re-
semble RGB images and shapes and objects can be iden-
tiﬁed, is it really reasonable reusing RGB features in this
inter-modal scenario?

In this paper we will focus on the low-level differences
between RGB and HHA data, and show that a large num-
ber of low-level ﬁlters are either useless or ignored during
ﬁne tuning the network from RGB to HHA. Figure 1 (mid-
dle) shows the average activation ratio (i.e. how often the
activation is non-zero) of the 96 ﬁlters in the layer conv1 of
Places-CNN for different input data (sorted in descending
order). When a network is properly designed and trained, it
tends to show a balanced activation rate curve (e.g. conv1 ac-
tivations extracted from the Places validation set, where the
curve is almost a constant) meaning that most of the ﬁlters
are contributing almost equally to build discriminative rep-
resentations. When transferred to other RGB scene datasets,

and use the quantized feature for segmentation and scene
classiﬁcation. More recently, multi-layered networks can
learn features directly from large amounts of data. Socher et
al.(Socher et al. 2012) use a single layer CNN trained unsu-
pervisedly on patches, and combined with a recurrent convo-
lutional network (RNN). Gupta et al.(Gupta et al. 2014a) use
R-CNN on depth images to detect objects in indoor scenes.
Since the training data is limited, they augment the training
set by rendering additional synthetic scenes.

Current state-of-the-art relies on transferring and ﬁne
tuning Places-CNN to RGB and depth data(Gupta, Hoff-
man, and Malik 2016; Wang et al. 2016; Zhu, Weibel,
and Lu 2016; Song, Lichtenberg, and Xiao 2015). Wang
et al. (Wang et al. 2016) extract deep features on both lo-
cal regions and whole images, then combine the features
of all RGB and depth patches and images in a component
aware fusion method. Some approaches(Zhu, Weibel, and
Lu 2016; Gupta, Hoffman, and Malik 2016) propose incor-
porating CNN architectures to ﬁne-tune jointly RGB and
depth image pairs. Zhu et al. (Zhu, Weibel, and Lu 2016)
jointly ﬁne-tune the RGB and depth CNN models by in-
cluding a multi-model fusion layer, simultaneously consid-
ering inter and intra-modality correlations, meanwhile regu-
larizing the learned features to be compact and discrimina-
tive. Alternatively, Gupta et al. (Gupta, Hoffman, and Malik
2016) propose to transfer RGB CNN model to the depth data
according to the RGB and depth image pairs.

In this paper we avoid relying on large yet biased RGB
models to obtain depth features, and train depth CNNs us-
ing weak supervision directly from depth data, learning truly
depth-speciﬁc and discriminative features, compared with
those transferred and adapted from biased RGB models.

Weakly-supervised CNNs
Recently, several works propose weakly supervised frame-
works(Durand, Thome, and Cord 2016; Bilen and Vedaldi
2016; Oquab et al. 2015), specially for object detection (ob-
ject labels are known but not the bounding boxes). Oquab et
al. (Oquab et al. 2015) propose an object detection frame-
work to ﬁne tune pretrained CNNs with multiple regions,
where a global max-pooling layer selects the regions to be
used in ﬁne tuning. Durand et al. (Durand, Thome, and Cord
2016) extend this idea by selecting both useful (positive)
and “useless” (negative) regions with a maximum and mini-
mum mixed pooling layer. The weakly supervised detection
network in (Bilen and Vedaldi 2016) uses a region proposal
method to select regions.

These works rely on CNNs already pretrained on large
datasets, and weak supervision is used in a subsequent ﬁne
tuning or adaptation stage to improve the ﬁnal features for a
particular task. In contrast, our motivation is training when
data is very scarce, with a weakly supervised CNN that does
not rely on any pretrained CNNs. In fact it is used to pretrain
convolutional layers prior to ﬁne tuning with full images.

Transferring from RGB to depth
Fine tuning Places-CNN with depth data Transferring
RGB CNNs and ﬁne tuning with RGB data (intra-modal

Figure 1: RGB and depth modalities. Top: examples of
scenes captured in RGB and depth (HHA encoding). Mid-
dle: average nonzero activations of ﬁlter in the conv1 layer
of Places-CNN for different datasets. Bottom: Conv1 ﬁlters
ordered by mean activation on SUN RGB-D HHA.

the curve is still very similar, showing that the majority of
the conv1 ﬁlters are still useful for 15 scenes (Lazebnik,
Schmid, and Ponce 2006) and MIT Indoor 67. However,
the curve for HHA shows a completely different behavior,
where only a subset of the ﬁlters are relevant, while a large
number are rarely activated, because they are not useful for
HHA (see Figure 1 bottom). Edges and smooth gradients
are common, while, Gabor-like patterns and high frequency
patterns are seldom found in HHA data.

Thus, we observe that ﬁlters at the very bottom layers are
crucial. However, conventional full ﬁne tuning from RGB
CNNs can hardly reach them (i.e. vanishing gradient prob-
lem), we explore other ways to make better use of the limited
data while focusing on bottom layers. In particular, we com-
pare the strategies of ﬁne tuning only top and only bottom
layers, and propose a weakly supervised strategy to learn
ﬁlters directly from the data. In addition, we combine the
pretrained RGB and depth networks into a new network,
ﬁne tuned with RGB-D image pairs. We show experimen-
tally that these features lead to state-of-the-art performance
with depth and RGB-D, and provide some insights and ev-
idences. Code is available at https://github.com/
songxinhang/D-CNN

Related work

RGB-D scene recognition

Earlier works use handcrafted features, engineered by an ex-
pert to capture some speciﬁc properties considered represen-
tative. Gupta et al. (Gupta et al. 2015) propose a method to
detect contours on depth images for segmentation, then fur-
ther quantize the segmentation outputs as local features for
scene classiﬁcation. Banica et al. (Banica and Sminchisescu
2015) quantize local features with second order pooling,

(a) FT-top

(b) FT-bottom (c) FT-keep

Figure 2: Strategies to ﬁne tune Places-CNN withdepth data,
(a) only top layers, bottom layers are frozen, (b) only bottom
layers, top layers are frozen; (c) bottom layers (top layers re-
trained and some convolutional layers removed). Each col-
umn represents a particular setting.

transfer) has been well studied. In general, low-level ﬁlters
from bottom layers capture generic patterns from the real vi-
sual world, and can be reused effectively in datasets with the
same modality and similar characteristics. Thus, ﬁne tuning
one or two top layers (e.g. fc6, fc7) is often enough. These
layers are more dataset-speciﬁc and need to be rewired to the
new target categories(Agrawal, Girshick, and Malik 2014).
In contrast, RGB and depth images have signiﬁcantly differ-
ent low-level visual patterns and regularities (e.g. depth pat-
terns in HHA encoding are typically smooth variations, con-
trasts and borders, but without textures and high frequency
patterns). Since the bottom convolutional layers are essen-
tial to capture this modality-speciﬁc visual appearances and
regularities, only ﬁne tuning top layers seem insufﬁcient to
adapt the RGB CNN properly to depth data.

Since we want to focus on bottom layers, we compare
conventional ﬁne tuning with other strategies that reach bot-
tom layers better (see Fig. 2, each column represents a par-
ticular setting). The strategies differ basically on which lay-
ers are ﬁne tuned, trained and which remain unaltered. Using
the AlexNet architecture and the pretrained Places-CNN, we
ﬁrst compare three strategies: a) FT-top (Places-CNN), the
conventional method where only a few top layers are ﬁne
tuned, b) FT-bottom (Places-CNN), where a few bottom lay-
ers are frozen, and c) FT-keep (Places-CNN), top layers are
directly removed. Note that fc8 is always trained, since it
must be resized according to the target number of categories.
The classiﬁcation accuracy on depth data (from SUN
RGB-D dataset) with different strategies is shown in Fig. 3.
Fine tuning top layers (FT-top) does not help signiﬁcantly
until including bottom convolutional layers, which is the op-
posite to ﬁne tuning for RGB(Agrawal, Girshick, and Ma-
lik 2014), where ﬁne tuning one or two top layers is almost
enough to reach the maximum gain, and further extending
to bottom layers helps very marginally. In contrast, ﬁne tun-
ing only the three bottom layers (FT-bottom) achieves 36.5%
which is higher than ﬁne tuning all layers. Furthermore, ﬁne
tuning after removing top layers (FT-keep) is also compa-
rable to ﬁne tuning all layers. All these results support our
intuition that bottom layers are much more important than
top layers when transferring RGB to depth, and that conven-

Figure 3: Comparisons of different learning strategies on
depth images in accuracy (%) obtained with softmax, the
ﬁne tuning strategies are illustrated in Fig. 2 and training
strategies are in Fig. 6.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Figure 4: Visualizing the ﬁrst convolutional layer (conv1):
(a) Places-CNN; (b) full ﬁne tuned Places-CNN; (c) FT-
bottom (Places-CNN); (d) FT-keep (Places-CNN), conv1;
(e) Train-Alex-CNN (scratch); (f) Train-Alex-CNN (WSP),
training with patches (99 × 99 pixels); (g) WSP-CNN, ker-
nel size 5 × 5 pixels, training with patches (35 × 35 pixels);
(f) Train-D-CNN (WSP). All methods are trained/ﬁne tuned
using only the depth data from SUN RGB-D.

tional transfer learning and adaptation tools used in RGB
modality may not be effective in this inter-modal case.

More insight from conv1 layer To provide complemen-
tary insight of why bottom layers are more important we
focus on the ﬁlters from the ﬁrst convolutional layer, i.e.
conv1, shown in Fig. 4. Although there is some gain in ac-
curacy, it can be observed that only a few particular ﬁlters
have noticeable changes during the ﬁne tuning process. This
suggest that the CNN is reusing RGB ﬁlters, and thus trying
to ﬁnd RGB-like patterns in depth data. Additionally, Fig. 1
middle shows that a large number of ﬁlters from Places-
CNN are signiﬁcantly underused on depth data (while they
are properly used on RGB data). These observations suggest
that reusing Places-CNN ﬁlters for conv1 and other bottom
layers may not be a good idea. Moreover, since ﬁlters also
represent tunable parameters, this results in a model with too
many parameters that is difﬁcult to train with limited data.

Figure 5: Two-step learning of depth CNNs combining weakly supervised pretraining and ﬁne tuning.

(a) Scratch

(b) WSP

(c) FT (WSP)

Weak supervision on patches

extract multiple patches from a single image, increasing the
amount of training data. Second, we include weakly super-
vised training. Patches typically cover objects or parts, not
the whole scene. However, since we do not have those lo-
cal labels we use the scene category as weak label, since we
know that all patches in a given image belong to the same
scene. We refer to this network as weakly supervised patch-
CNN (WSP-CNN).

The weakly-supervised strategy can be used for Alex-CNN
training. We ﬁrst sample a grid of 4 × 4 patches with 99 × 99
pixels for weakly-supervised pretraining, and ﬁne tune it
with full images. When switching the architecture from
WSP-CNN to full Alex-CNN, the amount of connections
in fc6 changes. Thus, only the weights of the convolutional
layers of the pretrained WSP-CNN are copied for ﬁne tun-
ing, similarly as other weakly supervised methods(Durand,
Thome, and Cord 2016; Bilen and Vedaldi 2016). Fig. 3
shows that using weak supervision on patches (WSP) signif-
icantly outperforms training with full image (compare Train-
Alex-CNN (WSP) vs Train-Alex-CNN (scratch)). Further-
more, in the conv1 ﬁlters shown in Fig. 4f (WSP) the depth
speciﬁc-patterns are much more evident than in Fig. 4e (full
image). Nevertheless, they still show a signiﬁcant amount of
noise, which suggests that AlexNet is still too complex, and
perhaps the kernels may be too big for depth data.

Since the complexity of depth images is signiﬁcantly
lower than that of RGB images (e.g. no textures), we re-
duced the size of the kernels in each layer to train our WSP-
CNN, which consists of C (C=3 works best after evaluation)
convolutional layers (see Fig 5 top row for the architecture

Figure 6: Training strategies for Alex-CNN variants with
depth images, (a) from scratch, (b) weakly-supervised with
patches, and (c) ﬁne-tuned after weakly supervised trainig
with patches.

Weakly supervised pretrained CNN
It is difﬁcult to learn deep CNN from scratch with depth
images, due to the lack of enough training data. We alterna-
tively train CNN from scratch with different architectures
(see Fig. 6a), from shallow to deep. The visualization of
conv1 (architecture with two convolutional layers) is illus-
trated in the top row of Fig. 4e. However, we cannot observe
any visual regularities from the visualizations, even though
this is a more shallow network.

So in order to adapt the amount of training data and the
complexity (i.e. number of parameters) of the model, we
modify slightly the training procedure. First, we reduce the
support of the CNN by working on patches rather than on the
whole image. Thus networks with simpler architectures with
fewer trainable parameters are capable. Additionally, we can

detail). The sizes of the kernelsare 5 × 5 (stride 2), 3 × 3
and 3 × 3, and the size of max pooling is 2 × 2, stride 2.
We sample a grid of 7 × 7 patches with 35 × 35 pixels for
weakly-supervised training.

Depth-CNN

Fig 5 bottom shows the full architecture of our depth-CNN
(D-CNN). After training the WSP-CNN, we transfer the
weights of the convolutional layers. The output of conv4 in
D-CNN is 29 × 29 × 512, almost 50 times larger than the
output of pool5 (size of 6 × 6 × 256) in Alex-CNN, which
leads to 50 times more parameters in this part. In order to re-
duce the parameters, we include three spatial pyramid pool-
ing (SPP) (He et al. 2014) layers with size of 29×29,15×15,
10 × 10. SPP also captures spatial information and allows us
to train the model end-to-end. The proposed Train-D-CNN
(WSP) outperforms both ﬁne tuning and weakly-supervised
training of Alex-CNN (see in Fig. 3). Comparing the visu-
alizations in Fig. 4, the proposed WSP-CNN and D-CNN
learn more representative kernels, supporting the better per-
formance. This also suggests that a smaller kernel size is
suitable for depth data.

RGB-D fusion

Most previous works use two independent networks for
RGB and depth, optimizing in different independent stages
the fusion parameters, ﬁne tuning and classiﬁcation(Zhu,
Weibel, and Lu 2016; Wang et al. 2016; 2015). In contrast,
we integrate both RGB-CNN, depth-CNN and the fusion
procedure into an integrated RGB-D-CNN, which can be
trained end-to-end, jointly learning the fusion parameters
and ﬁne tuning both RGB layers and depth layers of each
branch. As fusion mechanism, we use a network with two
fully connected layers followed by the loss (i.e. fusion net-
work), on top of the concatenated feature [Frgb, Fd], where
Frgb ∈ RDr×1 and Fdepth ∈ RDd×1 are the RGB and depth
features, respectively. The ﬁrst layer of the fusion network
learns modality-speciﬁc projections ¯W = [Wrgb, Wdepth]
to a common feature space as

Frgbd = [Wrgb Wdepth]

(cid:21)

(cid:20) Frgb
Fdepth

= ¯W

(cid:21)

(cid:20) Frgb
Fdepth

(1)

where Wrgb ∈ RDrgbd×Dr and Wdepth ∈ RDrgbd×Dd are
the modality-speciﬁc projection matrices.

Recent works exploit metric learning(Wang et al. 2015),
Fisher vector(Wang et al. 2016) and correlation analy-
sis(Zhu, Weibel, and Lu 2016) to reduce the redundancy in
the joint RGB-D representation. It is important to note that
this step is particularly effective when RGB and depth fea-
tures are signiﬁcantly correlated. This is likely to be the case
in recent works when both RGB and depth feature extrac-
tors are ﬁne tuned versions of the same CNN model (e.g.
Places-CNN). In our case depth models are learned directly
from the data and independently from RGB, so they are
already much less correlated, even without explicit multi-
modal analysis.

Table 1: Ablation study for different models (accuracy %).
Alex-CNN

Arch.
Weights
Layer
pool1
pool2
conv3
conv4
pool5
fc6
fc7
fc8

Places-CNN
FT
20.3
27.5
29.3
32.1
35.9
36.5
37.2
37.8

-
17.2
25.3
27.6
29.5
30.5
30.8
30.9
-

Scratch

D-CNN
Scratch
Train WSP WSP
25.3
23.5
22.3
33.9
30.4
26.8
35.1
34.6
29.8
38.3
-
-
-
-
-
-
36.1
30.7
40.5
36.8
32.0
41.2
37.5
32.8

Table 2: Comparison of depth on SUN RGB-D

Proposed

State-of-
the-art

Method
D-CNN
D-CNN (wSVM)
R-CNN+FV(Wang et al. 2016)
FT-Places-CNN(Wang et al. 2016)
FT-Places-CNN+SPP
FT-Places-CNN+SPP (wSVM)

Acc.(%)
41.2
42.4
34.6
37.5
37.7
38.9

Experiments
Dataset We evaluate our approach in two datasets: NYU
depth dataset second version (NYUD2) (Silberman et al.
2012) and SUN RGB-D(Song, Lichtenberg, and Xiao 2015).
The former is a relatively small dataset with 27 indoor cat-
egories, but only a few of them are well represented. Fol-
lowing the split in (Silberman et al. 2012), all 27 categories
are reorganized into 10 categories, including 9 most com-
mon categories and an ’other’ category consisting of the
remaining categories. The training/test split is 795/654 im-
ages. SUN RGB-D contains 40 categories with 10335 RGB-
D images. Following the publicly available split in (Song,
Lichtenberg, and Xiao 2015; Wang et al. 2016), the 19 most
common categories are selected, consisting of 4,845 images
for training and 4,659 images for test.

Classiﬁer and features Since we found that training lin-
ear SVM classiﬁers with the output of the fully connected
layer increases slightly performance, all the following re-
sults use SVMs, if not speciﬁed.

• (wSVM): this variant uses category-speciﬁc weights dur-
ing SVM training to compensate the imbalance in the
training data. The weight w = {w1...wK} of each cat-
, where Nk
egory k is computed as wk =
is the number of training images of the kth category. We
select p = 2 by cross validation.

(cid:16) mini∈K Ni
Nk

(cid:17)p

Evaluation metric Following (Song, Lichtenberg, and
Xiao 2015; Wang et al. 2016), we report the average pre-
cision over all scene classes for both datasets.

Evaluations on SUN RGB-D
Ablation study. We compare D-CNN and Alex-CNN on
SUN RGB-D depth data. The outputs of different layers are

Table 3: Comparisons of RGB-D data on SUN RGB-D

Method

Concate.
Concate.
Concate. (wSVM)
RGB-D-CNN
RGB-D-CNN
RGB-D-CNN (wSVM)
(Zhu, Weibel, and Lu 2016)
(Wang et al. 2016)

RGB

CNN models

Depth
Places-CNN

Accuracy (%)
RGB Depth RGB-D
Places-CNN
35.4
FT-Places-CNN FT-Places-CNN 41.5
FT-Places-CNN FT-Places-CNN 42.7
FT-Places-CNN FT-Places-CNN 41.5
41.5
FT-Places-CNN
FT-Places-CNN
42.7
FT-Places-CNN FT-Places-CNN 40.4
40.4
FT-Places-CNN
+ R-CNN

30.9
37.5
38.7
37.5
41.2
42.4
36.5
36.5

39.1
45.4
46.9
48.5
50.9
52.4
41.5
48.1

FT-Places-CNN
+ R-CNN

D-CNN
D-CNN

Baseline

Proposed

State-of-the-art

Table 4: Comparisons on NYUD2 in accuracy(%)

Method

RGB
Depth
Concate.

Features

Depth

Acc.

RGB
Baseline methods
FT-Places-CNN

53.4
FT-Places-CNN 51.8
FT-Places-CNN FT-Places-CNN 59.5
Proposed methods

Depth
RGB-D-CNN

FT-Places-CNN
State-of-the-art

D-CNN
D-CNN

(Gupta et al. 2014b)
(Wang et al. 2016) FT-Places-CNN

Segmentation responses

+ R-CNN

FT-Places-CNN
+ R-CNN

56.4
65.8

45.4
63.9

used as features to train the SVM classiﬁers. We select 5 dif-
ferent models for comparison in Table 1. We use the Alex-
CNN architecture with only 3 bottom convolutional layers
for training from scratch. With Alex-CNN architecture, the
bottom layers (pool1 to conv3) trained from scratch perform
better than the transferred from Places-CNN, even though
the top layers are worse. Using weakly-supervised training
on patches (WSP), the performance is comparable to ﬁne
tuned Places-CNN for top layers and better for bottom lay-
ers, with a smaller model and without relying on Places data.
D-CNN consistently achieves the best performance.

Comparisons with depth data We compare to related
methods on depth recognition in Table 2. For the fair com-
parison, we also implement SPP on Places-CNN for the ﬁne
tuning. Our D-CNN outperforms FT-Places-CNN+SPP with
3.5% in accuracy. When both models using weighted SVM
for training, our D-CNN works even better.

RGB-D fusion We compare to the state-of-the-art works
(Zhu, Weibel, and Lu 2016; Wang et al. 2016) of RGB-D in-
door recognition in Table 3, where a discriminative RGB-D
fusion CNN is proposed in (Zhu, Weibel, and Lu 2016) and a
joint feature fusion of RGB-D, scene and objects is proposed
in (Wang et al. 2016). The proposed RGB-D-CNN outper-
forms the RGB-D fusion method in (Wang et al. 2016) with

2.7% with linear SVM, 4.3% with weighted SVM, without
including external training of R-CNN (Gupta et al. 2014a)
as in that approach.

Comparisons on NYUD2
We compare our RGB-D-CNN to the state-of-the-art on
NYUD2 in Table 4. Gupta et al. (Gupta et al. 2014b)
propose to encode segmentation responses as features for
scene recognition. Our RGB-D-CNN largely outperforms
this work, where the segmentation is based on the hand-
crafted features. Comparing to RGB-D fusion in (Wang et
al. 2016), we achieves a gain of 1.9% in accuracy, without
including the R-CNN models as in that work.

Conclusion
Transferring deep representations within the same modal-
ity (e.g. Places-CNN ﬁne tuned on SUN397) works well,
since low-level patterns have similar distributions, and bot-
tom layers can be reused while adjusting the more dataset-
speciﬁc top layers. However, ﬁne tuning is not that effec-
tive in inter-modal transfer, such as Places-CNN to depth in
the HHA space, where low-level features require modality-
speciﬁc ﬁlters. In this paper, we focus on the bottom lay-
ers, because they are more critical to represent depth data
properly. By reducing the number of parameters of the net-
work, and using weakly supervised learning over patches,
the complexity of the model matches better the amount of
data available. This depth representation is not only more
discriminative than those ﬁne tuned from Places-CNN but
also when combined with RGB features the gain is higher,
showing that both are complementary. Notice also, that we
do not depend (for depth) on large datasets such as Places.

Acknowledgment
This work was supported in part by the National Natu-
ral Science Foundation of China under 61322212, Grant
61532018, and Grant 61550110505, in part by the National
High Technology Research and Development 863 Program
of China under Grant 2014AA015202, in part by the Beijing
Municipal Commission of Science and Technology under
Grant D161100001816001, in part by the Lenovo Outstand-
ing Young Scientists Program, in part by National Program
for Special Support of Eminent Professionals and National
Program for Support of Top-notch Young Professionals.

[Simonyan and Zisserman 2015] Simonyan, K., and Zisserman, A.
2015. Very deep convolutional networks for large-scale image
In International Conference on Learning Represen-
recognition.
tations.

[Socher et al. 2012] Socher, R.; Huval, B.; Bath, B.; Manning,
C. D.; and Ng, A. Y. 2012. Convolutional-recursive deep learn-
ing for 3d object classiﬁcation. In NIPS.

[Song, Lichtenberg, and Xiao 2015] Song, S.; Lichtenberg, S. P.;
and Xiao, J. 2015. Sun rgb-d: A rgb-d scene understanding bench-
mark suite. In Computer Vision and Pattern Recognition (CVPR),
2015 IEEE Conference on, 567–576.

[Wang et al. 2015] Wang, A.; Lu, J.; Cai, J.; Cham, T.-J.; and Wang,
G. 2015. Large-margin multi-modal deep learning for rgb-d object
recognition. IEEE Trans. on Multimedia 17.

[Wang et al. 2016] Wang, A.; Cai, J.; Lu, J.; and Cham, T.-J. 2016.
Modality and component aware feature fusion for rgb-d scene clas-
siﬁcation. In The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR).

[Xiao et al. 2010] Xiao, J.; Hayes, J.; Ehringer, K.; Olivia, A.; and
Torralba, A. 2010. SUN database: Largescale scene recognition
from abbey to zoo. In CVPR.

[Zhou et al. 2014] Zhou, B.; Lapedriza, A.; Xiao, J.; Torralba, A.;
and Oliva, A. 2014. Learning deep features for scene recognition
using places database. In Ghahramani, Z.; Welling, M.; Cortes, C.;
Lawrence, N.; and Weinberger, K., eds., NIPS, 487–495.

[Zhu, Weibel, and Lu 2016] Zhu, H.; Weibel, J.-B.; and Lu, S.
2016. Discriminative multi-modal feature fusion for rgbd indoor
scene recognition. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).

References

[Agrawal, Girshick, and Malik 2014] Agrawal, P.; Girshick, R.; and
Malik, J. 2014. Analyzing the performance of multilayer neural
networks for object recognition. In ECCV.

[Banica and Sminchisescu 2015] Banica, D., and Sminchisescu, C.
2015. Second-order constrained parametric proposals and sequen-
tial search-based structured prediction for semantic segmentation
in rgb-d images. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).

[Bilen and Vedaldi 2016] Bilen, H., and Vedaldi, A. 2016. Weakly
supervised deep detection networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

[Donahue et al. 2014] Donahue, J.; Jia, Y.; Vinyals, O.; Hoffman,
J.; Zhang, N.; Tzeng, E.; and Darrell, T. 2014. DeCAF: A deep
convolutional activation feature for generic visual recognition. In
ICML.

[Durand, Thome, and Cord 2016] Durand, T.; Thome, N.; and
Cord, M. 2016. Weldon: Weakly supervised learning of deep con-
volutional neural networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).

[Gupta et al. 2014a] Gupta, S.; Girshick, R.; Arbelaez, P.; and Ma-
lik, J. 2014a. Learning rich features from rgb-d images for object
detection and segmentation. In ECCV.

[Gupta et al. 2014b] Gupta, S.; Arbelaez, P.; Girshick, R.; and Ma-
Indoor scene understanding with rgb-d images:
lik, J. 2014b.
Bottom-up segmentation, object detection and semantic segmen-
tation. Int J Comput Vis 112:133–149.

[Gupta et al. 2015] Gupta, S.; Arbel´aez, P.; Girshick, R.; and Malik,
J. 2015. Indoor scene understanding with rgb-d images: Bottom-up
segmentation, object detection and semantic segmentation. Inter-
national Journal of Computer Vision 112(2):133–149.

[Gupta, Hoffman, and Malik 2016] Gupta, S.; Hoffman, J.; and
Malik, J. 2016. Cross modal distillation for supervision transfer. In
The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).

[He et al. 2014] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2014.
Spatial pyramid pooling in deep convolutional networks for visual
recognition. In ECCV.

[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,

A.;
Sutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁcation with
deep convolutional neural networks. In NIPS, 1106–1114.

[Lazebnik, Schmid, and Ponce 2006] Lazebnik, S.; Schmid, C.; and
Ponce, J. 2006. Beyond bags of features: Spatial pyramid matching
for recognizing natural scene categories. In CVPR.

[Oquab et al. 2015] Oquab, M.; Bottou, L.; Laptev, I.; and Sivic, J.
2015. Is object localization for free? - weakly-supervised learning
with convolutional neural networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

[Quattoni and Torralba 2009] Quattoni, A., and Torralba, A. 2009.

Recognizing indoor scenes. In CVPR.

[Silberman and Fergus 2011] Silberman, N., and Fergus, R. 2011.
Indoor scene segmentation using a structured light sensor. In Com-
puter Vision Workshops (ICCV Workshops), 2011 IEEE Interna-
tional Conference on, 601–608.

[Silberman et al. 2012] Silberman, N.; Hoiem, D.; Kohli, P.; and
Fergus, R. 2012. Indoor segmentation and support inference from
rgbd images. In Proceedings of the 12th European Conference on
Computer Vision - Volume Part V, ECCV’12, 746–760. Berlin,
Heidelberg: Springer-Verlag.

