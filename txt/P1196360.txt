0
2
0
2
 
b
e
F
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
2
8
1
1
1
.
2
0
0
2
:
v
i
X
r
a

Information Directed Sampling for Linear Partial Monitoring

Johannes Kirschner
ETH Zurich, Department of Computer Science

Tor Lattimore
DeepMind

Andreas Krause
ETH Zurich, Department of Computer Science

JKIRSCHNER@INF.ETHZ.CH

LATTIMORE@GOOGLE.COM

KRAUSEA@ETHZ.CH

Abstract
Partial monitoring is a rich framework for sequential decision making under uncertainty that gen-
eralizes many well known bandit models, including linear, combinatorial and dueling bandits. We
introduce information directed sampling (IDS) for stochastic partial monitoring with a linear reward
and observation structure. IDS achieves adaptive worst-case regret rates that depend on precise ob-
servability conditions of the game. Moreover, we prove lower bounds that classify the minimax
regret of all ﬁnite games into four possible regimes. IDS achieves the optimal rate in all cases up to
logarithmic factors, without tuning any hyper-parameters. We further extend our results to the con-
textual and the kernelized setting, which signiﬁcantly increases the range of possible applications.
Keywords: Information Directed Sampling, Linear Partial Monitoring, Bandits

1. Introduction

Partial monitoring is an expressive framework for sequential decision making in which the learner
does not directly observe the reward (Rustichini, 1999). Instead, the learner obtains observations
from pre-speciﬁed observation distributions that are associated to the actions and may or may not
provide direct information about the reward. In this work, we consider a stochastic version of the
problem with a linear reward and observation model, which is sometimes referred to as combina-
torial partial monitoring (Lin et al., 2014; Chaudhuri and Tewari, 2016). Among other settings as
described in Section 4, the linear partial monitoring model strictly generalizes linear bandits (Abe
and Long, 1999; Auer, 2003), combinatorial bandits (Cesa-Bianchi and Lugosi, 2012) both with
bandit and semi-bandit feedback, and some variants of dueling bandits (Yue and Joachims, 2009).
Rd be an unknown
Rd×m be a known linear observation operator. The
In each round t, the learner chooses an action
t=1 is a sequence of
xt, θ

Linear Partial Monitoring Let
parameter. For each action x
learner and environment interact over n rounds.
xt
independent ρ-subgaussian noise vectors such that (cid:15)t
and is not observed. As usual, the aim is to minimize cumulative regret

and receives an m-dimensional observation at = A(cid:62)

Rd be a compact set of actions and θ

Rm. The reward for the learner is

xtθ + (cid:15)t where ((cid:15)t)n

X ⊂
, let Ax

∈ X

∈ X

∈

∈

∈

(cid:105)

(cid:104)

Rn =

x∗
(cid:104)

−

xt, θ

,

(cid:105)

n
(cid:88)

t=1

c(cid:13) J. Kirschner, T. Lattimore & A. Krause.

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

x, θ

where x∗ = arg maxx∈X (cid:104)
is the optimal action, chosen arbitrarily whenever the choice is not
unique. A slightly more general formulation of the setup is in Appendix A.1, which we will use for
some applications. Bandit games are a special case where Ax = x. We discuss further applications
in detail in Section 4. Readers seeking further motivation and intuition for the setup will beneﬁt
from skipping ahead to this section.

(cid:105)

A linear partial monitoring game is called ﬁnite if it has ﬁnitely many actions. An action x

is called Pareto optimal if it is an extreme point of the convex hull of
optimal for θ is

X

∈ X
. The set of actions that are

which is deﬁned on sets

(θ). A game is called globally observable if

−
A game is called locally observable if for every convex set

∈ X

∈

:

x, θ
(cid:104)

(cid:105)

= max
y∈X (cid:104)

y, θ

,

(cid:105)}

∈ X

(θ) =

P
Rd by

x
{

(
C

C ⊆
x

y

) =

P
span(Az : z

∪

θ∈C

P

) for all x, y

.

∈ X

Rd,

C ⊂

x

y

−

∈

span(Az : z

(

)) for all x, y

∈ P

C

) .

(
C

∈ P

(1)

(2)

Intuitively, in globally observable games, the learner has access to actions from which reward dif-
ferences between different actions can be estimated. In locally observable games, the reward differ-
ences can be estimated in a local sense that greatly eases learning. Any locally observable game is
also globally observable. Although it is not important for this work, connoisseurs of partial moni-
toring will be pleased to know these deﬁnitions coincide with the usual deﬁnitions, as discussed in
Appendix F. At least for ﬁnite games, we will see that global observability leads to a ˜O(n2/3) regret,
while local observability leads to ˜O(n1/2) regret. The relation to ﬁnite partial monitoring is subtle,
however, and the classiﬁcation results do not imply each other as we explain in Appendix A.2.

Information Directed Sampling We propose a new algorithm for stochastic linear partial moni-
toring based on the information directed sampling (IDS) principle. This strategy uses the observa-
tions at = A(cid:62)
of the true gaps and an
associated information gain It(x), detailed below. The information gain quantiﬁes the uncertainty
x θ + (cid:15). IDS is
reduction in the parameter estimate when the learner chooses x
the policy that samples action xt from a distribution µt that minimizes the information ratio,

xtθ + (cid:15)t to construct conservative estimates ∆t(x)

−
and observes A(cid:62)

∈ X

x, θ

≥ (cid:104)

x∗

(cid:105)

µt = arg min

µ

Eµ[∆t(x)]2
Eµ[It(x)]

.

Our Contributions Our main contribution is a new algorithm for linear stochastic partial moni-
toring. We show that its regret dependence on the horizon is near-optimal in all ﬁnite-action games
without the need to tune any hyper-parameters. Along the way, we prove a classiﬁcation theo-
rem showing that, up to logarithmic factors, the minimax regret of all ﬁnite-action games is ei-
ther 0, ˜Θ(n1/2), ˜Θ(n2/3) or Ω(n). This result mirrors that for the standard setting (Lattimore and
Szepesv´ari, 2019), but neither result implies the other. Our upper bounds are general and apply be-
yond the ﬁnite case. For inﬁnite actions, however, the classiﬁcation theorem is no longer so straight-
forward: we show that the minimax regret depends on ﬁner geometric properties of the action set and
observation structure, such as curvature. We further consider a novel contextual partial monitoring
setting, where IDS exhibits an elegant planning behavior to exploit the distribution over contexts.
Lastly, our algorithm and analysis are easily kernelized, which enables utilizing practically impor-
tant smoothness priors, with applications such as Bayesian optimization with gradient observations.

2

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Related work Finite partial monitoring dates back to Rustichini (1999). The generality of partial
monitoring yields a rich structure of games (Bart´ok et al., 2014; Lattimore and Szepesv´ari, 2019)
where the minimax regret rate depends on precise observability conditions. The complete classiﬁca-
tion of ﬁnite games is achieved in a line of work by Cesa-Bianchi et al. (2006); Bart´ok et al. (2014);
Antos et al. (2013); Lattimore and Szepesv´ari (2019), with a focus on the stochastic version of the
problem in the work by Bart´ok et al. (2011); Bart´ok et al. (2012). Asymptotics for ﬁnite games
are known as well (Komiyama et al., 2015). Partial monitoring with prior information was studied
by Vanchinathan et al. (2014), and with side-information by Bart´ok and Szepesv´ari (2012). Latter
setup is different from our contextual setting. The linear version of the problem that we study here
is due to Lin et al. (2014); Chaudhuri and Tewari (2016). Both previous approaches rely on forced
exploration schemes and achieve a ˜
(n2/3) worst-case regret on globally observable games, but not
O
the faster ˜
(n1/2) rate on locally observable games. Information directed sampling was proposed
O
by Russo and Van Roy (2014) in the Bayesian setting to address short-comings of the UCB algo-
rithm (Auer, 2003) and Thompson sampling (Agrawal and Goyal, 2013) on examples that capture
the spirit of partial monitoring. The frequentist version of the algorithm that we analyze here was
proposed by Kirschner and Krause (2018) in a bandit setting with heteroscedastic noise, which we
strictly generalize. Recently, information theoretic tools were also introduced in the partial monitor-
ing literature, to obtain minimax rates (Lattimore and Szepesv´ari, 2019) and to deﬁne the sampling
distribution of an algorithm for ﬁnite (adversarial) partial monitoring (Lattimore and Szepesvari,
2019). Bandits (Lattimore and Szepesv´ari, 2018) are perhaps the most prominent special case of par-
tial monitoring. We discuss more relevant work in the context of speciﬁc applications in Section 4.

y

(cid:107)

x

X

X

X

−

), ∂

X ⊂

(cid:107)·(cid:107)
Rd we let conv(

) = supx,y∈X (cid:107)
x

and a collection of matrices (Az : z

for the standard euclidean norm. For positive semi-deﬁnite A let

2
A =
Notation We write
(cid:107)
(cid:107)
x(cid:62)Ax. Given
and diam(
denote its convex
hull, boundary and diameter respectively. The smallest and largest eigenvalues of a matrix A are
denoted by λmin(A) and λmax(A) respectively. The identity matrix of dimension d is denoted
by 1d. For two square matrices A, B, A
B means that B
A is positive semi-deﬁnite. For
λmax(C(cid:62)C) is the operator norm. Given an index
a possibly non-square matrix C,
), all with the same number of rows, we deﬁne
set
).
span(Az : z
∈ Z
with respect
When
P(
).
to the Borel σ-algebra. The Dirac probability measure at x
∈ X
X
The optimal action given parameter θ is x∗(θ) = arg maxx∈X (cid:104)
. Probability measures on
(cid:105)
subsets of Rd are always deﬁned over the Borel σ-algebra. Given a probability measure π on
let Vπ = (cid:82)
quantities at the end of round t. ˜
O

) to be the span of the collection of all columns of the matrices (Az : z

X
t = σ(x1, a1, . . . , xt, at) contains the observed

is the Landau notation with logarithmic factors suppressed.

) be the space of probability measures on

is Borel measurable we let P(

X
is denoted by δx

Rd×d. The ﬁltration

(cid:22)
2 = (cid:112)
(cid:107)

X AxA(cid:62)

x dπ(x)

∈ Z

∈ Z

x, θ

−

Z

F

C

X

X

∈

∈

(cid:107)

Assumptions Throughout, we make technical boundedness assumptions
and diam (
x
(cid:104)
X
R, E[eλ(cid:15)t
tionally ρ-subgaussian,
map x

1, which implies
−
λ
|F
Ax is assumed to be continuous.

1
≤
(cid:107)
. The noise vector (cid:15)t is condi-
exp(ρ2λ2/2) understood coordinate wise. The

y, θ
(cid:105) ≤
t−1, xt]

1 for all x, y

Ax
(cid:107)

∈ X

2
(cid:107)

1,

≤

≤

≤

∈

∀

(cid:107)

θ

)

2

(cid:55)→

2. Information Directed Sampling for Linear Partial Monitoring

Information directed sampling (IDS) was introduced by Russo and Van Roy (2014) for the Bayesian
bandit setting. IDS samples actions from a distribution that minimizes the ratio of squared expected

3

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

regret and mutual information. The information ratio appears in a sum under the square root in the
regret bound and IDS is the policy that (greedily) minimizes this term. Kirschner and Krause (2018)
introduced a frequentist analog of the algorithm that replaces the Bayesian expected suboptimality
and information gain with frequentist counterparts and exhibits high-probability regret bounds on
linear bandits. In the following we generalize the latter approach, which we simply refer to as IDS.
R≥0 an information gain that we

R≥0 be a gap estimate and It :

Formally, let ∆t :

will deﬁne shortly. We linearly extend the functions ∆t and It to probability distributions over
X
X It(x)dµ(x).
so that for distributions µ
Information directed sampling is the strategy that samples the action xt at step t from a distribution
µt

) that minimizes the information ratio Ψt(µ):

X ∆t(x)dµ(x) and It(µ) = (cid:82)

) we have ∆t(µ) = (cid:82)

P(

P(

X

∈

X →

X →

∈

X

µt = arg min
µ∈P(X )

Ψt(µ) , where Ψt(µ) =

∆t(µ)2
It(µ)

.

The minimizing distribution is well deﬁned and can always be chosen with a support of two actions.
Ψt(µ) is convex. These results were previously shown for the bandit setting by
Furthermore, µ
Kirschner and Krause (2018, Lemma 4,16,17) and continue to hold in the more general setting. We
brieﬂy discuss computational concerns in Section 5.

(cid:55)→

Let At = Axt be the observation operator for the action chosen in round t. To estimate the gap

∆t(x), IDS uses the regularized least squares estimator, which after t rounds is

ˆθt = arg min

θ∈Rd

t
(cid:88)
(cid:107)

s=1

A(cid:62)
s θ

2 +

as

(cid:107)

−

θ

(cid:107)

(cid:107)

2 = V −1

t

t
(cid:88)

s=1

Asas ,

where Vt = (cid:80)t

s=1 AsA(cid:62)

s + 1d. Deﬁne a sequence of conﬁdence sets (
C

t)n

t=0 by

(cid:110)

θ(cid:48)

t =

C

Rd :

ˆθt
(cid:107)

−

θ(cid:48)

Vt ≤

(cid:107)

∈

(cid:111)

1/2
t

β

, where β

1/2
t =

log det Vt + 2 log (cid:0) 1

(cid:1) + 1 .

δ

(3)

(cid:113)

The concentration bound by Abbasi-Yadkori et al. (2011, Theorem 2) shows that with probability
at least 1

t for all t. Our estimate of the suboptimality gap is deﬁned as

δ it holds that θ

−

∈ C

(cid:110)

∆t(x) = min

1, max
y
y∈X (cid:104)

−

x, ˆθt−1

+ β

(cid:105)

1/2
x
t−1 (cid:107)

y

(cid:107)V −1

t−1

−

(cid:111)

,

which is chosen so that with high probability
Note that ∆t(x)

0 for all x

−
. For the information gain we use

(cid:105) ≤

x, θ

∆t(x) for all x

∈ X

and all rounds t.

≥

∈ X

It(x) = log det

1m + A(cid:62)

x V −1

t−1Ax

(cid:17)

.

(4)

The deﬁnition corresponds to the usual Shannon mutual information when using a Gaussian prior
on the parameter and a Gaussian likelihood function. For the bandit setting, it was previously
demonstrated by Kirschner and Krause (2018) that the choice of It can have a large impact on
empirical performance. In Appendix B we discuss some alternative choices for both ∆t and It.

x∗
(cid:104)

(cid:16)

4

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

2.1. A General Regret Bound

t=1 Ψt(µt) and the total information gain, γn = (cid:80)n

The regret of any strategy can be bounded in terms of the cumulative sum of the information ratio
(cid:80)n
t=1 It(xt). The following result is a generic
regret bound that generalizes Theorem 1 of Kirschner and Krause (2018). Note that for deterministic
policies the result can be simpliﬁed (Kirschner and Krause, 2018, cf. Theorem 2).

Lemma 1 (IDS regret bound) For η
universal constant C > 1 such that for any n
(possibly) randomized policy (µt)n

≥

t=1 is bounded by

≥

0, let Gη =

t
{

∈

[n] : ∆t(µt)

1 with probability at least 1

η
. There exists a
}
δ the regret of any

≤
−



Rn

≤

C inf
η≥0

nη +

(cid:115) (cid:88)

t∈[n]\Gη



Ψt(µt) (cid:0)γn + log 1

(cid:1)

 + 4 log

δ

(cid:19)

(cid:18) 4n + 4
δ

.

For the proof, note that

∆t(xt) and consider the sum over the expected gap estimates,

x∗
(cid:104)

−

xt, θ

(cid:105) ≤

n
(cid:88)

t=1

∆t(µt) =

∆t(µt) +

(cid:88)

t∈Gη

(cid:88)

t∈[n]\Gη

∆t(µt)

nη +

≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:88)

t∈[n]\Gη

Ψt(µt)

It(µt) ,

n
(cid:88)

t=1

where we used the deﬁnition of Gη and the Cauchy–Schwarz inequality. A variance-dependent
martingale bound such as Freedman’s inequality shows that the regret Rn concentrates on the sum
(log n) term and the total expected information
over (conditional) expected regret up to an additive
gain is bounded by (cid:80)n

δ ). The complete proof is given in Appendix A.3.

c(γn + log 1

t=1 It(µt)

O

The next lemma is a standard result (Abbasi-Yadkori et al., 2011, cf. Lemma 10) and shows that

≤

for ﬁxed dimension, the total information gain depends only logarithmically on the horizon.

Lemma 2 For the information gain It as deﬁned in (4), γn = log det(Vn)
d log (cid:0)1 + nm
d

(cid:1) and βn =

γn + 2 log 1

δ + 1.

(cid:113)

−

log det(V0)

≤

Importantly, γn and βn have no dependence on the number of actions. Further, if θ
in a reproducing kernel Hilbert space
Gaussian process), even the dependence on d can be avoided (Srinivas et al., 2010).

with bounded Hilbert norm

θ
(cid:107)

H

≤

(cid:107)

H

is contained
1 (corresponding to a

∈ H

2.2. Regret Bound for Globally Observable Games

We ﬁrst analyze globally observable games (see Eq. (1)). The condition implies that
can
be estimated from data collected by the algorithm using appropriate actions. The game-dependent
constants that appear in the analysis depend on the degree to which the learner can efﬁciently gain
information, which roughly depends on how well the observation operators Ax are aligned with a
direction x
y in which we try to improve the accuracy of our estimation. We deﬁne the worst-case
alignment constant as

−

−

(cid:105)

(cid:104)

y, θ

x

Note that for games that are globally observable, α is always bounded, independent of the number
of actions (Lemma 13, Appendix A.6).

α = max
v∈Rd

max
x,y∈X

min
z∈X

2

.

x
(cid:104)

y, v
−
(cid:105)
2
A(cid:62)
z v
(cid:107)

(cid:107)

5

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Theorem 3 For any game that satisﬁes the global observability condition (1), there exists a uni-
versal constant C > 0 such that for any n

δ,

Cn2/3 (cid:0)αβn(γn + log 1

Rn

≤

≥

1 with probability at least 1
−
(cid:19)
(cid:18) 4n + 4
δ

δ )(cid:1)1/3 + 4 log

.

For ﬁxed feature dimension, βn and γn depend only logarithmically on the horizon, and there-
(n2/3). We show in Appendix G that all
fore the regret for globally observable games is Rn
globally observable games that are not locally observable have Rn = Ω(n2/3) in the worst case.
The ﬁrst step in the proof of Theorem 3 is to establish the existence of an action for which the
information gain is large relative to the regret of the greedy action.

˜
O

≤

Lemma 4 The greedy action ˆx∗

t = arg maxx∈X (cid:104)
Proof Let wt = arg maxw∈{x−y:x,y∈X } (cid:107)
w

2
V −1
(cid:107)
t−1

x, ˆθt−1

satisﬁes ∆t(ˆx∗

t )2

2αβt−1 maxz It(z).

(cid:105)

≤

be the most uncertain direction. Then,

∆t(ˆx∗

t ) = max
y∈X

(y

−

t )(cid:62) ˆθt−1 + β
ˆx∗

1/2
y
t−1(cid:107)

ˆx∗
t (cid:107)V −1

t−1 ≤

1/2
β
t−1(cid:107)

wt

.

(cid:107)V −1

t−1

−

Note three ways to write

w
(cid:107)

2
V −1
(cid:107)
t−1

=

(cid:107)

−1/2
t−1 wt
V

2 =
(cid:107)

wt, V −1
(cid:104)

t−1wt

. Basic linear algebra shows that
(cid:105)

(cid:107)

A(cid:62)

z V −1
t−1wt
−1/2
t−1 wt
V

2

(cid:107)
2 ≤
(cid:107)

(cid:107)

A(cid:62)

z V

−1/2
t−1 v
2
v

2
(cid:107)

max
v∈Rd

(cid:107)

(cid:107)
2 log det(1d + A(cid:62)

(cid:107)

≤

= λmax(A(cid:62)

z V −1

t−1Az)

z V −1

t−1Az) = 2It(z) .

For the last step we used the inequality a
of A(cid:62)
(cid:107)
informative action zt = arg maxx∈X It(x), it follows that

t−1Az are bounded in [0, 1] by the assumption

z V −1

≤

Az

2 log(1 + a) for a

(cid:107) ≤

[0, 1] and that the eigenvalues
1d. With the most

∈
1 and V −1

t−1 (cid:22)

∆t(ˆx∗
t )2
It(zt) ≤

2βt−1 min

z

wt, V −1
z V −1
A(cid:62)

t−1wt
t−1wt

2
(cid:105)

(cid:104)

(cid:107)

2 ≤
(cid:107)

2βt−1 max
v∈Rd

max
x,y∈X

min
z

(cid:104)

2
2 = 2αβt−1 .
(cid:105)

x

(cid:107)

y, v
−
A(cid:62)
z v

(cid:107)

Rearranging completes the proof.

proof is deferred to Appendix A.4.

The following lemma shows that IDS never plays a distribution that is too far from greedy. The

Lemma 5 Let µt be the IDS distribution at time t. Then ∆t(µt)

2 minx∈X ∆t(x).

≤

Proof of Theorem 3 Let zt = arg maxx∈X It(x) be the informative action. For p
µ(p) = (1
action. By deﬁnition, the information ratio of IDS is bounded by the ratio of µ(p),

[0, 1], let
+pδzt be the distribution that randomizes between the greedy and the informative

p)δˆx∗

−

∈

t

Ψt(µt)

min
p∈[0,1]

≤

∆t(µ(p))2
It(µ(p)) ≤

2αβt−1 min
p∈[0,1]

((1

−

t ) + p)2
p)∆t(ˆx∗
p∆t(ˆx∗
t )2

8αβt−1
∆t(ˆx∗

t ) ≤

16αβt−1
∆t(µt)

.

≤

6

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

The second inequality uses ∆t(z)
The third inequality follows by choosing p = ∆t(ˆx∗
t )
Next, Lemma 1 shows that with probability at least 1

1, It(ˆx∗
t )

≤

≥

0 and Lemma 4 to bound ∆t(ˆx∗

2αβt−1It(z).
[0, 1] and the last follows from Lemma 5.
∈
δ,
−

t )2

≤

Rn

≤

C inf
η≥0

nη +



(cid:115)

16nαβn(γn + log 1
δ )
η



 + 4 log

(cid:19)

(cid:18) 4n + 4
δ

,

where we used the fact that (βt)n

t=0 is non-decreasing. Optimizing η completes the proof.

2.3. Regret Bound for Locally Observable Games

In globally observable games, the learner can estimate the gaps for all actions, but may need to play
actions that are known to be suboptimal. The deﬁnition of local observability (see Eq. (2)) means
that the learner can gain information while playing only actions that appear plausibly optimal.

Recall the deﬁnition of the conﬁdence set

t−1) be the set of actions
that are plausibly optimal in round t. Again, our bound depends on the signal to noise ratio when
exploring. For a set of (plausible optimal) actions

, deﬁne the worst-case alignment for

t in Eq. (3) and let

(
C

t =

P

P

C

,

α(

) = max
v∈Rd

max
x,y∈Y

Y

Y ⊂ X
x
(cid:104)

min
z∈Y

2
y, v
−
(cid:105)
2
A(cid:62)
z v
(cid:107)
(cid:107)

.

Y

(5)

)

Y

≤

) <
Globally observable games satisfy α = α(
. The local observability condition implies that
). All games with bandit feedback (Ax = x) satisfy
this remains true if we restrict actions to
4. We refer to Lemma 13 in Appendix A.6 for details. We say a game is uniformly lo-
α(
Rd convex. All ﬁnite locally observable games are
cally observable if α(
uniformly locally observable because there are only ﬁnitely many subsets. The deﬁnition of the
alignment constant can be tightened with a more careful analysis, to obtain improved bounds on
model parameters such as the dimension in some cases. We refer to Appendix C.5 for details.

α0 for all

X
(
C
P

C ⊂

∞

≤

))

P

C

(

Theorem 6 For locally observable games denote αt = α(
C > 0 such that for any n

P
δ,

t). There exists a universal constant

≥

1 with probability at least 1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:0)γn + log 1

n
(cid:88)

αtβt

C

−

(cid:1) + 4 log

δ

Rn

≤

t=1

(cid:19)

(cid:18) 4n + 4
δ

.

For games that are uniformly locally observable, the regret bound is Rn

(√α0βnγnn).

˜
O

≤

We show in Appendix G that on locally observable games with more than one Pareto optimal
action, any algorithm suffers Ω(n1/2) regret in the worst case. To prove the upper bound, the ﬁrst
step is to construct an exploration distribution that is supported on the plausible maximizers
t and
P
t, nor is
has a constant information ratio. Note that IDS is not restricted to playing actions within
it required to explicitly compute this set. In fact, actions that are not plausible maximizers can have
a better trade-of between regret and information.

P

Lemma 7 For locally observable games, there exists an exploration action in

t such that,

P

x

∀

∈ P

t , ∆t(x)2

8αtβt−1 max
z∈Pt

It(z) .

≤

7

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

The complete proof is in Appendix A.5. The argument shows that for plausible maximizers x
∆t(x)2
Proof of Theorem 6 Let zt = arg maxx∈Pt It(x) be the most informative action in the current
plausible maximizer set

and is otherwise similar to the proof of Lemma 4.

4βt−1 maxy∈Pt (cid:107)
y

t. By Lemma 7,

2
V −1
t−1

x
(cid:107)

∈ P

−

≤

t,

P

Ψt(µt)

∆t(zt)2
It(zt) ≤

≤

8αtβt−1 .

Invoking the general IDS bound (Lemma 1) with η = 0 completes the proof.

The proof shows that randomization is not necessary to achieve a bounded information ratio in
locally observable games. Deterministic IDS (Kirschner and Krause, 2018), which optimizes the
ratio over a deterministic action choice xt = arg minx∈X Ψt(δx), achieves the same upper bound
with our analysis. Moreover, the bound shows how IDS adapts towards the current instance of the
partial monitoring game. Consider a globally observable game where after some ﬁnite time n0, the
t are locally observable in sense that α(
n0. In this
plausible maximizer sets
P
P
3/2
0 + (α0n)1/2). We have not yet identiﬁed non-artiﬁcial
case the regret bound is Rn
conditions that ensure this behavior, however. The gold standard would be to prove ﬁnite-time,
instance-dependent regret bounds with small constants. At present such results are more or less re-
stricted to ﬁnite-armed bandits, however, and remain open even for linear bandits (Hao et al., 2019).

(α1/3n

˜
O

α0,

t)

≤

≤

≥

∀

t

2.4. Smooth Convex Action Sets

X

X

) is not a polytope. Here we prove
The observability conditions are more ambiguous when conv(
has strictly positive principle curvature, then IDS enjoys ˜O(√n) regret on globally
that when
observable games. Curvature of the action set has been exploited in online learning (Huang et al.,
2017) and bandits (Bubeck et al., 2018). The latter article considers the starved adversarial linear
bandit, where the learner only observes the rewards when sampling an action from a pre-speciﬁed
distribution. They consider the case where the action set is the unit ball with respect to
(cid:107)·(cid:107)p and
prove that for p = 2 one can obtain O(n1/2) regret, but not for p > 2. This setting is close to a
R be the support
special case of linear partial monitoring (see Appendix C.3). Let hX : Rd
function of

X
Theorem 8 Assume that
is closed, convex, has a non-empty interior and that hX is twice dif-
ferentiable. Suppose furthermore that the game is globally observable according to Eq. (1) and has
strictly positive principle curvature everywhere:

, which is deﬁned by hX (u) = supx∈X (cid:104)

x, u
.
(cid:105)

→

X

(cid:32)

(cid:33)−1

κ◦ =

max
η∈Rd:(cid:107)η(cid:107)2=1

∇

λmax(

2hX (η))

> 0 .

Then, with probability at least 1

δ, for any n

1,

(cid:115)

−
(cid:18)

Rn

C

max

1,

≤

≥
(cid:0)γn + log 1

δ

(cid:19)

1
κ◦

βn

(cid:1) n + 4 log

(cid:19)

(cid:18) 4n + 4
δ

,

where C is a constant depending only on (Az : z

).

∈ X
The proof is given in Appendix D. The key argument shows that ∆t applied to the empirically
optimal action scales like the square of the diameter of the conﬁdence set. This compares favorably
with the case without curvature, where the error is about linear in the diameter of the conﬁdence set.

8

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

2.5. Contextual Partial Monitoring Games

The contextual bandit problem is a well known extension of the bandit setting where the learner
receives a context before choosing the action (Woodroofe, 1979; Langford and Zhang, 2008). We
introduce a novel contextual variant of linear partial monitoring, that strictly generalizes the linear
contextual bandit setting. Let
deﬁnes a
, where the map
partial monitoring game with action set
Az
(x, z)
and
∈ Z
xt θ + (cid:15)t where
chooses an action xt
(cid:105)
the parameter θ is the same in every context. The objective is to compete with the best in-hindsight
policy that maps context to actions. Regret is deﬁned with respect to the context-dependent solution
x∗
t = arg maxx∈Xzt (cid:104)

be a compact set of contexts. Each context z
Az
z and the observation operators
x}

x is assumed to be continuous. At time t, the learner receives a context zt

and the observation is at = Azt(cid:62)

zt. The reward is

xt, θ
(cid:104)

∈ Z

∈ X

x, θ

:
(cid:105)

(cid:55)→

Z

X

{

Rn =

x∗
t −
(cid:104)

xt, θ

.

(cid:105)

n
(cid:88)

t=1

The regret of the learner depends on the sequence of contexts observed and the corresponding
sequence of partial monitoring games which share the common parameter θ. All our notions extend
with the contextual argument,

∆t(x, z) = max
y∈Xz(cid:104)

y

−

x, ˆθt

+ β

(cid:105)

1/2
y
t−1(cid:107)

x

(cid:107)V −1

t−1

,

−

It(x, z) = log det(1 + Az(cid:62)

x V −1

t−1Az

x) .

Conditional IDS is the policy that minimizes the µt = arg minµ∈P(X ) Ψ(µ, zt) conditioned on the
observed context. The next result extends the regret guarantees for locally and globally observable
games to the contextual setting by making strong assumptions on the sequence of games deﬁned by
the context. We refer to Appendix E.1 for our formal result.

≤ O

(cid:0)(α0βn(γn + log 1

(cid:0)n2/3(αβn(γn +log 1

Corollary 9 (Informal) If the sequence of games deﬁned by the observed contexts z1, . . . , zn are
δ ))1/3(cid:1) regret with high
globally observable, conditional IDS achieves Rn
probability. If the sequence of games is uniformly locally observable, then conditional IDS achieves
Rn

≤ O
Perhaps surprisingly, the contextual case allows for much weaker conditions under which no-
regret is possible if the learner exploits the distribution of contexts. Here we study the case where
P(
); the case where the distribution is unknown
the context follows a known distribution ν
or the learner tries to adapt her behaviour towards an arbitrary sequence is left as an interesting
direction for future work. It is instructive to think about some examples:

δ )n)1/2(cid:1).

Z

∈

•

∈ X

the learner obtains no information (Az

x = 0 for
An extreme case is where for some z
all x
z). In such rounds the only sensible choice is the greedy action. Exploration needs
to happen in rounds where information is available and needs to be sufﬁciently diverse to
account for rounds where the learner is forced to play greedily. Note that while there can
be vanishing information gain in some rounds, the expected information gain, that takes the
distribution ν over the context into account, is non-zero.

∈ Z

•

Since also the greedy action depends on the random context, there can be cases where the
learner incurs sufﬁcient exploration by playing mostly greedy. This effect has been studied in
the bandit literature before (Bastani et al., 2017; Hao et al., 2019).

9

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Conditional IDS does not depend on the distribution ν and it is easy to see that it can behave
suboptimally in both examples. To include the randomness of the context within the IDS framework,
). As
consider a joint distribution ξ
before, ∆t(ξ) and It(ξ) extend linearly. At time t, contextual IDS computes a distribution ξ with
marginal ξz = ν, that minimizes the joint ratio,

) over context and actions with marginal ξz

X × C

P(

P(

Z

∈

∈

ξt =

arg min
ξ∈P(X ×Z), ξz=ν

Ψ(ξ) .

ξt(x

The action is sampled from xt
In the joint minimization of the
information ratio the contextual distribution ν contributes to exploration and a smaller information
ratio. The intuition is that to estimate along a direction x
z, the
learner can wait for a different context z(cid:48) to be realized where x
y can easily be estimated and at
low cost. This leads to the following condition that deﬁnes globally observable contextual games:

zt) after observing zt.
|

y in a contextual action set x, y

∈ X

∼

−

−

z
∀

∈ Z

and x, y

z
∈ X

⇒ ∃

∈ Z

s.t. x

y

−

∈

z(cid:48)

span(Ax : x

z(cid:48)) .

∈ X

(6)

The regret bound depends on the probability that a context occurs where estimation is possible. The
expected worst-case alignment α(ν) is deﬁned in Appendix E.2, Eq. (14). It satisﬁes the intuitive
Eν[α(z)] and recovers the previous deﬁnition for Dirac delta distributions
upper bound α(ν)
(Lemma 21, Appendix E.2). For ﬁnite games with ﬁnite context set, it further holds that

≤

α(ν)

max
v∈Rd

max
z∈Z

max
x,y∈Xz

min
z(cid:48)∈Z

min
u∈Xz(cid:48)

(cid:104)
ν(z(cid:48))

≤

v, x

2
y
(cid:105)
−
Az(cid:48)(cid:62)
u v
(cid:107)

(cid:107)

2 ,

thus it sufﬁces that a direction x
with non-zero probability ν(z(cid:48)). The next result quantiﬁes the rate in globally observable games.

z can be estimated under some context z(cid:48)

that appears

∈ Z

y in

−

X

Theorem 10 For globally observable contextual games with bounded expected worst-case align-
1, the regret is bounded with probability at least 1
ment α(ν), for any n

δ,

≥

Rn

≤

Cn2/3 (cid:0)α(ν)βn(γn + log 1

δ )(cid:1)1/3 + 4 log

−
(cid:18) 4n + 4
δ

(cid:19)

.

All proofs and details for this result can be found in Appendix E.2 and the analogous result for the
locally observable case is in Appendix E.3.

3. Classiﬁcation of Finite Games
The upper bounds show that for globally observable games the regret is ˜O(n2/3), while for locally
observable games it is ˜O(n1/2). Of course, if there is only one Pareto optimal action, then the
regret vanishes for any algorithm that just plays this action. The classiﬁcation theorem follows by
proving that for games that are not globally observable, the regret is linear in the worst case, that for
globally observable games that are not locally observable the regret is Ω(n2/3) and that for locally
observable games with more than one Pareto optimal action it is Ω(n1/2). These lower bounds are
supplied in Appendix G. For simplicity, our results are for the expected minimax regret, which is

R∗

n = inf
π

sup
θ

E[Rn(π, θ)] .

10

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

The inﬁmum is over policies π = (πt)n
and Rn(π, θ) = (cid:80)n
on
sampled from the policy π.

t=1(cid:104)

x∗

−

X

xt, θ

(cid:105)

t=1 deﬁned by a sequence of

is the regret for parameter θ

F

t-measurable random variables
Rd when the actions are

∈

Theorem 11 The minimax regret for any ﬁnite linear partial monitoring game satisﬁes

R∗

n =


0

˜Θ(n1/2)
˜Θ(n2/3)

Ω(n)

if there is only one Pareto optimal action,
for locally observable games,
for globally observable games,
otherwise .

The classiﬁcation theorem is proven by combining upper and lower bounds, carefully checking that
all cases have been covered. We further show in Appendix F that our deﬁnitions of local and global
observability coincide with the standard notions in ﬁnite partial monitoring that are based on the
neighborhood graph, as well as the notion of a global observer set used by Lin et al. (2014).

4. Applications and Extensions

The framework of linear partial monitoring captures many applications and models for sequential
decision making that were previously studied in the literature. We outline some of them below and
provide additional details in Appendix C.

Semi-Bandit and Full Information Feedback The observation operators can be deﬁned to yield
more information than in the bandit case, up to revealing the parameter in each round (Ax = 1d).
Naturally, additional information should only improve performance, but in our analysis, the bound
degrades logarithmically with the observation dimension m. For the case of full information feed-
back, we show in Appendix C.1 how to improve the bounds to get Rn

(√dn).

˜
O

≤

Linear Bandits The stochastic linear bandit setting is a special case of our setup with Ax = x
(Abe and Long, 1999; Auer, 2003; Dani et al., 2008; Abbasi-Yadkori et al., 2011). Our analysis
(d√n) dependency for the regret and generalizes the results for
achieves the optimal Rn
heteroscedastic bandits by Kirschner and Krause (2018). The UCB algorithm (Auer et al., 2002)
has a distinct relation to the IDS framework, as we explain in Appendix B.3.

˜
O

≤

0. The relative feedback for (x1, x2)

Dueling Bandits
In dueling bandits, the learner chooses a pair of actions and receives binary feed-
back indicating which action has higher reward (Yue and Joachims, 2009). This feedback model can
=
be cast as partial monitoring game (Gajane and Urvoy, 2015). Let
x2 and noise is added
0
X
×X
−
x1,x2θ. Note that bounded noise is subgaussian
such that at
and our analysis applies. A possible reward model is to use averaged features xx1,x2 = (x1 + x2)/2.
Dueling bandits are locally observable if the learner can compare any pair of actions, and globally
observable if comparisons are restricted to ‘adjacent’ actions. See Appendix C.5 for details.

is binary with expectation A(cid:62)

is deﬁned with Ax1,x2 = x1

Rd be a ground set and

∈ {−

∈ X

0
X

1, 1

⊂

X

}

Combinatorial Bandits This is the original motivation for the linear partial monitoring setting
by Lin et al. (2014) and Chaudhuri and Tewari (2016) and leads to games that are either locally
or globally observable. We refer to the previous works for further applications. Our formulation
covers combinatorial bandits both with bandit and semi-bandit feedback. An important special case
is the batch setting (Appendix C.4).

11

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Transductive and Starved Bandits The transductive linear bandit setting was recently proposed
by Fiez et al. (2019). The learner has access to a set of actions that is dedicated for exploration,
while the objective is to achieve low regret on a different, target set of actions. It was open to ﬁnd an
approach that minimizes cumulative regret, which we effectively resolve (Appendix C.2). Similar
in spirit are starved bandits (Bubeck et al., 2018), where the learner only obtains information when
sampling actions from a pre-deﬁned distribution. This setting is closely connected to our contextual
setting (see Appendix C.3) and the regret bounds on convex action sets in Section 2.4.

Product Testing and Invasive Measurements An early toy example for a globally, but not locally
observable game is that of “apple tasting” (Cesa-Bianchi et al., 2006). In this task, the learner
optimizes a production chain with the option to remove a product for inspection (and destroying
it in the process). Other applications include parameter tuning of experimental facilities such as
particle accelerators (Kirschner et al., 2019), where invasive measurement devices provide a very
rich signal at the expense of voiding any downstream measurements (for a stylized version of this
problem and a numerical demonstration of IDS, see Appendix C.6).

Kernelized Partial Monitoring Our approach and the analysis extend to the kernelized setting,
where the reward function is in a known reproducing kernel Hilbert space (RKHS). This includes
kernelized bandits (Srinivas et al., 2010; Abbasi-Yadkori, 2012; Chowdhury and Gopalan, 2017),
also known as Bayesian optimization, as a special case. Interesting applications beyond the bandit
setting include Bayesian optimization with gradients (Wu et al., 2017b) or even Hessian evaluations
(Wu et al., 2017a). Unlike previous results, our approach leverages all available information and
achieves a strong ﬁnite time convergence guarantee. We refer the reader to Appendix C.6 for a
detailed introduction and formal statements. In the limit with continuous action sets, dueling bandits
can be understood as global optimization where the learner has access only to the gradient.

5. Discussion

We introduced information directed sampling for stochastic linear partial monitoring, which – to
the best of our knowledge – is the ﬁrst approach that achieves the optimal regret rate in all ﬁnite
linear games. Our classiﬁcation theorem provides a complete picture of the achievable worst-case
regret rates in ﬁnite linear games. Nevertheless, many directions are left for future work. Proving
non-trivial instance-dependent regret bounds for IDS is an important open question, even for the
standard linear bandit setting. Another challenge is to ﬁnd precise observability conditions that
capture the rate achievable on continuous action sets.

(d

|X |

For a naive implementation of IDS for ﬁnite games, the computational complexity per step is
2), which is required to compute all gap estimates. The exact IDS distribution can be found
O
by iterating over pairs of actions (a solution supported on two actions always exists). Alternatively a
standard convex solver can be used to minimize the information ratio over the probability simplex.
With a weaker regret estimate (Appendix B.1), the action minimizing the information ratio can be
), which matches the computational cost of index based approaches for bandits like
found in
UCB. For larger or continuous action sets, some previous approaches rely on oracle solvers (Lin
et al., 2014; Chaudhuri and Tewari, 2016) and for the bandit setting, Thompson sampling is a well-
known oracle efﬁcient method (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017). Given the
generality of our results, ﬁnding an oracle-efﬁcient approximation of IDS is an important task for
future work.

(
|X |

O

12

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Acknowledgments

References

2012.

This project has received funding from the European Research Council (ERC) under the European
Unions Horizon 2020 research and innovation programme grant agreement No 815943.

Yasin Abbasi-Yadkori. Online Learning for Linearly Parametrized Control Problems. PhD thesis,

Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems, pages 2312–2320, 2011.

Naoki Abe and Philip M. Long. Associative reinforcement learning using linear probabilistic con-
cepts. In Proceedings of the Sixteenth International Conference on Machine Learning, ICML
’99, pages 3–11, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. ISBN 1-
55860-612-2.

Marc Abeille and Alessandro Lazaric. Linear thompson sampling revisited. In Artiﬁcial Intelligence

and Statistics, pages 176–184, 2017.

Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs.

In International Conference on Machine Learning, pages 127–135, 2013.

A. Antos, G. Bart´ok, D. P´al, and Cs. Szepesv´ari. Toward a classiﬁcation of ﬁnite partial-monitoring

games. Theoretical Computer Science, 473:77–99, 2013.

P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning, 47:235–256, 2002.

Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res.,

3:397–422, March 2003. ISSN 1532-4435.

G. Bart´ok, D. P´al, and Cs. Szepesv´ari. Minimax regret of ﬁnite partial-monitoring games in stochas-
In Proceedings of the 24th Annual Conference on Learning Theory, pages

tic environments.
133–154, 2011.

G. Bart´ok, N. Zolghadr, and Cs. Szepesv´ari. An adaptive algorithm for ﬁnite stochastic partial
monitoring. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, ICML, pages 1779–1786, USA, 2012. Omnipress.

G. Bart´ok, D. P. Foster, D. P´al, A. Rakhlin, and Cs. Szepesv´ari. Partial monitoring—classiﬁcation,

regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967–997, 2014.

G´abor Bart´ok and Csaba Szepesv´ari. Partial monitoring with side information. In International

Conference on Algorithmic Learning Theory, pages 305–319. Springer, 2012.

Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Mostly exploration-free algorithms for

contextual bandits. arXiv preprint arXiv:1704.09011, 2017.

13

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

J Bretagnolle and C Huber. Estimation des densit´es: risque minimax. Zeitschrift f¨ur Wahrschein-

lichkeitstheorie und verwandte Gebiete, 47(2):119–137, 1979.

S. Bubeck, M. Cohen, and Y. Li. Sparsity, variance and curvature in multi-armed bandits.

In
F. Janoos, M. Mohri, and K. Sridharan, editors, Proceedings of Algorithmic Learning Theory,
volume 83 of Proceedings of Machine Learning Research, pages 111–127. PMLR, 07–09 Apr
2018.

N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Regret minimization under partial monitoring. Mathe-

matics of Operations Research, 31:562–580, 2006.

Nicolo Cesa-Bianchi and G´abor Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences, 78(5):1404–1422, 2012.

Sougata Chaudhuri and Ambuj Tewari. Phased exploration with greedy exploitation in stochastic
combinatorial partial monitoring games. In Advances in Neural Information Processing Systems,
pages 2433–2441, 2016.

Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International

Conference on Machine Learning, 2017.

Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic Linear Optimization under Bandit

Feedback. In COLT, pages 355–366. Omnipress, 2008.

Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design for
In Advances in Neural Information Processing Systems 32, pages

transductive linear bandits.
10666–10676. Curran Associates, Inc., 2019.

Pratik Gajane and Tanguy Urvoy. Utility-based dueling bandits as a partial monitoring game. arXiv

preprint arXiv:1507.02750, 2015.

S. Gerchinovitz and T. Lattimore. Reﬁned lower bounds for adversarial bandits.

In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information
Processing Systems 29, NIPS, pages 1198–1206. Curran Associates, Inc., 2016.

Javier Gonz´alez, Zhenwen Dai, Andreas Damianou, and Neil D Lawrence. Preferential bayesian
optimization. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 1282–1291. JMLR. org, 2017.

Botao Hao, Tor Lattimore, and Csaba Szepesvari. Adaptive exploration in linear contextual bandit.

arXiv preprint arXiv:1910.06996, 2019.

R. Huang, T. Lattimore, A. Gy¨orgy, and Cs. Szepesv´ari. Following the leader and fast rates in online
linear prediction: Curved constraint sets and other regularities. Journal of Machine Learning
Research, 18:1–31, 2017.

Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaus-
sian processes and kernel methods: A review on connections and equivalences. arXiv preprint
arXiv:1807.02582, 2018.

14

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Johannes Kirschner and Andreas Krause.

Information directed sampling and bandits with het-

eroscedastic noise. arXiv preprint arXiv:1801.09667, 2018.

Johannes Kirschner, Mojmir Mutny, Nicole Hiller, Rasmus Ischebeck, and Andreas Krause. Adap-
tive and safe bayesian optimization in high dimensions via one-dimensional subspaces. In Inter-
national Conference on Machine Learning, pages 3429–3438, 2019.

J. Komiyama, J. Honda, and H. Nakagawa. Regret lower bound and optimal algorithm in ﬁnite
stochastic partial monitoring.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett, editors, Advances in Neural Information Processing Systems 28, NIPS, pages 1792–
1800. Curran Associates, Inc., 2015.

J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side informa-
tion. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural Information
Processing Systems 20, NIPS, pages 817–824. Curran Associates, Inc., 2008.

Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. 2018.

Tor Lattimore and Csaba Szepesv´ari. Cleaning up the neighborhood: A full classiﬁcation for adver-

sarial partial monitoring. In Algorithmic Learning Theory, pages 529–556, 2019.

Tor Lattimore and Csaba Szepesvari. Exploration by optimisation in partial monitoring. arXiv

preprint arXiv:1907.05772, 2019.

Tor Lattimore and Csaba Szepesv´ari. An information-theoretic approach to minimax regret in partial

monitoring. arXiv preprint arXiv:1902.00470, 2019.

Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen. Combinatorial partial mon-
itoring game with linear feedback and its applications. In International Conference on Machine
Learning, pages 901–909, 2014.

Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. In

Advances in Neural Information Processing Systems, pages 1583–1591, 2014.

A. Rustichini. Minimizing regret: The general case. Games and Economic Behavior, 29(1):224–

243, 1999.

Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. International Conference on
Machine Learning, 2010.

Yanan Sui, Yisong Yue, and Joel W Burdick. Correlational dueling bandits with application to

clinical treatment in large decision spaces. arXiv preprint arXiv:1707.02375, 2017a.

Yanan Sui, Vincent Zhuang, Joel W Burdick, and Yisong Yue. Multi-dueling bandits with dependent

arms. arXiv preprint arXiv:1705.00253, 2017b.

Yanan Sui, Masrour Zoghi, Katja Hofmann, and Yisong Yue. Advancements in dueling bandits. In

IJCAI, pages 5502–5510, 2018.

15

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Hastagiri Vanchinathan, G´abor Bart´ok, and Andreas Krause. Efﬁcient partial monitoring with prior

information. In Neural Information Processing Systems (NIPS), 2014.

Zi Wang, Bolei Zhou, and Stefanie Jegelka. Optimization as estimation with gaussian processes in

bandit settings. In Artiﬁcial Intelligence and Statistics, pages 1022–1031, 2016.

Michael Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the

American Statistical Association, 74(368):799–806, 1979.

Anqi Wu, Mikio C Aoi, and Jonathan W Pillow. Exploiting gradients and hessians in bayesian

optimization and bayesian quadrature. arXiv preprint arXiv:1704.00060, 2017a.

Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with
gradients. In Advances in Neural Information Processing Systems, pages 5267–5278, 2017b.

Yisong Yue and Thorsten Joachims.

Interactively optimizing information retrieval systems as a
dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine
Learning, pages 1201–1208, 2009.

16

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Appendix A. Additional Lemmas and Proofs

A.1. Linear Partial Monitoring: General Setup

Our setting can be formulated more generally, to allow applications where the learner can choose
between different observation maps that are associated to the same action. Let
be a compact
indexes an action-observation tuple (xi, Ai) and the collection of such tuples
index set. Each i
∈ I
Rd×m. At step t, the learner chooses an action
=
represents a game
G
∈ I} ⊂
index it and observes the outcome A(cid:62)
it θ + (cid:15)t. The unobserved reward is
xi, θ
. We assume that the
(cid:105)
(cid:104)
(xi, Ai) is continuous to guarantees that the IDS distribution exists. The dimension m of
map i
the observation can also depend on the action i in general, but for simplicity, we set mi = m. We
overload

(xi, Ai) : i

Rd

(cid:55)→

×

I

{

A.2. Finite Partial Monitoring

Unlike the standard ﬁnite and linear bandit frameworks, ﬁnite partial monitoring is not quite a spe-
cial case of the linear setting. On the one hand, our setting permits inﬁnite observation (and action)
spaces, which are not usually covered by existing results. On the other hand, the assumptions of our
setting mean the algorithm does not recover known bounds for algorithms in the ﬁnite unstructured
1, while in the ﬁ-
setting. The main reason is that we do not restrict θ except in terms of
nite setting the θ is effectively constrained to the probability simplex. Consider the following ﬁnite
game, characterized by reward and signal matrices

(cid:107) ≤

(cid:107)

θ

(cid:19)

(cid:18)1 1
0 0

,

=

R

Σ =

(cid:19)

(cid:18)0 0
0 0

.

The signal matrix is such that the learner observes no information. Meanwhile, however, the rewards
are such that the learner knows immediately that the ﬁrst action is optimal, so in the ﬁnite partial
monitoring literature this game is trivial and good algorithms suffer zero regret. Our algorithm,
√2),
however, does not assume that θ lies in the probability simplex, and when θ = (
the second action is clearly optimal. The different assumptions on θ mean that this game is now
hopeless and algorithms consequentially suffer linear regret.

√2,

−

−

A.3. Proof of Lemma 1

Proof Using Freedman’s inequality one can get the following concentration result on the regret
(Kirschner and Krause, 2018, Lemma 13). For any ﬁxed n, with probability at least 1

δ/2,

−

Rn

≤

5
4

n
(cid:88)

t=1

∆t(µt) + 4 log

(cid:19)

(cid:18) 4n + 4
δ

.

The ﬁrst sum is bounded by

n
(cid:88)

t=1

∆t(µt) =

∆t(µt) +

(cid:88)

t∈Gη

(cid:88)

t∈[n]\Gη

∆t(µt)

nη +

≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:88)

t∈[n]\Gη

Ψt(µt)

It(µt) .

n
(cid:88)

t=1

17

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

The inequality follows from the deﬁnition of Gη and we use Cauchy-Schwarz to bound

(cid:88)

∆t(µt)

(cid:88)

(cid:112)Ψt(µt)It(µt)

t∈[n]\Gη

≤

t∈[n]\Gη

(cid:115) (cid:88)

≤

t∈[n]\Gη

Ψt(µt)

It(µt) .

(cid:88)

t∈[n]

t=1 It(µt) is close to the realized information gain (cid:80)n

In the last step, we also used the non-negativity of It(µt). Finally, the sum over expected information
gain (cid:80)n
t=1 It(xt) with high probability. This
is made precise in Lemma 3 of Kirschner and Krause (2018), which shows that if It(x)
1, then
with probability at least 1

δ/2, for any n

1,

≤

−

n
(cid:88)

t=1

It(µt)

2

≤

It(xt) + 4 log

+ 8

(cid:19)

(cid:18) 1
δ

≥
n
(cid:88)

t=1

Note that our boundedness assumptions
assumption It(x)
≤
completes the proof.

1d imply the required
2
t (cid:22)
(cid:107)
1. By deﬁnition γn = (cid:80)n
t=1 It(xt). A union bound over the previous displays

1 and the fact that V −1

Ax

≤

(cid:107)

A.4. Proof of Lemma 5

Proof By assumption, for any p

[0, 1) and any x

,

∈ X

Ψt(µt) =

∈
∆t(µt)2
It(µt) ≤

∆t((1
(1

−
−

p)µt + pδx)2
p)It(µt)

=: Ψt(p)

Since Ψt(0) = Ψt(µt) and p

Ψt(p) is differentiable at p = 0 it follows that

(cid:55)→

0

≤

Ψ(cid:48)

t(0) =

2∆t(µt)∆t(x)

∆t(µt)2

−
It(µt)

.

The claim follows by rearranging.

A.5. Proof of Lemma 7

For the analysis it is useful to deﬁne a lower bound on the regret,

δt(x) = min
θ∈Ct−1

max
y
y∈X (cid:104)

−

x, θ

.

(cid:105)

(7)

By deﬁnition, with probability at least 1
sible maximizers is equivalently described by

is a compact set. We further deﬁne the relaxed bound ˜δt(x) = maxx(cid:48)∈X minθ∈Ct−1(cid:104)
x(cid:48)
By the minimax inequality it holds that ˜δt(x)
maximization to get ˜δt(x) = maxy∈X

−
: δt(x) = 0
t
P
}
x, θ
.
(cid:105)
δt(x). For ˜δt, we can explicitly solve the inner

δ it holds that δt(x)
x

. The set of plau-
(cid:105)

and by continuity

≤
x, ˆθt−1

∈ X

t =

≤ (cid:104)

x, θ

−

−

P

x

{

y

y

.

x∗

(cid:104)

−

1/2
β
t−1(cid:107)

(cid:105) −

−

(cid:107)V −1

t−1

Lemma 12 For the upper bound on the regret ∆t(x), it holds that

∆t(x) = max
y∈Pt

(y

−

x)(cid:62) ˆθt−1 + β

1/2
t−1(cid:107)

y

x

(cid:107)V −1

t−1

,

−

where we restricted the maximum to plausible maximizers.

18

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Proof [of Lemma 12] Assume that y is not a plausible maximizer, i.e. δt(y) > 0. Then for any
θ

y)(cid:62)θ > 0. For ﬁxed x

t, there exists a z = z(θ)

we ﬁnd,

s.t. (z

∈ C

∈ X

−

∈ X
x)(cid:62)θ = ∆t(x) .

(8)

(y

max
θ∈Ct

−

x)(cid:62)θ < max
θ∈Ct

(zθ

−

x)(cid:62)θ

max
θ∈Ct

max
z

(z

−

≤

Hence, the left-hand side is maximized only if y

t is a plausible maximizer.

∈ P

Proof [of Lemma 7] Lemma 12 shows that we can write ∆t+1(x) as follows:

∆t(x) = max
y∈Pt(cid:104)

y

−

x, ˆθt−1

+ β

(cid:105)

1/2
t−1(cid:107)

y

x

(cid:107)V −1

t−1

.

−

Further, for any plausible action x
x, ˆθt−1
y
certainty,

t, we can bound the estimated gap by the associated un-
˜δt(x).

∈ P
x

(cid:104)

y

(cid:105) ≤
−
2β
This implies that for all x
most uncertain direction in the set of plausible maximizers wt = arg maxw∈{x−y:x,y∈Pt} (cid:107)
Then, for x

, which follows from the fact that 0 = δt(x)
t−1 maxy∈Pt (cid:107)
x

−
t, ∆t(x)

(cid:107)V −1

(cid:107)V −1

∈ P

1/2

−

≤

t−1

t−1

y

t,

. Speciﬁcally, let wt be the

wt

2
V −1
(cid:107)
t−1

.

1/2
β
t−1(cid:107)

≥

∈ P

∆t(x)2

4βt−1

wt

(cid:107)

2
V −1
t−1 ≤

(cid:107)

≤

8αtβt−1 max
z∈Pt

It(z) .

The last step follows from the same argument as in the proof of Lemma 4, where we restrict x, y to

t and use the deﬁnition αt = α(

t−1).

C

P

A.6. Bounds for the Alignment Constant

Lemma 13 Let
Let A
most d columns of A such that span(B) = span(A). Then

such that for all x, y
Rd×pm be the matrix formed by concatenating (Az : z

ﬁnite with p =

P ⊂ X

|P|

∈

∈ P
∈ P

y

).
, x
) and let B be a subset of at

span(Az : z

∈ P

−

∈

α(

) = max
v∈Rd

max
x,y∈P,x(cid:54)=y

P

x
(cid:104)
−
maxz∈P

2

y, v

(cid:105)
A(cid:62)
z v

(cid:107)

2 ≤
(cid:107)

min
w:Aw=x−y

(cid:32)

(cid:33)2

(cid:88)

z∈P

wz

(cid:107)

(cid:107)

≤

dλmin(BB(cid:62))−1 .

Further, in the bandit game (where Ax = x), α(

)

4.

≤
= y. By assumption, there exists a w such that x

P

y = Aw with w

= 0.

Proof Let x, y
Then,

∈ P

with x

x

(cid:104)

−

y, v

2 =
(cid:105)

Aw, v
(cid:104)

(cid:105)

2 =

w, A(cid:62)v

(cid:104)

2 =
(cid:105)

wz, A(cid:62)
z v
(cid:104)

(cid:105)

,

(cid:32)

(cid:88)

z∈P

−

(cid:33)2

where we denote by wz
∈
proves the ﬁrst inequality,

Rm the weights corresponding to Az. An application of Cauchy-Schwarz

α(

)

P

≤

x
−
(cid:104)
maxz∈P

y, v

2
(cid:105)
A(cid:62)
z v

(cid:107)

(cid:107)

2 ≤

(cid:0)(cid:80)

wz
z∈P (cid:107)
maxz∈P

A(cid:62)
z v
(cid:107)
2
(cid:107)

(cid:107)(cid:107)
A(cid:62)
z v

(cid:107)

(cid:1)2

(cid:32)

(cid:88)

z∈P

wz

(cid:107)

(cid:107)

≤

(cid:33)2

.

19

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

In the bandit game we can choose wx = 1, wy =
In general, we can choose
symbol w in a different dimension),

Q ⊂ P

with

|Q|

1 and wz = 0,
= d s.t. wz = 0 for z

−

∈ P \{

z
∀
∈ P \ Q

x, y

, hence α(
4.
}
. Therefore (we reuse the

≤

P

)

α(

)

P

≤

(cid:32)

(cid:33)2

(cid:88)

z∈Q

wz
(cid:107)

(cid:107)

w
d
(cid:107)

2 .
(cid:107)

≤

Denote B = (Az : z
squares solution w∗ = (B(cid:62)B)†B(cid:62)(x
the properties of the pseudo inverse and

∈ Q

). The solution that minimizes the right-hand side is the ordinary least-
y) where † denotes the pseudo inverse. Therefore, using
−
1,
x
(cid:107)
B(B(cid:62)B)†(B(cid:62)B)†B(cid:62)(cid:17)

= dλmin(BB(cid:62))−1 .

(BB(cid:62))−1(cid:17)

= dλmax

(cid:107) ≤

−

(cid:16)

(cid:16)

y

α(

)

P

≤

2

w∗
d
(cid:107)

(cid:107)

≤

dλmax

Appendix B. Regret Estimators and Information Gain Functions

B.1. Regret estimate

x)(cid:62)θ is deﬁned the tightest way for the given
Our regret estimate ∆t(x) = maxθ∈Ct−1 maxy∈X (y
conﬁdence bounds (up to truncation for bounded gaps). An interesting fact is that ∆t(x) is a convex
function because the maximum is over convex functions. The estimate can be relaxed to

−

˜∆t(x) = max
y∈X (cid:104)

y, ˆθt−1

+ β

(cid:105)

1/2
t−1(cid:107)

y

(cid:107)V −1

t−1 −

x, ˆθt−1
(cid:104)

(cid:105) −

β

1/2
x
t−1(cid:107)

(cid:107)V −1

t−1

(cid:16)

(cid:17)

.

˜∆t(x). For ˜∆t, the maximum over

It holds that ∆t(x)
computational complexity to compute the regret estimate from
˜∆t relies on directly estimating the value of x(cid:62)θ for all actions x
in the general partial monitoring setting. The bandit game is an example where this is possible.

is independent of x, which reduces the
2) to
). The estimate
(
|X |
, which is not always possible

O
∈ X

(
|X |

O

≤

X

B.2. Directed Information Gain

Various ways of deﬁning the information gain It(x) are discussed in (Kirschner and Krause, 2018).
The choice It(x) = log det(1m + A(cid:62)
t Ax) that we use in our main exposition is perhaps the
most natural starting point, as it corresponds to the mutual information I(x, Axθ; θ
t−1) if we
|F
Ax = Vt + AxA(cid:62)
deﬁne a corresponding Gaussian prior and likelihood. We denote Vt
x . For a ﬁxed
|
w

Rd the directed information gain is

x V −1

∈

It(x; w) := log

(cid:16)

w
(cid:107)

(cid:107)

2
V −1
t−1

(cid:17)

(cid:16)

log

−

w

2
(Vt−1|Ax)−1
(cid:107)

(cid:107)

(cid:17)

.

(9)

The deﬁnition corresponds to the Shannon mutual information I(x, at;
) which measures the
(cid:105)
Gaussian entropy reduction of θ projected onto the subspace spanned by w. The next lemma shows
the information processing inequality It(x; w)

It(x).

w, θ

(cid:104)

Lemma 14 (Information processing inequality) For all w, x

Rd, It(x; w)

It(x).

≤

∈

≤

20

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Proof The proof is an exercises in linear algebra and makes use of the Sherman-Morrison formula
and the matrix determinant lemma.

It(x; w) = log





2
V −1
(cid:107)
t−1

w
(cid:107)
2
(Vt−1|Ax)−1





w

(cid:107)


(cid:107)

=

log

1

−

−

w(cid:62)V −1

x V −1
t−1Ax(1m + A(cid:62)
2
w
V −1
t−1

t−1Ax)−1A(cid:62)

x V −1

t−1w





(cid:16)

(cid:16)

v(cid:62)v
−
v(cid:62) (cid:16)

1d

−

max
v∈Rd:(cid:107)v(cid:107)2=1 −

≤

log

log

= max

v∈Rd:(cid:107)v(cid:107)2=1 −
(cid:18)(cid:16)

(cid:18)

= log

λmax

1d

−

(cid:107)
(cid:107)
−1/2
t−1 Ax(1m + A(cid:62)
v(cid:62)V

x V −1

t−1Ax)−1A(cid:62)
x V

−1/2
t−1 v

V

−1/2
t−1 Ax(1m + A(cid:62)

x V −1

t−1Ax)−1A(cid:62)
x V

(cid:17)

(cid:17)

(cid:17)

v

−1/2
t−1
(cid:17)−1(cid:19)(cid:19)

.

V

−1/2
t−1 Ax(1m + A(cid:62)

x V −1

t−1Ax)−1A(cid:62)
x V

−1/2
t−1

We ﬁrst used Sherman-Morrison to compute (Vt−1

Ax)−1 and then maximize over v =
|

V −1/2
t−1 w
(cid:107)w(cid:107)
V −1
t−1

.

log

det

1d

−1/2
t−1 Ax(1m + A(cid:62)
V

x V −1

t−1Ax)−1A(cid:62)
x V

−1/2
t−1

≤

−

(cid:17)−1(cid:19)

(cid:18)

(cid:18)

(cid:16)

(cid:16)

(cid:16)

= log

det

1m + A(cid:62)

x V −1

t−1Ax
(cid:17)

= log det

1m + A(cid:62)

x V −1

t−1Ax

= It(x)

(cid:17)

det

(cid:16)

1m + A(cid:62)

x V −1

t−1Ax

A(cid:62)

x V

−1/2
t−1 V

−1/2
t−1 Ax

(cid:17)−1(cid:19)

−

The inequality follows because all eigenvalues of the matrix inside the determinant are not smaller
than 1, and then the generalized matrix determinant lemma to rewrite the expression.

Lemma 15 Let
span(
{

Ax : x

∈ Y}

Y ⊂ X

be a subset of actions and let w = x
). Then the most informative action in the set

y for x, y
satisﬁes

∈ Y

such that w

∈

w

2
V −1
t−1 ≤
(cid:107)

(cid:107)

2α(

) max
z∈Y

Y

−
Y
It(z; w) .

Proof First, note that 1

2 (1d + A(cid:62)

z V −1

t−1Az)

1d by our assumption that

(cid:22)
2w(cid:62)V −1
t−1Az(1m + A(cid:62)

Az
2
(cid:107)
(cid:107)
z V −1
t−1Az)−1A(cid:62)

t−1w

z V −1

≤

1, hence

A(cid:62)

z V −1

t−1w

2

(cid:107)

≤

(cid:107)

We further bound the following fraction:

min
z∈Y

(w(cid:62)V −1
z V −1
A(cid:62)
(cid:107)

t−1w)2
t−1w

2 ≤
(cid:107)

max
x,y∈Y

max
v∈Rd

min
z∈Y

x
(cid:104)

y, v
−
A(cid:62)
z v
(cid:107)

(cid:107)

2
2 = α(
(cid:105)
Y

) .

21

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Since x

log(1

x) for all x

[0, 1],

≤ −

−

w
(cid:107)

2
V −1
(cid:107)
t−1

= min
z∈Y

2α(

Y

≤

2

∈
(w(cid:62)V −1
z V −1
t−1w)2
A(cid:62)
t−1w
(cid:107)
(cid:107)
w(cid:62)V −1
z V −1
2
A(cid:62)
t−1w
t−1w
(cid:107)
(cid:107)
z V −1
w(cid:62)V −1
t−1Az(1m + A(cid:62)
2
w
V −1
t−1

) max
z∈Y

max
z∈Y



2α(

) max
z∈Y

Y

≤ −

log

1

−

w

2
V −1
(cid:107)
t−1

(cid:107)
2
(Vt−1|Az)−1
(cid:107)







log



w
(cid:107)
It(z; w)

= 2α(

) max
z∈Y

Y

= 2α(

) max
z∈Y

Y

t−1Az)−1A(cid:62)

z V −1

t−1w

(cid:107)

(cid:107)
z V −1
w(cid:62)V −1
t−1Az(1m + A(cid:62)
2
w
V −1
t−1

(cid:107)

(cid:107)

t−1Az)−1A(cid:62)

z V −1

t−1w





This completes the proof.

Deﬁne the most uncertain direction in the set of plausible maximisers,

wt =

arg max
w∈{w=x−y:x,y∈Pt} (cid:107)

w

2
V −1
(cid:107)
t−1

.

(10)

Our next results extends the regret bounds to the variant of IDS that uses It(x, wt) as information
function. Note that the information processing inequality (Lemma 14) implies that (cid:80)n
t=1 It(xt; wt)
≤
(cid:80)n
t=1 It(xt), and therefore the bound in Lemma 2 on the total information gain γn continues to hold.

Theorem 16 IDS, deﬁned with the directed information gain It(x; wt), achieves for any n
with probability at least 1
and Rn

1,
δ ))1/3(cid:1) on globally observable games,

δ )n)1/2(cid:1) on uniformly locally observable games.

δ, Rn
(cid:0)(α0βn(γn + log 1

(cid:0)n2/3(αβn(γn + log 1

≤ O

−

≥

≤ O

Proof The proof is the same as for Theorem 3 and Theorem 6, but uses the stronger inequality of
α(
Lemma 15 to bound maxx,y∈Pt (cid:107)
x

t) maxz∈Pt It(x; wt).

2
V −1
t−1 ≤

−

P

(cid:107)

y

Unlike for IDS deﬁned with It(x), the information gain It(x; wt) requires to compute the set of
. This can be done by computing δt(x) =
plausible maximizers
minθ∈Ct maxy∈X
. Note that the minimization over θ is on a convex
function and therefore can be solved efﬁciently.

: δt(x) = 0
}

{
for each x

t =
P
x, θ

∈ X

∈ X

−

x

y

(cid:105)

(cid:104)

B.3. Relation to the UCB algorithm

Kirschner and Krause (2018) refer to the algorithm that chooses xt = arg minx∈X Ψt(δx) as de-
terministic IDS. Optimizing over a deterministic action choice is computationally cheaper and suf-
ﬁcient to obtain ˜
(√n) regret on locally observable games as evident by Lemma 7. We draw a
O
connection to the UCB algorithm. For m = 1 and

1 we have

Ax

2
V −1
t−1 (cid:28)

(cid:107)

(cid:107)

It(x) = log(1 +

Ax
(cid:107)

(cid:107)

2
V −1
t−1

)

Ax

2
V −1
t−1

.

(cid:107)

≈ (cid:107)

22

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

(cid:107)

2
V −1
t−1

Ax
(cid:107)

and ˜∆t(x) = maxy

y, ˆθt−1
Deﬁne ˜It(x) =
.
The next lemma shows that in bandit games (Ax = x), deterministic IDS with ˜∆t and ˜It as gap es-
timate and information gain, is equivalent to the UCB algorithm.
x, ˆθt−1

be the UCB action.

x, ˆθt−1
(cid:104)

1/2
x
β
t−1(cid:107)

1/2
y
t−1(cid:107)

+β
(cid:105)

(cid:107)V −1

(cid:107)V −1

t−1−

+ β

(cid:105) −

t−1

x

(cid:104)

Lemma 17 For a bandit game, let xucb
Then,

t = arg maxx∈X (cid:104)

1/2
t−1(cid:107)

(cid:107)V −1

t−1

(cid:105)

(cid:16)

(cid:17)

xucb
t ∈

arg min
x∈X

˜∆t(x)2
˜It(x)

.

Proof A related result appears in (Wang et al., 2016, Lemma 2.1). The information-ratio of the
UCB action is

(cid:16)

(cid:104)

=

˜∆t(xucb
t
˜It(xucb
t

)2
)

xucb
t

, ˆθt−1

+ β

1/2
t−1(cid:107)

xucb
t (cid:107)V −1

(cid:105)

(cid:16)

xucb
t
(cid:104)
t−1 −
2
xucb
t (cid:107)
V −1
t−1

(cid:107)

, ˆθt−1

β

1/2
t−1(cid:107)

xucb
t (cid:107)V −1

t−1

(cid:105) −

(cid:17)(cid:17)2

= 4βt−1 .

Further, for any x

, maxy

y, ˆθt−1
(cid:104)

(cid:105)

+ β

1/2
t−1(cid:107)

y

x, ˆθt−1

+ β

t−1 ≥ (cid:104)

(cid:105)

1/2
t−1(cid:107)

, therefore

∈ X
(cid:16)

˜∆t(x)2
˜It(x) ≥

(cid:107)V −1
(cid:0)

(cid:104)

2
V −1
(cid:107)
t−1

(cid:107)V −1

t−1 −
x

(cid:107)

x, ˆθt−1
(cid:104)

(cid:105)

+ β

1/2
x
t−1(cid:107)

x, ˆθt−1

1/2
β
t−1(cid:107)

x

(cid:107)V −1

t−1

(cid:105) −

x

t−1

(cid:107)V −1
(cid:1)(cid:17)2

= 4βt−1 .

This shows that the UCB action minimizes the deterministic information ratio.

Appendix C. Applications and Extensions

We discuss applications and extensions. Note that we make use of the generalized setup (Appendix
A.1) where necessary. In this case

is deﬁned to contain indexes plausible actions.

(

)

P

C

⊂ I

C.1. Full Information

The full information setting is perhaps not the most interesting case to study in the stochastic setting,
because IDS reduces to the naive algorithm that aggregates the information and always plays greedy.
Nevertheless, we demonstrate that the regret bounds improve given the additional information. Two
natural settings are Ax = 1d and Ax = X where X = (x
) collects the actions as columns.
In games where the information gain does not dependent on the action, IDS simply picks a regret
(√dn), which
minimizing action, xt = arg minx∈X ∆t(x). We show that IDS achieves Rn
improves a factor √d compared to the bandit setting. For simplicity, let Ax = 1d and therefore
Vt = (t + 1)1d. The information gain is

∈ X

˜
O

≤

It(x) = log det(1d + (t + 1)−11d) = d log(1 + 1

t ) .

Hence γn = d log(n), but the ratio for the greedy action ˆx∗

t is

Ψt(ˆx∗
t )

≤

βt maxx,y

y

x
−
(cid:107)
(cid:107)
d log(1 + 1
t )

2
V −1
t−1

βtd−1 .

≈

23

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

d log(n), this means the overall bound is Rn

(√dn). The same holds true for
Given that βn
the directed information gain It(x; wt). Interestingly, here the improvements stem from a reduced
total information gain γn

≤
log(n), and the ratio remains Ψt(x∗
t )

d log(t).

βt

≈

˜
O

≈

≈

≈

C.2. Transductive Bandits

S ⊂

V ⊂

Rd for exploration and a set of actions

In the transductive bandit setting (Fiez et al., 2019) the learner has access to a set of informative
Rd that, when played, return reward. The
actions
sets are allowed to overlap or be contained in the other. In the original formulation the objective
is to minimize the simple regret of a ﬁnal recommendation on the target set
by choosing actions
. When the objective is to minimize cumulative regret, we can model this setting as
only from
(0, x) : x
,
a partial monitoring game by deﬁning action-observation tuples
∈ S \ V}
, corresponding to informative actions
G
with zero reward, actions that return reward but no information, and actions with the usual bandit
, the game
information. The game is deﬁned by
G
can be either locally observable or globally observable (or even infeasible).

3. Depending on the sets

(x, x) : x

∈ S ∩ V}

(x, 0) : x

∈ V \ S}

1 =

2 =

3 =

∪ G

∪ G

and

and

1
G

=

V

V

S

S

G

G

{

{

{

2

C.3. Starved Bandits

In the starved bandit setting (Bubeck et al., 2018) the learner only receives information if the action
0 be a ground set of actions that, when played,
is sampled from a predeﬁned distribution. Let
yield no information (Ax = 0). Denote by ν
) the distribution that the learner can use for
ν is a sample from the distribution in round t. The starved bandit setting is
exploration and zt
closely related to the contextual partial monitoring game with
added
to the set of action-observation tuples. This game is globally observable if the distribution ν is
sufﬁciently diverse such that the samples (zt)n
.
∈ X }
(√n) as shown by Bubeck et al. (2018) (also
Note that on a curved actions set, the rate can still be
compare our results on curved action sets in Section 2.4).

t=1 span the set of differences

(zt, Azt = zt)

y : x, y

X
P(

∪ {

0
G

O

∼

=

−

X

∈

G

x

{

}

C.4. Batch Setting

In the batch setting, the learner commits to choosing B actions before observing the associated
outcomes. This is important for applications where querying the objective for a number of actions
in parallel is cheaper (or faster) than obtaining individual evaluations. This setting can be naturally
Rd be
formulated as a combinatorial partial monitoring game with semi-bandit feedback. Let
B
a ground set of actions. The learner chooses a batch (x1, . . . , xB)
0 . In the special case of a
and the observation operator is Ax1,...,xB =
bandit feedback game, the reward is
(x1, . . . , xB). With general feedback matrices, the batch game is

x1 +
(cid:104)

+ xB, θ

∈ X

0
X

· · ·

⊂

(cid:105)

(cid:40)(cid:32) B
(cid:88)

i=1

=

G

xi, (cid:0)Ax1, . . . , AxB

(cid:1)

: (x1, . . . , xB)

(cid:33)

(cid:41)

.

B
0
∈ X

4B2 (see Lemma 13). The disadvantage
The bandit batch game is locally observable with α0
of this formulation is, however, that the action space is exponentially large. Finding an efﬁcient
approximation of the IDS distribution is an interesting direction for future work.

≤

24

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

C.5. Dueling Bandits with Average Reward

Let
⊂
game with index set

0
X

=

0

X

× X

0:

I

Rd be a ground set of actions. The dueling bandit with average reward is the following

(cid:26)(cid:18) x1 + x2

(cid:19)

=

G

, x1

x2

: (x1, x2)

2

−

(cid:27)

.

∈ I

In words, the learner can pick any pair of actions x1, x2
∈ X
x2)(cid:62)θ/2 and a noisy observation of the reward difference (x1
also choose (x1, x1) with reward x(cid:62)
1 θ and no observation. Let
The ﬁrst observation is that if (x1, x2)
(x1 + x2)/2
(
C
∈ P
be two plausible actions. We can choose a path (z1, . . . , zl) with z1 = x1, zl = y1 and (zi, zi+1)
∈

0, obtains the average reward (x1 +
x2)(cid:62)θ. Note that the learner can
−
) be a plausible set of actions.
(
C
P
), because
) and (x2, x2)
(
C
∈ P
)

) then (x1, x1)
[x1, x2] lays on the line segment between x1 and x2. Let (x1, x2), (y1, y2)

∈ P

∈ P

(
C

(
C

∈

). Therefore we can write

(
C

P

x1

y1 =

−

xi

xi+1 =

−

A(xi,xi+1) .

l−1
(cid:88)

i=1

l−1
(cid:88)

i=1

The difference x2

y2 can be written similarly, which shows that x1+x2

y1+y2

2 −

2 ∈

span(Ai : i

∈

)). This shows that the game is locally observable. Turning to the local alignment constant

(
C

P

−

α(

−
−
Using Lemma 13 and the path construction above we can bound α

max
(x1,x2),(y1,y2)∈P

) = max
v∈Rd

min
(z1,z2)∈P

P

(cid:107)

(cid:104)

(x1 + x2)/2
(z1

(y1 + y2)/2, v
2
z2)(cid:62)v

(cid:105)

2

.

(cid:107)
l or α

≤

Cd.

≤

Tightening the Alignment Constant Deﬁne the sets

) =

) =

1(
2(

C

C

Q

Q

i

i

{

{

∈ I

∈ I

: xi
∈
: ∆C(xi)

conv(xj : j
max
j∈P(C)

≤

(
))
∈ P
C
}
∆C(xj)
}

with the regret estimate ∆C(x) = maxθ∈C maxj∈P(C)(xj
function which implies that
observation is that in locally observable games, we can play actions in
the regret bound. Consequently, the local alignment constant can be tightened to

x)(cid:62)θ. Note that ∆C(x) is a convex
), but equality is not true in general. The
t) without worsening

1(
C

2(
C

⊂ Q

⊂ Q

(
C

2(

Q

−

P

P

)

)

¯α(

) = max
v∈Rd

max
i,j∈P(C)

min
k∈Q2(C)

(cid:104)

C

2
(cid:105)

.

xi

xj, v
−
A(cid:62)
2
k v
(cid:107)

)

Clearly, ¯α(
) and therefore (x1 + y1)/2
game with average reward, recall that x1, y1
the same holds true for x2, y2. This means we can now choose A(x1,y1) = x1
x2

(cid:107)
) and all regret bounds hold true with α replaced by ¯α. For the dueling bandit
conv(x1, y1), and
y1 and A(x2,y2) =

y2 as a response to estimate along the direction (x1 + x2)/2

(y1 + y2)/2. We then write

α(
C

∈
−

∈ P

(
C

≤

C

−

x1 + x2
2

y1 + y2
2

−

1
2

=

A(x1,y1) +

A(x2,y2) ,

−

1
2

and therefore, using the argument of Lemma 13, ¯α(
C

)

≤

1.

25

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

C.6. Partial Monitoring in Reproducing Kernel Hilbert Spaces

0

X

× X

R. Let

∈ X
0
X

The kernelized setting is a practically relevant extension of the linear setting, where the feature
0 be a ground set of actions, not to be confused with the features.
dimension can be inﬁnite. Let
This is often a subset of Rd but can be deﬁned on other structures (e.g. graphs) as well. The actions
x
0 exhibit a non-linear dependence on the features through a positive-deﬁnite kernel map
k :
be the reproducing kernel Hilbert space (RKHS) corresponding to the
→
given kernel k and Hilbert norm
0, so we denote
represent functions over
the unknown parameter by f
(instead of θ). The standard boundedness assumption is that
the unknown function has bounded Hilbert norm
kx, f
(cid:105)
(cid:104)
H
associated to the actions is
the regret is

f
∈
(cid:107)
according to the reproducing property and the set of kernel features
. The best action is x∗ = arg maxx∈X0 f (x), and

1. The kernel features kx = k(x,

satisfy f (x) =

H. Vectors in

kx : x
{

0
∈ X

∈ H

(cid:107) · (cid:107)

)
·

H

H

≤

=

X

X

(cid:107)

}

H

Rn =

f (x∗)

f (xt) .

−

n
(cid:88)

t=1

The linear observation functions are linear operators Ax :
when choosing xt are at = Axtf + (cid:15)t. The regularized kernel least squares estimator is
t
(cid:88)

H →

Rm. As before, the observations

(11)

ˆft = arg min

f ∈H

(cid:107)

s=1

Axsf

as

(cid:107)

−

2 +

2
H .

f

(cid:107)

(cid:107)

(cid:105)

(cid:104)

y ∈

kx, ky

= k(x, y) evaluated on observed data points.

In the bandit setting, the kernel trick allows to express all quantities of interest in terms of the
inner product
In the general case where
observations are generated from the observation operators Ax, we will need a slightly stronger
x : Rm
assumption. Denote the adjoint map of Ax by A∗
. The requirement is that the matrix
Rm×m and the vectors kxA∗
Mx,y = AxA∗
0 (the
y ∈
theory also holds without the assumption, but it is needed to implement the algorithm if the feature
dimension is inﬁnite). We detail such a computation in examples below. By (a slight modiﬁcation
of) the representer theorem, we can write the solution to (11) as ˆft = (cid:80)t
xsϕs for weights
t )(cid:62), Kt
ϕs
Rmt×mt the kernel matrix that collects the matrices (Kt)ij = Mxi,yi and kt(x)
vector kt(x) = (kxA∗
evaluated at x
0 is

1 , . . . , a(cid:62)
∈
Rmt the evaluation
to the least squares problem

Rmt the vector that collects all observations at = (a(cid:62)

Rm can be computed for any x, y

xt)(cid:62). The solution ˆft(x) =

∈
x1, . . . , kxA∗

Rm. Denote at

s=1 A∗

kx, ˆft

→ H

∈ X

∈

∈

(cid:104)

(cid:105)

∈ X

ˆft(x) = kt(x)(cid:62)(Kt + 1mt)−1at .

The estimate corresponds to the posterior mean of a Gaussian process (GP) model with kernel k and
Gaussian likelihood (c.f. Kanagawa et al., 2018). The gap estimate at time t + 1 is deﬁned as
ˆft(x) + βt

σt(x)2 + σt(y)2

2kt(x, y) ,

ˆft(y)

(cid:112)

∆t+1(x) = max
y∈X0

−

−

where

kt(x)(cid:62)(Kt + 1mt)−1kt(y) ,

kt(x, y) = k(x, y)
σt(x) = (cid:112)
(cid:113)

−

kt(x, x) ,

1/2
t =

β

log det(Kt + 1mt) + 2 log 1

δ + 1 .

26

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

The estimate is chosen such that with probability at least 1
x

1 (Abbasi-Yadkori, 2012, Theorem 3.11).

and t

To compute the information gain, deﬁne Mt(x) = (M (cid:62)

∈ X

≥

δ, f (x∗)

f (x)

−
x,x1, . . . , M (cid:62)

−
x,xt)(cid:62)

∆t(x) for any

≤
Rmt×m. The ker-

nelized information gain (4) is given by
(cid:16)

It(x) = log det

1m + Mx,x

Mt(x)(cid:62)K−1

t Mt(x)

(cid:17)

.

−
Denote by kt|Az and σt|Az the uncertainty estimates that are (tentatively) updated with an ob-
servation generated from Az. Such an update does not require the observation outcome yt, similar
to the linear case, where we can update the precision matrix Vt
z . Further, let
wx,y = kx
ky be the difference of kernel features for the gap difference that we want to estimate.
The kernelized directed information gain is

Az = Vt + AzA(cid:62)
|

−

∈

It(z; wx,y) = log

(cid:18)

σt(x)2 + σt(y)2
−
σt|Az (x)2 + σt|Az (y)2

2kt(x, y)
2kt|Az (x, y)

(cid:19)

.

−

It(z). The
As before the information processing inequality (Lemma 14) implies that It(z; wx,y)
bound in Lemma 2 on the total information gain γn = (cid:80)n
t=1 It(xt) = log det(Kn + 1mn) for
ﬁnite feature dimension can be replaced by bounds that depend on the eigenspectrum of the kernel
(log(n)d+1) for the squared-exponential kernel on Rd.
(Srinivas et al., 2010), for example γn =
We remark that in the kernelized setting, only the computation of the estimator and information
gain are different compared to the linear setting. The regret analysis remains the same with the
appropriate constants βn and γn , deﬁned above. We therefore summarize our result:
(cid:0)n2/3(αβn(γn +log 1

O

≤

δ ))1/3(cid:1) on glob-
≤ O
δ )n)1/2(cid:1) on uniformly locally observable

Corollary 18 The kernelized variant of IDS achieves Rn
(cid:0)(α0βn(γn + log 1
ally observable games and Rn
games for any n

1 with probability at least 1

≤ O

δ.

≥

−
Example: Kernelized Dueling Bandits We illustrate a dueling bandit setting, where the learner
chooses two actions (x, x(cid:48))
f (x(cid:48)). In the partial
∈ X
monitoring formulation, the observation operator is Ax,x(cid:48) = kx
kx(cid:48), which means that the learner
−
f (x(cid:48)) up to noise. The learner obtains the reward of the ﬁrst action
observes
(other reward models are possible), so the set of action-observation tuples is
(cid:0)kx, Ax,x(cid:48) = kx

2
0 and observes binary feedback on f (x)

(cid:1) : (x, x(cid:48))

= f (x)

kx(cid:48), f

kx(cid:48)

kx

=

−

−

≥

(cid:104)

(cid:105)

0

.

G

{

The noise on the observation at = f (xt)
−
P[at =
f (xt)
−
−
required to compute the estimator are

t) (i.e. P[at = 1] = 1

f (x(cid:48)

−

0
∈ X

× X
t) + (cid:15)t is such that at

−
f (x(cid:48)
1] = (1 + f (x)

}

and E[at] =
1, 1
}
f (x(cid:48)))/2). The quantities that are

∈ {−

−

M(x,x(cid:48)),(z,z(cid:48)) = (kx

kx(cid:48))(kz

kz(cid:48))∗ = k(x, z)

k(x, z(cid:48))

k(x(cid:48), z) + k(x(cid:48), z(cid:48)) ,

kxA∗

(z,z(cid:48)) = kx(kz + k(cid:48)

−

−
z)∗ = k(x, z)

−
k(x, z(cid:48)) .

−

−
Kernelized dueling bandits have been studied in the literature (Gonz´alez et al., 2017; Sui et al.,
2017a, 2018) as well as extensions with multi-point comparisons (Sui et al., 2017b). Assuming
that the learner can compare any pair of actions (x, x(cid:48)), the setting is locally-observable by nature
(√βnγnn) regret bound. The same holds true for the
with α
deterministic variant that simply chooses the action which minimizes the information ratio.

1. Therefore, IDS achieves a ˜
O

≤

27

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Example: Bayesian Optimization with Gradients While Bayesian optimization (or kernelized
bandits) is typically phrased for the noisy, zero-order oracle, previous work also incorporates gra-
dient information where it is available (Wu et al., 2017b). We illustrate a setting where the learner
Rd be
only observes the gradient, which can be understood as a type of dueling bandit. Let
a compact, connected domain and f :
with a kernel that guarantees
H
x acts linearly on the function f and
that any f
is continuously differentiable. The gradient
Rd with m = d. The key step is
therefore is a valid choice for the observation operator Ax :
to compute the quantities required for the estimation,

Rd be an element in

∇
H →

∈ H

0
X

→

⊂

X

0

(AxA∗

y)ij =

ei, AxA∗
(cid:104)

yej

(cid:105)

= (

x

∇

(cid:104)

Aykx, ej

)i =
(cid:105)

∂
∂yi

∂
∂xj

k(x, y) ,

(kxA∗

y)i =

kx, A∗

yei

=

(cid:104)

(cid:105)

(cid:104)∇

(cid:105)

ykx, ei

=

k(x, y) .

∂
∂yi

The game where the learner observes only the gradient is globally observable, which means that for
all x, y
0 be a differentiable
∈
path with τ (0) = x and τ (1) = y. We claim that

0). To see this, let τ : [0, 1]

span(A∗

x : x

→ X

0, kx

∈ X

∈ X

ky

−

This is veriﬁed, because for any f

by the fundamental theorem of calculus,

kx

ky =

−

(cid:90) 1

0

A∗

α(t) ˙α(t)dt .

∈ H

A∗

α(t) ˙α(t)dt, f

(cid:28)(cid:90) 1

0

(cid:29)

(cid:90) 1

=

=

=

0 (cid:104)
(cid:90) 1

0 (cid:104)
(cid:90) 1

A∗

α(t) ˙α(t), f

dt

(cid:105)

˙α(t), Axf

dt
(cid:105)

0 (cid:104)
= f (x)

˙α(t),

xf

dt
∇
(cid:105)
f (y) =

−

kx
(cid:104)

−

ky, f

.

(cid:105)

If the learner observers both the function value and the gradient, the game is locally observable.

−

∈ X

0 = [

Example: Invasive Laser Alignment Consider a simplistic setup, where an experimenter wishes
1, 1]2 that correspond
to align a laser on a squared target using two parameters (x1, x2)
to a vertical and horizontal shift of the device (see Figure 1 for an illustration). The power of the
laser on a two-dimensional plane is given by an (initially) unknown function f : R2
R2. In
→
0.5)2)(cid:1). The objec-
the illustrated example it is set to f (z1, z2) = exp (cid:0)
tive is to ﬁnd a parameter setting that maximize the integrated intensity on the (e.g. 1
1) target,
It(x1, x2) = (cid:82) x1+1
f (z1, z2)dz1dz2. At any step, the experimenter can choose to evaluate a
setting (x1, x2) and observes the corresponding intensity It(x1, x2) up to noise. Since the intensity
is the reward, this action has standard bandit feedback. Alternatively, the experimenter can drive
m-grid G(x1, x2) centered at
a screen into the laser beam to measure the laser power on a m
×
(x1, x2), which yields m2 noisy measurements (f (z1, z2) : (z1, z2)
G(x1, x2)), possibly at a
lower noise level than the integrated intensity measurement. As the screen blocks of the beam, there
is no reward in such rounds (hence the term ‘invasive measurement‘). The learner therefore has the
choice between a direct measurement of the objective and a more informative action that yields no

0.5)2 + (z2

(cid:82) x2+1
x2

((z1

−

−

×

−

∈

x1

28

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Figure 1: A demonstration of the stylized laser example. The left plot shows the energy of the laser
on the two dimensional plane. The objective is to shift the square target such that the
integrated intensity within the square is maximized. The learner chooses to either ob-
serve a noisy measurement of the intensity, or alternatively, the energy function directly,
evaluated on a measurement grid within the square (invasive measurements). The latter
feedback is obtained from a screen that is put in the line of the laser, which blocks the
beam and voids the reward signal. In the second variant (transductive measurements), the
learner obtains information only through the grid measurements. To solve the task, the
learner needs to estimate the function and ‘blindly’ move the target to the position with
maximum integrated intensity. The plots on the right show the regret of IDS (with di-
rected and undirected information gain) compared to the UCB algorithm. Note that UCB
never chooses the informative actions and therefore suffers linear regret on the second
task.

reward. Clearly, the game is locally observable as each action contains the bandit feedback. We
remark that the UCB algorithm never chooses the invasive measurements, because the UCB score
for these actions is always zero. On the other hand, IDS naturally trades of between the informative
actions and those that lead to reward. In a (transductive) variant of the setup, the signal can only be
observed through the invasive measurements and the integrated signal is not observed. In this case,
the game is globally, but not locally observable.

We present a numerical simulation of this setup in Figure 1. Our set

0 is discrete with 9 actions
corresponding to a unit shift in any direction (or no shift). We use 25-dimensional features computed
from a radial basis function kernel. In the setup where the reward signal can be observed directly,
1000 steps; but then IDS gains an advantage from choosing
UCB outperforms IDS for the ﬁrst
the more informative measurements from time to time. Without the direct reward observation, UCB
continues to play actions that yield the integrated reward, but no longer receives any information.
The parameter estimate is therefore never updated, and the UCB algorithm suffers linear regret. On
the other hand, IDS still achieves no-regret through trading off the informative measurements with
parameter settings that yield reward.

∼

X

29

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Appendix D. Convex Action Sets

The proof of Theorem 8 follows by using the curvature to bound the information ratio. We will
show the following:

Ψt(µt)

Cβt−1 max

≤

(cid:18) diam(
κ◦

)

X

, diam(

(cid:19)

)2

.

X

where C > 0 is a constant depending only on (Az : z
function hX (u) = supx∈X (cid:104)
and ˆx∗
bounding the regret in terms of the curvature.

). Recall the deﬁnition of the support
hX (u) = arg maxx∈X (cid:104)
x, u
,
(cid:105)
hX (ˆθt) is the greedy action. Before the proof of the theorem we need a simple lemma

. A simple calculation shows that

x, u
(cid:105)

t =

∈ X

∇

∇

Lemma 19 Suppose that 1/κ◦ = maxx∈X λmax(

2hX (x)). Then for any θ, θ(cid:48)

Rd,

∈

∇

hX (θ)

(cid:104)∇

− ∇

hX (θ(cid:48)), θ

(cid:105) ≤

2
(cid:107)

.

2
θ
(cid:107)
κ◦

θ(cid:48)
−
θ
(cid:107)

(cid:107)

θ(cid:48)
and η(cid:48) = θ(cid:48)/
Proof Abbreviate η = θ/
(cid:107)
(cid:107)
(cid:107)
hX (u). Using the deﬁnitions,
u) =
implies that
∇

hX (c

∇

θ

·

. Note that for c > 0, hX (c
(cid:107)

·

u) = c

hX (u), which

·

hX (θ)

(cid:104)∇

− ∇

hX (θ(cid:48)), θ

hX (η(cid:48)), θ

hX (η)

(cid:104)∇
θ

(cid:107)

(cid:107) (cid:104)∇

− ∇
hX (η)

− ∇

(cid:105)

=

=

(i)

(cid:105)
hX (η(cid:48)), η

(cid:105)
hX (η(cid:48)), η

η(cid:48)

(cid:105)

−

θ

(cid:107)

(cid:107)

− ∇
2

(cid:107) (cid:104)∇
η

θ
≤ (cid:107)
(ii)

≤ (cid:107)
(iii)
2

hX (η)
η(cid:48)
−
κ◦
(cid:107)
θ(cid:48)
θ
(cid:107)
−
κ◦
θ
(cid:107)
hX (η(cid:48)) = arg maxx∈X (cid:104)

(cid:107)
(cid:107)

≤

2

,

where inequality (i) follows because
follows from the deﬁnition of κ◦ and because

∇

x, η(cid:48)

. The second inequality (ii)
(cid:105)

hX (η)

(cid:104)∇

− ∇

hX (η(cid:48)), η

η(cid:48)

(cid:105)

−

=

η
0 (cid:107)

−

η(cid:48)

2
∇2hX ((1−t)η+tη(cid:48))dt
(cid:107)

≤

(cid:90) 1

η
(cid:107)

2
(cid:107)

.

η(cid:48)
−
κ◦

The last inequality (iii) follows from the following geometric inequality:

Rd

x, y

∀

∈

(cid:13)
(cid:13)
(cid:13)x

−

(cid:107)x(cid:107)
(cid:107)y(cid:107) y

(cid:13)
2
(cid:13)
(cid:13)

2

x

(cid:107)

−

≤

y

2 .
(cid:107)

Proof of Theorem 8 Let ˆx∗

t =

hX (ˆθt−1). Then, by Lemma 19,

∇

∆t(x∗
t )

max
y
y∈X ,φ∈Ct−1(cid:104)

−

≤

x, φ

(cid:105) ≤

max
φ∈Ct−1

2
(cid:107)

ˆθt−1
φ
−
φ
κ◦
(cid:107)

(cid:107)

2

(cid:107)

.

30

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Let zt = arg maxz∈X It(z). By the assumption that
observability, it follows that span(Az : z
) such that
depending only on (Az : z

∈ X

spans Rd and the deﬁnition of global
) = Rd, which means there exists a constant c

X

where the second inequality follows from the same argument as in Lemma 4. Hence,

∈ X
λmax(V −1
t−1)

cλmax(A(cid:62)

ztV −1

t−1Azt)

cIt(zt) ,

≤

diam(

t−1)2 = max

C

φ

2
(cid:107)
−
λmax(V −1
t−1)

θ
θ,φ∈Ct−1 (cid:107)
max
θ,φ∈Ct−1
λmax(V −1
cβt−1It(zt) .

t−1)βt−1

θ
(cid:107)

−

φ

2
V −1
(cid:107)
t−1

≤

≤

≤

≤

The analysis of the information ratio is decomposed into two cases. The ﬁrst case is when
t−1 has
a large diameter, in which case the information ratio is well controlled without using curvature, and
by only exploration. Suppose that

C

(cid:32)

(cid:115)

2 max

1,

(cid:33)

1

κ◦ diam(

)

X

diam(

t−1)

C

max
φ∈Ct−1 (cid:107)

φ

(cid:107)

≥

(12)

Then, using Cauchy–Schwarz inequality and the deﬁnition of ∆t,

∆t(zt)2

diam(

)2 max

X
(cid:18)

φ∈Ct−1 (cid:107)
1

2

φ

(cid:107)

(cid:19)

4 max

1,

κ◦ diam(
(cid:18)

X

)
1

(cid:19)

κ◦ diam(

)

X

diam(

)2 diam(
C

X

t−1)2

diam(

)2It(zt) ,

X

4cβt−1 max

1,

≤

≤

≤

which implies that

Ψt(zt)

4cβt−1 max

1,

≤

(cid:18)

1

(cid:19)

κ◦ diam(

)

diam(

)2 .

X

X
Moving to the second case where Eq. (12) does not hold. Let

p =

2 diam(
κ◦ minφ∈Ct−1 (cid:107)
φ

t−1)2
C
maxφ∈Ct−1 (cid:107)
φ

(cid:107)

.

diam(

)

X

(cid:107)
[0, 1] follows by virtue of the fact that

That p

∈

min
φ∈Ct−1 (cid:107)

φ

(cid:107) ≥

max
φ∈Ct−1 (cid:107)

φ

(cid:107) −

diam(

t−1)

C

≥

max
φ∈Ct−1 (cid:107)

φ

(cid:107)

.

1
2

31

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Hence, µ = (1

). Using that ˆx∗

t =

hX (ˆθt−1) and Lemma 19,

p)δˆx∗

t

−
∆t(µ)

+ pδzt ∈
p diam(

≤

P(

X

) max

φ

+ max

X

φ∈Ct−1 (cid:107)

(cid:107)

φ∈Ct−1(cid:104)∇

hX (ˆθt−1), φ
(cid:105)

− ∇

∇
hX (φ)

p diam(

) max

φ

+

X

φ∈Ct−1 (cid:107)

(cid:107)

≤

p diam(

) max

≤

φ∈Ct−1 (cid:107)
X
t−1)2
4 diam(
κ◦ minφ∈Ct−1 (cid:107)
φ
Therefore, using the fact that cβt−1It(µ)

=

C

(cid:107)

.

+

φ

(cid:107)

2

φ

(cid:107)

(cid:107)

2
κ◦

ˆθt−1
max
−
φ
φ∈Ct−1
(cid:107)
(cid:107)
t−1)2
2 diam(
κ◦ minφ∈Ct−1 (cid:107)
φ

C

(cid:107)

∆t(µ)2
It(µ) ≤

p diam(
C

t−1)2,

cβt−1pIt(zt)

≥

≥
t−1)2
16cβt−1 diam(
C
2
pκ2
◦ minφ∈Ct−1 (cid:107)
φ
) maxφ∈Ct−1 (cid:107)
8cβt−1 diam(
X
κ◦ minφ∈Ct−1 (cid:107)
φ
(cid:107)
X

16cβt−1 diam(

(cid:107)

)

.

κ◦

=

≤

φ

(cid:107)

Combining the two parts shows that

Ψt(µt)

cβt−1 max

≤

κ◦

(cid:18) 16 diam(

)

(cid:18)

1

(cid:19)

X

, 4 max

1,

(cid:19)

)2

.

diam(

X

κ◦ diam(

)

X

With the bound on the information ratio and Lemma 1, the proof of Theorem 8 follows now imme-
diately.

Appendix E. Contextual Partial Monitoring

E.1. Conditional IDS for Contextual Games

Conditional IDS optimizes the sampling distribution for the given context zt,

µt = arg min

Ψt(µ; zt) .

µ

The computational complexity required to ﬁnd the minimizer of the information ratio is the same
as in the non-contextual case. We extend the notion of the alignment-constant with the contextual
argument,

α(
C

, z) = max
v∈Rd

max
x,y∈P(C,z)

min
u∈P(C,z)

y, v
x
−
Az(cid:62)
u v

(cid:104)
(cid:107)

2
2 ,
(cid:105)
(cid:107)

(13)

x

P

θ∈C

(
C

, z) =

z :
where
context z. For globally observable games, we denote α(z) = α(Rd, z).

maxy∈Xz (cid:104)
The next result is an immediate upper bound for the regret of conditional IDS under the assump-
z) is globally or locally observable, respectively.

is the plausible maximizer set for

tion that for any context z

, each game (

x, θ
(cid:104)

∈ X

(cid:105) ≥

y, θ

(cid:105)}

z,

∪

{

∈ Z

X

A

32

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Corollary 20 If a contextual game is globally observable in the sense that for any context z, the
z) is globally observable with uniformly bounded alignment constant α(z)
game (
α, then
X
1 with probability at least 1
for any n

δ, conditional IDS achieves

≤

z,

A
≥

−

Cn2/3 (cid:0)αβn(γn + log 1

δ )(cid:1)1/3 + 4 log

Rn

≤

(cid:19)

(cid:18) 4n + 4
δ

.

If the contextual game is locally observable in the sense that for any z
locally observable with uniformly bounded alignment constant α(
; z)
C
1 with probability at least 1
then for any n

δ, conditional IDS achieves

∈ Z
≤

, the game (
X
α0 for all convex

z) is
Rd,

z,

A
C ⊂

≥

(cid:113)

Rn

C

nα0βn

≤

−
(cid:0)γn + log 1

δ

(cid:1) + 4 log

(cid:19)

(cid:18) 4n + 4
δ

,

where C is a universal constant.

The corollary follows along the lines of our main results, Theorem 3 & 6. The assumptions of
Corollary 20 imply that the information ratio is bounded for any context z in the respective regimes.
One can achieve a slightly stronger result by replacing the alignment constant α0 with the average
observed alignment 1
t; zt). In this case the bound explicitly depends on the sequence of
n
observed contexts (zt)n
t=1, which can lead to improved bounds in
benign cases.

(cid:80)n
t=1 and the conﬁdence sets (

t=1 α(

t)n

C

C

E.2. Regret Bounds for Contextual IDS

In this section we summarize results for contextual IDS, which minimizes

ξt =

arg min
ξ∈P(X ×Z), ξz=ν

Ψ(ξ) ,

where ν
Prokhorov’s theorem guarantees the existence of a minimizer (cf. Kirschner and Krause, 2018).

) is a known distribution over the set of contexts. For general compact

Z

∈

X × Z

,

P(

We overload the notation and let

z∈Z
t; z) the joined set of plausible maximisers. For a function g :

z be the joint action space over all contexts and
and a vector

×

X

X

t =

=

R|X | by (xg)z = xg(z). The expected alignment constant is deﬁned as

Z → Z

(
z∈Z
×
P
, we deﬁne xg

C

P
x

∈ X

∈

α(
C

, ν) = max
v∈(Rd)Z

max
x,y∈P(C)

min
u∈P(C)

min
g:Z→Z

Eν

(cid:104)
(cid:104)

v, x

y

(cid:105)

−

1/2(cid:105)2

(cid:20)

Eν

(cid:21)−1

Auvg

(cid:107)
vg, xg
(cid:104)

2
(cid:107)
yg
−

(cid:105)

.

(14)

As before, this corresponds to the signal-to-noise ratio that can be achieved by choosing the best
z
u in context z, with the additional twist that learner can choose to
aligned observation operator
yz(cid:48) of a different context z(cid:48) = g(z). In the next lemma, we show that
estimate along a direction xz(cid:48)
the deﬁnition satisﬁes more intuitive upper bounds. We will see that in the ﬁnite case, the deﬁnition
of the alignment constant relates to natural conditions for local and global observability.

A
−

, ν) be the expected alignment (14) and α(

, z) the conditional alignment (13).

Lemma 21 Let α(
C
i) For any convex

C ⊂

Rd and distribution ν

P(

C
) it holds that,

∈

≤

Z
Ez∼ν[α(
C

; z)] .

α(

; ν)

C

33

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

ii) For ﬁnite context sets,

, ν)

α(
C

≤

max
z∈Z

max
x,y∈P(C;z),v∈Rd

min
z(cid:48)∈Z

min
u∈P(C,z(cid:48))

v, x
(cid:104)
ν(z(cid:48))
(cid:107)

2

y
−
(cid:105)
Az(cid:48)(cid:62)
u v

(cid:107)

2 .

iii) For Dirac-delta distributions ν = δz,

α(
C

, δz) = α(
C

, z) .

The ﬁrst inequality implies that the information ratio of contextual IDS is never worse than for
conditional IDS. The second inequality captures the intuition that for every direction x
z
∈ X
in a context z, there needs to be a context z(cid:48) that appears with positive probability ν(z(cid:48)) > 0 where
x
y can be estimated. The last inequality is a sanity check which shows that for a constant context,
we recover the previous deﬁnitions.
Proof For i), note that

−

−

y

α(

C

; ν) = max
v∈(Rd)Z

max
x,y∈P(C)

min
u∈P(C)

min
g:Z→Z

vz, xz
(cid:104)

−

yz

(cid:104)

Eν

max
v∈(Rd)Z

max
x,y∈P(C)

min
u∈P(C)

≤

max
x,y∈P(C)

min
u∈P(C)

Eν

max
v∈(Rd)Z

≤
= Eνα(
C

; z) .

(cid:104)

Eν

vz, xz
(cid:104)
(cid:20)

yz

1/2(cid:105)2
(cid:105)

−

Eν

2

(cid:21)

(cid:104)

vz, xz
Az

−
uz vz

(cid:107)

(cid:105)

yz
2
(cid:107)

(cid:34)

1/2(cid:105)2
(cid:105)

Eν

(cid:35)−1

2

Az
uz vg(z)(cid:107)
(cid:107)
vg(z), xg(z) −
(cid:21)−1

yg(z)(cid:105)

(cid:20)

(cid:104)
uz vz

Az
(cid:107)
vz, xz
(cid:104)

2
(cid:107)
yz
−

(cid:105)

The ﬁrst inequality follows by choosing the identity function g(z) = z. The second inequality uses
the fact that (a, b)

R>0 and Jensen’s inequality.

a2/b is convex on R

For ii), denote z∗ = z∗(v, x, y) = arg maxz∈Z (cid:104)

vz, xz

yz

(cid:105)

−

(cid:55)→

×

and deﬁne g(z) = z∗ for all z

Then

α(

C

; ν) = max
v∈(Rd)Z

max
x,y∈P(C)

min
u∈P(C)

min
g:Z→Z

Eν

(cid:104)
(cid:104)

vz, xz

yz

−

1/2(cid:105)2
(cid:105)

Eν

max
v∈(Rd)Z

max
x,y∈P(C)

min
u∈P(C)(cid:104)

≤

vz∗, xz∗

yz∗

2Eν

−

(cid:105)

(cid:2)
(cid:107)

Az

uz vz∗

(cid:107)

(cid:34)

(cid:104)
2(cid:3)−1

2

Az
uz vg(z)(cid:107)
(cid:107)
vg(z), xg(z) −

yg(z)(cid:105)

max
v∈(Rd)Z

max
x,y∈P(C)

min
z(cid:48)∈Z

min
u∈P(C;z(cid:48))(cid:104)

≤

vz∗, xz∗

yz∗

(cid:105)

−

2 (cid:16)

ν(z(cid:48))

Az(cid:48)
(cid:107)

uz(cid:48) vz∗

2(cid:17)−1
(cid:107)

,

.

∈ Z

(cid:35)−1

which proves the claim. We ﬁrst used the deﬁnition of z∗ and lower-bounded the expectation in the
last step. The last equality iii) is immediate.

Our next result extends Lemma 15 to account for the contextual distribution ν in the information
ratio. We provide the proof for the tighter information gain It(x, z; w) as deﬁned in (9). The
information processing inequality (Lemma 14) implies the result for It(x, z).
Rd let w

be a difference in the plausible

y : x, y

x

)

∈ {

−

(
C

}

∈ P

Lemma 22 For a convex set
action set

) =

z∈Z

(

C ⊂
; z). Then

C
w

P
Eν[
(cid:107)

×
(cid:107)V −1

t−1

]2

P

(
C
α(

≤

, ν) max
u∈P(C)

max
g:Z→Z

C

2It(u, ν; wg)

α(

, ν) max
u∈P(C)

C

≤

2It(u, ν) .

34

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Proof The proof is along the lines of Lemma 15, but keeps the expectation over ν. Let g :
be any function and u

. From the proof of the mentioned lemma, we ﬁnd

Z → Z

∈ X

Therefore, in expectation

2

Az(cid:62)
(cid:107)

t−1wg(z)(cid:107)

uz V −1
wg(z)(cid:107)V −1
(cid:107)

t−1

≤

2It(u, z; wg(z)) .

(cid:34)

Eν

(cid:107)

A(cid:62)

u V −1
wg

t−1wg
(cid:107)V −1

t−1

(cid:107)

(cid:35)

2
(cid:107)

≤

2It(u, ν; wg) .

Let u∗, g∗ = arg maxu∈P(C),g:Z→Z It(u, ν; wg). With this we ﬁnd

w

]2

Eν[
(cid:107)V −1
(cid:107)
It(u∗, ν; wg∗) ≤

t−1

2 min
u∈P(C)

min
g:Z→Z

Eν[

w
(cid:107)

(cid:107)V −1

t−1

]2Eν

(cid:35)−1

(cid:34)

(cid:107)

A(cid:62)

u V −1
wg

t−1wg
(cid:107)V −1

t−1

(cid:107)

2
(cid:107)

2 max
v∈X Z

≤
= 2α(
C

, ν)

Rearranging completes the proof.

max
x,y∈P(C)

min
u∈P(C)

min
g:Z→Z

Eν[
(cid:104)

v, x

y

1/2]2Eν
(cid:105)

−

2

(cid:21)−1

(cid:20)

u vg

A(cid:62)
(cid:107)
vg, xg

(cid:107)
yg
−

(cid:105)

(cid:104)

With these results the regret bounds for contextual IDS follow. For simplicity, the proof is given for
IDS with full information gain (4), but similar results can be obtained for the directed information
gain. First, the globally observable case (Theorem 10, Section 2.5).
Proof [of Theorem 10] Let x∗
Deﬁne the least accurate direction wt

be the greedy action for each context, x∗
t as

t (z) = arg maxx∈Xz x(cid:62) ˆθt.

in the set

t ∈ X

y : x, y

x

∈ {
wt(z) =

−
arg max
w=x−y:x,y∈Pt(z) (cid:107)

∈ X }
w

2
V −1
t−1

.

(cid:107)

P

(15)

Recall that x∗

t (z)(cid:62) ˆθt

wt(z)

. Lemma 22 implies

β

1/2
t−1(cid:107)
t ; ν)2

≤
∆t(x∗

t−1

(cid:107)V −1
βt−1Eν[
(cid:107)

≤

wt

(cid:107)V −1

t−1

]2

≤

2βt−1α(ν) max
u∈X

It(u; ν) .

The rest of argument is analogous to the proof of Lemma 4. Consider a sampling distribution
µ(p)
p), and the most informative action u(z) =
arg maxu∈Xz It(u, z) in context z with probability p. By deﬁnition of the IDS policy,

t (z) with probability (1

) that chooses x∗

∈ P

−

X

(

Ψt(ξt)

min
p∈[0,1]

≤

∆t(µ(p); ν)2
It(µ(p); ν)
((1

min
p∈[0,1]

≤

−

t ; ν) + p)2

p)∆t(x∗
pIt(µ(p); ν)

((1

−

p)∆t(x∗
p∆t(x∗

t ; ν) + p)2
t ; ν)

2αβt−1 min
p∈[0,1]

≤

4αβt−1

≤

∆t(x∗

t ; ν)2

35

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

We ﬁrst used that It(x∗
t )
that we derived above. Then we optimized over p
can show that IDS plays greedy most of the time. For any x

0, ∆t(u)

≥

≤

∈

1 and the inequality 2αβt−1 maxu It(u; ν)

t ; ν)2
[0, 1] in the last step. Similar to Lemma 5, one

∆t(x∗

≥

,

With this we ﬁnd

∈ X

∆t(µt; ν)

2∆(x; ν) .

≤

Ψt(ξt)

4αβt−1
∆t(x∗

t ; ν) ≤

8αβt−1
∆t(µt; ν)

.

≤

Invoking the general bound (Lemma 1) and balancing the terms completes the proof.

E.3. Locally Observable Contextual Games

The condition for locally observable games is that any difference in the plausible action set
for a context z
actions that appear plausible optimal in the context z(cid:48). Formally,

can be estimated under possibly different context z(cid:48)

∈ Z

∈ Z

, z)
by playing only

P

C

(

Rd convex, z

, x, y

; z)

t(
C

∀ C ⊂

∈ Z
If the condition holds true, Lemma 21 implies that α(
sets.

⇒ ∃

∈ P

∈ Z
, ν)

C

≤ O

z(cid:48)

s.t. x

y

span(Ax : x

∈

−
(cid:0)(minz∈Z ν(z))−1(cid:1) for ﬁnite action

∈ P

; z(cid:48))).

t(
C

Theorem 23 In contextual games that are uniformly locally observable in the sense that α(
C
Rd, the regret is bounded for any n
1 with probability at least 1
α0 for any convex set

, ν)
≤
δ,

−

C ⊂

(cid:113)

Rn

C

≤

α0βnn(γn + log 1

δ ) + 4 log

≥
(cid:18) 4n + 4
δ

(cid:19)

.

Proof Let wt be the least accurate direction in the current set of plausible maximisers deﬁned in
Eq. (15). For any plausible maximiser x

t it holds that

∈ P

∆t(x, c)

2β

1/2
t−1(cid:107)

≤

wt(c)

,

(cid:107)V −1

t−1

Therefore ∆t(x; ν)2

4βt−1Eν[

]2. By Lemma 22,

≤

wt
(cid:107)

(cid:107)V −1

t−1

∆t(x; ν)2

8βt−1α(
C

t; ν) max
z∈Pt

≤

It(z, ν) .

Finally, let zt = arg maxx∈Pt It(z, ν) be the most informative action that appears plausible optimal
for each context. This action has bounded information ratio:

where we also used that α(

t, ν)

α0 by assumption. The result follows from Lemma 1.

C

≤

Ψt(ξ)

∆(zt, ν)2
It(zt, ν) ≤

≤

8α0βt−1 ,

36

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Appendix F. Proof of the Classiﬁcation Theorem

The classiﬁcation theorem is proven by combining upper and lower bounds, carefully checking that
all cases have been covered. To begin, we introduce the classiﬁcation of actions that is now standard
in ﬁnite partial monitoring games. The lower bounds then follow using standard techniques and are
sketched in Appendix G.

C :
Notation We denote by relint(C) =
the
}
relative interior of a convex set C, and the set of extreme points by ext(C). The closure operator on
Rd, let
) and dim(
subsets of a metric space is cl(
·
·
[0, 1]
[x, y] =
.
}

) is the Hausdorff dimension. For points x, y

λ > 1 : λx + (1

tx + (1

t)y : t

y
∀

λ)y

−

−

C

C

∈

∈

∈

∈

∈

x

∃

{

{

Assumption 1 For the remainder of this section we assume that

is ﬁnite.

X

X

The set of Pareto optimal actions is the set of extreme points ext(conv(
. An action is degenerate if it is on the boundary of conv(

)) of the convex hull
), but not an extreme point. Actions
of
X
in the interior of conv(
) are called dominated. The situation is illustrated in Figure 2. Finite
partial monitoring games can be completely classiﬁed by considering a graph structure known as
, the cell of x is
the neighborhood graph (Lattimore and Szepesv´ari, 2019). Given an action x
the set of parameters for which action x is optimal:

∈ X

X

X

Cx =

θ
{

∈

Rd : x

(θ)

=

∈ P

}

θ
{

∈

Rd : max
y
y∈X (cid:104)

x, θ

= 0
}

.

(cid:105)

−

}

X

X

0
{

is ﬁnite, conv(

) is a polytope and Cx is either the singleton

Since
or an unbounded polytope.
An action x is Pareto optimal if dim(Cx) = d and degenerate otherwise, which can be seen by
observing that Cx is the normal cone of x with respect to the convex body conv(
). Pareto optimal
X
1, where the dimension of a polytope
actions x and y are called neighbours if dim(Cx
∩
is deﬁned as the dimension of the smallest afﬁne space containing it. The neighbourhood relation
deﬁnes a connected graph on the set of Pareto optimal actions. For neighboring actions x and y
xy =
xy contains only actions z with
. Note that, besides x and y,
let
}
dim(Cz) = d
1. Lin et al. (2014) and Chaudhuri and Tewari (2016) use a different notion to
ensure global observability and to construct an explicit exploration distribution. A global observer
set is a set of actions

such that span(Ax : x

z : Cx
{

) = span(x

Cy) = d

y : x, y

Cy

Cz

N

N

⊆

−

−

).

∩

Y ⊂ X

∈ Y

−

∈ X

Lemma 24 The following conditions equivalently characterize globally observable games:

i) For all actions x, y

it holds x

y

∈ X
ii) For all Pareto optimal actions x, y, it holds x

−

∈

Az : z
span
{

.

∈ X }

y

span
{

∈

−

Az : z

.

∈ X }

iii) There exists a global observer set.

Proof For the implication (ii
), therefore any x
conv(

X

iii) follows by taking
⇒
of a global observer set.

X

i), note that Pareto optimal actions are the extreme points of
⇒
can be written as a convex combination of Pareto optimal actions. (i
ii) immediately follows from the deﬁnition

∈ X
as global observer set. (iii

⇒

The next lemma shows the relation of neighboring actions and local observability.

37

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Figure 2: Green vectors are Pareto optimal, blue ones are degenerated and red are dominated. The
light blue cones are the normal cones associated with each Pareto optimal action indicat-
ing the direction of θ for which that action is optimal.

Lemma 25 Let

be ﬁnite and

Rd be any convex set. Then

X

C ⊂

i) The Pareto optimal actions within

P
ii) For two Pareto optimal actions x, y

(

) are connected on the neighborhood graph.

C

(
C

∈ P

) it holds that

xy

N

⊂ P

C

(

).

iii) Any x

) can be written as convex combination of Pareto optimal actions in

(

).

P

C

(
C

∈ P

Proof

∈

Cy. Then take the chord [θx, θy]

i) The proof is intuitively simple. Take any Pareto optimal actions x, y

) and let θx
(
Cx
C
and consider the path (xi)n
and θy
i=1 deﬁned by the
cells that intersect [θx, θy]
. There is a technicality that this chord may pass through
∅
2. A perturbation and dimension argument ﬁxes
intersections of cells that have dimension d
the proof (see Lemma 28 below). For a similar result see (Lattimore and Szepesv´ari, 2019,
Lemma 23).

Cxi (cid:54)

∈ P

⊂ C

−

=

∪

∈

) be Pareto optimal actions. Pick any θ

Cx

Cy

ii) Let x, y
Cy
Cx

(
Cz, hence θ

C

∈

∈

∩

∈ P
⊂

iii) Let F be the lowest dimensional face of conv(

∩
Cz and z is optimal for θ. Therefore z

∩ C
(
C
). Assume that x is in
X
the interior of F (otherwise it would be an extreme point and so Pareto optimal). Then let θ be
a parameter such that x is optimal. H =
z : (x
is a supporting hyperplane of
conv(F ). Hence F is a subset of H. Note that H
(θ) contains actions that are optimal
) and since x is in the convex hull of the
for θ. Therefore all extreme points of F are in
extreme points of F the result follows.

−
∩ X ⊂ P
P

) containing x

z)(cid:62)θ = 0

∈ P
(

∈ N

∈ P

C

C

}

{

(

xy, we have

. If z
).

The next lemma shows that observability can be characterized in terms of the neighborhood

relation.

38

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Lemma 26 The following conditions equivalently characterize locally observable games:

xy

.

∈ N

}
relint(Cx

i) For any convex

span
{
ii) For any two neighboring Pareto optimal actions x, y, x

and all x, y

∈ P

), x

−

∈

C

C

y

(

Az : z

(

∈ P

C
span
{

∈

.

)
}
Az : z

y

−

P

⇒

∈ X

N
“ii)

⇒
xy =

ii)”. Let x, y

i)”. Let x, y

Proof “i)
Then

(θ) by Lemma 25 and therefore x
(
C

∈ P
linear combination of Pareto optimal actions in
optimal. By Lemma 25, i), there exists a sequence (xi)m
and xm = y, such that xi, xi+1 are neighbors and
xi

be neighboring Pareto optimal actions. Pick θ
∈
)
(
by i).
}
C
∈ P
∈
). First note that by Lemma 25, iii), x
y can be written as
−
). Therefore we can assume that x, y are Pareto
i=1 of Pareto optimal actions with x1 = x
). By assumption,
(
C
xi+1 the claim follows.

xi : i = 1, . . . , m
i=1 xi

Ax : x
span
{

{
y = (cid:80)m−1

. Since x

Az : z

} ⊂ P

xi−1

(
C

Cy).

−

P

∩

y

−

span
{

∈

∈ N

xixi−1}

−

Lastly, the key lemma for proving the lower bound for globally observable games shows that
in games that are not locally observable, there exists a pair of neighbouring Pareto optimal actions
x, y and a parameter θ such that both actions x, y are optimal, but
can not be estimated
by playing only actions from the neighborhood

y, θ

−

xy.

x

(cid:104)

(cid:105)

−

N

Lemma 27 Suppose a game is not locally observable. Then there exists a pair x, y of neighbouring
Pareto optimal actions and θ

Cy) such that x

relint(Cx

Az : z

xy

y /
∈

span
{

−

∈ N

.
}

∈

∩

Proof The lemma follows from the deﬁnition of local observability its equivalent charaterization
provided in Lemma 26.

be a collection of disjoint open sets of Rd with the usual metric d(x, y) =

Lemma 28 Let
x

U
such that:

y

(cid:107)

(cid:107)

−
1. The union

2. dim (
3. A = (cid:83)

K ∩ {

tion with W .

We say that U1, U2
and y
Ui

∈
[x, y]

=

.

∩

∅

i=1 cl(Ui) is convex.

= (cid:83)∞

K
x : d(x, y)

K
cl(V )

∩

∩

(cid:15)
}

≤

⊂

) = dim(

) for all (cid:15) > 0 and y

U

.

∈

∈ U

U,V,W ∈U are distinct(cl(U )

cl(W )) has dim(A) < dim(

)

1.

K

−

4. For any compact set W

Rd, at most ﬁnitely many elements of

have non-empty intersec-

U

cl(V ), there exists a sequence (Ui)m

∈ U

are connected if (cl(U1)

cl(U2))

cl(U )
. Then, for any x
∅
i=1 of connected sets with U1 = U and Um = V and

=

A

∩

∈

\

∈

≤

⊂

cl(U ) and y

∈
B : [z, x]

V . Let B =
A

V and S =
z
{
dim(A) + 1 < dim(

Proof Suppose that x
small that B
dim(S)
By the last assumption, [x, z] intersects with at most ﬁnitely many elements of
the path between U and V . Next, suppose that y
limn→∞ d(yn, y) = 0. By the previous argument, for each n, there exists a sequence (U n
connected sets with U n

with (cid:15) sufﬁciently
z
{
. A straightforward calculation shows that
.
∅
, which form
cl(V ) and let (yn) be a sequence in V with
i=1 of
mn = V . By the fourth assumption, for suitably large n, there

=
∈
) = dim(B). Hence, there exists a z
K

: d(z, y) < (cid:15)
}

1 = U and U n

S and [x, z]

i )mn

A =

∈ K

∅}

B

∈

∩

∈

∩

U

\

39

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

are only ﬁnitely many sets in all the (U n
i )i,n and hence, by re-labelling if necessary, the sequence of
i )mn
connected sets can be chosen so that (U n
i=1 converges (in the sense that the identity/order of the
i=1.
sequences converges – the discrete topology on ﬁnite sequences of
We need to show that cl(Ui)
[x, y]
for each i. By the deﬁnition of convergence we have
[x, yn]
for all suitably large n. Taking a sequence (zn) with zn
Ui for all suitably
large n. Compactness again allows us to assume that (zn) converges to some z, which is easily seen
to lie on [x, z] and by closure of Ui also lies in Ui, as required.

) to some sequence (Ui)m

cl(Ui)

=

=

∩

∩

∈

U

∅

∅

Appendix G. Lower Bounds

The lower bounds complete the classiﬁcation theorem. These results are almost implied by exist-
ing theorems from ﬁnite partial monitoring. The only difference is that here the outcome space is
inﬁnite, which does not change the structure of the proofs. We include here the key details and in-
tuition. As expected, the key tool is Le Cam’s method in combination with the Bretagnolle–Huber
inequality (Bretagnolle and Huber, 1979) and an elementary calculation of the relative entropy be-
tween measures on interaction sequences induced by a ﬁxed policy and for different environments.
and feed-
For the remainder of this section, we ﬁx an arbitrary policy and ﬁnite game with actions
spans Rd. Given a
back functions (Ax)x∈X . For simplicity, we assume the noise is Gaussian and
θ be the measure on action/observation sequences of length n when the learner interacts
θ
with the game for parameter θ. Before the theorems and proofs we need a little more notation. Let

Rd let Pn

X

X

∈

Then deﬁne En(θ) as the binary random variable that the algorithm plays a suboptimal action at
least n/2 times.

Vn(θ) = Eθ[Vn] = Eθ

(cid:35)

AxtA(cid:62)
xt

.

(cid:34) n
(cid:88)

t=1

En(θ) = 1

1(xt /

(θ))

n/2

.

∈ P

≥

(cid:32) n
(cid:88)

t=1

(cid:33)

Notice that if θ, θ(cid:48) are such that
En(θ). For simplicity we
focus on proving lower bounds on the expected regret. The extension to high probability bounds is
possible using the techniques of Gerchinovitz and Lattimore (2016). Let

, then En(θ(cid:48))
∅

(θ(cid:48)) =

∩ P

(θ)

≥

−

P

1

Rn(θ) = Eθ[Rn]

be the expected regret when the learner interacts with the environment determined by θ.

Lemma 29 The relative entropy between Pn

θ and Pn

θ(cid:48) satisﬁes KL(Pn

θ , Pn

θ(cid:48)) =

(cid:13)
(cid:13)θ

1
2

−

θ(cid:48)(cid:13)
2
Vn(θ).
(cid:13)

For a proof refer to (Lattimore and Szepesv´ari, 2018, Theorem 24.1).

Lemma 30 (Bretagnolle-Huber inequality) Let P and Q be probability measures on the same mea-
surable space (F, Ω) and let A
F be an arbitrary event. Then

∈

P (A) + Q(Ac)

exp(

KL(P, Q)) .

−

(16)

1
2

≥

40

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

)

= Rd, then there exists a game-dependent constant

Theorem 31 Suppose that span(Ax : x
c > 0 such that for all n

∈ X
1 there exists a θ for which Rn(θ)
Rd be a non-zero vector such that Axθ = 0 for all x

cn.

≥

≥

Proof Let θ
∈
assumption that span(Ax : x

∈ X

)

= Rd. Next, let θ(cid:48) =

KL(Pn

θ , Pn

−
θ(cid:48)) = 0 .

∈ X
θ and notice that by Lemma 29,

, which exists by the

By our choice, the optimal action for the environment determined by θ and θ(cid:48) are different:

(θ)

(θ(cid:48)) =

P

∅

. The Bretagnolle-Huber inequality (Lemma 30) implies that

θ (En(θ)) + Pn
Pn

θ(cid:48)(En(θ(cid:48)))

θ (En(θ)) + Pn
Pn

θ(cid:48)(1

En(θ))

exp (

KL(Pn

θ , Pn

θ(cid:48)))

≥

−

1
2

≥

−

P

∩

1
2

.

≥

(17)

Furthermore, there exists an (cid:15) > 0 such that Rn(θ)
regret is linear for either environment θ or θ(cid:48).

≥

(cid:15)nPn

θ (En(θ))/2. Hence, by Eq. (17), the

Theorem 32 Suppose the game is globally observable, but not locally observable. Then there exists
a game-dependent constant c > 0 and θ

Rd such that the regret is Rn(θ)

cn2/3.

∈
Proof By Lemma 27, there exists a pair of neighboring Pareto optimal actions x, y
and θ
y /
∈
u

span(
−
{
L it follows that

Cy) such that x

L⊥. Since x

) = L. Let x

relint(Cx

L and v

(θ)
}

Az : z

∈ P

−

≥

∩

∈

))
ext(conv(
∈
y = u + v, where

X

∈

∈

In particular, for suitably small (cid:15) > 0 it holds that θ + (cid:15)v
and θ(cid:48)

θn = θ + n−1/3v

(cid:15)v

Cy. Deﬁne

∈

−

y /
∈
x
(cid:104)

−

y, v

=

(cid:105)

u + v, v
(cid:104)

(cid:105)

=

v

(cid:107)

2 > 0 .

(cid:107)
Cx and θ

−
n−1/3v

∈
n = θ
Cx and θ(cid:48)

−
n ∈

∈

Cy. Next, decompose Vn(θ) as

and let assume n is sufﬁciently large that θn
Vn(θ) = Un(θ) + Wn(θ), where
(cid:34) n
(cid:88)

(cid:35)

Un(θ) = Eθ

1(xt

(θ))AxtA(cid:62)
xt

t=1

) = (cid:80)n

t=1

1(xt

∈ P

∈ Y

Let Tn(
v

Y
L⊥, that
(cid:13)
(cid:13)θn

∈
1
2

and Wn(θ) = Eθ

1(xt /

(θ))AxtA(cid:62)
xt

.

∈ P

(cid:35)

(cid:34) n
(cid:88)

t=1

Y ⊂ X

) be the number of times an action in

is played. Notice, since

θ(cid:48)
n
where G = (cid:80)

−

(cid:13)
2
Vn(θn) = 2n−2/3
(cid:13)
x∈X AxA(cid:62)

v
(cid:107)

2
Vn(θn) = 2n−2/3
(cid:107)

v

2
Wn(θn) ≤

(cid:107)
x . Now, there exists a game-dependent constant (cid:15) > 0 such that

P

(cid:107)

2n−2/3Eθn[Tn(

(θ)c)]

2
G ,

v

(cid:107)

(cid:107)

Rn(θn)

(cid:15)Eθn[Tn(

(θ)c)] .

≥

P

Hence if Eθn[Tn(
n2/3. By
≥
the Bretagnolle-Huber inequality (Lemma 30), there exists another game-dependent constant (cid:15)(cid:48) > 0
such that

(cid:15)n2/3. Assume that Eθn[Tn(

n2/3, then Rn(θn)

(θ)c)]

(θ)c)]

≥

≤

P

P

Rn(θn) + Rn(θ(cid:48)

n)

n2/3(cid:15)(cid:48) exp

2n−2/3Eθn[Tn(

(θ)c)]

≥

(cid:16)

−

(cid:17)

v

2
G
(cid:107)

(cid:107)

≥

n2/3(cid:15)(cid:48) exp(

2
−

v

(cid:107)

(cid:107)

2
G) .

P

Combining the last two displays completes the proof.

41

INFORMATION DIRECTED SAMPLING FOR LINEAR PARTIAL MONITORING

Theorem 33 Suppose the game is locally observable, then there exists a constant c > 0 such that
for all n there is a θ for which Rn(θ)

cn1/2.

Proof Let θ
Rd, it follows that

∈

Rd be arbitrary and θn = n−1/2θ and θ(cid:48)
. By Lemma 29,

(θn)

(θ(cid:48)

n) =

n =

−

P

∩ P

θn. By the assumption that

spans

X

≥

∅

KL(Pn

θn, Pn
θ(cid:48)
n

) =

(cid:13)
(cid:13)θ(cid:48)

1
2

n −

θn

(cid:13)
2
Vn(θn) =
(cid:13)

1
2 (cid:107)

θ

2
Vn(θn)/n .
(cid:107)

Vn(θn)/n. Hence, there exists a constant c > 0 such that for all

Clearly, G = (cid:80)
n

1,

≥

x∈X AxA(cid:62)

x (cid:31)

KL(Pn

θn, Pn
θ(cid:48)
n

)

c .

≤

Then, using the same argument as in the proof of Theorem 31, we have

θn(En(θn)) + Pn
Pn
θ(cid:48)
n

(En(θ(cid:48)

n))

θn(En(θn)) + Pn
Pn
θ(cid:48)
n

(1

En(θn))

≥

1
2

≥

exp(

c) .

−

The result follows because there exists an (cid:15) > 0 such that Rn(θ)

Pn
θ (En(θ))(cid:15)√n/2.

−

≥

42

