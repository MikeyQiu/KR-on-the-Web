Action Recognition via Pose-Based Graph Convolutional Networks with
Intermediate Dense Supervision

Lei Shi1,2

Yifan Zhang1,2∗

Jian Cheng1,2,3

Hanqing Lu1,2

1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences
2University of Chinese Academy of Sciences
3CAS Center for Excellence in Brain Science and Intelligence Technology
{lei.shi, yfzhang, jcheng, luhq}@nlpr.ia.ac.cn

9
1
0
2
 
v
o
N
 
8
2
 
 
]

V
C
.
s
c
[
 
 
1
v
9
0
5
2
1
.
1
1
9
1
:
v
i
X
r
a

Abstract

Pose-based action recognition has drawn considerable
attention recently. Existing methods exploit the joint po-
sitions to extract the body-part features from the activa-
tion map of the convolutional networks to assist human ac-
tion recognition. However, these features are simply con-
catenated or max-pooled in previous works. The struc-
tured correlations among the body parts, which are essen-
tial for understanding complex human actions, are not fully
exploited. To address the problem, we propose a pose-
based graph convolutional network (PGCN), which encodes
the body-part features into a human-based spatiotemporal
graph, and explicitly models their correlations with a novel
light-weight adaptive graph convolutional module to pro-
duce a highly discriminative representation for human ac-
tion recognition. Besides, we discover that the backbone
network tends to identify patterns from the most discrim-
inative areas of the input regardless of the others. Thus
the features pooled by the joint positions from other ar-
eas are less informative, which consequently hampers the
performance of the followed aggregation process for rec-
ognizing actions. To alleviate this issue, we introduce a
simple intermediate dense supervision mechanism for the
backbone network, which adequately address the problem
with no extra computation cost during inference. We evalu-
ate the proposed approach on three popular benchmarks for
pose-based action recognition tasks, i.e., Sub-JHMDB, Pen-
nAction and NTU-RGBD, where our approach signiﬁcantly
outperforms state-of-the-arts without the bells and whistles.

1. Introduction

Human action recognition has been studied for decades
since it can be widely used in a number of applications

∗Corresponding Author

1

Figure 1. Visualization for actions of the pull-up (the ﬁrst row)
and swing-baseball (the second row) learned by PGCN. The ﬁrst
column shows the frames, joints (marked as blue x) and the corre-
sponding activated areas (the brighter areas) in I3D’s activations.
The second column shows the frame and the corresponding acti-
vated areas in PGCN’s feature map. Note that the PGCN focuses
more on the human joints that are highly discriminative for the
actions, e.g., the arms for pull-up and hands for swing-baseball.
The third column shows the learned graph topology of the PGCN
for the two samples. The blue circles and lines form the intrin-
sic graph represent the physical structure of the human body, and
the orange lines represent the new-learned edges, which reﬂect the
correlations among the human joints learned by the PGCN for the
target actions.

such as human-computer interaction and intelligent surveil-
lance [27, 7, 16, 34, 24, 35]. However, human action recog-
nition from the wild videos is still very challenging, mainly
due to the high dimension of the video data and the inter-
ference of the cluttered background. Currently, the main-
stream methods for video-based action recognition are built
upon two types of inputs, i.e., the RGB images and the cor-
responding optical ﬂow ﬁelds [33, 5]. However, these meth-
ods generally use the whole image as input, which may re-
sult in that the model does not focus on the human body

or speciﬁc body parts. The model may learn features from
background area due to the training data bias.

Recently, several pose-guided approaches for human ac-
tion recognition have been proposed to address the prob-
lem [7, 3, 9, 12]. They employ pose information to ex-
plicitly pool the body-part features from the ﬁnal feature
maps of the classiﬁcation network. Then the extracted fea-
tures are aggregated for action classiﬁcation. This can be
considered as a regularization mechanism by pose informa-
tion to make the model focus on the human body. How-
ever, in existing methods, the features pooled from different
body positions are independently used or simply concate-
nated or averaged to obtain a representation. Concatenating
or averaging the body-part features can not well model the
structured dependency and correlation of the body parts as
the correlated and uncorrelated body parts are given equal
weights.

To move beyond such limitations, we propose a novel
pose-based graph convolutional network (PGCN) for hu-
man action recognition. Instead of simply concatenating or
averaging the pooled body-part features, we encode them
into a spatiotemporal graph constructed based on the phys-
ical structure of the human body, and explicitly model the
body-related correlations among these features with a novel
light-weight adaptive graph convolutional module. By tak-
ing into account these correlations and dependencies, our
model can produce a more discriminative pose-related rep-
resentation for understanding human actions compared with
previous methods.

Another notable problem we discover is that the back-
bone CNN, from which the body-part features are extracted,
is “lazy”. It is inclined to identify patterns from the most
discriminative areas of the inputs, regardless of the others.
For instance, given a video containing the pull-up action,
the CNN can recognize it by identifying the posture of the
arms regardless of the remaining parts such as the hip and
legs. It is because for most of the popular classiﬁcation net-
works, the last feature map is global-average-pooled and su-
pervised with only one cross-entropy loss. Thus once a part
feature is discriminative enough to obtain a high accuracy
for the training set, other parts can not generate useful losses
anymore. It causes the problem that the network makes lit-
tle effort to identify the patterns of those less-discriminative
areas. When pooling features from those areas, it is hard
for the followed aggregation module to model the body part
correlations with these less informative features.

To tackle this problem, we propose to add a novel in-
termediate dense supervision mechanism for the backbone
network during the training process. The main idea is to
encourage the network to infer the categories using only
part of the input information. In detail, we drop the global-
average pooling layer and directly supervise all elements of
the last layer of feature maps of the backbone network with

a dense-head classiﬁcation loss. This method is simple but
effective, without the need for extra parameters and compu-
tations during inference. It forces the network to look be-
yond the most discriminative parts and mine rich cues from
all areas of the input. In this way, the extracted body-part
features that are less informative in previous methods can
contain more abundant information for inferring the cate-
gories, and the followed graph convolutional module can
better model the body-related correlations to recognize hu-
man actions.
To test

the effectiveness of the proposed methods,
we perform extensive experiments on three widely used
datasets for pose-based action recognition, namely, Sub-
JHMDB, PennaAction and NTU-RGBD, where our model
achieves state-of-the-art performance on both of them with-
out the bells and whistles. Note that, except for the RGB
and pose modalities, we do not use additional modalities
such as the optical ﬂow ﬁelds and the part afﬁnity ﬁelds.

Overall, our contributions lie in three aspects: Firstly,
we propose a pose-based graph convolutional network
(PGCN), which can employ the joint position to pool the
body-part features and aggregate them to better represent
human actions. It is the ﬁrst to employ the graph convo-
lutional module to model the spatiotemporal correlations
among the pooled features to produce a highly discrimi-
native pose-related representation for human action recog-
nition. Secondly, we discover that the backbone network
tends to ignore identifying patterns from the less discrimi-
native areas of the input in existing methods, which ham-
pers the performance of the followed aggregation process.
We propose to add a novel intermediate dense supervision
after the backbone network, which effectively addresses the
problem without the need for extra parameters and compu-
tations. Thirdly, we evaluate our approach on three pop-
ular benchmarks for pose-based action recognition, which
achieves state-of-the-art performance using only the RGB
and pose modalities without additional training tricks. Our
code is released to provide a new baseline and facilitate
communication.

2. Related Works

2.1. Action recognition

Early approaches for action recognition focus on design-
ing hand-crafted features [29, 30]. Recently, deep neural
networks have shown big success in many computer vision
tasks, which also exhibit excellent performance in the ﬁeld
of action recognition. There mainly exists three branches
for these data-driven approaches: RNN-based [38, 20] ap-
proaches, two-stream [26, 33] approaches and 3D-CNN-
based approaches [28, 11]. Among these approaches, the
3D-CNN-based approaches usually achieve higher perfor-
mance and faster speed. It can also be integrated with the

Figure 2. The pipeline of the proposed PGCN.

optical ﬂow to form the two-stream 3D CNNs [5, 31]. How-
ever, since the action is human-centered, these methods ig-
nore the important cues for recognizing human actions, i.e.,
the spatiotemporal evolutions of the human pose. Our work
is based on the 3D CNNs, but further incorporates the pose
information to help model understanding the complex hu-
man actions.

2.2. Pose-based action recognition

Human pose has been proved a discriminative cue in pre-
vious hand-crafted-feature-based methods for human action
recognition [15, 14]. Recently, with the emergence of the
data-driven methods, several works propose to exploit the
human pose in deep models to help classifying human ac-
tions [7, 3, 9, 6, 19, 18, 12, 35]. One stream of these meth-
ods try to pool a number of body-part features guided with
the pose information, which are then aggregated to produce
a discriminative representation to predict human actions. In
detail, Cheron et al. [7] employ a shared CNN to extract
the body-part features from the cropped image patches and
aggregate them with the max-min pooling strategy. Cao et
al. [3] pool the body-part features from the C3D’s feature
maps and concatenate them to feed into the SVM classi-
ﬁer. Du et al. [9] and Huang et al. [12] also pool the joint-
related features but from 2D CNNs. Du et al. [9] aggregate
the features with a body-part-pooling layer and fed them
into an LSTM to model the temporal evolutions. Huang
et al. [12] further propose a part-based hierarchical pooling
approach that can group and pool body-part features from
local to global. However, the body-part features are highly
coupled with each other based on the human body structure,
and these approaches fail to model the body-related corre-
lations using the straightforward aggregation strategies as
introduced above. Another stream tries to exploit the by-
products of the pose estimation to assist the human action
recognition, such as the part afﬁnity ﬁelds and the pose es-

timation heatmap [18, 19, 35]. In this work, we do not in-
vestigate these by-products since they are not our focus, but
it is simple and straightforward to integrate them within a
multi-stream framework.

2.3. Skeleton-based action recognition

Another stream, namely, skeleton-based action recog-
nition, use the pure pose information, i.e., the 2D/3D co-
to recognize human ac-
ordinates of the human joints,
tions [10, 2, 36, 23, 22]. However, for these approaches,
it is hard to classify the actions that have similar postures
such as “torch chest” versus “torch back” because the ap-
pearance information is discarded. In our methods, both the
joints coordinates and the appearance information are lever-
aged to solve the problem effectively.

2.4. Graph convolutional networks

Graph convolutional networks (GCNs) can generalize
CNNs from grid inputs such as images and videos to
non-Euclidean structured data such as graphs and mani-
folds [25, 8, 32, 36, 22]. There are two streams for con-
structing GCNs: 1) the spectral-stream approaches formu-
late the GCNs in the spectral domain, which are based
on the analogy between the classical Fourier transforms
and projections onto the eigenvectors of the graph Lapla-
cian operator [25, 8]; 2) the spatial-stream approaches
provide ﬁlter localization via the ﬁnite size of the ker-
nel, which is application-dependent and needs to design
the mapping functions manually to match the local neigh-
bors [32, 36, 23]. Our work follows the spatial-stream ap-
proaches, which are more ﬂexible and more targeted for par-
ticular tasks.

3. Our Methods

In this section, we introduce the proposed pose-guided
graph convolutional network (PGCN) in detail. First, the

overall pipeline of the proposed framework is presented
brieﬂy in Sec. 3.1. Then, we show how to obtain the body-
part features from the 3D CNN’s feature maps with the help
of the pose information in Sec. 3.2. Besides, the graph
convolutional module and some speciﬁc designs of the net-
work architecture are described thoroughly in Sec. 3.3. Fi-
nally, we introduce the “laziness” problem of the backbone
network and the proposed intermediate dense supervision
mechanism in Sec. 3.4.

3.1. Pipeline overview

Fig. 2 shows the overall pipeline of the proposed PGCN.
It is input with the RGB video and the corresponding human
pose information, which can be easily obtained from the
motion-capture device such as Kinect or the pose estimation
algorithm such as OpenPose [4]. First, the input video clip
is fed into a 3D CNN to extract the spatiotemporal features
from low-levels to high-levels. To solve the “laziness” prob-
lem of the backbone network, we add a dense-head clas-
siﬁcation loss after the backbone for intermediate supervi-
sion. Then, with the help of the coordinate mapping strat-
egy, the body-part features are pooled from the last feature
maps of the 3D CNN. Owing to the conception of the recep-
tive ﬁeld [3, 9], these features represent the surrounding in-
formation of the corresponding joints in the original video.
Finally, a spatiotemporal graph is built upon the structure
of the human body to encode these features and their cor-
relations, which is fed into a graph convolutional module
to produce a pose-related representation and generate the
human-based classiﬁcation scores. Moreover, the classi-
ﬁcation score of the original 3D CNN, which is global-
average-pooled from the spatiotemporal feature, naturally
represent the information of the whole scenes. The skeleton
information, i.e., the coordinates of the joint, naturally rep-
resents the posture information of the human body. We fuse
the human-based classiﬁcation score, the scene-based clas-
siﬁcation score and the posture-based classiﬁcation score to
get the ﬁnal prediction (green lines in Fig. 2). In the rest of
the paper, for a fair comparison, we call the mainstream, i.e.,
without exploiting the joint coordinates and scene-based in-
formation, as the PGCN and the overall framework as the
PGCN-Fusion.

3.2. Feature Extraction

In this section, we introduce the feature extraction part
of the PGCN, i.e., how to obtain the body-part features. As
introduced in Sec. 2.1, the two-stream 3D CNNs have been
proved useful in learning spatiotemporal representations of
videos. We use the spatial stream of the well-known two-
stream I3D [5] as the feature extractor. We do not use the
temporal-stream in this work because calculating the optical
ﬂow ﬁeld is time-consuming. It is also worth to note that
our approach is not restricted to the 3D CNNs. We have

also tried other feature extractors such as 2D CNNs [33],
but the performance is not as good as using 3D CNNs.

For each step of the convolution, the convolutional ﬁl-
ter only acts on a local region, which results in a feature
that corresponds to a particular sensory space of the feature
map in the preceding layer. This local space is called the
receptive ﬁeld of the point outputted in this convolutional
step [3, 9]. Because the 3D CNN employs the 3D ﬁlters
to extract the spatiotemporal features, its receptive ﬁeld is
a spatiotemporal cube. The points in the higher layers of
3D CNNs can be mapped to the corresponding cubes in the
lower layers and vice versa. By mapping the point layer by
layer, we can localize the point in arbitrary feature maps that
corresponds to the human joint of the input video. Because
the estimated coordinates of the human joints may have er-
rors, the features of the 3 × 3 × 3 cube around the joints
in the feature maps are extracted and max-pooled to reduce
the wrong mappings. We also tried using average-pooling
and max-min-pooling for the cube, but the performance is
not as good as using the max-pooling. Note that there may
be some joints that are not detected or outside the image.
We pad the features of these joints with 0. Another strategy
is to associate the semantically-related human joints into a
number of groups and replace the missed joints with the one
in the same group [9]. However, we found it has no effect.

3.3. Adaptive graph convolutional module

Figure 3. Illustration of the spatiotemporal graph building (left)
and the subset-partition strategy (right).

3.3.1 Graph building

Inspired by previous works for human parsing [37], we rep-
resent the human body as a tree-structured graph, where the
nodes are the human joints and the edges are the physical
connections among the joints (Fig. 3, left). The root node
is deﬁned as the gravity center of the human body (the blue
node). The spatial edges are pointed from the parent nodes
to the child nodes (the blue lines). The temporal edges are
pointed from the joint in the preceding frame to the same

joint in the later frame (the yellow lines). The attribute of
each node is the body-part features introduced in previous
sections. In this way, a video can be encoded into an acyclic
spatiotemporal directed graph.

3.3.2 Convolution on graphs

Performing convolution on graphs is not as straightforward
It is because the
as performing convolution on images.
number of weights of a convolutional kernel is ﬁxed, but the
number of the node neighbors in the graph is unﬁxed. This
leads to the mapping problem between the unﬁx-numbered
neighbor-nodes and the ﬁx-numbered weights. Inspired by
previous works [36, 23], we solve this problem by partition
the neighborhood nodes into ﬁx-numbered subsets, where
the number is equal with the kernel size.

Speciﬁcally, given the feature of the node j in the ith
feature map and tth frame, i.e., f i(vtj), the corresponding
activation in the (i + 1)th feature map, i.e., f i+1(vtj), is
calculated as

have no implicit arrangement. Here, we exploit the adja-
cency matrix of the graph to achieve the operation. Given
the extracted body-part features in the tensor form fin ∈
RCin×T ×J where T and J denote the number of frames
and the number of joints, the convolutions on graph can be
formulated as

fout =

Wk(fin ˆAk)

(2)

Ks(cid:88)

k

2

where Ks denotes the kernel size along the spatial dimen-
sion. Wk ∈ RCout×Cin is the weight matrix. ˆAk =
Λ− 1
k A. Ak ∈ RJ×J is an adjacency-like matrix that de-
notes the connectivity of the subset Sk. Its element Axy
k
denotes whether the node vx is in the subset of the node
vy, i.e., Sk(vy). Λ− 1
is a diagonal matrix that is used for
normalization, i.e., averaging the nodes in the subset. Its el-
ement Λxx
k . As for the temporal dimension, the
graph convolution is equipped with performing a 2D con-
volution with the kernel size as (Kt, 1) on the input tensor
fin ∈ RCin×T ×J . Where Kt denotes the kernel size along
the temporal dimension.

k = (cid:80)

y Λxy

k

2

f i+1(vtj) =

K
(cid:88)

k

wk
NSk(vtj )

(cid:88)

f i(vtl)

(1)

vtl∈Sk(vtj )

3.3.4 Adaptive graph

where K is the kernel size of the convolution. wk is the
weight of the kernel. Sk(vtj) denotes the node subset of
vtj that corresponds to the wk. NS denotes the number of
the nodes in the subset S. The nodes in the subset Sk(vtj)
will be average-pooled and multiplied with wk. Because the
number of the subsets is the same with the kernel size, the
mapping problem is avoided. Note that if NSk(vtj ) = 1,
Eq. 1 is same with the standard convolution.

Now, the main problem lies in how to partition the sub-
sets. For the spatial dimension, the neighborhood of one
node is deﬁned as the 1-distance nodes in the graph. Since
the graph is tree-structured, we set the kernel size to three
and divide the 1-distance neighborhood of one node into
three subsets: the subset of its parent nodes, the subset of
itself and the subset of its child nodes. This representation
is intuitive because the human body is naturally an articu-
lated system, where the position of one joint is always af-
fected by its parent joint and affects the position of its child
joints. Fig. 3 (right) shows this partitioning strategy, where
the numbered circles are the neighbors of the node 1 and
different numbers represent different subsets. For the tem-
poral dimension, since the temporal order is ﬁxed, one node
must have two neighbors, i.e., the same nodes in the preced-
ing frame and the next frame.

3.3.3 Implementation

Implementation of the convolution on graphs is not di-
rected, especially for the spatial dimension where the nodes

Inspired by [23, 32], we adapt the graph topology. The orig-
inal graph is deﬁned according to the physical structure of
the human body, which may not be the best choice for the
action recognition task. For example, when recognizing the
action of “clapping”, the correlations between two hands
should be signiﬁcant. However, they are far from each other
in the human-body-based graphs. Note that the graph topol-
ogy is determined by the adjacency matrix as introduced in
Sec. 3.3.3. Here, we add two additional adaptive graphs:
the global graph and the individual graph. The global graph
is the same with the human-body-based graph, but is set as
the parameter and is updated based on the action recognition
loss in the training process. The individual graph is calcu-
lated based on the similarities among the body-part features.
Eq. 3 shows the calculation of the graph structure.

Ak = Bk + αCk

Ck = T anh(fin

T WT

θkWφkfin)

(3)

where Ak is the adjacency-like matrix introduced in Eq. 2.
Bk denotes the global graph and Ck denotes the individual
graph. θ and φ are two embedding functions which are real-
ized with the 1×1 convolutions. Note that we use the T anh
instead of the Sof tM ax to normalize the Ck. We argue
that the T anh can provide more ﬂexibility to learn the graph
topology. For example, it can remove the edges learned in
the global graph based on the individual requirements of the
current data, but the Sof tM ax can only produce the posi-
tive value. Besides, T anh is insensitive to large values and
can mine more correlations among the features.

3.4. Intermediate dense supervision

The last feature map of the 3D CNN is a spatial-temporal
volume V ∈ RT ×H×W ×C, where T , H, W and C denote
the length, height, width of the feature map and the number
It consists of T × H × W C-
of channels, respectively.
dimension features each corresponds to a particular cube of
the original video as illustrated in Section 3.2. To encour-
age the network to fully extract the information contained in
every cube of the input video, we drop the global-average-
pooling layer of the I3D and require that every feature of V
should be fully exploited and make a prediction of the ac-
tions. In detail, as shown in Fig. 4, we add T × H × W
heads after the last feature maps, each of which is realized
with a single fully-connect layer. Every head inputs the cor-
responding C-dimension feature vector and outputs the la-
bel prediction. The cross-entropy loss is added after each
head and averaged to back-propagate the errors. Note that
the parameters of the heads are not shared so that the back-
propagation process of each branch does not affect each
other. In this way, it forces the model to identify patterns
from all areas of the input video rather than only focus on
the most discriminative one.

Figure 4. Illustration of the intermediate dense supervision.

4. Experiments

4.1. Datasets

In our experiments, we choose three popular datasets for
pose-based action recognition, namely, Sub-JHMDB [15],
PennaAcion [39] and NTU-RGBD [21], to verify the effec-
tiveness of the proposed methods. We do not use Kinet-
ics because the humans are poorly visible in many videos
of this dataset, which is also discussed in previous related
works [6, 35, 12]. Instead, we use NTU-RGBD as a substi-
tute, which is large, challenging and widely used for pose-
based action recognition.

Sub-JHMDB is a subset of JHMDB, which has 316
videos with 12 human action classes. Each video is anno-
tated with the action label and 15 body joints. We conduct

the experiments in the three ofﬁcial provided splits of the
dataset.

PennaAction consists of 2325 videos in the wild with 15
action classes. Each video is annotated with 13 body joints.
We conduct the experiments with the 50/50 training/testing
split provided by the dataset. Note that there are videos in
which not all the body joints are visible.

NTU-RGBD consists of 56,000 action clips in 60 ac-
tion classes. There are up to 2 subjects in each video,
where each subject is annotated with 25 joints. We use
the cross-subject benchmark to evaluate the performance,
which means the subjects are different for the training set
and test set. Since the person is relatively small compared
with the whole scene, we crop the person area based on the
pose information.

4.2. Training details

To highlight the keys instead of the training skills, we
use a simple strategy to process the input data. For training,
we randomly sample 40 frames and temporally randomly
crop 32 frames. Each frame is spatially randomly cropped
to 256 × 256, and resized to 224 × 224. As for the pose
information, the positions of the joints are randomly per-
turbed within 1%. For testing, we use uniform-sample and
center-crop strategies.

When training the PGCN, we use the I3D pretrained in
the Kinetics [5] and ﬁne-tune it in the target dataset. The
initial learning rate for ﬁne-tuning I3D is 0.01, which is di-
vided by 10 after the validation accuracy saturates. Weight
decay is 0.0005. We use four TITANXP-GPUs and the
batch size is 32. Then, we ﬁx the parameters of the back-
bone model and train the graph convolutional module. The
learning rate is initialized to 0.1, and is also divided by
10 after the validation accuracy saturates. Weight decay is
0.0001 and batch size is 32. Finally, both the I3D and graph
convolutional module are ﬁne-tuned together with the learn-
ing rate being 0.001.

4.3. Ablation study

In this section, we test the effectiveness of the proposed
modules on the ﬁrst split of the Sub-JHMDB since it is rel-
atively smaller. When testing one conﬁguration, the others
are set the same for a fair comparison.

4.3.1 Adaptive graph convolutional module

We test the necessary of pooling the body-part features and
the effectiveness of the proposed graph-based aggregation
strategy as shown in Tab. 1. I3D denotes the baseline that
does not use the pose information. I3D+FC means pooling
the body-part features based on the pose information and
uses a simple fully-connected layer to aggregate the fea-
tures. PGCN denotes our method that employs the graph

convolutional module to model the structured correlations
among these body-part features.
It shows that exploiting
the pose information (I3D+FC) performs better compared
with the baseline method (I3D). Besides, employing the
proposed graph convolutional module (PGCN) further im-
proves the performance with nearly no additional parame-
ters and computations. It conﬁrms our motivation that mod-
eling the correlations and dependencies among the body
parts is important for understanding human actions. More-
over, we also test the performance of using the pose infor-
mation obtained by the OpenPose [4] (PGCN*). It shows
that the performance drops a little since the pose estimated
by OpenPose is less accurate than GT-annotations.

Method Accuracy
I3D
I3D+FC
PGCN
PGCN*

85.4
86.5
88.8
87.6

Params(M) GMACS

12.33
13.06
13.07
13.07

55.75
55.75
55.78
55.78

Table 1. Recognition accuracy (%) of different strategies for ex-
ploiting the pose information on the spilt 1 of the Sub-JHMDB. *
denotes using the pose information estimated by the OpenPose [4].

We also investigate the effectiveness of adaptively learn-
ing the graph topology as shown in Tab. 2. There are two
types of adaptive graphs: the global graph (B) and the in-
dividual graph (C). It shows that both of the two types of
graphs bring consistent improvements.

Method
no adaptive
B only
B+C

Accuracy
84.3
86.5
88.8

Table 2. Recognition accuracy (%) of different adaptive graph con-
ﬁgurations on the spilt 1 of the Sub-JHMDB.

We visualize two examples for actions of the pull-up (the
ﬁrst row) and the swing-baseball (the second row) in our
model in Fig. 1. The ﬁrst column shows the activation ar-
eas of the I3D’s last convolutional layer in one frame. It
shows that the activation areas are distributed throughout
the scenes. The second column shows the activation areas
of the PGCN’s last layer. Because it is input with the joint-
related features, the PGCN only cares about the human-
centered areas.
It shows the PGCN focuses more on the
particular joints that are discriminative for the actions. For
example, when modeling the sample of the pull-up, it acti-
vates more on the arms of the human. When modeling the
sample of the swing-baseball, it activates more on the two
hands. The third column shows the learned graph topology
of the adaptive graph convolutional module. For pull-up,
the new edges are mainly built between the two arms, which
means the model successfully models the correlations be-
tween the two arms. For swing-baseball, the PGCN focuses

more on the correlations between the hands and other joints.
This consolidates the effectiveness of the proposed adaptive
graph convolutional module.

4.3.2

Intermediate dense supervision

We test the effectiveness of the intermediate dense supervi-
sion (IDS) introduced in Section 3.4. As shown in Tab. 3,
using the IDS harms the performance of the original 3D
CNN (“I3D w/ IDS” vs. “I3D w/o IDS”). It is because most
of the areas of the input video do not have enough informa-
tion to recognize the actions, and averaging their outputs
has a negative impact on the ﬁnal result. However, when
adding the fully-connected layers or the graph convolutional
module, the performance of using the intermediate supervi-
sion is better (“PGCN w/ IDS” vs. “PGCN w/o IDS” and
“I3D+FC w/ IDS” vs. “I3D+FC w/o IDS”). It is because the
IDS encourages the model to identify patterns from every
area of the inputs rather than only the most discriminative
area. Thus the body-part features that are less informative in
previous methods now contain more abundant information,
and modeling their correlations brings more improvement.
Fig. 5 shows some examples of the learned feature maps
of the “I3D w/o IDS” (ﬁrst row) and the “I3D w/ IDS” (sec-
ond row). It shows the I3D supervised by IDS actives more
than original I3D. Although the loss makes the model focus
on some background areas, the pose-based pooling mech-
anism and the graph convolutional module can effectively
solve this problem. Note that in the ﬁrst example, the orig-
inal I3D mistakenly focuses the bag and totally ignores the
people. By adding the IDS, the problem is avoid.

w/o IDS w/ IDS

Model
I3D
I3D+FC
PGCN

85.4
86.5
88.8
Table 3. Recognition accuracy (%) on the spilt 1 of the Sub-
JHMDB. IDS denotes the intermediate dense supervision. w/o
denotes without and w/ denotes with.

81.5
87.6
89.9

4.3.3 Feature fusion

We test the complementarity among different modalities in
Tab. 4. “Coordinate” means feeding only the coordinate of
the joints into the graph convolutional module to predict ac-
tions. Since it lacks the appearance information, its perfor-
mance is lower. By fusing both of the RGB image and the
pose information, our PGCN outperforms the single modal-
ity based methods. Furthermore, by fusing them all in a uni-
ﬁed framework, the performance is improved signiﬁcantly.

Figure 5. Comparison of the activations of the I3D without (the ﬁrst row) and with (the second row) the intermediate dense supervision
(IDS). The I3D supervised by the IDS actives more than original I3D. Although the loss makes the model focus on some background areas,
the pose-based pooling mechanism and the graph convolutional module can effectively solve this problem. Note that in the ﬁrst example,
the original I3D mistakenly focuses the bag and totally ignores the people. By adding the IDS, the problem is avoid.

Method
Coordinate
I3D
PGCN
PGCN-Fusion

Accuracy
69.7
85.4
89.9
91.0

Table 4. Recognition accuracy (%) of using different modalities on
the split 1 of the Sub-JHMDB.

4.4. Comparison with State of the Arts

In this section, we ﬁrst compare the recognition accuracy
of our PGCN with other approaches on the Sub-JHMDB
and PennaAction. The results of the Sub-JHMDB are aver-
aged over three ofﬁcial splits. Tab. 5 shows that our PGCN
achieves state-of-the-art performance on both of the two
datasets (+2.8% and +0.8%) without the bells and whistles.
Note that “P-Evo.” and “DPI+DTI” additionally exploit the
joint estimation map, i.e., the by-product of the pose esti-
mation algorithm. “P2RN” additionally exploits the optical
ﬂow ﬁelds. Our method only uses the pose information and
the RGB videos as input. We further evaluate the PGCN on
a larger widely used dataset, i.e., NTU-RGBD, as shown in
Tab. 6, where our PGCN outperforms state-of-the-arts sig-
niﬁcantly (+4.7%).

5. Conclusion

In this paper, we propose a novel pose-based graph con-
volutional network (PGCN) for human action recognition.
Unlike previous approaches for pose-based action recogni-
tion, it builds a spatiotemporal graph with the body-part fea-
tures pooled from the last feature map of the 3D CNN and
employs a light-weight adaptive graph convolutional mod-
ule to model the body-related correlations among these fea-
tures. Besides, we point that the backbone network tends
to ignore identifying patterns from less discriminative ares

Sub-JHMDB PennaAction

State-of-the-art
P-CNN [7]
JDD [3]
RPAN [9]
Pose+MD [13]
P-Evo. [18]
DPI+DTI [17]
P2RN [12]
PGCN
PGCN (Fusion)

Year
2015
2016
2017
2018
2018
2019
2019
2019
2019

72.5
83.3
78.6
78.9
-
-
86.5
87.0
89.3

-
95.7
97.4
97.6
98.2
95.9
98.0
98.1
99.0

Table 5. Comparison with state-of-the-arts on the Sub-JHMDB
(average over three splits) and PennaAction.

State-of-the-art
ST-GCN [36]
DGNN [22]
Luvizon et al. [19]
Baradel et al. [1]
P-Evo. [18]
DPI+DTI [17]
PGCN
PGCN (Fusion)

Year Accuracy
2018
2019
2018
2018
2018
2019
2019
2019

81.5
89.9
85.5
86.6
91.7
90.2
94.0
96.4

Table 6. Comparison with state-of-the-arts on the cross-subject
benchmark of the NTU-RGBD dataset.

in existing approaches, which hampers the performance of
the followed aggregation process. To address the problem,
we introduce a novel intermediate dense supervision mech-
anism, which is veriﬁed effective with no extra parameters
and computation cost during inference. Our method is eval-
uated on three popular datasets for pose-based action recog-
nition task and outperforms state-of-the-arts signiﬁcantly on
all of them. Future work can focus on how to integrate the
human pose estimation algorithms and the pose-guide ac-
tion recognition algorithms in a uniﬁed framework.

References

[1] Fabien Baradel, Christian Wolf, Julien Mille, and Gra-
ham W. Taylor. Glimpse Clouds: Human Activity
Recognition From Unstructured Feature Points.
In
The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 469–478, 2018. 8

[2] C. Cao, C. Lan, Y. Zhang, W. Zeng, H. Lu, and
Y. Zhang. Skeleton-Based Action Recognition with
Gated Convolutional Neural Networks. IEEE Trans-
actions on Circuits and Systems for Video Technology,
2018. 3

[3] Congqi Cao, Yifan Zhang, Chunjie Zhang, and Han-
qing Lu. Action Recognition with Joints-Pooled 3d
In IJCAI, page 3,
Deep Convolutional Descriptors.
2016. 2, 3, 4, 8

[4] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser
Sheikh. Realtime multi-person 2d pose estimation
In The IEEE Conference
using part afﬁnity ﬁelds.
on Computer Vision and Pattern Recognition (CVPR),
2017. 4, 7

[5] Joao Carreira and Andrew Zisserman. Quo Vadis, Ac-
tion Recognition? A New Model and the Kinetics
Dataset. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 6299–6308,
July 2017. 1, 3, 4, 6

[6] Vasileios Choutas, Philippe Weinzaepfel, Jrme Re-
vaud, and Cordelia Schmid. Potion: Pose motion rep-
resentation for action recognition. In The IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR), pages 7024–7033, 2018. 3, 6

[7] Guilhem Chron, Ivan Laptev, and Cordelia Schmid. P-
cnn: Pose-based cnn features for action recognition. In
The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3218–3226, 2015. 1, 2, 3,
8

[8] Michal Defferrard, Xavier Bresson, and Pierre Van-
Convolutional Neural Networks on
dergheynst.
Graphs with Fast Localized Spectral Filtering.
In
Advances in Neural Information Processing Systems,
pages 3844–3852, 2016. 3

[9] Wenbin Du, Yali Wang, and Yu Qiao. Rpan: An
end-to-end recurrent pose-attention network for ac-
tion recognition in videos. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
pages 3725–3734, 2017. 2, 3, 4, 8

[11] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.
Can Spatiotemporal 3d CNNs Retrace the History of
In The IEEE Conference
2d CNNs and ImageNet?
on Computer Vision and Pattern Recognition (CVPR),
pages 6546–6555, June 2018. 2

[12] Linjiang Huang, Yan Huang, Wangli Ouyang, and
Liang Wang. Part-Aligned Pose-Guided Recurrent
Network for Action Recognition. Pattern Recognition,
92:165–176, 2019. 2, 3, 6, 8

[13] Yi Huang, Shang-Hong Lai, and Shao-Heng Tai. Hu-
man Action Recognition Based on Temporal Pose
In European
CNN and Multi-dimensional Fusion.
Conference on Computer Vision, pages 426–440,
2018. 8

[14] U. Iqbal, M. Garbade, and J. Gall. Pose for Action -
Action for Pose. In IEEE International Conference on
Automatic Face Gesture Recognition, pages 438–445,
May 2017. 3

[15] Hueihan Jhuang, Juergen Gall, Silvia Zufﬁ, Cordelia
Schmid, and Michael J. Black. Towards Understand-
In The IEEE Conference
ing Action Recognition.
on Computer Vision and Pattern Recognition (CVPR),
pages 3192–3199, Dec. 2013. 3, 6

[16] Will Kay, Joao Carreira, Karen Simonyan, Brian
Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,
Fabio Viola, Tim Green, Trevor Back, Paul Nat-
sev, and others. The Kinetics Human Action Video
Dataset. arXiv:1705.06950, 2017. 1

[17] Mengyuan Liu, Fanyang Meng, Chen Chen, and
Songtao Wu. Joint Dynamic Pose Image and Space
Time Reversal for Human Action Recognition from
Videos. In AAAI, 2019. 8

[18] Mengyuan Liu and Junsong Yuan. Recognizing Hu-
man Actions as the Evolution of Pose Estimation
Maps. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 1159–1168,
2018. 3, 8

[19] Diogo C. Luvizon, David Picard, and Hedi Tabia.
2d/3d Pose Estimation and Action Recognition Using
In The IEEE Conference
Multitask Deep Learning.
on Computer Vision and Pattern Recognition (CVPR),
pages 5137–5146, 2018. 3, 8

[20] C. Ma, X. Yang, Zhang Chongyang, and M. H. Yang.
pages 5388–5396,

Long-term correlation tracking.
June 2015. 2

[10] Yong Du, Wei Wang, and Liang Wang. Hierarchi-
cal recurrent neural network for skeleton based action
In The IEEE Conference on Computer
recognition.
Vision and Pattern Recognition (CVPR), pages 1110–
1118, 2015. 3

[21] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang
Wang. NTU RGB+D: A Large Scale Dataset for 3d
In The IEEE Conference
Human Activity Analysis.
on Computer Vision and Pattern Recognition (CVPR),
pages 1010–1019, 2016. 6

In The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 5314–5322, 2018. 1,
2, 4

[34] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen
Tu, and Kevin Murphy. Rethinking spatiotemporal
feature learning: Speed-accuracy trade-offs in video
classiﬁcation. In The European Conference on Com-
puter Vision (ECCV), pages 305–321, 2018. 1
[35] An Yan, Yali Wang, Zhifeng Li, and Yu Qiao. PA3d:
Pose-Action 3d Machine for Video Recognition.
In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 7922–7931,
2019. 1, 3, 6

[36] Sijie Yan, Yuanjun Xiong, and Dahua Lin.

Spa-
tial Temporal Graph Convolutional Networks for
Skeleton-Based Action Recognition. In AAAI, 2018.
3, 5, 8

[37] Yi Yang and Deva Ramanan. Articulated pose esti-
mation with ﬂexible mixtures-of-parts. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1385–1392. IEEE, 2011. 4
[38] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra
Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and
George Toderici. Beyond short snippets: Deep net-
In The IEEE Con-
works for video classiﬁcation.
ference on Computer Vision and Pattern Recognition
(CVPR), pages 4694–4702, 2015. 2

[39] W. Zhang, M. Zhu, and K. G. Derpanis.

From
Actemes to Action: A Strongly-Supervised Repre-
sentation for Detailed Action Understanding. In The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 2248–2255, Dec. 2013. 6

[22] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.
Skeleton-Based Action Recognition With Directed
In The IEEE Conference
Graph Neural Networks.
on Computer Vision and Pattern Recognition (CVPR),
June 2019. 3, 8

[23] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.
Two-Stream Adaptive Graph Convolutional Networks
for Skeleton-Based Action Recognition. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2019. 3, 5

[24] Lei Shi, Yifan Zhang, Cheng Jian, and Lu Han-
qing. Gesture Recognition using Spatiotemporal De-
formable Convolutional Represention. In ICIP, Oct.
2019. 1

[25] David I Shuman, Sunil K Narang, Pascal Frossard,
Antonio Ortega, and Pierre Vandergheynst.
The
emerging ﬁeld of signal processing on graphs: Ex-
tending high-dimensional data analysis to networks
and other irregular domains. IEEE Signal Processing
Magazine, 30(3):83–98, 2013. 3

[26] Karen Simonyan and Andrew Zisserman.

Two-
stream convolutional networks for action recognition
in videos. In Advances in Neural Information Process-
ing Systems, pages 568–576, 2014. 2

[27] Khurram Soomro, Amir Roshan Zamir, and Mubarak
Shah. UCF101: A dataset of 101 human actions
classes from videos in the wild. arXiv:1212.0402,
2012. 1

[28] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-
resani, and Manohar Paluri. Learning Spatiotempo-
ral Features With 3d Convolutional Networks. In The
IEEE International Conference on Computer Vision
(ICCV), pages 4489–4497, Dec. 2015. 2

[29] Heng Wang, Alexander Klaser, Cordelia Schmid, and
Cheng-Lin Liu. Action recognition by dense trajecto-
ries. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 3169–3176, June
2011. 2

[30] Heng Wang and Cordelia Schmid. Action Recogni-
tion with Improved Trajectories. In The IEEE Interna-
tional Conference on Computer Vision (ICCV), pages
3551–3558, Dec. 2013. 2

[31] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and
In The
Kaiming He. Non-Local Neural Networks.
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018. 3

[32] Xiaolong Wang and Abhinav Gupta. Videos as Space-
Time Region Graphs. In Proceedings of the European
Conference on Computer Vision (ECCV), 2018. 3, 5

[33] Yali Wang, Lei Zhou, and Yu Qiao. Temporal hallu-
cinating for action recognition with few still images.

