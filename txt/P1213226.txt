8
1
0
2
 
r
a

M
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
8
5
2
0
.
8
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

CASCADE ADVERSARIAL MACHINE LEARNING REG-
ULARIZED WITH A UNIFIED EMBEDDING

Taesik Na, Jong Hwan Ko & Saibal Mukhopadhyay
School of Electrical and Computer Engineering
Georgia Institute of Technology
Atlanta, GA 30332, USA
{taesik.na, jonghwan.ko, smukhopadhyay6}@gatech.edu

ABSTRACT

Injecting adversarial examples during training, known as adversarial training, can
improve robustness against one-step attacks, but not for unknown iterative at-
tacks. To address this challenge, we ﬁrst show iteratively generated adversarial
images easily transfer between networks trained with the same strategy. Inspired
by this observation, we propose cascade adversarial training, which transfers the
knowledge of the end results of adversarial training. We train a network from
scratch by injecting iteratively generated adversarial images crafted from already
defended networks in addition to one-step adversarial images from the network
being trained. We also propose to utilize embedding space for both classiﬁcation
and low-level (pixel-level) similarity learning to ignore unknown pixel level per-
turbation. During training, we inject adversarial images without replacing their
corresponding clean images and penalize the distance between the two embed-
dings (clean and adversarial). Experimental results show that cascade adversarial
training together with our proposed low-level similarity learning efﬁciently en-
hances the robustness against iterative attacks, but at the expense of decreased ro-
bustness against one-step attacks. We show that combining those two techniques
can also improve robustness under the worst case black box attack scenario.

1

INTRODUCTION

Injecting adversarial examples during training (adversarial training), (Goodfellow et al., 2015; Ku-
rakin et al., 2017; Huang et al., 2015) increases the robustness of a network against adversarial
attacks. The networks trained with one-step methods have shown noticeable robustness against one-
step attacks, but, limited robustness against iterative attacks at test time. To address this challenge,
we have made the following contributions:

Cascade adversarial training: We ﬁrst show that iteratively generated adversarial images transfer
well between networks when the source and the target networks are trained with the same training
method. Inspired by this observation, we propose cascade adversarial training which transfers the
knowledge of the end results of adversarial training. In particular, we train a network by injecting
iter FGSM images (section 2.1) crafted from an already defended network (a network trained with
adversarial training) in addition to the one-step adversarial images crafted from the network being
trained. The concept of using already trained networks for adversarial training is also introduced in
(Tram`er et al., 2017). In their work, purely trained networks are used as another source networks
to generate one-step adversarial examples for training. On the contrary, our cascade adversarial
training uses already defended network for iter FGSM images generation.

Low level similarity learning: We advance the previous data augmentation approach (Kurakin
et al., 2017) by adding additional regularization in deep features to encourage a network to be insen-
sitive to adversarial perturbation. In particular, we inject adversarial images in the mini batch with-
out replacing their corresponding clean images and penalize distance between embeddings from the
clean and the adversarial examples. There are past examples of using embedding space for learning
similarity of high level features like face similarity between two different images (Schroff et al.,
Instead, we use the embedding space for learning
2015; Parkhi et al., 2015; Wen et al., 2016).

1

Published as a conference paper at ICLR 2018

similarity of the pixel level differences between two similar images. The intuition of using this
regularization is that small difference on input should not drastically change the high level feature
representation.

Analysis of adversarial training: We train ResNet models (He et al., 2016) on MNIST (Le-
Cun & Cortes, 2010) and CIFAR10 dataset (Krizhevsky, 2009) using the proposed adversarial
training. We ﬁrst show low level similarity learning improves robustness of the network against
adversarial images generated by one-step and iterative methods compared to the prior work.
We show that modifying the weight of the distance measure in the loss function can help con-
trol trade-off between accuracies for the clean and adversarial examples. Together with cascade
adversarial training and low-level similarity learning, we achieve accuracy increase against un-
known iterative attacks, but at the expense of decreased accuracy for one-step attacks. Finally,
we also show our cascade adversarial training and low level similarity learning provide much
better robustness against black box attack. Code to reproduce our experiments is available at
https://github.com/taesikna/cascade_adv_training.

2 BACKGROUND ON ADVERSARIAL ATTACKS

2.1 ATTACK METHODS

One-step fast gradient sign method (FGSM), referred to as “step FGSM”, generates adversarial
image X adv by adding sign of the gradients w.r.t. the clean image X multiplied by (cid:15) ∈ [0, 255] as
shown below (Goodfellow et al., 2015):

X adv = X + (cid:15) sign(∇X J(X, ytrue))

One-step target class method generates X adv by subtracting sign of the gradients computed on a
target false label as follows:

X adv = X − (cid:15) sign(∇X J(X, ytarget))

We use least likely class yLL as a target class and refer this method as “step ll”.

Basic iterative method, referred to as “iter FGSM”, applies FGSM with small α multiple times.
(cid:8)X adv

J(X adv

N −1 + α sign(∇X adv

0 = X, X adv

N −1, ytrue))(cid:9)

N = ClipX,(cid:15)

X adv

N −1

We use α = 1, number of iterations N to be min((cid:15) + 4, 1.25(cid:15)). ClipX,(cid:15) is elementwise clipping
function where the input is clipped to the range [max(0, X − (cid:15)), min(255, X + (cid:15))].

Iterative least-likely class method, referred to as “iter ll”, is to apply “step ll” with small α multi-
ple times.

X adv

0 = X, X adv

N = ClipX,(cid:15)

(cid:8)X adv

N −1 − α sign(∇X adv

N −1

J(X adv

N −1, yLL))(cid:9)

Carlini and Wagner attack (Carlini & Wagner, 2017) referred to as “CW” solves an optimization
problem which minimizes both an objective function f (such that attack is success if and only if
f (X adv) < 0) and a distance measure between X adv and X.

Black box attack is performed by testing accuracy on a target network with the adversarial images
crafted from a source network different from the target network. Lower accuracy means successful
black-box attack. When we use the same network for both target and source network, we call this
as white-box attack.

2.2 DEFENSE METHODS

Adversarial training (Kurakin et al., 2017):
is a form of data augmentation where it injects
adversarial examples during training. In this method, k examples are taken from the mini batch B
(size of m) and the adversarial examples are generated with one of step method. The k adversarial
examples replaces the corresponding clean examples when making mini batch. Below we refer this
adversarial training method as “Kurakin’s”.

2

Published as a conference paper at ICLR 2018

Table 1: CIFAR10 test results (%) under black box at-
tacks for (cid:15)=16. Source networks share the same ini-
tialization which is different from the target networks.
{Target: R20, R20K: standard, Kurakin’s, Source:
R202, R20K2: standard, Kurakin’s.}

Source: step FGSM Source: iter FGSM

Target

R202
16.2
R20
R20K 66.7

R20K2
31.6
82.7

R202
2.7
55.8

R20K2
60.1
28.5

Figure 1: Correlation between adversarial
noises from different networks for each (cid:15).
Shaded region shows ± 0.1 standard devi-
ation of each line.

Ensemble adversarial training (Tram`er et al., 2017): is essentially the same with the adversarial
training, but uses several pre-trained vanilla networks to generate one-step adversarial examples for
training. Below we refer this adversarial training method as “Ensemble”.

3 PROPOSED APPROACH

3.1 TRANSFERABILITY ANALYSIS

We ﬁrst show transferability between purely trained networks and adversarially trained networks
under black box attack. We use ResNet (He et al., 2016) models for CIFAR10 classiﬁcation. We ﬁrst
train 20-layer ResNets with different methods (standard training, adversarial training (Kurakin et al.,
2017)) and use those as target networks. We re-train networks (standard training and adversarial
training) with the different initialization from the target networks, and use the trained networks as
source networks. Experimental details and model descriptions can be found in Appendix A and B.
In table 1, we report test accuracies under black box attack.

Transferability (step attack): We ﬁrst observe that high robustness against one-step attack between
defended networks (R20K2 -> R20K), and low robustness between undefended networks (R202 -
> R20). This observation shows that error surfaces of neural networks are driven by the training
method and networks trained with the same method end up similar optimum states.

It is noteworthy to observe that the accuracies against step attack from the undefended network
(R202) are always lower than those from defended network (R20K2). Possible explanation for
this would be that adversarial training tweaks gradient seen from the clean image to point toward
weaker adversarial point along that gradient direction. As a result, one-step adversarial images from
defended networks become weaker than those from undefended network.

Transferability (iterative attack): We observe “iter FGSM” attack remains very strong even under
the black box attack scenario but only between undefended networks or defended networks. This is
because iter FGSM noises (X adv-X) from defended networks resemble each other. As shown in
ﬁgure 1, we observe higher correlation between iter FGSM noises from a defended network (R20K)
and those from another defended network (R20K2).

Difﬁculty of defense/attack under the black box attack scenario: As seen from this observation,
it is efﬁcient to attack an undefended/defended network with iter FGSM examples crafted from
another undefended/defended network. Thus, when we want to build a robust network under the
black box attack scenario, it is desired to check accuracies for the adversarial examples crafted from
other networks trained with the same strategy.

3.2 CASCADE ADVERSARIAL TRAINING

Inspired by the observation that iter FGSM images transfer well between defended networks, we
propose cascade adversarial training, which trains a network by injecting iter FGSM images crafted
from an already defended network. We hypothesize that the network being trained with cascade
adversarial training will learn to avoid such adversarial perturbation, enhancing robustness against
iter FGSM attack. The intuition behind this proposed method is that we transfer the knowledge of

3

Published as a conference paper at ICLR 2018

Figure 2: Cascade adversarial training regularized with a uniﬁed embedding.

Figure 3: (Left) Bidirectional loss. (Right) Pivot loss.

the end results of adversarial training. In particular, we train a network by injecting iter FGSM
images crafted from already defended network in addition to the one-step adversarial images crafted
from the network being trained.

3.3 REGULARIZATION WITH A UNIFIED EMBEDDING

We advance the algorithm proposed in (Kurakin et al., 2017) by adding low level similarity learning.
Unlike (Kurakin et al., 2017), we include the clean examples used for generating adversarial images
in the mini batch. Once one step forward pass is performed with the mini batch, embeddings are
followed by the softmax layer for the cross entropy loss for the standard classiﬁcation. At the same
time, we take clean embeddings and adversarial embeddings, and minimize the distance between
the two with the distance based loss.

The distance based loss encourages two similar images (clean and adversarial) to produce the same
outputs, not necessarily the true labels. Thus, low-level similarity learning can be considered as
an unsupervised learning. By adding regularization in higher embedding layer, convolution ﬁlters
gradually learn how to ignore such pixel-level perturbation. We have applied regularization on lower
layers with an assumption that low level pixel perturbation can be ignored in lower hierarchy of
networks. However, adding regularization term on higher embedding layer right before the softmax
layer showed best performance. The more convolutional ﬁlters have chance to learn such similarity,
the better the performance. Note that cross entropy doesn’t encourage two similar images to produce
the same output labels. Standard image classiﬁcation using cross entropy compares ground truth
labels with outputs of a network regardless of how similar training images are.

The entire training process combining cascade adversarial training and low level similarity learning
is shown in ﬁgure 2. We deﬁne the total loss as follows:

Loss =

1
(m − k) + λk

(cid:32) m−k
(cid:88)

i=1

L(Xi|yi) + λ

L(X adv
i

|yi)

+ λ2

Ldist(Eadv

i

, Ei)

k
(cid:88)

i=1

(cid:33)

k
(cid:88)

i=1

i

are the resulting embeddings from Xi and X adv

Ei and Eadv
, respectively. m is the size of the
mini batch, k (≤ m/2) is the number of adversarial images in the mini batch. λ is the parameter
to control the relative weight of classiﬁcation loss for adversarial images. λ2 is the parameter to
control the relative weight of the distance based loss Ldist in the total loss.

i

Bidirectional loss minimizes the distance between the two embeddings by moving both clean and
adversarial embeddings as shown in the left side of the ﬁgure 3.

Ldist(Eadv

i

, Ei) = ||Eadv

i − Ei||N

N , N ∈ 1, 2

i = 1, 2, ..., k

4

Published as a conference paper at ICLR 2018

Table 2: MNIST test results (%) for 20-layer ResNet models ((cid:15) = 0.3*255 at test time). { R20M:
standard training, R20MK: Kurakin’s adversarial training, R20MB: Bidirectional loss, R20MP :
P ivot loss.} CW L∞ attack is performed with 100 test samples (10 samples per each class) and the
number of adversarial examples with (cid:15) > 0.3*255 is reported. Additional details for CW attack can
be found in Appendix F

Model
R20M
R20MK
R20MB (Ours)
R20MP (Ours)

clean
99.6
99.6
99.5
99.5

step ll
9.7
96.7
97.3
97.1

step FGSM iter ll
0.0
89.0
97.2
96.9

10.3
94.5
96.2
95.7

iter FGSM CW

0.0
60.2
88.5
88.9

0
46
81
82

We tried N = 1, 2 and found not much difference between the two. We report the results with N =
2 for the rest of the paper otherwise noted. When N = 2, Ldist becomes L2 loss.

Pivot loss minimizes the distance between the two embeddings by moving only the adversarial
embeddings as shown in the right side of the ﬁgure 3.
i − Ei||N

|Ei) = ||Eadv

N , N ∈ 1, 2

Ldist(Eadv

i = 1, 2, ..., k

i

In this case, clean embeddings ( Ei ) serve as pivots to the adversarial embeddings. In particular,
we don’t back-propagate through the clean embeddings for the distance based loss. The intuition
behind the use of pivot loss is that the embedding from a clean image can be treated as the ground
truth embedding.

4 LOW LEVEL SIMILARITY LEARNING ANALYSIS

4.1 EXPERIMENTAL RESULTS ON MNIST

We ﬁrst analyze the effect of low level similarity learning on MNIST. We train ResNet models (He
et al., 2016) with different methods (standard training, Kurakin’s adversarial training and adversarial
training with our distance based loss). Experimental details can be found in Appendix A.

Table 2 shows the accuracy results for MNIST test dataset for different types of attack methods.
As shown in the table, our method achieves better accuracy than Kurakin’s method for all types
of attacks with a little sacriﬁce on the accuracy for the clean images. Even though adversarial
training is done only with “step ll”, additional regularization increases robustness against unknown
“step FGSM”, “iter ll”, “iter FGSM” and CW L∞ attacks. This shows that our low-level similarity
learning can successfully regularize the one-step adversarial perturbation and its vicinity for simple
image classiﬁcation like MNIST.

4.2 EMBEDDING SPACE VISUALIZATION

To visualize the embedding space, we modify 20-layer ResNet model where the last fully connected
layer (64x10) is changed to two fully connected layers (64x2 and 2x10). We re-train networks with
standard training, Kurakin’s method and our pivot loss on MNIST. 1 In ﬁgure 4, we draw embeddings
(dimension=2) between two fully connected layers. As seen from this ﬁgure, adversarial images
from the network trained with standard training cross the decision boundary easily as (cid:15) increases.
With Kurakin’s adversarial training, the distances between clean and adversarial embeddings are
minimized compared to standard training. And our pivot loss further minimizes distance between
the clean and adversarial embeddings. Note that our pivot loss also decreases absolute value of the
embeddings, thus, higher λ2 will eventually result in overlap between distinct embedding distribu-
tions. We also observe that intra class variation of the clean embeddings are also minimized for the
network trained with our pivot loss as shown in the scatter plot in ﬁgure 4 (c).

1Modiﬁed ResNet models showed slight decreased accuracy for both clean and adversarial images compared
to original ResNet counterparts, however, we observed similar trends (improved accuracy for iterative attacks
for the network trained with pivot loss) as in table 2.

5

Published as a conference paper at ICLR 2018

(a) Standard

(b) Kurakin

(c) P ivot (Ours)

Figure 4: Embedding space visualization for modiﬁed ResNet models trained on MNIST. x-axis
and y-axis show ﬁrst and second dimension of embeddings respectively. Scatter plot shows ﬁrst
100 clean embeddings per each class on MNIST test set. Each arrow shows difference between two
embeddings (one from iter FGSM image ((cid:15)) and the other from ((cid:15)+8)). We draw arrows from (cid:15) = 0
to (cid:15) = 76 (≈ 0.3*255) for one sample image per each class. We observe differences between clean
and corresponding adversarial embeddings are minimized for the network trained with pivot loss.

4.3 EFFECT OF λ2 ON CIFAR10

We train 20-layer ResNet models with pivot loss and various
λ2s for CIFAR10 dataset to study effects of the weight of the
distance measure in the loss function. Figure 5 shows that a
higher λ2 increases accuracy of the iteratively generated adver-
sarial images. However, it reduces accuracy on the clean im-
ages, and increasing λ2 above 0.3 even results in divergence of
the training. This is because embedding distributions of differ-
ent classes will eventually overlap since absolute value of the
embedding will be decreased as λ2 increases as seen from the
section 4.2. In this experiment, we show that there exists clear
trade-off between accuracy for the clean images and that for the
adversarial images, and we recommend using a very high λ2
only under strong adversarial environment.

5 CASCADE ADVERSARIAL TRAINING ANALYSIS

Figure 5: Accuracy vs. λ2

5.1 SOURCE NETWORK SELECTION

We further study the transferability
of iter FGSM images between vari-
ous architectures. To this end, we
ﬁrst train 56-layer ResNet networks
(Kurakin’s, pivot loss) with the same
initialization. Then we train another
56-layer ResNet network (Kurakin’s)
with different initialization. We re-
peat the training for the 110-layer
ResNet networks. We measure cor-
relation between iter FGSM noises
from different networks.

Figure 6 (a) shows correlation be-
tween iter FGSM noises crafted from
Kurakin’s network and those from
Pivot network with the same initial-
ization. Conjectured from (Kurakin
et al., 2017), we observe high corre-

(a) Corr: same initialization

(b) Corr: different initialization

Figure 6: Correlation between iter FGSM noises crafted
from different networks for each (cid:15). Correlation is averaged
over randomly chosen 128 images from CIFAR10 test-set.

6

Published as a conference paper at ICLR 2018

Table 3: CIFAR10 test results (%) for 110-layer ResNet models. CW L∞ attack is performed
with 100 test samples (10 samples per each class) and the number of adversarial examples with
(cid:15) > 2 or 4 is reported. {R110K: Kurakin’s, R110P : P ivot loss, R110E: Ensemble training,
R110K,C: Kurakin’s and Cascade training, R110P,E: P ivot loss and Ensemble training, and
R110P,C: P ivot loss and Cascade training}

Model

clean

R110K
R110P (Ours)
R110E
R110K,C (Ours)
R110P,E (Ours)
R110P,C (Ours)

92.3
92.3

92.3
92.3
91.3
91.5

step ll

step FGSM iter FGSM

CW

(cid:15)=2
88.3
86.0

86.3
86.2
84.0
85.7

(cid:15)=16
90.7
89.4

74.3
72.8
65.7
76.4

(cid:15)=2
86.0
81.6

84.1
82.6
77.6
82.4

(cid:15)=16
95.2
91.6

72.9
66.7
54.5
69.1

(cid:15)=2
59.4
64.1

63.5
69.3
66.8
73.5

(cid:15)=4
9.2
20.9

21.1
33.4
38.3
42.5

(cid:15)=2
25
32

24
20
38
27

(cid:15)=4
4
7

6
5
16
15

lation between iter FGSM noises from networks with the same initialization. Correlation between
iter FGSM noises from the networks with different initialization, however, becomes lower as the
network is deeper as shown in ﬁgure 6 (b). Since the degree of freedom increases as the network
size increases, adversarially trained networks prone to end up with different states, thus, making
transfer rate lower. To maximize the beneﬁt of the cascade adversarial training, we propose to use
the same initialization for a cascade network and a source network used for iterative adversarial
examples generation.

5.2 WHITE BOX ATTACK ANALYSIS

We ﬁrst compare a network trained with Kurakin’s method and that with pivot loss. We train 110-
layer ResNet models with/without pivot loss and report accuracy in table 3. We observe our low-
level similarity learning further improves robustness against iterative attacks compared to Kurakin’s
adversarial training. However, the accuracy improvements against iterative attacks (iter FGSM,
CW) are limited, showing regularization effect of low-level similarity learning is not sufﬁcient for
the iterative attacks on complex color images like CIFAR10. This is different from MNIST test
cases where we observed signiﬁcant accuracy increase for iterative attacks only with pivot loss. We
observe label leaking phenomenon reported in (Kurakin et al., 2017) happens even though we don’t
train a network with step FGSM images. Additional analysis for this phenomenon is explained in
Appendix D.

Next, we train a network from scratch with iter FGSM examples crafted from the defended network,
R110P . We use the same initialization used in R110P as discussed in 5.1. In particular, iter FGSM
images are crafted from R110P with CIFAR10 training images for (cid:15)= 1,2, ..., 16, and those are used
randomly together with step ll examples from the network being trained. We train cascade networks
with/without pivot loss. We also train networks with ensemble adversarial training (Tram`er et al.,
2017) with/without pivot loss for comparison. The implementation details for the trained models
can be found in Appendix B.

We ﬁnd several meaningful observations in table 3. First, ensemble and cascade models show im-
proved accuracy against iterative attack although at the expense of decreased accuracy for one-
step attacks compared to the baseline defended network (R110K). Additional data augmentation
from other networks enhances the robustness against iterative attack, weakening label leaking effect
caused by one-step adversarial training.

Second, our low-level similarity learning (R110P,E, R110P,C) further enhances robustness against
iterative attacks including fully unknown CW attack (especially for (cid:15)=4). Additional knowledge
learned from data augmentation through cascade/ensemble adversarial training enables networks to
learn partial knowledge of perturbations generated by an iterative method. And the learned iterative
perturbations become regularized further with our low-level similarity learning making networks
robust against unknown iterative attacks.

7

Published as a conference paper at ICLR 2018

Table 4: CIFAR10 test results (%) for 110-layer ResNet models under black box attacks ((cid:15)=16).
{Target: same networks in table 3 and R110K: Kurakin’s, Source: re-trained baseline, Kurakin’s,
cascade and ensemble networks with/without pivot loss. Source networks use the different initial-
ization from the target networks. Additional details of the models can be found in Appendix B.}

Source: iter FGSM

Target

R110K
R110E
R110P (Ours)
R110K,C (Ours)
R110P,E (Ours)
R110P,C (Ours)

R1102 R110K2 R110E2 R110P 2 R110K,C2 R110P,E2 R110P,C2
70.5
77.9
75.9
56.4
78.2
71.9

67.3
68.2
68.3
67.4
73.4
71.1

77.0
79.0
78.5
79.5
81.7
80.1

27.9
55.8
39.6
61.1
67.7
63.9

80.8
82.7
83.3
82.1
83.8
83.0

54.6
54.7
61.3
62.6
68.4
64.2

73.2
79.5
75.6
80.2
82.1
80.4

Third, our low-level similarity learning serves as a good regularizer for adversarial images, but
not for the clean images for ensemble/cascade models (reduced accuracy for the clean images for
R110P,E and R110P,C in table 3). Compared to pure adversarial training with one-step adversarial
examples, the network sees more various adversarial perturbations during training as a result of
ensemble and cascade training. Those perturbations are prone to end up embeddings in the vicinity
of decision boundary more often than perturbations caused by one-step adversarial training. Pivot
loss pulls the vicinity of those adversarial embeddings toward their corresponding clean embeddings.
During this process, clean embeddings from other classes might also be moved toward the decision
boundary which results in decreased accuracy for the clean images.

5.3 BLACK BOX ATTACK ANALYSIS

We ﬁnally perform black box attack analysis for the cascade/ensemble networks with/without pivot
loss. We report black box attack accuracy with the source networks trained with the same method,
but with different initialization from the target networks. The reason for this is adversarial ex-
amples transfer well between networks trained with the same strategy as observed in section 3.1.
We re-train 110-layer ResNet models using Kurakin’s, cascade and ensemble adversarial training
with/without low-level similarity learning and use those networks as source networks for black-box
attacks. Baseline 110-layer ResNet model is also included as a source network. Target networks
are the same networks used in table 3. We found iter FGSM attack resulted in lower accuracy than
step FGSM attack, thus, report iter FGSM attack only in table 4.

We ﬁrst observe that iter FGSM attack from ensemble models (R110E2, R110P,E2) is strong (re-
sults in lower accuracy) compared to that from any other trained networks. 2 Since ensemble models
learn various perturbation during training, adversarial noises crafted from those networks might be
more general for other networks making them transfer easily between defended networks.

Second, cascade adversarial training breaks chicken and egg problem. (In section 3.1, we found that
it is efﬁcient to use a defended network as a source network to attack another defended network.)
Even though the transferability between defended networks is reduced for deeper networks, cascade
network (R110K,C) shows worst case performance against the attack not from a defended network,
but from a purely trained network (R1102). Possible solution to further improve the worst case
robustness would be to use more than one network as source networks (including pure/defended
networks) for iter FGSM images generation for cascade adversarial training.

Third, ensemble/cascade networks together with our low-level similarity learning (R110P,E,
R110P,C) show better worst case accuracy under black box attack scenario. This shows that enhanc-
ing robustness against iterative white box attack also improves robustness against iterative black box
attack.

2We also observed this when we switch the source and the target networks. Additional details can be found

in Appendix E.

8

Published as a conference paper at ICLR 2018

6 CONCLUSION

We performed through transfer analysis and showed iter FGSM images transfer easily between net-
works trained with the same strategy. We exploited this and proposed cascade adversarial training,
a method to train a network with iter FGSM adversarial images crafted from already defended net-
works. We also proposed adversarial training regularized with a uniﬁed embedding for classiﬁcation
and low- level similarity learning by penalizing distance between the clean and their corresponding
adversarial embeddings. Combining those two techniques (low level similarity learning + cascade
adversarial training) with deeper networks further improved robustness against iterative attacks for
both white-box and black-box attacks.

However, there is still a gap between accuracy for the clean images and that for the adversarial
images. Improving robustness against both one-step and iterative attacks still remains challenging
since it is shown to be difﬁcult to train networks robust for both one-step and iterative attacks si-
multaneously. Future research is necessary to further improve the robustness against iterative attack
without sacriﬁcing the accuracy for step attacks or clean images under both white-box attack and
black-box attack scenarios.

ACKNOWLEDGMENTS

We thank Alexey Kurakin of Google Brain and Li Chen of Intel for comments that greatly improved
the manuscript. The research reported here was supported in part by the Defense Advanced Research
Projects Agency (DARPA) under contract number HR0011-17-2-0045. The views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing
the ofﬁcial policies or endorsements, either expressed or implied, of DARPA.

REFERENCES

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE

Symposium on Security and Privacy, 2017.

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Proceedings of the International Conference on Learning Representations (ICLR),
2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,
nition.
Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90. URL
http://dx.doi.org/10.1109/CVPR.2016.90.

Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesv´ari. Learning with a strong adver-

sary. CoRR, abs/1511.03034, 2015. URL http://arxiv.org/abs/1511.03034.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In

Proceedings of the International Conference on Learning Representations (ICLR), 2017.

Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.

lecun.com/exdb/mnist/.

O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proceedings of the British

Machine Vision Conference (BMVC), 2015.

Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face
recognition and clustering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015.

Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensem-
ble adversarial training: Attacks and defenses. CoRR, abs/1705.07204, 2017. URL http:
//arxiv.org/abs/1705.07204.

9

Published as a conference paper at ICLR 2018

Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning ap-
In Computer Vision - ECCV 2016 - 14th European Con-
proach for deep face recognition.
ference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VII, pp. 499–
515, 2016. doi: 10.1007/978-3-319-46478-7 31. URL http://dx.doi.org/10.1007/
978-3-319-46478-7_31.

10

Published as a conference paper at ICLR 2018

A EXPERIMENTAL SETUP

We scale down the image values to [0,1] and don’t perform any data augmentation for MNIST. For
CIFAR10, we scale down the image values to [0,1] and subtract per-pixel mean values. We perform
24x24 random crop and random ﬂip on 32x32 original images. 3 We generate adversarial images
with “step ll” after these steps unless otherwise noted.

For adversarial training, we generate k = 64 adversarial examples among 128 images in one mini-
batch. As in Kurakin et al. (2017), we use randomly chosen (cid:15) in the interval [0, max e] with clipped
normal distribution N (µ = 0, σ = max e/2), where max e is the maximum (cid:15) used in training.
We use max e = 0.3*255 and 16 for MNIST and CIFAR10 respectively. We use λ = 0.3 and λ2 =
0.0001 in the loss function.

We use stochastic gradient descent (SGD) optimizer with momentum of 0.9, weight decay of 0.0001.
We start with a learning rate of 0.1, divide it by 10 at 4k and 6k iterations, and terminate training
at 8k iterations for MNIST, and 48k and 72k iterations, and terminate training at 94k iterations for
CIFAR10. 4

We also found that initialization affects the training results slightly as in (Kurakin et al., 2017), thus,
we pre-train the networks 2 and 10 epochs for MNIST and CIFAR10, and use these as initial starting
points for different conﬁgurations.

B MODEL DESCRIPTIONS

We summarize the model names used in this paper in table 5. For ensemble adversarial training,
pre-trained networks as in table 7 together with the network being trained are used to generate one-
step adversarial examples during training. For cascade adversarial training, pre-trained defended
networks as in table 6 are used to generate iter FGSM images, and the network being trained is used
to generate one-step adversarial examples during training.

3Original paper (He et al., 2016) performed 32x32 random crop on zero-padded 40x40 images.
4We found that the adversarial training requires longer training time than the standard training. Authors in
the original paper (He et al., 2016) changed the learning rate at 32k and 48k iterations and terminated training
at 64k iterations.

11

Published as a conference paper at ICLR 2018

Table 5: Model descriptions

Dataset

ResNet

Initialization
Group

MNIST

20-layer

A

CIFAR10

56-layer

20-layer

110-layer

standard training

standard training

standard training
Kurakin’s
P ivot loss

Training
standard training
Kurakin’s
Bidirection loss
P ivot loss

standard training
Kurakin’s
Ensemble training
Bidirection loss
P ivot loss
Kurakin’s & Cascade training
P ivot loss & Ensemble training
P ivot loss & Cascade training

Model
R20M
R20MK
R20MB
R20MP
R20
R20K
R20E
R20B
R20P
R20K,C
R20P,E
R20P,C
R202
R20K2
R20P 2
R203
R204
R56K
R56P
R56K2
R110
R110K
R110P
R110E
R110K,C
R110P,E
R110P,C
R1102
standard training
Kurakin’s
R110K2
Ensemble training
R110E2
P ivot loss
R110P 2
Kurakin’s & Cascade training
R110K,C2
P ivot loss & Ensemble training R110P,E2
P ivot loss & Cascade training
R110P,C2
R1103
R1104

standard training
Kurakin’s
P ivot loss
Ensemble training
Kurakin’s & Cascade training
P ivot loss & Ensemble training
P ivot loss & Cascade training

Kurakin’s
P ivot loss

standard training

standard training

Kurakin’s

B

C

D

E

F

G

H

I

J

K

Table 6: Ensemble model description

Ensemble models
R20E, R20P,E, R110E, R110P,E
R110E2, R110P,E2

Pre-trained models
R203, R1103
R204, R1104

Table 7: Cascade model description

Cascade models
R20K,C, R20P,C
R110K,C, R110P,C
R110K,C2, R110P,C2

Pre-trained model
R20P
R110P
R110P 2

12

Published as a conference paper at ICLR 2018

C ALTERNATIVE VISUALIZATION ON EMBEDDINGS

(a) R20, step ll

(b) R20K , step ll

(c) R20P (Ours), step ll

(d) R20, step FGSM

(e) R20K , step FGSM

(f) R20P (Ours), step FGSM

(g) R20, random sign

(h) R20K , random sign

(i) R20P (Ours), random sign

Figure 7: Argument to the softmax vs. (cid:15) in test time. “step ll”, “step FGSM” and ”random sign”
methods were used to generate test-time adversarial images. Arguments to the softmax were mea-
sured by changing (cid:15) for each test method and averaged over randomly chosen 128 images from
CIFAR10 test-set. Blue line represents true class and the red line represents mean of the false
classes. Shaded region shows ± 1 standard deviation of each line.

We draw average value of the argument to the softmax layer for the true class and the false classes
to visualize how the adversarial training works as in ﬁgure 7. Standard training, as expected, shows
dramatic drop in the values for the true class as we increase (cid:15) in “step ll” or “step FGSM direction.
With adversarial training, we observe that the value drop is limited at small (cid:15) and our method even
increases the value in certain range upto (cid:15)=10. Note that adversarial training is not the same as
the gradient masking.As illustrated in ﬁgure 7, it exposes gradient information, however, quickly
distort gradients along the sign of the gradient (“step ll” or “step FGSM) direction. We also observe
improved results (broader margins than baseline) for “random sign” added images even though
we didn’t inject random sign added images during training. Overall shape of the argument to the
softmax layer in our case becomes smoother than Kurakin’s method, suggesting our method is good
for pixel level regularization. Even though actual value of the embeddings for the true class in our
case is smaller than that in Kurakin’s, the standard deviation of our case is less than Kurakin’s,
making better margin between the true class and false classes.

13

Published as a conference paper at ICLR 2018

D LABEL LEAKING ANALYSIS

(a) R20: Baseline

(b) R20K : Kurakin’s

(c) R20P : Pivot (Ours)

Figure 8: Averaged Pearson’s correlation coefﬁcient between the gradients w.r.t. two images. Corre-
lation were measured by changing (cid:15) for each adversarial image and averaged over randomly chosen
128 images from CIFAR10 test-set. Shaded region represents ± 0.5 standard deviation of each line.

We observe accuracies for the “step FGSM” adversarial images become higher than those for the
clean images (“label leaking” phenomenon) by training with “step FGSM” examples as in (Kurakin
et al., 2017). Interestingly, we also observe “label leaking” phenomenon even without providing
true labels for adversarial images generation. We argue that “label leaking” is a natural result of the
adversarial training.

To understand the nature of adversarial training, we measure correlation between gradients w.r.t.
different images (i.e. clean vs. adversarial) as a measure of error surface similarity. We measure
correlation between gradients w.r.t. (1) clean vs. “step ll” image, (2) clean vs. “step FGSM” image,
(3) clean vs. “random sign” added image, and (4) “step ll” image vs. “step FGSM” image for three
trained networks (a) R20, (b) R20K and (c) R20P (Ours) in table 5. Figure 8 draws average value
of correlations for each case.

Meaning of the correlation: In order to make strong adversarial images with “step FGSM” method,
correlation between the gradient w.r.t.
its corresponding
adversarial image should remain high since the “step FGSM” method only use the gradient w.r.t.
the clean image ((cid:15)=0). Lower correlation means perturbing the adversarial image at (cid:15) further to the
gradient (seen from the clean image) direction is no longer efﬁcient.

the clean image and the gradient w.r.t.

Results of adversarial training: We observe that (1) and (2) become quickly lower than (3) as
(cid:15) increases. This means that, when we move toward the steepest (gradient) direction on the error
surface, gradient is more quickly uncorrelated with the gradient w.r.t. the clean image than when
we move to random direction. As a result of adversarial training, this uncorrelation is observed at a
lower (cid:15) making one-step attack less efﬁcient even with small perturbation. (1), (2) and (3) for our
case are slightly lower than Kurakin’s method at the same (cid:15) which means that our method is better
at defending one-step attacks than Kurakin’s.

Error surface similarity between “step ll” and “step FGSM” images: We also observe (4) re-
mains high with higher (cid:15) for all trained networks. This means that the error surface (gradient) of
the “step ll” image and that of its corresponding “step FGSM” image resemble each other. That is
the reason why we get the robustness against “step FGSM” method only by training with “step ll”
(4) for our case is slightly higher than Kurakin’s method at the same (cid:15)
method and vice versa.
and that means our similarity learning tends to make error surfaces of the adversarial images with
“step ll” and “step FGSM” method to be more similar.

Analysis of label leaking phenomenon: Interestingly, (2) becomes slightly negative in certain
range (1 < (cid:15) < 3 for Kurakin’s, and 1 < (cid:15) < 4 for Pivot (Ours) ) and this could be the possible
reason for “label leaking” phenomenon. For example, let’s assume that we have a perturbed image
(by “step FGSM” method) at (cid:15) where the correlation between the gradients w.r.t. that image and the
corresponding clean image is negative. Further increase of (cid:15) with the gradient (w.r.t. the clean image)
direction actually decreases the loss resulting in increased accuracy (label leaking phenomenon).
Due to the error surface similarity between “step ll” and “step FGSM” images and this negative

14

Published as a conference paper at ICLR 2018

correlation effect, however, label leaking phenomenon can always happen for the networks trained
with one-step adversarial examples.

E ADDITIONAL BLACK BOX ATTACK RESULTS

Table 8: CIFAR10 test results (%) under black box attacks between the network with the same
initialization ((cid:15)=16)}

Target

Source: step FGSM

Source: iter FGSM

R20 R20K R20P
12.2
27.5
27.4
R20
R20K 65.7
81.8
81.5
58.1
91.7
89.3
R20P

R20 R20K R20P
44.7
45.9
0.0
18.2
0.0
51.5
13.4
0.0
48.9

Table 8 shows that black box attack between trained networks with the same initialization tends to
be more successful than that between networks with different initialization as explained in (Kurakin
et al., 2017).

Table 9: CIFAR10 test results (%) under black box attacks for (cid:15)=16. {Target and Source networks
are switched from the table 1}

Target

R202
R20K2
R20P 2

Source: step FGSM

Source: iter FGSM

R20 R20K R20P
17.9
34.5
33.9
65.0
84.5
84.6
66.4
87.2
88.2

R20 R20K R20P
4.1
54.3
54.8
30.4
25.3
61.2
36.1
27.7
61.6

In table 9, our method (R20P 2) is always better at one-step and iterative black box attack from
defended networks (R20K, R20P ) and undefended network (R20) than Kurakin’s method (R20B2).
However, it is hard to tell which method is better than the other one as explained in the main paper.

Table 10: CIFAR10 test results (%) for cascade networks under black box attacks for (cid:15)=16. {Target
and Source: Please see the model descriptions in Appendix B.}

Target

Source: iter FGSM

R110 R110K R110E R110P R110K,C R110P,E R110P,C
80.5
R110K2
82.7
R110E2
R110P 2 (Ours)
80.3
R110K,C2 (Ours) 62.1
81.5
R110P,E2 (Ours)
72.2
R110P,C2 (Ours)

68.0
59.6
72.2
72.3
77.3
73.8

72.7
59.1
75.9
74.7
79.0
76.4

49.3
39.5
54.2
61.5
50.0
60.6

49.6
51.5
54.9
46.5
56.9
51.0

67.9
69.6
72.4
67.9
75.2
72.0

41.0
40.3
44.3
39.0
45.5
40.9

In table 10, we show black box attack accuracies with the source and the target networks switched
from the table 4. We also observe that networks trained with both low-level similarity learning and
cascade/ensemble adversarial training (R110P,C2, R110P,E2) show better worst-case performance
than other networks. Overall, iter FGSM images crafted from ensemble model families (R110E,
R110P,E) remain strong on the defended networks.

15

Published as a conference paper at ICLR 2018

F IMPLEMENTATION DETAILS FOR CARLINI-WAGNER L∞ ATTACK

(a) MNIST

(b) CIFAR10, 20-layer

(c) CIFAR10, 110-layer

Figure 9: Cumulative distribution function vs. (cid:15) for 100 test adversarial examples generated by CW
L∞ attack. Lower CDF value for a ﬁxed (cid:15) means the better defense.

Carlini and Wagner (CW) L∞ attack solves the following optimization problem for every input X.

minimize

c · f (X + δ) +

[(|δi| − τ )+]

(cid:88)

i

such that X + δ ∈ [0, 1]n
where, the function f is deﬁned such that attack is success if and only if f (X + δ) < 0, δ is the
target perturbation deﬁned as X adv −X, c is the parameter to control the relative weight of function
f in the total cost function, and τ is the control threshold used to penalize any terms that exceed τ .

Since CW L∞ attack is computationally expensive, we only use 100 test examples (10 exam-
ples per each class). We search adversarial example X adv with c ∈ {0.1, 0.2, 0.5, 1, 2, 5, 10, 20}
and τ ∈ {0.02, 0.04, ..., 0.6} for MNIST and c ∈ {0.1, 0.3, 1, 3, 10, 30, 100} and τ ∈
{0.001, 0.002, ..., 0.01, 0.012, ..., 0.02, 0.024, ..., 0.04, 0.048, ..., 0.08} for CIFAR10. We use Adam
optimizer with an initial learning rate of 0.01/c since we found constant initial learning rate for
c · f (X + δ) term is critical for successful adversarial images generation. We terminate the search
after 2,000 iterations for each X, c and τ . If f (X + δ) < 0 and the resulting ||δ||∞ is lower than
the current best distance, we update X adv.

Figure 9 shows cumulative distribution function of (cid:15) for 100 successful adversarial examples per
each network. We report the number of adversarial examples with (cid:15) > 0.3*255 for MNIST and
that with (cid:15) > 2 or 4 for CIFAR10. As seen from this ﬁgure, our approaches provide robust defense
against CW L∞ attack compared to other approaches.

16

8
1
0
2
 
r
a

M
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
8
5
2
0
.
8
0
7
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

CASCADE ADVERSARIAL MACHINE LEARNING REG-
ULARIZED WITH A UNIFIED EMBEDDING

Taesik Na, Jong Hwan Ko & Saibal Mukhopadhyay
School of Electrical and Computer Engineering
Georgia Institute of Technology
Atlanta, GA 30332, USA
{taesik.na, jonghwan.ko, smukhopadhyay6}@gatech.edu

ABSTRACT

Injecting adversarial examples during training, known as adversarial training, can
improve robustness against one-step attacks, but not for unknown iterative at-
tacks. To address this challenge, we ﬁrst show iteratively generated adversarial
images easily transfer between networks trained with the same strategy. Inspired
by this observation, we propose cascade adversarial training, which transfers the
knowledge of the end results of adversarial training. We train a network from
scratch by injecting iteratively generated adversarial images crafted from already
defended networks in addition to one-step adversarial images from the network
being trained. We also propose to utilize embedding space for both classiﬁcation
and low-level (pixel-level) similarity learning to ignore unknown pixel level per-
turbation. During training, we inject adversarial images without replacing their
corresponding clean images and penalize the distance between the two embed-
dings (clean and adversarial). Experimental results show that cascade adversarial
training together with our proposed low-level similarity learning efﬁciently en-
hances the robustness against iterative attacks, but at the expense of decreased ro-
bustness against one-step attacks. We show that combining those two techniques
can also improve robustness under the worst case black box attack scenario.

1

INTRODUCTION

Injecting adversarial examples during training (adversarial training), (Goodfellow et al., 2015; Ku-
rakin et al., 2017; Huang et al., 2015) increases the robustness of a network against adversarial
attacks. The networks trained with one-step methods have shown noticeable robustness against one-
step attacks, but, limited robustness against iterative attacks at test time. To address this challenge,
we have made the following contributions:

Cascade adversarial training: We ﬁrst show that iteratively generated adversarial images transfer
well between networks when the source and the target networks are trained with the same training
method. Inspired by this observation, we propose cascade adversarial training which transfers the
knowledge of the end results of adversarial training. In particular, we train a network by injecting
iter FGSM images (section 2.1) crafted from an already defended network (a network trained with
adversarial training) in addition to the one-step adversarial images crafted from the network being
trained. The concept of using already trained networks for adversarial training is also introduced in
(Tram`er et al., 2017). In their work, purely trained networks are used as another source networks
to generate one-step adversarial examples for training. On the contrary, our cascade adversarial
training uses already defended network for iter FGSM images generation.

Low level similarity learning: We advance the previous data augmentation approach (Kurakin
et al., 2017) by adding additional regularization in deep features to encourage a network to be insen-
sitive to adversarial perturbation. In particular, we inject adversarial images in the mini batch with-
out replacing their corresponding clean images and penalize distance between embeddings from the
clean and the adversarial examples. There are past examples of using embedding space for learning
similarity of high level features like face similarity between two different images (Schroff et al.,
Instead, we use the embedding space for learning
2015; Parkhi et al., 2015; Wen et al., 2016).

1

Published as a conference paper at ICLR 2018

similarity of the pixel level differences between two similar images. The intuition of using this
regularization is that small difference on input should not drastically change the high level feature
representation.

Analysis of adversarial training: We train ResNet models (He et al., 2016) on MNIST (Le-
Cun & Cortes, 2010) and CIFAR10 dataset (Krizhevsky, 2009) using the proposed adversarial
training. We ﬁrst show low level similarity learning improves robustness of the network against
adversarial images generated by one-step and iterative methods compared to the prior work.
We show that modifying the weight of the distance measure in the loss function can help con-
trol trade-off between accuracies for the clean and adversarial examples. Together with cascade
adversarial training and low-level similarity learning, we achieve accuracy increase against un-
known iterative attacks, but at the expense of decreased accuracy for one-step attacks. Finally,
we also show our cascade adversarial training and low level similarity learning provide much
better robustness against black box attack. Code to reproduce our experiments is available at
https://github.com/taesikna/cascade_adv_training.

2 BACKGROUND ON ADVERSARIAL ATTACKS

2.1 ATTACK METHODS

One-step fast gradient sign method (FGSM), referred to as “step FGSM”, generates adversarial
image X adv by adding sign of the gradients w.r.t. the clean image X multiplied by (cid:15) ∈ [0, 255] as
shown below (Goodfellow et al., 2015):

X adv = X + (cid:15) sign(∇X J(X, ytrue))

One-step target class method generates X adv by subtracting sign of the gradients computed on a
target false label as follows:

X adv = X − (cid:15) sign(∇X J(X, ytarget))

We use least likely class yLL as a target class and refer this method as “step ll”.

Basic iterative method, referred to as “iter FGSM”, applies FGSM with small α multiple times.
(cid:8)X adv

J(X adv

N −1 + α sign(∇X adv

0 = X, X adv

N −1, ytrue))(cid:9)

N = ClipX,(cid:15)

X adv

N −1

We use α = 1, number of iterations N to be min((cid:15) + 4, 1.25(cid:15)). ClipX,(cid:15) is elementwise clipping
function where the input is clipped to the range [max(0, X − (cid:15)), min(255, X + (cid:15))].

Iterative least-likely class method, referred to as “iter ll”, is to apply “step ll” with small α multi-
ple times.

X adv

0 = X, X adv

N = ClipX,(cid:15)

(cid:8)X adv

N −1 − α sign(∇X adv

N −1

J(X adv

N −1, yLL))(cid:9)

Carlini and Wagner attack (Carlini & Wagner, 2017) referred to as “CW” solves an optimization
problem which minimizes both an objective function f (such that attack is success if and only if
f (X adv) < 0) and a distance measure between X adv and X.

Black box attack is performed by testing accuracy on a target network with the adversarial images
crafted from a source network different from the target network. Lower accuracy means successful
black-box attack. When we use the same network for both target and source network, we call this
as white-box attack.

2.2 DEFENSE METHODS

Adversarial training (Kurakin et al., 2017):
is a form of data augmentation where it injects
adversarial examples during training. In this method, k examples are taken from the mini batch B
(size of m) and the adversarial examples are generated with one of step method. The k adversarial
examples replaces the corresponding clean examples when making mini batch. Below we refer this
adversarial training method as “Kurakin’s”.

2

Published as a conference paper at ICLR 2018

Table 1: CIFAR10 test results (%) under black box at-
tacks for (cid:15)=16. Source networks share the same ini-
tialization which is different from the target networks.
{Target: R20, R20K: standard, Kurakin’s, Source:
R202, R20K2: standard, Kurakin’s.}

Source: step FGSM Source: iter FGSM

Target

R202
16.2
R20
R20K 66.7

R20K2
31.6
82.7

R202
2.7
55.8

R20K2
60.1
28.5

Figure 1: Correlation between adversarial
noises from different networks for each (cid:15).
Shaded region shows ± 0.1 standard devi-
ation of each line.

Ensemble adversarial training (Tram`er et al., 2017): is essentially the same with the adversarial
training, but uses several pre-trained vanilla networks to generate one-step adversarial examples for
training. Below we refer this adversarial training method as “Ensemble”.

3 PROPOSED APPROACH

3.1 TRANSFERABILITY ANALYSIS

We ﬁrst show transferability between purely trained networks and adversarially trained networks
under black box attack. We use ResNet (He et al., 2016) models for CIFAR10 classiﬁcation. We ﬁrst
train 20-layer ResNets with different methods (standard training, adversarial training (Kurakin et al.,
2017)) and use those as target networks. We re-train networks (standard training and adversarial
training) with the different initialization from the target networks, and use the trained networks as
source networks. Experimental details and model descriptions can be found in Appendix A and B.
In table 1, we report test accuracies under black box attack.

Transferability (step attack): We ﬁrst observe that high robustness against one-step attack between
defended networks (R20K2 -> R20K), and low robustness between undefended networks (R202 -
> R20). This observation shows that error surfaces of neural networks are driven by the training
method and networks trained with the same method end up similar optimum states.

It is noteworthy to observe that the accuracies against step attack from the undefended network
(R202) are always lower than those from defended network (R20K2). Possible explanation for
this would be that adversarial training tweaks gradient seen from the clean image to point toward
weaker adversarial point along that gradient direction. As a result, one-step adversarial images from
defended networks become weaker than those from undefended network.

Transferability (iterative attack): We observe “iter FGSM” attack remains very strong even under
the black box attack scenario but only between undefended networks or defended networks. This is
because iter FGSM noises (X adv-X) from defended networks resemble each other. As shown in
ﬁgure 1, we observe higher correlation between iter FGSM noises from a defended network (R20K)
and those from another defended network (R20K2).

Difﬁculty of defense/attack under the black box attack scenario: As seen from this observation,
it is efﬁcient to attack an undefended/defended network with iter FGSM examples crafted from
another undefended/defended network. Thus, when we want to build a robust network under the
black box attack scenario, it is desired to check accuracies for the adversarial examples crafted from
other networks trained with the same strategy.

3.2 CASCADE ADVERSARIAL TRAINING

Inspired by the observation that iter FGSM images transfer well between defended networks, we
propose cascade adversarial training, which trains a network by injecting iter FGSM images crafted
from an already defended network. We hypothesize that the network being trained with cascade
adversarial training will learn to avoid such adversarial perturbation, enhancing robustness against
iter FGSM attack. The intuition behind this proposed method is that we transfer the knowledge of

3

Published as a conference paper at ICLR 2018

Figure 2: Cascade adversarial training regularized with a uniﬁed embedding.

Figure 3: (Left) Bidirectional loss. (Right) Pivot loss.

the end results of adversarial training. In particular, we train a network by injecting iter FGSM
images crafted from already defended network in addition to the one-step adversarial images crafted
from the network being trained.

3.3 REGULARIZATION WITH A UNIFIED EMBEDDING

We advance the algorithm proposed in (Kurakin et al., 2017) by adding low level similarity learning.
Unlike (Kurakin et al., 2017), we include the clean examples used for generating adversarial images
in the mini batch. Once one step forward pass is performed with the mini batch, embeddings are
followed by the softmax layer for the cross entropy loss for the standard classiﬁcation. At the same
time, we take clean embeddings and adversarial embeddings, and minimize the distance between
the two with the distance based loss.

The distance based loss encourages two similar images (clean and adversarial) to produce the same
outputs, not necessarily the true labels. Thus, low-level similarity learning can be considered as
an unsupervised learning. By adding regularization in higher embedding layer, convolution ﬁlters
gradually learn how to ignore such pixel-level perturbation. We have applied regularization on lower
layers with an assumption that low level pixel perturbation can be ignored in lower hierarchy of
networks. However, adding regularization term on higher embedding layer right before the softmax
layer showed best performance. The more convolutional ﬁlters have chance to learn such similarity,
the better the performance. Note that cross entropy doesn’t encourage two similar images to produce
the same output labels. Standard image classiﬁcation using cross entropy compares ground truth
labels with outputs of a network regardless of how similar training images are.

The entire training process combining cascade adversarial training and low level similarity learning
is shown in ﬁgure 2. We deﬁne the total loss as follows:

Loss =

1
(m − k) + λk

(cid:32) m−k
(cid:88)

i=1

L(Xi|yi) + λ

L(X adv
i

|yi)

+ λ2

Ldist(Eadv

i

, Ei)

k
(cid:88)

i=1

(cid:33)

k
(cid:88)

i=1

i

are the resulting embeddings from Xi and X adv

Ei and Eadv
, respectively. m is the size of the
mini batch, k (≤ m/2) is the number of adversarial images in the mini batch. λ is the parameter
to control the relative weight of classiﬁcation loss for adversarial images. λ2 is the parameter to
control the relative weight of the distance based loss Ldist in the total loss.

i

Bidirectional loss minimizes the distance between the two embeddings by moving both clean and
adversarial embeddings as shown in the left side of the ﬁgure 3.

Ldist(Eadv

i

, Ei) = ||Eadv

i − Ei||N

N , N ∈ 1, 2

i = 1, 2, ..., k

4

Published as a conference paper at ICLR 2018

Table 2: MNIST test results (%) for 20-layer ResNet models ((cid:15) = 0.3*255 at test time). { R20M:
standard training, R20MK: Kurakin’s adversarial training, R20MB: Bidirectional loss, R20MP :
P ivot loss.} CW L∞ attack is performed with 100 test samples (10 samples per each class) and the
number of adversarial examples with (cid:15) > 0.3*255 is reported. Additional details for CW attack can
be found in Appendix F

Model
R20M
R20MK
R20MB (Ours)
R20MP (Ours)

clean
99.6
99.6
99.5
99.5

step ll
9.7
96.7
97.3
97.1

step FGSM iter ll
0.0
89.0
97.2
96.9

10.3
94.5
96.2
95.7

iter FGSM CW

0.0
60.2
88.5
88.9

0
46
81
82

We tried N = 1, 2 and found not much difference between the two. We report the results with N =
2 for the rest of the paper otherwise noted. When N = 2, Ldist becomes L2 loss.

Pivot loss minimizes the distance between the two embeddings by moving only the adversarial
embeddings as shown in the right side of the ﬁgure 3.
i − Ei||N

|Ei) = ||Eadv

N , N ∈ 1, 2

Ldist(Eadv

i = 1, 2, ..., k

i

In this case, clean embeddings ( Ei ) serve as pivots to the adversarial embeddings. In particular,
we don’t back-propagate through the clean embeddings for the distance based loss. The intuition
behind the use of pivot loss is that the embedding from a clean image can be treated as the ground
truth embedding.

4 LOW LEVEL SIMILARITY LEARNING ANALYSIS

4.1 EXPERIMENTAL RESULTS ON MNIST

We ﬁrst analyze the effect of low level similarity learning on MNIST. We train ResNet models (He
et al., 2016) with different methods (standard training, Kurakin’s adversarial training and adversarial
training with our distance based loss). Experimental details can be found in Appendix A.

Table 2 shows the accuracy results for MNIST test dataset for different types of attack methods.
As shown in the table, our method achieves better accuracy than Kurakin’s method for all types
of attacks with a little sacriﬁce on the accuracy for the clean images. Even though adversarial
training is done only with “step ll”, additional regularization increases robustness against unknown
“step FGSM”, “iter ll”, “iter FGSM” and CW L∞ attacks. This shows that our low-level similarity
learning can successfully regularize the one-step adversarial perturbation and its vicinity for simple
image classiﬁcation like MNIST.

4.2 EMBEDDING SPACE VISUALIZATION

To visualize the embedding space, we modify 20-layer ResNet model where the last fully connected
layer (64x10) is changed to two fully connected layers (64x2 and 2x10). We re-train networks with
standard training, Kurakin’s method and our pivot loss on MNIST. 1 In ﬁgure 4, we draw embeddings
(dimension=2) between two fully connected layers. As seen from this ﬁgure, adversarial images
from the network trained with standard training cross the decision boundary easily as (cid:15) increases.
With Kurakin’s adversarial training, the distances between clean and adversarial embeddings are
minimized compared to standard training. And our pivot loss further minimizes distance between
the clean and adversarial embeddings. Note that our pivot loss also decreases absolute value of the
embeddings, thus, higher λ2 will eventually result in overlap between distinct embedding distribu-
tions. We also observe that intra class variation of the clean embeddings are also minimized for the
network trained with our pivot loss as shown in the scatter plot in ﬁgure 4 (c).

1Modiﬁed ResNet models showed slight decreased accuracy for both clean and adversarial images compared
to original ResNet counterparts, however, we observed similar trends (improved accuracy for iterative attacks
for the network trained with pivot loss) as in table 2.

5

Published as a conference paper at ICLR 2018

(a) Standard

(b) Kurakin

(c) P ivot (Ours)

Figure 4: Embedding space visualization for modiﬁed ResNet models trained on MNIST. x-axis
and y-axis show ﬁrst and second dimension of embeddings respectively. Scatter plot shows ﬁrst
100 clean embeddings per each class on MNIST test set. Each arrow shows difference between two
embeddings (one from iter FGSM image ((cid:15)) and the other from ((cid:15)+8)). We draw arrows from (cid:15) = 0
to (cid:15) = 76 (≈ 0.3*255) for one sample image per each class. We observe differences between clean
and corresponding adversarial embeddings are minimized for the network trained with pivot loss.

4.3 EFFECT OF λ2 ON CIFAR10

We train 20-layer ResNet models with pivot loss and various
λ2s for CIFAR10 dataset to study effects of the weight of the
distance measure in the loss function. Figure 5 shows that a
higher λ2 increases accuracy of the iteratively generated adver-
sarial images. However, it reduces accuracy on the clean im-
ages, and increasing λ2 above 0.3 even results in divergence of
the training. This is because embedding distributions of differ-
ent classes will eventually overlap since absolute value of the
embedding will be decreased as λ2 increases as seen from the
section 4.2. In this experiment, we show that there exists clear
trade-off between accuracy for the clean images and that for the
adversarial images, and we recommend using a very high λ2
only under strong adversarial environment.

5 CASCADE ADVERSARIAL TRAINING ANALYSIS

Figure 5: Accuracy vs. λ2

5.1 SOURCE NETWORK SELECTION

We further study the transferability
of iter FGSM images between vari-
ous architectures. To this end, we
ﬁrst train 56-layer ResNet networks
(Kurakin’s, pivot loss) with the same
initialization. Then we train another
56-layer ResNet network (Kurakin’s)
with different initialization. We re-
peat the training for the 110-layer
ResNet networks. We measure cor-
relation between iter FGSM noises
from different networks.

Figure 6 (a) shows correlation be-
tween iter FGSM noises crafted from
Kurakin’s network and those from
Pivot network with the same initial-
ization. Conjectured from (Kurakin
et al., 2017), we observe high corre-

(a) Corr: same initialization

(b) Corr: different initialization

Figure 6: Correlation between iter FGSM noises crafted
from different networks for each (cid:15). Correlation is averaged
over randomly chosen 128 images from CIFAR10 test-set.

6

Published as a conference paper at ICLR 2018

Table 3: CIFAR10 test results (%) for 110-layer ResNet models. CW L∞ attack is performed
with 100 test samples (10 samples per each class) and the number of adversarial examples with
(cid:15) > 2 or 4 is reported. {R110K: Kurakin’s, R110P : P ivot loss, R110E: Ensemble training,
R110K,C: Kurakin’s and Cascade training, R110P,E: P ivot loss and Ensemble training, and
R110P,C: P ivot loss and Cascade training}

Model

clean

R110K
R110P (Ours)
R110E
R110K,C (Ours)
R110P,E (Ours)
R110P,C (Ours)

92.3
92.3

92.3
92.3
91.3
91.5

step ll

step FGSM iter FGSM

CW

(cid:15)=2
88.3
86.0

86.3
86.2
84.0
85.7

(cid:15)=16
90.7
89.4

74.3
72.8
65.7
76.4

(cid:15)=2
86.0
81.6

84.1
82.6
77.6
82.4

(cid:15)=16
95.2
91.6

72.9
66.7
54.5
69.1

(cid:15)=2
59.4
64.1

63.5
69.3
66.8
73.5

(cid:15)=4
9.2
20.9

21.1
33.4
38.3
42.5

(cid:15)=2
25
32

24
20
38
27

(cid:15)=4
4
7

6
5
16
15

lation between iter FGSM noises from networks with the same initialization. Correlation between
iter FGSM noises from the networks with different initialization, however, becomes lower as the
network is deeper as shown in ﬁgure 6 (b). Since the degree of freedom increases as the network
size increases, adversarially trained networks prone to end up with different states, thus, making
transfer rate lower. To maximize the beneﬁt of the cascade adversarial training, we propose to use
the same initialization for a cascade network and a source network used for iterative adversarial
examples generation.

5.2 WHITE BOX ATTACK ANALYSIS

We ﬁrst compare a network trained with Kurakin’s method and that with pivot loss. We train 110-
layer ResNet models with/without pivot loss and report accuracy in table 3. We observe our low-
level similarity learning further improves robustness against iterative attacks compared to Kurakin’s
adversarial training. However, the accuracy improvements against iterative attacks (iter FGSM,
CW) are limited, showing regularization effect of low-level similarity learning is not sufﬁcient for
the iterative attacks on complex color images like CIFAR10. This is different from MNIST test
cases where we observed signiﬁcant accuracy increase for iterative attacks only with pivot loss. We
observe label leaking phenomenon reported in (Kurakin et al., 2017) happens even though we don’t
train a network with step FGSM images. Additional analysis for this phenomenon is explained in
Appendix D.

Next, we train a network from scratch with iter FGSM examples crafted from the defended network,
R110P . We use the same initialization used in R110P as discussed in 5.1. In particular, iter FGSM
images are crafted from R110P with CIFAR10 training images for (cid:15)= 1,2, ..., 16, and those are used
randomly together with step ll examples from the network being trained. We train cascade networks
with/without pivot loss. We also train networks with ensemble adversarial training (Tram`er et al.,
2017) with/without pivot loss for comparison. The implementation details for the trained models
can be found in Appendix B.

We ﬁnd several meaningful observations in table 3. First, ensemble and cascade models show im-
proved accuracy against iterative attack although at the expense of decreased accuracy for one-
step attacks compared to the baseline defended network (R110K). Additional data augmentation
from other networks enhances the robustness against iterative attack, weakening label leaking effect
caused by one-step adversarial training.

Second, our low-level similarity learning (R110P,E, R110P,C) further enhances robustness against
iterative attacks including fully unknown CW attack (especially for (cid:15)=4). Additional knowledge
learned from data augmentation through cascade/ensemble adversarial training enables networks to
learn partial knowledge of perturbations generated by an iterative method. And the learned iterative
perturbations become regularized further with our low-level similarity learning making networks
robust against unknown iterative attacks.

7

Published as a conference paper at ICLR 2018

Table 4: CIFAR10 test results (%) for 110-layer ResNet models under black box attacks ((cid:15)=16).
{Target: same networks in table 3 and R110K: Kurakin’s, Source: re-trained baseline, Kurakin’s,
cascade and ensemble networks with/without pivot loss. Source networks use the different initial-
ization from the target networks. Additional details of the models can be found in Appendix B.}

Source: iter FGSM

Target

R110K
R110E
R110P (Ours)
R110K,C (Ours)
R110P,E (Ours)
R110P,C (Ours)

R1102 R110K2 R110E2 R110P 2 R110K,C2 R110P,E2 R110P,C2
70.5
77.9
75.9
56.4
78.2
71.9

67.3
68.2
68.3
67.4
73.4
71.1

77.0
79.0
78.5
79.5
81.7
80.1

27.9
55.8
39.6
61.1
67.7
63.9

80.8
82.7
83.3
82.1
83.8
83.0

54.6
54.7
61.3
62.6
68.4
64.2

73.2
79.5
75.6
80.2
82.1
80.4

Third, our low-level similarity learning serves as a good regularizer for adversarial images, but
not for the clean images for ensemble/cascade models (reduced accuracy for the clean images for
R110P,E and R110P,C in table 3). Compared to pure adversarial training with one-step adversarial
examples, the network sees more various adversarial perturbations during training as a result of
ensemble and cascade training. Those perturbations are prone to end up embeddings in the vicinity
of decision boundary more often than perturbations caused by one-step adversarial training. Pivot
loss pulls the vicinity of those adversarial embeddings toward their corresponding clean embeddings.
During this process, clean embeddings from other classes might also be moved toward the decision
boundary which results in decreased accuracy for the clean images.

5.3 BLACK BOX ATTACK ANALYSIS

We ﬁnally perform black box attack analysis for the cascade/ensemble networks with/without pivot
loss. We report black box attack accuracy with the source networks trained with the same method,
but with different initialization from the target networks. The reason for this is adversarial ex-
amples transfer well between networks trained with the same strategy as observed in section 3.1.
We re-train 110-layer ResNet models using Kurakin’s, cascade and ensemble adversarial training
with/without low-level similarity learning and use those networks as source networks for black-box
attacks. Baseline 110-layer ResNet model is also included as a source network. Target networks
are the same networks used in table 3. We found iter FGSM attack resulted in lower accuracy than
step FGSM attack, thus, report iter FGSM attack only in table 4.

We ﬁrst observe that iter FGSM attack from ensemble models (R110E2, R110P,E2) is strong (re-
sults in lower accuracy) compared to that from any other trained networks. 2 Since ensemble models
learn various perturbation during training, adversarial noises crafted from those networks might be
more general for other networks making them transfer easily between defended networks.

Second, cascade adversarial training breaks chicken and egg problem. (In section 3.1, we found that
it is efﬁcient to use a defended network as a source network to attack another defended network.)
Even though the transferability between defended networks is reduced for deeper networks, cascade
network (R110K,C) shows worst case performance against the attack not from a defended network,
but from a purely trained network (R1102). Possible solution to further improve the worst case
robustness would be to use more than one network as source networks (including pure/defended
networks) for iter FGSM images generation for cascade adversarial training.

Third, ensemble/cascade networks together with our low-level similarity learning (R110P,E,
R110P,C) show better worst case accuracy under black box attack scenario. This shows that enhanc-
ing robustness against iterative white box attack also improves robustness against iterative black box
attack.

2We also observed this when we switch the source and the target networks. Additional details can be found

in Appendix E.

8

Published as a conference paper at ICLR 2018

6 CONCLUSION

We performed through transfer analysis and showed iter FGSM images transfer easily between net-
works trained with the same strategy. We exploited this and proposed cascade adversarial training,
a method to train a network with iter FGSM adversarial images crafted from already defended net-
works. We also proposed adversarial training regularized with a uniﬁed embedding for classiﬁcation
and low- level similarity learning by penalizing distance between the clean and their corresponding
adversarial embeddings. Combining those two techniques (low level similarity learning + cascade
adversarial training) with deeper networks further improved robustness against iterative attacks for
both white-box and black-box attacks.

However, there is still a gap between accuracy for the clean images and that for the adversarial
images. Improving robustness against both one-step and iterative attacks still remains challenging
since it is shown to be difﬁcult to train networks robust for both one-step and iterative attacks si-
multaneously. Future research is necessary to further improve the robustness against iterative attack
without sacriﬁcing the accuracy for step attacks or clean images under both white-box attack and
black-box attack scenarios.

ACKNOWLEDGMENTS

We thank Alexey Kurakin of Google Brain and Li Chen of Intel for comments that greatly improved
the manuscript. The research reported here was supported in part by the Defense Advanced Research
Projects Agency (DARPA) under contract number HR0011-17-2-0045. The views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing
the ofﬁcial policies or endorsements, either expressed or implied, of DARPA.

REFERENCES

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE

Symposium on Security and Privacy, 2017.

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Proceedings of the International Conference on Learning Representations (ICLR),
2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,
nition.
Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90. URL
http://dx.doi.org/10.1109/CVPR.2016.90.

Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesv´ari. Learning with a strong adver-

sary. CoRR, abs/1511.03034, 2015. URL http://arxiv.org/abs/1511.03034.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In

Proceedings of the International Conference on Learning Representations (ICLR), 2017.

Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.

lecun.com/exdb/mnist/.

O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proceedings of the British

Machine Vision Conference (BMVC), 2015.

Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face
recognition and clustering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015.

Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensem-
ble adversarial training: Attacks and defenses. CoRR, abs/1705.07204, 2017. URL http:
//arxiv.org/abs/1705.07204.

9

Published as a conference paper at ICLR 2018

Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning ap-
In Computer Vision - ECCV 2016 - 14th European Con-
proach for deep face recognition.
ference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VII, pp. 499–
515, 2016. doi: 10.1007/978-3-319-46478-7 31. URL http://dx.doi.org/10.1007/
978-3-319-46478-7_31.

10

Published as a conference paper at ICLR 2018

A EXPERIMENTAL SETUP

We scale down the image values to [0,1] and don’t perform any data augmentation for MNIST. For
CIFAR10, we scale down the image values to [0,1] and subtract per-pixel mean values. We perform
24x24 random crop and random ﬂip on 32x32 original images. 3 We generate adversarial images
with “step ll” after these steps unless otherwise noted.

For adversarial training, we generate k = 64 adversarial examples among 128 images in one mini-
batch. As in Kurakin et al. (2017), we use randomly chosen (cid:15) in the interval [0, max e] with clipped
normal distribution N (µ = 0, σ = max e/2), where max e is the maximum (cid:15) used in training.
We use max e = 0.3*255 and 16 for MNIST and CIFAR10 respectively. We use λ = 0.3 and λ2 =
0.0001 in the loss function.

We use stochastic gradient descent (SGD) optimizer with momentum of 0.9, weight decay of 0.0001.
We start with a learning rate of 0.1, divide it by 10 at 4k and 6k iterations, and terminate training
at 8k iterations for MNIST, and 48k and 72k iterations, and terminate training at 94k iterations for
CIFAR10. 4

We also found that initialization affects the training results slightly as in (Kurakin et al., 2017), thus,
we pre-train the networks 2 and 10 epochs for MNIST and CIFAR10, and use these as initial starting
points for different conﬁgurations.

B MODEL DESCRIPTIONS

We summarize the model names used in this paper in table 5. For ensemble adversarial training,
pre-trained networks as in table 7 together with the network being trained are used to generate one-
step adversarial examples during training. For cascade adversarial training, pre-trained defended
networks as in table 6 are used to generate iter FGSM images, and the network being trained is used
to generate one-step adversarial examples during training.

3Original paper (He et al., 2016) performed 32x32 random crop on zero-padded 40x40 images.
4We found that the adversarial training requires longer training time than the standard training. Authors in
the original paper (He et al., 2016) changed the learning rate at 32k and 48k iterations and terminated training
at 64k iterations.

11

Published as a conference paper at ICLR 2018

Table 5: Model descriptions

Dataset

ResNet

Initialization
Group

MNIST

20-layer

A

CIFAR10

56-layer

20-layer

110-layer

standard training

standard training

standard training
Kurakin’s
P ivot loss

Training
standard training
Kurakin’s
Bidirection loss
P ivot loss

standard training
Kurakin’s
Ensemble training
Bidirection loss
P ivot loss
Kurakin’s & Cascade training
P ivot loss & Ensemble training
P ivot loss & Cascade training

Model
R20M
R20MK
R20MB
R20MP
R20
R20K
R20E
R20B
R20P
R20K,C
R20P,E
R20P,C
R202
R20K2
R20P 2
R203
R204
R56K
R56P
R56K2
R110
R110K
R110P
R110E
R110K,C
R110P,E
R110P,C
R1102
standard training
Kurakin’s
R110K2
Ensemble training
R110E2
P ivot loss
R110P 2
Kurakin’s & Cascade training
R110K,C2
P ivot loss & Ensemble training R110P,E2
P ivot loss & Cascade training
R110P,C2
R1103
R1104

standard training
Kurakin’s
P ivot loss
Ensemble training
Kurakin’s & Cascade training
P ivot loss & Ensemble training
P ivot loss & Cascade training

Kurakin’s
P ivot loss

standard training

standard training

Kurakin’s

B

C

D

E

F

G

H

I

J

K

Table 6: Ensemble model description

Ensemble models
R20E, R20P,E, R110E, R110P,E
R110E2, R110P,E2

Pre-trained models
R203, R1103
R204, R1104

Table 7: Cascade model description

Cascade models
R20K,C, R20P,C
R110K,C, R110P,C
R110K,C2, R110P,C2

Pre-trained model
R20P
R110P
R110P 2

12

Published as a conference paper at ICLR 2018

C ALTERNATIVE VISUALIZATION ON EMBEDDINGS

(a) R20, step ll

(b) R20K , step ll

(c) R20P (Ours), step ll

(d) R20, step FGSM

(e) R20K , step FGSM

(f) R20P (Ours), step FGSM

(g) R20, random sign

(h) R20K , random sign

(i) R20P (Ours), random sign

Figure 7: Argument to the softmax vs. (cid:15) in test time. “step ll”, “step FGSM” and ”random sign”
methods were used to generate test-time adversarial images. Arguments to the softmax were mea-
sured by changing (cid:15) for each test method and averaged over randomly chosen 128 images from
CIFAR10 test-set. Blue line represents true class and the red line represents mean of the false
classes. Shaded region shows ± 1 standard deviation of each line.

We draw average value of the argument to the softmax layer for the true class and the false classes
to visualize how the adversarial training works as in ﬁgure 7. Standard training, as expected, shows
dramatic drop in the values for the true class as we increase (cid:15) in “step ll” or “step FGSM direction.
With adversarial training, we observe that the value drop is limited at small (cid:15) and our method even
increases the value in certain range upto (cid:15)=10. Note that adversarial training is not the same as
the gradient masking.As illustrated in ﬁgure 7, it exposes gradient information, however, quickly
distort gradients along the sign of the gradient (“step ll” or “step FGSM) direction. We also observe
improved results (broader margins than baseline) for “random sign” added images even though
we didn’t inject random sign added images during training. Overall shape of the argument to the
softmax layer in our case becomes smoother than Kurakin’s method, suggesting our method is good
for pixel level regularization. Even though actual value of the embeddings for the true class in our
case is smaller than that in Kurakin’s, the standard deviation of our case is less than Kurakin’s,
making better margin between the true class and false classes.

13

Published as a conference paper at ICLR 2018

D LABEL LEAKING ANALYSIS

(a) R20: Baseline

(b) R20K : Kurakin’s

(c) R20P : Pivot (Ours)

Figure 8: Averaged Pearson’s correlation coefﬁcient between the gradients w.r.t. two images. Corre-
lation were measured by changing (cid:15) for each adversarial image and averaged over randomly chosen
128 images from CIFAR10 test-set. Shaded region represents ± 0.5 standard deviation of each line.

We observe accuracies for the “step FGSM” adversarial images become higher than those for the
clean images (“label leaking” phenomenon) by training with “step FGSM” examples as in (Kurakin
et al., 2017). Interestingly, we also observe “label leaking” phenomenon even without providing
true labels for adversarial images generation. We argue that “label leaking” is a natural result of the
adversarial training.

To understand the nature of adversarial training, we measure correlation between gradients w.r.t.
different images (i.e. clean vs. adversarial) as a measure of error surface similarity. We measure
correlation between gradients w.r.t. (1) clean vs. “step ll” image, (2) clean vs. “step FGSM” image,
(3) clean vs. “random sign” added image, and (4) “step ll” image vs. “step FGSM” image for three
trained networks (a) R20, (b) R20K and (c) R20P (Ours) in table 5. Figure 8 draws average value
of correlations for each case.

Meaning of the correlation: In order to make strong adversarial images with “step FGSM” method,
correlation between the gradient w.r.t.
its corresponding
adversarial image should remain high since the “step FGSM” method only use the gradient w.r.t.
the clean image ((cid:15)=0). Lower correlation means perturbing the adversarial image at (cid:15) further to the
gradient (seen from the clean image) direction is no longer efﬁcient.

the clean image and the gradient w.r.t.

Results of adversarial training: We observe that (1) and (2) become quickly lower than (3) as
(cid:15) increases. This means that, when we move toward the steepest (gradient) direction on the error
surface, gradient is more quickly uncorrelated with the gradient w.r.t. the clean image than when
we move to random direction. As a result of adversarial training, this uncorrelation is observed at a
lower (cid:15) making one-step attack less efﬁcient even with small perturbation. (1), (2) and (3) for our
case are slightly lower than Kurakin’s method at the same (cid:15) which means that our method is better
at defending one-step attacks than Kurakin’s.

Error surface similarity between “step ll” and “step FGSM” images: We also observe (4) re-
mains high with higher (cid:15) for all trained networks. This means that the error surface (gradient) of
the “step ll” image and that of its corresponding “step FGSM” image resemble each other. That is
the reason why we get the robustness against “step FGSM” method only by training with “step ll”
(4) for our case is slightly higher than Kurakin’s method at the same (cid:15)
method and vice versa.
and that means our similarity learning tends to make error surfaces of the adversarial images with
“step ll” and “step FGSM” method to be more similar.

Analysis of label leaking phenomenon: Interestingly, (2) becomes slightly negative in certain
range (1 < (cid:15) < 3 for Kurakin’s, and 1 < (cid:15) < 4 for Pivot (Ours) ) and this could be the possible
reason for “label leaking” phenomenon. For example, let’s assume that we have a perturbed image
(by “step FGSM” method) at (cid:15) where the correlation between the gradients w.r.t. that image and the
corresponding clean image is negative. Further increase of (cid:15) with the gradient (w.r.t. the clean image)
direction actually decreases the loss resulting in increased accuracy (label leaking phenomenon).
Due to the error surface similarity between “step ll” and “step FGSM” images and this negative

14

Published as a conference paper at ICLR 2018

correlation effect, however, label leaking phenomenon can always happen for the networks trained
with one-step adversarial examples.

E ADDITIONAL BLACK BOX ATTACK RESULTS

Table 8: CIFAR10 test results (%) under black box attacks between the network with the same
initialization ((cid:15)=16)}

Target

Source: step FGSM

Source: iter FGSM

R20 R20K R20P
12.2
27.5
27.4
R20
R20K 65.7
81.8
81.5
58.1
91.7
89.3
R20P

R20 R20K R20P
44.7
45.9
0.0
18.2
0.0
51.5
13.4
0.0
48.9

Table 8 shows that black box attack between trained networks with the same initialization tends to
be more successful than that between networks with different initialization as explained in (Kurakin
et al., 2017).

Table 9: CIFAR10 test results (%) under black box attacks for (cid:15)=16. {Target and Source networks
are switched from the table 1}

Target

R202
R20K2
R20P 2

Source: step FGSM

Source: iter FGSM

R20 R20K R20P
17.9
34.5
33.9
65.0
84.5
84.6
66.4
87.2
88.2

R20 R20K R20P
4.1
54.3
54.8
30.4
25.3
61.2
36.1
27.7
61.6

In table 9, our method (R20P 2) is always better at one-step and iterative black box attack from
defended networks (R20K, R20P ) and undefended network (R20) than Kurakin’s method (R20B2).
However, it is hard to tell which method is better than the other one as explained in the main paper.

Table 10: CIFAR10 test results (%) for cascade networks under black box attacks for (cid:15)=16. {Target
and Source: Please see the model descriptions in Appendix B.}

Target

Source: iter FGSM

R110 R110K R110E R110P R110K,C R110P,E R110P,C
80.5
R110K2
82.7
R110E2
R110P 2 (Ours)
80.3
R110K,C2 (Ours) 62.1
81.5
R110P,E2 (Ours)
72.2
R110P,C2 (Ours)

68.0
59.6
72.2
72.3
77.3
73.8

72.7
59.1
75.9
74.7
79.0
76.4

49.6
51.5
54.9
46.5
56.9
51.0

49.3
39.5
54.2
61.5
50.0
60.6

67.9
69.6
72.4
67.9
75.2
72.0

41.0
40.3
44.3
39.0
45.5
40.9

In table 10, we show black box attack accuracies with the source and the target networks switched
from the table 4. We also observe that networks trained with both low-level similarity learning and
cascade/ensemble adversarial training (R110P,C2, R110P,E2) show better worst-case performance
than other networks. Overall, iter FGSM images crafted from ensemble model families (R110E,
R110P,E) remain strong on the defended networks.

15

Published as a conference paper at ICLR 2018

F IMPLEMENTATION DETAILS FOR CARLINI-WAGNER L∞ ATTACK

(a) MNIST

(b) CIFAR10, 20-layer

(c) CIFAR10, 110-layer

Figure 9: Cumulative distribution function vs. (cid:15) for 100 test adversarial examples generated by CW
L∞ attack. Lower CDF value for a ﬁxed (cid:15) means the better defense.

Carlini and Wagner (CW) L∞ attack solves the following optimization problem for every input X.

minimize

c · f (X + δ) +

[(|δi| − τ )+]

(cid:88)

i

such that X + δ ∈ [0, 1]n
where, the function f is deﬁned such that attack is success if and only if f (X + δ) < 0, δ is the
target perturbation deﬁned as X adv −X, c is the parameter to control the relative weight of function
f in the total cost function, and τ is the control threshold used to penalize any terms that exceed τ .

Since CW L∞ attack is computationally expensive, we only use 100 test examples (10 exam-
ples per each class). We search adversarial example X adv with c ∈ {0.1, 0.2, 0.5, 1, 2, 5, 10, 20}
and τ ∈ {0.02, 0.04, ..., 0.6} for MNIST and c ∈ {0.1, 0.3, 1, 3, 10, 30, 100} and τ ∈
{0.001, 0.002, ..., 0.01, 0.012, ..., 0.02, 0.024, ..., 0.04, 0.048, ..., 0.08} for CIFAR10. We use Adam
optimizer with an initial learning rate of 0.01/c since we found constant initial learning rate for
c · f (X + δ) term is critical for successful adversarial images generation. We terminate the search
after 2,000 iterations for each X, c and τ . If f (X + δ) < 0 and the resulting ||δ||∞ is lower than
the current best distance, we update X adv.

Figure 9 shows cumulative distribution function of (cid:15) for 100 successful adversarial examples per
each network. We report the number of adversarial examples with (cid:15) > 0.3*255 for MNIST and
that with (cid:15) > 2 or 4 for CIFAR10. As seen from this ﬁgure, our approaches provide robust defense
against CW L∞ attack compared to other approaches.

16

