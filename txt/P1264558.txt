8
1
0
2
 
y
a
M
 
6
 
 
]
L
C
.
s
c
[
 
 
2
v
9
7
7
9
0
.
4
0
8
1
:
v
i
X
r
a

On the Evaluation of Semantic Phenomena in
Neural Machine Translation Using Natural Language Inference

Adam Poliak1

James Glass2

Yonatan Belinkov2
1Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218
2Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology, Cambridge, MA 02139
{azpoliak,vandurme}@cs.jhu.edu, {belinkov,glass}@mit.edu

Benjamin Van Durme1

Abstract

We propose a process for investigating the ex-
tent to which sentence representations arising
from neural machine translation (NMT) sys-
tems encode distinct semantic phenomena. We
use these representations as features to train a
natural language inference (NLI) classiﬁer ba-
sed on datasets recast from existing semantic
annotations. In applying this process to a re-
presentative NMT system, we ﬁnd its enco-
der appears most suited to supporting inferen-
ces at the syntax-semantics interface, as com-
pared to anaphora resolution requiring world-
knowledge. We conclude with a discussion on
the merits and potential deﬁciencies of the
existing process, and how it may be improved
and extended as a broader framework for eva-
luating semantic coverage.1

1.

Introduction

What do neural machine translation (NMT)
models learn about semantics? Many resear-
chers suggest
that state-of-the-art NMT mo-
dels learn representations that capture the mea-
ning of sentences (Gu et al., 2016; Johnson et al.,
2017; Zhou et al., 2017; Andreas and Klein, 2017;
Neubig, 2017; Koehn, 2017). However, there is li-
mited understanding of how speciﬁc semantic phe-
nomena are captured in NMT representations be-
yond this broad notion. For instance, how well do
these representations capture Dowty (1991)’s the-
matic proto-roles? Are these representations suf-
ﬁcient for understanding paraphrastic inference?
Do the sentence representations encompass com-
plex anaphora resolution? We argue that existing
semantic annotations recast as Natural Langua-
ge Inference (NLI) can be leveraged to investi-
gate whether sentence representations encoded by
NMT models capture these semantic phenomena.

DPR

FN+

SPR

Sara adopted Jill, she wanted a child
Sara adopted Jill, Jill wanted a child

Iran possesses ﬁve research reactors
Iran has ﬁve research reactors

Berry Rejoins WPP Group
Berry was sentient

✗

✓

✓

Figure 1: Example sentence pairs for the different se-
mantic phenomena. DPR deals with complex anaphora
resolution, FN+ is concerned with paraphrastic inferen-
ce, and SPR covers Reisinger et al. (2015)’s semantic
proto-roles. ✓ / ✗ indicates that the ﬁrst sentence en-
tails / does not entail the second.

We use sentence representations from pre-
trained NMT encoders as features to train classi-
ﬁers for NLI, the task of determining if one sen-
tence (a hypothesis) is supported by another (a
context).2 If the sentence representations learned
by NMT models capture distinct semantic pheno-
mena, we hypothesize that those representations
should be sufﬁcient to perform well on NLI da-
tasets that test a model’s ability to capture these
phenomena. Figure 1 shows example NLI senten-
ce pairs with their respective labels and semantic
phenomena.

We evaluate NMT sentence representations of 4
NMT models from 2 domains on 4 different NLI
datasets to investigate how well they capture dif-
ferent semantic phenomena. We use White et al.
(2017)’s Uniﬁed Semantic Evaluation Framework
(USEF) that recasts three semantic phenomena
NLI: 1) semantic proto-roles, 2) paraphrastic in-
ference, 3) and complex anaphora resolution. Ad-
ditionally, we evaluate the NMT sentence repre-
sentations on 4) Multi-NLI, a recent extension of
the Stanford Natural Language Inference dataset
(SNLI) (Bowman et al., 2015) that includes mul-
tiple genres and domains (Williams et al., 2017).

1Code developed and data used are available at

2Sometimes referred to as recognizing textual entail-

https://github.com/boknilev/nmt-repr-analysis.

ment (Dagan et al., 2006, 2013).

We contextualize our results with a standard neu-
ral encoder described in Bowman et al. (2015) and
used in White et al. (2017).

Based on the recast NLI datasets, our investiga-
tion suggests that NMT encoders might learn mo-
re about semantic proto-roles than anaphora reso-
lution or paraphrastic inference. We note that the
target-side language affects how an NMT source-
side encoder captures these semantic phenomena.

2. Motivation

Why use recast NLI? We focus on NLI, as op-
posed to a wide range of NLP taks, as a uniﬁed
framework that can capture a variety of seman-
tic phenomena based on arguments by White et al.
(2017). Their recast dataset enables us to study
whether NMT encoders capture “distinct types
of semantic reasoning” under just one task. We
choose these speciﬁc semantic phenomena for two
reasons. First, a long term goal is to understand
how combinations of different corpora and neural
architectures can contribute to a system’s ability to
perform general language understanding. As hu-
mans can understand (annotate consistently) the
sentence pairs used in our experiments, we would
similarly like our ﬁnal system to have this same
capability. We posit that it is necessary but not ne-
cessarily sufﬁcient for a language understanding
system to be able to capture the semantic pheno-
mena considered here. Second, we believe these
semantic phenomena might be relevant for trans-
lation. We demonstrate this with a few examples.

Anaphora Anaphora resolution connects to-
kens, typically pronouns, to their referents. Anap-
hora resolution should occur when translating
from morphologically poor languages into so-
me morphologically rich languages. For example,
when translating “The parent fed the child becau-
se she was hungry,” a Spanish translation should
describe the child as la niña (fem.) and not el
niño (masc.) since she refers to the child. Be-
cause world knowledge is often required to per-
form anaphora resolution (Rahman and Ng, 2012;
Javadpour, 2013),
this may enable evaluating
whether an NMT encoder learns world knowled-
ge. In this example, she refers to the child and
not the parent since world knowledge dictates
that parents often feed children when children are
hungry.

Proto-roles Dowty (1991)’s proto-roles may be
expressed differently in different languages, and
so correctly identifying them can be important for
translation. For example, English does not usually
explicitly mark volition, a proto-role, except by
using adverbs like intentionally or accidentally.
Other languages mark volitionality by using spe-
cial afﬁxes (e.g., Tibetan and Sesotho, a Bantu lan-
guage), case marking (Hindi, Sinhalese), or au-
xiliaries (Japanese).3 Correctly generating these
markers may require the MT system to encode vo-
litionality on the source side.

Paraphrases Callison-Burch (2007) discusses
how paraphrases help statistical MT (SMT) when
alignments from source words to target-language
words are unknown. If the alignment model can
map a paraphrase of the source word to a word
in the target language, then the SMT model can
translate the original word based on its paraph-
rase.4 Paraphrases are also used by professional
translators to deal with non-equivalence of words
in the source and target languages (Baker, 2018).

3. Methodology

to extract

We use NMT models based on bidirectio-
nal long short-term memory (Bi-LSTM) encoder-
decoders with attention (Sutskever et al., 2014;
Bahdanau et al., 2015), trained on a parallel cor-
pus. Given an NLI context-hypothesis pair, we
pass each sentence independently through a trai-
ned NMT encoder
their respective
vector representations. We represent each sen-
tence by concatenating the last hidden state
from the forward and backward encoders, re-
sulting in v and u (in R2d)
the con-
text and hypothesis.5 We follow the common
practice of feeding the concatenation (v, u) ∈
R4d to a classiﬁer
(Rocktäschel et al., 2016;
Bowman et al., 2015; Mou et al., 2016; Liu et al.,
2016; Cheng et al., 2016; Munkhdalai and Yu,
2017).

for

Sentence pair representations are fed into a clas-
siﬁer with a softmax layer that maps onto the num-
ber of labels. Experiments with both linear and

3

For

references

and

examples,

see:

en.wikipedia.org/wiki/Volition_(linguistics).
4Using paraphrases can help NMT models generate text in
the target language in some settings (Sekizawa et al., 2017).
5We experimented with other sentence representations
and their combinations, and did not see differences in ove-
rall conclusions. See Appendix A for these experiments.

Test

DPR: 50.0
zh

de

Train

ar

es

USEF

ar

es

USEF

ar

es

SPR: 65.4
zh

de

FN+: 57.5
zh

de

DPR
SPR
FN+

49.8
50.1
50.0

50.0
50.3
50.0

50.0
50.1
50.4

50.0
49.9
50.0

49.5
50.7
49.5

45.4
72.1
57.3

57.1
74.2
63.6

47.0
73.6
54.5

43.9
73.1
60.7

65.2
80.6
60.0

48.0
56.3
56.2

55.9
57.0
56.1

51.0
56.9
54.3

46.8
56.1
55.5

USEF

19.2
65.8
80.5

Table 1: Accuracy on NLI with representations generated by encoders of English→{ar,es,zh,de} NMT models.
Rows correspond to the training and validation sets and major columns correspond to the test set. The column
labeled “USEF” refers to the test accuracies reported in White et al. (2017). The numbers on the top row represents
each dataset’s majority baseline. Bold numbers indicate the highest performing model for the given dataset.

non-linear classiﬁers have not shown major diffe-
rences, so we report results with the linear classi-
ﬁer unless noted otherwise. We report implemen-
tation details in Appendix B.

4. Data

MT data We train NMT models on four lan-
guage pairs: English → {Arabic (ar), Spanish
(es), Chinese (zh), and German (de)}. See Appen-
dix B for training details. The ﬁrst three pairs use
the United Nations parallel corpus (Ziemski et al.,
2016) and for English-German, we use the WMT
dataset (Bojar et al., 2014). Although the entail-
ment classiﬁer only uses representations extracted
from the English-side encoders as features, using
multiple language pairs allows us to explore whet-
her different target languages affect what semantic
phenomena are captured by an NMT encoder.

Natural Language Inference data We use
four distinct datasets to train classiﬁers: Multi-
(Williams et al., 2017), a recent expan-
NLI
sion of SNLI containing a broad array of do-
mains that was used in the 2017 RepEval sha-
red task (Nangia et al., 2017), and three re-
cast NLI datasets from The JHU Decomposi-
tional Semantics Initiative (Decomp)6 released
by White et al. (2017). Sentence-pairs and labels
were recast,
i.e. automatically converted, from
existing semantic annotations: FrameNet Plus
(FN+) (Pavlick et al., 2015), Deﬁnite Pronoun Re-
solution (DPR) (Rahman and Ng, 2012), and Se-
mantic Proto-Roles (SPR) (Reisinger et al., 2015).
The FN+ portion contains sentence pairs based on
paraphrastic inference, DPR’s sentence pairs focus
on identifying the correct antecedent for a deﬁni-
te pronoun, and SPR’s sentence pairs test whet-
her the semantic proto-roles from Reisinger et al.
(2015) apply based on a given sentence.7 Recas-
ting makes it easy to determine how well an NLI

6decomp.net
7We refer the reader to White et al. (2017) for detailed dis-

cussion on how the existing datasets were recast as NLI.

DPR

SPR

FN+ MNLI

Train
Dev
Test

2K 123K 124K
15K
15K
.4K
14K
15K
1K

393K
9K
9K

Table 2: Number of sentences in NLI datasets.

method captures the ﬁne-grained semantics inspi-
red by Dowty (1991)’s thematic proto-roles, pa-
raphrastic inference, and complex anaphora reso-
lutions. Table 2 includes the datasets’ statistics.

5. Results

Table 1 shows results of NLI classiﬁers trained
on representations from different NMT encoders.
We also report the majority baseline and the results
of Bowman et al.’s 3-layer deep 200 dimensional
neural network used by White et al. (“USEF”).

Paraphrastic entailment (FN+) Our classiﬁers
predict FN+ entailment worse than the majo-
rity baseline, and drastically worse than USEF
when trained on FN+’s training set. Since FN+
tests paraphrastic inference and NMT models ha-
ve been shown to be useful to generate senten-
tial paraphrase pairs (Wieting and Gimpel, 2017;
Wieting et al., 2017), it is surprising that our clas-
siﬁers using the representations from the NMT en-
coder perform poorly. Although the sentences in
FN+ are much longer than in the other datasets,
sentence length does not seem to be responsible
for the poor FN+ results. The classiﬁers do not no-
ticeably perform better on shorter sentences than
longer ones, as noted in Appendix C.

Upon manual inspection, we noticed that in
many not-entailed examples, swapped paraphra-
ses had different part-of-speech (POS) tags. This
begs the question of whether different POS tags
for swapped paraphrases affects the accuracies.
Using Stanford CoreNLP (Manning et al., 2014),
we partition our validation set based on whether
the paraphrases share the same POS tag. Table 3
reports dev set accuracies using classiﬁers trained

Same Tag
Different Tag

ar

52.9
55.8

es

52.6
59.1

zh

52.6
53.4

de

50.2
46.0

Table 3: Accuracies on FN+’s dev set based on whether
the swapped paraphrases share the same POS tag.

on FN+. Classiﬁers using features from NMT en-
coders trained on the three languages from the UN
corpus noticeably perform better on cases where
paraphrases have different POS tags compared to
paraphrases with the same POS tags. These dif-
ferences might suggest that the recast FN+ might
not be an ideal dataset to test how well NMT enco-
ders capture paraphrastic inference. The sentence
representations may be impacted more by ungram-
maticality caused by different POS tags as oppo-
sed to poor paraphrases.

Anaphora entailment (DPR)
The low accura-
cies for predicting NLI targeting anaphora resolu-
tion are similar to White et al. (2017)’s ﬁndings.
They suggest that the model has difﬁculty in cap-
turing complex anaphora resolution. By using con-
trastive evaluation pairs, Bawden et al. (2017) re-
cently suggested as well that NMT models are
poorly suited for co-reference resolution. Our re-
sults are not surprising given that DPR tests whet-
her a model contains common sense knowled-
ge (Rahman and Ng, 2012). In DPR, syntactic
cues for co-reference are purposefully balanced
out as each pair of pro-nouns appears in at least
two context-hypothesis pairs (Table 9). This forces
the model’s decision to be informed by semantics
and world knowledge – a model cannot use syn-
tactic cues to help perform anaphora resolution.8
Although the poor performance of NMT represen-
tations may be explained by a variety of reasons,
e.g. training data, architectures, etc., we would still
like ideal MT systems to capture the semantics of
co-reference, as evidenced in the example in §2.

Even though the classiﬁers perform poorly
when predicting paraphrastic entailment, they sur-
prisingly outperform USEF by a large margin
(around 25–30 %) when using a model trained on
DPR.9 This might suggest that an NMT encoder
can pick up on how pronouns may be used as a ty-
pe of lexical paraphrase (Bhagat and Hovy, 2013).

Proto-role entailment (SPR) When predicting
SPR entailments using a classiﬁer trained on SPR

Proto-Role

ar

es

zh

de

avg MAJ

physically existed

sentient

aware

volitional

existed before

caused

changed

location

moved

used in

existed after

chang. state

chang. possession

stationary during

physical contact
existed during

70.6
78.5
75.9
74.3
68.4
69.4

64.2
91.1
90.6
34.9
62.7
61.8
89.6
86.3
85.0
59.3

70.8
82.2
77.0
76.8
70.5
74.1

62.4
90.1
88.8
38.1
69.0
60.7
88.6
84.4
82.0
71.8

77.2
80.5
76.6
74.7
66.5
72.2

63.8
90.4
90.1
31.8
65.6
60.9
89.9
90.5
84.5
60.8

70.8
81.7
76.7
73.7
68.4
72.7

62.0
90.2
90.3
34.2
65.2
60.7
88.3
86.0
84.4
64.4

72.4†
80.7†
76.6†
74.9†
68.5†
72.1†

63.1
90.4
89.9
34.7
65.7
61.0
89.1
86.8
84.0
64.1

65.9
75.5
60.9
64.5
64.8
63.4

65.1
91.7
93.3
55.2
69.7
65.2
93.9
96.3
85.8
84.7

Table 4: Accuracies on the SPR test set broken down
by each proto-role. “avg” represents the score for the
proto-role averaged across target languages. Bold and
† respectively indicate the best results for each proto-
role and whether all of our classiﬁers outperformed the
proto-role’s majority baseline.

data, we noticeably outperform the majority base-
line but are below USEF. Both ours and USEF’s
accuracies are lower than Teichert et al. (2017)’s
best reported numbers. This is not surprising as
Teichert et al. condition on observed semantic role
labels when predicting proto-role labels.

Table 4 reports accuracies for each proto-role.
Whenever one of the classiﬁers outperforms the
baseline for a proto-role, all the other classiﬁers
do as well. The classiﬁers outperform the majo-
rity baseline for 6 of the reported 16 proto-roles.
We observe these 6 properties are more associated
with proto-agents than proto-patients.

The larger improvements over the majority ba-
seline for SPR compared to FN+ and DPR is
not surprising. Dowty (1991) posited that proto-
agent, and -patient should correlate with En-
glish syntactic subject, and object, respectively,
and empirically the necessity of [syntactic] par-
sing for predicate argument recognition has been
observed in practice (Gildea and Palmer, 2002;
Punyakanok et al., 2008). Further, recent work is
suggestive that LSTM-based frameworks impli-
citly may encode syntax based on certain learning
objectives (Linzen et al., 2016; Shi et al., 2016;
Belinkov et al., 2017b). It is unclear whether NMT
encoders capture semantic proto-roles speciﬁcally
or just underlying syntax that affects the proto-
roles.

8Appendix D includes some illustrative examples.
9This is seen in the last columns of the top row in Table 1.

NMT target language Our experiments show
differences based on which target language was

ar

45.9
46.6

es

45.7
46.7

zh

46.6
48.2

de MAJ

48.0
48.9

35.6
36.5

MNLI-1
MNLI-2

Table 5: Accuracies for MNLI test sets. MNLI-1 refers
to the matched case and MNLI-2 is the mismatched.

used to train the NMT encoder, in capturing se-
mantic proto-roles and paraphrastic inference. In
Table 1, we notice a large improvement using sen-
tence representations from an NMT encoder that
was trained on en-es parallel text. The improve-
ments are most profound when a classiﬁer trained
on DPR data predicts entailment focused on se-
mantic proto-roles or paraphrastic inference. We
also note that using the NMT encoder trained on
en-es parallel text results in the highest results in
5 of the 6 proto-roles in the top portion of Ta-
ble 4. When using other sentence representations
(Appendix A), we notice that using representa-
tions from English-German encoders consistently
outperforms using the other encoders (Tables 6
and 7). This prevents us from making generaliza-
tions regarding speciﬁc target side languages.

NLI across multiple domains Though our main
focus is exploring what NMT encoders learn about
distinct semantic phenomena, we would like to
know how useful NMT models are for general
NLI across multiple domains. Therefore, we also
evaluate the sentence representations with Multi-
NLI. As indicated by Table 5, the representa-
tions perform noticeably better than a majority ba-
seline. However, our results are not competitive
with state-of-the-art systems trained speciﬁcally
for Multi-NLI (Nangia et al., 2017).

6. Related Work

In concurrent work, Poliak et al. (2018) explore
whether NLI datasets contain statistical irregulari-
ties by training a model with access to only hypot-
heses. Their model signiﬁcantly outperforms the
majority baseline and our results on Multi-NLI,
SPR, and FN+. They suggest that these, among ot-
her NLI datasets, contain statistical irregularities.
Their ﬁndings illuminate issues with the recast da-
tasets we consider, but do not invalidate our ap-
proach of using recast NLI to determine whether
NMT encoders capture distinct semantic pheno-
mena. Instead, they force us to re-evaluate the ma-
jority baseline as an indicator of whether encoders
learn distinct semantics and to what extent we can
make conclusions based on these recast datasets.

Prior work has focused on the relationship bet-
ween semantics and machine translation. MEANT
and its extension XMEANT evaluate MT systems
based on semantics (Lo and Wu, 2011; Lo et al.,
2014). Others have focused on incorporating se-
mantics directly in MT. Chan et al. (2007) use
word sense disambiguation to help statistical MT,
Gao and Vogel (2011) add semantic-roles to im-
prove phrase-based MT, and Carpuat et al. (2017)
demonstrate how ﬁltering parallel sentences that
are not parallel
in meaning improves transla-
tion. Recent work explores how representations
learned by NMT systems can improve semantic
tasks. McCann et al. (2017) show improvements
in many tasks by using contextualized word vec-
tors extracted from a LSTM encoder trained for
MT. Their goal is to use NMT to improve other
tasks while we focus on using NLI to determine
what NMT models learn about different semantic
phenomena.

Researchers have explored what NMT models
learn about other linguistic phenomena, such as
morphology (Dalvi et al., 2017; Belinkov et al.,
2017a), syntax (Shi et al., 2016), and lexical se-
mantics (Belinkov et al., 2017b), including word
senses (Marvin and Koehn, 2018; Liu et al., 2018)

7. Conclusion and Future Work

Researchers suggest that NMT models learn
sentence representations that capture meaning. We
inspected whether distinct types of semantics are
captured by NMT encoders. Our experiments sug-
gest that NMT encoders might learn the most
about semantic proto-roles, do not focus on anap-
hora resolution, and may poorly capture paraph-
rastic inference. We conclude by suggesting that
target-side language affects how well an NMT en-
coder captures these semantic phenomena.

In future work, we would like to study how
well NMT encoders capture other semantic pheno-
mena, possibly by recasting other datasets. Com-
paring how semantic phenomena are represen-
ted in different NMT architectures, e.g. purely
convolutional (Gehring et al., 2017) or attention-
based (Vaswani et al., 2017), may shed light on
whether different architectures may better captu-
re semantic phenomena. Finally, investigating how
multilingual systems learn semantics can bring a
new perspective to questions of universality of re-
presentation (Schwenk and Douze, 2017).

Acknowledgements

This work was supported by JHU-HLTCOE,
DARPA LORELEI, and Qatar Computing Re-
search Institute. We thank anonymous reviewers.
Views and conclusions contained in this publica-
tion are those of the authors and should not be in-
terpreted as representing ofﬁcial policies or endor-
sements of DARPA or the U.S. Government.

References

Jacob Andreas and Dan Klein. 2017. Analogs of lin-
In Pro-
guistic structure in deep representations.
ceedings of the 2017 Conference on Empirical Met-
hods in Natural Language Processing. Association
for Computational Linguistics, pages 2893–2897.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Mona Baker. 2018. In other words: A coursebook on

translation. Routledge.

Rachel Bawden, Rico Sennrich, Alexandra Birch, and
Barry Haddow. 2017. Evaluating discourse pheno-
mena in neural machine translation. arXiv preprint
arXiv:1711.00513 .

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infe-
In Proceedings of the 2015 Conference on
rence.
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Chris Callison-Burch.

Paraphrasing
and
thesis, Univer-
Scotland.
sity
http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf.

Ph.D.
Edinburgh,

Translation.
of

Edinburgh,

2007.

Marine Carpuat, Yogarshi Vyas, and Xing Niu. 2017.

Detecting cross-lingual semantic divergence for neural machine translation.
on
In Proceedings
Neural Machine
Association
for Computational Linguistics, pages 69–79.
http://aclweb.org/anthology/W17-3209.

the First Workshop
Translation.

of

Seng Yee Chan, Tou Hwee Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Sta-
tistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics. Association for Computational
Linguistics, pages 33–40.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
In Proceedings of the 2016 Conference
reading.
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
Austin, Texas, pages 551–561.

Yonatan Belinkov, Nadir Durrani, Fahim Dal-
vi, Hassan Sajjad, and James Glass. 2017a.
What do Neural Machine Translation Models Learn about Morphology?
In Proceedings of
the 55th Annual Meeting
of
the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for
Computational
861–872.
https://doi.org/10.18653/v1/P17-1080.

Kyunghyun Cho, Bart
mitry Bahdanau,
On the properties of neural machine translation: Encoder–decoder approaches.
In Proceedings of SSST-8, Eighth Workshop
on Syntax, Semantics and Structure in Sta-
tistical Translation. Association
for Compu-
tational Linguistics, Doha, Qatar, pages 103–111.
http://www.aclweb.org/anthology/W14-4012.

van Merrienboer, Dz-
and Yoshua Bengio. 2014.

Linguistics,

pages

Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Na-
dir Durrani, Fahim Dalvi, and James Glass. 2017b.
Evaluating Layers of Representation in Neural Ma-
chine Translation on Part-of-Speech and Semantic
In Proceedings of the Eighth In-
Tagging Tasks.
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Asian Fede-
ration of Natural Language Processing, Taipei, Tai-
wan, pages 1–10.

Rahul Bhagat and Eduard Hovy. 2013. What is a
paraphrase? Computational Linguistics 39(3):463–
472.

Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics,
Baltimore, Maryland, USA, pages 12–58.

Ronan Collobert, Koray Kavukcuoglu, and Clément
Farabet. 2011. Torch7: A Matlab-like Environment
In BigLearn, NIPS works-
for Machine Learning.
hop. EPFL-CONF-192376.

Alexis Conneau, Douwe Kiela, Holger Schwenk,
2017.

and Antoine Bordes.

Loïc Barrault,
Supervised learning of universal sentence representations from natural language inference data.
In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 670–680.
http://aclweb.org/anthology/D17-1070.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine learning challenges. evalua-
ting predictive uncertainty, visual object classiﬁca-
tion, and recognising tectual entailment, Springer,
pages 177–190.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies 6(4):1–220.

Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan
Belinkov, and Stephan Vogel. 2017. Understanding
and improving morphological learning in the neu-
ral machine translation decoder. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
Asian Federation of Natural Language Processing,
Taipei, Taiwan, pages 142–151.

David Dowty. 1991. Thematic proto-roles and argu-

ment selection. Language pages 547–619.

Qin Gao and Stephan Vogel. 2011. Utilizing Target-
Side Semantic Role Labels to Assist Hierarchical
Phrase-based Machine Translation. In Proceedings
of Fifth Workshop on Syntax, Semantics and Structu-
re in Statistical Translation. Association for Compu-
tational Linguistics, pages 107–115.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N. Dauphin. 2017. Convolutio-
nal Sequence to Sequence Learning. In Doina Pre-
cup and Yee Whye Teh, editors, Proceedings of the
34th International Conference on Machine Lear-
ning. PMLR, International Convention Centre, Syd-
ney, Australia, volume 70 of Proceedings of Machi-
ne Learning Research, pages 1243–1252.

Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics, pages 239–246.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Incorporating copying mechanism in
Li. 2016.
In Proceedings of
sequence-to-sequence learning.
the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics,
Berlin, Germany, pages 1631–1640.

Seyedeh Leili Javadpour. 2013. Resolving pronomi-
nal anaphora using commonsense knowledge. Ph.D.
thesis, Louisiana State University.

Melvin Johnson, Mike Schuster, Quoc Le, Maxim Kri-
kun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fer-
nand a ViÃ c(cid:13)gas, Martin Wattenberg, Greg Corra-
do, Macduff Hughes, and Jeffrey Dean. 2017. Goo-
gle’s multilingual neural machine translation sys-
tem: Enabling zero-shot translation. Transactions
of the Association for Computational Linguistics
5:339–351.

Philipp Koehn. 2017. Neural machine translation. ar-

Xiv preprint arXiv:1709.07809 .

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the Ability of LSTMs to Learn
Syntax-Sensitive Dependencies. Transactions of the
Association of Computational Linguistics 4(1):521–
535.

Frederick Liu, Han Lu, and Graham Neubig. 2018.
Handling Homographs in Neural Machine Transla-
tion. In Proceedings of the 16th Annual Conferen-
ce of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Langua-
ge Technologies.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference using bi-
directional lstm model and inner-attention. arXiv
preprint arXiv:1605.09090 .

Chi-kiu Lo, Meriem Beloucif, Markus Saers, and De-
kai Wu. 2014. Xmeant: Better semantic mt eva-
In Procee-
luation without reference translations.
dings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers). volume 2, pages 765–771.

Chi-kiu Lo and Dekai Wu. 2011. Meant: an inexpensi-
ve, high-accuracy, semi-automatic metric for evalua-
ting translation utility via semantic frames. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Langua-
ge Technologies-Volume 1. Association for Compu-
tational Linguistics, pages 220–229.

Christopher

D. Manning, Mihai

and

John Bauer,

David McClosky.

Jenny Finkel, Steven

nu,
Bethard,
The Stanford CoreNLP natural language processing toolkit.
In Association for Computational Linguistics
(ACL) System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.

Surdea-
J.
2014.

Rebecca Marvin and Philipp Koehn. 2018. Explo-
ring Word Sense Disambiguation Abilities of Neu-
ral Machine Translation Systems. In Proceedings of
the 13th Conference of The Association for Machi-
ne Translation in the Americas (Volume 1: Research
Track. pages 125–131.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In I. Guyon, U. V. Lux-
burg, S. Bengio, H. Wallach, R. Fergus, S. Vishwa-
nathan, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 30, Curran As-
sociates, Inc., pages 6297–6308.

Lili Mou, Rui Men, Ge

Li, Yan Xu,
2016.

Jin.

and Zhi

Lu Zhang, Rui Yan,
Natural language inference by tree-based convolution and heuristic matching.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers). Association for Computatio-
nal Linguistics, Berlin, Germany, pages 130–136.
http://anthology.aclweb.org/P16-2022.

Tsendsuren Munkhdalai and Hong Yu. 2017. Neural
tree indexers for text understanding. In Proceedings
of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Vo-
lume 1, Long Papers. Association for Computational
Linguistics, Valencia, Spain, pages 11–21.

Nikita Nangia, Adina Williams, Angeliki Lazaridou,
and Samuel Bowman. 2017. The repeval 2017
shared task: Multi-genre natural language inferen-
ce with sentence representations. In Proceedings of
the 2nd Workshop on Evaluating Vector Space Re-
presentations for NLP. pages 1–10.

Graham Neubig. 2017. Neural machine translation and
sequence-to-sequence models: A tutorial. arXiv pre-
print arXiv:1703.01619 .

Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi,
Chris Callison-Burch, Mark Dredze, and Benjamin
Van Durme. 2015. Framenet+: Fast paraphrastic tri-
pling of framenet. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confe-
rence on Natural Language Processing (Volume 2:
Short Papers). Association for Computational Lin-
guistics, Beijing, China, pages 408–413.

Adam Poliak, Jason Naradowsky, Aparajita Haldar, Ra-
chel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infe-
rence. In The Seventh Joint Conference on Lexical
and Computational Semantics (*SEM).

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics
34(2):257–287.

Xing Shi,

Inkit Padhi, and Kevin Knight. 2016.

Does string-based neural mt learn source syntax?
In Proceedings of
the 2016 Conference on
Empirical Methods in Natural Language Pro-
cessing. Association
for Computational Lin-
guistics, Austin,
1526–1534.
https://aclweb.org/anthology/D16-1159.

Texas,

pages

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information proces-
sing systems. pages 3104–3112.

Adam Teichert, Adam Poliak, Benjamin Van Durme,
and Matthew R Gormley. 2017. Semantic proto-role
labeling. In Thirty-First AAAI Conference on Artiﬁ-
cial Intelligence (AAAI-17).

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, Curran Associates, Inc., pages
5998–6008.

Aaron Steven White, Pushpendre Rastogi, Kevin Duh,
Inference is
and Benjamin Van Durme. 2017.
everything: Recasting semantic resources into a uni-
In Proceedings of the
ﬁed evaluation framework.
Eighth International Joint Conference on Natural
Language Processing (Volume 1: Long Papers).
Asian Federation of Natural Language Processing,
Taipei, Taiwan, pages 996–1005.

John Wieting and Kevin Gimpel. 2017. Pushing the
limits of paraphrastic sentence embeddings with mi-
llions of machine translations. arXiv preprint ar-
Xiv:1711.05732 .

Altaf

and

2012.

Vincent

Rahman

Ng.
Resolving complex cases of deﬁnite pronouns: The winograd schema challenge.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning. Association for Computational Lin-
guistics,
Jeju Island, Korea, pages 777–789.
http://www.aclweb.org/anthology/D12-1071.

Drew Reisinger, Rachel Rudinger, Francis Ferraro,
Craig Harman, Kyle Rawlins, and Benjamin Van
Durme. 2015. Semantic proto-roles. Transactions
of the Association for Computational Linguistics
3:475–488.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomas Kocisky, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In International Conference on Learning Represen-
tations (ICLR).

Holger Schwenk and Matthijs Douze. 2017. Learning
joint multilingual sentence representations with neu-
ral machine translation. In Proceedings of the 2nd
Workshop on Representation Learning for NLP. pa-
ges 157–167.

Yuuki Sekizawa, Tomoyuki Kajiwara, and Mamoru
Komachi. 2017.
Improving japanese-to-english
neural machine translation by paraphrasing the tar-
get language. In Proceedings of the 4th Workshop
on Asian Translation (WAT2017). Asian Federation
of Natural Language Processing, Taipei, Taiwan, pa-
ges 64–69.

John

Wieting,
and

Jonathan

Mallin-
2017.

Kevin

son,
Gimpel.
Learning paraphrastic sentence embeddings from back-translated bitext.
In Proceedings of
the 2017 Conference on
Empirical Methods in Natural Language Pro-
cessing. Association
for Computational Lin-
guistics, Copenhagen, Denmark, pages 274–285.
https://www.aclweb.org/anthology/D17-1026.

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
arXiv
sentence understanding through inference.
preprint arXiv:1704.05426 .

Hao

Tu,

Zhou,

Zhaopeng

Shujian Huang,
Xiaohua Liu, Hang Li, and Jiajun Chen. 2017.
Chunk-based bi-scale decoder for neural machine translation.
the 55th Annual Meeting
In Proceedings of
the Association for Computational Linguis-
of
tics (Volume 2: Short Papers). Association for
Computational
580–586.
https://doi.org/10.18653/v1/P17-2092.

Linguistics,

pages

Michał Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The united nations parallel cor-
pus v1.0. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Sara Goggi, Mar-
ko Grobelnik, Bente Maegaard, Joseph Mariani, He-
lene Mazo, Asuncion Moreno, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016). European Language Re-
sources Association (ELRA), Paris, France.

A. Sentence Representations

In the experiments reported in the main paper,
we used a simple sentence representation, the ﬁrst
and last hidden states of the forward and back-
ward encoders. We concatenated them for both
the context and the hypothesis and fed to a li-
near classiﬁer. Here we compare the results of
InferSent (Conneau et al., 2017), a more in-
volved representation that was found to provide
a good sentence representation based on NLI da-
ta. Speciﬁcally, we concatenate the forward and
backward encodings for each sentence, and max-
pool over the length of the sentence, resulting in v
and u (in R2d) for the context and hypothesis. The
InferSent representation is deﬁned by

(u, v, |u − v|, u ∗ v) ∈ R8d

where the product and subtraction are ca-
rried element-wise and commas denote vector-
concatenation.

The pair representation is fed into a multi-
layered perceptron (MLP) with one hidden layer
and a ReLU non-linearity. We set the hidden layer
size to 500 dimensions, similarly to Conneau et al.
(2017). The softmax layer maps onto the number
of labels, which is either 2 or 3 depending on the
dataset.

InferSent results Table 6 shows the results
of the classiﬁer trained on NMT representations
with the InferSent architecture. Here, the repre-
sentations from NMT encoders trained on the
English-German parallel corpus slightly outper-
forms the others. Since this data used a different
corpus compared to the other language pairs, we
cannot determine whether the improved results are
due to the different target side language or corpus.
The main difference with respects to the simpler
sentence representation (Concat) is improved re-
sults on FN+. Table 7 shows the results on Multi-
NLI. It is interesting to note that, when using
the sentence representations from NMT encoders,

en-ar
en-es
en-zh
en-de

en-ar
en-es
en-zh
en-de

FN+ DPR SPRL

56.2
56.1
54.3
55.5

57.9
58.0
57.8
58.3

57.5
80.5

49.8
50.0
50.0
50.0

50.0
50.0
49.8
50.1

50.0
49.5

72.1
74.2
73.1
73.1

73.6
72.7
72.4
73.7

65.4
80.6

NMT Concat

NMT
InferSent

Majority
(White et al., 2017)

Table 6: NLI results on ﬁne-grained semantic pheno-
mena. FN+ = paraphrases; DPR = pronoun resolution;
SPRL = proto-roles. NMT representations are com-
bined with either a simple concatenation (results co-
pied from Table 2) or the InferSent representation.
State-of-the-art (SOTA) is from White et al. (2017).

MNLI-1 MNLI-2

en-ar
en-es
en-zh
en-de

en-ar
en-es
en-zh
en-de

45.9
45.7
46.6
48.0

40.1
44.9
43.7
41.3

46.6
46.7
48.2
48.9

41.8
40.8
42.1
41.1

35.6
81.10

36.5
83.21

NMT
Concat

NMT
Infer-
Sent

Majority
SOTA

Table 7: Results on language inference on MultiN-
LI (Williams et al., 2017), matched/mismatched scena-
rio (MNLI1/2).

concatenating the sentence vectors outperformed
the InferSent method on Multi-NLI.

B.

Implementation & Experimental
Details

We use 4-layer NMT systems with 500-
dimensional word embeddings and LSTM states
(i.e., d = 500). The vocabulary size is 75K words.
We train NMT models until convergence and ta-
ke the models that performed best on the deve-
lopment set for generating representations to feed
into the entailment classiﬁer. We use the hidden
states from the top encoding layer for obtaining
sentence representations since it has been hypot-
hesized that higher layers focus on word meaning,
as opposed to syntax (Belinkov et al., 2017a,b).
We remove long sentences (> 50 words) when
training both the classiﬁer and the NMT model,
as is common NMT practice (Cho et al., 2014).
During testing, we use all test sentences regard-

Sentence length

ar

es

zh

de

total

0-10

10-20

20-30
30-40

40-50

50-60

60-70

70-80

46.8

49.0

48.4
48.4

47.7

49.1

46.4

59.9

63.7

53.3

54.0
54.1

59.0

56.1

53.6

51.6

66.0

57.4

53.2
51.2

55.0

54.5

43.9

43.3

65.4

56.5

54.9
53.9

58.7

57.5

44.1

43.3

526

2739

4889
4057

2064

877

444

252

Table 8: Accuracies on FN+’s dev set based on sen-
tence length. The ﬁrst column represents the range of
sentences length: ﬁrst number is inclusive and second
is exclusive. The last column represents how many con-
text sentences have lengths that are in the given row’s
range.

sectetuer id, vulputate a, magna. Donec vehicula
augue eu neque. Pellentesque habitant morbi tristi-
que senectus et netus et malesuada fames ac turpis
egestas. Mauris ut leo. Cras viverra metus rhon-
cus sem. Nulla et lectus vestibulum urna fringilla
ultrices. Phasellus eu tellus sit amet tortor gravi-
da placerat. Integer sapien est, iaculis in, pretium
quis, viverra ac, nunc. Praesent eget sem vel leo
ultrices bibendum. Aenean faucibus. Morbi dolor
nulla, malesuada eu, pulvinar at, mollis ac, nulla.
Curabitur auctor semper nulla. Donec varius or-
ci eget risus. Duis nibh mi, congue eu, accumsan
eleifend, sagittis quis, diam. Duis eget orci sit amet
orci dignissim rutrum.

less of sentence length. Our implementation ex-
tends Belinkov et al. (2017a)’s implementation in
Torch (Collobert et al., 2011).

We

train English→Arabic/Spanish/Chinese
NMT models on the ﬁrst 2 million senten-
corpus
ces of
the United Nations parallel
training set
and the
English→German model on the WMT data-
(Bojar et al., 2014). We use the ofﬁcial
set
training/development/test splits.

(Ziemski et al., 2016),

In our NLI experiments, we do not train on
Multi-NLI and test on the recast datasets, or vice-
versa, since Multi-NLI since Multi-NLI uses a 3-
way classiﬁcation (entailment, neutral, and con-
tradictions) while the recast datasets use just two
labels (entailed and not-entailed). In preliminary
experiments, we also used a 3-layered MLP. Alt-
hough the results slightly improved, we noted si-
milar trends to the linear classiﬁer.

C. Sentence length

The average sentence in the FN+ test dataset is
31 words and almost 10 % of the test sentences
are longer than 50 words. In SPR and DPR, each
premise sentence has on average 21 and 15 words
respectively and only 1 % of sentences in SPR ha-
ve more than 50 words. No DPR sentences have
> 50 words.

for

in FN+’s development

Table 8 reports accuracies

ranges of
sentence lengths
set.
When trained on sentence representations form
an English→Chinese,German NMT encoder, the
NLI accuracies steadily decrease. When using
English→Arabic, the accuracies stay consistent
until sentences have between 70–80 tokens while
the results from English→Spanish quickly drops
from 0–10 to 10–20 but then stays relatively con-
sistent.

D. World Knowledge in DPR

When released, Rahman and Ng (2012)’s DPR
dataset confounded the best co-reference models
because “its difﬁculty stems in part from its relian-
ce on sophisticated knowledge sources.” Table 9
includes examples that demonstrate how world
knowledge is needed to accurately predict these
recast NLI sentence-pairs.

Lorem ipsum dolor sit amet, consectetuer adi-
piscing elit. Ut purus elit, vestibulum ut, placerat
ac, adipiscing vitae, felis. Curabitur dictum gravi-
da mauris. Nam arcu libero, nonummy eget, con-

Chris was running after John, because he stole his watch
◮ Chris was running after John, because John stole his watch
◮ Chris was running after John, because Chris stole his watch

Chris was running after John, because he wanted to talk to him
◮ Chris was running after John, because Chris wanted to talk to him
◮ Chris was running after John, because John wanted to talk to him

The plane shot the rocket at the target, then it hit the target
◮ The plane shot the rocket at the target, then the rocket hit the target
◮ The plane shot the rocket at the target, then the target hit the target

Professors do a lot for students, but they are rarely thankful
◮ Professors do a lot for students, but students are rarely thankful
◮ Professors do a lot for students, but Professors are rarely thankful

MIT accepted the students, because they had good grades
◮ MIT accepted the students, because the students had good grades
◮ MIT accepted the students, because MIT had good grades

Obama beat John McCain, because he was the better candidate
◮ Obama beat John McCain, because Obama was the better candidate
◮ Obama beat John McCain, because John McCain was the better candidate

Obama beat John McCain, because he failed to win the majority of the
electoral votes
◮ Obama beat John McCain, because John McCain failed to win

the majority of the electoral votes

◮ Obama beat John McCain, because Obama failed to win

the majority of the electoral vote

Table 9: Examples from DPR’s dev set. The ﬁrst line in each section is a context and lines with ◮ are corresponding
hypotheses. ✓ (✗) in the last column indicates whether the hypothesis is entailed (or not) by the context.

✓
✗

✓
✗

✓
✗

✓
✗

✓
✗

✓
✗

✓

✗

8
1
0
2
 
y
a
M
 
6
 
 
]
L
C
.
s
c
[
 
 
2
v
9
7
7
9
0
.
4
0
8
1
:
v
i
X
r
a

On the Evaluation of Semantic Phenomena in
Neural Machine Translation Using Natural Language Inference

Adam Poliak1

James Glass2

Yonatan Belinkov2
1Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218
2Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology, Cambridge, MA 02139
{azpoliak,vandurme}@cs.jhu.edu, {belinkov,glass}@mit.edu

Benjamin Van Durme1

Abstract

We propose a process for investigating the ex-
tent to which sentence representations arising
from neural machine translation (NMT) sys-
tems encode distinct semantic phenomena. We
use these representations as features to train a
natural language inference (NLI) classiﬁer ba-
sed on datasets recast from existing semantic
annotations. In applying this process to a re-
presentative NMT system, we ﬁnd its enco-
der appears most suited to supporting inferen-
ces at the syntax-semantics interface, as com-
pared to anaphora resolution requiring world-
knowledge. We conclude with a discussion on
the merits and potential deﬁciencies of the
existing process, and how it may be improved
and extended as a broader framework for eva-
luating semantic coverage.1

1.

Introduction

What do neural machine translation (NMT)
models learn about semantics? Many resear-
chers suggest
that state-of-the-art NMT mo-
dels learn representations that capture the mea-
ning of sentences (Gu et al., 2016; Johnson et al.,
2017; Zhou et al., 2017; Andreas and Klein, 2017;
Neubig, 2017; Koehn, 2017). However, there is li-
mited understanding of how speciﬁc semantic phe-
nomena are captured in NMT representations be-
yond this broad notion. For instance, how well do
these representations capture Dowty (1991)’s the-
matic proto-roles? Are these representations suf-
ﬁcient for understanding paraphrastic inference?
Do the sentence representations encompass com-
plex anaphora resolution? We argue that existing
semantic annotations recast as Natural Langua-
ge Inference (NLI) can be leveraged to investi-
gate whether sentence representations encoded by
NMT models capture these semantic phenomena.

DPR

FN+

SPR

Sara adopted Jill, she wanted a child
Sara adopted Jill, Jill wanted a child

Iran possesses ﬁve research reactors
Iran has ﬁve research reactors

Berry Rejoins WPP Group
Berry was sentient

✗

✓

✓

Figure 1: Example sentence pairs for the different se-
mantic phenomena. DPR deals with complex anaphora
resolution, FN+ is concerned with paraphrastic inferen-
ce, and SPR covers Reisinger et al. (2015)’s semantic
proto-roles. ✓ / ✗ indicates that the ﬁrst sentence en-
tails / does not entail the second.

We use sentence representations from pre-
trained NMT encoders as features to train classi-
ﬁers for NLI, the task of determining if one sen-
tence (a hypothesis) is supported by another (a
context).2 If the sentence representations learned
by NMT models capture distinct semantic pheno-
mena, we hypothesize that those representations
should be sufﬁcient to perform well on NLI da-
tasets that test a model’s ability to capture these
phenomena. Figure 1 shows example NLI senten-
ce pairs with their respective labels and semantic
phenomena.

We evaluate NMT sentence representations of 4
NMT models from 2 domains on 4 different NLI
datasets to investigate how well they capture dif-
ferent semantic phenomena. We use White et al.
(2017)’s Uniﬁed Semantic Evaluation Framework
(USEF) that recasts three semantic phenomena
NLI: 1) semantic proto-roles, 2) paraphrastic in-
ference, 3) and complex anaphora resolution. Ad-
ditionally, we evaluate the NMT sentence repre-
sentations on 4) Multi-NLI, a recent extension of
the Stanford Natural Language Inference dataset
(SNLI) (Bowman et al., 2015) that includes mul-
tiple genres and domains (Williams et al., 2017).

1Code developed and data used are available at

2Sometimes referred to as recognizing textual entail-

https://github.com/boknilev/nmt-repr-analysis.

ment (Dagan et al., 2006, 2013).

We contextualize our results with a standard neu-
ral encoder described in Bowman et al. (2015) and
used in White et al. (2017).

Based on the recast NLI datasets, our investiga-
tion suggests that NMT encoders might learn mo-
re about semantic proto-roles than anaphora reso-
lution or paraphrastic inference. We note that the
target-side language affects how an NMT source-
side encoder captures these semantic phenomena.

2. Motivation

Why use recast NLI? We focus on NLI, as op-
posed to a wide range of NLP taks, as a uniﬁed
framework that can capture a variety of seman-
tic phenomena based on arguments by White et al.
(2017). Their recast dataset enables us to study
whether NMT encoders capture “distinct types
of semantic reasoning” under just one task. We
choose these speciﬁc semantic phenomena for two
reasons. First, a long term goal is to understand
how combinations of different corpora and neural
architectures can contribute to a system’s ability to
perform general language understanding. As hu-
mans can understand (annotate consistently) the
sentence pairs used in our experiments, we would
similarly like our ﬁnal system to have this same
capability. We posit that it is necessary but not ne-
cessarily sufﬁcient for a language understanding
system to be able to capture the semantic pheno-
mena considered here. Second, we believe these
semantic phenomena might be relevant for trans-
lation. We demonstrate this with a few examples.

Anaphora Anaphora resolution connects to-
kens, typically pronouns, to their referents. Anap-
hora resolution should occur when translating
from morphologically poor languages into so-
me morphologically rich languages. For example,
when translating “The parent fed the child becau-
se she was hungry,” a Spanish translation should
describe the child as la niña (fem.) and not el
niño (masc.) since she refers to the child. Be-
cause world knowledge is often required to per-
form anaphora resolution (Rahman and Ng, 2012;
Javadpour, 2013),
this may enable evaluating
whether an NMT encoder learns world knowled-
ge. In this example, she refers to the child and
not the parent since world knowledge dictates
that parents often feed children when children are
hungry.

Proto-roles Dowty (1991)’s proto-roles may be
expressed differently in different languages, and
so correctly identifying them can be important for
translation. For example, English does not usually
explicitly mark volition, a proto-role, except by
using adverbs like intentionally or accidentally.
Other languages mark volitionality by using spe-
cial afﬁxes (e.g., Tibetan and Sesotho, a Bantu lan-
guage), case marking (Hindi, Sinhalese), or au-
xiliaries (Japanese).3 Correctly generating these
markers may require the MT system to encode vo-
litionality on the source side.

Paraphrases Callison-Burch (2007) discusses
how paraphrases help statistical MT (SMT) when
alignments from source words to target-language
words are unknown. If the alignment model can
map a paraphrase of the source word to a word
in the target language, then the SMT model can
translate the original word based on its paraph-
rase.4 Paraphrases are also used by professional
translators to deal with non-equivalence of words
in the source and target languages (Baker, 2018).

3. Methodology

to extract

We use NMT models based on bidirectio-
nal long short-term memory (Bi-LSTM) encoder-
decoders with attention (Sutskever et al., 2014;
Bahdanau et al., 2015), trained on a parallel cor-
pus. Given an NLI context-hypothesis pair, we
pass each sentence independently through a trai-
ned NMT encoder
their respective
vector representations. We represent each sen-
tence by concatenating the last hidden state
from the forward and backward encoders, re-
sulting in v and u (in R2d)
the con-
text and hypothesis.5 We follow the common
practice of feeding the concatenation (v, u) ∈
R4d to a classiﬁer
(Rocktäschel et al., 2016;
Bowman et al., 2015; Mou et al., 2016; Liu et al.,
2016; Cheng et al., 2016; Munkhdalai and Yu,
2017).

for

Sentence pair representations are fed into a clas-
siﬁer with a softmax layer that maps onto the num-
ber of labels. Experiments with both linear and

3

For

references

and

examples,

see:

en.wikipedia.org/wiki/Volition_(linguistics).
4Using paraphrases can help NMT models generate text in
the target language in some settings (Sekizawa et al., 2017).
5We experimented with other sentence representations
and their combinations, and did not see differences in ove-
rall conclusions. See Appendix A for these experiments.

Test

DPR: 50.0
zh

de

Train

ar

es

USEF

ar

es

USEF

ar

es

SPR: 65.4
zh

de

FN+: 57.5
zh

de

DPR
SPR
FN+

49.8
50.1
50.0

50.0
50.3
50.0

50.0
50.1
50.4

50.0
49.9
50.0

49.5
50.7
49.5

45.4
72.1
57.3

57.1
74.2
63.6

47.0
73.6
54.5

43.9
73.1
60.7

65.2
80.6
60.0

48.0
56.3
56.2

55.9
57.0
56.1

51.0
56.9
54.3

46.8
56.1
55.5

USEF

19.2
65.8
80.5

Table 1: Accuracy on NLI with representations generated by encoders of English→{ar,es,zh,de} NMT models.
Rows correspond to the training and validation sets and major columns correspond to the test set. The column
labeled “USEF” refers to the test accuracies reported in White et al. (2017). The numbers on the top row represents
each dataset’s majority baseline. Bold numbers indicate the highest performing model for the given dataset.

non-linear classiﬁers have not shown major diffe-
rences, so we report results with the linear classi-
ﬁer unless noted otherwise. We report implemen-
tation details in Appendix B.

4. Data

MT data We train NMT models on four lan-
guage pairs: English → {Arabic (ar), Spanish
(es), Chinese (zh), and German (de)}. See Appen-
dix B for training details. The ﬁrst three pairs use
the United Nations parallel corpus (Ziemski et al.,
2016) and for English-German, we use the WMT
dataset (Bojar et al., 2014). Although the entail-
ment classiﬁer only uses representations extracted
from the English-side encoders as features, using
multiple language pairs allows us to explore whet-
her different target languages affect what semantic
phenomena are captured by an NMT encoder.

Natural Language Inference data We use
four distinct datasets to train classiﬁers: Multi-
(Williams et al., 2017), a recent expan-
NLI
sion of SNLI containing a broad array of do-
mains that was used in the 2017 RepEval sha-
red task (Nangia et al., 2017), and three re-
cast NLI datasets from The JHU Decomposi-
tional Semantics Initiative (Decomp)6 released
by White et al. (2017). Sentence-pairs and labels
were recast,
i.e. automatically converted, from
existing semantic annotations: FrameNet Plus
(FN+) (Pavlick et al., 2015), Deﬁnite Pronoun Re-
solution (DPR) (Rahman and Ng, 2012), and Se-
mantic Proto-Roles (SPR) (Reisinger et al., 2015).
The FN+ portion contains sentence pairs based on
paraphrastic inference, DPR’s sentence pairs focus
on identifying the correct antecedent for a deﬁni-
te pronoun, and SPR’s sentence pairs test whet-
her the semantic proto-roles from Reisinger et al.
(2015) apply based on a given sentence.7 Recas-
ting makes it easy to determine how well an NLI

6decomp.net
7We refer the reader to White et al. (2017) for detailed dis-

cussion on how the existing datasets were recast as NLI.

DPR

SPR

FN+ MNLI

Train
Dev
Test

2K 123K 124K
15K
15K
.4K
14K
15K
1K

393K
9K
9K

Table 2: Number of sentences in NLI datasets.

method captures the ﬁne-grained semantics inspi-
red by Dowty (1991)’s thematic proto-roles, pa-
raphrastic inference, and complex anaphora reso-
lutions. Table 2 includes the datasets’ statistics.

5. Results

Table 1 shows results of NLI classiﬁers trained
on representations from different NMT encoders.
We also report the majority baseline and the results
of Bowman et al.’s 3-layer deep 200 dimensional
neural network used by White et al. (“USEF”).

Paraphrastic entailment (FN+) Our classiﬁers
predict FN+ entailment worse than the majo-
rity baseline, and drastically worse than USEF
when trained on FN+’s training set. Since FN+
tests paraphrastic inference and NMT models ha-
ve been shown to be useful to generate senten-
tial paraphrase pairs (Wieting and Gimpel, 2017;
Wieting et al., 2017), it is surprising that our clas-
siﬁers using the representations from the NMT en-
coder perform poorly. Although the sentences in
FN+ are much longer than in the other datasets,
sentence length does not seem to be responsible
for the poor FN+ results. The classiﬁers do not no-
ticeably perform better on shorter sentences than
longer ones, as noted in Appendix C.

Upon manual inspection, we noticed that in
many not-entailed examples, swapped paraphra-
ses had different part-of-speech (POS) tags. This
begs the question of whether different POS tags
for swapped paraphrases affects the accuracies.
Using Stanford CoreNLP (Manning et al., 2014),
we partition our validation set based on whether
the paraphrases share the same POS tag. Table 3
reports dev set accuracies using classiﬁers trained

Same Tag
Different Tag

ar

52.9
55.8

es

52.6
59.1

zh

52.6
53.4

de

50.2
46.0

Table 3: Accuracies on FN+’s dev set based on whether
the swapped paraphrases share the same POS tag.

on FN+. Classiﬁers using features from NMT en-
coders trained on the three languages from the UN
corpus noticeably perform better on cases where
paraphrases have different POS tags compared to
paraphrases with the same POS tags. These dif-
ferences might suggest that the recast FN+ might
not be an ideal dataset to test how well NMT enco-
ders capture paraphrastic inference. The sentence
representations may be impacted more by ungram-
maticality caused by different POS tags as oppo-
sed to poor paraphrases.

Anaphora entailment (DPR)
The low accura-
cies for predicting NLI targeting anaphora resolu-
tion are similar to White et al. (2017)’s ﬁndings.
They suggest that the model has difﬁculty in cap-
turing complex anaphora resolution. By using con-
trastive evaluation pairs, Bawden et al. (2017) re-
cently suggested as well that NMT models are
poorly suited for co-reference resolution. Our re-
sults are not surprising given that DPR tests whet-
her a model contains common sense knowled-
ge (Rahman and Ng, 2012). In DPR, syntactic
cues for co-reference are purposefully balanced
out as each pair of pro-nouns appears in at least
two context-hypothesis pairs (Table 9). This forces
the model’s decision to be informed by semantics
and world knowledge – a model cannot use syn-
tactic cues to help perform anaphora resolution.8
Although the poor performance of NMT represen-
tations may be explained by a variety of reasons,
e.g. training data, architectures, etc., we would still
like ideal MT systems to capture the semantics of
co-reference, as evidenced in the example in §2.

Even though the classiﬁers perform poorly
when predicting paraphrastic entailment, they sur-
prisingly outperform USEF by a large margin
(around 25–30 %) when using a model trained on
DPR.9 This might suggest that an NMT encoder
can pick up on how pronouns may be used as a ty-
pe of lexical paraphrase (Bhagat and Hovy, 2013).

Proto-role entailment (SPR) When predicting
SPR entailments using a classiﬁer trained on SPR

Proto-Role

ar

es

zh

de

avg MAJ

physically existed

sentient

aware

volitional

existed before

caused

changed

location

moved

used in

existed after

chang. state

chang. possession

stationary during

physical contact
existed during

70.6
78.5
75.9
74.3
68.4
69.4

64.2
91.1
90.6
34.9
62.7
61.8
89.6
86.3
85.0
59.3

70.8
82.2
77.0
76.8
70.5
74.1

62.4
90.1
88.8
38.1
69.0
60.7
88.6
84.4
82.0
71.8

77.2
80.5
76.6
74.7
66.5
72.2

63.8
90.4
90.1
31.8
65.6
60.9
89.9
90.5
84.5
60.8

70.8
81.7
76.7
73.7
68.4
72.7

62.0
90.2
90.3
34.2
65.2
60.7
88.3
86.0
84.4
64.4

72.4†
80.7†
76.6†
74.9†
68.5†
72.1†

63.1
90.4
89.9
34.7
65.7
61.0
89.1
86.8
84.0
64.1

65.9
75.5
60.9
64.5
64.8
63.4

65.1
91.7
93.3
55.2
69.7
65.2
93.9
96.3
85.8
84.7

Table 4: Accuracies on the SPR test set broken down
by each proto-role. “avg” represents the score for the
proto-role averaged across target languages. Bold and
† respectively indicate the best results for each proto-
role and whether all of our classiﬁers outperformed the
proto-role’s majority baseline.

data, we noticeably outperform the majority base-
line but are below USEF. Both ours and USEF’s
accuracies are lower than Teichert et al. (2017)’s
best reported numbers. This is not surprising as
Teichert et al. condition on observed semantic role
labels when predicting proto-role labels.

Table 4 reports accuracies for each proto-role.
Whenever one of the classiﬁers outperforms the
baseline for a proto-role, all the other classiﬁers
do as well. The classiﬁers outperform the majo-
rity baseline for 6 of the reported 16 proto-roles.
We observe these 6 properties are more associated
with proto-agents than proto-patients.

The larger improvements over the majority ba-
seline for SPR compared to FN+ and DPR is
not surprising. Dowty (1991) posited that proto-
agent, and -patient should correlate with En-
glish syntactic subject, and object, respectively,
and empirically the necessity of [syntactic] par-
sing for predicate argument recognition has been
observed in practice (Gildea and Palmer, 2002;
Punyakanok et al., 2008). Further, recent work is
suggestive that LSTM-based frameworks impli-
citly may encode syntax based on certain learning
objectives (Linzen et al., 2016; Shi et al., 2016;
Belinkov et al., 2017b). It is unclear whether NMT
encoders capture semantic proto-roles speciﬁcally
or just underlying syntax that affects the proto-
roles.

8Appendix D includes some illustrative examples.
9This is seen in the last columns of the top row in Table 1.

NMT target language Our experiments show
differences based on which target language was

ar

45.9
46.6

es

45.7
46.7

zh

46.6
48.2

de MAJ

48.0
48.9

35.6
36.5

MNLI-1
MNLI-2

Table 5: Accuracies for MNLI test sets. MNLI-1 refers
to the matched case and MNLI-2 is the mismatched.

used to train the NMT encoder, in capturing se-
mantic proto-roles and paraphrastic inference. In
Table 1, we notice a large improvement using sen-
tence representations from an NMT encoder that
was trained on en-es parallel text. The improve-
ments are most profound when a classiﬁer trained
on DPR data predicts entailment focused on se-
mantic proto-roles or paraphrastic inference. We
also note that using the NMT encoder trained on
en-es parallel text results in the highest results in
5 of the 6 proto-roles in the top portion of Ta-
ble 4. When using other sentence representations
(Appendix A), we notice that using representa-
tions from English-German encoders consistently
outperforms using the other encoders (Tables 6
and 7). This prevents us from making generaliza-
tions regarding speciﬁc target side languages.

NLI across multiple domains Though our main
focus is exploring what NMT encoders learn about
distinct semantic phenomena, we would like to
know how useful NMT models are for general
NLI across multiple domains. Therefore, we also
evaluate the sentence representations with Multi-
NLI. As indicated by Table 5, the representa-
tions perform noticeably better than a majority ba-
seline. However, our results are not competitive
with state-of-the-art systems trained speciﬁcally
for Multi-NLI (Nangia et al., 2017).

6. Related Work

In concurrent work, Poliak et al. (2018) explore
whether NLI datasets contain statistical irregulari-
ties by training a model with access to only hypot-
heses. Their model signiﬁcantly outperforms the
majority baseline and our results on Multi-NLI,
SPR, and FN+. They suggest that these, among ot-
her NLI datasets, contain statistical irregularities.
Their ﬁndings illuminate issues with the recast da-
tasets we consider, but do not invalidate our ap-
proach of using recast NLI to determine whether
NMT encoders capture distinct semantic pheno-
mena. Instead, they force us to re-evaluate the ma-
jority baseline as an indicator of whether encoders
learn distinct semantics and to what extent we can
make conclusions based on these recast datasets.

Prior work has focused on the relationship bet-
ween semantics and machine translation. MEANT
and its extension XMEANT evaluate MT systems
based on semantics (Lo and Wu, 2011; Lo et al.,
2014). Others have focused on incorporating se-
mantics directly in MT. Chan et al. (2007) use
word sense disambiguation to help statistical MT,
Gao and Vogel (2011) add semantic-roles to im-
prove phrase-based MT, and Carpuat et al. (2017)
demonstrate how ﬁltering parallel sentences that
are not parallel
in meaning improves transla-
tion. Recent work explores how representations
learned by NMT systems can improve semantic
tasks. McCann et al. (2017) show improvements
in many tasks by using contextualized word vec-
tors extracted from a LSTM encoder trained for
MT. Their goal is to use NMT to improve other
tasks while we focus on using NLI to determine
what NMT models learn about different semantic
phenomena.

Researchers have explored what NMT models
learn about other linguistic phenomena, such as
morphology (Dalvi et al., 2017; Belinkov et al.,
2017a), syntax (Shi et al., 2016), and lexical se-
mantics (Belinkov et al., 2017b), including word
senses (Marvin and Koehn, 2018; Liu et al., 2018)

7. Conclusion and Future Work

Researchers suggest that NMT models learn
sentence representations that capture meaning. We
inspected whether distinct types of semantics are
captured by NMT encoders. Our experiments sug-
gest that NMT encoders might learn the most
about semantic proto-roles, do not focus on anap-
hora resolution, and may poorly capture paraph-
rastic inference. We conclude by suggesting that
target-side language affects how well an NMT en-
coder captures these semantic phenomena.

In future work, we would like to study how
well NMT encoders capture other semantic pheno-
mena, possibly by recasting other datasets. Com-
paring how semantic phenomena are represen-
ted in different NMT architectures, e.g. purely
convolutional (Gehring et al., 2017) or attention-
based (Vaswani et al., 2017), may shed light on
whether different architectures may better captu-
re semantic phenomena. Finally, investigating how
multilingual systems learn semantics can bring a
new perspective to questions of universality of re-
presentation (Schwenk and Douze, 2017).

Acknowledgements

This work was supported by JHU-HLTCOE,
DARPA LORELEI, and Qatar Computing Re-
search Institute. We thank anonymous reviewers.
Views and conclusions contained in this publica-
tion are those of the authors and should not be in-
terpreted as representing ofﬁcial policies or endor-
sements of DARPA or the U.S. Government.

References

Jacob Andreas and Dan Klein. 2017. Analogs of lin-
In Pro-
guistic structure in deep representations.
ceedings of the 2017 Conference on Empirical Met-
hods in Natural Language Processing. Association
for Computational Linguistics, pages 2893–2897.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Mona Baker. 2018. In other words: A coursebook on

translation. Routledge.

Rachel Bawden, Rico Sennrich, Alexandra Birch, and
Barry Haddow. 2017. Evaluating discourse pheno-
mena in neural machine translation. arXiv preprint
arXiv:1711.00513 .

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infe-
In Proceedings of the 2015 Conference on
rence.
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Chris Callison-Burch.

Paraphrasing
and
thesis, Univer-
Scotland.
sity
http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf.

Ph.D.
Edinburgh,

Translation.
of

Edinburgh,

2007.

Marine Carpuat, Yogarshi Vyas, and Xing Niu. 2017.

Detecting cross-lingual semantic divergence for neural machine translation.
on
In Proceedings
Neural Machine
Association
for Computational Linguistics, pages 69–79.
http://aclweb.org/anthology/W17-3209.

the First Workshop
Translation.

of

Seng Yee Chan, Tou Hwee Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Sta-
tistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics. Association for Computational
Linguistics, pages 33–40.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
In Proceedings of the 2016 Conference
reading.
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
Austin, Texas, pages 551–561.

Yonatan Belinkov, Nadir Durrani, Fahim Dal-
vi, Hassan Sajjad, and James Glass. 2017a.
What do Neural Machine Translation Models Learn about Morphology?
In Proceedings of
the 55th Annual Meeting
of
the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for
Computational
861–872.
https://doi.org/10.18653/v1/P17-1080.

Kyunghyun Cho, Bart
mitry Bahdanau,
On the properties of neural machine translation: Encoder–decoder approaches.
In Proceedings of SSST-8, Eighth Workshop
on Syntax, Semantics and Structure in Sta-
tistical Translation. Association
for Compu-
tational Linguistics, Doha, Qatar, pages 103–111.
http://www.aclweb.org/anthology/W14-4012.

van Merrienboer, Dz-
and Yoshua Bengio. 2014.

Linguistics,

pages

Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Na-
dir Durrani, Fahim Dalvi, and James Glass. 2017b.
Evaluating Layers of Representation in Neural Ma-
chine Translation on Part-of-Speech and Semantic
In Proceedings of the Eighth In-
Tagging Tasks.
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Asian Fede-
ration of Natural Language Processing, Taipei, Tai-
wan, pages 1–10.

Rahul Bhagat and Eduard Hovy. 2013. What is a
paraphrase? Computational Linguistics 39(3):463–
472.

Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics,
Baltimore, Maryland, USA, pages 12–58.

Ronan Collobert, Koray Kavukcuoglu, and Clément
Farabet. 2011. Torch7: A Matlab-like Environment
In BigLearn, NIPS works-
for Machine Learning.
hop. EPFL-CONF-192376.

Alexis Conneau, Douwe Kiela, Holger Schwenk,
2017.

and Antoine Bordes.

Loïc Barrault,
Supervised learning of universal sentence representations from natural language inference data.
In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 670–680.
http://aclweb.org/anthology/D17-1070.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine learning challenges. evalua-
ting predictive uncertainty, visual object classiﬁca-
tion, and recognising tectual entailment, Springer,
pages 177–190.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies 6(4):1–220.

Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan
Belinkov, and Stephan Vogel. 2017. Understanding
and improving morphological learning in the neu-
ral machine translation decoder. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
Asian Federation of Natural Language Processing,
Taipei, Taiwan, pages 142–151.

David Dowty. 1991. Thematic proto-roles and argu-

ment selection. Language pages 547–619.

Qin Gao and Stephan Vogel. 2011. Utilizing Target-
Side Semantic Role Labels to Assist Hierarchical
Phrase-based Machine Translation. In Proceedings
of Fifth Workshop on Syntax, Semantics and Structu-
re in Statistical Translation. Association for Compu-
tational Linguistics, pages 107–115.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N. Dauphin. 2017. Convolutio-
nal Sequence to Sequence Learning. In Doina Pre-
cup and Yee Whye Teh, editors, Proceedings of the
34th International Conference on Machine Lear-
ning. PMLR, International Convention Centre, Syd-
ney, Australia, volume 70 of Proceedings of Machi-
ne Learning Research, pages 1243–1252.

Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics, pages 239–246.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Incorporating copying mechanism in
Li. 2016.
In Proceedings of
sequence-to-sequence learning.
the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics,
Berlin, Germany, pages 1631–1640.

Seyedeh Leili Javadpour. 2013. Resolving pronomi-
nal anaphora using commonsense knowledge. Ph.D.
thesis, Louisiana State University.

Melvin Johnson, Mike Schuster, Quoc Le, Maxim Kri-
kun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fer-
nand a ViÃ c(cid:13)gas, Martin Wattenberg, Greg Corra-
do, Macduff Hughes, and Jeffrey Dean. 2017. Goo-
gle’s multilingual neural machine translation sys-
tem: Enabling zero-shot translation. Transactions
of the Association for Computational Linguistics
5:339–351.

Philipp Koehn. 2017. Neural machine translation. ar-

Xiv preprint arXiv:1709.07809 .

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the Ability of LSTMs to Learn
Syntax-Sensitive Dependencies. Transactions of the
Association of Computational Linguistics 4(1):521–
535.

Frederick Liu, Han Lu, and Graham Neubig. 2018.
Handling Homographs in Neural Machine Transla-
tion. In Proceedings of the 16th Annual Conferen-
ce of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Langua-
ge Technologies.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference using bi-
directional lstm model and inner-attention. arXiv
preprint arXiv:1605.09090 .

Chi-kiu Lo, Meriem Beloucif, Markus Saers, and De-
kai Wu. 2014. Xmeant: Better semantic mt eva-
In Procee-
luation without reference translations.
dings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers). volume 2, pages 765–771.

Chi-kiu Lo and Dekai Wu. 2011. Meant: an inexpensi-
ve, high-accuracy, semi-automatic metric for evalua-
ting translation utility via semantic frames. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Langua-
ge Technologies-Volume 1. Association for Compu-
tational Linguistics, pages 220–229.

Christopher

D. Manning, Mihai

and

John Bauer,

David McClosky.

Jenny Finkel, Steven

nu,
Bethard,
The Stanford CoreNLP natural language processing toolkit.
In Association for Computational Linguistics
(ACL) System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.

Surdea-
J.
2014.

Rebecca Marvin and Philipp Koehn. 2018. Explo-
ring Word Sense Disambiguation Abilities of Neu-
ral Machine Translation Systems. In Proceedings of
the 13th Conference of The Association for Machi-
ne Translation in the Americas (Volume 1: Research
Track. pages 125–131.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In I. Guyon, U. V. Lux-
burg, S. Bengio, H. Wallach, R. Fergus, S. Vishwa-
nathan, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 30, Curran As-
sociates, Inc., pages 6297–6308.

Lili Mou, Rui Men, Ge

Li, Yan Xu,
2016.

Jin.

and Zhi

Lu Zhang, Rui Yan,
Natural language inference by tree-based convolution and heuristic matching.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers). Association for Computatio-
nal Linguistics, Berlin, Germany, pages 130–136.
http://anthology.aclweb.org/P16-2022.

Tsendsuren Munkhdalai and Hong Yu. 2017. Neural
tree indexers for text understanding. In Proceedings
of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Vo-
lume 1, Long Papers. Association for Computational
Linguistics, Valencia, Spain, pages 11–21.

Nikita Nangia, Adina Williams, Angeliki Lazaridou,
and Samuel Bowman. 2017. The repeval 2017
shared task: Multi-genre natural language inferen-
ce with sentence representations. In Proceedings of
the 2nd Workshop on Evaluating Vector Space Re-
presentations for NLP. pages 1–10.

Graham Neubig. 2017. Neural machine translation and
sequence-to-sequence models: A tutorial. arXiv pre-
print arXiv:1703.01619 .

Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi,
Chris Callison-Burch, Mark Dredze, and Benjamin
Van Durme. 2015. Framenet+: Fast paraphrastic tri-
pling of framenet. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confe-
rence on Natural Language Processing (Volume 2:
Short Papers). Association for Computational Lin-
guistics, Beijing, China, pages 408–413.

Adam Poliak, Jason Naradowsky, Aparajita Haldar, Ra-
chel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infe-
rence. In The Seventh Joint Conference on Lexical
and Computational Semantics (*SEM).

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics
34(2):257–287.

Xing Shi,

Inkit Padhi, and Kevin Knight. 2016.

Does string-based neural mt learn source syntax?
In Proceedings of
the 2016 Conference on
Empirical Methods in Natural Language Pro-
cessing. Association
for Computational Lin-
guistics, Austin,
1526–1534.
https://aclweb.org/anthology/D16-1159.

Texas,

pages

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information proces-
sing systems. pages 3104–3112.

Adam Teichert, Adam Poliak, Benjamin Van Durme,
and Matthew R Gormley. 2017. Semantic proto-role
labeling. In Thirty-First AAAI Conference on Artiﬁ-
cial Intelligence (AAAI-17).

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, Curran Associates, Inc., pages
5998–6008.

Aaron Steven White, Pushpendre Rastogi, Kevin Duh,
Inference is
and Benjamin Van Durme. 2017.
everything: Recasting semantic resources into a uni-
In Proceedings of the
ﬁed evaluation framework.
Eighth International Joint Conference on Natural
Language Processing (Volume 1: Long Papers).
Asian Federation of Natural Language Processing,
Taipei, Taiwan, pages 996–1005.

John Wieting and Kevin Gimpel. 2017. Pushing the
limits of paraphrastic sentence embeddings with mi-
llions of machine translations. arXiv preprint ar-
Xiv:1711.05732 .

Altaf

and

2012.

Vincent

Rahman

Ng.
Resolving complex cases of deﬁnite pronouns: The winograd schema challenge.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning. Association for Computational Lin-
guistics,
Jeju Island, Korea, pages 777–789.
http://www.aclweb.org/anthology/D12-1071.

Drew Reisinger, Rachel Rudinger, Francis Ferraro,
Craig Harman, Kyle Rawlins, and Benjamin Van
Durme. 2015. Semantic proto-roles. Transactions
of the Association for Computational Linguistics
3:475–488.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomas Kocisky, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In International Conference on Learning Represen-
tations (ICLR).

Holger Schwenk and Matthijs Douze. 2017. Learning
joint multilingual sentence representations with neu-
ral machine translation. In Proceedings of the 2nd
Workshop on Representation Learning for NLP. pa-
ges 157–167.

Yuuki Sekizawa, Tomoyuki Kajiwara, and Mamoru
Komachi. 2017.
Improving japanese-to-english
neural machine translation by paraphrasing the tar-
get language. In Proceedings of the 4th Workshop
on Asian Translation (WAT2017). Asian Federation
of Natural Language Processing, Taipei, Taiwan, pa-
ges 64–69.

John

Wieting,
and

Jonathan

Mallin-
2017.

Kevin

son,
Gimpel.
Learning paraphrastic sentence embeddings from back-translated bitext.
In Proceedings of
the 2017 Conference on
Empirical Methods in Natural Language Pro-
cessing. Association
for Computational Lin-
guistics, Copenhagen, Denmark, pages 274–285.
https://www.aclweb.org/anthology/D17-1026.

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
arXiv
sentence understanding through inference.
preprint arXiv:1704.05426 .

Hao

Tu,

Zhou,

Zhaopeng

Shujian Huang,
Xiaohua Liu, Hang Li, and Jiajun Chen. 2017.
Chunk-based bi-scale decoder for neural machine translation.
the 55th Annual Meeting
In Proceedings of
the Association for Computational Linguis-
of
tics (Volume 2: Short Papers). Association for
Computational
580–586.
https://doi.org/10.18653/v1/P17-2092.

Linguistics,

pages

Michał Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The united nations parallel cor-
pus v1.0. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Sara Goggi, Mar-
ko Grobelnik, Bente Maegaard, Joseph Mariani, He-
lene Mazo, Asuncion Moreno, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016). European Language Re-
sources Association (ELRA), Paris, France.

A. Sentence Representations

In the experiments reported in the main paper,
we used a simple sentence representation, the ﬁrst
and last hidden states of the forward and back-
ward encoders. We concatenated them for both
the context and the hypothesis and fed to a li-
near classiﬁer. Here we compare the results of
InferSent (Conneau et al., 2017), a more in-
volved representation that was found to provide
a good sentence representation based on NLI da-
ta. Speciﬁcally, we concatenate the forward and
backward encodings for each sentence, and max-
pool over the length of the sentence, resulting in v
and u (in R2d) for the context and hypothesis. The
InferSent representation is deﬁned by

(u, v, |u − v|, u ∗ v) ∈ R8d

where the product and subtraction are ca-
rried element-wise and commas denote vector-
concatenation.

The pair representation is fed into a multi-
layered perceptron (MLP) with one hidden layer
and a ReLU non-linearity. We set the hidden layer
size to 500 dimensions, similarly to Conneau et al.
(2017). The softmax layer maps onto the number
of labels, which is either 2 or 3 depending on the
dataset.

InferSent results Table 6 shows the results
of the classiﬁer trained on NMT representations
with the InferSent architecture. Here, the repre-
sentations from NMT encoders trained on the
English-German parallel corpus slightly outper-
forms the others. Since this data used a different
corpus compared to the other language pairs, we
cannot determine whether the improved results are
due to the different target side language or corpus.
The main difference with respects to the simpler
sentence representation (Concat) is improved re-
sults on FN+. Table 7 shows the results on Multi-
NLI. It is interesting to note that, when using
the sentence representations from NMT encoders,

en-ar
en-es
en-zh
en-de

en-ar
en-es
en-zh
en-de

FN+ DPR SPRL

56.2
56.1
54.3
55.5

57.9
58.0
57.8
58.3

57.5
80.5

49.8
50.0
50.0
50.0

50.0
50.0
49.8
50.1

50.0
49.5

72.1
74.2
73.1
73.1

73.6
72.7
72.4
73.7

65.4
80.6

NMT Concat

NMT
InferSent

Majority
(White et al., 2017)

Table 6: NLI results on ﬁne-grained semantic pheno-
mena. FN+ = paraphrases; DPR = pronoun resolution;
SPRL = proto-roles. NMT representations are com-
bined with either a simple concatenation (results co-
pied from Table 2) or the InferSent representation.
State-of-the-art (SOTA) is from White et al. (2017).

MNLI-1 MNLI-2

en-ar
en-es
en-zh
en-de

en-ar
en-es
en-zh
en-de

45.9
45.7
46.6
48.0

40.1
44.9
43.7
41.3

46.6
46.7
48.2
48.9

41.8
40.8
42.1
41.1

35.6
81.10

36.5
83.21

NMT
Concat

NMT
Infer-
Sent

Majority
SOTA

Table 7: Results on language inference on MultiN-
LI (Williams et al., 2017), matched/mismatched scena-
rio (MNLI1/2).

concatenating the sentence vectors outperformed
the InferSent method on Multi-NLI.

B.

Implementation & Experimental
Details

We use 4-layer NMT systems with 500-
dimensional word embeddings and LSTM states
(i.e., d = 500). The vocabulary size is 75K words.
We train NMT models until convergence and ta-
ke the models that performed best on the deve-
lopment set for generating representations to feed
into the entailment classiﬁer. We use the hidden
states from the top encoding layer for obtaining
sentence representations since it has been hypot-
hesized that higher layers focus on word meaning,
as opposed to syntax (Belinkov et al., 2017a,b).
We remove long sentences (> 50 words) when
training both the classiﬁer and the NMT model,
as is common NMT practice (Cho et al., 2014).
During testing, we use all test sentences regard-

Sentence length

ar

es

zh

de

total

0-10

10-20

20-30
30-40

40-50

50-60

60-70

70-80

46.8

49.0

48.4
48.4

47.7

49.1

46.4

59.9

63.7

53.3

54.0
54.1

59.0

56.1

53.6

51.6

66.0

57.4

53.2
51.2

55.0

54.5

43.9

43.3

65.4

56.5

54.9
53.9

58.7

57.5

44.1

43.3

526

2739

4889
4057

2064

877

444

252

Table 8: Accuracies on FN+’s dev set based on sen-
tence length. The ﬁrst column represents the range of
sentences length: ﬁrst number is inclusive and second
is exclusive. The last column represents how many con-
text sentences have lengths that are in the given row’s
range.

sectetuer id, vulputate a, magna. Donec vehicula
augue eu neque. Pellentesque habitant morbi tristi-
que senectus et netus et malesuada fames ac turpis
egestas. Mauris ut leo. Cras viverra metus rhon-
cus sem. Nulla et lectus vestibulum urna fringilla
ultrices. Phasellus eu tellus sit amet tortor gravi-
da placerat. Integer sapien est, iaculis in, pretium
quis, viverra ac, nunc. Praesent eget sem vel leo
ultrices bibendum. Aenean faucibus. Morbi dolor
nulla, malesuada eu, pulvinar at, mollis ac, nulla.
Curabitur auctor semper nulla. Donec varius or-
ci eget risus. Duis nibh mi, congue eu, accumsan
eleifend, sagittis quis, diam. Duis eget orci sit amet
orci dignissim rutrum.

less of sentence length. Our implementation ex-
tends Belinkov et al. (2017a)’s implementation in
Torch (Collobert et al., 2011).

We

train English→Arabic/Spanish/Chinese
NMT models on the ﬁrst 2 million senten-
corpus
ces of
the United Nations parallel
training set
and the
English→German model on the WMT data-
(Bojar et al., 2014). We use the ofﬁcial
set
training/development/test splits.

(Ziemski et al., 2016),

In our NLI experiments, we do not train on
Multi-NLI and test on the recast datasets, or vice-
versa, since Multi-NLI since Multi-NLI uses a 3-
way classiﬁcation (entailment, neutral, and con-
tradictions) while the recast datasets use just two
labels (entailed and not-entailed). In preliminary
experiments, we also used a 3-layered MLP. Alt-
hough the results slightly improved, we noted si-
milar trends to the linear classiﬁer.

C. Sentence length

The average sentence in the FN+ test dataset is
31 words and almost 10 % of the test sentences
are longer than 50 words. In SPR and DPR, each
premise sentence has on average 21 and 15 words
respectively and only 1 % of sentences in SPR ha-
ve more than 50 words. No DPR sentences have
> 50 words.

for

in FN+’s development

Table 8 reports accuracies

ranges of
sentence lengths
set.
When trained on sentence representations form
an English→Chinese,German NMT encoder, the
NLI accuracies steadily decrease. When using
English→Arabic, the accuracies stay consistent
until sentences have between 70–80 tokens while
the results from English→Spanish quickly drops
from 0–10 to 10–20 but then stays relatively con-
sistent.

D. World Knowledge in DPR

When released, Rahman and Ng (2012)’s DPR
dataset confounded the best co-reference models
because “its difﬁculty stems in part from its relian-
ce on sophisticated knowledge sources.” Table 9
includes examples that demonstrate how world
knowledge is needed to accurately predict these
recast NLI sentence-pairs.

Lorem ipsum dolor sit amet, consectetuer adi-
piscing elit. Ut purus elit, vestibulum ut, placerat
ac, adipiscing vitae, felis. Curabitur dictum gravi-
da mauris. Nam arcu libero, nonummy eget, con-

Chris was running after John, because he stole his watch
◮ Chris was running after John, because John stole his watch
◮ Chris was running after John, because Chris stole his watch

Chris was running after John, because he wanted to talk to him
◮ Chris was running after John, because Chris wanted to talk to him
◮ Chris was running after John, because John wanted to talk to him

The plane shot the rocket at the target, then it hit the target
◮ The plane shot the rocket at the target, then the rocket hit the target
◮ The plane shot the rocket at the target, then the target hit the target

Professors do a lot for students, but they are rarely thankful
◮ Professors do a lot for students, but students are rarely thankful
◮ Professors do a lot for students, but Professors are rarely thankful

MIT accepted the students, because they had good grades
◮ MIT accepted the students, because the students had good grades
◮ MIT accepted the students, because MIT had good grades

Obama beat John McCain, because he was the better candidate
◮ Obama beat John McCain, because Obama was the better candidate
◮ Obama beat John McCain, because John McCain was the better candidate

Obama beat John McCain, because he failed to win the majority of the
electoral votes
◮ Obama beat John McCain, because John McCain failed to win

the majority of the electoral votes

◮ Obama beat John McCain, because Obama failed to win

the majority of the electoral vote

Table 9: Examples from DPR’s dev set. The ﬁrst line in each section is a context and lines with ◮ are corresponding
hypotheses. ✓ (✗) in the last column indicates whether the hypothesis is entailed (or not) by the context.

✓
✗

✓
✗

✓
✗

✓
✗

✓
✗

✓
✗

✓

✗

