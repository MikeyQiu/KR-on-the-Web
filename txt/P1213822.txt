7
1
0
2
 
v
o
N
 
9
1
 
 
]

G
L
.
s
c
[
 
 
2
v
8
3
0
2
0
.
7
0
7
1
:
v
i
X
r
a

A Tutorial on Thompson Sampling

Daniel J. Russo1, Benjamin Van Roy2, Abbas Kazerouni2, Ian Osband3, and Zheng Wen4

1Columbia University
2Stanford University
3Google Deepmind
4Adobe Research

November 21, 2017

Abstract

Thompson sampling is an algorithm for online decision problems where actions are taken
sequentially in a manner that must balance between exploiting what is known to maximize
immediate performance and investing to accumulate new information that may improve future
performance. The algorithm addresses a broad range of problems in a computationally eﬃcient
manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application,
illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest
path problems, product assortment, recommendation, active learning with neural networks, and
reinforcement learning in Markov decision processes. Most of these problems involve complex
information structures, where information revealed by taking an action informs beliefs about
other actions. We will also discuss when and why Thompson sampling is or is not eﬀective and
relations to alternative algorithms.

1 Introduction

The multi-armed bandit problem has been the subject of decades of intense study in statistics,
operations research, electrical engineering, computer science, and economics. A “one-armed
bandit” is a somewhat antiquated term for a slot machine, which tends to “rob” players of their
money. The colorful name for our problem comes from a motivating story in which a gambler
enters a casino and sits down at a slot machine with multiple levers, or arms, that can be pulled.
When pulled, an arm produces a random payout drawn independently of the past. Because the
distribution of payouts corresponding to each arm is not listed, the player can learn it only by
experimenting. As the gambler learns about the arms’ payouts, she faces a dilemma:
in the
immediate future she expects to earn more by exploiting arms that yielded high payouts in the
past, but by continuing to explore alternative arms she may learn how to earn higher payouts
in the future. Can she develop a sequential strategy for pulling arms that balances this tradeoﬀ
and maximizes the cumulative payout earned? The following Bernoulli bandit problem is a
canonical example.

Example 1 (Bernoulli Bandit) Suppose there are K actions, and when played,
any action yields either a success or a failure. Action k ∈ {1, ..., K} produces a success
with probability 0 ≤ θk ≤ 1. The success probabilities (θ1, .., θK) are unknown to
the agent, but are ﬁxed over time, and therefore can be learned by experimentation.

1

The objective, roughly speaking, is to maximize the cumulative number of successes
over T periods, where T is relatively large compared to the number of arms K.

The “arms” in this problem might represent diﬀerent banner ads that can be
displayed on a website. Users arriving at the site are shown versions of the website
with diﬀerent banner ads. A success is associated either with a click on the ad, or
with a conversion (a sale of the item being advertised). The parameters θk represent
either the click-through-rate or conversion-rate among the population of users who
frequent the site. The website hopes to balance exploration and exploitation in order
to maximize the total number of successes.

A naive approach to this problem involves allocating some ﬁxed fraction of time
periods to exploration and in each such period sampling an arm uniformly at random,
while aiming to select successful actions in other time periods. We will observe that
such an approach can be quite wasteful even for the simple Bernoulli bandit problem
described above and can fail completely for more complicated problems.

Problems like the Bernoulli bandit described above have been studied in the decision sciences
since the second world war, as they crystallize the fundamental trade-oﬀ between exploration
and exploitation in sequential decision making. But the information revolution has created
signiﬁcant new opportunities and challenges, which have spurred a particularly intense interest
in this problem in recent years. To understand this, let us contrast the Internet advertising
example given above with the problem of choosing a banner ad to display on a highway. A
physical banner ad might be changed only once every few months, and once posted will be seen
by every individual who drives on the road. There is value to experimentation, but data is
limited, and the cost of of trying a potentially ineﬀective ad is enormous. Online, a diﬀerent
banner ad can be shown to each individual out of a large pool of users, and data from each
such interaction is stored. Small-scale experiments are now a core tool at most leading Internet
companies.

Our interest in this problem is motivated by this broad phenomenon. Machine learning is
increasingly used to make rapid data-driven decisions. While standard algorithms in supervised
machine learning learn passively from historical data, these systems often drive the generation
of their own training data through interacting with users. An online recommendation system,
for example, uses historical data to optimize current recommendations, but the outcomes of
these recommendations are then fed back into the system and used to improve future recom-
mendations. As a result, there is enormous potential beneﬁt in the design of algorithms that not
only learn from past data, but also explore systemically to generate useful data that improves
future performance. There are signiﬁcant challenges in extending algorithms designed to address
Example 1 to treat more realistic and complicated decision problems. To understand some of
these challenges, consider the problem of learning by experimentation to solve a shortest path
problem.

Example 2 (Online Shortest Path) An agent commutes from home to work
every morning. She would like to commute along the path that requires the least
average travel time, but she is uncertain of the travel time along diﬀerent routes.
How can she learn eﬃciently and minimize the total travel time over a large number
of trips?

2

We can formalize this as a shortest path problem on a graph G = (V, E) with
vertices V = {1, ..., N } and edges E. Here vertex 1 is the source (her home) and
vertex N is the destination (work). Each vertex can be thought of as an intersection,
and for two vertices i, j ∈ V , an edge (i, j) ∈ E is present if there is a direct
road connecting the two intersections. Suppose that traveling along an edge e ∈ E
requires time θe on average. If these parameters were known, the agent would select
a path (e1, .., en), consisting of a sequence of adjacent edges connecting vertices 1
and N , such that the expected total time θe1 + ... + θen is minimized.
Instead,
she chooses paths in a sequence of periods.
In period t, the realized time yt,e to
traverse edge e is drawn independently from a distribution with mean θe. The agent
sequentially chooses a path xt, observes the realized travel time (yt,e)e∈xt along each
edge in the path, and incurs cost ct = (cid:80)
yt,e equal to the total travel time. By
exploring intelligently, she hopes to minimize cumulative travel time (cid:80)T
t=1 ct over a
large number of periods T .

e∈xt

This problem is conceptually similar to the Bernoulli bandit in Example 1, but
here the number of actions is the number of paths in the graph, which generally
scales exponentially in the number of edges. This raises substantial challenges. For
moderate sized graphs, trying each possible path would require a prohibitive number
of samples, and algorithms that require enumerating and searching through the set
of all paths to reach a decision will be computationally intractable. An eﬃcient
approach therefore needs to leverage the statistical and computational structure of
problem.

In this model, the agent observes the travel time along each edge traversed in
a given period. Other feedback models are also natural: the agent might start a
timer as she leaves home and checks it once she arrives, eﬀectively only tracking
the total travel time of the chosen path. This is closer to the Bernoulli bandit
model, where only the realized reward (or cost) of the chosen arm was observed. We
have also taken the random edge-delays yt,e to be independent, conditioned on θe.
A more realistic model might treat these as correlated random variables, reﬂecting
that neighboring roads are likely to be congested at the same time. Rather than
design a specialized algorithm for each possible statistical model, we seek a general
approach to exploration that accommodates ﬂexible modeling and works for a broad

3

array of problems. We will see that Thompson sampling accommodates such ﬂexible
modeling, and oﬀers an elegant and eﬃcient approach to exploration in a wide range
of structured decision problems, including the shortest path problem described here.

Thompson sampling was ﬁrst proposed in 1933 [1, 2] for allocating experimental eﬀort in two-
armed bandit problems arising in clinical trials. The algorithm was large largely ignored in the
academic literature until recently, although it was independently rediscovered several times in
the interim [3, 4] as an eﬀective heuristic. Now, more than eight decades after it was introduced,
Thompson sampling has seen a surge of interest among industry practitioners and academics.
This was spurred partly by two inﬂuential articles that displayed the algorithm’s strong empirical
performance [5, 6].
In the subsequent ﬁve years, the literature on Thompson sampling has
grown rapidly. Adaptations of Thompson sampling have now been successfully applied in a
wide variety of domains, including revenue management [7], marketing [8], Monte Carlo tree
search [9], A/B testing [10], Internet advertising [10, 11, 12], recommendation systems [13],
hyperparameter tuning [14], and arcade games [15]; and have been used at several companies,
including Microsoft [10], Google [6, 16], LinkedIn [11, 12], Twitter, Netﬂix, and Adobe.

The objective of this tutorial is to explain when, why, and how to apply Thompson sampling.
A range of examples are used to demonstrate how the algorithm can be used to solve interesting
problems and provide clear insight into why it works and when it oﬀers substantial beneﬁt
over naive alternatives. The tutorial also provides guidance on approximations to Thompson
sampling that can simplify computation as well as practical considerations like prior distribution
speciﬁcation, safety constraints and nonstationarity. Accompanying this tutorial we also release
a Python package1 that reproduces all experiments and ﬁgures in this paper [17]. This resource
is valuable not only for reproducible research, but also as a reference implementation that
may help practioners build intuition for how to practically implement some of the ideas and
algorithms we outline in this paper. A concluding section highlights settings where Thompson
sampling performs poorly and discusses alternative approaches studied in recent literature. As
a baseline and backdrop for our discussion of Thompson sampling, we begin with an alternative
approach that does not actively explore.

2 Greedy Decisions

Greedy algorithms serve as perhaps the simplest and most common approach to online decision
problems. The following two steps are taken to generate each action: (1) estimate a model from
historical data and (2) select the action that is optimal for the estimated model, breaking ties
in an arbitrary manner. Such an algorithm is greedy in the sense that an action is chosen solely
to maximize immediate reward. Figure 1 illustrates such a scheme. At each time t, a super-
vised learning algorithm ﬁts a model to historical data pairs Ht−1 = ((x1, y1), . . . , (xt−1, yt−1)),
generating an estimate ˆθ of model parameters. The resulting model can then be used to predict
the reward rt = r(yt) from applying action xt. Here, yt is an observed outcome, while r is a
known function that represents the agent’s preferences. Given estimated model parameters ˆθ,
an optimization algorithm selects the action xt that maximizes expected reward, assuming that
θ = ˆθ. This action is then applied to the exogenous system and an outcome yt is observed.

A shortcoming of the greedy approach, which can severely curtail performance, is that it
does not actively explore. To understand this issue, it is helpful to focus on the Bernoulli bandit
setting of Example 1. In that context, the observations are rewards, so rt = r(yt) = yt. At each
time t, a greedy algorithm would generate a estimate ˆθk of the mean reward for each kth action,
and select the action that attains the maximum among these estimates.

1Full code and documentation is available at https://github.com/iosband/ts_tutorial.

4

Figure 1: Online decision algorithm.

Suppose there are three actions with mean rewards θ ∈ R3.

In particular, each time an
action k is selected, a reward of 1 is generated with probability θk. Otherwise, a reward of 0 is
generated. The mean rewards are not known to the agent. Instead, the agent’s beliefs in any
given time period about these mean rewards can be expressed in terms of posterior distributions.
Suppose that, conditioned on the observed history Ht−1, posterior distributions are represented
by the probability density functions plotted in Figure 2. These distributions represent beliefs
after the agent tries actions 1 and 2 one thousand times each, action 3 ﬁve times, receives
cumulative rewards of 600, 400, and 2, respectively, and synthesizes these observations with
uniform prior distributions over mean rewards of each action. They indicate that the agent is
conﬁdent that mean rewards for actions 1 and 2 are close to their expectations of 0.6 and 0.4.
On the other hand, the agent is highly uncertain about the mean reward of action 3, though he
expects 0.4.

The greedy algorithm would select action 1, since that oﬀers the maximal expected mean
reward. Since the uncertainty around this expected mean reward is small, observations are
unlikely to change the expectation substantially, and therefore, action 1 is likely to be selected
ad inﬁnitum. It seems reasonable to avoid action 2, since it is extremely unlikely that θ2 > θ1.
On the other hand, if the agent plans to operate over many time periods, it should try action
3. This is because there is some chance that θ3 > θ1, and if this turns out to be the case, the
agent will beneﬁt from learning that and applying action 3. To learn whether θ3 > θ1, the agent
needs to try action 3, but the greedy algorithm will unlikely ever do that. The algorithm fails
to account for uncertainty in the mean reward of action 3, which should entice the agent to
explore and learn about that action.

Dithering is a common approach to exploration that operates through randomly perturbing
actions that would be selected by a greedy algorithm. One version of dithering, called (cid:15)-greedy
exploration, applies the greedy action with probability 1 − (cid:15) and otherwise selects an action
uniformly at random. Though this form of exploration can improve behavior relative to a
purely greedy approach, it wastes resources by failing to “write oﬀ” actions regardless of how
unlikely they are to be optimal. To understand why, consider again the posterior distributions
of Figure 2. Action 2 has almost no chance of being optimal, and therefore, does not deserve
experimental trials, while the uncertainty surrounding action 3 warrants exploration. However,

5

Figure 2: Probability density functions over mean rewards.

(cid:15)-greedy exploration would allocate an equal number of experimental trials to each action.
Though only half of the exploratory actions are wasted in this example, the issue is exacerbated
as the number of possible actions increases. Thompson sampling, introduced more than eight
decades ago [1], provides an alternative to dithering that more intelligently allocates exploration
eﬀort.

3 Thompson Sampling for the Bernoulli Bandit

To digest how Thompson sampling works, it is helpful to begin with a simple context that
builds on the Bernoulli bandit of Example 1 and incorporates a Bayesian model to represent
uncertainty.

Example 3 (Beta-Bernoulli Bandit) Recall the Bernoulli bandit of Example
1. There are K actions. When played, an action k produces a reward of one with
probability θk and a reward of zero with probability 1−θk. Each θk can be interpreted
as an action’s success probability or mean reward. The mean rewards θ = (θ1, ..., θK)
are unknown, but ﬁxed over time. In the ﬁrst period, an action x1 is applied, and a
reward r1 ∈ {0, 1} is generated with success probability P(r1 = 1|x1, θ) = θx1 . After
observing r1, the agent applies another action x2, observes a reward r2, and this
process continues.

Let the agent begin with an independent prior belief over each θk. Take these
priors to be beta-distributed with parameters α = (α1, . . . , αK) and β ∈ (β1, . . . , βK).
In particular, for each action k, the prior probability density function of θk is

p(θk) =

Γ(αk + βk)
Γ(αk)Γ(βk)

θαk−1
k

(1 − θk)βk−1,

6

where Γ denotes the gamma function. As observations are gathered, the distribution
is updated according to Bayes’ rule. It is particularly convenient to work with beta
distributions because of their conjugacy properties. In particular, each action’s pos-
terior distribution is also beta with parameters that can be updated according to a
simple rule:

(cid:40)

(αk, βk) ←

(αk, βk)
(αk, βk) + (rt, 1 − rt)

if xt (cid:54)= k
if xt = k.

Note that for the special case of αk = βk = 1, the prior p(θk) is uniform over [0, 1]. Note that
only the parameters of a selected arm are updated. The parameters (αk, βk) are sometimes
called pseudo-counts, since αk or βk increases by one with each observed success or failure,
respectively. A beta distribution with parameters (αk, βk) has mean αk/(αk + βk), and the
distribution becomes more concentrated as αk + βk grows. Figure 2 plots probability density
functions of beta distributions with parameters (α1, β1) = (600, 400), (α2, β2) = (400, 600), and
(α3, β3) = (4, 6).

Algorithm 2 presents a greedy algorithm for the beta-Bernoulli bandit. In each time period
t, the algorithm generates an estimate ˆθk = αk/(αk + βk), equal to its current expectation of the
success probability θk. The action xt with the largest estimate ˆθk is then applied, after which a
reward rt is observed and the distribution parameters αxt and βxt are updated.

Algorithm 1 BernGreedy(K, α, β)

Algorithm 2 BernThompson(K, α, β)

1: for t = 1, 2, . . . do
2:

#estimate model:
for k = 1, . . . , K do

3:

ˆθk ← αk/(αk + βk)

end for

1: for t = 1, 2, . . . do
2:

#sample model:
for k = 1, . . . , K do

3:

Sample ˆθk ∼ beta(αk, βk)

end for

#select and apply action:
xt ← argmaxk
Apply xt and observe rt

ˆθk

#select and apply action:
xt ← argmaxk
Apply xt and observe rt

ˆθk

#update distribution:
(αxt, βxt) ← (αxt, βxt)+(rt, 1−rt)

#update distribution:
(αxt, βxt) ← (αxt, βxt)+(rt, 1−rt)

12:
13: end for

12:
13: end for

4:

5:

6:

7:

8:

9:

10:

11:

Thompson sampling, specialized to the case of a beta-Bernoulli bandit, proceeds similarly,
as presented in Algorithm 2. The only diﬀerence is that the success probability estimate ˆθk is
randomly sampled from the posterior distribution, which is a beta distribution with parameters
αk and βk, rather than taken to be the expectation αk/(αk + βk). To avoid a common mis-
conception, it is worth emphasizing Thompson sampling does not sample ˆθk from the posterior
distribution of the binary value yt that would be observed if action k is selected. In particular,
ˆθk represents a statistically plausible success probability rather than a statistically plausible
observation.

To understand how Thompson sampling improves on greedy actions with or without dither-
ing, recall the three armed Bernoulli bandit with posterior distributions illustrated in Figure
2. In this context, a greedy action would forgo the potentially valuable opportunity to learn

4:

5:

6:

7:

8:

9:

10:

11:

7

about action 3. With dithering, equal chances would be assigned to probing actions 2 and 3,
though probing action 2 is virtually futile since it is extremely unlikely to be optimal. Thompson
sampling, on the other hand would sample actions 1, 2, or 3, with probabilities approximately
equal to 0.82, 0, and 0.18, respectively. In each case, this is the probability that the random
estimate drawn for the action exceeds those drawn for other actions. Since these estimates are
drawn from posterior distributions, each of these probabilities is also equal to the probability
that the corresponding action is optimal, conditioned on observed history. As such, Thompson
sampling explores to resolve uncertainty where there is a chance that resolution will help the
agent identify the optimal action, but avoids probing where feedback would not be helpful.

It is illuminating to compare simulated behavior of Thompson sampling to that of a greedy
algorithm. Consider a three-armed beta-Bernoulli bandit with mean rewards θ1 = 0.9, θ2 = 0.8,
and θ3 = 0.7. Let the prior distribution over each mean reward be uniform. Figure 3 plots
results based on ten thousand independent simulations of each algorithm. Each simulation is
over one thousand time periods. In each simulation, actions are randomly rank-ordered for the
purpose of tie-breaking so that the greedy algorithm is not biased toward selecting any particular
action. Each data point represents the fraction of simulations for which a particular action is
selected at a particular time.

(a) greedy algorithm

(b) Thompson sampling

Figure 3: Probability that the greedy algorithm and Thompson sampling selects an action.

From the plots, we see that the greedy algorithm does not always converge on action 1, which
is the optimal action. This is because the algorithm can get stuck, repeatedly applying a poor
action. For example, suppose the algorithm applies action 3 over the ﬁrst couple time periods
and receives a reward of 1 on both occasions. The algorithm would then continue to select action
3, since the expected mean reward of either alternative remains at 0.5. With repeated selection
of action 3, the expected mean reward converges to the true value of 0.7, which reinforces the
agent’s commitment to action 3. Thompson sampling, on the other hand, learns to select action
1 within the thousand periods. This is evident from the fact that, in an overwhelmingly large
fraction of simulations, Thompson sampling selects action 1 in the ﬁnal period.

The performance of online decision algorithms is often studied and compared through plots
of regret. The per-period regret of an algorithm over a time period t is the diﬀerence between the
mean reward of an optimal action and the action selected by the algorithm. For the Bernoulli
bandit problem, we can write this as regrett(θ) = maxk θk−θxt. Figure 4a plots per-period regret
realized by the greedy algorithm and Thompson sampling, again averaged over ten thousand
simulations. The average per-period regret of Thompson sampling vanishes as time progresses.
That is not the case for the greedy algorithm.

Comparing algorithms with ﬁxed mean rewards raises questions about the extent to which

8

the results depend on the particular choice of θ. As such, it is often useful to also examine
regret averaged over plausible values of θ. A natural approach to this involves sampling many
instances of θ from the prior distributions and generating an independent simulation for each.
Figure 4b plots averages over ten thousand such simulations, with each action reward sampled
independently from a uniform prior for each simulation. Qualitative features of these plots are
similar to those we inferred from Figure 4a, though regret in Figure 4a is generally smaller over
early time periods and larger over later time periods, relative to Figure 4b. The smaller regret
in early time periods is due to the fact that with θ = (0.9, 0.8, 0.7), mean rewards are closer
than for a typical randomly sampled θ, and therefore the regret of randomly selected actions is
smaller. The reduction in later time periods is also a consequence of proximity among rewards
with θ = (0.9, 0.8, 0.7). In this case, the diﬀerence is due to the fact that it takes longer to
diﬀerentiate actions than it would for a typical randomly sampled θ.

(a) θ = (0.9, 0.8, 0.7)

(b) average over random θ

Figure 4: Regret from applying greedy and Thompson sampling algorithms to the three-armed
Bernoulli bandit.

4 General Thompson Sampling

Thompson sampling can be applied fruitfully to a broad array of online decision problems beyond
the Bernoulli bandit, and we now consider a more general setting. Suppose the agent applies
a sequence of actions x1, x2, x3, . . . to a system, selecting each from a set X . This action set
could be ﬁnite, as in the case of the Bernoulli bandit, or inﬁnite. After applying action xt, the
agent observes an outcome yt, which the system randomly generates according to a conditional
probability measure qθ(·|xt). The agent enjoys a reward rt = r(yt), where r is a known function.
The agent is initially uncertain about the value of θ and represents his uncertainty using a prior
distribution p.

Algorithms 3 and 4 present greedy and Thompson sampling approaches in an abstract form
that accommodates this very general problem. The two diﬀer in the way they generate model
parameters ˆθ. The greedy algorithm takes ˆθ to be the expectation of θ with respect to the
distribution p, while Thompson sampling draws a random sample from p. Both algorithms then
apply actions that maximize expected reward for their respective models. Note that, if there
are a ﬁnite set of possible observations yt, this expectation is given by

(4.1)

Eq ˆθ

[r(yt)|xt = x] =

qˆθ(o|x)r(o).

(cid:88)

o

9

(4.2)

4:

5:

6:

7:

8:

9:

The distribution p is updated by conditioning on the realized observation ˆyt. If θ is restricted
to values from a ﬁnite set, this conditional distribution can be written by Bayes rule as

Pp,q(θ = u|xt, yt) =

p(u)qu(yt|xt)
v p(v)qv(yt|xt)

.

(cid:80)

Algorithm 3 Greedy(X , p, q, r)

Algorithm 4 Thompson(X , p, q, r)

1: for t = 1, 2, . . . do
2:

#estimate model:
ˆθ ← Ep[θ]

3:

1: for t = 1, 2, . . . do
2:

#sample model:
Sample ˆθ ∼ p

3:

#select and apply action:
xt ← argmaxx∈X
Apply xt and observe yt

Eq ˆθ

[r(yt)|xt = x]

#select and apply action:
xt ← argmaxx∈X
Apply xt and observe yt

Eq ˆθ

[r(yt)|xt = x]

#update distribution:
p ← Pp,q(θ ∈ ·|xt, yt)

10:
11: end for

#update distribution:
p ← Pp,q(θ ∈ ·|xt, yt)

10:
11: end for

4:

5:

6:

7:

8:

9:

The Bernoulli bandit with a beta prior serves as a special case of this more general formu-
lation. In this special case, the set of actions is X = {1, . . . , K} and only rewards are observed,
so yt = rt. Observations and rewards are modeled by conditional probabilities qθ(1|k) = θk and
qθ(0|k) = 1 − θk. The prior distribution is encoded by vectors α and β, with probability density
function given by:

p(θ) =

K
(cid:89)

k=1

Γ(α + β)
Γ(αk)Γ(βk)

θαk−1
k

(1 − θk)βk−1,

where Γ denotes the gamma function. In other words, under the prior distribution, components
of θ are independent and beta-distributed, with parameters α and β.

For this problem, the greedy algorithm (Algorithm 3) and Thompson sampling (Algorithm
4) begin each tth iteration with posterior parameters (αe, βe) for e ∈ E. The greedy algorithm
sets ˆθe to the expected value Ep[θe] = αe/(αe + βe), whereas Thompson sampling randomly
draws ˆθe from a beta distribution with parameters (αe, βe). Each algorithm then selects the
[r(yt)|xt = x] = ˆθx. After applying the selected action, a reward
action x that maximizes Eq ˆθ
rt = yt is observed, and belief distribution parameters are updated according to

(α, β) ← (α + rt1xt, β + (1 − rt)1xt),

where 1xt is a vector with component xt equal to 1 and all other components equal to 0.

Algorithms 3 and 4 can also be applied to much more complex problems. As an example,

let us consider a version of the shortest path problem presented in Example 2.

Example 4 (Independent Travel Times) Recall the shortest path problem of
Example 2. The model is deﬁned with respect to a directed graph G = (V, E), with
vertices V = {1, . . . , N }, edges E, and mean travel times θ ∈ RN . Vertex 1 is the
source and vertex N is the destination. An action is a sequence of distinct edges
leading from source to destination. After applying action xt, for each traversed edge
e ∈ xt, the agent observes a travel time yt,e that is independently sampled from a

10

distribution with mean θe. Further, the agent incurs a cost of (cid:80)
be thought of as a reward rt = − (cid:80)

yt,e.
Consider a prior for which each θe is independent and lognormally-distributed
e . That is, ln(θe) ∼ N (µe, σ2
with parameters µe and σ2
e ) is normally distributed.
Hence, E[θe] = eµe+σ2
e /2. Further, take yt,e|θ to be independent across edges e ∈ E
and lognormally distributed with parameters ln θe − ˜σ2/2 and ˜σ2, so that E[yt,e|θe] =
θe. Conjugacy properties accommodate a simple rule for updating the distribution
of θe upon observation of yt,e:

yt,e, which can

e∈xt

e∈xt

(4.3)

(µe, σ2

e ) ←

1
σ2
e





µe + 1
˜σ2
1
σ2
e

(cid:17)

(cid:16)

ln yt,e + ˜σ2
2
+ 1
˜σ2

1
+ 1
˜σ2

,

1
σ2
e



 .

To motivate this formulation, consider an agent who commutes from home to work every
morning. Suppose possible paths are represented by a graph G = (V, E). Suppose the agent
knows the travel distance de associated with each edge e ∈ E but is uncertain about average
travel times. It would be natural for her to construct a prior for which expectations are equal to
travel distances. With the lognormal prior, this can be accomplished by setting µe = ln de−σ2
e /2.
Note that the parameters µe and σ2
e also express a degree of uncertainty; in particular, the prior
variance of mean travel time along an edge is (eσ2

e − 1)d2
e.

The greedy algorithm (Algorithm 3) and Thompson sampling (Algorithm 4) can be applied
to Example 4 in a computationally eﬃcient manner. Each algorithm begins each tth iteration
with posterior parameters (µe, σe) for each e ∈ E. The greedy algorithm sets ˆθe to the expected
e /2, whereas Thompson sampling randomly draws ˆθe from a lognormal
value Ep[θe] = eµe+σ2
distribution with parameters µe and σ2
e . Each algorithm then selects its action x to maximize
ˆθe. This can be cast as a deterministic shortest path problem,
Eq ˆθ
which can be solved eﬃciently, for example, via Dijkstra’s algorithm. After applying the selected
action, an outcome yt is observed, and belief distribution parameters (µe, σ2
e ), for each e ∈ E,
are updated according to (4.3).

[r(yt)|xt = x] = − (cid:80)

e∈xt

Figure 6 presents results from applying greedy and Thompson sampling algorithms to Ex-
ample 4, with the graph taking the form of a binomial bridge, as shown in Figure 5, except
with twenty rather than six stages, so there are 184,756 paths from source to destination. Prior
e = 1 so that E[θe] = 1, for each e ∈ E, and the conditional
parameters are set to µe = − 1
distribution parameter is ˜σ2 = 1. Each data point represents an average over ten thousand
independent simulations.

2 and σ2

The plots of regret demonstrate that the performance of Thompson sampling converges
quickly to that of the optimal policy, while that is far from true for the greedy algorithm. We
also plot results generated by (cid:15)-greedy exploration, varying (cid:15). For each trip, with probability
1 − (cid:15), this algorithm traverses a path produced by a greedy algorithm. Otherwise, the algorithm
samples a path randomly. Though this form of exploration can be helpful, the plots demonstrate
that learning progresses at a far slower pace than with Thompson sampling. This is because
(cid:15)-greedy exploration is not judicious in how it selects paths to explore. Thompson sampling,
on the other hand, orients exploration eﬀort towards informative rather than entirely random
paths.

Plots of cumulative travel time relative to optimal oﬀer a sense for the fraction of driving
time wasted due to lack of information. Each point plots an average of the ratio between the
time incurred over some number of days and the minimal expected travel time given θ. With
Thompson sampling, this converges to one at a respectable rate. The same can not be said for
(cid:15)-greedy approaches.

Algorithm 4 can be applied to problems with complex information structures, and there is
often substantial value to careful modeling of such structures. As an example, we consider a

11

Figure 5: A binomial bridge with six stages.

(a) regret

(b) cumulative travel time relative to optimal

Figure 6: Performance of Thompson sampling and (cid:15)-greedy algorithms in the shortest path problem.

more complex variation of the binomial bridge example.

Example 5 (Correlated Travel Times) As with Example 4, let each θe be inde-
e . Let the observation

pendent and lognormally-distributed with parameters µe and σ2
distribution be characterized by

yt,e = ζt,eηtνt,(cid:96)(e)θe,

where each ζt,e represents an idiosyncratic factor associated with edge e, ηt represents
a factor that is common to all edges, (cid:96)(e) indicates whether edge e resides in the lower
half of the binomial bridge, and νt,0 and νt,1 represent factors that bear a common
inﬂuence on edges in the upper and lower halves, respectively. We take each ζt,e,
ηt, νt,0, and νt,1 to be independent lognormally distributed with parameters −˜σ2/6
and ˜σ2/3. The distributions of the shocks ζt,e, ηt, νt,0 and νt,1 are known, and only
the parameters θe corresponding to each individual edge must be learned through
experimentation. Note that, given these parameters, the marginal distribution of
yt,e|θ is identical to that of Example 4, though the joint distribution over yt|θ diﬀers.

12

The common factors induce correlations among travel times in the binomial
bridge: ηt models the impact of random events that inﬂuence traﬃc conditions every-
where, like the day’s weather, while νt,0 and νt,1 each reﬂect events that bear inﬂuence
only on traﬃc conditions along edges in half of the binomial bridge. Though mean
edge travel times are independent under the prior, correlated observations induce
dependencies in posterior distributions.

Conjugacy properties again facilitate eﬃcient updating of posterior parameters.

Let φ, zt ∈ RN be deﬁned by

φe = ln θe

and

zt,e =

(cid:26) ln yt,e
0

if e ∈ xt
otherwise.

Note that it is with some abuse of notation that we index vectors and matrices using
edge indices. Deﬁne a |xt| × |xt| covariance matrix ˜Σ with elements

˜Σe,e(cid:48) =






˜σ2
2˜σ2/3
˜σ2/3

for e = e(cid:48)
for e (cid:54)= e(cid:48), (cid:96)(e) = (cid:96)(e(cid:48))
otherwise,

for e, e(cid:48) ∈ xt, and a N × N concentration matrix

˜Ce,e(cid:48) =

(cid:26) ˜Σ−1
e,e(cid:48)
0

if e, e(cid:48) ∈ xt
otherwise,

for e, e(cid:48) ∈ E. Then, the posterior distribution of φ is normal with a mean vector µ
and covariance matrix Σ that can be updated according to

(4.4)

(µ, Σ) ←

Σ−1 + ˜C

Σ−1µ + ˜Czt

(cid:17)−1 (cid:16)

(cid:18)(cid:16)

(cid:17)

(cid:16)

,

Σ−1 + ˜C

(cid:17)−1(cid:19)

.

Thompson sampling (Algorithm 4) can again be applied in a computationally eﬃcient manner.
Each tth iteration begins with posterior parameters µ ∈ RN and Σ ∈ RN ×N . The sample ˆθ can
be drawn by ﬁrst sampling a vector ˆφ from a normal distribution with mean µ and covariance
matrix Σ, and then setting ˆθe = ˆφe for each e ∈ E. An action x is selected to maximize
ˆθe, using Djikstra’s algorithm or an alternative. After applying the
Eq ˆθ
selected action, an outcome yt is observed, and belief distribution parameters (µ, Σ) are updated
according to (4.4).

[r(yt)|xt = x] = − (cid:80)

e∈xt

2 , σ2

Figure 7 plots results from applying Thompson sampling to Example 5, again with the
binomial bridge, µe = − 1
e = 1, and ˜σ2 = 1. Each data point represents an average
over ten thousand independent simulations. Despite model diﬀerences, an agent can pretend
that observations made in this new context are generated by the model described in Example
In particular, the agent could maintain an independent lognormal posterior for each θe,
4.
updating parameters (µe, σ2
e ) as though each yt,e|θ is independently drawn from a lognormal
distribution. As a baseline for comparison, Figure 7 additionally plots results from application of
this approach, which we will refer to here as misspeciﬁed Thompson sampling. The comparison
demonstrates substantial improvement that results from accounting for interdependencies among
edge travel times, as is done by what we refer to here as coherent Thompson sampling. Note
that we have assumed here that the agent must select a path before initiating each trip. In
particular, while the agent may be able to reduce travel times in contexts with correlated delays
by adjusting the path during the trip based on delays experienced so far, our model does not
allow this behavior.

13

(a) regret

(b) cumulative travel time relative to optimal

Figure 7: Performance of two versions of Thompson sampling in the shortest path problem with
correlated travel times.

5 Approximations

Conjugacy properties in the Bernoulli bandit and shortest path examples that we have considered
so far facilitated simple and computationally eﬃcient Bayesian inference. Indeed, computational
eﬃciency can be an important consideration when formulating a model. However, many practi-
cal contexts call for more complex models for which exact Bayesian inference is computationally
intractable. Fortunately, there are reasonably eﬃcient and accurate methods that can be used
to approximately sample from posterior distributions.

In this section we discuss four approaches to approximate posterior sampling: Gibbs sam-
pling, Langevin Monte Carlo, sampling from a Laplace approximation, and the bootstrap. Such
methods are called for when dealing with problems that are not amenable to eﬃcient Bayesian
inference. As an example, we consider a variation of the online shortest path problem.

Example 6 (Binary Feedback) Consider Example 5, except with deterministic
travel times and noisy binary observations. Let the graph represent a binomial bridge
with M stages. Let each θe be independent and gamma-distributed with E[θe] = 1,
E[θ2

e] = 1.5, and observations be generated according to

yt|θ ∼

(cid:40) 1
0

with probability

1

1+exp((cid:80)

e∈xt

θe−M)

otherwise.

We take the reward to be the rating rt = yt. This information structure could be
used to model, for example, an Internet route recommendation service. Each day, the
system recommends a route xt and receives feedback yt from the driver, expressing
whether the route was desirable. When the realized travel time (cid:80)
θe falls short
of the prior expectation M , the feedback tends to be positive, and vice versa.

e∈xt

This new model does not enjoy conjugacy properties leveraged in Section 4 and is not amenable
to eﬃcient exact Bayesian inference. However, the problem may be addressed via approximation
methods. To illustrate, Figure 8 plots results from application of two approximate versions of
Thompson sampling to an online shortest path problem on a twenty-stage binomial bridge with
binary feedback. The algorithms leverage the Laplace approximation and the bootstrap, two
approaches we will discuss, and the results demonstrate eﬀective learning, in the sense that
regret vanishes over time.

In the remainder of this section, we will describe several approaches to approximate Thomp-
son sampling. It is worth mentioning that we do not cover an exhaustive list, and further, our

14

descriptions do not serve as comprehensive or deﬁnitive treatments of each approach. Rather,
our intent is to oﬀer simple descriptions that convey key ideas that may be extended or combined
to serve needs arising in any speciﬁc application.

Throughout this section, let ft−1 denote the posterior density of θ conditioned on the history
of observations Ht−1 = ((x1, y1), . . . , (xt−1, yt−1)). Thompson sampling generates an action
xt by sampling a parameter vector ˆθ from ft−1 and solving for the optimal path under ˆθ.
The methods we describe generate a sample ˆθ whose distribution approximates the posterior
ˆft−1, which enables approximate implementations of Thompson sampling when exact posterior
sampling is infeasible.

Figure 8: Regret experienced by approximation methods applied to the path recommendation
problem with binary feedback.

5.1 Gibbs Sampling

Gibbs sampling is a general Markov chain Monte Carlo (MCMC) algorithm for drawing approx-
imate samples from multivariate probability distributions. It produces a sequence of sampled
parameters (ˆθn : n = 0, 2 . . .) forming a Markov chain with stationary distribution ft−1. Under
reasonable technical conditions, the limiting distribution of this Markov chain is its stationary
distribution, and the distribution of ˆθn converges to ft−1.

Gibbs sampling starts with an initial guess ˆθ0. Iterating over sweeps n = 1, . . . , N , for each
nth sweep, the algorithm iterates over the components k = 1, . . . , K, for each k generating a
one-dimensional marginal distribution

t−1(θk) ∝ ft−1((ˆθn
f n,k

1 , . . . , ˆθn

k−1, θk, ˆθn−1

k+1 , . . . , ˆθn−1

K )),

and sampling the kth component according to ˆθn
t−1. After N of sweeps, the prevailing
vector ˆθN is taken to be the approximate posterior sample. We refer to [18] for a more thorough
introduction to the algorithm.

k ∼ f n,k

Gibbs sampling applies to a broad range of problems, and is often computationally viable
even when sampling from ft−1 is not. This is because sampling from a one-dimensional distribu-
tion is simpler. That said, for complex problems, Gibbs sampling can still be computationally
demanding. This is the case, for example, with our path recommendation problem with binary

15

(5.1)

tion

(5.2)

feedback. In this context, it is easy to implement a version of Gibbs sampling that generates
a close approximation to a posterior samples within well under a minute. However, running
thousands of simulations each over hundreds of time periods can be quite time-consuming. As
such, we turn to more eﬃcient approximation methods.

5.2 Langevin Monte Carlo

We now describe an alternative Markov chain Monte Carlo method that uses gradient informa-
tion about the target distribution. Let g(φ) denote a log-concave probability density function
over RK from which we wish to sample. Suppose that ln g(φ) is diﬀerentiable and its gradients
are eﬃciently computable. Arising ﬁrst in physics, Langevin daynmics refer to the diﬀusion
process

dφt = ∇ ln g(φt)dt +

2dBt

√

where Bt is a standard Brownian motion process. This process has g(φ) as its unique stationary
distribution, and under reasonable technical conditions, the distribution of φt converges rapidly
to this stationary distribution [19]. Therefore simulating the process (5.1) provides a means of
approximately sampling from g(φ).

Typically, one instead implements a Euler discretization of this stochastic diﬀerential equa-

φn+1 = φn + (cid:15)∇ ln g(φn) +

2(cid:15)Wn

n ∈ N,

√

where W1, W2, · · · are i.i.d. standard normal random variables and (cid:15) > 0 is a small step size.
Like a gradient ascent method, under this method φn tends to drift in directions of increasing
density g(φn). However, random Gaussian noise Wn is injected at each step so that, for large
n, the position of φn is random and captures the uncertainty in the distribution g. A number
of papers establish rigorous guarantees for the rate at which this Markov chain converges to
its stationary distribution [20, 21, 22, 23]. These papers typically require (cid:15) is suﬃciently small,
or that a decaying sequence of step-sizes ((cid:15)1, (cid:15)2, · · · ) is used. Recent work [24, 25] has studied
stochastic gradient Langevin Monte Carlo, which uses sampled minibatches of data to compute
approximate rather than exact gradients.

For the path recommendation problem in Example 6, we have found that the log posterior
density becomes ill conditioned in later time periods. For this reason, gradient ascent converges
very slowly to the posterior mode. Eﬀective optimization methods must somehow leverage
second order information. Similarly, due to poor conditioning, we may need to choose an
extremely small step-size (cid:15), causing the Markov chain in 5.2 to mix slowly. We have found that
preconditioning substantially improves performance. Langevin MCMC can be implemented
with a symmetric positive deﬁnite preconditioning matrix M by simulating the Markov chain

φn+1 = φn + (cid:15)M ∇ ln g(φn) +

2(cid:15)M 1/2Wn

n ∈ N,

√

where M 1/2 denotes the matrix square root of M . One natural choice is to take φ0 = argmaxφ ln g(φ),
so the chain is initialized the posterior mode, and to take the preconditioning matrix M =
(∇2 ln g(φ)|φ=φ0)−1 to be the inverse Hessian at the posterior mode.

5.3 Laplace Approximation

The last two methods we described are instances of Markov chain Monte Carlo, which gen-
erate approximate samples from the target distribution by simulating a carefully constructed
Markov chain. The technique described here instead explicitly approximates a potentially com-
plicated posterior distribution by a Gaussian distribution. Samples from this simpler Gaussian

16

distribution can then serve as approximate samples from the posterior distribution of interest.
[5] proposed using this method to approximate Thompson sampling in a display advertising
problem with a logistic regression model of ad-click-through rates.

Let g denote a probability density function over RK from which we wish to sample. If g is
unimodal, and its log density ln g(φ) is strictly concave around its mode φ, then g(φ) = eln g(φ)
is sharply peaked around φ. It is therefore natural to consider approximating g locally around
its mode. A second-order Taylor approximation to the log-density gives

ln g(φ) ≈ ln g(φ) −

(φ − φ)(cid:62)C(φ − φ),

1
2

where

As an approximation to the density g, we can then use

C = −∇2 ln g(φ).

˜g(φ) ∝ e− 1

2 (φ−φ)(cid:62)C(φ−φ).

This is proportional to the density of a Gaussian distribution with mean φ and covariance C −1,
and hence

˜g(φ) = (cid:112)|C/2π|e− 1

2 (φ−φ)(cid:62)C(φ−φ).

We refer to this as the Laplace approximation of g. Since there are eﬃcient algorithms for
generating normally-distributed samples, this oﬀers a viable means to approximately sampling
from g.

As an example, let us consider application of the Laplace approximation to Example 6.

Bayes rule implies that the posterior density ft−1 of θ satisﬁes

ft−1(θ) ∝ f0(θ)

(cid:32)

t−1
(cid:89)

τ =1

1 + exp (cid:0)(cid:80)

θe − M (cid:1)

1

e∈xτ

(cid:33)yτ (cid:32)

exp (cid:0)(cid:80)
1 + exp (cid:0)(cid:80)

e∈xτ

θe − M (cid:1)

θe − M (cid:1)

e∈xτ

(cid:33)1−yτ

.

The mode θ can be eﬃciently computed via maximizing ft−1, which is log-concave. An approx-
imate posterior sample ˆθ is then drawn from a normal distribution with mean θ and covariance
matrix (−∇2 ln ft−1(θ))−1. To produce the computational results reported in Figure 8, we
applied Newton’s method with a backtracking line search to maximize ln ft−1.

Laplace approximations are well suited for Example 6 because the log-posterior density is
strictly concave and its gradient and Hessian can be computed eﬃciently. Indeed, more broadly,
Laplace approximations tend to be eﬀective for posterior distributions with smooth densities
that are sharply peaked around their mode. They tend to be computationally eﬃcient when
one can eﬃciently compute the posterior mode, and can eﬃciently form the Hessian of the
log-posterior density.

The behavior of the Laplace approximation is not invariant to a substitution of variables,
and it can sometimes be helpful to apply such a substitution. To illustrate this point, let us
revisit the online shortest path problem of Example 5. For this problem, posterior distributions
of θ are log-normal. However, these distributions are normal in φ, where φe = ln θe for each
edge e ∈ E. As such, if the Laplace approximation approach is applied to generate a sample ˆφ
from the posterior distribution of φ, the normal approximation is no longer an approximation,
and, letting ˆθe = exp( ˆφe) for each e ∈ E, we obtain a sample ˆθ exactly from the posterior
distribution of θ. In this case, through a variable substitution, we can sample in a manner that
makes the Laplace approximation exact. More broadly, for any given problem, it may be possible
to introduce variable substitutions that enhance the eﬃcacy of the Laplace approximation.

17

5.4 Bootstrapping

As a an alternative, we discuss an approach based on the statistical bootstrap, which accommo-
dates even very complex densities. There are many versions of the bootstrap approach that can
be used to approximately sample from a posterior distribution. For concreteness, we introduce
a speciﬁc one that is suitable for examples we cover in this paper.

Like the Laplace approximation approach, our bootstrap method assumes that θ is drawn
from a Euclidean space RK. Consider ﬁrst a standard bootstrap method for evaluating the
sampling distribution of the maximum likelihood estimate of θ. The method generates a hypo-
thetical history ˆHt−1 = ((ˆx1, ˆy1), . . . , (ˆxt−1, ˆyt−1)), which is made up of t − 1 action-observation
pairs, each sampled uniformly with replacement from Ht−1. We then maximize the likelihood of
θ under the hypothetical history, which for our shortest path recommendation problem is given
by

ˆLt−1(θ) =

(cid:32)

t−1
(cid:89)

τ =1

1 + exp (cid:0)(cid:80)

θe − M (cid:1)

1

e∈ˆxτ

(cid:33)ˆyτ (cid:32)

exp (cid:0)(cid:80)
1 + exp (cid:0)(cid:80)

e∈ˆxτ

θe − M (cid:1)

θe − M (cid:1)

e∈ˆxτ

(cid:33)1−ˆyτ

.

The randomness in the maximizer of ˆLt−1 reﬂects the randomness in the sampling distribution
of the maximum likelihood estimate. Unfortunately, this method does not take the agent’s prior
into account. A more severe issue is that it grossly underestimates the agent’s real uncertainty
in initial periods. The modiﬁcation described here is intended to overcome these shortcomings
in a simple way.

The method proceeds as follows. First, as before, we draw a hypothetical history ˆHt−1 =
((ˆx1, ˆy1), . . . , (ˆxt−1, ˆyt−1)), which is made up of t − 1 action-observation pairs, each sampled
uniformly with replacement from Ht−1. Next, we draw a sample θ0 from the prior distribution
f0. Let Σ denote the covariance matrix of the prior f0. Finally, we solve the maximization
problem

ˆθ = argmax

e−(θ−θ0)(cid:62)Σ(θ−θ0) ˆLt−1(θ)

θ∈Rk

and treat ˆθ as an approximate posterior sample. This can be viewed as maximizing a randomized
approximation ˆft−1 to the posterior density, where ˆft−1(θ) ∝ e−(θ−θ0)(cid:62)Σ(θ−θ0) ˆLt−1(θ) is what
the posterior density would be if the prior were normal with mean θ0 and covariance matrix
Σ, and the history of observations were ˆHt−1. When very little data has been gathered, the
randomness in the samples mostly stems from the randomness in the prior sample θ0. This
random prior sample encourages the agent to explore in early periods. When t is large, so
a lot of a data has been gathered, the likelihood typically overwhelms the prior sample and
randomness in the samples mostly stems from the random selection of the history ˆHt−1.

In the context of the shortest path recommendation problem, ˆft−1(θ) is log-concave and
can therefore be eﬃciently maximized. Again, to produce our computational results reported
in Figure 8, we applied Newton’s method with a backtracking line search to maximize ln ˆft−1.
Even when it is not possible to eﬃciently maximize ˆft−1, however, the bootstrap approach can
be applied with heuristic optimization methods that identify local or approximate maxima.

5.5 Sanity Checks

Figure 8 demonstrates that Laplace approximation and bootstrap approaches, when applied to
the path recommendation problem, learn from binary feedback to improve performance over
time. This may leave one wondering, however, whether exact Thompson sampling would oﬀer
substantially better performance. Since we do not have a tractable means of carrying out exact
Thompson sampling for this problem, in this section, we apply our approximation methods to
problems for which exact Thompson sampling is tractable. This enables comparisons between
performance of exact and approximate methods.

18

Recall the three-armed beta-Bernoulli bandit problem for which results from application of
greedy and Thompson sampling algorithms were reported in Figure 4(b). For this problem,
components of θ are independent under posterior distributions, and as such, Gibbs sampling
yields exact posterior samples. Hence, the performance of an approximate version that uses
Gibbs sampling would be identical to that of exact Thompson sampling. Figure 9a plots re-
sults from applying Laplace approximation and bootstrap approaches. For this problem, both
approximation methods oﬀer performance that is qualitatively similar to exact Thompson sam-
pling. We do see that Laplace sampling performs marginally worse than Bootstrapping in this
setting.

Next, consider the online shortest path problem with correlated edge delays. Regret ex-
perienced by Thompson sampling applied to such a problem were reported in Figure 7a. As
discussed in Section 5.3, applying the Laplace approximation approach with an appropriate
variable substitution leads to the same results as exact Thompson sampling. Figure 9b com-
pares those results to what is generated by Gibbs sampling and bootstrap approaches. Again,
the approximation methods yield competitive results, although bootstrapping is marginally less
eﬀective than Gibbs sampling.

(a) Bernoulli bandit

(b) online shortest path

Figure 9: Regret of approximation methods versus exact Thompson sampling.

5.6 Incremental Implementation

For each of the three approximation methods we have discussed, the compute time required
per time period grows as time progresses. This is because each past observation must be
accessed to generate the next action. This diﬀers from exact Thompson sampling algorithms we
discussed earlier, which maintain parameters that encode a posterior distribution, and update
these parameters over each time period based only on the most recent observation.

In order to keep the computational burden manageable, it can be important to consider
incremental variants of our approximation methods. We refer to an algorithm as incremental if
it operates with ﬁxed rather than growing per-period compute time. There are many ways to
design incremental variants of approximate posterior sampling algorithms we have presented. As
concrete examples, we consider here particular incremental versions of Laplace approximation
and bootstrap approaches.

For each time t, let (cid:96)t(θ) denote the likelihood of yt conditioned on xt and θ. Hence,

conditioned on Ht−1, the posterior density satisﬁes

ft−1(θ) ∝ f0(θ)

(cid:96)τ (θ).

t−1
(cid:89)

τ =1

19

Let g0(θ) = ln f0(θ) and gt(θ) = ln (cid:96)t(θ) for t > 0. To identify the mode of ft−1, it suﬃces to
maximize (cid:80)t−1

τ =0 gτ (θ).

Consider an incremental version of the Laplace approximation. The algorithm maintains
statistics Ht, and θt, initialized with θ0 = argmaxθ g0(θ), and H0 = ∇2g0(θ0), and updating
according to

Ht = Ht−1 + ∇2gt(θt−1),
θt = θt−1 − H −1

t ∇gt(θt−1).

This algorithm is a type of online newton method for computing the posterior mode θt−1 that
maximizes (cid:80)t−1
τ =0 gτ (θ). Note that if each function gt−1 is strictly concave and quadratic, as
would be the case if the prior is normal and observations are linear in θ and perturbed only
by normal noise, each pair θt−1 and H −1
t−1 represents the mean and covariance matrix of ft−1.
More broadly, these iterates can be viewed as the mean and covariance matrix of a Gaus-
sian approximation to the posterior, and used to generate an approximate posterior sample
ˆθ ∼ N (θt−1, H −1
It worth noting that for linear and generalized linear models, the ma-
trix ∇2gt(θt−1) has rank one, and therefore H −1
t = (Ht−1 + ∇2gt(θt−1))−1 can be updated
incrementally using the Sherman-Woodbury-Morrison formula. This incremental version of the
Laplace approximation is closely related to the notion of an extended Kalman ﬁlter, which has
been explored in greater depth by G´omez-Uribe [26] as a means for incremental approximate
Thompson sampling with exponential families of distributions.

t−1).

Another approach involves incrementally updating each of an ensemble of models to behave
like a sample from the posterior distribution. The posterior can be interpreted as a distribution
of “statistically plausible” models, by which we mean models that are suﬃciently consistent
with prior beliefs and the history of observations. With this interpretation in mind, Thompson
sampling can be thought of as randomly drawing from the range of statistically plausible mod-
els. Ensemble sampling aims to maintain, incrementally update, and sample from a ﬁnite set
of such models. In the spirit of particle ﬁltering, this set of models approximates the posterior
distribution. The workings of ensemble sampling are in some ways more intricate than conven-
tional uses of particle ﬁltering, however, because interactions between the ensemble of models
and selected actions can skew the distribution. Ensemble sampling is presented in more depth
in [27], which draws inspiration from work on exploration in deep reinforcement learning [15].

There are multiple ways of generating suitable model ensembles. One builds on the afore-
mentioned bootstrap method and involves ﬁtting each model to a diﬀerent bootstrap sample.
: n = 1, . . . , N ). Each
To elaborate, consider maintaining N models with parameters (θ
set is initialized with θ

0 = 0, and updated according to

n
0 ∼ g0, H n

0 = ∇g0(θ

n
0 ), dn

n
t , H n
t

H n

t = H n

t−1 + zn

t ∇2gt(θ

n
t−1),

n
t = θ

n
t−1 − zn

θ

t (H n

t )−1∇gt(θ

n
t−1),

n
0 and the random weight zn

t placed on each observation. The variable, zn

is an independent Poisson-distributed sample with mean one. Each θ

n
where each zn
t can be
t
viewed as a random statistically plausible model, with randomness stemming from the initializa-
τ can loosely be
tion of θ
interpreted as a number of replicas of the data sample (xτ , yτ ) placed in a hypothetical history
ˆHn
t . Indeed, in a data set of size t, the number of replicas of a particular bootstrap data sample
follows a Binomial(t, 1/t) distribution, which is approximately Poisson(1) when t is large. With
t , distinguished by the random num-
this view, each θ
ber of replicas assigned to each data sample. To generate an action xt, n is sampled uniformly
n
from {1, . . . , N }, and the action is chosen to maximize E[rt|θ = θ
t−1 serves as the
approximate posterior sample. Note that the per-period compute time grows with N , which is
an algorithm tuning parameter.

t is eﬀectively ﬁt to a diﬀerent data set ˆHn

n
t−1]. Here, θ

n

20

This bootstrap approach oﬀers one mechanism for incrementally updating an ensemble of
models. In Section 7.3, we will discuss another along with its application to active learning in
neural networks.

6 Practical Modeling Considerations

Our narrative over previous sections has centered around a somewhat idealized view of Thomp-
son sampling, which ignored the process of prior speciﬁcation and assumed a simple model in
which the system and set of feasible actions is constant over time and there is no side informa-
tion on decision context. In this section, we provide greater perspective on the process of prior
speciﬁcation and on extensions of Thompson sampling that serve practical needs arising in some
applications.

6.1 Prior Distribution Speciﬁcation

The algorithms we have presented require as input a prior distribution over model parameters.
The choice of prior can be important, so let us now discuss its role and how it might be selected.
In designing an algorithm for an online decision problem, unless the value of θ were known with
certainty, it would not make sense to optimize performance for a single value, because that could
lead to poor performance for other plausible values. Instead, one might design the algorithm
to perform well on average across a collection of possibilities. The prior can be thought of as a
distribution over plausible values, and its choice directs the algorithm to perform well on average
over random samples from the prior.

For a practical example of prior selection, let us revisit the banner ad placement problem
introduced in Example 1. There are K banner ads for a single product, with unknown click-
through probabilities (θ1, . . . , θK). Given a prior, Thompson sampling can learn to select the
most successful ad. We could use a uniform or, equivalently, a beta(1, 1) distribution over each
θk. However, if some values of θk are more likely than others, using a uniform prior sacriﬁces
performance. In particular, this prior represents no understanding of the context, ignoring any
useful knowledge from past experience. Taking knowledge into account reduces what must be
learned and therefore reduces the time it takes for Thompson sampling to identify the most
eﬀective ads.

Suppose we have a data set collected from experience with previous products and their ads,
each distinguished by stylistic features such as language, font, and background, together with
accurate estimates of click-through probabilities. Let us consider an empirical approach to prior
selection that leverages this data. First, partition past ads into K sets, with each kth partition
consisting of those with stylistic features most similar to the kth ad under current consideration.
Figure 10 plots a hypothetical empirical cumulative distribution of click-through probabilities
for ads in the kth set. It is then natural to consider as a prior a smoothed approximation of
this distribution, such as the beta(1, 100) distribution also plotted in Figure 10. Intuitively, this
process assumes that click-through probabilities of past ads in set k represent plausible values
of θk. The resulting prior is informative; among other things, it virtually rules out click-through
probabilities greater than 0.05.

A careful choice of prior can improve learning performance. Figure 11 presents results from
simulations of a three-armed Bernoulli bandit. Mean rewards of the three actions are sampled
from beta(1, 50), beta(1, 100), and beta(1, 200) distributions, respectively. Thompson sampling
is applied with these as prior distributions and with a uniform prior distribution. We refer to the
latter as a misspeciﬁed prior because it is not consistent with our understanding of the problem.
A prior that is consistent in this sense is termed coherent. Each plot represents an average
over ten thousand independent simulations, each with independently sampled mean rewards.

21

Figure 10: An empirical cumulative distribution and an approximating beta distribution.

Figure 11a plots expected regret, demonstrating that the misspeciﬁed prior increases regret.
Figure 11a plots the evolution of the agent’s mean reward conditional expectations. For each
algorithm, there are three curves corresponding to the best, second-best, and worst actions, and
they illustrate how starting with a misspeciﬁed prior delays learning.

(a) regret

(b) expected mean rewards

Figure 11: Comparison of Thompson sampling for the Bernoulli bandit problem with coherent
versus misspeciﬁed priors.

6.2 Constraints, Context, and Caution

Though Algorithm 4, as we have presented it, treats a very general model, straightforward ex-
tensions accommodate even broader scope. One involves imposing time-varying constraints
on the actions. In particular, there could be a sequence of admissible action sets Xt that con-
strain actions xt. To motivate such an extension, consider our shortest path example. Here, on
any given day, the drive to work may be constrained by announced road closures. If Xt does

22

not depend on θ except through possible dependence on the history of observations, Thompson
sampling (Algorithm 4) remains an eﬀective approach, with the only required modiﬁcation being
to constrain the maximization problem in Line 6.

Another extension of practical import addresses contextual online decision problems.
In such problems, the response yt to action xt also depends on an independent random variable
zt that the agent observes prior to making her decision.
In such a setting, the conditional
distribution of yt takes the form pθ(·|xt, zt). To motivate this, consider again the shortest path
example, but with the agent observing a weather report zt from a news channel before selecting
a path xt. Weather may aﬀect delays along diﬀerent edges diﬀerently, and the agent can take
this into account before initiating her trip. Contextual problems of this ﬂavor can be addressed
through augmenting the action space and introducing time-varying constraint sets. In particular,
if we view ˜xt = (xt, zt) as the action and constrain its choice to Xt = {(x, zt) : x ∈ X }, where X
is the set from which xt must be chosen, then it is straightforward to apply Thompson sampling
to select actions ˜x1, ˜x2, . . .. For the shortest path problem, this can be interpreted as allowing
the agent to dictate both the weather report and the path to traverse, but constraining the
agent to provide a weather report identical to the one observed through the news channel.

In some applications, it may be important to ensure that expected performance exceeds some
prescribed baseline. This can be viewed as a level of caution against poor performance. For
example, we might want each action applied to oﬀer expected reward of at least some level r.
This can again be accomplished through constraining actions: in each tth time period, let the
action set be Xt = {x ∈ X : E[rt|xt = x] ≥ r}. Using such an action set ensures that expected
average reward exceeds r. When actions are related, an actions that is initially omitted from the
set can later be included if what is learned through experiments with similar actions increases
the agent’s expectation of reward from the initially omitted action.

6.3 Nonstationary Systems

Problems we have considered involve model parameters θ that are constant over time. As
Thompson sampling hones in on an optimal action, the frequency of exploratory actions con-
verges to zero. In many practical applications, the agent faces a nonstationary system, which
is more appropriately modeled by time-varying parameters θ1, θ2, . . ., such that the response
yt to action xt is generated according to pθt(·|xt).
In such contexts, the agent should never
stop exploring, since it needs to track changes as the system drifts. With minor modiﬁcation,
Thompson sampling remains an eﬀective approach so long as model parameters change little
over durations that are suﬃcient to identify eﬀective actions.

In principle, Thompson sampling could be applied to a broad range of problems where
the parameters θ1, θ2, ..., evolve according to some stochastic process p(θt|θ1, ..., θt−1) by using
techniques from ﬁltering and sequential Monte Carlo to generate posterior samples. Instead we
describe below some much simpler approaches to such problems.

One simple approach to addressing nonstationarity involves ignoring historical observations
made beyond some number τ of time periods in the past. With such an approach, at each time t,
the agent would produce a posterior distribution based on the prior and conditioned only on the
most recent τ actions and observations. Model parameters are sampled from this distribution,
and an action is selected to optimize the associated model. The agent never ceases to explore,
since the degree to which the posterior distribution can concentrate is limited by the number of
observations taken into account.

An alternative approach involves modeling evolution of a belief distribution in a manner that
discounts the relevance of past observations and tracks a time-varying parameters θt. We now
consider such a model and a suitable modiﬁcation of Thompson sampling. Let us start with the
simple context of a Bernoulli bandit. Take the prior for each kth mean reward to be beta(α, β).
Let the algorithm update parameters to identify the belief distribution of θt conditioned on the

23

history Ht−1 = ((x1, y1), . . . , (xt−1, yt−1)) according to

(6.1)

(αk, βk) ←

(cid:26) (cid:0)(1 − γ)αk + γα, (1 − γ)βk + γβ(cid:1)

(cid:0)(1 − γ)αk + γα + rt, (1 − γ)βk + γβ + 1 − rt

(cid:1)

xt (cid:54)= k
xt = k,

where γ ∈ [0, 1] and αk, βk > 0. This models a process for which the belief distribution converges
to beta(αk, βk) in the absence of observations. Note that, in the absence of observations, if γ > 0
then (αk, βk) converges to (αk, βk).
Intuitively, the process can be thought of as randomly
perturbing model parameters in each time period, injecting uncertainty. The parameter γ
controls how quickly uncertainty is injected. At one extreme, when γ = 0, no uncertainty is
injected. At the other extreme, γ = 1 and each θt,k is an independent beta(αk, βk)-distributed
process. A modiﬁed version of Algorithm 2 can be applied to this nonstationary Bernoulli bandit
problem, the diﬀerences being in the additional arguments γ, α, and β , and the formula used
to update distribution parameters.

The more general form of Thompson sampling presented in Algorithm 4 can be modiﬁed in
an analogous manner. For concreteness, let us focus on the case where θ is restricted to a ﬁnite
set; it is straightforward to extend things to inﬁnite sets. The conditional distribution update
in Algorithm 4 can be written as

To model nonstationary model parameters, we can use the following alternative:

p(u) ←

p(u)qu(yt|xt)
v p(v)qv(yt|xt)

.

(cid:80)

p(u) ←

pγ(u)p1−γ(u)qu(yt|xt)
v pγ(v)p1−γ(v)qv(yt|xt)

.

(cid:80)

This generalizes the formula provided earlier for the Bernoulli bandit case. Again, γ controls
the rate at which uncertainty is injected. The modiﬁed version of Algorithm 2, which we refer
to as nonstationary Thompson sampling, takes γ and p as additional arguments and replaces
the distribution update formula.

Figure 12 illustrates potential beneﬁts of nonstationary Thompson sampling when dealing
with a nonstationairy Bernoulli bandit problem. In these simulations, belief distributions evolve
according to Equation (6.1). The prior and stationary distributions are speciﬁed by α = α =
β = β = 1. The decay rate is γ = 0.01. Each plotted point represents an average over
10,000 independent simulations. Regret here is deﬁned by regrett(θt) = maxk θt,k − θt,xt. While
nonstationary Thompson sampling updates its belief distribution in a manner consistent with
the underlying system, Thompson sampling pretends that the success probabilities are constant
over time and updates its beliefs accordingly. As the system drifts over time, Thompson sampling
becomes less eﬀective, while nonstationary Thompson sampling retains reasonable performance.
Note, however, that due to nonstationarity, no algorithm can promise regret that vanishes with
time.

7 Further Examples

As contexts for illustrating the workings of Thompson sampling, we have presented the Bernoulli
bandit and variations of the online shortest path problem. To more broadly illustrate the scope
of Thompson sampling and issues that arise in various kinds of applications, we present several
additional examples in this section.

24

Figure 12: Comparison of Thompson sampling versus nonstationary Thompson sampling with a
nonstationary Bernoulli bandit problem.

7.1 Product Assortment

Let us start with an assortment planning problem. Consider an agent who has an ample supply
of each of n diﬀerent products, indexed by i = 1, 2, · · · , n. The seller collects a proﬁt of pi per
unit sold of product type i. In each period, the agent has the option of oﬀering a subset of
the products for sale. Products may be substitutes or complements, and therefore the demand
for a product may be inﬂuenced by the other products oﬀered for sale in the same period. In
order to maximize her proﬁt, the agent needs to carefully select the optimal set of products to
oﬀer in each period. We can represent the agent’s decision variable in each period as a vector
x ∈ {0, 1}n where xi = 1 indicates that product i is oﬀered and xi = 0 indicates that it is not.
Upon oﬀering an assortment containing product i in some period, the agent observes a random
lognormally distributed demand di. The mean of this lognormal distribution depends on the
entire assortment x and an uncertain matrix θ ∈ Rk×k. In particular

where σ2 is a known parameter that governs the level of idiosyncratic randomness in realized
demand across periods. For any product i contained in the assortment x,

log(di) | θ, x ∼ N (cid:0)(θx)i, σ2(cid:1)

(θx)i = θii +

xjθij,

(cid:88)

j(cid:54)=i

where θii captures the demand rate for item i if it were the sole product oﬀered and each θij
captures the eﬀect availability of product j has on demand for product i. When an assortment
x is oﬀered, the agent earns expected proﬁt

(7.1)

(cid:34) n
(cid:88)

E

i=1

pixidi | θ, x

=

pixie(θx)i+ σ2
2 .

(cid:35)

n
(cid:88)

i=1

If θ were known, the agent would always select the assortment x that maximizes her expected
proﬁt in (7.1). However, when θ is unknown, the agent needs to learn to maximize proﬁt by
exploring diﬀerent assortments and observing the realized demands.

25

Thompson sampling can be adopted as a computationally eﬃcient solution to this problem.
We assume the agent begins with a multivariate Gaussian prior over θ. Due to conjugacy
properties of normal and lognormal distributions, the posterior distribution of θ remains normal
after any number of periods. At the beginning of each t’th period, the Thompson sampling
algorithm draws a sample ˆθt from this normal posterior distribution. Then, the agent selects an
assortment that would maximize her expected proﬁt in period t if the sampled ˆθt were indeed
the true parameter.

As in Examples 4 and 5, the mean and covariance matrix of the posterior distribution of θ
can be updated in closed form. However, because θ is a matrix rather than a vector, the explicit
form of the update is more complicated. To describe the update rule, we ﬁrst introduce ¯θ as the
vectorized version of θ which is generated by stacking the columns of θ on top of each other.
Let x be the assortment selected in a period, i1, i2, · · · , ik denote the the products included in
this assortment (i.e., supp(x) = {i1, i2, · · · , ik}) and z ∈ Rk be deﬁned element-wise as

zj = ln dij ,

j = 1, 2, · · · , k.

Let S be a k × n “selection matrix” where Sj,ij = 1 for j = 1, 2, · · · , k and all its other elements
are 0. Also, deﬁne

W = x(cid:62) ⊗ S,

where ⊗ denotes the Kronecker product of matrices. At the end of current period, the posterior
mean µ and covariance matrix Σ of ¯θ are updated according to the following rules:

(cid:18)

µ ←

Σ−1 +

1
σ2 W (cid:62)W

(cid:19)−1 (cid:18)

Σ−1µ +

1
σ2 W (cid:62)z

(cid:19)

,

(cid:18)

Σ ←

Σ−1 +

1
σ2 W (cid:62)W

(cid:19)−1

.

To investigate the performance of Thompson sampling in this problem, we simulated a
scenario with n = 6 and σ2 = 0.04. We take the proﬁt associated to each product i to be
pi = 1/6. As the prior distribution, we assumed that each element of θ is independently
normally distributed with mean 0, the diagonal elements have a variance of 1, and the oﬀ-
diagonal elements have a variance of 0.2. To understand this choice, recall the impact of diagonal
and oﬀ-diagonal elements of θ. The diagonal element θii controls the mean demand when only
product i is available, and reﬂects the inherent quality or popularity of that item. The oﬀ-
diagonal element θij captures the inﬂuence availability of product j has on mean demand for
product i. Our choice of prior covariance encodes that the dominant eﬀect on demand of
a product is likely its own characteristics, rather than its interaction with any single other
product. Figure 13 presents the performance of diﬀerent learning algorithms in this problem.
In addition to Thompson sampling, we have simulated the greedy and (cid:15)-greedy algorithms for
various values of (cid:15). We found that (cid:15) = 0.07 provides the best performance for (cid:15)-greedy in this
problem.

As illustrated by this ﬁgure, the greedy algorithm performs poorly in this problem while
(cid:15)-greedy presents a much better performance. We found that the performance of (cid:15)-greedy can
be improved by using an annealing (cid:15) of m
m+t at each period t. Our simulations suggest using
m = 9 for the best performance in this problem. Figure 13 shows that Thompson sampling
outperforms both variations of (cid:15)–greedy in this problem.

7.2 Cascading Recommendations

We consider an online recommendation problem in which an agent learns to recommend a
desirable list of items to a user. As a concrete example, the agent could be a search engine
and the items could be web pages. We consider formulating this problem as a cascading bandit,
in which user selections are governed by a cascade model, as is commonly used in the ﬁelds of
information retrieval and online advertising [28].

26

Figure 13: Regret experienced by diﬀerent learning algorithms applied to product assortment
problem.

A cascading bandit is represented by a tuple (E, J, θ), where E = {1, . . . , K} is the item
set, J ≤ K is the number of recommended items in each period, and the model parameters
θ ∈ [0, 1]K encode the item attraction probabilities. The agent knows the item set E and the
display size J, but must learn the user preferences encoded in θ through sequential interactions.
We study the agent’s performance over T interactions.

During each time t, the agent chooses an ordered list xt = (xt,1, . . . , xt,J ) of J distinct items
from E based on its prior information and past observations. The list is then presented to
the user, who either clicks on a single item from the list, or departs without clicking. The
user’s choices depend on the item attraction probabilities, their ordering in the vector xt, and
idiosyncratic randomness associated with the current time period. In particular, the user’s choice
process can be described as follows. Let wt ∈ {0, 1}K be a random binary vector generated by
independently sampling each kth element wt,k from the Bernoulli distribution Bern(θk), where
θk is the kth element of θ. Item k is attractive for the user at time t if and only if wt,k = 1.
Following the cascade model, the user views items in the recommended list xt individually in
order, stopping at and clicking the ﬁrst that is attractive. Speciﬁcally, the jth item in xt, is
viewed:

• If xt,j is attractive at time t, then the user clicks xt,j and leaves.
• If xt,j is not attractive at time t and j < J, then the user continues to examine xt,j+1.
• If xt,j is not attractive at time t and j = J, then the user leaves without a click.

Note that item xt,1 is always examined, and there is at most one click at time t. Let yt =
argmin{1 ≤ j ≤ J : wt,xt,j = 1} denote the user’s choice, with the the convention that
argmin ∅ = ∞. Thus, yt = ∞ when the user leaves without clicking.

The agent observes the user’s choice yt and associates this with a reward rt = r(yt) =
1{yt ≤ J} indicating whether the user clicked on some item. Note that based on yt, the agent
can update its estimates of attraction probabilities of all items a1
examined by
the user. In particular, upon seeing the click yt < ∞ the agent infers that item was attractive
and each item presented before yt was unattractive. When no click was observed, the agent
infers that every item in xt was unattractive to the user at time t.

t , . . . , amin{yt,J}

t

27

For each ordered lists x = (x1, . . . , xJ ) and θ(cid:48) ∈ [0, 1]K, let
(cid:105)

h(x, θ(cid:48)) = 1 − (cid:81)J

(cid:104)
1 − θ(cid:48)
xj

,

j=1

is the xj-th element of θ(cid:48). Note that rt = h(xt, wt), and the expected reward at time t
where θ(cid:48)
xj
is E [rt|xt, θ] = h(xt, θ). The optimal solution x∗ ∈ argmaxx: |x|=J h(x, θ) consists of the J items
with highest attraction probabilities. The per-period regret of the cascading bandit is deﬁned
as

regrett(θ) = h(x∗, θ) − h(xt, θ).

Algorithm 5 CascadeUCB
1: Initialize αk and βk ∀k ∈ E
2: for t = 1, 2, . . . do
3:

#compute itemwise UCBs:
for k = 1, . . . , K do

4:

Compute UCB Ut(k)

end for

Algorithm 6 CascadeTS
1: Initialize αk and βk ∀k ∈ E
2: for t = 1, 2, . . . do
3:

#sample model:
for k = 1, . . . , K do

4:

Sample ˆθk ∼ Beta(αk, βk)

end for

#select and apply action:
xt ← argmaxx:|x|=J h(x, Ut)
Apply xt and observe yt and rt

#select and apply action:
xt ← argmaxx:|x|=J h(x, ˆθ)
Apply xt and observe yt and rt

#update suﬃcient statistics:
for j = 1, . . . , min{yt, J} do
αxt,j ← αxt,j + 1(j = yt)
βxt,j ← βxt,j + 1(j < yt)

#update posterior:
for j = 1, . . . , min{yt, J} do
αxt,j ← αxt,j + 1(j = yt)
βxt,j ← βxt,j + 1(j < yt)

end for

16:
17: end for

end for

16:
17: end for

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

Kveton et al.

[29] proposed learning algorithms for cascading bandits based on itemwise
UCB estimates. CascadeUCB (Algorithm 5) is practical variant that allows for speciﬁcation
of prior parameters (α, β) that guide early behavior of the algorithm. CascadeUCB computes
a UCB Ut(k) for each item k ∈ E and then chooses the a list that maximizes h(·, Ut), which
represents an upper conﬁdence bound on the list attraction probability. The list xt can be
eﬃciently generated by choosing the J items with highest UCBs. Upon observing the user’s
response, the algorithm updates the suﬃcient statistics (α, β), which count clicks and views.

CascadeTS (Algorithm 6) is a Thompson sampling algorithm for cascading bandits. Cas-
cadeTS operates in a manner similar to CascadeUCB except that xt is computed based on the
sampled attraction probabilities ˆθ, rather than the itemwise UCBs Ut.

A standard form of UCB, known as UCB1, which is considered and analyzed in the context

of cascading bandits in [29], takes the form

Ut(k) =

αk
αk + βk

+

1.5 log(t)
αk + βk

,

∀k ∈ E.

Note that αk/(αk + βk) represents the expected value of the click probability θk, while the
second term represents an optimistic boost that encourages exploration. As the observations
accumulate, the denominator

αk + βk grows, reducing the degree of optimism.

√

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

(cid:115)

28

Figure 14 presents results from applying CascadeTS and CascadeUCB based on UCB1.
These results are generated by randomly sampling 1000 cascading bandit instances, K =
1000 and J = 100, in each case sampling each attraction probability θk independently from
Beta(1, 40). For each instance, CascadeUCB and CascadeTS are applied over 20000 time pe-
riods, initialized with (αk, βk) = (1, 40). The plots are of per-period regrets averaged over the
1000 simulations.

Figure 14: Comparison of CascadeTS and CascadeUCB with K = 1000 items and J = 100 recom-
mendations per period.

The results demonstrate that TS far outperforms this version of CascadeUCB. Why? An
obvious reason is that h(x, Ut) is far too optimistic.
In particular, h(x, Ut) represents the
probability of a click if every item in x simultaneously takes on the largest attraction probability
that is statistically plausible. However, the wt,k’s are statistically independent across items and
the agent is unlikely to have substantially under-estimated the attraction probability of every
item in x. As such, h(x, Ut) tends to be far too optimistic. CascadeTS, on the other hand,
samples ˆθk’s independently across items. While any sample ˆθk might deviate substantially from
its mean, it is unlikely that the sampled attraction probability of every item in x greatly exceeds
its mean. As such, the variability in h(x, ˆθ) provides a much more accurate reﬂection of the
magnitude of uncertainty.

One can address excess optimism by tuning the degree of optimism associated with UCB1.

In particular, consider a UCB of the form

Ut(k) =

αk
αk + βk

+ c

1.5 log(t)
αk + βk

,

∀k ∈ E,

(cid:115)

with the parameter c, which indicates the degree of optimism, selected through simulations to
optimize performance. The plot labeled “UCB-tuned” in Figure 14 illustrates performance with
c = 0.05, which approximately minimizes cumulative regret over 20,000 time periods. Table 1
provides the cumulative regret averages for each algorithm and range of degrees of optimism
c, along with one standard deviation conﬁdence interval boundaries. It is interesting that even
after being tuned to the speciﬁc problem and horizon, the performance of CascadeUCB falls
short of Cascade TS. A likely source of loss stems from the shape of conﬁdence sets used by
CascadeUCB. Note that the algorithm uses hyper-rectangular conﬁdence sets, since the set
of statistically plausible attraction probability vectors is characterized by a Cartesian prod-
uct item-level conﬁdence intervals. However, the Bayesian central limit theorem suggests that

29

“ellipsoidal” conﬁdence sets oﬀer a more suitable choice. Speciﬁcally, as data is gathered the
posterior distribution over θ can be well approximated by a multivariate Gaussian, for which
level sets are ellipsoidal. Losses due to the use of hyper-rectangular conﬁdence sets have been
studied through regret analysis in [30] and through simple analytic examples in [31].

Table 1: Comparison of CascadeTS and CascadeUCB with K = 1000 items and J = 100 recom-
mendations per period over a range of optimism parameters.

algorithm

TS

UCB

degree of
optimism
NA
0 (greedy)
0.0001
0.001
0.005
0.01
0.05
0.1
0.2
0.3
0.5
0.75
1 (UCB1)

cumulative
regret
71.548 ± 0.395
135.410 ± 0.820
134.768 ± 0.807
133.906 ± 0.803
131.737 ± 0.756
127.454 ± 0.726
115.345 ± 0.605
125.351 ± 0.636
200.221 ± 1.069
324.586 ± 1.644
615.070 ± 2.705
954.670 ± 3.647
1222.757 ± 4.032

It is worth noting that tuned versions of CascadeUCB do sometimes perform as well or better
than CascadeTS. Table 2 and Figure 15 illustrates an example of this. The setting is identical to
that used to generate the results of 14, except that K = 50 and J = 10, and cumulative regret is
approximately optimized with c = 0.1. CascadeUCB outperforms CascadeTS. This qualitative
diﬀerence from the case of K = 1000 and J = 100 is likely due to the fact that hyper-rectangular
sets oﬀer poorer approximations of ellipsoids as the dimension increases. This phenomenon and
its impact on regret aligns with theoretical results of [30]. That said, CascadeUCB is somewhat
advantaged in this comparison because it is tuned speciﬁcally for the setting and time horizon.
Results presented so far initialize the algorithms with coherent priors. To illustrate both
robustness and potentially complex behaviors of these algorithms, we present in Table 3 and
Figure 16 results generated when the algorithms are initialized with uniform priors (αk = βk =
1). Aside from these priors, the setting is identical to that used to generate Table 1 and Figure
14. CascadeTS far outperforms the nominal version of CascadeUCB and does not fare too
much worse than CascadeTS initialized with a coherent prior. A puzzling fact, however, is
that the tuned version of CascadeUCB is optimized by c = 0, which corresponds to the greedy
algorithm.
It could be that the greedy algorithm performs well here because the particular
choice of misspeciﬁed prior induces a suitable kind of optimistic behavior. More broadly, this
illustrates how ad hoc tuning can substantially improve the behavior of an algorithm in any
given simulated context. However, Thompson sampling appears to operate robustly across
a broad range of problems even when applied in a principled manner that does not require
context-speciﬁc tuning.

30

Table 2: Comparison of CascadeTS and CascadeUCB with K = 50 items and J = 10 recommen-
dations per period over a range of optimism parameters.

algorithm

TS

UCB

degree of
optimism
NA
0 (greedy)
0.0001
0.001
0.005
0.01
0.05
0.1
0.2
0.3
0.5
0.75
1 (UCB1)

cumulative
regret
183.391 ± 1.050
333.650 ± 4.714
325.448 ± 4.618
325.007 ± 4.648
309.303 ± 4.408
297.496 ± 4.257
205.882 ± 2.861
159.626 ± 1.695
190.457 ± 1.099
260.022 ± 1.184
406.080 ± 1.630
570.547 ± 2.189
711.117 ± 2.627

Figure 15: Comparison of CascadeTS and CascadeUCB with K = 50 items and J = 10 recommen-
dations per period.

31

Table 3: Comparison of CascadeTS and CascadeUCB, both initialized with uniform priors, with
K = 1000 items and J = 100 recommendations per period over a range of optimism parameters.

algorithm

TS

UCB

degree of
optimism
NA
0 (greedy)
0.0001
0.001
0.005
0.01
0.05
0.1
0.2
0.3
0.5
0.75
1 (UCB1)

cumulative
regret
82.458 ± 0.449
101.556 ± 0.477
104.910 ± 0.474
104.572 ± 0.459
105.528 ± 0.474
108.136 ± 0.483
131.654 ± 0.559
172.275 ± 0.709
283.607 ± 1.149
417.461 ± 1.629
702.015 ± 2.583
1024.434 ± 3.390
1277.005 ± 3.812

Figure 16: Comparison of CascadeTS and CascadeUCB, both initialized with uniform priors, with
K = 1000 items and J = 100 recommendations per period.

32

7.3 Active Learning in Neural Networks

Neural networks are widely used in supervised learning, where given an existing set of predictor-
response data pairs, the objective is to produce a model that generalizes to accurately predict
future responses conditioned on associated predictors. They are also increasingly being used to
guide actions ranging from recommendations to robotic maneuvers. Active learning is called
for to close the loop by generating actions that do not solely maximize immediate performance
but also probe the environment to generate data that accelerates learning. Thompson sampling
oﬀers a useful principle upon which such active learning algorithms can be developed.

With neural networks or other complex model classes, computing the posterior distribution
over models becomes intractable. Approximations are called for, and incremental updating is
essential because ﬁtting a neural network is a computationally intensive task in its own right. In
such contexts, ensemble sampling oﬀers a viable approach [27]. In Section 5.6, we introduced a
particular mechanism for ensemble sampling based on the bootstrap. In this section, we consider
an alternative version of ensemble sampling and present results from [27] that demonstrate its
application to active learning in neural networks.

To motivate our algorithm, let us begin by discussing how it can be applied to the linear

bandit problem.

Example 7

(Linear Bandit) Let θ be drawn from RM and distributed according
to a N (µ0, Σ0) prior. There is a set of K actions X ⊆ RM . At each time t = 1, . . . , T ,
an action xt ∈ X is selected, after which a reward rt = yt = θ(cid:62)xt + wt is observed,
where wt ∼ N (0, σ2

w).

In this context, ensemble sampling is unwarranted, since exact Bayesian inference can be carried
out eﬃciently via Kalman ﬁltering. Nevertheless, the linear bandit oﬀers a simple setting for
explaining the workings of an ensemble sampling algorithm.

Consider maintaining a covariance matrix updated according to

Σt+1 = (cid:0)Σ−1

t + xtx(cid:62)

t /σ2
w

(cid:1)−1

,

1
t , . . . , θ
and N models θ
and updated incrementally according to

N
t , initialized with θ

1
1, . . . , θ

N
1 each drawn independently from N (µ0, Σ0)

n
t+1 = Σt+1

θ

Σ−1
t θ

n
t + xt(yt + ˜wn

t )/σ2
w

(cid:16)

(cid:17)

,

for n = 1, . . . , N , where ( ˜wn
t
samples drawn by the updating algorithm.
vectors satisfy

: t = 1, . . . , T, n = 1, . . . , N ) are independent N (0, σ2

w) random
It is easy to show that the resulting parameter

n
t = arg min

θ

ν

(cid:32)

1
σ2
w

t−1
(cid:88)

τ =1

(yτ + ˜wn

τ − x(cid:62)

τ ν)2 + (ν − θ

n
1 )(cid:62)Σ−1

0 (ν − θ

n
1 )

.

(cid:33)

n
t is a model ﬁt to a randomly perturbed prior
Thich admits an intuitive interpretation: each θ
and randomly perturbed observations. As established in [27], for any deterministic sequence
N
x1, . . . , xt−1, conditioned on the history, the models θ
t are independent and identically
distributed according to the posterior distribution of θ. In this sense, the ensemble approximates
the posterior.

1
t , . . . , θ

The ensemble sampling algorithm we have described for the linear bandit problem motivates

an analogous approach for the following neural network model.

Example 8

(Neural Network) Let gθ : RM (cid:55)→ RK denote a mapping induced
by a neural network with weights θ. Suppose there are K actions X ⊆ RM , which

33

serve as inputs to the neural network, and the goal is to select inputs that yield
desirable outputs. At each time t = 1, . . . , T , an action xt ∈ X is selected, after
which yt = gθ(xt) + wt is observed, where wt ∼ N (0, σ2
wI). A reward rt = r(yt)
is associated with each observation. Let θ be distributed according to a N (µ0, Σ0)
prior. The idea here is that data pairs (xt, yt) can be used to ﬁt a neural network
model, while actions are selected to trade oﬀ between generating data pairs that
reduce uncertainty in neural network weights and those that oﬀer desirable immediate
outcomes.

Consider an ensemble sampling algorithm that once again begins with N independent models
N
1 sampled from a N (µ0, Σ0) prior. It could be natural here
with connection weights θ
to let µ0 = 0 and Σ0 = σ2
0 chosen so that the range of probable models
spans plausible outcomes. To incrementally update parameters, at each time t, each nth model
applies some number of stochastic gradient descent iterations to reduce a loss function of the
form

1
1, . . . , θ
0I for some variance σ2

Lt(ν) =

(yτ + ˜wn

τ − gν(xτ ))2 + (ν − θ

n
1 )(cid:62)Σ−1

0 (ν − θ

n
1 ).

1
σ2
w

t−1
(cid:88)

τ =1

Figure 17 present results from simulations involving a two-layer neural network, with a set
of K actions, X ⊆ RM . The weights of the neural network, which we denote by w1 ∈ RD×N and
w2 ∈ RD, are each drawn from N (0, λ). Let θ ≡ (w1, w2). The mean reward of an action x ∈ X
is given by gθ(x) = w(cid:62)
2 max(0, w1a). At each time step, we select an action xt ∈ X and observe
reward yt = gθ(xt) + zt, where zt ∼ N (0, σ2
z ). We used M = 100 for the input dimension,
D = 50 for the dimension of the hidden layer, number of actions K = 100, prior variance λ = 1,
and noise variance σ2
z = 100. Each component of each action vector is sampled uniformly from
[−1, 1], except for the last component, which is set to 1 to model a constant oﬀset. All results
are averaged over 100 realizations.

In our application of the ensemble sampling algorithm we have described, to facilitate gra-
dient ﬂow, we use leaky rectiﬁed linear units of the form max(0.01x, x) during training, though
the target neural network is made up of regular rectiﬁed linear units as indicated above. In our
simulations, each update was carried out with 5 stochastic gradient steps, with a learning rate
of 10−3 and a minibatch size of 64.

Figure 17 illustrates the performance of several learning algorithms with an underlying neural
network. Figure 17a demonstrates the performance of an (cid:15)-greedy strategy across various levels
of (cid:15). We ﬁnd that we are able to improve performance with an annealing schedule (cid:15) = k
k+t
(Figure 17b). However, we ﬁnd that an ensemble sampling strategy outperforms even the best
tuned (cid:15)-schedules (Figure 17c). Further, we see that ensemble sampling strategy can perform
well with remarkably few members of this ensemble. Ensemble sampling with fewer members
leads to a greedier strategy, which can perform better for shorter horizons, but is prone to
premature and suboptimal convergence compared to true Thompson sampling [27].
In this
problem, using an ensemble of as few as 30 members provides very good performance.

7.4 Reinforcement Learning in Markov Decision Processes

Reinforcement learning (RL) extends upon the contextual online decision problems covered in
Section 6.2 to allow for delayed feedback and long term consequences [32, 33]. Concretely (using
the notation of Section 6.2) the response yt to the action xt depends on a context zt; but we no
longer assume that the evolution of the context zt+1 is independent of the action xt. As such,
the action xt may aﬀect not only the reward r(yt) but also, through the eﬀect upon the context
zt+1, the rewards earned in future timesteps {r(yt(cid:48))}t(cid:48)>t. As a motivating example, consider a
problem of sequentially recommending products xt where the customer response yt is informed
not only by the quality of the product, but also the history of past recommendations. The

34

(a) Fixed (cid:15)-greedy.

(b) Annealing (cid:15)-greedy.

(c) Ensemble TS.

Figure 17: Bandit learning with an underlying neural network.

evolution of the context zt+1–which captures relevant information about the current customer
available at time t + 1–is then directly aﬀected by the customer response yt; if a customer
watched ‘The Godfather’ and loved it, then chances are probably higher they may enjoy ‘The
Godfather 2’.

Maximizing cumulative rewards in a problem where decision have long term consequences
can require planning with regards to future rewards, rather than optimizing each timestep
myopically. Similarly, eﬃcient exploration in these domains can require planning with regards to
the potential for informative observations in future timesteps, rather than myopically considering
the information gained over a single timestep. This sophisticated form of temporally-extended
exploration, which can be absolutely critical for eﬀective performance, is sometimes called deep
exploration [34].

The Thompson sampling principle can also be applied successfully to reinforcement learning
in ﬁnite horizon Markov decision processes (MDPs) [35]. However, we also highlight that special
care must be taken with respect to the notion of ‘timestep or ‘period within Thompson sampling
to preserve deep exploration.

An episodic ﬁnite horizon MDP M = (S, A, RM, P M, H, ρ) is a type of online decision problem
that proceeds in distinct episodes, each with H timesteps within them. S is the state space, A
is the action space and H is the length of the episode. At the start of each episode the initial
state s0 is drawn from the distribution ρ. At each timestep h = 0, .., H − 1 within an episode
the agent observes state sh ∈ S, selects action ah ∈ A, receives a reward rh ∼ RM (sh, ah)
and transitions to a new state sh+1 ∼ P M (sh, ah). A policy µ is a function mapping each
state s ∈ S and timestep h = 0, .., H − 1 to an action a ∈ A. The value function V M
µ,h(s) =
E[(cid:80)H−1
j=h rj(sj, µ(sj, j)) | sh = s] encodes the expected reward accumulated under µ in the
remainder of the episode when starting from state s and timestep h. Finite horizon MDPs allow
for long term consequences through the evolution of the state, but the scope of this inﬂuence is
limited to within an individual episode.

Immediately we should note that we have already studied a ﬁnite horizon MDP under diﬀer-
ent terminology in Example 2: the online shortest path problem. To see the connection simply
view each choice of edge as sequential timesteps within the period (or episode); the state is the
current vertex, the action is the next choice of edge and the horizon is the maximal number
of decision stages. With this connection in mind we can express the problem of maximizing
the cumulative rewards in a ﬁnite horizon MDP (cid:80)K
h=0 r(skh, akh) equivalently as a ban-
dit problem over periods t = 1, 2, .., K where each bandit period is an entire episode of MDP

(cid:80)H−1

k=1

35

interaction and each bandit action is a policy µt for use within that episode. By contrast, a
naive application of Thompson sampling to reinforcement learning that resamples policies every
timestep within an episode could be extremely ineﬃcient as it does not perform deep exploration.

Figure 18: MDPs where Thompson sampling every timestep leads to ineﬃcient exploration.

Consider the example in Figure 18 where the underlying MDP is characterized by a long
chain of states {s−N , .., sN } and only the one of the far left or far right positions are rewarding
with equal probability; all other states produce zero reward and with known dynamics. Learning
about the true dynamics of the MDP requires a consistent policy over N steps right or N steps
left; a variant of Thompson sampling that resampled every timestep would be exponentially
unlikely to make it to either end within N steps [34]. By contrast, sampling at an episode level
and holding that policy ﬁxed for the duration of the episode would demonstrate deep exploration
and so be able to learn the optimal policy within a single episode.

In order to apply Thompson sampling to policy selection we need a way of sampling from
the posterior distribution for the optimal policy. One eﬃcient way to do this, at least for ﬁ-
nite |S|, |A| is to maintain a posterior distribution over the reward distribution RM and the
transition dynamics P M at each state and action (s, a). In order to generate a sample for the
optimal policy, simply take a single posterior sample for the reward distribution and transition
probabilities and then solve for the optimal policy for this sample. This process is equivalent
to maintaining a posterior distribution over the optimal policy, but may be more tractable de-
pending on the problem setting. Estimating a posterior distribution over rewards is no diﬀerent
from the setting of bandit learning that we have already discussed at length within this paper.
The transition function looks a little diﬀerent, but for transitions over a ﬁnite state space the
Dirichlet distribution is a useful conjugate prior.
It is a multi-dimensional generalization of
the Beta distribution from Example 3. The Dirichlet prior over outcomes in S = {1, .., S} is
speciﬁed by a positive vector of pseudo-observations α ∈ RS
+; updates to the Dirichlet posterior
can be performed analytically simply by incrementing the appropriate column of α [4].

In Figure 19 we present a computational comparison of Thompson sampling where a new
policy is drawn every ‘timestep’ and Thompson sampling where a new policy is drawn every
‘episode’. This ﬁgure compares performance in the example shown in Figure 18. Figure 19a
compares the performance of sampling schemes where the agent has an informative prior that
matches the true underlying system. As explained above, sampling once per episode Thompson
sampling is guaranteed to learn the true MDP structure in a single episode. By contrast,
resampling every timestep leads to uniformly random actions until either s−N or sN is visited.
Therefore, it takes a minimum of 2N epsiodes for the ﬁrst expected reward.

The diﬀerence in performance demonstrated by Figure 19a is particularly extreme because
the prior structure means that there is only value to deep exploration, and none to ‘shallow’
exploration per timestep [34]. In Figure 19b we present results for Thompson sampling variant
on the same environment but with uniform Dirichlet prior over transitions and N (0, 1) prior over
rewards in each state and action. With this prior structure Thompson sampling per timestep is
not as hopeless, but still performs worse than Thompson sampling per episode. Once again, this
diﬀerence increases with MDP problem size. Overall, Figure 19 demonstrates that the beneﬁt of
sampling per episode, rather than per timestep, can become arbitrarily large. As an additional
beneﬁt this approach is also more computationally eﬃcient, since we only need to solve for the

36

(a) Using informed prior.

(b) Using uninformed prior.

Figure 19: Comparing Thompson sampling by episode with Thompson sampling by episode.

optimal policy once every episode rather than at each timestep.

8 Why it Works, When it Fails, and Alternative Approaches

Earlier sections demonstrate that Thompson sampling approaches can be adapted to address
a number of problem classes of practical import. In this section, we provide intuition for why
Thompson sampling explores eﬃciently, and brieﬂy review theoretical work that formalizes this
intuition. We will then highlight problem classes for which Thompson sampling is poorly suited,
and refer to some alternative algorithms.

8.1 Why Thompson Sampling Works

To understand whether Thompson sampling is well suited to a particular application, it is useful
to develop a high level understanding of why it works. As information is gathered, beliefs about
arms’ payouts are carefully tracked. By sampling actions according to the posterior probability
they are optimal, the algorithm continues to sample all arms that could plausibly be optimal,
while shifting sampling away from those that are extremely unlikely to be optimal. Roughly
speaking, the algorithm tries all promising actions while gradually discarding those that are
believed to under-perform.

This intuition is formalized in recent theoretical analyses of Thompson sampling. [5] observed
empirically that for the simple Bernoulli bandit problem described in Example 1, the regret of
Thompson sampling scales in an asymptotically optimal manner in the sense deﬁned by [36]. A
series of papers provided proofs conﬁrming this ﬁnding [37, 38, 39]. Later papers have studied
Thompson sampling as a general tool for exploration in more complicated online optimization
problems. The ﬁrst regret bounds for such problems were established by [40] for linear contextual
bandits and by [41, 42] for an extremely general class of bandit models. Subsequent papers have
further developed this theory [43, 44, 45] and have studied extensions of Thompson sampling to
reinforcement-learning [35, 46, 47, 48].

37

8.2 Limitations of Thompson Sampling

Thompson sampling is a simple and eﬀective method for exploration in broad classes of problems,
but no heuristic works well for all problems. For example, Thompson sampling is certainly a poor
ﬁt for sequential learning problems that do not require much active exploration; in such cases by
greedier algorithms that don’t invest in costly exploration usually provide better performance.
We now highlight two more subtle problem features that pose challenges when applying standard
Thompson sampling.

8.2.1 Time Preference

Thompson sampling is eﬀective at minimizing the exploration costs required to converge on an
optimal action. It may perform poorly, however, in time-sensitive learning problems where it is
better to exploit a high performing suboptimal action than to invest resources exploring arms
that might oﬀer slightly improved performance.

To understand this issue, let us revisit the motivating story given at the beginning of the
paper. Suppose that while waiting for his friends to ﬁnish a game in another part of the casino,
a gambler sits down at a slot machine with k arms yielding uncertain payouts. For concreteness,
assume as in Example 3 that the machine’s payouts are binary with Beta distributed priors.
But now, suppose the number of plays τ the gambler will complete before his friends arrive is
uncertain, with τ ∼ Geometric(1 − δ) and E[τ ] = 1/(1 − δ). Is Thompson sampling still an
eﬀective strategy for maximizing the cumulative reward earned? This depends on the values of
δ and k. When δ → 1 so E[τ ] (cid:29) k, it is generally worth exploring to identify the optimal arm
so it can be played in subsequent periods. However, if E[τ ] < k, there is not time to play every
arm, and the gambler would be better oﬀ exploiting the best arm among those he tries initially
than continuing to explore alternatives.

Related issues also arise in the nonstationary learning problems described in Section 6.3.
When a nonstarionary system evolves rapidly, information gathered quickly becomes irrelevant
to optimizing future performance. In such cases, it may be impossible to converge on the current
optimal action before the system changes substantially, and the algorithms presented in Section
6.3 might perform better if they are modiﬁed to explore less aggressively.

This issue is discussed further in [49]. That paper proposes and analyzes satisﬁcing Thomp-
son sampling, a variant of Thompson sampling that is designed to minimize the exploration
costs required to identify an action that is suﬃciently close to optimal.

8.2.2 Problems Requiring Careful Assessment of Information Gain

Thompson sampling is well suited to problems where the best way to learn which action is
optimal is to test the most promising actions. However, there are natural problems where such
a strategy is far from optimal, and eﬃcient learning requires a more careful assessment of the
information actions provide. The following example is designed to make this point transparent.

Example 9 (A revealing action) Suppose there are k + 1 actions {0, 1, ..., k},
and θ is an unknown parameter drawn uniformly at random from Θ = {1, .., k}.
Rewards are deterministic conditioned on θ, and when played action i ∈ {1, ..., k}
always yields reward 1 if θ = i and 0 otherwise. Action 0 is a special “revealing”
action that yields reward 1/2θ when played.

Note that action 0 is known to never yield the maximal reward, and is therefore never selected
by Thompson sampling. Instead, TS will select among actions {1, ..., k}, ruling out only a single
action at a time until a reward 1 is earned and the optimal action is identiﬁed. An optimal
algorithm for this problem would recognize that although action 0 cannot yield the maximal
reward, sampling it is valuable because of the information it provides about other actions.

38

Indeed, by sampling action 0 in the ﬁrst period, the decision maker immediately learns the
value of θ, and can exploit that knowledge to play the optimal action in all subsequent periods.
This example is described in [50], which also presents two problems of greater practical
signiﬁcance that require careful assessment of information: one related to recommendation
systems and one involving bandits with sparse-linear reward models. That paper also proposes
an algorithm that carefully assesses the information actions reveal.

8.3 Alternative Approaches

Much of the the work on multi-armed bandit problems has focused on problems with a ﬁnite
number of independent arms, like Example 3 in this paper. For such problems, the Gittins index
theorem [51] characterizes a Bayes optimal policy, which exactly maximizes expected cumulative
discounted reward. Computing this policy requires solving an optimal stopping problem for each
period and arm, and is much more demanding than Thompson sampling. For more complicated
problems, the Gittins index theorem fails to hold, and computing an optimal policy is typically
infeasible. A thorough treatment of Gittins indices is provided in [52].

Upper-conﬁdence bound (UCB) algorithms oﬀer another approach to eﬃcient exploration.
As the name suggests, these algorithms maintain upper-conﬁdence bounds, representing the
largest mean-reward an arm could plausibly generate given past observations. The algorithm
then selects the action with the highest upper-conﬁdence bound. At a high level, these algo-
rithms are similar to Thompson sampling, in that they continue sampling all promising arms
while gradually discarding those that under-perform. A more formal link between the two ap-
proaches is established in [42]. UCB algorithms have been proposed for a variety of problems,
including bandit problems with independent arms [36, 53, 54, 55], bandit problems with linearly
parameterized arms [30, 56], bandits with continuous action spaces and smooth reward functions
[57, 58, 59], and exploration in reinforcement learning [60].

The knowledge gradient [61, 62] and information-directed sampling [50] are alternative al-
gorithms that attempt to reason more carefully about the value of information acquired by
sampling an action. Finally, there is a large literature on multi-armed bandit optimization in
adversarial environments, which we will not review here. See [63] for thorough coverage.

Acknowledgements

This work was generously supported by a research grant from Boeing, a Marketing Research
Award from Adobe, and a Stanford Graduate Fellowship. We thank Susan Murphy for helpful
suggestions. Section 7.3 draws material from a paper coauthored by Xiuyuan Lu. We thank her
for help with the experiments presented in that section and for the associated code.

References

[1] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

[2] William R Thompson. On the theory of apportionment. American Journal of Mathematics,

57(2):450–456, 1935.

[3] Jeremy Wyatt. Exploration and inference in learning from reinforcement. PhD thesis,
University of Edinburgh. College of Science and Engineering. School of Informatics., 1997.

[4] Malcolm Strens. A Bayesian framework for reinforcement learning. In ICML, pages 943–

950, 2000.

[5] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Neural Infor-

mation Processing Systems (NIPS), 2011.

39

[6] S.L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models

in Business and Industry, 26(6):639–658, 2010.

[7] Kris Johnson Ferreira, David Simchi-Levi, and He Wang. Online network revenue manage-

ment using Thompson sampling. Working Paper, 2016.

[8] Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display

advertising using multi-armed bandit experiments. Marketing Science, 2017.

[9] Aijun Bai, Feng Wu, and Xiaoping Chen. Bayesian mixture modelling and inference based
Thompson sampling in Monte-Carlo tree search. In Advances in Neural Information Pro-
cessing Systems, pages 1646–1654, 2013.

[10] T. Graepel, J.Q. Candela, T. Borchert, and R. Herbrich. Web-scale Bayesian click-through
rate prediction for sponsored search advertising in Microsoft’s Bing search engine.
In
Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages
13–20, 2010.

[11] Deepak Agarwal. Computational advertising: the linkedin way. In Proceedings of the 22nd
ACM international conference on Conference on information & knowledge management,
pages 1585–1586. ACM, 2013.

[12] Deepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, and Liang Zhang. Laser: A
scalable response prediction platform for online advertising. In Proceedings of the 7th ACM
international conference on Web search and data mining, pages 173–182. ACM, 2014.

[13] Jaya Kawale, Hung H Bui, Branislav Kveton, Long Tran-Thanh, and Sanjay Chawla.
Eﬃcient Thompson sampling for online matrix-factorization recommendation. In Advances
in Neural Information Processing Systems, pages 1297–1305, 2015.

[14] Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeﬀ Schneider, and Barnabas Poczos.
arXiv preprint

Asynchronous parallel Bayesian optimisation via Thompson sampling.
arXiv:1705.09236, 2017.

[15] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration
In Advances in Neural Information Processing Systems, pages

via bootstrapped DQN.
4026–4034, 2016.

[16] Steven L Scott. Multi-armed bandit experiments in the online service economy. Applied

Stochastic Models in Business and Industry, 31(1):37–45, 2015.

[17] Benjamin Van Roy Abbas Kazerouni Zheng Wen Ian Osband, Dan Russo. TS Tutorial: A

[18] George Casella and Edward I George. Explaining the Gibbs sampler. The American

tutorial on thompson sampling, 2017.

Statistician, 46(3):167–174, 1992.

[19] Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distribu-

tions and their discrete approximations. Bernoulli, pages 341–363, 1996.

[20] Gareth O Roberts and Jeﬀrey S Rosenthal. Optimal scaling of discrete approximations to
Langevin diﬀusions. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 60(1):255–268, 1998.

[21] S´ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribu-
tion with projected Langevin Monte Carlo. arXiv preprint arXiv:1507.02564, 2015.

[22] Alain Durmus and Eric Moulines. Sampling from strongly log-concave distributions with

the unadjusted Langevin algorithm. arXiv preprint arXiv:1605.01559, 2016.

[23] Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv

preprint arXiv:1705.09048, 2017.

40

[24] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dy-

namics. In ICML, 2011.

[25] Yee Whye Teh, Alexandre H Thiery, and Sebastian J Vollmer. Consistency and ﬂuctuations
for stochastic gradient Langevin dynamics. Journal of Machine Learning Research, 17(7):1–
33, 2016.

[26] Carlos G´omez-Uribe. Online algorithms for parameter mean and variance estimation in

dynamic regression. arXiv preprint arXiv:1605.05697v1, 2016.

[27] Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. arXiv preprint arXiv:1705.07347,

2017.

[28] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. An experimental comparison
of click position-bias models. In Proceedings of the 2008 International Conference on Web
Search and Data Mining, pages 87–94. ACM, 2008.

[29] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits:
Learning to rank in the cascade model. In Proceedings of the 32nd International Conference
on Machine Learning (ICML-15), pages 767–776, 2015.

[30] V. Dani, T.P. Hayes, and S.M. Kakade. Stochastic linear optimization under bandit feed-
back. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages
355–366, 2008.

[31] Ian Osband and Benjamin Van Roy. On optimistic versus randomized exploration in rein-
forcement learning. In Proceedings of The Multi-disciplinary Conference on Reinforcement
Learning and Decision Making. 2017.

[32] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1.

[33] Michael L Littman. Reinforcement learning improves behaviour from evaluative feedback.

MIT press Cambridge, 1998.

Nature, 521(7553):445–451, 2015.

[34] Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via

randomized value functions. arXiv preprint arXiv:1703.07608, 2017.

[35] I. Osband, D. Russo, and B. Van Roy. (More) eﬃcient reinforcement learning via posterior
sampling. In Advances in Neural Information Processing Systems 26. Curran Associates,
Inc., 2013.

[36] T.L. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in

applied mathematics, 6(1):4–22, 1985.

[37] S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit
problem. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), 2012.

[38] S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. In Pro-
ceedings of the Sixteenth International Conference on Artiﬁcial Intelligence and Statistics,
pages 99–107, 2013.

[39] E. Kauﬀmann, N. Korda, and R. Munos. Thompson sampling: an asymptotically optimal

ﬁnite time analysis. In International Conference on Algorithmic Learning Theory, 2012.

[40] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoﬀs.
In Proceedings of The 30th International Conference on Machine Learning, pages 127–135,
2013.

[41] D. Russo and B. Van Roy. Eluder dimension and the sample complexity of optimistic explo-
ration. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger,
editors, Advances in Neural Information Processing Systems 26, pages 2256–2264. Curran
Associates, Inc., 2013.

41

[42] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of

Operations Research, 39(4):1221–1243, 2014.

[43] A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex online problems.
In Proceedings of The 31st International Conference on Machine Learning, pages 100–108,
2014.

[44] D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling.

Journal of Machine Learning Research, 17(68):1–30, 2016.

[45] Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In AISTATS

2017-20th International Conference on Artiﬁcial Intelligence and Statistics, 2017.

[46] Aditya Gopalan and Shie Mannor. Thompson sampling for learning parameterized Markov

decision processes. In COLT, pages 861–898, 2015.

[47] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via ran-
domized value functions. In Proceedings of The 33rd International Conference on Machine
Learning, pages 2377–2386, 2016.

[48] Michael Jong Kim. Thompson sampling for stochastic control: The ﬁnite parameter case.

IEEE Transactions on Automatic Control, 2017.

[49] Daniel Russo, David Tse, and Benjamin Van Roy. Time-sensitive bandit learning and

satisﬁcing Thompson sampling. arXiv preprint arXiv:1704.09028, 2017.

[50] D. Russo and B. Van Roy. Learning to optimize via information-directed sampling.

In
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors,
Advances in Neural Information Processing Systems 27, pages 1583–1591. Curran Asso-
ciates, Inc., 2014.

[51] J.C. Gittins and D.M. Jones. A dynamic allocation index for the discounted multiarmed

bandit problem. Biometrika, 66(3):561–565, 1979.

[52] J. Gittins, K. Glazebrook, and R. Weber. Multi-Armed Bandit Allocation Indices. John

Wiley & Sons, Ltd, 2011.

[53] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine learning, 47(2):235–256, 2002.

[54] O. Capp´e, A. Garivier, O.-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper
conﬁdence bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516–1541,
2013.

[55] E. Kaufmann, O. Capp´e, and A. Garivier. On Bayesian upper conﬁdence bounds for bandit
problems. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2012.

[56] P. Rusmevichientong and J.N. Tsitsiklis. Linearly parameterized bandits. Mathematics of

Operations Research, 35(2):395–411, 2010.

[57] R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proceed-

ings of the 40th ACM Symposium on Theory of Computing, 2008.

[58] S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv´ari. X-armed bandits. Journal of Machine

Learning Research, 12:1655–1695, June 2011.

[59] N. Srinivas, A. Krause, S.M. Kakade, and M. Seeger. Information-theoretic regret bounds
for Gaussian process optimization in the bandit setting. IEEE Transactions on Information
Theory, 58(5):3250 –3265, may 2012.

[60] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning.

Journal of Machine Learning Research, 11:1563–1600, 2010.

[61] P.I. Frazier, W.B. Powell, and S. Dayanik. A knowledge-gradient policy for sequential
information collection. SIAM Journal on Control and Optimization, 47(5):2410–2439, 2008.

42

[62] P. Frazier, W. Powell, and S. Dayanik. The knowledge-gradient policy for correlated normal

beliefs. INFORMS journal on Computing, 21(4):599–613, 2009.

[63] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. Foundations and trends in machine learning, 5(1):1–122, 2012.

43

7
1
0
2
 
v
o
N
 
9
1
 
 
]

G
L
.
s
c
[
 
 
2
v
8
3
0
2
0
.
7
0
7
1
:
v
i
X
r
a

A Tutorial on Thompson Sampling

Daniel J. Russo1, Benjamin Van Roy2, Abbas Kazerouni2, Ian Osband3, and Zheng Wen4

1Columbia University
2Stanford University
3Google Deepmind
4Adobe Research

November 21, 2017

Abstract

Thompson sampling is an algorithm for online decision problems where actions are taken
sequentially in a manner that must balance between exploiting what is known to maximize
immediate performance and investing to accumulate new information that may improve future
performance. The algorithm addresses a broad range of problems in a computationally eﬃcient
manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application,
illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest
path problems, product assortment, recommendation, active learning with neural networks, and
reinforcement learning in Markov decision processes. Most of these problems involve complex
information structures, where information revealed by taking an action informs beliefs about
other actions. We will also discuss when and why Thompson sampling is or is not eﬀective and
relations to alternative algorithms.

1 Introduction

The multi-armed bandit problem has been the subject of decades of intense study in statistics,
operations research, electrical engineering, computer science, and economics. A “one-armed
bandit” is a somewhat antiquated term for a slot machine, which tends to “rob” players of their
money. The colorful name for our problem comes from a motivating story in which a gambler
enters a casino and sits down at a slot machine with multiple levers, or arms, that can be pulled.
When pulled, an arm produces a random payout drawn independently of the past. Because the
distribution of payouts corresponding to each arm is not listed, the player can learn it only by
experimenting. As the gambler learns about the arms’ payouts, she faces a dilemma:
in the
immediate future she expects to earn more by exploiting arms that yielded high payouts in the
past, but by continuing to explore alternative arms she may learn how to earn higher payouts
in the future. Can she develop a sequential strategy for pulling arms that balances this tradeoﬀ
and maximizes the cumulative payout earned? The following Bernoulli bandit problem is a
canonical example.

Example 1 (Bernoulli Bandit) Suppose there are K actions, and when played,
any action yields either a success or a failure. Action k ∈ {1, ..., K} produces a success
with probability 0 ≤ θk ≤ 1. The success probabilities (θ1, .., θK) are unknown to
the agent, but are ﬁxed over time, and therefore can be learned by experimentation.

1

The objective, roughly speaking, is to maximize the cumulative number of successes
over T periods, where T is relatively large compared to the number of arms K.

The “arms” in this problem might represent diﬀerent banner ads that can be
displayed on a website. Users arriving at the site are shown versions of the website
with diﬀerent banner ads. A success is associated either with a click on the ad, or
with a conversion (a sale of the item being advertised). The parameters θk represent
either the click-through-rate or conversion-rate among the population of users who
frequent the site. The website hopes to balance exploration and exploitation in order
to maximize the total number of successes.

A naive approach to this problem involves allocating some ﬁxed fraction of time
periods to exploration and in each such period sampling an arm uniformly at random,
while aiming to select successful actions in other time periods. We will observe that
such an approach can be quite wasteful even for the simple Bernoulli bandit problem
described above and can fail completely for more complicated problems.

Problems like the Bernoulli bandit described above have been studied in the decision sciences
since the second world war, as they crystallize the fundamental trade-oﬀ between exploration
and exploitation in sequential decision making. But the information revolution has created
signiﬁcant new opportunities and challenges, which have spurred a particularly intense interest
in this problem in recent years. To understand this, let us contrast the Internet advertising
example given above with the problem of choosing a banner ad to display on a highway. A
physical banner ad might be changed only once every few months, and once posted will be seen
by every individual who drives on the road. There is value to experimentation, but data is
limited, and the cost of of trying a potentially ineﬀective ad is enormous. Online, a diﬀerent
banner ad can be shown to each individual out of a large pool of users, and data from each
such interaction is stored. Small-scale experiments are now a core tool at most leading Internet
companies.

Our interest in this problem is motivated by this broad phenomenon. Machine learning is
increasingly used to make rapid data-driven decisions. While standard algorithms in supervised
machine learning learn passively from historical data, these systems often drive the generation
of their own training data through interacting with users. An online recommendation system,
for example, uses historical data to optimize current recommendations, but the outcomes of
these recommendations are then fed back into the system and used to improve future recom-
mendations. As a result, there is enormous potential beneﬁt in the design of algorithms that not
only learn from past data, but also explore systemically to generate useful data that improves
future performance. There are signiﬁcant challenges in extending algorithms designed to address
Example 1 to treat more realistic and complicated decision problems. To understand some of
these challenges, consider the problem of learning by experimentation to solve a shortest path
problem.

Example 2 (Online Shortest Path) An agent commutes from home to work
every morning. She would like to commute along the path that requires the least
average travel time, but she is uncertain of the travel time along diﬀerent routes.
How can she learn eﬃciently and minimize the total travel time over a large number
of trips?

2

We can formalize this as a shortest path problem on a graph G = (V, E) with
vertices V = {1, ..., N } and edges E. Here vertex 1 is the source (her home) and
vertex N is the destination (work). Each vertex can be thought of as an intersection,
and for two vertices i, j ∈ V , an edge (i, j) ∈ E is present if there is a direct
road connecting the two intersections. Suppose that traveling along an edge e ∈ E
requires time θe on average. If these parameters were known, the agent would select
a path (e1, .., en), consisting of a sequence of adjacent edges connecting vertices 1
and N , such that the expected total time θe1 + ... + θen is minimized.
Instead,
she chooses paths in a sequence of periods.
In period t, the realized time yt,e to
traverse edge e is drawn independently from a distribution with mean θe. The agent
sequentially chooses a path xt, observes the realized travel time (yt,e)e∈xt along each
edge in the path, and incurs cost ct = (cid:80)
yt,e equal to the total travel time. By
exploring intelligently, she hopes to minimize cumulative travel time (cid:80)T
t=1 ct over a
large number of periods T .

e∈xt

This problem is conceptually similar to the Bernoulli bandit in Example 1, but
here the number of actions is the number of paths in the graph, which generally
scales exponentially in the number of edges. This raises substantial challenges. For
moderate sized graphs, trying each possible path would require a prohibitive number
of samples, and algorithms that require enumerating and searching through the set
of all paths to reach a decision will be computationally intractable. An eﬃcient
approach therefore needs to leverage the statistical and computational structure of
problem.

In this model, the agent observes the travel time along each edge traversed in
a given period. Other feedback models are also natural: the agent might start a
timer as she leaves home and checks it once she arrives, eﬀectively only tracking
the total travel time of the chosen path. This is closer to the Bernoulli bandit
model, where only the realized reward (or cost) of the chosen arm was observed. We
have also taken the random edge-delays yt,e to be independent, conditioned on θe.
A more realistic model might treat these as correlated random variables, reﬂecting
that neighboring roads are likely to be congested at the same time. Rather than
design a specialized algorithm for each possible statistical model, we seek a general
approach to exploration that accommodates ﬂexible modeling and works for a broad

3

array of problems. We will see that Thompson sampling accommodates such ﬂexible
modeling, and oﬀers an elegant and eﬃcient approach to exploration in a wide range
of structured decision problems, including the shortest path problem described here.

Thompson sampling was ﬁrst proposed in 1933 [1, 2] for allocating experimental eﬀort in two-
armed bandit problems arising in clinical trials. The algorithm was large largely ignored in the
academic literature until recently, although it was independently rediscovered several times in
the interim [3, 4] as an eﬀective heuristic. Now, more than eight decades after it was introduced,
Thompson sampling has seen a surge of interest among industry practitioners and academics.
This was spurred partly by two inﬂuential articles that displayed the algorithm’s strong empirical
performance [5, 6].
In the subsequent ﬁve years, the literature on Thompson sampling has
grown rapidly. Adaptations of Thompson sampling have now been successfully applied in a
wide variety of domains, including revenue management [7], marketing [8], Monte Carlo tree
search [9], A/B testing [10], Internet advertising [10, 11, 12], recommendation systems [13],
hyperparameter tuning [14], and arcade games [15]; and have been used at several companies,
including Microsoft [10], Google [6, 16], LinkedIn [11, 12], Twitter, Netﬂix, and Adobe.

The objective of this tutorial is to explain when, why, and how to apply Thompson sampling.
A range of examples are used to demonstrate how the algorithm can be used to solve interesting
problems and provide clear insight into why it works and when it oﬀers substantial beneﬁt
over naive alternatives. The tutorial also provides guidance on approximations to Thompson
sampling that can simplify computation as well as practical considerations like prior distribution
speciﬁcation, safety constraints and nonstationarity. Accompanying this tutorial we also release
a Python package1 that reproduces all experiments and ﬁgures in this paper [17]. This resource
is valuable not only for reproducible research, but also as a reference implementation that
may help practioners build intuition for how to practically implement some of the ideas and
algorithms we outline in this paper. A concluding section highlights settings where Thompson
sampling performs poorly and discusses alternative approaches studied in recent literature. As
a baseline and backdrop for our discussion of Thompson sampling, we begin with an alternative
approach that does not actively explore.

2 Greedy Decisions

Greedy algorithms serve as perhaps the simplest and most common approach to online decision
problems. The following two steps are taken to generate each action: (1) estimate a model from
historical data and (2) select the action that is optimal for the estimated model, breaking ties
in an arbitrary manner. Such an algorithm is greedy in the sense that an action is chosen solely
to maximize immediate reward. Figure 1 illustrates such a scheme. At each time t, a super-
vised learning algorithm ﬁts a model to historical data pairs Ht−1 = ((x1, y1), . . . , (xt−1, yt−1)),
generating an estimate ˆθ of model parameters. The resulting model can then be used to predict
the reward rt = r(yt) from applying action xt. Here, yt is an observed outcome, while r is a
known function that represents the agent’s preferences. Given estimated model parameters ˆθ,
an optimization algorithm selects the action xt that maximizes expected reward, assuming that
θ = ˆθ. This action is then applied to the exogenous system and an outcome yt is observed.

A shortcoming of the greedy approach, which can severely curtail performance, is that it
does not actively explore. To understand this issue, it is helpful to focus on the Bernoulli bandit
setting of Example 1. In that context, the observations are rewards, so rt = r(yt) = yt. At each
time t, a greedy algorithm would generate a estimate ˆθk of the mean reward for each kth action,
and select the action that attains the maximum among these estimates.

1Full code and documentation is available at https://github.com/iosband/ts_tutorial.

4

Figure 1: Online decision algorithm.

Suppose there are three actions with mean rewards θ ∈ R3.

In particular, each time an
action k is selected, a reward of 1 is generated with probability θk. Otherwise, a reward of 0 is
generated. The mean rewards are not known to the agent. Instead, the agent’s beliefs in any
given time period about these mean rewards can be expressed in terms of posterior distributions.
Suppose that, conditioned on the observed history Ht−1, posterior distributions are represented
by the probability density functions plotted in Figure 2. These distributions represent beliefs
after the agent tries actions 1 and 2 one thousand times each, action 3 ﬁve times, receives
cumulative rewards of 600, 400, and 2, respectively, and synthesizes these observations with
uniform prior distributions over mean rewards of each action. They indicate that the agent is
conﬁdent that mean rewards for actions 1 and 2 are close to their expectations of 0.6 and 0.4.
On the other hand, the agent is highly uncertain about the mean reward of action 3, though he
expects 0.4.

The greedy algorithm would select action 1, since that oﬀers the maximal expected mean
reward. Since the uncertainty around this expected mean reward is small, observations are
unlikely to change the expectation substantially, and therefore, action 1 is likely to be selected
ad inﬁnitum. It seems reasonable to avoid action 2, since it is extremely unlikely that θ2 > θ1.
On the other hand, if the agent plans to operate over many time periods, it should try action
3. This is because there is some chance that θ3 > θ1, and if this turns out to be the case, the
agent will beneﬁt from learning that and applying action 3. To learn whether θ3 > θ1, the agent
needs to try action 3, but the greedy algorithm will unlikely ever do that. The algorithm fails
to account for uncertainty in the mean reward of action 3, which should entice the agent to
explore and learn about that action.

Dithering is a common approach to exploration that operates through randomly perturbing
actions that would be selected by a greedy algorithm. One version of dithering, called (cid:15)-greedy
exploration, applies the greedy action with probability 1 − (cid:15) and otherwise selects an action
uniformly at random. Though this form of exploration can improve behavior relative to a
purely greedy approach, it wastes resources by failing to “write oﬀ” actions regardless of how
unlikely they are to be optimal. To understand why, consider again the posterior distributions
of Figure 2. Action 2 has almost no chance of being optimal, and therefore, does not deserve
experimental trials, while the uncertainty surrounding action 3 warrants exploration. However,

5

Figure 2: Probability density functions over mean rewards.

(cid:15)-greedy exploration would allocate an equal number of experimental trials to each action.
Though only half of the exploratory actions are wasted in this example, the issue is exacerbated
as the number of possible actions increases. Thompson sampling, introduced more than eight
decades ago [1], provides an alternative to dithering that more intelligently allocates exploration
eﬀort.

3 Thompson Sampling for the Bernoulli Bandit

To digest how Thompson sampling works, it is helpful to begin with a simple context that
builds on the Bernoulli bandit of Example 1 and incorporates a Bayesian model to represent
uncertainty.

Example 3 (Beta-Bernoulli Bandit) Recall the Bernoulli bandit of Example
1. There are K actions. When played, an action k produces a reward of one with
probability θk and a reward of zero with probability 1−θk. Each θk can be interpreted
as an action’s success probability or mean reward. The mean rewards θ = (θ1, ..., θK)
are unknown, but ﬁxed over time. In the ﬁrst period, an action x1 is applied, and a
reward r1 ∈ {0, 1} is generated with success probability P(r1 = 1|x1, θ) = θx1 . After
observing r1, the agent applies another action x2, observes a reward r2, and this
process continues.

Let the agent begin with an independent prior belief over each θk. Take these
priors to be beta-distributed with parameters α = (α1, . . . , αK) and β ∈ (β1, . . . , βK).
In particular, for each action k, the prior probability density function of θk is

p(θk) =

Γ(αk + βk)
Γ(αk)Γ(βk)

θαk−1
k

(1 − θk)βk−1,

6

where Γ denotes the gamma function. As observations are gathered, the distribution
is updated according to Bayes’ rule. It is particularly convenient to work with beta
distributions because of their conjugacy properties. In particular, each action’s pos-
terior distribution is also beta with parameters that can be updated according to a
simple rule:

(cid:40)

(αk, βk) ←

(αk, βk)
(αk, βk) + (rt, 1 − rt)

if xt (cid:54)= k
if xt = k.

Note that for the special case of αk = βk = 1, the prior p(θk) is uniform over [0, 1]. Note that
only the parameters of a selected arm are updated. The parameters (αk, βk) are sometimes
called pseudo-counts, since αk or βk increases by one with each observed success or failure,
respectively. A beta distribution with parameters (αk, βk) has mean αk/(αk + βk), and the
distribution becomes more concentrated as αk + βk grows. Figure 2 plots probability density
functions of beta distributions with parameters (α1, β1) = (600, 400), (α2, β2) = (400, 600), and
(α3, β3) = (4, 6).

Algorithm 2 presents a greedy algorithm for the beta-Bernoulli bandit. In each time period
t, the algorithm generates an estimate ˆθk = αk/(αk + βk), equal to its current expectation of the
success probability θk. The action xt with the largest estimate ˆθk is then applied, after which a
reward rt is observed and the distribution parameters αxt and βxt are updated.

Algorithm 1 BernGreedy(K, α, β)

Algorithm 2 BernThompson(K, α, β)

1: for t = 1, 2, . . . do
2:

#estimate model:
for k = 1, . . . , K do

3:

ˆθk ← αk/(αk + βk)

end for

1: for t = 1, 2, . . . do
2:

#sample model:
for k = 1, . . . , K do

3:

Sample ˆθk ∼ beta(αk, βk)

end for

#select and apply action:
xt ← argmaxk
Apply xt and observe rt

ˆθk

#select and apply action:
xt ← argmaxk
Apply xt and observe rt

ˆθk

#update distribution:
(αxt, βxt) ← (αxt, βxt)+(rt, 1−rt)

#update distribution:
(αxt, βxt) ← (αxt, βxt)+(rt, 1−rt)

12:
13: end for

12:
13: end for

4:

5:

6:

7:

8:

9:

10:

11:

Thompson sampling, specialized to the case of a beta-Bernoulli bandit, proceeds similarly,
as presented in Algorithm 2. The only diﬀerence is that the success probability estimate ˆθk is
randomly sampled from the posterior distribution, which is a beta distribution with parameters
αk and βk, rather than taken to be the expectation αk/(αk + βk). To avoid a common mis-
conception, it is worth emphasizing Thompson sampling does not sample ˆθk from the posterior
distribution of the binary value yt that would be observed if action k is selected. In particular,
ˆθk represents a statistically plausible success probability rather than a statistically plausible
observation.

To understand how Thompson sampling improves on greedy actions with or without dither-
ing, recall the three armed Bernoulli bandit with posterior distributions illustrated in Figure
2. In this context, a greedy action would forgo the potentially valuable opportunity to learn

4:

5:

6:

7:

8:

9:

10:

11:

7

about action 3. With dithering, equal chances would be assigned to probing actions 2 and 3,
though probing action 2 is virtually futile since it is extremely unlikely to be optimal. Thompson
sampling, on the other hand would sample actions 1, 2, or 3, with probabilities approximately
equal to 0.82, 0, and 0.18, respectively. In each case, this is the probability that the random
estimate drawn for the action exceeds those drawn for other actions. Since these estimates are
drawn from posterior distributions, each of these probabilities is also equal to the probability
that the corresponding action is optimal, conditioned on observed history. As such, Thompson
sampling explores to resolve uncertainty where there is a chance that resolution will help the
agent identify the optimal action, but avoids probing where feedback would not be helpful.

It is illuminating to compare simulated behavior of Thompson sampling to that of a greedy
algorithm. Consider a three-armed beta-Bernoulli bandit with mean rewards θ1 = 0.9, θ2 = 0.8,
and θ3 = 0.7. Let the prior distribution over each mean reward be uniform. Figure 3 plots
results based on ten thousand independent simulations of each algorithm. Each simulation is
over one thousand time periods. In each simulation, actions are randomly rank-ordered for the
purpose of tie-breaking so that the greedy algorithm is not biased toward selecting any particular
action. Each data point represents the fraction of simulations for which a particular action is
selected at a particular time.

(a) greedy algorithm

(b) Thompson sampling

Figure 3: Probability that the greedy algorithm and Thompson sampling selects an action.

From the plots, we see that the greedy algorithm does not always converge on action 1, which
is the optimal action. This is because the algorithm can get stuck, repeatedly applying a poor
action. For example, suppose the algorithm applies action 3 over the ﬁrst couple time periods
and receives a reward of 1 on both occasions. The algorithm would then continue to select action
3, since the expected mean reward of either alternative remains at 0.5. With repeated selection
of action 3, the expected mean reward converges to the true value of 0.7, which reinforces the
agent’s commitment to action 3. Thompson sampling, on the other hand, learns to select action
1 within the thousand periods. This is evident from the fact that, in an overwhelmingly large
fraction of simulations, Thompson sampling selects action 1 in the ﬁnal period.

The performance of online decision algorithms is often studied and compared through plots
of regret. The per-period regret of an algorithm over a time period t is the diﬀerence between the
mean reward of an optimal action and the action selected by the algorithm. For the Bernoulli
bandit problem, we can write this as regrett(θ) = maxk θk−θxt. Figure 4a plots per-period regret
realized by the greedy algorithm and Thompson sampling, again averaged over ten thousand
simulations. The average per-period regret of Thompson sampling vanishes as time progresses.
That is not the case for the greedy algorithm.

Comparing algorithms with ﬁxed mean rewards raises questions about the extent to which

8

the results depend on the particular choice of θ. As such, it is often useful to also examine
regret averaged over plausible values of θ. A natural approach to this involves sampling many
instances of θ from the prior distributions and generating an independent simulation for each.
Figure 4b plots averages over ten thousand such simulations, with each action reward sampled
independently from a uniform prior for each simulation. Qualitative features of these plots are
similar to those we inferred from Figure 4a, though regret in Figure 4a is generally smaller over
early time periods and larger over later time periods, relative to Figure 4b. The smaller regret
in early time periods is due to the fact that with θ = (0.9, 0.8, 0.7), mean rewards are closer
than for a typical randomly sampled θ, and therefore the regret of randomly selected actions is
smaller. The reduction in later time periods is also a consequence of proximity among rewards
with θ = (0.9, 0.8, 0.7). In this case, the diﬀerence is due to the fact that it takes longer to
diﬀerentiate actions than it would for a typical randomly sampled θ.

(a) θ = (0.9, 0.8, 0.7)

(b) average over random θ

Figure 4: Regret from applying greedy and Thompson sampling algorithms to the three-armed
Bernoulli bandit.

4 General Thompson Sampling

Thompson sampling can be applied fruitfully to a broad array of online decision problems beyond
the Bernoulli bandit, and we now consider a more general setting. Suppose the agent applies
a sequence of actions x1, x2, x3, . . . to a system, selecting each from a set X . This action set
could be ﬁnite, as in the case of the Bernoulli bandit, or inﬁnite. After applying action xt, the
agent observes an outcome yt, which the system randomly generates according to a conditional
probability measure qθ(·|xt). The agent enjoys a reward rt = r(yt), where r is a known function.
The agent is initially uncertain about the value of θ and represents his uncertainty using a prior
distribution p.

Algorithms 3 and 4 present greedy and Thompson sampling approaches in an abstract form
that accommodates this very general problem. The two diﬀer in the way they generate model
parameters ˆθ. The greedy algorithm takes ˆθ to be the expectation of θ with respect to the
distribution p, while Thompson sampling draws a random sample from p. Both algorithms then
apply actions that maximize expected reward for their respective models. Note that, if there
are a ﬁnite set of possible observations yt, this expectation is given by

(4.1)

Eq ˆθ

[r(yt)|xt = x] =

qˆθ(o|x)r(o).

(cid:88)

o

9

(4.2)

4:

5:

6:

7:

8:

9:

The distribution p is updated by conditioning on the realized observation ˆyt. If θ is restricted
to values from a ﬁnite set, this conditional distribution can be written by Bayes rule as

Pp,q(θ = u|xt, yt) =

p(u)qu(yt|xt)
v p(v)qv(yt|xt)

.

(cid:80)

Algorithm 3 Greedy(X , p, q, r)

Algorithm 4 Thompson(X , p, q, r)

1: for t = 1, 2, . . . do
2:

#estimate model:
ˆθ ← Ep[θ]

3:

1: for t = 1, 2, . . . do
2:

#sample model:
Sample ˆθ ∼ p

3:

#select and apply action:
xt ← argmaxx∈X
Apply xt and observe yt

Eq ˆθ

[r(yt)|xt = x]

#select and apply action:
xt ← argmaxx∈X
Apply xt and observe yt

Eq ˆθ

[r(yt)|xt = x]

#update distribution:
p ← Pp,q(θ ∈ ·|xt, yt)

10:
11: end for

#update distribution:
p ← Pp,q(θ ∈ ·|xt, yt)

10:
11: end for

4:

5:

6:

7:

8:

9:

The Bernoulli bandit with a beta prior serves as a special case of this more general formu-
lation. In this special case, the set of actions is X = {1, . . . , K} and only rewards are observed,
so yt = rt. Observations and rewards are modeled by conditional probabilities qθ(1|k) = θk and
qθ(0|k) = 1 − θk. The prior distribution is encoded by vectors α and β, with probability density
function given by:

p(θ) =

K
(cid:89)

k=1

Γ(α + β)
Γ(αk)Γ(βk)

θαk−1
k

(1 − θk)βk−1,

where Γ denotes the gamma function. In other words, under the prior distribution, components
of θ are independent and beta-distributed, with parameters α and β.

For this problem, the greedy algorithm (Algorithm 3) and Thompson sampling (Algorithm
4) begin each tth iteration with posterior parameters (αe, βe) for e ∈ E. The greedy algorithm
sets ˆθe to the expected value Ep[θe] = αe/(αe + βe), whereas Thompson sampling randomly
draws ˆθe from a beta distribution with parameters (αe, βe). Each algorithm then selects the
[r(yt)|xt = x] = ˆθx. After applying the selected action, a reward
action x that maximizes Eq ˆθ
rt = yt is observed, and belief distribution parameters are updated according to

(α, β) ← (α + rt1xt, β + (1 − rt)1xt),

where 1xt is a vector with component xt equal to 1 and all other components equal to 0.

Algorithms 3 and 4 can also be applied to much more complex problems. As an example,

let us consider a version of the shortest path problem presented in Example 2.

Example 4 (Independent Travel Times) Recall the shortest path problem of
Example 2. The model is deﬁned with respect to a directed graph G = (V, E), with
vertices V = {1, . . . , N }, edges E, and mean travel times θ ∈ RN . Vertex 1 is the
source and vertex N is the destination. An action is a sequence of distinct edges
leading from source to destination. After applying action xt, for each traversed edge
e ∈ xt, the agent observes a travel time yt,e that is independently sampled from a

10

distribution with mean θe. Further, the agent incurs a cost of (cid:80)
be thought of as a reward rt = − (cid:80)

yt,e.
Consider a prior for which each θe is independent and lognormally-distributed
e . That is, ln(θe) ∼ N (µe, σ2
with parameters µe and σ2
e ) is normally distributed.
Hence, E[θe] = eµe+σ2
e /2. Further, take yt,e|θ to be independent across edges e ∈ E
and lognormally distributed with parameters ln θe − ˜σ2/2 and ˜σ2, so that E[yt,e|θe] =
θe. Conjugacy properties accommodate a simple rule for updating the distribution
of θe upon observation of yt,e:

yt,e, which can

e∈xt

e∈xt

(4.3)

(µe, σ2

e ) ←

1
σ2
e





µe + 1
˜σ2
1
σ2
e

(cid:17)

(cid:16)

ln yt,e + ˜σ2
2
+ 1
˜σ2

1
+ 1
˜σ2

,

1
σ2
e



 .

To motivate this formulation, consider an agent who commutes from home to work every
morning. Suppose possible paths are represented by a graph G = (V, E). Suppose the agent
knows the travel distance de associated with each edge e ∈ E but is uncertain about average
travel times. It would be natural for her to construct a prior for which expectations are equal to
travel distances. With the lognormal prior, this can be accomplished by setting µe = ln de−σ2
e /2.
Note that the parameters µe and σ2
e also express a degree of uncertainty; in particular, the prior
variance of mean travel time along an edge is (eσ2

e − 1)d2
e.

The greedy algorithm (Algorithm 3) and Thompson sampling (Algorithm 4) can be applied
to Example 4 in a computationally eﬃcient manner. Each algorithm begins each tth iteration
with posterior parameters (µe, σe) for each e ∈ E. The greedy algorithm sets ˆθe to the expected
e /2, whereas Thompson sampling randomly draws ˆθe from a lognormal
value Ep[θe] = eµe+σ2
distribution with parameters µe and σ2
e . Each algorithm then selects its action x to maximize
ˆθe. This can be cast as a deterministic shortest path problem,
Eq ˆθ
which can be solved eﬃciently, for example, via Dijkstra’s algorithm. After applying the selected
action, an outcome yt is observed, and belief distribution parameters (µe, σ2
e ), for each e ∈ E,
are updated according to (4.3).

[r(yt)|xt = x] = − (cid:80)

e∈xt

Figure 6 presents results from applying greedy and Thompson sampling algorithms to Ex-
ample 4, with the graph taking the form of a binomial bridge, as shown in Figure 5, except
with twenty rather than six stages, so there are 184,756 paths from source to destination. Prior
e = 1 so that E[θe] = 1, for each e ∈ E, and the conditional
parameters are set to µe = − 1
distribution parameter is ˜σ2 = 1. Each data point represents an average over ten thousand
independent simulations.

2 and σ2

The plots of regret demonstrate that the performance of Thompson sampling converges
quickly to that of the optimal policy, while that is far from true for the greedy algorithm. We
also plot results generated by (cid:15)-greedy exploration, varying (cid:15). For each trip, with probability
1 − (cid:15), this algorithm traverses a path produced by a greedy algorithm. Otherwise, the algorithm
samples a path randomly. Though this form of exploration can be helpful, the plots demonstrate
that learning progresses at a far slower pace than with Thompson sampling. This is because
(cid:15)-greedy exploration is not judicious in how it selects paths to explore. Thompson sampling,
on the other hand, orients exploration eﬀort towards informative rather than entirely random
paths.

Plots of cumulative travel time relative to optimal oﬀer a sense for the fraction of driving
time wasted due to lack of information. Each point plots an average of the ratio between the
time incurred over some number of days and the minimal expected travel time given θ. With
Thompson sampling, this converges to one at a respectable rate. The same can not be said for
(cid:15)-greedy approaches.

Algorithm 4 can be applied to problems with complex information structures, and there is
often substantial value to careful modeling of such structures. As an example, we consider a

11

Figure 5: A binomial bridge with six stages.

(a) regret

(b) cumulative travel time relative to optimal

Figure 6: Performance of Thompson sampling and (cid:15)-greedy algorithms in the shortest path problem.

more complex variation of the binomial bridge example.

Example 5 (Correlated Travel Times) As with Example 4, let each θe be inde-
e . Let the observation

pendent and lognormally-distributed with parameters µe and σ2
distribution be characterized by

yt,e = ζt,eηtνt,(cid:96)(e)θe,

where each ζt,e represents an idiosyncratic factor associated with edge e, ηt represents
a factor that is common to all edges, (cid:96)(e) indicates whether edge e resides in the lower
half of the binomial bridge, and νt,0 and νt,1 represent factors that bear a common
inﬂuence on edges in the upper and lower halves, respectively. We take each ζt,e,
ηt, νt,0, and νt,1 to be independent lognormally distributed with parameters −˜σ2/6
and ˜σ2/3. The distributions of the shocks ζt,e, ηt, νt,0 and νt,1 are known, and only
the parameters θe corresponding to each individual edge must be learned through
experimentation. Note that, given these parameters, the marginal distribution of
yt,e|θ is identical to that of Example 4, though the joint distribution over yt|θ diﬀers.

12

The common factors induce correlations among travel times in the binomial
bridge: ηt models the impact of random events that inﬂuence traﬃc conditions every-
where, like the day’s weather, while νt,0 and νt,1 each reﬂect events that bear inﬂuence
only on traﬃc conditions along edges in half of the binomial bridge. Though mean
edge travel times are independent under the prior, correlated observations induce
dependencies in posterior distributions.

Conjugacy properties again facilitate eﬃcient updating of posterior parameters.

Let φ, zt ∈ RN be deﬁned by

φe = ln θe

and

zt,e =

(cid:26) ln yt,e
0

if e ∈ xt
otherwise.

Note that it is with some abuse of notation that we index vectors and matrices using
edge indices. Deﬁne a |xt| × |xt| covariance matrix ˜Σ with elements

˜Σe,e(cid:48) =






˜σ2
2˜σ2/3
˜σ2/3

for e = e(cid:48)
for e (cid:54)= e(cid:48), (cid:96)(e) = (cid:96)(e(cid:48))
otherwise,

for e, e(cid:48) ∈ xt, and a N × N concentration matrix

˜Ce,e(cid:48) =

(cid:26) ˜Σ−1
e,e(cid:48)
0

if e, e(cid:48) ∈ xt
otherwise,

for e, e(cid:48) ∈ E. Then, the posterior distribution of φ is normal with a mean vector µ
and covariance matrix Σ that can be updated according to

(4.4)

(µ, Σ) ←

Σ−1 + ˜C

Σ−1µ + ˜Czt

(cid:17)−1 (cid:16)

(cid:18)(cid:16)

(cid:17)

(cid:16)

,

Σ−1 + ˜C

(cid:17)−1(cid:19)

.

Thompson sampling (Algorithm 4) can again be applied in a computationally eﬃcient manner.
Each tth iteration begins with posterior parameters µ ∈ RN and Σ ∈ RN ×N . The sample ˆθ can
be drawn by ﬁrst sampling a vector ˆφ from a normal distribution with mean µ and covariance
matrix Σ, and then setting ˆθe = ˆφe for each e ∈ E. An action x is selected to maximize
ˆθe, using Djikstra’s algorithm or an alternative. After applying the
Eq ˆθ
selected action, an outcome yt is observed, and belief distribution parameters (µ, Σ) are updated
according to (4.4).

[r(yt)|xt = x] = − (cid:80)

e∈xt

2 , σ2

Figure 7 plots results from applying Thompson sampling to Example 5, again with the
binomial bridge, µe = − 1
e = 1, and ˜σ2 = 1. Each data point represents an average
over ten thousand independent simulations. Despite model diﬀerences, an agent can pretend
that observations made in this new context are generated by the model described in Example
In particular, the agent could maintain an independent lognormal posterior for each θe,
4.
updating parameters (µe, σ2
e ) as though each yt,e|θ is independently drawn from a lognormal
distribution. As a baseline for comparison, Figure 7 additionally plots results from application of
this approach, which we will refer to here as misspeciﬁed Thompson sampling. The comparison
demonstrates substantial improvement that results from accounting for interdependencies among
edge travel times, as is done by what we refer to here as coherent Thompson sampling. Note
that we have assumed here that the agent must select a path before initiating each trip. In
particular, while the agent may be able to reduce travel times in contexts with correlated delays
by adjusting the path during the trip based on delays experienced so far, our model does not
allow this behavior.

13

(a) regret

(b) cumulative travel time relative to optimal

Figure 7: Performance of two versions of Thompson sampling in the shortest path problem with
correlated travel times.

5 Approximations

Conjugacy properties in the Bernoulli bandit and shortest path examples that we have considered
so far facilitated simple and computationally eﬃcient Bayesian inference. Indeed, computational
eﬃciency can be an important consideration when formulating a model. However, many practi-
cal contexts call for more complex models for which exact Bayesian inference is computationally
intractable. Fortunately, there are reasonably eﬃcient and accurate methods that can be used
to approximately sample from posterior distributions.

In this section we discuss four approaches to approximate posterior sampling: Gibbs sam-
pling, Langevin Monte Carlo, sampling from a Laplace approximation, and the bootstrap. Such
methods are called for when dealing with problems that are not amenable to eﬃcient Bayesian
inference. As an example, we consider a variation of the online shortest path problem.

Example 6 (Binary Feedback) Consider Example 5, except with deterministic
travel times and noisy binary observations. Let the graph represent a binomial bridge
with M stages. Let each θe be independent and gamma-distributed with E[θe] = 1,
E[θ2

e] = 1.5, and observations be generated according to

yt|θ ∼

(cid:40) 1
0

with probability

1

1+exp((cid:80)

e∈xt

θe−M)

otherwise.

We take the reward to be the rating rt = yt. This information structure could be
used to model, for example, an Internet route recommendation service. Each day, the
system recommends a route xt and receives feedback yt from the driver, expressing
whether the route was desirable. When the realized travel time (cid:80)
θe falls short
of the prior expectation M , the feedback tends to be positive, and vice versa.

e∈xt

This new model does not enjoy conjugacy properties leveraged in Section 4 and is not amenable
to eﬃcient exact Bayesian inference. However, the problem may be addressed via approximation
methods. To illustrate, Figure 8 plots results from application of two approximate versions of
Thompson sampling to an online shortest path problem on a twenty-stage binomial bridge with
binary feedback. The algorithms leverage the Laplace approximation and the bootstrap, two
approaches we will discuss, and the results demonstrate eﬀective learning, in the sense that
regret vanishes over time.

In the remainder of this section, we will describe several approaches to approximate Thomp-
son sampling. It is worth mentioning that we do not cover an exhaustive list, and further, our

14

descriptions do not serve as comprehensive or deﬁnitive treatments of each approach. Rather,
our intent is to oﬀer simple descriptions that convey key ideas that may be extended or combined
to serve needs arising in any speciﬁc application.

Throughout this section, let ft−1 denote the posterior density of θ conditioned on the history
of observations Ht−1 = ((x1, y1), . . . , (xt−1, yt−1)). Thompson sampling generates an action
xt by sampling a parameter vector ˆθ from ft−1 and solving for the optimal path under ˆθ.
The methods we describe generate a sample ˆθ whose distribution approximates the posterior
ˆft−1, which enables approximate implementations of Thompson sampling when exact posterior
sampling is infeasible.

Figure 8: Regret experienced by approximation methods applied to the path recommendation
problem with binary feedback.

5.1 Gibbs Sampling

Gibbs sampling is a general Markov chain Monte Carlo (MCMC) algorithm for drawing approx-
imate samples from multivariate probability distributions. It produces a sequence of sampled
parameters (ˆθn : n = 0, 2 . . .) forming a Markov chain with stationary distribution ft−1. Under
reasonable technical conditions, the limiting distribution of this Markov chain is its stationary
distribution, and the distribution of ˆθn converges to ft−1.

Gibbs sampling starts with an initial guess ˆθ0. Iterating over sweeps n = 1, . . . , N , for each
nth sweep, the algorithm iterates over the components k = 1, . . . , K, for each k generating a
one-dimensional marginal distribution

t−1(θk) ∝ ft−1((ˆθn
f n,k

1 , . . . , ˆθn

k−1, θk, ˆθn−1

k+1 , . . . , ˆθn−1

K )),

and sampling the kth component according to ˆθn
t−1. After N of sweeps, the prevailing
vector ˆθN is taken to be the approximate posterior sample. We refer to [18] for a more thorough
introduction to the algorithm.

k ∼ f n,k

Gibbs sampling applies to a broad range of problems, and is often computationally viable
even when sampling from ft−1 is not. This is because sampling from a one-dimensional distribu-
tion is simpler. That said, for complex problems, Gibbs sampling can still be computationally
demanding. This is the case, for example, with our path recommendation problem with binary

15

(5.1)

tion

(5.2)

feedback. In this context, it is easy to implement a version of Gibbs sampling that generates
a close approximation to a posterior samples within well under a minute. However, running
thousands of simulations each over hundreds of time periods can be quite time-consuming. As
such, we turn to more eﬃcient approximation methods.

5.2 Langevin Monte Carlo

We now describe an alternative Markov chain Monte Carlo method that uses gradient informa-
tion about the target distribution. Let g(φ) denote a log-concave probability density function
over RK from which we wish to sample. Suppose that ln g(φ) is diﬀerentiable and its gradients
are eﬃciently computable. Arising ﬁrst in physics, Langevin daynmics refer to the diﬀusion
process

dφt = ∇ ln g(φt)dt +

2dBt

√

where Bt is a standard Brownian motion process. This process has g(φ) as its unique stationary
distribution, and under reasonable technical conditions, the distribution of φt converges rapidly
to this stationary distribution [19]. Therefore simulating the process (5.1) provides a means of
approximately sampling from g(φ).

Typically, one instead implements a Euler discretization of this stochastic diﬀerential equa-

φn+1 = φn + (cid:15)∇ ln g(φn) +

2(cid:15)Wn

n ∈ N,

√

where W1, W2, · · · are i.i.d. standard normal random variables and (cid:15) > 0 is a small step size.
Like a gradient ascent method, under this method φn tends to drift in directions of increasing
density g(φn). However, random Gaussian noise Wn is injected at each step so that, for large
n, the position of φn is random and captures the uncertainty in the distribution g. A number
of papers establish rigorous guarantees for the rate at which this Markov chain converges to
its stationary distribution [20, 21, 22, 23]. These papers typically require (cid:15) is suﬃciently small,
or that a decaying sequence of step-sizes ((cid:15)1, (cid:15)2, · · · ) is used. Recent work [24, 25] has studied
stochastic gradient Langevin Monte Carlo, which uses sampled minibatches of data to compute
approximate rather than exact gradients.

For the path recommendation problem in Example 6, we have found that the log posterior
density becomes ill conditioned in later time periods. For this reason, gradient ascent converges
very slowly to the posterior mode. Eﬀective optimization methods must somehow leverage
second order information. Similarly, due to poor conditioning, we may need to choose an
extremely small step-size (cid:15), causing the Markov chain in 5.2 to mix slowly. We have found that
preconditioning substantially improves performance. Langevin MCMC can be implemented
with a symmetric positive deﬁnite preconditioning matrix M by simulating the Markov chain

φn+1 = φn + (cid:15)M ∇ ln g(φn) +

2(cid:15)M 1/2Wn

n ∈ N,

√

where M 1/2 denotes the matrix square root of M . One natural choice is to take φ0 = argmaxφ ln g(φ),
so the chain is initialized the posterior mode, and to take the preconditioning matrix M =
(∇2 ln g(φ)|φ=φ0)−1 to be the inverse Hessian at the posterior mode.

5.3 Laplace Approximation

The last two methods we described are instances of Markov chain Monte Carlo, which gen-
erate approximate samples from the target distribution by simulating a carefully constructed
Markov chain. The technique described here instead explicitly approximates a potentially com-
plicated posterior distribution by a Gaussian distribution. Samples from this simpler Gaussian

16

distribution can then serve as approximate samples from the posterior distribution of interest.
[5] proposed using this method to approximate Thompson sampling in a display advertising
problem with a logistic regression model of ad-click-through rates.

Let g denote a probability density function over RK from which we wish to sample. If g is
unimodal, and its log density ln g(φ) is strictly concave around its mode φ, then g(φ) = eln g(φ)
is sharply peaked around φ. It is therefore natural to consider approximating g locally around
its mode. A second-order Taylor approximation to the log-density gives

ln g(φ) ≈ ln g(φ) −

(φ − φ)(cid:62)C(φ − φ),

1
2

where

As an approximation to the density g, we can then use

C = −∇2 ln g(φ).

˜g(φ) ∝ e− 1

2 (φ−φ)(cid:62)C(φ−φ).

This is proportional to the density of a Gaussian distribution with mean φ and covariance C −1,
and hence

˜g(φ) = (cid:112)|C/2π|e− 1

2 (φ−φ)(cid:62)C(φ−φ).

We refer to this as the Laplace approximation of g. Since there are eﬃcient algorithms for
generating normally-distributed samples, this oﬀers a viable means to approximately sampling
from g.

As an example, let us consider application of the Laplace approximation to Example 6.

Bayes rule implies that the posterior density ft−1 of θ satisﬁes

ft−1(θ) ∝ f0(θ)

(cid:32)

t−1
(cid:89)

τ =1

1 + exp (cid:0)(cid:80)

θe − M (cid:1)

1

e∈xτ

(cid:33)yτ (cid:32)

exp (cid:0)(cid:80)
1 + exp (cid:0)(cid:80)

e∈xτ

θe − M (cid:1)

θe − M (cid:1)

e∈xτ

(cid:33)1−yτ

.

The mode θ can be eﬃciently computed via maximizing ft−1, which is log-concave. An approx-
imate posterior sample ˆθ is then drawn from a normal distribution with mean θ and covariance
matrix (−∇2 ln ft−1(θ))−1. To produce the computational results reported in Figure 8, we
applied Newton’s method with a backtracking line search to maximize ln ft−1.

Laplace approximations are well suited for Example 6 because the log-posterior density is
strictly concave and its gradient and Hessian can be computed eﬃciently. Indeed, more broadly,
Laplace approximations tend to be eﬀective for posterior distributions with smooth densities
that are sharply peaked around their mode. They tend to be computationally eﬃcient when
one can eﬃciently compute the posterior mode, and can eﬃciently form the Hessian of the
log-posterior density.

The behavior of the Laplace approximation is not invariant to a substitution of variables,
and it can sometimes be helpful to apply such a substitution. To illustrate this point, let us
revisit the online shortest path problem of Example 5. For this problem, posterior distributions
of θ are log-normal. However, these distributions are normal in φ, where φe = ln θe for each
edge e ∈ E. As such, if the Laplace approximation approach is applied to generate a sample ˆφ
from the posterior distribution of φ, the normal approximation is no longer an approximation,
and, letting ˆθe = exp( ˆφe) for each e ∈ E, we obtain a sample ˆθ exactly from the posterior
distribution of θ. In this case, through a variable substitution, we can sample in a manner that
makes the Laplace approximation exact. More broadly, for any given problem, it may be possible
to introduce variable substitutions that enhance the eﬃcacy of the Laplace approximation.

17

5.4 Bootstrapping

As a an alternative, we discuss an approach based on the statistical bootstrap, which accommo-
dates even very complex densities. There are many versions of the bootstrap approach that can
be used to approximately sample from a posterior distribution. For concreteness, we introduce
a speciﬁc one that is suitable for examples we cover in this paper.

Like the Laplace approximation approach, our bootstrap method assumes that θ is drawn
from a Euclidean space RK. Consider ﬁrst a standard bootstrap method for evaluating the
sampling distribution of the maximum likelihood estimate of θ. The method generates a hypo-
thetical history ˆHt−1 = ((ˆx1, ˆy1), . . . , (ˆxt−1, ˆyt−1)), which is made up of t − 1 action-observation
pairs, each sampled uniformly with replacement from Ht−1. We then maximize the likelihood of
θ under the hypothetical history, which for our shortest path recommendation problem is given
by

ˆLt−1(θ) =

(cid:32)

t−1
(cid:89)

τ =1

1 + exp (cid:0)(cid:80)

θe − M (cid:1)

1

e∈ˆxτ

(cid:33)ˆyτ (cid:32)

exp (cid:0)(cid:80)
1 + exp (cid:0)(cid:80)

e∈ˆxτ

θe − M (cid:1)

θe − M (cid:1)

e∈ˆxτ

(cid:33)1−ˆyτ

.

The randomness in the maximizer of ˆLt−1 reﬂects the randomness in the sampling distribution
of the maximum likelihood estimate. Unfortunately, this method does not take the agent’s prior
into account. A more severe issue is that it grossly underestimates the agent’s real uncertainty
in initial periods. The modiﬁcation described here is intended to overcome these shortcomings
in a simple way.

The method proceeds as follows. First, as before, we draw a hypothetical history ˆHt−1 =
((ˆx1, ˆy1), . . . , (ˆxt−1, ˆyt−1)), which is made up of t − 1 action-observation pairs, each sampled
uniformly with replacement from Ht−1. Next, we draw a sample θ0 from the prior distribution
f0. Let Σ denote the covariance matrix of the prior f0. Finally, we solve the maximization
problem

ˆθ = argmax

e−(θ−θ0)(cid:62)Σ(θ−θ0) ˆLt−1(θ)

θ∈Rk

and treat ˆθ as an approximate posterior sample. This can be viewed as maximizing a randomized
approximation ˆft−1 to the posterior density, where ˆft−1(θ) ∝ e−(θ−θ0)(cid:62)Σ(θ−θ0) ˆLt−1(θ) is what
the posterior density would be if the prior were normal with mean θ0 and covariance matrix
Σ, and the history of observations were ˆHt−1. When very little data has been gathered, the
randomness in the samples mostly stems from the randomness in the prior sample θ0. This
random prior sample encourages the agent to explore in early periods. When t is large, so
a lot of a data has been gathered, the likelihood typically overwhelms the prior sample and
randomness in the samples mostly stems from the random selection of the history ˆHt−1.

In the context of the shortest path recommendation problem, ˆft−1(θ) is log-concave and
can therefore be eﬃciently maximized. Again, to produce our computational results reported
in Figure 8, we applied Newton’s method with a backtracking line search to maximize ln ˆft−1.
Even when it is not possible to eﬃciently maximize ˆft−1, however, the bootstrap approach can
be applied with heuristic optimization methods that identify local or approximate maxima.

5.5 Sanity Checks

Figure 8 demonstrates that Laplace approximation and bootstrap approaches, when applied to
the path recommendation problem, learn from binary feedback to improve performance over
time. This may leave one wondering, however, whether exact Thompson sampling would oﬀer
substantially better performance. Since we do not have a tractable means of carrying out exact
Thompson sampling for this problem, in this section, we apply our approximation methods to
problems for which exact Thompson sampling is tractable. This enables comparisons between
performance of exact and approximate methods.

18

Recall the three-armed beta-Bernoulli bandit problem for which results from application of
greedy and Thompson sampling algorithms were reported in Figure 4(b). For this problem,
components of θ are independent under posterior distributions, and as such, Gibbs sampling
yields exact posterior samples. Hence, the performance of an approximate version that uses
Gibbs sampling would be identical to that of exact Thompson sampling. Figure 9a plots re-
sults from applying Laplace approximation and bootstrap approaches. For this problem, both
approximation methods oﬀer performance that is qualitatively similar to exact Thompson sam-
pling. We do see that Laplace sampling performs marginally worse than Bootstrapping in this
setting.

Next, consider the online shortest path problem with correlated edge delays. Regret ex-
perienced by Thompson sampling applied to such a problem were reported in Figure 7a. As
discussed in Section 5.3, applying the Laplace approximation approach with an appropriate
variable substitution leads to the same results as exact Thompson sampling. Figure 9b com-
pares those results to what is generated by Gibbs sampling and bootstrap approaches. Again,
the approximation methods yield competitive results, although bootstrapping is marginally less
eﬀective than Gibbs sampling.

(a) Bernoulli bandit

(b) online shortest path

Figure 9: Regret of approximation methods versus exact Thompson sampling.

5.6 Incremental Implementation

For each of the three approximation methods we have discussed, the compute time required
per time period grows as time progresses. This is because each past observation must be
accessed to generate the next action. This diﬀers from exact Thompson sampling algorithms we
discussed earlier, which maintain parameters that encode a posterior distribution, and update
these parameters over each time period based only on the most recent observation.

In order to keep the computational burden manageable, it can be important to consider
incremental variants of our approximation methods. We refer to an algorithm as incremental if
it operates with ﬁxed rather than growing per-period compute time. There are many ways to
design incremental variants of approximate posterior sampling algorithms we have presented. As
concrete examples, we consider here particular incremental versions of Laplace approximation
and bootstrap approaches.

For each time t, let (cid:96)t(θ) denote the likelihood of yt conditioned on xt and θ. Hence,

conditioned on Ht−1, the posterior density satisﬁes

ft−1(θ) ∝ f0(θ)

(cid:96)τ (θ).

t−1
(cid:89)

τ =1

19

Let g0(θ) = ln f0(θ) and gt(θ) = ln (cid:96)t(θ) for t > 0. To identify the mode of ft−1, it suﬃces to
maximize (cid:80)t−1

τ =0 gτ (θ).

Consider an incremental version of the Laplace approximation. The algorithm maintains
statistics Ht, and θt, initialized with θ0 = argmaxθ g0(θ), and H0 = ∇2g0(θ0), and updating
according to

Ht = Ht−1 + ∇2gt(θt−1),
θt = θt−1 − H −1

t ∇gt(θt−1).

This algorithm is a type of online newton method for computing the posterior mode θt−1 that
maximizes (cid:80)t−1
τ =0 gτ (θ). Note that if each function gt−1 is strictly concave and quadratic, as
would be the case if the prior is normal and observations are linear in θ and perturbed only
by normal noise, each pair θt−1 and H −1
t−1 represents the mean and covariance matrix of ft−1.
More broadly, these iterates can be viewed as the mean and covariance matrix of a Gaus-
sian approximation to the posterior, and used to generate an approximate posterior sample
ˆθ ∼ N (θt−1, H −1
It worth noting that for linear and generalized linear models, the ma-
trix ∇2gt(θt−1) has rank one, and therefore H −1
t = (Ht−1 + ∇2gt(θt−1))−1 can be updated
incrementally using the Sherman-Woodbury-Morrison formula. This incremental version of the
Laplace approximation is closely related to the notion of an extended Kalman ﬁlter, which has
been explored in greater depth by G´omez-Uribe [26] as a means for incremental approximate
Thompson sampling with exponential families of distributions.

t−1).

Another approach involves incrementally updating each of an ensemble of models to behave
like a sample from the posterior distribution. The posterior can be interpreted as a distribution
of “statistically plausible” models, by which we mean models that are suﬃciently consistent
with prior beliefs and the history of observations. With this interpretation in mind, Thompson
sampling can be thought of as randomly drawing from the range of statistically plausible mod-
els. Ensemble sampling aims to maintain, incrementally update, and sample from a ﬁnite set
of such models. In the spirit of particle ﬁltering, this set of models approximates the posterior
distribution. The workings of ensemble sampling are in some ways more intricate than conven-
tional uses of particle ﬁltering, however, because interactions between the ensemble of models
and selected actions can skew the distribution. Ensemble sampling is presented in more depth
in [27], which draws inspiration from work on exploration in deep reinforcement learning [15].

There are multiple ways of generating suitable model ensembles. One builds on the afore-
mentioned bootstrap method and involves ﬁtting each model to a diﬀerent bootstrap sample.
: n = 1, . . . , N ). Each
To elaborate, consider maintaining N models with parameters (θ
set is initialized with θ

0 = 0, and updated according to

n
0 ∼ g0, H n

0 = ∇g0(θ

n
0 ), dn

n
t , H n
t

H n

t = H n

t−1 + zn

t ∇2gt(θ

n
t−1),

n
t = θ

n
t−1 − zn

θ

t (H n

t )−1∇gt(θ

n
t−1),

n
0 and the random weight zn

t placed on each observation. The variable, zn

is an independent Poisson-distributed sample with mean one. Each θ

n
where each zn
t can be
t
viewed as a random statistically plausible model, with randomness stemming from the initializa-
τ can loosely be
tion of θ
interpreted as a number of replicas of the data sample (xτ , yτ ) placed in a hypothetical history
ˆHn
t . Indeed, in a data set of size t, the number of replicas of a particular bootstrap data sample
follows a Binomial(t, 1/t) distribution, which is approximately Poisson(1) when t is large. With
t , distinguished by the random num-
this view, each θ
ber of replicas assigned to each data sample. To generate an action xt, n is sampled uniformly
n
from {1, . . . , N }, and the action is chosen to maximize E[rt|θ = θ
t−1 serves as the
approximate posterior sample. Note that the per-period compute time grows with N , which is
an algorithm tuning parameter.

t is eﬀectively ﬁt to a diﬀerent data set ˆHn

n
t−1]. Here, θ

n

20

This bootstrap approach oﬀers one mechanism for incrementally updating an ensemble of
models. In Section 7.3, we will discuss another along with its application to active learning in
neural networks.

6 Practical Modeling Considerations

Our narrative over previous sections has centered around a somewhat idealized view of Thomp-
son sampling, which ignored the process of prior speciﬁcation and assumed a simple model in
which the system and set of feasible actions is constant over time and there is no side informa-
tion on decision context. In this section, we provide greater perspective on the process of prior
speciﬁcation and on extensions of Thompson sampling that serve practical needs arising in some
applications.

6.1 Prior Distribution Speciﬁcation

The algorithms we have presented require as input a prior distribution over model parameters.
The choice of prior can be important, so let us now discuss its role and how it might be selected.
In designing an algorithm for an online decision problem, unless the value of θ were known with
certainty, it would not make sense to optimize performance for a single value, because that could
lead to poor performance for other plausible values. Instead, one might design the algorithm
to perform well on average across a collection of possibilities. The prior can be thought of as a
distribution over plausible values, and its choice directs the algorithm to perform well on average
over random samples from the prior.

For a practical example of prior selection, let us revisit the banner ad placement problem
introduced in Example 1. There are K banner ads for a single product, with unknown click-
through probabilities (θ1, . . . , θK). Given a prior, Thompson sampling can learn to select the
most successful ad. We could use a uniform or, equivalently, a beta(1, 1) distribution over each
θk. However, if some values of θk are more likely than others, using a uniform prior sacriﬁces
performance. In particular, this prior represents no understanding of the context, ignoring any
useful knowledge from past experience. Taking knowledge into account reduces what must be
learned and therefore reduces the time it takes for Thompson sampling to identify the most
eﬀective ads.

Suppose we have a data set collected from experience with previous products and their ads,
each distinguished by stylistic features such as language, font, and background, together with
accurate estimates of click-through probabilities. Let us consider an empirical approach to prior
selection that leverages this data. First, partition past ads into K sets, with each kth partition
consisting of those with stylistic features most similar to the kth ad under current consideration.
Figure 10 plots a hypothetical empirical cumulative distribution of click-through probabilities
for ads in the kth set. It is then natural to consider as a prior a smoothed approximation of
this distribution, such as the beta(1, 100) distribution also plotted in Figure 10. Intuitively, this
process assumes that click-through probabilities of past ads in set k represent plausible values
of θk. The resulting prior is informative; among other things, it virtually rules out click-through
probabilities greater than 0.05.

A careful choice of prior can improve learning performance. Figure 11 presents results from
simulations of a three-armed Bernoulli bandit. Mean rewards of the three actions are sampled
from beta(1, 50), beta(1, 100), and beta(1, 200) distributions, respectively. Thompson sampling
is applied with these as prior distributions and with a uniform prior distribution. We refer to the
latter as a misspeciﬁed prior because it is not consistent with our understanding of the problem.
A prior that is consistent in this sense is termed coherent. Each plot represents an average
over ten thousand independent simulations, each with independently sampled mean rewards.

21

Figure 10: An empirical cumulative distribution and an approximating beta distribution.

Figure 11a plots expected regret, demonstrating that the misspeciﬁed prior increases regret.
Figure 11a plots the evolution of the agent’s mean reward conditional expectations. For each
algorithm, there are three curves corresponding to the best, second-best, and worst actions, and
they illustrate how starting with a misspeciﬁed prior delays learning.

(a) regret

(b) expected mean rewards

Figure 11: Comparison of Thompson sampling for the Bernoulli bandit problem with coherent
versus misspeciﬁed priors.

6.2 Constraints, Context, and Caution

Though Algorithm 4, as we have presented it, treats a very general model, straightforward ex-
tensions accommodate even broader scope. One involves imposing time-varying constraints
on the actions. In particular, there could be a sequence of admissible action sets Xt that con-
strain actions xt. To motivate such an extension, consider our shortest path example. Here, on
any given day, the drive to work may be constrained by announced road closures. If Xt does

22

not depend on θ except through possible dependence on the history of observations, Thompson
sampling (Algorithm 4) remains an eﬀective approach, with the only required modiﬁcation being
to constrain the maximization problem in Line 6.

Another extension of practical import addresses contextual online decision problems.
In such problems, the response yt to action xt also depends on an independent random variable
zt that the agent observes prior to making her decision.
In such a setting, the conditional
distribution of yt takes the form pθ(·|xt, zt). To motivate this, consider again the shortest path
example, but with the agent observing a weather report zt from a news channel before selecting
a path xt. Weather may aﬀect delays along diﬀerent edges diﬀerently, and the agent can take
this into account before initiating her trip. Contextual problems of this ﬂavor can be addressed
through augmenting the action space and introducing time-varying constraint sets. In particular,
if we view ˜xt = (xt, zt) as the action and constrain its choice to Xt = {(x, zt) : x ∈ X }, where X
is the set from which xt must be chosen, then it is straightforward to apply Thompson sampling
to select actions ˜x1, ˜x2, . . .. For the shortest path problem, this can be interpreted as allowing
the agent to dictate both the weather report and the path to traverse, but constraining the
agent to provide a weather report identical to the one observed through the news channel.

In some applications, it may be important to ensure that expected performance exceeds some
prescribed baseline. This can be viewed as a level of caution against poor performance. For
example, we might want each action applied to oﬀer expected reward of at least some level r.
This can again be accomplished through constraining actions: in each tth time period, let the
action set be Xt = {x ∈ X : E[rt|xt = x] ≥ r}. Using such an action set ensures that expected
average reward exceeds r. When actions are related, an actions that is initially omitted from the
set can later be included if what is learned through experiments with similar actions increases
the agent’s expectation of reward from the initially omitted action.

6.3 Nonstationary Systems

Problems we have considered involve model parameters θ that are constant over time. As
Thompson sampling hones in on an optimal action, the frequency of exploratory actions con-
verges to zero. In many practical applications, the agent faces a nonstationary system, which
is more appropriately modeled by time-varying parameters θ1, θ2, . . ., such that the response
yt to action xt is generated according to pθt(·|xt).
In such contexts, the agent should never
stop exploring, since it needs to track changes as the system drifts. With minor modiﬁcation,
Thompson sampling remains an eﬀective approach so long as model parameters change little
over durations that are suﬃcient to identify eﬀective actions.

In principle, Thompson sampling could be applied to a broad range of problems where
the parameters θ1, θ2, ..., evolve according to some stochastic process p(θt|θ1, ..., θt−1) by using
techniques from ﬁltering and sequential Monte Carlo to generate posterior samples. Instead we
describe below some much simpler approaches to such problems.

One simple approach to addressing nonstationarity involves ignoring historical observations
made beyond some number τ of time periods in the past. With such an approach, at each time t,
the agent would produce a posterior distribution based on the prior and conditioned only on the
most recent τ actions and observations. Model parameters are sampled from this distribution,
and an action is selected to optimize the associated model. The agent never ceases to explore,
since the degree to which the posterior distribution can concentrate is limited by the number of
observations taken into account.

An alternative approach involves modeling evolution of a belief distribution in a manner that
discounts the relevance of past observations and tracks a time-varying parameters θt. We now
consider such a model and a suitable modiﬁcation of Thompson sampling. Let us start with the
simple context of a Bernoulli bandit. Take the prior for each kth mean reward to be beta(α, β).
Let the algorithm update parameters to identify the belief distribution of θt conditioned on the

23

history Ht−1 = ((x1, y1), . . . , (xt−1, yt−1)) according to

(6.1)

(αk, βk) ←

(cid:26) (cid:0)(1 − γ)αk + γα, (1 − γ)βk + γβ(cid:1)

(cid:0)(1 − γ)αk + γα + rt, (1 − γ)βk + γβ + 1 − rt

(cid:1)

xt (cid:54)= k
xt = k,

where γ ∈ [0, 1] and αk, βk > 0. This models a process for which the belief distribution converges
to beta(αk, βk) in the absence of observations. Note that, in the absence of observations, if γ > 0
then (αk, βk) converges to (αk, βk).
Intuitively, the process can be thought of as randomly
perturbing model parameters in each time period, injecting uncertainty. The parameter γ
controls how quickly uncertainty is injected. At one extreme, when γ = 0, no uncertainty is
injected. At the other extreme, γ = 1 and each θt,k is an independent beta(αk, βk)-distributed
process. A modiﬁed version of Algorithm 2 can be applied to this nonstationary Bernoulli bandit
problem, the diﬀerences being in the additional arguments γ, α, and β , and the formula used
to update distribution parameters.

The more general form of Thompson sampling presented in Algorithm 4 can be modiﬁed in
an analogous manner. For concreteness, let us focus on the case where θ is restricted to a ﬁnite
set; it is straightforward to extend things to inﬁnite sets. The conditional distribution update
in Algorithm 4 can be written as

To model nonstationary model parameters, we can use the following alternative:

p(u) ←

p(u)qu(yt|xt)
v p(v)qv(yt|xt)

.

(cid:80)

p(u) ←

pγ(u)p1−γ(u)qu(yt|xt)
v pγ(v)p1−γ(v)qv(yt|xt)

.

(cid:80)

This generalizes the formula provided earlier for the Bernoulli bandit case. Again, γ controls
the rate at which uncertainty is injected. The modiﬁed version of Algorithm 2, which we refer
to as nonstationary Thompson sampling, takes γ and p as additional arguments and replaces
the distribution update formula.

Figure 12 illustrates potential beneﬁts of nonstationary Thompson sampling when dealing
with a nonstationairy Bernoulli bandit problem. In these simulations, belief distributions evolve
according to Equation (6.1). The prior and stationary distributions are speciﬁed by α = α =
β = β = 1. The decay rate is γ = 0.01. Each plotted point represents an average over
10,000 independent simulations. Regret here is deﬁned by regrett(θt) = maxk θt,k − θt,xt. While
nonstationary Thompson sampling updates its belief distribution in a manner consistent with
the underlying system, Thompson sampling pretends that the success probabilities are constant
over time and updates its beliefs accordingly. As the system drifts over time, Thompson sampling
becomes less eﬀective, while nonstationary Thompson sampling retains reasonable performance.
Note, however, that due to nonstationarity, no algorithm can promise regret that vanishes with
time.

7 Further Examples

As contexts for illustrating the workings of Thompson sampling, we have presented the Bernoulli
bandit and variations of the online shortest path problem. To more broadly illustrate the scope
of Thompson sampling and issues that arise in various kinds of applications, we present several
additional examples in this section.

24

Figure 12: Comparison of Thompson sampling versus nonstationary Thompson sampling with a
nonstationary Bernoulli bandit problem.

7.1 Product Assortment

Let us start with an assortment planning problem. Consider an agent who has an ample supply
of each of n diﬀerent products, indexed by i = 1, 2, · · · , n. The seller collects a proﬁt of pi per
unit sold of product type i. In each period, the agent has the option of oﬀering a subset of
the products for sale. Products may be substitutes or complements, and therefore the demand
for a product may be inﬂuenced by the other products oﬀered for sale in the same period. In
order to maximize her proﬁt, the agent needs to carefully select the optimal set of products to
oﬀer in each period. We can represent the agent’s decision variable in each period as a vector
x ∈ {0, 1}n where xi = 1 indicates that product i is oﬀered and xi = 0 indicates that it is not.
Upon oﬀering an assortment containing product i in some period, the agent observes a random
lognormally distributed demand di. The mean of this lognormal distribution depends on the
entire assortment x and an uncertain matrix θ ∈ Rk×k. In particular

where σ2 is a known parameter that governs the level of idiosyncratic randomness in realized
demand across periods. For any product i contained in the assortment x,

log(di) | θ, x ∼ N (cid:0)(θx)i, σ2(cid:1)

(θx)i = θii +

xjθij,

(cid:88)

j(cid:54)=i

where θii captures the demand rate for item i if it were the sole product oﬀered and each θij
captures the eﬀect availability of product j has on demand for product i. When an assortment
x is oﬀered, the agent earns expected proﬁt

(7.1)

(cid:34) n
(cid:88)

E

i=1

pixidi | θ, x

=

pixie(θx)i+ σ2
2 .

(cid:35)

n
(cid:88)

i=1

If θ were known, the agent would always select the assortment x that maximizes her expected
proﬁt in (7.1). However, when θ is unknown, the agent needs to learn to maximize proﬁt by
exploring diﬀerent assortments and observing the realized demands.

25

Thompson sampling can be adopted as a computationally eﬃcient solution to this problem.
We assume the agent begins with a multivariate Gaussian prior over θ. Due to conjugacy
properties of normal and lognormal distributions, the posterior distribution of θ remains normal
after any number of periods. At the beginning of each t’th period, the Thompson sampling
algorithm draws a sample ˆθt from this normal posterior distribution. Then, the agent selects an
assortment that would maximize her expected proﬁt in period t if the sampled ˆθt were indeed
the true parameter.

As in Examples 4 and 5, the mean and covariance matrix of the posterior distribution of θ
can be updated in closed form. However, because θ is a matrix rather than a vector, the explicit
form of the update is more complicated. To describe the update rule, we ﬁrst introduce ¯θ as the
vectorized version of θ which is generated by stacking the columns of θ on top of each other.
Let x be the assortment selected in a period, i1, i2, · · · , ik denote the the products included in
this assortment (i.e., supp(x) = {i1, i2, · · · , ik}) and z ∈ Rk be deﬁned element-wise as

zj = ln dij ,

j = 1, 2, · · · , k.

Let S be a k × n “selection matrix” where Sj,ij = 1 for j = 1, 2, · · · , k and all its other elements
are 0. Also, deﬁne

W = x(cid:62) ⊗ S,

where ⊗ denotes the Kronecker product of matrices. At the end of current period, the posterior
mean µ and covariance matrix Σ of ¯θ are updated according to the following rules:

(cid:18)

µ ←

Σ−1 +

1
σ2 W (cid:62)W

(cid:19)−1 (cid:18)

Σ−1µ +

1
σ2 W (cid:62)z

(cid:19)

,

(cid:18)

Σ ←

Σ−1 +

1
σ2 W (cid:62)W

(cid:19)−1

.

To investigate the performance of Thompson sampling in this problem, we simulated a
scenario with n = 6 and σ2 = 0.04. We take the proﬁt associated to each product i to be
pi = 1/6. As the prior distribution, we assumed that each element of θ is independently
normally distributed with mean 0, the diagonal elements have a variance of 1, and the oﬀ-
diagonal elements have a variance of 0.2. To understand this choice, recall the impact of diagonal
and oﬀ-diagonal elements of θ. The diagonal element θii controls the mean demand when only
product i is available, and reﬂects the inherent quality or popularity of that item. The oﬀ-
diagonal element θij captures the inﬂuence availability of product j has on mean demand for
product i. Our choice of prior covariance encodes that the dominant eﬀect on demand of
a product is likely its own characteristics, rather than its interaction with any single other
product. Figure 13 presents the performance of diﬀerent learning algorithms in this problem.
In addition to Thompson sampling, we have simulated the greedy and (cid:15)-greedy algorithms for
various values of (cid:15). We found that (cid:15) = 0.07 provides the best performance for (cid:15)-greedy in this
problem.

As illustrated by this ﬁgure, the greedy algorithm performs poorly in this problem while
(cid:15)-greedy presents a much better performance. We found that the performance of (cid:15)-greedy can
be improved by using an annealing (cid:15) of m
m+t at each period t. Our simulations suggest using
m = 9 for the best performance in this problem. Figure 13 shows that Thompson sampling
outperforms both variations of (cid:15)–greedy in this problem.

7.2 Cascading Recommendations

We consider an online recommendation problem in which an agent learns to recommend a
desirable list of items to a user. As a concrete example, the agent could be a search engine
and the items could be web pages. We consider formulating this problem as a cascading bandit,
in which user selections are governed by a cascade model, as is commonly used in the ﬁelds of
information retrieval and online advertising [28].

26

Figure 13: Regret experienced by diﬀerent learning algorithms applied to product assortment
problem.

A cascading bandit is represented by a tuple (E, J, θ), where E = {1, . . . , K} is the item
set, J ≤ K is the number of recommended items in each period, and the model parameters
θ ∈ [0, 1]K encode the item attraction probabilities. The agent knows the item set E and the
display size J, but must learn the user preferences encoded in θ through sequential interactions.
We study the agent’s performance over T interactions.

During each time t, the agent chooses an ordered list xt = (xt,1, . . . , xt,J ) of J distinct items
from E based on its prior information and past observations. The list is then presented to
the user, who either clicks on a single item from the list, or departs without clicking. The
user’s choices depend on the item attraction probabilities, their ordering in the vector xt, and
idiosyncratic randomness associated with the current time period. In particular, the user’s choice
process can be described as follows. Let wt ∈ {0, 1}K be a random binary vector generated by
independently sampling each kth element wt,k from the Bernoulli distribution Bern(θk), where
θk is the kth element of θ. Item k is attractive for the user at time t if and only if wt,k = 1.
Following the cascade model, the user views items in the recommended list xt individually in
order, stopping at and clicking the ﬁrst that is attractive. Speciﬁcally, the jth item in xt, is
viewed:

• If xt,j is attractive at time t, then the user clicks xt,j and leaves.
• If xt,j is not attractive at time t and j < J, then the user continues to examine xt,j+1.
• If xt,j is not attractive at time t and j = J, then the user leaves without a click.

Note that item xt,1 is always examined, and there is at most one click at time t. Let yt =
argmin{1 ≤ j ≤ J : wt,xt,j = 1} denote the user’s choice, with the the convention that
argmin ∅ = ∞. Thus, yt = ∞ when the user leaves without clicking.

The agent observes the user’s choice yt and associates this with a reward rt = r(yt) =
1{yt ≤ J} indicating whether the user clicked on some item. Note that based on yt, the agent
can update its estimates of attraction probabilities of all items a1
examined by
the user. In particular, upon seeing the click yt < ∞ the agent infers that item was attractive
and each item presented before yt was unattractive. When no click was observed, the agent
infers that every item in xt was unattractive to the user at time t.

t , . . . , amin{yt,J}

t

27

For each ordered lists x = (x1, . . . , xJ ) and θ(cid:48) ∈ [0, 1]K, let
(cid:105)

h(x, θ(cid:48)) = 1 − (cid:81)J

(cid:104)
1 − θ(cid:48)
xj

,

j=1

is the xj-th element of θ(cid:48). Note that rt = h(xt, wt), and the expected reward at time t
where θ(cid:48)
xj
is E [rt|xt, θ] = h(xt, θ). The optimal solution x∗ ∈ argmaxx: |x|=J h(x, θ) consists of the J items
with highest attraction probabilities. The per-period regret of the cascading bandit is deﬁned
as

regrett(θ) = h(x∗, θ) − h(xt, θ).

Algorithm 5 CascadeUCB
1: Initialize αk and βk ∀k ∈ E
2: for t = 1, 2, . . . do
3:

#compute itemwise UCBs:
for k = 1, . . . , K do

4:

Compute UCB Ut(k)

end for

Algorithm 6 CascadeTS
1: Initialize αk and βk ∀k ∈ E
2: for t = 1, 2, . . . do
3:

#sample model:
for k = 1, . . . , K do

4:

Sample ˆθk ∼ Beta(αk, βk)

end for

#select and apply action:
xt ← argmaxx:|x|=J h(x, Ut)
Apply xt and observe yt and rt

#select and apply action:
xt ← argmaxx:|x|=J h(x, ˆθ)
Apply xt and observe yt and rt

#update suﬃcient statistics:
for j = 1, . . . , min{yt, J} do
αxt,j ← αxt,j + 1(j = yt)
βxt,j ← βxt,j + 1(j < yt)

#update posterior:
for j = 1, . . . , min{yt, J} do
αxt,j ← αxt,j + 1(j = yt)
βxt,j ← βxt,j + 1(j < yt)

end for

16:
17: end for

end for

16:
17: end for

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

Kveton et al.

[29] proposed learning algorithms for cascading bandits based on itemwise
UCB estimates. CascadeUCB (Algorithm 5) is practical variant that allows for speciﬁcation
of prior parameters (α, β) that guide early behavior of the algorithm. CascadeUCB computes
a UCB Ut(k) for each item k ∈ E and then chooses the a list that maximizes h(·, Ut), which
represents an upper conﬁdence bound on the list attraction probability. The list xt can be
eﬃciently generated by choosing the J items with highest UCBs. Upon observing the user’s
response, the algorithm updates the suﬃcient statistics (α, β), which count clicks and views.

CascadeTS (Algorithm 6) is a Thompson sampling algorithm for cascading bandits. Cas-
cadeTS operates in a manner similar to CascadeUCB except that xt is computed based on the
sampled attraction probabilities ˆθ, rather than the itemwise UCBs Ut.

A standard form of UCB, known as UCB1, which is considered and analyzed in the context

of cascading bandits in [29], takes the form

Ut(k) =

αk
αk + βk

+

1.5 log(t)
αk + βk

,

∀k ∈ E.

Note that αk/(αk + βk) represents the expected value of the click probability θk, while the
second term represents an optimistic boost that encourages exploration. As the observations
accumulate, the denominator

αk + βk grows, reducing the degree of optimism.

√

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

(cid:115)

28

Figure 14 presents results from applying CascadeTS and CascadeUCB based on UCB1.
These results are generated by randomly sampling 1000 cascading bandit instances, K =
1000 and J = 100, in each case sampling each attraction probability θk independently from
Beta(1, 40). For each instance, CascadeUCB and CascadeTS are applied over 20000 time pe-
riods, initialized with (αk, βk) = (1, 40). The plots are of per-period regrets averaged over the
1000 simulations.

Figure 14: Comparison of CascadeTS and CascadeUCB with K = 1000 items and J = 100 recom-
mendations per period.

The results demonstrate that TS far outperforms this version of CascadeUCB. Why? An
obvious reason is that h(x, Ut) is far too optimistic.
In particular, h(x, Ut) represents the
probability of a click if every item in x simultaneously takes on the largest attraction probability
that is statistically plausible. However, the wt,k’s are statistically independent across items and
the agent is unlikely to have substantially under-estimated the attraction probability of every
item in x. As such, h(x, Ut) tends to be far too optimistic. CascadeTS, on the other hand,
samples ˆθk’s independently across items. While any sample ˆθk might deviate substantially from
its mean, it is unlikely that the sampled attraction probability of every item in x greatly exceeds
its mean. As such, the variability in h(x, ˆθ) provides a much more accurate reﬂection of the
magnitude of uncertainty.

One can address excess optimism by tuning the degree of optimism associated with UCB1.

In particular, consider a UCB of the form

Ut(k) =

αk
αk + βk

+ c

1.5 log(t)
αk + βk

,

∀k ∈ E,

(cid:115)

with the parameter c, which indicates the degree of optimism, selected through simulations to
optimize performance. The plot labeled “UCB-tuned” in Figure 14 illustrates performance with
c = 0.05, which approximately minimizes cumulative regret over 20,000 time periods. Table 1
provides the cumulative regret averages for each algorithm and range of degrees of optimism
c, along with one standard deviation conﬁdence interval boundaries. It is interesting that even
after being tuned to the speciﬁc problem and horizon, the performance of CascadeUCB falls
short of Cascade TS. A likely source of loss stems from the shape of conﬁdence sets used by
CascadeUCB. Note that the algorithm uses hyper-rectangular conﬁdence sets, since the set
of statistically plausible attraction probability vectors is characterized by a Cartesian prod-
uct item-level conﬁdence intervals. However, the Bayesian central limit theorem suggests that

29

“ellipsoidal” conﬁdence sets oﬀer a more suitable choice. Speciﬁcally, as data is gathered the
posterior distribution over θ can be well approximated by a multivariate Gaussian, for which
level sets are ellipsoidal. Losses due to the use of hyper-rectangular conﬁdence sets have been
studied through regret analysis in [30] and through simple analytic examples in [31].

Table 1: Comparison of CascadeTS and CascadeUCB with K = 1000 items and J = 100 recom-
mendations per period over a range of optimism parameters.

algorithm

TS

UCB

degree of
optimism
NA
0 (greedy)
0.0001
0.001
0.005
0.01
0.05
0.1
0.2
0.3
0.5
0.75
1 (UCB1)

cumulative
regret
71.548 ± 0.395
135.410 ± 0.820
134.768 ± 0.807
133.906 ± 0.803
131.737 ± 0.756
127.454 ± 0.726
115.345 ± 0.605
125.351 ± 0.636
200.221 ± 1.069
324.586 ± 1.644
615.070 ± 2.705
954.670 ± 3.647
1222.757 ± 4.032

It is worth noting that tuned versions of CascadeUCB do sometimes perform as well or better
than CascadeTS. Table 2 and Figure 15 illustrates an example of this. The setting is identical to
that used to generate the results of 14, except that K = 50 and J = 10, and cumulative regret is
approximately optimized with c = 0.1. CascadeUCB outperforms CascadeTS. This qualitative
diﬀerence from the case of K = 1000 and J = 100 is likely due to the fact that hyper-rectangular
sets oﬀer poorer approximations of ellipsoids as the dimension increases. This phenomenon and
its impact on regret aligns with theoretical results of [30]. That said, CascadeUCB is somewhat
advantaged in this comparison because it is tuned speciﬁcally for the setting and time horizon.
Results presented so far initialize the algorithms with coherent priors. To illustrate both
robustness and potentially complex behaviors of these algorithms, we present in Table 3 and
Figure 16 results generated when the algorithms are initialized with uniform priors (αk = βk =
1). Aside from these priors, the setting is identical to that used to generate Table 1 and Figure
14. CascadeTS far outperforms the nominal version of CascadeUCB and does not fare too
much worse than CascadeTS initialized with a coherent prior. A puzzling fact, however, is
that the tuned version of CascadeUCB is optimized by c = 0, which corresponds to the greedy
algorithm.
It could be that the greedy algorithm performs well here because the particular
choice of misspeciﬁed prior induces a suitable kind of optimistic behavior. More broadly, this
illustrates how ad hoc tuning can substantially improve the behavior of an algorithm in any
given simulated context. However, Thompson sampling appears to operate robustly across
a broad range of problems even when applied in a principled manner that does not require
context-speciﬁc tuning.

30

Table 2: Comparison of CascadeTS and CascadeUCB with K = 50 items and J = 10 recommen-
dations per period over a range of optimism parameters.

algorithm

TS

UCB

degree of
optimism
NA
0 (greedy)
0.0001
0.001
0.005
0.01
0.05
0.1
0.2
0.3
0.5
0.75
1 (UCB1)

cumulative
regret
183.391 ± 1.050
333.650 ± 4.714
325.448 ± 4.618
325.007 ± 4.648
309.303 ± 4.408
297.496 ± 4.257
205.882 ± 2.861
159.626 ± 1.695
190.457 ± 1.099
260.022 ± 1.184
406.080 ± 1.630
570.547 ± 2.189
711.117 ± 2.627

Figure 15: Comparison of CascadeTS and CascadeUCB with K = 50 items and J = 10 recommen-
dations per period.

31

Table 3: Comparison of CascadeTS and CascadeUCB, both initialized with uniform priors, with
K = 1000 items and J = 100 recommendations per period over a range of optimism parameters.

algorithm

TS

UCB

degree of
optimism
NA
0 (greedy)
0.0001
0.001
0.005
0.01
0.05
0.1
0.2
0.3
0.5
0.75
1 (UCB1)

cumulative
regret
82.458 ± 0.449
101.556 ± 0.477
104.910 ± 0.474
104.572 ± 0.459
105.528 ± 0.474
108.136 ± 0.483
131.654 ± 0.559
172.275 ± 0.709
283.607 ± 1.149
417.461 ± 1.629
702.015 ± 2.583
1024.434 ± 3.390
1277.005 ± 3.812

Figure 16: Comparison of CascadeTS and CascadeUCB, both initialized with uniform priors, with
K = 1000 items and J = 100 recommendations per period.

32

7.3 Active Learning in Neural Networks

Neural networks are widely used in supervised learning, where given an existing set of predictor-
response data pairs, the objective is to produce a model that generalizes to accurately predict
future responses conditioned on associated predictors. They are also increasingly being used to
guide actions ranging from recommendations to robotic maneuvers. Active learning is called
for to close the loop by generating actions that do not solely maximize immediate performance
but also probe the environment to generate data that accelerates learning. Thompson sampling
oﬀers a useful principle upon which such active learning algorithms can be developed.

With neural networks or other complex model classes, computing the posterior distribution
over models becomes intractable. Approximations are called for, and incremental updating is
essential because ﬁtting a neural network is a computationally intensive task in its own right. In
such contexts, ensemble sampling oﬀers a viable approach [27]. In Section 5.6, we introduced a
particular mechanism for ensemble sampling based on the bootstrap. In this section, we consider
an alternative version of ensemble sampling and present results from [27] that demonstrate its
application to active learning in neural networks.

To motivate our algorithm, let us begin by discussing how it can be applied to the linear

bandit problem.

Example 7

(Linear Bandit) Let θ be drawn from RM and distributed according
to a N (µ0, Σ0) prior. There is a set of K actions X ⊆ RM . At each time t = 1, . . . , T ,
an action xt ∈ X is selected, after which a reward rt = yt = θ(cid:62)xt + wt is observed,
where wt ∼ N (0, σ2

w).

In this context, ensemble sampling is unwarranted, since exact Bayesian inference can be carried
out eﬃciently via Kalman ﬁltering. Nevertheless, the linear bandit oﬀers a simple setting for
explaining the workings of an ensemble sampling algorithm.

Consider maintaining a covariance matrix updated according to

Σt+1 = (cid:0)Σ−1

t + xtx(cid:62)

t /σ2
w

(cid:1)−1

,

1
t , . . . , θ
and N models θ
and updated incrementally according to

N
t , initialized with θ

1
1, . . . , θ

N
1 each drawn independently from N (µ0, Σ0)

n
t+1 = Σt+1

θ

Σ−1
t θ

n
t + xt(yt + ˜wn

t )/σ2
w

(cid:16)

(cid:17)

,

for n = 1, . . . , N , where ( ˜wn
t
samples drawn by the updating algorithm.
vectors satisfy

: t = 1, . . . , T, n = 1, . . . , N ) are independent N (0, σ2

w) random
It is easy to show that the resulting parameter

n
t = arg min

θ

ν

(cid:32)

1
σ2
w

t−1
(cid:88)

τ =1

(yτ + ˜wn

τ − x(cid:62)

τ ν)2 + (ν − θ

n
1 )(cid:62)Σ−1

0 (ν − θ

n
1 )

.

(cid:33)

n
t is a model ﬁt to a randomly perturbed prior
Thich admits an intuitive interpretation: each θ
and randomly perturbed observations. As established in [27], for any deterministic sequence
N
x1, . . . , xt−1, conditioned on the history, the models θ
t are independent and identically
distributed according to the posterior distribution of θ. In this sense, the ensemble approximates
the posterior.

1
t , . . . , θ

The ensemble sampling algorithm we have described for the linear bandit problem motivates

an analogous approach for the following neural network model.

Example 8

(Neural Network) Let gθ : RM (cid:55)→ RK denote a mapping induced
by a neural network with weights θ. Suppose there are K actions X ⊆ RM , which

33

serve as inputs to the neural network, and the goal is to select inputs that yield
desirable outputs. At each time t = 1, . . . , T , an action xt ∈ X is selected, after
which yt = gθ(xt) + wt is observed, where wt ∼ N (0, σ2
wI). A reward rt = r(yt)
is associated with each observation. Let θ be distributed according to a N (µ0, Σ0)
prior. The idea here is that data pairs (xt, yt) can be used to ﬁt a neural network
model, while actions are selected to trade oﬀ between generating data pairs that
reduce uncertainty in neural network weights and those that oﬀer desirable immediate
outcomes.

Consider an ensemble sampling algorithm that once again begins with N independent models
N
1 sampled from a N (µ0, Σ0) prior. It could be natural here
with connection weights θ
to let µ0 = 0 and Σ0 = σ2
0 chosen so that the range of probable models
spans plausible outcomes. To incrementally update parameters, at each time t, each nth model
applies some number of stochastic gradient descent iterations to reduce a loss function of the
form

1
1, . . . , θ
0I for some variance σ2

Lt(ν) =

(yτ + ˜wn

τ − gν(xτ ))2 + (ν − θ

n
1 )(cid:62)Σ−1

0 (ν − θ

n
1 ).

1
σ2
w

t−1
(cid:88)

τ =1

Figure 17 present results from simulations involving a two-layer neural network, with a set
of K actions, X ⊆ RM . The weights of the neural network, which we denote by w1 ∈ RD×N and
w2 ∈ RD, are each drawn from N (0, λ). Let θ ≡ (w1, w2). The mean reward of an action x ∈ X
is given by gθ(x) = w(cid:62)
2 max(0, w1a). At each time step, we select an action xt ∈ X and observe
reward yt = gθ(xt) + zt, where zt ∼ N (0, σ2
z ). We used M = 100 for the input dimension,
D = 50 for the dimension of the hidden layer, number of actions K = 100, prior variance λ = 1,
and noise variance σ2
z = 100. Each component of each action vector is sampled uniformly from
[−1, 1], except for the last component, which is set to 1 to model a constant oﬀset. All results
are averaged over 100 realizations.

In our application of the ensemble sampling algorithm we have described, to facilitate gra-
dient ﬂow, we use leaky rectiﬁed linear units of the form max(0.01x, x) during training, though
the target neural network is made up of regular rectiﬁed linear units as indicated above. In our
simulations, each update was carried out with 5 stochastic gradient steps, with a learning rate
of 10−3 and a minibatch size of 64.

Figure 17 illustrates the performance of several learning algorithms with an underlying neural
network. Figure 17a demonstrates the performance of an (cid:15)-greedy strategy across various levels
of (cid:15). We ﬁnd that we are able to improve performance with an annealing schedule (cid:15) = k
k+t
(Figure 17b). However, we ﬁnd that an ensemble sampling strategy outperforms even the best
tuned (cid:15)-schedules (Figure 17c). Further, we see that ensemble sampling strategy can perform
well with remarkably few members of this ensemble. Ensemble sampling with fewer members
leads to a greedier strategy, which can perform better for shorter horizons, but is prone to
premature and suboptimal convergence compared to true Thompson sampling [27].
In this
problem, using an ensemble of as few as 30 members provides very good performance.

7.4 Reinforcement Learning in Markov Decision Processes

Reinforcement learning (RL) extends upon the contextual online decision problems covered in
Section 6.2 to allow for delayed feedback and long term consequences [32, 33]. Concretely (using
the notation of Section 6.2) the response yt to the action xt depends on a context zt; but we no
longer assume that the evolution of the context zt+1 is independent of the action xt. As such,
the action xt may aﬀect not only the reward r(yt) but also, through the eﬀect upon the context
zt+1, the rewards earned in future timesteps {r(yt(cid:48))}t(cid:48)>t. As a motivating example, consider a
problem of sequentially recommending products xt where the customer response yt is informed
not only by the quality of the product, but also the history of past recommendations. The

34

(a) Fixed (cid:15)-greedy.

(b) Annealing (cid:15)-greedy.

(c) Ensemble TS.

Figure 17: Bandit learning with an underlying neural network.

evolution of the context zt+1–which captures relevant information about the current customer
available at time t + 1–is then directly aﬀected by the customer response yt; if a customer
watched ‘The Godfather’ and loved it, then chances are probably higher they may enjoy ‘The
Godfather 2’.

Maximizing cumulative rewards in a problem where decision have long term consequences
can require planning with regards to future rewards, rather than optimizing each timestep
myopically. Similarly, eﬃcient exploration in these domains can require planning with regards to
the potential for informative observations in future timesteps, rather than myopically considering
the information gained over a single timestep. This sophisticated form of temporally-extended
exploration, which can be absolutely critical for eﬀective performance, is sometimes called deep
exploration [34].

The Thompson sampling principle can also be applied successfully to reinforcement learning
in ﬁnite horizon Markov decision processes (MDPs) [35]. However, we also highlight that special
care must be taken with respect to the notion of ‘timestep or ‘period within Thompson sampling
to preserve deep exploration.

An episodic ﬁnite horizon MDP M = (S, A, RM, P M, H, ρ) is a type of online decision problem
that proceeds in distinct episodes, each with H timesteps within them. S is the state space, A
is the action space and H is the length of the episode. At the start of each episode the initial
state s0 is drawn from the distribution ρ. At each timestep h = 0, .., H − 1 within an episode
the agent observes state sh ∈ S, selects action ah ∈ A, receives a reward rh ∼ RM (sh, ah)
and transitions to a new state sh+1 ∼ P M (sh, ah). A policy µ is a function mapping each
state s ∈ S and timestep h = 0, .., H − 1 to an action a ∈ A. The value function V M
µ,h(s) =
E[(cid:80)H−1
j=h rj(sj, µ(sj, j)) | sh = s] encodes the expected reward accumulated under µ in the
remainder of the episode when starting from state s and timestep h. Finite horizon MDPs allow
for long term consequences through the evolution of the state, but the scope of this inﬂuence is
limited to within an individual episode.

Immediately we should note that we have already studied a ﬁnite horizon MDP under diﬀer-
ent terminology in Example 2: the online shortest path problem. To see the connection simply
view each choice of edge as sequential timesteps within the period (or episode); the state is the
current vertex, the action is the next choice of edge and the horizon is the maximal number
of decision stages. With this connection in mind we can express the problem of maximizing
the cumulative rewards in a ﬁnite horizon MDP (cid:80)K
h=0 r(skh, akh) equivalently as a ban-
dit problem over periods t = 1, 2, .., K where each bandit period is an entire episode of MDP

(cid:80)H−1

k=1

35

interaction and each bandit action is a policy µt for use within that episode. By contrast, a
naive application of Thompson sampling to reinforcement learning that resamples policies every
timestep within an episode could be extremely ineﬃcient as it does not perform deep exploration.

Figure 18: MDPs where Thompson sampling every timestep leads to ineﬃcient exploration.

Consider the example in Figure 18 where the underlying MDP is characterized by a long
chain of states {s−N , .., sN } and only the one of the far left or far right positions are rewarding
with equal probability; all other states produce zero reward and with known dynamics. Learning
about the true dynamics of the MDP requires a consistent policy over N steps right or N steps
left; a variant of Thompson sampling that resampled every timestep would be exponentially
unlikely to make it to either end within N steps [34]. By contrast, sampling at an episode level
and holding that policy ﬁxed for the duration of the episode would demonstrate deep exploration
and so be able to learn the optimal policy within a single episode.

In order to apply Thompson sampling to policy selection we need a way of sampling from
the posterior distribution for the optimal policy. One eﬃcient way to do this, at least for ﬁ-
nite |S|, |A| is to maintain a posterior distribution over the reward distribution RM and the
transition dynamics P M at each state and action (s, a). In order to generate a sample for the
optimal policy, simply take a single posterior sample for the reward distribution and transition
probabilities and then solve for the optimal policy for this sample. This process is equivalent
to maintaining a posterior distribution over the optimal policy, but may be more tractable de-
pending on the problem setting. Estimating a posterior distribution over rewards is no diﬀerent
from the setting of bandit learning that we have already discussed at length within this paper.
The transition function looks a little diﬀerent, but for transitions over a ﬁnite state space the
Dirichlet distribution is a useful conjugate prior.
It is a multi-dimensional generalization of
the Beta distribution from Example 3. The Dirichlet prior over outcomes in S = {1, .., S} is
speciﬁed by a positive vector of pseudo-observations α ∈ RS
+; updates to the Dirichlet posterior
can be performed analytically simply by incrementing the appropriate column of α [4].

In Figure 19 we present a computational comparison of Thompson sampling where a new
policy is drawn every ‘timestep’ and Thompson sampling where a new policy is drawn every
‘episode’. This ﬁgure compares performance in the example shown in Figure 18. Figure 19a
compares the performance of sampling schemes where the agent has an informative prior that
matches the true underlying system. As explained above, sampling once per episode Thompson
sampling is guaranteed to learn the true MDP structure in a single episode. By contrast,
resampling every timestep leads to uniformly random actions until either s−N or sN is visited.
Therefore, it takes a minimum of 2N epsiodes for the ﬁrst expected reward.

The diﬀerence in performance demonstrated by Figure 19a is particularly extreme because
the prior structure means that there is only value to deep exploration, and none to ‘shallow’
exploration per timestep [34]. In Figure 19b we present results for Thompson sampling variant
on the same environment but with uniform Dirichlet prior over transitions and N (0, 1) prior over
rewards in each state and action. With this prior structure Thompson sampling per timestep is
not as hopeless, but still performs worse than Thompson sampling per episode. Once again, this
diﬀerence increases with MDP problem size. Overall, Figure 19 demonstrates that the beneﬁt of
sampling per episode, rather than per timestep, can become arbitrarily large. As an additional
beneﬁt this approach is also more computationally eﬃcient, since we only need to solve for the

36

(a) Using informed prior.

(b) Using uninformed prior.

Figure 19: Comparing Thompson sampling by episode with Thompson sampling by episode.

optimal policy once every episode rather than at each timestep.

8 Why it Works, When it Fails, and Alternative Approaches

Earlier sections demonstrate that Thompson sampling approaches can be adapted to address
a number of problem classes of practical import. In this section, we provide intuition for why
Thompson sampling explores eﬃciently, and brieﬂy review theoretical work that formalizes this
intuition. We will then highlight problem classes for which Thompson sampling is poorly suited,
and refer to some alternative algorithms.

8.1 Why Thompson Sampling Works

To understand whether Thompson sampling is well suited to a particular application, it is useful
to develop a high level understanding of why it works. As information is gathered, beliefs about
arms’ payouts are carefully tracked. By sampling actions according to the posterior probability
they are optimal, the algorithm continues to sample all arms that could plausibly be optimal,
while shifting sampling away from those that are extremely unlikely to be optimal. Roughly
speaking, the algorithm tries all promising actions while gradually discarding those that are
believed to under-perform.

This intuition is formalized in recent theoretical analyses of Thompson sampling. [5] observed
empirically that for the simple Bernoulli bandit problem described in Example 1, the regret of
Thompson sampling scales in an asymptotically optimal manner in the sense deﬁned by [36]. A
series of papers provided proofs conﬁrming this ﬁnding [37, 38, 39]. Later papers have studied
Thompson sampling as a general tool for exploration in more complicated online optimization
problems. The ﬁrst regret bounds for such problems were established by [40] for linear contextual
bandits and by [41, 42] for an extremely general class of bandit models. Subsequent papers have
further developed this theory [43, 44, 45] and have studied extensions of Thompson sampling to
reinforcement-learning [35, 46, 47, 48].

37

8.2 Limitations of Thompson Sampling

Thompson sampling is a simple and eﬀective method for exploration in broad classes of problems,
but no heuristic works well for all problems. For example, Thompson sampling is certainly a poor
ﬁt for sequential learning problems that do not require much active exploration; in such cases by
greedier algorithms that don’t invest in costly exploration usually provide better performance.
We now highlight two more subtle problem features that pose challenges when applying standard
Thompson sampling.

8.2.1 Time Preference

Thompson sampling is eﬀective at minimizing the exploration costs required to converge on an
optimal action. It may perform poorly, however, in time-sensitive learning problems where it is
better to exploit a high performing suboptimal action than to invest resources exploring arms
that might oﬀer slightly improved performance.

To understand this issue, let us revisit the motivating story given at the beginning of the
paper. Suppose that while waiting for his friends to ﬁnish a game in another part of the casino,
a gambler sits down at a slot machine with k arms yielding uncertain payouts. For concreteness,
assume as in Example 3 that the machine’s payouts are binary with Beta distributed priors.
But now, suppose the number of plays τ the gambler will complete before his friends arrive is
uncertain, with τ ∼ Geometric(1 − δ) and E[τ ] = 1/(1 − δ). Is Thompson sampling still an
eﬀective strategy for maximizing the cumulative reward earned? This depends on the values of
δ and k. When δ → 1 so E[τ ] (cid:29) k, it is generally worth exploring to identify the optimal arm
so it can be played in subsequent periods. However, if E[τ ] < k, there is not time to play every
arm, and the gambler would be better oﬀ exploiting the best arm among those he tries initially
than continuing to explore alternatives.

Related issues also arise in the nonstationary learning problems described in Section 6.3.
When a nonstarionary system evolves rapidly, information gathered quickly becomes irrelevant
to optimizing future performance. In such cases, it may be impossible to converge on the current
optimal action before the system changes substantially, and the algorithms presented in Section
6.3 might perform better if they are modiﬁed to explore less aggressively.

This issue is discussed further in [49]. That paper proposes and analyzes satisﬁcing Thomp-
son sampling, a variant of Thompson sampling that is designed to minimize the exploration
costs required to identify an action that is suﬃciently close to optimal.

8.2.2 Problems Requiring Careful Assessment of Information Gain

Thompson sampling is well suited to problems where the best way to learn which action is
optimal is to test the most promising actions. However, there are natural problems where such
a strategy is far from optimal, and eﬃcient learning requires a more careful assessment of the
information actions provide. The following example is designed to make this point transparent.

Example 9 (A revealing action) Suppose there are k + 1 actions {0, 1, ..., k},
and θ is an unknown parameter drawn uniformly at random from Θ = {1, .., k}.
Rewards are deterministic conditioned on θ, and when played action i ∈ {1, ..., k}
always yields reward 1 if θ = i and 0 otherwise. Action 0 is a special “revealing”
action that yields reward 1/2θ when played.

Note that action 0 is known to never yield the maximal reward, and is therefore never selected
by Thompson sampling. Instead, TS will select among actions {1, ..., k}, ruling out only a single
action at a time until a reward 1 is earned and the optimal action is identiﬁed. An optimal
algorithm for this problem would recognize that although action 0 cannot yield the maximal
reward, sampling it is valuable because of the information it provides about other actions.

38

Indeed, by sampling action 0 in the ﬁrst period, the decision maker immediately learns the
value of θ, and can exploit that knowledge to play the optimal action in all subsequent periods.
This example is described in [50], which also presents two problems of greater practical
signiﬁcance that require careful assessment of information: one related to recommendation
systems and one involving bandits with sparse-linear reward models. That paper also proposes
an algorithm that carefully assesses the information actions reveal.

8.3 Alternative Approaches

Much of the the work on multi-armed bandit problems has focused on problems with a ﬁnite
number of independent arms, like Example 3 in this paper. For such problems, the Gittins index
theorem [51] characterizes a Bayes optimal policy, which exactly maximizes expected cumulative
discounted reward. Computing this policy requires solving an optimal stopping problem for each
period and arm, and is much more demanding than Thompson sampling. For more complicated
problems, the Gittins index theorem fails to hold, and computing an optimal policy is typically
infeasible. A thorough treatment of Gittins indices is provided in [52].

Upper-conﬁdence bound (UCB) algorithms oﬀer another approach to eﬃcient exploration.
As the name suggests, these algorithms maintain upper-conﬁdence bounds, representing the
largest mean-reward an arm could plausibly generate given past observations. The algorithm
then selects the action with the highest upper-conﬁdence bound. At a high level, these algo-
rithms are similar to Thompson sampling, in that they continue sampling all promising arms
while gradually discarding those that under-perform. A more formal link between the two ap-
proaches is established in [42]. UCB algorithms have been proposed for a variety of problems,
including bandit problems with independent arms [36, 53, 54, 55], bandit problems with linearly
parameterized arms [30, 56], bandits with continuous action spaces and smooth reward functions
[57, 58, 59], and exploration in reinforcement learning [60].

The knowledge gradient [61, 62] and information-directed sampling [50] are alternative al-
gorithms that attempt to reason more carefully about the value of information acquired by
sampling an action. Finally, there is a large literature on multi-armed bandit optimization in
adversarial environments, which we will not review here. See [63] for thorough coverage.

Acknowledgements

This work was generously supported by a research grant from Boeing, a Marketing Research
Award from Adobe, and a Stanford Graduate Fellowship. We thank Susan Murphy for helpful
suggestions. Section 7.3 draws material from a paper coauthored by Xiuyuan Lu. We thank her
for help with the experiments presented in that section and for the associated code.

References

[1] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

[2] William R Thompson. On the theory of apportionment. American Journal of Mathematics,

57(2):450–456, 1935.

[3] Jeremy Wyatt. Exploration and inference in learning from reinforcement. PhD thesis,
University of Edinburgh. College of Science and Engineering. School of Informatics., 1997.

[4] Malcolm Strens. A Bayesian framework for reinforcement learning. In ICML, pages 943–

950, 2000.

[5] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Neural Infor-

mation Processing Systems (NIPS), 2011.

39

[6] S.L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models

in Business and Industry, 26(6):639–658, 2010.

[7] Kris Johnson Ferreira, David Simchi-Levi, and He Wang. Online network revenue manage-

ment using Thompson sampling. Working Paper, 2016.

[8] Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display

advertising using multi-armed bandit experiments. Marketing Science, 2017.

[9] Aijun Bai, Feng Wu, and Xiaoping Chen. Bayesian mixture modelling and inference based
Thompson sampling in Monte-Carlo tree search. In Advances in Neural Information Pro-
cessing Systems, pages 1646–1654, 2013.

[10] T. Graepel, J.Q. Candela, T. Borchert, and R. Herbrich. Web-scale Bayesian click-through
rate prediction for sponsored search advertising in Microsoft’s Bing search engine.
In
Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages
13–20, 2010.

[11] Deepak Agarwal. Computational advertising: the linkedin way. In Proceedings of the 22nd
ACM international conference on Conference on information & knowledge management,
pages 1585–1586. ACM, 2013.

[12] Deepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, and Liang Zhang. Laser: A
scalable response prediction platform for online advertising. In Proceedings of the 7th ACM
international conference on Web search and data mining, pages 173–182. ACM, 2014.

[13] Jaya Kawale, Hung H Bui, Branislav Kveton, Long Tran-Thanh, and Sanjay Chawla.
Eﬃcient Thompson sampling for online matrix-factorization recommendation. In Advances
in Neural Information Processing Systems, pages 1297–1305, 2015.

[14] Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeﬀ Schneider, and Barnabas Poczos.
arXiv preprint

Asynchronous parallel Bayesian optimisation via Thompson sampling.
arXiv:1705.09236, 2017.

[15] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration
In Advances in Neural Information Processing Systems, pages

via bootstrapped DQN.
4026–4034, 2016.

[16] Steven L Scott. Multi-armed bandit experiments in the online service economy. Applied

Stochastic Models in Business and Industry, 31(1):37–45, 2015.

[17] Benjamin Van Roy Abbas Kazerouni Zheng Wen Ian Osband, Dan Russo. TS Tutorial: A

[18] George Casella and Edward I George. Explaining the Gibbs sampler. The American

tutorial on thompson sampling, 2017.

Statistician, 46(3):167–174, 1992.

[19] Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distribu-

tions and their discrete approximations. Bernoulli, pages 341–363, 1996.

[20] Gareth O Roberts and Jeﬀrey S Rosenthal. Optimal scaling of discrete approximations to
Langevin diﬀusions. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 60(1):255–268, 1998.

[21] S´ebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribu-
tion with projected Langevin Monte Carlo. arXiv preprint arXiv:1507.02564, 2015.

[22] Alain Durmus and Eric Moulines. Sampling from strongly log-concave distributions with

the unadjusted Langevin algorithm. arXiv preprint arXiv:1605.01559, 2016.

[23] Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv

preprint arXiv:1705.09048, 2017.

40

[24] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dy-

namics. In ICML, 2011.

[25] Yee Whye Teh, Alexandre H Thiery, and Sebastian J Vollmer. Consistency and ﬂuctuations
for stochastic gradient Langevin dynamics. Journal of Machine Learning Research, 17(7):1–
33, 2016.

[26] Carlos G´omez-Uribe. Online algorithms for parameter mean and variance estimation in

dynamic regression. arXiv preprint arXiv:1605.05697v1, 2016.

[27] Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. arXiv preprint arXiv:1705.07347,

2017.

[28] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. An experimental comparison
of click position-bias models. In Proceedings of the 2008 International Conference on Web
Search and Data Mining, pages 87–94. ACM, 2008.

[29] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits:
Learning to rank in the cascade model. In Proceedings of the 32nd International Conference
on Machine Learning (ICML-15), pages 767–776, 2015.

[30] V. Dani, T.P. Hayes, and S.M. Kakade. Stochastic linear optimization under bandit feed-
back. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages
355–366, 2008.

[31] Ian Osband and Benjamin Van Roy. On optimistic versus randomized exploration in rein-
forcement learning. In Proceedings of The Multi-disciplinary Conference on Reinforcement
Learning and Decision Making. 2017.

[32] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1.

[33] Michael L Littman. Reinforcement learning improves behaviour from evaluative feedback.

MIT press Cambridge, 1998.

Nature, 521(7553):445–451, 2015.

[34] Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via

randomized value functions. arXiv preprint arXiv:1703.07608, 2017.

[35] I. Osband, D. Russo, and B. Van Roy. (More) eﬃcient reinforcement learning via posterior
sampling. In Advances in Neural Information Processing Systems 26. Curran Associates,
Inc., 2013.

[36] T.L. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in

applied mathematics, 6(1):4–22, 1985.

[37] S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit
problem. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), 2012.

[38] S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. In Pro-
ceedings of the Sixteenth International Conference on Artiﬁcial Intelligence and Statistics,
pages 99–107, 2013.

[39] E. Kauﬀmann, N. Korda, and R. Munos. Thompson sampling: an asymptotically optimal

ﬁnite time analysis. In International Conference on Algorithmic Learning Theory, 2012.

[40] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoﬀs.
In Proceedings of The 30th International Conference on Machine Learning, pages 127–135,
2013.

[41] D. Russo and B. Van Roy. Eluder dimension and the sample complexity of optimistic explo-
ration. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger,
editors, Advances in Neural Information Processing Systems 26, pages 2256–2264. Curran
Associates, Inc., 2013.

41

[42] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of

Operations Research, 39(4):1221–1243, 2014.

[43] A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex online problems.
In Proceedings of The 31st International Conference on Machine Learning, pages 100–108,
2014.

[44] D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling.

Journal of Machine Learning Research, 17(68):1–30, 2016.

[45] Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In AISTATS

2017-20th International Conference on Artiﬁcial Intelligence and Statistics, 2017.

[46] Aditya Gopalan and Shie Mannor. Thompson sampling for learning parameterized Markov

decision processes. In COLT, pages 861–898, 2015.

[47] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via ran-
domized value functions. In Proceedings of The 33rd International Conference on Machine
Learning, pages 2377–2386, 2016.

[48] Michael Jong Kim. Thompson sampling for stochastic control: The ﬁnite parameter case.

IEEE Transactions on Automatic Control, 2017.

[49] Daniel Russo, David Tse, and Benjamin Van Roy. Time-sensitive bandit learning and

satisﬁcing Thompson sampling. arXiv preprint arXiv:1704.09028, 2017.

[50] D. Russo and B. Van Roy. Learning to optimize via information-directed sampling.

In
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors,
Advances in Neural Information Processing Systems 27, pages 1583–1591. Curran Asso-
ciates, Inc., 2014.

[51] J.C. Gittins and D.M. Jones. A dynamic allocation index for the discounted multiarmed

bandit problem. Biometrika, 66(3):561–565, 1979.

[52] J. Gittins, K. Glazebrook, and R. Weber. Multi-Armed Bandit Allocation Indices. John

Wiley & Sons, Ltd, 2011.

[53] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine learning, 47(2):235–256, 2002.

[54] O. Capp´e, A. Garivier, O.-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper
conﬁdence bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516–1541,
2013.

[55] E. Kaufmann, O. Capp´e, and A. Garivier. On Bayesian upper conﬁdence bounds for bandit
problems. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2012.

[56] P. Rusmevichientong and J.N. Tsitsiklis. Linearly parameterized bandits. Mathematics of

Operations Research, 35(2):395–411, 2010.

[57] R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proceed-

ings of the 40th ACM Symposium on Theory of Computing, 2008.

[58] S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv´ari. X-armed bandits. Journal of Machine

Learning Research, 12:1655–1695, June 2011.

[59] N. Srinivas, A. Krause, S.M. Kakade, and M. Seeger. Information-theoretic regret bounds
for Gaussian process optimization in the bandit setting. IEEE Transactions on Information
Theory, 58(5):3250 –3265, may 2012.

[60] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning.

Journal of Machine Learning Research, 11:1563–1600, 2010.

[61] P.I. Frazier, W.B. Powell, and S. Dayanik. A knowledge-gradient policy for sequential
information collection. SIAM Journal on Control and Optimization, 47(5):2410–2439, 2008.

42

[62] P. Frazier, W. Powell, and S. Dayanik. The knowledge-gradient policy for correlated normal

beliefs. INFORMS journal on Computing, 21(4):599–613, 2009.

[63] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. Foundations and trends in machine learning, 5(1):1–122, 2012.

43

