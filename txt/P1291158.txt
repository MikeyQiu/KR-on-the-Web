Deep Saliency Hashing for Fine-grained Retrieval

Sheng Jin1 Hongxun Yao1 Xiaoshuai Sun1 Shangchen Zhou2 Lei Zhang3 Xiansheng Hua4
1School of Computer Science and Technology, Harbin Institute of Technology 2SenseTime Research
3Hong Kong Polytechnic University, Hong Kong, China 4Damo Academy, Alibaba Group
116B903055@stu.hit.edu.cn 1{h.yao, xiaoshuaisun}@hit.edu.cn
2zhoushangchen@sensetime.com 3cslzhang@comp.polyu.edu.hk 4xiansheng.hxs@alibaba-inc.com

9
1
0
2
 
b
e
F
 
1
 
 
]

V
C
.
s
c
[
 
 
2
v
9
5
4
1
0
.
7
0
8
1
:
v
i
X
r
a

Abstract

In recent years, hashing methods have been proved to be
effective and efﬁcient for the large-scale Web media search.
However, the existing general hashing methods have lim-
ited discriminative power for describing ﬁne-grained ob-
jects that share similar overall appearance but have subtle
difference. To solve this problem, we for the ﬁrst time intro-
duce the attention mechanism to the learning of ﬁne-grained
hashing codes. Speciﬁcally, we propose a novel deep hash-
ing model, named deep saliency hashing (DSaH), which
automatically mines salient regions and learns semantic-
preserving hashing codes simultaneously. DSaH is a two-
step end-to-end model consisting of an attention network
and a hashing network. Our loss function contains three
basic components, including the semantic loss, the saliency
loss, and the quantization loss. As the core of DSaH, the
saliency loss guides the attention network to mine discrim-
inative regions from pairs of images. We conduct exten-
sive experiments on both ﬁne-grained and general retrieval
datasets for performance evaluation. Experimental results
on ﬁne grained dataset, including Oxford Flowers-17, Stan-
ford Dogs-120 and CUB Bird demonstrate that our DSaH
performs the best for ﬁne-grained retrieval task and beats
strongest competitor (DTQ) by approximately 10% on both
Stanford Dogs-120 and CUB Bird. DSaH is also compara-
ble to several state-of-the-art hashing methods on general
datasets, including CIFAR-10 and NUS-WIDE.

1. Introduction

Searching for content relevant images in a large scale
dataset is widely used in pratical application. Such re-
trieval tasks remain a challenge because of the large compu-
tational cost and the high accuracy requirement. To address
the efﬁciency and effectiveness problems, a great number
of hashing methods are proposed to map images to binary
codes. These hashing methods can be classiﬁed into two
categories: data-independent [1] and data-dependent [3, 7].

Figure 1: The main idea of our method. We propose a
deep hashing method for the retrieval of ﬁne-grained objects
which share similar appearances. To produce more discrim-
inative hashing codes, our method highlights the discrimi-
native regions of input images using the attention network.

Since data-dependent methods preserve the semantic struc-
ture of the data, they usually achieve better performance.

Data-dependent methods can be further divided into
three categories: unsupervised methods, semi-supervised
methods, and supervised methods. Compared to the for-
mer two categories, supervised methods use semantic in-
formation in terms of reliable class labels to improve per-
formance. Many representative works have been proposed
along this direction, e.g., Binary Reconstruction Embed-
ding [14], Column Generation Hashing [17], Kernel-based
Supervised Hashing [22], Minimal Loss Hashing [27],
Hamming Distance Metric Learning [28], and Semantic
Hashing [30]. The success of supervised methods demon-
strates that class information can dramatically improve the
quality of hashing codes. However, these shallow hashing
methods use hand-crafted features to represent images and
generate the hashing codes. Thus, the quality of hashing
codes depends heavily on feature selection, which is the
most crucial limitation of such methods.

4321

It is hard to ensure that these hand-crafted features pre-
serve sufﬁcient semantic information. To solve this lim-
itation, Xia et al.
[37] introduce deep learning to hash-
ing (CNNH), which performs feature learning and hash-
ing codes learning simultaneously. Following this work,
many deep hashing methods have been proposed, including
Deep Cauchy Hashing [3], Deep Triplet Quantization[20],
Deep Supervised Hashing [21], and Deep Semantic Rank-
ing Hashing [45]. Extensive experiments demonstrate that
deep hashing methods achieve signiﬁcant improvements.

Nevertheless, existing deep hashing methods are mostly
studied and validated on general datasets, e.g., CIFAR-10
[12] and NUS-WIDE [6]. These datasets contain only a few
categories with a large number of images per class. Besides,
different classes have signiﬁcant differences in appearance
which make the problem simpler than real-world cases. To
support practical applications, two crucial issues still need
to be considered. Firstly, a robust hashing method should
be able to distinguish ﬁne-grained objects. The major chal-
lenge is that these ﬁne-grained objects share similar overall
appearance, making the inter-class differences more impor-
tant than the intra-class variance. Secondly, hashing meth-
ods should be able to support a large number of categories.
Different from CIFAR-10, the existing ﬁne-grained dataset
consists of much more categories with small amounts of im-
ages per class. This raises another challenge which is how
to generate hashing codes for much more categories with
relatively fewer data.

Similar challenges exist in other ﬁelds of computer vi-
sion, e.g., ﬁne-grained classiﬁcation [36] and person re-id
[46]. In these ﬁelds, representative methods deal with the
above challenges by mining discriminative parts for each
category, either manually [36] or automatically [8, 46]. The
deep methods proposed by [8] and [46] automatically mine
salient regions and achieve remarkable improvements com-
pared to traditional state-of-the-arts. Inspired by such meth-
ods, we propose a novel deep hashing method for ﬁne-
grained retrieval, termed Deep Saliency Hashing (DSaH),
to solve the two above-mentioned challenges jointly.

The main idea of the proposed saliency hashing is de-
picted in Fig. 1. Speciﬁcally, DSaH is an end-to-end CNN
model that simultaneously mines discriminative parts and
learns hashing codes. DSaH contains three components.
The ﬁrst component, named attention module, is a full con-
volutional network aiming to generate a saliency image
from the original image. The second component, named
hashing module, is a deep network based on VGG-16 model
aiming to map images to hashing codes. And the third com-
ponent is a loss function. The loss function contains 1) a
novel semantic loss to measure the semantic quality of hash-
ing codes based on the pairwise labels; 2) a saliency loss of
image quadruples to mine discriminative region; and 3) a
quantization loss to learn the optimal hashing codes from

the binary-like codes. All the components are intergraded
into a uniﬁed framework. We iteratively update the atten-
tion network and the hashing network to learn better hash-
ing codes. The main contributions of DSaH are three-fold:

• We propose a deep hashing method by integrating
saliency mechanism into hashing. To the best of our
knowledge, DSaH is the ﬁrst attentional deep hashing
model specially designed for ﬁne-grained tasks.

• A novel saliency loss of image quadruples is proposed
to guide the attention network for automatic discrim-
inative regions mining. Experimental results demon-
strate that the ﬁne-grained categories can be better dis-
tinguished based on these attractive regions.

• Experimental results on both general hashing datatsets
and ﬁne-grained retrieval datasets demonstrate the su-
perior performance of our method in comparison with
many state-of-art hashing methods.

2. Related Work

We introduce the most related works from two aspects:

hashing code learning and discriminative part mining.

Deep Hashing In the last few years, deep convolutional
neural network [37] has been employed to supervised hash-
ing. Followed by [37], some representative works have been
proposed [16, 19, 21, 45]. These deep methods are proved
to be effective for general object retrieval, where different
categories have signiﬁcant visual differences (e.g., CIFAR-
10). Recently, deep hashing methods emerge as a promis-
ing solution for efﬁcient person re-id [43, 48]. Different
from object retrieval, human bodies share similar appear-
ance with subtle difference in some salient regions. Zhang
[43] et al. propose DRSCH and introduce hashing into per-
son re-id. DRSCH is a triplet-based model and encodes the
entire person image to hashing codes without considering
the part-level semantics. PDH [48] integrates the part-based
model into the triplet model and achieves signiﬁcant im-
provements. However, the part partition strategy of PDH is
speciﬁed based on human structure. Since there are huge
variations in scale, multiple instances etc, in typical ﬁne-
grained datasets (e.g., CUB Bird [35]), the part partitioning
strategy of PDH is not suitable for non-human ﬁne-grained
objects. In this paper, we introduce the hashing method into
ﬁne-grained retrieval, where an attention network is embed-
ded to mine salient regions for accurate code learning.

Mining Discriminative Regions The key challenge of
learning accurate hashing codes for ﬁne-grained objects is
to locate the discriminative regions in images. Facing a sim-
ilar challenge, to boost the performance of ﬁne-grained im-
age classiﬁcation, researchers have proposed various salient
region localization approaches [34, 39, 47]. Previous works

Figure 2: The proposed framework for deep saliency hashing (DSaH). DSaH is comprised of three components: (1) an
attention network based on fcn-16s for learning a saliency image, (2) a hashing network based on vgg-16 for learning hashing
codes, (3) a set of loss functions including a semantic loss (Sem-L), a saliency loss (Sal-L) and a quantization loss (Qua-
L) for optimization.
In the training stage, the attention network and the hashing network are trained iteratively to mine
discriminative regions (Step1) and learn semantic-preserving hashing codes (Step2). For attention network, the whole set of
loss functions are used. For hashing network, we only use the semantic loss and quantization loss.

locate the salient regions either by unsupervised methods
[24, 38] or by leveraging manual part annotations [40, 41].
Following these works, the recent hashing methods [2, 32]
locate salient regions to improve performance in an unsu-
pervised manner. DPH [2] uses GBVS [10] to calculate
the saliency scores for each pixel. Then a series of salient
regions are generated by increasing the threshold values.
Shen et al. propose a cross-model hashing method, named
TVDB [32], which adopts RPN [29] to detect salient re-
gions and encodes the regional information of image, the
semantic dependencies, as well as the cues between words
by two modal-speciﬁc networks. However, these hashing
methods use the off-the-shelf models to locate salient re-
gions, which might not be accurate for new images or spe-
ciﬁc tasks. Instead, our model trains a saliency prediction
network jointly with the hashing network, where the two
modules are optimized together toward a uniﬁed objective.

Recent methods [24, 44] try to discover discriminative
regions automatically by deep networks. These deep meth-
ods do not require labeling information, such as the labeled
part masks or boxes, but only use the class label informa-
tion. Zhao et al.
[46] use the similarity information (a
pair of person images about the same person or not) to train
part model specially for person matching. Similar intuitions
can be found in recent ﬁne-grained classiﬁcation methods.
Fu et al. [8] propose a novel recurrent attention convolu-
tional neural network, named RA-CNN, to discover salient
regions and learn region-based feature representation recur-
sively. The basic idea of this method is that salient region
localization and ﬁne-grained feature learning are mutually
correlated and thus can reinforce each other. Motivated by
[8, 46], we adopt a novel attention network to automatically
mine the salient region for learning better hashing codes. To
the best of our knowledge, it is the ﬁrst time that attention
mechanism is formally employed to ﬁne-grained hashing.

3. Deep Saliency Hashing

Fig. 2 shows the proposed DSaH framework. Our
method includes three main components. The ﬁrst compo-
nent is an attention network. The attention network maps
each input image into a saliency image. The second com-
ponent is a hashing network. The hashing network learns
binary-like codes from an original image and its saliency
image. The third component is a set of loss terms, includ-
ing pairwise semantic loss, saliency loss of image quadru-
ples, and quantization loss. The semantic loss requires the
hashing codes learned from each image pair to preserve se-
mantic information. The saliency loss guides the attention
network to highlight the salient regions of original images.
The quantization loss is devised to measure the loss between
binary-like codes and hashing codes after binarization. The
whole cost function is written as below:

J = Jsem + Jsal + Jreg,

(1)

where Jsem represents the semantic loss, Jsal represents the
saliency loss and Jreg represents the quantization loss.

3.1. The Attention Network

The attention network is proposed to map the original
image xi to the saliency image yi : xi → yi. This module
includes two stages. In the ﬁrst stage, we assign a saliency
value of each pixel in the original image. Then we obtain
the saliency image by highlighting the salient pixels.

As described above, a dense prediction problem needs to
be solved in the ﬁrst stage. The location (p, q) in image xi
is denoted as xi(p, q). We denote the learned saliency value
of each pixel in image xi as Salmapi(p, q). Long. et al
[25] prove that FCN is effective for dense prediction which
maps each pixel of an image to a label vector.

Motivated by FCN [25], we propose an FCN-based at-
tention network, as illustrated in Fig. 2, to discover the
salient region automatically. Different from semantic seg-
mentation approaches, our method does not predict a label
vector but assign a saliency value for each pixel.

In the ﬁrst stage, the proposed FCN-based attention net-

work maps each pixel of images to a saliency value:

Salmapi = Attention(xi).

(2)

To regularize the output, we further normalize the saliency
map so its value is between 0 and 1:

Salmapi(p, q) =

Salmapi(p, q) − min(Salmapi)
max(Salmapi) − min(Salmapi)

.

(3)
Then the saliency image yi is computed through a matrix

dot product by the original image and their saliency map:

yi = Salmapi (cid:12) xi.

(4)

Then we encode the saliency image yi by the hashing
network. We can obtain the saliency loss deﬁned in Eq. 13.
By iteratively updating the parameters with the saliency
loss, the attention network is gradually ﬁne-tuned to mine
discriminative regions automatically.

3.2. The Hashing Network

As shown in Fig. 2, We directly adopt a pre-trained
VGG-16 [33] as the base model of the hashing network.
The raw image pixel, from either the original image and
the saliency image, is the input of the hashing model. The
output layer of VGG is replaced by a hashing layer where
the dimension is deﬁned based on the length of the required
hashing code. The hashing network is trained by the seman-
tic loss (Eq.6) and quantization loss (Eq. 10).

3.3. Loss Functions

Semantic Loss Similar to other hashing methods, our
goal is to learn efﬁcient binary codes for images: x →
b ∈ {1, −1}k, where k denotes the number of hashing bits.
Since discrete optimization is difﬁcult to be solved by deep
networks, we ﬁrstly ignore the binary constraint and con-
centrate on binary-like code for network training, where the
binary-like code is denoted as µi. Then we obtain the opti-
mal hashing codes bi from µi. We deﬁne a pairwise seman-
tic loss to ensure the binary-like codes preserve relevant se-
mantic information. Since each image in the dataset owns
a unique class label, image pairs could be further labeled as
similar or dissimilar:

Sij =

(cid:40)
1
0

images xi and xj share same class label
otherwise,

(5)

where Sij denotes the pairwise label of images xi, xj.
To preserve semantic information, the binary-like codes of
similar images should be as close as possible while the
binary-like codes of dissimilar images should be as far as
possible. Since hashing methods select the hamming dis-
tance to measure similarity in the testing phase, we use
µT
i ∗ µj to calculate the distance of image xi and xj. Given
bi ∈ {1, −1}, the inner product of µi and µj is in the range
of (−k, k). Thus we adopt µT
to transform the inner
product to (0, 1). The result of such linear transformation
is regarded as an estimation of the pairwise label Si,j. The
semantic loss of original image pairs is written as below:

i ∗µj +k
2k

Jsem ori =

Si,j −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

i,j

µT

i ∗ µj + k
2k

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

(6)

Our method also requires hashing codes to learn seman-
tic information from the salient region. To achieve this ob-
jective, the hashing codes of the saliency image also need to
preserve semantic information. Speciﬁcally, we use the at-
tention network to map the original image pairs (xi, xj) into
saliency image pairs (yi, yj). Similar to Eq. 6, the semantic
loss of saliency image pairs is deﬁned as below:

Jsem sal =

Si,j −

(7)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

i,j

µ(cid:48)T

i ∗ µ(cid:48)
2k

j + k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

To learn the saliency image, we propose an attention net-
work. The key idea is that the hashing codes learned from
saliency images are more discriminative. A saliency loss is
deﬁned to guide the attention model to highlight the salient
regions of the original image. Firstly, the proposed atten-
tion network outputs the saliency image yi from the origi-
nal image xi : xi → yi. Then the original image xi and its
saliency image yi are mapped to binary-like codes by the
hashing model, which is denoted as µi, µ(cid:48)

i respectively.

Saliency Loss Similar to the semantic loss, we use image
pairs to deﬁne the saliency loss. The original images xi and
xj are taken as the original image pair. Their saliency im-
ages yi and yj are regarded as the saliency image pair. The
original image pair and their saliency image pair construct
an image quadruple. The binary-like codes of saliency im-
age µ(cid:48)
j are more similar or dissimilar than those of
the original image pair µi and µj according to whether im-
ages xi and xj share the same labels or not. Eq. 6, µT
i ∗µj +k
2k
is used to approximate the value of pairwise label. We de-
note distance of the pairwise label and the estimated value
from original (saliency) image pairs as dij (d(cid:48)

i and µ(cid:48)

ij):

dij =

Si,j −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

µT

i ∗ µj + k
2k

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

, d(cid:48)

ij =

Si,j −

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

µ(cid:48)T

i ∗ µ(cid:48)
2k

j + k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

(8)

Jsal =

max(m − dij + d(cid:48)

ij, 0),

(9)

Output: Hashing function: Hash(x|Θ2) Attention func-

(cid:88)

i,j

As described before, the saliency loss with respect to the
quadruples of images is written as:

where m > 0 is a margin threshold. When the value of
dij − d(cid:48)
ij is below the margin threshold m, the saliency loss
punishes the attention network to make it better highlight
the salient regions.

Quantization Loss The binary constraint of hashing
codes makes it intractable to train an end-to-end deep model
with backpropagation algorithm. As discussed in [21],
some widely-used relaxation scheme working with non-
linear functions, such as sigmoid function, would inevitably
slow down or even restrain the convergence of the network
[13]. To overcome such limitation, we adopt a similar reg-
ularizer on the real-valued network outputs to approach the
desired binary codes. The quantization loss is written as:

Jreg =

(cid:107)µi − bi(cid:107)1 + (cid:107)µ(cid:48)

i − b(cid:48)

i(cid:107)1,

(10)

(cid:88)

i

where b ∈ {1, −1}k. Since bi only appears in the quan-
tization loss, we minimize this loss to obtain the optimal
hashing codes. Obviously, the sign of bi should be same as
that of the binary-like codes µi. Thus the hashing codes bi
can be directly optimized as:

bi = sign(µi).

(11)

3.4. Alternating Optimization

The overall training objective of DSaH integrates the
pairwise semantic loss deﬁned in Eq. 6, the saliency loss
of image quadruples deﬁned in Eq. 9 and the quantiza-
tion loss deﬁned in Eq. 10. DSaH is a two-stage end-to-
end deep model which consists of an attention network, i.e.
Attention, for automatic saliency image estimation and a
shared hashing network, i.e. Hash, for discriminative hash-
ing codes generation. As shown in Algorithm. 1, we train
the attention network and the hashing network iteratively.

In particular, for the shared hashing model, we update its

parameters according to the following overall loss:

(cid:88)

i,j

+

(cid:88)

i

Jhashing =

λJsem ori(xi, xj) + λJsem sal(yi, yj)

Jreg ori(xi) + Jreg sal(yi).

(12)
the shared hashing model is
By minimizing this term,
trained to preserve the relative similarity in of original im-
age pairs and that of saliency image pairs.

Algorithm 1 Deep Saliency Hashing

Input: Training set and their corresponding class label,

Total epochs T of deep optimization;

tion: Attention(x|Θ1).

1: For the entire training set, construct the pairwise label

matrix S according to Eq. 5.

2: for t = 1 : T epoch do
3:
4:

Compute B accordi.ng to Eq. 16
Update Θ1 according to Eq. 13
Update Θ2 according to Eq. 12

5:
6: end for
7: return Hash(x|Θ2), Attention(x|Θ1).

The attention network is trained by the following loss:

Jattention =

αJsal(xi, xj, yi, yj) + λJsem sal(yi, yj)

(cid:88)

i,j

+

(cid:88)

i

Jreg sal(yi).

(13)
By minimizing this term, the attention network is trained to
mine salient and semantic-preserving regions of the input
image, leading to more discriminative hashing codes.

3.5. Out-of-Sample Extension

After the model is trained, we can use it to encode an
input image with a k-bit binary-like code. Since the deep
saliency hashing model consists of two networks, ﬁrstly the
image xi is mapped to the salient image yi:

Then the hashing networks map yi to binary-like codes:

yi = Attention(xi).

µi = Hash(yi).

(14)

(15)

As discussed in Quantization Loss according to Eq. 11,

we adopt the sign function to produce the hashing codes

bi = sign(µi) = sign(Hash(Attention(xi))).

(16)

4. Experiments

In order to test the performance of our proposed DSaH
method, we conduct experiments on two widely used image
retrieval datasets, i.e. CIFAR-10 and NUS-WIDE, to ver-
ify the effectiveness of our method for the general hashing
task. Then we conduct experiments on three ﬁne-grained
datasets: Oxford Flower-17, Stanford Dogs-120 and CUB
Bird to prove that (1) the discriminative region of images
can improve retrieval performance of hashing codes on ﬁne-
grained cases, and (2) the attention model can effectively
mine the saliency region of images.

4.1. Dataset and Evaluation Metric

4.3. Implementation Details

CIFAR-10[12] consists of 60000 32×32 images in 10
classes. Each image in dataset belongs to one class (6000
images per class). We randomly select 100 images per class
as the test set and 500 images per class from the remaining
images as the training set.

NUSWIDE[6] is a multi-label dataset, including nearly
270k images with 81 semantic concepts. Followed [23] and
[37], we select the 21 most frequent concept. Each of con-
cepts is associated with at least 5000 images. We sample
100 images from each concept to form a test set and 500
images per class from the rest images to form a train set.

Oxford Flower-17[11] dataset consists of 1360 images
belonging to 17 mutually classes. Each class contains 80
images. The dataset is divided into three parts, including
a train set, test set, and validation set, with 40 images, 20
images, and 20 images respectively. We combine the vali-
dation set and train set to form the new training set.

Stanford Dogs-120[26] consists of 20,580 images in
120 classes. Each class contains about 150 images. The
dataset is divided into: the train set (100 images per class)
and test set (totally 8580 images for all categories).

CUB Bird[35] includes 11,788 images in mutually 200
classes. The dataset is divided into: the train set(5794 im-
ages) and the test set(5994 images).

We mainly use Mean Average Precision (MAP)and

Precision-Recall curves for quantitative evaluation.

4.2. Comparative Methods

For

the general datasets,

including CIFAR-10 and
NUSWIDE dataset, we compare our method (DSaH) with
six deep hashing method: CNNH [37], DNNH [15], DSH
[21], DQN [5], DVSQ [4], DPSH [16] and three shallow
methods: ITQ-CCA [9], KSH [22], SDH [31]. For shal-
low hashing method, we use deep features extracted by
VGG-16 to represent an image. For fair comparison, we re-
place the VGG-F network used as base model in DPSH [16]
which achieves the best retrieval performance in compara-
tive methods, with the VGG-16 model, named DPSH++.
We mainly compared with DPSH++.

For the ﬁne-grained datasets, our method (DSaH) is
compared with ﬁve deep methods: DSH [21], DQN [5],
DPSH [16], DCH [3], DTQ [20] and four shallow method:
SDH [31], LFH [42], KSH [22], FastH [18]. For fair com-
parison, ﬁrstly we ﬁnetune VGG-16 on each ﬁne grained
dataset respectively for classiﬁcation, respectively. Then
these non-deep hashing methods use CNN features ex-
tracted by the output of the second full-connected layer
(fc7) in the ﬁnetuned VGG-16 network. To be more fair, we
replace the base model(vgg16) of our method with alexnet,
which is same as DCH [3] and DTQ [20], named DSaH-.

The DSaH method is implemented based on PyTorch
and the deep model is trained by batch gradient descent. As
shown in Fig. 2, our model consists of an attention network
and a hashing model. We use VGG-16 as the base model.
It worth mentioning that VGG-16 is not ﬁnetuned on each
dataset. The full convolutional network [25] is adopted as
the base model for the attention network. As discussed in
[25], FCN is improved with multi-resolution layer combi-
nations. We use the fusing method of FCN-16s to improve
performance. Practically, we train the attention network
before the hashing network.
If we ﬁrst train the hashing
network, the attention network might output a semantic-
irrelevant saliency image, which would be a bad sample and
guide the training of hashing model to a wrong direction.

Since we propose a novel attention model for hashing,
we conduct additional experiments on three ﬁne-grained
datasets, Oxford Flower-17, Stanford Dogs-120 and CUB
Bird to further prove its effectiveness. Finally, we also
show some typical examples of the saliency images learned
by proposed attention network.

Additionally, we conduct analytical experiments to dis-
cuss these problems: (1)the analysis of hyper-parameters,
(2)the convergence of the two networks, (3)the effective-
ness of each loss. (4)the learned salient region

Network Parameters In our method, the value of hyper-
parameter λ is 30 and α is 40. We use the mini-batch
stochastic gradient descent with 0.9 momentum. We set the
value of the margin parameters m as k/4, where k is the bits
of hashing codes. The mini-batch size of images is ﬁxed as
32 and the weight decay parameter as 0.0005.

4.4. Experimental Results for Retrieval

Performance on general hashing datasets The Mean
Average Precision (MAP,%) results of different methods
for different numbers of bits on NUSWIDE and CIFAR-
10 dataset are shown in TABLE 1. Experimental results
on CIFAR-10 dataset show that DSaH outperforms existing
best retrieval performance (DPSH [21]) by 8.73%, 9.13%,
11.87%, 9.08% correspond to different hash bits. Similar to
the other hashing methods, we also conduct experiments for
large-scale image retrieval. For NUSWIDE dataset, we fol-
low the setting in [23] and [37], and if two images share at
least one same label, they are considered same. The experi-
mental results of NUSWIDE dataset on TABLE 1 show that
our proposed method outperforms the best retrieval baseline
(DPSH [21]) by 4.4%, 3.2%, 2.6%, 2.2%. According to the
experimental results, DSaH can be clearly seen to be more
effective for traditional hashing task.To ensure fairness, we
conduct experiments on different hashing methods based on
the same base model. The experimental results shown in
TABLE 1 prove that our method still outperform DPSH++
by 1.69%, 2.74%, 1.82%, 1.61% on NUSWIDE dataset and

Table 1: Mean Average Precision (MAP) results for different number of bits on general datasets

Dataset

ITQ-CCA[9]
KSH[22]
SDH[31]
CNNH[37]
DNNH[15]
DPSH[16]
DQN[5]
DSH[21]
DVSQ[4]
DPSH++[16]

12 bits
0.435
0.556
0.558
0.439
0.552
0.713
0.554
0.6157
0.715
0.7834
DSaH 0.8003

CIFAR-10

NUSWIDE

24 bits
0.435
0.572
0.596
0.476
0.566
0.727
0.558
0.6512
0.730
0.8183
0.8457

36 bits
0.435
0.581
0.607
0.472
0.558
0.744
0.564
0.6607
0.749
0.8294
0.8476

48 bits
0.435
0.588
0.614
0.489
0.581
0.757
0.580
0.673
0.760
0.8317
0.8478

12 bits
0.526
0.618
0.645
0.611
0.674
0.794
0.768
0.695
0.788
0.8271
0.838

24 bits
0.575
0.651
0.688
0.618
0.697
0.822
0.776
0.713
0.792
0.8508
0.854

36 bits
0.572
0.672
0.704
0.628
0.713
0.838
0.783
0.732
0.795
0.8592
0.864

48 bits
0.594
0.682
0.711
0.608
0.715
0.851
0.792
0.6755
0.803
0.8649
0.873

Table 2: MAP results for different number of bits on three ﬁne-grained datasets

Dataset

DQN[5]
DSH[21]
DCH[3]
DTQ[20]
DSaH-
SDH[31]
LFH[49]
KSH[22]
FastH[15]
DPSH++[16]

12 bits
0.476
0.566
0.9023
0.9077
0.9273
0.1081
0.1887
0.2431
0.4018
0.6578
DSaH 0.9325

Oxford Flower-17
36 bits
24 bits
0.562
0.537
0.637
0.614
0.9449
0.9117
0.9203
0.9155
0.9471
0.9354
0.1169
0.1399
0.6363
0.4755
0.2530
0.5012
0.5281
0.5244
0.8605
0.8295
0.9692
0.9467

48 bits
0.573
0.680
0.9534
0.9324
0.9565
0.1446
0.8137
0.3553
0.5355
0.8982
0.9756

12 bits
0.0089
0.0119
0.0287
0.0253
0.2442
0.0091
0.0249
0.0136
0.0434
0.2778
0.3976

Stanford Dogs-120
36 bits
24 bits
0.0347
0.0127
0.0117
0.0115
0.3090
0.1971
0.0268
0.0273
0.3628
0.2874
0.090
0.0176
0.0211
0.0247
0.1343
0.1228
0.3643
0.2231
0.5054
0.4409
0.5950
0.5283

48 bits
0.0531
0.0119
0.3073
0.0271
0.4075
0.0365
0.0244
0.1930
0.3927
0.5247
0.6452

CUB Bird

12 bits
-
0.0108
0.0198
0.0198
0.0912
0.0148
0.0064
-
0.0228
0.0723
0.1408

24 bits
-
0.0107
0.0725
0.0233
0.2087
0.0151
0.0064
-
0.0372
0.0764
0.2817

36 bits
-
0.0108
0.1112
0.0241
0.2318
0.0154
0.0065
-
0.0423
0.0838
0.3428

48 bits
-
0.0109
0.1676
0.0228
0.2847
0.0156
0.0067
-
0.0564
0.0792
0.4313

by 1.09%, 0.31%, 0.46%, 0.72% on CIFAR-10 dataset.

Performance on ﬁne-grained datasets The MAP re-
sults of different methods on ﬁne-grained datasets are
shown in TABLE 2. The precision curves are shown in
Fig. 3. Results on Oxford Flower-17 show that DSaH out-
performs existing best retrieval performance by a very large
margin 2.48%, 3.12%, 4.43%, 2.22% correspond to differ-
ent hash bits. We also conduct experiments on a large ﬁne-
grained dataset. For Stanford dog-120 and CUB Bird, this
dataset contains more categories and has smaller inter-class
variations across different classes. The MAP results of all
methods on these datasets are listed in TABLE 2 which
show that the proposed DSaH method substantially outper-
forms all the comparison methods. DSaH achieves absolute
increases of 11.98%, 12.74%, 8.97%, 12.04% and 6.85%,
20.53%, 23.16%, 26.37%. To ensure fairness, DSaH- use
the same base-model as DTQ[20] and DCH[3]. DSaH- still
outperfoms these methods by about 10% on Stanford dog-
120 and CUB Bird datasets. Compared with the MAP re-
sults on traditional hashing task, our method is proved to
achieve a signiﬁcant improvement in ﬁne-grained retrieval.

Figure 3: Comparison of retrieval performance of DSH
method and the other hashing methods on Stanford Dogs

4.5. Exploration Experiment

Hyper-Parameters Analysis In this subsections, we
study the effect of the hyper-parameters. The experiments
are conducted on Oxford Flower-17. The quantization
penalty parameter λ and saliency penalty parameter α is se-
lected by cross-validation from 1 to 100 with an additive
step-size 10. Fig. 4(a) shows that DSaH is not sensitive
to the hyper-parameters λ and α in a large range. For ex-
ample, DSaH can achieve good performance on Oxford-17
with 10 (cid:53) λ (cid:54) 80. As shown in Fig. 4(b), the value of mar-
gin parameters m should not be too large or too small. This
is because that according to Eq. 9, if the value of margin is

Figure 4: Sesitiveity to hyper-parameters (a, b) and the con-
vergence of the attention and hashing networks.

too large, the saliency loss is equal to the semantic loss. If
the value of margin is too small, the saliency loss punishes
the saliency image to be similar to the original images.

Convergence of Networks Since our method trains the
attention network and the hashing network iteratively, we
study the convergence of the proposed networks in CIFAR-
10 dataset. As shown in Fig. 4, it can be seen that both the
attention network and the hashing network converges after
a few epochs, which shows the efﬁciency of our solution.

Component Analysis of the Loss Function Our loss
function consists of two major components: semantic loss
Jsem and saliency loss Jsal. To evaluate the contribution
of each loss, we study the effect of different loss combina-
tions on the retrieval performance. The experimental results
are shown in TABLE 3. An interesting observation is that
for the attention networks, Jsal achieves better performance
than Jsem. The result is understandable because we use the
attention networks to highlight the most discriminative re-
gions. Yet the semantic loss only punishes the network so
that it can locate the semantic-preserving regions. Another
interesting ﬁnding is that for the hashing networks, using
the combination of Jsal and Jsem can obtain even worse
performance than using Jsem only. A possible reason is
that when we use the saliency loss for the hashing networks,
the binary codes b(cid:48)
i learned from saliency image is required
to be more discriminative than bi from the original images.
This might force the hashing codes bi of the original image
to become worse and make bi less effective in guiding the
attention network to highlight salient regions. As shown in
TABLE 3, the best performance is achieved when we use
the combination of the two components, Jsal and Jsem, for
the attention networks and only use the semantic loss Jsem
for the hashing networks.

Table 3: The MAP of DSaH on Stanford Dog-120 using
different combinations of components.

Hashing-Net Attention-Net

Jsem
Jsal
Jsem + Jsal

12 bits
-
-
0.3864
Jsem 0.3374
Jsal
0.3756
Jsem + Jsal
0.3976

Stanford Dog-120
24 bits
-
-
0.5032
0.4738
0.5051
0.5283

48 bits
-
-
0.6355
0.5931
0.6275
0.6452

Jsem + Jsal

Jsem

Figure 5: Examples of the salient region learned by the at-
tention network for Stanford Dogs-120 dataset. As the most
import part, the heads of dogs are correctly highlighted in
the saliency images under various conditions.

Learned Salient Region Fig. 5 shows some typical sam-
ples, including multi-objects, occlusion and so on. Each
row of Fig. 5 is corresponding to a single category. We
have several observations about the learned saliency re-
gions. Most importantly, these learned saliency regions al-
ways cover the heads of dogs. This is because the head
region is important for distinguishing the breed of dog. The
typical samples are detailed as:

(1) The ﬁrst image in Fig. 5(a) has a complex back-
ground. (2) The ﬁrst image in Fig. 5(b) shows that the body
of a dog is overshadowed (3) Compared the ﬁrst image in
Fig. 5(c) with Fig. 5(d), the dogs are in different positions
(sitting on or lying on grassland). The head region is accu-
rately mined no matter how the face is oriented (frontal or
not). (4) For the second image of each line in Fig. 5, human
body was regarded as background. (5) The third image of
each line in Fig. 5 exists more than one dog. The discrim-
inative region of both dogs could be detected. Compared
with the third image Fig. 5(c) and Fig. 5(d), the distance be-
tween two dogs does not affect the saliency results. (6) The
scales of the objects shown in the ﬁrst image of Fig. 5(a) and
the second image of Fig. 5(b) has a signiﬁcant difference.
The result shows that the heads of dogs are also correctly
highlighted in the saliency images.

5. Conclusion

In this paper, we propose a novel supervised deep hash-
ing method for ﬁne-grained retrieval, named deep saliency
hashing (DSaH). To distinguish ﬁne-grained objects, our
method consists of an attention network to automatically
mine discriminative region and a parallel hashing network
to learn semantic-preserving hashing codes. We train the
attention model and the hashing model alternatively. The
attention model is trained based on the semantic loss, quan-
tization loss, and saliency loss. Based on semantic loss
and quantization loss, we obtain semantic-preserving hash-

ing codes from the hashing model. Extensive experiments
on CIFAR-10 and NUSWIDE dataset demonstrate that our
proposed method is comparable to the state-of-art methods
for traditional hashing retrieval task. And the experiments
on Oxford Flower-17, Stanford Dogs-120 and CUB Bird
datasets show that our method achieves a signiﬁcant im-
provement for ﬁne-grained retrieval.

References

[1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for
approximate nearest neighbor in high dimensions. In Foun-
dations of Computer Science, 2006. FOCS’06. 47th Annual
IEEE Symposium on, pages 459–468. IEEE, 2006.

[2] J. Bai, B. Ni, M. Wang, Y. Shen, H. Lai, C. Zhang, L. Mei,
C. Hu, and C. Yao. Deep progressive hashing for image re-
trieval. In Proceedings of the 2017 ACM on Multimedia Con-
ference, pages 208–216. ACM, 2017.

[3] Y. Cao, M. Long, L. Bin, and J. Wang. Deep cauchy hashing

for hamming space retrieval. In CVPR, 2018.

[4] Y. Cao, M. Long, J. Wang, and S. Liu. Deep visual-semantic
In Proceedings
quantization for efﬁcient image retrieval.
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1328–1337, 2017.

[5] Y. Cao, M. Long, J. Wang, H. Zhu, and Q. Wen. Deep quan-
tization network for efﬁcient image retrieval. In AAAI, pages
3457–3463, 2016.

[6] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng.
Nus-wide: a real-world web image database from national
In Proceedings of the ACM inter-
university of singapore.
national conference on image and video retrieval, page 48.
ACM, 2009.

[7] Y. Duan, J. Lu, Z. Wang, J. Feng, and J. Zhou. Learn-
ing deep binary descriptor with multi-quantization. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), July 2017.

[8] J. Fu, H. Zheng, and T. Mei. Look closer to see better: recur-
rent attention convolutional neural network for ﬁne-grained
image recognition. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.

[9] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.

Itera-
tive quantization: A procrustean approach to learning binary
IEEE Transactions
codes for large-scale image retrieval.
on Pattern Analysis and Machine Intelligence, 35(12):2916–
2929, 2013.

[10] J. Harel, C. Koch, and P. Perona. Graph-based visual
saliency. In Advances in neural information processing sys-
tems, pages 545–552, 2007.

[11] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel
dataset for ﬁne-grained image categorization: Stanford dogs.
In Proc. CVPR Workshop on Fine-Grained Visual Catego-
rization (FGVC), volume 2, page 1, 2011.

[12] A. Krizhevsky and G. Hinton. Learning multiple layers of

features from tiny images. 2009.

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
In
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, pages
1097–1105, 2012.

[14] B. Kulis and T. Darrell. Learning to hash with binary re-
constructive embeddings. In Advances in neural information
processing systems, pages 1042–1050, 2009.

[15] H. Lai, Y. Pan, Y. Liu, and S. Yan. Simultaneous feature
learning and hash coding with deep neural networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3270–3278, 2015.

[16] W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based
deep supervised hashing with pairwise labels. arXiv preprint
arXiv:1511.03855, 2015.

[17] X. Li, G. Lin, C. Shen, A. Hengel, and A. Dick. Learning
In International

hash functions using column generation.
Conference on Machine Learning, pages 142–150, 2013.
[18] G. Lin, C. Shen, Q. Shi, A. van den Hengel, and D. Suter.
Fast supervised hashing with decision trees for high-
In Proceedings of the IEEE Conference
dimensional data.
on Computer Vision and Pattern Recognition, pages 1963–
1970, 2014.

[19] K. Lin, H.-F. Yang, J.-H. Hsiao, and C.-S. Chen. Deep learn-
ing of binary hash codes for fast image retrieval. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition workshops, pages 27–35, 2015.

[20] B. Liu, Y. Cao, M. Long, J. Wang, and J. Wang. Deep triplet

quantization. MM, ACM, 2018.

[21] H. Liu, R. Wang, S. Shan, and X. Chen. Deep supervised
hashing for fast image retrieval. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 2064–2072, 2016.

[22] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. Su-
pervised hashing with kernels. In Computer Vision and Pat-
tern Recognition (CVPR), 2012 IEEE Conference on, pages
2074–2081. IEEE, 2012.

[23] W. Liu, J. Wang, S. Kumar, and S.-F. Chang. Hashing with
graphs. In Proceedings of the 28th international conference
on machine learning (ICML-11), pages 1–8, 2011.

[24] X. Liu, T. Xia, J. Wang, and Y. Lin.

Fully convolu-
tional attention localization networks: Efﬁcient attention
arXiv preprint
localization for ﬁne-grained recognition.
arXiv:1603.06765, 2016.

[25] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2015.

[26] M.-E. Nilsback and A. Zisserman. A visual vocabulary for
ﬂower classiﬁcation. In Computer Vision and Pattern Recog-
nition, 2006 IEEE Computer Society Conference on, vol-
ume 2, pages 1447–1454. IEEE, 2006.

[27] M. Norouzi and D. M. Blei. Minimal loss hashing for com-
pact binary codes. In Proceedings of the 28th international
conference on machine learning (ICML-11), pages 353–360,
2011.

[28] M. Norouzi, D. J. Fleet, and R. R. Salakhutdinov. Hamming
distance metric learning. In Advances in neural information
processing systems, pages 1061–1069, 2012.

[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards
real-time object detection with region proposal networks. In
Advances in neural information processing systems, pages
91–99, 2015.

[45] F. Zhao, Y. Huang, L. Wang, and T. Tan. Deep semantic rank-
In Pro-
ing based hashing for multi-label image retrieval.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1556–1564, 2015.

[46] L. Zhao, X. Li, J. Wang, and Y. Zhuang. Deeply-learned part-
aligned representations for person re-identiﬁcation. arXiv
preprint arXiv:1707.07256, 2017.

[47] L. Zheng, Y. Huang, H. Lu, and Y. Yang. Pose invariant
embedding for deep person re-identiﬁcation. arXiv preprint
arXiv:1701.07732, 2017.

[48] F. Zhu, X. Kong, L. Zheng, H. Fu, and Q. Tian. Part-based
deep hashing for large-scale person re-identiﬁcation. IEEE
Transactions on Image Processing, 2017.

[49] H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing net-
work for efﬁcient similarity retrieval. In AAAI, pages 2415–
2421, 2016.

[30] R. Salakhutdinov and G. Hinton. Semantic hashing. Inter-
national Journal of Approximate Reasoning, 50(7):969–978,
2009.

[31] F. Shen, C. Shen, W. Liu, and H. Tao Shen. Supervised
In Proceedings of the IEEE Conference
discrete hashing.
on Computer Vision and Pattern Recognition, pages 37–45,
2015.

[32] Y. Shen, L. Liu, L. Shao, and J. Song. Deep binaries: En-
coding semantic-rich cues for efﬁcient textual-visual cross
retrieval. In Computer Vision (ICCV), 2017 IEEE Interna-
tional Conference on, pages 4117–4126. IEEE, 2017.
[33] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.

[34] S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery
In Computer Vision–

of mid-level discriminative patches.
ECCV 2012, pages 73–86. Springer, 2012.

[35] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical re-
port, 2011.

[36] Y. Wang, J. Choi, V. Morariu, and L. S. Davis. Mining dis-
criminative triplets of patches for ﬁne-grained classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1163–1172, 2016.

[37] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan. Supervised hash-
ing for image retrieval via image representation learning. In
AAAI, volume 1, pages 2156–2162, 2014.

[38] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, and Z. Zhang.
The application of two-level attention models in deep convo-
lutional neural network for ﬁne-grained image classiﬁcation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 842–850, 2015.

[39] Y. Xu, L. Lin, W.-S. Zheng, and X. Liu. Human re-
identiﬁcation by matching compositional template with clus-
ter sampling. In proceedings of the IEEE International Con-
ference on Computer Vision, pages 3152–3159, 2013.
[40] H. Zhang, T. Xu, M. Elhoseiny, X. Huang, S. Zhang, A. El-
gammal, and D. Metaxas. Spda-cnn: Unifying semantic
part detection and abstraction for ﬁne-grained recognition.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1143–1152, 2016.

[41] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-
In Eu-
based r-cnns for ﬁne-grained category detection.
ropean conference on computer vision, pages 834–849.
Springer, 2014.

[42] P. Zhang, W. Zhang, W.-J. Li, and M. Guo. Supervised hash-
ing with latent factor models. In Proceedings of the 37th in-
ternational ACM SIGIR conference on Research & develop-
ment in information retrieval, pages 173–182. ACM, 2014.

[43] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang. Bit-
scalable deep hashing with regularized similarity learning for
image retrieval and person re-identiﬁcation. IEEE Transac-
tions on Image Processing, 24(12):4766–4779, 2015.
[44] B. Zhao, X. Wu, J. Feng, Q. Peng, and S. Yan. Diversiﬁed vi-
sual attention networks for ﬁne-grained object classiﬁcation.
arXiv preprint arXiv:1606.08572, 2016.

