8
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
8
9
9
7
0
.
4
0
8
1
:
v
i
X
r
a

Generating Natural Language Adversarial Examples

Moustafa Alzantot1∗, Yash Sharma2∗, Ahmed Elgohary3,
Bo-Jhang Ho1, Mani B. Srivastava1, Kai-Wei Chang1

1Department of Computer Science, University of California, Los Angeles (UCLA)
{malzantot, bojhang, mbs, kwchang}@ucla.edu
2Cooper Union sharma2@cooper.edu
3Computer Science Department, University of Maryland elgohary@cs.umd.edu

Abstract

Deep neural networks (DNNs) are vulnera-
ble to adversarial examples, perturbations to
correctly classiﬁed examples which can cause
the model to misclassify.
In the image do-
main, these perturbations are often virtually
indistinguishable to human perception, caus-
ing humans and state-of-the-art models to dis-
agree. However, in the natural language do-
main, small perturbations are clearly percep-
tible, and the replacement of a single word
can drastically alter the semantics of the doc-
ument. Given these challenges, we use a
black-box population-based optimization al-
gorithm to generate semantically and syntac-
tically similar adversarial examples that fool
well-trained sentiment analysis and textual en-
tailment models with success rates of 97% and
70%, respectively. We additionally demon-
strate that 92.3% of the successful sentiment
analysis adversarial examples are classiﬁed to
their original label by 20 human annotators,
and that the examples are perceptibly quite
similar. Finally, we discuss an attempt to use
adversarial training as a defense, but fail to
yield improvement, demonstrating the strength
and diversity of our adversarial examples. We
hope our ﬁndings encourage researchers to
pursue improving the robustness of DNNs in
the natural language domain.

1 Introduction

Recent research has found that deep neural net-
works (DNNs) are vulnerable to adversarial ex-
amples (Goodfellow et al., 2014; Szegedy et al.,
ex-
2013).
amples has been shown in image classiﬁca-
tion (Szegedy et al., 2013) and speech recogni-
tion (Carlini and Wagner, 2018).
In this work,
we demonstrate that adversarial examples can

The existence of adversarial

∗Moustafa Alzantot and Yash Sharma contribute equally

to this work.

lan-
be constructed in the context of natural
guage. Using a black-box population-based op-
timization algorithm, we successfully generate
both semantically and syntactically similar adver-
sarial examples against models trained on both
the IMDB (Maas et al., 2011) sentiment analysis
task and the Stanford Natural Language Inference
(SNLI) (Bowman et al., 2015) textual entailment
task. In addition, we validate that the examples are
both correctly classiﬁed by human evaluators and
similar to the original via a human study. Finally,
we attempt to defend against said adversarial at-
tack using adversarial training, but fail to yield any
robustness, demonstrating the strength and diver-
sity of the generated adversarial examples.

Our results show that by minimizing the seman-
tic and syntactic dissimilarity, an attacker can per-
turb examples such that humans correctly classify,
but high-performing models misclassify. We are
open-sourcing our attack1 to encourage research
in training DNNs robust to adversarial attacks in
the natural language domain.

2 Natural Language Adversarial

Examples

have

been

examples

distortion

explored
Adversarial
primarily in the image recognition domain.
Examples have been generated through solv-
ing an optimization problem,
attempting to
induce misclassiﬁcation while minimizing the
2013;
perceptual
Carlini and Wagner, 2017; Chen et al., 2017b;
Sharma and Chen, 2017). Due to the computa-
tional cost of such approaches, fast methods were
introduced which, either in one-step or iteratively,
shift all pixels simultaneously until a distortion
is reached (Goodfellow et al., 2014;
constraint
Kurakin et al., 2016; Madry et al., 2017). Nearly

(Szegedy et al.,

1https://github.com/nesl/nlp_adversarial_examples

all popular methods are gradient-based.

Such methods, however, rely on the fact that
adding small perturbations to many pixels in the
image will not have a noticeable effect on a human
viewer. This approach obviously does not transfer
to the natural language domain, as all changes are
perceptible. Furthermore, unlike continuous im-
age pixel values, words in a sentence are discrete
tokens. Therefore, it is not possible to compute the
gradient of the network loss function with respect
to the input words. A straightforward workaround
is to project input sentences into a continuous
space (e.g. word embeddings) and consider this as
the model input. However, this approach also fails
because it still assumes that replacing every word
with words nearby in the embedding space will not
be noticeable. Replacing words without account-
ing for syntactic coherence will certainly lead to
improperly constructed sentences which will look
odd to the reader.

Relative to the image domain, little work has
been pursued for generating natural language ad-
versarial examples. Given the difﬁculty in gen-
erating semantics-preserving perturbations, dis-
tracting sentences have been added to the in-
put document in order to induce misclassiﬁca-
tion (Jia and Liang, 2017).
In our work, we
attempt
to generate semantically and syntac-
tically similar adversarial examples, via word
replacements, resolving the aforementioned is-
sues. Minimizing the number of word replace-
ments necessary to induce misclassiﬁcation has
been studied in previous work (Papernot et al.,
2016b), however without consideration given
to semantics or syntactics, yielding incoher-
ent generated examples.
In recent work,
there have been a few attempts at generat-
ing adversarial examples for language tasks by
using back-translation (Iyyer et al., 2018), ex-
ploiting machine-generated rules (Ribeiro et al.,
2018), and searching in underlying semantic
In addition, while
space (Zhao et al., 2018).
preparing our submission, we became aware of
recent work which target a similar contribu-
tion (Kuleshov et al., 2018; Ebrahimi et al., 2018).
We treat these contributions as parallel work.

3 Attack Design

3.1 Threat model
We assume the attacker has black-box access to
the target model; the attacker is not aware of the
model architecture, parameters, or training data,

and is only capable of querying the target model
with supplied inputs and obtaining the output pre-
dictions and their conﬁdence scores. This set-
ting has been extensively studied in the image do-
main (Papernot et al., 2016a; Chen et al., 2017a;
Alzantot et al., 2018), but has yet to be explored
in the context of natural language.

3.2 Algorithm
To avoid the limitations of gradient-based attack
methods, we design an algorithm for constructing
adversarial examples with the following goals in
mind. We aim to minimize the number of modiﬁed
words between the original and adversarial exam-
ples, but only perform modiﬁcations which retain
semantic similarity with the original and syntactic
coherence. To achieve these goals, instead of rely-
ing on gradient-based optimization, we developed
an attack algorithm that exploits population-based
gradient-free optimization via genetic algorithms.
An added beneﬁt of using gradient-free opti-
mization is enabling use in the black-box case;
gradient-reliant algorithms are inapplicable in this
case, as they are dependent on the model be-
ing differentiable and the internals being accessi-
ble (Papernot et al., 2016b; Ebrahimi et al., 2018).
Genetic algorithms are inspired by the process
of natural selection, iteratively evolving a popu-
lation of candidate solutions towards better solu-
tions. The population of each iteration is a called a
generation. In each generation, the quality of pop-
ulation members is evaluated using a ﬁtness func-
tion. “Fitter” solutions are more likely to be se-
lected for breeding the next generation. The next
generation is generated through a combination of
crossover and mutation. Crossover is the pro-
cess of taking more than one parent solution and
producing a child solution from them; it is anal-
ogous to reproduction and biological crossover.
Mutation is done in order to increase the diver-
sity of population members and provide better ex-
ploration of the search space. Genetic algorithms
are known to perform well in solving combinato-
rial optimization problems (Anderson and Ferris,
1994; M¨uhlenbein, 1989), and due to employing
a population of candidate solutions, these algo-
rithms can ﬁnd successful adversarial examples
with fewer modiﬁcations.

Perturb Subroutine:
In order to explain our
algorithm, we ﬁrst
introduce the subroutine
Perturb. This subroutine accepts an input sen-

tence xcur which can be either a modiﬁed sentence
or the same as xorig. It randomly selects a word w
in the sentence xcur and then selects a suitable re-
placement word that has similar semantic mean-
ing, ﬁts within the surrounding context, and in-
creases the target label prediction score.
In order to select
Perturb applies the following steps:

the best replacement word,

Algorithm 1 Finding adversarial examples

for i = 1, ..., S in population do

i ← Perturb(xorig, target)
P 0

for g = 1, 2...G generations do

for i = 1, ..., S in population do

)target

= f (P g−1
F g−1
i
i
xadv = P g−1
arg maxj F g−1
if arg maxc f (xadv)c == t then

j

• Computes the N nearest neighbors of the se-
lected word according to the distance in the
GloVe embedding space (Pennington et al.,
2014). We used euclidean distance, as we
did not see noticeable improvement using
cosine. We ﬁlter out candidates with dis-
tance to the selected word greater than δ.
We use the counter-ﬁtting method presented
in (Mrkˇsi´c et al., 2016) to post-process the
adversary’s GloVe vectors to ensure that the
nearest neighbors are synonyms. The result-
ing embedding is independent of the embed-
dings used by victim models.

• Second, we use the Google 1 billion words
(Chelba et al., 2013) to ﬁl-
language model
ter out words that do not ﬁt within the context
surrounding the word w in xcur. We do so by
ranking the candidate words based on their
language model scores when ﬁt within the re-
placement context, and keeping only the top
K words with the highest scores.

• From the remaining set of words, we pick the
one that will maximize the target label pre-
diction probability when it replaces the word
w in xcur.

• Finally, the selected word is inserted in place
of w, and Perturb returns the resulting sen-
tence.

The selection of which word to replace in the
input sentence is done by random sampling with
probabilities proportional to the number of neigh-
bors each word has within Euclidean distance δ in
the counter-ﬁtted embedding space, encouraging
the solution set to be large enough for the algo-
rithm to make appropriate modiﬁcations. We ex-
clude common articles and prepositions (e.g. a, to)
from being selected for replacement.

Optimization Procedure: The optimization al-
gorithm can be seen in Algorithm 1. The algo-
rithm starts by creating the initial generation P 0 of
size S by calling the Perturb subroutine S times
to create a set of distinct modiﬁcations to the orig-
inal sentence. Then, the ﬁtness of each popula-

return xadv ⊲ {Found successful attack}

else
P g
1 = {xadv}
p = N ormalize(F g−1)
for i = 2, ..., S in population do

Sample parent1 from P g−1 with probs p
Sample parent2 from P g−1 with probs p
child = Crossover(parent1, parent2)
childmut = Perturb(child, target)
P g

i = {childmut}

tion member in the current generation is computed
as the target label prediction probability, found by
querying the victim model function f . If a pop-
ulation member’s predicted label is equal to the
target label, the optimization is complete. Other-
wise, pairs of population members from the cur-
rent generation are randomly sampled with prob-
ability proportional to their ﬁtness values. A new
child sentence is then synthesized from a pair of
parent sentences by independently sampling from
the two using a uniform distribution. Finally, the
Perturb subroutine is applied to the resulting
children.

4 Experiments

To evaluate our attack method, we trained mod-
els for the sentiment analysis and textual en-
tailment classiﬁcation tasks. For both models,
each word in the input sentence is ﬁrst projected
into a ﬁxed 300-dimensional vector space using
GloVe (Pennington et al., 2014). Each of the mod-
els used are based on popular open-source bench-
marks, and can be found in the following reposito-
ries23. Model descriptions are given below.

Sentiment Analysis: We trained a sentiment
analysis model using the IMDB dataset of movie
reviews (Maas et al., 2011). The IMDB dataset
consists of 25,000 training examples and 25,000
test examples. The LSTM model is composed of

2https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py
3https://github.com/Smerity/keras_snli/blob/master/snli_rnn.py

Original Text Prediction = Negative. (Conﬁdence = 78.0%)
This movie had terrible acting, terrible plot, and terrible choice of actors. (Leslie Nielsen ...come on!!!)
the one part I considered slightly funny was the battling FBI/CIA agents, but because the audience was
mainly kids they didn’t understand that theme.
Adversarial Text Prediction = Positive. (Conﬁdence = 59.8%)
This movie had horriﬁc acting, horriﬁc plot, and horrifying choice of actors. (Leslie Nielsen ...come
on!!!) the one part I regarded slightly funny was the battling FBI/CIA agents, but because the audience
was mainly youngsters they didn’t understand that theme.

Table 1: Example of attack results for the sentiment analysis task. Modiﬁed words are highlighted in green and
red for the original and adversarial texts, respectively.

Original Text Prediction: Entailment (Conﬁdence = 86%)
Premise: A runner wearing purple strives for the ﬁnish line.
Hypothesis: A runner wants to head for the ﬁnish line.
Adversarial Text Prediction: Contradiction (Conﬁdence = 43%)
Premise: A runner wearing purple strives for the ﬁnish line.
Hypothesis: A racer wants to head for the ﬁnish line.

Table 2: Example of attack results for the textual entailment task. Modiﬁed words are highlighted in green and red
for the original and adversarial texts, respectively.

Sentiment Analysis

Textual Entailment

% success % modiﬁed % success % modiﬁed

Perturb baseline
Genetic attack

52%
97%

19%
14.7%

–
70%

–
23%

Table 3: Comparison between the attack success rate and mean percentage of modiﬁcations required by the genetic
attack and perturb baseline for the two tasks.

128 units, and the outputs across all time steps are
averaged and fed to the output layer. The test accu-
racy of the model is 90%, which is relatively close
to the state-of-the-art results on this dataset.

Textual Entailment: We trained a textual en-
tailment model using the Stanford Natural Lan-
guage Inference (SNLI) corpus (Bowman et al.,
2015). The model passes the input through a
ReLU “translation” layer (Bowman et al., 2015),
which encodes the premise and hypothesis sen-
tences by performing a summation over the word
embeddings, concatenates the two sentence em-
beddings, and ﬁnally passes the output through 3
600-dimensional ReLU layers before feeding it to
a 3-way softmax. The model predicts whether the
premise sentence entails, contradicts or is neutral
to the hypothesis sentence. The test accuracy of
the model is 83% which is also relatively close to
the state-of-the-art (Chen et al., 2017c).
4.1 Attack Evaluation Results
We randomly sampled 1000, and 500 correctly
classiﬁed examples from the test sets of the two
tasks to evaluate our algorithm. Correctly classi-
ﬁed examples were chosen to limit the accuracy

levels of the victim models from confounding our
results. For the sentiment analysis task, the at-
tacker aims to divert the prediction result from
positive to negative, and vice versa. For the tex-
tual entailment task, the attacker is only allowed
to modify the hypothesis, and aims to divert the
prediction result from ‘entailment’ to ‘contradic-
tion’, and vice versa. We limit the attacker to
maximum G = 20 iterations, and ﬁx the hyper-
parameter values to S = 60, N = 8, K = 4, and
δ = 0.5. We also ﬁxed the maximum percentage
of allowed changes to the document to be 20% and
25% for the two tasks, respectively. If increased,
the success rate would increase but the mean qual-
ity would decrease. If the attack does not succeed
within the iterations limit or exceeds the speciﬁed
threshold, it is counted as a failure.

Sample outputs produced by our attack are
shown in tables 4 and 5. Additional outputs can
be found in the supplementary material. Table 3
shows the attack success rate and mean percent-
age of modiﬁed words on each task. We compare
to the Perturb baseline, which greedily applies
the Perturb subroutine, to validate the use of

population-based optimization. As can be seen
from our results, we are able to achieve high suc-
cess rate with a limited number of modiﬁcations
on both tasks. In addition, the genetic algorithm
signiﬁcantly outperformed the Perturb baseline
in both success rate and percentage of words mod-
iﬁed, demonstrating the additional beneﬁt yielded
by using population-based optimization. Testing
using a single TitanX GPU, for sentiment analy-
sis and textual entailment, we measured average
runtimes on success to be 43.5 and 5 seconds per
example, respectively. The high success rate and
reasonable runtimes demonstrate the practicality
of our approach, even when scaling to long sen-
tences, such as those found in the IMDB dataset.

Speaking of which, our success rate on textual
entailment is lower due to the large disparity in
sentence length. On average, hypothesis sentences
in the SNLI corpus are 9 words long, which is
very short compared to IMDB (229 words, lim-
ited to 100 for experiments). With sentences that
short, applying successful perturbations becomes
much harder, however we were still able to achieve
a success rate of 70%. For the same reason, we
didn’t apply the Perturb baseline on the textual
entailment task, as the Perturb baseline fails to
achieve any success under the limits of the maxi-
mum allowed changes constraint.

4.2 User study

We performed a user study on the sentiment anal-
ysis task with 20 volunteers to evaluate how per-
ceptible our adversarial perturbations are. Note
that
the number of participating volunteers is
signiﬁcantly larger than used in previous stud-
ies (Jia and Liang, 2017; Ebrahimi et al., 2018).
The user study was composed of two parts. First,
we presented 100 adversarial examples to the par-
ticipants and asked them to label the sentiment of
the text (i.e., positive or negative.) 92.3% of the
responses matched the original text sentiment, in-
dicating that our modiﬁcation did not signiﬁcantly
affect human judgment on the text sentiment. Sec-
ond, we prepared 100 questions, each question in-
cludes the original example and the corresponding
adversarial example in a pair. Participants were
asked to judge the similarity of each pair on a scale
from 1 (very similar) to 4 (very different). The av-
erage rating is 2.23 ± 0.25, which shows the per-
ceived difference is also small.

4.3 Adversarial Training
The results demonstrated in section 4.1 raise the
following question: How can we defend against
these attacks? We performed a preliminary exper-
iment to see if adversarial training (Madry et al.,
2017), the only effective defense in the image do-
main, can be used to lower the attack success rate.
We generated 1000 adversarial examples on the
cleanly trained sentiment analysis model using the
IMDB training set, appended them to the existing
training set, and used the updated dataset to ad-
versarially train a model from scratch. We found
that adversarial training provided no additional ro-
bustness beneﬁt in our experiments using the test
set, despite the fact that the model achieves near
100% accuracy classifying adversarial examples
included in the training set. These results demon-
strate the diversity in the perturbations generated
by our attack algorithm, and illustrates the difﬁ-
culty in defending against adversarial attacks. We
hope these results inspire further work in increas-
ing the robustness of natural language models.

5 Conclusion
We demonstrate that despite the difﬁculties in gen-
erating imperceptible adversarial examples in the
natural language domain, semantically and syntac-
tically similar adversarial examples can be crafted
using a black-box population-based optimization
algorithm, yielding success on both the sentiment
analysis and textual entailment tasks. Our human
study validated that the generated examples were
indeed adversarial and perceptibly quite similar.
We hope our work encourages researchers to pur-
sue improving the robustness of DNNs in the nat-
ural language domain.

Acknowledgement
This research was supported in part by the U.S.
Army Research Laboratory and the UK Ministry
of Defence under Agreement Number W911NF-
16-3-0001, the National Science Foundation under
award # CNS-1705135, OAC-1640813, and IIS-
1760523, and the NIH Center of Excellence for
Mobile Sensor Data-to-Knowledge (MD2K) un-
der award 1-U54EB020404-01. Ahmed Elgohary
is funded by an IBM PhD Fellowship. Any ﬁnd-
ings in this material are those of the author(s) and
do not reﬂect the views of any of the above fund-
ing agencies. The U.S. and U.K. Governments are
authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copy-
right notation hereon.

References

M. Alzantot, Y. Sharma, S. Chakraborty, and M. Srivas-
tava. 2018. Genattack: Practical black-box attacks
arXiv preprint
with gradient-free optimization.
arXiv:1805.11090.

Edward J Anderson and Michael C Ferris. 1994. Ge-
netic algorithms for combinatorial optimization: the
assemble line balancing problem. ORSA Journal on
Computing, 6(2):161–173.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

N. Carlini and D. Wagner. 2017. Towards evaluating
the robustness of neural networks. arXiv preprint
arXiv:1608.04644.

Nicholas Carlini and David Wagner. 2018. Audio ad-
versarial examples: Targeted attacks on speech-to-
text. arXiv preprint arXiv:1801.01944.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.

P. Chen, H Zhang, Y. Sharma, J. Yi, and C. Hseih.
2017a.
Zoo: Zeroth order optimization based
black-box attacks to deep neural networks with-
arXiv preprint
out
arXiv:1708.03999.

training substitute models.

P. Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh.
2017b. Ead: Elastic-net attacks to deep neural
networks via adversarial examples. arXiv preprint
arXiv:1709.0414.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017c. Enhanced lstm for
In Proceedings of the
natural language inference.
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1657–1668.

J. Ebrahimi, A. Rao, D. Lowd, and D. Dou. 2018. Hot-
ﬂip: White-box adversarial examples for text classi-
ﬁcation. ACL’18; arXiv preprint arXiv:1712.06751.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adver-
sarial examples. arXiv preprint arXiv:1412.6572.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of NAACL.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328.

V. Kuleshov, S. Thakoor, T. Lau, and S. Ermon. 2018.
Adversarial examples for natural language classiﬁ-
cation problems. OpenReview submission OpenRe-
view:r1QZ3zbAZ.

A. Kurakin, I. Goodfellow, and S. Bengio. 2016. Ad-
versarial machine learning at scale. ICLR’17; arXiv
preprint arXiv:1611.01236.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.

A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. 2017.
Towards deep learning mod-
els resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083.

Nikola Mrkˇsi´c, Diarmuid O S´eaghdha, Blaise Thom-
son, Milica Gaˇsi´c, Lina Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Counter-ﬁtting word vec-
Steve Young. 2016.
arXiv preprint
tors to linguistic constraints.
arXiv:1603.00892.

Heinz M¨uhlenbein. 1989. Parallel genetic algorithms,
population genetics and combinatorial optimization.
In Workshop on Parallel Processing: Logic, Organi-
zation, and Technology, pages 398–406. Springer.

N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Ce-
lik, and A. Swami. 2016a.
Practical black-box
attacks against machine learning. arXiv preprint
arXiv:1602.02697.

N. Papernot, P. McDaniel, A. Swami, and R. Ha-
rang. 2016b. Crafting adversarial input sequences
arXiv preprint
for recurrent neural networks.
arXiv:1604.08275.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
In Proceed-
ial rules for debugging nlp models.
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 856–865.

Y. Sharma and P. Y. Chen. 2017. Attacking the madry
defense model with l1-based adversarial examples.
arXiv preprint arXiv:1710.10733.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2013.
Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199.

Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.
arXiv

Generating natural adversarial examples.
preprint arXiv:1710.11342.

Supplemental Materials: Generating Natural Language Adversarial

Examples

Additional Sentiment Analysis Results

Table 4 shows an additional set of attack results against the sentiment analysis model described in our
paper.

Original Text Prediction = Positive. (Conﬁdence = 78%)
The promise of Martin Donovan playing Jesus was, quite honestly, enough to get me to see the ﬁlm.
Deﬁnitely worthwhile; clever and funny without overdoing it. The low quality ﬁlming was probably
an appropriate effect but ended up being a little too jarring, and the ending sounded more like a PBS
program than Hartley. Still, too many memorable lines and great moments for me to judge it harshly.
Adversarial Text Prediction = Negative. (Conﬁdence = 59.9%)
The promise of Martin Donovan playing Jesus was, utterly frankly, enough to get me to see the ﬁlm.
Deﬁnitely worthwhile; clever and funny without overdoing it. The low quality ﬁlming was presumably
an appropriate effect but ended up being a little too jarring, and the ending sounded more like a PBS
program than Hartley. Still, too many memorable lines and great moments for me to judge it harshly.

Original Text Prediction = Negative. (Conﬁdence = 74.30%)
Some sort of accolade must be given to ‘Hellraiser: Bloodline’.
It’s actually out full-mooned Full
Moon. It bears all the marks of, say, your ‘demonic toys’ or ‘puppet master’ series, without their dopey,
uh, charm? Full Moon can get away with silly product because they know it’s silly. These Hellraiser
things, man, do they ever take themselves seriously. This increasingly stupid franchise (though not
nearly as stupid as I am for having watched it) once made up for its low budgets by being stylish. Now
it’s just ish.
Adversarial Text Prediction = Positive. (Conﬁdence = 51.03%)
Some kind of accolade must be given to ‘Hellraiser: Bloodline’. it’s truly out full-mooned Full Moon. It
bears all the marks of, say, your ‘demonic toys’ or ‘puppet master’ series, without their silly, uh, charm?
Full Moon can get away with daft product because they know it’s silly. These Hellraiser things, man, do
they ever take themselves seriously. This steadily daft franchise (whilst not nearly as daft as i am for
having witnessed it) once made up for its low budgets by being stylish. Now it’s just ish.

Original Text Prediction = Negative. (Conﬁdence = 50.53%)
Thinly-cloaked retelling of the garden-of-eden story – nothing new, nothing shocking, although I feel
that is what the ﬁlmmakers were going for. The idea is trite. Strong performance from Daisy Eagan,
that’s about it. I believed she was 13, and I was interested in her character, the rest left me cold.
Adversarial Text Prediction = Positive. (Conﬁdence = 63.04%)
Thinly-cloaked retelling of the garden-of-eden story – nothing new, nothing shocking, although I feel
that is what the ﬁlmmakers were going for. The idea is petty. Strong performance from Daisy Eagan,
that’s about it. I believed she was 13, and I was interested in her character, the rest left me cold.

Table 4: Example of attack results against the sentiment analysis model. Modiﬁed words are highlighted in green
and red for the original and adversarial texts, respectively.

Additional Textual Entailment Results

Table 5 shows an additional set of attack results against the textual entailment model described in our
paper.

Original Text Prediction: Contradiction (Conﬁdence = 91%)
Premise: A man and a woman stand in front of a Christmas tree contemplating a single thought.
Hypothesis: Two people talk loudly in front of a cactus.
Adversarial Text Prediction: Entailment (Conﬁdence = 51%)
Premise: A man and a woman stand in front of a Christmas tree contemplating a single thought.
Hypothesis: Two humans chitchat loudly in front of a cactus.

Original Text Prediction: Contradiction (Conﬁdence = 94%)
Premise: A young girl wearing yellow shorts and a white tank top using a cane pole to ﬁsh at a small
pond.
Hypothesis: A girl wearing a dress looks off a cliff.
Adversarial Text Prediction: Entailment (Conﬁdence = 40%)
Premise: A young girl wearing yellow shorts and a white tank top using a cane pole to ﬁsh at a small
pond.
Hypothesis: A girl wearing a skirt looks off a ravine.

Original Text Prediction: Entailment (Conﬁdence = 86%)
Premise: A large group of protesters are walking down the street with signs.
Hypothesis: Some people are holding up signs of protest in the street.
Adversarial Text Prediction: Contradiction (Conﬁdence = 43%)
Premise: A large group of protesters are walking down the street with signs.
Hypothesis: Some people are holding up signals of protest in the street.

Table 5: Example of attack results against the textual entailment model. Modiﬁed words are highlighted in green
and red for the original and adversarial texts, respectively.

8
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
8
9
9
7
0
.
4
0
8
1
:
v
i
X
r
a

Generating Natural Language Adversarial Examples

Moustafa Alzantot1∗, Yash Sharma2∗, Ahmed Elgohary3,
Bo-Jhang Ho1, Mani B. Srivastava1, Kai-Wei Chang1

1Department of Computer Science, University of California, Los Angeles (UCLA)
{malzantot, bojhang, mbs, kwchang}@ucla.edu
2Cooper Union sharma2@cooper.edu
3Computer Science Department, University of Maryland elgohary@cs.umd.edu

Abstract

Deep neural networks (DNNs) are vulnera-
ble to adversarial examples, perturbations to
correctly classiﬁed examples which can cause
the model to misclassify.
In the image do-
main, these perturbations are often virtually
indistinguishable to human perception, caus-
ing humans and state-of-the-art models to dis-
agree. However, in the natural language do-
main, small perturbations are clearly percep-
tible, and the replacement of a single word
can drastically alter the semantics of the doc-
ument. Given these challenges, we use a
black-box population-based optimization al-
gorithm to generate semantically and syntac-
tically similar adversarial examples that fool
well-trained sentiment analysis and textual en-
tailment models with success rates of 97% and
70%, respectively. We additionally demon-
strate that 92.3% of the successful sentiment
analysis adversarial examples are classiﬁed to
their original label by 20 human annotators,
and that the examples are perceptibly quite
similar. Finally, we discuss an attempt to use
adversarial training as a defense, but fail to
yield improvement, demonstrating the strength
and diversity of our adversarial examples. We
hope our ﬁndings encourage researchers to
pursue improving the robustness of DNNs in
the natural language domain.

1 Introduction

Recent research has found that deep neural net-
works (DNNs) are vulnerable to adversarial ex-
amples (Goodfellow et al., 2014; Szegedy et al.,
ex-
2013).
amples has been shown in image classiﬁca-
tion (Szegedy et al., 2013) and speech recogni-
tion (Carlini and Wagner, 2018).
In this work,
we demonstrate that adversarial examples can

The existence of adversarial

∗Moustafa Alzantot and Yash Sharma contribute equally

to this work.

lan-
be constructed in the context of natural
guage. Using a black-box population-based op-
timization algorithm, we successfully generate
both semantically and syntactically similar adver-
sarial examples against models trained on both
the IMDB (Maas et al., 2011) sentiment analysis
task and the Stanford Natural Language Inference
(SNLI) (Bowman et al., 2015) textual entailment
task. In addition, we validate that the examples are
both correctly classiﬁed by human evaluators and
similar to the original via a human study. Finally,
we attempt to defend against said adversarial at-
tack using adversarial training, but fail to yield any
robustness, demonstrating the strength and diver-
sity of the generated adversarial examples.

Our results show that by minimizing the seman-
tic and syntactic dissimilarity, an attacker can per-
turb examples such that humans correctly classify,
but high-performing models misclassify. We are
open-sourcing our attack1 to encourage research
in training DNNs robust to adversarial attacks in
the natural language domain.

2 Natural Language Adversarial

Examples

have

been

examples

distortion

explored
Adversarial
primarily in the image recognition domain.
Examples have been generated through solv-
ing an optimization problem,
attempting to
induce misclassiﬁcation while minimizing the
2013;
perceptual
Carlini and Wagner, 2017; Chen et al., 2017b;
Sharma and Chen, 2017). Due to the computa-
tional cost of such approaches, fast methods were
introduced which, either in one-step or iteratively,
shift all pixels simultaneously until a distortion
is reached (Goodfellow et al., 2014;
constraint
Kurakin et al., 2016; Madry et al., 2017). Nearly

(Szegedy et al.,

1https://github.com/nesl/nlp_adversarial_examples

all popular methods are gradient-based.

Such methods, however, rely on the fact that
adding small perturbations to many pixels in the
image will not have a noticeable effect on a human
viewer. This approach obviously does not transfer
to the natural language domain, as all changes are
perceptible. Furthermore, unlike continuous im-
age pixel values, words in a sentence are discrete
tokens. Therefore, it is not possible to compute the
gradient of the network loss function with respect
to the input words. A straightforward workaround
is to project input sentences into a continuous
space (e.g. word embeddings) and consider this as
the model input. However, this approach also fails
because it still assumes that replacing every word
with words nearby in the embedding space will not
be noticeable. Replacing words without account-
ing for syntactic coherence will certainly lead to
improperly constructed sentences which will look
odd to the reader.

Relative to the image domain, little work has
been pursued for generating natural language ad-
versarial examples. Given the difﬁculty in gen-
erating semantics-preserving perturbations, dis-
tracting sentences have been added to the in-
put document in order to induce misclassiﬁca-
tion (Jia and Liang, 2017).
In our work, we
attempt
to generate semantically and syntac-
tically similar adversarial examples, via word
replacements, resolving the aforementioned is-
sues. Minimizing the number of word replace-
ments necessary to induce misclassiﬁcation has
been studied in previous work (Papernot et al.,
2016b), however without consideration given
to semantics or syntactics, yielding incoher-
ent generated examples.
In recent work,
there have been a few attempts at generat-
ing adversarial examples for language tasks by
using back-translation (Iyyer et al., 2018), ex-
ploiting machine-generated rules (Ribeiro et al.,
2018), and searching in underlying semantic
In addition, while
space (Zhao et al., 2018).
preparing our submission, we became aware of
recent work which target a similar contribu-
tion (Kuleshov et al., 2018; Ebrahimi et al., 2018).
We treat these contributions as parallel work.

3 Attack Design

3.1 Threat model
We assume the attacker has black-box access to
the target model; the attacker is not aware of the
model architecture, parameters, or training data,

and is only capable of querying the target model
with supplied inputs and obtaining the output pre-
dictions and their conﬁdence scores. This set-
ting has been extensively studied in the image do-
main (Papernot et al., 2016a; Chen et al., 2017a;
Alzantot et al., 2018), but has yet to be explored
in the context of natural language.

3.2 Algorithm
To avoid the limitations of gradient-based attack
methods, we design an algorithm for constructing
adversarial examples with the following goals in
mind. We aim to minimize the number of modiﬁed
words between the original and adversarial exam-
ples, but only perform modiﬁcations which retain
semantic similarity with the original and syntactic
coherence. To achieve these goals, instead of rely-
ing on gradient-based optimization, we developed
an attack algorithm that exploits population-based
gradient-free optimization via genetic algorithms.
An added beneﬁt of using gradient-free opti-
mization is enabling use in the black-box case;
gradient-reliant algorithms are inapplicable in this
case, as they are dependent on the model be-
ing differentiable and the internals being accessi-
ble (Papernot et al., 2016b; Ebrahimi et al., 2018).
Genetic algorithms are inspired by the process
of natural selection, iteratively evolving a popu-
lation of candidate solutions towards better solu-
tions. The population of each iteration is a called a
generation. In each generation, the quality of pop-
ulation members is evaluated using a ﬁtness func-
tion. “Fitter” solutions are more likely to be se-
lected for breeding the next generation. The next
generation is generated through a combination of
crossover and mutation. Crossover is the pro-
cess of taking more than one parent solution and
producing a child solution from them; it is anal-
ogous to reproduction and biological crossover.
Mutation is done in order to increase the diver-
sity of population members and provide better ex-
ploration of the search space. Genetic algorithms
are known to perform well in solving combinato-
rial optimization problems (Anderson and Ferris,
1994; M¨uhlenbein, 1989), and due to employing
a population of candidate solutions, these algo-
rithms can ﬁnd successful adversarial examples
with fewer modiﬁcations.

Perturb Subroutine:
In order to explain our
algorithm, we ﬁrst
introduce the subroutine
Perturb. This subroutine accepts an input sen-

tence xcur which can be either a modiﬁed sentence
or the same as xorig. It randomly selects a word w
in the sentence xcur and then selects a suitable re-
placement word that has similar semantic mean-
ing, ﬁts within the surrounding context, and in-
creases the target label prediction score.
In order to select
Perturb applies the following steps:

the best replacement word,

Algorithm 1 Finding adversarial examples

for i = 1, ..., S in population do

i ← Perturb(xorig, target)
P 0

for g = 1, 2...G generations do

for i = 1, ..., S in population do

)target

= f (P g−1
F g−1
i
i
xadv = P g−1
arg maxj F g−1
if arg maxc f (xadv)c == t then

j

• Computes the N nearest neighbors of the se-
lected word according to the distance in the
GloVe embedding space (Pennington et al.,
2014). We used euclidean distance, as we
did not see noticeable improvement using
cosine. We ﬁlter out candidates with dis-
tance to the selected word greater than δ.
We use the counter-ﬁtting method presented
in (Mrkˇsi´c et al., 2016) to post-process the
adversary’s GloVe vectors to ensure that the
nearest neighbors are synonyms. The result-
ing embedding is independent of the embed-
dings used by victim models.

• Second, we use the Google 1 billion words
(Chelba et al., 2013) to ﬁl-
language model
ter out words that do not ﬁt within the context
surrounding the word w in xcur. We do so by
ranking the candidate words based on their
language model scores when ﬁt within the re-
placement context, and keeping only the top
K words with the highest scores.

• From the remaining set of words, we pick the
one that will maximize the target label pre-
diction probability when it replaces the word
w in xcur.

• Finally, the selected word is inserted in place
of w, and Perturb returns the resulting sen-
tence.

The selection of which word to replace in the
input sentence is done by random sampling with
probabilities proportional to the number of neigh-
bors each word has within Euclidean distance δ in
the counter-ﬁtted embedding space, encouraging
the solution set to be large enough for the algo-
rithm to make appropriate modiﬁcations. We ex-
clude common articles and prepositions (e.g. a, to)
from being selected for replacement.

Optimization Procedure: The optimization al-
gorithm can be seen in Algorithm 1. The algo-
rithm starts by creating the initial generation P 0 of
size S by calling the Perturb subroutine S times
to create a set of distinct modiﬁcations to the orig-
inal sentence. Then, the ﬁtness of each popula-

return xadv ⊲ {Found successful attack}

else
P g
1 = {xadv}
p = N ormalize(F g−1)
for i = 2, ..., S in population do

Sample parent1 from P g−1 with probs p
Sample parent2 from P g−1 with probs p
child = Crossover(parent1, parent2)
childmut = Perturb(child, target)
P g

i = {childmut}

tion member in the current generation is computed
as the target label prediction probability, found by
querying the victim model function f . If a pop-
ulation member’s predicted label is equal to the
target label, the optimization is complete. Other-
wise, pairs of population members from the cur-
rent generation are randomly sampled with prob-
ability proportional to their ﬁtness values. A new
child sentence is then synthesized from a pair of
parent sentences by independently sampling from
the two using a uniform distribution. Finally, the
Perturb subroutine is applied to the resulting
children.

4 Experiments

To evaluate our attack method, we trained mod-
els for the sentiment analysis and textual en-
tailment classiﬁcation tasks. For both models,
each word in the input sentence is ﬁrst projected
into a ﬁxed 300-dimensional vector space using
GloVe (Pennington et al., 2014). Each of the mod-
els used are based on popular open-source bench-
marks, and can be found in the following reposito-
ries23. Model descriptions are given below.

Sentiment Analysis: We trained a sentiment
analysis model using the IMDB dataset of movie
reviews (Maas et al., 2011). The IMDB dataset
consists of 25,000 training examples and 25,000
test examples. The LSTM model is composed of

2https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py
3https://github.com/Smerity/keras_snli/blob/master/snli_rnn.py

Original Text Prediction = Negative. (Conﬁdence = 78.0%)
This movie had terrible acting, terrible plot, and terrible choice of actors. (Leslie Nielsen ...come on!!!)
the one part I considered slightly funny was the battling FBI/CIA agents, but because the audience was
mainly kids they didn’t understand that theme.
Adversarial Text Prediction = Positive. (Conﬁdence = 59.8%)
This movie had horriﬁc acting, horriﬁc plot, and horrifying choice of actors. (Leslie Nielsen ...come
on!!!) the one part I regarded slightly funny was the battling FBI/CIA agents, but because the audience
was mainly youngsters they didn’t understand that theme.

Table 1: Example of attack results for the sentiment analysis task. Modiﬁed words are highlighted in green and
red for the original and adversarial texts, respectively.

Original Text Prediction: Entailment (Conﬁdence = 86%)
Premise: A runner wearing purple strives for the ﬁnish line.
Hypothesis: A runner wants to head for the ﬁnish line.
Adversarial Text Prediction: Contradiction (Conﬁdence = 43%)
Premise: A runner wearing purple strives for the ﬁnish line.
Hypothesis: A racer wants to head for the ﬁnish line.

Table 2: Example of attack results for the textual entailment task. Modiﬁed words are highlighted in green and red
for the original and adversarial texts, respectively.

Sentiment Analysis

Textual Entailment

% success % modiﬁed % success % modiﬁed

Perturb baseline
Genetic attack

52%
97%

19%
14.7%

–
70%

–
23%

Table 3: Comparison between the attack success rate and mean percentage of modiﬁcations required by the genetic
attack and perturb baseline for the two tasks.

128 units, and the outputs across all time steps are
averaged and fed to the output layer. The test accu-
racy of the model is 90%, which is relatively close
to the state-of-the-art results on this dataset.

Textual Entailment: We trained a textual en-
tailment model using the Stanford Natural Lan-
guage Inference (SNLI) corpus (Bowman et al.,
2015). The model passes the input through a
ReLU “translation” layer (Bowman et al., 2015),
which encodes the premise and hypothesis sen-
tences by performing a summation over the word
embeddings, concatenates the two sentence em-
beddings, and ﬁnally passes the output through 3
600-dimensional ReLU layers before feeding it to
a 3-way softmax. The model predicts whether the
premise sentence entails, contradicts or is neutral
to the hypothesis sentence. The test accuracy of
the model is 83% which is also relatively close to
the state-of-the-art (Chen et al., 2017c).
4.1 Attack Evaluation Results
We randomly sampled 1000, and 500 correctly
classiﬁed examples from the test sets of the two
tasks to evaluate our algorithm. Correctly classi-
ﬁed examples were chosen to limit the accuracy

levels of the victim models from confounding our
results. For the sentiment analysis task, the at-
tacker aims to divert the prediction result from
positive to negative, and vice versa. For the tex-
tual entailment task, the attacker is only allowed
to modify the hypothesis, and aims to divert the
prediction result from ‘entailment’ to ‘contradic-
tion’, and vice versa. We limit the attacker to
maximum G = 20 iterations, and ﬁx the hyper-
parameter values to S = 60, N = 8, K = 4, and
δ = 0.5. We also ﬁxed the maximum percentage
of allowed changes to the document to be 20% and
25% for the two tasks, respectively. If increased,
the success rate would increase but the mean qual-
ity would decrease. If the attack does not succeed
within the iterations limit or exceeds the speciﬁed
threshold, it is counted as a failure.

Sample outputs produced by our attack are
shown in tables 4 and 5. Additional outputs can
be found in the supplementary material. Table 3
shows the attack success rate and mean percent-
age of modiﬁed words on each task. We compare
to the Perturb baseline, which greedily applies
the Perturb subroutine, to validate the use of

population-based optimization. As can be seen
from our results, we are able to achieve high suc-
cess rate with a limited number of modiﬁcations
on both tasks. In addition, the genetic algorithm
signiﬁcantly outperformed the Perturb baseline
in both success rate and percentage of words mod-
iﬁed, demonstrating the additional beneﬁt yielded
by using population-based optimization. Testing
using a single TitanX GPU, for sentiment analy-
sis and textual entailment, we measured average
runtimes on success to be 43.5 and 5 seconds per
example, respectively. The high success rate and
reasonable runtimes demonstrate the practicality
of our approach, even when scaling to long sen-
tences, such as those found in the IMDB dataset.

Speaking of which, our success rate on textual
entailment is lower due to the large disparity in
sentence length. On average, hypothesis sentences
in the SNLI corpus are 9 words long, which is
very short compared to IMDB (229 words, lim-
ited to 100 for experiments). With sentences that
short, applying successful perturbations becomes
much harder, however we were still able to achieve
a success rate of 70%. For the same reason, we
didn’t apply the Perturb baseline on the textual
entailment task, as the Perturb baseline fails to
achieve any success under the limits of the maxi-
mum allowed changes constraint.

4.2 User study

We performed a user study on the sentiment anal-
ysis task with 20 volunteers to evaluate how per-
ceptible our adversarial perturbations are. Note
that
the number of participating volunteers is
signiﬁcantly larger than used in previous stud-
ies (Jia and Liang, 2017; Ebrahimi et al., 2018).
The user study was composed of two parts. First,
we presented 100 adversarial examples to the par-
ticipants and asked them to label the sentiment of
the text (i.e., positive or negative.) 92.3% of the
responses matched the original text sentiment, in-
dicating that our modiﬁcation did not signiﬁcantly
affect human judgment on the text sentiment. Sec-
ond, we prepared 100 questions, each question in-
cludes the original example and the corresponding
adversarial example in a pair. Participants were
asked to judge the similarity of each pair on a scale
from 1 (very similar) to 4 (very different). The av-
erage rating is 2.23 ± 0.25, which shows the per-
ceived difference is also small.

4.3 Adversarial Training
The results demonstrated in section 4.1 raise the
following question: How can we defend against
these attacks? We performed a preliminary exper-
iment to see if adversarial training (Madry et al.,
2017), the only effective defense in the image do-
main, can be used to lower the attack success rate.
We generated 1000 adversarial examples on the
cleanly trained sentiment analysis model using the
IMDB training set, appended them to the existing
training set, and used the updated dataset to ad-
versarially train a model from scratch. We found
that adversarial training provided no additional ro-
bustness beneﬁt in our experiments using the test
set, despite the fact that the model achieves near
100% accuracy classifying adversarial examples
included in the training set. These results demon-
strate the diversity in the perturbations generated
by our attack algorithm, and illustrates the difﬁ-
culty in defending against adversarial attacks. We
hope these results inspire further work in increas-
ing the robustness of natural language models.

5 Conclusion
We demonstrate that despite the difﬁculties in gen-
erating imperceptible adversarial examples in the
natural language domain, semantically and syntac-
tically similar adversarial examples can be crafted
using a black-box population-based optimization
algorithm, yielding success on both the sentiment
analysis and textual entailment tasks. Our human
study validated that the generated examples were
indeed adversarial and perceptibly quite similar.
We hope our work encourages researchers to pur-
sue improving the robustness of DNNs in the nat-
ural language domain.

Acknowledgement
This research was supported in part by the U.S.
Army Research Laboratory and the UK Ministry
of Defence under Agreement Number W911NF-
16-3-0001, the National Science Foundation under
award # CNS-1705135, OAC-1640813, and IIS-
1760523, and the NIH Center of Excellence for
Mobile Sensor Data-to-Knowledge (MD2K) un-
der award 1-U54EB020404-01. Ahmed Elgohary
is funded by an IBM PhD Fellowship. Any ﬁnd-
ings in this material are those of the author(s) and
do not reﬂect the views of any of the above fund-
ing agencies. The U.S. and U.K. Governments are
authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copy-
right notation hereon.

References

M. Alzantot, Y. Sharma, S. Chakraborty, and M. Srivas-
tava. 2018. Genattack: Practical black-box attacks
arXiv preprint
with gradient-free optimization.
arXiv:1805.11090.

Edward J Anderson and Michael C Ferris. 1994. Ge-
netic algorithms for combinatorial optimization: the
assemble line balancing problem. ORSA Journal on
Computing, 6(2):161–173.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

N. Carlini and D. Wagner. 2017. Towards evaluating
the robustness of neural networks. arXiv preprint
arXiv:1608.04644.

Nicholas Carlini and David Wagner. 2018. Audio ad-
versarial examples: Targeted attacks on speech-to-
text. arXiv preprint arXiv:1801.01944.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.

P. Chen, H Zhang, Y. Sharma, J. Yi, and C. Hseih.
2017a.
Zoo: Zeroth order optimization based
black-box attacks to deep neural networks with-
arXiv preprint
out
arXiv:1708.03999.

training substitute models.

P. Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh.
2017b. Ead: Elastic-net attacks to deep neural
networks via adversarial examples. arXiv preprint
arXiv:1709.0414.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017c. Enhanced lstm for
In Proceedings of the
natural language inference.
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1657–1668.

J. Ebrahimi, A. Rao, D. Lowd, and D. Dou. 2018. Hot-
ﬂip: White-box adversarial examples for text classi-
ﬁcation. ACL’18; arXiv preprint arXiv:1712.06751.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adver-
sarial examples. arXiv preprint arXiv:1412.6572.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of NAACL.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328.

V. Kuleshov, S. Thakoor, T. Lau, and S. Ermon. 2018.
Adversarial examples for natural language classiﬁ-
cation problems. OpenReview submission OpenRe-
view:r1QZ3zbAZ.

A. Kurakin, I. Goodfellow, and S. Bengio. 2016. Ad-
versarial machine learning at scale. ICLR’17; arXiv
preprint arXiv:1611.01236.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.

A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. 2017.
Towards deep learning mod-
els resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083.

Nikola Mrkˇsi´c, Diarmuid O S´eaghdha, Blaise Thom-
son, Milica Gaˇsi´c, Lina Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Counter-ﬁtting word vec-
Steve Young. 2016.
arXiv preprint
tors to linguistic constraints.
arXiv:1603.00892.

Heinz M¨uhlenbein. 1989. Parallel genetic algorithms,
population genetics and combinatorial optimization.
In Workshop on Parallel Processing: Logic, Organi-
zation, and Technology, pages 398–406. Springer.

N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Ce-
lik, and A. Swami. 2016a.
Practical black-box
attacks against machine learning. arXiv preprint
arXiv:1602.02697.

N. Papernot, P. McDaniel, A. Swami, and R. Ha-
rang. 2016b. Crafting adversarial input sequences
arXiv preprint
for recurrent neural networks.
arXiv:1604.08275.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
In Proceed-
ial rules for debugging nlp models.
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 856–865.

Y. Sharma and P. Y. Chen. 2017. Attacking the madry
defense model with l1-based adversarial examples.
arXiv preprint arXiv:1710.10733.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2013.
Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199.

Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.
arXiv

Generating natural adversarial examples.
preprint arXiv:1710.11342.

Supplemental Materials: Generating Natural Language Adversarial

Examples

Additional Sentiment Analysis Results

Table 4 shows an additional set of attack results against the sentiment analysis model described in our
paper.

Original Text Prediction = Positive. (Conﬁdence = 78%)
The promise of Martin Donovan playing Jesus was, quite honestly, enough to get me to see the ﬁlm.
Deﬁnitely worthwhile; clever and funny without overdoing it. The low quality ﬁlming was probably
an appropriate effect but ended up being a little too jarring, and the ending sounded more like a PBS
program than Hartley. Still, too many memorable lines and great moments for me to judge it harshly.
Adversarial Text Prediction = Negative. (Conﬁdence = 59.9%)
The promise of Martin Donovan playing Jesus was, utterly frankly, enough to get me to see the ﬁlm.
Deﬁnitely worthwhile; clever and funny without overdoing it. The low quality ﬁlming was presumably
an appropriate effect but ended up being a little too jarring, and the ending sounded more like a PBS
program than Hartley. Still, too many memorable lines and great moments for me to judge it harshly.

Original Text Prediction = Negative. (Conﬁdence = 74.30%)
Some sort of accolade must be given to ‘Hellraiser: Bloodline’.
It’s actually out full-mooned Full
Moon. It bears all the marks of, say, your ‘demonic toys’ or ‘puppet master’ series, without their dopey,
uh, charm? Full Moon can get away with silly product because they know it’s silly. These Hellraiser
things, man, do they ever take themselves seriously. This increasingly stupid franchise (though not
nearly as stupid as I am for having watched it) once made up for its low budgets by being stylish. Now
it’s just ish.
Adversarial Text Prediction = Positive. (Conﬁdence = 51.03%)
Some kind of accolade must be given to ‘Hellraiser: Bloodline’. it’s truly out full-mooned Full Moon. It
bears all the marks of, say, your ‘demonic toys’ or ‘puppet master’ series, without their silly, uh, charm?
Full Moon can get away with daft product because they know it’s silly. These Hellraiser things, man, do
they ever take themselves seriously. This steadily daft franchise (whilst not nearly as daft as i am for
having witnessed it) once made up for its low budgets by being stylish. Now it’s just ish.

Original Text Prediction = Negative. (Conﬁdence = 50.53%)
Thinly-cloaked retelling of the garden-of-eden story – nothing new, nothing shocking, although I feel
that is what the ﬁlmmakers were going for. The idea is trite. Strong performance from Daisy Eagan,
that’s about it. I believed she was 13, and I was interested in her character, the rest left me cold.
Adversarial Text Prediction = Positive. (Conﬁdence = 63.04%)
Thinly-cloaked retelling of the garden-of-eden story – nothing new, nothing shocking, although I feel
that is what the ﬁlmmakers were going for. The idea is petty. Strong performance from Daisy Eagan,
that’s about it. I believed she was 13, and I was interested in her character, the rest left me cold.

Table 4: Example of attack results against the sentiment analysis model. Modiﬁed words are highlighted in green
and red for the original and adversarial texts, respectively.

Additional Textual Entailment Results

Table 5 shows an additional set of attack results against the textual entailment model described in our
paper.

Original Text Prediction: Contradiction (Conﬁdence = 91%)
Premise: A man and a woman stand in front of a Christmas tree contemplating a single thought.
Hypothesis: Two people talk loudly in front of a cactus.
Adversarial Text Prediction: Entailment (Conﬁdence = 51%)
Premise: A man and a woman stand in front of a Christmas tree contemplating a single thought.
Hypothesis: Two humans chitchat loudly in front of a cactus.

Original Text Prediction: Contradiction (Conﬁdence = 94%)
Premise: A young girl wearing yellow shorts and a white tank top using a cane pole to ﬁsh at a small
pond.
Hypothesis: A girl wearing a dress looks off a cliff.
Adversarial Text Prediction: Entailment (Conﬁdence = 40%)
Premise: A young girl wearing yellow shorts and a white tank top using a cane pole to ﬁsh at a small
pond.
Hypothesis: A girl wearing a skirt looks off a ravine.

Original Text Prediction: Entailment (Conﬁdence = 86%)
Premise: A large group of protesters are walking down the street with signs.
Hypothesis: Some people are holding up signs of protest in the street.
Adversarial Text Prediction: Contradiction (Conﬁdence = 43%)
Premise: A large group of protesters are walking down the street with signs.
Hypothesis: Some people are holding up signals of protest in the street.

Table 5: Example of attack results against the textual entailment model. Modiﬁed words are highlighted in green
and red for the original and adversarial texts, respectively.

8
1
0
2
 
p
e
S
 
4
2
 
 
]
L
C
.
s
c
[
 
 
2
v
8
9
9
7
0
.
4
0
8
1
:
v
i
X
r
a

Generating Natural Language Adversarial Examples

Moustafa Alzantot1∗, Yash Sharma2∗, Ahmed Elgohary3,
Bo-Jhang Ho1, Mani B. Srivastava1, Kai-Wei Chang1

1Department of Computer Science, University of California, Los Angeles (UCLA)
{malzantot, bojhang, mbs, kwchang}@ucla.edu
2Cooper Union sharma2@cooper.edu
3Computer Science Department, University of Maryland elgohary@cs.umd.edu

Abstract

Deep neural networks (DNNs) are vulnera-
ble to adversarial examples, perturbations to
correctly classiﬁed examples which can cause
the model to misclassify.
In the image do-
main, these perturbations are often virtually
indistinguishable to human perception, caus-
ing humans and state-of-the-art models to dis-
agree. However, in the natural language do-
main, small perturbations are clearly percep-
tible, and the replacement of a single word
can drastically alter the semantics of the doc-
ument. Given these challenges, we use a
black-box population-based optimization al-
gorithm to generate semantically and syntac-
tically similar adversarial examples that fool
well-trained sentiment analysis and textual en-
tailment models with success rates of 97% and
70%, respectively. We additionally demon-
strate that 92.3% of the successful sentiment
analysis adversarial examples are classiﬁed to
their original label by 20 human annotators,
and that the examples are perceptibly quite
similar. Finally, we discuss an attempt to use
adversarial training as a defense, but fail to
yield improvement, demonstrating the strength
and diversity of our adversarial examples. We
hope our ﬁndings encourage researchers to
pursue improving the robustness of DNNs in
the natural language domain.

1 Introduction

Recent research has found that deep neural net-
works (DNNs) are vulnerable to adversarial ex-
amples (Goodfellow et al., 2014; Szegedy et al.,
ex-
2013).
amples has been shown in image classiﬁca-
tion (Szegedy et al., 2013) and speech recogni-
tion (Carlini and Wagner, 2018).
In this work,
we demonstrate that adversarial examples can

The existence of adversarial

∗Moustafa Alzantot and Yash Sharma contribute equally

to this work.

lan-
be constructed in the context of natural
guage. Using a black-box population-based op-
timization algorithm, we successfully generate
both semantically and syntactically similar adver-
sarial examples against models trained on both
the IMDB (Maas et al., 2011) sentiment analysis
task and the Stanford Natural Language Inference
(SNLI) (Bowman et al., 2015) textual entailment
task. In addition, we validate that the examples are
both correctly classiﬁed by human evaluators and
similar to the original via a human study. Finally,
we attempt to defend against said adversarial at-
tack using adversarial training, but fail to yield any
robustness, demonstrating the strength and diver-
sity of the generated adversarial examples.

Our results show that by minimizing the seman-
tic and syntactic dissimilarity, an attacker can per-
turb examples such that humans correctly classify,
but high-performing models misclassify. We are
open-sourcing our attack1 to encourage research
in training DNNs robust to adversarial attacks in
the natural language domain.

2 Natural Language Adversarial

Examples

have

been

examples

distortion

explored
Adversarial
primarily in the image recognition domain.
Examples have been generated through solv-
ing an optimization problem,
attempting to
induce misclassiﬁcation while minimizing the
2013;
perceptual
Carlini and Wagner, 2017; Chen et al., 2017b;
Sharma and Chen, 2017). Due to the computa-
tional cost of such approaches, fast methods were
introduced which, either in one-step or iteratively,
shift all pixels simultaneously until a distortion
is reached (Goodfellow et al., 2014;
constraint
Kurakin et al., 2016; Madry et al., 2017). Nearly

(Szegedy et al.,

1https://github.com/nesl/nlp_adversarial_examples

all popular methods are gradient-based.

Such methods, however, rely on the fact that
adding small perturbations to many pixels in the
image will not have a noticeable effect on a human
viewer. This approach obviously does not transfer
to the natural language domain, as all changes are
perceptible. Furthermore, unlike continuous im-
age pixel values, words in a sentence are discrete
tokens. Therefore, it is not possible to compute the
gradient of the network loss function with respect
to the input words. A straightforward workaround
is to project input sentences into a continuous
space (e.g. word embeddings) and consider this as
the model input. However, this approach also fails
because it still assumes that replacing every word
with words nearby in the embedding space will not
be noticeable. Replacing words without account-
ing for syntactic coherence will certainly lead to
improperly constructed sentences which will look
odd to the reader.

Relative to the image domain, little work has
been pursued for generating natural language ad-
versarial examples. Given the difﬁculty in gen-
erating semantics-preserving perturbations, dis-
tracting sentences have been added to the in-
put document in order to induce misclassiﬁca-
tion (Jia and Liang, 2017).
In our work, we
attempt
to generate semantically and syntac-
tically similar adversarial examples, via word
replacements, resolving the aforementioned is-
sues. Minimizing the number of word replace-
ments necessary to induce misclassiﬁcation has
been studied in previous work (Papernot et al.,
2016b), however without consideration given
to semantics or syntactics, yielding incoher-
ent generated examples.
In recent work,
there have been a few attempts at generat-
ing adversarial examples for language tasks by
using back-translation (Iyyer et al., 2018), ex-
ploiting machine-generated rules (Ribeiro et al.,
2018), and searching in underlying semantic
In addition, while
space (Zhao et al., 2018).
preparing our submission, we became aware of
recent work which target a similar contribu-
tion (Kuleshov et al., 2018; Ebrahimi et al., 2018).
We treat these contributions as parallel work.

3 Attack Design

3.1 Threat model
We assume the attacker has black-box access to
the target model; the attacker is not aware of the
model architecture, parameters, or training data,

and is only capable of querying the target model
with supplied inputs and obtaining the output pre-
dictions and their conﬁdence scores. This set-
ting has been extensively studied in the image do-
main (Papernot et al., 2016a; Chen et al., 2017a;
Alzantot et al., 2018), but has yet to be explored
in the context of natural language.

3.2 Algorithm
To avoid the limitations of gradient-based attack
methods, we design an algorithm for constructing
adversarial examples with the following goals in
mind. We aim to minimize the number of modiﬁed
words between the original and adversarial exam-
ples, but only perform modiﬁcations which retain
semantic similarity with the original and syntactic
coherence. To achieve these goals, instead of rely-
ing on gradient-based optimization, we developed
an attack algorithm that exploits population-based
gradient-free optimization via genetic algorithms.
An added beneﬁt of using gradient-free opti-
mization is enabling use in the black-box case;
gradient-reliant algorithms are inapplicable in this
case, as they are dependent on the model be-
ing differentiable and the internals being accessi-
ble (Papernot et al., 2016b; Ebrahimi et al., 2018).
Genetic algorithms are inspired by the process
of natural selection, iteratively evolving a popu-
lation of candidate solutions towards better solu-
tions. The population of each iteration is a called a
generation. In each generation, the quality of pop-
ulation members is evaluated using a ﬁtness func-
tion. “Fitter” solutions are more likely to be se-
lected for breeding the next generation. The next
generation is generated through a combination of
crossover and mutation. Crossover is the pro-
cess of taking more than one parent solution and
producing a child solution from them; it is anal-
ogous to reproduction and biological crossover.
Mutation is done in order to increase the diver-
sity of population members and provide better ex-
ploration of the search space. Genetic algorithms
are known to perform well in solving combinato-
rial optimization problems (Anderson and Ferris,
1994; M¨uhlenbein, 1989), and due to employing
a population of candidate solutions, these algo-
rithms can ﬁnd successful adversarial examples
with fewer modiﬁcations.

Perturb Subroutine:
In order to explain our
algorithm, we ﬁrst
introduce the subroutine
Perturb. This subroutine accepts an input sen-

tence xcur which can be either a modiﬁed sentence
or the same as xorig. It randomly selects a word w
in the sentence xcur and then selects a suitable re-
placement word that has similar semantic mean-
ing, ﬁts within the surrounding context, and in-
creases the target label prediction score.
In order to select
Perturb applies the following steps:

the best replacement word,

Algorithm 1 Finding adversarial examples

for i = 1, ..., S in population do

i ← Perturb(xorig, target)
P 0

for g = 1, 2...G generations do

for i = 1, ..., S in population do

)target

= f (P g−1
F g−1
i
i
xadv = P g−1
arg maxj F g−1
if arg maxc f (xadv)c == t then

j

• Computes the N nearest neighbors of the se-
lected word according to the distance in the
GloVe embedding space (Pennington et al.,
2014). We used euclidean distance, as we
did not see noticeable improvement using
cosine. We ﬁlter out candidates with dis-
tance to the selected word greater than δ.
We use the counter-ﬁtting method presented
in (Mrkˇsi´c et al., 2016) to post-process the
adversary’s GloVe vectors to ensure that the
nearest neighbors are synonyms. The result-
ing embedding is independent of the embed-
dings used by victim models.

• Second, we use the Google 1 billion words
(Chelba et al., 2013) to ﬁl-
language model
ter out words that do not ﬁt within the context
surrounding the word w in xcur. We do so by
ranking the candidate words based on their
language model scores when ﬁt within the re-
placement context, and keeping only the top
K words with the highest scores.

• From the remaining set of words, we pick the
one that will maximize the target label pre-
diction probability when it replaces the word
w in xcur.

• Finally, the selected word is inserted in place
of w, and Perturb returns the resulting sen-
tence.

The selection of which word to replace in the
input sentence is done by random sampling with
probabilities proportional to the number of neigh-
bors each word has within Euclidean distance δ in
the counter-ﬁtted embedding space, encouraging
the solution set to be large enough for the algo-
rithm to make appropriate modiﬁcations. We ex-
clude common articles and prepositions (e.g. a, to)
from being selected for replacement.

Optimization Procedure: The optimization al-
gorithm can be seen in Algorithm 1. The algo-
rithm starts by creating the initial generation P 0 of
size S by calling the Perturb subroutine S times
to create a set of distinct modiﬁcations to the orig-
inal sentence. Then, the ﬁtness of each popula-

return xadv ⊲ {Found successful attack}

else
P g
1 = {xadv}
p = N ormalize(F g−1)
for i = 2, ..., S in population do

Sample parent1 from P g−1 with probs p
Sample parent2 from P g−1 with probs p
child = Crossover(parent1, parent2)
childmut = Perturb(child, target)
P g

i = {childmut}

tion member in the current generation is computed
as the target label prediction probability, found by
querying the victim model function f . If a pop-
ulation member’s predicted label is equal to the
target label, the optimization is complete. Other-
wise, pairs of population members from the cur-
rent generation are randomly sampled with prob-
ability proportional to their ﬁtness values. A new
child sentence is then synthesized from a pair of
parent sentences by independently sampling from
the two using a uniform distribution. Finally, the
Perturb subroutine is applied to the resulting
children.

4 Experiments

To evaluate our attack method, we trained mod-
els for the sentiment analysis and textual en-
tailment classiﬁcation tasks. For both models,
each word in the input sentence is ﬁrst projected
into a ﬁxed 300-dimensional vector space using
GloVe (Pennington et al., 2014). Each of the mod-
els used are based on popular open-source bench-
marks, and can be found in the following reposito-
ries23. Model descriptions are given below.

Sentiment Analysis: We trained a sentiment
analysis model using the IMDB dataset of movie
reviews (Maas et al., 2011). The IMDB dataset
consists of 25,000 training examples and 25,000
test examples. The LSTM model is composed of

2https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py
3https://github.com/Smerity/keras_snli/blob/master/snli_rnn.py

Original Text Prediction = Negative. (Conﬁdence = 78.0%)
This movie had terrible acting, terrible plot, and terrible choice of actors. (Leslie Nielsen ...come on!!!)
the one part I considered slightly funny was the battling FBI/CIA agents, but because the audience was
mainly kids they didn’t understand that theme.
Adversarial Text Prediction = Positive. (Conﬁdence = 59.8%)
This movie had horriﬁc acting, horriﬁc plot, and horrifying choice of actors. (Leslie Nielsen ...come
on!!!) the one part I regarded slightly funny was the battling FBI/CIA agents, but because the audience
was mainly youngsters they didn’t understand that theme.

Table 1: Example of attack results for the sentiment analysis task. Modiﬁed words are highlighted in green and
red for the original and adversarial texts, respectively.

Original Text Prediction: Entailment (Conﬁdence = 86%)
Premise: A runner wearing purple strives for the ﬁnish line.
Hypothesis: A runner wants to head for the ﬁnish line.
Adversarial Text Prediction: Contradiction (Conﬁdence = 43%)
Premise: A runner wearing purple strives for the ﬁnish line.
Hypothesis: A racer wants to head for the ﬁnish line.

Table 2: Example of attack results for the textual entailment task. Modiﬁed words are highlighted in green and red
for the original and adversarial texts, respectively.

Sentiment Analysis

Textual Entailment

% success % modiﬁed % success % modiﬁed

Perturb baseline
Genetic attack

52%
97%

19%
14.7%

–
70%

–
23%

Table 3: Comparison between the attack success rate and mean percentage of modiﬁcations required by the genetic
attack and perturb baseline for the two tasks.

128 units, and the outputs across all time steps are
averaged and fed to the output layer. The test accu-
racy of the model is 90%, which is relatively close
to the state-of-the-art results on this dataset.

Textual Entailment: We trained a textual en-
tailment model using the Stanford Natural Lan-
guage Inference (SNLI) corpus (Bowman et al.,
2015). The model passes the input through a
ReLU “translation” layer (Bowman et al., 2015),
which encodes the premise and hypothesis sen-
tences by performing a summation over the word
embeddings, concatenates the two sentence em-
beddings, and ﬁnally passes the output through 3
600-dimensional ReLU layers before feeding it to
a 3-way softmax. The model predicts whether the
premise sentence entails, contradicts or is neutral
to the hypothesis sentence. The test accuracy of
the model is 83% which is also relatively close to
the state-of-the-art (Chen et al., 2017c).
4.1 Attack Evaluation Results
We randomly sampled 1000, and 500 correctly
classiﬁed examples from the test sets of the two
tasks to evaluate our algorithm. Correctly classi-
ﬁed examples were chosen to limit the accuracy

levels of the victim models from confounding our
results. For the sentiment analysis task, the at-
tacker aims to divert the prediction result from
positive to negative, and vice versa. For the tex-
tual entailment task, the attacker is only allowed
to modify the hypothesis, and aims to divert the
prediction result from ‘entailment’ to ‘contradic-
tion’, and vice versa. We limit the attacker to
maximum G = 20 iterations, and ﬁx the hyper-
parameter values to S = 60, N = 8, K = 4, and
δ = 0.5. We also ﬁxed the maximum percentage
of allowed changes to the document to be 20% and
25% for the two tasks, respectively. If increased,
the success rate would increase but the mean qual-
ity would decrease. If the attack does not succeed
within the iterations limit or exceeds the speciﬁed
threshold, it is counted as a failure.

Sample outputs produced by our attack are
shown in tables 4 and 5. Additional outputs can
be found in the supplementary material. Table 3
shows the attack success rate and mean percent-
age of modiﬁed words on each task. We compare
to the Perturb baseline, which greedily applies
the Perturb subroutine, to validate the use of

population-based optimization. As can be seen
from our results, we are able to achieve high suc-
cess rate with a limited number of modiﬁcations
on both tasks. In addition, the genetic algorithm
signiﬁcantly outperformed the Perturb baseline
in both success rate and percentage of words mod-
iﬁed, demonstrating the additional beneﬁt yielded
by using population-based optimization. Testing
using a single TitanX GPU, for sentiment analy-
sis and textual entailment, we measured average
runtimes on success to be 43.5 and 5 seconds per
example, respectively. The high success rate and
reasonable runtimes demonstrate the practicality
of our approach, even when scaling to long sen-
tences, such as those found in the IMDB dataset.

Speaking of which, our success rate on textual
entailment is lower due to the large disparity in
sentence length. On average, hypothesis sentences
in the SNLI corpus are 9 words long, which is
very short compared to IMDB (229 words, lim-
ited to 100 for experiments). With sentences that
short, applying successful perturbations becomes
much harder, however we were still able to achieve
a success rate of 70%. For the same reason, we
didn’t apply the Perturb baseline on the textual
entailment task, as the Perturb baseline fails to
achieve any success under the limits of the maxi-
mum allowed changes constraint.

4.2 User study

We performed a user study on the sentiment anal-
ysis task with 20 volunteers to evaluate how per-
ceptible our adversarial perturbations are. Note
that
the number of participating volunteers is
signiﬁcantly larger than used in previous stud-
ies (Jia and Liang, 2017; Ebrahimi et al., 2018).
The user study was composed of two parts. First,
we presented 100 adversarial examples to the par-
ticipants and asked them to label the sentiment of
the text (i.e., positive or negative.) 92.3% of the
responses matched the original text sentiment, in-
dicating that our modiﬁcation did not signiﬁcantly
affect human judgment on the text sentiment. Sec-
ond, we prepared 100 questions, each question in-
cludes the original example and the corresponding
adversarial example in a pair. Participants were
asked to judge the similarity of each pair on a scale
from 1 (very similar) to 4 (very different). The av-
erage rating is 2.23 ± 0.25, which shows the per-
ceived difference is also small.

4.3 Adversarial Training
The results demonstrated in section 4.1 raise the
following question: How can we defend against
these attacks? We performed a preliminary exper-
iment to see if adversarial training (Madry et al.,
2017), the only effective defense in the image do-
main, can be used to lower the attack success rate.
We generated 1000 adversarial examples on the
cleanly trained sentiment analysis model using the
IMDB training set, appended them to the existing
training set, and used the updated dataset to ad-
versarially train a model from scratch. We found
that adversarial training provided no additional ro-
bustness beneﬁt in our experiments using the test
set, despite the fact that the model achieves near
100% accuracy classifying adversarial examples
included in the training set. These results demon-
strate the diversity in the perturbations generated
by our attack algorithm, and illustrates the difﬁ-
culty in defending against adversarial attacks. We
hope these results inspire further work in increas-
ing the robustness of natural language models.

5 Conclusion
We demonstrate that despite the difﬁculties in gen-
erating imperceptible adversarial examples in the
natural language domain, semantically and syntac-
tically similar adversarial examples can be crafted
using a black-box population-based optimization
algorithm, yielding success on both the sentiment
analysis and textual entailment tasks. Our human
study validated that the generated examples were
indeed adversarial and perceptibly quite similar.
We hope our work encourages researchers to pur-
sue improving the robustness of DNNs in the nat-
ural language domain.

Acknowledgement
This research was supported in part by the U.S.
Army Research Laboratory and the UK Ministry
of Defence under Agreement Number W911NF-
16-3-0001, the National Science Foundation under
award # CNS-1705135, OAC-1640813, and IIS-
1760523, and the NIH Center of Excellence for
Mobile Sensor Data-to-Knowledge (MD2K) un-
der award 1-U54EB020404-01. Ahmed Elgohary
is funded by an IBM PhD Fellowship. Any ﬁnd-
ings in this material are those of the author(s) and
do not reﬂect the views of any of the above fund-
ing agencies. The U.S. and U.K. Governments are
authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copy-
right notation hereon.

References

M. Alzantot, Y. Sharma, S. Chakraborty, and M. Srivas-
tava. 2018. Genattack: Practical black-box attacks
arXiv preprint
with gradient-free optimization.
arXiv:1805.11090.

Edward J Anderson and Michael C Ferris. 1994. Ge-
netic algorithms for combinatorial optimization: the
assemble line balancing problem. ORSA Journal on
Computing, 6(2):161–173.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

N. Carlini and D. Wagner. 2017. Towards evaluating
the robustness of neural networks. arXiv preprint
arXiv:1608.04644.

Nicholas Carlini and David Wagner. 2018. Audio ad-
versarial examples: Targeted attacks on speech-to-
text. arXiv preprint arXiv:1801.01944.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.

P. Chen, H Zhang, Y. Sharma, J. Yi, and C. Hseih.
2017a.
Zoo: Zeroth order optimization based
black-box attacks to deep neural networks with-
arXiv preprint
out
arXiv:1708.03999.

training substitute models.

P. Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh.
2017b. Ead: Elastic-net attacks to deep neural
networks via adversarial examples. arXiv preprint
arXiv:1709.0414.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017c. Enhanced lstm for
In Proceedings of the
natural language inference.
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1657–1668.

J. Ebrahimi, A. Rao, D. Lowd, and D. Dou. 2018. Hot-
ﬂip: White-box adversarial examples for text classi-
ﬁcation. ACL’18; arXiv preprint arXiv:1712.06751.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adver-
sarial examples. arXiv preprint arXiv:1412.6572.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of NAACL.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328.

V. Kuleshov, S. Thakoor, T. Lau, and S. Ermon. 2018.
Adversarial examples for natural language classiﬁ-
cation problems. OpenReview submission OpenRe-
view:r1QZ3zbAZ.

A. Kurakin, I. Goodfellow, and S. Bengio. 2016. Ad-
versarial machine learning at scale. ICLR’17; arXiv
preprint arXiv:1611.01236.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.

A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. 2017.
Towards deep learning mod-
els resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083.

Nikola Mrkˇsi´c, Diarmuid O S´eaghdha, Blaise Thom-
son, Milica Gaˇsi´c, Lina Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Counter-ﬁtting word vec-
Steve Young. 2016.
arXiv preprint
tors to linguistic constraints.
arXiv:1603.00892.

Heinz M¨uhlenbein. 1989. Parallel genetic algorithms,
population genetics and combinatorial optimization.
In Workshop on Parallel Processing: Logic, Organi-
zation, and Technology, pages 398–406. Springer.

N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Ce-
lik, and A. Swami. 2016a.
Practical black-box
attacks against machine learning. arXiv preprint
arXiv:1602.02697.

N. Papernot, P. McDaniel, A. Swami, and R. Ha-
rang. 2016b. Crafting adversarial input sequences
arXiv preprint
for recurrent neural networks.
arXiv:1604.08275.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversar-
In Proceed-
ial rules for debugging nlp models.
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 856–865.

Y. Sharma and P. Y. Chen. 2017. Attacking the madry
defense model with l1-based adversarial examples.
arXiv preprint arXiv:1710.10733.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2013.
Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199.

Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.
arXiv

Generating natural adversarial examples.
preprint arXiv:1710.11342.

Supplemental Materials: Generating Natural Language Adversarial

Examples

Additional Sentiment Analysis Results

Table 4 shows an additional set of attack results against the sentiment analysis model described in our
paper.

Original Text Prediction = Positive. (Conﬁdence = 78%)
The promise of Martin Donovan playing Jesus was, quite honestly, enough to get me to see the ﬁlm.
Deﬁnitely worthwhile; clever and funny without overdoing it. The low quality ﬁlming was probably
an appropriate effect but ended up being a little too jarring, and the ending sounded more like a PBS
program than Hartley. Still, too many memorable lines and great moments for me to judge it harshly.
Adversarial Text Prediction = Negative. (Conﬁdence = 59.9%)
The promise of Martin Donovan playing Jesus was, utterly frankly, enough to get me to see the ﬁlm.
Deﬁnitely worthwhile; clever and funny without overdoing it. The low quality ﬁlming was presumably
an appropriate effect but ended up being a little too jarring, and the ending sounded more like a PBS
program than Hartley. Still, too many memorable lines and great moments for me to judge it harshly.

Original Text Prediction = Negative. (Conﬁdence = 74.30%)
Some sort of accolade must be given to ‘Hellraiser: Bloodline’.
It’s actually out full-mooned Full
Moon. It bears all the marks of, say, your ‘demonic toys’ or ‘puppet master’ series, without their dopey,
uh, charm? Full Moon can get away with silly product because they know it’s silly. These Hellraiser
things, man, do they ever take themselves seriously. This increasingly stupid franchise (though not
nearly as stupid as I am for having watched it) once made up for its low budgets by being stylish. Now
it’s just ish.
Adversarial Text Prediction = Positive. (Conﬁdence = 51.03%)
Some kind of accolade must be given to ‘Hellraiser: Bloodline’. it’s truly out full-mooned Full Moon. It
bears all the marks of, say, your ‘demonic toys’ or ‘puppet master’ series, without their silly, uh, charm?
Full Moon can get away with daft product because they know it’s silly. These Hellraiser things, man, do
they ever take themselves seriously. This steadily daft franchise (whilst not nearly as daft as i am for
having witnessed it) once made up for its low budgets by being stylish. Now it’s just ish.

Original Text Prediction = Negative. (Conﬁdence = 50.53%)
Thinly-cloaked retelling of the garden-of-eden story – nothing new, nothing shocking, although I feel
that is what the ﬁlmmakers were going for. The idea is trite. Strong performance from Daisy Eagan,
that’s about it. I believed she was 13, and I was interested in her character, the rest left me cold.
Adversarial Text Prediction = Positive. (Conﬁdence = 63.04%)
Thinly-cloaked retelling of the garden-of-eden story – nothing new, nothing shocking, although I feel
that is what the ﬁlmmakers were going for. The idea is petty. Strong performance from Daisy Eagan,
that’s about it. I believed she was 13, and I was interested in her character, the rest left me cold.

Table 4: Example of attack results against the sentiment analysis model. Modiﬁed words are highlighted in green
and red for the original and adversarial texts, respectively.

Additional Textual Entailment Results

Table 5 shows an additional set of attack results against the textual entailment model described in our
paper.

Original Text Prediction: Contradiction (Conﬁdence = 91%)
Premise: A man and a woman stand in front of a Christmas tree contemplating a single thought.
Hypothesis: Two people talk loudly in front of a cactus.
Adversarial Text Prediction: Entailment (Conﬁdence = 51%)
Premise: A man and a woman stand in front of a Christmas tree contemplating a single thought.
Hypothesis: Two humans chitchat loudly in front of a cactus.

Original Text Prediction: Contradiction (Conﬁdence = 94%)
Premise: A young girl wearing yellow shorts and a white tank top using a cane pole to ﬁsh at a small
pond.
Hypothesis: A girl wearing a dress looks off a cliff.
Adversarial Text Prediction: Entailment (Conﬁdence = 40%)
Premise: A young girl wearing yellow shorts and a white tank top using a cane pole to ﬁsh at a small
pond.
Hypothesis: A girl wearing a skirt looks off a ravine.

Original Text Prediction: Entailment (Conﬁdence = 86%)
Premise: A large group of protesters are walking down the street with signs.
Hypothesis: Some people are holding up signs of protest in the street.
Adversarial Text Prediction: Contradiction (Conﬁdence = 43%)
Premise: A large group of protesters are walking down the street with signs.
Hypothesis: Some people are holding up signals of protest in the street.

Table 5: Example of attack results against the textual entailment model. Modiﬁed words are highlighted in green
and red for the original and adversarial texts, respectively.

