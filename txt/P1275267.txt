8
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
1
v
2
7
5
0
1
.
5
0
8
1
:
v
i
X
r
a

BRITS: Bidirectional Recurrent Imputation for Time
Series

Wei Cao1,3

Dong Wang2

Jian Li1

Hao Zhou3

Lei Li3

Yitan Li3

1Tsinghua University

2Duke University

3 Toutiao AILab

Abstract

Time series are widely used as signals in many classiﬁcation/regression tasks.
It is ubiquitous that time series contains many missing values. Given multiple
correlated time series data, how to ﬁll in missing values and to predict their class
labels? Existing imputation methods often impose strong assumptions of the
underlying data generating process, such as linear dynamics in the state space. In
this paper, we propose BRITS, a novel method based on recurrent neural networks
for missing value imputation in time series data. Our proposed method directly
learns the missing values in a bidirectional recurrent dynamical system, without
any speciﬁc assumption. The imputed values are treated as variables of RNN
graph and can be effectively updated during the backpropagation. BRITS has three
advantages: (a) it can handle multiple correlated missing values in time series; (b)
it generalizes to time series with nonlinear dynamics underlying; (c) it provides
a data-driven imputation procedure and applies to general settings with missing
data. We evaluate our model on three real-world datasets, including an air quality
dataset, a health-care data, and a localization data for human activity. Experiments
show that our model outperforms the state-of-the-art methods in both imputation
and classiﬁcation/regression accuracies.

1

Introduction

Multivariate time series data are abundant in many application areas, such as ﬁnancial marketing
[5, 4], health-care [10, 22], meteorology [31, 26], and trafﬁc engineering [29, 35]. Time series
are widely used as signals for classiﬁcation/regression in various applications of corresponding
areas. However, missing values in time series are very common, due to unexpected accidents, such
as equipment damage or communication error, and may signiﬁcantly harm the performance of
downstream applications.

Much prior work proposed to ﬁx the missing data problem with statistics and machine learning
approaches. However most of them require fairly strong assumptions on missing values. We can ﬁll
the missing values using classical statistical time series models such as ARMA or ARIMA (e.g., [1]).
But these models are essentially linear (after differencing). Kreindler et al. [19] assume that the data
are smoothable, i.e., there is no sudden wave in the periods of missing values, hence imputing missing
values can be done by smoothing over nearby values. Matrix completion has also been used to
address missing value problems (e.g., [30, 34]). But it typically applies to only static data and requires
strong assumptions such as low-rankness. We can also predict missing values by ﬁtting a parametric
data-generating model with the observations [14, 2], which assumes that data of time series follow the
distribution of hypothetical models. These assumptions make corresponding imputation algorithms
less general, and the performance less desirable when the assumptions do not hold.

In this paper, we propose BRITS, a novel method for ﬁlling the missing values for multiple correlated
time series. Internally, BRITS adapts recurrent neural networks (RNN) [16, 11] for imputing missing

Preprint. Work in progress.

values, without any speciﬁc assumption over the data. Much prior work uses non-linear dynamical
systems for time series prediction [9, 24, 3]. In our method, we instantiate the dynamical system as a
bidirectional RNN, i.e., imputing missing values with bidirectional recurrent dynamics. In particular,
we make the following technical contributions:

• We design a bidirectional RNN model for imputing missing values. We directly use RNN for
predicting missing values, instead of tuning weights for smoothing as in [10]. Our method
does not impose speciﬁc assumption, hence works more generally than previous methods.

• We regard missing values as variables of the bidirectional RNN graph, which are involved
in the backpropagation process. In such case, missing values get delayed gradients in both
forward and backward directions with consistency constraints, which makes the estimation
of missing values more accurate.

• We simultaneously perform missing value imputation and classiﬁcation/regression of appli-
cations jointly in one neural graph. This alleviates the error propagation problem from impu-
tation to classiﬁcation/regression. Additionally, the supervision of classiﬁcation/regression
makes the estimation of missing values more accurate.

• We evaluate our model on three real-world datasets, including an air quality dataset, a
health-care dataset and a localization dataset of human activities. Experimental results show
that our model outperforms the state-of-the-art models for both imputation and classiﬁca-
tion/regression accuracies.

2 Related Work

There is a large body of literature on the imputation of missing values in time series. We only mention
a few closely related ones. The interpolate method tries to ﬁt a "smooth curve" to the observations
and thus reconstruct the missing values by the local interpolations [19, 14]. Such method discards
any relationships between the variables over time. The autoregressive method, including ARIMA,
SARIMA etc., eliminates the non-stationary parts in the time series data and ﬁt a parameterized
stationary model. The state space model further combines ARIMA and Kalman Filter [14, 15], which
provides more accurate results. Multivariate Imputation by Chained Equations (MICE) [2] ﬁrst
initializes the missing values arbitrarily and then estimates each missing variable based on the chain
equations. The graphical model [20] introduces a latent variable for each missing value, and ﬁnds
the latent variables by learning their transition matrix. There are also some data-driven methods
for missing value imputation. Yi et al. [32] imputed the missing values in air quality data with
geographical features. Wang et al. [30] imputed missing values in recommendation system with
collaborative ﬁltering. Yu et al. [34] utilized matrix factorization with temporal regularization to
impute the missing values in regularly sampled time series data.

Recently, some researchers attempted to impute the missing values with recurrent neural networks
[7, 10, 21, 12, 33]. The recurrent components are trained together with the classiﬁcation/regression
component, which signiﬁcantly boosts the accuracy. Che et al. [10] proposed GRU-D, which imputes
missing values in health-care data with a smooth fashion. It assumes that a missing variable can be
represented as the combination of its corresponding last observed value and the global mean. GRU-D
achieves remarkable performance on health-care data. However, it has many limitations on general
datasets [10]. A closely related work is M-RNN proposed by Yoon et al. [33]. M-RNN also utilizes
bi-directional RNN to impute missing values. Differing from our model, it drops the relationships
between missing variables. The imputed values in M-RNN are treated as constants and cannot be
sufﬁciently updated.

3 Preliminary

We ﬁrst present the problem formulation and some necessary preliminaries.

series X =
Deﬁnition 1 (Multivariate Time Series) We denote a multivariate
{x1, x2, . . . , xT } as a sequence of T observations. The t-th observation xt ∈ RD consists
of D features {x1
t }, and was observed at timestamp st (the time gap between different
timestamps may be not same). In reality, due to unexpected accidents, such as equipment damage

t , . . . , xD

t , x2

time

2

or communication error, xt may have the missing values (e.g., in Fig. 1, x3
represent the missing values in xt, we introduce a masking vector mt where,
(cid:26) 0
1

if xd
otherwise

t is not observed

t =

md

.

1 in x1 is missing). To

In many cases, some features can be missing for consecutive timestamps (e.g., blue blocks in Fig. 1).
We deﬁne δd
t to be the time gap from the last observation to the current timestamp st, i.e.,

δd
t =






st − st−1 + δd
st − st−1
0

t−1

if t > 1, md
if t > 1, md
if t = 1

t−1 = 0
t−1 = 1

.

See Fig. 1 for an illustration.

Figure 1: An example of multivariate time series with missing values. x1 to x6 are observed at
s1...6 = 0, 2, 7, 9, 14, 15 respectively. Considering the 2nd feature in x6, the last observation of the
2nd feature took place at s2 = 2, and we have that δ2

6 = s6 − s2 = 13.

In this paper, we study a general setting for time series classiﬁcation/regression problems with missing
values. We use y to denote the label of corresponding classiﬁcation/regression task. In general, y
can be either a scalar or a vector. Our goal is to predict y based on the given time series X. In the
meantime, we impute the missing values in X as accurate as possible. In another word, we aim to
design an effective multi-task learning algorithm for both classiﬁcation/regression and imputation.

4 BRITS

In this section, we describe the BRITS. Differing from the prior work which uses RNN to impute
missing values in a smooth fashion [10], we learn the missing values directly in a recurrent dynamical
system [25, 28] based on the observed data. The missing values are thus imputed according to the
recurrent dynamics, which signiﬁcantly boosts both the imputation accuracy and the ﬁnal classiﬁca-
tion/regression accuracy. We start the description with the simplest case: the variables observed at
the same time are mutually uncorrelated. For such case, we propose algorithms for imputation with
unidirectional recurrent dynamics and bidirectional recurrent dynamics, respectively. We further
propose an algorithm for correlated multivariate time series in Section 4.3.

4.1 Unidirectional Uncorrelated Recurrent Imputation

For the simplest case, we assume that for the t-th step, xi
be correlated with some xj
dynamics, denoted as RITS-I.

t may
t(cid:48)(cid:54)=t). We ﬁrst propose an imputation algorithm by unidirectional recurrent

t are uncorrelated if i (cid:54)= j (but xi

t and xj

In a unidirectional recurrent dynamical system, each value in the time series can be derived by its
predecessors with a ﬁxed arbitrary function [9, 24, 3]. Thus, we iteratively impute all the variables in
the time series according to the recurrent dynamics. For the t-th step, if xt is actually observed, we
use it to validate our imputation and pass xt to the next recurrent steps. Otherwise, since the future
observations are correlated with the current value, we replace xt with the obtained imputation, and
validate it by the future observations. To be more concrete, let us consider an example.

Example 1 Suppose we are given a time series X = {x1, x2, . . . , x10}, where x5, x6 and x7 are
missing. According to the recurrent dynamics, at each time step t, we can obtain an estimation ˆxt
based on the previous t − 1 steps. In the ﬁrst 4 steps, the estimation error can be obtained immediately

3

Figure 2: Imputation with unidirectional dynamics.

by calculating the estimation loss function Le(ˆxt, xt) for t = 1, . . . , 4. However, when t = 5, 6, 7,
we cannot calculate the error immediately since the true values are missing. Nevertheless, note that
at the 8-th step, ˆx8 depends on ˆx5 to ˆx7. We thus obtain a “delayed" error for ˆxt=5,6,7 at the 8-th
step.

4.1.1 Algorithm

We introduce a recurrent component and a regression component for imputation. The recurrent
component is achieved by a recurrent neural network and the regression component is achieved by a
fully-connected network. A standard recurrent network [17] can be represented as

ht = σ(Whht−1 + Uhxt + bh),
where σ is the sigmoid function, Wh, Uh and bh are parameters, and ht is the hidden state of
previous time steps.

In our case, since xt may have missing values, we cannot use xt as the input directly as in the above
equation. Instead, we use a “complement" input xc
t derived by our algorithm when xt is missing.
Formally, we initialize the initial hidden state h0 as an all-zero vector and then update the model by:
(1)
(2)
(3)
(4)
(5)

ˆxt = Wxht−1 + bx,
xc
t = mt (cid:12) xt + (1 − mt) (cid:12) ˆxt,
γt = exp{− max(0, Wγδt + bγ)},
ht = σ(Wh[ht−1 (cid:12) γt] + Uh[xc
(cid:96)t = (cid:104)mt, Le(xt, ˆxt)(cid:105) .

t ◦ mt] + bh),

Eq. (1) is the regression component which transfers the hidden state ht−1 to the estimated vector
ˆxt. In Eq. (2), we replace missing values in xt with corresponding values in ˆxt, and obtain the
complement vector xc
t . Besides, since the time series may be irregularly sampled, in Eq. (3), we
further introduce a temporal decay factor γt to decay the hidden vector ht−1. Intuitively, if δt is large
(i.e., the values are missing for a long time), we expect a small γt to decay the hidden state. Such
factor also represents the missing patterns in the time series which is critical to imputation [10]. In Eq.
(4), based on the decayed hidden state, we predict the next state ht. Here, ◦ indicates the concatenate
operation. In the mean time, we calculate the estimation error by the estimation loss function Le in
Eq. (5). In our experiment, we use the mean absolute error for Le. Finally, we predict the task label
y as

where fout can be either a fully-connected layer or a softmax layer depending on the speciﬁc task,
and αi is the weight for different hidden state which can be derived by the attention mechanism1 or
the mean pooling mechanism, i.e., αi = 1
T . We calculate the output loss by Lout(y, ˆy). Our model is
then updated by minimizing the accumulated loss 1
T

t=1 (cid:96)t + Lout(y, ˆy).

(cid:80)T

1The design of attention mechanism is out of this paper’s scope.

ˆy = fout(

αihi),

T
(cid:88)

i=1

4

4.1.2 Practical Issues

In practice, we use LSTM as the recurrent component in Eq. (4) to prevent the gradient vanish-
ing/exploding problems in vanilla RNN [17]. Standard RNN models including LSTM treat ˆxt as a
constant. During backpropagation, gradients are cut at ˆxt. This makes the estimation errors backprop-
agate insufﬁciently. For example, in Example 1, the estimation errors of ˆx5 to ˆx7 are obtained at the
8-th step as the delayed errors. However, if we treat ˆx5 to ˆx7 as constants, such delayed error cannot
be fully backpropagated. To overcome such issue, we treat ˆxt as a variable of RNN graph. We let the
estimation error passes through ˆxt during the backpropagation. Fig. 2 shows how RITS-I method
works in Example 1. In this example, the gradients are backpropagated through the opposite direction
of solid lines. Thus, the delayed error (cid:96)8 is passed to steps 5, 6 and 7. In the experiment, we ﬁnd that
our models are unstable during training if we treat ˆxt as a constant. See Appendix C for details.

4.2 Bidirectional Uncorrelated Recurrent Imputation

In the RITS-I, errors of estimated missing values are delayed until the presence of the next observation.
For example, in Example 1, the error of ˆx5 is delayed until x8 is observed. Such error delay makes
the model converge slowly and in turn leads to inefﬁciency in training. In the meantime, it also leads
to the bias exploding problem [6], i.e., the mistakes made early in the sequential prediction are fed as
input to the model and may be quickly ampliﬁed. In this section, we propose an improved version
called BRITS-I. The algorithm alleviates the above-mentioned issues by utilizing the bidirectional
recurrent dynamics on the given time series, i.e., besides the forward direction, each value in time
series can be also derived from the backward direction by another ﬁxed arbitrary function.

To illustrate the intuition of BRITS-I algorithm, again, we consider Example 1. Consider the
backward direction of the time series. In bidirectional recurrent dynamics, the estimation ˆx4 reversely
depends on ˆx5 to ˆx7. Thus, the error in the 5-th step is back-propagated from not only the 8-th step in
the forward direction (which is far from the current position), but also the 4-th step in the backward
direction (which is closer). Formally, the BRITS-I algorithm performs the RITS-I as shown in
Eq. (1) to Eq. (5) in forward and backward directions, respectively. In the forward direction, we
obtain the estimation sequence {ˆx1, ˆx2, . . . , ˆxT } and the loss sequence {(cid:96)1, (cid:96)2, . . . , (cid:96)T }. Similarly,
in the backward direction, we obtain another estimation sequence {ˆx(cid:48)
T } and another
(cid:48)}. We enforce the prediction in each step to be consistent in both
loss sequence {(cid:96)1
directions by introducing the “consistency loss”:

2, . . . , ˆx(cid:48)

(cid:48), . . . , (cid:96)T

1, ˆx(cid:48)

(cid:48), (cid:96)2

(6)
where we use the mean squared error as the discrepancy in our experiment. The ﬁnal estimation loss
is obtained by accumulating the forward loss (cid:96)t, the backward loss (cid:96)(cid:48)
t and the consistency loss (cid:96)cons
.
The ﬁnal estimation in the t-th step is the mean of ˆxt and ˆxt

= Discrepancy(ˆxt, ˆx(cid:48)
t)

(cid:96)cons
t

(cid:48).

t

4.3 Correlated Recurrent Imputation

The previously proposed algorithms RITS-I and BRITS-I assume that features observed at the same
time are mutually uncorrelated. This may be not true in some applications. For example, in the
air quality data [32], each feature represents one measurement in a monitoring station. Obviously,
the observed measurements are spatially correlated. In general, the measurement in one monitoring
station is close to those values observed in its neighboring stations. In this case, we can estimate a
missing measurement according to both its historical data, and the measurements in its neighbors.

In this section, we propose an algorithm, which utilizes the feature correlations in the unidirectional
recurrent dynamical system. We refer to such algorithm as RITS. The feature correlated algorithm
for bidirectional case (i.e., BRITS) can be derived in the same way. Note that in Section 4.1, the
estimation ˆxt is only correlated with the historical observations, but irrelevant with the the current
observation. We refer to ˆxt as a “history-based estimation". In this section, we derive another
“feature-based estimation" for each xd
t , based on the other features at time st. Speciﬁcally, at the t-th
step, we ﬁrst obtain the complement observation xc
t by Eq. (1) and Eq. (2). Then, we deﬁne our
feature-based estimation as ˆzt where

(7)
Wz, bz are corresponding parameters. We restrict the diagonal of parameter matrix Wz to be all
zeros. Thus, the d-th element in ˆzt is exactly the estimation of xd
t , based on the other features. We

t + bz,

ˆzt = Wzxc

5

further combine the historical-based estimation ˆxt and the feature-based estimation ˆzt. We denote
the combined vector as ˆct, and we have that

βt = σ(Wβ[γt ◦ mt] + bβ)
(8)
ˆct = βt (cid:12) ˆzt + (1 − βt) (cid:12) ˆxt.
(9)
Here we use βt ∈ [0, 1]D as the weight of combining the history-based estimation ˆxt and the
feature-based estimation ˆzt. Note that ˆzt is derived from xc
t can be
history-based estimations or truly observed values, depending on the presence of the observations.
Thus, we learn the weight βt by considering both the temporal decay γt and the masking vector mt
as shown in Eq. (8). The rest parts are similar as the feature uncorrelated case. We ﬁrst replace
missing values in xt with the corresponding values in ˆct. The obtained vector is then fed to the next
recurrent step to predict memory ht:

t by Eq. (7). The elements of xc

cc
t = mt (cid:12) xt + (1 − mt) (cid:12) ˆct
ht = σ(Wh[ht−1 (cid:12) γt] + Uh[cc

(10)
(11)
However, the estimation loss is slightly different with the feature uncorrelated case. We ﬁnd that
simply using (cid:96)t = Le(xt, ˆct) leads to a very slow convergence speed. Instead, we accumulate all the
estimation errors of ˆxt, ˆzt and ˆct:

t ◦ mt] + bh).

(cid:96)t = Le(xt, ˆxt) + Le(xt, ˆzt) + Le(xt, ˆct).

Our proposed methods are applicable to a wide variety of applications. We evaluate our methods on
three different real-world datasets. The download links of the datasets, as well as the implementation
codes can be found in the GitHub page2.

5 Experiment

5.1 Dataset Description

5.1.1 Air Quality Data

We evaluate our models on the air quality dataset, which consists of PM2.5 measurements from
36 monitoring stations in Beijing. The measurements are hourly collected from 2014/05/01 to
2015/04/30. Overall, there are 13.3% values are missing. For this dataset, we do pure imputation
task. We use the exactly same train/test setting as in prior work [32], i.e., we use the 3rd, 6th, 9th and
12th months as the test data and the other months as the training data. See Appendix D for details. To
train our model, we randomly select every 36 consecutive steps as one time series.

5.1.2 Health-care Data

We evaluate our models on health-care data in PhysioNet Challenge 2012 [27], which consists of
4000 multivariate clinical time series from intensive care unit (ICU). Each time series contains 35
measurements such as Albumin, heart-rate etc., which are irregularly sampled at the ﬁrst 48 hours
after the patient’s admission to ICU. We stress that this dataset is extremely sparse. There are up to
78% missing values in total. For this dataset, we do both the imputation task and the classiﬁcation
task. To evaluate the imputation performance, we randomly eliminate 10% of observed measurements
from data and use them as the ground-truth. At the same time, we predict the in-hospital death of
each patient as a binary classiﬁcation task. Note that the eliminated measurements are only used for
validating the imputation, and they are never visible to the model.

5.1.3 Localization for Human Activity Data

The UCI localization data for human activity [18] contains records of ﬁve people performing different
activities such as walking, falling, sitting down etc (there are 11 activities). Each person wore four
sensors on her/his left/right ankle, chest, and belt. Each sensor recorded a 3-dimensional coordinates
for about 20 to 40 millisecond. We randomly select 40 consecutive steps as one time series, and
there are 30, 917 time series in total. For this dataset, we do both imputation and classiﬁcation tasks.
Similarly, we randomly eliminate 10% observed data as the imputation ground-truth. We further
predict the corresponding activity of observed time series (i.e., walking, sitting, etc).

2https://github.com/NIPS-BRITS/BRITS

6

5.2 Experiment Setting

5.2.1 Model Implementations

We ﬁx the dimension of hidden state ht in RNN to be 64. We train our model by an Adam optimizer
with learning rate 0.001 and batch size 64. For all the tasks, we normalize the numerical values to
have zero mean and unit variance for stable training.

We use different early stopping strategies for pure imputation task and classiﬁcation tasks. For the
imputation tasks, we randomly select 10% of non-missing values as the validation data. The early
stopping is thus performed based on the validation error. For the classiﬁcation tasks, we ﬁrst pre-train
the model as an imputation task. Then we use 5-fold cross validation to further optimize both the
imputation and classiﬁcation losses simultaneously.

We evaluate the imputation performance in terms of mean absolute error (MAE) and mean relative
error (MRE). Suppose that labeli is the ground-truth of the i-th item, predi is the output of the i-th
item, and there are N items in total. Then, MAE and MRE are deﬁned as

MAE =

(cid:80)

i |predi − labeli|
N

,

(cid:80)

MRE =

i |predi − labeli|
i |labeli|

(cid:80)

.

For air quality data, the evaluation is performed on its original data. For heath-care data and activity
data, since the numerical values are not in the same scale, we evaluate the performances on their
normalized data. To further evaluate the classiﬁcation performances, we use area under ROC curve
(AUC) [8] for health-care data, since such data is highly unbalanced (there are 10% patients who
died in hospital). We then use standard accuracy for the activity data, since different activities are
relatively balanced.

5.2.2 Baseline Methods

We compare our model with both RNN-based methods and non-RNN based methods. The non-RNN
based imputation methods include:

• Mean: We simply replace the missing values with corresponding global mean.
• KNN: We use k-nearest neighbor [13] to ﬁnd the similar samples, and impute the missing

values with weighted average of its neighbors.

• Matrix Factorization (MF): We factorize the data matrix into two low-rank matrices, and

ﬁll the missing values by matrix completion [13].

• MICE: We use Multiple Imputation by Chained Equations (MICE), a widely used impu-
tation method, to ﬁll the missing values. MICE creates multiple imputations with chained
equations [2].

• ImputeTS: We use ImputeTS package in R to impute the missing values. ImputeTS is a
widely used package for missing value imputation, which utilizes the state space model and
kalman smoothing [23].

• STMVL: Speciﬁcally, we use STMVL for the air quality data imputation. STMVL is the
state-of-the-art method for air quality data imputation. It further utilizes the geo-locations
when imputing missing values [32].

We implement KNN, MF and MICE based on the python package fancyimpute3. In recent studies,
RNN-based models in missing value imputations achieve remarkable performances [10, 21, 12, 33].
We also compare our model with existing RNN-based imputation methods, including:

• GRU-D: GRU-D is proposed for handling missing values in health-care data. It imputes
each missing value by the weighted combination of its last observation, and the global mean,
together with a recurrent component [10].

• M-RNN: M-RNN also uses bi-directional RNN. It imputes the missing values according to
hidden states in both directions in RNN. M-RNN treats the imputed values as constants. It
does not consider the correlations among different missing values[33].

3https://github.com/iskandr/fancyimpute

7

Table 1: Performance Comparison for Imputation Tasks (in MAE(MRE%))

Method

Non-RNN

RNN

Ours

Mean
KNN
MF
MICE
ImputeTS
STMVL
GRU-D
M-RNN
RITS-I
BRITS-I
RITS
BRITS

Air Quality
55.51 (77.97%)
29.79 (41.85%)
27.94 (39.25%)
27.42 (38.52%)
19.58 (27.51%)
12.12 (17.40%)
/
14.24 (20.43%)
12.73 (18.32%)
11.58 (16.66%)
12.19 (17.54%)
11.56 (16.65%)

Health-care
0.720 (100.00%)
0.732 (101.66%)
0.622 (87.68%)
0.634 (89.17%)
0.390 (54.2%)
/
0.559 (77.58%)
0.451 (62.65%)
0.395 (54.80%)
0.361 (50.01%)
0.300 (41.89%)
0.281 (39.14%)

Human Activity
0.767 (96.43%)
0.479 (58.54%)
0.879 (110.44%)
0.477 (57.94%)
0.363 (45.65%)
/
0.558 (70.05%)
0.248 (31.19%)
0.240 (30.10%)
0.220 (27.61%)
0.248 (31.21%)
0.219 (27.59%)

We compare the baseline methods with our four models: RITS-I (Section 4.1), RITS (Section 4.2),
BRITS-I (Section 4.3) and BRITS (Section 4.3). We implement all the RNN-based models with
PyTorch, a widely used package for deep learning. All models are trained with GPU GTX 1080.

5.3 Experiment Results

Table 1 shows the imputation results. As we can see, simply applying naıve mean imputation is
very inaccurate. Alternatively, KNN, MF, and MICE perform much better than mean imputation.
ImputeTS achieves the best performance among all the non-RNN methods, especially for the heath-
care data (which is smooth and contains few sudden waves). STMVL performs well on the air
quality data. However, it is speciﬁcally designed for geographical data, and cannot be applied in
the other datasets. Most of RNN-based methods, except GRU-D, demonstrates signiﬁcantly better
performances in imputation tasks. We stress that GRU-D does not impute missing value explicitly. It
actually performs well on classiﬁcation tasks (See Appendix A for details). M-RNN uses explicitly
imputation procedure, and achieves remarkable imputation results. Our model BRITS outperforms
all the baseline models signiﬁcantly. According to the performances of our four models, we also ﬁnd
that both bidirectional recurrent dynamics, and the feature correlations are helpful to enhance the
model performance.

We also compare the accuracies of all RNN-based models in classiﬁcation tasks. GRU-D achieves
an AUC of 0.828 ± 0.004 on health-care data , and accuracy of 0.939 ± 0.010 on human activity;
M-RNN is slightly worse. It achieves 0.800 ± 0.003 and 0.938 ± 0.010 on two tasks. Our model
BRITS outperforms the baseline methods. The accuracies of BRITS are 0.850 ± 0.002 and
0.969 ± 0.008 respectively. See Appendix A for more classiﬁcation results.

Due to the space limitation, we provide more experimental results in Appendix A and B for better
understanding our model.

6 Conclusion

In this paper, we proposed BRITS, a novel method to use recurrent dynamics to effectively impute
the missing values in multivariate time series. Instead of imposing assumptions over the data-
generating process, our model directly learns the missing values in a bidirectional recurrent dynamical
system, without any speciﬁc assumption. Our model treats missing values as variables of the
bidirectional RNN graph. Thus, we get the delayed gradients for missing values in both forward and
backward directions, which makes the imputation of missing values more accurate. We performed the
missing value imputation and classiﬁcation/regression simultaneously within a joint neural network.
Experiment results show that our model demonstrates more accurate results for both imputation and
classiﬁcation/regression than state-of-the-art methods.

8

References

[1] C. F. Ansley and R. Kohn. On the estimation of arima models with missing values. In Time

series analysis of irregularly observed data, pages 9–37. Springer, 1984.

[2] M. J. Azur, E. A. Stuart, C. Frangakis, and P. J. Leaf. Multiple imputation by chained equations:
what is it and how does it work? International journal of methods in psychiatric research,
20(1):40–49, 2011.

[3] A. Basharat and M. Shah. Time series prediction by chaotic modeling of nonlinear dynamical
systems. In Computer Vision, 2009 IEEE 12th International Conference on, pages 1941–1948.
IEEE, 2009.

[4] B. Batres-Estrada. Deep learning for multivariate ﬁnancial time series, 2015.

[5] S. Bauer, B. Schölkopf, and J. Peters. The arrow of time in multivariate time series.

In

International Conference on Machine Learning, pages 2043–2051, 2016.

[6] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction
with recurrent neural networks. In Advances in Neural Information Processing Systems, pages
1171–1179, 2015.

[7] M. Berglund, T. Raiko, M. Honkala, L. Kärkkäinen, A. Vetek, and J. T. Karhunen. Bidirectional
recurrent neural networks as generative models. In Advances in Neural Information Processing
Systems, pages 856–864, 2015.

[8] A. P. Bradley. The use of the area under the roc curve in the evaluation of machine learning

algorithms. Pattern recognition, 30(7):1145–1159, 1997.

[9] P. Brakel, D. Stroobandt, and B. Schrauwen. Training energy-based models for time-series

imputation. The Journal of Machine Learning Research, 14(1):2771–2797, 2013.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for

multivariate time series with missing values. Scientiﬁc reports, 8(1):6085, 2018.

[11] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and
Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078, 2014.

[12] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun. Doctor ai: Predicting clinical
events via recurrent neural networks. In Machine Learning for Healthcare Conference, pages
301–318, 2016.

[13] J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning, volume 1.

Springer series in statistics Springer, Berlin, 2001.

[14] D. S. Fung. Methods for the estimation of missing values in time series. 2006.
[15] A. C. Harvey. Forecasting, structural time series models and the Kalman ﬁlter. Cambridge

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735,

university press, 1990.

1997.

[17] G. Ian, B. Yoshua, and C. Aaron. Deep learning. Book in preparation for MIT Press, 2016.

[18] B. Kaluža, V. Mirchevska, E. Dovgan, M. Luštrek, and M. Gams. An agent-based approach
to care in independent living. In International joint conference on ambient intelligence, pages
177–186. Springer, 2010.

[19] D. M. Kreindler and C. J. Lumsden. The effects of the irregular sample and missing data in
time series analysis. Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using
Real Data, page 135, 2012.

[20] L. Li, J. McCann, N. S. Pollard, and C. Faloutsos. Dynammo: Mining and summarization
In Proceedings of the 15th ACM SIGKDD
of coevolving sequences with missing values.
international conference on Knowledge discovery and data mining, pages 507–516. ACM,
2009.

[21] Z. C. Lipton, D. Kale, and R. Wetzel. Directly modeling missing data in sequences with rnns:
Improved classiﬁcation of clinical time series. In Machine Learning for Healthcare Conference,
pages 253–270, 2016.

9

[22] Z. Liu and M. Hauskrecht. Learning linear dynamical systems from multivariate time series:
In Proceedings of the 2016 SIAM International

A matrix factorization based framework.
Conference on Data Mining, pages 810–818. SIAM, 2016.

[23] S. Moritz and T. Bartz-Beielstein. imputeTS: Time Series Missing Value Imputation in R. The

[24] T. Ozaki. 2 non-linear time series models and dynamical systems. Handbook of statistics,

R Journal, 9(1):207–218, 2017.

5:25–83, 1985.

[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks.

In International Conference on Machine Learning, pages 1310–1318, 2013.

[26] S. Rani and G. Sikka. Recent techniques of clustering of time series data: a survey. International

Journal of Computer Applications, 52(15), 2012.

[27] I. Silva, G. Moody, D. J. Scott, L. A. Celi, and R. G. Mark. Predicting in-hospital mortality
of icu patients: The physionet/computing in cardiology challenge 2012. In Computing in
Cardiology (CinC), 2012, pages 245–248. IEEE, 2012.
[28] D. Sussillo and O. Barak. Opening the black box:

low-dimensional dynamics in high-

dimensional recurrent neural networks. Neural computation, 25(3):626–649, 2013.

[29] D. Wang, W. Cao, J. Li, and J. Ye. Deepsd: supply-demand prediction for online car-hailing ser-
vices using deep neural networks. In Data Engineering (ICDE), 2017 IEEE 33rd International
Conference on, pages 243–254. IEEE, 2017.

[30] J. Wang, A. P. De Vries, and M. J. Reinders. Unifying user-based and item-based collaborative
ﬁltering approaches by similarity fusion. In Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in information retrieval, pages 501–508. ACM,
2006.

[31] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo. Convolutional lstm
network: A machine learning approach for precipitation nowcasting. In Advances in neural
information processing systems, pages 802–810, 2015.

[32] X. Yi, Y. Zheng, J. Zhang, and T. Li. St-mvl: ﬁlling missing values in geo-sensory time series

data. 2016.

[33] J. Yoon, W. R. Zame, and M. van der Schaar. Multi-directional recurrent neural networks: A

novel method for estimating missing data. 2017.

[34] H.-F. Yu, N. Rao, and I. S. Dhillon. Temporal regularized matrix factorization for high-
dimensional time series prediction. In Advances in neural information processing systems,
pages 847–855, 2016.

[35] J. Zhang, Y. Zheng, and D. Qi. Deep spatio-temporal residual networks for citywide crowd

ﬂows prediction. AAAI, November 2017.

10

A Performance Comparison for Classiﬁcation Tasks

Table 2 shows the performances of different RNN-based models on the classiﬁcation tasks. As we can
see, our model BRITS outperforms other baseline methods for classiﬁcations. Comparing with Table
1, GRU-D performs well for classiﬁcation tasks. Furthermore, we ﬁnd that it is very important for
GRU-D to carefully apply the dropout techniques in order to prevent the overﬁtting (we use p = 0.25
dropout layer on the top classiﬁcation layer). However, our models further utilize the imputation
errors as the supervised signals. It seems that dropout is not necessary for our models during training.

Table 2: Performance Comparison for Classiﬁcation Tasks

Method Health-care (AUC) Human Activity (Accuracy)
GRU-D
M-RNN

0.939 ± 0.010
0.938 ± 0.010

0.828 ± 0.004
0.800 ± 0.003

RITS-I
BRITS-I
RITS
BRITS

0.821 ± 0.007
0.831 ± 0.003
0.840 ± 0.004
0.850 ± 0.002

0.934 ± 0.008
0.940 ± 0.012
0.968 ± 0.010
0.969 ± 0.008

B Performance Comparison for Univariate Synthetic Data

To better understand our model, we generate a set of univariate synthetic time series. Speﬁcically, we
randomly generate a time series with length T = 36, using the state-space representation [15]:
xt = µt + θt + (cid:15)t,
µt = µt−1 + λt−1 + ξt,
λt = λt−1 + ζt,
s−1
(cid:88)

θt =

−θt−j + ωt,

j=1

where xt is the t-th value in time series. The residual terms (cid:15)t, ξt, ζt and ωt are randomly drawn
from a normal distribution N(0, 0.3). We eliminate about 22% values from the generated series, and
compare our model BRITS-I (note the data is univariate) and ImputeTS. We show three examples in
Fig. 3.

Figure 3: Example for synthetic time series imputation. Each column corresponds to one time series.
The ﬁgures in the ﬁrst row are imputations of ImputeTS algorithm, and the ﬁgures in the second row
are imputations by BRITS-I.

The ﬁrst row corresponds to the imputations of ImputeTS and the second row corresponds to our model
BRITS-I. As we can see, our model demonstrates better performance than ImputeTS. Especially,
ImputeTS fails when the start part of time series is missing. However, for our model, the imputation
errors are backpropagated to previous time steps. Thus, our model can adjust the imputations in the
start part with delayed gradient, which leads to much more accurate results.

11

Figure 4: Validation errors for BRITS and BRITS-cut during training

C Performance for Non-differentiable ˆxt

As we claimed in Section 4.1, we regard the missing values as variables of RNN graph. During the
backpropagation, the imputed values can be further updated sufﬁciently. In the experiment, we ﬁnd
that if we cut the gradient of ˆxt (i.e., regard it as constant), the models are unstable and easy to overﬁt
during the training. We refer to the model with non-differentiate ˆxt as BRITS-cut. Fig. 4 shows the
validation errors of BRITS and BRITS-cut during training for the health-care data imputation. At the
ﬁrst 20 iterations, the validation error of BRITS-cut decreases fast. However, it soon fails due to the
overﬁtting.

D Test Data of Air Quality Imputation

We use the same method as in prior work [32] to select the test data for air quality imputation.
Speciﬁcally, we use the 3rd, 6th, 9th and 12th months as the test set and the rest data as the training
set. To evaluate our model, we select the test timeslots by the following rule: if a measurement
is observed at a timeslot in one of these four months (e.g., 8 o’clock 2015/03/07), we check the
corresponding position in its previous month (e.g., 8 o’clock 2015/02/07). If it is absent in the
previous month, we eliminate such value and use it as the imputation ground-truth of test data. Recall
that to train our model, we randomly select consecutive 36 time steps as one time series. Thus, a test
timeslot may be contained in multiple time series. We use the mean of the imputations in different
time series as the ﬁnal result.

12

8
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
1
v
2
7
5
0
1
.
5
0
8
1
:
v
i
X
r
a

BRITS: Bidirectional Recurrent Imputation for Time
Series

Wei Cao1,3

Dong Wang2

Jian Li1

Hao Zhou3

Lei Li3

Yitan Li3

1Tsinghua University

2Duke University

3 Toutiao AILab

Abstract

Time series are widely used as signals in many classiﬁcation/regression tasks.
It is ubiquitous that time series contains many missing values. Given multiple
correlated time series data, how to ﬁll in missing values and to predict their class
labels? Existing imputation methods often impose strong assumptions of the
underlying data generating process, such as linear dynamics in the state space. In
this paper, we propose BRITS, a novel method based on recurrent neural networks
for missing value imputation in time series data. Our proposed method directly
learns the missing values in a bidirectional recurrent dynamical system, without
any speciﬁc assumption. The imputed values are treated as variables of RNN
graph and can be effectively updated during the backpropagation. BRITS has three
advantages: (a) it can handle multiple correlated missing values in time series; (b)
it generalizes to time series with nonlinear dynamics underlying; (c) it provides
a data-driven imputation procedure and applies to general settings with missing
data. We evaluate our model on three real-world datasets, including an air quality
dataset, a health-care data, and a localization data for human activity. Experiments
show that our model outperforms the state-of-the-art methods in both imputation
and classiﬁcation/regression accuracies.

1

Introduction

Multivariate time series data are abundant in many application areas, such as ﬁnancial marketing
[5, 4], health-care [10, 22], meteorology [31, 26], and trafﬁc engineering [29, 35]. Time series
are widely used as signals for classiﬁcation/regression in various applications of corresponding
areas. However, missing values in time series are very common, due to unexpected accidents, such
as equipment damage or communication error, and may signiﬁcantly harm the performance of
downstream applications.

Much prior work proposed to ﬁx the missing data problem with statistics and machine learning
approaches. However most of them require fairly strong assumptions on missing values. We can ﬁll
the missing values using classical statistical time series models such as ARMA or ARIMA (e.g., [1]).
But these models are essentially linear (after differencing). Kreindler et al. [19] assume that the data
are smoothable, i.e., there is no sudden wave in the periods of missing values, hence imputing missing
values can be done by smoothing over nearby values. Matrix completion has also been used to
address missing value problems (e.g., [30, 34]). But it typically applies to only static data and requires
strong assumptions such as low-rankness. We can also predict missing values by ﬁtting a parametric
data-generating model with the observations [14, 2], which assumes that data of time series follow the
distribution of hypothetical models. These assumptions make corresponding imputation algorithms
less general, and the performance less desirable when the assumptions do not hold.

In this paper, we propose BRITS, a novel method for ﬁlling the missing values for multiple correlated
time series. Internally, BRITS adapts recurrent neural networks (RNN) [16, 11] for imputing missing

Preprint. Work in progress.

values, without any speciﬁc assumption over the data. Much prior work uses non-linear dynamical
systems for time series prediction [9, 24, 3]. In our method, we instantiate the dynamical system as a
bidirectional RNN, i.e., imputing missing values with bidirectional recurrent dynamics. In particular,
we make the following technical contributions:

• We design a bidirectional RNN model for imputing missing values. We directly use RNN for
predicting missing values, instead of tuning weights for smoothing as in [10]. Our method
does not impose speciﬁc assumption, hence works more generally than previous methods.

• We regard missing values as variables of the bidirectional RNN graph, which are involved
in the backpropagation process. In such case, missing values get delayed gradients in both
forward and backward directions with consistency constraints, which makes the estimation
of missing values more accurate.

• We simultaneously perform missing value imputation and classiﬁcation/regression of appli-
cations jointly in one neural graph. This alleviates the error propagation problem from impu-
tation to classiﬁcation/regression. Additionally, the supervision of classiﬁcation/regression
makes the estimation of missing values more accurate.

• We evaluate our model on three real-world datasets, including an air quality dataset, a
health-care dataset and a localization dataset of human activities. Experimental results show
that our model outperforms the state-of-the-art models for both imputation and classiﬁca-
tion/regression accuracies.

2 Related Work

There is a large body of literature on the imputation of missing values in time series. We only mention
a few closely related ones. The interpolate method tries to ﬁt a "smooth curve" to the observations
and thus reconstruct the missing values by the local interpolations [19, 14]. Such method discards
any relationships between the variables over time. The autoregressive method, including ARIMA,
SARIMA etc., eliminates the non-stationary parts in the time series data and ﬁt a parameterized
stationary model. The state space model further combines ARIMA and Kalman Filter [14, 15], which
provides more accurate results. Multivariate Imputation by Chained Equations (MICE) [2] ﬁrst
initializes the missing values arbitrarily and then estimates each missing variable based on the chain
equations. The graphical model [20] introduces a latent variable for each missing value, and ﬁnds
the latent variables by learning their transition matrix. There are also some data-driven methods
for missing value imputation. Yi et al. [32] imputed the missing values in air quality data with
geographical features. Wang et al. [30] imputed missing values in recommendation system with
collaborative ﬁltering. Yu et al. [34] utilized matrix factorization with temporal regularization to
impute the missing values in regularly sampled time series data.

Recently, some researchers attempted to impute the missing values with recurrent neural networks
[7, 10, 21, 12, 33]. The recurrent components are trained together with the classiﬁcation/regression
component, which signiﬁcantly boosts the accuracy. Che et al. [10] proposed GRU-D, which imputes
missing values in health-care data with a smooth fashion. It assumes that a missing variable can be
represented as the combination of its corresponding last observed value and the global mean. GRU-D
achieves remarkable performance on health-care data. However, it has many limitations on general
datasets [10]. A closely related work is M-RNN proposed by Yoon et al. [33]. M-RNN also utilizes
bi-directional RNN to impute missing values. Differing from our model, it drops the relationships
between missing variables. The imputed values in M-RNN are treated as constants and cannot be
sufﬁciently updated.

3 Preliminary

We ﬁrst present the problem formulation and some necessary preliminaries.

series X =
Deﬁnition 1 (Multivariate Time Series) We denote a multivariate
{x1, x2, . . . , xT } as a sequence of T observations. The t-th observation xt ∈ RD consists
of D features {x1
t }, and was observed at timestamp st (the time gap between different
timestamps may be not same). In reality, due to unexpected accidents, such as equipment damage

t , . . . , xD

t , x2

time

2

or communication error, xt may have the missing values (e.g., in Fig. 1, x3
represent the missing values in xt, we introduce a masking vector mt where,
(cid:26) 0
1

if xd
otherwise

t is not observed

t =

md

.

1 in x1 is missing). To

In many cases, some features can be missing for consecutive timestamps (e.g., blue blocks in Fig. 1).
We deﬁne δd
t to be the time gap from the last observation to the current timestamp st, i.e.,

δd
t =






st − st−1 + δd
st − st−1
0

t−1

if t > 1, md
if t > 1, md
if t = 1

t−1 = 0
t−1 = 1

.

See Fig. 1 for an illustration.

Figure 1: An example of multivariate time series with missing values. x1 to x6 are observed at
s1...6 = 0, 2, 7, 9, 14, 15 respectively. Considering the 2nd feature in x6, the last observation of the
2nd feature took place at s2 = 2, and we have that δ2

6 = s6 − s2 = 13.

In this paper, we study a general setting for time series classiﬁcation/regression problems with missing
values. We use y to denote the label of corresponding classiﬁcation/regression task. In general, y
can be either a scalar or a vector. Our goal is to predict y based on the given time series X. In the
meantime, we impute the missing values in X as accurate as possible. In another word, we aim to
design an effective multi-task learning algorithm for both classiﬁcation/regression and imputation.

4 BRITS

In this section, we describe the BRITS. Differing from the prior work which uses RNN to impute
missing values in a smooth fashion [10], we learn the missing values directly in a recurrent dynamical
system [25, 28] based on the observed data. The missing values are thus imputed according to the
recurrent dynamics, which signiﬁcantly boosts both the imputation accuracy and the ﬁnal classiﬁca-
tion/regression accuracy. We start the description with the simplest case: the variables observed at
the same time are mutually uncorrelated. For such case, we propose algorithms for imputation with
unidirectional recurrent dynamics and bidirectional recurrent dynamics, respectively. We further
propose an algorithm for correlated multivariate time series in Section 4.3.

4.1 Unidirectional Uncorrelated Recurrent Imputation

For the simplest case, we assume that for the t-th step, xi
be correlated with some xj
dynamics, denoted as RITS-I.

t may
t(cid:48)(cid:54)=t). We ﬁrst propose an imputation algorithm by unidirectional recurrent

t are uncorrelated if i (cid:54)= j (but xi

t and xj

In a unidirectional recurrent dynamical system, each value in the time series can be derived by its
predecessors with a ﬁxed arbitrary function [9, 24, 3]. Thus, we iteratively impute all the variables in
the time series according to the recurrent dynamics. For the t-th step, if xt is actually observed, we
use it to validate our imputation and pass xt to the next recurrent steps. Otherwise, since the future
observations are correlated with the current value, we replace xt with the obtained imputation, and
validate it by the future observations. To be more concrete, let us consider an example.

Example 1 Suppose we are given a time series X = {x1, x2, . . . , x10}, where x5, x6 and x7 are
missing. According to the recurrent dynamics, at each time step t, we can obtain an estimation ˆxt
based on the previous t − 1 steps. In the ﬁrst 4 steps, the estimation error can be obtained immediately

3

Figure 2: Imputation with unidirectional dynamics.

by calculating the estimation loss function Le(ˆxt, xt) for t = 1, . . . , 4. However, when t = 5, 6, 7,
we cannot calculate the error immediately since the true values are missing. Nevertheless, note that
at the 8-th step, ˆx8 depends on ˆx5 to ˆx7. We thus obtain a “delayed" error for ˆxt=5,6,7 at the 8-th
step.

4.1.1 Algorithm

We introduce a recurrent component and a regression component for imputation. The recurrent
component is achieved by a recurrent neural network and the regression component is achieved by a
fully-connected network. A standard recurrent network [17] can be represented as

ht = σ(Whht−1 + Uhxt + bh),
where σ is the sigmoid function, Wh, Uh and bh are parameters, and ht is the hidden state of
previous time steps.

In our case, since xt may have missing values, we cannot use xt as the input directly as in the above
equation. Instead, we use a “complement" input xc
t derived by our algorithm when xt is missing.
Formally, we initialize the initial hidden state h0 as an all-zero vector and then update the model by:
(1)
(2)
(3)
(4)
(5)

ˆxt = Wxht−1 + bx,
xc
t = mt (cid:12) xt + (1 − mt) (cid:12) ˆxt,
γt = exp{− max(0, Wγδt + bγ)},
ht = σ(Wh[ht−1 (cid:12) γt] + Uh[xc
(cid:96)t = (cid:104)mt, Le(xt, ˆxt)(cid:105) .

t ◦ mt] + bh),

Eq. (1) is the regression component which transfers the hidden state ht−1 to the estimated vector
ˆxt. In Eq. (2), we replace missing values in xt with corresponding values in ˆxt, and obtain the
complement vector xc
t . Besides, since the time series may be irregularly sampled, in Eq. (3), we
further introduce a temporal decay factor γt to decay the hidden vector ht−1. Intuitively, if δt is large
(i.e., the values are missing for a long time), we expect a small γt to decay the hidden state. Such
factor also represents the missing patterns in the time series which is critical to imputation [10]. In Eq.
(4), based on the decayed hidden state, we predict the next state ht. Here, ◦ indicates the concatenate
operation. In the mean time, we calculate the estimation error by the estimation loss function Le in
Eq. (5). In our experiment, we use the mean absolute error for Le. Finally, we predict the task label
y as

where fout can be either a fully-connected layer or a softmax layer depending on the speciﬁc task,
and αi is the weight for different hidden state which can be derived by the attention mechanism1 or
the mean pooling mechanism, i.e., αi = 1
T . We calculate the output loss by Lout(y, ˆy). Our model is
then updated by minimizing the accumulated loss 1
T

t=1 (cid:96)t + Lout(y, ˆy).

(cid:80)T

1The design of attention mechanism is out of this paper’s scope.

ˆy = fout(

αihi),

T
(cid:88)

i=1

4

4.1.2 Practical Issues

In practice, we use LSTM as the recurrent component in Eq. (4) to prevent the gradient vanish-
ing/exploding problems in vanilla RNN [17]. Standard RNN models including LSTM treat ˆxt as a
constant. During backpropagation, gradients are cut at ˆxt. This makes the estimation errors backprop-
agate insufﬁciently. For example, in Example 1, the estimation errors of ˆx5 to ˆx7 are obtained at the
8-th step as the delayed errors. However, if we treat ˆx5 to ˆx7 as constants, such delayed error cannot
be fully backpropagated. To overcome such issue, we treat ˆxt as a variable of RNN graph. We let the
estimation error passes through ˆxt during the backpropagation. Fig. 2 shows how RITS-I method
works in Example 1. In this example, the gradients are backpropagated through the opposite direction
of solid lines. Thus, the delayed error (cid:96)8 is passed to steps 5, 6 and 7. In the experiment, we ﬁnd that
our models are unstable during training if we treat ˆxt as a constant. See Appendix C for details.

4.2 Bidirectional Uncorrelated Recurrent Imputation

In the RITS-I, errors of estimated missing values are delayed until the presence of the next observation.
For example, in Example 1, the error of ˆx5 is delayed until x8 is observed. Such error delay makes
the model converge slowly and in turn leads to inefﬁciency in training. In the meantime, it also leads
to the bias exploding problem [6], i.e., the mistakes made early in the sequential prediction are fed as
input to the model and may be quickly ampliﬁed. In this section, we propose an improved version
called BRITS-I. The algorithm alleviates the above-mentioned issues by utilizing the bidirectional
recurrent dynamics on the given time series, i.e., besides the forward direction, each value in time
series can be also derived from the backward direction by another ﬁxed arbitrary function.

To illustrate the intuition of BRITS-I algorithm, again, we consider Example 1. Consider the
backward direction of the time series. In bidirectional recurrent dynamics, the estimation ˆx4 reversely
depends on ˆx5 to ˆx7. Thus, the error in the 5-th step is back-propagated from not only the 8-th step in
the forward direction (which is far from the current position), but also the 4-th step in the backward
direction (which is closer). Formally, the BRITS-I algorithm performs the RITS-I as shown in
Eq. (1) to Eq. (5) in forward and backward directions, respectively. In the forward direction, we
obtain the estimation sequence {ˆx1, ˆx2, . . . , ˆxT } and the loss sequence {(cid:96)1, (cid:96)2, . . . , (cid:96)T }. Similarly,
in the backward direction, we obtain another estimation sequence {ˆx(cid:48)
T } and another
(cid:48)}. We enforce the prediction in each step to be consistent in both
loss sequence {(cid:96)1
directions by introducing the “consistency loss”:

2, . . . , ˆx(cid:48)

(cid:48), . . . , (cid:96)T

1, ˆx(cid:48)

(cid:48), (cid:96)2

(6)
where we use the mean squared error as the discrepancy in our experiment. The ﬁnal estimation loss
is obtained by accumulating the forward loss (cid:96)t, the backward loss (cid:96)(cid:48)
t and the consistency loss (cid:96)cons
.
The ﬁnal estimation in the t-th step is the mean of ˆxt and ˆxt

= Discrepancy(ˆxt, ˆx(cid:48)
t)

(cid:96)cons
t

(cid:48).

t

4.3 Correlated Recurrent Imputation

The previously proposed algorithms RITS-I and BRITS-I assume that features observed at the same
time are mutually uncorrelated. This may be not true in some applications. For example, in the
air quality data [32], each feature represents one measurement in a monitoring station. Obviously,
the observed measurements are spatially correlated. In general, the measurement in one monitoring
station is close to those values observed in its neighboring stations. In this case, we can estimate a
missing measurement according to both its historical data, and the measurements in its neighbors.

In this section, we propose an algorithm, which utilizes the feature correlations in the unidirectional
recurrent dynamical system. We refer to such algorithm as RITS. The feature correlated algorithm
for bidirectional case (i.e., BRITS) can be derived in the same way. Note that in Section 4.1, the
estimation ˆxt is only correlated with the historical observations, but irrelevant with the the current
observation. We refer to ˆxt as a “history-based estimation". In this section, we derive another
“feature-based estimation" for each xd
t , based on the other features at time st. Speciﬁcally, at the t-th
step, we ﬁrst obtain the complement observation xc
t by Eq. (1) and Eq. (2). Then, we deﬁne our
feature-based estimation as ˆzt where

(7)
Wz, bz are corresponding parameters. We restrict the diagonal of parameter matrix Wz to be all
zeros. Thus, the d-th element in ˆzt is exactly the estimation of xd
t , based on the other features. We

t + bz,

ˆzt = Wzxc

5

further combine the historical-based estimation ˆxt and the feature-based estimation ˆzt. We denote
the combined vector as ˆct, and we have that

βt = σ(Wβ[γt ◦ mt] + bβ)
(8)
ˆct = βt (cid:12) ˆzt + (1 − βt) (cid:12) ˆxt.
(9)
Here we use βt ∈ [0, 1]D as the weight of combining the history-based estimation ˆxt and the
feature-based estimation ˆzt. Note that ˆzt is derived from xc
t can be
history-based estimations or truly observed values, depending on the presence of the observations.
Thus, we learn the weight βt by considering both the temporal decay γt and the masking vector mt
as shown in Eq. (8). The rest parts are similar as the feature uncorrelated case. We ﬁrst replace
missing values in xt with the corresponding values in ˆct. The obtained vector is then fed to the next
recurrent step to predict memory ht:

t by Eq. (7). The elements of xc

cc
t = mt (cid:12) xt + (1 − mt) (cid:12) ˆct
ht = σ(Wh[ht−1 (cid:12) γt] + Uh[cc

(10)
(11)
However, the estimation loss is slightly different with the feature uncorrelated case. We ﬁnd that
simply using (cid:96)t = Le(xt, ˆct) leads to a very slow convergence speed. Instead, we accumulate all the
estimation errors of ˆxt, ˆzt and ˆct:

t ◦ mt] + bh).

(cid:96)t = Le(xt, ˆxt) + Le(xt, ˆzt) + Le(xt, ˆct).

Our proposed methods are applicable to a wide variety of applications. We evaluate our methods on
three different real-world datasets. The download links of the datasets, as well as the implementation
codes can be found in the GitHub page2.

5 Experiment

5.1 Dataset Description

5.1.1 Air Quality Data

We evaluate our models on the air quality dataset, which consists of PM2.5 measurements from
36 monitoring stations in Beijing. The measurements are hourly collected from 2014/05/01 to
2015/04/30. Overall, there are 13.3% values are missing. For this dataset, we do pure imputation
task. We use the exactly same train/test setting as in prior work [32], i.e., we use the 3rd, 6th, 9th and
12th months as the test data and the other months as the training data. See Appendix D for details. To
train our model, we randomly select every 36 consecutive steps as one time series.

5.1.2 Health-care Data

We evaluate our models on health-care data in PhysioNet Challenge 2012 [27], which consists of
4000 multivariate clinical time series from intensive care unit (ICU). Each time series contains 35
measurements such as Albumin, heart-rate etc., which are irregularly sampled at the ﬁrst 48 hours
after the patient’s admission to ICU. We stress that this dataset is extremely sparse. There are up to
78% missing values in total. For this dataset, we do both the imputation task and the classiﬁcation
task. To evaluate the imputation performance, we randomly eliminate 10% of observed measurements
from data and use them as the ground-truth. At the same time, we predict the in-hospital death of
each patient as a binary classiﬁcation task. Note that the eliminated measurements are only used for
validating the imputation, and they are never visible to the model.

5.1.3 Localization for Human Activity Data

The UCI localization data for human activity [18] contains records of ﬁve people performing different
activities such as walking, falling, sitting down etc (there are 11 activities). Each person wore four
sensors on her/his left/right ankle, chest, and belt. Each sensor recorded a 3-dimensional coordinates
for about 20 to 40 millisecond. We randomly select 40 consecutive steps as one time series, and
there are 30, 917 time series in total. For this dataset, we do both imputation and classiﬁcation tasks.
Similarly, we randomly eliminate 10% observed data as the imputation ground-truth. We further
predict the corresponding activity of observed time series (i.e., walking, sitting, etc).

2https://github.com/NIPS-BRITS/BRITS

6

5.2 Experiment Setting

5.2.1 Model Implementations

We ﬁx the dimension of hidden state ht in RNN to be 64. We train our model by an Adam optimizer
with learning rate 0.001 and batch size 64. For all the tasks, we normalize the numerical values to
have zero mean and unit variance for stable training.

We use different early stopping strategies for pure imputation task and classiﬁcation tasks. For the
imputation tasks, we randomly select 10% of non-missing values as the validation data. The early
stopping is thus performed based on the validation error. For the classiﬁcation tasks, we ﬁrst pre-train
the model as an imputation task. Then we use 5-fold cross validation to further optimize both the
imputation and classiﬁcation losses simultaneously.

We evaluate the imputation performance in terms of mean absolute error (MAE) and mean relative
error (MRE). Suppose that labeli is the ground-truth of the i-th item, predi is the output of the i-th
item, and there are N items in total. Then, MAE and MRE are deﬁned as

MAE =

(cid:80)

i |predi − labeli|
N

,

(cid:80)

MRE =

i |predi − labeli|
i |labeli|

(cid:80)

.

For air quality data, the evaluation is performed on its original data. For heath-care data and activity
data, since the numerical values are not in the same scale, we evaluate the performances on their
normalized data. To further evaluate the classiﬁcation performances, we use area under ROC curve
(AUC) [8] for health-care data, since such data is highly unbalanced (there are 10% patients who
died in hospital). We then use standard accuracy for the activity data, since different activities are
relatively balanced.

5.2.2 Baseline Methods

We compare our model with both RNN-based methods and non-RNN based methods. The non-RNN
based imputation methods include:

• Mean: We simply replace the missing values with corresponding global mean.
• KNN: We use k-nearest neighbor [13] to ﬁnd the similar samples, and impute the missing

values with weighted average of its neighbors.

• Matrix Factorization (MF): We factorize the data matrix into two low-rank matrices, and

ﬁll the missing values by matrix completion [13].

• MICE: We use Multiple Imputation by Chained Equations (MICE), a widely used impu-
tation method, to ﬁll the missing values. MICE creates multiple imputations with chained
equations [2].

• ImputeTS: We use ImputeTS package in R to impute the missing values. ImputeTS is a
widely used package for missing value imputation, which utilizes the state space model and
kalman smoothing [23].

• STMVL: Speciﬁcally, we use STMVL for the air quality data imputation. STMVL is the
state-of-the-art method for air quality data imputation. It further utilizes the geo-locations
when imputing missing values [32].

We implement KNN, MF and MICE based on the python package fancyimpute3. In recent studies,
RNN-based models in missing value imputations achieve remarkable performances [10, 21, 12, 33].
We also compare our model with existing RNN-based imputation methods, including:

• GRU-D: GRU-D is proposed for handling missing values in health-care data. It imputes
each missing value by the weighted combination of its last observation, and the global mean,
together with a recurrent component [10].

• M-RNN: M-RNN also uses bi-directional RNN. It imputes the missing values according to
hidden states in both directions in RNN. M-RNN treats the imputed values as constants. It
does not consider the correlations among different missing values[33].

3https://github.com/iskandr/fancyimpute

7

Table 1: Performance Comparison for Imputation Tasks (in MAE(MRE%))

Method

Non-RNN

RNN

Ours

Mean
KNN
MF
MICE
ImputeTS
STMVL
GRU-D
M-RNN
RITS-I
BRITS-I
RITS
BRITS

Air Quality
55.51 (77.97%)
29.79 (41.85%)
27.94 (39.25%)
27.42 (38.52%)
19.58 (27.51%)
12.12 (17.40%)
/
14.24 (20.43%)
12.73 (18.32%)
11.58 (16.66%)
12.19 (17.54%)
11.56 (16.65%)

Health-care
0.720 (100.00%)
0.732 (101.66%)
0.622 (87.68%)
0.634 (89.17%)
0.390 (54.2%)
/
0.559 (77.58%)
0.451 (62.65%)
0.395 (54.80%)
0.361 (50.01%)
0.300 (41.89%)
0.281 (39.14%)

Human Activity
0.767 (96.43%)
0.479 (58.54%)
0.879 (110.44%)
0.477 (57.94%)
0.363 (45.65%)
/
0.558 (70.05%)
0.248 (31.19%)
0.240 (30.10%)
0.220 (27.61%)
0.248 (31.21%)
0.219 (27.59%)

We compare the baseline methods with our four models: RITS-I (Section 4.1), RITS (Section 4.2),
BRITS-I (Section 4.3) and BRITS (Section 4.3). We implement all the RNN-based models with
PyTorch, a widely used package for deep learning. All models are trained with GPU GTX 1080.

5.3 Experiment Results

Table 1 shows the imputation results. As we can see, simply applying naıve mean imputation is
very inaccurate. Alternatively, KNN, MF, and MICE perform much better than mean imputation.
ImputeTS achieves the best performance among all the non-RNN methods, especially for the heath-
care data (which is smooth and contains few sudden waves). STMVL performs well on the air
quality data. However, it is speciﬁcally designed for geographical data, and cannot be applied in
the other datasets. Most of RNN-based methods, except GRU-D, demonstrates signiﬁcantly better
performances in imputation tasks. We stress that GRU-D does not impute missing value explicitly. It
actually performs well on classiﬁcation tasks (See Appendix A for details). M-RNN uses explicitly
imputation procedure, and achieves remarkable imputation results. Our model BRITS outperforms
all the baseline models signiﬁcantly. According to the performances of our four models, we also ﬁnd
that both bidirectional recurrent dynamics, and the feature correlations are helpful to enhance the
model performance.

We also compare the accuracies of all RNN-based models in classiﬁcation tasks. GRU-D achieves
an AUC of 0.828 ± 0.004 on health-care data , and accuracy of 0.939 ± 0.010 on human activity;
M-RNN is slightly worse. It achieves 0.800 ± 0.003 and 0.938 ± 0.010 on two tasks. Our model
BRITS outperforms the baseline methods. The accuracies of BRITS are 0.850 ± 0.002 and
0.969 ± 0.008 respectively. See Appendix A for more classiﬁcation results.

Due to the space limitation, we provide more experimental results in Appendix A and B for better
understanding our model.

6 Conclusion

In this paper, we proposed BRITS, a novel method to use recurrent dynamics to effectively impute
the missing values in multivariate time series. Instead of imposing assumptions over the data-
generating process, our model directly learns the missing values in a bidirectional recurrent dynamical
system, without any speciﬁc assumption. Our model treats missing values as variables of the
bidirectional RNN graph. Thus, we get the delayed gradients for missing values in both forward and
backward directions, which makes the imputation of missing values more accurate. We performed the
missing value imputation and classiﬁcation/regression simultaneously within a joint neural network.
Experiment results show that our model demonstrates more accurate results for both imputation and
classiﬁcation/regression than state-of-the-art methods.

8

References

[1] C. F. Ansley and R. Kohn. On the estimation of arima models with missing values. In Time

series analysis of irregularly observed data, pages 9–37. Springer, 1984.

[2] M. J. Azur, E. A. Stuart, C. Frangakis, and P. J. Leaf. Multiple imputation by chained equations:
what is it and how does it work? International journal of methods in psychiatric research,
20(1):40–49, 2011.

[3] A. Basharat and M. Shah. Time series prediction by chaotic modeling of nonlinear dynamical
systems. In Computer Vision, 2009 IEEE 12th International Conference on, pages 1941–1948.
IEEE, 2009.

[4] B. Batres-Estrada. Deep learning for multivariate ﬁnancial time series, 2015.

[5] S. Bauer, B. Schölkopf, and J. Peters. The arrow of time in multivariate time series.

In

International Conference on Machine Learning, pages 2043–2051, 2016.

[6] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction
with recurrent neural networks. In Advances in Neural Information Processing Systems, pages
1171–1179, 2015.

[7] M. Berglund, T. Raiko, M. Honkala, L. Kärkkäinen, A. Vetek, and J. T. Karhunen. Bidirectional
recurrent neural networks as generative models. In Advances in Neural Information Processing
Systems, pages 856–864, 2015.

[8] A. P. Bradley. The use of the area under the roc curve in the evaluation of machine learning

algorithms. Pattern recognition, 30(7):1145–1159, 1997.

[9] P. Brakel, D. Stroobandt, and B. Schrauwen. Training energy-based models for time-series

imputation. The Journal of Machine Learning Research, 14(1):2771–2797, 2013.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for

multivariate time series with missing values. Scientiﬁc reports, 8(1):6085, 2018.

[11] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and
Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078, 2014.

[12] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun. Doctor ai: Predicting clinical
events via recurrent neural networks. In Machine Learning for Healthcare Conference, pages
301–318, 2016.

[13] J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning, volume 1.

Springer series in statistics Springer, Berlin, 2001.

[14] D. S. Fung. Methods for the estimation of missing values in time series. 2006.
[15] A. C. Harvey. Forecasting, structural time series models and the Kalman ﬁlter. Cambridge

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735,

university press, 1990.

1997.

[17] G. Ian, B. Yoshua, and C. Aaron. Deep learning. Book in preparation for MIT Press, 2016.

[18] B. Kaluža, V. Mirchevska, E. Dovgan, M. Luštrek, and M. Gams. An agent-based approach
to care in independent living. In International joint conference on ambient intelligence, pages
177–186. Springer, 2010.

[19] D. M. Kreindler and C. J. Lumsden. The effects of the irregular sample and missing data in
time series analysis. Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using
Real Data, page 135, 2012.

[20] L. Li, J. McCann, N. S. Pollard, and C. Faloutsos. Dynammo: Mining and summarization
In Proceedings of the 15th ACM SIGKDD
of coevolving sequences with missing values.
international conference on Knowledge discovery and data mining, pages 507–516. ACM,
2009.

[21] Z. C. Lipton, D. Kale, and R. Wetzel. Directly modeling missing data in sequences with rnns:
Improved classiﬁcation of clinical time series. In Machine Learning for Healthcare Conference,
pages 253–270, 2016.

9

[22] Z. Liu and M. Hauskrecht. Learning linear dynamical systems from multivariate time series:
In Proceedings of the 2016 SIAM International

A matrix factorization based framework.
Conference on Data Mining, pages 810–818. SIAM, 2016.

[23] S. Moritz and T. Bartz-Beielstein. imputeTS: Time Series Missing Value Imputation in R. The

[24] T. Ozaki. 2 non-linear time series models and dynamical systems. Handbook of statistics,

R Journal, 9(1):207–218, 2017.

5:25–83, 1985.

[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks.

In International Conference on Machine Learning, pages 1310–1318, 2013.

[26] S. Rani and G. Sikka. Recent techniques of clustering of time series data: a survey. International

Journal of Computer Applications, 52(15), 2012.

[27] I. Silva, G. Moody, D. J. Scott, L. A. Celi, and R. G. Mark. Predicting in-hospital mortality
of icu patients: The physionet/computing in cardiology challenge 2012. In Computing in
Cardiology (CinC), 2012, pages 245–248. IEEE, 2012.
[28] D. Sussillo and O. Barak. Opening the black box:

low-dimensional dynamics in high-

dimensional recurrent neural networks. Neural computation, 25(3):626–649, 2013.

[29] D. Wang, W. Cao, J. Li, and J. Ye. Deepsd: supply-demand prediction for online car-hailing ser-
vices using deep neural networks. In Data Engineering (ICDE), 2017 IEEE 33rd International
Conference on, pages 243–254. IEEE, 2017.

[30] J. Wang, A. P. De Vries, and M. J. Reinders. Unifying user-based and item-based collaborative
ﬁltering approaches by similarity fusion. In Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in information retrieval, pages 501–508. ACM,
2006.

[31] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo. Convolutional lstm
network: A machine learning approach for precipitation nowcasting. In Advances in neural
information processing systems, pages 802–810, 2015.

[32] X. Yi, Y. Zheng, J. Zhang, and T. Li. St-mvl: ﬁlling missing values in geo-sensory time series

data. 2016.

[33] J. Yoon, W. R. Zame, and M. van der Schaar. Multi-directional recurrent neural networks: A

novel method for estimating missing data. 2017.

[34] H.-F. Yu, N. Rao, and I. S. Dhillon. Temporal regularized matrix factorization for high-
dimensional time series prediction. In Advances in neural information processing systems,
pages 847–855, 2016.

[35] J. Zhang, Y. Zheng, and D. Qi. Deep spatio-temporal residual networks for citywide crowd

ﬂows prediction. AAAI, November 2017.

10

A Performance Comparison for Classiﬁcation Tasks

Table 2 shows the performances of different RNN-based models on the classiﬁcation tasks. As we can
see, our model BRITS outperforms other baseline methods for classiﬁcations. Comparing with Table
1, GRU-D performs well for classiﬁcation tasks. Furthermore, we ﬁnd that it is very important for
GRU-D to carefully apply the dropout techniques in order to prevent the overﬁtting (we use p = 0.25
dropout layer on the top classiﬁcation layer). However, our models further utilize the imputation
errors as the supervised signals. It seems that dropout is not necessary for our models during training.

Table 2: Performance Comparison for Classiﬁcation Tasks

Method Health-care (AUC) Human Activity (Accuracy)
GRU-D
M-RNN

0.939 ± 0.010
0.938 ± 0.010

0.828 ± 0.004
0.800 ± 0.003

RITS-I
BRITS-I
RITS
BRITS

0.821 ± 0.007
0.831 ± 0.003
0.840 ± 0.004
0.850 ± 0.002

0.934 ± 0.008
0.940 ± 0.012
0.968 ± 0.010
0.969 ± 0.008

B Performance Comparison for Univariate Synthetic Data

To better understand our model, we generate a set of univariate synthetic time series. Speﬁcically, we
randomly generate a time series with length T = 36, using the state-space representation [15]:
xt = µt + θt + (cid:15)t,
µt = µt−1 + λt−1 + ξt,
λt = λt−1 + ζt,
s−1
(cid:88)

θt =

−θt−j + ωt,

j=1

where xt is the t-th value in time series. The residual terms (cid:15)t, ξt, ζt and ωt are randomly drawn
from a normal distribution N(0, 0.3). We eliminate about 22% values from the generated series, and
compare our model BRITS-I (note the data is univariate) and ImputeTS. We show three examples in
Fig. 3.

Figure 3: Example for synthetic time series imputation. Each column corresponds to one time series.
The ﬁgures in the ﬁrst row are imputations of ImputeTS algorithm, and the ﬁgures in the second row
are imputations by BRITS-I.

The ﬁrst row corresponds to the imputations of ImputeTS and the second row corresponds to our model
BRITS-I. As we can see, our model demonstrates better performance than ImputeTS. Especially,
ImputeTS fails when the start part of time series is missing. However, for our model, the imputation
errors are backpropagated to previous time steps. Thus, our model can adjust the imputations in the
start part with delayed gradient, which leads to much more accurate results.

11

Figure 4: Validation errors for BRITS and BRITS-cut during training

C Performance for Non-differentiable ˆxt

As we claimed in Section 4.1, we regard the missing values as variables of RNN graph. During the
backpropagation, the imputed values can be further updated sufﬁciently. In the experiment, we ﬁnd
that if we cut the gradient of ˆxt (i.e., regard it as constant), the models are unstable and easy to overﬁt
during the training. We refer to the model with non-differentiate ˆxt as BRITS-cut. Fig. 4 shows the
validation errors of BRITS and BRITS-cut during training for the health-care data imputation. At the
ﬁrst 20 iterations, the validation error of BRITS-cut decreases fast. However, it soon fails due to the
overﬁtting.

D Test Data of Air Quality Imputation

We use the same method as in prior work [32] to select the test data for air quality imputation.
Speciﬁcally, we use the 3rd, 6th, 9th and 12th months as the test set and the rest data as the training
set. To evaluate our model, we select the test timeslots by the following rule: if a measurement
is observed at a timeslot in one of these four months (e.g., 8 o’clock 2015/03/07), we check the
corresponding position in its previous month (e.g., 8 o’clock 2015/02/07). If it is absent in the
previous month, we eliminate such value and use it as the imputation ground-truth of test data. Recall
that to train our model, we randomly select consecutive 36 time steps as one time series. Thus, a test
timeslot may be contained in multiple time series. We use the mean of the imputations in different
time series as the ﬁnal result.

12

8
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
1
v
2
7
5
0
1
.
5
0
8
1
:
v
i
X
r
a

BRITS: Bidirectional Recurrent Imputation for Time
Series

Wei Cao1,3

Dong Wang2

Jian Li1

Hao Zhou3

Lei Li3

Yitan Li3

1Tsinghua University

2Duke University

3 Toutiao AILab

Abstract

Time series are widely used as signals in many classiﬁcation/regression tasks.
It is ubiquitous that time series contains many missing values. Given multiple
correlated time series data, how to ﬁll in missing values and to predict their class
labels? Existing imputation methods often impose strong assumptions of the
underlying data generating process, such as linear dynamics in the state space. In
this paper, we propose BRITS, a novel method based on recurrent neural networks
for missing value imputation in time series data. Our proposed method directly
learns the missing values in a bidirectional recurrent dynamical system, without
any speciﬁc assumption. The imputed values are treated as variables of RNN
graph and can be effectively updated during the backpropagation. BRITS has three
advantages: (a) it can handle multiple correlated missing values in time series; (b)
it generalizes to time series with nonlinear dynamics underlying; (c) it provides
a data-driven imputation procedure and applies to general settings with missing
data. We evaluate our model on three real-world datasets, including an air quality
dataset, a health-care data, and a localization data for human activity. Experiments
show that our model outperforms the state-of-the-art methods in both imputation
and classiﬁcation/regression accuracies.

1

Introduction

Multivariate time series data are abundant in many application areas, such as ﬁnancial marketing
[5, 4], health-care [10, 22], meteorology [31, 26], and trafﬁc engineering [29, 35]. Time series
are widely used as signals for classiﬁcation/regression in various applications of corresponding
areas. However, missing values in time series are very common, due to unexpected accidents, such
as equipment damage or communication error, and may signiﬁcantly harm the performance of
downstream applications.

Much prior work proposed to ﬁx the missing data problem with statistics and machine learning
approaches. However most of them require fairly strong assumptions on missing values. We can ﬁll
the missing values using classical statistical time series models such as ARMA or ARIMA (e.g., [1]).
But these models are essentially linear (after differencing). Kreindler et al. [19] assume that the data
are smoothable, i.e., there is no sudden wave in the periods of missing values, hence imputing missing
values can be done by smoothing over nearby values. Matrix completion has also been used to
address missing value problems (e.g., [30, 34]). But it typically applies to only static data and requires
strong assumptions such as low-rankness. We can also predict missing values by ﬁtting a parametric
data-generating model with the observations [14, 2], which assumes that data of time series follow the
distribution of hypothetical models. These assumptions make corresponding imputation algorithms
less general, and the performance less desirable when the assumptions do not hold.

In this paper, we propose BRITS, a novel method for ﬁlling the missing values for multiple correlated
time series. Internally, BRITS adapts recurrent neural networks (RNN) [16, 11] for imputing missing

Preprint. Work in progress.

values, without any speciﬁc assumption over the data. Much prior work uses non-linear dynamical
systems for time series prediction [9, 24, 3]. In our method, we instantiate the dynamical system as a
bidirectional RNN, i.e., imputing missing values with bidirectional recurrent dynamics. In particular,
we make the following technical contributions:

• We design a bidirectional RNN model for imputing missing values. We directly use RNN for
predicting missing values, instead of tuning weights for smoothing as in [10]. Our method
does not impose speciﬁc assumption, hence works more generally than previous methods.

• We regard missing values as variables of the bidirectional RNN graph, which are involved
in the backpropagation process. In such case, missing values get delayed gradients in both
forward and backward directions with consistency constraints, which makes the estimation
of missing values more accurate.

• We simultaneously perform missing value imputation and classiﬁcation/regression of appli-
cations jointly in one neural graph. This alleviates the error propagation problem from impu-
tation to classiﬁcation/regression. Additionally, the supervision of classiﬁcation/regression
makes the estimation of missing values more accurate.

• We evaluate our model on three real-world datasets, including an air quality dataset, a
health-care dataset and a localization dataset of human activities. Experimental results show
that our model outperforms the state-of-the-art models for both imputation and classiﬁca-
tion/regression accuracies.

2 Related Work

There is a large body of literature on the imputation of missing values in time series. We only mention
a few closely related ones. The interpolate method tries to ﬁt a "smooth curve" to the observations
and thus reconstruct the missing values by the local interpolations [19, 14]. Such method discards
any relationships between the variables over time. The autoregressive method, including ARIMA,
SARIMA etc., eliminates the non-stationary parts in the time series data and ﬁt a parameterized
stationary model. The state space model further combines ARIMA and Kalman Filter [14, 15], which
provides more accurate results. Multivariate Imputation by Chained Equations (MICE) [2] ﬁrst
initializes the missing values arbitrarily and then estimates each missing variable based on the chain
equations. The graphical model [20] introduces a latent variable for each missing value, and ﬁnds
the latent variables by learning their transition matrix. There are also some data-driven methods
for missing value imputation. Yi et al. [32] imputed the missing values in air quality data with
geographical features. Wang et al. [30] imputed missing values in recommendation system with
collaborative ﬁltering. Yu et al. [34] utilized matrix factorization with temporal regularization to
impute the missing values in regularly sampled time series data.

Recently, some researchers attempted to impute the missing values with recurrent neural networks
[7, 10, 21, 12, 33]. The recurrent components are trained together with the classiﬁcation/regression
component, which signiﬁcantly boosts the accuracy. Che et al. [10] proposed GRU-D, which imputes
missing values in health-care data with a smooth fashion. It assumes that a missing variable can be
represented as the combination of its corresponding last observed value and the global mean. GRU-D
achieves remarkable performance on health-care data. However, it has many limitations on general
datasets [10]. A closely related work is M-RNN proposed by Yoon et al. [33]. M-RNN also utilizes
bi-directional RNN to impute missing values. Differing from our model, it drops the relationships
between missing variables. The imputed values in M-RNN are treated as constants and cannot be
sufﬁciently updated.

3 Preliminary

We ﬁrst present the problem formulation and some necessary preliminaries.

series X =
Deﬁnition 1 (Multivariate Time Series) We denote a multivariate
{x1, x2, . . . , xT } as a sequence of T observations. The t-th observation xt ∈ RD consists
of D features {x1
t }, and was observed at timestamp st (the time gap between different
timestamps may be not same). In reality, due to unexpected accidents, such as equipment damage

t , . . . , xD

t , x2

time

2

or communication error, xt may have the missing values (e.g., in Fig. 1, x3
represent the missing values in xt, we introduce a masking vector mt where,
(cid:26) 0
1

if xd
otherwise

t is not observed

t =

md

.

1 in x1 is missing). To

In many cases, some features can be missing for consecutive timestamps (e.g., blue blocks in Fig. 1).
We deﬁne δd
t to be the time gap from the last observation to the current timestamp st, i.e.,

δd
t =






st − st−1 + δd
st − st−1
0

t−1

if t > 1, md
if t > 1, md
if t = 1

t−1 = 0
t−1 = 1

.

See Fig. 1 for an illustration.

Figure 1: An example of multivariate time series with missing values. x1 to x6 are observed at
s1...6 = 0, 2, 7, 9, 14, 15 respectively. Considering the 2nd feature in x6, the last observation of the
2nd feature took place at s2 = 2, and we have that δ2

6 = s6 − s2 = 13.

In this paper, we study a general setting for time series classiﬁcation/regression problems with missing
values. We use y to denote the label of corresponding classiﬁcation/regression task. In general, y
can be either a scalar or a vector. Our goal is to predict y based on the given time series X. In the
meantime, we impute the missing values in X as accurate as possible. In another word, we aim to
design an effective multi-task learning algorithm for both classiﬁcation/regression and imputation.

4 BRITS

In this section, we describe the BRITS. Differing from the prior work which uses RNN to impute
missing values in a smooth fashion [10], we learn the missing values directly in a recurrent dynamical
system [25, 28] based on the observed data. The missing values are thus imputed according to the
recurrent dynamics, which signiﬁcantly boosts both the imputation accuracy and the ﬁnal classiﬁca-
tion/regression accuracy. We start the description with the simplest case: the variables observed at
the same time are mutually uncorrelated. For such case, we propose algorithms for imputation with
unidirectional recurrent dynamics and bidirectional recurrent dynamics, respectively. We further
propose an algorithm for correlated multivariate time series in Section 4.3.

4.1 Unidirectional Uncorrelated Recurrent Imputation

For the simplest case, we assume that for the t-th step, xi
be correlated with some xj
dynamics, denoted as RITS-I.

t may
t(cid:48)(cid:54)=t). We ﬁrst propose an imputation algorithm by unidirectional recurrent

t are uncorrelated if i (cid:54)= j (but xi

t and xj

In a unidirectional recurrent dynamical system, each value in the time series can be derived by its
predecessors with a ﬁxed arbitrary function [9, 24, 3]. Thus, we iteratively impute all the variables in
the time series according to the recurrent dynamics. For the t-th step, if xt is actually observed, we
use it to validate our imputation and pass xt to the next recurrent steps. Otherwise, since the future
observations are correlated with the current value, we replace xt with the obtained imputation, and
validate it by the future observations. To be more concrete, let us consider an example.

Example 1 Suppose we are given a time series X = {x1, x2, . . . , x10}, where x5, x6 and x7 are
missing. According to the recurrent dynamics, at each time step t, we can obtain an estimation ˆxt
based on the previous t − 1 steps. In the ﬁrst 4 steps, the estimation error can be obtained immediately

3

Figure 2: Imputation with unidirectional dynamics.

by calculating the estimation loss function Le(ˆxt, xt) for t = 1, . . . , 4. However, when t = 5, 6, 7,
we cannot calculate the error immediately since the true values are missing. Nevertheless, note that
at the 8-th step, ˆx8 depends on ˆx5 to ˆx7. We thus obtain a “delayed" error for ˆxt=5,6,7 at the 8-th
step.

4.1.1 Algorithm

We introduce a recurrent component and a regression component for imputation. The recurrent
component is achieved by a recurrent neural network and the regression component is achieved by a
fully-connected network. A standard recurrent network [17] can be represented as

ht = σ(Whht−1 + Uhxt + bh),
where σ is the sigmoid function, Wh, Uh and bh are parameters, and ht is the hidden state of
previous time steps.

In our case, since xt may have missing values, we cannot use xt as the input directly as in the above
equation. Instead, we use a “complement" input xc
t derived by our algorithm when xt is missing.
Formally, we initialize the initial hidden state h0 as an all-zero vector and then update the model by:
(1)
(2)
(3)
(4)
(5)

ˆxt = Wxht−1 + bx,
xc
t = mt (cid:12) xt + (1 − mt) (cid:12) ˆxt,
γt = exp{− max(0, Wγδt + bγ)},
ht = σ(Wh[ht−1 (cid:12) γt] + Uh[xc
(cid:96)t = (cid:104)mt, Le(xt, ˆxt)(cid:105) .

t ◦ mt] + bh),

Eq. (1) is the regression component which transfers the hidden state ht−1 to the estimated vector
ˆxt. In Eq. (2), we replace missing values in xt with corresponding values in ˆxt, and obtain the
complement vector xc
t . Besides, since the time series may be irregularly sampled, in Eq. (3), we
further introduce a temporal decay factor γt to decay the hidden vector ht−1. Intuitively, if δt is large
(i.e., the values are missing for a long time), we expect a small γt to decay the hidden state. Such
factor also represents the missing patterns in the time series which is critical to imputation [10]. In Eq.
(4), based on the decayed hidden state, we predict the next state ht. Here, ◦ indicates the concatenate
operation. In the mean time, we calculate the estimation error by the estimation loss function Le in
Eq. (5). In our experiment, we use the mean absolute error for Le. Finally, we predict the task label
y as

where fout can be either a fully-connected layer or a softmax layer depending on the speciﬁc task,
and αi is the weight for different hidden state which can be derived by the attention mechanism1 or
the mean pooling mechanism, i.e., αi = 1
T . We calculate the output loss by Lout(y, ˆy). Our model is
then updated by minimizing the accumulated loss 1
T

t=1 (cid:96)t + Lout(y, ˆy).

(cid:80)T

1The design of attention mechanism is out of this paper’s scope.

ˆy = fout(

αihi),

T
(cid:88)

i=1

4

4.1.2 Practical Issues

In practice, we use LSTM as the recurrent component in Eq. (4) to prevent the gradient vanish-
ing/exploding problems in vanilla RNN [17]. Standard RNN models including LSTM treat ˆxt as a
constant. During backpropagation, gradients are cut at ˆxt. This makes the estimation errors backprop-
agate insufﬁciently. For example, in Example 1, the estimation errors of ˆx5 to ˆx7 are obtained at the
8-th step as the delayed errors. However, if we treat ˆx5 to ˆx7 as constants, such delayed error cannot
be fully backpropagated. To overcome such issue, we treat ˆxt as a variable of RNN graph. We let the
estimation error passes through ˆxt during the backpropagation. Fig. 2 shows how RITS-I method
works in Example 1. In this example, the gradients are backpropagated through the opposite direction
of solid lines. Thus, the delayed error (cid:96)8 is passed to steps 5, 6 and 7. In the experiment, we ﬁnd that
our models are unstable during training if we treat ˆxt as a constant. See Appendix C for details.

4.2 Bidirectional Uncorrelated Recurrent Imputation

In the RITS-I, errors of estimated missing values are delayed until the presence of the next observation.
For example, in Example 1, the error of ˆx5 is delayed until x8 is observed. Such error delay makes
the model converge slowly and in turn leads to inefﬁciency in training. In the meantime, it also leads
to the bias exploding problem [6], i.e., the mistakes made early in the sequential prediction are fed as
input to the model and may be quickly ampliﬁed. In this section, we propose an improved version
called BRITS-I. The algorithm alleviates the above-mentioned issues by utilizing the bidirectional
recurrent dynamics on the given time series, i.e., besides the forward direction, each value in time
series can be also derived from the backward direction by another ﬁxed arbitrary function.

To illustrate the intuition of BRITS-I algorithm, again, we consider Example 1. Consider the
backward direction of the time series. In bidirectional recurrent dynamics, the estimation ˆx4 reversely
depends on ˆx5 to ˆx7. Thus, the error in the 5-th step is back-propagated from not only the 8-th step in
the forward direction (which is far from the current position), but also the 4-th step in the backward
direction (which is closer). Formally, the BRITS-I algorithm performs the RITS-I as shown in
Eq. (1) to Eq. (5) in forward and backward directions, respectively. In the forward direction, we
obtain the estimation sequence {ˆx1, ˆx2, . . . , ˆxT } and the loss sequence {(cid:96)1, (cid:96)2, . . . , (cid:96)T }. Similarly,
in the backward direction, we obtain another estimation sequence {ˆx(cid:48)
T } and another
(cid:48)}. We enforce the prediction in each step to be consistent in both
loss sequence {(cid:96)1
directions by introducing the “consistency loss”:

2, . . . , ˆx(cid:48)

(cid:48), . . . , (cid:96)T

1, ˆx(cid:48)

(cid:48), (cid:96)2

(6)
where we use the mean squared error as the discrepancy in our experiment. The ﬁnal estimation loss
is obtained by accumulating the forward loss (cid:96)t, the backward loss (cid:96)(cid:48)
t and the consistency loss (cid:96)cons
.
The ﬁnal estimation in the t-th step is the mean of ˆxt and ˆxt

= Discrepancy(ˆxt, ˆx(cid:48)
t)

(cid:96)cons
t

(cid:48).

t

4.3 Correlated Recurrent Imputation

The previously proposed algorithms RITS-I and BRITS-I assume that features observed at the same
time are mutually uncorrelated. This may be not true in some applications. For example, in the
air quality data [32], each feature represents one measurement in a monitoring station. Obviously,
the observed measurements are spatially correlated. In general, the measurement in one monitoring
station is close to those values observed in its neighboring stations. In this case, we can estimate a
missing measurement according to both its historical data, and the measurements in its neighbors.

In this section, we propose an algorithm, which utilizes the feature correlations in the unidirectional
recurrent dynamical system. We refer to such algorithm as RITS. The feature correlated algorithm
for bidirectional case (i.e., BRITS) can be derived in the same way. Note that in Section 4.1, the
estimation ˆxt is only correlated with the historical observations, but irrelevant with the the current
observation. We refer to ˆxt as a “history-based estimation". In this section, we derive another
“feature-based estimation" for each xd
t , based on the other features at time st. Speciﬁcally, at the t-th
step, we ﬁrst obtain the complement observation xc
t by Eq. (1) and Eq. (2). Then, we deﬁne our
feature-based estimation as ˆzt where

(7)
Wz, bz are corresponding parameters. We restrict the diagonal of parameter matrix Wz to be all
zeros. Thus, the d-th element in ˆzt is exactly the estimation of xd
t , based on the other features. We

t + bz,

ˆzt = Wzxc

5

further combine the historical-based estimation ˆxt and the feature-based estimation ˆzt. We denote
the combined vector as ˆct, and we have that

βt = σ(Wβ[γt ◦ mt] + bβ)
(8)
ˆct = βt (cid:12) ˆzt + (1 − βt) (cid:12) ˆxt.
(9)
Here we use βt ∈ [0, 1]D as the weight of combining the history-based estimation ˆxt and the
feature-based estimation ˆzt. Note that ˆzt is derived from xc
t can be
history-based estimations or truly observed values, depending on the presence of the observations.
Thus, we learn the weight βt by considering both the temporal decay γt and the masking vector mt
as shown in Eq. (8). The rest parts are similar as the feature uncorrelated case. We ﬁrst replace
missing values in xt with the corresponding values in ˆct. The obtained vector is then fed to the next
recurrent step to predict memory ht:

t by Eq. (7). The elements of xc

cc
t = mt (cid:12) xt + (1 − mt) (cid:12) ˆct
ht = σ(Wh[ht−1 (cid:12) γt] + Uh[cc

(10)
(11)
However, the estimation loss is slightly different with the feature uncorrelated case. We ﬁnd that
simply using (cid:96)t = Le(xt, ˆct) leads to a very slow convergence speed. Instead, we accumulate all the
estimation errors of ˆxt, ˆzt and ˆct:

t ◦ mt] + bh).

(cid:96)t = Le(xt, ˆxt) + Le(xt, ˆzt) + Le(xt, ˆct).

Our proposed methods are applicable to a wide variety of applications. We evaluate our methods on
three different real-world datasets. The download links of the datasets, as well as the implementation
codes can be found in the GitHub page2.

5 Experiment

5.1 Dataset Description

5.1.1 Air Quality Data

We evaluate our models on the air quality dataset, which consists of PM2.5 measurements from
36 monitoring stations in Beijing. The measurements are hourly collected from 2014/05/01 to
2015/04/30. Overall, there are 13.3% values are missing. For this dataset, we do pure imputation
task. We use the exactly same train/test setting as in prior work [32], i.e., we use the 3rd, 6th, 9th and
12th months as the test data and the other months as the training data. See Appendix D for details. To
train our model, we randomly select every 36 consecutive steps as one time series.

5.1.2 Health-care Data

We evaluate our models on health-care data in PhysioNet Challenge 2012 [27], which consists of
4000 multivariate clinical time series from intensive care unit (ICU). Each time series contains 35
measurements such as Albumin, heart-rate etc., which are irregularly sampled at the ﬁrst 48 hours
after the patient’s admission to ICU. We stress that this dataset is extremely sparse. There are up to
78% missing values in total. For this dataset, we do both the imputation task and the classiﬁcation
task. To evaluate the imputation performance, we randomly eliminate 10% of observed measurements
from data and use them as the ground-truth. At the same time, we predict the in-hospital death of
each patient as a binary classiﬁcation task. Note that the eliminated measurements are only used for
validating the imputation, and they are never visible to the model.

5.1.3 Localization for Human Activity Data

The UCI localization data for human activity [18] contains records of ﬁve people performing different
activities such as walking, falling, sitting down etc (there are 11 activities). Each person wore four
sensors on her/his left/right ankle, chest, and belt. Each sensor recorded a 3-dimensional coordinates
for about 20 to 40 millisecond. We randomly select 40 consecutive steps as one time series, and
there are 30, 917 time series in total. For this dataset, we do both imputation and classiﬁcation tasks.
Similarly, we randomly eliminate 10% observed data as the imputation ground-truth. We further
predict the corresponding activity of observed time series (i.e., walking, sitting, etc).

2https://github.com/NIPS-BRITS/BRITS

6

5.2 Experiment Setting

5.2.1 Model Implementations

We ﬁx the dimension of hidden state ht in RNN to be 64. We train our model by an Adam optimizer
with learning rate 0.001 and batch size 64. For all the tasks, we normalize the numerical values to
have zero mean and unit variance for stable training.

We use different early stopping strategies for pure imputation task and classiﬁcation tasks. For the
imputation tasks, we randomly select 10% of non-missing values as the validation data. The early
stopping is thus performed based on the validation error. For the classiﬁcation tasks, we ﬁrst pre-train
the model as an imputation task. Then we use 5-fold cross validation to further optimize both the
imputation and classiﬁcation losses simultaneously.

We evaluate the imputation performance in terms of mean absolute error (MAE) and mean relative
error (MRE). Suppose that labeli is the ground-truth of the i-th item, predi is the output of the i-th
item, and there are N items in total. Then, MAE and MRE are deﬁned as

MAE =

(cid:80)

i |predi − labeli|
N

,

(cid:80)

MRE =

i |predi − labeli|
i |labeli|

(cid:80)

.

For air quality data, the evaluation is performed on its original data. For heath-care data and activity
data, since the numerical values are not in the same scale, we evaluate the performances on their
normalized data. To further evaluate the classiﬁcation performances, we use area under ROC curve
(AUC) [8] for health-care data, since such data is highly unbalanced (there are 10% patients who
died in hospital). We then use standard accuracy for the activity data, since different activities are
relatively balanced.

5.2.2 Baseline Methods

We compare our model with both RNN-based methods and non-RNN based methods. The non-RNN
based imputation methods include:

• Mean: We simply replace the missing values with corresponding global mean.
• KNN: We use k-nearest neighbor [13] to ﬁnd the similar samples, and impute the missing

values with weighted average of its neighbors.

• Matrix Factorization (MF): We factorize the data matrix into two low-rank matrices, and

ﬁll the missing values by matrix completion [13].

• MICE: We use Multiple Imputation by Chained Equations (MICE), a widely used impu-
tation method, to ﬁll the missing values. MICE creates multiple imputations with chained
equations [2].

• ImputeTS: We use ImputeTS package in R to impute the missing values. ImputeTS is a
widely used package for missing value imputation, which utilizes the state space model and
kalman smoothing [23].

• STMVL: Speciﬁcally, we use STMVL for the air quality data imputation. STMVL is the
state-of-the-art method for air quality data imputation. It further utilizes the geo-locations
when imputing missing values [32].

We implement KNN, MF and MICE based on the python package fancyimpute3. In recent studies,
RNN-based models in missing value imputations achieve remarkable performances [10, 21, 12, 33].
We also compare our model with existing RNN-based imputation methods, including:

• GRU-D: GRU-D is proposed for handling missing values in health-care data. It imputes
each missing value by the weighted combination of its last observation, and the global mean,
together with a recurrent component [10].

• M-RNN: M-RNN also uses bi-directional RNN. It imputes the missing values according to
hidden states in both directions in RNN. M-RNN treats the imputed values as constants. It
does not consider the correlations among different missing values[33].

3https://github.com/iskandr/fancyimpute

7

Table 1: Performance Comparison for Imputation Tasks (in MAE(MRE%))

Method

Non-RNN

RNN

Ours

Mean
KNN
MF
MICE
ImputeTS
STMVL
GRU-D
M-RNN
RITS-I
BRITS-I
RITS
BRITS

Air Quality
55.51 (77.97%)
29.79 (41.85%)
27.94 (39.25%)
27.42 (38.52%)
19.58 (27.51%)
12.12 (17.40%)
/
14.24 (20.43%)
12.73 (18.32%)
11.58 (16.66%)
12.19 (17.54%)
11.56 (16.65%)

Health-care
0.720 (100.00%)
0.732 (101.66%)
0.622 (87.68%)
0.634 (89.17%)
0.390 (54.2%)
/
0.559 (77.58%)
0.451 (62.65%)
0.395 (54.80%)
0.361 (50.01%)
0.300 (41.89%)
0.281 (39.14%)

Human Activity
0.767 (96.43%)
0.479 (58.54%)
0.879 (110.44%)
0.477 (57.94%)
0.363 (45.65%)
/
0.558 (70.05%)
0.248 (31.19%)
0.240 (30.10%)
0.220 (27.61%)
0.248 (31.21%)
0.219 (27.59%)

We compare the baseline methods with our four models: RITS-I (Section 4.1), RITS (Section 4.2),
BRITS-I (Section 4.3) and BRITS (Section 4.3). We implement all the RNN-based models with
PyTorch, a widely used package for deep learning. All models are trained with GPU GTX 1080.

5.3 Experiment Results

Table 1 shows the imputation results. As we can see, simply applying naıve mean imputation is
very inaccurate. Alternatively, KNN, MF, and MICE perform much better than mean imputation.
ImputeTS achieves the best performance among all the non-RNN methods, especially for the heath-
care data (which is smooth and contains few sudden waves). STMVL performs well on the air
quality data. However, it is speciﬁcally designed for geographical data, and cannot be applied in
the other datasets. Most of RNN-based methods, except GRU-D, demonstrates signiﬁcantly better
performances in imputation tasks. We stress that GRU-D does not impute missing value explicitly. It
actually performs well on classiﬁcation tasks (See Appendix A for details). M-RNN uses explicitly
imputation procedure, and achieves remarkable imputation results. Our model BRITS outperforms
all the baseline models signiﬁcantly. According to the performances of our four models, we also ﬁnd
that both bidirectional recurrent dynamics, and the feature correlations are helpful to enhance the
model performance.

We also compare the accuracies of all RNN-based models in classiﬁcation tasks. GRU-D achieves
an AUC of 0.828 ± 0.004 on health-care data , and accuracy of 0.939 ± 0.010 on human activity;
M-RNN is slightly worse. It achieves 0.800 ± 0.003 and 0.938 ± 0.010 on two tasks. Our model
BRITS outperforms the baseline methods. The accuracies of BRITS are 0.850 ± 0.002 and
0.969 ± 0.008 respectively. See Appendix A for more classiﬁcation results.

Due to the space limitation, we provide more experimental results in Appendix A and B for better
understanding our model.

6 Conclusion

In this paper, we proposed BRITS, a novel method to use recurrent dynamics to effectively impute
the missing values in multivariate time series. Instead of imposing assumptions over the data-
generating process, our model directly learns the missing values in a bidirectional recurrent dynamical
system, without any speciﬁc assumption. Our model treats missing values as variables of the
bidirectional RNN graph. Thus, we get the delayed gradients for missing values in both forward and
backward directions, which makes the imputation of missing values more accurate. We performed the
missing value imputation and classiﬁcation/regression simultaneously within a joint neural network.
Experiment results show that our model demonstrates more accurate results for both imputation and
classiﬁcation/regression than state-of-the-art methods.

8

References

[1] C. F. Ansley and R. Kohn. On the estimation of arima models with missing values. In Time

series analysis of irregularly observed data, pages 9–37. Springer, 1984.

[2] M. J. Azur, E. A. Stuart, C. Frangakis, and P. J. Leaf. Multiple imputation by chained equations:
what is it and how does it work? International journal of methods in psychiatric research,
20(1):40–49, 2011.

[3] A. Basharat and M. Shah. Time series prediction by chaotic modeling of nonlinear dynamical
systems. In Computer Vision, 2009 IEEE 12th International Conference on, pages 1941–1948.
IEEE, 2009.

[4] B. Batres-Estrada. Deep learning for multivariate ﬁnancial time series, 2015.

[5] S. Bauer, B. Schölkopf, and J. Peters. The arrow of time in multivariate time series.

In

International Conference on Machine Learning, pages 2043–2051, 2016.

[6] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction
with recurrent neural networks. In Advances in Neural Information Processing Systems, pages
1171–1179, 2015.

[7] M. Berglund, T. Raiko, M. Honkala, L. Kärkkäinen, A. Vetek, and J. T. Karhunen. Bidirectional
recurrent neural networks as generative models. In Advances in Neural Information Processing
Systems, pages 856–864, 2015.

[8] A. P. Bradley. The use of the area under the roc curve in the evaluation of machine learning

algorithms. Pattern recognition, 30(7):1145–1159, 1997.

[9] P. Brakel, D. Stroobandt, and B. Schrauwen. Training energy-based models for time-series

imputation. The Journal of Machine Learning Research, 14(1):2771–2797, 2013.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for

multivariate time series with missing values. Scientiﬁc reports, 8(1):6085, 2018.

[11] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and
Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078, 2014.

[12] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun. Doctor ai: Predicting clinical
events via recurrent neural networks. In Machine Learning for Healthcare Conference, pages
301–318, 2016.

[13] J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning, volume 1.

Springer series in statistics Springer, Berlin, 2001.

[14] D. S. Fung. Methods for the estimation of missing values in time series. 2006.
[15] A. C. Harvey. Forecasting, structural time series models and the Kalman ﬁlter. Cambridge

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735,

university press, 1990.

1997.

[17] G. Ian, B. Yoshua, and C. Aaron. Deep learning. Book in preparation for MIT Press, 2016.

[18] B. Kaluža, V. Mirchevska, E. Dovgan, M. Luštrek, and M. Gams. An agent-based approach
to care in independent living. In International joint conference on ambient intelligence, pages
177–186. Springer, 2010.

[19] D. M. Kreindler and C. J. Lumsden. The effects of the irregular sample and missing data in
time series analysis. Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using
Real Data, page 135, 2012.

[20] L. Li, J. McCann, N. S. Pollard, and C. Faloutsos. Dynammo: Mining and summarization
In Proceedings of the 15th ACM SIGKDD
of coevolving sequences with missing values.
international conference on Knowledge discovery and data mining, pages 507–516. ACM,
2009.

[21] Z. C. Lipton, D. Kale, and R. Wetzel. Directly modeling missing data in sequences with rnns:
Improved classiﬁcation of clinical time series. In Machine Learning for Healthcare Conference,
pages 253–270, 2016.

9

[22] Z. Liu and M. Hauskrecht. Learning linear dynamical systems from multivariate time series:
In Proceedings of the 2016 SIAM International

A matrix factorization based framework.
Conference on Data Mining, pages 810–818. SIAM, 2016.

[23] S. Moritz and T. Bartz-Beielstein. imputeTS: Time Series Missing Value Imputation in R. The

[24] T. Ozaki. 2 non-linear time series models and dynamical systems. Handbook of statistics,

R Journal, 9(1):207–218, 2017.

5:25–83, 1985.

[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks.

In International Conference on Machine Learning, pages 1310–1318, 2013.

[26] S. Rani and G. Sikka. Recent techniques of clustering of time series data: a survey. International

Journal of Computer Applications, 52(15), 2012.

[27] I. Silva, G. Moody, D. J. Scott, L. A. Celi, and R. G. Mark. Predicting in-hospital mortality
of icu patients: The physionet/computing in cardiology challenge 2012. In Computing in
Cardiology (CinC), 2012, pages 245–248. IEEE, 2012.
[28] D. Sussillo and O. Barak. Opening the black box:

low-dimensional dynamics in high-

dimensional recurrent neural networks. Neural computation, 25(3):626–649, 2013.

[29] D. Wang, W. Cao, J. Li, and J. Ye. Deepsd: supply-demand prediction for online car-hailing ser-
vices using deep neural networks. In Data Engineering (ICDE), 2017 IEEE 33rd International
Conference on, pages 243–254. IEEE, 2017.

[30] J. Wang, A. P. De Vries, and M. J. Reinders. Unifying user-based and item-based collaborative
ﬁltering approaches by similarity fusion. In Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in information retrieval, pages 501–508. ACM,
2006.

[31] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo. Convolutional lstm
network: A machine learning approach for precipitation nowcasting. In Advances in neural
information processing systems, pages 802–810, 2015.

[32] X. Yi, Y. Zheng, J. Zhang, and T. Li. St-mvl: ﬁlling missing values in geo-sensory time series

data. 2016.

[33] J. Yoon, W. R. Zame, and M. van der Schaar. Multi-directional recurrent neural networks: A

novel method for estimating missing data. 2017.

[34] H.-F. Yu, N. Rao, and I. S. Dhillon. Temporal regularized matrix factorization for high-
dimensional time series prediction. In Advances in neural information processing systems,
pages 847–855, 2016.

[35] J. Zhang, Y. Zheng, and D. Qi. Deep spatio-temporal residual networks for citywide crowd

ﬂows prediction. AAAI, November 2017.

10

A Performance Comparison for Classiﬁcation Tasks

Table 2 shows the performances of different RNN-based models on the classiﬁcation tasks. As we can
see, our model BRITS outperforms other baseline methods for classiﬁcations. Comparing with Table
1, GRU-D performs well for classiﬁcation tasks. Furthermore, we ﬁnd that it is very important for
GRU-D to carefully apply the dropout techniques in order to prevent the overﬁtting (we use p = 0.25
dropout layer on the top classiﬁcation layer). However, our models further utilize the imputation
errors as the supervised signals. It seems that dropout is not necessary for our models during training.

Table 2: Performance Comparison for Classiﬁcation Tasks

Method Health-care (AUC) Human Activity (Accuracy)
GRU-D
M-RNN

0.939 ± 0.010
0.938 ± 0.010

0.828 ± 0.004
0.800 ± 0.003

RITS-I
BRITS-I
RITS
BRITS

0.821 ± 0.007
0.831 ± 0.003
0.840 ± 0.004
0.850 ± 0.002

0.934 ± 0.008
0.940 ± 0.012
0.968 ± 0.010
0.969 ± 0.008

B Performance Comparison for Univariate Synthetic Data

To better understand our model, we generate a set of univariate synthetic time series. Speﬁcically, we
randomly generate a time series with length T = 36, using the state-space representation [15]:
xt = µt + θt + (cid:15)t,
µt = µt−1 + λt−1 + ξt,
λt = λt−1 + ζt,
s−1
(cid:88)

θt =

−θt−j + ωt,

j=1

where xt is the t-th value in time series. The residual terms (cid:15)t, ξt, ζt and ωt are randomly drawn
from a normal distribution N(0, 0.3). We eliminate about 22% values from the generated series, and
compare our model BRITS-I (note the data is univariate) and ImputeTS. We show three examples in
Fig. 3.

Figure 3: Example for synthetic time series imputation. Each column corresponds to one time series.
The ﬁgures in the ﬁrst row are imputations of ImputeTS algorithm, and the ﬁgures in the second row
are imputations by BRITS-I.

The ﬁrst row corresponds to the imputations of ImputeTS and the second row corresponds to our model
BRITS-I. As we can see, our model demonstrates better performance than ImputeTS. Especially,
ImputeTS fails when the start part of time series is missing. However, for our model, the imputation
errors are backpropagated to previous time steps. Thus, our model can adjust the imputations in the
start part with delayed gradient, which leads to much more accurate results.

11

Figure 4: Validation errors for BRITS and BRITS-cut during training

C Performance for Non-differentiable ˆxt

As we claimed in Section 4.1, we regard the missing values as variables of RNN graph. During the
backpropagation, the imputed values can be further updated sufﬁciently. In the experiment, we ﬁnd
that if we cut the gradient of ˆxt (i.e., regard it as constant), the models are unstable and easy to overﬁt
during the training. We refer to the model with non-differentiate ˆxt as BRITS-cut. Fig. 4 shows the
validation errors of BRITS and BRITS-cut during training for the health-care data imputation. At the
ﬁrst 20 iterations, the validation error of BRITS-cut decreases fast. However, it soon fails due to the
overﬁtting.

D Test Data of Air Quality Imputation

We use the same method as in prior work [32] to select the test data for air quality imputation.
Speciﬁcally, we use the 3rd, 6th, 9th and 12th months as the test set and the rest data as the training
set. To evaluate our model, we select the test timeslots by the following rule: if a measurement
is observed at a timeslot in one of these four months (e.g., 8 o’clock 2015/03/07), we check the
corresponding position in its previous month (e.g., 8 o’clock 2015/02/07). If it is absent in the
previous month, we eliminate such value and use it as the imputation ground-truth of test data. Recall
that to train our model, we randomly select consecutive 36 time steps as one time series. Thus, a test
timeslot may be contained in multiple time series. We use the mean of the imputations in different
time series as the ﬁnal result.

12

8
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
1
v
2
7
5
0
1
.
5
0
8
1
:
v
i
X
r
a

BRITS: Bidirectional Recurrent Imputation for Time
Series

Wei Cao1,3

Dong Wang2

Jian Li1

Hao Zhou3

Lei Li3

Yitan Li3

1Tsinghua University

2Duke University

3 Toutiao AILab

Abstract

Time series are widely used as signals in many classiﬁcation/regression tasks.
It is ubiquitous that time series contains many missing values. Given multiple
correlated time series data, how to ﬁll in missing values and to predict their class
labels? Existing imputation methods often impose strong assumptions of the
underlying data generating process, such as linear dynamics in the state space. In
this paper, we propose BRITS, a novel method based on recurrent neural networks
for missing value imputation in time series data. Our proposed method directly
learns the missing values in a bidirectional recurrent dynamical system, without
any speciﬁc assumption. The imputed values are treated as variables of RNN
graph and can be effectively updated during the backpropagation. BRITS has three
advantages: (a) it can handle multiple correlated missing values in time series; (b)
it generalizes to time series with nonlinear dynamics underlying; (c) it provides
a data-driven imputation procedure and applies to general settings with missing
data. We evaluate our model on three real-world datasets, including an air quality
dataset, a health-care data, and a localization data for human activity. Experiments
show that our model outperforms the state-of-the-art methods in both imputation
and classiﬁcation/regression accuracies.

1

Introduction

Multivariate time series data are abundant in many application areas, such as ﬁnancial marketing
[5, 4], health-care [10, 22], meteorology [31, 26], and trafﬁc engineering [29, 35]. Time series
are widely used as signals for classiﬁcation/regression in various applications of corresponding
areas. However, missing values in time series are very common, due to unexpected accidents, such
as equipment damage or communication error, and may signiﬁcantly harm the performance of
downstream applications.

Much prior work proposed to ﬁx the missing data problem with statistics and machine learning
approaches. However most of them require fairly strong assumptions on missing values. We can ﬁll
the missing values using classical statistical time series models such as ARMA or ARIMA (e.g., [1]).
But these models are essentially linear (after differencing). Kreindler et al. [19] assume that the data
are smoothable, i.e., there is no sudden wave in the periods of missing values, hence imputing missing
values can be done by smoothing over nearby values. Matrix completion has also been used to
address missing value problems (e.g., [30, 34]). But it typically applies to only static data and requires
strong assumptions such as low-rankness. We can also predict missing values by ﬁtting a parametric
data-generating model with the observations [14, 2], which assumes that data of time series follow the
distribution of hypothetical models. These assumptions make corresponding imputation algorithms
less general, and the performance less desirable when the assumptions do not hold.

In this paper, we propose BRITS, a novel method for ﬁlling the missing values for multiple correlated
time series. Internally, BRITS adapts recurrent neural networks (RNN) [16, 11] for imputing missing

Preprint. Work in progress.

values, without any speciﬁc assumption over the data. Much prior work uses non-linear dynamical
systems for time series prediction [9, 24, 3]. In our method, we instantiate the dynamical system as a
bidirectional RNN, i.e., imputing missing values with bidirectional recurrent dynamics. In particular,
we make the following technical contributions:

• We design a bidirectional RNN model for imputing missing values. We directly use RNN for
predicting missing values, instead of tuning weights for smoothing as in [10]. Our method
does not impose speciﬁc assumption, hence works more generally than previous methods.

• We regard missing values as variables of the bidirectional RNN graph, which are involved
in the backpropagation process. In such case, missing values get delayed gradients in both
forward and backward directions with consistency constraints, which makes the estimation
of missing values more accurate.

• We simultaneously perform missing value imputation and classiﬁcation/regression of appli-
cations jointly in one neural graph. This alleviates the error propagation problem from impu-
tation to classiﬁcation/regression. Additionally, the supervision of classiﬁcation/regression
makes the estimation of missing values more accurate.

• We evaluate our model on three real-world datasets, including an air quality dataset, a
health-care dataset and a localization dataset of human activities. Experimental results show
that our model outperforms the state-of-the-art models for both imputation and classiﬁca-
tion/regression accuracies.

2 Related Work

There is a large body of literature on the imputation of missing values in time series. We only mention
a few closely related ones. The interpolate method tries to ﬁt a "smooth curve" to the observations
and thus reconstruct the missing values by the local interpolations [19, 14]. Such method discards
any relationships between the variables over time. The autoregressive method, including ARIMA,
SARIMA etc., eliminates the non-stationary parts in the time series data and ﬁt a parameterized
stationary model. The state space model further combines ARIMA and Kalman Filter [14, 15], which
provides more accurate results. Multivariate Imputation by Chained Equations (MICE) [2] ﬁrst
initializes the missing values arbitrarily and then estimates each missing variable based on the chain
equations. The graphical model [20] introduces a latent variable for each missing value, and ﬁnds
the latent variables by learning their transition matrix. There are also some data-driven methods
for missing value imputation. Yi et al. [32] imputed the missing values in air quality data with
geographical features. Wang et al. [30] imputed missing values in recommendation system with
collaborative ﬁltering. Yu et al. [34] utilized matrix factorization with temporal regularization to
impute the missing values in regularly sampled time series data.

Recently, some researchers attempted to impute the missing values with recurrent neural networks
[7, 10, 21, 12, 33]. The recurrent components are trained together with the classiﬁcation/regression
component, which signiﬁcantly boosts the accuracy. Che et al. [10] proposed GRU-D, which imputes
missing values in health-care data with a smooth fashion. It assumes that a missing variable can be
represented as the combination of its corresponding last observed value and the global mean. GRU-D
achieves remarkable performance on health-care data. However, it has many limitations on general
datasets [10]. A closely related work is M-RNN proposed by Yoon et al. [33]. M-RNN also utilizes
bi-directional RNN to impute missing values. Differing from our model, it drops the relationships
between missing variables. The imputed values in M-RNN are treated as constants and cannot be
sufﬁciently updated.

3 Preliminary

We ﬁrst present the problem formulation and some necessary preliminaries.

series X =
Deﬁnition 1 (Multivariate Time Series) We denote a multivariate
{x1, x2, . . . , xT } as a sequence of T observations. The t-th observation xt ∈ RD consists
of D features {x1
t }, and was observed at timestamp st (the time gap between different
timestamps may be not same). In reality, due to unexpected accidents, such as equipment damage

t , . . . , xD

t , x2

time

2

or communication error, xt may have the missing values (e.g., in Fig. 1, x3
represent the missing values in xt, we introduce a masking vector mt where,
(cid:26) 0
1

if xd
otherwise

t is not observed

t =

md

.

1 in x1 is missing). To

In many cases, some features can be missing for consecutive timestamps (e.g., blue blocks in Fig. 1).
We deﬁne δd
t to be the time gap from the last observation to the current timestamp st, i.e.,

δd
t =






st − st−1 + δd
st − st−1
0

t−1

if t > 1, md
if t > 1, md
if t = 1

t−1 = 0
t−1 = 1

.

See Fig. 1 for an illustration.

Figure 1: An example of multivariate time series with missing values. x1 to x6 are observed at
s1...6 = 0, 2, 7, 9, 14, 15 respectively. Considering the 2nd feature in x6, the last observation of the
2nd feature took place at s2 = 2, and we have that δ2

6 = s6 − s2 = 13.

In this paper, we study a general setting for time series classiﬁcation/regression problems with missing
values. We use y to denote the label of corresponding classiﬁcation/regression task. In general, y
can be either a scalar or a vector. Our goal is to predict y based on the given time series X. In the
meantime, we impute the missing values in X as accurate as possible. In another word, we aim to
design an effective multi-task learning algorithm for both classiﬁcation/regression and imputation.

4 BRITS

In this section, we describe the BRITS. Differing from the prior work which uses RNN to impute
missing values in a smooth fashion [10], we learn the missing values directly in a recurrent dynamical
system [25, 28] based on the observed data. The missing values are thus imputed according to the
recurrent dynamics, which signiﬁcantly boosts both the imputation accuracy and the ﬁnal classiﬁca-
tion/regression accuracy. We start the description with the simplest case: the variables observed at
the same time are mutually uncorrelated. For such case, we propose algorithms for imputation with
unidirectional recurrent dynamics and bidirectional recurrent dynamics, respectively. We further
propose an algorithm for correlated multivariate time series in Section 4.3.

4.1 Unidirectional Uncorrelated Recurrent Imputation

For the simplest case, we assume that for the t-th step, xi
be correlated with some xj
dynamics, denoted as RITS-I.

t may
t(cid:48)(cid:54)=t). We ﬁrst propose an imputation algorithm by unidirectional recurrent

t are uncorrelated if i (cid:54)= j (but xi

t and xj

In a unidirectional recurrent dynamical system, each value in the time series can be derived by its
predecessors with a ﬁxed arbitrary function [9, 24, 3]. Thus, we iteratively impute all the variables in
the time series according to the recurrent dynamics. For the t-th step, if xt is actually observed, we
use it to validate our imputation and pass xt to the next recurrent steps. Otherwise, since the future
observations are correlated with the current value, we replace xt with the obtained imputation, and
validate it by the future observations. To be more concrete, let us consider an example.

Example 1 Suppose we are given a time series X = {x1, x2, . . . , x10}, where x5, x6 and x7 are
missing. According to the recurrent dynamics, at each time step t, we can obtain an estimation ˆxt
based on the previous t − 1 steps. In the ﬁrst 4 steps, the estimation error can be obtained immediately

3

Figure 2: Imputation with unidirectional dynamics.

by calculating the estimation loss function Le(ˆxt, xt) for t = 1, . . . , 4. However, when t = 5, 6, 7,
we cannot calculate the error immediately since the true values are missing. Nevertheless, note that
at the 8-th step, ˆx8 depends on ˆx5 to ˆx7. We thus obtain a “delayed" error for ˆxt=5,6,7 at the 8-th
step.

4.1.1 Algorithm

We introduce a recurrent component and a regression component for imputation. The recurrent
component is achieved by a recurrent neural network and the regression component is achieved by a
fully-connected network. A standard recurrent network [17] can be represented as

ht = σ(Whht−1 + Uhxt + bh),
where σ is the sigmoid function, Wh, Uh and bh are parameters, and ht is the hidden state of
previous time steps.

In our case, since xt may have missing values, we cannot use xt as the input directly as in the above
equation. Instead, we use a “complement" input xc
t derived by our algorithm when xt is missing.
Formally, we initialize the initial hidden state h0 as an all-zero vector and then update the model by:
(1)
(2)
(3)
(4)
(5)

ˆxt = Wxht−1 + bx,
xc
t = mt (cid:12) xt + (1 − mt) (cid:12) ˆxt,
γt = exp{− max(0, Wγδt + bγ)},
ht = σ(Wh[ht−1 (cid:12) γt] + Uh[xc
(cid:96)t = (cid:104)mt, Le(xt, ˆxt)(cid:105) .

t ◦ mt] + bh),

Eq. (1) is the regression component which transfers the hidden state ht−1 to the estimated vector
ˆxt. In Eq. (2), we replace missing values in xt with corresponding values in ˆxt, and obtain the
complement vector xc
t . Besides, since the time series may be irregularly sampled, in Eq. (3), we
further introduce a temporal decay factor γt to decay the hidden vector ht−1. Intuitively, if δt is large
(i.e., the values are missing for a long time), we expect a small γt to decay the hidden state. Such
factor also represents the missing patterns in the time series which is critical to imputation [10]. In Eq.
(4), based on the decayed hidden state, we predict the next state ht. Here, ◦ indicates the concatenate
operation. In the mean time, we calculate the estimation error by the estimation loss function Le in
Eq. (5). In our experiment, we use the mean absolute error for Le. Finally, we predict the task label
y as

where fout can be either a fully-connected layer or a softmax layer depending on the speciﬁc task,
and αi is the weight for different hidden state which can be derived by the attention mechanism1 or
the mean pooling mechanism, i.e., αi = 1
T . We calculate the output loss by Lout(y, ˆy). Our model is
then updated by minimizing the accumulated loss 1
T

t=1 (cid:96)t + Lout(y, ˆy).

(cid:80)T

1The design of attention mechanism is out of this paper’s scope.

ˆy = fout(

αihi),

T
(cid:88)

i=1

4

4.1.2 Practical Issues

In practice, we use LSTM as the recurrent component in Eq. (4) to prevent the gradient vanish-
ing/exploding problems in vanilla RNN [17]. Standard RNN models including LSTM treat ˆxt as a
constant. During backpropagation, gradients are cut at ˆxt. This makes the estimation errors backprop-
agate insufﬁciently. For example, in Example 1, the estimation errors of ˆx5 to ˆx7 are obtained at the
8-th step as the delayed errors. However, if we treat ˆx5 to ˆx7 as constants, such delayed error cannot
be fully backpropagated. To overcome such issue, we treat ˆxt as a variable of RNN graph. We let the
estimation error passes through ˆxt during the backpropagation. Fig. 2 shows how RITS-I method
works in Example 1. In this example, the gradients are backpropagated through the opposite direction
of solid lines. Thus, the delayed error (cid:96)8 is passed to steps 5, 6 and 7. In the experiment, we ﬁnd that
our models are unstable during training if we treat ˆxt as a constant. See Appendix C for details.

4.2 Bidirectional Uncorrelated Recurrent Imputation

In the RITS-I, errors of estimated missing values are delayed until the presence of the next observation.
For example, in Example 1, the error of ˆx5 is delayed until x8 is observed. Such error delay makes
the model converge slowly and in turn leads to inefﬁciency in training. In the meantime, it also leads
to the bias exploding problem [6], i.e., the mistakes made early in the sequential prediction are fed as
input to the model and may be quickly ampliﬁed. In this section, we propose an improved version
called BRITS-I. The algorithm alleviates the above-mentioned issues by utilizing the bidirectional
recurrent dynamics on the given time series, i.e., besides the forward direction, each value in time
series can be also derived from the backward direction by another ﬁxed arbitrary function.

To illustrate the intuition of BRITS-I algorithm, again, we consider Example 1. Consider the
backward direction of the time series. In bidirectional recurrent dynamics, the estimation ˆx4 reversely
depends on ˆx5 to ˆx7. Thus, the error in the 5-th step is back-propagated from not only the 8-th step in
the forward direction (which is far from the current position), but also the 4-th step in the backward
direction (which is closer). Formally, the BRITS-I algorithm performs the RITS-I as shown in
Eq. (1) to Eq. (5) in forward and backward directions, respectively. In the forward direction, we
obtain the estimation sequence {ˆx1, ˆx2, . . . , ˆxT } and the loss sequence {(cid:96)1, (cid:96)2, . . . , (cid:96)T }. Similarly,
in the backward direction, we obtain another estimation sequence {ˆx(cid:48)
T } and another
(cid:48)}. We enforce the prediction in each step to be consistent in both
loss sequence {(cid:96)1
directions by introducing the “consistency loss”:

2, . . . , ˆx(cid:48)

(cid:48), . . . , (cid:96)T

1, ˆx(cid:48)

(cid:48), (cid:96)2

(6)
where we use the mean squared error as the discrepancy in our experiment. The ﬁnal estimation loss
is obtained by accumulating the forward loss (cid:96)t, the backward loss (cid:96)(cid:48)
t and the consistency loss (cid:96)cons
.
The ﬁnal estimation in the t-th step is the mean of ˆxt and ˆxt

= Discrepancy(ˆxt, ˆx(cid:48)
t)

(cid:96)cons
t

(cid:48).

t

4.3 Correlated Recurrent Imputation

The previously proposed algorithms RITS-I and BRITS-I assume that features observed at the same
time are mutually uncorrelated. This may be not true in some applications. For example, in the
air quality data [32], each feature represents one measurement in a monitoring station. Obviously,
the observed measurements are spatially correlated. In general, the measurement in one monitoring
station is close to those values observed in its neighboring stations. In this case, we can estimate a
missing measurement according to both its historical data, and the measurements in its neighbors.

In this section, we propose an algorithm, which utilizes the feature correlations in the unidirectional
recurrent dynamical system. We refer to such algorithm as RITS. The feature correlated algorithm
for bidirectional case (i.e., BRITS) can be derived in the same way. Note that in Section 4.1, the
estimation ˆxt is only correlated with the historical observations, but irrelevant with the the current
observation. We refer to ˆxt as a “history-based estimation". In this section, we derive another
“feature-based estimation" for each xd
t , based on the other features at time st. Speciﬁcally, at the t-th
step, we ﬁrst obtain the complement observation xc
t by Eq. (1) and Eq. (2). Then, we deﬁne our
feature-based estimation as ˆzt where

(7)
Wz, bz are corresponding parameters. We restrict the diagonal of parameter matrix Wz to be all
zeros. Thus, the d-th element in ˆzt is exactly the estimation of xd
t , based on the other features. We

t + bz,

ˆzt = Wzxc

5

further combine the historical-based estimation ˆxt and the feature-based estimation ˆzt. We denote
the combined vector as ˆct, and we have that

βt = σ(Wβ[γt ◦ mt] + bβ)
(8)
ˆct = βt (cid:12) ˆzt + (1 − βt) (cid:12) ˆxt.
(9)
Here we use βt ∈ [0, 1]D as the weight of combining the history-based estimation ˆxt and the
feature-based estimation ˆzt. Note that ˆzt is derived from xc
t can be
history-based estimations or truly observed values, depending on the presence of the observations.
Thus, we learn the weight βt by considering both the temporal decay γt and the masking vector mt
as shown in Eq. (8). The rest parts are similar as the feature uncorrelated case. We ﬁrst replace
missing values in xt with the corresponding values in ˆct. The obtained vector is then fed to the next
recurrent step to predict memory ht:

t by Eq. (7). The elements of xc

cc
t = mt (cid:12) xt + (1 − mt) (cid:12) ˆct
ht = σ(Wh[ht−1 (cid:12) γt] + Uh[cc

(10)
(11)
However, the estimation loss is slightly different with the feature uncorrelated case. We ﬁnd that
simply using (cid:96)t = Le(xt, ˆct) leads to a very slow convergence speed. Instead, we accumulate all the
estimation errors of ˆxt, ˆzt and ˆct:

t ◦ mt] + bh).

(cid:96)t = Le(xt, ˆxt) + Le(xt, ˆzt) + Le(xt, ˆct).

Our proposed methods are applicable to a wide variety of applications. We evaluate our methods on
three different real-world datasets. The download links of the datasets, as well as the implementation
codes can be found in the GitHub page2.

5 Experiment

5.1 Dataset Description

5.1.1 Air Quality Data

We evaluate our models on the air quality dataset, which consists of PM2.5 measurements from
36 monitoring stations in Beijing. The measurements are hourly collected from 2014/05/01 to
2015/04/30. Overall, there are 13.3% values are missing. For this dataset, we do pure imputation
task. We use the exactly same train/test setting as in prior work [32], i.e., we use the 3rd, 6th, 9th and
12th months as the test data and the other months as the training data. See Appendix D for details. To
train our model, we randomly select every 36 consecutive steps as one time series.

5.1.2 Health-care Data

We evaluate our models on health-care data in PhysioNet Challenge 2012 [27], which consists of
4000 multivariate clinical time series from intensive care unit (ICU). Each time series contains 35
measurements such as Albumin, heart-rate etc., which are irregularly sampled at the ﬁrst 48 hours
after the patient’s admission to ICU. We stress that this dataset is extremely sparse. There are up to
78% missing values in total. For this dataset, we do both the imputation task and the classiﬁcation
task. To evaluate the imputation performance, we randomly eliminate 10% of observed measurements
from data and use them as the ground-truth. At the same time, we predict the in-hospital death of
each patient as a binary classiﬁcation task. Note that the eliminated measurements are only used for
validating the imputation, and they are never visible to the model.

5.1.3 Localization for Human Activity Data

The UCI localization data for human activity [18] contains records of ﬁve people performing different
activities such as walking, falling, sitting down etc (there are 11 activities). Each person wore four
sensors on her/his left/right ankle, chest, and belt. Each sensor recorded a 3-dimensional coordinates
for about 20 to 40 millisecond. We randomly select 40 consecutive steps as one time series, and
there are 30, 917 time series in total. For this dataset, we do both imputation and classiﬁcation tasks.
Similarly, we randomly eliminate 10% observed data as the imputation ground-truth. We further
predict the corresponding activity of observed time series (i.e., walking, sitting, etc).

2https://github.com/NIPS-BRITS/BRITS

6

5.2 Experiment Setting

5.2.1 Model Implementations

We ﬁx the dimension of hidden state ht in RNN to be 64. We train our model by an Adam optimizer
with learning rate 0.001 and batch size 64. For all the tasks, we normalize the numerical values to
have zero mean and unit variance for stable training.

We use different early stopping strategies for pure imputation task and classiﬁcation tasks. For the
imputation tasks, we randomly select 10% of non-missing values as the validation data. The early
stopping is thus performed based on the validation error. For the classiﬁcation tasks, we ﬁrst pre-train
the model as an imputation task. Then we use 5-fold cross validation to further optimize both the
imputation and classiﬁcation losses simultaneously.

We evaluate the imputation performance in terms of mean absolute error (MAE) and mean relative
error (MRE). Suppose that labeli is the ground-truth of the i-th item, predi is the output of the i-th
item, and there are N items in total. Then, MAE and MRE are deﬁned as

MAE =

(cid:80)

i |predi − labeli|
N

,

(cid:80)

MRE =

i |predi − labeli|
i |labeli|

(cid:80)

.

For air quality data, the evaluation is performed on its original data. For heath-care data and activity
data, since the numerical values are not in the same scale, we evaluate the performances on their
normalized data. To further evaluate the classiﬁcation performances, we use area under ROC curve
(AUC) [8] for health-care data, since such data is highly unbalanced (there are 10% patients who
died in hospital). We then use standard accuracy for the activity data, since different activities are
relatively balanced.

5.2.2 Baseline Methods

We compare our model with both RNN-based methods and non-RNN based methods. The non-RNN
based imputation methods include:

• Mean: We simply replace the missing values with corresponding global mean.
• KNN: We use k-nearest neighbor [13] to ﬁnd the similar samples, and impute the missing

values with weighted average of its neighbors.

• Matrix Factorization (MF): We factorize the data matrix into two low-rank matrices, and

ﬁll the missing values by matrix completion [13].

• MICE: We use Multiple Imputation by Chained Equations (MICE), a widely used impu-
tation method, to ﬁll the missing values. MICE creates multiple imputations with chained
equations [2].

• ImputeTS: We use ImputeTS package in R to impute the missing values. ImputeTS is a
widely used package for missing value imputation, which utilizes the state space model and
kalman smoothing [23].

• STMVL: Speciﬁcally, we use STMVL for the air quality data imputation. STMVL is the
state-of-the-art method for air quality data imputation. It further utilizes the geo-locations
when imputing missing values [32].

We implement KNN, MF and MICE based on the python package fancyimpute3. In recent studies,
RNN-based models in missing value imputations achieve remarkable performances [10, 21, 12, 33].
We also compare our model with existing RNN-based imputation methods, including:

• GRU-D: GRU-D is proposed for handling missing values in health-care data. It imputes
each missing value by the weighted combination of its last observation, and the global mean,
together with a recurrent component [10].

• M-RNN: M-RNN also uses bi-directional RNN. It imputes the missing values according to
hidden states in both directions in RNN. M-RNN treats the imputed values as constants. It
does not consider the correlations among different missing values[33].

3https://github.com/iskandr/fancyimpute

7

Table 1: Performance Comparison for Imputation Tasks (in MAE(MRE%))

Method

Non-RNN

RNN

Ours

Mean
KNN
MF
MICE
ImputeTS
STMVL
GRU-D
M-RNN
RITS-I
BRITS-I
RITS
BRITS

Air Quality
55.51 (77.97%)
29.79 (41.85%)
27.94 (39.25%)
27.42 (38.52%)
19.58 (27.51%)
12.12 (17.40%)
/
14.24 (20.43%)
12.73 (18.32%)
11.58 (16.66%)
12.19 (17.54%)
11.56 (16.65%)

Health-care
0.720 (100.00%)
0.732 (101.66%)
0.622 (87.68%)
0.634 (89.17%)
0.390 (54.2%)
/
0.559 (77.58%)
0.451 (62.65%)
0.395 (54.80%)
0.361 (50.01%)
0.300 (41.89%)
0.281 (39.14%)

Human Activity
0.767 (96.43%)
0.479 (58.54%)
0.879 (110.44%)
0.477 (57.94%)
0.363 (45.65%)
/
0.558 (70.05%)
0.248 (31.19%)
0.240 (30.10%)
0.220 (27.61%)
0.248 (31.21%)
0.219 (27.59%)

We compare the baseline methods with our four models: RITS-I (Section 4.1), RITS (Section 4.2),
BRITS-I (Section 4.3) and BRITS (Section 4.3). We implement all the RNN-based models with
PyTorch, a widely used package for deep learning. All models are trained with GPU GTX 1080.

5.3 Experiment Results

Table 1 shows the imputation results. As we can see, simply applying naıve mean imputation is
very inaccurate. Alternatively, KNN, MF, and MICE perform much better than mean imputation.
ImputeTS achieves the best performance among all the non-RNN methods, especially for the heath-
care data (which is smooth and contains few sudden waves). STMVL performs well on the air
quality data. However, it is speciﬁcally designed for geographical data, and cannot be applied in
the other datasets. Most of RNN-based methods, except GRU-D, demonstrates signiﬁcantly better
performances in imputation tasks. We stress that GRU-D does not impute missing value explicitly. It
actually performs well on classiﬁcation tasks (See Appendix A for details). M-RNN uses explicitly
imputation procedure, and achieves remarkable imputation results. Our model BRITS outperforms
all the baseline models signiﬁcantly. According to the performances of our four models, we also ﬁnd
that both bidirectional recurrent dynamics, and the feature correlations are helpful to enhance the
model performance.

We also compare the accuracies of all RNN-based models in classiﬁcation tasks. GRU-D achieves
an AUC of 0.828 ± 0.004 on health-care data , and accuracy of 0.939 ± 0.010 on human activity;
M-RNN is slightly worse. It achieves 0.800 ± 0.003 and 0.938 ± 0.010 on two tasks. Our model
BRITS outperforms the baseline methods. The accuracies of BRITS are 0.850 ± 0.002 and
0.969 ± 0.008 respectively. See Appendix A for more classiﬁcation results.

Due to the space limitation, we provide more experimental results in Appendix A and B for better
understanding our model.

6 Conclusion

In this paper, we proposed BRITS, a novel method to use recurrent dynamics to effectively impute
the missing values in multivariate time series. Instead of imposing assumptions over the data-
generating process, our model directly learns the missing values in a bidirectional recurrent dynamical
system, without any speciﬁc assumption. Our model treats missing values as variables of the
bidirectional RNN graph. Thus, we get the delayed gradients for missing values in both forward and
backward directions, which makes the imputation of missing values more accurate. We performed the
missing value imputation and classiﬁcation/regression simultaneously within a joint neural network.
Experiment results show that our model demonstrates more accurate results for both imputation and
classiﬁcation/regression than state-of-the-art methods.

8

References

[1] C. F. Ansley and R. Kohn. On the estimation of arima models with missing values. In Time

series analysis of irregularly observed data, pages 9–37. Springer, 1984.

[2] M. J. Azur, E. A. Stuart, C. Frangakis, and P. J. Leaf. Multiple imputation by chained equations:
what is it and how does it work? International journal of methods in psychiatric research,
20(1):40–49, 2011.

[3] A. Basharat and M. Shah. Time series prediction by chaotic modeling of nonlinear dynamical
systems. In Computer Vision, 2009 IEEE 12th International Conference on, pages 1941–1948.
IEEE, 2009.

[4] B. Batres-Estrada. Deep learning for multivariate ﬁnancial time series, 2015.

[5] S. Bauer, B. Schölkopf, and J. Peters. The arrow of time in multivariate time series.

In

International Conference on Machine Learning, pages 2043–2051, 2016.

[6] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction
with recurrent neural networks. In Advances in Neural Information Processing Systems, pages
1171–1179, 2015.

[7] M. Berglund, T. Raiko, M. Honkala, L. Kärkkäinen, A. Vetek, and J. T. Karhunen. Bidirectional
recurrent neural networks as generative models. In Advances in Neural Information Processing
Systems, pages 856–864, 2015.

[8] A. P. Bradley. The use of the area under the roc curve in the evaluation of machine learning

algorithms. Pattern recognition, 30(7):1145–1159, 1997.

[9] P. Brakel, D. Stroobandt, and B. Schrauwen. Training energy-based models for time-series

imputation. The Journal of Machine Learning Research, 14(1):2771–2797, 2013.

[10] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for

multivariate time series with missing values. Scientiﬁc reports, 8(1):6085, 2018.

[11] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and
Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078, 2014.

[12] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun. Doctor ai: Predicting clinical
events via recurrent neural networks. In Machine Learning for Healthcare Conference, pages
301–318, 2016.

[13] J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning, volume 1.

Springer series in statistics Springer, Berlin, 2001.

[14] D. S. Fung. Methods for the estimation of missing values in time series. 2006.
[15] A. C. Harvey. Forecasting, structural time series models and the Kalman ﬁlter. Cambridge

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735,

university press, 1990.

1997.

[17] G. Ian, B. Yoshua, and C. Aaron. Deep learning. Book in preparation for MIT Press, 2016.

[18] B. Kaluža, V. Mirchevska, E. Dovgan, M. Luštrek, and M. Gams. An agent-based approach
to care in independent living. In International joint conference on ambient intelligence, pages
177–186. Springer, 2010.

[19] D. M. Kreindler and C. J. Lumsden. The effects of the irregular sample and missing data in
time series analysis. Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using
Real Data, page 135, 2012.

[20] L. Li, J. McCann, N. S. Pollard, and C. Faloutsos. Dynammo: Mining and summarization
In Proceedings of the 15th ACM SIGKDD
of coevolving sequences with missing values.
international conference on Knowledge discovery and data mining, pages 507–516. ACM,
2009.

[21] Z. C. Lipton, D. Kale, and R. Wetzel. Directly modeling missing data in sequences with rnns:
Improved classiﬁcation of clinical time series. In Machine Learning for Healthcare Conference,
pages 253–270, 2016.

9

[22] Z. Liu and M. Hauskrecht. Learning linear dynamical systems from multivariate time series:
In Proceedings of the 2016 SIAM International

A matrix factorization based framework.
Conference on Data Mining, pages 810–818. SIAM, 2016.

[23] S. Moritz and T. Bartz-Beielstein. imputeTS: Time Series Missing Value Imputation in R. The

[24] T. Ozaki. 2 non-linear time series models and dynamical systems. Handbook of statistics,

R Journal, 9(1):207–218, 2017.

5:25–83, 1985.

[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks.

In International Conference on Machine Learning, pages 1310–1318, 2013.

[26] S. Rani and G. Sikka. Recent techniques of clustering of time series data: a survey. International

Journal of Computer Applications, 52(15), 2012.

[27] I. Silva, G. Moody, D. J. Scott, L. A. Celi, and R. G. Mark. Predicting in-hospital mortality
of icu patients: The physionet/computing in cardiology challenge 2012. In Computing in
Cardiology (CinC), 2012, pages 245–248. IEEE, 2012.
[28] D. Sussillo and O. Barak. Opening the black box:

low-dimensional dynamics in high-

dimensional recurrent neural networks. Neural computation, 25(3):626–649, 2013.

[29] D. Wang, W. Cao, J. Li, and J. Ye. Deepsd: supply-demand prediction for online car-hailing ser-
vices using deep neural networks. In Data Engineering (ICDE), 2017 IEEE 33rd International
Conference on, pages 243–254. IEEE, 2017.

[30] J. Wang, A. P. De Vries, and M. J. Reinders. Unifying user-based and item-based collaborative
ﬁltering approaches by similarity fusion. In Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in information retrieval, pages 501–508. ACM,
2006.

[31] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo. Convolutional lstm
network: A machine learning approach for precipitation nowcasting. In Advances in neural
information processing systems, pages 802–810, 2015.

[32] X. Yi, Y. Zheng, J. Zhang, and T. Li. St-mvl: ﬁlling missing values in geo-sensory time series

data. 2016.

[33] J. Yoon, W. R. Zame, and M. van der Schaar. Multi-directional recurrent neural networks: A

novel method for estimating missing data. 2017.

[34] H.-F. Yu, N. Rao, and I. S. Dhillon. Temporal regularized matrix factorization for high-
dimensional time series prediction. In Advances in neural information processing systems,
pages 847–855, 2016.

[35] J. Zhang, Y. Zheng, and D. Qi. Deep spatio-temporal residual networks for citywide crowd

ﬂows prediction. AAAI, November 2017.

10

A Performance Comparison for Classiﬁcation Tasks

Table 2 shows the performances of different RNN-based models on the classiﬁcation tasks. As we can
see, our model BRITS outperforms other baseline methods for classiﬁcations. Comparing with Table
1, GRU-D performs well for classiﬁcation tasks. Furthermore, we ﬁnd that it is very important for
GRU-D to carefully apply the dropout techniques in order to prevent the overﬁtting (we use p = 0.25
dropout layer on the top classiﬁcation layer). However, our models further utilize the imputation
errors as the supervised signals. It seems that dropout is not necessary for our models during training.

Table 2: Performance Comparison for Classiﬁcation Tasks

Method Health-care (AUC) Human Activity (Accuracy)
GRU-D
M-RNN

0.939 ± 0.010
0.938 ± 0.010

0.828 ± 0.004
0.800 ± 0.003

RITS-I
BRITS-I
RITS
BRITS

0.821 ± 0.007
0.831 ± 0.003
0.840 ± 0.004
0.850 ± 0.002

0.934 ± 0.008
0.940 ± 0.012
0.968 ± 0.010
0.969 ± 0.008

B Performance Comparison for Univariate Synthetic Data

To better understand our model, we generate a set of univariate synthetic time series. Speﬁcically, we
randomly generate a time series with length T = 36, using the state-space representation [15]:
xt = µt + θt + (cid:15)t,
µt = µt−1 + λt−1 + ξt,
λt = λt−1 + ζt,
s−1
(cid:88)

θt =

−θt−j + ωt,

j=1

where xt is the t-th value in time series. The residual terms (cid:15)t, ξt, ζt and ωt are randomly drawn
from a normal distribution N(0, 0.3). We eliminate about 22% values from the generated series, and
compare our model BRITS-I (note the data is univariate) and ImputeTS. We show three examples in
Fig. 3.

Figure 3: Example for synthetic time series imputation. Each column corresponds to one time series.
The ﬁgures in the ﬁrst row are imputations of ImputeTS algorithm, and the ﬁgures in the second row
are imputations by BRITS-I.

The ﬁrst row corresponds to the imputations of ImputeTS and the second row corresponds to our model
BRITS-I. As we can see, our model demonstrates better performance than ImputeTS. Especially,
ImputeTS fails when the start part of time series is missing. However, for our model, the imputation
errors are backpropagated to previous time steps. Thus, our model can adjust the imputations in the
start part with delayed gradient, which leads to much more accurate results.

11

Figure 4: Validation errors for BRITS and BRITS-cut during training

C Performance for Non-differentiable ˆxt

As we claimed in Section 4.1, we regard the missing values as variables of RNN graph. During the
backpropagation, the imputed values can be further updated sufﬁciently. In the experiment, we ﬁnd
that if we cut the gradient of ˆxt (i.e., regard it as constant), the models are unstable and easy to overﬁt
during the training. We refer to the model with non-differentiate ˆxt as BRITS-cut. Fig. 4 shows the
validation errors of BRITS and BRITS-cut during training for the health-care data imputation. At the
ﬁrst 20 iterations, the validation error of BRITS-cut decreases fast. However, it soon fails due to the
overﬁtting.

D Test Data of Air Quality Imputation

We use the same method as in prior work [32] to select the test data for air quality imputation.
Speciﬁcally, we use the 3rd, 6th, 9th and 12th months as the test set and the rest data as the training
set. To evaluate our model, we select the test timeslots by the following rule: if a measurement
is observed at a timeslot in one of these four months (e.g., 8 o’clock 2015/03/07), we check the
corresponding position in its previous month (e.g., 8 o’clock 2015/02/07). If it is absent in the
previous month, we eliminate such value and use it as the imputation ground-truth of test data. Recall
that to train our model, we randomly select consecutive 36 time steps as one time series. Thus, a test
timeslot may be contained in multiple time series. We use the mean of the imputations in different
time series as the ﬁnal result.

12

