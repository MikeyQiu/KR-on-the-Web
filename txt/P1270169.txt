9
1
0
2
 
y
a
M
 
2
2
 
 
]

V
C
.
s
c
[
 
 
2
v
9
5
1
8
0
.
4
0
9
1
:
v
i
X
r
a

3D OBJECT RECOGNITION WITH ENSEMBLE LEARNING — A
STUDY OF POINT CLOUD-BASED DEEP LEARNING MODELS

Daniel Koguciuk
Warsaw University of Technology
Faculty of Mechatronics
Boboli 8, 05-525 Warsaw
daniel.koguciuk@gmail.com

Łukasz Chechli ´nski
Warsaw University of Technology
Faculty of Mechatronics
Boboli 8, 05-525 Warsaw
lukasz.chechlinski@gmail.com

Tarek El-Gaaly
Voyage
Palo Alto, CA
tgaaly@gmail.com

A PREPRINT

May 24, 2019

ABSTRACT

In this study, we present an analysis of model-based ensemble learning for 3D point-cloud object
classiﬁcation and detection. An ensemble of multiple model instances is known to outperform a single
model instance, but there is little study of the topic of ensemble learning for 3D point clouds. First, an
ensemble of multiple model instances trained on the same part of the ModelNet40 dataset was tested
for seven deep learning, point cloud-based classiﬁcation algorithms: PointNet, PointNet++, SO-Net,
KCNet, DeepSets, DGCNN, and PointCNN. Second, the ensemble of different architectures was
tested. Results of our experiments show that the tested ensemble learning methods improve over state-
of-the-art on the ModelNet40 dataset, from 92.65% to 93.64% for the ensemble of single architecture
instances, 94.03% for two different architectures, and 94.15% for ﬁve different architectures. We
show that the ensemble of two models with different architectures can be as effective as the ensemble
of 10 models with the same architecture. Third, a study on classic bagging (i.e. with different subsets
used for training multiple model instances) was tested and sources of ensemble accuracy growth were
investigated for best-performing architecture, i.e. SO-Net. We also investigate the ensemble learning
of Frustum PointNet approach in the task of 3D object detection, increasing the average precision of
3D box detection on the KITTI dataset from 63.1% to 66.5% using only three model instances. We
measure the inference time of all 3D classiﬁcation architectures on a Nvidia Jetson TX2, a common
embedded computer for mobile robots, to allude to the use of these models in real-life applications.

Keywords Point Cloud · Point Set · Classiﬁcation · Detection · Ensemble Learning · 3D Deep Learning

1

Introduction

Over the last few years with the rapid development of sensor technology, processing of three–dimensional data (3D) has
become an important topic of research. High quality, long range laser scanners are widely used in autonomous cars [1],
and the availability of cheap RGB-D sensors has resulted in signiﬁcant progress in 3D mobile robots perception [2].
Accurate object detection, segmentation, and classiﬁcation from 3D point-clouds are challenging problems, especially
so in real-world settings, and crucial for performing robotic tasks.

A lot of the handcrafted approaches to 3D point cloud analysis have been developed previously [3, 4, 5]; however, in
recent years, deep learning – based approaches have increased in popularity [6, 7, 8, 9]. Results of deep learning models
are strongly correlated to the available amount of data and to the quality of used regularization techniques [10]. The
study of using these deep neural networks in ensemble learning for 3D point cloud recognition is lacking.

In point cloud classiﬁcation task we assume that the object is already segmented, which means that all the points belong
to that single-class object. There are 3 main approaches to point cloud classiﬁcation:

A PREPRINT - MAY 24, 2019

Figure 1: A visualization of an example point cloud from ModelNet40 dataset with classiﬁcation output of analyzed
architectures on the left. On the right output of 10 PointNet instances as a vector of 40 scores for each of class in the
ModelNet40 dataset followed by the voting module and with ﬁnal classiﬁcation output. All outputs and score values are
made up.

• 3D ConvNets — point cloud is converted to voxel grid with a given resolution. This approach is not memory
efﬁcient in case of large volumes. The sparsity of 3D data leads to inefﬁcient and redundant computation.
However, some octree- or kd-tree–based approaches reduce these disadvantages and provide encouraging
results [6, 11].

• Rendering a set of 2D views of the 3D object — the problem is transformed into a set of 2D vision problems.
View-pooling layer [12] may be used to aggregate features from different views. This technique leverages the
performance of 3D ConvNets, but the loss of information during rendering makes this approach impractical in
point-level segmentation task [7, 13].

• Direct point cloud processing — architectures that directly process point-clouds in an order-invariant manner,
ﬁrst presented by the PointNet architecture. It can be adapted to the analysis of different kinds of problems
ranging from an estimation of population statistics [14], anomaly detection in piezometer data of embankment
dams [15], to cosmology [16].

In this paper, we focus on the direct point cloud processing, because such architectures can perform well not only in
classiﬁcation, but also in segmentation and detection tasks. Seven architectures are used in our experiments: PointNet
[8], PointNet++ [17], SO-Net [18], KCNet [19], DeepSets [20], DGCNN [21], and PointCNN [22]. We chose these
because of their prominence and the availability of author’s implementations that are open to the research community.

For object classiﬁcation, two types of datasets can be considered. The ﬁrst type is based on 3D CAD models:
PrincetonSB [23], ModelNet [11], ShapeNet [24], and many others. The second type are datasets of 3D objects/scenes
acquired from the real world with depth sensors [1, 25, 26, 1]. In this work, we focus on ModelNet40 [11], because it is
one of the most popular benchmarks for object classiﬁcation. It contains 40 classes of objects’ CAD models and ﬁgure
1 presents an example point cloud from ModelNet40. For real-world applications, the KITTI dataset [1] is the most
prominent and widely used benchmark for 3D perception of autonomous vehicles and thus is our focus in this study.

Above mentioned architectures are getting more and more complicated. For example, PointNet is a special case of
later introduced PointNet++ with 1.5% increase of instance classiﬁcation accuracy on ModelNet40. However, one can
achieve half of this accuracy increase with the ensembling of ten PointNet models. We do not want to prove there is no
need for further architecture exploration; rather, thanks to ensemble learning we want to gain more insights into those
architecture and the task itself.

Ensemble learning [27] increases performance of the prediction by leveraging multiple models. Several methods are
reported in the literature: bagging, boosting, stacking, a bucket of models, Bayesian methods, and many others. In
this paper, we focus on bagging, also known as bootstrap aggregating. We test three voting methods: direct output
averaging, soft voting, and hard voting. We compare the ensemble of model instances trained on the same training set
and its different subsets and evaluate their performance.

Our experiments show that an ensemble of neural networks trained on the whole training set is better than bagging
using random parts of the training set. An ensemble of different model with different architectures can even further
improve performance. In addition, we examine the number of trainable parameters and inference execution times on

2

A PREPRINT - MAY 24, 2019

NVIDIA Jetson TX2 platform for each approach. According to previous studies [28, 29], using the Jetson platform as a
high-level driver is a reasonable choice for energy-efﬁcient mobile robotic applications.

2 Related Work

To the best of our knowledge, there are no studies reporting on the strict inﬂuence of different ensemble methods and
the number of aggregated models to the prediction accuracy for direct point cloud classiﬁcation architectures. Su et
al. [30] studied a model combining different types of representations, but since there are fast advances in this ﬁeld,
there are more and more models in the point cloud classiﬁcation zoo. A previous article [31] has reported signiﬁcant
performance gain while using an ensemble of 10 instances of one voxel-based, deep learning architecture, introduced in
that article.

This section starts with a brief description of each tested architecture. We compare the reported results with test accuracy
reproduced in our experiments. The whole setup with exact versions of all libraries and code version used has been
shared online1. The difference with original reports may be caused by some implementation details, hyper–parameters,
or test methodology (reporting maximum vs. average score). Finally, we brieﬂy describe bagging.

DeepSets [20] is one of the two independently developed (along with PointNet) ﬁrst deep learning approaches to shape
classiﬁcation using bare point sets. The general idea is quite similar in both methods, but DeepSets focuses primarily on
producing the permutation equivariance layer, which consists of three operations: phi, sum, and rho. According to
the paper, phi could be an arbitrary neural network architecture applied iteratively over every point in the point cloud,
and the output should be summed along the set dimension. The reduced vector can then be passed into the multilayer
perceptrion (MLP) working as a classiﬁer.

Given the symmetry in weight sharing, the ﬁnal features of the whole point set (after the summation) are invariant to the
ordering of the input. This article demonstrates a strong mathematical background, many sample applications, and great
results on the ModelNet40 dataset. Despite this, there is no direct successor of the model in the literature.

The basic idea of PointNet [8] is to learn a spacial encoding of each point using a series of nonlinear mappings and then
aggregate all point features to one global point cloud signature. The ﬁrst part plays a similar role to the phi operation in
DeepSets (there are some slight differences in the weight sharing scheme), and the second one (symmetric function) is
also DeepSets alike, but the authors suggest using max pooling operation as the one achieving the best performance.

The model is also invariant to the order in which the points are presented, which can directly operate on point clouds
without any additional preprocessing such as spatial partitioning or graph construction. Moreover, the model is
extremely robust to deformation and noise, but by its design, it suffers from not being able to detect small local structure
details; thus, it leaves big space for modiﬁcations.

PointNet++ is an extended version of original PointNet architecture [17], where authors made hierarchical feature
extraction by building a pyramid-like aggregation scheme to combine features from multiple scales. There are three
steps on each pyramid level: sampling, grouping, and feature extraction. The ﬁrst two steps consist of partitioning
the input point cloud into overlapping local regions by the distance metric of the underlying space. The third step is
learning a higher dimensional representation of an input region with the so-called local learner which is, naturally, a
standard PointNet model. There are three such pyramid levels in the original PointNet++ article, which produce the
features of the whole point set.

SO-Net [18] is another hierarchical feature extraction model based on PointNet approach, but it has a different sampling
and grouping strategy in comparison to PointNet++. The main idea is to build a Self-Organizing Map (SOM) to model
the spatial distribution of the input point cloud and then assign each point into k nearest SOM nodes, which play a
similar role to sampling and grouping steps in PointNet++. Each local region is processed with a local pointnet-like

1https://github.com/dkoguciuk/ensemble_learning_for_point_clouds

3

2.1 DeepSets

2.2 PointNet

2.3 PointNet++

2.4 SO-Net

A PREPRINT - MAY 24, 2019

learner, and a channel-wise max pooling operation is applied to aggregate point features to node signatures. Now, each
SOM node with its features is processed with the second level learner and again aggregated with max-pool into a feature
vector that represents the whole input point cloud.

The main idea of the KCNet model [19] is to construct a kernel correlation layer as an afﬁnity measure between a query
point with its neighbors and kernel points, where the latter are allowed to move and adjust during training freely. KCNet
uses several kernels at the local level to augment the original 3D coordinates’ input of the PointNet architecture. The
second modiﬁcation to the PointNet model is a recursive max-pooling operation performed in the neighborhood of each
node.

Instead of generating the point embedding directly from the point coordinates, the DGCNN [21] introduces EdgeConv
operation, which incorporates a point’s neighboring structure. For each point of a point cloud, they construct a local
neighborhood graph and apply deep learning feature extraction on edges of this graph. EdgeConv is designed to be
invariant to the ordering of neighbors, and thus is permutation invariant.

The operation could be applied hierarchically just like in the traditional convolutional networks for 2D images, but the
authors propose to build the graph of neighboring points for each layer independently.

2.5 KCNet

2.6 DGCNN

2.7 PointCNN

The PointCNN [22] uses a Multi-layer Perceptron (MLP) on the local neighborhood to organize points into a latent
canonical order. The network learns, so-called, X -transformation results not only in permuting input but also in
weighing the features associated with the points. An element-wise product and sum operations are applied on the
X -transformed features. These operations can be applied hierarchically: after each convolution, a subset of points are
retained by downsampling, thus contain richer information aggregated by the expanding neighborhood.

2.8 Comparison of Classiﬁcation Architectures

All architectures can be split into the following groups: DeepSets and PointNet as pionering approaches using global
shape feature, PointNet++ and SO-Net as hierarchical pointnets, KCNet and DGCNN as learnable local feature
extractors and PointCNN as hierarchical feature extractor.

The general idea behind PointNet and DeepSets approaches is similar, but they differ mostly in weight-sharing schemes
in the MLP network and afﬁne transformation matrix prediction by a T-Net network in PointNet. Authors call it a
mini-network, but in fact there are two T-Net modules used in PointNet (for points and features transformation) and
they consist of about 75% of the whole network parameters. Without T-Net modules PointNet has a similar number of
network parameters to DeepSets. Both architectures are prominent works in the ﬁeld but do not explicitly use local
structure information.

PointNet++ and SO-Net both apply PointNet hierarchically but differ in the sampling and grouping strategy. PointNet++
samples centroids of local regions using farthest point sampling algorithm (FPS) and SO-Net uses SOM. Both algorithms
reveal the spatial distribution of points, but SO-Net implementation provides deterministic sampling, whereas in
PointNet++ sampled points depend on the choice of the starting point of FPS. Secondly, in SO-Net each point is
assigned into k nearest SOM nodes, which ensure regions to be overlapped. On the other hand grouping in PointNet++
is done by a ball query within a speciﬁed radius – the radius should be picked carefully so the regions will overlap
slightly. The third difference is multi-resolution or multi-scale grouping strategy used in PointNet++. SO-Net does
not have any similar strategy, but experiments with the former show that the strategy does not increase classiﬁcation
accuracy much, rather it helps in robustness against missing points in a point cloud.

KCNet and DGCNN both add local structure information, which is learnable — not designed by hand like in PointNet++
or SO-Net. KCNet uses interesting kernel correlation technique, where DGCNN uses feature extraction form graph
edges by traditional MLP. The former is deﬁne only in (cid:60)3, whereas the latter can operate on high dimensional input,
thus can be applied hierarchically. Authors of KCNet introduce feature pooling via graphs, which tend to be more
effective than max-pool in PointNet. KCNet and DGCNN architectures can be viewed as a PointNet working on points
with learnable local features and having more effective feature aggregation scheme than original max-pool.

PointCNN is the only architecture truly working in hierarchical manner with 4 repeated χ-conv operations applied and 3
FC layers on top. However, there is a ﬁxed number of neighbors, and they are found using K-Nearest Neighbor, which

4

A PREPRINT - MAY 24, 2019

assume an equal distribution of points in the whole point cloud. Besides, there is no clever way of sampling points —
rather they are sampled randomly. Despite all the assumptions PointCNN is different from the rest of the architectures
because of the permutation invariance approach: it tries to sort points in canonical order rather than using a symmetric
function like max-pool.

2.9 Frustum PointNet

The main author of PointNet and PointNet++ extended the work into a 3D object detection framework called Frustum
PointNet [32]. The main idea is to combine both 2D and 3D approaches by splitting the task into three main stages:
3D frustum proposal based on 2D object detection, 3D instance segmentation, and 3D bounding box estimation. The
modules are based on PointNet (denoted as v1) or PointNet++ (v2) architectures.

In the ﬁrst stage, a 2D CNN object detector is used to generate 2D region proposals. Then each region is lifted up to 3D
and thus become frustum proposal, containing point cloud I — all points in the LiDAR point cloud which lie inside the
2D region when projected onto the image plane. The output point cloud I is then fed into 3D instance segmentation
network with binary output P = fP (θp, I, c) (where θp are the model instance weights, c is class of the object and fp is
a model function) meaning if the given point is a part of a 3D object or not, which assumes there is only one meaningful
object in the frustum. Points belonging to the object form a point cloud O = fo(I, P ). Then, the T-Net module ﬁnd
transformation T = fT (θT , O) centering O. Centered point cloud is denoted as C = fC(O, T ). In the last step, C is
used to estimate amodal 3D bounding box of the object B = fB(θB, C), which is ﬁnally transformed to the global
frame. Amodal bounding box B is described by its position, size, and heading angle. Size and heading are represented
as discrete probability distributions, not raw values. Summing up, the network prediction can be denoted as:

B = fB(θB, fC(O, fT (θT , O)))
O = fO(I, fP (θP , I, c))

2.10 Ensemble Learning with Bagging and Boosting

An ensemble consists of a set of individually trained models, whose predictions are combined. It is well known that
ensemble methods can be used to improve prediction performance [27, 33].

Individual models may be trained using different training sets. In bagging, the training sets are selected independently
for each classiﬁer from the full training set. The selected set can be a subset of the entire training set (later referred to as
bagging without replacement) or can have the same size, but samples can repeat (bagging with replacement). However,
the result of neural network training depends on several random factors so that the ensemble can consist of classiﬁers
trained on the same training set, which we refer to as a simple ensemble.

In Boosting approaches a series of classiﬁers are tained, with the training set (or samples loss weights) of the next
classiﬁer focusing on the samples with a higher error for the previous classiﬁer. This can reduce errors, but noise in the
training data often results in boosting overﬁtting [27].

Given the output of the individual classiﬁer, the output of the ensemble can be calculated in different ways. Boosting
uses individual weight for each of the classiﬁer in series. Stacking trains a learning algorithm to combine predictions.
In bagging and simple ensembles, all classiﬁers are equivalent, so three aggregation methods are commonly used in
classiﬁcation task: direct output averaging, soft voting (sum of activation of all hypothesis for each sample equals to
one) and hard voting (each classiﬁer output is in one-hot form, , i.e. each classiﬁer votes for one hypothesis).

3 Ensemble Learning for 3D Object Recognition

We performed several experiments during the conduct of this study. Seven deep network architectures were selected:
PointNet, PointNet++, SO-Net, KCNet, DeepSets, DGCNN, and PointCNN. All networks were tested on one task, i.e.
ModelNet40 object classiﬁcation. All these networks take a raw 3D point cloud as an input and output a vector of class
scores for a given object, which can be denoted as follows:

F : {pi ∈ (cid:60)3, i = 1, . . . , N } → (cid:60)C
F = {fj, j = 1, . . . , C}

where N is the number of points in the point cloud, and C is the number of classes in the classiﬁcation task.

One set of hyper–parameters is selected for each network based on the authors’ settings. 10 model instances are trained
for each architecture. The inﬂuence of the number of models in the simple ensemble is tested for each architecture,
which is described in subsection 3.1.

(1)

(2)
(3)

5

A PREPRINT - MAY 24, 2019

Figure 2: Dependency between instance classiﬁcation accuracy (left) or mean class accuracy (right) and k-number of
ensemble models for a given voting method. For visibility, only results for best-performing architecture and mean
result of all architectures are plotted. Results for soft-voting and activation ensemble are approximately equal, usually
outperforming hard-voting.

Different architectures achieve the best result for different object classes. This suggests studying if the ensemble of
different models can outperform every single model. Such a comparison for PointNet, PointNet++, KCNet, DGCNN,
PointCNN, and 3.2. DeepSets was not included due to the challenges in technical implementation.

An SO-Net architecture achieved the highest accuracy during our experiments, so it was selected for further bagging
tests. We tested the classiﬁcation accuracy for bagging with and without replacement in subsection 3.3.

The ensemble of several model instances is computationally expensive. However, a deep network architecture can
be viewed as an encoder (transforming sample to a feature vector) followed by a classiﬁer (e.g. MLP, calculating
class probabilities based on a feature vector). The question arises, whether classiﬁcation accuracy can be improved
by an ensemble of classiﬁers, based on the same feature vector. This was tested for SO-Net architecture according to
subsection 3.4.

Our work shows that a simple ensemble of several model instances of the same architecture increases classiﬁcation
performance. Random factors cause differences between model instances. Inﬂuence of each factor is evaluated in
subsection 3.5.

An ensemble of three model instances of Frustum PointNet was tested on the KITTI dataset. This is described in section
3.6. Note, that Frustum PointNet is a pioneering approach, in the sense that it performs detection + classiﬁcation on raw
LiDAR scans, so it cannot be compared directly with the other models presented.

3.1 Simple Ensemble of Model Instances

We experiment with the simple ensemble, which is a special case of bagging, with the full training dataset being used to
train every model instance. The ensembling is performed by averaging the raw output activation, soft voting, or hard
voting (denoted as F , S, and H respectively):

Fe = {fi = mean(fkj, k = 1, . . . , K), j = 1, . . . , C}
Se = {fi = mean(sof tmax(fk)j, k = 1, . . . , K), j = 1, . . . , C}
He = {fi = mean(onehot(argmax(fk))j, k = 1, . . . , K), j = 1, . . . , C}

(4)
(5)
(6)
(7)

Where each model instance is denoted as:

Fk = {fkj, j = 1, . . . , C}
(8)
where k is a model index. For each architecture, 10 model instances were trained. Tests were performed for K =
(cid:1) model instances’ combinations were selected. Mean and standard deviation for
1, . . . , 10. For each value of K, all (cid:0)10
each value of K are reported in the experimental results.

K

Figure 2 shows the comparison of voting methods. One needs at least three votes for hard-voting to contribute any useful
information. With the inﬁnite number of ensembled models, all voting methods are expected to produce asymptotically

6

A PREPRINT - MAY 24, 2019

Figure 3: Dependency between instance classiﬁcation accuracy (left) or mean class accuracy (right) and k-number
of ensemble models. We have learned each approach independently 10 times (70 different models), then for each k
possible number included in the ensemble classiﬁer we have randomly chosen ten different k-subsets and have reported
mean accuracy with its standard deviation across those k-subsets. As one can observe, using the ensemble learning
makes the output more stable and classiﬁcation accuracy rise slightly.

same results, but for a ﬁnite number of ensembled models, raw activation averaging equals approximately to soft-voting
and usually outperforms hard-voting. The difference could be caused by the inﬂexibility of hard-voting: if a particular
model outputs high scores for two classes, a small score change means the instability of the output class. For simplicity,
only activation averaging is used in the rest of this paper.

Figure 3 presents the instance and mean class accuracy as a function of the number of models in the ensembles for
each architecture, along with their standard deviation. As one can observe, with the increasing number of models in the
ensemble, the classiﬁcation accuracy is slightly rising, and the standard deviation of classiﬁcation accuracy is getting
smaller, which means that the output is more stable and not so much dependent on a single learning session.

Table 1 shows the numerical comparison of classiﬁcation accuracy increase between all approaches. The simple
ensemble of KCNet instances has a noticeably higher increase in the classiﬁcation accuracy (2.52%), then second
SO-Net (0.99%) and other architectures (with mean instance accuracy increase equal to 0.50%).

Figures 4 and 5 show the loss or gain in the classiﬁcation accuracy per every class between version without and with
simple ensemble for all tested approaches. Some classes are easy to classify and all approaches achieve 100% accuracy,
for example, airplane and laptop. However, interestingly, there are classes where the PointNet approach does better
than others, despite the smallest accuracy of overall classiﬁcation, for example, glass box and stool. This suggests that
different methods can be focused on various aspects of point clouds, in particular, focusing on local structure leads to
overﬁtting for some classes with more discriminative global shape.

One can ask one more interesting question about those approaches and their ensembles: in how many classes a particular
model has the highest accuracy? Figure 6 answers that question and shows how this number is changed after simple
ensemble (if N classes reach same best accuracy, each of them scores 1/N in this rank). Note that the order of
architectures is different than that given in Table 1.

Table 1: Average instance and mean class accuracy for each architecture. Results for the single model, simple ensemble
of 10 models and the accuracy increase are detailed. The simple ensemble of KCNet instances has a high increase in the
classiﬁcation accuracy (2.52%).

Instance accuracy
(reported)
89.20
90.70
90.30
91.00
93.40
92.20
92.20

Instance accuracy
mean
88.65
90.14
89.71
89.62
92.65
91.55
91.82

Instance accuracy
ensemble
89.38
90.48
90.27
92.14
93.64
92.02
92.22

Class accuracy
mean
85.77
87.71
85.79
85.38
89.98
89.0
87.85

Class accuracy
ensemble
86.62
88.19
86.46
88.28
91.02
89.30
88.36

Instance accuracy
increase
0.74
0.34
0.56
2.52
0.99
0.47
0.41

Class accuracy
increase
0.86
0.48
0.67
2.89
1.05
0.27
0.50

PointNet
PointNet++
DeepSets
KCNet
SO-Net
DGCNN
PointCNN

7

A PREPRINT - MAY 24, 2019

Figure 4: Class classiﬁcation accuracy, its gain (green), or loss (red) for all architectures for less accurate classes. Please
note the ﬂower pot class is the hardest class to classify, and almost all architectures using point neighborhood are doing
slightly worse with ensemble learning here. This could suggest that the general shape for ﬂower pot is more meaningful
than local structure information.

3.2 Ensemble of Different Models

We evaluate ensembles of pairs of different models. First, an ensemble of output scores were calculated for pairs of
architectures according to the following formula:

Fpair =k1 · F1 + k2 · F2

k1 + k2 = 1
k1 = 0.1, 0.2, . . . , 0.9

(9)

Note that each output of the architecture is scaled to have identity standard deviation for the training set. The ensemble
of different models improved both instance and mean class accuracy. Top pairs consist of SO-Net model with higher
weight and the second architecture. Ensemble results are calculated for 10 model instances: 5 of one architecture and
5 of the other. Table 2 shows the results. As two model instances are used in the ensemble to calculate its accuracy,
results for two SO-Net instances ensemble are plotted for reference.

Table 2: Example results for ensemble of models with two different architectures.

F1

F2

k1

SO-Net
SO-Net
SO-Net KCNet
SO-Net DGCNN
SO-Net
SO-Net

PointNet
0.7
PointNet++ 0.7
0.8
0.9
0.8
0.5

PointCNN
SO-Net

k2

0.3
0.3
0.2
0.1
0.2
0.5

Instance
accuracy mean
93.23%
93.41%
93.21%
93.64%
93.55%
93.18%

Instance accuracy
ensemble
93.65%
93.75%
93.73%
93.95%
94.03%
93.64%

Class accuracy
mean
90.95%
91.33%
90.62%
91.59%
90.97%
90.57%

Class accuracy
ensemble
91.44%
91.61%
90.99%
92.00%
91.50%
91.02%

8

A PREPRINT - MAY 24, 2019

Figure 5: Class classiﬁcation accuracy, its gain (green) or lose (red) for all architectures for more accurate classes (with
accuracy between 80 and 100%). Please note that many classes, for example, bed or cone, all architectures achieves
better classiﬁcation accuracy using ensemble learning.

9

A PREPRINT - MAY 24, 2019

Figure 6: Number of classes where certain architecture is the best for a version without ensemble, its gain (green), or
loss (red). High gain of GDCNN and PointCNN architectures suggests they can be much better in classifying some
classes and much worse in other ones.

Ensemble of all considered architectures with the best-performing SO-Net was tested according to the following formula:

Fall = kpointnet · Fpointnet + kpointnet++ · Fpointnet++ + kkcnet · Fkcnet+

+ kdgcnn · Fdgcnn + kpointcnn · Fpointcnn + kso−net · Fso−net
kpointnet + kpointnet++ + kkcnet + kdgcnn + kpointcnn + kso−net = 1
kpointnet = 0.0, 0.05, . . . , 0.35
kpointnet++ = 0.0, 0.05, . . . , 0.35
kkcnet = 0.0, 0.05, . . . , 0.35
kdgcnn = 0.0, 0.05, . . . , 0.35
kpointcnn = 0.0, 0.05, . . . , 0.35
kso−net > 0.4

(10)

Note that this is the ensemble of different architectures including only one training instance of each architecture. The
obtained models are further tested in the aspect of multiple training instances learning as described in subsection 3.1.
The ensemble of all architectures with the major role of SO-Net, achieves the highest overall accuracy. The results of
the ensemble are calculated for ﬁve instances for each architecture with the nonzero factor. Table 3 shows the most
interesting results.

3.3 Ensemble Learning with Model Bagging

For the SO-Net architecture, which achieves the highest overall accuracy, model bagging was tested. For bagging with
replacement, 10 training sets were generated by randomly sampling with replacement of a number of samples equal to
the size of the training set. For bagging without replacement, S = 9 training subset sizes were used in experiments,

Table 3: Example results for ensemble of models with different architecture.

kpointnet

kpointnet++

kkcnet

kdgcnn

kpointnet

kso−net

-
0.05
-
-
0.05
-
0.05
-
-
-

0.1
0.05
0.05
0.05
0.05
0.3
0.3
0.15
0.15
0.2

-
-
-
-
-
-
-
-
-
-

-
-
0.05
-
-
0.05
0.05
0.1
0.1
0.1

0.2
0.25
0.3
0.2
0.2
-
-
0.05
-
-

Instance
accuracy mean
93.73%
93.74%
93.88%
93.65%
93.67%
93.73%
93.78%
93.76%
93.70%
93.68%

Instance accuracy
ensemble
94.13%
94.14%
94.14%
94.15%
94.15%
94.04%
94.04%
94.03%
94.05%
94.06%

Class accuracy
mean
91.24%
91.17%
91.52%
91.14%
91.12%
91.66%
91.79%
91.69%
91.64%
91.60%

Class accuracy
ensemble
91.77%
91.57%
91.92%
91.76%
91.71%
92.20%
92.20%
92.21%
92.22%
92.24%

0.7
0.65
0.6
0.75
0.7
0.65
0.6
0.7
0.75
0.7

10

A PREPRINT - MAY 24, 2019

Figure 7: Results of the simple ensemble, bagging without replacement for ten different training subset sizes and
bagging with replacement. Results for SO-Net architecture, instance (left) and class (right) classiﬁcation accuracy. Error
bars not plotted to prevent jamming. The biggest gain is achieved for the smallest training subset size, but the overall
classiﬁcation accuracy is the best for simple ensemble (aka bagging with replacement).

denoted as follows:

si = k · size(Ttrain), k = 0.1, 0.2, . . . , 0.9

(11)

For each si, 10 training set splits were generated (sampled without replacement), and one model instance was trained.
Output of each model for a given si was aggregated as detailed in subsection 3.1.

Figure 7 shows the results. The biggest gain is achieved for the smallest training subset size. Accuracy increase
for bagging with replacement is higher than that of without replacement. However, none of the bagging methods
outperforms simple ensemble in the given task.

3.4 Simple Ensemble of Last Layers

The SO-Net architecture consists of the explicitly deﬁned encoder (computationally expensive) and (fast) classiﬁer.
Now, we can check whether the accuracy growth of ensemble learning or SO-Net architecture is determined mostly by
encoder or classiﬁer part. In the case of the the latter, one could learn the ensemble of classiﬁers only and thus save
learning time by a signiﬁcant factor. To check this, for each of the 10 SO-Net encoder instances, 5 additional classiﬁers
were trained, with the same hyper–parameters, constant encoder weights, and 31 training epochs. The average result of
5–classiﬁer ensemble is compared to the average result of model with a single classiﬁer (both averages are calculated
for 10 encoder instances).

Table 4 shows the results. According to the results, the encoder causes the major advantage of the ensemble. This
means that computationally cheaper classiﬁer ensemble does not result in rewarding accuracy gain.

3.5

Inﬂuence of Random Factors in Simple Ensembles

We identiﬁed four random factors in SO-Net model training:

• The order of training samples and random data augmentation (note that eliminating this factor means that

samples are shufﬂed between training epochs but in the same way for every model instance);

• Initial values of weights and biases of the neural network;

• Random dropout regularization (eliminating this factor means ﬁxing the dropout seed so certain neuron would

be dropped, for example, always in epoch number 3, 7, 17, etc.);

• Random order of massively parallel computations, resulting in different summation order, which is not

alternating for ﬂoating point numbers.

The ﬁrst three factors can be eliminated, whereas the last one cannot be eliminated. To verify the inﬂuence of each factor,
ﬁve SO-Net model instances were trained for each conﬁguration with one, two, or three random factors eliminated.

11

Table 4: The inﬂuence of classiﬁers ensemble with one encoder for SO-Net architecture. Five classiﬁers were trained for
each of the 10 encoders. As one can see, the computationally cheaper classiﬁer ensemble does not result in rewarding
accuracy gain.

A PREPRINT - MAY 24, 2019

Parameter
Instance accuracy mean
Instance accuracy ensemble
Class accuracy mean
Class accuracy ensemble
Instance accuracy increase
Class accuracy increase

Mean
92.43%
92.69%
89.80%
89.98%
0.25%
0.18%

Standard Deviation
0.19%
0.28%
0.21%
0.28%
0.12%
0.12%

The experiments were time-consuming (35 additional training sessions), but the results, presented in Table 5, are coarse
because only one constant order of values for each factor was considered. However, one can observe that the increase in
the accuracy can be observed even if all model instances in the ensemble were trained with the same training data order
and augmentation, initial weights, and dropout order. This leads to the conclusion that for SO-Net architecture, diversity
in models is caused mainly just by the numerical issues of massively parallel computations.

3.6 Ensemble Methodology for Frustum PointNet

Since the ﬁrst step of Frustum PointNet approach use 2D CNN object detector, which is beyond the scope of this paper,
we look at ensemble learning for the three point cloud processing modules of the network. A single experiment on
the KITTI dataset was performed to verify conclusions based on ModelNet40 and to plan further work on knowledge
distillation with ensemble learning for real-world 3D data.

Frustum PointNet consists of three trainable modules. Ensembling can be used either for the last bounding-box
predicting module or for all modules (denoted as BL and BE, respectively). Results for other conﬁgurations (e.g.
ensemble of only segmentation modules) have been shared online. Given N independently trained model instances, BL
and BE can be deﬁned as follows (compare with equation 1):

BL =

fB(θBi, fC(O, fT (θT , O)))

BE =

fB(θBi, fC(OE,

fT (θT j, OE)))

(12)

1
N

N
(cid:88)

j=1

OE =

fO(I, fP (θP k, I, c))

1
N

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

k=1

Three Frustum PointNet instances were trained for both v1 and v2 conﬁgurations. Accuracy is calculated for ground
and 3D detection. Results are limited by the 2D bounding box proposal accuracy. Objects are divided into three classes
(car, pedestrian and cyclist) and three difﬁculty levels (easy, medium and hard). Tables 6 and 7 presents the accuracy for
object classes and for difﬁculty levels, respectively.

Table 5: The inﬂuence of random factors in model instances training to ensemble accuracy gain. One can observe that
accuracy increases even if results of massively parallel computations are the only one (irremovable) random factor. One
can observe all removable random factors in the training procedure have little inﬂuence on classiﬁcation accuracy in
ensemble learning.
Weights
initialization
const
const
random
const
random
const
random
random

Instance accuracy
mean
92.34%
92.48%
92.50%
92.51%
92.20%
92.47%
92.49%
92.65%

Instance accuracy
increase
0.85%
0.63%
0.65%
0.56%
0.79%
0.88%
0.66%
0.92%

Instance accuracy
ensemble
93.19%
93.11%
93.15%
93.07%
92.99%
93.35%
93.15%
93.57%

Class accuracy
ensemble
90.96%
90.36%
90.58%
90.40%
90.25%
90.51%
90.65%
90.87%

Class accuracy
mean
89.58%
89.89%
90.10%
89.88%
89.62%
89.78%
89.84%
89.98%

Class accuracy
increase
1.38%
0.48%
0.48%
0.52%
0.63%
0.73%
0.81%
0.90%

Training
data
const
const
const
random
random
random
const
random

const
random
const
const
const
random
random
random

Dropout

12

A PREPRINT - MAY 24, 2019

Figure 8: Comparison of the time of inference on the Jetson TX2 platform (on the bottom) and the number of parameters
(on the top) for each tested model. DeepSets has the smallest number of parameters and is signiﬁcantly faster than that
of the other approaches.

Note that conﬁguration v1 (based on PointNet) often outperforms v2 (based on PointNet++) when ensemble is not used,
whereas with ensemble of 3 model instances, v2 is better. The performance gain is also higher than that for ModelNet40
classiﬁcation, which suggests that training set size is smaller concerning the complexity of the task. Averaging outputs
of all modules outperforms averaging of only last module output.

3.7 Comparison of Computational Runtime

We benchmark the speed of all the architectures on the Jetson TX2 platform. We chose a mini-batch of four point clouds
because it seemed to be a reasonable amount of segmented objects visible in a typical mobile robot environment. One
has to keep in mind that we have not performed any target-speciﬁc optimization. All approaches used (NVIDIA CUDA)
acceleration and three different deep learning frameworks, based on the original authors’ implementations of these
methods.

Inference time of deep neural networks depends on many aspects including the number of parameters, depth, number
of operations, target-speciﬁc optimizations and more. In-depth implementation analysis of considered architectures
is beyond the scope of this article, as we want to give a general view on their performance. As depicted in Figure 8,
DeepSets has the smallest number of parameters and is signiﬁcantly faster than that of the other approaches, on the
other hand PointNet has a considerable amount of parameters but is also pretty fast. The results could be explained by
the design and types of operations, where DeepSets and PointNet has simple MLP-like structures thus are the fastest,

Table 6: Frustum PointNet ensemble results for object classes.

Pedestrian Cyclist

Model
2D proposal
Accuracy ground
v1, no ensemble
v1, last module ensemble
v1, all modules ensemble
v2, no ensemble
v2, last module ensemble
v2, all modules ensemble
Accuracy 3D
v1, no ensemble
v1, last module ensemble
v1, all modules ensemble
v2, no ensemble
v2, last module ensemble
v2, all modules ensemble

Car
91.5

81.7
82.6
82.9
82.3
83.3
83.6

72.4
74.1
74.4
72.0
73.6
74.0

13

77.8

62.6
64.4
65.9
63.3
65.7
66.5

58.1
59.4
60.1
56.5
59.6
60.5

77.4

63.8
65.5
66.9
65.0
67.8
68.3

58.9
61.5
62.9
60.8
63.7
65.0

A PREPRINT - MAY 24, 2019

KCNet is also pretty fast probably because of small number of parameters, PointNet++ and SO-Net are slower probably
because of hierarchical design, and DGCNN and PointCNN are the slowest since they have graph-based structures,
which are expensive to build and convolve over. In the end, it is worth pointing out that an increase in a few percentages
of classiﬁcation accuracy (i.e., SO-Net or PointCNN) is occupied by signiﬁcantly longer execution times.

4 Conclusion

In this article, we focus on the examination of ensemble learning on 3D point cloud classiﬁcation with seven most
popular architectures using raw point sets. We examine the possibility to leverage the classiﬁcation accuracy of
each of the seven cited models by ensemble learning. First, we observe which voting policy is the best for the task.
Second, we found slightly better classiﬁcation accuracy with the increasing number of models in ensemble along with
smaller standard deviation. It proves that the ensemble’s output is more stable and reliable. The biggest mean instance
classiﬁcation accuracy gain was observed for KCNet — (2.52%), SO-Net — (0.99%), and other for architectures —
(0.50%) on average. Signiﬁcant increase in classiﬁcation accuracy achieved by KCNet in comparison with all other
architectures could be caused by different underlying working principle of kernel correlation as a measure of neuron
activation. Or could suggest there is some more space for hyperparameters tweaks in KCNet, e.g. the number of ﬁlters
(sets of kernels).

One can see some interesting observations due to the comparative study of different models. For example, there are
classes where the simplest (global) approaches (PointNet, DeepSets) present the best classiﬁcation accuracy. This
suggests that the general shape of some objects is more important than the shape of local structures, which could
mislead more complex models.

We also show that the ensemble models with different architectures can further leverage the overall accuracy. We found
that the SO-Net got the highest instance and mean class classiﬁcation accuracy, but PointCNN wins in the number of
classes in which given network obtained the highest accuracy after ensemble (this score is also high for DGCNN).
This suggests that the latter can be much better in classifying some classes and much worse in other ones. This could
also explain why the ensemble of only two model instances with different architectures lead to approximately 1.% of
instance classiﬁcation accuracy increase compared to the state of the art results. This increase is equal to one obtained
while using 10 instances of SO-Net model. Further gain can be achieved while using multiple model instances for each
of mixed architectures. Instance accuracy of 94.03% can be obtained for two architectures and 94.15% while combining
three or four architectures.

The performance gain is even higher for Frustum PointNet’ architecture evaluated on real-world KITTI dataset, using
only three model instances.

We tested the source of randomness in ensemble learning analysis for SO-Net. We observed that numerical issues
of massively parallel computations in deep neural networks are essential and beneﬁcial in ensemble learning. The
ensemble of several classiﬁers with the same encoder does not result in a signiﬁcant performance gain. Simple ensemble
outperforms classic bagging for tested approaches.

Table 7: Frustum PointNet ensemble results for difﬁculty levels.

Easy Medium Hard All
82.2
90.0

79.8

76.9

Model
2D proposal
Accuracy ground
v1, no ensemble
v1, last module ensemble
v1, all modules ensemble
v2, no ensemble
v2, last module ensemble
v2, all modules ensemble
Accuracy 3D
v1, no ensemble
v1, last module ensemble
v1, all modules ensemble
v2, no ensemble
v2, last module ensemble
v2, all modules ensemble

67.6
69.0
70.6
68.9
71.2
71.8

60.5
62.4
63.2
60.8
63.2
64.0

61.5
63.2
63.9
63.0
64.8
65.3

54.5
56.1
56.9
54.9
57.2
58.2

69.4
70.8
71.9
70.2
72.3
72.8

63.1
65.0
65.8
63.1
65.6
66.5

79.1
80.3
81.3
78.8
80.8
81.3

74.3
76.4
77.3
73.5
76.6
77.5

14

A PREPRINT - MAY 24, 2019

In addition we provide some tips for implementing point cloud classiﬁcation into a mobile robot equipped with the
Jetson TX2 platform by comparing inference time for all the tested models.

There are more questions one can ask around the topic of ensemble learning for point cloud processing. In our
opinion, results of this study could leverage the beneﬁt of knowledge distillation in real-world 3D object detection for
autonomous cars and mobile robots.

This research was partially supported by the Dean of Faculty of Mechatronics (Grant No. 504/03731 and Grant No.
504/03272). We want to thank authors of all architectures for providing a public repository. We would also like to
gratefully acknowledge the helpful comments and suggestions of Tomasz Trzci´nski and Robert Sitnik.

Acknowledgements

References

[1] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets Robotics: The KITTI Dataset.

International Journal of Robotics Research (IJRR), 2013.

[2] Riyad A El-laithy, Jidong Huang, and Michael Yeh. Study on the use of Microsoft Kinect for robotics applications.
In Position Location and Navigation Symposium (PLANS), 2012 IEEE/ION, pages 1280–1288. IEEE, 2012.
[3] Michael Himmelsbach, Thorsten Luettel, and H-J Wuensche. Real-time object classiﬁcation in 3D point clouds
using point feature histograms. In Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International
Conference on, pages 994–1000. IEEE, 2009.

[4] Martin Rutzinger, Bernhard Höﬂe, Markus Hollaus, and Norbert Pfeifer. Object-Based Point Cloud Analysis of
Full-Waveform Airborne Laser Scanning Data for Urban Vegetation Classiﬁcation. Sensors, 8(8):4505–4528,
2008.

[5] Daniel Munoz, Nicolas Vandapel, Martial Hebert, et al. Directional Associative Markov Network for 3-D Point
Cloud Classiﬁcation. In Fourth international symposium on 3D data processing, visualization and transmission,
pages 65–72. Georgia Institute of Technology Atlanta, US, 2008.

[6] Daniel Maturana and Sebastian Scherer. VoxNet: A 3D Convolutional Neural Network for real-time object
recognition. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages
922–928. IEEE, 2015.

[7] Konstantinos Sﬁkas, Ioannis Pratikakis, and Theoharis Theoharis. Ensemble of PANORAMA-based convolutional

neural networks for 3D model classiﬁcation and retrieval. Computers & Graphics, 71:208–218, 2018.

[8] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. PointNet: Deep Learning on Point Sets for 3D
Classiﬁcation and Segmentation. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 1(2):4, 2017.
[9] Shuran Song and Jianxiong Xiao. Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 808–816, 2016.
[10] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep Learning, volume 1. MIT press

Cambridge, 2016.

[11] Zhirong Wu, S. Song, A. Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and J. Xiao. 3D ShapeNets: A deep
representation for volumetric shapes. In 2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 1912–1920, June 2015.

[12] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view Convolutional Neural
Networks for 3D Shape Recognition. In Proceedings of the IEEE international conference on computer vision,
pages 945–953, 2015.

[13] Kripasindhu Sarkar, Basavaraj Hampiholi, Kiran Varanasi, and Didier Stricker. Learning 3D Shapes as Multi-

Layered Height-maps using 2D Convolutional Networks. arXiv preprint arXiv:1807.08485, 2018.

[14] Barnabas Poczos, Aarti Singh, Alessandro Rinaldo, and Larry Wasserman. Distribution-free distribution regression.
In Carlos M. Carvalho and Pradeep Ravikumar, editors, Proceedings of the Sixteenth International Conference on
Artiﬁcial Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pages 507–515,
Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR.

[15] In-Soo Jung, Mario Berges, James H. Garrett, and Barnabas Poczos. Exploration and evaluation of AR, MPCA
and KL anomaly detection techniques to embankment dam piezometer data. Advanced Engineering Informatics,
29(4):902 – 917, 2015.

15

A PREPRINT - MAY 24, 2019

[16] M. Ntampaka, H. Trac, D. J. Sutherland, S. Fromenteau, B. Póczos, and J. Schneider. Dynamical mass mea-
surements of contaminated galaxy clusters using machine learning. The Astrophysical Journal, 831(2):135,
2016.

[17] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. PointNet++: Deep Hierarchical Feature Learning
on Point Sets in a Metric Space. In Advances in Neural Information Processing Systems, pages 5099–5108, 2017.
[18] Jiaxin Li, Ben M Chen, and Gim Hee Lee. SO-Net: Self-Organizing Network for Point Cloud Analysis. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9397–9406, 2018.
[19] Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian. Mining Point Cloud Local Structures by Kernel Correlation
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,

and Graph Pooling.
volume 4, 2018.

[20] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J

Smola. Deep Sets. In Advances in Neural Information Processing Systems, pages 3391–3401, 2017.

[21] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic

Graph CNN for Learning on Point Clouds. arXiv preprint arXiv:1801.07829, 2018.

[22] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. PointCNN: Convolution On
X-Transformed Points. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
editors, Advances in Neural Information Processing Systems 31, pages 828–838. Curran Associates, Inc., 2018.
[23] Yuanchao Gan, Yan Tang, and Qingchen Zhang. 3D Model Retrieval Method Based on Mesh Segmentation.

Proceedings of SPIE - The International Society for Optical Engineering, 8334:120–, 04 2012.

[24] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D
Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University —
Toyota Technological Institute at Chicago, 2015.

[25] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. SUN RGB-D: A RGB-D Scene Understanding
Benchmark Suite. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 567–576.
IEEE, 2015.

[26] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet:

Richly-annotated 3d reconstructions of indoor scenes, 2017. cite arxiv:1702.04405.

[27] David Opitz and Richard Maclin. Popular Ensemble Methods: An Empirical Study. Journal of artiﬁcial

[28] Shijie Lin, Jinwang Wang, Wen Yang, and Gui-Song Xia. Toward Autonomous Rotation-Aware Unmanned Aerial

[29] Jie Tang, Yong Ren, and Shaoshan Liu. Real-Time Robot Localization, Vision, and Speech Recognition on Nvidia

[30] Jong-Chyi Su, Matheus Gadelha, Rui Wang, and Subhransu Maji. A Deeper Look at 3D Shape Classiﬁers. CoRR,

intelligence research, 11:169–198, 1999.

Grasping. CoRR, abs/1811.03921, 2018.

Jetson TX1. CoRR, abs/1705.10945, 2017.

abs/1809.02560, 2018.

[31] Varun Arvind, Anthony Costa, Marcus Badgeley, Samuel Cho, and Eric Oermann. Wide and deep volumetric

residual networks for volumetric image classiﬁcation. arXiv preprint arXiv:1710.01217, 2017.

[32] Charles Ruizhongtai Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas Guibas. Frustum PointNets for 3D Object

Detection from RGB-D Data. pages 918–927, 06 2018.

[33] Lior Rokach. Ensemble-based classiﬁers. Artiﬁcial Intelligence Review, 33(1-2):1–39, 2010.

16

