0
2
0
2
 
r
a

M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
1
v
8
9
8
3
1
.
3
0
0
2
:
v
i
X
r
a

Edge Guided GANs with Semantic Preserving
for Semantic Image Synthesis

Hao Tang1, Xiaojuan Qi2, Dan Xu2, Philip H. S. Torr2, and Nicu Sebe1

1University of Trento, Italy

2University of Oxford, UK

Abstract. We propose a novel Edge guided Generative Adversarial Net-
work (EdgeGAN) for photo-realistic image synthesis from semantic lay-
outs. Although considerable improvement has been achieved, the quality
of synthesized images is far from satisfactory due to two largely un-
resolved challenges. First, the semantic labels do not provide detailed
structural information, making it diﬃcult to synthesize local details and
structures. Second, the widely adopted CNN operations such as con-
volution, down-sampling and normalization usually cause spatial reso-
lution loss and thus are unable to fully preserve the original semantic
information, leading to semantically inconsistent results (e.g., missing
small objects). To tackle the ﬁrst challenge, we propose to use edge as
an intermediate representation which is further adopted to guide im-
age generation via a proposed attention guided edge transfer module.
Edge information is produced by a convolutional generator and intro-
duces detailed structure information. Further, to preserve the seman-
tic information, we design an eﬀective module to selectively highlight
class-dependent feature maps according to the original semantic layout.
Extensive experiments on two challenging datasets show that the pro-
posed EdgeGAN can generate signiﬁcantly better results than state-of-
the-art methods. The source code and trained models are available at
https://github.com/Ha0Tang/EdgeGAN.

Keywords: Generative Adversarial Networks (GANs), Semantic Image
Synthesis, Edge Guided, Attention Guided, Semantic Preserving

1 Introduction

Semantic image synthesis refers to the task of generating photo-realistic images
conditioned on pixel-level semantic labels. This task has a wide range of appli-
cations such as image editing and content generation [9,27,45,20,4,5]. Although
existing approaches such as [9,27,45,20,38,46] conducted interesting explorations,
we still observe unsatisfactory aspects mainly in the generated local structures
and small-scale objects, which we believe are mainly due to two reasons. First,
conventional methods [45,59,38] generally take the semantic label map as in-
put directly. However, the input label map provides only structural information
between diﬀerent semantic-class regions and does not contain any structural in-
formation within each semantic-class region, making it diﬃcult for synthesizing
rich local structures within each class. Take Fig. 1 (Label map: S) as an ex-
ample, the generator does not have enough structural guidance to produce a

2

Hao Tang et al.

Fig. 1: Overview of the proposed EdgeGAN framework. It consists of a
parameter-sharing encoder E, an edge generator Ge, an image generator Gi, and
a multi-modality discriminator D. The edge and image generators are connected
by the proposed attention guided edge transfer module Gt from two levels, i.e.,
edge feature-level and edge content-level, in order to generate realistic images.
The semantic preserving module Gs is proposed to preserve the semantic infor-
mation of the input semantic labels. The discriminator D aims to distinguish
the outputs from two modalities, i.e., edge and image. The whole framework can
be end-to-end trained. The symbol c○ denotes channel-wise concatenation.

realistic bed, window and curtain from only the input label (S). Second, the
classic deep network architectures are constructed by stacking convolutional,
down-sampling, normalization, non-linearity and up-sampling layers, which will
cause the problem of spatial resolution losses of the input semantic labels.

To address both issues, in this paper, we propose a novel Edge guided Gener-
ative Adversarial Network (EdgeGAN) for semantic image synthesis tasks. The
overall framework of the proposed EdgeGAN is shown in Fig. 1. We ﬁrst propose
an edge generator to produce the edge features and edge maps, and then the
generated edge features and edge maps are selectively transferred to the image
generator for improving the quality of the image results by using the proposed
attention guided edge transfer module. Moreover, to tackle the issue of the spa-
tial resolution losses caused by the common operations in the deep networks, we
propose an eﬀective semantic preserving module, which aims at selectively high-
lighting class-dependent feature maps according to the original semantic layout.
Finally, we develop a multi-modality discriminator to simultaneously distinguish
the output from two modal spaces, i.e., the edge and the image space. All the
proposed modules are jointly optimized in an end-to-end fashion so that each
module can beneﬁt from each other in the training.

We conduct extensive experiments on two challenging datasets, i.e., Cityscapes
[13] and ADE20K [67]. Both qualitative and quantitative results show that the
proposed EdgeGAN is able to produce remarkably better results than existing
baseline models such as CRN [9], Pix2pixHD [59], SIMS [46], GauGAN [45] and
CC-FPSE [38], regarding both the visual ﬁdelity and the alignment with the
input semantic labels.

EdgeGAN

3

To summarize, the contributions of this paper are as follows:

– We propose a novel Edge Guided GAN (EdgeGAN) for challenging semantic
image synthesis tasks. To the best of our knowledge, we are the ﬁrst to explore
the edge generation from semantic layouts and then utilize the generated edges
to guide the generation of realistic images.

– We propose an eﬀective attention guided edge transfer module to selectively
transfer useful edge structure information from the edge generation branch to
the image generation branch. We also design a new semantic preserving mod-
ule to highlight class-dependent feature maps based on the input semantic
label map for generating semantically consistent results, which is not investi-
gated by any existing GAN-based generation works.

– Extensive experiments clearly demonstrate the eﬀectiveness of the proposed
diﬀerent modules and the EdgeGAN framework, and establish new state-of-
the-art results on two challenging datasets, i.e., Cityscapes [13] and ADE20K
[67]. The code will be made publicly available.

2 Related Work

Generative Adversarial Networks (GANs) [19] have two important com-
ponents, i.e., a generator and a discriminator. Both are trained in an adversarial
way to achieve a balance. Recently, GANs have shown the capability of generat-
ing realistic images [65,6,30,49,26,21,36,16,51,37,48,29,12,18,28,50,57]. Moreover,
to generate user-speciﬁc images, Conditional GANs (CGANs) [40] have been pro-
posed. CGANs usually combine a vanilla GAN and some external information
such as class labels [11,60], human poses [14,69,52,8], conditional images [27,56],
text descriptions [34,64] and segmentation maps [59,45,58,55,20,2,44].
Image-to-Image Translation aims to generate the target image based on
an input image. CGANs have achieved decent results in both paired [27,1,54]
and unpaired [17,68] image translation tasks. For instance, Isola et al. propose
Pix2pix [27], which employs a CGAN to learn a translation mapping from input
to output image domains, such as map-to-photo and day-to-night. To further im-
prove the quality of the generated images, the attention mechanism has been re-
cently investigated in image-to-image translation tasks, such as [55,31,39,10,62].
Diﬀerent from previous attention-related image generation works, we propose
a novel attention guided edge transfer module to transfer useful edge structure
information from the edge generation branch to the image generation branch
at two diﬀerent levels, i.e., feature level and content level. To the best of our
knowledge, our module is the ﬁrst attempt to incorporate both edge feature
attention and edge content attention within a GAN framework for image-to-
image translation tasks.
Edge Guided Image Generation. Edge maps are usually adopted in image
inpainting [47,42,35] and image super-resolution [43] tasks to reconstruct the
missing structure information of the inputs. For example, Nazeri et al. [42] pro-
pose an edge generator to hallucinate edges in the missing regions given edges,
which can be regarded as an edge completion problem. Using edge images as the
structural guidance, [42] achieves good results even for some highly structured

4

Hao Tang et al.

scenes. Moreover, Ghosh et al. [15] propose an interactive GAN-based sketch-
to-image translation model that can help users easily create images of some
simple objects. Pix2pix [27] adopts edge maps as input and aims to generate
photo-realistic shoe and handbag images, which can be seen as an edge-to-image
translation problem.

Diﬀerent from previous works, we propose a novel edge generator to perform
a new task, i.e., semantic label-to-edge translation. To the best of our knowledge,
we are the ﬁrst time to generate realistic edge maps from semantic labels. Then
the generated edge maps, which with more local structure information, can be
used to improve the quality of the image results.
Semantic Image Synthesis aims to generate a photo-realistic image from a
semantic label map [59,9,46,45,38,3]. With the semantic information as guid-
ance, existing methods have achieved promising performance. However, we can
still observe unsatisfying aspects, especially on the generation of the small-scale
objects, which we believe is mainly due to the problem of spatial resolution losses
associated with deep network operations such as convolution, normalization and
down-sampling, etc.

To solve this problem, Park et al. propose GauGAN [45], which uses the input
semantic labels to modulate the activations in normalization layers through a
spatially-adaptive transformation. However, the spatial resolution losses caused
by other operations such as convolution and down-sampling have not been re-
solved. Moreover, we observe that the input label map has only a few semantic
classes in the entire dataset. Thus the generator should focus more on learning
these existing semantic classes rather than all the semantic classes.

To tackle both limitations, we propose a novel semantic preserving module,
which aims to selectively highlight class-dependent feature maps according to
the input label for generating semantically consistent image. This idea is not
investigated by existing GAN-based generation works.

3 Edge Guided GANs with Semantic Preserving

In this section, we describe the proposed Edge Guided GAN (EdgeGAN) for
semantic image synthesis. We ﬁrst introduce an overview of the proposed Edge-
GAN, and then introduce the details of each module. Finally, we present the
optimization objective.
Framework Overview. Fig. 1 shows the overall structure of the proposed Edge-
GAN for semantic image synthesis, which consists of a semantic and edge guided
generator G and a multi-modality discriminator D. The generator G consists of
ﬁve components: 1) a parameter-sharing convolutional encoder E is proposed to
produce deep feature maps F ; 2) an edge generator Ge is adopted to generate
edge maps I
e taking as input deep features from the encoder; 3) an image gener-
ator Gi is used to produce intermediate images I
; 4) an attention guided edge
transfer module Gt is designed to forward useful structure information from the
edge generator to the image generator; and 5) the semantic preserving module
Gs is developed to selectively highlight class-dependent feature maps according
to the input label for generating semantically consistent images I
. Meanwhile,

(cid:48)(cid:48)

(cid:48)

(cid:48)

EdgeGAN

5

to eﬀectively train the network, we propose a multi-modality discriminator D
that can simultaneously distinguish the outputs from two modalities, i.e., edge
and image space.

EdgeGAN takes a semantic layout as input and outputs a semantically cor-
respondent photo-realistic image. During training, the ground truth edge map
is extracted from corresponding ground truth images with Canny Edge Detec-
tor [7].

3.1 Edge Guided Semantic Image Synthesis
Parameter-Sharing Encoder. The backbone encoder E could employ any
network structures, such as the commonly used AlexNet [33], VGG [53] and
ResNet [22]. We directly utilize the feature maps from the last convolutional
layer as deep feature representations F =E(S), where E represents the encoder,
S∈RN ×H×W is the input label, H and W are width and height of the input
semantic labels, and N is the total number of semantic classes. Optionally, one
can always combine multiple intermediate feature maps to enhance the feature
representation.

The encoder is shared by the edge generator and the image generator. Then,
the gradients from the two generators all contribute to updating the parameters
of the encoder. This compact design can potentially enhance the deep repre-
sentations since the encoder can simultaneously learn structure (from the edge
generation branch) and appearance (from the image generation branch) repre-
sentations.
Edge Guided Image Generation. As discussed, the lack of detailed structure
or geometry guidance makes it extremely diﬃcult for the generator to produce
realistic local structures and details. To overcome this limitation, we propose to
adopt the edge as guidance. A novel edge generator Ge is designed to directly
generate the edge maps from the input semantic labels. This also facilitates the
shared encoder to learn more local structures of the targeted images. Meanwhile,
the image generator Gi aims to generate photo-realistic images from the input
labels. In this way, the encoder is promoted to learn the appearance information
of the targeted images.

Previous works [45,38,46,9,59] directly use deep networks to output the tar-
get image, which is challenging since the network need to simultaneously learn
appearance and structure information from the input labels. In contrast, our
EdgeGAN separately learns structure and appearance via the proposed edge
generator and image generator. Moreover, the explicit guidance from ground
truth edge maps can also beneﬁt training the encoder.

The framework of the proposed edge and image generators are illustrated in
Fig. 2. Given the feature maps from the last convolutional layer of the encoder,
i.e., F ∈RC×H×W , where H and W are width and height of the features, and C
is the number of channels, the edge generator produces edge features and edge
maps which are further utilized to guide the image generator to produce the
intermediate image I

.

(cid:48)

The edge generator Ge contains n convolution layers and correspondingly
j=1. After that, another con-

produces n intermediate feature maps Fe={F j

e }n

6

Hao Tang et al.

Fig. 2: Structure of the proposed edge generator Ge (ﬁrst row ), the proposed
attention guided edge transfer module Gt (middle row ) and the proposed image
generator Gi (third row ). The edge generator Ge selectively transfers useful local
structure information to the image generator Gi using the proposed attention
guided transfer module Gt. The symbols ⊕, ⊗ and σ○ denote element-wise addi-
tion, element-wise multiplication and Sigmoid activation function, respectively.

(cid:48)

(cid:48)

i }n

volution layer with Tanh() non-linear activation is utilized to generate the edge
e∈R3×H×W . Meanwhile, the feature maps F is also fed into the image gen-
map I
erator Gi to generate n intermediate feature maps Fi={F j
j=1. Then another
convolution operation with Tanh() non-linear activation is adopted to produce
i ∈R3×H×W . In addition, the intermediate edge feature
the intermediate image I
maps Fe and the edge map I
e are utilized to guide the generation of the image
feature maps Fi and the intermediate image I
via the Attention Guided Edge
Transfer as detailed below.
Attention Guided Edge Transfer. We further propose a novel attention
guided edge transfer module Gt to explicitly employ the edge structure infor-
mation to reﬁne the intermediate image representations. The architecture of the
proposed transfer module Gt is illustrated in Fig. 2.

(cid:48)

(cid:48)

i }n

To transfer useful structure information from edge feature maps Fe={F j

j=1
to the image feature maps Fi={F j
j=1, the edge feature maps are ﬁrstly pro-
cessed by Sigmoid() activation function to generate the corresponding atten-
tion maps Fa=Sigmoid(Fe)={F j
j=1. Then, we multiply the generated atten-
tion maps with the corresponding image feature maps to obtain the reﬁned maps
which incorporate local structures and details as Eq. (1). Finally, the edge reﬁned
features are element-wisely summed with the original image features to produce
the ﬁnal edge reﬁned features, which are further fed to the next convolution
layer.

e }n

a }n

F j
i = Sigmoid(F j

e ) × F j

i + F j
i ,

for j = 1, · · · , n

(1)

EdgeGAN

7

Fig. 3: Overview of the proposed semantic preserving module Gs, which aims
to capture the semantic information and predict scaling factors that are condi-
tional on the combined feature maps F. These learned factors selectively high-
light class-dependent feature maps, which are visualized in diﬀerent colors. The
symbols ⊕, ⊗ and σ○ denote element-wise addition, element-wise multiplication
and Sigmoid activation function, respectively.

By this way, the image feature maps also contain the local structure information
provided by the edge feature maps.

Similarly, to directly employ the structure information from the generated
e for image generation, we adopt the attention guided edge transfer
edge map I
module to reﬁne the generated image directly with edge information as Eq. (2).

(cid:48)

(cid:48)

I

= Sigmoid(I

(cid:48)

(cid:48)

(cid:48)

e) × I

i + I

i .

(2)

3.2 Semantic Preserving Image Enhancement
Due to the spatial resolution loss caused by convolution, normalization and
down-sampling layers, existing models [59,45,46,9] could not be able to fully
preserve the semantic information of the input labels as illustrated in Fig. 8,
e.g., the small ‘pole is missing and the large ‘fence is incomplete. To ﬁx this
problem, we propose a novel semantic preserving module, which aims to select
class-dependent feature maps and further enhance it guided by the original se-
mantic layout. An overview of the proposed semantic preserving module Gs is
shown in Fig. 3.

Speciﬁcally, the input denoted as F to the module is the concatenation of
, and

the input label S, the generated intermediate edge map Ie and image I
the deep feature F produced from the shared encoder E.

(cid:48)

Then, we apply a convolution operation on F to produce a new feature map
Fc with channel number equal to the number of semantic categories, where each
channel corresponds to a speciﬁc semantic category. Next, we apply the averaging
pooling operation on Fc to obtain the global information of each class followed
by a Sigmoid() activation function to derive scaling factors γ
as Eq. (3), where
each value represents the importance of each class.

(cid:48)

= Sigmoid(AvgPool(Fc)),

(cid:48)

γ

(cid:48)

Then, the scaling factor γ
is adopted to reweight the feature map Fc and high-
light corresponding class-dependent feature maps as Eq. (4). The reweighted
feature map is further added with the original feature Fc to compensate infor-
mation loss due to multiplication, and produces F

c∈RN ×H×W ,

(cid:48)

(3)

(4)

(cid:48)

(cid:48)

F

c = Fc × γ

+ Fc.

8

Hao Tang et al.

(cid:48)

After that, we perform another convolution operation on F
c to obtain the fea-
∈R(C+N +3+3)×H×W to enhance the representative capability of the
ture map F
feature. In addition, F
has the same size as the original input one F, which
makes the module ﬂexible and can be plugged into other existing architectures
without modiﬁcation of other parts to reﬁne the output.

(cid:48)

(cid:48)

(cid:48)

Finally, the feature map F

is fed into a convolution layer followed by a
Tanh() non-linear activation layer to obtain the ﬁnal result I
. The proposed
semantic preserving module enhances the representational power of the model
by adaptively re-calibrating semantic class-dependent feature maps, and shares
similar spirits with style transfer [25], and recent works SENet [24] and EncNet
[66]. One intuitive example of the utility of the module is for the generation of
small object classes: the small object classes are easily missed in the generation
results due to spatial resolution loss while our scaling factor can put an emphasis
on small objects and help preserve them.

(cid:48)(cid:48)

3.3 Model Training

Multi-Modality Discriminator. To facilitate training the proposed Edge-
GAN for high-quality edge and image generation, a novel multi-modality dis-
criminator is developed to simultaneously distinguish outputs from two modality
spaces, i.e., edge and image. The framework of the proposed multi-modality dis-
criminator is shown in Fig. 1, which are capable of discriminating both real/fake
images and edges. To discriminate real/fake edges, the discriminator loss con-
sidering the semantic label S and the generated edge I
e (or the real edge Ie) is
as Eq. (5) which guide the model to distinguish real edges from fake generated
edges.

(cid:48)

LCGAN(Ge, D) = ES,Ie [log D(S, Ie)] + E

(cid:104)

S,I (cid:48)
e

log(1 − D(S, I

(5)

(cid:48)

(cid:105)
e))

,

Further, to discriminate real/fake images, the discriminator loss regarding se-
mantic label S and the generated images I
(or the real image I) is as
Eq. (6), which guide the model to discriminate real/fake images.

, I

(cid:48)(cid:48)

(cid:48)

LCGAN(Gi, Gs, D) = (λ + 1)ES,I [log D(S, I)]
(cid:105)

(cid:104)

(cid:48)

+ E

S,I (cid:48)

log(1 − D(S, I

))

(cid:104)

+ λE

S,I (cid:48)(cid:48)

log(1 − D(S, I

))

,

(cid:48)(cid:48)

(cid:105)

(6)
Therefore, the total loss of the proposed multi-modality discriminator D can be
written as LCGAN=LCGAN(Ge, D)+LCGAN(Gi, Gs, D).
Optimization Objective. Equipped with the multi-modality discriminator,
we elaborate the training objective for the generator as below. Three diﬀerent
losses, i.e., the conditional adversarial loss LCGAN , the discriminator feature
matching loss Lf and the perceptual loss Lp, are used to optimize the proposed

EdgeGAN

9

EdgeGAN,

min
G

max
D

L = λcLCGAN + λf (Lf (Ie, I

(cid:124)

e)+Lf (I, I
(cid:123)(cid:122)
Feature Matching Loss

)+λLf (I, I

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(7)

+ λp (Lp(Ie, I
(cid:124)

e)+Lp(I, I
(cid:123)(cid:122)
Perceptual Loss

)+λLp(I, I

(cid:48)(cid:48)

))
(cid:125)

(cid:48)(cid:48)

,
))
(cid:125)

where λc, λf and λp are three parameters of the corresponding loss that con-
tributes to the total loss L; where Lf matches the discriminator intermediate
features between the generated images/edges and the real images/edges; where
Lp matches the VGG [53] extracted features between the generated images/edges
and the real images/edges. By maximizing the discriminator loss, the generator
is promoted to simultaneously generate reasonable edge maps that can cap-
ture the local-aware structure information and generate photo-realistic images
semantically aligned with the input semantic labels.

4 Experiments
Implementation Details. We adopt ResNet [22] as the structure of our en-
coder E. Moreover, we employ the SPADE residual block [45] in our generator,
which has been shown very eﬀective in semantic image synthesis tasks. For both
the image generator Gi and edge generator Ge, the kernel size and padding size
of convolutions are all 3×3 and 1 for preserving the feature map size. We set
n=3 for generators Gi, Gs and Gt. The channel size of feature F is set to C=64.
For the semantic preserving module Gs, we adopt adaptive averaging pooling
operation. Spectral normalization [41] is applied to all the layers in both the
generator and discriminator.

We follow the training procedures of GANs and alternatively train the gen-
erator G and discriminator D, i.e., one gradient descent step on discriminator
and generator alternately. We ﬁrst train G with D ﬁxed, and then train D with
G ﬁxed. We use the Adam solver [32] and set β1=0, β2=0.999. λc, λf and λp in
Eq. (7) is set to 1, 10 and 10, respectively. All λ in both Eq. (6) and (7) are set
to 2. We conduct the experiments on an NVIDIA DGX1 with 8 V100 GPUs.
Datasets and Evaluation Metrics. We follow GauGAN [45] and conduct
experiments on two challenging datasets, i.e., Cityscapes [13] and ADE20K [67].
The size of the training and validation set on Cityscapes are 3,000 and 500,
respectively. For ADE20K, which contains challenging scenes with 150 semantic
classes, and consists of 20,210 training and 2,000 validation images. Moreover,
we adopt the mean Intersection-over-Union (mIoU), Pixel Accuracy (Acc) and
Fr´echet Inception Distance (FID) [23] as the evaluation metrics. All images on
Cityscapes and ADE20K are re-scaled to 512×256 and 256×256, respectively.
For both datasets, we preform 200 epochs of training with batch size 32, and the
learning rate is linearly decayed to 0 from epoch 100 to 200.

4.1 Comparisons with State-of-the-Art
Qualitative Comparisons. Qualitative results of Cityscapes and ADE20K
compared with existing methods, i.e., Pix2pixHD [59], CRN [9] and SIMS [46],

10

Hao Tang et al.

Fig. 4: Visual results generated by diﬀerent methods on Cityscapes (top) and
ADE20K (bottom).

Table 1: User preference study on Cityscapes and ADE20K. The numbers in-
dicate the percentage of users who favor the results of the proposed EdgeGAN
over the competing method. For this metric, higher is better.

AMT ↑

Cityscapes ADE20K

Ours vs. CRN [9]
Ours vs. Pix2pixHD [59]
Ours vs. SIMS [46]
Ours vs. GauGAN [45]
Ours vs. CC-FPSE [38]

70.28
60.85
57.67
56.54
55.81

81.35
85.18
N/A
60.49
57.75

are shown in Fig. 4. We can see that the proposed EdgeGAN achieves signiﬁ-
cantly better results with fewer visual artifacts than other baselines.

To further validate the eﬀectiveness of the proposed EdgeGAN, we compare
it with two stronger baselines, i.e., GauGAN [45] and CC-FPSE [38]. Note that
we download their well-trained models and generate the results for fair compar-
isons. Results compared with GauGAN are shown in Fig. 5. We observe that
the proposed EdgeGAN generates sharper images than GauGAN, especially at
local structures and details. Besides, as shown in Fig. 6, the proposed EdgeGAN
achieves signiﬁcantly better results than CC-FPSE, while CC-FPSE always gen-
erates results with lots of visual artifacts on both datasets.
User Study. We follow the evaluation protocol of GauGAN [45] and conduct a
user study. Speciﬁcally, we give the participants an input semantic label and two
translated images from diﬀerent models and ask them to choose the generated
image that looks more like a corresponding image of the semantic label. The
participants are given unlimited time to make the decision.

Results compared with Pix2pixHD [59], CRN [9], SIMS [46], GauGAN [45]
and CC-FPSE [38] are shown in Table 1. We observe that users favor our syn-
thesized results on both datasets compared with other competing methods in-
cluding GauGAN and CC-FPSE, further validating that the generated images
by the proposed EdgeGAN are more natural and photo-realistic.

EdgeGAN

11

Fig. 5: Visual results generated by GauGAN [45] and the proposed EdgeGAN
on Cityscapes (top) and ADE20K (bottom).

Table 2: Our method achieves very competitive results compared to the current
leading methods in semantic segmentation scores (mIoU and Acc) and FID. For
mIoU and Acc, higher is better. For FID, lower is better.

Method

Cityscapes

ADE20K

mIoU ↑ Acc ↑ FID ↓ mIoU ↑ Acc ↑ FID ↓

CRN [9]
SIMS [46]
Pix2pixHD [59]
GauGAN [45]
CC-FPSE [38]
EdgeGAN (Ours)

52.4
47.2
58.3
62.3
65.5
64.5

68.8

73.3
22.4
77.1 104.7
75.5 49.7 N/A N/A N/A
95.0
81.8
69.2
20.3
81.4
38.5
71.8
81.9
33.9
79.9
43.7 82.9 31.7
82.3
54.3
32.4
82.0
42.0
82.5 57.1

Quantitative Comparisons. Quantitative results of the mIoU, Acc and FID
metrics are shown in Table 2. It is clear that the proposed EdgeGAN outperforms
most existing leading methods by a large margin except CC-FPSE [38]. However,
CC-FPSE generates signiﬁcantly worse visual results than ours as shown in
Fig. 6. Moreover, we provide the number of model parameters in Table 3. We
see that the proposed EdgeGAN has much fewer model parameters than CC-
FPSE on both datasets, which means the proposed EdgeGAN can be trained
with less training time and GPU memory.

Visualization of Edge and Attention Maps. We also visualize the generated
edge and attention maps on both datasets in Fig. 7. We observe that the proposed
EdgeGAN can generate reasonable edge maps according to the input labels, thus
the generated edge maps can be used to provide more local structure information
for generating more photo-realistic images.

12

Hao Tang et al.

Fig. 6: Visual results generated by CC-FPSE [38] and the proposed EdgeGAN
on Cityscapes (top) and ADE20K (bottom).

Table 3: Comparison of the number of model parameters. ‘G’ and ‘D’ denote
Generator and Discriminator, respectively.

Method

Cityscapes

ADE20K

G

D

Total ↓

G

D

Total ↓

GauGAN [45]
CC-FPSE [38]
EdgeGAN (Ours) 93.2M 5.6M 98.8M (+0.2M)

93.0M 5.6M
138.6M 5.2M 143.8M (+45.2M) 151.2M 5.2M 156.4M (+54.1M)
97.2M 5.8M 103.0M (+0.7M)

96.5M 5.8M

102.3M

98.6M

Visualization of Segmentation Maps. We follow GauGAN [45] and apply
pre-trained segmentation networks [63,61] on the generated images to produce
segmentation maps. Results compared with GauGAN [45] are shown in Fig. 8.
We consistently observe that the proposed EdgeGAN generates better semantic
labels than GauGAN on both datasets.

4.2 Ablation Study
Variants of EdgeGAN. We conduct extensive ablation studies on Cityscapes
[13] to evaluate diﬀerent components of the proposed EdgeGAN. The proposed
EdgeGAN has four baselines as shown in Table 4: (i) ‘E+Gi’ means only us-
ing the encoder E and the proposed image generator Gi to synthesize the tar-
geted images; (ii) ‘E+Gi+Ge’ means adopting the proposed image generator Gi
and edge generator Ge to simultaneously produce both edge maps and images;
(iii) ‘E+Gi+Ge+Gt’ connects the image generator Gi and the edge genera-
tor Ge by using the proposed attention guided edge transfer module Gt; (iv)
‘E+Gi+Ge+Gt+Gs’ is our full model and employs the proposed semantic pre-
serving module Gs to further improve the quality of the ﬁnal results.

EdgeGAN

13

Fig. 7: Edge and attention maps generated by the proposed EdgeGAN on
Cityscapes (top) and ADE20K (bottom).

Table 4: Quantitative comparison of diﬀerent variants of the proposed EdgeGAN
on Cityscapes. For mIoU and Acc, higher is better. For FID, lower is better.

Variants of EdgeGAN mIoU ↑

Acc ↑

FID ↓

E+Gi
E+Gi+Ge
E+Gi+Ge+Gt
E+Gi+Ge+Gt+Gs
Total

58.6 (+0.0) 81.4 (+0.0) 65.7 (-0.0)
60.2 (+1.6) 81.7 (+0.3) 61.0 (-4.7)
61.5 (+1.3) 82.0 (+0.3) 59.0 (-2.0)
64.5 (+3.0) 82.5 (+0.5) 57.1 (-1.9)
(-8.6)

(+5.9)

(+1.1)

Eﬀect of Edge Guided Generation Strategy. The results of the ablation
study are shown in Table 4. When using the proposed edge generator Ge to
produce the corresponding edge map from the input label, performance on all
evaluation metrics is improved. Speciﬁcally, 1.6, 0.3 and 4.7 point gains on the
mIoU, Acc and FID metrics, respectively, which conﬁrms the eﬀectiveness of the
proposed edge guided generation strategy.

Eﬀect of Attention Guided Edge Transfer Module. We observe that the
implicitly learned edge structure information by the ‘E+Gi+Ge’ baseline is not
enough for such a challenging task. Thus we further adopt the proposed attention
guided edge transfer module Gt to transfer useful edge structure information
from the edge generation branch to the image generation branch. We observe
that 1.3, 0.3 and 2.0 point gains are obtained on the mIoU, Acc and FID metrics,
respectively. This means that the proposed transfer module Gt indeed learns
richer feature representations with more convincing structure cues and details,
and then transfers them from the edge generator Ge to the image generator Gi,
conﬁrming our design motivation.

Eﬀect of Semantic Preserving Module. By adding the proposed semantic
preserving module Gs, the overall performance is further boosted with 3.0, 0.5
and 1.9 point improvements on the mIoU, Acc and FID metrics, respectively.
This means the proposed semantic preserving module indeed learns and high-
lights class-speciﬁc semantic feature maps, leading to better generation results.

14

Hao Tang et al.

Fig. 8: Segmentation maps generated by GauGAN [45] and the proposed Edge-
GAN on Cityscapes (top) and ADE20K (bottom). ‘EdgeGAN I’ and ‘EdgeGAN
II’ stand for I

, respectively.

and I

(cid:48)(cid:48)

(cid:48)

Table 5: Performance before (‘EdgeGAN I’) and after (‘EdgeGAN II’) using the
proposed semantic preserving module. For mIoU and Acc, higher is better. For
FID, lower is better.

Stages of EdgeGAN

Cityscapes

ADE20K

mIoU ↑ Acc ↑ FID ↓ mIoU ↑ Acc ↑ FID ↓

EdgeGAN I
EdgeGAN II

61.7
59.1
82.1
64.5 82.5 57.1

39.6
34.2
80.9
42.0 82.0 32.4

In Fig. 8, we show some samples of the generated semantic maps. We observe
that the semantic maps produced by the results after the proposed semantic pre-
serving module (i.e., ‘Label by EdgeGAN II’ in Fig. 8) are more accurate than
those without using the proposed semantic preserving module (‘Label by Edge-
GAN I’ in Fig. 8). Moreover, we provide quantitative results on both datasets in
Table 5. We can see that the proposed semantic preserving module indeed learns
better class-speciﬁc feature representation, leading better performance on both
datasets. Lastly, we also observe that our generated semantic maps are much
better than those generated by GauGAN [45]. Both quantitative and qualitative
results conﬁrm the eﬀectiveness of the proposed semantic preserving module.

5 Conclusions

We propose a novel Edge guided GAN (EdgeGAN) for challenging semantic
image synthesis tasks. EdgeGAN introduces three core components: edge guided
image generation strategy, attention guided edge transfer module and semantic
preserving module. The ﬁrst component is employed to generate edge maps from
input semantic labels. The second one is used to selectively transfer the useful
structure information from the edge generation branch to the image generation
branch. The third one is adopted to alleviate the problem of the spatial resolution
losses caused by diﬀerent operations in the deep networks. Extensive experiments
show that SEGAN achieves signiﬁcantly better results than existing methods.
Furthermore, we believe that the proposed modules can be easily plugged into
existing GAN architectures to address other generation tasks.

EdgeGAN

15

References

1. AlBahar, B., Huang, J.B.: Guided image-to-image translation with bi-directional

feature transformation. In: ICCV (2019) 3

2. Azadi, S., Tschannen, M., Tzeng, E., Gelly, S., Darrell, T., Lucic, M.: Semantic

bottleneck scene generation. arXiv preprint arXiv:1911.11357 (2019) 3

3. Bansal, A., Sheikh, Y., Ramanan, D.: Shapes and context: in-the-wild image syn-

thesis & manipulation. In: CVPR (2019) 4

4. Bau, D., Strobelt, H., Peebles, W., Wulﬀ, J., Zhou, B., Zhu, J.Y., Torralba, A.:
Semantic photo manipulation with a generative image prior. ACM TOG 38(4),
1–11 (2019) 1

5. Bau, D., Zhu, J.Y., Strobelt, H., Zhou, B., Tenenbaum, J.B., Freeman, W.T.,
Torralba, A.: Gan dissection: Visualizing and understanding generative adversarial
networks. In: ICLR (2019) 1

6. Brock, A., Donahue, J., Simonyan, K.: Large scale gan training for high ﬁdelity

natural image synthesis. In: ICLR (2019) 3

7. Canny, J.: A computational approach to edge detection. IEEE TPAMI (6), 679–698

8. Chan, C., Ginosar, S., Zhou, T., Efros, A.A.: Everybody dance now. In: ICCV

(1986) 5

(2019) 3

9. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded reﬁnement

networks. In: ICCV (2017) 1, 2, 4, 5, 7, 9, 10, 11

10. Chen, X., Xu, C., Yang, X., Tao, D.: Attention-gan for object transﬁguration in

wild images. In: ECCV (2018) 3

11. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image translation. In: CVPR
(2018) 3

12. Choi, Y., Uh, Y., Yoo, J., Ha, J.W.: Stargan v2: Diverse image synthesis for mul-

tiple domains. In: CVPR (2020) 3

13. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: CVPR (2016) 2, 3, 9, 12, 19

14. Esser, P., Sutter, E., Ommer, B.: A variational u-net for conditional appearance

and shape generation. In: CVPR (2018) 3

15. Ghosh, A., Zhang, R., Dokania, P.K., Wang, O., Efros, A.A., Torr, P.H., Shecht-
man, E.: Interactive sketch & ﬁll: Multiclass sketch-to-image translation. In: ICCV
(2019) 4

16. Goetschalckx, L., Andonian, A., Oliva, A., Isola, P.: Ganalyze: Toward visual def-

initions of cognitive image properties. In: ICCV (2019) 3

17. Gong, R., Li, W., Chen, Y., Gool, L.V.: Dlow: Domain ﬂow for adaptation and

generalization. In: CVPR (2019) 3

18. Gong, X., Chang, S., Jiang, Y., Wang, Z.: Autogan: Neural architecture search for

generative adversarial networks. In: ICCV (2019) 3

19. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014) 3
20. Gu, S., Bao, J., Yang, H., Chen, D., Wen, F., Yuan, L.: Mask-guided portrait

editing with conditional gans. In: CVPR (2019) 1, 3

21. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.: Improved

training of wasserstein gans. In: NeurIPS (2017) 3

16

Hao Tang et al.

22. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR (2016) 5, 9

23. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained
by a two time-scale update rule converge to a local nash equilibrium. In: NeurIPS
(2017) 9

24. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: CVPR (2018) 8
25. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance

normalization. In: ICCV (2017) 8

26. Huh, M., Sun, S.H., Zhang, N.: Feedback adversarial learning: Spatial feedback for

improving generative adversarial networks. In: CVPR (2019) 3

27. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-

tional adversarial networks. In: CVPR (2017) 1, 3, 4

28. Jahanian, A., Chai, L., Isola, P.: On the”steerability” of generative adversarial

networks. In: ICLR (2020) 3

29. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for im-

proved quality, stability, and variation. In: ICLR (2018) 3

30. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative

adversarial networks. In: CVPR (2019) 3

31. Kim, J., Kim, M., Kang, H., Lee, K.: U-gat-it: unsupervised generative attentional
networks with adaptive layer-instance normalization for image-to-image transla-
tion. In: ICLR (2020) 3

32. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR

33. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-

volutional neural networks. In: NeurIPS (2012) 5

34. Li, B., Qi, X., Lukasiewicz, T., Torr, P.: Controllable text-to-image generation. In:

(2015) 9

NeurIPS (2019) 3

35. Li, J., He, F., Zhang, L., Du, B., Tao, D.: Progressive reconstruction of visual

structure for image inpainting. In: ICCV (2019) 3

36. Lin, C.H., Chang, C.C., Chen, Y.S., Juan, D.C., Wei, W., Chen, H.T.: Coco-gan:

generation by parts via conditional coordinating. In: ICCV (2019) 3

37. Liu, M.Y., Huang, X., Mallya, A., Karras, T., Aila, T., Lehtinen, J., Kautz, J.:

Few-shot unsupervised image-to-image translation. In: ICCV (2019) 3

38. Liu, X., Yin, G., Shao, J., Wang, X., et al.: Learning to predict layout-to-image
conditional convolutions for semantic image synthesis. In: NeurIPS (2019) 1, 2, 4,
5, 10, 11, 12, 19, 20, 21, 22, 23

39. Mejjati, Y.A., Richardt, C., Tompkin, J., Cosker, D., Kim, K.I.: Unsupervised

attention-guided image-to-image translation. In: NeurIPS (2018) 3

40. Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint

arXiv:1411.1784 (2014) 3

41. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for

generative adversarial networks. In: ICLR (2018) 9

42. Nazeri, K., Ng, E., Joseph, T., Qureshi, F., Ebrahimi, M.: Edgeconnect: Structure
guided image inpainting using edge prediction. In: ICCV Workshops (2019) 3
43. Nazeri, K., Thasarathan, H., Ebrahimi, M.: Edge-informed single image super-

resolution. In: ICCV Workshops (2019) 3

44. Pan, J., Wang, C., Jia, X., Shao, J., Sheng, L., Yan, J., Wang, X.: Video generation

from single semantic label map. In: CVPR (2019) 3

45. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with
spatially-adaptive normalization. In: CVPR (2019) 1, 2, 3, 4, 5, 7, 9, 10, 11,
12, 14, 19, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40

EdgeGAN

17

46. Qi, X., Chen, Q., Jia, J., Koltun, V.: Semi-parametric image synthesis. In: CVPR

(2018) 1, 2, 4, 5, 7, 9, 10, 11

47. Ren, Y., Yu, X., Zhang, R., Li, T.H., Liu, S., Li, G.: Structureﬂow: Image inpainting

via structure-aware appearance ﬂow. In: ICCV (2019) 3

48. Shaham, T.R., Dekel, T., Michaeli, T.: Singan: Learning a generative model from

a single natural image. In: ICCV (2019) 3

49. Shama, F., Mechrez, R., Shoshan, A., Zelnik-Manor, L.: Adversarial feedback loop.

In: ICCV (2019) 3

50. Shocher, A., Bagon, S., Isola, P., Irani, M.: Ingan: Capturing and remapping the”

dna” of a natural image. In: ICCV (2019) 3

51. Shocher, A., Bagon, S., Isola, P., Irani, M.: Ingan: Capturing and remapping the

dna of a natural image. In: ICCV (2019) 3

52. Siarohin, A., Sangineto, E., Lathuili`ere, S., Sebe, N.: Deformable gans for pose-

based human image generation. In: CVPR (2018) 3

53. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR (2015) 5, 9

54. Tang, H., Wang, W., Xu, D., Yan, Y., Sebe, N.: Gesturegan for hand gesture-to-

gesture translation in the wild. In: ACM MM (2018) 3

55. Tang, H., Xu, D., Sebe, N., Wang, Y., Corso, J.J., Yan, Y.: Multi-channel attention
selection gan with cascaded semantic guidance for cross-view image translation. In:
CVPR (2019) 3

56. Wang, M., Yang, G.Y., Li, R., Liang, R.Z., Zhang, S.H., Hall, P.M., Hu, S.M.:
Example-guided style-consistent image synthesis from semantic labeling. In: CVPR
(2019) 3

57. Wang, S.Y., Wang, O., Zhang, R., Owens, A., Efros, A.A.: Cnn-generated images

are surprisingly easy to spot... for now. In: CVPR (2020) 3

58. Wang, T.C., Liu, M.Y., Tao, A., Liu, G., Kautz, J., Catanzaro, B.: Few-shot video-

to-video synthesis. In: NeurIPS (2019) 3

59. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-
resolution image synthesis and semantic manipulation with conditional gans. In:
CVPR (2018) 1, 2, 3, 4, 5, 7, 9, 10, 11

60. Wu, P.W., Lin, Y.J., Chang, C.H., Chang, E.Y., Liao, S.W.: Relgan: Multi-domain

image-to-image translation via relative attributes. In: ICCV (2019) 3

61. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Uniﬁed perceptual parsing for scene

understanding. In: ECCV (2018) 12, 19

62. Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan:
Fine-grained text to image generation with attentional generative adversarial net-
works. In: CVPR (2018) 3

63. Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. In: CVPR (2017)

12, 19

64. Yu, X., Chen, Y., Liu, S., Li, T., Li, G.: Multi-mapping image-to-image translation

via learning disentanglement. In: NeurIPS (2019) 3

65. Zhang, H., Goodfellow, I., Metaxas, D., Odena, A.: Self-attention generative ad-

versarial networks. In: ICML (2019) 3

66. Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A.: Context

encoding for semantic segmentation. In: CVPR (2018) 8

67. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing

through ade20k dataset. In: CVPR (2017) 2, 3, 9, 19

68. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation

using cycle-consistent adversarial networks. In: ICCV (2017) 3

18

Hao Tang et al.

69. Zhu, Z., Huang, T., Shi, B., Yu, M., Wang, B., Bai, X.: Progressive pose attention

transfer for person image generation. In: CVPR (2019) 3

EdgeGAN

19

This document provides additional experimental results on the semantic im-
age synthesis task. First, we compare the proposed EdgeGAN with the state-of-
the-art method CC-FPSE [38] (Sec. 6). Additionally, we visualize the edge maps
and attention maps generated by the proposed EdgGAN (Sec. 7). Lastly, we also
provide the visualization results of the generated segmentation maps (Sec. 8).

6 Comparisons with State-of-the-Art

We provide more generation results of the proposed EdgeGAN and CC-FPSE
[38] on both the Cityscapes [13] and ADE20K [67] datasets. Note that we gen-
erated the results of CC-FPSE [38] using the well-trained models provided by
the authors1 for fair comparisons.

Results are shown in Fig. 9, 10, 11 and 12. We observe that the proposed
EdgeGAN consistently achieves photo-realistic results with fewer visual artifacts
than CC-FPSE on both challenging datasets.

7 Visualization of Edge and Attention Maps

We visualize the generated edge and attention maps on both datasets in Fig. 13,
14, 15, 16, 17, 18 and 19. We observe that the proposed EdgeGAN can generate
good edge maps according to the input semantic labels, thus the generated edge
maps can be used to provide more local structure guidance for generating more
photo-realistic images. These visualization results further prove the eﬀectiveness
of the proposed edge guided image generation strategy.

8 Visualization of Segmentation Maps

We follow GauGAN [45] and use the state-of-the-art segmentation networks on
the generated images to produce the corresponding segmentation maps: DRN-
D-105 [63] for Cityscapes [13] and UperNet101 [61] for ADE20K [67].

In Fig. 20, 21, 22, 23, 24, 25, 26, 27, 28 and 29, we show samples of the
generated segmentation maps on both datasets. ‘EdgeGAN I’ and ‘EdgeGAN
II’ in these ﬁgures stand for I

, respectively.

and I

(cid:48)(cid:48)

(cid:48)

We observe that the segmentation maps produced by the results after the pro-
posed semantic preserving module (i.e., ‘Label by EdgeGAN II’ in these ﬁgures)
are more accurate than those without using the proposed semantic preserving
module (‘Label by EdgeGAN I’ in these ﬁgures), which further validates the
eﬀectiveness of the proposed semantic preserving module.

Moreover, we consistently observe in these ﬁgures that the proposed Edge-
GAN generates signiﬁcantly better segmentation maps than GauGAN [45], es-
pecially on local texture and small-scale objects.

1 https://github.com/xh-liu/CC-FPSE

20

Hao Tang et al.

Fig. 9: Visual results generated by CC-FPSE [38] and EdgeGAN on Cityscapes.

EdgeGAN

21

Fig. 10: Visual results generated by CC-FPSE [38] and EdgeGAN on Cityscapes.

22

Hao Tang et al.

Fig. 11: Visual results generated by CC-FPSE [38] and EdgeGAN on ADE20K.

EdgeGAN

23

Fig. 12: Visual results generated by CC-FPSE [38] and EdgeGAN on ADE20K.

24

Hao Tang et al.

Fig. 13: Edge and attention maps generated by EdgeGAN on Cityscapes.

EdgeGAN

25

Fig. 14: Edge and attention maps generated by EdgeGAN on Cityscapes.

26

Hao Tang et al.

Fig. 15: Edge and attention maps generated by EdgeGAN on ADE20K.

EdgeGAN

27

Fig. 16: Edge and attention maps generated by EdgeGAN on ADE20K.

28

Hao Tang et al.

Fig. 17: Edge and attention maps generated by EdgeGAN on ADE20K.

EdgeGAN

29

Fig. 18: Edge and attention maps generated by EdgeGAN on ADE20K.

30

Hao Tang et al.

Fig. 19: Edge and attention maps generated by EdgeGAN on ADE20K.

EdgeGAN

31

Fig. 20: Semantic maps generated by GauGAN [45] and EdgeGAN on Cityscapes.

32

Hao Tang et al.

Fig. 21: Semantic maps generated by GauGAN [45] and EdgeGAN on Cityscapes.

EdgeGAN

33

Fig. 22: Semantic maps generated by GauGAN [45] and EdgeGAN on Cityscapes.

34

Hao Tang et al.

Fig. 23: Semantic maps generated by GauGAN [45] and EdgeGAN on Cityscapes.

EdgeGAN

35

Fig. 24: Semantic maps generated by GauGAN [45] and EdgeGAN on Cityscapes.

36

Hao Tang et al.

Fig. 25: Semantic maps generated by GauGAN [45] and EdgeGAN on ADE20K.

EdgeGAN

37

Fig. 26: Semantic maps generated by GauGAN [45] and EdgeGAN on ADE20K.

38

Hao Tang et al.

Fig. 27: Semantic maps generated by GauGAN [45] and EdgeGAN on ADE20K.

EdgeGAN

39

Fig. 28: Semantic maps generated by GauGAN [45] and EdgeGAN on ADE20K.

40

Hao Tang et al.

Fig. 29: Semantic maps generated by GauGAN [45] and EdgeGAN on ADE20K.

