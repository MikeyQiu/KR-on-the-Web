8
1
0
2
 
y
a
M
 
4
 
 
]
T
S
.
h
t
a
m

[
 
 
1
v
7
7
5
1
0
.
5
0
8
1
:
v
i
X
r
a

LOCAL ANGLES AND DIMENSION ESTIMATION
FROM DATA ON MANIFOLDS

MATEO D´IAZ, ADOLFO J. QUIROZ, AND MAURICIO VELASCO

Abstract. For data living in a manifold M ⊆ Rm and a point p ∈ M we consider a statistic
Uk,n which estimates the variance of the angle between pairs of vectors Xi − p and Xj − p,
for data points Xi, Xj, near p, and evaluate this statistic as a tool for estimation of the
intrinsic dimension of M at p. Consistency of the local dimension estimator is established
and the asymptotic distribution of Uk,n is found under minimal regularity assumptions.
Performance of the proposed methodology is compared against state-of-the-art methods on
simulated data.

1. Introduction

Understanding complex data sets often involves dimensionality reduction. This is partic-
ularly necessary in the analysis of images, and when dealing with genetic or text data. Such
data sets are usually presented as collections of vectors in Rm and it often happens that
there are non-linear dependencies among the components of these data vectors.
In more
geometric terms these non-linear dependencies amount to saying that the vectors lie on a
submanifold M ⊆ Rm whose dimension d is tipically much smaller than m. The expression
manifold learning has been coined in the literature for the process of ﬁnding properties of
M from the data points.

Several authors in the artiﬁcial intelligence literature have argued about the convenience of
having methods to ﬁnd or approximate these low-dimensional manifolds [3, 13, 27, 29, 31, 33].
Procedures for achieving this kind of low dimensional representation are called manifold
projection methods. Two fairly successful such methods are Isomap of Tenenbaum, de Silva
and Langford [33] and the Locally Linear Embedding method of Roweis and Saul [27]. For
these and other manifold projection procedures, a key initial ingredient is a precise estimation
of the integer d, ideally obtained at low computational cost.

The problem of estimating d has been the focus of much work in statistics starting from
the pioneering work of Grassberger-Procaccia [14]. Most of the most recent dimension iden-
tiﬁcation procedures appearing in the literature are either related to graph theoretic ideas
[6, 7, 23, 24, 35] or to nearest neighbor distances [10, 20, 21, 25]. A key contribution of the
latter group is the work of Levina and Bickel [20] who propose a “maximum likelihood”
estimator of intrinsic dimension. To describe it let Lk(Xi) be the distance from the sample
point Xi to its k-th nearest neighbor in the sample (with respect to the euclidean distance
in the ambient space Rm). Levina and Bickel show that, asymptotically, the expected value

2010 Mathematics Subject Classiﬁcation. 62G05, 62H10, 62H30.
Key words and phrases. dimension estimation, local U -statistics, angle variance, manifold learning.

1

of the statistic

(cid:98)mk(Xi) :=

(cid:34)

1
k − 2

k−1
(cid:88)

j=1

log

Lk(Xi)
Lj(Xi)

(cid:35)−1

(1)

coincides with the intrinsic dimension d of the data. As a result, they propose the corre-
sponding sample average mk := n−1 (cid:80)n
i=1 (cid:98)mk(Xi) as an estimator of dimension. Asymptotic
properties of this statistic have been obtained in the literature (see [24, Theorem 2.1]) allow-
ing for the construction of conﬁdence intervals. Both the asymptotic expected value and the
asymptotic distribution are independent of the underlying density from which the sample
points are drawn and thus lead to a truly non-parametric estimation of dimension.

In addition to distances, Ceruti et al. propose in [9] that angles should be incorporated in
the dimension estimators. This proposal, named DANCo, combines the idea of norm concen-
tration of nearest neighbors with the idea of angle concentration for pairs of points on the
d-dimensional unit sphere.

The resulting dimension identiﬁcation procedure is relatively involved. The method com-
bines two ideas. On one hand it uses the Kullback-Leibler divergence to measure the distance
between the estimated probability density function (pdf) of the normalized nearest neighbor
distance for the data considered and the corresponding pdf of the distance from the cen-
ter of an r-dimensional unit ball to its nearest neighbor under uniform sampling. On the
other hand, it uses a concentration result due to S¨odergren [30], for angles corresponding to
independent pairs of points on a sphere.

The main contribution of this article is a new and simple dimension identiﬁcation pro-
cedure based solely on angle concentration. We deﬁne a U -statistic which averages angle
squared deviations over all pairs of vectors in a nearest neighbor ball of a ﬁxed point and
determine its asymptotic distribution. In the basic version of our proposed method there
is no need of calibration of distributions and moreover our statistic is a U -statistic among
dependent pairs of data points and it is well known that these oﬀer fast convergence to their
mean and asymptotic distribution.

Our method has been called ANOVA in the literature1, given that the U -statistic used,
Uk,n to be deﬁned below, is an estimator of the variance of the angle between pairs of vec-
tors among uniformly chosen points in the sphere Sd−1. Our main results are to prove the
consistency of the proposed method of estimation (Proposition 3.8) and the description of
the (suitably normalized) asymptotic distribution of the statistic considered (Theorem 3.6),
a result that is very useful in the construction of asymptotic conﬁdence intervals in dimen-
sion estimation. We describe our proposed method in Section 2 and provide its theoretical
justiﬁcation in Section 3. Sections 4 and 5 discuss the details of our implementation of the
dimension identiﬁcation procedure together with some empirical improvements. It also con-
tains the result of performance evaluations on simulated examples, including comparisons
with current state-of-the-art methods.

2. A U -statistic for dimension identification

2.1. Description of the statistic. Suppose our data form an i.i.d. sample, X1, . . . , Xn
from a distribution P on Rm with support on a Riemannian C 2 manifold M of dimension

1The term was coined by Breiding, Kalisnik, Sturmfels and Weinstein in [5] when describing an earlier

preliminary version of this article

2

d < m. Given a point p ∈ M , the question to be addressed is to determine the dimension d
of the tangent space of M at p using only information from sample points near p (we want
to allow for the value of d to depend on the point p and for M to be disconnected).

The simplest version of our dimension identiﬁcation procedure is described by the following

steps:

(1) For an appropriate value of the constant C, to be speciﬁed below, let k := (cid:100)C log(n)(cid:101).
Assume, relabeling the sample if necessary, that X1, . . . , Xk are the k nearest neigh-
bors of p in the sample, according to the euclidean distance in Rm.

(2) Deﬁne the angle-variance U -statistic, Uk,n, by the formula

Uk,n :=

1
(cid:1)
(cid:0)k
2

(cid:18)

(cid:88)

arccos

(cid:28) Xi − p
(cid:107)Xi − p(cid:107)

,

Xj − p
(cid:107)Xj − p(cid:107)

(cid:29)

−

(cid:19)2

,

π
2

1≤i<j≤k

where (cid:104)·, ·(cid:105) denotes the dot product on Rm.

(3) Estimate the unknown dimension d as (cid:98)d, equal to the integer r such that βr is closest
to Uk,n, for a suﬃciently large sample size n, where βr is the quantity deﬁned by

βr :=

(cid:40) π2

4 − 2 (cid:80)s
12 − 2 (cid:80)s

π2

j=0

j=1

1

(2j+1)2 if r − 2 = 2s + 1 is odd or
1
(2j)2 if r − 2 = 2s is even.

The key idea of our estimator goes as follows: For large n and the chosen value of k, the
nearest neighbors of p in the data set, behave as uniform data on a small ball around p in
the embedded tangent space of M at this point, and the corresponding unit vectors, (Xi −
p)/(cid:107)Xi − p(cid:107), are nearly uniform on the unit sphere of the tangent space, Sd−1. For uniform
data on Sd−1, the expected angle between two random vectors is always π/2 (regardless of
d), but the variance of this angle decreases rapidly with d. Formula (3) gives the value of
this variance for every dimension r. Since our results below show that the U -statistic, Uk,n,
will converge in probability to βd for the actual dimension of M at p, estimation of d by
choosing the r such that βr closest to Uk,n will be consistent. An additional fact that helps
in this convergence is that the variance of Uk,n, which depends on the fourth moment of the
angles, is also converging rapidly to zero.

The following subsection establishes useful facts about angles between random points on

the unit sphere Sd−1 of Rd and, in particular, about moments of the function

(2)

(3)

(4)

h(z, z(cid:48)) =

arccos (cid:104)z, z(cid:48)(cid:105) −

(cid:16)

(cid:17)2

π
2

when computed on data uniformly distributed on Sd−1. Section 3, building on subsection
2.2, develops the theoretical results that serve as basis for the use of Uk,n on manifolds.

2.2. Angle-variance statistics for pairs of uniform points on Sd−1.

Lemma 2.1 (Angles between uniform vectors). Let Z1, Z2 be two independent vectors
with the uniform distribution on the unit sphere Sd−1 ⊆ Rd and let Θd := arccos(cid:104)Z1, Z2(cid:105) be
the angle between them. The following statements hold:

(1) The distribution of Θd is given by

P(Θd ≤ α) =

(cid:82) α
0 sind−2(φ)dφ
(cid:82) π
0 sind−2(φ)dφ
3

.

(2) The moment generating function of Θd, denoted by φd−2(s) := E (cid:2)esΘd(cid:3) is given by

φ2k(s) = esπ−1
sπ

(cid:81)k

j=1

(2j)2
(2j)2+s2

φ2k+1(s) = esπ+1
2(s2+1)

(cid:81)k

j=1

(2j+1)2
(2j+1)2+s2

according to whether d − 2 is even or odd respectively.

(3) In particular E[Θd] = π

2 for all d and Var[Θd] = βd where

βd :=





π2
4
π2
12

− 2 (cid:80)k

j=0

− 2 (cid:80)k

j=1

1
(2j + 1)2

1
(2j)2

if d − 2 = 2k + 1 is odd or

if d − 2 = 2k is even.

(4) The variance of the centered squared angle σ2

σ2
d =






− π4

8 + 12

k
(cid:80)
j=0

k
(cid:80)
j=1

(cid:32)

π2
4 − 2

1

(2j+1)4 + 2
(cid:32)

k
(cid:80)
j=0

1
(2j + 1)2
(cid:33)2

k
(cid:80)
j=1

1
(2j)2

− π4

120 + 12

1
(2j)4 + 2

π2
12 − 2

if d − 2 = 2k.

d := Var (cid:0)Θd − π
(cid:33)2

2

(cid:1)2 is given by

if d − 2 = 2k + 1 or

Proof.

(1) Passing to polar coordinates r, φ1, . . . , φd−1 with r ≤ 0, 0 ≤ φj ≤ π for
1 ≤ j ≤ d − 2 and 0 ≤ φd−1 ≤ 2π. The probability that Θd ≤ α is precisely the
fraction of the surface area of the sphere deﬁned by the inequality 0 ≤ φ1 ≤ α. Since
the surface element of the sphere is given by

dS = sind−2(φ1) sind−3(φ2) . . . sin(φd−2)dφ1 . . . dφd−1

the probability is given by
0 · · · (cid:82) π
(cid:82) π
0 · · · (cid:82) π
(cid:82) π

(cid:82) α
0
(cid:82) π
0

0

0

(cid:82) 2π
0 dS
(cid:82) 2π
0 dS

=

(cid:82) α
0 sind−2(φ)dφ
(cid:82) π
0 sind−2 φdφ

as claimed.

(2) We begin with a claim

Claim 2.2. Let u : R → R be a C 2-function and deﬁne
(cid:82) α
0 u(x)n sind−2(φ)dφ
Ad

Ed(u(x)) :=

where Ad := (cid:82) π

0 sind−2 φdφ. Then, the following recursion formula holds

d Ed(u(x)) = d Ed−2(u(x)) −

Ed(u(cid:48)(cid:48)(x)).

1
d

Proof. Using integration by parts one can show a recursive formula for Ad and con-
clude that

Ad :=

(cid:40) (2k)!
(2k d!)2
(2kk!)2
(2k+1)!)

π
2

if d − 2 = 2k or

if d − 2 = 2k + 1.

Applying integration by parts twice and using our formula for Ad gives the result. (cid:3)

4

In particular if we take u(x) = esx we get
(cid:19)

(cid:18) d2

Ed(esx) =

Ed−2(esx).

d2 + s2

(3) All densities are, like sine, symmetric around π

As a result we obtain the stated closed formula for the moment generating function.
2 and the ﬁrst statement follows.
To ease the computations we introduce the cumulant-generating function ψd−2 =
log(E(esΘd)). Then it is immediate that Var(Θd) = ψ(cid:48)(cid:48)
d−2(0). We consider two cases,
d even and odd, ﬁrst let us assume that d = 2k + 2. Then, we write the cumulant-
generating function as

ψd−2(s) = log

(cid:19)

(cid:18) esπ − 1
sπ

+

(cid:123)(cid:122)
t(s)

(cid:125)

k
(cid:88)

j=1
(cid:124)

(cid:124)

(cid:18) (2j)2

log

(2j)2 + s2
(cid:123)(cid:122)
r(s)

(cid:19)

.

(cid:125)

After some dry algebra we get t(cid:48)(cid:48)(0) = π2
result for the even case. The odd case follows from an analogous argument.

12 and r(cid:48)(cid:48)(0) = −2 (cid:80)k

1
(2j)2 , which gives the

j=1

(4) Let µj be the jth moment of the random variable (cid:0)Θd − π

(cid:1), i.e. µj = E (cid:0)Θd − π

(cid:1)j.

2

It is well known that µ2 = ψ(cid:48)(cid:48)(0) and µ4 = ψ(4)(0) + 3 (ψ(cid:48)(cid:48)(0))2 . Therefore,

(cid:16)

Var

Θd −

(cid:17)2

π
2

= µ4 − µ2

2 = ψ(4)(0) + 2 (ψ(cid:48)(cid:48)(0))2 .

Again, consider two cases: d even and d odd. Suppose d = 2k − 2, just as before, we
calculate t(4)(0) = − π4
1
(2j)4 . Substituting both these into (5)
yields the claim. A similar argument can be applied to the odd case.

120 and r(4)(0) = 12 (cid:80)k

j=1

At ﬁrst glance the formulas for βd and σd might seem a little complicated. In order to
derive our results we need tangible decrease rates in terms of the dimension. The following
claim gives us an easy way to interpret these quantities.

Claim 2.3. The following bounds hold for βd and σ2
d:

moreover the upper bound for σ2

d holds for all d ≥ 1.

Proof. We distinguish two cases according on whether d ≥ 1 is even or odd. If d is even, we
can deﬁne k by the equality d − 2 = 2k and compute

1
d

≤ βd ≤

1
d − 1

1
2d2 ≤ σ2

d ≤

2
(d − 1)2

for d ≥ 1,

for d ≥ 4,

βd = 2

1
(2j)2 .

∞
(cid:88)

j=k+1
5

2

(5)

(cid:3)

(6)

(7)

Since this series consists of monotonically decreasing terms and d ≥ 2 we conclude that

1
d

=

1
2k + 2

(cid:90) ∞

1

= 2

(2x)2 dx ≤ 2

k+1

∞
(cid:88)

j=k+1

1
(2j)2 ≤ 2

(cid:90) ∞

1

(2x)2 dx =

1
2k + 1

=

1
d − 1

k+ 1
2

as claimed. On the other hand, notice that the other term concerning the variance can be
written as

12

k
(cid:88)

j=1

1
(2j)4 −

π4
120

= −12

∞
(cid:88)

j=k+1

1
(2j)4 ,

which again can be bound by

2
d3 =

1

4(k + 1)3 = 12

(2x)4 dx ≤ 12

k+1

(cid:90) ∞

1

∞
(cid:88)

j=k+1

1
(2j)4 ≤ 2

(cid:90) ∞

1

k+ 1
2

(2x)2 dx =

2
(2k + 1)3 =

2
(d − 1)3 .

Then, we get

1
2d2 ≤

2
d2 −

2

(d − 1)3 ≤ σ2

d ≤

2
(d − 1)2 −

2
d3 ≤

2
(d − 1)2

where the ﬁrst inequality follows since d ≥ 4. The case when d is odd is proven similarly. (cid:3)

3. Theoretical foundations

3.1. Statement of results. In this subsection we state the theoretical results that serve
as basis for the proposed methodology. Proofs are given in the following subsection. The
setting is the following: An i.i.d. sample, X1, . . . , Xn, is available from a distribution P on
Rm. Additionally we have access to a distingushied point p, and near this point the data live
on a Riemannian C 2 manifold M , of dimension d < m. Furthermore, at p the distribution
P has a Lipschitz continuous non-vanishing density function g, with respect to the volume
measure on M . Without loss of generality, we assume that p = 0. Then, we have

Proposition 3.1 (Behavior of nearest neighbors). For a positive constant C, deﬁne
k = (cid:100)C log(n)(cid:101) and let R(n) = Lk+1(0) be the euclidean distance in Rm from p = 0 to its
(k + 1)-st nearest neighbor in the sample X1, . . . , Xn. Deﬁne BR(n)(0) to be the open ball of
radius R(n) around 0 in Rm. Then, the following holds true:

(1) For any suﬃciently large C > 0, we have that, with probability one, for large enough

n (n ≥ n0, for some n0 depending on the actual sample), R(n) ≤ r(n), where

(cid:32)(cid:18) log(n)

(cid:33)

(cid:19) 1

d

r(n) := O

n

is a deterministic function that only depends on the distribution P at p and C.

(2) Conditionally on the value of R(n), the k-nearest-neighbors of 0 in the sample X1, . . . , Xn,
have the same distribution as an independent sample of size k from the distribution
with density gn, equal to the normalized restriction of g to M ∩ BR(n)(0).

In what follows, with a slight abuse of notation, we will write X1, X2, . . . , Xk to denote
the k nearest neighbors of 0 in the sample and assume that these follow the distribution with
density gn of Proposition 3.1. Let π : Rm → TpM be the orthogonal projection onto the
(embedded) tangent space to M at p = 0. For a nonzero X ∈ Rm, let W := π(X), (cid:98)X := X
6

(cid:107)X(cid:107)

and (cid:99)W := W
space of M at 0.

(cid:107)W (cid:107). (cid:99)W takes values in the (d − 1)-dimensional unit sphere Sd−1 of the tangent

Our ﬁrst Lemma bounds the diﬀerence between the inner products (cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) and (cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)
In this Lemma, the random nature of the Xi is

in terms of the length of projections.
irrelevant.

Lemma 3.2 (Basic projection distance bounds). For any X, X1, X2 ∈ M :

(1) (cid:107)X − πX(cid:107) = O((cid:107)πX(cid:107)2)
(2) (cid:107) (cid:98)X − (cid:99)W (cid:107) = O((cid:107)πX(cid:107))
(3) The cosine of the angle between X1 and X2 is close to that between W1 and W2. More

precisely,

|(cid:104) (cid:99)X1, (cid:99)X2(cid:105) − (cid:104)(cid:99)W1, (cid:99)W2(cid:105)| ≤ Cr
for some C ∈ R, whenever r ≥ (cid:107)π(Xi)(cid:107) for i = 1, 2.

Using Lemma 3.2, we can establish the following approximation. Let X1, . . . , Xk be the
k-nearest-neighbors from the sample to p = 0 in Rm . Deﬁne Wi and (cid:99)Wi as above and let
Vk,n be given by the formula

Vk,n :=

1
(cid:1)
(cid:0)k
2

(cid:88)

(cid:16)

(cid:68)

(cid:69)

arccos

(cid:99)Wi, (cid:99)Wj

−

(cid:17)2

.

π
2

1≤i<j≤k

Proposition 3.3 (Approximating the statistic via its tangent analogue). For k =
C log(n), as above, we have

(1) The sequence k(Uk,n − Vk,n) converges to 0 in probability as n → ∞.
(2) limn→∞ E (Uk,n − Vk,n) = 0.

When X comes from the distribution producing the sample, but is restricted to fall very
close to 0, the distribution of πX will be nearly uniform in a ball centered at 0 in TpM . This
will allow us to establish a coupling between the normalized projection (cid:99)W and a variable
Z, uniformly distributed on the unit sphere of TpM , an approximation that leads to the
asymptotic distribution of Uk,n. Some geometric notation must be introduced to describe
these results. Since near 0, M ⊆ Rm is a Riemannian submanifold of dimension d, it
inherits, from the euclidean inner product in Rm, a smoothly varying inner product lp :
TpM × TpM → R, given by lp(u, v) = (cid:104)i∗(u), i∗(v)(cid:105), where i : M → Rm is the inclusion
with diﬀerential i∗. This metric determines a diﬀerential d-form ΩM which, in terms of
local coordinates ∂i for TpM and dual coordinates dxi of TpM ∗ with i = 1, . . . d, is given by
ΩM := (cid:112)det((cid:104)∂i, ∂j(cid:105))1≤i,j≤d)dx1 ∧ · · · ∧ dxd. The diﬀerential form endows M with a volume
measure ν(U ) = (cid:82)
U ΩM . We say that a random variable A on M has density g : M → R if
the distribution µA of A satisﬁes µA(D) = (cid:82)

D gΩM for all borel sets D in M .

If X is a random variable taking values on M with density g and r is a positive real number,
let X(r) be a random variable with distribution gr given by the normalized restriction of g
to M ∩ Br(0), that is:

gr(z) =

(cid:40)

(cid:82)

g(z)
Br (0)∩M gΩM
0, otherwise.

7

, if z ∈ M ∩ Br(0) and

Deﬁne W (r) := π(X(r)). The following geometric Lemma will be used for relating the

densities of X(r) and W (r).

Lemma 3.4 (Tangent space approximations). The following statements hold for all
suﬃciently small r and p = 0 in M .

(1) The map π : Br(0)∩M → π(Br(0)∩M ) is a diﬀeomorphism. Let Φ : Br(0)∩TpM →

Br(0) ∩ M be its inverse.

(2) The inclusion π(Br(0) ∩ M ) ⊆ Br(0) ∩ TpM holds and moreover |λ(Br(0) ∩ TpM ) −

λ(π(Br(0) ∩ M ))| = O(r) where λ denotes the Lebesgue measure on TpM .

(3) The following equality holds:

(cid:115)

1 −

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

1≤i,j≤d

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= O(r)

Let D(r) be a random variable uniformly distributed in Br(0) ∩ TpM and note that Z =
(cid:98)D(r) := D(r)
(cid:107)D(r)(cid:107) is uniformly distributed on the unit sphere Sd−1, regardless of the value of r.
Our next Lemma shows that under weak hypotheses there is a coupling between W (r) and
D(r) which concentrates on the diagonal as r decreases.

Lemma 3.5 (Coupling). Let r denote a small positive number. With D(r) as above and
Z a random vector with the uniform distribution on the unit sphere, Sd−1 of TpM , if the
density g of X in M , near 0, is locally Lipschitz continuous and nonvanishing at 0, then the
following hold:

(1) There exists a coupling A(r) = (W (r), D(r)) and a constant C > 0 such that

P{W (r) (cid:54)= D(r)} ≤ Cr for all suﬃciently small r.
(2) There exists a coupling A(cid:48)(r) = ( (cid:91)W (r), Z) such that P

(cid:110) (cid:91)W (r) (cid:54)= Z

(cid:111)

≤ Cr for all

suﬃciently small r.

The previous Lemma leads to the asymptotic distribution of the statistic Uk,n.

Theorem 3.6 (Local Limit Theorem for angle-variance). Let k := (cid:100)C log(n)(cid:101) for the
constant C of the proof of Proposition 3.1 and assume X1, . . . , Xk are the k nearest neighbors
to p = 0 in the sample, with respect to the euclidean distance in Rm. If dim TpM = d then
the following statements hold:

(1) The equality limn→∞ E[Uk,n] = βd holds and
(2) The quantity k (Uk,n − βd) converges, in distribution, to that of (cid:80)∞

1,i − 1)
1,i are i.i.d. chi-squared random variables with one degree of freedom and

where the χ2
the λi are the eigenvalues of the operator A on L2(Sd−1) deﬁned by

i=1 λi(χ2

(Au)(x) =

(h(x, z) − βd) u(z) dµ(z)

(cid:90)

Sd−1

for u ∈ L2(Sd−1), where h(v, v(cid:48)) := (cid:0)arccos(v · v(cid:48)) − π
measure on Sd−1.

2

(cid:1)2 and µ denotes the uniform

This limit theorem is obtained by the various approximation steps given in the preliminary
results together with the classical Central Limit Theorem for degenerate U statistics, as
described in Chapter 5 of [28]. Depending on the relative values of the λi’s appearing in
8

the statement of the Theorem, it could happen that the limiting distribution just obtained
approaches a Gaussian distribution as the dimension increases (this would happen if the λi
were such that Lindeberg’s condition holds).

Although theoretical study of the λi’s is left for future work, we conjecture that as d
increases the limiting distribution converges to a Gaussian distribution. Numerical experi-
ments seem to support our conjecture, see Figure 1.

(a) d = 2

(b) d = 25

(c) d = 50

Figure 1. QQ-plots. As established in the proof of Theorem 3.6, the limiting
distribution is in fact the asymptotic distribution of k(En−βd), with En deﬁned
in (Equation 14). For this ﬁgure, we generate 10000 samples of the variable
k(En − βd), for k = 10d in dimensions d = 5, 25, and 50. The plots compare
the quantiles of the sample distribution against the standard normal quantiles.

In order to get a consistency result for our basic local dimension estimator, we will use

the following fact.

Corollary 3.7 (Variance convergence). Under the conditions stated at the beginning of
this subsection,

(cid:18)k
2
for d equal to the dimension of M near 0.

lim
n→∞

(cid:19)

Var Uk,n = σ2
d

Recall from Section 2 that our basic procedure estimates the dimension d as (cid:98)d, equal to

the integer r such that βr is closest to Uk,n. This procedure is consistent, as stated next.

Proposition 3.8 (Consistency of basic dimension estimator). As in Section 2, write (cid:98)d
for the basic estimator described above. Let d be the true dimension of M in a neighborhood
of p = 0. Then, in the setting of the present section,

P(failure) = P( (cid:98)d (cid:54)= d) → 0,

as n → ∞.

3.2. Proofs.

Proof of Proposition 3.1.

Let us recall a probability bound for the Binomial distribution. For N an integer valued
random variable with Binomial(n, p) distribution and expected value λ = np, one of the
9

Chernoﬀ-Okamoto inequalities (see Section 1 in [16]) states that, for t > 0, P(N ≤ λ − t) ≤
exp(−t2/2λ). Letting t = λ/2, we get

P(N ≤ λ/2) ≤ exp(−λ/8).
(8)
For ﬁxed and small enough r > 0, let Br(0) denote the ball of radius r around 0 ∈ Rm. For a
random vector X, with the distribution P of our sample X1, X2, . . . , Xn, by our assumptions
on P and M near 0, we have that

P(X ∈ Br(0) ∩ M ) ≤ ανdrd,
where νd is the volume (Lebesgue measure) of the unit ball in Rd and α is a positive number.
Let N = Nr denote the amount of sample points that fall in Br(0) ∩ M . We have that
λ = E(N ) ≤ ανdrdn. We choose r such that λ ≤ C ln n, for a constant C to be speciﬁed in
a moment. Then, by (8), we get

(cid:18)

P

(cid:19)

C
2

N ≤

ln n

≤ exp(−C ln n/8) =

(cid:19)C/8

(cid:18) 1
n

(9)

Pick any value of C > 8. For this choice, the bound in (9) will add to a ﬁnite value when
summed over n. By the Borel-Cantelli Lemma, the inequality N > C
2 ln n will hold for all
n suﬃciently large. It follows that if k = C
2 ln n, the k-nearest-neighbors of 0 in the sample,
will fall in Br(0) for every n suﬃciently large and the chosen value of r, namely

r = r(n) =

(cid:19)1/d

(cid:18)C ln n
ανdn

which is OPr((ln n/n)1/d)). The proof of the ﬁrst part of the Proposition ends by renaming
C.
The statement of the second part of Proposition 3.1 is intuitive and has been used in the
literature without proof. Luckily, Kaufmann and Reiss [18] provide a formal proof of these
type of results in a very general setting. In particular, (ii) of Proposition 3.1 holds by formula
(cid:50)
(6) of [18].

Proof of Lemma 3.2
By an orthogonal change of coordinates, we can assume that TpM is spanned by the ﬁrst
d basis vectors in Rm. The projection π : M → TpM is a diﬀerentiable function whose
derivative at p = 0 is the identity. By the implicit function theorem we can conclude that
there exists an r > 0, such that π : Br(0) ∩ M → π(Br(0) ∩ M ) is a diﬀeomorphism and that
M admits, near p a chart Φ : Br(0) ∩ TpM → M of the form

Φ(z1, . . . , zd) = (z1, . . . , zd, F1(z1, . . . , zd), . . . , Ft(z1, . . . , zd))

(10)

where m = d + t, Φ(0) = p = 0 and such that ∂Fi
(0) = 0 for 1 ≤ i ≤ t and 1 ≤ j ≤ d. As
∂zj
a result, the euclidean distance between a point of M near 0 and the tangent space at 0 is
given, in the local coordinates z, by

d(z1, . . . , zd) =

F 2

i (z).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

t
(cid:88)

i=1

10

We will prove that there exists a constant K such that, for all suﬃciently small δ > 0 and
all z with (cid:107)z(cid:107) ≤ δ the inequality d(z) ≤ K(cid:107)z(cid:107)2 holds. By Applying Taylor’s Theorem
at 0 to the diﬀerentiable function d(z) we conclude, since Φ(0) = 0 and ∂Fi
(0) = 0, that
∂zj
the constant and linear term vanish from the expansion. This proves the claim because
(cid:107)z(cid:107)2 = (cid:107)π(Φ(z))(cid:107)2. Assume K is a constant which satisﬁes (cid:107)X − πX(cid:107) ≤ K(cid:107)πX(cid:107)2. Thus

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)πX(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ K(cid:107)πX(cid:107).

(11)

(12)

On the other side,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

X
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)X(cid:107)

−

1
(cid:107)πX(cid:107)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (cid:107)X(cid:107)

=

1
(cid:107)πX(cid:107)

1
(cid:107)πX(cid:107)

|(cid:107)X(cid:107) − (cid:107)πX(cid:107)| ≤

(cid:107)X − πX(cid:107) ≤ K(cid:107)πX(cid:107).

Altogether,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

X
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)πX(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2K(cid:107)πX(cid:107),

where the ﬁrst inequality is just the triangle inequality and the second one follows from (11)
and (12). The third item in the Lemma follows immediately from the triangle inequality
(cid:50)
and the second item by adding and subtracting (cid:104)(cid:99)Xi, (cid:99)Wj(cid:105).

Remark 3.9. The quadratic term of G is the second fundamental form of M and, therefore,
the constant K can be chosen to be the largest sectional curvature of M at p.

Before proving Proposition 3.3, we need a Lemma on the behavior of the arccos function.

Lemma 3.10. Suppose that −1 ≤ c1 ≤ c2 ≤ 1 and let δ = c2 − c1 be suﬃciently small (for
our purposes it suﬃces to have δ ≤ 1/4). Then,

| arccos(c2) − arccos(c1)| ≤ 2(cid:112)|c2 − c1|

Proof. Assume ﬁrst that both c1 and c2 are positive. We have

| arccos(c2) − arccos(c1)| =

(cid:90) c2

c1

√

1
1 − x2

dx.

Using that the integrand in the last expression is increasing in [0, 1] and by the change of
variables u = 1 − x, we get

| arccos(c2) − arccos(c1)| ≤

(cid:90) 1

1−δ

√

1
1 − x2

dx =

(cid:90) δ

0

1
(cid:112)u(2 − u)

du ≤

(cid:90) δ

0

1
√
u

du

since 2 − u ≥ 1 for u ≤ δ. From this last bound, the result follows in this case by integration.
The argument for the case in which both c1 and c2 are negative is identical, by symmetry. In
the case c1 ≤ 0 ≤ c2, both c1 and c2 fall in a ﬁxed interval ([−1/4, 1/4]) where the derivative
(cid:3)
of arccos is bounded and the result follows easily.

11

Proof of Proposition 3.3
To prove part (1), putting together Proposition 3.1 and Lemma 3.2 we have

max
i≤k

(cid:107)(cid:99)Xi − (cid:99)Wi(cid:107) = OPr(r(n)) = OPr

(cid:19)1/d(cid:33)

(cid:32)(cid:18) ln n
n

From this, it follows easily that

and, using Lemma 3.10 we get

max
i<j≤k

|(cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) − (cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)| = OPr(r(n)),

| arccos(cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) − arccos(cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)| = OPr((cid:112)r(n))

max
i<j≤k

The bound is preserved by the application of the function u (cid:55)→ (u − π/2)2 (since the function
is locally Lipschitz) and by taking averages over all pairs, and we get
(cid:19)1/2d(cid:33)

Uk,n − Vk,n = OPr((cid:112)r(n)) = OPr

.

(13)

(cid:32)(cid:18) ln n
n

The result follows by observing that, for the value of k considered,

k OPr

(cid:32)(cid:18) ln n
n

(cid:19)1/2d(cid:33)

= oPr(1).

To prove (2), notice that from part (1) it is immediate that |Uk,n − Vk,n| converges to zero
in probability, which implies that limn→∞ E (Uk,n − Vk,n) = 0, since Uk,n − Vk,n is a bounded
(cid:50)
random variable.

Proof of Lemma 3.4
Recall, from the proof of Lemma 3.2, that for r > 0, small enough, the projection π :
Br(0) ∩ M → π(Br(0) ∩ M ) is a diﬀeomorphism and that M admits, near p a chart (inverse)
Φ : Br(0) ∩ TpM → M of the form given in (10) and satisfying that Φ(0) = p = 0 and such
that ∂Fi
(0) = 0 for 1 ≤ i ≤ t and 1 ≤ j ≤ d. Also from that proof, recall that there exists a
∂zj
constant K such that for small enough δ > 0 and all z with (cid:107)z(cid:107) ≤ δ, we have d(z) ≤ K(cid:107)z(cid:107)2,
where d(z) is the distance between a point z ∈ M and its projection on TpM . It follows
that the image π(Br(0) ∩ M ) contains a ball of radius r(cid:48) < r such that r − r(cid:48) = O(r2) and
therefore

Br(0) ∩ TpM ⊇ π(Br(0) ∩ M )) ⊇ Br(cid:48)(0) ∩ TpM,
proving part (2) of the Lemma, since the volume of the ﬁrst and last term diﬀer by at
= ei + (cid:80)m−d
most O(r2). For part (3) note that ∂Φ
are O(r2) the inner
∂zi
(cid:113)
products (cid:104) ∂Φ
, ∂Φ
(cid:105)
∂zj
∂zi
(cid:50)
is 1 + O(r) as claimed.

(cid:105) are 1+O(r2) if i = j and O(r2) otherwise and we conclude that

ed+t. Since ∂Ft
∂zi

, ∂Φ
∂zj

(cid:104) ∂Φ
∂zi

∂Ft
∂zi

t=1

Proof of Lemma 3.5
The total variation distance between two probability measures µ and ν, deﬁned as (cid:107)µ −
ν(cid:107)T V := supA |µ(A) − ν(A)|, satisﬁes

(cid:107)µ − ν(cid:107)T V = inf{P{X (cid:54)= Y } : A = (X, Y )}
12

where the inﬁmum runs over all couplings A = (X, Y ) of random variables X, Y with dis-
tributions given by µ and ν, respectively (see, for instance, page 22, Chapter 1 of [34]).
Moreover, if µ and ν are given by densities g1, g2 then the following inequality holds

(cid:107)µ − ν(cid:107)T V ≤ (cid:107)g1 − g2(cid:107)L1.
We will prove the ﬁrst part of the Lemma by bounding the L1-norm of the diﬀerence of
the densities of W (r) and D(r). Recall the deﬁnition of W (r) right before the statement of
Lemma 3.4. The diﬀerence of the two densities is given by

(cid:90)

Br(0)∩TpM

(cid:12)
(cid:12)
(cid:12)
(cid:12)

hr −

1
λ(Br)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dλ

where hr denotes the density of W (r) with respect to the d-dimensional Lebesgue measure
λ in TpM and Br := Br(0) ∩ TpM . More precisely, deﬁning Φ as in Lemma 3.4 the density
of W (r) is given by

hr(u) = gr(Φ(u))

det

(cid:115)

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

(u)

1≤i,j≤d

Since g is locally Lipschitz continuous there exists a constant K such that

g(0) − K1r ≤ g(Φ(u)) ≤ g(0) + K1r.

By Lemma 3.4 there exist constants K2, K3 such that the following inequalities hold for
u ∈ Br:

(cid:115)

1 − K2r ≤

det

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

1≤i,j≤d

(u) ≤ 1 + K2r and

λ(Br) − K3r ≤ λ(π(Br(0) ∩ M )) ≤ λ(Br) + K3r
Combining these inequalities we conclude that there exists a constant ˜K such that for all
u ∈ Br

λ(Br)(g(0) − ˜Kr) ≤

gΩ ≤ λ(Br)(g(0) + ˜Kr).

(cid:90)

Br(0)∩M

As a result the inequality

1
λ(Br)

(cid:32)

g(0) − ˜Kr
g(0) + ˜Kr

(cid:33)

− 1

≤ hr −

1
λ(Br)

≤

1
λ(Br)

(cid:32)

g(0) + ˜Kr
g(0) − ˜Kr

(cid:33)

− 1

so, using the fact that g(0) > 0 we conclude that there exists a constant such that

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Br

hr −

1
λ(Br)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ Cr

as claimed, ﬁnishing the proof of the ﬁrst part of the Lemma.
For part (2), let A(cid:48)(r) be the random pair obtained from A(r) = (W (r), D(r)) by normalizing
(cid:17)
(cid:16) (cid:91)W (r), (cid:91)D(r)
its components, that is A(cid:48)(r) =
= O(r),
because this probability is bounded above by the probability that W (r) and D(r) diﬀer.
Note that the random vector Z = D(r)
(cid:107)D(r)(cid:107) is uniform on the unit sphere, and in particular its
(cid:50)
distribution is independent of the value of r.

(cid:107)W (r)(cid:107) (cid:54)= D(r)

and note that P

(cid:110) W (r)

(cid:107)D(r)(cid:107)

(cid:111)

13

For k = (cid:100)C log(n)(cid:101), as before, let Z1, . . . , Zk be an i.i.d. sample distributed uniformly on the
unit sphere Sd−1 of TpM and deﬁne

En :=

1
(cid:1)
(cid:0)k
2

(cid:88)

(cid:16)

1≤i<j≤n

arccos(cid:104)Zi, Zj(cid:105) −

(cid:17)2

.

π
2

(14)

In view of Proposition 3.3, in order to prove Theorem 3.6 it will suﬃce to show that the
limiting standardized distribution of the Vk,n deﬁned in that Proposition is the same as that
of En, and establish the asymptotics for En.

Proof of Theorem 3.6

By Proposition 3.1 part (1) on a set of probability 1 (on the set of inﬁnite samples of

data), for some n0, the inequality Rn := R(n) ≤ r(n) holds for n ≥ n0.
By Proposition 3.1 part (2), for i = 1, . . . , k the distribution of Wi = π(Xi), the projections
on TpM of the k nearest neighbors of 0 in the sample, is that of an independent sample
W1(Rn), . . . , Wk(Rn), with the Wj(Rn), j ≤ k as deﬁned before Lemma 3.4.

From the previous Lemma, conditionally on Rn, we have a coupling A(cid:48)

j(Rn) = ( (cid:92)Wj(Rn), Zj)
for each j ≤ k. These couplings can be taken such that the (cid:92)Wj(Rn), j ≤ k form an i.i.d.
sample and the same holds for the Zj’s. By the previous lemma, we have that for each j,
P( (cid:92)Wj(Rn) (cid:54)= Zj) ≤ Cr(n). Then, except for a set of measure 0, we get the following event
inclusion

{kVk,n (cid:54)= kEn} ⊆

{ (cid:92)Wi(Rn) (cid:54)= Zi}

k
(cid:91)

j=1

By the union bound, the probability of the rightmost event is bounded by Ck(n)r(n) which
goes to zero, as n goes to inﬁnity by the choice of k(n) and the value of r(n). It follows that
Vk,n and En have the same standardized asymptotic distributions, and being both random
variables bounded, it follows that limn→∞ E(Vk,n − En) = 0.

For part (2) of Theorem 3.6 it only remains to establish the limiting distribution of En.
This statistic falls in the framework of classical U -statistics, that have played an important
role in the theory of many non-parametric procedures. See for instance [26] for several
applications of the theory of U -statistics. Even an empirical processes theory is available
for U -processes, see for instance [2], which has found application in the study of notions of
multivariate depth. Still, we will only require the classical theory, as exposed in Chapter 5
of [28] and Chapter 3 of [26].

Recall the deﬁnition of the kernel h in Equation 4. By the symmetry of the uniform
distribution for three independent vectors, Z1, Z2, Z3 with uniform distribution on Sd−1, it
can be easily veriﬁed that,

This means that the U -statistic associated to h is degenerate. It follows (see the variance

calculation in [26]) that

E (cid:0)h(Z1, Z2)h(Z1, Z3) − β2

(cid:1) = 0.

d

(cid:19)

(cid:18)k
2

Var(En) = Var(h(Z1, Z2))

(15)

(16)

and by the Theorem for degenerate U -statistics in Section 5.5 of [28], part (2) of our Theorem
(cid:50)
follows.

14

Proof of Corollary 3.7
By (16), it suﬃces to show that k2 lim(EU 2
k,n − EE2
n) = 0, for En as in the proof of Theorem
3.6. Applying equation (13) and the facts that the function h is bounded and that k = O(ln n)
on a set of probability 1, we have

k2EU 2

k,n ≤ k2EV 2

k,n + O

ln2 n

(cid:32)

(cid:19)1/2d(cid:33)

.

(cid:18)ln n
n

The opposite inequality (interchanging the roles of Uk,n and Vk,n) is obtained by the same
reasoning, and we get

(17)
By the coupling argument of the proof of Theorem 3.6, we have that Vk,n and En might
diﬀer at most on a set of measure O(ln(n) r(n)). Taking expectations on the sets were they
coincide and diﬀer, we obtain

k,n) = 0.

lim k2(EU 2

k,n − EV 2

k2 EV 2

k,n ≤ k2 EE2

n + k2 O(ln(n) r(n)).

Observing that the second term in the right hand side of this inequality goes to zero, as n
grows, and that the reverse inequality is obtained similarly, we conclude

and the result follows by combining (17) and (18).

lim k2(EV 2

k,n − EE2

n) = 0,

(18)
(cid:50)

Proof of Proposition 3.8.
Let d be the true value of the dimension. βd+1 is the expected value closest to βd and, by
the proof of Claim 2.3, (βd − βd+1)/2 ≥ 1/(d − 1)2. For the basic procedure to incur in error
it is necessary that |Uk,n − βd| ≥ |Uk,n − βd+1|, which means
βd − βd+1
2

|EUk,n − βd| + |Uk,n − EUk,n| ≥

1
(d − 1)2 .

≥

1

Since |EUk,n − βd| converges to zero, the condition above requires that |Uk,n − EUk,n| ≥
1/
2(d−1)2 , for n large enough. The probability of this last event is bounded as follows. Let c
be a sup norm bound for h( (cid:99)X1, (cid:99)X2) − Eh( (cid:99)X1, (cid:99)X2), for the kernel h given in (4) and (cid:99)Xi as in
Lemma 3.2. Clearly, c is bounded above by π2/4. By Bernstein’s inequality for U -statistics
[2, Proposition 2.3(a)], we get

(cid:18)

P

|Uk,n − EUk,n| ≥

(cid:19)

1
2(d − 1)2

(cid:32)

≤ 2 exp

−k/(8(d − 1)4)
2Var(h( (cid:99)X1, (cid:99)X2)) + 2c

6(d−1)2

(cid:33)

Now, using Claim 2.3 and Corollary 3.7, we have, after some calculations,

(cid:18)

P

|Uk,n − EUk,n| ≥

(cid:19)

1
2(d − 1)2

(cid:32)

≤ 2 exp

−k/(8(d − 1)4)
(d−1)2 + π2

12(d−1)2

5

(cid:33)

≤ 2 exp

(cid:18)

−k
47(d − 1)2

(cid:19)

This bound goes to zero as n (and k) grow to inﬁnity, ﬁnishing the proof.

(19)

(cid:50)

Forcing the estimation error bound in (19) to be less that a given δ > 0 will give a value of
k of the order of Ad 2 ln(2/δ), for some constant A, reﬂecting that precise estimation is more
demanding, in terms of sample size, as the dimension d grows.

15

4. Estimators

In this section we present two dimension estimators based on the statistic Uk,n. First,
a local estimator that gives the dimension of M around a distinguished non-singular point
p ∈ M is discussed. Then, the case in which the manifold is equidimensional is considered,
by building upon our local estimator to propose a global dimension estimator. Some imple-
mentation issues are discussed and in the following section we evaluate the performance of
our estimators. The programming code used in these experiments is publicly available, it
can be found at https://github.com/mateodd25/ANOVA_dimension_estimator.

Algorithm 1: Local dimension estimation
Data: k ∈ N+ and X1, . . . , Xn, p ∈ M ⊆ Rm
Result: Estimated dimension (cid:98)d at p ∈ M
Find the k-nearest neighbors to p;
Use these neighbors to compute Uk,n as in (2);
Choose (cid:98)d associated with Uk,n;

4.1. Local estimators. The theory presented in Section 3 suggests that k ∼ log(n) should
be a good choice, asymptotically-speaking, for the number of neighbors to consider in the
local dimension estimation procedure. However, one could potentially leverage prior knowl-
edge of the structure of the problem to set this parameter diﬀerently. In our implementation
we set it to k = round(10 log10(n)).

In our theoretical analysis presented above, it was assumed that we are given a center
point p where the local dimension is to be estimated. A natural question that arises in
practice is the following: given a sample, how to select good center points. In Section 4.2
we present a simple heuristic to select “good” centers.

Both estimators presented in what follows are based on the Algorithm 1. The diﬀerence
between the estimators considered lies on the last line of the algorithm, namely, on how to
pick the dimension estimator, given Uk,n. Next, the two diﬀerent rules to execute this step
are discussed.

4.1.1. Basic estimator. Since Uk,n converges in probability to βd, a natural way to estimate
the dimension from Uk,n is to set

(cid:98)dbasic := arg min
d∈[Dmax]

|βd − Uk,n|,

where Dmax is the ambient dimension or some bound we know a priori on the dimension
of the manifold. Interestingly, such a rule is fairly accurate, as established in Proposition
3.8. Another advantage of this estimator is that there is no need to train it, since all the
quantities involved have been analytically computed (and presented in Section2).

Remark 4.1. Classical discriminant analysis results (see, for example, Section 4.1 in [11])
would advice to incorporate available variance information on the selection of (cid:98)d, by choosing

(cid:98)ddisc := arg max{d | Uk,n ≥ ηd}

where

ηd = βd +

(βd−1 − βd).

σd
σd + σd+1

16

Algorithm 2: Global dimension estimation
Data: c ∈ N+, k ∈ N+ and X1, . . . , Xn ∈ M ⊆ Rm
Result: Estimated dimension (cid:98)d of M
Choose c centers p1, . . . , pc from the sample;
Apply Algorithm 1 to each center pi, let (cid:98)di be its output;
Set (cid:98)d to the median of { (cid:98)di}c
i=1;

Still, simulation evaluations (not included) show that (cid:98)ddisc and (cid:98)dbasic have a very similar
performance in practice (and also in theory, since both are consistent). Thus, we prefer to
use the later, being the simpler one.

1

, . . . , Y (d)

4.1.2. Kernel-based estimator. For our second estimator we start by simulating multiple
instances Y (d)
M of the random variable k(En − βd), for a large value of M (=5000,
for instance) and with En as deﬁned in equation (14), for each dimension d ∈ [Dmax] =
{1, . . . , Dmax}. From these data, the density ˆf (d)
k , of k(En − βd), is estimated, for each d, as
M
(cid:88)

(cid:32)

(cid:33)

ˆf (d)
k (y) =

1
M h

ϕ

y − Y (d)
h

i

i=1
where ϕ(·) is the standard Gaussian density and the parameter h (the “bandwidth”) can be
set at h = (4/3M )1/5. This choice of bandwidth guarantees consistent density estimation (see
Section 4.1 in [4]). Then, a Bayesian classiﬁcation procedure with uniform prior distribution
on the set [Dmax], would select the dimension as that for which the kernel density estimator
is maximized at the standardized Uk,n, namely
ˆf (d)
k (k(Uk,n − βd)).

(cid:98)dker = arg max
d∈[Dmax]

(20)

Notice that the simulations described above need to be performed only once for each dimen-
sion, since they are made on uniform data on Sd−1 and do not depend on the particular data
being studied.

4.2. Global estimators. We now turn our attention to extending the local dimension es-
timation algorithm to a global one. Assuming that M is equidimensional, the local method
can be extended by running multiple instances of Algorithm 1 on diﬀerent centers, and
combining the results, as outlined in Algorithm 2.

After getting dimension estimates at each center, one could use diﬀerent summary statistics
to choose the global dimension, such as the mean, the mode or the median. To make a method
robust against outliers, we chose to use the median. To decide about the parameter c we
ran a cross-validation algorithm. Empirically, it appears that c ∼ log(n) is a good choice for
this parameter.

4.2.1. Choosing centers. To pick the centers pi in Algorithm 2, we divide the sample into c
disjoint subsamples of approximately equal size. Assume, for simplicity of the exposition,
Inside each subsample we pick a center by assigning each point a score of
that c = 1.
centrality and then choosing the one with highest score. Scores are assigned through the
following procedure:

17

(1) For each coordinate i, we order the sample based on the i-th entry, let τi be the
permutation giving this ordering, that is, the i-th row in (Xτi(1), . . . , Xτi(n)) is non-
decreasing.

(2) Then, the centrality score of Xj is given by (cid:80)m

(cid:12)
(cid:12)
(cid:12) .
For the i-th coordinate, the weight function f gives the maximum scores to the point (or
points) such that τi(j) is closest to n/2 and thus, the mechanism used chooses as center a
point which for many components, appears near the center of these orderings.

i=1 f (τi(j)), where f (x) =

2 − 2(x−1)

(cid:12)
(cid:12)
(cid:12)

2n

1

4.2.2. Heuristic to discard centers. Finally, we present a simple heuristic for discarding some
of the selected centers, based on the mean of the angles between its neighbors, taking as
always, the point considered as origin. This is done in order to improve the performance of
the statistic. Consider again the angle

(cid:28) Xi − p
(cid:107)Xi − p(cid:107)
for each pair of nearest neighbors Xi, Xj of the point p ∈ M , as used in the basic deﬁnition
(2). Consider the average of these angles,

Xj − p
(cid:107)Xj − p(cid:107)

θi,j = arccos

(cid:29)

,

θ(p) =

1
(cid:1)
(cid:0)k
2

(cid:88)

θi,j.

1≤i<j≤k

If the manifold M , near p, is approximately ﬂat, θ(p) should be close to π/2, regardless of
the value of the dimension d, since π/2 is the expected value of the angle between uniformly
sampled points in every dimension and the U -statistic θ(p) should converge rapidly to this
expectation. Thus, when θ(p) is far from π/2, it can be taken as a suggestion of strong
curvature that is causing non-uniformity of the angles, and therefore, p might not be a good
point to consider for dimension estimation. For these reason, in our implementation, the user
is allowed to use this heuristic and discard a fraction of the centers pi with largest values of
|θ(pi) − π/2|. Experiments presented in the next section suggest that this heuristic is useful
when the manifold is highly curved.

5. Numerical results

We compare our methods against two powerful dimension estimators, DANCo [9] and
Levina-Bickel [20], using a manifold library proposed in [15]. The ﬁrst estimator is, ar-
guably, the state-of-the-art for this problem, while the second one is a classical well-known
estimator with great performance. To see a comparison between these and other estimators
we refer the reader to [8, 9].

Table 1 presents a brief description of the manifolds included in the study. Additionally
Table 2 contains a list of the parameters used for each one of the estimators. We compare
two error measures, namely the Mean Square Error (MSE) and the Mean Percentage Error
(MPE) which are deﬁned as

MSE( (cid:98)d) :=

( (cid:98)di − di)2

and

MPE( (cid:98)d) :=

1
T

T
(cid:88)

i=1

100
T

T
(cid:88)

i=1

| (cid:98)di − di|
di

where T is the number of trials included in the test and (cid:98)di and di are the estimated dimension
and the correct dimension of the ith trial, respectively.

18

For each one of the aforementioned manifolds, we draw T = 50 random samples with
n = 2500 data points and then compute the MSE and MPE of the following four esti-
mators: the global basic estimator (Basic), the global basic estimator combined with the
centers heuristic (B+H), the global kernel-based estimator (Kernel), the global kernel-based
estimator with centers heuristic (K+H), the Levina-Bickel estimator (LB), and the DANCo
estimator. Tables 3 and 4 summarize the results.

Table 1. Library of manifolds used for benchmark, for more details consult [15].

Description
Sphere S9
Aﬃne subspace
Nonlinear manifold
Nonlinear manifold
Helix
Nonlinear manifold
Swiss roll
Highly curved manifold
Full-dimensional cube
9-dimensional cube

Manifold d m
10
9
5
3
6
4
8
4
3
2
36
6
2
3
12 72
20 20
10
9
2
3 Ten-times twisted Mobius band
10 10
10
1

M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
M11
M12
M13

Multivariate Gaussian
Curve

Table 2. Parameters of each algorithm.

Method Parameters
ANOVA k = 34, c = 16
k1 = 10, k2 = 20
k = 10

LB
DANCo

In both these tables, the last column shows the average of the performance measure over
the examples. It is clear from these tables that the angle-variance methods introduced in the
present article do well, in terms of average performance, against the very strong competitors
considered. This is more evident when the measure of error is the MSE. Still, for many of the
manifolds considered, namely M1, M3, M4 (in this case tied with LB), M9 and M12, DANCo
clearly displays the best performance. The Levina-Bickel estimator is the best for manifold
M6, tying for ﬁrst with DANCo in M4, while the procedures proposed in this article show
the best performance in the cases of manifolds M8 and M10, having very good performance
also in cases M3, M5, M7 and M12. It is interesting that our methods do particularly well
in case M8, a high curvature manifold of a relatively high dimensional in a large dimension
ambient space. It does not appear to exist a signiﬁcant diﬀerence in performance between
the Basic procedure and the procedure that uses Kernel Density Estimation. On the other
hand, the introduction of the heuristics discussed in Section 4 turns out to be beneﬁcial for
our estimators in some of the relatively high dimensional cases, namely M9 and M12, while
19

Table 3. Rounded Mean Square Error for diﬀerent manifolds, last column
displays the average MSE over all the examples. The darker cells show the
best results in each column.

M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 Mean
1.72
1.11
Basic
0.95 0.00 0.58 0.03 0.00 0.67 0.00
2.14
0.79
B+H 1.09 0.00 0.66 0.28 0.00 1.30 0.01
1.27
1.10
Kernel 0.95 0.00 0.68 0.08 0.00 0.69 0.29
0.80
2.48
K+H 0.99 0.00 0.76 0.45 0.00 1.31 0.31
2.69
2.27
0.49 0.02 0.05 0.00 0.00 0.10 0.00
LB
2.11
DANCo 0.16 0.00 0.00 0.00 0.00 1.00 0.00 25.22

10.39 0.00 0.00 0.12 0.00
4.64
0.03 0.00 0.10 0.00
10.20 0.00 0.00 0.15 0.00
4.06
0.01 0.00 0.04 0.00
29.33 2.23 0.00 0.54 0.00
0.10 0.00 0.00 0.00
0.96

Table 4. Rounded Mean Percentage Error for diﬀerent manifolds, last col-
umn displays the average MPE over all the examples.

M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 Mean
9.00
1.00
4.93
Basic
10.56 0.00 15.50
5.95
9.83
8.25
B+H 11.33 0.00 17.75
6.45
7.75
2.50
Kernel 10.56 0.00 17.75
7.67
K+H 11.00 0.00 20.00 12.25 0.00 18.00 17.50 10.75
7.31
1.49
LB
5.09
0.00
DANCo

0.00 1.50 0.00
0.00 1.20 0.00
0.00 1.80 0.00
0.00 0.60 0.00
12.55 27.07 16.57 1.64 7.34 0.50
0.00 0.00 0.00
41.83

0.00
0.00 10.83
0.00 17.83
0.50
0.00 11.50 16.50

1.84
5.13
0.00 16.66

15.50
10.00
15.55
9.60

0.00
0.67
0.00
0.11

2.63
0.00

5.77
0.00

4.84
0.00

7.77
1.77

4.80

1.11

these heuristics degrade somehow the performance in the intermediate dimension cases, M4
and M6. In all other cases, the use of the heuristics for center selection and center elimination
does not appear to have a strong eﬀect.

References

[1] Aldous, D. and J. Shun (2010) Connected spatial networks over random points and a route-length

statistic. Statistical Science 25, 275 - 288.

[2] Arcones, M. A. and Gin´e, E. (1993) Limit Theory for U -Processes. Annals of Probability 21, No. 3,

1494-1542.

[3] Belkin, M. and Niyogi, P. (2004) Semi-supervised learning on Riemannian manifolds. Invited Paper

inMachine Learning. Special Issue on Clustering 56, 209-239.

[4] Bowman, A. W. and Foster, P. J. (1993) Adaptive smoothing and density-based tests of multivariate

normality. Journal of the American Statistical Association 88 No. 422, 529-537.

[5] Breiding, P., Kalisnik, S., Sturmfels, B. and Weinstein, M. (2018) Learning Algebraic Varieties from

Samples. arXiv:1802.094[math.AG]

[6] Brito, M. R., Quiroz, A. J. and Yukich, J. E. (2002) Graph theoretic procedures for dimension identiﬁ-

cation. Journal of Multivariate Analysis 81, 67-84.

[7] Brito, M. R., Quiroz, A. J. and Yukich, (2013) Intrinsic dimension identiﬁcation via graph-theoretic

methods. Journal of Multivariate Analysis 116, 263277.

[8] Campadelli, P., Casiraghi, E., Ceruti, C. and Rozza, A. (2015) Intrinsic Dimension Estimation: Relevant
Techniques and a Benchmark Framework. Mathematical Problems in Engineering 2015 Article ID:
759567, 21 pages.

[9] Ceruti, C., Bassis, S., Rozza, A., Lombardi, G., Casiraghi, E. and Campadelli, P. (2014) DANCo: An
intrinsic dimensionality estimator exploiting angle and norm concentration. Pattern Recognition 47, No.
8, 2569-2581.

20

[10] Costa, J. A., Girotra, A. and Hero, A .O. (2005) Estimating local intrinsic dimension with k-nearest
neighbor graphs. In IEEE/SP 13th Workshop on Statistical Signal Processing, 417-422. IEEE Conference
Publication.

[11] Devroye, Luc, Gy¨orﬁ, L´aszl´o and Lugosi, G´abor. (2013) A probabilistic theory of pattern recognition

(Vol. 31). Springer Science and Business Media.

[12] Duda, R. O., Hart, P. E. and Stork, D. G. (2001) Pattern Classiﬁcation. 2nd. edition. John Wiley and

Sons, New York.

189-208.

[13] Farahmand, A., Szepesv´ari, C. and Audibert, J-Y (2007) Manifold-adaptive dimension estimation. In
Proceedings of the 24th International Conference on Machine Learning, Z. Ghahramani, editor, 265-272.
ACM, New York.

[14] Grassberger, P. and Procaccia, I. (1983) Measuring the strangeness of strange attractors. Physica 9D,

[15] Hein, M. and Audibert, J.-Y. (2005) Intrinsic dimensionality estimation of submanifolds in Rd. In

Proceedings of the 22nd International Conference on Machine learning, 289-296, ACM.

[16] Janson, S. (2002). On concentration of probability. In Bollob´as, B. (Ed.) Contemporary Combinatorics,
10. Proceedings of the Workshop on Probabilistic Combinatorics at the Paul Erd¨os Summer Research
Center, Budapest, 1998, pp. 289-301.

[17] Johnson, M. E. (1987) Multivariate Statistical Simulation. John Wiley and Sons, New York.
[18] Kaufmann, E. and Reiss, R.-D. (1992) On Conditional Distribution of Nearest Neighbors. Journal of

Multivariate Analysis 42, 67-76.

[19] Kegl, B. (2003) Intrinsic dimension estimation using packing numbers. In Advances in Neural Infor-
mation Processing Systems, Volume 15, Eds. S. Becker, S. Thrun and K. Obermayer. M.I.T. Press,
Cambridge, Massachusetts.

[20] Levina, E. and Bickel, P. J. (2005) Maximum likelihood estimation of intrinsic dimension. In Advances
in Neural Information Processing Systems, Volume 17, Eds. L. K. Saul, Y. Weiss and L. Bottou.
[21] Lombardi et. al., 2011 Lombardi, G., Rozza, A., Ceruti, C., Casiraghi, E. and Campadelli, P. (2011).
Minimum Neighbor Distance Estimators of Intrinsic Dimension. In D. Gunopulos et al. (Eds.): ECML
PKDD 2011, Part II, LNAI 6912, pp. 374-389. Springer-Verlag. Berlin.

[22] Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979) Multivariate Analysis. Academic Press, New York.
[23] Penrose, M. D. and Yukich, J. E. (2001) Central limit theorems for some graphs in computational

geometry. Annals of Applied Probability 11, 1005-1041.

[24] Penrose, M. D. and Yukich, J. E. (2013) Limit theory for point processes in manifolds. Annals of Applied

Probability 23, No. 6, 2161-2211.

[25] Pettis, K. W., Bailey, T. A. Jain, A. K. and Dubes, R. C. (1979) An intrinsic dimensionality estimator
from near-neighbor information. IEEE Transactions on Pattern Analysis and Machine Intelligence 1,
25-37.

[26] Randles, R. H. and Wolfe, D. A. (1979) Introduction to the Theory of Nonparametric Statistics. John

[27] Roweis, S. T. and Saul, L. K. (2000) Nonlinear dimensionality reduction by locally linear embedding.

[28] Serﬂing, R. J. (1980) Approximation Theorems of Mathematical Statistics. John Wiley and Sons, New

[29] Sindhwani, V., Belkin, M. and Nigoyi, P. (2006) The Geometric Basis of Semi-supervised Learning.
Book chapter in Semi-supervised Learning, O. Chapelle, B. Sch¨olkopf and A. Zien, editors, M.I.T.
Press, Cambridge, Massachusetts.

[30] S¨odergren, A. (2011) On the distribution of angles between the N shortest vectors in a random lattice

Journal of the London Mathematical Society, 84, No. 3, 749-764.

[31] Sricharan, K. Raich, R. and Hero, A. O. (2010) Optimized intrinsic dimension estimation using near-
est neighbor graphs. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 5418-5421. IEEE Conference Publication.

[32] Steele, J. M., Shepp, L. A. and Eddy, W. F. (1987) On the number of leaves of a Euclidean minimal

spanning tree. Journal of Applied Probability, 24, 809-826.

Wiley and Sons, New York.

Science, 290, 2323-2326.

York.

21

[33] Tenenbaum, J. B., de Silva, V. and Langford, J. C. (2000) A global geometric framework for nonlinear

dimensionality reduction. Science 290, 2319-2323.

[34] Villani, C. (2009) Optimal Transport. Old and New. Volume 338 in series Grundlehren der matematischen

[35] Yukich, J. E. (1998) Probability Theory of Classical Euclidean Optimization Problems. Lecture Notes in

Wissenschaften. Springer. Berlin, Heidelberg.

Mathematics, 1675, Springer, New York.

Mateo D´ıaz, Center for Applied Mathematics, 657 Frank H.T. Rhodes Hall, Cornell

University, Ithaca, NY 14853

E-mail address: md825@cornell.edu

Adolfo J. Quiroz, Departamento de Matem´aticas, Universidad de los Andes, Carrera 1

No. 18a 10, Edificio H, Primer Piso, 111711 Bogot´a, Colombia

E-mail address: aj.quiroz1079@uniandes.edu.co

Mauricio Velasco, Departamento de Matem´aticas, Universidad de los Andes, Carrera 1

No. 18a 10, Edificio H, Primer Piso, 111711 Bogot´a, Colombia

E-mail address: mvelasco@uniandes.edu.co

22

8
1
0
2
 
y
a
M
 
4
 
 
]
T
S
.
h
t
a
m

[
 
 
1
v
7
7
5
1
0
.
5
0
8
1
:
v
i
X
r
a

LOCAL ANGLES AND DIMENSION ESTIMATION
FROM DATA ON MANIFOLDS

MATEO D´IAZ, ADOLFO J. QUIROZ, AND MAURICIO VELASCO

Abstract. For data living in a manifold M ⊆ Rm and a point p ∈ M we consider a statistic
Uk,n which estimates the variance of the angle between pairs of vectors Xi − p and Xj − p,
for data points Xi, Xj, near p, and evaluate this statistic as a tool for estimation of the
intrinsic dimension of M at p. Consistency of the local dimension estimator is established
and the asymptotic distribution of Uk,n is found under minimal regularity assumptions.
Performance of the proposed methodology is compared against state-of-the-art methods on
simulated data.

1. Introduction

Understanding complex data sets often involves dimensionality reduction. This is partic-
ularly necessary in the analysis of images, and when dealing with genetic or text data. Such
data sets are usually presented as collections of vectors in Rm and it often happens that
there are non-linear dependencies among the components of these data vectors.
In more
geometric terms these non-linear dependencies amount to saying that the vectors lie on a
submanifold M ⊆ Rm whose dimension d is tipically much smaller than m. The expression
manifold learning has been coined in the literature for the process of ﬁnding properties of
M from the data points.

Several authors in the artiﬁcial intelligence literature have argued about the convenience of
having methods to ﬁnd or approximate these low-dimensional manifolds [3, 13, 27, 29, 31, 33].
Procedures for achieving this kind of low dimensional representation are called manifold
projection methods. Two fairly successful such methods are Isomap of Tenenbaum, de Silva
and Langford [33] and the Locally Linear Embedding method of Roweis and Saul [27]. For
these and other manifold projection procedures, a key initial ingredient is a precise estimation
of the integer d, ideally obtained at low computational cost.

The problem of estimating d has been the focus of much work in statistics starting from
the pioneering work of Grassberger-Procaccia [14]. Most of the most recent dimension iden-
tiﬁcation procedures appearing in the literature are either related to graph theoretic ideas
[6, 7, 23, 24, 35] or to nearest neighbor distances [10, 20, 21, 25]. A key contribution of the
latter group is the work of Levina and Bickel [20] who propose a “maximum likelihood”
estimator of intrinsic dimension. To describe it let Lk(Xi) be the distance from the sample
point Xi to its k-th nearest neighbor in the sample (with respect to the euclidean distance
in the ambient space Rm). Levina and Bickel show that, asymptotically, the expected value

2010 Mathematics Subject Classiﬁcation. 62G05, 62H10, 62H30.
Key words and phrases. dimension estimation, local U -statistics, angle variance, manifold learning.

1

of the statistic

(cid:98)mk(Xi) :=

(cid:34)

1
k − 2

k−1
(cid:88)

j=1

log

Lk(Xi)
Lj(Xi)

(cid:35)−1

(1)

coincides with the intrinsic dimension d of the data. As a result, they propose the corre-
sponding sample average mk := n−1 (cid:80)n
i=1 (cid:98)mk(Xi) as an estimator of dimension. Asymptotic
properties of this statistic have been obtained in the literature (see [24, Theorem 2.1]) allow-
ing for the construction of conﬁdence intervals. Both the asymptotic expected value and the
asymptotic distribution are independent of the underlying density from which the sample
points are drawn and thus lead to a truly non-parametric estimation of dimension.

In addition to distances, Ceruti et al. propose in [9] that angles should be incorporated in
the dimension estimators. This proposal, named DANCo, combines the idea of norm concen-
tration of nearest neighbors with the idea of angle concentration for pairs of points on the
d-dimensional unit sphere.

The resulting dimension identiﬁcation procedure is relatively involved. The method com-
bines two ideas. On one hand it uses the Kullback-Leibler divergence to measure the distance
between the estimated probability density function (pdf) of the normalized nearest neighbor
distance for the data considered and the corresponding pdf of the distance from the cen-
ter of an r-dimensional unit ball to its nearest neighbor under uniform sampling. On the
other hand, it uses a concentration result due to S¨odergren [30], for angles corresponding to
independent pairs of points on a sphere.

The main contribution of this article is a new and simple dimension identiﬁcation pro-
cedure based solely on angle concentration. We deﬁne a U -statistic which averages angle
squared deviations over all pairs of vectors in a nearest neighbor ball of a ﬁxed point and
determine its asymptotic distribution. In the basic version of our proposed method there
is no need of calibration of distributions and moreover our statistic is a U -statistic among
dependent pairs of data points and it is well known that these oﬀer fast convergence to their
mean and asymptotic distribution.

Our method has been called ANOVA in the literature1, given that the U -statistic used,
Uk,n to be deﬁned below, is an estimator of the variance of the angle between pairs of vec-
tors among uniformly chosen points in the sphere Sd−1. Our main results are to prove the
consistency of the proposed method of estimation (Proposition 3.8) and the description of
the (suitably normalized) asymptotic distribution of the statistic considered (Theorem 3.6),
a result that is very useful in the construction of asymptotic conﬁdence intervals in dimen-
sion estimation. We describe our proposed method in Section 2 and provide its theoretical
justiﬁcation in Section 3. Sections 4 and 5 discuss the details of our implementation of the
dimension identiﬁcation procedure together with some empirical improvements. It also con-
tains the result of performance evaluations on simulated examples, including comparisons
with current state-of-the-art methods.

2. A U -statistic for dimension identification

2.1. Description of the statistic. Suppose our data form an i.i.d. sample, X1, . . . , Xn
from a distribution P on Rm with support on a Riemannian C 2 manifold M of dimension

1The term was coined by Breiding, Kalisnik, Sturmfels and Weinstein in [5] when describing an earlier

preliminary version of this article

2

d < m. Given a point p ∈ M , the question to be addressed is to determine the dimension d
of the tangent space of M at p using only information from sample points near p (we want
to allow for the value of d to depend on the point p and for M to be disconnected).

The simplest version of our dimension identiﬁcation procedure is described by the following

steps:

(1) For an appropriate value of the constant C, to be speciﬁed below, let k := (cid:100)C log(n)(cid:101).
Assume, relabeling the sample if necessary, that X1, . . . , Xk are the k nearest neigh-
bors of p in the sample, according to the euclidean distance in Rm.

(2) Deﬁne the angle-variance U -statistic, Uk,n, by the formula

Uk,n :=

1
(cid:1)
(cid:0)k
2

(cid:18)

(cid:88)

arccos

(cid:28) Xi − p
(cid:107)Xi − p(cid:107)

,

Xj − p
(cid:107)Xj − p(cid:107)

(cid:29)

−

(cid:19)2

,

π
2

1≤i<j≤k

where (cid:104)·, ·(cid:105) denotes the dot product on Rm.

(3) Estimate the unknown dimension d as (cid:98)d, equal to the integer r such that βr is closest
to Uk,n, for a suﬃciently large sample size n, where βr is the quantity deﬁned by

βr :=

(cid:40) π2

4 − 2 (cid:80)s
12 − 2 (cid:80)s

π2

j=0

j=1

1

(2j+1)2 if r − 2 = 2s + 1 is odd or
1
(2j)2 if r − 2 = 2s is even.

The key idea of our estimator goes as follows: For large n and the chosen value of k, the
nearest neighbors of p in the data set, behave as uniform data on a small ball around p in
the embedded tangent space of M at this point, and the corresponding unit vectors, (Xi −
p)/(cid:107)Xi − p(cid:107), are nearly uniform on the unit sphere of the tangent space, Sd−1. For uniform
data on Sd−1, the expected angle between two random vectors is always π/2 (regardless of
d), but the variance of this angle decreases rapidly with d. Formula (3) gives the value of
this variance for every dimension r. Since our results below show that the U -statistic, Uk,n,
will converge in probability to βd for the actual dimension of M at p, estimation of d by
choosing the r such that βr closest to Uk,n will be consistent. An additional fact that helps
in this convergence is that the variance of Uk,n, which depends on the fourth moment of the
angles, is also converging rapidly to zero.

The following subsection establishes useful facts about angles between random points on

the unit sphere Sd−1 of Rd and, in particular, about moments of the function

(2)

(3)

(4)

h(z, z(cid:48)) =

arccos (cid:104)z, z(cid:48)(cid:105) −

(cid:16)

(cid:17)2

π
2

when computed on data uniformly distributed on Sd−1. Section 3, building on subsection
2.2, develops the theoretical results that serve as basis for the use of Uk,n on manifolds.

2.2. Angle-variance statistics for pairs of uniform points on Sd−1.

Lemma 2.1 (Angles between uniform vectors). Let Z1, Z2 be two independent vectors
with the uniform distribution on the unit sphere Sd−1 ⊆ Rd and let Θd := arccos(cid:104)Z1, Z2(cid:105) be
the angle between them. The following statements hold:

(1) The distribution of Θd is given by

P(Θd ≤ α) =

(cid:82) α
0 sind−2(φ)dφ
(cid:82) π
0 sind−2(φ)dφ
3

.

(2) The moment generating function of Θd, denoted by φd−2(s) := E (cid:2)esΘd(cid:3) is given by

φ2k(s) = esπ−1
sπ

(cid:81)k

j=1

(2j)2
(2j)2+s2

φ2k+1(s) = esπ+1
2(s2+1)

(cid:81)k

j=1

(2j+1)2
(2j+1)2+s2

according to whether d − 2 is even or odd respectively.

(3) In particular E[Θd] = π

2 for all d and Var[Θd] = βd where

βd :=





π2
4
π2
12

− 2 (cid:80)k

j=0

− 2 (cid:80)k

j=1

1
(2j + 1)2

1
(2j)2

if d − 2 = 2k + 1 is odd or

if d − 2 = 2k is even.

(4) The variance of the centered squared angle σ2

σ2
d =






− π4

8 + 12

k
(cid:80)
j=0

k
(cid:80)
j=1

(cid:32)

π2
4 − 2

1

(2j+1)4 + 2
(cid:32)

k
(cid:80)
j=0

1
(2j + 1)2
(cid:33)2

k
(cid:80)
j=1

1
(2j)2

− π4

120 + 12

1
(2j)4 + 2

π2
12 − 2

if d − 2 = 2k.

d := Var (cid:0)Θd − π
(cid:33)2

2

(cid:1)2 is given by

if d − 2 = 2k + 1 or

Proof.

(1) Passing to polar coordinates r, φ1, . . . , φd−1 with r ≤ 0, 0 ≤ φj ≤ π for
1 ≤ j ≤ d − 2 and 0 ≤ φd−1 ≤ 2π. The probability that Θd ≤ α is precisely the
fraction of the surface area of the sphere deﬁned by the inequality 0 ≤ φ1 ≤ α. Since
the surface element of the sphere is given by

dS = sind−2(φ1) sind−3(φ2) . . . sin(φd−2)dφ1 . . . dφd−1

the probability is given by
0 · · · (cid:82) π
(cid:82) π
0 · · · (cid:82) π
(cid:82) π

(cid:82) α
0
(cid:82) π
0

0

0

(cid:82) 2π
0 dS
(cid:82) 2π
0 dS

=

(cid:82) α
0 sind−2(φ)dφ
(cid:82) π
0 sind−2 φdφ

as claimed.

(2) We begin with a claim

Claim 2.2. Let u : R → R be a C 2-function and deﬁne
(cid:82) α
0 u(x)n sind−2(φ)dφ
Ad

Ed(u(x)) :=

where Ad := (cid:82) π

0 sind−2 φdφ. Then, the following recursion formula holds

d Ed(u(x)) = d Ed−2(u(x)) −

Ed(u(cid:48)(cid:48)(x)).

1
d

Proof. Using integration by parts one can show a recursive formula for Ad and con-
clude that

Ad :=

(cid:40) (2k)!
(2k d!)2
(2kk!)2
(2k+1)!)

π
2

if d − 2 = 2k or

if d − 2 = 2k + 1.

Applying integration by parts twice and using our formula for Ad gives the result. (cid:3)

4

In particular if we take u(x) = esx we get
(cid:19)

(cid:18) d2

Ed(esx) =

Ed−2(esx).

d2 + s2

(3) All densities are, like sine, symmetric around π

As a result we obtain the stated closed formula for the moment generating function.
2 and the ﬁrst statement follows.
To ease the computations we introduce the cumulant-generating function ψd−2 =
log(E(esΘd)). Then it is immediate that Var(Θd) = ψ(cid:48)(cid:48)
d−2(0). We consider two cases,
d even and odd, ﬁrst let us assume that d = 2k + 2. Then, we write the cumulant-
generating function as

ψd−2(s) = log

(cid:19)

(cid:18) esπ − 1
sπ

+

(cid:123)(cid:122)
t(s)

(cid:125)

k
(cid:88)

j=1
(cid:124)

(cid:124)

(cid:18) (2j)2

log

(2j)2 + s2
(cid:123)(cid:122)
r(s)

(cid:19)

.

(cid:125)

After some dry algebra we get t(cid:48)(cid:48)(0) = π2
result for the even case. The odd case follows from an analogous argument.

12 and r(cid:48)(cid:48)(0) = −2 (cid:80)k

1
(2j)2 , which gives the

j=1

(4) Let µj be the jth moment of the random variable (cid:0)Θd − π

(cid:1), i.e. µj = E (cid:0)Θd − π

(cid:1)j.

2

It is well known that µ2 = ψ(cid:48)(cid:48)(0) and µ4 = ψ(4)(0) + 3 (ψ(cid:48)(cid:48)(0))2 . Therefore,

(cid:16)

Var

Θd −

(cid:17)2

π
2

= µ4 − µ2

2 = ψ(4)(0) + 2 (ψ(cid:48)(cid:48)(0))2 .

Again, consider two cases: d even and d odd. Suppose d = 2k − 2, just as before, we
calculate t(4)(0) = − π4
1
(2j)4 . Substituting both these into (5)
yields the claim. A similar argument can be applied to the odd case.

120 and r(4)(0) = 12 (cid:80)k

j=1

At ﬁrst glance the formulas for βd and σd might seem a little complicated. In order to
derive our results we need tangible decrease rates in terms of the dimension. The following
claim gives us an easy way to interpret these quantities.

Claim 2.3. The following bounds hold for βd and σ2
d:

moreover the upper bound for σ2

d holds for all d ≥ 1.

Proof. We distinguish two cases according on whether d ≥ 1 is even or odd. If d is even, we
can deﬁne k by the equality d − 2 = 2k and compute

1
d

≤ βd ≤

1
d − 1

1
2d2 ≤ σ2

d ≤

2
(d − 1)2

for d ≥ 1,

for d ≥ 4,

βd = 2

1
(2j)2 .

∞
(cid:88)

j=k+1
5

2

(5)

(cid:3)

(6)

(7)

Since this series consists of monotonically decreasing terms and d ≥ 2 we conclude that

1
d

=

1
2k + 2

(cid:90) ∞

1

= 2

(2x)2 dx ≤ 2

k+1

∞
(cid:88)

j=k+1

1
(2j)2 ≤ 2

(cid:90) ∞

1

(2x)2 dx =

1
2k + 1

=

1
d − 1

k+ 1
2

as claimed. On the other hand, notice that the other term concerning the variance can be
written as

12

k
(cid:88)

j=1

1
(2j)4 −

π4
120

= −12

∞
(cid:88)

j=k+1

1
(2j)4 ,

which again can be bound by

2
d3 =

1

4(k + 1)3 = 12

(2x)4 dx ≤ 12

k+1

(cid:90) ∞

1

∞
(cid:88)

j=k+1

1
(2j)4 ≤ 2

(cid:90) ∞

1

k+ 1
2

(2x)2 dx =

2
(2k + 1)3 =

2
(d − 1)3 .

Then, we get

1
2d2 ≤

2
d2 −

2

(d − 1)3 ≤ σ2

d ≤

2
(d − 1)2 −

2
d3 ≤

2
(d − 1)2

where the ﬁrst inequality follows since d ≥ 4. The case when d is odd is proven similarly. (cid:3)

3. Theoretical foundations

3.1. Statement of results. In this subsection we state the theoretical results that serve
as basis for the proposed methodology. Proofs are given in the following subsection. The
setting is the following: An i.i.d. sample, X1, . . . , Xn, is available from a distribution P on
Rm. Additionally we have access to a distingushied point p, and near this point the data live
on a Riemannian C 2 manifold M , of dimension d < m. Furthermore, at p the distribution
P has a Lipschitz continuous non-vanishing density function g, with respect to the volume
measure on M . Without loss of generality, we assume that p = 0. Then, we have

Proposition 3.1 (Behavior of nearest neighbors). For a positive constant C, deﬁne
k = (cid:100)C log(n)(cid:101) and let R(n) = Lk+1(0) be the euclidean distance in Rm from p = 0 to its
(k + 1)-st nearest neighbor in the sample X1, . . . , Xn. Deﬁne BR(n)(0) to be the open ball of
radius R(n) around 0 in Rm. Then, the following holds true:

(1) For any suﬃciently large C > 0, we have that, with probability one, for large enough

n (n ≥ n0, for some n0 depending on the actual sample), R(n) ≤ r(n), where

(cid:32)(cid:18) log(n)

(cid:33)

(cid:19) 1

d

r(n) := O

n

is a deterministic function that only depends on the distribution P at p and C.

(2) Conditionally on the value of R(n), the k-nearest-neighbors of 0 in the sample X1, . . . , Xn,
have the same distribution as an independent sample of size k from the distribution
with density gn, equal to the normalized restriction of g to M ∩ BR(n)(0).

In what follows, with a slight abuse of notation, we will write X1, X2, . . . , Xk to denote
the k nearest neighbors of 0 in the sample and assume that these follow the distribution with
density gn of Proposition 3.1. Let π : Rm → TpM be the orthogonal projection onto the
(embedded) tangent space to M at p = 0. For a nonzero X ∈ Rm, let W := π(X), (cid:98)X := X
6

(cid:107)X(cid:107)

and (cid:99)W := W
space of M at 0.

(cid:107)W (cid:107). (cid:99)W takes values in the (d − 1)-dimensional unit sphere Sd−1 of the tangent

Our ﬁrst Lemma bounds the diﬀerence between the inner products (cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) and (cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)
In this Lemma, the random nature of the Xi is

in terms of the length of projections.
irrelevant.

Lemma 3.2 (Basic projection distance bounds). For any X, X1, X2 ∈ M :

(1) (cid:107)X − πX(cid:107) = O((cid:107)πX(cid:107)2)
(2) (cid:107) (cid:98)X − (cid:99)W (cid:107) = O((cid:107)πX(cid:107))
(3) The cosine of the angle between X1 and X2 is close to that between W1 and W2. More

precisely,

|(cid:104) (cid:99)X1, (cid:99)X2(cid:105) − (cid:104)(cid:99)W1, (cid:99)W2(cid:105)| ≤ Cr
for some C ∈ R, whenever r ≥ (cid:107)π(Xi)(cid:107) for i = 1, 2.

Using Lemma 3.2, we can establish the following approximation. Let X1, . . . , Xk be the
k-nearest-neighbors from the sample to p = 0 in Rm . Deﬁne Wi and (cid:99)Wi as above and let
Vk,n be given by the formula

Vk,n :=

1
(cid:1)
(cid:0)k
2

(cid:88)

(cid:16)

(cid:68)

(cid:69)

arccos

(cid:99)Wi, (cid:99)Wj

−

(cid:17)2

.

π
2

1≤i<j≤k

Proposition 3.3 (Approximating the statistic via its tangent analogue). For k =
C log(n), as above, we have

(1) The sequence k(Uk,n − Vk,n) converges to 0 in probability as n → ∞.
(2) limn→∞ E (Uk,n − Vk,n) = 0.

When X comes from the distribution producing the sample, but is restricted to fall very
close to 0, the distribution of πX will be nearly uniform in a ball centered at 0 in TpM . This
will allow us to establish a coupling between the normalized projection (cid:99)W and a variable
Z, uniformly distributed on the unit sphere of TpM , an approximation that leads to the
asymptotic distribution of Uk,n. Some geometric notation must be introduced to describe
these results. Since near 0, M ⊆ Rm is a Riemannian submanifold of dimension d, it
inherits, from the euclidean inner product in Rm, a smoothly varying inner product lp :
TpM × TpM → R, given by lp(u, v) = (cid:104)i∗(u), i∗(v)(cid:105), where i : M → Rm is the inclusion
with diﬀerential i∗. This metric determines a diﬀerential d-form ΩM which, in terms of
local coordinates ∂i for TpM and dual coordinates dxi of TpM ∗ with i = 1, . . . d, is given by
ΩM := (cid:112)det((cid:104)∂i, ∂j(cid:105))1≤i,j≤d)dx1 ∧ · · · ∧ dxd. The diﬀerential form endows M with a volume
measure ν(U ) = (cid:82)
U ΩM . We say that a random variable A on M has density g : M → R if
the distribution µA of A satisﬁes µA(D) = (cid:82)

D gΩM for all borel sets D in M .

If X is a random variable taking values on M with density g and r is a positive real number,
let X(r) be a random variable with distribution gr given by the normalized restriction of g
to M ∩ Br(0), that is:

gr(z) =

(cid:40)

(cid:82)

g(z)
Br (0)∩M gΩM
0, otherwise.

7

, if z ∈ M ∩ Br(0) and

Deﬁne W (r) := π(X(r)). The following geometric Lemma will be used for relating the

densities of X(r) and W (r).

Lemma 3.4 (Tangent space approximations). The following statements hold for all
suﬃciently small r and p = 0 in M .

(1) The map π : Br(0)∩M → π(Br(0)∩M ) is a diﬀeomorphism. Let Φ : Br(0)∩TpM →

Br(0) ∩ M be its inverse.

(2) The inclusion π(Br(0) ∩ M ) ⊆ Br(0) ∩ TpM holds and moreover |λ(Br(0) ∩ TpM ) −

λ(π(Br(0) ∩ M ))| = O(r) where λ denotes the Lebesgue measure on TpM .

(3) The following equality holds:

(cid:115)

1 −

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

1≤i,j≤d

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= O(r)

Let D(r) be a random variable uniformly distributed in Br(0) ∩ TpM and note that Z =
(cid:98)D(r) := D(r)
(cid:107)D(r)(cid:107) is uniformly distributed on the unit sphere Sd−1, regardless of the value of r.
Our next Lemma shows that under weak hypotheses there is a coupling between W (r) and
D(r) which concentrates on the diagonal as r decreases.

Lemma 3.5 (Coupling). Let r denote a small positive number. With D(r) as above and
Z a random vector with the uniform distribution on the unit sphere, Sd−1 of TpM , if the
density g of X in M , near 0, is locally Lipschitz continuous and nonvanishing at 0, then the
following hold:

(1) There exists a coupling A(r) = (W (r), D(r)) and a constant C > 0 such that

P{W (r) (cid:54)= D(r)} ≤ Cr for all suﬃciently small r.
(2) There exists a coupling A(cid:48)(r) = ( (cid:91)W (r), Z) such that P

(cid:110) (cid:91)W (r) (cid:54)= Z

(cid:111)

≤ Cr for all

suﬃciently small r.

The previous Lemma leads to the asymptotic distribution of the statistic Uk,n.

Theorem 3.6 (Local Limit Theorem for angle-variance). Let k := (cid:100)C log(n)(cid:101) for the
constant C of the proof of Proposition 3.1 and assume X1, . . . , Xk are the k nearest neighbors
to p = 0 in the sample, with respect to the euclidean distance in Rm. If dim TpM = d then
the following statements hold:

(1) The equality limn→∞ E[Uk,n] = βd holds and
(2) The quantity k (Uk,n − βd) converges, in distribution, to that of (cid:80)∞

1,i − 1)
1,i are i.i.d. chi-squared random variables with one degree of freedom and

where the χ2
the λi are the eigenvalues of the operator A on L2(Sd−1) deﬁned by

i=1 λi(χ2

(Au)(x) =

(h(x, z) − βd) u(z) dµ(z)

(cid:90)

Sd−1

for u ∈ L2(Sd−1), where h(v, v(cid:48)) := (cid:0)arccos(v · v(cid:48)) − π
measure on Sd−1.

2

(cid:1)2 and µ denotes the uniform

This limit theorem is obtained by the various approximation steps given in the preliminary
results together with the classical Central Limit Theorem for degenerate U statistics, as
described in Chapter 5 of [28]. Depending on the relative values of the λi’s appearing in
8

the statement of the Theorem, it could happen that the limiting distribution just obtained
approaches a Gaussian distribution as the dimension increases (this would happen if the λi
were such that Lindeberg’s condition holds).

Although theoretical study of the λi’s is left for future work, we conjecture that as d
increases the limiting distribution converges to a Gaussian distribution. Numerical experi-
ments seem to support our conjecture, see Figure 1.

(a) d = 2

(b) d = 25

(c) d = 50

Figure 1. QQ-plots. As established in the proof of Theorem 3.6, the limiting
distribution is in fact the asymptotic distribution of k(En−βd), with En deﬁned
in (Equation 14). For this ﬁgure, we generate 10000 samples of the variable
k(En − βd), for k = 10d in dimensions d = 5, 25, and 50. The plots compare
the quantiles of the sample distribution against the standard normal quantiles.

In order to get a consistency result for our basic local dimension estimator, we will use

the following fact.

Corollary 3.7 (Variance convergence). Under the conditions stated at the beginning of
this subsection,

(cid:18)k
2
for d equal to the dimension of M near 0.

lim
n→∞

(cid:19)

Var Uk,n = σ2
d

Recall from Section 2 that our basic procedure estimates the dimension d as (cid:98)d, equal to

the integer r such that βr is closest to Uk,n. This procedure is consistent, as stated next.

Proposition 3.8 (Consistency of basic dimension estimator). As in Section 2, write (cid:98)d
for the basic estimator described above. Let d be the true dimension of M in a neighborhood
of p = 0. Then, in the setting of the present section,

P(failure) = P( (cid:98)d (cid:54)= d) → 0,

as n → ∞.

3.2. Proofs.

Proof of Proposition 3.1.

Let us recall a probability bound for the Binomial distribution. For N an integer valued
random variable with Binomial(n, p) distribution and expected value λ = np, one of the
9

Chernoﬀ-Okamoto inequalities (see Section 1 in [16]) states that, for t > 0, P(N ≤ λ − t) ≤
exp(−t2/2λ). Letting t = λ/2, we get

P(N ≤ λ/2) ≤ exp(−λ/8).
(8)
For ﬁxed and small enough r > 0, let Br(0) denote the ball of radius r around 0 ∈ Rm. For a
random vector X, with the distribution P of our sample X1, X2, . . . , Xn, by our assumptions
on P and M near 0, we have that

P(X ∈ Br(0) ∩ M ) ≤ ανdrd,
where νd is the volume (Lebesgue measure) of the unit ball in Rd and α is a positive number.
Let N = Nr denote the amount of sample points that fall in Br(0) ∩ M . We have that
λ = E(N ) ≤ ανdrdn. We choose r such that λ ≤ C ln n, for a constant C to be speciﬁed in
a moment. Then, by (8), we get

(cid:18)

P

(cid:19)

C
2

N ≤

ln n

≤ exp(−C ln n/8) =

(cid:19)C/8

(cid:18) 1
n

(9)

Pick any value of C > 8. For this choice, the bound in (9) will add to a ﬁnite value when
summed over n. By the Borel-Cantelli Lemma, the inequality N > C
2 ln n will hold for all
n suﬃciently large. It follows that if k = C
2 ln n, the k-nearest-neighbors of 0 in the sample,
will fall in Br(0) for every n suﬃciently large and the chosen value of r, namely

r = r(n) =

(cid:19)1/d

(cid:18)C ln n
ανdn

which is OPr((ln n/n)1/d)). The proof of the ﬁrst part of the Proposition ends by renaming
C.
The statement of the second part of Proposition 3.1 is intuitive and has been used in the
literature without proof. Luckily, Kaufmann and Reiss [18] provide a formal proof of these
type of results in a very general setting. In particular, (ii) of Proposition 3.1 holds by formula
(cid:50)
(6) of [18].

Proof of Lemma 3.2
By an orthogonal change of coordinates, we can assume that TpM is spanned by the ﬁrst
d basis vectors in Rm. The projection π : M → TpM is a diﬀerentiable function whose
derivative at p = 0 is the identity. By the implicit function theorem we can conclude that
there exists an r > 0, such that π : Br(0) ∩ M → π(Br(0) ∩ M ) is a diﬀeomorphism and that
M admits, near p a chart Φ : Br(0) ∩ TpM → M of the form

Φ(z1, . . . , zd) = (z1, . . . , zd, F1(z1, . . . , zd), . . . , Ft(z1, . . . , zd))

(10)

where m = d + t, Φ(0) = p = 0 and such that ∂Fi
(0) = 0 for 1 ≤ i ≤ t and 1 ≤ j ≤ d. As
∂zj
a result, the euclidean distance between a point of M near 0 and the tangent space at 0 is
given, in the local coordinates z, by

d(z1, . . . , zd) =

F 2

i (z).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

t
(cid:88)

i=1

10

We will prove that there exists a constant K such that, for all suﬃciently small δ > 0 and
all z with (cid:107)z(cid:107) ≤ δ the inequality d(z) ≤ K(cid:107)z(cid:107)2 holds. By Applying Taylor’s Theorem
at 0 to the diﬀerentiable function d(z) we conclude, since Φ(0) = 0 and ∂Fi
(0) = 0, that
∂zj
the constant and linear term vanish from the expansion. This proves the claim because
(cid:107)z(cid:107)2 = (cid:107)π(Φ(z))(cid:107)2. Assume K is a constant which satisﬁes (cid:107)X − πX(cid:107) ≤ K(cid:107)πX(cid:107)2. Thus

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)πX(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ K(cid:107)πX(cid:107).

(11)

(12)

On the other side,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

X
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)X(cid:107)

−

1
(cid:107)πX(cid:107)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (cid:107)X(cid:107)

=

1
(cid:107)πX(cid:107)

1
(cid:107)πX(cid:107)

|(cid:107)X(cid:107) − (cid:107)πX(cid:107)| ≤

(cid:107)X − πX(cid:107) ≤ K(cid:107)πX(cid:107).

Altogether,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

X
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)πX(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2K(cid:107)πX(cid:107),

where the ﬁrst inequality is just the triangle inequality and the second one follows from (11)
and (12). The third item in the Lemma follows immediately from the triangle inequality
(cid:50)
and the second item by adding and subtracting (cid:104)(cid:99)Xi, (cid:99)Wj(cid:105).

Remark 3.9. The quadratic term of G is the second fundamental form of M and, therefore,
the constant K can be chosen to be the largest sectional curvature of M at p.

Before proving Proposition 3.3, we need a Lemma on the behavior of the arccos function.

Lemma 3.10. Suppose that −1 ≤ c1 ≤ c2 ≤ 1 and let δ = c2 − c1 be suﬃciently small (for
our purposes it suﬃces to have δ ≤ 1/4). Then,

| arccos(c2) − arccos(c1)| ≤ 2(cid:112)|c2 − c1|

Proof. Assume ﬁrst that both c1 and c2 are positive. We have

| arccos(c2) − arccos(c1)| =

(cid:90) c2

c1

√

1
1 − x2

dx.

Using that the integrand in the last expression is increasing in [0, 1] and by the change of
variables u = 1 − x, we get

| arccos(c2) − arccos(c1)| ≤

(cid:90) 1

1−δ

√

1
1 − x2

dx =

(cid:90) δ

0

1
(cid:112)u(2 − u)

du ≤

(cid:90) δ

0

1
√
u

du

since 2 − u ≥ 1 for u ≤ δ. From this last bound, the result follows in this case by integration.
The argument for the case in which both c1 and c2 are negative is identical, by symmetry. In
the case c1 ≤ 0 ≤ c2, both c1 and c2 fall in a ﬁxed interval ([−1/4, 1/4]) where the derivative
(cid:3)
of arccos is bounded and the result follows easily.

11

Proof of Proposition 3.3
To prove part (1), putting together Proposition 3.1 and Lemma 3.2 we have

max
i≤k

(cid:107)(cid:99)Xi − (cid:99)Wi(cid:107) = OPr(r(n)) = OPr

(cid:19)1/d(cid:33)

(cid:32)(cid:18) ln n
n

From this, it follows easily that

and, using Lemma 3.10 we get

max
i<j≤k

|(cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) − (cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)| = OPr(r(n)),

| arccos(cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) − arccos(cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)| = OPr((cid:112)r(n))

max
i<j≤k

The bound is preserved by the application of the function u (cid:55)→ (u − π/2)2 (since the function
is locally Lipschitz) and by taking averages over all pairs, and we get
(cid:19)1/2d(cid:33)

Uk,n − Vk,n = OPr((cid:112)r(n)) = OPr

.

(13)

(cid:32)(cid:18) ln n
n

The result follows by observing that, for the value of k considered,

k OPr

(cid:32)(cid:18) ln n
n

(cid:19)1/2d(cid:33)

= oPr(1).

To prove (2), notice that from part (1) it is immediate that |Uk,n − Vk,n| converges to zero
in probability, which implies that limn→∞ E (Uk,n − Vk,n) = 0, since Uk,n − Vk,n is a bounded
(cid:50)
random variable.

Proof of Lemma 3.4
Recall, from the proof of Lemma 3.2, that for r > 0, small enough, the projection π :
Br(0) ∩ M → π(Br(0) ∩ M ) is a diﬀeomorphism and that M admits, near p a chart (inverse)
Φ : Br(0) ∩ TpM → M of the form given in (10) and satisfying that Φ(0) = p = 0 and such
that ∂Fi
(0) = 0 for 1 ≤ i ≤ t and 1 ≤ j ≤ d. Also from that proof, recall that there exists a
∂zj
constant K such that for small enough δ > 0 and all z with (cid:107)z(cid:107) ≤ δ, we have d(z) ≤ K(cid:107)z(cid:107)2,
where d(z) is the distance between a point z ∈ M and its projection on TpM . It follows
that the image π(Br(0) ∩ M ) contains a ball of radius r(cid:48) < r such that r − r(cid:48) = O(r2) and
therefore

Br(0) ∩ TpM ⊇ π(Br(0) ∩ M )) ⊇ Br(cid:48)(0) ∩ TpM,
proving part (2) of the Lemma, since the volume of the ﬁrst and last term diﬀer by at
= ei + (cid:80)m−d
most O(r2). For part (3) note that ∂Φ
are O(r2) the inner
∂zi
(cid:113)
products (cid:104) ∂Φ
, ∂Φ
(cid:105)
∂zj
∂zi
(cid:50)
is 1 + O(r) as claimed.

(cid:105) are 1+O(r2) if i = j and O(r2) otherwise and we conclude that

ed+t. Since ∂Ft
∂zi

, ∂Φ
∂zj

(cid:104) ∂Φ
∂zi

∂Ft
∂zi

t=1

Proof of Lemma 3.5
The total variation distance between two probability measures µ and ν, deﬁned as (cid:107)µ −
ν(cid:107)T V := supA |µ(A) − ν(A)|, satisﬁes

(cid:107)µ − ν(cid:107)T V = inf{P{X (cid:54)= Y } : A = (X, Y )}
12

where the inﬁmum runs over all couplings A = (X, Y ) of random variables X, Y with dis-
tributions given by µ and ν, respectively (see, for instance, page 22, Chapter 1 of [34]).
Moreover, if µ and ν are given by densities g1, g2 then the following inequality holds

(cid:107)µ − ν(cid:107)T V ≤ (cid:107)g1 − g2(cid:107)L1.
We will prove the ﬁrst part of the Lemma by bounding the L1-norm of the diﬀerence of
the densities of W (r) and D(r). Recall the deﬁnition of W (r) right before the statement of
Lemma 3.4. The diﬀerence of the two densities is given by

(cid:90)

Br(0)∩TpM

(cid:12)
(cid:12)
(cid:12)
(cid:12)

hr −

1
λ(Br)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dλ

where hr denotes the density of W (r) with respect to the d-dimensional Lebesgue measure
λ in TpM and Br := Br(0) ∩ TpM . More precisely, deﬁning Φ as in Lemma 3.4 the density
of W (r) is given by

hr(u) = gr(Φ(u))

det

(cid:115)

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

(u)

1≤i,j≤d

Since g is locally Lipschitz continuous there exists a constant K such that

g(0) − K1r ≤ g(Φ(u)) ≤ g(0) + K1r.

By Lemma 3.4 there exist constants K2, K3 such that the following inequalities hold for
u ∈ Br:

(cid:115)

1 − K2r ≤

det

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

1≤i,j≤d

(u) ≤ 1 + K2r and

λ(Br) − K3r ≤ λ(π(Br(0) ∩ M )) ≤ λ(Br) + K3r
Combining these inequalities we conclude that there exists a constant ˜K such that for all
u ∈ Br

λ(Br)(g(0) − ˜Kr) ≤

gΩ ≤ λ(Br)(g(0) + ˜Kr).

(cid:90)

Br(0)∩M

As a result the inequality

1
λ(Br)

(cid:32)

g(0) − ˜Kr
g(0) + ˜Kr

(cid:33)

− 1

≤ hr −

1
λ(Br)

≤

1
λ(Br)

(cid:32)

g(0) + ˜Kr
g(0) − ˜Kr

(cid:33)

− 1

so, using the fact that g(0) > 0 we conclude that there exists a constant such that

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Br

hr −

1
λ(Br)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ Cr

as claimed, ﬁnishing the proof of the ﬁrst part of the Lemma.
For part (2), let A(cid:48)(r) be the random pair obtained from A(r) = (W (r), D(r)) by normalizing
(cid:17)
(cid:16) (cid:91)W (r), (cid:91)D(r)
its components, that is A(cid:48)(r) =
= O(r),
because this probability is bounded above by the probability that W (r) and D(r) diﬀer.
Note that the random vector Z = D(r)
(cid:107)D(r)(cid:107) is uniform on the unit sphere, and in particular its
(cid:50)
distribution is independent of the value of r.

(cid:107)W (r)(cid:107) (cid:54)= D(r)

and note that P

(cid:110) W (r)

(cid:107)D(r)(cid:107)

(cid:111)

13

For k = (cid:100)C log(n)(cid:101), as before, let Z1, . . . , Zk be an i.i.d. sample distributed uniformly on the
unit sphere Sd−1 of TpM and deﬁne

En :=

1
(cid:1)
(cid:0)k
2

(cid:88)

(cid:16)

1≤i<j≤n

arccos(cid:104)Zi, Zj(cid:105) −

(cid:17)2

.

π
2

(14)

In view of Proposition 3.3, in order to prove Theorem 3.6 it will suﬃce to show that the
limiting standardized distribution of the Vk,n deﬁned in that Proposition is the same as that
of En, and establish the asymptotics for En.

Proof of Theorem 3.6

By Proposition 3.1 part (1) on a set of probability 1 (on the set of inﬁnite samples of

data), for some n0, the inequality Rn := R(n) ≤ r(n) holds for n ≥ n0.
By Proposition 3.1 part (2), for i = 1, . . . , k the distribution of Wi = π(Xi), the projections
on TpM of the k nearest neighbors of 0 in the sample, is that of an independent sample
W1(Rn), . . . , Wk(Rn), with the Wj(Rn), j ≤ k as deﬁned before Lemma 3.4.

From the previous Lemma, conditionally on Rn, we have a coupling A(cid:48)

j(Rn) = ( (cid:92)Wj(Rn), Zj)
for each j ≤ k. These couplings can be taken such that the (cid:92)Wj(Rn), j ≤ k form an i.i.d.
sample and the same holds for the Zj’s. By the previous lemma, we have that for each j,
P( (cid:92)Wj(Rn) (cid:54)= Zj) ≤ Cr(n). Then, except for a set of measure 0, we get the following event
inclusion

{kVk,n (cid:54)= kEn} ⊆

{ (cid:92)Wi(Rn) (cid:54)= Zi}

k
(cid:91)

j=1

By the union bound, the probability of the rightmost event is bounded by Ck(n)r(n) which
goes to zero, as n goes to inﬁnity by the choice of k(n) and the value of r(n). It follows that
Vk,n and En have the same standardized asymptotic distributions, and being both random
variables bounded, it follows that limn→∞ E(Vk,n − En) = 0.

For part (2) of Theorem 3.6 it only remains to establish the limiting distribution of En.
This statistic falls in the framework of classical U -statistics, that have played an important
role in the theory of many non-parametric procedures. See for instance [26] for several
applications of the theory of U -statistics. Even an empirical processes theory is available
for U -processes, see for instance [2], which has found application in the study of notions of
multivariate depth. Still, we will only require the classical theory, as exposed in Chapter 5
of [28] and Chapter 3 of [26].

Recall the deﬁnition of the kernel h in Equation 4. By the symmetry of the uniform
distribution for three independent vectors, Z1, Z2, Z3 with uniform distribution on Sd−1, it
can be easily veriﬁed that,

This means that the U -statistic associated to h is degenerate. It follows (see the variance

calculation in [26]) that

E (cid:0)h(Z1, Z2)h(Z1, Z3) − β2

(cid:1) = 0.

d

(cid:19)

(cid:18)k
2

Var(En) = Var(h(Z1, Z2))

(15)

(16)

and by the Theorem for degenerate U -statistics in Section 5.5 of [28], part (2) of our Theorem
(cid:50)
follows.

14

Proof of Corollary 3.7
By (16), it suﬃces to show that k2 lim(EU 2
k,n − EE2
n) = 0, for En as in the proof of Theorem
3.6. Applying equation (13) and the facts that the function h is bounded and that k = O(ln n)
on a set of probability 1, we have

k2EU 2

k,n ≤ k2EV 2

k,n + O

ln2 n

(cid:32)

(cid:19)1/2d(cid:33)

.

(cid:18)ln n
n

The opposite inequality (interchanging the roles of Uk,n and Vk,n) is obtained by the same
reasoning, and we get

(17)
By the coupling argument of the proof of Theorem 3.6, we have that Vk,n and En might
diﬀer at most on a set of measure O(ln(n) r(n)). Taking expectations on the sets were they
coincide and diﬀer, we obtain

k,n) = 0.

lim k2(EU 2

k,n − EV 2

k2 EV 2

k,n ≤ k2 EE2

n + k2 O(ln(n) r(n)).

Observing that the second term in the right hand side of this inequality goes to zero, as n
grows, and that the reverse inequality is obtained similarly, we conclude

and the result follows by combining (17) and (18).

lim k2(EV 2

k,n − EE2

n) = 0,

(18)
(cid:50)

Proof of Proposition 3.8.
Let d be the true value of the dimension. βd+1 is the expected value closest to βd and, by
the proof of Claim 2.3, (βd − βd+1)/2 ≥ 1/(d − 1)2. For the basic procedure to incur in error
it is necessary that |Uk,n − βd| ≥ |Uk,n − βd+1|, which means
βd − βd+1
2

|EUk,n − βd| + |Uk,n − EUk,n| ≥

1
(d − 1)2 .

≥

1

Since |EUk,n − βd| converges to zero, the condition above requires that |Uk,n − EUk,n| ≥
1/
2(d−1)2 , for n large enough. The probability of this last event is bounded as follows. Let c
be a sup norm bound for h( (cid:99)X1, (cid:99)X2) − Eh( (cid:99)X1, (cid:99)X2), for the kernel h given in (4) and (cid:99)Xi as in
Lemma 3.2. Clearly, c is bounded above by π2/4. By Bernstein’s inequality for U -statistics
[2, Proposition 2.3(a)], we get

(cid:18)

P

|Uk,n − EUk,n| ≥

(cid:19)

1
2(d − 1)2

(cid:32)

≤ 2 exp

−k/(8(d − 1)4)
2Var(h( (cid:99)X1, (cid:99)X2)) + 2c

6(d−1)2

(cid:33)

Now, using Claim 2.3 and Corollary 3.7, we have, after some calculations,

(cid:18)

P

|Uk,n − EUk,n| ≥

(cid:19)

1
2(d − 1)2

(cid:32)

≤ 2 exp

−k/(8(d − 1)4)
(d−1)2 + π2

12(d−1)2

5

(cid:33)

≤ 2 exp

(cid:18)

−k
47(d − 1)2

(cid:19)

This bound goes to zero as n (and k) grow to inﬁnity, ﬁnishing the proof.

(19)

(cid:50)

Forcing the estimation error bound in (19) to be less that a given δ > 0 will give a value of
k of the order of Ad 2 ln(2/δ), for some constant A, reﬂecting that precise estimation is more
demanding, in terms of sample size, as the dimension d grows.

15

4. Estimators

In this section we present two dimension estimators based on the statistic Uk,n. First,
a local estimator that gives the dimension of M around a distinguished non-singular point
p ∈ M is discussed. Then, the case in which the manifold is equidimensional is considered,
by building upon our local estimator to propose a global dimension estimator. Some imple-
mentation issues are discussed and in the following section we evaluate the performance of
our estimators. The programming code used in these experiments is publicly available, it
can be found at https://github.com/mateodd25/ANOVA_dimension_estimator.

Algorithm 1: Local dimension estimation
Data: k ∈ N+ and X1, . . . , Xn, p ∈ M ⊆ Rm
Result: Estimated dimension (cid:98)d at p ∈ M
Find the k-nearest neighbors to p;
Use these neighbors to compute Uk,n as in (2);
Choose (cid:98)d associated with Uk,n;

4.1. Local estimators. The theory presented in Section 3 suggests that k ∼ log(n) should
be a good choice, asymptotically-speaking, for the number of neighbors to consider in the
local dimension estimation procedure. However, one could potentially leverage prior knowl-
edge of the structure of the problem to set this parameter diﬀerently. In our implementation
we set it to k = round(10 log10(n)).

In our theoretical analysis presented above, it was assumed that we are given a center
point p where the local dimension is to be estimated. A natural question that arises in
practice is the following: given a sample, how to select good center points. In Section 4.2
we present a simple heuristic to select “good” centers.

Both estimators presented in what follows are based on the Algorithm 1. The diﬀerence
between the estimators considered lies on the last line of the algorithm, namely, on how to
pick the dimension estimator, given Uk,n. Next, the two diﬀerent rules to execute this step
are discussed.

4.1.1. Basic estimator. Since Uk,n converges in probability to βd, a natural way to estimate
the dimension from Uk,n is to set

(cid:98)dbasic := arg min
d∈[Dmax]

|βd − Uk,n|,

where Dmax is the ambient dimension or some bound we know a priori on the dimension
of the manifold. Interestingly, such a rule is fairly accurate, as established in Proposition
3.8. Another advantage of this estimator is that there is no need to train it, since all the
quantities involved have been analytically computed (and presented in Section2).

Remark 4.1. Classical discriminant analysis results (see, for example, Section 4.1 in [11])
would advice to incorporate available variance information on the selection of (cid:98)d, by choosing

(cid:98)ddisc := arg max{d | Uk,n ≥ ηd}

where

ηd = βd +

(βd−1 − βd).

σd
σd + σd+1

16

Algorithm 2: Global dimension estimation
Data: c ∈ N+, k ∈ N+ and X1, . . . , Xn ∈ M ⊆ Rm
Result: Estimated dimension (cid:98)d of M
Choose c centers p1, . . . , pc from the sample;
Apply Algorithm 1 to each center pi, let (cid:98)di be its output;
Set (cid:98)d to the median of { (cid:98)di}c
i=1;

Still, simulation evaluations (not included) show that (cid:98)ddisc and (cid:98)dbasic have a very similar
performance in practice (and also in theory, since both are consistent). Thus, we prefer to
use the later, being the simpler one.

1

, . . . , Y (d)

4.1.2. Kernel-based estimator. For our second estimator we start by simulating multiple
instances Y (d)
M of the random variable k(En − βd), for a large value of M (=5000,
for instance) and with En as deﬁned in equation (14), for each dimension d ∈ [Dmax] =
{1, . . . , Dmax}. From these data, the density ˆf (d)
k , of k(En − βd), is estimated, for each d, as
M
(cid:88)

(cid:32)

(cid:33)

ˆf (d)
k (y) =

1
M h

ϕ

y − Y (d)
h

i

i=1
where ϕ(·) is the standard Gaussian density and the parameter h (the “bandwidth”) can be
set at h = (4/3M )1/5. This choice of bandwidth guarantees consistent density estimation (see
Section 4.1 in [4]). Then, a Bayesian classiﬁcation procedure with uniform prior distribution
on the set [Dmax], would select the dimension as that for which the kernel density estimator
is maximized at the standardized Uk,n, namely
ˆf (d)
k (k(Uk,n − βd)).

(cid:98)dker = arg max
d∈[Dmax]

(20)

Notice that the simulations described above need to be performed only once for each dimen-
sion, since they are made on uniform data on Sd−1 and do not depend on the particular data
being studied.

4.2. Global estimators. We now turn our attention to extending the local dimension es-
timation algorithm to a global one. Assuming that M is equidimensional, the local method
can be extended by running multiple instances of Algorithm 1 on diﬀerent centers, and
combining the results, as outlined in Algorithm 2.

After getting dimension estimates at each center, one could use diﬀerent summary statistics
to choose the global dimension, such as the mean, the mode or the median. To make a method
robust against outliers, we chose to use the median. To decide about the parameter c we
ran a cross-validation algorithm. Empirically, it appears that c ∼ log(n) is a good choice for
this parameter.

4.2.1. Choosing centers. To pick the centers pi in Algorithm 2, we divide the sample into c
disjoint subsamples of approximately equal size. Assume, for simplicity of the exposition,
Inside each subsample we pick a center by assigning each point a score of
that c = 1.
centrality and then choosing the one with highest score. Scores are assigned through the
following procedure:

17

(1) For each coordinate i, we order the sample based on the i-th entry, let τi be the
permutation giving this ordering, that is, the i-th row in (Xτi(1), . . . , Xτi(n)) is non-
decreasing.

(2) Then, the centrality score of Xj is given by (cid:80)m

(cid:12)
(cid:12)
(cid:12) .
For the i-th coordinate, the weight function f gives the maximum scores to the point (or
points) such that τi(j) is closest to n/2 and thus, the mechanism used chooses as center a
point which for many components, appears near the center of these orderings.

i=1 f (τi(j)), where f (x) =

2 − 2(x−1)

(cid:12)
(cid:12)
(cid:12)

2n

1

4.2.2. Heuristic to discard centers. Finally, we present a simple heuristic for discarding some
of the selected centers, based on the mean of the angles between its neighbors, taking as
always, the point considered as origin. This is done in order to improve the performance of
the statistic. Consider again the angle

(cid:28) Xi − p
(cid:107)Xi − p(cid:107)
for each pair of nearest neighbors Xi, Xj of the point p ∈ M , as used in the basic deﬁnition
(2). Consider the average of these angles,

Xj − p
(cid:107)Xj − p(cid:107)

θi,j = arccos

(cid:29)

,

θ(p) =

1
(cid:1)
(cid:0)k
2

(cid:88)

θi,j.

1≤i<j≤k

If the manifold M , near p, is approximately ﬂat, θ(p) should be close to π/2, regardless of
the value of the dimension d, since π/2 is the expected value of the angle between uniformly
sampled points in every dimension and the U -statistic θ(p) should converge rapidly to this
expectation. Thus, when θ(p) is far from π/2, it can be taken as a suggestion of strong
curvature that is causing non-uniformity of the angles, and therefore, p might not be a good
point to consider for dimension estimation. For these reason, in our implementation, the user
is allowed to use this heuristic and discard a fraction of the centers pi with largest values of
|θ(pi) − π/2|. Experiments presented in the next section suggest that this heuristic is useful
when the manifold is highly curved.

5. Numerical results

We compare our methods against two powerful dimension estimators, DANCo [9] and
Levina-Bickel [20], using a manifold library proposed in [15]. The ﬁrst estimator is, ar-
guably, the state-of-the-art for this problem, while the second one is a classical well-known
estimator with great performance. To see a comparison between these and other estimators
we refer the reader to [8, 9].

Table 1 presents a brief description of the manifolds included in the study. Additionally
Table 2 contains a list of the parameters used for each one of the estimators. We compare
two error measures, namely the Mean Square Error (MSE) and the Mean Percentage Error
(MPE) which are deﬁned as

MSE( (cid:98)d) :=

( (cid:98)di − di)2

and

MPE( (cid:98)d) :=

1
T

T
(cid:88)

i=1

100
T

T
(cid:88)

i=1

| (cid:98)di − di|
di

where T is the number of trials included in the test and (cid:98)di and di are the estimated dimension
and the correct dimension of the ith trial, respectively.

18

For each one of the aforementioned manifolds, we draw T = 50 random samples with
n = 2500 data points and then compute the MSE and MPE of the following four esti-
mators: the global basic estimator (Basic), the global basic estimator combined with the
centers heuristic (B+H), the global kernel-based estimator (Kernel), the global kernel-based
estimator with centers heuristic (K+H), the Levina-Bickel estimator (LB), and the DANCo
estimator. Tables 3 and 4 summarize the results.

Table 1. Library of manifolds used for benchmark, for more details consult [15].

Description
Sphere S9
Aﬃne subspace
Nonlinear manifold
Nonlinear manifold
Helix
Nonlinear manifold
Swiss roll
Highly curved manifold
Full-dimensional cube
9-dimensional cube

Manifold d m
10
9
5
3
6
4
8
4
3
2
36
6
2
3
12 72
20 20
10
9
2
3 Ten-times twisted Mobius band
10 10
10
1

M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
M11
M12
M13

Multivariate Gaussian
Curve

Table 2. Parameters of each algorithm.

Method Parameters
ANOVA k = 34, c = 16
k1 = 10, k2 = 20
k = 10

LB
DANCo

In both these tables, the last column shows the average of the performance measure over
the examples. It is clear from these tables that the angle-variance methods introduced in the
present article do well, in terms of average performance, against the very strong competitors
considered. This is more evident when the measure of error is the MSE. Still, for many of the
manifolds considered, namely M1, M3, M4 (in this case tied with LB), M9 and M12, DANCo
clearly displays the best performance. The Levina-Bickel estimator is the best for manifold
M6, tying for ﬁrst with DANCo in M4, while the procedures proposed in this article show
the best performance in the cases of manifolds M8 and M10, having very good performance
also in cases M3, M5, M7 and M12. It is interesting that our methods do particularly well
in case M8, a high curvature manifold of a relatively high dimensional in a large dimension
ambient space. It does not appear to exist a signiﬁcant diﬀerence in performance between
the Basic procedure and the procedure that uses Kernel Density Estimation. On the other
hand, the introduction of the heuristics discussed in Section 4 turns out to be beneﬁcial for
our estimators in some of the relatively high dimensional cases, namely M9 and M12, while
19

Table 3. Rounded Mean Square Error for diﬀerent manifolds, last column
displays the average MSE over all the examples. The darker cells show the
best results in each column.

M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 Mean
1.72
1.11
Basic
0.95 0.00 0.58 0.03 0.00 0.67 0.00
2.14
0.79
B+H 1.09 0.00 0.66 0.28 0.00 1.30 0.01
1.27
1.10
Kernel 0.95 0.00 0.68 0.08 0.00 0.69 0.29
0.80
2.48
K+H 0.99 0.00 0.76 0.45 0.00 1.31 0.31
2.69
2.27
0.49 0.02 0.05 0.00 0.00 0.10 0.00
LB
2.11
DANCo 0.16 0.00 0.00 0.00 0.00 1.00 0.00 25.22

10.39 0.00 0.00 0.12 0.00
4.64
0.03 0.00 0.10 0.00
10.20 0.00 0.00 0.15 0.00
4.06
0.01 0.00 0.04 0.00
29.33 2.23 0.00 0.54 0.00
0.10 0.00 0.00 0.00
0.96

Table 4. Rounded Mean Percentage Error for diﬀerent manifolds, last col-
umn displays the average MPE over all the examples.

M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 Mean
9.00
1.00
4.93
Basic
10.56 0.00 15.50
5.95
9.83
8.25
B+H 11.33 0.00 17.75
6.45
7.75
2.50
Kernel 10.56 0.00 17.75
7.67
K+H 11.00 0.00 20.00 12.25 0.00 18.00 17.50 10.75
7.31
1.49
LB
5.09
0.00
DANCo

0.00 1.50 0.00
0.00 1.20 0.00
0.00 1.80 0.00
0.00 0.60 0.00
12.55 27.07 16.57 1.64 7.34 0.50
0.00 0.00 0.00
41.83

0.00
0.00 10.83
0.00 17.83
0.50
0.00 11.50 16.50

1.84
5.13
0.00 16.66

15.50
10.00
15.55
9.60

0.00
0.67
0.00
0.11

2.63
0.00

4.84
0.00

5.77
0.00

7.77
1.77

4.80

1.11

these heuristics degrade somehow the performance in the intermediate dimension cases, M4
and M6. In all other cases, the use of the heuristics for center selection and center elimination
does not appear to have a strong eﬀect.

References

[1] Aldous, D. and J. Shun (2010) Connected spatial networks over random points and a route-length

statistic. Statistical Science 25, 275 - 288.

[2] Arcones, M. A. and Gin´e, E. (1993) Limit Theory for U -Processes. Annals of Probability 21, No. 3,

1494-1542.

[3] Belkin, M. and Niyogi, P. (2004) Semi-supervised learning on Riemannian manifolds. Invited Paper

inMachine Learning. Special Issue on Clustering 56, 209-239.

[4] Bowman, A. W. and Foster, P. J. (1993) Adaptive smoothing and density-based tests of multivariate

normality. Journal of the American Statistical Association 88 No. 422, 529-537.

[5] Breiding, P., Kalisnik, S., Sturmfels, B. and Weinstein, M. (2018) Learning Algebraic Varieties from

Samples. arXiv:1802.094[math.AG]

[6] Brito, M. R., Quiroz, A. J. and Yukich, J. E. (2002) Graph theoretic procedures for dimension identiﬁ-

cation. Journal of Multivariate Analysis 81, 67-84.

[7] Brito, M. R., Quiroz, A. J. and Yukich, (2013) Intrinsic dimension identiﬁcation via graph-theoretic

methods. Journal of Multivariate Analysis 116, 263277.

[8] Campadelli, P., Casiraghi, E., Ceruti, C. and Rozza, A. (2015) Intrinsic Dimension Estimation: Relevant
Techniques and a Benchmark Framework. Mathematical Problems in Engineering 2015 Article ID:
759567, 21 pages.

[9] Ceruti, C., Bassis, S., Rozza, A., Lombardi, G., Casiraghi, E. and Campadelli, P. (2014) DANCo: An
intrinsic dimensionality estimator exploiting angle and norm concentration. Pattern Recognition 47, No.
8, 2569-2581.

20

[10] Costa, J. A., Girotra, A. and Hero, A .O. (2005) Estimating local intrinsic dimension with k-nearest
neighbor graphs. In IEEE/SP 13th Workshop on Statistical Signal Processing, 417-422. IEEE Conference
Publication.

[11] Devroye, Luc, Gy¨orﬁ, L´aszl´o and Lugosi, G´abor. (2013) A probabilistic theory of pattern recognition

(Vol. 31). Springer Science and Business Media.

[12] Duda, R. O., Hart, P. E. and Stork, D. G. (2001) Pattern Classiﬁcation. 2nd. edition. John Wiley and

Sons, New York.

189-208.

[13] Farahmand, A., Szepesv´ari, C. and Audibert, J-Y (2007) Manifold-adaptive dimension estimation. In
Proceedings of the 24th International Conference on Machine Learning, Z. Ghahramani, editor, 265-272.
ACM, New York.

[14] Grassberger, P. and Procaccia, I. (1983) Measuring the strangeness of strange attractors. Physica 9D,

[15] Hein, M. and Audibert, J.-Y. (2005) Intrinsic dimensionality estimation of submanifolds in Rd. In

Proceedings of the 22nd International Conference on Machine learning, 289-296, ACM.

[16] Janson, S. (2002). On concentration of probability. In Bollob´as, B. (Ed.) Contemporary Combinatorics,
10. Proceedings of the Workshop on Probabilistic Combinatorics at the Paul Erd¨os Summer Research
Center, Budapest, 1998, pp. 289-301.

[17] Johnson, M. E. (1987) Multivariate Statistical Simulation. John Wiley and Sons, New York.
[18] Kaufmann, E. and Reiss, R.-D. (1992) On Conditional Distribution of Nearest Neighbors. Journal of

Multivariate Analysis 42, 67-76.

[19] Kegl, B. (2003) Intrinsic dimension estimation using packing numbers. In Advances in Neural Infor-
mation Processing Systems, Volume 15, Eds. S. Becker, S. Thrun and K. Obermayer. M.I.T. Press,
Cambridge, Massachusetts.

[20] Levina, E. and Bickel, P. J. (2005) Maximum likelihood estimation of intrinsic dimension. In Advances
in Neural Information Processing Systems, Volume 17, Eds. L. K. Saul, Y. Weiss and L. Bottou.
[21] Lombardi et. al., 2011 Lombardi, G., Rozza, A., Ceruti, C., Casiraghi, E. and Campadelli, P. (2011).
Minimum Neighbor Distance Estimators of Intrinsic Dimension. In D. Gunopulos et al. (Eds.): ECML
PKDD 2011, Part II, LNAI 6912, pp. 374-389. Springer-Verlag. Berlin.

[22] Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979) Multivariate Analysis. Academic Press, New York.
[23] Penrose, M. D. and Yukich, J. E. (2001) Central limit theorems for some graphs in computational

geometry. Annals of Applied Probability 11, 1005-1041.

[24] Penrose, M. D. and Yukich, J. E. (2013) Limit theory for point processes in manifolds. Annals of Applied

Probability 23, No. 6, 2161-2211.

[25] Pettis, K. W., Bailey, T. A. Jain, A. K. and Dubes, R. C. (1979) An intrinsic dimensionality estimator
from near-neighbor information. IEEE Transactions on Pattern Analysis and Machine Intelligence 1,
25-37.

[26] Randles, R. H. and Wolfe, D. A. (1979) Introduction to the Theory of Nonparametric Statistics. John

[27] Roweis, S. T. and Saul, L. K. (2000) Nonlinear dimensionality reduction by locally linear embedding.

[28] Serﬂing, R. J. (1980) Approximation Theorems of Mathematical Statistics. John Wiley and Sons, New

[29] Sindhwani, V., Belkin, M. and Nigoyi, P. (2006) The Geometric Basis of Semi-supervised Learning.
Book chapter in Semi-supervised Learning, O. Chapelle, B. Sch¨olkopf and A. Zien, editors, M.I.T.
Press, Cambridge, Massachusetts.

[30] S¨odergren, A. (2011) On the distribution of angles between the N shortest vectors in a random lattice

Journal of the London Mathematical Society, 84, No. 3, 749-764.

[31] Sricharan, K. Raich, R. and Hero, A. O. (2010) Optimized intrinsic dimension estimation using near-
est neighbor graphs. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 5418-5421. IEEE Conference Publication.

[32] Steele, J. M., Shepp, L. A. and Eddy, W. F. (1987) On the number of leaves of a Euclidean minimal

spanning tree. Journal of Applied Probability, 24, 809-826.

Wiley and Sons, New York.

Science, 290, 2323-2326.

York.

21

[33] Tenenbaum, J. B., de Silva, V. and Langford, J. C. (2000) A global geometric framework for nonlinear

dimensionality reduction. Science 290, 2319-2323.

[34] Villani, C. (2009) Optimal Transport. Old and New. Volume 338 in series Grundlehren der matematischen

[35] Yukich, J. E. (1998) Probability Theory of Classical Euclidean Optimization Problems. Lecture Notes in

Wissenschaften. Springer. Berlin, Heidelberg.

Mathematics, 1675, Springer, New York.

Mateo D´ıaz, Center for Applied Mathematics, 657 Frank H.T. Rhodes Hall, Cornell

University, Ithaca, NY 14853

E-mail address: md825@cornell.edu

Adolfo J. Quiroz, Departamento de Matem´aticas, Universidad de los Andes, Carrera 1

No. 18a 10, Edificio H, Primer Piso, 111711 Bogot´a, Colombia

E-mail address: aj.quiroz1079@uniandes.edu.co

Mauricio Velasco, Departamento de Matem´aticas, Universidad de los Andes, Carrera 1

No. 18a 10, Edificio H, Primer Piso, 111711 Bogot´a, Colombia

E-mail address: mvelasco@uniandes.edu.co

22

8
1
0
2
 
y
a
M
 
4
 
 
]
T
S
.
h
t
a
m

[
 
 
1
v
7
7
5
1
0
.
5
0
8
1
:
v
i
X
r
a

LOCAL ANGLES AND DIMENSION ESTIMATION
FROM DATA ON MANIFOLDS

MATEO D´IAZ, ADOLFO J. QUIROZ, AND MAURICIO VELASCO

Abstract. For data living in a manifold M ⊆ Rm and a point p ∈ M we consider a statistic
Uk,n which estimates the variance of the angle between pairs of vectors Xi − p and Xj − p,
for data points Xi, Xj, near p, and evaluate this statistic as a tool for estimation of the
intrinsic dimension of M at p. Consistency of the local dimension estimator is established
and the asymptotic distribution of Uk,n is found under minimal regularity assumptions.
Performance of the proposed methodology is compared against state-of-the-art methods on
simulated data.

1. Introduction

Understanding complex data sets often involves dimensionality reduction. This is partic-
ularly necessary in the analysis of images, and when dealing with genetic or text data. Such
data sets are usually presented as collections of vectors in Rm and it often happens that
there are non-linear dependencies among the components of these data vectors.
In more
geometric terms these non-linear dependencies amount to saying that the vectors lie on a
submanifold M ⊆ Rm whose dimension d is tipically much smaller than m. The expression
manifold learning has been coined in the literature for the process of ﬁnding properties of
M from the data points.

Several authors in the artiﬁcial intelligence literature have argued about the convenience of
having methods to ﬁnd or approximate these low-dimensional manifolds [3, 13, 27, 29, 31, 33].
Procedures for achieving this kind of low dimensional representation are called manifold
projection methods. Two fairly successful such methods are Isomap of Tenenbaum, de Silva
and Langford [33] and the Locally Linear Embedding method of Roweis and Saul [27]. For
these and other manifold projection procedures, a key initial ingredient is a precise estimation
of the integer d, ideally obtained at low computational cost.

The problem of estimating d has been the focus of much work in statistics starting from
the pioneering work of Grassberger-Procaccia [14]. Most of the most recent dimension iden-
tiﬁcation procedures appearing in the literature are either related to graph theoretic ideas
[6, 7, 23, 24, 35] or to nearest neighbor distances [10, 20, 21, 25]. A key contribution of the
latter group is the work of Levina and Bickel [20] who propose a “maximum likelihood”
estimator of intrinsic dimension. To describe it let Lk(Xi) be the distance from the sample
point Xi to its k-th nearest neighbor in the sample (with respect to the euclidean distance
in the ambient space Rm). Levina and Bickel show that, asymptotically, the expected value

2010 Mathematics Subject Classiﬁcation. 62G05, 62H10, 62H30.
Key words and phrases. dimension estimation, local U -statistics, angle variance, manifold learning.

1

of the statistic

(cid:98)mk(Xi) :=

(cid:34)

1
k − 2

k−1
(cid:88)

j=1

log

Lk(Xi)
Lj(Xi)

(cid:35)−1

(1)

coincides with the intrinsic dimension d of the data. As a result, they propose the corre-
sponding sample average mk := n−1 (cid:80)n
i=1 (cid:98)mk(Xi) as an estimator of dimension. Asymptotic
properties of this statistic have been obtained in the literature (see [24, Theorem 2.1]) allow-
ing for the construction of conﬁdence intervals. Both the asymptotic expected value and the
asymptotic distribution are independent of the underlying density from which the sample
points are drawn and thus lead to a truly non-parametric estimation of dimension.

In addition to distances, Ceruti et al. propose in [9] that angles should be incorporated in
the dimension estimators. This proposal, named DANCo, combines the idea of norm concen-
tration of nearest neighbors with the idea of angle concentration for pairs of points on the
d-dimensional unit sphere.

The resulting dimension identiﬁcation procedure is relatively involved. The method com-
bines two ideas. On one hand it uses the Kullback-Leibler divergence to measure the distance
between the estimated probability density function (pdf) of the normalized nearest neighbor
distance for the data considered and the corresponding pdf of the distance from the cen-
ter of an r-dimensional unit ball to its nearest neighbor under uniform sampling. On the
other hand, it uses a concentration result due to S¨odergren [30], for angles corresponding to
independent pairs of points on a sphere.

The main contribution of this article is a new and simple dimension identiﬁcation pro-
cedure based solely on angle concentration. We deﬁne a U -statistic which averages angle
squared deviations over all pairs of vectors in a nearest neighbor ball of a ﬁxed point and
determine its asymptotic distribution. In the basic version of our proposed method there
is no need of calibration of distributions and moreover our statistic is a U -statistic among
dependent pairs of data points and it is well known that these oﬀer fast convergence to their
mean and asymptotic distribution.

Our method has been called ANOVA in the literature1, given that the U -statistic used,
Uk,n to be deﬁned below, is an estimator of the variance of the angle between pairs of vec-
tors among uniformly chosen points in the sphere Sd−1. Our main results are to prove the
consistency of the proposed method of estimation (Proposition 3.8) and the description of
the (suitably normalized) asymptotic distribution of the statistic considered (Theorem 3.6),
a result that is very useful in the construction of asymptotic conﬁdence intervals in dimen-
sion estimation. We describe our proposed method in Section 2 and provide its theoretical
justiﬁcation in Section 3. Sections 4 and 5 discuss the details of our implementation of the
dimension identiﬁcation procedure together with some empirical improvements. It also con-
tains the result of performance evaluations on simulated examples, including comparisons
with current state-of-the-art methods.

2. A U -statistic for dimension identification

2.1. Description of the statistic. Suppose our data form an i.i.d. sample, X1, . . . , Xn
from a distribution P on Rm with support on a Riemannian C 2 manifold M of dimension

1The term was coined by Breiding, Kalisnik, Sturmfels and Weinstein in [5] when describing an earlier

preliminary version of this article

2

d < m. Given a point p ∈ M , the question to be addressed is to determine the dimension d
of the tangent space of M at p using only information from sample points near p (we want
to allow for the value of d to depend on the point p and for M to be disconnected).

The simplest version of our dimension identiﬁcation procedure is described by the following

steps:

(1) For an appropriate value of the constant C, to be speciﬁed below, let k := (cid:100)C log(n)(cid:101).
Assume, relabeling the sample if necessary, that X1, . . . , Xk are the k nearest neigh-
bors of p in the sample, according to the euclidean distance in Rm.

(2) Deﬁne the angle-variance U -statistic, Uk,n, by the formula

Uk,n :=

1
(cid:1)
(cid:0)k
2

(cid:18)

(cid:88)

arccos

(cid:28) Xi − p
(cid:107)Xi − p(cid:107)

,

Xj − p
(cid:107)Xj − p(cid:107)

(cid:29)

−

(cid:19)2

,

π
2

1≤i<j≤k

where (cid:104)·, ·(cid:105) denotes the dot product on Rm.

(3) Estimate the unknown dimension d as (cid:98)d, equal to the integer r such that βr is closest
to Uk,n, for a suﬃciently large sample size n, where βr is the quantity deﬁned by

βr :=

(cid:40) π2

4 − 2 (cid:80)s
12 − 2 (cid:80)s

π2

j=0

j=1

1

(2j+1)2 if r − 2 = 2s + 1 is odd or
1
(2j)2 if r − 2 = 2s is even.

The key idea of our estimator goes as follows: For large n and the chosen value of k, the
nearest neighbors of p in the data set, behave as uniform data on a small ball around p in
the embedded tangent space of M at this point, and the corresponding unit vectors, (Xi −
p)/(cid:107)Xi − p(cid:107), are nearly uniform on the unit sphere of the tangent space, Sd−1. For uniform
data on Sd−1, the expected angle between two random vectors is always π/2 (regardless of
d), but the variance of this angle decreases rapidly with d. Formula (3) gives the value of
this variance for every dimension r. Since our results below show that the U -statistic, Uk,n,
will converge in probability to βd for the actual dimension of M at p, estimation of d by
choosing the r such that βr closest to Uk,n will be consistent. An additional fact that helps
in this convergence is that the variance of Uk,n, which depends on the fourth moment of the
angles, is also converging rapidly to zero.

The following subsection establishes useful facts about angles between random points on

the unit sphere Sd−1 of Rd and, in particular, about moments of the function

(2)

(3)

(4)

h(z, z(cid:48)) =

arccos (cid:104)z, z(cid:48)(cid:105) −

(cid:16)

(cid:17)2

π
2

when computed on data uniformly distributed on Sd−1. Section 3, building on subsection
2.2, develops the theoretical results that serve as basis for the use of Uk,n on manifolds.

2.2. Angle-variance statistics for pairs of uniform points on Sd−1.

Lemma 2.1 (Angles between uniform vectors). Let Z1, Z2 be two independent vectors
with the uniform distribution on the unit sphere Sd−1 ⊆ Rd and let Θd := arccos(cid:104)Z1, Z2(cid:105) be
the angle between them. The following statements hold:

(1) The distribution of Θd is given by

P(Θd ≤ α) =

(cid:82) α
0 sind−2(φ)dφ
(cid:82) π
0 sind−2(φ)dφ
3

.

(2) The moment generating function of Θd, denoted by φd−2(s) := E (cid:2)esΘd(cid:3) is given by

φ2k(s) = esπ−1
sπ

(cid:81)k

j=1

(2j)2
(2j)2+s2

φ2k+1(s) = esπ+1
2(s2+1)

(cid:81)k

j=1

(2j+1)2
(2j+1)2+s2

according to whether d − 2 is even or odd respectively.

(3) In particular E[Θd] = π

2 for all d and Var[Θd] = βd where

βd :=





π2
4
π2
12

− 2 (cid:80)k

j=0

− 2 (cid:80)k

j=1

1
(2j + 1)2

1
(2j)2

if d − 2 = 2k + 1 is odd or

if d − 2 = 2k is even.

(4) The variance of the centered squared angle σ2

σ2
d =






− π4

8 + 12

k
(cid:80)
j=0

k
(cid:80)
j=1

(cid:32)

π2
4 − 2

1

(2j+1)4 + 2
(cid:32)

k
(cid:80)
j=0

1
(2j + 1)2
(cid:33)2

k
(cid:80)
j=1

1
(2j)2

− π4

120 + 12

1
(2j)4 + 2

π2
12 − 2

if d − 2 = 2k.

d := Var (cid:0)Θd − π
(cid:33)2

2

(cid:1)2 is given by

if d − 2 = 2k + 1 or

Proof.

(1) Passing to polar coordinates r, φ1, . . . , φd−1 with r ≤ 0, 0 ≤ φj ≤ π for
1 ≤ j ≤ d − 2 and 0 ≤ φd−1 ≤ 2π. The probability that Θd ≤ α is precisely the
fraction of the surface area of the sphere deﬁned by the inequality 0 ≤ φ1 ≤ α. Since
the surface element of the sphere is given by

dS = sind−2(φ1) sind−3(φ2) . . . sin(φd−2)dφ1 . . . dφd−1

the probability is given by
0 · · · (cid:82) π
(cid:82) π
0 · · · (cid:82) π
(cid:82) π

(cid:82) α
0
(cid:82) π
0

0

0

(cid:82) 2π
0 dS
(cid:82) 2π
0 dS

=

(cid:82) α
0 sind−2(φ)dφ
(cid:82) π
0 sind−2 φdφ

as claimed.

(2) We begin with a claim

Claim 2.2. Let u : R → R be a C 2-function and deﬁne
(cid:82) α
0 u(x)n sind−2(φ)dφ
Ad

Ed(u(x)) :=

where Ad := (cid:82) π

0 sind−2 φdφ. Then, the following recursion formula holds

d Ed(u(x)) = d Ed−2(u(x)) −

Ed(u(cid:48)(cid:48)(x)).

1
d

Proof. Using integration by parts one can show a recursive formula for Ad and con-
clude that

Ad :=

(cid:40) (2k)!
(2k d!)2
(2kk!)2
(2k+1)!)

π
2

if d − 2 = 2k or

if d − 2 = 2k + 1.

Applying integration by parts twice and using our formula for Ad gives the result. (cid:3)

4

In particular if we take u(x) = esx we get
(cid:19)

(cid:18) d2

Ed(esx) =

Ed−2(esx).

d2 + s2

(3) All densities are, like sine, symmetric around π

As a result we obtain the stated closed formula for the moment generating function.
2 and the ﬁrst statement follows.
To ease the computations we introduce the cumulant-generating function ψd−2 =
log(E(esΘd)). Then it is immediate that Var(Θd) = ψ(cid:48)(cid:48)
d−2(0). We consider two cases,
d even and odd, ﬁrst let us assume that d = 2k + 2. Then, we write the cumulant-
generating function as

ψd−2(s) = log

(cid:19)

(cid:18) esπ − 1
sπ

+

(cid:123)(cid:122)
t(s)

(cid:125)

k
(cid:88)

j=1
(cid:124)

(cid:124)

(cid:18) (2j)2

log

(2j)2 + s2
(cid:123)(cid:122)
r(s)

(cid:19)

.

(cid:125)

After some dry algebra we get t(cid:48)(cid:48)(0) = π2
result for the even case. The odd case follows from an analogous argument.

12 and r(cid:48)(cid:48)(0) = −2 (cid:80)k

1
(2j)2 , which gives the

j=1

(4) Let µj be the jth moment of the random variable (cid:0)Θd − π

(cid:1), i.e. µj = E (cid:0)Θd − π

(cid:1)j.

2

It is well known that µ2 = ψ(cid:48)(cid:48)(0) and µ4 = ψ(4)(0) + 3 (ψ(cid:48)(cid:48)(0))2 . Therefore,

(cid:16)

Var

Θd −

(cid:17)2

π
2

= µ4 − µ2

2 = ψ(4)(0) + 2 (ψ(cid:48)(cid:48)(0))2 .

Again, consider two cases: d even and d odd. Suppose d = 2k − 2, just as before, we
calculate t(4)(0) = − π4
1
(2j)4 . Substituting both these into (5)
yields the claim. A similar argument can be applied to the odd case.

120 and r(4)(0) = 12 (cid:80)k

j=1

At ﬁrst glance the formulas for βd and σd might seem a little complicated. In order to
derive our results we need tangible decrease rates in terms of the dimension. The following
claim gives us an easy way to interpret these quantities.

Claim 2.3. The following bounds hold for βd and σ2
d:

moreover the upper bound for σ2

d holds for all d ≥ 1.

Proof. We distinguish two cases according on whether d ≥ 1 is even or odd. If d is even, we
can deﬁne k by the equality d − 2 = 2k and compute

1
d

≤ βd ≤

1
d − 1

1
2d2 ≤ σ2

d ≤

2
(d − 1)2

for d ≥ 1,

for d ≥ 4,

βd = 2

1
(2j)2 .

∞
(cid:88)

j=k+1
5

2

(5)

(cid:3)

(6)

(7)

Since this series consists of monotonically decreasing terms and d ≥ 2 we conclude that

1
d

=

1
2k + 2

(cid:90) ∞

1

= 2

(2x)2 dx ≤ 2

k+1

∞
(cid:88)

j=k+1

1
(2j)2 ≤ 2

(cid:90) ∞

1

(2x)2 dx =

1
2k + 1

=

1
d − 1

k+ 1
2

as claimed. On the other hand, notice that the other term concerning the variance can be
written as

12

k
(cid:88)

j=1

1
(2j)4 −

π4
120

= −12

∞
(cid:88)

j=k+1

1
(2j)4 ,

which again can be bound by

2
d3 =

1

4(k + 1)3 = 12

(2x)4 dx ≤ 12

k+1

(cid:90) ∞

1

∞
(cid:88)

j=k+1

1
(2j)4 ≤ 2

(cid:90) ∞

1

k+ 1
2

(2x)2 dx =

2
(2k + 1)3 =

2
(d − 1)3 .

Then, we get

1
2d2 ≤

2
d2 −

2

(d − 1)3 ≤ σ2

d ≤

2
(d − 1)2 −

2
d3 ≤

2
(d − 1)2

where the ﬁrst inequality follows since d ≥ 4. The case when d is odd is proven similarly. (cid:3)

3. Theoretical foundations

3.1. Statement of results. In this subsection we state the theoretical results that serve
as basis for the proposed methodology. Proofs are given in the following subsection. The
setting is the following: An i.i.d. sample, X1, . . . , Xn, is available from a distribution P on
Rm. Additionally we have access to a distingushied point p, and near this point the data live
on a Riemannian C 2 manifold M , of dimension d < m. Furthermore, at p the distribution
P has a Lipschitz continuous non-vanishing density function g, with respect to the volume
measure on M . Without loss of generality, we assume that p = 0. Then, we have

Proposition 3.1 (Behavior of nearest neighbors). For a positive constant C, deﬁne
k = (cid:100)C log(n)(cid:101) and let R(n) = Lk+1(0) be the euclidean distance in Rm from p = 0 to its
(k + 1)-st nearest neighbor in the sample X1, . . . , Xn. Deﬁne BR(n)(0) to be the open ball of
radius R(n) around 0 in Rm. Then, the following holds true:

(1) For any suﬃciently large C > 0, we have that, with probability one, for large enough

n (n ≥ n0, for some n0 depending on the actual sample), R(n) ≤ r(n), where

(cid:32)(cid:18) log(n)

(cid:33)

(cid:19) 1

d

r(n) := O

n

is a deterministic function that only depends on the distribution P at p and C.

(2) Conditionally on the value of R(n), the k-nearest-neighbors of 0 in the sample X1, . . . , Xn,
have the same distribution as an independent sample of size k from the distribution
with density gn, equal to the normalized restriction of g to M ∩ BR(n)(0).

In what follows, with a slight abuse of notation, we will write X1, X2, . . . , Xk to denote
the k nearest neighbors of 0 in the sample and assume that these follow the distribution with
density gn of Proposition 3.1. Let π : Rm → TpM be the orthogonal projection onto the
(embedded) tangent space to M at p = 0. For a nonzero X ∈ Rm, let W := π(X), (cid:98)X := X
6

(cid:107)X(cid:107)

and (cid:99)W := W
space of M at 0.

(cid:107)W (cid:107). (cid:99)W takes values in the (d − 1)-dimensional unit sphere Sd−1 of the tangent

Our ﬁrst Lemma bounds the diﬀerence between the inner products (cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) and (cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)
In this Lemma, the random nature of the Xi is

in terms of the length of projections.
irrelevant.

Lemma 3.2 (Basic projection distance bounds). For any X, X1, X2 ∈ M :

(1) (cid:107)X − πX(cid:107) = O((cid:107)πX(cid:107)2)
(2) (cid:107) (cid:98)X − (cid:99)W (cid:107) = O((cid:107)πX(cid:107))
(3) The cosine of the angle between X1 and X2 is close to that between W1 and W2. More

precisely,

|(cid:104) (cid:99)X1, (cid:99)X2(cid:105) − (cid:104)(cid:99)W1, (cid:99)W2(cid:105)| ≤ Cr
for some C ∈ R, whenever r ≥ (cid:107)π(Xi)(cid:107) for i = 1, 2.

Using Lemma 3.2, we can establish the following approximation. Let X1, . . . , Xk be the
k-nearest-neighbors from the sample to p = 0 in Rm . Deﬁne Wi and (cid:99)Wi as above and let
Vk,n be given by the formula

Vk,n :=

1
(cid:1)
(cid:0)k
2

(cid:88)

(cid:16)

(cid:68)

(cid:69)

arccos

(cid:99)Wi, (cid:99)Wj

−

(cid:17)2

.

π
2

1≤i<j≤k

Proposition 3.3 (Approximating the statistic via its tangent analogue). For k =
C log(n), as above, we have

(1) The sequence k(Uk,n − Vk,n) converges to 0 in probability as n → ∞.
(2) limn→∞ E (Uk,n − Vk,n) = 0.

When X comes from the distribution producing the sample, but is restricted to fall very
close to 0, the distribution of πX will be nearly uniform in a ball centered at 0 in TpM . This
will allow us to establish a coupling between the normalized projection (cid:99)W and a variable
Z, uniformly distributed on the unit sphere of TpM , an approximation that leads to the
asymptotic distribution of Uk,n. Some geometric notation must be introduced to describe
these results. Since near 0, M ⊆ Rm is a Riemannian submanifold of dimension d, it
inherits, from the euclidean inner product in Rm, a smoothly varying inner product lp :
TpM × TpM → R, given by lp(u, v) = (cid:104)i∗(u), i∗(v)(cid:105), where i : M → Rm is the inclusion
with diﬀerential i∗. This metric determines a diﬀerential d-form ΩM which, in terms of
local coordinates ∂i for TpM and dual coordinates dxi of TpM ∗ with i = 1, . . . d, is given by
ΩM := (cid:112)det((cid:104)∂i, ∂j(cid:105))1≤i,j≤d)dx1 ∧ · · · ∧ dxd. The diﬀerential form endows M with a volume
measure ν(U ) = (cid:82)
U ΩM . We say that a random variable A on M has density g : M → R if
the distribution µA of A satisﬁes µA(D) = (cid:82)

D gΩM for all borel sets D in M .

If X is a random variable taking values on M with density g and r is a positive real number,
let X(r) be a random variable with distribution gr given by the normalized restriction of g
to M ∩ Br(0), that is:

gr(z) =

(cid:40)

(cid:82)

g(z)
Br (0)∩M gΩM
0, otherwise.

7

, if z ∈ M ∩ Br(0) and

Deﬁne W (r) := π(X(r)). The following geometric Lemma will be used for relating the

densities of X(r) and W (r).

Lemma 3.4 (Tangent space approximations). The following statements hold for all
suﬃciently small r and p = 0 in M .

(1) The map π : Br(0)∩M → π(Br(0)∩M ) is a diﬀeomorphism. Let Φ : Br(0)∩TpM →

Br(0) ∩ M be its inverse.

(2) The inclusion π(Br(0) ∩ M ) ⊆ Br(0) ∩ TpM holds and moreover |λ(Br(0) ∩ TpM ) −

λ(π(Br(0) ∩ M ))| = O(r) where λ denotes the Lebesgue measure on TpM .

(3) The following equality holds:

(cid:115)

1 −

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

1≤i,j≤d

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= O(r)

Let D(r) be a random variable uniformly distributed in Br(0) ∩ TpM and note that Z =
(cid:98)D(r) := D(r)
(cid:107)D(r)(cid:107) is uniformly distributed on the unit sphere Sd−1, regardless of the value of r.
Our next Lemma shows that under weak hypotheses there is a coupling between W (r) and
D(r) which concentrates on the diagonal as r decreases.

Lemma 3.5 (Coupling). Let r denote a small positive number. With D(r) as above and
Z a random vector with the uniform distribution on the unit sphere, Sd−1 of TpM , if the
density g of X in M , near 0, is locally Lipschitz continuous and nonvanishing at 0, then the
following hold:

(1) There exists a coupling A(r) = (W (r), D(r)) and a constant C > 0 such that

P{W (r) (cid:54)= D(r)} ≤ Cr for all suﬃciently small r.
(2) There exists a coupling A(cid:48)(r) = ( (cid:91)W (r), Z) such that P

(cid:110) (cid:91)W (r) (cid:54)= Z

(cid:111)

≤ Cr for all

suﬃciently small r.

The previous Lemma leads to the asymptotic distribution of the statistic Uk,n.

Theorem 3.6 (Local Limit Theorem for angle-variance). Let k := (cid:100)C log(n)(cid:101) for the
constant C of the proof of Proposition 3.1 and assume X1, . . . , Xk are the k nearest neighbors
to p = 0 in the sample, with respect to the euclidean distance in Rm. If dim TpM = d then
the following statements hold:

(1) The equality limn→∞ E[Uk,n] = βd holds and
(2) The quantity k (Uk,n − βd) converges, in distribution, to that of (cid:80)∞

1,i − 1)
1,i are i.i.d. chi-squared random variables with one degree of freedom and

where the χ2
the λi are the eigenvalues of the operator A on L2(Sd−1) deﬁned by

i=1 λi(χ2

(Au)(x) =

(h(x, z) − βd) u(z) dµ(z)

(cid:90)

Sd−1

for u ∈ L2(Sd−1), where h(v, v(cid:48)) := (cid:0)arccos(v · v(cid:48)) − π
measure on Sd−1.

2

(cid:1)2 and µ denotes the uniform

This limit theorem is obtained by the various approximation steps given in the preliminary
results together with the classical Central Limit Theorem for degenerate U statistics, as
described in Chapter 5 of [28]. Depending on the relative values of the λi’s appearing in
8

the statement of the Theorem, it could happen that the limiting distribution just obtained
approaches a Gaussian distribution as the dimension increases (this would happen if the λi
were such that Lindeberg’s condition holds).

Although theoretical study of the λi’s is left for future work, we conjecture that as d
increases the limiting distribution converges to a Gaussian distribution. Numerical experi-
ments seem to support our conjecture, see Figure 1.

(a) d = 2

(b) d = 25

(c) d = 50

Figure 1. QQ-plots. As established in the proof of Theorem 3.6, the limiting
distribution is in fact the asymptotic distribution of k(En−βd), with En deﬁned
in (Equation 14). For this ﬁgure, we generate 10000 samples of the variable
k(En − βd), for k = 10d in dimensions d = 5, 25, and 50. The plots compare
the quantiles of the sample distribution against the standard normal quantiles.

In order to get a consistency result for our basic local dimension estimator, we will use

the following fact.

Corollary 3.7 (Variance convergence). Under the conditions stated at the beginning of
this subsection,

(cid:18)k
2
for d equal to the dimension of M near 0.

lim
n→∞

(cid:19)

Var Uk,n = σ2
d

Recall from Section 2 that our basic procedure estimates the dimension d as (cid:98)d, equal to

the integer r such that βr is closest to Uk,n. This procedure is consistent, as stated next.

Proposition 3.8 (Consistency of basic dimension estimator). As in Section 2, write (cid:98)d
for the basic estimator described above. Let d be the true dimension of M in a neighborhood
of p = 0. Then, in the setting of the present section,

P(failure) = P( (cid:98)d (cid:54)= d) → 0,

as n → ∞.

3.2. Proofs.

Proof of Proposition 3.1.

Let us recall a probability bound for the Binomial distribution. For N an integer valued
random variable with Binomial(n, p) distribution and expected value λ = np, one of the
9

Chernoﬀ-Okamoto inequalities (see Section 1 in [16]) states that, for t > 0, P(N ≤ λ − t) ≤
exp(−t2/2λ). Letting t = λ/2, we get

P(N ≤ λ/2) ≤ exp(−λ/8).
(8)
For ﬁxed and small enough r > 0, let Br(0) denote the ball of radius r around 0 ∈ Rm. For a
random vector X, with the distribution P of our sample X1, X2, . . . , Xn, by our assumptions
on P and M near 0, we have that

P(X ∈ Br(0) ∩ M ) ≤ ανdrd,
where νd is the volume (Lebesgue measure) of the unit ball in Rd and α is a positive number.
Let N = Nr denote the amount of sample points that fall in Br(0) ∩ M . We have that
λ = E(N ) ≤ ανdrdn. We choose r such that λ ≤ C ln n, for a constant C to be speciﬁed in
a moment. Then, by (8), we get

(cid:18)

P

(cid:19)

C
2

N ≤

ln n

≤ exp(−C ln n/8) =

(cid:19)C/8

(cid:18) 1
n

(9)

Pick any value of C > 8. For this choice, the bound in (9) will add to a ﬁnite value when
summed over n. By the Borel-Cantelli Lemma, the inequality N > C
2 ln n will hold for all
n suﬃciently large. It follows that if k = C
2 ln n, the k-nearest-neighbors of 0 in the sample,
will fall in Br(0) for every n suﬃciently large and the chosen value of r, namely

r = r(n) =

(cid:19)1/d

(cid:18)C ln n
ανdn

which is OPr((ln n/n)1/d)). The proof of the ﬁrst part of the Proposition ends by renaming
C.
The statement of the second part of Proposition 3.1 is intuitive and has been used in the
literature without proof. Luckily, Kaufmann and Reiss [18] provide a formal proof of these
type of results in a very general setting. In particular, (ii) of Proposition 3.1 holds by formula
(cid:50)
(6) of [18].

Proof of Lemma 3.2
By an orthogonal change of coordinates, we can assume that TpM is spanned by the ﬁrst
d basis vectors in Rm. The projection π : M → TpM is a diﬀerentiable function whose
derivative at p = 0 is the identity. By the implicit function theorem we can conclude that
there exists an r > 0, such that π : Br(0) ∩ M → π(Br(0) ∩ M ) is a diﬀeomorphism and that
M admits, near p a chart Φ : Br(0) ∩ TpM → M of the form

Φ(z1, . . . , zd) = (z1, . . . , zd, F1(z1, . . . , zd), . . . , Ft(z1, . . . , zd))

(10)

where m = d + t, Φ(0) = p = 0 and such that ∂Fi
(0) = 0 for 1 ≤ i ≤ t and 1 ≤ j ≤ d. As
∂zj
a result, the euclidean distance between a point of M near 0 and the tangent space at 0 is
given, in the local coordinates z, by

d(z1, . . . , zd) =

F 2

i (z).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

t
(cid:88)

i=1

10

We will prove that there exists a constant K such that, for all suﬃciently small δ > 0 and
all z with (cid:107)z(cid:107) ≤ δ the inequality d(z) ≤ K(cid:107)z(cid:107)2 holds. By Applying Taylor’s Theorem
at 0 to the diﬀerentiable function d(z) we conclude, since Φ(0) = 0 and ∂Fi
(0) = 0, that
∂zj
the constant and linear term vanish from the expansion. This proves the claim because
(cid:107)z(cid:107)2 = (cid:107)π(Φ(z))(cid:107)2. Assume K is a constant which satisﬁes (cid:107)X − πX(cid:107) ≤ K(cid:107)πX(cid:107)2. Thus

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)πX(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ K(cid:107)πX(cid:107).

(11)

(12)

On the other side,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

X
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)X(cid:107)

−

1
(cid:107)πX(cid:107)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (cid:107)X(cid:107)

=

1
(cid:107)πX(cid:107)

1
(cid:107)πX(cid:107)

|(cid:107)X(cid:107) − (cid:107)πX(cid:107)| ≤

(cid:107)X − πX(cid:107) ≤ K(cid:107)πX(cid:107).

Altogether,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

X
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)πX(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2K(cid:107)πX(cid:107),

where the ﬁrst inequality is just the triangle inequality and the second one follows from (11)
and (12). The third item in the Lemma follows immediately from the triangle inequality
(cid:50)
and the second item by adding and subtracting (cid:104)(cid:99)Xi, (cid:99)Wj(cid:105).

Remark 3.9. The quadratic term of G is the second fundamental form of M and, therefore,
the constant K can be chosen to be the largest sectional curvature of M at p.

Before proving Proposition 3.3, we need a Lemma on the behavior of the arccos function.

Lemma 3.10. Suppose that −1 ≤ c1 ≤ c2 ≤ 1 and let δ = c2 − c1 be suﬃciently small (for
our purposes it suﬃces to have δ ≤ 1/4). Then,

| arccos(c2) − arccos(c1)| ≤ 2(cid:112)|c2 − c1|

Proof. Assume ﬁrst that both c1 and c2 are positive. We have

| arccos(c2) − arccos(c1)| =

(cid:90) c2

c1

√

1
1 − x2

dx.

Using that the integrand in the last expression is increasing in [0, 1] and by the change of
variables u = 1 − x, we get

| arccos(c2) − arccos(c1)| ≤

(cid:90) 1

1−δ

√

1
1 − x2

dx =

(cid:90) δ

0

1
(cid:112)u(2 − u)

du ≤

(cid:90) δ

0

1
√
u

du

since 2 − u ≥ 1 for u ≤ δ. From this last bound, the result follows in this case by integration.
The argument for the case in which both c1 and c2 are negative is identical, by symmetry. In
the case c1 ≤ 0 ≤ c2, both c1 and c2 fall in a ﬁxed interval ([−1/4, 1/4]) where the derivative
(cid:3)
of arccos is bounded and the result follows easily.

11

Proof of Proposition 3.3
To prove part (1), putting together Proposition 3.1 and Lemma 3.2 we have

max
i≤k

(cid:107)(cid:99)Xi − (cid:99)Wi(cid:107) = OPr(r(n)) = OPr

(cid:19)1/d(cid:33)

(cid:32)(cid:18) ln n
n

From this, it follows easily that

and, using Lemma 3.10 we get

max
i<j≤k

|(cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) − (cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)| = OPr(r(n)),

| arccos(cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) − arccos(cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)| = OPr((cid:112)r(n))

max
i<j≤k

The bound is preserved by the application of the function u (cid:55)→ (u − π/2)2 (since the function
is locally Lipschitz) and by taking averages over all pairs, and we get
(cid:19)1/2d(cid:33)

Uk,n − Vk,n = OPr((cid:112)r(n)) = OPr

.

(13)

(cid:32)(cid:18) ln n
n

The result follows by observing that, for the value of k considered,

k OPr

(cid:32)(cid:18) ln n
n

(cid:19)1/2d(cid:33)

= oPr(1).

To prove (2), notice that from part (1) it is immediate that |Uk,n − Vk,n| converges to zero
in probability, which implies that limn→∞ E (Uk,n − Vk,n) = 0, since Uk,n − Vk,n is a bounded
(cid:50)
random variable.

Proof of Lemma 3.4
Recall, from the proof of Lemma 3.2, that for r > 0, small enough, the projection π :
Br(0) ∩ M → π(Br(0) ∩ M ) is a diﬀeomorphism and that M admits, near p a chart (inverse)
Φ : Br(0) ∩ TpM → M of the form given in (10) and satisfying that Φ(0) = p = 0 and such
that ∂Fi
(0) = 0 for 1 ≤ i ≤ t and 1 ≤ j ≤ d. Also from that proof, recall that there exists a
∂zj
constant K such that for small enough δ > 0 and all z with (cid:107)z(cid:107) ≤ δ, we have d(z) ≤ K(cid:107)z(cid:107)2,
where d(z) is the distance between a point z ∈ M and its projection on TpM . It follows
that the image π(Br(0) ∩ M ) contains a ball of radius r(cid:48) < r such that r − r(cid:48) = O(r2) and
therefore

Br(0) ∩ TpM ⊇ π(Br(0) ∩ M )) ⊇ Br(cid:48)(0) ∩ TpM,
proving part (2) of the Lemma, since the volume of the ﬁrst and last term diﬀer by at
= ei + (cid:80)m−d
most O(r2). For part (3) note that ∂Φ
are O(r2) the inner
∂zi
(cid:113)
products (cid:104) ∂Φ
, ∂Φ
(cid:105)
∂zj
∂zi
(cid:50)
is 1 + O(r) as claimed.

(cid:105) are 1+O(r2) if i = j and O(r2) otherwise and we conclude that

ed+t. Since ∂Ft
∂zi

, ∂Φ
∂zj

(cid:104) ∂Φ
∂zi

∂Ft
∂zi

t=1

Proof of Lemma 3.5
The total variation distance between two probability measures µ and ν, deﬁned as (cid:107)µ −
ν(cid:107)T V := supA |µ(A) − ν(A)|, satisﬁes

(cid:107)µ − ν(cid:107)T V = inf{P{X (cid:54)= Y } : A = (X, Y )}
12

where the inﬁmum runs over all couplings A = (X, Y ) of random variables X, Y with dis-
tributions given by µ and ν, respectively (see, for instance, page 22, Chapter 1 of [34]).
Moreover, if µ and ν are given by densities g1, g2 then the following inequality holds

(cid:107)µ − ν(cid:107)T V ≤ (cid:107)g1 − g2(cid:107)L1.
We will prove the ﬁrst part of the Lemma by bounding the L1-norm of the diﬀerence of
the densities of W (r) and D(r). Recall the deﬁnition of W (r) right before the statement of
Lemma 3.4. The diﬀerence of the two densities is given by

(cid:90)

Br(0)∩TpM

(cid:12)
(cid:12)
(cid:12)
(cid:12)

hr −

1
λ(Br)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dλ

where hr denotes the density of W (r) with respect to the d-dimensional Lebesgue measure
λ in TpM and Br := Br(0) ∩ TpM . More precisely, deﬁning Φ as in Lemma 3.4 the density
of W (r) is given by

hr(u) = gr(Φ(u))

det

(cid:115)

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

(u)

1≤i,j≤d

Since g is locally Lipschitz continuous there exists a constant K such that

g(0) − K1r ≤ g(Φ(u)) ≤ g(0) + K1r.

By Lemma 3.4 there exist constants K2, K3 such that the following inequalities hold for
u ∈ Br:

(cid:115)

1 − K2r ≤

det

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

1≤i,j≤d

(u) ≤ 1 + K2r and

λ(Br) − K3r ≤ λ(π(Br(0) ∩ M )) ≤ λ(Br) + K3r
Combining these inequalities we conclude that there exists a constant ˜K such that for all
u ∈ Br

λ(Br)(g(0) − ˜Kr) ≤

gΩ ≤ λ(Br)(g(0) + ˜Kr).

(cid:90)

Br(0)∩M

As a result the inequality

1
λ(Br)

(cid:32)

g(0) − ˜Kr
g(0) + ˜Kr

(cid:33)

− 1

≤ hr −

1
λ(Br)

≤

1
λ(Br)

(cid:32)

g(0) + ˜Kr
g(0) − ˜Kr

(cid:33)

− 1

so, using the fact that g(0) > 0 we conclude that there exists a constant such that

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Br

hr −

1
λ(Br)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ Cr

as claimed, ﬁnishing the proof of the ﬁrst part of the Lemma.
For part (2), let A(cid:48)(r) be the random pair obtained from A(r) = (W (r), D(r)) by normalizing
(cid:17)
(cid:16) (cid:91)W (r), (cid:91)D(r)
its components, that is A(cid:48)(r) =
= O(r),
because this probability is bounded above by the probability that W (r) and D(r) diﬀer.
Note that the random vector Z = D(r)
(cid:107)D(r)(cid:107) is uniform on the unit sphere, and in particular its
(cid:50)
distribution is independent of the value of r.

(cid:107)W (r)(cid:107) (cid:54)= D(r)

and note that P

(cid:110) W (r)

(cid:107)D(r)(cid:107)

(cid:111)

13

For k = (cid:100)C log(n)(cid:101), as before, let Z1, . . . , Zk be an i.i.d. sample distributed uniformly on the
unit sphere Sd−1 of TpM and deﬁne

En :=

1
(cid:1)
(cid:0)k
2

(cid:88)

(cid:16)

1≤i<j≤n

arccos(cid:104)Zi, Zj(cid:105) −

(cid:17)2

.

π
2

(14)

In view of Proposition 3.3, in order to prove Theorem 3.6 it will suﬃce to show that the
limiting standardized distribution of the Vk,n deﬁned in that Proposition is the same as that
of En, and establish the asymptotics for En.

Proof of Theorem 3.6

By Proposition 3.1 part (1) on a set of probability 1 (on the set of inﬁnite samples of

data), for some n0, the inequality Rn := R(n) ≤ r(n) holds for n ≥ n0.
By Proposition 3.1 part (2), for i = 1, . . . , k the distribution of Wi = π(Xi), the projections
on TpM of the k nearest neighbors of 0 in the sample, is that of an independent sample
W1(Rn), . . . , Wk(Rn), with the Wj(Rn), j ≤ k as deﬁned before Lemma 3.4.

From the previous Lemma, conditionally on Rn, we have a coupling A(cid:48)

j(Rn) = ( (cid:92)Wj(Rn), Zj)
for each j ≤ k. These couplings can be taken such that the (cid:92)Wj(Rn), j ≤ k form an i.i.d.
sample and the same holds for the Zj’s. By the previous lemma, we have that for each j,
P( (cid:92)Wj(Rn) (cid:54)= Zj) ≤ Cr(n). Then, except for a set of measure 0, we get the following event
inclusion

{kVk,n (cid:54)= kEn} ⊆

{ (cid:92)Wi(Rn) (cid:54)= Zi}

k
(cid:91)

j=1

By the union bound, the probability of the rightmost event is bounded by Ck(n)r(n) which
goes to zero, as n goes to inﬁnity by the choice of k(n) and the value of r(n). It follows that
Vk,n and En have the same standardized asymptotic distributions, and being both random
variables bounded, it follows that limn→∞ E(Vk,n − En) = 0.

For part (2) of Theorem 3.6 it only remains to establish the limiting distribution of En.
This statistic falls in the framework of classical U -statistics, that have played an important
role in the theory of many non-parametric procedures. See for instance [26] for several
applications of the theory of U -statistics. Even an empirical processes theory is available
for U -processes, see for instance [2], which has found application in the study of notions of
multivariate depth. Still, we will only require the classical theory, as exposed in Chapter 5
of [28] and Chapter 3 of [26].

Recall the deﬁnition of the kernel h in Equation 4. By the symmetry of the uniform
distribution for three independent vectors, Z1, Z2, Z3 with uniform distribution on Sd−1, it
can be easily veriﬁed that,

This means that the U -statistic associated to h is degenerate. It follows (see the variance

calculation in [26]) that

E (cid:0)h(Z1, Z2)h(Z1, Z3) − β2

(cid:1) = 0.

d

(cid:19)

(cid:18)k
2

Var(En) = Var(h(Z1, Z2))

(15)

(16)

and by the Theorem for degenerate U -statistics in Section 5.5 of [28], part (2) of our Theorem
(cid:50)
follows.

14

Proof of Corollary 3.7
By (16), it suﬃces to show that k2 lim(EU 2
k,n − EE2
n) = 0, for En as in the proof of Theorem
3.6. Applying equation (13) and the facts that the function h is bounded and that k = O(ln n)
on a set of probability 1, we have

k2EU 2

k,n ≤ k2EV 2

k,n + O

ln2 n

(cid:32)

(cid:19)1/2d(cid:33)

.

(cid:18)ln n
n

The opposite inequality (interchanging the roles of Uk,n and Vk,n) is obtained by the same
reasoning, and we get

(17)
By the coupling argument of the proof of Theorem 3.6, we have that Vk,n and En might
diﬀer at most on a set of measure O(ln(n) r(n)). Taking expectations on the sets were they
coincide and diﬀer, we obtain

k,n) = 0.

lim k2(EU 2

k,n − EV 2

k2 EV 2

k,n ≤ k2 EE2

n + k2 O(ln(n) r(n)).

Observing that the second term in the right hand side of this inequality goes to zero, as n
grows, and that the reverse inequality is obtained similarly, we conclude

and the result follows by combining (17) and (18).

lim k2(EV 2

k,n − EE2

n) = 0,

(18)
(cid:50)

Proof of Proposition 3.8.
Let d be the true value of the dimension. βd+1 is the expected value closest to βd and, by
the proof of Claim 2.3, (βd − βd+1)/2 ≥ 1/(d − 1)2. For the basic procedure to incur in error
it is necessary that |Uk,n − βd| ≥ |Uk,n − βd+1|, which means
βd − βd+1
2

|EUk,n − βd| + |Uk,n − EUk,n| ≥

1
(d − 1)2 .

≥

1

Since |EUk,n − βd| converges to zero, the condition above requires that |Uk,n − EUk,n| ≥
1/
2(d−1)2 , for n large enough. The probability of this last event is bounded as follows. Let c
be a sup norm bound for h( (cid:99)X1, (cid:99)X2) − Eh( (cid:99)X1, (cid:99)X2), for the kernel h given in (4) and (cid:99)Xi as in
Lemma 3.2. Clearly, c is bounded above by π2/4. By Bernstein’s inequality for U -statistics
[2, Proposition 2.3(a)], we get

(cid:18)

P

|Uk,n − EUk,n| ≥

(cid:19)

1
2(d − 1)2

(cid:32)

≤ 2 exp

−k/(8(d − 1)4)
2Var(h( (cid:99)X1, (cid:99)X2)) + 2c

6(d−1)2

(cid:33)

Now, using Claim 2.3 and Corollary 3.7, we have, after some calculations,

(cid:18)

P

|Uk,n − EUk,n| ≥

(cid:19)

1
2(d − 1)2

(cid:32)

≤ 2 exp

−k/(8(d − 1)4)
(d−1)2 + π2

12(d−1)2

5

(cid:33)

≤ 2 exp

(cid:18)

−k
47(d − 1)2

(cid:19)

This bound goes to zero as n (and k) grow to inﬁnity, ﬁnishing the proof.

(19)

(cid:50)

Forcing the estimation error bound in (19) to be less that a given δ > 0 will give a value of
k of the order of Ad 2 ln(2/δ), for some constant A, reﬂecting that precise estimation is more
demanding, in terms of sample size, as the dimension d grows.

15

4. Estimators

In this section we present two dimension estimators based on the statistic Uk,n. First,
a local estimator that gives the dimension of M around a distinguished non-singular point
p ∈ M is discussed. Then, the case in which the manifold is equidimensional is considered,
by building upon our local estimator to propose a global dimension estimator. Some imple-
mentation issues are discussed and in the following section we evaluate the performance of
our estimators. The programming code used in these experiments is publicly available, it
can be found at https://github.com/mateodd25/ANOVA_dimension_estimator.

Algorithm 1: Local dimension estimation
Data: k ∈ N+ and X1, . . . , Xn, p ∈ M ⊆ Rm
Result: Estimated dimension (cid:98)d at p ∈ M
Find the k-nearest neighbors to p;
Use these neighbors to compute Uk,n as in (2);
Choose (cid:98)d associated with Uk,n;

4.1. Local estimators. The theory presented in Section 3 suggests that k ∼ log(n) should
be a good choice, asymptotically-speaking, for the number of neighbors to consider in the
local dimension estimation procedure. However, one could potentially leverage prior knowl-
edge of the structure of the problem to set this parameter diﬀerently. In our implementation
we set it to k = round(10 log10(n)).

In our theoretical analysis presented above, it was assumed that we are given a center
point p where the local dimension is to be estimated. A natural question that arises in
practice is the following: given a sample, how to select good center points. In Section 4.2
we present a simple heuristic to select “good” centers.

Both estimators presented in what follows are based on the Algorithm 1. The diﬀerence
between the estimators considered lies on the last line of the algorithm, namely, on how to
pick the dimension estimator, given Uk,n. Next, the two diﬀerent rules to execute this step
are discussed.

4.1.1. Basic estimator. Since Uk,n converges in probability to βd, a natural way to estimate
the dimension from Uk,n is to set

(cid:98)dbasic := arg min
d∈[Dmax]

|βd − Uk,n|,

where Dmax is the ambient dimension or some bound we know a priori on the dimension
of the manifold. Interestingly, such a rule is fairly accurate, as established in Proposition
3.8. Another advantage of this estimator is that there is no need to train it, since all the
quantities involved have been analytically computed (and presented in Section2).

Remark 4.1. Classical discriminant analysis results (see, for example, Section 4.1 in [11])
would advice to incorporate available variance information on the selection of (cid:98)d, by choosing

(cid:98)ddisc := arg max{d | Uk,n ≥ ηd}

where

ηd = βd +

(βd−1 − βd).

σd
σd + σd+1

16

Algorithm 2: Global dimension estimation
Data: c ∈ N+, k ∈ N+ and X1, . . . , Xn ∈ M ⊆ Rm
Result: Estimated dimension (cid:98)d of M
Choose c centers p1, . . . , pc from the sample;
Apply Algorithm 1 to each center pi, let (cid:98)di be its output;
Set (cid:98)d to the median of { (cid:98)di}c
i=1;

Still, simulation evaluations (not included) show that (cid:98)ddisc and (cid:98)dbasic have a very similar
performance in practice (and also in theory, since both are consistent). Thus, we prefer to
use the later, being the simpler one.

1

, . . . , Y (d)

4.1.2. Kernel-based estimator. For our second estimator we start by simulating multiple
instances Y (d)
M of the random variable k(En − βd), for a large value of M (=5000,
for instance) and with En as deﬁned in equation (14), for each dimension d ∈ [Dmax] =
{1, . . . , Dmax}. From these data, the density ˆf (d)
k , of k(En − βd), is estimated, for each d, as
M
(cid:88)

(cid:33)

(cid:32)

ˆf (d)
k (y) =

1
M h

ϕ

y − Y (d)
h

i

i=1
where ϕ(·) is the standard Gaussian density and the parameter h (the “bandwidth”) can be
set at h = (4/3M )1/5. This choice of bandwidth guarantees consistent density estimation (see
Section 4.1 in [4]). Then, a Bayesian classiﬁcation procedure with uniform prior distribution
on the set [Dmax], would select the dimension as that for which the kernel density estimator
is maximized at the standardized Uk,n, namely
ˆf (d)
k (k(Uk,n − βd)).

(cid:98)dker = arg max
d∈[Dmax]

(20)

Notice that the simulations described above need to be performed only once for each dimen-
sion, since they are made on uniform data on Sd−1 and do not depend on the particular data
being studied.

4.2. Global estimators. We now turn our attention to extending the local dimension es-
timation algorithm to a global one. Assuming that M is equidimensional, the local method
can be extended by running multiple instances of Algorithm 1 on diﬀerent centers, and
combining the results, as outlined in Algorithm 2.

After getting dimension estimates at each center, one could use diﬀerent summary statistics
to choose the global dimension, such as the mean, the mode or the median. To make a method
robust against outliers, we chose to use the median. To decide about the parameter c we
ran a cross-validation algorithm. Empirically, it appears that c ∼ log(n) is a good choice for
this parameter.

4.2.1. Choosing centers. To pick the centers pi in Algorithm 2, we divide the sample into c
disjoint subsamples of approximately equal size. Assume, for simplicity of the exposition,
Inside each subsample we pick a center by assigning each point a score of
that c = 1.
centrality and then choosing the one with highest score. Scores are assigned through the
following procedure:

17

(1) For each coordinate i, we order the sample based on the i-th entry, let τi be the
permutation giving this ordering, that is, the i-th row in (Xτi(1), . . . , Xτi(n)) is non-
decreasing.

(2) Then, the centrality score of Xj is given by (cid:80)m

(cid:12)
(cid:12)
(cid:12) .
For the i-th coordinate, the weight function f gives the maximum scores to the point (or
points) such that τi(j) is closest to n/2 and thus, the mechanism used chooses as center a
point which for many components, appears near the center of these orderings.

i=1 f (τi(j)), where f (x) =

2 − 2(x−1)

(cid:12)
(cid:12)
(cid:12)

2n

1

4.2.2. Heuristic to discard centers. Finally, we present a simple heuristic for discarding some
of the selected centers, based on the mean of the angles between its neighbors, taking as
always, the point considered as origin. This is done in order to improve the performance of
the statistic. Consider again the angle

(cid:28) Xi − p
(cid:107)Xi − p(cid:107)
for each pair of nearest neighbors Xi, Xj of the point p ∈ M , as used in the basic deﬁnition
(2). Consider the average of these angles,

Xj − p
(cid:107)Xj − p(cid:107)

θi,j = arccos

(cid:29)

,

θ(p) =

1
(cid:1)
(cid:0)k
2

(cid:88)

θi,j.

1≤i<j≤k

If the manifold M , near p, is approximately ﬂat, θ(p) should be close to π/2, regardless of
the value of the dimension d, since π/2 is the expected value of the angle between uniformly
sampled points in every dimension and the U -statistic θ(p) should converge rapidly to this
expectation. Thus, when θ(p) is far from π/2, it can be taken as a suggestion of strong
curvature that is causing non-uniformity of the angles, and therefore, p might not be a good
point to consider for dimension estimation. For these reason, in our implementation, the user
is allowed to use this heuristic and discard a fraction of the centers pi with largest values of
|θ(pi) − π/2|. Experiments presented in the next section suggest that this heuristic is useful
when the manifold is highly curved.

5. Numerical results

We compare our methods against two powerful dimension estimators, DANCo [9] and
Levina-Bickel [20], using a manifold library proposed in [15]. The ﬁrst estimator is, ar-
guably, the state-of-the-art for this problem, while the second one is a classical well-known
estimator with great performance. To see a comparison between these and other estimators
we refer the reader to [8, 9].

Table 1 presents a brief description of the manifolds included in the study. Additionally
Table 2 contains a list of the parameters used for each one of the estimators. We compare
two error measures, namely the Mean Square Error (MSE) and the Mean Percentage Error
(MPE) which are deﬁned as

MSE( (cid:98)d) :=

( (cid:98)di − di)2

and

MPE( (cid:98)d) :=

1
T

T
(cid:88)

i=1

100
T

T
(cid:88)

i=1

| (cid:98)di − di|
di

where T is the number of trials included in the test and (cid:98)di and di are the estimated dimension
and the correct dimension of the ith trial, respectively.

18

For each one of the aforementioned manifolds, we draw T = 50 random samples with
n = 2500 data points and then compute the MSE and MPE of the following four esti-
mators: the global basic estimator (Basic), the global basic estimator combined with the
centers heuristic (B+H), the global kernel-based estimator (Kernel), the global kernel-based
estimator with centers heuristic (K+H), the Levina-Bickel estimator (LB), and the DANCo
estimator. Tables 3 and 4 summarize the results.

Table 1. Library of manifolds used for benchmark, for more details consult [15].

Description
Sphere S9
Aﬃne subspace
Nonlinear manifold
Nonlinear manifold
Helix
Nonlinear manifold
Swiss roll
Highly curved manifold
Full-dimensional cube
9-dimensional cube

Manifold d m
10
9
5
3
6
4
8
4
3
2
36
6
2
3
12 72
20 20
10
9
2
3 Ten-times twisted Mobius band
10 10
10
1

M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
M11
M12
M13

Multivariate Gaussian
Curve

Table 2. Parameters of each algorithm.

Method Parameters
ANOVA k = 34, c = 16
k1 = 10, k2 = 20
k = 10

LB
DANCo

In both these tables, the last column shows the average of the performance measure over
the examples. It is clear from these tables that the angle-variance methods introduced in the
present article do well, in terms of average performance, against the very strong competitors
considered. This is more evident when the measure of error is the MSE. Still, for many of the
manifolds considered, namely M1, M3, M4 (in this case tied with LB), M9 and M12, DANCo
clearly displays the best performance. The Levina-Bickel estimator is the best for manifold
M6, tying for ﬁrst with DANCo in M4, while the procedures proposed in this article show
the best performance in the cases of manifolds M8 and M10, having very good performance
also in cases M3, M5, M7 and M12. It is interesting that our methods do particularly well
in case M8, a high curvature manifold of a relatively high dimensional in a large dimension
ambient space. It does not appear to exist a signiﬁcant diﬀerence in performance between
the Basic procedure and the procedure that uses Kernel Density Estimation. On the other
hand, the introduction of the heuristics discussed in Section 4 turns out to be beneﬁcial for
our estimators in some of the relatively high dimensional cases, namely M9 and M12, while
19

Table 3. Rounded Mean Square Error for diﬀerent manifolds, last column
displays the average MSE over all the examples. The darker cells show the
best results in each column.

M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 Mean
1.72
1.11
Basic
0.95 0.00 0.58 0.03 0.00 0.67 0.00
2.14
0.79
B+H 1.09 0.00 0.66 0.28 0.00 1.30 0.01
1.27
1.10
Kernel 0.95 0.00 0.68 0.08 0.00 0.69 0.29
0.80
2.48
K+H 0.99 0.00 0.76 0.45 0.00 1.31 0.31
2.69
2.27
0.49 0.02 0.05 0.00 0.00 0.10 0.00
LB
2.11
DANCo 0.16 0.00 0.00 0.00 0.00 1.00 0.00 25.22

10.39 0.00 0.00 0.12 0.00
4.64
0.03 0.00 0.10 0.00
10.20 0.00 0.00 0.15 0.00
4.06
0.01 0.00 0.04 0.00
29.33 2.23 0.00 0.54 0.00
0.10 0.00 0.00 0.00
0.96

Table 4. Rounded Mean Percentage Error for diﬀerent manifolds, last col-
umn displays the average MPE over all the examples.

M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 Mean
9.00
1.00
4.93
Basic
10.56 0.00 15.50
5.95
9.83
8.25
B+H 11.33 0.00 17.75
6.45
7.75
2.50
Kernel 10.56 0.00 17.75
7.67
K+H 11.00 0.00 20.00 12.25 0.00 18.00 17.50 10.75
7.31
1.49
LB
5.09
0.00
DANCo

0.00 1.50 0.00
0.00 1.20 0.00
0.00 1.80 0.00
0.00 0.60 0.00
12.55 27.07 16.57 1.64 7.34 0.50
0.00 0.00 0.00
41.83

0.00
0.00 10.83
0.00 17.83
0.50
0.00 11.50 16.50

1.84
5.13
0.00 16.66

15.50
10.00
15.55
9.60

0.00
0.67
0.00
0.11

2.63
0.00

4.84
0.00

5.77
0.00

7.77
1.77

4.80

1.11

these heuristics degrade somehow the performance in the intermediate dimension cases, M4
and M6. In all other cases, the use of the heuristics for center selection and center elimination
does not appear to have a strong eﬀect.

References

[1] Aldous, D. and J. Shun (2010) Connected spatial networks over random points and a route-length

statistic. Statistical Science 25, 275 - 288.

[2] Arcones, M. A. and Gin´e, E. (1993) Limit Theory for U -Processes. Annals of Probability 21, No. 3,

1494-1542.

[3] Belkin, M. and Niyogi, P. (2004) Semi-supervised learning on Riemannian manifolds. Invited Paper

inMachine Learning. Special Issue on Clustering 56, 209-239.

[4] Bowman, A. W. and Foster, P. J. (1993) Adaptive smoothing and density-based tests of multivariate

normality. Journal of the American Statistical Association 88 No. 422, 529-537.

[5] Breiding, P., Kalisnik, S., Sturmfels, B. and Weinstein, M. (2018) Learning Algebraic Varieties from

Samples. arXiv:1802.094[math.AG]

[6] Brito, M. R., Quiroz, A. J. and Yukich, J. E. (2002) Graph theoretic procedures for dimension identiﬁ-

cation. Journal of Multivariate Analysis 81, 67-84.

[7] Brito, M. R., Quiroz, A. J. and Yukich, (2013) Intrinsic dimension identiﬁcation via graph-theoretic

methods. Journal of Multivariate Analysis 116, 263277.

[8] Campadelli, P., Casiraghi, E., Ceruti, C. and Rozza, A. (2015) Intrinsic Dimension Estimation: Relevant
Techniques and a Benchmark Framework. Mathematical Problems in Engineering 2015 Article ID:
759567, 21 pages.

[9] Ceruti, C., Bassis, S., Rozza, A., Lombardi, G., Casiraghi, E. and Campadelli, P. (2014) DANCo: An
intrinsic dimensionality estimator exploiting angle and norm concentration. Pattern Recognition 47, No.
8, 2569-2581.

20

[10] Costa, J. A., Girotra, A. and Hero, A .O. (2005) Estimating local intrinsic dimension with k-nearest
neighbor graphs. In IEEE/SP 13th Workshop on Statistical Signal Processing, 417-422. IEEE Conference
Publication.

[11] Devroye, Luc, Gy¨orﬁ, L´aszl´o and Lugosi, G´abor. (2013) A probabilistic theory of pattern recognition

(Vol. 31). Springer Science and Business Media.

[12] Duda, R. O., Hart, P. E. and Stork, D. G. (2001) Pattern Classiﬁcation. 2nd. edition. John Wiley and

Sons, New York.

189-208.

[13] Farahmand, A., Szepesv´ari, C. and Audibert, J-Y (2007) Manifold-adaptive dimension estimation. In
Proceedings of the 24th International Conference on Machine Learning, Z. Ghahramani, editor, 265-272.
ACM, New York.

[14] Grassberger, P. and Procaccia, I. (1983) Measuring the strangeness of strange attractors. Physica 9D,

[15] Hein, M. and Audibert, J.-Y. (2005) Intrinsic dimensionality estimation of submanifolds in Rd. In

Proceedings of the 22nd International Conference on Machine learning, 289-296, ACM.

[16] Janson, S. (2002). On concentration of probability. In Bollob´as, B. (Ed.) Contemporary Combinatorics,
10. Proceedings of the Workshop on Probabilistic Combinatorics at the Paul Erd¨os Summer Research
Center, Budapest, 1998, pp. 289-301.

[17] Johnson, M. E. (1987) Multivariate Statistical Simulation. John Wiley and Sons, New York.
[18] Kaufmann, E. and Reiss, R.-D. (1992) On Conditional Distribution of Nearest Neighbors. Journal of

Multivariate Analysis 42, 67-76.

[19] Kegl, B. (2003) Intrinsic dimension estimation using packing numbers. In Advances in Neural Infor-
mation Processing Systems, Volume 15, Eds. S. Becker, S. Thrun and K. Obermayer. M.I.T. Press,
Cambridge, Massachusetts.

[20] Levina, E. and Bickel, P. J. (2005) Maximum likelihood estimation of intrinsic dimension. In Advances
in Neural Information Processing Systems, Volume 17, Eds. L. K. Saul, Y. Weiss and L. Bottou.
[21] Lombardi et. al., 2011 Lombardi, G., Rozza, A., Ceruti, C., Casiraghi, E. and Campadelli, P. (2011).
Minimum Neighbor Distance Estimators of Intrinsic Dimension. In D. Gunopulos et al. (Eds.): ECML
PKDD 2011, Part II, LNAI 6912, pp. 374-389. Springer-Verlag. Berlin.

[22] Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979) Multivariate Analysis. Academic Press, New York.
[23] Penrose, M. D. and Yukich, J. E. (2001) Central limit theorems for some graphs in computational

geometry. Annals of Applied Probability 11, 1005-1041.

[24] Penrose, M. D. and Yukich, J. E. (2013) Limit theory for point processes in manifolds. Annals of Applied

Probability 23, No. 6, 2161-2211.

[25] Pettis, K. W., Bailey, T. A. Jain, A. K. and Dubes, R. C. (1979) An intrinsic dimensionality estimator
from near-neighbor information. IEEE Transactions on Pattern Analysis and Machine Intelligence 1,
25-37.

[26] Randles, R. H. and Wolfe, D. A. (1979) Introduction to the Theory of Nonparametric Statistics. John

[27] Roweis, S. T. and Saul, L. K. (2000) Nonlinear dimensionality reduction by locally linear embedding.

[28] Serﬂing, R. J. (1980) Approximation Theorems of Mathematical Statistics. John Wiley and Sons, New

[29] Sindhwani, V., Belkin, M. and Nigoyi, P. (2006) The Geometric Basis of Semi-supervised Learning.
Book chapter in Semi-supervised Learning, O. Chapelle, B. Sch¨olkopf and A. Zien, editors, M.I.T.
Press, Cambridge, Massachusetts.

[30] S¨odergren, A. (2011) On the distribution of angles between the N shortest vectors in a random lattice

Journal of the London Mathematical Society, 84, No. 3, 749-764.

[31] Sricharan, K. Raich, R. and Hero, A. O. (2010) Optimized intrinsic dimension estimation using near-
est neighbor graphs. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 5418-5421. IEEE Conference Publication.

[32] Steele, J. M., Shepp, L. A. and Eddy, W. F. (1987) On the number of leaves of a Euclidean minimal

spanning tree. Journal of Applied Probability, 24, 809-826.

Wiley and Sons, New York.

Science, 290, 2323-2326.

York.

21

[33] Tenenbaum, J. B., de Silva, V. and Langford, J. C. (2000) A global geometric framework for nonlinear

dimensionality reduction. Science 290, 2319-2323.

[34] Villani, C. (2009) Optimal Transport. Old and New. Volume 338 in series Grundlehren der matematischen

[35] Yukich, J. E. (1998) Probability Theory of Classical Euclidean Optimization Problems. Lecture Notes in

Wissenschaften. Springer. Berlin, Heidelberg.

Mathematics, 1675, Springer, New York.

Mateo D´ıaz, Center for Applied Mathematics, 657 Frank H.T. Rhodes Hall, Cornell

University, Ithaca, NY 14853

E-mail address: md825@cornell.edu

Adolfo J. Quiroz, Departamento de Matem´aticas, Universidad de los Andes, Carrera 1

No. 18a 10, Edificio H, Primer Piso, 111711 Bogot´a, Colombia

E-mail address: aj.quiroz1079@uniandes.edu.co

Mauricio Velasco, Departamento de Matem´aticas, Universidad de los Andes, Carrera 1

No. 18a 10, Edificio H, Primer Piso, 111711 Bogot´a, Colombia

E-mail address: mvelasco@uniandes.edu.co

22

8
1
0
2
 
y
a
M
 
4
 
 
]
T
S
.
h
t
a
m

[
 
 
1
v
7
7
5
1
0
.
5
0
8
1
:
v
i
X
r
a

LOCAL ANGLES AND DIMENSION ESTIMATION
FROM DATA ON MANIFOLDS

MATEO D´IAZ, ADOLFO J. QUIROZ, AND MAURICIO VELASCO

Abstract. For data living in a manifold M ⊆ Rm and a point p ∈ M we consider a statistic
Uk,n which estimates the variance of the angle between pairs of vectors Xi − p and Xj − p,
for data points Xi, Xj, near p, and evaluate this statistic as a tool for estimation of the
intrinsic dimension of M at p. Consistency of the local dimension estimator is established
and the asymptotic distribution of Uk,n is found under minimal regularity assumptions.
Performance of the proposed methodology is compared against state-of-the-art methods on
simulated data.

1. Introduction

Understanding complex data sets often involves dimensionality reduction. This is partic-
ularly necessary in the analysis of images, and when dealing with genetic or text data. Such
data sets are usually presented as collections of vectors in Rm and it often happens that
there are non-linear dependencies among the components of these data vectors.
In more
geometric terms these non-linear dependencies amount to saying that the vectors lie on a
submanifold M ⊆ Rm whose dimension d is tipically much smaller than m. The expression
manifold learning has been coined in the literature for the process of ﬁnding properties of
M from the data points.

Several authors in the artiﬁcial intelligence literature have argued about the convenience of
having methods to ﬁnd or approximate these low-dimensional manifolds [3, 13, 27, 29, 31, 33].
Procedures for achieving this kind of low dimensional representation are called manifold
projection methods. Two fairly successful such methods are Isomap of Tenenbaum, de Silva
and Langford [33] and the Locally Linear Embedding method of Roweis and Saul [27]. For
these and other manifold projection procedures, a key initial ingredient is a precise estimation
of the integer d, ideally obtained at low computational cost.

The problem of estimating d has been the focus of much work in statistics starting from
the pioneering work of Grassberger-Procaccia [14]. Most of the most recent dimension iden-
tiﬁcation procedures appearing in the literature are either related to graph theoretic ideas
[6, 7, 23, 24, 35] or to nearest neighbor distances [10, 20, 21, 25]. A key contribution of the
latter group is the work of Levina and Bickel [20] who propose a “maximum likelihood”
estimator of intrinsic dimension. To describe it let Lk(Xi) be the distance from the sample
point Xi to its k-th nearest neighbor in the sample (with respect to the euclidean distance
in the ambient space Rm). Levina and Bickel show that, asymptotically, the expected value

2010 Mathematics Subject Classiﬁcation. 62G05, 62H10, 62H30.
Key words and phrases. dimension estimation, local U -statistics, angle variance, manifold learning.

1

of the statistic

(cid:98)mk(Xi) :=

(cid:34)

1
k − 2

k−1
(cid:88)

j=1

log

Lk(Xi)
Lj(Xi)

(cid:35)−1

(1)

coincides with the intrinsic dimension d of the data. As a result, they propose the corre-
sponding sample average mk := n−1 (cid:80)n
i=1 (cid:98)mk(Xi) as an estimator of dimension. Asymptotic
properties of this statistic have been obtained in the literature (see [24, Theorem 2.1]) allow-
ing for the construction of conﬁdence intervals. Both the asymptotic expected value and the
asymptotic distribution are independent of the underlying density from which the sample
points are drawn and thus lead to a truly non-parametric estimation of dimension.

In addition to distances, Ceruti et al. propose in [9] that angles should be incorporated in
the dimension estimators. This proposal, named DANCo, combines the idea of norm concen-
tration of nearest neighbors with the idea of angle concentration for pairs of points on the
d-dimensional unit sphere.

The resulting dimension identiﬁcation procedure is relatively involved. The method com-
bines two ideas. On one hand it uses the Kullback-Leibler divergence to measure the distance
between the estimated probability density function (pdf) of the normalized nearest neighbor
distance for the data considered and the corresponding pdf of the distance from the cen-
ter of an r-dimensional unit ball to its nearest neighbor under uniform sampling. On the
other hand, it uses a concentration result due to S¨odergren [30], for angles corresponding to
independent pairs of points on a sphere.

The main contribution of this article is a new and simple dimension identiﬁcation pro-
cedure based solely on angle concentration. We deﬁne a U -statistic which averages angle
squared deviations over all pairs of vectors in a nearest neighbor ball of a ﬁxed point and
determine its asymptotic distribution. In the basic version of our proposed method there
is no need of calibration of distributions and moreover our statistic is a U -statistic among
dependent pairs of data points and it is well known that these oﬀer fast convergence to their
mean and asymptotic distribution.

Our method has been called ANOVA in the literature1, given that the U -statistic used,
Uk,n to be deﬁned below, is an estimator of the variance of the angle between pairs of vec-
tors among uniformly chosen points in the sphere Sd−1. Our main results are to prove the
consistency of the proposed method of estimation (Proposition 3.8) and the description of
the (suitably normalized) asymptotic distribution of the statistic considered (Theorem 3.6),
a result that is very useful in the construction of asymptotic conﬁdence intervals in dimen-
sion estimation. We describe our proposed method in Section 2 and provide its theoretical
justiﬁcation in Section 3. Sections 4 and 5 discuss the details of our implementation of the
dimension identiﬁcation procedure together with some empirical improvements. It also con-
tains the result of performance evaluations on simulated examples, including comparisons
with current state-of-the-art methods.

2. A U -statistic for dimension identification

2.1. Description of the statistic. Suppose our data form an i.i.d. sample, X1, . . . , Xn
from a distribution P on Rm with support on a Riemannian C 2 manifold M of dimension

1The term was coined by Breiding, Kalisnik, Sturmfels and Weinstein in [5] when describing an earlier

preliminary version of this article

2

d < m. Given a point p ∈ M , the question to be addressed is to determine the dimension d
of the tangent space of M at p using only information from sample points near p (we want
to allow for the value of d to depend on the point p and for M to be disconnected).

The simplest version of our dimension identiﬁcation procedure is described by the following

steps:

(1) For an appropriate value of the constant C, to be speciﬁed below, let k := (cid:100)C log(n)(cid:101).
Assume, relabeling the sample if necessary, that X1, . . . , Xk are the k nearest neigh-
bors of p in the sample, according to the euclidean distance in Rm.

(2) Deﬁne the angle-variance U -statistic, Uk,n, by the formula

Uk,n :=

1
(cid:1)
(cid:0)k
2

(cid:18)

(cid:88)

arccos

(cid:28) Xi − p
(cid:107)Xi − p(cid:107)

,

Xj − p
(cid:107)Xj − p(cid:107)

(cid:29)

−

(cid:19)2

,

π
2

1≤i<j≤k

where (cid:104)·, ·(cid:105) denotes the dot product on Rm.

(3) Estimate the unknown dimension d as (cid:98)d, equal to the integer r such that βr is closest
to Uk,n, for a suﬃciently large sample size n, where βr is the quantity deﬁned by

βr :=

(cid:40) π2

4 − 2 (cid:80)s
12 − 2 (cid:80)s

π2

j=0

j=1

1

(2j+1)2 if r − 2 = 2s + 1 is odd or
1
(2j)2 if r − 2 = 2s is even.

The key idea of our estimator goes as follows: For large n and the chosen value of k, the
nearest neighbors of p in the data set, behave as uniform data on a small ball around p in
the embedded tangent space of M at this point, and the corresponding unit vectors, (Xi −
p)/(cid:107)Xi − p(cid:107), are nearly uniform on the unit sphere of the tangent space, Sd−1. For uniform
data on Sd−1, the expected angle between two random vectors is always π/2 (regardless of
d), but the variance of this angle decreases rapidly with d. Formula (3) gives the value of
this variance for every dimension r. Since our results below show that the U -statistic, Uk,n,
will converge in probability to βd for the actual dimension of M at p, estimation of d by
choosing the r such that βr closest to Uk,n will be consistent. An additional fact that helps
in this convergence is that the variance of Uk,n, which depends on the fourth moment of the
angles, is also converging rapidly to zero.

The following subsection establishes useful facts about angles between random points on

the unit sphere Sd−1 of Rd and, in particular, about moments of the function

(2)

(3)

(4)

h(z, z(cid:48)) =

arccos (cid:104)z, z(cid:48)(cid:105) −

(cid:16)

(cid:17)2

π
2

when computed on data uniformly distributed on Sd−1. Section 3, building on subsection
2.2, develops the theoretical results that serve as basis for the use of Uk,n on manifolds.

2.2. Angle-variance statistics for pairs of uniform points on Sd−1.

Lemma 2.1 (Angles between uniform vectors). Let Z1, Z2 be two independent vectors
with the uniform distribution on the unit sphere Sd−1 ⊆ Rd and let Θd := arccos(cid:104)Z1, Z2(cid:105) be
the angle between them. The following statements hold:

(1) The distribution of Θd is given by

P(Θd ≤ α) =

(cid:82) α
0 sind−2(φ)dφ
(cid:82) π
0 sind−2(φ)dφ
3

.

(2) The moment generating function of Θd, denoted by φd−2(s) := E (cid:2)esΘd(cid:3) is given by

φ2k(s) = esπ−1
sπ

(cid:81)k

j=1

(2j)2
(2j)2+s2

φ2k+1(s) = esπ+1
2(s2+1)

(cid:81)k

j=1

(2j+1)2
(2j+1)2+s2

according to whether d − 2 is even or odd respectively.

(3) In particular E[Θd] = π

2 for all d and Var[Θd] = βd where

βd :=





π2
4
π2
12

− 2 (cid:80)k

j=0

− 2 (cid:80)k

j=1

1
(2j + 1)2

1
(2j)2

if d − 2 = 2k + 1 is odd or

if d − 2 = 2k is even.

(4) The variance of the centered squared angle σ2

σ2
d =






− π4

8 + 12

k
(cid:80)
j=0

k
(cid:80)
j=1

(cid:32)

π2
4 − 2

1

(2j+1)4 + 2
(cid:32)

k
(cid:80)
j=0

1
(2j + 1)2
(cid:33)2

k
(cid:80)
j=1

1
(2j)2

− π4

120 + 12

1
(2j)4 + 2

π2
12 − 2

if d − 2 = 2k.

d := Var (cid:0)Θd − π
(cid:33)2

2

(cid:1)2 is given by

if d − 2 = 2k + 1 or

Proof.

(1) Passing to polar coordinates r, φ1, . . . , φd−1 with r ≤ 0, 0 ≤ φj ≤ π for
1 ≤ j ≤ d − 2 and 0 ≤ φd−1 ≤ 2π. The probability that Θd ≤ α is precisely the
fraction of the surface area of the sphere deﬁned by the inequality 0 ≤ φ1 ≤ α. Since
the surface element of the sphere is given by

dS = sind−2(φ1) sind−3(φ2) . . . sin(φd−2)dφ1 . . . dφd−1

the probability is given by
0 · · · (cid:82) π
(cid:82) π
0 · · · (cid:82) π
(cid:82) π

(cid:82) α
0
(cid:82) π
0

0

0

(cid:82) 2π
0 dS
(cid:82) 2π
0 dS

=

(cid:82) α
0 sind−2(φ)dφ
(cid:82) π
0 sind−2 φdφ

as claimed.

(2) We begin with a claim

Claim 2.2. Let u : R → R be a C 2-function and deﬁne
(cid:82) α
0 u(x)n sind−2(φ)dφ
Ad

Ed(u(x)) :=

where Ad := (cid:82) π

0 sind−2 φdφ. Then, the following recursion formula holds

d Ed(u(x)) = d Ed−2(u(x)) −

Ed(u(cid:48)(cid:48)(x)).

1
d

Proof. Using integration by parts one can show a recursive formula for Ad and con-
clude that

Ad :=

(cid:40) (2k)!
(2k d!)2
(2kk!)2
(2k+1)!)

π
2

if d − 2 = 2k or

if d − 2 = 2k + 1.

Applying integration by parts twice and using our formula for Ad gives the result. (cid:3)

4

In particular if we take u(x) = esx we get
(cid:19)

(cid:18) d2

Ed(esx) =

Ed−2(esx).

d2 + s2

(3) All densities are, like sine, symmetric around π

As a result we obtain the stated closed formula for the moment generating function.
2 and the ﬁrst statement follows.
To ease the computations we introduce the cumulant-generating function ψd−2 =
log(E(esΘd)). Then it is immediate that Var(Θd) = ψ(cid:48)(cid:48)
d−2(0). We consider two cases,
d even and odd, ﬁrst let us assume that d = 2k + 2. Then, we write the cumulant-
generating function as

ψd−2(s) = log

(cid:19)

(cid:18) esπ − 1
sπ

+

(cid:123)(cid:122)
t(s)

(cid:125)

k
(cid:88)

j=1
(cid:124)

(cid:124)

(cid:18) (2j)2

log

(2j)2 + s2
(cid:123)(cid:122)
r(s)

(cid:19)

.

(cid:125)

After some dry algebra we get t(cid:48)(cid:48)(0) = π2
result for the even case. The odd case follows from an analogous argument.

12 and r(cid:48)(cid:48)(0) = −2 (cid:80)k

1
(2j)2 , which gives the

j=1

(4) Let µj be the jth moment of the random variable (cid:0)Θd − π

(cid:1), i.e. µj = E (cid:0)Θd − π

(cid:1)j.

2

It is well known that µ2 = ψ(cid:48)(cid:48)(0) and µ4 = ψ(4)(0) + 3 (ψ(cid:48)(cid:48)(0))2 . Therefore,

(cid:16)

Var

Θd −

(cid:17)2

π
2

= µ4 − µ2

2 = ψ(4)(0) + 2 (ψ(cid:48)(cid:48)(0))2 .

Again, consider two cases: d even and d odd. Suppose d = 2k − 2, just as before, we
calculate t(4)(0) = − π4
1
(2j)4 . Substituting both these into (5)
yields the claim. A similar argument can be applied to the odd case.

120 and r(4)(0) = 12 (cid:80)k

j=1

At ﬁrst glance the formulas for βd and σd might seem a little complicated. In order to
derive our results we need tangible decrease rates in terms of the dimension. The following
claim gives us an easy way to interpret these quantities.

Claim 2.3. The following bounds hold for βd and σ2
d:

moreover the upper bound for σ2

d holds for all d ≥ 1.

Proof. We distinguish two cases according on whether d ≥ 1 is even or odd. If d is even, we
can deﬁne k by the equality d − 2 = 2k and compute

1
d

≤ βd ≤

1
d − 1

1
2d2 ≤ σ2

d ≤

2
(d − 1)2

for d ≥ 1,

for d ≥ 4,

βd = 2

1
(2j)2 .

∞
(cid:88)

j=k+1
5

2

(5)

(cid:3)

(6)

(7)

Since this series consists of monotonically decreasing terms and d ≥ 2 we conclude that

1
d

=

1
2k + 2

(cid:90) ∞

1

= 2

(2x)2 dx ≤ 2

k+1

∞
(cid:88)

j=k+1

1
(2j)2 ≤ 2

(cid:90) ∞

1

(2x)2 dx =

1
2k + 1

=

1
d − 1

k+ 1
2

as claimed. On the other hand, notice that the other term concerning the variance can be
written as

12

k
(cid:88)

j=1

1
(2j)4 −

π4
120

= −12

∞
(cid:88)

j=k+1

1
(2j)4 ,

which again can be bound by

2
d3 =

1

4(k + 1)3 = 12

(2x)4 dx ≤ 12

k+1

(cid:90) ∞

1

∞
(cid:88)

j=k+1

1
(2j)4 ≤ 2

(cid:90) ∞

1

k+ 1
2

(2x)2 dx =

2
(2k + 1)3 =

2
(d − 1)3 .

Then, we get

1
2d2 ≤

2
d2 −

2

(d − 1)3 ≤ σ2

d ≤

2
(d − 1)2 −

2
d3 ≤

2
(d − 1)2

where the ﬁrst inequality follows since d ≥ 4. The case when d is odd is proven similarly. (cid:3)

3. Theoretical foundations

3.1. Statement of results. In this subsection we state the theoretical results that serve
as basis for the proposed methodology. Proofs are given in the following subsection. The
setting is the following: An i.i.d. sample, X1, . . . , Xn, is available from a distribution P on
Rm. Additionally we have access to a distingushied point p, and near this point the data live
on a Riemannian C 2 manifold M , of dimension d < m. Furthermore, at p the distribution
P has a Lipschitz continuous non-vanishing density function g, with respect to the volume
measure on M . Without loss of generality, we assume that p = 0. Then, we have

Proposition 3.1 (Behavior of nearest neighbors). For a positive constant C, deﬁne
k = (cid:100)C log(n)(cid:101) and let R(n) = Lk+1(0) be the euclidean distance in Rm from p = 0 to its
(k + 1)-st nearest neighbor in the sample X1, . . . , Xn. Deﬁne BR(n)(0) to be the open ball of
radius R(n) around 0 in Rm. Then, the following holds true:

(1) For any suﬃciently large C > 0, we have that, with probability one, for large enough

n (n ≥ n0, for some n0 depending on the actual sample), R(n) ≤ r(n), where

(cid:32)(cid:18) log(n)

(cid:33)

(cid:19) 1

d

r(n) := O

n

is a deterministic function that only depends on the distribution P at p and C.

(2) Conditionally on the value of R(n), the k-nearest-neighbors of 0 in the sample X1, . . . , Xn,
have the same distribution as an independent sample of size k from the distribution
with density gn, equal to the normalized restriction of g to M ∩ BR(n)(0).

In what follows, with a slight abuse of notation, we will write X1, X2, . . . , Xk to denote
the k nearest neighbors of 0 in the sample and assume that these follow the distribution with
density gn of Proposition 3.1. Let π : Rm → TpM be the orthogonal projection onto the
(embedded) tangent space to M at p = 0. For a nonzero X ∈ Rm, let W := π(X), (cid:98)X := X
6

(cid:107)X(cid:107)

and (cid:99)W := W
space of M at 0.

(cid:107)W (cid:107). (cid:99)W takes values in the (d − 1)-dimensional unit sphere Sd−1 of the tangent

Our ﬁrst Lemma bounds the diﬀerence between the inner products (cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) and (cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)
In this Lemma, the random nature of the Xi is

in terms of the length of projections.
irrelevant.

Lemma 3.2 (Basic projection distance bounds). For any X, X1, X2 ∈ M :

(1) (cid:107)X − πX(cid:107) = O((cid:107)πX(cid:107)2)
(2) (cid:107) (cid:98)X − (cid:99)W (cid:107) = O((cid:107)πX(cid:107))
(3) The cosine of the angle between X1 and X2 is close to that between W1 and W2. More

precisely,

|(cid:104) (cid:99)X1, (cid:99)X2(cid:105) − (cid:104)(cid:99)W1, (cid:99)W2(cid:105)| ≤ Cr
for some C ∈ R, whenever r ≥ (cid:107)π(Xi)(cid:107) for i = 1, 2.

Using Lemma 3.2, we can establish the following approximation. Let X1, . . . , Xk be the
k-nearest-neighbors from the sample to p = 0 in Rm . Deﬁne Wi and (cid:99)Wi as above and let
Vk,n be given by the formula

Vk,n :=

1
(cid:1)
(cid:0)k
2

(cid:88)

(cid:16)

(cid:68)

(cid:69)

arccos

(cid:99)Wi, (cid:99)Wj

−

(cid:17)2

.

π
2

1≤i<j≤k

Proposition 3.3 (Approximating the statistic via its tangent analogue). For k =
C log(n), as above, we have

(1) The sequence k(Uk,n − Vk,n) converges to 0 in probability as n → ∞.
(2) limn→∞ E (Uk,n − Vk,n) = 0.

When X comes from the distribution producing the sample, but is restricted to fall very
close to 0, the distribution of πX will be nearly uniform in a ball centered at 0 in TpM . This
will allow us to establish a coupling between the normalized projection (cid:99)W and a variable
Z, uniformly distributed on the unit sphere of TpM , an approximation that leads to the
asymptotic distribution of Uk,n. Some geometric notation must be introduced to describe
these results. Since near 0, M ⊆ Rm is a Riemannian submanifold of dimension d, it
inherits, from the euclidean inner product in Rm, a smoothly varying inner product lp :
TpM × TpM → R, given by lp(u, v) = (cid:104)i∗(u), i∗(v)(cid:105), where i : M → Rm is the inclusion
with diﬀerential i∗. This metric determines a diﬀerential d-form ΩM which, in terms of
local coordinates ∂i for TpM and dual coordinates dxi of TpM ∗ with i = 1, . . . d, is given by
ΩM := (cid:112)det((cid:104)∂i, ∂j(cid:105))1≤i,j≤d)dx1 ∧ · · · ∧ dxd. The diﬀerential form endows M with a volume
measure ν(U ) = (cid:82)
U ΩM . We say that a random variable A on M has density g : M → R if
the distribution µA of A satisﬁes µA(D) = (cid:82)

D gΩM for all borel sets D in M .

If X is a random variable taking values on M with density g and r is a positive real number,
let X(r) be a random variable with distribution gr given by the normalized restriction of g
to M ∩ Br(0), that is:

gr(z) =

(cid:40)

(cid:82)

g(z)
Br (0)∩M gΩM
0, otherwise.

7

, if z ∈ M ∩ Br(0) and

Deﬁne W (r) := π(X(r)). The following geometric Lemma will be used for relating the

densities of X(r) and W (r).

Lemma 3.4 (Tangent space approximations). The following statements hold for all
suﬃciently small r and p = 0 in M .

(1) The map π : Br(0)∩M → π(Br(0)∩M ) is a diﬀeomorphism. Let Φ : Br(0)∩TpM →

Br(0) ∩ M be its inverse.

(2) The inclusion π(Br(0) ∩ M ) ⊆ Br(0) ∩ TpM holds and moreover |λ(Br(0) ∩ TpM ) −

λ(π(Br(0) ∩ M ))| = O(r) where λ denotes the Lebesgue measure on TpM .

(3) The following equality holds:

(cid:115)

1 −

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

1≤i,j≤d

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= O(r)

Let D(r) be a random variable uniformly distributed in Br(0) ∩ TpM and note that Z =
(cid:98)D(r) := D(r)
(cid:107)D(r)(cid:107) is uniformly distributed on the unit sphere Sd−1, regardless of the value of r.
Our next Lemma shows that under weak hypotheses there is a coupling between W (r) and
D(r) which concentrates on the diagonal as r decreases.

Lemma 3.5 (Coupling). Let r denote a small positive number. With D(r) as above and
Z a random vector with the uniform distribution on the unit sphere, Sd−1 of TpM , if the
density g of X in M , near 0, is locally Lipschitz continuous and nonvanishing at 0, then the
following hold:

(1) There exists a coupling A(r) = (W (r), D(r)) and a constant C > 0 such that

P{W (r) (cid:54)= D(r)} ≤ Cr for all suﬃciently small r.
(2) There exists a coupling A(cid:48)(r) = ( (cid:91)W (r), Z) such that P

(cid:110) (cid:91)W (r) (cid:54)= Z

(cid:111)

≤ Cr for all

suﬃciently small r.

The previous Lemma leads to the asymptotic distribution of the statistic Uk,n.

Theorem 3.6 (Local Limit Theorem for angle-variance). Let k := (cid:100)C log(n)(cid:101) for the
constant C of the proof of Proposition 3.1 and assume X1, . . . , Xk are the k nearest neighbors
to p = 0 in the sample, with respect to the euclidean distance in Rm. If dim TpM = d then
the following statements hold:

(1) The equality limn→∞ E[Uk,n] = βd holds and
(2) The quantity k (Uk,n − βd) converges, in distribution, to that of (cid:80)∞

1,i − 1)
1,i are i.i.d. chi-squared random variables with one degree of freedom and

where the χ2
the λi are the eigenvalues of the operator A on L2(Sd−1) deﬁned by

i=1 λi(χ2

(Au)(x) =

(h(x, z) − βd) u(z) dµ(z)

(cid:90)

Sd−1

for u ∈ L2(Sd−1), where h(v, v(cid:48)) := (cid:0)arccos(v · v(cid:48)) − π
measure on Sd−1.

2

(cid:1)2 and µ denotes the uniform

This limit theorem is obtained by the various approximation steps given in the preliminary
results together with the classical Central Limit Theorem for degenerate U statistics, as
described in Chapter 5 of [28]. Depending on the relative values of the λi’s appearing in
8

the statement of the Theorem, it could happen that the limiting distribution just obtained
approaches a Gaussian distribution as the dimension increases (this would happen if the λi
were such that Lindeberg’s condition holds).

Although theoretical study of the λi’s is left for future work, we conjecture that as d
increases the limiting distribution converges to a Gaussian distribution. Numerical experi-
ments seem to support our conjecture, see Figure 1.

(a) d = 2

(b) d = 25

(c) d = 50

Figure 1. QQ-plots. As established in the proof of Theorem 3.6, the limiting
distribution is in fact the asymptotic distribution of k(En−βd), with En deﬁned
in (Equation 14). For this ﬁgure, we generate 10000 samples of the variable
k(En − βd), for k = 10d in dimensions d = 5, 25, and 50. The plots compare
the quantiles of the sample distribution against the standard normal quantiles.

In order to get a consistency result for our basic local dimension estimator, we will use

the following fact.

Corollary 3.7 (Variance convergence). Under the conditions stated at the beginning of
this subsection,

(cid:18)k
2
for d equal to the dimension of M near 0.

lim
n→∞

(cid:19)

Var Uk,n = σ2
d

Recall from Section 2 that our basic procedure estimates the dimension d as (cid:98)d, equal to

the integer r such that βr is closest to Uk,n. This procedure is consistent, as stated next.

Proposition 3.8 (Consistency of basic dimension estimator). As in Section 2, write (cid:98)d
for the basic estimator described above. Let d be the true dimension of M in a neighborhood
of p = 0. Then, in the setting of the present section,

P(failure) = P( (cid:98)d (cid:54)= d) → 0,

as n → ∞.

3.2. Proofs.

Proof of Proposition 3.1.

Let us recall a probability bound for the Binomial distribution. For N an integer valued
random variable with Binomial(n, p) distribution and expected value λ = np, one of the
9

Chernoﬀ-Okamoto inequalities (see Section 1 in [16]) states that, for t > 0, P(N ≤ λ − t) ≤
exp(−t2/2λ). Letting t = λ/2, we get

P(N ≤ λ/2) ≤ exp(−λ/8).
(8)
For ﬁxed and small enough r > 0, let Br(0) denote the ball of radius r around 0 ∈ Rm. For a
random vector X, with the distribution P of our sample X1, X2, . . . , Xn, by our assumptions
on P and M near 0, we have that

P(X ∈ Br(0) ∩ M ) ≤ ανdrd,
where νd is the volume (Lebesgue measure) of the unit ball in Rd and α is a positive number.
Let N = Nr denote the amount of sample points that fall in Br(0) ∩ M . We have that
λ = E(N ) ≤ ανdrdn. We choose r such that λ ≤ C ln n, for a constant C to be speciﬁed in
a moment. Then, by (8), we get

(cid:18)

P

(cid:19)

C
2

N ≤

ln n

≤ exp(−C ln n/8) =

(cid:19)C/8

(cid:18) 1
n

(9)

Pick any value of C > 8. For this choice, the bound in (9) will add to a ﬁnite value when
summed over n. By the Borel-Cantelli Lemma, the inequality N > C
2 ln n will hold for all
n suﬃciently large. It follows that if k = C
2 ln n, the k-nearest-neighbors of 0 in the sample,
will fall in Br(0) for every n suﬃciently large and the chosen value of r, namely

r = r(n) =

(cid:19)1/d

(cid:18)C ln n
ανdn

which is OPr((ln n/n)1/d)). The proof of the ﬁrst part of the Proposition ends by renaming
C.
The statement of the second part of Proposition 3.1 is intuitive and has been used in the
literature without proof. Luckily, Kaufmann and Reiss [18] provide a formal proof of these
type of results in a very general setting. In particular, (ii) of Proposition 3.1 holds by formula
(cid:50)
(6) of [18].

Proof of Lemma 3.2
By an orthogonal change of coordinates, we can assume that TpM is spanned by the ﬁrst
d basis vectors in Rm. The projection π : M → TpM is a diﬀerentiable function whose
derivative at p = 0 is the identity. By the implicit function theorem we can conclude that
there exists an r > 0, such that π : Br(0) ∩ M → π(Br(0) ∩ M ) is a diﬀeomorphism and that
M admits, near p a chart Φ : Br(0) ∩ TpM → M of the form

Φ(z1, . . . , zd) = (z1, . . . , zd, F1(z1, . . . , zd), . . . , Ft(z1, . . . , zd))

(10)

where m = d + t, Φ(0) = p = 0 and such that ∂Fi
(0) = 0 for 1 ≤ i ≤ t and 1 ≤ j ≤ d. As
∂zj
a result, the euclidean distance between a point of M near 0 and the tangent space at 0 is
given, in the local coordinates z, by

d(z1, . . . , zd) =

F 2

i (z).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

t
(cid:88)

i=1

10

We will prove that there exists a constant K such that, for all suﬃciently small δ > 0 and
all z with (cid:107)z(cid:107) ≤ δ the inequality d(z) ≤ K(cid:107)z(cid:107)2 holds. By Applying Taylor’s Theorem
at 0 to the diﬀerentiable function d(z) we conclude, since Φ(0) = 0 and ∂Fi
(0) = 0, that
∂zj
the constant and linear term vanish from the expansion. This proves the claim because
(cid:107)z(cid:107)2 = (cid:107)π(Φ(z))(cid:107)2. Assume K is a constant which satisﬁes (cid:107)X − πX(cid:107) ≤ K(cid:107)πX(cid:107)2. Thus

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)πX(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ K(cid:107)πX(cid:107).

(11)

(12)

On the other side,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

X
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(cid:107)X(cid:107)

−

1
(cid:107)πX(cid:107)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (cid:107)X(cid:107)

=

1
(cid:107)πX(cid:107)

1
(cid:107)πX(cid:107)

|(cid:107)X(cid:107) − (cid:107)πX(cid:107)| ≤

(cid:107)X − πX(cid:107) ≤ K(cid:107)πX(cid:107).

Altogether,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)X(cid:107)

−

X
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
(cid:107)πX(cid:107)

−

πX
(cid:107)πX(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2K(cid:107)πX(cid:107),

where the ﬁrst inequality is just the triangle inequality and the second one follows from (11)
and (12). The third item in the Lemma follows immediately from the triangle inequality
(cid:50)
and the second item by adding and subtracting (cid:104)(cid:99)Xi, (cid:99)Wj(cid:105).

Remark 3.9. The quadratic term of G is the second fundamental form of M and, therefore,
the constant K can be chosen to be the largest sectional curvature of M at p.

Before proving Proposition 3.3, we need a Lemma on the behavior of the arccos function.

Lemma 3.10. Suppose that −1 ≤ c1 ≤ c2 ≤ 1 and let δ = c2 − c1 be suﬃciently small (for
our purposes it suﬃces to have δ ≤ 1/4). Then,

| arccos(c2) − arccos(c1)| ≤ 2(cid:112)|c2 − c1|

Proof. Assume ﬁrst that both c1 and c2 are positive. We have

| arccos(c2) − arccos(c1)| =

(cid:90) c2

c1

√

1
1 − x2

dx.

Using that the integrand in the last expression is increasing in [0, 1] and by the change of
variables u = 1 − x, we get

| arccos(c2) − arccos(c1)| ≤

(cid:90) 1

1−δ

√

1
1 − x2

dx =

(cid:90) δ

0

1
(cid:112)u(2 − u)

du ≤

(cid:90) δ

0

1
√
u

du

since 2 − u ≥ 1 for u ≤ δ. From this last bound, the result follows in this case by integration.
The argument for the case in which both c1 and c2 are negative is identical, by symmetry. In
the case c1 ≤ 0 ≤ c2, both c1 and c2 fall in a ﬁxed interval ([−1/4, 1/4]) where the derivative
(cid:3)
of arccos is bounded and the result follows easily.

11

Proof of Proposition 3.3
To prove part (1), putting together Proposition 3.1 and Lemma 3.2 we have

max
i≤k

(cid:107)(cid:99)Xi − (cid:99)Wi(cid:107) = OPr(r(n)) = OPr

(cid:19)1/d(cid:33)

(cid:32)(cid:18) ln n
n

From this, it follows easily that

and, using Lemma 3.10 we get

max
i<j≤k

|(cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) − (cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)| = OPr(r(n)),

| arccos(cid:104)(cid:99)Xi, (cid:99)Xj(cid:105) − arccos(cid:104) (cid:99)Wi, (cid:99)Wj(cid:105)| = OPr((cid:112)r(n))

max
i<j≤k

The bound is preserved by the application of the function u (cid:55)→ (u − π/2)2 (since the function
is locally Lipschitz) and by taking averages over all pairs, and we get
(cid:19)1/2d(cid:33)

Uk,n − Vk,n = OPr((cid:112)r(n)) = OPr

.

(13)

(cid:32)(cid:18) ln n
n

The result follows by observing that, for the value of k considered,

k OPr

(cid:32)(cid:18) ln n
n

(cid:19)1/2d(cid:33)

= oPr(1).

To prove (2), notice that from part (1) it is immediate that |Uk,n − Vk,n| converges to zero
in probability, which implies that limn→∞ E (Uk,n − Vk,n) = 0, since Uk,n − Vk,n is a bounded
(cid:50)
random variable.

Proof of Lemma 3.4
Recall, from the proof of Lemma 3.2, that for r > 0, small enough, the projection π :
Br(0) ∩ M → π(Br(0) ∩ M ) is a diﬀeomorphism and that M admits, near p a chart (inverse)
Φ : Br(0) ∩ TpM → M of the form given in (10) and satisfying that Φ(0) = p = 0 and such
that ∂Fi
(0) = 0 for 1 ≤ i ≤ t and 1 ≤ j ≤ d. Also from that proof, recall that there exists a
∂zj
constant K such that for small enough δ > 0 and all z with (cid:107)z(cid:107) ≤ δ, we have d(z) ≤ K(cid:107)z(cid:107)2,
where d(z) is the distance between a point z ∈ M and its projection on TpM . It follows
that the image π(Br(0) ∩ M ) contains a ball of radius r(cid:48) < r such that r − r(cid:48) = O(r2) and
therefore

Br(0) ∩ TpM ⊇ π(Br(0) ∩ M )) ⊇ Br(cid:48)(0) ∩ TpM,
proving part (2) of the Lemma, since the volume of the ﬁrst and last term diﬀer by at
= ei + (cid:80)m−d
most O(r2). For part (3) note that ∂Φ
are O(r2) the inner
∂zi
(cid:113)
products (cid:104) ∂Φ
, ∂Φ
(cid:105)
∂zj
∂zi
(cid:50)
is 1 + O(r) as claimed.

(cid:105) are 1+O(r2) if i = j and O(r2) otherwise and we conclude that

ed+t. Since ∂Ft
∂zi

, ∂Φ
∂zj

(cid:104) ∂Φ
∂zi

∂Ft
∂zi

t=1

Proof of Lemma 3.5
The total variation distance between two probability measures µ and ν, deﬁned as (cid:107)µ −
ν(cid:107)T V := supA |µ(A) − ν(A)|, satisﬁes

(cid:107)µ − ν(cid:107)T V = inf{P{X (cid:54)= Y } : A = (X, Y )}
12

where the inﬁmum runs over all couplings A = (X, Y ) of random variables X, Y with dis-
tributions given by µ and ν, respectively (see, for instance, page 22, Chapter 1 of [34]).
Moreover, if µ and ν are given by densities g1, g2 then the following inequality holds

(cid:107)µ − ν(cid:107)T V ≤ (cid:107)g1 − g2(cid:107)L1.
We will prove the ﬁrst part of the Lemma by bounding the L1-norm of the diﬀerence of
the densities of W (r) and D(r). Recall the deﬁnition of W (r) right before the statement of
Lemma 3.4. The diﬀerence of the two densities is given by

(cid:90)

Br(0)∩TpM

(cid:12)
(cid:12)
(cid:12)
(cid:12)

hr −

1
λ(Br)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dλ

where hr denotes the density of W (r) with respect to the d-dimensional Lebesgue measure
λ in TpM and Br := Br(0) ∩ TpM . More precisely, deﬁning Φ as in Lemma 3.4 the density
of W (r) is given by

hr(u) = gr(Φ(u))

det

(cid:115)

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

(u)

1≤i,j≤d

Since g is locally Lipschitz continuous there exists a constant K such that

g(0) − K1r ≤ g(Φ(u)) ≤ g(0) + K1r.

By Lemma 3.4 there exist constants K2, K3 such that the following inequalities hold for
u ∈ Br:

(cid:115)

1 − K2r ≤

det

(cid:29)

(cid:28) ∂Φ
∂xi

,

∂Φ
∂xj

1≤i,j≤d

(u) ≤ 1 + K2r and

λ(Br) − K3r ≤ λ(π(Br(0) ∩ M )) ≤ λ(Br) + K3r
Combining these inequalities we conclude that there exists a constant ˜K such that for all
u ∈ Br

λ(Br)(g(0) − ˜Kr) ≤

gΩ ≤ λ(Br)(g(0) + ˜Kr).

(cid:90)

Br(0)∩M

As a result the inequality

1
λ(Br)

(cid:32)

g(0) − ˜Kr
g(0) + ˜Kr

(cid:33)

− 1

≤ hr −

1
λ(Br)

≤

1
λ(Br)

(cid:32)

g(0) + ˜Kr
g(0) − ˜Kr

(cid:33)

− 1

so, using the fact that g(0) > 0 we conclude that there exists a constant such that

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Br

hr −

1
λ(Br)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ Cr

as claimed, ﬁnishing the proof of the ﬁrst part of the Lemma.
For part (2), let A(cid:48)(r) be the random pair obtained from A(r) = (W (r), D(r)) by normalizing
(cid:17)
(cid:16) (cid:91)W (r), (cid:91)D(r)
its components, that is A(cid:48)(r) =
= O(r),
because this probability is bounded above by the probability that W (r) and D(r) diﬀer.
Note that the random vector Z = D(r)
(cid:107)D(r)(cid:107) is uniform on the unit sphere, and in particular its
(cid:50)
distribution is independent of the value of r.

(cid:107)W (r)(cid:107) (cid:54)= D(r)

and note that P

(cid:110) W (r)

(cid:107)D(r)(cid:107)

(cid:111)

13

For k = (cid:100)C log(n)(cid:101), as before, let Z1, . . . , Zk be an i.i.d. sample distributed uniformly on the
unit sphere Sd−1 of TpM and deﬁne

En :=

1
(cid:1)
(cid:0)k
2

(cid:88)

(cid:16)

1≤i<j≤n

arccos(cid:104)Zi, Zj(cid:105) −

(cid:17)2

.

π
2

(14)

In view of Proposition 3.3, in order to prove Theorem 3.6 it will suﬃce to show that the
limiting standardized distribution of the Vk,n deﬁned in that Proposition is the same as that
of En, and establish the asymptotics for En.

Proof of Theorem 3.6

By Proposition 3.1 part (1) on a set of probability 1 (on the set of inﬁnite samples of

data), for some n0, the inequality Rn := R(n) ≤ r(n) holds for n ≥ n0.
By Proposition 3.1 part (2), for i = 1, . . . , k the distribution of Wi = π(Xi), the projections
on TpM of the k nearest neighbors of 0 in the sample, is that of an independent sample
W1(Rn), . . . , Wk(Rn), with the Wj(Rn), j ≤ k as deﬁned before Lemma 3.4.

From the previous Lemma, conditionally on Rn, we have a coupling A(cid:48)

j(Rn) = ( (cid:92)Wj(Rn), Zj)
for each j ≤ k. These couplings can be taken such that the (cid:92)Wj(Rn), j ≤ k form an i.i.d.
sample and the same holds for the Zj’s. By the previous lemma, we have that for each j,
P( (cid:92)Wj(Rn) (cid:54)= Zj) ≤ Cr(n). Then, except for a set of measure 0, we get the following event
inclusion

{kVk,n (cid:54)= kEn} ⊆

{ (cid:92)Wi(Rn) (cid:54)= Zi}

k
(cid:91)

j=1

By the union bound, the probability of the rightmost event is bounded by Ck(n)r(n) which
goes to zero, as n goes to inﬁnity by the choice of k(n) and the value of r(n). It follows that
Vk,n and En have the same standardized asymptotic distributions, and being both random
variables bounded, it follows that limn→∞ E(Vk,n − En) = 0.

For part (2) of Theorem 3.6 it only remains to establish the limiting distribution of En.
This statistic falls in the framework of classical U -statistics, that have played an important
role in the theory of many non-parametric procedures. See for instance [26] for several
applications of the theory of U -statistics. Even an empirical processes theory is available
for U -processes, see for instance [2], which has found application in the study of notions of
multivariate depth. Still, we will only require the classical theory, as exposed in Chapter 5
of [28] and Chapter 3 of [26].

Recall the deﬁnition of the kernel h in Equation 4. By the symmetry of the uniform
distribution for three independent vectors, Z1, Z2, Z3 with uniform distribution on Sd−1, it
can be easily veriﬁed that,

This means that the U -statistic associated to h is degenerate. It follows (see the variance

calculation in [26]) that

E (cid:0)h(Z1, Z2)h(Z1, Z3) − β2

(cid:1) = 0.

d

(cid:19)

(cid:18)k
2

Var(En) = Var(h(Z1, Z2))

(15)

(16)

and by the Theorem for degenerate U -statistics in Section 5.5 of [28], part (2) of our Theorem
(cid:50)
follows.

14

Proof of Corollary 3.7
By (16), it suﬃces to show that k2 lim(EU 2
k,n − EE2
n) = 0, for En as in the proof of Theorem
3.6. Applying equation (13) and the facts that the function h is bounded and that k = O(ln n)
on a set of probability 1, we have

k2EU 2

k,n ≤ k2EV 2

k,n + O

ln2 n

(cid:32)

(cid:19)1/2d(cid:33)

.

(cid:18)ln n
n

The opposite inequality (interchanging the roles of Uk,n and Vk,n) is obtained by the same
reasoning, and we get

(17)
By the coupling argument of the proof of Theorem 3.6, we have that Vk,n and En might
diﬀer at most on a set of measure O(ln(n) r(n)). Taking expectations on the sets were they
coincide and diﬀer, we obtain

k,n) = 0.

lim k2(EU 2

k,n − EV 2

k2 EV 2

k,n ≤ k2 EE2

n + k2 O(ln(n) r(n)).

Observing that the second term in the right hand side of this inequality goes to zero, as n
grows, and that the reverse inequality is obtained similarly, we conclude

and the result follows by combining (17) and (18).

lim k2(EV 2

k,n − EE2

n) = 0,

(18)
(cid:50)

Proof of Proposition 3.8.
Let d be the true value of the dimension. βd+1 is the expected value closest to βd and, by
the proof of Claim 2.3, (βd − βd+1)/2 ≥ 1/(d − 1)2. For the basic procedure to incur in error
it is necessary that |Uk,n − βd| ≥ |Uk,n − βd+1|, which means
βd − βd+1
2

|EUk,n − βd| + |Uk,n − EUk,n| ≥

1
(d − 1)2 .

≥

1

Since |EUk,n − βd| converges to zero, the condition above requires that |Uk,n − EUk,n| ≥
1/
2(d−1)2 , for n large enough. The probability of this last event is bounded as follows. Let c
be a sup norm bound for h( (cid:99)X1, (cid:99)X2) − Eh( (cid:99)X1, (cid:99)X2), for the kernel h given in (4) and (cid:99)Xi as in
Lemma 3.2. Clearly, c is bounded above by π2/4. By Bernstein’s inequality for U -statistics
[2, Proposition 2.3(a)], we get

(cid:18)

P

|Uk,n − EUk,n| ≥

(cid:19)

1
2(d − 1)2

(cid:32)

≤ 2 exp

−k/(8(d − 1)4)
2Var(h( (cid:99)X1, (cid:99)X2)) + 2c

6(d−1)2

(cid:33)

Now, using Claim 2.3 and Corollary 3.7, we have, after some calculations,

(cid:18)

P

|Uk,n − EUk,n| ≥

(cid:19)

1
2(d − 1)2

(cid:32)

≤ 2 exp

−k/(8(d − 1)4)
(d−1)2 + π2

12(d−1)2

5

(cid:33)

≤ 2 exp

(cid:18)

−k
47(d − 1)2

(cid:19)

This bound goes to zero as n (and k) grow to inﬁnity, ﬁnishing the proof.

(19)

(cid:50)

Forcing the estimation error bound in (19) to be less that a given δ > 0 will give a value of
k of the order of Ad 2 ln(2/δ), for some constant A, reﬂecting that precise estimation is more
demanding, in terms of sample size, as the dimension d grows.

15

4. Estimators

In this section we present two dimension estimators based on the statistic Uk,n. First,
a local estimator that gives the dimension of M around a distinguished non-singular point
p ∈ M is discussed. Then, the case in which the manifold is equidimensional is considered,
by building upon our local estimator to propose a global dimension estimator. Some imple-
mentation issues are discussed and in the following section we evaluate the performance of
our estimators. The programming code used in these experiments is publicly available, it
can be found at https://github.com/mateodd25/ANOVA_dimension_estimator.

Algorithm 1: Local dimension estimation
Data: k ∈ N+ and X1, . . . , Xn, p ∈ M ⊆ Rm
Result: Estimated dimension (cid:98)d at p ∈ M
Find the k-nearest neighbors to p;
Use these neighbors to compute Uk,n as in (2);
Choose (cid:98)d associated with Uk,n;

4.1. Local estimators. The theory presented in Section 3 suggests that k ∼ log(n) should
be a good choice, asymptotically-speaking, for the number of neighbors to consider in the
local dimension estimation procedure. However, one could potentially leverage prior knowl-
edge of the structure of the problem to set this parameter diﬀerently. In our implementation
we set it to k = round(10 log10(n)).

In our theoretical analysis presented above, it was assumed that we are given a center
point p where the local dimension is to be estimated. A natural question that arises in
practice is the following: given a sample, how to select good center points. In Section 4.2
we present a simple heuristic to select “good” centers.

Both estimators presented in what follows are based on the Algorithm 1. The diﬀerence
between the estimators considered lies on the last line of the algorithm, namely, on how to
pick the dimension estimator, given Uk,n. Next, the two diﬀerent rules to execute this step
are discussed.

4.1.1. Basic estimator. Since Uk,n converges in probability to βd, a natural way to estimate
the dimension from Uk,n is to set

(cid:98)dbasic := arg min
d∈[Dmax]

|βd − Uk,n|,

where Dmax is the ambient dimension or some bound we know a priori on the dimension
of the manifold. Interestingly, such a rule is fairly accurate, as established in Proposition
3.8. Another advantage of this estimator is that there is no need to train it, since all the
quantities involved have been analytically computed (and presented in Section2).

Remark 4.1. Classical discriminant analysis results (see, for example, Section 4.1 in [11])
would advice to incorporate available variance information on the selection of (cid:98)d, by choosing

(cid:98)ddisc := arg max{d | Uk,n ≥ ηd}

where

ηd = βd +

(βd−1 − βd).

σd
σd + σd+1

16

Algorithm 2: Global dimension estimation
Data: c ∈ N+, k ∈ N+ and X1, . . . , Xn ∈ M ⊆ Rm
Result: Estimated dimension (cid:98)d of M
Choose c centers p1, . . . , pc from the sample;
Apply Algorithm 1 to each center pi, let (cid:98)di be its output;
Set (cid:98)d to the median of { (cid:98)di}c
i=1;

Still, simulation evaluations (not included) show that (cid:98)ddisc and (cid:98)dbasic have a very similar
performance in practice (and also in theory, since both are consistent). Thus, we prefer to
use the later, being the simpler one.

1

, . . . , Y (d)

4.1.2. Kernel-based estimator. For our second estimator we start by simulating multiple
instances Y (d)
M of the random variable k(En − βd), for a large value of M (=5000,
for instance) and with En as deﬁned in equation (14), for each dimension d ∈ [Dmax] =
{1, . . . , Dmax}. From these data, the density ˆf (d)
k , of k(En − βd), is estimated, for each d, as
M
(cid:88)

(cid:32)

(cid:33)

ˆf (d)
k (y) =

1
M h

ϕ

y − Y (d)
h

i

i=1
where ϕ(·) is the standard Gaussian density and the parameter h (the “bandwidth”) can be
set at h = (4/3M )1/5. This choice of bandwidth guarantees consistent density estimation (see
Section 4.1 in [4]). Then, a Bayesian classiﬁcation procedure with uniform prior distribution
on the set [Dmax], would select the dimension as that for which the kernel density estimator
is maximized at the standardized Uk,n, namely
ˆf (d)
k (k(Uk,n − βd)).

(cid:98)dker = arg max
d∈[Dmax]

(20)

Notice that the simulations described above need to be performed only once for each dimen-
sion, since they are made on uniform data on Sd−1 and do not depend on the particular data
being studied.

4.2. Global estimators. We now turn our attention to extending the local dimension es-
timation algorithm to a global one. Assuming that M is equidimensional, the local method
can be extended by running multiple instances of Algorithm 1 on diﬀerent centers, and
combining the results, as outlined in Algorithm 2.

After getting dimension estimates at each center, one could use diﬀerent summary statistics
to choose the global dimension, such as the mean, the mode or the median. To make a method
robust against outliers, we chose to use the median. To decide about the parameter c we
ran a cross-validation algorithm. Empirically, it appears that c ∼ log(n) is a good choice for
this parameter.

4.2.1. Choosing centers. To pick the centers pi in Algorithm 2, we divide the sample into c
disjoint subsamples of approximately equal size. Assume, for simplicity of the exposition,
Inside each subsample we pick a center by assigning each point a score of
that c = 1.
centrality and then choosing the one with highest score. Scores are assigned through the
following procedure:

17

(1) For each coordinate i, we order the sample based on the i-th entry, let τi be the
permutation giving this ordering, that is, the i-th row in (Xτi(1), . . . , Xτi(n)) is non-
decreasing.

(2) Then, the centrality score of Xj is given by (cid:80)m

(cid:12)
(cid:12)
(cid:12) .
For the i-th coordinate, the weight function f gives the maximum scores to the point (or
points) such that τi(j) is closest to n/2 and thus, the mechanism used chooses as center a
point which for many components, appears near the center of these orderings.

i=1 f (τi(j)), where f (x) =

2 − 2(x−1)

(cid:12)
(cid:12)
(cid:12)

2n

1

4.2.2. Heuristic to discard centers. Finally, we present a simple heuristic for discarding some
of the selected centers, based on the mean of the angles between its neighbors, taking as
always, the point considered as origin. This is done in order to improve the performance of
the statistic. Consider again the angle

(cid:28) Xi − p
(cid:107)Xi − p(cid:107)
for each pair of nearest neighbors Xi, Xj of the point p ∈ M , as used in the basic deﬁnition
(2). Consider the average of these angles,

Xj − p
(cid:107)Xj − p(cid:107)

θi,j = arccos

(cid:29)

,

θ(p) =

1
(cid:1)
(cid:0)k
2

(cid:88)

θi,j.

1≤i<j≤k

If the manifold M , near p, is approximately ﬂat, θ(p) should be close to π/2, regardless of
the value of the dimension d, since π/2 is the expected value of the angle between uniformly
sampled points in every dimension and the U -statistic θ(p) should converge rapidly to this
expectation. Thus, when θ(p) is far from π/2, it can be taken as a suggestion of strong
curvature that is causing non-uniformity of the angles, and therefore, p might not be a good
point to consider for dimension estimation. For these reason, in our implementation, the user
is allowed to use this heuristic and discard a fraction of the centers pi with largest values of
|θ(pi) − π/2|. Experiments presented in the next section suggest that this heuristic is useful
when the manifold is highly curved.

5. Numerical results

We compare our methods against two powerful dimension estimators, DANCo [9] and
Levina-Bickel [20], using a manifold library proposed in [15]. The ﬁrst estimator is, ar-
guably, the state-of-the-art for this problem, while the second one is a classical well-known
estimator with great performance. To see a comparison between these and other estimators
we refer the reader to [8, 9].

Table 1 presents a brief description of the manifolds included in the study. Additionally
Table 2 contains a list of the parameters used for each one of the estimators. We compare
two error measures, namely the Mean Square Error (MSE) and the Mean Percentage Error
(MPE) which are deﬁned as

MSE( (cid:98)d) :=

( (cid:98)di − di)2

and

MPE( (cid:98)d) :=

1
T

T
(cid:88)

i=1

100
T

T
(cid:88)

i=1

| (cid:98)di − di|
di

where T is the number of trials included in the test and (cid:98)di and di are the estimated dimension
and the correct dimension of the ith trial, respectively.

18

For each one of the aforementioned manifolds, we draw T = 50 random samples with
n = 2500 data points and then compute the MSE and MPE of the following four esti-
mators: the global basic estimator (Basic), the global basic estimator combined with the
centers heuristic (B+H), the global kernel-based estimator (Kernel), the global kernel-based
estimator with centers heuristic (K+H), the Levina-Bickel estimator (LB), and the DANCo
estimator. Tables 3 and 4 summarize the results.

Table 1. Library of manifolds used for benchmark, for more details consult [15].

Description
Sphere S9
Aﬃne subspace
Nonlinear manifold
Nonlinear manifold
Helix
Nonlinear manifold
Swiss roll
Highly curved manifold
Full-dimensional cube
9-dimensional cube

Manifold d m
10
9
5
3
6
4
8
4
3
2
36
6
2
3
12 72
20 20
10
9
2
3 Ten-times twisted Mobius band
10 10
10
1

M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
M11
M12
M13

Multivariate Gaussian
Curve

Table 2. Parameters of each algorithm.

Method Parameters
ANOVA k = 34, c = 16
k1 = 10, k2 = 20
k = 10

LB
DANCo

In both these tables, the last column shows the average of the performance measure over
the examples. It is clear from these tables that the angle-variance methods introduced in the
present article do well, in terms of average performance, against the very strong competitors
considered. This is more evident when the measure of error is the MSE. Still, for many of the
manifolds considered, namely M1, M3, M4 (in this case tied with LB), M9 and M12, DANCo
clearly displays the best performance. The Levina-Bickel estimator is the best for manifold
M6, tying for ﬁrst with DANCo in M4, while the procedures proposed in this article show
the best performance in the cases of manifolds M8 and M10, having very good performance
also in cases M3, M5, M7 and M12. It is interesting that our methods do particularly well
in case M8, a high curvature manifold of a relatively high dimensional in a large dimension
ambient space. It does not appear to exist a signiﬁcant diﬀerence in performance between
the Basic procedure and the procedure that uses Kernel Density Estimation. On the other
hand, the introduction of the heuristics discussed in Section 4 turns out to be beneﬁcial for
our estimators in some of the relatively high dimensional cases, namely M9 and M12, while
19

Table 3. Rounded Mean Square Error for diﬀerent manifolds, last column
displays the average MSE over all the examples. The darker cells show the
best results in each column.

M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 Mean
1.72
1.11
Basic
0.95 0.00 0.58 0.03 0.00 0.67 0.00
2.14
0.79
B+H 1.09 0.00 0.66 0.28 0.00 1.30 0.01
1.27
1.10
Kernel 0.95 0.00 0.68 0.08 0.00 0.69 0.29
0.80
2.48
K+H 0.99 0.00 0.76 0.45 0.00 1.31 0.31
2.69
2.27
0.49 0.02 0.05 0.00 0.00 0.10 0.00
LB
2.11
DANCo 0.16 0.00 0.00 0.00 0.00 1.00 0.00 25.22

10.39 0.00 0.00 0.12 0.00
4.64
0.03 0.00 0.10 0.00
10.20 0.00 0.00 0.15 0.00
4.06
0.01 0.00 0.04 0.00
29.33 2.23 0.00 0.54 0.00
0.10 0.00 0.00 0.00
0.96

Table 4. Rounded Mean Percentage Error for diﬀerent manifolds, last col-
umn displays the average MPE over all the examples.

M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 Mean
9.00
1.00
4.93
Basic
10.56 0.00 15.50
5.95
9.83
8.25
B+H 11.33 0.00 17.75
6.45
7.75
2.50
Kernel 10.56 0.00 17.75
7.67
K+H 11.00 0.00 20.00 12.25 0.00 18.00 17.50 10.75
7.31
1.49
LB
5.09
0.00
DANCo

0.00 1.50 0.00
0.00 1.20 0.00
0.00 1.80 0.00
0.00 0.60 0.00
12.55 27.07 16.57 1.64 7.34 0.50
0.00 0.00 0.00
41.83

0.00
0.00 10.83
0.00 17.83
0.50
0.00 11.50 16.50

1.84
5.13
0.00 16.66

15.50
10.00
15.55
9.60

0.00
0.67
0.00
0.11

2.63
0.00

5.77
0.00

4.84
0.00

7.77
1.77

1.11

4.80

these heuristics degrade somehow the performance in the intermediate dimension cases, M4
and M6. In all other cases, the use of the heuristics for center selection and center elimination
does not appear to have a strong eﬀect.

References

[1] Aldous, D. and J. Shun (2010) Connected spatial networks over random points and a route-length

statistic. Statistical Science 25, 275 - 288.

[2] Arcones, M. A. and Gin´e, E. (1993) Limit Theory for U -Processes. Annals of Probability 21, No. 3,

1494-1542.

[3] Belkin, M. and Niyogi, P. (2004) Semi-supervised learning on Riemannian manifolds. Invited Paper

inMachine Learning. Special Issue on Clustering 56, 209-239.

[4] Bowman, A. W. and Foster, P. J. (1993) Adaptive smoothing and density-based tests of multivariate

normality. Journal of the American Statistical Association 88 No. 422, 529-537.

[5] Breiding, P., Kalisnik, S., Sturmfels, B. and Weinstein, M. (2018) Learning Algebraic Varieties from

Samples. arXiv:1802.094[math.AG]

[6] Brito, M. R., Quiroz, A. J. and Yukich, J. E. (2002) Graph theoretic procedures for dimension identiﬁ-

cation. Journal of Multivariate Analysis 81, 67-84.

[7] Brito, M. R., Quiroz, A. J. and Yukich, (2013) Intrinsic dimension identiﬁcation via graph-theoretic

methods. Journal of Multivariate Analysis 116, 263277.

[8] Campadelli, P., Casiraghi, E., Ceruti, C. and Rozza, A. (2015) Intrinsic Dimension Estimation: Relevant
Techniques and a Benchmark Framework. Mathematical Problems in Engineering 2015 Article ID:
759567, 21 pages.

[9] Ceruti, C., Bassis, S., Rozza, A., Lombardi, G., Casiraghi, E. and Campadelli, P. (2014) DANCo: An
intrinsic dimensionality estimator exploiting angle and norm concentration. Pattern Recognition 47, No.
8, 2569-2581.

20

[10] Costa, J. A., Girotra, A. and Hero, A .O. (2005) Estimating local intrinsic dimension with k-nearest
neighbor graphs. In IEEE/SP 13th Workshop on Statistical Signal Processing, 417-422. IEEE Conference
Publication.

[11] Devroye, Luc, Gy¨orﬁ, L´aszl´o and Lugosi, G´abor. (2013) A probabilistic theory of pattern recognition

(Vol. 31). Springer Science and Business Media.

[12] Duda, R. O., Hart, P. E. and Stork, D. G. (2001) Pattern Classiﬁcation. 2nd. edition. John Wiley and

Sons, New York.

189-208.

[13] Farahmand, A., Szepesv´ari, C. and Audibert, J-Y (2007) Manifold-adaptive dimension estimation. In
Proceedings of the 24th International Conference on Machine Learning, Z. Ghahramani, editor, 265-272.
ACM, New York.

[14] Grassberger, P. and Procaccia, I. (1983) Measuring the strangeness of strange attractors. Physica 9D,

[15] Hein, M. and Audibert, J.-Y. (2005) Intrinsic dimensionality estimation of submanifolds in Rd. In

Proceedings of the 22nd International Conference on Machine learning, 289-296, ACM.

[16] Janson, S. (2002). On concentration of probability. In Bollob´as, B. (Ed.) Contemporary Combinatorics,
10. Proceedings of the Workshop on Probabilistic Combinatorics at the Paul Erd¨os Summer Research
Center, Budapest, 1998, pp. 289-301.

[17] Johnson, M. E. (1987) Multivariate Statistical Simulation. John Wiley and Sons, New York.
[18] Kaufmann, E. and Reiss, R.-D. (1992) On Conditional Distribution of Nearest Neighbors. Journal of

Multivariate Analysis 42, 67-76.

[19] Kegl, B. (2003) Intrinsic dimension estimation using packing numbers. In Advances in Neural Infor-
mation Processing Systems, Volume 15, Eds. S. Becker, S. Thrun and K. Obermayer. M.I.T. Press,
Cambridge, Massachusetts.

[20] Levina, E. and Bickel, P. J. (2005) Maximum likelihood estimation of intrinsic dimension. In Advances
in Neural Information Processing Systems, Volume 17, Eds. L. K. Saul, Y. Weiss and L. Bottou.
[21] Lombardi et. al., 2011 Lombardi, G., Rozza, A., Ceruti, C., Casiraghi, E. and Campadelli, P. (2011).
Minimum Neighbor Distance Estimators of Intrinsic Dimension. In D. Gunopulos et al. (Eds.): ECML
PKDD 2011, Part II, LNAI 6912, pp. 374-389. Springer-Verlag. Berlin.

[22] Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979) Multivariate Analysis. Academic Press, New York.
[23] Penrose, M. D. and Yukich, J. E. (2001) Central limit theorems for some graphs in computational

geometry. Annals of Applied Probability 11, 1005-1041.

[24] Penrose, M. D. and Yukich, J. E. (2013) Limit theory for point processes in manifolds. Annals of Applied

Probability 23, No. 6, 2161-2211.

[25] Pettis, K. W., Bailey, T. A. Jain, A. K. and Dubes, R. C. (1979) An intrinsic dimensionality estimator
from near-neighbor information. IEEE Transactions on Pattern Analysis and Machine Intelligence 1,
25-37.

[26] Randles, R. H. and Wolfe, D. A. (1979) Introduction to the Theory of Nonparametric Statistics. John

[27] Roweis, S. T. and Saul, L. K. (2000) Nonlinear dimensionality reduction by locally linear embedding.

[28] Serﬂing, R. J. (1980) Approximation Theorems of Mathematical Statistics. John Wiley and Sons, New

[29] Sindhwani, V., Belkin, M. and Nigoyi, P. (2006) The Geometric Basis of Semi-supervised Learning.
Book chapter in Semi-supervised Learning, O. Chapelle, B. Sch¨olkopf and A. Zien, editors, M.I.T.
Press, Cambridge, Massachusetts.

[30] S¨odergren, A. (2011) On the distribution of angles between the N shortest vectors in a random lattice

Journal of the London Mathematical Society, 84, No. 3, 749-764.

[31] Sricharan, K. Raich, R. and Hero, A. O. (2010) Optimized intrinsic dimension estimation using near-
est neighbor graphs. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 5418-5421. IEEE Conference Publication.

[32] Steele, J. M., Shepp, L. A. and Eddy, W. F. (1987) On the number of leaves of a Euclidean minimal

spanning tree. Journal of Applied Probability, 24, 809-826.

Wiley and Sons, New York.

Science, 290, 2323-2326.

York.

21

[33] Tenenbaum, J. B., de Silva, V. and Langford, J. C. (2000) A global geometric framework for nonlinear

dimensionality reduction. Science 290, 2319-2323.

[34] Villani, C. (2009) Optimal Transport. Old and New. Volume 338 in series Grundlehren der matematischen

[35] Yukich, J. E. (1998) Probability Theory of Classical Euclidean Optimization Problems. Lecture Notes in

Wissenschaften. Springer. Berlin, Heidelberg.

Mathematics, 1675, Springer, New York.

Mateo D´ıaz, Center for Applied Mathematics, 657 Frank H.T. Rhodes Hall, Cornell

University, Ithaca, NY 14853

E-mail address: md825@cornell.edu

Adolfo J. Quiroz, Departamento de Matem´aticas, Universidad de los Andes, Carrera 1

No. 18a 10, Edificio H, Primer Piso, 111711 Bogot´a, Colombia

E-mail address: aj.quiroz1079@uniandes.edu.co

Mauricio Velasco, Departamento de Matem´aticas, Universidad de los Andes, Carrera 1

No. 18a 10, Edificio H, Primer Piso, 111711 Bogot´a, Colombia

E-mail address: mvelasco@uniandes.edu.co

22

