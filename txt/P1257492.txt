Coupling Adaptive Batch Sizes with Learning Rates

7
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
6
8
0
5
0
.
2
1
6
1
:
v
i
X
r
a

Lukas Balles
MPI for Intelligent Systems
T¨ubingen, Germany

Javier Romero∗
Body Labs Inc.
New York, NY, USA

Philipp Hennig
MPI for Intelligent Systems
T¨ubingen, Germany

Abstract

Mini-batch stochastic gradient descent and
variants thereof have become standard for
large-scale empirical risk minimization like
the training of neural networks. These meth-
ods are usually used with a constant batch size
chosen by simple empirical inspection. The
batch size signiﬁcantly inﬂuences the behav-
ior of the stochastic optimization algorithm,
though, since it determines the variance of the
gradient estimates. This variance also changes
over the optimization process; when using a
constant batch size, stability and convergence
is thus often enforced by means of a (manually
tuned) decreasing learning rate schedule.

We propose a practical method for dynamic
batch size adaptation.
It estimates the vari-
ance of the stochastic gradients and adapts the
batch size to decrease the variance proportion-
ally to the value of the objective function, re-
moving the need for the aforementioned learn-
ing rate decrease. In contrast to recent related
work, our algorithm couples the batch size to
the learning rate, directly reﬂecting the known
relationship between the two. On popular im-
age classiﬁcation benchmarks, our batch size
adaptation yields faster optimization conver-
gence, while simultaneously simplifying learn-
ing rate tuning. A TensorFlow implementation
is available.

1 INTRODUCTION

In parametric machine learning models, like logistic re-
gression or neural networks, the performance of a pa-
rameter vector w ∈ Rd on datum x is quantiﬁed by a
∗ Work done while at MPI for Intelligent Systems.

loss function ℓ(w; x). Assuming the data comes from a
distribution x ∼ p, the goal is to minimize the expected
loss, or risk,

R(w) = Ex∼p[ℓ(w; x)].

(1)

We consider empirical risk minimization tasks of the
form

min
w∈Rd

F (w) =

ℓ(w; xi),

(2)

1
M

M

i=1
X

where the risk is approximated using a training set
{x1, . . . , xM } of data sampled (approximately) from p.
Typical optimization algorithms used to minimize (2) re-
peatedly evaluate the gradient

∇F (w) =

∇ℓ(w; xi).

(3)

1
M

M

i=1
X

For large-scale problems where M and/or d are large, it
is inefﬁcient or impossible to evaluate the exact gradient
(3), and one typically resorts to stochastic gradients by
randomly drawing a mini-batch B ⊂ {1, . . . , M }, |B| =
m ≪ M , at each step of the optimization algorithm and
using the gradient approximation

1
m

g(w) =

∇ℓ(w; xi).

(4)

i∈B
X
The simplest, but still widely used, stochastic opti-
mization algorithm is stochastic gradient descent (SGD,
Robbins & Monro, 1951), which updates

wk+1 = wk − αkg(wk),

(5)

where αk ∈ R+ is the step size parameter, often called
learning rate in the machine learning context. Variants of
SGD include ADAGRAD (Duchi et al., 2011), ADADELTA
(Zeiler, 2012), and ADAM (Kingma & Ba, 2015). We re-
strict our considerations to SGD in this paper.

(7)

(8)

(9)

1.1 THE EFFECT OF THE BATCH SIZE

If i is drawn uniformly at random from {1, . . . , M },
∇ℓi(w) = ∇ℓ(w; xi) is a random variable with mean

E[∇ℓi(w)] =

∇ℓi(w) = ∇F (w),

(6)

1
M

M

i=1
X

and covariance matrix
Σ(w) := cov [∇ℓi(w)] =

1
M

M

i=1
X

set (7) is prohibitively costly to compute, but it can be
estimated by the sample variance computed on a mini-
batch. As will be described below, we only require the
diagonal elements of Σ(w), corresponding to the vari-
ances of the individual components. These can be esti-
mated by

S(w) =

∇ℓi(w).2 − g(w).2 ∈ Rd,

(10)

1
m

i∈B
X

where .2 signiﬁes an element-wise square.

(∇ℓi(w) − ∇F (w))(∇ℓi(w) − ∇F (w))T .

1.2 NOTATION

Likewise, a stochastic gradient g(w) computed on a
randomly-drawn mini-batch B is a random variable with
mean ∇F (w). Assuming that it is composed of m sam-
ples drawn independently with replacement, its covari-
ance matrix is

The following discussion addresses the choice of batch
size for a single SGD step, assuming that we are currently
at some arbitrary but ﬁxed point w in parameter space.
For notational convenience, we will thus drop w from
the notation and write F = F (w), ∇F = ∇F (w), et
cetera.

cov[g(w)] =

Σ(w)
m

and, by the Central Limit Theorem, g(w) is approxi-
mately normally distributed:

g(w) ∼ N

∇F (w),

(cid:18)

Σ(w)
m

.

(cid:19)

When sampling without replacement, as is usually done
in practice, the same holds approximately as long as
m ≪ M .

In practice, the batch size m is often set to a ﬁxed value,
which is chosen ad hoc or by simple empirical tests. But
it is actually a crucial variable, which poses an intricate
trade-off that affects the optimizer’s performance. On
the one hand, the variance of the stochastic gradients de-
creases linearly with m, so small batches give vague gra-
dient information, thus slow convergence in the number
of optimization steps. On the other hand, the cost per
step increases linearly with m. (This assumes that batch
sizes are large enough to fully utilize the available paral-
lel computing resources, which can easily be guaranteed
by enforcing an appropriate minimal batch size.) While
we can thus linearly trade off variance and cost, the gra-
dient variance does not linearly affect the performance of
the optimizer; its effect depends on the local structure of
the objective and interacts with other parameter choices
of the optimizer, notably the learning rate. In general,
there should thus be an optimal batch size that balances
these two aspects. Choosing such good batch sizes is an
important aspect in the design of a numerical optimizer.

Below, we propose an algorithm that adapts the batch
size based on the gradient variance observed by the opti-
mizer at runtime. The exact variance over the entire data

2 RELATED WORK

derive

(2012)

adaption of batch sizes has

al-
The dynamic
recent works.
ready attracted attention in other
decreasing
Friedlander & Schmidt
series of bounds on the gradient variance that provably
yield fast convergence rates with a constant learning
rate, showing that an increasing batch size can replace
a decreasing learning rate. To realize these bounds in
practice, they propose to increase the batch size by a
pre-speciﬁed constant factor in each iteration, without
adaptation to (an estimate of) the gradient variance.

The prior works closest to ours in spirit are by Byrd et al.
(2012) and De et al. (2017), who propose to adapt the
batch size based on variance estimates. Their criterion is
based on the observation that −g is a descent direction if

kg − ∇F k ≤ θkgk, with

0 ≤ θ < 1

(11)

(proof in Appendix A). While the left-hand side of (11) is
of course unknown, one can compute its expected square

E

kg − ∇F k2

=

E

(gj − ∇Fj)2

(cid:2)

d

j=1
X
d

(cid:3)

=

(cid:3)

(12)

(cid:2)
σ2
jj
m

=

tr(Σ)
m

.

j=1
X
Consequently, (11) holds in expectation if tr(Σ)/m ≤
θ2kgk2 or (with equality)

m =

1
θ2

tr(Σ)
kgk2 .

(13)

While this is a practical and intuitive method, the “de-
scent direction” criterion is agnostic of the actual step
being taken, which depends on the learning rate α in ad-
dition to the direction −g. Moreover, the method intro-
duces an additional free parameter θ. In this work we
strive to alleviate these issues, while the resulting batch
size adaptation rule will stay close to (13) in form and
spirit.

A somewhat related line of research aims to reduce
the variance of stochastic gradients by incorporating
gradient information from previous iterations into the
current gradient estimate. Notable methods are SVRG
(Johnson & Zhang, 2013) and SAGA (Defazio et al.,
2014). Both are not mini-batch methods, since they
update after gradient evaluations on individual train-
ing examples (which are then modiﬁed using stored
two recent papers
gradient information). However,
(Harikandeh et al., 2015; Daneshmand et al., 2016) com-
bine these variance-reduced methods with increasing
sample sizes, i.e., the effective size of the training set
is increased over time. In both, a sample size schedule
has to be pre-speciﬁed and is not adapted at runtime.

We note that another recent line of work on non-uniform
sampling of training samples with the goal of variance
reduction (including, but not limited to, Needell et al.,
2014; Zhao & Zhang, 2015; Schmidt et al., 2015;
Csiba & Richt´arik, 2016) is orthogonal to our work,
since it is concerned with the composition of batches
rather than their size.

More generally, our work ﬁts into a recent effort to au-
tomate or simplify the tuning of parameters in stochastic
optimization algorithms, most notably the learning rate
(Schaul et al., 2013; Mahsereci & Hennig, 2015).

3 COUPLED ADAPTIVE BATCH SIZE

We will cast the problem of ﬁnding a “good” batch size
as maximizing a lower bound on the expected gain per
computational cost for an individual optimization step.
While the resulting rule is similar in form to (13), it pro-
vides a new interpretation and introduces an explicit in-
teraction with the learning rate. This criterion will subse-
quently be simpliﬁed, removing all unknown quantities
and free parameters from the equation.

3.1 MAXIMIZING A BOUND ON THE

EXPECTED GAIN

We deﬁne the gain of the SGD step from w to w+ =
w − αg as the drop in function value, F − F+, where
F+ = F (w+). In order to quantify this gain, we will
assume that F has Lipschitz-continuous gradients, i.e.,

L
2

Lα2
2

there is a constant L > 0 such that

k∇F (u) − ∇F (v)k ≤ Lku − vk ∀u, v ∈ Rd.

(14)

This is a standard assumption in the analysis of stochastic
optimization algorithms, setting a not overly restrictive
bound on how fast the gradient can change when mov-
ing in parameter space. As a consequence, the change
in F from v ∈ Rd to u ∈ Rd is bounded (see, e.g.,
Bottou et al. (2016), Eq. 4.3) by

F (u) ≤ F (v) + ∇F (v)T (u − v) +

ku − vk2.

(15)

Inserting v = w and u = w+ = w − αg and rearranging
yields a lower bound G on the gain:

F − F+ ≥ G := α∇F T g −

kgk2.

(16)

To derive the expectation of G, recall from Equation (9)
that E[g] = ∇F and

d

d

E

kgk2

=

E

g2
j

=

∇F 2

j +

(cid:2)

(cid:3)

j=1
X

(cid:2)

(cid:3)
= k∇F k2 +

j=1  
X
,

tr(Σ)
m

σ2
jj
m !

(17)

where we used that, for X ∼ N (µ, σ2), the second mo-
ment is E[X 2] = µ2 + σ2. Thus,

E[G] =

α −

k∇F k2 −

tr(Σ).

(18)

Lα2
2

(cid:19)

(cid:18)

Lα2
2m

The ﬁrst term in (18) is the gain in absence of noise, de-
termined by α and ∇F . It is reduced by a term that de-
pends on the gradient variance and drops with m. We
see from (18) that, for an expected descent, E[G] > 0,
we require

α <

2k∇F k2
L (k∇F k2 + tr(Σ)/m)

,

(19)

which exhibits a clear relationship between learning rate
and batch size. Small batch sizes require a small learning
rate, while larger batch sizes enable larger steps. We will
exploit this relationship later on by explicitly coupling
the two parameters. As a side note, for zero variance, we
recover the well-known condition α < 2/L that guaran-
tees convergence of gradient descent in the deterministic
case.
Obviously, the larger m, the larger E[G], so that the de-
terministic case is optimal if we ignore computational
cost. Since that cost scales linearly with m, the optimal
batch size is the one that maximizes expected gain per
cost,

max
m

E[G]
m

.

(20)

A recent workshop paper (Pirotta & Restelli, 2016) used
a similar idea, although on a different quantity (a statisti-
cal lower bound on the linearized improvement). In our
setting, maximal gain per cost is achieved by (derivation
in Appendix A)

m =

2Lα
2 − Lα

tr(Σ)
k∇F k2 .

(21)

3.2 THE CABS CRITERION

The result in (21) poses two practical problems. First,
the Lipschitz constant L is an unknown property of the
objective function. Even more importantly, it is difﬁcult
to reliably and robustly estimate the squared norm of the
true gradient k∇F k2 from a single batch. One might be
tempted to replace it with kgk2, recovering a criterion
similar to (13), but this is not an unbiased estimator for
the true gradient norm, as Equation (17) shows. Depend-
ing on the noise level and, intriguingly, the batch size m,
the second term in (17) can introduce a signiﬁcant bias.

In an effort to address these practical problems, we pro-
pose to replace Eq. (21) with the following simpler rule,
which we term the Coupled Adaptive Batch Size (CABS):

m = α

tr(Σ)
F

.

(22)

A formal justiﬁcation for this simpliﬁcation will be given
in §3.3, but ﬁrst we want to highlight some intuitive ben-
eﬁts of this batch size adaptation scheme.

A major advantage of the CABS rule, emphasized in its
name, is the direct coupling of learning rate and batch
size. We have established that a large learning rate
demands large batches while a smaller, more cautious
learning rate can be used with smaller batches (Equa-
tion 19). The CABS rule explicitly reﬂects this known
relationship. Using CABS can thus be seen as “tailoring”
the noise level to the learning rate the user has chosen.
We show experimentally, see §5, that this makes ﬁnding
a well-performing learning rate easier.

and

that,

theoretical
2012)

Apart
from
considerations
experimental
(Friedlander & Schmidt,
evidence show that it is beneﬁcial to have small batches
in the beginning and larger ones later in the optimiza-
tion process. Hence, one may want to think of the
denominators of (22), (21) and (13) as a measure of
“optimization progress”. The function value F used in
our CABS rule is, by deﬁnition, the measure for training
progress. The norm of the true gradient k∇F k2 conveys
similar information (even though it might be misleading
near non-optimal stationary points like saddle points or
plateaus), but can not simply be estimated by kgk2 as
previously noted. We have also investigated unbiased

estimators for k∇F k2 by correcting the bias in kgk2
using the variance estimate S, but these turned out to be
too unreliable in experiments. Additionally, Equation
(17) also shows that using kgk2 in the denominator leads
larger batches cause
to a disadvantageous feedback:
kgk2 to become smaller in expectation which, in turn,
leads to larger batches according to (13) (and the other
way round).

Readers who are rightly worried about the change of
“unit” or “type” when replacing the gradient norm in (21)
with the function value in (22) may ﬁnd it helpful to con-
sider the units of measure for the quantities in (22). Let
[w] and [F ] denote the units of the parameters and the
objective function, respectively. The gradient has unit
[F ]/[w] and its variance [F ]2/[w]2. It is a key insight that
a well-chosen learning rate has to be driven by quantities
with unit [w]2/[F ] (see MacKay (2003) §34.4 for more
discussion). If this was not the case, the gradient descent
update −αk∇F (wk) would not be covariant, i.e., inde-
pendent of the units of measure of w and F . It is also evi-
dent in Newton’s method: in the one-dimensional case, a
Newton update step is −F ′′(wk)−1F ′(wk), correspond-
ing to a “learning rate” that is given by the inverse second
derivative, having unit [F ]/[w]2. Putting it all together,
the right-hand side of (22) has unit

[w]2
[F ]

[F ]2/[w]2
[F ]

= [1].

(23)

Hence, the chosen batch size is invariant under rescaling
of the objective.

Lastly, CABS realizes a bound on the gradient variance
that is decreasing with the distance to optimality, similar
to that in Theorem 2.5 of Friedlander & Schmidt (2012),
which they have shown to guarantee convergence of SGD
with a constant, non-decreasing learning rate.

3.3 MATHEMATICAL MOTIVATION FOR

CABS

For a more systematic motivation for the CABS rule, we
will show that it is approximately equal to (21), and
hence optimal in the sense of (20), if we assume that F
locally has an approximately scalar Hessian, i.e.,

∇2F (w) ≈ hI,

h > 0.

(24)

First, note that under this assumption, the Lipschitz con-
stant L is exactly h and the optimal batch size according
to (21) becomes

m =

2hα
2 − hα

tr(Σ)
k∇F k2 .

(25)

Furthermore, the second-order Taylor expansion of F
around w now reads

F (u) ≈ F (w) + ∇F (w)T (u − w) +

ku − wk2. (26)

h
2

We minimize both sides with respect to u. The left-
hand side takes on the optimal value F∗.
For the
right-hand side, we set the gradient with respect to u,
∇F (w) + h(u − w), to zero, which yields the minimizer
u = −∇F (w)/h + w. Inserting this back into (26) and
rearranging yields

k∇F (w)k2 ≈ 2h(F (w) − F∗).

(27)

That is, we can replace the squared gradient norm with a
scaled distance to optimality. Doing so in (25) reads

m =

α
2 − hα

tr(Σ)
F − F∗

.

(28)

We eliminate h from this equation by realizing that, un-
der the scalar Hessian assumption, a good learning rate
is α = 1/h. It corresponds both to the Newton step, as
well as to the optimal constant learning rate 1/L for gra-
dient descent, given that (24) holds. Hence, if we assume
a well-chosen learning rate with hα ≈ 1, then Eq. (28)
further simpliﬁes to

m = α

tr(Σ)
F − F∗

.

(29)

Assuming a scalar Hessian is, of course, a substantial
simpliﬁcation. The result can partly be generalized to the
less restrictive assumption of µ-strong convexity, under
which we still have (see Appendix A)

k∇F k2 ≥ 2µ(F − F∗),

(30)

If the problem is well-conditioned in that µ and L are
not too far from each other, the above argument carries
through as an upper bound on the batch size.

To ﬁnally arrive at the CABS rule, we drop F∗. This is
based on the assumption of a non-negative loss, which
holds for all standard loss functions like least-squares or
cross-entropy. In this case, F ≥ F − F∗, i.e., the func-
tion value F is a non-trivial upper bound on the distance
to optimality. If the optimum is close to zero, F will be a
good proxy for F − F∗. If not, which is not uncommon,
the denominator of the CABS rule has a small positive
offset compared to (29), but this will not fundamentally
alter its implications, as long as we do not come too close
to the optimum, which is usually the case for even mod-
estly complex problems. The more general form (29) can
be used in lieu of (22) if one has access to a tighter lower
bound on F∗, e.g., due to prior experience from similar
problems. In fact, when the objective function includes
an additive regularization term, we suggest to use the un-
regularized loss as a proxy for F − F∗.

4 PRACTICAL IMPLEMENTATION

We outline a practical implementation of the CABS crite-
rion. Obviously, neither F nor tr(Σ) are known exactly
at each individual SGD step, but estimates of both quan-
tities can be obtained from a mini-batch. This is straight-
forward for the objective F . For the variance, we use the
estimate S explained in Equation (10). Since S only es-
timates the diagonal elements of the covariance matrix,
it is tr(Σ) ≈ kSk1. Considerations on how to practically
compute S can be found in §4.2.

4.1 MECHANICAL DETAILS

We realize the CABS criterion in a predictive manner,
meaning that we do not ﬁnd the exact batch size that sat-
isﬁes (22) in each single optimization step. To achieve
such an exact enforcement of their criterion, Byrd et al.
(2012) and De et al. (2017) increase the batch size by a
small increment whenever the criterion is not satisﬁed,
and only then perform the update. This incremental com-
putation introduces an overhead and, when the increment
is small, can lead to under-utilization of computing re-
sources. Instead, we leverage the observation that gradi-
ent variance and function value change only slowly from
one optimization step to the next, which allows us to use
our current estimates of F and tr(Σ) to set the batch size
used for the next optimization step. It also allows for a
smoothing of both quantities over multiple optimization
steps. The estimates can be fairly noisy, especially that
of tr(Σ) at small batch sizes. We use exponential mov-
ing averages (see Algorithm 1) to obtain more robust es-
timates.

The resulting batch size is rounded to the nearest integer
and clipped at minimal and maximal batch sizes. A min-
imal batch size avoids under-utilization of the compu-
tational resources with very small batches and provides
additional stability of the algorithm in the small-batch
regime. A maximal batch size is necessary due to hard-
ware limitations: In contemporary deep learning, GPU
memory limits the number of samples that can be pro-
cessed at once. Our implementation has such a limit but
it was never reached in our experiments. We note in pass-
ing that algorithmic batch size (the number of training
samples used to compute a gradient estimate before up-
dating the parameters) and computational batch size (the
number of training samples that are processed simulta-
neously) are in principle independent—a future imple-
mentation could split an algorithmic batch into feasible
computational batches when necessary, freeing the algo-
rithm from hardware-speciﬁc constraints. Algorithm 1
provides pseudo-code.

Algorithm 1 SGD with Coupled Adaptive Batch Size
Require: Learning rate α, initial parameters w0, num-
ber of steps K, batch size bounds (mmin, mmax), run-
ning average constant µ = 0.95

Draw a mini-batch B of size m
F, g, S ← EVALUATE(w, B)

w ← w − αg
ξ ← µξ + (1 − µ)kSk1
Favg ← µFavg + (1 − µ)F

1: w ← w0, m ← mmin, Favg ← 0, ξ ← 0
2: for k = 1, . . . , K do
3:
4:
5:
6:
7:
8:
9: end for
Note: EVALUATE(w, B) denotes an evaluation of func-
tion value F (w), stochastic gradient g(w) and vari-
ance estimate S(w) (Eq. 10) using mini-batch B.
ROUND & CLIP(m, mmin, mmax) rounds m to the nearest
integer and clips it at the provided minimal and maximal
values.

m ←ROUND & CLIP(αξ/Favg, mmin, mmax)

4.2 VARIANCE ESTIMATE

If the individual gradients ∇ℓi in the mini-batch are ac-
cessible, then S can be computed directly by Eq. (10),
adding only the computational cost of squaring and sum-
ming the gradients. Unfortunately, these individual gra-
dients are not available in practical implementations of
the backpropagation algorithm (Rumelhart et al., 1986)
used to compute gradients in the training of neural net-
works. A complete discussion of this technical issue is
beyond the scope of this paper, but we brieﬂy sketch a
solution.

Consider a fully-connected layer in a neural network
with weight matrix Wl+1 ∈ Rnl×nl+1. During the for-
ward pass, the matrix of activations Al ∈ Rm×nl (con-
taining the activations for each of the m input training
samples) is propagated forward by a matrix multiplica-
tion,

Zl+1 = AlWl+1 ∈ Rm×nl+1.

(31)

Once the backward pass arrives at this layer, the gradient
with respect to Wl+1 is computed as

dWl+1 = AT

l dZl+1.

(32)

The aggregation of individual gradients is implicit in this
matrix multiplication. Practical implementations rely on
the efﬁciency of these matrix operations and, even more
importantly, it is infeasible to store m individual gradi-
ents in memory if the number of parameters d is high.

However, one can similarly compute the second mo-
i (∇ℓi).2, that is needed in
ment of the gradients, 1
m
(10) without giving up efﬁcient batch processing. It is
P

straight-forward to verify that this second moment of
gradients with respect to W (l+1) can be computed as

AT
l

.2

(dZl+1).2 .

(33)

(cid:1)

(cid:0)

In this form, the computation of the gradient variance
adds non-negligible but manageable computational cost.
Since it duplicates half of the operations in the backward
pass, the additional cost can be pinned down to roughly
25%. This is primarily an implementation issue; the cost
could be reduced by implementing special matrix opera-
tions to compute (32) and (33) jointly.

5 EXPERIMENTS

We evaluate the proposed batch size adaptation method
by training convolutional neural networks (CNNs) on
four popular image classiﬁcation benchmark data sets:
MNIST (LeCun et al., 1998), Street View House Num-
bers (SVHN) (Netzer et al., 2011), as well as CIFAR-10
and CIFAR-100 (Krizhevsky, 2009). While these are
small to medium-scale problems by contemporary stan-
dards, they exhibit many of the typical difﬁculties of neu-
ral network training. We opted for these benchmarks to
keep the computational cost for a thorough evaluation of
the method manageable (this required approximately 60
training runs per benchmark, see the following section).

5.1 EXPERIMENT DESIGN

We compare against constant batch sizes 16, 32, 64, 128,
256 and 512. To keep the plots readable, we only re-
port results for batch sizes 32, 128 and 512 in the main
text; results for the other batch sizes can be found in
the supplements. We also compare against a batch size
adaptation based on the criterion (13) used in Byrd et al.
(2012) and De et al. (2017). Since implementation de-
tails differ between these two works, and both com-
bine batch size adaptation with other measures (Newton-
CG method in Byrd et al. (2012) and a backtracking line
search in De et al. (2017)), we resort to a custom imple-
mentation of said criterion. For a fair comparison, we
realize it in a similar manner as CABS. That is, we use
criterion (13), while keeping the predictive update mech-
anism for the batch size, the smoothing via exponential
moving averages, rounding and clipping exactly as in our
CABS implementation described in §4.1 and Algorithm 1.
This method will simply be referred to as Competitor in
the remainder of this section.

During the optimization process, we periodically evalu-
ate the training loss as well as the classiﬁcation accuracy
on a held-out test set. Since each method uses a different
batch size, both quantities are tracked as a function of
the number of accessed training examples, instead of the

Const. 32 (α = .006)
Const. 128 (α = .06)
Const. 512 (α = .1)
Comp. (θ = .8, α = .006)
CABS (α = .01)

Const. 32 (α = .01)
Const. 128 (α = .03)
Const. 512 (α = .1)
Comp. (θ = .6, α = .01)
CABS (α = .03)

101

100

0.95

0.9

0.85

0.8

0.75

s
s
o
l
n
i
a
r
T

y
c
a
r
u
c
c
a

t
s
e
T

m

60
40
20

0

101

100

−1

10
0.85

0.8

0.75

0.7

0.65

s
s
o
l

n
i
a
r
T

y
c
a
r
u
c
c
a
t
s
e
T

m

100
50
0

0

0.2

0.4
0.6
Examples accessed

0.8

1

·107

0.2

0.4
0.6
Examples accessed

0.8

1

·107

Figure 1: Results for SVHN. Shared horizontal axis indi-
cates the number of examples used for training. Top and
middle panel depict evolution of training loss and test ac-
curacy, respectively, color-coded for different batch size
methods, each with its optimal learning rate. Bottom
panel shows batch size chosen by CABS.

number of optimization steps. This measure is propor-
tional to wall-clock time up to per-batch overheads that
depend on the speciﬁc problem and implementation.

The (constant) learning rate for each batch size method
was tuned for maximum test accuracy given the ﬁxed
budget of accessed training examples. We tried six can-
didates α ∈ {0.3, 0.1, 0.06, 0.03, 0.01, 0.006}; this rele-
vant range has been determined with a few exploratory
experiments. In addition to the learning rate, the com-
petitor method has a free parameter θ. De et al. (2017)
suggest setting it to 1.0, the highest possible noise tol-
erance, by default.
In our experiments, we found the
performance of the method to be fairly sensitive to the
choice of θ. We thus tried θ ∈ {0.6, 0.8, 1.0} and report
results for the best-performing choice. For CABS, there
is no analogous parameter to tune.

MNIST We start with experiments on the well-known
MNIST image classiﬁcation task of identifying handwrit-
ten digits in 28×28 pixel gray scale images. Our network
has two convolutional layers with 5×5 ﬁlters (32 and 64
ﬁlters, respectively) and subsequent max-pooling over

Figure 2: Results for CIFAR-10. Set-up as in Fig. 1.

2×2 windows. This is followed by a fully-connected
layer with 1024 units. The activation function is ReLU
for all layers. The output layer has 10 units with softmax
activation and we use cross-entropy loss.

SVHN Next, we train a CNN on the digit classiﬁca-
tion task of the Street View House Numbers (SVHN) data
set. While the task is similar to MNIST, the images are in
RGB and larger (32×32). They exhibit real-world views
of digits in house numbers, partially with clutter, mis-
alignment and distracting digits at the sides. We train
a CNN with two convolutional layers, each with 64 ﬁl-
ters of size 5×5 and subsequent max-pooling over 3×3
windows with stride 2. They are followed by two fully-
connected layers with 256 and 128 units, respectively.
The activation function is ReLU for all layers. The out-
put layer has 10 units with softmax activation and we use
cross-entropy loss. We apply L2-regularization and per-
form data augmentation operations (random cropping of
24×24 pixel subimages, left-right mirroring, color dis-
tortion) on the training inputs.

CIFAR-10 and CIFAR-100 Finally, we train CNNs
on the CIFAR-10 and CIFAR-100 data sets, where the
task is to classify 32×32 pixel RGB images into one of
10 and 100 object categories, respectively. For CIFAR-
10, we crop the images to 24×24 pixels and train a CNN
with two convolutional layers, each with 64 ﬁlters of size

Const. 32 (α = .01)
Const. 128 (α = .03)
Const. 512 (α = .06)
Comp. (θ = .6, α = .006)
CABS (α = .01)

Const. 32 (α = .06)
Const. 128 (α = .1)
Const. 512 (α = .3)
Comp. (θ = 1.0, α = .1)
CABS (α = .1)

101

100

0.5

0.4

0.3

s
s
o
l
n
i
a
r
T

y
c
a
r
u
c
c
a

t
s
e
T

m

200
150
100
50
0

0

s
s
o
l

n
i
a
r
T

101

100

−1

10

−2

10

1

y
c
a
r
u
c
c
a
t
s
e
T

0.95

0.9

0.85

300
200
100
0

0

m

2

4
Examples accessed

6

8

·106

1

2
Examples accessed

3

4

·105

Figure 3: Results for CIFAR-100. Set-up as in Fig. 1.

Figure 4: Results for MNIST. Set-up as in Fig. 1.

5x5 and subsequent max-pooling over 3x3 windows with
stride 2. They are followed by two fully-connected lay-
ers with 384 and 192 units, respectively. The activation
function is ReLU for all layers. The output layer has 10
units with softmax activation and we use cross-entropy
loss. We perform data augmentation operations (ran-
dom cropping, left-right mirroring, color distortion) on
the training set.

For CIFAR-100, we use the full 32×32 image and add a
third convolutional layer (64 ﬁlters of size 5 × 5 followed
by max pooling). The fully-connected layers have 512
and 256 units, respectively, and the output layer has 100
units. We add L2-regularization.

5.2 RESULTS AND DISCUSSION

On SVHN, CIFAR-10 and CIFAR-100 (Figures 1, 2 and
3), CABS yields signiﬁcantly faster decrease in training
loss with the curve contiuously lying below all others. It
also achieves the best test set accuracy of all methods on
all three problems. While the margin over the second-
best method is very small on SVHN, it amounts to a no-
ticeable 0.4 percentage points on CIFAR-10 and even 1.4
points on CIFAR-100.

Surprisingly, on MNIST—the least complex of the bench-
mark problems we investigated—our method is outper-
formed by the small constant batch size of 32 and the

competitor method, which also chooses very small batch
sizes throughout. Our method does, however, surpass
non-adaptive larger batch sizes (128, 512) in terms of
speed and all contenders reach virtually the same test set
accuracy on this problem. CABS makes rapid progress
initially, but seems to choose unnecessarily large batch
sizes later on. The resulting high per-iteration cost ev-
idently can not be compensated by the higher learning
rate it enables (.1 for CABS compared to .01 with constant
batch size of 32). We conjecture that CABS overestimates
the gradient variance due to the homogeneous structure
of the MNIST data set; if the distribution of gradients is
very closely-centered, outliers in a few coordinate direc-
tions lead to comparably high variance estimates.

Overall, CABS outperforms alternative batch size
schemes on three out of the four benchmark problems
we investigated and the beneﬁts seem to increase with the
complexity of the problem (MNIST → SVHN → CIFAR-
10 → CIFAR-100). When considering the CABS batch
size schedules, depicted in the bottom panels of Figures
1 to 4, a common behavior (with the exception of MNIST)
seems to be that CABS uses the minimal batch size (16 in
our experiments) for a considerable portion of the train-
ing process and increases approximately linearly after-
wards.

Finally, we present our ﬁndings regarding the sensitiv-
ity to the choice of learning rate when using CABS. As

Const. 128
Comp. (θ = 0.8)
CABS

In our experiments, CABS was able to speed up SGD
training in neural networks and simplify the tuning of the
learning rate. In contrast to existing methods, it does not
introduce any additional free parameters. A TensorFlow1
implementation of SGD with CABS can be found on
http://github.com/ProbabilisticNumerics/cabs.

101

s
s
o
l
n
i
a
r
T

100

101

100

−1

10

−2

10

s
s
o
l

n
i
a
r
T

0

1

2
Examples accessed

3

·105

4

to zero and rearranging yields (21).

0

0.2

0.6
0.4
Examples accessed

0.8

1

·107

Figure 5: Learning rate sensitivity on SVHN. Families of
training loss curves for CABS, Competitor and a constant
batch size (color-coded). Each individual curve corre-
sponds to a learning rate α ∈ {.1, .06, .03, .01, .006}.

Const. 128
Comp. (θ = 1.0)
CABS

Figure 6: Learning rate sensitivity on MNIST. Set-up as
in Fig. 5.

detailed above, the coupling of learning rate and batch
size in CABS can be seen as tayloring the noise level to
the chosen learning rate. This suggests that the perfor-
mance of the optimizer should be less sensitive to the
choice of learning rate when adapting the batch size with
our method. Indeed, this became evident in our experi-
ments by considering the families of training loss curves
for various learning rates. Figures 5 and 6 compare a
constant batch size, the competitor method and CABS on
the SVHN and MNIST benchmark problems, respectively.
CABS signiﬁcantly reduces the dependency of the perfor-
mance on the learning rate compared to both the constant
batch size and the competing adaptive method. In prac-
tical applications, this ﬁnding could drastically simplify
the often tedious process of learning rate tuning.

6 CONCLUSION

We proposed CABS, a practical rule for dynamic batch
size adaptation based on estimates of the gradient vari-
ance and coupled to the chosen learning rate as well as
optimization progress represented by the function value.

A MATHEMATICAL DETAILS

Proof of Equation (11) By the Cauchy-Schwarz in-
equality, hg, ∇F i = hg, gi − hg, g − ∇F i ≥ kgk2 −
kgkkg − ∇F k = kgk(kgk − kg − ∇F k). This becomes
positive if kg − ∇F k < kgk.

Solving the Maximization Problem (20) We want to
maximize

U (m) =

E[G]
m

=

2α − Lα2
2m

k∇F k2 −

tr(Σ).

Lα2
2m2

(34)

Setting the derivative

U ′(m) = −

2α − Lα2
2m2

k∇F k2 +

tr(Σ)

(35)

Lα2
m3

Proof of Equation (30) The deﬁnition of strong con-
vexity is that, for all w, u ∈ Rd

F (u) ≥ F (w) + ∇F (w)T (u − w) +

ku − wk2 (36)

µ
2

for µ > 0. From there, the proof is identical to that of
Equation (27) in the main text. We minimize both sides
of the inequality. The left-hand side has minimal value
F∗. The gradient with respect to u of the right-hand side
is ∇F (w) + µ(u − w). By setting this to zero we ﬁnd
the minimizer u = −∇F (w)/µ + w. Inserting this back
(cid:3)
into (36) and rearranging yields (30).

References

Bottou, L´eon, Curtis, Frank E, and Nocedal, Jorge. Op-
timization methods for large-scale machine learning.
arXiv preprint arXiv:1606.04838, 2016.

Byrd, Richard H, Chin, Gillian M, Nocedal, Jorge, and
Wu, Yuchen. Sample size selection in optimization
methods for machine learning. Mathematical Pro-
gramming, 134(1):127–155, 2012.

Csiba, Dominik and Richt´arik, Peter.

tance sampling for minibatches.
arXiv:1602.02283, 2016.

Impor-
arXiv preprint

1http://tensorflow.org

the randomized Kaczmarz algorithm. In Advances in
Neural Information Processing Systems 27, pp. 1017–
1025. 2014.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,
Alessandro, Wu, Bo, and Ng, Andrew Y. Reading dig-
its in natural images with unsupervised feature learn-
ing. NIPS Workshop on Deep Learning and Unsuper-
vised Feature Learning, 2011.

Pirotta, Matteo and Restelli, Marcello. Cost-sensitive ap-
proach for batch size optimization. NIPS Workshop on
Optimizing the Optimizer, 2016.

Robbins, Herbert and Monro, Sutton. A stochastic ap-
proximation method. The Annals of Mathematical
Statistics, pp. 400–407, 1951.

Rumelhart, David E., Hinton, Geoffrey E., and Williams,
Learning representations by back-

Ronald J.
propagating errors. Nature, 323:533–536, 1986.

Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more
In Proceedings of the 30th In-
pesky learning rates.
ternational Conference on Machine Learning (ICML),
pp. 343–351, 2013.

Schmidt, Mark, Babanezhad, Reza, Ahmed, Mo-
hamed Osama, Defazio, Aaron, Clifton, Ann, and
Sarkar, Anoop. Non-uniform stochastic average gradi-
ent method for training conditional random ﬁelds. In
Proceedings of the 18th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

Zeiler, Matthew D. ADADELTA: An adaptive learning
rate method. arXiv preprint arXiv:1212.5701, 2012.

Zhao, Peilin and Zhang, Tong. Stochastic optimization
with importance sampling for regularized loss mini-
In Proceedings of The 32nd International
mization.
Conference on Machine Learning (ICML), 2015.

Daneshmand, Hadi, Lucchi, Aurelien, and Hofmann,
Thomas. Starting small—learning with adaptive sam-
In Proceedings of the 33nd International
ple sizes.
Conference on Machine Learning (ICML), pp. 1463–
1471, 2016.

De, Soham, Yadav, Abhay, Jacobs, David, and Goldstein,
Tom. Automated inference with adaptive batches. In
Proceedings of the 20th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), 2017.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Si-
mon. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives.
In Advances in Neural Information Processing Sys-
tems 27, pp. 1646–1654. 2014.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12(Jul):2121–2159, 2011.

Friedlander, Michael P and Schmidt, Mark.

Hy-
brid deterministic-stochastic methods for data ﬁtting.
SIAM Journal on Scientiﬁc Computing, 34(3):A1380–
A1405, 2012.

Harikandeh, Reza, Ahmed, Mohamed Osama, Virani,
Alim, Schmidt, Mark, Koneˇcn´y, Jakub, and Sallinen,
Scott. StopWasting my gradients: Practical SVRG. In
Advances in Neural Information Processing Systems
28, pp. 2251–2259. 2015.

Johnson, Rie and Zhang, Tong. Accelerating stochas-
tic gradient descent using predictive variance reduc-
tion. In Advances in Neural Information Processing
Systems 26, pp. 315–323. 2013.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. The International Conference
on Learning Representations (ICLR), 2015.

Krizhevsky, Alex. Learning multiple layers of features
from tiny images. Technical report, University of
Toronto, 2009.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and
Haffner, Patrick. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86
(11):2278–2324, 1998.

MacKay, David JC. Information Theory, Inference and
Learning Algorithms. Cambridge University Press,
2003.

Mahsereci, Maren and Hennig, Philipp. Probabilistic line
searches for stochastic optimization. In Advances in
Neural Information Processing Systems 28, pp. 181–
189. 2015.

Needell, Deanna, Ward, Rachel, and Srebro, Nati.
Stochastic gradient descent, weighted sampling, and

Supplementary Material

Figures 7 to 10 show results for constant batch sizes 16,
64 and 256, which have been omitted from the main text
for lack of space.

101

s
s
o
l
n
i
a
r
T

100

0.95

0.9

0.85

0.8

0.75

m

60
40
20

y
c
a
r
u
c
c
a

t
s
e
T

101

100

s
s
o
l

n
i
a
r
T

−1

10
0.85

0.8

0.75

0.7

0.65

100
50
0

y
c
a
r
u
c
c
a
t
s
e
T

m

Const. 16 (α = .006)
Const. 64 (α = .01)
Const. 256 (α = .1)
CABS (α = .01)

Const. 16 (α = .006)
Const. 64 (α = .01)
Const. 256 (α = .06)
CABS (α = .01)

101

100

0.5

0.4

0.3

s
s
o
l
n
i
a
r
T

y
c
a
r
u
c
c
a

t
s
e
T

m

200
150
100
50
0

0

s
s
o
l

n
i
a
r
T

101

100

−1

10

−2

10

1

y
c
a
r
u
c
c
a
t
s
e
T

0.95

0.9

0.85

300
200
100
0

m

0

0

0.2

0.4
0.6
Examples accessed

0.8

1

·107

2

4
Examples accessed

6

8

·106

Figure 7: Additional results for SVHN.

Figure 9: Additional results for CIFAR-100.

Const. 16 (α = .006)
Const. 64 (α = .01)
Const. 256 (α = .06)
CABS (α = .03)

Const. 16 (α = .06)
Const. 64 (α = .1)
Const. 256 (α = .3)
CABS (α = .1)

0

0.2

0.4
0.6
Examples accessed

0.8

1

·107

1

2
Examples accessed

3

4

·105

Figure 8: Additional results for CIFAR-10.

Figure 10: Additional results for MNIST.

Coupling Adaptive Batch Sizes with Learning Rates

7
1
0
2
 
n
u
J
 
8
2
 
 
]

G
L
.
s
c
[
 
 
2
v
6
8
0
5
0
.
2
1
6
1
:
v
i
X
r
a

Lukas Balles
MPI for Intelligent Systems
T¨ubingen, Germany

Javier Romero∗
Body Labs Inc.
New York, NY, USA

Philipp Hennig
MPI for Intelligent Systems
T¨ubingen, Germany

Abstract

Mini-batch stochastic gradient descent and
variants thereof have become standard for
large-scale empirical risk minimization like
the training of neural networks. These meth-
ods are usually used with a constant batch size
chosen by simple empirical inspection. The
batch size signiﬁcantly inﬂuences the behav-
ior of the stochastic optimization algorithm,
though, since it determines the variance of the
gradient estimates. This variance also changes
over the optimization process; when using a
constant batch size, stability and convergence
is thus often enforced by means of a (manually
tuned) decreasing learning rate schedule.

We propose a practical method for dynamic
batch size adaptation.
It estimates the vari-
ance of the stochastic gradients and adapts the
batch size to decrease the variance proportion-
ally to the value of the objective function, re-
moving the need for the aforementioned learn-
ing rate decrease. In contrast to recent related
work, our algorithm couples the batch size to
the learning rate, directly reﬂecting the known
relationship between the two. On popular im-
age classiﬁcation benchmarks, our batch size
adaptation yields faster optimization conver-
gence, while simultaneously simplifying learn-
ing rate tuning. A TensorFlow implementation
is available.

1 INTRODUCTION

In parametric machine learning models, like logistic re-
gression or neural networks, the performance of a pa-
rameter vector w ∈ Rd on datum x is quantiﬁed by a
∗ Work done while at MPI for Intelligent Systems.

loss function ℓ(w; x). Assuming the data comes from a
distribution x ∼ p, the goal is to minimize the expected
loss, or risk,

R(w) = Ex∼p[ℓ(w; x)].

(1)

We consider empirical risk minimization tasks of the
form

min
w∈Rd

F (w) =

ℓ(w; xi),

(2)

1
M

M

i=1
X

where the risk is approximated using a training set
{x1, . . . , xM } of data sampled (approximately) from p.
Typical optimization algorithms used to minimize (2) re-
peatedly evaluate the gradient

∇F (w) =

∇ℓ(w; xi).

(3)

1
M

M

i=1
X

For large-scale problems where M and/or d are large, it
is inefﬁcient or impossible to evaluate the exact gradient
(3), and one typically resorts to stochastic gradients by
randomly drawing a mini-batch B ⊂ {1, . . . , M }, |B| =
m ≪ M , at each step of the optimization algorithm and
using the gradient approximation

1
m

g(w) =

∇ℓ(w; xi).

(4)

i∈B
X
The simplest, but still widely used, stochastic opti-
mization algorithm is stochastic gradient descent (SGD,
Robbins & Monro, 1951), which updates

wk+1 = wk − αkg(wk),

(5)

where αk ∈ R+ is the step size parameter, often called
learning rate in the machine learning context. Variants of
SGD include ADAGRAD (Duchi et al., 2011), ADADELTA
(Zeiler, 2012), and ADAM (Kingma & Ba, 2015). We re-
strict our considerations to SGD in this paper.

(7)

(8)

(9)

1.1 THE EFFECT OF THE BATCH SIZE

If i is drawn uniformly at random from {1, . . . , M },
∇ℓi(w) = ∇ℓ(w; xi) is a random variable with mean

E[∇ℓi(w)] =

∇ℓi(w) = ∇F (w),

(6)

1
M

M

i=1
X

and covariance matrix
Σ(w) := cov [∇ℓi(w)] =

1
M

M

i=1
X

set (7) is prohibitively costly to compute, but it can be
estimated by the sample variance computed on a mini-
batch. As will be described below, we only require the
diagonal elements of Σ(w), corresponding to the vari-
ances of the individual components. These can be esti-
mated by

S(w) =

∇ℓi(w).2 − g(w).2 ∈ Rd,

(10)

1
m

i∈B
X

where .2 signiﬁes an element-wise square.

(∇ℓi(w) − ∇F (w))(∇ℓi(w) − ∇F (w))T .

1.2 NOTATION

Likewise, a stochastic gradient g(w) computed on a
randomly-drawn mini-batch B is a random variable with
mean ∇F (w). Assuming that it is composed of m sam-
ples drawn independently with replacement, its covari-
ance matrix is

The following discussion addresses the choice of batch
size for a single SGD step, assuming that we are currently
at some arbitrary but ﬁxed point w in parameter space.
For notational convenience, we will thus drop w from
the notation and write F = F (w), ∇F = ∇F (w), et
cetera.

cov[g(w)] =

Σ(w)
m

and, by the Central Limit Theorem, g(w) is approxi-
mately normally distributed:

g(w) ∼ N

∇F (w),

(cid:18)

Σ(w)
m

.

(cid:19)

When sampling without replacement, as is usually done
in practice, the same holds approximately as long as
m ≪ M .

In practice, the batch size m is often set to a ﬁxed value,
which is chosen ad hoc or by simple empirical tests. But
it is actually a crucial variable, which poses an intricate
trade-off that affects the optimizer’s performance. On
the one hand, the variance of the stochastic gradients de-
creases linearly with m, so small batches give vague gra-
dient information, thus slow convergence in the number
of optimization steps. On the other hand, the cost per
step increases linearly with m. (This assumes that batch
sizes are large enough to fully utilize the available paral-
lel computing resources, which can easily be guaranteed
by enforcing an appropriate minimal batch size.) While
we can thus linearly trade off variance and cost, the gra-
dient variance does not linearly affect the performance of
the optimizer; its effect depends on the local structure of
the objective and interacts with other parameter choices
of the optimizer, notably the learning rate. In general,
there should thus be an optimal batch size that balances
these two aspects. Choosing such good batch sizes is an
important aspect in the design of a numerical optimizer.

Below, we propose an algorithm that adapts the batch
size based on the gradient variance observed by the opti-
mizer at runtime. The exact variance over the entire data

2 RELATED WORK

derive

(2012)

adaption of batch sizes has

al-
The dynamic
recent works.
ready attracted attention in other
decreasing
Friedlander & Schmidt
series of bounds on the gradient variance that provably
yield fast convergence rates with a constant learning
rate, showing that an increasing batch size can replace
a decreasing learning rate. To realize these bounds in
practice, they propose to increase the batch size by a
pre-speciﬁed constant factor in each iteration, without
adaptation to (an estimate of) the gradient variance.

The prior works closest to ours in spirit are by Byrd et al.
(2012) and De et al. (2017), who propose to adapt the
batch size based on variance estimates. Their criterion is
based on the observation that −g is a descent direction if

kg − ∇F k ≤ θkgk, with

0 ≤ θ < 1

(11)

(proof in Appendix A). While the left-hand side of (11) is
of course unknown, one can compute its expected square

E

kg − ∇F k2

=

E

(gj − ∇Fj)2

(cid:2)

d

j=1
X
d

(cid:3)

=

(cid:3)

(12)

(cid:2)
σ2
jj
m

=

tr(Σ)
m

.

j=1
X
Consequently, (11) holds in expectation if tr(Σ)/m ≤
θ2kgk2 or (with equality)

m =

1
θ2

tr(Σ)
kgk2 .

(13)

While this is a practical and intuitive method, the “de-
scent direction” criterion is agnostic of the actual step
being taken, which depends on the learning rate α in ad-
dition to the direction −g. Moreover, the method intro-
duces an additional free parameter θ. In this work we
strive to alleviate these issues, while the resulting batch
size adaptation rule will stay close to (13) in form and
spirit.

A somewhat related line of research aims to reduce
the variance of stochastic gradients by incorporating
gradient information from previous iterations into the
current gradient estimate. Notable methods are SVRG
(Johnson & Zhang, 2013) and SAGA (Defazio et al.,
2014). Both are not mini-batch methods, since they
update after gradient evaluations on individual train-
ing examples (which are then modiﬁed using stored
two recent papers
gradient information). However,
(Harikandeh et al., 2015; Daneshmand et al., 2016) com-
bine these variance-reduced methods with increasing
sample sizes, i.e., the effective size of the training set
is increased over time. In both, a sample size schedule
has to be pre-speciﬁed and is not adapted at runtime.

We note that another recent line of work on non-uniform
sampling of training samples with the goal of variance
reduction (including, but not limited to, Needell et al.,
2014; Zhao & Zhang, 2015; Schmidt et al., 2015;
Csiba & Richt´arik, 2016) is orthogonal to our work,
since it is concerned with the composition of batches
rather than their size.

More generally, our work ﬁts into a recent effort to au-
tomate or simplify the tuning of parameters in stochastic
optimization algorithms, most notably the learning rate
(Schaul et al., 2013; Mahsereci & Hennig, 2015).

3 COUPLED ADAPTIVE BATCH SIZE

We will cast the problem of ﬁnding a “good” batch size
as maximizing a lower bound on the expected gain per
computational cost for an individual optimization step.
While the resulting rule is similar in form to (13), it pro-
vides a new interpretation and introduces an explicit in-
teraction with the learning rate. This criterion will subse-
quently be simpliﬁed, removing all unknown quantities
and free parameters from the equation.

3.1 MAXIMIZING A BOUND ON THE

EXPECTED GAIN

We deﬁne the gain of the SGD step from w to w+ =
w − αg as the drop in function value, F − F+, where
F+ = F (w+). In order to quantify this gain, we will
assume that F has Lipschitz-continuous gradients, i.e.,

L
2

Lα2
2

there is a constant L > 0 such that

k∇F (u) − ∇F (v)k ≤ Lku − vk ∀u, v ∈ Rd.

(14)

This is a standard assumption in the analysis of stochastic
optimization algorithms, setting a not overly restrictive
bound on how fast the gradient can change when mov-
ing in parameter space. As a consequence, the change
in F from v ∈ Rd to u ∈ Rd is bounded (see, e.g.,
Bottou et al. (2016), Eq. 4.3) by

F (u) ≤ F (v) + ∇F (v)T (u − v) +

ku − vk2.

(15)

Inserting v = w and u = w+ = w − αg and rearranging
yields a lower bound G on the gain:

F − F+ ≥ G := α∇F T g −

kgk2.

(16)

To derive the expectation of G, recall from Equation (9)
that E[g] = ∇F and

d

d

E

kgk2

=

E

g2
j

=

∇F 2

j +

(cid:2)

(cid:3)

j=1
X

(cid:2)

(cid:3)
= k∇F k2 +

j=1  
X
,

tr(Σ)
m

σ2
jj
m !

(17)

where we used that, for X ∼ N (µ, σ2), the second mo-
ment is E[X 2] = µ2 + σ2. Thus,

E[G] =

α −

k∇F k2 −

tr(Σ).

(18)

Lα2
2

(cid:19)

(cid:18)

Lα2
2m

The ﬁrst term in (18) is the gain in absence of noise, de-
termined by α and ∇F . It is reduced by a term that de-
pends on the gradient variance and drops with m. We
see from (18) that, for an expected descent, E[G] > 0,
we require

α <

2k∇F k2
L (k∇F k2 + tr(Σ)/m)

,

(19)

which exhibits a clear relationship between learning rate
and batch size. Small batch sizes require a small learning
rate, while larger batch sizes enable larger steps. We will
exploit this relationship later on by explicitly coupling
the two parameters. As a side note, for zero variance, we
recover the well-known condition α < 2/L that guaran-
tees convergence of gradient descent in the deterministic
case.
Obviously, the larger m, the larger E[G], so that the de-
terministic case is optimal if we ignore computational
cost. Since that cost scales linearly with m, the optimal
batch size is the one that maximizes expected gain per
cost,

max
m

E[G]
m

.

(20)

A recent workshop paper (Pirotta & Restelli, 2016) used
a similar idea, although on a different quantity (a statisti-
cal lower bound on the linearized improvement). In our
setting, maximal gain per cost is achieved by (derivation
in Appendix A)

m =

2Lα
2 − Lα

tr(Σ)
k∇F k2 .

(21)

3.2 THE CABS CRITERION

The result in (21) poses two practical problems. First,
the Lipschitz constant L is an unknown property of the
objective function. Even more importantly, it is difﬁcult
to reliably and robustly estimate the squared norm of the
true gradient k∇F k2 from a single batch. One might be
tempted to replace it with kgk2, recovering a criterion
similar to (13), but this is not an unbiased estimator for
the true gradient norm, as Equation (17) shows. Depend-
ing on the noise level and, intriguingly, the batch size m,
the second term in (17) can introduce a signiﬁcant bias.

In an effort to address these practical problems, we pro-
pose to replace Eq. (21) with the following simpler rule,
which we term the Coupled Adaptive Batch Size (CABS):

m = α

tr(Σ)
F

.

(22)

A formal justiﬁcation for this simpliﬁcation will be given
in §3.3, but ﬁrst we want to highlight some intuitive ben-
eﬁts of this batch size adaptation scheme.

A major advantage of the CABS rule, emphasized in its
name, is the direct coupling of learning rate and batch
size. We have established that a large learning rate
demands large batches while a smaller, more cautious
learning rate can be used with smaller batches (Equa-
tion 19). The CABS rule explicitly reﬂects this known
relationship. Using CABS can thus be seen as “tailoring”
the noise level to the learning rate the user has chosen.
We show experimentally, see §5, that this makes ﬁnding
a well-performing learning rate easier.

and

that,

theoretical
2012)

Apart
from
considerations
experimental
(Friedlander & Schmidt,
evidence show that it is beneﬁcial to have small batches
in the beginning and larger ones later in the optimiza-
tion process. Hence, one may want to think of the
denominators of (22), (21) and (13) as a measure of
“optimization progress”. The function value F used in
our CABS rule is, by deﬁnition, the measure for training
progress. The norm of the true gradient k∇F k2 conveys
similar information (even though it might be misleading
near non-optimal stationary points like saddle points or
plateaus), but can not simply be estimated by kgk2 as
previously noted. We have also investigated unbiased

estimators for k∇F k2 by correcting the bias in kgk2
using the variance estimate S, but these turned out to be
too unreliable in experiments. Additionally, Equation
(17) also shows that using kgk2 in the denominator leads
larger batches cause
to a disadvantageous feedback:
kgk2 to become smaller in expectation which, in turn,
leads to larger batches according to (13) (and the other
way round).

Readers who are rightly worried about the change of
“unit” or “type” when replacing the gradient norm in (21)
with the function value in (22) may ﬁnd it helpful to con-
sider the units of measure for the quantities in (22). Let
[w] and [F ] denote the units of the parameters and the
objective function, respectively. The gradient has unit
[F ]/[w] and its variance [F ]2/[w]2. It is a key insight that
a well-chosen learning rate has to be driven by quantities
with unit [w]2/[F ] (see MacKay (2003) §34.4 for more
discussion). If this was not the case, the gradient descent
update −αk∇F (wk) would not be covariant, i.e., inde-
pendent of the units of measure of w and F . It is also evi-
dent in Newton’s method: in the one-dimensional case, a
Newton update step is −F ′′(wk)−1F ′(wk), correspond-
ing to a “learning rate” that is given by the inverse second
derivative, having unit [F ]/[w]2. Putting it all together,
the right-hand side of (22) has unit

[w]2
[F ]

[F ]2/[w]2
[F ]

= [1].

(23)

Hence, the chosen batch size is invariant under rescaling
of the objective.

Lastly, CABS realizes a bound on the gradient variance
that is decreasing with the distance to optimality, similar
to that in Theorem 2.5 of Friedlander & Schmidt (2012),
which they have shown to guarantee convergence of SGD
with a constant, non-decreasing learning rate.

3.3 MATHEMATICAL MOTIVATION FOR

CABS

For a more systematic motivation for the CABS rule, we
will show that it is approximately equal to (21), and
hence optimal in the sense of (20), if we assume that F
locally has an approximately scalar Hessian, i.e.,

∇2F (w) ≈ hI,

h > 0.

(24)

First, note that under this assumption, the Lipschitz con-
stant L is exactly h and the optimal batch size according
to (21) becomes

m =

2hα
2 − hα

tr(Σ)
k∇F k2 .

(25)

Furthermore, the second-order Taylor expansion of F
around w now reads

F (u) ≈ F (w) + ∇F (w)T (u − w) +

ku − wk2. (26)

h
2

We minimize both sides with respect to u. The left-
hand side takes on the optimal value F∗.
For the
right-hand side, we set the gradient with respect to u,
∇F (w) + h(u − w), to zero, which yields the minimizer
u = −∇F (w)/h + w. Inserting this back into (26) and
rearranging yields

k∇F (w)k2 ≈ 2h(F (w) − F∗).

(27)

That is, we can replace the squared gradient norm with a
scaled distance to optimality. Doing so in (25) reads

m =

α
2 − hα

tr(Σ)
F − F∗

.

(28)

We eliminate h from this equation by realizing that, un-
der the scalar Hessian assumption, a good learning rate
is α = 1/h. It corresponds both to the Newton step, as
well as to the optimal constant learning rate 1/L for gra-
dient descent, given that (24) holds. Hence, if we assume
a well-chosen learning rate with hα ≈ 1, then Eq. (28)
further simpliﬁes to

m = α

tr(Σ)
F − F∗

.

(29)

Assuming a scalar Hessian is, of course, a substantial
simpliﬁcation. The result can partly be generalized to the
less restrictive assumption of µ-strong convexity, under
which we still have (see Appendix A)

k∇F k2 ≥ 2µ(F − F∗),

(30)

If the problem is well-conditioned in that µ and L are
not too far from each other, the above argument carries
through as an upper bound on the batch size.

To ﬁnally arrive at the CABS rule, we drop F∗. This is
based on the assumption of a non-negative loss, which
holds for all standard loss functions like least-squares or
cross-entropy. In this case, F ≥ F − F∗, i.e., the func-
tion value F is a non-trivial upper bound on the distance
to optimality. If the optimum is close to zero, F will be a
good proxy for F − F∗. If not, which is not uncommon,
the denominator of the CABS rule has a small positive
offset compared to (29), but this will not fundamentally
alter its implications, as long as we do not come too close
to the optimum, which is usually the case for even mod-
estly complex problems. The more general form (29) can
be used in lieu of (22) if one has access to a tighter lower
bound on F∗, e.g., due to prior experience from similar
problems. In fact, when the objective function includes
an additive regularization term, we suggest to use the un-
regularized loss as a proxy for F − F∗.

4 PRACTICAL IMPLEMENTATION

We outline a practical implementation of the CABS crite-
rion. Obviously, neither F nor tr(Σ) are known exactly
at each individual SGD step, but estimates of both quan-
tities can be obtained from a mini-batch. This is straight-
forward for the objective F . For the variance, we use the
estimate S explained in Equation (10). Since S only es-
timates the diagonal elements of the covariance matrix,
it is tr(Σ) ≈ kSk1. Considerations on how to practically
compute S can be found in §4.2.

4.1 MECHANICAL DETAILS

We realize the CABS criterion in a predictive manner,
meaning that we do not ﬁnd the exact batch size that sat-
isﬁes (22) in each single optimization step. To achieve
such an exact enforcement of their criterion, Byrd et al.
(2012) and De et al. (2017) increase the batch size by a
small increment whenever the criterion is not satisﬁed,
and only then perform the update. This incremental com-
putation introduces an overhead and, when the increment
is small, can lead to under-utilization of computing re-
sources. Instead, we leverage the observation that gradi-
ent variance and function value change only slowly from
one optimization step to the next, which allows us to use
our current estimates of F and tr(Σ) to set the batch size
used for the next optimization step. It also allows for a
smoothing of both quantities over multiple optimization
steps. The estimates can be fairly noisy, especially that
of tr(Σ) at small batch sizes. We use exponential mov-
ing averages (see Algorithm 1) to obtain more robust es-
timates.

The resulting batch size is rounded to the nearest integer
and clipped at minimal and maximal batch sizes. A min-
imal batch size avoids under-utilization of the compu-
tational resources with very small batches and provides
additional stability of the algorithm in the small-batch
regime. A maximal batch size is necessary due to hard-
ware limitations: In contemporary deep learning, GPU
memory limits the number of samples that can be pro-
cessed at once. Our implementation has such a limit but
it was never reached in our experiments. We note in pass-
ing that algorithmic batch size (the number of training
samples used to compute a gradient estimate before up-
dating the parameters) and computational batch size (the
number of training samples that are processed simulta-
neously) are in principle independent—a future imple-
mentation could split an algorithmic batch into feasible
computational batches when necessary, freeing the algo-
rithm from hardware-speciﬁc constraints. Algorithm 1
provides pseudo-code.

Algorithm 1 SGD with Coupled Adaptive Batch Size
Require: Learning rate α, initial parameters w0, num-
ber of steps K, batch size bounds (mmin, mmax), run-
ning average constant µ = 0.95

Draw a mini-batch B of size m
F, g, S ← EVALUATE(w, B)

w ← w − αg
ξ ← µξ + (1 − µ)kSk1
Favg ← µFavg + (1 − µ)F

1: w ← w0, m ← mmin, Favg ← 0, ξ ← 0
2: for k = 1, . . . , K do
3:
4:
5:
6:
7:
8:
9: end for
Note: EVALUATE(w, B) denotes an evaluation of func-
tion value F (w), stochastic gradient g(w) and vari-
ance estimate S(w) (Eq. 10) using mini-batch B.
ROUND & CLIP(m, mmin, mmax) rounds m to the nearest
integer and clips it at the provided minimal and maximal
values.

m ←ROUND & CLIP(αξ/Favg, mmin, mmax)

4.2 VARIANCE ESTIMATE

If the individual gradients ∇ℓi in the mini-batch are ac-
cessible, then S can be computed directly by Eq. (10),
adding only the computational cost of squaring and sum-
ming the gradients. Unfortunately, these individual gra-
dients are not available in practical implementations of
the backpropagation algorithm (Rumelhart et al., 1986)
used to compute gradients in the training of neural net-
works. A complete discussion of this technical issue is
beyond the scope of this paper, but we brieﬂy sketch a
solution.

Consider a fully-connected layer in a neural network
with weight matrix Wl+1 ∈ Rnl×nl+1. During the for-
ward pass, the matrix of activations Al ∈ Rm×nl (con-
taining the activations for each of the m input training
samples) is propagated forward by a matrix multiplica-
tion,

Zl+1 = AlWl+1 ∈ Rm×nl+1.

(31)

Once the backward pass arrives at this layer, the gradient
with respect to Wl+1 is computed as

dWl+1 = AT

l dZl+1.

(32)

The aggregation of individual gradients is implicit in this
matrix multiplication. Practical implementations rely on
the efﬁciency of these matrix operations and, even more
importantly, it is infeasible to store m individual gradi-
ents in memory if the number of parameters d is high.

However, one can similarly compute the second mo-
i (∇ℓi).2, that is needed in
ment of the gradients, 1
m
(10) without giving up efﬁcient batch processing. It is
P

straight-forward to verify that this second moment of
gradients with respect to W (l+1) can be computed as

AT
l

.2

(dZl+1).2 .

(33)

(cid:1)

(cid:0)

In this form, the computation of the gradient variance
adds non-negligible but manageable computational cost.
Since it duplicates half of the operations in the backward
pass, the additional cost can be pinned down to roughly
25%. This is primarily an implementation issue; the cost
could be reduced by implementing special matrix opera-
tions to compute (32) and (33) jointly.

5 EXPERIMENTS

We evaluate the proposed batch size adaptation method
by training convolutional neural networks (CNNs) on
four popular image classiﬁcation benchmark data sets:
MNIST (LeCun et al., 1998), Street View House Num-
bers (SVHN) (Netzer et al., 2011), as well as CIFAR-10
and CIFAR-100 (Krizhevsky, 2009). While these are
small to medium-scale problems by contemporary stan-
dards, they exhibit many of the typical difﬁculties of neu-
ral network training. We opted for these benchmarks to
keep the computational cost for a thorough evaluation of
the method manageable (this required approximately 60
training runs per benchmark, see the following section).

5.1 EXPERIMENT DESIGN

We compare against constant batch sizes 16, 32, 64, 128,
256 and 512. To keep the plots readable, we only re-
port results for batch sizes 32, 128 and 512 in the main
text; results for the other batch sizes can be found in
the supplements. We also compare against a batch size
adaptation based on the criterion (13) used in Byrd et al.
(2012) and De et al. (2017). Since implementation de-
tails differ between these two works, and both com-
bine batch size adaptation with other measures (Newton-
CG method in Byrd et al. (2012) and a backtracking line
search in De et al. (2017)), we resort to a custom imple-
mentation of said criterion. For a fair comparison, we
realize it in a similar manner as CABS. That is, we use
criterion (13), while keeping the predictive update mech-
anism for the batch size, the smoothing via exponential
moving averages, rounding and clipping exactly as in our
CABS implementation described in §4.1 and Algorithm 1.
This method will simply be referred to as Competitor in
the remainder of this section.

During the optimization process, we periodically evalu-
ate the training loss as well as the classiﬁcation accuracy
on a held-out test set. Since each method uses a different
batch size, both quantities are tracked as a function of
the number of accessed training examples, instead of the

Const. 32 (α = .006)
Const. 128 (α = .06)
Const. 512 (α = .1)
Comp. (θ = .8, α = .006)
CABS (α = .01)

Const. 32 (α = .01)
Const. 128 (α = .03)
Const. 512 (α = .1)
Comp. (θ = .6, α = .01)
CABS (α = .03)

101

100

0.95

0.9

0.85

0.8

0.75

s
s
o
l
n
i
a
r
T

y
c
a
r
u
c
c
a

t
s
e
T

m

60
40
20

0

101

100

−1

10
0.85

0.8

0.75

0.7

0.65

s
s
o
l

n
i
a
r
T

y
c
a
r
u
c
c
a
t
s
e
T

m

100
50
0

0

0.2

0.4
0.6
Examples accessed

0.8

1

·107

0.2

0.4
0.6
Examples accessed

0.8

1

·107

Figure 1: Results for SVHN. Shared horizontal axis indi-
cates the number of examples used for training. Top and
middle panel depict evolution of training loss and test ac-
curacy, respectively, color-coded for different batch size
methods, each with its optimal learning rate. Bottom
panel shows batch size chosen by CABS.

number of optimization steps. This measure is propor-
tional to wall-clock time up to per-batch overheads that
depend on the speciﬁc problem and implementation.

The (constant) learning rate for each batch size method
was tuned for maximum test accuracy given the ﬁxed
budget of accessed training examples. We tried six can-
didates α ∈ {0.3, 0.1, 0.06, 0.03, 0.01, 0.006}; this rele-
vant range has been determined with a few exploratory
experiments. In addition to the learning rate, the com-
petitor method has a free parameter θ. De et al. (2017)
suggest setting it to 1.0, the highest possible noise tol-
erance, by default.
In our experiments, we found the
performance of the method to be fairly sensitive to the
choice of θ. We thus tried θ ∈ {0.6, 0.8, 1.0} and report
results for the best-performing choice. For CABS, there
is no analogous parameter to tune.

MNIST We start with experiments on the well-known
MNIST image classiﬁcation task of identifying handwrit-
ten digits in 28×28 pixel gray scale images. Our network
has two convolutional layers with 5×5 ﬁlters (32 and 64
ﬁlters, respectively) and subsequent max-pooling over

Figure 2: Results for CIFAR-10. Set-up as in Fig. 1.

2×2 windows. This is followed by a fully-connected
layer with 1024 units. The activation function is ReLU
for all layers. The output layer has 10 units with softmax
activation and we use cross-entropy loss.

SVHN Next, we train a CNN on the digit classiﬁca-
tion task of the Street View House Numbers (SVHN) data
set. While the task is similar to MNIST, the images are in
RGB and larger (32×32). They exhibit real-world views
of digits in house numbers, partially with clutter, mis-
alignment and distracting digits at the sides. We train
a CNN with two convolutional layers, each with 64 ﬁl-
ters of size 5×5 and subsequent max-pooling over 3×3
windows with stride 2. They are followed by two fully-
connected layers with 256 and 128 units, respectively.
The activation function is ReLU for all layers. The out-
put layer has 10 units with softmax activation and we use
cross-entropy loss. We apply L2-regularization and per-
form data augmentation operations (random cropping of
24×24 pixel subimages, left-right mirroring, color dis-
tortion) on the training inputs.

CIFAR-10 and CIFAR-100 Finally, we train CNNs
on the CIFAR-10 and CIFAR-100 data sets, where the
task is to classify 32×32 pixel RGB images into one of
10 and 100 object categories, respectively. For CIFAR-
10, we crop the images to 24×24 pixels and train a CNN
with two convolutional layers, each with 64 ﬁlters of size

Const. 32 (α = .01)
Const. 128 (α = .03)
Const. 512 (α = .06)
Comp. (θ = .6, α = .006)
CABS (α = .01)

Const. 32 (α = .06)
Const. 128 (α = .1)
Const. 512 (α = .3)
Comp. (θ = 1.0, α = .1)
CABS (α = .1)

101

100

0.5

0.4

0.3

s
s
o
l
n
i
a
r
T

y
c
a
r
u
c
c
a

t
s
e
T

m

200
150
100
50
0

0

s
s
o
l

n
i
a
r
T

101

100

−1

10

−2

10

1

y
c
a
r
u
c
c
a
t
s
e
T

0.95

0.9

0.85

300
200
100
0

0

m

2

4
Examples accessed

6

8

·106

1

2
Examples accessed

3

4

·105

Figure 3: Results for CIFAR-100. Set-up as in Fig. 1.

Figure 4: Results for MNIST. Set-up as in Fig. 1.

5x5 and subsequent max-pooling over 3x3 windows with
stride 2. They are followed by two fully-connected lay-
ers with 384 and 192 units, respectively. The activation
function is ReLU for all layers. The output layer has 10
units with softmax activation and we use cross-entropy
loss. We perform data augmentation operations (ran-
dom cropping, left-right mirroring, color distortion) on
the training set.

For CIFAR-100, we use the full 32×32 image and add a
third convolutional layer (64 ﬁlters of size 5 × 5 followed
by max pooling). The fully-connected layers have 512
and 256 units, respectively, and the output layer has 100
units. We add L2-regularization.

5.2 RESULTS AND DISCUSSION

On SVHN, CIFAR-10 and CIFAR-100 (Figures 1, 2 and
3), CABS yields signiﬁcantly faster decrease in training
loss with the curve contiuously lying below all others. It
also achieves the best test set accuracy of all methods on
all three problems. While the margin over the second-
best method is very small on SVHN, it amounts to a no-
ticeable 0.4 percentage points on CIFAR-10 and even 1.4
points on CIFAR-100.

Surprisingly, on MNIST—the least complex of the bench-
mark problems we investigated—our method is outper-
formed by the small constant batch size of 32 and the

competitor method, which also chooses very small batch
sizes throughout. Our method does, however, surpass
non-adaptive larger batch sizes (128, 512) in terms of
speed and all contenders reach virtually the same test set
accuracy on this problem. CABS makes rapid progress
initially, but seems to choose unnecessarily large batch
sizes later on. The resulting high per-iteration cost ev-
idently can not be compensated by the higher learning
rate it enables (.1 for CABS compared to .01 with constant
batch size of 32). We conjecture that CABS overestimates
the gradient variance due to the homogeneous structure
of the MNIST data set; if the distribution of gradients is
very closely-centered, outliers in a few coordinate direc-
tions lead to comparably high variance estimates.

Overall, CABS outperforms alternative batch size
schemes on three out of the four benchmark problems
we investigated and the beneﬁts seem to increase with the
complexity of the problem (MNIST → SVHN → CIFAR-
10 → CIFAR-100). When considering the CABS batch
size schedules, depicted in the bottom panels of Figures
1 to 4, a common behavior (with the exception of MNIST)
seems to be that CABS uses the minimal batch size (16 in
our experiments) for a considerable portion of the train-
ing process and increases approximately linearly after-
wards.

Finally, we present our ﬁndings regarding the sensitiv-
ity to the choice of learning rate when using CABS. As

Const. 128
Comp. (θ = 0.8)
CABS

In our experiments, CABS was able to speed up SGD
training in neural networks and simplify the tuning of the
learning rate. In contrast to existing methods, it does not
introduce any additional free parameters. A TensorFlow1
implementation of SGD with CABS can be found on
http://github.com/ProbabilisticNumerics/cabs.

101

s
s
o
l
n
i
a
r
T

100

101

100

−1

10

−2

10

s
s
o
l

n
i
a
r
T

0

1

2
Examples accessed

3

·105

4

to zero and rearranging yields (21).

0

0.2

0.6
0.4
Examples accessed

0.8

1

·107

Figure 5: Learning rate sensitivity on SVHN. Families of
training loss curves for CABS, Competitor and a constant
batch size (color-coded). Each individual curve corre-
sponds to a learning rate α ∈ {.1, .06, .03, .01, .006}.

Const. 128
Comp. (θ = 1.0)
CABS

Figure 6: Learning rate sensitivity on MNIST. Set-up as
in Fig. 5.

detailed above, the coupling of learning rate and batch
size in CABS can be seen as tayloring the noise level to
the chosen learning rate. This suggests that the perfor-
mance of the optimizer should be less sensitive to the
choice of learning rate when adapting the batch size with
our method. Indeed, this became evident in our experi-
ments by considering the families of training loss curves
for various learning rates. Figures 5 and 6 compare a
constant batch size, the competitor method and CABS on
the SVHN and MNIST benchmark problems, respectively.
CABS signiﬁcantly reduces the dependency of the perfor-
mance on the learning rate compared to both the constant
batch size and the competing adaptive method. In prac-
tical applications, this ﬁnding could drastically simplify
the often tedious process of learning rate tuning.

6 CONCLUSION

We proposed CABS, a practical rule for dynamic batch
size adaptation based on estimates of the gradient vari-
ance and coupled to the chosen learning rate as well as
optimization progress represented by the function value.

A MATHEMATICAL DETAILS

Proof of Equation (11) By the Cauchy-Schwarz in-
equality, hg, ∇F i = hg, gi − hg, g − ∇F i ≥ kgk2 −
kgkkg − ∇F k = kgk(kgk − kg − ∇F k). This becomes
positive if kg − ∇F k < kgk.

Solving the Maximization Problem (20) We want to
maximize

U (m) =

E[G]
m

=

2α − Lα2
2m

k∇F k2 −

tr(Σ).

Lα2
2m2

(34)

Setting the derivative

U ′(m) = −

2α − Lα2
2m2

k∇F k2 +

tr(Σ)

(35)

Lα2
m3

Proof of Equation (30) The deﬁnition of strong con-
vexity is that, for all w, u ∈ Rd

F (u) ≥ F (w) + ∇F (w)T (u − w) +

ku − wk2 (36)

µ
2

for µ > 0. From there, the proof is identical to that of
Equation (27) in the main text. We minimize both sides
of the inequality. The left-hand side has minimal value
F∗. The gradient with respect to u of the right-hand side
is ∇F (w) + µ(u − w). By setting this to zero we ﬁnd
the minimizer u = −∇F (w)/µ + w. Inserting this back
(cid:3)
into (36) and rearranging yields (30).

References

Bottou, L´eon, Curtis, Frank E, and Nocedal, Jorge. Op-
timization methods for large-scale machine learning.
arXiv preprint arXiv:1606.04838, 2016.

Byrd, Richard H, Chin, Gillian M, Nocedal, Jorge, and
Wu, Yuchen. Sample size selection in optimization
methods for machine learning. Mathematical Pro-
gramming, 134(1):127–155, 2012.

Csiba, Dominik and Richt´arik, Peter.

tance sampling for minibatches.
arXiv:1602.02283, 2016.

Impor-
arXiv preprint

1http://tensorflow.org

the randomized Kaczmarz algorithm. In Advances in
Neural Information Processing Systems 27, pp. 1017–
1025. 2014.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,
Alessandro, Wu, Bo, and Ng, Andrew Y. Reading dig-
its in natural images with unsupervised feature learn-
ing. NIPS Workshop on Deep Learning and Unsuper-
vised Feature Learning, 2011.

Pirotta, Matteo and Restelli, Marcello. Cost-sensitive ap-
proach for batch size optimization. NIPS Workshop on
Optimizing the Optimizer, 2016.

Robbins, Herbert and Monro, Sutton. A stochastic ap-
proximation method. The Annals of Mathematical
Statistics, pp. 400–407, 1951.

Rumelhart, David E., Hinton, Geoffrey E., and Williams,
Learning representations by back-

Ronald J.
propagating errors. Nature, 323:533–536, 1986.

Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more
In Proceedings of the 30th In-
pesky learning rates.
ternational Conference on Machine Learning (ICML),
pp. 343–351, 2013.

Schmidt, Mark, Babanezhad, Reza, Ahmed, Mo-
hamed Osama, Defazio, Aaron, Clifton, Ann, and
Sarkar, Anoop. Non-uniform stochastic average gradi-
ent method for training conditional random ﬁelds. In
Proceedings of the 18th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), 2015.

Zeiler, Matthew D. ADADELTA: An adaptive learning
rate method. arXiv preprint arXiv:1212.5701, 2012.

Zhao, Peilin and Zhang, Tong. Stochastic optimization
with importance sampling for regularized loss mini-
In Proceedings of The 32nd International
mization.
Conference on Machine Learning (ICML), 2015.

Daneshmand, Hadi, Lucchi, Aurelien, and Hofmann,
Thomas. Starting small—learning with adaptive sam-
In Proceedings of the 33nd International
ple sizes.
Conference on Machine Learning (ICML), pp. 1463–
1471, 2016.

De, Soham, Yadav, Abhay, Jacobs, David, and Goldstein,
Tom. Automated inference with adaptive batches. In
Proceedings of the 20th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), 2017.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Si-
mon. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives.
In Advances in Neural Information Processing Sys-
tems 27, pp. 1646–1654. 2014.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12(Jul):2121–2159, 2011.

Friedlander, Michael P and Schmidt, Mark.

Hy-
brid deterministic-stochastic methods for data ﬁtting.
SIAM Journal on Scientiﬁc Computing, 34(3):A1380–
A1405, 2012.

Harikandeh, Reza, Ahmed, Mohamed Osama, Virani,
Alim, Schmidt, Mark, Koneˇcn´y, Jakub, and Sallinen,
Scott. StopWasting my gradients: Practical SVRG. In
Advances in Neural Information Processing Systems
28, pp. 2251–2259. 2015.

Johnson, Rie and Zhang, Tong. Accelerating stochas-
tic gradient descent using predictive variance reduc-
tion. In Advances in Neural Information Processing
Systems 26, pp. 315–323. 2013.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. The International Conference
on Learning Representations (ICLR), 2015.

Krizhevsky, Alex. Learning multiple layers of features
from tiny images. Technical report, University of
Toronto, 2009.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and
Haffner, Patrick. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86
(11):2278–2324, 1998.

MacKay, David JC. Information Theory, Inference and
Learning Algorithms. Cambridge University Press,
2003.

Mahsereci, Maren and Hennig, Philipp. Probabilistic line
searches for stochastic optimization. In Advances in
Neural Information Processing Systems 28, pp. 181–
189. 2015.

Needell, Deanna, Ward, Rachel, and Srebro, Nati.
Stochastic gradient descent, weighted sampling, and

Supplementary Material

Figures 7 to 10 show results for constant batch sizes 16,
64 and 256, which have been omitted from the main text
for lack of space.

101

s
s
o
l
n
i
a
r
T

100

0.95

0.9

0.85

0.8

0.75

m

60
40
20

y
c
a
r
u
c
c
a

t
s
e
T

101

100

s
s
o
l

n
i
a
r
T

−1

10
0.85

0.8

0.75

0.7

0.65

100
50
0

y
c
a
r
u
c
c
a
t
s
e
T

m

Const. 16 (α = .006)
Const. 64 (α = .01)
Const. 256 (α = .1)
CABS (α = .01)

Const. 16 (α = .006)
Const. 64 (α = .01)
Const. 256 (α = .06)
CABS (α = .01)

101

100

0.5

0.4

0.3

s
s
o
l
n
i
a
r
T

y
c
a
r
u
c
c
a

t
s
e
T

m

200
150
100
50
0

0

s
s
o
l

n
i
a
r
T

101

100

−1

10

−2

10

1

y
c
a
r
u
c
c
a
t
s
e
T

0.95

0.9

0.85

300
200
100
0

m

0

0

0.2

0.4
0.6
Examples accessed

0.8

1

·107

2

4
Examples accessed

6

8

·106

Figure 7: Additional results for SVHN.

Figure 9: Additional results for CIFAR-100.

Const. 16 (α = .006)
Const. 64 (α = .01)
Const. 256 (α = .06)
CABS (α = .03)

Const. 16 (α = .06)
Const. 64 (α = .1)
Const. 256 (α = .3)
CABS (α = .1)

0

0.2

0.4
0.6
Examples accessed

0.8

1

·107

1

2
Examples accessed

3

4

·105

Figure 8: Additional results for CIFAR-10.

Figure 10: Additional results for MNIST.

