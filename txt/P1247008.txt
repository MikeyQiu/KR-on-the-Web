Bilingual Sentiment Embeddings:
Joint Projection of Sentiment Across Languages

Jeremy Barnes, Roman Klinger, and Sabine Schulte im Walde
Institut f¨ur Maschinelle Sprachverarbeitung
University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{barnesjy,klinger,schulte}@ims.uni-stuttgart.de

8
1
0
2
 
y
a
M
 
3
2
 
 
]
L
C
.
s
c
[
 
 
1
v
6
1
0
9
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Sentiment analysis in low-resource lan-
guages suffers from a lack of annotated
corpora to estimate high-performing mod-
els. Machine translation and bilingual word
embeddings provide some relief through
cross-lingual sentiment approaches. How-
ever, they either require large amounts of
parallel data or do not sufﬁciently capture
sentiment information. We introduce Bilin-
gual Sentiment Embeddings (BLSE), which
jointly represent sentiment information in a
source and target language. This model
only requires a small bilingual lexicon,
a source-language corpus annotated for
sentiment, and monolingual word embed-
dings for each language. We perform ex-
periments on three language combinations
(Spanish, Catalan, Basque) for sentence-
level cross-lingual sentiment classiﬁcation
and ﬁnd that our model signiﬁcantly out-
performs state-of-the-art methods on four
out of six experimental setups, as well as
capturing complementary information to
machine translation. Our analysis of the re-
sulting embedding space provides evidence
that it represents sentiment information in
the resource-poor target language without
any annotated data in that language.

1

Introduction

Cross-lingual approaches to sentiment analysis are
motivated by the lack of training data in the vast
majority of languages. Even languages spoken
by several million people, such as Catalan, often
have few resources available to perform sentiment
analysis in speciﬁc domains. We therefore aim
to harness the knowledge previously collected in
resource-rich languages.

Previous approaches for cross-lingual sentiment
analysis typically exploit machine translation based
methods or multilingual models. Machine trans-
lation (MT) can provide a way to transfer senti-
ment information from a resource-rich to resource-
poor languages (Mihalcea et al., 2007; Balahur and
Turchi, 2014). However, MT-based methods re-
quire large parallel corpora to train the translation
system, which are often not available for under-
resourced languages.

Examples of multilingual methods that have
been applied to cross-lingual sentiment analysis
include domain adaptation methods (Prettenhofer
and Stein, 2011), delexicalization (Almeida et al.,
2015), and bilingual word embeddings (Mikolov
et al., 2013; Hermann and Blunsom, 2014; Artetxe
et al., 2016). These approaches however do not in-
corporate enough sentiment information to perform
well cross-lingually, as we will show later.

We propose a novel approach to incorporate sen-
timent information in a model, which does not have
these disadvantages. Bilingual Sentiment Embed-
dings (BLSE) are embeddings that are jointly opti-
mized to represent both (a) semantic information in
the source and target languages, which are bound
to each other through a small bilingual dictionary,
and (b) sentiment information, which is annotated
on the source language only. We only need three
resources: (i) a comparably small bilingual lexicon,
(ii) an annotated sentiment corpus in the resource-
rich language, and (iii) monolingual word embed-
dings for the two involved languages.

We show that our model outperforms previous
state-of-the-art models in nearly all experimental
settings across six benchmarks. In addition, we
offer an in-depth analysis and demonstrate that our
model is aware of sentiment. Finally, we provide a
qualitative analysis of the joint bilingual sentiment
space. Our implementation is publicly available at
https://github.com/jbarnesspain/blse.

2 Related Work

Machine Translation: Early work in cross-lingual
sentiment analysis found that machine translation
(MT) had reached a point of maturity that enabled
the transfer of sentiment across languages. Re-
searchers translated sentiment lexicons (Mihalcea
et al., 2007; Meng et al., 2012) or annotated corpora
and used word alignments to project sentiment an-
notation and create target-language annotated cor-
pora (Banea et al., 2008; Duh et al., 2011; Demirtas
and Pechenizkiy, 2013; Balahur and Turchi, 2014).
Several approaches included a multi-view repre-
sentation of the data (Banea et al., 2010; Xiao and
Guo, 2012) or co-training (Wan, 2009; Demirtas
and Pechenizkiy, 2013) to improve over a naive
implementation of machine translation, where only
the translated data is used. There are also ap-
proaches which only require parallel data (Meng
et al., 2012; Zhou et al., 2016; Rasooli et al., 2017),
instead of machine translation.

All of these approaches, however, require large
amounts of parallel data or an existing high qual-
ity translation tool, which are not always available.
A notable exception is the approach proposed by
Chen et al. (2016), an adversarial deep averaging
network, which trains a joint feature extractor for
two languages. They minimize the difference be-
tween these features across languages by learning
to fool a language discriminator, which requires
no parallel data. It does, however, require large
amounts of unlabeled data.

Bilingual Embedding Methods: Recently pro-
posed bilingual embedding methods (Hermann and
Blunsom, 2014; Chandar et al., 2014; Gouws et al.,
2015) offer a natural way to bridge the language
gap. These particular approaches to bilingual em-
beddings, however, require large parallel corpora
in order to build the bilingual space, which are not
available for all language combinations.

An approach to create bilingual embeddings that
has a less prohibitive data requirement is to create
monolingual vector spaces and then learn a projec-
tion from one to the other. Mikolov et al. (2013)
ﬁnd that vector spaces in different languages have
similar arrangements. Therefore, they propose a
linear projection which consists of learning a rota-
tion and scaling matrix. Artetxe et al. (2016, 2017)
improve upon this approach by requiring the pro-
jection to be orthogonal, thereby preserving the
monolingual quality of the original word vectors.

Given source embeddings S,

target embed-
dings T , and a bilingual lexicon L, Artetxe et al.
(2016) learn a projection matrix W by minimizing
the square of Euclidean distances

arg min
W

(cid:88)

i

||S(cid:48)W − T (cid:48)||2

F ,

(1)

where S(cid:48) ∈ S and T (cid:48) ∈ T are the word embedding
matrices for the tokens in the bilingual lexicon L.
This is solved using the Moore-Penrose pseudoin-
verse S(cid:48)+ = (S(cid:48)T S(cid:48))−1S(cid:48)T as W = S(cid:48)+T (cid:48), which
can be computed using SVD. We refer to this ap-
proach as ARTETXE.

Gouws and Søgaard (2015) propose a method to
create a pseudo-bilingual corpus with a small task-
speciﬁc bilingual lexicon, which can then be used
to train bilingual embeddings (BARISTA). This
approach requires a monolingual corpus in both
the source and target languages and a set of trans-
lation pairs. The source and target corpora are
concatenated and then every word is randomly kept
or replaced by its translation with a probability of
0.5. Any kind of word embedding algorithm can be
trained with this pseudo-bilingual corpus to create
bilingual word embeddings.

These last techniques have the advantage of re-
quiring relatively little parallel training data while
taking advantage of larger amounts of monolingual
data. However, they are not optimized for senti-
ment.

Sentiment Embeddings: Maas et al. (2011) ﬁrst
explored the idea of incorporating sentiment in-
formation into semantic word vectors. They pro-
posed a topic modeling approach similar to latent
Dirichlet allocation in order to collect the semantic
information in their word vectors. To incorporate
the sentiment information, they included a second
objective whereby they maximize the probability
of the sentiment label for each word in a labeled
document.

Tang et al. (2014) exploit distantly annotated
tweets to create Twitter sentiment embeddings. To
incorporate distributional information about tokens,
they use a hinge loss and maximize the likelihood
of a true n-gram over a corrupted n-gram. They
include a second objective where they classify the
polarity of the tweet given the true n-gram. While
these techniques have proven useful, they are not
easily transferred to a cross-lingual setting.

Zhou et al. (2015) create bilingual sentiment
embeddings by translating all source data to the

target language and vice versa. This requires the
existence of a machine translation system, which is
a prohibitive assumption for many under-resourced
languages, especially if it must be open and freely
accessible. This motivates approaches which can
use smaller amounts of parallel data to achieve
similar results.

3 Model

In order to project not only semantic similarity and
relatedness but also sentiment information to our
target language, we propose a new model, namely
Bilingual Sentiment Embeddings (BLSE), which
jointly learns to predict sentiment and to minimize
the distance between translation pairs in vector
space. We detail the projection objective in Sec-
tion 3.1, the sentiment objective in Section 3.2, and
the full objective in Section 3.3. A sketch of the
model is depicted in Figure 1.

3.1 Cross-lingual Projection

We assume that we have two precomputed vector
spaces S = Rv×d and T = Rv(cid:48)×d(cid:48)
for our source
and target languages, where v (v(cid:48)) is the length of
the source vocabulary (target vocabulary) and d
(d(cid:48)) is the dimensionality of the embeddings. We
also assume that we have a bilingual lexicon L
of length n which consists of word-to-word trans-
lation pairs L = {(s1, t1), (s2, t2), . . . , (sn, tn)}
which map from source to target.

In order to create a mapping from both origi-
nal vector spaces S and T to shared sentiment-
informed bilingual spaces z and ˆz, we employ two
linear projection matrices, M and M (cid:48). During
training, for each translation pair in L, we ﬁrst look
up their associated vectors, project them through
their associated projection matrix and ﬁnally mini-
mize the mean squared error of the two projected
vectors. This is very similar to the approach taken
by Mikolov et al. (2013), but includes an additional
target projection matrix.

The intuition for including this second matrix is
that a single projection matrix does not support the
transfer of sentiment information from the source
language to the target language. Without M (cid:48), any
signal coming from the sentiment classiﬁer (see
Section 3.2) would have no affect on the target
embedding space T , and optimizing M to predict
sentiment and projection would only be detrimental
to classiﬁcation of the target language. We analyze
this further in Section 6.3. Note that in this con-

ﬁguration, we do not need to update the original
vector spaces, which would be problematic with
such small training data.

The projection quality is ensured by minimizing

the mean squared error12

MSE =

(zi − ˆzi)2 ,

(2)

1
n

n
(cid:88)

i=1

where zi = Ssi · M is the dot product of the embed-
ding for source word si and the source projection
matrix and ˆzi = Tti · M (cid:48) is the same for the target
word ti.

3.2 Sentiment Classiﬁcation

We add a second training objective to optimize
the projected source vectors to predict the senti-
ment of source phrases. This inevitably changes
the projection characteristics of the matrix M , and
consequently M (cid:48) and encourages M (cid:48) to learn to
predict sentiment without any training examples in
the target language.

To train M to predict sentiment, we re-
quire a source-language corpus Csource =
{(x1, y1), (x2, y2), . . . , (xi, yi)} where each sen-
tence xi is associated with a label yi.

For classiﬁcation, we use a two-layer feed-
forward averaging network, loosely following Iyyer
et al. (2015)3. For a sentence xi we take the word
embeddings from the source embedding S and av-
erage them to ai ∈ Rd. We then project this vector
to the joint bilingual space zi = ai · M . Finally,
we pass zi through a softmax layer P to get our
prediction ˆyi = softmax(zi · P ).

To train our model to predict sentiment, we min-

imize the cross-entropy error of our predictions

H = −

yi log ˆyi − (1 − yi) log(1 − ˆyi) . (3)

n
(cid:88)

i=1

3.3

Joint Learning

In order to jointly train both the projection com-
ponent and the sentiment component, we combine
the two loss functions to optimize the parameter

1We omit parameters in equations for better readability.
2We also experimented with cosine distance, but found

that it performed worse than Euclidean distance.

3Our model employs a linear transformation after the aver-
aging layer instead of including a non-linearity function. We
choose this architecture because the weights M and M (cid:48) are
also used to learn a linear cross-lingual projection.

Figure 1: Bilingual Sentiment Embedding Model (BLSE)

EN

ES

CA

EU

Spanish Catalan Basque

y
r
a
n
i
B

s
s
a
l
c
-
4

+ 1258
−
473
Total
1731

++
+
−
−−
Total

379
879
399
74
1731

1216
256
1472

370
846
218
38
1472

718
467
1185

256
462
409
58
1185

956
173
1129

384
572
153
20
1129

Table 1: Statistics for the OpeNER English (EN)
and Spanish (ES) as well as the MultiBooked Cata-
lan (CA) and Basque (EU) datasets.

matrices M , M (cid:48), and P by

(cid:88)

(cid:88)

J =

(x,y)∈Csource

(s,t)∈L

αH(x, y) + (1 − α) · MSE(s, t) ,

(4)
where α is a hyperparameter that weights sentiment
loss vs. projection loss.

3.4 Target-language Classiﬁcation

For inference, we classify sentences from a target-
language corpus Ctarget. As in the training proce-
dure, for each sentence, we take the word embed-
dings from the target embeddings T and average
them to ai ∈ Rd. We then project this vector to the
joint bilingual space ˆzi = ai · M (cid:48). Finally, we pass

Sentences
Tokens
Embeddings

23 M 9.6 M 0.7 M
610 M 183 M
25 M
0.83 M 0.4 M 0.14 M

Table 2: Statistics for the Wikipedia corpora and
monolingual vector spaces.

ˆzi through a softmax layer P to get our prediction
ˆyi = softmax(ˆzi · P ).

4 Datasets and Resources

4.1 OpeNER and MultiBooked

To evaluate our proposed model, we conduct ex-
periments using four benchmark datasets and three
bilingual combinations. We use the OpeNER En-
glish and Spanish datasets (Agerri et al., 2013)
and the MultiBooked Catalan and Basque datasets
(Barnes et al., 2018). All datasets contain hotel
reviews which are annotated for aspect-level senti-
ment analysis. The labels include Strong Negative
(−−), Negative (−), Positive (+), and Strong Pos-
itive (++). We map the aspect-level annotations
to sentence level by taking the most common label
and remove instances of mixed polarity. We also
create a binary setup by combining the strong and
weak classes. This gives us a total of six experi-
ments. The details of the sentence-level datasets
are summarized in Table 1. For each of the experi-

5 Experiments

5.1 Setting

We compare BLSE (Sections 3.1–3.3) to ARTETXE
(Section 2) and BARISTA (Section 2) as baselines,
which have similar data requirements and to ma-
chine translation (MT) and monolingual (MONO)
upper bounds which request more resources. For
all models (MONO, MT, ARTETXE, BARISTA),
we take the average of the word embeddings in
the source-language training examples and train a
linear SVM7. We report this instead of using the
same feed-forward network as in BLSE as it is the
stronger upper bound. We choose the parameter c
on the target language development set and evalu-
ate on the target language test set.

Upper Bound MONO. We set an empirical up-
per bound by training and testing a linear SVM
on the target language data. As mentioned in Sec-
tion 5.1, we train the model on the averaged em-
beddings from target language training data, tuning
the c parameter on the development data. We test
on the target language test data.

Upper Bound MT. To test the effectiveness of
machine translation, we translate all of the senti-
ment corpora from the target language to English
using the Google Translate API8. Note that this
approach is not considered a baseline, as we as-
sume not to have access to high-quality machine
translation for low-resource languages of interest.
Baseline ARTETXE. We compare with the ap-
proach proposed by Artetxe et al. (2016) which
has shown promise on other tasks, such as word
similarity. In order to learn the projection matrix
W , we need translation pairs. We use the same
word-to-word bilingual lexicon mentioned in Sec-
tion 3.1. We then map the source vector space
S to the bilingual space ˆS = SW and use these
embeddings.

Baseline BARISTA. We also compare with the
approach proposed by Gouws and Søgaard (2015).
The bilingual lexicon used to create the pseudo-
bilingual corpus is the same word-to-word bilin-
gual lexicon mentioned in Section 3.1. We follow
the authors’ setup to create the pseudo-bilingual
corpus. We create bilingual embeddings by train-
ing skip-gram embeddings using the Word2Vec
toolkit on the pseudo-bilingual corpus using the
same parameters from Section 4.2.

7LinearSVC implementation from scikit-learn.
8https://translate.google.com

Figure 2: Binary and four class macro F1 on Span-
ish (ES), Catalan (CA), and Basque (EU).

ments, we take 70 percent of the data for training,
20 percent for testing and the remaining 10 percent
are used as development data for tuning.

4.2 Monolingual Word Embeddings

For BLSE, ARTETXE, and MT, we require monolin-
gual vector spaces for each of our languages. For
English, we use the publicly available GoogleNews
vectors4. For Spanish, Catalan, and Basque, we
train skip-gram embeddings using the Word2Vec
toolkit4 with 300 dimensions, subsampling of 10−4,
window of 5, negative sampling of 15 based on a
2016 Wikipedia corpus5 (sentence-split, tokenized
with IXA pipes (Agerri et al., 2014) and lower-
cased). The statistics of the Wikipedia corpora are
given in Table 2.

4.3 Bilingual Lexicon

For BLSE, ARTETXE, and BARISTA, we also re-
quire a bilingual lexicon. We use the sentiment
lexicon from Hu and Liu (2004) (to which we refer
in the following as Bing Liu) and its translation
into each target language. We translate the lexicon
using Google Translate and exclude multi-word ex-
pressions.6 This leaves a dictionary of 5700 trans-
lations in Spanish, 5271 in Catalan, and 4577 in
Basque. We set aside ten percent of the translation
pairs as a development set in order to check that the
distances between translation pairs not seen during
training are also minimized during training.

4https://code.google.com/archive/p/word2vec/
5http://attardi.github.io/wikiextractor/
6Note that we only do that for convenience. Using a ma-
chine translation service to generate this list could easily be
replaced by a manual translation, as the lexicon is comparably
small.

Our method: BLSE. We implement our model
BLSE in Pytorch (Paszke et al., 2016) and initial-
ize the word embeddings with the pretrained word
embeddings S and T mentioned in Section 4.2.
We use the word-to-word bilingual lexicon from
Section 4.3, tune the hyperparameters α, training
epochs, and batch size on the target development
set and use the best hyperparameters achieved on
the development set for testing. ADAM (Kingma
and Ba, 2014) is used in order to minimize the
average loss of the training batches.

Binary

4-class

ES

CA

EU

ES CA EU

s
d
n
u
o
B

r
e
p
p
U

T
M

N
O
M

O P
R
F1
P
R
F1
E P
S
L
B

74.0
67.4
69.8
75.6
66.5
69.4

79.0
79.6
79.2
78.0
76.8
77.2

55.2 50.0 48.3
75.0
42.8 50.9 46.5
72.3
45.5 49.9 47.1
73.5
51.8 58.9 43.6
82.3
48.5 50.5 45.2
76.6
48.8 52.7 43.6
79.0
72.1 **72.8 **67.5 **60.0 38.1 *42.5
R **80.1 **73.0 **72.7 *43.4 38.1 37.4
F1 **74.6 **72.9 **69.3 *41.2 35.9 30.0

s
e
n
i
l
e
s
a
B

x
t
e
t
r

e P
R
F1

A

t
s
i
r
a
B

aP
R
F1

x
t
e
t
r

e P
R
F1

A

e
l
b
m
e
s
n
E

t
s
i
r
a
B

aP
R
F1

EP
R
F1

S
L
B

75.0
64.3
67.1

64.7
59.8
61.2

65.3
61.3
62.6

60.1
55.5
56.0

79.5
78.7
80.3

60.1
61.2
60.7

65.3
61.2
60.1

63.1
63.3
63.2

63.4
62.3
62.5

84.7
85.5
85.0

42.2
49.5
45.6

55.5
54.5
54.8

70.4
64.3
66.4

50.7
50.4
49.8

80.9
69.9
73.5

40.1 21.6 30.0
36.9 29.8 35.7
34.9 23.0 21.3

44.1 36.4 34.1
37.9 38.5 34.3
39.5 36.2 33.8

43.5 46.5 50.1
44.1 48.7 50.7
43.8 47.6 49.9

48.3 52.8 50.8
46.6 53.7 49.8
47.1 53.0 47.8

49.5 54.1 50.3
51.2 53.9 51.4
50.3 53.9 50.5

Table 3: Precision (P), Recall (R), and macro F1 of
four models trained on English and tested on Span-
ish (ES), Catalan (CA), and Basque (EU). The bold
numbers show the best results for each metric per
column and the highlighted numbers show where
BLSE is better than the other projection methods,
ARTETXE and BARISTA (** p < 0.01, * p < 0.05).

Ensembles We create an ensemble of MT
and each projection method (BLSE, ARTETXE,
BARISTA) by training a random forest classiﬁer
on the predictions from MT and each of these ap-
proaches. This allows us to evaluate to what extent
each projection model adds complementary infor-
mation to the machine translation approach.

5.2 Results

In Figure 2, we report the results of all four meth-
ods. Our method outperforms the other projection
methods (the baselines ARTETXE and BARISTA)
on four of the six experiments substantially. It per-
forms only slightly worse than the more resource-
costly upper bounds (MT and MONO). This is espe-
cially noticeable for the binary classiﬁcation task,
where BLSE performs nearly as well as machine
translation and signiﬁcantly better than the other
methods. We perform approximate randomization
tests (Yeh, 2000) with 10,000 runs and highlight
the results that are statistically signiﬁcant (**p <
0.01, *p < 0.05) in Table 3.

In more detail, we see that MT generally per-
forms better than the projection methods (79–69
F1 on binary, 52–44 on 4-class). BLSE (75–69
on binary, 41–30 on 4-class) has the best perfor-
mance of the projection methods and is comparable
with MT on the binary setup, with no signiﬁcant
difference on binary Basque. ARTETXE (67–46
on binary, 35–21 on 4-class) and BARISTA (61–
55 on binary, 40–34 on 4-class) are signiﬁcantly
worse than BLSE on all experiments except Cata-
lan and Basque 4-class. On the binary experiment,
ARTETXE outperforms BARISTA on Spanish (67.1
vs. 61.2) and Catalan (60.7 vs. 60.1) but suffers
more than the other methods on the four-class ex-
periments, with a maximum F1 of 34.9. BARISTA

Model

MT

ARTETXE

BARISTA

BLSE

c
o
v

49
147
80
182
89
191
67
146

d
o
m

26
94
44
141
41
109
45
125

g
e
n

19
19
27
19
27
24
21
29

bi
4
bi
4
bi
4
bi
4

w
o
n
k

14
21
14
24
20
31
15
22

r
e
h
t
o

5
12
7
19
7
15
8
19

l
a
t
o
t

113
293
172
385
184
370
156
341

Table 4: Error analysis for different phenomena.
See text for explanation of error classes.

This suggests that BLSE is better ARTETXE and
BARISTA at transferring sentiment of the most im-
portant sentiment bearing words.

Negation: Negation is a well-studied phe-
nomenon in sentiment analysis (Pang et al., 2002;
Wiegand et al., 2010; Zhu et al., 2014; Reitan et al.,
2015). Therefore, we are interested in how these
four models perform on phrases that include the
negation of a key element, for example “In general,
this hotel isn’t bad”. We would like our models
to recognize that the combination of two negative
elements “isn’t” and “bad” lead to a Positive label.
Given the simple classiﬁcation strategy, all mod-
els perform relatively well on phrases with negation
(all reach nearly 60 F1 in the binary setting). How-
ever, while BLSE performs the best on negation in
the binary setting (82.9 F1), it has more problems
with negation in the 4-class setting (36.9 F1).

Adverbial Modiﬁers: Phrases that are modiﬁed
by an adverb, e. g., the food was incredibly good,
are important for the four-class setup, as they often
differentiate between the base and Strong labels.
In the binary case, all models reach more than 55
F1. In the 4-class setup, BLSE only achieves 27.2
F1 compared to 46.6 or 31.3 of MT and BARISTA,
respectively. Therefore, presumably, our model
does currently not capture the semantics of the
target adverbs well. This is likely due to the fact
that it assigns too much sentiment to functional
words (see Figure 6).

External Knowledge Required: These errors
are difﬁcult for any of the models to get cor-
rect. Many of these include numbers which imply
positive or negative sentiment (350 meters from
the beach is Positive while 3 kilometers from the
beach is Negative). BLSE performs the best (63.5
F1) while MT performs comparably well (62.5).
BARISTA performs the worst (43.6).

Binary vs. 4-class: All of the models suffer
when moving from the binary to 4-class setting;
an average of 26.8 in macro F1 for MT, 31.4 for
ARTETXE, 22.2 for BARISTA, and for 36.6 BLSE.
The two vector projection methods (ARTETXE and
BLSE) suffer the most, suggesting that they are
currently more apt for the binary setting.

6.2 Effect of Bilingual Lexicon

We analyze how the number of translation pairs
affects our model. We train on the 4-class Span-
ish setup using the best hyper-parameters from the
previous experiment.

Figure 3: Macro F1 for translation pairs in the
Spanish 4-class setup.

is relatively stable across languages.

ENSEMBLE performs the best, which shows that
BLSE adds complementary information to MT. Fi-
nally, we note that all systems perform successively
worse on Catalan and Basque. This is presum-
ably due to the quality of the word embeddings, as
well as the increased morphological complexity of
Basque.

6 Model and Error Analysis

We analyze three aspects of our model in further
detail: (i) where most mistakes originate, (ii) the ef-
fect of the bilingual lexicon, and (iii) the effect and
necessity of the target-language projection matrix
M (cid:48).

6.1 Phenomena

In order to analyze where each model struggles, we
categorize the mistakes and annotate all of the test
phrases with one of the following error classes: vo-
cabulary (voc), adverbial modiﬁers (mod), negation
(neg), external knowledge (know) or other. Table 4
shows the results.

Vocabulary: The most common way to express
sentiment in hotel reviews is through the use of
polar adjectives (as in “the room was great) or the
mention of certain nouns that are desirable (“it
had a pool”). Although this phenomenon has the
largest total number of mistakes (an average of
71 per model on binary and 167 on 4-class), it is
mainly due to its prevalence. MT performed the
best on the test examples which according to the an-
notation require a correct understanding of the vo-
cabulary (81 F1 on binary /54 F1 on 4-class), with
BLSE (79/48) slightly worse. ARTETXE (70/35)
and BARISTA (67/41) perform signiﬁcantly worse.

Figure 4: Average cosine similarity between a subsample of translation pairs of same polarity (“sentiment
synonyms”) and of opposing polarity (“sentiment antonyms”) in both target and source languages in each
model. The x-axis shows training epochs. We see that BLSE is able to learn that sentiment synonyms
should be close to one another in vector space and sentiment antonyms should not.

Research into projection techniques for bilingual
word embeddings (Mikolov et al., 2013; Lazaridou
et al., 2015; Artetxe et al., 2016) often uses a lex-
icon of the most frequent 8–10 thousand words
in English and their translations as training data.
We test this approach by taking the 10,000 word-
to-word translations from the Apertium English-
to-Spanish dictionary9. We also use the Google
Translate API to translate the NRC hashtag senti-
ment lexicon (Mohammad et al., 2013) and keep
the 22,984 word-to-word translations. We perform
the same experiment as above and vary the amount
of training data from 0, 100, 300, 600, 1000, 3000,
6000, 10,000 up to 20,000 training pairs. Finally,
we compile a small hand translated dictionary of
200 pairs, which we then expand using target lan-
guage morphological information, ﬁnally giving
us 657 translation pairs10. The macro F1 score for
the Bing Liu dictionary climbs constantly with the
increasing translation pairs. Both the Apertium
and NRC dictionaries perform worse than the trans-
lated lexicon by Bing Liu, while the expanded hand
translated dictionary is competitive, as shown in
Figure 3.

While for some tasks, e. g., bilingual lexicon
induction, using the most frequent words as trans-
lation pairs is an effective approach, for sentiment
analysis, this does not seem to help. Using a trans-
lated sentiment lexicon, even if it is small, gives
better results.

9http://www.meta-share.org
10The translation took approximately one hour. We can
extrapolate that hand translating a sentiment lexicon the size
of the Bing Liu lexicon would take no more than 5 hours.

Figure 5: BLSE model (solid lines) compared to a
variant without target language projection matrix
M (cid:48) (dashed lines). “Translation” lines show the
average cosine similarity between translation pairs.
The remaining lines show F1 scores for the source
and target language with both varints of BLSE. The
modiﬁed model cannot learn to predict sentiment
in the target language (red lines). This illustrates
the need for the second projection matrix M (cid:48).

6.3 Analysis of M (cid:48)

The main motivation for using two projection ma-
trices M and M (cid:48) is to allow the original embed-
dings to remain stable, while the projection ma-
trices have the ﬂexibility to align translations and
separate these into distinct sentiment subspaces. To
justify this design decision empirically, we perform
an experiment to evaluate the actual need for the
target language projection matrix M (cid:48): We create a
simpliﬁed version of our model without M (cid:48), using
M to project from the source to target and then P
to classify sentiment.

The results of this model are shown in Figure 5.
The modiﬁed model does learn to predict in the
source language, but not in the target language.
This conﬁrms that M (cid:48) is necessary to transfer sen-
timent in our model.

7 Qualitative Analyses of Joint Bilingual

Sentiment Space

In order to understand how well our model trans-
fers sentiment information to the target language,
we perform two qualitative analyses. First, we
collect two sets of 100 positive sentiment words
and one set of 100 negative sentiment words. An
effective cross-lingual sentiment classiﬁer using
embeddings should learn that two positive words
should be closer in the shared bilingual space than a
positive word and a negative word. We test if BLSE
is able to do this by training our model and after
every epoch observing the mean cosine similarity
between the sentiment synonyms and sentiment
antonyms after projecting to the joint space.

We compare BLSE with ARTETXE and BARISTA
by replacing the Linear SVM classiﬁers with the
same multi-layer classiﬁer used in BLSE and ob-
serving the distances in the hidden layer. Figure 4
shows this similarity in both source and target lan-
guage, along with the mean cosine similarity be-
tween a held-out set of translation pairs and the
macro F1 scores on the development set for both
source and target languages for BLSE, BARISTA,
and ARTETXE. From this plot, it is clear that BLSE
is able to learn that sentiment synonyms should be
close to one another in vector space and antonyms
should have a negative cosine similarity. While
the other models also learn this to some degree,
jointly optimizing both sentiment and projection
gives better results.

Secondly, we would like to know how well the
projected vectors compare to the original space.
Our hypothesis is that some relatedness and simi-
larity information is lost during projection. There-
fore, we visualize six categories of words in t-SNE
(Van der Maaten and Hinton, 2008): positive senti-
ment words, negative sentiment words, functional
words, verbs, animals, and transport.

The t-SNE plots in Figure 6 show that the posi-
tive and negative sentiment words are rather clearly
separated after projection in BLSE. This indicates
that we are able to incorporate sentiment informa-
tion into our target language without any labeled
data in the target language. However, the downside

Figure 6: t-SNE-based visualization of the Spanish
vector space before and after projection with BLSE.
There is a clear separation of positive and negative
words after projection, despite the fact that we have
used no labeled data in Spanish.

of this is that functional words and transportation
words are highly correlated with positive sentiment.

8 Conclusion

We have presented a new model, BLSE, which
is able to leverage sentiment information from a
resource-rich language to perform sentiment analy-
sis on a resource-poor target language. This model
requires less parallel data than MT and performs
better than other state-of-the-art methods with sim-
ilar data requirements, an average of 14 percentage
points in F1 on binary and 4 pp on 4-class cross-
lingual sentiment analysis. We have also performed
a phenomena-driven error analysis which showed
that BLSE is better than ARTETXE and BARISTA
at transferring sentiment, but assigns too much sen-
timent to functional words. In the future, we will
extend our model so that it can project multi-word
phrases, as well as single words, which could help
with negations and modiﬁers.

Acknowledgements

We thank Sebastian Pad´o, Sebastian Riedel, Eneko
Agirre, and Mikel Artetxe for their conversations
and feedback.

References

Rodrigo Agerri, Josu Bermudez, and German Rigau.
2014. Ixa pipeline: Efﬁcient and ready to use mul-
tilingual nlp tools. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14). pages 3823–3828.

Rodrigo Agerri, Montse Cuadros, Sean Gaines, and
German Rigau. 2013. OpeNER: Open polarity
Sociedad
enhanced named entity recognition.
Espa˜nola para el Procesamiento del Lenguaje Nat-
ural 51(Septiembre):215–218.

Mariana S. C. Almeida, Claudia Pinto, Helena Figueira,
Pedro Mendes, and Andr´e F. T. Martins. 2015.
Aligning opinions: Cross-lingual opinion mining
with dependencies. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers). pages 408–418.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing. pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). pages 451–462.

Alexandra Balahur and Marco Turchi. 2014. Compar-
ative experiments using supervised learning and ma-
chine translation for multilingual sentiment analysis.
Computer Speech & Language 28(1):56–75.

Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more lan-
In Proceedings of the 23rd Inter-
guages better?
national Conference on Computational Linguistics
(Coling 2010). pages 28–36.

Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity anal-
In Proceedings of
ysis using machine translation.
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing. pages 127–135.

Jeremy Barnes, Patrik Lambert, and Toni Badia. 2018.
Multibooked: A corpus of basque and catalan hotel
reviews annotated for aspect-level sentiment classiﬁ-
cation. In Proceedings of 11th Language Resources
and Evaluation Conference (LREC’18).

Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, Cur-
ran Associates, Inc., pages 1853–1861.

Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Q.
Adver-
Weinberger, and Claire Cardie. 2016.
sarial deep averaging networks for cross-lingual
CoRR abs/1606.01614.
sentiment classiﬁcation.
http://arxiv.org/abs/1606.01614.

Erkin Demirtas and Mykola Pechenizkiy. 2013. Cross-
lingual polarity detection with machine translation.
Proceedings of the International Workshop on Issues
of Sentiment Discovery and Opinion Mining - WIS-
DOM ’13 pages 9:1–9:8.

Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for cross-lingual senti-
ment classiﬁcation? Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers 2:429–433.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. BilBOWA: Fast bilingual distributed repre-
sentations without word alignments. Proceedings
of The 32nd International Conference on Machine
Learning pages 748–756.

Stephan Gouws and Anders Søgaard. 2015. Simple
In Pro-
task-speciﬁc bilingual word embeddings.
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
1386–1390.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 58–
68.

Minqing Hu and Bing Liu. 2004. Mining opinion
In Proceedings of
features in customer reviews.
the 10th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2004). pages 168–177.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daume III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classiﬁcation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). Beijing,
China, pages 1681–1691.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR) .

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: delving into
cross-space mapping for zero-shot learning. Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing pages 270–280.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. pages 142–150.

Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-lingual
mixture model for sentiment classiﬁcation. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Jeju Island, Korea, pages 572–581.
http://www.aclweb.org/anthology/P12-1060.

Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics. pages 976–983.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9:2579–2605.

Xiaojun Wan. 2009. Co-training for cross-lingual sen-
In Proceedings of the Joint
timent classiﬁcation.
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP. pages 235–
243.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing. pages 60–
68.

Min Xiao and Yuhong Guo. 2012. Multi-view ad-
aboost for multilingual subjectivity analysis. In Pro-
ceedings of COLING 2012. pages 2851–2866.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
Exploiting similarities among languages
CoRR abs/1309.4168.

2013.
for machine translation.
http://arxiv.org/abs/1309.4168.

Alexander Yeh. 2000. More accurate tests for the statis-
tical signiﬁcance of result differences. In Proceed-
ings of the 18th Conference on Computational lin-
guistics (COLING). pages 947–953.

Guangyou Zhou, Zhiyuan Zhu, Tingting He, and Xiao-
hua Tony Hu. 2016. Cross-lingual sentiment classi-
ﬁcation with stacked autoencoders. Knowledge and
Information Systems 47(1):27–44.

HuiWei Zhou, Long Chen, Fulin Shi, and Degen
Huang. 2015. Learning bilingual sentiment word
embeddings for cross-language sentiment classiﬁ-
In Proceedings of the 53rd Annual Meet-
cation.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
pages 430–440.

Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svet-
lana Kiritchenko. 2014. An empirical study on the
effect of negation words on sentiment. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). pages 304–313.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of the seventh international workshop on Se-
mantic Evaluation Exercises (SemEval-2013).

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classiﬁcation using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical methods in natu-
ral language processing-Volume 10. Association for
Computational Linguistics, pages 79–86.

Adam Paszke, Sam Gross, Soumith Chintala, and Gre-
gory Chanan. 2016. Pytorch deeplearning frame-
work. http://pytorch.org. Accessed: 2017-08-10.

Peter Prettenhofer and Benno Stein. 2011. Cross-
lingual adaptation using structural correspondence
learning. ACM Transactions on Intelligent Systems
and Technology 3(1):1–22.

Mohammad Sadegh Rasooli, Noura Farra, Axinia
Radeva, Tao Yu, and Kathleen McKeown. 2017.
Cross-lingual sentiment
transfer with limited re-
sources. Machine Translation .

Johan Reitan, Jørgen Faret, Bj¨orn Gamb¨ack, and Lars
Bungum. 2015. Negation scope detection for twitter
sentiment analysis. In Proceedings of the 6th Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis. pages 99–108.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin. 2014. Learning sentiment-speciﬁc
word embedding for twitter sentiment classiﬁcation.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). pages 1555–1565.

Bilingual Sentiment Embeddings:
Joint Projection of Sentiment Across Languages

Jeremy Barnes, Roman Klinger, and Sabine Schulte im Walde
Institut f¨ur Maschinelle Sprachverarbeitung
University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{barnesjy,klinger,schulte}@ims.uni-stuttgart.de

8
1
0
2
 
y
a
M
 
3
2
 
 
]
L
C
.
s
c
[
 
 
1
v
6
1
0
9
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Sentiment analysis in low-resource lan-
guages suffers from a lack of annotated
corpora to estimate high-performing mod-
els. Machine translation and bilingual word
embeddings provide some relief through
cross-lingual sentiment approaches. How-
ever, they either require large amounts of
parallel data or do not sufﬁciently capture
sentiment information. We introduce Bilin-
gual Sentiment Embeddings (BLSE), which
jointly represent sentiment information in a
source and target language. This model
only requires a small bilingual lexicon,
a source-language corpus annotated for
sentiment, and monolingual word embed-
dings for each language. We perform ex-
periments on three language combinations
(Spanish, Catalan, Basque) for sentence-
level cross-lingual sentiment classiﬁcation
and ﬁnd that our model signiﬁcantly out-
performs state-of-the-art methods on four
out of six experimental setups, as well as
capturing complementary information to
machine translation. Our analysis of the re-
sulting embedding space provides evidence
that it represents sentiment information in
the resource-poor target language without
any annotated data in that language.

1

Introduction

Cross-lingual approaches to sentiment analysis are
motivated by the lack of training data in the vast
majority of languages. Even languages spoken
by several million people, such as Catalan, often
have few resources available to perform sentiment
analysis in speciﬁc domains. We therefore aim
to harness the knowledge previously collected in
resource-rich languages.

Previous approaches for cross-lingual sentiment
analysis typically exploit machine translation based
methods or multilingual models. Machine trans-
lation (MT) can provide a way to transfer senti-
ment information from a resource-rich to resource-
poor languages (Mihalcea et al., 2007; Balahur and
Turchi, 2014). However, MT-based methods re-
quire large parallel corpora to train the translation
system, which are often not available for under-
resourced languages.

Examples of multilingual methods that have
been applied to cross-lingual sentiment analysis
include domain adaptation methods (Prettenhofer
and Stein, 2011), delexicalization (Almeida et al.,
2015), and bilingual word embeddings (Mikolov
et al., 2013; Hermann and Blunsom, 2014; Artetxe
et al., 2016). These approaches however do not in-
corporate enough sentiment information to perform
well cross-lingually, as we will show later.

We propose a novel approach to incorporate sen-
timent information in a model, which does not have
these disadvantages. Bilingual Sentiment Embed-
dings (BLSE) are embeddings that are jointly opti-
mized to represent both (a) semantic information in
the source and target languages, which are bound
to each other through a small bilingual dictionary,
and (b) sentiment information, which is annotated
on the source language only. We only need three
resources: (i) a comparably small bilingual lexicon,
(ii) an annotated sentiment corpus in the resource-
rich language, and (iii) monolingual word embed-
dings for the two involved languages.

We show that our model outperforms previous
state-of-the-art models in nearly all experimental
settings across six benchmarks. In addition, we
offer an in-depth analysis and demonstrate that our
model is aware of sentiment. Finally, we provide a
qualitative analysis of the joint bilingual sentiment
space. Our implementation is publicly available at
https://github.com/jbarnesspain/blse.

2 Related Work

Machine Translation: Early work in cross-lingual
sentiment analysis found that machine translation
(MT) had reached a point of maturity that enabled
the transfer of sentiment across languages. Re-
searchers translated sentiment lexicons (Mihalcea
et al., 2007; Meng et al., 2012) or annotated corpora
and used word alignments to project sentiment an-
notation and create target-language annotated cor-
pora (Banea et al., 2008; Duh et al., 2011; Demirtas
and Pechenizkiy, 2013; Balahur and Turchi, 2014).
Several approaches included a multi-view repre-
sentation of the data (Banea et al., 2010; Xiao and
Guo, 2012) or co-training (Wan, 2009; Demirtas
and Pechenizkiy, 2013) to improve over a naive
implementation of machine translation, where only
the translated data is used. There are also ap-
proaches which only require parallel data (Meng
et al., 2012; Zhou et al., 2016; Rasooli et al., 2017),
instead of machine translation.

All of these approaches, however, require large
amounts of parallel data or an existing high qual-
ity translation tool, which are not always available.
A notable exception is the approach proposed by
Chen et al. (2016), an adversarial deep averaging
network, which trains a joint feature extractor for
two languages. They minimize the difference be-
tween these features across languages by learning
to fool a language discriminator, which requires
no parallel data. It does, however, require large
amounts of unlabeled data.

Bilingual Embedding Methods: Recently pro-
posed bilingual embedding methods (Hermann and
Blunsom, 2014; Chandar et al., 2014; Gouws et al.,
2015) offer a natural way to bridge the language
gap. These particular approaches to bilingual em-
beddings, however, require large parallel corpora
in order to build the bilingual space, which are not
available for all language combinations.

An approach to create bilingual embeddings that
has a less prohibitive data requirement is to create
monolingual vector spaces and then learn a projec-
tion from one to the other. Mikolov et al. (2013)
ﬁnd that vector spaces in different languages have
similar arrangements. Therefore, they propose a
linear projection which consists of learning a rota-
tion and scaling matrix. Artetxe et al. (2016, 2017)
improve upon this approach by requiring the pro-
jection to be orthogonal, thereby preserving the
monolingual quality of the original word vectors.

Given source embeddings S,

target embed-
dings T , and a bilingual lexicon L, Artetxe et al.
(2016) learn a projection matrix W by minimizing
the square of Euclidean distances

arg min
W

(cid:88)

i

||S(cid:48)W − T (cid:48)||2

F ,

(1)

where S(cid:48) ∈ S and T (cid:48) ∈ T are the word embedding
matrices for the tokens in the bilingual lexicon L.
This is solved using the Moore-Penrose pseudoin-
verse S(cid:48)+ = (S(cid:48)T S(cid:48))−1S(cid:48)T as W = S(cid:48)+T (cid:48), which
can be computed using SVD. We refer to this ap-
proach as ARTETXE.

Gouws and Søgaard (2015) propose a method to
create a pseudo-bilingual corpus with a small task-
speciﬁc bilingual lexicon, which can then be used
to train bilingual embeddings (BARISTA). This
approach requires a monolingual corpus in both
the source and target languages and a set of trans-
lation pairs. The source and target corpora are
concatenated and then every word is randomly kept
or replaced by its translation with a probability of
0.5. Any kind of word embedding algorithm can be
trained with this pseudo-bilingual corpus to create
bilingual word embeddings.

These last techniques have the advantage of re-
quiring relatively little parallel training data while
taking advantage of larger amounts of monolingual
data. However, they are not optimized for senti-
ment.

Sentiment Embeddings: Maas et al. (2011) ﬁrst
explored the idea of incorporating sentiment in-
formation into semantic word vectors. They pro-
posed a topic modeling approach similar to latent
Dirichlet allocation in order to collect the semantic
information in their word vectors. To incorporate
the sentiment information, they included a second
objective whereby they maximize the probability
of the sentiment label for each word in a labeled
document.

Tang et al. (2014) exploit distantly annotated
tweets to create Twitter sentiment embeddings. To
incorporate distributional information about tokens,
they use a hinge loss and maximize the likelihood
of a true n-gram over a corrupted n-gram. They
include a second objective where they classify the
polarity of the tweet given the true n-gram. While
these techniques have proven useful, they are not
easily transferred to a cross-lingual setting.

Zhou et al. (2015) create bilingual sentiment
embeddings by translating all source data to the

target language and vice versa. This requires the
existence of a machine translation system, which is
a prohibitive assumption for many under-resourced
languages, especially if it must be open and freely
accessible. This motivates approaches which can
use smaller amounts of parallel data to achieve
similar results.

3 Model

In order to project not only semantic similarity and
relatedness but also sentiment information to our
target language, we propose a new model, namely
Bilingual Sentiment Embeddings (BLSE), which
jointly learns to predict sentiment and to minimize
the distance between translation pairs in vector
space. We detail the projection objective in Sec-
tion 3.1, the sentiment objective in Section 3.2, and
the full objective in Section 3.3. A sketch of the
model is depicted in Figure 1.

3.1 Cross-lingual Projection

We assume that we have two precomputed vector
spaces S = Rv×d and T = Rv(cid:48)×d(cid:48)
for our source
and target languages, where v (v(cid:48)) is the length of
the source vocabulary (target vocabulary) and d
(d(cid:48)) is the dimensionality of the embeddings. We
also assume that we have a bilingual lexicon L
of length n which consists of word-to-word trans-
lation pairs L = {(s1, t1), (s2, t2), . . . , (sn, tn)}
which map from source to target.

In order to create a mapping from both origi-
nal vector spaces S and T to shared sentiment-
informed bilingual spaces z and ˆz, we employ two
linear projection matrices, M and M (cid:48). During
training, for each translation pair in L, we ﬁrst look
up their associated vectors, project them through
their associated projection matrix and ﬁnally mini-
mize the mean squared error of the two projected
vectors. This is very similar to the approach taken
by Mikolov et al. (2013), but includes an additional
target projection matrix.

The intuition for including this second matrix is
that a single projection matrix does not support the
transfer of sentiment information from the source
language to the target language. Without M (cid:48), any
signal coming from the sentiment classiﬁer (see
Section 3.2) would have no affect on the target
embedding space T , and optimizing M to predict
sentiment and projection would only be detrimental
to classiﬁcation of the target language. We analyze
this further in Section 6.3. Note that in this con-

ﬁguration, we do not need to update the original
vector spaces, which would be problematic with
such small training data.

The projection quality is ensured by minimizing

the mean squared error12

MSE =

(zi − ˆzi)2 ,

(2)

1
n

n
(cid:88)

i=1

where zi = Ssi · M is the dot product of the embed-
ding for source word si and the source projection
matrix and ˆzi = Tti · M (cid:48) is the same for the target
word ti.

3.2 Sentiment Classiﬁcation

We add a second training objective to optimize
the projected source vectors to predict the senti-
ment of source phrases. This inevitably changes
the projection characteristics of the matrix M , and
consequently M (cid:48) and encourages M (cid:48) to learn to
predict sentiment without any training examples in
the target language.

To train M to predict sentiment, we re-
quire a source-language corpus Csource =
{(x1, y1), (x2, y2), . . . , (xi, yi)} where each sen-
tence xi is associated with a label yi.

For classiﬁcation, we use a two-layer feed-
forward averaging network, loosely following Iyyer
et al. (2015)3. For a sentence xi we take the word
embeddings from the source embedding S and av-
erage them to ai ∈ Rd. We then project this vector
to the joint bilingual space zi = ai · M . Finally,
we pass zi through a softmax layer P to get our
prediction ˆyi = softmax(zi · P ).

To train our model to predict sentiment, we min-

imize the cross-entropy error of our predictions

H = −

yi log ˆyi − (1 − yi) log(1 − ˆyi) . (3)

n
(cid:88)

i=1

3.3

Joint Learning

In order to jointly train both the projection com-
ponent and the sentiment component, we combine
the two loss functions to optimize the parameter

1We omit parameters in equations for better readability.
2We also experimented with cosine distance, but found

that it performed worse than Euclidean distance.

3Our model employs a linear transformation after the aver-
aging layer instead of including a non-linearity function. We
choose this architecture because the weights M and M (cid:48) are
also used to learn a linear cross-lingual projection.

Figure 1: Bilingual Sentiment Embedding Model (BLSE)

EN

ES

CA

EU

Spanish Catalan Basque

y
r
a
n
i
B

s
s
a
l
c
-
4

+ 1258
−
473
Total
1731

++
+
−
−−
Total

379
879
399
74
1731

1216
256
1472

370
846
218
38
1472

718
467
1185

256
462
409
58
1185

956
173
1129

384
572
153
20
1129

Table 1: Statistics for the OpeNER English (EN)
and Spanish (ES) as well as the MultiBooked Cata-
lan (CA) and Basque (EU) datasets.

matrices M , M (cid:48), and P by

(cid:88)

(cid:88)

J =

(x,y)∈Csource

(s,t)∈L

αH(x, y) + (1 − α) · MSE(s, t) ,

(4)
where α is a hyperparameter that weights sentiment
loss vs. projection loss.

3.4 Target-language Classiﬁcation

For inference, we classify sentences from a target-
language corpus Ctarget. As in the training proce-
dure, for each sentence, we take the word embed-
dings from the target embeddings T and average
them to ai ∈ Rd. We then project this vector to the
joint bilingual space ˆzi = ai · M (cid:48). Finally, we pass

Sentences
Tokens
Embeddings

23 M 9.6 M 0.7 M
610 M 183 M
25 M
0.83 M 0.4 M 0.14 M

Table 2: Statistics for the Wikipedia corpora and
monolingual vector spaces.

ˆzi through a softmax layer P to get our prediction
ˆyi = softmax(ˆzi · P ).

4 Datasets and Resources

4.1 OpeNER and MultiBooked

To evaluate our proposed model, we conduct ex-
periments using four benchmark datasets and three
bilingual combinations. We use the OpeNER En-
glish and Spanish datasets (Agerri et al., 2013)
and the MultiBooked Catalan and Basque datasets
(Barnes et al., 2018). All datasets contain hotel
reviews which are annotated for aspect-level senti-
ment analysis. The labels include Strong Negative
(−−), Negative (−), Positive (+), and Strong Pos-
itive (++). We map the aspect-level annotations
to sentence level by taking the most common label
and remove instances of mixed polarity. We also
create a binary setup by combining the strong and
weak classes. This gives us a total of six experi-
ments. The details of the sentence-level datasets
are summarized in Table 1. For each of the experi-

5 Experiments

5.1 Setting

We compare BLSE (Sections 3.1–3.3) to ARTETXE
(Section 2) and BARISTA (Section 2) as baselines,
which have similar data requirements and to ma-
chine translation (MT) and monolingual (MONO)
upper bounds which request more resources. For
all models (MONO, MT, ARTETXE, BARISTA),
we take the average of the word embeddings in
the source-language training examples and train a
linear SVM7. We report this instead of using the
same feed-forward network as in BLSE as it is the
stronger upper bound. We choose the parameter c
on the target language development set and evalu-
ate on the target language test set.

Upper Bound MONO. We set an empirical up-
per bound by training and testing a linear SVM
on the target language data. As mentioned in Sec-
tion 5.1, we train the model on the averaged em-
beddings from target language training data, tuning
the c parameter on the development data. We test
on the target language test data.

Upper Bound MT. To test the effectiveness of
machine translation, we translate all of the senti-
ment corpora from the target language to English
using the Google Translate API8. Note that this
approach is not considered a baseline, as we as-
sume not to have access to high-quality machine
translation for low-resource languages of interest.
Baseline ARTETXE. We compare with the ap-
proach proposed by Artetxe et al. (2016) which
has shown promise on other tasks, such as word
similarity. In order to learn the projection matrix
W , we need translation pairs. We use the same
word-to-word bilingual lexicon mentioned in Sec-
tion 3.1. We then map the source vector space
S to the bilingual space ˆS = SW and use these
embeddings.

Baseline BARISTA. We also compare with the
approach proposed by Gouws and Søgaard (2015).
The bilingual lexicon used to create the pseudo-
bilingual corpus is the same word-to-word bilin-
gual lexicon mentioned in Section 3.1. We follow
the authors’ setup to create the pseudo-bilingual
corpus. We create bilingual embeddings by train-
ing skip-gram embeddings using the Word2Vec
toolkit on the pseudo-bilingual corpus using the
same parameters from Section 4.2.

7LinearSVC implementation from scikit-learn.
8https://translate.google.com

Figure 2: Binary and four class macro F1 on Span-
ish (ES), Catalan (CA), and Basque (EU).

ments, we take 70 percent of the data for training,
20 percent for testing and the remaining 10 percent
are used as development data for tuning.

4.2 Monolingual Word Embeddings

For BLSE, ARTETXE, and MT, we require monolin-
gual vector spaces for each of our languages. For
English, we use the publicly available GoogleNews
vectors4. For Spanish, Catalan, and Basque, we
train skip-gram embeddings using the Word2Vec
toolkit4 with 300 dimensions, subsampling of 10−4,
window of 5, negative sampling of 15 based on a
2016 Wikipedia corpus5 (sentence-split, tokenized
with IXA pipes (Agerri et al., 2014) and lower-
cased). The statistics of the Wikipedia corpora are
given in Table 2.

4.3 Bilingual Lexicon

For BLSE, ARTETXE, and BARISTA, we also re-
quire a bilingual lexicon. We use the sentiment
lexicon from Hu and Liu (2004) (to which we refer
in the following as Bing Liu) and its translation
into each target language. We translate the lexicon
using Google Translate and exclude multi-word ex-
pressions.6 This leaves a dictionary of 5700 trans-
lations in Spanish, 5271 in Catalan, and 4577 in
Basque. We set aside ten percent of the translation
pairs as a development set in order to check that the
distances between translation pairs not seen during
training are also minimized during training.

4https://code.google.com/archive/p/word2vec/
5http://attardi.github.io/wikiextractor/
6Note that we only do that for convenience. Using a ma-
chine translation service to generate this list could easily be
replaced by a manual translation, as the lexicon is comparably
small.

Our method: BLSE. We implement our model
BLSE in Pytorch (Paszke et al., 2016) and initial-
ize the word embeddings with the pretrained word
embeddings S and T mentioned in Section 4.2.
We use the word-to-word bilingual lexicon from
Section 4.3, tune the hyperparameters α, training
epochs, and batch size on the target development
set and use the best hyperparameters achieved on
the development set for testing. ADAM (Kingma
and Ba, 2014) is used in order to minimize the
average loss of the training batches.

Binary

4-class

ES

CA

EU

ES CA EU

s
d
n
u
o
B

r
e
p
p
U

T
M

N
O
M

O P
R
F1
P
R
F1
E P
S
L
B

74.0
67.4
69.8
75.6
66.5
69.4

79.0
79.6
79.2
78.0
76.8
77.2

55.2 50.0 48.3
75.0
42.8 50.9 46.5
72.3
45.5 49.9 47.1
73.5
51.8 58.9 43.6
82.3
48.5 50.5 45.2
76.6
48.8 52.7 43.6
79.0
72.1 **72.8 **67.5 **60.0 38.1 *42.5
R **80.1 **73.0 **72.7 *43.4 38.1 37.4
F1 **74.6 **72.9 **69.3 *41.2 35.9 30.0

s
e
n
i
l
e
s
a
B

x
t
e
t
r

e P
R
F1

A

t
s
i
r
a
B

aP
R
F1

x
t
e
t
r

e P
R
F1

A

e
l
b
m
e
s
n
E

t
s
i
r
a
B

aP
R
F1

EP
R
F1

S
L
B

75.0
64.3
67.1

64.7
59.8
61.2

65.3
61.3
62.6

60.1
55.5
56.0

79.5
78.7
80.3

60.1
61.2
60.7

65.3
61.2
60.1

63.1
63.3
63.2

63.4
62.3
62.5

84.7
85.5
85.0

42.2
49.5
45.6

55.5
54.5
54.8

70.4
64.3
66.4

50.7
50.4
49.8

80.9
69.9
73.5

40.1 21.6 30.0
36.9 29.8 35.7
34.9 23.0 21.3

44.1 36.4 34.1
37.9 38.5 34.3
39.5 36.2 33.8

43.5 46.5 50.1
44.1 48.7 50.7
43.8 47.6 49.9

48.3 52.8 50.8
46.6 53.7 49.8
47.1 53.0 47.8

49.5 54.1 50.3
51.2 53.9 51.4
50.3 53.9 50.5

Table 3: Precision (P), Recall (R), and macro F1 of
four models trained on English and tested on Span-
ish (ES), Catalan (CA), and Basque (EU). The bold
numbers show the best results for each metric per
column and the highlighted numbers show where
BLSE is better than the other projection methods,
ARTETXE and BARISTA (** p < 0.01, * p < 0.05).

Ensembles We create an ensemble of MT
and each projection method (BLSE, ARTETXE,
BARISTA) by training a random forest classiﬁer
on the predictions from MT and each of these ap-
proaches. This allows us to evaluate to what extent
each projection model adds complementary infor-
mation to the machine translation approach.

5.2 Results

In Figure 2, we report the results of all four meth-
ods. Our method outperforms the other projection
methods (the baselines ARTETXE and BARISTA)
on four of the six experiments substantially. It per-
forms only slightly worse than the more resource-
costly upper bounds (MT and MONO). This is espe-
cially noticeable for the binary classiﬁcation task,
where BLSE performs nearly as well as machine
translation and signiﬁcantly better than the other
methods. We perform approximate randomization
tests (Yeh, 2000) with 10,000 runs and highlight
the results that are statistically signiﬁcant (**p <
0.01, *p < 0.05) in Table 3.

In more detail, we see that MT generally per-
forms better than the projection methods (79–69
F1 on binary, 52–44 on 4-class). BLSE (75–69
on binary, 41–30 on 4-class) has the best perfor-
mance of the projection methods and is comparable
with MT on the binary setup, with no signiﬁcant
difference on binary Basque. ARTETXE (67–46
on binary, 35–21 on 4-class) and BARISTA (61–
55 on binary, 40–34 on 4-class) are signiﬁcantly
worse than BLSE on all experiments except Cata-
lan and Basque 4-class. On the binary experiment,
ARTETXE outperforms BARISTA on Spanish (67.1
vs. 61.2) and Catalan (60.7 vs. 60.1) but suffers
more than the other methods on the four-class ex-
periments, with a maximum F1 of 34.9. BARISTA

Model

MT

ARTETXE

BARISTA

BLSE

c
o
v

49
147
80
182
89
191
67
146

d
o
m

26
94
44
141
41
109
45
125

g
e
n

19
19
27
19
27
24
21
29

bi
4
bi
4
bi
4
bi
4

w
o
n
k

14
21
14
24
20
31
15
22

r
e
h
t
o

5
12
7
19
7
15
8
19

l
a
t
o
t

113
293
172
385
184
370
156
341

Table 4: Error analysis for different phenomena.
See text for explanation of error classes.

This suggests that BLSE is better ARTETXE and
BARISTA at transferring sentiment of the most im-
portant sentiment bearing words.

Negation: Negation is a well-studied phe-
nomenon in sentiment analysis (Pang et al., 2002;
Wiegand et al., 2010; Zhu et al., 2014; Reitan et al.,
2015). Therefore, we are interested in how these
four models perform on phrases that include the
negation of a key element, for example “In general,
this hotel isn’t bad”. We would like our models
to recognize that the combination of two negative
elements “isn’t” and “bad” lead to a Positive label.
Given the simple classiﬁcation strategy, all mod-
els perform relatively well on phrases with negation
(all reach nearly 60 F1 in the binary setting). How-
ever, while BLSE performs the best on negation in
the binary setting (82.9 F1), it has more problems
with negation in the 4-class setting (36.9 F1).

Adverbial Modiﬁers: Phrases that are modiﬁed
by an adverb, e. g., the food was incredibly good,
are important for the four-class setup, as they often
differentiate between the base and Strong labels.
In the binary case, all models reach more than 55
F1. In the 4-class setup, BLSE only achieves 27.2
F1 compared to 46.6 or 31.3 of MT and BARISTA,
respectively. Therefore, presumably, our model
does currently not capture the semantics of the
target adverbs well. This is likely due to the fact
that it assigns too much sentiment to functional
words (see Figure 6).

External Knowledge Required: These errors
are difﬁcult for any of the models to get cor-
rect. Many of these include numbers which imply
positive or negative sentiment (350 meters from
the beach is Positive while 3 kilometers from the
beach is Negative). BLSE performs the best (63.5
F1) while MT performs comparably well (62.5).
BARISTA performs the worst (43.6).

Binary vs. 4-class: All of the models suffer
when moving from the binary to 4-class setting;
an average of 26.8 in macro F1 for MT, 31.4 for
ARTETXE, 22.2 for BARISTA, and for 36.6 BLSE.
The two vector projection methods (ARTETXE and
BLSE) suffer the most, suggesting that they are
currently more apt for the binary setting.

6.2 Effect of Bilingual Lexicon

We analyze how the number of translation pairs
affects our model. We train on the 4-class Span-
ish setup using the best hyper-parameters from the
previous experiment.

Figure 3: Macro F1 for translation pairs in the
Spanish 4-class setup.

is relatively stable across languages.

ENSEMBLE performs the best, which shows that
BLSE adds complementary information to MT. Fi-
nally, we note that all systems perform successively
worse on Catalan and Basque. This is presum-
ably due to the quality of the word embeddings, as
well as the increased morphological complexity of
Basque.

6 Model and Error Analysis

We analyze three aspects of our model in further
detail: (i) where most mistakes originate, (ii) the ef-
fect of the bilingual lexicon, and (iii) the effect and
necessity of the target-language projection matrix
M (cid:48).

6.1 Phenomena

In order to analyze where each model struggles, we
categorize the mistakes and annotate all of the test
phrases with one of the following error classes: vo-
cabulary (voc), adverbial modiﬁers (mod), negation
(neg), external knowledge (know) or other. Table 4
shows the results.

Vocabulary: The most common way to express
sentiment in hotel reviews is through the use of
polar adjectives (as in “the room was great) or the
mention of certain nouns that are desirable (“it
had a pool”). Although this phenomenon has the
largest total number of mistakes (an average of
71 per model on binary and 167 on 4-class), it is
mainly due to its prevalence. MT performed the
best on the test examples which according to the an-
notation require a correct understanding of the vo-
cabulary (81 F1 on binary /54 F1 on 4-class), with
BLSE (79/48) slightly worse. ARTETXE (70/35)
and BARISTA (67/41) perform signiﬁcantly worse.

Figure 4: Average cosine similarity between a subsample of translation pairs of same polarity (“sentiment
synonyms”) and of opposing polarity (“sentiment antonyms”) in both target and source languages in each
model. The x-axis shows training epochs. We see that BLSE is able to learn that sentiment synonyms
should be close to one another in vector space and sentiment antonyms should not.

Research into projection techniques for bilingual
word embeddings (Mikolov et al., 2013; Lazaridou
et al., 2015; Artetxe et al., 2016) often uses a lex-
icon of the most frequent 8–10 thousand words
in English and their translations as training data.
We test this approach by taking the 10,000 word-
to-word translations from the Apertium English-
to-Spanish dictionary9. We also use the Google
Translate API to translate the NRC hashtag senti-
ment lexicon (Mohammad et al., 2013) and keep
the 22,984 word-to-word translations. We perform
the same experiment as above and vary the amount
of training data from 0, 100, 300, 600, 1000, 3000,
6000, 10,000 up to 20,000 training pairs. Finally,
we compile a small hand translated dictionary of
200 pairs, which we then expand using target lan-
guage morphological information, ﬁnally giving
us 657 translation pairs10. The macro F1 score for
the Bing Liu dictionary climbs constantly with the
increasing translation pairs. Both the Apertium
and NRC dictionaries perform worse than the trans-
lated lexicon by Bing Liu, while the expanded hand
translated dictionary is competitive, as shown in
Figure 3.

While for some tasks, e. g., bilingual lexicon
induction, using the most frequent words as trans-
lation pairs is an effective approach, for sentiment
analysis, this does not seem to help. Using a trans-
lated sentiment lexicon, even if it is small, gives
better results.

9http://www.meta-share.org
10The translation took approximately one hour. We can
extrapolate that hand translating a sentiment lexicon the size
of the Bing Liu lexicon would take no more than 5 hours.

Figure 5: BLSE model (solid lines) compared to a
variant without target language projection matrix
M (cid:48) (dashed lines). “Translation” lines show the
average cosine similarity between translation pairs.
The remaining lines show F1 scores for the source
and target language with both varints of BLSE. The
modiﬁed model cannot learn to predict sentiment
in the target language (red lines). This illustrates
the need for the second projection matrix M (cid:48).

6.3 Analysis of M (cid:48)

The main motivation for using two projection ma-
trices M and M (cid:48) is to allow the original embed-
dings to remain stable, while the projection ma-
trices have the ﬂexibility to align translations and
separate these into distinct sentiment subspaces. To
justify this design decision empirically, we perform
an experiment to evaluate the actual need for the
target language projection matrix M (cid:48): We create a
simpliﬁed version of our model without M (cid:48), using
M to project from the source to target and then P
to classify sentiment.

The results of this model are shown in Figure 5.
The modiﬁed model does learn to predict in the
source language, but not in the target language.
This conﬁrms that M (cid:48) is necessary to transfer sen-
timent in our model.

7 Qualitative Analyses of Joint Bilingual

Sentiment Space

In order to understand how well our model trans-
fers sentiment information to the target language,
we perform two qualitative analyses. First, we
collect two sets of 100 positive sentiment words
and one set of 100 negative sentiment words. An
effective cross-lingual sentiment classiﬁer using
embeddings should learn that two positive words
should be closer in the shared bilingual space than a
positive word and a negative word. We test if BLSE
is able to do this by training our model and after
every epoch observing the mean cosine similarity
between the sentiment synonyms and sentiment
antonyms after projecting to the joint space.

We compare BLSE with ARTETXE and BARISTA
by replacing the Linear SVM classiﬁers with the
same multi-layer classiﬁer used in BLSE and ob-
serving the distances in the hidden layer. Figure 4
shows this similarity in both source and target lan-
guage, along with the mean cosine similarity be-
tween a held-out set of translation pairs and the
macro F1 scores on the development set for both
source and target languages for BLSE, BARISTA,
and ARTETXE. From this plot, it is clear that BLSE
is able to learn that sentiment synonyms should be
close to one another in vector space and antonyms
should have a negative cosine similarity. While
the other models also learn this to some degree,
jointly optimizing both sentiment and projection
gives better results.

Secondly, we would like to know how well the
projected vectors compare to the original space.
Our hypothesis is that some relatedness and simi-
larity information is lost during projection. There-
fore, we visualize six categories of words in t-SNE
(Van der Maaten and Hinton, 2008): positive senti-
ment words, negative sentiment words, functional
words, verbs, animals, and transport.

The t-SNE plots in Figure 6 show that the posi-
tive and negative sentiment words are rather clearly
separated after projection in BLSE. This indicates
that we are able to incorporate sentiment informa-
tion into our target language without any labeled
data in the target language. However, the downside

Figure 6: t-SNE-based visualization of the Spanish
vector space before and after projection with BLSE.
There is a clear separation of positive and negative
words after projection, despite the fact that we have
used no labeled data in Spanish.

of this is that functional words and transportation
words are highly correlated with positive sentiment.

8 Conclusion

We have presented a new model, BLSE, which
is able to leverage sentiment information from a
resource-rich language to perform sentiment analy-
sis on a resource-poor target language. This model
requires less parallel data than MT and performs
better than other state-of-the-art methods with sim-
ilar data requirements, an average of 14 percentage
points in F1 on binary and 4 pp on 4-class cross-
lingual sentiment analysis. We have also performed
a phenomena-driven error analysis which showed
that BLSE is better than ARTETXE and BARISTA
at transferring sentiment, but assigns too much sen-
timent to functional words. In the future, we will
extend our model so that it can project multi-word
phrases, as well as single words, which could help
with negations and modiﬁers.

Acknowledgements

We thank Sebastian Pad´o, Sebastian Riedel, Eneko
Agirre, and Mikel Artetxe for their conversations
and feedback.

References

Rodrigo Agerri, Josu Bermudez, and German Rigau.
2014. Ixa pipeline: Efﬁcient and ready to use mul-
tilingual nlp tools. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14). pages 3823–3828.

Rodrigo Agerri, Montse Cuadros, Sean Gaines, and
German Rigau. 2013. OpeNER: Open polarity
Sociedad
enhanced named entity recognition.
Espa˜nola para el Procesamiento del Lenguaje Nat-
ural 51(Septiembre):215–218.

Mariana S. C. Almeida, Claudia Pinto, Helena Figueira,
Pedro Mendes, and Andr´e F. T. Martins. 2015.
Aligning opinions: Cross-lingual opinion mining
with dependencies. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers). pages 408–418.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing. pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). pages 451–462.

Alexandra Balahur and Marco Turchi. 2014. Compar-
ative experiments using supervised learning and ma-
chine translation for multilingual sentiment analysis.
Computer Speech & Language 28(1):56–75.

Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more lan-
In Proceedings of the 23rd Inter-
guages better?
national Conference on Computational Linguistics
(Coling 2010). pages 28–36.

Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity anal-
In Proceedings of
ysis using machine translation.
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing. pages 127–135.

Jeremy Barnes, Patrik Lambert, and Toni Badia. 2018.
Multibooked: A corpus of basque and catalan hotel
reviews annotated for aspect-level sentiment classiﬁ-
cation. In Proceedings of 11th Language Resources
and Evaluation Conference (LREC’18).

Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, Cur-
ran Associates, Inc., pages 1853–1861.

Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Q.
Adver-
Weinberger, and Claire Cardie. 2016.
sarial deep averaging networks for cross-lingual
CoRR abs/1606.01614.
sentiment classiﬁcation.
http://arxiv.org/abs/1606.01614.

Erkin Demirtas and Mykola Pechenizkiy. 2013. Cross-
lingual polarity detection with machine translation.
Proceedings of the International Workshop on Issues
of Sentiment Discovery and Opinion Mining - WIS-
DOM ’13 pages 9:1–9:8.

Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for cross-lingual senti-
ment classiﬁcation? Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers 2:429–433.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. BilBOWA: Fast bilingual distributed repre-
sentations without word alignments. Proceedings
of The 32nd International Conference on Machine
Learning pages 748–756.

Stephan Gouws and Anders Søgaard. 2015. Simple
In Pro-
task-speciﬁc bilingual word embeddings.
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
1386–1390.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 58–
68.

Minqing Hu and Bing Liu. 2004. Mining opinion
In Proceedings of
features in customer reviews.
the 10th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2004). pages 168–177.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daume III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classiﬁcation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). Beijing,
China, pages 1681–1691.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR) .

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: delving into
cross-space mapping for zero-shot learning. Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing pages 270–280.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. pages 142–150.

Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-lingual
mixture model for sentiment classiﬁcation. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Jeju Island, Korea, pages 572–581.
http://www.aclweb.org/anthology/P12-1060.

Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics. pages 976–983.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9:2579–2605.

Xiaojun Wan. 2009. Co-training for cross-lingual sen-
In Proceedings of the Joint
timent classiﬁcation.
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP. pages 235–
243.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing. pages 60–
68.

Min Xiao and Yuhong Guo. 2012. Multi-view ad-
aboost for multilingual subjectivity analysis. In Pro-
ceedings of COLING 2012. pages 2851–2866.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
Exploiting similarities among languages
CoRR abs/1309.4168.

2013.
for machine translation.
http://arxiv.org/abs/1309.4168.

Alexander Yeh. 2000. More accurate tests for the statis-
tical signiﬁcance of result differences. In Proceed-
ings of the 18th Conference on Computational lin-
guistics (COLING). pages 947–953.

Guangyou Zhou, Zhiyuan Zhu, Tingting He, and Xiao-
hua Tony Hu. 2016. Cross-lingual sentiment classi-
ﬁcation with stacked autoencoders. Knowledge and
Information Systems 47(1):27–44.

HuiWei Zhou, Long Chen, Fulin Shi, and Degen
Huang. 2015. Learning bilingual sentiment word
embeddings for cross-language sentiment classiﬁ-
In Proceedings of the 53rd Annual Meet-
cation.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
pages 430–440.

Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svet-
lana Kiritchenko. 2014. An empirical study on the
effect of negation words on sentiment. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). pages 304–313.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of the seventh international workshop on Se-
mantic Evaluation Exercises (SemEval-2013).

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classiﬁcation using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical methods in natu-
ral language processing-Volume 10. Association for
Computational Linguistics, pages 79–86.

Adam Paszke, Sam Gross, Soumith Chintala, and Gre-
gory Chanan. 2016. Pytorch deeplearning frame-
work. http://pytorch.org. Accessed: 2017-08-10.

Peter Prettenhofer and Benno Stein. 2011. Cross-
lingual adaptation using structural correspondence
learning. ACM Transactions on Intelligent Systems
and Technology 3(1):1–22.

Mohammad Sadegh Rasooli, Noura Farra, Axinia
Radeva, Tao Yu, and Kathleen McKeown. 2017.
Cross-lingual sentiment
transfer with limited re-
sources. Machine Translation .

Johan Reitan, Jørgen Faret, Bj¨orn Gamb¨ack, and Lars
Bungum. 2015. Negation scope detection for twitter
sentiment analysis. In Proceedings of the 6th Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis. pages 99–108.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin. 2014. Learning sentiment-speciﬁc
word embedding for twitter sentiment classiﬁcation.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). pages 1555–1565.

Bilingual Sentiment Embeddings:
Joint Projection of Sentiment Across Languages

Jeremy Barnes, Roman Klinger, and Sabine Schulte im Walde
Institut f¨ur Maschinelle Sprachverarbeitung
University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{barnesjy,klinger,schulte}@ims.uni-stuttgart.de

8
1
0
2
 
y
a
M
 
3
2
 
 
]
L
C
.
s
c
[
 
 
1
v
6
1
0
9
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Sentiment analysis in low-resource lan-
guages suffers from a lack of annotated
corpora to estimate high-performing mod-
els. Machine translation and bilingual word
embeddings provide some relief through
cross-lingual sentiment approaches. How-
ever, they either require large amounts of
parallel data or do not sufﬁciently capture
sentiment information. We introduce Bilin-
gual Sentiment Embeddings (BLSE), which
jointly represent sentiment information in a
source and target language. This model
only requires a small bilingual lexicon,
a source-language corpus annotated for
sentiment, and monolingual word embed-
dings for each language. We perform ex-
periments on three language combinations
(Spanish, Catalan, Basque) for sentence-
level cross-lingual sentiment classiﬁcation
and ﬁnd that our model signiﬁcantly out-
performs state-of-the-art methods on four
out of six experimental setups, as well as
capturing complementary information to
machine translation. Our analysis of the re-
sulting embedding space provides evidence
that it represents sentiment information in
the resource-poor target language without
any annotated data in that language.

1

Introduction

Cross-lingual approaches to sentiment analysis are
motivated by the lack of training data in the vast
majority of languages. Even languages spoken
by several million people, such as Catalan, often
have few resources available to perform sentiment
analysis in speciﬁc domains. We therefore aim
to harness the knowledge previously collected in
resource-rich languages.

Previous approaches for cross-lingual sentiment
analysis typically exploit machine translation based
methods or multilingual models. Machine trans-
lation (MT) can provide a way to transfer senti-
ment information from a resource-rich to resource-
poor languages (Mihalcea et al., 2007; Balahur and
Turchi, 2014). However, MT-based methods re-
quire large parallel corpora to train the translation
system, which are often not available for under-
resourced languages.

Examples of multilingual methods that have
been applied to cross-lingual sentiment analysis
include domain adaptation methods (Prettenhofer
and Stein, 2011), delexicalization (Almeida et al.,
2015), and bilingual word embeddings (Mikolov
et al., 2013; Hermann and Blunsom, 2014; Artetxe
et al., 2016). These approaches however do not in-
corporate enough sentiment information to perform
well cross-lingually, as we will show later.

We propose a novel approach to incorporate sen-
timent information in a model, which does not have
these disadvantages. Bilingual Sentiment Embed-
dings (BLSE) are embeddings that are jointly opti-
mized to represent both (a) semantic information in
the source and target languages, which are bound
to each other through a small bilingual dictionary,
and (b) sentiment information, which is annotated
on the source language only. We only need three
resources: (i) a comparably small bilingual lexicon,
(ii) an annotated sentiment corpus in the resource-
rich language, and (iii) monolingual word embed-
dings for the two involved languages.

We show that our model outperforms previous
state-of-the-art models in nearly all experimental
settings across six benchmarks. In addition, we
offer an in-depth analysis and demonstrate that our
model is aware of sentiment. Finally, we provide a
qualitative analysis of the joint bilingual sentiment
space. Our implementation is publicly available at
https://github.com/jbarnesspain/blse.

2 Related Work

Machine Translation: Early work in cross-lingual
sentiment analysis found that machine translation
(MT) had reached a point of maturity that enabled
the transfer of sentiment across languages. Re-
searchers translated sentiment lexicons (Mihalcea
et al., 2007; Meng et al., 2012) or annotated corpora
and used word alignments to project sentiment an-
notation and create target-language annotated cor-
pora (Banea et al., 2008; Duh et al., 2011; Demirtas
and Pechenizkiy, 2013; Balahur and Turchi, 2014).
Several approaches included a multi-view repre-
sentation of the data (Banea et al., 2010; Xiao and
Guo, 2012) or co-training (Wan, 2009; Demirtas
and Pechenizkiy, 2013) to improve over a naive
implementation of machine translation, where only
the translated data is used. There are also ap-
proaches which only require parallel data (Meng
et al., 2012; Zhou et al., 2016; Rasooli et al., 2017),
instead of machine translation.

All of these approaches, however, require large
amounts of parallel data or an existing high qual-
ity translation tool, which are not always available.
A notable exception is the approach proposed by
Chen et al. (2016), an adversarial deep averaging
network, which trains a joint feature extractor for
two languages. They minimize the difference be-
tween these features across languages by learning
to fool a language discriminator, which requires
no parallel data. It does, however, require large
amounts of unlabeled data.

Bilingual Embedding Methods: Recently pro-
posed bilingual embedding methods (Hermann and
Blunsom, 2014; Chandar et al., 2014; Gouws et al.,
2015) offer a natural way to bridge the language
gap. These particular approaches to bilingual em-
beddings, however, require large parallel corpora
in order to build the bilingual space, which are not
available for all language combinations.

An approach to create bilingual embeddings that
has a less prohibitive data requirement is to create
monolingual vector spaces and then learn a projec-
tion from one to the other. Mikolov et al. (2013)
ﬁnd that vector spaces in different languages have
similar arrangements. Therefore, they propose a
linear projection which consists of learning a rota-
tion and scaling matrix. Artetxe et al. (2016, 2017)
improve upon this approach by requiring the pro-
jection to be orthogonal, thereby preserving the
monolingual quality of the original word vectors.

Given source embeddings S,

target embed-
dings T , and a bilingual lexicon L, Artetxe et al.
(2016) learn a projection matrix W by minimizing
the square of Euclidean distances

arg min
W

(cid:88)

i

||S(cid:48)W − T (cid:48)||2

F ,

(1)

where S(cid:48) ∈ S and T (cid:48) ∈ T are the word embedding
matrices for the tokens in the bilingual lexicon L.
This is solved using the Moore-Penrose pseudoin-
verse S(cid:48)+ = (S(cid:48)T S(cid:48))−1S(cid:48)T as W = S(cid:48)+T (cid:48), which
can be computed using SVD. We refer to this ap-
proach as ARTETXE.

Gouws and Søgaard (2015) propose a method to
create a pseudo-bilingual corpus with a small task-
speciﬁc bilingual lexicon, which can then be used
to train bilingual embeddings (BARISTA). This
approach requires a monolingual corpus in both
the source and target languages and a set of trans-
lation pairs. The source and target corpora are
concatenated and then every word is randomly kept
or replaced by its translation with a probability of
0.5. Any kind of word embedding algorithm can be
trained with this pseudo-bilingual corpus to create
bilingual word embeddings.

These last techniques have the advantage of re-
quiring relatively little parallel training data while
taking advantage of larger amounts of monolingual
data. However, they are not optimized for senti-
ment.

Sentiment Embeddings: Maas et al. (2011) ﬁrst
explored the idea of incorporating sentiment in-
formation into semantic word vectors. They pro-
posed a topic modeling approach similar to latent
Dirichlet allocation in order to collect the semantic
information in their word vectors. To incorporate
the sentiment information, they included a second
objective whereby they maximize the probability
of the sentiment label for each word in a labeled
document.

Tang et al. (2014) exploit distantly annotated
tweets to create Twitter sentiment embeddings. To
incorporate distributional information about tokens,
they use a hinge loss and maximize the likelihood
of a true n-gram over a corrupted n-gram. They
include a second objective where they classify the
polarity of the tweet given the true n-gram. While
these techniques have proven useful, they are not
easily transferred to a cross-lingual setting.

Zhou et al. (2015) create bilingual sentiment
embeddings by translating all source data to the

target language and vice versa. This requires the
existence of a machine translation system, which is
a prohibitive assumption for many under-resourced
languages, especially if it must be open and freely
accessible. This motivates approaches which can
use smaller amounts of parallel data to achieve
similar results.

3 Model

In order to project not only semantic similarity and
relatedness but also sentiment information to our
target language, we propose a new model, namely
Bilingual Sentiment Embeddings (BLSE), which
jointly learns to predict sentiment and to minimize
the distance between translation pairs in vector
space. We detail the projection objective in Sec-
tion 3.1, the sentiment objective in Section 3.2, and
the full objective in Section 3.3. A sketch of the
model is depicted in Figure 1.

3.1 Cross-lingual Projection

We assume that we have two precomputed vector
spaces S = Rv×d and T = Rv(cid:48)×d(cid:48)
for our source
and target languages, where v (v(cid:48)) is the length of
the source vocabulary (target vocabulary) and d
(d(cid:48)) is the dimensionality of the embeddings. We
also assume that we have a bilingual lexicon L
of length n which consists of word-to-word trans-
lation pairs L = {(s1, t1), (s2, t2), . . . , (sn, tn)}
which map from source to target.

In order to create a mapping from both origi-
nal vector spaces S and T to shared sentiment-
informed bilingual spaces z and ˆz, we employ two
linear projection matrices, M and M (cid:48). During
training, for each translation pair in L, we ﬁrst look
up their associated vectors, project them through
their associated projection matrix and ﬁnally mini-
mize the mean squared error of the two projected
vectors. This is very similar to the approach taken
by Mikolov et al. (2013), but includes an additional
target projection matrix.

The intuition for including this second matrix is
that a single projection matrix does not support the
transfer of sentiment information from the source
language to the target language. Without M (cid:48), any
signal coming from the sentiment classiﬁer (see
Section 3.2) would have no affect on the target
embedding space T , and optimizing M to predict
sentiment and projection would only be detrimental
to classiﬁcation of the target language. We analyze
this further in Section 6.3. Note that in this con-

ﬁguration, we do not need to update the original
vector spaces, which would be problematic with
such small training data.

The projection quality is ensured by minimizing

the mean squared error12

MSE =

(zi − ˆzi)2 ,

(2)

1
n

n
(cid:88)

i=1

where zi = Ssi · M is the dot product of the embed-
ding for source word si and the source projection
matrix and ˆzi = Tti · M (cid:48) is the same for the target
word ti.

3.2 Sentiment Classiﬁcation

We add a second training objective to optimize
the projected source vectors to predict the senti-
ment of source phrases. This inevitably changes
the projection characteristics of the matrix M , and
consequently M (cid:48) and encourages M (cid:48) to learn to
predict sentiment without any training examples in
the target language.

To train M to predict sentiment, we re-
quire a source-language corpus Csource =
{(x1, y1), (x2, y2), . . . , (xi, yi)} where each sen-
tence xi is associated with a label yi.

For classiﬁcation, we use a two-layer feed-
forward averaging network, loosely following Iyyer
et al. (2015)3. For a sentence xi we take the word
embeddings from the source embedding S and av-
erage them to ai ∈ Rd. We then project this vector
to the joint bilingual space zi = ai · M . Finally,
we pass zi through a softmax layer P to get our
prediction ˆyi = softmax(zi · P ).

To train our model to predict sentiment, we min-

imize the cross-entropy error of our predictions

H = −

yi log ˆyi − (1 − yi) log(1 − ˆyi) . (3)

n
(cid:88)

i=1

3.3

Joint Learning

In order to jointly train both the projection com-
ponent and the sentiment component, we combine
the two loss functions to optimize the parameter

1We omit parameters in equations for better readability.
2We also experimented with cosine distance, but found

that it performed worse than Euclidean distance.

3Our model employs a linear transformation after the aver-
aging layer instead of including a non-linearity function. We
choose this architecture because the weights M and M (cid:48) are
also used to learn a linear cross-lingual projection.

Figure 1: Bilingual Sentiment Embedding Model (BLSE)

EN

ES

CA

EU

Spanish Catalan Basque

y
r
a
n
i
B

s
s
a
l
c
-
4

+ 1258
−
473
Total
1731

++
+
−
−−
Total

379
879
399
74
1731

1216
256
1472

370
846
218
38
1472

718
467
1185

256
462
409
58
1185

956
173
1129

384
572
153
20
1129

Table 1: Statistics for the OpeNER English (EN)
and Spanish (ES) as well as the MultiBooked Cata-
lan (CA) and Basque (EU) datasets.

matrices M , M (cid:48), and P by

(cid:88)

(cid:88)

J =

(x,y)∈Csource

(s,t)∈L

αH(x, y) + (1 − α) · MSE(s, t) ,

(4)
where α is a hyperparameter that weights sentiment
loss vs. projection loss.

3.4 Target-language Classiﬁcation

For inference, we classify sentences from a target-
language corpus Ctarget. As in the training proce-
dure, for each sentence, we take the word embed-
dings from the target embeddings T and average
them to ai ∈ Rd. We then project this vector to the
joint bilingual space ˆzi = ai · M (cid:48). Finally, we pass

Sentences
Tokens
Embeddings

23 M 9.6 M 0.7 M
610 M 183 M
25 M
0.83 M 0.4 M 0.14 M

Table 2: Statistics for the Wikipedia corpora and
monolingual vector spaces.

ˆzi through a softmax layer P to get our prediction
ˆyi = softmax(ˆzi · P ).

4 Datasets and Resources

4.1 OpeNER and MultiBooked

To evaluate our proposed model, we conduct ex-
periments using four benchmark datasets and three
bilingual combinations. We use the OpeNER En-
glish and Spanish datasets (Agerri et al., 2013)
and the MultiBooked Catalan and Basque datasets
(Barnes et al., 2018). All datasets contain hotel
reviews which are annotated for aspect-level senti-
ment analysis. The labels include Strong Negative
(−−), Negative (−), Positive (+), and Strong Pos-
itive (++). We map the aspect-level annotations
to sentence level by taking the most common label
and remove instances of mixed polarity. We also
create a binary setup by combining the strong and
weak classes. This gives us a total of six experi-
ments. The details of the sentence-level datasets
are summarized in Table 1. For each of the experi-

5 Experiments

5.1 Setting

We compare BLSE (Sections 3.1–3.3) to ARTETXE
(Section 2) and BARISTA (Section 2) as baselines,
which have similar data requirements and to ma-
chine translation (MT) and monolingual (MONO)
upper bounds which request more resources. For
all models (MONO, MT, ARTETXE, BARISTA),
we take the average of the word embeddings in
the source-language training examples and train a
linear SVM7. We report this instead of using the
same feed-forward network as in BLSE as it is the
stronger upper bound. We choose the parameter c
on the target language development set and evalu-
ate on the target language test set.

Upper Bound MONO. We set an empirical up-
per bound by training and testing a linear SVM
on the target language data. As mentioned in Sec-
tion 5.1, we train the model on the averaged em-
beddings from target language training data, tuning
the c parameter on the development data. We test
on the target language test data.

Upper Bound MT. To test the effectiveness of
machine translation, we translate all of the senti-
ment corpora from the target language to English
using the Google Translate API8. Note that this
approach is not considered a baseline, as we as-
sume not to have access to high-quality machine
translation for low-resource languages of interest.
Baseline ARTETXE. We compare with the ap-
proach proposed by Artetxe et al. (2016) which
has shown promise on other tasks, such as word
similarity. In order to learn the projection matrix
W , we need translation pairs. We use the same
word-to-word bilingual lexicon mentioned in Sec-
tion 3.1. We then map the source vector space
S to the bilingual space ˆS = SW and use these
embeddings.

Baseline BARISTA. We also compare with the
approach proposed by Gouws and Søgaard (2015).
The bilingual lexicon used to create the pseudo-
bilingual corpus is the same word-to-word bilin-
gual lexicon mentioned in Section 3.1. We follow
the authors’ setup to create the pseudo-bilingual
corpus. We create bilingual embeddings by train-
ing skip-gram embeddings using the Word2Vec
toolkit on the pseudo-bilingual corpus using the
same parameters from Section 4.2.

7LinearSVC implementation from scikit-learn.
8https://translate.google.com

Figure 2: Binary and four class macro F1 on Span-
ish (ES), Catalan (CA), and Basque (EU).

ments, we take 70 percent of the data for training,
20 percent for testing and the remaining 10 percent
are used as development data for tuning.

4.2 Monolingual Word Embeddings

For BLSE, ARTETXE, and MT, we require monolin-
gual vector spaces for each of our languages. For
English, we use the publicly available GoogleNews
vectors4. For Spanish, Catalan, and Basque, we
train skip-gram embeddings using the Word2Vec
toolkit4 with 300 dimensions, subsampling of 10−4,
window of 5, negative sampling of 15 based on a
2016 Wikipedia corpus5 (sentence-split, tokenized
with IXA pipes (Agerri et al., 2014) and lower-
cased). The statistics of the Wikipedia corpora are
given in Table 2.

4.3 Bilingual Lexicon

For BLSE, ARTETXE, and BARISTA, we also re-
quire a bilingual lexicon. We use the sentiment
lexicon from Hu and Liu (2004) (to which we refer
in the following as Bing Liu) and its translation
into each target language. We translate the lexicon
using Google Translate and exclude multi-word ex-
pressions.6 This leaves a dictionary of 5700 trans-
lations in Spanish, 5271 in Catalan, and 4577 in
Basque. We set aside ten percent of the translation
pairs as a development set in order to check that the
distances between translation pairs not seen during
training are also minimized during training.

4https://code.google.com/archive/p/word2vec/
5http://attardi.github.io/wikiextractor/
6Note that we only do that for convenience. Using a ma-
chine translation service to generate this list could easily be
replaced by a manual translation, as the lexicon is comparably
small.

Our method: BLSE. We implement our model
BLSE in Pytorch (Paszke et al., 2016) and initial-
ize the word embeddings with the pretrained word
embeddings S and T mentioned in Section 4.2.
We use the word-to-word bilingual lexicon from
Section 4.3, tune the hyperparameters α, training
epochs, and batch size on the target development
set and use the best hyperparameters achieved on
the development set for testing. ADAM (Kingma
and Ba, 2014) is used in order to minimize the
average loss of the training batches.

Binary

4-class

ES

CA

EU

ES CA EU

s
d
n
u
o
B

r
e
p
p
U

T
M

N
O
M

O P
R
F1
P
R
F1
E P
S
L
B

74.0
67.4
69.8
75.6
66.5
69.4

79.0
79.6
79.2
78.0
76.8
77.2

55.2 50.0 48.3
75.0
42.8 50.9 46.5
72.3
45.5 49.9 47.1
73.5
51.8 58.9 43.6
82.3
48.5 50.5 45.2
76.6
48.8 52.7 43.6
79.0
72.1 **72.8 **67.5 **60.0 38.1 *42.5
R **80.1 **73.0 **72.7 *43.4 38.1 37.4
F1 **74.6 **72.9 **69.3 *41.2 35.9 30.0

s
e
n
i
l
e
s
a
B

x
t
e
t
r

e P
R
F1

A

t
s
i
r
a
B

aP
R
F1

x
t
e
t
r

e P
R
F1

A

e
l
b
m
e
s
n
E

t
s
i
r
a
B

aP
R
F1

EP
R
F1

S
L
B

75.0
64.3
67.1

64.7
59.8
61.2

65.3
61.3
62.6

60.1
55.5
56.0

79.5
78.7
80.3

60.1
61.2
60.7

65.3
61.2
60.1

63.1
63.3
63.2

63.4
62.3
62.5

84.7
85.5
85.0

42.2
49.5
45.6

55.5
54.5
54.8

70.4
64.3
66.4

50.7
50.4
49.8

80.9
69.9
73.5

40.1 21.6 30.0
36.9 29.8 35.7
34.9 23.0 21.3

44.1 36.4 34.1
37.9 38.5 34.3
39.5 36.2 33.8

43.5 46.5 50.1
44.1 48.7 50.7
43.8 47.6 49.9

48.3 52.8 50.8
46.6 53.7 49.8
47.1 53.0 47.8

49.5 54.1 50.3
51.2 53.9 51.4
50.3 53.9 50.5

Table 3: Precision (P), Recall (R), and macro F1 of
four models trained on English and tested on Span-
ish (ES), Catalan (CA), and Basque (EU). The bold
numbers show the best results for each metric per
column and the highlighted numbers show where
BLSE is better than the other projection methods,
ARTETXE and BARISTA (** p < 0.01, * p < 0.05).

Ensembles We create an ensemble of MT
and each projection method (BLSE, ARTETXE,
BARISTA) by training a random forest classiﬁer
on the predictions from MT and each of these ap-
proaches. This allows us to evaluate to what extent
each projection model adds complementary infor-
mation to the machine translation approach.

5.2 Results

In Figure 2, we report the results of all four meth-
ods. Our method outperforms the other projection
methods (the baselines ARTETXE and BARISTA)
on four of the six experiments substantially. It per-
forms only slightly worse than the more resource-
costly upper bounds (MT and MONO). This is espe-
cially noticeable for the binary classiﬁcation task,
where BLSE performs nearly as well as machine
translation and signiﬁcantly better than the other
methods. We perform approximate randomization
tests (Yeh, 2000) with 10,000 runs and highlight
the results that are statistically signiﬁcant (**p <
0.01, *p < 0.05) in Table 3.

In more detail, we see that MT generally per-
forms better than the projection methods (79–69
F1 on binary, 52–44 on 4-class). BLSE (75–69
on binary, 41–30 on 4-class) has the best perfor-
mance of the projection methods and is comparable
with MT on the binary setup, with no signiﬁcant
difference on binary Basque. ARTETXE (67–46
on binary, 35–21 on 4-class) and BARISTA (61–
55 on binary, 40–34 on 4-class) are signiﬁcantly
worse than BLSE on all experiments except Cata-
lan and Basque 4-class. On the binary experiment,
ARTETXE outperforms BARISTA on Spanish (67.1
vs. 61.2) and Catalan (60.7 vs. 60.1) but suffers
more than the other methods on the four-class ex-
periments, with a maximum F1 of 34.9. BARISTA

Model

MT

ARTETXE

BARISTA

BLSE

c
o
v

49
147
80
182
89
191
67
146

d
o
m

26
94
44
141
41
109
45
125

g
e
n

19
19
27
19
27
24
21
29

bi
4
bi
4
bi
4
bi
4

w
o
n
k

14
21
14
24
20
31
15
22

r
e
h
t
o

5
12
7
19
7
15
8
19

l
a
t
o
t

113
293
172
385
184
370
156
341

Table 4: Error analysis for different phenomena.
See text for explanation of error classes.

This suggests that BLSE is better ARTETXE and
BARISTA at transferring sentiment of the most im-
portant sentiment bearing words.

Negation: Negation is a well-studied phe-
nomenon in sentiment analysis (Pang et al., 2002;
Wiegand et al., 2010; Zhu et al., 2014; Reitan et al.,
2015). Therefore, we are interested in how these
four models perform on phrases that include the
negation of a key element, for example “In general,
this hotel isn’t bad”. We would like our models
to recognize that the combination of two negative
elements “isn’t” and “bad” lead to a Positive label.
Given the simple classiﬁcation strategy, all mod-
els perform relatively well on phrases with negation
(all reach nearly 60 F1 in the binary setting). How-
ever, while BLSE performs the best on negation in
the binary setting (82.9 F1), it has more problems
with negation in the 4-class setting (36.9 F1).

Adverbial Modiﬁers: Phrases that are modiﬁed
by an adverb, e. g., the food was incredibly good,
are important for the four-class setup, as they often
differentiate between the base and Strong labels.
In the binary case, all models reach more than 55
F1. In the 4-class setup, BLSE only achieves 27.2
F1 compared to 46.6 or 31.3 of MT and BARISTA,
respectively. Therefore, presumably, our model
does currently not capture the semantics of the
target adverbs well. This is likely due to the fact
that it assigns too much sentiment to functional
words (see Figure 6).

External Knowledge Required: These errors
are difﬁcult for any of the models to get cor-
rect. Many of these include numbers which imply
positive or negative sentiment (350 meters from
the beach is Positive while 3 kilometers from the
beach is Negative). BLSE performs the best (63.5
F1) while MT performs comparably well (62.5).
BARISTA performs the worst (43.6).

Binary vs. 4-class: All of the models suffer
when moving from the binary to 4-class setting;
an average of 26.8 in macro F1 for MT, 31.4 for
ARTETXE, 22.2 for BARISTA, and for 36.6 BLSE.
The two vector projection methods (ARTETXE and
BLSE) suffer the most, suggesting that they are
currently more apt for the binary setting.

6.2 Effect of Bilingual Lexicon

We analyze how the number of translation pairs
affects our model. We train on the 4-class Span-
ish setup using the best hyper-parameters from the
previous experiment.

Figure 3: Macro F1 for translation pairs in the
Spanish 4-class setup.

is relatively stable across languages.

ENSEMBLE performs the best, which shows that
BLSE adds complementary information to MT. Fi-
nally, we note that all systems perform successively
worse on Catalan and Basque. This is presum-
ably due to the quality of the word embeddings, as
well as the increased morphological complexity of
Basque.

6 Model and Error Analysis

We analyze three aspects of our model in further
detail: (i) where most mistakes originate, (ii) the ef-
fect of the bilingual lexicon, and (iii) the effect and
necessity of the target-language projection matrix
M (cid:48).

6.1 Phenomena

In order to analyze where each model struggles, we
categorize the mistakes and annotate all of the test
phrases with one of the following error classes: vo-
cabulary (voc), adverbial modiﬁers (mod), negation
(neg), external knowledge (know) or other. Table 4
shows the results.

Vocabulary: The most common way to express
sentiment in hotel reviews is through the use of
polar adjectives (as in “the room was great) or the
mention of certain nouns that are desirable (“it
had a pool”). Although this phenomenon has the
largest total number of mistakes (an average of
71 per model on binary and 167 on 4-class), it is
mainly due to its prevalence. MT performed the
best on the test examples which according to the an-
notation require a correct understanding of the vo-
cabulary (81 F1 on binary /54 F1 on 4-class), with
BLSE (79/48) slightly worse. ARTETXE (70/35)
and BARISTA (67/41) perform signiﬁcantly worse.

Figure 4: Average cosine similarity between a subsample of translation pairs of same polarity (“sentiment
synonyms”) and of opposing polarity (“sentiment antonyms”) in both target and source languages in each
model. The x-axis shows training epochs. We see that BLSE is able to learn that sentiment synonyms
should be close to one another in vector space and sentiment antonyms should not.

Research into projection techniques for bilingual
word embeddings (Mikolov et al., 2013; Lazaridou
et al., 2015; Artetxe et al., 2016) often uses a lex-
icon of the most frequent 8–10 thousand words
in English and their translations as training data.
We test this approach by taking the 10,000 word-
to-word translations from the Apertium English-
to-Spanish dictionary9. We also use the Google
Translate API to translate the NRC hashtag senti-
ment lexicon (Mohammad et al., 2013) and keep
the 22,984 word-to-word translations. We perform
the same experiment as above and vary the amount
of training data from 0, 100, 300, 600, 1000, 3000,
6000, 10,000 up to 20,000 training pairs. Finally,
we compile a small hand translated dictionary of
200 pairs, which we then expand using target lan-
guage morphological information, ﬁnally giving
us 657 translation pairs10. The macro F1 score for
the Bing Liu dictionary climbs constantly with the
increasing translation pairs. Both the Apertium
and NRC dictionaries perform worse than the trans-
lated lexicon by Bing Liu, while the expanded hand
translated dictionary is competitive, as shown in
Figure 3.

While for some tasks, e. g., bilingual lexicon
induction, using the most frequent words as trans-
lation pairs is an effective approach, for sentiment
analysis, this does not seem to help. Using a trans-
lated sentiment lexicon, even if it is small, gives
better results.

9http://www.meta-share.org
10The translation took approximately one hour. We can
extrapolate that hand translating a sentiment lexicon the size
of the Bing Liu lexicon would take no more than 5 hours.

Figure 5: BLSE model (solid lines) compared to a
variant without target language projection matrix
M (cid:48) (dashed lines). “Translation” lines show the
average cosine similarity between translation pairs.
The remaining lines show F1 scores for the source
and target language with both varints of BLSE. The
modiﬁed model cannot learn to predict sentiment
in the target language (red lines). This illustrates
the need for the second projection matrix M (cid:48).

6.3 Analysis of M (cid:48)

The main motivation for using two projection ma-
trices M and M (cid:48) is to allow the original embed-
dings to remain stable, while the projection ma-
trices have the ﬂexibility to align translations and
separate these into distinct sentiment subspaces. To
justify this design decision empirically, we perform
an experiment to evaluate the actual need for the
target language projection matrix M (cid:48): We create a
simpliﬁed version of our model without M (cid:48), using
M to project from the source to target and then P
to classify sentiment.

The results of this model are shown in Figure 5.
The modiﬁed model does learn to predict in the
source language, but not in the target language.
This conﬁrms that M (cid:48) is necessary to transfer sen-
timent in our model.

7 Qualitative Analyses of Joint Bilingual

Sentiment Space

In order to understand how well our model trans-
fers sentiment information to the target language,
we perform two qualitative analyses. First, we
collect two sets of 100 positive sentiment words
and one set of 100 negative sentiment words. An
effective cross-lingual sentiment classiﬁer using
embeddings should learn that two positive words
should be closer in the shared bilingual space than a
positive word and a negative word. We test if BLSE
is able to do this by training our model and after
every epoch observing the mean cosine similarity
between the sentiment synonyms and sentiment
antonyms after projecting to the joint space.

We compare BLSE with ARTETXE and BARISTA
by replacing the Linear SVM classiﬁers with the
same multi-layer classiﬁer used in BLSE and ob-
serving the distances in the hidden layer. Figure 4
shows this similarity in both source and target lan-
guage, along with the mean cosine similarity be-
tween a held-out set of translation pairs and the
macro F1 scores on the development set for both
source and target languages for BLSE, BARISTA,
and ARTETXE. From this plot, it is clear that BLSE
is able to learn that sentiment synonyms should be
close to one another in vector space and antonyms
should have a negative cosine similarity. While
the other models also learn this to some degree,
jointly optimizing both sentiment and projection
gives better results.

Secondly, we would like to know how well the
projected vectors compare to the original space.
Our hypothesis is that some relatedness and simi-
larity information is lost during projection. There-
fore, we visualize six categories of words in t-SNE
(Van der Maaten and Hinton, 2008): positive senti-
ment words, negative sentiment words, functional
words, verbs, animals, and transport.

The t-SNE plots in Figure 6 show that the posi-
tive and negative sentiment words are rather clearly
separated after projection in BLSE. This indicates
that we are able to incorporate sentiment informa-
tion into our target language without any labeled
data in the target language. However, the downside

Figure 6: t-SNE-based visualization of the Spanish
vector space before and after projection with BLSE.
There is a clear separation of positive and negative
words after projection, despite the fact that we have
used no labeled data in Spanish.

of this is that functional words and transportation
words are highly correlated with positive sentiment.

8 Conclusion

We have presented a new model, BLSE, which
is able to leverage sentiment information from a
resource-rich language to perform sentiment analy-
sis on a resource-poor target language. This model
requires less parallel data than MT and performs
better than other state-of-the-art methods with sim-
ilar data requirements, an average of 14 percentage
points in F1 on binary and 4 pp on 4-class cross-
lingual sentiment analysis. We have also performed
a phenomena-driven error analysis which showed
that BLSE is better than ARTETXE and BARISTA
at transferring sentiment, but assigns too much sen-
timent to functional words. In the future, we will
extend our model so that it can project multi-word
phrases, as well as single words, which could help
with negations and modiﬁers.

Acknowledgements

We thank Sebastian Pad´o, Sebastian Riedel, Eneko
Agirre, and Mikel Artetxe for their conversations
and feedback.

References

Rodrigo Agerri, Josu Bermudez, and German Rigau.
2014. Ixa pipeline: Efﬁcient and ready to use mul-
tilingual nlp tools. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14). pages 3823–3828.

Rodrigo Agerri, Montse Cuadros, Sean Gaines, and
German Rigau. 2013. OpeNER: Open polarity
Sociedad
enhanced named entity recognition.
Espa˜nola para el Procesamiento del Lenguaje Nat-
ural 51(Septiembre):215–218.

Mariana S. C. Almeida, Claudia Pinto, Helena Figueira,
Pedro Mendes, and Andr´e F. T. Martins. 2015.
Aligning opinions: Cross-lingual opinion mining
with dependencies. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers). pages 408–418.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing. pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). pages 451–462.

Alexandra Balahur and Marco Turchi. 2014. Compar-
ative experiments using supervised learning and ma-
chine translation for multilingual sentiment analysis.
Computer Speech & Language 28(1):56–75.

Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more lan-
In Proceedings of the 23rd Inter-
guages better?
national Conference on Computational Linguistics
(Coling 2010). pages 28–36.

Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity anal-
In Proceedings of
ysis using machine translation.
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing. pages 127–135.

Jeremy Barnes, Patrik Lambert, and Toni Badia. 2018.
Multibooked: A corpus of basque and catalan hotel
reviews annotated for aspect-level sentiment classiﬁ-
cation. In Proceedings of 11th Language Resources
and Evaluation Conference (LREC’18).

Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, Cur-
ran Associates, Inc., pages 1853–1861.

Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Q.
Adver-
Weinberger, and Claire Cardie. 2016.
sarial deep averaging networks for cross-lingual
CoRR abs/1606.01614.
sentiment classiﬁcation.
http://arxiv.org/abs/1606.01614.

Erkin Demirtas and Mykola Pechenizkiy. 2013. Cross-
lingual polarity detection with machine translation.
Proceedings of the International Workshop on Issues
of Sentiment Discovery and Opinion Mining - WIS-
DOM ’13 pages 9:1–9:8.

Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for cross-lingual senti-
ment classiﬁcation? Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers 2:429–433.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. BilBOWA: Fast bilingual distributed repre-
sentations without word alignments. Proceedings
of The 32nd International Conference on Machine
Learning pages 748–756.

Stephan Gouws and Anders Søgaard. 2015. Simple
In Pro-
task-speciﬁc bilingual word embeddings.
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
1386–1390.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 58–
68.

Minqing Hu and Bing Liu. 2004. Mining opinion
In Proceedings of
features in customer reviews.
the 10th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2004). pages 168–177.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daume III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classiﬁcation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). Beijing,
China, pages 1681–1691.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR) .

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: delving into
cross-space mapping for zero-shot learning. Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing pages 270–280.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. pages 142–150.

Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-lingual
mixture model for sentiment classiﬁcation. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Jeju Island, Korea, pages 572–581.
http://www.aclweb.org/anthology/P12-1060.

Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics. pages 976–983.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9:2579–2605.

Xiaojun Wan. 2009. Co-training for cross-lingual sen-
In Proceedings of the Joint
timent classiﬁcation.
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP. pages 235–
243.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing. pages 60–
68.

Min Xiao and Yuhong Guo. 2012. Multi-view ad-
aboost for multilingual subjectivity analysis. In Pro-
ceedings of COLING 2012. pages 2851–2866.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
Exploiting similarities among languages
CoRR abs/1309.4168.

2013.
for machine translation.
http://arxiv.org/abs/1309.4168.

Alexander Yeh. 2000. More accurate tests for the statis-
tical signiﬁcance of result differences. In Proceed-
ings of the 18th Conference on Computational lin-
guistics (COLING). pages 947–953.

Guangyou Zhou, Zhiyuan Zhu, Tingting He, and Xiao-
hua Tony Hu. 2016. Cross-lingual sentiment classi-
ﬁcation with stacked autoencoders. Knowledge and
Information Systems 47(1):27–44.

HuiWei Zhou, Long Chen, Fulin Shi, and Degen
Huang. 2015. Learning bilingual sentiment word
embeddings for cross-language sentiment classiﬁ-
In Proceedings of the 53rd Annual Meet-
cation.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
pages 430–440.

Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svet-
lana Kiritchenko. 2014. An empirical study on the
effect of negation words on sentiment. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). pages 304–313.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of the seventh international workshop on Se-
mantic Evaluation Exercises (SemEval-2013).

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classiﬁcation using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical methods in natu-
ral language processing-Volume 10. Association for
Computational Linguistics, pages 79–86.

Adam Paszke, Sam Gross, Soumith Chintala, and Gre-
gory Chanan. 2016. Pytorch deeplearning frame-
work. http://pytorch.org. Accessed: 2017-08-10.

Peter Prettenhofer and Benno Stein. 2011. Cross-
lingual adaptation using structural correspondence
learning. ACM Transactions on Intelligent Systems
and Technology 3(1):1–22.

Mohammad Sadegh Rasooli, Noura Farra, Axinia
Radeva, Tao Yu, and Kathleen McKeown. 2017.
Cross-lingual sentiment
transfer with limited re-
sources. Machine Translation .

Johan Reitan, Jørgen Faret, Bj¨orn Gamb¨ack, and Lars
Bungum. 2015. Negation scope detection for twitter
sentiment analysis. In Proceedings of the 6th Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis. pages 99–108.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin. 2014. Learning sentiment-speciﬁc
word embedding for twitter sentiment classiﬁcation.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). pages 1555–1565.

Bilingual Sentiment Embeddings:
Joint Projection of Sentiment Across Languages

Jeremy Barnes, Roman Klinger, and Sabine Schulte im Walde
Institut f¨ur Maschinelle Sprachverarbeitung
University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{barnesjy,klinger,schulte}@ims.uni-stuttgart.de

8
1
0
2
 
y
a
M
 
3
2
 
 
]
L
C
.
s
c
[
 
 
1
v
6
1
0
9
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Sentiment analysis in low-resource lan-
guages suffers from a lack of annotated
corpora to estimate high-performing mod-
els. Machine translation and bilingual word
embeddings provide some relief through
cross-lingual sentiment approaches. How-
ever, they either require large amounts of
parallel data or do not sufﬁciently capture
sentiment information. We introduce Bilin-
gual Sentiment Embeddings (BLSE), which
jointly represent sentiment information in a
source and target language. This model
only requires a small bilingual lexicon,
a source-language corpus annotated for
sentiment, and monolingual word embed-
dings for each language. We perform ex-
periments on three language combinations
(Spanish, Catalan, Basque) for sentence-
level cross-lingual sentiment classiﬁcation
and ﬁnd that our model signiﬁcantly out-
performs state-of-the-art methods on four
out of six experimental setups, as well as
capturing complementary information to
machine translation. Our analysis of the re-
sulting embedding space provides evidence
that it represents sentiment information in
the resource-poor target language without
any annotated data in that language.

1

Introduction

Cross-lingual approaches to sentiment analysis are
motivated by the lack of training data in the vast
majority of languages. Even languages spoken
by several million people, such as Catalan, often
have few resources available to perform sentiment
analysis in speciﬁc domains. We therefore aim
to harness the knowledge previously collected in
resource-rich languages.

Previous approaches for cross-lingual sentiment
analysis typically exploit machine translation based
methods or multilingual models. Machine trans-
lation (MT) can provide a way to transfer senti-
ment information from a resource-rich to resource-
poor languages (Mihalcea et al., 2007; Balahur and
Turchi, 2014). However, MT-based methods re-
quire large parallel corpora to train the translation
system, which are often not available for under-
resourced languages.

Examples of multilingual methods that have
been applied to cross-lingual sentiment analysis
include domain adaptation methods (Prettenhofer
and Stein, 2011), delexicalization (Almeida et al.,
2015), and bilingual word embeddings (Mikolov
et al., 2013; Hermann and Blunsom, 2014; Artetxe
et al., 2016). These approaches however do not in-
corporate enough sentiment information to perform
well cross-lingually, as we will show later.

We propose a novel approach to incorporate sen-
timent information in a model, which does not have
these disadvantages. Bilingual Sentiment Embed-
dings (BLSE) are embeddings that are jointly opti-
mized to represent both (a) semantic information in
the source and target languages, which are bound
to each other through a small bilingual dictionary,
and (b) sentiment information, which is annotated
on the source language only. We only need three
resources: (i) a comparably small bilingual lexicon,
(ii) an annotated sentiment corpus in the resource-
rich language, and (iii) monolingual word embed-
dings for the two involved languages.

We show that our model outperforms previous
state-of-the-art models in nearly all experimental
settings across six benchmarks. In addition, we
offer an in-depth analysis and demonstrate that our
model is aware of sentiment. Finally, we provide a
qualitative analysis of the joint bilingual sentiment
space. Our implementation is publicly available at
https://github.com/jbarnesspain/blse.

2 Related Work

Machine Translation: Early work in cross-lingual
sentiment analysis found that machine translation
(MT) had reached a point of maturity that enabled
the transfer of sentiment across languages. Re-
searchers translated sentiment lexicons (Mihalcea
et al., 2007; Meng et al., 2012) or annotated corpora
and used word alignments to project sentiment an-
notation and create target-language annotated cor-
pora (Banea et al., 2008; Duh et al., 2011; Demirtas
and Pechenizkiy, 2013; Balahur and Turchi, 2014).
Several approaches included a multi-view repre-
sentation of the data (Banea et al., 2010; Xiao and
Guo, 2012) or co-training (Wan, 2009; Demirtas
and Pechenizkiy, 2013) to improve over a naive
implementation of machine translation, where only
the translated data is used. There are also ap-
proaches which only require parallel data (Meng
et al., 2012; Zhou et al., 2016; Rasooli et al., 2017),
instead of machine translation.

All of these approaches, however, require large
amounts of parallel data or an existing high qual-
ity translation tool, which are not always available.
A notable exception is the approach proposed by
Chen et al. (2016), an adversarial deep averaging
network, which trains a joint feature extractor for
two languages. They minimize the difference be-
tween these features across languages by learning
to fool a language discriminator, which requires
no parallel data. It does, however, require large
amounts of unlabeled data.

Bilingual Embedding Methods: Recently pro-
posed bilingual embedding methods (Hermann and
Blunsom, 2014; Chandar et al., 2014; Gouws et al.,
2015) offer a natural way to bridge the language
gap. These particular approaches to bilingual em-
beddings, however, require large parallel corpora
in order to build the bilingual space, which are not
available for all language combinations.

An approach to create bilingual embeddings that
has a less prohibitive data requirement is to create
monolingual vector spaces and then learn a projec-
tion from one to the other. Mikolov et al. (2013)
ﬁnd that vector spaces in different languages have
similar arrangements. Therefore, they propose a
linear projection which consists of learning a rota-
tion and scaling matrix. Artetxe et al. (2016, 2017)
improve upon this approach by requiring the pro-
jection to be orthogonal, thereby preserving the
monolingual quality of the original word vectors.

Given source embeddings S,

target embed-
dings T , and a bilingual lexicon L, Artetxe et al.
(2016) learn a projection matrix W by minimizing
the square of Euclidean distances

arg min
W

(cid:88)

i

||S(cid:48)W − T (cid:48)||2

F ,

(1)

where S(cid:48) ∈ S and T (cid:48) ∈ T are the word embedding
matrices for the tokens in the bilingual lexicon L.
This is solved using the Moore-Penrose pseudoin-
verse S(cid:48)+ = (S(cid:48)T S(cid:48))−1S(cid:48)T as W = S(cid:48)+T (cid:48), which
can be computed using SVD. We refer to this ap-
proach as ARTETXE.

Gouws and Søgaard (2015) propose a method to
create a pseudo-bilingual corpus with a small task-
speciﬁc bilingual lexicon, which can then be used
to train bilingual embeddings (BARISTA). This
approach requires a monolingual corpus in both
the source and target languages and a set of trans-
lation pairs. The source and target corpora are
concatenated and then every word is randomly kept
or replaced by its translation with a probability of
0.5. Any kind of word embedding algorithm can be
trained with this pseudo-bilingual corpus to create
bilingual word embeddings.

These last techniques have the advantage of re-
quiring relatively little parallel training data while
taking advantage of larger amounts of monolingual
data. However, they are not optimized for senti-
ment.

Sentiment Embeddings: Maas et al. (2011) ﬁrst
explored the idea of incorporating sentiment in-
formation into semantic word vectors. They pro-
posed a topic modeling approach similar to latent
Dirichlet allocation in order to collect the semantic
information in their word vectors. To incorporate
the sentiment information, they included a second
objective whereby they maximize the probability
of the sentiment label for each word in a labeled
document.

Tang et al. (2014) exploit distantly annotated
tweets to create Twitter sentiment embeddings. To
incorporate distributional information about tokens,
they use a hinge loss and maximize the likelihood
of a true n-gram over a corrupted n-gram. They
include a second objective where they classify the
polarity of the tweet given the true n-gram. While
these techniques have proven useful, they are not
easily transferred to a cross-lingual setting.

Zhou et al. (2015) create bilingual sentiment
embeddings by translating all source data to the

target language and vice versa. This requires the
existence of a machine translation system, which is
a prohibitive assumption for many under-resourced
languages, especially if it must be open and freely
accessible. This motivates approaches which can
use smaller amounts of parallel data to achieve
similar results.

3 Model

In order to project not only semantic similarity and
relatedness but also sentiment information to our
target language, we propose a new model, namely
Bilingual Sentiment Embeddings (BLSE), which
jointly learns to predict sentiment and to minimize
the distance between translation pairs in vector
space. We detail the projection objective in Sec-
tion 3.1, the sentiment objective in Section 3.2, and
the full objective in Section 3.3. A sketch of the
model is depicted in Figure 1.

3.1 Cross-lingual Projection

We assume that we have two precomputed vector
spaces S = Rv×d and T = Rv(cid:48)×d(cid:48)
for our source
and target languages, where v (v(cid:48)) is the length of
the source vocabulary (target vocabulary) and d
(d(cid:48)) is the dimensionality of the embeddings. We
also assume that we have a bilingual lexicon L
of length n which consists of word-to-word trans-
lation pairs L = {(s1, t1), (s2, t2), . . . , (sn, tn)}
which map from source to target.

In order to create a mapping from both origi-
nal vector spaces S and T to shared sentiment-
informed bilingual spaces z and ˆz, we employ two
linear projection matrices, M and M (cid:48). During
training, for each translation pair in L, we ﬁrst look
up their associated vectors, project them through
their associated projection matrix and ﬁnally mini-
mize the mean squared error of the two projected
vectors. This is very similar to the approach taken
by Mikolov et al. (2013), but includes an additional
target projection matrix.

The intuition for including this second matrix is
that a single projection matrix does not support the
transfer of sentiment information from the source
language to the target language. Without M (cid:48), any
signal coming from the sentiment classiﬁer (see
Section 3.2) would have no affect on the target
embedding space T , and optimizing M to predict
sentiment and projection would only be detrimental
to classiﬁcation of the target language. We analyze
this further in Section 6.3. Note that in this con-

ﬁguration, we do not need to update the original
vector spaces, which would be problematic with
such small training data.

The projection quality is ensured by minimizing

the mean squared error12

MSE =

(zi − ˆzi)2 ,

(2)

1
n

n
(cid:88)

i=1

where zi = Ssi · M is the dot product of the embed-
ding for source word si and the source projection
matrix and ˆzi = Tti · M (cid:48) is the same for the target
word ti.

3.2 Sentiment Classiﬁcation

We add a second training objective to optimize
the projected source vectors to predict the senti-
ment of source phrases. This inevitably changes
the projection characteristics of the matrix M , and
consequently M (cid:48) and encourages M (cid:48) to learn to
predict sentiment without any training examples in
the target language.

To train M to predict sentiment, we re-
quire a source-language corpus Csource =
{(x1, y1), (x2, y2), . . . , (xi, yi)} where each sen-
tence xi is associated with a label yi.

For classiﬁcation, we use a two-layer feed-
forward averaging network, loosely following Iyyer
et al. (2015)3. For a sentence xi we take the word
embeddings from the source embedding S and av-
erage them to ai ∈ Rd. We then project this vector
to the joint bilingual space zi = ai · M . Finally,
we pass zi through a softmax layer P to get our
prediction ˆyi = softmax(zi · P ).

To train our model to predict sentiment, we min-

imize the cross-entropy error of our predictions

H = −

yi log ˆyi − (1 − yi) log(1 − ˆyi) . (3)

n
(cid:88)

i=1

3.3

Joint Learning

In order to jointly train both the projection com-
ponent and the sentiment component, we combine
the two loss functions to optimize the parameter

1We omit parameters in equations for better readability.
2We also experimented with cosine distance, but found

that it performed worse than Euclidean distance.

3Our model employs a linear transformation after the aver-
aging layer instead of including a non-linearity function. We
choose this architecture because the weights M and M (cid:48) are
also used to learn a linear cross-lingual projection.

Figure 1: Bilingual Sentiment Embedding Model (BLSE)

EN

ES

CA

EU

Spanish Catalan Basque

y
r
a
n
i
B

s
s
a
l
c
-
4

+ 1258
−
473
Total
1731

++
+
−
−−
Total

379
879
399
74
1731

1216
256
1472

370
846
218
38
1472

718
467
1185

256
462
409
58
1185

956
173
1129

384
572
153
20
1129

Table 1: Statistics for the OpeNER English (EN)
and Spanish (ES) as well as the MultiBooked Cata-
lan (CA) and Basque (EU) datasets.

matrices M , M (cid:48), and P by

(cid:88)

(cid:88)

J =

(x,y)∈Csource

(s,t)∈L

αH(x, y) + (1 − α) · MSE(s, t) ,

(4)
where α is a hyperparameter that weights sentiment
loss vs. projection loss.

3.4 Target-language Classiﬁcation

For inference, we classify sentences from a target-
language corpus Ctarget. As in the training proce-
dure, for each sentence, we take the word embed-
dings from the target embeddings T and average
them to ai ∈ Rd. We then project this vector to the
joint bilingual space ˆzi = ai · M (cid:48). Finally, we pass

Sentences
Tokens
Embeddings

23 M 9.6 M 0.7 M
610 M 183 M
25 M
0.83 M 0.4 M 0.14 M

Table 2: Statistics for the Wikipedia corpora and
monolingual vector spaces.

ˆzi through a softmax layer P to get our prediction
ˆyi = softmax(ˆzi · P ).

4 Datasets and Resources

4.1 OpeNER and MultiBooked

To evaluate our proposed model, we conduct ex-
periments using four benchmark datasets and three
bilingual combinations. We use the OpeNER En-
glish and Spanish datasets (Agerri et al., 2013)
and the MultiBooked Catalan and Basque datasets
(Barnes et al., 2018). All datasets contain hotel
reviews which are annotated for aspect-level senti-
ment analysis. The labels include Strong Negative
(−−), Negative (−), Positive (+), and Strong Pos-
itive (++). We map the aspect-level annotations
to sentence level by taking the most common label
and remove instances of mixed polarity. We also
create a binary setup by combining the strong and
weak classes. This gives us a total of six experi-
ments. The details of the sentence-level datasets
are summarized in Table 1. For each of the experi-

5 Experiments

5.1 Setting

We compare BLSE (Sections 3.1–3.3) to ARTETXE
(Section 2) and BARISTA (Section 2) as baselines,
which have similar data requirements and to ma-
chine translation (MT) and monolingual (MONO)
upper bounds which request more resources. For
all models (MONO, MT, ARTETXE, BARISTA),
we take the average of the word embeddings in
the source-language training examples and train a
linear SVM7. We report this instead of using the
same feed-forward network as in BLSE as it is the
stronger upper bound. We choose the parameter c
on the target language development set and evalu-
ate on the target language test set.

Upper Bound MONO. We set an empirical up-
per bound by training and testing a linear SVM
on the target language data. As mentioned in Sec-
tion 5.1, we train the model on the averaged em-
beddings from target language training data, tuning
the c parameter on the development data. We test
on the target language test data.

Upper Bound MT. To test the effectiveness of
machine translation, we translate all of the senti-
ment corpora from the target language to English
using the Google Translate API8. Note that this
approach is not considered a baseline, as we as-
sume not to have access to high-quality machine
translation for low-resource languages of interest.
Baseline ARTETXE. We compare with the ap-
proach proposed by Artetxe et al. (2016) which
has shown promise on other tasks, such as word
similarity. In order to learn the projection matrix
W , we need translation pairs. We use the same
word-to-word bilingual lexicon mentioned in Sec-
tion 3.1. We then map the source vector space
S to the bilingual space ˆS = SW and use these
embeddings.

Baseline BARISTA. We also compare with the
approach proposed by Gouws and Søgaard (2015).
The bilingual lexicon used to create the pseudo-
bilingual corpus is the same word-to-word bilin-
gual lexicon mentioned in Section 3.1. We follow
the authors’ setup to create the pseudo-bilingual
corpus. We create bilingual embeddings by train-
ing skip-gram embeddings using the Word2Vec
toolkit on the pseudo-bilingual corpus using the
same parameters from Section 4.2.

7LinearSVC implementation from scikit-learn.
8https://translate.google.com

Figure 2: Binary and four class macro F1 on Span-
ish (ES), Catalan (CA), and Basque (EU).

ments, we take 70 percent of the data for training,
20 percent for testing and the remaining 10 percent
are used as development data for tuning.

4.2 Monolingual Word Embeddings

For BLSE, ARTETXE, and MT, we require monolin-
gual vector spaces for each of our languages. For
English, we use the publicly available GoogleNews
vectors4. For Spanish, Catalan, and Basque, we
train skip-gram embeddings using the Word2Vec
toolkit4 with 300 dimensions, subsampling of 10−4,
window of 5, negative sampling of 15 based on a
2016 Wikipedia corpus5 (sentence-split, tokenized
with IXA pipes (Agerri et al., 2014) and lower-
cased). The statistics of the Wikipedia corpora are
given in Table 2.

4.3 Bilingual Lexicon

For BLSE, ARTETXE, and BARISTA, we also re-
quire a bilingual lexicon. We use the sentiment
lexicon from Hu and Liu (2004) (to which we refer
in the following as Bing Liu) and its translation
into each target language. We translate the lexicon
using Google Translate and exclude multi-word ex-
pressions.6 This leaves a dictionary of 5700 trans-
lations in Spanish, 5271 in Catalan, and 4577 in
Basque. We set aside ten percent of the translation
pairs as a development set in order to check that the
distances between translation pairs not seen during
training are also minimized during training.

4https://code.google.com/archive/p/word2vec/
5http://attardi.github.io/wikiextractor/
6Note that we only do that for convenience. Using a ma-
chine translation service to generate this list could easily be
replaced by a manual translation, as the lexicon is comparably
small.

Our method: BLSE. We implement our model
BLSE in Pytorch (Paszke et al., 2016) and initial-
ize the word embeddings with the pretrained word
embeddings S and T mentioned in Section 4.2.
We use the word-to-word bilingual lexicon from
Section 4.3, tune the hyperparameters α, training
epochs, and batch size on the target development
set and use the best hyperparameters achieved on
the development set for testing. ADAM (Kingma
and Ba, 2014) is used in order to minimize the
average loss of the training batches.

Binary

4-class

ES

CA

EU

ES CA EU

s
d
n
u
o
B

r
e
p
p
U

T
M

N
O
M

O P
R
F1
P
R
F1
E P
S
L
B

74.0
67.4
69.8
75.6
66.5
69.4

79.0
79.6
79.2
78.0
76.8
77.2

55.2 50.0 48.3
75.0
42.8 50.9 46.5
72.3
45.5 49.9 47.1
73.5
51.8 58.9 43.6
82.3
48.5 50.5 45.2
76.6
48.8 52.7 43.6
79.0
72.1 **72.8 **67.5 **60.0 38.1 *42.5
R **80.1 **73.0 **72.7 *43.4 38.1 37.4
F1 **74.6 **72.9 **69.3 *41.2 35.9 30.0

s
e
n
i
l
e
s
a
B

x
t
e
t
r

e P
R
F1

A

t
s
i
r
a
B

aP
R
F1

x
t
e
t
r

e P
R
F1

A

e
l
b
m
e
s
n
E

t
s
i
r
a
B

aP
R
F1

EP
R
F1

S
L
B

75.0
64.3
67.1

64.7
59.8
61.2

65.3
61.3
62.6

60.1
55.5
56.0

79.5
78.7
80.3

60.1
61.2
60.7

65.3
61.2
60.1

63.1
63.3
63.2

63.4
62.3
62.5

84.7
85.5
85.0

42.2
49.5
45.6

55.5
54.5
54.8

70.4
64.3
66.4

50.7
50.4
49.8

80.9
69.9
73.5

40.1 21.6 30.0
36.9 29.8 35.7
34.9 23.0 21.3

44.1 36.4 34.1
37.9 38.5 34.3
39.5 36.2 33.8

43.5 46.5 50.1
44.1 48.7 50.7
43.8 47.6 49.9

48.3 52.8 50.8
46.6 53.7 49.8
47.1 53.0 47.8

49.5 54.1 50.3
51.2 53.9 51.4
50.3 53.9 50.5

Table 3: Precision (P), Recall (R), and macro F1 of
four models trained on English and tested on Span-
ish (ES), Catalan (CA), and Basque (EU). The bold
numbers show the best results for each metric per
column and the highlighted numbers show where
BLSE is better than the other projection methods,
ARTETXE and BARISTA (** p < 0.01, * p < 0.05).

Ensembles We create an ensemble of MT
and each projection method (BLSE, ARTETXE,
BARISTA) by training a random forest classiﬁer
on the predictions from MT and each of these ap-
proaches. This allows us to evaluate to what extent
each projection model adds complementary infor-
mation to the machine translation approach.

5.2 Results

In Figure 2, we report the results of all four meth-
ods. Our method outperforms the other projection
methods (the baselines ARTETXE and BARISTA)
on four of the six experiments substantially. It per-
forms only slightly worse than the more resource-
costly upper bounds (MT and MONO). This is espe-
cially noticeable for the binary classiﬁcation task,
where BLSE performs nearly as well as machine
translation and signiﬁcantly better than the other
methods. We perform approximate randomization
tests (Yeh, 2000) with 10,000 runs and highlight
the results that are statistically signiﬁcant (**p <
0.01, *p < 0.05) in Table 3.

In more detail, we see that MT generally per-
forms better than the projection methods (79–69
F1 on binary, 52–44 on 4-class). BLSE (75–69
on binary, 41–30 on 4-class) has the best perfor-
mance of the projection methods and is comparable
with MT on the binary setup, with no signiﬁcant
difference on binary Basque. ARTETXE (67–46
on binary, 35–21 on 4-class) and BARISTA (61–
55 on binary, 40–34 on 4-class) are signiﬁcantly
worse than BLSE on all experiments except Cata-
lan and Basque 4-class. On the binary experiment,
ARTETXE outperforms BARISTA on Spanish (67.1
vs. 61.2) and Catalan (60.7 vs. 60.1) but suffers
more than the other methods on the four-class ex-
periments, with a maximum F1 of 34.9. BARISTA

Model

MT

ARTETXE

BARISTA

BLSE

c
o
v

49
147
80
182
89
191
67
146

d
o
m

26
94
44
141
41
109
45
125

g
e
n

19
19
27
19
27
24
21
29

bi
4
bi
4
bi
4
bi
4

w
o
n
k

14
21
14
24
20
31
15
22

r
e
h
t
o

5
12
7
19
7
15
8
19

l
a
t
o
t

113
293
172
385
184
370
156
341

Table 4: Error analysis for different phenomena.
See text for explanation of error classes.

This suggests that BLSE is better ARTETXE and
BARISTA at transferring sentiment of the most im-
portant sentiment bearing words.

Negation: Negation is a well-studied phe-
nomenon in sentiment analysis (Pang et al., 2002;
Wiegand et al., 2010; Zhu et al., 2014; Reitan et al.,
2015). Therefore, we are interested in how these
four models perform on phrases that include the
negation of a key element, for example “In general,
this hotel isn’t bad”. We would like our models
to recognize that the combination of two negative
elements “isn’t” and “bad” lead to a Positive label.
Given the simple classiﬁcation strategy, all mod-
els perform relatively well on phrases with negation
(all reach nearly 60 F1 in the binary setting). How-
ever, while BLSE performs the best on negation in
the binary setting (82.9 F1), it has more problems
with negation in the 4-class setting (36.9 F1).

Adverbial Modiﬁers: Phrases that are modiﬁed
by an adverb, e. g., the food was incredibly good,
are important for the four-class setup, as they often
differentiate between the base and Strong labels.
In the binary case, all models reach more than 55
F1. In the 4-class setup, BLSE only achieves 27.2
F1 compared to 46.6 or 31.3 of MT and BARISTA,
respectively. Therefore, presumably, our model
does currently not capture the semantics of the
target adverbs well. This is likely due to the fact
that it assigns too much sentiment to functional
words (see Figure 6).

External Knowledge Required: These errors
are difﬁcult for any of the models to get cor-
rect. Many of these include numbers which imply
positive or negative sentiment (350 meters from
the beach is Positive while 3 kilometers from the
beach is Negative). BLSE performs the best (63.5
F1) while MT performs comparably well (62.5).
BARISTA performs the worst (43.6).

Binary vs. 4-class: All of the models suffer
when moving from the binary to 4-class setting;
an average of 26.8 in macro F1 for MT, 31.4 for
ARTETXE, 22.2 for BARISTA, and for 36.6 BLSE.
The two vector projection methods (ARTETXE and
BLSE) suffer the most, suggesting that they are
currently more apt for the binary setting.

6.2 Effect of Bilingual Lexicon

We analyze how the number of translation pairs
affects our model. We train on the 4-class Span-
ish setup using the best hyper-parameters from the
previous experiment.

Figure 3: Macro F1 for translation pairs in the
Spanish 4-class setup.

is relatively stable across languages.

ENSEMBLE performs the best, which shows that
BLSE adds complementary information to MT. Fi-
nally, we note that all systems perform successively
worse on Catalan and Basque. This is presum-
ably due to the quality of the word embeddings, as
well as the increased morphological complexity of
Basque.

6 Model and Error Analysis

We analyze three aspects of our model in further
detail: (i) where most mistakes originate, (ii) the ef-
fect of the bilingual lexicon, and (iii) the effect and
necessity of the target-language projection matrix
M (cid:48).

6.1 Phenomena

In order to analyze where each model struggles, we
categorize the mistakes and annotate all of the test
phrases with one of the following error classes: vo-
cabulary (voc), adverbial modiﬁers (mod), negation
(neg), external knowledge (know) or other. Table 4
shows the results.

Vocabulary: The most common way to express
sentiment in hotel reviews is through the use of
polar adjectives (as in “the room was great) or the
mention of certain nouns that are desirable (“it
had a pool”). Although this phenomenon has the
largest total number of mistakes (an average of
71 per model on binary and 167 on 4-class), it is
mainly due to its prevalence. MT performed the
best on the test examples which according to the an-
notation require a correct understanding of the vo-
cabulary (81 F1 on binary /54 F1 on 4-class), with
BLSE (79/48) slightly worse. ARTETXE (70/35)
and BARISTA (67/41) perform signiﬁcantly worse.

Figure 4: Average cosine similarity between a subsample of translation pairs of same polarity (“sentiment
synonyms”) and of opposing polarity (“sentiment antonyms”) in both target and source languages in each
model. The x-axis shows training epochs. We see that BLSE is able to learn that sentiment synonyms
should be close to one another in vector space and sentiment antonyms should not.

Research into projection techniques for bilingual
word embeddings (Mikolov et al., 2013; Lazaridou
et al., 2015; Artetxe et al., 2016) often uses a lex-
icon of the most frequent 8–10 thousand words
in English and their translations as training data.
We test this approach by taking the 10,000 word-
to-word translations from the Apertium English-
to-Spanish dictionary9. We also use the Google
Translate API to translate the NRC hashtag senti-
ment lexicon (Mohammad et al., 2013) and keep
the 22,984 word-to-word translations. We perform
the same experiment as above and vary the amount
of training data from 0, 100, 300, 600, 1000, 3000,
6000, 10,000 up to 20,000 training pairs. Finally,
we compile a small hand translated dictionary of
200 pairs, which we then expand using target lan-
guage morphological information, ﬁnally giving
us 657 translation pairs10. The macro F1 score for
the Bing Liu dictionary climbs constantly with the
increasing translation pairs. Both the Apertium
and NRC dictionaries perform worse than the trans-
lated lexicon by Bing Liu, while the expanded hand
translated dictionary is competitive, as shown in
Figure 3.

While for some tasks, e. g., bilingual lexicon
induction, using the most frequent words as trans-
lation pairs is an effective approach, for sentiment
analysis, this does not seem to help. Using a trans-
lated sentiment lexicon, even if it is small, gives
better results.

9http://www.meta-share.org
10The translation took approximately one hour. We can
extrapolate that hand translating a sentiment lexicon the size
of the Bing Liu lexicon would take no more than 5 hours.

Figure 5: BLSE model (solid lines) compared to a
variant without target language projection matrix
M (cid:48) (dashed lines). “Translation” lines show the
average cosine similarity between translation pairs.
The remaining lines show F1 scores for the source
and target language with both varints of BLSE. The
modiﬁed model cannot learn to predict sentiment
in the target language (red lines). This illustrates
the need for the second projection matrix M (cid:48).

6.3 Analysis of M (cid:48)

The main motivation for using two projection ma-
trices M and M (cid:48) is to allow the original embed-
dings to remain stable, while the projection ma-
trices have the ﬂexibility to align translations and
separate these into distinct sentiment subspaces. To
justify this design decision empirically, we perform
an experiment to evaluate the actual need for the
target language projection matrix M (cid:48): We create a
simpliﬁed version of our model without M (cid:48), using
M to project from the source to target and then P
to classify sentiment.

The results of this model are shown in Figure 5.
The modiﬁed model does learn to predict in the
source language, but not in the target language.
This conﬁrms that M (cid:48) is necessary to transfer sen-
timent in our model.

7 Qualitative Analyses of Joint Bilingual

Sentiment Space

In order to understand how well our model trans-
fers sentiment information to the target language,
we perform two qualitative analyses. First, we
collect two sets of 100 positive sentiment words
and one set of 100 negative sentiment words. An
effective cross-lingual sentiment classiﬁer using
embeddings should learn that two positive words
should be closer in the shared bilingual space than a
positive word and a negative word. We test if BLSE
is able to do this by training our model and after
every epoch observing the mean cosine similarity
between the sentiment synonyms and sentiment
antonyms after projecting to the joint space.

We compare BLSE with ARTETXE and BARISTA
by replacing the Linear SVM classiﬁers with the
same multi-layer classiﬁer used in BLSE and ob-
serving the distances in the hidden layer. Figure 4
shows this similarity in both source and target lan-
guage, along with the mean cosine similarity be-
tween a held-out set of translation pairs and the
macro F1 scores on the development set for both
source and target languages for BLSE, BARISTA,
and ARTETXE. From this plot, it is clear that BLSE
is able to learn that sentiment synonyms should be
close to one another in vector space and antonyms
should have a negative cosine similarity. While
the other models also learn this to some degree,
jointly optimizing both sentiment and projection
gives better results.

Secondly, we would like to know how well the
projected vectors compare to the original space.
Our hypothesis is that some relatedness and simi-
larity information is lost during projection. There-
fore, we visualize six categories of words in t-SNE
(Van der Maaten and Hinton, 2008): positive senti-
ment words, negative sentiment words, functional
words, verbs, animals, and transport.

The t-SNE plots in Figure 6 show that the posi-
tive and negative sentiment words are rather clearly
separated after projection in BLSE. This indicates
that we are able to incorporate sentiment informa-
tion into our target language without any labeled
data in the target language. However, the downside

Figure 6: t-SNE-based visualization of the Spanish
vector space before and after projection with BLSE.
There is a clear separation of positive and negative
words after projection, despite the fact that we have
used no labeled data in Spanish.

of this is that functional words and transportation
words are highly correlated with positive sentiment.

8 Conclusion

We have presented a new model, BLSE, which
is able to leverage sentiment information from a
resource-rich language to perform sentiment analy-
sis on a resource-poor target language. This model
requires less parallel data than MT and performs
better than other state-of-the-art methods with sim-
ilar data requirements, an average of 14 percentage
points in F1 on binary and 4 pp on 4-class cross-
lingual sentiment analysis. We have also performed
a phenomena-driven error analysis which showed
that BLSE is better than ARTETXE and BARISTA
at transferring sentiment, but assigns too much sen-
timent to functional words. In the future, we will
extend our model so that it can project multi-word
phrases, as well as single words, which could help
with negations and modiﬁers.

Acknowledgements

We thank Sebastian Pad´o, Sebastian Riedel, Eneko
Agirre, and Mikel Artetxe for their conversations
and feedback.

References

Rodrigo Agerri, Josu Bermudez, and German Rigau.
2014. Ixa pipeline: Efﬁcient and ready to use mul-
tilingual nlp tools. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14). pages 3823–3828.

Rodrigo Agerri, Montse Cuadros, Sean Gaines, and
German Rigau. 2013. OpeNER: Open polarity
Sociedad
enhanced named entity recognition.
Espa˜nola para el Procesamiento del Lenguaje Nat-
ural 51(Septiembre):215–218.

Mariana S. C. Almeida, Claudia Pinto, Helena Figueira,
Pedro Mendes, and Andr´e F. T. Martins. 2015.
Aligning opinions: Cross-lingual opinion mining
with dependencies. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers). pages 408–418.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing. pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). pages 451–462.

Alexandra Balahur and Marco Turchi. 2014. Compar-
ative experiments using supervised learning and ma-
chine translation for multilingual sentiment analysis.
Computer Speech & Language 28(1):56–75.

Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more lan-
In Proceedings of the 23rd Inter-
guages better?
national Conference on Computational Linguistics
(Coling 2010). pages 28–36.

Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity anal-
In Proceedings of
ysis using machine translation.
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing. pages 127–135.

Jeremy Barnes, Patrik Lambert, and Toni Badia. 2018.
Multibooked: A corpus of basque and catalan hotel
reviews annotated for aspect-level sentiment classiﬁ-
cation. In Proceedings of 11th Language Resources
and Evaluation Conference (LREC’18).

Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, Cur-
ran Associates, Inc., pages 1853–1861.

Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Q.
Adver-
Weinberger, and Claire Cardie. 2016.
sarial deep averaging networks for cross-lingual
CoRR abs/1606.01614.
sentiment classiﬁcation.
http://arxiv.org/abs/1606.01614.

Erkin Demirtas and Mykola Pechenizkiy. 2013. Cross-
lingual polarity detection with machine translation.
Proceedings of the International Workshop on Issues
of Sentiment Discovery and Opinion Mining - WIS-
DOM ’13 pages 9:1–9:8.

Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for cross-lingual senti-
ment classiﬁcation? Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers 2:429–433.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. BilBOWA: Fast bilingual distributed repre-
sentations without word alignments. Proceedings
of The 32nd International Conference on Machine
Learning pages 748–756.

Stephan Gouws and Anders Søgaard. 2015. Simple
In Pro-
task-speciﬁc bilingual word embeddings.
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
1386–1390.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 58–
68.

Minqing Hu and Bing Liu. 2004. Mining opinion
In Proceedings of
features in customer reviews.
the 10th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2004). pages 168–177.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daume III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classiﬁcation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). Beijing,
China, pages 1681–1691.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR) .

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: delving into
cross-space mapping for zero-shot learning. Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing pages 270–280.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. pages 142–150.

Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-lingual
mixture model for sentiment classiﬁcation. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Jeju Island, Korea, pages 572–581.
http://www.aclweb.org/anthology/P12-1060.

Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics. pages 976–983.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9:2579–2605.

Xiaojun Wan. 2009. Co-training for cross-lingual sen-
In Proceedings of the Joint
timent classiﬁcation.
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP. pages 235–
243.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andr´es Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing. pages 60–
68.

Min Xiao and Yuhong Guo. 2012. Multi-view ad-
aboost for multilingual subjectivity analysis. In Pro-
ceedings of COLING 2012. pages 2851–2866.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
Exploiting similarities among languages
CoRR abs/1309.4168.

2013.
for machine translation.
http://arxiv.org/abs/1309.4168.

Alexander Yeh. 2000. More accurate tests for the statis-
tical signiﬁcance of result differences. In Proceed-
ings of the 18th Conference on Computational lin-
guistics (COLING). pages 947–953.

Guangyou Zhou, Zhiyuan Zhu, Tingting He, and Xiao-
hua Tony Hu. 2016. Cross-lingual sentiment classi-
ﬁcation with stacked autoencoders. Knowledge and
Information Systems 47(1):27–44.

HuiWei Zhou, Long Chen, Fulin Shi, and Degen
Huang. 2015. Learning bilingual sentiment word
embeddings for cross-language sentiment classiﬁ-
In Proceedings of the 53rd Annual Meet-
cation.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
pages 430–440.

Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svet-
lana Kiritchenko. 2014. An empirical study on the
effect of negation words on sentiment. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). pages 304–313.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of the seventh international workshop on Se-
mantic Evaluation Exercises (SemEval-2013).

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classiﬁcation using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical methods in natu-
ral language processing-Volume 10. Association for
Computational Linguistics, pages 79–86.

Adam Paszke, Sam Gross, Soumith Chintala, and Gre-
gory Chanan. 2016. Pytorch deeplearning frame-
work. http://pytorch.org. Accessed: 2017-08-10.

Peter Prettenhofer and Benno Stein. 2011. Cross-
lingual adaptation using structural correspondence
learning. ACM Transactions on Intelligent Systems
and Technology 3(1):1–22.

Mohammad Sadegh Rasooli, Noura Farra, Axinia
Radeva, Tao Yu, and Kathleen McKeown. 2017.
Cross-lingual sentiment
transfer with limited re-
sources. Machine Translation .

Johan Reitan, Jørgen Faret, Bj¨orn Gamb¨ack, and Lars
Bungum. 2015. Negation scope detection for twitter
sentiment analysis. In Proceedings of the 6th Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis. pages 99–108.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin. 2014. Learning sentiment-speciﬁc
word embedding for twitter sentiment classiﬁcation.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). pages 1555–1565.

