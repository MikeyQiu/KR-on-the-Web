Jointly Optimizing Diversity and Relevance in Neural Response
Generation

Xiang Gao

Sungjin Lee

Yizhe Zhang

Chris Brockett

Michel Galley

Jianfeng Gao

Bill Dolan

Microsoft Research, Redmond, WA, USA
{xiag,sule,yizzhang,chrisbkt,mgalley,jfgao,billdol}@microsoft.com

9
1
0
2
 
r
p
A
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
5
0
2
1
1
.
2
0
9
1
:
v
i
X
r
a

Abstract

Although recent neural conversation models
have shown great potential, they often generate
bland and generic responses. While various
approaches have been explored to diversify the
output of the conversation model, the improve-
ment often comes at the cost of decreased rel-
evance (Zhang et al., 2018).
In this paper,
we propose a SPACEFUSION model to jointly
optimize diversity and relevance that essen-
tially fuses the latent space of a sequence-
to-sequence model and that of an autoen-
coder model by leveraging novel regulariza-
tion terms. As a result, our approach induces a
latent space in which the distance and direction
from the predicted response vector roughly
match the relevance and diversity, respectively.
This property also lends itself well to an intu-
itive visualization of the latent space. Both au-
tomatic and human evaluation results demon-
strate that the proposed approach brings signif-
icant improvement compared to strong base-
lines in both diversity and relevance. 1

1

Introduction

The ﬁeld of neural response generation is advanc-
ing rapidly both in terms of research and commer-
cial applications (Gao et al., 2019; Zhou et al.,
2018; Yoshino et al., 2019; Zhang et al., 2019).
Nevertheless, vanilla sequence-to-sequence (S2S)
models often generate bland and generic responses
(Li et al., 2016a). Li et al. (2016a) encourage di-
versity by re-ranking the beam search results ac-
cording to their mutual information with the con-
versation context. However, as beam search itself
often produces lists of nearly identical sequences,
this method can require a large beam width (e.g.
200). As a result, re-ranking can be extremely

1An implementation of our model is available at https:

//github.com/golsun/SpaceFusion

2For simplicity, we omitted the response at the center: ”I
would love to play this game”. See Table 2 for more details.

Figure 1: Illustration of one context and its multiple
responses in the latent space induced by our model.
Distance and direction from the predicted response
vector given the context roughly match the relevance
and diversity, respectively. Based on the example in
Table 2.2

time-consuming, raising difﬁculties for real-time
applications. This highlights the need to improve
the diversity of candidates before re-ranking, and
the need to optimize for diversity during training
rather than just at the decoding stage.

While various approaches have been explored
to diversify the output of conversation models, the
improvement often comes at the cost of decreased
response relevance along other dimensions. For
instance, Zhao et al. (2017) present an approach
to enhancing diversity by mapping diverse re-
sponses to a probability distribution using a con-
ditional variational autoencoder (CVAE). Despite
the improved response diversity, this approach re-
duces response relevance as measured against the
baseline. One possible reason for this diversity-
relevance trade-off is that such probabilistic ap-
proaches are not explicitly encouraged to induce
a disentangled representation in latent space for

controlling diversity and relevance independently.
Consider a Gaussian distribution, which is widely
used for CVAE. A Gaussian distribution naturally
brings frequent responses near its mean and such
responses are often generic and boring. To gen-
erate diverse and interesting responses, one needs
to sample a little distance from the mean. But do-
ing so naturally leads to infrequent and thus even
irrelevant responses.

In this paper, we propose a novel geometrical
approach that explicitly encourages a structured
latent space in which the distance and direction
from a predicted response vector roughly match
the relevance and diversity, respectively, as illus-
trated in Figure 1. To induce such a latent space,
we leverage two different models: 1) a S2S model,
producing the predicted response vector (the black
dot at the center in Figure 1), and 2) an autoen-
coder (AE) model, yielding the vectors for poten-
tial responses (the colored dots). In order to make
the S2S and AE share the same latent space (the
cloud), we use the same decoder for both and train
them jointly end-to-end with novel regularization
terms. As this fuses the two latent spaces, we refer
to our model as SPACEFUSION.

Regularization is necessary because only shar-
ing the decoder, as in (Luan et al., 2017), does
not necessarily align the latent spaces obtained by
S2S and AE respectively or impose a disentan-
gled structure onto the space. We introduce two
regularization terms to tackle this issue. 1) inter-
polation term: we encourage a smooth semantic
transition along the path between the predicted re-
sponse vector and each target response vector (ar-
rowed lines in Figure 1). This term effectively pre-
vents semantically different responses from align-
ing in the same direction, essentially scattering
them over different directions. 2) fusion term:
we want the vectors from the two models to be
distributed in a homogeneous manner, rather than
forming two separate clusters (Figure 5) that can
potentially make sampling non-trivial. With the
resulting latent space, we can control relevance
and diversity by respectively adjusting distance
and direction from a predicted response vector,
without sacriﬁcing each other greatly.

Our approach also lends itself well to the intu-
itive visualization of latent space. Since our model
allows us to geometrically ﬁnd not only the pre-
dicted response vector but also the target response
vector as in Figure 5, we can visually interpret the

structure of latent space and identify major issues
thereof. We devote Section 5.1 to show compre-
hensive examples for visualization-based analysis.
Automatic and human evaluations demonstrate
that the proposed approach improves both the di-
versity and relevance of the responses, compared
to strong baselines on two datasets with one-to-
many context-response mapping.

2 Related Work

Grounded conversation models utilize extra
context inputs besides conversation history, such
as persona (Li et al., 2016b), textual knowledge
(Ghazvininejad et al., 2017; Galley et al., 2019),
dialog act (Zhao et al., 2017) and emotion (Huber
et al., 2018). Our approach does not depend on
such extra input and thus is complementary to this
line of studies.

Variational autoencoder (VAE) models
ex-
plicitly model the uncertainty of responses in la-
tent space. Bowman et al. (2016) used VAE with
Long-Short Term Memory (LSTM) cells to gen-
erate sentences. The basic idea of VAE is to en-
code the input x into a probability distribution
(e.g. Gaussian) z instead of a point encoding.
However, it suffers from the vanishing latent vari-
able problem (Bowman et al., 2016; Zhao et al.,
2017) when applied to text generation tasks. Bow-
man et al. (2016); Fu et al. (2019) proposed to
tackle this problem with word dropping and spe-
ciﬁc KL annealing methods. Zhao et al. (2017)
proposed to add a bag-of-word loss, complemen-
tary to KL annealing. Applying this to a CVAE
conversation model, they showed that even greedy
decoding can generate diverse responses. How-
ever, as VAE/CVAE conversation models can be
limited to a simple latent representations such as
standard Gaussian distribution, Gu et al. (2018)
proposed to enrich the latent space by leveraging
a Gaussian mixture prior. Our work takes a geo-
metrical approach that is fundamentally different
from probabilistic approaches to tackle the limita-
tions of parameteric distributions in representation
and difﬁculties in training.

Decoding and ranking
encourage diversity dur-
ing the decoding stage. As “vanilla” beam search
often produces lists of nearly identical sequences,
Vijayakumar et al. (2016) propose to include a dis-
similarity term in the objective of beam search
decoding. Li et al. (2016a) re-ranked the results

obtained by beam search based on mutual infor-
mation with the context using a separately trained
response-to-context S2S model.

Multi-task learning is another line of studies
related to the present work (see Section 3.2).
Sennrich et al. (2016) use multi-task learning to
improve neural machine translation by utilizing
monolingual data, which usually far exceeds the
amount of parallel data. A similar idea is applied
by Luan et al. (2017) to conversational modeling,
involving two tasks: 1) a S2S model that learns
a context-to-response mapping using conversation
data, and 2) an AE model that utilizes speaker-
speciﬁc non-conversational data. The decoders of
S2S and AE were shared, and the two tasks were
trained alternately.

3 The SPACEFUSION Model

3.1 Problem statement

Let D = [(x0, y0), (x1, y1), · · · , (xn, yn)] denote
a conversational dataset, where xi and yi are a con-
text and its response, respectively. xi consists of
one or more utterances. Our aim is to train a model
on D to generate relevant and diverse responses
given a context.

3.2 Fusing latent spaces

We design our model to induce a latent space
where different responses for a given context are in
different directions around the predicted response
vector, as illustrated in Figure 1. Then we can
obtain diverse responses by varying the direction
and keep their relevance by sampling near the pre-
dicted response vector.

To fulﬁll this goal, we ﬁrst produce the pre-
dicted response representation zS2S and target re-
sponse representations zAE using an S2S model
and an AE model, respectively, as illustrated in
Figure 2. Both encoders are implemented using
stacked Gated Recurrent Unit (GRU) (Cho et al.,
2014) cells followed by a noise layer that adds
multivariate Gaussian noise (cid:15) ∼ N (0, σ2I). We
then explicitly encourage smooth semantic tran-
sition along the path from zS2S to zAE by impos-
ing any interpolation between them to generate the
same response via the following loss term:

Linterp = −

log p(y|zinterp)

(1)

1
|y|

where zinterp = uzS2S + (1 − u)zAE and u ∼
U (0, 1) is a uniformly distributed random vari-

Figure 2: SPACEFUSION model architecture.

|y| is the number of words in y. Note that
able.
it is this regularization term that effectively pre-
vents signiﬁcantly different responses from align-
ing in the same direction, essentially scattering
them over different directions.
In order for this
interpolation loss to work, we share the same de-
coder for both AE and S2S models as in (Luan
et al., 2017). The decoder consists of stacked GRU
cells followed by a softmax layer. It is worth men-
tioning that zinterp is not just randomly drawn from
a single line but from a richer probabilistic region
as both zinterp and zS2S are stochastic due to the
random component (cid:15).

Now, we want vectors from both the AE and
S2S models to be distributed in a homogeneous
manner scattered over the entire space while keep-
ing the distance between zS2S and zAE as small
as possible for any (context-response) pair in the
training data. This objective is represented in the
following regularization term:

Lfuse =

(cid:88)

d(zS2S(xi), zAE(yi))
n

i∈batch
(cid:88)

i,j∈batch,i(cid:54)=j

(cid:88)

i,j∈batch,i(cid:54)=j

−

−

d(zS2S(xi), zS2S(xj))
n2 − n

d(zAE(yi), zAE(yj))
n2 − n

(2)

where n is the batch size and d(a, b) is the root
mean square of the difference between a and b.
For each batch, we basically disperse vectors ob-
tained by the same model and pull the predicted
response vectors to the corresponding target re-
sponse vectors.
In practice, we found that the
performance is better if the Euclidean distance is
clipped to a prescribed maximum value.3

Finally, with weight parameters α and β, the

3This value is set as 0.3 for the present experiments

loss function is deﬁned as:

L = −

log p(y|zS2S)

−

log p(y|zAE)

1
|y|
1
|y|

+ αLinterp + βLfuse

(3)

train (x, y) samples
test (x, y) samples
ref. source
ref. availability
ref. per context

Switchboard
0.2M
5418
IR+ﬁltering
test only
7.7

Reddit
7.3M
5000
natural
train/vali/test
24.1

Table 1: Key features of the datasets.

As Linterp and Lfuse encourage the path between
zS2S and zAE to be smooth and short while scatter-
ing vectors over the entire space, they effectively
fuse the zS2S latent space and the zAE latent space.
Accordingly we refer this approach as SPACEFU-
SION with path regularization.

train/validate/test, respectively. Each conversation
has multiple turns and thus multiple (x, y) pairs, as
listed in Table 1. As our approach does not utilize
extra information except conversation history, we
removed the meta data (e.g. gender, age, prompt)
from this dataset.

3.3 Training

In contrast to previous multi-task conversation
model (Luan et al., 2017), where S2S and AE are
trained alternately, our approach trains S2S and
AE at the same time by minimizing the loss func-
tion of Equation 3.

3.4

Inference

Like Zhao et al. (2017); Bowman et al. (2016), for
a given context, we sample different latent vectors
to obtain multiple hypotheses. This is done by
adding a random vector r that is uniformly sam-
pled from a hypersphere of radius |r| to the pre-
diction zS2S(x).

z(x, r) = zS2S(x) + r

(4)

where |r| is tuned on the validation set to opti-
mize the trade-off between relevance and diversity.
z(x, r) is then fed to the decoder as the initial state
of GRU cells. We then generate responses using
greedy decoding.4

4 Experiment Setup

4.1 Datasets

We used the following datasets. Some of their key
features are presented in Table 1.

Switchboard: We use the version offered by
Zhao et al. (2017), which is an extension of the
original version by Godfrey and Holliman (1997).
Zhao et al. (2017) collected multiple references for
the test set using information retrieval (IR) tech-
niques followed by human ﬁltering, and randomly
split the data into 2316/60/62 conversations for

Reddit: As the Switchboard dataset is relatively
small and multiple references are synthetically
constructed, we have developed another multi-
reference dataset by extracting posts and com-
ments on Reddit.com during 2011 collected by a
third party.5 As each Reddit post and comment
may have multiple comments, it is a natural source
of multi-reference responses. We further ﬁltered
the data based on the number of replies to ob-
tain the ﬁnal conversation dataset in which each
context has at least 10 different responses, and
on average the number of responses is 24.1 for a
given context. The size is signiﬁcantly larger than
Switchboard, as listed in Table 1. The conversa-
tions are randomly shufﬂed before being split into
train/valid/test subsets.

4.2 Model setup

Both encoders and the shared decoder consist of
two GRU cells, each with 128 hidden units. The
variance of the noise layer in each decoder is
σ2 = 0.12. The word embedding dimension is
128. The weight parameters (see Equation 3) are
set as α = 1 and β = 30. For both datasets, the
inference radius |r| (see Equation 4) is set to 1.5
which optimizes F1 score on the validation set.
All models are trained using the Adam method
(Kingma and Ba, 2014) with a learning rate of
0.001 on both datasets until convergence (around 4
epochs for Reddit and 10 epochs for Switchboard).

4.3 Automatic evaluation

For a given context x, we have Nr reference re-
sponses and generate the same number of hypothe-

4Although we use greedy decoding in this work, other de-

5http://files.pushshift.io/reddit/

coding techniques, such as beam search, can be applied.

comments/

ses.6 We deﬁne the following metrics based on 4-
gram BLEU (Papineni et al., 2002), as suggested
by Zhao et al. (2017).

1
Nr

1
Nr

Nr(cid:88)

i=1

Nr(cid:88)

j=1

Precision =

max
j∈[1,Nr]

BLEU(rj, hi)

Recall =

max
i∈[1,Nr]

BLEU(rj, hi)

F1 = 2

precision · recall
precision + recall

We use Precision as an approximate surrogate
metric for relevance and Recall for diversity.
It
should be noted that recall is not equivalent to
other diversity metrics, e.g., distinct (Li et al.,
2016a) and entropy (Zhang et al., 2018), which
only depend on hypotheses. One potential issue
of these metrics is that even randomly generated
responses may yield a high diversity score. F1 is
the harmonic average of these two and is used to
measure the overall response quality.

4.4 Human evaluation

We conduct a human evaluation using crowdwork-
ers. For each hypothesis, given its context, we ask
three annotators to individually measure the qual-
ity, on a scale of 1 to 5, in terms of two aspects:
relevance and interest.
Interestingness is treated
as an estimation of the diversity, as these two are
often correlated. The hypotheses from all systems
are shufﬂed before being provided to annotators.
System names are invisible to the annotators.

4.5 Baselines

We compare the proposed model with the follow-
ing baseline models:

S2S+Sampling: We consider a vanilla version
of S2S model. The dimensions are similar to our
model: both encoder and decoder consist of two
stacked GRU cells with 128 hidden units, and the
word embedding size is 128. As in the baseline in
Zhao et al. (2017), we applied softmax sampling
at inference time to generate multiple hypotheses.

CVAE+BOW: For
the CVAE conversation
model, we use the original implementation and

6We set the number of hypotheses equal to the number of
references to encourage precision and recall have comparable
impact on F1

hyperparameters of Zhao et al.
(2017) with
the bag-of-words (BOW) loss. The number of
trainable model parameters is 15.4M, which is
much larger than our model (3.2M).

MTask: Since our approach utilizes a multi-task
learning scheme, we also compare it against a
vanilla multi-task learning model, MTask, sim-
ilar to (Luan et al., 2017), to illustrate the ef-
fect of space fusion. The model architecture
and hyperparameters are identical
to the pro-
posed model except that the loss function is L =
− log p(y|zS2S) − log p(y|zAE).

5 Results and Analysis

5.1

In-depth analysis of latent space

In this section, we undertake an in-depth analysis
to verify whether the latent space induced by our
method manifests desirable properties, namely:
1) disentangled space structure between relevance
and diversity, 2) homogeneous space distribution
in which semantics changes smoothly without
holes. We ﬁrst provide a qualitative investigation
based on real examples. Then, we present a set of
corpus-level quantitative analyses focused on geo-
metric properties.

5.1.1 Qualitative examples

In Table 2, we investigate three different directions
from the context “Anyone want to start
this game?” , which is a real example taken
from Reddit. The three different directions cor-
respond to clearly different semantics: “No I
don’t”, “when?” and “Yes I do.” If we
generate a response with the vector predicted by
the S2S model (u = 0), our model outputs “I
would love to play this game” which
is highly relevant to the context. Now as we
move along each direction, we can see our model
gradually transforms the response toward the cor-
responding responses of each direction. For in-
stance, towards “No I don’t”, our model grad-
ually transforms the response to “I am not
interested in the game” (u = 0.18)
and then “I am not interested.” (u =
In contrary, towards “Yes I do”, the
0.21).
response transforms to “I would love to
play it.” (u = 0.15). Besides the positive or
negative directions, the same transition applies to
other directions such as “When?”. This example
clearly shows that there is a rough correspondence

towards “No I don’t.”
I am not interested in the game.
I am not interested.

u
0.18
0.21
0.30 No I don’t.

context x: Anyone want to start this game?
response at u = 0: I would love to play this game.
u
0.15
0.27 Yes I do.

towards “when?”
I’d be interested in the game

u
0.15
0.31 When is it?
0.40 When will you?
1.00 When?

towards “Yes I do.”
I’d love to play it.

Table 2: Semantic interpolation along different directions y. Results decoded from zinterp See Fig. 1 for a visual-
ization.

.

context x: Anyone want to start this game?
towards one possible target y: Yes I do.

with regularization
I would love to play this game.
I would love to play it.

u
0.00
0.15
0.30 Yes I do

without regularization
I would have to play with the game.

u
0.00
0.29 Dude, I know, but, or etc.
0.61 Op I was after though today
0.85
I’m single :( though
0.90 Yes I do.

Table 3: Semantic interpolation with and without regularization. Results decoded from zinterp .

between geometric properties and semantic prop-
erties in the latent space induced by our method as
shown in Figure 1– the relevance of the response
decreases as we move away from the predicted re-
sponse vector and different directions are associ-
ated with semantically different responses.

5.1.2 Direction vs. diversity

In order to quantitatively verify the correspon-
dence between direction and diversity, we visu-
alize the distribution of cosine similarities among
multiple references for each context for a set of
1000 random samples drawn from the test dataset.
Speciﬁcally, for a context xk and its associated
reference responses [yk,0, yk,1, · · · ], we compute
the cosine similarity between zAE(yk,i) − zS2S(xk)
and zAE(yk,j) − zS2S(xk). In Figure 3, we com-
pare the distribution of our model with that of
MTask, which does not employ our regulariza-
tion terms. While our method yields a bell shaped
curve with average cosine similarity being close to
zero (0.38), the distribution of MTask is extremely
skewed with average cosine similarity being close
to 1 (0.95). This indicates that the directions of the
reference responses are more evenly distributed in
our latent space whereas everything is packed in
a narrow band in the MTask’s space. This essen-
tially makes the inference process simple and ro-
bust in that one can choose arbitrary directions to
generate diverse responses.

5.1.3 Distance vs. relevance

In order to quantitatively verify the correspon-
dence between distance and relevance, we visu-

Figure 3: Distribution of the directions from a given
context to its multiple responses, measured by the
cosine similarity between zAE(yk,i) − zS2S(xk) and
zAE(yk,j) − zS2S(xk). Histogram calculated based on
1000 xk from Reddit test data and visualized with bin
width of 0.02.

alize the perplexity of reference responses along
the path from the associated zS2S (u = 0) to the
zAE (u = 1) corresponding to the predicted re-
sponse. In Figure 4, we compare our model with
MTask, which as already noted, does not employ
our regularization terms. While our model shows
a gradual increase in perplexity, there is a huge
bump for MTask’s line. This clearly indicates that
there is a rough correspondence between distance
and relevance in our latent space whereas even a
slight change can lead to an irrelevant response in
the MTask’s space.

We further illustrate the smooth change in rel-
evance according to distance for a speciﬁc ex-
ample in Table 3. Given the context “Anyone
want to start this game?”, our model

Figure 4: Perplexity of zinterp on the Reddit test dataset
as a function of u for simple multi-task model (without
regularization, dashed line) and SPACEFUSION (with
regularization, solid line).

is able to transition from the predicted response
“I would love to play this game” to
a one of reference responses “Yes I do”. The
relevance smoothly descreases, generating inter-
mediate responses such as “I would love to
play it.” In contrary, the MTask model tends
to produce irrelevant or ungrammatical responses
as it moves away from the predicted response.

5.1.4 Homogeneity and Convexity

Other desirable properties, with which we want to
equip our latent space are homogeneity and con-
vexity. If the space is not homogeneous, we have
to sample differently depending on the regional
traits. If the space is not convex, we have to worry
about running into the holes that are not properly
associated with valid semantic meanings. In order
to verify homogeneity and convexity, we visual-
ize our latent space in a 2D space produced by the
multidimensional scaling (MDS) algorithm (Borg
and Groenen, 2003), which approximately pre-
serves pairwise distance. For comparison, we also
provide a visualization for MTask. As shown in
Figure 5, our latent space offers great homogeneity
and convexity regardless of which model is used
to produce a dot (i.e. zS2S or zAE). In contrary,
MTask’s latent space forms two separate clusters
for zS2S and zAE with a large gap in-between
where no training samples were mapped to.

Figure 5: MDS visualization of the two latent spaces:
zs2s (red dots) and zAE (blue dots) of 1000 ran-
domly picked (x, y) pairs from the Reddit test dataset.
Left: multi-task model (without regularization); right:
SPACEFUSION (with regularization).

For fair comparison, λ is tuned such that the aver-
age hypothesis length becomes roughly the same
for all systems and approaches the average length
of the references.7

The automatic evaluation results are reported in
Table 4. On both datasets, the proposed system
consistently outperforms the baselines by a large
margin in Precision, Recall, and F1.

Examples of system outputs and human refer-
ences can be found in Table 5 and Table 6 for
Reddit and Switchboard, respectively. As shown
in the examples, CVAE+BOW and other baseline
models may generate diverse but not-so-relevant
responses.

dataset

Switchboard

Reddit

model
SPACEFUSION
CVAE+BOW
MTask
S2S+Sampling
SPACEFUSION
CVAE+BOW
MTask
S2S+Sampling

Precision Recall F1
1.22
0.76
0.75
0.57
0.40
0.16
0.31
0.10

0.66
0.57
0.43
0.48
0.26
0.18
0.18
0.11

0.86
0.65
0.54
0.52
0.31
0.17
0.23
0.11

Table 4: Performance of each model on automatic mea-
sures. The highest score in each row is in bold for each
dataset. Note that our BLEU scores are normalized to
[0, 100].

5.3 Human evaluation

5.2 Automatic evaluation

We let each system generate 100 hypotheses {hj}
for each context xi in the test dataset. Assuming
xi has Nr,i references, we pick the top Nr,i distinct
hypotheses ranked by log p(hj|xi)+λ|hj|. Similar
to (Li et al., 2016a; Wu et al., 2016), we takes |hj|
into consideration, as BLEU is sensitive to length.

We randomly sampled 500 contexts from the
Reddit
test dataset and picked the top 1 hy-
pothesis generated for each context ranked by
log p(hj|xi) + λ|hj|. As in the automatic evalua-
tion, we tuned λ such that all systems have roughly

7Approximately 10 words/tokens for Switchboard and 12

for Reddit

context
SPACE
FUSION

CVAE
+BOW

MTask

S2S+
sampling

human

Everything about this movie is awesome!
◦ I love this movie.
◦ It’s so awesome!!! I have no idea how
to watch this movie. I can’t wait for the
trailer.
◦ I don’t think i’m a fan of the movie.
◦ I would love to see this.
◦ I want to watch this movie.
◦ Smartphones of the best games!.
◦ I’m in the same boat! I feel the same
way about this
◦ I don’t know why but can’t tell if the
New York will be the only one.
◦ Caves would never say yes, but I’d love
to know.
◦ I know where this movie is.
◦ This is so funny.
◦ I think I saw this.
◦ I don’t know how many people do you.
◦ I would.
◦ I remember watching was was disap-
pointed.
◦ Whoa man, this is amazing!!!
◦ Man, I love Tom Cruise and I love the
show.
◦ Is this a bill on the right?
◦ More like samurai jack.
◦ I weep for the hivemind.
◦ I love this movie.
◦ Watched this yesterday! so amazing.
◦ My favorite Muppet movie by far.
◦ Seriously. Good times. Great oldies.
◦ Now you have got the song into my
head.

Table 5: Example system outputs and human reference
on Reddit dataset

the same average hypothesis length. We also ran-
domly select one reference for each context and
compare them with the systems (labeled ”human”
in Table 7)

As illustrated in Table 7, the proposed model
outperforms all systems except human, consistent
with our automatic evaluation results.

SPACEFUSION
CVAE+BOW
Multi-Task
S2S+Sampling
human

relevance
2.72
2.51
2.34
2.58
3.59

interest
2.53
2.37
2.14
2.43
3.41

average
2.63
2.44
2.24
2.50
3.50

Table 7: Performance of each model on human evalu-
ation. The highest score, except human, in each row is
in bold.

6 Conclusion

We propose a SPACEFUSION model to jointly op-
timize diversity and relevance that leverages novel
regularization terms to essentially fuse the latent
space of a S2S model with that of an autoen-

context

SPACE
FUSION

CVAE
+BOW

MTask

S2S+
sampling

human

A: Are they doing a lot of recycling out in ***?
B: Well at my workplace they are. We have
places for aluminum cans and we have every-
body been issued a separate trash can for re-
cyclable paper. Let’s see them get that word
straight.
A: Uh-huh
B: As far as the community goes, it’s pretty
much voluntary. Nothing’s been done. ***.
We have just private garbage services.
◦ They’re not recycling as a matter of fact.
◦ And it’s a waste of time
◦ You know the paper is a big recycling pro-
gram.
◦ But they just put it in the trash
◦ Or if you have a recycling bag or anything
like that.
◦ Um - hum.
◦ Oh that would be nice.
◦ Is that right?
◦ It’s a it’s a luxury
◦ Well that’s interesting.
◦ Um - hum.
◦ That’s right.
◦ I don’t know how much money you’re going
to do it.
◦ That’s right.
◦ Yeah
◦ That’s great.
◦ What do you feel about this uh - huh.
◦ It’s just kind of a glass of town and it’s even
irritating to me. I don’t know
◦ Somebody and it’s so much better.
◦ It now and it’s hard to go.
◦ Yeah it is to some degree inconvenient i’d
have to say that

Table 6: Example system outputs and human refer-
ence on Switchboard dataset. Controversial or offen-
sive words are replaced by ***.

coder model. This fused latent space exhibits de-
sirable properties such as smooth semantic inter-
polation between two points. The distance and di-
rection from the predicted response vector roughly
match relevance and diversity, respectively. These
properties also enable intuitive visualization of the
latent space. Both automatic and human eval-
uation results demonstrate that the proposed ap-
proach brings signiﬁcant improvement compared
to strong baselines in terms of both diversity and
In future work, we will provide the-
relevance.
oretical justiﬁcation of the effectiveness of the
proposed regularization terms. We expect that
this technique will ﬁnd application as an efﬁcient
”mixing board” for conversation that draws on
multiple sources of information.

References

Ingwer Borg and P Groenen. 2003. Modern multidi-
mensional scaling: theory and applications. Journal
of Educational Measurement, 40(3):277–280.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew Dai, Rafal Jozefowicz, and Samy Bengio.
2016. Generating sentences from a continuous
space. In Proceedings of The 20th SIGNLL Confer-
ence on Computational Natural Language Learning,
pages 10–21.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. In Proceedings of SSST-8, Eighth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation, pages 103–111.

Hao Fu, Chunyuan Li, Xiaodong Liu,

Jianfeng
Gao, Asli Celikyilmaz,
and Lawrence Carin.
2019. Cyclical annealing schedule: A simple ap-
proach to mitigating kl vanishing. arXiv preprint
arXiv:1903.10145.

Michel Galley, Chris Brockett, Xiang Gao, Jianfeng
Gao, and Bill Dolan. 2019. Grounded response gen-
eration task at dstc7.

Jianfeng Gao, Michel Galley, and Lihong Li. 2019.
Neural approaches to conversational ai. Founda-
tions and Trends R(cid:13) in Information Retrieval, 13(2-
3):127–298.

Marjan Ghazvininejad, Chris Brockett, Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih,
and Michel Galley. 2017. A knowledge-grounded
arXiv preprint
neural conversation model.
arXiv:1702.01932.

John J Godfrey and Edward Holliman. 1997.
Switchboard-1 release 2. Linguistic Data Consor-
tium, Philadelphia, 926:927.

Xiaodong Gu, Kyunghyun Cho, Jungwoo Ha, and
Sunghun Kim. 2018. DialogWAE: Multimodal
response generation with conditional wasserstein
auto-encoder. arXiv preprint arXiv:1805.12352.

Bernd Huber, Daniel McDuff, Chris Brockett, Michel
Galley, and Bill Dolan. 2018. Emotional dialogue
generation using image-grounded language models.
In Proceedings of the 2018 CHI Conference on
Human Factors in Computing Systems, page 277.
ACM.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 110–119.

Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp-
ithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A
In Pro-
persona-based neural conversation model.
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 994–1003.

Yi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao,
and Michel Galley. 2017. Multi-task learning for
speaker-role adaptation in neural conversation mod-
els. In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 605–614.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation mod-
2016.
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 86–96.

Ashwin K Vijayakumar, Michael Cogswell, Ram-
prasath R Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. 2016. Diverse beam
search: Decoding diverse solutions from neural se-
quence models. arXiv preprint arXiv:1610.02424.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016.
Google’s neural ma-
chine translation system: Bridging the gap between
arXiv preprint
human and machine translation.
arXiv:1609.08144.

Koichiro Yoshino, Chiori Hori, Julien Perez, Luis Fer-
nando D’Haro, Lazaros Polymenakos, Chulaka Gu-
nasekara, Walter S Lasecki, Jonathan K Kummer-
feld, Michel Galley, Chris Brockett, et al. 2019. Di-
alog system technology challenge 7. arXiv preprint
arXiv:1901.03461.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,
Xiujun Li, Chris Brockett, and Bill Dolan. 2018.
Generating informative and diverse conversational
responses via adversarial information maximization.
In Advances in Neural Information Processing Sys-
tems, pages 1813–1823.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North

Yizhe Zhang, Xiang Gao, Sungjin Lee, Chris Brockett,
Michel Galley, Jianfeng Gao, and Bill Dolan. 2019.
Consistent dialogue generation with self-supervised
feature learning. arXiv preprint arXiv:1903.05759.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
In Proceedings of the 55th Annual Meet-
coders.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 654–664.

Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung
Shum. 2018. The design and implementation of xi-
aoice, an empathetic social chatbot. arXiv preprint
arXiv:1812.08989.

